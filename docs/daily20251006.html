<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251005.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Towards Scalable and Consistent 3D Editing", "author": "Ruihao Xia and Yang Tang and Pan Zhou", "abstract": "  3D editing - the task of locally modifying the geometry or appearance of a 3D\nasset - has wide applications in immersive content creation, digital\nentertainment, and AR/VR. However, unlike 2D editing, it remains challenging\ndue to the need for cross-view consistency, structural fidelity, and\nfine-grained controllability. Existing approaches are often slow, prone to\ngeometric distortions, or dependent on manual and accurate 3D masks that are\nerror-prone and impractical. To address these challenges, we advance both the\ndata and model fronts. On the data side, we introduce 3DEditVerse, the largest\npaired 3D editing benchmark to date, comprising 116,309 high-quality training\npairs and 1,500 curated test pairs. Built through complementary pipelines of\npose-driven geometric edits and foundation model-guided appearance edits,\n3DEditVerse ensures edit locality, multi-view consistency, and semantic\nalignment. On the model side, we propose 3DEditFormer, a\n3D-structure-preserving conditional transformer. By enhancing image-to-3D\ngeneration with dual-guidance attention and time-adaptive gating, 3DEditFormer\ndisentangles editable regions from preserved structure, enabling precise and\nconsistent edits without requiring auxiliary 3D masks. Extensive experiments\ndemonstrate that our framework outperforms state-of-the-art baselines both\nquantitatively and qualitatively, establishing a new standard for practical and\nscalable 3D editing. Dataset and code will be released. Project:\nhttps://www.lv-lab.org/3DEditFormer/\n", "link": "http://arxiv.org/abs/2510.02994v1", "date": "2025-10-03", "relevancy": 3.1167, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6308}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6308}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Scalable%20and%20Consistent%203D%20Editing&body=Title%3A%20Towards%20Scalable%20and%20Consistent%203D%20Editing%0AAuthor%3A%20Ruihao%20Xia%20and%20Yang%20Tang%20and%20Pan%20Zhou%0AAbstract%3A%20%20%203D%20editing%20-%20the%20task%20of%20locally%20modifying%20the%20geometry%20or%20appearance%20of%20a%203D%0Aasset%20-%20has%20wide%20applications%20in%20immersive%20content%20creation%2C%20digital%0Aentertainment%2C%20and%20AR/VR.%20However%2C%20unlike%202D%20editing%2C%20it%20remains%20challenging%0Adue%20to%20the%20need%20for%20cross-view%20consistency%2C%20structural%20fidelity%2C%20and%0Afine-grained%20controllability.%20Existing%20approaches%20are%20often%20slow%2C%20prone%20to%0Ageometric%20distortions%2C%20or%20dependent%20on%20manual%20and%20accurate%203D%20masks%20that%20are%0Aerror-prone%20and%20impractical.%20To%20address%20these%20challenges%2C%20we%20advance%20both%20the%0Adata%20and%20model%20fronts.%20On%20the%20data%20side%2C%20we%20introduce%203DEditVerse%2C%20the%20largest%0Apaired%203D%20editing%20benchmark%20to%20date%2C%20comprising%20116%2C309%20high-quality%20training%0Apairs%20and%201%2C500%20curated%20test%20pairs.%20Built%20through%20complementary%20pipelines%20of%0Apose-driven%20geometric%20edits%20and%20foundation%20model-guided%20appearance%20edits%2C%0A3DEditVerse%20ensures%20edit%20locality%2C%20multi-view%20consistency%2C%20and%20semantic%0Aalignment.%20On%20the%20model%20side%2C%20we%20propose%203DEditFormer%2C%20a%0A3D-structure-preserving%20conditional%20transformer.%20By%20enhancing%20image-to-3D%0Ageneration%20with%20dual-guidance%20attention%20and%20time-adaptive%20gating%2C%203DEditFormer%0Adisentangles%20editable%20regions%20from%20preserved%20structure%2C%20enabling%20precise%20and%0Aconsistent%20edits%20without%20requiring%20auxiliary%203D%20masks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20baselines%20both%0Aquantitatively%20and%20qualitatively%2C%20establishing%20a%20new%20standard%20for%20practical%20and%0Ascalable%203D%20editing.%20Dataset%20and%20code%20will%20be%20released.%20Project%3A%0Ahttps%3A//www.lv-lab.org/3DEditFormer/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Scalable%2520and%2520Consistent%25203D%2520Editing%26entry.906535625%3DRuihao%2520Xia%2520and%2520Yang%2520Tang%2520and%2520Pan%2520Zhou%26entry.1292438233%3D%2520%25203D%2520editing%2520-%2520the%2520task%2520of%2520locally%2520modifying%2520the%2520geometry%2520or%2520appearance%2520of%2520a%25203D%250Aasset%2520-%2520has%2520wide%2520applications%2520in%2520immersive%2520content%2520creation%252C%2520digital%250Aentertainment%252C%2520and%2520AR/VR.%2520However%252C%2520unlike%25202D%2520editing%252C%2520it%2520remains%2520challenging%250Adue%2520to%2520the%2520need%2520for%2520cross-view%2520consistency%252C%2520structural%2520fidelity%252C%2520and%250Afine-grained%2520controllability.%2520Existing%2520approaches%2520are%2520often%2520slow%252C%2520prone%2520to%250Ageometric%2520distortions%252C%2520or%2520dependent%2520on%2520manual%2520and%2520accurate%25203D%2520masks%2520that%2520are%250Aerror-prone%2520and%2520impractical.%2520To%2520address%2520these%2520challenges%252C%2520we%2520advance%2520both%2520the%250Adata%2520and%2520model%2520fronts.%2520On%2520the%2520data%2520side%252C%2520we%2520introduce%25203DEditVerse%252C%2520the%2520largest%250Apaired%25203D%2520editing%2520benchmark%2520to%2520date%252C%2520comprising%2520116%252C309%2520high-quality%2520training%250Apairs%2520and%25201%252C500%2520curated%2520test%2520pairs.%2520Built%2520through%2520complementary%2520pipelines%2520of%250Apose-driven%2520geometric%2520edits%2520and%2520foundation%2520model-guided%2520appearance%2520edits%252C%250A3DEditVerse%2520ensures%2520edit%2520locality%252C%2520multi-view%2520consistency%252C%2520and%2520semantic%250Aalignment.%2520On%2520the%2520model%2520side%252C%2520we%2520propose%25203DEditFormer%252C%2520a%250A3D-structure-preserving%2520conditional%2520transformer.%2520By%2520enhancing%2520image-to-3D%250Ageneration%2520with%2520dual-guidance%2520attention%2520and%2520time-adaptive%2520gating%252C%25203DEditFormer%250Adisentangles%2520editable%2520regions%2520from%2520preserved%2520structure%252C%2520enabling%2520precise%2520and%250Aconsistent%2520edits%2520without%2520requiring%2520auxiliary%25203D%2520masks.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520framework%2520outperforms%2520state-of-the-art%2520baselines%2520both%250Aquantitatively%2520and%2520qualitatively%252C%2520establishing%2520a%2520new%2520standard%2520for%2520practical%2520and%250Ascalable%25203D%2520editing.%2520Dataset%2520and%2520code%2520will%2520be%2520released.%2520Project%253A%250Ahttps%253A//www.lv-lab.org/3DEditFormer/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Scalable%20and%20Consistent%203D%20Editing&entry.906535625=Ruihao%20Xia%20and%20Yang%20Tang%20and%20Pan%20Zhou&entry.1292438233=%20%203D%20editing%20-%20the%20task%20of%20locally%20modifying%20the%20geometry%20or%20appearance%20of%20a%203D%0Aasset%20-%20has%20wide%20applications%20in%20immersive%20content%20creation%2C%20digital%0Aentertainment%2C%20and%20AR/VR.%20However%2C%20unlike%202D%20editing%2C%20it%20remains%20challenging%0Adue%20to%20the%20need%20for%20cross-view%20consistency%2C%20structural%20fidelity%2C%20and%0Afine-grained%20controllability.%20Existing%20approaches%20are%20often%20slow%2C%20prone%20to%0Ageometric%20distortions%2C%20or%20dependent%20on%20manual%20and%20accurate%203D%20masks%20that%20are%0Aerror-prone%20and%20impractical.%20To%20address%20these%20challenges%2C%20we%20advance%20both%20the%0Adata%20and%20model%20fronts.%20On%20the%20data%20side%2C%20we%20introduce%203DEditVerse%2C%20the%20largest%0Apaired%203D%20editing%20benchmark%20to%20date%2C%20comprising%20116%2C309%20high-quality%20training%0Apairs%20and%201%2C500%20curated%20test%20pairs.%20Built%20through%20complementary%20pipelines%20of%0Apose-driven%20geometric%20edits%20and%20foundation%20model-guided%20appearance%20edits%2C%0A3DEditVerse%20ensures%20edit%20locality%2C%20multi-view%20consistency%2C%20and%20semantic%0Aalignment.%20On%20the%20model%20side%2C%20we%20propose%203DEditFormer%2C%20a%0A3D-structure-preserving%20conditional%20transformer.%20By%20enhancing%20image-to-3D%0Ageneration%20with%20dual-guidance%20attention%20and%20time-adaptive%20gating%2C%203DEditFormer%0Adisentangles%20editable%20regions%20from%20preserved%20structure%2C%20enabling%20precise%20and%0Aconsistent%20edits%20without%20requiring%20auxiliary%203D%20masks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20baselines%20both%0Aquantitatively%20and%20qualitatively%2C%20establishing%20a%20new%20standard%20for%20practical%20and%0Ascalable%203D%20editing.%20Dataset%20and%20code%20will%20be%20released.%20Project%3A%0Ahttps%3A//www.lv-lab.org/3DEditFormer/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02994v1&entry.124074799=Read"},
{"title": "HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided\n  Versatile Diffusion", "author": "Shiyi Zhang and Dong Liang and Hairong Zheng and Yihang Zhou", "abstract": "  The reconstruction of visual information from brain activity fosters\ninterdisciplinary integration between neuroscience and computer vision.\nHowever, existing methods still face challenges in accurately recovering highly\ncomplex visual stimuli. This difficulty stems from the characteristics of\nnatural scenes: low-level features exhibit heterogeneity, while high-level\nfeatures show semantic entanglement due to contextual overlaps. Inspired by the\nhierarchical representation theory of the visual cortex, we propose the HAVIR\nmodel, which separates the visual cortex into two hierarchical regions and\nextracts distinct features from each. Specifically, the Structural Generator\nextracts structural information from spatial processing voxels and converts it\ninto latent diffusion priors, while the Semantic Extractor converts semantic\nprocessing voxels into CLIP embeddings. These components are integrated via the\nVersatile Diffusion model to synthesize the final image. Experimental results\ndemonstrate that HAVIR enhances both the structural and semantic quality of\nreconstructions, even in complex scenes, and outperforms existing models.\n", "link": "http://arxiv.org/abs/2510.03122v1", "date": "2025-10-03", "relevancy": 3.0963, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6245}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAVIR%3A%20HierArchical%20Vision%20to%20Image%20Reconstruction%20using%20CLIP-Guided%0A%20%20Versatile%20Diffusion&body=Title%3A%20HAVIR%3A%20HierArchical%20Vision%20to%20Image%20Reconstruction%20using%20CLIP-Guided%0A%20%20Versatile%20Diffusion%0AAuthor%3A%20Shiyi%20Zhang%20and%20Dong%20Liang%20and%20Hairong%20Zheng%20and%20Yihang%20Zhou%0AAbstract%3A%20%20%20The%20reconstruction%20of%20visual%20information%20from%20brain%20activity%20fosters%0Ainterdisciplinary%20integration%20between%20neuroscience%20and%20computer%20vision.%0AHowever%2C%20existing%20methods%20still%20face%20challenges%20in%20accurately%20recovering%20highly%0Acomplex%20visual%20stimuli.%20This%20difficulty%20stems%20from%20the%20characteristics%20of%0Anatural%20scenes%3A%20low-level%20features%20exhibit%20heterogeneity%2C%20while%20high-level%0Afeatures%20show%20semantic%20entanglement%20due%20to%20contextual%20overlaps.%20Inspired%20by%20the%0Ahierarchical%20representation%20theory%20of%20the%20visual%20cortex%2C%20we%20propose%20the%20HAVIR%0Amodel%2C%20which%20separates%20the%20visual%20cortex%20into%20two%20hierarchical%20regions%20and%0Aextracts%20distinct%20features%20from%20each.%20Specifically%2C%20the%20Structural%20Generator%0Aextracts%20structural%20information%20from%20spatial%20processing%20voxels%20and%20converts%20it%0Ainto%20latent%20diffusion%20priors%2C%20while%20the%20Semantic%20Extractor%20converts%20semantic%0Aprocessing%20voxels%20into%20CLIP%20embeddings.%20These%20components%20are%20integrated%20via%20the%0AVersatile%20Diffusion%20model%20to%20synthesize%20the%20final%20image.%20Experimental%20results%0Ademonstrate%20that%20HAVIR%20enhances%20both%20the%20structural%20and%20semantic%20quality%20of%0Areconstructions%2C%20even%20in%20complex%20scenes%2C%20and%20outperforms%20existing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAVIR%253A%2520HierArchical%2520Vision%2520to%2520Image%2520Reconstruction%2520using%2520CLIP-Guided%250A%2520%2520Versatile%2520Diffusion%26entry.906535625%3DShiyi%2520Zhang%2520and%2520Dong%2520Liang%2520and%2520Hairong%2520Zheng%2520and%2520Yihang%2520Zhou%26entry.1292438233%3D%2520%2520The%2520reconstruction%2520of%2520visual%2520information%2520from%2520brain%2520activity%2520fosters%250Ainterdisciplinary%2520integration%2520between%2520neuroscience%2520and%2520computer%2520vision.%250AHowever%252C%2520existing%2520methods%2520still%2520face%2520challenges%2520in%2520accurately%2520recovering%2520highly%250Acomplex%2520visual%2520stimuli.%2520This%2520difficulty%2520stems%2520from%2520the%2520characteristics%2520of%250Anatural%2520scenes%253A%2520low-level%2520features%2520exhibit%2520heterogeneity%252C%2520while%2520high-level%250Afeatures%2520show%2520semantic%2520entanglement%2520due%2520to%2520contextual%2520overlaps.%2520Inspired%2520by%2520the%250Ahierarchical%2520representation%2520theory%2520of%2520the%2520visual%2520cortex%252C%2520we%2520propose%2520the%2520HAVIR%250Amodel%252C%2520which%2520separates%2520the%2520visual%2520cortex%2520into%2520two%2520hierarchical%2520regions%2520and%250Aextracts%2520distinct%2520features%2520from%2520each.%2520Specifically%252C%2520the%2520Structural%2520Generator%250Aextracts%2520structural%2520information%2520from%2520spatial%2520processing%2520voxels%2520and%2520converts%2520it%250Ainto%2520latent%2520diffusion%2520priors%252C%2520while%2520the%2520Semantic%2520Extractor%2520converts%2520semantic%250Aprocessing%2520voxels%2520into%2520CLIP%2520embeddings.%2520These%2520components%2520are%2520integrated%2520via%2520the%250AVersatile%2520Diffusion%2520model%2520to%2520synthesize%2520the%2520final%2520image.%2520Experimental%2520results%250Ademonstrate%2520that%2520HAVIR%2520enhances%2520both%2520the%2520structural%2520and%2520semantic%2520quality%2520of%250Areconstructions%252C%2520even%2520in%2520complex%2520scenes%252C%2520and%2520outperforms%2520existing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAVIR%3A%20HierArchical%20Vision%20to%20Image%20Reconstruction%20using%20CLIP-Guided%0A%20%20Versatile%20Diffusion&entry.906535625=Shiyi%20Zhang%20and%20Dong%20Liang%20and%20Hairong%20Zheng%20and%20Yihang%20Zhou&entry.1292438233=%20%20The%20reconstruction%20of%20visual%20information%20from%20brain%20activity%20fosters%0Ainterdisciplinary%20integration%20between%20neuroscience%20and%20computer%20vision.%0AHowever%2C%20existing%20methods%20still%20face%20challenges%20in%20accurately%20recovering%20highly%0Acomplex%20visual%20stimuli.%20This%20difficulty%20stems%20from%20the%20characteristics%20of%0Anatural%20scenes%3A%20low-level%20features%20exhibit%20heterogeneity%2C%20while%20high-level%0Afeatures%20show%20semantic%20entanglement%20due%20to%20contextual%20overlaps.%20Inspired%20by%20the%0Ahierarchical%20representation%20theory%20of%20the%20visual%20cortex%2C%20we%20propose%20the%20HAVIR%0Amodel%2C%20which%20separates%20the%20visual%20cortex%20into%20two%20hierarchical%20regions%20and%0Aextracts%20distinct%20features%20from%20each.%20Specifically%2C%20the%20Structural%20Generator%0Aextracts%20structural%20information%20from%20spatial%20processing%20voxels%20and%20converts%20it%0Ainto%20latent%20diffusion%20priors%2C%20while%20the%20Semantic%20Extractor%20converts%20semantic%0Aprocessing%20voxels%20into%20CLIP%20embeddings.%20These%20components%20are%20integrated%20via%20the%0AVersatile%20Diffusion%20model%20to%20synthesize%20the%20final%20image.%20Experimental%20results%0Ademonstrate%20that%20HAVIR%20enhances%20both%20the%20structural%20and%20semantic%20quality%20of%0Areconstructions%2C%20even%20in%20complex%20scenes%2C%20and%20outperforms%20existing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03122v1&entry.124074799=Read"},
{"title": "3D-CovDiffusion: 3D-Aware Diffusion Policy for Coverage Path Planning", "author": "Chenyuan Chen and Haoran Ding and Ran Ding and Tianyu Liu and Zewen He and Anqing Duan and Dezhen Song and Xiaodan Liang and Yoshihiko Nakamura", "abstract": "  Diffusion models, as a class of deep generative models, have recently emerged\nas powerful tools for robot skills by enabling stable training with reliable\nconvergence. In this paper, we present an end-to-end framework for generating\nlong, smooth trajectories that explicitly target high surface coverage across\nvarious industrial tasks, including polishing, robotic painting, and spray\ncoating. The conventional methods are always fundamentally constrained by their\npredefined functional forms, which limit the shapes of the trajectories they\ncan represent and make it difficult to handle complex and diverse tasks.\nMoreover, their generalization is poor, often requiring manual redesign or\nextensive parameter tuning when applied to new scenarios. These limitations\nhighlight the need for more expressive generative models, making\ndiffusion-based approaches a compelling choice for trajectory generation. By\niteratively denoising trajectories with carefully learned noise schedules and\nconditioning mechanisms, diffusion models not only ensure smooth and consistent\nmotion but also flexibly adapt to the task context. In experiments, our method\nimproves trajectory continuity, maintains high coverage, and generalizes to\nunseen shapes, paving the way for unified end-to-end trajectory learning across\nindustrial surface-processing tasks without category-specific models. On\naverage, our approach improves Point-wise Chamfer Distance by 98.2\\% and\nsmoothness by 97.0\\%, while increasing surface coverage by 61\\% compared to\nprior methods. The link to our code can be found\n\\href{https://anonymous.4open.science/r/spraydiffusion_ral-2FCE/README.md}{here}.\n", "link": "http://arxiv.org/abs/2510.03011v1", "date": "2025-10-03", "relevancy": 3.0707, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6374}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.614}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-CovDiffusion%3A%203D-Aware%20Diffusion%20Policy%20for%20Coverage%20Path%20Planning&body=Title%3A%203D-CovDiffusion%3A%203D-Aware%20Diffusion%20Policy%20for%20Coverage%20Path%20Planning%0AAuthor%3A%20Chenyuan%20Chen%20and%20Haoran%20Ding%20and%20Ran%20Ding%20and%20Tianyu%20Liu%20and%20Zewen%20He%20and%20Anqing%20Duan%20and%20Dezhen%20Song%20and%20Xiaodan%20Liang%20and%20Yoshihiko%20Nakamura%0AAbstract%3A%20%20%20Diffusion%20models%2C%20as%20a%20class%20of%20deep%20generative%20models%2C%20have%20recently%20emerged%0Aas%20powerful%20tools%20for%20robot%20skills%20by%20enabling%20stable%20training%20with%20reliable%0Aconvergence.%20In%20this%20paper%2C%20we%20present%20an%20end-to-end%20framework%20for%20generating%0Along%2C%20smooth%20trajectories%20that%20explicitly%20target%20high%20surface%20coverage%20across%0Avarious%20industrial%20tasks%2C%20including%20polishing%2C%20robotic%20painting%2C%20and%20spray%0Acoating.%20The%20conventional%20methods%20are%20always%20fundamentally%20constrained%20by%20their%0Apredefined%20functional%20forms%2C%20which%20limit%20the%20shapes%20of%20the%20trajectories%20they%0Acan%20represent%20and%20make%20it%20difficult%20to%20handle%20complex%20and%20diverse%20tasks.%0AMoreover%2C%20their%20generalization%20is%20poor%2C%20often%20requiring%20manual%20redesign%20or%0Aextensive%20parameter%20tuning%20when%20applied%20to%20new%20scenarios.%20These%20limitations%0Ahighlight%20the%20need%20for%20more%20expressive%20generative%20models%2C%20making%0Adiffusion-based%20approaches%20a%20compelling%20choice%20for%20trajectory%20generation.%20By%0Aiteratively%20denoising%20trajectories%20with%20carefully%20learned%20noise%20schedules%20and%0Aconditioning%20mechanisms%2C%20diffusion%20models%20not%20only%20ensure%20smooth%20and%20consistent%0Amotion%20but%20also%20flexibly%20adapt%20to%20the%20task%20context.%20In%20experiments%2C%20our%20method%0Aimproves%20trajectory%20continuity%2C%20maintains%20high%20coverage%2C%20and%20generalizes%20to%0Aunseen%20shapes%2C%20paving%20the%20way%20for%20unified%20end-to-end%20trajectory%20learning%20across%0Aindustrial%20surface-processing%20tasks%20without%20category-specific%20models.%20On%0Aaverage%2C%20our%20approach%20improves%20Point-wise%20Chamfer%20Distance%20by%2098.2%5C%25%20and%0Asmoothness%20by%2097.0%5C%25%2C%20while%20increasing%20surface%20coverage%20by%2061%5C%25%20compared%20to%0Aprior%20methods.%20The%20link%20to%20our%20code%20can%20be%20found%0A%5Chref%7Bhttps%3A//anonymous.4open.science/r/spraydiffusion_ral-2FCE/README.md%7D%7Bhere%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-CovDiffusion%253A%25203D-Aware%2520Diffusion%2520Policy%2520for%2520Coverage%2520Path%2520Planning%26entry.906535625%3DChenyuan%2520Chen%2520and%2520Haoran%2520Ding%2520and%2520Ran%2520Ding%2520and%2520Tianyu%2520Liu%2520and%2520Zewen%2520He%2520and%2520Anqing%2520Duan%2520and%2520Dezhen%2520Song%2520and%2520Xiaodan%2520Liang%2520and%2520Yoshihiko%2520Nakamura%26entry.1292438233%3D%2520%2520Diffusion%2520models%252C%2520as%2520a%2520class%2520of%2520deep%2520generative%2520models%252C%2520have%2520recently%2520emerged%250Aas%2520powerful%2520tools%2520for%2520robot%2520skills%2520by%2520enabling%2520stable%2520training%2520with%2520reliable%250Aconvergence.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520end-to-end%2520framework%2520for%2520generating%250Along%252C%2520smooth%2520trajectories%2520that%2520explicitly%2520target%2520high%2520surface%2520coverage%2520across%250Avarious%2520industrial%2520tasks%252C%2520including%2520polishing%252C%2520robotic%2520painting%252C%2520and%2520spray%250Acoating.%2520The%2520conventional%2520methods%2520are%2520always%2520fundamentally%2520constrained%2520by%2520their%250Apredefined%2520functional%2520forms%252C%2520which%2520limit%2520the%2520shapes%2520of%2520the%2520trajectories%2520they%250Acan%2520represent%2520and%2520make%2520it%2520difficult%2520to%2520handle%2520complex%2520and%2520diverse%2520tasks.%250AMoreover%252C%2520their%2520generalization%2520is%2520poor%252C%2520often%2520requiring%2520manual%2520redesign%2520or%250Aextensive%2520parameter%2520tuning%2520when%2520applied%2520to%2520new%2520scenarios.%2520These%2520limitations%250Ahighlight%2520the%2520need%2520for%2520more%2520expressive%2520generative%2520models%252C%2520making%250Adiffusion-based%2520approaches%2520a%2520compelling%2520choice%2520for%2520trajectory%2520generation.%2520By%250Aiteratively%2520denoising%2520trajectories%2520with%2520carefully%2520learned%2520noise%2520schedules%2520and%250Aconditioning%2520mechanisms%252C%2520diffusion%2520models%2520not%2520only%2520ensure%2520smooth%2520and%2520consistent%250Amotion%2520but%2520also%2520flexibly%2520adapt%2520to%2520the%2520task%2520context.%2520In%2520experiments%252C%2520our%2520method%250Aimproves%2520trajectory%2520continuity%252C%2520maintains%2520high%2520coverage%252C%2520and%2520generalizes%2520to%250Aunseen%2520shapes%252C%2520paving%2520the%2520way%2520for%2520unified%2520end-to-end%2520trajectory%2520learning%2520across%250Aindustrial%2520surface-processing%2520tasks%2520without%2520category-specific%2520models.%2520On%250Aaverage%252C%2520our%2520approach%2520improves%2520Point-wise%2520Chamfer%2520Distance%2520by%252098.2%255C%2525%2520and%250Asmoothness%2520by%252097.0%255C%2525%252C%2520while%2520increasing%2520surface%2520coverage%2520by%252061%255C%2525%2520compared%2520to%250Aprior%2520methods.%2520The%2520link%2520to%2520our%2520code%2520can%2520be%2520found%250A%255Chref%257Bhttps%253A//anonymous.4open.science/r/spraydiffusion_ral-2FCE/README.md%257D%257Bhere%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-CovDiffusion%3A%203D-Aware%20Diffusion%20Policy%20for%20Coverage%20Path%20Planning&entry.906535625=Chenyuan%20Chen%20and%20Haoran%20Ding%20and%20Ran%20Ding%20and%20Tianyu%20Liu%20and%20Zewen%20He%20and%20Anqing%20Duan%20and%20Dezhen%20Song%20and%20Xiaodan%20Liang%20and%20Yoshihiko%20Nakamura&entry.1292438233=%20%20Diffusion%20models%2C%20as%20a%20class%20of%20deep%20generative%20models%2C%20have%20recently%20emerged%0Aas%20powerful%20tools%20for%20robot%20skills%20by%20enabling%20stable%20training%20with%20reliable%0Aconvergence.%20In%20this%20paper%2C%20we%20present%20an%20end-to-end%20framework%20for%20generating%0Along%2C%20smooth%20trajectories%20that%20explicitly%20target%20high%20surface%20coverage%20across%0Avarious%20industrial%20tasks%2C%20including%20polishing%2C%20robotic%20painting%2C%20and%20spray%0Acoating.%20The%20conventional%20methods%20are%20always%20fundamentally%20constrained%20by%20their%0Apredefined%20functional%20forms%2C%20which%20limit%20the%20shapes%20of%20the%20trajectories%20they%0Acan%20represent%20and%20make%20it%20difficult%20to%20handle%20complex%20and%20diverse%20tasks.%0AMoreover%2C%20their%20generalization%20is%20poor%2C%20often%20requiring%20manual%20redesign%20or%0Aextensive%20parameter%20tuning%20when%20applied%20to%20new%20scenarios.%20These%20limitations%0Ahighlight%20the%20need%20for%20more%20expressive%20generative%20models%2C%20making%0Adiffusion-based%20approaches%20a%20compelling%20choice%20for%20trajectory%20generation.%20By%0Aiteratively%20denoising%20trajectories%20with%20carefully%20learned%20noise%20schedules%20and%0Aconditioning%20mechanisms%2C%20diffusion%20models%20not%20only%20ensure%20smooth%20and%20consistent%0Amotion%20but%20also%20flexibly%20adapt%20to%20the%20task%20context.%20In%20experiments%2C%20our%20method%0Aimproves%20trajectory%20continuity%2C%20maintains%20high%20coverage%2C%20and%20generalizes%20to%0Aunseen%20shapes%2C%20paving%20the%20way%20for%20unified%20end-to-end%20trajectory%20learning%20across%0Aindustrial%20surface-processing%20tasks%20without%20category-specific%20models.%20On%0Aaverage%2C%20our%20approach%20improves%20Point-wise%20Chamfer%20Distance%20by%2098.2%5C%25%20and%0Asmoothness%20by%2097.0%5C%25%2C%20while%20increasing%20surface%20coverage%20by%2061%5C%25%20compared%20to%0Aprior%20methods.%20The%20link%20to%20our%20code%20can%20be%20found%0A%5Chref%7Bhttps%3A//anonymous.4open.science/r/spraydiffusion_ral-2FCE/README.md%7D%7Bhere%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03011v1&entry.124074799=Read"},
{"title": "MonSTeR: a Unified Model for Motion, Scene, Text Retrieval", "author": "Luca Collorone and Matteo Gioia and Massimiliano Pappa and Paolo Leoni and Giovanni Ficarra and Or Litany and Indro Spinelli and Fabio Galasso", "abstract": "  Intention drives human movement in complex environments, but such movement\ncan only happen if the surrounding context supports it. Despite the intuitive\nnature of this mechanism, existing research has not yet provided tools to\nevaluate the alignment between skeletal movement (motion), intention (text),\nand the surrounding context (scene). In this work, we introduce MonSTeR, the\nfirst MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of\nhigher-order relations, MonSTeR constructs a unified latent space by leveraging\nunimodal and cross-modal representations. This allows MonSTeR to capture the\nintricate dependencies between modalities, enabling flexible but robust\nretrieval across various tasks. Our results show that MonSTeR outperforms\ntrimodal models that rely solely on unimodal representations. Furthermore, we\nvalidate the alignment of our retrieval scores with human preferences through a\ndedicated user study. We demonstrate the versatility of MonSTeR's latent space\non zero-shot in-Scene Object Placement and Motion Captioning. Code and\npre-trained models are available at github.com/colloroneluca/MonSTeR.\n", "link": "http://arxiv.org/abs/2510.03200v1", "date": "2025-10-03", "relevancy": 2.9967, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonSTeR%3A%20a%20Unified%20Model%20for%20Motion%2C%20Scene%2C%20Text%20Retrieval&body=Title%3A%20MonSTeR%3A%20a%20Unified%20Model%20for%20Motion%2C%20Scene%2C%20Text%20Retrieval%0AAuthor%3A%20Luca%20Collorone%20and%20Matteo%20Gioia%20and%20Massimiliano%20Pappa%20and%20Paolo%20Leoni%20and%20Giovanni%20Ficarra%20and%20Or%20Litany%20and%20Indro%20Spinelli%20and%20Fabio%20Galasso%0AAbstract%3A%20%20%20Intention%20drives%20human%20movement%20in%20complex%20environments%2C%20but%20such%20movement%0Acan%20only%20happen%20if%20the%20surrounding%20context%20supports%20it.%20Despite%20the%20intuitive%0Anature%20of%20this%20mechanism%2C%20existing%20research%20has%20not%20yet%20provided%20tools%20to%0Aevaluate%20the%20alignment%20between%20skeletal%20movement%20%28motion%29%2C%20intention%20%28text%29%2C%0Aand%20the%20surrounding%20context%20%28scene%29.%20In%20this%20work%2C%20we%20introduce%20MonSTeR%2C%20the%0Afirst%20MOtioN-Scene-TExt%20Retrieval%20model.%20Inspired%20by%20the%20modeling%20of%0Ahigher-order%20relations%2C%20MonSTeR%20constructs%20a%20unified%20latent%20space%20by%20leveraging%0Aunimodal%20and%20cross-modal%20representations.%20This%20allows%20MonSTeR%20to%20capture%20the%0Aintricate%20dependencies%20between%20modalities%2C%20enabling%20flexible%20but%20robust%0Aretrieval%20across%20various%20tasks.%20Our%20results%20show%20that%20MonSTeR%20outperforms%0Atrimodal%20models%20that%20rely%20solely%20on%20unimodal%20representations.%20Furthermore%2C%20we%0Avalidate%20the%20alignment%20of%20our%20retrieval%20scores%20with%20human%20preferences%20through%20a%0Adedicated%20user%20study.%20We%20demonstrate%20the%20versatility%20of%20MonSTeR%27s%20latent%20space%0Aon%20zero-shot%20in-Scene%20Object%20Placement%20and%20Motion%20Captioning.%20Code%20and%0Apre-trained%20models%20are%20available%20at%20github.com/colloroneluca/MonSTeR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonSTeR%253A%2520a%2520Unified%2520Model%2520for%2520Motion%252C%2520Scene%252C%2520Text%2520Retrieval%26entry.906535625%3DLuca%2520Collorone%2520and%2520Matteo%2520Gioia%2520and%2520Massimiliano%2520Pappa%2520and%2520Paolo%2520Leoni%2520and%2520Giovanni%2520Ficarra%2520and%2520Or%2520Litany%2520and%2520Indro%2520Spinelli%2520and%2520Fabio%2520Galasso%26entry.1292438233%3D%2520%2520Intention%2520drives%2520human%2520movement%2520in%2520complex%2520environments%252C%2520but%2520such%2520movement%250Acan%2520only%2520happen%2520if%2520the%2520surrounding%2520context%2520supports%2520it.%2520Despite%2520the%2520intuitive%250Anature%2520of%2520this%2520mechanism%252C%2520existing%2520research%2520has%2520not%2520yet%2520provided%2520tools%2520to%250Aevaluate%2520the%2520alignment%2520between%2520skeletal%2520movement%2520%2528motion%2529%252C%2520intention%2520%2528text%2529%252C%250Aand%2520the%2520surrounding%2520context%2520%2528scene%2529.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MonSTeR%252C%2520the%250Afirst%2520MOtioN-Scene-TExt%2520Retrieval%2520model.%2520Inspired%2520by%2520the%2520modeling%2520of%250Ahigher-order%2520relations%252C%2520MonSTeR%2520constructs%2520a%2520unified%2520latent%2520space%2520by%2520leveraging%250Aunimodal%2520and%2520cross-modal%2520representations.%2520This%2520allows%2520MonSTeR%2520to%2520capture%2520the%250Aintricate%2520dependencies%2520between%2520modalities%252C%2520enabling%2520flexible%2520but%2520robust%250Aretrieval%2520across%2520various%2520tasks.%2520Our%2520results%2520show%2520that%2520MonSTeR%2520outperforms%250Atrimodal%2520models%2520that%2520rely%2520solely%2520on%2520unimodal%2520representations.%2520Furthermore%252C%2520we%250Avalidate%2520the%2520alignment%2520of%2520our%2520retrieval%2520scores%2520with%2520human%2520preferences%2520through%2520a%250Adedicated%2520user%2520study.%2520We%2520demonstrate%2520the%2520versatility%2520of%2520MonSTeR%2527s%2520latent%2520space%250Aon%2520zero-shot%2520in-Scene%2520Object%2520Placement%2520and%2520Motion%2520Captioning.%2520Code%2520and%250Apre-trained%2520models%2520are%2520available%2520at%2520github.com/colloroneluca/MonSTeR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonSTeR%3A%20a%20Unified%20Model%20for%20Motion%2C%20Scene%2C%20Text%20Retrieval&entry.906535625=Luca%20Collorone%20and%20Matteo%20Gioia%20and%20Massimiliano%20Pappa%20and%20Paolo%20Leoni%20and%20Giovanni%20Ficarra%20and%20Or%20Litany%20and%20Indro%20Spinelli%20and%20Fabio%20Galasso&entry.1292438233=%20%20Intention%20drives%20human%20movement%20in%20complex%20environments%2C%20but%20such%20movement%0Acan%20only%20happen%20if%20the%20surrounding%20context%20supports%20it.%20Despite%20the%20intuitive%0Anature%20of%20this%20mechanism%2C%20existing%20research%20has%20not%20yet%20provided%20tools%20to%0Aevaluate%20the%20alignment%20between%20skeletal%20movement%20%28motion%29%2C%20intention%20%28text%29%2C%0Aand%20the%20surrounding%20context%20%28scene%29.%20In%20this%20work%2C%20we%20introduce%20MonSTeR%2C%20the%0Afirst%20MOtioN-Scene-TExt%20Retrieval%20model.%20Inspired%20by%20the%20modeling%20of%0Ahigher-order%20relations%2C%20MonSTeR%20constructs%20a%20unified%20latent%20space%20by%20leveraging%0Aunimodal%20and%20cross-modal%20representations.%20This%20allows%20MonSTeR%20to%20capture%20the%0Aintricate%20dependencies%20between%20modalities%2C%20enabling%20flexible%20but%20robust%0Aretrieval%20across%20various%20tasks.%20Our%20results%20show%20that%20MonSTeR%20outperforms%0Atrimodal%20models%20that%20rely%20solely%20on%20unimodal%20representations.%20Furthermore%2C%20we%0Avalidate%20the%20alignment%20of%20our%20retrieval%20scores%20with%20human%20preferences%20through%20a%0Adedicated%20user%20study.%20We%20demonstrate%20the%20versatility%20of%20MonSTeR%27s%20latent%20space%0Aon%20zero-shot%20in-Scene%20Object%20Placement%20and%20Motion%20Captioning.%20Code%20and%0Apre-trained%20models%20are%20available%20at%20github.com/colloroneluca/MonSTeR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03200v1&entry.124074799=Read"},
{"title": "Toward a Holistic Evaluation of Robustness in CLIP Models", "author": "Weijie Tu and Weijian Deng and Tom Gedeon", "abstract": "  Contrastive Language-Image Pre-training (CLIP) models have shown significant\npotential, particularly in zero-shot classification across diverse distribution\nshifts. Building on existing evaluations of overall classification robustness,\nthis work aims to provide a more comprehensive assessment of CLIP by\nintroducing several new perspectives. First, we investigate their robustness to\nvariations in specific visual factors. Second, we assess two critical safety\nobjectives--confidence uncertainty and out-of-distribution detection--beyond\nmere classification accuracy. Third, we evaluate the finesse with which CLIP\nmodels bridge the image and text modalities. Fourth, we extend our examination\nto 3D awareness in CLIP models, moving beyond traditional 2D image\nunderstanding. Finally, we explore the interaction between vision and language\nencoders within modern large multimodal models (LMMs) that utilize CLIP as the\nvisual backbone, focusing on how this interaction impacts classification\nrobustness. In each aspect, we consider the impact of six factors on CLIP\nmodels: model architecture, training distribution, training set size,\nfine-tuning, contrastive loss, and test-time prompts. Our study uncovers\nseveral previously unknown insights into CLIP. For instance, the architecture\nof the visual encoder in CLIP plays a significant role in their robustness\nagainst 3D corruption. CLIP models tend to exhibit a bias towards shape when\nmaking predictions. Moreover, this bias tends to diminish after fine-tuning on\nImageNet. Vision-language models like LLaVA, leveraging the CLIP vision\nencoder, could exhibit benefits in classification performance for challenging\ncategories over CLIP alone. Our findings are poised to offer valuable guidance\nfor enhancing the robustness and reliability of CLIP models.\n", "link": "http://arxiv.org/abs/2410.01534v2", "date": "2025-10-03", "relevancy": 2.9737, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20a%20Holistic%20Evaluation%20of%20Robustness%20in%20CLIP%20Models&body=Title%3A%20Toward%20a%20Holistic%20Evaluation%20of%20Robustness%20in%20CLIP%20Models%0AAuthor%3A%20Weijie%20Tu%20and%20Weijian%20Deng%20and%20Tom%20Gedeon%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20models%20have%20shown%20significant%0Apotential%2C%20particularly%20in%20zero-shot%20classification%20across%20diverse%20distribution%0Ashifts.%20Building%20on%20existing%20evaluations%20of%20overall%20classification%20robustness%2C%0Athis%20work%20aims%20to%20provide%20a%20more%20comprehensive%20assessment%20of%20CLIP%20by%0Aintroducing%20several%20new%20perspectives.%20First%2C%20we%20investigate%20their%20robustness%20to%0Avariations%20in%20specific%20visual%20factors.%20Second%2C%20we%20assess%20two%20critical%20safety%0Aobjectives--confidence%20uncertainty%20and%20out-of-distribution%20detection--beyond%0Amere%20classification%20accuracy.%20Third%2C%20we%20evaluate%20the%20finesse%20with%20which%20CLIP%0Amodels%20bridge%20the%20image%20and%20text%20modalities.%20Fourth%2C%20we%20extend%20our%20examination%0Ato%203D%20awareness%20in%20CLIP%20models%2C%20moving%20beyond%20traditional%202D%20image%0Aunderstanding.%20Finally%2C%20we%20explore%20the%20interaction%20between%20vision%20and%20language%0Aencoders%20within%20modern%20large%20multimodal%20models%20%28LMMs%29%20that%20utilize%20CLIP%20as%20the%0Avisual%20backbone%2C%20focusing%20on%20how%20this%20interaction%20impacts%20classification%0Arobustness.%20In%20each%20aspect%2C%20we%20consider%20the%20impact%20of%20six%20factors%20on%20CLIP%0Amodels%3A%20model%20architecture%2C%20training%20distribution%2C%20training%20set%20size%2C%0Afine-tuning%2C%20contrastive%20loss%2C%20and%20test-time%20prompts.%20Our%20study%20uncovers%0Aseveral%20previously%20unknown%20insights%20into%20CLIP.%20For%20instance%2C%20the%20architecture%0Aof%20the%20visual%20encoder%20in%20CLIP%20plays%20a%20significant%20role%20in%20their%20robustness%0Aagainst%203D%20corruption.%20CLIP%20models%20tend%20to%20exhibit%20a%20bias%20towards%20shape%20when%0Amaking%20predictions.%20Moreover%2C%20this%20bias%20tends%20to%20diminish%20after%20fine-tuning%20on%0AImageNet.%20Vision-language%20models%20like%20LLaVA%2C%20leveraging%20the%20CLIP%20vision%0Aencoder%2C%20could%20exhibit%20benefits%20in%20classification%20performance%20for%20challenging%0Acategories%20over%20CLIP%20alone.%20Our%20findings%20are%20poised%20to%20offer%20valuable%20guidance%0Afor%20enhancing%20the%20robustness%20and%20reliability%20of%20CLIP%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01534v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520a%2520Holistic%2520Evaluation%2520of%2520Robustness%2520in%2520CLIP%2520Models%26entry.906535625%3DWeijie%2520Tu%2520and%2520Weijian%2520Deng%2520and%2520Tom%2520Gedeon%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520models%2520have%2520shown%2520significant%250Apotential%252C%2520particularly%2520in%2520zero-shot%2520classification%2520across%2520diverse%2520distribution%250Ashifts.%2520Building%2520on%2520existing%2520evaluations%2520of%2520overall%2520classification%2520robustness%252C%250Athis%2520work%2520aims%2520to%2520provide%2520a%2520more%2520comprehensive%2520assessment%2520of%2520CLIP%2520by%250Aintroducing%2520several%2520new%2520perspectives.%2520First%252C%2520we%2520investigate%2520their%2520robustness%2520to%250Avariations%2520in%2520specific%2520visual%2520factors.%2520Second%252C%2520we%2520assess%2520two%2520critical%2520safety%250Aobjectives--confidence%2520uncertainty%2520and%2520out-of-distribution%2520detection--beyond%250Amere%2520classification%2520accuracy.%2520Third%252C%2520we%2520evaluate%2520the%2520finesse%2520with%2520which%2520CLIP%250Amodels%2520bridge%2520the%2520image%2520and%2520text%2520modalities.%2520Fourth%252C%2520we%2520extend%2520our%2520examination%250Ato%25203D%2520awareness%2520in%2520CLIP%2520models%252C%2520moving%2520beyond%2520traditional%25202D%2520image%250Aunderstanding.%2520Finally%252C%2520we%2520explore%2520the%2520interaction%2520between%2520vision%2520and%2520language%250Aencoders%2520within%2520modern%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520that%2520utilize%2520CLIP%2520as%2520the%250Avisual%2520backbone%252C%2520focusing%2520on%2520how%2520this%2520interaction%2520impacts%2520classification%250Arobustness.%2520In%2520each%2520aspect%252C%2520we%2520consider%2520the%2520impact%2520of%2520six%2520factors%2520on%2520CLIP%250Amodels%253A%2520model%2520architecture%252C%2520training%2520distribution%252C%2520training%2520set%2520size%252C%250Afine-tuning%252C%2520contrastive%2520loss%252C%2520and%2520test-time%2520prompts.%2520Our%2520study%2520uncovers%250Aseveral%2520previously%2520unknown%2520insights%2520into%2520CLIP.%2520For%2520instance%252C%2520the%2520architecture%250Aof%2520the%2520visual%2520encoder%2520in%2520CLIP%2520plays%2520a%2520significant%2520role%2520in%2520their%2520robustness%250Aagainst%25203D%2520corruption.%2520CLIP%2520models%2520tend%2520to%2520exhibit%2520a%2520bias%2520towards%2520shape%2520when%250Amaking%2520predictions.%2520Moreover%252C%2520this%2520bias%2520tends%2520to%2520diminish%2520after%2520fine-tuning%2520on%250AImageNet.%2520Vision-language%2520models%2520like%2520LLaVA%252C%2520leveraging%2520the%2520CLIP%2520vision%250Aencoder%252C%2520could%2520exhibit%2520benefits%2520in%2520classification%2520performance%2520for%2520challenging%250Acategories%2520over%2520CLIP%2520alone.%2520Our%2520findings%2520are%2520poised%2520to%2520offer%2520valuable%2520guidance%250Afor%2520enhancing%2520the%2520robustness%2520and%2520reliability%2520of%2520CLIP%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01534v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20a%20Holistic%20Evaluation%20of%20Robustness%20in%20CLIP%20Models&entry.906535625=Weijie%20Tu%20and%20Weijian%20Deng%20and%20Tom%20Gedeon&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20models%20have%20shown%20significant%0Apotential%2C%20particularly%20in%20zero-shot%20classification%20across%20diverse%20distribution%0Ashifts.%20Building%20on%20existing%20evaluations%20of%20overall%20classification%20robustness%2C%0Athis%20work%20aims%20to%20provide%20a%20more%20comprehensive%20assessment%20of%20CLIP%20by%0Aintroducing%20several%20new%20perspectives.%20First%2C%20we%20investigate%20their%20robustness%20to%0Avariations%20in%20specific%20visual%20factors.%20Second%2C%20we%20assess%20two%20critical%20safety%0Aobjectives--confidence%20uncertainty%20and%20out-of-distribution%20detection--beyond%0Amere%20classification%20accuracy.%20Third%2C%20we%20evaluate%20the%20finesse%20with%20which%20CLIP%0Amodels%20bridge%20the%20image%20and%20text%20modalities.%20Fourth%2C%20we%20extend%20our%20examination%0Ato%203D%20awareness%20in%20CLIP%20models%2C%20moving%20beyond%20traditional%202D%20image%0Aunderstanding.%20Finally%2C%20we%20explore%20the%20interaction%20between%20vision%20and%20language%0Aencoders%20within%20modern%20large%20multimodal%20models%20%28LMMs%29%20that%20utilize%20CLIP%20as%20the%0Avisual%20backbone%2C%20focusing%20on%20how%20this%20interaction%20impacts%20classification%0Arobustness.%20In%20each%20aspect%2C%20we%20consider%20the%20impact%20of%20six%20factors%20on%20CLIP%0Amodels%3A%20model%20architecture%2C%20training%20distribution%2C%20training%20set%20size%2C%0Afine-tuning%2C%20contrastive%20loss%2C%20and%20test-time%20prompts.%20Our%20study%20uncovers%0Aseveral%20previously%20unknown%20insights%20into%20CLIP.%20For%20instance%2C%20the%20architecture%0Aof%20the%20visual%20encoder%20in%20CLIP%20plays%20a%20significant%20role%20in%20their%20robustness%0Aagainst%203D%20corruption.%20CLIP%20models%20tend%20to%20exhibit%20a%20bias%20towards%20shape%20when%0Amaking%20predictions.%20Moreover%2C%20this%20bias%20tends%20to%20diminish%20after%20fine-tuning%20on%0AImageNet.%20Vision-language%20models%20like%20LLaVA%2C%20leveraging%20the%20CLIP%20vision%0Aencoder%2C%20could%20exhibit%20benefits%20in%20classification%20performance%20for%20challenging%0Acategories%20over%20CLIP%20alone.%20Our%20findings%20are%20poised%20to%20offer%20valuable%20guidance%0Afor%20enhancing%20the%20robustness%20and%20reliability%20of%20CLIP%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01534v2&entry.124074799=Read"},
{"title": "Less is More: Lean yet Powerful Vision-Language Model for Autonomous\n  Driving", "author": "Sheng Yang and Tong Zhan and Guancheng Chen and Yanfeng Lu and Jian Wang", "abstract": "  In this work, we reconceptualize autonomous driving as a generalized language\nand formulate the trajectory planning task as next waypoint prediction. We\nintroduce Max-V1, a novel framework for one-stage end-to-end autonomous\ndriving. Our framework presents a single-pass generation paradigm that aligns\nwith the inherent sequentiality of driving. This approach leverages the\ngenerative capacity of the VLM (Vision-Language Model) to enable end-to-end\ntrajectory prediction directly from front-view camera input. The efficacy of\nthis method is underpinned by a principled supervision strategy derived from\nstatistical modeling. This provides a well-defined learning objective, which\nmakes the framework highly amenable to master complex driving policies through\nimitation learning from large-scale expert demonstrations. Empirically, our\nmethod achieves the state-of-the-art performance on the nuScenes dataset,\ndelivers an overall improvement of over 30% compared to prior baselines.\nFurthermore, it exhibits superior generalization performance on cross-domain\ndatasets acquired from diverse vehicles, demonstrating notable potential for\ncross-vehicle robustness and adaptability. Due to these empirical strengths,\nthis work introduces a model enabling fundamental driving behaviors, laying the\nfoundation for the development of more capable self-driving agents. Code will\nbe available upon publication.\n", "link": "http://arxiv.org/abs/2510.00060v2", "date": "2025-10-03", "relevancy": 2.9514, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Lean%20yet%20Powerful%20Vision-Language%20Model%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20Less%20is%20More%3A%20Lean%20yet%20Powerful%20Vision-Language%20Model%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Sheng%20Yang%20and%20Tong%20Zhan%20and%20Guancheng%20Chen%20and%20Yanfeng%20Lu%20and%20Jian%20Wang%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20reconceptualize%20autonomous%20driving%20as%20a%20generalized%20language%0Aand%20formulate%20the%20trajectory%20planning%20task%20as%20next%20waypoint%20prediction.%20We%0Aintroduce%20Max-V1%2C%20a%20novel%20framework%20for%20one-stage%20end-to-end%20autonomous%0Adriving.%20Our%20framework%20presents%20a%20single-pass%20generation%20paradigm%20that%20aligns%0Awith%20the%20inherent%20sequentiality%20of%20driving.%20This%20approach%20leverages%20the%0Agenerative%20capacity%20of%20the%20VLM%20%28Vision-Language%20Model%29%20to%20enable%20end-to-end%0Atrajectory%20prediction%20directly%20from%20front-view%20camera%20input.%20The%20efficacy%20of%0Athis%20method%20is%20underpinned%20by%20a%20principled%20supervision%20strategy%20derived%20from%0Astatistical%20modeling.%20This%20provides%20a%20well-defined%20learning%20objective%2C%20which%0Amakes%20the%20framework%20highly%20amenable%20to%20master%20complex%20driving%20policies%20through%0Aimitation%20learning%20from%20large-scale%20expert%20demonstrations.%20Empirically%2C%20our%0Amethod%20achieves%20the%20state-of-the-art%20performance%20on%20the%20nuScenes%20dataset%2C%0Adelivers%20an%20overall%20improvement%20of%20over%2030%25%20compared%20to%20prior%20baselines.%0AFurthermore%2C%20it%20exhibits%20superior%20generalization%20performance%20on%20cross-domain%0Adatasets%20acquired%20from%20diverse%20vehicles%2C%20demonstrating%20notable%20potential%20for%0Across-vehicle%20robustness%20and%20adaptability.%20Due%20to%20these%20empirical%20strengths%2C%0Athis%20work%20introduces%20a%20model%20enabling%20fundamental%20driving%20behaviors%2C%20laying%20the%0Afoundation%20for%20the%20development%20of%20more%20capable%20self-driving%20agents.%20Code%20will%0Abe%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Lean%2520yet%2520Powerful%2520Vision-Language%2520Model%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DSheng%2520Yang%2520and%2520Tong%2520Zhan%2520and%2520Guancheng%2520Chen%2520and%2520Yanfeng%2520Lu%2520and%2520Jian%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520reconceptualize%2520autonomous%2520driving%2520as%2520a%2520generalized%2520language%250Aand%2520formulate%2520the%2520trajectory%2520planning%2520task%2520as%2520next%2520waypoint%2520prediction.%2520We%250Aintroduce%2520Max-V1%252C%2520a%2520novel%2520framework%2520for%2520one-stage%2520end-to-end%2520autonomous%250Adriving.%2520Our%2520framework%2520presents%2520a%2520single-pass%2520generation%2520paradigm%2520that%2520aligns%250Awith%2520the%2520inherent%2520sequentiality%2520of%2520driving.%2520This%2520approach%2520leverages%2520the%250Agenerative%2520capacity%2520of%2520the%2520VLM%2520%2528Vision-Language%2520Model%2529%2520to%2520enable%2520end-to-end%250Atrajectory%2520prediction%2520directly%2520from%2520front-view%2520camera%2520input.%2520The%2520efficacy%2520of%250Athis%2520method%2520is%2520underpinned%2520by%2520a%2520principled%2520supervision%2520strategy%2520derived%2520from%250Astatistical%2520modeling.%2520This%2520provides%2520a%2520well-defined%2520learning%2520objective%252C%2520which%250Amakes%2520the%2520framework%2520highly%2520amenable%2520to%2520master%2520complex%2520driving%2520policies%2520through%250Aimitation%2520learning%2520from%2520large-scale%2520expert%2520demonstrations.%2520Empirically%252C%2520our%250Amethod%2520achieves%2520the%2520state-of-the-art%2520performance%2520on%2520the%2520nuScenes%2520dataset%252C%250Adelivers%2520an%2520overall%2520improvement%2520of%2520over%252030%2525%2520compared%2520to%2520prior%2520baselines.%250AFurthermore%252C%2520it%2520exhibits%2520superior%2520generalization%2520performance%2520on%2520cross-domain%250Adatasets%2520acquired%2520from%2520diverse%2520vehicles%252C%2520demonstrating%2520notable%2520potential%2520for%250Across-vehicle%2520robustness%2520and%2520adaptability.%2520Due%2520to%2520these%2520empirical%2520strengths%252C%250Athis%2520work%2520introduces%2520a%2520model%2520enabling%2520fundamental%2520driving%2520behaviors%252C%2520laying%2520the%250Afoundation%2520for%2520the%2520development%2520of%2520more%2520capable%2520self-driving%2520agents.%2520Code%2520will%250Abe%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Lean%20yet%20Powerful%20Vision-Language%20Model%20for%20Autonomous%0A%20%20Driving&entry.906535625=Sheng%20Yang%20and%20Tong%20Zhan%20and%20Guancheng%20Chen%20and%20Yanfeng%20Lu%20and%20Jian%20Wang&entry.1292438233=%20%20In%20this%20work%2C%20we%20reconceptualize%20autonomous%20driving%20as%20a%20generalized%20language%0Aand%20formulate%20the%20trajectory%20planning%20task%20as%20next%20waypoint%20prediction.%20We%0Aintroduce%20Max-V1%2C%20a%20novel%20framework%20for%20one-stage%20end-to-end%20autonomous%0Adriving.%20Our%20framework%20presents%20a%20single-pass%20generation%20paradigm%20that%20aligns%0Awith%20the%20inherent%20sequentiality%20of%20driving.%20This%20approach%20leverages%20the%0Agenerative%20capacity%20of%20the%20VLM%20%28Vision-Language%20Model%29%20to%20enable%20end-to-end%0Atrajectory%20prediction%20directly%20from%20front-view%20camera%20input.%20The%20efficacy%20of%0Athis%20method%20is%20underpinned%20by%20a%20principled%20supervision%20strategy%20derived%20from%0Astatistical%20modeling.%20This%20provides%20a%20well-defined%20learning%20objective%2C%20which%0Amakes%20the%20framework%20highly%20amenable%20to%20master%20complex%20driving%20policies%20through%0Aimitation%20learning%20from%20large-scale%20expert%20demonstrations.%20Empirically%2C%20our%0Amethod%20achieves%20the%20state-of-the-art%20performance%20on%20the%20nuScenes%20dataset%2C%0Adelivers%20an%20overall%20improvement%20of%20over%2030%25%20compared%20to%20prior%20baselines.%0AFurthermore%2C%20it%20exhibits%20superior%20generalization%20performance%20on%20cross-domain%0Adatasets%20acquired%20from%20diverse%20vehicles%2C%20demonstrating%20notable%20potential%20for%0Across-vehicle%20robustness%20and%20adaptability.%20Due%20to%20these%20empirical%20strengths%2C%0Athis%20work%20introduces%20a%20model%20enabling%20fundamental%20driving%20behaviors%2C%20laying%20the%0Afoundation%20for%20the%20development%20of%20more%20capable%20self-driving%20agents.%20Code%20will%0Abe%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00060v2&entry.124074799=Read"},
{"title": "Training-Free Out-Of-Distribution Segmentation With Foundation Models", "author": "Laith Nayal and Hadi Salloum and Ahmad Taha and Yaroslav Kholodov and Alexander Gasnikov", "abstract": "  Detecting unknown objects in semantic segmentation is crucial for\nsafety-critical applications such as autonomous driving. Large vision\nfoundation models, including DINOv2, InternImage, and CLIP, have advanced\nvisual representation learning by providing rich features that generalize well\nacross diverse tasks. While their strength in closed-set semantic tasks is\nestablished, their capability to detect out-of-distribution (OoD) regions in\nsemantic segmentation remains underexplored. In this work, we investigate\nwhether foundation models fine-tuned on segmentation datasets can inherently\ndistinguish in-distribution (ID) from OoD regions without any outlier\nsupervision. We propose a simple, training-free approach that utilizes features\nfrom the InternImage backbone and applies K-Means clustering alongside\nconfidence thresholding on raw decoder logits to identify OoD clusters. Our\nmethod achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77\non the benchmark of ADE-OoD with InternImage-L, surpassing several supervised\nand unsupervised baselines. These results suggest a promising direction for\ngeneric OoD segmentation methods that require minimal assumptions or additional\ndata.\n", "link": "http://arxiv.org/abs/2510.02909v1", "date": "2025-10-03", "relevancy": 2.9441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6067}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Out-Of-Distribution%20Segmentation%20With%20Foundation%20Models&body=Title%3A%20Training-Free%20Out-Of-Distribution%20Segmentation%20With%20Foundation%20Models%0AAuthor%3A%20Laith%20Nayal%20and%20Hadi%20Salloum%20and%20Ahmad%20Taha%20and%20Yaroslav%20Kholodov%20and%20Alexander%20Gasnikov%0AAbstract%3A%20%20%20Detecting%20unknown%20objects%20in%20semantic%20segmentation%20is%20crucial%20for%0Asafety-critical%20applications%20such%20as%20autonomous%20driving.%20Large%20vision%0Afoundation%20models%2C%20including%20DINOv2%2C%20InternImage%2C%20and%20CLIP%2C%20have%20advanced%0Avisual%20representation%20learning%20by%20providing%20rich%20features%20that%20generalize%20well%0Aacross%20diverse%20tasks.%20While%20their%20strength%20in%20closed-set%20semantic%20tasks%20is%0Aestablished%2C%20their%20capability%20to%20detect%20out-of-distribution%20%28OoD%29%20regions%20in%0Asemantic%20segmentation%20remains%20underexplored.%20In%20this%20work%2C%20we%20investigate%0Awhether%20foundation%20models%20fine-tuned%20on%20segmentation%20datasets%20can%20inherently%0Adistinguish%20in-distribution%20%28ID%29%20from%20OoD%20regions%20without%20any%20outlier%0Asupervision.%20We%20propose%20a%20simple%2C%20training-free%20approach%20that%20utilizes%20features%0Afrom%20the%20InternImage%20backbone%20and%20applies%20K-Means%20clustering%20alongside%0Aconfidence%20thresholding%20on%20raw%20decoder%20logits%20to%20identify%20OoD%20clusters.%20Our%0Amethod%20achieves%2050.02%20Average%20Precision%20on%20the%20RoadAnomaly%20benchmark%20and%2048.77%0Aon%20the%20benchmark%20of%20ADE-OoD%20with%20InternImage-L%2C%20surpassing%20several%20supervised%0Aand%20unsupervised%20baselines.%20These%20results%20suggest%20a%20promising%20direction%20for%0Ageneric%20OoD%20segmentation%20methods%20that%20require%20minimal%20assumptions%20or%20additional%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Out-Of-Distribution%2520Segmentation%2520With%2520Foundation%2520Models%26entry.906535625%3DLaith%2520Nayal%2520and%2520Hadi%2520Salloum%2520and%2520Ahmad%2520Taha%2520and%2520Yaroslav%2520Kholodov%2520and%2520Alexander%2520Gasnikov%26entry.1292438233%3D%2520%2520Detecting%2520unknown%2520objects%2520in%2520semantic%2520segmentation%2520is%2520crucial%2520for%250Asafety-critical%2520applications%2520such%2520as%2520autonomous%2520driving.%2520Large%2520vision%250Afoundation%2520models%252C%2520including%2520DINOv2%252C%2520InternImage%252C%2520and%2520CLIP%252C%2520have%2520advanced%250Avisual%2520representation%2520learning%2520by%2520providing%2520rich%2520features%2520that%2520generalize%2520well%250Aacross%2520diverse%2520tasks.%2520While%2520their%2520strength%2520in%2520closed-set%2520semantic%2520tasks%2520is%250Aestablished%252C%2520their%2520capability%2520to%2520detect%2520out-of-distribution%2520%2528OoD%2529%2520regions%2520in%250Asemantic%2520segmentation%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520investigate%250Awhether%2520foundation%2520models%2520fine-tuned%2520on%2520segmentation%2520datasets%2520can%2520inherently%250Adistinguish%2520in-distribution%2520%2528ID%2529%2520from%2520OoD%2520regions%2520without%2520any%2520outlier%250Asupervision.%2520We%2520propose%2520a%2520simple%252C%2520training-free%2520approach%2520that%2520utilizes%2520features%250Afrom%2520the%2520InternImage%2520backbone%2520and%2520applies%2520K-Means%2520clustering%2520alongside%250Aconfidence%2520thresholding%2520on%2520raw%2520decoder%2520logits%2520to%2520identify%2520OoD%2520clusters.%2520Our%250Amethod%2520achieves%252050.02%2520Average%2520Precision%2520on%2520the%2520RoadAnomaly%2520benchmark%2520and%252048.77%250Aon%2520the%2520benchmark%2520of%2520ADE-OoD%2520with%2520InternImage-L%252C%2520surpassing%2520several%2520supervised%250Aand%2520unsupervised%2520baselines.%2520These%2520results%2520suggest%2520a%2520promising%2520direction%2520for%250Ageneric%2520OoD%2520segmentation%2520methods%2520that%2520require%2520minimal%2520assumptions%2520or%2520additional%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Out-Of-Distribution%20Segmentation%20With%20Foundation%20Models&entry.906535625=Laith%20Nayal%20and%20Hadi%20Salloum%20and%20Ahmad%20Taha%20and%20Yaroslav%20Kholodov%20and%20Alexander%20Gasnikov&entry.1292438233=%20%20Detecting%20unknown%20objects%20in%20semantic%20segmentation%20is%20crucial%20for%0Asafety-critical%20applications%20such%20as%20autonomous%20driving.%20Large%20vision%0Afoundation%20models%2C%20including%20DINOv2%2C%20InternImage%2C%20and%20CLIP%2C%20have%20advanced%0Avisual%20representation%20learning%20by%20providing%20rich%20features%20that%20generalize%20well%0Aacross%20diverse%20tasks.%20While%20their%20strength%20in%20closed-set%20semantic%20tasks%20is%0Aestablished%2C%20their%20capability%20to%20detect%20out-of-distribution%20%28OoD%29%20regions%20in%0Asemantic%20segmentation%20remains%20underexplored.%20In%20this%20work%2C%20we%20investigate%0Awhether%20foundation%20models%20fine-tuned%20on%20segmentation%20datasets%20can%20inherently%0Adistinguish%20in-distribution%20%28ID%29%20from%20OoD%20regions%20without%20any%20outlier%0Asupervision.%20We%20propose%20a%20simple%2C%20training-free%20approach%20that%20utilizes%20features%0Afrom%20the%20InternImage%20backbone%20and%20applies%20K-Means%20clustering%20alongside%0Aconfidence%20thresholding%20on%20raw%20decoder%20logits%20to%20identify%20OoD%20clusters.%20Our%0Amethod%20achieves%2050.02%20Average%20Precision%20on%20the%20RoadAnomaly%20benchmark%20and%2048.77%0Aon%20the%20benchmark%20of%20ADE-OoD%20with%20InternImage-L%2C%20surpassing%20several%20supervised%0Aand%20unsupervised%20baselines.%20These%20results%20suggest%20a%20promising%20direction%20for%0Ageneric%20OoD%20segmentation%20methods%20that%20require%20minimal%20assumptions%20or%20additional%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02909v1&entry.124074799=Read"},
{"title": "Don't Just Chase \"Highlighted Tokens\" in MLLMs: Revisiting Visual\n  Holistic Context Retention", "author": "Xin Zou and Di Lu and Yizhou Wang and Yibo Yan and Yuanhuiyi Lyu and Xu Zheng and Linfeng Zhang and Xuming Hu", "abstract": "  Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)\nsuffer from considerable computational overhead due to their reliance on\nmassive visual tokens. Recent studies have explored token pruning to alleviate\nthis problem, which typically uses text-vision cross-attention or\n[\\texttt{CLS}] attention to assess and discard redundant visual tokens. In this\nwork, we identify a critical limitation of such attention-first pruning\napproaches, i.e., they tend to preserve semantically similar tokens, resulting\nin pronounced performance drops under high pruning ratios. To this end, we\npropose {HoloV}, a simple yet effective, plug-and-play visual token pruning\nframework for efficient inference. Distinct from previous attention-first\nschemes, HoloV rethinks token retention from a holistic perspective. By\nadaptively distributing the pruning budget across different spatial crops,\nHoloV ensures that the retained tokens capture the global visual context rather\nthan isolated salient features. This strategy minimizes representational\ncollapse and maintains task-relevant information even under aggressive pruning.\nExperimental results demonstrate that our HoloV achieves superior performance\nacross various tasks, MLLM architectures, and pruning ratios compared to SOTA\nmethods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\\% of the\noriginal performance after pruning 88.9\\% of visual tokens, achieving superior\nefficiency-accuracy trade-offs.\n", "link": "http://arxiv.org/abs/2510.02912v1", "date": "2025-10-03", "relevancy": 2.821, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Just%20Chase%20%22Highlighted%20Tokens%22%20in%20MLLMs%3A%20Revisiting%20Visual%0A%20%20Holistic%20Context%20Retention&body=Title%3A%20Don%27t%20Just%20Chase%20%22Highlighted%20Tokens%22%20in%20MLLMs%3A%20Revisiting%20Visual%0A%20%20Holistic%20Context%20Retention%0AAuthor%3A%20Xin%20Zou%20and%20Di%20Lu%20and%20Yizhou%20Wang%20and%20Yibo%20Yan%20and%20Yuanhuiyi%20Lyu%20and%20Xu%20Zheng%20and%20Linfeng%20Zhang%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20Despite%20their%20powerful%20capabilities%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Asuffer%20from%20considerable%20computational%20overhead%20due%20to%20their%20reliance%20on%0Amassive%20visual%20tokens.%20Recent%20studies%20have%20explored%20token%20pruning%20to%20alleviate%0Athis%20problem%2C%20which%20typically%20uses%20text-vision%20cross-attention%20or%0A%5B%5Ctexttt%7BCLS%7D%5D%20attention%20to%20assess%20and%20discard%20redundant%20visual%20tokens.%20In%20this%0Awork%2C%20we%20identify%20a%20critical%20limitation%20of%20such%20attention-first%20pruning%0Aapproaches%2C%20i.e.%2C%20they%20tend%20to%20preserve%20semantically%20similar%20tokens%2C%20resulting%0Ain%20pronounced%20performance%20drops%20under%20high%20pruning%20ratios.%20To%20this%20end%2C%20we%0Apropose%20%7BHoloV%7D%2C%20a%20simple%20yet%20effective%2C%20plug-and-play%20visual%20token%20pruning%0Aframework%20for%20efficient%20inference.%20Distinct%20from%20previous%20attention-first%0Aschemes%2C%20HoloV%20rethinks%20token%20retention%20from%20a%20holistic%20perspective.%20By%0Aadaptively%20distributing%20the%20pruning%20budget%20across%20different%20spatial%20crops%2C%0AHoloV%20ensures%20that%20the%20retained%20tokens%20capture%20the%20global%20visual%20context%20rather%0Athan%20isolated%20salient%20features.%20This%20strategy%20minimizes%20representational%0Acollapse%20and%20maintains%20task-relevant%20information%20even%20under%20aggressive%20pruning.%0AExperimental%20results%20demonstrate%20that%20our%20HoloV%20achieves%20superior%20performance%0Aacross%20various%20tasks%2C%20MLLM%20architectures%2C%20and%20pruning%20ratios%20compared%20to%20SOTA%0Amethods.%20For%20instance%2C%20LLaVA1.5%20equipped%20with%20HoloV%20preserves%2095.8%5C%25%20of%20the%0Aoriginal%20performance%20after%20pruning%2088.9%5C%25%20of%20visual%20tokens%2C%20achieving%20superior%0Aefficiency-accuracy%20trade-offs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Just%2520Chase%2520%2522Highlighted%2520Tokens%2522%2520in%2520MLLMs%253A%2520Revisiting%2520Visual%250A%2520%2520Holistic%2520Context%2520Retention%26entry.906535625%3DXin%2520Zou%2520and%2520Di%2520Lu%2520and%2520Yizhou%2520Wang%2520and%2520Yibo%2520Yan%2520and%2520Yuanhuiyi%2520Lyu%2520and%2520Xu%2520Zheng%2520and%2520Linfeng%2520Zhang%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520Despite%2520their%2520powerful%2520capabilities%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%250Asuffer%2520from%2520considerable%2520computational%2520overhead%2520due%2520to%2520their%2520reliance%2520on%250Amassive%2520visual%2520tokens.%2520Recent%2520studies%2520have%2520explored%2520token%2520pruning%2520to%2520alleviate%250Athis%2520problem%252C%2520which%2520typically%2520uses%2520text-vision%2520cross-attention%2520or%250A%255B%255Ctexttt%257BCLS%257D%255D%2520attention%2520to%2520assess%2520and%2520discard%2520redundant%2520visual%2520tokens.%2520In%2520this%250Awork%252C%2520we%2520identify%2520a%2520critical%2520limitation%2520of%2520such%2520attention-first%2520pruning%250Aapproaches%252C%2520i.e.%252C%2520they%2520tend%2520to%2520preserve%2520semantically%2520similar%2520tokens%252C%2520resulting%250Ain%2520pronounced%2520performance%2520drops%2520under%2520high%2520pruning%2520ratios.%2520To%2520this%2520end%252C%2520we%250Apropose%2520%257BHoloV%257D%252C%2520a%2520simple%2520yet%2520effective%252C%2520plug-and-play%2520visual%2520token%2520pruning%250Aframework%2520for%2520efficient%2520inference.%2520Distinct%2520from%2520previous%2520attention-first%250Aschemes%252C%2520HoloV%2520rethinks%2520token%2520retention%2520from%2520a%2520holistic%2520perspective.%2520By%250Aadaptively%2520distributing%2520the%2520pruning%2520budget%2520across%2520different%2520spatial%2520crops%252C%250AHoloV%2520ensures%2520that%2520the%2520retained%2520tokens%2520capture%2520the%2520global%2520visual%2520context%2520rather%250Athan%2520isolated%2520salient%2520features.%2520This%2520strategy%2520minimizes%2520representational%250Acollapse%2520and%2520maintains%2520task-relevant%2520information%2520even%2520under%2520aggressive%2520pruning.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520HoloV%2520achieves%2520superior%2520performance%250Aacross%2520various%2520tasks%252C%2520MLLM%2520architectures%252C%2520and%2520pruning%2520ratios%2520compared%2520to%2520SOTA%250Amethods.%2520For%2520instance%252C%2520LLaVA1.5%2520equipped%2520with%2520HoloV%2520preserves%252095.8%255C%2525%2520of%2520the%250Aoriginal%2520performance%2520after%2520pruning%252088.9%255C%2525%2520of%2520visual%2520tokens%252C%2520achieving%2520superior%250Aefficiency-accuracy%2520trade-offs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Just%20Chase%20%22Highlighted%20Tokens%22%20in%20MLLMs%3A%20Revisiting%20Visual%0A%20%20Holistic%20Context%20Retention&entry.906535625=Xin%20Zou%20and%20Di%20Lu%20and%20Yizhou%20Wang%20and%20Yibo%20Yan%20and%20Yuanhuiyi%20Lyu%20and%20Xu%20Zheng%20and%20Linfeng%20Zhang%20and%20Xuming%20Hu&entry.1292438233=%20%20Despite%20their%20powerful%20capabilities%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Asuffer%20from%20considerable%20computational%20overhead%20due%20to%20their%20reliance%20on%0Amassive%20visual%20tokens.%20Recent%20studies%20have%20explored%20token%20pruning%20to%20alleviate%0Athis%20problem%2C%20which%20typically%20uses%20text-vision%20cross-attention%20or%0A%5B%5Ctexttt%7BCLS%7D%5D%20attention%20to%20assess%20and%20discard%20redundant%20visual%20tokens.%20In%20this%0Awork%2C%20we%20identify%20a%20critical%20limitation%20of%20such%20attention-first%20pruning%0Aapproaches%2C%20i.e.%2C%20they%20tend%20to%20preserve%20semantically%20similar%20tokens%2C%20resulting%0Ain%20pronounced%20performance%20drops%20under%20high%20pruning%20ratios.%20To%20this%20end%2C%20we%0Apropose%20%7BHoloV%7D%2C%20a%20simple%20yet%20effective%2C%20plug-and-play%20visual%20token%20pruning%0Aframework%20for%20efficient%20inference.%20Distinct%20from%20previous%20attention-first%0Aschemes%2C%20HoloV%20rethinks%20token%20retention%20from%20a%20holistic%20perspective.%20By%0Aadaptively%20distributing%20the%20pruning%20budget%20across%20different%20spatial%20crops%2C%0AHoloV%20ensures%20that%20the%20retained%20tokens%20capture%20the%20global%20visual%20context%20rather%0Athan%20isolated%20salient%20features.%20This%20strategy%20minimizes%20representational%0Acollapse%20and%20maintains%20task-relevant%20information%20even%20under%20aggressive%20pruning.%0AExperimental%20results%20demonstrate%20that%20our%20HoloV%20achieves%20superior%20performance%0Aacross%20various%20tasks%2C%20MLLM%20architectures%2C%20and%20pruning%20ratios%20compared%20to%20SOTA%0Amethods.%20For%20instance%2C%20LLaVA1.5%20equipped%20with%20HoloV%20preserves%2095.8%5C%25%20of%20the%0Aoriginal%20performance%20after%20pruning%2088.9%5C%25%20of%20visual%20tokens%2C%20achieving%20superior%0Aefficiency-accuracy%20trade-offs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02912v1&entry.124074799=Read"},
{"title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework", "author": "Lorenzo Bianchi and Giacomo Pacini and Fabio Carrara and Nicola Messina and Giuseppe Amato and Fabrizio Falchi", "abstract": "  Zero-shot captioners are recently proposed models that utilize common-space\nvision-language representations to caption images without relying on paired\nimage-text data. To caption an image, they proceed by textually decoding a\ntext-aligned image feature, but they limit their scope to global\nrepresentations and whole-image captions. We present \\frameworkName{}, a\nunified framework for zero-shot captioning that shifts from an image-centric to\na patch-centric paradigm, enabling the captioning of arbitrary regions without\nthe need of region-level supervision. Instead of relying on global image\nrepresentations, we treat individual patches as atomic captioning units and\naggregate them to describe arbitrary regions, from single patches to\nnon-contiguous areas and entire images. We analyze the key ingredients that\nenable current latent captioners to work in our novel proposed framework.\nExperiments demonstrate that backbones producing meaningful, dense visual\nfeatures, such as DINO, are key to achieving state-of-the-art performance in\nmultiple region-based captioning tasks. Compared to other baselines and\nstate-of-the-art competitors, our models achieve better performance on\nzero-shot dense, region-set, and a newly introduced trace captioning task,\nhighlighting the effectiveness of patch-wise semantic representations for\nscalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .\n", "link": "http://arxiv.org/abs/2510.02898v1", "date": "2025-10-03", "relevancy": 2.8186, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Patch%20to%20Caption%20Them%20All%3A%20A%20Unified%20Zero-Shot%20Captioning%20Framework&body=Title%3A%20One%20Patch%20to%20Caption%20Them%20All%3A%20A%20Unified%20Zero-Shot%20Captioning%20Framework%0AAuthor%3A%20Lorenzo%20Bianchi%20and%20Giacomo%20Pacini%20and%20Fabio%20Carrara%20and%20Nicola%20Messina%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi%0AAbstract%3A%20%20%20Zero-shot%20captioners%20are%20recently%20proposed%20models%20that%20utilize%20common-space%0Avision-language%20representations%20to%20caption%20images%20without%20relying%20on%20paired%0Aimage-text%20data.%20To%20caption%20an%20image%2C%20they%20proceed%20by%20textually%20decoding%20a%0Atext-aligned%20image%20feature%2C%20but%20they%20limit%20their%20scope%20to%20global%0Arepresentations%20and%20whole-image%20captions.%20We%20present%20%5CframeworkName%7B%7D%2C%20a%0Aunified%20framework%20for%20zero-shot%20captioning%20that%20shifts%20from%20an%20image-centric%20to%0Aa%20patch-centric%20paradigm%2C%20enabling%20the%20captioning%20of%20arbitrary%20regions%20without%0Athe%20need%20of%20region-level%20supervision.%20Instead%20of%20relying%20on%20global%20image%0Arepresentations%2C%20we%20treat%20individual%20patches%20as%20atomic%20captioning%20units%20and%0Aaggregate%20them%20to%20describe%20arbitrary%20regions%2C%20from%20single%20patches%20to%0Anon-contiguous%20areas%20and%20entire%20images.%20We%20analyze%20the%20key%20ingredients%20that%0Aenable%20current%20latent%20captioners%20to%20work%20in%20our%20novel%20proposed%20framework.%0AExperiments%20demonstrate%20that%20backbones%20producing%20meaningful%2C%20dense%20visual%0Afeatures%2C%20such%20as%20DINO%2C%20are%20key%20to%20achieving%20state-of-the-art%20performance%20in%0Amultiple%20region-based%20captioning%20tasks.%20Compared%20to%20other%20baselines%20and%0Astate-of-the-art%20competitors%2C%20our%20models%20achieve%20better%20performance%20on%0Azero-shot%20dense%2C%20region-set%2C%20and%20a%20newly%20introduced%20trace%20captioning%20task%2C%0Ahighlighting%20the%20effectiveness%20of%20patch-wise%20semantic%20representations%20for%0Ascalable%20caption%20generation.%20Project%20page%20at%20https%3A//paciosoft.com/Patch-ioner/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Patch%2520to%2520Caption%2520Them%2520All%253A%2520A%2520Unified%2520Zero-Shot%2520Captioning%2520Framework%26entry.906535625%3DLorenzo%2520Bianchi%2520and%2520Giacomo%2520Pacini%2520and%2520Fabio%2520Carrara%2520and%2520Nicola%2520Messina%2520and%2520Giuseppe%2520Amato%2520and%2520Fabrizio%2520Falchi%26entry.1292438233%3D%2520%2520Zero-shot%2520captioners%2520are%2520recently%2520proposed%2520models%2520that%2520utilize%2520common-space%250Avision-language%2520representations%2520to%2520caption%2520images%2520without%2520relying%2520on%2520paired%250Aimage-text%2520data.%2520To%2520caption%2520an%2520image%252C%2520they%2520proceed%2520by%2520textually%2520decoding%2520a%250Atext-aligned%2520image%2520feature%252C%2520but%2520they%2520limit%2520their%2520scope%2520to%2520global%250Arepresentations%2520and%2520whole-image%2520captions.%2520We%2520present%2520%255CframeworkName%257B%257D%252C%2520a%250Aunified%2520framework%2520for%2520zero-shot%2520captioning%2520that%2520shifts%2520from%2520an%2520image-centric%2520to%250Aa%2520patch-centric%2520paradigm%252C%2520enabling%2520the%2520captioning%2520of%2520arbitrary%2520regions%2520without%250Athe%2520need%2520of%2520region-level%2520supervision.%2520Instead%2520of%2520relying%2520on%2520global%2520image%250Arepresentations%252C%2520we%2520treat%2520individual%2520patches%2520as%2520atomic%2520captioning%2520units%2520and%250Aaggregate%2520them%2520to%2520describe%2520arbitrary%2520regions%252C%2520from%2520single%2520patches%2520to%250Anon-contiguous%2520areas%2520and%2520entire%2520images.%2520We%2520analyze%2520the%2520key%2520ingredients%2520that%250Aenable%2520current%2520latent%2520captioners%2520to%2520work%2520in%2520our%2520novel%2520proposed%2520framework.%250AExperiments%2520demonstrate%2520that%2520backbones%2520producing%2520meaningful%252C%2520dense%2520visual%250Afeatures%252C%2520such%2520as%2520DINO%252C%2520are%2520key%2520to%2520achieving%2520state-of-the-art%2520performance%2520in%250Amultiple%2520region-based%2520captioning%2520tasks.%2520Compared%2520to%2520other%2520baselines%2520and%250Astate-of-the-art%2520competitors%252C%2520our%2520models%2520achieve%2520better%2520performance%2520on%250Azero-shot%2520dense%252C%2520region-set%252C%2520and%2520a%2520newly%2520introduced%2520trace%2520captioning%2520task%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520patch-wise%2520semantic%2520representations%2520for%250Ascalable%2520caption%2520generation.%2520Project%2520page%2520at%2520https%253A//paciosoft.com/Patch-ioner/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Patch%20to%20Caption%20Them%20All%3A%20A%20Unified%20Zero-Shot%20Captioning%20Framework&entry.906535625=Lorenzo%20Bianchi%20and%20Giacomo%20Pacini%20and%20Fabio%20Carrara%20and%20Nicola%20Messina%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi&entry.1292438233=%20%20Zero-shot%20captioners%20are%20recently%20proposed%20models%20that%20utilize%20common-space%0Avision-language%20representations%20to%20caption%20images%20without%20relying%20on%20paired%0Aimage-text%20data.%20To%20caption%20an%20image%2C%20they%20proceed%20by%20textually%20decoding%20a%0Atext-aligned%20image%20feature%2C%20but%20they%20limit%20their%20scope%20to%20global%0Arepresentations%20and%20whole-image%20captions.%20We%20present%20%5CframeworkName%7B%7D%2C%20a%0Aunified%20framework%20for%20zero-shot%20captioning%20that%20shifts%20from%20an%20image-centric%20to%0Aa%20patch-centric%20paradigm%2C%20enabling%20the%20captioning%20of%20arbitrary%20regions%20without%0Athe%20need%20of%20region-level%20supervision.%20Instead%20of%20relying%20on%20global%20image%0Arepresentations%2C%20we%20treat%20individual%20patches%20as%20atomic%20captioning%20units%20and%0Aaggregate%20them%20to%20describe%20arbitrary%20regions%2C%20from%20single%20patches%20to%0Anon-contiguous%20areas%20and%20entire%20images.%20We%20analyze%20the%20key%20ingredients%20that%0Aenable%20current%20latent%20captioners%20to%20work%20in%20our%20novel%20proposed%20framework.%0AExperiments%20demonstrate%20that%20backbones%20producing%20meaningful%2C%20dense%20visual%0Afeatures%2C%20such%20as%20DINO%2C%20are%20key%20to%20achieving%20state-of-the-art%20performance%20in%0Amultiple%20region-based%20captioning%20tasks.%20Compared%20to%20other%20baselines%20and%0Astate-of-the-art%20competitors%2C%20our%20models%20achieve%20better%20performance%20on%0Azero-shot%20dense%2C%20region-set%2C%20and%20a%20newly%20introduced%20trace%20captioning%20task%2C%0Ahighlighting%20the%20effectiveness%20of%20patch-wise%20semantic%20representations%20for%0Ascalable%20caption%20generation.%20Project%20page%20at%20https%3A//paciosoft.com/Patch-ioner/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02898v1&entry.124074799=Read"},
{"title": "Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled\n  Fields", "author": "Zhiting Mei and Ola Shorinwa and Anirudha Majumdar", "abstract": "  Semantic distillation in radiance fields has spurred significant advances in\nopen-vocabulary robot policies, e.g., in manipulation and navigation, founded\non pretrained semantics from large vision models. While prior work has\ndemonstrated the effectiveness of visual-only semantic features (e.g., DINO and\nCLIP) in Gaussian Splatting and neural radiance fields, the potential benefit\nof geometry-grounding in distilled fields remains an open question. In\nprinciple, visual-geometry features seem very promising for spatial tasks such\nas pose estimation, prompting the question: Do geometry-grounded semantic\nfeatures offer an edge in distilled fields? Specifically, we ask three critical\nquestions: First, does spatial-grounding produce higher-fidelity geometry-aware\nsemantic features? We find that image features from geometry-grounded backbones\ncontain finer structural details compared to their counterparts. Secondly, does\ngeometry-grounding improve semantic object localization? We observe no\nsignificant difference in this task. Thirdly, does geometry-grounding enable\nhigher-accuracy radiance field inversion? Given the limitations of prior work\nand their lack of semantics integration, we propose a novel framework SPINE for\ninverting radiance fields without an initial guess, consisting of two core\ncomponents: coarse inversion using distilled semantics, and fine inversion\nusing photometric-based optimization. Surprisingly, we find that the pose\nestimation accuracy decreases with geometry-grounded features. Our results\nsuggest that visual-only features offer greater versatility for a broader range\nof downstream tasks, although geometry-grounded features contain more geometric\ndetail. Notably, our findings underscore the necessity of future research on\neffective strategies for geometry-grounding that augment the versatility and\nperformance of pretrained semantic features.\n", "link": "http://arxiv.org/abs/2510.03104v1", "date": "2025-10-03", "relevancy": 2.7957, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5644}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Meets%20Vision%3A%20Revisiting%20Pretrained%20Semantics%20in%20Distilled%0A%20%20Fields&body=Title%3A%20Geometry%20Meets%20Vision%3A%20Revisiting%20Pretrained%20Semantics%20in%20Distilled%0A%20%20Fields%0AAuthor%3A%20Zhiting%20Mei%20and%20Ola%20Shorinwa%20and%20Anirudha%20Majumdar%0AAbstract%3A%20%20%20Semantic%20distillation%20in%20radiance%20fields%20has%20spurred%20significant%20advances%20in%0Aopen-vocabulary%20robot%20policies%2C%20e.g.%2C%20in%20manipulation%20and%20navigation%2C%20founded%0Aon%20pretrained%20semantics%20from%20large%20vision%20models.%20While%20prior%20work%20has%0Ademonstrated%20the%20effectiveness%20of%20visual-only%20semantic%20features%20%28e.g.%2C%20DINO%20and%0ACLIP%29%20in%20Gaussian%20Splatting%20and%20neural%20radiance%20fields%2C%20the%20potential%20benefit%0Aof%20geometry-grounding%20in%20distilled%20fields%20remains%20an%20open%20question.%20In%0Aprinciple%2C%20visual-geometry%20features%20seem%20very%20promising%20for%20spatial%20tasks%20such%0Aas%20pose%20estimation%2C%20prompting%20the%20question%3A%20Do%20geometry-grounded%20semantic%0Afeatures%20offer%20an%20edge%20in%20distilled%20fields%3F%20Specifically%2C%20we%20ask%20three%20critical%0Aquestions%3A%20First%2C%20does%20spatial-grounding%20produce%20higher-fidelity%20geometry-aware%0Asemantic%20features%3F%20We%20find%20that%20image%20features%20from%20geometry-grounded%20backbones%0Acontain%20finer%20structural%20details%20compared%20to%20their%20counterparts.%20Secondly%2C%20does%0Ageometry-grounding%20improve%20semantic%20object%20localization%3F%20We%20observe%20no%0Asignificant%20difference%20in%20this%20task.%20Thirdly%2C%20does%20geometry-grounding%20enable%0Ahigher-accuracy%20radiance%20field%20inversion%3F%20Given%20the%20limitations%20of%20prior%20work%0Aand%20their%20lack%20of%20semantics%20integration%2C%20we%20propose%20a%20novel%20framework%20SPINE%20for%0Ainverting%20radiance%20fields%20without%20an%20initial%20guess%2C%20consisting%20of%20two%20core%0Acomponents%3A%20coarse%20inversion%20using%20distilled%20semantics%2C%20and%20fine%20inversion%0Ausing%20photometric-based%20optimization.%20Surprisingly%2C%20we%20find%20that%20the%20pose%0Aestimation%20accuracy%20decreases%20with%20geometry-grounded%20features.%20Our%20results%0Asuggest%20that%20visual-only%20features%20offer%20greater%20versatility%20for%20a%20broader%20range%0Aof%20downstream%20tasks%2C%20although%20geometry-grounded%20features%20contain%20more%20geometric%0Adetail.%20Notably%2C%20our%20findings%20underscore%20the%20necessity%20of%20future%20research%20on%0Aeffective%20strategies%20for%20geometry-grounding%20that%20augment%20the%20versatility%20and%0Aperformance%20of%20pretrained%20semantic%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Meets%2520Vision%253A%2520Revisiting%2520Pretrained%2520Semantics%2520in%2520Distilled%250A%2520%2520Fields%26entry.906535625%3DZhiting%2520Mei%2520and%2520Ola%2520Shorinwa%2520and%2520Anirudha%2520Majumdar%26entry.1292438233%3D%2520%2520Semantic%2520distillation%2520in%2520radiance%2520fields%2520has%2520spurred%2520significant%2520advances%2520in%250Aopen-vocabulary%2520robot%2520policies%252C%2520e.g.%252C%2520in%2520manipulation%2520and%2520navigation%252C%2520founded%250Aon%2520pretrained%2520semantics%2520from%2520large%2520vision%2520models.%2520While%2520prior%2520work%2520has%250Ademonstrated%2520the%2520effectiveness%2520of%2520visual-only%2520semantic%2520features%2520%2528e.g.%252C%2520DINO%2520and%250ACLIP%2529%2520in%2520Gaussian%2520Splatting%2520and%2520neural%2520radiance%2520fields%252C%2520the%2520potential%2520benefit%250Aof%2520geometry-grounding%2520in%2520distilled%2520fields%2520remains%2520an%2520open%2520question.%2520In%250Aprinciple%252C%2520visual-geometry%2520features%2520seem%2520very%2520promising%2520for%2520spatial%2520tasks%2520such%250Aas%2520pose%2520estimation%252C%2520prompting%2520the%2520question%253A%2520Do%2520geometry-grounded%2520semantic%250Afeatures%2520offer%2520an%2520edge%2520in%2520distilled%2520fields%253F%2520Specifically%252C%2520we%2520ask%2520three%2520critical%250Aquestions%253A%2520First%252C%2520does%2520spatial-grounding%2520produce%2520higher-fidelity%2520geometry-aware%250Asemantic%2520features%253F%2520We%2520find%2520that%2520image%2520features%2520from%2520geometry-grounded%2520backbones%250Acontain%2520finer%2520structural%2520details%2520compared%2520to%2520their%2520counterparts.%2520Secondly%252C%2520does%250Ageometry-grounding%2520improve%2520semantic%2520object%2520localization%253F%2520We%2520observe%2520no%250Asignificant%2520difference%2520in%2520this%2520task.%2520Thirdly%252C%2520does%2520geometry-grounding%2520enable%250Ahigher-accuracy%2520radiance%2520field%2520inversion%253F%2520Given%2520the%2520limitations%2520of%2520prior%2520work%250Aand%2520their%2520lack%2520of%2520semantics%2520integration%252C%2520we%2520propose%2520a%2520novel%2520framework%2520SPINE%2520for%250Ainverting%2520radiance%2520fields%2520without%2520an%2520initial%2520guess%252C%2520consisting%2520of%2520two%2520core%250Acomponents%253A%2520coarse%2520inversion%2520using%2520distilled%2520semantics%252C%2520and%2520fine%2520inversion%250Ausing%2520photometric-based%2520optimization.%2520Surprisingly%252C%2520we%2520find%2520that%2520the%2520pose%250Aestimation%2520accuracy%2520decreases%2520with%2520geometry-grounded%2520features.%2520Our%2520results%250Asuggest%2520that%2520visual-only%2520features%2520offer%2520greater%2520versatility%2520for%2520a%2520broader%2520range%250Aof%2520downstream%2520tasks%252C%2520although%2520geometry-grounded%2520features%2520contain%2520more%2520geometric%250Adetail.%2520Notably%252C%2520our%2520findings%2520underscore%2520the%2520necessity%2520of%2520future%2520research%2520on%250Aeffective%2520strategies%2520for%2520geometry-grounding%2520that%2520augment%2520the%2520versatility%2520and%250Aperformance%2520of%2520pretrained%2520semantic%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Meets%20Vision%3A%20Revisiting%20Pretrained%20Semantics%20in%20Distilled%0A%20%20Fields&entry.906535625=Zhiting%20Mei%20and%20Ola%20Shorinwa%20and%20Anirudha%20Majumdar&entry.1292438233=%20%20Semantic%20distillation%20in%20radiance%20fields%20has%20spurred%20significant%20advances%20in%0Aopen-vocabulary%20robot%20policies%2C%20e.g.%2C%20in%20manipulation%20and%20navigation%2C%20founded%0Aon%20pretrained%20semantics%20from%20large%20vision%20models.%20While%20prior%20work%20has%0Ademonstrated%20the%20effectiveness%20of%20visual-only%20semantic%20features%20%28e.g.%2C%20DINO%20and%0ACLIP%29%20in%20Gaussian%20Splatting%20and%20neural%20radiance%20fields%2C%20the%20potential%20benefit%0Aof%20geometry-grounding%20in%20distilled%20fields%20remains%20an%20open%20question.%20In%0Aprinciple%2C%20visual-geometry%20features%20seem%20very%20promising%20for%20spatial%20tasks%20such%0Aas%20pose%20estimation%2C%20prompting%20the%20question%3A%20Do%20geometry-grounded%20semantic%0Afeatures%20offer%20an%20edge%20in%20distilled%20fields%3F%20Specifically%2C%20we%20ask%20three%20critical%0Aquestions%3A%20First%2C%20does%20spatial-grounding%20produce%20higher-fidelity%20geometry-aware%0Asemantic%20features%3F%20We%20find%20that%20image%20features%20from%20geometry-grounded%20backbones%0Acontain%20finer%20structural%20details%20compared%20to%20their%20counterparts.%20Secondly%2C%20does%0Ageometry-grounding%20improve%20semantic%20object%20localization%3F%20We%20observe%20no%0Asignificant%20difference%20in%20this%20task.%20Thirdly%2C%20does%20geometry-grounding%20enable%0Ahigher-accuracy%20radiance%20field%20inversion%3F%20Given%20the%20limitations%20of%20prior%20work%0Aand%20their%20lack%20of%20semantics%20integration%2C%20we%20propose%20a%20novel%20framework%20SPINE%20for%0Ainverting%20radiance%20fields%20without%20an%20initial%20guess%2C%20consisting%20of%20two%20core%0Acomponents%3A%20coarse%20inversion%20using%20distilled%20semantics%2C%20and%20fine%20inversion%0Ausing%20photometric-based%20optimization.%20Surprisingly%2C%20we%20find%20that%20the%20pose%0Aestimation%20accuracy%20decreases%20with%20geometry-grounded%20features.%20Our%20results%0Asuggest%20that%20visual-only%20features%20offer%20greater%20versatility%20for%20a%20broader%20range%0Aof%20downstream%20tasks%2C%20although%20geometry-grounded%20features%20contain%20more%20geometric%0Adetail.%20Notably%2C%20our%20findings%20underscore%20the%20necessity%20of%20future%20research%20on%0Aeffective%20strategies%20for%20geometry-grounding%20that%20augment%20the%20versatility%20and%0Aperformance%20of%20pretrained%20semantic%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03104v1&entry.124074799=Read"},
{"title": "Multimodal Carotid Risk Stratification with Large Vision-Language\n  Models: Benchmarking, Fine-Tuning, and Clinical Insights", "author": "Daphne Tsolissou and Theofanis Ganitidis and Konstantinos Mitsis and Stergios CHristodoulidis and Maria Vakalopoulou and Konstantina Nikita", "abstract": "  Reliable risk assessment for carotid atheromatous disease remains a major\nclinical challenge, as it requires integrating diverse clinical and imaging\ninformation in a manner that is transparent and interpretable to clinicians.\nThis study investigates the potential of state-of-the-art and recent large\nvision-language models (LVLMs) for multimodal carotid plaque assessment by\nintegrating ultrasound imaging (USI) with structured clinical, demographic,\nlaboratory, and protein biomarker data. A framework that simulates realistic\ndiagnostic scenarios through interview-style question sequences is proposed,\ncomparing a range of open-source LVLMs, including both general-purpose and\nmedically tuned models. Zero-shot experiments reveal that even if they are very\npowerful, not all LVLMs can accurately identify imaging modality and anatomy,\nwhile all of them perform poorly in accurate risk classification. To address\nthis limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using\nlow-rank adaptation (LoRA), resulting in substantial improvements in stroke\nrisk stratification. The integration of multimodal tabular data in the form of\ntext further enhances specificity and balanced accuracy, yielding competitive\nperformance compared to prior convolutional neural network (CNN) baselines\ntrained on the same dataset. Our findings highlight both the promise and\nlimitations of LVLMs in ultrasound-based cardiovascular risk prediction,\nunderscoring the importance of multimodal integration, model calibration, and\ndomain adaptation for clinical translation.\n", "link": "http://arxiv.org/abs/2510.02922v1", "date": "2025-10-03", "relevancy": 2.7398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Carotid%20Risk%20Stratification%20with%20Large%20Vision-Language%0A%20%20Models%3A%20Benchmarking%2C%20Fine-Tuning%2C%20and%20Clinical%20Insights&body=Title%3A%20Multimodal%20Carotid%20Risk%20Stratification%20with%20Large%20Vision-Language%0A%20%20Models%3A%20Benchmarking%2C%20Fine-Tuning%2C%20and%20Clinical%20Insights%0AAuthor%3A%20Daphne%20Tsolissou%20and%20Theofanis%20Ganitidis%20and%20Konstantinos%20Mitsis%20and%20Stergios%20CHristodoulidis%20and%20Maria%20Vakalopoulou%20and%20Konstantina%20Nikita%0AAbstract%3A%20%20%20Reliable%20risk%20assessment%20for%20carotid%20atheromatous%20disease%20remains%20a%20major%0Aclinical%20challenge%2C%20as%20it%20requires%20integrating%20diverse%20clinical%20and%20imaging%0Ainformation%20in%20a%20manner%20that%20is%20transparent%20and%20interpretable%20to%20clinicians.%0AThis%20study%20investigates%20the%20potential%20of%20state-of-the-art%20and%20recent%20large%0Avision-language%20models%20%28LVLMs%29%20for%20multimodal%20carotid%20plaque%20assessment%20by%0Aintegrating%20ultrasound%20imaging%20%28USI%29%20with%20structured%20clinical%2C%20demographic%2C%0Alaboratory%2C%20and%20protein%20biomarker%20data.%20A%20framework%20that%20simulates%20realistic%0Adiagnostic%20scenarios%20through%20interview-style%20question%20sequences%20is%20proposed%2C%0Acomparing%20a%20range%20of%20open-source%20LVLMs%2C%20including%20both%20general-purpose%20and%0Amedically%20tuned%20models.%20Zero-shot%20experiments%20reveal%20that%20even%20if%20they%20are%20very%0Apowerful%2C%20not%20all%20LVLMs%20can%20accurately%20identify%20imaging%20modality%20and%20anatomy%2C%0Awhile%20all%20of%20them%20perform%20poorly%20in%20accurate%20risk%20classification.%20To%20address%0Athis%20limitation%2C%20LLaVa-NeXT-Vicuna%20is%20adapted%20to%20the%20ultrasound%20domain%20using%0Alow-rank%20adaptation%20%28LoRA%29%2C%20resulting%20in%20substantial%20improvements%20in%20stroke%0Arisk%20stratification.%20The%20integration%20of%20multimodal%20tabular%20data%20in%20the%20form%20of%0Atext%20further%20enhances%20specificity%20and%20balanced%20accuracy%2C%20yielding%20competitive%0Aperformance%20compared%20to%20prior%20convolutional%20neural%20network%20%28CNN%29%20baselines%0Atrained%20on%20the%20same%20dataset.%20Our%20findings%20highlight%20both%20the%20promise%20and%0Alimitations%20of%20LVLMs%20in%20ultrasound-based%20cardiovascular%20risk%20prediction%2C%0Aunderscoring%20the%20importance%20of%20multimodal%20integration%2C%20model%20calibration%2C%20and%0Adomain%20adaptation%20for%20clinical%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Carotid%2520Risk%2520Stratification%2520with%2520Large%2520Vision-Language%250A%2520%2520Models%253A%2520Benchmarking%252C%2520Fine-Tuning%252C%2520and%2520Clinical%2520Insights%26entry.906535625%3DDaphne%2520Tsolissou%2520and%2520Theofanis%2520Ganitidis%2520and%2520Konstantinos%2520Mitsis%2520and%2520Stergios%2520CHristodoulidis%2520and%2520Maria%2520Vakalopoulou%2520and%2520Konstantina%2520Nikita%26entry.1292438233%3D%2520%2520Reliable%2520risk%2520assessment%2520for%2520carotid%2520atheromatous%2520disease%2520remains%2520a%2520major%250Aclinical%2520challenge%252C%2520as%2520it%2520requires%2520integrating%2520diverse%2520clinical%2520and%2520imaging%250Ainformation%2520in%2520a%2520manner%2520that%2520is%2520transparent%2520and%2520interpretable%2520to%2520clinicians.%250AThis%2520study%2520investigates%2520the%2520potential%2520of%2520state-of-the-art%2520and%2520recent%2520large%250Avision-language%2520models%2520%2528LVLMs%2529%2520for%2520multimodal%2520carotid%2520plaque%2520assessment%2520by%250Aintegrating%2520ultrasound%2520imaging%2520%2528USI%2529%2520with%2520structured%2520clinical%252C%2520demographic%252C%250Alaboratory%252C%2520and%2520protein%2520biomarker%2520data.%2520A%2520framework%2520that%2520simulates%2520realistic%250Adiagnostic%2520scenarios%2520through%2520interview-style%2520question%2520sequences%2520is%2520proposed%252C%250Acomparing%2520a%2520range%2520of%2520open-source%2520LVLMs%252C%2520including%2520both%2520general-purpose%2520and%250Amedically%2520tuned%2520models.%2520Zero-shot%2520experiments%2520reveal%2520that%2520even%2520if%2520they%2520are%2520very%250Apowerful%252C%2520not%2520all%2520LVLMs%2520can%2520accurately%2520identify%2520imaging%2520modality%2520and%2520anatomy%252C%250Awhile%2520all%2520of%2520them%2520perform%2520poorly%2520in%2520accurate%2520risk%2520classification.%2520To%2520address%250Athis%2520limitation%252C%2520LLaVa-NeXT-Vicuna%2520is%2520adapted%2520to%2520the%2520ultrasound%2520domain%2520using%250Alow-rank%2520adaptation%2520%2528LoRA%2529%252C%2520resulting%2520in%2520substantial%2520improvements%2520in%2520stroke%250Arisk%2520stratification.%2520The%2520integration%2520of%2520multimodal%2520tabular%2520data%2520in%2520the%2520form%2520of%250Atext%2520further%2520enhances%2520specificity%2520and%2520balanced%2520accuracy%252C%2520yielding%2520competitive%250Aperformance%2520compared%2520to%2520prior%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520baselines%250Atrained%2520on%2520the%2520same%2520dataset.%2520Our%2520findings%2520highlight%2520both%2520the%2520promise%2520and%250Alimitations%2520of%2520LVLMs%2520in%2520ultrasound-based%2520cardiovascular%2520risk%2520prediction%252C%250Aunderscoring%2520the%2520importance%2520of%2520multimodal%2520integration%252C%2520model%2520calibration%252C%2520and%250Adomain%2520adaptation%2520for%2520clinical%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Carotid%20Risk%20Stratification%20with%20Large%20Vision-Language%0A%20%20Models%3A%20Benchmarking%2C%20Fine-Tuning%2C%20and%20Clinical%20Insights&entry.906535625=Daphne%20Tsolissou%20and%20Theofanis%20Ganitidis%20and%20Konstantinos%20Mitsis%20and%20Stergios%20CHristodoulidis%20and%20Maria%20Vakalopoulou%20and%20Konstantina%20Nikita&entry.1292438233=%20%20Reliable%20risk%20assessment%20for%20carotid%20atheromatous%20disease%20remains%20a%20major%0Aclinical%20challenge%2C%20as%20it%20requires%20integrating%20diverse%20clinical%20and%20imaging%0Ainformation%20in%20a%20manner%20that%20is%20transparent%20and%20interpretable%20to%20clinicians.%0AThis%20study%20investigates%20the%20potential%20of%20state-of-the-art%20and%20recent%20large%0Avision-language%20models%20%28LVLMs%29%20for%20multimodal%20carotid%20plaque%20assessment%20by%0Aintegrating%20ultrasound%20imaging%20%28USI%29%20with%20structured%20clinical%2C%20demographic%2C%0Alaboratory%2C%20and%20protein%20biomarker%20data.%20A%20framework%20that%20simulates%20realistic%0Adiagnostic%20scenarios%20through%20interview-style%20question%20sequences%20is%20proposed%2C%0Acomparing%20a%20range%20of%20open-source%20LVLMs%2C%20including%20both%20general-purpose%20and%0Amedically%20tuned%20models.%20Zero-shot%20experiments%20reveal%20that%20even%20if%20they%20are%20very%0Apowerful%2C%20not%20all%20LVLMs%20can%20accurately%20identify%20imaging%20modality%20and%20anatomy%2C%0Awhile%20all%20of%20them%20perform%20poorly%20in%20accurate%20risk%20classification.%20To%20address%0Athis%20limitation%2C%20LLaVa-NeXT-Vicuna%20is%20adapted%20to%20the%20ultrasound%20domain%20using%0Alow-rank%20adaptation%20%28LoRA%29%2C%20resulting%20in%20substantial%20improvements%20in%20stroke%0Arisk%20stratification.%20The%20integration%20of%20multimodal%20tabular%20data%20in%20the%20form%20of%0Atext%20further%20enhances%20specificity%20and%20balanced%20accuracy%2C%20yielding%20competitive%0Aperformance%20compared%20to%20prior%20convolutional%20neural%20network%20%28CNN%29%20baselines%0Atrained%20on%20the%20same%20dataset.%20Our%20findings%20highlight%20both%20the%20promise%20and%0Alimitations%20of%20LVLMs%20in%20ultrasound-based%20cardiovascular%20risk%20prediction%2C%0Aunderscoring%20the%20importance%20of%20multimodal%20integration%2C%20model%20calibration%2C%20and%0Adomain%20adaptation%20for%20clinical%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02922v1&entry.124074799=Read"},
{"title": "Gate-Shift-Pose: Enhancing Action Recognition in Sports with Skeleton\n  Information", "author": "Edoardo Bianchi and Oswald Lanz", "abstract": "  This paper introduces Gate-Shift-Pose, an enhanced version of Gate-Shift-Fuse\nnetworks, designed for athlete fall classification in figure skating by\nintegrating skeleton pose data alongside RGB frames. We evaluate two fusion\nstrategies: early-fusion, which combines RGB frames with Gaussian heatmaps of\npose keypoints at the input stage, and late-fusion, which employs a\nmulti-stream architecture with attention mechanisms to combine RGB and pose\nfeatures. Experiments on the FR-FS dataset demonstrate that Gate-Shift-Pose\nsignificantly outperforms the RGB-only baseline, improving accuracy by up to\n40% with ResNet18 and 20% with ResNet50. Early-fusion achieves the highest\naccuracy (98.08%) with ResNet50, leveraging the model's capacity for effective\nmultimodal integration, while late-fusion is better suited for lighter\nbackbones like ResNet18. These results highlight the potential of multimodal\narchitectures for sports action recognition and the critical role of skeleton\npose information in capturing complex motion patterns. Visit the project page\nat https://edowhite.github.io/Gate-Shift-Pose\n", "link": "http://arxiv.org/abs/2503.04470v2", "date": "2025-10-03", "relevancy": 2.6917, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5511}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5375}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gate-Shift-Pose%3A%20Enhancing%20Action%20Recognition%20in%20Sports%20with%20Skeleton%0A%20%20Information&body=Title%3A%20Gate-Shift-Pose%3A%20Enhancing%20Action%20Recognition%20in%20Sports%20with%20Skeleton%0A%20%20Information%0AAuthor%3A%20Edoardo%20Bianchi%20and%20Oswald%20Lanz%0AAbstract%3A%20%20%20This%20paper%20introduces%20Gate-Shift-Pose%2C%20an%20enhanced%20version%20of%20Gate-Shift-Fuse%0Anetworks%2C%20designed%20for%20athlete%20fall%20classification%20in%20figure%20skating%20by%0Aintegrating%20skeleton%20pose%20data%20alongside%20RGB%20frames.%20We%20evaluate%20two%20fusion%0Astrategies%3A%20early-fusion%2C%20which%20combines%20RGB%20frames%20with%20Gaussian%20heatmaps%20of%0Apose%20keypoints%20at%20the%20input%20stage%2C%20and%20late-fusion%2C%20which%20employs%20a%0Amulti-stream%20architecture%20with%20attention%20mechanisms%20to%20combine%20RGB%20and%20pose%0Afeatures.%20Experiments%20on%20the%20FR-FS%20dataset%20demonstrate%20that%20Gate-Shift-Pose%0Asignificantly%20outperforms%20the%20RGB-only%20baseline%2C%20improving%20accuracy%20by%20up%20to%0A40%25%20with%20ResNet18%20and%2020%25%20with%20ResNet50.%20Early-fusion%20achieves%20the%20highest%0Aaccuracy%20%2898.08%25%29%20with%20ResNet50%2C%20leveraging%20the%20model%27s%20capacity%20for%20effective%0Amultimodal%20integration%2C%20while%20late-fusion%20is%20better%20suited%20for%20lighter%0Abackbones%20like%20ResNet18.%20These%20results%20highlight%20the%20potential%20of%20multimodal%0Aarchitectures%20for%20sports%20action%20recognition%20and%20the%20critical%20role%20of%20skeleton%0Apose%20information%20in%20capturing%20complex%20motion%20patterns.%20Visit%20the%20project%20page%0Aat%20https%3A//edowhite.github.io/Gate-Shift-Pose%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04470v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGate-Shift-Pose%253A%2520Enhancing%2520Action%2520Recognition%2520in%2520Sports%2520with%2520Skeleton%250A%2520%2520Information%26entry.906535625%3DEdoardo%2520Bianchi%2520and%2520Oswald%2520Lanz%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Gate-Shift-Pose%252C%2520an%2520enhanced%2520version%2520of%2520Gate-Shift-Fuse%250Anetworks%252C%2520designed%2520for%2520athlete%2520fall%2520classification%2520in%2520figure%2520skating%2520by%250Aintegrating%2520skeleton%2520pose%2520data%2520alongside%2520RGB%2520frames.%2520We%2520evaluate%2520two%2520fusion%250Astrategies%253A%2520early-fusion%252C%2520which%2520combines%2520RGB%2520frames%2520with%2520Gaussian%2520heatmaps%2520of%250Apose%2520keypoints%2520at%2520the%2520input%2520stage%252C%2520and%2520late-fusion%252C%2520which%2520employs%2520a%250Amulti-stream%2520architecture%2520with%2520attention%2520mechanisms%2520to%2520combine%2520RGB%2520and%2520pose%250Afeatures.%2520Experiments%2520on%2520the%2520FR-FS%2520dataset%2520demonstrate%2520that%2520Gate-Shift-Pose%250Asignificantly%2520outperforms%2520the%2520RGB-only%2520baseline%252C%2520improving%2520accuracy%2520by%2520up%2520to%250A40%2525%2520with%2520ResNet18%2520and%252020%2525%2520with%2520ResNet50.%2520Early-fusion%2520achieves%2520the%2520highest%250Aaccuracy%2520%252898.08%2525%2529%2520with%2520ResNet50%252C%2520leveraging%2520the%2520model%2527s%2520capacity%2520for%2520effective%250Amultimodal%2520integration%252C%2520while%2520late-fusion%2520is%2520better%2520suited%2520for%2520lighter%250Abackbones%2520like%2520ResNet18.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520multimodal%250Aarchitectures%2520for%2520sports%2520action%2520recognition%2520and%2520the%2520critical%2520role%2520of%2520skeleton%250Apose%2520information%2520in%2520capturing%2520complex%2520motion%2520patterns.%2520Visit%2520the%2520project%2520page%250Aat%2520https%253A//edowhite.github.io/Gate-Shift-Pose%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04470v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gate-Shift-Pose%3A%20Enhancing%20Action%20Recognition%20in%20Sports%20with%20Skeleton%0A%20%20Information&entry.906535625=Edoardo%20Bianchi%20and%20Oswald%20Lanz&entry.1292438233=%20%20This%20paper%20introduces%20Gate-Shift-Pose%2C%20an%20enhanced%20version%20of%20Gate-Shift-Fuse%0Anetworks%2C%20designed%20for%20athlete%20fall%20classification%20in%20figure%20skating%20by%0Aintegrating%20skeleton%20pose%20data%20alongside%20RGB%20frames.%20We%20evaluate%20two%20fusion%0Astrategies%3A%20early-fusion%2C%20which%20combines%20RGB%20frames%20with%20Gaussian%20heatmaps%20of%0Apose%20keypoints%20at%20the%20input%20stage%2C%20and%20late-fusion%2C%20which%20employs%20a%0Amulti-stream%20architecture%20with%20attention%20mechanisms%20to%20combine%20RGB%20and%20pose%0Afeatures.%20Experiments%20on%20the%20FR-FS%20dataset%20demonstrate%20that%20Gate-Shift-Pose%0Asignificantly%20outperforms%20the%20RGB-only%20baseline%2C%20improving%20accuracy%20by%20up%20to%0A40%25%20with%20ResNet18%20and%2020%25%20with%20ResNet50.%20Early-fusion%20achieves%20the%20highest%0Aaccuracy%20%2898.08%25%29%20with%20ResNet50%2C%20leveraging%20the%20model%27s%20capacity%20for%20effective%0Amultimodal%20integration%2C%20while%20late-fusion%20is%20better%20suited%20for%20lighter%0Abackbones%20like%20ResNet18.%20These%20results%20highlight%20the%20potential%20of%20multimodal%0Aarchitectures%20for%20sports%20action%20recognition%20and%20the%20critical%20role%20of%20skeleton%0Apose%20information%20in%20capturing%20complex%20motion%20patterns.%20Visit%20the%20project%20page%0Aat%20https%3A//edowhite.github.io/Gate-Shift-Pose%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04470v2&entry.124074799=Read"},
{"title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping", "author": "Suyuchen Wang and Tianyu Zhang and Ahmed Masry and Christopher Pal and Spandana Gella and Bang Liu and Perouz Taslakian", "abstract": "  GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.\n", "link": "http://arxiv.org/abs/2510.03230v1", "date": "2025-10-03", "relevancy": 2.6559, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5569}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5197}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20GUI%20Grounding%20with%20Explicit%20Position-to-Coordinate%20Mapping&body=Title%3A%20Improving%20GUI%20Grounding%20with%20Explicit%20Position-to-Coordinate%20Mapping%0AAuthor%3A%20Suyuchen%20Wang%20and%20Tianyu%20Zhang%20and%20Ahmed%20Masry%20and%20Christopher%20Pal%20and%20Spandana%20Gella%20and%20Bang%20Liu%20and%20Perouz%20Taslakian%0AAbstract%3A%20%20%20GUI%20grounding%2C%20the%20task%20of%20mapping%20natural-language%20instructions%20to%20pixel%0Acoordinates%2C%20is%20crucial%20for%20autonomous%20agents%2C%20yet%20remains%20difficult%20for%0Acurrent%20VLMs.%20The%20core%20bottleneck%20is%20reliable%20patch-to-pixel%20mapping%2C%20which%0Abreaks%20when%20extrapolating%20to%20high-resolution%20displays%20unseen%20during%20training.%0ACurrent%20approaches%20generate%20coordinates%20as%20text%20tokens%20directly%20from%20visual%0Afeatures%2C%20forcing%20the%20model%20to%20infer%20complex%20position-to-pixel%20mappings%0Aimplicitly%3B%20as%20a%20result%2C%20accuracy%20degrades%20and%20failures%20proliferate%20on%20new%0Aresolutions.%20We%20address%20this%20with%20two%20complementary%20innovations.%20First%2C%20RULER%0Atokens%20serve%20as%20explicit%20coordinate%20markers%2C%20letting%20the%20model%20reference%0Apositions%20similar%20to%20gridlines%20on%20a%20map%20and%20adjust%20rather%20than%20generate%0Acoordinates%20from%20scratch.%20Second%2C%20Interleaved%20MRoPE%20%28I-MRoPE%29%20improves%20spatial%0Aencoding%20by%20ensuring%20that%20width%20and%20height%20dimensions%20are%20represented%20equally%2C%0Aaddressing%20the%20asymmetry%20of%20standard%20positional%20schemes.%20Experiments%20on%0AScreenSpot%2C%20ScreenSpot-V2%2C%20and%20ScreenSpot-Pro%20show%20consistent%20gains%20in%0Agrounding%20accuracy%2C%20with%20the%20largest%20improvements%20on%20high-resolution%0Ainterfaces.%20By%20providing%20explicit%20spatial%20guidance%20rather%20than%20relying%20on%0Aimplicit%20learning%2C%20our%20approach%20enables%20more%20reliable%20GUI%20automation%20across%0Adiverse%20resolutions%20and%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520GUI%2520Grounding%2520with%2520Explicit%2520Position-to-Coordinate%2520Mapping%26entry.906535625%3DSuyuchen%2520Wang%2520and%2520Tianyu%2520Zhang%2520and%2520Ahmed%2520Masry%2520and%2520Christopher%2520Pal%2520and%2520Spandana%2520Gella%2520and%2520Bang%2520Liu%2520and%2520Perouz%2520Taslakian%26entry.1292438233%3D%2520%2520GUI%2520grounding%252C%2520the%2520task%2520of%2520mapping%2520natural-language%2520instructions%2520to%2520pixel%250Acoordinates%252C%2520is%2520crucial%2520for%2520autonomous%2520agents%252C%2520yet%2520remains%2520difficult%2520for%250Acurrent%2520VLMs.%2520The%2520core%2520bottleneck%2520is%2520reliable%2520patch-to-pixel%2520mapping%252C%2520which%250Abreaks%2520when%2520extrapolating%2520to%2520high-resolution%2520displays%2520unseen%2520during%2520training.%250ACurrent%2520approaches%2520generate%2520coordinates%2520as%2520text%2520tokens%2520directly%2520from%2520visual%250Afeatures%252C%2520forcing%2520the%2520model%2520to%2520infer%2520complex%2520position-to-pixel%2520mappings%250Aimplicitly%253B%2520as%2520a%2520result%252C%2520accuracy%2520degrades%2520and%2520failures%2520proliferate%2520on%2520new%250Aresolutions.%2520We%2520address%2520this%2520with%2520two%2520complementary%2520innovations.%2520First%252C%2520RULER%250Atokens%2520serve%2520as%2520explicit%2520coordinate%2520markers%252C%2520letting%2520the%2520model%2520reference%250Apositions%2520similar%2520to%2520gridlines%2520on%2520a%2520map%2520and%2520adjust%2520rather%2520than%2520generate%250Acoordinates%2520from%2520scratch.%2520Second%252C%2520Interleaved%2520MRoPE%2520%2528I-MRoPE%2529%2520improves%2520spatial%250Aencoding%2520by%2520ensuring%2520that%2520width%2520and%2520height%2520dimensions%2520are%2520represented%2520equally%252C%250Aaddressing%2520the%2520asymmetry%2520of%2520standard%2520positional%2520schemes.%2520Experiments%2520on%250AScreenSpot%252C%2520ScreenSpot-V2%252C%2520and%2520ScreenSpot-Pro%2520show%2520consistent%2520gains%2520in%250Agrounding%2520accuracy%252C%2520with%2520the%2520largest%2520improvements%2520on%2520high-resolution%250Ainterfaces.%2520By%2520providing%2520explicit%2520spatial%2520guidance%2520rather%2520than%2520relying%2520on%250Aimplicit%2520learning%252C%2520our%2520approach%2520enables%2520more%2520reliable%2520GUI%2520automation%2520across%250Adiverse%2520resolutions%2520and%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20GUI%20Grounding%20with%20Explicit%20Position-to-Coordinate%20Mapping&entry.906535625=Suyuchen%20Wang%20and%20Tianyu%20Zhang%20and%20Ahmed%20Masry%20and%20Christopher%20Pal%20and%20Spandana%20Gella%20and%20Bang%20Liu%20and%20Perouz%20Taslakian&entry.1292438233=%20%20GUI%20grounding%2C%20the%20task%20of%20mapping%20natural-language%20instructions%20to%20pixel%0Acoordinates%2C%20is%20crucial%20for%20autonomous%20agents%2C%20yet%20remains%20difficult%20for%0Acurrent%20VLMs.%20The%20core%20bottleneck%20is%20reliable%20patch-to-pixel%20mapping%2C%20which%0Abreaks%20when%20extrapolating%20to%20high-resolution%20displays%20unseen%20during%20training.%0ACurrent%20approaches%20generate%20coordinates%20as%20text%20tokens%20directly%20from%20visual%0Afeatures%2C%20forcing%20the%20model%20to%20infer%20complex%20position-to-pixel%20mappings%0Aimplicitly%3B%20as%20a%20result%2C%20accuracy%20degrades%20and%20failures%20proliferate%20on%20new%0Aresolutions.%20We%20address%20this%20with%20two%20complementary%20innovations.%20First%2C%20RULER%0Atokens%20serve%20as%20explicit%20coordinate%20markers%2C%20letting%20the%20model%20reference%0Apositions%20similar%20to%20gridlines%20on%20a%20map%20and%20adjust%20rather%20than%20generate%0Acoordinates%20from%20scratch.%20Second%2C%20Interleaved%20MRoPE%20%28I-MRoPE%29%20improves%20spatial%0Aencoding%20by%20ensuring%20that%20width%20and%20height%20dimensions%20are%20represented%20equally%2C%0Aaddressing%20the%20asymmetry%20of%20standard%20positional%20schemes.%20Experiments%20on%0AScreenSpot%2C%20ScreenSpot-V2%2C%20and%20ScreenSpot-Pro%20show%20consistent%20gains%20in%0Agrounding%20accuracy%2C%20with%20the%20largest%20improvements%20on%20high-resolution%0Ainterfaces.%20By%20providing%20explicit%20spatial%20guidance%20rather%20than%20relying%20on%0Aimplicit%20learning%2C%20our%20approach%20enables%20more%20reliable%20GUI%20automation%20across%0Adiverse%20resolutions%20and%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03230v1&entry.124074799=Read"},
{"title": "MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture\n  Recognition", "author": "Ricardo T. Fares and Lucas C. Ribas", "abstract": "  Randomized neural networks for representation learning have consistently\nachieved prominent results in texture recognition tasks, effectively combining\nthe advantages of both traditional techniques and learning-based approaches.\nHowever, existing approaches have so far focused mainly on improving\ncross-information prediction, without introducing significant advancements to\nthe overall randomized network architecture. In this paper, we propose Mixer, a\nnovel randomized neural network for texture representation learning. At its\ncore, the method leverages hyperspherical random embeddings coupled with a\ndual-branch learning module to capture both intra- and inter-channel\nrelationships, further enhanced by a newly formulated optimization problem for\nbuilding rich texture representations. Experimental results have shown the\ninteresting results of the proposed approach across several pure texture\nbenchmarks, each with distinct characteristics and challenges. The source code\nwill be available upon publication.\n", "link": "http://arxiv.org/abs/2510.03228v1", "date": "2025-10-03", "relevancy": 2.6348, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5339}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5308}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIXER%3A%20Mixed%20Hyperspherical%20Random%20Embedding%20Neural%20Network%20for%20Texture%0A%20%20Recognition&body=Title%3A%20MIXER%3A%20Mixed%20Hyperspherical%20Random%20Embedding%20Neural%20Network%20for%20Texture%0A%20%20Recognition%0AAuthor%3A%20Ricardo%20T.%20Fares%20and%20Lucas%20C.%20Ribas%0AAbstract%3A%20%20%20Randomized%20neural%20networks%20for%20representation%20learning%20have%20consistently%0Aachieved%20prominent%20results%20in%20texture%20recognition%20tasks%2C%20effectively%20combining%0Athe%20advantages%20of%20both%20traditional%20techniques%20and%20learning-based%20approaches.%0AHowever%2C%20existing%20approaches%20have%20so%20far%20focused%20mainly%20on%20improving%0Across-information%20prediction%2C%20without%20introducing%20significant%20advancements%20to%0Athe%20overall%20randomized%20network%20architecture.%20In%20this%20paper%2C%20we%20propose%20Mixer%2C%20a%0Anovel%20randomized%20neural%20network%20for%20texture%20representation%20learning.%20At%20its%0Acore%2C%20the%20method%20leverages%20hyperspherical%20random%20embeddings%20coupled%20with%20a%0Adual-branch%20learning%20module%20to%20capture%20both%20intra-%20and%20inter-channel%0Arelationships%2C%20further%20enhanced%20by%20a%20newly%20formulated%20optimization%20problem%20for%0Abuilding%20rich%20texture%20representations.%20Experimental%20results%20have%20shown%20the%0Ainteresting%20results%20of%20the%20proposed%20approach%20across%20several%20pure%20texture%0Abenchmarks%2C%20each%20with%20distinct%20characteristics%20and%20challenges.%20The%20source%20code%0Awill%20be%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIXER%253A%2520Mixed%2520Hyperspherical%2520Random%2520Embedding%2520Neural%2520Network%2520for%2520Texture%250A%2520%2520Recognition%26entry.906535625%3DRicardo%2520T.%2520Fares%2520and%2520Lucas%2520C.%2520Ribas%26entry.1292438233%3D%2520%2520Randomized%2520neural%2520networks%2520for%2520representation%2520learning%2520have%2520consistently%250Aachieved%2520prominent%2520results%2520in%2520texture%2520recognition%2520tasks%252C%2520effectively%2520combining%250Athe%2520advantages%2520of%2520both%2520traditional%2520techniques%2520and%2520learning-based%2520approaches.%250AHowever%252C%2520existing%2520approaches%2520have%2520so%2520far%2520focused%2520mainly%2520on%2520improving%250Across-information%2520prediction%252C%2520without%2520introducing%2520significant%2520advancements%2520to%250Athe%2520overall%2520randomized%2520network%2520architecture.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Mixer%252C%2520a%250Anovel%2520randomized%2520neural%2520network%2520for%2520texture%2520representation%2520learning.%2520At%2520its%250Acore%252C%2520the%2520method%2520leverages%2520hyperspherical%2520random%2520embeddings%2520coupled%2520with%2520a%250Adual-branch%2520learning%2520module%2520to%2520capture%2520both%2520intra-%2520and%2520inter-channel%250Arelationships%252C%2520further%2520enhanced%2520by%2520a%2520newly%2520formulated%2520optimization%2520problem%2520for%250Abuilding%2520rich%2520texture%2520representations.%2520Experimental%2520results%2520have%2520shown%2520the%250Ainteresting%2520results%2520of%2520the%2520proposed%2520approach%2520across%2520several%2520pure%2520texture%250Abenchmarks%252C%2520each%2520with%2520distinct%2520characteristics%2520and%2520challenges.%2520The%2520source%2520code%250Awill%2520be%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIXER%3A%20Mixed%20Hyperspherical%20Random%20Embedding%20Neural%20Network%20for%20Texture%0A%20%20Recognition&entry.906535625=Ricardo%20T.%20Fares%20and%20Lucas%20C.%20Ribas&entry.1292438233=%20%20Randomized%20neural%20networks%20for%20representation%20learning%20have%20consistently%0Aachieved%20prominent%20results%20in%20texture%20recognition%20tasks%2C%20effectively%20combining%0Athe%20advantages%20of%20both%20traditional%20techniques%20and%20learning-based%20approaches.%0AHowever%2C%20existing%20approaches%20have%20so%20far%20focused%20mainly%20on%20improving%0Across-information%20prediction%2C%20without%20introducing%20significant%20advancements%20to%0Athe%20overall%20randomized%20network%20architecture.%20In%20this%20paper%2C%20we%20propose%20Mixer%2C%20a%0Anovel%20randomized%20neural%20network%20for%20texture%20representation%20learning.%20At%20its%0Acore%2C%20the%20method%20leverages%20hyperspherical%20random%20embeddings%20coupled%20with%20a%0Adual-branch%20learning%20module%20to%20capture%20both%20intra-%20and%20inter-channel%0Arelationships%2C%20further%20enhanced%20by%20a%20newly%20formulated%20optimization%20problem%20for%0Abuilding%20rich%20texture%20representations.%20Experimental%20results%20have%20shown%20the%0Ainteresting%20results%20of%20the%20proposed%20approach%20across%20several%20pure%20texture%0Abenchmarks%2C%20each%20with%20distinct%20characteristics%20and%20challenges.%20The%20source%20code%0Awill%20be%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03228v1&entry.124074799=Read"},
{"title": "C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale\n  Recommendation Systems", "author": "Mertcan Cokbas and Ziteng Liu and Zeyi Tao and Elder Veliz and Qin Huang and Ellie Wen and Huayu Li and Qiang Jin and Murat Duman and Benjamin Au and Guy Lebanon and Sagar Chordia and Chengkai Zhang", "abstract": "  Training large-scale recommendation models under a single global objective\nimplicitly assumes homogeneity across user populations. However, real-world\ndata are composites of heterogeneous cohorts with distinct conditional\ndistributions. As models increase in scale and complexity and as more data is\nused for training, they become dominated by central distribution patterns,\nneglecting head and tail regions. This imbalance limits the model's learning\nability and can result in inactive attention weights or dead neurons. In this\npaper, we reveal how the attention mechanism can play a key role in\nfactorization machines for shared embedding selection, and propose to address\nthis challenge by analyzing the substructures in the dataset and exposing those\nwith strong distributional contrast through auxiliary learning. Unlike previous\nresearch, which heuristically applies weighted labels or multi-task heads to\nmitigate such biases, we leverage partially conflicting auxiliary labels to\nregularize the shared representation. This approach customizes the learning\nprocess of attention layers to preserve mutual information with minority\ncohorts while improving global performance. We evaluated C2AL on massive\nproduction datasets with billions of data points each for six SOTA models.\nExperiments show that the factorization machine is able to capture fine-grained\nuser-ad interactions using the proposed method, achieving up to a 0.16%\nreduction in normalized entropy overall and delivering gains exceeding 0.30% on\ntargeted minority cohorts.\n", "link": "http://arxiv.org/abs/2510.02215v2", "date": "2025-10-03", "relevancy": 2.6178, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5222}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C2AL%3A%20Cohort-Contrastive%20Auxiliary%20Learning%20for%20Large-scale%0A%20%20Recommendation%20Systems&body=Title%3A%20C2AL%3A%20Cohort-Contrastive%20Auxiliary%20Learning%20for%20Large-scale%0A%20%20Recommendation%20Systems%0AAuthor%3A%20Mertcan%20Cokbas%20and%20Ziteng%20Liu%20and%20Zeyi%20Tao%20and%20Elder%20Veliz%20and%20Qin%20Huang%20and%20Ellie%20Wen%20and%20Huayu%20Li%20and%20Qiang%20Jin%20and%20Murat%20Duman%20and%20Benjamin%20Au%20and%20Guy%20Lebanon%20and%20Sagar%20Chordia%20and%20Chengkai%20Zhang%0AAbstract%3A%20%20%20Training%20large-scale%20recommendation%20models%20under%20a%20single%20global%20objective%0Aimplicitly%20assumes%20homogeneity%20across%20user%20populations.%20However%2C%20real-world%0Adata%20are%20composites%20of%20heterogeneous%20cohorts%20with%20distinct%20conditional%0Adistributions.%20As%20models%20increase%20in%20scale%20and%20complexity%20and%20as%20more%20data%20is%0Aused%20for%20training%2C%20they%20become%20dominated%20by%20central%20distribution%20patterns%2C%0Aneglecting%20head%20and%20tail%20regions.%20This%20imbalance%20limits%20the%20model%27s%20learning%0Aability%20and%20can%20result%20in%20inactive%20attention%20weights%20or%20dead%20neurons.%20In%20this%0Apaper%2C%20we%20reveal%20how%20the%20attention%20mechanism%20can%20play%20a%20key%20role%20in%0Afactorization%20machines%20for%20shared%20embedding%20selection%2C%20and%20propose%20to%20address%0Athis%20challenge%20by%20analyzing%20the%20substructures%20in%20the%20dataset%20and%20exposing%20those%0Awith%20strong%20distributional%20contrast%20through%20auxiliary%20learning.%20Unlike%20previous%0Aresearch%2C%20which%20heuristically%20applies%20weighted%20labels%20or%20multi-task%20heads%20to%0Amitigate%20such%20biases%2C%20we%20leverage%20partially%20conflicting%20auxiliary%20labels%20to%0Aregularize%20the%20shared%20representation.%20This%20approach%20customizes%20the%20learning%0Aprocess%20of%20attention%20layers%20to%20preserve%20mutual%20information%20with%20minority%0Acohorts%20while%20improving%20global%20performance.%20We%20evaluated%20C2AL%20on%20massive%0Aproduction%20datasets%20with%20billions%20of%20data%20points%20each%20for%20six%20SOTA%20models.%0AExperiments%20show%20that%20the%20factorization%20machine%20is%20able%20to%20capture%20fine-grained%0Auser-ad%20interactions%20using%20the%20proposed%20method%2C%20achieving%20up%20to%20a%200.16%25%0Areduction%20in%20normalized%20entropy%20overall%20and%20delivering%20gains%20exceeding%200.30%25%20on%0Atargeted%20minority%20cohorts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02215v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC2AL%253A%2520Cohort-Contrastive%2520Auxiliary%2520Learning%2520for%2520Large-scale%250A%2520%2520Recommendation%2520Systems%26entry.906535625%3DMertcan%2520Cokbas%2520and%2520Ziteng%2520Liu%2520and%2520Zeyi%2520Tao%2520and%2520Elder%2520Veliz%2520and%2520Qin%2520Huang%2520and%2520Ellie%2520Wen%2520and%2520Huayu%2520Li%2520and%2520Qiang%2520Jin%2520and%2520Murat%2520Duman%2520and%2520Benjamin%2520Au%2520and%2520Guy%2520Lebanon%2520and%2520Sagar%2520Chordia%2520and%2520Chengkai%2520Zhang%26entry.1292438233%3D%2520%2520Training%2520large-scale%2520recommendation%2520models%2520under%2520a%2520single%2520global%2520objective%250Aimplicitly%2520assumes%2520homogeneity%2520across%2520user%2520populations.%2520However%252C%2520real-world%250Adata%2520are%2520composites%2520of%2520heterogeneous%2520cohorts%2520with%2520distinct%2520conditional%250Adistributions.%2520As%2520models%2520increase%2520in%2520scale%2520and%2520complexity%2520and%2520as%2520more%2520data%2520is%250Aused%2520for%2520training%252C%2520they%2520become%2520dominated%2520by%2520central%2520distribution%2520patterns%252C%250Aneglecting%2520head%2520and%2520tail%2520regions.%2520This%2520imbalance%2520limits%2520the%2520model%2527s%2520learning%250Aability%2520and%2520can%2520result%2520in%2520inactive%2520attention%2520weights%2520or%2520dead%2520neurons.%2520In%2520this%250Apaper%252C%2520we%2520reveal%2520how%2520the%2520attention%2520mechanism%2520can%2520play%2520a%2520key%2520role%2520in%250Afactorization%2520machines%2520for%2520shared%2520embedding%2520selection%252C%2520and%2520propose%2520to%2520address%250Athis%2520challenge%2520by%2520analyzing%2520the%2520substructures%2520in%2520the%2520dataset%2520and%2520exposing%2520those%250Awith%2520strong%2520distributional%2520contrast%2520through%2520auxiliary%2520learning.%2520Unlike%2520previous%250Aresearch%252C%2520which%2520heuristically%2520applies%2520weighted%2520labels%2520or%2520multi-task%2520heads%2520to%250Amitigate%2520such%2520biases%252C%2520we%2520leverage%2520partially%2520conflicting%2520auxiliary%2520labels%2520to%250Aregularize%2520the%2520shared%2520representation.%2520This%2520approach%2520customizes%2520the%2520learning%250Aprocess%2520of%2520attention%2520layers%2520to%2520preserve%2520mutual%2520information%2520with%2520minority%250Acohorts%2520while%2520improving%2520global%2520performance.%2520We%2520evaluated%2520C2AL%2520on%2520massive%250Aproduction%2520datasets%2520with%2520billions%2520of%2520data%2520points%2520each%2520for%2520six%2520SOTA%2520models.%250AExperiments%2520show%2520that%2520the%2520factorization%2520machine%2520is%2520able%2520to%2520capture%2520fine-grained%250Auser-ad%2520interactions%2520using%2520the%2520proposed%2520method%252C%2520achieving%2520up%2520to%2520a%25200.16%2525%250Areduction%2520in%2520normalized%2520entropy%2520overall%2520and%2520delivering%2520gains%2520exceeding%25200.30%2525%2520on%250Atargeted%2520minority%2520cohorts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02215v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C2AL%3A%20Cohort-Contrastive%20Auxiliary%20Learning%20for%20Large-scale%0A%20%20Recommendation%20Systems&entry.906535625=Mertcan%20Cokbas%20and%20Ziteng%20Liu%20and%20Zeyi%20Tao%20and%20Elder%20Veliz%20and%20Qin%20Huang%20and%20Ellie%20Wen%20and%20Huayu%20Li%20and%20Qiang%20Jin%20and%20Murat%20Duman%20and%20Benjamin%20Au%20and%20Guy%20Lebanon%20and%20Sagar%20Chordia%20and%20Chengkai%20Zhang&entry.1292438233=%20%20Training%20large-scale%20recommendation%20models%20under%20a%20single%20global%20objective%0Aimplicitly%20assumes%20homogeneity%20across%20user%20populations.%20However%2C%20real-world%0Adata%20are%20composites%20of%20heterogeneous%20cohorts%20with%20distinct%20conditional%0Adistributions.%20As%20models%20increase%20in%20scale%20and%20complexity%20and%20as%20more%20data%20is%0Aused%20for%20training%2C%20they%20become%20dominated%20by%20central%20distribution%20patterns%2C%0Aneglecting%20head%20and%20tail%20regions.%20This%20imbalance%20limits%20the%20model%27s%20learning%0Aability%20and%20can%20result%20in%20inactive%20attention%20weights%20or%20dead%20neurons.%20In%20this%0Apaper%2C%20we%20reveal%20how%20the%20attention%20mechanism%20can%20play%20a%20key%20role%20in%0Afactorization%20machines%20for%20shared%20embedding%20selection%2C%20and%20propose%20to%20address%0Athis%20challenge%20by%20analyzing%20the%20substructures%20in%20the%20dataset%20and%20exposing%20those%0Awith%20strong%20distributional%20contrast%20through%20auxiliary%20learning.%20Unlike%20previous%0Aresearch%2C%20which%20heuristically%20applies%20weighted%20labels%20or%20multi-task%20heads%20to%0Amitigate%20such%20biases%2C%20we%20leverage%20partially%20conflicting%20auxiliary%20labels%20to%0Aregularize%20the%20shared%20representation.%20This%20approach%20customizes%20the%20learning%0Aprocess%20of%20attention%20layers%20to%20preserve%20mutual%20information%20with%20minority%0Acohorts%20while%20improving%20global%20performance.%20We%20evaluated%20C2AL%20on%20massive%0Aproduction%20datasets%20with%20billions%20of%20data%20points%20each%20for%20six%20SOTA%20models.%0AExperiments%20show%20that%20the%20factorization%20machine%20is%20able%20to%20capture%20fine-grained%0Auser-ad%20interactions%20using%20the%20proposed%20method%2C%20achieving%20up%20to%20a%200.16%25%0Areduction%20in%20normalized%20entropy%20overall%20and%20delivering%20gains%20exceeding%200.30%25%20on%0Atargeted%20minority%20cohorts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02215v2&entry.124074799=Read"},
{"title": "Zero-Shot Robustness of Vision Language Models Via Confidence-Aware\n  Weighting", "author": "Nikoo Naghavian and Mostafa Tavassolipour", "abstract": "  Vision-language models like CLIP demonstrate impressive zero-shot\ngeneralization but remain highly vulnerable to adversarial attacks. In this\nwork, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot\nrobustness in vision-language models. CAW consists of two components: (1) a\nConfidence-Aware loss that prioritizes uncertain adversarial examples by\nscaling the KL divergence between clean and adversarial predictions, and (2) a\nfeature alignment regularization that preserves semantic consistency by\nminimizing the distance between frozen and fine-tuned image encoder features on\nadversarial inputs. These components work jointly to improve both clean and\nrobust accuracy without sacrificing generalization. Extensive experiments on\nTinyImageNet and 14 additional datasets show that CAW outperforms recent\nmethods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while\nusing less memory.\n", "link": "http://arxiv.org/abs/2510.02913v1", "date": "2025-10-03", "relevancy": 2.6177, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5365}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5187}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Robustness%20of%20Vision%20Language%20Models%20Via%20Confidence-Aware%0A%20%20Weighting&body=Title%3A%20Zero-Shot%20Robustness%20of%20Vision%20Language%20Models%20Via%20Confidence-Aware%0A%20%20Weighting%0AAuthor%3A%20Nikoo%20Naghavian%20and%20Mostafa%20Tavassolipour%0AAbstract%3A%20%20%20Vision-language%20models%20like%20CLIP%20demonstrate%20impressive%20zero-shot%0Ageneralization%20but%20remain%20highly%20vulnerable%20to%20adversarial%20attacks.%20In%20this%0Awork%2C%20we%20propose%20Confidence-Aware%20Weighting%20%28CAW%29%20to%20enhance%20zero-shot%0Arobustness%20in%20vision-language%20models.%20CAW%20consists%20of%20two%20components%3A%20%281%29%20a%0AConfidence-Aware%20loss%20that%20prioritizes%20uncertain%20adversarial%20examples%20by%0Ascaling%20the%20KL%20divergence%20between%20clean%20and%20adversarial%20predictions%2C%20and%20%282%29%20a%0Afeature%20alignment%20regularization%20that%20preserves%20semantic%20consistency%20by%0Aminimizing%20the%20distance%20between%20frozen%20and%20fine-tuned%20image%20encoder%20features%20on%0Aadversarial%20inputs.%20These%20components%20work%20jointly%20to%20improve%20both%20clean%20and%0Arobust%20accuracy%20without%20sacrificing%20generalization.%20Extensive%20experiments%20on%0ATinyImageNet%20and%2014%20additional%20datasets%20show%20that%20CAW%20outperforms%20recent%0Amethods%20such%20as%20PMG-AFT%20and%20TGA-ZSR%20under%20strong%20attacks%20like%20AutoAttack%2C%20while%0Ausing%20less%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Robustness%2520of%2520Vision%2520Language%2520Models%2520Via%2520Confidence-Aware%250A%2520%2520Weighting%26entry.906535625%3DNikoo%2520Naghavian%2520and%2520Mostafa%2520Tavassolipour%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520like%2520CLIP%2520demonstrate%2520impressive%2520zero-shot%250Ageneralization%2520but%2520remain%2520highly%2520vulnerable%2520to%2520adversarial%2520attacks.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Confidence-Aware%2520Weighting%2520%2528CAW%2529%2520to%2520enhance%2520zero-shot%250Arobustness%2520in%2520vision-language%2520models.%2520CAW%2520consists%2520of%2520two%2520components%253A%2520%25281%2529%2520a%250AConfidence-Aware%2520loss%2520that%2520prioritizes%2520uncertain%2520adversarial%2520examples%2520by%250Ascaling%2520the%2520KL%2520divergence%2520between%2520clean%2520and%2520adversarial%2520predictions%252C%2520and%2520%25282%2529%2520a%250Afeature%2520alignment%2520regularization%2520that%2520preserves%2520semantic%2520consistency%2520by%250Aminimizing%2520the%2520distance%2520between%2520frozen%2520and%2520fine-tuned%2520image%2520encoder%2520features%2520on%250Aadversarial%2520inputs.%2520These%2520components%2520work%2520jointly%2520to%2520improve%2520both%2520clean%2520and%250Arobust%2520accuracy%2520without%2520sacrificing%2520generalization.%2520Extensive%2520experiments%2520on%250ATinyImageNet%2520and%252014%2520additional%2520datasets%2520show%2520that%2520CAW%2520outperforms%2520recent%250Amethods%2520such%2520as%2520PMG-AFT%2520and%2520TGA-ZSR%2520under%2520strong%2520attacks%2520like%2520AutoAttack%252C%2520while%250Ausing%2520less%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Robustness%20of%20Vision%20Language%20Models%20Via%20Confidence-Aware%0A%20%20Weighting&entry.906535625=Nikoo%20Naghavian%20and%20Mostafa%20Tavassolipour&entry.1292438233=%20%20Vision-language%20models%20like%20CLIP%20demonstrate%20impressive%20zero-shot%0Ageneralization%20but%20remain%20highly%20vulnerable%20to%20adversarial%20attacks.%20In%20this%0Awork%2C%20we%20propose%20Confidence-Aware%20Weighting%20%28CAW%29%20to%20enhance%20zero-shot%0Arobustness%20in%20vision-language%20models.%20CAW%20consists%20of%20two%20components%3A%20%281%29%20a%0AConfidence-Aware%20loss%20that%20prioritizes%20uncertain%20adversarial%20examples%20by%0Ascaling%20the%20KL%20divergence%20between%20clean%20and%20adversarial%20predictions%2C%20and%20%282%29%20a%0Afeature%20alignment%20regularization%20that%20preserves%20semantic%20consistency%20by%0Aminimizing%20the%20distance%20between%20frozen%20and%20fine-tuned%20image%20encoder%20features%20on%0Aadversarial%20inputs.%20These%20components%20work%20jointly%20to%20improve%20both%20clean%20and%0Arobust%20accuracy%20without%20sacrificing%20generalization.%20Extensive%20experiments%20on%0ATinyImageNet%20and%2014%20additional%20datasets%20show%20that%20CAW%20outperforms%20recent%0Amethods%20such%20as%20PMG-AFT%20and%20TGA-ZSR%20under%20strong%20attacks%20like%20AutoAttack%2C%20while%0Ausing%20less%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02913v1&entry.124074799=Read"},
{"title": "Mask2IV: Interaction-Centric Video Generation via Mask Trajectories", "author": "Gen Li and Bo Zhao and Jianfei Yang and Laura Sevilla-Lara", "abstract": "  Generating interaction-centric videos, such as those depicting humans or\nrobots interacting with objects, is crucial for embodied intelligence, as they\nprovide rich and diverse visual priors for robot learning, manipulation policy\ntraining, and affordance reasoning. However, existing methods often struggle to\nmodel such complex and dynamic interactions. While recent studies show that\nmasks can serve as effective control signals and enhance generation quality,\nobtaining dense and precise mask annotations remains a major challenge for\nreal-world use. To overcome this limitation, we introduce Mask2IV, a novel\nframework specifically designed for interaction-centric video generation. It\nadopts a decoupled two-stage pipeline that first predicts plausible motion\ntrajectories for both actor and object, then generates a video conditioned on\nthese trajectories. This design eliminates the need for dense mask inputs from\nusers while preserving the flexibility to manipulate the interaction process.\nFurthermore, Mask2IV supports versatile and intuitive control, allowing users\nto specify the target object of interaction and guide the motion trajectory\nthrough action descriptions or spatial position cues. To support systematic\ntraining and evaluation, we curate two benchmarks covering diverse action and\nobject categories across both human-object interaction and robotic manipulation\nscenarios. Extensive experiments demonstrate that our method achieves superior\nvisual realism and controllability compared to existing baselines.\n", "link": "http://arxiv.org/abs/2510.03135v1", "date": "2025-10-03", "relevancy": 2.5903, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6557}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6458}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask2IV%3A%20Interaction-Centric%20Video%20Generation%20via%20Mask%20Trajectories&body=Title%3A%20Mask2IV%3A%20Interaction-Centric%20Video%20Generation%20via%20Mask%20Trajectories%0AAuthor%3A%20Gen%20Li%20and%20Bo%20Zhao%20and%20Jianfei%20Yang%20and%20Laura%20Sevilla-Lara%0AAbstract%3A%20%20%20Generating%20interaction-centric%20videos%2C%20such%20as%20those%20depicting%20humans%20or%0Arobots%20interacting%20with%20objects%2C%20is%20crucial%20for%20embodied%20intelligence%2C%20as%20they%0Aprovide%20rich%20and%20diverse%20visual%20priors%20for%20robot%20learning%2C%20manipulation%20policy%0Atraining%2C%20and%20affordance%20reasoning.%20However%2C%20existing%20methods%20often%20struggle%20to%0Amodel%20such%20complex%20and%20dynamic%20interactions.%20While%20recent%20studies%20show%20that%0Amasks%20can%20serve%20as%20effective%20control%20signals%20and%20enhance%20generation%20quality%2C%0Aobtaining%20dense%20and%20precise%20mask%20annotations%20remains%20a%20major%20challenge%20for%0Areal-world%20use.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20Mask2IV%2C%20a%20novel%0Aframework%20specifically%20designed%20for%20interaction-centric%20video%20generation.%20It%0Aadopts%20a%20decoupled%20two-stage%20pipeline%20that%20first%20predicts%20plausible%20motion%0Atrajectories%20for%20both%20actor%20and%20object%2C%20then%20generates%20a%20video%20conditioned%20on%0Athese%20trajectories.%20This%20design%20eliminates%20the%20need%20for%20dense%20mask%20inputs%20from%0Ausers%20while%20preserving%20the%20flexibility%20to%20manipulate%20the%20interaction%20process.%0AFurthermore%2C%20Mask2IV%20supports%20versatile%20and%20intuitive%20control%2C%20allowing%20users%0Ato%20specify%20the%20target%20object%20of%20interaction%20and%20guide%20the%20motion%20trajectory%0Athrough%20action%20descriptions%20or%20spatial%20position%20cues.%20To%20support%20systematic%0Atraining%20and%20evaluation%2C%20we%20curate%20two%20benchmarks%20covering%20diverse%20action%20and%0Aobject%20categories%20across%20both%20human-object%20interaction%20and%20robotic%20manipulation%0Ascenarios.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20superior%0Avisual%20realism%20and%20controllability%20compared%20to%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask2IV%253A%2520Interaction-Centric%2520Video%2520Generation%2520via%2520Mask%2520Trajectories%26entry.906535625%3DGen%2520Li%2520and%2520Bo%2520Zhao%2520and%2520Jianfei%2520Yang%2520and%2520Laura%2520Sevilla-Lara%26entry.1292438233%3D%2520%2520Generating%2520interaction-centric%2520videos%252C%2520such%2520as%2520those%2520depicting%2520humans%2520or%250Arobots%2520interacting%2520with%2520objects%252C%2520is%2520crucial%2520for%2520embodied%2520intelligence%252C%2520as%2520they%250Aprovide%2520rich%2520and%2520diverse%2520visual%2520priors%2520for%2520robot%2520learning%252C%2520manipulation%2520policy%250Atraining%252C%2520and%2520affordance%2520reasoning.%2520However%252C%2520existing%2520methods%2520often%2520struggle%2520to%250Amodel%2520such%2520complex%2520and%2520dynamic%2520interactions.%2520While%2520recent%2520studies%2520show%2520that%250Amasks%2520can%2520serve%2520as%2520effective%2520control%2520signals%2520and%2520enhance%2520generation%2520quality%252C%250Aobtaining%2520dense%2520and%2520precise%2520mask%2520annotations%2520remains%2520a%2520major%2520challenge%2520for%250Areal-world%2520use.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520Mask2IV%252C%2520a%2520novel%250Aframework%2520specifically%2520designed%2520for%2520interaction-centric%2520video%2520generation.%2520It%250Aadopts%2520a%2520decoupled%2520two-stage%2520pipeline%2520that%2520first%2520predicts%2520plausible%2520motion%250Atrajectories%2520for%2520both%2520actor%2520and%2520object%252C%2520then%2520generates%2520a%2520video%2520conditioned%2520on%250Athese%2520trajectories.%2520This%2520design%2520eliminates%2520the%2520need%2520for%2520dense%2520mask%2520inputs%2520from%250Ausers%2520while%2520preserving%2520the%2520flexibility%2520to%2520manipulate%2520the%2520interaction%2520process.%250AFurthermore%252C%2520Mask2IV%2520supports%2520versatile%2520and%2520intuitive%2520control%252C%2520allowing%2520users%250Ato%2520specify%2520the%2520target%2520object%2520of%2520interaction%2520and%2520guide%2520the%2520motion%2520trajectory%250Athrough%2520action%2520descriptions%2520or%2520spatial%2520position%2520cues.%2520To%2520support%2520systematic%250Atraining%2520and%2520evaluation%252C%2520we%2520curate%2520two%2520benchmarks%2520covering%2520diverse%2520action%2520and%250Aobject%2520categories%2520across%2520both%2520human-object%2520interaction%2520and%2520robotic%2520manipulation%250Ascenarios.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%250Avisual%2520realism%2520and%2520controllability%2520compared%2520to%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask2IV%3A%20Interaction-Centric%20Video%20Generation%20via%20Mask%20Trajectories&entry.906535625=Gen%20Li%20and%20Bo%20Zhao%20and%20Jianfei%20Yang%20and%20Laura%20Sevilla-Lara&entry.1292438233=%20%20Generating%20interaction-centric%20videos%2C%20such%20as%20those%20depicting%20humans%20or%0Arobots%20interacting%20with%20objects%2C%20is%20crucial%20for%20embodied%20intelligence%2C%20as%20they%0Aprovide%20rich%20and%20diverse%20visual%20priors%20for%20robot%20learning%2C%20manipulation%20policy%0Atraining%2C%20and%20affordance%20reasoning.%20However%2C%20existing%20methods%20often%20struggle%20to%0Amodel%20such%20complex%20and%20dynamic%20interactions.%20While%20recent%20studies%20show%20that%0Amasks%20can%20serve%20as%20effective%20control%20signals%20and%20enhance%20generation%20quality%2C%0Aobtaining%20dense%20and%20precise%20mask%20annotations%20remains%20a%20major%20challenge%20for%0Areal-world%20use.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20Mask2IV%2C%20a%20novel%0Aframework%20specifically%20designed%20for%20interaction-centric%20video%20generation.%20It%0Aadopts%20a%20decoupled%20two-stage%20pipeline%20that%20first%20predicts%20plausible%20motion%0Atrajectories%20for%20both%20actor%20and%20object%2C%20then%20generates%20a%20video%20conditioned%20on%0Athese%20trajectories.%20This%20design%20eliminates%20the%20need%20for%20dense%20mask%20inputs%20from%0Ausers%20while%20preserving%20the%20flexibility%20to%20manipulate%20the%20interaction%20process.%0AFurthermore%2C%20Mask2IV%20supports%20versatile%20and%20intuitive%20control%2C%20allowing%20users%0Ato%20specify%20the%20target%20object%20of%20interaction%20and%20guide%20the%20motion%20trajectory%0Athrough%20action%20descriptions%20or%20spatial%20position%20cues.%20To%20support%20systematic%0Atraining%20and%20evaluation%2C%20we%20curate%20two%20benchmarks%20covering%20diverse%20action%20and%0Aobject%20categories%20across%20both%20human-object%20interaction%20and%20robotic%20manipulation%0Ascenarios.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20superior%0Avisual%20realism%20and%20controllability%20compared%20to%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03135v1&entry.124074799=Read"},
{"title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector\n  Refinement", "author": "Anyi Wang and Xuansheng Wu and Dong Shu and Yunpu Ma and Ninghao Liu", "abstract": "  Steering has emerged as a promising approach in controlling large language\nmodels (LLMs) without modifying model parameters. However, most existing\nsteering methods rely on large-scale datasets to learn clear behavioral\ninformation, which limits their applicability in many real-world scenarios. The\nsteering vectors extracted from small dataset often contain task-irrelevant\nnoising features, which degrades their effectiveness. To refine the steering\nvectors learned from limited data, we introduce Refinement of Steering Vector\nvia Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise\nand augment the steering vectors. In our framework, we first remove\ntask-irrelevant features according to their semantics provided by SAEs, and\nthen enrich task-relevant features missing from the small dataset through their\nsemantic similarity to the identified relevant features. Extensive experiments\ndemonstrate that the proposed SAE-RSV substantially outperforms all the\nbaseline methods including supervised fine-tuning. Our findings show that\neffective steering vector can be constructed from limited training data by\nrefining the original steering vector through SAEs.\n", "link": "http://arxiv.org/abs/2509.23799v2", "date": "2025-10-03", "relevancy": 2.5555, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20LLM%20Steering%20through%20Sparse%20Autoencoder-Based%20Vector%0A%20%20Refinement&body=Title%3A%20Enhancing%20LLM%20Steering%20through%20Sparse%20Autoencoder-Based%20Vector%0A%20%20Refinement%0AAuthor%3A%20Anyi%20Wang%20and%20Xuansheng%20Wu%20and%20Dong%20Shu%20and%20Yunpu%20Ma%20and%20Ninghao%20Liu%0AAbstract%3A%20%20%20Steering%20has%20emerged%20as%20a%20promising%20approach%20in%20controlling%20large%20language%0Amodels%20%28LLMs%29%20without%20modifying%20model%20parameters.%20However%2C%20most%20existing%0Asteering%20methods%20rely%20on%20large-scale%20datasets%20to%20learn%20clear%20behavioral%0Ainformation%2C%20which%20limits%20their%20applicability%20in%20many%20real-world%20scenarios.%20The%0Asteering%20vectors%20extracted%20from%20small%20dataset%20often%20contain%20task-irrelevant%0Anoising%20features%2C%20which%20degrades%20their%20effectiveness.%20To%20refine%20the%20steering%0Avectors%20learned%20from%20limited%20data%2C%20we%20introduce%20Refinement%20of%20Steering%20Vector%0Avia%20Sparse%20Autoencoder%20%28SAE-RSV%29%20that%20leverages%20SAEs%20to%20semantically%20denoise%0Aand%20augment%20the%20steering%20vectors.%20In%20our%20framework%2C%20we%20first%20remove%0Atask-irrelevant%20features%20according%20to%20their%20semantics%20provided%20by%20SAEs%2C%20and%0Athen%20enrich%20task-relevant%20features%20missing%20from%20the%20small%20dataset%20through%20their%0Asemantic%20similarity%20to%20the%20identified%20relevant%20features.%20Extensive%20experiments%0Ademonstrate%20that%20the%20proposed%20SAE-RSV%20substantially%20outperforms%20all%20the%0Abaseline%20methods%20including%20supervised%20fine-tuning.%20Our%20findings%20show%20that%0Aeffective%20steering%20vector%20can%20be%20constructed%20from%20limited%20training%20data%20by%0Arefining%20the%20original%20steering%20vector%20through%20SAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520LLM%2520Steering%2520through%2520Sparse%2520Autoencoder-Based%2520Vector%250A%2520%2520Refinement%26entry.906535625%3DAnyi%2520Wang%2520and%2520Xuansheng%2520Wu%2520and%2520Dong%2520Shu%2520and%2520Yunpu%2520Ma%2520and%2520Ninghao%2520Liu%26entry.1292438233%3D%2520%2520Steering%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520in%2520controlling%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520without%2520modifying%2520model%2520parameters.%2520However%252C%2520most%2520existing%250Asteering%2520methods%2520rely%2520on%2520large-scale%2520datasets%2520to%2520learn%2520clear%2520behavioral%250Ainformation%252C%2520which%2520limits%2520their%2520applicability%2520in%2520many%2520real-world%2520scenarios.%2520The%250Asteering%2520vectors%2520extracted%2520from%2520small%2520dataset%2520often%2520contain%2520task-irrelevant%250Anoising%2520features%252C%2520which%2520degrades%2520their%2520effectiveness.%2520To%2520refine%2520the%2520steering%250Avectors%2520learned%2520from%2520limited%2520data%252C%2520we%2520introduce%2520Refinement%2520of%2520Steering%2520Vector%250Avia%2520Sparse%2520Autoencoder%2520%2528SAE-RSV%2529%2520that%2520leverages%2520SAEs%2520to%2520semantically%2520denoise%250Aand%2520augment%2520the%2520steering%2520vectors.%2520In%2520our%2520framework%252C%2520we%2520first%2520remove%250Atask-irrelevant%2520features%2520according%2520to%2520their%2520semantics%2520provided%2520by%2520SAEs%252C%2520and%250Athen%2520enrich%2520task-relevant%2520features%2520missing%2520from%2520the%2520small%2520dataset%2520through%2520their%250Asemantic%2520similarity%2520to%2520the%2520identified%2520relevant%2520features.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520the%2520proposed%2520SAE-RSV%2520substantially%2520outperforms%2520all%2520the%250Abaseline%2520methods%2520including%2520supervised%2520fine-tuning.%2520Our%2520findings%2520show%2520that%250Aeffective%2520steering%2520vector%2520can%2520be%2520constructed%2520from%2520limited%2520training%2520data%2520by%250Arefining%2520the%2520original%2520steering%2520vector%2520through%2520SAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20LLM%20Steering%20through%20Sparse%20Autoencoder-Based%20Vector%0A%20%20Refinement&entry.906535625=Anyi%20Wang%20and%20Xuansheng%20Wu%20and%20Dong%20Shu%20and%20Yunpu%20Ma%20and%20Ninghao%20Liu&entry.1292438233=%20%20Steering%20has%20emerged%20as%20a%20promising%20approach%20in%20controlling%20large%20language%0Amodels%20%28LLMs%29%20without%20modifying%20model%20parameters.%20However%2C%20most%20existing%0Asteering%20methods%20rely%20on%20large-scale%20datasets%20to%20learn%20clear%20behavioral%0Ainformation%2C%20which%20limits%20their%20applicability%20in%20many%20real-world%20scenarios.%20The%0Asteering%20vectors%20extracted%20from%20small%20dataset%20often%20contain%20task-irrelevant%0Anoising%20features%2C%20which%20degrades%20their%20effectiveness.%20To%20refine%20the%20steering%0Avectors%20learned%20from%20limited%20data%2C%20we%20introduce%20Refinement%20of%20Steering%20Vector%0Avia%20Sparse%20Autoencoder%20%28SAE-RSV%29%20that%20leverages%20SAEs%20to%20semantically%20denoise%0Aand%20augment%20the%20steering%20vectors.%20In%20our%20framework%2C%20we%20first%20remove%0Atask-irrelevant%20features%20according%20to%20their%20semantics%20provided%20by%20SAEs%2C%20and%0Athen%20enrich%20task-relevant%20features%20missing%20from%20the%20small%20dataset%20through%20their%0Asemantic%20similarity%20to%20the%20identified%20relevant%20features.%20Extensive%20experiments%0Ademonstrate%20that%20the%20proposed%20SAE-RSV%20substantially%20outperforms%20all%20the%0Abaseline%20methods%20including%20supervised%20fine-tuning.%20Our%20findings%20show%20that%0Aeffective%20steering%20vector%20can%20be%20constructed%20from%20limited%20training%20data%20by%0Arefining%20the%20original%20steering%20vector%20through%20SAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23799v2&entry.124074799=Read"},
{"title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers", "author": "Jingze Zhu and Yongliang Wu and Wenbo Zhu and Jiawang Cao and Yanqiang Zheng and Jiawei Chen and Xu Yang and Bernt Schiele and Jonas Fischer and Xinting Hu", "abstract": "  Large language models (LLMs) excel at natural language understanding and\ngeneration but remain vulnerable to factual errors, limiting their reliability\nin knowledge-intensive tasks. While decoding-time strategies provide a\npromising efficient solution without training, existing methods typically treat\ntoken-level and layer-level signals in isolation, overlooking the joint\ndynamics between them. In this work, we introduce a token-aware,\nlayer-localized contrastive decoding method that aligns specific token types\nwith their most influential transformer layers to improve factual generation.\nThrough empirical attention analysis, we identify two key patterns: punctuation\ntokens receive dominant attention in early layers, while conceptual tokens\ngovern semantic reasoning in intermediate layers. By selectively suppressing\nattention to these token types at their respective depths, we achieve the\ninduction of controlled factual degradation and derive contrastive signals to\nguide the final factual decoding. Our method requires no additional training or\nmodel modification, and experiments demonstrate that our method consistently\nimproves factuality across multiple LLMs and various benchmarks.\n", "link": "http://arxiv.org/abs/2507.04404v2", "date": "2025-10-03", "relevancy": 2.5501, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerCake%3A%20Token-Aware%20Contrastive%20Decoding%20within%20Large%20Language%20Model%0A%20%20Layers&body=Title%3A%20LayerCake%3A%20Token-Aware%20Contrastive%20Decoding%20within%20Large%20Language%20Model%0A%20%20Layers%0AAuthor%3A%20Jingze%20Zhu%20and%20Yongliang%20Wu%20and%20Wenbo%20Zhu%20and%20Jiawang%20Cao%20and%20Yanqiang%20Zheng%20and%20Jiawei%20Chen%20and%20Xu%20Yang%20and%20Bernt%20Schiele%20and%20Jonas%20Fischer%20and%20Xinting%20Hu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20natural%20language%20understanding%20and%0Ageneration%20but%20remain%20vulnerable%20to%20factual%20errors%2C%20limiting%20their%20reliability%0Ain%20knowledge-intensive%20tasks.%20While%20decoding-time%20strategies%20provide%20a%0Apromising%20efficient%20solution%20without%20training%2C%20existing%20methods%20typically%20treat%0Atoken-level%20and%20layer-level%20signals%20in%20isolation%2C%20overlooking%20the%20joint%0Adynamics%20between%20them.%20In%20this%20work%2C%20we%20introduce%20a%20token-aware%2C%0Alayer-localized%20contrastive%20decoding%20method%20that%20aligns%20specific%20token%20types%0Awith%20their%20most%20influential%20transformer%20layers%20to%20improve%20factual%20generation.%0AThrough%20empirical%20attention%20analysis%2C%20we%20identify%20two%20key%20patterns%3A%20punctuation%0Atokens%20receive%20dominant%20attention%20in%20early%20layers%2C%20while%20conceptual%20tokens%0Agovern%20semantic%20reasoning%20in%20intermediate%20layers.%20By%20selectively%20suppressing%0Aattention%20to%20these%20token%20types%20at%20their%20respective%20depths%2C%20we%20achieve%20the%0Ainduction%20of%20controlled%20factual%20degradation%20and%20derive%20contrastive%20signals%20to%0Aguide%20the%20final%20factual%20decoding.%20Our%20method%20requires%20no%20additional%20training%20or%0Amodel%20modification%2C%20and%20experiments%20demonstrate%20that%20our%20method%20consistently%0Aimproves%20factuality%20across%20multiple%20LLMs%20and%20various%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerCake%253A%2520Token-Aware%2520Contrastive%2520Decoding%2520within%2520Large%2520Language%2520Model%250A%2520%2520Layers%26entry.906535625%3DJingze%2520Zhu%2520and%2520Yongliang%2520Wu%2520and%2520Wenbo%2520Zhu%2520and%2520Jiawang%2520Cao%2520and%2520Yanqiang%2520Zheng%2520and%2520Jiawei%2520Chen%2520and%2520Xu%2520Yang%2520and%2520Bernt%2520Schiele%2520and%2520Jonas%2520Fischer%2520and%2520Xinting%2520Hu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520natural%2520language%2520understanding%2520and%250Ageneration%2520but%2520remain%2520vulnerable%2520to%2520factual%2520errors%252C%2520limiting%2520their%2520reliability%250Ain%2520knowledge-intensive%2520tasks.%2520While%2520decoding-time%2520strategies%2520provide%2520a%250Apromising%2520efficient%2520solution%2520without%2520training%252C%2520existing%2520methods%2520typically%2520treat%250Atoken-level%2520and%2520layer-level%2520signals%2520in%2520isolation%252C%2520overlooking%2520the%2520joint%250Adynamics%2520between%2520them.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520token-aware%252C%250Alayer-localized%2520contrastive%2520decoding%2520method%2520that%2520aligns%2520specific%2520token%2520types%250Awith%2520their%2520most%2520influential%2520transformer%2520layers%2520to%2520improve%2520factual%2520generation.%250AThrough%2520empirical%2520attention%2520analysis%252C%2520we%2520identify%2520two%2520key%2520patterns%253A%2520punctuation%250Atokens%2520receive%2520dominant%2520attention%2520in%2520early%2520layers%252C%2520while%2520conceptual%2520tokens%250Agovern%2520semantic%2520reasoning%2520in%2520intermediate%2520layers.%2520By%2520selectively%2520suppressing%250Aattention%2520to%2520these%2520token%2520types%2520at%2520their%2520respective%2520depths%252C%2520we%2520achieve%2520the%250Ainduction%2520of%2520controlled%2520factual%2520degradation%2520and%2520derive%2520contrastive%2520signals%2520to%250Aguide%2520the%2520final%2520factual%2520decoding.%2520Our%2520method%2520requires%2520no%2520additional%2520training%2520or%250Amodel%2520modification%252C%2520and%2520experiments%2520demonstrate%2520that%2520our%2520method%2520consistently%250Aimproves%2520factuality%2520across%2520multiple%2520LLMs%2520and%2520various%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerCake%3A%20Token-Aware%20Contrastive%20Decoding%20within%20Large%20Language%20Model%0A%20%20Layers&entry.906535625=Jingze%20Zhu%20and%20Yongliang%20Wu%20and%20Wenbo%20Zhu%20and%20Jiawang%20Cao%20and%20Yanqiang%20Zheng%20and%20Jiawei%20Chen%20and%20Xu%20Yang%20and%20Bernt%20Schiele%20and%20Jonas%20Fischer%20and%20Xinting%20Hu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20natural%20language%20understanding%20and%0Ageneration%20but%20remain%20vulnerable%20to%20factual%20errors%2C%20limiting%20their%20reliability%0Ain%20knowledge-intensive%20tasks.%20While%20decoding-time%20strategies%20provide%20a%0Apromising%20efficient%20solution%20without%20training%2C%20existing%20methods%20typically%20treat%0Atoken-level%20and%20layer-level%20signals%20in%20isolation%2C%20overlooking%20the%20joint%0Adynamics%20between%20them.%20In%20this%20work%2C%20we%20introduce%20a%20token-aware%2C%0Alayer-localized%20contrastive%20decoding%20method%20that%20aligns%20specific%20token%20types%0Awith%20their%20most%20influential%20transformer%20layers%20to%20improve%20factual%20generation.%0AThrough%20empirical%20attention%20analysis%2C%20we%20identify%20two%20key%20patterns%3A%20punctuation%0Atokens%20receive%20dominant%20attention%20in%20early%20layers%2C%20while%20conceptual%20tokens%0Agovern%20semantic%20reasoning%20in%20intermediate%20layers.%20By%20selectively%20suppressing%0Aattention%20to%20these%20token%20types%20at%20their%20respective%20depths%2C%20we%20achieve%20the%0Ainduction%20of%20controlled%20factual%20degradation%20and%20derive%20contrastive%20signals%20to%0Aguide%20the%20final%20factual%20decoding.%20Our%20method%20requires%20no%20additional%20training%20or%0Amodel%20modification%2C%20and%20experiments%20demonstrate%20that%20our%20method%20consistently%0Aimproves%20factuality%20across%20multiple%20LLMs%20and%20various%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04404v2&entry.124074799=Read"},
{"title": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation", "author": "Yaoyu Zhu and Di Huang and Hanqi Lyu and Xiaoyun Zhang and Chongxiao Li and Wenxuan Shi and Yutong Wu and Jianan Mu and Jinghua Wang and Yang Zhao and Pengwei Jin and Shuyao Cheng and Shengwen Liang and Xishan Zhang and Rui Zhang and Zidong Du and Qi Guo and Xing Hu and Yunji Chen", "abstract": "  Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while even exceeding the\nperformance of 671B DeepSeek-R1 on RTLLM. We have released our model, training\ncode, and dataset to facilitate research in EDA and LLM communities.\n", "link": "http://arxiv.org/abs/2505.24183v3", "date": "2025-10-03", "relevancy": 2.5476, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QiMeng-CodeV-R1%3A%20Reasoning-Enhanced%20Verilog%20Generation&body=Title%3A%20QiMeng-CodeV-R1%3A%20Reasoning-Enhanced%20Verilog%20Generation%0AAuthor%3A%20Yaoyu%20Zhu%20and%20Di%20Huang%20and%20Hanqi%20Lyu%20and%20Xiaoyun%20Zhang%20and%20Chongxiao%20Li%20and%20Wenxuan%20Shi%20and%20Yutong%20Wu%20and%20Jianan%20Mu%20and%20Jinghua%20Wang%20and%20Yang%20Zhao%20and%20Pengwei%20Jin%20and%20Shuyao%20Cheng%20and%20Shengwen%20Liang%20and%20Xishan%20Zhang%20and%20Rui%20Zhang%20and%20Zidong%20Du%20and%20Qi%20Guo%20and%20Xing%20Hu%20and%20Yunji%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20trained%20via%20reinforcement%20learning%20with%0Averifiable%20reward%20%28RLVR%29%20have%20achieved%20breakthroughs%20on%20tasks%20with%20explicit%2C%0Aautomatable%20verification%2C%20such%20as%20software%20programming%20and%20mathematical%0Aproblems.%20Extending%20RLVR%20to%20electronic%20design%20automation%20%28EDA%29%2C%20especially%0Aautomatically%20generating%20hardware%20description%20languages%20%28HDLs%29%20like%20Verilog%0Afrom%20natural-language%20%28NL%29%20specifications%2C%20however%2C%20poses%20three%20key%20challenges%3A%0Athe%20lack%20of%20automated%20and%20accurate%20verification%20environments%2C%20the%20scarcity%20of%0Ahigh-quality%20NL-code%20pairs%2C%20and%20the%20prohibitive%20computation%20cost%20of%20RLVR.%20To%0Athis%20end%2C%20we%20introduce%20CodeV-R1%2C%20an%20RLVR%20framework%20for%20training%20Verilog%0Ageneration%20LLMs.%20First%2C%20we%20develop%20a%20rule-based%20testbench%20generator%20that%0Aperforms%20robust%20equivalence%20checking%20against%20golden%20references.%20Second%2C%20we%0Apropose%20a%20round-trip%20data%20synthesis%20method%20that%20pairs%20open-source%20Verilog%0Asnippets%20with%20LLM-generated%20NL%20descriptions%2C%20verifies%20code-NL-code%20consistency%0Avia%20the%20generated%20testbench%2C%20and%20filters%20out%20inequivalent%20examples%20to%20yield%20a%0Ahigh-quality%20dataset.%20Third%2C%20we%20employ%20a%20two-stage%20%22distill-then-RL%22%20training%0Apipeline%3A%20distillation%20for%20the%20cold%20start%20of%20reasoning%20abilities%2C%20followed%20by%0Aadaptive%20DAPO%2C%20our%20novel%20RLVR%20algorithm%20that%20can%20reduce%20training%20cost%20by%0Aadaptively%20adjusting%20sampling%20rate.%20The%20resulting%20model%2C%20CodeV-R1-7B%2C%20achieves%0A68.6%25%20and%2072.9%25%20pass%401%20on%20VerilogEval%20v2%20and%20RTLLM%20v1.1%2C%20respectively%2C%0Asurpassing%20prior%20state-of-the-art%20by%2012~20%25%2C%20while%20even%20exceeding%20the%0Aperformance%20of%20671B%20DeepSeek-R1%20on%20RTLLM.%20We%20have%20released%20our%20model%2C%20training%0Acode%2C%20and%20dataset%20to%20facilitate%20research%20in%20EDA%20and%20LLM%20communities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24183v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQiMeng-CodeV-R1%253A%2520Reasoning-Enhanced%2520Verilog%2520Generation%26entry.906535625%3DYaoyu%2520Zhu%2520and%2520Di%2520Huang%2520and%2520Hanqi%2520Lyu%2520and%2520Xiaoyun%2520Zhang%2520and%2520Chongxiao%2520Li%2520and%2520Wenxuan%2520Shi%2520and%2520Yutong%2520Wu%2520and%2520Jianan%2520Mu%2520and%2520Jinghua%2520Wang%2520and%2520Yang%2520Zhao%2520and%2520Pengwei%2520Jin%2520and%2520Shuyao%2520Cheng%2520and%2520Shengwen%2520Liang%2520and%2520Xishan%2520Zhang%2520and%2520Rui%2520Zhang%2520and%2520Zidong%2520Du%2520and%2520Qi%2520Guo%2520and%2520Xing%2520Hu%2520and%2520Yunji%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520trained%2520via%2520reinforcement%2520learning%2520with%250Averifiable%2520reward%2520%2528RLVR%2529%2520have%2520achieved%2520breakthroughs%2520on%2520tasks%2520with%2520explicit%252C%250Aautomatable%2520verification%252C%2520such%2520as%2520software%2520programming%2520and%2520mathematical%250Aproblems.%2520Extending%2520RLVR%2520to%2520electronic%2520design%2520automation%2520%2528EDA%2529%252C%2520especially%250Aautomatically%2520generating%2520hardware%2520description%2520languages%2520%2528HDLs%2529%2520like%2520Verilog%250Afrom%2520natural-language%2520%2528NL%2529%2520specifications%252C%2520however%252C%2520poses%2520three%2520key%2520challenges%253A%250Athe%2520lack%2520of%2520automated%2520and%2520accurate%2520verification%2520environments%252C%2520the%2520scarcity%2520of%250Ahigh-quality%2520NL-code%2520pairs%252C%2520and%2520the%2520prohibitive%2520computation%2520cost%2520of%2520RLVR.%2520To%250Athis%2520end%252C%2520we%2520introduce%2520CodeV-R1%252C%2520an%2520RLVR%2520framework%2520for%2520training%2520Verilog%250Ageneration%2520LLMs.%2520First%252C%2520we%2520develop%2520a%2520rule-based%2520testbench%2520generator%2520that%250Aperforms%2520robust%2520equivalence%2520checking%2520against%2520golden%2520references.%2520Second%252C%2520we%250Apropose%2520a%2520round-trip%2520data%2520synthesis%2520method%2520that%2520pairs%2520open-source%2520Verilog%250Asnippets%2520with%2520LLM-generated%2520NL%2520descriptions%252C%2520verifies%2520code-NL-code%2520consistency%250Avia%2520the%2520generated%2520testbench%252C%2520and%2520filters%2520out%2520inequivalent%2520examples%2520to%2520yield%2520a%250Ahigh-quality%2520dataset.%2520Third%252C%2520we%2520employ%2520a%2520two-stage%2520%2522distill-then-RL%2522%2520training%250Apipeline%253A%2520distillation%2520for%2520the%2520cold%2520start%2520of%2520reasoning%2520abilities%252C%2520followed%2520by%250Aadaptive%2520DAPO%252C%2520our%2520novel%2520RLVR%2520algorithm%2520that%2520can%2520reduce%2520training%2520cost%2520by%250Aadaptively%2520adjusting%2520sampling%2520rate.%2520The%2520resulting%2520model%252C%2520CodeV-R1-7B%252C%2520achieves%250A68.6%2525%2520and%252072.9%2525%2520pass%25401%2520on%2520VerilogEval%2520v2%2520and%2520RTLLM%2520v1.1%252C%2520respectively%252C%250Asurpassing%2520prior%2520state-of-the-art%2520by%252012~20%2525%252C%2520while%2520even%2520exceeding%2520the%250Aperformance%2520of%2520671B%2520DeepSeek-R1%2520on%2520RTLLM.%2520We%2520have%2520released%2520our%2520model%252C%2520training%250Acode%252C%2520and%2520dataset%2520to%2520facilitate%2520research%2520in%2520EDA%2520and%2520LLM%2520communities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24183v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QiMeng-CodeV-R1%3A%20Reasoning-Enhanced%20Verilog%20Generation&entry.906535625=Yaoyu%20Zhu%20and%20Di%20Huang%20and%20Hanqi%20Lyu%20and%20Xiaoyun%20Zhang%20and%20Chongxiao%20Li%20and%20Wenxuan%20Shi%20and%20Yutong%20Wu%20and%20Jianan%20Mu%20and%20Jinghua%20Wang%20and%20Yang%20Zhao%20and%20Pengwei%20Jin%20and%20Shuyao%20Cheng%20and%20Shengwen%20Liang%20and%20Xishan%20Zhang%20and%20Rui%20Zhang%20and%20Zidong%20Du%20and%20Qi%20Guo%20and%20Xing%20Hu%20and%20Yunji%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20trained%20via%20reinforcement%20learning%20with%0Averifiable%20reward%20%28RLVR%29%20have%20achieved%20breakthroughs%20on%20tasks%20with%20explicit%2C%0Aautomatable%20verification%2C%20such%20as%20software%20programming%20and%20mathematical%0Aproblems.%20Extending%20RLVR%20to%20electronic%20design%20automation%20%28EDA%29%2C%20especially%0Aautomatically%20generating%20hardware%20description%20languages%20%28HDLs%29%20like%20Verilog%0Afrom%20natural-language%20%28NL%29%20specifications%2C%20however%2C%20poses%20three%20key%20challenges%3A%0Athe%20lack%20of%20automated%20and%20accurate%20verification%20environments%2C%20the%20scarcity%20of%0Ahigh-quality%20NL-code%20pairs%2C%20and%20the%20prohibitive%20computation%20cost%20of%20RLVR.%20To%0Athis%20end%2C%20we%20introduce%20CodeV-R1%2C%20an%20RLVR%20framework%20for%20training%20Verilog%0Ageneration%20LLMs.%20First%2C%20we%20develop%20a%20rule-based%20testbench%20generator%20that%0Aperforms%20robust%20equivalence%20checking%20against%20golden%20references.%20Second%2C%20we%0Apropose%20a%20round-trip%20data%20synthesis%20method%20that%20pairs%20open-source%20Verilog%0Asnippets%20with%20LLM-generated%20NL%20descriptions%2C%20verifies%20code-NL-code%20consistency%0Avia%20the%20generated%20testbench%2C%20and%20filters%20out%20inequivalent%20examples%20to%20yield%20a%0Ahigh-quality%20dataset.%20Third%2C%20we%20employ%20a%20two-stage%20%22distill-then-RL%22%20training%0Apipeline%3A%20distillation%20for%20the%20cold%20start%20of%20reasoning%20abilities%2C%20followed%20by%0Aadaptive%20DAPO%2C%20our%20novel%20RLVR%20algorithm%20that%20can%20reduce%20training%20cost%20by%0Aadaptively%20adjusting%20sampling%20rate.%20The%20resulting%20model%2C%20CodeV-R1-7B%2C%20achieves%0A68.6%25%20and%2072.9%25%20pass%401%20on%20VerilogEval%20v2%20and%20RTLLM%20v1.1%2C%20respectively%2C%0Asurpassing%20prior%20state-of-the-art%20by%2012~20%25%2C%20while%20even%20exceeding%20the%0Aperformance%20of%20671B%20DeepSeek-R1%20on%20RTLLM.%20We%20have%20released%20our%20model%2C%20training%0Acode%2C%20and%20dataset%20to%20facilitate%20research%20in%20EDA%20and%20LLM%20communities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24183v3&entry.124074799=Read"},
{"title": "Understanding Adversarial Transfer: Why Representation-Space Attacks\n  Fail Where Data-Space Attacks Succeed", "author": "Isha Gupta and Rylan Schaeffer and Joshua Kazdan and Ken Ziyu Liu and Sanmi Koyejo", "abstract": "  The field of adversarial robustness has long established that adversarial\nexamples can successfully transfer between image classifiers and that text\njailbreaks can successfully transfer between language models (LMs). However, a\npair of recent studies reported being unable to successfully transfer image\njailbreaks between vision-language models (VLMs). To explain this striking\ndifference, we propose a fundamental distinction regarding the transferability\nof attacks against machine learning models: attacks in the input data-space can\ntransfer, whereas attacks in model representation space do not, at least not\nwithout geometric alignment of representations. We then provide theoretical and\nempirical evidence of this hypothesis in four different settings. First, we\nmathematically prove this distinction in a simple setting where two networks\ncompute the same input-output map but via different representations. Second, we\nconstruct representation-space attacks against image classifiers that are as\nsuccessful as well-known data-space attacks, but fail to transfer. Third, we\nconstruct representation-space attacks against LMs that successfully jailbreak\nthe attacked models but again fail to transfer. Fourth, we construct data-space\nattacks against VLMs that successfully transfer to new VLMs, and we show that\nrepresentation space attacks can transfer when VLMs' latent geometries are\nsufficiently aligned in post-projector space. Our work reveals that adversarial\ntransfer is not an inherent property of all attacks but contingent on their\noperational domain - the shared data-space versus models' unique representation\nspaces - a critical insight for building more robust models.\n", "link": "http://arxiv.org/abs/2510.01494v2", "date": "2025-10-03", "relevancy": 2.5407, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4889}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Adversarial%20Transfer%3A%20Why%20Representation-Space%20Attacks%0A%20%20Fail%20Where%20Data-Space%20Attacks%20Succeed&body=Title%3A%20Understanding%20Adversarial%20Transfer%3A%20Why%20Representation-Space%20Attacks%0A%20%20Fail%20Where%20Data-Space%20Attacks%20Succeed%0AAuthor%3A%20Isha%20Gupta%20and%20Rylan%20Schaeffer%20and%20Joshua%20Kazdan%20and%20Ken%20Ziyu%20Liu%20and%20Sanmi%20Koyejo%0AAbstract%3A%20%20%20The%20field%20of%20adversarial%20robustness%20has%20long%20established%20that%20adversarial%0Aexamples%20can%20successfully%20transfer%20between%20image%20classifiers%20and%20that%20text%0Ajailbreaks%20can%20successfully%20transfer%20between%20language%20models%20%28LMs%29.%20However%2C%20a%0Apair%20of%20recent%20studies%20reported%20being%20unable%20to%20successfully%20transfer%20image%0Ajailbreaks%20between%20vision-language%20models%20%28VLMs%29.%20To%20explain%20this%20striking%0Adifference%2C%20we%20propose%20a%20fundamental%20distinction%20regarding%20the%20transferability%0Aof%20attacks%20against%20machine%20learning%20models%3A%20attacks%20in%20the%20input%20data-space%20can%0Atransfer%2C%20whereas%20attacks%20in%20model%20representation%20space%20do%20not%2C%20at%20least%20not%0Awithout%20geometric%20alignment%20of%20representations.%20We%20then%20provide%20theoretical%20and%0Aempirical%20evidence%20of%20this%20hypothesis%20in%20four%20different%20settings.%20First%2C%20we%0Amathematically%20prove%20this%20distinction%20in%20a%20simple%20setting%20where%20two%20networks%0Acompute%20the%20same%20input-output%20map%20but%20via%20different%20representations.%20Second%2C%20we%0Aconstruct%20representation-space%20attacks%20against%20image%20classifiers%20that%20are%20as%0Asuccessful%20as%20well-known%20data-space%20attacks%2C%20but%20fail%20to%20transfer.%20Third%2C%20we%0Aconstruct%20representation-space%20attacks%20against%20LMs%20that%20successfully%20jailbreak%0Athe%20attacked%20models%20but%20again%20fail%20to%20transfer.%20Fourth%2C%20we%20construct%20data-space%0Aattacks%20against%20VLMs%20that%20successfully%20transfer%20to%20new%20VLMs%2C%20and%20we%20show%20that%0Arepresentation%20space%20attacks%20can%20transfer%20when%20VLMs%27%20latent%20geometries%20are%0Asufficiently%20aligned%20in%20post-projector%20space.%20Our%20work%20reveals%20that%20adversarial%0Atransfer%20is%20not%20an%20inherent%20property%20of%20all%20attacks%20but%20contingent%20on%20their%0Aoperational%20domain%20-%20the%20shared%20data-space%20versus%20models%27%20unique%20representation%0Aspaces%20-%20a%20critical%20insight%20for%20building%20more%20robust%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Adversarial%2520Transfer%253A%2520Why%2520Representation-Space%2520Attacks%250A%2520%2520Fail%2520Where%2520Data-Space%2520Attacks%2520Succeed%26entry.906535625%3DIsha%2520Gupta%2520and%2520Rylan%2520Schaeffer%2520and%2520Joshua%2520Kazdan%2520and%2520Ken%2520Ziyu%2520Liu%2520and%2520Sanmi%2520Koyejo%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520adversarial%2520robustness%2520has%2520long%2520established%2520that%2520adversarial%250Aexamples%2520can%2520successfully%2520transfer%2520between%2520image%2520classifiers%2520and%2520that%2520text%250Ajailbreaks%2520can%2520successfully%2520transfer%2520between%2520language%2520models%2520%2528LMs%2529.%2520However%252C%2520a%250Apair%2520of%2520recent%2520studies%2520reported%2520being%2520unable%2520to%2520successfully%2520transfer%2520image%250Ajailbreaks%2520between%2520vision-language%2520models%2520%2528VLMs%2529.%2520To%2520explain%2520this%2520striking%250Adifference%252C%2520we%2520propose%2520a%2520fundamental%2520distinction%2520regarding%2520the%2520transferability%250Aof%2520attacks%2520against%2520machine%2520learning%2520models%253A%2520attacks%2520in%2520the%2520input%2520data-space%2520can%250Atransfer%252C%2520whereas%2520attacks%2520in%2520model%2520representation%2520space%2520do%2520not%252C%2520at%2520least%2520not%250Awithout%2520geometric%2520alignment%2520of%2520representations.%2520We%2520then%2520provide%2520theoretical%2520and%250Aempirical%2520evidence%2520of%2520this%2520hypothesis%2520in%2520four%2520different%2520settings.%2520First%252C%2520we%250Amathematically%2520prove%2520this%2520distinction%2520in%2520a%2520simple%2520setting%2520where%2520two%2520networks%250Acompute%2520the%2520same%2520input-output%2520map%2520but%2520via%2520different%2520representations.%2520Second%252C%2520we%250Aconstruct%2520representation-space%2520attacks%2520against%2520image%2520classifiers%2520that%2520are%2520as%250Asuccessful%2520as%2520well-known%2520data-space%2520attacks%252C%2520but%2520fail%2520to%2520transfer.%2520Third%252C%2520we%250Aconstruct%2520representation-space%2520attacks%2520against%2520LMs%2520that%2520successfully%2520jailbreak%250Athe%2520attacked%2520models%2520but%2520again%2520fail%2520to%2520transfer.%2520Fourth%252C%2520we%2520construct%2520data-space%250Aattacks%2520against%2520VLMs%2520that%2520successfully%2520transfer%2520to%2520new%2520VLMs%252C%2520and%2520we%2520show%2520that%250Arepresentation%2520space%2520attacks%2520can%2520transfer%2520when%2520VLMs%2527%2520latent%2520geometries%2520are%250Asufficiently%2520aligned%2520in%2520post-projector%2520space.%2520Our%2520work%2520reveals%2520that%2520adversarial%250Atransfer%2520is%2520not%2520an%2520inherent%2520property%2520of%2520all%2520attacks%2520but%2520contingent%2520on%2520their%250Aoperational%2520domain%2520-%2520the%2520shared%2520data-space%2520versus%2520models%2527%2520unique%2520representation%250Aspaces%2520-%2520a%2520critical%2520insight%2520for%2520building%2520more%2520robust%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Adversarial%20Transfer%3A%20Why%20Representation-Space%20Attacks%0A%20%20Fail%20Where%20Data-Space%20Attacks%20Succeed&entry.906535625=Isha%20Gupta%20and%20Rylan%20Schaeffer%20and%20Joshua%20Kazdan%20and%20Ken%20Ziyu%20Liu%20and%20Sanmi%20Koyejo&entry.1292438233=%20%20The%20field%20of%20adversarial%20robustness%20has%20long%20established%20that%20adversarial%0Aexamples%20can%20successfully%20transfer%20between%20image%20classifiers%20and%20that%20text%0Ajailbreaks%20can%20successfully%20transfer%20between%20language%20models%20%28LMs%29.%20However%2C%20a%0Apair%20of%20recent%20studies%20reported%20being%20unable%20to%20successfully%20transfer%20image%0Ajailbreaks%20between%20vision-language%20models%20%28VLMs%29.%20To%20explain%20this%20striking%0Adifference%2C%20we%20propose%20a%20fundamental%20distinction%20regarding%20the%20transferability%0Aof%20attacks%20against%20machine%20learning%20models%3A%20attacks%20in%20the%20input%20data-space%20can%0Atransfer%2C%20whereas%20attacks%20in%20model%20representation%20space%20do%20not%2C%20at%20least%20not%0Awithout%20geometric%20alignment%20of%20representations.%20We%20then%20provide%20theoretical%20and%0Aempirical%20evidence%20of%20this%20hypothesis%20in%20four%20different%20settings.%20First%2C%20we%0Amathematically%20prove%20this%20distinction%20in%20a%20simple%20setting%20where%20two%20networks%0Acompute%20the%20same%20input-output%20map%20but%20via%20different%20representations.%20Second%2C%20we%0Aconstruct%20representation-space%20attacks%20against%20image%20classifiers%20that%20are%20as%0Asuccessful%20as%20well-known%20data-space%20attacks%2C%20but%20fail%20to%20transfer.%20Third%2C%20we%0Aconstruct%20representation-space%20attacks%20against%20LMs%20that%20successfully%20jailbreak%0Athe%20attacked%20models%20but%20again%20fail%20to%20transfer.%20Fourth%2C%20we%20construct%20data-space%0Aattacks%20against%20VLMs%20that%20successfully%20transfer%20to%20new%20VLMs%2C%20and%20we%20show%20that%0Arepresentation%20space%20attacks%20can%20transfer%20when%20VLMs%27%20latent%20geometries%20are%0Asufficiently%20aligned%20in%20post-projector%20space.%20Our%20work%20reveals%20that%20adversarial%0Atransfer%20is%20not%20an%20inherent%20property%20of%20all%20attacks%20but%20contingent%20on%20their%0Aoperational%20domain%20-%20the%20shared%20data-space%20versus%20models%27%20unique%20representation%0Aspaces%20-%20a%20critical%20insight%20for%20building%20more%20robust%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01494v2&entry.124074799=Read"},
{"title": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model\n  Reasoning", "author": "Tian Qin and David Alvarez-Melis and Samy Jelassi and Eran Malach", "abstract": "  Recent advancements in large language models (LLMs) have significantly\nimproved their reasoning abilities, particularly through techniques involving\nsearch and backtracking. Backtracking naturally scales test-time compute by\nenabling sequential, linearized exploration via long chain-of-thought (CoT)\ngeneration. However, this is not the only strategy for scaling test\ntime-compute: parallel sampling with best-of-N selection provides an\nalternative that generates diverse solutions simultaneously. Despite the\ngrowing adoption of sequential search, its advantages over parallel\nsampling-especially under a fixed compute budget-remain poorly understood. In\nthis paper, we systematically compare these two approaches on two challenging\nreasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential\nsearch underperforms parallel sampling on CountDown but outperforms it on\nSudoku, suggesting that backtracking is not universally beneficial. We identify\ntwo factors that can cause backtracking to degrade performance: (1) training on\nfixed search traces can lock models intro suboptimal strategies, and (2)\nexplicit CoT supervision can discourage implicit (non verbalized) reasoning.\nExtending our analysis to reinforcement learning (RL), we show that models with\nbacktracking capabilities benefit significantly from RL fine-tuning, while\nmodels without backtracking see limited, mixed gains. Together, these findings\nchallenge the assumption that backtracking universally enhances LLM reasoning,\ninstead revealing a complex interaction between task structure, training data,\nmodel scale, and learning paradigm.\n", "link": "http://arxiv.org/abs/2504.07052v2", "date": "2025-10-03", "relevancy": 2.5225, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Backtrack%20or%20Not%20to%20Backtrack%3A%20When%20Sequential%20Search%20Limits%20Model%0A%20%20Reasoning&body=Title%3A%20To%20Backtrack%20or%20Not%20to%20Backtrack%3A%20When%20Sequential%20Search%20Limits%20Model%0A%20%20Reasoning%0AAuthor%3A%20Tian%20Qin%20and%20David%20Alvarez-Melis%20and%20Samy%20Jelassi%20and%20Eran%20Malach%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aimproved%20their%20reasoning%20abilities%2C%20particularly%20through%20techniques%20involving%0Asearch%20and%20backtracking.%20Backtracking%20naturally%20scales%20test-time%20compute%20by%0Aenabling%20sequential%2C%20linearized%20exploration%20via%20long%20chain-of-thought%20%28CoT%29%0Ageneration.%20However%2C%20this%20is%20not%20the%20only%20strategy%20for%20scaling%20test%0Atime-compute%3A%20parallel%20sampling%20with%20best-of-N%20selection%20provides%20an%0Aalternative%20that%20generates%20diverse%20solutions%20simultaneously.%20Despite%20the%0Agrowing%20adoption%20of%20sequential%20search%2C%20its%20advantages%20over%20parallel%0Asampling-especially%20under%20a%20fixed%20compute%20budget-remain%20poorly%20understood.%20In%0Athis%20paper%2C%20we%20systematically%20compare%20these%20two%20approaches%20on%20two%20challenging%0Areasoning%20tasks%3A%20CountDown%20and%20Sudoku.%20Surprisingly%2C%20we%20find%20that%20sequential%0Asearch%20underperforms%20parallel%20sampling%20on%20CountDown%20but%20outperforms%20it%20on%0ASudoku%2C%20suggesting%20that%20backtracking%20is%20not%20universally%20beneficial.%20We%20identify%0Atwo%20factors%20that%20can%20cause%20backtracking%20to%20degrade%20performance%3A%20%281%29%20training%20on%0Afixed%20search%20traces%20can%20lock%20models%20intro%20suboptimal%20strategies%2C%20and%20%282%29%0Aexplicit%20CoT%20supervision%20can%20discourage%20implicit%20%28non%20verbalized%29%20reasoning.%0AExtending%20our%20analysis%20to%20reinforcement%20learning%20%28RL%29%2C%20we%20show%20that%20models%20with%0Abacktracking%20capabilities%20benefit%20significantly%20from%20RL%20fine-tuning%2C%20while%0Amodels%20without%20backtracking%20see%20limited%2C%20mixed%20gains.%20Together%2C%20these%20findings%0Achallenge%20the%20assumption%20that%20backtracking%20universally%20enhances%20LLM%20reasoning%2C%0Ainstead%20revealing%20a%20complex%20interaction%20between%20task%20structure%2C%20training%20data%2C%0Amodel%20scale%2C%20and%20learning%20paradigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Backtrack%2520or%2520Not%2520to%2520Backtrack%253A%2520When%2520Sequential%2520Search%2520Limits%2520Model%250A%2520%2520Reasoning%26entry.906535625%3DTian%2520Qin%2520and%2520David%2520Alvarez-Melis%2520and%2520Samy%2520Jelassi%2520and%2520Eran%2520Malach%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%250Aimproved%2520their%2520reasoning%2520abilities%252C%2520particularly%2520through%2520techniques%2520involving%250Asearch%2520and%2520backtracking.%2520Backtracking%2520naturally%2520scales%2520test-time%2520compute%2520by%250Aenabling%2520sequential%252C%2520linearized%2520exploration%2520via%2520long%2520chain-of-thought%2520%2528CoT%2529%250Ageneration.%2520However%252C%2520this%2520is%2520not%2520the%2520only%2520strategy%2520for%2520scaling%2520test%250Atime-compute%253A%2520parallel%2520sampling%2520with%2520best-of-N%2520selection%2520provides%2520an%250Aalternative%2520that%2520generates%2520diverse%2520solutions%2520simultaneously.%2520Despite%2520the%250Agrowing%2520adoption%2520of%2520sequential%2520search%252C%2520its%2520advantages%2520over%2520parallel%250Asampling-especially%2520under%2520a%2520fixed%2520compute%2520budget-remain%2520poorly%2520understood.%2520In%250Athis%2520paper%252C%2520we%2520systematically%2520compare%2520these%2520two%2520approaches%2520on%2520two%2520challenging%250Areasoning%2520tasks%253A%2520CountDown%2520and%2520Sudoku.%2520Surprisingly%252C%2520we%2520find%2520that%2520sequential%250Asearch%2520underperforms%2520parallel%2520sampling%2520on%2520CountDown%2520but%2520outperforms%2520it%2520on%250ASudoku%252C%2520suggesting%2520that%2520backtracking%2520is%2520not%2520universally%2520beneficial.%2520We%2520identify%250Atwo%2520factors%2520that%2520can%2520cause%2520backtracking%2520to%2520degrade%2520performance%253A%2520%25281%2529%2520training%2520on%250Afixed%2520search%2520traces%2520can%2520lock%2520models%2520intro%2520suboptimal%2520strategies%252C%2520and%2520%25282%2529%250Aexplicit%2520CoT%2520supervision%2520can%2520discourage%2520implicit%2520%2528non%2520verbalized%2529%2520reasoning.%250AExtending%2520our%2520analysis%2520to%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520we%2520show%2520that%2520models%2520with%250Abacktracking%2520capabilities%2520benefit%2520significantly%2520from%2520RL%2520fine-tuning%252C%2520while%250Amodels%2520without%2520backtracking%2520see%2520limited%252C%2520mixed%2520gains.%2520Together%252C%2520these%2520findings%250Achallenge%2520the%2520assumption%2520that%2520backtracking%2520universally%2520enhances%2520LLM%2520reasoning%252C%250Ainstead%2520revealing%2520a%2520complex%2520interaction%2520between%2520task%2520structure%252C%2520training%2520data%252C%250Amodel%2520scale%252C%2520and%2520learning%2520paradigm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Backtrack%20or%20Not%20to%20Backtrack%3A%20When%20Sequential%20Search%20Limits%20Model%0A%20%20Reasoning&entry.906535625=Tian%20Qin%20and%20David%20Alvarez-Melis%20and%20Samy%20Jelassi%20and%20Eran%20Malach&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aimproved%20their%20reasoning%20abilities%2C%20particularly%20through%20techniques%20involving%0Asearch%20and%20backtracking.%20Backtracking%20naturally%20scales%20test-time%20compute%20by%0Aenabling%20sequential%2C%20linearized%20exploration%20via%20long%20chain-of-thought%20%28CoT%29%0Ageneration.%20However%2C%20this%20is%20not%20the%20only%20strategy%20for%20scaling%20test%0Atime-compute%3A%20parallel%20sampling%20with%20best-of-N%20selection%20provides%20an%0Aalternative%20that%20generates%20diverse%20solutions%20simultaneously.%20Despite%20the%0Agrowing%20adoption%20of%20sequential%20search%2C%20its%20advantages%20over%20parallel%0Asampling-especially%20under%20a%20fixed%20compute%20budget-remain%20poorly%20understood.%20In%0Athis%20paper%2C%20we%20systematically%20compare%20these%20two%20approaches%20on%20two%20challenging%0Areasoning%20tasks%3A%20CountDown%20and%20Sudoku.%20Surprisingly%2C%20we%20find%20that%20sequential%0Asearch%20underperforms%20parallel%20sampling%20on%20CountDown%20but%20outperforms%20it%20on%0ASudoku%2C%20suggesting%20that%20backtracking%20is%20not%20universally%20beneficial.%20We%20identify%0Atwo%20factors%20that%20can%20cause%20backtracking%20to%20degrade%20performance%3A%20%281%29%20training%20on%0Afixed%20search%20traces%20can%20lock%20models%20intro%20suboptimal%20strategies%2C%20and%20%282%29%0Aexplicit%20CoT%20supervision%20can%20discourage%20implicit%20%28non%20verbalized%29%20reasoning.%0AExtending%20our%20analysis%20to%20reinforcement%20learning%20%28RL%29%2C%20we%20show%20that%20models%20with%0Abacktracking%20capabilities%20benefit%20significantly%20from%20RL%20fine-tuning%2C%20while%0Amodels%20without%20backtracking%20see%20limited%2C%20mixed%20gains.%20Together%2C%20these%20findings%0Achallenge%20the%20assumption%20that%20backtracking%20universally%20enhances%20LLM%20reasoning%2C%0Ainstead%20revealing%20a%20complex%20interaction%20between%20task%20structure%2C%20training%20data%2C%0Amodel%20scale%2C%20and%20learning%20paradigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07052v2&entry.124074799=Read"},
{"title": "Superposition disentanglement of neural representations reveals hidden\n  alignment", "author": "Andr\u00e9 Longon and David Klindt and Meenakshi Khosla", "abstract": "  The superposition hypothesis states that a single neuron within a population\nmay participate in the representation of multiple features in order for the\npopulation to represent more features than the number of neurons. In\nneuroscience and AI, representational alignment metrics measure the extent to\nwhich different deep neural networks (DNNs) or brains represent similar\ninformation. In this work, we explore a critical question: \\textit{does\nsuperposition interact with alignment metrics in any undesirable way?} We\nhypothesize that models which represent the same features in \\textit{different\nsuperposition arrangements}, i.e., their neurons have different linear\ncombinations of the features, will interfere with predictive mapping metrics\n(semi-matching, soft-matching, linear regression), producing lower alignment\nthan expected. We first develop a theory for how the strict permutation metrics\nare dependent on superposition arrangements. This is tested by training sparse\nautoencoders (SAEs) to disentangle superposition in toy models, where alignment\nscores are shown to typically increase when a model's base neurons are replaced\nwith its sparse overcomplete latent codes. We find similar increases for\nDNN\\(\\rightarrow\\)DNN and DNN\\(\\rightarrow\\)brain linear regression alignment\nin the visual domain. Our results suggest that superposition disentanglement is\nnecessary for mapping metrics to uncover the true representational alignment\nbetween neural codes.\n", "link": "http://arxiv.org/abs/2510.03186v1", "date": "2025-10-03", "relevancy": 2.522, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superposition%20disentanglement%20of%20neural%20representations%20reveals%20hidden%0A%20%20alignment&body=Title%3A%20Superposition%20disentanglement%20of%20neural%20representations%20reveals%20hidden%0A%20%20alignment%0AAuthor%3A%20Andr%C3%A9%20Longon%20and%20David%20Klindt%20and%20Meenakshi%20Khosla%0AAbstract%3A%20%20%20The%20superposition%20hypothesis%20states%20that%20a%20single%20neuron%20within%20a%20population%0Amay%20participate%20in%20the%20representation%20of%20multiple%20features%20in%20order%20for%20the%0Apopulation%20to%20represent%20more%20features%20than%20the%20number%20of%20neurons.%20In%0Aneuroscience%20and%20AI%2C%20representational%20alignment%20metrics%20measure%20the%20extent%20to%0Awhich%20different%20deep%20neural%20networks%20%28DNNs%29%20or%20brains%20represent%20similar%0Ainformation.%20In%20this%20work%2C%20we%20explore%20a%20critical%20question%3A%20%5Ctextit%7Bdoes%0Asuperposition%20interact%20with%20alignment%20metrics%20in%20any%20undesirable%20way%3F%7D%20We%0Ahypothesize%20that%20models%20which%20represent%20the%20same%20features%20in%20%5Ctextit%7Bdifferent%0Asuperposition%20arrangements%7D%2C%20i.e.%2C%20their%20neurons%20have%20different%20linear%0Acombinations%20of%20the%20features%2C%20will%20interfere%20with%20predictive%20mapping%20metrics%0A%28semi-matching%2C%20soft-matching%2C%20linear%20regression%29%2C%20producing%20lower%20alignment%0Athan%20expected.%20We%20first%20develop%20a%20theory%20for%20how%20the%20strict%20permutation%20metrics%0Aare%20dependent%20on%20superposition%20arrangements.%20This%20is%20tested%20by%20training%20sparse%0Aautoencoders%20%28SAEs%29%20to%20disentangle%20superposition%20in%20toy%20models%2C%20where%20alignment%0Ascores%20are%20shown%20to%20typically%20increase%20when%20a%20model%27s%20base%20neurons%20are%20replaced%0Awith%20its%20sparse%20overcomplete%20latent%20codes.%20We%20find%20similar%20increases%20for%0ADNN%5C%28%5Crightarrow%5C%29DNN%20and%20DNN%5C%28%5Crightarrow%5C%29brain%20linear%20regression%20alignment%0Ain%20the%20visual%20domain.%20Our%20results%20suggest%20that%20superposition%20disentanglement%20is%0Anecessary%20for%20mapping%20metrics%20to%20uncover%20the%20true%20representational%20alignment%0Abetween%20neural%20codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperposition%2520disentanglement%2520of%2520neural%2520representations%2520reveals%2520hidden%250A%2520%2520alignment%26entry.906535625%3DAndr%25C3%25A9%2520Longon%2520and%2520David%2520Klindt%2520and%2520Meenakshi%2520Khosla%26entry.1292438233%3D%2520%2520The%2520superposition%2520hypothesis%2520states%2520that%2520a%2520single%2520neuron%2520within%2520a%2520population%250Amay%2520participate%2520in%2520the%2520representation%2520of%2520multiple%2520features%2520in%2520order%2520for%2520the%250Apopulation%2520to%2520represent%2520more%2520features%2520than%2520the%2520number%2520of%2520neurons.%2520In%250Aneuroscience%2520and%2520AI%252C%2520representational%2520alignment%2520metrics%2520measure%2520the%2520extent%2520to%250Awhich%2520different%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520or%2520brains%2520represent%2520similar%250Ainformation.%2520In%2520this%2520work%252C%2520we%2520explore%2520a%2520critical%2520question%253A%2520%255Ctextit%257Bdoes%250Asuperposition%2520interact%2520with%2520alignment%2520metrics%2520in%2520any%2520undesirable%2520way%253F%257D%2520We%250Ahypothesize%2520that%2520models%2520which%2520represent%2520the%2520same%2520features%2520in%2520%255Ctextit%257Bdifferent%250Asuperposition%2520arrangements%257D%252C%2520i.e.%252C%2520their%2520neurons%2520have%2520different%2520linear%250Acombinations%2520of%2520the%2520features%252C%2520will%2520interfere%2520with%2520predictive%2520mapping%2520metrics%250A%2528semi-matching%252C%2520soft-matching%252C%2520linear%2520regression%2529%252C%2520producing%2520lower%2520alignment%250Athan%2520expected.%2520We%2520first%2520develop%2520a%2520theory%2520for%2520how%2520the%2520strict%2520permutation%2520metrics%250Aare%2520dependent%2520on%2520superposition%2520arrangements.%2520This%2520is%2520tested%2520by%2520training%2520sparse%250Aautoencoders%2520%2528SAEs%2529%2520to%2520disentangle%2520superposition%2520in%2520toy%2520models%252C%2520where%2520alignment%250Ascores%2520are%2520shown%2520to%2520typically%2520increase%2520when%2520a%2520model%2527s%2520base%2520neurons%2520are%2520replaced%250Awith%2520its%2520sparse%2520overcomplete%2520latent%2520codes.%2520We%2520find%2520similar%2520increases%2520for%250ADNN%255C%2528%255Crightarrow%255C%2529DNN%2520and%2520DNN%255C%2528%255Crightarrow%255C%2529brain%2520linear%2520regression%2520alignment%250Ain%2520the%2520visual%2520domain.%2520Our%2520results%2520suggest%2520that%2520superposition%2520disentanglement%2520is%250Anecessary%2520for%2520mapping%2520metrics%2520to%2520uncover%2520the%2520true%2520representational%2520alignment%250Abetween%2520neural%2520codes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superposition%20disentanglement%20of%20neural%20representations%20reveals%20hidden%0A%20%20alignment&entry.906535625=Andr%C3%A9%20Longon%20and%20David%20Klindt%20and%20Meenakshi%20Khosla&entry.1292438233=%20%20The%20superposition%20hypothesis%20states%20that%20a%20single%20neuron%20within%20a%20population%0Amay%20participate%20in%20the%20representation%20of%20multiple%20features%20in%20order%20for%20the%0Apopulation%20to%20represent%20more%20features%20than%20the%20number%20of%20neurons.%20In%0Aneuroscience%20and%20AI%2C%20representational%20alignment%20metrics%20measure%20the%20extent%20to%0Awhich%20different%20deep%20neural%20networks%20%28DNNs%29%20or%20brains%20represent%20similar%0Ainformation.%20In%20this%20work%2C%20we%20explore%20a%20critical%20question%3A%20%5Ctextit%7Bdoes%0Asuperposition%20interact%20with%20alignment%20metrics%20in%20any%20undesirable%20way%3F%7D%20We%0Ahypothesize%20that%20models%20which%20represent%20the%20same%20features%20in%20%5Ctextit%7Bdifferent%0Asuperposition%20arrangements%7D%2C%20i.e.%2C%20their%20neurons%20have%20different%20linear%0Acombinations%20of%20the%20features%2C%20will%20interfere%20with%20predictive%20mapping%20metrics%0A%28semi-matching%2C%20soft-matching%2C%20linear%20regression%29%2C%20producing%20lower%20alignment%0Athan%20expected.%20We%20first%20develop%20a%20theory%20for%20how%20the%20strict%20permutation%20metrics%0Aare%20dependent%20on%20superposition%20arrangements.%20This%20is%20tested%20by%20training%20sparse%0Aautoencoders%20%28SAEs%29%20to%20disentangle%20superposition%20in%20toy%20models%2C%20where%20alignment%0Ascores%20are%20shown%20to%20typically%20increase%20when%20a%20model%27s%20base%20neurons%20are%20replaced%0Awith%20its%20sparse%20overcomplete%20latent%20codes.%20We%20find%20similar%20increases%20for%0ADNN%5C%28%5Crightarrow%5C%29DNN%20and%20DNN%5C%28%5Crightarrow%5C%29brain%20linear%20regression%20alignment%0Ain%20the%20visual%20domain.%20Our%20results%20suggest%20that%20superposition%20disentanglement%20is%0Anecessary%20for%20mapping%20metrics%20to%20uncover%20the%20true%20representational%20alignment%0Abetween%20neural%20codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03186v1&entry.124074799=Read"},
{"title": "GPT and Prejudice: A Sparse Approach to Understanding Learned\n  Representations in Large Language Models", "author": "Mariam Mahran and Katharina Simbeck", "abstract": "  As large language models (LLMs) are increasingly trained on massive,\nuncurated corpora, understanding both model representations and the data they\ninternalize has become a major challenge. In this work, we show that pairing\nLLMs with sparse autoencoders (SAEs) enables interpretation not only of model\nbehavior but also of the deeper structures, themes, and biases embedded in the\ntraining data. We train a GPT-style transformer model exclusively on the novels\nof Jane Austen, a corpus rich in social constructs and narrative patterns. We\nthen apply SAEs to hidden states across multiple layers, uncovering sparse,\ninterpretable features that reflect the key narratives and concepts present in\nthe corpus, including gender, class, and societal duty. Our findings\ndemonstrate that LLMs combined with SAEs can act as scalable probes into\ncomplex datasets, offering a new path for corpus exploration, bias discovery,\nand model interpretability at scale.\n", "link": "http://arxiv.org/abs/2510.01252v2", "date": "2025-10-03", "relevancy": 2.5161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT%20and%20Prejudice%3A%20A%20Sparse%20Approach%20to%20Understanding%20Learned%0A%20%20Representations%20in%20Large%20Language%20Models&body=Title%3A%20GPT%20and%20Prejudice%3A%20A%20Sparse%20Approach%20to%20Understanding%20Learned%0A%20%20Representations%20in%20Large%20Language%20Models%0AAuthor%3A%20Mariam%20Mahran%20and%20Katharina%20Simbeck%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20trained%20on%20massive%2C%0Auncurated%20corpora%2C%20understanding%20both%20model%20representations%20and%20the%20data%20they%0Ainternalize%20has%20become%20a%20major%20challenge.%20In%20this%20work%2C%20we%20show%20that%20pairing%0ALLMs%20with%20sparse%20autoencoders%20%28SAEs%29%20enables%20interpretation%20not%20only%20of%20model%0Abehavior%20but%20also%20of%20the%20deeper%20structures%2C%20themes%2C%20and%20biases%20embedded%20in%20the%0Atraining%20data.%20We%20train%20a%20GPT-style%20transformer%20model%20exclusively%20on%20the%20novels%0Aof%20Jane%20Austen%2C%20a%20corpus%20rich%20in%20social%20constructs%20and%20narrative%20patterns.%20We%0Athen%20apply%20SAEs%20to%20hidden%20states%20across%20multiple%20layers%2C%20uncovering%20sparse%2C%0Ainterpretable%20features%20that%20reflect%20the%20key%20narratives%20and%20concepts%20present%20in%0Athe%20corpus%2C%20including%20gender%2C%20class%2C%20and%20societal%20duty.%20Our%20findings%0Ademonstrate%20that%20LLMs%20combined%20with%20SAEs%20can%20act%20as%20scalable%20probes%20into%0Acomplex%20datasets%2C%20offering%20a%20new%20path%20for%20corpus%20exploration%2C%20bias%20discovery%2C%0Aand%20model%20interpretability%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT%2520and%2520Prejudice%253A%2520A%2520Sparse%2520Approach%2520to%2520Understanding%2520Learned%250A%2520%2520Representations%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DMariam%2520Mahran%2520and%2520Katharina%2520Simbeck%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520trained%2520on%2520massive%252C%250Auncurated%2520corpora%252C%2520understanding%2520both%2520model%2520representations%2520and%2520the%2520data%2520they%250Ainternalize%2520has%2520become%2520a%2520major%2520challenge.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520pairing%250ALLMs%2520with%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520enables%2520interpretation%2520not%2520only%2520of%2520model%250Abehavior%2520but%2520also%2520of%2520the%2520deeper%2520structures%252C%2520themes%252C%2520and%2520biases%2520embedded%2520in%2520the%250Atraining%2520data.%2520We%2520train%2520a%2520GPT-style%2520transformer%2520model%2520exclusively%2520on%2520the%2520novels%250Aof%2520Jane%2520Austen%252C%2520a%2520corpus%2520rich%2520in%2520social%2520constructs%2520and%2520narrative%2520patterns.%2520We%250Athen%2520apply%2520SAEs%2520to%2520hidden%2520states%2520across%2520multiple%2520layers%252C%2520uncovering%2520sparse%252C%250Ainterpretable%2520features%2520that%2520reflect%2520the%2520key%2520narratives%2520and%2520concepts%2520present%2520in%250Athe%2520corpus%252C%2520including%2520gender%252C%2520class%252C%2520and%2520societal%2520duty.%2520Our%2520findings%250Ademonstrate%2520that%2520LLMs%2520combined%2520with%2520SAEs%2520can%2520act%2520as%2520scalable%2520probes%2520into%250Acomplex%2520datasets%252C%2520offering%2520a%2520new%2520path%2520for%2520corpus%2520exploration%252C%2520bias%2520discovery%252C%250Aand%2520model%2520interpretability%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT%20and%20Prejudice%3A%20A%20Sparse%20Approach%20to%20Understanding%20Learned%0A%20%20Representations%20in%20Large%20Language%20Models&entry.906535625=Mariam%20Mahran%20and%20Katharina%20Simbeck&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20trained%20on%20massive%2C%0Auncurated%20corpora%2C%20understanding%20both%20model%20representations%20and%20the%20data%20they%0Ainternalize%20has%20become%20a%20major%20challenge.%20In%20this%20work%2C%20we%20show%20that%20pairing%0ALLMs%20with%20sparse%20autoencoders%20%28SAEs%29%20enables%20interpretation%20not%20only%20of%20model%0Abehavior%20but%20also%20of%20the%20deeper%20structures%2C%20themes%2C%20and%20biases%20embedded%20in%20the%0Atraining%20data.%20We%20train%20a%20GPT-style%20transformer%20model%20exclusively%20on%20the%20novels%0Aof%20Jane%20Austen%2C%20a%20corpus%20rich%20in%20social%20constructs%20and%20narrative%20patterns.%20We%0Athen%20apply%20SAEs%20to%20hidden%20states%20across%20multiple%20layers%2C%20uncovering%20sparse%2C%0Ainterpretable%20features%20that%20reflect%20the%20key%20narratives%20and%20concepts%20present%20in%0Athe%20corpus%2C%20including%20gender%2C%20class%2C%20and%20societal%20duty.%20Our%20findings%0Ademonstrate%20that%20LLMs%20combined%20with%20SAEs%20can%20act%20as%20scalable%20probes%20into%0Acomplex%20datasets%2C%20offering%20a%20new%20path%20for%20corpus%20exploration%2C%20bias%20discovery%2C%0Aand%20model%20interpretability%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01252v2&entry.124074799=Read"},
{"title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition\n  and Interaction", "author": "Kaisi Guan and Xihua Wang and Zhengfeng Lai and Xin Cheng and Peng Zhang and XiaoJiang Liu and Ruihua Song and Meng Cao", "abstract": "  This study focuses on a challenging yet promising task,\nText-to-Sounding-Video (T2SV) generation, which aims to generate a video with\nsynchronized audio from text conditions, meanwhile ensuring both modalities are\naligned with text. Despite progress in joint audio-video training, two critical\nchallenges still remain unaddressed: (1) a single, shared text caption where\nthe text for video is equal to the text for audio often creates modal\ninterference, confusing the pretrained backbones, and (2) the optimal mechanism\nfor cross-modal feature interaction remains unclear. To address these\nchallenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)\nframework that generates pairs of disentangled captions, a video caption, and\nan audio caption, eliminating interference at the conditioning stage. Based on\nHVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,\nwhich employs a Dual CrossAttention (DCA) mechanism that acts as a robust\n``bridge\" to enable a symmetric, bidirectional exchange of information,\nachieving both semantic and temporal synchronization. Extensive experiments on\nthree benchmark datasets, supported by human evaluations, demonstrate that our\nmethod achieves state-of-the-art results on most metrics. Comprehensive\nablation studies further validate the effectiveness of our contributions,\noffering key insights for the future T2SV task. All the codes and checkpoints\nwill be publicly released.\n", "link": "http://arxiv.org/abs/2510.03117v1", "date": "2025-10-03", "relevancy": 2.5129, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6497}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6369}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20Text-to-Sounding%20Video%20Generation%20via%20Advanced%20Modality%20Condition%0A%20%20and%20Interaction&body=Title%3A%20Taming%20Text-to-Sounding%20Video%20Generation%20via%20Advanced%20Modality%20Condition%0A%20%20and%20Interaction%0AAuthor%3A%20Kaisi%20Guan%20and%20Xihua%20Wang%20and%20Zhengfeng%20Lai%20and%20Xin%20Cheng%20and%20Peng%20Zhang%20and%20XiaoJiang%20Liu%20and%20Ruihua%20Song%20and%20Meng%20Cao%0AAbstract%3A%20%20%20This%20study%20focuses%20on%20a%20challenging%20yet%20promising%20task%2C%0AText-to-Sounding-Video%20%28T2SV%29%20generation%2C%20which%20aims%20to%20generate%20a%20video%20with%0Asynchronized%20audio%20from%20text%20conditions%2C%20meanwhile%20ensuring%20both%20modalities%20are%0Aaligned%20with%20text.%20Despite%20progress%20in%20joint%20audio-video%20training%2C%20two%20critical%0Achallenges%20still%20remain%20unaddressed%3A%20%281%29%20a%20single%2C%20shared%20text%20caption%20where%0Athe%20text%20for%20video%20is%20equal%20to%20the%20text%20for%20audio%20often%20creates%20modal%0Ainterference%2C%20confusing%20the%20pretrained%20backbones%2C%20and%20%282%29%20the%20optimal%20mechanism%0Afor%20cross-modal%20feature%20interaction%20remains%20unclear.%20To%20address%20these%0Achallenges%2C%20we%20first%20propose%20the%20Hierarchical%20Visual-Grounded%20Captioning%20%28HVGC%29%0Aframework%20that%20generates%20pairs%20of%20disentangled%20captions%2C%20a%20video%20caption%2C%20and%0Aan%20audio%20caption%2C%20eliminating%20interference%20at%20the%20conditioning%20stage.%20Based%20on%0AHVGC%2C%20we%20further%20introduce%20BridgeDiT%2C%20a%20novel%20dual-tower%20diffusion%20transformer%2C%0Awhich%20employs%20a%20Dual%20CrossAttention%20%28DCA%29%20mechanism%20that%20acts%20as%20a%20robust%0A%60%60bridge%22%20to%20enable%20a%20symmetric%2C%20bidirectional%20exchange%20of%20information%2C%0Aachieving%20both%20semantic%20and%20temporal%20synchronization.%20Extensive%20experiments%20on%0Athree%20benchmark%20datasets%2C%20supported%20by%20human%20evaluations%2C%20demonstrate%20that%20our%0Amethod%20achieves%20state-of-the-art%20results%20on%20most%20metrics.%20Comprehensive%0Aablation%20studies%20further%20validate%20the%20effectiveness%20of%20our%20contributions%2C%0Aoffering%20key%20insights%20for%20the%20future%20T2SV%20task.%20All%20the%20codes%20and%20checkpoints%0Awill%20be%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520Text-to-Sounding%2520Video%2520Generation%2520via%2520Advanced%2520Modality%2520Condition%250A%2520%2520and%2520Interaction%26entry.906535625%3DKaisi%2520Guan%2520and%2520Xihua%2520Wang%2520and%2520Zhengfeng%2520Lai%2520and%2520Xin%2520Cheng%2520and%2520Peng%2520Zhang%2520and%2520XiaoJiang%2520Liu%2520and%2520Ruihua%2520Song%2520and%2520Meng%2520Cao%26entry.1292438233%3D%2520%2520This%2520study%2520focuses%2520on%2520a%2520challenging%2520yet%2520promising%2520task%252C%250AText-to-Sounding-Video%2520%2528T2SV%2529%2520generation%252C%2520which%2520aims%2520to%2520generate%2520a%2520video%2520with%250Asynchronized%2520audio%2520from%2520text%2520conditions%252C%2520meanwhile%2520ensuring%2520both%2520modalities%2520are%250Aaligned%2520with%2520text.%2520Despite%2520progress%2520in%2520joint%2520audio-video%2520training%252C%2520two%2520critical%250Achallenges%2520still%2520remain%2520unaddressed%253A%2520%25281%2529%2520a%2520single%252C%2520shared%2520text%2520caption%2520where%250Athe%2520text%2520for%2520video%2520is%2520equal%2520to%2520the%2520text%2520for%2520audio%2520often%2520creates%2520modal%250Ainterference%252C%2520confusing%2520the%2520pretrained%2520backbones%252C%2520and%2520%25282%2529%2520the%2520optimal%2520mechanism%250Afor%2520cross-modal%2520feature%2520interaction%2520remains%2520unclear.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520first%2520propose%2520the%2520Hierarchical%2520Visual-Grounded%2520Captioning%2520%2528HVGC%2529%250Aframework%2520that%2520generates%2520pairs%2520of%2520disentangled%2520captions%252C%2520a%2520video%2520caption%252C%2520and%250Aan%2520audio%2520caption%252C%2520eliminating%2520interference%2520at%2520the%2520conditioning%2520stage.%2520Based%2520on%250AHVGC%252C%2520we%2520further%2520introduce%2520BridgeDiT%252C%2520a%2520novel%2520dual-tower%2520diffusion%2520transformer%252C%250Awhich%2520employs%2520a%2520Dual%2520CrossAttention%2520%2528DCA%2529%2520mechanism%2520that%2520acts%2520as%2520a%2520robust%250A%2560%2560bridge%2522%2520to%2520enable%2520a%2520symmetric%252C%2520bidirectional%2520exchange%2520of%2520information%252C%250Aachieving%2520both%2520semantic%2520and%2520temporal%2520synchronization.%2520Extensive%2520experiments%2520on%250Athree%2520benchmark%2520datasets%252C%2520supported%2520by%2520human%2520evaluations%252C%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520state-of-the-art%2520results%2520on%2520most%2520metrics.%2520Comprehensive%250Aablation%2520studies%2520further%2520validate%2520the%2520effectiveness%2520of%2520our%2520contributions%252C%250Aoffering%2520key%2520insights%2520for%2520the%2520future%2520T2SV%2520task.%2520All%2520the%2520codes%2520and%2520checkpoints%250Awill%2520be%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Text-to-Sounding%20Video%20Generation%20via%20Advanced%20Modality%20Condition%0A%20%20and%20Interaction&entry.906535625=Kaisi%20Guan%20and%20Xihua%20Wang%20and%20Zhengfeng%20Lai%20and%20Xin%20Cheng%20and%20Peng%20Zhang%20and%20XiaoJiang%20Liu%20and%20Ruihua%20Song%20and%20Meng%20Cao&entry.1292438233=%20%20This%20study%20focuses%20on%20a%20challenging%20yet%20promising%20task%2C%0AText-to-Sounding-Video%20%28T2SV%29%20generation%2C%20which%20aims%20to%20generate%20a%20video%20with%0Asynchronized%20audio%20from%20text%20conditions%2C%20meanwhile%20ensuring%20both%20modalities%20are%0Aaligned%20with%20text.%20Despite%20progress%20in%20joint%20audio-video%20training%2C%20two%20critical%0Achallenges%20still%20remain%20unaddressed%3A%20%281%29%20a%20single%2C%20shared%20text%20caption%20where%0Athe%20text%20for%20video%20is%20equal%20to%20the%20text%20for%20audio%20often%20creates%20modal%0Ainterference%2C%20confusing%20the%20pretrained%20backbones%2C%20and%20%282%29%20the%20optimal%20mechanism%0Afor%20cross-modal%20feature%20interaction%20remains%20unclear.%20To%20address%20these%0Achallenges%2C%20we%20first%20propose%20the%20Hierarchical%20Visual-Grounded%20Captioning%20%28HVGC%29%0Aframework%20that%20generates%20pairs%20of%20disentangled%20captions%2C%20a%20video%20caption%2C%20and%0Aan%20audio%20caption%2C%20eliminating%20interference%20at%20the%20conditioning%20stage.%20Based%20on%0AHVGC%2C%20we%20further%20introduce%20BridgeDiT%2C%20a%20novel%20dual-tower%20diffusion%20transformer%2C%0Awhich%20employs%20a%20Dual%20CrossAttention%20%28DCA%29%20mechanism%20that%20acts%20as%20a%20robust%0A%60%60bridge%22%20to%20enable%20a%20symmetric%2C%20bidirectional%20exchange%20of%20information%2C%0Aachieving%20both%20semantic%20and%20temporal%20synchronization.%20Extensive%20experiments%20on%0Athree%20benchmark%20datasets%2C%20supported%20by%20human%20evaluations%2C%20demonstrate%20that%20our%0Amethod%20achieves%20state-of-the-art%20results%20on%20most%20metrics.%20Comprehensive%0Aablation%20studies%20further%20validate%20the%20effectiveness%20of%20our%20contributions%2C%0Aoffering%20key%20insights%20for%20the%20future%20T2SV%20task.%20All%20the%20codes%20and%20checkpoints%0Awill%20be%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03117v1&entry.124074799=Read"},
{"title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention\n  Alignment", "author": "Hongxiang Zhang and Yuan Tian and Tianyi Zhang", "abstract": "  To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining.\n", "link": "http://arxiv.org/abs/2510.03223v1", "date": "2025-10-03", "relevancy": 2.4855, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Anchor%3A%20Large%20Language%20Model%20Reasoning%20via%20Step-by-step%20Attention%0A%20%20Alignment&body=Title%3A%20Self-Anchor%3A%20Large%20Language%20Model%20Reasoning%20via%20Step-by-step%20Attention%0A%20%20Alignment%0AAuthor%3A%20Hongxiang%20Zhang%20and%20Yuan%20Tian%20and%20Tianyi%20Zhang%0AAbstract%3A%20%20%20To%20solve%20complex%20reasoning%20tasks%20for%20Large%20Language%20Models%20%28LLMs%29%2C%0Aprompting-based%20methods%20offer%20a%20lightweight%20alternative%20to%20fine-tuning%20and%0Areinforcement%20learning.%20However%2C%20as%20reasoning%20chains%20extend%2C%20critical%0Aintermediate%20steps%20and%20the%20original%20prompt%20will%20be%20buried%20in%20the%20context%2C%0Areceiving%20insufficient%20attention%20and%20leading%20to%20errors.%20In%20this%20paper%2C%20we%0Apropose%20Self-Anchor%2C%20a%20novel%20pipeline%20that%20leverages%20the%20inherent%20structure%20of%0Areasoning%20to%20steer%20LLM%20attention.%20Self-Anchor%20decomposes%20reasoning%20trajectories%0Ainto%20structured%20plans%20and%20automatically%20aligns%20the%20model%27s%20attention%20to%20the%0Amost%20relevant%20inference%20steps%2C%20allowing%20the%20model%20to%20maintain%20focus%20throughout%0Ageneration.%20Our%20experiment%20shows%20that%20Self-Anchor%20outperforms%20SOTA%20prompting%0Amethods%20across%20six%20benchmarks.%20Notably%2C%20Self-Anchor%20significantly%20reduces%20the%0Aperformance%20gap%20between%20%60%60non-reasoning%27%27%20models%20and%20specialized%20reasoning%0Amodels%2C%20with%20the%20potential%20to%20enable%20most%20LLMs%20to%20tackle%20complex%20reasoning%0Atasks%20without%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Anchor%253A%2520Large%2520Language%2520Model%2520Reasoning%2520via%2520Step-by-step%2520Attention%250A%2520%2520Alignment%26entry.906535625%3DHongxiang%2520Zhang%2520and%2520Yuan%2520Tian%2520and%2520Tianyi%2520Zhang%26entry.1292438233%3D%2520%2520To%2520solve%2520complex%2520reasoning%2520tasks%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Aprompting-based%2520methods%2520offer%2520a%2520lightweight%2520alternative%2520to%2520fine-tuning%2520and%250Areinforcement%2520learning.%2520However%252C%2520as%2520reasoning%2520chains%2520extend%252C%2520critical%250Aintermediate%2520steps%2520and%2520the%2520original%2520prompt%2520will%2520be%2520buried%2520in%2520the%2520context%252C%250Areceiving%2520insufficient%2520attention%2520and%2520leading%2520to%2520errors.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Self-Anchor%252C%2520a%2520novel%2520pipeline%2520that%2520leverages%2520the%2520inherent%2520structure%2520of%250Areasoning%2520to%2520steer%2520LLM%2520attention.%2520Self-Anchor%2520decomposes%2520reasoning%2520trajectories%250Ainto%2520structured%2520plans%2520and%2520automatically%2520aligns%2520the%2520model%2527s%2520attention%2520to%2520the%250Amost%2520relevant%2520inference%2520steps%252C%2520allowing%2520the%2520model%2520to%2520maintain%2520focus%2520throughout%250Ageneration.%2520Our%2520experiment%2520shows%2520that%2520Self-Anchor%2520outperforms%2520SOTA%2520prompting%250Amethods%2520across%2520six%2520benchmarks.%2520Notably%252C%2520Self-Anchor%2520significantly%2520reduces%2520the%250Aperformance%2520gap%2520between%2520%2560%2560non-reasoning%2527%2527%2520models%2520and%2520specialized%2520reasoning%250Amodels%252C%2520with%2520the%2520potential%2520to%2520enable%2520most%2520LLMs%2520to%2520tackle%2520complex%2520reasoning%250Atasks%2520without%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Anchor%3A%20Large%20Language%20Model%20Reasoning%20via%20Step-by-step%20Attention%0A%20%20Alignment&entry.906535625=Hongxiang%20Zhang%20and%20Yuan%20Tian%20and%20Tianyi%20Zhang&entry.1292438233=%20%20To%20solve%20complex%20reasoning%20tasks%20for%20Large%20Language%20Models%20%28LLMs%29%2C%0Aprompting-based%20methods%20offer%20a%20lightweight%20alternative%20to%20fine-tuning%20and%0Areinforcement%20learning.%20However%2C%20as%20reasoning%20chains%20extend%2C%20critical%0Aintermediate%20steps%20and%20the%20original%20prompt%20will%20be%20buried%20in%20the%20context%2C%0Areceiving%20insufficient%20attention%20and%20leading%20to%20errors.%20In%20this%20paper%2C%20we%0Apropose%20Self-Anchor%2C%20a%20novel%20pipeline%20that%20leverages%20the%20inherent%20structure%20of%0Areasoning%20to%20steer%20LLM%20attention.%20Self-Anchor%20decomposes%20reasoning%20trajectories%0Ainto%20structured%20plans%20and%20automatically%20aligns%20the%20model%27s%20attention%20to%20the%0Amost%20relevant%20inference%20steps%2C%20allowing%20the%20model%20to%20maintain%20focus%20throughout%0Ageneration.%20Our%20experiment%20shows%20that%20Self-Anchor%20outperforms%20SOTA%20prompting%0Amethods%20across%20six%20benchmarks.%20Notably%2C%20Self-Anchor%20significantly%20reduces%20the%0Aperformance%20gap%20between%20%60%60non-reasoning%27%27%20models%20and%20specialized%20reasoning%0Amodels%2C%20with%20the%20potential%20to%20enable%20most%20LLMs%20to%20tackle%20complex%20reasoning%0Atasks%20without%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03223v1&entry.124074799=Read"},
{"title": "Bootstrap Learning for Combinatorial Graph Alignment with Sequential\n  GNNs", "author": "Marc Lelarge", "abstract": "  Graph neural networks (GNNs) have struggled to outperform traditional\noptimization methods on combinatorial problems, limiting their practical\nimpact. We address this gap by introducing a novel chaining procedure for the\ngraph alignment problem, a fundamental NP-hard task of finding optimal node\ncorrespondences between unlabeled graphs using only structural information. Our\nmethod trains a sequence of GNNs where each network learns to iteratively\nrefine similarity matrices produced by previous networks. During inference,\nthis creates a bootstrap effect: each GNN improves upon partial solutions by\nincorporating discrete ranking information about node alignment quality from\nprior iterations. We combine this with a powerful architecture that operates on\nnode pairs rather than individual nodes, capturing global structural patterns\nessential for alignment that standard message-passing networks cannot\nrepresent. Extensive experiments on synthetic benchmarks demonstrate\nsubstantial improvements: our chained GNNs achieve over 3x better accuracy than\nexisting methods on challenging instances, and uniquely solve regular graphs\nwhere all competing approaches fail. When combined with traditional\noptimization as post-processing, our method substantially outperforms\nstate-of-the-art solvers on the graph alignment benchmark.\n", "link": "http://arxiv.org/abs/2510.03086v1", "date": "2025-10-03", "relevancy": 2.4763, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5111}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4876}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrap%20Learning%20for%20Combinatorial%20Graph%20Alignment%20with%20Sequential%0A%20%20GNNs&body=Title%3A%20Bootstrap%20Learning%20for%20Combinatorial%20Graph%20Alignment%20with%20Sequential%0A%20%20GNNs%0AAuthor%3A%20Marc%20Lelarge%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20struggled%20to%20outperform%20traditional%0Aoptimization%20methods%20on%20combinatorial%20problems%2C%20limiting%20their%20practical%0Aimpact.%20We%20address%20this%20gap%20by%20introducing%20a%20novel%20chaining%20procedure%20for%20the%0Agraph%20alignment%20problem%2C%20a%20fundamental%20NP-hard%20task%20of%20finding%20optimal%20node%0Acorrespondences%20between%20unlabeled%20graphs%20using%20only%20structural%20information.%20Our%0Amethod%20trains%20a%20sequence%20of%20GNNs%20where%20each%20network%20learns%20to%20iteratively%0Arefine%20similarity%20matrices%20produced%20by%20previous%20networks.%20During%20inference%2C%0Athis%20creates%20a%20bootstrap%20effect%3A%20each%20GNN%20improves%20upon%20partial%20solutions%20by%0Aincorporating%20discrete%20ranking%20information%20about%20node%20alignment%20quality%20from%0Aprior%20iterations.%20We%20combine%20this%20with%20a%20powerful%20architecture%20that%20operates%20on%0Anode%20pairs%20rather%20than%20individual%20nodes%2C%20capturing%20global%20structural%20patterns%0Aessential%20for%20alignment%20that%20standard%20message-passing%20networks%20cannot%0Arepresent.%20Extensive%20experiments%20on%20synthetic%20benchmarks%20demonstrate%0Asubstantial%20improvements%3A%20our%20chained%20GNNs%20achieve%20over%203x%20better%20accuracy%20than%0Aexisting%20methods%20on%20challenging%20instances%2C%20and%20uniquely%20solve%20regular%20graphs%0Awhere%20all%20competing%20approaches%20fail.%20When%20combined%20with%20traditional%0Aoptimization%20as%20post-processing%2C%20our%20method%20substantially%20outperforms%0Astate-of-the-art%20solvers%20on%20the%20graph%20alignment%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrap%2520Learning%2520for%2520Combinatorial%2520Graph%2520Alignment%2520with%2520Sequential%250A%2520%2520GNNs%26entry.906535625%3DMarc%2520Lelarge%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520struggled%2520to%2520outperform%2520traditional%250Aoptimization%2520methods%2520on%2520combinatorial%2520problems%252C%2520limiting%2520their%2520practical%250Aimpact.%2520We%2520address%2520this%2520gap%2520by%2520introducing%2520a%2520novel%2520chaining%2520procedure%2520for%2520the%250Agraph%2520alignment%2520problem%252C%2520a%2520fundamental%2520NP-hard%2520task%2520of%2520finding%2520optimal%2520node%250Acorrespondences%2520between%2520unlabeled%2520graphs%2520using%2520only%2520structural%2520information.%2520Our%250Amethod%2520trains%2520a%2520sequence%2520of%2520GNNs%2520where%2520each%2520network%2520learns%2520to%2520iteratively%250Arefine%2520similarity%2520matrices%2520produced%2520by%2520previous%2520networks.%2520During%2520inference%252C%250Athis%2520creates%2520a%2520bootstrap%2520effect%253A%2520each%2520GNN%2520improves%2520upon%2520partial%2520solutions%2520by%250Aincorporating%2520discrete%2520ranking%2520information%2520about%2520node%2520alignment%2520quality%2520from%250Aprior%2520iterations.%2520We%2520combine%2520this%2520with%2520a%2520powerful%2520architecture%2520that%2520operates%2520on%250Anode%2520pairs%2520rather%2520than%2520individual%2520nodes%252C%2520capturing%2520global%2520structural%2520patterns%250Aessential%2520for%2520alignment%2520that%2520standard%2520message-passing%2520networks%2520cannot%250Arepresent.%2520Extensive%2520experiments%2520on%2520synthetic%2520benchmarks%2520demonstrate%250Asubstantial%2520improvements%253A%2520our%2520chained%2520GNNs%2520achieve%2520over%25203x%2520better%2520accuracy%2520than%250Aexisting%2520methods%2520on%2520challenging%2520instances%252C%2520and%2520uniquely%2520solve%2520regular%2520graphs%250Awhere%2520all%2520competing%2520approaches%2520fail.%2520When%2520combined%2520with%2520traditional%250Aoptimization%2520as%2520post-processing%252C%2520our%2520method%2520substantially%2520outperforms%250Astate-of-the-art%2520solvers%2520on%2520the%2520graph%2520alignment%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrap%20Learning%20for%20Combinatorial%20Graph%20Alignment%20with%20Sequential%0A%20%20GNNs&entry.906535625=Marc%20Lelarge&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20struggled%20to%20outperform%20traditional%0Aoptimization%20methods%20on%20combinatorial%20problems%2C%20limiting%20their%20practical%0Aimpact.%20We%20address%20this%20gap%20by%20introducing%20a%20novel%20chaining%20procedure%20for%20the%0Agraph%20alignment%20problem%2C%20a%20fundamental%20NP-hard%20task%20of%20finding%20optimal%20node%0Acorrespondences%20between%20unlabeled%20graphs%20using%20only%20structural%20information.%20Our%0Amethod%20trains%20a%20sequence%20of%20GNNs%20where%20each%20network%20learns%20to%20iteratively%0Arefine%20similarity%20matrices%20produced%20by%20previous%20networks.%20During%20inference%2C%0Athis%20creates%20a%20bootstrap%20effect%3A%20each%20GNN%20improves%20upon%20partial%20solutions%20by%0Aincorporating%20discrete%20ranking%20information%20about%20node%20alignment%20quality%20from%0Aprior%20iterations.%20We%20combine%20this%20with%20a%20powerful%20architecture%20that%20operates%20on%0Anode%20pairs%20rather%20than%20individual%20nodes%2C%20capturing%20global%20structural%20patterns%0Aessential%20for%20alignment%20that%20standard%20message-passing%20networks%20cannot%0Arepresent.%20Extensive%20experiments%20on%20synthetic%20benchmarks%20demonstrate%0Asubstantial%20improvements%3A%20our%20chained%20GNNs%20achieve%20over%203x%20better%20accuracy%20than%0Aexisting%20methods%20on%20challenging%20instances%2C%20and%20uniquely%20solve%20regular%20graphs%0Awhere%20all%20competing%20approaches%20fail.%20When%20combined%20with%20traditional%0Aoptimization%20as%20post-processing%2C%20our%20method%20substantially%20outperforms%0Astate-of-the-art%20solvers%20on%20the%20graph%20alignment%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03086v1&entry.124074799=Read"},
{"title": "Towards Provable Emergence of In-Context Reinforcement Learning", "author": "Jiuqi Wang and Rohan Chandra and Shangtong Zhang", "abstract": "  Typically, a modern reinforcement learning (RL) agent solves a task by\nupdating its neural network parameters to adapt its policy to the task.\nRecently, it has been observed that some RL agents can solve a wide range of\nnew out-of-distribution tasks without parameter updates after pretraining on\nsome task distribution. When evaluated in a new task, instead of making\nparameter updates, the pretrained agent conditions its policy on additional\ninput called the context, e.g., the agent's interaction history in the new\ntask. The agent's performance increases as the information in the context\nincreases, with the agent's parameters fixed. This phenomenon is typically\ncalled in-context RL (ICRL). The pretrained parameters of the agent network\nenable the remarkable ICRL phenomenon. However, many ICRL works perform the\npretraining with standard RL algorithms. This raises the central question this\npaper aims to address: Why can the RL pretraining algorithm generate network\nparameters that enable ICRL? We hypothesize that the parameters capable of ICRL\nare minimizers of the pretraining loss. This work provides initial support for\nthis hypothesis through a case study. In particular, we prove that when a\nTransformer is pretrained for policy evaluation, one of the global minimizers\nof the pretraining loss can enable in-context temporal difference learning.\n", "link": "http://arxiv.org/abs/2509.18389v2", "date": "2025-10-03", "relevancy": 2.4646, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Provable%20Emergence%20of%20In-Context%20Reinforcement%20Learning&body=Title%3A%20Towards%20Provable%20Emergence%20of%20In-Context%20Reinforcement%20Learning%0AAuthor%3A%20Jiuqi%20Wang%20and%20Rohan%20Chandra%20and%20Shangtong%20Zhang%0AAbstract%3A%20%20%20Typically%2C%20a%20modern%20reinforcement%20learning%20%28RL%29%20agent%20solves%20a%20task%20by%0Aupdating%20its%20neural%20network%20parameters%20to%20adapt%20its%20policy%20to%20the%20task.%0ARecently%2C%20it%20has%20been%20observed%20that%20some%20RL%20agents%20can%20solve%20a%20wide%20range%20of%0Anew%20out-of-distribution%20tasks%20without%20parameter%20updates%20after%20pretraining%20on%0Asome%20task%20distribution.%20When%20evaluated%20in%20a%20new%20task%2C%20instead%20of%20making%0Aparameter%20updates%2C%20the%20pretrained%20agent%20conditions%20its%20policy%20on%20additional%0Ainput%20called%20the%20context%2C%20e.g.%2C%20the%20agent%27s%20interaction%20history%20in%20the%20new%0Atask.%20The%20agent%27s%20performance%20increases%20as%20the%20information%20in%20the%20context%0Aincreases%2C%20with%20the%20agent%27s%20parameters%20fixed.%20This%20phenomenon%20is%20typically%0Acalled%20in-context%20RL%20%28ICRL%29.%20The%20pretrained%20parameters%20of%20the%20agent%20network%0Aenable%20the%20remarkable%20ICRL%20phenomenon.%20However%2C%20many%20ICRL%20works%20perform%20the%0Apretraining%20with%20standard%20RL%20algorithms.%20This%20raises%20the%20central%20question%20this%0Apaper%20aims%20to%20address%3A%20Why%20can%20the%20RL%20pretraining%20algorithm%20generate%20network%0Aparameters%20that%20enable%20ICRL%3F%20We%20hypothesize%20that%20the%20parameters%20capable%20of%20ICRL%0Aare%20minimizers%20of%20the%20pretraining%20loss.%20This%20work%20provides%20initial%20support%20for%0Athis%20hypothesis%20through%20a%20case%20study.%20In%20particular%2C%20we%20prove%20that%20when%20a%0ATransformer%20is%20pretrained%20for%20policy%20evaluation%2C%20one%20of%20the%20global%20minimizers%0Aof%20the%20pretraining%20loss%20can%20enable%20in-context%20temporal%20difference%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Provable%2520Emergence%2520of%2520In-Context%2520Reinforcement%2520Learning%26entry.906535625%3DJiuqi%2520Wang%2520and%2520Rohan%2520Chandra%2520and%2520Shangtong%2520Zhang%26entry.1292438233%3D%2520%2520Typically%252C%2520a%2520modern%2520reinforcement%2520learning%2520%2528RL%2529%2520agent%2520solves%2520a%2520task%2520by%250Aupdating%2520its%2520neural%2520network%2520parameters%2520to%2520adapt%2520its%2520policy%2520to%2520the%2520task.%250ARecently%252C%2520it%2520has%2520been%2520observed%2520that%2520some%2520RL%2520agents%2520can%2520solve%2520a%2520wide%2520range%2520of%250Anew%2520out-of-distribution%2520tasks%2520without%2520parameter%2520updates%2520after%2520pretraining%2520on%250Asome%2520task%2520distribution.%2520When%2520evaluated%2520in%2520a%2520new%2520task%252C%2520instead%2520of%2520making%250Aparameter%2520updates%252C%2520the%2520pretrained%2520agent%2520conditions%2520its%2520policy%2520on%2520additional%250Ainput%2520called%2520the%2520context%252C%2520e.g.%252C%2520the%2520agent%2527s%2520interaction%2520history%2520in%2520the%2520new%250Atask.%2520The%2520agent%2527s%2520performance%2520increases%2520as%2520the%2520information%2520in%2520the%2520context%250Aincreases%252C%2520with%2520the%2520agent%2527s%2520parameters%2520fixed.%2520This%2520phenomenon%2520is%2520typically%250Acalled%2520in-context%2520RL%2520%2528ICRL%2529.%2520The%2520pretrained%2520parameters%2520of%2520the%2520agent%2520network%250Aenable%2520the%2520remarkable%2520ICRL%2520phenomenon.%2520However%252C%2520many%2520ICRL%2520works%2520perform%2520the%250Apretraining%2520with%2520standard%2520RL%2520algorithms.%2520This%2520raises%2520the%2520central%2520question%2520this%250Apaper%2520aims%2520to%2520address%253A%2520Why%2520can%2520the%2520RL%2520pretraining%2520algorithm%2520generate%2520network%250Aparameters%2520that%2520enable%2520ICRL%253F%2520We%2520hypothesize%2520that%2520the%2520parameters%2520capable%2520of%2520ICRL%250Aare%2520minimizers%2520of%2520the%2520pretraining%2520loss.%2520This%2520work%2520provides%2520initial%2520support%2520for%250Athis%2520hypothesis%2520through%2520a%2520case%2520study.%2520In%2520particular%252C%2520we%2520prove%2520that%2520when%2520a%250ATransformer%2520is%2520pretrained%2520for%2520policy%2520evaluation%252C%2520one%2520of%2520the%2520global%2520minimizers%250Aof%2520the%2520pretraining%2520loss%2520can%2520enable%2520in-context%2520temporal%2520difference%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Provable%20Emergence%20of%20In-Context%20Reinforcement%20Learning&entry.906535625=Jiuqi%20Wang%20and%20Rohan%20Chandra%20and%20Shangtong%20Zhang&entry.1292438233=%20%20Typically%2C%20a%20modern%20reinforcement%20learning%20%28RL%29%20agent%20solves%20a%20task%20by%0Aupdating%20its%20neural%20network%20parameters%20to%20adapt%20its%20policy%20to%20the%20task.%0ARecently%2C%20it%20has%20been%20observed%20that%20some%20RL%20agents%20can%20solve%20a%20wide%20range%20of%0Anew%20out-of-distribution%20tasks%20without%20parameter%20updates%20after%20pretraining%20on%0Asome%20task%20distribution.%20When%20evaluated%20in%20a%20new%20task%2C%20instead%20of%20making%0Aparameter%20updates%2C%20the%20pretrained%20agent%20conditions%20its%20policy%20on%20additional%0Ainput%20called%20the%20context%2C%20e.g.%2C%20the%20agent%27s%20interaction%20history%20in%20the%20new%0Atask.%20The%20agent%27s%20performance%20increases%20as%20the%20information%20in%20the%20context%0Aincreases%2C%20with%20the%20agent%27s%20parameters%20fixed.%20This%20phenomenon%20is%20typically%0Acalled%20in-context%20RL%20%28ICRL%29.%20The%20pretrained%20parameters%20of%20the%20agent%20network%0Aenable%20the%20remarkable%20ICRL%20phenomenon.%20However%2C%20many%20ICRL%20works%20perform%20the%0Apretraining%20with%20standard%20RL%20algorithms.%20This%20raises%20the%20central%20question%20this%0Apaper%20aims%20to%20address%3A%20Why%20can%20the%20RL%20pretraining%20algorithm%20generate%20network%0Aparameters%20that%20enable%20ICRL%3F%20We%20hypothesize%20that%20the%20parameters%20capable%20of%20ICRL%0Aare%20minimizers%20of%20the%20pretraining%20loss.%20This%20work%20provides%20initial%20support%20for%0Athis%20hypothesis%20through%20a%20case%20study.%20In%20particular%2C%20we%20prove%20that%20when%20a%0ATransformer%20is%20pretrained%20for%20policy%20evaluation%2C%20one%20of%20the%20global%20minimizers%0Aof%20the%20pretraining%20loss%20can%20enable%20in-context%20temporal%20difference%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18389v2&entry.124074799=Read"},
{"title": "PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical\n  images within PyRadiomics", "author": "Jakub Lisowski and Piotr Tyrakowski and Szymon Zygu\u0142a and Krzysztof Kaczmarski", "abstract": "  PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,\ndesigned to address the computational challenges of extracting\nthree-dimensional shape features from medical images. By offloading key\ngeometric computations to GPU hardware it dramatically reduces processing times\nfor large volumetric datasets. The system maintains full compatibility with the\noriginal PyRadiomics API, enabling seamless integration into existing AI\nworkflows without code modifications. This transparent acceleration facilitates\nefficient, scalable radiomics analysis, supporting rapid feature extraction\nessential for high-throughput AI pipeline. Tests performed on a typical\ncomputational cluster, budget and home devices prove usefulness in all\nscenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely\navailable under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA\nAdditionally PyRadiomics-cuda test suite is available at\nhttps://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed\nhandbook and sample scripts suited for different kinds of workflows plus\ndetailed installation instructions. The dataset used for testing is available\nat Kaggle\nhttps://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19\n", "link": "http://arxiv.org/abs/2510.02894v1", "date": "2025-10-03", "relevancy": 2.4488, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5043}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5043}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PyRadiomics-cuda%3A%20a%20GPU-accelerated%203D%20features%20extraction%20from%20medical%0A%20%20images%20within%20PyRadiomics&body=Title%3A%20PyRadiomics-cuda%3A%20a%20GPU-accelerated%203D%20features%20extraction%20from%20medical%0A%20%20images%20within%20PyRadiomics%0AAuthor%3A%20Jakub%20Lisowski%20and%20Piotr%20Tyrakowski%20and%20Szymon%20Zygu%C5%82a%20and%20Krzysztof%20Kaczmarski%0AAbstract%3A%20%20%20PyRadiomics-cuda%20is%20a%20GPU-accelerated%20extension%20of%20the%20PyRadiomics%20library%2C%0Adesigned%20to%20address%20the%20computational%20challenges%20of%20extracting%0Athree-dimensional%20shape%20features%20from%20medical%20images.%20By%20offloading%20key%0Ageometric%20computations%20to%20GPU%20hardware%20it%20dramatically%20reduces%20processing%20times%0Afor%20large%20volumetric%20datasets.%20The%20system%20maintains%20full%20compatibility%20with%20the%0Aoriginal%20PyRadiomics%20API%2C%20enabling%20seamless%20integration%20into%20existing%20AI%0Aworkflows%20without%20code%20modifications.%20This%20transparent%20acceleration%20facilitates%0Aefficient%2C%20scalable%20radiomics%20analysis%2C%20supporting%20rapid%20feature%20extraction%0Aessential%20for%20high-throughput%20AI%20pipeline.%20Tests%20performed%20on%20a%20typical%0Acomputational%20cluster%2C%20budget%20and%20home%20devices%20prove%20usefulness%20in%20all%0Ascenarios.%20PyRadiomics-cuda%20is%20implemented%20in%20Python%20and%20C/CUDA%20and%20is%20freely%0Aavailable%20under%20the%20BSD%20license%20at%20https%3A//github.com/mis-wut/pyradiomics-CUDA%0AAdditionally%20PyRadiomics-cuda%20test%20suite%20is%20available%20at%0Ahttps%3A//github.com/mis-wut/pyradiomics-cuda-data-gen.%20It%20provides%20detailed%0Ahandbook%20and%20sample%20scripts%20suited%20for%20different%20kinds%20of%20workflows%20plus%0Adetailed%20installation%20instructions.%20The%20dataset%20used%20for%20testing%20is%20available%0Aat%20Kaggle%0Ahttps%3A//www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyRadiomics-cuda%253A%2520a%2520GPU-accelerated%25203D%2520features%2520extraction%2520from%2520medical%250A%2520%2520images%2520within%2520PyRadiomics%26entry.906535625%3DJakub%2520Lisowski%2520and%2520Piotr%2520Tyrakowski%2520and%2520Szymon%2520Zygu%25C5%2582a%2520and%2520Krzysztof%2520Kaczmarski%26entry.1292438233%3D%2520%2520PyRadiomics-cuda%2520is%2520a%2520GPU-accelerated%2520extension%2520of%2520the%2520PyRadiomics%2520library%252C%250Adesigned%2520to%2520address%2520the%2520computational%2520challenges%2520of%2520extracting%250Athree-dimensional%2520shape%2520features%2520from%2520medical%2520images.%2520By%2520offloading%2520key%250Ageometric%2520computations%2520to%2520GPU%2520hardware%2520it%2520dramatically%2520reduces%2520processing%2520times%250Afor%2520large%2520volumetric%2520datasets.%2520The%2520system%2520maintains%2520full%2520compatibility%2520with%2520the%250Aoriginal%2520PyRadiomics%2520API%252C%2520enabling%2520seamless%2520integration%2520into%2520existing%2520AI%250Aworkflows%2520without%2520code%2520modifications.%2520This%2520transparent%2520acceleration%2520facilitates%250Aefficient%252C%2520scalable%2520radiomics%2520analysis%252C%2520supporting%2520rapid%2520feature%2520extraction%250Aessential%2520for%2520high-throughput%2520AI%2520pipeline.%2520Tests%2520performed%2520on%2520a%2520typical%250Acomputational%2520cluster%252C%2520budget%2520and%2520home%2520devices%2520prove%2520usefulness%2520in%2520all%250Ascenarios.%2520PyRadiomics-cuda%2520is%2520implemented%2520in%2520Python%2520and%2520C/CUDA%2520and%2520is%2520freely%250Aavailable%2520under%2520the%2520BSD%2520license%2520at%2520https%253A//github.com/mis-wut/pyradiomics-CUDA%250AAdditionally%2520PyRadiomics-cuda%2520test%2520suite%2520is%2520available%2520at%250Ahttps%253A//github.com/mis-wut/pyradiomics-cuda-data-gen.%2520It%2520provides%2520detailed%250Ahandbook%2520and%2520sample%2520scripts%2520suited%2520for%2520different%2520kinds%2520of%2520workflows%2520plus%250Adetailed%2520installation%2520instructions.%2520The%2520dataset%2520used%2520for%2520testing%2520is%2520available%250Aat%2520Kaggle%250Ahttps%253A//www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PyRadiomics-cuda%3A%20a%20GPU-accelerated%203D%20features%20extraction%20from%20medical%0A%20%20images%20within%20PyRadiomics&entry.906535625=Jakub%20Lisowski%20and%20Piotr%20Tyrakowski%20and%20Szymon%20Zygu%C5%82a%20and%20Krzysztof%20Kaczmarski&entry.1292438233=%20%20PyRadiomics-cuda%20is%20a%20GPU-accelerated%20extension%20of%20the%20PyRadiomics%20library%2C%0Adesigned%20to%20address%20the%20computational%20challenges%20of%20extracting%0Athree-dimensional%20shape%20features%20from%20medical%20images.%20By%20offloading%20key%0Ageometric%20computations%20to%20GPU%20hardware%20it%20dramatically%20reduces%20processing%20times%0Afor%20large%20volumetric%20datasets.%20The%20system%20maintains%20full%20compatibility%20with%20the%0Aoriginal%20PyRadiomics%20API%2C%20enabling%20seamless%20integration%20into%20existing%20AI%0Aworkflows%20without%20code%20modifications.%20This%20transparent%20acceleration%20facilitates%0Aefficient%2C%20scalable%20radiomics%20analysis%2C%20supporting%20rapid%20feature%20extraction%0Aessential%20for%20high-throughput%20AI%20pipeline.%20Tests%20performed%20on%20a%20typical%0Acomputational%20cluster%2C%20budget%20and%20home%20devices%20prove%20usefulness%20in%20all%0Ascenarios.%20PyRadiomics-cuda%20is%20implemented%20in%20Python%20and%20C/CUDA%20and%20is%20freely%0Aavailable%20under%20the%20BSD%20license%20at%20https%3A//github.com/mis-wut/pyradiomics-CUDA%0AAdditionally%20PyRadiomics-cuda%20test%20suite%20is%20available%20at%0Ahttps%3A//github.com/mis-wut/pyradiomics-cuda-data-gen.%20It%20provides%20detailed%0Ahandbook%20and%20sample%20scripts%20suited%20for%20different%20kinds%20of%20workflows%20plus%0Adetailed%20installation%20instructions.%20The%20dataset%20used%20for%20testing%20is%20available%0Aat%20Kaggle%0Ahttps%3A//www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02894v1&entry.124074799=Read"},
{"title": "Topological Autoencoders++: Fast and Accurate Cycle-Aware Dimensionality\n  Reduction", "author": "Matt\u00e9o Cl\u00e9mot and Julie Digne and Julien Tierny", "abstract": "  This paper presents a novel topology-aware dimensionality reduction approach\naiming at accurately visualizing the cyclic patterns present in high\ndimensional data. To that end, we build on the Topological Autoencoders\n(TopoAE) formulation. First, we provide a novel theoretical analysis of its\nassociated loss and show that a zero loss indeed induces identical persistence\npairs (in high and low dimensions) for the $0$-dimensional persistent homology\n(PH$^0$) of the Rips filtration. We also provide a counter example showing that\nthis property no longer holds for a naive extension of TopoAE to PH$^d$ for\n$d\\ge 1$. Based on this observation, we introduce a novel generalization of\nTopoAE to $1$-dimensional persistent homology (PH$^1$), called TopoAE++, for\nthe accurate generation of cycle-aware planar embeddings, addressing the above\nfailure case. This generalization is based on the notion of cascade distortion,\na new penalty term favoring an isometric embedding of the $2$-chains filling\npersistent $1$-cycles, hence resulting in more faithful geometrical\nreconstructions of the $1$-cycles in the plane. We further introduce a novel,\nfast algorithm for the exact computation of PH for Rips filtrations in the\nplane, yielding improved runtimes over previously documented topology-aware\nmethods. Our method also achieves a better balance between the topological\naccuracy, as measured by the Wasserstein distance, and the visual preservation\nof the cycles in low dimensions. Our C++ implementation is available at\nhttps://github.com/MClemot/TopologicalAutoencodersPlusPlus.\n", "link": "http://arxiv.org/abs/2502.20215v2", "date": "2025-10-03", "relevancy": 2.4417, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4935}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4902}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20Autoencoders%2B%2B%3A%20Fast%20and%20Accurate%20Cycle-Aware%20Dimensionality%0A%20%20Reduction&body=Title%3A%20Topological%20Autoencoders%2B%2B%3A%20Fast%20and%20Accurate%20Cycle-Aware%20Dimensionality%0A%20%20Reduction%0AAuthor%3A%20Matt%C3%A9o%20Cl%C3%A9mot%20and%20Julie%20Digne%20and%20Julien%20Tierny%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20topology-aware%20dimensionality%20reduction%20approach%0Aaiming%20at%20accurately%20visualizing%20the%20cyclic%20patterns%20present%20in%20high%0Adimensional%20data.%20To%20that%20end%2C%20we%20build%20on%20the%20Topological%20Autoencoders%0A%28TopoAE%29%20formulation.%20First%2C%20we%20provide%20a%20novel%20theoretical%20analysis%20of%20its%0Aassociated%20loss%20and%20show%20that%20a%20zero%20loss%20indeed%20induces%20identical%20persistence%0Apairs%20%28in%20high%20and%20low%20dimensions%29%20for%20the%20%240%24-dimensional%20persistent%20homology%0A%28PH%24%5E0%24%29%20of%20the%20Rips%20filtration.%20We%20also%20provide%20a%20counter%20example%20showing%20that%0Athis%20property%20no%20longer%20holds%20for%20a%20naive%20extension%20of%20TopoAE%20to%20PH%24%5Ed%24%20for%0A%24d%5Cge%201%24.%20Based%20on%20this%20observation%2C%20we%20introduce%20a%20novel%20generalization%20of%0ATopoAE%20to%20%241%24-dimensional%20persistent%20homology%20%28PH%24%5E1%24%29%2C%20called%20TopoAE%2B%2B%2C%20for%0Athe%20accurate%20generation%20of%20cycle-aware%20planar%20embeddings%2C%20addressing%20the%20above%0Afailure%20case.%20This%20generalization%20is%20based%20on%20the%20notion%20of%20cascade%20distortion%2C%0Aa%20new%20penalty%20term%20favoring%20an%20isometric%20embedding%20of%20the%20%242%24-chains%20filling%0Apersistent%20%241%24-cycles%2C%20hence%20resulting%20in%20more%20faithful%20geometrical%0Areconstructions%20of%20the%20%241%24-cycles%20in%20the%20plane.%20We%20further%20introduce%20a%20novel%2C%0Afast%20algorithm%20for%20the%20exact%20computation%20of%20PH%20for%20Rips%20filtrations%20in%20the%0Aplane%2C%20yielding%20improved%20runtimes%20over%20previously%20documented%20topology-aware%0Amethods.%20Our%20method%20also%20achieves%20a%20better%20balance%20between%20the%20topological%0Aaccuracy%2C%20as%20measured%20by%20the%20Wasserstein%20distance%2C%20and%20the%20visual%20preservation%0Aof%20the%20cycles%20in%20low%20dimensions.%20Our%20C%2B%2B%20implementation%20is%20available%20at%0Ahttps%3A//github.com/MClemot/TopologicalAutoencodersPlusPlus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20215v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520Autoencoders%252B%252B%253A%2520Fast%2520and%2520Accurate%2520Cycle-Aware%2520Dimensionality%250A%2520%2520Reduction%26entry.906535625%3DMatt%25C3%25A9o%2520Cl%25C3%25A9mot%2520and%2520Julie%2520Digne%2520and%2520Julien%2520Tierny%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520topology-aware%2520dimensionality%2520reduction%2520approach%250Aaiming%2520at%2520accurately%2520visualizing%2520the%2520cyclic%2520patterns%2520present%2520in%2520high%250Adimensional%2520data.%2520To%2520that%2520end%252C%2520we%2520build%2520on%2520the%2520Topological%2520Autoencoders%250A%2528TopoAE%2529%2520formulation.%2520First%252C%2520we%2520provide%2520a%2520novel%2520theoretical%2520analysis%2520of%2520its%250Aassociated%2520loss%2520and%2520show%2520that%2520a%2520zero%2520loss%2520indeed%2520induces%2520identical%2520persistence%250Apairs%2520%2528in%2520high%2520and%2520low%2520dimensions%2529%2520for%2520the%2520%25240%2524-dimensional%2520persistent%2520homology%250A%2528PH%2524%255E0%2524%2529%2520of%2520the%2520Rips%2520filtration.%2520We%2520also%2520provide%2520a%2520counter%2520example%2520showing%2520that%250Athis%2520property%2520no%2520longer%2520holds%2520for%2520a%2520naive%2520extension%2520of%2520TopoAE%2520to%2520PH%2524%255Ed%2524%2520for%250A%2524d%255Cge%25201%2524.%2520Based%2520on%2520this%2520observation%252C%2520we%2520introduce%2520a%2520novel%2520generalization%2520of%250ATopoAE%2520to%2520%25241%2524-dimensional%2520persistent%2520homology%2520%2528PH%2524%255E1%2524%2529%252C%2520called%2520TopoAE%252B%252B%252C%2520for%250Athe%2520accurate%2520generation%2520of%2520cycle-aware%2520planar%2520embeddings%252C%2520addressing%2520the%2520above%250Afailure%2520case.%2520This%2520generalization%2520is%2520based%2520on%2520the%2520notion%2520of%2520cascade%2520distortion%252C%250Aa%2520new%2520penalty%2520term%2520favoring%2520an%2520isometric%2520embedding%2520of%2520the%2520%25242%2524-chains%2520filling%250Apersistent%2520%25241%2524-cycles%252C%2520hence%2520resulting%2520in%2520more%2520faithful%2520geometrical%250Areconstructions%2520of%2520the%2520%25241%2524-cycles%2520in%2520the%2520plane.%2520We%2520further%2520introduce%2520a%2520novel%252C%250Afast%2520algorithm%2520for%2520the%2520exact%2520computation%2520of%2520PH%2520for%2520Rips%2520filtrations%2520in%2520the%250Aplane%252C%2520yielding%2520improved%2520runtimes%2520over%2520previously%2520documented%2520topology-aware%250Amethods.%2520Our%2520method%2520also%2520achieves%2520a%2520better%2520balance%2520between%2520the%2520topological%250Aaccuracy%252C%2520as%2520measured%2520by%2520the%2520Wasserstein%2520distance%252C%2520and%2520the%2520visual%2520preservation%250Aof%2520the%2520cycles%2520in%2520low%2520dimensions.%2520Our%2520C%252B%252B%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/MClemot/TopologicalAutoencodersPlusPlus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20215v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20Autoencoders%2B%2B%3A%20Fast%20and%20Accurate%20Cycle-Aware%20Dimensionality%0A%20%20Reduction&entry.906535625=Matt%C3%A9o%20Cl%C3%A9mot%20and%20Julie%20Digne%20and%20Julien%20Tierny&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20topology-aware%20dimensionality%20reduction%20approach%0Aaiming%20at%20accurately%20visualizing%20the%20cyclic%20patterns%20present%20in%20high%0Adimensional%20data.%20To%20that%20end%2C%20we%20build%20on%20the%20Topological%20Autoencoders%0A%28TopoAE%29%20formulation.%20First%2C%20we%20provide%20a%20novel%20theoretical%20analysis%20of%20its%0Aassociated%20loss%20and%20show%20that%20a%20zero%20loss%20indeed%20induces%20identical%20persistence%0Apairs%20%28in%20high%20and%20low%20dimensions%29%20for%20the%20%240%24-dimensional%20persistent%20homology%0A%28PH%24%5E0%24%29%20of%20the%20Rips%20filtration.%20We%20also%20provide%20a%20counter%20example%20showing%20that%0Athis%20property%20no%20longer%20holds%20for%20a%20naive%20extension%20of%20TopoAE%20to%20PH%24%5Ed%24%20for%0A%24d%5Cge%201%24.%20Based%20on%20this%20observation%2C%20we%20introduce%20a%20novel%20generalization%20of%0ATopoAE%20to%20%241%24-dimensional%20persistent%20homology%20%28PH%24%5E1%24%29%2C%20called%20TopoAE%2B%2B%2C%20for%0Athe%20accurate%20generation%20of%20cycle-aware%20planar%20embeddings%2C%20addressing%20the%20above%0Afailure%20case.%20This%20generalization%20is%20based%20on%20the%20notion%20of%20cascade%20distortion%2C%0Aa%20new%20penalty%20term%20favoring%20an%20isometric%20embedding%20of%20the%20%242%24-chains%20filling%0Apersistent%20%241%24-cycles%2C%20hence%20resulting%20in%20more%20faithful%20geometrical%0Areconstructions%20of%20the%20%241%24-cycles%20in%20the%20plane.%20We%20further%20introduce%20a%20novel%2C%0Afast%20algorithm%20for%20the%20exact%20computation%20of%20PH%20for%20Rips%20filtrations%20in%20the%0Aplane%2C%20yielding%20improved%20runtimes%20over%20previously%20documented%20topology-aware%0Amethods.%20Our%20method%20also%20achieves%20a%20better%20balance%20between%20the%20topological%0Aaccuracy%2C%20as%20measured%20by%20the%20Wasserstein%20distance%2C%20and%20the%20visual%20preservation%0Aof%20the%20cycles%20in%20low%20dimensions.%20Our%20C%2B%2B%20implementation%20is%20available%20at%0Ahttps%3A//github.com/MClemot/TopologicalAutoencodersPlusPlus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20215v2&entry.124074799=Read"},
{"title": "RACCooN: A Versatile Instructional Video Editing Framework with\n  Auto-Generated Narratives", "author": "Jaehong Yoon and Shoubin Yu and Mohit Bansal", "abstract": "  Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. (3) RACCooN also plans to imagine new objects in a\ngiven video, so users simply prompt the model to receive a detailed video\nediting plan for complex video editing. The proposed framework demonstrates\nimpressive versatile capabilities in video-to-paragraph generation, video\ncontent editing, and can be incorporated into other SoTA video generative\nmodels for further enhancement.\n", "link": "http://arxiv.org/abs/2405.18406v4", "date": "2025-10-03", "relevancy": 2.4372, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6264}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6112}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RACCooN%3A%20A%20Versatile%20Instructional%20Video%20Editing%20Framework%20with%0A%20%20Auto-Generated%20Narratives&body=Title%3A%20RACCooN%3A%20A%20Versatile%20Instructional%20Video%20Editing%20Framework%20with%0A%20%20Auto-Generated%20Narratives%0AAuthor%3A%20Jaehong%20Yoon%20and%20Shoubin%20Yu%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Recent%20video%20generative%20models%20primarily%20rely%20on%20carefully%20written%20text%0Aprompts%20for%20specific%20tasks%2C%20like%20inpainting%20or%20style%20editing.%20They%20require%0Alabor-intensive%20textual%20descriptions%20for%20input%20videos%2C%20hindering%20their%0Aflexibility%20to%20adapt%20personal/raw%20videos%20to%20user%20specifications.%20This%20paper%0Aproposes%20RACCooN%2C%20a%20versatile%20and%20user-friendly%20video-to-paragraph-to-video%0Agenerative%20framework%20that%20supports%20multiple%20video%20editing%20capabilities%20such%20as%0Aremoval%2C%20addition%2C%20and%20modification%2C%20through%20a%20unified%20pipeline.%20RACCooN%0Aconsists%20of%20two%20principal%20stages%3A%20Video-to-Paragraph%20%28V2P%29%20and%0AParagraph-to-Video%20%28P2V%29.%20In%20the%20V2P%20stage%2C%20we%20automatically%20describe%20video%0Ascenes%20in%20well-structured%20natural%20language%2C%20capturing%20both%20the%20holistic%20context%0Aand%20focused%20object%20details.%20Subsequently%2C%20in%20the%20P2V%20stage%2C%20users%20can%0Aoptionally%20refine%20these%20descriptions%20to%20guide%20the%20video%20diffusion%20model%2C%0Aenabling%20various%20modifications%20to%20the%20input%20video%2C%20such%20as%20removing%2C%20changing%0Asubjects%2C%20and/or%20adding%20new%20objects.%20The%20proposed%20approach%20stands%20out%20from%0Aother%20methods%20through%20several%20significant%20contributions%3A%20%281%29%20RACCooN%20suggests%20a%0Amulti-granular%20spatiotemporal%20pooling%20strategy%20to%20generate%20well-structured%0Avideo%20descriptions%2C%20capturing%20both%20the%20broad%20context%20and%20object%20details%20without%0Arequiring%20complex%20human%20annotations%2C%20simplifying%20precise%20video%20content%20editing%0Abased%20on%20text%20for%20users.%20%282%29%20Our%20video%20generative%20model%20incorporates%0Aauto-generated%20narratives%20or%20instructions%20to%20enhance%20the%20quality%20and%20accuracy%0Aof%20the%20generated%20content.%20%283%29%20RACCooN%20also%20plans%20to%20imagine%20new%20objects%20in%20a%0Agiven%20video%2C%20so%20users%20simply%20prompt%20the%20model%20to%20receive%20a%20detailed%20video%0Aediting%20plan%20for%20complex%20video%20editing.%20The%20proposed%20framework%20demonstrates%0Aimpressive%20versatile%20capabilities%20in%20video-to-paragraph%20generation%2C%20video%0Acontent%20editing%2C%20and%20can%20be%20incorporated%20into%20other%20SoTA%20video%20generative%0Amodels%20for%20further%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18406v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRACCooN%253A%2520A%2520Versatile%2520Instructional%2520Video%2520Editing%2520Framework%2520with%250A%2520%2520Auto-Generated%2520Narratives%26entry.906535625%3DJaehong%2520Yoon%2520and%2520Shoubin%2520Yu%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Recent%2520video%2520generative%2520models%2520primarily%2520rely%2520on%2520carefully%2520written%2520text%250Aprompts%2520for%2520specific%2520tasks%252C%2520like%2520inpainting%2520or%2520style%2520editing.%2520They%2520require%250Alabor-intensive%2520textual%2520descriptions%2520for%2520input%2520videos%252C%2520hindering%2520their%250Aflexibility%2520to%2520adapt%2520personal/raw%2520videos%2520to%2520user%2520specifications.%2520This%2520paper%250Aproposes%2520RACCooN%252C%2520a%2520versatile%2520and%2520user-friendly%2520video-to-paragraph-to-video%250Agenerative%2520framework%2520that%2520supports%2520multiple%2520video%2520editing%2520capabilities%2520such%2520as%250Aremoval%252C%2520addition%252C%2520and%2520modification%252C%2520through%2520a%2520unified%2520pipeline.%2520RACCooN%250Aconsists%2520of%2520two%2520principal%2520stages%253A%2520Video-to-Paragraph%2520%2528V2P%2529%2520and%250AParagraph-to-Video%2520%2528P2V%2529.%2520In%2520the%2520V2P%2520stage%252C%2520we%2520automatically%2520describe%2520video%250Ascenes%2520in%2520well-structured%2520natural%2520language%252C%2520capturing%2520both%2520the%2520holistic%2520context%250Aand%2520focused%2520object%2520details.%2520Subsequently%252C%2520in%2520the%2520P2V%2520stage%252C%2520users%2520can%250Aoptionally%2520refine%2520these%2520descriptions%2520to%2520guide%2520the%2520video%2520diffusion%2520model%252C%250Aenabling%2520various%2520modifications%2520to%2520the%2520input%2520video%252C%2520such%2520as%2520removing%252C%2520changing%250Asubjects%252C%2520and/or%2520adding%2520new%2520objects.%2520The%2520proposed%2520approach%2520stands%2520out%2520from%250Aother%2520methods%2520through%2520several%2520significant%2520contributions%253A%2520%25281%2529%2520RACCooN%2520suggests%2520a%250Amulti-granular%2520spatiotemporal%2520pooling%2520strategy%2520to%2520generate%2520well-structured%250Avideo%2520descriptions%252C%2520capturing%2520both%2520the%2520broad%2520context%2520and%2520object%2520details%2520without%250Arequiring%2520complex%2520human%2520annotations%252C%2520simplifying%2520precise%2520video%2520content%2520editing%250Abased%2520on%2520text%2520for%2520users.%2520%25282%2529%2520Our%2520video%2520generative%2520model%2520incorporates%250Aauto-generated%2520narratives%2520or%2520instructions%2520to%2520enhance%2520the%2520quality%2520and%2520accuracy%250Aof%2520the%2520generated%2520content.%2520%25283%2529%2520RACCooN%2520also%2520plans%2520to%2520imagine%2520new%2520objects%2520in%2520a%250Agiven%2520video%252C%2520so%2520users%2520simply%2520prompt%2520the%2520model%2520to%2520receive%2520a%2520detailed%2520video%250Aediting%2520plan%2520for%2520complex%2520video%2520editing.%2520The%2520proposed%2520framework%2520demonstrates%250Aimpressive%2520versatile%2520capabilities%2520in%2520video-to-paragraph%2520generation%252C%2520video%250Acontent%2520editing%252C%2520and%2520can%2520be%2520incorporated%2520into%2520other%2520SoTA%2520video%2520generative%250Amodels%2520for%2520further%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18406v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RACCooN%3A%20A%20Versatile%20Instructional%20Video%20Editing%20Framework%20with%0A%20%20Auto-Generated%20Narratives&entry.906535625=Jaehong%20Yoon%20and%20Shoubin%20Yu%20and%20Mohit%20Bansal&entry.1292438233=%20%20Recent%20video%20generative%20models%20primarily%20rely%20on%20carefully%20written%20text%0Aprompts%20for%20specific%20tasks%2C%20like%20inpainting%20or%20style%20editing.%20They%20require%0Alabor-intensive%20textual%20descriptions%20for%20input%20videos%2C%20hindering%20their%0Aflexibility%20to%20adapt%20personal/raw%20videos%20to%20user%20specifications.%20This%20paper%0Aproposes%20RACCooN%2C%20a%20versatile%20and%20user-friendly%20video-to-paragraph-to-video%0Agenerative%20framework%20that%20supports%20multiple%20video%20editing%20capabilities%20such%20as%0Aremoval%2C%20addition%2C%20and%20modification%2C%20through%20a%20unified%20pipeline.%20RACCooN%0Aconsists%20of%20two%20principal%20stages%3A%20Video-to-Paragraph%20%28V2P%29%20and%0AParagraph-to-Video%20%28P2V%29.%20In%20the%20V2P%20stage%2C%20we%20automatically%20describe%20video%0Ascenes%20in%20well-structured%20natural%20language%2C%20capturing%20both%20the%20holistic%20context%0Aand%20focused%20object%20details.%20Subsequently%2C%20in%20the%20P2V%20stage%2C%20users%20can%0Aoptionally%20refine%20these%20descriptions%20to%20guide%20the%20video%20diffusion%20model%2C%0Aenabling%20various%20modifications%20to%20the%20input%20video%2C%20such%20as%20removing%2C%20changing%0Asubjects%2C%20and/or%20adding%20new%20objects.%20The%20proposed%20approach%20stands%20out%20from%0Aother%20methods%20through%20several%20significant%20contributions%3A%20%281%29%20RACCooN%20suggests%20a%0Amulti-granular%20spatiotemporal%20pooling%20strategy%20to%20generate%20well-structured%0Avideo%20descriptions%2C%20capturing%20both%20the%20broad%20context%20and%20object%20details%20without%0Arequiring%20complex%20human%20annotations%2C%20simplifying%20precise%20video%20content%20editing%0Abased%20on%20text%20for%20users.%20%282%29%20Our%20video%20generative%20model%20incorporates%0Aauto-generated%20narratives%20or%20instructions%20to%20enhance%20the%20quality%20and%20accuracy%0Aof%20the%20generated%20content.%20%283%29%20RACCooN%20also%20plans%20to%20imagine%20new%20objects%20in%20a%0Agiven%20video%2C%20so%20users%20simply%20prompt%20the%20model%20to%20receive%20a%20detailed%20video%0Aediting%20plan%20for%20complex%20video%20editing.%20The%20proposed%20framework%20demonstrates%0Aimpressive%20versatile%20capabilities%20in%20video-to-paragraph%20generation%2C%20video%0Acontent%20editing%2C%20and%20can%20be%20incorporated%20into%20other%20SoTA%20video%20generative%0Amodels%20for%20further%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18406v4&entry.124074799=Read"},
{"title": "Topic Modeling as Long-Form Generation: Can Long-Context LLMs\n  revolutionize NTM via Zero-Shot Prompting?", "author": "Xuan Xu and Haolun Li and Zhongliang Yang and Beilin Chu and Jia Song and Moxuan Xu and Linna Zhou", "abstract": "  Traditional topic models such as neural topic models rely on inference and\ngeneration networks to learn latent topic distributions. This paper explores a\nnew paradigm for topic modeling in the era of large language models, framing TM\nas a long-form generation task whose definition is updated in this paradigm. We\npropose a simple but practical approach to implement LLM-based topic model\ntasks out of the box (sample a data subset, generate topics and representative\ntext with our prompt, text assignment with keyword match). We then investigate\nwhether the long-form generation paradigm can beat NTMs via zero-shot\nprompting. We conduct a systematic comparison between NTMs and LLMs in terms of\ntopic quality and empirically examine the claim that \"a majority of NTMs are\noutdated.\"\n", "link": "http://arxiv.org/abs/2510.03174v1", "date": "2025-10-03", "relevancy": 2.4224, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4791}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topic%20Modeling%20as%20Long-Form%20Generation%3A%20Can%20Long-Context%20LLMs%0A%20%20revolutionize%20NTM%20via%20Zero-Shot%20Prompting%3F&body=Title%3A%20Topic%20Modeling%20as%20Long-Form%20Generation%3A%20Can%20Long-Context%20LLMs%0A%20%20revolutionize%20NTM%20via%20Zero-Shot%20Prompting%3F%0AAuthor%3A%20Xuan%20Xu%20and%20Haolun%20Li%20and%20Zhongliang%20Yang%20and%20Beilin%20Chu%20and%20Jia%20Song%20and%20Moxuan%20Xu%20and%20Linna%20Zhou%0AAbstract%3A%20%20%20Traditional%20topic%20models%20such%20as%20neural%20topic%20models%20rely%20on%20inference%20and%0Ageneration%20networks%20to%20learn%20latent%20topic%20distributions.%20This%20paper%20explores%20a%0Anew%20paradigm%20for%20topic%20modeling%20in%20the%20era%20of%20large%20language%20models%2C%20framing%20TM%0Aas%20a%20long-form%20generation%20task%20whose%20definition%20is%20updated%20in%20this%20paradigm.%20We%0Apropose%20a%20simple%20but%20practical%20approach%20to%20implement%20LLM-based%20topic%20model%0Atasks%20out%20of%20the%20box%20%28sample%20a%20data%20subset%2C%20generate%20topics%20and%20representative%0Atext%20with%20our%20prompt%2C%20text%20assignment%20with%20keyword%20match%29.%20We%20then%20investigate%0Awhether%20the%20long-form%20generation%20paradigm%20can%20beat%20NTMs%20via%20zero-shot%0Aprompting.%20We%20conduct%20a%20systematic%20comparison%20between%20NTMs%20and%20LLMs%20in%20terms%20of%0Atopic%20quality%20and%20empirically%20examine%20the%20claim%20that%20%22a%20majority%20of%20NTMs%20are%0Aoutdated.%22%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopic%2520Modeling%2520as%2520Long-Form%2520Generation%253A%2520Can%2520Long-Context%2520LLMs%250A%2520%2520revolutionize%2520NTM%2520via%2520Zero-Shot%2520Prompting%253F%26entry.906535625%3DXuan%2520Xu%2520and%2520Haolun%2520Li%2520and%2520Zhongliang%2520Yang%2520and%2520Beilin%2520Chu%2520and%2520Jia%2520Song%2520and%2520Moxuan%2520Xu%2520and%2520Linna%2520Zhou%26entry.1292438233%3D%2520%2520Traditional%2520topic%2520models%2520such%2520as%2520neural%2520topic%2520models%2520rely%2520on%2520inference%2520and%250Ageneration%2520networks%2520to%2520learn%2520latent%2520topic%2520distributions.%2520This%2520paper%2520explores%2520a%250Anew%2520paradigm%2520for%2520topic%2520modeling%2520in%2520the%2520era%2520of%2520large%2520language%2520models%252C%2520framing%2520TM%250Aas%2520a%2520long-form%2520generation%2520task%2520whose%2520definition%2520is%2520updated%2520in%2520this%2520paradigm.%2520We%250Apropose%2520a%2520simple%2520but%2520practical%2520approach%2520to%2520implement%2520LLM-based%2520topic%2520model%250Atasks%2520out%2520of%2520the%2520box%2520%2528sample%2520a%2520data%2520subset%252C%2520generate%2520topics%2520and%2520representative%250Atext%2520with%2520our%2520prompt%252C%2520text%2520assignment%2520with%2520keyword%2520match%2529.%2520We%2520then%2520investigate%250Awhether%2520the%2520long-form%2520generation%2520paradigm%2520can%2520beat%2520NTMs%2520via%2520zero-shot%250Aprompting.%2520We%2520conduct%2520a%2520systematic%2520comparison%2520between%2520NTMs%2520and%2520LLMs%2520in%2520terms%2520of%250Atopic%2520quality%2520and%2520empirically%2520examine%2520the%2520claim%2520that%2520%2522a%2520majority%2520of%2520NTMs%2520are%250Aoutdated.%2522%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topic%20Modeling%20as%20Long-Form%20Generation%3A%20Can%20Long-Context%20LLMs%0A%20%20revolutionize%20NTM%20via%20Zero-Shot%20Prompting%3F&entry.906535625=Xuan%20Xu%20and%20Haolun%20Li%20and%20Zhongliang%20Yang%20and%20Beilin%20Chu%20and%20Jia%20Song%20and%20Moxuan%20Xu%20and%20Linna%20Zhou&entry.1292438233=%20%20Traditional%20topic%20models%20such%20as%20neural%20topic%20models%20rely%20on%20inference%20and%0Ageneration%20networks%20to%20learn%20latent%20topic%20distributions.%20This%20paper%20explores%20a%0Anew%20paradigm%20for%20topic%20modeling%20in%20the%20era%20of%20large%20language%20models%2C%20framing%20TM%0Aas%20a%20long-form%20generation%20task%20whose%20definition%20is%20updated%20in%20this%20paradigm.%20We%0Apropose%20a%20simple%20but%20practical%20approach%20to%20implement%20LLM-based%20topic%20model%0Atasks%20out%20of%20the%20box%20%28sample%20a%20data%20subset%2C%20generate%20topics%20and%20representative%0Atext%20with%20our%20prompt%2C%20text%20assignment%20with%20keyword%20match%29.%20We%20then%20investigate%0Awhether%20the%20long-form%20generation%20paradigm%20can%20beat%20NTMs%20via%20zero-shot%0Aprompting.%20We%20conduct%20a%20systematic%20comparison%20between%20NTMs%20and%20LLMs%20in%20terms%20of%0Atopic%20quality%20and%20empirically%20examine%20the%20claim%20that%20%22a%20majority%20of%20NTMs%20are%0Aoutdated.%22%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03174v1&entry.124074799=Read"},
{"title": "GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image\n  Completion", "author": "Beibei Lin and Tingting Chen and Robby T. Tan", "abstract": "  Reference-driven image completion, which restores missing regions in a target\nview using additional images, is particularly challenging when the target view\ndiffers significantly from the references. Existing generative methods rely\nsolely on diffusion priors and, without geometric cues such as camera pose or\ndepth, often produce misaligned or implausible content. We propose GeoComplete,\na novel framework that incorporates explicit 3D structural guidance to enforce\ngeometric consistency in the completed regions, setting it apart from prior\nimage-only approaches. GeoComplete introduces two key ideas: conditioning the\ndiffusion process on projected point clouds to infuse geometric information,\nand applying target-aware masking to guide the model toward relevant reference\ncues. The framework features a dual-branch diffusion architecture. One branch\nsynthesizes the missing regions from the masked target, while the other\nextracts geometric features from the projected point cloud. Joint\nself-attention across branches ensures coherent and accurate completion. To\naddress regions visible in references but absent in the target, we project the\ntarget view into each reference to detect occluded areas, which are then masked\nduring training. This target-aware masking directs the model to focus on useful\ncues, enhancing performance in difficult scenarios. By integrating a\ngeometry-aware dual-branch diffusion architecture with a target-aware masking\nstrategy, GeoComplete offers a unified and robust solution for\ngeometry-conditioned image completion. Experiments show that GeoComplete\nachieves a 17.1 PSNR improvement over state-of-the-art methods, significantly\nboosting geometric accuracy while maintaining high visual quality.\n", "link": "http://arxiv.org/abs/2510.03110v1", "date": "2025-10-03", "relevancy": 2.4011, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.64}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5923}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoComplete%3A%20Geometry-Aware%20Diffusion%20for%20Reference-Driven%20Image%0A%20%20Completion&body=Title%3A%20GeoComplete%3A%20Geometry-Aware%20Diffusion%20for%20Reference-Driven%20Image%0A%20%20Completion%0AAuthor%3A%20Beibei%20Lin%20and%20Tingting%20Chen%20and%20Robby%20T.%20Tan%0AAbstract%3A%20%20%20Reference-driven%20image%20completion%2C%20which%20restores%20missing%20regions%20in%20a%20target%0Aview%20using%20additional%20images%2C%20is%20particularly%20challenging%20when%20the%20target%20view%0Adiffers%20significantly%20from%20the%20references.%20Existing%20generative%20methods%20rely%0Asolely%20on%20diffusion%20priors%20and%2C%20without%20geometric%20cues%20such%20as%20camera%20pose%20or%0Adepth%2C%20often%20produce%20misaligned%20or%20implausible%20content.%20We%20propose%20GeoComplete%2C%0Aa%20novel%20framework%20that%20incorporates%20explicit%203D%20structural%20guidance%20to%20enforce%0Ageometric%20consistency%20in%20the%20completed%20regions%2C%20setting%20it%20apart%20from%20prior%0Aimage-only%20approaches.%20GeoComplete%20introduces%20two%20key%20ideas%3A%20conditioning%20the%0Adiffusion%20process%20on%20projected%20point%20clouds%20to%20infuse%20geometric%20information%2C%0Aand%20applying%20target-aware%20masking%20to%20guide%20the%20model%20toward%20relevant%20reference%0Acues.%20The%20framework%20features%20a%20dual-branch%20diffusion%20architecture.%20One%20branch%0Asynthesizes%20the%20missing%20regions%20from%20the%20masked%20target%2C%20while%20the%20other%0Aextracts%20geometric%20features%20from%20the%20projected%20point%20cloud.%20Joint%0Aself-attention%20across%20branches%20ensures%20coherent%20and%20accurate%20completion.%20To%0Aaddress%20regions%20visible%20in%20references%20but%20absent%20in%20the%20target%2C%20we%20project%20the%0Atarget%20view%20into%20each%20reference%20to%20detect%20occluded%20areas%2C%20which%20are%20then%20masked%0Aduring%20training.%20This%20target-aware%20masking%20directs%20the%20model%20to%20focus%20on%20useful%0Acues%2C%20enhancing%20performance%20in%20difficult%20scenarios.%20By%20integrating%20a%0Ageometry-aware%20dual-branch%20diffusion%20architecture%20with%20a%20target-aware%20masking%0Astrategy%2C%20GeoComplete%20offers%20a%20unified%20and%20robust%20solution%20for%0Ageometry-conditioned%20image%20completion.%20Experiments%20show%20that%20GeoComplete%0Aachieves%20a%2017.1%20PSNR%20improvement%20over%20state-of-the-art%20methods%2C%20significantly%0Aboosting%20geometric%20accuracy%20while%20maintaining%20high%20visual%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoComplete%253A%2520Geometry-Aware%2520Diffusion%2520for%2520Reference-Driven%2520Image%250A%2520%2520Completion%26entry.906535625%3DBeibei%2520Lin%2520and%2520Tingting%2520Chen%2520and%2520Robby%2520T.%2520Tan%26entry.1292438233%3D%2520%2520Reference-driven%2520image%2520completion%252C%2520which%2520restores%2520missing%2520regions%2520in%2520a%2520target%250Aview%2520using%2520additional%2520images%252C%2520is%2520particularly%2520challenging%2520when%2520the%2520target%2520view%250Adiffers%2520significantly%2520from%2520the%2520references.%2520Existing%2520generative%2520methods%2520rely%250Asolely%2520on%2520diffusion%2520priors%2520and%252C%2520without%2520geometric%2520cues%2520such%2520as%2520camera%2520pose%2520or%250Adepth%252C%2520often%2520produce%2520misaligned%2520or%2520implausible%2520content.%2520We%2520propose%2520GeoComplete%252C%250Aa%2520novel%2520framework%2520that%2520incorporates%2520explicit%25203D%2520structural%2520guidance%2520to%2520enforce%250Ageometric%2520consistency%2520in%2520the%2520completed%2520regions%252C%2520setting%2520it%2520apart%2520from%2520prior%250Aimage-only%2520approaches.%2520GeoComplete%2520introduces%2520two%2520key%2520ideas%253A%2520conditioning%2520the%250Adiffusion%2520process%2520on%2520projected%2520point%2520clouds%2520to%2520infuse%2520geometric%2520information%252C%250Aand%2520applying%2520target-aware%2520masking%2520to%2520guide%2520the%2520model%2520toward%2520relevant%2520reference%250Acues.%2520The%2520framework%2520features%2520a%2520dual-branch%2520diffusion%2520architecture.%2520One%2520branch%250Asynthesizes%2520the%2520missing%2520regions%2520from%2520the%2520masked%2520target%252C%2520while%2520the%2520other%250Aextracts%2520geometric%2520features%2520from%2520the%2520projected%2520point%2520cloud.%2520Joint%250Aself-attention%2520across%2520branches%2520ensures%2520coherent%2520and%2520accurate%2520completion.%2520To%250Aaddress%2520regions%2520visible%2520in%2520references%2520but%2520absent%2520in%2520the%2520target%252C%2520we%2520project%2520the%250Atarget%2520view%2520into%2520each%2520reference%2520to%2520detect%2520occluded%2520areas%252C%2520which%2520are%2520then%2520masked%250Aduring%2520training.%2520This%2520target-aware%2520masking%2520directs%2520the%2520model%2520to%2520focus%2520on%2520useful%250Acues%252C%2520enhancing%2520performance%2520in%2520difficult%2520scenarios.%2520By%2520integrating%2520a%250Ageometry-aware%2520dual-branch%2520diffusion%2520architecture%2520with%2520a%2520target-aware%2520masking%250Astrategy%252C%2520GeoComplete%2520offers%2520a%2520unified%2520and%2520robust%2520solution%2520for%250Ageometry-conditioned%2520image%2520completion.%2520Experiments%2520show%2520that%2520GeoComplete%250Aachieves%2520a%252017.1%2520PSNR%2520improvement%2520over%2520state-of-the-art%2520methods%252C%2520significantly%250Aboosting%2520geometric%2520accuracy%2520while%2520maintaining%2520high%2520visual%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoComplete%3A%20Geometry-Aware%20Diffusion%20for%20Reference-Driven%20Image%0A%20%20Completion&entry.906535625=Beibei%20Lin%20and%20Tingting%20Chen%20and%20Robby%20T.%20Tan&entry.1292438233=%20%20Reference-driven%20image%20completion%2C%20which%20restores%20missing%20regions%20in%20a%20target%0Aview%20using%20additional%20images%2C%20is%20particularly%20challenging%20when%20the%20target%20view%0Adiffers%20significantly%20from%20the%20references.%20Existing%20generative%20methods%20rely%0Asolely%20on%20diffusion%20priors%20and%2C%20without%20geometric%20cues%20such%20as%20camera%20pose%20or%0Adepth%2C%20often%20produce%20misaligned%20or%20implausible%20content.%20We%20propose%20GeoComplete%2C%0Aa%20novel%20framework%20that%20incorporates%20explicit%203D%20structural%20guidance%20to%20enforce%0Ageometric%20consistency%20in%20the%20completed%20regions%2C%20setting%20it%20apart%20from%20prior%0Aimage-only%20approaches.%20GeoComplete%20introduces%20two%20key%20ideas%3A%20conditioning%20the%0Adiffusion%20process%20on%20projected%20point%20clouds%20to%20infuse%20geometric%20information%2C%0Aand%20applying%20target-aware%20masking%20to%20guide%20the%20model%20toward%20relevant%20reference%0Acues.%20The%20framework%20features%20a%20dual-branch%20diffusion%20architecture.%20One%20branch%0Asynthesizes%20the%20missing%20regions%20from%20the%20masked%20target%2C%20while%20the%20other%0Aextracts%20geometric%20features%20from%20the%20projected%20point%20cloud.%20Joint%0Aself-attention%20across%20branches%20ensures%20coherent%20and%20accurate%20completion.%20To%0Aaddress%20regions%20visible%20in%20references%20but%20absent%20in%20the%20target%2C%20we%20project%20the%0Atarget%20view%20into%20each%20reference%20to%20detect%20occluded%20areas%2C%20which%20are%20then%20masked%0Aduring%20training.%20This%20target-aware%20masking%20directs%20the%20model%20to%20focus%20on%20useful%0Acues%2C%20enhancing%20performance%20in%20difficult%20scenarios.%20By%20integrating%20a%0Ageometry-aware%20dual-branch%20diffusion%20architecture%20with%20a%20target-aware%20masking%0Astrategy%2C%20GeoComplete%20offers%20a%20unified%20and%20robust%20solution%20for%0Ageometry-conditioned%20image%20completion.%20Experiments%20show%20that%20GeoComplete%0Aachieves%20a%2017.1%20PSNR%20improvement%20over%20state-of-the-art%20methods%2C%20significantly%0Aboosting%20geometric%20accuracy%20while%20maintaining%20high%20visual%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03110v1&entry.124074799=Read"},
{"title": "BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck\n  for Functional Brain Biomarkers in Schizophrenia", "author": "Tianzheng Hu and Qiang Li and Shu Liu and Vince D. Calhoun and Guido van Wingen and Shujian Yu", "abstract": "  The development of diagnostic models is gaining traction in the field of\npsychiatric disorders. Recently, machine learning classifiers based on\nresting-state functional magnetic resonance imaging (rs-fMRI) have been\ndeveloped to identify brain biomarkers that differentiate psychiatric disorders\nfrom healthy controls. However, conventional machine learning-based diagnostic\nmodels often depend on extensive feature engineering, which introduces bias\nthrough manual intervention. While deep learning models are expected to operate\nwithout manual involvement, their lack of interpretability poses significant\nchallenges in obtaining explainable and reliable brain biomarkers to support\ndiagnostic decisions, ultimately limiting their clinical applicability. In this\nstudy, we introduce an end-to-end innovative graph neural network framework\nnamed BrainIB++, which applies the information bottleneck (IB) principle to\nidentify the most informative data-driven brain regions as subgraphs during\nmodel training for interpretation. We evaluate the performance of our model\nagainst nine established brain network classification methods across three\nmulti-cohort schizophrenia datasets. It consistently demonstrates superior\ndiagnostic accuracy and exhibits generalizability to unseen data. Furthermore,\nthe subgraphs identified by our model also correspond with established clinical\nbiomarkers in schizophrenia, particularly emphasizing abnormalities in the\nvisual, sensorimotor, and higher cognition brain functional network. This\nalignment enhances the model's interpretability and underscores its relevance\nfor real-world diagnostic applications.\n", "link": "http://arxiv.org/abs/2510.03004v1", "date": "2025-10-03", "relevancy": 2.3954, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrainIB%2B%2B%3A%20Leveraging%20Graph%20Neural%20Networks%20and%20Information%20Bottleneck%0A%20%20for%20Functional%20Brain%20Biomarkers%20in%20Schizophrenia&body=Title%3A%20BrainIB%2B%2B%3A%20Leveraging%20Graph%20Neural%20Networks%20and%20Information%20Bottleneck%0A%20%20for%20Functional%20Brain%20Biomarkers%20in%20Schizophrenia%0AAuthor%3A%20Tianzheng%20Hu%20and%20Qiang%20Li%20and%20Shu%20Liu%20and%20Vince%20D.%20Calhoun%20and%20Guido%20van%20Wingen%20and%20Shujian%20Yu%0AAbstract%3A%20%20%20The%20development%20of%20diagnostic%20models%20is%20gaining%20traction%20in%20the%20field%20of%0Apsychiatric%20disorders.%20Recently%2C%20machine%20learning%20classifiers%20based%20on%0Aresting-state%20functional%20magnetic%20resonance%20imaging%20%28rs-fMRI%29%20have%20been%0Adeveloped%20to%20identify%20brain%20biomarkers%20that%20differentiate%20psychiatric%20disorders%0Afrom%20healthy%20controls.%20However%2C%20conventional%20machine%20learning-based%20diagnostic%0Amodels%20often%20depend%20on%20extensive%20feature%20engineering%2C%20which%20introduces%20bias%0Athrough%20manual%20intervention.%20While%20deep%20learning%20models%20are%20expected%20to%20operate%0Awithout%20manual%20involvement%2C%20their%20lack%20of%20interpretability%20poses%20significant%0Achallenges%20in%20obtaining%20explainable%20and%20reliable%20brain%20biomarkers%20to%20support%0Adiagnostic%20decisions%2C%20ultimately%20limiting%20their%20clinical%20applicability.%20In%20this%0Astudy%2C%20we%20introduce%20an%20end-to-end%20innovative%20graph%20neural%20network%20framework%0Anamed%20BrainIB%2B%2B%2C%20which%20applies%20the%20information%20bottleneck%20%28IB%29%20principle%20to%0Aidentify%20the%20most%20informative%20data-driven%20brain%20regions%20as%20subgraphs%20during%0Amodel%20training%20for%20interpretation.%20We%20evaluate%20the%20performance%20of%20our%20model%0Aagainst%20nine%20established%20brain%20network%20classification%20methods%20across%20three%0Amulti-cohort%20schizophrenia%20datasets.%20It%20consistently%20demonstrates%20superior%0Adiagnostic%20accuracy%20and%20exhibits%20generalizability%20to%20unseen%20data.%20Furthermore%2C%0Athe%20subgraphs%20identified%20by%20our%20model%20also%20correspond%20with%20established%20clinical%0Abiomarkers%20in%20schizophrenia%2C%20particularly%20emphasizing%20abnormalities%20in%20the%0Avisual%2C%20sensorimotor%2C%20and%20higher%20cognition%20brain%20functional%20network.%20This%0Aalignment%20enhances%20the%20model%27s%20interpretability%20and%20underscores%20its%20relevance%0Afor%20real-world%20diagnostic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrainIB%252B%252B%253A%2520Leveraging%2520Graph%2520Neural%2520Networks%2520and%2520Information%2520Bottleneck%250A%2520%2520for%2520Functional%2520Brain%2520Biomarkers%2520in%2520Schizophrenia%26entry.906535625%3DTianzheng%2520Hu%2520and%2520Qiang%2520Li%2520and%2520Shu%2520Liu%2520and%2520Vince%2520D.%2520Calhoun%2520and%2520Guido%2520van%2520Wingen%2520and%2520Shujian%2520Yu%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520diagnostic%2520models%2520is%2520gaining%2520traction%2520in%2520the%2520field%2520of%250Apsychiatric%2520disorders.%2520Recently%252C%2520machine%2520learning%2520classifiers%2520based%2520on%250Aresting-state%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528rs-fMRI%2529%2520have%2520been%250Adeveloped%2520to%2520identify%2520brain%2520biomarkers%2520that%2520differentiate%2520psychiatric%2520disorders%250Afrom%2520healthy%2520controls.%2520However%252C%2520conventional%2520machine%2520learning-based%2520diagnostic%250Amodels%2520often%2520depend%2520on%2520extensive%2520feature%2520engineering%252C%2520which%2520introduces%2520bias%250Athrough%2520manual%2520intervention.%2520While%2520deep%2520learning%2520models%2520are%2520expected%2520to%2520operate%250Awithout%2520manual%2520involvement%252C%2520their%2520lack%2520of%2520interpretability%2520poses%2520significant%250Achallenges%2520in%2520obtaining%2520explainable%2520and%2520reliable%2520brain%2520biomarkers%2520to%2520support%250Adiagnostic%2520decisions%252C%2520ultimately%2520limiting%2520their%2520clinical%2520applicability.%2520In%2520this%250Astudy%252C%2520we%2520introduce%2520an%2520end-to-end%2520innovative%2520graph%2520neural%2520network%2520framework%250Anamed%2520BrainIB%252B%252B%252C%2520which%2520applies%2520the%2520information%2520bottleneck%2520%2528IB%2529%2520principle%2520to%250Aidentify%2520the%2520most%2520informative%2520data-driven%2520brain%2520regions%2520as%2520subgraphs%2520during%250Amodel%2520training%2520for%2520interpretation.%2520We%2520evaluate%2520the%2520performance%2520of%2520our%2520model%250Aagainst%2520nine%2520established%2520brain%2520network%2520classification%2520methods%2520across%2520three%250Amulti-cohort%2520schizophrenia%2520datasets.%2520It%2520consistently%2520demonstrates%2520superior%250Adiagnostic%2520accuracy%2520and%2520exhibits%2520generalizability%2520to%2520unseen%2520data.%2520Furthermore%252C%250Athe%2520subgraphs%2520identified%2520by%2520our%2520model%2520also%2520correspond%2520with%2520established%2520clinical%250Abiomarkers%2520in%2520schizophrenia%252C%2520particularly%2520emphasizing%2520abnormalities%2520in%2520the%250Avisual%252C%2520sensorimotor%252C%2520and%2520higher%2520cognition%2520brain%2520functional%2520network.%2520This%250Aalignment%2520enhances%2520the%2520model%2527s%2520interpretability%2520and%2520underscores%2520its%2520relevance%250Afor%2520real-world%2520diagnostic%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrainIB%2B%2B%3A%20Leveraging%20Graph%20Neural%20Networks%20and%20Information%20Bottleneck%0A%20%20for%20Functional%20Brain%20Biomarkers%20in%20Schizophrenia&entry.906535625=Tianzheng%20Hu%20and%20Qiang%20Li%20and%20Shu%20Liu%20and%20Vince%20D.%20Calhoun%20and%20Guido%20van%20Wingen%20and%20Shujian%20Yu&entry.1292438233=%20%20The%20development%20of%20diagnostic%20models%20is%20gaining%20traction%20in%20the%20field%20of%0Apsychiatric%20disorders.%20Recently%2C%20machine%20learning%20classifiers%20based%20on%0Aresting-state%20functional%20magnetic%20resonance%20imaging%20%28rs-fMRI%29%20have%20been%0Adeveloped%20to%20identify%20brain%20biomarkers%20that%20differentiate%20psychiatric%20disorders%0Afrom%20healthy%20controls.%20However%2C%20conventional%20machine%20learning-based%20diagnostic%0Amodels%20often%20depend%20on%20extensive%20feature%20engineering%2C%20which%20introduces%20bias%0Athrough%20manual%20intervention.%20While%20deep%20learning%20models%20are%20expected%20to%20operate%0Awithout%20manual%20involvement%2C%20their%20lack%20of%20interpretability%20poses%20significant%0Achallenges%20in%20obtaining%20explainable%20and%20reliable%20brain%20biomarkers%20to%20support%0Adiagnostic%20decisions%2C%20ultimately%20limiting%20their%20clinical%20applicability.%20In%20this%0Astudy%2C%20we%20introduce%20an%20end-to-end%20innovative%20graph%20neural%20network%20framework%0Anamed%20BrainIB%2B%2B%2C%20which%20applies%20the%20information%20bottleneck%20%28IB%29%20principle%20to%0Aidentify%20the%20most%20informative%20data-driven%20brain%20regions%20as%20subgraphs%20during%0Amodel%20training%20for%20interpretation.%20We%20evaluate%20the%20performance%20of%20our%20model%0Aagainst%20nine%20established%20brain%20network%20classification%20methods%20across%20three%0Amulti-cohort%20schizophrenia%20datasets.%20It%20consistently%20demonstrates%20superior%0Adiagnostic%20accuracy%20and%20exhibits%20generalizability%20to%20unseen%20data.%20Furthermore%2C%0Athe%20subgraphs%20identified%20by%20our%20model%20also%20correspond%20with%20established%20clinical%0Abiomarkers%20in%20schizophrenia%2C%20particularly%20emphasizing%20abnormalities%20in%20the%0Avisual%2C%20sensorimotor%2C%20and%20higher%20cognition%20brain%20functional%20network.%20This%0Aalignment%20enhances%20the%20model%27s%20interpretability%20and%20underscores%20its%20relevance%0Afor%20real-world%20diagnostic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03004v1&entry.124074799=Read"},
{"title": "Pack and Force Your Memory: Long-form and Consistent Video Generation", "author": "Xiaofei Wu and Guozhen Zhang and Zhiyong Xu and Yuan Zhou and Qinglin Lu and Xuming He", "abstract": "  Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models.\n", "link": "http://arxiv.org/abs/2510.01784v2", "date": "2025-10-03", "relevancy": 2.337, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5939}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5834}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pack%20and%20Force%20Your%20Memory%3A%20Long-form%20and%20Consistent%20Video%20Generation&body=Title%3A%20Pack%20and%20Force%20Your%20Memory%3A%20Long-form%20and%20Consistent%20Video%20Generation%0AAuthor%3A%20Xiaofei%20Wu%20and%20Guozhen%20Zhang%20and%20Zhiyong%20Xu%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Xuming%20He%0AAbstract%3A%20%20%20Long-form%20video%20generation%20presents%20a%20dual%20challenge%3A%20models%20must%20capture%0Along-range%20dependencies%20while%20preventing%20the%20error%20accumulation%20inherent%20in%0Aautoregressive%20decoding.%20To%20address%20these%20challenges%2C%20we%20make%20two%0Acontributions.%20First%2C%20for%20dynamic%20context%20modeling%2C%20we%20propose%20MemoryPack%2C%20a%0Alearnable%20context-retrieval%20mechanism%20that%20leverages%20both%20textual%20and%20image%0Ainformation%20as%20global%20guidance%20to%20jointly%20model%20short-%20and%20long-term%0Adependencies%2C%20achieving%20minute-level%20temporal%20consistency.%20This%20design%20scales%0Agracefully%20with%20video%20length%2C%20preserves%20computational%20efficiency%2C%20and%20maintains%0Alinear%20complexity.%20Second%2C%20to%20mitigate%20error%20accumulation%2C%20we%20introduce%20Direct%0AForcing%2C%20an%20efficient%20single-step%20approximating%20strategy%20that%20improves%0Atraining-inference%20alignment%20and%20thereby%20curtails%20error%20propagation%20during%0Ainference.%20Together%2C%20MemoryPack%20and%20Direct%20Forcing%20substantially%20enhance%20the%0Acontext%20consistency%20and%20reliability%20of%20long-form%20video%20generation%2C%20advancing%0Athe%20practical%20usability%20of%20autoregressive%20video%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPack%2520and%2520Force%2520Your%2520Memory%253A%2520Long-form%2520and%2520Consistent%2520Video%2520Generation%26entry.906535625%3DXiaofei%2520Wu%2520and%2520Guozhen%2520Zhang%2520and%2520Zhiyong%2520Xu%2520and%2520Yuan%2520Zhou%2520and%2520Qinglin%2520Lu%2520and%2520Xuming%2520He%26entry.1292438233%3D%2520%2520Long-form%2520video%2520generation%2520presents%2520a%2520dual%2520challenge%253A%2520models%2520must%2520capture%250Along-range%2520dependencies%2520while%2520preventing%2520the%2520error%2520accumulation%2520inherent%2520in%250Aautoregressive%2520decoding.%2520To%2520address%2520these%2520challenges%252C%2520we%2520make%2520two%250Acontributions.%2520First%252C%2520for%2520dynamic%2520context%2520modeling%252C%2520we%2520propose%2520MemoryPack%252C%2520a%250Alearnable%2520context-retrieval%2520mechanism%2520that%2520leverages%2520both%2520textual%2520and%2520image%250Ainformation%2520as%2520global%2520guidance%2520to%2520jointly%2520model%2520short-%2520and%2520long-term%250Adependencies%252C%2520achieving%2520minute-level%2520temporal%2520consistency.%2520This%2520design%2520scales%250Agracefully%2520with%2520video%2520length%252C%2520preserves%2520computational%2520efficiency%252C%2520and%2520maintains%250Alinear%2520complexity.%2520Second%252C%2520to%2520mitigate%2520error%2520accumulation%252C%2520we%2520introduce%2520Direct%250AForcing%252C%2520an%2520efficient%2520single-step%2520approximating%2520strategy%2520that%2520improves%250Atraining-inference%2520alignment%2520and%2520thereby%2520curtails%2520error%2520propagation%2520during%250Ainference.%2520Together%252C%2520MemoryPack%2520and%2520Direct%2520Forcing%2520substantially%2520enhance%2520the%250Acontext%2520consistency%2520and%2520reliability%2520of%2520long-form%2520video%2520generation%252C%2520advancing%250Athe%2520practical%2520usability%2520of%2520autoregressive%2520video%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pack%20and%20Force%20Your%20Memory%3A%20Long-form%20and%20Consistent%20Video%20Generation&entry.906535625=Xiaofei%20Wu%20and%20Guozhen%20Zhang%20and%20Zhiyong%20Xu%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Xuming%20He&entry.1292438233=%20%20Long-form%20video%20generation%20presents%20a%20dual%20challenge%3A%20models%20must%20capture%0Along-range%20dependencies%20while%20preventing%20the%20error%20accumulation%20inherent%20in%0Aautoregressive%20decoding.%20To%20address%20these%20challenges%2C%20we%20make%20two%0Acontributions.%20First%2C%20for%20dynamic%20context%20modeling%2C%20we%20propose%20MemoryPack%2C%20a%0Alearnable%20context-retrieval%20mechanism%20that%20leverages%20both%20textual%20and%20image%0Ainformation%20as%20global%20guidance%20to%20jointly%20model%20short-%20and%20long-term%0Adependencies%2C%20achieving%20minute-level%20temporal%20consistency.%20This%20design%20scales%0Agracefully%20with%20video%20length%2C%20preserves%20computational%20efficiency%2C%20and%20maintains%0Alinear%20complexity.%20Second%2C%20to%20mitigate%20error%20accumulation%2C%20we%20introduce%20Direct%0AForcing%2C%20an%20efficient%20single-step%20approximating%20strategy%20that%20improves%0Atraining-inference%20alignment%20and%20thereby%20curtails%20error%20propagation%20during%0Ainference.%20Together%2C%20MemoryPack%20and%20Direct%20Forcing%20substantially%20enhance%20the%0Acontext%20consistency%20and%20reliability%20of%20long-form%20video%20generation%2C%20advancing%0Athe%20practical%20usability%20of%20autoregressive%20video%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01784v2&entry.124074799=Read"},
{"title": "PocketSR: The Super-Resolution Expert in Your Pocket Mobiles", "author": "Haoze Sun and Linfeng Jiang and Fan Li and Renjing Pei and Zhixin Wang and Yong Guo and Jiaqi Xu and Haoyu Chen and Jin Han and Fenglong Song and Yujiu Yang and Wenbo Li", "abstract": "  Real-world image super-resolution (RealSR) aims to enhance the visual quality\nof in-the-wild images, such as those captured by mobile phones. While existing\nmethods leveraging large generative models demonstrate impressive results, the\nhigh computational cost and latency make them impractical for edge deployment.\nIn this paper, we introduce PocketSR, an ultra-lightweight, single-step model\nthat brings generative modeling capabilities to RealSR while maintaining high\nfidelity. To achieve this, we design LiteED, a highly efficient alternative to\nthe original computationally intensive VAE in SD, reducing parameters by 97.5%\nwhile preserving high-quality encoding and decoding. Additionally, we propose\nonline annealing pruning for the U-Net, which progressively shifts generative\npriors from heavy modules to lightweight counterparts, ensuring effective\nknowledge transfer and further optimizing efficiency. To mitigate the loss of\nprior knowledge during pruning, we incorporate a multi-layer feature\ndistillation loss. Through an in-depth analysis of each design component, we\nprovide valuable insights for future research. PocketSR, with a model size of\n146M parameters, processes 4K images in just 0.8 seconds, achieving a\nremarkable speedup over previous methods. Notably, it delivers performance on\npar with state-of-the-art single-step and even multi-step RealSR models, making\nit a highly practical solution for edge-device applications.\n", "link": "http://arxiv.org/abs/2510.03012v1", "date": "2025-10-03", "relevancy": 2.335, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6303}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5895}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PocketSR%3A%20The%20Super-Resolution%20Expert%20in%20Your%20Pocket%20Mobiles&body=Title%3A%20PocketSR%3A%20The%20Super-Resolution%20Expert%20in%20Your%20Pocket%20Mobiles%0AAuthor%3A%20Haoze%20Sun%20and%20Linfeng%20Jiang%20and%20Fan%20Li%20and%20Renjing%20Pei%20and%20Zhixin%20Wang%20and%20Yong%20Guo%20and%20Jiaqi%20Xu%20and%20Haoyu%20Chen%20and%20Jin%20Han%20and%20Fenglong%20Song%20and%20Yujiu%20Yang%20and%20Wenbo%20Li%0AAbstract%3A%20%20%20Real-world%20image%20super-resolution%20%28RealSR%29%20aims%20to%20enhance%20the%20visual%20quality%0Aof%20in-the-wild%20images%2C%20such%20as%20those%20captured%20by%20mobile%20phones.%20While%20existing%0Amethods%20leveraging%20large%20generative%20models%20demonstrate%20impressive%20results%2C%20the%0Ahigh%20computational%20cost%20and%20latency%20make%20them%20impractical%20for%20edge%20deployment.%0AIn%20this%20paper%2C%20we%20introduce%20PocketSR%2C%20an%20ultra-lightweight%2C%20single-step%20model%0Athat%20brings%20generative%20modeling%20capabilities%20to%20RealSR%20while%20maintaining%20high%0Afidelity.%20To%20achieve%20this%2C%20we%20design%20LiteED%2C%20a%20highly%20efficient%20alternative%20to%0Athe%20original%20computationally%20intensive%20VAE%20in%20SD%2C%20reducing%20parameters%20by%2097.5%25%0Awhile%20preserving%20high-quality%20encoding%20and%20decoding.%20Additionally%2C%20we%20propose%0Aonline%20annealing%20pruning%20for%20the%20U-Net%2C%20which%20progressively%20shifts%20generative%0Apriors%20from%20heavy%20modules%20to%20lightweight%20counterparts%2C%20ensuring%20effective%0Aknowledge%20transfer%20and%20further%20optimizing%20efficiency.%20To%20mitigate%20the%20loss%20of%0Aprior%20knowledge%20during%20pruning%2C%20we%20incorporate%20a%20multi-layer%20feature%0Adistillation%20loss.%20Through%20an%20in-depth%20analysis%20of%20each%20design%20component%2C%20we%0Aprovide%20valuable%20insights%20for%20future%20research.%20PocketSR%2C%20with%20a%20model%20size%20of%0A146M%20parameters%2C%20processes%204K%20images%20in%20just%200.8%20seconds%2C%20achieving%20a%0Aremarkable%20speedup%20over%20previous%20methods.%20Notably%2C%20it%20delivers%20performance%20on%0Apar%20with%20state-of-the-art%20single-step%20and%20even%20multi-step%20RealSR%20models%2C%20making%0Ait%20a%20highly%20practical%20solution%20for%20edge-device%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPocketSR%253A%2520The%2520Super-Resolution%2520Expert%2520in%2520Your%2520Pocket%2520Mobiles%26entry.906535625%3DHaoze%2520Sun%2520and%2520Linfeng%2520Jiang%2520and%2520Fan%2520Li%2520and%2520Renjing%2520Pei%2520and%2520Zhixin%2520Wang%2520and%2520Yong%2520Guo%2520and%2520Jiaqi%2520Xu%2520and%2520Haoyu%2520Chen%2520and%2520Jin%2520Han%2520and%2520Fenglong%2520Song%2520and%2520Yujiu%2520Yang%2520and%2520Wenbo%2520Li%26entry.1292438233%3D%2520%2520Real-world%2520image%2520super-resolution%2520%2528RealSR%2529%2520aims%2520to%2520enhance%2520the%2520visual%2520quality%250Aof%2520in-the-wild%2520images%252C%2520such%2520as%2520those%2520captured%2520by%2520mobile%2520phones.%2520While%2520existing%250Amethods%2520leveraging%2520large%2520generative%2520models%2520demonstrate%2520impressive%2520results%252C%2520the%250Ahigh%2520computational%2520cost%2520and%2520latency%2520make%2520them%2520impractical%2520for%2520edge%2520deployment.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520PocketSR%252C%2520an%2520ultra-lightweight%252C%2520single-step%2520model%250Athat%2520brings%2520generative%2520modeling%2520capabilities%2520to%2520RealSR%2520while%2520maintaining%2520high%250Afidelity.%2520To%2520achieve%2520this%252C%2520we%2520design%2520LiteED%252C%2520a%2520highly%2520efficient%2520alternative%2520to%250Athe%2520original%2520computationally%2520intensive%2520VAE%2520in%2520SD%252C%2520reducing%2520parameters%2520by%252097.5%2525%250Awhile%2520preserving%2520high-quality%2520encoding%2520and%2520decoding.%2520Additionally%252C%2520we%2520propose%250Aonline%2520annealing%2520pruning%2520for%2520the%2520U-Net%252C%2520which%2520progressively%2520shifts%2520generative%250Apriors%2520from%2520heavy%2520modules%2520to%2520lightweight%2520counterparts%252C%2520ensuring%2520effective%250Aknowledge%2520transfer%2520and%2520further%2520optimizing%2520efficiency.%2520To%2520mitigate%2520the%2520loss%2520of%250Aprior%2520knowledge%2520during%2520pruning%252C%2520we%2520incorporate%2520a%2520multi-layer%2520feature%250Adistillation%2520loss.%2520Through%2520an%2520in-depth%2520analysis%2520of%2520each%2520design%2520component%252C%2520we%250Aprovide%2520valuable%2520insights%2520for%2520future%2520research.%2520PocketSR%252C%2520with%2520a%2520model%2520size%2520of%250A146M%2520parameters%252C%2520processes%25204K%2520images%2520in%2520just%25200.8%2520seconds%252C%2520achieving%2520a%250Aremarkable%2520speedup%2520over%2520previous%2520methods.%2520Notably%252C%2520it%2520delivers%2520performance%2520on%250Apar%2520with%2520state-of-the-art%2520single-step%2520and%2520even%2520multi-step%2520RealSR%2520models%252C%2520making%250Ait%2520a%2520highly%2520practical%2520solution%2520for%2520edge-device%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PocketSR%3A%20The%20Super-Resolution%20Expert%20in%20Your%20Pocket%20Mobiles&entry.906535625=Haoze%20Sun%20and%20Linfeng%20Jiang%20and%20Fan%20Li%20and%20Renjing%20Pei%20and%20Zhixin%20Wang%20and%20Yong%20Guo%20and%20Jiaqi%20Xu%20and%20Haoyu%20Chen%20and%20Jin%20Han%20and%20Fenglong%20Song%20and%20Yujiu%20Yang%20and%20Wenbo%20Li&entry.1292438233=%20%20Real-world%20image%20super-resolution%20%28RealSR%29%20aims%20to%20enhance%20the%20visual%20quality%0Aof%20in-the-wild%20images%2C%20such%20as%20those%20captured%20by%20mobile%20phones.%20While%20existing%0Amethods%20leveraging%20large%20generative%20models%20demonstrate%20impressive%20results%2C%20the%0Ahigh%20computational%20cost%20and%20latency%20make%20them%20impractical%20for%20edge%20deployment.%0AIn%20this%20paper%2C%20we%20introduce%20PocketSR%2C%20an%20ultra-lightweight%2C%20single-step%20model%0Athat%20brings%20generative%20modeling%20capabilities%20to%20RealSR%20while%20maintaining%20high%0Afidelity.%20To%20achieve%20this%2C%20we%20design%20LiteED%2C%20a%20highly%20efficient%20alternative%20to%0Athe%20original%20computationally%20intensive%20VAE%20in%20SD%2C%20reducing%20parameters%20by%2097.5%25%0Awhile%20preserving%20high-quality%20encoding%20and%20decoding.%20Additionally%2C%20we%20propose%0Aonline%20annealing%20pruning%20for%20the%20U-Net%2C%20which%20progressively%20shifts%20generative%0Apriors%20from%20heavy%20modules%20to%20lightweight%20counterparts%2C%20ensuring%20effective%0Aknowledge%20transfer%20and%20further%20optimizing%20efficiency.%20To%20mitigate%20the%20loss%20of%0Aprior%20knowledge%20during%20pruning%2C%20we%20incorporate%20a%20multi-layer%20feature%0Adistillation%20loss.%20Through%20an%20in-depth%20analysis%20of%20each%20design%20component%2C%20we%0Aprovide%20valuable%20insights%20for%20future%20research.%20PocketSR%2C%20with%20a%20model%20size%20of%0A146M%20parameters%2C%20processes%204K%20images%20in%20just%200.8%20seconds%2C%20achieving%20a%0Aremarkable%20speedup%20over%20previous%20methods.%20Notably%2C%20it%20delivers%20performance%20on%0Apar%20with%20state-of-the-art%20single-step%20and%20even%20multi-step%20RealSR%20models%2C%20making%0Ait%20a%20highly%20practical%20solution%20for%20edge-device%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03012v1&entry.124074799=Read"},
{"title": "RelayFormer: A Unified Local-Global Attention Framework for Scalable\n  Image and Video Manipulation Localization", "author": "Wen Huang and Jiarui Yang and Tao Dai and Jiawei Li and Shaoxiong Zhan and Bin Wang and Shu-Tao Xia", "abstract": "  Visual manipulation localization (VML) aims to identify tampered regions in\nimages and videos, a task that has become increasingly challenging with the\nrise of advanced editing tools. Existing methods face two main issues:\nresolution diversity, where resizing or padding distorts forensic traces and\nreduces efficiency, and the modality gap, as images and videos often require\nseparate models. To address these challenges, we propose RelayFormer, a unified\nframework that adapts to varying resolutions and modalities. RelayFormer\npartitions inputs into fixed-size sub-images and introduces Global-Local Relay\n(GLR) tokens, which propagate structured context through a global-local relay\nattention (GLRA) mechanism. This enables efficient exchange of global cues,\nsuch as semantic or temporal consistency, while preserving fine-grained\nmanipulation artifacts. Unlike prior methods that rely on uniform resizing or\nsparse attention, RelayFormer naturally scales to arbitrary resolutions and\nvideo sequences without excessive overhead. Experiments across diverse\nbenchmarks demonstrate that RelayFormer achieves state-of-the-art performance\nwith notable efficiency, combining resolution adaptivity without interpolation\nor excessive padding, unified modeling for both images and videos, and a strong\nbalance between accuracy and computational cost. Code is available at:\nhttps://github.com/WenOOI/RelayFormer.\n", "link": "http://arxiv.org/abs/2508.09459v2", "date": "2025-10-03", "relevancy": 2.3339, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5923}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5819}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelayFormer%3A%20A%20Unified%20Local-Global%20Attention%20Framework%20for%20Scalable%0A%20%20Image%20and%20Video%20Manipulation%20Localization&body=Title%3A%20RelayFormer%3A%20A%20Unified%20Local-Global%20Attention%20Framework%20for%20Scalable%0A%20%20Image%20and%20Video%20Manipulation%20Localization%0AAuthor%3A%20Wen%20Huang%20and%20Jiarui%20Yang%20and%20Tao%20Dai%20and%20Jiawei%20Li%20and%20Shaoxiong%20Zhan%20and%20Bin%20Wang%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Visual%20manipulation%20localization%20%28VML%29%20aims%20to%20identify%20tampered%20regions%20in%0Aimages%20and%20videos%2C%20a%20task%20that%20has%20become%20increasingly%20challenging%20with%20the%0Arise%20of%20advanced%20editing%20tools.%20Existing%20methods%20face%20two%20main%20issues%3A%0Aresolution%20diversity%2C%20where%20resizing%20or%20padding%20distorts%20forensic%20traces%20and%0Areduces%20efficiency%2C%20and%20the%20modality%20gap%2C%20as%20images%20and%20videos%20often%20require%0Aseparate%20models.%20To%20address%20these%20challenges%2C%20we%20propose%20RelayFormer%2C%20a%20unified%0Aframework%20that%20adapts%20to%20varying%20resolutions%20and%20modalities.%20RelayFormer%0Apartitions%20inputs%20into%20fixed-size%20sub-images%20and%20introduces%20Global-Local%20Relay%0A%28GLR%29%20tokens%2C%20which%20propagate%20structured%20context%20through%20a%20global-local%20relay%0Aattention%20%28GLRA%29%20mechanism.%20This%20enables%20efficient%20exchange%20of%20global%20cues%2C%0Asuch%20as%20semantic%20or%20temporal%20consistency%2C%20while%20preserving%20fine-grained%0Amanipulation%20artifacts.%20Unlike%20prior%20methods%20that%20rely%20on%20uniform%20resizing%20or%0Asparse%20attention%2C%20RelayFormer%20naturally%20scales%20to%20arbitrary%20resolutions%20and%0Avideo%20sequences%20without%20excessive%20overhead.%20Experiments%20across%20diverse%0Abenchmarks%20demonstrate%20that%20RelayFormer%20achieves%20state-of-the-art%20performance%0Awith%20notable%20efficiency%2C%20combining%20resolution%20adaptivity%20without%20interpolation%0Aor%20excessive%20padding%2C%20unified%20modeling%20for%20both%20images%20and%20videos%2C%20and%20a%20strong%0Abalance%20between%20accuracy%20and%20computational%20cost.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/WenOOI/RelayFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelayFormer%253A%2520A%2520Unified%2520Local-Global%2520Attention%2520Framework%2520for%2520Scalable%250A%2520%2520Image%2520and%2520Video%2520Manipulation%2520Localization%26entry.906535625%3DWen%2520Huang%2520and%2520Jiarui%2520Yang%2520and%2520Tao%2520Dai%2520and%2520Jiawei%2520Li%2520and%2520Shaoxiong%2520Zhan%2520and%2520Bin%2520Wang%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Visual%2520manipulation%2520localization%2520%2528VML%2529%2520aims%2520to%2520identify%2520tampered%2520regions%2520in%250Aimages%2520and%2520videos%252C%2520a%2520task%2520that%2520has%2520become%2520increasingly%2520challenging%2520with%2520the%250Arise%2520of%2520advanced%2520editing%2520tools.%2520Existing%2520methods%2520face%2520two%2520main%2520issues%253A%250Aresolution%2520diversity%252C%2520where%2520resizing%2520or%2520padding%2520distorts%2520forensic%2520traces%2520and%250Areduces%2520efficiency%252C%2520and%2520the%2520modality%2520gap%252C%2520as%2520images%2520and%2520videos%2520often%2520require%250Aseparate%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520RelayFormer%252C%2520a%2520unified%250Aframework%2520that%2520adapts%2520to%2520varying%2520resolutions%2520and%2520modalities.%2520RelayFormer%250Apartitions%2520inputs%2520into%2520fixed-size%2520sub-images%2520and%2520introduces%2520Global-Local%2520Relay%250A%2528GLR%2529%2520tokens%252C%2520which%2520propagate%2520structured%2520context%2520through%2520a%2520global-local%2520relay%250Aattention%2520%2528GLRA%2529%2520mechanism.%2520This%2520enables%2520efficient%2520exchange%2520of%2520global%2520cues%252C%250Asuch%2520as%2520semantic%2520or%2520temporal%2520consistency%252C%2520while%2520preserving%2520fine-grained%250Amanipulation%2520artifacts.%2520Unlike%2520prior%2520methods%2520that%2520rely%2520on%2520uniform%2520resizing%2520or%250Asparse%2520attention%252C%2520RelayFormer%2520naturally%2520scales%2520to%2520arbitrary%2520resolutions%2520and%250Avideo%2520sequences%2520without%2520excessive%2520overhead.%2520Experiments%2520across%2520diverse%250Abenchmarks%2520demonstrate%2520that%2520RelayFormer%2520achieves%2520state-of-the-art%2520performance%250Awith%2520notable%2520efficiency%252C%2520combining%2520resolution%2520adaptivity%2520without%2520interpolation%250Aor%2520excessive%2520padding%252C%2520unified%2520modeling%2520for%2520both%2520images%2520and%2520videos%252C%2520and%2520a%2520strong%250Abalance%2520between%2520accuracy%2520and%2520computational%2520cost.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/WenOOI/RelayFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelayFormer%3A%20A%20Unified%20Local-Global%20Attention%20Framework%20for%20Scalable%0A%20%20Image%20and%20Video%20Manipulation%20Localization&entry.906535625=Wen%20Huang%20and%20Jiarui%20Yang%20and%20Tao%20Dai%20and%20Jiawei%20Li%20and%20Shaoxiong%20Zhan%20and%20Bin%20Wang%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Visual%20manipulation%20localization%20%28VML%29%20aims%20to%20identify%20tampered%20regions%20in%0Aimages%20and%20videos%2C%20a%20task%20that%20has%20become%20increasingly%20challenging%20with%20the%0Arise%20of%20advanced%20editing%20tools.%20Existing%20methods%20face%20two%20main%20issues%3A%0Aresolution%20diversity%2C%20where%20resizing%20or%20padding%20distorts%20forensic%20traces%20and%0Areduces%20efficiency%2C%20and%20the%20modality%20gap%2C%20as%20images%20and%20videos%20often%20require%0Aseparate%20models.%20To%20address%20these%20challenges%2C%20we%20propose%20RelayFormer%2C%20a%20unified%0Aframework%20that%20adapts%20to%20varying%20resolutions%20and%20modalities.%20RelayFormer%0Apartitions%20inputs%20into%20fixed-size%20sub-images%20and%20introduces%20Global-Local%20Relay%0A%28GLR%29%20tokens%2C%20which%20propagate%20structured%20context%20through%20a%20global-local%20relay%0Aattention%20%28GLRA%29%20mechanism.%20This%20enables%20efficient%20exchange%20of%20global%20cues%2C%0Asuch%20as%20semantic%20or%20temporal%20consistency%2C%20while%20preserving%20fine-grained%0Amanipulation%20artifacts.%20Unlike%20prior%20methods%20that%20rely%20on%20uniform%20resizing%20or%0Asparse%20attention%2C%20RelayFormer%20naturally%20scales%20to%20arbitrary%20resolutions%20and%0Avideo%20sequences%20without%20excessive%20overhead.%20Experiments%20across%20diverse%0Abenchmarks%20demonstrate%20that%20RelayFormer%20achieves%20state-of-the-art%20performance%0Awith%20notable%20efficiency%2C%20combining%20resolution%20adaptivity%20without%20interpolation%0Aor%20excessive%20padding%2C%20unified%20modeling%20for%20both%20images%20and%20videos%2C%20and%20a%20strong%0Abalance%20between%20accuracy%20and%20computational%20cost.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/WenOOI/RelayFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09459v2&entry.124074799=Read"},
{"title": "Point Cloud-Based Control Barrier Functions for Model Predictive Control\n  in Safety-Critical Navigation of Autonomous Mobile Robots", "author": "Faduo Liang and Yunfeng Yang and Shi-Lu Dai", "abstract": "  In this work, we propose a novel motion planning algorithm to facilitate\nsafety-critical navigation for autonomous mobile robots. The proposed algorithm\nintegrates a real-time dynamic obstacle tracking and mapping system that\ncategorizes point clouds into dynamic and static components. For dynamic point\nclouds, the Kalman filter is employed to estimate and predict their motion\nstates. Based on these predictions, we extrapolate the future states of dynamic\npoint clouds, which are subsequently merged with static point clouds to\nconstruct the forward-time-domain (FTD) map. By combining control barrier\nfunctions (CBFs) with nonlinear model predictive control, the proposed\nalgorithm enables the robot to effectively avoid both static and dynamic\nobstacles. The CBF constraints are formulated based on risk points identified\nthrough collision detection between the predicted future states and the FTD\nmap. Experimental results from both simulated and real-world scenarios\ndemonstrate the efficacy of the proposed algorithm in complex environments. In\nsimulation experiments, the proposed algorithm is compared with two baseline\napproaches, showing superior performance in terms of safety and robustness in\nobstacle avoidance. The source code is released for the reference of the\nrobotics community.\n", "link": "http://arxiv.org/abs/2510.02885v1", "date": "2025-10-03", "relevancy": 2.324, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5978}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5856}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20Cloud-Based%20Control%20Barrier%20Functions%20for%20Model%20Predictive%20Control%0A%20%20in%20Safety-Critical%20Navigation%20of%20Autonomous%20Mobile%20Robots&body=Title%3A%20Point%20Cloud-Based%20Control%20Barrier%20Functions%20for%20Model%20Predictive%20Control%0A%20%20in%20Safety-Critical%20Navigation%20of%20Autonomous%20Mobile%20Robots%0AAuthor%3A%20Faduo%20Liang%20and%20Yunfeng%20Yang%20and%20Shi-Lu%20Dai%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20motion%20planning%20algorithm%20to%20facilitate%0Asafety-critical%20navigation%20for%20autonomous%20mobile%20robots.%20The%20proposed%20algorithm%0Aintegrates%20a%20real-time%20dynamic%20obstacle%20tracking%20and%20mapping%20system%20that%0Acategorizes%20point%20clouds%20into%20dynamic%20and%20static%20components.%20For%20dynamic%20point%0Aclouds%2C%20the%20Kalman%20filter%20is%20employed%20to%20estimate%20and%20predict%20their%20motion%0Astates.%20Based%20on%20these%20predictions%2C%20we%20extrapolate%20the%20future%20states%20of%20dynamic%0Apoint%20clouds%2C%20which%20are%20subsequently%20merged%20with%20static%20point%20clouds%20to%0Aconstruct%20the%20forward-time-domain%20%28FTD%29%20map.%20By%20combining%20control%20barrier%0Afunctions%20%28CBFs%29%20with%20nonlinear%20model%20predictive%20control%2C%20the%20proposed%0Aalgorithm%20enables%20the%20robot%20to%20effectively%20avoid%20both%20static%20and%20dynamic%0Aobstacles.%20The%20CBF%20constraints%20are%20formulated%20based%20on%20risk%20points%20identified%0Athrough%20collision%20detection%20between%20the%20predicted%20future%20states%20and%20the%20FTD%0Amap.%20Experimental%20results%20from%20both%20simulated%20and%20real-world%20scenarios%0Ademonstrate%20the%20efficacy%20of%20the%20proposed%20algorithm%20in%20complex%20environments.%20In%0Asimulation%20experiments%2C%20the%20proposed%20algorithm%20is%20compared%20with%20two%20baseline%0Aapproaches%2C%20showing%20superior%20performance%20in%20terms%20of%20safety%20and%20robustness%20in%0Aobstacle%20avoidance.%20The%20source%20code%20is%20released%20for%20the%20reference%20of%20the%0Arobotics%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520Cloud-Based%2520Control%2520Barrier%2520Functions%2520for%2520Model%2520Predictive%2520Control%250A%2520%2520in%2520Safety-Critical%2520Navigation%2520of%2520Autonomous%2520Mobile%2520Robots%26entry.906535625%3DFaduo%2520Liang%2520and%2520Yunfeng%2520Yang%2520and%2520Shi-Lu%2520Dai%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520motion%2520planning%2520algorithm%2520to%2520facilitate%250Asafety-critical%2520navigation%2520for%2520autonomous%2520mobile%2520robots.%2520The%2520proposed%2520algorithm%250Aintegrates%2520a%2520real-time%2520dynamic%2520obstacle%2520tracking%2520and%2520mapping%2520system%2520that%250Acategorizes%2520point%2520clouds%2520into%2520dynamic%2520and%2520static%2520components.%2520For%2520dynamic%2520point%250Aclouds%252C%2520the%2520Kalman%2520filter%2520is%2520employed%2520to%2520estimate%2520and%2520predict%2520their%2520motion%250Astates.%2520Based%2520on%2520these%2520predictions%252C%2520we%2520extrapolate%2520the%2520future%2520states%2520of%2520dynamic%250Apoint%2520clouds%252C%2520which%2520are%2520subsequently%2520merged%2520with%2520static%2520point%2520clouds%2520to%250Aconstruct%2520the%2520forward-time-domain%2520%2528FTD%2529%2520map.%2520By%2520combining%2520control%2520barrier%250Afunctions%2520%2528CBFs%2529%2520with%2520nonlinear%2520model%2520predictive%2520control%252C%2520the%2520proposed%250Aalgorithm%2520enables%2520the%2520robot%2520to%2520effectively%2520avoid%2520both%2520static%2520and%2520dynamic%250Aobstacles.%2520The%2520CBF%2520constraints%2520are%2520formulated%2520based%2520on%2520risk%2520points%2520identified%250Athrough%2520collision%2520detection%2520between%2520the%2520predicted%2520future%2520states%2520and%2520the%2520FTD%250Amap.%2520Experimental%2520results%2520from%2520both%2520simulated%2520and%2520real-world%2520scenarios%250Ademonstrate%2520the%2520efficacy%2520of%2520the%2520proposed%2520algorithm%2520in%2520complex%2520environments.%2520In%250Asimulation%2520experiments%252C%2520the%2520proposed%2520algorithm%2520is%2520compared%2520with%2520two%2520baseline%250Aapproaches%252C%2520showing%2520superior%2520performance%2520in%2520terms%2520of%2520safety%2520and%2520robustness%2520in%250Aobstacle%2520avoidance.%2520The%2520source%2520code%2520is%2520released%2520for%2520the%2520reference%2520of%2520the%250Arobotics%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Cloud-Based%20Control%20Barrier%20Functions%20for%20Model%20Predictive%20Control%0A%20%20in%20Safety-Critical%20Navigation%20of%20Autonomous%20Mobile%20Robots&entry.906535625=Faduo%20Liang%20and%20Yunfeng%20Yang%20and%20Shi-Lu%20Dai&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20motion%20planning%20algorithm%20to%20facilitate%0Asafety-critical%20navigation%20for%20autonomous%20mobile%20robots.%20The%20proposed%20algorithm%0Aintegrates%20a%20real-time%20dynamic%20obstacle%20tracking%20and%20mapping%20system%20that%0Acategorizes%20point%20clouds%20into%20dynamic%20and%20static%20components.%20For%20dynamic%20point%0Aclouds%2C%20the%20Kalman%20filter%20is%20employed%20to%20estimate%20and%20predict%20their%20motion%0Astates.%20Based%20on%20these%20predictions%2C%20we%20extrapolate%20the%20future%20states%20of%20dynamic%0Apoint%20clouds%2C%20which%20are%20subsequently%20merged%20with%20static%20point%20clouds%20to%0Aconstruct%20the%20forward-time-domain%20%28FTD%29%20map.%20By%20combining%20control%20barrier%0Afunctions%20%28CBFs%29%20with%20nonlinear%20model%20predictive%20control%2C%20the%20proposed%0Aalgorithm%20enables%20the%20robot%20to%20effectively%20avoid%20both%20static%20and%20dynamic%0Aobstacles.%20The%20CBF%20constraints%20are%20formulated%20based%20on%20risk%20points%20identified%0Athrough%20collision%20detection%20between%20the%20predicted%20future%20states%20and%20the%20FTD%0Amap.%20Experimental%20results%20from%20both%20simulated%20and%20real-world%20scenarios%0Ademonstrate%20the%20efficacy%20of%20the%20proposed%20algorithm%20in%20complex%20environments.%20In%0Asimulation%20experiments%2C%20the%20proposed%20algorithm%20is%20compared%20with%20two%20baseline%0Aapproaches%2C%20showing%20superior%20performance%20in%20terms%20of%20safety%20and%20robustness%20in%0Aobstacle%20avoidance.%20The%20source%20code%20is%20released%20for%20the%20reference%20of%20the%0Arobotics%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02885v1&entry.124074799=Read"},
{"title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft", "author": "Junchao Huang and Xinting Hu and Boyao Han and Shaoshuai Shi and Zhuotao Tian and Tianyu He and Li Jiang", "abstract": "  Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences.\n", "link": "http://arxiv.org/abs/2510.03198v1", "date": "2025-10-03", "relevancy": 2.3076, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6279}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5673}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Forcing%3A%20Spatio-Temporal%20Memory%20for%20Consistent%20Scene%20Generation%0A%20%20on%20Minecraft&body=Title%3A%20Memory%20Forcing%3A%20Spatio-Temporal%20Memory%20for%20Consistent%20Scene%20Generation%0A%20%20on%20Minecraft%0AAuthor%3A%20Junchao%20Huang%20and%20Xinting%20Hu%20and%20Boyao%20Han%20and%20Shaoshuai%20Shi%20and%20Zhuotao%20Tian%20and%20Tianyu%20He%20and%20Li%20Jiang%0AAbstract%3A%20%20%20Autoregressive%20video%20diffusion%20models%20have%20proved%20effective%20for%20world%0Amodeling%20and%20interactive%20scene%20generation%2C%20with%20Minecraft%20gameplay%20as%20a%0Arepresentative%20application.%20To%20faithfully%20simulate%20play%2C%20a%20model%20must%20generate%0Anatural%20content%20while%20exploring%20new%20scenes%20and%20preserve%20spatial%20consistency%0Awhen%20revisiting%20explored%20areas.%20Under%20limited%20computation%20budgets%2C%20it%20must%0Acompress%20and%20exploit%20historical%20cues%20within%20a%20finite%20context%20window%2C%20which%0Aexposes%20a%20trade-off%3A%20Temporal-only%20memory%20lacks%20long-term%20spatial%20consistency%2C%0Awhereas%20adding%20spatial%20memory%20strengthens%20consistency%20but%20may%20degrade%20new%20scene%0Ageneration%20quality%20when%20the%20model%20over-relies%20on%20insufficient%20spatial%20context.%0AWe%20present%20Memory%20Forcing%2C%20a%20learning%20framework%20that%20pairs%20training%20protocols%0Awith%20a%20geometry-indexed%20spatial%20memory.%20Hybrid%20Training%20exposes%20distinct%0Agameplay%20regimes%2C%20guiding%20the%20model%20to%20rely%20on%20temporal%20memory%20during%0Aexploration%20and%20incorporate%20spatial%20memory%20for%20revisits.%20Chained%20Forward%0ATraining%20extends%20autoregressive%20training%20with%20model%20rollouts%2C%20where%20chained%0Apredictions%20create%20larger%20pose%20variations%20and%20encourage%20reliance%20on%20spatial%0Amemory%20for%20maintaining%20consistency.%20Point-to-Frame%20Retrieval%20efficiently%0Aretrieves%20history%20by%20mapping%20currently%20visible%20points%20to%20their%20source%20frames%2C%0Awhile%20Incremental%203D%20Reconstruction%20maintains%20and%20updates%20an%20explicit%203D%20cache.%0AExtensive%20experiments%20demonstrate%20that%20Memory%20Forcing%20achieves%20superior%0Along-term%20spatial%20consistency%20and%20generative%20quality%20across%20diverse%0Aenvironments%2C%20while%20maintaining%20computational%20efficiency%20for%20extended%0Asequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Forcing%253A%2520Spatio-Temporal%2520Memory%2520for%2520Consistent%2520Scene%2520Generation%250A%2520%2520on%2520Minecraft%26entry.906535625%3DJunchao%2520Huang%2520and%2520Xinting%2520Hu%2520and%2520Boyao%2520Han%2520and%2520Shaoshuai%2520Shi%2520and%2520Zhuotao%2520Tian%2520and%2520Tianyu%2520He%2520and%2520Li%2520Jiang%26entry.1292438233%3D%2520%2520Autoregressive%2520video%2520diffusion%2520models%2520have%2520proved%2520effective%2520for%2520world%250Amodeling%2520and%2520interactive%2520scene%2520generation%252C%2520with%2520Minecraft%2520gameplay%2520as%2520a%250Arepresentative%2520application.%2520To%2520faithfully%2520simulate%2520play%252C%2520a%2520model%2520must%2520generate%250Anatural%2520content%2520while%2520exploring%2520new%2520scenes%2520and%2520preserve%2520spatial%2520consistency%250Awhen%2520revisiting%2520explored%2520areas.%2520Under%2520limited%2520computation%2520budgets%252C%2520it%2520must%250Acompress%2520and%2520exploit%2520historical%2520cues%2520within%2520a%2520finite%2520context%2520window%252C%2520which%250Aexposes%2520a%2520trade-off%253A%2520Temporal-only%2520memory%2520lacks%2520long-term%2520spatial%2520consistency%252C%250Awhereas%2520adding%2520spatial%2520memory%2520strengthens%2520consistency%2520but%2520may%2520degrade%2520new%2520scene%250Ageneration%2520quality%2520when%2520the%2520model%2520over-relies%2520on%2520insufficient%2520spatial%2520context.%250AWe%2520present%2520Memory%2520Forcing%252C%2520a%2520learning%2520framework%2520that%2520pairs%2520training%2520protocols%250Awith%2520a%2520geometry-indexed%2520spatial%2520memory.%2520Hybrid%2520Training%2520exposes%2520distinct%250Agameplay%2520regimes%252C%2520guiding%2520the%2520model%2520to%2520rely%2520on%2520temporal%2520memory%2520during%250Aexploration%2520and%2520incorporate%2520spatial%2520memory%2520for%2520revisits.%2520Chained%2520Forward%250ATraining%2520extends%2520autoregressive%2520training%2520with%2520model%2520rollouts%252C%2520where%2520chained%250Apredictions%2520create%2520larger%2520pose%2520variations%2520and%2520encourage%2520reliance%2520on%2520spatial%250Amemory%2520for%2520maintaining%2520consistency.%2520Point-to-Frame%2520Retrieval%2520efficiently%250Aretrieves%2520history%2520by%2520mapping%2520currently%2520visible%2520points%2520to%2520their%2520source%2520frames%252C%250Awhile%2520Incremental%25203D%2520Reconstruction%2520maintains%2520and%2520updates%2520an%2520explicit%25203D%2520cache.%250AExtensive%2520experiments%2520demonstrate%2520that%2520Memory%2520Forcing%2520achieves%2520superior%250Along-term%2520spatial%2520consistency%2520and%2520generative%2520quality%2520across%2520diverse%250Aenvironments%252C%2520while%2520maintaining%2520computational%2520efficiency%2520for%2520extended%250Asequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Forcing%3A%20Spatio-Temporal%20Memory%20for%20Consistent%20Scene%20Generation%0A%20%20on%20Minecraft&entry.906535625=Junchao%20Huang%20and%20Xinting%20Hu%20and%20Boyao%20Han%20and%20Shaoshuai%20Shi%20and%20Zhuotao%20Tian%20and%20Tianyu%20He%20and%20Li%20Jiang&entry.1292438233=%20%20Autoregressive%20video%20diffusion%20models%20have%20proved%20effective%20for%20world%0Amodeling%20and%20interactive%20scene%20generation%2C%20with%20Minecraft%20gameplay%20as%20a%0Arepresentative%20application.%20To%20faithfully%20simulate%20play%2C%20a%20model%20must%20generate%0Anatural%20content%20while%20exploring%20new%20scenes%20and%20preserve%20spatial%20consistency%0Awhen%20revisiting%20explored%20areas.%20Under%20limited%20computation%20budgets%2C%20it%20must%0Acompress%20and%20exploit%20historical%20cues%20within%20a%20finite%20context%20window%2C%20which%0Aexposes%20a%20trade-off%3A%20Temporal-only%20memory%20lacks%20long-term%20spatial%20consistency%2C%0Awhereas%20adding%20spatial%20memory%20strengthens%20consistency%20but%20may%20degrade%20new%20scene%0Ageneration%20quality%20when%20the%20model%20over-relies%20on%20insufficient%20spatial%20context.%0AWe%20present%20Memory%20Forcing%2C%20a%20learning%20framework%20that%20pairs%20training%20protocols%0Awith%20a%20geometry-indexed%20spatial%20memory.%20Hybrid%20Training%20exposes%20distinct%0Agameplay%20regimes%2C%20guiding%20the%20model%20to%20rely%20on%20temporal%20memory%20during%0Aexploration%20and%20incorporate%20spatial%20memory%20for%20revisits.%20Chained%20Forward%0ATraining%20extends%20autoregressive%20training%20with%20model%20rollouts%2C%20where%20chained%0Apredictions%20create%20larger%20pose%20variations%20and%20encourage%20reliance%20on%20spatial%0Amemory%20for%20maintaining%20consistency.%20Point-to-Frame%20Retrieval%20efficiently%0Aretrieves%20history%20by%20mapping%20currently%20visible%20points%20to%20their%20source%20frames%2C%0Awhile%20Incremental%203D%20Reconstruction%20maintains%20and%20updates%20an%20explicit%203D%20cache.%0AExtensive%20experiments%20demonstrate%20that%20Memory%20Forcing%20achieves%20superior%0Along-term%20spatial%20consistency%20and%20generative%20quality%20across%20diverse%0Aenvironments%2C%20while%20maintaining%20computational%20efficiency%20for%20extended%0Asequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03198v1&entry.124074799=Read"},
{"title": "Investigating The Smells of LLM Generated Code", "author": "Debalina Ghosh Paul and Hong Zhu and Ian Bayley", "abstract": "  Context: Large Language Models (LLMs) are increasingly being used to generate\nprogram code. Much research has been reported on the functional correctness of\ngenerated code, but there is far less on code quality.\n  Objectives: In this study, we propose a scenario-based method of evaluating\nthe quality of LLM-generated code to identify the weakest scenarios in which\nthe quality of LLM generated code should be improved.\n  Methods: The method measures code smells, an important indicator of code\nquality, and compares them with a baseline formed from reference solutions of\nprofessionally written code. The test dataset is divided into various subsets\naccording to the topics of the code and complexity of the coding tasks to\nrepresent different scenarios of using LLMs for code generation. We will also\npresent an automated test system for this purpose and report experiments with\nthe Java programs generated in response to prompts given to four\nstate-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.\n  Results: We find that LLM-generated code has a higher incidence of code\nsmells compared to reference solutions. Falcon performed the least badly, with\na smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)\nand finally Codex (84.97%). The average smell increase across all LLMs was\n63.34%, comprising 73.35% for implementation smells and 21.42% for design\nsmells. We also found that the increase in code smells is greater for more\ncomplex coding tasks and for more advanced topics, such as those involving\nobject-orientated concepts.\n  Conclusion: In terms of code smells, LLM's performances on various coding\ntask complexities and topics are highly correlated to the quality of human\nwritten code in the corresponding scenarios. However, the quality of LLM\ngenerated code is noticeably poorer than human written code.\n", "link": "http://arxiv.org/abs/2510.03029v1", "date": "2025-10-03", "relevancy": 2.2952, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20The%20Smells%20of%20LLM%20Generated%20Code&body=Title%3A%20Investigating%20The%20Smells%20of%20LLM%20Generated%20Code%0AAuthor%3A%20Debalina%20Ghosh%20Paul%20and%20Hong%20Zhu%20and%20Ian%20Bayley%0AAbstract%3A%20%20%20Context%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20being%20used%20to%20generate%0Aprogram%20code.%20Much%20research%20has%20been%20reported%20on%20the%20functional%20correctness%20of%0Agenerated%20code%2C%20but%20there%20is%20far%20less%20on%20code%20quality.%0A%20%20Objectives%3A%20In%20this%20study%2C%20we%20propose%20a%20scenario-based%20method%20of%20evaluating%0Athe%20quality%20of%20LLM-generated%20code%20to%20identify%20the%20weakest%20scenarios%20in%20which%0Athe%20quality%20of%20LLM%20generated%20code%20should%20be%20improved.%0A%20%20Methods%3A%20The%20method%20measures%20code%20smells%2C%20an%20important%20indicator%20of%20code%0Aquality%2C%20and%20compares%20them%20with%20a%20baseline%20formed%20from%20reference%20solutions%20of%0Aprofessionally%20written%20code.%20The%20test%20dataset%20is%20divided%20into%20various%20subsets%0Aaccording%20to%20the%20topics%20of%20the%20code%20and%20complexity%20of%20the%20coding%20tasks%20to%0Arepresent%20different%20scenarios%20of%20using%20LLMs%20for%20code%20generation.%20We%20will%20also%0Apresent%20an%20automated%20test%20system%20for%20this%20purpose%20and%20report%20experiments%20with%0Athe%20Java%20programs%20generated%20in%20response%20to%20prompts%20given%20to%20four%0Astate-of-the-art%20LLMs%3A%20Gemini%20Pro%2C%20ChatGPT%2C%20Codex%2C%20and%20Falcon.%0A%20%20Results%3A%20We%20find%20that%20LLM-generated%20code%20has%20a%20higher%20incidence%20of%20code%0Asmells%20compared%20to%20reference%20solutions.%20Falcon%20performed%20the%20least%20badly%2C%20with%0Aa%20smell%20increase%20of%2042.28%25%2C%20followed%20by%20Gemini%20Pro%20%2862.07%25%29%2C%20ChatGPT%20%2865.05%25%29%0Aand%20finally%20Codex%20%2884.97%25%29.%20The%20average%20smell%20increase%20across%20all%20LLMs%20was%0A63.34%25%2C%20comprising%2073.35%25%20for%20implementation%20smells%20and%2021.42%25%20for%20design%0Asmells.%20We%20also%20found%20that%20the%20increase%20in%20code%20smells%20is%20greater%20for%20more%0Acomplex%20coding%20tasks%20and%20for%20more%20advanced%20topics%2C%20such%20as%20those%20involving%0Aobject-orientated%20concepts.%0A%20%20Conclusion%3A%20In%20terms%20of%20code%20smells%2C%20LLM%27s%20performances%20on%20various%20coding%0Atask%20complexities%20and%20topics%20are%20highly%20correlated%20to%20the%20quality%20of%20human%0Awritten%20code%20in%20the%20corresponding%20scenarios.%20However%2C%20the%20quality%20of%20LLM%0Agenerated%20code%20is%20noticeably%20poorer%20than%20human%20written%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520The%2520Smells%2520of%2520LLM%2520Generated%2520Code%26entry.906535625%3DDebalina%2520Ghosh%2520Paul%2520and%2520Hong%2520Zhu%2520and%2520Ian%2520Bayley%26entry.1292438233%3D%2520%2520Context%253A%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520being%2520used%2520to%2520generate%250Aprogram%2520code.%2520Much%2520research%2520has%2520been%2520reported%2520on%2520the%2520functional%2520correctness%2520of%250Agenerated%2520code%252C%2520but%2520there%2520is%2520far%2520less%2520on%2520code%2520quality.%250A%2520%2520Objectives%253A%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520scenario-based%2520method%2520of%2520evaluating%250Athe%2520quality%2520of%2520LLM-generated%2520code%2520to%2520identify%2520the%2520weakest%2520scenarios%2520in%2520which%250Athe%2520quality%2520of%2520LLM%2520generated%2520code%2520should%2520be%2520improved.%250A%2520%2520Methods%253A%2520The%2520method%2520measures%2520code%2520smells%252C%2520an%2520important%2520indicator%2520of%2520code%250Aquality%252C%2520and%2520compares%2520them%2520with%2520a%2520baseline%2520formed%2520from%2520reference%2520solutions%2520of%250Aprofessionally%2520written%2520code.%2520The%2520test%2520dataset%2520is%2520divided%2520into%2520various%2520subsets%250Aaccording%2520to%2520the%2520topics%2520of%2520the%2520code%2520and%2520complexity%2520of%2520the%2520coding%2520tasks%2520to%250Arepresent%2520different%2520scenarios%2520of%2520using%2520LLMs%2520for%2520code%2520generation.%2520We%2520will%2520also%250Apresent%2520an%2520automated%2520test%2520system%2520for%2520this%2520purpose%2520and%2520report%2520experiments%2520with%250Athe%2520Java%2520programs%2520generated%2520in%2520response%2520to%2520prompts%2520given%2520to%2520four%250Astate-of-the-art%2520LLMs%253A%2520Gemini%2520Pro%252C%2520ChatGPT%252C%2520Codex%252C%2520and%2520Falcon.%250A%2520%2520Results%253A%2520We%2520find%2520that%2520LLM-generated%2520code%2520has%2520a%2520higher%2520incidence%2520of%2520code%250Asmells%2520compared%2520to%2520reference%2520solutions.%2520Falcon%2520performed%2520the%2520least%2520badly%252C%2520with%250Aa%2520smell%2520increase%2520of%252042.28%2525%252C%2520followed%2520by%2520Gemini%2520Pro%2520%252862.07%2525%2529%252C%2520ChatGPT%2520%252865.05%2525%2529%250Aand%2520finally%2520Codex%2520%252884.97%2525%2529.%2520The%2520average%2520smell%2520increase%2520across%2520all%2520LLMs%2520was%250A63.34%2525%252C%2520comprising%252073.35%2525%2520for%2520implementation%2520smells%2520and%252021.42%2525%2520for%2520design%250Asmells.%2520We%2520also%2520found%2520that%2520the%2520increase%2520in%2520code%2520smells%2520is%2520greater%2520for%2520more%250Acomplex%2520coding%2520tasks%2520and%2520for%2520more%2520advanced%2520topics%252C%2520such%2520as%2520those%2520involving%250Aobject-orientated%2520concepts.%250A%2520%2520Conclusion%253A%2520In%2520terms%2520of%2520code%2520smells%252C%2520LLM%2527s%2520performances%2520on%2520various%2520coding%250Atask%2520complexities%2520and%2520topics%2520are%2520highly%2520correlated%2520to%2520the%2520quality%2520of%2520human%250Awritten%2520code%2520in%2520the%2520corresponding%2520scenarios.%2520However%252C%2520the%2520quality%2520of%2520LLM%250Agenerated%2520code%2520is%2520noticeably%2520poorer%2520than%2520human%2520written%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20The%20Smells%20of%20LLM%20Generated%20Code&entry.906535625=Debalina%20Ghosh%20Paul%20and%20Hong%20Zhu%20and%20Ian%20Bayley&entry.1292438233=%20%20Context%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20being%20used%20to%20generate%0Aprogram%20code.%20Much%20research%20has%20been%20reported%20on%20the%20functional%20correctness%20of%0Agenerated%20code%2C%20but%20there%20is%20far%20less%20on%20code%20quality.%0A%20%20Objectives%3A%20In%20this%20study%2C%20we%20propose%20a%20scenario-based%20method%20of%20evaluating%0Athe%20quality%20of%20LLM-generated%20code%20to%20identify%20the%20weakest%20scenarios%20in%20which%0Athe%20quality%20of%20LLM%20generated%20code%20should%20be%20improved.%0A%20%20Methods%3A%20The%20method%20measures%20code%20smells%2C%20an%20important%20indicator%20of%20code%0Aquality%2C%20and%20compares%20them%20with%20a%20baseline%20formed%20from%20reference%20solutions%20of%0Aprofessionally%20written%20code.%20The%20test%20dataset%20is%20divided%20into%20various%20subsets%0Aaccording%20to%20the%20topics%20of%20the%20code%20and%20complexity%20of%20the%20coding%20tasks%20to%0Arepresent%20different%20scenarios%20of%20using%20LLMs%20for%20code%20generation.%20We%20will%20also%0Apresent%20an%20automated%20test%20system%20for%20this%20purpose%20and%20report%20experiments%20with%0Athe%20Java%20programs%20generated%20in%20response%20to%20prompts%20given%20to%20four%0Astate-of-the-art%20LLMs%3A%20Gemini%20Pro%2C%20ChatGPT%2C%20Codex%2C%20and%20Falcon.%0A%20%20Results%3A%20We%20find%20that%20LLM-generated%20code%20has%20a%20higher%20incidence%20of%20code%0Asmells%20compared%20to%20reference%20solutions.%20Falcon%20performed%20the%20least%20badly%2C%20with%0Aa%20smell%20increase%20of%2042.28%25%2C%20followed%20by%20Gemini%20Pro%20%2862.07%25%29%2C%20ChatGPT%20%2865.05%25%29%0Aand%20finally%20Codex%20%2884.97%25%29.%20The%20average%20smell%20increase%20across%20all%20LLMs%20was%0A63.34%25%2C%20comprising%2073.35%25%20for%20implementation%20smells%20and%2021.42%25%20for%20design%0Asmells.%20We%20also%20found%20that%20the%20increase%20in%20code%20smells%20is%20greater%20for%20more%0Acomplex%20coding%20tasks%20and%20for%20more%20advanced%20topics%2C%20such%20as%20those%20involving%0Aobject-orientated%20concepts.%0A%20%20Conclusion%3A%20In%20terms%20of%20code%20smells%2C%20LLM%27s%20performances%20on%20various%20coding%0Atask%20complexities%20and%20topics%20are%20highly%20correlated%20to%20the%20quality%20of%20human%0Awritten%20code%20in%20the%20corresponding%20scenarios.%20However%2C%20the%20quality%20of%20LLM%0Agenerated%20code%20is%20noticeably%20poorer%20than%20human%20written%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03029v1&entry.124074799=Read"},
{"title": "Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics", "author": "Yufei Zhu and Andrey Rudenko and Tomasz P. Kucner and Achim J. Lilienthal and Martin Magnusson", "abstract": "  Long-term human motion prediction (LHMP) is important for the safe and\nefficient operation of autonomous robots and vehicles in environments shared\nwith humans. Accurate predictions are important for applications including\nmotion planning, tracking, human-robot interaction, and safety monitoring. In\nthis paper, we exploit Maps of Dynamics (MoDs), which encode spatial or\nspatio-temporal motion patterns as environment features, to achieve LHMP for\nhorizons of up to 60 seconds. We propose an MoD-informed LHMP framework that\nsupports various types of MoDs and includes a ranking method to output the most\nlikely predicted trajectory, improving practical utility in robotics. Further,\na time-conditioned MoD is introduced to capture motion patterns that vary\nacross different times of day. We evaluate MoD-LHMP instantiated with three\ntypes of MoDs. Experiments on two real-world datasets show that MoD-informed\nmethod outperforms learning-based ones, with up to 50\\% improvement in average\ndisplacement error, and the time-conditioned variant achieves the highest\naccuracy overall. Project code is available at\nhttps://github.com/test-bai-cpu/LHMP-with-MoDs.git\n", "link": "http://arxiv.org/abs/2510.03031v1", "date": "2025-10-03", "relevancy": 2.2908, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6062}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6007}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Term%20Human%20Motion%20Prediction%20Using%20Spatio-Temporal%20Maps%20of%20Dynamics&body=Title%3A%20Long-Term%20Human%20Motion%20Prediction%20Using%20Spatio-Temporal%20Maps%20of%20Dynamics%0AAuthor%3A%20Yufei%20Zhu%20and%20Andrey%20Rudenko%20and%20Tomasz%20P.%20Kucner%20and%20Achim%20J.%20Lilienthal%20and%20Martin%20Magnusson%0AAbstract%3A%20%20%20Long-term%20human%20motion%20prediction%20%28LHMP%29%20is%20important%20for%20the%20safe%20and%0Aefficient%20operation%20of%20autonomous%20robots%20and%20vehicles%20in%20environments%20shared%0Awith%20humans.%20Accurate%20predictions%20are%20important%20for%20applications%20including%0Amotion%20planning%2C%20tracking%2C%20human-robot%20interaction%2C%20and%20safety%20monitoring.%20In%0Athis%20paper%2C%20we%20exploit%20Maps%20of%20Dynamics%20%28MoDs%29%2C%20which%20encode%20spatial%20or%0Aspatio-temporal%20motion%20patterns%20as%20environment%20features%2C%20to%20achieve%20LHMP%20for%0Ahorizons%20of%20up%20to%2060%20seconds.%20We%20propose%20an%20MoD-informed%20LHMP%20framework%20that%0Asupports%20various%20types%20of%20MoDs%20and%20includes%20a%20ranking%20method%20to%20output%20the%20most%0Alikely%20predicted%20trajectory%2C%20improving%20practical%20utility%20in%20robotics.%20Further%2C%0Aa%20time-conditioned%20MoD%20is%20introduced%20to%20capture%20motion%20patterns%20that%20vary%0Aacross%20different%20times%20of%20day.%20We%20evaluate%20MoD-LHMP%20instantiated%20with%20three%0Atypes%20of%20MoDs.%20Experiments%20on%20two%20real-world%20datasets%20show%20that%20MoD-informed%0Amethod%20outperforms%20learning-based%20ones%2C%20with%20up%20to%2050%5C%25%20improvement%20in%20average%0Adisplacement%20error%2C%20and%20the%20time-conditioned%20variant%20achieves%20the%20highest%0Aaccuracy%20overall.%20Project%20code%20is%20available%20at%0Ahttps%3A//github.com/test-bai-cpu/LHMP-with-MoDs.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Term%2520Human%2520Motion%2520Prediction%2520Using%2520Spatio-Temporal%2520Maps%2520of%2520Dynamics%26entry.906535625%3DYufei%2520Zhu%2520and%2520Andrey%2520Rudenko%2520and%2520Tomasz%2520P.%2520Kucner%2520and%2520Achim%2520J.%2520Lilienthal%2520and%2520Martin%2520Magnusson%26entry.1292438233%3D%2520%2520Long-term%2520human%2520motion%2520prediction%2520%2528LHMP%2529%2520is%2520important%2520for%2520the%2520safe%2520and%250Aefficient%2520operation%2520of%2520autonomous%2520robots%2520and%2520vehicles%2520in%2520environments%2520shared%250Awith%2520humans.%2520Accurate%2520predictions%2520are%2520important%2520for%2520applications%2520including%250Amotion%2520planning%252C%2520tracking%252C%2520human-robot%2520interaction%252C%2520and%2520safety%2520monitoring.%2520In%250Athis%2520paper%252C%2520we%2520exploit%2520Maps%2520of%2520Dynamics%2520%2528MoDs%2529%252C%2520which%2520encode%2520spatial%2520or%250Aspatio-temporal%2520motion%2520patterns%2520as%2520environment%2520features%252C%2520to%2520achieve%2520LHMP%2520for%250Ahorizons%2520of%2520up%2520to%252060%2520seconds.%2520We%2520propose%2520an%2520MoD-informed%2520LHMP%2520framework%2520that%250Asupports%2520various%2520types%2520of%2520MoDs%2520and%2520includes%2520a%2520ranking%2520method%2520to%2520output%2520the%2520most%250Alikely%2520predicted%2520trajectory%252C%2520improving%2520practical%2520utility%2520in%2520robotics.%2520Further%252C%250Aa%2520time-conditioned%2520MoD%2520is%2520introduced%2520to%2520capture%2520motion%2520patterns%2520that%2520vary%250Aacross%2520different%2520times%2520of%2520day.%2520We%2520evaluate%2520MoD-LHMP%2520instantiated%2520with%2520three%250Atypes%2520of%2520MoDs.%2520Experiments%2520on%2520two%2520real-world%2520datasets%2520show%2520that%2520MoD-informed%250Amethod%2520outperforms%2520learning-based%2520ones%252C%2520with%2520up%2520to%252050%255C%2525%2520improvement%2520in%2520average%250Adisplacement%2520error%252C%2520and%2520the%2520time-conditioned%2520variant%2520achieves%2520the%2520highest%250Aaccuracy%2520overall.%2520Project%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/test-bai-cpu/LHMP-with-MoDs.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Term%20Human%20Motion%20Prediction%20Using%20Spatio-Temporal%20Maps%20of%20Dynamics&entry.906535625=Yufei%20Zhu%20and%20Andrey%20Rudenko%20and%20Tomasz%20P.%20Kucner%20and%20Achim%20J.%20Lilienthal%20and%20Martin%20Magnusson&entry.1292438233=%20%20Long-term%20human%20motion%20prediction%20%28LHMP%29%20is%20important%20for%20the%20safe%20and%0Aefficient%20operation%20of%20autonomous%20robots%20and%20vehicles%20in%20environments%20shared%0Awith%20humans.%20Accurate%20predictions%20are%20important%20for%20applications%20including%0Amotion%20planning%2C%20tracking%2C%20human-robot%20interaction%2C%20and%20safety%20monitoring.%20In%0Athis%20paper%2C%20we%20exploit%20Maps%20of%20Dynamics%20%28MoDs%29%2C%20which%20encode%20spatial%20or%0Aspatio-temporal%20motion%20patterns%20as%20environment%20features%2C%20to%20achieve%20LHMP%20for%0Ahorizons%20of%20up%20to%2060%20seconds.%20We%20propose%20an%20MoD-informed%20LHMP%20framework%20that%0Asupports%20various%20types%20of%20MoDs%20and%20includes%20a%20ranking%20method%20to%20output%20the%20most%0Alikely%20predicted%20trajectory%2C%20improving%20practical%20utility%20in%20robotics.%20Further%2C%0Aa%20time-conditioned%20MoD%20is%20introduced%20to%20capture%20motion%20patterns%20that%20vary%0Aacross%20different%20times%20of%20day.%20We%20evaluate%20MoD-LHMP%20instantiated%20with%20three%0Atypes%20of%20MoDs.%20Experiments%20on%20two%20real-world%20datasets%20show%20that%20MoD-informed%0Amethod%20outperforms%20learning-based%20ones%2C%20with%20up%20to%2050%5C%25%20improvement%20in%20average%0Adisplacement%20error%2C%20and%20the%20time-conditioned%20variant%20achieves%20the%20highest%0Aaccuracy%20overall.%20Project%20code%20is%20available%20at%0Ahttps%3A//github.com/test-bai-cpu/LHMP-with-MoDs.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03031v1&entry.124074799=Read"},
{"title": "Not a nuisance but a useful heuristic: Outlier dimensions favor frequent\n  tokens in language models", "author": "Iuri Macocco and Nora Graichen and Gemma Boleda and Marco Baroni", "abstract": "  We study last-layer outlier dimensions, i.e. dimensions that display extreme\nactivations for the majority of inputs. We show that outlier dimensions arise\nin many different modern language models, and trace their function back to the\nheuristic of constantly predicting frequent words. We further show how a model\ncan block this heuristic when it is not contextually appropriate, by assigning\na counterbalancing weight mass to the remaining dimensions, and we investigate\nwhich model parameters boost outlier dimensions and when they arise during\ntraining. We conclude that outlier dimensions are a specialized mechanism\ndiscovered by many distinct models to implement a useful token prediction\nheuristic.\n", "link": "http://arxiv.org/abs/2503.21718v4", "date": "2025-10-03", "relevancy": 2.2891, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4924}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20a%20nuisance%20but%20a%20useful%20heuristic%3A%20Outlier%20dimensions%20favor%20frequent%0A%20%20tokens%20in%20language%20models&body=Title%3A%20Not%20a%20nuisance%20but%20a%20useful%20heuristic%3A%20Outlier%20dimensions%20favor%20frequent%0A%20%20tokens%20in%20language%20models%0AAuthor%3A%20Iuri%20Macocco%20and%20Nora%20Graichen%20and%20Gemma%20Boleda%20and%20Marco%20Baroni%0AAbstract%3A%20%20%20We%20study%20last-layer%20outlier%20dimensions%2C%20i.e.%20dimensions%20that%20display%20extreme%0Aactivations%20for%20the%20majority%20of%20inputs.%20We%20show%20that%20outlier%20dimensions%20arise%0Ain%20many%20different%20modern%20language%20models%2C%20and%20trace%20their%20function%20back%20to%20the%0Aheuristic%20of%20constantly%20predicting%20frequent%20words.%20We%20further%20show%20how%20a%20model%0Acan%20block%20this%20heuristic%20when%20it%20is%20not%20contextually%20appropriate%2C%20by%20assigning%0Aa%20counterbalancing%20weight%20mass%20to%20the%20remaining%20dimensions%2C%20and%20we%20investigate%0Awhich%20model%20parameters%20boost%20outlier%20dimensions%20and%20when%20they%20arise%20during%0Atraining.%20We%20conclude%20that%20outlier%20dimensions%20are%20a%20specialized%20mechanism%0Adiscovered%20by%20many%20distinct%20models%20to%20implement%20a%20useful%20token%20prediction%0Aheuristic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21718v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520a%2520nuisance%2520but%2520a%2520useful%2520heuristic%253A%2520Outlier%2520dimensions%2520favor%2520frequent%250A%2520%2520tokens%2520in%2520language%2520models%26entry.906535625%3DIuri%2520Macocco%2520and%2520Nora%2520Graichen%2520and%2520Gemma%2520Boleda%2520and%2520Marco%2520Baroni%26entry.1292438233%3D%2520%2520We%2520study%2520last-layer%2520outlier%2520dimensions%252C%2520i.e.%2520dimensions%2520that%2520display%2520extreme%250Aactivations%2520for%2520the%2520majority%2520of%2520inputs.%2520We%2520show%2520that%2520outlier%2520dimensions%2520arise%250Ain%2520many%2520different%2520modern%2520language%2520models%252C%2520and%2520trace%2520their%2520function%2520back%2520to%2520the%250Aheuristic%2520of%2520constantly%2520predicting%2520frequent%2520words.%2520We%2520further%2520show%2520how%2520a%2520model%250Acan%2520block%2520this%2520heuristic%2520when%2520it%2520is%2520not%2520contextually%2520appropriate%252C%2520by%2520assigning%250Aa%2520counterbalancing%2520weight%2520mass%2520to%2520the%2520remaining%2520dimensions%252C%2520and%2520we%2520investigate%250Awhich%2520model%2520parameters%2520boost%2520outlier%2520dimensions%2520and%2520when%2520they%2520arise%2520during%250Atraining.%2520We%2520conclude%2520that%2520outlier%2520dimensions%2520are%2520a%2520specialized%2520mechanism%250Adiscovered%2520by%2520many%2520distinct%2520models%2520to%2520implement%2520a%2520useful%2520token%2520prediction%250Aheuristic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21718v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20a%20nuisance%20but%20a%20useful%20heuristic%3A%20Outlier%20dimensions%20favor%20frequent%0A%20%20tokens%20in%20language%20models&entry.906535625=Iuri%20Macocco%20and%20Nora%20Graichen%20and%20Gemma%20Boleda%20and%20Marco%20Baroni&entry.1292438233=%20%20We%20study%20last-layer%20outlier%20dimensions%2C%20i.e.%20dimensions%20that%20display%20extreme%0Aactivations%20for%20the%20majority%20of%20inputs.%20We%20show%20that%20outlier%20dimensions%20arise%0Ain%20many%20different%20modern%20language%20models%2C%20and%20trace%20their%20function%20back%20to%20the%0Aheuristic%20of%20constantly%20predicting%20frequent%20words.%20We%20further%20show%20how%20a%20model%0Acan%20block%20this%20heuristic%20when%20it%20is%20not%20contextually%20appropriate%2C%20by%20assigning%0Aa%20counterbalancing%20weight%20mass%20to%20the%20remaining%20dimensions%2C%20and%20we%20investigate%0Awhich%20model%20parameters%20boost%20outlier%20dimensions%20and%20when%20they%20arise%20during%0Atraining.%20We%20conclude%20that%20outlier%20dimensions%20are%20a%20specialized%20mechanism%0Adiscovered%20by%20many%20distinct%20models%20to%20implement%20a%20useful%20token%20prediction%0Aheuristic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21718v4&entry.124074799=Read"},
{"title": "Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation\n  Training", "author": "Tidiane Camaret Ndir and Alexander Pfefferle and Robin Tibor Schirrmeister", "abstract": "  Interactive 3D biomedical image segmentation requires efficient models that\ncan iteratively refine predictions based on user prompts. Current foundation\nmodels either lack volumetric awareness or suffer from limited interactive\ncapabilities. We propose a training strategy that combines dynamic volumetric\nprompt generation with content-aware adaptive cropping to optimize the use of\nthe image encoder. Our method simulates realistic user interaction patterns\nduring training while addressing the computational challenges of learning from\nsequential refinement feedback on a single GPU. For efficient training, we\ninitialize our network using the publicly available weights from the\nnnInteractive segmentation model. Evaluation on the \\textbf{Foundation Models\nfor Interactive 3D Biomedical Image Segmentation} competition demonstrates\nstrong performance with an average final Dice score of 0.6385, normalized\nsurface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)\nand 2.5671 (NSD).\n", "link": "http://arxiv.org/abs/2510.03189v1", "date": "2025-10-03", "relevancy": 2.289, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Prompt%20Generation%20for%20Interactive%203D%20Medical%20Image%20Segmentation%0A%20%20Training&body=Title%3A%20Dynamic%20Prompt%20Generation%20for%20Interactive%203D%20Medical%20Image%20Segmentation%0A%20%20Training%0AAuthor%3A%20Tidiane%20Camaret%20Ndir%20and%20Alexander%20Pfefferle%20and%20Robin%20Tibor%20Schirrmeister%0AAbstract%3A%20%20%20Interactive%203D%20biomedical%20image%20segmentation%20requires%20efficient%20models%20that%0Acan%20iteratively%20refine%20predictions%20based%20on%20user%20prompts.%20Current%20foundation%0Amodels%20either%20lack%20volumetric%20awareness%20or%20suffer%20from%20limited%20interactive%0Acapabilities.%20We%20propose%20a%20training%20strategy%20that%20combines%20dynamic%20volumetric%0Aprompt%20generation%20with%20content-aware%20adaptive%20cropping%20to%20optimize%20the%20use%20of%0Athe%20image%20encoder.%20Our%20method%20simulates%20realistic%20user%20interaction%20patterns%0Aduring%20training%20while%20addressing%20the%20computational%20challenges%20of%20learning%20from%0Asequential%20refinement%20feedback%20on%20a%20single%20GPU.%20For%20efficient%20training%2C%20we%0Ainitialize%20our%20network%20using%20the%20publicly%20available%20weights%20from%20the%0AnnInteractive%20segmentation%20model.%20Evaluation%20on%20the%20%5Ctextbf%7BFoundation%20Models%0Afor%20Interactive%203D%20Biomedical%20Image%20Segmentation%7D%20competition%20demonstrates%0Astrong%20performance%20with%20an%20average%20final%20Dice%20score%20of%200.6385%2C%20normalized%0Asurface%20distance%20of%200.6614%2C%20and%20area-under-the-curve%20metrics%20of%202.4799%20%28Dice%29%0Aand%202.5671%20%28NSD%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Prompt%2520Generation%2520for%2520Interactive%25203D%2520Medical%2520Image%2520Segmentation%250A%2520%2520Training%26entry.906535625%3DTidiane%2520Camaret%2520Ndir%2520and%2520Alexander%2520Pfefferle%2520and%2520Robin%2520Tibor%2520Schirrmeister%26entry.1292438233%3D%2520%2520Interactive%25203D%2520biomedical%2520image%2520segmentation%2520requires%2520efficient%2520models%2520that%250Acan%2520iteratively%2520refine%2520predictions%2520based%2520on%2520user%2520prompts.%2520Current%2520foundation%250Amodels%2520either%2520lack%2520volumetric%2520awareness%2520or%2520suffer%2520from%2520limited%2520interactive%250Acapabilities.%2520We%2520propose%2520a%2520training%2520strategy%2520that%2520combines%2520dynamic%2520volumetric%250Aprompt%2520generation%2520with%2520content-aware%2520adaptive%2520cropping%2520to%2520optimize%2520the%2520use%2520of%250Athe%2520image%2520encoder.%2520Our%2520method%2520simulates%2520realistic%2520user%2520interaction%2520patterns%250Aduring%2520training%2520while%2520addressing%2520the%2520computational%2520challenges%2520of%2520learning%2520from%250Asequential%2520refinement%2520feedback%2520on%2520a%2520single%2520GPU.%2520For%2520efficient%2520training%252C%2520we%250Ainitialize%2520our%2520network%2520using%2520the%2520publicly%2520available%2520weights%2520from%2520the%250AnnInteractive%2520segmentation%2520model.%2520Evaluation%2520on%2520the%2520%255Ctextbf%257BFoundation%2520Models%250Afor%2520Interactive%25203D%2520Biomedical%2520Image%2520Segmentation%257D%2520competition%2520demonstrates%250Astrong%2520performance%2520with%2520an%2520average%2520final%2520Dice%2520score%2520of%25200.6385%252C%2520normalized%250Asurface%2520distance%2520of%25200.6614%252C%2520and%2520area-under-the-curve%2520metrics%2520of%25202.4799%2520%2528Dice%2529%250Aand%25202.5671%2520%2528NSD%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Prompt%20Generation%20for%20Interactive%203D%20Medical%20Image%20Segmentation%0A%20%20Training&entry.906535625=Tidiane%20Camaret%20Ndir%20and%20Alexander%20Pfefferle%20and%20Robin%20Tibor%20Schirrmeister&entry.1292438233=%20%20Interactive%203D%20biomedical%20image%20segmentation%20requires%20efficient%20models%20that%0Acan%20iteratively%20refine%20predictions%20based%20on%20user%20prompts.%20Current%20foundation%0Amodels%20either%20lack%20volumetric%20awareness%20or%20suffer%20from%20limited%20interactive%0Acapabilities.%20We%20propose%20a%20training%20strategy%20that%20combines%20dynamic%20volumetric%0Aprompt%20generation%20with%20content-aware%20adaptive%20cropping%20to%20optimize%20the%20use%20of%0Athe%20image%20encoder.%20Our%20method%20simulates%20realistic%20user%20interaction%20patterns%0Aduring%20training%20while%20addressing%20the%20computational%20challenges%20of%20learning%20from%0Asequential%20refinement%20feedback%20on%20a%20single%20GPU.%20For%20efficient%20training%2C%20we%0Ainitialize%20our%20network%20using%20the%20publicly%20available%20weights%20from%20the%0AnnInteractive%20segmentation%20model.%20Evaluation%20on%20the%20%5Ctextbf%7BFoundation%20Models%0Afor%20Interactive%203D%20Biomedical%20Image%20Segmentation%7D%20competition%20demonstrates%0Astrong%20performance%20with%20an%20average%20final%20Dice%20score%20of%200.6385%2C%20normalized%0Asurface%20distance%20of%200.6614%2C%20and%20area-under-the-curve%20metrics%20of%202.4799%20%28Dice%29%0Aand%202.5671%20%28NSD%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03189v1&entry.124074799=Read"},
{"title": "When and Where do Events Switch in Multi-Event Video Generation?", "author": "Ruotong Liao and Guowen Huang and Qing Cheng and Thomas Seidl and Daniel Cremers and Volker Tresp", "abstract": "  Text-to-video (T2V) generation has surged in response to challenging\nquestions, especially when a long video must depict multiple sequential events\nwith temporal coherence and controllable content. Existing methods that extend\nto multi-event generation omit an inspection of the intrinsic factor in event\nshifting. The paper aims to answer the central question: When and where\nmulti-event prompts control event transition during T2V generation. This work\nintroduces MEve, a self-curated prompt suite for evaluating multi-event\ntext-to-video (T2V) generation, and conducts a systematic study of two\nrepresentative model families, i.e., OpenSora and CogVideoX. Extensive\nexperiments demonstrate the importance of early intervention in denoising steps\nand block-wise model layers, revealing the essential factor for multi-event\nvideo generation and highlighting the possibilities for multi-event\nconditioning in future models.\n", "link": "http://arxiv.org/abs/2510.03049v1", "date": "2025-10-03", "relevancy": 2.2878, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5893}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.579}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20and%20Where%20do%20Events%20Switch%20in%20Multi-Event%20Video%20Generation%3F&body=Title%3A%20When%20and%20Where%20do%20Events%20Switch%20in%20Multi-Event%20Video%20Generation%3F%0AAuthor%3A%20Ruotong%20Liao%20and%20Guowen%20Huang%20and%20Qing%20Cheng%20and%20Thomas%20Seidl%20and%20Daniel%20Cremers%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20Text-to-video%20%28T2V%29%20generation%20has%20surged%20in%20response%20to%20challenging%0Aquestions%2C%20especially%20when%20a%20long%20video%20must%20depict%20multiple%20sequential%20events%0Awith%20temporal%20coherence%20and%20controllable%20content.%20Existing%20methods%20that%20extend%0Ato%20multi-event%20generation%20omit%20an%20inspection%20of%20the%20intrinsic%20factor%20in%20event%0Ashifting.%20The%20paper%20aims%20to%20answer%20the%20central%20question%3A%20When%20and%20where%0Amulti-event%20prompts%20control%20event%20transition%20during%20T2V%20generation.%20This%20work%0Aintroduces%20MEve%2C%20a%20self-curated%20prompt%20suite%20for%20evaluating%20multi-event%0Atext-to-video%20%28T2V%29%20generation%2C%20and%20conducts%20a%20systematic%20study%20of%20two%0Arepresentative%20model%20families%2C%20i.e.%2C%20OpenSora%20and%20CogVideoX.%20Extensive%0Aexperiments%20demonstrate%20the%20importance%20of%20early%20intervention%20in%20denoising%20steps%0Aand%20block-wise%20model%20layers%2C%20revealing%20the%20essential%20factor%20for%20multi-event%0Avideo%20generation%20and%20highlighting%20the%20possibilities%20for%20multi-event%0Aconditioning%20in%20future%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520and%2520Where%2520do%2520Events%2520Switch%2520in%2520Multi-Event%2520Video%2520Generation%253F%26entry.906535625%3DRuotong%2520Liao%2520and%2520Guowen%2520Huang%2520and%2520Qing%2520Cheng%2520and%2520Thomas%2520Seidl%2520and%2520Daniel%2520Cremers%2520and%2520Volker%2520Tresp%26entry.1292438233%3D%2520%2520Text-to-video%2520%2528T2V%2529%2520generation%2520has%2520surged%2520in%2520response%2520to%2520challenging%250Aquestions%252C%2520especially%2520when%2520a%2520long%2520video%2520must%2520depict%2520multiple%2520sequential%2520events%250Awith%2520temporal%2520coherence%2520and%2520controllable%2520content.%2520Existing%2520methods%2520that%2520extend%250Ato%2520multi-event%2520generation%2520omit%2520an%2520inspection%2520of%2520the%2520intrinsic%2520factor%2520in%2520event%250Ashifting.%2520The%2520paper%2520aims%2520to%2520answer%2520the%2520central%2520question%253A%2520When%2520and%2520where%250Amulti-event%2520prompts%2520control%2520event%2520transition%2520during%2520T2V%2520generation.%2520This%2520work%250Aintroduces%2520MEve%252C%2520a%2520self-curated%2520prompt%2520suite%2520for%2520evaluating%2520multi-event%250Atext-to-video%2520%2528T2V%2529%2520generation%252C%2520and%2520conducts%2520a%2520systematic%2520study%2520of%2520two%250Arepresentative%2520model%2520families%252C%2520i.e.%252C%2520OpenSora%2520and%2520CogVideoX.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520importance%2520of%2520early%2520intervention%2520in%2520denoising%2520steps%250Aand%2520block-wise%2520model%2520layers%252C%2520revealing%2520the%2520essential%2520factor%2520for%2520multi-event%250Avideo%2520generation%2520and%2520highlighting%2520the%2520possibilities%2520for%2520multi-event%250Aconditioning%2520in%2520future%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20and%20Where%20do%20Events%20Switch%20in%20Multi-Event%20Video%20Generation%3F&entry.906535625=Ruotong%20Liao%20and%20Guowen%20Huang%20and%20Qing%20Cheng%20and%20Thomas%20Seidl%20and%20Daniel%20Cremers%20and%20Volker%20Tresp&entry.1292438233=%20%20Text-to-video%20%28T2V%29%20generation%20has%20surged%20in%20response%20to%20challenging%0Aquestions%2C%20especially%20when%20a%20long%20video%20must%20depict%20multiple%20sequential%20events%0Awith%20temporal%20coherence%20and%20controllable%20content.%20Existing%20methods%20that%20extend%0Ato%20multi-event%20generation%20omit%20an%20inspection%20of%20the%20intrinsic%20factor%20in%20event%0Ashifting.%20The%20paper%20aims%20to%20answer%20the%20central%20question%3A%20When%20and%20where%0Amulti-event%20prompts%20control%20event%20transition%20during%20T2V%20generation.%20This%20work%0Aintroduces%20MEve%2C%20a%20self-curated%20prompt%20suite%20for%20evaluating%20multi-event%0Atext-to-video%20%28T2V%29%20generation%2C%20and%20conducts%20a%20systematic%20study%20of%20two%0Arepresentative%20model%20families%2C%20i.e.%2C%20OpenSora%20and%20CogVideoX.%20Extensive%0Aexperiments%20demonstrate%20the%20importance%20of%20early%20intervention%20in%20denoising%20steps%0Aand%20block-wise%20model%20layers%2C%20revealing%20the%20essential%20factor%20for%20multi-event%0Avideo%20generation%20and%20highlighting%20the%20possibilities%20for%20multi-event%0Aconditioning%20in%20future%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03049v1&entry.124074799=Read"},
{"title": "What Drives Compositional Generalization in Visual Generative Models?", "author": "Karim Farid and Rajat Sahay and Yumna Ali Alnaggar and Simon Schrodi and Volker Fischer and Cordelia Schmid and Thomas Brox", "abstract": "  Compositional generalization, the ability to generate novel combinations of\nknown concepts, is a key ingredient for visual generative models. Yet, not all\nmechanisms that enable or inhibit it are fully understood. In this work, we\nconduct a systematic study of how various design choices influence\ncompositional generalization in image and video generation in a positive or\nnegative way. Through controlled experiments, we identify two key factors: (i)\nwhether the training objective operates on a discrete or continuous\ndistribution, and (ii) to what extent conditioning provides information about\nthe constituent concepts during training. Building on these insights, we show\nthat relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based\nobjective can improve compositional performance in discrete models like\nMaskGIT.\n", "link": "http://arxiv.org/abs/2510.03075v1", "date": "2025-10-03", "relevancy": 2.2851, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6876}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5606}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Drives%20Compositional%20Generalization%20in%20Visual%20Generative%20Models%3F&body=Title%3A%20What%20Drives%20Compositional%20Generalization%20in%20Visual%20Generative%20Models%3F%0AAuthor%3A%20Karim%20Farid%20and%20Rajat%20Sahay%20and%20Yumna%20Ali%20Alnaggar%20and%20Simon%20Schrodi%20and%20Volker%20Fischer%20and%20Cordelia%20Schmid%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20Compositional%20generalization%2C%20the%20ability%20to%20generate%20novel%20combinations%20of%0Aknown%20concepts%2C%20is%20a%20key%20ingredient%20for%20visual%20generative%20models.%20Yet%2C%20not%20all%0Amechanisms%20that%20enable%20or%20inhibit%20it%20are%20fully%20understood.%20In%20this%20work%2C%20we%0Aconduct%20a%20systematic%20study%20of%20how%20various%20design%20choices%20influence%0Acompositional%20generalization%20in%20image%20and%20video%20generation%20in%20a%20positive%20or%0Anegative%20way.%20Through%20controlled%20experiments%2C%20we%20identify%20two%20key%20factors%3A%20%28i%29%0Awhether%20the%20training%20objective%20operates%20on%20a%20discrete%20or%20continuous%0Adistribution%2C%20and%20%28ii%29%20to%20what%20extent%20conditioning%20provides%20information%20about%0Athe%20constituent%20concepts%20during%20training.%20Building%20on%20these%20insights%2C%20we%20show%0Athat%20relaxing%20the%20MaskGIT%20discrete%20loss%20with%20an%20auxiliary%20continuous%20JEPA-based%0Aobjective%20can%20improve%20compositional%20performance%20in%20discrete%20models%20like%0AMaskGIT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Drives%2520Compositional%2520Generalization%2520in%2520Visual%2520Generative%2520Models%253F%26entry.906535625%3DKarim%2520Farid%2520and%2520Rajat%2520Sahay%2520and%2520Yumna%2520Ali%2520Alnaggar%2520and%2520Simon%2520Schrodi%2520and%2520Volker%2520Fischer%2520and%2520Cordelia%2520Schmid%2520and%2520Thomas%2520Brox%26entry.1292438233%3D%2520%2520Compositional%2520generalization%252C%2520the%2520ability%2520to%2520generate%2520novel%2520combinations%2520of%250Aknown%2520concepts%252C%2520is%2520a%2520key%2520ingredient%2520for%2520visual%2520generative%2520models.%2520Yet%252C%2520not%2520all%250Amechanisms%2520that%2520enable%2520or%2520inhibit%2520it%2520are%2520fully%2520understood.%2520In%2520this%2520work%252C%2520we%250Aconduct%2520a%2520systematic%2520study%2520of%2520how%2520various%2520design%2520choices%2520influence%250Acompositional%2520generalization%2520in%2520image%2520and%2520video%2520generation%2520in%2520a%2520positive%2520or%250Anegative%2520way.%2520Through%2520controlled%2520experiments%252C%2520we%2520identify%2520two%2520key%2520factors%253A%2520%2528i%2529%250Awhether%2520the%2520training%2520objective%2520operates%2520on%2520a%2520discrete%2520or%2520continuous%250Adistribution%252C%2520and%2520%2528ii%2529%2520to%2520what%2520extent%2520conditioning%2520provides%2520information%2520about%250Athe%2520constituent%2520concepts%2520during%2520training.%2520Building%2520on%2520these%2520insights%252C%2520we%2520show%250Athat%2520relaxing%2520the%2520MaskGIT%2520discrete%2520loss%2520with%2520an%2520auxiliary%2520continuous%2520JEPA-based%250Aobjective%2520can%2520improve%2520compositional%2520performance%2520in%2520discrete%2520models%2520like%250AMaskGIT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Drives%20Compositional%20Generalization%20in%20Visual%20Generative%20Models%3F&entry.906535625=Karim%20Farid%20and%20Rajat%20Sahay%20and%20Yumna%20Ali%20Alnaggar%20and%20Simon%20Schrodi%20and%20Volker%20Fischer%20and%20Cordelia%20Schmid%20and%20Thomas%20Brox&entry.1292438233=%20%20Compositional%20generalization%2C%20the%20ability%20to%20generate%20novel%20combinations%20of%0Aknown%20concepts%2C%20is%20a%20key%20ingredient%20for%20visual%20generative%20models.%20Yet%2C%20not%20all%0Amechanisms%20that%20enable%20or%20inhibit%20it%20are%20fully%20understood.%20In%20this%20work%2C%20we%0Aconduct%20a%20systematic%20study%20of%20how%20various%20design%20choices%20influence%0Acompositional%20generalization%20in%20image%20and%20video%20generation%20in%20a%20positive%20or%0Anegative%20way.%20Through%20controlled%20experiments%2C%20we%20identify%20two%20key%20factors%3A%20%28i%29%0Awhether%20the%20training%20objective%20operates%20on%20a%20discrete%20or%20continuous%0Adistribution%2C%20and%20%28ii%29%20to%20what%20extent%20conditioning%20provides%20information%20about%0Athe%20constituent%20concepts%20during%20training.%20Building%20on%20these%20insights%2C%20we%20show%0Athat%20relaxing%20the%20MaskGIT%20discrete%20loss%20with%20an%20auxiliary%20continuous%20JEPA-based%0Aobjective%20can%20improve%20compositional%20performance%20in%20discrete%20models%20like%0AMaskGIT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03075v1&entry.124074799=Read"},
{"title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via\n  Multi-Expert Learning", "author": "Tianyu Xu and Jiawei Chen and Jiazhao Zhang and Wenyao Zhang and Zekun Qi and Minghan Li and Zhizheng Zhang and He Wang", "abstract": "  Visual navigation policy is widely regarded as a promising direction, as it\nmimics humans by using egocentric visual observations for navigation. However,\noptical information of visual observations is difficult to be explicitly\nmodeled like LiDAR point clouds or depth maps, which subsequently requires\nintelligent models and large-scale data. To this end, we propose to leverage\nthe intelligence of the Vision-Language-Action (VLA) model to learn diverse\nnavigation capabilities from synthetic expert data in a teacher-student manner.\nSpecifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360\nobservations) based on pretrained large language models and visual foundation\nmodels. For large-scale navigation data, we collect expert data from three\nreinforcement learning (RL) experts trained with privileged depth information\nin three challenging tailor-made environments for different navigation\ncapabilities: reaching, squeezing, and avoiding. We iteratively train our VLA\nmodel using data collected online from RL experts, where the training ratio is\ndynamically balanced based on performance on individual capabilities. Through\nextensive experiments in synthetic environments, we demonstrate that our model\nachieves strong generalization capability. Moreover, we find that our student\nVLA model outperforms the RL teachers, demonstrating the synergistic effect of\nintegrating multiple capabilities. Extensive real-world experiments further\nconfirm the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2510.03142v1", "date": "2025-10-03", "relevancy": 2.2832, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-Nav%3A%20Multi-View%20VLA%20Model%20for%20Robust%20Visual%20Navigation%20via%0A%20%20Multi-Expert%20Learning&body=Title%3A%20MM-Nav%3A%20Multi-View%20VLA%20Model%20for%20Robust%20Visual%20Navigation%20via%0A%20%20Multi-Expert%20Learning%0AAuthor%3A%20Tianyu%20Xu%20and%20Jiawei%20Chen%20and%20Jiazhao%20Zhang%20and%20Wenyao%20Zhang%20and%20Zekun%20Qi%20and%20Minghan%20Li%20and%20Zhizheng%20Zhang%20and%20He%20Wang%0AAbstract%3A%20%20%20Visual%20navigation%20policy%20is%20widely%20regarded%20as%20a%20promising%20direction%2C%20as%20it%0Amimics%20humans%20by%20using%20egocentric%20visual%20observations%20for%20navigation.%20However%2C%0Aoptical%20information%20of%20visual%20observations%20is%20difficult%20to%20be%20explicitly%0Amodeled%20like%20LiDAR%20point%20clouds%20or%20depth%20maps%2C%20which%20subsequently%20requires%0Aintelligent%20models%20and%20large-scale%20data.%20To%20this%20end%2C%20we%20propose%20to%20leverage%0Athe%20intelligence%20of%20the%20Vision-Language-Action%20%28VLA%29%20model%20to%20learn%20diverse%0Anavigation%20capabilities%20from%20synthetic%20expert%20data%20in%20a%20teacher-student%20manner.%0ASpecifically%2C%20we%20implement%20the%20VLA%20model%2C%20MM-Nav%2C%20as%20a%20multi-view%20VLA%20%28with%20360%0Aobservations%29%20based%20on%20pretrained%20large%20language%20models%20and%20visual%20foundation%0Amodels.%20For%20large-scale%20navigation%20data%2C%20we%20collect%20expert%20data%20from%20three%0Areinforcement%20learning%20%28RL%29%20experts%20trained%20with%20privileged%20depth%20information%0Ain%20three%20challenging%20tailor-made%20environments%20for%20different%20navigation%0Acapabilities%3A%20reaching%2C%20squeezing%2C%20and%20avoiding.%20We%20iteratively%20train%20our%20VLA%0Amodel%20using%20data%20collected%20online%20from%20RL%20experts%2C%20where%20the%20training%20ratio%20is%0Adynamically%20balanced%20based%20on%20performance%20on%20individual%20capabilities.%20Through%0Aextensive%20experiments%20in%20synthetic%20environments%2C%20we%20demonstrate%20that%20our%20model%0Aachieves%20strong%20generalization%20capability.%20Moreover%2C%20we%20find%20that%20our%20student%0AVLA%20model%20outperforms%20the%20RL%20teachers%2C%20demonstrating%20the%20synergistic%20effect%20of%0Aintegrating%20multiple%20capabilities.%20Extensive%20real-world%20experiments%20further%0Aconfirm%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-Nav%253A%2520Multi-View%2520VLA%2520Model%2520for%2520Robust%2520Visual%2520Navigation%2520via%250A%2520%2520Multi-Expert%2520Learning%26entry.906535625%3DTianyu%2520Xu%2520and%2520Jiawei%2520Chen%2520and%2520Jiazhao%2520Zhang%2520and%2520Wenyao%2520Zhang%2520and%2520Zekun%2520Qi%2520and%2520Minghan%2520Li%2520and%2520Zhizheng%2520Zhang%2520and%2520He%2520Wang%26entry.1292438233%3D%2520%2520Visual%2520navigation%2520policy%2520is%2520widely%2520regarded%2520as%2520a%2520promising%2520direction%252C%2520as%2520it%250Amimics%2520humans%2520by%2520using%2520egocentric%2520visual%2520observations%2520for%2520navigation.%2520However%252C%250Aoptical%2520information%2520of%2520visual%2520observations%2520is%2520difficult%2520to%2520be%2520explicitly%250Amodeled%2520like%2520LiDAR%2520point%2520clouds%2520or%2520depth%2520maps%252C%2520which%2520subsequently%2520requires%250Aintelligent%2520models%2520and%2520large-scale%2520data.%2520To%2520this%2520end%252C%2520we%2520propose%2520to%2520leverage%250Athe%2520intelligence%2520of%2520the%2520Vision-Language-Action%2520%2528VLA%2529%2520model%2520to%2520learn%2520diverse%250Anavigation%2520capabilities%2520from%2520synthetic%2520expert%2520data%2520in%2520a%2520teacher-student%2520manner.%250ASpecifically%252C%2520we%2520implement%2520the%2520VLA%2520model%252C%2520MM-Nav%252C%2520as%2520a%2520multi-view%2520VLA%2520%2528with%2520360%250Aobservations%2529%2520based%2520on%2520pretrained%2520large%2520language%2520models%2520and%2520visual%2520foundation%250Amodels.%2520For%2520large-scale%2520navigation%2520data%252C%2520we%2520collect%2520expert%2520data%2520from%2520three%250Areinforcement%2520learning%2520%2528RL%2529%2520experts%2520trained%2520with%2520privileged%2520depth%2520information%250Ain%2520three%2520challenging%2520tailor-made%2520environments%2520for%2520different%2520navigation%250Acapabilities%253A%2520reaching%252C%2520squeezing%252C%2520and%2520avoiding.%2520We%2520iteratively%2520train%2520our%2520VLA%250Amodel%2520using%2520data%2520collected%2520online%2520from%2520RL%2520experts%252C%2520where%2520the%2520training%2520ratio%2520is%250Adynamically%2520balanced%2520based%2520on%2520performance%2520on%2520individual%2520capabilities.%2520Through%250Aextensive%2520experiments%2520in%2520synthetic%2520environments%252C%2520we%2520demonstrate%2520that%2520our%2520model%250Aachieves%2520strong%2520generalization%2520capability.%2520Moreover%252C%2520we%2520find%2520that%2520our%2520student%250AVLA%2520model%2520outperforms%2520the%2520RL%2520teachers%252C%2520demonstrating%2520the%2520synergistic%2520effect%2520of%250Aintegrating%2520multiple%2520capabilities.%2520Extensive%2520real-world%2520experiments%2520further%250Aconfirm%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Nav%3A%20Multi-View%20VLA%20Model%20for%20Robust%20Visual%20Navigation%20via%0A%20%20Multi-Expert%20Learning&entry.906535625=Tianyu%20Xu%20and%20Jiawei%20Chen%20and%20Jiazhao%20Zhang%20and%20Wenyao%20Zhang%20and%20Zekun%20Qi%20and%20Minghan%20Li%20and%20Zhizheng%20Zhang%20and%20He%20Wang&entry.1292438233=%20%20Visual%20navigation%20policy%20is%20widely%20regarded%20as%20a%20promising%20direction%2C%20as%20it%0Amimics%20humans%20by%20using%20egocentric%20visual%20observations%20for%20navigation.%20However%2C%0Aoptical%20information%20of%20visual%20observations%20is%20difficult%20to%20be%20explicitly%0Amodeled%20like%20LiDAR%20point%20clouds%20or%20depth%20maps%2C%20which%20subsequently%20requires%0Aintelligent%20models%20and%20large-scale%20data.%20To%20this%20end%2C%20we%20propose%20to%20leverage%0Athe%20intelligence%20of%20the%20Vision-Language-Action%20%28VLA%29%20model%20to%20learn%20diverse%0Anavigation%20capabilities%20from%20synthetic%20expert%20data%20in%20a%20teacher-student%20manner.%0ASpecifically%2C%20we%20implement%20the%20VLA%20model%2C%20MM-Nav%2C%20as%20a%20multi-view%20VLA%20%28with%20360%0Aobservations%29%20based%20on%20pretrained%20large%20language%20models%20and%20visual%20foundation%0Amodels.%20For%20large-scale%20navigation%20data%2C%20we%20collect%20expert%20data%20from%20three%0Areinforcement%20learning%20%28RL%29%20experts%20trained%20with%20privileged%20depth%20information%0Ain%20three%20challenging%20tailor-made%20environments%20for%20different%20navigation%0Acapabilities%3A%20reaching%2C%20squeezing%2C%20and%20avoiding.%20We%20iteratively%20train%20our%20VLA%0Amodel%20using%20data%20collected%20online%20from%20RL%20experts%2C%20where%20the%20training%20ratio%20is%0Adynamically%20balanced%20based%20on%20performance%20on%20individual%20capabilities.%20Through%0Aextensive%20experiments%20in%20synthetic%20environments%2C%20we%20demonstrate%20that%20our%20model%0Aachieves%20strong%20generalization%20capability.%20Moreover%2C%20we%20find%20that%20our%20student%0AVLA%20model%20outperforms%20the%20RL%20teachers%2C%20demonstrating%20the%20synergistic%20effect%20of%0Aintegrating%20multiple%20capabilities.%20Extensive%20real-world%20experiments%20further%0Aconfirm%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03142v1&entry.124074799=Read"},
{"title": "Revisiting Reweighted Risk for Calibration: AURC, Focal, and Inverse\n  Focal Loss", "author": "Han Zhou and Sebastian G. Gruber and Teodora Popordanoska and Matthew B. Blaschko", "abstract": "  Several variants of reweighted risk functionals, such as focal loss, inverse\nfocal loss, and the Area Under the Risk--Coverage Curve (AURC), have been\nproposed for improving model calibration, yet their theoretical connections to\ncalibration errors remain unclear. In this paper, we revisit a broad class of\nweighted risk functions commonly used in deep learning and establish a\nprincipled connection between calibration error and selective classification.\nWe show that minimizing calibration error is closely linked to the selective\nclassification paradigm and demonstrate that optimizing selective risk in\nlow-confidence region naturally leads to improved calibration. This loss shares\na similar reweighting strategy with dual focal loss but offers greater\nflexibility through the choice of confidence score functions (CSFs). Our\napproach uses a bin-based cumulative distribution function (CDF) approximation,\nenabling efficient gradient-based optimization without requiring expensive\nsorting and achieving $O(nK)$ complexity. Empirical evaluations demonstrate\nthat our method achieves competitive calibration performance across a range of\ndatasets and model architectures.\n", "link": "http://arxiv.org/abs/2505.23463v3", "date": "2025-10-03", "relevancy": 2.2807, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4851}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4417}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Reweighted%20Risk%20for%20Calibration%3A%20AURC%2C%20Focal%2C%20and%20Inverse%0A%20%20Focal%20Loss&body=Title%3A%20Revisiting%20Reweighted%20Risk%20for%20Calibration%3A%20AURC%2C%20Focal%2C%20and%20Inverse%0A%20%20Focal%20Loss%0AAuthor%3A%20Han%20Zhou%20and%20Sebastian%20G.%20Gruber%20and%20Teodora%20Popordanoska%20and%20Matthew%20B.%20Blaschko%0AAbstract%3A%20%20%20Several%20variants%20of%20reweighted%20risk%20functionals%2C%20such%20as%20focal%20loss%2C%20inverse%0Afocal%20loss%2C%20and%20the%20Area%20Under%20the%20Risk--Coverage%20Curve%20%28AURC%29%2C%20have%20been%0Aproposed%20for%20improving%20model%20calibration%2C%20yet%20their%20theoretical%20connections%20to%0Acalibration%20errors%20remain%20unclear.%20In%20this%20paper%2C%20we%20revisit%20a%20broad%20class%20of%0Aweighted%20risk%20functions%20commonly%20used%20in%20deep%20learning%20and%20establish%20a%0Aprincipled%20connection%20between%20calibration%20error%20and%20selective%20classification.%0AWe%20show%20that%20minimizing%20calibration%20error%20is%20closely%20linked%20to%20the%20selective%0Aclassification%20paradigm%20and%20demonstrate%20that%20optimizing%20selective%20risk%20in%0Alow-confidence%20region%20naturally%20leads%20to%20improved%20calibration.%20This%20loss%20shares%0Aa%20similar%20reweighting%20strategy%20with%20dual%20focal%20loss%20but%20offers%20greater%0Aflexibility%20through%20the%20choice%20of%20confidence%20score%20functions%20%28CSFs%29.%20Our%0Aapproach%20uses%20a%20bin-based%20cumulative%20distribution%20function%20%28CDF%29%20approximation%2C%0Aenabling%20efficient%20gradient-based%20optimization%20without%20requiring%20expensive%0Asorting%20and%20achieving%20%24O%28nK%29%24%20complexity.%20Empirical%20evaluations%20demonstrate%0Athat%20our%20method%20achieves%20competitive%20calibration%20performance%20across%20a%20range%20of%0Adatasets%20and%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23463v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Reweighted%2520Risk%2520for%2520Calibration%253A%2520AURC%252C%2520Focal%252C%2520and%2520Inverse%250A%2520%2520Focal%2520Loss%26entry.906535625%3DHan%2520Zhou%2520and%2520Sebastian%2520G.%2520Gruber%2520and%2520Teodora%2520Popordanoska%2520and%2520Matthew%2520B.%2520Blaschko%26entry.1292438233%3D%2520%2520Several%2520variants%2520of%2520reweighted%2520risk%2520functionals%252C%2520such%2520as%2520focal%2520loss%252C%2520inverse%250Afocal%2520loss%252C%2520and%2520the%2520Area%2520Under%2520the%2520Risk--Coverage%2520Curve%2520%2528AURC%2529%252C%2520have%2520been%250Aproposed%2520for%2520improving%2520model%2520calibration%252C%2520yet%2520their%2520theoretical%2520connections%2520to%250Acalibration%2520errors%2520remain%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520a%2520broad%2520class%2520of%250Aweighted%2520risk%2520functions%2520commonly%2520used%2520in%2520deep%2520learning%2520and%2520establish%2520a%250Aprincipled%2520connection%2520between%2520calibration%2520error%2520and%2520selective%2520classification.%250AWe%2520show%2520that%2520minimizing%2520calibration%2520error%2520is%2520closely%2520linked%2520to%2520the%2520selective%250Aclassification%2520paradigm%2520and%2520demonstrate%2520that%2520optimizing%2520selective%2520risk%2520in%250Alow-confidence%2520region%2520naturally%2520leads%2520to%2520improved%2520calibration.%2520This%2520loss%2520shares%250Aa%2520similar%2520reweighting%2520strategy%2520with%2520dual%2520focal%2520loss%2520but%2520offers%2520greater%250Aflexibility%2520through%2520the%2520choice%2520of%2520confidence%2520score%2520functions%2520%2528CSFs%2529.%2520Our%250Aapproach%2520uses%2520a%2520bin-based%2520cumulative%2520distribution%2520function%2520%2528CDF%2529%2520approximation%252C%250Aenabling%2520efficient%2520gradient-based%2520optimization%2520without%2520requiring%2520expensive%250Asorting%2520and%2520achieving%2520%2524O%2528nK%2529%2524%2520complexity.%2520Empirical%2520evaluations%2520demonstrate%250Athat%2520our%2520method%2520achieves%2520competitive%2520calibration%2520performance%2520across%2520a%2520range%2520of%250Adatasets%2520and%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23463v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Reweighted%20Risk%20for%20Calibration%3A%20AURC%2C%20Focal%2C%20and%20Inverse%0A%20%20Focal%20Loss&entry.906535625=Han%20Zhou%20and%20Sebastian%20G.%20Gruber%20and%20Teodora%20Popordanoska%20and%20Matthew%20B.%20Blaschko&entry.1292438233=%20%20Several%20variants%20of%20reweighted%20risk%20functionals%2C%20such%20as%20focal%20loss%2C%20inverse%0Afocal%20loss%2C%20and%20the%20Area%20Under%20the%20Risk--Coverage%20Curve%20%28AURC%29%2C%20have%20been%0Aproposed%20for%20improving%20model%20calibration%2C%20yet%20their%20theoretical%20connections%20to%0Acalibration%20errors%20remain%20unclear.%20In%20this%20paper%2C%20we%20revisit%20a%20broad%20class%20of%0Aweighted%20risk%20functions%20commonly%20used%20in%20deep%20learning%20and%20establish%20a%0Aprincipled%20connection%20between%20calibration%20error%20and%20selective%20classification.%0AWe%20show%20that%20minimizing%20calibration%20error%20is%20closely%20linked%20to%20the%20selective%0Aclassification%20paradigm%20and%20demonstrate%20that%20optimizing%20selective%20risk%20in%0Alow-confidence%20region%20naturally%20leads%20to%20improved%20calibration.%20This%20loss%20shares%0Aa%20similar%20reweighting%20strategy%20with%20dual%20focal%20loss%20but%20offers%20greater%0Aflexibility%20through%20the%20choice%20of%20confidence%20score%20functions%20%28CSFs%29.%20Our%0Aapproach%20uses%20a%20bin-based%20cumulative%20distribution%20function%20%28CDF%29%20approximation%2C%0Aenabling%20efficient%20gradient-based%20optimization%20without%20requiring%20expensive%0Asorting%20and%20achieving%20%24O%28nK%29%24%20complexity.%20Empirical%20evaluations%20demonstrate%0Athat%20our%20method%20achieves%20competitive%20calibration%20performance%20across%20a%20range%20of%0Adatasets%20and%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23463v3&entry.124074799=Read"},
{"title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized\n  Speech Generation", "author": "Jeongsoo Choi and Ji-Hoon Kim and Kim Sung-Bin and Tae-Hyun Oh and Joon Son Chung", "abstract": "  In this paper, we address the task of multimodal-to-speech generation, which\naims to synthesize high-quality speech from multiple input modalities: text,\nvideo, and reference audio. This task has gained increasing attention due to\nits wide range of applications, such as film production, dubbing, and virtual\navatars. Despite recent progress, existing methods still suffer from\nlimitations in speech intelligibility, audio-video synchronization, speech\nnaturalness, and voice similarity to the reference speaker. To address these\nchallenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer\nthat generates accurate, synchronized, and natural-sounding speech from aligned\nmultimodal inputs. Built upon the in-context learning capability of the DiT\narchitecture, AlignDiT explores three effective strategies to align multimodal\nrepresentations. Furthermore, we introduce a novel multimodal classifier-free\nguidance mechanism that allows the model to adaptively balance information from\neach modality during speech synthesis. Extensive experiments demonstrate that\nAlignDiT significantly outperforms existing methods across multiple benchmarks\nin terms of quality, synchronization, and speaker similarity. Moreover,\nAlignDiT exhibits strong generalization capability across various multimodal\ntasks, such as video-to-speech synthesis and visual forced alignment,\nconsistently achieving state-of-the-art performance. The demo page is available\nat https://mm.kaist.ac.kr/projects/AlignDiT.\n", "link": "http://arxiv.org/abs/2504.20629v2", "date": "2025-10-03", "relevancy": 2.2784, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5872}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5578}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignDiT%3A%20Multimodal%20Aligned%20Diffusion%20Transformer%20for%20Synchronized%0A%20%20Speech%20Generation&body=Title%3A%20AlignDiT%3A%20Multimodal%20Aligned%20Diffusion%20Transformer%20for%20Synchronized%0A%20%20Speech%20Generation%0AAuthor%3A%20Jeongsoo%20Choi%20and%20Ji-Hoon%20Kim%20and%20Kim%20Sung-Bin%20and%20Tae-Hyun%20Oh%20and%20Joon%20Son%20Chung%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20task%20of%20multimodal-to-speech%20generation%2C%20which%0Aaims%20to%20synthesize%20high-quality%20speech%20from%20multiple%20input%20modalities%3A%20text%2C%0Avideo%2C%20and%20reference%20audio.%20This%20task%20has%20gained%20increasing%20attention%20due%20to%0Aits%20wide%20range%20of%20applications%2C%20such%20as%20film%20production%2C%20dubbing%2C%20and%20virtual%0Aavatars.%20Despite%20recent%20progress%2C%20existing%20methods%20still%20suffer%20from%0Alimitations%20in%20speech%20intelligibility%2C%20audio-video%20synchronization%2C%20speech%0Anaturalness%2C%20and%20voice%20similarity%20to%20the%20reference%20speaker.%20To%20address%20these%0Achallenges%2C%20we%20propose%20AlignDiT%2C%20a%20multimodal%20Aligned%20Diffusion%20Transformer%0Athat%20generates%20accurate%2C%20synchronized%2C%20and%20natural-sounding%20speech%20from%20aligned%0Amultimodal%20inputs.%20Built%20upon%20the%20in-context%20learning%20capability%20of%20the%20DiT%0Aarchitecture%2C%20AlignDiT%20explores%20three%20effective%20strategies%20to%20align%20multimodal%0Arepresentations.%20Furthermore%2C%20we%20introduce%20a%20novel%20multimodal%20classifier-free%0Aguidance%20mechanism%20that%20allows%20the%20model%20to%20adaptively%20balance%20information%20from%0Aeach%20modality%20during%20speech%20synthesis.%20Extensive%20experiments%20demonstrate%20that%0AAlignDiT%20significantly%20outperforms%20existing%20methods%20across%20multiple%20benchmarks%0Ain%20terms%20of%20quality%2C%20synchronization%2C%20and%20speaker%20similarity.%20Moreover%2C%0AAlignDiT%20exhibits%20strong%20generalization%20capability%20across%20various%20multimodal%0Atasks%2C%20such%20as%20video-to-speech%20synthesis%20and%20visual%20forced%20alignment%2C%0Aconsistently%20achieving%20state-of-the-art%20performance.%20The%20demo%20page%20is%20available%0Aat%20https%3A//mm.kaist.ac.kr/projects/AlignDiT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20629v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignDiT%253A%2520Multimodal%2520Aligned%2520Diffusion%2520Transformer%2520for%2520Synchronized%250A%2520%2520Speech%2520Generation%26entry.906535625%3DJeongsoo%2520Choi%2520and%2520Ji-Hoon%2520Kim%2520and%2520Kim%2520Sung-Bin%2520and%2520Tae-Hyun%2520Oh%2520and%2520Joon%2520Son%2520Chung%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520task%2520of%2520multimodal-to-speech%2520generation%252C%2520which%250Aaims%2520to%2520synthesize%2520high-quality%2520speech%2520from%2520multiple%2520input%2520modalities%253A%2520text%252C%250Avideo%252C%2520and%2520reference%2520audio.%2520This%2520task%2520has%2520gained%2520increasing%2520attention%2520due%2520to%250Aits%2520wide%2520range%2520of%2520applications%252C%2520such%2520as%2520film%2520production%252C%2520dubbing%252C%2520and%2520virtual%250Aavatars.%2520Despite%2520recent%2520progress%252C%2520existing%2520methods%2520still%2520suffer%2520from%250Alimitations%2520in%2520speech%2520intelligibility%252C%2520audio-video%2520synchronization%252C%2520speech%250Anaturalness%252C%2520and%2520voice%2520similarity%2520to%2520the%2520reference%2520speaker.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520AlignDiT%252C%2520a%2520multimodal%2520Aligned%2520Diffusion%2520Transformer%250Athat%2520generates%2520accurate%252C%2520synchronized%252C%2520and%2520natural-sounding%2520speech%2520from%2520aligned%250Amultimodal%2520inputs.%2520Built%2520upon%2520the%2520in-context%2520learning%2520capability%2520of%2520the%2520DiT%250Aarchitecture%252C%2520AlignDiT%2520explores%2520three%2520effective%2520strategies%2520to%2520align%2520multimodal%250Arepresentations.%2520Furthermore%252C%2520we%2520introduce%2520a%2520novel%2520multimodal%2520classifier-free%250Aguidance%2520mechanism%2520that%2520allows%2520the%2520model%2520to%2520adaptively%2520balance%2520information%2520from%250Aeach%2520modality%2520during%2520speech%2520synthesis.%2520Extensive%2520experiments%2520demonstrate%2520that%250AAlignDiT%2520significantly%2520outperforms%2520existing%2520methods%2520across%2520multiple%2520benchmarks%250Ain%2520terms%2520of%2520quality%252C%2520synchronization%252C%2520and%2520speaker%2520similarity.%2520Moreover%252C%250AAlignDiT%2520exhibits%2520strong%2520generalization%2520capability%2520across%2520various%2520multimodal%250Atasks%252C%2520such%2520as%2520video-to-speech%2520synthesis%2520and%2520visual%2520forced%2520alignment%252C%250Aconsistently%2520achieving%2520state-of-the-art%2520performance.%2520The%2520demo%2520page%2520is%2520available%250Aat%2520https%253A//mm.kaist.ac.kr/projects/AlignDiT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20629v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignDiT%3A%20Multimodal%20Aligned%20Diffusion%20Transformer%20for%20Synchronized%0A%20%20Speech%20Generation&entry.906535625=Jeongsoo%20Choi%20and%20Ji-Hoon%20Kim%20and%20Kim%20Sung-Bin%20and%20Tae-Hyun%20Oh%20and%20Joon%20Son%20Chung&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20task%20of%20multimodal-to-speech%20generation%2C%20which%0Aaims%20to%20synthesize%20high-quality%20speech%20from%20multiple%20input%20modalities%3A%20text%2C%0Avideo%2C%20and%20reference%20audio.%20This%20task%20has%20gained%20increasing%20attention%20due%20to%0Aits%20wide%20range%20of%20applications%2C%20such%20as%20film%20production%2C%20dubbing%2C%20and%20virtual%0Aavatars.%20Despite%20recent%20progress%2C%20existing%20methods%20still%20suffer%20from%0Alimitations%20in%20speech%20intelligibility%2C%20audio-video%20synchronization%2C%20speech%0Anaturalness%2C%20and%20voice%20similarity%20to%20the%20reference%20speaker.%20To%20address%20these%0Achallenges%2C%20we%20propose%20AlignDiT%2C%20a%20multimodal%20Aligned%20Diffusion%20Transformer%0Athat%20generates%20accurate%2C%20synchronized%2C%20and%20natural-sounding%20speech%20from%20aligned%0Amultimodal%20inputs.%20Built%20upon%20the%20in-context%20learning%20capability%20of%20the%20DiT%0Aarchitecture%2C%20AlignDiT%20explores%20three%20effective%20strategies%20to%20align%20multimodal%0Arepresentations.%20Furthermore%2C%20we%20introduce%20a%20novel%20multimodal%20classifier-free%0Aguidance%20mechanism%20that%20allows%20the%20model%20to%20adaptively%20balance%20information%20from%0Aeach%20modality%20during%20speech%20synthesis.%20Extensive%20experiments%20demonstrate%20that%0AAlignDiT%20significantly%20outperforms%20existing%20methods%20across%20multiple%20benchmarks%0Ain%20terms%20of%20quality%2C%20synchronization%2C%20and%20speaker%20similarity.%20Moreover%2C%0AAlignDiT%20exhibits%20strong%20generalization%20capability%20across%20various%20multimodal%0Atasks%2C%20such%20as%20video-to-speech%20synthesis%20and%20visual%20forced%20alignment%2C%0Aconsistently%20achieving%20state-of-the-art%20performance.%20The%20demo%20page%20is%20available%0Aat%20https%3A//mm.kaist.ac.kr/projects/AlignDiT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20629v2&entry.124074799=Read"},
{"title": "Learned IMU Bias Prediction for Invariant Visual Inertial Odometry", "author": "Abdullah Altawaitan and Jason Stanley and Sambaran Ghosal and Thai Duong and Nikolay Atanasov", "abstract": "  Autonomous mobile robots operating in novel environments depend critically on\naccurate state estimation, often utilizing visual and inertial measurements.\nRecent work has shown that an invariant formulation of the extended Kalman\nfilter improves the convergence and robustness of visual-inertial odometry by\nutilizing the Lie group structure of a robot's position, velocity, and\norientation states. However, inertial sensors also require measurement bias\nestimation, yet introducing the bias in the filter state breaks the Lie group\nsymmetry. In this paper, we design a neural network to predict the bias of an\ninertial measurement unit (IMU) from a sequence of previous IMU measurements.\nThis allows us to use an invariant filter for visual inertial odometry, relying\non the learned bias prediction rather than introducing the bias in the filter\nstate. We demonstrate that an invariant multi-state constraint Kalman filter\n(MSCKF) with learned bias predictions achieves robust visual-inertial odometry\nin real experiments, even when visual information is unavailable for extended\nperiods and the system needs to rely solely on IMU measurements.\n", "link": "http://arxiv.org/abs/2505.06748v2", "date": "2025-10-03", "relevancy": 2.2677, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6429}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20IMU%20Bias%20Prediction%20for%20Invariant%20Visual%20Inertial%20Odometry&body=Title%3A%20Learned%20IMU%20Bias%20Prediction%20for%20Invariant%20Visual%20Inertial%20Odometry%0AAuthor%3A%20Abdullah%20Altawaitan%20and%20Jason%20Stanley%20and%20Sambaran%20Ghosal%20and%20Thai%20Duong%20and%20Nikolay%20Atanasov%0AAbstract%3A%20%20%20Autonomous%20mobile%20robots%20operating%20in%20novel%20environments%20depend%20critically%20on%0Aaccurate%20state%20estimation%2C%20often%20utilizing%20visual%20and%20inertial%20measurements.%0ARecent%20work%20has%20shown%20that%20an%20invariant%20formulation%20of%20the%20extended%20Kalman%0Afilter%20improves%20the%20convergence%20and%20robustness%20of%20visual-inertial%20odometry%20by%0Autilizing%20the%20Lie%20group%20structure%20of%20a%20robot%27s%20position%2C%20velocity%2C%20and%0Aorientation%20states.%20However%2C%20inertial%20sensors%20also%20require%20measurement%20bias%0Aestimation%2C%20yet%20introducing%20the%20bias%20in%20the%20filter%20state%20breaks%20the%20Lie%20group%0Asymmetry.%20In%20this%20paper%2C%20we%20design%20a%20neural%20network%20to%20predict%20the%20bias%20of%20an%0Ainertial%20measurement%20unit%20%28IMU%29%20from%20a%20sequence%20of%20previous%20IMU%20measurements.%0AThis%20allows%20us%20to%20use%20an%20invariant%20filter%20for%20visual%20inertial%20odometry%2C%20relying%0Aon%20the%20learned%20bias%20prediction%20rather%20than%20introducing%20the%20bias%20in%20the%20filter%0Astate.%20We%20demonstrate%20that%20an%20invariant%20multi-state%20constraint%20Kalman%20filter%0A%28MSCKF%29%20with%20learned%20bias%20predictions%20achieves%20robust%20visual-inertial%20odometry%0Ain%20real%20experiments%2C%20even%20when%20visual%20information%20is%20unavailable%20for%20extended%0Aperiods%20and%20the%20system%20needs%20to%20rely%20solely%20on%20IMU%20measurements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06748v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520IMU%2520Bias%2520Prediction%2520for%2520Invariant%2520Visual%2520Inertial%2520Odometry%26entry.906535625%3DAbdullah%2520Altawaitan%2520and%2520Jason%2520Stanley%2520and%2520Sambaran%2520Ghosal%2520and%2520Thai%2520Duong%2520and%2520Nikolay%2520Atanasov%26entry.1292438233%3D%2520%2520Autonomous%2520mobile%2520robots%2520operating%2520in%2520novel%2520environments%2520depend%2520critically%2520on%250Aaccurate%2520state%2520estimation%252C%2520often%2520utilizing%2520visual%2520and%2520inertial%2520measurements.%250ARecent%2520work%2520has%2520shown%2520that%2520an%2520invariant%2520formulation%2520of%2520the%2520extended%2520Kalman%250Afilter%2520improves%2520the%2520convergence%2520and%2520robustness%2520of%2520visual-inertial%2520odometry%2520by%250Autilizing%2520the%2520Lie%2520group%2520structure%2520of%2520a%2520robot%2527s%2520position%252C%2520velocity%252C%2520and%250Aorientation%2520states.%2520However%252C%2520inertial%2520sensors%2520also%2520require%2520measurement%2520bias%250Aestimation%252C%2520yet%2520introducing%2520the%2520bias%2520in%2520the%2520filter%2520state%2520breaks%2520the%2520Lie%2520group%250Asymmetry.%2520In%2520this%2520paper%252C%2520we%2520design%2520a%2520neural%2520network%2520to%2520predict%2520the%2520bias%2520of%2520an%250Ainertial%2520measurement%2520unit%2520%2528IMU%2529%2520from%2520a%2520sequence%2520of%2520previous%2520IMU%2520measurements.%250AThis%2520allows%2520us%2520to%2520use%2520an%2520invariant%2520filter%2520for%2520visual%2520inertial%2520odometry%252C%2520relying%250Aon%2520the%2520learned%2520bias%2520prediction%2520rather%2520than%2520introducing%2520the%2520bias%2520in%2520the%2520filter%250Astate.%2520We%2520demonstrate%2520that%2520an%2520invariant%2520multi-state%2520constraint%2520Kalman%2520filter%250A%2528MSCKF%2529%2520with%2520learned%2520bias%2520predictions%2520achieves%2520robust%2520visual-inertial%2520odometry%250Ain%2520real%2520experiments%252C%2520even%2520when%2520visual%2520information%2520is%2520unavailable%2520for%2520extended%250Aperiods%2520and%2520the%2520system%2520needs%2520to%2520rely%2520solely%2520on%2520IMU%2520measurements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06748v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20IMU%20Bias%20Prediction%20for%20Invariant%20Visual%20Inertial%20Odometry&entry.906535625=Abdullah%20Altawaitan%20and%20Jason%20Stanley%20and%20Sambaran%20Ghosal%20and%20Thai%20Duong%20and%20Nikolay%20Atanasov&entry.1292438233=%20%20Autonomous%20mobile%20robots%20operating%20in%20novel%20environments%20depend%20critically%20on%0Aaccurate%20state%20estimation%2C%20often%20utilizing%20visual%20and%20inertial%20measurements.%0ARecent%20work%20has%20shown%20that%20an%20invariant%20formulation%20of%20the%20extended%20Kalman%0Afilter%20improves%20the%20convergence%20and%20robustness%20of%20visual-inertial%20odometry%20by%0Autilizing%20the%20Lie%20group%20structure%20of%20a%20robot%27s%20position%2C%20velocity%2C%20and%0Aorientation%20states.%20However%2C%20inertial%20sensors%20also%20require%20measurement%20bias%0Aestimation%2C%20yet%20introducing%20the%20bias%20in%20the%20filter%20state%20breaks%20the%20Lie%20group%0Asymmetry.%20In%20this%20paper%2C%20we%20design%20a%20neural%20network%20to%20predict%20the%20bias%20of%20an%0Ainertial%20measurement%20unit%20%28IMU%29%20from%20a%20sequence%20of%20previous%20IMU%20measurements.%0AThis%20allows%20us%20to%20use%20an%20invariant%20filter%20for%20visual%20inertial%20odometry%2C%20relying%0Aon%20the%20learned%20bias%20prediction%20rather%20than%20introducing%20the%20bias%20in%20the%20filter%0Astate.%20We%20demonstrate%20that%20an%20invariant%20multi-state%20constraint%20Kalman%20filter%0A%28MSCKF%29%20with%20learned%20bias%20predictions%20achieves%20robust%20visual-inertial%20odometry%0Ain%20real%20experiments%2C%20even%20when%20visual%20information%20is%20unavailable%20for%20extended%0Aperiods%20and%20the%20system%20needs%20to%20rely%20solely%20on%20IMU%20measurements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06748v2&entry.124074799=Read"},
{"title": "LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks\n  for Multimodal Large Language Models", "author": "Ci-Siang Lin and Min-Hung Chen and Yu-Yang Sheng and Yu-Chiang Frank Wang", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved strong performance on\ngeneral visual benchmarks but struggle with out-of-distribution (OOD) tasks in\nspecialized domains such as medical imaging, where labeled data is limited and\nexpensive. We introduce LEAML, a label-efficient adaptation framework that\nleverages both scarce labeled VQA samples and abundant unlabeled images. Our\napproach generates domain-relevant pseudo question-answer pairs for unlabeled\ndata using a QA generator regularized by caption distillation. Importantly, we\nselectively update only those neurons most relevant to question-answering,\nenabling the QA Generator to efficiently acquire domain-specific knowledge\nduring distillation. Experiments on gastrointestinal endoscopy and sports VQA\ndemonstrate that LEAML consistently outperforms standard fine-tuning under\nminimal supervision, highlighting the effectiveness of our proposed LEAML\nframework.\n", "link": "http://arxiv.org/abs/2510.03232v1", "date": "2025-10-03", "relevancy": 2.2624, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5894}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEAML%3A%20Label-Efficient%20Adaptation%20to%20Out-of-Distribution%20Visual%20Tasks%0A%20%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20LEAML%3A%20Label-Efficient%20Adaptation%20to%20Out-of-Distribution%20Visual%20Tasks%0A%20%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Ci-Siang%20Lin%20and%20Min-Hung%20Chen%20and%20Yu-Yang%20Sheng%20and%20Yu-Chiang%20Frank%20Wang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20strong%20performance%20on%0Ageneral%20visual%20benchmarks%20but%20struggle%20with%20out-of-distribution%20%28OOD%29%20tasks%20in%0Aspecialized%20domains%20such%20as%20medical%20imaging%2C%20where%20labeled%20data%20is%20limited%20and%0Aexpensive.%20We%20introduce%20LEAML%2C%20a%20label-efficient%20adaptation%20framework%20that%0Aleverages%20both%20scarce%20labeled%20VQA%20samples%20and%20abundant%20unlabeled%20images.%20Our%0Aapproach%20generates%20domain-relevant%20pseudo%20question-answer%20pairs%20for%20unlabeled%0Adata%20using%20a%20QA%20generator%20regularized%20by%20caption%20distillation.%20Importantly%2C%20we%0Aselectively%20update%20only%20those%20neurons%20most%20relevant%20to%20question-answering%2C%0Aenabling%20the%20QA%20Generator%20to%20efficiently%20acquire%20domain-specific%20knowledge%0Aduring%20distillation.%20Experiments%20on%20gastrointestinal%20endoscopy%20and%20sports%20VQA%0Ademonstrate%20that%20LEAML%20consistently%20outperforms%20standard%20fine-tuning%20under%0Aminimal%20supervision%2C%20highlighting%20the%20effectiveness%20of%20our%20proposed%20LEAML%0Aframework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEAML%253A%2520Label-Efficient%2520Adaptation%2520to%2520Out-of-Distribution%2520Visual%2520Tasks%250A%2520%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DCi-Siang%2520Lin%2520and%2520Min-Hung%2520Chen%2520and%2520Yu-Yang%2520Sheng%2520and%2520Yu-Chiang%2520Frank%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520strong%2520performance%2520on%250Ageneral%2520visual%2520benchmarks%2520but%2520struggle%2520with%2520out-of-distribution%2520%2528OOD%2529%2520tasks%2520in%250Aspecialized%2520domains%2520such%2520as%2520medical%2520imaging%252C%2520where%2520labeled%2520data%2520is%2520limited%2520and%250Aexpensive.%2520We%2520introduce%2520LEAML%252C%2520a%2520label-efficient%2520adaptation%2520framework%2520that%250Aleverages%2520both%2520scarce%2520labeled%2520VQA%2520samples%2520and%2520abundant%2520unlabeled%2520images.%2520Our%250Aapproach%2520generates%2520domain-relevant%2520pseudo%2520question-answer%2520pairs%2520for%2520unlabeled%250Adata%2520using%2520a%2520QA%2520generator%2520regularized%2520by%2520caption%2520distillation.%2520Importantly%252C%2520we%250Aselectively%2520update%2520only%2520those%2520neurons%2520most%2520relevant%2520to%2520question-answering%252C%250Aenabling%2520the%2520QA%2520Generator%2520to%2520efficiently%2520acquire%2520domain-specific%2520knowledge%250Aduring%2520distillation.%2520Experiments%2520on%2520gastrointestinal%2520endoscopy%2520and%2520sports%2520VQA%250Ademonstrate%2520that%2520LEAML%2520consistently%2520outperforms%2520standard%2520fine-tuning%2520under%250Aminimal%2520supervision%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520proposed%2520LEAML%250Aframework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAML%3A%20Label-Efficient%20Adaptation%20to%20Out-of-Distribution%20Visual%20Tasks%0A%20%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Ci-Siang%20Lin%20and%20Min-Hung%20Chen%20and%20Yu-Yang%20Sheng%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20strong%20performance%20on%0Ageneral%20visual%20benchmarks%20but%20struggle%20with%20out-of-distribution%20%28OOD%29%20tasks%20in%0Aspecialized%20domains%20such%20as%20medical%20imaging%2C%20where%20labeled%20data%20is%20limited%20and%0Aexpensive.%20We%20introduce%20LEAML%2C%20a%20label-efficient%20adaptation%20framework%20that%0Aleverages%20both%20scarce%20labeled%20VQA%20samples%20and%20abundant%20unlabeled%20images.%20Our%0Aapproach%20generates%20domain-relevant%20pseudo%20question-answer%20pairs%20for%20unlabeled%0Adata%20using%20a%20QA%20generator%20regularized%20by%20caption%20distillation.%20Importantly%2C%20we%0Aselectively%20update%20only%20those%20neurons%20most%20relevant%20to%20question-answering%2C%0Aenabling%20the%20QA%20Generator%20to%20efficiently%20acquire%20domain-specific%20knowledge%0Aduring%20distillation.%20Experiments%20on%20gastrointestinal%20endoscopy%20and%20sports%20VQA%0Ademonstrate%20that%20LEAML%20consistently%20outperforms%20standard%20fine-tuning%20under%0Aminimal%20supervision%2C%20highlighting%20the%20effectiveness%20of%20our%20proposed%20LEAML%0Aframework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03232v1&entry.124074799=Read"},
{"title": "Ultra-Efficient Decoding for End-to-End Neural Compression and\n  Reconstruction", "author": "Ethan G. Rogers and Cheng Wang", "abstract": "  Image compression and reconstruction are crucial for various digital\napplications. While contemporary neural compression methods achieve impressive\ncompression rates, the adoption of such technology has been largely hindered by\nthe complexity and large computational costs of the convolution-based decoders\nduring data reconstruction. To address the decoder bottleneck in neural\ncompression, we develop a new compression-reconstruction framework based on\nincorporating low-rank representation in an autoencoder with vector\nquantization. We demonstrated that performing a series of computationally\nefficient low-rank operations on the learned latent representation of images\ncan efficiently reconstruct the data with high quality. Our approach\ndramatically reduces the computational overhead in the decoding phase of neural\ncompression/reconstruction, essentially eliminating the decoder compute\nbottleneck while maintaining high fidelity of image outputs.\n", "link": "http://arxiv.org/abs/2510.01407v2", "date": "2025-10-03", "relevancy": 2.248, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5656}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5641}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra-Efficient%20Decoding%20for%20End-to-End%20Neural%20Compression%20and%0A%20%20Reconstruction&body=Title%3A%20Ultra-Efficient%20Decoding%20for%20End-to-End%20Neural%20Compression%20and%0A%20%20Reconstruction%0AAuthor%3A%20Ethan%20G.%20Rogers%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20Image%20compression%20and%20reconstruction%20are%20crucial%20for%20various%20digital%0Aapplications.%20While%20contemporary%20neural%20compression%20methods%20achieve%20impressive%0Acompression%20rates%2C%20the%20adoption%20of%20such%20technology%20has%20been%20largely%20hindered%20by%0Athe%20complexity%20and%20large%20computational%20costs%20of%20the%20convolution-based%20decoders%0Aduring%20data%20reconstruction.%20To%20address%20the%20decoder%20bottleneck%20in%20neural%0Acompression%2C%20we%20develop%20a%20new%20compression-reconstruction%20framework%20based%20on%0Aincorporating%20low-rank%20representation%20in%20an%20autoencoder%20with%20vector%0Aquantization.%20We%20demonstrated%20that%20performing%20a%20series%20of%20computationally%0Aefficient%20low-rank%20operations%20on%20the%20learned%20latent%20representation%20of%20images%0Acan%20efficiently%20reconstruct%20the%20data%20with%20high%20quality.%20Our%20approach%0Adramatically%20reduces%20the%20computational%20overhead%20in%20the%20decoding%20phase%20of%20neural%0Acompression/reconstruction%2C%20essentially%20eliminating%20the%20decoder%20compute%0Abottleneck%20while%20maintaining%20high%20fidelity%20of%20image%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra-Efficient%2520Decoding%2520for%2520End-to-End%2520Neural%2520Compression%2520and%250A%2520%2520Reconstruction%26entry.906535625%3DEthan%2520G.%2520Rogers%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520Image%2520compression%2520and%2520reconstruction%2520are%2520crucial%2520for%2520various%2520digital%250Aapplications.%2520While%2520contemporary%2520neural%2520compression%2520methods%2520achieve%2520impressive%250Acompression%2520rates%252C%2520the%2520adoption%2520of%2520such%2520technology%2520has%2520been%2520largely%2520hindered%2520by%250Athe%2520complexity%2520and%2520large%2520computational%2520costs%2520of%2520the%2520convolution-based%2520decoders%250Aduring%2520data%2520reconstruction.%2520To%2520address%2520the%2520decoder%2520bottleneck%2520in%2520neural%250Acompression%252C%2520we%2520develop%2520a%2520new%2520compression-reconstruction%2520framework%2520based%2520on%250Aincorporating%2520low-rank%2520representation%2520in%2520an%2520autoencoder%2520with%2520vector%250Aquantization.%2520We%2520demonstrated%2520that%2520performing%2520a%2520series%2520of%2520computationally%250Aefficient%2520low-rank%2520operations%2520on%2520the%2520learned%2520latent%2520representation%2520of%2520images%250Acan%2520efficiently%2520reconstruct%2520the%2520data%2520with%2520high%2520quality.%2520Our%2520approach%250Adramatically%2520reduces%2520the%2520computational%2520overhead%2520in%2520the%2520decoding%2520phase%2520of%2520neural%250Acompression/reconstruction%252C%2520essentially%2520eliminating%2520the%2520decoder%2520compute%250Abottleneck%2520while%2520maintaining%2520high%2520fidelity%2520of%2520image%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-Efficient%20Decoding%20for%20End-to-End%20Neural%20Compression%20and%0A%20%20Reconstruction&entry.906535625=Ethan%20G.%20Rogers%20and%20Cheng%20Wang&entry.1292438233=%20%20Image%20compression%20and%20reconstruction%20are%20crucial%20for%20various%20digital%0Aapplications.%20While%20contemporary%20neural%20compression%20methods%20achieve%20impressive%0Acompression%20rates%2C%20the%20adoption%20of%20such%20technology%20has%20been%20largely%20hindered%20by%0Athe%20complexity%20and%20large%20computational%20costs%20of%20the%20convolution-based%20decoders%0Aduring%20data%20reconstruction.%20To%20address%20the%20decoder%20bottleneck%20in%20neural%0Acompression%2C%20we%20develop%20a%20new%20compression-reconstruction%20framework%20based%20on%0Aincorporating%20low-rank%20representation%20in%20an%20autoencoder%20with%20vector%0Aquantization.%20We%20demonstrated%20that%20performing%20a%20series%20of%20computationally%0Aefficient%20low-rank%20operations%20on%20the%20learned%20latent%20representation%20of%20images%0Acan%20efficiently%20reconstruct%20the%20data%20with%20high%20quality.%20Our%20approach%0Adramatically%20reduces%20the%20computational%20overhead%20in%20the%20decoding%20phase%20of%20neural%0Acompression/reconstruction%2C%20essentially%20eliminating%20the%20decoder%20compute%0Abottleneck%20while%20maintaining%20high%20fidelity%20of%20image%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01407v2&entry.124074799=Read"},
{"title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing\n  Framework with Multimodal Narrative Understanding", "author": "Xiangfeng Wang and Xiao Li and Yadong Wei and Xueyu Song and Yang Song and Xiaoqiang Xia and Fangrui Zeng and Zaiyi Chen and Liu Liu and Gu Xu and Tong Xu", "abstract": "  The rapid growth of online video content, especially on short video\nplatforms, has created a growing demand for efficient video editing techniques\nthat can condense long-form videos into concise and engaging clips. Existing\nautomatic editing methods predominantly rely on textual cues from ASR\ntranscripts and end-to-end segment selection, often neglecting the rich visual\ncontext and leading to incoherent outputs. In this paper, we propose a\nhuman-inspired automatic video editing framework (HIVE) that leverages\nmultimodal narrative understanding to address these limitations. Our approach\nincorporates character extraction, dialogue analysis, and narrative\nsummarization through multimodal large language models, enabling a holistic\nunderstanding of the video content. To further enhance coherence, we apply\nscene-level segmentation and decompose the editing process into three subtasks:\nhighlight detection, opening/ending selection, and pruning of irrelevant\ncontent. To facilitate research in this area, we introduce DramaAD, a novel\nbenchmark dataset comprising over 800 short drama episodes and 500\nprofessionally edited advertisement clips. Experimental results demonstrate\nthat our framework consistently outperforms existing baselines across both\ngeneral and advertisement-oriented editing tasks, significantly narrowing the\nquality gap between automatic and human-edited videos.\n", "link": "http://arxiv.org/abs/2507.02790v2", "date": "2025-10-03", "relevancy": 2.2447, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5883}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Long%20Videos%20to%20Engaging%20Clips%3A%20A%20Human-Inspired%20Video%20Editing%0A%20%20Framework%20with%20Multimodal%20Narrative%20Understanding&body=Title%3A%20From%20Long%20Videos%20to%20Engaging%20Clips%3A%20A%20Human-Inspired%20Video%20Editing%0A%20%20Framework%20with%20Multimodal%20Narrative%20Understanding%0AAuthor%3A%20Xiangfeng%20Wang%20and%20Xiao%20Li%20and%20Yadong%20Wei%20and%20Xueyu%20Song%20and%20Yang%20Song%20and%20Xiaoqiang%20Xia%20and%20Fangrui%20Zeng%20and%20Zaiyi%20Chen%20and%20Liu%20Liu%20and%20Gu%20Xu%20and%20Tong%20Xu%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20online%20video%20content%2C%20especially%20on%20short%20video%0Aplatforms%2C%20has%20created%20a%20growing%20demand%20for%20efficient%20video%20editing%20techniques%0Athat%20can%20condense%20long-form%20videos%20into%20concise%20and%20engaging%20clips.%20Existing%0Aautomatic%20editing%20methods%20predominantly%20rely%20on%20textual%20cues%20from%20ASR%0Atranscripts%20and%20end-to-end%20segment%20selection%2C%20often%20neglecting%20the%20rich%20visual%0Acontext%20and%20leading%20to%20incoherent%20outputs.%20In%20this%20paper%2C%20we%20propose%20a%0Ahuman-inspired%20automatic%20video%20editing%20framework%20%28HIVE%29%20that%20leverages%0Amultimodal%20narrative%20understanding%20to%20address%20these%20limitations.%20Our%20approach%0Aincorporates%20character%20extraction%2C%20dialogue%20analysis%2C%20and%20narrative%0Asummarization%20through%20multimodal%20large%20language%20models%2C%20enabling%20a%20holistic%0Aunderstanding%20of%20the%20video%20content.%20To%20further%20enhance%20coherence%2C%20we%20apply%0Ascene-level%20segmentation%20and%20decompose%20the%20editing%20process%20into%20three%20subtasks%3A%0Ahighlight%20detection%2C%20opening/ending%20selection%2C%20and%20pruning%20of%20irrelevant%0Acontent.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20introduce%20DramaAD%2C%20a%20novel%0Abenchmark%20dataset%20comprising%20over%20800%20short%20drama%20episodes%20and%20500%0Aprofessionally%20edited%20advertisement%20clips.%20Experimental%20results%20demonstrate%0Athat%20our%20framework%20consistently%20outperforms%20existing%20baselines%20across%20both%0Ageneral%20and%20advertisement-oriented%20editing%20tasks%2C%20significantly%20narrowing%20the%0Aquality%20gap%20between%20automatic%20and%20human-edited%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02790v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Long%2520Videos%2520to%2520Engaging%2520Clips%253A%2520A%2520Human-Inspired%2520Video%2520Editing%250A%2520%2520Framework%2520with%2520Multimodal%2520Narrative%2520Understanding%26entry.906535625%3DXiangfeng%2520Wang%2520and%2520Xiao%2520Li%2520and%2520Yadong%2520Wei%2520and%2520Xueyu%2520Song%2520and%2520Yang%2520Song%2520and%2520Xiaoqiang%2520Xia%2520and%2520Fangrui%2520Zeng%2520and%2520Zaiyi%2520Chen%2520and%2520Liu%2520Liu%2520and%2520Gu%2520Xu%2520and%2520Tong%2520Xu%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520online%2520video%2520content%252C%2520especially%2520on%2520short%2520video%250Aplatforms%252C%2520has%2520created%2520a%2520growing%2520demand%2520for%2520efficient%2520video%2520editing%2520techniques%250Athat%2520can%2520condense%2520long-form%2520videos%2520into%2520concise%2520and%2520engaging%2520clips.%2520Existing%250Aautomatic%2520editing%2520methods%2520predominantly%2520rely%2520on%2520textual%2520cues%2520from%2520ASR%250Atranscripts%2520and%2520end-to-end%2520segment%2520selection%252C%2520often%2520neglecting%2520the%2520rich%2520visual%250Acontext%2520and%2520leading%2520to%2520incoherent%2520outputs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Ahuman-inspired%2520automatic%2520video%2520editing%2520framework%2520%2528HIVE%2529%2520that%2520leverages%250Amultimodal%2520narrative%2520understanding%2520to%2520address%2520these%2520limitations.%2520Our%2520approach%250Aincorporates%2520character%2520extraction%252C%2520dialogue%2520analysis%252C%2520and%2520narrative%250Asummarization%2520through%2520multimodal%2520large%2520language%2520models%252C%2520enabling%2520a%2520holistic%250Aunderstanding%2520of%2520the%2520video%2520content.%2520To%2520further%2520enhance%2520coherence%252C%2520we%2520apply%250Ascene-level%2520segmentation%2520and%2520decompose%2520the%2520editing%2520process%2520into%2520three%2520subtasks%253A%250Ahighlight%2520detection%252C%2520opening/ending%2520selection%252C%2520and%2520pruning%2520of%2520irrelevant%250Acontent.%2520To%2520facilitate%2520research%2520in%2520this%2520area%252C%2520we%2520introduce%2520DramaAD%252C%2520a%2520novel%250Abenchmark%2520dataset%2520comprising%2520over%2520800%2520short%2520drama%2520episodes%2520and%2520500%250Aprofessionally%2520edited%2520advertisement%2520clips.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520framework%2520consistently%2520outperforms%2520existing%2520baselines%2520across%2520both%250Ageneral%2520and%2520advertisement-oriented%2520editing%2520tasks%252C%2520significantly%2520narrowing%2520the%250Aquality%2520gap%2520between%2520automatic%2520and%2520human-edited%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02790v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Long%20Videos%20to%20Engaging%20Clips%3A%20A%20Human-Inspired%20Video%20Editing%0A%20%20Framework%20with%20Multimodal%20Narrative%20Understanding&entry.906535625=Xiangfeng%20Wang%20and%20Xiao%20Li%20and%20Yadong%20Wei%20and%20Xueyu%20Song%20and%20Yang%20Song%20and%20Xiaoqiang%20Xia%20and%20Fangrui%20Zeng%20and%20Zaiyi%20Chen%20and%20Liu%20Liu%20and%20Gu%20Xu%20and%20Tong%20Xu&entry.1292438233=%20%20The%20rapid%20growth%20of%20online%20video%20content%2C%20especially%20on%20short%20video%0Aplatforms%2C%20has%20created%20a%20growing%20demand%20for%20efficient%20video%20editing%20techniques%0Athat%20can%20condense%20long-form%20videos%20into%20concise%20and%20engaging%20clips.%20Existing%0Aautomatic%20editing%20methods%20predominantly%20rely%20on%20textual%20cues%20from%20ASR%0Atranscripts%20and%20end-to-end%20segment%20selection%2C%20often%20neglecting%20the%20rich%20visual%0Acontext%20and%20leading%20to%20incoherent%20outputs.%20In%20this%20paper%2C%20we%20propose%20a%0Ahuman-inspired%20automatic%20video%20editing%20framework%20%28HIVE%29%20that%20leverages%0Amultimodal%20narrative%20understanding%20to%20address%20these%20limitations.%20Our%20approach%0Aincorporates%20character%20extraction%2C%20dialogue%20analysis%2C%20and%20narrative%0Asummarization%20through%20multimodal%20large%20language%20models%2C%20enabling%20a%20holistic%0Aunderstanding%20of%20the%20video%20content.%20To%20further%20enhance%20coherence%2C%20we%20apply%0Ascene-level%20segmentation%20and%20decompose%20the%20editing%20process%20into%20three%20subtasks%3A%0Ahighlight%20detection%2C%20opening/ending%20selection%2C%20and%20pruning%20of%20irrelevant%0Acontent.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20introduce%20DramaAD%2C%20a%20novel%0Abenchmark%20dataset%20comprising%20over%20800%20short%20drama%20episodes%20and%20500%0Aprofessionally%20edited%20advertisement%20clips.%20Experimental%20results%20demonstrate%0Athat%20our%20framework%20consistently%20outperforms%20existing%20baselines%20across%20both%0Ageneral%20and%20advertisement-oriented%20editing%20tasks%2C%20significantly%20narrowing%20the%0Aquality%20gap%20between%20automatic%20and%20human-edited%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02790v2&entry.124074799=Read"},
{"title": "Neural Posterior Estimation with Autoregressive Tiling for Detecting\n  Objects in Astronomical Images", "author": "Jeffrey Regier", "abstract": "  Upcoming astronomical surveys will produce petabytes of high-resolution\nimages of the night sky, providing information about billions of stars and\ngalaxies. Detecting and characterizing the astronomical objects in these images\nis a fundamental task in astronomy -- and a challenging one, as most of these\nobjects are faint and many visually overlap with other objects. We propose an\namortized variational inference procedure to solve this instance of\nsmall-object detection. Our key innovation is a family of spatially\nautoregressive variational distributions that partition and order the latent\nspace according to a $K$-color checkerboard pattern. By construction, the\nconditional independencies of this variational family mirror those of the\nposterior distribution. We fit the variational distribution, which is\nparameterized by a convolutional neural network, using neural posterior\nestimation (NPE) to minimize an expectation of the forward KL divergence. Using\nimages from the Sloan Digital Sky Survey, our method achieves state-of-the-art\nperformance. We further demonstrate that the proposed autoregressive structure\ngreatly improves posterior calibration.\n", "link": "http://arxiv.org/abs/2510.03074v1", "date": "2025-10-03", "relevancy": 2.2409, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.574}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5524}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Posterior%20Estimation%20with%20Autoregressive%20Tiling%20for%20Detecting%0A%20%20Objects%20in%20Astronomical%20Images&body=Title%3A%20Neural%20Posterior%20Estimation%20with%20Autoregressive%20Tiling%20for%20Detecting%0A%20%20Objects%20in%20Astronomical%20Images%0AAuthor%3A%20Jeffrey%20Regier%0AAbstract%3A%20%20%20Upcoming%20astronomical%20surveys%20will%20produce%20petabytes%20of%20high-resolution%0Aimages%20of%20the%20night%20sky%2C%20providing%20information%20about%20billions%20of%20stars%20and%0Agalaxies.%20Detecting%20and%20characterizing%20the%20astronomical%20objects%20in%20these%20images%0Ais%20a%20fundamental%20task%20in%20astronomy%20--%20and%20a%20challenging%20one%2C%20as%20most%20of%20these%0Aobjects%20are%20faint%20and%20many%20visually%20overlap%20with%20other%20objects.%20We%20propose%20an%0Aamortized%20variational%20inference%20procedure%20to%20solve%20this%20instance%20of%0Asmall-object%20detection.%20Our%20key%20innovation%20is%20a%20family%20of%20spatially%0Aautoregressive%20variational%20distributions%20that%20partition%20and%20order%20the%20latent%0Aspace%20according%20to%20a%20%24K%24-color%20checkerboard%20pattern.%20By%20construction%2C%20the%0Aconditional%20independencies%20of%20this%20variational%20family%20mirror%20those%20of%20the%0Aposterior%20distribution.%20We%20fit%20the%20variational%20distribution%2C%20which%20is%0Aparameterized%20by%20a%20convolutional%20neural%20network%2C%20using%20neural%20posterior%0Aestimation%20%28NPE%29%20to%20minimize%20an%20expectation%20of%20the%20forward%20KL%20divergence.%20Using%0Aimages%20from%20the%20Sloan%20Digital%20Sky%20Survey%2C%20our%20method%20achieves%20state-of-the-art%0Aperformance.%20We%20further%20demonstrate%20that%20the%20proposed%20autoregressive%20structure%0Agreatly%20improves%20posterior%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Posterior%2520Estimation%2520with%2520Autoregressive%2520Tiling%2520for%2520Detecting%250A%2520%2520Objects%2520in%2520Astronomical%2520Images%26entry.906535625%3DJeffrey%2520Regier%26entry.1292438233%3D%2520%2520Upcoming%2520astronomical%2520surveys%2520will%2520produce%2520petabytes%2520of%2520high-resolution%250Aimages%2520of%2520the%2520night%2520sky%252C%2520providing%2520information%2520about%2520billions%2520of%2520stars%2520and%250Agalaxies.%2520Detecting%2520and%2520characterizing%2520the%2520astronomical%2520objects%2520in%2520these%2520images%250Ais%2520a%2520fundamental%2520task%2520in%2520astronomy%2520--%2520and%2520a%2520challenging%2520one%252C%2520as%2520most%2520of%2520these%250Aobjects%2520are%2520faint%2520and%2520many%2520visually%2520overlap%2520with%2520other%2520objects.%2520We%2520propose%2520an%250Aamortized%2520variational%2520inference%2520procedure%2520to%2520solve%2520this%2520instance%2520of%250Asmall-object%2520detection.%2520Our%2520key%2520innovation%2520is%2520a%2520family%2520of%2520spatially%250Aautoregressive%2520variational%2520distributions%2520that%2520partition%2520and%2520order%2520the%2520latent%250Aspace%2520according%2520to%2520a%2520%2524K%2524-color%2520checkerboard%2520pattern.%2520By%2520construction%252C%2520the%250Aconditional%2520independencies%2520of%2520this%2520variational%2520family%2520mirror%2520those%2520of%2520the%250Aposterior%2520distribution.%2520We%2520fit%2520the%2520variational%2520distribution%252C%2520which%2520is%250Aparameterized%2520by%2520a%2520convolutional%2520neural%2520network%252C%2520using%2520neural%2520posterior%250Aestimation%2520%2528NPE%2529%2520to%2520minimize%2520an%2520expectation%2520of%2520the%2520forward%2520KL%2520divergence.%2520Using%250Aimages%2520from%2520the%2520Sloan%2520Digital%2520Sky%2520Survey%252C%2520our%2520method%2520achieves%2520state-of-the-art%250Aperformance.%2520We%2520further%2520demonstrate%2520that%2520the%2520proposed%2520autoregressive%2520structure%250Agreatly%2520improves%2520posterior%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Posterior%20Estimation%20with%20Autoregressive%20Tiling%20for%20Detecting%0A%20%20Objects%20in%20Astronomical%20Images&entry.906535625=Jeffrey%20Regier&entry.1292438233=%20%20Upcoming%20astronomical%20surveys%20will%20produce%20petabytes%20of%20high-resolution%0Aimages%20of%20the%20night%20sky%2C%20providing%20information%20about%20billions%20of%20stars%20and%0Agalaxies.%20Detecting%20and%20characterizing%20the%20astronomical%20objects%20in%20these%20images%0Ais%20a%20fundamental%20task%20in%20astronomy%20--%20and%20a%20challenging%20one%2C%20as%20most%20of%20these%0Aobjects%20are%20faint%20and%20many%20visually%20overlap%20with%20other%20objects.%20We%20propose%20an%0Aamortized%20variational%20inference%20procedure%20to%20solve%20this%20instance%20of%0Asmall-object%20detection.%20Our%20key%20innovation%20is%20a%20family%20of%20spatially%0Aautoregressive%20variational%20distributions%20that%20partition%20and%20order%20the%20latent%0Aspace%20according%20to%20a%20%24K%24-color%20checkerboard%20pattern.%20By%20construction%2C%20the%0Aconditional%20independencies%20of%20this%20variational%20family%20mirror%20those%20of%20the%0Aposterior%20distribution.%20We%20fit%20the%20variational%20distribution%2C%20which%20is%0Aparameterized%20by%20a%20convolutional%20neural%20network%2C%20using%20neural%20posterior%0Aestimation%20%28NPE%29%20to%20minimize%20an%20expectation%20of%20the%20forward%20KL%20divergence.%20Using%0Aimages%20from%20the%20Sloan%20Digital%20Sky%20Survey%2C%20our%20method%20achieves%20state-of-the-art%0Aperformance.%20We%20further%20demonstrate%20that%20the%20proposed%20autoregressive%20structure%0Agreatly%20improves%20posterior%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03074v1&entry.124074799=Read"},
{"title": "So-Fake: Benchmarking and Explaining Social Media Image Forgery\n  Detection", "author": "Zhenglin Huang and Tianxiao Li and Xiangtai Li and Haiquan Wen and Yiwei He and Jiangning Zhang and Hao Fei and Xi Yang and Xiaowei Huang and Bei Peng and Guangliang Cheng", "abstract": "  Recent advances in AI-powered generative models have enabled the creation of\nincreasingly realistic synthetic images, posing significant risks to\ninformation integrity and public trust on social media platforms. While robust\ndetection frameworks and diverse, large-scale datasets are essential to\nmitigate these risks, existing academic efforts remain limited in scope:\ncurrent datasets lack the diversity, scale, and realism required for social\nmedia contexts, while detection methods struggle with generalization to unseen\ngenerative technologies. To bridge this gap, we introduce So-Fake-Set, a\ncomprehensive social media-oriented dataset with over 2 million high-quality\nimages, diverse generative sources, and photorealistic imagery synthesized\nusing 35 state-of-the-art generative models. To rigorously evaluate\ncross-domain robustness, we establish a novel and large-scale (100K)\nout-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from\ncommercial models explicitly excluded from the training distribution, creating\na realistic testbed for evaluating real-world performance. Leveraging these\nresources, we present So-Fake-R1, an advanced vision-language framework that\nemploys reinforcement learning for highly accurate forgery detection, precise\nlocalization, and explainable inference through interpretable visual\nrationales. Extensive experiments show that So-Fake-R1 outperforms the\nsecond-best method, with a 1.3% gain in detection accuracy and a 4.5% increase\nin localization IoU. By integrating a scalable dataset, a challenging OOD\nbenchmark, and an advanced detection framework, this work establishes a new\nfoundation for social media-centric forgery detection research. The code,\nmodels, and datasets will be released publicly.\n", "link": "http://arxiv.org/abs/2505.18660v4", "date": "2025-10-03", "relevancy": 2.2243, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5618}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.56}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20So-Fake%3A%20Benchmarking%20and%20Explaining%20Social%20Media%20Image%20Forgery%0A%20%20Detection&body=Title%3A%20So-Fake%3A%20Benchmarking%20and%20Explaining%20Social%20Media%20Image%20Forgery%0A%20%20Detection%0AAuthor%3A%20Zhenglin%20Huang%20and%20Tianxiao%20Li%20and%20Xiangtai%20Li%20and%20Haiquan%20Wen%20and%20Yiwei%20He%20and%20Jiangning%20Zhang%20and%20Hao%20Fei%20and%20Xi%20Yang%20and%20Xiaowei%20Huang%20and%20Bei%20Peng%20and%20Guangliang%20Cheng%0AAbstract%3A%20%20%20Recent%20advances%20in%20AI-powered%20generative%20models%20have%20enabled%20the%20creation%20of%0Aincreasingly%20realistic%20synthetic%20images%2C%20posing%20significant%20risks%20to%0Ainformation%20integrity%20and%20public%20trust%20on%20social%20media%20platforms.%20While%20robust%0Adetection%20frameworks%20and%20diverse%2C%20large-scale%20datasets%20are%20essential%20to%0Amitigate%20these%20risks%2C%20existing%20academic%20efforts%20remain%20limited%20in%20scope%3A%0Acurrent%20datasets%20lack%20the%20diversity%2C%20scale%2C%20and%20realism%20required%20for%20social%0Amedia%20contexts%2C%20while%20detection%20methods%20struggle%20with%20generalization%20to%20unseen%0Agenerative%20technologies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20So-Fake-Set%2C%20a%0Acomprehensive%20social%20media-oriented%20dataset%20with%20over%202%20million%20high-quality%0Aimages%2C%20diverse%20generative%20sources%2C%20and%20photorealistic%20imagery%20synthesized%0Ausing%2035%20state-of-the-art%20generative%20models.%20To%20rigorously%20evaluate%0Across-domain%20robustness%2C%20we%20establish%20a%20novel%20and%20large-scale%20%28100K%29%0Aout-of-domain%20benchmark%20%28So-Fake-OOD%29%20featuring%20synthetic%20imagery%20from%0Acommercial%20models%20explicitly%20excluded%20from%20the%20training%20distribution%2C%20creating%0Aa%20realistic%20testbed%20for%20evaluating%20real-world%20performance.%20Leveraging%20these%0Aresources%2C%20we%20present%20So-Fake-R1%2C%20an%20advanced%20vision-language%20framework%20that%0Aemploys%20reinforcement%20learning%20for%20highly%20accurate%20forgery%20detection%2C%20precise%0Alocalization%2C%20and%20explainable%20inference%20through%20interpretable%20visual%0Arationales.%20Extensive%20experiments%20show%20that%20So-Fake-R1%20outperforms%20the%0Asecond-best%20method%2C%20with%20a%201.3%25%20gain%20in%20detection%20accuracy%20and%20a%204.5%25%20increase%0Ain%20localization%20IoU.%20By%20integrating%20a%20scalable%20dataset%2C%20a%20challenging%20OOD%0Abenchmark%2C%20and%20an%20advanced%20detection%20framework%2C%20this%20work%20establishes%20a%20new%0Afoundation%20for%20social%20media-centric%20forgery%20detection%20research.%20The%20code%2C%0Amodels%2C%20and%20datasets%20will%20be%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18660v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSo-Fake%253A%2520Benchmarking%2520and%2520Explaining%2520Social%2520Media%2520Image%2520Forgery%250A%2520%2520Detection%26entry.906535625%3DZhenglin%2520Huang%2520and%2520Tianxiao%2520Li%2520and%2520Xiangtai%2520Li%2520and%2520Haiquan%2520Wen%2520and%2520Yiwei%2520He%2520and%2520Jiangning%2520Zhang%2520and%2520Hao%2520Fei%2520and%2520Xi%2520Yang%2520and%2520Xiaowei%2520Huang%2520and%2520Bei%2520Peng%2520and%2520Guangliang%2520Cheng%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520AI-powered%2520generative%2520models%2520have%2520enabled%2520the%2520creation%2520of%250Aincreasingly%2520realistic%2520synthetic%2520images%252C%2520posing%2520significant%2520risks%2520to%250Ainformation%2520integrity%2520and%2520public%2520trust%2520on%2520social%2520media%2520platforms.%2520While%2520robust%250Adetection%2520frameworks%2520and%2520diverse%252C%2520large-scale%2520datasets%2520are%2520essential%2520to%250Amitigate%2520these%2520risks%252C%2520existing%2520academic%2520efforts%2520remain%2520limited%2520in%2520scope%253A%250Acurrent%2520datasets%2520lack%2520the%2520diversity%252C%2520scale%252C%2520and%2520realism%2520required%2520for%2520social%250Amedia%2520contexts%252C%2520while%2520detection%2520methods%2520struggle%2520with%2520generalization%2520to%2520unseen%250Agenerative%2520technologies.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520So-Fake-Set%252C%2520a%250Acomprehensive%2520social%2520media-oriented%2520dataset%2520with%2520over%25202%2520million%2520high-quality%250Aimages%252C%2520diverse%2520generative%2520sources%252C%2520and%2520photorealistic%2520imagery%2520synthesized%250Ausing%252035%2520state-of-the-art%2520generative%2520models.%2520To%2520rigorously%2520evaluate%250Across-domain%2520robustness%252C%2520we%2520establish%2520a%2520novel%2520and%2520large-scale%2520%2528100K%2529%250Aout-of-domain%2520benchmark%2520%2528So-Fake-OOD%2529%2520featuring%2520synthetic%2520imagery%2520from%250Acommercial%2520models%2520explicitly%2520excluded%2520from%2520the%2520training%2520distribution%252C%2520creating%250Aa%2520realistic%2520testbed%2520for%2520evaluating%2520real-world%2520performance.%2520Leveraging%2520these%250Aresources%252C%2520we%2520present%2520So-Fake-R1%252C%2520an%2520advanced%2520vision-language%2520framework%2520that%250Aemploys%2520reinforcement%2520learning%2520for%2520highly%2520accurate%2520forgery%2520detection%252C%2520precise%250Alocalization%252C%2520and%2520explainable%2520inference%2520through%2520interpretable%2520visual%250Arationales.%2520Extensive%2520experiments%2520show%2520that%2520So-Fake-R1%2520outperforms%2520the%250Asecond-best%2520method%252C%2520with%2520a%25201.3%2525%2520gain%2520in%2520detection%2520accuracy%2520and%2520a%25204.5%2525%2520increase%250Ain%2520localization%2520IoU.%2520By%2520integrating%2520a%2520scalable%2520dataset%252C%2520a%2520challenging%2520OOD%250Abenchmark%252C%2520and%2520an%2520advanced%2520detection%2520framework%252C%2520this%2520work%2520establishes%2520a%2520new%250Afoundation%2520for%2520social%2520media-centric%2520forgery%2520detection%2520research.%2520The%2520code%252C%250Amodels%252C%2520and%2520datasets%2520will%2520be%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18660v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=So-Fake%3A%20Benchmarking%20and%20Explaining%20Social%20Media%20Image%20Forgery%0A%20%20Detection&entry.906535625=Zhenglin%20Huang%20and%20Tianxiao%20Li%20and%20Xiangtai%20Li%20and%20Haiquan%20Wen%20and%20Yiwei%20He%20and%20Jiangning%20Zhang%20and%20Hao%20Fei%20and%20Xi%20Yang%20and%20Xiaowei%20Huang%20and%20Bei%20Peng%20and%20Guangliang%20Cheng&entry.1292438233=%20%20Recent%20advances%20in%20AI-powered%20generative%20models%20have%20enabled%20the%20creation%20of%0Aincreasingly%20realistic%20synthetic%20images%2C%20posing%20significant%20risks%20to%0Ainformation%20integrity%20and%20public%20trust%20on%20social%20media%20platforms.%20While%20robust%0Adetection%20frameworks%20and%20diverse%2C%20large-scale%20datasets%20are%20essential%20to%0Amitigate%20these%20risks%2C%20existing%20academic%20efforts%20remain%20limited%20in%20scope%3A%0Acurrent%20datasets%20lack%20the%20diversity%2C%20scale%2C%20and%20realism%20required%20for%20social%0Amedia%20contexts%2C%20while%20detection%20methods%20struggle%20with%20generalization%20to%20unseen%0Agenerative%20technologies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20So-Fake-Set%2C%20a%0Acomprehensive%20social%20media-oriented%20dataset%20with%20over%202%20million%20high-quality%0Aimages%2C%20diverse%20generative%20sources%2C%20and%20photorealistic%20imagery%20synthesized%0Ausing%2035%20state-of-the-art%20generative%20models.%20To%20rigorously%20evaluate%0Across-domain%20robustness%2C%20we%20establish%20a%20novel%20and%20large-scale%20%28100K%29%0Aout-of-domain%20benchmark%20%28So-Fake-OOD%29%20featuring%20synthetic%20imagery%20from%0Acommercial%20models%20explicitly%20excluded%20from%20the%20training%20distribution%2C%20creating%0Aa%20realistic%20testbed%20for%20evaluating%20real-world%20performance.%20Leveraging%20these%0Aresources%2C%20we%20present%20So-Fake-R1%2C%20an%20advanced%20vision-language%20framework%20that%0Aemploys%20reinforcement%20learning%20for%20highly%20accurate%20forgery%20detection%2C%20precise%0Alocalization%2C%20and%20explainable%20inference%20through%20interpretable%20visual%0Arationales.%20Extensive%20experiments%20show%20that%20So-Fake-R1%20outperforms%20the%0Asecond-best%20method%2C%20with%20a%201.3%25%20gain%20in%20detection%20accuracy%20and%20a%204.5%25%20increase%0Ain%20localization%20IoU.%20By%20integrating%20a%20scalable%20dataset%2C%20a%20challenging%20OOD%0Abenchmark%2C%20and%20an%20advanced%20detection%20framework%2C%20this%20work%20establishes%20a%20new%0Afoundation%20for%20social%20media-centric%20forgery%20detection%20research.%20The%20code%2C%0Amodels%2C%20and%20datasets%20will%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18660v4&entry.124074799=Read"},
{"title": "AdaBet: Gradient-free Layer Selection for Efficient Training of Deep\n  Neural Networks", "author": "Irene Tenison and Soumyajit Chatterjee and Fahim Kawsar and Mohammad Malekzadeh", "abstract": "  To utilize pre-trained neural networks on edge and mobile devices, we often\nrequire efficient adaptation to user-specific runtime data distributions while\noperating under limited compute and memory resources. On-device retraining with\na target dataset can facilitate such adaptations; however, it remains\nimpractical due to the increasing depth of modern neural nets, as well as the\ncomputational overhead associated with gradient-based optimization across all\nlayers. Current approaches reduce training cost by selecting a subset of layers\nfor retraining, however, they rely on labeled data, at least one full-model\nbackpropagation, or server-side meta-training; limiting their suitability for\nconstrained devices. We introduce AdaBet, a gradient-free layer selection\napproach to rank important layers by analyzing topological features of their\nactivation spaces through Betti Numbers and using forward passes alone. AdaBet\nallows selecting layers with high learning capacity, which are important for\nretraining and adaptation, without requiring labels or gradients. Evaluating\nAdaBet on sixteen pairs of benchmark models and datasets, shows AdaBet achieves\nan average gain of 5% more classification accuracy over gradient-based\nbaselines while reducing average peak memory consumption by 40%.\n", "link": "http://arxiv.org/abs/2510.03101v1", "date": "2025-10-03", "relevancy": 2.2199, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6005}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5278}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaBet%3A%20Gradient-free%20Layer%20Selection%20for%20Efficient%20Training%20of%20Deep%0A%20%20Neural%20Networks&body=Title%3A%20AdaBet%3A%20Gradient-free%20Layer%20Selection%20for%20Efficient%20Training%20of%20Deep%0A%20%20Neural%20Networks%0AAuthor%3A%20Irene%20Tenison%20and%20Soumyajit%20Chatterjee%20and%20Fahim%20Kawsar%20and%20Mohammad%20Malekzadeh%0AAbstract%3A%20%20%20To%20utilize%20pre-trained%20neural%20networks%20on%20edge%20and%20mobile%20devices%2C%20we%20often%0Arequire%20efficient%20adaptation%20to%20user-specific%20runtime%20data%20distributions%20while%0Aoperating%20under%20limited%20compute%20and%20memory%20resources.%20On-device%20retraining%20with%0Aa%20target%20dataset%20can%20facilitate%20such%20adaptations%3B%20however%2C%20it%20remains%0Aimpractical%20due%20to%20the%20increasing%20depth%20of%20modern%20neural%20nets%2C%20as%20well%20as%20the%0Acomputational%20overhead%20associated%20with%20gradient-based%20optimization%20across%20all%0Alayers.%20Current%20approaches%20reduce%20training%20cost%20by%20selecting%20a%20subset%20of%20layers%0Afor%20retraining%2C%20however%2C%20they%20rely%20on%20labeled%20data%2C%20at%20least%20one%20full-model%0Abackpropagation%2C%20or%20server-side%20meta-training%3B%20limiting%20their%20suitability%20for%0Aconstrained%20devices.%20We%20introduce%20AdaBet%2C%20a%20gradient-free%20layer%20selection%0Aapproach%20to%20rank%20important%20layers%20by%20analyzing%20topological%20features%20of%20their%0Aactivation%20spaces%20through%20Betti%20Numbers%20and%20using%20forward%20passes%20alone.%20AdaBet%0Aallows%20selecting%20layers%20with%20high%20learning%20capacity%2C%20which%20are%20important%20for%0Aretraining%20and%20adaptation%2C%20without%20requiring%20labels%20or%20gradients.%20Evaluating%0AAdaBet%20on%20sixteen%20pairs%20of%20benchmark%20models%20and%20datasets%2C%20shows%20AdaBet%20achieves%0Aan%20average%20gain%20of%205%25%20more%20classification%20accuracy%20over%20gradient-based%0Abaselines%20while%20reducing%20average%20peak%20memory%20consumption%20by%2040%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaBet%253A%2520Gradient-free%2520Layer%2520Selection%2520for%2520Efficient%2520Training%2520of%2520Deep%250A%2520%2520Neural%2520Networks%26entry.906535625%3DIrene%2520Tenison%2520and%2520Soumyajit%2520Chatterjee%2520and%2520Fahim%2520Kawsar%2520and%2520Mohammad%2520Malekzadeh%26entry.1292438233%3D%2520%2520To%2520utilize%2520pre-trained%2520neural%2520networks%2520on%2520edge%2520and%2520mobile%2520devices%252C%2520we%2520often%250Arequire%2520efficient%2520adaptation%2520to%2520user-specific%2520runtime%2520data%2520distributions%2520while%250Aoperating%2520under%2520limited%2520compute%2520and%2520memory%2520resources.%2520On-device%2520retraining%2520with%250Aa%2520target%2520dataset%2520can%2520facilitate%2520such%2520adaptations%253B%2520however%252C%2520it%2520remains%250Aimpractical%2520due%2520to%2520the%2520increasing%2520depth%2520of%2520modern%2520neural%2520nets%252C%2520as%2520well%2520as%2520the%250Acomputational%2520overhead%2520associated%2520with%2520gradient-based%2520optimization%2520across%2520all%250Alayers.%2520Current%2520approaches%2520reduce%2520training%2520cost%2520by%2520selecting%2520a%2520subset%2520of%2520layers%250Afor%2520retraining%252C%2520however%252C%2520they%2520rely%2520on%2520labeled%2520data%252C%2520at%2520least%2520one%2520full-model%250Abackpropagation%252C%2520or%2520server-side%2520meta-training%253B%2520limiting%2520their%2520suitability%2520for%250Aconstrained%2520devices.%2520We%2520introduce%2520AdaBet%252C%2520a%2520gradient-free%2520layer%2520selection%250Aapproach%2520to%2520rank%2520important%2520layers%2520by%2520analyzing%2520topological%2520features%2520of%2520their%250Aactivation%2520spaces%2520through%2520Betti%2520Numbers%2520and%2520using%2520forward%2520passes%2520alone.%2520AdaBet%250Aallows%2520selecting%2520layers%2520with%2520high%2520learning%2520capacity%252C%2520which%2520are%2520important%2520for%250Aretraining%2520and%2520adaptation%252C%2520without%2520requiring%2520labels%2520or%2520gradients.%2520Evaluating%250AAdaBet%2520on%2520sixteen%2520pairs%2520of%2520benchmark%2520models%2520and%2520datasets%252C%2520shows%2520AdaBet%2520achieves%250Aan%2520average%2520gain%2520of%25205%2525%2520more%2520classification%2520accuracy%2520over%2520gradient-based%250Abaselines%2520while%2520reducing%2520average%2520peak%2520memory%2520consumption%2520by%252040%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaBet%3A%20Gradient-free%20Layer%20Selection%20for%20Efficient%20Training%20of%20Deep%0A%20%20Neural%20Networks&entry.906535625=Irene%20Tenison%20and%20Soumyajit%20Chatterjee%20and%20Fahim%20Kawsar%20and%20Mohammad%20Malekzadeh&entry.1292438233=%20%20To%20utilize%20pre-trained%20neural%20networks%20on%20edge%20and%20mobile%20devices%2C%20we%20often%0Arequire%20efficient%20adaptation%20to%20user-specific%20runtime%20data%20distributions%20while%0Aoperating%20under%20limited%20compute%20and%20memory%20resources.%20On-device%20retraining%20with%0Aa%20target%20dataset%20can%20facilitate%20such%20adaptations%3B%20however%2C%20it%20remains%0Aimpractical%20due%20to%20the%20increasing%20depth%20of%20modern%20neural%20nets%2C%20as%20well%20as%20the%0Acomputational%20overhead%20associated%20with%20gradient-based%20optimization%20across%20all%0Alayers.%20Current%20approaches%20reduce%20training%20cost%20by%20selecting%20a%20subset%20of%20layers%0Afor%20retraining%2C%20however%2C%20they%20rely%20on%20labeled%20data%2C%20at%20least%20one%20full-model%0Abackpropagation%2C%20or%20server-side%20meta-training%3B%20limiting%20their%20suitability%20for%0Aconstrained%20devices.%20We%20introduce%20AdaBet%2C%20a%20gradient-free%20layer%20selection%0Aapproach%20to%20rank%20important%20layers%20by%20analyzing%20topological%20features%20of%20their%0Aactivation%20spaces%20through%20Betti%20Numbers%20and%20using%20forward%20passes%20alone.%20AdaBet%0Aallows%20selecting%20layers%20with%20high%20learning%20capacity%2C%20which%20are%20important%20for%0Aretraining%20and%20adaptation%2C%20without%20requiring%20labels%20or%20gradients.%20Evaluating%0AAdaBet%20on%20sixteen%20pairs%20of%20benchmark%20models%20and%20datasets%2C%20shows%20AdaBet%20achieves%0Aan%20average%20gain%20of%205%25%20more%20classification%20accuracy%20over%20gradient-based%0Abaselines%20while%20reducing%20average%20peak%20memory%20consumption%20by%2040%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03101v1&entry.124074799=Read"},
{"title": "Adaptive Node Feature Selection For Graph Neural Networks", "author": "Ali Azizpour and Madeline Navarro and Santiago Segarra", "abstract": "  We propose an adaptive node feature selection approach for graph neural\nnetworks (GNNs) that identifies and removes unnecessary features during\ntraining. The ability to measure how features contribute to model output is key\nfor interpreting decisions, reducing dimensionality, and even improving\nperformance by eliminating unhelpful variables. However, graph-structured data\nintroduces complex dependencies that may not be amenable to classical feature\nimportance metrics. Inspired by this challenge, we present a model- and\ntask-agnostic method that determines relevant features during training based on\nchanges in validation performance upon permuting feature values. We\ntheoretically motivate our intervention-based approach by characterizing how\nGNN performance depends on the relationships between node data and graph\nstructure. Not only do we return feature importance scores once training\nconcludes, we also track how relevance evolves as features are successively\ndropped. We can therefore monitor if features are eliminated effectively and\nalso evaluate other metrics with this technique. Our empirical results verify\nthe flexibility of our approach to different graph architectures as well as its\nadaptability to more challenging graph learning settings.\n", "link": "http://arxiv.org/abs/2510.03096v1", "date": "2025-10-03", "relevancy": 2.2125, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4784}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4291}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.42}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Node%20Feature%20Selection%20For%20Graph%20Neural%20Networks&body=Title%3A%20Adaptive%20Node%20Feature%20Selection%20For%20Graph%20Neural%20Networks%0AAuthor%3A%20Ali%20Azizpour%20and%20Madeline%20Navarro%20and%20Santiago%20Segarra%0AAbstract%3A%20%20%20We%20propose%20an%20adaptive%20node%20feature%20selection%20approach%20for%20graph%20neural%0Anetworks%20%28GNNs%29%20that%20identifies%20and%20removes%20unnecessary%20features%20during%0Atraining.%20The%20ability%20to%20measure%20how%20features%20contribute%20to%20model%20output%20is%20key%0Afor%20interpreting%20decisions%2C%20reducing%20dimensionality%2C%20and%20even%20improving%0Aperformance%20by%20eliminating%20unhelpful%20variables.%20However%2C%20graph-structured%20data%0Aintroduces%20complex%20dependencies%20that%20may%20not%20be%20amenable%20to%20classical%20feature%0Aimportance%20metrics.%20Inspired%20by%20this%20challenge%2C%20we%20present%20a%20model-%20and%0Atask-agnostic%20method%20that%20determines%20relevant%20features%20during%20training%20based%20on%0Achanges%20in%20validation%20performance%20upon%20permuting%20feature%20values.%20We%0Atheoretically%20motivate%20our%20intervention-based%20approach%20by%20characterizing%20how%0AGNN%20performance%20depends%20on%20the%20relationships%20between%20node%20data%20and%20graph%0Astructure.%20Not%20only%20do%20we%20return%20feature%20importance%20scores%20once%20training%0Aconcludes%2C%20we%20also%20track%20how%20relevance%20evolves%20as%20features%20are%20successively%0Adropped.%20We%20can%20therefore%20monitor%20if%20features%20are%20eliminated%20effectively%20and%0Aalso%20evaluate%20other%20metrics%20with%20this%20technique.%20Our%20empirical%20results%20verify%0Athe%20flexibility%20of%20our%20approach%20to%20different%20graph%20architectures%20as%20well%20as%20its%0Aadaptability%20to%20more%20challenging%20graph%20learning%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Node%2520Feature%2520Selection%2520For%2520Graph%2520Neural%2520Networks%26entry.906535625%3DAli%2520Azizpour%2520and%2520Madeline%2520Navarro%2520and%2520Santiago%2520Segarra%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520adaptive%2520node%2520feature%2520selection%2520approach%2520for%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%2520that%2520identifies%2520and%2520removes%2520unnecessary%2520features%2520during%250Atraining.%2520The%2520ability%2520to%2520measure%2520how%2520features%2520contribute%2520to%2520model%2520output%2520is%2520key%250Afor%2520interpreting%2520decisions%252C%2520reducing%2520dimensionality%252C%2520and%2520even%2520improving%250Aperformance%2520by%2520eliminating%2520unhelpful%2520variables.%2520However%252C%2520graph-structured%2520data%250Aintroduces%2520complex%2520dependencies%2520that%2520may%2520not%2520be%2520amenable%2520to%2520classical%2520feature%250Aimportance%2520metrics.%2520Inspired%2520by%2520this%2520challenge%252C%2520we%2520present%2520a%2520model-%2520and%250Atask-agnostic%2520method%2520that%2520determines%2520relevant%2520features%2520during%2520training%2520based%2520on%250Achanges%2520in%2520validation%2520performance%2520upon%2520permuting%2520feature%2520values.%2520We%250Atheoretically%2520motivate%2520our%2520intervention-based%2520approach%2520by%2520characterizing%2520how%250AGNN%2520performance%2520depends%2520on%2520the%2520relationships%2520between%2520node%2520data%2520and%2520graph%250Astructure.%2520Not%2520only%2520do%2520we%2520return%2520feature%2520importance%2520scores%2520once%2520training%250Aconcludes%252C%2520we%2520also%2520track%2520how%2520relevance%2520evolves%2520as%2520features%2520are%2520successively%250Adropped.%2520We%2520can%2520therefore%2520monitor%2520if%2520features%2520are%2520eliminated%2520effectively%2520and%250Aalso%2520evaluate%2520other%2520metrics%2520with%2520this%2520technique.%2520Our%2520empirical%2520results%2520verify%250Athe%2520flexibility%2520of%2520our%2520approach%2520to%2520different%2520graph%2520architectures%2520as%2520well%2520as%2520its%250Aadaptability%2520to%2520more%2520challenging%2520graph%2520learning%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Node%20Feature%20Selection%20For%20Graph%20Neural%20Networks&entry.906535625=Ali%20Azizpour%20and%20Madeline%20Navarro%20and%20Santiago%20Segarra&entry.1292438233=%20%20We%20propose%20an%20adaptive%20node%20feature%20selection%20approach%20for%20graph%20neural%0Anetworks%20%28GNNs%29%20that%20identifies%20and%20removes%20unnecessary%20features%20during%0Atraining.%20The%20ability%20to%20measure%20how%20features%20contribute%20to%20model%20output%20is%20key%0Afor%20interpreting%20decisions%2C%20reducing%20dimensionality%2C%20and%20even%20improving%0Aperformance%20by%20eliminating%20unhelpful%20variables.%20However%2C%20graph-structured%20data%0Aintroduces%20complex%20dependencies%20that%20may%20not%20be%20amenable%20to%20classical%20feature%0Aimportance%20metrics.%20Inspired%20by%20this%20challenge%2C%20we%20present%20a%20model-%20and%0Atask-agnostic%20method%20that%20determines%20relevant%20features%20during%20training%20based%20on%0Achanges%20in%20validation%20performance%20upon%20permuting%20feature%20values.%20We%0Atheoretically%20motivate%20our%20intervention-based%20approach%20by%20characterizing%20how%0AGNN%20performance%20depends%20on%20the%20relationships%20between%20node%20data%20and%20graph%0Astructure.%20Not%20only%20do%20we%20return%20feature%20importance%20scores%20once%20training%0Aconcludes%2C%20we%20also%20track%20how%20relevance%20evolves%20as%20features%20are%20successively%0Adropped.%20We%20can%20therefore%20monitor%20if%20features%20are%20eliminated%20effectively%20and%0Aalso%20evaluate%20other%20metrics%20with%20this%20technique.%20Our%20empirical%20results%20verify%0Athe%20flexibility%20of%20our%20approach%20to%20different%20graph%20architectures%20as%20well%20as%20its%0Aadaptability%20to%20more%20challenging%20graph%20learning%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03096v1&entry.124074799=Read"},
{"title": "SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos", "author": "Amir Dellali and Luca A. Lanzend\u00f6rfer and Florian Gr\u00f6tschla and Roger Wattenhofer", "abstract": "  We propose SALSA-V, a multimodal video-to-audio generation model capable of\nsynthesizing highly synchronized, high-fidelity long-form audio from silent\nvideo content. Our approach introduces a masked diffusion objective, enabling\naudio-conditioned generation and the seamless synthesis of audio sequences of\nunconstrained length. Additionally, by integrating a shortcut loss into our\ntraining process, we achieve rapid generation of high-quality audio samples in\nas few as eight sampling steps, paving the way for near-real-time applications\nwithout requiring dedicated fine-tuning or retraining. We demonstrate that\nSALSA-V significantly outperforms existing state-of-the-art methods in both\naudiovisual alignment and synchronization with video content in quantitative\nevaluation and a human listening study. Furthermore, our use of random masking\nduring training enables our model to match spectral characteristics of\nreference audio samples, broadening its applicability to professional audio\nsynthesis tasks such as Foley generation and sound design.\n", "link": "http://arxiv.org/abs/2510.02916v1", "date": "2025-10-03", "relevancy": 2.2064, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5693}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5516}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SALSA-V%3A%20Shortcut-Augmented%20Long-form%20Synchronized%20Audio%20from%20Videos&body=Title%3A%20SALSA-V%3A%20Shortcut-Augmented%20Long-form%20Synchronized%20Audio%20from%20Videos%0AAuthor%3A%20Amir%20Dellali%20and%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Florian%20Gr%C3%B6tschla%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20We%20propose%20SALSA-V%2C%20a%20multimodal%20video-to-audio%20generation%20model%20capable%20of%0Asynthesizing%20highly%20synchronized%2C%20high-fidelity%20long-form%20audio%20from%20silent%0Avideo%20content.%20Our%20approach%20introduces%20a%20masked%20diffusion%20objective%2C%20enabling%0Aaudio-conditioned%20generation%20and%20the%20seamless%20synthesis%20of%20audio%20sequences%20of%0Aunconstrained%20length.%20Additionally%2C%20by%20integrating%20a%20shortcut%20loss%20into%20our%0Atraining%20process%2C%20we%20achieve%20rapid%20generation%20of%20high-quality%20audio%20samples%20in%0Aas%20few%20as%20eight%20sampling%20steps%2C%20paving%20the%20way%20for%20near-real-time%20applications%0Awithout%20requiring%20dedicated%20fine-tuning%20or%20retraining.%20We%20demonstrate%20that%0ASALSA-V%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20both%0Aaudiovisual%20alignment%20and%20synchronization%20with%20video%20content%20in%20quantitative%0Aevaluation%20and%20a%20human%20listening%20study.%20Furthermore%2C%20our%20use%20of%20random%20masking%0Aduring%20training%20enables%20our%20model%20to%20match%20spectral%20characteristics%20of%0Areference%20audio%20samples%2C%20broadening%20its%20applicability%20to%20professional%20audio%0Asynthesis%20tasks%20such%20as%20Foley%20generation%20and%20sound%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSALSA-V%253A%2520Shortcut-Augmented%2520Long-form%2520Synchronized%2520Audio%2520from%2520Videos%26entry.906535625%3DAmir%2520Dellali%2520and%2520Luca%2520A.%2520Lanzend%25C3%25B6rfer%2520and%2520Florian%2520Gr%25C3%25B6tschla%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520We%2520propose%2520SALSA-V%252C%2520a%2520multimodal%2520video-to-audio%2520generation%2520model%2520capable%2520of%250Asynthesizing%2520highly%2520synchronized%252C%2520high-fidelity%2520long-form%2520audio%2520from%2520silent%250Avideo%2520content.%2520Our%2520approach%2520introduces%2520a%2520masked%2520diffusion%2520objective%252C%2520enabling%250Aaudio-conditioned%2520generation%2520and%2520the%2520seamless%2520synthesis%2520of%2520audio%2520sequences%2520of%250Aunconstrained%2520length.%2520Additionally%252C%2520by%2520integrating%2520a%2520shortcut%2520loss%2520into%2520our%250Atraining%2520process%252C%2520we%2520achieve%2520rapid%2520generation%2520of%2520high-quality%2520audio%2520samples%2520in%250Aas%2520few%2520as%2520eight%2520sampling%2520steps%252C%2520paving%2520the%2520way%2520for%2520near-real-time%2520applications%250Awithout%2520requiring%2520dedicated%2520fine-tuning%2520or%2520retraining.%2520We%2520demonstrate%2520that%250ASALSA-V%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520both%250Aaudiovisual%2520alignment%2520and%2520synchronization%2520with%2520video%2520content%2520in%2520quantitative%250Aevaluation%2520and%2520a%2520human%2520listening%2520study.%2520Furthermore%252C%2520our%2520use%2520of%2520random%2520masking%250Aduring%2520training%2520enables%2520our%2520model%2520to%2520match%2520spectral%2520characteristics%2520of%250Areference%2520audio%2520samples%252C%2520broadening%2520its%2520applicability%2520to%2520professional%2520audio%250Asynthesis%2520tasks%2520such%2520as%2520Foley%2520generation%2520and%2520sound%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SALSA-V%3A%20Shortcut-Augmented%20Long-form%20Synchronized%20Audio%20from%20Videos&entry.906535625=Amir%20Dellali%20and%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Florian%20Gr%C3%B6tschla%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20We%20propose%20SALSA-V%2C%20a%20multimodal%20video-to-audio%20generation%20model%20capable%20of%0Asynthesizing%20highly%20synchronized%2C%20high-fidelity%20long-form%20audio%20from%20silent%0Avideo%20content.%20Our%20approach%20introduces%20a%20masked%20diffusion%20objective%2C%20enabling%0Aaudio-conditioned%20generation%20and%20the%20seamless%20synthesis%20of%20audio%20sequences%20of%0Aunconstrained%20length.%20Additionally%2C%20by%20integrating%20a%20shortcut%20loss%20into%20our%0Atraining%20process%2C%20we%20achieve%20rapid%20generation%20of%20high-quality%20audio%20samples%20in%0Aas%20few%20as%20eight%20sampling%20steps%2C%20paving%20the%20way%20for%20near-real-time%20applications%0Awithout%20requiring%20dedicated%20fine-tuning%20or%20retraining.%20We%20demonstrate%20that%0ASALSA-V%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20both%0Aaudiovisual%20alignment%20and%20synchronization%20with%20video%20content%20in%20quantitative%0Aevaluation%20and%20a%20human%20listening%20study.%20Furthermore%2C%20our%20use%20of%20random%20masking%0Aduring%20training%20enables%20our%20model%20to%20match%20spectral%20characteristics%20of%0Areference%20audio%20samples%2C%20broadening%20its%20applicability%20to%20professional%20audio%0Asynthesis%20tasks%20such%20as%20Foley%20generation%20and%20sound%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02916v1&entry.124074799=Read"},
{"title": "Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image\n  Segmentation", "author": "Talha Ahmed and Nehal Ahmed Shaikh and Hassan Mohy-ud-Din", "abstract": "  For equitable deployment of AI tools in hospitals and healthcare facilities,\nwe need Deep Segmentation Networks that offer high performance and can be\ntrained on cost-effective GPUs with limited memory and large batch sizes. In\nthis work, we propose Wave-GMS, a lightweight and efficient multi-scale\ngenerative model for medical image segmentation. Wave-GMS has a substantially\nsmaller number of trainable parameters, does not require loading\nmemory-intensive pretrained vision foundation models, and supports training\nwith large batch sizes on GPUs with limited memory. We conducted extensive\nexperiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument,\nand HAM10000), demonstrating that Wave-GMS achieves state-of-the-art\nsegmentation performance with superior cross-domain generalizability, while\nrequiring only ~2.6M trainable parameters. Code is available at\nhttps://github.com/ATPLab-LUMS/Wave-GMS.\n", "link": "http://arxiv.org/abs/2510.03216v1", "date": "2025-10-03", "relevancy": 2.2035, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5789}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5418}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wave-GMS%3A%20Lightweight%20Multi-Scale%20Generative%20Model%20for%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Wave-GMS%3A%20Lightweight%20Multi-Scale%20Generative%20Model%20for%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Talha%20Ahmed%20and%20Nehal%20Ahmed%20Shaikh%20and%20Hassan%20Mohy-ud-Din%0AAbstract%3A%20%20%20For%20equitable%20deployment%20of%20AI%20tools%20in%20hospitals%20and%20healthcare%20facilities%2C%0Awe%20need%20Deep%20Segmentation%20Networks%20that%20offer%20high%20performance%20and%20can%20be%0Atrained%20on%20cost-effective%20GPUs%20with%20limited%20memory%20and%20large%20batch%20sizes.%20In%0Athis%20work%2C%20we%20propose%20Wave-GMS%2C%20a%20lightweight%20and%20efficient%20multi-scale%0Agenerative%20model%20for%20medical%20image%20segmentation.%20Wave-GMS%20has%20a%20substantially%0Asmaller%20number%20of%20trainable%20parameters%2C%20does%20not%20require%20loading%0Amemory-intensive%20pretrained%20vision%20foundation%20models%2C%20and%20supports%20training%0Awith%20large%20batch%20sizes%20on%20GPUs%20with%20limited%20memory.%20We%20conducted%20extensive%0Aexperiments%20on%20four%20publicly%20available%20datasets%20%28BUS%2C%20BUSI%2C%20Kvasir-Instrument%2C%0Aand%20HAM10000%29%2C%20demonstrating%20that%20Wave-GMS%20achieves%20state-of-the-art%0Asegmentation%20performance%20with%20superior%20cross-domain%20generalizability%2C%20while%0Arequiring%20only%20~2.6M%20trainable%20parameters.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ATPLab-LUMS/Wave-GMS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWave-GMS%253A%2520Lightweight%2520Multi-Scale%2520Generative%2520Model%2520for%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DTalha%2520Ahmed%2520and%2520Nehal%2520Ahmed%2520Shaikh%2520and%2520Hassan%2520Mohy-ud-Din%26entry.1292438233%3D%2520%2520For%2520equitable%2520deployment%2520of%2520AI%2520tools%2520in%2520hospitals%2520and%2520healthcare%2520facilities%252C%250Awe%2520need%2520Deep%2520Segmentation%2520Networks%2520that%2520offer%2520high%2520performance%2520and%2520can%2520be%250Atrained%2520on%2520cost-effective%2520GPUs%2520with%2520limited%2520memory%2520and%2520large%2520batch%2520sizes.%2520In%250Athis%2520work%252C%2520we%2520propose%2520Wave-GMS%252C%2520a%2520lightweight%2520and%2520efficient%2520multi-scale%250Agenerative%2520model%2520for%2520medical%2520image%2520segmentation.%2520Wave-GMS%2520has%2520a%2520substantially%250Asmaller%2520number%2520of%2520trainable%2520parameters%252C%2520does%2520not%2520require%2520loading%250Amemory-intensive%2520pretrained%2520vision%2520foundation%2520models%252C%2520and%2520supports%2520training%250Awith%2520large%2520batch%2520sizes%2520on%2520GPUs%2520with%2520limited%2520memory.%2520We%2520conducted%2520extensive%250Aexperiments%2520on%2520four%2520publicly%2520available%2520datasets%2520%2528BUS%252C%2520BUSI%252C%2520Kvasir-Instrument%252C%250Aand%2520HAM10000%2529%252C%2520demonstrating%2520that%2520Wave-GMS%2520achieves%2520state-of-the-art%250Asegmentation%2520performance%2520with%2520superior%2520cross-domain%2520generalizability%252C%2520while%250Arequiring%2520only%2520~2.6M%2520trainable%2520parameters.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ATPLab-LUMS/Wave-GMS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wave-GMS%3A%20Lightweight%20Multi-Scale%20Generative%20Model%20for%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Talha%20Ahmed%20and%20Nehal%20Ahmed%20Shaikh%20and%20Hassan%20Mohy-ud-Din&entry.1292438233=%20%20For%20equitable%20deployment%20of%20AI%20tools%20in%20hospitals%20and%20healthcare%20facilities%2C%0Awe%20need%20Deep%20Segmentation%20Networks%20that%20offer%20high%20performance%20and%20can%20be%0Atrained%20on%20cost-effective%20GPUs%20with%20limited%20memory%20and%20large%20batch%20sizes.%20In%0Athis%20work%2C%20we%20propose%20Wave-GMS%2C%20a%20lightweight%20and%20efficient%20multi-scale%0Agenerative%20model%20for%20medical%20image%20segmentation.%20Wave-GMS%20has%20a%20substantially%0Asmaller%20number%20of%20trainable%20parameters%2C%20does%20not%20require%20loading%0Amemory-intensive%20pretrained%20vision%20foundation%20models%2C%20and%20supports%20training%0Awith%20large%20batch%20sizes%20on%20GPUs%20with%20limited%20memory.%20We%20conducted%20extensive%0Aexperiments%20on%20four%20publicly%20available%20datasets%20%28BUS%2C%20BUSI%2C%20Kvasir-Instrument%2C%0Aand%20HAM10000%29%2C%20demonstrating%20that%20Wave-GMS%20achieves%20state-of-the-art%0Asegmentation%20performance%20with%20superior%20cross-domain%20generalizability%2C%20while%0Arequiring%20only%20~2.6M%20trainable%20parameters.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ATPLab-LUMS/Wave-GMS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03216v1&entry.124074799=Read"},
{"title": "AI Generated Child Sexual Abuse Material -- What's the Harm?", "author": "Caoilte \u00d3 Ciardha and John Buckley and Rebecca S. Portnoff", "abstract": "  The development of generative artificial intelligence (AI) tools capable of\nproducing wholly or partially synthetic child sexual abuse material (AI CSAM)\npresents profound challenges for child protection, law enforcement, and\nsocietal responses to child exploitation. While some argue that the harmfulness\nof AI CSAM differs fundamentally from other CSAM due to a perceived absence of\ndirect victimization, this perspective fails to account for the range of risks\nassociated with its production and consumption. AI has been implicated in the\ncreation of synthetic CSAM of children who have not previously been abused, the\nrevictimization of known survivors of abuse, the facilitation of grooming,\ncoercion and sexual extortion, and the normalization of child sexual\nexploitation. Additionally, AI CSAM may serve as a new or enhanced pathway into\noffending by lowering barriers to engagement, desensitizing users to\nprogressively extreme content, and undermining protective factors for\nindividuals with a sexual interest in children. This paper provides a primer on\nsome key technologies, critically examines the harms associated with AI CSAM,\nand cautions against claims that it may function as a harm reduction tool,\nemphasizing how some appeals to harmlessness obscure its real risks and may\ncontribute to inertia in ecosystem responses.\n", "link": "http://arxiv.org/abs/2510.02978v1", "date": "2025-10-03", "relevancy": 2.1791, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4604}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4344}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Generated%20Child%20Sexual%20Abuse%20Material%20--%20What%27s%20the%20Harm%3F&body=Title%3A%20AI%20Generated%20Child%20Sexual%20Abuse%20Material%20--%20What%27s%20the%20Harm%3F%0AAuthor%3A%20Caoilte%20%C3%93%20Ciardha%20and%20John%20Buckley%20and%20Rebecca%20S.%20Portnoff%0AAbstract%3A%20%20%20The%20development%20of%20generative%20artificial%20intelligence%20%28AI%29%20tools%20capable%20of%0Aproducing%20wholly%20or%20partially%20synthetic%20child%20sexual%20abuse%20material%20%28AI%20CSAM%29%0Apresents%20profound%20challenges%20for%20child%20protection%2C%20law%20enforcement%2C%20and%0Asocietal%20responses%20to%20child%20exploitation.%20While%20some%20argue%20that%20the%20harmfulness%0Aof%20AI%20CSAM%20differs%20fundamentally%20from%20other%20CSAM%20due%20to%20a%20perceived%20absence%20of%0Adirect%20victimization%2C%20this%20perspective%20fails%20to%20account%20for%20the%20range%20of%20risks%0Aassociated%20with%20its%20production%20and%20consumption.%20AI%20has%20been%20implicated%20in%20the%0Acreation%20of%20synthetic%20CSAM%20of%20children%20who%20have%20not%20previously%20been%20abused%2C%20the%0Arevictimization%20of%20known%20survivors%20of%20abuse%2C%20the%20facilitation%20of%20grooming%2C%0Acoercion%20and%20sexual%20extortion%2C%20and%20the%20normalization%20of%20child%20sexual%0Aexploitation.%20Additionally%2C%20AI%20CSAM%20may%20serve%20as%20a%20new%20or%20enhanced%20pathway%20into%0Aoffending%20by%20lowering%20barriers%20to%20engagement%2C%20desensitizing%20users%20to%0Aprogressively%20extreme%20content%2C%20and%20undermining%20protective%20factors%20for%0Aindividuals%20with%20a%20sexual%20interest%20in%20children.%20This%20paper%20provides%20a%20primer%20on%0Asome%20key%20technologies%2C%20critically%20examines%20the%20harms%20associated%20with%20AI%20CSAM%2C%0Aand%20cautions%20against%20claims%20that%20it%20may%20function%20as%20a%20harm%20reduction%20tool%2C%0Aemphasizing%20how%20some%20appeals%20to%20harmlessness%20obscure%20its%20real%20risks%20and%20may%0Acontribute%20to%20inertia%20in%20ecosystem%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Generated%2520Child%2520Sexual%2520Abuse%2520Material%2520--%2520What%2527s%2520the%2520Harm%253F%26entry.906535625%3DCaoilte%2520%25C3%2593%2520Ciardha%2520and%2520John%2520Buckley%2520and%2520Rebecca%2520S.%2520Portnoff%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520generative%2520artificial%2520intelligence%2520%2528AI%2529%2520tools%2520capable%2520of%250Aproducing%2520wholly%2520or%2520partially%2520synthetic%2520child%2520sexual%2520abuse%2520material%2520%2528AI%2520CSAM%2529%250Apresents%2520profound%2520challenges%2520for%2520child%2520protection%252C%2520law%2520enforcement%252C%2520and%250Asocietal%2520responses%2520to%2520child%2520exploitation.%2520While%2520some%2520argue%2520that%2520the%2520harmfulness%250Aof%2520AI%2520CSAM%2520differs%2520fundamentally%2520from%2520other%2520CSAM%2520due%2520to%2520a%2520perceived%2520absence%2520of%250Adirect%2520victimization%252C%2520this%2520perspective%2520fails%2520to%2520account%2520for%2520the%2520range%2520of%2520risks%250Aassociated%2520with%2520its%2520production%2520and%2520consumption.%2520AI%2520has%2520been%2520implicated%2520in%2520the%250Acreation%2520of%2520synthetic%2520CSAM%2520of%2520children%2520who%2520have%2520not%2520previously%2520been%2520abused%252C%2520the%250Arevictimization%2520of%2520known%2520survivors%2520of%2520abuse%252C%2520the%2520facilitation%2520of%2520grooming%252C%250Acoercion%2520and%2520sexual%2520extortion%252C%2520and%2520the%2520normalization%2520of%2520child%2520sexual%250Aexploitation.%2520Additionally%252C%2520AI%2520CSAM%2520may%2520serve%2520as%2520a%2520new%2520or%2520enhanced%2520pathway%2520into%250Aoffending%2520by%2520lowering%2520barriers%2520to%2520engagement%252C%2520desensitizing%2520users%2520to%250Aprogressively%2520extreme%2520content%252C%2520and%2520undermining%2520protective%2520factors%2520for%250Aindividuals%2520with%2520a%2520sexual%2520interest%2520in%2520children.%2520This%2520paper%2520provides%2520a%2520primer%2520on%250Asome%2520key%2520technologies%252C%2520critically%2520examines%2520the%2520harms%2520associated%2520with%2520AI%2520CSAM%252C%250Aand%2520cautions%2520against%2520claims%2520that%2520it%2520may%2520function%2520as%2520a%2520harm%2520reduction%2520tool%252C%250Aemphasizing%2520how%2520some%2520appeals%2520to%2520harmlessness%2520obscure%2520its%2520real%2520risks%2520and%2520may%250Acontribute%2520to%2520inertia%2520in%2520ecosystem%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Generated%20Child%20Sexual%20Abuse%20Material%20--%20What%27s%20the%20Harm%3F&entry.906535625=Caoilte%20%C3%93%20Ciardha%20and%20John%20Buckley%20and%20Rebecca%20S.%20Portnoff&entry.1292438233=%20%20The%20development%20of%20generative%20artificial%20intelligence%20%28AI%29%20tools%20capable%20of%0Aproducing%20wholly%20or%20partially%20synthetic%20child%20sexual%20abuse%20material%20%28AI%20CSAM%29%0Apresents%20profound%20challenges%20for%20child%20protection%2C%20law%20enforcement%2C%20and%0Asocietal%20responses%20to%20child%20exploitation.%20While%20some%20argue%20that%20the%20harmfulness%0Aof%20AI%20CSAM%20differs%20fundamentally%20from%20other%20CSAM%20due%20to%20a%20perceived%20absence%20of%0Adirect%20victimization%2C%20this%20perspective%20fails%20to%20account%20for%20the%20range%20of%20risks%0Aassociated%20with%20its%20production%20and%20consumption.%20AI%20has%20been%20implicated%20in%20the%0Acreation%20of%20synthetic%20CSAM%20of%20children%20who%20have%20not%20previously%20been%20abused%2C%20the%0Arevictimization%20of%20known%20survivors%20of%20abuse%2C%20the%20facilitation%20of%20grooming%2C%0Acoercion%20and%20sexual%20extortion%2C%20and%20the%20normalization%20of%20child%20sexual%0Aexploitation.%20Additionally%2C%20AI%20CSAM%20may%20serve%20as%20a%20new%20or%20enhanced%20pathway%20into%0Aoffending%20by%20lowering%20barriers%20to%20engagement%2C%20desensitizing%20users%20to%0Aprogressively%20extreme%20content%2C%20and%20undermining%20protective%20factors%20for%0Aindividuals%20with%20a%20sexual%20interest%20in%20children.%20This%20paper%20provides%20a%20primer%20on%0Asome%20key%20technologies%2C%20critically%20examines%20the%20harms%20associated%20with%20AI%20CSAM%2C%0Aand%20cautions%20against%20claims%20that%20it%20may%20function%20as%20a%20harm%20reduction%20tool%2C%0Aemphasizing%20how%20some%20appeals%20to%20harmlessness%20obscure%20its%20real%20risks%20and%20may%0Acontribute%20to%20inertia%20in%20ecosystem%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02978v1&entry.124074799=Read"},
{"title": "AI-Enhanced Kinematic Modeling of Flexible Manipulators Using Multi-IMU\n  Sensor Fusion", "author": "Amir Hossein Barjini and Jouni Mattila", "abstract": "  This paper presents a novel framework for estimating the position and\norientation of flexible manipulators undergoing vertical motion using multiple\ninertial measurement units (IMUs), optimized and calibrated with ground truth\ndata. The flexible links are modeled as a series of rigid segments, with joint\nangles estimated from accelerometer and gyroscope measurements acquired by\ncost-effective IMUs. A complementary filter is employed to fuse the\nmeasurements, with its parameters optimized through particle swarm optimization\n(PSO) to mitigate noise and delay. To further improve estimation accuracy,\nresidual errors in position and orientation are compensated using radial basis\nfunction neural networks (RBFNN). Experimental results validate the\neffectiveness of the proposed intelligent multi-IMU kinematic estimation\nmethod, achieving root mean square errors (RMSE) of 0.00021~m, 0.00041~m, and\n0.00024~rad for $y$, $z$, and $\\theta$, respectively.\n", "link": "http://arxiv.org/abs/2510.02975v1", "date": "2025-10-03", "relevancy": 2.1751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6328}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5266}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Enhanced%20Kinematic%20Modeling%20of%20Flexible%20Manipulators%20Using%20Multi-IMU%0A%20%20Sensor%20Fusion&body=Title%3A%20AI-Enhanced%20Kinematic%20Modeling%20of%20Flexible%20Manipulators%20Using%20Multi-IMU%0A%20%20Sensor%20Fusion%0AAuthor%3A%20Amir%20Hossein%20Barjini%20and%20Jouni%20Mattila%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20framework%20for%20estimating%20the%20position%20and%0Aorientation%20of%20flexible%20manipulators%20undergoing%20vertical%20motion%20using%20multiple%0Ainertial%20measurement%20units%20%28IMUs%29%2C%20optimized%20and%20calibrated%20with%20ground%20truth%0Adata.%20The%20flexible%20links%20are%20modeled%20as%20a%20series%20of%20rigid%20segments%2C%20with%20joint%0Aangles%20estimated%20from%20accelerometer%20and%20gyroscope%20measurements%20acquired%20by%0Acost-effective%20IMUs.%20A%20complementary%20filter%20is%20employed%20to%20fuse%20the%0Ameasurements%2C%20with%20its%20parameters%20optimized%20through%20particle%20swarm%20optimization%0A%28PSO%29%20to%20mitigate%20noise%20and%20delay.%20To%20further%20improve%20estimation%20accuracy%2C%0Aresidual%20errors%20in%20position%20and%20orientation%20are%20compensated%20using%20radial%20basis%0Afunction%20neural%20networks%20%28RBFNN%29.%20Experimental%20results%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20intelligent%20multi-IMU%20kinematic%20estimation%0Amethod%2C%20achieving%20root%20mean%20square%20errors%20%28RMSE%29%20of%200.00021~m%2C%200.00041~m%2C%20and%0A0.00024~rad%20for%20%24y%24%2C%20%24z%24%2C%20and%20%24%5Ctheta%24%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Enhanced%2520Kinematic%2520Modeling%2520of%2520Flexible%2520Manipulators%2520Using%2520Multi-IMU%250A%2520%2520Sensor%2520Fusion%26entry.906535625%3DAmir%2520Hossein%2520Barjini%2520and%2520Jouni%2520Mattila%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520for%2520estimating%2520the%2520position%2520and%250Aorientation%2520of%2520flexible%2520manipulators%2520undergoing%2520vertical%2520motion%2520using%2520multiple%250Ainertial%2520measurement%2520units%2520%2528IMUs%2529%252C%2520optimized%2520and%2520calibrated%2520with%2520ground%2520truth%250Adata.%2520The%2520flexible%2520links%2520are%2520modeled%2520as%2520a%2520series%2520of%2520rigid%2520segments%252C%2520with%2520joint%250Aangles%2520estimated%2520from%2520accelerometer%2520and%2520gyroscope%2520measurements%2520acquired%2520by%250Acost-effective%2520IMUs.%2520A%2520complementary%2520filter%2520is%2520employed%2520to%2520fuse%2520the%250Ameasurements%252C%2520with%2520its%2520parameters%2520optimized%2520through%2520particle%2520swarm%2520optimization%250A%2528PSO%2529%2520to%2520mitigate%2520noise%2520and%2520delay.%2520To%2520further%2520improve%2520estimation%2520accuracy%252C%250Aresidual%2520errors%2520in%2520position%2520and%2520orientation%2520are%2520compensated%2520using%2520radial%2520basis%250Afunction%2520neural%2520networks%2520%2528RBFNN%2529.%2520Experimental%2520results%2520validate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520intelligent%2520multi-IMU%2520kinematic%2520estimation%250Amethod%252C%2520achieving%2520root%2520mean%2520square%2520errors%2520%2528RMSE%2529%2520of%25200.00021~m%252C%25200.00041~m%252C%2520and%250A0.00024~rad%2520for%2520%2524y%2524%252C%2520%2524z%2524%252C%2520and%2520%2524%255Ctheta%2524%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Enhanced%20Kinematic%20Modeling%20of%20Flexible%20Manipulators%20Using%20Multi-IMU%0A%20%20Sensor%20Fusion&entry.906535625=Amir%20Hossein%20Barjini%20and%20Jouni%20Mattila&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20framework%20for%20estimating%20the%20position%20and%0Aorientation%20of%20flexible%20manipulators%20undergoing%20vertical%20motion%20using%20multiple%0Ainertial%20measurement%20units%20%28IMUs%29%2C%20optimized%20and%20calibrated%20with%20ground%20truth%0Adata.%20The%20flexible%20links%20are%20modeled%20as%20a%20series%20of%20rigid%20segments%2C%20with%20joint%0Aangles%20estimated%20from%20accelerometer%20and%20gyroscope%20measurements%20acquired%20by%0Acost-effective%20IMUs.%20A%20complementary%20filter%20is%20employed%20to%20fuse%20the%0Ameasurements%2C%20with%20its%20parameters%20optimized%20through%20particle%20swarm%20optimization%0A%28PSO%29%20to%20mitigate%20noise%20and%20delay.%20To%20further%20improve%20estimation%20accuracy%2C%0Aresidual%20errors%20in%20position%20and%20orientation%20are%20compensated%20using%20radial%20basis%0Afunction%20neural%20networks%20%28RBFNN%29.%20Experimental%20results%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20intelligent%20multi-IMU%20kinematic%20estimation%0Amethod%2C%20achieving%20root%20mean%20square%20errors%20%28RMSE%29%20of%200.00021~m%2C%200.00041~m%2C%20and%0A0.00024~rad%20for%20%24y%24%2C%20%24z%24%2C%20and%20%24%5Ctheta%24%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02975v1&entry.124074799=Read"},
{"title": "Product-Quantised Image Representation for High-Quality Image Synthesis", "author": "Denis Zavadski and Nikita Philip Tatsch and Carsten Rother", "abstract": "  Product quantisation (PQ) is a classical method for scalable vector encoding,\nyet it has seen limited usage for latent representations in high-fidelity image\ngeneration. In this work, we introduce PQGAN, a quantised image autoencoder\nthat integrates PQ into the well-known vector quantisation (VQ) framework of\nVQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in\nterms of reconstruction performance, including both quantisation methods and\ntheir continuous counterparts. We achieve a PSNR score of 37dB, where prior\nwork achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up\nto 96%. Our key to success is a thorough analysis of the interaction between\ncodebook size, embedding dimensionality, and subspace factorisation, with\nvector and scalar quantisation as special cases. We obtain novel findings, such\nthat the performance of VQ and PQ behaves in opposite ways when scaling the\nembedding dimension. Furthermore, our analysis shows performance trends for PQ\nthat help guide optimal hyperparameter selection. Finally, we demonstrate that\nPQGAN can be seamlessly integrated into pre-trained diffusion models. This\nenables either a significantly faster and more compute-efficient generation, or\na doubling of the output resolution at no additional cost, positioning PQ as a\nstrong extension for discrete latent representation in image synthesis.\n", "link": "http://arxiv.org/abs/2510.03191v1", "date": "2025-10-03", "relevancy": 2.1745, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5496}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5417}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Product-Quantised%20Image%20Representation%20for%20High-Quality%20Image%20Synthesis&body=Title%3A%20Product-Quantised%20Image%20Representation%20for%20High-Quality%20Image%20Synthesis%0AAuthor%3A%20Denis%20Zavadski%20and%20Nikita%20Philip%20Tatsch%20and%20Carsten%20Rother%0AAbstract%3A%20%20%20Product%20quantisation%20%28PQ%29%20is%20a%20classical%20method%20for%20scalable%20vector%20encoding%2C%0Ayet%20it%20has%20seen%20limited%20usage%20for%20latent%20representations%20in%20high-fidelity%20image%0Ageneration.%20In%20this%20work%2C%20we%20introduce%20PQGAN%2C%20a%20quantised%20image%20autoencoder%0Athat%20integrates%20PQ%20into%20the%20well-known%20vector%20quantisation%20%28VQ%29%20framework%20of%0AVQGAN.%20PQGAN%20achieves%20a%20noticeable%20improvement%20over%20state-of-the-art%20methods%20in%0Aterms%20of%20reconstruction%20performance%2C%20including%20both%20quantisation%20methods%20and%0Atheir%20continuous%20counterparts.%20We%20achieve%20a%20PSNR%20score%20of%2037dB%2C%20where%20prior%0Awork%20achieves%2027dB%2C%20and%20are%20able%20to%20reduce%20the%20FID%2C%20LPIPS%2C%20and%20CMMD%20score%20by%20up%0Ato%2096%25.%20Our%20key%20to%20success%20is%20a%20thorough%20analysis%20of%20the%20interaction%20between%0Acodebook%20size%2C%20embedding%20dimensionality%2C%20and%20subspace%20factorisation%2C%20with%0Avector%20and%20scalar%20quantisation%20as%20special%20cases.%20We%20obtain%20novel%20findings%2C%20such%0Athat%20the%20performance%20of%20VQ%20and%20PQ%20behaves%20in%20opposite%20ways%20when%20scaling%20the%0Aembedding%20dimension.%20Furthermore%2C%20our%20analysis%20shows%20performance%20trends%20for%20PQ%0Athat%20help%20guide%20optimal%20hyperparameter%20selection.%20Finally%2C%20we%20demonstrate%20that%0APQGAN%20can%20be%20seamlessly%20integrated%20into%20pre-trained%20diffusion%20models.%20This%0Aenables%20either%20a%20significantly%20faster%20and%20more%20compute-efficient%20generation%2C%20or%0Aa%20doubling%20of%20the%20output%20resolution%20at%20no%20additional%20cost%2C%20positioning%20PQ%20as%20a%0Astrong%20extension%20for%20discrete%20latent%20representation%20in%20image%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProduct-Quantised%2520Image%2520Representation%2520for%2520High-Quality%2520Image%2520Synthesis%26entry.906535625%3DDenis%2520Zavadski%2520and%2520Nikita%2520Philip%2520Tatsch%2520and%2520Carsten%2520Rother%26entry.1292438233%3D%2520%2520Product%2520quantisation%2520%2528PQ%2529%2520is%2520a%2520classical%2520method%2520for%2520scalable%2520vector%2520encoding%252C%250Ayet%2520it%2520has%2520seen%2520limited%2520usage%2520for%2520latent%2520representations%2520in%2520high-fidelity%2520image%250Ageneration.%2520In%2520this%2520work%252C%2520we%2520introduce%2520PQGAN%252C%2520a%2520quantised%2520image%2520autoencoder%250Athat%2520integrates%2520PQ%2520into%2520the%2520well-known%2520vector%2520quantisation%2520%2528VQ%2529%2520framework%2520of%250AVQGAN.%2520PQGAN%2520achieves%2520a%2520noticeable%2520improvement%2520over%2520state-of-the-art%2520methods%2520in%250Aterms%2520of%2520reconstruction%2520performance%252C%2520including%2520both%2520quantisation%2520methods%2520and%250Atheir%2520continuous%2520counterparts.%2520We%2520achieve%2520a%2520PSNR%2520score%2520of%252037dB%252C%2520where%2520prior%250Awork%2520achieves%252027dB%252C%2520and%2520are%2520able%2520to%2520reduce%2520the%2520FID%252C%2520LPIPS%252C%2520and%2520CMMD%2520score%2520by%2520up%250Ato%252096%2525.%2520Our%2520key%2520to%2520success%2520is%2520a%2520thorough%2520analysis%2520of%2520the%2520interaction%2520between%250Acodebook%2520size%252C%2520embedding%2520dimensionality%252C%2520and%2520subspace%2520factorisation%252C%2520with%250Avector%2520and%2520scalar%2520quantisation%2520as%2520special%2520cases.%2520We%2520obtain%2520novel%2520findings%252C%2520such%250Athat%2520the%2520performance%2520of%2520VQ%2520and%2520PQ%2520behaves%2520in%2520opposite%2520ways%2520when%2520scaling%2520the%250Aembedding%2520dimension.%2520Furthermore%252C%2520our%2520analysis%2520shows%2520performance%2520trends%2520for%2520PQ%250Athat%2520help%2520guide%2520optimal%2520hyperparameter%2520selection.%2520Finally%252C%2520we%2520demonstrate%2520that%250APQGAN%2520can%2520be%2520seamlessly%2520integrated%2520into%2520pre-trained%2520diffusion%2520models.%2520This%250Aenables%2520either%2520a%2520significantly%2520faster%2520and%2520more%2520compute-efficient%2520generation%252C%2520or%250Aa%2520doubling%2520of%2520the%2520output%2520resolution%2520at%2520no%2520additional%2520cost%252C%2520positioning%2520PQ%2520as%2520a%250Astrong%2520extension%2520for%2520discrete%2520latent%2520representation%2520in%2520image%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Product-Quantised%20Image%20Representation%20for%20High-Quality%20Image%20Synthesis&entry.906535625=Denis%20Zavadski%20and%20Nikita%20Philip%20Tatsch%20and%20Carsten%20Rother&entry.1292438233=%20%20Product%20quantisation%20%28PQ%29%20is%20a%20classical%20method%20for%20scalable%20vector%20encoding%2C%0Ayet%20it%20has%20seen%20limited%20usage%20for%20latent%20representations%20in%20high-fidelity%20image%0Ageneration.%20In%20this%20work%2C%20we%20introduce%20PQGAN%2C%20a%20quantised%20image%20autoencoder%0Athat%20integrates%20PQ%20into%20the%20well-known%20vector%20quantisation%20%28VQ%29%20framework%20of%0AVQGAN.%20PQGAN%20achieves%20a%20noticeable%20improvement%20over%20state-of-the-art%20methods%20in%0Aterms%20of%20reconstruction%20performance%2C%20including%20both%20quantisation%20methods%20and%0Atheir%20continuous%20counterparts.%20We%20achieve%20a%20PSNR%20score%20of%2037dB%2C%20where%20prior%0Awork%20achieves%2027dB%2C%20and%20are%20able%20to%20reduce%20the%20FID%2C%20LPIPS%2C%20and%20CMMD%20score%20by%20up%0Ato%2096%25.%20Our%20key%20to%20success%20is%20a%20thorough%20analysis%20of%20the%20interaction%20between%0Acodebook%20size%2C%20embedding%20dimensionality%2C%20and%20subspace%20factorisation%2C%20with%0Avector%20and%20scalar%20quantisation%20as%20special%20cases.%20We%20obtain%20novel%20findings%2C%20such%0Athat%20the%20performance%20of%20VQ%20and%20PQ%20behaves%20in%20opposite%20ways%20when%20scaling%20the%0Aembedding%20dimension.%20Furthermore%2C%20our%20analysis%20shows%20performance%20trends%20for%20PQ%0Athat%20help%20guide%20optimal%20hyperparameter%20selection.%20Finally%2C%20we%20demonstrate%20that%0APQGAN%20can%20be%20seamlessly%20integrated%20into%20pre-trained%20diffusion%20models.%20This%0Aenables%20either%20a%20significantly%20faster%20and%20more%20compute-efficient%20generation%2C%20or%0Aa%20doubling%20of%20the%20output%20resolution%20at%20no%20additional%20cost%2C%20positioning%20PQ%20as%20a%0Astrong%20extension%20for%20discrete%20latent%20representation%20in%20image%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03191v1&entry.124074799=Read"},
{"title": "Automatic Generation of Digital Twins for Network Testing", "author": "Shenjia Ding and David Flynn and Paul Harvey", "abstract": "  The increased use of software in the operation and management of\ntelecommunication networks has moved the industry one step closer to realizing\nautonomous network operation. One consequence of this shift is the\nsignificantly increased need for testing and validation before such software\ncan be deployed. Complementing existing simulation or hardware-based\napproaches, digital twins present an environment to achieve this testing;\nhowever, they require significant time and human effort to configure and\nexecute. This paper explores the automatic generation of digital twins to\nprovide efficient and accurate validation tools, aligned to the ITU-T\nautonomous network architecture's experimentation subsystem. We present\nexperimental results for an initial use case, demonstrating that the approach\nis feasible in automatically creating efficient digital twins with sufficient\naccuracy to be included as part of existing validation pipelines.\n", "link": "http://arxiv.org/abs/2510.03205v1", "date": "2025-10-03", "relevancy": 2.1728, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4385}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4326}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Generation%20of%20Digital%20Twins%20for%20Network%20Testing&body=Title%3A%20Automatic%20Generation%20of%20Digital%20Twins%20for%20Network%20Testing%0AAuthor%3A%20Shenjia%20Ding%20and%20David%20Flynn%20and%20Paul%20Harvey%0AAbstract%3A%20%20%20The%20increased%20use%20of%20software%20in%20the%20operation%20and%20management%20of%0Atelecommunication%20networks%20has%20moved%20the%20industry%20one%20step%20closer%20to%20realizing%0Aautonomous%20network%20operation.%20One%20consequence%20of%20this%20shift%20is%20the%0Asignificantly%20increased%20need%20for%20testing%20and%20validation%20before%20such%20software%0Acan%20be%20deployed.%20Complementing%20existing%20simulation%20or%20hardware-based%0Aapproaches%2C%20digital%20twins%20present%20an%20environment%20to%20achieve%20this%20testing%3B%0Ahowever%2C%20they%20require%20significant%20time%20and%20human%20effort%20to%20configure%20and%0Aexecute.%20This%20paper%20explores%20the%20automatic%20generation%20of%20digital%20twins%20to%0Aprovide%20efficient%20and%20accurate%20validation%20tools%2C%20aligned%20to%20the%20ITU-T%0Aautonomous%20network%20architecture%27s%20experimentation%20subsystem.%20We%20present%0Aexperimental%20results%20for%20an%20initial%20use%20case%2C%20demonstrating%20that%20the%20approach%0Ais%20feasible%20in%20automatically%20creating%20efficient%20digital%20twins%20with%20sufficient%0Aaccuracy%20to%20be%20included%20as%20part%20of%20existing%20validation%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Generation%2520of%2520Digital%2520Twins%2520for%2520Network%2520Testing%26entry.906535625%3DShenjia%2520Ding%2520and%2520David%2520Flynn%2520and%2520Paul%2520Harvey%26entry.1292438233%3D%2520%2520The%2520increased%2520use%2520of%2520software%2520in%2520the%2520operation%2520and%2520management%2520of%250Atelecommunication%2520networks%2520has%2520moved%2520the%2520industry%2520one%2520step%2520closer%2520to%2520realizing%250Aautonomous%2520network%2520operation.%2520One%2520consequence%2520of%2520this%2520shift%2520is%2520the%250Asignificantly%2520increased%2520need%2520for%2520testing%2520and%2520validation%2520before%2520such%2520software%250Acan%2520be%2520deployed.%2520Complementing%2520existing%2520simulation%2520or%2520hardware-based%250Aapproaches%252C%2520digital%2520twins%2520present%2520an%2520environment%2520to%2520achieve%2520this%2520testing%253B%250Ahowever%252C%2520they%2520require%2520significant%2520time%2520and%2520human%2520effort%2520to%2520configure%2520and%250Aexecute.%2520This%2520paper%2520explores%2520the%2520automatic%2520generation%2520of%2520digital%2520twins%2520to%250Aprovide%2520efficient%2520and%2520accurate%2520validation%2520tools%252C%2520aligned%2520to%2520the%2520ITU-T%250Aautonomous%2520network%2520architecture%2527s%2520experimentation%2520subsystem.%2520We%2520present%250Aexperimental%2520results%2520for%2520an%2520initial%2520use%2520case%252C%2520demonstrating%2520that%2520the%2520approach%250Ais%2520feasible%2520in%2520automatically%2520creating%2520efficient%2520digital%2520twins%2520with%2520sufficient%250Aaccuracy%2520to%2520be%2520included%2520as%2520part%2520of%2520existing%2520validation%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Generation%20of%20Digital%20Twins%20for%20Network%20Testing&entry.906535625=Shenjia%20Ding%20and%20David%20Flynn%20and%20Paul%20Harvey&entry.1292438233=%20%20The%20increased%20use%20of%20software%20in%20the%20operation%20and%20management%20of%0Atelecommunication%20networks%20has%20moved%20the%20industry%20one%20step%20closer%20to%20realizing%0Aautonomous%20network%20operation.%20One%20consequence%20of%20this%20shift%20is%20the%0Asignificantly%20increased%20need%20for%20testing%20and%20validation%20before%20such%20software%0Acan%20be%20deployed.%20Complementing%20existing%20simulation%20or%20hardware-based%0Aapproaches%2C%20digital%20twins%20present%20an%20environment%20to%20achieve%20this%20testing%3B%0Ahowever%2C%20they%20require%20significant%20time%20and%20human%20effort%20to%20configure%20and%0Aexecute.%20This%20paper%20explores%20the%20automatic%20generation%20of%20digital%20twins%20to%0Aprovide%20efficient%20and%20accurate%20validation%20tools%2C%20aligned%20to%20the%20ITU-T%0Aautonomous%20network%20architecture%27s%20experimentation%20subsystem.%20We%20present%0Aexperimental%20results%20for%20an%20initial%20use%20case%2C%20demonstrating%20that%20the%20approach%0Ais%20feasible%20in%20automatically%20creating%20efficient%20digital%20twins%20with%20sufficient%0Aaccuracy%20to%20be%20included%20as%20part%20of%20existing%20validation%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03205v1&entry.124074799=Read"},
{"title": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content", "author": "Shaina Raza and Caesar Saleh and Azib Farooq and Emrul Hasan and Franklin Ogidi and Maximus Powers and Veronica Chatrath and Marcelo Lotif and Karanpal Sekhon and Roya Javadi and Haad Zahid and Anam Zahid and Vahid Reza Khazaie and Zhenyu Yu", "abstract": "  Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality.\n", "link": "http://arxiv.org/abs/2412.17052v5", "date": "2025-10-03", "relevancy": 2.1637, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViLBias%3A%20Detecting%20and%20Reasoning%20about%20Bias%20in%20Multimodal%20Content&body=Title%3A%20ViLBias%3A%20Detecting%20and%20Reasoning%20about%20Bias%20in%20Multimodal%20Content%0AAuthor%3A%20Shaina%20Raza%20and%20Caesar%20Saleh%20and%20Azib%20Farooq%20and%20Emrul%20Hasan%20and%20Franklin%20Ogidi%20and%20Maximus%20Powers%20and%20Veronica%20Chatrath%20and%20Marcelo%20Lotif%20and%20Karanpal%20Sekhon%20and%20Roya%20Javadi%20and%20Haad%20Zahid%20and%20Anam%20Zahid%20and%20Vahid%20Reza%20Khazaie%20and%20Zhenyu%20Yu%0AAbstract%3A%20%20%20Detecting%20bias%20in%20multimodal%20news%20requires%20models%20that%20reason%20over%0Atext--image%20pairs%2C%20not%20just%20classify%20text.%20In%20response%2C%20we%20present%20ViLBias%2C%20a%0AVQA-style%20benchmark%20and%20framework%20for%20detecting%20and%20reasoning%20about%20bias%20in%0Amultimodal%20news.%20The%20dataset%20comprises%2040%2C945%20text--image%20pairs%20from%20diverse%0Aoutlets%2C%20each%20annotated%20with%20a%20bias%20label%20and%20concise%20rationale%20using%20a%0Atwo-stage%20LLM-as-annotator%20pipeline%20with%20hierarchical%20majority%20voting%20and%0Ahuman-in-the-loop%20validation.%20We%20evaluate%20Small%20Language%20Models%20%28SLMs%29%2C%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20and%20Vision--Language%20Models%20%28VLMs%29%20across%20closed-ended%0Aclassification%20and%20open-ended%20reasoning%20%28oVQA%29%2C%20and%20compare%20parameter-efficient%0Atuning%20strategies.%20Results%20show%20that%20incorporating%20images%20alongside%20text%0Aimproves%20detection%20accuracy%20by%203--5%5C%25%2C%20and%20that%20LLMs/VLMs%20better%20capture%20subtle%0Aframing%20and%20text--image%20inconsistencies%20than%20SLMs.%20Parameter-efficient%20methods%0A%28LoRA/QLoRA/Adapters%29%20recover%2097--99%5C%25%20of%20full%20fine-tuning%20performance%20with%0A%24%3C5%5C%25%24%20trainable%20parameters.%20For%20oVQA%2C%20reasoning%20accuracy%20spans%2052--79%5C%25%20and%0Afaithfulness%2068--89%5C%25%2C%20both%20improved%20by%20instruction%20tuning%3B%20closed%20accuracy%0Acorrelates%20strongly%20with%20reasoning%20%28%24r%20%3D%200.91%24%29.%20ViLBias%20offers%20a%20scalable%0Abenchmark%20and%20strong%20baselines%20for%20multimodal%20bias%20detection%20and%20rationale%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17052v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViLBias%253A%2520Detecting%2520and%2520Reasoning%2520about%2520Bias%2520in%2520Multimodal%2520Content%26entry.906535625%3DShaina%2520Raza%2520and%2520Caesar%2520Saleh%2520and%2520Azib%2520Farooq%2520and%2520Emrul%2520Hasan%2520and%2520Franklin%2520Ogidi%2520and%2520Maximus%2520Powers%2520and%2520Veronica%2520Chatrath%2520and%2520Marcelo%2520Lotif%2520and%2520Karanpal%2520Sekhon%2520and%2520Roya%2520Javadi%2520and%2520Haad%2520Zahid%2520and%2520Anam%2520Zahid%2520and%2520Vahid%2520Reza%2520Khazaie%2520and%2520Zhenyu%2520Yu%26entry.1292438233%3D%2520%2520Detecting%2520bias%2520in%2520multimodal%2520news%2520requires%2520models%2520that%2520reason%2520over%250Atext--image%2520pairs%252C%2520not%2520just%2520classify%2520text.%2520In%2520response%252C%2520we%2520present%2520ViLBias%252C%2520a%250AVQA-style%2520benchmark%2520and%2520framework%2520for%2520detecting%2520and%2520reasoning%2520about%2520bias%2520in%250Amultimodal%2520news.%2520The%2520dataset%2520comprises%252040%252C945%2520text--image%2520pairs%2520from%2520diverse%250Aoutlets%252C%2520each%2520annotated%2520with%2520a%2520bias%2520label%2520and%2520concise%2520rationale%2520using%2520a%250Atwo-stage%2520LLM-as-annotator%2520pipeline%2520with%2520hierarchical%2520majority%2520voting%2520and%250Ahuman-in-the-loop%2520validation.%2520We%2520evaluate%2520Small%2520Language%2520Models%2520%2528SLMs%2529%252C%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520and%2520Vision--Language%2520Models%2520%2528VLMs%2529%2520across%2520closed-ended%250Aclassification%2520and%2520open-ended%2520reasoning%2520%2528oVQA%2529%252C%2520and%2520compare%2520parameter-efficient%250Atuning%2520strategies.%2520Results%2520show%2520that%2520incorporating%2520images%2520alongside%2520text%250Aimproves%2520detection%2520accuracy%2520by%25203--5%255C%2525%252C%2520and%2520that%2520LLMs/VLMs%2520better%2520capture%2520subtle%250Aframing%2520and%2520text--image%2520inconsistencies%2520than%2520SLMs.%2520Parameter-efficient%2520methods%250A%2528LoRA/QLoRA/Adapters%2529%2520recover%252097--99%255C%2525%2520of%2520full%2520fine-tuning%2520performance%2520with%250A%2524%253C5%255C%2525%2524%2520trainable%2520parameters.%2520For%2520oVQA%252C%2520reasoning%2520accuracy%2520spans%252052--79%255C%2525%2520and%250Afaithfulness%252068--89%255C%2525%252C%2520both%2520improved%2520by%2520instruction%2520tuning%253B%2520closed%2520accuracy%250Acorrelates%2520strongly%2520with%2520reasoning%2520%2528%2524r%2520%253D%25200.91%2524%2529.%2520ViLBias%2520offers%2520a%2520scalable%250Abenchmark%2520and%2520strong%2520baselines%2520for%2520multimodal%2520bias%2520detection%2520and%2520rationale%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17052v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViLBias%3A%20Detecting%20and%20Reasoning%20about%20Bias%20in%20Multimodal%20Content&entry.906535625=Shaina%20Raza%20and%20Caesar%20Saleh%20and%20Azib%20Farooq%20and%20Emrul%20Hasan%20and%20Franklin%20Ogidi%20and%20Maximus%20Powers%20and%20Veronica%20Chatrath%20and%20Marcelo%20Lotif%20and%20Karanpal%20Sekhon%20and%20Roya%20Javadi%20and%20Haad%20Zahid%20and%20Anam%20Zahid%20and%20Vahid%20Reza%20Khazaie%20and%20Zhenyu%20Yu&entry.1292438233=%20%20Detecting%20bias%20in%20multimodal%20news%20requires%20models%20that%20reason%20over%0Atext--image%20pairs%2C%20not%20just%20classify%20text.%20In%20response%2C%20we%20present%20ViLBias%2C%20a%0AVQA-style%20benchmark%20and%20framework%20for%20detecting%20and%20reasoning%20about%20bias%20in%0Amultimodal%20news.%20The%20dataset%20comprises%2040%2C945%20text--image%20pairs%20from%20diverse%0Aoutlets%2C%20each%20annotated%20with%20a%20bias%20label%20and%20concise%20rationale%20using%20a%0Atwo-stage%20LLM-as-annotator%20pipeline%20with%20hierarchical%20majority%20voting%20and%0Ahuman-in-the-loop%20validation.%20We%20evaluate%20Small%20Language%20Models%20%28SLMs%29%2C%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20and%20Vision--Language%20Models%20%28VLMs%29%20across%20closed-ended%0Aclassification%20and%20open-ended%20reasoning%20%28oVQA%29%2C%20and%20compare%20parameter-efficient%0Atuning%20strategies.%20Results%20show%20that%20incorporating%20images%20alongside%20text%0Aimproves%20detection%20accuracy%20by%203--5%5C%25%2C%20and%20that%20LLMs/VLMs%20better%20capture%20subtle%0Aframing%20and%20text--image%20inconsistencies%20than%20SLMs.%20Parameter-efficient%20methods%0A%28LoRA/QLoRA/Adapters%29%20recover%2097--99%5C%25%20of%20full%20fine-tuning%20performance%20with%0A%24%3C5%5C%25%24%20trainable%20parameters.%20For%20oVQA%2C%20reasoning%20accuracy%20spans%2052--79%5C%25%20and%0Afaithfulness%2068--89%5C%25%2C%20both%20improved%20by%20instruction%20tuning%3B%20closed%20accuracy%0Acorrelates%20strongly%20with%20reasoning%20%28%24r%20%3D%200.91%24%29.%20ViLBias%20offers%20a%20scalable%0Abenchmark%20and%20strong%20baselines%20for%20multimodal%20bias%20detection%20and%20rationale%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17052v5&entry.124074799=Read"},
{"title": "FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting", "author": "Tharuka Kasthuri Arachchige and Veselka Boeva and Shahrooz Abghari", "abstract": "  This work focuses on improving the performance and fairness of Federated\nLearning (FL) in non IID settings by enhancing model aggregation and boosting\nthe training of underperforming clients. We propose FeDABoost, a novel FL\nframework that integrates a dynamic boosting mechanism and an adaptive gradient\naggregation strategy. Inspired by the weighting mechanism of the Multiclass\nAdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to\nclients with lower local error rates, thereby promoting more reliable\ncontributions to the global model. In parallel, FeDABoost dynamically boosts\nunderperforming clients by adjusting the focal loss focusing parameter,\nemphasizing hard to classify examples during local training. We have evaluated\nFeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared\nits performance with those of FedAvg and Ditto. The results show that FeDABoost\nachieves improved fairness and competitive performance.\n", "link": "http://arxiv.org/abs/2510.02914v1", "date": "2025-10-03", "relevancy": 2.1525, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5963}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5028}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FeDABoost%3A%20Fairness%20Aware%20Federated%20Learning%20with%20Adaptive%20Boosting&body=Title%3A%20FeDABoost%3A%20Fairness%20Aware%20Federated%20Learning%20with%20Adaptive%20Boosting%0AAuthor%3A%20Tharuka%20Kasthuri%20Arachchige%20and%20Veselka%20Boeva%20and%20Shahrooz%20Abghari%0AAbstract%3A%20%20%20This%20work%20focuses%20on%20improving%20the%20performance%20and%20fairness%20of%20Federated%0ALearning%20%28FL%29%20in%20non%20IID%20settings%20by%20enhancing%20model%20aggregation%20and%20boosting%0Athe%20training%20of%20underperforming%20clients.%20We%20propose%20FeDABoost%2C%20a%20novel%20FL%0Aframework%20that%20integrates%20a%20dynamic%20boosting%20mechanism%20and%20an%20adaptive%20gradient%0Aaggregation%20strategy.%20Inspired%20by%20the%20weighting%20mechanism%20of%20the%20Multiclass%0AAdaBoost%20%28SAMME%29%20algorithm%2C%20our%20aggregation%20method%20assigns%20higher%20weights%20to%0Aclients%20with%20lower%20local%20error%20rates%2C%20thereby%20promoting%20more%20reliable%0Acontributions%20to%20the%20global%20model.%20In%20parallel%2C%20FeDABoost%20dynamically%20boosts%0Aunderperforming%20clients%20by%20adjusting%20the%20focal%20loss%20focusing%20parameter%2C%0Aemphasizing%20hard%20to%20classify%20examples%20during%20local%20training.%20We%20have%20evaluated%0AFeDABoost%20on%20three%20benchmark%20datasets%20MNIST%2C%20FEMNIST%2C%20and%20CIFAR10%2C%20and%20compared%0Aits%20performance%20with%20those%20of%20FedAvg%20and%20Ditto.%20The%20results%20show%20that%20FeDABoost%0Aachieves%20improved%20fairness%20and%20competitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeDABoost%253A%2520Fairness%2520Aware%2520Federated%2520Learning%2520with%2520Adaptive%2520Boosting%26entry.906535625%3DTharuka%2520Kasthuri%2520Arachchige%2520and%2520Veselka%2520Boeva%2520and%2520Shahrooz%2520Abghari%26entry.1292438233%3D%2520%2520This%2520work%2520focuses%2520on%2520improving%2520the%2520performance%2520and%2520fairness%2520of%2520Federated%250ALearning%2520%2528FL%2529%2520in%2520non%2520IID%2520settings%2520by%2520enhancing%2520model%2520aggregation%2520and%2520boosting%250Athe%2520training%2520of%2520underperforming%2520clients.%2520We%2520propose%2520FeDABoost%252C%2520a%2520novel%2520FL%250Aframework%2520that%2520integrates%2520a%2520dynamic%2520boosting%2520mechanism%2520and%2520an%2520adaptive%2520gradient%250Aaggregation%2520strategy.%2520Inspired%2520by%2520the%2520weighting%2520mechanism%2520of%2520the%2520Multiclass%250AAdaBoost%2520%2528SAMME%2529%2520algorithm%252C%2520our%2520aggregation%2520method%2520assigns%2520higher%2520weights%2520to%250Aclients%2520with%2520lower%2520local%2520error%2520rates%252C%2520thereby%2520promoting%2520more%2520reliable%250Acontributions%2520to%2520the%2520global%2520model.%2520In%2520parallel%252C%2520FeDABoost%2520dynamically%2520boosts%250Aunderperforming%2520clients%2520by%2520adjusting%2520the%2520focal%2520loss%2520focusing%2520parameter%252C%250Aemphasizing%2520hard%2520to%2520classify%2520examples%2520during%2520local%2520training.%2520We%2520have%2520evaluated%250AFeDABoost%2520on%2520three%2520benchmark%2520datasets%2520MNIST%252C%2520FEMNIST%252C%2520and%2520CIFAR10%252C%2520and%2520compared%250Aits%2520performance%2520with%2520those%2520of%2520FedAvg%2520and%2520Ditto.%2520The%2520results%2520show%2520that%2520FeDABoost%250Aachieves%2520improved%2520fairness%2520and%2520competitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FeDABoost%3A%20Fairness%20Aware%20Federated%20Learning%20with%20Adaptive%20Boosting&entry.906535625=Tharuka%20Kasthuri%20Arachchige%20and%20Veselka%20Boeva%20and%20Shahrooz%20Abghari&entry.1292438233=%20%20This%20work%20focuses%20on%20improving%20the%20performance%20and%20fairness%20of%20Federated%0ALearning%20%28FL%29%20in%20non%20IID%20settings%20by%20enhancing%20model%20aggregation%20and%20boosting%0Athe%20training%20of%20underperforming%20clients.%20We%20propose%20FeDABoost%2C%20a%20novel%20FL%0Aframework%20that%20integrates%20a%20dynamic%20boosting%20mechanism%20and%20an%20adaptive%20gradient%0Aaggregation%20strategy.%20Inspired%20by%20the%20weighting%20mechanism%20of%20the%20Multiclass%0AAdaBoost%20%28SAMME%29%20algorithm%2C%20our%20aggregation%20method%20assigns%20higher%20weights%20to%0Aclients%20with%20lower%20local%20error%20rates%2C%20thereby%20promoting%20more%20reliable%0Acontributions%20to%20the%20global%20model.%20In%20parallel%2C%20FeDABoost%20dynamically%20boosts%0Aunderperforming%20clients%20by%20adjusting%20the%20focal%20loss%20focusing%20parameter%2C%0Aemphasizing%20hard%20to%20classify%20examples%20during%20local%20training.%20We%20have%20evaluated%0AFeDABoost%20on%20three%20benchmark%20datasets%20MNIST%2C%20FEMNIST%2C%20and%20CIFAR10%2C%20and%20compared%0Aits%20performance%20with%20those%20of%20FedAvg%20and%20Ditto.%20The%20results%20show%20that%20FeDABoost%0Aachieves%20improved%20fairness%20and%20competitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02914v1&entry.124074799=Read"},
{"title": "Calibrated Uncertainty Sampling for Active Learning", "author": "Ha Manh Bui and Iliana Maifeld-Carucci and Anqi Liu", "abstract": "  We study the problem of actively learning a classifier with a low calibration\nerror. One of the most popular Acquisition Functions (AFs) in pool-based Active\nLearning (AL) is querying by the model's uncertainty. However, we recognize\nthat an uncalibrated uncertainty model on the unlabeled pool may significantly\naffect the AF effectiveness, leading to sub-optimal generalization and high\ncalibration error on unseen data. Deep Neural Networks (DNNs) make it even\nworse as the model uncertainty from DNN is usually uncalibrated. Therefore, we\npropose a new AF by estimating calibration errors and query samples with the\nhighest calibration error before leveraging DNN uncertainty. Specifically, we\nutilize a kernel calibration error estimator under the covariate shift and\nformally show that AL with this AF eventually leads to a bounded calibration\nerror on the unlabeled pool and unseen test data. Empirically, our proposed\nmethod surpasses other AF baselines by having a lower calibration and\ngeneralization error across pool-based AL settings.\n", "link": "http://arxiv.org/abs/2510.03162v1", "date": "2025-10-03", "relevancy": 2.1503, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5623}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5601}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrated%20Uncertainty%20Sampling%20for%20Active%20Learning&body=Title%3A%20Calibrated%20Uncertainty%20Sampling%20for%20Active%20Learning%0AAuthor%3A%20Ha%20Manh%20Bui%20and%20Iliana%20Maifeld-Carucci%20and%20Anqi%20Liu%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20actively%20learning%20a%20classifier%20with%20a%20low%20calibration%0Aerror.%20One%20of%20the%20most%20popular%20Acquisition%20Functions%20%28AFs%29%20in%20pool-based%20Active%0ALearning%20%28AL%29%20is%20querying%20by%20the%20model%27s%20uncertainty.%20However%2C%20we%20recognize%0Athat%20an%20uncalibrated%20uncertainty%20model%20on%20the%20unlabeled%20pool%20may%20significantly%0Aaffect%20the%20AF%20effectiveness%2C%20leading%20to%20sub-optimal%20generalization%20and%20high%0Acalibration%20error%20on%20unseen%20data.%20Deep%20Neural%20Networks%20%28DNNs%29%20make%20it%20even%0Aworse%20as%20the%20model%20uncertainty%20from%20DNN%20is%20usually%20uncalibrated.%20Therefore%2C%20we%0Apropose%20a%20new%20AF%20by%20estimating%20calibration%20errors%20and%20query%20samples%20with%20the%0Ahighest%20calibration%20error%20before%20leveraging%20DNN%20uncertainty.%20Specifically%2C%20we%0Autilize%20a%20kernel%20calibration%20error%20estimator%20under%20the%20covariate%20shift%20and%0Aformally%20show%20that%20AL%20with%20this%20AF%20eventually%20leads%20to%20a%20bounded%20calibration%0Aerror%20on%20the%20unlabeled%20pool%20and%20unseen%20test%20data.%20Empirically%2C%20our%20proposed%0Amethod%20surpasses%20other%20AF%20baselines%20by%20having%20a%20lower%20calibration%20and%0Ageneralization%20error%20across%20pool-based%20AL%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrated%2520Uncertainty%2520Sampling%2520for%2520Active%2520Learning%26entry.906535625%3DHa%2520Manh%2520Bui%2520and%2520Iliana%2520Maifeld-Carucci%2520and%2520Anqi%2520Liu%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520actively%2520learning%2520a%2520classifier%2520with%2520a%2520low%2520calibration%250Aerror.%2520One%2520of%2520the%2520most%2520popular%2520Acquisition%2520Functions%2520%2528AFs%2529%2520in%2520pool-based%2520Active%250ALearning%2520%2528AL%2529%2520is%2520querying%2520by%2520the%2520model%2527s%2520uncertainty.%2520However%252C%2520we%2520recognize%250Athat%2520an%2520uncalibrated%2520uncertainty%2520model%2520on%2520the%2520unlabeled%2520pool%2520may%2520significantly%250Aaffect%2520the%2520AF%2520effectiveness%252C%2520leading%2520to%2520sub-optimal%2520generalization%2520and%2520high%250Acalibration%2520error%2520on%2520unseen%2520data.%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520make%2520it%2520even%250Aworse%2520as%2520the%2520model%2520uncertainty%2520from%2520DNN%2520is%2520usually%2520uncalibrated.%2520Therefore%252C%2520we%250Apropose%2520a%2520new%2520AF%2520by%2520estimating%2520calibration%2520errors%2520and%2520query%2520samples%2520with%2520the%250Ahighest%2520calibration%2520error%2520before%2520leveraging%2520DNN%2520uncertainty.%2520Specifically%252C%2520we%250Autilize%2520a%2520kernel%2520calibration%2520error%2520estimator%2520under%2520the%2520covariate%2520shift%2520and%250Aformally%2520show%2520that%2520AL%2520with%2520this%2520AF%2520eventually%2520leads%2520to%2520a%2520bounded%2520calibration%250Aerror%2520on%2520the%2520unlabeled%2520pool%2520and%2520unseen%2520test%2520data.%2520Empirically%252C%2520our%2520proposed%250Amethod%2520surpasses%2520other%2520AF%2520baselines%2520by%2520having%2520a%2520lower%2520calibration%2520and%250Ageneralization%2520error%2520across%2520pool-based%2520AL%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrated%20Uncertainty%20Sampling%20for%20Active%20Learning&entry.906535625=Ha%20Manh%20Bui%20and%20Iliana%20Maifeld-Carucci%20and%20Anqi%20Liu&entry.1292438233=%20%20We%20study%20the%20problem%20of%20actively%20learning%20a%20classifier%20with%20a%20low%20calibration%0Aerror.%20One%20of%20the%20most%20popular%20Acquisition%20Functions%20%28AFs%29%20in%20pool-based%20Active%0ALearning%20%28AL%29%20is%20querying%20by%20the%20model%27s%20uncertainty.%20However%2C%20we%20recognize%0Athat%20an%20uncalibrated%20uncertainty%20model%20on%20the%20unlabeled%20pool%20may%20significantly%0Aaffect%20the%20AF%20effectiveness%2C%20leading%20to%20sub-optimal%20generalization%20and%20high%0Acalibration%20error%20on%20unseen%20data.%20Deep%20Neural%20Networks%20%28DNNs%29%20make%20it%20even%0Aworse%20as%20the%20model%20uncertainty%20from%20DNN%20is%20usually%20uncalibrated.%20Therefore%2C%20we%0Apropose%20a%20new%20AF%20by%20estimating%20calibration%20errors%20and%20query%20samples%20with%20the%0Ahighest%20calibration%20error%20before%20leveraging%20DNN%20uncertainty.%20Specifically%2C%20we%0Autilize%20a%20kernel%20calibration%20error%20estimator%20under%20the%20covariate%20shift%20and%0Aformally%20show%20that%20AL%20with%20this%20AF%20eventually%20leads%20to%20a%20bounded%20calibration%0Aerror%20on%20the%20unlabeled%20pool%20and%20unseen%20test%20data.%20Empirically%2C%20our%20proposed%0Amethod%20surpasses%20other%20AF%20baselines%20by%20having%20a%20lower%20calibration%20and%0Ageneralization%20error%20across%20pool-based%20AL%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03162v1&entry.124074799=Read"},
{"title": "jina-reranker-v3: Last but Not Late Interaction for Listwise Document\n  Reranking", "author": "Feng Wang and Yuqing Li and Han Xiao", "abstract": "  jina-reranker-v3 is a 0.6B-parameter multilingual listwise reranker that\nintroduces a novel \"last but not late\" interaction. Unlike late interaction\nmodels like ColBERT that encode documents separately before multi-vector\nmatching, our approach applies causal attention between the query and all\ncandidate documents in the same context window, enabling rich interactions\nbefore extracting contextual embeddings from each document's final token. The\nnew model achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while\nbeing significantly smaller than other models with comparable performance.\n", "link": "http://arxiv.org/abs/2509.25085v3", "date": "2025-10-03", "relevancy": 2.1313, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.429}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20jina-reranker-v3%3A%20Last%20but%20Not%20Late%20Interaction%20for%20Listwise%20Document%0A%20%20Reranking&body=Title%3A%20jina-reranker-v3%3A%20Last%20but%20Not%20Late%20Interaction%20for%20Listwise%20Document%0A%20%20Reranking%0AAuthor%3A%20Feng%20Wang%20and%20Yuqing%20Li%20and%20Han%20Xiao%0AAbstract%3A%20%20%20jina-reranker-v3%20is%20a%200.6B-parameter%20multilingual%20listwise%20reranker%20that%0Aintroduces%20a%20novel%20%22last%20but%20not%20late%22%20interaction.%20Unlike%20late%20interaction%0Amodels%20like%20ColBERT%20that%20encode%20documents%20separately%20before%20multi-vector%0Amatching%2C%20our%20approach%20applies%20causal%20attention%20between%20the%20query%20and%20all%0Acandidate%20documents%20in%20the%20same%20context%20window%2C%20enabling%20rich%20interactions%0Abefore%20extracting%20contextual%20embeddings%20from%20each%20document%27s%20final%20token.%20The%0Anew%20model%20achieves%20state-of-the-art%20BEIR%20performance%20with%2061.94%20nDCG%4010%20while%0Abeing%20significantly%20smaller%20than%20other%20models%20with%20comparable%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25085v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Djina-reranker-v3%253A%2520Last%2520but%2520Not%2520Late%2520Interaction%2520for%2520Listwise%2520Document%250A%2520%2520Reranking%26entry.906535625%3DFeng%2520Wang%2520and%2520Yuqing%2520Li%2520and%2520Han%2520Xiao%26entry.1292438233%3D%2520%2520jina-reranker-v3%2520is%2520a%25200.6B-parameter%2520multilingual%2520listwise%2520reranker%2520that%250Aintroduces%2520a%2520novel%2520%2522last%2520but%2520not%2520late%2522%2520interaction.%2520Unlike%2520late%2520interaction%250Amodels%2520like%2520ColBERT%2520that%2520encode%2520documents%2520separately%2520before%2520multi-vector%250Amatching%252C%2520our%2520approach%2520applies%2520causal%2520attention%2520between%2520the%2520query%2520and%2520all%250Acandidate%2520documents%2520in%2520the%2520same%2520context%2520window%252C%2520enabling%2520rich%2520interactions%250Abefore%2520extracting%2520contextual%2520embeddings%2520from%2520each%2520document%2527s%2520final%2520token.%2520The%250Anew%2520model%2520achieves%2520state-of-the-art%2520BEIR%2520performance%2520with%252061.94%2520nDCG%254010%2520while%250Abeing%2520significantly%2520smaller%2520than%2520other%2520models%2520with%2520comparable%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25085v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=jina-reranker-v3%3A%20Last%20but%20Not%20Late%20Interaction%20for%20Listwise%20Document%0A%20%20Reranking&entry.906535625=Feng%20Wang%20and%20Yuqing%20Li%20and%20Han%20Xiao&entry.1292438233=%20%20jina-reranker-v3%20is%20a%200.6B-parameter%20multilingual%20listwise%20reranker%20that%0Aintroduces%20a%20novel%20%22last%20but%20not%20late%22%20interaction.%20Unlike%20late%20interaction%0Amodels%20like%20ColBERT%20that%20encode%20documents%20separately%20before%20multi-vector%0Amatching%2C%20our%20approach%20applies%20causal%20attention%20between%20the%20query%20and%20all%0Acandidate%20documents%20in%20the%20same%20context%20window%2C%20enabling%20rich%20interactions%0Abefore%20extracting%20contextual%20embeddings%20from%20each%20document%27s%20final%20token.%20The%0Anew%20model%20achieves%20state-of-the-art%20BEIR%20performance%20with%2061.94%20nDCG%4010%20while%0Abeing%20significantly%20smaller%20than%20other%20models%20with%20comparable%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25085v3&entry.124074799=Read"},
{"title": "A Study of Rule Omission in Raven's Progressive Matrices", "author": "Binze Li", "abstract": "  Analogical reasoning lies at the core of human cognition and remains a\nfundamental challenge for artificial intelligence. Raven's Progressive Matrices\n(RPM) serve as a widely used benchmark to assess abstract reasoning by\nrequiring the inference of underlying structural rules. While many vision-based\nand language-based models have achieved success on RPM tasks, it remains\nunclear whether their performance reflects genuine reasoning ability or\nreliance on statistical shortcuts. This study investigates the generalization\ncapacity of modern AI systems under conditions of incomplete training by\ndeliberately omitting several structural rules during training. Both\nsequence-to-sequence transformer models and vision-based architectures such as\nCoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN\n(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate\nstrong performance on familiar rules, their accuracy declines sharply when\nfaced with novel or omitted rules. Moreover, the gap between token-level\naccuracy and complete answer accuracy highlights fundamental limitations in\ncurrent approaches. These findings provide new insights into the reasoning\nmechanisms underlying deep learning models and underscore the need for\narchitectures that move beyond pattern recognition toward robust abstract\nreasoning.\n", "link": "http://arxiv.org/abs/2510.03127v1", "date": "2025-10-03", "relevancy": 2.1309, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20of%20Rule%20Omission%20in%20Raven%27s%20Progressive%20Matrices&body=Title%3A%20A%20Study%20of%20Rule%20Omission%20in%20Raven%27s%20Progressive%20Matrices%0AAuthor%3A%20Binze%20Li%0AAbstract%3A%20%20%20Analogical%20reasoning%20lies%20at%20the%20core%20of%20human%20cognition%20and%20remains%20a%0Afundamental%20challenge%20for%20artificial%20intelligence.%20Raven%27s%20Progressive%20Matrices%0A%28RPM%29%20serve%20as%20a%20widely%20used%20benchmark%20to%20assess%20abstract%20reasoning%20by%0Arequiring%20the%20inference%20of%20underlying%20structural%20rules.%20While%20many%20vision-based%0Aand%20language-based%20models%20have%20achieved%20success%20on%20RPM%20tasks%2C%20it%20remains%0Aunclear%20whether%20their%20performance%20reflects%20genuine%20reasoning%20ability%20or%0Areliance%20on%20statistical%20shortcuts.%20This%20study%20investigates%20the%20generalization%0Acapacity%20of%20modern%20AI%20systems%20under%20conditions%20of%20incomplete%20training%20by%0Adeliberately%20omitting%20several%20structural%20rules%20during%20training.%20Both%0Asequence-to-sequence%20transformer%20models%20and%20vision-based%20architectures%20such%20as%0ACoPINet%20and%20the%20Dual-Contrast%20Network%20are%20evaluated%20on%20the%20Impartial-RAVEN%0A%28I-RAVEN%29%20dataset.%20Experiments%20reveal%20that%20although%20transformers%20demonstrate%0Astrong%20performance%20on%20familiar%20rules%2C%20their%20accuracy%20declines%20sharply%20when%0Afaced%20with%20novel%20or%20omitted%20rules.%20Moreover%2C%20the%20gap%20between%20token-level%0Aaccuracy%20and%20complete%20answer%20accuracy%20highlights%20fundamental%20limitations%20in%0Acurrent%20approaches.%20These%20findings%20provide%20new%20insights%20into%20the%20reasoning%0Amechanisms%20underlying%20deep%20learning%20models%20and%20underscore%20the%20need%20for%0Aarchitectures%20that%20move%20beyond%20pattern%20recognition%20toward%20robust%20abstract%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520of%2520Rule%2520Omission%2520in%2520Raven%2527s%2520Progressive%2520Matrices%26entry.906535625%3DBinze%2520Li%26entry.1292438233%3D%2520%2520Analogical%2520reasoning%2520lies%2520at%2520the%2520core%2520of%2520human%2520cognition%2520and%2520remains%2520a%250Afundamental%2520challenge%2520for%2520artificial%2520intelligence.%2520Raven%2527s%2520Progressive%2520Matrices%250A%2528RPM%2529%2520serve%2520as%2520a%2520widely%2520used%2520benchmark%2520to%2520assess%2520abstract%2520reasoning%2520by%250Arequiring%2520the%2520inference%2520of%2520underlying%2520structural%2520rules.%2520While%2520many%2520vision-based%250Aand%2520language-based%2520models%2520have%2520achieved%2520success%2520on%2520RPM%2520tasks%252C%2520it%2520remains%250Aunclear%2520whether%2520their%2520performance%2520reflects%2520genuine%2520reasoning%2520ability%2520or%250Areliance%2520on%2520statistical%2520shortcuts.%2520This%2520study%2520investigates%2520the%2520generalization%250Acapacity%2520of%2520modern%2520AI%2520systems%2520under%2520conditions%2520of%2520incomplete%2520training%2520by%250Adeliberately%2520omitting%2520several%2520structural%2520rules%2520during%2520training.%2520Both%250Asequence-to-sequence%2520transformer%2520models%2520and%2520vision-based%2520architectures%2520such%2520as%250ACoPINet%2520and%2520the%2520Dual-Contrast%2520Network%2520are%2520evaluated%2520on%2520the%2520Impartial-RAVEN%250A%2528I-RAVEN%2529%2520dataset.%2520Experiments%2520reveal%2520that%2520although%2520transformers%2520demonstrate%250Astrong%2520performance%2520on%2520familiar%2520rules%252C%2520their%2520accuracy%2520declines%2520sharply%2520when%250Afaced%2520with%2520novel%2520or%2520omitted%2520rules.%2520Moreover%252C%2520the%2520gap%2520between%2520token-level%250Aaccuracy%2520and%2520complete%2520answer%2520accuracy%2520highlights%2520fundamental%2520limitations%2520in%250Acurrent%2520approaches.%2520These%2520findings%2520provide%2520new%2520insights%2520into%2520the%2520reasoning%250Amechanisms%2520underlying%2520deep%2520learning%2520models%2520and%2520underscore%2520the%2520need%2520for%250Aarchitectures%2520that%2520move%2520beyond%2520pattern%2520recognition%2520toward%2520robust%2520abstract%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20of%20Rule%20Omission%20in%20Raven%27s%20Progressive%20Matrices&entry.906535625=Binze%20Li&entry.1292438233=%20%20Analogical%20reasoning%20lies%20at%20the%20core%20of%20human%20cognition%20and%20remains%20a%0Afundamental%20challenge%20for%20artificial%20intelligence.%20Raven%27s%20Progressive%20Matrices%0A%28RPM%29%20serve%20as%20a%20widely%20used%20benchmark%20to%20assess%20abstract%20reasoning%20by%0Arequiring%20the%20inference%20of%20underlying%20structural%20rules.%20While%20many%20vision-based%0Aand%20language-based%20models%20have%20achieved%20success%20on%20RPM%20tasks%2C%20it%20remains%0Aunclear%20whether%20their%20performance%20reflects%20genuine%20reasoning%20ability%20or%0Areliance%20on%20statistical%20shortcuts.%20This%20study%20investigates%20the%20generalization%0Acapacity%20of%20modern%20AI%20systems%20under%20conditions%20of%20incomplete%20training%20by%0Adeliberately%20omitting%20several%20structural%20rules%20during%20training.%20Both%0Asequence-to-sequence%20transformer%20models%20and%20vision-based%20architectures%20such%20as%0ACoPINet%20and%20the%20Dual-Contrast%20Network%20are%20evaluated%20on%20the%20Impartial-RAVEN%0A%28I-RAVEN%29%20dataset.%20Experiments%20reveal%20that%20although%20transformers%20demonstrate%0Astrong%20performance%20on%20familiar%20rules%2C%20their%20accuracy%20declines%20sharply%20when%0Afaced%20with%20novel%20or%20omitted%20rules.%20Moreover%2C%20the%20gap%20between%20token-level%0Aaccuracy%20and%20complete%20answer%20accuracy%20highlights%20fundamental%20limitations%20in%0Acurrent%20approaches.%20These%20findings%20provide%20new%20insights%20into%20the%20reasoning%0Amechanisms%20underlying%20deep%20learning%20models%20and%20underscore%20the%20need%20for%0Aarchitectures%20that%20move%20beyond%20pattern%20recognition%20toward%20robust%20abstract%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03127v1&entry.124074799=Read"},
{"title": "Efficient & Correct Predictive Equivalence for Decision Trees", "author": "Joao Marques-Silva and Alexey Ignatiev", "abstract": "  The Rashomon set of decision trees (DTs) finds importance uses. Recent work\nshowed that DTs computing the same classification function, i.e. predictive\nequivalent DTs, can represent a significant fraction of the Rashomon set. Such\nredundancy is undesirable. For example, feature importance based on the\nRashomon set becomes inaccurate due the existence of predictive equivalent DTs,\ni.e. DTs with the same prediction for every possible input. In recent work,\nMcTavish et al. proposed solutions for several computational problems related\nwith DTs, including that of deciding predictive equivalent DTs. The approach of\nMcTavish et al. consists of applying the well-known method of Quine-McCluskey\n(QM) for obtaining minimum-size DNF (disjunctive normal form) representations\nof DTs, which are then used for comparing DTs for predictive equivalence.\nFurthermore, the minimum-size DNF representation was also applied to computing\nexplanations for the predictions made by DTs, and to finding predictions in the\npresence of missing data. However, the problem of formula minimization is hard\nfor the second level of the polynomial hierarchy, and the QM method may exhibit\nworst-case exponential running time and space. This paper first demonstrates\nthat there exist decision trees that trigger the worst-case exponential running\ntime and space of the QM method. Second, the paper shows that the QM method may\nincorrectly decide predictive equivalence, if two key constraints are not\nrespected, and one may be difficult to formally guarantee. Third, the paper\nshows that any of the problems to which the smallest DNF representation has\nbeen applied to can be solved in polynomial time, in the size of the DT. The\nexperiments confirm that, for DTs for which the worst-case of the QM method is\ntriggered, the algorithms proposed in this paper are orders of magnitude faster\nthan the ones proposed by McTavish et al.\n", "link": "http://arxiv.org/abs/2509.17774v3", "date": "2025-10-03", "relevancy": 2.1198, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4348}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4199}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20%26%20Correct%20Predictive%20Equivalence%20for%20Decision%20Trees&body=Title%3A%20Efficient%20%26%20Correct%20Predictive%20Equivalence%20for%20Decision%20Trees%0AAuthor%3A%20Joao%20Marques-Silva%20and%20Alexey%20Ignatiev%0AAbstract%3A%20%20%20The%20Rashomon%20set%20of%20decision%20trees%20%28DTs%29%20finds%20importance%20uses.%20Recent%20work%0Ashowed%20that%20DTs%20computing%20the%20same%20classification%20function%2C%20i.e.%20predictive%0Aequivalent%20DTs%2C%20can%20represent%20a%20significant%20fraction%20of%20the%20Rashomon%20set.%20Such%0Aredundancy%20is%20undesirable.%20For%20example%2C%20feature%20importance%20based%20on%20the%0ARashomon%20set%20becomes%20inaccurate%20due%20the%20existence%20of%20predictive%20equivalent%20DTs%2C%0Ai.e.%20DTs%20with%20the%20same%20prediction%20for%20every%20possible%20input.%20In%20recent%20work%2C%0AMcTavish%20et%20al.%20proposed%20solutions%20for%20several%20computational%20problems%20related%0Awith%20DTs%2C%20including%20that%20of%20deciding%20predictive%20equivalent%20DTs.%20The%20approach%20of%0AMcTavish%20et%20al.%20consists%20of%20applying%20the%20well-known%20method%20of%20Quine-McCluskey%0A%28QM%29%20for%20obtaining%20minimum-size%20DNF%20%28disjunctive%20normal%20form%29%20representations%0Aof%20DTs%2C%20which%20are%20then%20used%20for%20comparing%20DTs%20for%20predictive%20equivalence.%0AFurthermore%2C%20the%20minimum-size%20DNF%20representation%20was%20also%20applied%20to%20computing%0Aexplanations%20for%20the%20predictions%20made%20by%20DTs%2C%20and%20to%20finding%20predictions%20in%20the%0Apresence%20of%20missing%20data.%20However%2C%20the%20problem%20of%20formula%20minimization%20is%20hard%0Afor%20the%20second%20level%20of%20the%20polynomial%20hierarchy%2C%20and%20the%20QM%20method%20may%20exhibit%0Aworst-case%20exponential%20running%20time%20and%20space.%20This%20paper%20first%20demonstrates%0Athat%20there%20exist%20decision%20trees%20that%20trigger%20the%20worst-case%20exponential%20running%0Atime%20and%20space%20of%20the%20QM%20method.%20Second%2C%20the%20paper%20shows%20that%20the%20QM%20method%20may%0Aincorrectly%20decide%20predictive%20equivalence%2C%20if%20two%20key%20constraints%20are%20not%0Arespected%2C%20and%20one%20may%20be%20difficult%20to%20formally%20guarantee.%20Third%2C%20the%20paper%0Ashows%20that%20any%20of%20the%20problems%20to%20which%20the%20smallest%20DNF%20representation%20has%0Abeen%20applied%20to%20can%20be%20solved%20in%20polynomial%20time%2C%20in%20the%20size%20of%20the%20DT.%20The%0Aexperiments%20confirm%20that%2C%20for%20DTs%20for%20which%20the%20worst-case%20of%20the%20QM%20method%20is%0Atriggered%2C%20the%20algorithms%20proposed%20in%20this%20paper%20are%20orders%20of%20magnitude%20faster%0Athan%20the%20ones%20proposed%20by%20McTavish%20et%20al.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17774v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520%2526%2520Correct%2520Predictive%2520Equivalence%2520for%2520Decision%2520Trees%26entry.906535625%3DJoao%2520Marques-Silva%2520and%2520Alexey%2520Ignatiev%26entry.1292438233%3D%2520%2520The%2520Rashomon%2520set%2520of%2520decision%2520trees%2520%2528DTs%2529%2520finds%2520importance%2520uses.%2520Recent%2520work%250Ashowed%2520that%2520DTs%2520computing%2520the%2520same%2520classification%2520function%252C%2520i.e.%2520predictive%250Aequivalent%2520DTs%252C%2520can%2520represent%2520a%2520significant%2520fraction%2520of%2520the%2520Rashomon%2520set.%2520Such%250Aredundancy%2520is%2520undesirable.%2520For%2520example%252C%2520feature%2520importance%2520based%2520on%2520the%250ARashomon%2520set%2520becomes%2520inaccurate%2520due%2520the%2520existence%2520of%2520predictive%2520equivalent%2520DTs%252C%250Ai.e.%2520DTs%2520with%2520the%2520same%2520prediction%2520for%2520every%2520possible%2520input.%2520In%2520recent%2520work%252C%250AMcTavish%2520et%2520al.%2520proposed%2520solutions%2520for%2520several%2520computational%2520problems%2520related%250Awith%2520DTs%252C%2520including%2520that%2520of%2520deciding%2520predictive%2520equivalent%2520DTs.%2520The%2520approach%2520of%250AMcTavish%2520et%2520al.%2520consists%2520of%2520applying%2520the%2520well-known%2520method%2520of%2520Quine-McCluskey%250A%2528QM%2529%2520for%2520obtaining%2520minimum-size%2520DNF%2520%2528disjunctive%2520normal%2520form%2529%2520representations%250Aof%2520DTs%252C%2520which%2520are%2520then%2520used%2520for%2520comparing%2520DTs%2520for%2520predictive%2520equivalence.%250AFurthermore%252C%2520the%2520minimum-size%2520DNF%2520representation%2520was%2520also%2520applied%2520to%2520computing%250Aexplanations%2520for%2520the%2520predictions%2520made%2520by%2520DTs%252C%2520and%2520to%2520finding%2520predictions%2520in%2520the%250Apresence%2520of%2520missing%2520data.%2520However%252C%2520the%2520problem%2520of%2520formula%2520minimization%2520is%2520hard%250Afor%2520the%2520second%2520level%2520of%2520the%2520polynomial%2520hierarchy%252C%2520and%2520the%2520QM%2520method%2520may%2520exhibit%250Aworst-case%2520exponential%2520running%2520time%2520and%2520space.%2520This%2520paper%2520first%2520demonstrates%250Athat%2520there%2520exist%2520decision%2520trees%2520that%2520trigger%2520the%2520worst-case%2520exponential%2520running%250Atime%2520and%2520space%2520of%2520the%2520QM%2520method.%2520Second%252C%2520the%2520paper%2520shows%2520that%2520the%2520QM%2520method%2520may%250Aincorrectly%2520decide%2520predictive%2520equivalence%252C%2520if%2520two%2520key%2520constraints%2520are%2520not%250Arespected%252C%2520and%2520one%2520may%2520be%2520difficult%2520to%2520formally%2520guarantee.%2520Third%252C%2520the%2520paper%250Ashows%2520that%2520any%2520of%2520the%2520problems%2520to%2520which%2520the%2520smallest%2520DNF%2520representation%2520has%250Abeen%2520applied%2520to%2520can%2520be%2520solved%2520in%2520polynomial%2520time%252C%2520in%2520the%2520size%2520of%2520the%2520DT.%2520The%250Aexperiments%2520confirm%2520that%252C%2520for%2520DTs%2520for%2520which%2520the%2520worst-case%2520of%2520the%2520QM%2520method%2520is%250Atriggered%252C%2520the%2520algorithms%2520proposed%2520in%2520this%2520paper%2520are%2520orders%2520of%2520magnitude%2520faster%250Athan%2520the%2520ones%2520proposed%2520by%2520McTavish%2520et%2520al.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17774v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20%26%20Correct%20Predictive%20Equivalence%20for%20Decision%20Trees&entry.906535625=Joao%20Marques-Silva%20and%20Alexey%20Ignatiev&entry.1292438233=%20%20The%20Rashomon%20set%20of%20decision%20trees%20%28DTs%29%20finds%20importance%20uses.%20Recent%20work%0Ashowed%20that%20DTs%20computing%20the%20same%20classification%20function%2C%20i.e.%20predictive%0Aequivalent%20DTs%2C%20can%20represent%20a%20significant%20fraction%20of%20the%20Rashomon%20set.%20Such%0Aredundancy%20is%20undesirable.%20For%20example%2C%20feature%20importance%20based%20on%20the%0ARashomon%20set%20becomes%20inaccurate%20due%20the%20existence%20of%20predictive%20equivalent%20DTs%2C%0Ai.e.%20DTs%20with%20the%20same%20prediction%20for%20every%20possible%20input.%20In%20recent%20work%2C%0AMcTavish%20et%20al.%20proposed%20solutions%20for%20several%20computational%20problems%20related%0Awith%20DTs%2C%20including%20that%20of%20deciding%20predictive%20equivalent%20DTs.%20The%20approach%20of%0AMcTavish%20et%20al.%20consists%20of%20applying%20the%20well-known%20method%20of%20Quine-McCluskey%0A%28QM%29%20for%20obtaining%20minimum-size%20DNF%20%28disjunctive%20normal%20form%29%20representations%0Aof%20DTs%2C%20which%20are%20then%20used%20for%20comparing%20DTs%20for%20predictive%20equivalence.%0AFurthermore%2C%20the%20minimum-size%20DNF%20representation%20was%20also%20applied%20to%20computing%0Aexplanations%20for%20the%20predictions%20made%20by%20DTs%2C%20and%20to%20finding%20predictions%20in%20the%0Apresence%20of%20missing%20data.%20However%2C%20the%20problem%20of%20formula%20minimization%20is%20hard%0Afor%20the%20second%20level%20of%20the%20polynomial%20hierarchy%2C%20and%20the%20QM%20method%20may%20exhibit%0Aworst-case%20exponential%20running%20time%20and%20space.%20This%20paper%20first%20demonstrates%0Athat%20there%20exist%20decision%20trees%20that%20trigger%20the%20worst-case%20exponential%20running%0Atime%20and%20space%20of%20the%20QM%20method.%20Second%2C%20the%20paper%20shows%20that%20the%20QM%20method%20may%0Aincorrectly%20decide%20predictive%20equivalence%2C%20if%20two%20key%20constraints%20are%20not%0Arespected%2C%20and%20one%20may%20be%20difficult%20to%20formally%20guarantee.%20Third%2C%20the%20paper%0Ashows%20that%20any%20of%20the%20problems%20to%20which%20the%20smallest%20DNF%20representation%20has%0Abeen%20applied%20to%20can%20be%20solved%20in%20polynomial%20time%2C%20in%20the%20size%20of%20the%20DT.%20The%0Aexperiments%20confirm%20that%2C%20for%20DTs%20for%20which%20the%20worst-case%20of%20the%20QM%20method%20is%0Atriggered%2C%20the%20algorithms%20proposed%20in%20this%20paper%20are%20orders%20of%20magnitude%20faster%0Athan%20the%20ones%20proposed%20by%20McTavish%20et%20al.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17774v3&entry.124074799=Read"},
{"title": "Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning", "author": "Yilun Hao and Yongchao Chen and Chuchu Fan and Yang Zhang", "abstract": "  Vision Language Models (VLMs) show strong potential for visual planning but\nstruggle with precise spatial and long-horizon reasoning. In contrast, Planning\nDomain Definition Language (PDDL) planners excel at long-horizon formal\nplanning, but cannot interpret visual inputs. Recent works combine these\ncomplementary advantages by enabling VLMs to turn visual planning problems into\nPDDL files for formal planning. However, while VLMs can generate PDDL problem\nfiles satisfactorily, they struggle to accurately generate the PDDL domain\nfiles, which describe all the planning rules. As a result, prior methods rely\non human experts to predefine domain files or on constant environment access\nfor refinement. We propose VLMFP, a Dual-VLM-guided framework that can\nautonomously generate both PDDL problem and domain files for formal visual\nplanning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A\nSimVLM that simulates action consequences based on input rule descriptions, and\na GenVLM that generates and iteratively refines PDDL files by comparing the\nPDDL and SimVLM execution results. VLMFP unleashes multiple levels of\ngeneralizability: The same generated PDDL domain file works for all the\ndifferent instances under the same problem, and VLMs generalize to different\nproblems with varied appearances and rules. We evaluate VLMFP with 6 grid-world\ndomains and test its generalization to unseen instances, appearance, and game\nrules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios,\nsimulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal\nreaching for seen and unseen appearances, respectively. With the guidance of\nSimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for\nunseen instances in seen and unseen appearances, respectively. Project page:\nhttps://sites.google.com/view/vlmfp.\n", "link": "http://arxiv.org/abs/2510.03182v1", "date": "2025-10-03", "relevancy": 2.1194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulation%20to%20Rules%3A%20A%20Dual-VLM%20Framework%20for%20Formal%20Visual%20Planning&body=Title%3A%20Simulation%20to%20Rules%3A%20A%20Dual-VLM%20Framework%20for%20Formal%20Visual%20Planning%0AAuthor%3A%20Yilun%20Hao%20and%20Yongchao%20Chen%20and%20Chuchu%20Fan%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20show%20strong%20potential%20for%20visual%20planning%20but%0Astruggle%20with%20precise%20spatial%20and%20long-horizon%20reasoning.%20In%20contrast%2C%20Planning%0ADomain%20Definition%20Language%20%28PDDL%29%20planners%20excel%20at%20long-horizon%20formal%0Aplanning%2C%20but%20cannot%20interpret%20visual%20inputs.%20Recent%20works%20combine%20these%0Acomplementary%20advantages%20by%20enabling%20VLMs%20to%20turn%20visual%20planning%20problems%20into%0APDDL%20files%20for%20formal%20planning.%20However%2C%20while%20VLMs%20can%20generate%20PDDL%20problem%0Afiles%20satisfactorily%2C%20they%20struggle%20to%20accurately%20generate%20the%20PDDL%20domain%0Afiles%2C%20which%20describe%20all%20the%20planning%20rules.%20As%20a%20result%2C%20prior%20methods%20rely%0Aon%20human%20experts%20to%20predefine%20domain%20files%20or%20on%20constant%20environment%20access%0Afor%20refinement.%20We%20propose%20VLMFP%2C%20a%20Dual-VLM-guided%20framework%20that%20can%0Aautonomously%20generate%20both%20PDDL%20problem%20and%20domain%20files%20for%20formal%20visual%0Aplanning.%20VLMFP%20introduces%20two%20VLMs%20to%20ensure%20reliable%20PDDL%20file%20generation%3A%20A%0ASimVLM%20that%20simulates%20action%20consequences%20based%20on%20input%20rule%20descriptions%2C%20and%0Aa%20GenVLM%20that%20generates%20and%20iteratively%20refines%20PDDL%20files%20by%20comparing%20the%0APDDL%20and%20SimVLM%20execution%20results.%20VLMFP%20unleashes%20multiple%20levels%20of%0Ageneralizability%3A%20The%20same%20generated%20PDDL%20domain%20file%20works%20for%20all%20the%0Adifferent%20instances%20under%20the%20same%20problem%2C%20and%20VLMs%20generalize%20to%20different%0Aproblems%20with%20varied%20appearances%20and%20rules.%20We%20evaluate%20VLMFP%20with%206%20grid-world%0Adomains%20and%20test%20its%20generalization%20to%20unseen%20instances%2C%20appearance%2C%20and%20game%0Arules.%20On%20average%2C%20SimVLM%20accurately%20describes%2095.5%25%2C%2082.6%25%20of%20scenarios%2C%0Asimulates%2085.5%25%2C%2087.8%25%20of%20action%20sequence%2C%20and%20judges%2082.4%25%2C%2085.6%25%20goal%0Areaching%20for%20seen%20and%20unseen%20appearances%2C%20respectively.%20With%20the%20guidance%20of%0ASimVLM%2C%20VLMFP%20can%20generate%20PDDL%20files%20to%20reach%2070.0%25%2C%2054.1%25%20valid%20plans%20for%0Aunseen%20instances%20in%20seen%20and%20unseen%20appearances%2C%20respectively.%20Project%20page%3A%0Ahttps%3A//sites.google.com/view/vlmfp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulation%2520to%2520Rules%253A%2520A%2520Dual-VLM%2520Framework%2520for%2520Formal%2520Visual%2520Planning%26entry.906535625%3DYilun%2520Hao%2520and%2520Yongchao%2520Chen%2520and%2520Chuchu%2520Fan%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520show%2520strong%2520potential%2520for%2520visual%2520planning%2520but%250Astruggle%2520with%2520precise%2520spatial%2520and%2520long-horizon%2520reasoning.%2520In%2520contrast%252C%2520Planning%250ADomain%2520Definition%2520Language%2520%2528PDDL%2529%2520planners%2520excel%2520at%2520long-horizon%2520formal%250Aplanning%252C%2520but%2520cannot%2520interpret%2520visual%2520inputs.%2520Recent%2520works%2520combine%2520these%250Acomplementary%2520advantages%2520by%2520enabling%2520VLMs%2520to%2520turn%2520visual%2520planning%2520problems%2520into%250APDDL%2520files%2520for%2520formal%2520planning.%2520However%252C%2520while%2520VLMs%2520can%2520generate%2520PDDL%2520problem%250Afiles%2520satisfactorily%252C%2520they%2520struggle%2520to%2520accurately%2520generate%2520the%2520PDDL%2520domain%250Afiles%252C%2520which%2520describe%2520all%2520the%2520planning%2520rules.%2520As%2520a%2520result%252C%2520prior%2520methods%2520rely%250Aon%2520human%2520experts%2520to%2520predefine%2520domain%2520files%2520or%2520on%2520constant%2520environment%2520access%250Afor%2520refinement.%2520We%2520propose%2520VLMFP%252C%2520a%2520Dual-VLM-guided%2520framework%2520that%2520can%250Aautonomously%2520generate%2520both%2520PDDL%2520problem%2520and%2520domain%2520files%2520for%2520formal%2520visual%250Aplanning.%2520VLMFP%2520introduces%2520two%2520VLMs%2520to%2520ensure%2520reliable%2520PDDL%2520file%2520generation%253A%2520A%250ASimVLM%2520that%2520simulates%2520action%2520consequences%2520based%2520on%2520input%2520rule%2520descriptions%252C%2520and%250Aa%2520GenVLM%2520that%2520generates%2520and%2520iteratively%2520refines%2520PDDL%2520files%2520by%2520comparing%2520the%250APDDL%2520and%2520SimVLM%2520execution%2520results.%2520VLMFP%2520unleashes%2520multiple%2520levels%2520of%250Ageneralizability%253A%2520The%2520same%2520generated%2520PDDL%2520domain%2520file%2520works%2520for%2520all%2520the%250Adifferent%2520instances%2520under%2520the%2520same%2520problem%252C%2520and%2520VLMs%2520generalize%2520to%2520different%250Aproblems%2520with%2520varied%2520appearances%2520and%2520rules.%2520We%2520evaluate%2520VLMFP%2520with%25206%2520grid-world%250Adomains%2520and%2520test%2520its%2520generalization%2520to%2520unseen%2520instances%252C%2520appearance%252C%2520and%2520game%250Arules.%2520On%2520average%252C%2520SimVLM%2520accurately%2520describes%252095.5%2525%252C%252082.6%2525%2520of%2520scenarios%252C%250Asimulates%252085.5%2525%252C%252087.8%2525%2520of%2520action%2520sequence%252C%2520and%2520judges%252082.4%2525%252C%252085.6%2525%2520goal%250Areaching%2520for%2520seen%2520and%2520unseen%2520appearances%252C%2520respectively.%2520With%2520the%2520guidance%2520of%250ASimVLM%252C%2520VLMFP%2520can%2520generate%2520PDDL%2520files%2520to%2520reach%252070.0%2525%252C%252054.1%2525%2520valid%2520plans%2520for%250Aunseen%2520instances%2520in%2520seen%2520and%2520unseen%2520appearances%252C%2520respectively.%2520Project%2520page%253A%250Ahttps%253A//sites.google.com/view/vlmfp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulation%20to%20Rules%3A%20A%20Dual-VLM%20Framework%20for%20Formal%20Visual%20Planning&entry.906535625=Yilun%20Hao%20and%20Yongchao%20Chen%20and%20Chuchu%20Fan%20and%20Yang%20Zhang&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20show%20strong%20potential%20for%20visual%20planning%20but%0Astruggle%20with%20precise%20spatial%20and%20long-horizon%20reasoning.%20In%20contrast%2C%20Planning%0ADomain%20Definition%20Language%20%28PDDL%29%20planners%20excel%20at%20long-horizon%20formal%0Aplanning%2C%20but%20cannot%20interpret%20visual%20inputs.%20Recent%20works%20combine%20these%0Acomplementary%20advantages%20by%20enabling%20VLMs%20to%20turn%20visual%20planning%20problems%20into%0APDDL%20files%20for%20formal%20planning.%20However%2C%20while%20VLMs%20can%20generate%20PDDL%20problem%0Afiles%20satisfactorily%2C%20they%20struggle%20to%20accurately%20generate%20the%20PDDL%20domain%0Afiles%2C%20which%20describe%20all%20the%20planning%20rules.%20As%20a%20result%2C%20prior%20methods%20rely%0Aon%20human%20experts%20to%20predefine%20domain%20files%20or%20on%20constant%20environment%20access%0Afor%20refinement.%20We%20propose%20VLMFP%2C%20a%20Dual-VLM-guided%20framework%20that%20can%0Aautonomously%20generate%20both%20PDDL%20problem%20and%20domain%20files%20for%20formal%20visual%0Aplanning.%20VLMFP%20introduces%20two%20VLMs%20to%20ensure%20reliable%20PDDL%20file%20generation%3A%20A%0ASimVLM%20that%20simulates%20action%20consequences%20based%20on%20input%20rule%20descriptions%2C%20and%0Aa%20GenVLM%20that%20generates%20and%20iteratively%20refines%20PDDL%20files%20by%20comparing%20the%0APDDL%20and%20SimVLM%20execution%20results.%20VLMFP%20unleashes%20multiple%20levels%20of%0Ageneralizability%3A%20The%20same%20generated%20PDDL%20domain%20file%20works%20for%20all%20the%0Adifferent%20instances%20under%20the%20same%20problem%2C%20and%20VLMs%20generalize%20to%20different%0Aproblems%20with%20varied%20appearances%20and%20rules.%20We%20evaluate%20VLMFP%20with%206%20grid-world%0Adomains%20and%20test%20its%20generalization%20to%20unseen%20instances%2C%20appearance%2C%20and%20game%0Arules.%20On%20average%2C%20SimVLM%20accurately%20describes%2095.5%25%2C%2082.6%25%20of%20scenarios%2C%0Asimulates%2085.5%25%2C%2087.8%25%20of%20action%20sequence%2C%20and%20judges%2082.4%25%2C%2085.6%25%20goal%0Areaching%20for%20seen%20and%20unseen%20appearances%2C%20respectively.%20With%20the%20guidance%20of%0ASimVLM%2C%20VLMFP%20can%20generate%20PDDL%20files%20to%20reach%2070.0%25%2C%2054.1%25%20valid%20plans%20for%0Aunseen%20instances%20in%20seen%20and%20unseen%20appearances%2C%20respectively.%20Project%20page%3A%0Ahttps%3A//sites.google.com/view/vlmfp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03182v1&entry.124074799=Read"},
{"title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via\n  Text-to-Image-to-Text Consistency", "author": "Juntong Wang and Huiyu Duan and Jiarui Wang and Ziheng Jia and Guangtao Zhai and Xiongkuo Min", "abstract": "  With the rapid advancement of large multimodal models (LMMs), recent\ntext-to-image (T2I) models can generate high-quality images and demonstrate\ngreat alignment to short prompts. However, they still struggle to effectively\nunderstand and follow long and detailed prompts, displaying inconsistent\ngeneration. To address this challenge, we introduce LPG-Bench, a comprehensive\nbenchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench\nfeatures 200 meticulously crafted prompts with an average length of over 250\nwords, approaching the input capacity of several leading commercial models.\nUsing these prompts, we generate 2,600 images from 13 state-of-the-art models\nand further perform comprehensive human-ranked annotations. Based on LPG-Bench,\nwe observe that state-of-the-art T2I alignment evaluation metrics exhibit poor\nconsistency with human preferences on long-prompt-based image generation. To\naddress the gap, we introduce a novel zero-shot metric based on\ntext-to-image-to-text consistency, termed TIT, for evaluating\nlong-prompt-generated images. The core concept of TIT is to quantify T2I\nalignment by directly comparing the consistency between the raw prompt and the\nLMM-produced description on the generated image, which includes an efficient\nscore-based instantiation TIT-Score and a large-language-model (LLM) based\ninstantiation TIT-Score-LLM. Extensive experiments demonstrate that our\nframework achieves superior alignment with human judgment compared to\nCLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute\nimprovement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT\nmethods together offer a deeper perspective to benchmark and foster the\ndevelopment of T2I models. All resources will be made publicly available.\n", "link": "http://arxiv.org/abs/2510.02987v1", "date": "2025-10-03", "relevancy": 2.1175, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5612}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5087}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIT-Score%3A%20Evaluating%20Long-Prompt%20Based%20Text-to-Image%20Alignment%20via%0A%20%20Text-to-Image-to-Text%20Consistency&body=Title%3A%20TIT-Score%3A%20Evaluating%20Long-Prompt%20Based%20Text-to-Image%20Alignment%20via%0A%20%20Text-to-Image-to-Text%20Consistency%0AAuthor%3A%20Juntong%20Wang%20and%20Huiyu%20Duan%20and%20Jiarui%20Wang%20and%20Ziheng%20Jia%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20large%20multimodal%20models%20%28LMMs%29%2C%20recent%0Atext-to-image%20%28T2I%29%20models%20can%20generate%20high-quality%20images%20and%20demonstrate%0Agreat%20alignment%20to%20short%20prompts.%20However%2C%20they%20still%20struggle%20to%20effectively%0Aunderstand%20and%20follow%20long%20and%20detailed%20prompts%2C%20displaying%20inconsistent%0Ageneration.%20To%20address%20this%20challenge%2C%20we%20introduce%20LPG-Bench%2C%20a%20comprehensive%0Abenchmark%20for%20evaluating%20long-prompt-based%20text-to-image%20generation.%20LPG-Bench%0Afeatures%20200%20meticulously%20crafted%20prompts%20with%20an%20average%20length%20of%20over%20250%0Awords%2C%20approaching%20the%20input%20capacity%20of%20several%20leading%20commercial%20models.%0AUsing%20these%20prompts%2C%20we%20generate%202%2C600%20images%20from%2013%20state-of-the-art%20models%0Aand%20further%20perform%20comprehensive%20human-ranked%20annotations.%20Based%20on%20LPG-Bench%2C%0Awe%20observe%20that%20state-of-the-art%20T2I%20alignment%20evaluation%20metrics%20exhibit%20poor%0Aconsistency%20with%20human%20preferences%20on%20long-prompt-based%20image%20generation.%20To%0Aaddress%20the%20gap%2C%20we%20introduce%20a%20novel%20zero-shot%20metric%20based%20on%0Atext-to-image-to-text%20consistency%2C%20termed%20TIT%2C%20for%20evaluating%0Along-prompt-generated%20images.%20The%20core%20concept%20of%20TIT%20is%20to%20quantify%20T2I%0Aalignment%20by%20directly%20comparing%20the%20consistency%20between%20the%20raw%20prompt%20and%20the%0ALMM-produced%20description%20on%20the%20generated%20image%2C%20which%20includes%20an%20efficient%0Ascore-based%20instantiation%20TIT-Score%20and%20a%20large-language-model%20%28LLM%29%20based%0Ainstantiation%20TIT-Score-LLM.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20achieves%20superior%20alignment%20with%20human%20judgment%20compared%20to%0ACLIP-score%2C%20LMM-score%2C%20etc.%2C%20with%20TIT-Score-LLM%20attaining%20a%207.31%25%20absolute%0Aimprovement%20in%20pairwise%20accuracy%20over%20the%20strongest%20baseline.%20LPG-Bench%20and%20TIT%0Amethods%20together%20offer%20a%20deeper%20perspective%20to%20benchmark%20and%20foster%20the%0Adevelopment%20of%20T2I%20models.%20All%20resources%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIT-Score%253A%2520Evaluating%2520Long-Prompt%2520Based%2520Text-to-Image%2520Alignment%2520via%250A%2520%2520Text-to-Image-to-Text%2520Consistency%26entry.906535625%3DJuntong%2520Wang%2520and%2520Huiyu%2520Duan%2520and%2520Jiarui%2520Wang%2520and%2520Ziheng%2520Jia%2520and%2520Guangtao%2520Zhai%2520and%2520Xiongkuo%2520Min%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520large%2520multimodal%2520models%2520%2528LMMs%2529%252C%2520recent%250Atext-to-image%2520%2528T2I%2529%2520models%2520can%2520generate%2520high-quality%2520images%2520and%2520demonstrate%250Agreat%2520alignment%2520to%2520short%2520prompts.%2520However%252C%2520they%2520still%2520struggle%2520to%2520effectively%250Aunderstand%2520and%2520follow%2520long%2520and%2520detailed%2520prompts%252C%2520displaying%2520inconsistent%250Ageneration.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520LPG-Bench%252C%2520a%2520comprehensive%250Abenchmark%2520for%2520evaluating%2520long-prompt-based%2520text-to-image%2520generation.%2520LPG-Bench%250Afeatures%2520200%2520meticulously%2520crafted%2520prompts%2520with%2520an%2520average%2520length%2520of%2520over%2520250%250Awords%252C%2520approaching%2520the%2520input%2520capacity%2520of%2520several%2520leading%2520commercial%2520models.%250AUsing%2520these%2520prompts%252C%2520we%2520generate%25202%252C600%2520images%2520from%252013%2520state-of-the-art%2520models%250Aand%2520further%2520perform%2520comprehensive%2520human-ranked%2520annotations.%2520Based%2520on%2520LPG-Bench%252C%250Awe%2520observe%2520that%2520state-of-the-art%2520T2I%2520alignment%2520evaluation%2520metrics%2520exhibit%2520poor%250Aconsistency%2520with%2520human%2520preferences%2520on%2520long-prompt-based%2520image%2520generation.%2520To%250Aaddress%2520the%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520zero-shot%2520metric%2520based%2520on%250Atext-to-image-to-text%2520consistency%252C%2520termed%2520TIT%252C%2520for%2520evaluating%250Along-prompt-generated%2520images.%2520The%2520core%2520concept%2520of%2520TIT%2520is%2520to%2520quantify%2520T2I%250Aalignment%2520by%2520directly%2520comparing%2520the%2520consistency%2520between%2520the%2520raw%2520prompt%2520and%2520the%250ALMM-produced%2520description%2520on%2520the%2520generated%2520image%252C%2520which%2520includes%2520an%2520efficient%250Ascore-based%2520instantiation%2520TIT-Score%2520and%2520a%2520large-language-model%2520%2528LLM%2529%2520based%250Ainstantiation%2520TIT-Score-LLM.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aframework%2520achieves%2520superior%2520alignment%2520with%2520human%2520judgment%2520compared%2520to%250ACLIP-score%252C%2520LMM-score%252C%2520etc.%252C%2520with%2520TIT-Score-LLM%2520attaining%2520a%25207.31%2525%2520absolute%250Aimprovement%2520in%2520pairwise%2520accuracy%2520over%2520the%2520strongest%2520baseline.%2520LPG-Bench%2520and%2520TIT%250Amethods%2520together%2520offer%2520a%2520deeper%2520perspective%2520to%2520benchmark%2520and%2520foster%2520the%250Adevelopment%2520of%2520T2I%2520models.%2520All%2520resources%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIT-Score%3A%20Evaluating%20Long-Prompt%20Based%20Text-to-Image%20Alignment%20via%0A%20%20Text-to-Image-to-Text%20Consistency&entry.906535625=Juntong%20Wang%20and%20Huiyu%20Duan%20and%20Jiarui%20Wang%20and%20Ziheng%20Jia%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20large%20multimodal%20models%20%28LMMs%29%2C%20recent%0Atext-to-image%20%28T2I%29%20models%20can%20generate%20high-quality%20images%20and%20demonstrate%0Agreat%20alignment%20to%20short%20prompts.%20However%2C%20they%20still%20struggle%20to%20effectively%0Aunderstand%20and%20follow%20long%20and%20detailed%20prompts%2C%20displaying%20inconsistent%0Ageneration.%20To%20address%20this%20challenge%2C%20we%20introduce%20LPG-Bench%2C%20a%20comprehensive%0Abenchmark%20for%20evaluating%20long-prompt-based%20text-to-image%20generation.%20LPG-Bench%0Afeatures%20200%20meticulously%20crafted%20prompts%20with%20an%20average%20length%20of%20over%20250%0Awords%2C%20approaching%20the%20input%20capacity%20of%20several%20leading%20commercial%20models.%0AUsing%20these%20prompts%2C%20we%20generate%202%2C600%20images%20from%2013%20state-of-the-art%20models%0Aand%20further%20perform%20comprehensive%20human-ranked%20annotations.%20Based%20on%20LPG-Bench%2C%0Awe%20observe%20that%20state-of-the-art%20T2I%20alignment%20evaluation%20metrics%20exhibit%20poor%0Aconsistency%20with%20human%20preferences%20on%20long-prompt-based%20image%20generation.%20To%0Aaddress%20the%20gap%2C%20we%20introduce%20a%20novel%20zero-shot%20metric%20based%20on%0Atext-to-image-to-text%20consistency%2C%20termed%20TIT%2C%20for%20evaluating%0Along-prompt-generated%20images.%20The%20core%20concept%20of%20TIT%20is%20to%20quantify%20T2I%0Aalignment%20by%20directly%20comparing%20the%20consistency%20between%20the%20raw%20prompt%20and%20the%0ALMM-produced%20description%20on%20the%20generated%20image%2C%20which%20includes%20an%20efficient%0Ascore-based%20instantiation%20TIT-Score%20and%20a%20large-language-model%20%28LLM%29%20based%0Ainstantiation%20TIT-Score-LLM.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20achieves%20superior%20alignment%20with%20human%20judgment%20compared%20to%0ACLIP-score%2C%20LMM-score%2C%20etc.%2C%20with%20TIT-Score-LLM%20attaining%20a%207.31%25%20absolute%0Aimprovement%20in%20pairwise%20accuracy%20over%20the%20strongest%20baseline.%20LPG-Bench%20and%20TIT%0Amethods%20together%20offer%20a%20deeper%20perspective%20to%20benchmark%20and%20foster%20the%0Adevelopment%20of%20T2I%20models.%20All%20resources%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02987v1&entry.124074799=Read"},
{"title": "ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal\n  Trajectories", "author": "Anantajit Subrahmanya and Chandrakanth Gudavalli and Connor Levenson and Umang Garg and B. S. Manjunath", "abstract": "  Accurately modeling human mobility is critical for urban planning,\nepidemiology, and traffic management. In this work, we introduce Markovian Reeb\nGraphs, a novel framework for simulating spatiotemporal trajectories that\npreserve Patterns of Life (PoLs) learned from baseline data. By combining\nindividual- and population-level mobility structures within a probabilistic\ntopological model, our approach generates realistic future trajectories that\ncapture both consistency and variability in daily life. Evaluations on the\nUrban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon\nDivergence (JSD) across population- and agent-level metrics demonstrate that\nthe proposed method achieves strong fidelity while remaining data- and\ncompute-efficient. These results position Markovian Reeb Graphs as a scalable\nframework for trajectory simulation with broad applicability across diverse\nurban environments.\n", "link": "http://arxiv.org/abs/2510.03152v1", "date": "2025-10-03", "relevancy": 2.1101, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.536}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5342}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReeMark%3A%20Reeb%20Graphs%20for%20Simulating%20Patterns%20of%20Life%20in%20Spatiotemporal%0A%20%20Trajectories&body=Title%3A%20ReeMark%3A%20Reeb%20Graphs%20for%20Simulating%20Patterns%20of%20Life%20in%20Spatiotemporal%0A%20%20Trajectories%0AAuthor%3A%20Anantajit%20Subrahmanya%20and%20Chandrakanth%20Gudavalli%20and%20Connor%20Levenson%20and%20Umang%20Garg%20and%20B.%20S.%20Manjunath%0AAbstract%3A%20%20%20Accurately%20modeling%20human%20mobility%20is%20critical%20for%20urban%20planning%2C%0Aepidemiology%2C%20and%20traffic%20management.%20In%20this%20work%2C%20we%20introduce%20Markovian%20Reeb%0AGraphs%2C%20a%20novel%20framework%20for%20simulating%20spatiotemporal%20trajectories%20that%0Apreserve%20Patterns%20of%20Life%20%28PoLs%29%20learned%20from%20baseline%20data.%20By%20combining%0Aindividual-%20and%20population-level%20mobility%20structures%20within%20a%20probabilistic%0Atopological%20model%2C%20our%20approach%20generates%20realistic%20future%20trajectories%20that%0Acapture%20both%20consistency%20and%20variability%20in%20daily%20life.%20Evaluations%20on%20the%0AUrban%20Anomalies%20dataset%20%28Atlanta%20and%20Berlin%20subsets%29%20using%20the%20Jensen-Shannon%0ADivergence%20%28JSD%29%20across%20population-%20and%20agent-level%20metrics%20demonstrate%20that%0Athe%20proposed%20method%20achieves%20strong%20fidelity%20while%20remaining%20data-%20and%0Acompute-efficient.%20These%20results%20position%20Markovian%20Reeb%20Graphs%20as%20a%20scalable%0Aframework%20for%20trajectory%20simulation%20with%20broad%20applicability%20across%20diverse%0Aurban%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReeMark%253A%2520Reeb%2520Graphs%2520for%2520Simulating%2520Patterns%2520of%2520Life%2520in%2520Spatiotemporal%250A%2520%2520Trajectories%26entry.906535625%3DAnantajit%2520Subrahmanya%2520and%2520Chandrakanth%2520Gudavalli%2520and%2520Connor%2520Levenson%2520and%2520Umang%2520Garg%2520and%2520B.%2520S.%2520Manjunath%26entry.1292438233%3D%2520%2520Accurately%2520modeling%2520human%2520mobility%2520is%2520critical%2520for%2520urban%2520planning%252C%250Aepidemiology%252C%2520and%2520traffic%2520management.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Markovian%2520Reeb%250AGraphs%252C%2520a%2520novel%2520framework%2520for%2520simulating%2520spatiotemporal%2520trajectories%2520that%250Apreserve%2520Patterns%2520of%2520Life%2520%2528PoLs%2529%2520learned%2520from%2520baseline%2520data.%2520By%2520combining%250Aindividual-%2520and%2520population-level%2520mobility%2520structures%2520within%2520a%2520probabilistic%250Atopological%2520model%252C%2520our%2520approach%2520generates%2520realistic%2520future%2520trajectories%2520that%250Acapture%2520both%2520consistency%2520and%2520variability%2520in%2520daily%2520life.%2520Evaluations%2520on%2520the%250AUrban%2520Anomalies%2520dataset%2520%2528Atlanta%2520and%2520Berlin%2520subsets%2529%2520using%2520the%2520Jensen-Shannon%250ADivergence%2520%2528JSD%2529%2520across%2520population-%2520and%2520agent-level%2520metrics%2520demonstrate%2520that%250Athe%2520proposed%2520method%2520achieves%2520strong%2520fidelity%2520while%2520remaining%2520data-%2520and%250Acompute-efficient.%2520These%2520results%2520position%2520Markovian%2520Reeb%2520Graphs%2520as%2520a%2520scalable%250Aframework%2520for%2520trajectory%2520simulation%2520with%2520broad%2520applicability%2520across%2520diverse%250Aurban%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReeMark%3A%20Reeb%20Graphs%20for%20Simulating%20Patterns%20of%20Life%20in%20Spatiotemporal%0A%20%20Trajectories&entry.906535625=Anantajit%20Subrahmanya%20and%20Chandrakanth%20Gudavalli%20and%20Connor%20Levenson%20and%20Umang%20Garg%20and%20B.%20S.%20Manjunath&entry.1292438233=%20%20Accurately%20modeling%20human%20mobility%20is%20critical%20for%20urban%20planning%2C%0Aepidemiology%2C%20and%20traffic%20management.%20In%20this%20work%2C%20we%20introduce%20Markovian%20Reeb%0AGraphs%2C%20a%20novel%20framework%20for%20simulating%20spatiotemporal%20trajectories%20that%0Apreserve%20Patterns%20of%20Life%20%28PoLs%29%20learned%20from%20baseline%20data.%20By%20combining%0Aindividual-%20and%20population-level%20mobility%20structures%20within%20a%20probabilistic%0Atopological%20model%2C%20our%20approach%20generates%20realistic%20future%20trajectories%20that%0Acapture%20both%20consistency%20and%20variability%20in%20daily%20life.%20Evaluations%20on%20the%0AUrban%20Anomalies%20dataset%20%28Atlanta%20and%20Berlin%20subsets%29%20using%20the%20Jensen-Shannon%0ADivergence%20%28JSD%29%20across%20population-%20and%20agent-level%20metrics%20demonstrate%20that%0Athe%20proposed%20method%20achieves%20strong%20fidelity%20while%20remaining%20data-%20and%0Acompute-efficient.%20These%20results%20position%20Markovian%20Reeb%20Graphs%20as%20a%20scalable%0Aframework%20for%20trajectory%20simulation%20with%20broad%20applicability%20across%20diverse%0Aurban%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03152v1&entry.124074799=Read"},
{"title": "Efficient Preimage Approximation for Neural Network Certification", "author": "Anton Bj\u00f6rklund and Mykola Zaitsev and Marta Kwiatkowska", "abstract": "  The growing reliance on artificial intelligence in safety- and\nsecurity-critical applications demands effective neural network certification.\nA challenging real-world use case is \"patch attacks\", where adversarial patches\nor lighting conditions obscure parts of images, for example, traffic signs. A\nsignificant step towards certification against patch attacks was recently\nachieved using PREMAP, which uses under- and over-approximations of the\npreimage, the set of inputs that lead to a specified output, for the\ncertification. While the PREMAP approach is versatile, it is currently limited\nto fully-connected neural networks of moderate dimensionality. In order to\ntackle broader real-world use cases, we present novel algorithmic extensions to\nPREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved\nbranching heuristics. Firstly, we demonstrate that these efficiency\nimprovements significantly outperform the original PREMAP and enable scaling to\nconvolutional neural networks that were previously intractable. Secondly, we\nshowcase the potential of preimage approximation methodology for analysing and\ncertifying reliability and robustness on a range of use cases from computer\nvision and control.\n", "link": "http://arxiv.org/abs/2505.22798v2", "date": "2025-10-03", "relevancy": 2.1091, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5623}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5072}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Preimage%20Approximation%20for%20Neural%20Network%20Certification&body=Title%3A%20Efficient%20Preimage%20Approximation%20for%20Neural%20Network%20Certification%0AAuthor%3A%20Anton%20Bj%C3%B6rklund%20and%20Mykola%20Zaitsev%20and%20Marta%20Kwiatkowska%0AAbstract%3A%20%20%20The%20growing%20reliance%20on%20artificial%20intelligence%20in%20safety-%20and%0Asecurity-critical%20applications%20demands%20effective%20neural%20network%20certification.%0AA%20challenging%20real-world%20use%20case%20is%20%22patch%20attacks%22%2C%20where%20adversarial%20patches%0Aor%20lighting%20conditions%20obscure%20parts%20of%20images%2C%20for%20example%2C%20traffic%20signs.%20A%0Asignificant%20step%20towards%20certification%20against%20patch%20attacks%20was%20recently%0Aachieved%20using%20PREMAP%2C%20which%20uses%20under-%20and%20over-approximations%20of%20the%0Apreimage%2C%20the%20set%20of%20inputs%20that%20lead%20to%20a%20specified%20output%2C%20for%20the%0Acertification.%20While%20the%20PREMAP%20approach%20is%20versatile%2C%20it%20is%20currently%20limited%0Ato%20fully-connected%20neural%20networks%20of%20moderate%20dimensionality.%20In%20order%20to%0Atackle%20broader%20real-world%20use%20cases%2C%20we%20present%20novel%20algorithmic%20extensions%20to%0APREMAP%20involving%20tighter%20bounds%2C%20adaptive%20Monte%20Carlo%20sampling%2C%20and%20improved%0Abranching%20heuristics.%20Firstly%2C%20we%20demonstrate%20that%20these%20efficiency%0Aimprovements%20significantly%20outperform%20the%20original%20PREMAP%20and%20enable%20scaling%20to%0Aconvolutional%20neural%20networks%20that%20were%20previously%20intractable.%20Secondly%2C%20we%0Ashowcase%20the%20potential%20of%20preimage%20approximation%20methodology%20for%20analysing%20and%0Acertifying%20reliability%20and%20robustness%20on%20a%20range%20of%20use%20cases%20from%20computer%0Avision%20and%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22798v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Preimage%2520Approximation%2520for%2520Neural%2520Network%2520Certification%26entry.906535625%3DAnton%2520Bj%25C3%25B6rklund%2520and%2520Mykola%2520Zaitsev%2520and%2520Marta%2520Kwiatkowska%26entry.1292438233%3D%2520%2520The%2520growing%2520reliance%2520on%2520artificial%2520intelligence%2520in%2520safety-%2520and%250Asecurity-critical%2520applications%2520demands%2520effective%2520neural%2520network%2520certification.%250AA%2520challenging%2520real-world%2520use%2520case%2520is%2520%2522patch%2520attacks%2522%252C%2520where%2520adversarial%2520patches%250Aor%2520lighting%2520conditions%2520obscure%2520parts%2520of%2520images%252C%2520for%2520example%252C%2520traffic%2520signs.%2520A%250Asignificant%2520step%2520towards%2520certification%2520against%2520patch%2520attacks%2520was%2520recently%250Aachieved%2520using%2520PREMAP%252C%2520which%2520uses%2520under-%2520and%2520over-approximations%2520of%2520the%250Apreimage%252C%2520the%2520set%2520of%2520inputs%2520that%2520lead%2520to%2520a%2520specified%2520output%252C%2520for%2520the%250Acertification.%2520While%2520the%2520PREMAP%2520approach%2520is%2520versatile%252C%2520it%2520is%2520currently%2520limited%250Ato%2520fully-connected%2520neural%2520networks%2520of%2520moderate%2520dimensionality.%2520In%2520order%2520to%250Atackle%2520broader%2520real-world%2520use%2520cases%252C%2520we%2520present%2520novel%2520algorithmic%2520extensions%2520to%250APREMAP%2520involving%2520tighter%2520bounds%252C%2520adaptive%2520Monte%2520Carlo%2520sampling%252C%2520and%2520improved%250Abranching%2520heuristics.%2520Firstly%252C%2520we%2520demonstrate%2520that%2520these%2520efficiency%250Aimprovements%2520significantly%2520outperform%2520the%2520original%2520PREMAP%2520and%2520enable%2520scaling%2520to%250Aconvolutional%2520neural%2520networks%2520that%2520were%2520previously%2520intractable.%2520Secondly%252C%2520we%250Ashowcase%2520the%2520potential%2520of%2520preimage%2520approximation%2520methodology%2520for%2520analysing%2520and%250Acertifying%2520reliability%2520and%2520robustness%2520on%2520a%2520range%2520of%2520use%2520cases%2520from%2520computer%250Avision%2520and%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22798v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Preimage%20Approximation%20for%20Neural%20Network%20Certification&entry.906535625=Anton%20Bj%C3%B6rklund%20and%20Mykola%20Zaitsev%20and%20Marta%20Kwiatkowska&entry.1292438233=%20%20The%20growing%20reliance%20on%20artificial%20intelligence%20in%20safety-%20and%0Asecurity-critical%20applications%20demands%20effective%20neural%20network%20certification.%0AA%20challenging%20real-world%20use%20case%20is%20%22patch%20attacks%22%2C%20where%20adversarial%20patches%0Aor%20lighting%20conditions%20obscure%20parts%20of%20images%2C%20for%20example%2C%20traffic%20signs.%20A%0Asignificant%20step%20towards%20certification%20against%20patch%20attacks%20was%20recently%0Aachieved%20using%20PREMAP%2C%20which%20uses%20under-%20and%20over-approximations%20of%20the%0Apreimage%2C%20the%20set%20of%20inputs%20that%20lead%20to%20a%20specified%20output%2C%20for%20the%0Acertification.%20While%20the%20PREMAP%20approach%20is%20versatile%2C%20it%20is%20currently%20limited%0Ato%20fully-connected%20neural%20networks%20of%20moderate%20dimensionality.%20In%20order%20to%0Atackle%20broader%20real-world%20use%20cases%2C%20we%20present%20novel%20algorithmic%20extensions%20to%0APREMAP%20involving%20tighter%20bounds%2C%20adaptive%20Monte%20Carlo%20sampling%2C%20and%20improved%0Abranching%20heuristics.%20Firstly%2C%20we%20demonstrate%20that%20these%20efficiency%0Aimprovements%20significantly%20outperform%20the%20original%20PREMAP%20and%20enable%20scaling%20to%0Aconvolutional%20neural%20networks%20that%20were%20previously%20intractable.%20Secondly%2C%20we%0Ashowcase%20the%20potential%20of%20preimage%20approximation%20methodology%20for%20analysing%20and%0Acertifying%20reliability%20and%20robustness%20on%20a%20range%20of%20use%20cases%20from%20computer%0Avision%20and%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22798v2&entry.124074799=Read"},
{"title": "InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust\n  Multi-Class Facial Emotion Recognition", "author": "Ahsan Farabi and Israt Khandaker and Ibrahim Khalil Shanto and Md Abdul Ahad Minhaz and Tanisha Zaman", "abstract": "  Facial Emotion Recognition (FER) is a key task in affective computing,\nenabling applications in human-computer interaction, e-learning, healthcare,\nand safety systems. Despite advances in deep learning, FER remains challenging\ndue to occlusions, illumination and pose variations, subtle intra-class\ndifferences, and dataset imbalance that hinders recognition of minority\nemotions. We present InsideOut, a reproducible FER framework built on\nEfficientNetV2-S with transfer learning, strong data augmentation, and\nimbalance-aware optimization. The approach standardizes FER2013 images, applies\nstratified splitting and augmentation, and fine-tunes a lightweight\nclassification head with class-weighted loss to address skewed distributions.\nInsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,\nshowing competitive results compared to conventional CNN baselines. The novelty\nlies in demonstrating that efficient architectures, combined with tailored\nimbalance handling, can provide practical, transparent, and reproducible FER\nsolutions.\n", "link": "http://arxiv.org/abs/2510.03066v1", "date": "2025-10-03", "relevancy": 2.1057, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5329}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5275}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InsideOut%3A%20An%20EfficientNetV2-S%20Based%20Deep%20Learning%20Framework%20for%20Robust%0A%20%20Multi-Class%20Facial%20Emotion%20Recognition&body=Title%3A%20InsideOut%3A%20An%20EfficientNetV2-S%20Based%20Deep%20Learning%20Framework%20for%20Robust%0A%20%20Multi-Class%20Facial%20Emotion%20Recognition%0AAuthor%3A%20Ahsan%20Farabi%20and%20Israt%20Khandaker%20and%20Ibrahim%20Khalil%20Shanto%20and%20Md%20Abdul%20Ahad%20Minhaz%20and%20Tanisha%20Zaman%0AAbstract%3A%20%20%20Facial%20Emotion%20Recognition%20%28FER%29%20is%20a%20key%20task%20in%20affective%20computing%2C%0Aenabling%20applications%20in%20human-computer%20interaction%2C%20e-learning%2C%20healthcare%2C%0Aand%20safety%20systems.%20Despite%20advances%20in%20deep%20learning%2C%20FER%20remains%20challenging%0Adue%20to%20occlusions%2C%20illumination%20and%20pose%20variations%2C%20subtle%20intra-class%0Adifferences%2C%20and%20dataset%20imbalance%20that%20hinders%20recognition%20of%20minority%0Aemotions.%20We%20present%20InsideOut%2C%20a%20reproducible%20FER%20framework%20built%20on%0AEfficientNetV2-S%20with%20transfer%20learning%2C%20strong%20data%20augmentation%2C%20and%0Aimbalance-aware%20optimization.%20The%20approach%20standardizes%20FER2013%20images%2C%20applies%0Astratified%20splitting%20and%20augmentation%2C%20and%20fine-tunes%20a%20lightweight%0Aclassification%20head%20with%20class-weighted%20loss%20to%20address%20skewed%20distributions.%0AInsideOut%20achieves%2062.8%25%20accuracy%20with%20a%20macro%20averaged%20F1%20of%200.590%20on%20FER2013%2C%0Ashowing%20competitive%20results%20compared%20to%20conventional%20CNN%20baselines.%20The%20novelty%0Alies%20in%20demonstrating%20that%20efficient%20architectures%2C%20combined%20with%20tailored%0Aimbalance%20handling%2C%20can%20provide%20practical%2C%20transparent%2C%20and%20reproducible%20FER%0Asolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsideOut%253A%2520An%2520EfficientNetV2-S%2520Based%2520Deep%2520Learning%2520Framework%2520for%2520Robust%250A%2520%2520Multi-Class%2520Facial%2520Emotion%2520Recognition%26entry.906535625%3DAhsan%2520Farabi%2520and%2520Israt%2520Khandaker%2520and%2520Ibrahim%2520Khalil%2520Shanto%2520and%2520Md%2520Abdul%2520Ahad%2520Minhaz%2520and%2520Tanisha%2520Zaman%26entry.1292438233%3D%2520%2520Facial%2520Emotion%2520Recognition%2520%2528FER%2529%2520is%2520a%2520key%2520task%2520in%2520affective%2520computing%252C%250Aenabling%2520applications%2520in%2520human-computer%2520interaction%252C%2520e-learning%252C%2520healthcare%252C%250Aand%2520safety%2520systems.%2520Despite%2520advances%2520in%2520deep%2520learning%252C%2520FER%2520remains%2520challenging%250Adue%2520to%2520occlusions%252C%2520illumination%2520and%2520pose%2520variations%252C%2520subtle%2520intra-class%250Adifferences%252C%2520and%2520dataset%2520imbalance%2520that%2520hinders%2520recognition%2520of%2520minority%250Aemotions.%2520We%2520present%2520InsideOut%252C%2520a%2520reproducible%2520FER%2520framework%2520built%2520on%250AEfficientNetV2-S%2520with%2520transfer%2520learning%252C%2520strong%2520data%2520augmentation%252C%2520and%250Aimbalance-aware%2520optimization.%2520The%2520approach%2520standardizes%2520FER2013%2520images%252C%2520applies%250Astratified%2520splitting%2520and%2520augmentation%252C%2520and%2520fine-tunes%2520a%2520lightweight%250Aclassification%2520head%2520with%2520class-weighted%2520loss%2520to%2520address%2520skewed%2520distributions.%250AInsideOut%2520achieves%252062.8%2525%2520accuracy%2520with%2520a%2520macro%2520averaged%2520F1%2520of%25200.590%2520on%2520FER2013%252C%250Ashowing%2520competitive%2520results%2520compared%2520to%2520conventional%2520CNN%2520baselines.%2520The%2520novelty%250Alies%2520in%2520demonstrating%2520that%2520efficient%2520architectures%252C%2520combined%2520with%2520tailored%250Aimbalance%2520handling%252C%2520can%2520provide%2520practical%252C%2520transparent%252C%2520and%2520reproducible%2520FER%250Asolutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InsideOut%3A%20An%20EfficientNetV2-S%20Based%20Deep%20Learning%20Framework%20for%20Robust%0A%20%20Multi-Class%20Facial%20Emotion%20Recognition&entry.906535625=Ahsan%20Farabi%20and%20Israt%20Khandaker%20and%20Ibrahim%20Khalil%20Shanto%20and%20Md%20Abdul%20Ahad%20Minhaz%20and%20Tanisha%20Zaman&entry.1292438233=%20%20Facial%20Emotion%20Recognition%20%28FER%29%20is%20a%20key%20task%20in%20affective%20computing%2C%0Aenabling%20applications%20in%20human-computer%20interaction%2C%20e-learning%2C%20healthcare%2C%0Aand%20safety%20systems.%20Despite%20advances%20in%20deep%20learning%2C%20FER%20remains%20challenging%0Adue%20to%20occlusions%2C%20illumination%20and%20pose%20variations%2C%20subtle%20intra-class%0Adifferences%2C%20and%20dataset%20imbalance%20that%20hinders%20recognition%20of%20minority%0Aemotions.%20We%20present%20InsideOut%2C%20a%20reproducible%20FER%20framework%20built%20on%0AEfficientNetV2-S%20with%20transfer%20learning%2C%20strong%20data%20augmentation%2C%20and%0Aimbalance-aware%20optimization.%20The%20approach%20standardizes%20FER2013%20images%2C%20applies%0Astratified%20splitting%20and%20augmentation%2C%20and%20fine-tunes%20a%20lightweight%0Aclassification%20head%20with%20class-weighted%20loss%20to%20address%20skewed%20distributions.%0AInsideOut%20achieves%2062.8%25%20accuracy%20with%20a%20macro%20averaged%20F1%20of%200.590%20on%20FER2013%2C%0Ashowing%20competitive%20results%20compared%20to%20conventional%20CNN%20baselines.%20The%20novelty%0Alies%20in%20demonstrating%20that%20efficient%20architectures%2C%20combined%20with%20tailored%0Aimbalance%20handling%2C%20can%20provide%20practical%2C%20transparent%2C%20and%20reproducible%20FER%0Asolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03066v1&entry.124074799=Read"},
{"title": "Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection", "author": "Boyu Han and Qianqian Xu and Shilong Bao and Zhiyong Yang and Sicong Li and Qingming Huang", "abstract": "  In this report, we address the problem of determining whether a user performs\nan action incorrectly from egocentric video data. To handle the challenges\nposed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted\nMixture-of-Experts (DR-MoE) framework. In the first stage, features are\nextracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are\ncombined through a feature-level expert module. In the second stage, three\nclassifiers are trained with different objectives: reweighted cross-entropy to\nmitigate class imbalance, AUC loss to improve ranking under skewed\ndistributions, and label-aware loss with sharpness-aware minimization to\nenhance calibration and generalization. Their predictions are fused using a\nclassification-level expert module. The proposed method achieves strong\nperformance, particularly in identifying rare and ambiguous mistake instances.\nThe code is available at https://github.com/boyuh/DR-MoE.\n", "link": "http://arxiv.org/abs/2509.12990v2", "date": "2025-10-03", "relevancy": 2.1034, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5666}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5383}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Stage%20Reweighted%20MoE%20for%20Long-Tailed%20Egocentric%20Mistake%20Detection&body=Title%3A%20Dual-Stage%20Reweighted%20MoE%20for%20Long-Tailed%20Egocentric%20Mistake%20Detection%0AAuthor%3A%20Boyu%20Han%20and%20Qianqian%20Xu%20and%20Shilong%20Bao%20and%20Zhiyong%20Yang%20and%20Sicong%20Li%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20address%20the%20problem%20of%20determining%20whether%20a%20user%20performs%0Aan%20action%20incorrectly%20from%20egocentric%20video%20data.%20To%20handle%20the%20challenges%0Aposed%20by%20subtle%20and%20infrequent%20mistakes%2C%20we%20propose%20a%20Dual-Stage%20Reweighted%0AMixture-of-Experts%20%28DR-MoE%29%20framework.%20In%20the%20first%20stage%2C%20features%20are%0Aextracted%20using%20a%20frozen%20ViViT%20model%20and%20a%20LoRA-tuned%20ViViT%20model%2C%20which%20are%0Acombined%20through%20a%20feature-level%20expert%20module.%20In%20the%20second%20stage%2C%20three%0Aclassifiers%20are%20trained%20with%20different%20objectives%3A%20reweighted%20cross-entropy%20to%0Amitigate%20class%20imbalance%2C%20AUC%20loss%20to%20improve%20ranking%20under%20skewed%0Adistributions%2C%20and%20label-aware%20loss%20with%20sharpness-aware%20minimization%20to%0Aenhance%20calibration%20and%20generalization.%20Their%20predictions%20are%20fused%20using%20a%0Aclassification-level%20expert%20module.%20The%20proposed%20method%20achieves%20strong%0Aperformance%2C%20particularly%20in%20identifying%20rare%20and%20ambiguous%20mistake%20instances.%0AThe%20code%20is%20available%20at%20https%3A//github.com/boyuh/DR-MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Stage%2520Reweighted%2520MoE%2520for%2520Long-Tailed%2520Egocentric%2520Mistake%2520Detection%26entry.906535625%3DBoyu%2520Han%2520and%2520Qianqian%2520Xu%2520and%2520Shilong%2520Bao%2520and%2520Zhiyong%2520Yang%2520and%2520Sicong%2520Li%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520address%2520the%2520problem%2520of%2520determining%2520whether%2520a%2520user%2520performs%250Aan%2520action%2520incorrectly%2520from%2520egocentric%2520video%2520data.%2520To%2520handle%2520the%2520challenges%250Aposed%2520by%2520subtle%2520and%2520infrequent%2520mistakes%252C%2520we%2520propose%2520a%2520Dual-Stage%2520Reweighted%250AMixture-of-Experts%2520%2528DR-MoE%2529%2520framework.%2520In%2520the%2520first%2520stage%252C%2520features%2520are%250Aextracted%2520using%2520a%2520frozen%2520ViViT%2520model%2520and%2520a%2520LoRA-tuned%2520ViViT%2520model%252C%2520which%2520are%250Acombined%2520through%2520a%2520feature-level%2520expert%2520module.%2520In%2520the%2520second%2520stage%252C%2520three%250Aclassifiers%2520are%2520trained%2520with%2520different%2520objectives%253A%2520reweighted%2520cross-entropy%2520to%250Amitigate%2520class%2520imbalance%252C%2520AUC%2520loss%2520to%2520improve%2520ranking%2520under%2520skewed%250Adistributions%252C%2520and%2520label-aware%2520loss%2520with%2520sharpness-aware%2520minimization%2520to%250Aenhance%2520calibration%2520and%2520generalization.%2520Their%2520predictions%2520are%2520fused%2520using%2520a%250Aclassification-level%2520expert%2520module.%2520The%2520proposed%2520method%2520achieves%2520strong%250Aperformance%252C%2520particularly%2520in%2520identifying%2520rare%2520and%2520ambiguous%2520mistake%2520instances.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/boyuh/DR-MoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Stage%20Reweighted%20MoE%20for%20Long-Tailed%20Egocentric%20Mistake%20Detection&entry.906535625=Boyu%20Han%20and%20Qianqian%20Xu%20and%20Shilong%20Bao%20and%20Zhiyong%20Yang%20and%20Sicong%20Li%20and%20Qingming%20Huang&entry.1292438233=%20%20In%20this%20report%2C%20we%20address%20the%20problem%20of%20determining%20whether%20a%20user%20performs%0Aan%20action%20incorrectly%20from%20egocentric%20video%20data.%20To%20handle%20the%20challenges%0Aposed%20by%20subtle%20and%20infrequent%20mistakes%2C%20we%20propose%20a%20Dual-Stage%20Reweighted%0AMixture-of-Experts%20%28DR-MoE%29%20framework.%20In%20the%20first%20stage%2C%20features%20are%0Aextracted%20using%20a%20frozen%20ViViT%20model%20and%20a%20LoRA-tuned%20ViViT%20model%2C%20which%20are%0Acombined%20through%20a%20feature-level%20expert%20module.%20In%20the%20second%20stage%2C%20three%0Aclassifiers%20are%20trained%20with%20different%20objectives%3A%20reweighted%20cross-entropy%20to%0Amitigate%20class%20imbalance%2C%20AUC%20loss%20to%20improve%20ranking%20under%20skewed%0Adistributions%2C%20and%20label-aware%20loss%20with%20sharpness-aware%20minimization%20to%0Aenhance%20calibration%20and%20generalization.%20Their%20predictions%20are%20fused%20using%20a%0Aclassification-level%20expert%20module.%20The%20proposed%20method%20achieves%20strong%0Aperformance%2C%20particularly%20in%20identifying%20rare%20and%20ambiguous%20mistake%20instances.%0AThe%20code%20is%20available%20at%20https%3A//github.com/boyuh/DR-MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12990v2&entry.124074799=Read"},
{"title": "Test-Time Defense Against Adversarial Attacks via Stochastic Resonance\n  of Latent Ensembles", "author": "Dong Lao and Yuxiang Zhang and Haniyeh Ehsani Oskouie and Yangchao Wu and Alex Wong and Stefano Soatto", "abstract": "  We propose a test-time defense mechanism against adversarial attacks:\nimperceptible image perturbations that significantly alter the predictions of a\nmodel. Unlike existing methods that rely on feature filtering or smoothing,\nwhich can lead to information loss, we propose to \"combat noise with noise\" by\nleveraging stochastic resonance to enhance robustness while minimizing\ninformation loss. Our approach introduces small translational perturbations to\nthe input image, aligns the transformed feature embeddings, and aggregates them\nbefore mapping back to the original reference image. This can be expressed in a\nclosed-form formula, which can be deployed on diverse existing network\narchitectures without introducing additional network modules or fine-tuning for\nspecific attack types. The resulting method is entirely training-free,\narchitecture-agnostic, and attack-agnostic. Empirical results show\nstate-of-the-art robustness on image classification and, for the first time,\nestablish a generic test-time defense for dense prediction tasks, including\nstereo matching and optical flow, highlighting the method's versatility and\npracticality. Specifically, relative to clean (unperturbed) performance, our\nmethod recovers up to 68.1% of the accuracy loss on image classification, 71.9%\non stereo matching, and 29.2% on optical flow under various types of\nadversarial attacks.\n", "link": "http://arxiv.org/abs/2510.03224v1", "date": "2025-10-03", "relevancy": 2.0954, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5366}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5235}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Defense%20Against%20Adversarial%20Attacks%20via%20Stochastic%20Resonance%0A%20%20of%20Latent%20Ensembles&body=Title%3A%20Test-Time%20Defense%20Against%20Adversarial%20Attacks%20via%20Stochastic%20Resonance%0A%20%20of%20Latent%20Ensembles%0AAuthor%3A%20Dong%20Lao%20and%20Yuxiang%20Zhang%20and%20Haniyeh%20Ehsani%20Oskouie%20and%20Yangchao%20Wu%20and%20Alex%20Wong%20and%20Stefano%20Soatto%0AAbstract%3A%20%20%20We%20propose%20a%20test-time%20defense%20mechanism%20against%20adversarial%20attacks%3A%0Aimperceptible%20image%20perturbations%20that%20significantly%20alter%20the%20predictions%20of%20a%0Amodel.%20Unlike%20existing%20methods%20that%20rely%20on%20feature%20filtering%20or%20smoothing%2C%0Awhich%20can%20lead%20to%20information%20loss%2C%20we%20propose%20to%20%22combat%20noise%20with%20noise%22%20by%0Aleveraging%20stochastic%20resonance%20to%20enhance%20robustness%20while%20minimizing%0Ainformation%20loss.%20Our%20approach%20introduces%20small%20translational%20perturbations%20to%0Athe%20input%20image%2C%20aligns%20the%20transformed%20feature%20embeddings%2C%20and%20aggregates%20them%0Abefore%20mapping%20back%20to%20the%20original%20reference%20image.%20This%20can%20be%20expressed%20in%20a%0Aclosed-form%20formula%2C%20which%20can%20be%20deployed%20on%20diverse%20existing%20network%0Aarchitectures%20without%20introducing%20additional%20network%20modules%20or%20fine-tuning%20for%0Aspecific%20attack%20types.%20The%20resulting%20method%20is%20entirely%20training-free%2C%0Aarchitecture-agnostic%2C%20and%20attack-agnostic.%20Empirical%20results%20show%0Astate-of-the-art%20robustness%20on%20image%20classification%20and%2C%20for%20the%20first%20time%2C%0Aestablish%20a%20generic%20test-time%20defense%20for%20dense%20prediction%20tasks%2C%20including%0Astereo%20matching%20and%20optical%20flow%2C%20highlighting%20the%20method%27s%20versatility%20and%0Apracticality.%20Specifically%2C%20relative%20to%20clean%20%28unperturbed%29%20performance%2C%20our%0Amethod%20recovers%20up%20to%2068.1%25%20of%20the%20accuracy%20loss%20on%20image%20classification%2C%2071.9%25%0Aon%20stereo%20matching%2C%20and%2029.2%25%20on%20optical%20flow%20under%20various%20types%20of%0Aadversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Defense%2520Against%2520Adversarial%2520Attacks%2520via%2520Stochastic%2520Resonance%250A%2520%2520of%2520Latent%2520Ensembles%26entry.906535625%3DDong%2520Lao%2520and%2520Yuxiang%2520Zhang%2520and%2520Haniyeh%2520Ehsani%2520Oskouie%2520and%2520Yangchao%2520Wu%2520and%2520Alex%2520Wong%2520and%2520Stefano%2520Soatto%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520test-time%2520defense%2520mechanism%2520against%2520adversarial%2520attacks%253A%250Aimperceptible%2520image%2520perturbations%2520that%2520significantly%2520alter%2520the%2520predictions%2520of%2520a%250Amodel.%2520Unlike%2520existing%2520methods%2520that%2520rely%2520on%2520feature%2520filtering%2520or%2520smoothing%252C%250Awhich%2520can%2520lead%2520to%2520information%2520loss%252C%2520we%2520propose%2520to%2520%2522combat%2520noise%2520with%2520noise%2522%2520by%250Aleveraging%2520stochastic%2520resonance%2520to%2520enhance%2520robustness%2520while%2520minimizing%250Ainformation%2520loss.%2520Our%2520approach%2520introduces%2520small%2520translational%2520perturbations%2520to%250Athe%2520input%2520image%252C%2520aligns%2520the%2520transformed%2520feature%2520embeddings%252C%2520and%2520aggregates%2520them%250Abefore%2520mapping%2520back%2520to%2520the%2520original%2520reference%2520image.%2520This%2520can%2520be%2520expressed%2520in%2520a%250Aclosed-form%2520formula%252C%2520which%2520can%2520be%2520deployed%2520on%2520diverse%2520existing%2520network%250Aarchitectures%2520without%2520introducing%2520additional%2520network%2520modules%2520or%2520fine-tuning%2520for%250Aspecific%2520attack%2520types.%2520The%2520resulting%2520method%2520is%2520entirely%2520training-free%252C%250Aarchitecture-agnostic%252C%2520and%2520attack-agnostic.%2520Empirical%2520results%2520show%250Astate-of-the-art%2520robustness%2520on%2520image%2520classification%2520and%252C%2520for%2520the%2520first%2520time%252C%250Aestablish%2520a%2520generic%2520test-time%2520defense%2520for%2520dense%2520prediction%2520tasks%252C%2520including%250Astereo%2520matching%2520and%2520optical%2520flow%252C%2520highlighting%2520the%2520method%2527s%2520versatility%2520and%250Apracticality.%2520Specifically%252C%2520relative%2520to%2520clean%2520%2528unperturbed%2529%2520performance%252C%2520our%250Amethod%2520recovers%2520up%2520to%252068.1%2525%2520of%2520the%2520accuracy%2520loss%2520on%2520image%2520classification%252C%252071.9%2525%250Aon%2520stereo%2520matching%252C%2520and%252029.2%2525%2520on%2520optical%2520flow%2520under%2520various%2520types%2520of%250Aadversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Defense%20Against%20Adversarial%20Attacks%20via%20Stochastic%20Resonance%0A%20%20of%20Latent%20Ensembles&entry.906535625=Dong%20Lao%20and%20Yuxiang%20Zhang%20and%20Haniyeh%20Ehsani%20Oskouie%20and%20Yangchao%20Wu%20and%20Alex%20Wong%20and%20Stefano%20Soatto&entry.1292438233=%20%20We%20propose%20a%20test-time%20defense%20mechanism%20against%20adversarial%20attacks%3A%0Aimperceptible%20image%20perturbations%20that%20significantly%20alter%20the%20predictions%20of%20a%0Amodel.%20Unlike%20existing%20methods%20that%20rely%20on%20feature%20filtering%20or%20smoothing%2C%0Awhich%20can%20lead%20to%20information%20loss%2C%20we%20propose%20to%20%22combat%20noise%20with%20noise%22%20by%0Aleveraging%20stochastic%20resonance%20to%20enhance%20robustness%20while%20minimizing%0Ainformation%20loss.%20Our%20approach%20introduces%20small%20translational%20perturbations%20to%0Athe%20input%20image%2C%20aligns%20the%20transformed%20feature%20embeddings%2C%20and%20aggregates%20them%0Abefore%20mapping%20back%20to%20the%20original%20reference%20image.%20This%20can%20be%20expressed%20in%20a%0Aclosed-form%20formula%2C%20which%20can%20be%20deployed%20on%20diverse%20existing%20network%0Aarchitectures%20without%20introducing%20additional%20network%20modules%20or%20fine-tuning%20for%0Aspecific%20attack%20types.%20The%20resulting%20method%20is%20entirely%20training-free%2C%0Aarchitecture-agnostic%2C%20and%20attack-agnostic.%20Empirical%20results%20show%0Astate-of-the-art%20robustness%20on%20image%20classification%20and%2C%20for%20the%20first%20time%2C%0Aestablish%20a%20generic%20test-time%20defense%20for%20dense%20prediction%20tasks%2C%20including%0Astereo%20matching%20and%20optical%20flow%2C%20highlighting%20the%20method%27s%20versatility%20and%0Apracticality.%20Specifically%2C%20relative%20to%20clean%20%28unperturbed%29%20performance%2C%20our%0Amethod%20recovers%20up%20to%2068.1%25%20of%20the%20accuracy%20loss%20on%20image%20classification%2C%2071.9%25%0Aon%20stereo%20matching%2C%20and%2029.2%25%20on%20optical%20flow%20under%20various%20types%20of%0Aadversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03224v1&entry.124074799=Read"},
{"title": "Equivariant Splitting: Self-supervised learning from incomplete data", "author": "Victor Sechaud and J\u00e9r\u00e9my Scanvic and Quentin Barth\u00e9lemy and Patrice Abry and Juli\u00e1n Tachella", "abstract": "  Self-supervised learning for inverse problems allows to train a\nreconstruction network from noise and/or incomplete data alone. These methods\nhave the potential of enabling learning-based solutions when obtaining\nground-truth references for training is expensive or even impossible. In this\npaper, we propose a new self-supervised learning strategy devised for the\nchallenging setting where measurements are observed via a single incomplete\nobservation model. We introduce a new definition of equivariance in the context\nof reconstruction networks, and show that the combination of self-supervised\nsplitting losses and equivariant reconstruction networks results in the same\nminimizer in expectation as the one of a supervised loss. Through a series of\nexperiments on image inpainting, accelerated magnetic resonance imaging, and\ncompressive sensing, we demonstrate that the proposed loss achieves\nstate-of-the-art performance in settings with highly rank-deficient forward\nmodels.\n", "link": "http://arxiv.org/abs/2510.00929v3", "date": "2025-10-03", "relevancy": 2.0934, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5647}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5007}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Splitting%3A%20Self-supervised%20learning%20from%20incomplete%20data&body=Title%3A%20Equivariant%20Splitting%3A%20Self-supervised%20learning%20from%20incomplete%20data%0AAuthor%3A%20Victor%20Sechaud%20and%20J%C3%A9r%C3%A9my%20Scanvic%20and%20Quentin%20Barth%C3%A9lemy%20and%20Patrice%20Abry%20and%20Juli%C3%A1n%20Tachella%0AAbstract%3A%20%20%20Self-supervised%20learning%20for%20inverse%20problems%20allows%20to%20train%20a%0Areconstruction%20network%20from%20noise%20and/or%20incomplete%20data%20alone.%20These%20methods%0Ahave%20the%20potential%20of%20enabling%20learning-based%20solutions%20when%20obtaining%0Aground-truth%20references%20for%20training%20is%20expensive%20or%20even%20impossible.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20self-supervised%20learning%20strategy%20devised%20for%20the%0Achallenging%20setting%20where%20measurements%20are%20observed%20via%20a%20single%20incomplete%0Aobservation%20model.%20We%20introduce%20a%20new%20definition%20of%20equivariance%20in%20the%20context%0Aof%20reconstruction%20networks%2C%20and%20show%20that%20the%20combination%20of%20self-supervised%0Asplitting%20losses%20and%20equivariant%20reconstruction%20networks%20results%20in%20the%20same%0Aminimizer%20in%20expectation%20as%20the%20one%20of%20a%20supervised%20loss.%20Through%20a%20series%20of%0Aexperiments%20on%20image%20inpainting%2C%20accelerated%20magnetic%20resonance%20imaging%2C%20and%0Acompressive%20sensing%2C%20we%20demonstrate%20that%20the%20proposed%20loss%20achieves%0Astate-of-the-art%20performance%20in%20settings%20with%20highly%20rank-deficient%20forward%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00929v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Splitting%253A%2520Self-supervised%2520learning%2520from%2520incomplete%2520data%26entry.906535625%3DVictor%2520Sechaud%2520and%2520J%25C3%25A9r%25C3%25A9my%2520Scanvic%2520and%2520Quentin%2520Barth%25C3%25A9lemy%2520and%2520Patrice%2520Abry%2520and%2520Juli%25C3%25A1n%2520Tachella%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520for%2520inverse%2520problems%2520allows%2520to%2520train%2520a%250Areconstruction%2520network%2520from%2520noise%2520and/or%2520incomplete%2520data%2520alone.%2520These%2520methods%250Ahave%2520the%2520potential%2520of%2520enabling%2520learning-based%2520solutions%2520when%2520obtaining%250Aground-truth%2520references%2520for%2520training%2520is%2520expensive%2520or%2520even%2520impossible.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520new%2520self-supervised%2520learning%2520strategy%2520devised%2520for%2520the%250Achallenging%2520setting%2520where%2520measurements%2520are%2520observed%2520via%2520a%2520single%2520incomplete%250Aobservation%2520model.%2520We%2520introduce%2520a%2520new%2520definition%2520of%2520equivariance%2520in%2520the%2520context%250Aof%2520reconstruction%2520networks%252C%2520and%2520show%2520that%2520the%2520combination%2520of%2520self-supervised%250Asplitting%2520losses%2520and%2520equivariant%2520reconstruction%2520networks%2520results%2520in%2520the%2520same%250Aminimizer%2520in%2520expectation%2520as%2520the%2520one%2520of%2520a%2520supervised%2520loss.%2520Through%2520a%2520series%2520of%250Aexperiments%2520on%2520image%2520inpainting%252C%2520accelerated%2520magnetic%2520resonance%2520imaging%252C%2520and%250Acompressive%2520sensing%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520loss%2520achieves%250Astate-of-the-art%2520performance%2520in%2520settings%2520with%2520highly%2520rank-deficient%2520forward%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00929v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Splitting%3A%20Self-supervised%20learning%20from%20incomplete%20data&entry.906535625=Victor%20Sechaud%20and%20J%C3%A9r%C3%A9my%20Scanvic%20and%20Quentin%20Barth%C3%A9lemy%20and%20Patrice%20Abry%20and%20Juli%C3%A1n%20Tachella&entry.1292438233=%20%20Self-supervised%20learning%20for%20inverse%20problems%20allows%20to%20train%20a%0Areconstruction%20network%20from%20noise%20and/or%20incomplete%20data%20alone.%20These%20methods%0Ahave%20the%20potential%20of%20enabling%20learning-based%20solutions%20when%20obtaining%0Aground-truth%20references%20for%20training%20is%20expensive%20or%20even%20impossible.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20self-supervised%20learning%20strategy%20devised%20for%20the%0Achallenging%20setting%20where%20measurements%20are%20observed%20via%20a%20single%20incomplete%0Aobservation%20model.%20We%20introduce%20a%20new%20definition%20of%20equivariance%20in%20the%20context%0Aof%20reconstruction%20networks%2C%20and%20show%20that%20the%20combination%20of%20self-supervised%0Asplitting%20losses%20and%20equivariant%20reconstruction%20networks%20results%20in%20the%20same%0Aminimizer%20in%20expectation%20as%20the%20one%20of%20a%20supervised%20loss.%20Through%20a%20series%20of%0Aexperiments%20on%20image%20inpainting%2C%20accelerated%20magnetic%20resonance%20imaging%2C%20and%0Acompressive%20sensing%2C%20we%20demonstrate%20that%20the%20proposed%20loss%20achieves%0Astate-of-the-art%20performance%20in%20settings%20with%20highly%20rank-deficient%20forward%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00929v3&entry.124074799=Read"},
{"title": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse\n  Autoencoders", "author": "Kriz Tahimic and Charibeth Cheng", "abstract": "  As Large Language Models become integral to software development, with\nsubstantial portions of AI-suggested code entering production, understanding\ntheir internal correctness mechanisms becomes critical for safe deployment. We\napply sparse autoencoders to decompose LLM representations, identifying\ndirections that correspond to code correctness. We select predictor directions\nusing t-statistics and steering directions through separation scores from base\nmodel representations, then analyze their mechanistic properties through\nsteering, attention analysis, and weight orthogonalization. We find that code\ncorrectness directions in LLMs reliably predict incorrect code, while\ncorrection capabilities, though statistically significant, involve tradeoffs\nbetween fixing errors and preserving correct code. Mechanistically, successful\ncode generation depends on attending to test cases rather than problem\ndescriptions. Moreover, directions identified in base models retain their\neffectiveness after instruction-tuning, suggesting code correctness mechanisms\nlearned during pre-training are repurposed during fine-tuning. Our mechanistic\ninsights suggest three practical applications: prompting strategies should\nprioritize test examples over elaborate problem descriptions, predictor\ndirections can serve as error alarms for developer review, and these same\npredictors can guide selective steering, intervening only when errors are\nanticipated to prevent the code corruption from constant steering.\n", "link": "http://arxiv.org/abs/2510.02917v1", "date": "2025-10-03", "relevancy": 2.0899, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanistic%20Interpretability%20of%20Code%20Correctness%20in%20LLMs%20via%20Sparse%0A%20%20Autoencoders&body=Title%3A%20Mechanistic%20Interpretability%20of%20Code%20Correctness%20in%20LLMs%20via%20Sparse%0A%20%20Autoencoders%0AAuthor%3A%20Kriz%20Tahimic%20and%20Charibeth%20Cheng%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20become%20integral%20to%20software%20development%2C%20with%0Asubstantial%20portions%20of%20AI-suggested%20code%20entering%20production%2C%20understanding%0Atheir%20internal%20correctness%20mechanisms%20becomes%20critical%20for%20safe%20deployment.%20We%0Aapply%20sparse%20autoencoders%20to%20decompose%20LLM%20representations%2C%20identifying%0Adirections%20that%20correspond%20to%20code%20correctness.%20We%20select%20predictor%20directions%0Ausing%20t-statistics%20and%20steering%20directions%20through%20separation%20scores%20from%20base%0Amodel%20representations%2C%20then%20analyze%20their%20mechanistic%20properties%20through%0Asteering%2C%20attention%20analysis%2C%20and%20weight%20orthogonalization.%20We%20find%20that%20code%0Acorrectness%20directions%20in%20LLMs%20reliably%20predict%20incorrect%20code%2C%20while%0Acorrection%20capabilities%2C%20though%20statistically%20significant%2C%20involve%20tradeoffs%0Abetween%20fixing%20errors%20and%20preserving%20correct%20code.%20Mechanistically%2C%20successful%0Acode%20generation%20depends%20on%20attending%20to%20test%20cases%20rather%20than%20problem%0Adescriptions.%20Moreover%2C%20directions%20identified%20in%20base%20models%20retain%20their%0Aeffectiveness%20after%20instruction-tuning%2C%20suggesting%20code%20correctness%20mechanisms%0Alearned%20during%20pre-training%20are%20repurposed%20during%20fine-tuning.%20Our%20mechanistic%0Ainsights%20suggest%20three%20practical%20applications%3A%20prompting%20strategies%20should%0Aprioritize%20test%20examples%20over%20elaborate%20problem%20descriptions%2C%20predictor%0Adirections%20can%20serve%20as%20error%20alarms%20for%20developer%20review%2C%20and%20these%20same%0Apredictors%20can%20guide%20selective%20steering%2C%20intervening%20only%20when%20errors%20are%0Aanticipated%20to%20prevent%20the%20code%20corruption%20from%20constant%20steering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanistic%2520Interpretability%2520of%2520Code%2520Correctness%2520in%2520LLMs%2520via%2520Sparse%250A%2520%2520Autoencoders%26entry.906535625%3DKriz%2520Tahimic%2520and%2520Charibeth%2520Cheng%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520become%2520integral%2520to%2520software%2520development%252C%2520with%250Asubstantial%2520portions%2520of%2520AI-suggested%2520code%2520entering%2520production%252C%2520understanding%250Atheir%2520internal%2520correctness%2520mechanisms%2520becomes%2520critical%2520for%2520safe%2520deployment.%2520We%250Aapply%2520sparse%2520autoencoders%2520to%2520decompose%2520LLM%2520representations%252C%2520identifying%250Adirections%2520that%2520correspond%2520to%2520code%2520correctness.%2520We%2520select%2520predictor%2520directions%250Ausing%2520t-statistics%2520and%2520steering%2520directions%2520through%2520separation%2520scores%2520from%2520base%250Amodel%2520representations%252C%2520then%2520analyze%2520their%2520mechanistic%2520properties%2520through%250Asteering%252C%2520attention%2520analysis%252C%2520and%2520weight%2520orthogonalization.%2520We%2520find%2520that%2520code%250Acorrectness%2520directions%2520in%2520LLMs%2520reliably%2520predict%2520incorrect%2520code%252C%2520while%250Acorrection%2520capabilities%252C%2520though%2520statistically%2520significant%252C%2520involve%2520tradeoffs%250Abetween%2520fixing%2520errors%2520and%2520preserving%2520correct%2520code.%2520Mechanistically%252C%2520successful%250Acode%2520generation%2520depends%2520on%2520attending%2520to%2520test%2520cases%2520rather%2520than%2520problem%250Adescriptions.%2520Moreover%252C%2520directions%2520identified%2520in%2520base%2520models%2520retain%2520their%250Aeffectiveness%2520after%2520instruction-tuning%252C%2520suggesting%2520code%2520correctness%2520mechanisms%250Alearned%2520during%2520pre-training%2520are%2520repurposed%2520during%2520fine-tuning.%2520Our%2520mechanistic%250Ainsights%2520suggest%2520three%2520practical%2520applications%253A%2520prompting%2520strategies%2520should%250Aprioritize%2520test%2520examples%2520over%2520elaborate%2520problem%2520descriptions%252C%2520predictor%250Adirections%2520can%2520serve%2520as%2520error%2520alarms%2520for%2520developer%2520review%252C%2520and%2520these%2520same%250Apredictors%2520can%2520guide%2520selective%2520steering%252C%2520intervening%2520only%2520when%2520errors%2520are%250Aanticipated%2520to%2520prevent%2520the%2520code%2520corruption%2520from%2520constant%2520steering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanistic%20Interpretability%20of%20Code%20Correctness%20in%20LLMs%20via%20Sparse%0A%20%20Autoencoders&entry.906535625=Kriz%20Tahimic%20and%20Charibeth%20Cheng&entry.1292438233=%20%20As%20Large%20Language%20Models%20become%20integral%20to%20software%20development%2C%20with%0Asubstantial%20portions%20of%20AI-suggested%20code%20entering%20production%2C%20understanding%0Atheir%20internal%20correctness%20mechanisms%20becomes%20critical%20for%20safe%20deployment.%20We%0Aapply%20sparse%20autoencoders%20to%20decompose%20LLM%20representations%2C%20identifying%0Adirections%20that%20correspond%20to%20code%20correctness.%20We%20select%20predictor%20directions%0Ausing%20t-statistics%20and%20steering%20directions%20through%20separation%20scores%20from%20base%0Amodel%20representations%2C%20then%20analyze%20their%20mechanistic%20properties%20through%0Asteering%2C%20attention%20analysis%2C%20and%20weight%20orthogonalization.%20We%20find%20that%20code%0Acorrectness%20directions%20in%20LLMs%20reliably%20predict%20incorrect%20code%2C%20while%0Acorrection%20capabilities%2C%20though%20statistically%20significant%2C%20involve%20tradeoffs%0Abetween%20fixing%20errors%20and%20preserving%20correct%20code.%20Mechanistically%2C%20successful%0Acode%20generation%20depends%20on%20attending%20to%20test%20cases%20rather%20than%20problem%0Adescriptions.%20Moreover%2C%20directions%20identified%20in%20base%20models%20retain%20their%0Aeffectiveness%20after%20instruction-tuning%2C%20suggesting%20code%20correctness%20mechanisms%0Alearned%20during%20pre-training%20are%20repurposed%20during%20fine-tuning.%20Our%20mechanistic%0Ainsights%20suggest%20three%20practical%20applications%3A%20prompting%20strategies%20should%0Aprioritize%20test%20examples%20over%20elaborate%20problem%20descriptions%2C%20predictor%0Adirections%20can%20serve%20as%20error%20alarms%20for%20developer%20review%2C%20and%20these%20same%0Apredictors%20can%20guide%20selective%20steering%2C%20intervening%20only%20when%20errors%20are%0Aanticipated%20to%20prevent%20the%20code%20corruption%20from%20constant%20steering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02917v1&entry.124074799=Read"},
{"title": "Semantic Differentiation in Speech Emotion Recognition: Insights from\n  Descriptive and Expressive Speech Roles", "author": "Rongchen Guo and Vincent Francoeur and Isar Nejadgholi and Sylvain Gagnon and Miodrag Bolic", "abstract": "  Speech Emotion Recognition (SER) is essential for improving human-computer\ninteraction, yet its accuracy remains constrained by the complexity of\nemotional nuances in speech. In this study, we distinguish between descriptive\nsemantics, which represents the contextual content of speech, and expressive\nsemantics, which reflects the speaker's emotional state. After watching\nemotionally charged movie segments, we recorded audio clips of participants\ndescribing their experiences, along with the intended emotion tags for each\nclip, participants' self-rated emotional responses, and their valence/arousal\nscores. Through experiments, we show that descriptive semantics align with\nintended emotions, while expressive semantics correlate with evoked emotions.\nOur findings inform SER applications in human-AI interaction and pave the way\nfor more context-aware AI systems.\n", "link": "http://arxiv.org/abs/2510.03060v1", "date": "2025-10-03", "relevancy": 2.0848, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5226}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5226}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Differentiation%20in%20Speech%20Emotion%20Recognition%3A%20Insights%20from%0A%20%20Descriptive%20and%20Expressive%20Speech%20Roles&body=Title%3A%20Semantic%20Differentiation%20in%20Speech%20Emotion%20Recognition%3A%20Insights%20from%0A%20%20Descriptive%20and%20Expressive%20Speech%20Roles%0AAuthor%3A%20Rongchen%20Guo%20and%20Vincent%20Francoeur%20and%20Isar%20Nejadgholi%20and%20Sylvain%20Gagnon%20and%20Miodrag%20Bolic%0AAbstract%3A%20%20%20Speech%20Emotion%20Recognition%20%28SER%29%20is%20essential%20for%20improving%20human-computer%0Ainteraction%2C%20yet%20its%20accuracy%20remains%20constrained%20by%20the%20complexity%20of%0Aemotional%20nuances%20in%20speech.%20In%20this%20study%2C%20we%20distinguish%20between%20descriptive%0Asemantics%2C%20which%20represents%20the%20contextual%20content%20of%20speech%2C%20and%20expressive%0Asemantics%2C%20which%20reflects%20the%20speaker%27s%20emotional%20state.%20After%20watching%0Aemotionally%20charged%20movie%20segments%2C%20we%20recorded%20audio%20clips%20of%20participants%0Adescribing%20their%20experiences%2C%20along%20with%20the%20intended%20emotion%20tags%20for%20each%0Aclip%2C%20participants%27%20self-rated%20emotional%20responses%2C%20and%20their%20valence/arousal%0Ascores.%20Through%20experiments%2C%20we%20show%20that%20descriptive%20semantics%20align%20with%0Aintended%20emotions%2C%20while%20expressive%20semantics%20correlate%20with%20evoked%20emotions.%0AOur%20findings%20inform%20SER%20applications%20in%20human-AI%20interaction%20and%20pave%20the%20way%0Afor%20more%20context-aware%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Differentiation%2520in%2520Speech%2520Emotion%2520Recognition%253A%2520Insights%2520from%250A%2520%2520Descriptive%2520and%2520Expressive%2520Speech%2520Roles%26entry.906535625%3DRongchen%2520Guo%2520and%2520Vincent%2520Francoeur%2520and%2520Isar%2520Nejadgholi%2520and%2520Sylvain%2520Gagnon%2520and%2520Miodrag%2520Bolic%26entry.1292438233%3D%2520%2520Speech%2520Emotion%2520Recognition%2520%2528SER%2529%2520is%2520essential%2520for%2520improving%2520human-computer%250Ainteraction%252C%2520yet%2520its%2520accuracy%2520remains%2520constrained%2520by%2520the%2520complexity%2520of%250Aemotional%2520nuances%2520in%2520speech.%2520In%2520this%2520study%252C%2520we%2520distinguish%2520between%2520descriptive%250Asemantics%252C%2520which%2520represents%2520the%2520contextual%2520content%2520of%2520speech%252C%2520and%2520expressive%250Asemantics%252C%2520which%2520reflects%2520the%2520speaker%2527s%2520emotional%2520state.%2520After%2520watching%250Aemotionally%2520charged%2520movie%2520segments%252C%2520we%2520recorded%2520audio%2520clips%2520of%2520participants%250Adescribing%2520their%2520experiences%252C%2520along%2520with%2520the%2520intended%2520emotion%2520tags%2520for%2520each%250Aclip%252C%2520participants%2527%2520self-rated%2520emotional%2520responses%252C%2520and%2520their%2520valence/arousal%250Ascores.%2520Through%2520experiments%252C%2520we%2520show%2520that%2520descriptive%2520semantics%2520align%2520with%250Aintended%2520emotions%252C%2520while%2520expressive%2520semantics%2520correlate%2520with%2520evoked%2520emotions.%250AOur%2520findings%2520inform%2520SER%2520applications%2520in%2520human-AI%2520interaction%2520and%2520pave%2520the%2520way%250Afor%2520more%2520context-aware%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Differentiation%20in%20Speech%20Emotion%20Recognition%3A%20Insights%20from%0A%20%20Descriptive%20and%20Expressive%20Speech%20Roles&entry.906535625=Rongchen%20Guo%20and%20Vincent%20Francoeur%20and%20Isar%20Nejadgholi%20and%20Sylvain%20Gagnon%20and%20Miodrag%20Bolic&entry.1292438233=%20%20Speech%20Emotion%20Recognition%20%28SER%29%20is%20essential%20for%20improving%20human-computer%0Ainteraction%2C%20yet%20its%20accuracy%20remains%20constrained%20by%20the%20complexity%20of%0Aemotional%20nuances%20in%20speech.%20In%20this%20study%2C%20we%20distinguish%20between%20descriptive%0Asemantics%2C%20which%20represents%20the%20contextual%20content%20of%20speech%2C%20and%20expressive%0Asemantics%2C%20which%20reflects%20the%20speaker%27s%20emotional%20state.%20After%20watching%0Aemotionally%20charged%20movie%20segments%2C%20we%20recorded%20audio%20clips%20of%20participants%0Adescribing%20their%20experiences%2C%20along%20with%20the%20intended%20emotion%20tags%20for%20each%0Aclip%2C%20participants%27%20self-rated%20emotional%20responses%2C%20and%20their%20valence/arousal%0Ascores.%20Through%20experiments%2C%20we%20show%20that%20descriptive%20semantics%20align%20with%0Aintended%20emotions%2C%20while%20expressive%20semantics%20correlate%20with%20evoked%20emotions.%0AOur%20findings%20inform%20SER%20applications%20in%20human-AI%20interaction%20and%20pave%20the%20way%0Afor%20more%20context-aware%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03060v1&entry.124074799=Read"},
{"title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs", "author": "Tao Ji and Bin Guo and Yuanbin Wu and Qipeng Guo and Lixing Shen and Zhan Chen and Xipeng Qiu and Qi Zhang and Tao Gui", "abstract": "  Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.\n", "link": "http://arxiv.org/abs/2502.14837v2", "date": "2025-10-03", "relevancy": 2.0812, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5169}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Economical%20Inference%3A%20Enabling%20DeepSeek%27s%20Multi-Head%20Latent%0A%20%20Attention%20in%20Any%20Transformer-based%20LLMs&body=Title%3A%20Towards%20Economical%20Inference%3A%20Enabling%20DeepSeek%27s%20Multi-Head%20Latent%0A%20%20Attention%20in%20Any%20Transformer-based%20LLMs%0AAuthor%3A%20Tao%20Ji%20and%20Bin%20Guo%20and%20Yuanbin%20Wu%20and%20Qipeng%20Guo%20and%20Lixing%20Shen%20and%20Zhan%20Chen%20and%20Xipeng%20Qiu%20and%20Qi%20Zhang%20and%20Tao%20Gui%0AAbstract%3A%20%20%20Multi-head%20Latent%20Attention%20%28MLA%29%20is%20an%20innovative%20architecture%20proposed%20by%0ADeepSeek%2C%20designed%20to%20ensure%20efficient%20and%20economical%20inference%20by%0Asignificantly%20compressing%20the%20Key-Value%20%28KV%29%20cache%20into%20a%20latent%20vector.%0ACompared%20to%20MLA%2C%20standard%20LLMs%20employing%20Multi-Head%20Attention%20%28MHA%29%20and%20its%0Avariants%20such%20as%20Grouped-Query%20Attention%20%28GQA%29%20exhibit%20significant%20cost%0Adisadvantages.%20Enabling%20well-trained%20LLMs%20%28e.g.%2C%20Llama%29%20to%20rapidly%20adapt%20to%20MLA%0Awithout%20pre-training%20from%20scratch%20is%20both%20meaningful%20and%20challenging.%20This%0Apaper%20proposes%20the%20first%20data-efficient%20fine-tuning%20method%20for%20transitioning%0Afrom%20MHA%20to%20MLA%20%28MHA2MLA%29%2C%20which%20includes%20two%20key%20components%3A%20for%20partial-RoPE%2C%0Awe%20remove%20RoPE%20from%20dimensions%20of%20queries%20and%20keys%20that%20contribute%20less%20to%20the%0Aattention%20scores%2C%20for%20low-rank%20approximation%2C%20we%20introduce%20joint%20SVD%0Aapproximations%20based%20on%20the%20pre-trained%20parameters%20of%20keys%20and%20values.%20These%0Acarefully%20designed%20strategies%20enable%20MHA2MLA%20to%20recover%20performance%20using%20only%0Aa%20small%20fraction%20%280.3%25%20to%200.6%25%29%20of%20the%20data%2C%20significantly%20reducing%20inference%0Acosts%20while%20seamlessly%20integrating%20with%20compression%20techniques%20such%20as%20KV%20cache%0Aquantization.%20For%20example%2C%20the%20KV%20cache%20size%20of%20Llama2-7B%20is%20reduced%20by%2092.19%25%2C%0Awith%20only%20a%200.5%25%20drop%20in%20LongBench%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14837v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Economical%2520Inference%253A%2520Enabling%2520DeepSeek%2527s%2520Multi-Head%2520Latent%250A%2520%2520Attention%2520in%2520Any%2520Transformer-based%2520LLMs%26entry.906535625%3DTao%2520Ji%2520and%2520Bin%2520Guo%2520and%2520Yuanbin%2520Wu%2520and%2520Qipeng%2520Guo%2520and%2520Lixing%2520Shen%2520and%2520Zhan%2520Chen%2520and%2520Xipeng%2520Qiu%2520and%2520Qi%2520Zhang%2520and%2520Tao%2520Gui%26entry.1292438233%3D%2520%2520Multi-head%2520Latent%2520Attention%2520%2528MLA%2529%2520is%2520an%2520innovative%2520architecture%2520proposed%2520by%250ADeepSeek%252C%2520designed%2520to%2520ensure%2520efficient%2520and%2520economical%2520inference%2520by%250Asignificantly%2520compressing%2520the%2520Key-Value%2520%2528KV%2529%2520cache%2520into%2520a%2520latent%2520vector.%250ACompared%2520to%2520MLA%252C%2520standard%2520LLMs%2520employing%2520Multi-Head%2520Attention%2520%2528MHA%2529%2520and%2520its%250Avariants%2520such%2520as%2520Grouped-Query%2520Attention%2520%2528GQA%2529%2520exhibit%2520significant%2520cost%250Adisadvantages.%2520Enabling%2520well-trained%2520LLMs%2520%2528e.g.%252C%2520Llama%2529%2520to%2520rapidly%2520adapt%2520to%2520MLA%250Awithout%2520pre-training%2520from%2520scratch%2520is%2520both%2520meaningful%2520and%2520challenging.%2520This%250Apaper%2520proposes%2520the%2520first%2520data-efficient%2520fine-tuning%2520method%2520for%2520transitioning%250Afrom%2520MHA%2520to%2520MLA%2520%2528MHA2MLA%2529%252C%2520which%2520includes%2520two%2520key%2520components%253A%2520for%2520partial-RoPE%252C%250Awe%2520remove%2520RoPE%2520from%2520dimensions%2520of%2520queries%2520and%2520keys%2520that%2520contribute%2520less%2520to%2520the%250Aattention%2520scores%252C%2520for%2520low-rank%2520approximation%252C%2520we%2520introduce%2520joint%2520SVD%250Aapproximations%2520based%2520on%2520the%2520pre-trained%2520parameters%2520of%2520keys%2520and%2520values.%2520These%250Acarefully%2520designed%2520strategies%2520enable%2520MHA2MLA%2520to%2520recover%2520performance%2520using%2520only%250Aa%2520small%2520fraction%2520%25280.3%2525%2520to%25200.6%2525%2529%2520of%2520the%2520data%252C%2520significantly%2520reducing%2520inference%250Acosts%2520while%2520seamlessly%2520integrating%2520with%2520compression%2520techniques%2520such%2520as%2520KV%2520cache%250Aquantization.%2520For%2520example%252C%2520the%2520KV%2520cache%2520size%2520of%2520Llama2-7B%2520is%2520reduced%2520by%252092.19%2525%252C%250Awith%2520only%2520a%25200.5%2525%2520drop%2520in%2520LongBench%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14837v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Economical%20Inference%3A%20Enabling%20DeepSeek%27s%20Multi-Head%20Latent%0A%20%20Attention%20in%20Any%20Transformer-based%20LLMs&entry.906535625=Tao%20Ji%20and%20Bin%20Guo%20and%20Yuanbin%20Wu%20and%20Qipeng%20Guo%20and%20Lixing%20Shen%20and%20Zhan%20Chen%20and%20Xipeng%20Qiu%20and%20Qi%20Zhang%20and%20Tao%20Gui&entry.1292438233=%20%20Multi-head%20Latent%20Attention%20%28MLA%29%20is%20an%20innovative%20architecture%20proposed%20by%0ADeepSeek%2C%20designed%20to%20ensure%20efficient%20and%20economical%20inference%20by%0Asignificantly%20compressing%20the%20Key-Value%20%28KV%29%20cache%20into%20a%20latent%20vector.%0ACompared%20to%20MLA%2C%20standard%20LLMs%20employing%20Multi-Head%20Attention%20%28MHA%29%20and%20its%0Avariants%20such%20as%20Grouped-Query%20Attention%20%28GQA%29%20exhibit%20significant%20cost%0Adisadvantages.%20Enabling%20well-trained%20LLMs%20%28e.g.%2C%20Llama%29%20to%20rapidly%20adapt%20to%20MLA%0Awithout%20pre-training%20from%20scratch%20is%20both%20meaningful%20and%20challenging.%20This%0Apaper%20proposes%20the%20first%20data-efficient%20fine-tuning%20method%20for%20transitioning%0Afrom%20MHA%20to%20MLA%20%28MHA2MLA%29%2C%20which%20includes%20two%20key%20components%3A%20for%20partial-RoPE%2C%0Awe%20remove%20RoPE%20from%20dimensions%20of%20queries%20and%20keys%20that%20contribute%20less%20to%20the%0Aattention%20scores%2C%20for%20low-rank%20approximation%2C%20we%20introduce%20joint%20SVD%0Aapproximations%20based%20on%20the%20pre-trained%20parameters%20of%20keys%20and%20values.%20These%0Acarefully%20designed%20strategies%20enable%20MHA2MLA%20to%20recover%20performance%20using%20only%0Aa%20small%20fraction%20%280.3%25%20to%200.6%25%29%20of%20the%20data%2C%20significantly%20reducing%20inference%0Acosts%20while%20seamlessly%20integrating%20with%20compression%20techniques%20such%20as%20KV%20cache%0Aquantization.%20For%20example%2C%20the%20KV%20cache%20size%20of%20Llama2-7B%20is%20reduced%20by%2092.19%25%2C%0Awith%20only%20a%200.5%25%20drop%20in%20LongBench%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14837v2&entry.124074799=Read"},
{"title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models", "author": "Tianyu Fu and Zihan Min and Hanling Zhang and Jichao Yan and Guohao Dai and Wanli Ouyang and Yu Wang", "abstract": "  Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.\n", "link": "http://arxiv.org/abs/2510.03215v1", "date": "2025-10-03", "relevancy": 2.0806, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cache-to-Cache%3A%20Direct%20Semantic%20Communication%20Between%20Large%20Language%0A%20%20Models&body=Title%3A%20Cache-to-Cache%3A%20Direct%20Semantic%20Communication%20Between%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Tianyu%20Fu%20and%20Zihan%20Min%20and%20Hanling%20Zhang%20and%20Jichao%20Yan%20and%20Guohao%20Dai%20and%20Wanli%20Ouyang%20and%20Yu%20Wang%0AAbstract%3A%20%20%20Multi-LLM%20systems%20harness%20the%20complementary%20strengths%20of%20diverse%20Large%0ALanguage%20Models%2C%20achieving%20performance%20and%20efficiency%20gains%20unattainable%20by%20a%0Asingle%20model.%20In%20existing%20designs%2C%20LLMs%20communicate%20through%20text%2C%20forcing%0Ainternal%20representations%20to%20be%20transformed%20into%20output%20token%20sequences.%20This%0Aprocess%20both%20loses%20rich%20semantic%20information%20and%20incurs%20token-by-token%0Ageneration%20latency.%20Motivated%20by%20these%20limitations%2C%20we%20ask%3A%20Can%20LLMs%0Acommunicate%20beyond%20text%3F%20Oracle%20experiments%20show%20that%20enriching%20the%20KV-Cache%0Asemantics%20can%20improve%20response%20quality%20without%20increasing%20cache%20size%2C%0Asupporting%20KV-Cache%20as%20an%20effective%20medium%20for%20inter-model%20communication.%20Thus%2C%0Awe%20propose%20Cache-to-Cache%20%28C2C%29%2C%20a%20new%20paradigm%20for%20direct%20semantic%0Acommunication%20between%20LLMs.%20C2C%20uses%20a%20neural%20network%20to%20project%20and%20fuse%20the%0Asource%20model%27s%20KV-cache%20with%20that%20of%20the%20target%20model%20to%20enable%20direct%20semantic%0Atransfer.%20A%20learnable%20gating%20mechanism%20selects%20the%20target%20layers%20that%20benefit%0Afrom%20cache%20communication.%20Compared%20with%20text%20communication%2C%20C2C%20utilizes%20the%0Adeep%2C%20specialized%20semantics%20from%20both%20models%2C%20while%20avoiding%20explicit%0Aintermediate%20text%20generation.%20Experiments%20show%20that%20C2C%20achieves%208.5-10.5%25%0Ahigher%20average%20accuracy%20than%20individual%20models.%20It%20further%20outperforms%20the%20text%0Acommunication%20paradigm%20by%20approximately%203.0-5.0%25%2C%20while%20delivering%20an%20average%0A2.0x%20speedup%20in%20latency.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/thu-nics/C2C.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCache-to-Cache%253A%2520Direct%2520Semantic%2520Communication%2520Between%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DTianyu%2520Fu%2520and%2520Zihan%2520Min%2520and%2520Hanling%2520Zhang%2520and%2520Jichao%2520Yan%2520and%2520Guohao%2520Dai%2520and%2520Wanli%2520Ouyang%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520Multi-LLM%2520systems%2520harness%2520the%2520complementary%2520strengths%2520of%2520diverse%2520Large%250ALanguage%2520Models%252C%2520achieving%2520performance%2520and%2520efficiency%2520gains%2520unattainable%2520by%2520a%250Asingle%2520model.%2520In%2520existing%2520designs%252C%2520LLMs%2520communicate%2520through%2520text%252C%2520forcing%250Ainternal%2520representations%2520to%2520be%2520transformed%2520into%2520output%2520token%2520sequences.%2520This%250Aprocess%2520both%2520loses%2520rich%2520semantic%2520information%2520and%2520incurs%2520token-by-token%250Ageneration%2520latency.%2520Motivated%2520by%2520these%2520limitations%252C%2520we%2520ask%253A%2520Can%2520LLMs%250Acommunicate%2520beyond%2520text%253F%2520Oracle%2520experiments%2520show%2520that%2520enriching%2520the%2520KV-Cache%250Asemantics%2520can%2520improve%2520response%2520quality%2520without%2520increasing%2520cache%2520size%252C%250Asupporting%2520KV-Cache%2520as%2520an%2520effective%2520medium%2520for%2520inter-model%2520communication.%2520Thus%252C%250Awe%2520propose%2520Cache-to-Cache%2520%2528C2C%2529%252C%2520a%2520new%2520paradigm%2520for%2520direct%2520semantic%250Acommunication%2520between%2520LLMs.%2520C2C%2520uses%2520a%2520neural%2520network%2520to%2520project%2520and%2520fuse%2520the%250Asource%2520model%2527s%2520KV-cache%2520with%2520that%2520of%2520the%2520target%2520model%2520to%2520enable%2520direct%2520semantic%250Atransfer.%2520A%2520learnable%2520gating%2520mechanism%2520selects%2520the%2520target%2520layers%2520that%2520benefit%250Afrom%2520cache%2520communication.%2520Compared%2520with%2520text%2520communication%252C%2520C2C%2520utilizes%2520the%250Adeep%252C%2520specialized%2520semantics%2520from%2520both%2520models%252C%2520while%2520avoiding%2520explicit%250Aintermediate%2520text%2520generation.%2520Experiments%2520show%2520that%2520C2C%2520achieves%25208.5-10.5%2525%250Ahigher%2520average%2520accuracy%2520than%2520individual%2520models.%2520It%2520further%2520outperforms%2520the%2520text%250Acommunication%2520paradigm%2520by%2520approximately%25203.0-5.0%2525%252C%2520while%2520delivering%2520an%2520average%250A2.0x%2520speedup%2520in%2520latency.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/thu-nics/C2C.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cache-to-Cache%3A%20Direct%20Semantic%20Communication%20Between%20Large%20Language%0A%20%20Models&entry.906535625=Tianyu%20Fu%20and%20Zihan%20Min%20and%20Hanling%20Zhang%20and%20Jichao%20Yan%20and%20Guohao%20Dai%20and%20Wanli%20Ouyang%20and%20Yu%20Wang&entry.1292438233=%20%20Multi-LLM%20systems%20harness%20the%20complementary%20strengths%20of%20diverse%20Large%0ALanguage%20Models%2C%20achieving%20performance%20and%20efficiency%20gains%20unattainable%20by%20a%0Asingle%20model.%20In%20existing%20designs%2C%20LLMs%20communicate%20through%20text%2C%20forcing%0Ainternal%20representations%20to%20be%20transformed%20into%20output%20token%20sequences.%20This%0Aprocess%20both%20loses%20rich%20semantic%20information%20and%20incurs%20token-by-token%0Ageneration%20latency.%20Motivated%20by%20these%20limitations%2C%20we%20ask%3A%20Can%20LLMs%0Acommunicate%20beyond%20text%3F%20Oracle%20experiments%20show%20that%20enriching%20the%20KV-Cache%0Asemantics%20can%20improve%20response%20quality%20without%20increasing%20cache%20size%2C%0Asupporting%20KV-Cache%20as%20an%20effective%20medium%20for%20inter-model%20communication.%20Thus%2C%0Awe%20propose%20Cache-to-Cache%20%28C2C%29%2C%20a%20new%20paradigm%20for%20direct%20semantic%0Acommunication%20between%20LLMs.%20C2C%20uses%20a%20neural%20network%20to%20project%20and%20fuse%20the%0Asource%20model%27s%20KV-cache%20with%20that%20of%20the%20target%20model%20to%20enable%20direct%20semantic%0Atransfer.%20A%20learnable%20gating%20mechanism%20selects%20the%20target%20layers%20that%20benefit%0Afrom%20cache%20communication.%20Compared%20with%20text%20communication%2C%20C2C%20utilizes%20the%0Adeep%2C%20specialized%20semantics%20from%20both%20models%2C%20while%20avoiding%20explicit%0Aintermediate%20text%20generation.%20Experiments%20show%20that%20C2C%20achieves%208.5-10.5%25%0Ahigher%20average%20accuracy%20than%20individual%20models.%20It%20further%20outperforms%20the%20text%0Acommunication%20paradigm%20by%20approximately%203.0-5.0%25%2C%20while%20delivering%20an%20average%0A2.0x%20speedup%20in%20latency.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/thu-nics/C2C.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03215v1&entry.124074799=Read"},
{"title": "Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered\n  Mobile Platform for Trajectory Tracking Tasks", "author": "Alvaro Paz and Pauli Mustalahti and Mohammad Dastranj and Jouni Mattila", "abstract": "  This paper presents a framework for real-time optimal controlling of a\nheavy-duty skid-steered mobile platform for trajectory tracking. The importance\nof accurate real-time performance of the controller lies in safety\nconsiderations of situations where the dynamic system under control is affected\nby uncertainties and disturbances, and the controller should compensate for\nsuch phenomena in order to provide stable performance. A multiple-shooting\nnonlinear model-predictive control framework is proposed in this paper. This\nframework benefits from suitable algorithm along with readings from various\nsensors for genuine real-time performance with extremely high accuracy. The\ncontroller is then tested for tracking different trajectories where it\ndemonstrates highly desirable performance in terms of both speed and accuracy.\nThis controller shows remarkable improvement when compared to existing\nnonlinear model-predictive controllers in the literature that were implemented\non skid-steered mobile platforms.\n", "link": "http://arxiv.org/abs/2510.02976v1", "date": "2025-10-03", "relevancy": 2.0778, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5622}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5181}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Nonlinear%20Model%20Predictive%20Control%20of%20Heavy-Duty%20Skid-Steered%0A%20%20Mobile%20Platform%20for%20Trajectory%20Tracking%20Tasks&body=Title%3A%20Real-Time%20Nonlinear%20Model%20Predictive%20Control%20of%20Heavy-Duty%20Skid-Steered%0A%20%20Mobile%20Platform%20for%20Trajectory%20Tracking%20Tasks%0AAuthor%3A%20Alvaro%20Paz%20and%20Pauli%20Mustalahti%20and%20Mohammad%20Dastranj%20and%20Jouni%20Mattila%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20framework%20for%20real-time%20optimal%20controlling%20of%20a%0Aheavy-duty%20skid-steered%20mobile%20platform%20for%20trajectory%20tracking.%20The%20importance%0Aof%20accurate%20real-time%20performance%20of%20the%20controller%20lies%20in%20safety%0Aconsiderations%20of%20situations%20where%20the%20dynamic%20system%20under%20control%20is%20affected%0Aby%20uncertainties%20and%20disturbances%2C%20and%20the%20controller%20should%20compensate%20for%0Asuch%20phenomena%20in%20order%20to%20provide%20stable%20performance.%20A%20multiple-shooting%0Anonlinear%20model-predictive%20control%20framework%20is%20proposed%20in%20this%20paper.%20This%0Aframework%20benefits%20from%20suitable%20algorithm%20along%20with%20readings%20from%20various%0Asensors%20for%20genuine%20real-time%20performance%20with%20extremely%20high%20accuracy.%20The%0Acontroller%20is%20then%20tested%20for%20tracking%20different%20trajectories%20where%20it%0Ademonstrates%20highly%20desirable%20performance%20in%20terms%20of%20both%20speed%20and%20accuracy.%0AThis%20controller%20shows%20remarkable%20improvement%20when%20compared%20to%20existing%0Anonlinear%20model-predictive%20controllers%20in%20the%20literature%20that%20were%20implemented%0Aon%20skid-steered%20mobile%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Nonlinear%2520Model%2520Predictive%2520Control%2520of%2520Heavy-Duty%2520Skid-Steered%250A%2520%2520Mobile%2520Platform%2520for%2520Trajectory%2520Tracking%2520Tasks%26entry.906535625%3DAlvaro%2520Paz%2520and%2520Pauli%2520Mustalahti%2520and%2520Mohammad%2520Dastranj%2520and%2520Jouni%2520Mattila%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520framework%2520for%2520real-time%2520optimal%2520controlling%2520of%2520a%250Aheavy-duty%2520skid-steered%2520mobile%2520platform%2520for%2520trajectory%2520tracking.%2520The%2520importance%250Aof%2520accurate%2520real-time%2520performance%2520of%2520the%2520controller%2520lies%2520in%2520safety%250Aconsiderations%2520of%2520situations%2520where%2520the%2520dynamic%2520system%2520under%2520control%2520is%2520affected%250Aby%2520uncertainties%2520and%2520disturbances%252C%2520and%2520the%2520controller%2520should%2520compensate%2520for%250Asuch%2520phenomena%2520in%2520order%2520to%2520provide%2520stable%2520performance.%2520A%2520multiple-shooting%250Anonlinear%2520model-predictive%2520control%2520framework%2520is%2520proposed%2520in%2520this%2520paper.%2520This%250Aframework%2520benefits%2520from%2520suitable%2520algorithm%2520along%2520with%2520readings%2520from%2520various%250Asensors%2520for%2520genuine%2520real-time%2520performance%2520with%2520extremely%2520high%2520accuracy.%2520The%250Acontroller%2520is%2520then%2520tested%2520for%2520tracking%2520different%2520trajectories%2520where%2520it%250Ademonstrates%2520highly%2520desirable%2520performance%2520in%2520terms%2520of%2520both%2520speed%2520and%2520accuracy.%250AThis%2520controller%2520shows%2520remarkable%2520improvement%2520when%2520compared%2520to%2520existing%250Anonlinear%2520model-predictive%2520controllers%2520in%2520the%2520literature%2520that%2520were%2520implemented%250Aon%2520skid-steered%2520mobile%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Nonlinear%20Model%20Predictive%20Control%20of%20Heavy-Duty%20Skid-Steered%0A%20%20Mobile%20Platform%20for%20Trajectory%20Tracking%20Tasks&entry.906535625=Alvaro%20Paz%20and%20Pauli%20Mustalahti%20and%20Mohammad%20Dastranj%20and%20Jouni%20Mattila&entry.1292438233=%20%20This%20paper%20presents%20a%20framework%20for%20real-time%20optimal%20controlling%20of%20a%0Aheavy-duty%20skid-steered%20mobile%20platform%20for%20trajectory%20tracking.%20The%20importance%0Aof%20accurate%20real-time%20performance%20of%20the%20controller%20lies%20in%20safety%0Aconsiderations%20of%20situations%20where%20the%20dynamic%20system%20under%20control%20is%20affected%0Aby%20uncertainties%20and%20disturbances%2C%20and%20the%20controller%20should%20compensate%20for%0Asuch%20phenomena%20in%20order%20to%20provide%20stable%20performance.%20A%20multiple-shooting%0Anonlinear%20model-predictive%20control%20framework%20is%20proposed%20in%20this%20paper.%20This%0Aframework%20benefits%20from%20suitable%20algorithm%20along%20with%20readings%20from%20various%0Asensors%20for%20genuine%20real-time%20performance%20with%20extremely%20high%20accuracy.%20The%0Acontroller%20is%20then%20tested%20for%20tracking%20different%20trajectories%20where%20it%0Ademonstrates%20highly%20desirable%20performance%20in%20terms%20of%20both%20speed%20and%20accuracy.%0AThis%20controller%20shows%20remarkable%20improvement%20when%20compared%20to%20existing%0Anonlinear%20model-predictive%20controllers%20in%20the%20literature%20that%20were%20implemented%0Aon%20skid-steered%20mobile%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02976v1&entry.124074799=Read"},
{"title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus", "author": "Ming Zhao and Wenhui Dong and Yang Zhang and Xiang Zheng and Zhonghao Zhang and Zian Zhou and Yunzhi Guan and Liukun Xu and Wei Peng and Zhaoyang Gong and Zhicheng Zhang and Dachuan Li and Xiaosheng Ma and Yuli Ma and Jianing Ni and Changjiang Jiang and Lixia Tian and Qixin Chen and Kaishun Xia and Pingping Liu and Tongshun Zhang and Zhiqiang Liu and Zhongan Bi and Chenyang Si and Tiansheng Sun and Caifeng Shan", "abstract": "  Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.\n", "link": "http://arxiv.org/abs/2510.03160v1", "date": "2025-10-03", "relevancy": 2.0656, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpineBench%3A%20A%20Clinically%20Salient%2C%20Level-Aware%20Benchmark%20Powered%20by%20the%0A%20%20SpineMed-450k%20Corpus&body=Title%3A%20SpineBench%3A%20A%20Clinically%20Salient%2C%20Level-Aware%20Benchmark%20Powered%20by%20the%0A%20%20SpineMed-450k%20Corpus%0AAuthor%3A%20Ming%20Zhao%20and%20Wenhui%20Dong%20and%20Yang%20Zhang%20and%20Xiang%20Zheng%20and%20Zhonghao%20Zhang%20and%20Zian%20Zhou%20and%20Yunzhi%20Guan%20and%20Liukun%20Xu%20and%20Wei%20Peng%20and%20Zhaoyang%20Gong%20and%20Zhicheng%20Zhang%20and%20Dachuan%20Li%20and%20Xiaosheng%20Ma%20and%20Yuli%20Ma%20and%20Jianing%20Ni%20and%20Changjiang%20Jiang%20and%20Lixia%20Tian%20and%20Qixin%20Chen%20and%20Kaishun%20Xia%20and%20Pingping%20Liu%20and%20Tongshun%20Zhang%20and%20Zhiqiang%20Liu%20and%20Zhongan%20Bi%20and%20Chenyang%20Si%20and%20Tiansheng%20Sun%20and%20Caifeng%20Shan%0AAbstract%3A%20%20%20Spine%20disorders%20affect%20619%20million%20people%20globally%20and%20are%20a%20leading%20cause%20of%0Adisability%2C%20yet%20AI-assisted%20diagnosis%20remains%20limited%20by%20the%20lack%20of%0Alevel-aware%2C%20multimodal%20datasets.%20Clinical%20decision-making%20for%20spine%20disorders%0Arequires%20sophisticated%20reasoning%20across%20X-ray%2C%20CT%2C%20and%20MRI%20at%20specific%0Avertebral%20levels.%20However%2C%20progress%20has%20been%20constrained%20by%20the%20absence%20of%0Atraceable%2C%20clinically-grounded%20instruction%20data%20and%20standardized%2C%0Aspine-specific%20benchmarks.%20To%20address%20this%2C%20we%20introduce%20SpineMed%2C%20an%20ecosystem%0Aco-designed%20with%20practicing%20spine%20surgeons.%20It%20features%20SpineMed-450k%2C%20the%0Afirst%20large-scale%20dataset%20explicitly%20designed%20for%20vertebral-level%20reasoning%0Aacross%20imaging%20modalities%20with%20over%20450%2C000%20instruction%20instances%2C%20and%0ASpineBench%2C%20a%20clinically-grounded%20evaluation%20framework.%20SpineMed-450k%20is%0Acurated%20from%20diverse%20sources%2C%20including%20textbooks%2C%20guidelines%2C%20open%20datasets%2C%0Aand%20~1%2C000%20de-identified%20hospital%20cases%2C%20using%20a%20clinician-in-the-loop%20pipeline%0Awith%20a%20two-stage%20LLM%20generation%20method%20%28draft%20and%20revision%29%20to%20ensure%0Ahigh-quality%2C%20traceable%20data%20for%20question-answering%2C%20multi-turn%20consultations%2C%0Aand%20report%20generation.%20SpineBench%20evaluates%20models%20on%20clinically%20salient%20axes%2C%0Aincluding%20level%20identification%2C%20pathology%20assessment%2C%20and%20surgical%20planning.%0AOur%20comprehensive%20evaluation%20of%20several%20recently%20advanced%20large%20vision-language%0Amodels%20%28LVLMs%29%20on%20SpineBench%20reveals%20systematic%20weaknesses%20in%20fine-grained%2C%0Alevel-specific%20reasoning.%20In%20contrast%2C%20our%20model%20fine-tuned%20on%20SpineMed-450k%0Ademonstrates%20consistent%20and%20significant%20improvements%20across%20all%20tasks.%0AClinician%20assessments%20confirm%20the%20diagnostic%20clarity%20and%20practical%20utility%20of%0Aour%20model%27s%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpineBench%253A%2520A%2520Clinically%2520Salient%252C%2520Level-Aware%2520Benchmark%2520Powered%2520by%2520the%250A%2520%2520SpineMed-450k%2520Corpus%26entry.906535625%3DMing%2520Zhao%2520and%2520Wenhui%2520Dong%2520and%2520Yang%2520Zhang%2520and%2520Xiang%2520Zheng%2520and%2520Zhonghao%2520Zhang%2520and%2520Zian%2520Zhou%2520and%2520Yunzhi%2520Guan%2520and%2520Liukun%2520Xu%2520and%2520Wei%2520Peng%2520and%2520Zhaoyang%2520Gong%2520and%2520Zhicheng%2520Zhang%2520and%2520Dachuan%2520Li%2520and%2520Xiaosheng%2520Ma%2520and%2520Yuli%2520Ma%2520and%2520Jianing%2520Ni%2520and%2520Changjiang%2520Jiang%2520and%2520Lixia%2520Tian%2520and%2520Qixin%2520Chen%2520and%2520Kaishun%2520Xia%2520and%2520Pingping%2520Liu%2520and%2520Tongshun%2520Zhang%2520and%2520Zhiqiang%2520Liu%2520and%2520Zhongan%2520Bi%2520and%2520Chenyang%2520Si%2520and%2520Tiansheng%2520Sun%2520and%2520Caifeng%2520Shan%26entry.1292438233%3D%2520%2520Spine%2520disorders%2520affect%2520619%2520million%2520people%2520globally%2520and%2520are%2520a%2520leading%2520cause%2520of%250Adisability%252C%2520yet%2520AI-assisted%2520diagnosis%2520remains%2520limited%2520by%2520the%2520lack%2520of%250Alevel-aware%252C%2520multimodal%2520datasets.%2520Clinical%2520decision-making%2520for%2520spine%2520disorders%250Arequires%2520sophisticated%2520reasoning%2520across%2520X-ray%252C%2520CT%252C%2520and%2520MRI%2520at%2520specific%250Avertebral%2520levels.%2520However%252C%2520progress%2520has%2520been%2520constrained%2520by%2520the%2520absence%2520of%250Atraceable%252C%2520clinically-grounded%2520instruction%2520data%2520and%2520standardized%252C%250Aspine-specific%2520benchmarks.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SpineMed%252C%2520an%2520ecosystem%250Aco-designed%2520with%2520practicing%2520spine%2520surgeons.%2520It%2520features%2520SpineMed-450k%252C%2520the%250Afirst%2520large-scale%2520dataset%2520explicitly%2520designed%2520for%2520vertebral-level%2520reasoning%250Aacross%2520imaging%2520modalities%2520with%2520over%2520450%252C000%2520instruction%2520instances%252C%2520and%250ASpineBench%252C%2520a%2520clinically-grounded%2520evaluation%2520framework.%2520SpineMed-450k%2520is%250Acurated%2520from%2520diverse%2520sources%252C%2520including%2520textbooks%252C%2520guidelines%252C%2520open%2520datasets%252C%250Aand%2520~1%252C000%2520de-identified%2520hospital%2520cases%252C%2520using%2520a%2520clinician-in-the-loop%2520pipeline%250Awith%2520a%2520two-stage%2520LLM%2520generation%2520method%2520%2528draft%2520and%2520revision%2529%2520to%2520ensure%250Ahigh-quality%252C%2520traceable%2520data%2520for%2520question-answering%252C%2520multi-turn%2520consultations%252C%250Aand%2520report%2520generation.%2520SpineBench%2520evaluates%2520models%2520on%2520clinically%2520salient%2520axes%252C%250Aincluding%2520level%2520identification%252C%2520pathology%2520assessment%252C%2520and%2520surgical%2520planning.%250AOur%2520comprehensive%2520evaluation%2520of%2520several%2520recently%2520advanced%2520large%2520vision-language%250Amodels%2520%2528LVLMs%2529%2520on%2520SpineBench%2520reveals%2520systematic%2520weaknesses%2520in%2520fine-grained%252C%250Alevel-specific%2520reasoning.%2520In%2520contrast%252C%2520our%2520model%2520fine-tuned%2520on%2520SpineMed-450k%250Ademonstrates%2520consistent%2520and%2520significant%2520improvements%2520across%2520all%2520tasks.%250AClinician%2520assessments%2520confirm%2520the%2520diagnostic%2520clarity%2520and%2520practical%2520utility%2520of%250Aour%2520model%2527s%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpineBench%3A%20A%20Clinically%20Salient%2C%20Level-Aware%20Benchmark%20Powered%20by%20the%0A%20%20SpineMed-450k%20Corpus&entry.906535625=Ming%20Zhao%20and%20Wenhui%20Dong%20and%20Yang%20Zhang%20and%20Xiang%20Zheng%20and%20Zhonghao%20Zhang%20and%20Zian%20Zhou%20and%20Yunzhi%20Guan%20and%20Liukun%20Xu%20and%20Wei%20Peng%20and%20Zhaoyang%20Gong%20and%20Zhicheng%20Zhang%20and%20Dachuan%20Li%20and%20Xiaosheng%20Ma%20and%20Yuli%20Ma%20and%20Jianing%20Ni%20and%20Changjiang%20Jiang%20and%20Lixia%20Tian%20and%20Qixin%20Chen%20and%20Kaishun%20Xia%20and%20Pingping%20Liu%20and%20Tongshun%20Zhang%20and%20Zhiqiang%20Liu%20and%20Zhongan%20Bi%20and%20Chenyang%20Si%20and%20Tiansheng%20Sun%20and%20Caifeng%20Shan&entry.1292438233=%20%20Spine%20disorders%20affect%20619%20million%20people%20globally%20and%20are%20a%20leading%20cause%20of%0Adisability%2C%20yet%20AI-assisted%20diagnosis%20remains%20limited%20by%20the%20lack%20of%0Alevel-aware%2C%20multimodal%20datasets.%20Clinical%20decision-making%20for%20spine%20disorders%0Arequires%20sophisticated%20reasoning%20across%20X-ray%2C%20CT%2C%20and%20MRI%20at%20specific%0Avertebral%20levels.%20However%2C%20progress%20has%20been%20constrained%20by%20the%20absence%20of%0Atraceable%2C%20clinically-grounded%20instruction%20data%20and%20standardized%2C%0Aspine-specific%20benchmarks.%20To%20address%20this%2C%20we%20introduce%20SpineMed%2C%20an%20ecosystem%0Aco-designed%20with%20practicing%20spine%20surgeons.%20It%20features%20SpineMed-450k%2C%20the%0Afirst%20large-scale%20dataset%20explicitly%20designed%20for%20vertebral-level%20reasoning%0Aacross%20imaging%20modalities%20with%20over%20450%2C000%20instruction%20instances%2C%20and%0ASpineBench%2C%20a%20clinically-grounded%20evaluation%20framework.%20SpineMed-450k%20is%0Acurated%20from%20diverse%20sources%2C%20including%20textbooks%2C%20guidelines%2C%20open%20datasets%2C%0Aand%20~1%2C000%20de-identified%20hospital%20cases%2C%20using%20a%20clinician-in-the-loop%20pipeline%0Awith%20a%20two-stage%20LLM%20generation%20method%20%28draft%20and%20revision%29%20to%20ensure%0Ahigh-quality%2C%20traceable%20data%20for%20question-answering%2C%20multi-turn%20consultations%2C%0Aand%20report%20generation.%20SpineBench%20evaluates%20models%20on%20clinically%20salient%20axes%2C%0Aincluding%20level%20identification%2C%20pathology%20assessment%2C%20and%20surgical%20planning.%0AOur%20comprehensive%20evaluation%20of%20several%20recently%20advanced%20large%20vision-language%0Amodels%20%28LVLMs%29%20on%20SpineBench%20reveals%20systematic%20weaknesses%20in%20fine-grained%2C%0Alevel-specific%20reasoning.%20In%20contrast%2C%20our%20model%20fine-tuned%20on%20SpineMed-450k%0Ademonstrates%20consistent%20and%20significant%20improvements%20across%20all%20tasks.%0AClinician%20assessments%20confirm%20the%20diagnostic%20clarity%20and%20practical%20utility%20of%0Aour%20model%27s%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03160v1&entry.124074799=Read"},
{"title": "Learning Stability Certificate for Robotics in Real-World Environments", "author": "Zhe Shen", "abstract": "  Stability certificates play a critical role in ensuring the safety and\nreliability of robotic systems. However, deriving these certificates for\ncomplex, unknown systems has traditionally required explicit knowledge of\nsystem dynamics, often making it a daunting task. This work introduces a novel\nframework that learns a Lyapunov function directly from trajectory data,\nenabling the certification of stability for autonomous systems without needing\ndetailed system models. By parameterizing the Lyapunov candidate using a neural\nnetwork and ensuring positive definiteness through Cholesky factorization, our\napproach automatically identifies whether the system is stable under the given\ntrajectory. To address the challenges posed by noisy, real-world data, we allow\nfor controlled violations of the stability condition, focusing on maintaining\nhigh confidence in the stability certification process. Our results demonstrate\nthat this framework can provide data-driven stability guarantees, offering a\nrobust method for certifying the safety of robotic systems in dynamic,\nreal-world environments. This approach works without access to the internal\ncontrol algorithms, making it applicable even in situations where system\nbehavior is opaque or proprietary. The tool for learning the stability proof is\nopen-sourced by this research: https://github.com/HansOersted/stability.\n", "link": "http://arxiv.org/abs/2510.03123v1", "date": "2025-10-03", "relevancy": 2.0553, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5645}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5039}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Stability%20Certificate%20for%20Robotics%20in%20Real-World%20Environments&body=Title%3A%20Learning%20Stability%20Certificate%20for%20Robotics%20in%20Real-World%20Environments%0AAuthor%3A%20Zhe%20Shen%0AAbstract%3A%20%20%20Stability%20certificates%20play%20a%20critical%20role%20in%20ensuring%20the%20safety%20and%0Areliability%20of%20robotic%20systems.%20However%2C%20deriving%20these%20certificates%20for%0Acomplex%2C%20unknown%20systems%20has%20traditionally%20required%20explicit%20knowledge%20of%0Asystem%20dynamics%2C%20often%20making%20it%20a%20daunting%20task.%20This%20work%20introduces%20a%20novel%0Aframework%20that%20learns%20a%20Lyapunov%20function%20directly%20from%20trajectory%20data%2C%0Aenabling%20the%20certification%20of%20stability%20for%20autonomous%20systems%20without%20needing%0Adetailed%20system%20models.%20By%20parameterizing%20the%20Lyapunov%20candidate%20using%20a%20neural%0Anetwork%20and%20ensuring%20positive%20definiteness%20through%20Cholesky%20factorization%2C%20our%0Aapproach%20automatically%20identifies%20whether%20the%20system%20is%20stable%20under%20the%20given%0Atrajectory.%20To%20address%20the%20challenges%20posed%20by%20noisy%2C%20real-world%20data%2C%20we%20allow%0Afor%20controlled%20violations%20of%20the%20stability%20condition%2C%20focusing%20on%20maintaining%0Ahigh%20confidence%20in%20the%20stability%20certification%20process.%20Our%20results%20demonstrate%0Athat%20this%20framework%20can%20provide%20data-driven%20stability%20guarantees%2C%20offering%20a%0Arobust%20method%20for%20certifying%20the%20safety%20of%20robotic%20systems%20in%20dynamic%2C%0Areal-world%20environments.%20This%20approach%20works%20without%20access%20to%20the%20internal%0Acontrol%20algorithms%2C%20making%20it%20applicable%20even%20in%20situations%20where%20system%0Abehavior%20is%20opaque%20or%20proprietary.%20The%20tool%20for%20learning%20the%20stability%20proof%20is%0Aopen-sourced%20by%20this%20research%3A%20https%3A//github.com/HansOersted/stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Stability%2520Certificate%2520for%2520Robotics%2520in%2520Real-World%2520Environments%26entry.906535625%3DZhe%2520Shen%26entry.1292438233%3D%2520%2520Stability%2520certificates%2520play%2520a%2520critical%2520role%2520in%2520ensuring%2520the%2520safety%2520and%250Areliability%2520of%2520robotic%2520systems.%2520However%252C%2520deriving%2520these%2520certificates%2520for%250Acomplex%252C%2520unknown%2520systems%2520has%2520traditionally%2520required%2520explicit%2520knowledge%2520of%250Asystem%2520dynamics%252C%2520often%2520making%2520it%2520a%2520daunting%2520task.%2520This%2520work%2520introduces%2520a%2520novel%250Aframework%2520that%2520learns%2520a%2520Lyapunov%2520function%2520directly%2520from%2520trajectory%2520data%252C%250Aenabling%2520the%2520certification%2520of%2520stability%2520for%2520autonomous%2520systems%2520without%2520needing%250Adetailed%2520system%2520models.%2520By%2520parameterizing%2520the%2520Lyapunov%2520candidate%2520using%2520a%2520neural%250Anetwork%2520and%2520ensuring%2520positive%2520definiteness%2520through%2520Cholesky%2520factorization%252C%2520our%250Aapproach%2520automatically%2520identifies%2520whether%2520the%2520system%2520is%2520stable%2520under%2520the%2520given%250Atrajectory.%2520To%2520address%2520the%2520challenges%2520posed%2520by%2520noisy%252C%2520real-world%2520data%252C%2520we%2520allow%250Afor%2520controlled%2520violations%2520of%2520the%2520stability%2520condition%252C%2520focusing%2520on%2520maintaining%250Ahigh%2520confidence%2520in%2520the%2520stability%2520certification%2520process.%2520Our%2520results%2520demonstrate%250Athat%2520this%2520framework%2520can%2520provide%2520data-driven%2520stability%2520guarantees%252C%2520offering%2520a%250Arobust%2520method%2520for%2520certifying%2520the%2520safety%2520of%2520robotic%2520systems%2520in%2520dynamic%252C%250Areal-world%2520environments.%2520This%2520approach%2520works%2520without%2520access%2520to%2520the%2520internal%250Acontrol%2520algorithms%252C%2520making%2520it%2520applicable%2520even%2520in%2520situations%2520where%2520system%250Abehavior%2520is%2520opaque%2520or%2520proprietary.%2520The%2520tool%2520for%2520learning%2520the%2520stability%2520proof%2520is%250Aopen-sourced%2520by%2520this%2520research%253A%2520https%253A//github.com/HansOersted/stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Stability%20Certificate%20for%20Robotics%20in%20Real-World%20Environments&entry.906535625=Zhe%20Shen&entry.1292438233=%20%20Stability%20certificates%20play%20a%20critical%20role%20in%20ensuring%20the%20safety%20and%0Areliability%20of%20robotic%20systems.%20However%2C%20deriving%20these%20certificates%20for%0Acomplex%2C%20unknown%20systems%20has%20traditionally%20required%20explicit%20knowledge%20of%0Asystem%20dynamics%2C%20often%20making%20it%20a%20daunting%20task.%20This%20work%20introduces%20a%20novel%0Aframework%20that%20learns%20a%20Lyapunov%20function%20directly%20from%20trajectory%20data%2C%0Aenabling%20the%20certification%20of%20stability%20for%20autonomous%20systems%20without%20needing%0Adetailed%20system%20models.%20By%20parameterizing%20the%20Lyapunov%20candidate%20using%20a%20neural%0Anetwork%20and%20ensuring%20positive%20definiteness%20through%20Cholesky%20factorization%2C%20our%0Aapproach%20automatically%20identifies%20whether%20the%20system%20is%20stable%20under%20the%20given%0Atrajectory.%20To%20address%20the%20challenges%20posed%20by%20noisy%2C%20real-world%20data%2C%20we%20allow%0Afor%20controlled%20violations%20of%20the%20stability%20condition%2C%20focusing%20on%20maintaining%0Ahigh%20confidence%20in%20the%20stability%20certification%20process.%20Our%20results%20demonstrate%0Athat%20this%20framework%20can%20provide%20data-driven%20stability%20guarantees%2C%20offering%20a%0Arobust%20method%20for%20certifying%20the%20safety%20of%20robotic%20systems%20in%20dynamic%2C%0Areal-world%20environments.%20This%20approach%20works%20without%20access%20to%20the%20internal%0Acontrol%20algorithms%2C%20making%20it%20applicable%20even%20in%20situations%20where%20system%0Abehavior%20is%20opaque%20or%20proprietary.%20The%20tool%20for%20learning%20the%20stability%20proof%20is%0Aopen-sourced%20by%20this%20research%3A%20https%3A//github.com/HansOersted/stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03123v1&entry.124074799=Read"},
{"title": "IntrusionX: A Hybrid Convolutional-LSTM Deep Learning Framework with\n  Squirrel Search Optimization for Network Intrusion Detection", "author": "Ahsan Farabi and Muhaiminul Rashid Shad and Israt Khandaker", "abstract": "  Intrusion Detection Systems (IDS) face persistent challenges due to evolving\ncyberattacks, high-dimensional traffic data, and severe class imbalance in\nbenchmark datasets such as NSL-KDD. To address these issues, we propose\nIntrusionX, a hybrid deep learning framework that integrates Convolutional\nNeural Networks (CNNs) for local feature extraction and Long Short-Term Memory\n(LSTM) networks for temporal modeling. The architecture is further optimized\nusing the Squirrel Search Algorithm (SSA), enabling effective hyperparameter\ntuning while maintaining computational efficiency. Our pipeline incorporates\nrigorous preprocessing, stratified data splitting, and dynamic class weighting\nto enhance the detection of rare classes. Experimental evaluation on NSL-KDD\ndemonstrates that IntrusionX achieves 98% accuracy in binary classification and\n87% in 5-class classification, with significant improvements in minority class\nrecall (U2R: 71%, R2L: 93%). The novelty of IntrusionX lies in its\nreproducible, imbalance-aware design with metaheuristic optimization.\n", "link": "http://arxiv.org/abs/2510.00572v2", "date": "2025-10-03", "relevancy": 2.0405, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5264}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5224}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IntrusionX%3A%20A%20Hybrid%20Convolutional-LSTM%20Deep%20Learning%20Framework%20with%0A%20%20Squirrel%20Search%20Optimization%20for%20Network%20Intrusion%20Detection&body=Title%3A%20IntrusionX%3A%20A%20Hybrid%20Convolutional-LSTM%20Deep%20Learning%20Framework%20with%0A%20%20Squirrel%20Search%20Optimization%20for%20Network%20Intrusion%20Detection%0AAuthor%3A%20Ahsan%20Farabi%20and%20Muhaiminul%20Rashid%20Shad%20and%20Israt%20Khandaker%0AAbstract%3A%20%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20face%20persistent%20challenges%20due%20to%20evolving%0Acyberattacks%2C%20high-dimensional%20traffic%20data%2C%20and%20severe%20class%20imbalance%20in%0Abenchmark%20datasets%20such%20as%20NSL-KDD.%20To%20address%20these%20issues%2C%20we%20propose%0AIntrusionX%2C%20a%20hybrid%20deep%20learning%20framework%20that%20integrates%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20for%20local%20feature%20extraction%20and%20Long%20Short-Term%20Memory%0A%28LSTM%29%20networks%20for%20temporal%20modeling.%20The%20architecture%20is%20further%20optimized%0Ausing%20the%20Squirrel%20Search%20Algorithm%20%28SSA%29%2C%20enabling%20effective%20hyperparameter%0Atuning%20while%20maintaining%20computational%20efficiency.%20Our%20pipeline%20incorporates%0Arigorous%20preprocessing%2C%20stratified%20data%20splitting%2C%20and%20dynamic%20class%20weighting%0Ato%20enhance%20the%20detection%20of%20rare%20classes.%20Experimental%20evaluation%20on%20NSL-KDD%0Ademonstrates%20that%20IntrusionX%20achieves%2098%25%20accuracy%20in%20binary%20classification%20and%0A87%25%20in%205-class%20classification%2C%20with%20significant%20improvements%20in%20minority%20class%0Arecall%20%28U2R%3A%2071%25%2C%20R2L%3A%2093%25%29.%20The%20novelty%20of%20IntrusionX%20lies%20in%20its%0Areproducible%2C%20imbalance-aware%20design%20with%20metaheuristic%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrusionX%253A%2520A%2520Hybrid%2520Convolutional-LSTM%2520Deep%2520Learning%2520Framework%2520with%250A%2520%2520Squirrel%2520Search%2520Optimization%2520for%2520Network%2520Intrusion%2520Detection%26entry.906535625%3DAhsan%2520Farabi%2520and%2520Muhaiminul%2520Rashid%2520Shad%2520and%2520Israt%2520Khandaker%26entry.1292438233%3D%2520%2520Intrusion%2520Detection%2520Systems%2520%2528IDS%2529%2520face%2520persistent%2520challenges%2520due%2520to%2520evolving%250Acyberattacks%252C%2520high-dimensional%2520traffic%2520data%252C%2520and%2520severe%2520class%2520imbalance%2520in%250Abenchmark%2520datasets%2520such%2520as%2520NSL-KDD.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250AIntrusionX%252C%2520a%2520hybrid%2520deep%2520learning%2520framework%2520that%2520integrates%2520Convolutional%250ANeural%2520Networks%2520%2528CNNs%2529%2520for%2520local%2520feature%2520extraction%2520and%2520Long%2520Short-Term%2520Memory%250A%2528LSTM%2529%2520networks%2520for%2520temporal%2520modeling.%2520The%2520architecture%2520is%2520further%2520optimized%250Ausing%2520the%2520Squirrel%2520Search%2520Algorithm%2520%2528SSA%2529%252C%2520enabling%2520effective%2520hyperparameter%250Atuning%2520while%2520maintaining%2520computational%2520efficiency.%2520Our%2520pipeline%2520incorporates%250Arigorous%2520preprocessing%252C%2520stratified%2520data%2520splitting%252C%2520and%2520dynamic%2520class%2520weighting%250Ato%2520enhance%2520the%2520detection%2520of%2520rare%2520classes.%2520Experimental%2520evaluation%2520on%2520NSL-KDD%250Ademonstrates%2520that%2520IntrusionX%2520achieves%252098%2525%2520accuracy%2520in%2520binary%2520classification%2520and%250A87%2525%2520in%25205-class%2520classification%252C%2520with%2520significant%2520improvements%2520in%2520minority%2520class%250Arecall%2520%2528U2R%253A%252071%2525%252C%2520R2L%253A%252093%2525%2529.%2520The%2520novelty%2520of%2520IntrusionX%2520lies%2520in%2520its%250Areproducible%252C%2520imbalance-aware%2520design%2520with%2520metaheuristic%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntrusionX%3A%20A%20Hybrid%20Convolutional-LSTM%20Deep%20Learning%20Framework%20with%0A%20%20Squirrel%20Search%20Optimization%20for%20Network%20Intrusion%20Detection&entry.906535625=Ahsan%20Farabi%20and%20Muhaiminul%20Rashid%20Shad%20and%20Israt%20Khandaker&entry.1292438233=%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20face%20persistent%20challenges%20due%20to%20evolving%0Acyberattacks%2C%20high-dimensional%20traffic%20data%2C%20and%20severe%20class%20imbalance%20in%0Abenchmark%20datasets%20such%20as%20NSL-KDD.%20To%20address%20these%20issues%2C%20we%20propose%0AIntrusionX%2C%20a%20hybrid%20deep%20learning%20framework%20that%20integrates%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20for%20local%20feature%20extraction%20and%20Long%20Short-Term%20Memory%0A%28LSTM%29%20networks%20for%20temporal%20modeling.%20The%20architecture%20is%20further%20optimized%0Ausing%20the%20Squirrel%20Search%20Algorithm%20%28SSA%29%2C%20enabling%20effective%20hyperparameter%0Atuning%20while%20maintaining%20computational%20efficiency.%20Our%20pipeline%20incorporates%0Arigorous%20preprocessing%2C%20stratified%20data%20splitting%2C%20and%20dynamic%20class%20weighting%0Ato%20enhance%20the%20detection%20of%20rare%20classes.%20Experimental%20evaluation%20on%20NSL-KDD%0Ademonstrates%20that%20IntrusionX%20achieves%2098%25%20accuracy%20in%20binary%20classification%20and%0A87%25%20in%205-class%20classification%2C%20with%20significant%20improvements%20in%20minority%20class%0Arecall%20%28U2R%3A%2071%25%2C%20R2L%3A%2093%25%29.%20The%20novelty%20of%20IntrusionX%20lies%20in%20its%0Areproducible%2C%20imbalance-aware%20design%20with%20metaheuristic%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00572v2&entry.124074799=Read"},
{"title": "EFC++: Elastic Feature Consolidation with Prototype Re-balancing for\n  Cold Start Exemplar-free Incremental Learning", "author": "Simone Magistri and Tomaso Trinci and Albin Soutif-Cormerais and Joost van de Weijer and Andrew D. Bagdanov", "abstract": "  Exemplar-free Class Incremental Learning (EFCIL) aims to learn from a\nsequence of tasks without having access to previous task data. In this paper,\nwe consider the challenging Cold Start scenario in which insufficient data is\navailable in the first task to learn a high-quality backbone. This is\nespecially challenging for EFCIL since it requires high plasticity, resulting\nin feature drift which is difficult to compensate for in the exemplar-free\nsetting. To address this problem, we propose an effective approach to\nconsolidate feature representations by regularizing drift in directions highly\nrelevant to previous tasks while employing prototypes to reduce task-recency\nbias. Our approach, which we call Elastic Feature Consolidation++ (EFC++)\nexploits a tractable second-order approximation of feature drift based on a\nproposed Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in\nfeature space which we use to regularize feature drift in important directions\nand to update Gaussian prototypes. In addition, we introduce a post-training\nprototype re-balancing phase that updates classifiers to compensate for feature\ndrift. Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset,\nImageNet-1K and DomainNet demonstrate that EFC++ is better able to learn new\ntasks by maintaining model plasticity and significantly outperforms the\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2503.10439v3", "date": "2025-10-03", "relevancy": 2.0336, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5122}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5098}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EFC%2B%2B%3A%20Elastic%20Feature%20Consolidation%20with%20Prototype%20Re-balancing%20for%0A%20%20Cold%20Start%20Exemplar-free%20Incremental%20Learning&body=Title%3A%20EFC%2B%2B%3A%20Elastic%20Feature%20Consolidation%20with%20Prototype%20Re-balancing%20for%0A%20%20Cold%20Start%20Exemplar-free%20Incremental%20Learning%0AAuthor%3A%20Simone%20Magistri%20and%20Tomaso%20Trinci%20and%20Albin%20Soutif-Cormerais%20and%20Joost%20van%20de%20Weijer%20and%20Andrew%20D.%20Bagdanov%0AAbstract%3A%20%20%20Exemplar-free%20Class%20Incremental%20Learning%20%28EFCIL%29%20aims%20to%20learn%20from%20a%0Asequence%20of%20tasks%20without%20having%20access%20to%20previous%20task%20data.%20In%20this%20paper%2C%0Awe%20consider%20the%20challenging%20Cold%20Start%20scenario%20in%20which%20insufficient%20data%20is%0Aavailable%20in%20the%20first%20task%20to%20learn%20a%20high-quality%20backbone.%20This%20is%0Aespecially%20challenging%20for%20EFCIL%20since%20it%20requires%20high%20plasticity%2C%20resulting%0Ain%20feature%20drift%20which%20is%20difficult%20to%20compensate%20for%20in%20the%20exemplar-free%0Asetting.%20To%20address%20this%20problem%2C%20we%20propose%20an%20effective%20approach%20to%0Aconsolidate%20feature%20representations%20by%20regularizing%20drift%20in%20directions%20highly%0Arelevant%20to%20previous%20tasks%20while%20employing%20prototypes%20to%20reduce%20task-recency%0Abias.%20Our%20approach%2C%20which%20we%20call%20Elastic%20Feature%20Consolidation%2B%2B%20%28EFC%2B%2B%29%0Aexploits%20a%20tractable%20second-order%20approximation%20of%20feature%20drift%20based%20on%20a%0Aproposed%20Empirical%20Feature%20Matrix%20%28EFM%29.%20The%20EFM%20induces%20a%20pseudo-metric%20in%0Afeature%20space%20which%20we%20use%20to%20regularize%20feature%20drift%20in%20important%20directions%0Aand%20to%20update%20Gaussian%20prototypes.%20In%20addition%2C%20we%20introduce%20a%20post-training%0Aprototype%20re-balancing%20phase%20that%20updates%20classifiers%20to%20compensate%20for%20feature%0Adrift.%20Experimental%20results%20on%20CIFAR-100%2C%20Tiny-ImageNet%2C%20ImageNet-Subset%2C%0AImageNet-1K%20and%20DomainNet%20demonstrate%20that%20EFC%2B%2B%20is%20better%20able%20to%20learn%20new%0Atasks%20by%20maintaining%20model%20plasticity%20and%20significantly%20outperforms%20the%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10439v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEFC%252B%252B%253A%2520Elastic%2520Feature%2520Consolidation%2520with%2520Prototype%2520Re-balancing%2520for%250A%2520%2520Cold%2520Start%2520Exemplar-free%2520Incremental%2520Learning%26entry.906535625%3DSimone%2520Magistri%2520and%2520Tomaso%2520Trinci%2520and%2520Albin%2520Soutif-Cormerais%2520and%2520Joost%2520van%2520de%2520Weijer%2520and%2520Andrew%2520D.%2520Bagdanov%26entry.1292438233%3D%2520%2520Exemplar-free%2520Class%2520Incremental%2520Learning%2520%2528EFCIL%2529%2520aims%2520to%2520learn%2520from%2520a%250Asequence%2520of%2520tasks%2520without%2520having%2520access%2520to%2520previous%2520task%2520data.%2520In%2520this%2520paper%252C%250Awe%2520consider%2520the%2520challenging%2520Cold%2520Start%2520scenario%2520in%2520which%2520insufficient%2520data%2520is%250Aavailable%2520in%2520the%2520first%2520task%2520to%2520learn%2520a%2520high-quality%2520backbone.%2520This%2520is%250Aespecially%2520challenging%2520for%2520EFCIL%2520since%2520it%2520requires%2520high%2520plasticity%252C%2520resulting%250Ain%2520feature%2520drift%2520which%2520is%2520difficult%2520to%2520compensate%2520for%2520in%2520the%2520exemplar-free%250Asetting.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520an%2520effective%2520approach%2520to%250Aconsolidate%2520feature%2520representations%2520by%2520regularizing%2520drift%2520in%2520directions%2520highly%250Arelevant%2520to%2520previous%2520tasks%2520while%2520employing%2520prototypes%2520to%2520reduce%2520task-recency%250Abias.%2520Our%2520approach%252C%2520which%2520we%2520call%2520Elastic%2520Feature%2520Consolidation%252B%252B%2520%2528EFC%252B%252B%2529%250Aexploits%2520a%2520tractable%2520second-order%2520approximation%2520of%2520feature%2520drift%2520based%2520on%2520a%250Aproposed%2520Empirical%2520Feature%2520Matrix%2520%2528EFM%2529.%2520The%2520EFM%2520induces%2520a%2520pseudo-metric%2520in%250Afeature%2520space%2520which%2520we%2520use%2520to%2520regularize%2520feature%2520drift%2520in%2520important%2520directions%250Aand%2520to%2520update%2520Gaussian%2520prototypes.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520post-training%250Aprototype%2520re-balancing%2520phase%2520that%2520updates%2520classifiers%2520to%2520compensate%2520for%2520feature%250Adrift.%2520Experimental%2520results%2520on%2520CIFAR-100%252C%2520Tiny-ImageNet%252C%2520ImageNet-Subset%252C%250AImageNet-1K%2520and%2520DomainNet%2520demonstrate%2520that%2520EFC%252B%252B%2520is%2520better%2520able%2520to%2520learn%2520new%250Atasks%2520by%2520maintaining%2520model%2520plasticity%2520and%2520significantly%2520outperforms%2520the%250Astate-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10439v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EFC%2B%2B%3A%20Elastic%20Feature%20Consolidation%20with%20Prototype%20Re-balancing%20for%0A%20%20Cold%20Start%20Exemplar-free%20Incremental%20Learning&entry.906535625=Simone%20Magistri%20and%20Tomaso%20Trinci%20and%20Albin%20Soutif-Cormerais%20and%20Joost%20van%20de%20Weijer%20and%20Andrew%20D.%20Bagdanov&entry.1292438233=%20%20Exemplar-free%20Class%20Incremental%20Learning%20%28EFCIL%29%20aims%20to%20learn%20from%20a%0Asequence%20of%20tasks%20without%20having%20access%20to%20previous%20task%20data.%20In%20this%20paper%2C%0Awe%20consider%20the%20challenging%20Cold%20Start%20scenario%20in%20which%20insufficient%20data%20is%0Aavailable%20in%20the%20first%20task%20to%20learn%20a%20high-quality%20backbone.%20This%20is%0Aespecially%20challenging%20for%20EFCIL%20since%20it%20requires%20high%20plasticity%2C%20resulting%0Ain%20feature%20drift%20which%20is%20difficult%20to%20compensate%20for%20in%20the%20exemplar-free%0Asetting.%20To%20address%20this%20problem%2C%20we%20propose%20an%20effective%20approach%20to%0Aconsolidate%20feature%20representations%20by%20regularizing%20drift%20in%20directions%20highly%0Arelevant%20to%20previous%20tasks%20while%20employing%20prototypes%20to%20reduce%20task-recency%0Abias.%20Our%20approach%2C%20which%20we%20call%20Elastic%20Feature%20Consolidation%2B%2B%20%28EFC%2B%2B%29%0Aexploits%20a%20tractable%20second-order%20approximation%20of%20feature%20drift%20based%20on%20a%0Aproposed%20Empirical%20Feature%20Matrix%20%28EFM%29.%20The%20EFM%20induces%20a%20pseudo-metric%20in%0Afeature%20space%20which%20we%20use%20to%20regularize%20feature%20drift%20in%20important%20directions%0Aand%20to%20update%20Gaussian%20prototypes.%20In%20addition%2C%20we%20introduce%20a%20post-training%0Aprototype%20re-balancing%20phase%20that%20updates%20classifiers%20to%20compensate%20for%20feature%0Adrift.%20Experimental%20results%20on%20CIFAR-100%2C%20Tiny-ImageNet%2C%20ImageNet-Subset%2C%0AImageNet-1K%20and%20DomainNet%20demonstrate%20that%20EFC%2B%2B%20is%20better%20able%20to%20learn%20new%0Atasks%20by%20maintaining%20model%20plasticity%20and%20significantly%20outperforms%20the%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10439v3&entry.124074799=Read"},
{"title": "PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment", "author": "Edoardo Bianchi and Antonio Liotta", "abstract": "  Automated sports skill assessment requires capturing fundamental movement\npatterns that distinguish expert from novice performance, yet current video\nsampling methods disrupt the temporal continuity essential for proficiency\nevaluation. To this end, we introduce Proficiency-Aware Temporal Sampling\n(PATS), a novel sampling strategy that preserves complete fundamental movements\nwithin continuous temporal segments for multi-view skill assessment. PATS\nadaptively segments videos to ensure each analyzed portion contains full\nexecution of critical performance components, repeating this process across\nmultiple segments to maximize information coverage while maintaining temporal\ncoherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses\nthe state-of-the-art accuracy across all viewing configurations (+0.65% to\n+3.05%) and delivers substantial gains in challenging domains (+26.22%\nbouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that\nPATS successfully adapts to diverse activity characteristics-from\nhigh-frequency sampling for dynamic sports to fine-grained segmentation for\nsequential skills-demonstrating its effectiveness as an adaptive approach to\ntemporal sampling that advances automated skill assessment for real-world\napplications. Visit our project page at https://edowhite.github.io/PATS\n", "link": "http://arxiv.org/abs/2506.04996v5", "date": "2025-10-03", "relevancy": 2.0326, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5232}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5002}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PATS%3A%20Proficiency-Aware%20Temporal%20Sampling%20for%20Multi-View%20Sports%20Skill%0A%20%20Assessment&body=Title%3A%20PATS%3A%20Proficiency-Aware%20Temporal%20Sampling%20for%20Multi-View%20Sports%20Skill%0A%20%20Assessment%0AAuthor%3A%20Edoardo%20Bianchi%20and%20Antonio%20Liotta%0AAbstract%3A%20%20%20Automated%20sports%20skill%20assessment%20requires%20capturing%20fundamental%20movement%0Apatterns%20that%20distinguish%20expert%20from%20novice%20performance%2C%20yet%20current%20video%0Asampling%20methods%20disrupt%20the%20temporal%20continuity%20essential%20for%20proficiency%0Aevaluation.%20To%20this%20end%2C%20we%20introduce%20Proficiency-Aware%20Temporal%20Sampling%0A%28PATS%29%2C%20a%20novel%20sampling%20strategy%20that%20preserves%20complete%20fundamental%20movements%0Awithin%20continuous%20temporal%20segments%20for%20multi-view%20skill%20assessment.%20PATS%0Aadaptively%20segments%20videos%20to%20ensure%20each%20analyzed%20portion%20contains%20full%0Aexecution%20of%20critical%20performance%20components%2C%20repeating%20this%20process%20across%0Amultiple%20segments%20to%20maximize%20information%20coverage%20while%20maintaining%20temporal%0Acoherence.%20Evaluated%20on%20the%20EgoExo4D%20benchmark%20with%20SkillFormer%2C%20PATS%20surpasses%0Athe%20state-of-the-art%20accuracy%20across%20all%20viewing%20configurations%20%28%2B0.65%25%20to%0A%2B3.05%25%29%20and%20delivers%20substantial%20gains%20in%20challenging%20domains%20%28%2B26.22%25%0Abouldering%2C%20%2B2.39%25%20music%2C%20%2B1.13%25%20basketball%29.%20Systematic%20analysis%20reveals%20that%0APATS%20successfully%20adapts%20to%20diverse%20activity%20characteristics-from%0Ahigh-frequency%20sampling%20for%20dynamic%20sports%20to%20fine-grained%20segmentation%20for%0Asequential%20skills-demonstrating%20its%20effectiveness%20as%20an%20adaptive%20approach%20to%0Atemporal%20sampling%20that%20advances%20automated%20skill%20assessment%20for%20real-world%0Aapplications.%20Visit%20our%20project%20page%20at%20https%3A//edowhite.github.io/PATS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04996v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPATS%253A%2520Proficiency-Aware%2520Temporal%2520Sampling%2520for%2520Multi-View%2520Sports%2520Skill%250A%2520%2520Assessment%26entry.906535625%3DEdoardo%2520Bianchi%2520and%2520Antonio%2520Liotta%26entry.1292438233%3D%2520%2520Automated%2520sports%2520skill%2520assessment%2520requires%2520capturing%2520fundamental%2520movement%250Apatterns%2520that%2520distinguish%2520expert%2520from%2520novice%2520performance%252C%2520yet%2520current%2520video%250Asampling%2520methods%2520disrupt%2520the%2520temporal%2520continuity%2520essential%2520for%2520proficiency%250Aevaluation.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Proficiency-Aware%2520Temporal%2520Sampling%250A%2528PATS%2529%252C%2520a%2520novel%2520sampling%2520strategy%2520that%2520preserves%2520complete%2520fundamental%2520movements%250Awithin%2520continuous%2520temporal%2520segments%2520for%2520multi-view%2520skill%2520assessment.%2520PATS%250Aadaptively%2520segments%2520videos%2520to%2520ensure%2520each%2520analyzed%2520portion%2520contains%2520full%250Aexecution%2520of%2520critical%2520performance%2520components%252C%2520repeating%2520this%2520process%2520across%250Amultiple%2520segments%2520to%2520maximize%2520information%2520coverage%2520while%2520maintaining%2520temporal%250Acoherence.%2520Evaluated%2520on%2520the%2520EgoExo4D%2520benchmark%2520with%2520SkillFormer%252C%2520PATS%2520surpasses%250Athe%2520state-of-the-art%2520accuracy%2520across%2520all%2520viewing%2520configurations%2520%2528%252B0.65%2525%2520to%250A%252B3.05%2525%2529%2520and%2520delivers%2520substantial%2520gains%2520in%2520challenging%2520domains%2520%2528%252B26.22%2525%250Abouldering%252C%2520%252B2.39%2525%2520music%252C%2520%252B1.13%2525%2520basketball%2529.%2520Systematic%2520analysis%2520reveals%2520that%250APATS%2520successfully%2520adapts%2520to%2520diverse%2520activity%2520characteristics-from%250Ahigh-frequency%2520sampling%2520for%2520dynamic%2520sports%2520to%2520fine-grained%2520segmentation%2520for%250Asequential%2520skills-demonstrating%2520its%2520effectiveness%2520as%2520an%2520adaptive%2520approach%2520to%250Atemporal%2520sampling%2520that%2520advances%2520automated%2520skill%2520assessment%2520for%2520real-world%250Aapplications.%2520Visit%2520our%2520project%2520page%2520at%2520https%253A//edowhite.github.io/PATS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04996v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PATS%3A%20Proficiency-Aware%20Temporal%20Sampling%20for%20Multi-View%20Sports%20Skill%0A%20%20Assessment&entry.906535625=Edoardo%20Bianchi%20and%20Antonio%20Liotta&entry.1292438233=%20%20Automated%20sports%20skill%20assessment%20requires%20capturing%20fundamental%20movement%0Apatterns%20that%20distinguish%20expert%20from%20novice%20performance%2C%20yet%20current%20video%0Asampling%20methods%20disrupt%20the%20temporal%20continuity%20essential%20for%20proficiency%0Aevaluation.%20To%20this%20end%2C%20we%20introduce%20Proficiency-Aware%20Temporal%20Sampling%0A%28PATS%29%2C%20a%20novel%20sampling%20strategy%20that%20preserves%20complete%20fundamental%20movements%0Awithin%20continuous%20temporal%20segments%20for%20multi-view%20skill%20assessment.%20PATS%0Aadaptively%20segments%20videos%20to%20ensure%20each%20analyzed%20portion%20contains%20full%0Aexecution%20of%20critical%20performance%20components%2C%20repeating%20this%20process%20across%0Amultiple%20segments%20to%20maximize%20information%20coverage%20while%20maintaining%20temporal%0Acoherence.%20Evaluated%20on%20the%20EgoExo4D%20benchmark%20with%20SkillFormer%2C%20PATS%20surpasses%0Athe%20state-of-the-art%20accuracy%20across%20all%20viewing%20configurations%20%28%2B0.65%25%20to%0A%2B3.05%25%29%20and%20delivers%20substantial%20gains%20in%20challenging%20domains%20%28%2B26.22%25%0Abouldering%2C%20%2B2.39%25%20music%2C%20%2B1.13%25%20basketball%29.%20Systematic%20analysis%20reveals%20that%0APATS%20successfully%20adapts%20to%20diverse%20activity%20characteristics-from%0Ahigh-frequency%20sampling%20for%20dynamic%20sports%20to%20fine-grained%20segmentation%20for%0Asequential%20skills-demonstrating%20its%20effectiveness%20as%20an%20adaptive%20approach%20to%0Atemporal%20sampling%20that%20advances%20automated%20skill%20assessment%20for%20real-world%0Aapplications.%20Visit%20our%20project%20page%20at%20https%3A//edowhite.github.io/PATS%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04996v5&entry.124074799=Read"},
{"title": "To Distill or Decide? Understanding the Algorithmic Trade-off in\n  Partially Observable Reinforcement Learning", "author": "Yuda Song and Dhruv Rohatgi and Aarti Singh and J. Andrew Bagnell", "abstract": "  Partial observability is a notorious challenge in reinforcement learning\n(RL), due to the need to learn complex, history-dependent policies. Recent\nempirical successes have used privileged expert distillation--which leverages\navailability of latent state information during training (e.g., from a\nsimulator) to learn and imitate the optimal latent, Markovian policy--to\ndisentangle the task of \"learning to see\" from \"learning to act\". While expert\ndistillation is more computationally efficient than RL without latent state\ninformation, it also has well-documented failure modes. In this paper--through\na simple but instructive theoretical model called the perturbed Block MDP, and\ncontrolled experiments on challenging simulated locomotion tasks--we\ninvestigate the algorithmic trade-off between privileged expert distillation\nand standard RL without privileged information. Our main findings are: (1) The\ntrade-off empirically hinges on the stochasticity of the latent dynamics, as\ntheoretically predicted by contrasting approximate decodability with belief\ncontraction in the perturbed Block MDP; and (2) The optimal latent policy is\nnot always the best latent policy to distill. Our results suggest new\nguidelines for effectively exploiting privileged information, potentially\nadvancing the efficiency of policy learning across many practical partially\nobservable domains.\n", "link": "http://arxiv.org/abs/2510.03207v1", "date": "2025-10-03", "relevancy": 2.0231, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5142}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5121}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Distill%20or%20Decide%3F%20Understanding%20the%20Algorithmic%20Trade-off%20in%0A%20%20Partially%20Observable%20Reinforcement%20Learning&body=Title%3A%20To%20Distill%20or%20Decide%3F%20Understanding%20the%20Algorithmic%20Trade-off%20in%0A%20%20Partially%20Observable%20Reinforcement%20Learning%0AAuthor%3A%20Yuda%20Song%20and%20Dhruv%20Rohatgi%20and%20Aarti%20Singh%20and%20J.%20Andrew%20Bagnell%0AAbstract%3A%20%20%20Partial%20observability%20is%20a%20notorious%20challenge%20in%20reinforcement%20learning%0A%28RL%29%2C%20due%20to%20the%20need%20to%20learn%20complex%2C%20history-dependent%20policies.%20Recent%0Aempirical%20successes%20have%20used%20privileged%20expert%20distillation--which%20leverages%0Aavailability%20of%20latent%20state%20information%20during%20training%20%28e.g.%2C%20from%20a%0Asimulator%29%20to%20learn%20and%20imitate%20the%20optimal%20latent%2C%20Markovian%20policy--to%0Adisentangle%20the%20task%20of%20%22learning%20to%20see%22%20from%20%22learning%20to%20act%22.%20While%20expert%0Adistillation%20is%20more%20computationally%20efficient%20than%20RL%20without%20latent%20state%0Ainformation%2C%20it%20also%20has%20well-documented%20failure%20modes.%20In%20this%20paper--through%0Aa%20simple%20but%20instructive%20theoretical%20model%20called%20the%20perturbed%20Block%20MDP%2C%20and%0Acontrolled%20experiments%20on%20challenging%20simulated%20locomotion%20tasks--we%0Ainvestigate%20the%20algorithmic%20trade-off%20between%20privileged%20expert%20distillation%0Aand%20standard%20RL%20without%20privileged%20information.%20Our%20main%20findings%20are%3A%20%281%29%20The%0Atrade-off%20empirically%20hinges%20on%20the%20stochasticity%20of%20the%20latent%20dynamics%2C%20as%0Atheoretically%20predicted%20by%20contrasting%20approximate%20decodability%20with%20belief%0Acontraction%20in%20the%20perturbed%20Block%20MDP%3B%20and%20%282%29%20The%20optimal%20latent%20policy%20is%0Anot%20always%20the%20best%20latent%20policy%20to%20distill.%20Our%20results%20suggest%20new%0Aguidelines%20for%20effectively%20exploiting%20privileged%20information%2C%20potentially%0Aadvancing%20the%20efficiency%20of%20policy%20learning%20across%20many%20practical%20partially%0Aobservable%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Distill%2520or%2520Decide%253F%2520Understanding%2520the%2520Algorithmic%2520Trade-off%2520in%250A%2520%2520Partially%2520Observable%2520Reinforcement%2520Learning%26entry.906535625%3DYuda%2520Song%2520and%2520Dhruv%2520Rohatgi%2520and%2520Aarti%2520Singh%2520and%2520J.%2520Andrew%2520Bagnell%26entry.1292438233%3D%2520%2520Partial%2520observability%2520is%2520a%2520notorious%2520challenge%2520in%2520reinforcement%2520learning%250A%2528RL%2529%252C%2520due%2520to%2520the%2520need%2520to%2520learn%2520complex%252C%2520history-dependent%2520policies.%2520Recent%250Aempirical%2520successes%2520have%2520used%2520privileged%2520expert%2520distillation--which%2520leverages%250Aavailability%2520of%2520latent%2520state%2520information%2520during%2520training%2520%2528e.g.%252C%2520from%2520a%250Asimulator%2529%2520to%2520learn%2520and%2520imitate%2520the%2520optimal%2520latent%252C%2520Markovian%2520policy--to%250Adisentangle%2520the%2520task%2520of%2520%2522learning%2520to%2520see%2522%2520from%2520%2522learning%2520to%2520act%2522.%2520While%2520expert%250Adistillation%2520is%2520more%2520computationally%2520efficient%2520than%2520RL%2520without%2520latent%2520state%250Ainformation%252C%2520it%2520also%2520has%2520well-documented%2520failure%2520modes.%2520In%2520this%2520paper--through%250Aa%2520simple%2520but%2520instructive%2520theoretical%2520model%2520called%2520the%2520perturbed%2520Block%2520MDP%252C%2520and%250Acontrolled%2520experiments%2520on%2520challenging%2520simulated%2520locomotion%2520tasks--we%250Ainvestigate%2520the%2520algorithmic%2520trade-off%2520between%2520privileged%2520expert%2520distillation%250Aand%2520standard%2520RL%2520without%2520privileged%2520information.%2520Our%2520main%2520findings%2520are%253A%2520%25281%2529%2520The%250Atrade-off%2520empirically%2520hinges%2520on%2520the%2520stochasticity%2520of%2520the%2520latent%2520dynamics%252C%2520as%250Atheoretically%2520predicted%2520by%2520contrasting%2520approximate%2520decodability%2520with%2520belief%250Acontraction%2520in%2520the%2520perturbed%2520Block%2520MDP%253B%2520and%2520%25282%2529%2520The%2520optimal%2520latent%2520policy%2520is%250Anot%2520always%2520the%2520best%2520latent%2520policy%2520to%2520distill.%2520Our%2520results%2520suggest%2520new%250Aguidelines%2520for%2520effectively%2520exploiting%2520privileged%2520information%252C%2520potentially%250Aadvancing%2520the%2520efficiency%2520of%2520policy%2520learning%2520across%2520many%2520practical%2520partially%250Aobservable%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Distill%20or%20Decide%3F%20Understanding%20the%20Algorithmic%20Trade-off%20in%0A%20%20Partially%20Observable%20Reinforcement%20Learning&entry.906535625=Yuda%20Song%20and%20Dhruv%20Rohatgi%20and%20Aarti%20Singh%20and%20J.%20Andrew%20Bagnell&entry.1292438233=%20%20Partial%20observability%20is%20a%20notorious%20challenge%20in%20reinforcement%20learning%0A%28RL%29%2C%20due%20to%20the%20need%20to%20learn%20complex%2C%20history-dependent%20policies.%20Recent%0Aempirical%20successes%20have%20used%20privileged%20expert%20distillation--which%20leverages%0Aavailability%20of%20latent%20state%20information%20during%20training%20%28e.g.%2C%20from%20a%0Asimulator%29%20to%20learn%20and%20imitate%20the%20optimal%20latent%2C%20Markovian%20policy--to%0Adisentangle%20the%20task%20of%20%22learning%20to%20see%22%20from%20%22learning%20to%20act%22.%20While%20expert%0Adistillation%20is%20more%20computationally%20efficient%20than%20RL%20without%20latent%20state%0Ainformation%2C%20it%20also%20has%20well-documented%20failure%20modes.%20In%20this%20paper--through%0Aa%20simple%20but%20instructive%20theoretical%20model%20called%20the%20perturbed%20Block%20MDP%2C%20and%0Acontrolled%20experiments%20on%20challenging%20simulated%20locomotion%20tasks--we%0Ainvestigate%20the%20algorithmic%20trade-off%20between%20privileged%20expert%20distillation%0Aand%20standard%20RL%20without%20privileged%20information.%20Our%20main%20findings%20are%3A%20%281%29%20The%0Atrade-off%20empirically%20hinges%20on%20the%20stochasticity%20of%20the%20latent%20dynamics%2C%20as%0Atheoretically%20predicted%20by%20contrasting%20approximate%20decodability%20with%20belief%0Acontraction%20in%20the%20perturbed%20Block%20MDP%3B%20and%20%282%29%20The%20optimal%20latent%20policy%20is%0Anot%20always%20the%20best%20latent%20policy%20to%20distill.%20Our%20results%20suggest%20new%0Aguidelines%20for%20effectively%20exploiting%20privileged%20information%2C%20potentially%0Aadvancing%20the%20efficiency%20of%20policy%20learning%20across%20many%20practical%20partially%0Aobservable%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03207v1&entry.124074799=Read"},
{"title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning\n  with Verifiable Reward", "author": "Guanhua Huang and Tingqiang Xu and Mingze Wang and Qi Yi and Xue Gong and Siheng Li and Ruibin Xiong and Kejiao Li and Yuhao Jiang and Bo Zhou", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large\nLanguage Models in complex reasoning, yet its scalability is often hindered by\na training bottleneck where performance plateaus as policy entropy collapses,\nsignaling a loss of exploration. Previous methods typically address this by\nmaintaining high policy entropy, yet the precise mechanisms that govern\nmeaningful exploration have remained underexplored. Our analysis suggests that\nan unselective focus on entropy risks amplifying irrelevant tokens and\ndestabilizing training. This paper investigates the exploration dynamics within\nRLVR and identifies a key issue: the gradual elimination of valuable\nlow-probability exploratory tokens, which we term \\textbf{\\textit{reasoning\nsparks}}. We find that while abundant in pre-trained models, these sparks are\nsystematically extinguished during RLVR due to over-penalization, leading to a\ndegeneracy in exploration. To address this, we introduce Low-probability\nRegularization (Lp-Reg). Its core mechanism regularizes the policy towards a\nheuristic proxy distribution. This proxy is constructed by filtering out\npresumed noise tokens and re-normalizing the distribution over the remaining\ncandidates. The result is a less-noisy proxy where the probability of\n\\textit{reasoning sparks} is amplified, which then serves as a soft\nregularization target to shield these valuable tokens from elimination via KL\ndivergence. Experiments show that Lp-Reg enables stable on-policy training for\naround 1,000 steps, a regime where baseline entropy-control methods collapse.\nThis sustained exploration leads to state-of-the-art performance, achieving a\n$60.17\\%$ average accuracy on five math benchmarks, an improvement of $2.66\\%$\nover prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.\n", "link": "http://arxiv.org/abs/2510.03222v1", "date": "2025-10-03", "relevancy": 2.0006, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-probability%20Tokens%20Sustain%20Exploration%20in%20Reinforcement%20Learning%0A%20%20with%20Verifiable%20Reward&body=Title%3A%20Low-probability%20Tokens%20Sustain%20Exploration%20in%20Reinforcement%20Learning%0A%20%20with%20Verifiable%20Reward%0AAuthor%3A%20Guanhua%20Huang%20and%20Tingqiang%20Xu%20and%20Mingze%20Wang%20and%20Qi%20Yi%20and%20Xue%20Gong%20and%20Siheng%20Li%20and%20Ruibin%20Xiong%20and%20Kejiao%20Li%20and%20Yuhao%20Jiang%20and%20Bo%20Zhou%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20propelled%20Large%0ALanguage%20Models%20in%20complex%20reasoning%2C%20yet%20its%20scalability%20is%20often%20hindered%20by%0Aa%20training%20bottleneck%20where%20performance%20plateaus%20as%20policy%20entropy%20collapses%2C%0Asignaling%20a%20loss%20of%20exploration.%20Previous%20methods%20typically%20address%20this%20by%0Amaintaining%20high%20policy%20entropy%2C%20yet%20the%20precise%20mechanisms%20that%20govern%0Ameaningful%20exploration%20have%20remained%20underexplored.%20Our%20analysis%20suggests%20that%0Aan%20unselective%20focus%20on%20entropy%20risks%20amplifying%20irrelevant%20tokens%20and%0Adestabilizing%20training.%20This%20paper%20investigates%20the%20exploration%20dynamics%20within%0ARLVR%20and%20identifies%20a%20key%20issue%3A%20the%20gradual%20elimination%20of%20valuable%0Alow-probability%20exploratory%20tokens%2C%20which%20we%20term%20%5Ctextbf%7B%5Ctextit%7Breasoning%0Asparks%7D%7D.%20We%20find%20that%20while%20abundant%20in%20pre-trained%20models%2C%20these%20sparks%20are%0Asystematically%20extinguished%20during%20RLVR%20due%20to%20over-penalization%2C%20leading%20to%20a%0Adegeneracy%20in%20exploration.%20To%20address%20this%2C%20we%20introduce%20Low-probability%0ARegularization%20%28Lp-Reg%29.%20Its%20core%20mechanism%20regularizes%20the%20policy%20towards%20a%0Aheuristic%20proxy%20distribution.%20This%20proxy%20is%20constructed%20by%20filtering%20out%0Apresumed%20noise%20tokens%20and%20re-normalizing%20the%20distribution%20over%20the%20remaining%0Acandidates.%20The%20result%20is%20a%20less-noisy%20proxy%20where%20the%20probability%20of%0A%5Ctextit%7Breasoning%20sparks%7D%20is%20amplified%2C%20which%20then%20serves%20as%20a%20soft%0Aregularization%20target%20to%20shield%20these%20valuable%20tokens%20from%20elimination%20via%20KL%0Adivergence.%20Experiments%20show%20that%20Lp-Reg%20enables%20stable%20on-policy%20training%20for%0Aaround%201%2C000%20steps%2C%20a%20regime%20where%20baseline%20entropy-control%20methods%20collapse.%0AThis%20sustained%20exploration%20leads%20to%20state-of-the-art%20performance%2C%20achieving%20a%0A%2460.17%5C%25%24%20average%20accuracy%20on%20five%20math%20benchmarks%2C%20an%20improvement%20of%20%242.66%5C%25%24%0Aover%20prior%20methods.%20Code%20is%20available%20at%20https%3A//github.com/CarlanLark/Lp-Reg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-probability%2520Tokens%2520Sustain%2520Exploration%2520in%2520Reinforcement%2520Learning%250A%2520%2520with%2520Verifiable%2520Reward%26entry.906535625%3DGuanhua%2520Huang%2520and%2520Tingqiang%2520Xu%2520and%2520Mingze%2520Wang%2520and%2520Qi%2520Yi%2520and%2520Xue%2520Gong%2520and%2520Siheng%2520Li%2520and%2520Ruibin%2520Xiong%2520and%2520Kejiao%2520Li%2520and%2520Yuhao%2520Jiang%2520and%2520Bo%2520Zhou%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520propelled%2520Large%250ALanguage%2520Models%2520in%2520complex%2520reasoning%252C%2520yet%2520its%2520scalability%2520is%2520often%2520hindered%2520by%250Aa%2520training%2520bottleneck%2520where%2520performance%2520plateaus%2520as%2520policy%2520entropy%2520collapses%252C%250Asignaling%2520a%2520loss%2520of%2520exploration.%2520Previous%2520methods%2520typically%2520address%2520this%2520by%250Amaintaining%2520high%2520policy%2520entropy%252C%2520yet%2520the%2520precise%2520mechanisms%2520that%2520govern%250Ameaningful%2520exploration%2520have%2520remained%2520underexplored.%2520Our%2520analysis%2520suggests%2520that%250Aan%2520unselective%2520focus%2520on%2520entropy%2520risks%2520amplifying%2520irrelevant%2520tokens%2520and%250Adestabilizing%2520training.%2520This%2520paper%2520investigates%2520the%2520exploration%2520dynamics%2520within%250ARLVR%2520and%2520identifies%2520a%2520key%2520issue%253A%2520the%2520gradual%2520elimination%2520of%2520valuable%250Alow-probability%2520exploratory%2520tokens%252C%2520which%2520we%2520term%2520%255Ctextbf%257B%255Ctextit%257Breasoning%250Asparks%257D%257D.%2520We%2520find%2520that%2520while%2520abundant%2520in%2520pre-trained%2520models%252C%2520these%2520sparks%2520are%250Asystematically%2520extinguished%2520during%2520RLVR%2520due%2520to%2520over-penalization%252C%2520leading%2520to%2520a%250Adegeneracy%2520in%2520exploration.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Low-probability%250ARegularization%2520%2528Lp-Reg%2529.%2520Its%2520core%2520mechanism%2520regularizes%2520the%2520policy%2520towards%2520a%250Aheuristic%2520proxy%2520distribution.%2520This%2520proxy%2520is%2520constructed%2520by%2520filtering%2520out%250Apresumed%2520noise%2520tokens%2520and%2520re-normalizing%2520the%2520distribution%2520over%2520the%2520remaining%250Acandidates.%2520The%2520result%2520is%2520a%2520less-noisy%2520proxy%2520where%2520the%2520probability%2520of%250A%255Ctextit%257Breasoning%2520sparks%257D%2520is%2520amplified%252C%2520which%2520then%2520serves%2520as%2520a%2520soft%250Aregularization%2520target%2520to%2520shield%2520these%2520valuable%2520tokens%2520from%2520elimination%2520via%2520KL%250Adivergence.%2520Experiments%2520show%2520that%2520Lp-Reg%2520enables%2520stable%2520on-policy%2520training%2520for%250Aaround%25201%252C000%2520steps%252C%2520a%2520regime%2520where%2520baseline%2520entropy-control%2520methods%2520collapse.%250AThis%2520sustained%2520exploration%2520leads%2520to%2520state-of-the-art%2520performance%252C%2520achieving%2520a%250A%252460.17%255C%2525%2524%2520average%2520accuracy%2520on%2520five%2520math%2520benchmarks%252C%2520an%2520improvement%2520of%2520%25242.66%255C%2525%2524%250Aover%2520prior%2520methods.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/CarlanLark/Lp-Reg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-probability%20Tokens%20Sustain%20Exploration%20in%20Reinforcement%20Learning%0A%20%20with%20Verifiable%20Reward&entry.906535625=Guanhua%20Huang%20and%20Tingqiang%20Xu%20and%20Mingze%20Wang%20and%20Qi%20Yi%20and%20Xue%20Gong%20and%20Siheng%20Li%20and%20Ruibin%20Xiong%20and%20Kejiao%20Li%20and%20Yuhao%20Jiang%20and%20Bo%20Zhou&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20propelled%20Large%0ALanguage%20Models%20in%20complex%20reasoning%2C%20yet%20its%20scalability%20is%20often%20hindered%20by%0Aa%20training%20bottleneck%20where%20performance%20plateaus%20as%20policy%20entropy%20collapses%2C%0Asignaling%20a%20loss%20of%20exploration.%20Previous%20methods%20typically%20address%20this%20by%0Amaintaining%20high%20policy%20entropy%2C%20yet%20the%20precise%20mechanisms%20that%20govern%0Ameaningful%20exploration%20have%20remained%20underexplored.%20Our%20analysis%20suggests%20that%0Aan%20unselective%20focus%20on%20entropy%20risks%20amplifying%20irrelevant%20tokens%20and%0Adestabilizing%20training.%20This%20paper%20investigates%20the%20exploration%20dynamics%20within%0ARLVR%20and%20identifies%20a%20key%20issue%3A%20the%20gradual%20elimination%20of%20valuable%0Alow-probability%20exploratory%20tokens%2C%20which%20we%20term%20%5Ctextbf%7B%5Ctextit%7Breasoning%0Asparks%7D%7D.%20We%20find%20that%20while%20abundant%20in%20pre-trained%20models%2C%20these%20sparks%20are%0Asystematically%20extinguished%20during%20RLVR%20due%20to%20over-penalization%2C%20leading%20to%20a%0Adegeneracy%20in%20exploration.%20To%20address%20this%2C%20we%20introduce%20Low-probability%0ARegularization%20%28Lp-Reg%29.%20Its%20core%20mechanism%20regularizes%20the%20policy%20towards%20a%0Aheuristic%20proxy%20distribution.%20This%20proxy%20is%20constructed%20by%20filtering%20out%0Apresumed%20noise%20tokens%20and%20re-normalizing%20the%20distribution%20over%20the%20remaining%0Acandidates.%20The%20result%20is%20a%20less-noisy%20proxy%20where%20the%20probability%20of%0A%5Ctextit%7Breasoning%20sparks%7D%20is%20amplified%2C%20which%20then%20serves%20as%20a%20soft%0Aregularization%20target%20to%20shield%20these%20valuable%20tokens%20from%20elimination%20via%20KL%0Adivergence.%20Experiments%20show%20that%20Lp-Reg%20enables%20stable%20on-policy%20training%20for%0Aaround%201%2C000%20steps%2C%20a%20regime%20where%20baseline%20entropy-control%20methods%20collapse.%0AThis%20sustained%20exploration%20leads%20to%20state-of-the-art%20performance%2C%20achieving%20a%0A%2460.17%5C%25%24%20average%20accuracy%20on%20five%20math%20benchmarks%2C%20an%20improvement%20of%20%242.66%5C%25%24%0Aover%20prior%20methods.%20Code%20is%20available%20at%20https%3A//github.com/CarlanLark/Lp-Reg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03222v1&entry.124074799=Read"},
{"title": "Exponential Family Variational Flow Matching for Tabular Data Generation", "author": "Andr\u00e9s Guzm\u00e1n-Cordero and Floor Eijkelboom and Jan-Willem van de Meent", "abstract": "  While denoising diffusion and flow matching have driven major advances in\ngenerative modeling, their application to tabular data remains limited, despite\nits ubiquity in real-world applications. To this end, we develop TabbyFlow, a\nvariational Flow Matching (VFM) method for tabular data generation. To apply\nVFM to data with mixed continuous and discrete features, we introduce\nExponential Family Variational Flow Matching (EF-VFM), which represents\nheterogeneous data types using a general exponential family distribution. We\nhereby obtain an efficient, data-driven objective based on moment matching,\nenabling principled learning of probability paths over mixed continuous and\ndiscrete variables. We also establish a connection between variational flow\nmatching and generalized flow matching objectives based on Bregman divergences.\nEvaluation on tabular data benchmarks demonstrates state-of-the-art performance\ncompared to baselines.\n", "link": "http://arxiv.org/abs/2506.05940v4", "date": "2025-10-03", "relevancy": 1.9662, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5547}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4889}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exponential%20Family%20Variational%20Flow%20Matching%20for%20Tabular%20Data%20Generation&body=Title%3A%20Exponential%20Family%20Variational%20Flow%20Matching%20for%20Tabular%20Data%20Generation%0AAuthor%3A%20Andr%C3%A9s%20Guzm%C3%A1n-Cordero%20and%20Floor%20Eijkelboom%20and%20Jan-Willem%20van%20de%20Meent%0AAbstract%3A%20%20%20While%20denoising%20diffusion%20and%20flow%20matching%20have%20driven%20major%20advances%20in%0Agenerative%20modeling%2C%20their%20application%20to%20tabular%20data%20remains%20limited%2C%20despite%0Aits%20ubiquity%20in%20real-world%20applications.%20To%20this%20end%2C%20we%20develop%20TabbyFlow%2C%20a%0Avariational%20Flow%20Matching%20%28VFM%29%20method%20for%20tabular%20data%20generation.%20To%20apply%0AVFM%20to%20data%20with%20mixed%20continuous%20and%20discrete%20features%2C%20we%20introduce%0AExponential%20Family%20Variational%20Flow%20Matching%20%28EF-VFM%29%2C%20which%20represents%0Aheterogeneous%20data%20types%20using%20a%20general%20exponential%20family%20distribution.%20We%0Ahereby%20obtain%20an%20efficient%2C%20data-driven%20objective%20based%20on%20moment%20matching%2C%0Aenabling%20principled%20learning%20of%20probability%20paths%20over%20mixed%20continuous%20and%0Adiscrete%20variables.%20We%20also%20establish%20a%20connection%20between%20variational%20flow%0Amatching%20and%20generalized%20flow%20matching%20objectives%20based%20on%20Bregman%20divergences.%0AEvaluation%20on%20tabular%20data%20benchmarks%20demonstrates%20state-of-the-art%20performance%0Acompared%20to%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05940v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExponential%2520Family%2520Variational%2520Flow%2520Matching%2520for%2520Tabular%2520Data%2520Generation%26entry.906535625%3DAndr%25C3%25A9s%2520Guzm%25C3%25A1n-Cordero%2520and%2520Floor%2520Eijkelboom%2520and%2520Jan-Willem%2520van%2520de%2520Meent%26entry.1292438233%3D%2520%2520While%2520denoising%2520diffusion%2520and%2520flow%2520matching%2520have%2520driven%2520major%2520advances%2520in%250Agenerative%2520modeling%252C%2520their%2520application%2520to%2520tabular%2520data%2520remains%2520limited%252C%2520despite%250Aits%2520ubiquity%2520in%2520real-world%2520applications.%2520To%2520this%2520end%252C%2520we%2520develop%2520TabbyFlow%252C%2520a%250Avariational%2520Flow%2520Matching%2520%2528VFM%2529%2520method%2520for%2520tabular%2520data%2520generation.%2520To%2520apply%250AVFM%2520to%2520data%2520with%2520mixed%2520continuous%2520and%2520discrete%2520features%252C%2520we%2520introduce%250AExponential%2520Family%2520Variational%2520Flow%2520Matching%2520%2528EF-VFM%2529%252C%2520which%2520represents%250Aheterogeneous%2520data%2520types%2520using%2520a%2520general%2520exponential%2520family%2520distribution.%2520We%250Ahereby%2520obtain%2520an%2520efficient%252C%2520data-driven%2520objective%2520based%2520on%2520moment%2520matching%252C%250Aenabling%2520principled%2520learning%2520of%2520probability%2520paths%2520over%2520mixed%2520continuous%2520and%250Adiscrete%2520variables.%2520We%2520also%2520establish%2520a%2520connection%2520between%2520variational%2520flow%250Amatching%2520and%2520generalized%2520flow%2520matching%2520objectives%2520based%2520on%2520Bregman%2520divergences.%250AEvaluation%2520on%2520tabular%2520data%2520benchmarks%2520demonstrates%2520state-of-the-art%2520performance%250Acompared%2520to%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05940v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exponential%20Family%20Variational%20Flow%20Matching%20for%20Tabular%20Data%20Generation&entry.906535625=Andr%C3%A9s%20Guzm%C3%A1n-Cordero%20and%20Floor%20Eijkelboom%20and%20Jan-Willem%20van%20de%20Meent&entry.1292438233=%20%20While%20denoising%20diffusion%20and%20flow%20matching%20have%20driven%20major%20advances%20in%0Agenerative%20modeling%2C%20their%20application%20to%20tabular%20data%20remains%20limited%2C%20despite%0Aits%20ubiquity%20in%20real-world%20applications.%20To%20this%20end%2C%20we%20develop%20TabbyFlow%2C%20a%0Avariational%20Flow%20Matching%20%28VFM%29%20method%20for%20tabular%20data%20generation.%20To%20apply%0AVFM%20to%20data%20with%20mixed%20continuous%20and%20discrete%20features%2C%20we%20introduce%0AExponential%20Family%20Variational%20Flow%20Matching%20%28EF-VFM%29%2C%20which%20represents%0Aheterogeneous%20data%20types%20using%20a%20general%20exponential%20family%20distribution.%20We%0Ahereby%20obtain%20an%20efficient%2C%20data-driven%20objective%20based%20on%20moment%20matching%2C%0Aenabling%20principled%20learning%20of%20probability%20paths%20over%20mixed%20continuous%20and%0Adiscrete%20variables.%20We%20also%20establish%20a%20connection%20between%20variational%20flow%0Amatching%20and%20generalized%20flow%20matching%20objectives%20based%20on%20Bregman%20divergences.%0AEvaluation%20on%20tabular%20data%20benchmarks%20demonstrates%20state-of-the-art%20performance%0Acompared%20to%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05940v4&entry.124074799=Read"},
{"title": "A Survey of Deep Learning for Complex Speech Spectrograms", "author": "Yuying Xie and Zheng-Hua Tan", "abstract": "  Recent advancements in deep learning have significantly impacted the field of\nspeech signal processing, particularly in the analysis and manipulation of\ncomplex spectrograms. This survey provides a comprehensive overview of the\nstate-of-the-art techniques leveraging deep neural networks for processing\ncomplex spectrograms, which encapsulate both magnitude and phase information.\nWe begin by introducing complex spectrograms and their associated features for\nvarious speech processing tasks. Next, we examine the key components and\narchitectures of complex-valued neural networks, which are specifically\ndesigned to handle complex-valued data and have been applied to complex\nspectrogram processing. As recent studies have primarily focused on applying\nreal-valued neural networks to complex spectrograms, we revisit these\napproaches and their architectural designs. We then discuss various training\nstrategies and loss functions tailored for training neural networks to process\nand model complex spectrograms. The survey further examines key applications,\nincluding phase retrieval, speech enhancement, and speaker separation, where\ndeep learning has achieved significant progress by leveraging complex\nspectrograms or their derived feature representations. Additionally, we examine\nthe intersection of complex spectrograms with generative models. This survey\naims to serve as a valuable resource for researchers and practitioners in the\nfield of speech signal processing, deep learning and related fields.\n", "link": "http://arxiv.org/abs/2505.08694v2", "date": "2025-10-03", "relevancy": 1.9596, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Deep%20Learning%20for%20Complex%20Speech%20Spectrograms&body=Title%3A%20A%20Survey%20of%20Deep%20Learning%20for%20Complex%20Speech%20Spectrograms%0AAuthor%3A%20Yuying%20Xie%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20have%20significantly%20impacted%20the%20field%20of%0Aspeech%20signal%20processing%2C%20particularly%20in%20the%20analysis%20and%20manipulation%20of%0Acomplex%20spectrograms.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20the%0Astate-of-the-art%20techniques%20leveraging%20deep%20neural%20networks%20for%20processing%0Acomplex%20spectrograms%2C%20which%20encapsulate%20both%20magnitude%20and%20phase%20information.%0AWe%20begin%20by%20introducing%20complex%20spectrograms%20and%20their%20associated%20features%20for%0Avarious%20speech%20processing%20tasks.%20Next%2C%20we%20examine%20the%20key%20components%20and%0Aarchitectures%20of%20complex-valued%20neural%20networks%2C%20which%20are%20specifically%0Adesigned%20to%20handle%20complex-valued%20data%20and%20have%20been%20applied%20to%20complex%0Aspectrogram%20processing.%20As%20recent%20studies%20have%20primarily%20focused%20on%20applying%0Areal-valued%20neural%20networks%20to%20complex%20spectrograms%2C%20we%20revisit%20these%0Aapproaches%20and%20their%20architectural%20designs.%20We%20then%20discuss%20various%20training%0Astrategies%20and%20loss%20functions%20tailored%20for%20training%20neural%20networks%20to%20process%0Aand%20model%20complex%20spectrograms.%20The%20survey%20further%20examines%20key%20applications%2C%0Aincluding%20phase%20retrieval%2C%20speech%20enhancement%2C%20and%20speaker%20separation%2C%20where%0Adeep%20learning%20has%20achieved%20significant%20progress%20by%20leveraging%20complex%0Aspectrograms%20or%20their%20derived%20feature%20representations.%20Additionally%2C%20we%20examine%0Athe%20intersection%20of%20complex%20spectrograms%20with%20generative%20models.%20This%20survey%0Aaims%20to%20serve%20as%20a%20valuable%20resource%20for%20researchers%20and%20practitioners%20in%20the%0Afield%20of%20speech%20signal%20processing%2C%20deep%20learning%20and%20related%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08694v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Deep%2520Learning%2520for%2520Complex%2520Speech%2520Spectrograms%26entry.906535625%3DYuying%2520Xie%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520learning%2520have%2520significantly%2520impacted%2520the%2520field%2520of%250Aspeech%2520signal%2520processing%252C%2520particularly%2520in%2520the%2520analysis%2520and%2520manipulation%2520of%250Acomplex%2520spectrograms.%2520This%2520survey%2520provides%2520a%2520comprehensive%2520overview%2520of%2520the%250Astate-of-the-art%2520techniques%2520leveraging%2520deep%2520neural%2520networks%2520for%2520processing%250Acomplex%2520spectrograms%252C%2520which%2520encapsulate%2520both%2520magnitude%2520and%2520phase%2520information.%250AWe%2520begin%2520by%2520introducing%2520complex%2520spectrograms%2520and%2520their%2520associated%2520features%2520for%250Avarious%2520speech%2520processing%2520tasks.%2520Next%252C%2520we%2520examine%2520the%2520key%2520components%2520and%250Aarchitectures%2520of%2520complex-valued%2520neural%2520networks%252C%2520which%2520are%2520specifically%250Adesigned%2520to%2520handle%2520complex-valued%2520data%2520and%2520have%2520been%2520applied%2520to%2520complex%250Aspectrogram%2520processing.%2520As%2520recent%2520studies%2520have%2520primarily%2520focused%2520on%2520applying%250Areal-valued%2520neural%2520networks%2520to%2520complex%2520spectrograms%252C%2520we%2520revisit%2520these%250Aapproaches%2520and%2520their%2520architectural%2520designs.%2520We%2520then%2520discuss%2520various%2520training%250Astrategies%2520and%2520loss%2520functions%2520tailored%2520for%2520training%2520neural%2520networks%2520to%2520process%250Aand%2520model%2520complex%2520spectrograms.%2520The%2520survey%2520further%2520examines%2520key%2520applications%252C%250Aincluding%2520phase%2520retrieval%252C%2520speech%2520enhancement%252C%2520and%2520speaker%2520separation%252C%2520where%250Adeep%2520learning%2520has%2520achieved%2520significant%2520progress%2520by%2520leveraging%2520complex%250Aspectrograms%2520or%2520their%2520derived%2520feature%2520representations.%2520Additionally%252C%2520we%2520examine%250Athe%2520intersection%2520of%2520complex%2520spectrograms%2520with%2520generative%2520models.%2520This%2520survey%250Aaims%2520to%2520serve%2520as%2520a%2520valuable%2520resource%2520for%2520researchers%2520and%2520practitioners%2520in%2520the%250Afield%2520of%2520speech%2520signal%2520processing%252C%2520deep%2520learning%2520and%2520related%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08694v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Deep%20Learning%20for%20Complex%20Speech%20Spectrograms&entry.906535625=Yuying%20Xie%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20have%20significantly%20impacted%20the%20field%20of%0Aspeech%20signal%20processing%2C%20particularly%20in%20the%20analysis%20and%20manipulation%20of%0Acomplex%20spectrograms.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20the%0Astate-of-the-art%20techniques%20leveraging%20deep%20neural%20networks%20for%20processing%0Acomplex%20spectrograms%2C%20which%20encapsulate%20both%20magnitude%20and%20phase%20information.%0AWe%20begin%20by%20introducing%20complex%20spectrograms%20and%20their%20associated%20features%20for%0Avarious%20speech%20processing%20tasks.%20Next%2C%20we%20examine%20the%20key%20components%20and%0Aarchitectures%20of%20complex-valued%20neural%20networks%2C%20which%20are%20specifically%0Adesigned%20to%20handle%20complex-valued%20data%20and%20have%20been%20applied%20to%20complex%0Aspectrogram%20processing.%20As%20recent%20studies%20have%20primarily%20focused%20on%20applying%0Areal-valued%20neural%20networks%20to%20complex%20spectrograms%2C%20we%20revisit%20these%0Aapproaches%20and%20their%20architectural%20designs.%20We%20then%20discuss%20various%20training%0Astrategies%20and%20loss%20functions%20tailored%20for%20training%20neural%20networks%20to%20process%0Aand%20model%20complex%20spectrograms.%20The%20survey%20further%20examines%20key%20applications%2C%0Aincluding%20phase%20retrieval%2C%20speech%20enhancement%2C%20and%20speaker%20separation%2C%20where%0Adeep%20learning%20has%20achieved%20significant%20progress%20by%20leveraging%20complex%0Aspectrograms%20or%20their%20derived%20feature%20representations.%20Additionally%2C%20we%20examine%0Athe%20intersection%20of%20complex%20spectrograms%20with%20generative%20models.%20This%20survey%0Aaims%20to%20serve%20as%20a%20valuable%20resource%20for%20researchers%20and%20practitioners%20in%20the%0Afield%20of%20speech%20signal%20processing%2C%20deep%20learning%20and%20related%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08694v2&entry.124074799=Read"},
{"title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation", "author": "Edoardo Bianchi and Antonio Liotta", "abstract": "  Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment. Project page at https://edowhite.github.io/SkillFormer\n", "link": "http://arxiv.org/abs/2505.08665v5", "date": "2025-10-03", "relevancy": 1.664, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5624}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkillFormer%3A%20Unified%20Multi-View%20Video%20Understanding%20for%20Proficiency%0A%20%20Estimation&body=Title%3A%20SkillFormer%3A%20Unified%20Multi-View%20Video%20Understanding%20for%20Proficiency%0A%20%20Estimation%0AAuthor%3A%20Edoardo%20Bianchi%20and%20Antonio%20Liotta%0AAbstract%3A%20%20%20Assessing%20human%20skill%20levels%20in%20complex%20activities%20is%20a%20challenging%20problem%0Awith%20applications%20in%20sports%2C%20rehabilitation%2C%20and%20training.%20In%20this%20work%2C%20we%0Apresent%20SkillFormer%2C%20a%20parameter-efficient%20architecture%20for%20unified%20multi-view%0Aproficiency%20estimation%20from%20egocentric%20and%20exocentric%20videos.%20Building%20on%20the%0ATimeSformer%20backbone%2C%20SkillFormer%20introduces%20a%20CrossViewFusion%20module%20that%0Afuses%20view-specific%20features%20using%20multi-head%20cross-attention%2C%20learnable%0Agating%2C%20and%20adaptive%20self-calibration.%20We%20leverage%20Low-Rank%20Adaptation%20to%0Afine-tune%20only%20a%20small%20subset%20of%20parameters%2C%20significantly%20reducing%20training%0Acosts.%20In%20fact%2C%20when%20evaluated%20on%20the%20EgoExo4D%20dataset%2C%20SkillFormer%20achieves%0Astate-of-the-art%20accuracy%20in%20multi-view%20settings%20while%20demonstrating%20remarkable%0Acomputational%20efficiency%2C%20using%204.5x%20fewer%20parameters%20and%20requiring%203.75x%20fewer%0Atraining%20epochs%20than%20prior%20baselines.%20It%20excels%20in%20multiple%20structured%20tasks%2C%0Aconfirming%20the%20value%20of%20multi-view%20integration%20for%20fine-grained%20skill%0Aassessment.%20Project%20page%20at%20https%3A//edowhite.github.io/SkillFormer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08665v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkillFormer%253A%2520Unified%2520Multi-View%2520Video%2520Understanding%2520for%2520Proficiency%250A%2520%2520Estimation%26entry.906535625%3DEdoardo%2520Bianchi%2520and%2520Antonio%2520Liotta%26entry.1292438233%3D%2520%2520Assessing%2520human%2520skill%2520levels%2520in%2520complex%2520activities%2520is%2520a%2520challenging%2520problem%250Awith%2520applications%2520in%2520sports%252C%2520rehabilitation%252C%2520and%2520training.%2520In%2520this%2520work%252C%2520we%250Apresent%2520SkillFormer%252C%2520a%2520parameter-efficient%2520architecture%2520for%2520unified%2520multi-view%250Aproficiency%2520estimation%2520from%2520egocentric%2520and%2520exocentric%2520videos.%2520Building%2520on%2520the%250ATimeSformer%2520backbone%252C%2520SkillFormer%2520introduces%2520a%2520CrossViewFusion%2520module%2520that%250Afuses%2520view-specific%2520features%2520using%2520multi-head%2520cross-attention%252C%2520learnable%250Agating%252C%2520and%2520adaptive%2520self-calibration.%2520We%2520leverage%2520Low-Rank%2520Adaptation%2520to%250Afine-tune%2520only%2520a%2520small%2520subset%2520of%2520parameters%252C%2520significantly%2520reducing%2520training%250Acosts.%2520In%2520fact%252C%2520when%2520evaluated%2520on%2520the%2520EgoExo4D%2520dataset%252C%2520SkillFormer%2520achieves%250Astate-of-the-art%2520accuracy%2520in%2520multi-view%2520settings%2520while%2520demonstrating%2520remarkable%250Acomputational%2520efficiency%252C%2520using%25204.5x%2520fewer%2520parameters%2520and%2520requiring%25203.75x%2520fewer%250Atraining%2520epochs%2520than%2520prior%2520baselines.%2520It%2520excels%2520in%2520multiple%2520structured%2520tasks%252C%250Aconfirming%2520the%2520value%2520of%2520multi-view%2520integration%2520for%2520fine-grained%2520skill%250Aassessment.%2520Project%2520page%2520at%2520https%253A//edowhite.github.io/SkillFormer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08665v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkillFormer%3A%20Unified%20Multi-View%20Video%20Understanding%20for%20Proficiency%0A%20%20Estimation&entry.906535625=Edoardo%20Bianchi%20and%20Antonio%20Liotta&entry.1292438233=%20%20Assessing%20human%20skill%20levels%20in%20complex%20activities%20is%20a%20challenging%20problem%0Awith%20applications%20in%20sports%2C%20rehabilitation%2C%20and%20training.%20In%20this%20work%2C%20we%0Apresent%20SkillFormer%2C%20a%20parameter-efficient%20architecture%20for%20unified%20multi-view%0Aproficiency%20estimation%20from%20egocentric%20and%20exocentric%20videos.%20Building%20on%20the%0ATimeSformer%20backbone%2C%20SkillFormer%20introduces%20a%20CrossViewFusion%20module%20that%0Afuses%20view-specific%20features%20using%20multi-head%20cross-attention%2C%20learnable%0Agating%2C%20and%20adaptive%20self-calibration.%20We%20leverage%20Low-Rank%20Adaptation%20to%0Afine-tune%20only%20a%20small%20subset%20of%20parameters%2C%20significantly%20reducing%20training%0Acosts.%20In%20fact%2C%20when%20evaluated%20on%20the%20EgoExo4D%20dataset%2C%20SkillFormer%20achieves%0Astate-of-the-art%20accuracy%20in%20multi-view%20settings%20while%20demonstrating%20remarkable%0Acomputational%20efficiency%2C%20using%204.5x%20fewer%20parameters%20and%20requiring%203.75x%20fewer%0Atraining%20epochs%20than%20prior%20baselines.%20It%20excels%20in%20multiple%20structured%20tasks%2C%0Aconfirming%20the%20value%20of%20multi-view%20integration%20for%20fine-grained%20skill%0Aassessment.%20Project%20page%20at%20https%3A//edowhite.github.io/SkillFormer%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08665v5&entry.124074799=Read"},
{"title": "DMark: Order-Agnostic Watermarking for Diffusion Large Language Models", "author": "Linyu Wu and Linhao Zhong and Wenjie Qu and Yuexin Li and Yue Liu and Shengfang Zhai and Chunhua Shen and Jiaheng Zhang", "abstract": "  Diffusion large language models (dLLMs) offer faster generation than\nautoregressive models while maintaining comparable quality, but existing\nwatermarking methods fail on them due to their non-sequential decoding. Unlike\nautoregressive models that generate tokens left-to-right, dLLMs can finalize\ntokens in arbitrary order, breaking the causal design underlying traditional\nwatermarks. We present DMark, the first watermarking framework designed\nspecifically for dLLMs. DMark introduces three complementary strategies to\nrestore watermark detectability: predictive watermarking uses model-predicted\ntokens when actual context is unavailable; bidirectional watermarking exploits\nboth forward and backward dependencies unique to diffusion decoding; and\npredictive-bidirectional watermarking combines both approaches to maximize\ndetection strength. Experiments across multiple dLLMs show that DMark achieves\n92.0-99.5% detection rates at 1% false positive rate while maintaining text\nquality, compared to only 49.6-71.2% for naive adaptations of existing methods.\nDMark also demonstrates robustness against text manipulations, establishing\nthat effective watermarking is feasible for non-autoregressive language models.\n", "link": "http://arxiv.org/abs/2510.02902v1", "date": "2025-10-03", "relevancy": 1.0687, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5456}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5407}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DMark%3A%20Order-Agnostic%20Watermarking%20for%20Diffusion%20Large%20Language%20Models&body=Title%3A%20DMark%3A%20Order-Agnostic%20Watermarking%20for%20Diffusion%20Large%20Language%20Models%0AAuthor%3A%20Linyu%20Wu%20and%20Linhao%20Zhong%20and%20Wenjie%20Qu%20and%20Yuexin%20Li%20and%20Yue%20Liu%20and%20Shengfang%20Zhai%20and%20Chunhua%20Shen%20and%20Jiaheng%20Zhang%0AAbstract%3A%20%20%20Diffusion%20large%20language%20models%20%28dLLMs%29%20offer%20faster%20generation%20than%0Aautoregressive%20models%20while%20maintaining%20comparable%20quality%2C%20but%20existing%0Awatermarking%20methods%20fail%20on%20them%20due%20to%20their%20non-sequential%20decoding.%20Unlike%0Aautoregressive%20models%20that%20generate%20tokens%20left-to-right%2C%20dLLMs%20can%20finalize%0Atokens%20in%20arbitrary%20order%2C%20breaking%20the%20causal%20design%20underlying%20traditional%0Awatermarks.%20We%20present%20DMark%2C%20the%20first%20watermarking%20framework%20designed%0Aspecifically%20for%20dLLMs.%20DMark%20introduces%20three%20complementary%20strategies%20to%0Arestore%20watermark%20detectability%3A%20predictive%20watermarking%20uses%20model-predicted%0Atokens%20when%20actual%20context%20is%20unavailable%3B%20bidirectional%20watermarking%20exploits%0Aboth%20forward%20and%20backward%20dependencies%20unique%20to%20diffusion%20decoding%3B%20and%0Apredictive-bidirectional%20watermarking%20combines%20both%20approaches%20to%20maximize%0Adetection%20strength.%20Experiments%20across%20multiple%20dLLMs%20show%20that%20DMark%20achieves%0A92.0-99.5%25%20detection%20rates%20at%201%25%20false%20positive%20rate%20while%20maintaining%20text%0Aquality%2C%20compared%20to%20only%2049.6-71.2%25%20for%20naive%20adaptations%20of%20existing%20methods.%0ADMark%20also%20demonstrates%20robustness%20against%20text%20manipulations%2C%20establishing%0Athat%20effective%20watermarking%20is%20feasible%20for%20non-autoregressive%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDMark%253A%2520Order-Agnostic%2520Watermarking%2520for%2520Diffusion%2520Large%2520Language%2520Models%26entry.906535625%3DLinyu%2520Wu%2520and%2520Linhao%2520Zhong%2520and%2520Wenjie%2520Qu%2520and%2520Yuexin%2520Li%2520and%2520Yue%2520Liu%2520and%2520Shengfang%2520Zhai%2520and%2520Chunhua%2520Shen%2520and%2520Jiaheng%2520Zhang%26entry.1292438233%3D%2520%2520Diffusion%2520large%2520language%2520models%2520%2528dLLMs%2529%2520offer%2520faster%2520generation%2520than%250Aautoregressive%2520models%2520while%2520maintaining%2520comparable%2520quality%252C%2520but%2520existing%250Awatermarking%2520methods%2520fail%2520on%2520them%2520due%2520to%2520their%2520non-sequential%2520decoding.%2520Unlike%250Aautoregressive%2520models%2520that%2520generate%2520tokens%2520left-to-right%252C%2520dLLMs%2520can%2520finalize%250Atokens%2520in%2520arbitrary%2520order%252C%2520breaking%2520the%2520causal%2520design%2520underlying%2520traditional%250Awatermarks.%2520We%2520present%2520DMark%252C%2520the%2520first%2520watermarking%2520framework%2520designed%250Aspecifically%2520for%2520dLLMs.%2520DMark%2520introduces%2520three%2520complementary%2520strategies%2520to%250Arestore%2520watermark%2520detectability%253A%2520predictive%2520watermarking%2520uses%2520model-predicted%250Atokens%2520when%2520actual%2520context%2520is%2520unavailable%253B%2520bidirectional%2520watermarking%2520exploits%250Aboth%2520forward%2520and%2520backward%2520dependencies%2520unique%2520to%2520diffusion%2520decoding%253B%2520and%250Apredictive-bidirectional%2520watermarking%2520combines%2520both%2520approaches%2520to%2520maximize%250Adetection%2520strength.%2520Experiments%2520across%2520multiple%2520dLLMs%2520show%2520that%2520DMark%2520achieves%250A92.0-99.5%2525%2520detection%2520rates%2520at%25201%2525%2520false%2520positive%2520rate%2520while%2520maintaining%2520text%250Aquality%252C%2520compared%2520to%2520only%252049.6-71.2%2525%2520for%2520naive%2520adaptations%2520of%2520existing%2520methods.%250ADMark%2520also%2520demonstrates%2520robustness%2520against%2520text%2520manipulations%252C%2520establishing%250Athat%2520effective%2520watermarking%2520is%2520feasible%2520for%2520non-autoregressive%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DMark%3A%20Order-Agnostic%20Watermarking%20for%20Diffusion%20Large%20Language%20Models&entry.906535625=Linyu%20Wu%20and%20Linhao%20Zhong%20and%20Wenjie%20Qu%20and%20Yuexin%20Li%20and%20Yue%20Liu%20and%20Shengfang%20Zhai%20and%20Chunhua%20Shen%20and%20Jiaheng%20Zhang&entry.1292438233=%20%20Diffusion%20large%20language%20models%20%28dLLMs%29%20offer%20faster%20generation%20than%0Aautoregressive%20models%20while%20maintaining%20comparable%20quality%2C%20but%20existing%0Awatermarking%20methods%20fail%20on%20them%20due%20to%20their%20non-sequential%20decoding.%20Unlike%0Aautoregressive%20models%20that%20generate%20tokens%20left-to-right%2C%20dLLMs%20can%20finalize%0Atokens%20in%20arbitrary%20order%2C%20breaking%20the%20causal%20design%20underlying%20traditional%0Awatermarks.%20We%20present%20DMark%2C%20the%20first%20watermarking%20framework%20designed%0Aspecifically%20for%20dLLMs.%20DMark%20introduces%20three%20complementary%20strategies%20to%0Arestore%20watermark%20detectability%3A%20predictive%20watermarking%20uses%20model-predicted%0Atokens%20when%20actual%20context%20is%20unavailable%3B%20bidirectional%20watermarking%20exploits%0Aboth%20forward%20and%20backward%20dependencies%20unique%20to%20diffusion%20decoding%3B%20and%0Apredictive-bidirectional%20watermarking%20combines%20both%20approaches%20to%20maximize%0Adetection%20strength.%20Experiments%20across%20multiple%20dLLMs%20show%20that%20DMark%20achieves%0A92.0-99.5%25%20detection%20rates%20at%201%25%20false%20positive%20rate%20while%20maintaining%20text%0Aquality%2C%20compared%20to%20only%2049.6-71.2%25%20for%20naive%20adaptations%20of%20existing%20methods.%0ADMark%20also%20demonstrates%20robustness%20against%20text%20manipulations%2C%20establishing%0Athat%20effective%20watermarking%20is%20feasible%20for%20non-autoregressive%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02902v1&entry.124074799=Read"},
{"title": "oRANS: Online optimisation of RANS machine learning models with embedded\n  DNS data generation", "author": "Daniel Dehtyriov and Jonathan F. MacArt and Justin Sirignano", "abstract": "  Deep learning (DL) has demonstrated promise for accelerating and enhancing\nthe accuracy of flow physics simulations, but progress is constrained by the\nscarcity of high-fidelity training data, which is costly to generate and\ninherently limited to a small set of flow conditions. Consequently, closures\ntrained in the conventional offline paradigm tend to overfit and fail to\ngeneralise to new regimes. We introduce an online optimisation framework for\nDL-based Reynolds-averaged Navier--Stokes (RANS) closures which seeks to\naddress the challenge of limited high-fidelity datasets. Training data is\ndynamically generated by embedding a direct numerical simulation (DNS) within a\nsubdomain of the RANS domain. The RANS solution supplies boundary conditions to\nthe DNS, while the DNS provides mean velocity and turbulence statistics that\nare used to update a DL closure model during the simulation. This feedback loop\nenables the closure to adapt to the embedded DNS target flow, avoiding reliance\non precomputed datasets and improving out-of-distribution performance. The\napproach is demonstrated for the stochastically forced Burgers equation and for\nturbulent channel flow at $Re_\\tau=180$, $270$, $395$ and $590$ with varying\nembedded domain lengths $1\\leq L_0/L\\leq 8$. Online-optimised RANS models\nsignificantly outperform both offline-trained and literature-calibrated\nclosures, with accurate training achieved using modest DNS subdomains.\nPerformance degrades primarily when boundary-condition contamination dominates\nor when domains are too short to capture low-wavenumber modes. This framework\nprovides a scalable route to physics-informed machine learning closures,\nenabling data-adaptive reduced-order models that generalise across flow regimes\nwithout requiring large precomputed training datasets.\n", "link": "http://arxiv.org/abs/2510.02982v1", "date": "2025-10-03", "relevancy": 1.5307, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5709}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5169}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20oRANS%3A%20Online%20optimisation%20of%20RANS%20machine%20learning%20models%20with%20embedded%0A%20%20DNS%20data%20generation&body=Title%3A%20oRANS%3A%20Online%20optimisation%20of%20RANS%20machine%20learning%20models%20with%20embedded%0A%20%20DNS%20data%20generation%0AAuthor%3A%20Daniel%20Dehtyriov%20and%20Jonathan%20F.%20MacArt%20and%20Justin%20Sirignano%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20has%20demonstrated%20promise%20for%20accelerating%20and%20enhancing%0Athe%20accuracy%20of%20flow%20physics%20simulations%2C%20but%20progress%20is%20constrained%20by%20the%0Ascarcity%20of%20high-fidelity%20training%20data%2C%20which%20is%20costly%20to%20generate%20and%0Ainherently%20limited%20to%20a%20small%20set%20of%20flow%20conditions.%20Consequently%2C%20closures%0Atrained%20in%20the%20conventional%20offline%20paradigm%20tend%20to%20overfit%20and%20fail%20to%0Ageneralise%20to%20new%20regimes.%20We%20introduce%20an%20online%20optimisation%20framework%20for%0ADL-based%20Reynolds-averaged%20Navier--Stokes%20%28RANS%29%20closures%20which%20seeks%20to%0Aaddress%20the%20challenge%20of%20limited%20high-fidelity%20datasets.%20Training%20data%20is%0Adynamically%20generated%20by%20embedding%20a%20direct%20numerical%20simulation%20%28DNS%29%20within%20a%0Asubdomain%20of%20the%20RANS%20domain.%20The%20RANS%20solution%20supplies%20boundary%20conditions%20to%0Athe%20DNS%2C%20while%20the%20DNS%20provides%20mean%20velocity%20and%20turbulence%20statistics%20that%0Aare%20used%20to%20update%20a%20DL%20closure%20model%20during%20the%20simulation.%20This%20feedback%20loop%0Aenables%20the%20closure%20to%20adapt%20to%20the%20embedded%20DNS%20target%20flow%2C%20avoiding%20reliance%0Aon%20precomputed%20datasets%20and%20improving%20out-of-distribution%20performance.%20The%0Aapproach%20is%20demonstrated%20for%20the%20stochastically%20forced%20Burgers%20equation%20and%20for%0Aturbulent%20channel%20flow%20at%20%24Re_%5Ctau%3D180%24%2C%20%24270%24%2C%20%24395%24%20and%20%24590%24%20with%20varying%0Aembedded%20domain%20lengths%20%241%5Cleq%20L_0/L%5Cleq%208%24.%20Online-optimised%20RANS%20models%0Asignificantly%20outperform%20both%20offline-trained%20and%20literature-calibrated%0Aclosures%2C%20with%20accurate%20training%20achieved%20using%20modest%20DNS%20subdomains.%0APerformance%20degrades%20primarily%20when%20boundary-condition%20contamination%20dominates%0Aor%20when%20domains%20are%20too%20short%20to%20capture%20low-wavenumber%20modes.%20This%20framework%0Aprovides%20a%20scalable%20route%20to%20physics-informed%20machine%20learning%20closures%2C%0Aenabling%20data-adaptive%20reduced-order%20models%20that%20generalise%20across%20flow%20regimes%0Awithout%20requiring%20large%20precomputed%20training%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DoRANS%253A%2520Online%2520optimisation%2520of%2520RANS%2520machine%2520learning%2520models%2520with%2520embedded%250A%2520%2520DNS%2520data%2520generation%26entry.906535625%3DDaniel%2520Dehtyriov%2520and%2520Jonathan%2520F.%2520MacArt%2520and%2520Justin%2520Sirignano%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520has%2520demonstrated%2520promise%2520for%2520accelerating%2520and%2520enhancing%250Athe%2520accuracy%2520of%2520flow%2520physics%2520simulations%252C%2520but%2520progress%2520is%2520constrained%2520by%2520the%250Ascarcity%2520of%2520high-fidelity%2520training%2520data%252C%2520which%2520is%2520costly%2520to%2520generate%2520and%250Ainherently%2520limited%2520to%2520a%2520small%2520set%2520of%2520flow%2520conditions.%2520Consequently%252C%2520closures%250Atrained%2520in%2520the%2520conventional%2520offline%2520paradigm%2520tend%2520to%2520overfit%2520and%2520fail%2520to%250Ageneralise%2520to%2520new%2520regimes.%2520We%2520introduce%2520an%2520online%2520optimisation%2520framework%2520for%250ADL-based%2520Reynolds-averaged%2520Navier--Stokes%2520%2528RANS%2529%2520closures%2520which%2520seeks%2520to%250Aaddress%2520the%2520challenge%2520of%2520limited%2520high-fidelity%2520datasets.%2520Training%2520data%2520is%250Adynamically%2520generated%2520by%2520embedding%2520a%2520direct%2520numerical%2520simulation%2520%2528DNS%2529%2520within%2520a%250Asubdomain%2520of%2520the%2520RANS%2520domain.%2520The%2520RANS%2520solution%2520supplies%2520boundary%2520conditions%2520to%250Athe%2520DNS%252C%2520while%2520the%2520DNS%2520provides%2520mean%2520velocity%2520and%2520turbulence%2520statistics%2520that%250Aare%2520used%2520to%2520update%2520a%2520DL%2520closure%2520model%2520during%2520the%2520simulation.%2520This%2520feedback%2520loop%250Aenables%2520the%2520closure%2520to%2520adapt%2520to%2520the%2520embedded%2520DNS%2520target%2520flow%252C%2520avoiding%2520reliance%250Aon%2520precomputed%2520datasets%2520and%2520improving%2520out-of-distribution%2520performance.%2520The%250Aapproach%2520is%2520demonstrated%2520for%2520the%2520stochastically%2520forced%2520Burgers%2520equation%2520and%2520for%250Aturbulent%2520channel%2520flow%2520at%2520%2524Re_%255Ctau%253D180%2524%252C%2520%2524270%2524%252C%2520%2524395%2524%2520and%2520%2524590%2524%2520with%2520varying%250Aembedded%2520domain%2520lengths%2520%25241%255Cleq%2520L_0/L%255Cleq%25208%2524.%2520Online-optimised%2520RANS%2520models%250Asignificantly%2520outperform%2520both%2520offline-trained%2520and%2520literature-calibrated%250Aclosures%252C%2520with%2520accurate%2520training%2520achieved%2520using%2520modest%2520DNS%2520subdomains.%250APerformance%2520degrades%2520primarily%2520when%2520boundary-condition%2520contamination%2520dominates%250Aor%2520when%2520domains%2520are%2520too%2520short%2520to%2520capture%2520low-wavenumber%2520modes.%2520This%2520framework%250Aprovides%2520a%2520scalable%2520route%2520to%2520physics-informed%2520machine%2520learning%2520closures%252C%250Aenabling%2520data-adaptive%2520reduced-order%2520models%2520that%2520generalise%2520across%2520flow%2520regimes%250Awithout%2520requiring%2520large%2520precomputed%2520training%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=oRANS%3A%20Online%20optimisation%20of%20RANS%20machine%20learning%20models%20with%20embedded%0A%20%20DNS%20data%20generation&entry.906535625=Daniel%20Dehtyriov%20and%20Jonathan%20F.%20MacArt%20and%20Justin%20Sirignano&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20has%20demonstrated%20promise%20for%20accelerating%20and%20enhancing%0Athe%20accuracy%20of%20flow%20physics%20simulations%2C%20but%20progress%20is%20constrained%20by%20the%0Ascarcity%20of%20high-fidelity%20training%20data%2C%20which%20is%20costly%20to%20generate%20and%0Ainherently%20limited%20to%20a%20small%20set%20of%20flow%20conditions.%20Consequently%2C%20closures%0Atrained%20in%20the%20conventional%20offline%20paradigm%20tend%20to%20overfit%20and%20fail%20to%0Ageneralise%20to%20new%20regimes.%20We%20introduce%20an%20online%20optimisation%20framework%20for%0ADL-based%20Reynolds-averaged%20Navier--Stokes%20%28RANS%29%20closures%20which%20seeks%20to%0Aaddress%20the%20challenge%20of%20limited%20high-fidelity%20datasets.%20Training%20data%20is%0Adynamically%20generated%20by%20embedding%20a%20direct%20numerical%20simulation%20%28DNS%29%20within%20a%0Asubdomain%20of%20the%20RANS%20domain.%20The%20RANS%20solution%20supplies%20boundary%20conditions%20to%0Athe%20DNS%2C%20while%20the%20DNS%20provides%20mean%20velocity%20and%20turbulence%20statistics%20that%0Aare%20used%20to%20update%20a%20DL%20closure%20model%20during%20the%20simulation.%20This%20feedback%20loop%0Aenables%20the%20closure%20to%20adapt%20to%20the%20embedded%20DNS%20target%20flow%2C%20avoiding%20reliance%0Aon%20precomputed%20datasets%20and%20improving%20out-of-distribution%20performance.%20The%0Aapproach%20is%20demonstrated%20for%20the%20stochastically%20forced%20Burgers%20equation%20and%20for%0Aturbulent%20channel%20flow%20at%20%24Re_%5Ctau%3D180%24%2C%20%24270%24%2C%20%24395%24%20and%20%24590%24%20with%20varying%0Aembedded%20domain%20lengths%20%241%5Cleq%20L_0/L%5Cleq%208%24.%20Online-optimised%20RANS%20models%0Asignificantly%20outperform%20both%20offline-trained%20and%20literature-calibrated%0Aclosures%2C%20with%20accurate%20training%20achieved%20using%20modest%20DNS%20subdomains.%0APerformance%20degrades%20primarily%20when%20boundary-condition%20contamination%20dominates%0Aor%20when%20domains%20are%20too%20short%20to%20capture%20low-wavenumber%20modes.%20This%20framework%0Aprovides%20a%20scalable%20route%20to%20physics-informed%20machine%20learning%20closures%2C%0Aenabling%20data-adaptive%20reduced-order%20models%20that%20generalise%20across%20flow%20regimes%0Awithout%20requiring%20large%20precomputed%20training%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02982v1&entry.124074799=Read"},
{"title": "ROGR: Relightable 3D Objects using Generative Relighting", "author": "Jiapeng Tang and Matthew Lavine and Dor Verbin and Stephan J. Garbin and Matthias Nie\u00dfner and Ricardo Martin Brualla and Pratul P. Srinivasan and Philipp Henzler", "abstract": "  We introduce ROGR, a novel approach that reconstructs a relightable 3D model\nof an object captured from multiple views, driven by a generative relighting\nmodel that simulates the effects of placing the object under novel environment\nilluminations. Our method samples the appearance of the object under multiple\nlighting environments, creating a dataset that is used to train a\nlighting-conditioned Neural Radiance Field (NeRF) that outputs the object's\nappearance under any input environmental lighting. The lighting-conditioned\nNeRF uses a novel dual-branch architecture to encode the general lighting\neffects and specularities separately. The optimized lighting-conditioned NeRF\nenables efficient feed-forward relighting under arbitrary environment maps\nwithout requiring per-illumination optimization or light transport simulation.\nWe evaluate our approach on the established TensoIR and Stanford-ORB datasets,\nwhere it improves upon the state-of-the-art on most metrics, and showcase our\napproach on real-world object captures.\n", "link": "http://arxiv.org/abs/2510.03163v1", "date": "2025-10-03", "relevancy": 1.8009, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6153}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6103}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROGR%3A%20Relightable%203D%20Objects%20using%20Generative%20Relighting&body=Title%3A%20ROGR%3A%20Relightable%203D%20Objects%20using%20Generative%20Relighting%0AAuthor%3A%20Jiapeng%20Tang%20and%20Matthew%20Lavine%20and%20Dor%20Verbin%20and%20Stephan%20J.%20Garbin%20and%20Matthias%20Nie%C3%9Fner%20and%20Ricardo%20Martin%20Brualla%20and%20Pratul%20P.%20Srinivasan%20and%20Philipp%20Henzler%0AAbstract%3A%20%20%20We%20introduce%20ROGR%2C%20a%20novel%20approach%20that%20reconstructs%20a%20relightable%203D%20model%0Aof%20an%20object%20captured%20from%20multiple%20views%2C%20driven%20by%20a%20generative%20relighting%0Amodel%20that%20simulates%20the%20effects%20of%20placing%20the%20object%20under%20novel%20environment%0Ailluminations.%20Our%20method%20samples%20the%20appearance%20of%20the%20object%20under%20multiple%0Alighting%20environments%2C%20creating%20a%20dataset%20that%20is%20used%20to%20train%20a%0Alighting-conditioned%20Neural%20Radiance%20Field%20%28NeRF%29%20that%20outputs%20the%20object%27s%0Aappearance%20under%20any%20input%20environmental%20lighting.%20The%20lighting-conditioned%0ANeRF%20uses%20a%20novel%20dual-branch%20architecture%20to%20encode%20the%20general%20lighting%0Aeffects%20and%20specularities%20separately.%20The%20optimized%20lighting-conditioned%20NeRF%0Aenables%20efficient%20feed-forward%20relighting%20under%20arbitrary%20environment%20maps%0Awithout%20requiring%20per-illumination%20optimization%20or%20light%20transport%20simulation.%0AWe%20evaluate%20our%20approach%20on%20the%20established%20TensoIR%20and%20Stanford-ORB%20datasets%2C%0Awhere%20it%20improves%20upon%20the%20state-of-the-art%20on%20most%20metrics%2C%20and%20showcase%20our%0Aapproach%20on%20real-world%20object%20captures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROGR%253A%2520Relightable%25203D%2520Objects%2520using%2520Generative%2520Relighting%26entry.906535625%3DJiapeng%2520Tang%2520and%2520Matthew%2520Lavine%2520and%2520Dor%2520Verbin%2520and%2520Stephan%2520J.%2520Garbin%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Ricardo%2520Martin%2520Brualla%2520and%2520Pratul%2520P.%2520Srinivasan%2520and%2520Philipp%2520Henzler%26entry.1292438233%3D%2520%2520We%2520introduce%2520ROGR%252C%2520a%2520novel%2520approach%2520that%2520reconstructs%2520a%2520relightable%25203D%2520model%250Aof%2520an%2520object%2520captured%2520from%2520multiple%2520views%252C%2520driven%2520by%2520a%2520generative%2520relighting%250Amodel%2520that%2520simulates%2520the%2520effects%2520of%2520placing%2520the%2520object%2520under%2520novel%2520environment%250Ailluminations.%2520Our%2520method%2520samples%2520the%2520appearance%2520of%2520the%2520object%2520under%2520multiple%250Alighting%2520environments%252C%2520creating%2520a%2520dataset%2520that%2520is%2520used%2520to%2520train%2520a%250Alighting-conditioned%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520that%2520outputs%2520the%2520object%2527s%250Aappearance%2520under%2520any%2520input%2520environmental%2520lighting.%2520The%2520lighting-conditioned%250ANeRF%2520uses%2520a%2520novel%2520dual-branch%2520architecture%2520to%2520encode%2520the%2520general%2520lighting%250Aeffects%2520and%2520specularities%2520separately.%2520The%2520optimized%2520lighting-conditioned%2520NeRF%250Aenables%2520efficient%2520feed-forward%2520relighting%2520under%2520arbitrary%2520environment%2520maps%250Awithout%2520requiring%2520per-illumination%2520optimization%2520or%2520light%2520transport%2520simulation.%250AWe%2520evaluate%2520our%2520approach%2520on%2520the%2520established%2520TensoIR%2520and%2520Stanford-ORB%2520datasets%252C%250Awhere%2520it%2520improves%2520upon%2520the%2520state-of-the-art%2520on%2520most%2520metrics%252C%2520and%2520showcase%2520our%250Aapproach%2520on%2520real-world%2520object%2520captures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROGR%3A%20Relightable%203D%20Objects%20using%20Generative%20Relighting&entry.906535625=Jiapeng%20Tang%20and%20Matthew%20Lavine%20and%20Dor%20Verbin%20and%20Stephan%20J.%20Garbin%20and%20Matthias%20Nie%C3%9Fner%20and%20Ricardo%20Martin%20Brualla%20and%20Pratul%20P.%20Srinivasan%20and%20Philipp%20Henzler&entry.1292438233=%20%20We%20introduce%20ROGR%2C%20a%20novel%20approach%20that%20reconstructs%20a%20relightable%203D%20model%0Aof%20an%20object%20captured%20from%20multiple%20views%2C%20driven%20by%20a%20generative%20relighting%0Amodel%20that%20simulates%20the%20effects%20of%20placing%20the%20object%20under%20novel%20environment%0Ailluminations.%20Our%20method%20samples%20the%20appearance%20of%20the%20object%20under%20multiple%0Alighting%20environments%2C%20creating%20a%20dataset%20that%20is%20used%20to%20train%20a%0Alighting-conditioned%20Neural%20Radiance%20Field%20%28NeRF%29%20that%20outputs%20the%20object%27s%0Aappearance%20under%20any%20input%20environmental%20lighting.%20The%20lighting-conditioned%0ANeRF%20uses%20a%20novel%20dual-branch%20architecture%20to%20encode%20the%20general%20lighting%0Aeffects%20and%20specularities%20separately.%20The%20optimized%20lighting-conditioned%20NeRF%0Aenables%20efficient%20feed-forward%20relighting%20under%20arbitrary%20environment%20maps%0Awithout%20requiring%20per-illumination%20optimization%20or%20light%20transport%20simulation.%0AWe%20evaluate%20our%20approach%20on%20the%20established%20TensoIR%20and%20Stanford-ORB%20datasets%2C%0Awhere%20it%20improves%20upon%20the%20state-of-the-art%20on%20most%20metrics%2C%20and%20showcase%20our%0Aapproach%20on%20real-world%20object%20captures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03163v1&entry.124074799=Read"},
{"title": "Bayesian E(3)-Equivariant Interatomic Potential with Iterative\n  Restratification of Many-body Message Passing", "author": "Soohaeng Yoo Willow and Tae Hyeon Park and Gi Beom Sim and Sung Wook Moon and Seung Kyu Min and D. ChangMo Yang and Hyun Woo Kim and Juho Lee and Chang Woo Myung", "abstract": "  Machine learning potentials (MLPs) have become essential for large-scale\natomistic simulations, enabling ab initio-level accuracy with computational\nefficiency. However, current MLPs struggle with uncertainty quantification,\nlimiting their reliability for active learning, calibration, and\nout-of-distribution (OOD) detection. We address these challenges by developing\nBayesian E(3) equivariant MLPs with iterative restratification of many-body\nmessage passing. Our approach introduces the joint energy-force negative\nlog-likelihood (NLL$_\\text{JEF}$) loss function, which explicitly models\nuncertainty in both energies and interatomic forces, yielding superior accuracy\ncompared to conventional NLL losses. We systematically benchmark multiple\nBayesian approaches, including deep ensembles with mean-variance estimation,\nstochastic weight averaging Gaussian, improved variational online Newton, and\nlaplace approximation by evaluating their performance on uncertainty\nprediction, OOD detection, calibration, and active learning tasks. We further\ndemonstrate that NLL$_\\text{JEF}$ facilitates efficient active learning by\nquantifying energy and force uncertainties. Using Bayesian active learning by\ndisagreement (BALD), our framework outperforms random sampling and\nenergy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs\nachieve competitive accuracy with state-of-the-art models while enabling\nuncertainty-guided active learning, OOD detection, and energy/forces\ncalibration. This work establishes Bayesian equivariant neural networks as a\npowerful framework for developing uncertainty-aware MLPs for atomistic\nsimulations at scale.\n", "link": "http://arxiv.org/abs/2510.03046v1", "date": "2025-10-03", "relevancy": 1.6086, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5694}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5626}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20E%283%29-Equivariant%20Interatomic%20Potential%20with%20Iterative%0A%20%20Restratification%20of%20Many-body%20Message%20Passing&body=Title%3A%20Bayesian%20E%283%29-Equivariant%20Interatomic%20Potential%20with%20Iterative%0A%20%20Restratification%20of%20Many-body%20Message%20Passing%0AAuthor%3A%20Soohaeng%20Yoo%20Willow%20and%20Tae%20Hyeon%20Park%20and%20Gi%20Beom%20Sim%20and%20Sung%20Wook%20Moon%20and%20Seung%20Kyu%20Min%20and%20D.%20ChangMo%20Yang%20and%20Hyun%20Woo%20Kim%20and%20Juho%20Lee%20and%20Chang%20Woo%20Myung%0AAbstract%3A%20%20%20Machine%20learning%20potentials%20%28MLPs%29%20have%20become%20essential%20for%20large-scale%0Aatomistic%20simulations%2C%20enabling%20ab%20initio-level%20accuracy%20with%20computational%0Aefficiency.%20However%2C%20current%20MLPs%20struggle%20with%20uncertainty%20quantification%2C%0Alimiting%20their%20reliability%20for%20active%20learning%2C%20calibration%2C%20and%0Aout-of-distribution%20%28OOD%29%20detection.%20We%20address%20these%20challenges%20by%20developing%0ABayesian%20E%283%29%20equivariant%20MLPs%20with%20iterative%20restratification%20of%20many-body%0Amessage%20passing.%20Our%20approach%20introduces%20the%20joint%20energy-force%20negative%0Alog-likelihood%20%28NLL%24_%5Ctext%7BJEF%7D%24%29%20loss%20function%2C%20which%20explicitly%20models%0Auncertainty%20in%20both%20energies%20and%20interatomic%20forces%2C%20yielding%20superior%20accuracy%0Acompared%20to%20conventional%20NLL%20losses.%20We%20systematically%20benchmark%20multiple%0ABayesian%20approaches%2C%20including%20deep%20ensembles%20with%20mean-variance%20estimation%2C%0Astochastic%20weight%20averaging%20Gaussian%2C%20improved%20variational%20online%20Newton%2C%20and%0Alaplace%20approximation%20by%20evaluating%20their%20performance%20on%20uncertainty%0Aprediction%2C%20OOD%20detection%2C%20calibration%2C%20and%20active%20learning%20tasks.%20We%20further%0Ademonstrate%20that%20NLL%24_%5Ctext%7BJEF%7D%24%20facilitates%20efficient%20active%20learning%20by%0Aquantifying%20energy%20and%20force%20uncertainties.%20Using%20Bayesian%20active%20learning%20by%0Adisagreement%20%28BALD%29%2C%20our%20framework%20outperforms%20random%20sampling%20and%0Aenergy-uncertainty-based%20sampling.%20Our%20results%20demonstrate%20that%20Bayesian%20MLPs%0Aachieve%20competitive%20accuracy%20with%20state-of-the-art%20models%20while%20enabling%0Auncertainty-guided%20active%20learning%2C%20OOD%20detection%2C%20and%20energy/forces%0Acalibration.%20This%20work%20establishes%20Bayesian%20equivariant%20neural%20networks%20as%20a%0Apowerful%20framework%20for%20developing%20uncertainty-aware%20MLPs%20for%20atomistic%0Asimulations%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520E%25283%2529-Equivariant%2520Interatomic%2520Potential%2520with%2520Iterative%250A%2520%2520Restratification%2520of%2520Many-body%2520Message%2520Passing%26entry.906535625%3DSoohaeng%2520Yoo%2520Willow%2520and%2520Tae%2520Hyeon%2520Park%2520and%2520Gi%2520Beom%2520Sim%2520and%2520Sung%2520Wook%2520Moon%2520and%2520Seung%2520Kyu%2520Min%2520and%2520D.%2520ChangMo%2520Yang%2520and%2520Hyun%2520Woo%2520Kim%2520and%2520Juho%2520Lee%2520and%2520Chang%2520Woo%2520Myung%26entry.1292438233%3D%2520%2520Machine%2520learning%2520potentials%2520%2528MLPs%2529%2520have%2520become%2520essential%2520for%2520large-scale%250Aatomistic%2520simulations%252C%2520enabling%2520ab%2520initio-level%2520accuracy%2520with%2520computational%250Aefficiency.%2520However%252C%2520current%2520MLPs%2520struggle%2520with%2520uncertainty%2520quantification%252C%250Alimiting%2520their%2520reliability%2520for%2520active%2520learning%252C%2520calibration%252C%2520and%250Aout-of-distribution%2520%2528OOD%2529%2520detection.%2520We%2520address%2520these%2520challenges%2520by%2520developing%250ABayesian%2520E%25283%2529%2520equivariant%2520MLPs%2520with%2520iterative%2520restratification%2520of%2520many-body%250Amessage%2520passing.%2520Our%2520approach%2520introduces%2520the%2520joint%2520energy-force%2520negative%250Alog-likelihood%2520%2528NLL%2524_%255Ctext%257BJEF%257D%2524%2529%2520loss%2520function%252C%2520which%2520explicitly%2520models%250Auncertainty%2520in%2520both%2520energies%2520and%2520interatomic%2520forces%252C%2520yielding%2520superior%2520accuracy%250Acompared%2520to%2520conventional%2520NLL%2520losses.%2520We%2520systematically%2520benchmark%2520multiple%250ABayesian%2520approaches%252C%2520including%2520deep%2520ensembles%2520with%2520mean-variance%2520estimation%252C%250Astochastic%2520weight%2520averaging%2520Gaussian%252C%2520improved%2520variational%2520online%2520Newton%252C%2520and%250Alaplace%2520approximation%2520by%2520evaluating%2520their%2520performance%2520on%2520uncertainty%250Aprediction%252C%2520OOD%2520detection%252C%2520calibration%252C%2520and%2520active%2520learning%2520tasks.%2520We%2520further%250Ademonstrate%2520that%2520NLL%2524_%255Ctext%257BJEF%257D%2524%2520facilitates%2520efficient%2520active%2520learning%2520by%250Aquantifying%2520energy%2520and%2520force%2520uncertainties.%2520Using%2520Bayesian%2520active%2520learning%2520by%250Adisagreement%2520%2528BALD%2529%252C%2520our%2520framework%2520outperforms%2520random%2520sampling%2520and%250Aenergy-uncertainty-based%2520sampling.%2520Our%2520results%2520demonstrate%2520that%2520Bayesian%2520MLPs%250Aachieve%2520competitive%2520accuracy%2520with%2520state-of-the-art%2520models%2520while%2520enabling%250Auncertainty-guided%2520active%2520learning%252C%2520OOD%2520detection%252C%2520and%2520energy/forces%250Acalibration.%2520This%2520work%2520establishes%2520Bayesian%2520equivariant%2520neural%2520networks%2520as%2520a%250Apowerful%2520framework%2520for%2520developing%2520uncertainty-aware%2520MLPs%2520for%2520atomistic%250Asimulations%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20E%283%29-Equivariant%20Interatomic%20Potential%20with%20Iterative%0A%20%20Restratification%20of%20Many-body%20Message%20Passing&entry.906535625=Soohaeng%20Yoo%20Willow%20and%20Tae%20Hyeon%20Park%20and%20Gi%20Beom%20Sim%20and%20Sung%20Wook%20Moon%20and%20Seung%20Kyu%20Min%20and%20D.%20ChangMo%20Yang%20and%20Hyun%20Woo%20Kim%20and%20Juho%20Lee%20and%20Chang%20Woo%20Myung&entry.1292438233=%20%20Machine%20learning%20potentials%20%28MLPs%29%20have%20become%20essential%20for%20large-scale%0Aatomistic%20simulations%2C%20enabling%20ab%20initio-level%20accuracy%20with%20computational%0Aefficiency.%20However%2C%20current%20MLPs%20struggle%20with%20uncertainty%20quantification%2C%0Alimiting%20their%20reliability%20for%20active%20learning%2C%20calibration%2C%20and%0Aout-of-distribution%20%28OOD%29%20detection.%20We%20address%20these%20challenges%20by%20developing%0ABayesian%20E%283%29%20equivariant%20MLPs%20with%20iterative%20restratification%20of%20many-body%0Amessage%20passing.%20Our%20approach%20introduces%20the%20joint%20energy-force%20negative%0Alog-likelihood%20%28NLL%24_%5Ctext%7BJEF%7D%24%29%20loss%20function%2C%20which%20explicitly%20models%0Auncertainty%20in%20both%20energies%20and%20interatomic%20forces%2C%20yielding%20superior%20accuracy%0Acompared%20to%20conventional%20NLL%20losses.%20We%20systematically%20benchmark%20multiple%0ABayesian%20approaches%2C%20including%20deep%20ensembles%20with%20mean-variance%20estimation%2C%0Astochastic%20weight%20averaging%20Gaussian%2C%20improved%20variational%20online%20Newton%2C%20and%0Alaplace%20approximation%20by%20evaluating%20their%20performance%20on%20uncertainty%0Aprediction%2C%20OOD%20detection%2C%20calibration%2C%20and%20active%20learning%20tasks.%20We%20further%0Ademonstrate%20that%20NLL%24_%5Ctext%7BJEF%7D%24%20facilitates%20efficient%20active%20learning%20by%0Aquantifying%20energy%20and%20force%20uncertainties.%20Using%20Bayesian%20active%20learning%20by%0Adisagreement%20%28BALD%29%2C%20our%20framework%20outperforms%20random%20sampling%20and%0Aenergy-uncertainty-based%20sampling.%20Our%20results%20demonstrate%20that%20Bayesian%20MLPs%0Aachieve%20competitive%20accuracy%20with%20state-of-the-art%20models%20while%20enabling%0Auncertainty-guided%20active%20learning%2C%20OOD%20detection%2C%20and%20energy/forces%0Acalibration.%20This%20work%20establishes%20Bayesian%20equivariant%20neural%20networks%20as%20a%0Apowerful%20framework%20for%20developing%20uncertainty-aware%20MLPs%20for%20atomistic%0Asimulations%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03046v1&entry.124074799=Read"},
{"title": "Why Do We Need Warm-up? A Theoretical Perspective", "author": "Foivos Alimisis and Rustem Islamov and Aurelien Lucchi", "abstract": "  Learning rate warm-up - increasing the learning rate at the beginning of\ntraining - has become a ubiquitous heuristic in modern deep learning, yet its\ntheoretical foundations remain poorly understood. In this work, we provide a\nprincipled explanation for why warm-up improves training. We rely on a\ngeneralization of the $(L_0, L_1)$-smoothness condition, which bounds local\ncurvature as a linear function of the loss sub-optimality and exhibits\ndesirable closure properties. We demonstrate both theoretically and empirically\nthat this condition holds for common neural architectures trained with\nmean-squared error and cross-entropy losses. Under this assumption, we prove\nthat Gradient Descent with a warm-up schedule achieves faster convergence than\nwith a fixed step-size, establishing upper and lower complexity bounds.\nFinally, we validate our theoretical insights through experiments on language\nand vision models, confirming the practical benefits of warm-up schedules.\n", "link": "http://arxiv.org/abs/2510.03164v1", "date": "2025-10-03", "relevancy": 0.8653, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4373}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4342}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Do%20We%20Need%20Warm-up%3F%20A%20Theoretical%20Perspective&body=Title%3A%20Why%20Do%20We%20Need%20Warm-up%3F%20A%20Theoretical%20Perspective%0AAuthor%3A%20Foivos%20Alimisis%20and%20Rustem%20Islamov%20and%20Aurelien%20Lucchi%0AAbstract%3A%20%20%20Learning%20rate%20warm-up%20-%20increasing%20the%20learning%20rate%20at%20the%20beginning%20of%0Atraining%20-%20has%20become%20a%20ubiquitous%20heuristic%20in%20modern%20deep%20learning%2C%20yet%20its%0Atheoretical%20foundations%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20provide%20a%0Aprincipled%20explanation%20for%20why%20warm-up%20improves%20training.%20We%20rely%20on%20a%0Ageneralization%20of%20the%20%24%28L_0%2C%20L_1%29%24-smoothness%20condition%2C%20which%20bounds%20local%0Acurvature%20as%20a%20linear%20function%20of%20the%20loss%20sub-optimality%20and%20exhibits%0Adesirable%20closure%20properties.%20We%20demonstrate%20both%20theoretically%20and%20empirically%0Athat%20this%20condition%20holds%20for%20common%20neural%20architectures%20trained%20with%0Amean-squared%20error%20and%20cross-entropy%20losses.%20Under%20this%20assumption%2C%20we%20prove%0Athat%20Gradient%20Descent%20with%20a%20warm-up%20schedule%20achieves%20faster%20convergence%20than%0Awith%20a%20fixed%20step-size%2C%20establishing%20upper%20and%20lower%20complexity%20bounds.%0AFinally%2C%20we%20validate%20our%20theoretical%20insights%20through%20experiments%20on%20language%0Aand%20vision%20models%2C%20confirming%20the%20practical%20benefits%20of%20warm-up%20schedules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Do%2520We%2520Need%2520Warm-up%253F%2520A%2520Theoretical%2520Perspective%26entry.906535625%3DFoivos%2520Alimisis%2520and%2520Rustem%2520Islamov%2520and%2520Aurelien%2520Lucchi%26entry.1292438233%3D%2520%2520Learning%2520rate%2520warm-up%2520-%2520increasing%2520the%2520learning%2520rate%2520at%2520the%2520beginning%2520of%250Atraining%2520-%2520has%2520become%2520a%2520ubiquitous%2520heuristic%2520in%2520modern%2520deep%2520learning%252C%2520yet%2520its%250Atheoretical%2520foundations%2520remain%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520provide%2520a%250Aprincipled%2520explanation%2520for%2520why%2520warm-up%2520improves%2520training.%2520We%2520rely%2520on%2520a%250Ageneralization%2520of%2520the%2520%2524%2528L_0%252C%2520L_1%2529%2524-smoothness%2520condition%252C%2520which%2520bounds%2520local%250Acurvature%2520as%2520a%2520linear%2520function%2520of%2520the%2520loss%2520sub-optimality%2520and%2520exhibits%250Adesirable%2520closure%2520properties.%2520We%2520demonstrate%2520both%2520theoretically%2520and%2520empirically%250Athat%2520this%2520condition%2520holds%2520for%2520common%2520neural%2520architectures%2520trained%2520with%250Amean-squared%2520error%2520and%2520cross-entropy%2520losses.%2520Under%2520this%2520assumption%252C%2520we%2520prove%250Athat%2520Gradient%2520Descent%2520with%2520a%2520warm-up%2520schedule%2520achieves%2520faster%2520convergence%2520than%250Awith%2520a%2520fixed%2520step-size%252C%2520establishing%2520upper%2520and%2520lower%2520complexity%2520bounds.%250AFinally%252C%2520we%2520validate%2520our%2520theoretical%2520insights%2520through%2520experiments%2520on%2520language%250Aand%2520vision%2520models%252C%2520confirming%2520the%2520practical%2520benefits%2520of%2520warm-up%2520schedules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Do%20We%20Need%20Warm-up%3F%20A%20Theoretical%20Perspective&entry.906535625=Foivos%20Alimisis%20and%20Rustem%20Islamov%20and%20Aurelien%20Lucchi&entry.1292438233=%20%20Learning%20rate%20warm-up%20-%20increasing%20the%20learning%20rate%20at%20the%20beginning%20of%0Atraining%20-%20has%20become%20a%20ubiquitous%20heuristic%20in%20modern%20deep%20learning%2C%20yet%20its%0Atheoretical%20foundations%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20provide%20a%0Aprincipled%20explanation%20for%20why%20warm-up%20improves%20training.%20We%20rely%20on%20a%0Ageneralization%20of%20the%20%24%28L_0%2C%20L_1%29%24-smoothness%20condition%2C%20which%20bounds%20local%0Acurvature%20as%20a%20linear%20function%20of%20the%20loss%20sub-optimality%20and%20exhibits%0Adesirable%20closure%20properties.%20We%20demonstrate%20both%20theoretically%20and%20empirically%0Athat%20this%20condition%20holds%20for%20common%20neural%20architectures%20trained%20with%0Amean-squared%20error%20and%20cross-entropy%20losses.%20Under%20this%20assumption%2C%20we%20prove%0Athat%20Gradient%20Descent%20with%20a%20warm-up%20schedule%20achieves%20faster%20convergence%20than%0Awith%20a%20fixed%20step-size%2C%20establishing%20upper%20and%20lower%20complexity%20bounds.%0AFinally%2C%20we%20validate%20our%20theoretical%20insights%20through%20experiments%20on%20language%0Aand%20vision%20models%2C%20confirming%20the%20practical%20benefits%20of%20warm-up%20schedules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03164v1&entry.124074799=Read"},
{"title": "Variance Reduction and Low Sample Complexity in Stochastic Optimization\n  via Proximal Point Method", "author": "Jiaming Liang", "abstract": "  High-probability guarantees in stochastic optimization are often obtained\nonly under strong noise assumptions such as sub-Gaussian tails. We show that\nsuch guarantees can also be achieved under the weaker assumption of bounded\nvariance by developing a stochastic proximal point method. This method combines\na proximal subproblem solver, which inherently reduces variance, with a\nprobability booster that amplifies per-iteration reliability into\nhigh-confidence results. The analysis demonstrates convergence with low sample\ncomplexity, without restrictive noise assumptions or reliance on mini-batching.\n", "link": "http://arxiv.org/abs/2402.08992v2", "date": "2025-10-03", "relevancy": 1.711, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4561}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4222}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variance%20Reduction%20and%20Low%20Sample%20Complexity%20in%20Stochastic%20Optimization%0A%20%20via%20Proximal%20Point%20Method&body=Title%3A%20Variance%20Reduction%20and%20Low%20Sample%20Complexity%20in%20Stochastic%20Optimization%0A%20%20via%20Proximal%20Point%20Method%0AAuthor%3A%20Jiaming%20Liang%0AAbstract%3A%20%20%20High-probability%20guarantees%20in%20stochastic%20optimization%20are%20often%20obtained%0Aonly%20under%20strong%20noise%20assumptions%20such%20as%20sub-Gaussian%20tails.%20We%20show%20that%0Asuch%20guarantees%20can%20also%20be%20achieved%20under%20the%20weaker%20assumption%20of%20bounded%0Avariance%20by%20developing%20a%20stochastic%20proximal%20point%20method.%20This%20method%20combines%0Aa%20proximal%20subproblem%20solver%2C%20which%20inherently%20reduces%20variance%2C%20with%20a%0Aprobability%20booster%20that%20amplifies%20per-iteration%20reliability%20into%0Ahigh-confidence%20results.%20The%20analysis%20demonstrates%20convergence%20with%20low%20sample%0Acomplexity%2C%20without%20restrictive%20noise%20assumptions%20or%20reliance%20on%20mini-batching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08992v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariance%2520Reduction%2520and%2520Low%2520Sample%2520Complexity%2520in%2520Stochastic%2520Optimization%250A%2520%2520via%2520Proximal%2520Point%2520Method%26entry.906535625%3DJiaming%2520Liang%26entry.1292438233%3D%2520%2520High-probability%2520guarantees%2520in%2520stochastic%2520optimization%2520are%2520often%2520obtained%250Aonly%2520under%2520strong%2520noise%2520assumptions%2520such%2520as%2520sub-Gaussian%2520tails.%2520We%2520show%2520that%250Asuch%2520guarantees%2520can%2520also%2520be%2520achieved%2520under%2520the%2520weaker%2520assumption%2520of%2520bounded%250Avariance%2520by%2520developing%2520a%2520stochastic%2520proximal%2520point%2520method.%2520This%2520method%2520combines%250Aa%2520proximal%2520subproblem%2520solver%252C%2520which%2520inherently%2520reduces%2520variance%252C%2520with%2520a%250Aprobability%2520booster%2520that%2520amplifies%2520per-iteration%2520reliability%2520into%250Ahigh-confidence%2520results.%2520The%2520analysis%2520demonstrates%2520convergence%2520with%2520low%2520sample%250Acomplexity%252C%2520without%2520restrictive%2520noise%2520assumptions%2520or%2520reliance%2520on%2520mini-batching.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08992v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variance%20Reduction%20and%20Low%20Sample%20Complexity%20in%20Stochastic%20Optimization%0A%20%20via%20Proximal%20Point%20Method&entry.906535625=Jiaming%20Liang&entry.1292438233=%20%20High-probability%20guarantees%20in%20stochastic%20optimization%20are%20often%20obtained%0Aonly%20under%20strong%20noise%20assumptions%20such%20as%20sub-Gaussian%20tails.%20We%20show%20that%0Asuch%20guarantees%20can%20also%20be%20achieved%20under%20the%20weaker%20assumption%20of%20bounded%0Avariance%20by%20developing%20a%20stochastic%20proximal%20point%20method.%20This%20method%20combines%0Aa%20proximal%20subproblem%20solver%2C%20which%20inherently%20reduces%20variance%2C%20with%20a%0Aprobability%20booster%20that%20amplifies%20per-iteration%20reliability%20into%0Ahigh-confidence%20results.%20The%20analysis%20demonstrates%20convergence%20with%20low%20sample%0Acomplexity%2C%20without%20restrictive%20noise%20assumptions%20or%20reliance%20on%20mini-batching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08992v2&entry.124074799=Read"},
{"title": "FTTE: Federated Learning on Resource-Constrained Devices", "author": "Irene Tenison and Anna Murphy and Charles Beauville and Lalana Kagal", "abstract": "  Federated learning (FL) enables collaborative model training across\ndistributed devices while preserving data privacy, but deployment on\nresource-constrained edge nodes remains challenging due to limited memory,\nenergy, and communication bandwidth. Traditional synchronous and asynchronous\nFL approaches further suffer from straggler induced delays and slow convergence\nin heterogeneous, large scale networks. We present FTTE (Federated Tiny\nTraining Engine),a novel semi-asynchronous FL framework that uniquely employs\nsparse parameter updates and a staleness-weighted aggregation based on both age\nand variance of client updates. Extensive experiments across diverse models and\ndata distributions - including up to 500 clients and 90% stragglers -\ndemonstrate that FTTE not only achieves 81% faster convergence, 80% lower\non-device memory usage, and 69% communication payload reduction than\nsynchronous FL (eg.FedAVG), but also consistently reaches comparable or higher\ntarget accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes.\nThese results establish FTTE as the first practical and scalable solution for\nreal-world FL deployments on heterogeneous and predominantly\nresource-constrained edge devices.\n", "link": "http://arxiv.org/abs/2510.03165v1", "date": "2025-10-03", "relevancy": 1.455, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4969}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.48}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FTTE%3A%20Federated%20Learning%20on%20Resource-Constrained%20Devices&body=Title%3A%20FTTE%3A%20Federated%20Learning%20on%20Resource-Constrained%20Devices%0AAuthor%3A%20Irene%20Tenison%20and%20Anna%20Murphy%20and%20Charles%20Beauville%20and%20Lalana%20Kagal%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20devices%20while%20preserving%20data%20privacy%2C%20but%20deployment%20on%0Aresource-constrained%20edge%20nodes%20remains%20challenging%20due%20to%20limited%20memory%2C%0Aenergy%2C%20and%20communication%20bandwidth.%20Traditional%20synchronous%20and%20asynchronous%0AFL%20approaches%20further%20suffer%20from%20straggler%20induced%20delays%20and%20slow%20convergence%0Ain%20heterogeneous%2C%20large%20scale%20networks.%20We%20present%20FTTE%20%28Federated%20Tiny%0ATraining%20Engine%29%2Ca%20novel%20semi-asynchronous%20FL%20framework%20that%20uniquely%20employs%0Asparse%20parameter%20updates%20and%20a%20staleness-weighted%20aggregation%20based%20on%20both%20age%0Aand%20variance%20of%20client%20updates.%20Extensive%20experiments%20across%20diverse%20models%20and%0Adata%20distributions%20-%20including%20up%20to%20500%20clients%20and%2090%25%20stragglers%20-%0Ademonstrate%20that%20FTTE%20not%20only%20achieves%2081%25%20faster%20convergence%2C%2080%25%20lower%0Aon-device%20memory%20usage%2C%20and%2069%25%20communication%20payload%20reduction%20than%0Asynchronous%20FL%20%28eg.FedAVG%29%2C%20but%20also%20consistently%20reaches%20comparable%20or%20higher%0Atarget%20accuracy%20than%20semi-asynchronous%20%28eg.FedBuff%29%20in%20challenging%20regimes.%0AThese%20results%20establish%20FTTE%20as%20the%20first%20practical%20and%20scalable%20solution%20for%0Areal-world%20FL%20deployments%20on%20heterogeneous%20and%20predominantly%0Aresource-constrained%20edge%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFTTE%253A%2520Federated%2520Learning%2520on%2520Resource-Constrained%2520Devices%26entry.906535625%3DIrene%2520Tenison%2520and%2520Anna%2520Murphy%2520and%2520Charles%2520Beauville%2520and%2520Lalana%2520Kagal%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adistributed%2520devices%2520while%2520preserving%2520data%2520privacy%252C%2520but%2520deployment%2520on%250Aresource-constrained%2520edge%2520nodes%2520remains%2520challenging%2520due%2520to%2520limited%2520memory%252C%250Aenergy%252C%2520and%2520communication%2520bandwidth.%2520Traditional%2520synchronous%2520and%2520asynchronous%250AFL%2520approaches%2520further%2520suffer%2520from%2520straggler%2520induced%2520delays%2520and%2520slow%2520convergence%250Ain%2520heterogeneous%252C%2520large%2520scale%2520networks.%2520We%2520present%2520FTTE%2520%2528Federated%2520Tiny%250ATraining%2520Engine%2529%252Ca%2520novel%2520semi-asynchronous%2520FL%2520framework%2520that%2520uniquely%2520employs%250Asparse%2520parameter%2520updates%2520and%2520a%2520staleness-weighted%2520aggregation%2520based%2520on%2520both%2520age%250Aand%2520variance%2520of%2520client%2520updates.%2520Extensive%2520experiments%2520across%2520diverse%2520models%2520and%250Adata%2520distributions%2520-%2520including%2520up%2520to%2520500%2520clients%2520and%252090%2525%2520stragglers%2520-%250Ademonstrate%2520that%2520FTTE%2520not%2520only%2520achieves%252081%2525%2520faster%2520convergence%252C%252080%2525%2520lower%250Aon-device%2520memory%2520usage%252C%2520and%252069%2525%2520communication%2520payload%2520reduction%2520than%250Asynchronous%2520FL%2520%2528eg.FedAVG%2529%252C%2520but%2520also%2520consistently%2520reaches%2520comparable%2520or%2520higher%250Atarget%2520accuracy%2520than%2520semi-asynchronous%2520%2528eg.FedBuff%2529%2520in%2520challenging%2520regimes.%250AThese%2520results%2520establish%2520FTTE%2520as%2520the%2520first%2520practical%2520and%2520scalable%2520solution%2520for%250Areal-world%2520FL%2520deployments%2520on%2520heterogeneous%2520and%2520predominantly%250Aresource-constrained%2520edge%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FTTE%3A%20Federated%20Learning%20on%20Resource-Constrained%20Devices&entry.906535625=Irene%20Tenison%20and%20Anna%20Murphy%20and%20Charles%20Beauville%20and%20Lalana%20Kagal&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20devices%20while%20preserving%20data%20privacy%2C%20but%20deployment%20on%0Aresource-constrained%20edge%20nodes%20remains%20challenging%20due%20to%20limited%20memory%2C%0Aenergy%2C%20and%20communication%20bandwidth.%20Traditional%20synchronous%20and%20asynchronous%0AFL%20approaches%20further%20suffer%20from%20straggler%20induced%20delays%20and%20slow%20convergence%0Ain%20heterogeneous%2C%20large%20scale%20networks.%20We%20present%20FTTE%20%28Federated%20Tiny%0ATraining%20Engine%29%2Ca%20novel%20semi-asynchronous%20FL%20framework%20that%20uniquely%20employs%0Asparse%20parameter%20updates%20and%20a%20staleness-weighted%20aggregation%20based%20on%20both%20age%0Aand%20variance%20of%20client%20updates.%20Extensive%20experiments%20across%20diverse%20models%20and%0Adata%20distributions%20-%20including%20up%20to%20500%20clients%20and%2090%25%20stragglers%20-%0Ademonstrate%20that%20FTTE%20not%20only%20achieves%2081%25%20faster%20convergence%2C%2080%25%20lower%0Aon-device%20memory%20usage%2C%20and%2069%25%20communication%20payload%20reduction%20than%0Asynchronous%20FL%20%28eg.FedAVG%29%2C%20but%20also%20consistently%20reaches%20comparable%20or%20higher%0Atarget%20accuracy%20than%20semi-asynchronous%20%28eg.FedBuff%29%20in%20challenging%20regimes.%0AThese%20results%20establish%20FTTE%20as%20the%20first%20practical%20and%20scalable%20solution%20for%0Areal-world%20FL%20deployments%20on%20heterogeneous%20and%20predominantly%0Aresource-constrained%20edge%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03165v1&entry.124074799=Read"},
{"title": "Post Reinforcement Learning Inference", "author": "Vasilis Syrgkanis and Ruohan Zhan", "abstract": "  We study estimation and inference using data collected by reinforcement\nlearning (RL) algorithms. These algorithms adaptively experiment by interacting\nwith individual units over multiple stages, updating their strategies based on\npast outcomes. Our goal is to evaluate a counterfactual policy after data\ncollection and estimate structural parameters, such as dynamic treatment\neffects, that support credit assignment and quantify the impact of early\nactions on final outcomes. These parameters can often be defined as solutions\nto moment equations, motivating moment-based estimation methods developed for\nstatic data. In RL settings, however, data are often collected adaptively under\nnonstationary behavior policies. As a result, standard estimators fail to\nachieve asymptotic normality due to time-varying variance. We propose a\nweighted generalized method of moments (GMM) approach that uses adaptive\nweights to stabilize this variance. We characterize weighting schemes that\nensure consistency and asymptotic normality of the weighted GMM estimators,\nenabling valid hypothesis testing and uniform confidence region construction.\nKey applications include dynamic treatment effect estimation and dynamic\noff-policy evaluation.\n", "link": "http://arxiv.org/abs/2302.08854v5", "date": "2025-10-03", "relevancy": 1.8213, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5195}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4474}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post%20Reinforcement%20Learning%20Inference&body=Title%3A%20Post%20Reinforcement%20Learning%20Inference%0AAuthor%3A%20Vasilis%20Syrgkanis%20and%20Ruohan%20Zhan%0AAbstract%3A%20%20%20We%20study%20estimation%20and%20inference%20using%20data%20collected%20by%20reinforcement%0Alearning%20%28RL%29%20algorithms.%20These%20algorithms%20adaptively%20experiment%20by%20interacting%0Awith%20individual%20units%20over%20multiple%20stages%2C%20updating%20their%20strategies%20based%20on%0Apast%20outcomes.%20Our%20goal%20is%20to%20evaluate%20a%20counterfactual%20policy%20after%20data%0Acollection%20and%20estimate%20structural%20parameters%2C%20such%20as%20dynamic%20treatment%0Aeffects%2C%20that%20support%20credit%20assignment%20and%20quantify%20the%20impact%20of%20early%0Aactions%20on%20final%20outcomes.%20These%20parameters%20can%20often%20be%20defined%20as%20solutions%0Ato%20moment%20equations%2C%20motivating%20moment-based%20estimation%20methods%20developed%20for%0Astatic%20data.%20In%20RL%20settings%2C%20however%2C%20data%20are%20often%20collected%20adaptively%20under%0Anonstationary%20behavior%20policies.%20As%20a%20result%2C%20standard%20estimators%20fail%20to%0Aachieve%20asymptotic%20normality%20due%20to%20time-varying%20variance.%20We%20propose%20a%0Aweighted%20generalized%20method%20of%20moments%20%28GMM%29%20approach%20that%20uses%20adaptive%0Aweights%20to%20stabilize%20this%20variance.%20We%20characterize%20weighting%20schemes%20that%0Aensure%20consistency%20and%20asymptotic%20normality%20of%20the%20weighted%20GMM%20estimators%2C%0Aenabling%20valid%20hypothesis%20testing%20and%20uniform%20confidence%20region%20construction.%0AKey%20applications%20include%20dynamic%20treatment%20effect%20estimation%20and%20dynamic%0Aoff-policy%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.08854v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost%2520Reinforcement%2520Learning%2520Inference%26entry.906535625%3DVasilis%2520Syrgkanis%2520and%2520Ruohan%2520Zhan%26entry.1292438233%3D%2520%2520We%2520study%2520estimation%2520and%2520inference%2520using%2520data%2520collected%2520by%2520reinforcement%250Alearning%2520%2528RL%2529%2520algorithms.%2520These%2520algorithms%2520adaptively%2520experiment%2520by%2520interacting%250Awith%2520individual%2520units%2520over%2520multiple%2520stages%252C%2520updating%2520their%2520strategies%2520based%2520on%250Apast%2520outcomes.%2520Our%2520goal%2520is%2520to%2520evaluate%2520a%2520counterfactual%2520policy%2520after%2520data%250Acollection%2520and%2520estimate%2520structural%2520parameters%252C%2520such%2520as%2520dynamic%2520treatment%250Aeffects%252C%2520that%2520support%2520credit%2520assignment%2520and%2520quantify%2520the%2520impact%2520of%2520early%250Aactions%2520on%2520final%2520outcomes.%2520These%2520parameters%2520can%2520often%2520be%2520defined%2520as%2520solutions%250Ato%2520moment%2520equations%252C%2520motivating%2520moment-based%2520estimation%2520methods%2520developed%2520for%250Astatic%2520data.%2520In%2520RL%2520settings%252C%2520however%252C%2520data%2520are%2520often%2520collected%2520adaptively%2520under%250Anonstationary%2520behavior%2520policies.%2520As%2520a%2520result%252C%2520standard%2520estimators%2520fail%2520to%250Aachieve%2520asymptotic%2520normality%2520due%2520to%2520time-varying%2520variance.%2520We%2520propose%2520a%250Aweighted%2520generalized%2520method%2520of%2520moments%2520%2528GMM%2529%2520approach%2520that%2520uses%2520adaptive%250Aweights%2520to%2520stabilize%2520this%2520variance.%2520We%2520characterize%2520weighting%2520schemes%2520that%250Aensure%2520consistency%2520and%2520asymptotic%2520normality%2520of%2520the%2520weighted%2520GMM%2520estimators%252C%250Aenabling%2520valid%2520hypothesis%2520testing%2520and%2520uniform%2520confidence%2520region%2520construction.%250AKey%2520applications%2520include%2520dynamic%2520treatment%2520effect%2520estimation%2520and%2520dynamic%250Aoff-policy%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.08854v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post%20Reinforcement%20Learning%20Inference&entry.906535625=Vasilis%20Syrgkanis%20and%20Ruohan%20Zhan&entry.1292438233=%20%20We%20study%20estimation%20and%20inference%20using%20data%20collected%20by%20reinforcement%0Alearning%20%28RL%29%20algorithms.%20These%20algorithms%20adaptively%20experiment%20by%20interacting%0Awith%20individual%20units%20over%20multiple%20stages%2C%20updating%20their%20strategies%20based%20on%0Apast%20outcomes.%20Our%20goal%20is%20to%20evaluate%20a%20counterfactual%20policy%20after%20data%0Acollection%20and%20estimate%20structural%20parameters%2C%20such%20as%20dynamic%20treatment%0Aeffects%2C%20that%20support%20credit%20assignment%20and%20quantify%20the%20impact%20of%20early%0Aactions%20on%20final%20outcomes.%20These%20parameters%20can%20often%20be%20defined%20as%20solutions%0Ato%20moment%20equations%2C%20motivating%20moment-based%20estimation%20methods%20developed%20for%0Astatic%20data.%20In%20RL%20settings%2C%20however%2C%20data%20are%20often%20collected%20adaptively%20under%0Anonstationary%20behavior%20policies.%20As%20a%20result%2C%20standard%20estimators%20fail%20to%0Aachieve%20asymptotic%20normality%20due%20to%20time-varying%20variance.%20We%20propose%20a%0Aweighted%20generalized%20method%20of%20moments%20%28GMM%29%20approach%20that%20uses%20adaptive%0Aweights%20to%20stabilize%20this%20variance.%20We%20characterize%20weighting%20schemes%20that%0Aensure%20consistency%20and%20asymptotic%20normality%20of%20the%20weighted%20GMM%20estimators%2C%0Aenabling%20valid%20hypothesis%20testing%20and%20uniform%20confidence%20region%20construction.%0AKey%20applications%20include%20dynamic%20treatment%20effect%20estimation%20and%20dynamic%0Aoff-policy%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.08854v5&entry.124074799=Read"},
{"title": "Improved Monte Carlo Planning via Causal Disentanglement for\n  Structurally-Decomposed Markov Decision Processes", "author": "Larkin Liu and Shiqi Liu and Yinruo Hua and Matej Jusup", "abstract": "  Markov Decision Processes (MDPs), as a general-purpose framework, often\noverlook the benefits of incorporating the causal structure of the transition\nand reward dynamics. For a subclass of resource allocation problems, we\nintroduce the Structurally Decomposed MDP (SD-MDP), which leverages causal\ndisentanglement to partition an MDP's temporal causal graph into independent\ncomponents. By exploiting this disentanglement, SD-MDP enables dimensionality\nreduction and computational efficiency gains in optimal value function\nestimation. We reduce the sequential optimization problem to a fractional\nknapsack problem with log-linear complexity $O(T \\log T)$, outperforming\ntraditional stochastic programming methods that exhibit polynomial complexity\nwith respect to the time horizon $T$. Additionally, SD-MDP's computational\nadvantages are independent of state-action space size, making it viable for\nhigh-dimensional spaces. Furthermore, our approach integrates seamlessly with\nMonte Carlo Tree Search (MCTS), achieving higher expected rewards under\nconstrained simulation budgets while providing a vanishing simple regret bound.\nEmpirical results demonstrate superior policy performance over benchmarks\nacross various logistics and finance domains.\n", "link": "http://arxiv.org/abs/2406.16151v2", "date": "2025-10-03", "relevancy": 0.9095, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4591}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Monte%20Carlo%20Planning%20via%20Causal%20Disentanglement%20for%0A%20%20Structurally-Decomposed%20Markov%20Decision%20Processes&body=Title%3A%20Improved%20Monte%20Carlo%20Planning%20via%20Causal%20Disentanglement%20for%0A%20%20Structurally-Decomposed%20Markov%20Decision%20Processes%0AAuthor%3A%20Larkin%20Liu%20and%20Shiqi%20Liu%20and%20Yinruo%20Hua%20and%20Matej%20Jusup%0AAbstract%3A%20%20%20Markov%20Decision%20Processes%20%28MDPs%29%2C%20as%20a%20general-purpose%20framework%2C%20often%0Aoverlook%20the%20benefits%20of%20incorporating%20the%20causal%20structure%20of%20the%20transition%0Aand%20reward%20dynamics.%20For%20a%20subclass%20of%20resource%20allocation%20problems%2C%20we%0Aintroduce%20the%20Structurally%20Decomposed%20MDP%20%28SD-MDP%29%2C%20which%20leverages%20causal%0Adisentanglement%20to%20partition%20an%20MDP%27s%20temporal%20causal%20graph%20into%20independent%0Acomponents.%20By%20exploiting%20this%20disentanglement%2C%20SD-MDP%20enables%20dimensionality%0Areduction%20and%20computational%20efficiency%20gains%20in%20optimal%20value%20function%0Aestimation.%20We%20reduce%20the%20sequential%20optimization%20problem%20to%20a%20fractional%0Aknapsack%20problem%20with%20log-linear%20complexity%20%24O%28T%20%5Clog%20T%29%24%2C%20outperforming%0Atraditional%20stochastic%20programming%20methods%20that%20exhibit%20polynomial%20complexity%0Awith%20respect%20to%20the%20time%20horizon%20%24T%24.%20Additionally%2C%20SD-MDP%27s%20computational%0Aadvantages%20are%20independent%20of%20state-action%20space%20size%2C%20making%20it%20viable%20for%0Ahigh-dimensional%20spaces.%20Furthermore%2C%20our%20approach%20integrates%20seamlessly%20with%0AMonte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20achieving%20higher%20expected%20rewards%20under%0Aconstrained%20simulation%20budgets%20while%20providing%20a%20vanishing%20simple%20regret%20bound.%0AEmpirical%20results%20demonstrate%20superior%20policy%20performance%20over%20benchmarks%0Aacross%20various%20logistics%20and%20finance%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Monte%2520Carlo%2520Planning%2520via%2520Causal%2520Disentanglement%2520for%250A%2520%2520Structurally-Decomposed%2520Markov%2520Decision%2520Processes%26entry.906535625%3DLarkin%2520Liu%2520and%2520Shiqi%2520Liu%2520and%2520Yinruo%2520Hua%2520and%2520Matej%2520Jusup%26entry.1292438233%3D%2520%2520Markov%2520Decision%2520Processes%2520%2528MDPs%2529%252C%2520as%2520a%2520general-purpose%2520framework%252C%2520often%250Aoverlook%2520the%2520benefits%2520of%2520incorporating%2520the%2520causal%2520structure%2520of%2520the%2520transition%250Aand%2520reward%2520dynamics.%2520For%2520a%2520subclass%2520of%2520resource%2520allocation%2520problems%252C%2520we%250Aintroduce%2520the%2520Structurally%2520Decomposed%2520MDP%2520%2528SD-MDP%2529%252C%2520which%2520leverages%2520causal%250Adisentanglement%2520to%2520partition%2520an%2520MDP%2527s%2520temporal%2520causal%2520graph%2520into%2520independent%250Acomponents.%2520By%2520exploiting%2520this%2520disentanglement%252C%2520SD-MDP%2520enables%2520dimensionality%250Areduction%2520and%2520computational%2520efficiency%2520gains%2520in%2520optimal%2520value%2520function%250Aestimation.%2520We%2520reduce%2520the%2520sequential%2520optimization%2520problem%2520to%2520a%2520fractional%250Aknapsack%2520problem%2520with%2520log-linear%2520complexity%2520%2524O%2528T%2520%255Clog%2520T%2529%2524%252C%2520outperforming%250Atraditional%2520stochastic%2520programming%2520methods%2520that%2520exhibit%2520polynomial%2520complexity%250Awith%2520respect%2520to%2520the%2520time%2520horizon%2520%2524T%2524.%2520Additionally%252C%2520SD-MDP%2527s%2520computational%250Aadvantages%2520are%2520independent%2520of%2520state-action%2520space%2520size%252C%2520making%2520it%2520viable%2520for%250Ahigh-dimensional%2520spaces.%2520Furthermore%252C%2520our%2520approach%2520integrates%2520seamlessly%2520with%250AMonte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%252C%2520achieving%2520higher%2520expected%2520rewards%2520under%250Aconstrained%2520simulation%2520budgets%2520while%2520providing%2520a%2520vanishing%2520simple%2520regret%2520bound.%250AEmpirical%2520results%2520demonstrate%2520superior%2520policy%2520performance%2520over%2520benchmarks%250Aacross%2520various%2520logistics%2520and%2520finance%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Monte%20Carlo%20Planning%20via%20Causal%20Disentanglement%20for%0A%20%20Structurally-Decomposed%20Markov%20Decision%20Processes&entry.906535625=Larkin%20Liu%20and%20Shiqi%20Liu%20and%20Yinruo%20Hua%20and%20Matej%20Jusup&entry.1292438233=%20%20Markov%20Decision%20Processes%20%28MDPs%29%2C%20as%20a%20general-purpose%20framework%2C%20often%0Aoverlook%20the%20benefits%20of%20incorporating%20the%20causal%20structure%20of%20the%20transition%0Aand%20reward%20dynamics.%20For%20a%20subclass%20of%20resource%20allocation%20problems%2C%20we%0Aintroduce%20the%20Structurally%20Decomposed%20MDP%20%28SD-MDP%29%2C%20which%20leverages%20causal%0Adisentanglement%20to%20partition%20an%20MDP%27s%20temporal%20causal%20graph%20into%20independent%0Acomponents.%20By%20exploiting%20this%20disentanglement%2C%20SD-MDP%20enables%20dimensionality%0Areduction%20and%20computational%20efficiency%20gains%20in%20optimal%20value%20function%0Aestimation.%20We%20reduce%20the%20sequential%20optimization%20problem%20to%20a%20fractional%0Aknapsack%20problem%20with%20log-linear%20complexity%20%24O%28T%20%5Clog%20T%29%24%2C%20outperforming%0Atraditional%20stochastic%20programming%20methods%20that%20exhibit%20polynomial%20complexity%0Awith%20respect%20to%20the%20time%20horizon%20%24T%24.%20Additionally%2C%20SD-MDP%27s%20computational%0Aadvantages%20are%20independent%20of%20state-action%20space%20size%2C%20making%20it%20viable%20for%0Ahigh-dimensional%20spaces.%20Furthermore%2C%20our%20approach%20integrates%20seamlessly%20with%0AMonte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20achieving%20higher%20expected%20rewards%20under%0Aconstrained%20simulation%20budgets%20while%20providing%20a%20vanishing%20simple%20regret%20bound.%0AEmpirical%20results%20demonstrate%20superior%20policy%20performance%20over%20benchmarks%0Aacross%20various%20logistics%20and%20finance%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16151v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


