<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250923.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with\n  Voxel-Aligned Prediction", "author": "Weijie Wang and Yeqing Chen and Zeyu Zhang and Hengyu Liu and Haoxiao Wang and Zhiyuan Feng and Wenkang Qin and Zheng Zhu and Donny Y. Chen and Bohan Zhuang", "abstract": "  Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\nsolution for novel view synthesis. Existing methods predominantly rely on a\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\n3D Gaussian. We rethink this widely adopted formulation and identify several\ninherent limitations: it renders the reconstructed 3D models heavily dependent\non the number of input views, leads to view-biased density distributions, and\nintroduces alignment errors, particularly when source views contain occlusions\nor low texture. To address these challenges, we introduce VolSplat, a new\nmulti-view feed-forward paradigm that replaces pixel alignment with\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\nadaptive control over Gaussian density based on 3D scene complexity, yielding\nmore faithful Gaussian point clouds, improved geometric consistency, and\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\nstate-of-the-art performance while producing more plausible and view-consistent\nGaussian reconstructions. In addition to superior results, our approach\nestablishes a more scalable framework for feed-forward 3D reconstruction with\ndenser and more robust representations, paving the way for further research in\nwider communities. The video results, code and trained models are available on\nour project page: https://lhmd.top/volsplat.\n", "link": "http://arxiv.org/abs/2509.19297v1", "date": "2025-09-23", "relevancy": 3.5246, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7325}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7169}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VolSplat%3A%20Rethinking%20Feed-Forward%203D%20Gaussian%20Splatting%20with%0A%20%20Voxel-Aligned%20Prediction&body=Title%3A%20VolSplat%3A%20Rethinking%20Feed-Forward%203D%20Gaussian%20Splatting%20with%0A%20%20Voxel-Aligned%20Prediction%0AAuthor%3A%20Weijie%20Wang%20and%20Yeqing%20Chen%20and%20Zeyu%20Zhang%20and%20Hengyu%20Liu%20and%20Haoxiao%20Wang%20and%20Zhiyuan%20Feng%20and%20Wenkang%20Qin%20and%20Zheng%20Zhu%20and%20Donny%20Y.%20Chen%20and%20Bohan%20Zhuang%0AAbstract%3A%20%20%20Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20highly%20effective%0Asolution%20for%20novel%20view%20synthesis.%20Existing%20methods%20predominantly%20rely%20on%20a%0Apixel-aligned%20Gaussian%20prediction%20paradigm%2C%20where%20each%202D%20pixel%20is%20mapped%20to%20a%0A3D%20Gaussian.%20We%20rethink%20this%20widely%20adopted%20formulation%20and%20identify%20several%0Ainherent%20limitations%3A%20it%20renders%20the%20reconstructed%203D%20models%20heavily%20dependent%0Aon%20the%20number%20of%20input%20views%2C%20leads%20to%20view-biased%20density%20distributions%2C%20and%0Aintroduces%20alignment%20errors%2C%20particularly%20when%20source%20views%20contain%20occlusions%0Aor%20low%20texture.%20To%20address%20these%20challenges%2C%20we%20introduce%20VolSplat%2C%20a%20new%0Amulti-view%20feed-forward%20paradigm%20that%20replaces%20pixel%20alignment%20with%0Avoxel-aligned%20Gaussians.%20By%20directly%20predicting%20Gaussians%20from%20a%20predicted%203D%0Avoxel%20grid%2C%20it%20overcomes%20pixel%20alignment%27s%20reliance%20on%20error-prone%202D%20feature%0Amatching%2C%20ensuring%20robust%20multi-view%20consistency.%20Furthermore%2C%20it%20enables%0Aadaptive%20control%20over%20Gaussian%20density%20based%20on%203D%20scene%20complexity%2C%20yielding%0Amore%20faithful%20Gaussian%20point%20clouds%2C%20improved%20geometric%20consistency%2C%20and%0Aenhanced%20novel-view%20rendering%20quality.%20Experiments%20on%20widely%20used%20benchmarks%0Aincluding%20RealEstate10K%20and%20ScanNet%20demonstrate%20that%20VolSplat%20achieves%0Astate-of-the-art%20performance%20while%20producing%20more%20plausible%20and%20view-consistent%0AGaussian%20reconstructions.%20In%20addition%20to%20superior%20results%2C%20our%20approach%0Aestablishes%20a%20more%20scalable%20framework%20for%20feed-forward%203D%20reconstruction%20with%0Adenser%20and%20more%20robust%20representations%2C%20paving%20the%20way%20for%20further%20research%20in%0Awider%20communities.%20The%20video%20results%2C%20code%20and%20trained%20models%20are%20available%20on%0Aour%20project%20page%3A%20https%3A//lhmd.top/volsplat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolSplat%253A%2520Rethinking%2520Feed-Forward%25203D%2520Gaussian%2520Splatting%2520with%250A%2520%2520Voxel-Aligned%2520Prediction%26entry.906535625%3DWeijie%2520Wang%2520and%2520Yeqing%2520Chen%2520and%2520Zeyu%2520Zhang%2520and%2520Hengyu%2520Liu%2520and%2520Haoxiao%2520Wang%2520and%2520Zhiyuan%2520Feng%2520and%2520Wenkang%2520Qin%2520and%2520Zheng%2520Zhu%2520and%2520Donny%2520Y.%2520Chen%2520and%2520Bohan%2520Zhuang%26entry.1292438233%3D%2520%2520Feed-forward%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520highly%2520effective%250Asolution%2520for%2520novel%2520view%2520synthesis.%2520Existing%2520methods%2520predominantly%2520rely%2520on%2520a%250Apixel-aligned%2520Gaussian%2520prediction%2520paradigm%252C%2520where%2520each%25202D%2520pixel%2520is%2520mapped%2520to%2520a%250A3D%2520Gaussian.%2520We%2520rethink%2520this%2520widely%2520adopted%2520formulation%2520and%2520identify%2520several%250Ainherent%2520limitations%253A%2520it%2520renders%2520the%2520reconstructed%25203D%2520models%2520heavily%2520dependent%250Aon%2520the%2520number%2520of%2520input%2520views%252C%2520leads%2520to%2520view-biased%2520density%2520distributions%252C%2520and%250Aintroduces%2520alignment%2520errors%252C%2520particularly%2520when%2520source%2520views%2520contain%2520occlusions%250Aor%2520low%2520texture.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520VolSplat%252C%2520a%2520new%250Amulti-view%2520feed-forward%2520paradigm%2520that%2520replaces%2520pixel%2520alignment%2520with%250Avoxel-aligned%2520Gaussians.%2520By%2520directly%2520predicting%2520Gaussians%2520from%2520a%2520predicted%25203D%250Avoxel%2520grid%252C%2520it%2520overcomes%2520pixel%2520alignment%2527s%2520reliance%2520on%2520error-prone%25202D%2520feature%250Amatching%252C%2520ensuring%2520robust%2520multi-view%2520consistency.%2520Furthermore%252C%2520it%2520enables%250Aadaptive%2520control%2520over%2520Gaussian%2520density%2520based%2520on%25203D%2520scene%2520complexity%252C%2520yielding%250Amore%2520faithful%2520Gaussian%2520point%2520clouds%252C%2520improved%2520geometric%2520consistency%252C%2520and%250Aenhanced%2520novel-view%2520rendering%2520quality.%2520Experiments%2520on%2520widely%2520used%2520benchmarks%250Aincluding%2520RealEstate10K%2520and%2520ScanNet%2520demonstrate%2520that%2520VolSplat%2520achieves%250Astate-of-the-art%2520performance%2520while%2520producing%2520more%2520plausible%2520and%2520view-consistent%250AGaussian%2520reconstructions.%2520In%2520addition%2520to%2520superior%2520results%252C%2520our%2520approach%250Aestablishes%2520a%2520more%2520scalable%2520framework%2520for%2520feed-forward%25203D%2520reconstruction%2520with%250Adenser%2520and%2520more%2520robust%2520representations%252C%2520paving%2520the%2520way%2520for%2520further%2520research%2520in%250Awider%2520communities.%2520The%2520video%2520results%252C%2520code%2520and%2520trained%2520models%2520are%2520available%2520on%250Aour%2520project%2520page%253A%2520https%253A//lhmd.top/volsplat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VolSplat%3A%20Rethinking%20Feed-Forward%203D%20Gaussian%20Splatting%20with%0A%20%20Voxel-Aligned%20Prediction&entry.906535625=Weijie%20Wang%20and%20Yeqing%20Chen%20and%20Zeyu%20Zhang%20and%20Hengyu%20Liu%20and%20Haoxiao%20Wang%20and%20Zhiyuan%20Feng%20and%20Wenkang%20Qin%20and%20Zheng%20Zhu%20and%20Donny%20Y.%20Chen%20and%20Bohan%20Zhuang&entry.1292438233=%20%20Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20highly%20effective%0Asolution%20for%20novel%20view%20synthesis.%20Existing%20methods%20predominantly%20rely%20on%20a%0Apixel-aligned%20Gaussian%20prediction%20paradigm%2C%20where%20each%202D%20pixel%20is%20mapped%20to%20a%0A3D%20Gaussian.%20We%20rethink%20this%20widely%20adopted%20formulation%20and%20identify%20several%0Ainherent%20limitations%3A%20it%20renders%20the%20reconstructed%203D%20models%20heavily%20dependent%0Aon%20the%20number%20of%20input%20views%2C%20leads%20to%20view-biased%20density%20distributions%2C%20and%0Aintroduces%20alignment%20errors%2C%20particularly%20when%20source%20views%20contain%20occlusions%0Aor%20low%20texture.%20To%20address%20these%20challenges%2C%20we%20introduce%20VolSplat%2C%20a%20new%0Amulti-view%20feed-forward%20paradigm%20that%20replaces%20pixel%20alignment%20with%0Avoxel-aligned%20Gaussians.%20By%20directly%20predicting%20Gaussians%20from%20a%20predicted%203D%0Avoxel%20grid%2C%20it%20overcomes%20pixel%20alignment%27s%20reliance%20on%20error-prone%202D%20feature%0Amatching%2C%20ensuring%20robust%20multi-view%20consistency.%20Furthermore%2C%20it%20enables%0Aadaptive%20control%20over%20Gaussian%20density%20based%20on%203D%20scene%20complexity%2C%20yielding%0Amore%20faithful%20Gaussian%20point%20clouds%2C%20improved%20geometric%20consistency%2C%20and%0Aenhanced%20novel-view%20rendering%20quality.%20Experiments%20on%20widely%20used%20benchmarks%0Aincluding%20RealEstate10K%20and%20ScanNet%20demonstrate%20that%20VolSplat%20achieves%0Astate-of-the-art%20performance%20while%20producing%20more%20plausible%20and%20view-consistent%0AGaussian%20reconstructions.%20In%20addition%20to%20superior%20results%2C%20our%20approach%0Aestablishes%20a%20more%20scalable%20framework%20for%20feed-forward%203D%20reconstruction%20with%0Adenser%20and%20more%20robust%20representations%2C%20paving%20the%20way%20for%20further%20research%20in%0Awider%20communities.%20The%20video%20results%2C%20code%20and%20trained%20models%20are%20available%20on%0Aour%20project%20page%3A%20https%3A//lhmd.top/volsplat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19297v1&entry.124074799=Read"},
{"title": "Audio-Driven Universal Gaussian Head Avatars", "author": "Kartik Teotia and Helge Rhodin and Mohit Mendiratta and Hyeongwoo Kim and Marc Habermann and Christian Theobalt", "abstract": "  We introduce the first method for audio-driven universal photorealistic\navatar synthesis, combining a person-agnostic speech model with our novel\nUniversal Head Avatar Prior (UHAP). UHAP is trained on cross-identity\nmulti-view videos. In particular, our UHAP is supervised with neutral scan\ndata, enabling it to capture the identity-specific details at high fidelity. In\ncontrast to previous approaches, which predominantly map audio features to\ngeometric deformations only while ignoring audio-dependent appearance\nvariations, our universal speech model directly maps raw audio inputs into the\nUHAP latent expression space. This expression space inherently encodes, both,\ngeometric and appearance variations. For efficient personalization to new\nsubjects, we employ a monocular encoder, which enables lightweight regression\nof dynamic expression variations across video frames. By accounting for these\nexpression-dependent changes, it enables the subsequent model fine-tuning stage\nto focus exclusively on capturing the subject's global appearance and geometry.\nDecoding these audio-driven expression codes via UHAP generates highly\nrealistic avatars with precise lip synchronization and nuanced expressive\ndetails, such as eyebrow movement, gaze shifts, and realistic mouth interior\nappearance as well as motion. Extensive evaluations demonstrate that our method\nis not only the first generalizable audio-driven avatar model that can account\nfor detailed appearance modeling and rendering, but it also outperforms\ncompeting (geometry-only) methods across metrics measuring lip-sync accuracy,\nquantitative image quality, and perceptual realism.\n", "link": "http://arxiv.org/abs/2509.18924v1", "date": "2025-09-23", "relevancy": 3.4547, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.738}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.738}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Driven%20Universal%20Gaussian%20Head%20Avatars&body=Title%3A%20Audio-Driven%20Universal%20Gaussian%20Head%20Avatars%0AAuthor%3A%20Kartik%20Teotia%20and%20Helge%20Rhodin%20and%20Mohit%20Mendiratta%20and%20Hyeongwoo%20Kim%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20We%20introduce%20the%20first%20method%20for%20audio-driven%20universal%20photorealistic%0Aavatar%20synthesis%2C%20combining%20a%20person-agnostic%20speech%20model%20with%20our%20novel%0AUniversal%20Head%20Avatar%20Prior%20%28UHAP%29.%20UHAP%20is%20trained%20on%20cross-identity%0Amulti-view%20videos.%20In%20particular%2C%20our%20UHAP%20is%20supervised%20with%20neutral%20scan%0Adata%2C%20enabling%20it%20to%20capture%20the%20identity-specific%20details%20at%20high%20fidelity.%20In%0Acontrast%20to%20previous%20approaches%2C%20which%20predominantly%20map%20audio%20features%20to%0Ageometric%20deformations%20only%20while%20ignoring%20audio-dependent%20appearance%0Avariations%2C%20our%20universal%20speech%20model%20directly%20maps%20raw%20audio%20inputs%20into%20the%0AUHAP%20latent%20expression%20space.%20This%20expression%20space%20inherently%20encodes%2C%20both%2C%0Ageometric%20and%20appearance%20variations.%20For%20efficient%20personalization%20to%20new%0Asubjects%2C%20we%20employ%20a%20monocular%20encoder%2C%20which%20enables%20lightweight%20regression%0Aof%20dynamic%20expression%20variations%20across%20video%20frames.%20By%20accounting%20for%20these%0Aexpression-dependent%20changes%2C%20it%20enables%20the%20subsequent%20model%20fine-tuning%20stage%0Ato%20focus%20exclusively%20on%20capturing%20the%20subject%27s%20global%20appearance%20and%20geometry.%0ADecoding%20these%20audio-driven%20expression%20codes%20via%20UHAP%20generates%20highly%0Arealistic%20avatars%20with%20precise%20lip%20synchronization%20and%20nuanced%20expressive%0Adetails%2C%20such%20as%20eyebrow%20movement%2C%20gaze%20shifts%2C%20and%20realistic%20mouth%20interior%0Aappearance%20as%20well%20as%20motion.%20Extensive%20evaluations%20demonstrate%20that%20our%20method%0Ais%20not%20only%20the%20first%20generalizable%20audio-driven%20avatar%20model%20that%20can%20account%0Afor%20detailed%20appearance%20modeling%20and%20rendering%2C%20but%20it%20also%20outperforms%0Acompeting%20%28geometry-only%29%20methods%20across%20metrics%20measuring%20lip-sync%20accuracy%2C%0Aquantitative%20image%20quality%2C%20and%20perceptual%20realism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Driven%2520Universal%2520Gaussian%2520Head%2520Avatars%26entry.906535625%3DKartik%2520Teotia%2520and%2520Helge%2520Rhodin%2520and%2520Mohit%2520Mendiratta%2520and%2520Hyeongwoo%2520Kim%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520first%2520method%2520for%2520audio-driven%2520universal%2520photorealistic%250Aavatar%2520synthesis%252C%2520combining%2520a%2520person-agnostic%2520speech%2520model%2520with%2520our%2520novel%250AUniversal%2520Head%2520Avatar%2520Prior%2520%2528UHAP%2529.%2520UHAP%2520is%2520trained%2520on%2520cross-identity%250Amulti-view%2520videos.%2520In%2520particular%252C%2520our%2520UHAP%2520is%2520supervised%2520with%2520neutral%2520scan%250Adata%252C%2520enabling%2520it%2520to%2520capture%2520the%2520identity-specific%2520details%2520at%2520high%2520fidelity.%2520In%250Acontrast%2520to%2520previous%2520approaches%252C%2520which%2520predominantly%2520map%2520audio%2520features%2520to%250Ageometric%2520deformations%2520only%2520while%2520ignoring%2520audio-dependent%2520appearance%250Avariations%252C%2520our%2520universal%2520speech%2520model%2520directly%2520maps%2520raw%2520audio%2520inputs%2520into%2520the%250AUHAP%2520latent%2520expression%2520space.%2520This%2520expression%2520space%2520inherently%2520encodes%252C%2520both%252C%250Ageometric%2520and%2520appearance%2520variations.%2520For%2520efficient%2520personalization%2520to%2520new%250Asubjects%252C%2520we%2520employ%2520a%2520monocular%2520encoder%252C%2520which%2520enables%2520lightweight%2520regression%250Aof%2520dynamic%2520expression%2520variations%2520across%2520video%2520frames.%2520By%2520accounting%2520for%2520these%250Aexpression-dependent%2520changes%252C%2520it%2520enables%2520the%2520subsequent%2520model%2520fine-tuning%2520stage%250Ato%2520focus%2520exclusively%2520on%2520capturing%2520the%2520subject%2527s%2520global%2520appearance%2520and%2520geometry.%250ADecoding%2520these%2520audio-driven%2520expression%2520codes%2520via%2520UHAP%2520generates%2520highly%250Arealistic%2520avatars%2520with%2520precise%2520lip%2520synchronization%2520and%2520nuanced%2520expressive%250Adetails%252C%2520such%2520as%2520eyebrow%2520movement%252C%2520gaze%2520shifts%252C%2520and%2520realistic%2520mouth%2520interior%250Aappearance%2520as%2520well%2520as%2520motion.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520our%2520method%250Ais%2520not%2520only%2520the%2520first%2520generalizable%2520audio-driven%2520avatar%2520model%2520that%2520can%2520account%250Afor%2520detailed%2520appearance%2520modeling%2520and%2520rendering%252C%2520but%2520it%2520also%2520outperforms%250Acompeting%2520%2528geometry-only%2529%2520methods%2520across%2520metrics%2520measuring%2520lip-sync%2520accuracy%252C%250Aquantitative%2520image%2520quality%252C%2520and%2520perceptual%2520realism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Driven%20Universal%20Gaussian%20Head%20Avatars&entry.906535625=Kartik%20Teotia%20and%20Helge%20Rhodin%20and%20Mohit%20Mendiratta%20and%20Hyeongwoo%20Kim%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=%20%20We%20introduce%20the%20first%20method%20for%20audio-driven%20universal%20photorealistic%0Aavatar%20synthesis%2C%20combining%20a%20person-agnostic%20speech%20model%20with%20our%20novel%0AUniversal%20Head%20Avatar%20Prior%20%28UHAP%29.%20UHAP%20is%20trained%20on%20cross-identity%0Amulti-view%20videos.%20In%20particular%2C%20our%20UHAP%20is%20supervised%20with%20neutral%20scan%0Adata%2C%20enabling%20it%20to%20capture%20the%20identity-specific%20details%20at%20high%20fidelity.%20In%0Acontrast%20to%20previous%20approaches%2C%20which%20predominantly%20map%20audio%20features%20to%0Ageometric%20deformations%20only%20while%20ignoring%20audio-dependent%20appearance%0Avariations%2C%20our%20universal%20speech%20model%20directly%20maps%20raw%20audio%20inputs%20into%20the%0AUHAP%20latent%20expression%20space.%20This%20expression%20space%20inherently%20encodes%2C%20both%2C%0Ageometric%20and%20appearance%20variations.%20For%20efficient%20personalization%20to%20new%0Asubjects%2C%20we%20employ%20a%20monocular%20encoder%2C%20which%20enables%20lightweight%20regression%0Aof%20dynamic%20expression%20variations%20across%20video%20frames.%20By%20accounting%20for%20these%0Aexpression-dependent%20changes%2C%20it%20enables%20the%20subsequent%20model%20fine-tuning%20stage%0Ato%20focus%20exclusively%20on%20capturing%20the%20subject%27s%20global%20appearance%20and%20geometry.%0ADecoding%20these%20audio-driven%20expression%20codes%20via%20UHAP%20generates%20highly%0Arealistic%20avatars%20with%20precise%20lip%20synchronization%20and%20nuanced%20expressive%0Adetails%2C%20such%20as%20eyebrow%20movement%2C%20gaze%20shifts%2C%20and%20realistic%20mouth%20interior%0Aappearance%20as%20well%20as%20motion.%20Extensive%20evaluations%20demonstrate%20that%20our%20method%0Ais%20not%20only%20the%20first%20generalizable%20audio-driven%20avatar%20model%20that%20can%20account%0Afor%20detailed%20appearance%20modeling%20and%20rendering%2C%20but%20it%20also%20outperforms%0Acompeting%20%28geometry-only%29%20methods%20across%20metrics%20measuring%20lip-sync%20accuracy%2C%0Aquantitative%20image%20quality%2C%20and%20perceptual%20realism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18924v1&entry.124074799=Read"},
{"title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in\n  Mirror-Containing Environments with Gaussian Splatting", "author": "Zijing Guo and Yunyang Zhao and Lin Wang", "abstract": "  Mirror-containing environments pose unique challenges for 3D reconstruction\nand novel view synthesis (NVS), as reflective surfaces introduce view-dependent\ndistortions and inconsistencies. While cutting-edge methods such as Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical\nscenes, their performance deteriorates in the presence of mirrors. Existing\nsolutions mainly focus on handling mirror surfaces through symmetry mapping but\noften overlook the rich information carried by mirror reflections. These\nreflections offer complementary perspectives that can fill in absent details\nand significantly enhance reconstruction quality. To advance 3D reconstruction\nin mirror-rich environments, we present MirrorScene3D, a comprehensive dataset\nfeaturing diverse indoor scenes, 1256 high-quality images, and annotated mirror\nmasks, providing a benchmark for evaluating reconstruction methods in\nreflective settings. Building on this, we propose ReflectiveGS, an extension of\n3D Gaussian Splatting that utilizes mirror reflections as complementary\nviewpoints rather than simple symmetry artifacts, enhancing scene geometry and\nrecovering absent details. Experiments on MirrorScene3D show that\nReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and\ntraining speed, setting a new benchmark for 3D reconstruction in mirror-rich\nenvironments.\n", "link": "http://arxiv.org/abs/2509.18956v1", "date": "2025-09-23", "relevancy": 3.3573, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.7075}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6811}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Through%20Reflections%3A%20Advancing%203D%20Scene%20Reconstruction%20in%0A%20%20Mirror-Containing%20Environments%20with%20Gaussian%20Splatting&body=Title%3A%20Seeing%20Through%20Reflections%3A%20Advancing%203D%20Scene%20Reconstruction%20in%0A%20%20Mirror-Containing%20Environments%20with%20Gaussian%20Splatting%0AAuthor%3A%20Zijing%20Guo%20and%20Yunyang%20Zhao%20and%20Lin%20Wang%0AAbstract%3A%20%20%20Mirror-containing%20environments%20pose%20unique%20challenges%20for%203D%20reconstruction%0Aand%20novel%20view%20synthesis%20%28NVS%29%2C%20as%20reflective%20surfaces%20introduce%20view-dependent%0Adistortions%20and%20inconsistencies.%20While%20cutting-edge%20methods%20such%20as%20Neural%0ARadiance%20Fields%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%20excel%20in%20typical%0Ascenes%2C%20their%20performance%20deteriorates%20in%20the%20presence%20of%20mirrors.%20Existing%0Asolutions%20mainly%20focus%20on%20handling%20mirror%20surfaces%20through%20symmetry%20mapping%20but%0Aoften%20overlook%20the%20rich%20information%20carried%20by%20mirror%20reflections.%20These%0Areflections%20offer%20complementary%20perspectives%20that%20can%20fill%20in%20absent%20details%0Aand%20significantly%20enhance%20reconstruction%20quality.%20To%20advance%203D%20reconstruction%0Ain%20mirror-rich%20environments%2C%20we%20present%20MirrorScene3D%2C%20a%20comprehensive%20dataset%0Afeaturing%20diverse%20indoor%20scenes%2C%201256%20high-quality%20images%2C%20and%20annotated%20mirror%0Amasks%2C%20providing%20a%20benchmark%20for%20evaluating%20reconstruction%20methods%20in%0Areflective%20settings.%20Building%20on%20this%2C%20we%20propose%20ReflectiveGS%2C%20an%20extension%20of%0A3D%20Gaussian%20Splatting%20that%20utilizes%20mirror%20reflections%20as%20complementary%0Aviewpoints%20rather%20than%20simple%20symmetry%20artifacts%2C%20enhancing%20scene%20geometry%20and%0Arecovering%20absent%20details.%20Experiments%20on%20MirrorScene3D%20show%20that%0AReflectiveGaussian%20outperforms%20existing%20methods%20in%20SSIM%2C%20PSNR%2C%20LPIPS%2C%20and%0Atraining%20speed%2C%20setting%20a%20new%20benchmark%20for%203D%20reconstruction%20in%20mirror-rich%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Through%2520Reflections%253A%2520Advancing%25203D%2520Scene%2520Reconstruction%2520in%250A%2520%2520Mirror-Containing%2520Environments%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DZijing%2520Guo%2520and%2520Yunyang%2520Zhao%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520Mirror-containing%2520environments%2520pose%2520unique%2520challenges%2520for%25203D%2520reconstruction%250Aand%2520novel%2520view%2520synthesis%2520%2528NVS%2529%252C%2520as%2520reflective%2520surfaces%2520introduce%2520view-dependent%250Adistortions%2520and%2520inconsistencies.%2520While%2520cutting-edge%2520methods%2520such%2520as%2520Neural%250ARadiance%2520Fields%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520excel%2520in%2520typical%250Ascenes%252C%2520their%2520performance%2520deteriorates%2520in%2520the%2520presence%2520of%2520mirrors.%2520Existing%250Asolutions%2520mainly%2520focus%2520on%2520handling%2520mirror%2520surfaces%2520through%2520symmetry%2520mapping%2520but%250Aoften%2520overlook%2520the%2520rich%2520information%2520carried%2520by%2520mirror%2520reflections.%2520These%250Areflections%2520offer%2520complementary%2520perspectives%2520that%2520can%2520fill%2520in%2520absent%2520details%250Aand%2520significantly%2520enhance%2520reconstruction%2520quality.%2520To%2520advance%25203D%2520reconstruction%250Ain%2520mirror-rich%2520environments%252C%2520we%2520present%2520MirrorScene3D%252C%2520a%2520comprehensive%2520dataset%250Afeaturing%2520diverse%2520indoor%2520scenes%252C%25201256%2520high-quality%2520images%252C%2520and%2520annotated%2520mirror%250Amasks%252C%2520providing%2520a%2520benchmark%2520for%2520evaluating%2520reconstruction%2520methods%2520in%250Areflective%2520settings.%2520Building%2520on%2520this%252C%2520we%2520propose%2520ReflectiveGS%252C%2520an%2520extension%2520of%250A3D%2520Gaussian%2520Splatting%2520that%2520utilizes%2520mirror%2520reflections%2520as%2520complementary%250Aviewpoints%2520rather%2520than%2520simple%2520symmetry%2520artifacts%252C%2520enhancing%2520scene%2520geometry%2520and%250Arecovering%2520absent%2520details.%2520Experiments%2520on%2520MirrorScene3D%2520show%2520that%250AReflectiveGaussian%2520outperforms%2520existing%2520methods%2520in%2520SSIM%252C%2520PSNR%252C%2520LPIPS%252C%2520and%250Atraining%2520speed%252C%2520setting%2520a%2520new%2520benchmark%2520for%25203D%2520reconstruction%2520in%2520mirror-rich%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Through%20Reflections%3A%20Advancing%203D%20Scene%20Reconstruction%20in%0A%20%20Mirror-Containing%20Environments%20with%20Gaussian%20Splatting&entry.906535625=Zijing%20Guo%20and%20Yunyang%20Zhao%20and%20Lin%20Wang&entry.1292438233=%20%20Mirror-containing%20environments%20pose%20unique%20challenges%20for%203D%20reconstruction%0Aand%20novel%20view%20synthesis%20%28NVS%29%2C%20as%20reflective%20surfaces%20introduce%20view-dependent%0Adistortions%20and%20inconsistencies.%20While%20cutting-edge%20methods%20such%20as%20Neural%0ARadiance%20Fields%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%20excel%20in%20typical%0Ascenes%2C%20their%20performance%20deteriorates%20in%20the%20presence%20of%20mirrors.%20Existing%0Asolutions%20mainly%20focus%20on%20handling%20mirror%20surfaces%20through%20symmetry%20mapping%20but%0Aoften%20overlook%20the%20rich%20information%20carried%20by%20mirror%20reflections.%20These%0Areflections%20offer%20complementary%20perspectives%20that%20can%20fill%20in%20absent%20details%0Aand%20significantly%20enhance%20reconstruction%20quality.%20To%20advance%203D%20reconstruction%0Ain%20mirror-rich%20environments%2C%20we%20present%20MirrorScene3D%2C%20a%20comprehensive%20dataset%0Afeaturing%20diverse%20indoor%20scenes%2C%201256%20high-quality%20images%2C%20and%20annotated%20mirror%0Amasks%2C%20providing%20a%20benchmark%20for%20evaluating%20reconstruction%20methods%20in%0Areflective%20settings.%20Building%20on%20this%2C%20we%20propose%20ReflectiveGS%2C%20an%20extension%20of%0A3D%20Gaussian%20Splatting%20that%20utilizes%20mirror%20reflections%20as%20complementary%0Aviewpoints%20rather%20than%20simple%20symmetry%20artifacts%2C%20enhancing%20scene%20geometry%20and%0Arecovering%20absent%20details.%20Experiments%20on%20MirrorScene3D%20show%20that%0AReflectiveGaussian%20outperforms%20existing%20methods%20in%20SSIM%2C%20PSNR%2C%20LPIPS%2C%20and%0Atraining%20speed%2C%20setting%20a%20new%20benchmark%20for%203D%20reconstruction%20in%20mirror-rich%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18956v1&entry.124074799=Read"},
{"title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian\n  Object Reconstruction", "author": "Hung Nguyen and Runfa Li and An Le and Truong Nguyen", "abstract": "  3D Gaussian Splatting (3DGS) has become a powerful representation for\nimage-based object reconstruction, yet its performance drops sharply in\nsparse-view settings. Prior works address this limitation by employing\ndiffusion models to repair corrupted renders, subsequently using them as pseudo\nground truths for later optimization. While effective, such approaches incur\nheavy computation from the diffusion fine-tuning and repair steps. We present\nWaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object\nreconstruction. Our key idea is to shift diffusion into the wavelet domain:\ndiffusion is applied only to the low-resolution LL subband, while\nhigh-frequency subbands are refined with a lightweight network. We further\npropose an efficient online random masking strategy to curate training pairs\nfor diffusion fine-tuning, replacing the commonly used, but inefficient,\nleave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360\nand OmniObject3D, show WaveletGaussian achieves competitive rendering quality\nwhile substantially reducing training time.\n", "link": "http://arxiv.org/abs/2509.19073v1", "date": "2025-09-23", "relevancy": 3.3082, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.672}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6664}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WaveletGaussian%3A%20Wavelet-domain%20Diffusion%20for%20Sparse-view%203D%20Gaussian%0A%20%20Object%20Reconstruction&body=Title%3A%20WaveletGaussian%3A%20Wavelet-domain%20Diffusion%20for%20Sparse-view%203D%20Gaussian%0A%20%20Object%20Reconstruction%0AAuthor%3A%20Hung%20Nguyen%20and%20Runfa%20Li%20and%20An%20Le%20and%20Truong%20Nguyen%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20a%20powerful%20representation%20for%0Aimage-based%20object%20reconstruction%2C%20yet%20its%20performance%20drops%20sharply%20in%0Asparse-view%20settings.%20Prior%20works%20address%20this%20limitation%20by%20employing%0Adiffusion%20models%20to%20repair%20corrupted%20renders%2C%20subsequently%20using%20them%20as%20pseudo%0Aground%20truths%20for%20later%20optimization.%20While%20effective%2C%20such%20approaches%20incur%0Aheavy%20computation%20from%20the%20diffusion%20fine-tuning%20and%20repair%20steps.%20We%20present%0AWaveletGaussian%2C%20a%20framework%20for%20more%20efficient%20sparse-view%203D%20Gaussian%20object%0Areconstruction.%20Our%20key%20idea%20is%20to%20shift%20diffusion%20into%20the%20wavelet%20domain%3A%0Adiffusion%20is%20applied%20only%20to%20the%20low-resolution%20LL%20subband%2C%20while%0Ahigh-frequency%20subbands%20are%20refined%20with%20a%20lightweight%20network.%20We%20further%0Apropose%20an%20efficient%20online%20random%20masking%20strategy%20to%20curate%20training%20pairs%0Afor%20diffusion%20fine-tuning%2C%20replacing%20the%20commonly%20used%2C%20but%20inefficient%2C%0Aleave-one-out%20strategy.%20Experiments%20across%20two%20benchmark%20datasets%2C%20Mip-NeRF%20360%0Aand%20OmniObject3D%2C%20show%20WaveletGaussian%20achieves%20competitive%20rendering%20quality%0Awhile%20substantially%20reducing%20training%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaveletGaussian%253A%2520Wavelet-domain%2520Diffusion%2520for%2520Sparse-view%25203D%2520Gaussian%250A%2520%2520Object%2520Reconstruction%26entry.906535625%3DHung%2520Nguyen%2520and%2520Runfa%2520Li%2520and%2520An%2520Le%2520and%2520Truong%2520Nguyen%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520become%2520a%2520powerful%2520representation%2520for%250Aimage-based%2520object%2520reconstruction%252C%2520yet%2520its%2520performance%2520drops%2520sharply%2520in%250Asparse-view%2520settings.%2520Prior%2520works%2520address%2520this%2520limitation%2520by%2520employing%250Adiffusion%2520models%2520to%2520repair%2520corrupted%2520renders%252C%2520subsequently%2520using%2520them%2520as%2520pseudo%250Aground%2520truths%2520for%2520later%2520optimization.%2520While%2520effective%252C%2520such%2520approaches%2520incur%250Aheavy%2520computation%2520from%2520the%2520diffusion%2520fine-tuning%2520and%2520repair%2520steps.%2520We%2520present%250AWaveletGaussian%252C%2520a%2520framework%2520for%2520more%2520efficient%2520sparse-view%25203D%2520Gaussian%2520object%250Areconstruction.%2520Our%2520key%2520idea%2520is%2520to%2520shift%2520diffusion%2520into%2520the%2520wavelet%2520domain%253A%250Adiffusion%2520is%2520applied%2520only%2520to%2520the%2520low-resolution%2520LL%2520subband%252C%2520while%250Ahigh-frequency%2520subbands%2520are%2520refined%2520with%2520a%2520lightweight%2520network.%2520We%2520further%250Apropose%2520an%2520efficient%2520online%2520random%2520masking%2520strategy%2520to%2520curate%2520training%2520pairs%250Afor%2520diffusion%2520fine-tuning%252C%2520replacing%2520the%2520commonly%2520used%252C%2520but%2520inefficient%252C%250Aleave-one-out%2520strategy.%2520Experiments%2520across%2520two%2520benchmark%2520datasets%252C%2520Mip-NeRF%2520360%250Aand%2520OmniObject3D%252C%2520show%2520WaveletGaussian%2520achieves%2520competitive%2520rendering%2520quality%250Awhile%2520substantially%2520reducing%2520training%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaveletGaussian%3A%20Wavelet-domain%20Diffusion%20for%20Sparse-view%203D%20Gaussian%0A%20%20Object%20Reconstruction&entry.906535625=Hung%20Nguyen%20and%20Runfa%20Li%20and%20An%20Le%20and%20Truong%20Nguyen&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20a%20powerful%20representation%20for%0Aimage-based%20object%20reconstruction%2C%20yet%20its%20performance%20drops%20sharply%20in%0Asparse-view%20settings.%20Prior%20works%20address%20this%20limitation%20by%20employing%0Adiffusion%20models%20to%20repair%20corrupted%20renders%2C%20subsequently%20using%20them%20as%20pseudo%0Aground%20truths%20for%20later%20optimization.%20While%20effective%2C%20such%20approaches%20incur%0Aheavy%20computation%20from%20the%20diffusion%20fine-tuning%20and%20repair%20steps.%20We%20present%0AWaveletGaussian%2C%20a%20framework%20for%20more%20efficient%20sparse-view%203D%20Gaussian%20object%0Areconstruction.%20Our%20key%20idea%20is%20to%20shift%20diffusion%20into%20the%20wavelet%20domain%3A%0Adiffusion%20is%20applied%20only%20to%20the%20low-resolution%20LL%20subband%2C%20while%0Ahigh-frequency%20subbands%20are%20refined%20with%20a%20lightweight%20network.%20We%20further%0Apropose%20an%20efficient%20online%20random%20masking%20strategy%20to%20curate%20training%20pairs%0Afor%20diffusion%20fine-tuning%2C%20replacing%20the%20commonly%20used%2C%20but%20inefficient%2C%0Aleave-one-out%20strategy.%20Experiments%20across%20two%20benchmark%20datasets%2C%20Mip-NeRF%20360%0Aand%20OmniObject3D%2C%20show%20WaveletGaussian%20achieves%20competitive%20rendering%20quality%0Awhile%20substantially%20reducing%20training%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19073v1&entry.124074799=Read"},
{"title": "SpinMeRound: Consistent Multi-View Identity Generation Using Diffusion\n  Models", "author": "Stathis Galanakis and Alexandros Lattas and Stylianos Moschoglou and Bernhard Kainz and Stefanos Zafeiriou", "abstract": "  Despite recent progress in diffusion models, generating realistic head\nportraits from novel viewpoints remains a significant challenge. Most current\napproaches are constrained to limited angular ranges, predominantly focusing on\nfrontal or near-frontal views. Moreover, although the recent emerging\nlarge-scale diffusion models have been proven robust in handling 3D scenes,\nthey underperform on facial data, given their complex structure and the uncanny\nvalley pitfalls. In this paper, we propose SpinMeRound, a diffusion-based\napproach designed to generate consistent and accurate head portraits from novel\nviewpoints. By leveraging a number of input views alongside an identity\nembedding, our method effectively synthesizes diverse viewpoints of a subject\nwhilst robustly maintaining its unique identity features. Through\nexperimentation, we showcase our model's generation capabilities in 360 head\nsynthesis, while beating current state-of-the-art multiview diffusion models.\n", "link": "http://arxiv.org/abs/2504.10716v2", "date": "2025-09-23", "relevancy": 3.2851, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6662}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6662}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpinMeRound%3A%20Consistent%20Multi-View%20Identity%20Generation%20Using%20Diffusion%0A%20%20Models&body=Title%3A%20SpinMeRound%3A%20Consistent%20Multi-View%20Identity%20Generation%20Using%20Diffusion%0A%20%20Models%0AAuthor%3A%20Stathis%20Galanakis%20and%20Alexandros%20Lattas%20and%20Stylianos%20Moschoglou%20and%20Bernhard%20Kainz%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20%20%20Despite%20recent%20progress%20in%20diffusion%20models%2C%20generating%20realistic%20head%0Aportraits%20from%20novel%20viewpoints%20remains%20a%20significant%20challenge.%20Most%20current%0Aapproaches%20are%20constrained%20to%20limited%20angular%20ranges%2C%20predominantly%20focusing%20on%0Afrontal%20or%20near-frontal%20views.%20Moreover%2C%20although%20the%20recent%20emerging%0Alarge-scale%20diffusion%20models%20have%20been%20proven%20robust%20in%20handling%203D%20scenes%2C%0Athey%20underperform%20on%20facial%20data%2C%20given%20their%20complex%20structure%20and%20the%20uncanny%0Avalley%20pitfalls.%20In%20this%20paper%2C%20we%20propose%20SpinMeRound%2C%20a%20diffusion-based%0Aapproach%20designed%20to%20generate%20consistent%20and%20accurate%20head%20portraits%20from%20novel%0Aviewpoints.%20By%20leveraging%20a%20number%20of%20input%20views%20alongside%20an%20identity%0Aembedding%2C%20our%20method%20effectively%20synthesizes%20diverse%20viewpoints%20of%20a%20subject%0Awhilst%20robustly%20maintaining%20its%20unique%20identity%20features.%20Through%0Aexperimentation%2C%20we%20showcase%20our%20model%27s%20generation%20capabilities%20in%20360%20head%0Asynthesis%2C%20while%20beating%20current%20state-of-the-art%20multiview%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpinMeRound%253A%2520Consistent%2520Multi-View%2520Identity%2520Generation%2520Using%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DStathis%2520Galanakis%2520and%2520Alexandros%2520Lattas%2520and%2520Stylianos%2520Moschoglou%2520and%2520Bernhard%2520Kainz%2520and%2520Stefanos%2520Zafeiriou%26entry.1292438233%3D%2520%2520Despite%2520recent%2520progress%2520in%2520diffusion%2520models%252C%2520generating%2520realistic%2520head%250Aportraits%2520from%2520novel%2520viewpoints%2520remains%2520a%2520significant%2520challenge.%2520Most%2520current%250Aapproaches%2520are%2520constrained%2520to%2520limited%2520angular%2520ranges%252C%2520predominantly%2520focusing%2520on%250Afrontal%2520or%2520near-frontal%2520views.%2520Moreover%252C%2520although%2520the%2520recent%2520emerging%250Alarge-scale%2520diffusion%2520models%2520have%2520been%2520proven%2520robust%2520in%2520handling%25203D%2520scenes%252C%250Athey%2520underperform%2520on%2520facial%2520data%252C%2520given%2520their%2520complex%2520structure%2520and%2520the%2520uncanny%250Avalley%2520pitfalls.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SpinMeRound%252C%2520a%2520diffusion-based%250Aapproach%2520designed%2520to%2520generate%2520consistent%2520and%2520accurate%2520head%2520portraits%2520from%2520novel%250Aviewpoints.%2520By%2520leveraging%2520a%2520number%2520of%2520input%2520views%2520alongside%2520an%2520identity%250Aembedding%252C%2520our%2520method%2520effectively%2520synthesizes%2520diverse%2520viewpoints%2520of%2520a%2520subject%250Awhilst%2520robustly%2520maintaining%2520its%2520unique%2520identity%2520features.%2520Through%250Aexperimentation%252C%2520we%2520showcase%2520our%2520model%2527s%2520generation%2520capabilities%2520in%2520360%2520head%250Asynthesis%252C%2520while%2520beating%2520current%2520state-of-the-art%2520multiview%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpinMeRound%3A%20Consistent%20Multi-View%20Identity%20Generation%20Using%20Diffusion%0A%20%20Models&entry.906535625=Stathis%20Galanakis%20and%20Alexandros%20Lattas%20and%20Stylianos%20Moschoglou%20and%20Bernhard%20Kainz%20and%20Stefanos%20Zafeiriou&entry.1292438233=%20%20Despite%20recent%20progress%20in%20diffusion%20models%2C%20generating%20realistic%20head%0Aportraits%20from%20novel%20viewpoints%20remains%20a%20significant%20challenge.%20Most%20current%0Aapproaches%20are%20constrained%20to%20limited%20angular%20ranges%2C%20predominantly%20focusing%20on%0Afrontal%20or%20near-frontal%20views.%20Moreover%2C%20although%20the%20recent%20emerging%0Alarge-scale%20diffusion%20models%20have%20been%20proven%20robust%20in%20handling%203D%20scenes%2C%0Athey%20underperform%20on%20facial%20data%2C%20given%20their%20complex%20structure%20and%20the%20uncanny%0Avalley%20pitfalls.%20In%20this%20paper%2C%20we%20propose%20SpinMeRound%2C%20a%20diffusion-based%0Aapproach%20designed%20to%20generate%20consistent%20and%20accurate%20head%20portraits%20from%20novel%0Aviewpoints.%20By%20leveraging%20a%20number%20of%20input%20views%20alongside%20an%20identity%0Aembedding%2C%20our%20method%20effectively%20synthesizes%20diverse%20viewpoints%20of%20a%20subject%0Awhilst%20robustly%20maintaining%20its%20unique%20identity%20features.%20Through%0Aexperimentation%2C%20we%20showcase%20our%20model%27s%20generation%20capabilities%20in%20360%20head%0Asynthesis%2C%20while%20beating%20current%20state-of-the-art%20multiview%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10716v2&entry.124074799=Read"},
{"title": "MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians\n  and Unified Pruning", "author": "Jiarui Chen and Yikeng Chen and Yingshuang Zou and Ye Huang and Peng Wang and Yuan Liu and Yujing Sun and Wenping Wang", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis\ntechnique, but its high memory consumption severely limits its applicability on\nedge devices. A growing number of 3DGS compression methods have been proposed\nto make 3DGS more efficient, yet most only focus on storage compression and\nfail to address the critical bottleneck of rendering memory. To address this\nproblem, we introduce MEGS$^{2}$, a novel memory-efficient framework that\ntackles this challenge by jointly optimizing two key factors: the total\nprimitive number and the parameters per primitive, achieving unprecedented\nmemory compression. Specifically, we replace the memory-intensive spherical\nharmonics with lightweight, arbitrarily oriented spherical Gaussian lobes as\nour color representations. More importantly, we propose a unified soft pruning\nframework that models primitive-number and lobe-number pruning as a single\nconstrained optimization problem. Experiments show that MEGS$^{2}$ achieves a\n50% static VRAM reduction and a 40% rendering VRAM reduction compared to\nexisting methods, while maintaining comparable rendering quality. Project page:\nhttps://megs-2.github.io/\n", "link": "http://arxiv.org/abs/2509.07021v2", "date": "2025-09-23", "relevancy": 3.2679, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6741}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.646}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGS%24%5E%7B2%7D%24%3A%20Memory-Efficient%20Gaussian%20Splatting%20via%20Spherical%20Gaussians%0A%20%20and%20Unified%20Pruning&body=Title%3A%20MEGS%24%5E%7B2%7D%24%3A%20Memory-Efficient%20Gaussian%20Splatting%20via%20Spherical%20Gaussians%0A%20%20and%20Unified%20Pruning%0AAuthor%3A%20Jiarui%20Chen%20and%20Yikeng%20Chen%20and%20Yingshuang%20Zou%20and%20Ye%20Huang%20and%20Peng%20Wang%20and%20Yuan%20Liu%20and%20Yujing%20Sun%20and%20Wenping%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20dominant%20novel-view%20synthesis%0Atechnique%2C%20but%20its%20high%20memory%20consumption%20severely%20limits%20its%20applicability%20on%0Aedge%20devices.%20A%20growing%20number%20of%203DGS%20compression%20methods%20have%20been%20proposed%0Ato%20make%203DGS%20more%20efficient%2C%20yet%20most%20only%20focus%20on%20storage%20compression%20and%0Afail%20to%20address%20the%20critical%20bottleneck%20of%20rendering%20memory.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20MEGS%24%5E%7B2%7D%24%2C%20a%20novel%20memory-efficient%20framework%20that%0Atackles%20this%20challenge%20by%20jointly%20optimizing%20two%20key%20factors%3A%20the%20total%0Aprimitive%20number%20and%20the%20parameters%20per%20primitive%2C%20achieving%20unprecedented%0Amemory%20compression.%20Specifically%2C%20we%20replace%20the%20memory-intensive%20spherical%0Aharmonics%20with%20lightweight%2C%20arbitrarily%20oriented%20spherical%20Gaussian%20lobes%20as%0Aour%20color%20representations.%20More%20importantly%2C%20we%20propose%20a%20unified%20soft%20pruning%0Aframework%20that%20models%20primitive-number%20and%20lobe-number%20pruning%20as%20a%20single%0Aconstrained%20optimization%20problem.%20Experiments%20show%20that%20MEGS%24%5E%7B2%7D%24%20achieves%20a%0A50%25%20static%20VRAM%20reduction%20and%20a%2040%25%20rendering%20VRAM%20reduction%20compared%20to%0Aexisting%20methods%2C%20while%20maintaining%20comparable%20rendering%20quality.%20Project%20page%3A%0Ahttps%3A//megs-2.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07021v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGS%2524%255E%257B2%257D%2524%253A%2520Memory-Efficient%2520Gaussian%2520Splatting%2520via%2520Spherical%2520Gaussians%250A%2520%2520and%2520Unified%2520Pruning%26entry.906535625%3DJiarui%2520Chen%2520and%2520Yikeng%2520Chen%2520and%2520Yingshuang%2520Zou%2520and%2520Ye%2520Huang%2520and%2520Peng%2520Wang%2520and%2520Yuan%2520Liu%2520and%2520Yujing%2520Sun%2520and%2520Wenping%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520dominant%2520novel-view%2520synthesis%250Atechnique%252C%2520but%2520its%2520high%2520memory%2520consumption%2520severely%2520limits%2520its%2520applicability%2520on%250Aedge%2520devices.%2520A%2520growing%2520number%2520of%25203DGS%2520compression%2520methods%2520have%2520been%2520proposed%250Ato%2520make%25203DGS%2520more%2520efficient%252C%2520yet%2520most%2520only%2520focus%2520on%2520storage%2520compression%2520and%250Afail%2520to%2520address%2520the%2520critical%2520bottleneck%2520of%2520rendering%2520memory.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520introduce%2520MEGS%2524%255E%257B2%257D%2524%252C%2520a%2520novel%2520memory-efficient%2520framework%2520that%250Atackles%2520this%2520challenge%2520by%2520jointly%2520optimizing%2520two%2520key%2520factors%253A%2520the%2520total%250Aprimitive%2520number%2520and%2520the%2520parameters%2520per%2520primitive%252C%2520achieving%2520unprecedented%250Amemory%2520compression.%2520Specifically%252C%2520we%2520replace%2520the%2520memory-intensive%2520spherical%250Aharmonics%2520with%2520lightweight%252C%2520arbitrarily%2520oriented%2520spherical%2520Gaussian%2520lobes%2520as%250Aour%2520color%2520representations.%2520More%2520importantly%252C%2520we%2520propose%2520a%2520unified%2520soft%2520pruning%250Aframework%2520that%2520models%2520primitive-number%2520and%2520lobe-number%2520pruning%2520as%2520a%2520single%250Aconstrained%2520optimization%2520problem.%2520Experiments%2520show%2520that%2520MEGS%2524%255E%257B2%257D%2524%2520achieves%2520a%250A50%2525%2520static%2520VRAM%2520reduction%2520and%2520a%252040%2525%2520rendering%2520VRAM%2520reduction%2520compared%2520to%250Aexisting%2520methods%252C%2520while%2520maintaining%2520comparable%2520rendering%2520quality.%2520Project%2520page%253A%250Ahttps%253A//megs-2.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07021v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGS%24%5E%7B2%7D%24%3A%20Memory-Efficient%20Gaussian%20Splatting%20via%20Spherical%20Gaussians%0A%20%20and%20Unified%20Pruning&entry.906535625=Jiarui%20Chen%20and%20Yikeng%20Chen%20and%20Yingshuang%20Zou%20and%20Ye%20Huang%20and%20Peng%20Wang%20and%20Yuan%20Liu%20and%20Yujing%20Sun%20and%20Wenping%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20dominant%20novel-view%20synthesis%0Atechnique%2C%20but%20its%20high%20memory%20consumption%20severely%20limits%20its%20applicability%20on%0Aedge%20devices.%20A%20growing%20number%20of%203DGS%20compression%20methods%20have%20been%20proposed%0Ato%20make%203DGS%20more%20efficient%2C%20yet%20most%20only%20focus%20on%20storage%20compression%20and%0Afail%20to%20address%20the%20critical%20bottleneck%20of%20rendering%20memory.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20MEGS%24%5E%7B2%7D%24%2C%20a%20novel%20memory-efficient%20framework%20that%0Atackles%20this%20challenge%20by%20jointly%20optimizing%20two%20key%20factors%3A%20the%20total%0Aprimitive%20number%20and%20the%20parameters%20per%20primitive%2C%20achieving%20unprecedented%0Amemory%20compression.%20Specifically%2C%20we%20replace%20the%20memory-intensive%20spherical%0Aharmonics%20with%20lightweight%2C%20arbitrarily%20oriented%20spherical%20Gaussian%20lobes%20as%0Aour%20color%20representations.%20More%20importantly%2C%20we%20propose%20a%20unified%20soft%20pruning%0Aframework%20that%20models%20primitive-number%20and%20lobe-number%20pruning%20as%20a%20single%0Aconstrained%20optimization%20problem.%20Experiments%20show%20that%20MEGS%24%5E%7B2%7D%24%20achieves%20a%0A50%25%20static%20VRAM%20reduction%20and%20a%2040%25%20rendering%20VRAM%20reduction%20compared%20to%0Aexisting%20methods%2C%20while%20maintaining%20comparable%20rendering%20quality.%20Project%20page%3A%0Ahttps%3A//megs-2.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07021v2&entry.124074799=Read"},
{"title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene\n  Descriptions", "author": "Ioanna Ntinou and Alexandros Xenos and Yassine Ouali and Adrian Bulat and Georgios Tzimiropoulos", "abstract": "  Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have\nbecome the standard approach for learning discriminative vision-language\nrepresentations. However, these models often exhibit shallow language\nunderstanding, manifesting bag-of-words behaviour. These limitations are\nreinforced by their dual-encoder design, which induces a modality gap.\nAdditionally, the reliance on vast web-collected data corpora for training\nmakes the process computationally expensive and introduces significant privacy\nconcerns. To address these limitations, in this work, we challenge the\nnecessity of vision encoders for retrieval tasks by introducing a vision-free,\nsingle-encoder retrieval pipeline. Departing from the traditional text-to-image\nretrieval paradigm, we migrate to a text-to-text paradigm with the assistance\nof VLLM-generated structured image descriptions. We demonstrate that this\nparadigm shift has significant advantages, including a substantial reduction of\nthe modality gap, improved compositionality, and better performance on short\nand long caption queries, all attainable with only a few hours of calibration\non two GPUs. Additionally, substituting raw images with textual descriptions\nintroduces a more privacy-friendly alternative for retrieval. To further assess\ngeneralisation and address some of the shortcomings of prior compositionality\nbenchmarks, we release two benchmarks derived from Flickr30k and COCO,\ncontaining diverse compositional queries made of short captions, which we coin\nsubFlickr and subCOCO. Our vision-free retriever matches and often surpasses\ntraditional multimodal models. Importantly, our approach achieves\nstate-of-the-art zero-shot performance on multiple retrieval and\ncompositionality benchmarks, with models as small as 0.3B parameters. Code is\navailable at: https://github.com/IoannaNti/LexiCLIP\n", "link": "http://arxiv.org/abs/2509.19203v1", "date": "2025-09-23", "relevancy": 3.2624, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Free%20Retrieval%3A%20Rethinking%20Multimodal%20Search%20with%20Textual%20Scene%0A%20%20Descriptions&body=Title%3A%20Vision-Free%20Retrieval%3A%20Rethinking%20Multimodal%20Search%20with%20Textual%20Scene%0A%20%20Descriptions%0AAuthor%3A%20Ioanna%20Ntinou%20and%20Alexandros%20Xenos%20and%20Yassine%20Ouali%20and%20Adrian%20Bulat%20and%20Georgios%20Tzimiropoulos%0AAbstract%3A%20%20%20Contrastively-trained%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%0Abecome%20the%20standard%20approach%20for%20learning%20discriminative%20vision-language%0Arepresentations.%20However%2C%20these%20models%20often%20exhibit%20shallow%20language%0Aunderstanding%2C%20manifesting%20bag-of-words%20behaviour.%20These%20limitations%20are%0Areinforced%20by%20their%20dual-encoder%20design%2C%20which%20induces%20a%20modality%20gap.%0AAdditionally%2C%20the%20reliance%20on%20vast%20web-collected%20data%20corpora%20for%20training%0Amakes%20the%20process%20computationally%20expensive%20and%20introduces%20significant%20privacy%0Aconcerns.%20To%20address%20these%20limitations%2C%20in%20this%20work%2C%20we%20challenge%20the%0Anecessity%20of%20vision%20encoders%20for%20retrieval%20tasks%20by%20introducing%20a%20vision-free%2C%0Asingle-encoder%20retrieval%20pipeline.%20Departing%20from%20the%20traditional%20text-to-image%0Aretrieval%20paradigm%2C%20we%20migrate%20to%20a%20text-to-text%20paradigm%20with%20the%20assistance%0Aof%20VLLM-generated%20structured%20image%20descriptions.%20We%20demonstrate%20that%20this%0Aparadigm%20shift%20has%20significant%20advantages%2C%20including%20a%20substantial%20reduction%20of%0Athe%20modality%20gap%2C%20improved%20compositionality%2C%20and%20better%20performance%20on%20short%0Aand%20long%20caption%20queries%2C%20all%20attainable%20with%20only%20a%20few%20hours%20of%20calibration%0Aon%20two%20GPUs.%20Additionally%2C%20substituting%20raw%20images%20with%20textual%20descriptions%0Aintroduces%20a%20more%20privacy-friendly%20alternative%20for%20retrieval.%20To%20further%20assess%0Ageneralisation%20and%20address%20some%20of%20the%20shortcomings%20of%20prior%20compositionality%0Abenchmarks%2C%20we%20release%20two%20benchmarks%20derived%20from%20Flickr30k%20and%20COCO%2C%0Acontaining%20diverse%20compositional%20queries%20made%20of%20short%20captions%2C%20which%20we%20coin%0AsubFlickr%20and%20subCOCO.%20Our%20vision-free%20retriever%20matches%20and%20often%20surpasses%0Atraditional%20multimodal%20models.%20Importantly%2C%20our%20approach%20achieves%0Astate-of-the-art%20zero-shot%20performance%20on%20multiple%20retrieval%20and%0Acompositionality%20benchmarks%2C%20with%20models%20as%20small%20as%200.3B%20parameters.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/IoannaNti/LexiCLIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Free%2520Retrieval%253A%2520Rethinking%2520Multimodal%2520Search%2520with%2520Textual%2520Scene%250A%2520%2520Descriptions%26entry.906535625%3DIoanna%2520Ntinou%2520and%2520Alexandros%2520Xenos%2520and%2520Yassine%2520Ouali%2520and%2520Adrian%2520Bulat%2520and%2520Georgios%2520Tzimiropoulos%26entry.1292438233%3D%2520%2520Contrastively-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520have%250Abecome%2520the%2520standard%2520approach%2520for%2520learning%2520discriminative%2520vision-language%250Arepresentations.%2520However%252C%2520these%2520models%2520often%2520exhibit%2520shallow%2520language%250Aunderstanding%252C%2520manifesting%2520bag-of-words%2520behaviour.%2520These%2520limitations%2520are%250Areinforced%2520by%2520their%2520dual-encoder%2520design%252C%2520which%2520induces%2520a%2520modality%2520gap.%250AAdditionally%252C%2520the%2520reliance%2520on%2520vast%2520web-collected%2520data%2520corpora%2520for%2520training%250Amakes%2520the%2520process%2520computationally%2520expensive%2520and%2520introduces%2520significant%2520privacy%250Aconcerns.%2520To%2520address%2520these%2520limitations%252C%2520in%2520this%2520work%252C%2520we%2520challenge%2520the%250Anecessity%2520of%2520vision%2520encoders%2520for%2520retrieval%2520tasks%2520by%2520introducing%2520a%2520vision-free%252C%250Asingle-encoder%2520retrieval%2520pipeline.%2520Departing%2520from%2520the%2520traditional%2520text-to-image%250Aretrieval%2520paradigm%252C%2520we%2520migrate%2520to%2520a%2520text-to-text%2520paradigm%2520with%2520the%2520assistance%250Aof%2520VLLM-generated%2520structured%2520image%2520descriptions.%2520We%2520demonstrate%2520that%2520this%250Aparadigm%2520shift%2520has%2520significant%2520advantages%252C%2520including%2520a%2520substantial%2520reduction%2520of%250Athe%2520modality%2520gap%252C%2520improved%2520compositionality%252C%2520and%2520better%2520performance%2520on%2520short%250Aand%2520long%2520caption%2520queries%252C%2520all%2520attainable%2520with%2520only%2520a%2520few%2520hours%2520of%2520calibration%250Aon%2520two%2520GPUs.%2520Additionally%252C%2520substituting%2520raw%2520images%2520with%2520textual%2520descriptions%250Aintroduces%2520a%2520more%2520privacy-friendly%2520alternative%2520for%2520retrieval.%2520To%2520further%2520assess%250Ageneralisation%2520and%2520address%2520some%2520of%2520the%2520shortcomings%2520of%2520prior%2520compositionality%250Abenchmarks%252C%2520we%2520release%2520two%2520benchmarks%2520derived%2520from%2520Flickr30k%2520and%2520COCO%252C%250Acontaining%2520diverse%2520compositional%2520queries%2520made%2520of%2520short%2520captions%252C%2520which%2520we%2520coin%250AsubFlickr%2520and%2520subCOCO.%2520Our%2520vision-free%2520retriever%2520matches%2520and%2520often%2520surpasses%250Atraditional%2520multimodal%2520models.%2520Importantly%252C%2520our%2520approach%2520achieves%250Astate-of-the-art%2520zero-shot%2520performance%2520on%2520multiple%2520retrieval%2520and%250Acompositionality%2520benchmarks%252C%2520with%2520models%2520as%2520small%2520as%25200.3B%2520parameters.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/IoannaNti/LexiCLIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Free%20Retrieval%3A%20Rethinking%20Multimodal%20Search%20with%20Textual%20Scene%0A%20%20Descriptions&entry.906535625=Ioanna%20Ntinou%20and%20Alexandros%20Xenos%20and%20Yassine%20Ouali%20and%20Adrian%20Bulat%20and%20Georgios%20Tzimiropoulos&entry.1292438233=%20%20Contrastively-trained%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%0Abecome%20the%20standard%20approach%20for%20learning%20discriminative%20vision-language%0Arepresentations.%20However%2C%20these%20models%20often%20exhibit%20shallow%20language%0Aunderstanding%2C%20manifesting%20bag-of-words%20behaviour.%20These%20limitations%20are%0Areinforced%20by%20their%20dual-encoder%20design%2C%20which%20induces%20a%20modality%20gap.%0AAdditionally%2C%20the%20reliance%20on%20vast%20web-collected%20data%20corpora%20for%20training%0Amakes%20the%20process%20computationally%20expensive%20and%20introduces%20significant%20privacy%0Aconcerns.%20To%20address%20these%20limitations%2C%20in%20this%20work%2C%20we%20challenge%20the%0Anecessity%20of%20vision%20encoders%20for%20retrieval%20tasks%20by%20introducing%20a%20vision-free%2C%0Asingle-encoder%20retrieval%20pipeline.%20Departing%20from%20the%20traditional%20text-to-image%0Aretrieval%20paradigm%2C%20we%20migrate%20to%20a%20text-to-text%20paradigm%20with%20the%20assistance%0Aof%20VLLM-generated%20structured%20image%20descriptions.%20We%20demonstrate%20that%20this%0Aparadigm%20shift%20has%20significant%20advantages%2C%20including%20a%20substantial%20reduction%20of%0Athe%20modality%20gap%2C%20improved%20compositionality%2C%20and%20better%20performance%20on%20short%0Aand%20long%20caption%20queries%2C%20all%20attainable%20with%20only%20a%20few%20hours%20of%20calibration%0Aon%20two%20GPUs.%20Additionally%2C%20substituting%20raw%20images%20with%20textual%20descriptions%0Aintroduces%20a%20more%20privacy-friendly%20alternative%20for%20retrieval.%20To%20further%20assess%0Ageneralisation%20and%20address%20some%20of%20the%20shortcomings%20of%20prior%20compositionality%0Abenchmarks%2C%20we%20release%20two%20benchmarks%20derived%20from%20Flickr30k%20and%20COCO%2C%0Acontaining%20diverse%20compositional%20queries%20made%20of%20short%20captions%2C%20which%20we%20coin%0AsubFlickr%20and%20subCOCO.%20Our%20vision-free%20retriever%20matches%20and%20often%20surpasses%0Atraditional%20multimodal%20models.%20Importantly%2C%20our%20approach%20achieves%0Astate-of-the-art%20zero-shot%20performance%20on%20multiple%20retrieval%20and%0Acompositionality%20benchmarks%2C%20with%20models%20as%20small%20as%200.3B%20parameters.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/IoannaNti/LexiCLIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19203v1&entry.124074799=Read"},
{"title": "Reading Images Like Texts: Sequential Image Understanding in\n  Vision-Language Models", "author": "Yueyan Li and Chenggong Zhao and Zeyuan Zang and Caixia Yuan and Xiaojie Wang", "abstract": "  Vision-Language Models (VLMs) have demonstrated remarkable performance across\na variety of real-world tasks. However, existing VLMs typically process visual\ninformation by serializing images, a method that diverges significantly from\nthe parallel nature of human vision. Moreover, their opaque internal mechanisms\nhinder both deeper understanding and architectural innovation. Inspired by the\ndual-stream hypothesis of human vision, which distinguishes the \"what\" and\n\"where\" pathways, we deconstruct the visual processing in VLMs into object\nrecognition and spatial perception for separate study. For object recognition,\nwe convert images into text token maps and find that the model's perception of\nimage content unfolds as a two-stage process from shallow to deep layers,\nbeginning with attribute recognition and culminating in semantic\ndisambiguation. For spatial perception, we theoretically derive and empirically\nverify the geometric structure underlying the positional representation in\nVLMs. Based on these findings, we introduce an instruction-agnostic token\ncompression algorithm based on a plug-and-play visual decoder to improve\ndecoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.\nThrough rigorous experiments, our work validates these analyses, offering a\ndeeper understanding of VLM internals and providing clear principles for\ndesigning more capable future architectures.\n", "link": "http://arxiv.org/abs/2509.19191v1", "date": "2025-09-23", "relevancy": 3.2555, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6745}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6745}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reading%20Images%20Like%20Texts%3A%20Sequential%20Image%20Understanding%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20Reading%20Images%20Like%20Texts%3A%20Sequential%20Image%20Understanding%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Yueyan%20Li%20and%20Chenggong%20Zhao%20and%20Zeyuan%20Zang%20and%20Caixia%20Yuan%20and%20Xiaojie%20Wang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Aa%20variety%20of%20real-world%20tasks.%20However%2C%20existing%20VLMs%20typically%20process%20visual%0Ainformation%20by%20serializing%20images%2C%20a%20method%20that%20diverges%20significantly%20from%0Athe%20parallel%20nature%20of%20human%20vision.%20Moreover%2C%20their%20opaque%20internal%20mechanisms%0Ahinder%20both%20deeper%20understanding%20and%20architectural%20innovation.%20Inspired%20by%20the%0Adual-stream%20hypothesis%20of%20human%20vision%2C%20which%20distinguishes%20the%20%22what%22%20and%0A%22where%22%20pathways%2C%20we%20deconstruct%20the%20visual%20processing%20in%20VLMs%20into%20object%0Arecognition%20and%20spatial%20perception%20for%20separate%20study.%20For%20object%20recognition%2C%0Awe%20convert%20images%20into%20text%20token%20maps%20and%20find%20that%20the%20model%27s%20perception%20of%0Aimage%20content%20unfolds%20as%20a%20two-stage%20process%20from%20shallow%20to%20deep%20layers%2C%0Abeginning%20with%20attribute%20recognition%20and%20culminating%20in%20semantic%0Adisambiguation.%20For%20spatial%20perception%2C%20we%20theoretically%20derive%20and%20empirically%0Averify%20the%20geometric%20structure%20underlying%20the%20positional%20representation%20in%0AVLMs.%20Based%20on%20these%20findings%2C%20we%20introduce%20an%20instruction-agnostic%20token%0Acompression%20algorithm%20based%20on%20a%20plug-and-play%20visual%20decoder%20to%20improve%0Adecoding%20efficiency%2C%20and%20a%20RoPE%20scaling%20technique%20to%20enhance%20spatial%20reasoning.%0AThrough%20rigorous%20experiments%2C%20our%20work%20validates%20these%20analyses%2C%20offering%20a%0Adeeper%20understanding%20of%20VLM%20internals%20and%20providing%20clear%20principles%20for%0Adesigning%20more%20capable%20future%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReading%2520Images%2520Like%2520Texts%253A%2520Sequential%2520Image%2520Understanding%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DYueyan%2520Li%2520and%2520Chenggong%2520Zhao%2520and%2520Zeyuan%2520Zang%2520and%2520Caixia%2520Yuan%2520and%2520Xiaojie%2520Wang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%250Aa%2520variety%2520of%2520real-world%2520tasks.%2520However%252C%2520existing%2520VLMs%2520typically%2520process%2520visual%250Ainformation%2520by%2520serializing%2520images%252C%2520a%2520method%2520that%2520diverges%2520significantly%2520from%250Athe%2520parallel%2520nature%2520of%2520human%2520vision.%2520Moreover%252C%2520their%2520opaque%2520internal%2520mechanisms%250Ahinder%2520both%2520deeper%2520understanding%2520and%2520architectural%2520innovation.%2520Inspired%2520by%2520the%250Adual-stream%2520hypothesis%2520of%2520human%2520vision%252C%2520which%2520distinguishes%2520the%2520%2522what%2522%2520and%250A%2522where%2522%2520pathways%252C%2520we%2520deconstruct%2520the%2520visual%2520processing%2520in%2520VLMs%2520into%2520object%250Arecognition%2520and%2520spatial%2520perception%2520for%2520separate%2520study.%2520For%2520object%2520recognition%252C%250Awe%2520convert%2520images%2520into%2520text%2520token%2520maps%2520and%2520find%2520that%2520the%2520model%2527s%2520perception%2520of%250Aimage%2520content%2520unfolds%2520as%2520a%2520two-stage%2520process%2520from%2520shallow%2520to%2520deep%2520layers%252C%250Abeginning%2520with%2520attribute%2520recognition%2520and%2520culminating%2520in%2520semantic%250Adisambiguation.%2520For%2520spatial%2520perception%252C%2520we%2520theoretically%2520derive%2520and%2520empirically%250Averify%2520the%2520geometric%2520structure%2520underlying%2520the%2520positional%2520representation%2520in%250AVLMs.%2520Based%2520on%2520these%2520findings%252C%2520we%2520introduce%2520an%2520instruction-agnostic%2520token%250Acompression%2520algorithm%2520based%2520on%2520a%2520plug-and-play%2520visual%2520decoder%2520to%2520improve%250Adecoding%2520efficiency%252C%2520and%2520a%2520RoPE%2520scaling%2520technique%2520to%2520enhance%2520spatial%2520reasoning.%250AThrough%2520rigorous%2520experiments%252C%2520our%2520work%2520validates%2520these%2520analyses%252C%2520offering%2520a%250Adeeper%2520understanding%2520of%2520VLM%2520internals%2520and%2520providing%2520clear%2520principles%2520for%250Adesigning%2520more%2520capable%2520future%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reading%20Images%20Like%20Texts%3A%20Sequential%20Image%20Understanding%20in%0A%20%20Vision-Language%20Models&entry.906535625=Yueyan%20Li%20and%20Chenggong%20Zhao%20and%20Zeyuan%20Zang%20and%20Caixia%20Yuan%20and%20Xiaojie%20Wang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Aa%20variety%20of%20real-world%20tasks.%20However%2C%20existing%20VLMs%20typically%20process%20visual%0Ainformation%20by%20serializing%20images%2C%20a%20method%20that%20diverges%20significantly%20from%0Athe%20parallel%20nature%20of%20human%20vision.%20Moreover%2C%20their%20opaque%20internal%20mechanisms%0Ahinder%20both%20deeper%20understanding%20and%20architectural%20innovation.%20Inspired%20by%20the%0Adual-stream%20hypothesis%20of%20human%20vision%2C%20which%20distinguishes%20the%20%22what%22%20and%0A%22where%22%20pathways%2C%20we%20deconstruct%20the%20visual%20processing%20in%20VLMs%20into%20object%0Arecognition%20and%20spatial%20perception%20for%20separate%20study.%20For%20object%20recognition%2C%0Awe%20convert%20images%20into%20text%20token%20maps%20and%20find%20that%20the%20model%27s%20perception%20of%0Aimage%20content%20unfolds%20as%20a%20two-stage%20process%20from%20shallow%20to%20deep%20layers%2C%0Abeginning%20with%20attribute%20recognition%20and%20culminating%20in%20semantic%0Adisambiguation.%20For%20spatial%20perception%2C%20we%20theoretically%20derive%20and%20empirically%0Averify%20the%20geometric%20structure%20underlying%20the%20positional%20representation%20in%0AVLMs.%20Based%20on%20these%20findings%2C%20we%20introduce%20an%20instruction-agnostic%20token%0Acompression%20algorithm%20based%20on%20a%20plug-and-play%20visual%20decoder%20to%20improve%0Adecoding%20efficiency%2C%20and%20a%20RoPE%20scaling%20technique%20to%20enhance%20spatial%20reasoning.%0AThrough%20rigorous%20experiments%2C%20our%20work%20validates%20these%20analyses%2C%20offering%20a%0Adeeper%20understanding%20of%20VLM%20internals%20and%20providing%20clear%20principles%20for%0Adesigning%20more%20capable%20future%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19191v1&entry.124074799=Read"},
{"title": "Without Paired Labeled Data: End-to-End Self-Supervised Learning for\n  Drone-view Geo-Localization", "author": "Zhongwei Chen and Zhao-Xu Yang and Hai-Jun Rong and Guoqi Li", "abstract": "  Drone-view Geo-Localization (DVGL) aims to achieve accurate localization of\ndrones by retrieving the most relevant GPS-tagged satellite images. However,\nmost existing methods heavily rely on strictly pre-paired drone-satellite\nimages for supervised learning. When the target region shifts, new paired\nsamples are typically required to adapt to the distribution changes. The high\ncost of annotation and the limited transferability of these methods\nsignificantly hinder the practical deployment of DVGL in open-world scenarios.\nTo address these limitations, we propose a novel end-to-end self-supervised\nlearning method with a shallow backbone network, called the dynamic\nmemory-driven and neighborhood information learning (DMNIL) method. It employs\na clustering algorithm to generate pseudo-labels and adopts a dual-path\ncontrastive learning framework to learn discriminative intra-view\nrepresentations. Furthermore, DMNIL incorporates two core modules, including\nthe dynamic hierarchical memory learning (DHML) module and the information\nconsistency evolution learning (ICEL) module. The DHML module combines\nshort-term and long-term memory to enhance intra-view feature consistency and\ndiscriminability. Meanwhile, the ICEL module utilizes a neighborhood-driven\ndynamic constraint mechanism to systematically capture implicit cross-view\nsemantic correlations, consequently improving cross-view feature alignment. To\nfurther stabilize and strengthen the self-supervised training process, a\npseudo-label enhancement strategy is introduced to enhance the quality of\npseudo supervision. Extensive experiments on three public benchmark datasets\ndemonstrate that the proposed method consistently outperforms existing\nself-supervised methods and even surpasses several state-of-the-art supervised\nmethods. Our code is available at https://github.com/ISChenawei/DMNIL.\n", "link": "http://arxiv.org/abs/2502.11381v4", "date": "2025-09-23", "relevancy": 3.1207, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6865}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.611}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Without%20Paired%20Labeled%20Data%3A%20End-to-End%20Self-Supervised%20Learning%20for%0A%20%20Drone-view%20Geo-Localization&body=Title%3A%20Without%20Paired%20Labeled%20Data%3A%20End-to-End%20Self-Supervised%20Learning%20for%0A%20%20Drone-view%20Geo-Localization%0AAuthor%3A%20Zhongwei%20Chen%20and%20Zhao-Xu%20Yang%20and%20Hai-Jun%20Rong%20and%20Guoqi%20Li%0AAbstract%3A%20%20%20Drone-view%20Geo-Localization%20%28DVGL%29%20aims%20to%20achieve%20accurate%20localization%20of%0Adrones%20by%20retrieving%20the%20most%20relevant%20GPS-tagged%20satellite%20images.%20However%2C%0Amost%20existing%20methods%20heavily%20rely%20on%20strictly%20pre-paired%20drone-satellite%0Aimages%20for%20supervised%20learning.%20When%20the%20target%20region%20shifts%2C%20new%20paired%0Asamples%20are%20typically%20required%20to%20adapt%20to%20the%20distribution%20changes.%20The%20high%0Acost%20of%20annotation%20and%20the%20limited%20transferability%20of%20these%20methods%0Asignificantly%20hinder%20the%20practical%20deployment%20of%20DVGL%20in%20open-world%20scenarios.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20end-to-end%20self-supervised%0Alearning%20method%20with%20a%20shallow%20backbone%20network%2C%20called%20the%20dynamic%0Amemory-driven%20and%20neighborhood%20information%20learning%20%28DMNIL%29%20method.%20It%20employs%0Aa%20clustering%20algorithm%20to%20generate%20pseudo-labels%20and%20adopts%20a%20dual-path%0Acontrastive%20learning%20framework%20to%20learn%20discriminative%20intra-view%0Arepresentations.%20Furthermore%2C%20DMNIL%20incorporates%20two%20core%20modules%2C%20including%0Athe%20dynamic%20hierarchical%20memory%20learning%20%28DHML%29%20module%20and%20the%20information%0Aconsistency%20evolution%20learning%20%28ICEL%29%20module.%20The%20DHML%20module%20combines%0Ashort-term%20and%20long-term%20memory%20to%20enhance%20intra-view%20feature%20consistency%20and%0Adiscriminability.%20Meanwhile%2C%20the%20ICEL%20module%20utilizes%20a%20neighborhood-driven%0Adynamic%20constraint%20mechanism%20to%20systematically%20capture%20implicit%20cross-view%0Asemantic%20correlations%2C%20consequently%20improving%20cross-view%20feature%20alignment.%20To%0Afurther%20stabilize%20and%20strengthen%20the%20self-supervised%20training%20process%2C%20a%0Apseudo-label%20enhancement%20strategy%20is%20introduced%20to%20enhance%20the%20quality%20of%0Apseudo%20supervision.%20Extensive%20experiments%20on%20three%20public%20benchmark%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20existing%0Aself-supervised%20methods%20and%20even%20surpasses%20several%20state-of-the-art%20supervised%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/ISChenawei/DMNIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11381v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWithout%2520Paired%2520Labeled%2520Data%253A%2520End-to-End%2520Self-Supervised%2520Learning%2520for%250A%2520%2520Drone-view%2520Geo-Localization%26entry.906535625%3DZhongwei%2520Chen%2520and%2520Zhao-Xu%2520Yang%2520and%2520Hai-Jun%2520Rong%2520and%2520Guoqi%2520Li%26entry.1292438233%3D%2520%2520Drone-view%2520Geo-Localization%2520%2528DVGL%2529%2520aims%2520to%2520achieve%2520accurate%2520localization%2520of%250Adrones%2520by%2520retrieving%2520the%2520most%2520relevant%2520GPS-tagged%2520satellite%2520images.%2520However%252C%250Amost%2520existing%2520methods%2520heavily%2520rely%2520on%2520strictly%2520pre-paired%2520drone-satellite%250Aimages%2520for%2520supervised%2520learning.%2520When%2520the%2520target%2520region%2520shifts%252C%2520new%2520paired%250Asamples%2520are%2520typically%2520required%2520to%2520adapt%2520to%2520the%2520distribution%2520changes.%2520The%2520high%250Acost%2520of%2520annotation%2520and%2520the%2520limited%2520transferability%2520of%2520these%2520methods%250Asignificantly%2520hinder%2520the%2520practical%2520deployment%2520of%2520DVGL%2520in%2520open-world%2520scenarios.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520end-to-end%2520self-supervised%250Alearning%2520method%2520with%2520a%2520shallow%2520backbone%2520network%252C%2520called%2520the%2520dynamic%250Amemory-driven%2520and%2520neighborhood%2520information%2520learning%2520%2528DMNIL%2529%2520method.%2520It%2520employs%250Aa%2520clustering%2520algorithm%2520to%2520generate%2520pseudo-labels%2520and%2520adopts%2520a%2520dual-path%250Acontrastive%2520learning%2520framework%2520to%2520learn%2520discriminative%2520intra-view%250Arepresentations.%2520Furthermore%252C%2520DMNIL%2520incorporates%2520two%2520core%2520modules%252C%2520including%250Athe%2520dynamic%2520hierarchical%2520memory%2520learning%2520%2528DHML%2529%2520module%2520and%2520the%2520information%250Aconsistency%2520evolution%2520learning%2520%2528ICEL%2529%2520module.%2520The%2520DHML%2520module%2520combines%250Ashort-term%2520and%2520long-term%2520memory%2520to%2520enhance%2520intra-view%2520feature%2520consistency%2520and%250Adiscriminability.%2520Meanwhile%252C%2520the%2520ICEL%2520module%2520utilizes%2520a%2520neighborhood-driven%250Adynamic%2520constraint%2520mechanism%2520to%2520systematically%2520capture%2520implicit%2520cross-view%250Asemantic%2520correlations%252C%2520consequently%2520improving%2520cross-view%2520feature%2520alignment.%2520To%250Afurther%2520stabilize%2520and%2520strengthen%2520the%2520self-supervised%2520training%2520process%252C%2520a%250Apseudo-label%2520enhancement%2520strategy%2520is%2520introduced%2520to%2520enhance%2520the%2520quality%2520of%250Apseudo%2520supervision.%2520Extensive%2520experiments%2520on%2520three%2520public%2520benchmark%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520consistently%2520outperforms%2520existing%250Aself-supervised%2520methods%2520and%2520even%2520surpasses%2520several%2520state-of-the-art%2520supervised%250Amethods.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ISChenawei/DMNIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11381v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Without%20Paired%20Labeled%20Data%3A%20End-to-End%20Self-Supervised%20Learning%20for%0A%20%20Drone-view%20Geo-Localization&entry.906535625=Zhongwei%20Chen%20and%20Zhao-Xu%20Yang%20and%20Hai-Jun%20Rong%20and%20Guoqi%20Li&entry.1292438233=%20%20Drone-view%20Geo-Localization%20%28DVGL%29%20aims%20to%20achieve%20accurate%20localization%20of%0Adrones%20by%20retrieving%20the%20most%20relevant%20GPS-tagged%20satellite%20images.%20However%2C%0Amost%20existing%20methods%20heavily%20rely%20on%20strictly%20pre-paired%20drone-satellite%0Aimages%20for%20supervised%20learning.%20When%20the%20target%20region%20shifts%2C%20new%20paired%0Asamples%20are%20typically%20required%20to%20adapt%20to%20the%20distribution%20changes.%20The%20high%0Acost%20of%20annotation%20and%20the%20limited%20transferability%20of%20these%20methods%0Asignificantly%20hinder%20the%20practical%20deployment%20of%20DVGL%20in%20open-world%20scenarios.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20end-to-end%20self-supervised%0Alearning%20method%20with%20a%20shallow%20backbone%20network%2C%20called%20the%20dynamic%0Amemory-driven%20and%20neighborhood%20information%20learning%20%28DMNIL%29%20method.%20It%20employs%0Aa%20clustering%20algorithm%20to%20generate%20pseudo-labels%20and%20adopts%20a%20dual-path%0Acontrastive%20learning%20framework%20to%20learn%20discriminative%20intra-view%0Arepresentations.%20Furthermore%2C%20DMNIL%20incorporates%20two%20core%20modules%2C%20including%0Athe%20dynamic%20hierarchical%20memory%20learning%20%28DHML%29%20module%20and%20the%20information%0Aconsistency%20evolution%20learning%20%28ICEL%29%20module.%20The%20DHML%20module%20combines%0Ashort-term%20and%20long-term%20memory%20to%20enhance%20intra-view%20feature%20consistency%20and%0Adiscriminability.%20Meanwhile%2C%20the%20ICEL%20module%20utilizes%20a%20neighborhood-driven%0Adynamic%20constraint%20mechanism%20to%20systematically%20capture%20implicit%20cross-view%0Asemantic%20correlations%2C%20consequently%20improving%20cross-view%20feature%20alignment.%20To%0Afurther%20stabilize%20and%20strengthen%20the%20self-supervised%20training%20process%2C%20a%0Apseudo-label%20enhancement%20strategy%20is%20introduced%20to%20enhance%20the%20quality%20of%0Apseudo%20supervision.%20Extensive%20experiments%20on%20three%20public%20benchmark%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20existing%0Aself-supervised%20methods%20and%20even%20surpasses%20several%20state-of-the-art%20supervised%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/ISChenawei/DMNIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11381v4&entry.124074799=Read"},
{"title": "Moving by Looking: Towards Vision-Driven Avatar Motion Generation", "author": "Markos Diomataris and Berat Mert Albaba and Giorgio Becherini and Partha Ghosh and Omid Taheri and Michael J. Black", "abstract": "  The way we perceive the world fundamentally shapes how we move, whether it is\nhow we navigate in a room or how we interact with other humans. Current human\nmotion generation methods, neglect this interdependency and use task-specific\n``perception'' that differs radically from that of humans. We argue that the\ngeneration of human-like avatar behavior requires human-like perception.\nConsequently, in this work we present CLOPS, the first human avatar that solely\nuses egocentric vision to perceive its surroundings and navigate. Using vision\nas the primary driver of motion however, gives rise to a significant challenge\nfor training avatars: existing datasets have either isolated human motion,\nwithout the context of a scene, or lack scale. We overcome this challenge by\ndecoupling the learning of low-level motion skills from learning of high-level\ncontrol that maps visual input to motion. First, we train a motion prior model\non a large motion capture dataset. Then, a policy is trained using Q-learning\nto map egocentric visual inputs to high-level control commands for the motion\nprior. Our experiments empirically demonstrate that egocentric vision can give\nrise to human-like motion characteristics in our avatars. For example, the\navatars walk such that they avoid obstacles present in their visual field.\nThese findings suggest that equipping avatars with human-like sensors,\nparticularly egocentric vision, holds promise for training avatars that behave\nlike humans.\n", "link": "http://arxiv.org/abs/2509.19259v1", "date": "2025-09-23", "relevancy": 3.1096, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6575}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6278}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moving%20by%20Looking%3A%20Towards%20Vision-Driven%20Avatar%20Motion%20Generation&body=Title%3A%20Moving%20by%20Looking%3A%20Towards%20Vision-Driven%20Avatar%20Motion%20Generation%0AAuthor%3A%20Markos%20Diomataris%20and%20Berat%20Mert%20Albaba%20and%20Giorgio%20Becherini%20and%20Partha%20Ghosh%20and%20Omid%20Taheri%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20The%20way%20we%20perceive%20the%20world%20fundamentally%20shapes%20how%20we%20move%2C%20whether%20it%20is%0Ahow%20we%20navigate%20in%20a%20room%20or%20how%20we%20interact%20with%20other%20humans.%20Current%20human%0Amotion%20generation%20methods%2C%20neglect%20this%20interdependency%20and%20use%20task-specific%0A%60%60perception%27%27%20that%20differs%20radically%20from%20that%20of%20humans.%20We%20argue%20that%20the%0Ageneration%20of%20human-like%20avatar%20behavior%20requires%20human-like%20perception.%0AConsequently%2C%20in%20this%20work%20we%20present%20CLOPS%2C%20the%20first%20human%20avatar%20that%20solely%0Auses%20egocentric%20vision%20to%20perceive%20its%20surroundings%20and%20navigate.%20Using%20vision%0Aas%20the%20primary%20driver%20of%20motion%20however%2C%20gives%20rise%20to%20a%20significant%20challenge%0Afor%20training%20avatars%3A%20existing%20datasets%20have%20either%20isolated%20human%20motion%2C%0Awithout%20the%20context%20of%20a%20scene%2C%20or%20lack%20scale.%20We%20overcome%20this%20challenge%20by%0Adecoupling%20the%20learning%20of%20low-level%20motion%20skills%20from%20learning%20of%20high-level%0Acontrol%20that%20maps%20visual%20input%20to%20motion.%20First%2C%20we%20train%20a%20motion%20prior%20model%0Aon%20a%20large%20motion%20capture%20dataset.%20Then%2C%20a%20policy%20is%20trained%20using%20Q-learning%0Ato%20map%20egocentric%20visual%20inputs%20to%20high-level%20control%20commands%20for%20the%20motion%0Aprior.%20Our%20experiments%20empirically%20demonstrate%20that%20egocentric%20vision%20can%20give%0Arise%20to%20human-like%20motion%20characteristics%20in%20our%20avatars.%20For%20example%2C%20the%0Aavatars%20walk%20such%20that%20they%20avoid%20obstacles%20present%20in%20their%20visual%20field.%0AThese%20findings%20suggest%20that%20equipping%20avatars%20with%20human-like%20sensors%2C%0Aparticularly%20egocentric%20vision%2C%20holds%20promise%20for%20training%20avatars%20that%20behave%0Alike%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoving%2520by%2520Looking%253A%2520Towards%2520Vision-Driven%2520Avatar%2520Motion%2520Generation%26entry.906535625%3DMarkos%2520Diomataris%2520and%2520Berat%2520Mert%2520Albaba%2520and%2520Giorgio%2520Becherini%2520and%2520Partha%2520Ghosh%2520and%2520Omid%2520Taheri%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520The%2520way%2520we%2520perceive%2520the%2520world%2520fundamentally%2520shapes%2520how%2520we%2520move%252C%2520whether%2520it%2520is%250Ahow%2520we%2520navigate%2520in%2520a%2520room%2520or%2520how%2520we%2520interact%2520with%2520other%2520humans.%2520Current%2520human%250Amotion%2520generation%2520methods%252C%2520neglect%2520this%2520interdependency%2520and%2520use%2520task-specific%250A%2560%2560perception%2527%2527%2520that%2520differs%2520radically%2520from%2520that%2520of%2520humans.%2520We%2520argue%2520that%2520the%250Ageneration%2520of%2520human-like%2520avatar%2520behavior%2520requires%2520human-like%2520perception.%250AConsequently%252C%2520in%2520this%2520work%2520we%2520present%2520CLOPS%252C%2520the%2520first%2520human%2520avatar%2520that%2520solely%250Auses%2520egocentric%2520vision%2520to%2520perceive%2520its%2520surroundings%2520and%2520navigate.%2520Using%2520vision%250Aas%2520the%2520primary%2520driver%2520of%2520motion%2520however%252C%2520gives%2520rise%2520to%2520a%2520significant%2520challenge%250Afor%2520training%2520avatars%253A%2520existing%2520datasets%2520have%2520either%2520isolated%2520human%2520motion%252C%250Awithout%2520the%2520context%2520of%2520a%2520scene%252C%2520or%2520lack%2520scale.%2520We%2520overcome%2520this%2520challenge%2520by%250Adecoupling%2520the%2520learning%2520of%2520low-level%2520motion%2520skills%2520from%2520learning%2520of%2520high-level%250Acontrol%2520that%2520maps%2520visual%2520input%2520to%2520motion.%2520First%252C%2520we%2520train%2520a%2520motion%2520prior%2520model%250Aon%2520a%2520large%2520motion%2520capture%2520dataset.%2520Then%252C%2520a%2520policy%2520is%2520trained%2520using%2520Q-learning%250Ato%2520map%2520egocentric%2520visual%2520inputs%2520to%2520high-level%2520control%2520commands%2520for%2520the%2520motion%250Aprior.%2520Our%2520experiments%2520empirically%2520demonstrate%2520that%2520egocentric%2520vision%2520can%2520give%250Arise%2520to%2520human-like%2520motion%2520characteristics%2520in%2520our%2520avatars.%2520For%2520example%252C%2520the%250Aavatars%2520walk%2520such%2520that%2520they%2520avoid%2520obstacles%2520present%2520in%2520their%2520visual%2520field.%250AThese%2520findings%2520suggest%2520that%2520equipping%2520avatars%2520with%2520human-like%2520sensors%252C%250Aparticularly%2520egocentric%2520vision%252C%2520holds%2520promise%2520for%2520training%2520avatars%2520that%2520behave%250Alike%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moving%20by%20Looking%3A%20Towards%20Vision-Driven%20Avatar%20Motion%20Generation&entry.906535625=Markos%20Diomataris%20and%20Berat%20Mert%20Albaba%20and%20Giorgio%20Becherini%20and%20Partha%20Ghosh%20and%20Omid%20Taheri%20and%20Michael%20J.%20Black&entry.1292438233=%20%20The%20way%20we%20perceive%20the%20world%20fundamentally%20shapes%20how%20we%20move%2C%20whether%20it%20is%0Ahow%20we%20navigate%20in%20a%20room%20or%20how%20we%20interact%20with%20other%20humans.%20Current%20human%0Amotion%20generation%20methods%2C%20neglect%20this%20interdependency%20and%20use%20task-specific%0A%60%60perception%27%27%20that%20differs%20radically%20from%20that%20of%20humans.%20We%20argue%20that%20the%0Ageneration%20of%20human-like%20avatar%20behavior%20requires%20human-like%20perception.%0AConsequently%2C%20in%20this%20work%20we%20present%20CLOPS%2C%20the%20first%20human%20avatar%20that%20solely%0Auses%20egocentric%20vision%20to%20perceive%20its%20surroundings%20and%20navigate.%20Using%20vision%0Aas%20the%20primary%20driver%20of%20motion%20however%2C%20gives%20rise%20to%20a%20significant%20challenge%0Afor%20training%20avatars%3A%20existing%20datasets%20have%20either%20isolated%20human%20motion%2C%0Awithout%20the%20context%20of%20a%20scene%2C%20or%20lack%20scale.%20We%20overcome%20this%20challenge%20by%0Adecoupling%20the%20learning%20of%20low-level%20motion%20skills%20from%20learning%20of%20high-level%0Acontrol%20that%20maps%20visual%20input%20to%20motion.%20First%2C%20we%20train%20a%20motion%20prior%20model%0Aon%20a%20large%20motion%20capture%20dataset.%20Then%2C%20a%20policy%20is%20trained%20using%20Q-learning%0Ato%20map%20egocentric%20visual%20inputs%20to%20high-level%20control%20commands%20for%20the%20motion%0Aprior.%20Our%20experiments%20empirically%20demonstrate%20that%20egocentric%20vision%20can%20give%0Arise%20to%20human-like%20motion%20characteristics%20in%20our%20avatars.%20For%20example%2C%20the%0Aavatars%20walk%20such%20that%20they%20avoid%20obstacles%20present%20in%20their%20visual%20field.%0AThese%20findings%20suggest%20that%20equipping%20avatars%20with%20human-like%20sensors%2C%0Aparticularly%20egocentric%20vision%2C%20holds%20promise%20for%20training%20avatars%20that%20behave%0Alike%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19259v1&entry.124074799=Read"},
{"title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion\n  Probabilistic Models", "author": "Amirhesam Aghanouri and Cristina Olaverri-Monreal", "abstract": "  Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.\n", "link": "http://arxiv.org/abs/2509.18917v1", "date": "2025-09-23", "relevancy": 3.0476, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6102}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6102}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR%20Point%20Cloud%20Image-based%20Generation%20Using%20Denoising%20Diffusion%0A%20%20Probabilistic%20Models&body=Title%3A%20LiDAR%20Point%20Cloud%20Image-based%20Generation%20Using%20Denoising%20Diffusion%0A%20%20Probabilistic%20Models%0AAuthor%3A%20Amirhesam%20Aghanouri%20and%20Cristina%20Olaverri-Monreal%0AAbstract%3A%20%20%20Autonomous%20vehicles%20%28AVs%29%20are%20expected%20to%20revolutionize%20transportation%20by%0Aimproving%20efficiency%20and%20safety.%20Their%20success%20relies%20on%203D%20vision%20systems%20that%0Aeffectively%20sense%20the%20environment%20and%20detect%20traffic%20agents.%20Among%20sensors%20AVs%0Ause%20to%20create%20a%20comprehensive%20view%20of%20surroundings%2C%20LiDAR%20provides%0Ahigh-resolution%20depth%20data%20enabling%20accurate%20object%20detection%2C%20safe%20navigation%2C%0Aand%20collision%20avoidance.%20However%2C%20collecting%20real-world%20LiDAR%20data%20is%0Atime-consuming%20and%20often%20affected%20by%20noise%20and%20sparsity%20due%20to%20adverse%20weather%0Aor%20sensor%20limitations.%20This%20work%20applies%20a%20denoising%20diffusion%20probabilistic%0Amodel%20%28DDPM%29%2C%20enhanced%20with%20novel%20noise%20scheduling%20and%20time-step%20embedding%0Atechniques%20to%20generate%20high-quality%20synthetic%20data%20for%20augmentation%2C%20thereby%0Aimproving%20performance%20across%20a%20range%20of%20computer%20vision%20tasks%2C%20particularly%20in%0AAV%20perception.%20These%20modifications%20impact%20the%20denoising%20process%20and%20the%20model%27s%0Atemporal%20awareness%2C%20allowing%20it%20to%20produce%20more%20realistic%20point%20clouds%20based%20on%0Athe%20projection.%20The%20proposed%20method%20was%20extensively%20evaluated%20under%20various%0Aconfigurations%20using%20the%20IAMCV%20and%20KITTI-360%20datasets%2C%20with%20four%20performance%0Ametrics%20compared%20against%20state-of-the-art%20%28SOTA%29%20methods.%20The%20results%0Ademonstrate%20the%20model%27s%20superior%20performance%20over%20most%20existing%20baselines%20and%0Aits%20effectiveness%20in%20mitigating%20the%20effects%20of%20noisy%20and%20sparse%20LiDAR%20data%2C%0Aproducing%20diverse%20point%20clouds%20with%20rich%20spatial%20relationships%20and%20structural%0Adetail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR%2520Point%2520Cloud%2520Image-based%2520Generation%2520Using%2520Denoising%2520Diffusion%250A%2520%2520Probabilistic%2520Models%26entry.906535625%3DAmirhesam%2520Aghanouri%2520and%2520Cristina%2520Olaverri-Monreal%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520%2528AVs%2529%2520are%2520expected%2520to%2520revolutionize%2520transportation%2520by%250Aimproving%2520efficiency%2520and%2520safety.%2520Their%2520success%2520relies%2520on%25203D%2520vision%2520systems%2520that%250Aeffectively%2520sense%2520the%2520environment%2520and%2520detect%2520traffic%2520agents.%2520Among%2520sensors%2520AVs%250Ause%2520to%2520create%2520a%2520comprehensive%2520view%2520of%2520surroundings%252C%2520LiDAR%2520provides%250Ahigh-resolution%2520depth%2520data%2520enabling%2520accurate%2520object%2520detection%252C%2520safe%2520navigation%252C%250Aand%2520collision%2520avoidance.%2520However%252C%2520collecting%2520real-world%2520LiDAR%2520data%2520is%250Atime-consuming%2520and%2520often%2520affected%2520by%2520noise%2520and%2520sparsity%2520due%2520to%2520adverse%2520weather%250Aor%2520sensor%2520limitations.%2520This%2520work%2520applies%2520a%2520denoising%2520diffusion%2520probabilistic%250Amodel%2520%2528DDPM%2529%252C%2520enhanced%2520with%2520novel%2520noise%2520scheduling%2520and%2520time-step%2520embedding%250Atechniques%2520to%2520generate%2520high-quality%2520synthetic%2520data%2520for%2520augmentation%252C%2520thereby%250Aimproving%2520performance%2520across%2520a%2520range%2520of%2520computer%2520vision%2520tasks%252C%2520particularly%2520in%250AAV%2520perception.%2520These%2520modifications%2520impact%2520the%2520denoising%2520process%2520and%2520the%2520model%2527s%250Atemporal%2520awareness%252C%2520allowing%2520it%2520to%2520produce%2520more%2520realistic%2520point%2520clouds%2520based%2520on%250Athe%2520projection.%2520The%2520proposed%2520method%2520was%2520extensively%2520evaluated%2520under%2520various%250Aconfigurations%2520using%2520the%2520IAMCV%2520and%2520KITTI-360%2520datasets%252C%2520with%2520four%2520performance%250Ametrics%2520compared%2520against%2520state-of-the-art%2520%2528SOTA%2529%2520methods.%2520The%2520results%250Ademonstrate%2520the%2520model%2527s%2520superior%2520performance%2520over%2520most%2520existing%2520baselines%2520and%250Aits%2520effectiveness%2520in%2520mitigating%2520the%2520effects%2520of%2520noisy%2520and%2520sparse%2520LiDAR%2520data%252C%250Aproducing%2520diverse%2520point%2520clouds%2520with%2520rich%2520spatial%2520relationships%2520and%2520structural%250Adetail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR%20Point%20Cloud%20Image-based%20Generation%20Using%20Denoising%20Diffusion%0A%20%20Probabilistic%20Models&entry.906535625=Amirhesam%20Aghanouri%20and%20Cristina%20Olaverri-Monreal&entry.1292438233=%20%20Autonomous%20vehicles%20%28AVs%29%20are%20expected%20to%20revolutionize%20transportation%20by%0Aimproving%20efficiency%20and%20safety.%20Their%20success%20relies%20on%203D%20vision%20systems%20that%0Aeffectively%20sense%20the%20environment%20and%20detect%20traffic%20agents.%20Among%20sensors%20AVs%0Ause%20to%20create%20a%20comprehensive%20view%20of%20surroundings%2C%20LiDAR%20provides%0Ahigh-resolution%20depth%20data%20enabling%20accurate%20object%20detection%2C%20safe%20navigation%2C%0Aand%20collision%20avoidance.%20However%2C%20collecting%20real-world%20LiDAR%20data%20is%0Atime-consuming%20and%20often%20affected%20by%20noise%20and%20sparsity%20due%20to%20adverse%20weather%0Aor%20sensor%20limitations.%20This%20work%20applies%20a%20denoising%20diffusion%20probabilistic%0Amodel%20%28DDPM%29%2C%20enhanced%20with%20novel%20noise%20scheduling%20and%20time-step%20embedding%0Atechniques%20to%20generate%20high-quality%20synthetic%20data%20for%20augmentation%2C%20thereby%0Aimproving%20performance%20across%20a%20range%20of%20computer%20vision%20tasks%2C%20particularly%20in%0AAV%20perception.%20These%20modifications%20impact%20the%20denoising%20process%20and%20the%20model%27s%0Atemporal%20awareness%2C%20allowing%20it%20to%20produce%20more%20realistic%20point%20clouds%20based%20on%0Athe%20projection.%20The%20proposed%20method%20was%20extensively%20evaluated%20under%20various%0Aconfigurations%20using%20the%20IAMCV%20and%20KITTI-360%20datasets%2C%20with%20four%20performance%0Ametrics%20compared%20against%20state-of-the-art%20%28SOTA%29%20methods.%20The%20results%0Ademonstrate%20the%20model%27s%20superior%20performance%20over%20most%20existing%20baselines%20and%0Aits%20effectiveness%20in%20mitigating%20the%20effects%20of%20noisy%20and%20sparse%20LiDAR%20data%2C%0Aproducing%20diverse%20point%20clouds%20with%20rich%20spatial%20relationships%20and%20structural%0Adetail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18917v1&entry.124074799=Read"},
{"title": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval\n  via Latent Space Alignment", "author": "Teng Xiao and Zuchao Li and Lefei Zhang", "abstract": "  Recent advances in multimodal large language models (LLMs) have led to\nsignificant progress in understanding, generation, and retrieval tasks.\nHowever, current solutions often treat these tasks in isolation or require\ntraining LLMs from scratch, resulting in high computational costs and limited\ngeneralization across modalities. In this work, we present OmniBridge, a\nunified and modular multimodal framework that supports vision-language\nunderstanding, generation, and retrieval within a unified architecture.\nOmniBridge adopts a language-centric design that reuses pretrained LLMs and\nintroduces a lightweight bidirectional latent alignment module. To address the\nchallenge of task interference, we propose a two-stage decoupled training\nstrategy: supervised fine-tuning and latent space alignment for aligning LLM\nbehavior with multimodal reasoning, and semantic-guided diffusion training to\nalign cross-modal latent spaces via learnable query embeddings. Extensive\nexperiments across a wide range of benchmarks demonstrate that OmniBridge\nachieves competitive or state-of-the-art performance in all three tasks.\nMoreover, our results highlight the effectiveness of latent space alignment for\nunifying multimodal modeling under a shared representation space. Code and\nmodels are released at https://github.com/xiao-xt/OmniBridge.\n", "link": "http://arxiv.org/abs/2509.19018v1", "date": "2025-09-23", "relevancy": 3.0009, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6051}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniBridge%3A%20Unified%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Retrieval%0A%20%20via%20Latent%20Space%20Alignment&body=Title%3A%20OmniBridge%3A%20Unified%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Retrieval%0A%20%20via%20Latent%20Space%20Alignment%0AAuthor%3A%20Teng%20Xiao%20and%20Zuchao%20Li%20and%20Lefei%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28LLMs%29%20have%20led%20to%0Asignificant%20progress%20in%20understanding%2C%20generation%2C%20and%20retrieval%20tasks.%0AHowever%2C%20current%20solutions%20often%20treat%20these%20tasks%20in%20isolation%20or%20require%0Atraining%20LLMs%20from%20scratch%2C%20resulting%20in%20high%20computational%20costs%20and%20limited%0Ageneralization%20across%20modalities.%20In%20this%20work%2C%20we%20present%20OmniBridge%2C%20a%0Aunified%20and%20modular%20multimodal%20framework%20that%20supports%20vision-language%0Aunderstanding%2C%20generation%2C%20and%20retrieval%20within%20a%20unified%20architecture.%0AOmniBridge%20adopts%20a%20language-centric%20design%20that%20reuses%20pretrained%20LLMs%20and%0Aintroduces%20a%20lightweight%20bidirectional%20latent%20alignment%20module.%20To%20address%20the%0Achallenge%20of%20task%20interference%2C%20we%20propose%20a%20two-stage%20decoupled%20training%0Astrategy%3A%20supervised%20fine-tuning%20and%20latent%20space%20alignment%20for%20aligning%20LLM%0Abehavior%20with%20multimodal%20reasoning%2C%20and%20semantic-guided%20diffusion%20training%20to%0Aalign%20cross-modal%20latent%20spaces%20via%20learnable%20query%20embeddings.%20Extensive%0Aexperiments%20across%20a%20wide%20range%20of%20benchmarks%20demonstrate%20that%20OmniBridge%0Aachieves%20competitive%20or%20state-of-the-art%20performance%20in%20all%20three%20tasks.%0AMoreover%2C%20our%20results%20highlight%20the%20effectiveness%20of%20latent%20space%20alignment%20for%0Aunifying%20multimodal%20modeling%20under%20a%20shared%20representation%20space.%20Code%20and%0Amodels%20are%20released%20at%20https%3A//github.com/xiao-xt/OmniBridge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniBridge%253A%2520Unified%2520Multimodal%2520Understanding%252C%2520Generation%252C%2520and%2520Retrieval%250A%2520%2520via%2520Latent%2520Space%2520Alignment%26entry.906535625%3DTeng%2520Xiao%2520and%2520Zuchao%2520Li%2520and%2520Lefei%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520led%2520to%250Asignificant%2520progress%2520in%2520understanding%252C%2520generation%252C%2520and%2520retrieval%2520tasks.%250AHowever%252C%2520current%2520solutions%2520often%2520treat%2520these%2520tasks%2520in%2520isolation%2520or%2520require%250Atraining%2520LLMs%2520from%2520scratch%252C%2520resulting%2520in%2520high%2520computational%2520costs%2520and%2520limited%250Ageneralization%2520across%2520modalities.%2520In%2520this%2520work%252C%2520we%2520present%2520OmniBridge%252C%2520a%250Aunified%2520and%2520modular%2520multimodal%2520framework%2520that%2520supports%2520vision-language%250Aunderstanding%252C%2520generation%252C%2520and%2520retrieval%2520within%2520a%2520unified%2520architecture.%250AOmniBridge%2520adopts%2520a%2520language-centric%2520design%2520that%2520reuses%2520pretrained%2520LLMs%2520and%250Aintroduces%2520a%2520lightweight%2520bidirectional%2520latent%2520alignment%2520module.%2520To%2520address%2520the%250Achallenge%2520of%2520task%2520interference%252C%2520we%2520propose%2520a%2520two-stage%2520decoupled%2520training%250Astrategy%253A%2520supervised%2520fine-tuning%2520and%2520latent%2520space%2520alignment%2520for%2520aligning%2520LLM%250Abehavior%2520with%2520multimodal%2520reasoning%252C%2520and%2520semantic-guided%2520diffusion%2520training%2520to%250Aalign%2520cross-modal%2520latent%2520spaces%2520via%2520learnable%2520query%2520embeddings.%2520Extensive%250Aexperiments%2520across%2520a%2520wide%2520range%2520of%2520benchmarks%2520demonstrate%2520that%2520OmniBridge%250Aachieves%2520competitive%2520or%2520state-of-the-art%2520performance%2520in%2520all%2520three%2520tasks.%250AMoreover%252C%2520our%2520results%2520highlight%2520the%2520effectiveness%2520of%2520latent%2520space%2520alignment%2520for%250Aunifying%2520multimodal%2520modeling%2520under%2520a%2520shared%2520representation%2520space.%2520Code%2520and%250Amodels%2520are%2520released%2520at%2520https%253A//github.com/xiao-xt/OmniBridge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniBridge%3A%20Unified%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Retrieval%0A%20%20via%20Latent%20Space%20Alignment&entry.906535625=Teng%20Xiao%20and%20Zuchao%20Li%20and%20Lefei%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28LLMs%29%20have%20led%20to%0Asignificant%20progress%20in%20understanding%2C%20generation%2C%20and%20retrieval%20tasks.%0AHowever%2C%20current%20solutions%20often%20treat%20these%20tasks%20in%20isolation%20or%20require%0Atraining%20LLMs%20from%20scratch%2C%20resulting%20in%20high%20computational%20costs%20and%20limited%0Ageneralization%20across%20modalities.%20In%20this%20work%2C%20we%20present%20OmniBridge%2C%20a%0Aunified%20and%20modular%20multimodal%20framework%20that%20supports%20vision-language%0Aunderstanding%2C%20generation%2C%20and%20retrieval%20within%20a%20unified%20architecture.%0AOmniBridge%20adopts%20a%20language-centric%20design%20that%20reuses%20pretrained%20LLMs%20and%0Aintroduces%20a%20lightweight%20bidirectional%20latent%20alignment%20module.%20To%20address%20the%0Achallenge%20of%20task%20interference%2C%20we%20propose%20a%20two-stage%20decoupled%20training%0Astrategy%3A%20supervised%20fine-tuning%20and%20latent%20space%20alignment%20for%20aligning%20LLM%0Abehavior%20with%20multimodal%20reasoning%2C%20and%20semantic-guided%20diffusion%20training%20to%0Aalign%20cross-modal%20latent%20spaces%20via%20learnable%20query%20embeddings.%20Extensive%0Aexperiments%20across%20a%20wide%20range%20of%20benchmarks%20demonstrate%20that%20OmniBridge%0Aachieves%20competitive%20or%20state-of-the-art%20performance%20in%20all%20three%20tasks.%0AMoreover%2C%20our%20results%20highlight%20the%20effectiveness%20of%20latent%20space%20alignment%20for%0Aunifying%20multimodal%20modeling%20under%20a%20shared%20representation%20space.%20Code%20and%0Amodels%20are%20released%20at%20https%3A//github.com/xiao-xt/OmniBridge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19018v1&entry.124074799=Read"},
{"title": "Long Story Short: Disentangling Compositionality and Long-Caption\n  Understanding in VLMs", "author": "Israfel Salazar and Desmond Elliott and Yova Kementchedjhieva", "abstract": "  Contrastive vision-language models (VLMs) have made significant progress in\nbinding visual and textual information, but understanding long, dense captions\nremains an open challenge. We hypothesize that compositionality, the capacity\nto reason about object-attribute bindings and inter-object relationships, is\nkey to understanding longer captions. In this paper, we investigate the\ninteraction between compositionality and long-caption understanding, asking\nwhether training for one property enhances the other. We train and evaluate a\nrange of models that target each of these capabilities. Our results reveal a\nbidirectional relationship: compositional training improves performance on\nlong-caption retrieval, and training on long captions promotes\ncompositionality. However, these gains are sensitive to data quality and model\ndesign. We find that training on poorly structured captions, or with limited\nparameter updates, fails to support generalization. Likewise, strategies that\naim at retaining general alignment, such as freezing positional embeddings, do\nnot improve compositional understanding. Overall, we find that compositional\nunderstanding and long-caption understanding are intertwined capabilities that\ncan be jointly learned through training on dense, grounded descriptions.\nDespite these challenges, we show that models trained on high-quality,\nlong-caption data can achieve strong performance in both tasks, offering\npractical guidance for improving VLM generalization.\n", "link": "http://arxiv.org/abs/2509.19207v1", "date": "2025-09-23", "relevancy": 2.9899, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6175}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long%20Story%20Short%3A%20Disentangling%20Compositionality%20and%20Long-Caption%0A%20%20Understanding%20in%20VLMs&body=Title%3A%20Long%20Story%20Short%3A%20Disentangling%20Compositionality%20and%20Long-Caption%0A%20%20Understanding%20in%20VLMs%0AAuthor%3A%20Israfel%20Salazar%20and%20Desmond%20Elliott%20and%20Yova%20Kementchedjhieva%0AAbstract%3A%20%20%20Contrastive%20vision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%0Abinding%20visual%20and%20textual%20information%2C%20but%20understanding%20long%2C%20dense%20captions%0Aremains%20an%20open%20challenge.%20We%20hypothesize%20that%20compositionality%2C%20the%20capacity%0Ato%20reason%20about%20object-attribute%20bindings%20and%20inter-object%20relationships%2C%20is%0Akey%20to%20understanding%20longer%20captions.%20In%20this%20paper%2C%20we%20investigate%20the%0Ainteraction%20between%20compositionality%20and%20long-caption%20understanding%2C%20asking%0Awhether%20training%20for%20one%20property%20enhances%20the%20other.%20We%20train%20and%20evaluate%20a%0Arange%20of%20models%20that%20target%20each%20of%20these%20capabilities.%20Our%20results%20reveal%20a%0Abidirectional%20relationship%3A%20compositional%20training%20improves%20performance%20on%0Along-caption%20retrieval%2C%20and%20training%20on%20long%20captions%20promotes%0Acompositionality.%20However%2C%20these%20gains%20are%20sensitive%20to%20data%20quality%20and%20model%0Adesign.%20We%20find%20that%20training%20on%20poorly%20structured%20captions%2C%20or%20with%20limited%0Aparameter%20updates%2C%20fails%20to%20support%20generalization.%20Likewise%2C%20strategies%20that%0Aaim%20at%20retaining%20general%20alignment%2C%20such%20as%20freezing%20positional%20embeddings%2C%20do%0Anot%20improve%20compositional%20understanding.%20Overall%2C%20we%20find%20that%20compositional%0Aunderstanding%20and%20long-caption%20understanding%20are%20intertwined%20capabilities%20that%0Acan%20be%20jointly%20learned%20through%20training%20on%20dense%2C%20grounded%20descriptions.%0ADespite%20these%20challenges%2C%20we%20show%20that%20models%20trained%20on%20high-quality%2C%0Along-caption%20data%20can%20achieve%20strong%20performance%20in%20both%20tasks%2C%20offering%0Apractical%20guidance%20for%20improving%20VLM%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong%2520Story%2520Short%253A%2520Disentangling%2520Compositionality%2520and%2520Long-Caption%250A%2520%2520Understanding%2520in%2520VLMs%26entry.906535625%3DIsrafel%2520Salazar%2520and%2520Desmond%2520Elliott%2520and%2520Yova%2520Kementchedjhieva%26entry.1292438233%3D%2520%2520Contrastive%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520made%2520significant%2520progress%2520in%250Abinding%2520visual%2520and%2520textual%2520information%252C%2520but%2520understanding%2520long%252C%2520dense%2520captions%250Aremains%2520an%2520open%2520challenge.%2520We%2520hypothesize%2520that%2520compositionality%252C%2520the%2520capacity%250Ato%2520reason%2520about%2520object-attribute%2520bindings%2520and%2520inter-object%2520relationships%252C%2520is%250Akey%2520to%2520understanding%2520longer%2520captions.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Ainteraction%2520between%2520compositionality%2520and%2520long-caption%2520understanding%252C%2520asking%250Awhether%2520training%2520for%2520one%2520property%2520enhances%2520the%2520other.%2520We%2520train%2520and%2520evaluate%2520a%250Arange%2520of%2520models%2520that%2520target%2520each%2520of%2520these%2520capabilities.%2520Our%2520results%2520reveal%2520a%250Abidirectional%2520relationship%253A%2520compositional%2520training%2520improves%2520performance%2520on%250Along-caption%2520retrieval%252C%2520and%2520training%2520on%2520long%2520captions%2520promotes%250Acompositionality.%2520However%252C%2520these%2520gains%2520are%2520sensitive%2520to%2520data%2520quality%2520and%2520model%250Adesign.%2520We%2520find%2520that%2520training%2520on%2520poorly%2520structured%2520captions%252C%2520or%2520with%2520limited%250Aparameter%2520updates%252C%2520fails%2520to%2520support%2520generalization.%2520Likewise%252C%2520strategies%2520that%250Aaim%2520at%2520retaining%2520general%2520alignment%252C%2520such%2520as%2520freezing%2520positional%2520embeddings%252C%2520do%250Anot%2520improve%2520compositional%2520understanding.%2520Overall%252C%2520we%2520find%2520that%2520compositional%250Aunderstanding%2520and%2520long-caption%2520understanding%2520are%2520intertwined%2520capabilities%2520that%250Acan%2520be%2520jointly%2520learned%2520through%2520training%2520on%2520dense%252C%2520grounded%2520descriptions.%250ADespite%2520these%2520challenges%252C%2520we%2520show%2520that%2520models%2520trained%2520on%2520high-quality%252C%250Along-caption%2520data%2520can%2520achieve%2520strong%2520performance%2520in%2520both%2520tasks%252C%2520offering%250Apractical%2520guidance%2520for%2520improving%2520VLM%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long%20Story%20Short%3A%20Disentangling%20Compositionality%20and%20Long-Caption%0A%20%20Understanding%20in%20VLMs&entry.906535625=Israfel%20Salazar%20and%20Desmond%20Elliott%20and%20Yova%20Kementchedjhieva&entry.1292438233=%20%20Contrastive%20vision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%0Abinding%20visual%20and%20textual%20information%2C%20but%20understanding%20long%2C%20dense%20captions%0Aremains%20an%20open%20challenge.%20We%20hypothesize%20that%20compositionality%2C%20the%20capacity%0Ato%20reason%20about%20object-attribute%20bindings%20and%20inter-object%20relationships%2C%20is%0Akey%20to%20understanding%20longer%20captions.%20In%20this%20paper%2C%20we%20investigate%20the%0Ainteraction%20between%20compositionality%20and%20long-caption%20understanding%2C%20asking%0Awhether%20training%20for%20one%20property%20enhances%20the%20other.%20We%20train%20and%20evaluate%20a%0Arange%20of%20models%20that%20target%20each%20of%20these%20capabilities.%20Our%20results%20reveal%20a%0Abidirectional%20relationship%3A%20compositional%20training%20improves%20performance%20on%0Along-caption%20retrieval%2C%20and%20training%20on%20long%20captions%20promotes%0Acompositionality.%20However%2C%20these%20gains%20are%20sensitive%20to%20data%20quality%20and%20model%0Adesign.%20We%20find%20that%20training%20on%20poorly%20structured%20captions%2C%20or%20with%20limited%0Aparameter%20updates%2C%20fails%20to%20support%20generalization.%20Likewise%2C%20strategies%20that%0Aaim%20at%20retaining%20general%20alignment%2C%20such%20as%20freezing%20positional%20embeddings%2C%20do%0Anot%20improve%20compositional%20understanding.%20Overall%2C%20we%20find%20that%20compositional%0Aunderstanding%20and%20long-caption%20understanding%20are%20intertwined%20capabilities%20that%0Acan%20be%20jointly%20learned%20through%20training%20on%20dense%2C%20grounded%20descriptions.%0ADespite%20these%20challenges%2C%20we%20show%20that%20models%20trained%20on%20high-quality%2C%0Along-caption%20data%20can%20achieve%20strong%20performance%20in%20both%20tasks%2C%20offering%0Apractical%20guidance%20for%20improving%20VLM%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19207v1&entry.124074799=Read"},
{"title": "RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote\n  Sensing", "author": "Jiayu Wang and Ruizhi Wang and Jie Song and Haofei Zhang and Mingli Song and Zunlei Feng and Li Sun", "abstract": "  In this paper, we introduce a novel benchmark designed to propel the\nadvancement of general-purpose, large-scale 3D vision models for remote sensing\nimagery. While several datasets have been proposed within the realm of remote\nsensing, many existing collections either lack comprehensive depth information\nor fail to establish precise alignment between depth data and remote sensing\nimages. To address this deficiency, we present a visual Benchmark for 3D\nunderstanding of Remotely Sensed images, dubbed RS3DBench. This dataset\nencompasses 54,951 pairs of remote sensing images and pixel-level aligned depth\nmaps, accompanied by corresponding textual descriptions, spanning a broad array\nof geographical contexts. It serves as a tool for training and assessing 3D\nvisual perception models within remote sensing image spatial understanding\ntasks. Furthermore, we introduce a remotely sensed depth estimation model\nderived from stable diffusion, harnessing its multimodal fusion capabilities,\nthereby delivering state-of-the-art performance on our dataset. Our endeavor\nseeks to make a profound contribution to the evolution of 3D visual perception\nmodels and the advancement of geographic artificial intelligence within the\nremote sensing domain. The dataset, models and code will be accessed on the\nhttps://rs3dbench.github.io.\n", "link": "http://arxiv.org/abs/2509.18897v1", "date": "2025-09-23", "relevancy": 2.979, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6001}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RS3DBench%3A%20A%20Comprehensive%20Benchmark%20for%203D%20Spatial%20Perception%20in%20Remote%0A%20%20Sensing&body=Title%3A%20RS3DBench%3A%20A%20Comprehensive%20Benchmark%20for%203D%20Spatial%20Perception%20in%20Remote%0A%20%20Sensing%0AAuthor%3A%20Jiayu%20Wang%20and%20Ruizhi%20Wang%20and%20Jie%20Song%20and%20Haofei%20Zhang%20and%20Mingli%20Song%20and%20Zunlei%20Feng%20and%20Li%20Sun%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20benchmark%20designed%20to%20propel%20the%0Aadvancement%20of%20general-purpose%2C%20large-scale%203D%20vision%20models%20for%20remote%20sensing%0Aimagery.%20While%20several%20datasets%20have%20been%20proposed%20within%20the%20realm%20of%20remote%0Asensing%2C%20many%20existing%20collections%20either%20lack%20comprehensive%20depth%20information%0Aor%20fail%20to%20establish%20precise%20alignment%20between%20depth%20data%20and%20remote%20sensing%0Aimages.%20To%20address%20this%20deficiency%2C%20we%20present%20a%20visual%20Benchmark%20for%203D%0Aunderstanding%20of%20Remotely%20Sensed%20images%2C%20dubbed%20RS3DBench.%20This%20dataset%0Aencompasses%2054%2C951%20pairs%20of%20remote%20sensing%20images%20and%20pixel-level%20aligned%20depth%0Amaps%2C%20accompanied%20by%20corresponding%20textual%20descriptions%2C%20spanning%20a%20broad%20array%0Aof%20geographical%20contexts.%20It%20serves%20as%20a%20tool%20for%20training%20and%20assessing%203D%0Avisual%20perception%20models%20within%20remote%20sensing%20image%20spatial%20understanding%0Atasks.%20Furthermore%2C%20we%20introduce%20a%20remotely%20sensed%20depth%20estimation%20model%0Aderived%20from%20stable%20diffusion%2C%20harnessing%20its%20multimodal%20fusion%20capabilities%2C%0Athereby%20delivering%20state-of-the-art%20performance%20on%20our%20dataset.%20Our%20endeavor%0Aseeks%20to%20make%20a%20profound%20contribution%20to%20the%20evolution%20of%203D%20visual%20perception%0Amodels%20and%20the%20advancement%20of%20geographic%20artificial%20intelligence%20within%20the%0Aremote%20sensing%20domain.%20The%20dataset%2C%20models%20and%20code%20will%20be%20accessed%20on%20the%0Ahttps%3A//rs3dbench.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRS3DBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%25203D%2520Spatial%2520Perception%2520in%2520Remote%250A%2520%2520Sensing%26entry.906535625%3DJiayu%2520Wang%2520and%2520Ruizhi%2520Wang%2520and%2520Jie%2520Song%2520and%2520Haofei%2520Zhang%2520and%2520Mingli%2520Song%2520and%2520Zunlei%2520Feng%2520and%2520Li%2520Sun%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%2520designed%2520to%2520propel%2520the%250Aadvancement%2520of%2520general-purpose%252C%2520large-scale%25203D%2520vision%2520models%2520for%2520remote%2520sensing%250Aimagery.%2520While%2520several%2520datasets%2520have%2520been%2520proposed%2520within%2520the%2520realm%2520of%2520remote%250Asensing%252C%2520many%2520existing%2520collections%2520either%2520lack%2520comprehensive%2520depth%2520information%250Aor%2520fail%2520to%2520establish%2520precise%2520alignment%2520between%2520depth%2520data%2520and%2520remote%2520sensing%250Aimages.%2520To%2520address%2520this%2520deficiency%252C%2520we%2520present%2520a%2520visual%2520Benchmark%2520for%25203D%250Aunderstanding%2520of%2520Remotely%2520Sensed%2520images%252C%2520dubbed%2520RS3DBench.%2520This%2520dataset%250Aencompasses%252054%252C951%2520pairs%2520of%2520remote%2520sensing%2520images%2520and%2520pixel-level%2520aligned%2520depth%250Amaps%252C%2520accompanied%2520by%2520corresponding%2520textual%2520descriptions%252C%2520spanning%2520a%2520broad%2520array%250Aof%2520geographical%2520contexts.%2520It%2520serves%2520as%2520a%2520tool%2520for%2520training%2520and%2520assessing%25203D%250Avisual%2520perception%2520models%2520within%2520remote%2520sensing%2520image%2520spatial%2520understanding%250Atasks.%2520Furthermore%252C%2520we%2520introduce%2520a%2520remotely%2520sensed%2520depth%2520estimation%2520model%250Aderived%2520from%2520stable%2520diffusion%252C%2520harnessing%2520its%2520multimodal%2520fusion%2520capabilities%252C%250Athereby%2520delivering%2520state-of-the-art%2520performance%2520on%2520our%2520dataset.%2520Our%2520endeavor%250Aseeks%2520to%2520make%2520a%2520profound%2520contribution%2520to%2520the%2520evolution%2520of%25203D%2520visual%2520perception%250Amodels%2520and%2520the%2520advancement%2520of%2520geographic%2520artificial%2520intelligence%2520within%2520the%250Aremote%2520sensing%2520domain.%2520The%2520dataset%252C%2520models%2520and%2520code%2520will%2520be%2520accessed%2520on%2520the%250Ahttps%253A//rs3dbench.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RS3DBench%3A%20A%20Comprehensive%20Benchmark%20for%203D%20Spatial%20Perception%20in%20Remote%0A%20%20Sensing&entry.906535625=Jiayu%20Wang%20and%20Ruizhi%20Wang%20and%20Jie%20Song%20and%20Haofei%20Zhang%20and%20Mingli%20Song%20and%20Zunlei%20Feng%20and%20Li%20Sun&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20benchmark%20designed%20to%20propel%20the%0Aadvancement%20of%20general-purpose%2C%20large-scale%203D%20vision%20models%20for%20remote%20sensing%0Aimagery.%20While%20several%20datasets%20have%20been%20proposed%20within%20the%20realm%20of%20remote%0Asensing%2C%20many%20existing%20collections%20either%20lack%20comprehensive%20depth%20information%0Aor%20fail%20to%20establish%20precise%20alignment%20between%20depth%20data%20and%20remote%20sensing%0Aimages.%20To%20address%20this%20deficiency%2C%20we%20present%20a%20visual%20Benchmark%20for%203D%0Aunderstanding%20of%20Remotely%20Sensed%20images%2C%20dubbed%20RS3DBench.%20This%20dataset%0Aencompasses%2054%2C951%20pairs%20of%20remote%20sensing%20images%20and%20pixel-level%20aligned%20depth%0Amaps%2C%20accompanied%20by%20corresponding%20textual%20descriptions%2C%20spanning%20a%20broad%20array%0Aof%20geographical%20contexts.%20It%20serves%20as%20a%20tool%20for%20training%20and%20assessing%203D%0Avisual%20perception%20models%20within%20remote%20sensing%20image%20spatial%20understanding%0Atasks.%20Furthermore%2C%20we%20introduce%20a%20remotely%20sensed%20depth%20estimation%20model%0Aderived%20from%20stable%20diffusion%2C%20harnessing%20its%20multimodal%20fusion%20capabilities%2C%0Athereby%20delivering%20state-of-the-art%20performance%20on%20our%20dataset.%20Our%20endeavor%0Aseeks%20to%20make%20a%20profound%20contribution%20to%20the%20evolution%20of%203D%20visual%20perception%0Amodels%20and%20the%20advancement%20of%20geographic%20artificial%20intelligence%20within%20the%0Aremote%20sensing%20domain.%20The%20dataset%2C%20models%20and%20code%20will%20be%20accessed%20on%20the%0Ahttps%3A//rs3dbench.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18897v1&entry.124074799=Read"},
{"title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Synthetic\n  Video Detection", "author": "Zhipei Xu and Xuanyu Zhang and Qing Huang and Xing Zhou and Jian Zhang", "abstract": "  Recent advances in Artificial Intelligence Generated Content have led to\nhighly realistic synthetic videos, particularly in human-centric scenarios\ninvolving speech, gestures, and full-body motion, posing serious threats to\ninformation authenticity and public trust. Unlike DeepFake techniques that\nfocus on localized facial manipulation, human-centric video generation methods\ncan synthesize entire human bodies with controllable movements, enabling\ncomplex interactions with environments, objects, and even other people.\nHowever, existing detection methods largely overlook the growing risks posed by\nsuch full-body synthetic content. Meanwhile, a growing body of research has\nexplored leveraging LLMs for interpretable fake detection, aiming to explain\ndecisions in natural language. Yet these approaches heavily depend on\nsupervised fine-tuning, which introduces limitations such as annotation bias,\nhallucinated supervision, and weakened generalization. To address these\nchallenges, we propose AvatarShield, a novel multimodal human-centric synthetic\nvideo detection framework that eliminates the need for dense textual\nsupervision by adopting Group Relative Policy Optimization, enabling LLMs to\ndevelop reasoning capabilities from simple binary labels. Our architecture\ncombines a discrete vision tower for high-level semantic inconsistencies and a\nresidual extractor for fine-grained artifact analysis. We further introduce\nFakeHumanVid, a large-scale benchmark containing 15K real and synthetic videos\nacross nine state-of-the-art human generation methods driven by text, pose, or\naudio. Extensive experiments demonstrate that AvatarShield outperforms existing\nmethods in both in-domain and cross-domain settings.\n", "link": "http://arxiv.org/abs/2505.15173v3", "date": "2025-09-23", "relevancy": 2.9377, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.596}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5837}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AvatarShield%3A%20Visual%20Reinforcement%20Learning%20for%20Human-Centric%20Synthetic%0A%20%20Video%20Detection&body=Title%3A%20AvatarShield%3A%20Visual%20Reinforcement%20Learning%20for%20Human-Centric%20Synthetic%0A%20%20Video%20Detection%0AAuthor%3A%20Zhipei%20Xu%20and%20Xuanyu%20Zhang%20and%20Qing%20Huang%20and%20Xing%20Zhou%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20Artificial%20Intelligence%20Generated%20Content%20have%20led%20to%0Ahighly%20realistic%20synthetic%20videos%2C%20particularly%20in%20human-centric%20scenarios%0Ainvolving%20speech%2C%20gestures%2C%20and%20full-body%20motion%2C%20posing%20serious%20threats%20to%0Ainformation%20authenticity%20and%20public%20trust.%20Unlike%20DeepFake%20techniques%20that%0Afocus%20on%20localized%20facial%20manipulation%2C%20human-centric%20video%20generation%20methods%0Acan%20synthesize%20entire%20human%20bodies%20with%20controllable%20movements%2C%20enabling%0Acomplex%20interactions%20with%20environments%2C%20objects%2C%20and%20even%20other%20people.%0AHowever%2C%20existing%20detection%20methods%20largely%20overlook%20the%20growing%20risks%20posed%20by%0Asuch%20full-body%20synthetic%20content.%20Meanwhile%2C%20a%20growing%20body%20of%20research%20has%0Aexplored%20leveraging%20LLMs%20for%20interpretable%20fake%20detection%2C%20aiming%20to%20explain%0Adecisions%20in%20natural%20language.%20Yet%20these%20approaches%20heavily%20depend%20on%0Asupervised%20fine-tuning%2C%20which%20introduces%20limitations%20such%20as%20annotation%20bias%2C%0Ahallucinated%20supervision%2C%20and%20weakened%20generalization.%20To%20address%20these%0Achallenges%2C%20we%20propose%20AvatarShield%2C%20a%20novel%20multimodal%20human-centric%20synthetic%0Avideo%20detection%20framework%20that%20eliminates%20the%20need%20for%20dense%20textual%0Asupervision%20by%20adopting%20Group%20Relative%20Policy%20Optimization%2C%20enabling%20LLMs%20to%0Adevelop%20reasoning%20capabilities%20from%20simple%20binary%20labels.%20Our%20architecture%0Acombines%20a%20discrete%20vision%20tower%20for%20high-level%20semantic%20inconsistencies%20and%20a%0Aresidual%20extractor%20for%20fine-grained%20artifact%20analysis.%20We%20further%20introduce%0AFakeHumanVid%2C%20a%20large-scale%20benchmark%20containing%2015K%20real%20and%20synthetic%20videos%0Aacross%20nine%20state-of-the-art%20human%20generation%20methods%20driven%20by%20text%2C%20pose%2C%20or%0Aaudio.%20Extensive%20experiments%20demonstrate%20that%20AvatarShield%20outperforms%20existing%0Amethods%20in%20both%20in-domain%20and%20cross-domain%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15173v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvatarShield%253A%2520Visual%2520Reinforcement%2520Learning%2520for%2520Human-Centric%2520Synthetic%250A%2520%2520Video%2520Detection%26entry.906535625%3DZhipei%2520Xu%2520and%2520Xuanyu%2520Zhang%2520and%2520Qing%2520Huang%2520and%2520Xing%2520Zhou%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Artificial%2520Intelligence%2520Generated%2520Content%2520have%2520led%2520to%250Ahighly%2520realistic%2520synthetic%2520videos%252C%2520particularly%2520in%2520human-centric%2520scenarios%250Ainvolving%2520speech%252C%2520gestures%252C%2520and%2520full-body%2520motion%252C%2520posing%2520serious%2520threats%2520to%250Ainformation%2520authenticity%2520and%2520public%2520trust.%2520Unlike%2520DeepFake%2520techniques%2520that%250Afocus%2520on%2520localized%2520facial%2520manipulation%252C%2520human-centric%2520video%2520generation%2520methods%250Acan%2520synthesize%2520entire%2520human%2520bodies%2520with%2520controllable%2520movements%252C%2520enabling%250Acomplex%2520interactions%2520with%2520environments%252C%2520objects%252C%2520and%2520even%2520other%2520people.%250AHowever%252C%2520existing%2520detection%2520methods%2520largely%2520overlook%2520the%2520growing%2520risks%2520posed%2520by%250Asuch%2520full-body%2520synthetic%2520content.%2520Meanwhile%252C%2520a%2520growing%2520body%2520of%2520research%2520has%250Aexplored%2520leveraging%2520LLMs%2520for%2520interpretable%2520fake%2520detection%252C%2520aiming%2520to%2520explain%250Adecisions%2520in%2520natural%2520language.%2520Yet%2520these%2520approaches%2520heavily%2520depend%2520on%250Asupervised%2520fine-tuning%252C%2520which%2520introduces%2520limitations%2520such%2520as%2520annotation%2520bias%252C%250Ahallucinated%2520supervision%252C%2520and%2520weakened%2520generalization.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520AvatarShield%252C%2520a%2520novel%2520multimodal%2520human-centric%2520synthetic%250Avideo%2520detection%2520framework%2520that%2520eliminates%2520the%2520need%2520for%2520dense%2520textual%250Asupervision%2520by%2520adopting%2520Group%2520Relative%2520Policy%2520Optimization%252C%2520enabling%2520LLMs%2520to%250Adevelop%2520reasoning%2520capabilities%2520from%2520simple%2520binary%2520labels.%2520Our%2520architecture%250Acombines%2520a%2520discrete%2520vision%2520tower%2520for%2520high-level%2520semantic%2520inconsistencies%2520and%2520a%250Aresidual%2520extractor%2520for%2520fine-grained%2520artifact%2520analysis.%2520We%2520further%2520introduce%250AFakeHumanVid%252C%2520a%2520large-scale%2520benchmark%2520containing%252015K%2520real%2520and%2520synthetic%2520videos%250Aacross%2520nine%2520state-of-the-art%2520human%2520generation%2520methods%2520driven%2520by%2520text%252C%2520pose%252C%2520or%250Aaudio.%2520Extensive%2520experiments%2520demonstrate%2520that%2520AvatarShield%2520outperforms%2520existing%250Amethods%2520in%2520both%2520in-domain%2520and%2520cross-domain%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15173v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AvatarShield%3A%20Visual%20Reinforcement%20Learning%20for%20Human-Centric%20Synthetic%0A%20%20Video%20Detection&entry.906535625=Zhipei%20Xu%20and%20Xuanyu%20Zhang%20and%20Qing%20Huang%20and%20Xing%20Zhou%20and%20Jian%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20Artificial%20Intelligence%20Generated%20Content%20have%20led%20to%0Ahighly%20realistic%20synthetic%20videos%2C%20particularly%20in%20human-centric%20scenarios%0Ainvolving%20speech%2C%20gestures%2C%20and%20full-body%20motion%2C%20posing%20serious%20threats%20to%0Ainformation%20authenticity%20and%20public%20trust.%20Unlike%20DeepFake%20techniques%20that%0Afocus%20on%20localized%20facial%20manipulation%2C%20human-centric%20video%20generation%20methods%0Acan%20synthesize%20entire%20human%20bodies%20with%20controllable%20movements%2C%20enabling%0Acomplex%20interactions%20with%20environments%2C%20objects%2C%20and%20even%20other%20people.%0AHowever%2C%20existing%20detection%20methods%20largely%20overlook%20the%20growing%20risks%20posed%20by%0Asuch%20full-body%20synthetic%20content.%20Meanwhile%2C%20a%20growing%20body%20of%20research%20has%0Aexplored%20leveraging%20LLMs%20for%20interpretable%20fake%20detection%2C%20aiming%20to%20explain%0Adecisions%20in%20natural%20language.%20Yet%20these%20approaches%20heavily%20depend%20on%0Asupervised%20fine-tuning%2C%20which%20introduces%20limitations%20such%20as%20annotation%20bias%2C%0Ahallucinated%20supervision%2C%20and%20weakened%20generalization.%20To%20address%20these%0Achallenges%2C%20we%20propose%20AvatarShield%2C%20a%20novel%20multimodal%20human-centric%20synthetic%0Avideo%20detection%20framework%20that%20eliminates%20the%20need%20for%20dense%20textual%0Asupervision%20by%20adopting%20Group%20Relative%20Policy%20Optimization%2C%20enabling%20LLMs%20to%0Adevelop%20reasoning%20capabilities%20from%20simple%20binary%20labels.%20Our%20architecture%0Acombines%20a%20discrete%20vision%20tower%20for%20high-level%20semantic%20inconsistencies%20and%20a%0Aresidual%20extractor%20for%20fine-grained%20artifact%20analysis.%20We%20further%20introduce%0AFakeHumanVid%2C%20a%20large-scale%20benchmark%20containing%2015K%20real%20and%20synthetic%20videos%0Aacross%20nine%20state-of-the-art%20human%20generation%20methods%20driven%20by%20text%2C%20pose%2C%20or%0Aaudio.%20Extensive%20experiments%20demonstrate%20that%20AvatarShield%20outperforms%20existing%0Amethods%20in%20both%20in-domain%20and%20cross-domain%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15173v3&entry.124074799=Read"},
{"title": "Large Vision-Language Model Alignment and Misalignment: A Survey Through\n  the Lens of Explainability", "author": "Dong Shu and Haiyan Zhao and Jingyu Hu and Weiru Liu and Ali Payani and Lu Cheng and Mengnan Du", "abstract": "  Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in processing both visual and textual information. However, the\ncritical challenge of alignment between visual and textual representations is\nnot fully understood. This survey presents a comprehensive examination of\nalignment and misalignment in LVLMs through an explainability lens. We first\nexamine the fundamentals of alignment, exploring its representational and\nbehavioral aspects, training methodologies, and theoretical foundations. We\nthen analyze misalignment phenomena across three semantic levels: object,\nattribute, and relational misalignment. Our investigation reveals that\nmisalignment emerges from challenges at multiple levels: the data level, the\nmodel level, and the inference level. We provide a comprehensive review of\nexisting mitigation strategies, categorizing them into parameter-frozen and\nparameter-tuning approaches. Finally, we outline promising future research\ndirections, emphasizing the need for standardized evaluation protocols and\nin-depth explainability studies.\n", "link": "http://arxiv.org/abs/2501.01346v3", "date": "2025-09-23", "relevancy": 2.8841, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5988}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Vision-Language%20Model%20Alignment%20and%20Misalignment%3A%20A%20Survey%20Through%0A%20%20the%20Lens%20of%20Explainability&body=Title%3A%20Large%20Vision-Language%20Model%20Alignment%20and%20Misalignment%3A%20A%20Survey%20Through%0A%20%20the%20Lens%20of%20Explainability%0AAuthor%3A%20Dong%20Shu%20and%20Haiyan%20Zhao%20and%20Jingyu%20Hu%20and%20Weiru%20Liu%20and%20Ali%20Payani%20and%20Lu%20Cheng%20and%20Mengnan%20Du%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20processing%20both%20visual%20and%20textual%20information.%20However%2C%20the%0Acritical%20challenge%20of%20alignment%20between%20visual%20and%20textual%20representations%20is%0Anot%20fully%20understood.%20This%20survey%20presents%20a%20comprehensive%20examination%20of%0Aalignment%20and%20misalignment%20in%20LVLMs%20through%20an%20explainability%20lens.%20We%20first%0Aexamine%20the%20fundamentals%20of%20alignment%2C%20exploring%20its%20representational%20and%0Abehavioral%20aspects%2C%20training%20methodologies%2C%20and%20theoretical%20foundations.%20We%0Athen%20analyze%20misalignment%20phenomena%20across%20three%20semantic%20levels%3A%20object%2C%0Aattribute%2C%20and%20relational%20misalignment.%20Our%20investigation%20reveals%20that%0Amisalignment%20emerges%20from%20challenges%20at%20multiple%20levels%3A%20the%20data%20level%2C%20the%0Amodel%20level%2C%20and%20the%20inference%20level.%20We%20provide%20a%20comprehensive%20review%20of%0Aexisting%20mitigation%20strategies%2C%20categorizing%20them%20into%20parameter-frozen%20and%0Aparameter-tuning%20approaches.%20Finally%2C%20we%20outline%20promising%20future%20research%0Adirections%2C%20emphasizing%20the%20need%20for%20standardized%20evaluation%20protocols%20and%0Ain-depth%20explainability%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01346v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Vision-Language%2520Model%2520Alignment%2520and%2520Misalignment%253A%2520A%2520Survey%2520Through%250A%2520%2520the%2520Lens%2520of%2520Explainability%26entry.906535625%3DDong%2520Shu%2520and%2520Haiyan%2520Zhao%2520and%2520Jingyu%2520Hu%2520and%2520Weiru%2520Liu%2520and%2520Ali%2520Payani%2520and%2520Lu%2520Cheng%2520and%2520Mengnan%2520Du%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520processing%2520both%2520visual%2520and%2520textual%2520information.%2520However%252C%2520the%250Acritical%2520challenge%2520of%2520alignment%2520between%2520visual%2520and%2520textual%2520representations%2520is%250Anot%2520fully%2520understood.%2520This%2520survey%2520presents%2520a%2520comprehensive%2520examination%2520of%250Aalignment%2520and%2520misalignment%2520in%2520LVLMs%2520through%2520an%2520explainability%2520lens.%2520We%2520first%250Aexamine%2520the%2520fundamentals%2520of%2520alignment%252C%2520exploring%2520its%2520representational%2520and%250Abehavioral%2520aspects%252C%2520training%2520methodologies%252C%2520and%2520theoretical%2520foundations.%2520We%250Athen%2520analyze%2520misalignment%2520phenomena%2520across%2520three%2520semantic%2520levels%253A%2520object%252C%250Aattribute%252C%2520and%2520relational%2520misalignment.%2520Our%2520investigation%2520reveals%2520that%250Amisalignment%2520emerges%2520from%2520challenges%2520at%2520multiple%2520levels%253A%2520the%2520data%2520level%252C%2520the%250Amodel%2520level%252C%2520and%2520the%2520inference%2520level.%2520We%2520provide%2520a%2520comprehensive%2520review%2520of%250Aexisting%2520mitigation%2520strategies%252C%2520categorizing%2520them%2520into%2520parameter-frozen%2520and%250Aparameter-tuning%2520approaches.%2520Finally%252C%2520we%2520outline%2520promising%2520future%2520research%250Adirections%252C%2520emphasizing%2520the%2520need%2520for%2520standardized%2520evaluation%2520protocols%2520and%250Ain-depth%2520explainability%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01346v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Vision-Language%20Model%20Alignment%20and%20Misalignment%3A%20A%20Survey%20Through%0A%20%20the%20Lens%20of%20Explainability&entry.906535625=Dong%20Shu%20and%20Haiyan%20Zhao%20and%20Jingyu%20Hu%20and%20Weiru%20Liu%20and%20Ali%20Payani%20and%20Lu%20Cheng%20and%20Mengnan%20Du&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20processing%20both%20visual%20and%20textual%20information.%20However%2C%20the%0Acritical%20challenge%20of%20alignment%20between%20visual%20and%20textual%20representations%20is%0Anot%20fully%20understood.%20This%20survey%20presents%20a%20comprehensive%20examination%20of%0Aalignment%20and%20misalignment%20in%20LVLMs%20through%20an%20explainability%20lens.%20We%20first%0Aexamine%20the%20fundamentals%20of%20alignment%2C%20exploring%20its%20representational%20and%0Abehavioral%20aspects%2C%20training%20methodologies%2C%20and%20theoretical%20foundations.%20We%0Athen%20analyze%20misalignment%20phenomena%20across%20three%20semantic%20levels%3A%20object%2C%0Aattribute%2C%20and%20relational%20misalignment.%20Our%20investigation%20reveals%20that%0Amisalignment%20emerges%20from%20challenges%20at%20multiple%20levels%3A%20the%20data%20level%2C%20the%0Amodel%20level%2C%20and%20the%20inference%20level.%20We%20provide%20a%20comprehensive%20review%20of%0Aexisting%20mitigation%20strategies%2C%20categorizing%20them%20into%20parameter-frozen%20and%0Aparameter-tuning%20approaches.%20Finally%2C%20we%20outline%20promising%20future%20research%0Adirections%2C%20emphasizing%20the%20need%20for%20standardized%20evaluation%20protocols%20and%0Ain-depth%20explainability%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01346v3&entry.124074799=Read"},
{"title": "xAI-CV: An Overview of Explainable Artificial Intelligence in Computer\n  Vision", "author": "Nguyen Van Tu and Pham Nguyen Hai Long and Vo Hoai Viet", "abstract": "  Deep learning has become the de facto standard and dominant paradigm in image\nanalysis tasks, achieving state-of-the-art performance. However, this approach\noften results in \"black-box\" models, whose decision-making processes are\ndifficult to interpret, raising concerns about reliability in critical\napplications. To address this challenge and provide human a method to\nunderstand how AI model process and make decision, the field of xAI has\nemerged. This paper surveys four representative approaches in xAI for visual\nperception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),\n(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their\nunderlying mechanisms, strengths and limitations, as well as evaluation\nmetrics, thereby providing a comprehensive overview to guide future research\nand applications.\n", "link": "http://arxiv.org/abs/2509.18913v1", "date": "2025-09-23", "relevancy": 2.8815, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xAI-CV%3A%20An%20Overview%20of%20Explainable%20Artificial%20Intelligence%20in%20Computer%0A%20%20Vision&body=Title%3A%20xAI-CV%3A%20An%20Overview%20of%20Explainable%20Artificial%20Intelligence%20in%20Computer%0A%20%20Vision%0AAuthor%3A%20Nguyen%20Van%20Tu%20and%20Pham%20Nguyen%20Hai%20Long%20and%20Vo%20Hoai%20Viet%0AAbstract%3A%20%20%20Deep%20learning%20has%20become%20the%20de%20facto%20standard%20and%20dominant%20paradigm%20in%20image%0Aanalysis%20tasks%2C%20achieving%20state-of-the-art%20performance.%20However%2C%20this%20approach%0Aoften%20results%20in%20%22black-box%22%20models%2C%20whose%20decision-making%20processes%20are%0Adifficult%20to%20interpret%2C%20raising%20concerns%20about%20reliability%20in%20critical%0Aapplications.%20To%20address%20this%20challenge%20and%20provide%20human%20a%20method%20to%0Aunderstand%20how%20AI%20model%20process%20and%20make%20decision%2C%20the%20field%20of%20xAI%20has%0Aemerged.%20This%20paper%20surveys%20four%20representative%20approaches%20in%20xAI%20for%20visual%0Aperception%20tasks%3A%20%28i%29%20Saliency%20Maps%2C%20%28ii%29%20Concept%20Bottleneck%20Models%20%28CBM%29%2C%0A%28iii%29%20Prototype-based%20methods%2C%20and%20%28iv%29%20Hybrid%20approaches.%20We%20analyze%20their%0Aunderlying%20mechanisms%2C%20strengths%20and%20limitations%2C%20as%20well%20as%20evaluation%0Ametrics%2C%20thereby%20providing%20a%20comprehensive%20overview%20to%20guide%20future%20research%0Aand%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxAI-CV%253A%2520An%2520Overview%2520of%2520Explainable%2520Artificial%2520Intelligence%2520in%2520Computer%250A%2520%2520Vision%26entry.906535625%3DNguyen%2520Van%2520Tu%2520and%2520Pham%2520Nguyen%2520Hai%2520Long%2520and%2520Vo%2520Hoai%2520Viet%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520become%2520the%2520de%2520facto%2520standard%2520and%2520dominant%2520paradigm%2520in%2520image%250Aanalysis%2520tasks%252C%2520achieving%2520state-of-the-art%2520performance.%2520However%252C%2520this%2520approach%250Aoften%2520results%2520in%2520%2522black-box%2522%2520models%252C%2520whose%2520decision-making%2520processes%2520are%250Adifficult%2520to%2520interpret%252C%2520raising%2520concerns%2520about%2520reliability%2520in%2520critical%250Aapplications.%2520To%2520address%2520this%2520challenge%2520and%2520provide%2520human%2520a%2520method%2520to%250Aunderstand%2520how%2520AI%2520model%2520process%2520and%2520make%2520decision%252C%2520the%2520field%2520of%2520xAI%2520has%250Aemerged.%2520This%2520paper%2520surveys%2520four%2520representative%2520approaches%2520in%2520xAI%2520for%2520visual%250Aperception%2520tasks%253A%2520%2528i%2529%2520Saliency%2520Maps%252C%2520%2528ii%2529%2520Concept%2520Bottleneck%2520Models%2520%2528CBM%2529%252C%250A%2528iii%2529%2520Prototype-based%2520methods%252C%2520and%2520%2528iv%2529%2520Hybrid%2520approaches.%2520We%2520analyze%2520their%250Aunderlying%2520mechanisms%252C%2520strengths%2520and%2520limitations%252C%2520as%2520well%2520as%2520evaluation%250Ametrics%252C%2520thereby%2520providing%2520a%2520comprehensive%2520overview%2520to%2520guide%2520future%2520research%250Aand%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xAI-CV%3A%20An%20Overview%20of%20Explainable%20Artificial%20Intelligence%20in%20Computer%0A%20%20Vision&entry.906535625=Nguyen%20Van%20Tu%20and%20Pham%20Nguyen%20Hai%20Long%20and%20Vo%20Hoai%20Viet&entry.1292438233=%20%20Deep%20learning%20has%20become%20the%20de%20facto%20standard%20and%20dominant%20paradigm%20in%20image%0Aanalysis%20tasks%2C%20achieving%20state-of-the-art%20performance.%20However%2C%20this%20approach%0Aoften%20results%20in%20%22black-box%22%20models%2C%20whose%20decision-making%20processes%20are%0Adifficult%20to%20interpret%2C%20raising%20concerns%20about%20reliability%20in%20critical%0Aapplications.%20To%20address%20this%20challenge%20and%20provide%20human%20a%20method%20to%0Aunderstand%20how%20AI%20model%20process%20and%20make%20decision%2C%20the%20field%20of%20xAI%20has%0Aemerged.%20This%20paper%20surveys%20four%20representative%20approaches%20in%20xAI%20for%20visual%0Aperception%20tasks%3A%20%28i%29%20Saliency%20Maps%2C%20%28ii%29%20Concept%20Bottleneck%20Models%20%28CBM%29%2C%0A%28iii%29%20Prototype-based%20methods%2C%20and%20%28iv%29%20Hybrid%20approaches.%20We%20analyze%20their%0Aunderlying%20mechanisms%2C%20strengths%20and%20limitations%2C%20as%20well%20as%20evaluation%0Ametrics%2C%20thereby%20providing%20a%20comprehensive%20overview%20to%20guide%20future%20research%0Aand%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18913v1&entry.124074799=Read"},
{"title": "3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review", "author": "Salma Galaaoui and Eduardo Valle and David Picard and Nermin Samet", "abstract": "  In this paper, we present a comprehensive review of 3D human pose estimation\nand human mesh recovery from in-the-wild LiDAR point clouds. We compare\nexisting approaches across several key dimensions, and propose a structured\ntaxonomy to classify these methods. Following this taxonomy, we analyze each\nmethod's strengths, limitations, and design choices. In addition, (i) we\nperform a quantitative comparison of the three most widely used datasets,\ndetailing their characteristics; (ii) we compile unified definitions of all\nevaluation metrics; and (iii) we establish benchmark tables for both tasks on\nthese datasets to enable fair comparisons and promote progress in the field. We\nalso outline open challenges and research directions critical for advancing\nLiDAR-based 3D human understanding. Moreover, we maintain an accompanying\nwebpage that organizes papers according to our taxonomy and continuously update\nit with new studies:\nhttps://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR\n", "link": "http://arxiv.org/abs/2509.12197v2", "date": "2025-09-23", "relevancy": 2.8354, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5806}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Human%20Pose%20and%20Shape%20Estimation%20from%20LiDAR%20Point%20Clouds%3A%20A%20Review&body=Title%3A%203D%20Human%20Pose%20and%20Shape%20Estimation%20from%20LiDAR%20Point%20Clouds%3A%20A%20Review%0AAuthor%3A%20Salma%20Galaaoui%20and%20Eduardo%20Valle%20and%20David%20Picard%20and%20Nermin%20Samet%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20review%20of%203D%20human%20pose%20estimation%0Aand%20human%20mesh%20recovery%20from%20in-the-wild%20LiDAR%20point%20clouds.%20We%20compare%0Aexisting%20approaches%20across%20several%20key%20dimensions%2C%20and%20propose%20a%20structured%0Ataxonomy%20to%20classify%20these%20methods.%20Following%20this%20taxonomy%2C%20we%20analyze%20each%0Amethod%27s%20strengths%2C%20limitations%2C%20and%20design%20choices.%20In%20addition%2C%20%28i%29%20we%0Aperform%20a%20quantitative%20comparison%20of%20the%20three%20most%20widely%20used%20datasets%2C%0Adetailing%20their%20characteristics%3B%20%28ii%29%20we%20compile%20unified%20definitions%20of%20all%0Aevaluation%20metrics%3B%20and%20%28iii%29%20we%20establish%20benchmark%20tables%20for%20both%20tasks%20on%0Athese%20datasets%20to%20enable%20fair%20comparisons%20and%20promote%20progress%20in%20the%20field.%20We%0Aalso%20outline%20open%20challenges%20and%20research%20directions%20critical%20for%20advancing%0ALiDAR-based%203D%20human%20understanding.%20Moreover%2C%20we%20maintain%20an%20accompanying%0Awebpage%20that%20organizes%20papers%20according%20to%20our%20taxonomy%20and%20continuously%20update%0Ait%20with%20new%20studies%3A%0Ahttps%3A//github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Human%2520Pose%2520and%2520Shape%2520Estimation%2520from%2520LiDAR%2520Point%2520Clouds%253A%2520A%2520Review%26entry.906535625%3DSalma%2520Galaaoui%2520and%2520Eduardo%2520Valle%2520and%2520David%2520Picard%2520and%2520Nermin%2520Samet%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520review%2520of%25203D%2520human%2520pose%2520estimation%250Aand%2520human%2520mesh%2520recovery%2520from%2520in-the-wild%2520LiDAR%2520point%2520clouds.%2520We%2520compare%250Aexisting%2520approaches%2520across%2520several%2520key%2520dimensions%252C%2520and%2520propose%2520a%2520structured%250Ataxonomy%2520to%2520classify%2520these%2520methods.%2520Following%2520this%2520taxonomy%252C%2520we%2520analyze%2520each%250Amethod%2527s%2520strengths%252C%2520limitations%252C%2520and%2520design%2520choices.%2520In%2520addition%252C%2520%2528i%2529%2520we%250Aperform%2520a%2520quantitative%2520comparison%2520of%2520the%2520three%2520most%2520widely%2520used%2520datasets%252C%250Adetailing%2520their%2520characteristics%253B%2520%2528ii%2529%2520we%2520compile%2520unified%2520definitions%2520of%2520all%250Aevaluation%2520metrics%253B%2520and%2520%2528iii%2529%2520we%2520establish%2520benchmark%2520tables%2520for%2520both%2520tasks%2520on%250Athese%2520datasets%2520to%2520enable%2520fair%2520comparisons%2520and%2520promote%2520progress%2520in%2520the%2520field.%2520We%250Aalso%2520outline%2520open%2520challenges%2520and%2520research%2520directions%2520critical%2520for%2520advancing%250ALiDAR-based%25203D%2520human%2520understanding.%2520Moreover%252C%2520we%2520maintain%2520an%2520accompanying%250Awebpage%2520that%2520organizes%2520papers%2520according%2520to%2520our%2520taxonomy%2520and%2520continuously%2520update%250Ait%2520with%2520new%2520studies%253A%250Ahttps%253A//github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human%20Pose%20and%20Shape%20Estimation%20from%20LiDAR%20Point%20Clouds%3A%20A%20Review&entry.906535625=Salma%20Galaaoui%20and%20Eduardo%20Valle%20and%20David%20Picard%20and%20Nermin%20Samet&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20review%20of%203D%20human%20pose%20estimation%0Aand%20human%20mesh%20recovery%20from%20in-the-wild%20LiDAR%20point%20clouds.%20We%20compare%0Aexisting%20approaches%20across%20several%20key%20dimensions%2C%20and%20propose%20a%20structured%0Ataxonomy%20to%20classify%20these%20methods.%20Following%20this%20taxonomy%2C%20we%20analyze%20each%0Amethod%27s%20strengths%2C%20limitations%2C%20and%20design%20choices.%20In%20addition%2C%20%28i%29%20we%0Aperform%20a%20quantitative%20comparison%20of%20the%20three%20most%20widely%20used%20datasets%2C%0Adetailing%20their%20characteristics%3B%20%28ii%29%20we%20compile%20unified%20definitions%20of%20all%0Aevaluation%20metrics%3B%20and%20%28iii%29%20we%20establish%20benchmark%20tables%20for%20both%20tasks%20on%0Athese%20datasets%20to%20enable%20fair%20comparisons%20and%20promote%20progress%20in%20the%20field.%20We%0Aalso%20outline%20open%20challenges%20and%20research%20directions%20critical%20for%20advancing%0ALiDAR-based%203D%20human%20understanding.%20Moreover%2C%20we%20maintain%20an%20accompanying%0Awebpage%20that%20organizes%20papers%20according%20to%20our%20taxonomy%20and%20continuously%20update%0Ait%20with%20new%20studies%3A%0Ahttps%3A//github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12197v2&entry.124074799=Read"},
{"title": "DeepResonance: Enhancing Multimodal Music Understanding via\n  Music-centric Multi-way Instruction Tuning", "author": "Zhuoyuan Mao and Mengjie Zhao and Qiyu Wu and Hiromi Wakaki and Yuki Mitsufuji", "abstract": "  Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a pre-LLM\nfusion Transformer to enhance modality fusion prior to input into text LLMs,\ntailoring for multi-way instruction tuning. Our model achieves state-of-the-art\nperformances across six music understanding tasks, highlighting the benefits of\nthe auxiliary modalities and the structural superiority of DeepResonance. We\nopen-source the codes, models and datasets we constructed:\ngithub.com/sony/DeepResonance.\n", "link": "http://arxiv.org/abs/2502.12623v3", "date": "2025-09-23", "relevancy": 2.8311, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepResonance%3A%20Enhancing%20Multimodal%20Music%20Understanding%20via%0A%20%20Music-centric%20Multi-way%20Instruction%20Tuning&body=Title%3A%20DeepResonance%3A%20Enhancing%20Multimodal%20Music%20Understanding%20via%0A%20%20Music-centric%20Multi-way%20Instruction%20Tuning%0AAuthor%3A%20Zhuoyuan%20Mao%20and%20Mengjie%20Zhao%20and%20Qiyu%20Wu%20and%20Hiromi%20Wakaki%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20Recent%20advancements%20in%20music%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aimproved%20music%20understanding%20tasks%2C%20which%20involve%20the%20model%27s%20ability%20to%0Aanalyze%20and%20interpret%20various%20musical%20elements.%20These%20improvements%20primarily%0Afocused%20on%20integrating%20both%20music%20and%20text%20inputs.%20However%2C%20the%20potential%20of%0Aincorporating%20additional%20modalities%20such%20as%20images%2C%20videos%20and%20textual%20music%0Afeatures%20to%20enhance%20music%20understanding%20remains%20unexplored.%20To%20bridge%20this%20gap%2C%0Awe%20propose%20DeepResonance%2C%20a%20multimodal%20music%20understanding%20LLM%20fine-tuned%20via%0Amulti-way%20instruction%20tuning%20with%20multi-way%20aligned%20music%2C%20text%2C%20image%2C%20and%0Avideo%20data.%20To%20this%20end%2C%20we%20construct%20Music4way-MI2T%2C%20Music4way-MV2T%2C%20and%0AMusic4way-Any2T%2C%20three%204-way%20training%20and%20evaluation%20datasets%20designed%20to%0Aenable%20DeepResonance%20to%20integrate%20both%20visual%20and%20textual%20music%20feature%0Acontent.%20We%20also%20introduce%20multi-sampled%20ImageBind%20embeddings%20and%20a%20pre-LLM%0Afusion%20Transformer%20to%20enhance%20modality%20fusion%20prior%20to%20input%20into%20text%20LLMs%2C%0Atailoring%20for%20multi-way%20instruction%20tuning.%20Our%20model%20achieves%20state-of-the-art%0Aperformances%20across%20six%20music%20understanding%20tasks%2C%20highlighting%20the%20benefits%20of%0Athe%20auxiliary%20modalities%20and%20the%20structural%20superiority%20of%20DeepResonance.%20We%0Aopen-source%20the%20codes%2C%20models%20and%20datasets%20we%20constructed%3A%0Agithub.com/sony/DeepResonance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12623v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepResonance%253A%2520Enhancing%2520Multimodal%2520Music%2520Understanding%2520via%250A%2520%2520Music-centric%2520Multi-way%2520Instruction%2520Tuning%26entry.906535625%3DZhuoyuan%2520Mao%2520and%2520Mengjie%2520Zhao%2520and%2520Qiyu%2520Wu%2520and%2520Hiromi%2520Wakaki%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520music%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%250Aimproved%2520music%2520understanding%2520tasks%252C%2520which%2520involve%2520the%2520model%2527s%2520ability%2520to%250Aanalyze%2520and%2520interpret%2520various%2520musical%2520elements.%2520These%2520improvements%2520primarily%250Afocused%2520on%2520integrating%2520both%2520music%2520and%2520text%2520inputs.%2520However%252C%2520the%2520potential%2520of%250Aincorporating%2520additional%2520modalities%2520such%2520as%2520images%252C%2520videos%2520and%2520textual%2520music%250Afeatures%2520to%2520enhance%2520music%2520understanding%2520remains%2520unexplored.%2520To%2520bridge%2520this%2520gap%252C%250Awe%2520propose%2520DeepResonance%252C%2520a%2520multimodal%2520music%2520understanding%2520LLM%2520fine-tuned%2520via%250Amulti-way%2520instruction%2520tuning%2520with%2520multi-way%2520aligned%2520music%252C%2520text%252C%2520image%252C%2520and%250Avideo%2520data.%2520To%2520this%2520end%252C%2520we%2520construct%2520Music4way-MI2T%252C%2520Music4way-MV2T%252C%2520and%250AMusic4way-Any2T%252C%2520three%25204-way%2520training%2520and%2520evaluation%2520datasets%2520designed%2520to%250Aenable%2520DeepResonance%2520to%2520integrate%2520both%2520visual%2520and%2520textual%2520music%2520feature%250Acontent.%2520We%2520also%2520introduce%2520multi-sampled%2520ImageBind%2520embeddings%2520and%2520a%2520pre-LLM%250Afusion%2520Transformer%2520to%2520enhance%2520modality%2520fusion%2520prior%2520to%2520input%2520into%2520text%2520LLMs%252C%250Atailoring%2520for%2520multi-way%2520instruction%2520tuning.%2520Our%2520model%2520achieves%2520state-of-the-art%250Aperformances%2520across%2520six%2520music%2520understanding%2520tasks%252C%2520highlighting%2520the%2520benefits%2520of%250Athe%2520auxiliary%2520modalities%2520and%2520the%2520structural%2520superiority%2520of%2520DeepResonance.%2520We%250Aopen-source%2520the%2520codes%252C%2520models%2520and%2520datasets%2520we%2520constructed%253A%250Agithub.com/sony/DeepResonance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12623v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepResonance%3A%20Enhancing%20Multimodal%20Music%20Understanding%20via%0A%20%20Music-centric%20Multi-way%20Instruction%20Tuning&entry.906535625=Zhuoyuan%20Mao%20and%20Mengjie%20Zhao%20and%20Qiyu%20Wu%20and%20Hiromi%20Wakaki%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20Recent%20advancements%20in%20music%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aimproved%20music%20understanding%20tasks%2C%20which%20involve%20the%20model%27s%20ability%20to%0Aanalyze%20and%20interpret%20various%20musical%20elements.%20These%20improvements%20primarily%0Afocused%20on%20integrating%20both%20music%20and%20text%20inputs.%20However%2C%20the%20potential%20of%0Aincorporating%20additional%20modalities%20such%20as%20images%2C%20videos%20and%20textual%20music%0Afeatures%20to%20enhance%20music%20understanding%20remains%20unexplored.%20To%20bridge%20this%20gap%2C%0Awe%20propose%20DeepResonance%2C%20a%20multimodal%20music%20understanding%20LLM%20fine-tuned%20via%0Amulti-way%20instruction%20tuning%20with%20multi-way%20aligned%20music%2C%20text%2C%20image%2C%20and%0Avideo%20data.%20To%20this%20end%2C%20we%20construct%20Music4way-MI2T%2C%20Music4way-MV2T%2C%20and%0AMusic4way-Any2T%2C%20three%204-way%20training%20and%20evaluation%20datasets%20designed%20to%0Aenable%20DeepResonance%20to%20integrate%20both%20visual%20and%20textual%20music%20feature%0Acontent.%20We%20also%20introduce%20multi-sampled%20ImageBind%20embeddings%20and%20a%20pre-LLM%0Afusion%20Transformer%20to%20enhance%20modality%20fusion%20prior%20to%20input%20into%20text%20LLMs%2C%0Atailoring%20for%20multi-way%20instruction%20tuning.%20Our%20model%20achieves%20state-of-the-art%0Aperformances%20across%20six%20music%20understanding%20tasks%2C%20highlighting%20the%20benefits%20of%0Athe%20auxiliary%20modalities%20and%20the%20structural%20superiority%20of%20DeepResonance.%20We%0Aopen-source%20the%20codes%2C%20models%20and%20datasets%20we%20constructed%3A%0Agithub.com/sony/DeepResonance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12623v3&entry.124074799=Read"},
{"title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of\n  Large Language Models", "author": "Dong Shu and Xuansheng Wu and Haiyan Zhao and Daking Rai and Ziyu Yao and Ninghao Liu and Mengnan Du", "abstract": "  Large Language Models (LLMs) have transformed natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a promising method due to their ability to disentangle the complex,\nsuperimposed features within LLMs into more interpretable components. This\npaper presents a comprehensive survey of SAEs for interpreting and\nunderstanding the internal workings of LLMs. Our major contributions include:\n(1) exploring the technical framework of SAEs, covering basic architecture,\ndesign improvements, and effective training strategies; (2) examining different\napproaches to explaining SAE features, categorized into input-based and\noutput-based explanation methods; (3) discussing evaluation methods for\nassessing SAE performance, covering both structural and functional metrics; and\n(4) investigating real-world applications of SAEs in understanding and\nmanipulating LLM behaviors.\n", "link": "http://arxiv.org/abs/2503.05613v3", "date": "2025-09-23", "relevancy": 2.7853, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Sparse%20Autoencoders%3A%20Interpreting%20the%20Internal%20Mechanisms%20of%0A%20%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20on%20Sparse%20Autoencoders%3A%20Interpreting%20the%20Internal%20Mechanisms%20of%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Dong%20Shu%20and%20Xuansheng%20Wu%20and%20Haiyan%20Zhao%20and%20Daking%20Rai%20and%20Ziyu%20Yao%20and%20Ninghao%20Liu%20and%20Mengnan%20Du%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20transformed%20natural%20language%20processing%2C%0Ayet%20their%20internal%20mechanisms%20remain%20largely%20opaque.%20Recently%2C%20mechanistic%0Ainterpretability%20has%20attracted%20significant%20attention%20from%20the%20research%0Acommunity%20as%20a%20means%20to%20understand%20the%20inner%20workings%20of%20LLMs.%20Among%20various%0Amechanistic%20interpretability%20approaches%2C%20Sparse%20Autoencoders%20%28SAEs%29%20have%0Aemerged%20as%20a%20promising%20method%20due%20to%20their%20ability%20to%20disentangle%20the%20complex%2C%0Asuperimposed%20features%20within%20LLMs%20into%20more%20interpretable%20components.%20This%0Apaper%20presents%20a%20comprehensive%20survey%20of%20SAEs%20for%20interpreting%20and%0Aunderstanding%20the%20internal%20workings%20of%20LLMs.%20Our%20major%20contributions%20include%3A%0A%281%29%20exploring%20the%20technical%20framework%20of%20SAEs%2C%20covering%20basic%20architecture%2C%0Adesign%20improvements%2C%20and%20effective%20training%20strategies%3B%20%282%29%20examining%20different%0Aapproaches%20to%20explaining%20SAE%20features%2C%20categorized%20into%20input-based%20and%0Aoutput-based%20explanation%20methods%3B%20%283%29%20discussing%20evaluation%20methods%20for%0Aassessing%20SAE%20performance%2C%20covering%20both%20structural%20and%20functional%20metrics%3B%20and%0A%284%29%20investigating%20real-world%20applications%20of%20SAEs%20in%20understanding%20and%0Amanipulating%20LLM%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05613v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Sparse%2520Autoencoders%253A%2520Interpreting%2520the%2520Internal%2520Mechanisms%2520of%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DDong%2520Shu%2520and%2520Xuansheng%2520Wu%2520and%2520Haiyan%2520Zhao%2520and%2520Daking%2520Rai%2520and%2520Ziyu%2520Yao%2520and%2520Ninghao%2520Liu%2520and%2520Mengnan%2520Du%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520transformed%2520natural%2520language%2520processing%252C%250Ayet%2520their%2520internal%2520mechanisms%2520remain%2520largely%2520opaque.%2520Recently%252C%2520mechanistic%250Ainterpretability%2520has%2520attracted%2520significant%2520attention%2520from%2520the%2520research%250Acommunity%2520as%2520a%2520means%2520to%2520understand%2520the%2520inner%2520workings%2520of%2520LLMs.%2520Among%2520various%250Amechanistic%2520interpretability%2520approaches%252C%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520have%250Aemerged%2520as%2520a%2520promising%2520method%2520due%2520to%2520their%2520ability%2520to%2520disentangle%2520the%2520complex%252C%250Asuperimposed%2520features%2520within%2520LLMs%2520into%2520more%2520interpretable%2520components.%2520This%250Apaper%2520presents%2520a%2520comprehensive%2520survey%2520of%2520SAEs%2520for%2520interpreting%2520and%250Aunderstanding%2520the%2520internal%2520workings%2520of%2520LLMs.%2520Our%2520major%2520contributions%2520include%253A%250A%25281%2529%2520exploring%2520the%2520technical%2520framework%2520of%2520SAEs%252C%2520covering%2520basic%2520architecture%252C%250Adesign%2520improvements%252C%2520and%2520effective%2520training%2520strategies%253B%2520%25282%2529%2520examining%2520different%250Aapproaches%2520to%2520explaining%2520SAE%2520features%252C%2520categorized%2520into%2520input-based%2520and%250Aoutput-based%2520explanation%2520methods%253B%2520%25283%2529%2520discussing%2520evaluation%2520methods%2520for%250Aassessing%2520SAE%2520performance%252C%2520covering%2520both%2520structural%2520and%2520functional%2520metrics%253B%2520and%250A%25284%2529%2520investigating%2520real-world%2520applications%2520of%2520SAEs%2520in%2520understanding%2520and%250Amanipulating%2520LLM%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05613v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Sparse%20Autoencoders%3A%20Interpreting%20the%20Internal%20Mechanisms%20of%0A%20%20Large%20Language%20Models&entry.906535625=Dong%20Shu%20and%20Xuansheng%20Wu%20and%20Haiyan%20Zhao%20and%20Daking%20Rai%20and%20Ziyu%20Yao%20and%20Ninghao%20Liu%20and%20Mengnan%20Du&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20transformed%20natural%20language%20processing%2C%0Ayet%20their%20internal%20mechanisms%20remain%20largely%20opaque.%20Recently%2C%20mechanistic%0Ainterpretability%20has%20attracted%20significant%20attention%20from%20the%20research%0Acommunity%20as%20a%20means%20to%20understand%20the%20inner%20workings%20of%20LLMs.%20Among%20various%0Amechanistic%20interpretability%20approaches%2C%20Sparse%20Autoencoders%20%28SAEs%29%20have%0Aemerged%20as%20a%20promising%20method%20due%20to%20their%20ability%20to%20disentangle%20the%20complex%2C%0Asuperimposed%20features%20within%20LLMs%20into%20more%20interpretable%20components.%20This%0Apaper%20presents%20a%20comprehensive%20survey%20of%20SAEs%20for%20interpreting%20and%0Aunderstanding%20the%20internal%20workings%20of%20LLMs.%20Our%20major%20contributions%20include%3A%0A%281%29%20exploring%20the%20technical%20framework%20of%20SAEs%2C%20covering%20basic%20architecture%2C%0Adesign%20improvements%2C%20and%20effective%20training%20strategies%3B%20%282%29%20examining%20different%0Aapproaches%20to%20explaining%20SAE%20features%2C%20categorized%20into%20input-based%20and%0Aoutput-based%20explanation%20methods%3B%20%283%29%20discussing%20evaluation%20methods%20for%0Aassessing%20SAE%20performance%2C%20covering%20both%20structural%20and%20functional%20metrics%3B%20and%0A%284%29%20investigating%20real-world%20applications%20of%20SAEs%20in%20understanding%20and%0Amanipulating%20LLM%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05613v3&entry.124074799=Read"},
{"title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual\n  Segmentation", "author": "Yunzhe Shen and Kai Peng and Leiye Liu and Wei Ji and Jingjing Li and Miao Zhang and Yongri Piao and Huchuan Lu", "abstract": "  Audio-visual segmentation (AVS) plays a critical role in multimodal machine\nlearning by effectively integrating audio and visual cues to precisely segment\nobjects or regions within visual scenes. Recent AVS methods have demonstrated\nsignificant improvements. However, they overlook the inherent frequency-domain\ncontradictions between audio and visual modalities--the pervasively interfering\nnoise in audio high-frequency signals vs. the structurally rich details in\nvisual high-frequency signals. Ignoring these differences can result in\nsuboptimal performance. In this paper, we rethink the AVS task from a deeper\nperspective by reformulating AVS task as a frequency-domain decomposition and\nrecomposition problem. To this end, we introduce a novel Frequency-Aware\nAudio-Visual Segmentation (FAVS) framework consisting of two key modules:\nFrequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal\nConsistency (SCMC) module. FDED module employs a residual-based iterative\nfrequency decomposition to discriminate modality-specific semantics and\nstructural features, and SCMC module leverages a mixture-of-experts\narchitecture to reinforce semantic consistency and modality-specific feature\npreservation through dynamic expert routing. Extensive experiments demonstrate\nthat our FAVS framework achieves state-of-the-art performance on three\nbenchmark datasets, and abundant qualitative visualizations further verify the\neffectiveness of the proposed FDED and SCMC modules. The code will be released\nas open source upon acceptance of the paper.\n", "link": "http://arxiv.org/abs/2509.18912v1", "date": "2025-09-23", "relevancy": 2.7845, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Domain%20Decomposition%20and%20Recomposition%20for%20Robust%20Audio-Visual%0A%20%20Segmentation&body=Title%3A%20Frequency-Domain%20Decomposition%20and%20Recomposition%20for%20Robust%20Audio-Visual%0A%20%20Segmentation%0AAuthor%3A%20Yunzhe%20Shen%20and%20Kai%20Peng%20and%20Leiye%20Liu%20and%20Wei%20Ji%20and%20Jingjing%20Li%20and%20Miao%20Zhang%20and%20Yongri%20Piao%20and%20Huchuan%20Lu%0AAbstract%3A%20%20%20Audio-visual%20segmentation%20%28AVS%29%20plays%20a%20critical%20role%20in%20multimodal%20machine%0Alearning%20by%20effectively%20integrating%20audio%20and%20visual%20cues%20to%20precisely%20segment%0Aobjects%20or%20regions%20within%20visual%20scenes.%20Recent%20AVS%20methods%20have%20demonstrated%0Asignificant%20improvements.%20However%2C%20they%20overlook%20the%20inherent%20frequency-domain%0Acontradictions%20between%20audio%20and%20visual%20modalities--the%20pervasively%20interfering%0Anoise%20in%20audio%20high-frequency%20signals%20vs.%20the%20structurally%20rich%20details%20in%0Avisual%20high-frequency%20signals.%20Ignoring%20these%20differences%20can%20result%20in%0Asuboptimal%20performance.%20In%20this%20paper%2C%20we%20rethink%20the%20AVS%20task%20from%20a%20deeper%0Aperspective%20by%20reformulating%20AVS%20task%20as%20a%20frequency-domain%20decomposition%20and%0Arecomposition%20problem.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20Frequency-Aware%0AAudio-Visual%20Segmentation%20%28FAVS%29%20framework%20consisting%20of%20two%20key%20modules%3A%0AFrequency-Domain%20Enhanced%20Decomposer%20%28FDED%29%20module%20and%20Synergistic%20Cross-Modal%0AConsistency%20%28SCMC%29%20module.%20FDED%20module%20employs%20a%20residual-based%20iterative%0Afrequency%20decomposition%20to%20discriminate%20modality-specific%20semantics%20and%0Astructural%20features%2C%20and%20SCMC%20module%20leverages%20a%20mixture-of-experts%0Aarchitecture%20to%20reinforce%20semantic%20consistency%20and%20modality-specific%20feature%0Apreservation%20through%20dynamic%20expert%20routing.%20Extensive%20experiments%20demonstrate%0Athat%20our%20FAVS%20framework%20achieves%20state-of-the-art%20performance%20on%20three%0Abenchmark%20datasets%2C%20and%20abundant%20qualitative%20visualizations%20further%20verify%20the%0Aeffectiveness%20of%20the%20proposed%20FDED%20and%20SCMC%20modules.%20The%20code%20will%20be%20released%0Aas%20open%20source%20upon%20acceptance%20of%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Domain%2520Decomposition%2520and%2520Recomposition%2520for%2520Robust%2520Audio-Visual%250A%2520%2520Segmentation%26entry.906535625%3DYunzhe%2520Shen%2520and%2520Kai%2520Peng%2520and%2520Leiye%2520Liu%2520and%2520Wei%2520Ji%2520and%2520Jingjing%2520Li%2520and%2520Miao%2520Zhang%2520and%2520Yongri%2520Piao%2520and%2520Huchuan%2520Lu%26entry.1292438233%3D%2520%2520Audio-visual%2520segmentation%2520%2528AVS%2529%2520plays%2520a%2520critical%2520role%2520in%2520multimodal%2520machine%250Alearning%2520by%2520effectively%2520integrating%2520audio%2520and%2520visual%2520cues%2520to%2520precisely%2520segment%250Aobjects%2520or%2520regions%2520within%2520visual%2520scenes.%2520Recent%2520AVS%2520methods%2520have%2520demonstrated%250Asignificant%2520improvements.%2520However%252C%2520they%2520overlook%2520the%2520inherent%2520frequency-domain%250Acontradictions%2520between%2520audio%2520and%2520visual%2520modalities--the%2520pervasively%2520interfering%250Anoise%2520in%2520audio%2520high-frequency%2520signals%2520vs.%2520the%2520structurally%2520rich%2520details%2520in%250Avisual%2520high-frequency%2520signals.%2520Ignoring%2520these%2520differences%2520can%2520result%2520in%250Asuboptimal%2520performance.%2520In%2520this%2520paper%252C%2520we%2520rethink%2520the%2520AVS%2520task%2520from%2520a%2520deeper%250Aperspective%2520by%2520reformulating%2520AVS%2520task%2520as%2520a%2520frequency-domain%2520decomposition%2520and%250Arecomposition%2520problem.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520novel%2520Frequency-Aware%250AAudio-Visual%2520Segmentation%2520%2528FAVS%2529%2520framework%2520consisting%2520of%2520two%2520key%2520modules%253A%250AFrequency-Domain%2520Enhanced%2520Decomposer%2520%2528FDED%2529%2520module%2520and%2520Synergistic%2520Cross-Modal%250AConsistency%2520%2528SCMC%2529%2520module.%2520FDED%2520module%2520employs%2520a%2520residual-based%2520iterative%250Afrequency%2520decomposition%2520to%2520discriminate%2520modality-specific%2520semantics%2520and%250Astructural%2520features%252C%2520and%2520SCMC%2520module%2520leverages%2520a%2520mixture-of-experts%250Aarchitecture%2520to%2520reinforce%2520semantic%2520consistency%2520and%2520modality-specific%2520feature%250Apreservation%2520through%2520dynamic%2520expert%2520routing.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520FAVS%2520framework%2520achieves%2520state-of-the-art%2520performance%2520on%2520three%250Abenchmark%2520datasets%252C%2520and%2520abundant%2520qualitative%2520visualizations%2520further%2520verify%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520FDED%2520and%2520SCMC%2520modules.%2520The%2520code%2520will%2520be%2520released%250Aas%2520open%2520source%2520upon%2520acceptance%2520of%2520the%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Domain%20Decomposition%20and%20Recomposition%20for%20Robust%20Audio-Visual%0A%20%20Segmentation&entry.906535625=Yunzhe%20Shen%20and%20Kai%20Peng%20and%20Leiye%20Liu%20and%20Wei%20Ji%20and%20Jingjing%20Li%20and%20Miao%20Zhang%20and%20Yongri%20Piao%20and%20Huchuan%20Lu&entry.1292438233=%20%20Audio-visual%20segmentation%20%28AVS%29%20plays%20a%20critical%20role%20in%20multimodal%20machine%0Alearning%20by%20effectively%20integrating%20audio%20and%20visual%20cues%20to%20precisely%20segment%0Aobjects%20or%20regions%20within%20visual%20scenes.%20Recent%20AVS%20methods%20have%20demonstrated%0Asignificant%20improvements.%20However%2C%20they%20overlook%20the%20inherent%20frequency-domain%0Acontradictions%20between%20audio%20and%20visual%20modalities--the%20pervasively%20interfering%0Anoise%20in%20audio%20high-frequency%20signals%20vs.%20the%20structurally%20rich%20details%20in%0Avisual%20high-frequency%20signals.%20Ignoring%20these%20differences%20can%20result%20in%0Asuboptimal%20performance.%20In%20this%20paper%2C%20we%20rethink%20the%20AVS%20task%20from%20a%20deeper%0Aperspective%20by%20reformulating%20AVS%20task%20as%20a%20frequency-domain%20decomposition%20and%0Arecomposition%20problem.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20Frequency-Aware%0AAudio-Visual%20Segmentation%20%28FAVS%29%20framework%20consisting%20of%20two%20key%20modules%3A%0AFrequency-Domain%20Enhanced%20Decomposer%20%28FDED%29%20module%20and%20Synergistic%20Cross-Modal%0AConsistency%20%28SCMC%29%20module.%20FDED%20module%20employs%20a%20residual-based%20iterative%0Afrequency%20decomposition%20to%20discriminate%20modality-specific%20semantics%20and%0Astructural%20features%2C%20and%20SCMC%20module%20leverages%20a%20mixture-of-experts%0Aarchitecture%20to%20reinforce%20semantic%20consistency%20and%20modality-specific%20feature%0Apreservation%20through%20dynamic%20expert%20routing.%20Extensive%20experiments%20demonstrate%0Athat%20our%20FAVS%20framework%20achieves%20state-of-the-art%20performance%20on%20three%0Abenchmark%20datasets%2C%20and%20abundant%20qualitative%20visualizations%20further%20verify%20the%0Aeffectiveness%20of%20the%20proposed%20FDED%20and%20SCMC%20modules.%20The%20code%20will%20be%20released%0Aas%20open%20source%20upon%20acceptance%20of%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18912v1&entry.124074799=Read"},
{"title": "CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable\n  Remote Sensing Semantic Segmentation", "author": "Ziyang Gong and Zhixiang Wei and Di Wang and Xiaoxing Hu and Xianzheng Ma and Hongruixuan Chen and Yuru Jia and Yupeng Deng and Zhenming Ji and Xiangwei Zhu and Xue Yang and Naoto Yokoya and Jing Zhang and Bo Du and Junchi Yan and Liangpei Zhang", "abstract": "  The field of Remote Sensing Domain Generalization (RSDG) has emerged as a\ncritical and valuable research frontier, focusing on developing models that\ngeneralize effectively across diverse scenarios. Despite the substantial domain\ngaps in RS images that are characterized by variabilities such as location,\nwavelength, and sensor type, research in this area remains underexplored: (1)\nCurrent cross-domain methods primarily focus on Domain Adaptation (DA), which\nadapts models to predefined domains rather than to unseen ones; (2) Few studies\ntargeting the RSDG issue, especially for semantic segmentation tasks, where\nexisting models are developed for specific unknown domains, struggling with\nissues of underfitting on other unknown scenarios; (3) Existing RS foundation\nmodels tend to prioritize in-domain performance over cross-domain\ngeneralization. To this end, we introduce the first vision foundation model for\nRSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong\ncross-domain generalization through a specially designed data-level Earth-Style\nInjection pipeline and a model-level Multi-Task Training pipeline. In addition,\nfor the semantic segmentation task, we have curated an RSDG benchmark\ncomprising 32 cross-domain settings across various regions, spectral bands,\nplatforms, and climates, providing a comprehensive framework for testing the\ngeneralizability of future RSDG models. Extensive experiments on this benchmark\ndemonstrate the superiority of CrossEarth over existing state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2410.22629v3", "date": "2025-09-23", "relevancy": 2.7724, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrossEarth%3A%20Geospatial%20Vision%20Foundation%20Model%20for%20Domain%20Generalizable%0A%20%20Remote%20Sensing%20Semantic%20Segmentation&body=Title%3A%20CrossEarth%3A%20Geospatial%20Vision%20Foundation%20Model%20for%20Domain%20Generalizable%0A%20%20Remote%20Sensing%20Semantic%20Segmentation%0AAuthor%3A%20Ziyang%20Gong%20and%20Zhixiang%20Wei%20and%20Di%20Wang%20and%20Xiaoxing%20Hu%20and%20Xianzheng%20Ma%20and%20Hongruixuan%20Chen%20and%20Yuru%20Jia%20and%20Yupeng%20Deng%20and%20Zhenming%20Ji%20and%20Xiangwei%20Zhu%20and%20Xue%20Yang%20and%20Naoto%20Yokoya%20and%20Jing%20Zhang%20and%20Bo%20Du%20and%20Junchi%20Yan%20and%20Liangpei%20Zhang%0AAbstract%3A%20%20%20The%20field%20of%20Remote%20Sensing%20Domain%20Generalization%20%28RSDG%29%20has%20emerged%20as%20a%0Acritical%20and%20valuable%20research%20frontier%2C%20focusing%20on%20developing%20models%20that%0Ageneralize%20effectively%20across%20diverse%20scenarios.%20Despite%20the%20substantial%20domain%0Agaps%20in%20RS%20images%20that%20are%20characterized%20by%20variabilities%20such%20as%20location%2C%0Awavelength%2C%20and%20sensor%20type%2C%20research%20in%20this%20area%20remains%20underexplored%3A%20%281%29%0ACurrent%20cross-domain%20methods%20primarily%20focus%20on%20Domain%20Adaptation%20%28DA%29%2C%20which%0Aadapts%20models%20to%20predefined%20domains%20rather%20than%20to%20unseen%20ones%3B%20%282%29%20Few%20studies%0Atargeting%20the%20RSDG%20issue%2C%20especially%20for%20semantic%20segmentation%20tasks%2C%20where%0Aexisting%20models%20are%20developed%20for%20specific%20unknown%20domains%2C%20struggling%20with%0Aissues%20of%20underfitting%20on%20other%20unknown%20scenarios%3B%20%283%29%20Existing%20RS%20foundation%0Amodels%20tend%20to%20prioritize%20in-domain%20performance%20over%20cross-domain%0Ageneralization.%20To%20this%20end%2C%20we%20introduce%20the%20first%20vision%20foundation%20model%20for%0ARSDG%20semantic%20segmentation%2C%20CrossEarth.%20CrossEarth%20demonstrates%20strong%0Across-domain%20generalization%20through%20a%20specially%20designed%20data-level%20Earth-Style%0AInjection%20pipeline%20and%20a%20model-level%20Multi-Task%20Training%20pipeline.%20In%20addition%2C%0Afor%20the%20semantic%20segmentation%20task%2C%20we%20have%20curated%20an%20RSDG%20benchmark%0Acomprising%2032%20cross-domain%20settings%20across%20various%20regions%2C%20spectral%20bands%2C%0Aplatforms%2C%20and%20climates%2C%20providing%20a%20comprehensive%20framework%20for%20testing%20the%0Ageneralizability%20of%20future%20RSDG%20models.%20Extensive%20experiments%20on%20this%20benchmark%0Ademonstrate%20the%20superiority%20of%20CrossEarth%20over%20existing%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22629v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrossEarth%253A%2520Geospatial%2520Vision%2520Foundation%2520Model%2520for%2520Domain%2520Generalizable%250A%2520%2520Remote%2520Sensing%2520Semantic%2520Segmentation%26entry.906535625%3DZiyang%2520Gong%2520and%2520Zhixiang%2520Wei%2520and%2520Di%2520Wang%2520and%2520Xiaoxing%2520Hu%2520and%2520Xianzheng%2520Ma%2520and%2520Hongruixuan%2520Chen%2520and%2520Yuru%2520Jia%2520and%2520Yupeng%2520Deng%2520and%2520Zhenming%2520Ji%2520and%2520Xiangwei%2520Zhu%2520and%2520Xue%2520Yang%2520and%2520Naoto%2520Yokoya%2520and%2520Jing%2520Zhang%2520and%2520Bo%2520Du%2520and%2520Junchi%2520Yan%2520and%2520Liangpei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520Remote%2520Sensing%2520Domain%2520Generalization%2520%2528RSDG%2529%2520has%2520emerged%2520as%2520a%250Acritical%2520and%2520valuable%2520research%2520frontier%252C%2520focusing%2520on%2520developing%2520models%2520that%250Ageneralize%2520effectively%2520across%2520diverse%2520scenarios.%2520Despite%2520the%2520substantial%2520domain%250Agaps%2520in%2520RS%2520images%2520that%2520are%2520characterized%2520by%2520variabilities%2520such%2520as%2520location%252C%250Awavelength%252C%2520and%2520sensor%2520type%252C%2520research%2520in%2520this%2520area%2520remains%2520underexplored%253A%2520%25281%2529%250ACurrent%2520cross-domain%2520methods%2520primarily%2520focus%2520on%2520Domain%2520Adaptation%2520%2528DA%2529%252C%2520which%250Aadapts%2520models%2520to%2520predefined%2520domains%2520rather%2520than%2520to%2520unseen%2520ones%253B%2520%25282%2529%2520Few%2520studies%250Atargeting%2520the%2520RSDG%2520issue%252C%2520especially%2520for%2520semantic%2520segmentation%2520tasks%252C%2520where%250Aexisting%2520models%2520are%2520developed%2520for%2520specific%2520unknown%2520domains%252C%2520struggling%2520with%250Aissues%2520of%2520underfitting%2520on%2520other%2520unknown%2520scenarios%253B%2520%25283%2529%2520Existing%2520RS%2520foundation%250Amodels%2520tend%2520to%2520prioritize%2520in-domain%2520performance%2520over%2520cross-domain%250Ageneralization.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520first%2520vision%2520foundation%2520model%2520for%250ARSDG%2520semantic%2520segmentation%252C%2520CrossEarth.%2520CrossEarth%2520demonstrates%2520strong%250Across-domain%2520generalization%2520through%2520a%2520specially%2520designed%2520data-level%2520Earth-Style%250AInjection%2520pipeline%2520and%2520a%2520model-level%2520Multi-Task%2520Training%2520pipeline.%2520In%2520addition%252C%250Afor%2520the%2520semantic%2520segmentation%2520task%252C%2520we%2520have%2520curated%2520an%2520RSDG%2520benchmark%250Acomprising%252032%2520cross-domain%2520settings%2520across%2520various%2520regions%252C%2520spectral%2520bands%252C%250Aplatforms%252C%2520and%2520climates%252C%2520providing%2520a%2520comprehensive%2520framework%2520for%2520testing%2520the%250Ageneralizability%2520of%2520future%2520RSDG%2520models.%2520Extensive%2520experiments%2520on%2520this%2520benchmark%250Ademonstrate%2520the%2520superiority%2520of%2520CrossEarth%2520over%2520existing%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22629v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrossEarth%3A%20Geospatial%20Vision%20Foundation%20Model%20for%20Domain%20Generalizable%0A%20%20Remote%20Sensing%20Semantic%20Segmentation&entry.906535625=Ziyang%20Gong%20and%20Zhixiang%20Wei%20and%20Di%20Wang%20and%20Xiaoxing%20Hu%20and%20Xianzheng%20Ma%20and%20Hongruixuan%20Chen%20and%20Yuru%20Jia%20and%20Yupeng%20Deng%20and%20Zhenming%20Ji%20and%20Xiangwei%20Zhu%20and%20Xue%20Yang%20and%20Naoto%20Yokoya%20and%20Jing%20Zhang%20and%20Bo%20Du%20and%20Junchi%20Yan%20and%20Liangpei%20Zhang&entry.1292438233=%20%20The%20field%20of%20Remote%20Sensing%20Domain%20Generalization%20%28RSDG%29%20has%20emerged%20as%20a%0Acritical%20and%20valuable%20research%20frontier%2C%20focusing%20on%20developing%20models%20that%0Ageneralize%20effectively%20across%20diverse%20scenarios.%20Despite%20the%20substantial%20domain%0Agaps%20in%20RS%20images%20that%20are%20characterized%20by%20variabilities%20such%20as%20location%2C%0Awavelength%2C%20and%20sensor%20type%2C%20research%20in%20this%20area%20remains%20underexplored%3A%20%281%29%0ACurrent%20cross-domain%20methods%20primarily%20focus%20on%20Domain%20Adaptation%20%28DA%29%2C%20which%0Aadapts%20models%20to%20predefined%20domains%20rather%20than%20to%20unseen%20ones%3B%20%282%29%20Few%20studies%0Atargeting%20the%20RSDG%20issue%2C%20especially%20for%20semantic%20segmentation%20tasks%2C%20where%0Aexisting%20models%20are%20developed%20for%20specific%20unknown%20domains%2C%20struggling%20with%0Aissues%20of%20underfitting%20on%20other%20unknown%20scenarios%3B%20%283%29%20Existing%20RS%20foundation%0Amodels%20tend%20to%20prioritize%20in-domain%20performance%20over%20cross-domain%0Ageneralization.%20To%20this%20end%2C%20we%20introduce%20the%20first%20vision%20foundation%20model%20for%0ARSDG%20semantic%20segmentation%2C%20CrossEarth.%20CrossEarth%20demonstrates%20strong%0Across-domain%20generalization%20through%20a%20specially%20designed%20data-level%20Earth-Style%0AInjection%20pipeline%20and%20a%20model-level%20Multi-Task%20Training%20pipeline.%20In%20addition%2C%0Afor%20the%20semantic%20segmentation%20task%2C%20we%20have%20curated%20an%20RSDG%20benchmark%0Acomprising%2032%20cross-domain%20settings%20across%20various%20regions%2C%20spectral%20bands%2C%0Aplatforms%2C%20and%20climates%2C%20providing%20a%20comprehensive%20framework%20for%20testing%20the%0Ageneralizability%20of%20future%20RSDG%20models.%20Extensive%20experiments%20on%20this%20benchmark%0Ademonstrate%20the%20superiority%20of%20CrossEarth%20over%20existing%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22629v3&entry.124074799=Read"},
{"title": "Deep Spherical Superpixels", "author": "R\u00e9mi Giraud and Micha\u00ebl Cl\u00e9ment", "abstract": "  Over the years, the use of superpixel segmentation has become very popular in\nvarious applications, serving as a preprocessing step to reduce data size by\nadapting to the content of the image, regardless of its semantic content. While\nthe superpixel segmentation of standard planar images, captured with a 90{\\deg}\nfield of view, has been extensively studied, there has been limited focus on\ndedicated methods to omnidirectional or spherical images, captured with a\n360{\\deg} field of view. In this study, we introduce the first deep\nlearning-based superpixel segmentation approach tailored for omnidirectional\nimages called DSS (for Deep Spherical Superpixels). Our methodology leverages\non spherical CNN architectures and the differentiable K-means clustering\nparadigm for superpixels, to generate superpixels that follow the spherical\ngeometry. Additionally, we propose to use data augmentation techniques\nspecifically designed for 360{\\deg} images, enabling our model to efficiently\nlearn from a limited set of annotated omnidirectional data. Our extensive\nvalidation across two datasets demonstrates that taking into account the\ninherent circular geometry of such images into our framework improves the\nsegmentation performance over traditional and deep learning-based superpixel\nmethods. Our code is available online.\n", "link": "http://arxiv.org/abs/2407.17354v2", "date": "2025-09-23", "relevancy": 2.7599, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5524}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5521}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Spherical%20Superpixels&body=Title%3A%20Deep%20Spherical%20Superpixels%0AAuthor%3A%20R%C3%A9mi%20Giraud%20and%20Micha%C3%ABl%20Cl%C3%A9ment%0AAbstract%3A%20%20%20Over%20the%20years%2C%20the%20use%20of%20superpixel%20segmentation%20has%20become%20very%20popular%20in%0Avarious%20applications%2C%20serving%20as%20a%20preprocessing%20step%20to%20reduce%20data%20size%20by%0Aadapting%20to%20the%20content%20of%20the%20image%2C%20regardless%20of%20its%20semantic%20content.%20While%0Athe%20superpixel%20segmentation%20of%20standard%20planar%20images%2C%20captured%20with%20a%2090%7B%5Cdeg%7D%0Afield%20of%20view%2C%20has%20been%20extensively%20studied%2C%20there%20has%20been%20limited%20focus%20on%0Adedicated%20methods%20to%20omnidirectional%20or%20spherical%20images%2C%20captured%20with%20a%0A360%7B%5Cdeg%7D%20field%20of%20view.%20In%20this%20study%2C%20we%20introduce%20the%20first%20deep%0Alearning-based%20superpixel%20segmentation%20approach%20tailored%20for%20omnidirectional%0Aimages%20called%20DSS%20%28for%20Deep%20Spherical%20Superpixels%29.%20Our%20methodology%20leverages%0Aon%20spherical%20CNN%20architectures%20and%20the%20differentiable%20K-means%20clustering%0Aparadigm%20for%20superpixels%2C%20to%20generate%20superpixels%20that%20follow%20the%20spherical%0Ageometry.%20Additionally%2C%20we%20propose%20to%20use%20data%20augmentation%20techniques%0Aspecifically%20designed%20for%20360%7B%5Cdeg%7D%20images%2C%20enabling%20our%20model%20to%20efficiently%0Alearn%20from%20a%20limited%20set%20of%20annotated%20omnidirectional%20data.%20Our%20extensive%0Avalidation%20across%20two%20datasets%20demonstrates%20that%20taking%20into%20account%20the%0Ainherent%20circular%20geometry%20of%20such%20images%20into%20our%20framework%20improves%20the%0Asegmentation%20performance%20over%20traditional%20and%20deep%20learning-based%20superpixel%0Amethods.%20Our%20code%20is%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17354v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Spherical%2520Superpixels%26entry.906535625%3DR%25C3%25A9mi%2520Giraud%2520and%2520Micha%25C3%25ABl%2520Cl%25C3%25A9ment%26entry.1292438233%3D%2520%2520Over%2520the%2520years%252C%2520the%2520use%2520of%2520superpixel%2520segmentation%2520has%2520become%2520very%2520popular%2520in%250Avarious%2520applications%252C%2520serving%2520as%2520a%2520preprocessing%2520step%2520to%2520reduce%2520data%2520size%2520by%250Aadapting%2520to%2520the%2520content%2520of%2520the%2520image%252C%2520regardless%2520of%2520its%2520semantic%2520content.%2520While%250Athe%2520superpixel%2520segmentation%2520of%2520standard%2520planar%2520images%252C%2520captured%2520with%2520a%252090%257B%255Cdeg%257D%250Afield%2520of%2520view%252C%2520has%2520been%2520extensively%2520studied%252C%2520there%2520has%2520been%2520limited%2520focus%2520on%250Adedicated%2520methods%2520to%2520omnidirectional%2520or%2520spherical%2520images%252C%2520captured%2520with%2520a%250A360%257B%255Cdeg%257D%2520field%2520of%2520view.%2520In%2520this%2520study%252C%2520we%2520introduce%2520the%2520first%2520deep%250Alearning-based%2520superpixel%2520segmentation%2520approach%2520tailored%2520for%2520omnidirectional%250Aimages%2520called%2520DSS%2520%2528for%2520Deep%2520Spherical%2520Superpixels%2529.%2520Our%2520methodology%2520leverages%250Aon%2520spherical%2520CNN%2520architectures%2520and%2520the%2520differentiable%2520K-means%2520clustering%250Aparadigm%2520for%2520superpixels%252C%2520to%2520generate%2520superpixels%2520that%2520follow%2520the%2520spherical%250Ageometry.%2520Additionally%252C%2520we%2520propose%2520to%2520use%2520data%2520augmentation%2520techniques%250Aspecifically%2520designed%2520for%2520360%257B%255Cdeg%257D%2520images%252C%2520enabling%2520our%2520model%2520to%2520efficiently%250Alearn%2520from%2520a%2520limited%2520set%2520of%2520annotated%2520omnidirectional%2520data.%2520Our%2520extensive%250Avalidation%2520across%2520two%2520datasets%2520demonstrates%2520that%2520taking%2520into%2520account%2520the%250Ainherent%2520circular%2520geometry%2520of%2520such%2520images%2520into%2520our%2520framework%2520improves%2520the%250Asegmentation%2520performance%2520over%2520traditional%2520and%2520deep%2520learning-based%2520superpixel%250Amethods.%2520Our%2520code%2520is%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17354v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Spherical%20Superpixels&entry.906535625=R%C3%A9mi%20Giraud%20and%20Micha%C3%ABl%20Cl%C3%A9ment&entry.1292438233=%20%20Over%20the%20years%2C%20the%20use%20of%20superpixel%20segmentation%20has%20become%20very%20popular%20in%0Avarious%20applications%2C%20serving%20as%20a%20preprocessing%20step%20to%20reduce%20data%20size%20by%0Aadapting%20to%20the%20content%20of%20the%20image%2C%20regardless%20of%20its%20semantic%20content.%20While%0Athe%20superpixel%20segmentation%20of%20standard%20planar%20images%2C%20captured%20with%20a%2090%7B%5Cdeg%7D%0Afield%20of%20view%2C%20has%20been%20extensively%20studied%2C%20there%20has%20been%20limited%20focus%20on%0Adedicated%20methods%20to%20omnidirectional%20or%20spherical%20images%2C%20captured%20with%20a%0A360%7B%5Cdeg%7D%20field%20of%20view.%20In%20this%20study%2C%20we%20introduce%20the%20first%20deep%0Alearning-based%20superpixel%20segmentation%20approach%20tailored%20for%20omnidirectional%0Aimages%20called%20DSS%20%28for%20Deep%20Spherical%20Superpixels%29.%20Our%20methodology%20leverages%0Aon%20spherical%20CNN%20architectures%20and%20the%20differentiable%20K-means%20clustering%0Aparadigm%20for%20superpixels%2C%20to%20generate%20superpixels%20that%20follow%20the%20spherical%0Ageometry.%20Additionally%2C%20we%20propose%20to%20use%20data%20augmentation%20techniques%0Aspecifically%20designed%20for%20360%7B%5Cdeg%7D%20images%2C%20enabling%20our%20model%20to%20efficiently%0Alearn%20from%20a%20limited%20set%20of%20annotated%20omnidirectional%20data.%20Our%20extensive%0Avalidation%20across%20two%20datasets%20demonstrates%20that%20taking%20into%20account%20the%0Ainherent%20circular%20geometry%20of%20such%20images%20into%20our%20framework%20improves%20the%0Asegmentation%20performance%20over%20traditional%20and%20deep%20learning-based%20superpixel%0Amethods.%20Our%20code%20is%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17354v2&entry.124074799=Read"},
{"title": "Improving Image Captioning Descriptiveness by Ranking and LLM-based\n  Fusion", "author": "Luigi Celona and Simone Bianco and Marco Donzella and Paolo Napoletano", "abstract": "  State-of-The-Art (SoTA) image captioning models are often trained on the\nMicroSoft Common Objects in Context (MS-COCO) dataset, which contains\nhuman-annotated captions with an average length of approximately ten tokens.\nAlthough effective for general scene understanding, these short captions often\nfail to capture complex scenes and convey detailed information. Moreover,\ncaptioning models tend to exhibit bias towards the ``average'' caption, which\ncaptures only the more general aspects, thus overlooking finer details. In this\npaper, we present a novel approach to generate richer and more informative\nimage captions by combining the captions generated from different SoTA\ncaptioning models. Our proposed method requires no additional model training:\ngiven an image, it leverages pre-trained models from the literature to generate\nthe initial captions, and then ranks them using a newly introduced\nimage-text-based metric, which we name BLIPScore. Subsequently, the top two\ncaptions are fused using a Large Language Model (LLM) to produce the final,\nmore detailed description. Experimental results on the MS-COCO and Flickr30k\ntest sets demonstrate the effectiveness of our approach in terms of\ncaption-image alignment and hallucination reduction according to the ALOHa,\nCAPTURE, and Polos metrics. A subjective study lends additional support to\nthese results, suggesting that the captions produced by our model are generally\nperceived as more consistent with human judgment. By combining the strengths of\ndiverse SoTA models, our method enhances the quality and appeal of image\ncaptions, bridging the gap between automated systems and the rich and\ninformative nature of human-generated descriptions. This advance enables the\ngeneration of more suitable captions for the training of both vision-language\nand captioning models.\n", "link": "http://arxiv.org/abs/2306.11593v2", "date": "2025-09-23", "relevancy": 2.759, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Image%20Captioning%20Descriptiveness%20by%20Ranking%20and%20LLM-based%0A%20%20Fusion&body=Title%3A%20Improving%20Image%20Captioning%20Descriptiveness%20by%20Ranking%20and%20LLM-based%0A%20%20Fusion%0AAuthor%3A%20Luigi%20Celona%20and%20Simone%20Bianco%20and%20Marco%20Donzella%20and%20Paolo%20Napoletano%0AAbstract%3A%20%20%20State-of-The-Art%20%28SoTA%29%20image%20captioning%20models%20are%20often%20trained%20on%20the%0AMicroSoft%20Common%20Objects%20in%20Context%20%28MS-COCO%29%20dataset%2C%20which%20contains%0Ahuman-annotated%20captions%20with%20an%20average%20length%20of%20approximately%20ten%20tokens.%0AAlthough%20effective%20for%20general%20scene%20understanding%2C%20these%20short%20captions%20often%0Afail%20to%20capture%20complex%20scenes%20and%20convey%20detailed%20information.%20Moreover%2C%0Acaptioning%20models%20tend%20to%20exhibit%20bias%20towards%20the%20%60%60average%27%27%20caption%2C%20which%0Acaptures%20only%20the%20more%20general%20aspects%2C%20thus%20overlooking%20finer%20details.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20approach%20to%20generate%20richer%20and%20more%20informative%0Aimage%20captions%20by%20combining%20the%20captions%20generated%20from%20different%20SoTA%0Acaptioning%20models.%20Our%20proposed%20method%20requires%20no%20additional%20model%20training%3A%0Agiven%20an%20image%2C%20it%20leverages%20pre-trained%20models%20from%20the%20literature%20to%20generate%0Athe%20initial%20captions%2C%20and%20then%20ranks%20them%20using%20a%20newly%20introduced%0Aimage-text-based%20metric%2C%20which%20we%20name%20BLIPScore.%20Subsequently%2C%20the%20top%20two%0Acaptions%20are%20fused%20using%20a%20Large%20Language%20Model%20%28LLM%29%20to%20produce%20the%20final%2C%0Amore%20detailed%20description.%20Experimental%20results%20on%20the%20MS-COCO%20and%20Flickr30k%0Atest%20sets%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20terms%20of%0Acaption-image%20alignment%20and%20hallucination%20reduction%20according%20to%20the%20ALOHa%2C%0ACAPTURE%2C%20and%20Polos%20metrics.%20A%20subjective%20study%20lends%20additional%20support%20to%0Athese%20results%2C%20suggesting%20that%20the%20captions%20produced%20by%20our%20model%20are%20generally%0Aperceived%20as%20more%20consistent%20with%20human%20judgment.%20By%20combining%20the%20strengths%20of%0Adiverse%20SoTA%20models%2C%20our%20method%20enhances%20the%20quality%20and%20appeal%20of%20image%0Acaptions%2C%20bridging%20the%20gap%20between%20automated%20systems%20and%20the%20rich%20and%0Ainformative%20nature%20of%20human-generated%20descriptions.%20This%20advance%20enables%20the%0Ageneration%20of%20more%20suitable%20captions%20for%20the%20training%20of%20both%20vision-language%0Aand%20captioning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Image%2520Captioning%2520Descriptiveness%2520by%2520Ranking%2520and%2520LLM-based%250A%2520%2520Fusion%26entry.906535625%3DLuigi%2520Celona%2520and%2520Simone%2520Bianco%2520and%2520Marco%2520Donzella%2520and%2520Paolo%2520Napoletano%26entry.1292438233%3D%2520%2520State-of-The-Art%2520%2528SoTA%2529%2520image%2520captioning%2520models%2520are%2520often%2520trained%2520on%2520the%250AMicroSoft%2520Common%2520Objects%2520in%2520Context%2520%2528MS-COCO%2529%2520dataset%252C%2520which%2520contains%250Ahuman-annotated%2520captions%2520with%2520an%2520average%2520length%2520of%2520approximately%2520ten%2520tokens.%250AAlthough%2520effective%2520for%2520general%2520scene%2520understanding%252C%2520these%2520short%2520captions%2520often%250Afail%2520to%2520capture%2520complex%2520scenes%2520and%2520convey%2520detailed%2520information.%2520Moreover%252C%250Acaptioning%2520models%2520tend%2520to%2520exhibit%2520bias%2520towards%2520the%2520%2560%2560average%2527%2527%2520caption%252C%2520which%250Acaptures%2520only%2520the%2520more%2520general%2520aspects%252C%2520thus%2520overlooking%2520finer%2520details.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520novel%2520approach%2520to%2520generate%2520richer%2520and%2520more%2520informative%250Aimage%2520captions%2520by%2520combining%2520the%2520captions%2520generated%2520from%2520different%2520SoTA%250Acaptioning%2520models.%2520Our%2520proposed%2520method%2520requires%2520no%2520additional%2520model%2520training%253A%250Agiven%2520an%2520image%252C%2520it%2520leverages%2520pre-trained%2520models%2520from%2520the%2520literature%2520to%2520generate%250Athe%2520initial%2520captions%252C%2520and%2520then%2520ranks%2520them%2520using%2520a%2520newly%2520introduced%250Aimage-text-based%2520metric%252C%2520which%2520we%2520name%2520BLIPScore.%2520Subsequently%252C%2520the%2520top%2520two%250Acaptions%2520are%2520fused%2520using%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520produce%2520the%2520final%252C%250Amore%2520detailed%2520description.%2520Experimental%2520results%2520on%2520the%2520MS-COCO%2520and%2520Flickr30k%250Atest%2520sets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520terms%2520of%250Acaption-image%2520alignment%2520and%2520hallucination%2520reduction%2520according%2520to%2520the%2520ALOHa%252C%250ACAPTURE%252C%2520and%2520Polos%2520metrics.%2520A%2520subjective%2520study%2520lends%2520additional%2520support%2520to%250Athese%2520results%252C%2520suggesting%2520that%2520the%2520captions%2520produced%2520by%2520our%2520model%2520are%2520generally%250Aperceived%2520as%2520more%2520consistent%2520with%2520human%2520judgment.%2520By%2520combining%2520the%2520strengths%2520of%250Adiverse%2520SoTA%2520models%252C%2520our%2520method%2520enhances%2520the%2520quality%2520and%2520appeal%2520of%2520image%250Acaptions%252C%2520bridging%2520the%2520gap%2520between%2520automated%2520systems%2520and%2520the%2520rich%2520and%250Ainformative%2520nature%2520of%2520human-generated%2520descriptions.%2520This%2520advance%2520enables%2520the%250Ageneration%2520of%2520more%2520suitable%2520captions%2520for%2520the%2520training%2520of%2520both%2520vision-language%250Aand%2520captioning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.11593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Image%20Captioning%20Descriptiveness%20by%20Ranking%20and%20LLM-based%0A%20%20Fusion&entry.906535625=Luigi%20Celona%20and%20Simone%20Bianco%20and%20Marco%20Donzella%20and%20Paolo%20Napoletano&entry.1292438233=%20%20State-of-The-Art%20%28SoTA%29%20image%20captioning%20models%20are%20often%20trained%20on%20the%0AMicroSoft%20Common%20Objects%20in%20Context%20%28MS-COCO%29%20dataset%2C%20which%20contains%0Ahuman-annotated%20captions%20with%20an%20average%20length%20of%20approximately%20ten%20tokens.%0AAlthough%20effective%20for%20general%20scene%20understanding%2C%20these%20short%20captions%20often%0Afail%20to%20capture%20complex%20scenes%20and%20convey%20detailed%20information.%20Moreover%2C%0Acaptioning%20models%20tend%20to%20exhibit%20bias%20towards%20the%20%60%60average%27%27%20caption%2C%20which%0Acaptures%20only%20the%20more%20general%20aspects%2C%20thus%20overlooking%20finer%20details.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20approach%20to%20generate%20richer%20and%20more%20informative%0Aimage%20captions%20by%20combining%20the%20captions%20generated%20from%20different%20SoTA%0Acaptioning%20models.%20Our%20proposed%20method%20requires%20no%20additional%20model%20training%3A%0Agiven%20an%20image%2C%20it%20leverages%20pre-trained%20models%20from%20the%20literature%20to%20generate%0Athe%20initial%20captions%2C%20and%20then%20ranks%20them%20using%20a%20newly%20introduced%0Aimage-text-based%20metric%2C%20which%20we%20name%20BLIPScore.%20Subsequently%2C%20the%20top%20two%0Acaptions%20are%20fused%20using%20a%20Large%20Language%20Model%20%28LLM%29%20to%20produce%20the%20final%2C%0Amore%20detailed%20description.%20Experimental%20results%20on%20the%20MS-COCO%20and%20Flickr30k%0Atest%20sets%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20terms%20of%0Acaption-image%20alignment%20and%20hallucination%20reduction%20according%20to%20the%20ALOHa%2C%0ACAPTURE%2C%20and%20Polos%20metrics.%20A%20subjective%20study%20lends%20additional%20support%20to%0Athese%20results%2C%20suggesting%20that%20the%20captions%20produced%20by%20our%20model%20are%20generally%0Aperceived%20as%20more%20consistent%20with%20human%20judgment.%20By%20combining%20the%20strengths%20of%0Adiverse%20SoTA%20models%2C%20our%20method%20enhances%20the%20quality%20and%20appeal%20of%20image%0Acaptions%2C%20bridging%20the%20gap%20between%20automated%20systems%20and%20the%20rich%20and%0Ainformative%20nature%20of%20human-generated%20descriptions.%20This%20advance%20enables%20the%0Ageneration%20of%20more%20suitable%20captions%20for%20the%20training%20of%20both%20vision-language%0Aand%20captioning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11593v2&entry.124074799=Read"},
{"title": "Theoretical Foundations of Representation Learning using Unlabeled Data:\n  Statistics and Optimization", "author": "Pascal Esser and Maximilian Fleissner and Debarghya Ghoshdastidar", "abstract": "  Representation learning from unlabeled data has been extensively studied in\nstatistics, data science and signal processing with a rich literature on\ntechniques for dimension reduction, compression, multi-dimensional scaling\namong others. However, current deep learning models use new principles for\nunsupervised representation learning that cannot be easily analyzed using\nclassical theories. For example, visual foundation models have found tremendous\nsuccess using self-supervision or denoising/masked autoencoders, which\neffectively learn representations from massive amounts of unlabeled data.\nHowever, it remains difficult to characterize the representations learned by\nthese models and to explain why they perform well for diverse prediction tasks\nor show emergent behavior. To answer these questions, one needs to combine\nmathematical tools from statistics and optimization. This paper provides an\noverview of recent theoretical advances in representation learning from\nunlabeled data and mentions our contributions in this direction.\n", "link": "http://arxiv.org/abs/2509.18997v1", "date": "2025-09-23", "relevancy": 2.7306, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6005}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5225}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theoretical%20Foundations%20of%20Representation%20Learning%20using%20Unlabeled%20Data%3A%0A%20%20Statistics%20and%20Optimization&body=Title%3A%20Theoretical%20Foundations%20of%20Representation%20Learning%20using%20Unlabeled%20Data%3A%0A%20%20Statistics%20and%20Optimization%0AAuthor%3A%20Pascal%20Esser%20and%20Maximilian%20Fleissner%20and%20Debarghya%20Ghoshdastidar%0AAbstract%3A%20%20%20Representation%20learning%20from%20unlabeled%20data%20has%20been%20extensively%20studied%20in%0Astatistics%2C%20data%20science%20and%20signal%20processing%20with%20a%20rich%20literature%20on%0Atechniques%20for%20dimension%20reduction%2C%20compression%2C%20multi-dimensional%20scaling%0Aamong%20others.%20However%2C%20current%20deep%20learning%20models%20use%20new%20principles%20for%0Aunsupervised%20representation%20learning%20that%20cannot%20be%20easily%20analyzed%20using%0Aclassical%20theories.%20For%20example%2C%20visual%20foundation%20models%20have%20found%20tremendous%0Asuccess%20using%20self-supervision%20or%20denoising/masked%20autoencoders%2C%20which%0Aeffectively%20learn%20representations%20from%20massive%20amounts%20of%20unlabeled%20data.%0AHowever%2C%20it%20remains%20difficult%20to%20characterize%20the%20representations%20learned%20by%0Athese%20models%20and%20to%20explain%20why%20they%20perform%20well%20for%20diverse%20prediction%20tasks%0Aor%20show%20emergent%20behavior.%20To%20answer%20these%20questions%2C%20one%20needs%20to%20combine%0Amathematical%20tools%20from%20statistics%20and%20optimization.%20This%20paper%20provides%20an%0Aoverview%20of%20recent%20theoretical%20advances%20in%20representation%20learning%20from%0Aunlabeled%20data%20and%20mentions%20our%20contributions%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheoretical%2520Foundations%2520of%2520Representation%2520Learning%2520using%2520Unlabeled%2520Data%253A%250A%2520%2520Statistics%2520and%2520Optimization%26entry.906535625%3DPascal%2520Esser%2520and%2520Maximilian%2520Fleissner%2520and%2520Debarghya%2520Ghoshdastidar%26entry.1292438233%3D%2520%2520Representation%2520learning%2520from%2520unlabeled%2520data%2520has%2520been%2520extensively%2520studied%2520in%250Astatistics%252C%2520data%2520science%2520and%2520signal%2520processing%2520with%2520a%2520rich%2520literature%2520on%250Atechniques%2520for%2520dimension%2520reduction%252C%2520compression%252C%2520multi-dimensional%2520scaling%250Aamong%2520others.%2520However%252C%2520current%2520deep%2520learning%2520models%2520use%2520new%2520principles%2520for%250Aunsupervised%2520representation%2520learning%2520that%2520cannot%2520be%2520easily%2520analyzed%2520using%250Aclassical%2520theories.%2520For%2520example%252C%2520visual%2520foundation%2520models%2520have%2520found%2520tremendous%250Asuccess%2520using%2520self-supervision%2520or%2520denoising/masked%2520autoencoders%252C%2520which%250Aeffectively%2520learn%2520representations%2520from%2520massive%2520amounts%2520of%2520unlabeled%2520data.%250AHowever%252C%2520it%2520remains%2520difficult%2520to%2520characterize%2520the%2520representations%2520learned%2520by%250Athese%2520models%2520and%2520to%2520explain%2520why%2520they%2520perform%2520well%2520for%2520diverse%2520prediction%2520tasks%250Aor%2520show%2520emergent%2520behavior.%2520To%2520answer%2520these%2520questions%252C%2520one%2520needs%2520to%2520combine%250Amathematical%2520tools%2520from%2520statistics%2520and%2520optimization.%2520This%2520paper%2520provides%2520an%250Aoverview%2520of%2520recent%2520theoretical%2520advances%2520in%2520representation%2520learning%2520from%250Aunlabeled%2520data%2520and%2520mentions%2520our%2520contributions%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theoretical%20Foundations%20of%20Representation%20Learning%20using%20Unlabeled%20Data%3A%0A%20%20Statistics%20and%20Optimization&entry.906535625=Pascal%20Esser%20and%20Maximilian%20Fleissner%20and%20Debarghya%20Ghoshdastidar&entry.1292438233=%20%20Representation%20learning%20from%20unlabeled%20data%20has%20been%20extensively%20studied%20in%0Astatistics%2C%20data%20science%20and%20signal%20processing%20with%20a%20rich%20literature%20on%0Atechniques%20for%20dimension%20reduction%2C%20compression%2C%20multi-dimensional%20scaling%0Aamong%20others.%20However%2C%20current%20deep%20learning%20models%20use%20new%20principles%20for%0Aunsupervised%20representation%20learning%20that%20cannot%20be%20easily%20analyzed%20using%0Aclassical%20theories.%20For%20example%2C%20visual%20foundation%20models%20have%20found%20tremendous%0Asuccess%20using%20self-supervision%20or%20denoising/masked%20autoencoders%2C%20which%0Aeffectively%20learn%20representations%20from%20massive%20amounts%20of%20unlabeled%20data.%0AHowever%2C%20it%20remains%20difficult%20to%20characterize%20the%20representations%20learned%20by%0Athese%20models%20and%20to%20explain%20why%20they%20perform%20well%20for%20diverse%20prediction%20tasks%0Aor%20show%20emergent%20behavior.%20To%20answer%20these%20questions%2C%20one%20needs%20to%20combine%0Amathematical%20tools%20from%20statistics%20and%20optimization.%20This%20paper%20provides%20an%0Aoverview%20of%20recent%20theoretical%20advances%20in%20representation%20learning%20from%0Aunlabeled%20data%20and%20mentions%20our%20contributions%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18997v1&entry.124074799=Read"},
{"title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty\n  Estimation", "author": "Minoo Dolatabadi and Fardin Ayar and Ehsan Javanmardi and Manabu Tsukada and Mahdi Javanmardi", "abstract": "  LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.\n", "link": "http://arxiv.org/abs/2509.18954v1", "date": "2025-09-23", "relevancy": 2.7254, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.7449}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6493}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20LiDAR%20Localization%3A%20Deep%20Learning-based%20Uncertainty%0A%20%20Estimation&body=Title%3A%20Towards%20Robust%20LiDAR%20Localization%3A%20Deep%20Learning-based%20Uncertainty%0A%20%20Estimation%0AAuthor%3A%20Minoo%20Dolatabadi%20and%20Fardin%20Ayar%20and%20Ehsan%20Javanmardi%20and%20Manabu%20Tsukada%20and%20Mahdi%20Javanmardi%0AAbstract%3A%20%20%20LiDAR-based%20localization%20and%20SLAM%20often%20rely%20on%20iterative%20matching%0Aalgorithms%2C%20particularly%20the%20Iterative%20Closest%20Point%20%28ICP%29%20algorithm%2C%20to%20align%0Asensor%20data%20with%20pre-existing%20maps%20or%20previous%20scans.%20However%2C%20ICP%20is%20prone%20to%0Aerrors%20in%20featureless%20environments%20and%20dynamic%20scenes%2C%20leading%20to%20inaccurate%0Apose%20estimation.%20Accurately%20predicting%20the%20uncertainty%20associated%20with%20ICP%20is%0Acrucial%20for%20robust%20state%20estimation%20but%20remains%20challenging%2C%20as%20existing%0Aapproaches%20often%20rely%20on%20handcrafted%20models%20or%20simplified%20assumptions.%0AMoreover%2C%20a%20few%20deep%20learning-based%20methods%20for%20localizability%20estimation%0Aeither%20depend%20on%20a%20pre-built%20map%2C%20which%20may%20not%20always%20be%20available%2C%20or%20provide%0Aa%20binary%20classification%20of%20localizable%20versus%20non-localizable%2C%20which%20fails%20to%0Aproperly%20model%20uncertainty.%20In%20this%20work%2C%20we%20propose%20a%20data-driven%20framework%0Athat%20leverages%20deep%20learning%20to%20estimate%20the%20registration%20error%20covariance%20of%0AICP%20before%20matching%2C%20even%20in%20the%20absence%20of%20a%20reference%20map.%20By%20associating%0Aeach%20LiDAR%20scan%20with%20a%20reliable%206-DoF%20error%20covariance%20estimate%2C%20our%20method%0Aenables%20seamless%20integration%20of%20ICP%20within%20Kalman%20filtering%2C%20enhancing%0Alocalization%20accuracy%20and%20robustness.%20Extensive%20experiments%20on%20the%20KITTI%0Adataset%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20that%20it%0Aaccurately%20predicts%20covariance%20and%2C%20when%20applied%20to%20localization%20using%20a%0Apre-built%20map%20or%20SLAM%2C%20reduces%20localization%20errors%20and%20improves%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520LiDAR%2520Localization%253A%2520Deep%2520Learning-based%2520Uncertainty%250A%2520%2520Estimation%26entry.906535625%3DMinoo%2520Dolatabadi%2520and%2520Fardin%2520Ayar%2520and%2520Ehsan%2520Javanmardi%2520and%2520Manabu%2520Tsukada%2520and%2520Mahdi%2520Javanmardi%26entry.1292438233%3D%2520%2520LiDAR-based%2520localization%2520and%2520SLAM%2520often%2520rely%2520on%2520iterative%2520matching%250Aalgorithms%252C%2520particularly%2520the%2520Iterative%2520Closest%2520Point%2520%2528ICP%2529%2520algorithm%252C%2520to%2520align%250Asensor%2520data%2520with%2520pre-existing%2520maps%2520or%2520previous%2520scans.%2520However%252C%2520ICP%2520is%2520prone%2520to%250Aerrors%2520in%2520featureless%2520environments%2520and%2520dynamic%2520scenes%252C%2520leading%2520to%2520inaccurate%250Apose%2520estimation.%2520Accurately%2520predicting%2520the%2520uncertainty%2520associated%2520with%2520ICP%2520is%250Acrucial%2520for%2520robust%2520state%2520estimation%2520but%2520remains%2520challenging%252C%2520as%2520existing%250Aapproaches%2520often%2520rely%2520on%2520handcrafted%2520models%2520or%2520simplified%2520assumptions.%250AMoreover%252C%2520a%2520few%2520deep%2520learning-based%2520methods%2520for%2520localizability%2520estimation%250Aeither%2520depend%2520on%2520a%2520pre-built%2520map%252C%2520which%2520may%2520not%2520always%2520be%2520available%252C%2520or%2520provide%250Aa%2520binary%2520classification%2520of%2520localizable%2520versus%2520non-localizable%252C%2520which%2520fails%2520to%250Aproperly%2520model%2520uncertainty.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520data-driven%2520framework%250Athat%2520leverages%2520deep%2520learning%2520to%2520estimate%2520the%2520registration%2520error%2520covariance%2520of%250AICP%2520before%2520matching%252C%2520even%2520in%2520the%2520absence%2520of%2520a%2520reference%2520map.%2520By%2520associating%250Aeach%2520LiDAR%2520scan%2520with%2520a%2520reliable%25206-DoF%2520error%2520covariance%2520estimate%252C%2520our%2520method%250Aenables%2520seamless%2520integration%2520of%2520ICP%2520within%2520Kalman%2520filtering%252C%2520enhancing%250Alocalization%2520accuracy%2520and%2520robustness.%2520Extensive%2520experiments%2520on%2520the%2520KITTI%250Adataset%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520showing%2520that%2520it%250Aaccurately%2520predicts%2520covariance%2520and%252C%2520when%2520applied%2520to%2520localization%2520using%2520a%250Apre-built%2520map%2520or%2520SLAM%252C%2520reduces%2520localization%2520errors%2520and%2520improves%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20LiDAR%20Localization%3A%20Deep%20Learning-based%20Uncertainty%0A%20%20Estimation&entry.906535625=Minoo%20Dolatabadi%20and%20Fardin%20Ayar%20and%20Ehsan%20Javanmardi%20and%20Manabu%20Tsukada%20and%20Mahdi%20Javanmardi&entry.1292438233=%20%20LiDAR-based%20localization%20and%20SLAM%20often%20rely%20on%20iterative%20matching%0Aalgorithms%2C%20particularly%20the%20Iterative%20Closest%20Point%20%28ICP%29%20algorithm%2C%20to%20align%0Asensor%20data%20with%20pre-existing%20maps%20or%20previous%20scans.%20However%2C%20ICP%20is%20prone%20to%0Aerrors%20in%20featureless%20environments%20and%20dynamic%20scenes%2C%20leading%20to%20inaccurate%0Apose%20estimation.%20Accurately%20predicting%20the%20uncertainty%20associated%20with%20ICP%20is%0Acrucial%20for%20robust%20state%20estimation%20but%20remains%20challenging%2C%20as%20existing%0Aapproaches%20often%20rely%20on%20handcrafted%20models%20or%20simplified%20assumptions.%0AMoreover%2C%20a%20few%20deep%20learning-based%20methods%20for%20localizability%20estimation%0Aeither%20depend%20on%20a%20pre-built%20map%2C%20which%20may%20not%20always%20be%20available%2C%20or%20provide%0Aa%20binary%20classification%20of%20localizable%20versus%20non-localizable%2C%20which%20fails%20to%0Aproperly%20model%20uncertainty.%20In%20this%20work%2C%20we%20propose%20a%20data-driven%20framework%0Athat%20leverages%20deep%20learning%20to%20estimate%20the%20registration%20error%20covariance%20of%0AICP%20before%20matching%2C%20even%20in%20the%20absence%20of%20a%20reference%20map.%20By%20associating%0Aeach%20LiDAR%20scan%20with%20a%20reliable%206-DoF%20error%20covariance%20estimate%2C%20our%20method%0Aenables%20seamless%20integration%20of%20ICP%20within%20Kalman%20filtering%2C%20enhancing%0Alocalization%20accuracy%20and%20robustness.%20Extensive%20experiments%20on%20the%20KITTI%0Adataset%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20that%20it%0Aaccurately%20predicts%20covariance%20and%2C%20when%20applied%20to%20localization%20using%20a%0Apre-built%20map%20or%20SLAM%2C%20reduces%20localization%20errors%20and%20improves%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18954v1&entry.124074799=Read"},
{"title": "Token Preference Optimization with Self-Calibrated Visual-Anchored\n  Rewards for Hallucination Mitigation", "author": "Jihao Gu and Yingyao Wang and Meng Cao and Pi Bu and Jun Song and Yancheng He and Shilong Li and Bo Zheng", "abstract": "  Direct Preference Optimization (DPO) has been demonstrated to be highly\neffective in mitigating hallucinations in Large Vision Language Models (LVLMs)\nby aligning their outputs more closely with human preferences. Despite the\nrecent progress, existing methods suffer from two drawbacks: 1) Lack of\nscalable token-level rewards; and 2) Neglect of visual-anchored tokens. To this\nend, we propose a novel Token Preference Optimization model with\nself-calibrated rewards (dubbed as TPO), which adaptively attends to\nvisual-correlated tokens without fine-grained annotations. Specifically, we\nintroduce a token-level \\emph{visual-anchored} \\emph{reward} as the difference\nof the logistic distributions of generated tokens conditioned on the raw image\nand the corrupted one. In addition, to highlight the informative\nvisual-anchored tokens, a visual-aware training objective is proposed to\nenhance more accurate token-level optimization. Extensive experimental results\nhave manifested the state-of-the-art performance of the proposed TPO. For\nexample, by building on top of LLAVA-1.5-7B, our TPO boosts the performance\nabsolute improvement for hallucination benchmarks.\n", "link": "http://arxiv.org/abs/2412.14487v4", "date": "2025-09-23", "relevancy": 2.6988, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5235}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Preference%20Optimization%20with%20Self-Calibrated%20Visual-Anchored%0A%20%20Rewards%20for%20Hallucination%20Mitigation&body=Title%3A%20Token%20Preference%20Optimization%20with%20Self-Calibrated%20Visual-Anchored%0A%20%20Rewards%20for%20Hallucination%20Mitigation%0AAuthor%3A%20Jihao%20Gu%20and%20Yingyao%20Wang%20and%20Meng%20Cao%20and%20Pi%20Bu%20and%20Jun%20Song%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20been%20demonstrated%20to%20be%20highly%0Aeffective%20in%20mitigating%20hallucinations%20in%20Large%20Vision%20Language%20Models%20%28LVLMs%29%0Aby%20aligning%20their%20outputs%20more%20closely%20with%20human%20preferences.%20Despite%20the%0Arecent%20progress%2C%20existing%20methods%20suffer%20from%20two%20drawbacks%3A%201%29%20Lack%20of%0Ascalable%20token-level%20rewards%3B%20and%202%29%20Neglect%20of%20visual-anchored%20tokens.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20Token%20Preference%20Optimization%20model%20with%0Aself-calibrated%20rewards%20%28dubbed%20as%20TPO%29%2C%20which%20adaptively%20attends%20to%0Avisual-correlated%20tokens%20without%20fine-grained%20annotations.%20Specifically%2C%20we%0Aintroduce%20a%20token-level%20%5Cemph%7Bvisual-anchored%7D%20%5Cemph%7Breward%7D%20as%20the%20difference%0Aof%20the%20logistic%20distributions%20of%20generated%20tokens%20conditioned%20on%20the%20raw%20image%0Aand%20the%20corrupted%20one.%20In%20addition%2C%20to%20highlight%20the%20informative%0Avisual-anchored%20tokens%2C%20a%20visual-aware%20training%20objective%20is%20proposed%20to%0Aenhance%20more%20accurate%20token-level%20optimization.%20Extensive%20experimental%20results%0Ahave%20manifested%20the%20state-of-the-art%20performance%20of%20the%20proposed%20TPO.%20For%0Aexample%2C%20by%20building%20on%20top%20of%20LLAVA-1.5-7B%2C%20our%20TPO%20boosts%20the%20performance%0Aabsolute%20improvement%20for%20hallucination%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14487v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Preference%2520Optimization%2520with%2520Self-Calibrated%2520Visual-Anchored%250A%2520%2520Rewards%2520for%2520Hallucination%2520Mitigation%26entry.906535625%3DJihao%2520Gu%2520and%2520Yingyao%2520Wang%2520and%2520Meng%2520Cao%2520and%2520Pi%2520Bu%2520and%2520Jun%2520Song%2520and%2520Yancheng%2520He%2520and%2520Shilong%2520Li%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520has%2520been%2520demonstrated%2520to%2520be%2520highly%250Aeffective%2520in%2520mitigating%2520hallucinations%2520in%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%250Aby%2520aligning%2520their%2520outputs%2520more%2520closely%2520with%2520human%2520preferences.%2520Despite%2520the%250Arecent%2520progress%252C%2520existing%2520methods%2520suffer%2520from%2520two%2520drawbacks%253A%25201%2529%2520Lack%2520of%250Ascalable%2520token-level%2520rewards%253B%2520and%25202%2529%2520Neglect%2520of%2520visual-anchored%2520tokens.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520novel%2520Token%2520Preference%2520Optimization%2520model%2520with%250Aself-calibrated%2520rewards%2520%2528dubbed%2520as%2520TPO%2529%252C%2520which%2520adaptively%2520attends%2520to%250Avisual-correlated%2520tokens%2520without%2520fine-grained%2520annotations.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520token-level%2520%255Cemph%257Bvisual-anchored%257D%2520%255Cemph%257Breward%257D%2520as%2520the%2520difference%250Aof%2520the%2520logistic%2520distributions%2520of%2520generated%2520tokens%2520conditioned%2520on%2520the%2520raw%2520image%250Aand%2520the%2520corrupted%2520one.%2520In%2520addition%252C%2520to%2520highlight%2520the%2520informative%250Avisual-anchored%2520tokens%252C%2520a%2520visual-aware%2520training%2520objective%2520is%2520proposed%2520to%250Aenhance%2520more%2520accurate%2520token-level%2520optimization.%2520Extensive%2520experimental%2520results%250Ahave%2520manifested%2520the%2520state-of-the-art%2520performance%2520of%2520the%2520proposed%2520TPO.%2520For%250Aexample%252C%2520by%2520building%2520on%2520top%2520of%2520LLAVA-1.5-7B%252C%2520our%2520TPO%2520boosts%2520the%2520performance%250Aabsolute%2520improvement%2520for%2520hallucination%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14487v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Preference%20Optimization%20with%20Self-Calibrated%20Visual-Anchored%0A%20%20Rewards%20for%20Hallucination%20Mitigation&entry.906535625=Jihao%20Gu%20and%20Yingyao%20Wang%20and%20Meng%20Cao%20and%20Pi%20Bu%20and%20Jun%20Song%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Bo%20Zheng&entry.1292438233=%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20been%20demonstrated%20to%20be%20highly%0Aeffective%20in%20mitigating%20hallucinations%20in%20Large%20Vision%20Language%20Models%20%28LVLMs%29%0Aby%20aligning%20their%20outputs%20more%20closely%20with%20human%20preferences.%20Despite%20the%0Arecent%20progress%2C%20existing%20methods%20suffer%20from%20two%20drawbacks%3A%201%29%20Lack%20of%0Ascalable%20token-level%20rewards%3B%20and%202%29%20Neglect%20of%20visual-anchored%20tokens.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20Token%20Preference%20Optimization%20model%20with%0Aself-calibrated%20rewards%20%28dubbed%20as%20TPO%29%2C%20which%20adaptively%20attends%20to%0Avisual-correlated%20tokens%20without%20fine-grained%20annotations.%20Specifically%2C%20we%0Aintroduce%20a%20token-level%20%5Cemph%7Bvisual-anchored%7D%20%5Cemph%7Breward%7D%20as%20the%20difference%0Aof%20the%20logistic%20distributions%20of%20generated%20tokens%20conditioned%20on%20the%20raw%20image%0Aand%20the%20corrupted%20one.%20In%20addition%2C%20to%20highlight%20the%20informative%0Avisual-anchored%20tokens%2C%20a%20visual-aware%20training%20objective%20is%20proposed%20to%0Aenhance%20more%20accurate%20token-level%20optimization.%20Extensive%20experimental%20results%0Ahave%20manifested%20the%20state-of-the-art%20performance%20of%20the%20proposed%20TPO.%20For%0Aexample%2C%20by%20building%20on%20top%20of%20LLAVA-1.5-7B%2C%20our%20TPO%20boosts%20the%20performance%0Aabsolute%20improvement%20for%20hallucination%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14487v4&entry.124074799=Read"},
{"title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation", "author": "Sherwin Bahmani and Tianchang Shen and Jiawei Ren and Jiahui Huang and Yifeng Jiang and Haithem Turki and Andrea Tagliasacchi and David B. Lindell and Zan Gojcic and Sanja Fidler and Huan Ling and Jun Gao and Xuanchi Ren", "abstract": "  The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.\n", "link": "http://arxiv.org/abs/2509.19296v1", "date": "2025-09-23", "relevancy": 2.6981, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6887}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6717}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lyra%3A%20Generative%203D%20Scene%20Reconstruction%20via%20Video%20Diffusion%20Model%0A%20%20Self-Distillation&body=Title%3A%20Lyra%3A%20Generative%203D%20Scene%20Reconstruction%20via%20Video%20Diffusion%20Model%0A%20%20Self-Distillation%0AAuthor%3A%20Sherwin%20Bahmani%20and%20Tianchang%20Shen%20and%20Jiawei%20Ren%20and%20Jiahui%20Huang%20and%20Yifeng%20Jiang%20and%20Haithem%20Turki%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell%20and%20Zan%20Gojcic%20and%20Sanja%20Fidler%20and%20Huan%20Ling%20and%20Jun%20Gao%20and%20Xuanchi%20Ren%0AAbstract%3A%20%20%20The%20ability%20to%20generate%20virtual%20environments%20is%20crucial%20for%20applications%0Aranging%20from%20gaming%20to%20physical%20AI%20domains%20such%20as%20robotics%2C%20autonomous%0Adriving%2C%20and%20industrial%20AI.%20Current%20learning-based%203D%20reconstruction%20methods%0Arely%20on%20the%20availability%20of%20captured%20real-world%20multi-view%20data%2C%20which%20is%20not%0Aalways%20readily%20available.%20Recent%20advancements%20in%20video%20diffusion%20models%20have%0Ashown%20remarkable%20imagination%20capabilities%2C%20yet%20their%202D%20nature%20limits%20the%0Aapplications%20to%20simulation%20where%20a%20robot%20needs%20to%20navigate%20and%20interact%20with%0Athe%20environment.%20In%20this%20paper%2C%20we%20propose%20a%20self-distillation%20framework%20that%0Aaims%20to%20distill%20the%20implicit%203D%20knowledge%20in%20the%20video%20diffusion%20models%20into%20an%0Aexplicit%203D%20Gaussian%20Splatting%20%283DGS%29%20representation%2C%20eliminating%20the%20need%20for%0Amulti-view%20training%20data.%20Specifically%2C%20we%20augment%20the%20typical%20RGB%20decoder%20with%0Aa%203DGS%20decoder%2C%20which%20is%20supervised%20by%20the%20output%20of%20the%20RGB%20decoder.%20In%20this%0Aapproach%2C%20the%203DGS%20decoder%20can%20be%20purely%20trained%20with%20synthetic%20data%20generated%0Aby%20video%20diffusion%20models.%20At%20inference%20time%2C%20our%20model%20can%20synthesize%203D%0Ascenes%20from%20either%20a%20text%20prompt%20or%20a%20single%20image%20for%20real-time%20rendering.%20Our%0Aframework%20further%20extends%20to%20dynamic%203D%20scene%20generation%20from%20a%20monocular%20input%0Avideo.%20Experimental%20results%20show%20that%20our%20framework%20achieves%20state-of-the-art%0Aperformance%20in%20static%20and%20dynamic%203D%20scene%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLyra%253A%2520Generative%25203D%2520Scene%2520Reconstruction%2520via%2520Video%2520Diffusion%2520Model%250A%2520%2520Self-Distillation%26entry.906535625%3DSherwin%2520Bahmani%2520and%2520Tianchang%2520Shen%2520and%2520Jiawei%2520Ren%2520and%2520Jiahui%2520Huang%2520and%2520Yifeng%2520Jiang%2520and%2520Haithem%2520Turki%2520and%2520Andrea%2520Tagliasacchi%2520and%2520David%2520B.%2520Lindell%2520and%2520Zan%2520Gojcic%2520and%2520Sanja%2520Fidler%2520and%2520Huan%2520Ling%2520and%2520Jun%2520Gao%2520and%2520Xuanchi%2520Ren%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520generate%2520virtual%2520environments%2520is%2520crucial%2520for%2520applications%250Aranging%2520from%2520gaming%2520to%2520physical%2520AI%2520domains%2520such%2520as%2520robotics%252C%2520autonomous%250Adriving%252C%2520and%2520industrial%2520AI.%2520Current%2520learning-based%25203D%2520reconstruction%2520methods%250Arely%2520on%2520the%2520availability%2520of%2520captured%2520real-world%2520multi-view%2520data%252C%2520which%2520is%2520not%250Aalways%2520readily%2520available.%2520Recent%2520advancements%2520in%2520video%2520diffusion%2520models%2520have%250Ashown%2520remarkable%2520imagination%2520capabilities%252C%2520yet%2520their%25202D%2520nature%2520limits%2520the%250Aapplications%2520to%2520simulation%2520where%2520a%2520robot%2520needs%2520to%2520navigate%2520and%2520interact%2520with%250Athe%2520environment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520self-distillation%2520framework%2520that%250Aaims%2520to%2520distill%2520the%2520implicit%25203D%2520knowledge%2520in%2520the%2520video%2520diffusion%2520models%2520into%2520an%250Aexplicit%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520representation%252C%2520eliminating%2520the%2520need%2520for%250Amulti-view%2520training%2520data.%2520Specifically%252C%2520we%2520augment%2520the%2520typical%2520RGB%2520decoder%2520with%250Aa%25203DGS%2520decoder%252C%2520which%2520is%2520supervised%2520by%2520the%2520output%2520of%2520the%2520RGB%2520decoder.%2520In%2520this%250Aapproach%252C%2520the%25203DGS%2520decoder%2520can%2520be%2520purely%2520trained%2520with%2520synthetic%2520data%2520generated%250Aby%2520video%2520diffusion%2520models.%2520At%2520inference%2520time%252C%2520our%2520model%2520can%2520synthesize%25203D%250Ascenes%2520from%2520either%2520a%2520text%2520prompt%2520or%2520a%2520single%2520image%2520for%2520real-time%2520rendering.%2520Our%250Aframework%2520further%2520extends%2520to%2520dynamic%25203D%2520scene%2520generation%2520from%2520a%2520monocular%2520input%250Avideo.%2520Experimental%2520results%2520show%2520that%2520our%2520framework%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520static%2520and%2520dynamic%25203D%2520scene%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lyra%3A%20Generative%203D%20Scene%20Reconstruction%20via%20Video%20Diffusion%20Model%0A%20%20Self-Distillation&entry.906535625=Sherwin%20Bahmani%20and%20Tianchang%20Shen%20and%20Jiawei%20Ren%20and%20Jiahui%20Huang%20and%20Yifeng%20Jiang%20and%20Haithem%20Turki%20and%20Andrea%20Tagliasacchi%20and%20David%20B.%20Lindell%20and%20Zan%20Gojcic%20and%20Sanja%20Fidler%20and%20Huan%20Ling%20and%20Jun%20Gao%20and%20Xuanchi%20Ren&entry.1292438233=%20%20The%20ability%20to%20generate%20virtual%20environments%20is%20crucial%20for%20applications%0Aranging%20from%20gaming%20to%20physical%20AI%20domains%20such%20as%20robotics%2C%20autonomous%0Adriving%2C%20and%20industrial%20AI.%20Current%20learning-based%203D%20reconstruction%20methods%0Arely%20on%20the%20availability%20of%20captured%20real-world%20multi-view%20data%2C%20which%20is%20not%0Aalways%20readily%20available.%20Recent%20advancements%20in%20video%20diffusion%20models%20have%0Ashown%20remarkable%20imagination%20capabilities%2C%20yet%20their%202D%20nature%20limits%20the%0Aapplications%20to%20simulation%20where%20a%20robot%20needs%20to%20navigate%20and%20interact%20with%0Athe%20environment.%20In%20this%20paper%2C%20we%20propose%20a%20self-distillation%20framework%20that%0Aaims%20to%20distill%20the%20implicit%203D%20knowledge%20in%20the%20video%20diffusion%20models%20into%20an%0Aexplicit%203D%20Gaussian%20Splatting%20%283DGS%29%20representation%2C%20eliminating%20the%20need%20for%0Amulti-view%20training%20data.%20Specifically%2C%20we%20augment%20the%20typical%20RGB%20decoder%20with%0Aa%203DGS%20decoder%2C%20which%20is%20supervised%20by%20the%20output%20of%20the%20RGB%20decoder.%20In%20this%0Aapproach%2C%20the%203DGS%20decoder%20can%20be%20purely%20trained%20with%20synthetic%20data%20generated%0Aby%20video%20diffusion%20models.%20At%20inference%20time%2C%20our%20model%20can%20synthesize%203D%0Ascenes%20from%20either%20a%20text%20prompt%20or%20a%20single%20image%20for%20real-time%20rendering.%20Our%0Aframework%20further%20extends%20to%20dynamic%203D%20scene%20generation%20from%20a%20monocular%20input%0Avideo.%20Experimental%20results%20show%20that%20our%20framework%20achieves%20state-of-the-art%0Aperformance%20in%20static%20and%20dynamic%203D%20scene%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19296v1&entry.124074799=Read"},
{"title": "LightThinker: Thinking Step-by-Step Compression", "author": "Jintian Zhang and Yuqi Zhu and Mengshu Sun and Yujie Luo and Shuofei Qiao and Lun Du and Da Zheng and Huajun Chen and Ningyu Zhang", "abstract": "  Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code is released at https://github.com/zjunlp/LightThinker.\n", "link": "http://arxiv.org/abs/2502.15589v2", "date": "2025-09-23", "relevancy": 2.6571, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightThinker%3A%20Thinking%20Step-by-Step%20Compression&body=Title%3A%20LightThinker%3A%20Thinking%20Step-by-Step%20Compression%0AAuthor%3A%20Jintian%20Zhang%20and%20Yuqi%20Zhu%20and%20Mengshu%20Sun%20and%20Yujie%20Luo%20and%20Shuofei%20Qiao%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20in%20complex%0Areasoning%20tasks%2C%20but%20their%20efficiency%20is%20hindered%20by%20the%20substantial%20memory%20and%0Acomputational%20costs%20associated%20with%20generating%20lengthy%20tokens.%20In%20this%20paper%2C%0Awe%20propose%20LightThinker%2C%20a%20novel%20method%20that%20enables%20LLMs%20to%20dynamically%0Acompress%20intermediate%20thoughts%20during%20reasoning.%20Inspired%20by%20human%20cognitive%0Aprocesses%2C%20LightThinker%20compresses%20verbose%20thought%20steps%20into%20compact%0Arepresentations%20and%20discards%20the%20original%20reasoning%20chains%2C%20thereby%0Asignificantly%20reducing%20the%20number%20of%20tokens%20stored%20in%20the%20context%20window.%20This%0Ais%20achieved%20by%20training%20the%20model%20on%20when%20and%20how%20to%20perform%20compression%0Athrough%20data%20construction%2C%20mapping%20hidden%20states%20to%20condensed%20gist%20tokens%2C%20and%0Acreating%20specialized%20attention%20masks.%20Additionally%2C%20we%20introduce%20the%20Dependency%0A%28Dep%29%20metric%20to%20quantify%20the%20degree%20of%20compression%20by%20measuring%20the%20reliance%20on%0Ahistorical%20tokens%20during%20generation.%20Extensive%20experiments%20on%20four%20datasets%20and%0Atwo%20models%20show%20that%20LightThinker%20reduces%20peak%20memory%20usage%20and%20inference%20time%2C%0Awhile%20maintaining%20competitive%20accuracy.%20Our%20work%20provides%20a%20new%20direction%20for%0Aimproving%20the%20efficiency%20of%20LLMs%20in%20complex%20reasoning%20tasks%20without%20sacrificing%0Aperformance.%20Code%20is%20released%20at%20https%3A//github.com/zjunlp/LightThinker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15589v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightThinker%253A%2520Thinking%2520Step-by-Step%2520Compression%26entry.906535625%3DJintian%2520Zhang%2520and%2520Yuqi%2520Zhu%2520and%2520Mengshu%2520Sun%2520and%2520Yujie%2520Luo%2520and%2520Shuofei%2520Qiao%2520and%2520Lun%2520Du%2520and%2520Da%2520Zheng%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520in%2520complex%250Areasoning%2520tasks%252C%2520but%2520their%2520efficiency%2520is%2520hindered%2520by%2520the%2520substantial%2520memory%2520and%250Acomputational%2520costs%2520associated%2520with%2520generating%2520lengthy%2520tokens.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520LightThinker%252C%2520a%2520novel%2520method%2520that%2520enables%2520LLMs%2520to%2520dynamically%250Acompress%2520intermediate%2520thoughts%2520during%2520reasoning.%2520Inspired%2520by%2520human%2520cognitive%250Aprocesses%252C%2520LightThinker%2520compresses%2520verbose%2520thought%2520steps%2520into%2520compact%250Arepresentations%2520and%2520discards%2520the%2520original%2520reasoning%2520chains%252C%2520thereby%250Asignificantly%2520reducing%2520the%2520number%2520of%2520tokens%2520stored%2520in%2520the%2520context%2520window.%2520This%250Ais%2520achieved%2520by%2520training%2520the%2520model%2520on%2520when%2520and%2520how%2520to%2520perform%2520compression%250Athrough%2520data%2520construction%252C%2520mapping%2520hidden%2520states%2520to%2520condensed%2520gist%2520tokens%252C%2520and%250Acreating%2520specialized%2520attention%2520masks.%2520Additionally%252C%2520we%2520introduce%2520the%2520Dependency%250A%2528Dep%2529%2520metric%2520to%2520quantify%2520the%2520degree%2520of%2520compression%2520by%2520measuring%2520the%2520reliance%2520on%250Ahistorical%2520tokens%2520during%2520generation.%2520Extensive%2520experiments%2520on%2520four%2520datasets%2520and%250Atwo%2520models%2520show%2520that%2520LightThinker%2520reduces%2520peak%2520memory%2520usage%2520and%2520inference%2520time%252C%250Awhile%2520maintaining%2520competitive%2520accuracy.%2520Our%2520work%2520provides%2520a%2520new%2520direction%2520for%250Aimproving%2520the%2520efficiency%2520of%2520LLMs%2520in%2520complex%2520reasoning%2520tasks%2520without%2520sacrificing%250Aperformance.%2520Code%2520is%2520released%2520at%2520https%253A//github.com/zjunlp/LightThinker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15589v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightThinker%3A%20Thinking%20Step-by-Step%20Compression&entry.906535625=Jintian%20Zhang%20and%20Yuqi%20Zhu%20and%20Mengshu%20Sun%20and%20Yujie%20Luo%20and%20Shuofei%20Qiao%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20in%20complex%0Areasoning%20tasks%2C%20but%20their%20efficiency%20is%20hindered%20by%20the%20substantial%20memory%20and%0Acomputational%20costs%20associated%20with%20generating%20lengthy%20tokens.%20In%20this%20paper%2C%0Awe%20propose%20LightThinker%2C%20a%20novel%20method%20that%20enables%20LLMs%20to%20dynamically%0Acompress%20intermediate%20thoughts%20during%20reasoning.%20Inspired%20by%20human%20cognitive%0Aprocesses%2C%20LightThinker%20compresses%20verbose%20thought%20steps%20into%20compact%0Arepresentations%20and%20discards%20the%20original%20reasoning%20chains%2C%20thereby%0Asignificantly%20reducing%20the%20number%20of%20tokens%20stored%20in%20the%20context%20window.%20This%0Ais%20achieved%20by%20training%20the%20model%20on%20when%20and%20how%20to%20perform%20compression%0Athrough%20data%20construction%2C%20mapping%20hidden%20states%20to%20condensed%20gist%20tokens%2C%20and%0Acreating%20specialized%20attention%20masks.%20Additionally%2C%20we%20introduce%20the%20Dependency%0A%28Dep%29%20metric%20to%20quantify%20the%20degree%20of%20compression%20by%20measuring%20the%20reliance%20on%0Ahistorical%20tokens%20during%20generation.%20Extensive%20experiments%20on%20four%20datasets%20and%0Atwo%20models%20show%20that%20LightThinker%20reduces%20peak%20memory%20usage%20and%20inference%20time%2C%0Awhile%20maintaining%20competitive%20accuracy.%20Our%20work%20provides%20a%20new%20direction%20for%0Aimproving%20the%20efficiency%20of%20LLMs%20in%20complex%20reasoning%20tasks%20without%20sacrificing%0Aperformance.%20Code%20is%20released%20at%20https%3A//github.com/zjunlp/LightThinker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15589v2&entry.124074799=Read"},
{"title": "SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic\n  Spines", "author": "Pamela Osuna-Vargas and Altug Kamacioglu and Dominik F. Aschauer and Petros E. Vlachos and Sercan Alipek and Jochen Triesch and Simon Rumpel and Matthias Kaschube", "abstract": "  Dendritic spines are key structural components of excitatory synapses in the\nbrain. Given the size of dendritic spines provides a proxy for synaptic\nefficacy, their detection and tracking across time is important for studies of\nthe neural basis of learning and memory. Despite their relevance, large-scale\nanalyses of the structural dynamics of dendritic spines in 3D+time microscopy\ndata remain challenging and labor-intense. Here, we present a modular machine\nlearning-based pipeline designed to automate the detection, time-tracking, and\nfeature extraction of dendritic spines in volumes chronically recorded with\ntwo-photon microscopy. Our approach tackles the challenges posed by biological\ndata by combining a transformer-based detection module, a depth-tracking\ncomponent that integrates spatial features, a time-tracking module to associate\n3D spines across time by leveraging spatial consistency, and a feature\nextraction unit that quantifies biologically relevant spine properties. We\nvalidate our method on open-source labeled spine data, and on two complementary\nannotated datasets that we publish alongside this work: one for detection and\ndepth-tracking, and one for time-tracking, which, to the best of our knowledge,\nis the first data of this kind. To encourage future research, we release our\ndata, code, and pre-trained weights at\nhttps://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,\nend-to-end analysis of dendritic spine dynamics.\n", "link": "http://arxiv.org/abs/2509.18926v1", "date": "2025-09-23", "relevancy": 2.6376, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5222}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynapFlow%3A%20A%20Modular%20Framework%20Towards%20Large-Scale%20Analysis%20of%20Dendritic%0A%20%20Spines&body=Title%3A%20SynapFlow%3A%20A%20Modular%20Framework%20Towards%20Large-Scale%20Analysis%20of%20Dendritic%0A%20%20Spines%0AAuthor%3A%20Pamela%20Osuna-Vargas%20and%20Altug%20Kamacioglu%20and%20Dominik%20F.%20Aschauer%20and%20Petros%20E.%20Vlachos%20and%20Sercan%20Alipek%20and%20Jochen%20Triesch%20and%20Simon%20Rumpel%20and%20Matthias%20Kaschube%0AAbstract%3A%20%20%20Dendritic%20spines%20are%20key%20structural%20components%20of%20excitatory%20synapses%20in%20the%0Abrain.%20Given%20the%20size%20of%20dendritic%20spines%20provides%20a%20proxy%20for%20synaptic%0Aefficacy%2C%20their%20detection%20and%20tracking%20across%20time%20is%20important%20for%20studies%20of%0Athe%20neural%20basis%20of%20learning%20and%20memory.%20Despite%20their%20relevance%2C%20large-scale%0Aanalyses%20of%20the%20structural%20dynamics%20of%20dendritic%20spines%20in%203D%2Btime%20microscopy%0Adata%20remain%20challenging%20and%20labor-intense.%20Here%2C%20we%20present%20a%20modular%20machine%0Alearning-based%20pipeline%20designed%20to%20automate%20the%20detection%2C%20time-tracking%2C%20and%0Afeature%20extraction%20of%20dendritic%20spines%20in%20volumes%20chronically%20recorded%20with%0Atwo-photon%20microscopy.%20Our%20approach%20tackles%20the%20challenges%20posed%20by%20biological%0Adata%20by%20combining%20a%20transformer-based%20detection%20module%2C%20a%20depth-tracking%0Acomponent%20that%20integrates%20spatial%20features%2C%20a%20time-tracking%20module%20to%20associate%0A3D%20spines%20across%20time%20by%20leveraging%20spatial%20consistency%2C%20and%20a%20feature%0Aextraction%20unit%20that%20quantifies%20biologically%20relevant%20spine%20properties.%20We%0Avalidate%20our%20method%20on%20open-source%20labeled%20spine%20data%2C%20and%20on%20two%20complementary%0Aannotated%20datasets%20that%20we%20publish%20alongside%20this%20work%3A%20one%20for%20detection%20and%0Adepth-tracking%2C%20and%20one%20for%20time-tracking%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%0Ais%20the%20first%20data%20of%20this%20kind.%20To%20encourage%20future%20research%2C%20we%20release%20our%0Adata%2C%20code%2C%20and%20pre-trained%20weights%20at%0Ahttps%3A//github.com/pamelaosuna/SynapFlow%2C%20establishing%20a%20baseline%20for%20scalable%2C%0Aend-to-end%20analysis%20of%20dendritic%20spine%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynapFlow%253A%2520A%2520Modular%2520Framework%2520Towards%2520Large-Scale%2520Analysis%2520of%2520Dendritic%250A%2520%2520Spines%26entry.906535625%3DPamela%2520Osuna-Vargas%2520and%2520Altug%2520Kamacioglu%2520and%2520Dominik%2520F.%2520Aschauer%2520and%2520Petros%2520E.%2520Vlachos%2520and%2520Sercan%2520Alipek%2520and%2520Jochen%2520Triesch%2520and%2520Simon%2520Rumpel%2520and%2520Matthias%2520Kaschube%26entry.1292438233%3D%2520%2520Dendritic%2520spines%2520are%2520key%2520structural%2520components%2520of%2520excitatory%2520synapses%2520in%2520the%250Abrain.%2520Given%2520the%2520size%2520of%2520dendritic%2520spines%2520provides%2520a%2520proxy%2520for%2520synaptic%250Aefficacy%252C%2520their%2520detection%2520and%2520tracking%2520across%2520time%2520is%2520important%2520for%2520studies%2520of%250Athe%2520neural%2520basis%2520of%2520learning%2520and%2520memory.%2520Despite%2520their%2520relevance%252C%2520large-scale%250Aanalyses%2520of%2520the%2520structural%2520dynamics%2520of%2520dendritic%2520spines%2520in%25203D%252Btime%2520microscopy%250Adata%2520remain%2520challenging%2520and%2520labor-intense.%2520Here%252C%2520we%2520present%2520a%2520modular%2520machine%250Alearning-based%2520pipeline%2520designed%2520to%2520automate%2520the%2520detection%252C%2520time-tracking%252C%2520and%250Afeature%2520extraction%2520of%2520dendritic%2520spines%2520in%2520volumes%2520chronically%2520recorded%2520with%250Atwo-photon%2520microscopy.%2520Our%2520approach%2520tackles%2520the%2520challenges%2520posed%2520by%2520biological%250Adata%2520by%2520combining%2520a%2520transformer-based%2520detection%2520module%252C%2520a%2520depth-tracking%250Acomponent%2520that%2520integrates%2520spatial%2520features%252C%2520a%2520time-tracking%2520module%2520to%2520associate%250A3D%2520spines%2520across%2520time%2520by%2520leveraging%2520spatial%2520consistency%252C%2520and%2520a%2520feature%250Aextraction%2520unit%2520that%2520quantifies%2520biologically%2520relevant%2520spine%2520properties.%2520We%250Avalidate%2520our%2520method%2520on%2520open-source%2520labeled%2520spine%2520data%252C%2520and%2520on%2520two%2520complementary%250Aannotated%2520datasets%2520that%2520we%2520publish%2520alongside%2520this%2520work%253A%2520one%2520for%2520detection%2520and%250Adepth-tracking%252C%2520and%2520one%2520for%2520time-tracking%252C%2520which%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%250Ais%2520the%2520first%2520data%2520of%2520this%2520kind.%2520To%2520encourage%2520future%2520research%252C%2520we%2520release%2520our%250Adata%252C%2520code%252C%2520and%2520pre-trained%2520weights%2520at%250Ahttps%253A//github.com/pamelaosuna/SynapFlow%252C%2520establishing%2520a%2520baseline%2520for%2520scalable%252C%250Aend-to-end%2520analysis%2520of%2520dendritic%2520spine%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynapFlow%3A%20A%20Modular%20Framework%20Towards%20Large-Scale%20Analysis%20of%20Dendritic%0A%20%20Spines&entry.906535625=Pamela%20Osuna-Vargas%20and%20Altug%20Kamacioglu%20and%20Dominik%20F.%20Aschauer%20and%20Petros%20E.%20Vlachos%20and%20Sercan%20Alipek%20and%20Jochen%20Triesch%20and%20Simon%20Rumpel%20and%20Matthias%20Kaschube&entry.1292438233=%20%20Dendritic%20spines%20are%20key%20structural%20components%20of%20excitatory%20synapses%20in%20the%0Abrain.%20Given%20the%20size%20of%20dendritic%20spines%20provides%20a%20proxy%20for%20synaptic%0Aefficacy%2C%20their%20detection%20and%20tracking%20across%20time%20is%20important%20for%20studies%20of%0Athe%20neural%20basis%20of%20learning%20and%20memory.%20Despite%20their%20relevance%2C%20large-scale%0Aanalyses%20of%20the%20structural%20dynamics%20of%20dendritic%20spines%20in%203D%2Btime%20microscopy%0Adata%20remain%20challenging%20and%20labor-intense.%20Here%2C%20we%20present%20a%20modular%20machine%0Alearning-based%20pipeline%20designed%20to%20automate%20the%20detection%2C%20time-tracking%2C%20and%0Afeature%20extraction%20of%20dendritic%20spines%20in%20volumes%20chronically%20recorded%20with%0Atwo-photon%20microscopy.%20Our%20approach%20tackles%20the%20challenges%20posed%20by%20biological%0Adata%20by%20combining%20a%20transformer-based%20detection%20module%2C%20a%20depth-tracking%0Acomponent%20that%20integrates%20spatial%20features%2C%20a%20time-tracking%20module%20to%20associate%0A3D%20spines%20across%20time%20by%20leveraging%20spatial%20consistency%2C%20and%20a%20feature%0Aextraction%20unit%20that%20quantifies%20biologically%20relevant%20spine%20properties.%20We%0Avalidate%20our%20method%20on%20open-source%20labeled%20spine%20data%2C%20and%20on%20two%20complementary%0Aannotated%20datasets%20that%20we%20publish%20alongside%20this%20work%3A%20one%20for%20detection%20and%0Adepth-tracking%2C%20and%20one%20for%20time-tracking%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%0Ais%20the%20first%20data%20of%20this%20kind.%20To%20encourage%20future%20research%2C%20we%20release%20our%0Adata%2C%20code%2C%20and%20pre-trained%20weights%20at%0Ahttps%3A//github.com/pamelaosuna/SynapFlow%2C%20establishing%20a%20baseline%20for%20scalable%2C%0Aend-to-end%20analysis%20of%20dendritic%20spine%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18926v1&entry.124074799=Read"},
{"title": "Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM\n  and FastTOM Datasets", "author": "Nikolaos Dionelis and Riccardo Musto and Jente Bosmans and Simone Sarti and Giancarlo Paoletti and S\u00e9bastien Lef\u00e8vre and Bertrand Le Saux and Nicolas Long\u00e9p\u00e9", "abstract": "  Today, Earth Observation (EO) satellites generate massive volumes of data. To\nfully exploit this, it is essential to pretrain EO Foundation Models (FMs) on\nlarge unlabeled datasets, enabling efficient fine-tuning for downstream tasks\nwith minimal labeled data. In this paper, we study scaling-up FMs: we train our\nmodels on the pretraining dataset MajorTOM 23TB which includes all regions, and\nthe performance on average is competitive versus models pretrained on more\nspecialized datasets which are substantially smaller and include only land. The\nadditional data of oceans and ice do not decrease the performance on\nland-focused downstream tasks. These results indicate that large FMs trained on\nglobal datasets for a wider variety of downstream tasks can be useful for\ndownstream applications that only require a subset of the information included\nin their training. The second contribution is the exploration of U-Net\nConvolutional Neural Network (CNN), Vision Transformers (ViT), and Mamba\nState-Space Models (SSM) as FMs. U-Net captures local correlations amongst\npixels, while ViT and Mamba capture local and distant correlations. We develop\nvarious models using different architectures, including U-Net, ViT, and Mamba,\nand different number of parameters. We evaluate the FLoating-point OPerations\n(FLOPs) needed by the models. We fine-tune on the PhilEO Bench for different\ndownstream tasks: roads, buildings, and land cover. For most n-shots for roads\nand buildings, U-Net 200M-2T outperforms the other models. Using Mamba, we\nachieve comparable results on the downstream tasks, with less computational\nexpenses. We also compare with the recent FM TerraMind which we evaluate on\nPhilEO Bench.\n", "link": "http://arxiv.org/abs/2506.14765v4", "date": "2025-09-23", "relevancy": 2.6349, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5381}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Earth%20Observation%20Foundation%20Model%20PhilEO%3A%20Pretraining%20on%20the%20MajorTOM%0A%20%20and%20FastTOM%20Datasets&body=Title%3A%20Earth%20Observation%20Foundation%20Model%20PhilEO%3A%20Pretraining%20on%20the%20MajorTOM%0A%20%20and%20FastTOM%20Datasets%0AAuthor%3A%20Nikolaos%20Dionelis%20and%20Riccardo%20Musto%20and%20Jente%20Bosmans%20and%20Simone%20Sarti%20and%20Giancarlo%20Paoletti%20and%20S%C3%A9bastien%20Lef%C3%A8vre%20and%20Bertrand%20Le%20Saux%20and%20Nicolas%20Long%C3%A9p%C3%A9%0AAbstract%3A%20%20%20Today%2C%20Earth%20Observation%20%28EO%29%20satellites%20generate%20massive%20volumes%20of%20data.%20To%0Afully%20exploit%20this%2C%20it%20is%20essential%20to%20pretrain%20EO%20Foundation%20Models%20%28FMs%29%20on%0Alarge%20unlabeled%20datasets%2C%20enabling%20efficient%20fine-tuning%20for%20downstream%20tasks%0Awith%20minimal%20labeled%20data.%20In%20this%20paper%2C%20we%20study%20scaling-up%20FMs%3A%20we%20train%20our%0Amodels%20on%20the%20pretraining%20dataset%20MajorTOM%2023TB%20which%20includes%20all%20regions%2C%20and%0Athe%20performance%20on%20average%20is%20competitive%20versus%20models%20pretrained%20on%20more%0Aspecialized%20datasets%20which%20are%20substantially%20smaller%20and%20include%20only%20land.%20The%0Aadditional%20data%20of%20oceans%20and%20ice%20do%20not%20decrease%20the%20performance%20on%0Aland-focused%20downstream%20tasks.%20These%20results%20indicate%20that%20large%20FMs%20trained%20on%0Aglobal%20datasets%20for%20a%20wider%20variety%20of%20downstream%20tasks%20can%20be%20useful%20for%0Adownstream%20applications%20that%20only%20require%20a%20subset%20of%20the%20information%20included%0Ain%20their%20training.%20The%20second%20contribution%20is%20the%20exploration%20of%20U-Net%0AConvolutional%20Neural%20Network%20%28CNN%29%2C%20Vision%20Transformers%20%28ViT%29%2C%20and%20Mamba%0AState-Space%20Models%20%28SSM%29%20as%20FMs.%20U-Net%20captures%20local%20correlations%20amongst%0Apixels%2C%20while%20ViT%20and%20Mamba%20capture%20local%20and%20distant%20correlations.%20We%20develop%0Avarious%20models%20using%20different%20architectures%2C%20including%20U-Net%2C%20ViT%2C%20and%20Mamba%2C%0Aand%20different%20number%20of%20parameters.%20We%20evaluate%20the%20FLoating-point%20OPerations%0A%28FLOPs%29%20needed%20by%20the%20models.%20We%20fine-tune%20on%20the%20PhilEO%20Bench%20for%20different%0Adownstream%20tasks%3A%20roads%2C%20buildings%2C%20and%20land%20cover.%20For%20most%20n-shots%20for%20roads%0Aand%20buildings%2C%20U-Net%20200M-2T%20outperforms%20the%20other%20models.%20Using%20Mamba%2C%20we%0Aachieve%20comparable%20results%20on%20the%20downstream%20tasks%2C%20with%20less%20computational%0Aexpenses.%20We%20also%20compare%20with%20the%20recent%20FM%20TerraMind%20which%20we%20evaluate%20on%0APhilEO%20Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14765v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarth%2520Observation%2520Foundation%2520Model%2520PhilEO%253A%2520Pretraining%2520on%2520the%2520MajorTOM%250A%2520%2520and%2520FastTOM%2520Datasets%26entry.906535625%3DNikolaos%2520Dionelis%2520and%2520Riccardo%2520Musto%2520and%2520Jente%2520Bosmans%2520and%2520Simone%2520Sarti%2520and%2520Giancarlo%2520Paoletti%2520and%2520S%25C3%25A9bastien%2520Lef%25C3%25A8vre%2520and%2520Bertrand%2520Le%2520Saux%2520and%2520Nicolas%2520Long%25C3%25A9p%25C3%25A9%26entry.1292438233%3D%2520%2520Today%252C%2520Earth%2520Observation%2520%2528EO%2529%2520satellites%2520generate%2520massive%2520volumes%2520of%2520data.%2520To%250Afully%2520exploit%2520this%252C%2520it%2520is%2520essential%2520to%2520pretrain%2520EO%2520Foundation%2520Models%2520%2528FMs%2529%2520on%250Alarge%2520unlabeled%2520datasets%252C%2520enabling%2520efficient%2520fine-tuning%2520for%2520downstream%2520tasks%250Awith%2520minimal%2520labeled%2520data.%2520In%2520this%2520paper%252C%2520we%2520study%2520scaling-up%2520FMs%253A%2520we%2520train%2520our%250Amodels%2520on%2520the%2520pretraining%2520dataset%2520MajorTOM%252023TB%2520which%2520includes%2520all%2520regions%252C%2520and%250Athe%2520performance%2520on%2520average%2520is%2520competitive%2520versus%2520models%2520pretrained%2520on%2520more%250Aspecialized%2520datasets%2520which%2520are%2520substantially%2520smaller%2520and%2520include%2520only%2520land.%2520The%250Aadditional%2520data%2520of%2520oceans%2520and%2520ice%2520do%2520not%2520decrease%2520the%2520performance%2520on%250Aland-focused%2520downstream%2520tasks.%2520These%2520results%2520indicate%2520that%2520large%2520FMs%2520trained%2520on%250Aglobal%2520datasets%2520for%2520a%2520wider%2520variety%2520of%2520downstream%2520tasks%2520can%2520be%2520useful%2520for%250Adownstream%2520applications%2520that%2520only%2520require%2520a%2520subset%2520of%2520the%2520information%2520included%250Ain%2520their%2520training.%2520The%2520second%2520contribution%2520is%2520the%2520exploration%2520of%2520U-Net%250AConvolutional%2520Neural%2520Network%2520%2528CNN%2529%252C%2520Vision%2520Transformers%2520%2528ViT%2529%252C%2520and%2520Mamba%250AState-Space%2520Models%2520%2528SSM%2529%2520as%2520FMs.%2520U-Net%2520captures%2520local%2520correlations%2520amongst%250Apixels%252C%2520while%2520ViT%2520and%2520Mamba%2520capture%2520local%2520and%2520distant%2520correlations.%2520We%2520develop%250Avarious%2520models%2520using%2520different%2520architectures%252C%2520including%2520U-Net%252C%2520ViT%252C%2520and%2520Mamba%252C%250Aand%2520different%2520number%2520of%2520parameters.%2520We%2520evaluate%2520the%2520FLoating-point%2520OPerations%250A%2528FLOPs%2529%2520needed%2520by%2520the%2520models.%2520We%2520fine-tune%2520on%2520the%2520PhilEO%2520Bench%2520for%2520different%250Adownstream%2520tasks%253A%2520roads%252C%2520buildings%252C%2520and%2520land%2520cover.%2520For%2520most%2520n-shots%2520for%2520roads%250Aand%2520buildings%252C%2520U-Net%2520200M-2T%2520outperforms%2520the%2520other%2520models.%2520Using%2520Mamba%252C%2520we%250Aachieve%2520comparable%2520results%2520on%2520the%2520downstream%2520tasks%252C%2520with%2520less%2520computational%250Aexpenses.%2520We%2520also%2520compare%2520with%2520the%2520recent%2520FM%2520TerraMind%2520which%2520we%2520evaluate%2520on%250APhilEO%2520Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14765v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Earth%20Observation%20Foundation%20Model%20PhilEO%3A%20Pretraining%20on%20the%20MajorTOM%0A%20%20and%20FastTOM%20Datasets&entry.906535625=Nikolaos%20Dionelis%20and%20Riccardo%20Musto%20and%20Jente%20Bosmans%20and%20Simone%20Sarti%20and%20Giancarlo%20Paoletti%20and%20S%C3%A9bastien%20Lef%C3%A8vre%20and%20Bertrand%20Le%20Saux%20and%20Nicolas%20Long%C3%A9p%C3%A9&entry.1292438233=%20%20Today%2C%20Earth%20Observation%20%28EO%29%20satellites%20generate%20massive%20volumes%20of%20data.%20To%0Afully%20exploit%20this%2C%20it%20is%20essential%20to%20pretrain%20EO%20Foundation%20Models%20%28FMs%29%20on%0Alarge%20unlabeled%20datasets%2C%20enabling%20efficient%20fine-tuning%20for%20downstream%20tasks%0Awith%20minimal%20labeled%20data.%20In%20this%20paper%2C%20we%20study%20scaling-up%20FMs%3A%20we%20train%20our%0Amodels%20on%20the%20pretraining%20dataset%20MajorTOM%2023TB%20which%20includes%20all%20regions%2C%20and%0Athe%20performance%20on%20average%20is%20competitive%20versus%20models%20pretrained%20on%20more%0Aspecialized%20datasets%20which%20are%20substantially%20smaller%20and%20include%20only%20land.%20The%0Aadditional%20data%20of%20oceans%20and%20ice%20do%20not%20decrease%20the%20performance%20on%0Aland-focused%20downstream%20tasks.%20These%20results%20indicate%20that%20large%20FMs%20trained%20on%0Aglobal%20datasets%20for%20a%20wider%20variety%20of%20downstream%20tasks%20can%20be%20useful%20for%0Adownstream%20applications%20that%20only%20require%20a%20subset%20of%20the%20information%20included%0Ain%20their%20training.%20The%20second%20contribution%20is%20the%20exploration%20of%20U-Net%0AConvolutional%20Neural%20Network%20%28CNN%29%2C%20Vision%20Transformers%20%28ViT%29%2C%20and%20Mamba%0AState-Space%20Models%20%28SSM%29%20as%20FMs.%20U-Net%20captures%20local%20correlations%20amongst%0Apixels%2C%20while%20ViT%20and%20Mamba%20capture%20local%20and%20distant%20correlations.%20We%20develop%0Avarious%20models%20using%20different%20architectures%2C%20including%20U-Net%2C%20ViT%2C%20and%20Mamba%2C%0Aand%20different%20number%20of%20parameters.%20We%20evaluate%20the%20FLoating-point%20OPerations%0A%28FLOPs%29%20needed%20by%20the%20models.%20We%20fine-tune%20on%20the%20PhilEO%20Bench%20for%20different%0Adownstream%20tasks%3A%20roads%2C%20buildings%2C%20and%20land%20cover.%20For%20most%20n-shots%20for%20roads%0Aand%20buildings%2C%20U-Net%20200M-2T%20outperforms%20the%20other%20models.%20Using%20Mamba%2C%20we%0Aachieve%20comparable%20results%20on%20the%20downstream%20tasks%2C%20with%20less%20computational%0Aexpenses.%20We%20also%20compare%20with%20the%20recent%20FM%20TerraMind%20which%20we%20evaluate%20on%0APhilEO%20Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14765v4&entry.124074799=Read"},
{"title": "Gaussian Process Diffeomorphic Statistical Shape Modelling Outperforms\n  Angle-Based Methods for Assessment of Hip Dysplasia", "author": "Allen Paul and George Grammatopoulos and Adwaye Rambojun and Neill D. F. Campbell and Harinderjit S. Gill and Tony Shardlow", "abstract": "  Dysplasia is a recognised risk factor for osteoarthritis (OA) of the hip,\nearly diagnosis of dysplasia is important to provide opportunities for surgical\ninterventions aimed at reducing the risk of hip OA. We have developed a\npipeline for semi-automated classification of dysplasia using volumetric CT\nscans of patients' hips and a minimal set of clinically annotated landmarks,\ncombining the framework of the Gaussian Process Latent Variable Model with\ndiffeomorphism to create a statistical shape model, which we termed the\nGaussian Process Diffeomorphic Statistical Shape Model (GPDSSM). We used 192 CT\nscans, 100 for model training and 92 for testing. The GPDSSM effectively\ndistinguishes dysplastic samples from controls while also highlighting regions\nof the underlying surface that show dysplastic variations. As well as improving\nclassification accuracy compared to angle-based methods (AUC 96.2% vs 91.2%),\nthe GPDSSM can save time for clinicians by removing the need to manually\nmeasure angles and interpreting 2D scans for possible markers of dysplasia.\n", "link": "http://arxiv.org/abs/2506.04886v2", "date": "2025-09-23", "relevancy": 2.6176, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5605}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5207}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Process%20Diffeomorphic%20Statistical%20Shape%20Modelling%20Outperforms%0A%20%20Angle-Based%20Methods%20for%20Assessment%20of%20Hip%20Dysplasia&body=Title%3A%20Gaussian%20Process%20Diffeomorphic%20Statistical%20Shape%20Modelling%20Outperforms%0A%20%20Angle-Based%20Methods%20for%20Assessment%20of%20Hip%20Dysplasia%0AAuthor%3A%20Allen%20Paul%20and%20George%20Grammatopoulos%20and%20Adwaye%20Rambojun%20and%20Neill%20D.%20F.%20Campbell%20and%20Harinderjit%20S.%20Gill%20and%20Tony%20Shardlow%0AAbstract%3A%20%20%20Dysplasia%20is%20a%20recognised%20risk%20factor%20for%20osteoarthritis%20%28OA%29%20of%20the%20hip%2C%0Aearly%20diagnosis%20of%20dysplasia%20is%20important%20to%20provide%20opportunities%20for%20surgical%0Ainterventions%20aimed%20at%20reducing%20the%20risk%20of%20hip%20OA.%20We%20have%20developed%20a%0Apipeline%20for%20semi-automated%20classification%20of%20dysplasia%20using%20volumetric%20CT%0Ascans%20of%20patients%27%20hips%20and%20a%20minimal%20set%20of%20clinically%20annotated%20landmarks%2C%0Acombining%20the%20framework%20of%20the%20Gaussian%20Process%20Latent%20Variable%20Model%20with%0Adiffeomorphism%20to%20create%20a%20statistical%20shape%20model%2C%20which%20we%20termed%20the%0AGaussian%20Process%20Diffeomorphic%20Statistical%20Shape%20Model%20%28GPDSSM%29.%20We%20used%20192%20CT%0Ascans%2C%20100%20for%20model%20training%20and%2092%20for%20testing.%20The%20GPDSSM%20effectively%0Adistinguishes%20dysplastic%20samples%20from%20controls%20while%20also%20highlighting%20regions%0Aof%20the%20underlying%20surface%20that%20show%20dysplastic%20variations.%20As%20well%20as%20improving%0Aclassification%20accuracy%20compared%20to%20angle-based%20methods%20%28AUC%2096.2%25%20vs%2091.2%25%29%2C%0Athe%20GPDSSM%20can%20save%20time%20for%20clinicians%20by%20removing%20the%20need%20to%20manually%0Ameasure%20angles%20and%20interpreting%202D%20scans%20for%20possible%20markers%20of%20dysplasia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04886v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Process%2520Diffeomorphic%2520Statistical%2520Shape%2520Modelling%2520Outperforms%250A%2520%2520Angle-Based%2520Methods%2520for%2520Assessment%2520of%2520Hip%2520Dysplasia%26entry.906535625%3DAllen%2520Paul%2520and%2520George%2520Grammatopoulos%2520and%2520Adwaye%2520Rambojun%2520and%2520Neill%2520D.%2520F.%2520Campbell%2520and%2520Harinderjit%2520S.%2520Gill%2520and%2520Tony%2520Shardlow%26entry.1292438233%3D%2520%2520Dysplasia%2520is%2520a%2520recognised%2520risk%2520factor%2520for%2520osteoarthritis%2520%2528OA%2529%2520of%2520the%2520hip%252C%250Aearly%2520diagnosis%2520of%2520dysplasia%2520is%2520important%2520to%2520provide%2520opportunities%2520for%2520surgical%250Ainterventions%2520aimed%2520at%2520reducing%2520the%2520risk%2520of%2520hip%2520OA.%2520We%2520have%2520developed%2520a%250Apipeline%2520for%2520semi-automated%2520classification%2520of%2520dysplasia%2520using%2520volumetric%2520CT%250Ascans%2520of%2520patients%2527%2520hips%2520and%2520a%2520minimal%2520set%2520of%2520clinically%2520annotated%2520landmarks%252C%250Acombining%2520the%2520framework%2520of%2520the%2520Gaussian%2520Process%2520Latent%2520Variable%2520Model%2520with%250Adiffeomorphism%2520to%2520create%2520a%2520statistical%2520shape%2520model%252C%2520which%2520we%2520termed%2520the%250AGaussian%2520Process%2520Diffeomorphic%2520Statistical%2520Shape%2520Model%2520%2528GPDSSM%2529.%2520We%2520used%2520192%2520CT%250Ascans%252C%2520100%2520for%2520model%2520training%2520and%252092%2520for%2520testing.%2520The%2520GPDSSM%2520effectively%250Adistinguishes%2520dysplastic%2520samples%2520from%2520controls%2520while%2520also%2520highlighting%2520regions%250Aof%2520the%2520underlying%2520surface%2520that%2520show%2520dysplastic%2520variations.%2520As%2520well%2520as%2520improving%250Aclassification%2520accuracy%2520compared%2520to%2520angle-based%2520methods%2520%2528AUC%252096.2%2525%2520vs%252091.2%2525%2529%252C%250Athe%2520GPDSSM%2520can%2520save%2520time%2520for%2520clinicians%2520by%2520removing%2520the%2520need%2520to%2520manually%250Ameasure%2520angles%2520and%2520interpreting%25202D%2520scans%2520for%2520possible%2520markers%2520of%2520dysplasia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04886v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Process%20Diffeomorphic%20Statistical%20Shape%20Modelling%20Outperforms%0A%20%20Angle-Based%20Methods%20for%20Assessment%20of%20Hip%20Dysplasia&entry.906535625=Allen%20Paul%20and%20George%20Grammatopoulos%20and%20Adwaye%20Rambojun%20and%20Neill%20D.%20F.%20Campbell%20and%20Harinderjit%20S.%20Gill%20and%20Tony%20Shardlow&entry.1292438233=%20%20Dysplasia%20is%20a%20recognised%20risk%20factor%20for%20osteoarthritis%20%28OA%29%20of%20the%20hip%2C%0Aearly%20diagnosis%20of%20dysplasia%20is%20important%20to%20provide%20opportunities%20for%20surgical%0Ainterventions%20aimed%20at%20reducing%20the%20risk%20of%20hip%20OA.%20We%20have%20developed%20a%0Apipeline%20for%20semi-automated%20classification%20of%20dysplasia%20using%20volumetric%20CT%0Ascans%20of%20patients%27%20hips%20and%20a%20minimal%20set%20of%20clinically%20annotated%20landmarks%2C%0Acombining%20the%20framework%20of%20the%20Gaussian%20Process%20Latent%20Variable%20Model%20with%0Adiffeomorphism%20to%20create%20a%20statistical%20shape%20model%2C%20which%20we%20termed%20the%0AGaussian%20Process%20Diffeomorphic%20Statistical%20Shape%20Model%20%28GPDSSM%29.%20We%20used%20192%20CT%0Ascans%2C%20100%20for%20model%20training%20and%2092%20for%20testing.%20The%20GPDSSM%20effectively%0Adistinguishes%20dysplastic%20samples%20from%20controls%20while%20also%20highlighting%20regions%0Aof%20the%20underlying%20surface%20that%20show%20dysplastic%20variations.%20As%20well%20as%20improving%0Aclassification%20accuracy%20compared%20to%20angle-based%20methods%20%28AUC%2096.2%25%20vs%2091.2%25%29%2C%0Athe%20GPDSSM%20can%20save%20time%20for%20clinicians%20by%20removing%20the%20need%20to%20manually%0Ameasure%20angles%20and%20interpreting%202D%20scans%20for%20possible%20markers%20of%20dysplasia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04886v2&entry.124074799=Read"},
{"title": "Weakly Supervised Food Image Segmentation using Vision Transformers and\n  Segment Anything Model", "author": "Ioannis Sarafis and Alexandros Papadopoulos and Anastasios Delopoulos", "abstract": "  In this paper, we propose a weakly supervised semantic segmentation approach\nfor food images which takes advantage of the zero-shot capabilities and\npromptability of the Segment Anything Model (SAM) along with the attention\nmechanisms of Vision Transformers (ViTs). Specifically, we use class activation\nmaps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable\nfor food image segmentation. The ViT model, a Swin Transformer, is trained\nexclusively using image-level annotations, eliminating the need for pixel-level\nannotations during training. Additionally, to enhance the quality of the\nSAM-generated masks, we examine the use of image preprocessing techniques in\ncombination with single-mask and multi-mask SAM generation strategies. The\nmethodology is evaluated on the FoodSeg103 dataset, generating an average of\n2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for\nthe multi-mask scenario. We envision the proposed approach as a tool to\naccelerate food image annotation tasks or as an integrated component in food\nand nutrition tracking applications.\n", "link": "http://arxiv.org/abs/2509.19028v1", "date": "2025-09-23", "relevancy": 2.5949, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5271}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5224}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Food%20Image%20Segmentation%20using%20Vision%20Transformers%20and%0A%20%20Segment%20Anything%20Model&body=Title%3A%20Weakly%20Supervised%20Food%20Image%20Segmentation%20using%20Vision%20Transformers%20and%0A%20%20Segment%20Anything%20Model%0AAuthor%3A%20Ioannis%20Sarafis%20and%20Alexandros%20Papadopoulos%20and%20Anastasios%20Delopoulos%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20weakly%20supervised%20semantic%20segmentation%20approach%0Afor%20food%20images%20which%20takes%20advantage%20of%20the%20zero-shot%20capabilities%20and%0Apromptability%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20along%20with%20the%20attention%0Amechanisms%20of%20Vision%20Transformers%20%28ViTs%29.%20Specifically%2C%20we%20use%20class%20activation%0Amaps%20%28CAMs%29%20from%20ViTs%20to%20generate%20prompts%20for%20SAM%2C%20resulting%20in%20masks%20suitable%0Afor%20food%20image%20segmentation.%20The%20ViT%20model%2C%20a%20Swin%20Transformer%2C%20is%20trained%0Aexclusively%20using%20image-level%20annotations%2C%20eliminating%20the%20need%20for%20pixel-level%0Aannotations%20during%20training.%20Additionally%2C%20to%20enhance%20the%20quality%20of%20the%0ASAM-generated%20masks%2C%20we%20examine%20the%20use%20of%20image%20preprocessing%20techniques%20in%0Acombination%20with%20single-mask%20and%20multi-mask%20SAM%20generation%20strategies.%20The%0Amethodology%20is%20evaluated%20on%20the%20FoodSeg103%20dataset%2C%20generating%20an%20average%20of%0A2.4%20masks%20per%20image%20%28excluding%20background%29%2C%20and%20achieving%20an%20mIoU%20of%200.54%20for%0Athe%20multi-mask%20scenario.%20We%20envision%20the%20proposed%20approach%20as%20a%20tool%20to%0Aaccelerate%20food%20image%20annotation%20tasks%20or%20as%20an%20integrated%20component%20in%20food%0Aand%20nutrition%20tracking%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Food%2520Image%2520Segmentation%2520using%2520Vision%2520Transformers%2520and%250A%2520%2520Segment%2520Anything%2520Model%26entry.906535625%3DIoannis%2520Sarafis%2520and%2520Alexandros%2520Papadopoulos%2520and%2520Anastasios%2520Delopoulos%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520weakly%2520supervised%2520semantic%2520segmentation%2520approach%250Afor%2520food%2520images%2520which%2520takes%2520advantage%2520of%2520the%2520zero-shot%2520capabilities%2520and%250Apromptability%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520along%2520with%2520the%2520attention%250Amechanisms%2520of%2520Vision%2520Transformers%2520%2528ViTs%2529.%2520Specifically%252C%2520we%2520use%2520class%2520activation%250Amaps%2520%2528CAMs%2529%2520from%2520ViTs%2520to%2520generate%2520prompts%2520for%2520SAM%252C%2520resulting%2520in%2520masks%2520suitable%250Afor%2520food%2520image%2520segmentation.%2520The%2520ViT%2520model%252C%2520a%2520Swin%2520Transformer%252C%2520is%2520trained%250Aexclusively%2520using%2520image-level%2520annotations%252C%2520eliminating%2520the%2520need%2520for%2520pixel-level%250Aannotations%2520during%2520training.%2520Additionally%252C%2520to%2520enhance%2520the%2520quality%2520of%2520the%250ASAM-generated%2520masks%252C%2520we%2520examine%2520the%2520use%2520of%2520image%2520preprocessing%2520techniques%2520in%250Acombination%2520with%2520single-mask%2520and%2520multi-mask%2520SAM%2520generation%2520strategies.%2520The%250Amethodology%2520is%2520evaluated%2520on%2520the%2520FoodSeg103%2520dataset%252C%2520generating%2520an%2520average%2520of%250A2.4%2520masks%2520per%2520image%2520%2528excluding%2520background%2529%252C%2520and%2520achieving%2520an%2520mIoU%2520of%25200.54%2520for%250Athe%2520multi-mask%2520scenario.%2520We%2520envision%2520the%2520proposed%2520approach%2520as%2520a%2520tool%2520to%250Aaccelerate%2520food%2520image%2520annotation%2520tasks%2520or%2520as%2520an%2520integrated%2520component%2520in%2520food%250Aand%2520nutrition%2520tracking%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Food%20Image%20Segmentation%20using%20Vision%20Transformers%20and%0A%20%20Segment%20Anything%20Model&entry.906535625=Ioannis%20Sarafis%20and%20Alexandros%20Papadopoulos%20and%20Anastasios%20Delopoulos&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20weakly%20supervised%20semantic%20segmentation%20approach%0Afor%20food%20images%20which%20takes%20advantage%20of%20the%20zero-shot%20capabilities%20and%0Apromptability%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20along%20with%20the%20attention%0Amechanisms%20of%20Vision%20Transformers%20%28ViTs%29.%20Specifically%2C%20we%20use%20class%20activation%0Amaps%20%28CAMs%29%20from%20ViTs%20to%20generate%20prompts%20for%20SAM%2C%20resulting%20in%20masks%20suitable%0Afor%20food%20image%20segmentation.%20The%20ViT%20model%2C%20a%20Swin%20Transformer%2C%20is%20trained%0Aexclusively%20using%20image-level%20annotations%2C%20eliminating%20the%20need%20for%20pixel-level%0Aannotations%20during%20training.%20Additionally%2C%20to%20enhance%20the%20quality%20of%20the%0ASAM-generated%20masks%2C%20we%20examine%20the%20use%20of%20image%20preprocessing%20techniques%20in%0Acombination%20with%20single-mask%20and%20multi-mask%20SAM%20generation%20strategies.%20The%0Amethodology%20is%20evaluated%20on%20the%20FoodSeg103%20dataset%2C%20generating%20an%20average%20of%0A2.4%20masks%20per%20image%20%28excluding%20background%29%2C%20and%20achieving%20an%20mIoU%20of%200.54%20for%0Athe%20multi-mask%20scenario.%20We%20envision%20the%20proposed%20approach%20as%20a%20tool%20to%0Aaccelerate%20food%20image%20annotation%20tasks%20or%20as%20an%20integrated%20component%20in%20food%0Aand%20nutrition%20tracking%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19028v1&entry.124074799=Read"},
{"title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust\n  Deblurring", "author": "Pengteng Li and Yunfan Lu and Pinhao Song and Weiyu Guo and Huizai Yao and F. Richard Yu and Hui Xiong", "abstract": "  In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS.\n", "link": "http://arxiv.org/abs/2509.18898v1", "date": "2025-09-23", "relevancy": 2.5792, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6832}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6297}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeblurSplat%3A%20SfM-free%203D%20Gaussian%20Splatting%20with%20Event%20Camera%20for%20Robust%0A%20%20Deblurring&body=Title%3A%20DeblurSplat%3A%20SfM-free%203D%20Gaussian%20Splatting%20with%20Event%20Camera%20for%20Robust%0A%20%20Deblurring%0AAuthor%3A%20Pengteng%20Li%20and%20Yunfan%20Lu%20and%20Pinhao%20Song%20and%20Weiyu%20Guo%20and%20Huizai%20Yao%20and%20F.%20Richard%20Yu%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20the%20first%20Structure-from-Motion%20%28SfM%29-free%0Adeblurring%203D%20Gaussian%20Splatting%20method%20via%20event%20camera%2C%20dubbed%20DeblurSplat.%0AWe%20address%20the%20motion-deblurring%20problem%20in%20two%20ways.%20First%2C%20we%20leverage%20the%0Apretrained%20capability%20of%20the%20dense%20stereo%20module%20%28DUSt3R%29%20to%20directly%20obtain%0Aaccurate%20initial%20point%20clouds%20from%20blurred%20images.%20Without%20calculating%20camera%0Aposes%20as%20an%20intermediate%20result%2C%20we%20avoid%20the%20cumulative%20errors%20transfer%20from%0Ainaccurate%20camera%20poses%20to%20the%20initial%20point%20clouds%27%20positions.%20Second%2C%20we%0Aintroduce%20the%20event%20stream%20into%20the%20deblur%20pipeline%20for%20its%20high%20sensitivity%20to%0Adynamic%20change.%20By%20decoding%20the%20latent%20sharp%20images%20from%20the%20event%20stream%20and%0Ablurred%20images%2C%20we%20can%20provide%20a%20fine-grained%20supervision%20signal%20for%20scene%0Areconstruction%20optimization.%20Extensive%20experiments%20across%20a%20range%20of%20scenes%0Ademonstrate%20that%20DeblurSplat%20not%20only%20excels%20in%20generating%20high-fidelity%20novel%0Aviews%20but%20also%20achieves%20significant%20rendering%20efficiency%20compared%20to%20the%20SOTAs%0Ain%20deblur%203D-GS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeblurSplat%253A%2520SfM-free%25203D%2520Gaussian%2520Splatting%2520with%2520Event%2520Camera%2520for%2520Robust%250A%2520%2520Deblurring%26entry.906535625%3DPengteng%2520Li%2520and%2520Yunfan%2520Lu%2520and%2520Pinhao%2520Song%2520and%2520Weiyu%2520Guo%2520and%2520Huizai%2520Yao%2520and%2520F.%2520Richard%2520Yu%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%2520Structure-from-Motion%2520%2528SfM%2529-free%250Adeblurring%25203D%2520Gaussian%2520Splatting%2520method%2520via%2520event%2520camera%252C%2520dubbed%2520DeblurSplat.%250AWe%2520address%2520the%2520motion-deblurring%2520problem%2520in%2520two%2520ways.%2520First%252C%2520we%2520leverage%2520the%250Apretrained%2520capability%2520of%2520the%2520dense%2520stereo%2520module%2520%2528DUSt3R%2529%2520to%2520directly%2520obtain%250Aaccurate%2520initial%2520point%2520clouds%2520from%2520blurred%2520images.%2520Without%2520calculating%2520camera%250Aposes%2520as%2520an%2520intermediate%2520result%252C%2520we%2520avoid%2520the%2520cumulative%2520errors%2520transfer%2520from%250Ainaccurate%2520camera%2520poses%2520to%2520the%2520initial%2520point%2520clouds%2527%2520positions.%2520Second%252C%2520we%250Aintroduce%2520the%2520event%2520stream%2520into%2520the%2520deblur%2520pipeline%2520for%2520its%2520high%2520sensitivity%2520to%250Adynamic%2520change.%2520By%2520decoding%2520the%2520latent%2520sharp%2520images%2520from%2520the%2520event%2520stream%2520and%250Ablurred%2520images%252C%2520we%2520can%2520provide%2520a%2520fine-grained%2520supervision%2520signal%2520for%2520scene%250Areconstruction%2520optimization.%2520Extensive%2520experiments%2520across%2520a%2520range%2520of%2520scenes%250Ademonstrate%2520that%2520DeblurSplat%2520not%2520only%2520excels%2520in%2520generating%2520high-fidelity%2520novel%250Aviews%2520but%2520also%2520achieves%2520significant%2520rendering%2520efficiency%2520compared%2520to%2520the%2520SOTAs%250Ain%2520deblur%25203D-GS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeblurSplat%3A%20SfM-free%203D%20Gaussian%20Splatting%20with%20Event%20Camera%20for%20Robust%0A%20%20Deblurring&entry.906535625=Pengteng%20Li%20and%20Yunfan%20Lu%20and%20Pinhao%20Song%20and%20Weiyu%20Guo%20and%20Huizai%20Yao%20and%20F.%20Richard%20Yu%20and%20Hui%20Xiong&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20the%20first%20Structure-from-Motion%20%28SfM%29-free%0Adeblurring%203D%20Gaussian%20Splatting%20method%20via%20event%20camera%2C%20dubbed%20DeblurSplat.%0AWe%20address%20the%20motion-deblurring%20problem%20in%20two%20ways.%20First%2C%20we%20leverage%20the%0Apretrained%20capability%20of%20the%20dense%20stereo%20module%20%28DUSt3R%29%20to%20directly%20obtain%0Aaccurate%20initial%20point%20clouds%20from%20blurred%20images.%20Without%20calculating%20camera%0Aposes%20as%20an%20intermediate%20result%2C%20we%20avoid%20the%20cumulative%20errors%20transfer%20from%0Ainaccurate%20camera%20poses%20to%20the%20initial%20point%20clouds%27%20positions.%20Second%2C%20we%0Aintroduce%20the%20event%20stream%20into%20the%20deblur%20pipeline%20for%20its%20high%20sensitivity%20to%0Adynamic%20change.%20By%20decoding%20the%20latent%20sharp%20images%20from%20the%20event%20stream%20and%0Ablurred%20images%2C%20we%20can%20provide%20a%20fine-grained%20supervision%20signal%20for%20scene%0Areconstruction%20optimization.%20Extensive%20experiments%20across%20a%20range%20of%20scenes%0Ademonstrate%20that%20DeblurSplat%20not%20only%20excels%20in%20generating%20high-fidelity%20novel%0Aviews%20but%20also%20achieves%20significant%20rendering%20efficiency%20compared%20to%20the%20SOTAs%0Ain%20deblur%203D-GS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18898v1&entry.124074799=Read"},
{"title": "CalFuse: Feature Calibration Enhanced Parameter Fusion for\n  Class-Continual Learning", "author": "Juncen Guo and Siao Liu and Xiaoguang Zhu and Lianlong Sun and Liangyu Teng and Jingyi Wu and Di Li and Linxiao Gong and Weiwei Jiang and Wei Zhou and Ahmed Ghoneim and Liang Song", "abstract": "  Class-Continual Learning (CCL) enables models to continuously learn new class\nknowledge while retaining previous classes, facilitating adaptation and\nevolution in dynamic, real-world environments. Traditional CCL methods\nprimarily rely on visual features, which limits their effectiveness in complex,\nmultimodal scenarios. In contrast, Vision-Language Models (VLMs) show promising\npotential for enhancing CCL by leveraging pre-trained knowledge and fusing\nmulti-modal semantic cues such as text and vision. However, existing approaches\nstruggle to mitigate catastrophic forgetting while preserving the\ngeneralization strengths of VLMs across diverse modalities. To address these\nchallenges, we propose CalFuse, a framework for feature Calibration enhanced\nparameter Fusion, which enhances dynamic knowledge fusion. CalFuse introduces a\ndynamic feature calibration mechanism that iteratively adjusts the contribution\nof original visual features to the final class decision, thereby preserving the\nmodel's intrinsic generalization capability across modalities. Simultaneously,\na parameter fusion strategy effectively fuses newly acquired knowledge with\nprior task parameters, maintaining a balance between acquiring new class\nrepresentations and preserving old knowledge. Experimental results on popular\nbenchmarks (e.g., CIFAR100 and ImageNet100) validate the superiority of the\nproposed method.\n", "link": "http://arxiv.org/abs/2503.18672v7", "date": "2025-09-23", "relevancy": 2.5718, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CalFuse%3A%20Feature%20Calibration%20Enhanced%20Parameter%20Fusion%20for%0A%20%20Class-Continual%20Learning&body=Title%3A%20CalFuse%3A%20Feature%20Calibration%20Enhanced%20Parameter%20Fusion%20for%0A%20%20Class-Continual%20Learning%0AAuthor%3A%20Juncen%20Guo%20and%20Siao%20Liu%20and%20Xiaoguang%20Zhu%20and%20Lianlong%20Sun%20and%20Liangyu%20Teng%20and%20Jingyi%20Wu%20and%20Di%20Li%20and%20Linxiao%20Gong%20and%20Weiwei%20Jiang%20and%20Wei%20Zhou%20and%20Ahmed%20Ghoneim%20and%20Liang%20Song%0AAbstract%3A%20%20%20Class-Continual%20Learning%20%28CCL%29%20enables%20models%20to%20continuously%20learn%20new%20class%0Aknowledge%20while%20retaining%20previous%20classes%2C%20facilitating%20adaptation%20and%0Aevolution%20in%20dynamic%2C%20real-world%20environments.%20Traditional%20CCL%20methods%0Aprimarily%20rely%20on%20visual%20features%2C%20which%20limits%20their%20effectiveness%20in%20complex%2C%0Amultimodal%20scenarios.%20In%20contrast%2C%20Vision-Language%20Models%20%28VLMs%29%20show%20promising%0Apotential%20for%20enhancing%20CCL%20by%20leveraging%20pre-trained%20knowledge%20and%20fusing%0Amulti-modal%20semantic%20cues%20such%20as%20text%20and%20vision.%20However%2C%20existing%20approaches%0Astruggle%20to%20mitigate%20catastrophic%20forgetting%20while%20preserving%20the%0Ageneralization%20strengths%20of%20VLMs%20across%20diverse%20modalities.%20To%20address%20these%0Achallenges%2C%20we%20propose%20CalFuse%2C%20a%20framework%20for%20feature%20Calibration%20enhanced%0Aparameter%20Fusion%2C%20which%20enhances%20dynamic%20knowledge%20fusion.%20CalFuse%20introduces%20a%0Adynamic%20feature%20calibration%20mechanism%20that%20iteratively%20adjusts%20the%20contribution%0Aof%20original%20visual%20features%20to%20the%20final%20class%20decision%2C%20thereby%20preserving%20the%0Amodel%27s%20intrinsic%20generalization%20capability%20across%20modalities.%20Simultaneously%2C%0Aa%20parameter%20fusion%20strategy%20effectively%20fuses%20newly%20acquired%20knowledge%20with%0Aprior%20task%20parameters%2C%20maintaining%20a%20balance%20between%20acquiring%20new%20class%0Arepresentations%20and%20preserving%20old%20knowledge.%20Experimental%20results%20on%20popular%0Abenchmarks%20%28e.g.%2C%20CIFAR100%20and%20ImageNet100%29%20validate%20the%20superiority%20of%20the%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18672v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalFuse%253A%2520Feature%2520Calibration%2520Enhanced%2520Parameter%2520Fusion%2520for%250A%2520%2520Class-Continual%2520Learning%26entry.906535625%3DJuncen%2520Guo%2520and%2520Siao%2520Liu%2520and%2520Xiaoguang%2520Zhu%2520and%2520Lianlong%2520Sun%2520and%2520Liangyu%2520Teng%2520and%2520Jingyi%2520Wu%2520and%2520Di%2520Li%2520and%2520Linxiao%2520Gong%2520and%2520Weiwei%2520Jiang%2520and%2520Wei%2520Zhou%2520and%2520Ahmed%2520Ghoneim%2520and%2520Liang%2520Song%26entry.1292438233%3D%2520%2520Class-Continual%2520Learning%2520%2528CCL%2529%2520enables%2520models%2520to%2520continuously%2520learn%2520new%2520class%250Aknowledge%2520while%2520retaining%2520previous%2520classes%252C%2520facilitating%2520adaptation%2520and%250Aevolution%2520in%2520dynamic%252C%2520real-world%2520environments.%2520Traditional%2520CCL%2520methods%250Aprimarily%2520rely%2520on%2520visual%2520features%252C%2520which%2520limits%2520their%2520effectiveness%2520in%2520complex%252C%250Amultimodal%2520scenarios.%2520In%2520contrast%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520show%2520promising%250Apotential%2520for%2520enhancing%2520CCL%2520by%2520leveraging%2520pre-trained%2520knowledge%2520and%2520fusing%250Amulti-modal%2520semantic%2520cues%2520such%2520as%2520text%2520and%2520vision.%2520However%252C%2520existing%2520approaches%250Astruggle%2520to%2520mitigate%2520catastrophic%2520forgetting%2520while%2520preserving%2520the%250Ageneralization%2520strengths%2520of%2520VLMs%2520across%2520diverse%2520modalities.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520CalFuse%252C%2520a%2520framework%2520for%2520feature%2520Calibration%2520enhanced%250Aparameter%2520Fusion%252C%2520which%2520enhances%2520dynamic%2520knowledge%2520fusion.%2520CalFuse%2520introduces%2520a%250Adynamic%2520feature%2520calibration%2520mechanism%2520that%2520iteratively%2520adjusts%2520the%2520contribution%250Aof%2520original%2520visual%2520features%2520to%2520the%2520final%2520class%2520decision%252C%2520thereby%2520preserving%2520the%250Amodel%2527s%2520intrinsic%2520generalization%2520capability%2520across%2520modalities.%2520Simultaneously%252C%250Aa%2520parameter%2520fusion%2520strategy%2520effectively%2520fuses%2520newly%2520acquired%2520knowledge%2520with%250Aprior%2520task%2520parameters%252C%2520maintaining%2520a%2520balance%2520between%2520acquiring%2520new%2520class%250Arepresentations%2520and%2520preserving%2520old%2520knowledge.%2520Experimental%2520results%2520on%2520popular%250Abenchmarks%2520%2528e.g.%252C%2520CIFAR100%2520and%2520ImageNet100%2529%2520validate%2520the%2520superiority%2520of%2520the%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18672v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CalFuse%3A%20Feature%20Calibration%20Enhanced%20Parameter%20Fusion%20for%0A%20%20Class-Continual%20Learning&entry.906535625=Juncen%20Guo%20and%20Siao%20Liu%20and%20Xiaoguang%20Zhu%20and%20Lianlong%20Sun%20and%20Liangyu%20Teng%20and%20Jingyi%20Wu%20and%20Di%20Li%20and%20Linxiao%20Gong%20and%20Weiwei%20Jiang%20and%20Wei%20Zhou%20and%20Ahmed%20Ghoneim%20and%20Liang%20Song&entry.1292438233=%20%20Class-Continual%20Learning%20%28CCL%29%20enables%20models%20to%20continuously%20learn%20new%20class%0Aknowledge%20while%20retaining%20previous%20classes%2C%20facilitating%20adaptation%20and%0Aevolution%20in%20dynamic%2C%20real-world%20environments.%20Traditional%20CCL%20methods%0Aprimarily%20rely%20on%20visual%20features%2C%20which%20limits%20their%20effectiveness%20in%20complex%2C%0Amultimodal%20scenarios.%20In%20contrast%2C%20Vision-Language%20Models%20%28VLMs%29%20show%20promising%0Apotential%20for%20enhancing%20CCL%20by%20leveraging%20pre-trained%20knowledge%20and%20fusing%0Amulti-modal%20semantic%20cues%20such%20as%20text%20and%20vision.%20However%2C%20existing%20approaches%0Astruggle%20to%20mitigate%20catastrophic%20forgetting%20while%20preserving%20the%0Ageneralization%20strengths%20of%20VLMs%20across%20diverse%20modalities.%20To%20address%20these%0Achallenges%2C%20we%20propose%20CalFuse%2C%20a%20framework%20for%20feature%20Calibration%20enhanced%0Aparameter%20Fusion%2C%20which%20enhances%20dynamic%20knowledge%20fusion.%20CalFuse%20introduces%20a%0Adynamic%20feature%20calibration%20mechanism%20that%20iteratively%20adjusts%20the%20contribution%0Aof%20original%20visual%20features%20to%20the%20final%20class%20decision%2C%20thereby%20preserving%20the%0Amodel%27s%20intrinsic%20generalization%20capability%20across%20modalities.%20Simultaneously%2C%0Aa%20parameter%20fusion%20strategy%20effectively%20fuses%20newly%20acquired%20knowledge%20with%0Aprior%20task%20parameters%2C%20maintaining%20a%20balance%20between%20acquiring%20new%20class%0Arepresentations%20and%20preserving%20old%20knowledge.%20Experimental%20results%20on%20popular%0Abenchmarks%20%28e.g.%2C%20CIFAR100%20and%20ImageNet100%29%20validate%20the%20superiority%20of%20the%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18672v7&entry.124074799=Read"},
{"title": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation\n  in Large Language Models", "author": "Tsz Ting Chung and Lemao Liu and Mo Yu and Dit-Yan Yeung", "abstract": "  Logic reasoning in natural language has been recognized as an important\nmeasure of human intelligence for Large Language Models (LLMs). Popular\nbenchmarks may entangle multiple reasoning skills and thus provide unfaithful\nevaluations on the logic reasoning skill. Meanwhile, existing logic reasoning\nbenchmarks are limited in language diversity and their distributions are\ndeviated from the distribution of an ideal logic reasoning benchmark, which may\nlead to biased evaluation results. This paper thereby proposes a new classical\nlogic benchmark DivLogicEval, consisting of natural sentences composed of\ndiverse statements in a counterintuitive way. To ensure a more reliable\nevaluation, we also introduce a new evaluation metric that mitigates the\ninfluence of bias and randomness inherent in LLMs. Through experiments, we\ndemonstrate the extent to which logical reasoning is required to answer the\nquestions in DivLogicEval and compare the performance of different popular LLMs\nin conducting logical reasoning.\n", "link": "http://arxiv.org/abs/2509.15587v2", "date": "2025-09-23", "relevancy": 2.5516, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DivLogicEval%3A%20A%20Framework%20for%20Benchmarking%20Logical%20Reasoning%20Evaluation%0A%20%20in%20Large%20Language%20Models&body=Title%3A%20DivLogicEval%3A%20A%20Framework%20for%20Benchmarking%20Logical%20Reasoning%20Evaluation%0A%20%20in%20Large%20Language%20Models%0AAuthor%3A%20Tsz%20Ting%20Chung%20and%20Lemao%20Liu%20and%20Mo%20Yu%20and%20Dit-Yan%20Yeung%0AAbstract%3A%20%20%20Logic%20reasoning%20in%20natural%20language%20has%20been%20recognized%20as%20an%20important%0Ameasure%20of%20human%20intelligence%20for%20Large%20Language%20Models%20%28LLMs%29.%20Popular%0Abenchmarks%20may%20entangle%20multiple%20reasoning%20skills%20and%20thus%20provide%20unfaithful%0Aevaluations%20on%20the%20logic%20reasoning%20skill.%20Meanwhile%2C%20existing%20logic%20reasoning%0Abenchmarks%20are%20limited%20in%20language%20diversity%20and%20their%20distributions%20are%0Adeviated%20from%20the%20distribution%20of%20an%20ideal%20logic%20reasoning%20benchmark%2C%20which%20may%0Alead%20to%20biased%20evaluation%20results.%20This%20paper%20thereby%20proposes%20a%20new%20classical%0Alogic%20benchmark%20DivLogicEval%2C%20consisting%20of%20natural%20sentences%20composed%20of%0Adiverse%20statements%20in%20a%20counterintuitive%20way.%20To%20ensure%20a%20more%20reliable%0Aevaluation%2C%20we%20also%20introduce%20a%20new%20evaluation%20metric%20that%20mitigates%20the%0Ainfluence%20of%20bias%20and%20randomness%20inherent%20in%20LLMs.%20Through%20experiments%2C%20we%0Ademonstrate%20the%20extent%20to%20which%20logical%20reasoning%20is%20required%20to%20answer%20the%0Aquestions%20in%20DivLogicEval%20and%20compare%20the%20performance%20of%20different%20popular%20LLMs%0Ain%20conducting%20logical%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivLogicEval%253A%2520A%2520Framework%2520for%2520Benchmarking%2520Logical%2520Reasoning%2520Evaluation%250A%2520%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DTsz%2520Ting%2520Chung%2520and%2520Lemao%2520Liu%2520and%2520Mo%2520Yu%2520and%2520Dit-Yan%2520Yeung%26entry.1292438233%3D%2520%2520Logic%2520reasoning%2520in%2520natural%2520language%2520has%2520been%2520recognized%2520as%2520an%2520important%250Ameasure%2520of%2520human%2520intelligence%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Popular%250Abenchmarks%2520may%2520entangle%2520multiple%2520reasoning%2520skills%2520and%2520thus%2520provide%2520unfaithful%250Aevaluations%2520on%2520the%2520logic%2520reasoning%2520skill.%2520Meanwhile%252C%2520existing%2520logic%2520reasoning%250Abenchmarks%2520are%2520limited%2520in%2520language%2520diversity%2520and%2520their%2520distributions%2520are%250Adeviated%2520from%2520the%2520distribution%2520of%2520an%2520ideal%2520logic%2520reasoning%2520benchmark%252C%2520which%2520may%250Alead%2520to%2520biased%2520evaluation%2520results.%2520This%2520paper%2520thereby%2520proposes%2520a%2520new%2520classical%250Alogic%2520benchmark%2520DivLogicEval%252C%2520consisting%2520of%2520natural%2520sentences%2520composed%2520of%250Adiverse%2520statements%2520in%2520a%2520counterintuitive%2520way.%2520To%2520ensure%2520a%2520more%2520reliable%250Aevaluation%252C%2520we%2520also%2520introduce%2520a%2520new%2520evaluation%2520metric%2520that%2520mitigates%2520the%250Ainfluence%2520of%2520bias%2520and%2520randomness%2520inherent%2520in%2520LLMs.%2520Through%2520experiments%252C%2520we%250Ademonstrate%2520the%2520extent%2520to%2520which%2520logical%2520reasoning%2520is%2520required%2520to%2520answer%2520the%250Aquestions%2520in%2520DivLogicEval%2520and%2520compare%2520the%2520performance%2520of%2520different%2520popular%2520LLMs%250Ain%2520conducting%2520logical%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DivLogicEval%3A%20A%20Framework%20for%20Benchmarking%20Logical%20Reasoning%20Evaluation%0A%20%20in%20Large%20Language%20Models&entry.906535625=Tsz%20Ting%20Chung%20and%20Lemao%20Liu%20and%20Mo%20Yu%20and%20Dit-Yan%20Yeung&entry.1292438233=%20%20Logic%20reasoning%20in%20natural%20language%20has%20been%20recognized%20as%20an%20important%0Ameasure%20of%20human%20intelligence%20for%20Large%20Language%20Models%20%28LLMs%29.%20Popular%0Abenchmarks%20may%20entangle%20multiple%20reasoning%20skills%20and%20thus%20provide%20unfaithful%0Aevaluations%20on%20the%20logic%20reasoning%20skill.%20Meanwhile%2C%20existing%20logic%20reasoning%0Abenchmarks%20are%20limited%20in%20language%20diversity%20and%20their%20distributions%20are%0Adeviated%20from%20the%20distribution%20of%20an%20ideal%20logic%20reasoning%20benchmark%2C%20which%20may%0Alead%20to%20biased%20evaluation%20results.%20This%20paper%20thereby%20proposes%20a%20new%20classical%0Alogic%20benchmark%20DivLogicEval%2C%20consisting%20of%20natural%20sentences%20composed%20of%0Adiverse%20statements%20in%20a%20counterintuitive%20way.%20To%20ensure%20a%20more%20reliable%0Aevaluation%2C%20we%20also%20introduce%20a%20new%20evaluation%20metric%20that%20mitigates%20the%0Ainfluence%20of%20bias%20and%20randomness%20inherent%20in%20LLMs.%20Through%20experiments%2C%20we%0Ademonstrate%20the%20extent%20to%20which%20logical%20reasoning%20is%20required%20to%20answer%20the%0Aquestions%20in%20DivLogicEval%20and%20compare%20the%20performance%20of%20different%20popular%20LLMs%0Ain%20conducting%20logical%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15587v2&entry.124074799=Read"},
{"title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study", "author": "DongGeon Lee and Joonwon Jang and Jihae Jeong and Hwanjo Yu", "abstract": "  Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench.\n", "link": "http://arxiv.org/abs/2505.15389v3", "date": "2025-09-23", "relevancy": 2.5402, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Vision-Language%20Models%20Safe%20in%20the%20Wild%3F%20A%20Meme-Based%20Benchmark%0A%20%20Study&body=Title%3A%20Are%20Vision-Language%20Models%20Safe%20in%20the%20Wild%3F%20A%20Meme-Based%20Benchmark%0A%20%20Study%0AAuthor%3A%20DongGeon%20Lee%20and%20Joonwon%20Jang%20and%20Jihae%20Jeong%20and%20Hwanjo%20Yu%0AAbstract%3A%20%20%20Rapid%20deployment%20of%20vision-language%20models%20%28VLMs%29%20magnifies%20safety%20risks%2C%20yet%0Amost%20evaluations%20rely%20on%20artificial%20images.%20This%20study%20asks%3A%20How%20safe%20are%0Acurrent%20VLMs%20when%20confronted%20with%20meme%20images%20that%20ordinary%20users%20share%3F%20To%0Ainvestigate%20this%20question%2C%20we%20introduce%20MemeSafetyBench%2C%20a%2050%2C430-instance%0Abenchmark%20pairing%20real%20meme%20images%20with%20both%20harmful%20and%20benign%20instructions.%0AUsing%20a%20comprehensive%20safety%20taxonomy%20and%20LLM-based%20instruction%20generation%2C%20we%0Aassess%20multiple%20VLMs%20across%20single%20and%20multi-turn%20interactions.%20We%20investigate%0Ahow%20real-world%20memes%20influence%20harmful%20outputs%2C%20the%20mitigating%20effects%20of%0Aconversational%20context%2C%20and%20the%20relationship%20between%20model%20scale%20and%20safety%0Ametrics.%20Our%20findings%20demonstrate%20that%20VLMs%20are%20more%20vulnerable%20to%20meme-based%0Aharmful%20prompts%20than%20to%20synthetic%20or%20typographic%20images.%20Memes%20significantly%0Aincrease%20harmful%20responses%20and%20decrease%20refusals%20compared%20to%20text-only%20inputs.%0AThough%20multi-turn%20interactions%20provide%20partial%20mitigation%2C%20elevated%0Avulnerability%20persists.%20These%20results%20highlight%20the%20need%20for%20ecologically%20valid%0Aevaluations%20and%20stronger%20safety%20mechanisms.%20MemeSafetyBench%20is%20publicly%0Aavailable%20at%20https%3A//github.com/oneonlee/Meme-Safety-Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15389v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Vision-Language%2520Models%2520Safe%2520in%2520the%2520Wild%253F%2520A%2520Meme-Based%2520Benchmark%250A%2520%2520Study%26entry.906535625%3DDongGeon%2520Lee%2520and%2520Joonwon%2520Jang%2520and%2520Jihae%2520Jeong%2520and%2520Hwanjo%2520Yu%26entry.1292438233%3D%2520%2520Rapid%2520deployment%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520magnifies%2520safety%2520risks%252C%2520yet%250Amost%2520evaluations%2520rely%2520on%2520artificial%2520images.%2520This%2520study%2520asks%253A%2520How%2520safe%2520are%250Acurrent%2520VLMs%2520when%2520confronted%2520with%2520meme%2520images%2520that%2520ordinary%2520users%2520share%253F%2520To%250Ainvestigate%2520this%2520question%252C%2520we%2520introduce%2520MemeSafetyBench%252C%2520a%252050%252C430-instance%250Abenchmark%2520pairing%2520real%2520meme%2520images%2520with%2520both%2520harmful%2520and%2520benign%2520instructions.%250AUsing%2520a%2520comprehensive%2520safety%2520taxonomy%2520and%2520LLM-based%2520instruction%2520generation%252C%2520we%250Aassess%2520multiple%2520VLMs%2520across%2520single%2520and%2520multi-turn%2520interactions.%2520We%2520investigate%250Ahow%2520real-world%2520memes%2520influence%2520harmful%2520outputs%252C%2520the%2520mitigating%2520effects%2520of%250Aconversational%2520context%252C%2520and%2520the%2520relationship%2520between%2520model%2520scale%2520and%2520safety%250Ametrics.%2520Our%2520findings%2520demonstrate%2520that%2520VLMs%2520are%2520more%2520vulnerable%2520to%2520meme-based%250Aharmful%2520prompts%2520than%2520to%2520synthetic%2520or%2520typographic%2520images.%2520Memes%2520significantly%250Aincrease%2520harmful%2520responses%2520and%2520decrease%2520refusals%2520compared%2520to%2520text-only%2520inputs.%250AThough%2520multi-turn%2520interactions%2520provide%2520partial%2520mitigation%252C%2520elevated%250Avulnerability%2520persists.%2520These%2520results%2520highlight%2520the%2520need%2520for%2520ecologically%2520valid%250Aevaluations%2520and%2520stronger%2520safety%2520mechanisms.%2520MemeSafetyBench%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/oneonlee/Meme-Safety-Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15389v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Vision-Language%20Models%20Safe%20in%20the%20Wild%3F%20A%20Meme-Based%20Benchmark%0A%20%20Study&entry.906535625=DongGeon%20Lee%20and%20Joonwon%20Jang%20and%20Jihae%20Jeong%20and%20Hwanjo%20Yu&entry.1292438233=%20%20Rapid%20deployment%20of%20vision-language%20models%20%28VLMs%29%20magnifies%20safety%20risks%2C%20yet%0Amost%20evaluations%20rely%20on%20artificial%20images.%20This%20study%20asks%3A%20How%20safe%20are%0Acurrent%20VLMs%20when%20confronted%20with%20meme%20images%20that%20ordinary%20users%20share%3F%20To%0Ainvestigate%20this%20question%2C%20we%20introduce%20MemeSafetyBench%2C%20a%2050%2C430-instance%0Abenchmark%20pairing%20real%20meme%20images%20with%20both%20harmful%20and%20benign%20instructions.%0AUsing%20a%20comprehensive%20safety%20taxonomy%20and%20LLM-based%20instruction%20generation%2C%20we%0Aassess%20multiple%20VLMs%20across%20single%20and%20multi-turn%20interactions.%20We%20investigate%0Ahow%20real-world%20memes%20influence%20harmful%20outputs%2C%20the%20mitigating%20effects%20of%0Aconversational%20context%2C%20and%20the%20relationship%20between%20model%20scale%20and%20safety%0Ametrics.%20Our%20findings%20demonstrate%20that%20VLMs%20are%20more%20vulnerable%20to%20meme-based%0Aharmful%20prompts%20than%20to%20synthetic%20or%20typographic%20images.%20Memes%20significantly%0Aincrease%20harmful%20responses%20and%20decrease%20refusals%20compared%20to%20text-only%20inputs.%0AThough%20multi-turn%20interactions%20provide%20partial%20mitigation%2C%20elevated%0Avulnerability%20persists.%20These%20results%20highlight%20the%20need%20for%20ecologically%20valid%0Aevaluations%20and%20stronger%20safety%20mechanisms.%20MemeSafetyBench%20is%20publicly%0Aavailable%20at%20https%3A//github.com/oneonlee/Meme-Safety-Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15389v3&entry.124074799=Read"},
{"title": "Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer\n  on Low-resource Languages", "author": "Yuemei Xu and Kexin Xu and Jian Zhou and Ling Hu and Lin Gui", "abstract": "  The current Large Language Models (LLMs) face significant challenges in\nimproving their performance on low-resource languages and urgently need\ndata-efficient methods without costly fine-tuning. From the perspective of\nlanguage-bridge, we propose a simple yet effective method, namely BridgeX-ICL,\nto improve the zero-shot Cross-lingual In-Context Learning (X-ICL) for\nlow-resource languages. Unlike existing works focusing on language-specific\nneurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual\nperformance in LLMs. We construct neuron probe data from the ground-truth MUSE\nbilingual dictionaries, and define a subset of language overlap neurons\naccordingly to ensure full activation of these anchored neurons. Subsequently,\nwe propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum\nbased on overlapping neurons, guiding optimal bridge selection. The experiments\nconducted on 4 cross-lingual tasks and 15 language pairs from 7 diverse\nfamilies, covering both high-low and moderate-low pairs, validate the\neffectiveness of BridgeX-ICL and offer empirical insights into the underlying\nmultilingual mechanisms of LLMs. The code is publicly available at\nhttps://github.com/xuyuemei/BridgeX-ICL.\n", "link": "http://arxiv.org/abs/2508.17078v2", "date": "2025-09-23", "relevancy": 2.5242, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linguistic%20Neuron%20Overlap%20Patterns%20to%20Facilitate%20Cross-lingual%20Transfer%0A%20%20on%20Low-resource%20Languages&body=Title%3A%20Linguistic%20Neuron%20Overlap%20Patterns%20to%20Facilitate%20Cross-lingual%20Transfer%0A%20%20on%20Low-resource%20Languages%0AAuthor%3A%20Yuemei%20Xu%20and%20Kexin%20Xu%20and%20Jian%20Zhou%20and%20Ling%20Hu%20and%20Lin%20Gui%0AAbstract%3A%20%20%20The%20current%20Large%20Language%20Models%20%28LLMs%29%20face%20significant%20challenges%20in%0Aimproving%20their%20performance%20on%20low-resource%20languages%20and%20urgently%20need%0Adata-efficient%20methods%20without%20costly%20fine-tuning.%20From%20the%20perspective%20of%0Alanguage-bridge%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20namely%20BridgeX-ICL%2C%0Ato%20improve%20the%20zero-shot%20Cross-lingual%20In-Context%20Learning%20%28X-ICL%29%20for%0Alow-resource%20languages.%20Unlike%20existing%20works%20focusing%20on%20language-specific%0Aneurons%2C%20BridgeX-ICL%20explores%20whether%20sharing%20neurons%20can%20improve%20cross-lingual%0Aperformance%20in%20LLMs.%20We%20construct%20neuron%20probe%20data%20from%20the%20ground-truth%20MUSE%0Abilingual%20dictionaries%2C%20and%20define%20a%20subset%20of%20language%20overlap%20neurons%0Aaccordingly%20to%20ensure%20full%20activation%20of%20these%20anchored%20neurons.%20Subsequently%2C%0Awe%20propose%20an%20HSIC-based%20metric%20to%20quantify%20LLMs%27%20internal%20linguistic%20spectrum%0Abased%20on%20overlapping%20neurons%2C%20guiding%20optimal%20bridge%20selection.%20The%20experiments%0Aconducted%20on%204%20cross-lingual%20tasks%20and%2015%20language%20pairs%20from%207%20diverse%0Afamilies%2C%20covering%20both%20high-low%20and%20moderate-low%20pairs%2C%20validate%20the%0Aeffectiveness%20of%20BridgeX-ICL%20and%20offer%20empirical%20insights%20into%20the%20underlying%0Amultilingual%20mechanisms%20of%20LLMs.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/xuyuemei/BridgeX-ICL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinguistic%2520Neuron%2520Overlap%2520Patterns%2520to%2520Facilitate%2520Cross-lingual%2520Transfer%250A%2520%2520on%2520Low-resource%2520Languages%26entry.906535625%3DYuemei%2520Xu%2520and%2520Kexin%2520Xu%2520and%2520Jian%2520Zhou%2520and%2520Ling%2520Hu%2520and%2520Lin%2520Gui%26entry.1292438233%3D%2520%2520The%2520current%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520significant%2520challenges%2520in%250Aimproving%2520their%2520performance%2520on%2520low-resource%2520languages%2520and%2520urgently%2520need%250Adata-efficient%2520methods%2520without%2520costly%2520fine-tuning.%2520From%2520the%2520perspective%2520of%250Alanguage-bridge%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520method%252C%2520namely%2520BridgeX-ICL%252C%250Ato%2520improve%2520the%2520zero-shot%2520Cross-lingual%2520In-Context%2520Learning%2520%2528X-ICL%2529%2520for%250Alow-resource%2520languages.%2520Unlike%2520existing%2520works%2520focusing%2520on%2520language-specific%250Aneurons%252C%2520BridgeX-ICL%2520explores%2520whether%2520sharing%2520neurons%2520can%2520improve%2520cross-lingual%250Aperformance%2520in%2520LLMs.%2520We%2520construct%2520neuron%2520probe%2520data%2520from%2520the%2520ground-truth%2520MUSE%250Abilingual%2520dictionaries%252C%2520and%2520define%2520a%2520subset%2520of%2520language%2520overlap%2520neurons%250Aaccordingly%2520to%2520ensure%2520full%2520activation%2520of%2520these%2520anchored%2520neurons.%2520Subsequently%252C%250Awe%2520propose%2520an%2520HSIC-based%2520metric%2520to%2520quantify%2520LLMs%2527%2520internal%2520linguistic%2520spectrum%250Abased%2520on%2520overlapping%2520neurons%252C%2520guiding%2520optimal%2520bridge%2520selection.%2520The%2520experiments%250Aconducted%2520on%25204%2520cross-lingual%2520tasks%2520and%252015%2520language%2520pairs%2520from%25207%2520diverse%250Afamilies%252C%2520covering%2520both%2520high-low%2520and%2520moderate-low%2520pairs%252C%2520validate%2520the%250Aeffectiveness%2520of%2520BridgeX-ICL%2520and%2520offer%2520empirical%2520insights%2520into%2520the%2520underlying%250Amultilingual%2520mechanisms%2520of%2520LLMs.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/xuyuemei/BridgeX-ICL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linguistic%20Neuron%20Overlap%20Patterns%20to%20Facilitate%20Cross-lingual%20Transfer%0A%20%20on%20Low-resource%20Languages&entry.906535625=Yuemei%20Xu%20and%20Kexin%20Xu%20and%20Jian%20Zhou%20and%20Ling%20Hu%20and%20Lin%20Gui&entry.1292438233=%20%20The%20current%20Large%20Language%20Models%20%28LLMs%29%20face%20significant%20challenges%20in%0Aimproving%20their%20performance%20on%20low-resource%20languages%20and%20urgently%20need%0Adata-efficient%20methods%20without%20costly%20fine-tuning.%20From%20the%20perspective%20of%0Alanguage-bridge%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20namely%20BridgeX-ICL%2C%0Ato%20improve%20the%20zero-shot%20Cross-lingual%20In-Context%20Learning%20%28X-ICL%29%20for%0Alow-resource%20languages.%20Unlike%20existing%20works%20focusing%20on%20language-specific%0Aneurons%2C%20BridgeX-ICL%20explores%20whether%20sharing%20neurons%20can%20improve%20cross-lingual%0Aperformance%20in%20LLMs.%20We%20construct%20neuron%20probe%20data%20from%20the%20ground-truth%20MUSE%0Abilingual%20dictionaries%2C%20and%20define%20a%20subset%20of%20language%20overlap%20neurons%0Aaccordingly%20to%20ensure%20full%20activation%20of%20these%20anchored%20neurons.%20Subsequently%2C%0Awe%20propose%20an%20HSIC-based%20metric%20to%20quantify%20LLMs%27%20internal%20linguistic%20spectrum%0Abased%20on%20overlapping%20neurons%2C%20guiding%20optimal%20bridge%20selection.%20The%20experiments%0Aconducted%20on%204%20cross-lingual%20tasks%20and%2015%20language%20pairs%20from%207%20diverse%0Afamilies%2C%20covering%20both%20high-low%20and%20moderate-low%20pairs%2C%20validate%20the%0Aeffectiveness%20of%20BridgeX-ICL%20and%20offer%20empirical%20insights%20into%20the%20underlying%0Amultilingual%20mechanisms%20of%20LLMs.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/xuyuemei/BridgeX-ICL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17078v2&entry.124074799=Read"},
{"title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution", "author": "Dianxing Zhang and Wendong Li and Kani Song and Jiaye Lu and Gang Li and Liuchun Yang and Sheng Li", "abstract": "  Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.\n", "link": "http://arxiv.org/abs/2509.18868v1", "date": "2025-09-23", "relevancy": 2.5222, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20in%20Large%20Language%20Models%3A%20Mechanisms%2C%20Evaluation%20and%20Evolution&body=Title%3A%20Memory%20in%20Large%20Language%20Models%3A%20Mechanisms%2C%20Evaluation%20and%20Evolution%0AAuthor%3A%20Dianxing%20Zhang%20and%20Wendong%20Li%20and%20Kani%20Song%20and%20Jiaye%20Lu%20and%20Gang%20Li%20and%20Liuchun%20Yang%20and%20Sheng%20Li%0AAbstract%3A%20%20%20Under%20a%20unified%20operational%20definition%2C%20we%20define%20LLM%20memory%20as%20a%20persistent%0Astate%20written%20during%20pretraining%2C%20finetuning%2C%20or%20inference%20that%20can%20later%20be%0Aaddressed%20and%20that%20stably%20influences%20outputs.%20We%20propose%20a%20four-part%20taxonomy%0A%28parametric%2C%20contextual%2C%20external%2C%20procedural/episodic%29%20and%20a%20memory%20quadruple%0A%28location%2C%20persistence%2C%20write/access%20path%2C%20controllability%29.%20We%20link%20mechanism%2C%0Aevaluation%2C%20and%20governance%20via%20the%20chain%20write%20-%3E%20read%20-%3E%20inhibit/update.%20To%0Aavoid%20distorted%20comparisons%20across%20heterogeneous%20setups%2C%20we%20adopt%20a%0Athree-setting%20protocol%20%28parametric%20only%2C%20offline%20retrieval%2C%20online%20retrieval%29%0Athat%20decouples%20capability%20from%20information%20availability%20on%20the%20same%20data%20and%0Atimeline.%20On%20this%20basis%20we%20build%20a%20layered%20evaluation%3A%20parametric%20%28closed-book%0Arecall%2C%20edit%20differential%2C%20memorization/privacy%29%2C%20contextual%20%28position%20curves%0Aand%20the%20mid-sequence%20drop%29%2C%20external%20%28answer%20correctness%20vs%20snippet%0Aattribution/faithfulness%29%2C%20and%20procedural/episodic%20%28cross-session%20consistency%0Aand%20timeline%20replay%2C%20E%20MARS%2B%29.%20The%20framework%20integrates%20temporal%20governance%20and%0Aleakage%20auditing%20%28freshness%20hits%2C%20outdated%20answers%2C%20refusal%20slices%29%20and%0Auncertainty%20reporting%20via%20inter-rater%20agreement%20plus%20paired%20tests%20with%0Amultiple-comparison%20correction.%20For%20updating%20and%20forgetting%2C%20we%20present%20DMM%0AGov%3A%20coordinating%20DAPT/TAPT%2C%20PEFT%2C%20model%20editing%20%28ROME%2C%20MEND%2C%20MEMIT%2C%20SERAC%29%2C%0Aand%20RAG%20to%20form%20an%20auditable%20loop%20covering%20admission%20thresholds%2C%20rollout%2C%0Amonitoring%2C%20rollback%2C%20and%20change%20audits%2C%20with%20specs%20for%20timeliness%2C%20conflict%0Ahandling%2C%20and%20long-horizon%20consistency.%20Finally%2C%20we%20give%20four%20testable%0Apropositions%3A%20minimum%20identifiability%3B%20a%20minimal%20evaluation%20card%3B%20causally%0Aconstrained%20editing%20with%20verifiable%20forgetting%3B%20and%20when%20retrieval%20with%0Asmall-window%20replay%20outperforms%20ultra-long-context%20reading.%20This%20yields%20a%0Areproducible%2C%20comparable%2C%20and%20governable%20coordinate%20system%20for%20research%20and%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520in%2520Large%2520Language%2520Models%253A%2520Mechanisms%252C%2520Evaluation%2520and%2520Evolution%26entry.906535625%3DDianxing%2520Zhang%2520and%2520Wendong%2520Li%2520and%2520Kani%2520Song%2520and%2520Jiaye%2520Lu%2520and%2520Gang%2520Li%2520and%2520Liuchun%2520Yang%2520and%2520Sheng%2520Li%26entry.1292438233%3D%2520%2520Under%2520a%2520unified%2520operational%2520definition%252C%2520we%2520define%2520LLM%2520memory%2520as%2520a%2520persistent%250Astate%2520written%2520during%2520pretraining%252C%2520finetuning%252C%2520or%2520inference%2520that%2520can%2520later%2520be%250Aaddressed%2520and%2520that%2520stably%2520influences%2520outputs.%2520We%2520propose%2520a%2520four-part%2520taxonomy%250A%2528parametric%252C%2520contextual%252C%2520external%252C%2520procedural/episodic%2529%2520and%2520a%2520memory%2520quadruple%250A%2528location%252C%2520persistence%252C%2520write/access%2520path%252C%2520controllability%2529.%2520We%2520link%2520mechanism%252C%250Aevaluation%252C%2520and%2520governance%2520via%2520the%2520chain%2520write%2520-%253E%2520read%2520-%253E%2520inhibit/update.%2520To%250Aavoid%2520distorted%2520comparisons%2520across%2520heterogeneous%2520setups%252C%2520we%2520adopt%2520a%250Athree-setting%2520protocol%2520%2528parametric%2520only%252C%2520offline%2520retrieval%252C%2520online%2520retrieval%2529%250Athat%2520decouples%2520capability%2520from%2520information%2520availability%2520on%2520the%2520same%2520data%2520and%250Atimeline.%2520On%2520this%2520basis%2520we%2520build%2520a%2520layered%2520evaluation%253A%2520parametric%2520%2528closed-book%250Arecall%252C%2520edit%2520differential%252C%2520memorization/privacy%2529%252C%2520contextual%2520%2528position%2520curves%250Aand%2520the%2520mid-sequence%2520drop%2529%252C%2520external%2520%2528answer%2520correctness%2520vs%2520snippet%250Aattribution/faithfulness%2529%252C%2520and%2520procedural/episodic%2520%2528cross-session%2520consistency%250Aand%2520timeline%2520replay%252C%2520E%2520MARS%252B%2529.%2520The%2520framework%2520integrates%2520temporal%2520governance%2520and%250Aleakage%2520auditing%2520%2528freshness%2520hits%252C%2520outdated%2520answers%252C%2520refusal%2520slices%2529%2520and%250Auncertainty%2520reporting%2520via%2520inter-rater%2520agreement%2520plus%2520paired%2520tests%2520with%250Amultiple-comparison%2520correction.%2520For%2520updating%2520and%2520forgetting%252C%2520we%2520present%2520DMM%250AGov%253A%2520coordinating%2520DAPT/TAPT%252C%2520PEFT%252C%2520model%2520editing%2520%2528ROME%252C%2520MEND%252C%2520MEMIT%252C%2520SERAC%2529%252C%250Aand%2520RAG%2520to%2520form%2520an%2520auditable%2520loop%2520covering%2520admission%2520thresholds%252C%2520rollout%252C%250Amonitoring%252C%2520rollback%252C%2520and%2520change%2520audits%252C%2520with%2520specs%2520for%2520timeliness%252C%2520conflict%250Ahandling%252C%2520and%2520long-horizon%2520consistency.%2520Finally%252C%2520we%2520give%2520four%2520testable%250Apropositions%253A%2520minimum%2520identifiability%253B%2520a%2520minimal%2520evaluation%2520card%253B%2520causally%250Aconstrained%2520editing%2520with%2520verifiable%2520forgetting%253B%2520and%2520when%2520retrieval%2520with%250Asmall-window%2520replay%2520outperforms%2520ultra-long-context%2520reading.%2520This%2520yields%2520a%250Areproducible%252C%2520comparable%252C%2520and%2520governable%2520coordinate%2520system%2520for%2520research%2520and%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20in%20Large%20Language%20Models%3A%20Mechanisms%2C%20Evaluation%20and%20Evolution&entry.906535625=Dianxing%20Zhang%20and%20Wendong%20Li%20and%20Kani%20Song%20and%20Jiaye%20Lu%20and%20Gang%20Li%20and%20Liuchun%20Yang%20and%20Sheng%20Li&entry.1292438233=%20%20Under%20a%20unified%20operational%20definition%2C%20we%20define%20LLM%20memory%20as%20a%20persistent%0Astate%20written%20during%20pretraining%2C%20finetuning%2C%20or%20inference%20that%20can%20later%20be%0Aaddressed%20and%20that%20stably%20influences%20outputs.%20We%20propose%20a%20four-part%20taxonomy%0A%28parametric%2C%20contextual%2C%20external%2C%20procedural/episodic%29%20and%20a%20memory%20quadruple%0A%28location%2C%20persistence%2C%20write/access%20path%2C%20controllability%29.%20We%20link%20mechanism%2C%0Aevaluation%2C%20and%20governance%20via%20the%20chain%20write%20-%3E%20read%20-%3E%20inhibit/update.%20To%0Aavoid%20distorted%20comparisons%20across%20heterogeneous%20setups%2C%20we%20adopt%20a%0Athree-setting%20protocol%20%28parametric%20only%2C%20offline%20retrieval%2C%20online%20retrieval%29%0Athat%20decouples%20capability%20from%20information%20availability%20on%20the%20same%20data%20and%0Atimeline.%20On%20this%20basis%20we%20build%20a%20layered%20evaluation%3A%20parametric%20%28closed-book%0Arecall%2C%20edit%20differential%2C%20memorization/privacy%29%2C%20contextual%20%28position%20curves%0Aand%20the%20mid-sequence%20drop%29%2C%20external%20%28answer%20correctness%20vs%20snippet%0Aattribution/faithfulness%29%2C%20and%20procedural/episodic%20%28cross-session%20consistency%0Aand%20timeline%20replay%2C%20E%20MARS%2B%29.%20The%20framework%20integrates%20temporal%20governance%20and%0Aleakage%20auditing%20%28freshness%20hits%2C%20outdated%20answers%2C%20refusal%20slices%29%20and%0Auncertainty%20reporting%20via%20inter-rater%20agreement%20plus%20paired%20tests%20with%0Amultiple-comparison%20correction.%20For%20updating%20and%20forgetting%2C%20we%20present%20DMM%0AGov%3A%20coordinating%20DAPT/TAPT%2C%20PEFT%2C%20model%20editing%20%28ROME%2C%20MEND%2C%20MEMIT%2C%20SERAC%29%2C%0Aand%20RAG%20to%20form%20an%20auditable%20loop%20covering%20admission%20thresholds%2C%20rollout%2C%0Amonitoring%2C%20rollback%2C%20and%20change%20audits%2C%20with%20specs%20for%20timeliness%2C%20conflict%0Ahandling%2C%20and%20long-horizon%20consistency.%20Finally%2C%20we%20give%20four%20testable%0Apropositions%3A%20minimum%20identifiability%3B%20a%20minimal%20evaluation%20card%3B%20causally%0Aconstrained%20editing%20with%20verifiable%20forgetting%3B%20and%20when%20retrieval%20with%0Asmall-window%20replay%20outperforms%20ultra-long-context%20reading.%20This%20yields%20a%0Areproducible%2C%20comparable%2C%20and%20governable%20coordinate%20system%20for%20research%20and%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18868v1&entry.124074799=Read"},
{"title": "Graph Neural Networks with Similarity-Navigated Probabilistic Feature\n  Copying", "author": "Asela Hevapathige", "abstract": "  Graph Neural Networks (GNNs) have demonstrated remarkable success across\nvarious graph-based tasks. However, they face some fundamental limitations:\nfeature oversmoothing can cause node representations to become\nindistinguishable in deeper networks, they struggle to effectively manage\nheterogeneous relationships where connected nodes differ significantly, and\nthey process entire feature vectors as indivisible units, which limits\nflexibility. We seek to address these limitations. We propose AxelGNN, a novel\nGNN architecture inspired by Axelrod's cultural dissemination model that\naddresses these limitations through a unified framework. AxelGNN incorporates\nsimilarity-gated probabilistic interactions that adaptively promote convergence\nor divergence based on node similarity, implements trait-level copying\nmechanisms for fine-grained feature aggregation at the segment level, and\nmaintains global polarization to preserve node distinctiveness across multiple\nrepresentation clusters. The model's bistable convergence dynamics naturally\nhandle both homophilic and heterophilic graphs within a single architecture.\nExtensive experiments on node classification and influence estimation\nbenchmarks demonstrate that AxelGNN consistently outperforms or matches\nstate-of-the-art GNN methods across diverse graph structures with varying\nhomophily-heterophily characteristics.\n", "link": "http://arxiv.org/abs/2509.19084v1", "date": "2025-09-23", "relevancy": 2.4937, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5086}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5036}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20with%20Similarity-Navigated%20Probabilistic%20Feature%0A%20%20Copying&body=Title%3A%20Graph%20Neural%20Networks%20with%20Similarity-Navigated%20Probabilistic%20Feature%0A%20%20Copying%0AAuthor%3A%20Asela%20Hevapathige%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20success%20across%0Avarious%20graph-based%20tasks.%20However%2C%20they%20face%20some%20fundamental%20limitations%3A%0Afeature%20oversmoothing%20can%20cause%20node%20representations%20to%20become%0Aindistinguishable%20in%20deeper%20networks%2C%20they%20struggle%20to%20effectively%20manage%0Aheterogeneous%20relationships%20where%20connected%20nodes%20differ%20significantly%2C%20and%0Athey%20process%20entire%20feature%20vectors%20as%20indivisible%20units%2C%20which%20limits%0Aflexibility.%20We%20seek%20to%20address%20these%20limitations.%20We%20propose%20AxelGNN%2C%20a%20novel%0AGNN%20architecture%20inspired%20by%20Axelrod%27s%20cultural%20dissemination%20model%20that%0Aaddresses%20these%20limitations%20through%20a%20unified%20framework.%20AxelGNN%20incorporates%0Asimilarity-gated%20probabilistic%20interactions%20that%20adaptively%20promote%20convergence%0Aor%20divergence%20based%20on%20node%20similarity%2C%20implements%20trait-level%20copying%0Amechanisms%20for%20fine-grained%20feature%20aggregation%20at%20the%20segment%20level%2C%20and%0Amaintains%20global%20polarization%20to%20preserve%20node%20distinctiveness%20across%20multiple%0Arepresentation%20clusters.%20The%20model%27s%20bistable%20convergence%20dynamics%20naturally%0Ahandle%20both%20homophilic%20and%20heterophilic%20graphs%20within%20a%20single%20architecture.%0AExtensive%20experiments%20on%20node%20classification%20and%20influence%20estimation%0Abenchmarks%20demonstrate%20that%20AxelGNN%20consistently%20outperforms%20or%20matches%0Astate-of-the-art%20GNN%20methods%20across%20diverse%20graph%20structures%20with%20varying%0Ahomophily-heterophily%20characteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520with%2520Similarity-Navigated%2520Probabilistic%2520Feature%250A%2520%2520Copying%26entry.906535625%3DAsela%2520Hevapathige%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520remarkable%2520success%2520across%250Avarious%2520graph-based%2520tasks.%2520However%252C%2520they%2520face%2520some%2520fundamental%2520limitations%253A%250Afeature%2520oversmoothing%2520can%2520cause%2520node%2520representations%2520to%2520become%250Aindistinguishable%2520in%2520deeper%2520networks%252C%2520they%2520struggle%2520to%2520effectively%2520manage%250Aheterogeneous%2520relationships%2520where%2520connected%2520nodes%2520differ%2520significantly%252C%2520and%250Athey%2520process%2520entire%2520feature%2520vectors%2520as%2520indivisible%2520units%252C%2520which%2520limits%250Aflexibility.%2520We%2520seek%2520to%2520address%2520these%2520limitations.%2520We%2520propose%2520AxelGNN%252C%2520a%2520novel%250AGNN%2520architecture%2520inspired%2520by%2520Axelrod%2527s%2520cultural%2520dissemination%2520model%2520that%250Aaddresses%2520these%2520limitations%2520through%2520a%2520unified%2520framework.%2520AxelGNN%2520incorporates%250Asimilarity-gated%2520probabilistic%2520interactions%2520that%2520adaptively%2520promote%2520convergence%250Aor%2520divergence%2520based%2520on%2520node%2520similarity%252C%2520implements%2520trait-level%2520copying%250Amechanisms%2520for%2520fine-grained%2520feature%2520aggregation%2520at%2520the%2520segment%2520level%252C%2520and%250Amaintains%2520global%2520polarization%2520to%2520preserve%2520node%2520distinctiveness%2520across%2520multiple%250Arepresentation%2520clusters.%2520The%2520model%2527s%2520bistable%2520convergence%2520dynamics%2520naturally%250Ahandle%2520both%2520homophilic%2520and%2520heterophilic%2520graphs%2520within%2520a%2520single%2520architecture.%250AExtensive%2520experiments%2520on%2520node%2520classification%2520and%2520influence%2520estimation%250Abenchmarks%2520demonstrate%2520that%2520AxelGNN%2520consistently%2520outperforms%2520or%2520matches%250Astate-of-the-art%2520GNN%2520methods%2520across%2520diverse%2520graph%2520structures%2520with%2520varying%250Ahomophily-heterophily%2520characteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20with%20Similarity-Navigated%20Probabilistic%20Feature%0A%20%20Copying&entry.906535625=Asela%20Hevapathige&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20success%20across%0Avarious%20graph-based%20tasks.%20However%2C%20they%20face%20some%20fundamental%20limitations%3A%0Afeature%20oversmoothing%20can%20cause%20node%20representations%20to%20become%0Aindistinguishable%20in%20deeper%20networks%2C%20they%20struggle%20to%20effectively%20manage%0Aheterogeneous%20relationships%20where%20connected%20nodes%20differ%20significantly%2C%20and%0Athey%20process%20entire%20feature%20vectors%20as%20indivisible%20units%2C%20which%20limits%0Aflexibility.%20We%20seek%20to%20address%20these%20limitations.%20We%20propose%20AxelGNN%2C%20a%20novel%0AGNN%20architecture%20inspired%20by%20Axelrod%27s%20cultural%20dissemination%20model%20that%0Aaddresses%20these%20limitations%20through%20a%20unified%20framework.%20AxelGNN%20incorporates%0Asimilarity-gated%20probabilistic%20interactions%20that%20adaptively%20promote%20convergence%0Aor%20divergence%20based%20on%20node%20similarity%2C%20implements%20trait-level%20copying%0Amechanisms%20for%20fine-grained%20feature%20aggregation%20at%20the%20segment%20level%2C%20and%0Amaintains%20global%20polarization%20to%20preserve%20node%20distinctiveness%20across%20multiple%0Arepresentation%20clusters.%20The%20model%27s%20bistable%20convergence%20dynamics%20naturally%0Ahandle%20both%20homophilic%20and%20heterophilic%20graphs%20within%20a%20single%20architecture.%0AExtensive%20experiments%20on%20node%20classification%20and%20influence%20estimation%0Abenchmarks%20demonstrate%20that%20AxelGNN%20consistently%20outperforms%20or%20matches%0Astate-of-the-art%20GNN%20methods%20across%20diverse%20graph%20structures%20with%20varying%0Ahomophily-heterophily%20characteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19084v1&entry.124074799=Read"},
{"title": "PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly\n  Detection", "author": "Qihang Zhou and Shibo He and Jiangtao Yan and Wenchao Meng and Jiming Chen", "abstract": "  In this paper, we aim to transfer CLIP's robust 2D generalization\ncapabilities to identify 3D anomalies across unseen objects of highly diverse\nclass semantics. To this end, we propose a unified framework to comprehensively\ndetect and segment 3D anomalies by leveraging both point- and pixel-level\ninformation. We first design PointAD, which leverages point-pixel\ncorrespondence to represent 3D anomalies through their associated rendering\npixel representations. This approach is referred to as implicit 3D\nrepresentation, as it focuses solely on rendering pixel anomalies but neglects\nthe inherent spatial relationships within point clouds. Then, we propose\nPointAD+ to further broaden the interpretation of 3D anomalies by introducing\nexplicit 3D representation, emphasizing spatial abnormality to uncover abnormal\nspatial relationships. Hence, we propose G-aggregation to involve geometry\ninformation to enable the aggregated point representations spatially aware. To\nsimultaneously capture rendering and spatial abnormality, PointAD+ proposes\nhierarchical representation learning, incorporating implicit and explicit\nanomaly semantics into hierarchical text prompts: rendering prompts for the\nrendering layer and geometry prompts for the geometry layer. A cross-hierarchy\ncontrastive alignment is further introduced to promote the interaction between\nthe rendering and geometry layers, facilitating mutual anomaly learning.\nFinally, PointAD+ integrates anomaly semantics from both layers to capture the\ngeneralized anomaly semantics. During the test, PointAD+ can integrate RGB\ninformation in a plug-and-play manner and further improve its detection\nperformance. Extensive experiments demonstrate the superiority of PointAD+ in\nZS 3D anomaly detection across unseen objects with highly diverse class\nsemantics, achieving a holistic understanding of abnormality.\n", "link": "http://arxiv.org/abs/2509.03277v2", "date": "2025-09-23", "relevancy": 2.4921, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.637}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointAD%2B%3A%20Learning%20Hierarchical%20Representations%20for%20Zero-shot%203D%20Anomaly%0A%20%20Detection&body=Title%3A%20PointAD%2B%3A%20Learning%20Hierarchical%20Representations%20for%20Zero-shot%203D%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Qihang%20Zhou%20and%20Shibo%20He%20and%20Jiangtao%20Yan%20and%20Wenchao%20Meng%20and%20Jiming%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20transfer%20CLIP%27s%20robust%202D%20generalization%0Acapabilities%20to%20identify%203D%20anomalies%20across%20unseen%20objects%20of%20highly%20diverse%0Aclass%20semantics.%20To%20this%20end%2C%20we%20propose%20a%20unified%20framework%20to%20comprehensively%0Adetect%20and%20segment%203D%20anomalies%20by%20leveraging%20both%20point-%20and%20pixel-level%0Ainformation.%20We%20first%20design%20PointAD%2C%20which%20leverages%20point-pixel%0Acorrespondence%20to%20represent%203D%20anomalies%20through%20their%20associated%20rendering%0Apixel%20representations.%20This%20approach%20is%20referred%20to%20as%20implicit%203D%0Arepresentation%2C%20as%20it%20focuses%20solely%20on%20rendering%20pixel%20anomalies%20but%20neglects%0Athe%20inherent%20spatial%20relationships%20within%20point%20clouds.%20Then%2C%20we%20propose%0APointAD%2B%20to%20further%20broaden%20the%20interpretation%20of%203D%20anomalies%20by%20introducing%0Aexplicit%203D%20representation%2C%20emphasizing%20spatial%20abnormality%20to%20uncover%20abnormal%0Aspatial%20relationships.%20Hence%2C%20we%20propose%20G-aggregation%20to%20involve%20geometry%0Ainformation%20to%20enable%20the%20aggregated%20point%20representations%20spatially%20aware.%20To%0Asimultaneously%20capture%20rendering%20and%20spatial%20abnormality%2C%20PointAD%2B%20proposes%0Ahierarchical%20representation%20learning%2C%20incorporating%20implicit%20and%20explicit%0Aanomaly%20semantics%20into%20hierarchical%20text%20prompts%3A%20rendering%20prompts%20for%20the%0Arendering%20layer%20and%20geometry%20prompts%20for%20the%20geometry%20layer.%20A%20cross-hierarchy%0Acontrastive%20alignment%20is%20further%20introduced%20to%20promote%20the%20interaction%20between%0Athe%20rendering%20and%20geometry%20layers%2C%20facilitating%20mutual%20anomaly%20learning.%0AFinally%2C%20PointAD%2B%20integrates%20anomaly%20semantics%20from%20both%20layers%20to%20capture%20the%0Ageneralized%20anomaly%20semantics.%20During%20the%20test%2C%20PointAD%2B%20can%20integrate%20RGB%0Ainformation%20in%20a%20plug-and-play%20manner%20and%20further%20improve%20its%20detection%0Aperformance.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20PointAD%2B%20in%0AZS%203D%20anomaly%20detection%20across%20unseen%20objects%20with%20highly%20diverse%20class%0Asemantics%2C%20achieving%20a%20holistic%20understanding%20of%20abnormality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointAD%252B%253A%2520Learning%2520Hierarchical%2520Representations%2520for%2520Zero-shot%25203D%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DQihang%2520Zhou%2520and%2520Shibo%2520He%2520and%2520Jiangtao%2520Yan%2520and%2520Wenchao%2520Meng%2520and%2520Jiming%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520transfer%2520CLIP%2527s%2520robust%25202D%2520generalization%250Acapabilities%2520to%2520identify%25203D%2520anomalies%2520across%2520unseen%2520objects%2520of%2520highly%2520diverse%250Aclass%2520semantics.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520unified%2520framework%2520to%2520comprehensively%250Adetect%2520and%2520segment%25203D%2520anomalies%2520by%2520leveraging%2520both%2520point-%2520and%2520pixel-level%250Ainformation.%2520We%2520first%2520design%2520PointAD%252C%2520which%2520leverages%2520point-pixel%250Acorrespondence%2520to%2520represent%25203D%2520anomalies%2520through%2520their%2520associated%2520rendering%250Apixel%2520representations.%2520This%2520approach%2520is%2520referred%2520to%2520as%2520implicit%25203D%250Arepresentation%252C%2520as%2520it%2520focuses%2520solely%2520on%2520rendering%2520pixel%2520anomalies%2520but%2520neglects%250Athe%2520inherent%2520spatial%2520relationships%2520within%2520point%2520clouds.%2520Then%252C%2520we%2520propose%250APointAD%252B%2520to%2520further%2520broaden%2520the%2520interpretation%2520of%25203D%2520anomalies%2520by%2520introducing%250Aexplicit%25203D%2520representation%252C%2520emphasizing%2520spatial%2520abnormality%2520to%2520uncover%2520abnormal%250Aspatial%2520relationships.%2520Hence%252C%2520we%2520propose%2520G-aggregation%2520to%2520involve%2520geometry%250Ainformation%2520to%2520enable%2520the%2520aggregated%2520point%2520representations%2520spatially%2520aware.%2520To%250Asimultaneously%2520capture%2520rendering%2520and%2520spatial%2520abnormality%252C%2520PointAD%252B%2520proposes%250Ahierarchical%2520representation%2520learning%252C%2520incorporating%2520implicit%2520and%2520explicit%250Aanomaly%2520semantics%2520into%2520hierarchical%2520text%2520prompts%253A%2520rendering%2520prompts%2520for%2520the%250Arendering%2520layer%2520and%2520geometry%2520prompts%2520for%2520the%2520geometry%2520layer.%2520A%2520cross-hierarchy%250Acontrastive%2520alignment%2520is%2520further%2520introduced%2520to%2520promote%2520the%2520interaction%2520between%250Athe%2520rendering%2520and%2520geometry%2520layers%252C%2520facilitating%2520mutual%2520anomaly%2520learning.%250AFinally%252C%2520PointAD%252B%2520integrates%2520anomaly%2520semantics%2520from%2520both%2520layers%2520to%2520capture%2520the%250Ageneralized%2520anomaly%2520semantics.%2520During%2520the%2520test%252C%2520PointAD%252B%2520can%2520integrate%2520RGB%250Ainformation%2520in%2520a%2520plug-and-play%2520manner%2520and%2520further%2520improve%2520its%2520detection%250Aperformance.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520PointAD%252B%2520in%250AZS%25203D%2520anomaly%2520detection%2520across%2520unseen%2520objects%2520with%2520highly%2520diverse%2520class%250Asemantics%252C%2520achieving%2520a%2520holistic%2520understanding%2520of%2520abnormality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointAD%2B%3A%20Learning%20Hierarchical%20Representations%20for%20Zero-shot%203D%20Anomaly%0A%20%20Detection&entry.906535625=Qihang%20Zhou%20and%20Shibo%20He%20and%20Jiangtao%20Yan%20and%20Wenchao%20Meng%20and%20Jiming%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20transfer%20CLIP%27s%20robust%202D%20generalization%0Acapabilities%20to%20identify%203D%20anomalies%20across%20unseen%20objects%20of%20highly%20diverse%0Aclass%20semantics.%20To%20this%20end%2C%20we%20propose%20a%20unified%20framework%20to%20comprehensively%0Adetect%20and%20segment%203D%20anomalies%20by%20leveraging%20both%20point-%20and%20pixel-level%0Ainformation.%20We%20first%20design%20PointAD%2C%20which%20leverages%20point-pixel%0Acorrespondence%20to%20represent%203D%20anomalies%20through%20their%20associated%20rendering%0Apixel%20representations.%20This%20approach%20is%20referred%20to%20as%20implicit%203D%0Arepresentation%2C%20as%20it%20focuses%20solely%20on%20rendering%20pixel%20anomalies%20but%20neglects%0Athe%20inherent%20spatial%20relationships%20within%20point%20clouds.%20Then%2C%20we%20propose%0APointAD%2B%20to%20further%20broaden%20the%20interpretation%20of%203D%20anomalies%20by%20introducing%0Aexplicit%203D%20representation%2C%20emphasizing%20spatial%20abnormality%20to%20uncover%20abnormal%0Aspatial%20relationships.%20Hence%2C%20we%20propose%20G-aggregation%20to%20involve%20geometry%0Ainformation%20to%20enable%20the%20aggregated%20point%20representations%20spatially%20aware.%20To%0Asimultaneously%20capture%20rendering%20and%20spatial%20abnormality%2C%20PointAD%2B%20proposes%0Ahierarchical%20representation%20learning%2C%20incorporating%20implicit%20and%20explicit%0Aanomaly%20semantics%20into%20hierarchical%20text%20prompts%3A%20rendering%20prompts%20for%20the%0Arendering%20layer%20and%20geometry%20prompts%20for%20the%20geometry%20layer.%20A%20cross-hierarchy%0Acontrastive%20alignment%20is%20further%20introduced%20to%20promote%20the%20interaction%20between%0Athe%20rendering%20and%20geometry%20layers%2C%20facilitating%20mutual%20anomaly%20learning.%0AFinally%2C%20PointAD%2B%20integrates%20anomaly%20semantics%20from%20both%20layers%20to%20capture%20the%0Ageneralized%20anomaly%20semantics.%20During%20the%20test%2C%20PointAD%2B%20can%20integrate%20RGB%0Ainformation%20in%20a%20plug-and-play%20manner%20and%20further%20improve%20its%20detection%0Aperformance.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20PointAD%2B%20in%0AZS%203D%20anomaly%20detection%20across%20unseen%20objects%20with%20highly%20diverse%20class%0Asemantics%2C%20achieving%20a%20holistic%20understanding%20of%20abnormality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03277v2&entry.124074799=Read"},
{"title": "Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence\n  Understanding Capability of Large Language Models", "author": "Haonan He and Yuchen Ren and Yining Tang and Ziyang Xu and Junxian Li and Minghao Yang and Di Zhang and Dong Yuan and Tao Chen and Shufei Zhang and Yuqiang Li and Nanqing Dong and Wanli Ouyang and Dongzhan Zhou and Peng Ye", "abstract": "  Large language models (LLMs) have shown remarkable capabilities in general\ndomains, but their application to multi-omics biology remains underexplored. To\naddress this gap, we introduce Biology-Instructions, the first large-scale\ninstruction-tuning dataset for multi-omics biological sequences, including DNA,\nRNA, proteins, and multi-molecules. This dataset bridges LLMs and complex\nbiological sequence-related tasks, enhancing their versatility and reasoning\nwhile maintaining conversational fluency. We also highlight significant\nlimitations of current state-of-the-art LLMs on multi-omics tasks without\nspecialized training. To overcome this, we propose ChatMultiOmics, a strong\nbaseline with a novel three-stage training pipeline, demonstrating superior\nbiological understanding through Biology-Instructions. Both resources are\npublicly available, paving the way for better integration of LLMs in\nmulti-omics analysis. The Biology-Instructions is publicly available at:\nhttps://github.com/hhnqqq/Biology-Instructions.\n", "link": "http://arxiv.org/abs/2412.19191v2", "date": "2025-09-23", "relevancy": 2.4769, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biology-Instructions%3A%20A%20Dataset%20and%20Benchmark%20for%20Multi-Omics%20Sequence%0A%20%20Understanding%20Capability%20of%20Large%20Language%20Models&body=Title%3A%20Biology-Instructions%3A%20A%20Dataset%20and%20Benchmark%20for%20Multi-Omics%20Sequence%0A%20%20Understanding%20Capability%20of%20Large%20Language%20Models%0AAuthor%3A%20Haonan%20He%20and%20Yuchen%20Ren%20and%20Yining%20Tang%20and%20Ziyang%20Xu%20and%20Junxian%20Li%20and%20Minghao%20Yang%20and%20Di%20Zhang%20and%20Dong%20Yuan%20and%20Tao%20Chen%20and%20Shufei%20Zhang%20and%20Yuqiang%20Li%20and%20Nanqing%20Dong%20and%20Wanli%20Ouyang%20and%20Dongzhan%20Zhou%20and%20Peng%20Ye%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20general%0Adomains%2C%20but%20their%20application%20to%20multi-omics%20biology%20remains%20underexplored.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20Biology-Instructions%2C%20the%20first%20large-scale%0Ainstruction-tuning%20dataset%20for%20multi-omics%20biological%20sequences%2C%20including%20DNA%2C%0ARNA%2C%20proteins%2C%20and%20multi-molecules.%20This%20dataset%20bridges%20LLMs%20and%20complex%0Abiological%20sequence-related%20tasks%2C%20enhancing%20their%20versatility%20and%20reasoning%0Awhile%20maintaining%20conversational%20fluency.%20We%20also%20highlight%20significant%0Alimitations%20of%20current%20state-of-the-art%20LLMs%20on%20multi-omics%20tasks%20without%0Aspecialized%20training.%20To%20overcome%20this%2C%20we%20propose%20ChatMultiOmics%2C%20a%20strong%0Abaseline%20with%20a%20novel%20three-stage%20training%20pipeline%2C%20demonstrating%20superior%0Abiological%20understanding%20through%20Biology-Instructions.%20Both%20resources%20are%0Apublicly%20available%2C%20paving%20the%20way%20for%20better%20integration%20of%20LLMs%20in%0Amulti-omics%20analysis.%20The%20Biology-Instructions%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/hhnqqq/Biology-Instructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19191v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiology-Instructions%253A%2520A%2520Dataset%2520and%2520Benchmark%2520for%2520Multi-Omics%2520Sequence%250A%2520%2520Understanding%2520Capability%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DHaonan%2520He%2520and%2520Yuchen%2520Ren%2520and%2520Yining%2520Tang%2520and%2520Ziyang%2520Xu%2520and%2520Junxian%2520Li%2520and%2520Minghao%2520Yang%2520and%2520Di%2520Zhang%2520and%2520Dong%2520Yuan%2520and%2520Tao%2520Chen%2520and%2520Shufei%2520Zhang%2520and%2520Yuqiang%2520Li%2520and%2520Nanqing%2520Dong%2520and%2520Wanli%2520Ouyang%2520and%2520Dongzhan%2520Zhou%2520and%2520Peng%2520Ye%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520general%250Adomains%252C%2520but%2520their%2520application%2520to%2520multi-omics%2520biology%2520remains%2520underexplored.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520Biology-Instructions%252C%2520the%2520first%2520large-scale%250Ainstruction-tuning%2520dataset%2520for%2520multi-omics%2520biological%2520sequences%252C%2520including%2520DNA%252C%250ARNA%252C%2520proteins%252C%2520and%2520multi-molecules.%2520This%2520dataset%2520bridges%2520LLMs%2520and%2520complex%250Abiological%2520sequence-related%2520tasks%252C%2520enhancing%2520their%2520versatility%2520and%2520reasoning%250Awhile%2520maintaining%2520conversational%2520fluency.%2520We%2520also%2520highlight%2520significant%250Alimitations%2520of%2520current%2520state-of-the-art%2520LLMs%2520on%2520multi-omics%2520tasks%2520without%250Aspecialized%2520training.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520ChatMultiOmics%252C%2520a%2520strong%250Abaseline%2520with%2520a%2520novel%2520three-stage%2520training%2520pipeline%252C%2520demonstrating%2520superior%250Abiological%2520understanding%2520through%2520Biology-Instructions.%2520Both%2520resources%2520are%250Apublicly%2520available%252C%2520paving%2520the%2520way%2520for%2520better%2520integration%2520of%2520LLMs%2520in%250Amulti-omics%2520analysis.%2520The%2520Biology-Instructions%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/hhnqqq/Biology-Instructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19191v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biology-Instructions%3A%20A%20Dataset%20and%20Benchmark%20for%20Multi-Omics%20Sequence%0A%20%20Understanding%20Capability%20of%20Large%20Language%20Models&entry.906535625=Haonan%20He%20and%20Yuchen%20Ren%20and%20Yining%20Tang%20and%20Ziyang%20Xu%20and%20Junxian%20Li%20and%20Minghao%20Yang%20and%20Di%20Zhang%20and%20Dong%20Yuan%20and%20Tao%20Chen%20and%20Shufei%20Zhang%20and%20Yuqiang%20Li%20and%20Nanqing%20Dong%20and%20Wanli%20Ouyang%20and%20Dongzhan%20Zhou%20and%20Peng%20Ye&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20general%0Adomains%2C%20but%20their%20application%20to%20multi-omics%20biology%20remains%20underexplored.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20Biology-Instructions%2C%20the%20first%20large-scale%0Ainstruction-tuning%20dataset%20for%20multi-omics%20biological%20sequences%2C%20including%20DNA%2C%0ARNA%2C%20proteins%2C%20and%20multi-molecules.%20This%20dataset%20bridges%20LLMs%20and%20complex%0Abiological%20sequence-related%20tasks%2C%20enhancing%20their%20versatility%20and%20reasoning%0Awhile%20maintaining%20conversational%20fluency.%20We%20also%20highlight%20significant%0Alimitations%20of%20current%20state-of-the-art%20LLMs%20on%20multi-omics%20tasks%20without%0Aspecialized%20training.%20To%20overcome%20this%2C%20we%20propose%20ChatMultiOmics%2C%20a%20strong%0Abaseline%20with%20a%20novel%20three-stage%20training%20pipeline%2C%20demonstrating%20superior%0Abiological%20understanding%20through%20Biology-Instructions.%20Both%20resources%20are%0Apublicly%20available%2C%20paving%20the%20way%20for%20better%20integration%20of%20LLMs%20in%0Amulti-omics%20analysis.%20The%20Biology-Instructions%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/hhnqqq/Biology-Instructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19191v2&entry.124074799=Read"},
{"title": "MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion\n  interactive segmentation of neurobromas in whole-body MRI", "author": "Georgii Kolokolnikov and Marie-Lena Schmalhofer and Sophie G\u007f\u00f6tz and Lennart Well and Said Farschtschi and Victor-Felix Mautner and Inka Ristow and Rene Werner", "abstract": "  Background and Objectives: Neurofibromatosis type 1 is a genetic disorder\ncharacterized by the development of numerous neurofibromas (NFs) throughout the\nbody. Whole-body MRI (WB-MRI) is the clinical standard for detection and\nlongitudinal surveillance of NF tumor growth. Existing interactive segmentation\nmethods fail to combine high lesion-wise precision with scalability to hundreds\nof lesions. This study proposes a novel interactive segmentation model tailored\nto this challenge.\n  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation\nmodel that extends the state-of-the-art, transformer-based, promptable Segment\nAnything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was\ntrained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using\nT2-weighted fat-suppressed sequences. The dataset was split at the patient\nlevel into a training set and four test sets (one in-domain and three\nreflecting different domain shift scenarios, e.g., MRI field strength\nvariation, low tumor burden, differences in clinical site and scanner vendor).\n  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of\n0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:\n0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained\nunder MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:\n0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1\nscores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader\nvariability analysis showed model-to-expert agreement (DSC: 0.62-0.68),\ncomparable to inter-expert agreement (DSC: 0.57-0.69).\n  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable\ninteractive segmentation of NFs in WB-MRI with minimal user input and strong\ngeneralization, supporting integration into clinical workflows.\n", "link": "http://arxiv.org/abs/2509.19277v1", "date": "2025-09-23", "relevancy": 2.4736, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOIS-SAM2%3A%20Exemplar-based%20Segment%20Anything%20Model%202%20for%20multilesion%0A%20%20interactive%20segmentation%20of%20neurobromas%20in%20whole-body%20MRI&body=Title%3A%20MOIS-SAM2%3A%20Exemplar-based%20Segment%20Anything%20Model%202%20for%20multilesion%0A%20%20interactive%20segmentation%20of%20neurobromas%20in%20whole-body%20MRI%0AAuthor%3A%20Georgii%20Kolokolnikov%20and%20Marie-Lena%20Schmalhofer%20and%20Sophie%20G%7F%C3%B6tz%20and%20Lennart%20Well%20and%20Said%20Farschtschi%20and%20Victor-Felix%20Mautner%20and%20Inka%20Ristow%20and%20Rene%20Werner%0AAbstract%3A%20%20%20Background%20and%20Objectives%3A%20Neurofibromatosis%20type%201%20is%20a%20genetic%20disorder%0Acharacterized%20by%20the%20development%20of%20numerous%20neurofibromas%20%28NFs%29%20throughout%20the%0Abody.%20Whole-body%20MRI%20%28WB-MRI%29%20is%20the%20clinical%20standard%20for%20detection%20and%0Alongitudinal%20surveillance%20of%20NF%20tumor%20growth.%20Existing%20interactive%20segmentation%0Amethods%20fail%20to%20combine%20high%20lesion-wise%20precision%20with%20scalability%20to%20hundreds%0Aof%20lesions.%20This%20study%20proposes%20a%20novel%20interactive%20segmentation%20model%20tailored%0Ato%20this%20challenge.%0A%20%20Methods%3A%20We%20introduce%20MOIS-SAM2%2C%20a%20multi-object%20interactive%20segmentation%0Amodel%20that%20extends%20the%20state-of-the-art%2C%20transformer-based%2C%20promptable%20Segment%0AAnything%20Model%202%20%28SAM2%29%20with%20exemplar-based%20semantic%20propagation.%20MOIS-SAM2%20was%0Atrained%20and%20evaluated%20on%20119%20WB-MRI%20scans%20from%2084%20NF1%20patients%20acquired%20using%0AT2-weighted%20fat-suppressed%20sequences.%20The%20dataset%20was%20split%20at%20the%20patient%0Alevel%20into%20a%20training%20set%20and%20four%20test%20sets%20%28one%20in-domain%20and%20three%0Areflecting%20different%20domain%20shift%20scenarios%2C%20e.g.%2C%20MRI%20field%20strength%0Avariation%2C%20low%20tumor%20burden%2C%20differences%20in%20clinical%20site%20and%20scanner%20vendor%29.%0A%20%20Results%3A%20On%20the%20in-domain%20test%20set%2C%20MOIS-SAM2%20achieved%20a%20scan-wise%20DSC%20of%0A0.60%20against%20expert%20manual%20annotations%2C%20outperforming%20baseline%203D%20nnU-Net%20%28DSC%3A%0A0.54%29%20and%20SAM2%20%28DSC%3A%200.35%29.%20Performance%20of%20the%20proposed%20model%20was%20maintained%0Aunder%20MRI%20field%20strength%20shift%20%28DSC%3A%200.53%29%20and%20scanner%20vendor%20variation%20%28DSC%3A%0A0.50%29%2C%20and%20improved%20in%20low%20tumor%20burden%20cases%20%28DSC%3A%200.61%29.%20Lesion%20detection%20F1%0Ascores%20ranged%20from%200.62%20to%200.78%20across%20test%20sets.%20Preliminary%20inter-reader%0Avariability%20analysis%20showed%20model-to-expert%20agreement%20%28DSC%3A%200.62-0.68%29%2C%0Acomparable%20to%20inter-expert%20agreement%20%28DSC%3A%200.57-0.69%29.%0A%20%20Conclusions%3A%20The%20proposed%20MOIS-SAM2%20enables%20efficient%20and%20scalable%0Ainteractive%20segmentation%20of%20NFs%20in%20WB-MRI%20with%20minimal%20user%20input%20and%20strong%0Ageneralization%2C%20supporting%20integration%20into%20clinical%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOIS-SAM2%253A%2520Exemplar-based%2520Segment%2520Anything%2520Model%25202%2520for%2520multilesion%250A%2520%2520interactive%2520segmentation%2520of%2520neurobromas%2520in%2520whole-body%2520MRI%26entry.906535625%3DGeorgii%2520Kolokolnikov%2520and%2520Marie-Lena%2520Schmalhofer%2520and%2520Sophie%2520G%257F%25C3%25B6tz%2520and%2520Lennart%2520Well%2520and%2520Said%2520Farschtschi%2520and%2520Victor-Felix%2520Mautner%2520and%2520Inka%2520Ristow%2520and%2520Rene%2520Werner%26entry.1292438233%3D%2520%2520Background%2520and%2520Objectives%253A%2520Neurofibromatosis%2520type%25201%2520is%2520a%2520genetic%2520disorder%250Acharacterized%2520by%2520the%2520development%2520of%2520numerous%2520neurofibromas%2520%2528NFs%2529%2520throughout%2520the%250Abody.%2520Whole-body%2520MRI%2520%2528WB-MRI%2529%2520is%2520the%2520clinical%2520standard%2520for%2520detection%2520and%250Alongitudinal%2520surveillance%2520of%2520NF%2520tumor%2520growth.%2520Existing%2520interactive%2520segmentation%250Amethods%2520fail%2520to%2520combine%2520high%2520lesion-wise%2520precision%2520with%2520scalability%2520to%2520hundreds%250Aof%2520lesions.%2520This%2520study%2520proposes%2520a%2520novel%2520interactive%2520segmentation%2520model%2520tailored%250Ato%2520this%2520challenge.%250A%2520%2520Methods%253A%2520We%2520introduce%2520MOIS-SAM2%252C%2520a%2520multi-object%2520interactive%2520segmentation%250Amodel%2520that%2520extends%2520the%2520state-of-the-art%252C%2520transformer-based%252C%2520promptable%2520Segment%250AAnything%2520Model%25202%2520%2528SAM2%2529%2520with%2520exemplar-based%2520semantic%2520propagation.%2520MOIS-SAM2%2520was%250Atrained%2520and%2520evaluated%2520on%2520119%2520WB-MRI%2520scans%2520from%252084%2520NF1%2520patients%2520acquired%2520using%250AT2-weighted%2520fat-suppressed%2520sequences.%2520The%2520dataset%2520was%2520split%2520at%2520the%2520patient%250Alevel%2520into%2520a%2520training%2520set%2520and%2520four%2520test%2520sets%2520%2528one%2520in-domain%2520and%2520three%250Areflecting%2520different%2520domain%2520shift%2520scenarios%252C%2520e.g.%252C%2520MRI%2520field%2520strength%250Avariation%252C%2520low%2520tumor%2520burden%252C%2520differences%2520in%2520clinical%2520site%2520and%2520scanner%2520vendor%2529.%250A%2520%2520Results%253A%2520On%2520the%2520in-domain%2520test%2520set%252C%2520MOIS-SAM2%2520achieved%2520a%2520scan-wise%2520DSC%2520of%250A0.60%2520against%2520expert%2520manual%2520annotations%252C%2520outperforming%2520baseline%25203D%2520nnU-Net%2520%2528DSC%253A%250A0.54%2529%2520and%2520SAM2%2520%2528DSC%253A%25200.35%2529.%2520Performance%2520of%2520the%2520proposed%2520model%2520was%2520maintained%250Aunder%2520MRI%2520field%2520strength%2520shift%2520%2528DSC%253A%25200.53%2529%2520and%2520scanner%2520vendor%2520variation%2520%2528DSC%253A%250A0.50%2529%252C%2520and%2520improved%2520in%2520low%2520tumor%2520burden%2520cases%2520%2528DSC%253A%25200.61%2529.%2520Lesion%2520detection%2520F1%250Ascores%2520ranged%2520from%25200.62%2520to%25200.78%2520across%2520test%2520sets.%2520Preliminary%2520inter-reader%250Avariability%2520analysis%2520showed%2520model-to-expert%2520agreement%2520%2528DSC%253A%25200.62-0.68%2529%252C%250Acomparable%2520to%2520inter-expert%2520agreement%2520%2528DSC%253A%25200.57-0.69%2529.%250A%2520%2520Conclusions%253A%2520The%2520proposed%2520MOIS-SAM2%2520enables%2520efficient%2520and%2520scalable%250Ainteractive%2520segmentation%2520of%2520NFs%2520in%2520WB-MRI%2520with%2520minimal%2520user%2520input%2520and%2520strong%250Ageneralization%252C%2520supporting%2520integration%2520into%2520clinical%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOIS-SAM2%3A%20Exemplar-based%20Segment%20Anything%20Model%202%20for%20multilesion%0A%20%20interactive%20segmentation%20of%20neurobromas%20in%20whole-body%20MRI&entry.906535625=Georgii%20Kolokolnikov%20and%20Marie-Lena%20Schmalhofer%20and%20Sophie%20G%7F%C3%B6tz%20and%20Lennart%20Well%20and%20Said%20Farschtschi%20and%20Victor-Felix%20Mautner%20and%20Inka%20Ristow%20and%20Rene%20Werner&entry.1292438233=%20%20Background%20and%20Objectives%3A%20Neurofibromatosis%20type%201%20is%20a%20genetic%20disorder%0Acharacterized%20by%20the%20development%20of%20numerous%20neurofibromas%20%28NFs%29%20throughout%20the%0Abody.%20Whole-body%20MRI%20%28WB-MRI%29%20is%20the%20clinical%20standard%20for%20detection%20and%0Alongitudinal%20surveillance%20of%20NF%20tumor%20growth.%20Existing%20interactive%20segmentation%0Amethods%20fail%20to%20combine%20high%20lesion-wise%20precision%20with%20scalability%20to%20hundreds%0Aof%20lesions.%20This%20study%20proposes%20a%20novel%20interactive%20segmentation%20model%20tailored%0Ato%20this%20challenge.%0A%20%20Methods%3A%20We%20introduce%20MOIS-SAM2%2C%20a%20multi-object%20interactive%20segmentation%0Amodel%20that%20extends%20the%20state-of-the-art%2C%20transformer-based%2C%20promptable%20Segment%0AAnything%20Model%202%20%28SAM2%29%20with%20exemplar-based%20semantic%20propagation.%20MOIS-SAM2%20was%0Atrained%20and%20evaluated%20on%20119%20WB-MRI%20scans%20from%2084%20NF1%20patients%20acquired%20using%0AT2-weighted%20fat-suppressed%20sequences.%20The%20dataset%20was%20split%20at%20the%20patient%0Alevel%20into%20a%20training%20set%20and%20four%20test%20sets%20%28one%20in-domain%20and%20three%0Areflecting%20different%20domain%20shift%20scenarios%2C%20e.g.%2C%20MRI%20field%20strength%0Avariation%2C%20low%20tumor%20burden%2C%20differences%20in%20clinical%20site%20and%20scanner%20vendor%29.%0A%20%20Results%3A%20On%20the%20in-domain%20test%20set%2C%20MOIS-SAM2%20achieved%20a%20scan-wise%20DSC%20of%0A0.60%20against%20expert%20manual%20annotations%2C%20outperforming%20baseline%203D%20nnU-Net%20%28DSC%3A%0A0.54%29%20and%20SAM2%20%28DSC%3A%200.35%29.%20Performance%20of%20the%20proposed%20model%20was%20maintained%0Aunder%20MRI%20field%20strength%20shift%20%28DSC%3A%200.53%29%20and%20scanner%20vendor%20variation%20%28DSC%3A%0A0.50%29%2C%20and%20improved%20in%20low%20tumor%20burden%20cases%20%28DSC%3A%200.61%29.%20Lesion%20detection%20F1%0Ascores%20ranged%20from%200.62%20to%200.78%20across%20test%20sets.%20Preliminary%20inter-reader%0Avariability%20analysis%20showed%20model-to-expert%20agreement%20%28DSC%3A%200.62-0.68%29%2C%0Acomparable%20to%20inter-expert%20agreement%20%28DSC%3A%200.57-0.69%29.%0A%20%20Conclusions%3A%20The%20proposed%20MOIS-SAM2%20enables%20efficient%20and%20scalable%0Ainteractive%20segmentation%20of%20NFs%20in%20WB-MRI%20with%20minimal%20user%20input%20and%20strong%0Ageneralization%2C%20supporting%20integration%20into%20clinical%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19277v1&entry.124074799=Read"},
{"title": "Beyond Input Activations: Identifying Influential Latents by Gradient\n  Sparse Autoencoders", "author": "Dong Shu and Xuansheng Wu and Haiyan Zhao and Mengnan Du and Ninghao Liu", "abstract": "  Sparse Autoencoders (SAEs) have recently emerged as powerful tools for\ninterpreting and steering the internal representations of large language models\n(LLMs). However, conventional approaches to analyzing SAEs typically rely\nsolely on input-side activations, without considering the causal influence\nbetween each latent feature and the model's output. This work is built on two\nkey hypotheses: (1) activated latents do not contribute equally to the\nconstruction of the model's output, and (2) only latents with high causal\ninfluence are effective for model steering. To validate these hypotheses, we\npropose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method\nthat identifies the most influential latents by incorporating output-side\ngradient information.\n", "link": "http://arxiv.org/abs/2505.08080v2", "date": "2025-09-23", "relevancy": 2.4727, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5304}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.478}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Input%20Activations%3A%20Identifying%20Influential%20Latents%20by%20Gradient%0A%20%20Sparse%20Autoencoders&body=Title%3A%20Beyond%20Input%20Activations%3A%20Identifying%20Influential%20Latents%20by%20Gradient%0A%20%20Sparse%20Autoencoders%0AAuthor%3A%20Dong%20Shu%20and%20Xuansheng%20Wu%20and%20Haiyan%20Zhao%20and%20Mengnan%20Du%20and%20Ninghao%20Liu%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20recently%20emerged%20as%20powerful%20tools%20for%0Ainterpreting%20and%20steering%20the%20internal%20representations%20of%20large%20language%20models%0A%28LLMs%29.%20However%2C%20conventional%20approaches%20to%20analyzing%20SAEs%20typically%20rely%0Asolely%20on%20input-side%20activations%2C%20without%20considering%20the%20causal%20influence%0Abetween%20each%20latent%20feature%20and%20the%20model%27s%20output.%20This%20work%20is%20built%20on%20two%0Akey%20hypotheses%3A%20%281%29%20activated%20latents%20do%20not%20contribute%20equally%20to%20the%0Aconstruction%20of%20the%20model%27s%20output%2C%20and%20%282%29%20only%20latents%20with%20high%20causal%0Ainfluence%20are%20effective%20for%20model%20steering.%20To%20validate%20these%20hypotheses%2C%20we%0Apropose%20Gradient%20Sparse%20Autoencoder%20%28GradSAE%29%2C%20a%20simple%20yet%20effective%20method%0Athat%20identifies%20the%20most%20influential%20latents%20by%20incorporating%20output-side%0Agradient%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08080v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Input%2520Activations%253A%2520Identifying%2520Influential%2520Latents%2520by%2520Gradient%250A%2520%2520Sparse%2520Autoencoders%26entry.906535625%3DDong%2520Shu%2520and%2520Xuansheng%2520Wu%2520and%2520Haiyan%2520Zhao%2520and%2520Mengnan%2520Du%2520and%2520Ninghao%2520Liu%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520have%2520recently%2520emerged%2520as%2520powerful%2520tools%2520for%250Ainterpreting%2520and%2520steering%2520the%2520internal%2520representations%2520of%2520large%2520language%2520models%250A%2528LLMs%2529.%2520However%252C%2520conventional%2520approaches%2520to%2520analyzing%2520SAEs%2520typically%2520rely%250Asolely%2520on%2520input-side%2520activations%252C%2520without%2520considering%2520the%2520causal%2520influence%250Abetween%2520each%2520latent%2520feature%2520and%2520the%2520model%2527s%2520output.%2520This%2520work%2520is%2520built%2520on%2520two%250Akey%2520hypotheses%253A%2520%25281%2529%2520activated%2520latents%2520do%2520not%2520contribute%2520equally%2520to%2520the%250Aconstruction%2520of%2520the%2520model%2527s%2520output%252C%2520and%2520%25282%2529%2520only%2520latents%2520with%2520high%2520causal%250Ainfluence%2520are%2520effective%2520for%2520model%2520steering.%2520To%2520validate%2520these%2520hypotheses%252C%2520we%250Apropose%2520Gradient%2520Sparse%2520Autoencoder%2520%2528GradSAE%2529%252C%2520a%2520simple%2520yet%2520effective%2520method%250Athat%2520identifies%2520the%2520most%2520influential%2520latents%2520by%2520incorporating%2520output-side%250Agradient%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08080v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Input%20Activations%3A%20Identifying%20Influential%20Latents%20by%20Gradient%0A%20%20Sparse%20Autoencoders&entry.906535625=Dong%20Shu%20and%20Xuansheng%20Wu%20and%20Haiyan%20Zhao%20and%20Mengnan%20Du%20and%20Ninghao%20Liu&entry.1292438233=%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20recently%20emerged%20as%20powerful%20tools%20for%0Ainterpreting%20and%20steering%20the%20internal%20representations%20of%20large%20language%20models%0A%28LLMs%29.%20However%2C%20conventional%20approaches%20to%20analyzing%20SAEs%20typically%20rely%0Asolely%20on%20input-side%20activations%2C%20without%20considering%20the%20causal%20influence%0Abetween%20each%20latent%20feature%20and%20the%20model%27s%20output.%20This%20work%20is%20built%20on%20two%0Akey%20hypotheses%3A%20%281%29%20activated%20latents%20do%20not%20contribute%20equally%20to%20the%0Aconstruction%20of%20the%20model%27s%20output%2C%20and%20%282%29%20only%20latents%20with%20high%20causal%0Ainfluence%20are%20effective%20for%20model%20steering.%20To%20validate%20these%20hypotheses%2C%20we%0Apropose%20Gradient%20Sparse%20Autoencoder%20%28GradSAE%29%2C%20a%20simple%20yet%20effective%20method%0Athat%20identifies%20the%20most%20influential%20latents%20by%20incorporating%20output-side%0Agradient%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08080v2&entry.124074799=Read"},
{"title": "Generative Propaganda", "author": "Madeleine I. G. Daepp and Alejandro Cuevas and Robert Osazuwa Ness and Vickie Yu-Ping Wang and Bharat Kumar Nayak and Dibyendu Mishra and Ti-Chung Cheng and Shaily Desai and Joyojeet Pal", "abstract": "  Generative propaganda is the use of generative artificial intelligence (AI)\nto shape public opinion. To characterize its use in real-world settings, we\nconducted interviews with defenders (e.g., factcheckers, journalists,\nofficials) in Taiwan and creators (e.g., influencers, political consultants,\nadvertisers) as well as defenders in India, centering two places characterized\nby high levels of online propaganda. The term \"deepfakes\", we find, exerts\noutsized discursive power in shaping defenders' expectations of misuse and, in\nturn, the interventions that are prioritized. To better characterize the space\nof generative propaganda, we develop a taxonomy that distinguishes between\nobvious versus hidden and promotional versus derogatory use. Deception was\nneither the main driver nor the main impact vector of AI's use; instead, Indian\ncreators sought to persuade rather than to deceive, often making AI's use\nobvious in order to reduce legal and reputational risks, while Taiwan's\ndefenders saw deception as a subset of broader efforts to distort the\nprevalence of strategic narratives online. AI was useful and used, however, in\nproducing efficiency gains in communicating across languages and modes, and in\nevading human and algorithmic detection. Security researchers should reconsider\nthreat models to clearly differentiate deepfakes from promotional and obvious\nuses, to complement and bolster the social factors that constrain misuse by\ninternal actors, and to counter efficiency gains globally.\n", "link": "http://arxiv.org/abs/2509.19147v1", "date": "2025-09-23", "relevancy": 2.4412, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5002}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4981}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Propaganda&body=Title%3A%20Generative%20Propaganda%0AAuthor%3A%20Madeleine%20I.%20G.%20Daepp%20and%20Alejandro%20Cuevas%20and%20Robert%20Osazuwa%20Ness%20and%20Vickie%20Yu-Ping%20Wang%20and%20Bharat%20Kumar%20Nayak%20and%20Dibyendu%20Mishra%20and%20Ti-Chung%20Cheng%20and%20Shaily%20Desai%20and%20Joyojeet%20Pal%0AAbstract%3A%20%20%20Generative%20propaganda%20is%20the%20use%20of%20generative%20artificial%20intelligence%20%28AI%29%0Ato%20shape%20public%20opinion.%20To%20characterize%20its%20use%20in%20real-world%20settings%2C%20we%0Aconducted%20interviews%20with%20defenders%20%28e.g.%2C%20factcheckers%2C%20journalists%2C%0Aofficials%29%20in%20Taiwan%20and%20creators%20%28e.g.%2C%20influencers%2C%20political%20consultants%2C%0Aadvertisers%29%20as%20well%20as%20defenders%20in%20India%2C%20centering%20two%20places%20characterized%0Aby%20high%20levels%20of%20online%20propaganda.%20The%20term%20%22deepfakes%22%2C%20we%20find%2C%20exerts%0Aoutsized%20discursive%20power%20in%20shaping%20defenders%27%20expectations%20of%20misuse%20and%2C%20in%0Aturn%2C%20the%20interventions%20that%20are%20prioritized.%20To%20better%20characterize%20the%20space%0Aof%20generative%20propaganda%2C%20we%20develop%20a%20taxonomy%20that%20distinguishes%20between%0Aobvious%20versus%20hidden%20and%20promotional%20versus%20derogatory%20use.%20Deception%20was%0Aneither%20the%20main%20driver%20nor%20the%20main%20impact%20vector%20of%20AI%27s%20use%3B%20instead%2C%20Indian%0Acreators%20sought%20to%20persuade%20rather%20than%20to%20deceive%2C%20often%20making%20AI%27s%20use%0Aobvious%20in%20order%20to%20reduce%20legal%20and%20reputational%20risks%2C%20while%20Taiwan%27s%0Adefenders%20saw%20deception%20as%20a%20subset%20of%20broader%20efforts%20to%20distort%20the%0Aprevalence%20of%20strategic%20narratives%20online.%20AI%20was%20useful%20and%20used%2C%20however%2C%20in%0Aproducing%20efficiency%20gains%20in%20communicating%20across%20languages%20and%20modes%2C%20and%20in%0Aevading%20human%20and%20algorithmic%20detection.%20Security%20researchers%20should%20reconsider%0Athreat%20models%20to%20clearly%20differentiate%20deepfakes%20from%20promotional%20and%20obvious%0Auses%2C%20to%20complement%20and%20bolster%20the%20social%20factors%20that%20constrain%20misuse%20by%0Ainternal%20actors%2C%20and%20to%20counter%20efficiency%20gains%20globally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Propaganda%26entry.906535625%3DMadeleine%2520I.%2520G.%2520Daepp%2520and%2520Alejandro%2520Cuevas%2520and%2520Robert%2520Osazuwa%2520Ness%2520and%2520Vickie%2520Yu-Ping%2520Wang%2520and%2520Bharat%2520Kumar%2520Nayak%2520and%2520Dibyendu%2520Mishra%2520and%2520Ti-Chung%2520Cheng%2520and%2520Shaily%2520Desai%2520and%2520Joyojeet%2520Pal%26entry.1292438233%3D%2520%2520Generative%2520propaganda%2520is%2520the%2520use%2520of%2520generative%2520artificial%2520intelligence%2520%2528AI%2529%250Ato%2520shape%2520public%2520opinion.%2520To%2520characterize%2520its%2520use%2520in%2520real-world%2520settings%252C%2520we%250Aconducted%2520interviews%2520with%2520defenders%2520%2528e.g.%252C%2520factcheckers%252C%2520journalists%252C%250Aofficials%2529%2520in%2520Taiwan%2520and%2520creators%2520%2528e.g.%252C%2520influencers%252C%2520political%2520consultants%252C%250Aadvertisers%2529%2520as%2520well%2520as%2520defenders%2520in%2520India%252C%2520centering%2520two%2520places%2520characterized%250Aby%2520high%2520levels%2520of%2520online%2520propaganda.%2520The%2520term%2520%2522deepfakes%2522%252C%2520we%2520find%252C%2520exerts%250Aoutsized%2520discursive%2520power%2520in%2520shaping%2520defenders%2527%2520expectations%2520of%2520misuse%2520and%252C%2520in%250Aturn%252C%2520the%2520interventions%2520that%2520are%2520prioritized.%2520To%2520better%2520characterize%2520the%2520space%250Aof%2520generative%2520propaganda%252C%2520we%2520develop%2520a%2520taxonomy%2520that%2520distinguishes%2520between%250Aobvious%2520versus%2520hidden%2520and%2520promotional%2520versus%2520derogatory%2520use.%2520Deception%2520was%250Aneither%2520the%2520main%2520driver%2520nor%2520the%2520main%2520impact%2520vector%2520of%2520AI%2527s%2520use%253B%2520instead%252C%2520Indian%250Acreators%2520sought%2520to%2520persuade%2520rather%2520than%2520to%2520deceive%252C%2520often%2520making%2520AI%2527s%2520use%250Aobvious%2520in%2520order%2520to%2520reduce%2520legal%2520and%2520reputational%2520risks%252C%2520while%2520Taiwan%2527s%250Adefenders%2520saw%2520deception%2520as%2520a%2520subset%2520of%2520broader%2520efforts%2520to%2520distort%2520the%250Aprevalence%2520of%2520strategic%2520narratives%2520online.%2520AI%2520was%2520useful%2520and%2520used%252C%2520however%252C%2520in%250Aproducing%2520efficiency%2520gains%2520in%2520communicating%2520across%2520languages%2520and%2520modes%252C%2520and%2520in%250Aevading%2520human%2520and%2520algorithmic%2520detection.%2520Security%2520researchers%2520should%2520reconsider%250Athreat%2520models%2520to%2520clearly%2520differentiate%2520deepfakes%2520from%2520promotional%2520and%2520obvious%250Auses%252C%2520to%2520complement%2520and%2520bolster%2520the%2520social%2520factors%2520that%2520constrain%2520misuse%2520by%250Ainternal%2520actors%252C%2520and%2520to%2520counter%2520efficiency%2520gains%2520globally.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Propaganda&entry.906535625=Madeleine%20I.%20G.%20Daepp%20and%20Alejandro%20Cuevas%20and%20Robert%20Osazuwa%20Ness%20and%20Vickie%20Yu-Ping%20Wang%20and%20Bharat%20Kumar%20Nayak%20and%20Dibyendu%20Mishra%20and%20Ti-Chung%20Cheng%20and%20Shaily%20Desai%20and%20Joyojeet%20Pal&entry.1292438233=%20%20Generative%20propaganda%20is%20the%20use%20of%20generative%20artificial%20intelligence%20%28AI%29%0Ato%20shape%20public%20opinion.%20To%20characterize%20its%20use%20in%20real-world%20settings%2C%20we%0Aconducted%20interviews%20with%20defenders%20%28e.g.%2C%20factcheckers%2C%20journalists%2C%0Aofficials%29%20in%20Taiwan%20and%20creators%20%28e.g.%2C%20influencers%2C%20political%20consultants%2C%0Aadvertisers%29%20as%20well%20as%20defenders%20in%20India%2C%20centering%20two%20places%20characterized%0Aby%20high%20levels%20of%20online%20propaganda.%20The%20term%20%22deepfakes%22%2C%20we%20find%2C%20exerts%0Aoutsized%20discursive%20power%20in%20shaping%20defenders%27%20expectations%20of%20misuse%20and%2C%20in%0Aturn%2C%20the%20interventions%20that%20are%20prioritized.%20To%20better%20characterize%20the%20space%0Aof%20generative%20propaganda%2C%20we%20develop%20a%20taxonomy%20that%20distinguishes%20between%0Aobvious%20versus%20hidden%20and%20promotional%20versus%20derogatory%20use.%20Deception%20was%0Aneither%20the%20main%20driver%20nor%20the%20main%20impact%20vector%20of%20AI%27s%20use%3B%20instead%2C%20Indian%0Acreators%20sought%20to%20persuade%20rather%20than%20to%20deceive%2C%20often%20making%20AI%27s%20use%0Aobvious%20in%20order%20to%20reduce%20legal%20and%20reputational%20risks%2C%20while%20Taiwan%27s%0Adefenders%20saw%20deception%20as%20a%20subset%20of%20broader%20efforts%20to%20distort%20the%0Aprevalence%20of%20strategic%20narratives%20online.%20AI%20was%20useful%20and%20used%2C%20however%2C%20in%0Aproducing%20efficiency%20gains%20in%20communicating%20across%20languages%20and%20modes%2C%20and%20in%0Aevading%20human%20and%20algorithmic%20detection.%20Security%20researchers%20should%20reconsider%0Athreat%20models%20to%20clearly%20differentiate%20deepfakes%20from%20promotional%20and%20obvious%0Auses%2C%20to%20complement%20and%20bolster%20the%20social%20factors%20that%20constrain%20misuse%20by%0Ainternal%20actors%2C%20and%20to%20counter%20efficiency%20gains%20globally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19147v1&entry.124074799=Read"},
{"title": "Exploring Model Kinship for Merging Large Language Models", "author": "Yedi Hu and Yunzhi Yao and Ningyu Zhang and Huajun Chen and Shumin Deng", "abstract": "  Model merging has emerged as a key technique for enhancing the capabilities\nand efficiency of Large Language Models (LLMs). The open-source community has\ndriven model evolution by iteratively merging existing models, yet a principled\nunderstanding of the gains and underlying factors in model merging remains\nlimited. In this work, we study model evolution through iterative merging,\ndrawing an analogy to biological evolution, and introduce the concept of model\nkinship, the degree of similarity or relatedness between LLMs. Through\ncomprehensive empirical analysis, we show that model kinship is closely linked\nto the performance improvements achieved by merging, providing a useful\ncriterion for selecting candidate models. Building on this insight, we propose\na new model merging strategy: Top-k Greedy Merging with Model Kinship, which\ncan improve benchmark performance. Specifically, we discover that incorporating\nmodel kinship as a guiding criterion enables continuous merging while\nmitigating performance degradation caused by local optima, thereby facilitating\nmore effective model evolution. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n", "link": "http://arxiv.org/abs/2410.12613v3", "date": "2025-09-23", "relevancy": 2.435, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Model%20Kinship%20for%20Merging%20Large%20Language%20Models&body=Title%3A%20Exploring%20Model%20Kinship%20for%20Merging%20Large%20Language%20Models%0AAuthor%3A%20Yedi%20Hu%20and%20Yunzhi%20Yao%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen%20and%20Shumin%20Deng%0AAbstract%3A%20%20%20Model%20merging%20has%20emerged%20as%20a%20key%20technique%20for%20enhancing%20the%20capabilities%0Aand%20efficiency%20of%20Large%20Language%20Models%20%28LLMs%29.%20The%20open-source%20community%20has%0Adriven%20model%20evolution%20by%20iteratively%20merging%20existing%20models%2C%20yet%20a%20principled%0Aunderstanding%20of%20the%20gains%20and%20underlying%20factors%20in%20model%20merging%20remains%0Alimited.%20In%20this%20work%2C%20we%20study%20model%20evolution%20through%20iterative%20merging%2C%0Adrawing%20an%20analogy%20to%20biological%20evolution%2C%20and%20introduce%20the%20concept%20of%20model%0Akinship%2C%20the%20degree%20of%20similarity%20or%20relatedness%20between%20LLMs.%20Through%0Acomprehensive%20empirical%20analysis%2C%20we%20show%20that%20model%20kinship%20is%20closely%20linked%0Ato%20the%20performance%20improvements%20achieved%20by%20merging%2C%20providing%20a%20useful%0Acriterion%20for%20selecting%20candidate%20models.%20Building%20on%20this%20insight%2C%20we%20propose%0Aa%20new%20model%20merging%20strategy%3A%20Top-k%20Greedy%20Merging%20with%20Model%20Kinship%2C%20which%0Acan%20improve%20benchmark%20performance.%20Specifically%2C%20we%20discover%20that%20incorporating%0Amodel%20kinship%20as%20a%20guiding%20criterion%20enables%20continuous%20merging%20while%0Amitigating%20performance%20degradation%20caused%20by%20local%20optima%2C%20thereby%20facilitating%0Amore%20effective%20model%20evolution.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/ModelKinship.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12613v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Model%2520Kinship%2520for%2520Merging%2520Large%2520Language%2520Models%26entry.906535625%3DYedi%2520Hu%2520and%2520Yunzhi%2520Yao%2520and%2520Ningyu%2520Zhang%2520and%2520Huajun%2520Chen%2520and%2520Shumin%2520Deng%26entry.1292438233%3D%2520%2520Model%2520merging%2520has%2520emerged%2520as%2520a%2520key%2520technique%2520for%2520enhancing%2520the%2520capabilities%250Aand%2520efficiency%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520The%2520open-source%2520community%2520has%250Adriven%2520model%2520evolution%2520by%2520iteratively%2520merging%2520existing%2520models%252C%2520yet%2520a%2520principled%250Aunderstanding%2520of%2520the%2520gains%2520and%2520underlying%2520factors%2520in%2520model%2520merging%2520remains%250Alimited.%2520In%2520this%2520work%252C%2520we%2520study%2520model%2520evolution%2520through%2520iterative%2520merging%252C%250Adrawing%2520an%2520analogy%2520to%2520biological%2520evolution%252C%2520and%2520introduce%2520the%2520concept%2520of%2520model%250Akinship%252C%2520the%2520degree%2520of%2520similarity%2520or%2520relatedness%2520between%2520LLMs.%2520Through%250Acomprehensive%2520empirical%2520analysis%252C%2520we%2520show%2520that%2520model%2520kinship%2520is%2520closely%2520linked%250Ato%2520the%2520performance%2520improvements%2520achieved%2520by%2520merging%252C%2520providing%2520a%2520useful%250Acriterion%2520for%2520selecting%2520candidate%2520models.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%250Aa%2520new%2520model%2520merging%2520strategy%253A%2520Top-k%2520Greedy%2520Merging%2520with%2520Model%2520Kinship%252C%2520which%250Acan%2520improve%2520benchmark%2520performance.%2520Specifically%252C%2520we%2520discover%2520that%2520incorporating%250Amodel%2520kinship%2520as%2520a%2520guiding%2520criterion%2520enables%2520continuous%2520merging%2520while%250Amitigating%2520performance%2520degradation%2520caused%2520by%2520local%2520optima%252C%2520thereby%2520facilitating%250Amore%2520effective%2520model%2520evolution.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/zjunlp/ModelKinship.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12613v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Model%20Kinship%20for%20Merging%20Large%20Language%20Models&entry.906535625=Yedi%20Hu%20and%20Yunzhi%20Yao%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen%20and%20Shumin%20Deng&entry.1292438233=%20%20Model%20merging%20has%20emerged%20as%20a%20key%20technique%20for%20enhancing%20the%20capabilities%0Aand%20efficiency%20of%20Large%20Language%20Models%20%28LLMs%29.%20The%20open-source%20community%20has%0Adriven%20model%20evolution%20by%20iteratively%20merging%20existing%20models%2C%20yet%20a%20principled%0Aunderstanding%20of%20the%20gains%20and%20underlying%20factors%20in%20model%20merging%20remains%0Alimited.%20In%20this%20work%2C%20we%20study%20model%20evolution%20through%20iterative%20merging%2C%0Adrawing%20an%20analogy%20to%20biological%20evolution%2C%20and%20introduce%20the%20concept%20of%20model%0Akinship%2C%20the%20degree%20of%20similarity%20or%20relatedness%20between%20LLMs.%20Through%0Acomprehensive%20empirical%20analysis%2C%20we%20show%20that%20model%20kinship%20is%20closely%20linked%0Ato%20the%20performance%20improvements%20achieved%20by%20merging%2C%20providing%20a%20useful%0Acriterion%20for%20selecting%20candidate%20models.%20Building%20on%20this%20insight%2C%20we%20propose%0Aa%20new%20model%20merging%20strategy%3A%20Top-k%20Greedy%20Merging%20with%20Model%20Kinship%2C%20which%0Acan%20improve%20benchmark%20performance.%20Specifically%2C%20we%20discover%20that%20incorporating%0Amodel%20kinship%20as%20a%20guiding%20criterion%20enables%20continuous%20merging%20while%0Amitigating%20performance%20degradation%20caused%20by%20local%20optima%2C%20thereby%20facilitating%0Amore%20effective%20model%20evolution.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/ModelKinship.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12613v3&entry.124074799=Read"},
{"title": "Single-stream Policy Optimization", "author": "Zhongwen Xu and Zihan Ding", "abstract": "  We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.\n", "link": "http://arxiv.org/abs/2509.13232v2", "date": "2025-09-23", "relevancy": 2.4336, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-stream%20Policy%20Optimization&body=Title%3A%20Single-stream%20Policy%20Optimization%0AAuthor%3A%20Zhongwen%20Xu%20and%20Zihan%20Ding%0AAbstract%3A%20%20%20We%20revisit%20policy-gradient%20optimization%20for%20Large%20Language%20Models%20%28LLMs%29%20from%0Aa%20single-stream%20perspective.%20Prevailing%20group-based%20methods%20like%20GRPO%20reduce%0Avariance%20with%20on-the-fly%20baselines%20but%20suffer%20from%20critical%20flaws%3A%20frequent%0Adegenerate%20groups%20erase%20learning%20signals%2C%20and%20synchronization%20barriers%20hinder%0Ascalability.%20We%20introduce%20Single-stream%20Policy%20Optimization%20%28SPO%29%2C%20which%0Aeliminates%20these%20issues%20by%20design.%20SPO%20replaces%20per-group%20baselines%20with%20a%0Apersistent%2C%20KL-adaptive%20value%20tracker%20and%20normalizes%20advantages%20globally%20across%0Athe%20batch%2C%20providing%20a%20stable%2C%20low-variance%20learning%20signal%20for%20every%20sample.%0ABeing%20group-free%2C%20SPO%20enables%20higher%20throughput%20and%20scales%20effectively%20in%0Along-horizon%20or%20tool-integrated%20settings%20where%20generation%20times%20vary.%0AFurthermore%2C%20the%20persistent%20value%20tracker%20naturally%20enables%20an%20adaptive%0Acurriculum%20via%20prioritized%20sampling.%20Experiments%20using%20Qwen3-8B%20show%20that%20SPO%0Aconverges%20more%20smoothly%20and%20attains%20higher%20accuracy%20than%20GRPO%2C%20while%0Aeliminating%20computation%20wasted%20on%20degenerate%20groups.%20Ablation%20studies%20confirm%0Athat%20SPO%27s%20gains%20stem%20from%20its%20principled%20approach%20to%20baseline%20estimation%20and%0Aadvantage%20normalization%2C%20offering%20a%20more%20robust%20and%20efficient%20path%20for%20LLM%0Areasoning.%20Across%20five%20hard%20math%20benchmarks%20with%20Qwen3%208B%2C%20SPO%20improves%20the%0Aaverage%20maj%4032%20by%20%2B3.4%20percentage%20points%20%28pp%29%20over%20GRPO%2C%20driven%20by%20substantial%0Aabsolute%20point%20gains%20on%20challenging%20datasets%2C%20including%20%2B7.3%20pp%20on%20BRUMO%2025%2C%0A%2B4.4%20pp%20on%20AIME%2025%2C%20%2B3.3%20pp%20on%20HMMT%2025%2C%20and%20achieves%20consistent%20relative%20gain%0Ain%20pass%40%24k%24%20across%20the%20evaluated%20%24k%24%20values.%20SPO%27s%20success%20challenges%20the%0Aprevailing%20trend%20of%20adding%20incidental%20complexity%20to%20RL%20algorithms%2C%20highlighting%0Aa%20path%20where%20fundamental%20principles%2C%20not%20architectural%20workarounds%2C%20drive%20the%0Anext%20wave%20of%20progress%20in%20LLM%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13232v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-stream%2520Policy%2520Optimization%26entry.906535625%3DZhongwen%2520Xu%2520and%2520Zihan%2520Ding%26entry.1292438233%3D%2520%2520We%2520revisit%2520policy-gradient%2520optimization%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520from%250Aa%2520single-stream%2520perspective.%2520Prevailing%2520group-based%2520methods%2520like%2520GRPO%2520reduce%250Avariance%2520with%2520on-the-fly%2520baselines%2520but%2520suffer%2520from%2520critical%2520flaws%253A%2520frequent%250Adegenerate%2520groups%2520erase%2520learning%2520signals%252C%2520and%2520synchronization%2520barriers%2520hinder%250Ascalability.%2520We%2520introduce%2520Single-stream%2520Policy%2520Optimization%2520%2528SPO%2529%252C%2520which%250Aeliminates%2520these%2520issues%2520by%2520design.%2520SPO%2520replaces%2520per-group%2520baselines%2520with%2520a%250Apersistent%252C%2520KL-adaptive%2520value%2520tracker%2520and%2520normalizes%2520advantages%2520globally%2520across%250Athe%2520batch%252C%2520providing%2520a%2520stable%252C%2520low-variance%2520learning%2520signal%2520for%2520every%2520sample.%250ABeing%2520group-free%252C%2520SPO%2520enables%2520higher%2520throughput%2520and%2520scales%2520effectively%2520in%250Along-horizon%2520or%2520tool-integrated%2520settings%2520where%2520generation%2520times%2520vary.%250AFurthermore%252C%2520the%2520persistent%2520value%2520tracker%2520naturally%2520enables%2520an%2520adaptive%250Acurriculum%2520via%2520prioritized%2520sampling.%2520Experiments%2520using%2520Qwen3-8B%2520show%2520that%2520SPO%250Aconverges%2520more%2520smoothly%2520and%2520attains%2520higher%2520accuracy%2520than%2520GRPO%252C%2520while%250Aeliminating%2520computation%2520wasted%2520on%2520degenerate%2520groups.%2520Ablation%2520studies%2520confirm%250Athat%2520SPO%2527s%2520gains%2520stem%2520from%2520its%2520principled%2520approach%2520to%2520baseline%2520estimation%2520and%250Aadvantage%2520normalization%252C%2520offering%2520a%2520more%2520robust%2520and%2520efficient%2520path%2520for%2520LLM%250Areasoning.%2520Across%2520five%2520hard%2520math%2520benchmarks%2520with%2520Qwen3%25208B%252C%2520SPO%2520improves%2520the%250Aaverage%2520maj%254032%2520by%2520%252B3.4%2520percentage%2520points%2520%2528pp%2529%2520over%2520GRPO%252C%2520driven%2520by%2520substantial%250Aabsolute%2520point%2520gains%2520on%2520challenging%2520datasets%252C%2520including%2520%252B7.3%2520pp%2520on%2520BRUMO%252025%252C%250A%252B4.4%2520pp%2520on%2520AIME%252025%252C%2520%252B3.3%2520pp%2520on%2520HMMT%252025%252C%2520and%2520achieves%2520consistent%2520relative%2520gain%250Ain%2520pass%2540%2524k%2524%2520across%2520the%2520evaluated%2520%2524k%2524%2520values.%2520SPO%2527s%2520success%2520challenges%2520the%250Aprevailing%2520trend%2520of%2520adding%2520incidental%2520complexity%2520to%2520RL%2520algorithms%252C%2520highlighting%250Aa%2520path%2520where%2520fundamental%2520principles%252C%2520not%2520architectural%2520workarounds%252C%2520drive%2520the%250Anext%2520wave%2520of%2520progress%2520in%2520LLM%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13232v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-stream%20Policy%20Optimization&entry.906535625=Zhongwen%20Xu%20and%20Zihan%20Ding&entry.1292438233=%20%20We%20revisit%20policy-gradient%20optimization%20for%20Large%20Language%20Models%20%28LLMs%29%20from%0Aa%20single-stream%20perspective.%20Prevailing%20group-based%20methods%20like%20GRPO%20reduce%0Avariance%20with%20on-the-fly%20baselines%20but%20suffer%20from%20critical%20flaws%3A%20frequent%0Adegenerate%20groups%20erase%20learning%20signals%2C%20and%20synchronization%20barriers%20hinder%0Ascalability.%20We%20introduce%20Single-stream%20Policy%20Optimization%20%28SPO%29%2C%20which%0Aeliminates%20these%20issues%20by%20design.%20SPO%20replaces%20per-group%20baselines%20with%20a%0Apersistent%2C%20KL-adaptive%20value%20tracker%20and%20normalizes%20advantages%20globally%20across%0Athe%20batch%2C%20providing%20a%20stable%2C%20low-variance%20learning%20signal%20for%20every%20sample.%0ABeing%20group-free%2C%20SPO%20enables%20higher%20throughput%20and%20scales%20effectively%20in%0Along-horizon%20or%20tool-integrated%20settings%20where%20generation%20times%20vary.%0AFurthermore%2C%20the%20persistent%20value%20tracker%20naturally%20enables%20an%20adaptive%0Acurriculum%20via%20prioritized%20sampling.%20Experiments%20using%20Qwen3-8B%20show%20that%20SPO%0Aconverges%20more%20smoothly%20and%20attains%20higher%20accuracy%20than%20GRPO%2C%20while%0Aeliminating%20computation%20wasted%20on%20degenerate%20groups.%20Ablation%20studies%20confirm%0Athat%20SPO%27s%20gains%20stem%20from%20its%20principled%20approach%20to%20baseline%20estimation%20and%0Aadvantage%20normalization%2C%20offering%20a%20more%20robust%20and%20efficient%20path%20for%20LLM%0Areasoning.%20Across%20five%20hard%20math%20benchmarks%20with%20Qwen3%208B%2C%20SPO%20improves%20the%0Aaverage%20maj%4032%20by%20%2B3.4%20percentage%20points%20%28pp%29%20over%20GRPO%2C%20driven%20by%20substantial%0Aabsolute%20point%20gains%20on%20challenging%20datasets%2C%20including%20%2B7.3%20pp%20on%20BRUMO%2025%2C%0A%2B4.4%20pp%20on%20AIME%2025%2C%20%2B3.3%20pp%20on%20HMMT%2025%2C%20and%20achieves%20consistent%20relative%20gain%0Ain%20pass%40%24k%24%20across%20the%20evaluated%20%24k%24%20values.%20SPO%27s%20success%20challenges%20the%0Aprevailing%20trend%20of%20adding%20incidental%20complexity%20to%20RL%20algorithms%2C%20highlighting%0Aa%20path%20where%20fundamental%20principles%2C%20not%20architectural%20workarounds%2C%20drive%20the%0Anext%20wave%20of%20progress%20in%20LLM%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13232v2&entry.124074799=Read"},
{"title": "Video Killed the Energy Budget: Characterizing the Latency and Power\n  Regimes of Open Text-to-Video Models", "author": "Julien Delavande and Regis Pierrard and Sasha Luccioni", "abstract": "  Recent advances in text-to-video (T2V) generation have enabled the creation\nof high-fidelity, temporally coherent clips from natural language prompts. Yet\nthese systems come with significant computational costs, and their energy\ndemands remain poorly understood. In this paper, we present a systematic study\nof the latency and energy consumption of state-of-the-art open-source T2V\nmodels. We first develop a compute-bound analytical model that predicts scaling\nlaws with respect to spatial resolution, temporal length, and denoising steps.\nWe then validate these predictions through fine-grained experiments on\nWAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and\nlinear scaling with the number of denoising steps. Finally, we extend our\nanalysis to six diverse T2V models, comparing their runtime and energy profiles\nunder default settings. Our results provide both a benchmark reference and\npractical insights for designing and deploying more sustainable generative\nvideo systems.\n", "link": "http://arxiv.org/abs/2509.19222v1", "date": "2025-09-23", "relevancy": 2.4255, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6212}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6113}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Killed%20the%20Energy%20Budget%3A%20Characterizing%20the%20Latency%20and%20Power%0A%20%20Regimes%20of%20Open%20Text-to-Video%20Models&body=Title%3A%20Video%20Killed%20the%20Energy%20Budget%3A%20Characterizing%20the%20Latency%20and%20Power%0A%20%20Regimes%20of%20Open%20Text-to-Video%20Models%0AAuthor%3A%20Julien%20Delavande%20and%20Regis%20Pierrard%20and%20Sasha%20Luccioni%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20generation%20have%20enabled%20the%20creation%0Aof%20high-fidelity%2C%20temporally%20coherent%20clips%20from%20natural%20language%20prompts.%20Yet%0Athese%20systems%20come%20with%20significant%20computational%20costs%2C%20and%20their%20energy%0Ademands%20remain%20poorly%20understood.%20In%20this%20paper%2C%20we%20present%20a%20systematic%20study%0Aof%20the%20latency%20and%20energy%20consumption%20of%20state-of-the-art%20open-source%20T2V%0Amodels.%20We%20first%20develop%20a%20compute-bound%20analytical%20model%20that%20predicts%20scaling%0Alaws%20with%20respect%20to%20spatial%20resolution%2C%20temporal%20length%2C%20and%20denoising%20steps.%0AWe%20then%20validate%20these%20predictions%20through%20fine-grained%20experiments%20on%0AWAN2.1-T2V%2C%20showing%20quadratic%20growth%20with%20spatial%20and%20temporal%20dimensions%2C%20and%0Alinear%20scaling%20with%20the%20number%20of%20denoising%20steps.%20Finally%2C%20we%20extend%20our%0Aanalysis%20to%20six%20diverse%20T2V%20models%2C%20comparing%20their%20runtime%20and%20energy%20profiles%0Aunder%20default%20settings.%20Our%20results%20provide%20both%20a%20benchmark%20reference%20and%0Apractical%20insights%20for%20designing%20and%20deploying%20more%20sustainable%20generative%0Avideo%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Killed%2520the%2520Energy%2520Budget%253A%2520Characterizing%2520the%2520Latency%2520and%2520Power%250A%2520%2520Regimes%2520of%2520Open%2520Text-to-Video%2520Models%26entry.906535625%3DJulien%2520Delavande%2520and%2520Regis%2520Pierrard%2520and%2520Sasha%2520Luccioni%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-video%2520%2528T2V%2529%2520generation%2520have%2520enabled%2520the%2520creation%250Aof%2520high-fidelity%252C%2520temporally%2520coherent%2520clips%2520from%2520natural%2520language%2520prompts.%2520Yet%250Athese%2520systems%2520come%2520with%2520significant%2520computational%2520costs%252C%2520and%2520their%2520energy%250Ademands%2520remain%2520poorly%2520understood.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520systematic%2520study%250Aof%2520the%2520latency%2520and%2520energy%2520consumption%2520of%2520state-of-the-art%2520open-source%2520T2V%250Amodels.%2520We%2520first%2520develop%2520a%2520compute-bound%2520analytical%2520model%2520that%2520predicts%2520scaling%250Alaws%2520with%2520respect%2520to%2520spatial%2520resolution%252C%2520temporal%2520length%252C%2520and%2520denoising%2520steps.%250AWe%2520then%2520validate%2520these%2520predictions%2520through%2520fine-grained%2520experiments%2520on%250AWAN2.1-T2V%252C%2520showing%2520quadratic%2520growth%2520with%2520spatial%2520and%2520temporal%2520dimensions%252C%2520and%250Alinear%2520scaling%2520with%2520the%2520number%2520of%2520denoising%2520steps.%2520Finally%252C%2520we%2520extend%2520our%250Aanalysis%2520to%2520six%2520diverse%2520T2V%2520models%252C%2520comparing%2520their%2520runtime%2520and%2520energy%2520profiles%250Aunder%2520default%2520settings.%2520Our%2520results%2520provide%2520both%2520a%2520benchmark%2520reference%2520and%250Apractical%2520insights%2520for%2520designing%2520and%2520deploying%2520more%2520sustainable%2520generative%250Avideo%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Killed%20the%20Energy%20Budget%3A%20Characterizing%20the%20Latency%20and%20Power%0A%20%20Regimes%20of%20Open%20Text-to-Video%20Models&entry.906535625=Julien%20Delavande%20and%20Regis%20Pierrard%20and%20Sasha%20Luccioni&entry.1292438233=%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20generation%20have%20enabled%20the%20creation%0Aof%20high-fidelity%2C%20temporally%20coherent%20clips%20from%20natural%20language%20prompts.%20Yet%0Athese%20systems%20come%20with%20significant%20computational%20costs%2C%20and%20their%20energy%0Ademands%20remain%20poorly%20understood.%20In%20this%20paper%2C%20we%20present%20a%20systematic%20study%0Aof%20the%20latency%20and%20energy%20consumption%20of%20state-of-the-art%20open-source%20T2V%0Amodels.%20We%20first%20develop%20a%20compute-bound%20analytical%20model%20that%20predicts%20scaling%0Alaws%20with%20respect%20to%20spatial%20resolution%2C%20temporal%20length%2C%20and%20denoising%20steps.%0AWe%20then%20validate%20these%20predictions%20through%20fine-grained%20experiments%20on%0AWAN2.1-T2V%2C%20showing%20quadratic%20growth%20with%20spatial%20and%20temporal%20dimensions%2C%20and%0Alinear%20scaling%20with%20the%20number%20of%20denoising%20steps.%20Finally%2C%20we%20extend%20our%0Aanalysis%20to%20six%20diverse%20T2V%20models%2C%20comparing%20their%20runtime%20and%20energy%20profiles%0Aunder%20default%20settings.%20Our%20results%20provide%20both%20a%20benchmark%20reference%20and%0Apractical%20insights%20for%20designing%20and%20deploying%20more%20sustainable%20generative%0Avideo%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19222v1&entry.124074799=Read"},
{"title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer", "author": "Kangmin Kim and Seunghyeok Back and Geonhyup Lee and Sangbeom Lee and Sangjun Noh and Kyoobin Lee", "abstract": "  Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer\n", "link": "http://arxiv.org/abs/2509.19142v1", "date": "2025-09-23", "relevancy": 2.4186, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6918}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5493}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiGraspFormer%3A%20End-to-End%20Bimanual%20Grasp%20Transformer&body=Title%3A%20BiGraspFormer%3A%20End-to-End%20Bimanual%20Grasp%20Transformer%0AAuthor%3A%20Kangmin%20Kim%20and%20Seunghyeok%20Back%20and%20Geonhyup%20Lee%20and%20Sangbeom%20Lee%20and%20Sangjun%20Noh%20and%20Kyoobin%20Lee%0AAbstract%3A%20%20%20Bimanual%20grasping%20is%20essential%20for%20robots%20to%20handle%20large%20and%20complex%0Aobjects.%20However%2C%20existing%20methods%20either%20focus%20solely%20on%20single-arm%20grasping%0Aor%20employ%20separate%20grasp%20generation%20and%20bimanual%20evaluation%20stages%2C%20leading%20to%0Acoordination%20problems%20including%20collision%20risks%20and%20unbalanced%20force%0Adistribution.%20To%20address%20these%20limitations%2C%20we%20propose%20BiGraspFormer%2C%20a%20unified%0Aend-to-end%20transformer%20framework%20that%20directly%20generates%20coordinated%20bimanual%0Agrasps%20from%20object%20point%20clouds.%20Our%20key%20idea%20is%20the%20Single-Guided%20Bimanual%0A%28SGB%29%20strategy%2C%20which%20first%20generates%20diverse%20single%20grasp%20candidates%20using%20a%0Atransformer%20decoder%2C%20then%20leverages%20their%20learned%20features%20through%20specialized%0Aattention%20mechanisms%20to%20jointly%20predict%20bimanual%20poses%20and%20quality%20scores.%20This%0Aconditioning%20strategy%20reduces%20the%20complexity%20of%20the%2012-DoF%20search%20space%20while%0Aensuring%20coordinated%20bimanual%20manipulation.%20Comprehensive%20simulation%0Aexperiments%20and%20real-world%20validation%20demonstrate%20that%20BiGraspFormer%0Aconsistently%20outperforms%20existing%20methods%20while%20maintaining%20efficient%20inference%0Aspeed%20%28%3C0.05s%29%2C%20confirming%20the%20effectiveness%20of%20our%20framework.%20Code%20and%0Asupplementary%20materials%20are%20available%20at%20https%3A//sites.google.com/bigraspformer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiGraspFormer%253A%2520End-to-End%2520Bimanual%2520Grasp%2520Transformer%26entry.906535625%3DKangmin%2520Kim%2520and%2520Seunghyeok%2520Back%2520and%2520Geonhyup%2520Lee%2520and%2520Sangbeom%2520Lee%2520and%2520Sangjun%2520Noh%2520and%2520Kyoobin%2520Lee%26entry.1292438233%3D%2520%2520Bimanual%2520grasping%2520is%2520essential%2520for%2520robots%2520to%2520handle%2520large%2520and%2520complex%250Aobjects.%2520However%252C%2520existing%2520methods%2520either%2520focus%2520solely%2520on%2520single-arm%2520grasping%250Aor%2520employ%2520separate%2520grasp%2520generation%2520and%2520bimanual%2520evaluation%2520stages%252C%2520leading%2520to%250Acoordination%2520problems%2520including%2520collision%2520risks%2520and%2520unbalanced%2520force%250Adistribution.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520BiGraspFormer%252C%2520a%2520unified%250Aend-to-end%2520transformer%2520framework%2520that%2520directly%2520generates%2520coordinated%2520bimanual%250Agrasps%2520from%2520object%2520point%2520clouds.%2520Our%2520key%2520idea%2520is%2520the%2520Single-Guided%2520Bimanual%250A%2528SGB%2529%2520strategy%252C%2520which%2520first%2520generates%2520diverse%2520single%2520grasp%2520candidates%2520using%2520a%250Atransformer%2520decoder%252C%2520then%2520leverages%2520their%2520learned%2520features%2520through%2520specialized%250Aattention%2520mechanisms%2520to%2520jointly%2520predict%2520bimanual%2520poses%2520and%2520quality%2520scores.%2520This%250Aconditioning%2520strategy%2520reduces%2520the%2520complexity%2520of%2520the%252012-DoF%2520search%2520space%2520while%250Aensuring%2520coordinated%2520bimanual%2520manipulation.%2520Comprehensive%2520simulation%250Aexperiments%2520and%2520real-world%2520validation%2520demonstrate%2520that%2520BiGraspFormer%250Aconsistently%2520outperforms%2520existing%2520methods%2520while%2520maintaining%2520efficient%2520inference%250Aspeed%2520%2528%253C0.05s%2529%252C%2520confirming%2520the%2520effectiveness%2520of%2520our%2520framework.%2520Code%2520and%250Asupplementary%2520materials%2520are%2520available%2520at%2520https%253A//sites.google.com/bigraspformer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiGraspFormer%3A%20End-to-End%20Bimanual%20Grasp%20Transformer&entry.906535625=Kangmin%20Kim%20and%20Seunghyeok%20Back%20and%20Geonhyup%20Lee%20and%20Sangbeom%20Lee%20and%20Sangjun%20Noh%20and%20Kyoobin%20Lee&entry.1292438233=%20%20Bimanual%20grasping%20is%20essential%20for%20robots%20to%20handle%20large%20and%20complex%0Aobjects.%20However%2C%20existing%20methods%20either%20focus%20solely%20on%20single-arm%20grasping%0Aor%20employ%20separate%20grasp%20generation%20and%20bimanual%20evaluation%20stages%2C%20leading%20to%0Acoordination%20problems%20including%20collision%20risks%20and%20unbalanced%20force%0Adistribution.%20To%20address%20these%20limitations%2C%20we%20propose%20BiGraspFormer%2C%20a%20unified%0Aend-to-end%20transformer%20framework%20that%20directly%20generates%20coordinated%20bimanual%0Agrasps%20from%20object%20point%20clouds.%20Our%20key%20idea%20is%20the%20Single-Guided%20Bimanual%0A%28SGB%29%20strategy%2C%20which%20first%20generates%20diverse%20single%20grasp%20candidates%20using%20a%0Atransformer%20decoder%2C%20then%20leverages%20their%20learned%20features%20through%20specialized%0Aattention%20mechanisms%20to%20jointly%20predict%20bimanual%20poses%20and%20quality%20scores.%20This%0Aconditioning%20strategy%20reduces%20the%20complexity%20of%20the%2012-DoF%20search%20space%20while%0Aensuring%20coordinated%20bimanual%20manipulation.%20Comprehensive%20simulation%0Aexperiments%20and%20real-world%20validation%20demonstrate%20that%20BiGraspFormer%0Aconsistently%20outperforms%20existing%20methods%20while%20maintaining%20efficient%20inference%0Aspeed%20%28%3C0.05s%29%2C%20confirming%20the%20effectiveness%20of%20our%20framework.%20Code%20and%0Asupplementary%20materials%20are%20available%20at%20https%3A//sites.google.com/bigraspformer%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19142v1&entry.124074799=Read"},
{"title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven\n  Perspective", "author": "Songsong Yu and Yuxin Chen and Hao Ju and Lianjie Jia and Fuxi Zhang and Shaofei Huang and Yuhan Wu and Rundi Cui and Binghao Ran and Zaibin Zhang and Zhedong Zheng and Zhipeng Zhang and Yifan Wang and Lin Song and Lijun Wang and Yanwei Li and Ying Shan and Huchuan Lu", "abstract": "  Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.\n", "link": "http://arxiv.org/abs/2509.18905v1", "date": "2025-09-23", "relevancy": 2.4093, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6183}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Far%20are%20VLMs%20from%20Visual%20Spatial%20Intelligence%3F%20A%20Benchmark-Driven%0A%20%20Perspective&body=Title%3A%20How%20Far%20are%20VLMs%20from%20Visual%20Spatial%20Intelligence%3F%20A%20Benchmark-Driven%0A%20%20Perspective%0AAuthor%3A%20Songsong%20Yu%20and%20Yuxin%20Chen%20and%20Hao%20Ju%20and%20Lianjie%20Jia%20and%20Fuxi%20Zhang%20and%20Shaofei%20Huang%20and%20Yuhan%20Wu%20and%20Rundi%20Cui%20and%20Binghao%20Ran%20and%20Zaibin%20Zhang%20and%20Zhedong%20Zheng%20and%20Zhipeng%20Zhang%20and%20Yifan%20Wang%20and%20Lin%20Song%20and%20Lijun%20Wang%20and%20Yanwei%20Li%20and%20Ying%20Shan%20and%20Huchuan%20Lu%0AAbstract%3A%20%20%20Visual%20Spatial%20Reasoning%20%28VSR%29%20is%20a%20core%20human%20cognitive%20ability%20and%20a%0Acritical%20requirement%20for%20advancing%20embodied%20intelligence%20and%20autonomous%0Asystems.%20Despite%20recent%20progress%20in%20Vision-Language%20Models%20%28VLMs%29%2C%20achieving%0Ahuman-level%20VSR%20remains%20highly%20challenging%20due%20to%20the%20complexity%20of%0Arepresenting%20and%20reasoning%20over%20three-dimensional%20space.%20In%20this%20paper%2C%20we%0Apresent%20a%20systematic%20investigation%20of%20VSR%20in%20VLMs%2C%20encompassing%20a%20review%20of%0Aexisting%20methodologies%20across%20input%20modalities%2C%20model%20architectures%2C%20training%0Astrategies%2C%20and%20reasoning%20mechanisms.%20Furthermore%2C%20we%20categorize%20spatial%0Aintelligence%20into%20three%20levels%20of%20capability%2C%20ie%2C%20basic%20perception%2C%20spatial%0Aunderstanding%2C%20spatial%20planning%2C%20and%20curate%20SIBench%2C%20a%20spatial%20intelligence%0Abenchmark%20encompassing%20nearly%2020%20open-source%20datasets%20across%2023%20task%20settings.%0AExperiments%20with%20state-of-the-art%20VLMs%20reveal%20a%20pronounced%20gap%20between%0Aperception%20and%20reasoning%2C%20as%20models%20show%20competence%20in%20basic%20perceptual%20tasks%0Abut%20consistently%20underperform%20in%20understanding%20and%20planning%20tasks%2C%20particularly%0Ain%20numerical%20estimation%2C%20multi-view%20reasoning%2C%20temporal%20dynamics%2C%20and%20spatial%0Aimagination.%20These%20findings%20underscore%20the%20substantial%20challenges%20that%20remain%0Ain%20achieving%20spatial%20intelligence%2C%20while%20providing%20both%20a%20systematic%20roadmap%0Aand%20a%20comprehensive%20benchmark%20to%20drive%20future%20research%20in%20the%20field.%20The%0Arelated%20resources%20of%20this%20study%20are%20accessible%20at%0Ahttps%3A//sibench.github.io/Awesome-Visual-Spatial-Reasoning/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Far%2520are%2520VLMs%2520from%2520Visual%2520Spatial%2520Intelligence%253F%2520A%2520Benchmark-Driven%250A%2520%2520Perspective%26entry.906535625%3DSongsong%2520Yu%2520and%2520Yuxin%2520Chen%2520and%2520Hao%2520Ju%2520and%2520Lianjie%2520Jia%2520and%2520Fuxi%2520Zhang%2520and%2520Shaofei%2520Huang%2520and%2520Yuhan%2520Wu%2520and%2520Rundi%2520Cui%2520and%2520Binghao%2520Ran%2520and%2520Zaibin%2520Zhang%2520and%2520Zhedong%2520Zheng%2520and%2520Zhipeng%2520Zhang%2520and%2520Yifan%2520Wang%2520and%2520Lin%2520Song%2520and%2520Lijun%2520Wang%2520and%2520Yanwei%2520Li%2520and%2520Ying%2520Shan%2520and%2520Huchuan%2520Lu%26entry.1292438233%3D%2520%2520Visual%2520Spatial%2520Reasoning%2520%2528VSR%2529%2520is%2520a%2520core%2520human%2520cognitive%2520ability%2520and%2520a%250Acritical%2520requirement%2520for%2520advancing%2520embodied%2520intelligence%2520and%2520autonomous%250Asystems.%2520Despite%2520recent%2520progress%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520achieving%250Ahuman-level%2520VSR%2520remains%2520highly%2520challenging%2520due%2520to%2520the%2520complexity%2520of%250Arepresenting%2520and%2520reasoning%2520over%2520three-dimensional%2520space.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520systematic%2520investigation%2520of%2520VSR%2520in%2520VLMs%252C%2520encompassing%2520a%2520review%2520of%250Aexisting%2520methodologies%2520across%2520input%2520modalities%252C%2520model%2520architectures%252C%2520training%250Astrategies%252C%2520and%2520reasoning%2520mechanisms.%2520Furthermore%252C%2520we%2520categorize%2520spatial%250Aintelligence%2520into%2520three%2520levels%2520of%2520capability%252C%2520ie%252C%2520basic%2520perception%252C%2520spatial%250Aunderstanding%252C%2520spatial%2520planning%252C%2520and%2520curate%2520SIBench%252C%2520a%2520spatial%2520intelligence%250Abenchmark%2520encompassing%2520nearly%252020%2520open-source%2520datasets%2520across%252023%2520task%2520settings.%250AExperiments%2520with%2520state-of-the-art%2520VLMs%2520reveal%2520a%2520pronounced%2520gap%2520between%250Aperception%2520and%2520reasoning%252C%2520as%2520models%2520show%2520competence%2520in%2520basic%2520perceptual%2520tasks%250Abut%2520consistently%2520underperform%2520in%2520understanding%2520and%2520planning%2520tasks%252C%2520particularly%250Ain%2520numerical%2520estimation%252C%2520multi-view%2520reasoning%252C%2520temporal%2520dynamics%252C%2520and%2520spatial%250Aimagination.%2520These%2520findings%2520underscore%2520the%2520substantial%2520challenges%2520that%2520remain%250Ain%2520achieving%2520spatial%2520intelligence%252C%2520while%2520providing%2520both%2520a%2520systematic%2520roadmap%250Aand%2520a%2520comprehensive%2520benchmark%2520to%2520drive%2520future%2520research%2520in%2520the%2520field.%2520The%250Arelated%2520resources%2520of%2520this%2520study%2520are%2520accessible%2520at%250Ahttps%253A//sibench.github.io/Awesome-Visual-Spatial-Reasoning/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Far%20are%20VLMs%20from%20Visual%20Spatial%20Intelligence%3F%20A%20Benchmark-Driven%0A%20%20Perspective&entry.906535625=Songsong%20Yu%20and%20Yuxin%20Chen%20and%20Hao%20Ju%20and%20Lianjie%20Jia%20and%20Fuxi%20Zhang%20and%20Shaofei%20Huang%20and%20Yuhan%20Wu%20and%20Rundi%20Cui%20and%20Binghao%20Ran%20and%20Zaibin%20Zhang%20and%20Zhedong%20Zheng%20and%20Zhipeng%20Zhang%20and%20Yifan%20Wang%20and%20Lin%20Song%20and%20Lijun%20Wang%20and%20Yanwei%20Li%20and%20Ying%20Shan%20and%20Huchuan%20Lu&entry.1292438233=%20%20Visual%20Spatial%20Reasoning%20%28VSR%29%20is%20a%20core%20human%20cognitive%20ability%20and%20a%0Acritical%20requirement%20for%20advancing%20embodied%20intelligence%20and%20autonomous%0Asystems.%20Despite%20recent%20progress%20in%20Vision-Language%20Models%20%28VLMs%29%2C%20achieving%0Ahuman-level%20VSR%20remains%20highly%20challenging%20due%20to%20the%20complexity%20of%0Arepresenting%20and%20reasoning%20over%20three-dimensional%20space.%20In%20this%20paper%2C%20we%0Apresent%20a%20systematic%20investigation%20of%20VSR%20in%20VLMs%2C%20encompassing%20a%20review%20of%0Aexisting%20methodologies%20across%20input%20modalities%2C%20model%20architectures%2C%20training%0Astrategies%2C%20and%20reasoning%20mechanisms.%20Furthermore%2C%20we%20categorize%20spatial%0Aintelligence%20into%20three%20levels%20of%20capability%2C%20ie%2C%20basic%20perception%2C%20spatial%0Aunderstanding%2C%20spatial%20planning%2C%20and%20curate%20SIBench%2C%20a%20spatial%20intelligence%0Abenchmark%20encompassing%20nearly%2020%20open-source%20datasets%20across%2023%20task%20settings.%0AExperiments%20with%20state-of-the-art%20VLMs%20reveal%20a%20pronounced%20gap%20between%0Aperception%20and%20reasoning%2C%20as%20models%20show%20competence%20in%20basic%20perceptual%20tasks%0Abut%20consistently%20underperform%20in%20understanding%20and%20planning%20tasks%2C%20particularly%0Ain%20numerical%20estimation%2C%20multi-view%20reasoning%2C%20temporal%20dynamics%2C%20and%20spatial%0Aimagination.%20These%20findings%20underscore%20the%20substantial%20challenges%20that%20remain%0Ain%20achieving%20spatial%20intelligence%2C%20while%20providing%20both%20a%20systematic%20roadmap%0Aand%20a%20comprehensive%20benchmark%20to%20drive%20future%20research%20in%20the%20field.%20The%0Arelated%20resources%20of%20this%20study%20are%20accessible%20at%0Ahttps%3A//sibench.github.io/Awesome-Visual-Spatial-Reasoning/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18905v1&entry.124074799=Read"},
{"title": "Occlusion-Aware Consistent Model Predictive Control for Robot Navigation\n  in Occluded Obstacle-Dense Environments", "author": "Minzhe Zheng and Lei Zheng and Lei Zhu and Jun Ma", "abstract": "  Ensuring safety and motion consistency for robot navigation in occluded,\nobstacle-dense environments is a critical challenge. In this context, this\nstudy presents an occlusion-aware Consistent Model Predictive Control (CMPC)\nstrategy. To account for the occluded obstacles, it incorporates adjustable\nrisk regions that represent their potential future locations. Subsequently,\ndynamic risk boundary constraints are developed online to ensure safety. The\nCMPC then constructs multiple locally optimal trajectory branches (each\ntailored to different risk regions) to strike a balance between safety and\nperformance. A shared consensus segment is generated to ensure smooth\ntransitions between branches without significant velocity fluctuations, further\npreserving motion consistency. To facilitate high computational efficiency and\nensure coordination across local trajectories, we use the alternating direction\nmethod of multipliers (ADMM) to decompose the CMPC into manageable sub-problems\nfor parallel solving. The proposed strategy is validated through simulations\nand real-world experiments on an Ackermann-steering robot platform. The results\ndemonstrate the effectiveness of the proposed CMPC strategy through comparisons\nwith baseline approaches in occluded, obstacle-dense environments.\n", "link": "http://arxiv.org/abs/2503.04563v3", "date": "2025-09-23", "relevancy": 2.4071, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6101}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6062}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occlusion-Aware%20Consistent%20Model%20Predictive%20Control%20for%20Robot%20Navigation%0A%20%20in%20Occluded%20Obstacle-Dense%20Environments&body=Title%3A%20Occlusion-Aware%20Consistent%20Model%20Predictive%20Control%20for%20Robot%20Navigation%0A%20%20in%20Occluded%20Obstacle-Dense%20Environments%0AAuthor%3A%20Minzhe%20Zheng%20and%20Lei%20Zheng%20and%20Lei%20Zhu%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Ensuring%20safety%20and%20motion%20consistency%20for%20robot%20navigation%20in%20occluded%2C%0Aobstacle-dense%20environments%20is%20a%20critical%20challenge.%20In%20this%20context%2C%20this%0Astudy%20presents%20an%20occlusion-aware%20Consistent%20Model%20Predictive%20Control%20%28CMPC%29%0Astrategy.%20To%20account%20for%20the%20occluded%20obstacles%2C%20it%20incorporates%20adjustable%0Arisk%20regions%20that%20represent%20their%20potential%20future%20locations.%20Subsequently%2C%0Adynamic%20risk%20boundary%20constraints%20are%20developed%20online%20to%20ensure%20safety.%20The%0ACMPC%20then%20constructs%20multiple%20locally%20optimal%20trajectory%20branches%20%28each%0Atailored%20to%20different%20risk%20regions%29%20to%20strike%20a%20balance%20between%20safety%20and%0Aperformance.%20A%20shared%20consensus%20segment%20is%20generated%20to%20ensure%20smooth%0Atransitions%20between%20branches%20without%20significant%20velocity%20fluctuations%2C%20further%0Apreserving%20motion%20consistency.%20To%20facilitate%20high%20computational%20efficiency%20and%0Aensure%20coordination%20across%20local%20trajectories%2C%20we%20use%20the%20alternating%20direction%0Amethod%20of%20multipliers%20%28ADMM%29%20to%20decompose%20the%20CMPC%20into%20manageable%20sub-problems%0Afor%20parallel%20solving.%20The%20proposed%20strategy%20is%20validated%20through%20simulations%0Aand%20real-world%20experiments%20on%20an%20Ackermann-steering%20robot%20platform.%20The%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20CMPC%20strategy%20through%20comparisons%0Awith%20baseline%20approaches%20in%20occluded%2C%20obstacle-dense%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04563v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOcclusion-Aware%2520Consistent%2520Model%2520Predictive%2520Control%2520for%2520Robot%2520Navigation%250A%2520%2520in%2520Occluded%2520Obstacle-Dense%2520Environments%26entry.906535625%3DMinzhe%2520Zheng%2520and%2520Lei%2520Zheng%2520and%2520Lei%2520Zhu%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Ensuring%2520safety%2520and%2520motion%2520consistency%2520for%2520robot%2520navigation%2520in%2520occluded%252C%250Aobstacle-dense%2520environments%2520is%2520a%2520critical%2520challenge.%2520In%2520this%2520context%252C%2520this%250Astudy%2520presents%2520an%2520occlusion-aware%2520Consistent%2520Model%2520Predictive%2520Control%2520%2528CMPC%2529%250Astrategy.%2520To%2520account%2520for%2520the%2520occluded%2520obstacles%252C%2520it%2520incorporates%2520adjustable%250Arisk%2520regions%2520that%2520represent%2520their%2520potential%2520future%2520locations.%2520Subsequently%252C%250Adynamic%2520risk%2520boundary%2520constraints%2520are%2520developed%2520online%2520to%2520ensure%2520safety.%2520The%250ACMPC%2520then%2520constructs%2520multiple%2520locally%2520optimal%2520trajectory%2520branches%2520%2528each%250Atailored%2520to%2520different%2520risk%2520regions%2529%2520to%2520strike%2520a%2520balance%2520between%2520safety%2520and%250Aperformance.%2520A%2520shared%2520consensus%2520segment%2520is%2520generated%2520to%2520ensure%2520smooth%250Atransitions%2520between%2520branches%2520without%2520significant%2520velocity%2520fluctuations%252C%2520further%250Apreserving%2520motion%2520consistency.%2520To%2520facilitate%2520high%2520computational%2520efficiency%2520and%250Aensure%2520coordination%2520across%2520local%2520trajectories%252C%2520we%2520use%2520the%2520alternating%2520direction%250Amethod%2520of%2520multipliers%2520%2528ADMM%2529%2520to%2520decompose%2520the%2520CMPC%2520into%2520manageable%2520sub-problems%250Afor%2520parallel%2520solving.%2520The%2520proposed%2520strategy%2520is%2520validated%2520through%2520simulations%250Aand%2520real-world%2520experiments%2520on%2520an%2520Ackermann-steering%2520robot%2520platform.%2520The%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520CMPC%2520strategy%2520through%2520comparisons%250Awith%2520baseline%2520approaches%2520in%2520occluded%252C%2520obstacle-dense%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04563v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occlusion-Aware%20Consistent%20Model%20Predictive%20Control%20for%20Robot%20Navigation%0A%20%20in%20Occluded%20Obstacle-Dense%20Environments&entry.906535625=Minzhe%20Zheng%20and%20Lei%20Zheng%20and%20Lei%20Zhu%20and%20Jun%20Ma&entry.1292438233=%20%20Ensuring%20safety%20and%20motion%20consistency%20for%20robot%20navigation%20in%20occluded%2C%0Aobstacle-dense%20environments%20is%20a%20critical%20challenge.%20In%20this%20context%2C%20this%0Astudy%20presents%20an%20occlusion-aware%20Consistent%20Model%20Predictive%20Control%20%28CMPC%29%0Astrategy.%20To%20account%20for%20the%20occluded%20obstacles%2C%20it%20incorporates%20adjustable%0Arisk%20regions%20that%20represent%20their%20potential%20future%20locations.%20Subsequently%2C%0Adynamic%20risk%20boundary%20constraints%20are%20developed%20online%20to%20ensure%20safety.%20The%0ACMPC%20then%20constructs%20multiple%20locally%20optimal%20trajectory%20branches%20%28each%0Atailored%20to%20different%20risk%20regions%29%20to%20strike%20a%20balance%20between%20safety%20and%0Aperformance.%20A%20shared%20consensus%20segment%20is%20generated%20to%20ensure%20smooth%0Atransitions%20between%20branches%20without%20significant%20velocity%20fluctuations%2C%20further%0Apreserving%20motion%20consistency.%20To%20facilitate%20high%20computational%20efficiency%20and%0Aensure%20coordination%20across%20local%20trajectories%2C%20we%20use%20the%20alternating%20direction%0Amethod%20of%20multipliers%20%28ADMM%29%20to%20decompose%20the%20CMPC%20into%20manageable%20sub-problems%0Afor%20parallel%20solving.%20The%20proposed%20strategy%20is%20validated%20through%20simulations%0Aand%20real-world%20experiments%20on%20an%20Ackermann-steering%20robot%20platform.%20The%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20CMPC%20strategy%20through%20comparisons%0Awith%20baseline%20approaches%20in%20occluded%2C%20obstacle-dense%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04563v3&entry.124074799=Read"},
{"title": "Hierarchical Federated Learning for Social Network with Mobility", "author": "Zeyu Chen and Wen Chen and Jun Li and Qingqing Wu and Ming Ding and Xuefeng Han and Xiumei Deng and Liwei Wang", "abstract": "  Federated Learning (FL) offers a decentralized solution that allows\ncollaborative local model training and global aggregation, thereby protecting\ndata privacy. In conventional FL frameworks, data privacy is typically\npreserved under the assumption that local data remains absolutely private,\nwhereas the mobility of clients is frequently neglected in explicit modeling.\nIn this paper, we propose a hierarchical federated learning framework based on\nthe social network with mobility namely HFL-SNM that considers both data\nsharing among clients and their mobility patterns. Under the constraints of\nlimited resources, we formulate a joint optimization problem of resource\nallocation and client scheduling, which objective is to minimize the energy\nconsumption of clients during the FL process. In social network, we introduce\nthe concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate.\nWe analyze the impact of effective data and redundant data on the model\nperformance through preliminary experiments. We decouple the optimization\nproblem into multiple sub-problems, analyze them based on preliminary\nexperimental results, and propose Dynamic Optimization in Social Network with\nMobility (DO-SNM) algorithm. Experimental results demonstrate that our\nalgorithm achieves superior model performance while significantly reducing\nenergy consumption, compared to traditional baseline algorithms.\n", "link": "http://arxiv.org/abs/2509.14938v2", "date": "2025-09-23", "relevancy": 2.3929, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4815}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4775}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Federated%20Learning%20for%20Social%20Network%20with%20Mobility&body=Title%3A%20Hierarchical%20Federated%20Learning%20for%20Social%20Network%20with%20Mobility%0AAuthor%3A%20Zeyu%20Chen%20and%20Wen%20Chen%20and%20Jun%20Li%20and%20Qingqing%20Wu%20and%20Ming%20Ding%20and%20Xuefeng%20Han%20and%20Xiumei%20Deng%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20offers%20a%20decentralized%20solution%20that%20allows%0Acollaborative%20local%20model%20training%20and%20global%20aggregation%2C%20thereby%20protecting%0Adata%20privacy.%20In%20conventional%20FL%20frameworks%2C%20data%20privacy%20is%20typically%0Apreserved%20under%20the%20assumption%20that%20local%20data%20remains%20absolutely%20private%2C%0Awhereas%20the%20mobility%20of%20clients%20is%20frequently%20neglected%20in%20explicit%20modeling.%0AIn%20this%20paper%2C%20we%20propose%20a%20hierarchical%20federated%20learning%20framework%20based%20on%0Athe%20social%20network%20with%20mobility%20namely%20HFL-SNM%20that%20considers%20both%20data%0Asharing%20among%20clients%20and%20their%20mobility%20patterns.%20Under%20the%20constraints%20of%0Alimited%20resources%2C%20we%20formulate%20a%20joint%20optimization%20problem%20of%20resource%0Aallocation%20and%20client%20scheduling%2C%20which%20objective%20is%20to%20minimize%20the%20energy%0Aconsumption%20of%20clients%20during%20the%20FL%20process.%20In%20social%20network%2C%20we%20introduce%0Athe%20concepts%20of%20Effective%20Data%20Coverage%20Rate%20and%20Redundant%20Data%20Coverage%20Rate.%0AWe%20analyze%20the%20impact%20of%20effective%20data%20and%20redundant%20data%20on%20the%20model%0Aperformance%20through%20preliminary%20experiments.%20We%20decouple%20the%20optimization%0Aproblem%20into%20multiple%20sub-problems%2C%20analyze%20them%20based%20on%20preliminary%0Aexperimental%20results%2C%20and%20propose%20Dynamic%20Optimization%20in%20Social%20Network%20with%0AMobility%20%28DO-SNM%29%20algorithm.%20Experimental%20results%20demonstrate%20that%20our%0Aalgorithm%20achieves%20superior%20model%20performance%20while%20significantly%20reducing%0Aenergy%20consumption%2C%20compared%20to%20traditional%20baseline%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14938v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Federated%2520Learning%2520for%2520Social%2520Network%2520with%2520Mobility%26entry.906535625%3DZeyu%2520Chen%2520and%2520Wen%2520Chen%2520and%2520Jun%2520Li%2520and%2520Qingqing%2520Wu%2520and%2520Ming%2520Ding%2520and%2520Xuefeng%2520Han%2520and%2520Xiumei%2520Deng%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520offers%2520a%2520decentralized%2520solution%2520that%2520allows%250Acollaborative%2520local%2520model%2520training%2520and%2520global%2520aggregation%252C%2520thereby%2520protecting%250Adata%2520privacy.%2520In%2520conventional%2520FL%2520frameworks%252C%2520data%2520privacy%2520is%2520typically%250Apreserved%2520under%2520the%2520assumption%2520that%2520local%2520data%2520remains%2520absolutely%2520private%252C%250Awhereas%2520the%2520mobility%2520of%2520clients%2520is%2520frequently%2520neglected%2520in%2520explicit%2520modeling.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520hierarchical%2520federated%2520learning%2520framework%2520based%2520on%250Athe%2520social%2520network%2520with%2520mobility%2520namely%2520HFL-SNM%2520that%2520considers%2520both%2520data%250Asharing%2520among%2520clients%2520and%2520their%2520mobility%2520patterns.%2520Under%2520the%2520constraints%2520of%250Alimited%2520resources%252C%2520we%2520formulate%2520a%2520joint%2520optimization%2520problem%2520of%2520resource%250Aallocation%2520and%2520client%2520scheduling%252C%2520which%2520objective%2520is%2520to%2520minimize%2520the%2520energy%250Aconsumption%2520of%2520clients%2520during%2520the%2520FL%2520process.%2520In%2520social%2520network%252C%2520we%2520introduce%250Athe%2520concepts%2520of%2520Effective%2520Data%2520Coverage%2520Rate%2520and%2520Redundant%2520Data%2520Coverage%2520Rate.%250AWe%2520analyze%2520the%2520impact%2520of%2520effective%2520data%2520and%2520redundant%2520data%2520on%2520the%2520model%250Aperformance%2520through%2520preliminary%2520experiments.%2520We%2520decouple%2520the%2520optimization%250Aproblem%2520into%2520multiple%2520sub-problems%252C%2520analyze%2520them%2520based%2520on%2520preliminary%250Aexperimental%2520results%252C%2520and%2520propose%2520Dynamic%2520Optimization%2520in%2520Social%2520Network%2520with%250AMobility%2520%2528DO-SNM%2529%2520algorithm.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Aalgorithm%2520achieves%2520superior%2520model%2520performance%2520while%2520significantly%2520reducing%250Aenergy%2520consumption%252C%2520compared%2520to%2520traditional%2520baseline%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14938v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Federated%20Learning%20for%20Social%20Network%20with%20Mobility&entry.906535625=Zeyu%20Chen%20and%20Wen%20Chen%20and%20Jun%20Li%20and%20Qingqing%20Wu%20and%20Ming%20Ding%20and%20Xuefeng%20Han%20and%20Xiumei%20Deng%20and%20Liwei%20Wang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20offers%20a%20decentralized%20solution%20that%20allows%0Acollaborative%20local%20model%20training%20and%20global%20aggregation%2C%20thereby%20protecting%0Adata%20privacy.%20In%20conventional%20FL%20frameworks%2C%20data%20privacy%20is%20typically%0Apreserved%20under%20the%20assumption%20that%20local%20data%20remains%20absolutely%20private%2C%0Awhereas%20the%20mobility%20of%20clients%20is%20frequently%20neglected%20in%20explicit%20modeling.%0AIn%20this%20paper%2C%20we%20propose%20a%20hierarchical%20federated%20learning%20framework%20based%20on%0Athe%20social%20network%20with%20mobility%20namely%20HFL-SNM%20that%20considers%20both%20data%0Asharing%20among%20clients%20and%20their%20mobility%20patterns.%20Under%20the%20constraints%20of%0Alimited%20resources%2C%20we%20formulate%20a%20joint%20optimization%20problem%20of%20resource%0Aallocation%20and%20client%20scheduling%2C%20which%20objective%20is%20to%20minimize%20the%20energy%0Aconsumption%20of%20clients%20during%20the%20FL%20process.%20In%20social%20network%2C%20we%20introduce%0Athe%20concepts%20of%20Effective%20Data%20Coverage%20Rate%20and%20Redundant%20Data%20Coverage%20Rate.%0AWe%20analyze%20the%20impact%20of%20effective%20data%20and%20redundant%20data%20on%20the%20model%0Aperformance%20through%20preliminary%20experiments.%20We%20decouple%20the%20optimization%0Aproblem%20into%20multiple%20sub-problems%2C%20analyze%20them%20based%20on%20preliminary%0Aexperimental%20results%2C%20and%20propose%20Dynamic%20Optimization%20in%20Social%20Network%20with%0AMobility%20%28DO-SNM%29%20algorithm.%20Experimental%20results%20demonstrate%20that%20our%0Aalgorithm%20achieves%20superior%20model%20performance%20while%20significantly%20reducing%0Aenergy%20consumption%2C%20compared%20to%20traditional%20baseline%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14938v2&entry.124074799=Read"},
{"title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image\n  Grounding for Clinical Reasoning", "author": "Guoxin Wang and Jun Zhao and Xinyi Liu and Yanbo Liu and Xuyang Cao and Chao Li and Zhuoyun Liu and Qintian Sun and Fangru Zhou and Haoqiang Xing and Zhenhong Yang", "abstract": "  Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions.\n", "link": "http://arxiv.org/abs/2509.19090v1", "date": "2025-09-23", "relevancy": 2.3921, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6105}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Citrus-V%3A%20Advancing%20Medical%20Foundation%20Models%20with%20Unified%20Medical%20Image%0A%20%20Grounding%20for%20Clinical%20Reasoning&body=Title%3A%20Citrus-V%3A%20Advancing%20Medical%20Foundation%20Models%20with%20Unified%20Medical%20Image%0A%20%20Grounding%20for%20Clinical%20Reasoning%0AAuthor%3A%20Guoxin%20Wang%20and%20Jun%20Zhao%20and%20Xinyi%20Liu%20and%20Yanbo%20Liu%20and%20Xuyang%20Cao%20and%20Chao%20Li%20and%20Zhuoyun%20Liu%20and%20Qintian%20Sun%20and%20Fangru%20Zhou%20and%20Haoqiang%20Xing%20and%20Zhenhong%20Yang%0AAbstract%3A%20%20%20Medical%20imaging%20provides%20critical%20evidence%20for%20clinical%20diagnosis%2C%20treatment%0Aplanning%2C%20and%20surgical%20decisions%2C%20yet%20most%20existing%20imaging%20models%20are%20narrowly%0Afocused%20and%20require%20multiple%20specialized%20networks%2C%20limiting%20their%0Ageneralization.%20Although%20large-scale%20language%20and%20multimodal%20models%20exhibit%0Astrong%20reasoning%20and%20multi-task%20capabilities%2C%20real-world%20clinical%20applications%0Ademand%20precise%20visual%20grounding%2C%20multimodal%20integration%2C%20and%20chain-of-thought%0Areasoning.%20We%20introduce%20Citrus-V%2C%20a%20multimodal%20medical%20foundation%20model%20that%0Acombines%20image%20analysis%20with%20textual%20reasoning.%20The%20model%20integrates%20detection%2C%0Asegmentation%2C%20and%20multimodal%20chain-of-thought%20reasoning%2C%20enabling%20pixel-level%0Alesion%20localization%2C%20structured%20report%20generation%2C%20and%20physician-like%0Adiagnostic%20inference%20in%20a%20single%20framework.%20We%20propose%20a%20novel%20multimodal%0Atraining%20approach%20and%20release%20a%20curated%20open-source%20data%20suite%20covering%0Areasoning%2C%20detection%2C%20segmentation%2C%20and%20document%20understanding%20tasks.%0AEvaluations%20demonstrate%20that%20Citrus-V%20outperforms%20existing%20open-source%20medical%0Amodels%20and%20expert-level%20imaging%20systems%20across%20multiple%20benchmarks%2C%20delivering%0Aa%20unified%20pipeline%20from%20visual%20grounding%20to%20clinical%20reasoning%20and%20supporting%0Aprecise%20lesion%20quantification%2C%20automated%20reporting%2C%20and%20reliable%20second%0Aopinions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCitrus-V%253A%2520Advancing%2520Medical%2520Foundation%2520Models%2520with%2520Unified%2520Medical%2520Image%250A%2520%2520Grounding%2520for%2520Clinical%2520Reasoning%26entry.906535625%3DGuoxin%2520Wang%2520and%2520Jun%2520Zhao%2520and%2520Xinyi%2520Liu%2520and%2520Yanbo%2520Liu%2520and%2520Xuyang%2520Cao%2520and%2520Chao%2520Li%2520and%2520Zhuoyun%2520Liu%2520and%2520Qintian%2520Sun%2520and%2520Fangru%2520Zhou%2520and%2520Haoqiang%2520Xing%2520and%2520Zhenhong%2520Yang%26entry.1292438233%3D%2520%2520Medical%2520imaging%2520provides%2520critical%2520evidence%2520for%2520clinical%2520diagnosis%252C%2520treatment%250Aplanning%252C%2520and%2520surgical%2520decisions%252C%2520yet%2520most%2520existing%2520imaging%2520models%2520are%2520narrowly%250Afocused%2520and%2520require%2520multiple%2520specialized%2520networks%252C%2520limiting%2520their%250Ageneralization.%2520Although%2520large-scale%2520language%2520and%2520multimodal%2520models%2520exhibit%250Astrong%2520reasoning%2520and%2520multi-task%2520capabilities%252C%2520real-world%2520clinical%2520applications%250Ademand%2520precise%2520visual%2520grounding%252C%2520multimodal%2520integration%252C%2520and%2520chain-of-thought%250Areasoning.%2520We%2520introduce%2520Citrus-V%252C%2520a%2520multimodal%2520medical%2520foundation%2520model%2520that%250Acombines%2520image%2520analysis%2520with%2520textual%2520reasoning.%2520The%2520model%2520integrates%2520detection%252C%250Asegmentation%252C%2520and%2520multimodal%2520chain-of-thought%2520reasoning%252C%2520enabling%2520pixel-level%250Alesion%2520localization%252C%2520structured%2520report%2520generation%252C%2520and%2520physician-like%250Adiagnostic%2520inference%2520in%2520a%2520single%2520framework.%2520We%2520propose%2520a%2520novel%2520multimodal%250Atraining%2520approach%2520and%2520release%2520a%2520curated%2520open-source%2520data%2520suite%2520covering%250Areasoning%252C%2520detection%252C%2520segmentation%252C%2520and%2520document%2520understanding%2520tasks.%250AEvaluations%2520demonstrate%2520that%2520Citrus-V%2520outperforms%2520existing%2520open-source%2520medical%250Amodels%2520and%2520expert-level%2520imaging%2520systems%2520across%2520multiple%2520benchmarks%252C%2520delivering%250Aa%2520unified%2520pipeline%2520from%2520visual%2520grounding%2520to%2520clinical%2520reasoning%2520and%2520supporting%250Aprecise%2520lesion%2520quantification%252C%2520automated%2520reporting%252C%2520and%2520reliable%2520second%250Aopinions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Citrus-V%3A%20Advancing%20Medical%20Foundation%20Models%20with%20Unified%20Medical%20Image%0A%20%20Grounding%20for%20Clinical%20Reasoning&entry.906535625=Guoxin%20Wang%20and%20Jun%20Zhao%20and%20Xinyi%20Liu%20and%20Yanbo%20Liu%20and%20Xuyang%20Cao%20and%20Chao%20Li%20and%20Zhuoyun%20Liu%20and%20Qintian%20Sun%20and%20Fangru%20Zhou%20and%20Haoqiang%20Xing%20and%20Zhenhong%20Yang&entry.1292438233=%20%20Medical%20imaging%20provides%20critical%20evidence%20for%20clinical%20diagnosis%2C%20treatment%0Aplanning%2C%20and%20surgical%20decisions%2C%20yet%20most%20existing%20imaging%20models%20are%20narrowly%0Afocused%20and%20require%20multiple%20specialized%20networks%2C%20limiting%20their%0Ageneralization.%20Although%20large-scale%20language%20and%20multimodal%20models%20exhibit%0Astrong%20reasoning%20and%20multi-task%20capabilities%2C%20real-world%20clinical%20applications%0Ademand%20precise%20visual%20grounding%2C%20multimodal%20integration%2C%20and%20chain-of-thought%0Areasoning.%20We%20introduce%20Citrus-V%2C%20a%20multimodal%20medical%20foundation%20model%20that%0Acombines%20image%20analysis%20with%20textual%20reasoning.%20The%20model%20integrates%20detection%2C%0Asegmentation%2C%20and%20multimodal%20chain-of-thought%20reasoning%2C%20enabling%20pixel-level%0Alesion%20localization%2C%20structured%20report%20generation%2C%20and%20physician-like%0Adiagnostic%20inference%20in%20a%20single%20framework.%20We%20propose%20a%20novel%20multimodal%0Atraining%20approach%20and%20release%20a%20curated%20open-source%20data%20suite%20covering%0Areasoning%2C%20detection%2C%20segmentation%2C%20and%20document%20understanding%20tasks.%0AEvaluations%20demonstrate%20that%20Citrus-V%20outperforms%20existing%20open-source%20medical%0Amodels%20and%20expert-level%20imaging%20systems%20across%20multiple%20benchmarks%2C%20delivering%0Aa%20unified%20pipeline%20from%20visual%20grounding%20to%20clinical%20reasoning%20and%20supporting%0Aprecise%20lesion%20quantification%2C%20automated%20reporting%2C%20and%20reliable%20second%0Aopinions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19090v1&entry.124074799=Read"},
{"title": "Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging\n  Heterogeneity in Confounding Tumor Pathologies", "author": "Dheerendranath Battalapalli and Apoorva Safai and Maria Jaramillo and Hyemin Um and Gustavo Adalfo Pineda Ortiz and Ulas Bagci and Manmeet Singh Ahluwalia and Marwa Ismail and Pallavi Tiwari", "abstract": "  A significant challenge in solid tumors is reliably distinguishing\nconfounding pathologies from malignant neoplasms on routine imaging. While\nradiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,\nmany aggregate features across the region of interest (ROI) and miss complex\nspatial relationships among varying intensity compositions. We present a new\nGraph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional\nheterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of\nsub-regions using per-voxel radiomic measurements, then (2) computes\ngraph-theoretic metrics to quantify spatial associations among clusters. The\nresulting weighted graphs encode higher-order spatial relationships within the\nROI, aiming to reliably capture ILH and disambiguate confounding pathologies\nfrom malignancy. To assess efficacy and clinical feasibility, GrRAiL was\nevaluated in n=947 subjects spanning three use cases: differentiating tumor\nrecurrence from radiation effects in glioblastoma (GBM; n=106) and brain\nmetastasis (n=233), and stratifying pancreatic intraductal papillary mucinous\nneoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional\nsetting, GrRAiL consistently outperformed state-of-the-art baselines - Graph\nNeural Networks (GNNs), textural radiomics, and intensity-graph analysis. In\nGBM, cross-validation (CV) and test accuracies for recurrence vs\npseudo-progression were 89% and 78% with >10% test-accuracy gains over\ncomparators. In brain metastasis, CV and test accuracies for recurrence vs\nradiation necrosis were 84% and 74% (>13% improvement). For IPMN risk\nstratification, CV and test accuracies were 84% and 75%, showing >10%\nimprovement.\n", "link": "http://arxiv.org/abs/2509.19258v1", "date": "2025-09-23", "relevancy": 2.3915, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4973}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4688}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Radiomic%20Learning%20%28GrRAiL%29%20Descriptor%20to%20Characterize%20Imaging%0A%20%20Heterogeneity%20in%20Confounding%20Tumor%20Pathologies&body=Title%3A%20Graph-Radiomic%20Learning%20%28GrRAiL%29%20Descriptor%20to%20Characterize%20Imaging%0A%20%20Heterogeneity%20in%20Confounding%20Tumor%20Pathologies%0AAuthor%3A%20Dheerendranath%20Battalapalli%20and%20Apoorva%20Safai%20and%20Maria%20Jaramillo%20and%20Hyemin%20Um%20and%20Gustavo%20Adalfo%20Pineda%20Ortiz%20and%20Ulas%20Bagci%20and%20Manmeet%20Singh%20Ahluwalia%20and%20Marwa%20Ismail%20and%20Pallavi%20Tiwari%0AAbstract%3A%20%20%20A%20significant%20challenge%20in%20solid%20tumors%20is%20reliably%20distinguishing%0Aconfounding%20pathologies%20from%20malignant%20neoplasms%20on%20routine%20imaging.%20While%0Aradiomics%20methods%20seek%20surrogate%20markers%20of%20lesion%20heterogeneity%20on%20CT/MRI%2C%0Amany%20aggregate%20features%20across%20the%20region%20of%20interest%20%28ROI%29%20and%20miss%20complex%0Aspatial%20relationships%20among%20varying%20intensity%20compositions.%20We%20present%20a%20new%0AGraph-Radiomic%20Learning%20%28GrRAiL%29%20descriptor%20for%20characterizing%20intralesional%0Aheterogeneity%20%28ILH%29%20on%20clinical%20MRI%20scans.%20GrRAiL%20%281%29%20identifies%20clusters%20of%0Asub-regions%20using%20per-voxel%20radiomic%20measurements%2C%20then%20%282%29%20computes%0Agraph-theoretic%20metrics%20to%20quantify%20spatial%20associations%20among%20clusters.%20The%0Aresulting%20weighted%20graphs%20encode%20higher-order%20spatial%20relationships%20within%20the%0AROI%2C%20aiming%20to%20reliably%20capture%20ILH%20and%20disambiguate%20confounding%20pathologies%0Afrom%20malignancy.%20To%20assess%20efficacy%20and%20clinical%20feasibility%2C%20GrRAiL%20was%0Aevaluated%20in%20n%3D947%20subjects%20spanning%20three%20use%20cases%3A%20differentiating%20tumor%0Arecurrence%20from%20radiation%20effects%20in%20glioblastoma%20%28GBM%3B%20n%3D106%29%20and%20brain%0Ametastasis%20%28n%3D233%29%2C%20and%20stratifying%20pancreatic%20intraductal%20papillary%20mucinous%0Aneoplasms%20%28IPMNs%29%20into%20no%2Blow%20vs%20high%20risk%20%28n%3D608%29.%20In%20a%20multi-institutional%0Asetting%2C%20GrRAiL%20consistently%20outperformed%20state-of-the-art%20baselines%20-%20Graph%0ANeural%20Networks%20%28GNNs%29%2C%20textural%20radiomics%2C%20and%20intensity-graph%20analysis.%20In%0AGBM%2C%20cross-validation%20%28CV%29%20and%20test%20accuracies%20for%20recurrence%20vs%0Apseudo-progression%20were%2089%25%20and%2078%25%20with%20%3E10%25%20test-accuracy%20gains%20over%0Acomparators.%20In%20brain%20metastasis%2C%20CV%20and%20test%20accuracies%20for%20recurrence%20vs%0Aradiation%20necrosis%20were%2084%25%20and%2074%25%20%28%3E13%25%20improvement%29.%20For%20IPMN%20risk%0Astratification%2C%20CV%20and%20test%20accuracies%20were%2084%25%20and%2075%25%2C%20showing%20%3E10%25%0Aimprovement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Radiomic%2520Learning%2520%2528GrRAiL%2529%2520Descriptor%2520to%2520Characterize%2520Imaging%250A%2520%2520Heterogeneity%2520in%2520Confounding%2520Tumor%2520Pathologies%26entry.906535625%3DDheerendranath%2520Battalapalli%2520and%2520Apoorva%2520Safai%2520and%2520Maria%2520Jaramillo%2520and%2520Hyemin%2520Um%2520and%2520Gustavo%2520Adalfo%2520Pineda%2520Ortiz%2520and%2520Ulas%2520Bagci%2520and%2520Manmeet%2520Singh%2520Ahluwalia%2520and%2520Marwa%2520Ismail%2520and%2520Pallavi%2520Tiwari%26entry.1292438233%3D%2520%2520A%2520significant%2520challenge%2520in%2520solid%2520tumors%2520is%2520reliably%2520distinguishing%250Aconfounding%2520pathologies%2520from%2520malignant%2520neoplasms%2520on%2520routine%2520imaging.%2520While%250Aradiomics%2520methods%2520seek%2520surrogate%2520markers%2520of%2520lesion%2520heterogeneity%2520on%2520CT/MRI%252C%250Amany%2520aggregate%2520features%2520across%2520the%2520region%2520of%2520interest%2520%2528ROI%2529%2520and%2520miss%2520complex%250Aspatial%2520relationships%2520among%2520varying%2520intensity%2520compositions.%2520We%2520present%2520a%2520new%250AGraph-Radiomic%2520Learning%2520%2528GrRAiL%2529%2520descriptor%2520for%2520characterizing%2520intralesional%250Aheterogeneity%2520%2528ILH%2529%2520on%2520clinical%2520MRI%2520scans.%2520GrRAiL%2520%25281%2529%2520identifies%2520clusters%2520of%250Asub-regions%2520using%2520per-voxel%2520radiomic%2520measurements%252C%2520then%2520%25282%2529%2520computes%250Agraph-theoretic%2520metrics%2520to%2520quantify%2520spatial%2520associations%2520among%2520clusters.%2520The%250Aresulting%2520weighted%2520graphs%2520encode%2520higher-order%2520spatial%2520relationships%2520within%2520the%250AROI%252C%2520aiming%2520to%2520reliably%2520capture%2520ILH%2520and%2520disambiguate%2520confounding%2520pathologies%250Afrom%2520malignancy.%2520To%2520assess%2520efficacy%2520and%2520clinical%2520feasibility%252C%2520GrRAiL%2520was%250Aevaluated%2520in%2520n%253D947%2520subjects%2520spanning%2520three%2520use%2520cases%253A%2520differentiating%2520tumor%250Arecurrence%2520from%2520radiation%2520effects%2520in%2520glioblastoma%2520%2528GBM%253B%2520n%253D106%2529%2520and%2520brain%250Ametastasis%2520%2528n%253D233%2529%252C%2520and%2520stratifying%2520pancreatic%2520intraductal%2520papillary%2520mucinous%250Aneoplasms%2520%2528IPMNs%2529%2520into%2520no%252Blow%2520vs%2520high%2520risk%2520%2528n%253D608%2529.%2520In%2520a%2520multi-institutional%250Asetting%252C%2520GrRAiL%2520consistently%2520outperformed%2520state-of-the-art%2520baselines%2520-%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%252C%2520textural%2520radiomics%252C%2520and%2520intensity-graph%2520analysis.%2520In%250AGBM%252C%2520cross-validation%2520%2528CV%2529%2520and%2520test%2520accuracies%2520for%2520recurrence%2520vs%250Apseudo-progression%2520were%252089%2525%2520and%252078%2525%2520with%2520%253E10%2525%2520test-accuracy%2520gains%2520over%250Acomparators.%2520In%2520brain%2520metastasis%252C%2520CV%2520and%2520test%2520accuracies%2520for%2520recurrence%2520vs%250Aradiation%2520necrosis%2520were%252084%2525%2520and%252074%2525%2520%2528%253E13%2525%2520improvement%2529.%2520For%2520IPMN%2520risk%250Astratification%252C%2520CV%2520and%2520test%2520accuracies%2520were%252084%2525%2520and%252075%2525%252C%2520showing%2520%253E10%2525%250Aimprovement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Radiomic%20Learning%20%28GrRAiL%29%20Descriptor%20to%20Characterize%20Imaging%0A%20%20Heterogeneity%20in%20Confounding%20Tumor%20Pathologies&entry.906535625=Dheerendranath%20Battalapalli%20and%20Apoorva%20Safai%20and%20Maria%20Jaramillo%20and%20Hyemin%20Um%20and%20Gustavo%20Adalfo%20Pineda%20Ortiz%20and%20Ulas%20Bagci%20and%20Manmeet%20Singh%20Ahluwalia%20and%20Marwa%20Ismail%20and%20Pallavi%20Tiwari&entry.1292438233=%20%20A%20significant%20challenge%20in%20solid%20tumors%20is%20reliably%20distinguishing%0Aconfounding%20pathologies%20from%20malignant%20neoplasms%20on%20routine%20imaging.%20While%0Aradiomics%20methods%20seek%20surrogate%20markers%20of%20lesion%20heterogeneity%20on%20CT/MRI%2C%0Amany%20aggregate%20features%20across%20the%20region%20of%20interest%20%28ROI%29%20and%20miss%20complex%0Aspatial%20relationships%20among%20varying%20intensity%20compositions.%20We%20present%20a%20new%0AGraph-Radiomic%20Learning%20%28GrRAiL%29%20descriptor%20for%20characterizing%20intralesional%0Aheterogeneity%20%28ILH%29%20on%20clinical%20MRI%20scans.%20GrRAiL%20%281%29%20identifies%20clusters%20of%0Asub-regions%20using%20per-voxel%20radiomic%20measurements%2C%20then%20%282%29%20computes%0Agraph-theoretic%20metrics%20to%20quantify%20spatial%20associations%20among%20clusters.%20The%0Aresulting%20weighted%20graphs%20encode%20higher-order%20spatial%20relationships%20within%20the%0AROI%2C%20aiming%20to%20reliably%20capture%20ILH%20and%20disambiguate%20confounding%20pathologies%0Afrom%20malignancy.%20To%20assess%20efficacy%20and%20clinical%20feasibility%2C%20GrRAiL%20was%0Aevaluated%20in%20n%3D947%20subjects%20spanning%20three%20use%20cases%3A%20differentiating%20tumor%0Arecurrence%20from%20radiation%20effects%20in%20glioblastoma%20%28GBM%3B%20n%3D106%29%20and%20brain%0Ametastasis%20%28n%3D233%29%2C%20and%20stratifying%20pancreatic%20intraductal%20papillary%20mucinous%0Aneoplasms%20%28IPMNs%29%20into%20no%2Blow%20vs%20high%20risk%20%28n%3D608%29.%20In%20a%20multi-institutional%0Asetting%2C%20GrRAiL%20consistently%20outperformed%20state-of-the-art%20baselines%20-%20Graph%0ANeural%20Networks%20%28GNNs%29%2C%20textural%20radiomics%2C%20and%20intensity-graph%20analysis.%20In%0AGBM%2C%20cross-validation%20%28CV%29%20and%20test%20accuracies%20for%20recurrence%20vs%0Apseudo-progression%20were%2089%25%20and%2078%25%20with%20%3E10%25%20test-accuracy%20gains%20over%0Acomparators.%20In%20brain%20metastasis%2C%20CV%20and%20test%20accuracies%20for%20recurrence%20vs%0Aradiation%20necrosis%20were%2084%25%20and%2074%25%20%28%3E13%25%20improvement%29.%20For%20IPMN%20risk%0Astratification%2C%20CV%20and%20test%20accuracies%20were%2084%25%20and%2075%25%2C%20showing%20%3E10%25%0Aimprovement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19258v1&entry.124074799=Read"},
{"title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from\n  the Arab World", "author": "Saeed Almheiri and Rania Hossam and Mena Attia and Chenxi Wang and Preslav Nakov and Timothy Baldwin and Fajri Koto", "abstract": "  Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.\n", "link": "http://arxiv.org/abs/2509.19265v1", "date": "2025-09-23", "relevancy": 2.3715, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4781}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Cultural%20Transfer%20of%20Commonsense%20Reasoning%20in%20LLMs%3A%20Evidence%20from%0A%20%20the%20Arab%20World&body=Title%3A%20Cross-Cultural%20Transfer%20of%20Commonsense%20Reasoning%20in%20LLMs%3A%20Evidence%20from%0A%20%20the%20Arab%20World%0AAuthor%3A%20Saeed%20Almheiri%20and%20Rania%20Hossam%20and%20Mena%20Attia%20and%20Chenxi%20Wang%20and%20Preslav%20Nakov%20and%20Timothy%20Baldwin%20and%20Fajri%20Koto%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20reflect%20Western-centric%20biases%2C%20limiting%0Atheir%20effectiveness%20in%20diverse%20cultural%20contexts.%20Although%20some%20work%20has%0Aexplored%20cultural%20alignment%2C%20the%20potential%20for%20cross-cultural%20transfer%2C%20using%0Aalignment%20in%20one%20culture%20to%20improve%20performance%20in%20others%2C%20remains%0Aunderexplored.%20This%20paper%20investigates%20cross-cultural%20transfer%20of%20commonsense%0Areasoning%20in%20the%20Arab%20world%2C%20where%20linguistic%20and%20historical%20similarities%0Acoexist%20with%20local%20cultural%20differences.%20Using%20a%20culturally%20grounded%0Acommonsense%20reasoning%20dataset%20covering%2013%20Arab%20countries%2C%20we%20evaluate%0Alightweight%20alignment%20methods%20such%20as%20in-context%20learning%20and%0Ademonstration-based%20reinforcement%20%28DITTO%29%2C%20alongside%20baselines%20like%20supervised%0Afine-tuning%20and%20direct%20preference%20optimization.%20Our%20results%20show%20that%20merely%2012%0Aculture-specific%20examples%20from%20one%20country%20can%20improve%20performance%20in%20others%20by%0A10%5C%25%20on%20average%2C%20within%20multilingual%20models.%20In%20addition%2C%20we%20demonstrate%20that%0Aout-of-culture%20demonstrations%20from%20Indonesia%20and%20US%20contexts%20can%20match%20or%0Asurpass%20in-culture%20alignment%20for%20MCQ%20reasoning%2C%20highlighting%20cultural%0Acommonsense%20transferability%20beyond%20the%20Arab%20world.%20These%20findings%20demonstrate%0Athat%20efficient%20cross-cultural%20alignment%20is%20possible%20and%20offer%20a%20promising%0Aapproach%20to%20adapt%20LLMs%20to%20low-resource%20cultural%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Cultural%2520Transfer%2520of%2520Commonsense%2520Reasoning%2520in%2520LLMs%253A%2520Evidence%2520from%250A%2520%2520the%2520Arab%2520World%26entry.906535625%3DSaeed%2520Almheiri%2520and%2520Rania%2520Hossam%2520and%2520Mena%2520Attia%2520and%2520Chenxi%2520Wang%2520and%2520Preslav%2520Nakov%2520and%2520Timothy%2520Baldwin%2520and%2520Fajri%2520Koto%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520reflect%2520Western-centric%2520biases%252C%2520limiting%250Atheir%2520effectiveness%2520in%2520diverse%2520cultural%2520contexts.%2520Although%2520some%2520work%2520has%250Aexplored%2520cultural%2520alignment%252C%2520the%2520potential%2520for%2520cross-cultural%2520transfer%252C%2520using%250Aalignment%2520in%2520one%2520culture%2520to%2520improve%2520performance%2520in%2520others%252C%2520remains%250Aunderexplored.%2520This%2520paper%2520investigates%2520cross-cultural%2520transfer%2520of%2520commonsense%250Areasoning%2520in%2520the%2520Arab%2520world%252C%2520where%2520linguistic%2520and%2520historical%2520similarities%250Acoexist%2520with%2520local%2520cultural%2520differences.%2520Using%2520a%2520culturally%2520grounded%250Acommonsense%2520reasoning%2520dataset%2520covering%252013%2520Arab%2520countries%252C%2520we%2520evaluate%250Alightweight%2520alignment%2520methods%2520such%2520as%2520in-context%2520learning%2520and%250Ademonstration-based%2520reinforcement%2520%2528DITTO%2529%252C%2520alongside%2520baselines%2520like%2520supervised%250Afine-tuning%2520and%2520direct%2520preference%2520optimization.%2520Our%2520results%2520show%2520that%2520merely%252012%250Aculture-specific%2520examples%2520from%2520one%2520country%2520can%2520improve%2520performance%2520in%2520others%2520by%250A10%255C%2525%2520on%2520average%252C%2520within%2520multilingual%2520models.%2520In%2520addition%252C%2520we%2520demonstrate%2520that%250Aout-of-culture%2520demonstrations%2520from%2520Indonesia%2520and%2520US%2520contexts%2520can%2520match%2520or%250Asurpass%2520in-culture%2520alignment%2520for%2520MCQ%2520reasoning%252C%2520highlighting%2520cultural%250Acommonsense%2520transferability%2520beyond%2520the%2520Arab%2520world.%2520These%2520findings%2520demonstrate%250Athat%2520efficient%2520cross-cultural%2520alignment%2520is%2520possible%2520and%2520offer%2520a%2520promising%250Aapproach%2520to%2520adapt%2520LLMs%2520to%2520low-resource%2520cultural%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Cultural%20Transfer%20of%20Commonsense%20Reasoning%20in%20LLMs%3A%20Evidence%20from%0A%20%20the%20Arab%20World&entry.906535625=Saeed%20Almheiri%20and%20Rania%20Hossam%20and%20Mena%20Attia%20and%20Chenxi%20Wang%20and%20Preslav%20Nakov%20and%20Timothy%20Baldwin%20and%20Fajri%20Koto&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20reflect%20Western-centric%20biases%2C%20limiting%0Atheir%20effectiveness%20in%20diverse%20cultural%20contexts.%20Although%20some%20work%20has%0Aexplored%20cultural%20alignment%2C%20the%20potential%20for%20cross-cultural%20transfer%2C%20using%0Aalignment%20in%20one%20culture%20to%20improve%20performance%20in%20others%2C%20remains%0Aunderexplored.%20This%20paper%20investigates%20cross-cultural%20transfer%20of%20commonsense%0Areasoning%20in%20the%20Arab%20world%2C%20where%20linguistic%20and%20historical%20similarities%0Acoexist%20with%20local%20cultural%20differences.%20Using%20a%20culturally%20grounded%0Acommonsense%20reasoning%20dataset%20covering%2013%20Arab%20countries%2C%20we%20evaluate%0Alightweight%20alignment%20methods%20such%20as%20in-context%20learning%20and%0Ademonstration-based%20reinforcement%20%28DITTO%29%2C%20alongside%20baselines%20like%20supervised%0Afine-tuning%20and%20direct%20preference%20optimization.%20Our%20results%20show%20that%20merely%2012%0Aculture-specific%20examples%20from%20one%20country%20can%20improve%20performance%20in%20others%20by%0A10%5C%25%20on%20average%2C%20within%20multilingual%20models.%20In%20addition%2C%20we%20demonstrate%20that%0Aout-of-culture%20demonstrations%20from%20Indonesia%20and%20US%20contexts%20can%20match%20or%0Asurpass%20in-culture%20alignment%20for%20MCQ%20reasoning%2C%20highlighting%20cultural%0Acommonsense%20transferability%20beyond%20the%20Arab%20world.%20These%20findings%20demonstrate%0Athat%20efficient%20cross-cultural%20alignment%20is%20possible%20and%20offer%20a%20promising%0Aapproach%20to%20adapt%20LLMs%20to%20low-resource%20cultural%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19265v1&entry.124074799=Read"},
{"title": "GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility\n  Understanding", "author": "Wenying Luo and Zhiyuan Lin and Wenhao Xu and Minghao Liu and Zhi Li", "abstract": "  Human mobility traces, often recorded as sequences of check-ins, provide a\nunique window into both short-term visiting patterns and persistent lifestyle\nregularities. In this work we introduce GSTM-HMU, a generative spatio-temporal\nframework designed to advance mobility analysis by explicitly modeling the\nsemantic and temporal complexity of human movement. The framework consists of\nfour key innovations. First, a Spatio-Temporal Concept Encoder (STCE)\nintegrates geographic location, POI category semantics, and periodic temporal\nrhythms into unified vector representations. Second, a Cognitive Trajectory\nMemory (CTM) adaptively filters historical visits, emphasizing recent and\nbehaviorally salient events in order to capture user intent more effectively.\nThird, a Lifestyle Concept Bank (LCB) contributes structured human preference\ncues, such as activity types and lifestyle patterns, to enhance\ninterpretability and personalization. Finally, task-oriented generative heads\ntransform the learned representations into predictions for multiple downstream\ntasks. We conduct extensive experiments on four widely used real-world\ndatasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate\nperformance on three benchmark tasks: next-location prediction, trajectory-user\nidentification, and time estimation. The results demonstrate consistent and\nsubstantial improvements over strong baselines, confirming the effectiveness of\nGSTM-HMU in extracting semantic regularities from complex mobility data. Beyond\nraw performance gains, our findings also suggest that generative modeling\nprovides a promising foundation for building more robust, interpretable, and\ngeneralizable systems for human mobility intelligence.\n", "link": "http://arxiv.org/abs/2509.19135v1", "date": "2025-09-23", "relevancy": 2.3644, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6287}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5752}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSTM-HMU%3A%20Generative%20Spatio-Temporal%20Modeling%20for%20Human%20Mobility%0A%20%20Understanding&body=Title%3A%20GSTM-HMU%3A%20Generative%20Spatio-Temporal%20Modeling%20for%20Human%20Mobility%0A%20%20Understanding%0AAuthor%3A%20Wenying%20Luo%20and%20Zhiyuan%20Lin%20and%20Wenhao%20Xu%20and%20Minghao%20Liu%20and%20Zhi%20Li%0AAbstract%3A%20%20%20Human%20mobility%20traces%2C%20often%20recorded%20as%20sequences%20of%20check-ins%2C%20provide%20a%0Aunique%20window%20into%20both%20short-term%20visiting%20patterns%20and%20persistent%20lifestyle%0Aregularities.%20In%20this%20work%20we%20introduce%20GSTM-HMU%2C%20a%20generative%20spatio-temporal%0Aframework%20designed%20to%20advance%20mobility%20analysis%20by%20explicitly%20modeling%20the%0Asemantic%20and%20temporal%20complexity%20of%20human%20movement.%20The%20framework%20consists%20of%0Afour%20key%20innovations.%20First%2C%20a%20Spatio-Temporal%20Concept%20Encoder%20%28STCE%29%0Aintegrates%20geographic%20location%2C%20POI%20category%20semantics%2C%20and%20periodic%20temporal%0Arhythms%20into%20unified%20vector%20representations.%20Second%2C%20a%20Cognitive%20Trajectory%0AMemory%20%28CTM%29%20adaptively%20filters%20historical%20visits%2C%20emphasizing%20recent%20and%0Abehaviorally%20salient%20events%20in%20order%20to%20capture%20user%20intent%20more%20effectively.%0AThird%2C%20a%20Lifestyle%20Concept%20Bank%20%28LCB%29%20contributes%20structured%20human%20preference%0Acues%2C%20such%20as%20activity%20types%20and%20lifestyle%20patterns%2C%20to%20enhance%0Ainterpretability%20and%20personalization.%20Finally%2C%20task-oriented%20generative%20heads%0Atransform%20the%20learned%20representations%20into%20predictions%20for%20multiple%20downstream%0Atasks.%20We%20conduct%20extensive%20experiments%20on%20four%20widely%20used%20real-world%0Adatasets%2C%20including%20Gowalla%2C%20WeePlace%2C%20Brightkite%2C%20and%20FourSquare%2C%20and%20evaluate%0Aperformance%20on%20three%20benchmark%20tasks%3A%20next-location%20prediction%2C%20trajectory-user%0Aidentification%2C%20and%20time%20estimation.%20The%20results%20demonstrate%20consistent%20and%0Asubstantial%20improvements%20over%20strong%20baselines%2C%20confirming%20the%20effectiveness%20of%0AGSTM-HMU%20in%20extracting%20semantic%20regularities%20from%20complex%20mobility%20data.%20Beyond%0Araw%20performance%20gains%2C%20our%20findings%20also%20suggest%20that%20generative%20modeling%0Aprovides%20a%20promising%20foundation%20for%20building%20more%20robust%2C%20interpretable%2C%20and%0Ageneralizable%20systems%20for%20human%20mobility%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSTM-HMU%253A%2520Generative%2520Spatio-Temporal%2520Modeling%2520for%2520Human%2520Mobility%250A%2520%2520Understanding%26entry.906535625%3DWenying%2520Luo%2520and%2520Zhiyuan%2520Lin%2520and%2520Wenhao%2520Xu%2520and%2520Minghao%2520Liu%2520and%2520Zhi%2520Li%26entry.1292438233%3D%2520%2520Human%2520mobility%2520traces%252C%2520often%2520recorded%2520as%2520sequences%2520of%2520check-ins%252C%2520provide%2520a%250Aunique%2520window%2520into%2520both%2520short-term%2520visiting%2520patterns%2520and%2520persistent%2520lifestyle%250Aregularities.%2520In%2520this%2520work%2520we%2520introduce%2520GSTM-HMU%252C%2520a%2520generative%2520spatio-temporal%250Aframework%2520designed%2520to%2520advance%2520mobility%2520analysis%2520by%2520explicitly%2520modeling%2520the%250Asemantic%2520and%2520temporal%2520complexity%2520of%2520human%2520movement.%2520The%2520framework%2520consists%2520of%250Afour%2520key%2520innovations.%2520First%252C%2520a%2520Spatio-Temporal%2520Concept%2520Encoder%2520%2528STCE%2529%250Aintegrates%2520geographic%2520location%252C%2520POI%2520category%2520semantics%252C%2520and%2520periodic%2520temporal%250Arhythms%2520into%2520unified%2520vector%2520representations.%2520Second%252C%2520a%2520Cognitive%2520Trajectory%250AMemory%2520%2528CTM%2529%2520adaptively%2520filters%2520historical%2520visits%252C%2520emphasizing%2520recent%2520and%250Abehaviorally%2520salient%2520events%2520in%2520order%2520to%2520capture%2520user%2520intent%2520more%2520effectively.%250AThird%252C%2520a%2520Lifestyle%2520Concept%2520Bank%2520%2528LCB%2529%2520contributes%2520structured%2520human%2520preference%250Acues%252C%2520such%2520as%2520activity%2520types%2520and%2520lifestyle%2520patterns%252C%2520to%2520enhance%250Ainterpretability%2520and%2520personalization.%2520Finally%252C%2520task-oriented%2520generative%2520heads%250Atransform%2520the%2520learned%2520representations%2520into%2520predictions%2520for%2520multiple%2520downstream%250Atasks.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520four%2520widely%2520used%2520real-world%250Adatasets%252C%2520including%2520Gowalla%252C%2520WeePlace%252C%2520Brightkite%252C%2520and%2520FourSquare%252C%2520and%2520evaluate%250Aperformance%2520on%2520three%2520benchmark%2520tasks%253A%2520next-location%2520prediction%252C%2520trajectory-user%250Aidentification%252C%2520and%2520time%2520estimation.%2520The%2520results%2520demonstrate%2520consistent%2520and%250Asubstantial%2520improvements%2520over%2520strong%2520baselines%252C%2520confirming%2520the%2520effectiveness%2520of%250AGSTM-HMU%2520in%2520extracting%2520semantic%2520regularities%2520from%2520complex%2520mobility%2520data.%2520Beyond%250Araw%2520performance%2520gains%252C%2520our%2520findings%2520also%2520suggest%2520that%2520generative%2520modeling%250Aprovides%2520a%2520promising%2520foundation%2520for%2520building%2520more%2520robust%252C%2520interpretable%252C%2520and%250Ageneralizable%2520systems%2520for%2520human%2520mobility%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSTM-HMU%3A%20Generative%20Spatio-Temporal%20Modeling%20for%20Human%20Mobility%0A%20%20Understanding&entry.906535625=Wenying%20Luo%20and%20Zhiyuan%20Lin%20and%20Wenhao%20Xu%20and%20Minghao%20Liu%20and%20Zhi%20Li&entry.1292438233=%20%20Human%20mobility%20traces%2C%20often%20recorded%20as%20sequences%20of%20check-ins%2C%20provide%20a%0Aunique%20window%20into%20both%20short-term%20visiting%20patterns%20and%20persistent%20lifestyle%0Aregularities.%20In%20this%20work%20we%20introduce%20GSTM-HMU%2C%20a%20generative%20spatio-temporal%0Aframework%20designed%20to%20advance%20mobility%20analysis%20by%20explicitly%20modeling%20the%0Asemantic%20and%20temporal%20complexity%20of%20human%20movement.%20The%20framework%20consists%20of%0Afour%20key%20innovations.%20First%2C%20a%20Spatio-Temporal%20Concept%20Encoder%20%28STCE%29%0Aintegrates%20geographic%20location%2C%20POI%20category%20semantics%2C%20and%20periodic%20temporal%0Arhythms%20into%20unified%20vector%20representations.%20Second%2C%20a%20Cognitive%20Trajectory%0AMemory%20%28CTM%29%20adaptively%20filters%20historical%20visits%2C%20emphasizing%20recent%20and%0Abehaviorally%20salient%20events%20in%20order%20to%20capture%20user%20intent%20more%20effectively.%0AThird%2C%20a%20Lifestyle%20Concept%20Bank%20%28LCB%29%20contributes%20structured%20human%20preference%0Acues%2C%20such%20as%20activity%20types%20and%20lifestyle%20patterns%2C%20to%20enhance%0Ainterpretability%20and%20personalization.%20Finally%2C%20task-oriented%20generative%20heads%0Atransform%20the%20learned%20representations%20into%20predictions%20for%20multiple%20downstream%0Atasks.%20We%20conduct%20extensive%20experiments%20on%20four%20widely%20used%20real-world%0Adatasets%2C%20including%20Gowalla%2C%20WeePlace%2C%20Brightkite%2C%20and%20FourSquare%2C%20and%20evaluate%0Aperformance%20on%20three%20benchmark%20tasks%3A%20next-location%20prediction%2C%20trajectory-user%0Aidentification%2C%20and%20time%20estimation.%20The%20results%20demonstrate%20consistent%20and%0Asubstantial%20improvements%20over%20strong%20baselines%2C%20confirming%20the%20effectiveness%20of%0AGSTM-HMU%20in%20extracting%20semantic%20regularities%20from%20complex%20mobility%20data.%20Beyond%0Araw%20performance%20gains%2C%20our%20findings%20also%20suggest%20that%20generative%20modeling%0Aprovides%20a%20promising%20foundation%20for%20building%20more%20robust%2C%20interpretable%2C%20and%0Ageneralizable%20systems%20for%20human%20mobility%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19135v1&entry.124074799=Read"},
{"title": "Unveiling Chain of Step Reasoning for Vision-Language Models with\n  Fine-grained Rewards", "author": "Honghao Chen and Xingzhou Lou and Xiaokun Feng and Kaiqi Huang and Xinlong Wang", "abstract": "  Chain of thought reasoning has demonstrated remarkable success in large\nlanguage models, yet its adaptation to vision-language reasoning remains an\nopen challenge with unclear best practices. Existing attempts typically employ\nreasoning chains at a coarse-grained level, which struggles to perform\nfine-grained structured reasoning and, more importantly, are difficult to\nevaluate the reward and quality of intermediate reasoning. In this work, we\ndelve into chain of step reasoning for vision-language models, enabling\nassessing reasoning step quality accurately and leading to effective\nreinforcement learning and inference-time scaling with fine-grained rewards. We\npresent a simple, effective, and fully transparent framework, including the\nstep-level reasoning data, process reward model (PRM), and reinforcement\nlearning training. With the proposed approaches, our models set strong\nbaselines with consistent improvements on challenging vision-language\nbenchmarks. More importantly, we conduct a thorough empirical analysis and\nablation study, unveiling the impact of each component and several intriguing\nproperties of inference-time scaling. We believe this paper serves as a\nbaseline for vision-language models and offers insights into more complex\nmultimodal reasoning. Our dataset, PRM, and code will be available at\nhttps://github.com/baaivision/CoS.\n", "link": "http://arxiv.org/abs/2509.19003v1", "date": "2025-09-23", "relevancy": 2.3585, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Chain%20of%20Step%20Reasoning%20for%20Vision-Language%20Models%20with%0A%20%20Fine-grained%20Rewards&body=Title%3A%20Unveiling%20Chain%20of%20Step%20Reasoning%20for%20Vision-Language%20Models%20with%0A%20%20Fine-grained%20Rewards%0AAuthor%3A%20Honghao%20Chen%20and%20Xingzhou%20Lou%20and%20Xiaokun%20Feng%20and%20Kaiqi%20Huang%20and%20Xinlong%20Wang%0AAbstract%3A%20%20%20Chain%20of%20thought%20reasoning%20has%20demonstrated%20remarkable%20success%20in%20large%0Alanguage%20models%2C%20yet%20its%20adaptation%20to%20vision-language%20reasoning%20remains%20an%0Aopen%20challenge%20with%20unclear%20best%20practices.%20Existing%20attempts%20typically%20employ%0Areasoning%20chains%20at%20a%20coarse-grained%20level%2C%20which%20struggles%20to%20perform%0Afine-grained%20structured%20reasoning%20and%2C%20more%20importantly%2C%20are%20difficult%20to%0Aevaluate%20the%20reward%20and%20quality%20of%20intermediate%20reasoning.%20In%20this%20work%2C%20we%0Adelve%20into%20chain%20of%20step%20reasoning%20for%20vision-language%20models%2C%20enabling%0Aassessing%20reasoning%20step%20quality%20accurately%20and%20leading%20to%20effective%0Areinforcement%20learning%20and%20inference-time%20scaling%20with%20fine-grained%20rewards.%20We%0Apresent%20a%20simple%2C%20effective%2C%20and%20fully%20transparent%20framework%2C%20including%20the%0Astep-level%20reasoning%20data%2C%20process%20reward%20model%20%28PRM%29%2C%20and%20reinforcement%0Alearning%20training.%20With%20the%20proposed%20approaches%2C%20our%20models%20set%20strong%0Abaselines%20with%20consistent%20improvements%20on%20challenging%20vision-language%0Abenchmarks.%20More%20importantly%2C%20we%20conduct%20a%20thorough%20empirical%20analysis%20and%0Aablation%20study%2C%20unveiling%20the%20impact%20of%20each%20component%20and%20several%20intriguing%0Aproperties%20of%20inference-time%20scaling.%20We%20believe%20this%20paper%20serves%20as%20a%0Abaseline%20for%20vision-language%20models%20and%20offers%20insights%20into%20more%20complex%0Amultimodal%20reasoning.%20Our%20dataset%2C%20PRM%2C%20and%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/baaivision/CoS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Chain%2520of%2520Step%2520Reasoning%2520for%2520Vision-Language%2520Models%2520with%250A%2520%2520Fine-grained%2520Rewards%26entry.906535625%3DHonghao%2520Chen%2520and%2520Xingzhou%2520Lou%2520and%2520Xiaokun%2520Feng%2520and%2520Kaiqi%2520Huang%2520and%2520Xinlong%2520Wang%26entry.1292438233%3D%2520%2520Chain%2520of%2520thought%2520reasoning%2520has%2520demonstrated%2520remarkable%2520success%2520in%2520large%250Alanguage%2520models%252C%2520yet%2520its%2520adaptation%2520to%2520vision-language%2520reasoning%2520remains%2520an%250Aopen%2520challenge%2520with%2520unclear%2520best%2520practices.%2520Existing%2520attempts%2520typically%2520employ%250Areasoning%2520chains%2520at%2520a%2520coarse-grained%2520level%252C%2520which%2520struggles%2520to%2520perform%250Afine-grained%2520structured%2520reasoning%2520and%252C%2520more%2520importantly%252C%2520are%2520difficult%2520to%250Aevaluate%2520the%2520reward%2520and%2520quality%2520of%2520intermediate%2520reasoning.%2520In%2520this%2520work%252C%2520we%250Adelve%2520into%2520chain%2520of%2520step%2520reasoning%2520for%2520vision-language%2520models%252C%2520enabling%250Aassessing%2520reasoning%2520step%2520quality%2520accurately%2520and%2520leading%2520to%2520effective%250Areinforcement%2520learning%2520and%2520inference-time%2520scaling%2520with%2520fine-grained%2520rewards.%2520We%250Apresent%2520a%2520simple%252C%2520effective%252C%2520and%2520fully%2520transparent%2520framework%252C%2520including%2520the%250Astep-level%2520reasoning%2520data%252C%2520process%2520reward%2520model%2520%2528PRM%2529%252C%2520and%2520reinforcement%250Alearning%2520training.%2520With%2520the%2520proposed%2520approaches%252C%2520our%2520models%2520set%2520strong%250Abaselines%2520with%2520consistent%2520improvements%2520on%2520challenging%2520vision-language%250Abenchmarks.%2520More%2520importantly%252C%2520we%2520conduct%2520a%2520thorough%2520empirical%2520analysis%2520and%250Aablation%2520study%252C%2520unveiling%2520the%2520impact%2520of%2520each%2520component%2520and%2520several%2520intriguing%250Aproperties%2520of%2520inference-time%2520scaling.%2520We%2520believe%2520this%2520paper%2520serves%2520as%2520a%250Abaseline%2520for%2520vision-language%2520models%2520and%2520offers%2520insights%2520into%2520more%2520complex%250Amultimodal%2520reasoning.%2520Our%2520dataset%252C%2520PRM%252C%2520and%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/baaivision/CoS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Chain%20of%20Step%20Reasoning%20for%20Vision-Language%20Models%20with%0A%20%20Fine-grained%20Rewards&entry.906535625=Honghao%20Chen%20and%20Xingzhou%20Lou%20and%20Xiaokun%20Feng%20and%20Kaiqi%20Huang%20and%20Xinlong%20Wang&entry.1292438233=%20%20Chain%20of%20thought%20reasoning%20has%20demonstrated%20remarkable%20success%20in%20large%0Alanguage%20models%2C%20yet%20its%20adaptation%20to%20vision-language%20reasoning%20remains%20an%0Aopen%20challenge%20with%20unclear%20best%20practices.%20Existing%20attempts%20typically%20employ%0Areasoning%20chains%20at%20a%20coarse-grained%20level%2C%20which%20struggles%20to%20perform%0Afine-grained%20structured%20reasoning%20and%2C%20more%20importantly%2C%20are%20difficult%20to%0Aevaluate%20the%20reward%20and%20quality%20of%20intermediate%20reasoning.%20In%20this%20work%2C%20we%0Adelve%20into%20chain%20of%20step%20reasoning%20for%20vision-language%20models%2C%20enabling%0Aassessing%20reasoning%20step%20quality%20accurately%20and%20leading%20to%20effective%0Areinforcement%20learning%20and%20inference-time%20scaling%20with%20fine-grained%20rewards.%20We%0Apresent%20a%20simple%2C%20effective%2C%20and%20fully%20transparent%20framework%2C%20including%20the%0Astep-level%20reasoning%20data%2C%20process%20reward%20model%20%28PRM%29%2C%20and%20reinforcement%0Alearning%20training.%20With%20the%20proposed%20approaches%2C%20our%20models%20set%20strong%0Abaselines%20with%20consistent%20improvements%20on%20challenging%20vision-language%0Abenchmarks.%20More%20importantly%2C%20we%20conduct%20a%20thorough%20empirical%20analysis%20and%0Aablation%20study%2C%20unveiling%20the%20impact%20of%20each%20component%20and%20several%20intriguing%0Aproperties%20of%20inference-time%20scaling.%20We%20believe%20this%20paper%20serves%20as%20a%0Abaseline%20for%20vision-language%20models%20and%20offers%20insights%20into%20more%20complex%0Amultimodal%20reasoning.%20Our%20dataset%2C%20PRM%2C%20and%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/baaivision/CoS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19003v1&entry.124074799=Read"},
{"title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for\n  Spatio-Temporal Heatmaps", "author": "Gabriel Maldonado and Narges Rashvand and Armin Danesh Pazho and Ghazal Alinezhad Noghre and Vinit Katariya and Hamed Tabkhi", "abstract": "  Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization.\n", "link": "http://arxiv.org/abs/2509.19252v1", "date": "2025-09-23", "relevancy": 2.3276, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5841}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5811}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarially-Refined%20VQ-GAN%20with%20Dense%20Motion%20Tokenization%20for%0A%20%20Spatio-Temporal%20Heatmaps&body=Title%3A%20Adversarially-Refined%20VQ-GAN%20with%20Dense%20Motion%20Tokenization%20for%0A%20%20Spatio-Temporal%20Heatmaps%0AAuthor%3A%20Gabriel%20Maldonado%20and%20Narges%20Rashvand%20and%20Armin%20Danesh%20Pazho%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Vinit%20Katariya%20and%20Hamed%20Tabkhi%0AAbstract%3A%20%20%20Continuous%20human%20motion%20understanding%20remains%20a%20core%20challenge%20in%20computer%0Avision%20due%20to%20its%20high%20dimensionality%20and%20inherent%20redundancy.%20Efficient%0Acompression%20and%20representation%20are%20crucial%20for%20analyzing%20complex%20motion%0Adynamics.%20In%20this%20work%2C%20we%20introduce%20an%20adversarially-refined%20VQ-GAN%20framework%0Awith%20dense%20motion%20tokenization%20for%20compressing%20spatio-temporal%20heatmaps%20while%0Apreserving%20the%20fine-grained%20traces%20of%20human%20motion.%20Our%20approach%20combines%20dense%0Amotion%20tokenization%20with%20adversarial%20refinement%2C%20which%20eliminates%0Areconstruction%20artifacts%20like%20motion%20smearing%20and%20temporal%20misalignment%0Aobserved%20in%20non-adversarial%20baselines.%20Our%20experiments%20on%20the%20CMU%20Panoptic%0Adataset%20provide%20conclusive%20evidence%20of%20our%20method%27s%20superiority%2C%20outperforming%0Athe%20dVAE%20baseline%20by%209.31%25%20SSIM%20and%20reducing%20temporal%20instability%20by%2037.1%25.%0AFurthermore%2C%20our%20dense%20tokenization%20strategy%20enables%20a%20novel%20analysis%20of%20motion%0Acomplexity%2C%20revealing%20that%202D%20motion%20can%20be%20optimally%20represented%20with%20a%0Acompact%20128-token%20vocabulary%2C%20while%203D%20motion%27s%20complexity%20demands%20a%20much%0Alarger%201024-token%20codebook%20for%20faithful%20reconstruction.%20These%20results%20establish%0Apractical%20deployment%20feasibility%20across%20diverse%20motion%20analysis%20applications.%0AThe%20code%20base%20for%20this%20work%20is%20available%20at%0Ahttps%3A//github.com/TeCSAR-UNCC/Pose-Quantization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarially-Refined%2520VQ-GAN%2520with%2520Dense%2520Motion%2520Tokenization%2520for%250A%2520%2520Spatio-Temporal%2520Heatmaps%26entry.906535625%3DGabriel%2520Maldonado%2520and%2520Narges%2520Rashvand%2520and%2520Armin%2520Danesh%2520Pazho%2520and%2520Ghazal%2520Alinezhad%2520Noghre%2520and%2520Vinit%2520Katariya%2520and%2520Hamed%2520Tabkhi%26entry.1292438233%3D%2520%2520Continuous%2520human%2520motion%2520understanding%2520remains%2520a%2520core%2520challenge%2520in%2520computer%250Avision%2520due%2520to%2520its%2520high%2520dimensionality%2520and%2520inherent%2520redundancy.%2520Efficient%250Acompression%2520and%2520representation%2520are%2520crucial%2520for%2520analyzing%2520complex%2520motion%250Adynamics.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520adversarially-refined%2520VQ-GAN%2520framework%250Awith%2520dense%2520motion%2520tokenization%2520for%2520compressing%2520spatio-temporal%2520heatmaps%2520while%250Apreserving%2520the%2520fine-grained%2520traces%2520of%2520human%2520motion.%2520Our%2520approach%2520combines%2520dense%250Amotion%2520tokenization%2520with%2520adversarial%2520refinement%252C%2520which%2520eliminates%250Areconstruction%2520artifacts%2520like%2520motion%2520smearing%2520and%2520temporal%2520misalignment%250Aobserved%2520in%2520non-adversarial%2520baselines.%2520Our%2520experiments%2520on%2520the%2520CMU%2520Panoptic%250Adataset%2520provide%2520conclusive%2520evidence%2520of%2520our%2520method%2527s%2520superiority%252C%2520outperforming%250Athe%2520dVAE%2520baseline%2520by%25209.31%2525%2520SSIM%2520and%2520reducing%2520temporal%2520instability%2520by%252037.1%2525.%250AFurthermore%252C%2520our%2520dense%2520tokenization%2520strategy%2520enables%2520a%2520novel%2520analysis%2520of%2520motion%250Acomplexity%252C%2520revealing%2520that%25202D%2520motion%2520can%2520be%2520optimally%2520represented%2520with%2520a%250Acompact%2520128-token%2520vocabulary%252C%2520while%25203D%2520motion%2527s%2520complexity%2520demands%2520a%2520much%250Alarger%25201024-token%2520codebook%2520for%2520faithful%2520reconstruction.%2520These%2520results%2520establish%250Apractical%2520deployment%2520feasibility%2520across%2520diverse%2520motion%2520analysis%2520applications.%250AThe%2520code%2520base%2520for%2520this%2520work%2520is%2520available%2520at%250Ahttps%253A//github.com/TeCSAR-UNCC/Pose-Quantization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarially-Refined%20VQ-GAN%20with%20Dense%20Motion%20Tokenization%20for%0A%20%20Spatio-Temporal%20Heatmaps&entry.906535625=Gabriel%20Maldonado%20and%20Narges%20Rashvand%20and%20Armin%20Danesh%20Pazho%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Vinit%20Katariya%20and%20Hamed%20Tabkhi&entry.1292438233=%20%20Continuous%20human%20motion%20understanding%20remains%20a%20core%20challenge%20in%20computer%0Avision%20due%20to%20its%20high%20dimensionality%20and%20inherent%20redundancy.%20Efficient%0Acompression%20and%20representation%20are%20crucial%20for%20analyzing%20complex%20motion%0Adynamics.%20In%20this%20work%2C%20we%20introduce%20an%20adversarially-refined%20VQ-GAN%20framework%0Awith%20dense%20motion%20tokenization%20for%20compressing%20spatio-temporal%20heatmaps%20while%0Apreserving%20the%20fine-grained%20traces%20of%20human%20motion.%20Our%20approach%20combines%20dense%0Amotion%20tokenization%20with%20adversarial%20refinement%2C%20which%20eliminates%0Areconstruction%20artifacts%20like%20motion%20smearing%20and%20temporal%20misalignment%0Aobserved%20in%20non-adversarial%20baselines.%20Our%20experiments%20on%20the%20CMU%20Panoptic%0Adataset%20provide%20conclusive%20evidence%20of%20our%20method%27s%20superiority%2C%20outperforming%0Athe%20dVAE%20baseline%20by%209.31%25%20SSIM%20and%20reducing%20temporal%20instability%20by%2037.1%25.%0AFurthermore%2C%20our%20dense%20tokenization%20strategy%20enables%20a%20novel%20analysis%20of%20motion%0Acomplexity%2C%20revealing%20that%202D%20motion%20can%20be%20optimally%20represented%20with%20a%0Acompact%20128-token%20vocabulary%2C%20while%203D%20motion%27s%20complexity%20demands%20a%20much%0Alarger%201024-token%20codebook%20for%20faithful%20reconstruction.%20These%20results%20establish%0Apractical%20deployment%20feasibility%20across%20diverse%20motion%20analysis%20applications.%0AThe%20code%20base%20for%20this%20work%20is%20available%20at%0Ahttps%3A//github.com/TeCSAR-UNCC/Pose-Quantization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19252v1&entry.124074799=Read"},
{"title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense\n  Overlaps", "author": "Bingnan Li and Chen-Yu Wang and Haiyang Xu and Xiang Zhang and Ethan Armand and Divyansh Srivastava and Xiaojun Shan and Zeyuan Chen and Jianwen Xie and Zhuowen Tu", "abstract": "  Despite steady progress in layout-to-image generation, current methods still\nstruggle with layouts containing significant overlap between bounding boxes. We\nidentify two primary challenges: (1) large overlapping regions and (2)\noverlapping instances with minimal semantic distinction. Through both\nqualitative examples and quantitative analysis, we demonstrate how these\nfactors degrade generation quality. To systematically assess this issue, we\nintroduce OverLayScore, a novel metric that quantifies the complexity of\noverlapping bounding boxes. Our analysis reveals that existing benchmarks are\nbiased toward simpler cases with low OverLayScore values, limiting their\neffectiveness in evaluating model performance under more challenging\nconditions. To bridge this gap, we present OverLayBench, a new benchmark\nfeaturing high-quality annotations and a balanced distribution across different\nlevels of OverLayScore. As an initial step toward improving performance on\ncomplex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a\ncurated amodal mask dataset. Together, our contributions lay the groundwork for\nmore robust layout-to-image generation under realistic and challenging\nscenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.\n", "link": "http://arxiv.org/abs/2509.19282v1", "date": "2025-09-23", "relevancy": 2.3274, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6437}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6041}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OverLayBench%3A%20A%20Benchmark%20for%20Layout-to-Image%20Generation%20with%20Dense%0A%20%20Overlaps&body=Title%3A%20OverLayBench%3A%20A%20Benchmark%20for%20Layout-to-Image%20Generation%20with%20Dense%0A%20%20Overlaps%0AAuthor%3A%20Bingnan%20Li%20and%20Chen-Yu%20Wang%20and%20Haiyang%20Xu%20and%20Xiang%20Zhang%20and%20Ethan%20Armand%20and%20Divyansh%20Srivastava%20and%20Xiaojun%20Shan%20and%20Zeyuan%20Chen%20and%20Jianwen%20Xie%20and%20Zhuowen%20Tu%0AAbstract%3A%20%20%20Despite%20steady%20progress%20in%20layout-to-image%20generation%2C%20current%20methods%20still%0Astruggle%20with%20layouts%20containing%20significant%20overlap%20between%20bounding%20boxes.%20We%0Aidentify%20two%20primary%20challenges%3A%20%281%29%20large%20overlapping%20regions%20and%20%282%29%0Aoverlapping%20instances%20with%20minimal%20semantic%20distinction.%20Through%20both%0Aqualitative%20examples%20and%20quantitative%20analysis%2C%20we%20demonstrate%20how%20these%0Afactors%20degrade%20generation%20quality.%20To%20systematically%20assess%20this%20issue%2C%20we%0Aintroduce%20OverLayScore%2C%20a%20novel%20metric%20that%20quantifies%20the%20complexity%20of%0Aoverlapping%20bounding%20boxes.%20Our%20analysis%20reveals%20that%20existing%20benchmarks%20are%0Abiased%20toward%20simpler%20cases%20with%20low%20OverLayScore%20values%2C%20limiting%20their%0Aeffectiveness%20in%20evaluating%20model%20performance%20under%20more%20challenging%0Aconditions.%20To%20bridge%20this%20gap%2C%20we%20present%20OverLayBench%2C%20a%20new%20benchmark%0Afeaturing%20high-quality%20annotations%20and%20a%20balanced%20distribution%20across%20different%0Alevels%20of%20OverLayScore.%20As%20an%20initial%20step%20toward%20improving%20performance%20on%0Acomplex%20overlaps%2C%20we%20also%20propose%20CreatiLayout-AM%2C%20a%20model%20fine-tuned%20on%20a%0Acurated%20amodal%20mask%20dataset.%20Together%2C%20our%20contributions%20lay%20the%20groundwork%20for%0Amore%20robust%20layout-to-image%20generation%20under%20realistic%20and%20challenging%0Ascenarios.%20Project%20link%3A%20https%3A//mlpc-ucsd.github.io/OverLayBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverLayBench%253A%2520A%2520Benchmark%2520for%2520Layout-to-Image%2520Generation%2520with%2520Dense%250A%2520%2520Overlaps%26entry.906535625%3DBingnan%2520Li%2520and%2520Chen-Yu%2520Wang%2520and%2520Haiyang%2520Xu%2520and%2520Xiang%2520Zhang%2520and%2520Ethan%2520Armand%2520and%2520Divyansh%2520Srivastava%2520and%2520Xiaojun%2520Shan%2520and%2520Zeyuan%2520Chen%2520and%2520Jianwen%2520Xie%2520and%2520Zhuowen%2520Tu%26entry.1292438233%3D%2520%2520Despite%2520steady%2520progress%2520in%2520layout-to-image%2520generation%252C%2520current%2520methods%2520still%250Astruggle%2520with%2520layouts%2520containing%2520significant%2520overlap%2520between%2520bounding%2520boxes.%2520We%250Aidentify%2520two%2520primary%2520challenges%253A%2520%25281%2529%2520large%2520overlapping%2520regions%2520and%2520%25282%2529%250Aoverlapping%2520instances%2520with%2520minimal%2520semantic%2520distinction.%2520Through%2520both%250Aqualitative%2520examples%2520and%2520quantitative%2520analysis%252C%2520we%2520demonstrate%2520how%2520these%250Afactors%2520degrade%2520generation%2520quality.%2520To%2520systematically%2520assess%2520this%2520issue%252C%2520we%250Aintroduce%2520OverLayScore%252C%2520a%2520novel%2520metric%2520that%2520quantifies%2520the%2520complexity%2520of%250Aoverlapping%2520bounding%2520boxes.%2520Our%2520analysis%2520reveals%2520that%2520existing%2520benchmarks%2520are%250Abiased%2520toward%2520simpler%2520cases%2520with%2520low%2520OverLayScore%2520values%252C%2520limiting%2520their%250Aeffectiveness%2520in%2520evaluating%2520model%2520performance%2520under%2520more%2520challenging%250Aconditions.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520OverLayBench%252C%2520a%2520new%2520benchmark%250Afeaturing%2520high-quality%2520annotations%2520and%2520a%2520balanced%2520distribution%2520across%2520different%250Alevels%2520of%2520OverLayScore.%2520As%2520an%2520initial%2520step%2520toward%2520improving%2520performance%2520on%250Acomplex%2520overlaps%252C%2520we%2520also%2520propose%2520CreatiLayout-AM%252C%2520a%2520model%2520fine-tuned%2520on%2520a%250Acurated%2520amodal%2520mask%2520dataset.%2520Together%252C%2520our%2520contributions%2520lay%2520the%2520groundwork%2520for%250Amore%2520robust%2520layout-to-image%2520generation%2520under%2520realistic%2520and%2520challenging%250Ascenarios.%2520Project%2520link%253A%2520https%253A//mlpc-ucsd.github.io/OverLayBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OverLayBench%3A%20A%20Benchmark%20for%20Layout-to-Image%20Generation%20with%20Dense%0A%20%20Overlaps&entry.906535625=Bingnan%20Li%20and%20Chen-Yu%20Wang%20and%20Haiyang%20Xu%20and%20Xiang%20Zhang%20and%20Ethan%20Armand%20and%20Divyansh%20Srivastava%20and%20Xiaojun%20Shan%20and%20Zeyuan%20Chen%20and%20Jianwen%20Xie%20and%20Zhuowen%20Tu&entry.1292438233=%20%20Despite%20steady%20progress%20in%20layout-to-image%20generation%2C%20current%20methods%20still%0Astruggle%20with%20layouts%20containing%20significant%20overlap%20between%20bounding%20boxes.%20We%0Aidentify%20two%20primary%20challenges%3A%20%281%29%20large%20overlapping%20regions%20and%20%282%29%0Aoverlapping%20instances%20with%20minimal%20semantic%20distinction.%20Through%20both%0Aqualitative%20examples%20and%20quantitative%20analysis%2C%20we%20demonstrate%20how%20these%0Afactors%20degrade%20generation%20quality.%20To%20systematically%20assess%20this%20issue%2C%20we%0Aintroduce%20OverLayScore%2C%20a%20novel%20metric%20that%20quantifies%20the%20complexity%20of%0Aoverlapping%20bounding%20boxes.%20Our%20analysis%20reveals%20that%20existing%20benchmarks%20are%0Abiased%20toward%20simpler%20cases%20with%20low%20OverLayScore%20values%2C%20limiting%20their%0Aeffectiveness%20in%20evaluating%20model%20performance%20under%20more%20challenging%0Aconditions.%20To%20bridge%20this%20gap%2C%20we%20present%20OverLayBench%2C%20a%20new%20benchmark%0Afeaturing%20high-quality%20annotations%20and%20a%20balanced%20distribution%20across%20different%0Alevels%20of%20OverLayScore.%20As%20an%20initial%20step%20toward%20improving%20performance%20on%0Acomplex%20overlaps%2C%20we%20also%20propose%20CreatiLayout-AM%2C%20a%20model%20fine-tuned%20on%20a%0Acurated%20amodal%20mask%20dataset.%20Together%2C%20our%20contributions%20lay%20the%20groundwork%20for%0Amore%20robust%20layout-to-image%20generation%20under%20realistic%20and%20challenging%0Ascenarios.%20Project%20link%3A%20https%3A//mlpc-ucsd.github.io/OverLayBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19282v1&entry.124074799=Read"},
{"title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation", "author": "Sarvesh Prajapati and Ananya Trivedi and Nathaniel Hanson and Bruce Maxwell and Taskin Padir", "abstract": "  Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.\n", "link": "http://arxiv.org/abs/2509.19105v1", "date": "2025-09-23", "relevancy": 2.3205, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6589}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5805}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Signature%20Mapping%20from%20RGB%20Imagery%20for%20Terrain-Aware%20Navigation&body=Title%3A%20Spectral%20Signature%20Mapping%20from%20RGB%20Imagery%20for%20Terrain-Aware%20Navigation%0AAuthor%3A%20Sarvesh%20Prajapati%20and%20Ananya%20Trivedi%20and%20Nathaniel%20Hanson%20and%20Bruce%20Maxwell%20and%20Taskin%20Padir%0AAbstract%3A%20%20%20Successful%20navigation%20in%20outdoor%20environments%20requires%20accurate%20prediction%20of%0Athe%20physical%20interactions%20between%20the%20robot%20and%20the%20terrain.%20To%20this%20end%2C%0Aseveral%20methods%20rely%20on%20geometric%20or%20semantic%20labels%20to%20classify%20traversable%0Asurfaces.%20However%2C%20such%20labels%20cannot%20distinguish%20visually%20similar%20surfaces%0Athat%20differ%20in%20material%20properties.%20Spectral%20sensors%20enable%20inference%20of%0Amaterial%20composition%20from%20surface%20reflectance%20measured%20across%20multiple%0Awavelength%20bands.%20Although%20spectral%20sensing%20is%20gaining%20traction%20in%20robotics%2C%0Awidespread%20deployment%20remains%20constrained%20by%20the%20need%20for%20custom%20hardware%0Aintegration%2C%20high%20sensor%20costs%2C%20and%20compute-intensive%20processing%20pipelines.%20In%0Athis%20paper%2C%20we%20present%20RGB%20Image%20to%20Spectral%20Signature%20Neural%20Network%20%28RS-Net%29%2C%0Aa%20deep%20neural%20network%20designed%20to%20bridge%20the%20gap%20between%20the%20accessibility%20of%0ARGB%20sensing%20and%20the%20rich%20material%20information%20provided%20by%20spectral%20data.%20RS-Net%0Apredicts%20spectral%20signatures%20from%20RGB%20patches%2C%20which%20we%20map%20to%20terrain%20labels%0Aand%20friction%20coefficients.%20The%20resulting%20terrain%20classifications%20are%20integrated%0Ainto%20a%20sampling-based%20motion%20planner%20for%20a%20wheeled%20robot%20operating%20in%20outdoor%0Aenvironments.%20Likewise%2C%20the%20friction%20estimates%20are%20incorporated%20into%20a%0Acontact-force-based%20MPC%20for%20a%20quadruped%20robot%20navigating%20slippery%20surfaces.%0AThus%2C%20we%20introduce%20a%20framework%20that%20learns%20the%20task-relevant%20physical%20property%0Aonce%20during%20training%20and%20thereafter%20relies%20solely%20on%20RGB%20sensing%20at%20test%20time.%0AThe%20code%20is%20available%20at%20https%3A//github.com/prajapatisarvesh/RS-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Signature%2520Mapping%2520from%2520RGB%2520Imagery%2520for%2520Terrain-Aware%2520Navigation%26entry.906535625%3DSarvesh%2520Prajapati%2520and%2520Ananya%2520Trivedi%2520and%2520Nathaniel%2520Hanson%2520and%2520Bruce%2520Maxwell%2520and%2520Taskin%2520Padir%26entry.1292438233%3D%2520%2520Successful%2520navigation%2520in%2520outdoor%2520environments%2520requires%2520accurate%2520prediction%2520of%250Athe%2520physical%2520interactions%2520between%2520the%2520robot%2520and%2520the%2520terrain.%2520To%2520this%2520end%252C%250Aseveral%2520methods%2520rely%2520on%2520geometric%2520or%2520semantic%2520labels%2520to%2520classify%2520traversable%250Asurfaces.%2520However%252C%2520such%2520labels%2520cannot%2520distinguish%2520visually%2520similar%2520surfaces%250Athat%2520differ%2520in%2520material%2520properties.%2520Spectral%2520sensors%2520enable%2520inference%2520of%250Amaterial%2520composition%2520from%2520surface%2520reflectance%2520measured%2520across%2520multiple%250Awavelength%2520bands.%2520Although%2520spectral%2520sensing%2520is%2520gaining%2520traction%2520in%2520robotics%252C%250Awidespread%2520deployment%2520remains%2520constrained%2520by%2520the%2520need%2520for%2520custom%2520hardware%250Aintegration%252C%2520high%2520sensor%2520costs%252C%2520and%2520compute-intensive%2520processing%2520pipelines.%2520In%250Athis%2520paper%252C%2520we%2520present%2520RGB%2520Image%2520to%2520Spectral%2520Signature%2520Neural%2520Network%2520%2528RS-Net%2529%252C%250Aa%2520deep%2520neural%2520network%2520designed%2520to%2520bridge%2520the%2520gap%2520between%2520the%2520accessibility%2520of%250ARGB%2520sensing%2520and%2520the%2520rich%2520material%2520information%2520provided%2520by%2520spectral%2520data.%2520RS-Net%250Apredicts%2520spectral%2520signatures%2520from%2520RGB%2520patches%252C%2520which%2520we%2520map%2520to%2520terrain%2520labels%250Aand%2520friction%2520coefficients.%2520The%2520resulting%2520terrain%2520classifications%2520are%2520integrated%250Ainto%2520a%2520sampling-based%2520motion%2520planner%2520for%2520a%2520wheeled%2520robot%2520operating%2520in%2520outdoor%250Aenvironments.%2520Likewise%252C%2520the%2520friction%2520estimates%2520are%2520incorporated%2520into%2520a%250Acontact-force-based%2520MPC%2520for%2520a%2520quadruped%2520robot%2520navigating%2520slippery%2520surfaces.%250AThus%252C%2520we%2520introduce%2520a%2520framework%2520that%2520learns%2520the%2520task-relevant%2520physical%2520property%250Aonce%2520during%2520training%2520and%2520thereafter%2520relies%2520solely%2520on%2520RGB%2520sensing%2520at%2520test%2520time.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/prajapatisarvesh/RS-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Signature%20Mapping%20from%20RGB%20Imagery%20for%20Terrain-Aware%20Navigation&entry.906535625=Sarvesh%20Prajapati%20and%20Ananya%20Trivedi%20and%20Nathaniel%20Hanson%20and%20Bruce%20Maxwell%20and%20Taskin%20Padir&entry.1292438233=%20%20Successful%20navigation%20in%20outdoor%20environments%20requires%20accurate%20prediction%20of%0Athe%20physical%20interactions%20between%20the%20robot%20and%20the%20terrain.%20To%20this%20end%2C%0Aseveral%20methods%20rely%20on%20geometric%20or%20semantic%20labels%20to%20classify%20traversable%0Asurfaces.%20However%2C%20such%20labels%20cannot%20distinguish%20visually%20similar%20surfaces%0Athat%20differ%20in%20material%20properties.%20Spectral%20sensors%20enable%20inference%20of%0Amaterial%20composition%20from%20surface%20reflectance%20measured%20across%20multiple%0Awavelength%20bands.%20Although%20spectral%20sensing%20is%20gaining%20traction%20in%20robotics%2C%0Awidespread%20deployment%20remains%20constrained%20by%20the%20need%20for%20custom%20hardware%0Aintegration%2C%20high%20sensor%20costs%2C%20and%20compute-intensive%20processing%20pipelines.%20In%0Athis%20paper%2C%20we%20present%20RGB%20Image%20to%20Spectral%20Signature%20Neural%20Network%20%28RS-Net%29%2C%0Aa%20deep%20neural%20network%20designed%20to%20bridge%20the%20gap%20between%20the%20accessibility%20of%0ARGB%20sensing%20and%20the%20rich%20material%20information%20provided%20by%20spectral%20data.%20RS-Net%0Apredicts%20spectral%20signatures%20from%20RGB%20patches%2C%20which%20we%20map%20to%20terrain%20labels%0Aand%20friction%20coefficients.%20The%20resulting%20terrain%20classifications%20are%20integrated%0Ainto%20a%20sampling-based%20motion%20planner%20for%20a%20wheeled%20robot%20operating%20in%20outdoor%0Aenvironments.%20Likewise%2C%20the%20friction%20estimates%20are%20incorporated%20into%20a%0Acontact-force-based%20MPC%20for%20a%20quadruped%20robot%20navigating%20slippery%20surfaces.%0AThus%2C%20we%20introduce%20a%20framework%20that%20learns%20the%20task-relevant%20physical%20property%0Aonce%20during%20training%20and%20thereafter%20relies%20solely%20on%20RGB%20sensing%20at%20test%20time.%0AThe%20code%20is%20available%20at%20https%3A//github.com/prajapatisarvesh/RS-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19105v1&entry.124074799=Read"},
{"title": "Do It Yourself: Learning Semantic Correspondence from Pseudo-Labels", "author": "Olaf D\u00fcnkel and Thomas Wimmer and Christian Theobalt and Christian Rupprecht and Adam Kortylewski", "abstract": "  Finding correspondences between semantically similar points across images and\nobject instances is one of the everlasting challenges in computer vision. While\nlarge pre-trained vision models have recently been demonstrated as effective\npriors for semantic matching, they still suffer from ambiguities for symmetric\nobjects or repeated object parts. We propose improving semantic correspondence\nestimation through 3D-aware pseudo-labeling. Specifically, we train an adapter\nto refine off-the-shelf features using pseudo-labels obtained via 3D-aware\nchaining, filtering wrong labels through relaxed cyclic consistency, and 3D\nspherical prototype mapping constraints. While reducing the need for\ndataset-specific annotations compared to prior work, we establish a new\nstate-of-the-art on SPair-71k, achieving an absolute gain of over 4% and of\nover 7% compared to methods with similar supervision requirements. The\ngenerality of our proposed approach simplifies the extension of training to\nother data sources, which we demonstrate in our experiments.\n", "link": "http://arxiv.org/abs/2506.05312v3", "date": "2025-09-23", "relevancy": 2.3188, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5836}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5816}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20It%20Yourself%3A%20Learning%20Semantic%20Correspondence%20from%20Pseudo-Labels&body=Title%3A%20Do%20It%20Yourself%3A%20Learning%20Semantic%20Correspondence%20from%20Pseudo-Labels%0AAuthor%3A%20Olaf%20D%C3%BCnkel%20and%20Thomas%20Wimmer%20and%20Christian%20Theobalt%20and%20Christian%20Rupprecht%20and%20Adam%20Kortylewski%0AAbstract%3A%20%20%20Finding%20correspondences%20between%20semantically%20similar%20points%20across%20images%20and%0Aobject%20instances%20is%20one%20of%20the%20everlasting%20challenges%20in%20computer%20vision.%20While%0Alarge%20pre-trained%20vision%20models%20have%20recently%20been%20demonstrated%20as%20effective%0Apriors%20for%20semantic%20matching%2C%20they%20still%20suffer%20from%20ambiguities%20for%20symmetric%0Aobjects%20or%20repeated%20object%20parts.%20We%20propose%20improving%20semantic%20correspondence%0Aestimation%20through%203D-aware%20pseudo-labeling.%20Specifically%2C%20we%20train%20an%20adapter%0Ato%20refine%20off-the-shelf%20features%20using%20pseudo-labels%20obtained%20via%203D-aware%0Achaining%2C%20filtering%20wrong%20labels%20through%20relaxed%20cyclic%20consistency%2C%20and%203D%0Aspherical%20prototype%20mapping%20constraints.%20While%20reducing%20the%20need%20for%0Adataset-specific%20annotations%20compared%20to%20prior%20work%2C%20we%20establish%20a%20new%0Astate-of-the-art%20on%20SPair-71k%2C%20achieving%20an%20absolute%20gain%20of%20over%204%25%20and%20of%0Aover%207%25%20compared%20to%20methods%20with%20similar%20supervision%20requirements.%20The%0Agenerality%20of%20our%20proposed%20approach%20simplifies%20the%20extension%20of%20training%20to%0Aother%20data%20sources%2C%20which%20we%20demonstrate%20in%20our%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05312v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520It%2520Yourself%253A%2520Learning%2520Semantic%2520Correspondence%2520from%2520Pseudo-Labels%26entry.906535625%3DOlaf%2520D%25C3%25BCnkel%2520and%2520Thomas%2520Wimmer%2520and%2520Christian%2520Theobalt%2520and%2520Christian%2520Rupprecht%2520and%2520Adam%2520Kortylewski%26entry.1292438233%3D%2520%2520Finding%2520correspondences%2520between%2520semantically%2520similar%2520points%2520across%2520images%2520and%250Aobject%2520instances%2520is%2520one%2520of%2520the%2520everlasting%2520challenges%2520in%2520computer%2520vision.%2520While%250Alarge%2520pre-trained%2520vision%2520models%2520have%2520recently%2520been%2520demonstrated%2520as%2520effective%250Apriors%2520for%2520semantic%2520matching%252C%2520they%2520still%2520suffer%2520from%2520ambiguities%2520for%2520symmetric%250Aobjects%2520or%2520repeated%2520object%2520parts.%2520We%2520propose%2520improving%2520semantic%2520correspondence%250Aestimation%2520through%25203D-aware%2520pseudo-labeling.%2520Specifically%252C%2520we%2520train%2520an%2520adapter%250Ato%2520refine%2520off-the-shelf%2520features%2520using%2520pseudo-labels%2520obtained%2520via%25203D-aware%250Achaining%252C%2520filtering%2520wrong%2520labels%2520through%2520relaxed%2520cyclic%2520consistency%252C%2520and%25203D%250Aspherical%2520prototype%2520mapping%2520constraints.%2520While%2520reducing%2520the%2520need%2520for%250Adataset-specific%2520annotations%2520compared%2520to%2520prior%2520work%252C%2520we%2520establish%2520a%2520new%250Astate-of-the-art%2520on%2520SPair-71k%252C%2520achieving%2520an%2520absolute%2520gain%2520of%2520over%25204%2525%2520and%2520of%250Aover%25207%2525%2520compared%2520to%2520methods%2520with%2520similar%2520supervision%2520requirements.%2520The%250Agenerality%2520of%2520our%2520proposed%2520approach%2520simplifies%2520the%2520extension%2520of%2520training%2520to%250Aother%2520data%2520sources%252C%2520which%2520we%2520demonstrate%2520in%2520our%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05312v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20It%20Yourself%3A%20Learning%20Semantic%20Correspondence%20from%20Pseudo-Labels&entry.906535625=Olaf%20D%C3%BCnkel%20and%20Thomas%20Wimmer%20and%20Christian%20Theobalt%20and%20Christian%20Rupprecht%20and%20Adam%20Kortylewski&entry.1292438233=%20%20Finding%20correspondences%20between%20semantically%20similar%20points%20across%20images%20and%0Aobject%20instances%20is%20one%20of%20the%20everlasting%20challenges%20in%20computer%20vision.%20While%0Alarge%20pre-trained%20vision%20models%20have%20recently%20been%20demonstrated%20as%20effective%0Apriors%20for%20semantic%20matching%2C%20they%20still%20suffer%20from%20ambiguities%20for%20symmetric%0Aobjects%20or%20repeated%20object%20parts.%20We%20propose%20improving%20semantic%20correspondence%0Aestimation%20through%203D-aware%20pseudo-labeling.%20Specifically%2C%20we%20train%20an%20adapter%0Ato%20refine%20off-the-shelf%20features%20using%20pseudo-labels%20obtained%20via%203D-aware%0Achaining%2C%20filtering%20wrong%20labels%20through%20relaxed%20cyclic%20consistency%2C%20and%203D%0Aspherical%20prototype%20mapping%20constraints.%20While%20reducing%20the%20need%20for%0Adataset-specific%20annotations%20compared%20to%20prior%20work%2C%20we%20establish%20a%20new%0Astate-of-the-art%20on%20SPair-71k%2C%20achieving%20an%20absolute%20gain%20of%20over%204%25%20and%20of%0Aover%207%25%20compared%20to%20methods%20with%20similar%20supervision%20requirements.%20The%0Agenerality%20of%20our%20proposed%20approach%20simplifies%20the%20extension%20of%20training%20to%0Aother%20data%20sources%2C%20which%20we%20demonstrate%20in%20our%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05312v3&entry.124074799=Read"},
{"title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "author": "Dapeng Zhang and Jin Sun and Chenghui Hu and Xiaoyan Wu and Zhenlong Yuan and Rui Zhou and Fei Shen and Qingguo Zhou", "abstract": "  The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.\n", "link": "http://arxiv.org/abs/2509.19012v1", "date": "2025-09-23", "relevancy": 2.3163, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pure%20Vision%20Language%20Action%20%28VLA%29%20Models%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Pure%20Vision%20Language%20Action%20%28VLA%29%20Models%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Dapeng%20Zhang%20and%20Jin%20Sun%20and%20Chenghui%20Hu%20and%20Xiaoyan%20Wu%20and%20Zhenlong%20Yuan%20and%20Rui%20Zhou%20and%20Fei%20Shen%20and%20Qingguo%20Zhou%0AAbstract%3A%20%20%20The%20emergence%20of%20Vision%20Language%20Action%20%28VLA%29%20models%20marks%20a%20paradigm%20shift%0Afrom%20traditional%20policy-based%20control%20to%20generalized%20robotics%2C%20reframing%20Vision%0ALanguage%20Models%20%28VLMs%29%20from%20passive%20sequence%20generators%20into%20active%20agents%20for%0Amanipulation%20and%20decision-making%20in%20complex%2C%20dynamic%20environments.%20This%20survey%0Adelves%20into%20advanced%20VLA%20methods%2C%20aiming%20to%20provide%20a%20clear%20taxonomy%20and%20a%0Asystematic%2C%20comprehensive%20review%20of%20existing%20research.%20It%20presents%20a%0Acomprehensive%20analysis%20of%20VLA%20applications%20across%20different%20scenarios%20and%0Aclassifies%20VLA%20approaches%20into%20several%20paradigms%3A%20autoregression-based%2C%0Adiffusion-based%2C%20reinforcement-based%2C%20hybrid%2C%20and%20specialized%20methods%3B%20while%0Aexamining%20their%20motivations%2C%20core%20strategies%2C%20and%20implementations%20in%20detail.%20In%0Aaddition%2C%20foundational%20datasets%2C%20benchmarks%2C%20and%20simulation%20platforms%20are%0Aintroduced.%20Building%20on%20the%20current%20VLA%20landscape%2C%20the%20review%20further%20proposes%0Aperspectives%20on%20key%20challenges%20and%20future%20directions%20to%20advance%20research%20in%20VLA%0Amodels%20and%20generalizable%20robotics.%20By%20synthesizing%20insights%20from%20over%20three%0Ahundred%20recent%20studies%2C%20this%20survey%20maps%20the%20contours%20of%20this%20rapidly%20evolving%0Afield%20and%20highlights%20the%20opportunities%20and%20challenges%20that%20will%20shape%20the%0Adevelopment%20of%20scalable%2C%20general-purpose%20VLA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPure%2520Vision%2520Language%2520Action%2520%2528VLA%2529%2520Models%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DDapeng%2520Zhang%2520and%2520Jin%2520Sun%2520and%2520Chenghui%2520Hu%2520and%2520Xiaoyan%2520Wu%2520and%2520Zhenlong%2520Yuan%2520and%2520Rui%2520Zhou%2520and%2520Fei%2520Shen%2520and%2520Qingguo%2520Zhou%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Vision%2520Language%2520Action%2520%2528VLA%2529%2520models%2520marks%2520a%2520paradigm%2520shift%250Afrom%2520traditional%2520policy-based%2520control%2520to%2520generalized%2520robotics%252C%2520reframing%2520Vision%250ALanguage%2520Models%2520%2528VLMs%2529%2520from%2520passive%2520sequence%2520generators%2520into%2520active%2520agents%2520for%250Amanipulation%2520and%2520decision-making%2520in%2520complex%252C%2520dynamic%2520environments.%2520This%2520survey%250Adelves%2520into%2520advanced%2520VLA%2520methods%252C%2520aiming%2520to%2520provide%2520a%2520clear%2520taxonomy%2520and%2520a%250Asystematic%252C%2520comprehensive%2520review%2520of%2520existing%2520research.%2520It%2520presents%2520a%250Acomprehensive%2520analysis%2520of%2520VLA%2520applications%2520across%2520different%2520scenarios%2520and%250Aclassifies%2520VLA%2520approaches%2520into%2520several%2520paradigms%253A%2520autoregression-based%252C%250Adiffusion-based%252C%2520reinforcement-based%252C%2520hybrid%252C%2520and%2520specialized%2520methods%253B%2520while%250Aexamining%2520their%2520motivations%252C%2520core%2520strategies%252C%2520and%2520implementations%2520in%2520detail.%2520In%250Aaddition%252C%2520foundational%2520datasets%252C%2520benchmarks%252C%2520and%2520simulation%2520platforms%2520are%250Aintroduced.%2520Building%2520on%2520the%2520current%2520VLA%2520landscape%252C%2520the%2520review%2520further%2520proposes%250Aperspectives%2520on%2520key%2520challenges%2520and%2520future%2520directions%2520to%2520advance%2520research%2520in%2520VLA%250Amodels%2520and%2520generalizable%2520robotics.%2520By%2520synthesizing%2520insights%2520from%2520over%2520three%250Ahundred%2520recent%2520studies%252C%2520this%2520survey%2520maps%2520the%2520contours%2520of%2520this%2520rapidly%2520evolving%250Afield%2520and%2520highlights%2520the%2520opportunities%2520and%2520challenges%2520that%2520will%2520shape%2520the%250Adevelopment%2520of%2520scalable%252C%2520general-purpose%2520VLA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pure%20Vision%20Language%20Action%20%28VLA%29%20Models%3A%20A%20Comprehensive%20Survey&entry.906535625=Dapeng%20Zhang%20and%20Jin%20Sun%20and%20Chenghui%20Hu%20and%20Xiaoyan%20Wu%20and%20Zhenlong%20Yuan%20and%20Rui%20Zhou%20and%20Fei%20Shen%20and%20Qingguo%20Zhou&entry.1292438233=%20%20The%20emergence%20of%20Vision%20Language%20Action%20%28VLA%29%20models%20marks%20a%20paradigm%20shift%0Afrom%20traditional%20policy-based%20control%20to%20generalized%20robotics%2C%20reframing%20Vision%0ALanguage%20Models%20%28VLMs%29%20from%20passive%20sequence%20generators%20into%20active%20agents%20for%0Amanipulation%20and%20decision-making%20in%20complex%2C%20dynamic%20environments.%20This%20survey%0Adelves%20into%20advanced%20VLA%20methods%2C%20aiming%20to%20provide%20a%20clear%20taxonomy%20and%20a%0Asystematic%2C%20comprehensive%20review%20of%20existing%20research.%20It%20presents%20a%0Acomprehensive%20analysis%20of%20VLA%20applications%20across%20different%20scenarios%20and%0Aclassifies%20VLA%20approaches%20into%20several%20paradigms%3A%20autoregression-based%2C%0Adiffusion-based%2C%20reinforcement-based%2C%20hybrid%2C%20and%20specialized%20methods%3B%20while%0Aexamining%20their%20motivations%2C%20core%20strategies%2C%20and%20implementations%20in%20detail.%20In%0Aaddition%2C%20foundational%20datasets%2C%20benchmarks%2C%20and%20simulation%20platforms%20are%0Aintroduced.%20Building%20on%20the%20current%20VLA%20landscape%2C%20the%20review%20further%20proposes%0Aperspectives%20on%20key%20challenges%20and%20future%20directions%20to%20advance%20research%20in%20VLA%0Amodels%20and%20generalizable%20robotics.%20By%20synthesizing%20insights%20from%20over%20three%0Ahundred%20recent%20studies%2C%20this%20survey%20maps%20the%20contours%20of%20this%20rapidly%20evolving%0Afield%20and%20highlights%20the%20opportunities%20and%20challenges%20that%20will%20shape%20the%0Adevelopment%20of%20scalable%2C%20general-purpose%20VLA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19012v1&entry.124074799=Read"},
{"title": "Efficient Reinforcement Learning by Reducing Forgetting with Elephant\n  Activation Functions", "author": "Qingfeng Lan and Gautham Vasan and A. Rupam Mahmood", "abstract": "  Catastrophic forgetting has remained a significant challenge for efficient\nreinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While\nrecent works have proposed effective methods to mitigate this issue, they\nmainly focus on the algorithmic side. Meanwhile, we do not fully understand\nwhat architectural properties of neural networks lead to catastrophic\nforgetting. This study aims to fill this gap by studying the role of activation\nfunctions in the training dynamics of neural networks and their impact on\ncatastrophic forgetting in reinforcement learning setup. Our study reveals\nthat, besides sparse representations, the gradient sparsity of activation\nfunctions also plays an important role in reducing forgetting. Based on this\ninsight, we propose a new class of activation functions, elephant activation\nfunctions, that can generate both sparse outputs and sparse gradients. We show\nthat by simply replacing classical activation functions with elephant\nactivation functions in the neural networks of value-based algorithms, we can\nsignificantly improve the resilience of neural networks to catastrophic\nforgetting, thus making reinforcement learning more sample-efficient and\nmemory-efficient.\n", "link": "http://arxiv.org/abs/2509.19159v1", "date": "2025-09-23", "relevancy": 2.2945, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4829}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4686}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Reinforcement%20Learning%20by%20Reducing%20Forgetting%20with%20Elephant%0A%20%20Activation%20Functions&body=Title%3A%20Efficient%20Reinforcement%20Learning%20by%20Reducing%20Forgetting%20with%20Elephant%0A%20%20Activation%20Functions%0AAuthor%3A%20Qingfeng%20Lan%20and%20Gautham%20Vasan%20and%20A.%20Rupam%20Mahmood%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20has%20remained%20a%20significant%20challenge%20for%20efficient%0Areinforcement%20learning%20for%20decades%20%28Ring%201994%2C%20Rivest%20and%20Precup%202003%29.%20While%0Arecent%20works%20have%20proposed%20effective%20methods%20to%20mitigate%20this%20issue%2C%20they%0Amainly%20focus%20on%20the%20algorithmic%20side.%20Meanwhile%2C%20we%20do%20not%20fully%20understand%0Awhat%20architectural%20properties%20of%20neural%20networks%20lead%20to%20catastrophic%0Aforgetting.%20This%20study%20aims%20to%20fill%20this%20gap%20by%20studying%20the%20role%20of%20activation%0Afunctions%20in%20the%20training%20dynamics%20of%20neural%20networks%20and%20their%20impact%20on%0Acatastrophic%20forgetting%20in%20reinforcement%20learning%20setup.%20Our%20study%20reveals%0Athat%2C%20besides%20sparse%20representations%2C%20the%20gradient%20sparsity%20of%20activation%0Afunctions%20also%20plays%20an%20important%20role%20in%20reducing%20forgetting.%20Based%20on%20this%0Ainsight%2C%20we%20propose%20a%20new%20class%20of%20activation%20functions%2C%20elephant%20activation%0Afunctions%2C%20that%20can%20generate%20both%20sparse%20outputs%20and%20sparse%20gradients.%20We%20show%0Athat%20by%20simply%20replacing%20classical%20activation%20functions%20with%20elephant%0Aactivation%20functions%20in%20the%20neural%20networks%20of%20value-based%20algorithms%2C%20we%20can%0Asignificantly%20improve%20the%20resilience%20of%20neural%20networks%20to%20catastrophic%0Aforgetting%2C%20thus%20making%20reinforcement%20learning%20more%20sample-efficient%20and%0Amemory-efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Reinforcement%2520Learning%2520by%2520Reducing%2520Forgetting%2520with%2520Elephant%250A%2520%2520Activation%2520Functions%26entry.906535625%3DQingfeng%2520Lan%2520and%2520Gautham%2520Vasan%2520and%2520A.%2520Rupam%2520Mahmood%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520has%2520remained%2520a%2520significant%2520challenge%2520for%2520efficient%250Areinforcement%2520learning%2520for%2520decades%2520%2528Ring%25201994%252C%2520Rivest%2520and%2520Precup%25202003%2529.%2520While%250Arecent%2520works%2520have%2520proposed%2520effective%2520methods%2520to%2520mitigate%2520this%2520issue%252C%2520they%250Amainly%2520focus%2520on%2520the%2520algorithmic%2520side.%2520Meanwhile%252C%2520we%2520do%2520not%2520fully%2520understand%250Awhat%2520architectural%2520properties%2520of%2520neural%2520networks%2520lead%2520to%2520catastrophic%250Aforgetting.%2520This%2520study%2520aims%2520to%2520fill%2520this%2520gap%2520by%2520studying%2520the%2520role%2520of%2520activation%250Afunctions%2520in%2520the%2520training%2520dynamics%2520of%2520neural%2520networks%2520and%2520their%2520impact%2520on%250Acatastrophic%2520forgetting%2520in%2520reinforcement%2520learning%2520setup.%2520Our%2520study%2520reveals%250Athat%252C%2520besides%2520sparse%2520representations%252C%2520the%2520gradient%2520sparsity%2520of%2520activation%250Afunctions%2520also%2520plays%2520an%2520important%2520role%2520in%2520reducing%2520forgetting.%2520Based%2520on%2520this%250Ainsight%252C%2520we%2520propose%2520a%2520new%2520class%2520of%2520activation%2520functions%252C%2520elephant%2520activation%250Afunctions%252C%2520that%2520can%2520generate%2520both%2520sparse%2520outputs%2520and%2520sparse%2520gradients.%2520We%2520show%250Athat%2520by%2520simply%2520replacing%2520classical%2520activation%2520functions%2520with%2520elephant%250Aactivation%2520functions%2520in%2520the%2520neural%2520networks%2520of%2520value-based%2520algorithms%252C%2520we%2520can%250Asignificantly%2520improve%2520the%2520resilience%2520of%2520neural%2520networks%2520to%2520catastrophic%250Aforgetting%252C%2520thus%2520making%2520reinforcement%2520learning%2520more%2520sample-efficient%2520and%250Amemory-efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Reinforcement%20Learning%20by%20Reducing%20Forgetting%20with%20Elephant%0A%20%20Activation%20Functions&entry.906535625=Qingfeng%20Lan%20and%20Gautham%20Vasan%20and%20A.%20Rupam%20Mahmood&entry.1292438233=%20%20Catastrophic%20forgetting%20has%20remained%20a%20significant%20challenge%20for%20efficient%0Areinforcement%20learning%20for%20decades%20%28Ring%201994%2C%20Rivest%20and%20Precup%202003%29.%20While%0Arecent%20works%20have%20proposed%20effective%20methods%20to%20mitigate%20this%20issue%2C%20they%0Amainly%20focus%20on%20the%20algorithmic%20side.%20Meanwhile%2C%20we%20do%20not%20fully%20understand%0Awhat%20architectural%20properties%20of%20neural%20networks%20lead%20to%20catastrophic%0Aforgetting.%20This%20study%20aims%20to%20fill%20this%20gap%20by%20studying%20the%20role%20of%20activation%0Afunctions%20in%20the%20training%20dynamics%20of%20neural%20networks%20and%20their%20impact%20on%0Acatastrophic%20forgetting%20in%20reinforcement%20learning%20setup.%20Our%20study%20reveals%0Athat%2C%20besides%20sparse%20representations%2C%20the%20gradient%20sparsity%20of%20activation%0Afunctions%20also%20plays%20an%20important%20role%20in%20reducing%20forgetting.%20Based%20on%20this%0Ainsight%2C%20we%20propose%20a%20new%20class%20of%20activation%20functions%2C%20elephant%20activation%0Afunctions%2C%20that%20can%20generate%20both%20sparse%20outputs%20and%20sparse%20gradients.%20We%20show%0Athat%20by%20simply%20replacing%20classical%20activation%20functions%20with%20elephant%0Aactivation%20functions%20in%20the%20neural%20networks%20of%20value-based%20algorithms%2C%20we%20can%0Asignificantly%20improve%20the%20resilience%20of%20neural%20networks%20to%20catastrophic%0Aforgetting%2C%20thus%20making%20reinforcement%20learning%20more%20sample-efficient%20and%0Amemory-efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19159v1&entry.124074799=Read"},
{"title": "Topological Feature Compression for Molecular Graph Neural Networks", "author": "Rahul Khorana", "abstract": "  Recent advances in molecular representation learning have produced highly\neffective encodings of molecules for numerous cheminformatics and\nbioinformatics tasks. However, extracting general chemical insight while\nbalancing predictive accuracy, interpretability, and computational efficiency\nremains a major challenge. In this work, we introduce a novel Graph Neural\nNetwork (GNN) architecture that combines compressed higher-order topological\nsignals with standard molecular features. Our approach captures global\ngeometric information while preserving computational tractability and\nhuman-interpretable structure. We evaluate our model across a range of\nbenchmarks, from small-molecule datasets to complex material datasets, and\ndemonstrate superior performance using a parameter-efficient architecture. We\nachieve the best performing results in both accuracy and robustness across\nalmost all benchmarks. We open source all code \\footnote{All code and results\ncan be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.\n", "link": "http://arxiv.org/abs/2508.07807v2", "date": "2025-09-23", "relevancy": 2.2845, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4609}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4573}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20Feature%20Compression%20for%20Molecular%20Graph%20Neural%20Networks&body=Title%3A%20Topological%20Feature%20Compression%20for%20Molecular%20Graph%20Neural%20Networks%0AAuthor%3A%20Rahul%20Khorana%0AAbstract%3A%20%20%20Recent%20advances%20in%20molecular%20representation%20learning%20have%20produced%20highly%0Aeffective%20encodings%20of%20molecules%20for%20numerous%20cheminformatics%20and%0Abioinformatics%20tasks.%20However%2C%20extracting%20general%20chemical%20insight%20while%0Abalancing%20predictive%20accuracy%2C%20interpretability%2C%20and%20computational%20efficiency%0Aremains%20a%20major%20challenge.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20Graph%20Neural%0ANetwork%20%28GNN%29%20architecture%20that%20combines%20compressed%20higher-order%20topological%0Asignals%20with%20standard%20molecular%20features.%20Our%20approach%20captures%20global%0Ageometric%20information%20while%20preserving%20computational%20tractability%20and%0Ahuman-interpretable%20structure.%20We%20evaluate%20our%20model%20across%20a%20range%20of%0Abenchmarks%2C%20from%20small-molecule%20datasets%20to%20complex%20material%20datasets%2C%20and%0Ademonstrate%20superior%20performance%20using%20a%20parameter-efficient%20architecture.%20We%0Aachieve%20the%20best%20performing%20results%20in%20both%20accuracy%20and%20robustness%20across%0Aalmost%20all%20benchmarks.%20We%20open%20source%20all%20code%20%5Cfootnote%7BAll%20code%20and%20results%0Acan%20be%20found%20on%20Github%20https%3A//github.com/rahulkhorana/TFC-PACT-Net%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520Feature%2520Compression%2520for%2520Molecular%2520Graph%2520Neural%2520Networks%26entry.906535625%3DRahul%2520Khorana%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520molecular%2520representation%2520learning%2520have%2520produced%2520highly%250Aeffective%2520encodings%2520of%2520molecules%2520for%2520numerous%2520cheminformatics%2520and%250Abioinformatics%2520tasks.%2520However%252C%2520extracting%2520general%2520chemical%2520insight%2520while%250Abalancing%2520predictive%2520accuracy%252C%2520interpretability%252C%2520and%2520computational%2520efficiency%250Aremains%2520a%2520major%2520challenge.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520Graph%2520Neural%250ANetwork%2520%2528GNN%2529%2520architecture%2520that%2520combines%2520compressed%2520higher-order%2520topological%250Asignals%2520with%2520standard%2520molecular%2520features.%2520Our%2520approach%2520captures%2520global%250Ageometric%2520information%2520while%2520preserving%2520computational%2520tractability%2520and%250Ahuman-interpretable%2520structure.%2520We%2520evaluate%2520our%2520model%2520across%2520a%2520range%2520of%250Abenchmarks%252C%2520from%2520small-molecule%2520datasets%2520to%2520complex%2520material%2520datasets%252C%2520and%250Ademonstrate%2520superior%2520performance%2520using%2520a%2520parameter-efficient%2520architecture.%2520We%250Aachieve%2520the%2520best%2520performing%2520results%2520in%2520both%2520accuracy%2520and%2520robustness%2520across%250Aalmost%2520all%2520benchmarks.%2520We%2520open%2520source%2520all%2520code%2520%255Cfootnote%257BAll%2520code%2520and%2520results%250Acan%2520be%2520found%2520on%2520Github%2520https%253A//github.com/rahulkhorana/TFC-PACT-Net%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20Feature%20Compression%20for%20Molecular%20Graph%20Neural%20Networks&entry.906535625=Rahul%20Khorana&entry.1292438233=%20%20Recent%20advances%20in%20molecular%20representation%20learning%20have%20produced%20highly%0Aeffective%20encodings%20of%20molecules%20for%20numerous%20cheminformatics%20and%0Abioinformatics%20tasks.%20However%2C%20extracting%20general%20chemical%20insight%20while%0Abalancing%20predictive%20accuracy%2C%20interpretability%2C%20and%20computational%20efficiency%0Aremains%20a%20major%20challenge.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20Graph%20Neural%0ANetwork%20%28GNN%29%20architecture%20that%20combines%20compressed%20higher-order%20topological%0Asignals%20with%20standard%20molecular%20features.%20Our%20approach%20captures%20global%0Ageometric%20information%20while%20preserving%20computational%20tractability%20and%0Ahuman-interpretable%20structure.%20We%20evaluate%20our%20model%20across%20a%20range%20of%0Abenchmarks%2C%20from%20small-molecule%20datasets%20to%20complex%20material%20datasets%2C%20and%0Ademonstrate%20superior%20performance%20using%20a%20parameter-efficient%20architecture.%20We%0Aachieve%20the%20best%20performing%20results%20in%20both%20accuracy%20and%20robustness%20across%0Aalmost%20all%20benchmarks.%20We%20open%20source%20all%20code%20%5Cfootnote%7BAll%20code%20and%20results%0Acan%20be%20found%20on%20Github%20https%3A//github.com/rahulkhorana/TFC-PACT-Net%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07807v2&entry.124074799=Read"},
{"title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images", "author": "Boyang Deng and Songyou Peng and Kyle Genova and Gordon Wetzstein and Noah Snavely and Leonidas Guibas and Thomas Funkhouser", "abstract": "  We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.\n", "link": "http://arxiv.org/abs/2504.08727v3", "date": "2025-09-23", "relevancy": 2.2814, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5865}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Chronicles%3A%20Using%20Multimodal%20LLMs%20to%20Analyze%20Massive%20Collections%0A%20%20of%20Images&body=Title%3A%20Visual%20Chronicles%3A%20Using%20Multimodal%20LLMs%20to%20Analyze%20Massive%20Collections%0A%20%20of%20Images%0AAuthor%3A%20Boyang%20Deng%20and%20Songyou%20Peng%20and%20Kyle%20Genova%20and%20Gordon%20Wetzstein%20and%20Noah%20Snavely%20and%20Leonidas%20Guibas%20and%20Thomas%20Funkhouser%0AAbstract%3A%20%20%20We%20present%20a%20system%20using%20Multimodal%20LLMs%20%28MLLMs%29%20to%20analyze%20a%20large%20database%0Awith%20tens%20of%20millions%20of%20images%20captured%20at%20different%20times%2C%20with%20the%20aim%20of%0Adiscovering%20patterns%20in%20temporal%20changes.%20Specifically%2C%20we%20aim%20to%20capture%0Afrequent%20co-occurring%20changes%20%28%22trends%22%29%20across%20a%20city%20over%20a%20certain%20period.%0AUnlike%20previous%20visual%20analyses%2C%20our%20analysis%20answers%20open-ended%20queries%20%28e.g.%2C%0A%22what%20are%20the%20frequent%20types%20of%20changes%20in%20the%20city%3F%22%29%20without%20any%0Apredetermined%20target%20subjects%20or%20training%20labels.%20These%20properties%20cast%20prior%0Alearning-based%20or%20unsupervised%20visual%20analysis%20tools%20unsuitable.%20We%20identify%0AMLLMs%20as%20a%20novel%20tool%20for%20their%20open-ended%20semantic%20understanding%20capabilities.%0AYet%2C%20our%20datasets%20are%20four%20orders%20of%20magnitude%20too%20large%20for%20an%20MLLM%20to%20ingest%0Aas%20context.%20So%20we%20introduce%20a%20bottom-up%20procedure%20that%20decomposes%20the%20massive%0Avisual%20analysis%20problem%20into%20more%20tractable%20sub-problems.%20We%20carefully%20design%0AMLLM-based%20solutions%20to%20each%20sub-problem.%20During%20experiments%20and%20ablation%0Astudies%20with%20our%20system%2C%20we%20find%20it%20significantly%20outperforms%20baselines%20and%20is%0Aable%20to%20discover%20interesting%20trends%20from%20images%20captured%20in%20large%20cities%20%28e.g.%2C%0A%22addition%20of%20outdoor%20dining%2C%22%2C%20%22overpass%20was%20painted%20blue%2C%22%20etc.%29.%20See%20more%0Aresults%20and%20interactive%20demos%20at%20https%3A//boyangdeng.com/visual-chronicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08727v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Chronicles%253A%2520Using%2520Multimodal%2520LLMs%2520to%2520Analyze%2520Massive%2520Collections%250A%2520%2520of%2520Images%26entry.906535625%3DBoyang%2520Deng%2520and%2520Songyou%2520Peng%2520and%2520Kyle%2520Genova%2520and%2520Gordon%2520Wetzstein%2520and%2520Noah%2520Snavely%2520and%2520Leonidas%2520Guibas%2520and%2520Thomas%2520Funkhouser%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520system%2520using%2520Multimodal%2520LLMs%2520%2528MLLMs%2529%2520to%2520analyze%2520a%2520large%2520database%250Awith%2520tens%2520of%2520millions%2520of%2520images%2520captured%2520at%2520different%2520times%252C%2520with%2520the%2520aim%2520of%250Adiscovering%2520patterns%2520in%2520temporal%2520changes.%2520Specifically%252C%2520we%2520aim%2520to%2520capture%250Afrequent%2520co-occurring%2520changes%2520%2528%2522trends%2522%2529%2520across%2520a%2520city%2520over%2520a%2520certain%2520period.%250AUnlike%2520previous%2520visual%2520analyses%252C%2520our%2520analysis%2520answers%2520open-ended%2520queries%2520%2528e.g.%252C%250A%2522what%2520are%2520the%2520frequent%2520types%2520of%2520changes%2520in%2520the%2520city%253F%2522%2529%2520without%2520any%250Apredetermined%2520target%2520subjects%2520or%2520training%2520labels.%2520These%2520properties%2520cast%2520prior%250Alearning-based%2520or%2520unsupervised%2520visual%2520analysis%2520tools%2520unsuitable.%2520We%2520identify%250AMLLMs%2520as%2520a%2520novel%2520tool%2520for%2520their%2520open-ended%2520semantic%2520understanding%2520capabilities.%250AYet%252C%2520our%2520datasets%2520are%2520four%2520orders%2520of%2520magnitude%2520too%2520large%2520for%2520an%2520MLLM%2520to%2520ingest%250Aas%2520context.%2520So%2520we%2520introduce%2520a%2520bottom-up%2520procedure%2520that%2520decomposes%2520the%2520massive%250Avisual%2520analysis%2520problem%2520into%2520more%2520tractable%2520sub-problems.%2520We%2520carefully%2520design%250AMLLM-based%2520solutions%2520to%2520each%2520sub-problem.%2520During%2520experiments%2520and%2520ablation%250Astudies%2520with%2520our%2520system%252C%2520we%2520find%2520it%2520significantly%2520outperforms%2520baselines%2520and%2520is%250Aable%2520to%2520discover%2520interesting%2520trends%2520from%2520images%2520captured%2520in%2520large%2520cities%2520%2528e.g.%252C%250A%2522addition%2520of%2520outdoor%2520dining%252C%2522%252C%2520%2522overpass%2520was%2520painted%2520blue%252C%2522%2520etc.%2529.%2520See%2520more%250Aresults%2520and%2520interactive%2520demos%2520at%2520https%253A//boyangdeng.com/visual-chronicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08727v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Chronicles%3A%20Using%20Multimodal%20LLMs%20to%20Analyze%20Massive%20Collections%0A%20%20of%20Images&entry.906535625=Boyang%20Deng%20and%20Songyou%20Peng%20and%20Kyle%20Genova%20and%20Gordon%20Wetzstein%20and%20Noah%20Snavely%20and%20Leonidas%20Guibas%20and%20Thomas%20Funkhouser&entry.1292438233=%20%20We%20present%20a%20system%20using%20Multimodal%20LLMs%20%28MLLMs%29%20to%20analyze%20a%20large%20database%0Awith%20tens%20of%20millions%20of%20images%20captured%20at%20different%20times%2C%20with%20the%20aim%20of%0Adiscovering%20patterns%20in%20temporal%20changes.%20Specifically%2C%20we%20aim%20to%20capture%0Afrequent%20co-occurring%20changes%20%28%22trends%22%29%20across%20a%20city%20over%20a%20certain%20period.%0AUnlike%20previous%20visual%20analyses%2C%20our%20analysis%20answers%20open-ended%20queries%20%28e.g.%2C%0A%22what%20are%20the%20frequent%20types%20of%20changes%20in%20the%20city%3F%22%29%20without%20any%0Apredetermined%20target%20subjects%20or%20training%20labels.%20These%20properties%20cast%20prior%0Alearning-based%20or%20unsupervised%20visual%20analysis%20tools%20unsuitable.%20We%20identify%0AMLLMs%20as%20a%20novel%20tool%20for%20their%20open-ended%20semantic%20understanding%20capabilities.%0AYet%2C%20our%20datasets%20are%20four%20orders%20of%20magnitude%20too%20large%20for%20an%20MLLM%20to%20ingest%0Aas%20context.%20So%20we%20introduce%20a%20bottom-up%20procedure%20that%20decomposes%20the%20massive%0Avisual%20analysis%20problem%20into%20more%20tractable%20sub-problems.%20We%20carefully%20design%0AMLLM-based%20solutions%20to%20each%20sub-problem.%20During%20experiments%20and%20ablation%0Astudies%20with%20our%20system%2C%20we%20find%20it%20significantly%20outperforms%20baselines%20and%20is%0Aable%20to%20discover%20interesting%20trends%20from%20images%20captured%20in%20large%20cities%20%28e.g.%2C%0A%22addition%20of%20outdoor%20dining%2C%22%2C%20%22overpass%20was%20painted%20blue%2C%22%20etc.%29.%20See%20more%0Aresults%20and%20interactive%20demos%20at%20https%3A//boyangdeng.com/visual-chronicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08727v3&entry.124074799=Read"},
{"title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via\n  Travel Video Itinerary Reconstruction", "author": "Hao Wang and Eiki Murata and Lingfang Zhang and Ayako Sato and So Fukuda and Ziqi Yin and Wentao Hu and Keisuke Nakao and Yusuke Nakamura and Sebastian Zwirner and Yi-Chia Chen and Hiroyuki Otomo and Hiroki Ouchi and Daisuke Kawahara", "abstract": "  Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.\n", "link": "http://arxiv.org/abs/2509.19002v1", "date": "2025-09-23", "relevancy": 2.2729, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIR-Bench%3A%20Evaluating%20Geospatial%20and%20Temporal%20Understanding%20of%20MLLMs%20via%0A%20%20Travel%20Video%20Itinerary%20Reconstruction&body=Title%3A%20VIR-Bench%3A%20Evaluating%20Geospatial%20and%20Temporal%20Understanding%20of%20MLLMs%20via%0A%20%20Travel%20Video%20Itinerary%20Reconstruction%0AAuthor%3A%20Hao%20Wang%20and%20Eiki%20Murata%20and%20Lingfang%20Zhang%20and%20Ayako%20Sato%20and%20So%20Fukuda%20and%20Ziqi%20Yin%20and%20Wentao%20Hu%20and%20Keisuke%20Nakao%20and%20Yusuke%20Nakamura%20and%20Sebastian%20Zwirner%20and%20Yi-Chia%20Chen%20and%20Hiroyuki%20Otomo%20and%20Hiroki%20Ouchi%20and%20Daisuke%20Kawahara%0AAbstract%3A%20%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Asignificantly%20enhanced%20video%20understanding%20capabilities%2C%20opening%20new%0Apossibilities%20for%20practical%20applications.%20Yet%20current%20video%20benchmarks%20focus%0Alargely%20on%20indoor%20scenes%20or%20short-range%20outdoor%20activities%2C%20leaving%20the%0Achallenges%20associated%20with%20long-distance%20travel%20largely%20unexplored.%20Mastering%0Aextended%20geospatial-temporal%20trajectories%20is%20critical%20for%20next-generation%0AMLLMs%2C%20underpinning%20real-world%20tasks%20such%20as%20embodied-AI%20planning%20and%0Anavigation.%20To%20bridge%20this%20gap%2C%20we%20present%20VIR-Bench%2C%20a%20novel%20benchmark%0Aconsisting%20of%20200%20travel%20videos%20that%20frames%20itinerary%20reconstruction%20as%20a%0Achallenging%20task%20designed%20to%20evaluate%20and%20push%20forward%20MLLMs%27%0Ageospatial-temporal%20intelligence.%20Experimental%20results%20reveal%20that%0Astate-of-the-art%20MLLMs%2C%20including%20proprietary%20ones%2C%20struggle%20to%20achieve%20high%0Ascores%2C%20underscoring%20the%20difficulty%20of%20handling%20videos%20that%20span%20extended%0Aspatial%20and%20temporal%20scales.%20Moreover%2C%20we%20conduct%20an%20in-depth%20case%20study%20in%0Awhich%20we%20develop%20a%20prototype%20travel-planning%20agent%20that%20leverages%20the%20insights%0Agained%20from%20VIR-Bench.%20The%20agent%27s%20markedly%20improved%20itinerary%20recommendations%0Averify%20that%20our%20evaluation%20protocol%20not%20only%20benchmarks%20models%20effectively%20but%0Aalso%20translates%20into%20concrete%20performance%20gains%20in%20user-facing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIR-Bench%253A%2520Evaluating%2520Geospatial%2520and%2520Temporal%2520Understanding%2520of%2520MLLMs%2520via%250A%2520%2520Travel%2520Video%2520Itinerary%2520Reconstruction%26entry.906535625%3DHao%2520Wang%2520and%2520Eiki%2520Murata%2520and%2520Lingfang%2520Zhang%2520and%2520Ayako%2520Sato%2520and%2520So%2520Fukuda%2520and%2520Ziqi%2520Yin%2520and%2520Wentao%2520Hu%2520and%2520Keisuke%2520Nakao%2520and%2520Yusuke%2520Nakamura%2520and%2520Sebastian%2520Zwirner%2520and%2520Yi-Chia%2520Chen%2520and%2520Hiroyuki%2520Otomo%2520and%2520Hiroki%2520Ouchi%2520and%2520Daisuke%2520Kawahara%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%250Asignificantly%2520enhanced%2520video%2520understanding%2520capabilities%252C%2520opening%2520new%250Apossibilities%2520for%2520practical%2520applications.%2520Yet%2520current%2520video%2520benchmarks%2520focus%250Alargely%2520on%2520indoor%2520scenes%2520or%2520short-range%2520outdoor%2520activities%252C%2520leaving%2520the%250Achallenges%2520associated%2520with%2520long-distance%2520travel%2520largely%2520unexplored.%2520Mastering%250Aextended%2520geospatial-temporal%2520trajectories%2520is%2520critical%2520for%2520next-generation%250AMLLMs%252C%2520underpinning%2520real-world%2520tasks%2520such%2520as%2520embodied-AI%2520planning%2520and%250Anavigation.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520VIR-Bench%252C%2520a%2520novel%2520benchmark%250Aconsisting%2520of%2520200%2520travel%2520videos%2520that%2520frames%2520itinerary%2520reconstruction%2520as%2520a%250Achallenging%2520task%2520designed%2520to%2520evaluate%2520and%2520push%2520forward%2520MLLMs%2527%250Ageospatial-temporal%2520intelligence.%2520Experimental%2520results%2520reveal%2520that%250Astate-of-the-art%2520MLLMs%252C%2520including%2520proprietary%2520ones%252C%2520struggle%2520to%2520achieve%2520high%250Ascores%252C%2520underscoring%2520the%2520difficulty%2520of%2520handling%2520videos%2520that%2520span%2520extended%250Aspatial%2520and%2520temporal%2520scales.%2520Moreover%252C%2520we%2520conduct%2520an%2520in-depth%2520case%2520study%2520in%250Awhich%2520we%2520develop%2520a%2520prototype%2520travel-planning%2520agent%2520that%2520leverages%2520the%2520insights%250Agained%2520from%2520VIR-Bench.%2520The%2520agent%2527s%2520markedly%2520improved%2520itinerary%2520recommendations%250Averify%2520that%2520our%2520evaluation%2520protocol%2520not%2520only%2520benchmarks%2520models%2520effectively%2520but%250Aalso%2520translates%2520into%2520concrete%2520performance%2520gains%2520in%2520user-facing%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIR-Bench%3A%20Evaluating%20Geospatial%20and%20Temporal%20Understanding%20of%20MLLMs%20via%0A%20%20Travel%20Video%20Itinerary%20Reconstruction&entry.906535625=Hao%20Wang%20and%20Eiki%20Murata%20and%20Lingfang%20Zhang%20and%20Ayako%20Sato%20and%20So%20Fukuda%20and%20Ziqi%20Yin%20and%20Wentao%20Hu%20and%20Keisuke%20Nakao%20and%20Yusuke%20Nakamura%20and%20Sebastian%20Zwirner%20and%20Yi-Chia%20Chen%20and%20Hiroyuki%20Otomo%20and%20Hiroki%20Ouchi%20and%20Daisuke%20Kawahara&entry.1292438233=%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Asignificantly%20enhanced%20video%20understanding%20capabilities%2C%20opening%20new%0Apossibilities%20for%20practical%20applications.%20Yet%20current%20video%20benchmarks%20focus%0Alargely%20on%20indoor%20scenes%20or%20short-range%20outdoor%20activities%2C%20leaving%20the%0Achallenges%20associated%20with%20long-distance%20travel%20largely%20unexplored.%20Mastering%0Aextended%20geospatial-temporal%20trajectories%20is%20critical%20for%20next-generation%0AMLLMs%2C%20underpinning%20real-world%20tasks%20such%20as%20embodied-AI%20planning%20and%0Anavigation.%20To%20bridge%20this%20gap%2C%20we%20present%20VIR-Bench%2C%20a%20novel%20benchmark%0Aconsisting%20of%20200%20travel%20videos%20that%20frames%20itinerary%20reconstruction%20as%20a%0Achallenging%20task%20designed%20to%20evaluate%20and%20push%20forward%20MLLMs%27%0Ageospatial-temporal%20intelligence.%20Experimental%20results%20reveal%20that%0Astate-of-the-art%20MLLMs%2C%20including%20proprietary%20ones%2C%20struggle%20to%20achieve%20high%0Ascores%2C%20underscoring%20the%20difficulty%20of%20handling%20videos%20that%20span%20extended%0Aspatial%20and%20temporal%20scales.%20Moreover%2C%20we%20conduct%20an%20in-depth%20case%20study%20in%0Awhich%20we%20develop%20a%20prototype%20travel-planning%20agent%20that%20leverages%20the%20insights%0Agained%20from%20VIR-Bench.%20The%20agent%27s%20markedly%20improved%20itinerary%20recommendations%0Averify%20that%20our%20evaluation%20protocol%20not%20only%20benchmarks%20models%20effectively%20but%0Aalso%20translates%20into%20concrete%20performance%20gains%20in%20user-facing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19002v1&entry.124074799=Read"},
{"title": "A DyL-Unet framework based on dynamic learning for Temporally Consistent\n  Echocardiographic Segmentation", "author": "Jierui Qu and Jianchun Zhao", "abstract": "  Accurate segmentation of cardiac anatomy in echocardiography is essential for\ncardiovascular diagnosis and treatment. Yet echocardiography is prone to\ndeformation and speckle noise, causing frame-to-frame segmentation jitter. Even\nwith high accuracy in single-frame segmentation, temporal instability can\nweaken functional estimates and impair clinical interpretability. To address\nthese issues, we propose DyL-UNet, a dynamic learning-based temporal\nconsistency U-Net segmentation architecture designed to achieve temporally\nstable and precise echocardiographic segmentation. The framework constructs an\nEcho-Dynamics Graph (EDG) through dynamic learning to extract dynamic\ninformation from videos. DyL-UNet incorporates multiple Swin-Transformer-based\nencoder-decoder branches for processing single-frame images. It further\nintroduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,\nwhich uses EDG-encoded dynamic features and cardiac-phase cues to enforce\ntemporal consistency during segmentation. Extensive experiments on the CAMUS\nand EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation\naccuracy comparable to existing methods while achieving superior temporal\nconsistency, providing a reliable solution for automated clinical\nechocardiography.\n", "link": "http://arxiv.org/abs/2509.19052v1", "date": "2025-09-23", "relevancy": 2.2699, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5971}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5688}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20DyL-Unet%20framework%20based%20on%20dynamic%20learning%20for%20Temporally%20Consistent%0A%20%20Echocardiographic%20Segmentation&body=Title%3A%20A%20DyL-Unet%20framework%20based%20on%20dynamic%20learning%20for%20Temporally%20Consistent%0A%20%20Echocardiographic%20Segmentation%0AAuthor%3A%20Jierui%20Qu%20and%20Jianchun%20Zhao%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20cardiac%20anatomy%20in%20echocardiography%20is%20essential%20for%0Acardiovascular%20diagnosis%20and%20treatment.%20Yet%20echocardiography%20is%20prone%20to%0Adeformation%20and%20speckle%20noise%2C%20causing%20frame-to-frame%20segmentation%20jitter.%20Even%0Awith%20high%20accuracy%20in%20single-frame%20segmentation%2C%20temporal%20instability%20can%0Aweaken%20functional%20estimates%20and%20impair%20clinical%20interpretability.%20To%20address%0Athese%20issues%2C%20we%20propose%20DyL-UNet%2C%20a%20dynamic%20learning-based%20temporal%0Aconsistency%20U-Net%20segmentation%20architecture%20designed%20to%20achieve%20temporally%0Astable%20and%20precise%20echocardiographic%20segmentation.%20The%20framework%20constructs%20an%0AEcho-Dynamics%20Graph%20%28EDG%29%20through%20dynamic%20learning%20to%20extract%20dynamic%0Ainformation%20from%20videos.%20DyL-UNet%20incorporates%20multiple%20Swin-Transformer-based%0Aencoder-decoder%20branches%20for%20processing%20single-frame%20images.%20It%20further%0Aintroduces%20Cardiac%20Phase-Dynamics%20Attention%20%28CPDA%29%20at%20the%20skip%20connections%2C%0Awhich%20uses%20EDG-encoded%20dynamic%20features%20and%20cardiac-phase%20cues%20to%20enforce%0Atemporal%20consistency%20during%20segmentation.%20Extensive%20experiments%20on%20the%20CAMUS%0Aand%20EchoNet-Dynamic%20datasets%20demonstrate%20that%20DyL-UNet%20maintains%20segmentation%0Aaccuracy%20comparable%20to%20existing%20methods%20while%20achieving%20superior%20temporal%0Aconsistency%2C%20providing%20a%20reliable%20solution%20for%20automated%20clinical%0Aechocardiography.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520DyL-Unet%2520framework%2520based%2520on%2520dynamic%2520learning%2520for%2520Temporally%2520Consistent%250A%2520%2520Echocardiographic%2520Segmentation%26entry.906535625%3DJierui%2520Qu%2520and%2520Jianchun%2520Zhao%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520cardiac%2520anatomy%2520in%2520echocardiography%2520is%2520essential%2520for%250Acardiovascular%2520diagnosis%2520and%2520treatment.%2520Yet%2520echocardiography%2520is%2520prone%2520to%250Adeformation%2520and%2520speckle%2520noise%252C%2520causing%2520frame-to-frame%2520segmentation%2520jitter.%2520Even%250Awith%2520high%2520accuracy%2520in%2520single-frame%2520segmentation%252C%2520temporal%2520instability%2520can%250Aweaken%2520functional%2520estimates%2520and%2520impair%2520clinical%2520interpretability.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520DyL-UNet%252C%2520a%2520dynamic%2520learning-based%2520temporal%250Aconsistency%2520U-Net%2520segmentation%2520architecture%2520designed%2520to%2520achieve%2520temporally%250Astable%2520and%2520precise%2520echocardiographic%2520segmentation.%2520The%2520framework%2520constructs%2520an%250AEcho-Dynamics%2520Graph%2520%2528EDG%2529%2520through%2520dynamic%2520learning%2520to%2520extract%2520dynamic%250Ainformation%2520from%2520videos.%2520DyL-UNet%2520incorporates%2520multiple%2520Swin-Transformer-based%250Aencoder-decoder%2520branches%2520for%2520processing%2520single-frame%2520images.%2520It%2520further%250Aintroduces%2520Cardiac%2520Phase-Dynamics%2520Attention%2520%2528CPDA%2529%2520at%2520the%2520skip%2520connections%252C%250Awhich%2520uses%2520EDG-encoded%2520dynamic%2520features%2520and%2520cardiac-phase%2520cues%2520to%2520enforce%250Atemporal%2520consistency%2520during%2520segmentation.%2520Extensive%2520experiments%2520on%2520the%2520CAMUS%250Aand%2520EchoNet-Dynamic%2520datasets%2520demonstrate%2520that%2520DyL-UNet%2520maintains%2520segmentation%250Aaccuracy%2520comparable%2520to%2520existing%2520methods%2520while%2520achieving%2520superior%2520temporal%250Aconsistency%252C%2520providing%2520a%2520reliable%2520solution%2520for%2520automated%2520clinical%250Aechocardiography.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20DyL-Unet%20framework%20based%20on%20dynamic%20learning%20for%20Temporally%20Consistent%0A%20%20Echocardiographic%20Segmentation&entry.906535625=Jierui%20Qu%20and%20Jianchun%20Zhao&entry.1292438233=%20%20Accurate%20segmentation%20of%20cardiac%20anatomy%20in%20echocardiography%20is%20essential%20for%0Acardiovascular%20diagnosis%20and%20treatment.%20Yet%20echocardiography%20is%20prone%20to%0Adeformation%20and%20speckle%20noise%2C%20causing%20frame-to-frame%20segmentation%20jitter.%20Even%0Awith%20high%20accuracy%20in%20single-frame%20segmentation%2C%20temporal%20instability%20can%0Aweaken%20functional%20estimates%20and%20impair%20clinical%20interpretability.%20To%20address%0Athese%20issues%2C%20we%20propose%20DyL-UNet%2C%20a%20dynamic%20learning-based%20temporal%0Aconsistency%20U-Net%20segmentation%20architecture%20designed%20to%20achieve%20temporally%0Astable%20and%20precise%20echocardiographic%20segmentation.%20The%20framework%20constructs%20an%0AEcho-Dynamics%20Graph%20%28EDG%29%20through%20dynamic%20learning%20to%20extract%20dynamic%0Ainformation%20from%20videos.%20DyL-UNet%20incorporates%20multiple%20Swin-Transformer-based%0Aencoder-decoder%20branches%20for%20processing%20single-frame%20images.%20It%20further%0Aintroduces%20Cardiac%20Phase-Dynamics%20Attention%20%28CPDA%29%20at%20the%20skip%20connections%2C%0Awhich%20uses%20EDG-encoded%20dynamic%20features%20and%20cardiac-phase%20cues%20to%20enforce%0Atemporal%20consistency%20during%20segmentation.%20Extensive%20experiments%20on%20the%20CAMUS%0Aand%20EchoNet-Dynamic%20datasets%20demonstrate%20that%20DyL-UNet%20maintains%20segmentation%0Aaccuracy%20comparable%20to%20existing%20methods%20while%20achieving%20superior%20temporal%0Aconsistency%2C%20providing%20a%20reliable%20solution%20for%20automated%20clinical%0Aechocardiography.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19052v1&entry.124074799=Read"},
{"title": "Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving\n  Linear and Angular Momentum for Dynamical Systems", "author": "Vinay Sharma and Olga Fink", "abstract": "  Accurate, interpretable, and real-time modeling of multi-body dynamical\nsystems is essential for predicting behaviors and inferring physical properties\nin natural and engineered environments. Traditional physics-based models face\nscalability challenges and are computationally demanding, while data-driven\napproaches like Graph Neural Networks (GNNs) often lack physical consistency,\ninterpretability, and generalization. In this paper, we propose Dynami-CAL\nGraphNet, a Physics-Informed Graph Neural Network that integrates the learning\ncapabilities of GNNs with physics-based inductive biases to address these\nlimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and\nangular momentum for interacting nodes using edge-local reference frames that\nare equivariant to rotational symmetries, invariant to translations, and\nequivariant to node permutations. This design ensures physically consistent\npredictions of node dynamics while offering interpretable, edge-wise linear and\nangular impulses resulting from pairwise interactions. Evaluated on a 3D\ngranular system with inelastic collisions, Dynami-CAL GraphNet demonstrates\nstable error accumulation over extended rollouts, effective extrapolations to\nunseen configurations, and robust handling of heterogeneous interactions and\nexternal forces. Dynami-CAL GraphNet offers significant advantages in fields\nrequiring accurate, interpretable, and real-time modeling of complex multi-body\ndynamical systems, such as robotics, aerospace engineering, and materials\nscience. By providing physically consistent and scalable predictions that\nadhere to fundamental conservation laws, it enables the inference of forces and\nmoments while efficiently handling heterogeneous interactions and external\nforces.\n", "link": "http://arxiv.org/abs/2501.07373v2", "date": "2025-09-23", "relevancy": 2.2564, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5753}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5609}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynami-CAL%20GraphNet%3A%20A%20Physics-Informed%20Graph%20Neural%20Network%20Conserving%0A%20%20Linear%20and%20Angular%20Momentum%20for%20Dynamical%20Systems&body=Title%3A%20Dynami-CAL%20GraphNet%3A%20A%20Physics-Informed%20Graph%20Neural%20Network%20Conserving%0A%20%20Linear%20and%20Angular%20Momentum%20for%20Dynamical%20Systems%0AAuthor%3A%20Vinay%20Sharma%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Accurate%2C%20interpretable%2C%20and%20real-time%20modeling%20of%20multi-body%20dynamical%0Asystems%20is%20essential%20for%20predicting%20behaviors%20and%20inferring%20physical%20properties%0Ain%20natural%20and%20engineered%20environments.%20Traditional%20physics-based%20models%20face%0Ascalability%20challenges%20and%20are%20computationally%20demanding%2C%20while%20data-driven%0Aapproaches%20like%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20lack%20physical%20consistency%2C%0Ainterpretability%2C%20and%20generalization.%20In%20this%20paper%2C%20we%20propose%20Dynami-CAL%0AGraphNet%2C%20a%20Physics-Informed%20Graph%20Neural%20Network%20that%20integrates%20the%20learning%0Acapabilities%20of%20GNNs%20with%20physics-based%20inductive%20biases%20to%20address%20these%0Alimitations.%20Dynami-CAL%20GraphNet%20enforces%20pairwise%20conservation%20of%20linear%20and%0Aangular%20momentum%20for%20interacting%20nodes%20using%20edge-local%20reference%20frames%20that%0Aare%20equivariant%20to%20rotational%20symmetries%2C%20invariant%20to%20translations%2C%20and%0Aequivariant%20to%20node%20permutations.%20This%20design%20ensures%20physically%20consistent%0Apredictions%20of%20node%20dynamics%20while%20offering%20interpretable%2C%20edge-wise%20linear%20and%0Aangular%20impulses%20resulting%20from%20pairwise%20interactions.%20Evaluated%20on%20a%203D%0Agranular%20system%20with%20inelastic%20collisions%2C%20Dynami-CAL%20GraphNet%20demonstrates%0Astable%20error%20accumulation%20over%20extended%20rollouts%2C%20effective%20extrapolations%20to%0Aunseen%20configurations%2C%20and%20robust%20handling%20of%20heterogeneous%20interactions%20and%0Aexternal%20forces.%20Dynami-CAL%20GraphNet%20offers%20significant%20advantages%20in%20fields%0Arequiring%20accurate%2C%20interpretable%2C%20and%20real-time%20modeling%20of%20complex%20multi-body%0Adynamical%20systems%2C%20such%20as%20robotics%2C%20aerospace%20engineering%2C%20and%20materials%0Ascience.%20By%20providing%20physically%20consistent%20and%20scalable%20predictions%20that%0Aadhere%20to%20fundamental%20conservation%20laws%2C%20it%20enables%20the%20inference%20of%20forces%20and%0Amoments%20while%20efficiently%20handling%20heterogeneous%20interactions%20and%20external%0Aforces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07373v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynami-CAL%2520GraphNet%253A%2520A%2520Physics-Informed%2520Graph%2520Neural%2520Network%2520Conserving%250A%2520%2520Linear%2520and%2520Angular%2520Momentum%2520for%2520Dynamical%2520Systems%26entry.906535625%3DVinay%2520Sharma%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Accurate%252C%2520interpretable%252C%2520and%2520real-time%2520modeling%2520of%2520multi-body%2520dynamical%250Asystems%2520is%2520essential%2520for%2520predicting%2520behaviors%2520and%2520inferring%2520physical%2520properties%250Ain%2520natural%2520and%2520engineered%2520environments.%2520Traditional%2520physics-based%2520models%2520face%250Ascalability%2520challenges%2520and%2520are%2520computationally%2520demanding%252C%2520while%2520data-driven%250Aapproaches%2520like%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520often%2520lack%2520physical%2520consistency%252C%250Ainterpretability%252C%2520and%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Dynami-CAL%250AGraphNet%252C%2520a%2520Physics-Informed%2520Graph%2520Neural%2520Network%2520that%2520integrates%2520the%2520learning%250Acapabilities%2520of%2520GNNs%2520with%2520physics-based%2520inductive%2520biases%2520to%2520address%2520these%250Alimitations.%2520Dynami-CAL%2520GraphNet%2520enforces%2520pairwise%2520conservation%2520of%2520linear%2520and%250Aangular%2520momentum%2520for%2520interacting%2520nodes%2520using%2520edge-local%2520reference%2520frames%2520that%250Aare%2520equivariant%2520to%2520rotational%2520symmetries%252C%2520invariant%2520to%2520translations%252C%2520and%250Aequivariant%2520to%2520node%2520permutations.%2520This%2520design%2520ensures%2520physically%2520consistent%250Apredictions%2520of%2520node%2520dynamics%2520while%2520offering%2520interpretable%252C%2520edge-wise%2520linear%2520and%250Aangular%2520impulses%2520resulting%2520from%2520pairwise%2520interactions.%2520Evaluated%2520on%2520a%25203D%250Agranular%2520system%2520with%2520inelastic%2520collisions%252C%2520Dynami-CAL%2520GraphNet%2520demonstrates%250Astable%2520error%2520accumulation%2520over%2520extended%2520rollouts%252C%2520effective%2520extrapolations%2520to%250Aunseen%2520configurations%252C%2520and%2520robust%2520handling%2520of%2520heterogeneous%2520interactions%2520and%250Aexternal%2520forces.%2520Dynami-CAL%2520GraphNet%2520offers%2520significant%2520advantages%2520in%2520fields%250Arequiring%2520accurate%252C%2520interpretable%252C%2520and%2520real-time%2520modeling%2520of%2520complex%2520multi-body%250Adynamical%2520systems%252C%2520such%2520as%2520robotics%252C%2520aerospace%2520engineering%252C%2520and%2520materials%250Ascience.%2520By%2520providing%2520physically%2520consistent%2520and%2520scalable%2520predictions%2520that%250Aadhere%2520to%2520fundamental%2520conservation%2520laws%252C%2520it%2520enables%2520the%2520inference%2520of%2520forces%2520and%250Amoments%2520while%2520efficiently%2520handling%2520heterogeneous%2520interactions%2520and%2520external%250Aforces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07373v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynami-CAL%20GraphNet%3A%20A%20Physics-Informed%20Graph%20Neural%20Network%20Conserving%0A%20%20Linear%20and%20Angular%20Momentum%20for%20Dynamical%20Systems&entry.906535625=Vinay%20Sharma%20and%20Olga%20Fink&entry.1292438233=%20%20Accurate%2C%20interpretable%2C%20and%20real-time%20modeling%20of%20multi-body%20dynamical%0Asystems%20is%20essential%20for%20predicting%20behaviors%20and%20inferring%20physical%20properties%0Ain%20natural%20and%20engineered%20environments.%20Traditional%20physics-based%20models%20face%0Ascalability%20challenges%20and%20are%20computationally%20demanding%2C%20while%20data-driven%0Aapproaches%20like%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20lack%20physical%20consistency%2C%0Ainterpretability%2C%20and%20generalization.%20In%20this%20paper%2C%20we%20propose%20Dynami-CAL%0AGraphNet%2C%20a%20Physics-Informed%20Graph%20Neural%20Network%20that%20integrates%20the%20learning%0Acapabilities%20of%20GNNs%20with%20physics-based%20inductive%20biases%20to%20address%20these%0Alimitations.%20Dynami-CAL%20GraphNet%20enforces%20pairwise%20conservation%20of%20linear%20and%0Aangular%20momentum%20for%20interacting%20nodes%20using%20edge-local%20reference%20frames%20that%0Aare%20equivariant%20to%20rotational%20symmetries%2C%20invariant%20to%20translations%2C%20and%0Aequivariant%20to%20node%20permutations.%20This%20design%20ensures%20physically%20consistent%0Apredictions%20of%20node%20dynamics%20while%20offering%20interpretable%2C%20edge-wise%20linear%20and%0Aangular%20impulses%20resulting%20from%20pairwise%20interactions.%20Evaluated%20on%20a%203D%0Agranular%20system%20with%20inelastic%20collisions%2C%20Dynami-CAL%20GraphNet%20demonstrates%0Astable%20error%20accumulation%20over%20extended%20rollouts%2C%20effective%20extrapolations%20to%0Aunseen%20configurations%2C%20and%20robust%20handling%20of%20heterogeneous%20interactions%20and%0Aexternal%20forces.%20Dynami-CAL%20GraphNet%20offers%20significant%20advantages%20in%20fields%0Arequiring%20accurate%2C%20interpretable%2C%20and%20real-time%20modeling%20of%20complex%20multi-body%0Adynamical%20systems%2C%20such%20as%20robotics%2C%20aerospace%20engineering%2C%20and%20materials%0Ascience.%20By%20providing%20physically%20consistent%20and%20scalable%20predictions%20that%0Aadhere%20to%20fundamental%20conservation%20laws%2C%20it%20enables%20the%20inference%20of%20forces%20and%0Amoments%20while%20efficiently%20handling%20heterogeneous%20interactions%20and%20external%0Aforces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07373v2&entry.124074799=Read"},
{"title": "ConViS-Bench: Estimating Video Similarity Through Semantic Concepts", "author": "Benedetta Liberatori and Alessandro Conti and Lorenzo Vaquero and Yiming Wang and Elisa Ricci and Paolo Rota", "abstract": "  What does it mean for two videos to be similar? Videos may appear similar\nwhen judged by the actions they depict, yet entirely different if evaluated\nbased on the locations where they were filmed. While humans naturally compare\nvideos by taking different aspects into account, this ability has not been\nthoroughly studied and presents a challenge for models that often depend on\nbroad global similarity scores. Large Multimodal Models (LMMs) with video\nunderstanding capabilities open new opportunities for leveraging natural\nlanguage in comparative video tasks. We introduce Concept-based Video\nSimilarity estimation (ConViS), a novel task that compares pairs of videos by\ncomputing interpretable similarity scores across a predefined set of key\nsemantic concepts. ConViS allows for human-like reasoning about video\nsimilarity and enables new applications such as concept-conditioned video\nretrieval. To support this task, we also introduce ConViS-Bench, a new\nbenchmark comprising carefully annotated video pairs spanning multiple domains.\nEach pair comes with concept-level similarity scores and textual descriptions\nof both differences and similarities. Additionally, we benchmark several\nstate-of-the-art models on ConViS, providing insights into their alignment with\nhuman judgments. Our results reveal significant performance differences on\nConViS, indicating that some concepts present greater challenges for estimating\nvideo similarity. We believe that ConViS-Bench will serve as a valuable\nresource for advancing research in language-driven video understanding.\n", "link": "http://arxiv.org/abs/2509.19245v1", "date": "2025-09-23", "relevancy": 2.2546, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConViS-Bench%3A%20Estimating%20Video%20Similarity%20Through%20Semantic%20Concepts&body=Title%3A%20ConViS-Bench%3A%20Estimating%20Video%20Similarity%20Through%20Semantic%20Concepts%0AAuthor%3A%20Benedetta%20Liberatori%20and%20Alessandro%20Conti%20and%20Lorenzo%20Vaquero%20and%20Yiming%20Wang%20and%20Elisa%20Ricci%20and%20Paolo%20Rota%0AAbstract%3A%20%20%20What%20does%20it%20mean%20for%20two%20videos%20to%20be%20similar%3F%20Videos%20may%20appear%20similar%0Awhen%20judged%20by%20the%20actions%20they%20depict%2C%20yet%20entirely%20different%20if%20evaluated%0Abased%20on%20the%20locations%20where%20they%20were%20filmed.%20While%20humans%20naturally%20compare%0Avideos%20by%20taking%20different%20aspects%20into%20account%2C%20this%20ability%20has%20not%20been%0Athoroughly%20studied%20and%20presents%20a%20challenge%20for%20models%20that%20often%20depend%20on%0Abroad%20global%20similarity%20scores.%20Large%20Multimodal%20Models%20%28LMMs%29%20with%20video%0Aunderstanding%20capabilities%20open%20new%20opportunities%20for%20leveraging%20natural%0Alanguage%20in%20comparative%20video%20tasks.%20We%20introduce%20Concept-based%20Video%0ASimilarity%20estimation%20%28ConViS%29%2C%20a%20novel%20task%20that%20compares%20pairs%20of%20videos%20by%0Acomputing%20interpretable%20similarity%20scores%20across%20a%20predefined%20set%20of%20key%0Asemantic%20concepts.%20ConViS%20allows%20for%20human-like%20reasoning%20about%20video%0Asimilarity%20and%20enables%20new%20applications%20such%20as%20concept-conditioned%20video%0Aretrieval.%20To%20support%20this%20task%2C%20we%20also%20introduce%20ConViS-Bench%2C%20a%20new%0Abenchmark%20comprising%20carefully%20annotated%20video%20pairs%20spanning%20multiple%20domains.%0AEach%20pair%20comes%20with%20concept-level%20similarity%20scores%20and%20textual%20descriptions%0Aof%20both%20differences%20and%20similarities.%20Additionally%2C%20we%20benchmark%20several%0Astate-of-the-art%20models%20on%20ConViS%2C%20providing%20insights%20into%20their%20alignment%20with%0Ahuman%20judgments.%20Our%20results%20reveal%20significant%20performance%20differences%20on%0AConViS%2C%20indicating%20that%20some%20concepts%20present%20greater%20challenges%20for%20estimating%0Avideo%20similarity.%20We%20believe%20that%20ConViS-Bench%20will%20serve%20as%20a%20valuable%0Aresource%20for%20advancing%20research%20in%20language-driven%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConViS-Bench%253A%2520Estimating%2520Video%2520Similarity%2520Through%2520Semantic%2520Concepts%26entry.906535625%3DBenedetta%2520Liberatori%2520and%2520Alessandro%2520Conti%2520and%2520Lorenzo%2520Vaquero%2520and%2520Yiming%2520Wang%2520and%2520Elisa%2520Ricci%2520and%2520Paolo%2520Rota%26entry.1292438233%3D%2520%2520What%2520does%2520it%2520mean%2520for%2520two%2520videos%2520to%2520be%2520similar%253F%2520Videos%2520may%2520appear%2520similar%250Awhen%2520judged%2520by%2520the%2520actions%2520they%2520depict%252C%2520yet%2520entirely%2520different%2520if%2520evaluated%250Abased%2520on%2520the%2520locations%2520where%2520they%2520were%2520filmed.%2520While%2520humans%2520naturally%2520compare%250Avideos%2520by%2520taking%2520different%2520aspects%2520into%2520account%252C%2520this%2520ability%2520has%2520not%2520been%250Athoroughly%2520studied%2520and%2520presents%2520a%2520challenge%2520for%2520models%2520that%2520often%2520depend%2520on%250Abroad%2520global%2520similarity%2520scores.%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520with%2520video%250Aunderstanding%2520capabilities%2520open%2520new%2520opportunities%2520for%2520leveraging%2520natural%250Alanguage%2520in%2520comparative%2520video%2520tasks.%2520We%2520introduce%2520Concept-based%2520Video%250ASimilarity%2520estimation%2520%2528ConViS%2529%252C%2520a%2520novel%2520task%2520that%2520compares%2520pairs%2520of%2520videos%2520by%250Acomputing%2520interpretable%2520similarity%2520scores%2520across%2520a%2520predefined%2520set%2520of%2520key%250Asemantic%2520concepts.%2520ConViS%2520allows%2520for%2520human-like%2520reasoning%2520about%2520video%250Asimilarity%2520and%2520enables%2520new%2520applications%2520such%2520as%2520concept-conditioned%2520video%250Aretrieval.%2520To%2520support%2520this%2520task%252C%2520we%2520also%2520introduce%2520ConViS-Bench%252C%2520a%2520new%250Abenchmark%2520comprising%2520carefully%2520annotated%2520video%2520pairs%2520spanning%2520multiple%2520domains.%250AEach%2520pair%2520comes%2520with%2520concept-level%2520similarity%2520scores%2520and%2520textual%2520descriptions%250Aof%2520both%2520differences%2520and%2520similarities.%2520Additionally%252C%2520we%2520benchmark%2520several%250Astate-of-the-art%2520models%2520on%2520ConViS%252C%2520providing%2520insights%2520into%2520their%2520alignment%2520with%250Ahuman%2520judgments.%2520Our%2520results%2520reveal%2520significant%2520performance%2520differences%2520on%250AConViS%252C%2520indicating%2520that%2520some%2520concepts%2520present%2520greater%2520challenges%2520for%2520estimating%250Avideo%2520similarity.%2520We%2520believe%2520that%2520ConViS-Bench%2520will%2520serve%2520as%2520a%2520valuable%250Aresource%2520for%2520advancing%2520research%2520in%2520language-driven%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConViS-Bench%3A%20Estimating%20Video%20Similarity%20Through%20Semantic%20Concepts&entry.906535625=Benedetta%20Liberatori%20and%20Alessandro%20Conti%20and%20Lorenzo%20Vaquero%20and%20Yiming%20Wang%20and%20Elisa%20Ricci%20and%20Paolo%20Rota&entry.1292438233=%20%20What%20does%20it%20mean%20for%20two%20videos%20to%20be%20similar%3F%20Videos%20may%20appear%20similar%0Awhen%20judged%20by%20the%20actions%20they%20depict%2C%20yet%20entirely%20different%20if%20evaluated%0Abased%20on%20the%20locations%20where%20they%20were%20filmed.%20While%20humans%20naturally%20compare%0Avideos%20by%20taking%20different%20aspects%20into%20account%2C%20this%20ability%20has%20not%20been%0Athoroughly%20studied%20and%20presents%20a%20challenge%20for%20models%20that%20often%20depend%20on%0Abroad%20global%20similarity%20scores.%20Large%20Multimodal%20Models%20%28LMMs%29%20with%20video%0Aunderstanding%20capabilities%20open%20new%20opportunities%20for%20leveraging%20natural%0Alanguage%20in%20comparative%20video%20tasks.%20We%20introduce%20Concept-based%20Video%0ASimilarity%20estimation%20%28ConViS%29%2C%20a%20novel%20task%20that%20compares%20pairs%20of%20videos%20by%0Acomputing%20interpretable%20similarity%20scores%20across%20a%20predefined%20set%20of%20key%0Asemantic%20concepts.%20ConViS%20allows%20for%20human-like%20reasoning%20about%20video%0Asimilarity%20and%20enables%20new%20applications%20such%20as%20concept-conditioned%20video%0Aretrieval.%20To%20support%20this%20task%2C%20we%20also%20introduce%20ConViS-Bench%2C%20a%20new%0Abenchmark%20comprising%20carefully%20annotated%20video%20pairs%20spanning%20multiple%20domains.%0AEach%20pair%20comes%20with%20concept-level%20similarity%20scores%20and%20textual%20descriptions%0Aof%20both%20differences%20and%20similarities.%20Additionally%2C%20we%20benchmark%20several%0Astate-of-the-art%20models%20on%20ConViS%2C%20providing%20insights%20into%20their%20alignment%20with%0Ahuman%20judgments.%20Our%20results%20reveal%20significant%20performance%20differences%20on%0AConViS%2C%20indicating%20that%20some%20concepts%20present%20greater%20challenges%20for%20estimating%0Avideo%20similarity.%20We%20believe%20that%20ConViS-Bench%20will%20serve%20as%20a%20valuable%0Aresource%20for%20advancing%20research%20in%20language-driven%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19245v1&entry.124074799=Read"},
{"title": "Injecting Explainability and Lightweight Design into Weakly Supervised\n  Video Anomaly Detection Systems", "author": "Wen-Dong Jiang and Chih-Yung Chang and Hsiang-Chuan Chang and Ji-Yuan Chen and Diptendu Sinha Roy", "abstract": "  Weakly Supervised Monitoring Anomaly Detection (WSMAD) utilizes weak\nsupervision learning to identify anomalies, a critical task for smart city\nmonitoring. However, existing multimodal approaches often fail to meet the\nreal-time and interpretability requirements of edge devices due to their\ncomplexity. This paper presents TCVADS (Two-stage Cross-modal Video Anomaly\nDetection System), which leverages knowledge distillation and cross-modal\ncontrastive learning to enable efficient, accurate, and interpretable anomaly\ndetection on edge devices.TCVADS operates in two stages: coarse-grained rapid\nclassification and fine-grained detailed analysis. In the first stage, TCVADS\nextracts features from video frames and inputs them into a time series analysis\nmodule, which acts as the teacher model. Insights are then transferred via\nknowledge distillation to a simplified convolutional network (student model)\nfor binary classification. Upon detecting an anomaly, the second stage is\ntriggered, employing a fine-grained multi-class classification model. This\nstage uses CLIP for cross-modal contrastive learning with text and images,\nenhancing interpretability and achieving refined classification through\nspecially designed triplet textual relationships. Experimental results\ndemonstrate that TCVADS significantly outperforms existing methods in model\nperformance, detection efficiency, and interpretability, offering valuable\ncontributions to smart city monitoring applications.\n", "link": "http://arxiv.org/abs/2412.20201v2", "date": "2025-09-23", "relevancy": 2.2315, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5663}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.562}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Injecting%20Explainability%20and%20Lightweight%20Design%20into%20Weakly%20Supervised%0A%20%20Video%20Anomaly%20Detection%20Systems&body=Title%3A%20Injecting%20Explainability%20and%20Lightweight%20Design%20into%20Weakly%20Supervised%0A%20%20Video%20Anomaly%20Detection%20Systems%0AAuthor%3A%20Wen-Dong%20Jiang%20and%20Chih-Yung%20Chang%20and%20Hsiang-Chuan%20Chang%20and%20Ji-Yuan%20Chen%20and%20Diptendu%20Sinha%20Roy%0AAbstract%3A%20%20%20Weakly%20Supervised%20Monitoring%20Anomaly%20Detection%20%28WSMAD%29%20utilizes%20weak%0Asupervision%20learning%20to%20identify%20anomalies%2C%20a%20critical%20task%20for%20smart%20city%0Amonitoring.%20However%2C%20existing%20multimodal%20approaches%20often%20fail%20to%20meet%20the%0Areal-time%20and%20interpretability%20requirements%20of%20edge%20devices%20due%20to%20their%0Acomplexity.%20This%20paper%20presents%20TCVADS%20%28Two-stage%20Cross-modal%20Video%20Anomaly%0ADetection%20System%29%2C%20which%20leverages%20knowledge%20distillation%20and%20cross-modal%0Acontrastive%20learning%20to%20enable%20efficient%2C%20accurate%2C%20and%20interpretable%20anomaly%0Adetection%20on%20edge%20devices.TCVADS%20operates%20in%20two%20stages%3A%20coarse-grained%20rapid%0Aclassification%20and%20fine-grained%20detailed%20analysis.%20In%20the%20first%20stage%2C%20TCVADS%0Aextracts%20features%20from%20video%20frames%20and%20inputs%20them%20into%20a%20time%20series%20analysis%0Amodule%2C%20which%20acts%20as%20the%20teacher%20model.%20Insights%20are%20then%20transferred%20via%0Aknowledge%20distillation%20to%20a%20simplified%20convolutional%20network%20%28student%20model%29%0Afor%20binary%20classification.%20Upon%20detecting%20an%20anomaly%2C%20the%20second%20stage%20is%0Atriggered%2C%20employing%20a%20fine-grained%20multi-class%20classification%20model.%20This%0Astage%20uses%20CLIP%20for%20cross-modal%20contrastive%20learning%20with%20text%20and%20images%2C%0Aenhancing%20interpretability%20and%20achieving%20refined%20classification%20through%0Aspecially%20designed%20triplet%20textual%20relationships.%20Experimental%20results%0Ademonstrate%20that%20TCVADS%20significantly%20outperforms%20existing%20methods%20in%20model%0Aperformance%2C%20detection%20efficiency%2C%20and%20interpretability%2C%20offering%20valuable%0Acontributions%20to%20smart%20city%20monitoring%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20201v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInjecting%2520Explainability%2520and%2520Lightweight%2520Design%2520into%2520Weakly%2520Supervised%250A%2520%2520Video%2520Anomaly%2520Detection%2520Systems%26entry.906535625%3DWen-Dong%2520Jiang%2520and%2520Chih-Yung%2520Chang%2520and%2520Hsiang-Chuan%2520Chang%2520and%2520Ji-Yuan%2520Chen%2520and%2520Diptendu%2520Sinha%2520Roy%26entry.1292438233%3D%2520%2520Weakly%2520Supervised%2520Monitoring%2520Anomaly%2520Detection%2520%2528WSMAD%2529%2520utilizes%2520weak%250Asupervision%2520learning%2520to%2520identify%2520anomalies%252C%2520a%2520critical%2520task%2520for%2520smart%2520city%250Amonitoring.%2520However%252C%2520existing%2520multimodal%2520approaches%2520often%2520fail%2520to%2520meet%2520the%250Areal-time%2520and%2520interpretability%2520requirements%2520of%2520edge%2520devices%2520due%2520to%2520their%250Acomplexity.%2520This%2520paper%2520presents%2520TCVADS%2520%2528Two-stage%2520Cross-modal%2520Video%2520Anomaly%250ADetection%2520System%2529%252C%2520which%2520leverages%2520knowledge%2520distillation%2520and%2520cross-modal%250Acontrastive%2520learning%2520to%2520enable%2520efficient%252C%2520accurate%252C%2520and%2520interpretable%2520anomaly%250Adetection%2520on%2520edge%2520devices.TCVADS%2520operates%2520in%2520two%2520stages%253A%2520coarse-grained%2520rapid%250Aclassification%2520and%2520fine-grained%2520detailed%2520analysis.%2520In%2520the%2520first%2520stage%252C%2520TCVADS%250Aextracts%2520features%2520from%2520video%2520frames%2520and%2520inputs%2520them%2520into%2520a%2520time%2520series%2520analysis%250Amodule%252C%2520which%2520acts%2520as%2520the%2520teacher%2520model.%2520Insights%2520are%2520then%2520transferred%2520via%250Aknowledge%2520distillation%2520to%2520a%2520simplified%2520convolutional%2520network%2520%2528student%2520model%2529%250Afor%2520binary%2520classification.%2520Upon%2520detecting%2520an%2520anomaly%252C%2520the%2520second%2520stage%2520is%250Atriggered%252C%2520employing%2520a%2520fine-grained%2520multi-class%2520classification%2520model.%2520This%250Astage%2520uses%2520CLIP%2520for%2520cross-modal%2520contrastive%2520learning%2520with%2520text%2520and%2520images%252C%250Aenhancing%2520interpretability%2520and%2520achieving%2520refined%2520classification%2520through%250Aspecially%2520designed%2520triplet%2520textual%2520relationships.%2520Experimental%2520results%250Ademonstrate%2520that%2520TCVADS%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520model%250Aperformance%252C%2520detection%2520efficiency%252C%2520and%2520interpretability%252C%2520offering%2520valuable%250Acontributions%2520to%2520smart%2520city%2520monitoring%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20201v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Injecting%20Explainability%20and%20Lightweight%20Design%20into%20Weakly%20Supervised%0A%20%20Video%20Anomaly%20Detection%20Systems&entry.906535625=Wen-Dong%20Jiang%20and%20Chih-Yung%20Chang%20and%20Hsiang-Chuan%20Chang%20and%20Ji-Yuan%20Chen%20and%20Diptendu%20Sinha%20Roy&entry.1292438233=%20%20Weakly%20Supervised%20Monitoring%20Anomaly%20Detection%20%28WSMAD%29%20utilizes%20weak%0Asupervision%20learning%20to%20identify%20anomalies%2C%20a%20critical%20task%20for%20smart%20city%0Amonitoring.%20However%2C%20existing%20multimodal%20approaches%20often%20fail%20to%20meet%20the%0Areal-time%20and%20interpretability%20requirements%20of%20edge%20devices%20due%20to%20their%0Acomplexity.%20This%20paper%20presents%20TCVADS%20%28Two-stage%20Cross-modal%20Video%20Anomaly%0ADetection%20System%29%2C%20which%20leverages%20knowledge%20distillation%20and%20cross-modal%0Acontrastive%20learning%20to%20enable%20efficient%2C%20accurate%2C%20and%20interpretable%20anomaly%0Adetection%20on%20edge%20devices.TCVADS%20operates%20in%20two%20stages%3A%20coarse-grained%20rapid%0Aclassification%20and%20fine-grained%20detailed%20analysis.%20In%20the%20first%20stage%2C%20TCVADS%0Aextracts%20features%20from%20video%20frames%20and%20inputs%20them%20into%20a%20time%20series%20analysis%0Amodule%2C%20which%20acts%20as%20the%20teacher%20model.%20Insights%20are%20then%20transferred%20via%0Aknowledge%20distillation%20to%20a%20simplified%20convolutional%20network%20%28student%20model%29%0Afor%20binary%20classification.%20Upon%20detecting%20an%20anomaly%2C%20the%20second%20stage%20is%0Atriggered%2C%20employing%20a%20fine-grained%20multi-class%20classification%20model.%20This%0Astage%20uses%20CLIP%20for%20cross-modal%20contrastive%20learning%20with%20text%20and%20images%2C%0Aenhancing%20interpretability%20and%20achieving%20refined%20classification%20through%0Aspecially%20designed%20triplet%20textual%20relationships.%20Experimental%20results%0Ademonstrate%20that%20TCVADS%20significantly%20outperforms%20existing%20methods%20in%20model%0Aperformance%2C%20detection%20efficiency%2C%20and%20interpretability%2C%20offering%20valuable%0Acontributions%20to%20smart%20city%20monitoring%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20201v2&entry.124074799=Read"},
{"title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human\n  Demonstration to Robotic Deployment Gap", "author": "Tianyu Wu and Xudong Han and Haoran Sun and Zishang Zhang and Bangchao Huang and Chaoyang Song and Fang Wan", "abstract": "  The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.\n", "link": "http://arxiv.org/abs/2509.19169v1", "date": "2025-09-23", "relevancy": 2.2202, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5724}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5669}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagiClaw%3A%20A%20Dual-Use%2C%20Vision-Based%20Soft%20Gripper%20for%20Bridging%20the%20Human%0A%20%20Demonstration%20to%20Robotic%20Deployment%20Gap&body=Title%3A%20MagiClaw%3A%20A%20Dual-Use%2C%20Vision-Based%20Soft%20Gripper%20for%20Bridging%20the%20Human%0A%20%20Demonstration%20to%20Robotic%20Deployment%20Gap%0AAuthor%3A%20Tianyu%20Wu%20and%20Xudong%20Han%20and%20Haoran%20Sun%20and%20Zishang%20Zhang%20and%20Bangchao%20Huang%20and%20Chaoyang%20Song%20and%20Fang%20Wan%0AAbstract%3A%20%20%20The%20transfer%20of%20manipulation%20skills%20from%20human%20demonstration%20to%20robotic%0Aexecution%20is%20often%20hindered%20by%20a%20%22domain%20gap%22%20in%20sensing%20and%20morphology.%20This%0Apaper%20introduces%20MagiClaw%2C%20a%20versatile%20two-finger%20end-effector%20designed%20to%0Abridge%20this%20gap.%20MagiClaw%20functions%20interchangeably%20as%20both%20a%20handheld%20tool%20for%0Aintuitive%20data%20collection%20and%20a%20robotic%20end-effector%20for%20policy%20deployment%2C%0Aensuring%20hardware%20consistency%20and%20reliability.%20Each%20finger%20incorporates%20a%20Soft%0APolyhedral%20Network%20%28SPN%29%20with%20an%20embedded%20camera%2C%20enabling%20vision-based%0Aestimation%20of%206-DoF%20forces%20and%20contact%20deformation.%20This%20proprioceptive%20data%20is%0Afused%20with%20exteroceptive%20environmental%20sensing%20from%20an%20integrated%20iPhone%2C%20which%0Aprovides%206D%20pose%2C%20RGB%20video%2C%20and%20LiDAR-based%20depth%20maps.%20Through%20a%20custom%20iOS%0Aapplication%2C%20MagiClaw%20streams%20synchronized%2C%20multi-modal%20data%20for%20real-time%0Ateleoperation%2C%20offline%20policy%20learning%2C%20and%20immersive%20control%20via%20mixed-reality%0Ainterfaces.%20We%20demonstrate%20how%20this%20unified%20system%20architecture%20lowers%20the%0Abarrier%20to%20collecting%20high-fidelity%2C%20contact-rich%20datasets%20and%20accelerates%20the%0Adevelopment%20of%20generalizable%20manipulation%20policies.%20Please%20refer%20to%20the%20iOS%20app%0Aat%20https%3A//apps.apple.com/cn/app/magiclaw/id6661033548%20for%20further%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagiClaw%253A%2520A%2520Dual-Use%252C%2520Vision-Based%2520Soft%2520Gripper%2520for%2520Bridging%2520the%2520Human%250A%2520%2520Demonstration%2520to%2520Robotic%2520Deployment%2520Gap%26entry.906535625%3DTianyu%2520Wu%2520and%2520Xudong%2520Han%2520and%2520Haoran%2520Sun%2520and%2520Zishang%2520Zhang%2520and%2520Bangchao%2520Huang%2520and%2520Chaoyang%2520Song%2520and%2520Fang%2520Wan%26entry.1292438233%3D%2520%2520The%2520transfer%2520of%2520manipulation%2520skills%2520from%2520human%2520demonstration%2520to%2520robotic%250Aexecution%2520is%2520often%2520hindered%2520by%2520a%2520%2522domain%2520gap%2522%2520in%2520sensing%2520and%2520morphology.%2520This%250Apaper%2520introduces%2520MagiClaw%252C%2520a%2520versatile%2520two-finger%2520end-effector%2520designed%2520to%250Abridge%2520this%2520gap.%2520MagiClaw%2520functions%2520interchangeably%2520as%2520both%2520a%2520handheld%2520tool%2520for%250Aintuitive%2520data%2520collection%2520and%2520a%2520robotic%2520end-effector%2520for%2520policy%2520deployment%252C%250Aensuring%2520hardware%2520consistency%2520and%2520reliability.%2520Each%2520finger%2520incorporates%2520a%2520Soft%250APolyhedral%2520Network%2520%2528SPN%2529%2520with%2520an%2520embedded%2520camera%252C%2520enabling%2520vision-based%250Aestimation%2520of%25206-DoF%2520forces%2520and%2520contact%2520deformation.%2520This%2520proprioceptive%2520data%2520is%250Afused%2520with%2520exteroceptive%2520environmental%2520sensing%2520from%2520an%2520integrated%2520iPhone%252C%2520which%250Aprovides%25206D%2520pose%252C%2520RGB%2520video%252C%2520and%2520LiDAR-based%2520depth%2520maps.%2520Through%2520a%2520custom%2520iOS%250Aapplication%252C%2520MagiClaw%2520streams%2520synchronized%252C%2520multi-modal%2520data%2520for%2520real-time%250Ateleoperation%252C%2520offline%2520policy%2520learning%252C%2520and%2520immersive%2520control%2520via%2520mixed-reality%250Ainterfaces.%2520We%2520demonstrate%2520how%2520this%2520unified%2520system%2520architecture%2520lowers%2520the%250Abarrier%2520to%2520collecting%2520high-fidelity%252C%2520contact-rich%2520datasets%2520and%2520accelerates%2520the%250Adevelopment%2520of%2520generalizable%2520manipulation%2520policies.%2520Please%2520refer%2520to%2520the%2520iOS%2520app%250Aat%2520https%253A//apps.apple.com/cn/app/magiclaw/id6661033548%2520for%2520further%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagiClaw%3A%20A%20Dual-Use%2C%20Vision-Based%20Soft%20Gripper%20for%20Bridging%20the%20Human%0A%20%20Demonstration%20to%20Robotic%20Deployment%20Gap&entry.906535625=Tianyu%20Wu%20and%20Xudong%20Han%20and%20Haoran%20Sun%20and%20Zishang%20Zhang%20and%20Bangchao%20Huang%20and%20Chaoyang%20Song%20and%20Fang%20Wan&entry.1292438233=%20%20The%20transfer%20of%20manipulation%20skills%20from%20human%20demonstration%20to%20robotic%0Aexecution%20is%20often%20hindered%20by%20a%20%22domain%20gap%22%20in%20sensing%20and%20morphology.%20This%0Apaper%20introduces%20MagiClaw%2C%20a%20versatile%20two-finger%20end-effector%20designed%20to%0Abridge%20this%20gap.%20MagiClaw%20functions%20interchangeably%20as%20both%20a%20handheld%20tool%20for%0Aintuitive%20data%20collection%20and%20a%20robotic%20end-effector%20for%20policy%20deployment%2C%0Aensuring%20hardware%20consistency%20and%20reliability.%20Each%20finger%20incorporates%20a%20Soft%0APolyhedral%20Network%20%28SPN%29%20with%20an%20embedded%20camera%2C%20enabling%20vision-based%0Aestimation%20of%206-DoF%20forces%20and%20contact%20deformation.%20This%20proprioceptive%20data%20is%0Afused%20with%20exteroceptive%20environmental%20sensing%20from%20an%20integrated%20iPhone%2C%20which%0Aprovides%206D%20pose%2C%20RGB%20video%2C%20and%20LiDAR-based%20depth%20maps.%20Through%20a%20custom%20iOS%0Aapplication%2C%20MagiClaw%20streams%20synchronized%2C%20multi-modal%20data%20for%20real-time%0Ateleoperation%2C%20offline%20policy%20learning%2C%20and%20immersive%20control%20via%20mixed-reality%0Ainterfaces.%20We%20demonstrate%20how%20this%20unified%20system%20architecture%20lowers%20the%0Abarrier%20to%20collecting%20high-fidelity%2C%20contact-rich%20datasets%20and%20accelerates%20the%0Adevelopment%20of%20generalizable%20manipulation%20policies.%20Please%20refer%20to%20the%20iOS%20app%0Aat%20https%3A//apps.apple.com/cn/app/magiclaw/id6661033548%20for%20further%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19169v1&entry.124074799=Read"},
{"title": "No Labels Needed: Zero-Shot Image Classification with Collaborative\n  Self-Learning", "author": "Matheus Vin\u00edcius Todescato and Joel Lu\u00eds Carbonera", "abstract": "  While deep learning, including Convolutional Neural Networks (CNNs) and\nVision Transformers (ViTs), has significantly advanced classification\nperformance, its typical reliance on extensive annotated datasets presents a\nmajor obstacle in many practical scenarios where such data is scarce.\nVision-language models (VLMs) and transfer learning with pre-trained visual\nmodels appear as promising techniques to deal with this problem. This paper\nproposes a novel zero-shot image classification framework that combines a VLM\nand a pre-trained visual model within a self-learning cycle. Requiring only the\nset of class names and no labeled training data, our method utilizes a\nconfidence-based pseudo-labeling strategy to train a lightweight classifier\ndirectly on the test data, enabling dynamic adaptation. The VLM identifies\nhigh-confidence samples, and the pre-trained visual model enhances their visual\nrepresentations. These enhanced features then iteratively train the classifier,\nallowing the system to capture complementary semantic and visual cues without\nsupervision. Notably, our approach avoids VLM fine-tuning and the use of large\nlanguage models, relying on the visual-only model to reduce the dependence on\nsemantic representation. Experimental evaluations on ten diverse datasets\ndemonstrate that our approach outperforms the baseline zero-shot method.\n", "link": "http://arxiv.org/abs/2509.18938v1", "date": "2025-09-23", "relevancy": 2.2189, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5608}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5596}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Labels%20Needed%3A%20Zero-Shot%20Image%20Classification%20with%20Collaborative%0A%20%20Self-Learning&body=Title%3A%20No%20Labels%20Needed%3A%20Zero-Shot%20Image%20Classification%20with%20Collaborative%0A%20%20Self-Learning%0AAuthor%3A%20Matheus%20Vin%C3%ADcius%20Todescato%20and%20Joel%20Lu%C3%ADs%20Carbonera%0AAbstract%3A%20%20%20While%20deep%20learning%2C%20including%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%0AVision%20Transformers%20%28ViTs%29%2C%20has%20significantly%20advanced%20classification%0Aperformance%2C%20its%20typical%20reliance%20on%20extensive%20annotated%20datasets%20presents%20a%0Amajor%20obstacle%20in%20many%20practical%20scenarios%20where%20such%20data%20is%20scarce.%0AVision-language%20models%20%28VLMs%29%20and%20transfer%20learning%20with%20pre-trained%20visual%0Amodels%20appear%20as%20promising%20techniques%20to%20deal%20with%20this%20problem.%20This%20paper%0Aproposes%20a%20novel%20zero-shot%20image%20classification%20framework%20that%20combines%20a%20VLM%0Aand%20a%20pre-trained%20visual%20model%20within%20a%20self-learning%20cycle.%20Requiring%20only%20the%0Aset%20of%20class%20names%20and%20no%20labeled%20training%20data%2C%20our%20method%20utilizes%20a%0Aconfidence-based%20pseudo-labeling%20strategy%20to%20train%20a%20lightweight%20classifier%0Adirectly%20on%20the%20test%20data%2C%20enabling%20dynamic%20adaptation.%20The%20VLM%20identifies%0Ahigh-confidence%20samples%2C%20and%20the%20pre-trained%20visual%20model%20enhances%20their%20visual%0Arepresentations.%20These%20enhanced%20features%20then%20iteratively%20train%20the%20classifier%2C%0Aallowing%20the%20system%20to%20capture%20complementary%20semantic%20and%20visual%20cues%20without%0Asupervision.%20Notably%2C%20our%20approach%20avoids%20VLM%20fine-tuning%20and%20the%20use%20of%20large%0Alanguage%20models%2C%20relying%20on%20the%20visual-only%20model%20to%20reduce%20the%20dependence%20on%0Asemantic%20representation.%20Experimental%20evaluations%20on%20ten%20diverse%20datasets%0Ademonstrate%20that%20our%20approach%20outperforms%20the%20baseline%20zero-shot%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Labels%2520Needed%253A%2520Zero-Shot%2520Image%2520Classification%2520with%2520Collaborative%250A%2520%2520Self-Learning%26entry.906535625%3DMatheus%2520Vin%25C3%25ADcius%2520Todescato%2520and%2520Joel%2520Lu%25C3%25ADs%2520Carbonera%26entry.1292438233%3D%2520%2520While%2520deep%2520learning%252C%2520including%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%250AVision%2520Transformers%2520%2528ViTs%2529%252C%2520has%2520significantly%2520advanced%2520classification%250Aperformance%252C%2520its%2520typical%2520reliance%2520on%2520extensive%2520annotated%2520datasets%2520presents%2520a%250Amajor%2520obstacle%2520in%2520many%2520practical%2520scenarios%2520where%2520such%2520data%2520is%2520scarce.%250AVision-language%2520models%2520%2528VLMs%2529%2520and%2520transfer%2520learning%2520with%2520pre-trained%2520visual%250Amodels%2520appear%2520as%2520promising%2520techniques%2520to%2520deal%2520with%2520this%2520problem.%2520This%2520paper%250Aproposes%2520a%2520novel%2520zero-shot%2520image%2520classification%2520framework%2520that%2520combines%2520a%2520VLM%250Aand%2520a%2520pre-trained%2520visual%2520model%2520within%2520a%2520self-learning%2520cycle.%2520Requiring%2520only%2520the%250Aset%2520of%2520class%2520names%2520and%2520no%2520labeled%2520training%2520data%252C%2520our%2520method%2520utilizes%2520a%250Aconfidence-based%2520pseudo-labeling%2520strategy%2520to%2520train%2520a%2520lightweight%2520classifier%250Adirectly%2520on%2520the%2520test%2520data%252C%2520enabling%2520dynamic%2520adaptation.%2520The%2520VLM%2520identifies%250Ahigh-confidence%2520samples%252C%2520and%2520the%2520pre-trained%2520visual%2520model%2520enhances%2520their%2520visual%250Arepresentations.%2520These%2520enhanced%2520features%2520then%2520iteratively%2520train%2520the%2520classifier%252C%250Aallowing%2520the%2520system%2520to%2520capture%2520complementary%2520semantic%2520and%2520visual%2520cues%2520without%250Asupervision.%2520Notably%252C%2520our%2520approach%2520avoids%2520VLM%2520fine-tuning%2520and%2520the%2520use%2520of%2520large%250Alanguage%2520models%252C%2520relying%2520on%2520the%2520visual-only%2520model%2520to%2520reduce%2520the%2520dependence%2520on%250Asemantic%2520representation.%2520Experimental%2520evaluations%2520on%2520ten%2520diverse%2520datasets%250Ademonstrate%2520that%2520our%2520approach%2520outperforms%2520the%2520baseline%2520zero-shot%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Labels%20Needed%3A%20Zero-Shot%20Image%20Classification%20with%20Collaborative%0A%20%20Self-Learning&entry.906535625=Matheus%20Vin%C3%ADcius%20Todescato%20and%20Joel%20Lu%C3%ADs%20Carbonera&entry.1292438233=%20%20While%20deep%20learning%2C%20including%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%0AVision%20Transformers%20%28ViTs%29%2C%20has%20significantly%20advanced%20classification%0Aperformance%2C%20its%20typical%20reliance%20on%20extensive%20annotated%20datasets%20presents%20a%0Amajor%20obstacle%20in%20many%20practical%20scenarios%20where%20such%20data%20is%20scarce.%0AVision-language%20models%20%28VLMs%29%20and%20transfer%20learning%20with%20pre-trained%20visual%0Amodels%20appear%20as%20promising%20techniques%20to%20deal%20with%20this%20problem.%20This%20paper%0Aproposes%20a%20novel%20zero-shot%20image%20classification%20framework%20that%20combines%20a%20VLM%0Aand%20a%20pre-trained%20visual%20model%20within%20a%20self-learning%20cycle.%20Requiring%20only%20the%0Aset%20of%20class%20names%20and%20no%20labeled%20training%20data%2C%20our%20method%20utilizes%20a%0Aconfidence-based%20pseudo-labeling%20strategy%20to%20train%20a%20lightweight%20classifier%0Adirectly%20on%20the%20test%20data%2C%20enabling%20dynamic%20adaptation.%20The%20VLM%20identifies%0Ahigh-confidence%20samples%2C%20and%20the%20pre-trained%20visual%20model%20enhances%20their%20visual%0Arepresentations.%20These%20enhanced%20features%20then%20iteratively%20train%20the%20classifier%2C%0Aallowing%20the%20system%20to%20capture%20complementary%20semantic%20and%20visual%20cues%20without%0Asupervision.%20Notably%2C%20our%20approach%20avoids%20VLM%20fine-tuning%20and%20the%20use%20of%20large%0Alanguage%20models%2C%20relying%20on%20the%20visual-only%20model%20to%20reduce%20the%20dependence%20on%0Asemantic%20representation.%20Experimental%20evaluations%20on%20ten%20diverse%20datasets%0Ademonstrate%20that%20our%20approach%20outperforms%20the%20baseline%20zero-shot%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18938v1&entry.124074799=Read"},
{"title": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA\n  Results with Consistent Training and Inference", "author": "Alexey Nekrasov and Ali Athar and Daan de Geus and Alexander Hermans and Bastian Leibe", "abstract": "  Sa2VA is a recent model for language-guided dense grounding in images and\nvideo that achieves state-of-the-art results on multiple segmentation\nbenchmarks and that has become widely popular. However, we found that Sa2VA\ndoes not perform according to its full potential for referring video object\nsegmentation tasks. We identify inconsistencies between training and inference\nprocedures as the key factor holding it back. To mitigate this issue, we\npropose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and\nimproves the results. In fact, Sa2VA-i sets a new state of the art for multiple\nvideo benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on\nRef-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA\ncheckpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the\noriginal Sa2VA-26B model on the MeViS benchmark. We hope that this work will\nshow the importance of seemingly trivial implementation details and that it\nwill provide valuable insights for the referring video segmentation field. We\nprovide the code and updated models at https://github.com/kumuji/sa2va-i\n", "link": "http://arxiv.org/abs/2509.19082v1", "date": "2025-09-23", "relevancy": 2.2143, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203rd%20Place%20Report%20of%20LSVOS%202025%20MeViS%20Track%3A%20Sa2VA-i%3A%20Improving%20Sa2VA%0A%20%20Results%20with%20Consistent%20Training%20and%20Inference&body=Title%3A%203rd%20Place%20Report%20of%20LSVOS%202025%20MeViS%20Track%3A%20Sa2VA-i%3A%20Improving%20Sa2VA%0A%20%20Results%20with%20Consistent%20Training%20and%20Inference%0AAuthor%3A%20Alexey%20Nekrasov%20and%20Ali%20Athar%20and%20Daan%20de%20Geus%20and%20Alexander%20Hermans%20and%20Bastian%20Leibe%0AAbstract%3A%20%20%20Sa2VA%20is%20a%20recent%20model%20for%20language-guided%20dense%20grounding%20in%20images%20and%0Avideo%20that%20achieves%20state-of-the-art%20results%20on%20multiple%20segmentation%0Abenchmarks%20and%20that%20has%20become%20widely%20popular.%20However%2C%20we%20found%20that%20Sa2VA%0Adoes%20not%20perform%20according%20to%20its%20full%20potential%20for%20referring%20video%20object%0Asegmentation%20tasks.%20We%20identify%20inconsistencies%20between%20training%20and%20inference%0Aprocedures%20as%20the%20key%20factor%20holding%20it%20back.%20To%20mitigate%20this%20issue%2C%20we%0Apropose%20an%20improved%20version%20of%20Sa2VA%2C%20Sa2VA-i%2C%20that%20rectifies%20these%20issues%20and%0Aimproves%20the%20results.%20In%20fact%2C%20Sa2VA-i%20sets%20a%20new%20state%20of%20the%20art%20for%20multiple%0Avideo%20benchmarks%20and%20achieves%20improvements%20of%20up%20to%20%2B11.6%20J%26F%20on%20MeViS%2C%20%2B1.4%20on%0ARef-YT-VOS%2C%20%2B3.3%20on%20Ref-DAVIS%20and%20%2B4.1%20on%20ReVOS%20using%20the%20same%20Sa2VA%0Acheckpoints.%20With%20our%20fixes%2C%20the%20Sa2VA-i-1B%20model%20even%20performs%20on%20par%20with%20the%0Aoriginal%20Sa2VA-26B%20model%20on%20the%20MeViS%20benchmark.%20We%20hope%20that%20this%20work%20will%0Ashow%20the%20importance%20of%20seemingly%20trivial%20implementation%20details%20and%20that%20it%0Awill%20provide%20valuable%20insights%20for%20the%20referring%20video%20segmentation%20field.%20We%0Aprovide%20the%20code%20and%20updated%20models%20at%20https%3A//github.com/kumuji/sa2va-i%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3rd%2520Place%2520Report%2520of%2520LSVOS%25202025%2520MeViS%2520Track%253A%2520Sa2VA-i%253A%2520Improving%2520Sa2VA%250A%2520%2520Results%2520with%2520Consistent%2520Training%2520and%2520Inference%26entry.906535625%3DAlexey%2520Nekrasov%2520and%2520Ali%2520Athar%2520and%2520Daan%2520de%2520Geus%2520and%2520Alexander%2520Hermans%2520and%2520Bastian%2520Leibe%26entry.1292438233%3D%2520%2520Sa2VA%2520is%2520a%2520recent%2520model%2520for%2520language-guided%2520dense%2520grounding%2520in%2520images%2520and%250Avideo%2520that%2520achieves%2520state-of-the-art%2520results%2520on%2520multiple%2520segmentation%250Abenchmarks%2520and%2520that%2520has%2520become%2520widely%2520popular.%2520However%252C%2520we%2520found%2520that%2520Sa2VA%250Adoes%2520not%2520perform%2520according%2520to%2520its%2520full%2520potential%2520for%2520referring%2520video%2520object%250Asegmentation%2520tasks.%2520We%2520identify%2520inconsistencies%2520between%2520training%2520and%2520inference%250Aprocedures%2520as%2520the%2520key%2520factor%2520holding%2520it%2520back.%2520To%2520mitigate%2520this%2520issue%252C%2520we%250Apropose%2520an%2520improved%2520version%2520of%2520Sa2VA%252C%2520Sa2VA-i%252C%2520that%2520rectifies%2520these%2520issues%2520and%250Aimproves%2520the%2520results.%2520In%2520fact%252C%2520Sa2VA-i%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520for%2520multiple%250Avideo%2520benchmarks%2520and%2520achieves%2520improvements%2520of%2520up%2520to%2520%252B11.6%2520J%2526F%2520on%2520MeViS%252C%2520%252B1.4%2520on%250ARef-YT-VOS%252C%2520%252B3.3%2520on%2520Ref-DAVIS%2520and%2520%252B4.1%2520on%2520ReVOS%2520using%2520the%2520same%2520Sa2VA%250Acheckpoints.%2520With%2520our%2520fixes%252C%2520the%2520Sa2VA-i-1B%2520model%2520even%2520performs%2520on%2520par%2520with%2520the%250Aoriginal%2520Sa2VA-26B%2520model%2520on%2520the%2520MeViS%2520benchmark.%2520We%2520hope%2520that%2520this%2520work%2520will%250Ashow%2520the%2520importance%2520of%2520seemingly%2520trivial%2520implementation%2520details%2520and%2520that%2520it%250Awill%2520provide%2520valuable%2520insights%2520for%2520the%2520referring%2520video%2520segmentation%2520field.%2520We%250Aprovide%2520the%2520code%2520and%2520updated%2520models%2520at%2520https%253A//github.com/kumuji/sa2va-i%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3rd%20Place%20Report%20of%20LSVOS%202025%20MeViS%20Track%3A%20Sa2VA-i%3A%20Improving%20Sa2VA%0A%20%20Results%20with%20Consistent%20Training%20and%20Inference&entry.906535625=Alexey%20Nekrasov%20and%20Ali%20Athar%20and%20Daan%20de%20Geus%20and%20Alexander%20Hermans%20and%20Bastian%20Leibe&entry.1292438233=%20%20Sa2VA%20is%20a%20recent%20model%20for%20language-guided%20dense%20grounding%20in%20images%20and%0Avideo%20that%20achieves%20state-of-the-art%20results%20on%20multiple%20segmentation%0Abenchmarks%20and%20that%20has%20become%20widely%20popular.%20However%2C%20we%20found%20that%20Sa2VA%0Adoes%20not%20perform%20according%20to%20its%20full%20potential%20for%20referring%20video%20object%0Asegmentation%20tasks.%20We%20identify%20inconsistencies%20between%20training%20and%20inference%0Aprocedures%20as%20the%20key%20factor%20holding%20it%20back.%20To%20mitigate%20this%20issue%2C%20we%0Apropose%20an%20improved%20version%20of%20Sa2VA%2C%20Sa2VA-i%2C%20that%20rectifies%20these%20issues%20and%0Aimproves%20the%20results.%20In%20fact%2C%20Sa2VA-i%20sets%20a%20new%20state%20of%20the%20art%20for%20multiple%0Avideo%20benchmarks%20and%20achieves%20improvements%20of%20up%20to%20%2B11.6%20J%26F%20on%20MeViS%2C%20%2B1.4%20on%0ARef-YT-VOS%2C%20%2B3.3%20on%20Ref-DAVIS%20and%20%2B4.1%20on%20ReVOS%20using%20the%20same%20Sa2VA%0Acheckpoints.%20With%20our%20fixes%2C%20the%20Sa2VA-i-1B%20model%20even%20performs%20on%20par%20with%20the%0Aoriginal%20Sa2VA-26B%20model%20on%20the%20MeViS%20benchmark.%20We%20hope%20that%20this%20work%20will%0Ashow%20the%20importance%20of%20seemingly%20trivial%20implementation%20details%20and%20that%20it%0Awill%20provide%20valuable%20insights%20for%20the%20referring%20video%20segmentation%20field.%20We%0Aprovide%20the%20code%20and%20updated%20models%20at%20https%3A//github.com/kumuji/sa2va-i%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19082v1&entry.124074799=Read"},
{"title": "ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness\n  Tests?", "author": "Zijian Ling and Han Zhang and Yazhuo Zhou and Jiahao Cui", "abstract": "  This paper presents ColorBlindnessEval, a novel benchmark designed to\nevaluate the robustness of Vision-Language Models (VLMs) in visually\nadversarial scenarios inspired by the Ishihara color blindness test. Our\ndataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with\nvarying color combinations, challenging VLMs to accurately recognize numerical\ninformation embedded in complex visual patterns. We assess 9 VLMs using Yes/No\nand open-ended prompts and compare their performance with human participants.\nOur experiments reveal limitations in the models' ability to interpret numbers\nin adversarial contexts, highlighting prevalent hallucination issues. These\nfindings underscore the need to improve the robustness of VLMs in complex\nvisual environments. ColorBlindnessEval serves as a valuable tool for\nbenchmarking and improving the reliability of VLMs in real-world applications\nwhere accuracy is critical.\n", "link": "http://arxiv.org/abs/2509.19070v1", "date": "2025-09-23", "relevancy": 2.2083, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5575}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ColorBlindnessEval%3A%20Can%20Vision-Language%20Models%20Pass%20Color%20Blindness%0A%20%20Tests%3F&body=Title%3A%20ColorBlindnessEval%3A%20Can%20Vision-Language%20Models%20Pass%20Color%20Blindness%0A%20%20Tests%3F%0AAuthor%3A%20Zijian%20Ling%20and%20Han%20Zhang%20and%20Yazhuo%20Zhou%20and%20Jiahao%20Cui%0AAbstract%3A%20%20%20This%20paper%20presents%20ColorBlindnessEval%2C%20a%20novel%20benchmark%20designed%20to%0Aevaluate%20the%20robustness%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20visually%0Aadversarial%20scenarios%20inspired%20by%20the%20Ishihara%20color%20blindness%20test.%20Our%0Adataset%20comprises%20500%20Ishihara-like%20images%20featuring%20numbers%20from%200%20to%2099%20with%0Avarying%20color%20combinations%2C%20challenging%20VLMs%20to%20accurately%20recognize%20numerical%0Ainformation%20embedded%20in%20complex%20visual%20patterns.%20We%20assess%209%20VLMs%20using%20Yes/No%0Aand%20open-ended%20prompts%20and%20compare%20their%20performance%20with%20human%20participants.%0AOur%20experiments%20reveal%20limitations%20in%20the%20models%27%20ability%20to%20interpret%20numbers%0Ain%20adversarial%20contexts%2C%20highlighting%20prevalent%20hallucination%20issues.%20These%0Afindings%20underscore%20the%20need%20to%20improve%20the%20robustness%20of%20VLMs%20in%20complex%0Avisual%20environments.%20ColorBlindnessEval%20serves%20as%20a%20valuable%20tool%20for%0Abenchmarking%20and%20improving%20the%20reliability%20of%20VLMs%20in%20real-world%20applications%0Awhere%20accuracy%20is%20critical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColorBlindnessEval%253A%2520Can%2520Vision-Language%2520Models%2520Pass%2520Color%2520Blindness%250A%2520%2520Tests%253F%26entry.906535625%3DZijian%2520Ling%2520and%2520Han%2520Zhang%2520and%2520Yazhuo%2520Zhou%2520and%2520Jiahao%2520Cui%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520ColorBlindnessEval%252C%2520a%2520novel%2520benchmark%2520designed%2520to%250Aevaluate%2520the%2520robustness%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520in%2520visually%250Aadversarial%2520scenarios%2520inspired%2520by%2520the%2520Ishihara%2520color%2520blindness%2520test.%2520Our%250Adataset%2520comprises%2520500%2520Ishihara-like%2520images%2520featuring%2520numbers%2520from%25200%2520to%252099%2520with%250Avarying%2520color%2520combinations%252C%2520challenging%2520VLMs%2520to%2520accurately%2520recognize%2520numerical%250Ainformation%2520embedded%2520in%2520complex%2520visual%2520patterns.%2520We%2520assess%25209%2520VLMs%2520using%2520Yes/No%250Aand%2520open-ended%2520prompts%2520and%2520compare%2520their%2520performance%2520with%2520human%2520participants.%250AOur%2520experiments%2520reveal%2520limitations%2520in%2520the%2520models%2527%2520ability%2520to%2520interpret%2520numbers%250Ain%2520adversarial%2520contexts%252C%2520highlighting%2520prevalent%2520hallucination%2520issues.%2520These%250Afindings%2520underscore%2520the%2520need%2520to%2520improve%2520the%2520robustness%2520of%2520VLMs%2520in%2520complex%250Avisual%2520environments.%2520ColorBlindnessEval%2520serves%2520as%2520a%2520valuable%2520tool%2520for%250Abenchmarking%2520and%2520improving%2520the%2520reliability%2520of%2520VLMs%2520in%2520real-world%2520applications%250Awhere%2520accuracy%2520is%2520critical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColorBlindnessEval%3A%20Can%20Vision-Language%20Models%20Pass%20Color%20Blindness%0A%20%20Tests%3F&entry.906535625=Zijian%20Ling%20and%20Han%20Zhang%20and%20Yazhuo%20Zhou%20and%20Jiahao%20Cui&entry.1292438233=%20%20This%20paper%20presents%20ColorBlindnessEval%2C%20a%20novel%20benchmark%20designed%20to%0Aevaluate%20the%20robustness%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20visually%0Aadversarial%20scenarios%20inspired%20by%20the%20Ishihara%20color%20blindness%20test.%20Our%0Adataset%20comprises%20500%20Ishihara-like%20images%20featuring%20numbers%20from%200%20to%2099%20with%0Avarying%20color%20combinations%2C%20challenging%20VLMs%20to%20accurately%20recognize%20numerical%0Ainformation%20embedded%20in%20complex%20visual%20patterns.%20We%20assess%209%20VLMs%20using%20Yes/No%0Aand%20open-ended%20prompts%20and%20compare%20their%20performance%20with%20human%20participants.%0AOur%20experiments%20reveal%20limitations%20in%20the%20models%27%20ability%20to%20interpret%20numbers%0Ain%20adversarial%20contexts%2C%20highlighting%20prevalent%20hallucination%20issues.%20These%0Afindings%20underscore%20the%20need%20to%20improve%20the%20robustness%20of%20VLMs%20in%20complex%0Avisual%20environments.%20ColorBlindnessEval%20serves%20as%20a%20valuable%20tool%20for%0Abenchmarking%20and%20improving%20the%20reliability%20of%20VLMs%20in%20real-world%20applications%0Awhere%20accuracy%20is%20critical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19070v1&entry.124074799=Read"},
{"title": "The Transparent Earth: A Multimodal Foundation Model for the Earth's\n  Subsurface", "author": "Arnab Mazumder and Javier E. Santos and Noah Hobbs and Mohamed Mehana and Daniel O'Malley", "abstract": "  We present the Transparent Earth, a transformer-based architecture for\nreconstructing subsurface properties from heterogeneous datasets that vary in\nsparsity, resolution, and modality, where each modality represents a distinct\ntype of observation (e.g., stress angle, mantle temperature, tectonic plate\ntype). The model incorporates positional encodings of observations together\nwith modality encodings, derived from a text embedding model applied to a\ndescription of each modality. This design enables the model to scale to an\narbitrary number of modalities, making it straightforward to add new ones not\nconsidered in the initial design. We currently include eight modalities\nspanning directional angles, categorical classes, and continuous properties\nsuch as temperature and thickness. These capabilities support in-context\nlearning, enabling the model to generate predictions either with no inputs or\nwith an arbitrary number of additional observations from any subset of\nmodalities. On validation data, this reduces errors in predicting stress angle\nby more than a factor of three. The proposed architecture is scalable and\ndemonstrates improved performance with increased parameters. Together, these\nadvances make the Transparent Earth an initial foundation model for the Earth's\nsubsurface that ultimately aims to predict any subsurface property anywhere on\nEarth.\n", "link": "http://arxiv.org/abs/2509.02783v2", "date": "2025-09-23", "relevancy": 2.2082, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Transparent%20Earth%3A%20A%20Multimodal%20Foundation%20Model%20for%20the%20Earth%27s%0A%20%20Subsurface&body=Title%3A%20The%20Transparent%20Earth%3A%20A%20Multimodal%20Foundation%20Model%20for%20the%20Earth%27s%0A%20%20Subsurface%0AAuthor%3A%20Arnab%20Mazumder%20and%20Javier%20E.%20Santos%20and%20Noah%20Hobbs%20and%20Mohamed%20Mehana%20and%20Daniel%20O%27Malley%0AAbstract%3A%20%20%20We%20present%20the%20Transparent%20Earth%2C%20a%20transformer-based%20architecture%20for%0Areconstructing%20subsurface%20properties%20from%20heterogeneous%20datasets%20that%20vary%20in%0Asparsity%2C%20resolution%2C%20and%20modality%2C%20where%20each%20modality%20represents%20a%20distinct%0Atype%20of%20observation%20%28e.g.%2C%20stress%20angle%2C%20mantle%20temperature%2C%20tectonic%20plate%0Atype%29.%20The%20model%20incorporates%20positional%20encodings%20of%20observations%20together%0Awith%20modality%20encodings%2C%20derived%20from%20a%20text%20embedding%20model%20applied%20to%20a%0Adescription%20of%20each%20modality.%20This%20design%20enables%20the%20model%20to%20scale%20to%20an%0Aarbitrary%20number%20of%20modalities%2C%20making%20it%20straightforward%20to%20add%20new%20ones%20not%0Aconsidered%20in%20the%20initial%20design.%20We%20currently%20include%20eight%20modalities%0Aspanning%20directional%20angles%2C%20categorical%20classes%2C%20and%20continuous%20properties%0Asuch%20as%20temperature%20and%20thickness.%20These%20capabilities%20support%20in-context%0Alearning%2C%20enabling%20the%20model%20to%20generate%20predictions%20either%20with%20no%20inputs%20or%0Awith%20an%20arbitrary%20number%20of%20additional%20observations%20from%20any%20subset%20of%0Amodalities.%20On%20validation%20data%2C%20this%20reduces%20errors%20in%20predicting%20stress%20angle%0Aby%20more%20than%20a%20factor%20of%20three.%20The%20proposed%20architecture%20is%20scalable%20and%0Ademonstrates%20improved%20performance%20with%20increased%20parameters.%20Together%2C%20these%0Aadvances%20make%20the%20Transparent%20Earth%20an%20initial%20foundation%20model%20for%20the%20Earth%27s%0Asubsurface%20that%20ultimately%20aims%20to%20predict%20any%20subsurface%20property%20anywhere%20on%0AEarth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Transparent%2520Earth%253A%2520A%2520Multimodal%2520Foundation%2520Model%2520for%2520the%2520Earth%2527s%250A%2520%2520Subsurface%26entry.906535625%3DArnab%2520Mazumder%2520and%2520Javier%2520E.%2520Santos%2520and%2520Noah%2520Hobbs%2520and%2520Mohamed%2520Mehana%2520and%2520Daniel%2520O%2527Malley%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520Transparent%2520Earth%252C%2520a%2520transformer-based%2520architecture%2520for%250Areconstructing%2520subsurface%2520properties%2520from%2520heterogeneous%2520datasets%2520that%2520vary%2520in%250Asparsity%252C%2520resolution%252C%2520and%2520modality%252C%2520where%2520each%2520modality%2520represents%2520a%2520distinct%250Atype%2520of%2520observation%2520%2528e.g.%252C%2520stress%2520angle%252C%2520mantle%2520temperature%252C%2520tectonic%2520plate%250Atype%2529.%2520The%2520model%2520incorporates%2520positional%2520encodings%2520of%2520observations%2520together%250Awith%2520modality%2520encodings%252C%2520derived%2520from%2520a%2520text%2520embedding%2520model%2520applied%2520to%2520a%250Adescription%2520of%2520each%2520modality.%2520This%2520design%2520enables%2520the%2520model%2520to%2520scale%2520to%2520an%250Aarbitrary%2520number%2520of%2520modalities%252C%2520making%2520it%2520straightforward%2520to%2520add%2520new%2520ones%2520not%250Aconsidered%2520in%2520the%2520initial%2520design.%2520We%2520currently%2520include%2520eight%2520modalities%250Aspanning%2520directional%2520angles%252C%2520categorical%2520classes%252C%2520and%2520continuous%2520properties%250Asuch%2520as%2520temperature%2520and%2520thickness.%2520These%2520capabilities%2520support%2520in-context%250Alearning%252C%2520enabling%2520the%2520model%2520to%2520generate%2520predictions%2520either%2520with%2520no%2520inputs%2520or%250Awith%2520an%2520arbitrary%2520number%2520of%2520additional%2520observations%2520from%2520any%2520subset%2520of%250Amodalities.%2520On%2520validation%2520data%252C%2520this%2520reduces%2520errors%2520in%2520predicting%2520stress%2520angle%250Aby%2520more%2520than%2520a%2520factor%2520of%2520three.%2520The%2520proposed%2520architecture%2520is%2520scalable%2520and%250Ademonstrates%2520improved%2520performance%2520with%2520increased%2520parameters.%2520Together%252C%2520these%250Aadvances%2520make%2520the%2520Transparent%2520Earth%2520an%2520initial%2520foundation%2520model%2520for%2520the%2520Earth%2527s%250Asubsurface%2520that%2520ultimately%2520aims%2520to%2520predict%2520any%2520subsurface%2520property%2520anywhere%2520on%250AEarth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Transparent%20Earth%3A%20A%20Multimodal%20Foundation%20Model%20for%20the%20Earth%27s%0A%20%20Subsurface&entry.906535625=Arnab%20Mazumder%20and%20Javier%20E.%20Santos%20and%20Noah%20Hobbs%20and%20Mohamed%20Mehana%20and%20Daniel%20O%27Malley&entry.1292438233=%20%20We%20present%20the%20Transparent%20Earth%2C%20a%20transformer-based%20architecture%20for%0Areconstructing%20subsurface%20properties%20from%20heterogeneous%20datasets%20that%20vary%20in%0Asparsity%2C%20resolution%2C%20and%20modality%2C%20where%20each%20modality%20represents%20a%20distinct%0Atype%20of%20observation%20%28e.g.%2C%20stress%20angle%2C%20mantle%20temperature%2C%20tectonic%20plate%0Atype%29.%20The%20model%20incorporates%20positional%20encodings%20of%20observations%20together%0Awith%20modality%20encodings%2C%20derived%20from%20a%20text%20embedding%20model%20applied%20to%20a%0Adescription%20of%20each%20modality.%20This%20design%20enables%20the%20model%20to%20scale%20to%20an%0Aarbitrary%20number%20of%20modalities%2C%20making%20it%20straightforward%20to%20add%20new%20ones%20not%0Aconsidered%20in%20the%20initial%20design.%20We%20currently%20include%20eight%20modalities%0Aspanning%20directional%20angles%2C%20categorical%20classes%2C%20and%20continuous%20properties%0Asuch%20as%20temperature%20and%20thickness.%20These%20capabilities%20support%20in-context%0Alearning%2C%20enabling%20the%20model%20to%20generate%20predictions%20either%20with%20no%20inputs%20or%0Awith%20an%20arbitrary%20number%20of%20additional%20observations%20from%20any%20subset%20of%0Amodalities.%20On%20validation%20data%2C%20this%20reduces%20errors%20in%20predicting%20stress%20angle%0Aby%20more%20than%20a%20factor%20of%20three.%20The%20proposed%20architecture%20is%20scalable%20and%0Ademonstrates%20improved%20performance%20with%20increased%20parameters.%20Together%2C%20these%0Aadvances%20make%20the%20Transparent%20Earth%20an%20initial%20foundation%20model%20for%20the%20Earth%27s%0Asubsurface%20that%20ultimately%20aims%20to%20predict%20any%20subsurface%20property%20anywhere%20on%0AEarth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02783v2&entry.124074799=Read"},
{"title": "Category-Level Object Shape and Pose Estimation in Less Than a\n  Millisecond", "author": "Lorenzo Shaikewitz and Tim Nguyen and Luca Carlone", "abstract": "  Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.\n", "link": "http://arxiv.org/abs/2509.18979v1", "date": "2025-09-23", "relevancy": 2.2045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5675}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5494}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Category-Level%20Object%20Shape%20and%20Pose%20Estimation%20in%20Less%20Than%20a%0A%20%20Millisecond&body=Title%3A%20Category-Level%20Object%20Shape%20and%20Pose%20Estimation%20in%20Less%20Than%20a%0A%20%20Millisecond%0AAuthor%3A%20Lorenzo%20Shaikewitz%20and%20Tim%20Nguyen%20and%20Luca%20Carlone%0AAbstract%3A%20%20%20Object%20shape%20and%20pose%20estimation%20is%20a%20foundational%20robotics%20problem%2C%0Asupporting%20tasks%20from%20manipulation%20to%20scene%20understanding%20and%20navigation.%20We%0Apresent%20a%20fast%20local%20solver%20for%20shape%20and%20pose%20estimation%20which%20requires%20only%0Acategory-level%20object%20priors%20and%20admits%20an%20efficient%20certificate%20of%20global%0Aoptimality.%20Given%20an%20RGB-D%20image%20of%20an%20object%2C%20we%20use%20a%20learned%20front-end%20to%0Adetect%20sparse%2C%20category-level%20semantic%20keypoints%20on%20the%20target%20object.%20We%0Arepresent%20the%20target%20object%27s%20unknown%20shape%20using%20a%20linear%20active%20shape%20model%0Aand%20pose%20a%20maximum%20a%20posteriori%20optimization%20problem%20to%20solve%20for%20position%2C%0Aorientation%2C%20and%20shape%20simultaneously.%20Expressed%20in%20unit%20quaternions%2C%20this%0Aproblem%20admits%20first-order%20optimality%20conditions%20in%20the%20form%20of%20an%20eigenvalue%0Aproblem%20with%20eigenvector%20nonlinearities.%20Our%20primary%20contribution%20is%20to%20solve%0Athis%20problem%20efficiently%20with%20self-consistent%20field%20iteration%2C%20which%20only%0Arequires%20computing%20a%204-by-4%20matrix%20and%20finding%20its%20minimum%20eigenvalue-vector%0Apair%20at%20each%20iterate.%20Solving%20a%20linear%20system%20for%20the%20corresponding%20Lagrange%0Amultipliers%20gives%20a%20simple%20global%20optimality%20certificate.%20One%20iteration%20of%20our%0Asolver%20runs%20in%20about%20100%20microseconds%2C%20enabling%20fast%20outlier%20rejection.%20We%20test%0Aour%20method%20on%20synthetic%20data%20and%20a%20variety%20of%20real-world%20settings%2C%20including%0Atwo%20public%20datasets%20and%20a%20drone%20tracking%20scenario.%20Code%20is%20released%20at%0Ahttps%3A//github.com/MIT-SPARK/Fast-ShapeAndPose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategory-Level%2520Object%2520Shape%2520and%2520Pose%2520Estimation%2520in%2520Less%2520Than%2520a%250A%2520%2520Millisecond%26entry.906535625%3DLorenzo%2520Shaikewitz%2520and%2520Tim%2520Nguyen%2520and%2520Luca%2520Carlone%26entry.1292438233%3D%2520%2520Object%2520shape%2520and%2520pose%2520estimation%2520is%2520a%2520foundational%2520robotics%2520problem%252C%250Asupporting%2520tasks%2520from%2520manipulation%2520to%2520scene%2520understanding%2520and%2520navigation.%2520We%250Apresent%2520a%2520fast%2520local%2520solver%2520for%2520shape%2520and%2520pose%2520estimation%2520which%2520requires%2520only%250Acategory-level%2520object%2520priors%2520and%2520admits%2520an%2520efficient%2520certificate%2520of%2520global%250Aoptimality.%2520Given%2520an%2520RGB-D%2520image%2520of%2520an%2520object%252C%2520we%2520use%2520a%2520learned%2520front-end%2520to%250Adetect%2520sparse%252C%2520category-level%2520semantic%2520keypoints%2520on%2520the%2520target%2520object.%2520We%250Arepresent%2520the%2520target%2520object%2527s%2520unknown%2520shape%2520using%2520a%2520linear%2520active%2520shape%2520model%250Aand%2520pose%2520a%2520maximum%2520a%2520posteriori%2520optimization%2520problem%2520to%2520solve%2520for%2520position%252C%250Aorientation%252C%2520and%2520shape%2520simultaneously.%2520Expressed%2520in%2520unit%2520quaternions%252C%2520this%250Aproblem%2520admits%2520first-order%2520optimality%2520conditions%2520in%2520the%2520form%2520of%2520an%2520eigenvalue%250Aproblem%2520with%2520eigenvector%2520nonlinearities.%2520Our%2520primary%2520contribution%2520is%2520to%2520solve%250Athis%2520problem%2520efficiently%2520with%2520self-consistent%2520field%2520iteration%252C%2520which%2520only%250Arequires%2520computing%2520a%25204-by-4%2520matrix%2520and%2520finding%2520its%2520minimum%2520eigenvalue-vector%250Apair%2520at%2520each%2520iterate.%2520Solving%2520a%2520linear%2520system%2520for%2520the%2520corresponding%2520Lagrange%250Amultipliers%2520gives%2520a%2520simple%2520global%2520optimality%2520certificate.%2520One%2520iteration%2520of%2520our%250Asolver%2520runs%2520in%2520about%2520100%2520microseconds%252C%2520enabling%2520fast%2520outlier%2520rejection.%2520We%2520test%250Aour%2520method%2520on%2520synthetic%2520data%2520and%2520a%2520variety%2520of%2520real-world%2520settings%252C%2520including%250Atwo%2520public%2520datasets%2520and%2520a%2520drone%2520tracking%2520scenario.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/MIT-SPARK/Fast-ShapeAndPose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Category-Level%20Object%20Shape%20and%20Pose%20Estimation%20in%20Less%20Than%20a%0A%20%20Millisecond&entry.906535625=Lorenzo%20Shaikewitz%20and%20Tim%20Nguyen%20and%20Luca%20Carlone&entry.1292438233=%20%20Object%20shape%20and%20pose%20estimation%20is%20a%20foundational%20robotics%20problem%2C%0Asupporting%20tasks%20from%20manipulation%20to%20scene%20understanding%20and%20navigation.%20We%0Apresent%20a%20fast%20local%20solver%20for%20shape%20and%20pose%20estimation%20which%20requires%20only%0Acategory-level%20object%20priors%20and%20admits%20an%20efficient%20certificate%20of%20global%0Aoptimality.%20Given%20an%20RGB-D%20image%20of%20an%20object%2C%20we%20use%20a%20learned%20front-end%20to%0Adetect%20sparse%2C%20category-level%20semantic%20keypoints%20on%20the%20target%20object.%20We%0Arepresent%20the%20target%20object%27s%20unknown%20shape%20using%20a%20linear%20active%20shape%20model%0Aand%20pose%20a%20maximum%20a%20posteriori%20optimization%20problem%20to%20solve%20for%20position%2C%0Aorientation%2C%20and%20shape%20simultaneously.%20Expressed%20in%20unit%20quaternions%2C%20this%0Aproblem%20admits%20first-order%20optimality%20conditions%20in%20the%20form%20of%20an%20eigenvalue%0Aproblem%20with%20eigenvector%20nonlinearities.%20Our%20primary%20contribution%20is%20to%20solve%0Athis%20problem%20efficiently%20with%20self-consistent%20field%20iteration%2C%20which%20only%0Arequires%20computing%20a%204-by-4%20matrix%20and%20finding%20its%20minimum%20eigenvalue-vector%0Apair%20at%20each%20iterate.%20Solving%20a%20linear%20system%20for%20the%20corresponding%20Lagrange%0Amultipliers%20gives%20a%20simple%20global%20optimality%20certificate.%20One%20iteration%20of%20our%0Asolver%20runs%20in%20about%20100%20microseconds%2C%20enabling%20fast%20outlier%20rejection.%20We%20test%0Aour%20method%20on%20synthetic%20data%20and%20a%20variety%20of%20real-world%20settings%2C%20including%0Atwo%20public%20datasets%20and%20a%20drone%20tracking%20scenario.%20Code%20is%20released%20at%0Ahttps%3A//github.com/MIT-SPARK/Fast-ShapeAndPose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18979v1&entry.124074799=Read"},
{"title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and\n  Dynamic Early-Exit", "author": "Maurf Hassan and Steven Davy and Muhammad Zawish and Owais Bin Zuber and Nouman Ashraf", "abstract": "  Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments.\n", "link": "http://arxiv.org/abs/2509.19156v1", "date": "2025-09-23", "relevancy": 2.1977, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5951}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5776}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuCODEX%3A%20Edge-Cloud%20Co-Inference%20with%20Spike-Driven%20Compression%20and%0A%20%20Dynamic%20Early-Exit&body=Title%3A%20NeuCODEX%3A%20Edge-Cloud%20Co-Inference%20with%20Spike-Driven%20Compression%20and%0A%20%20Dynamic%20Early-Exit%0AAuthor%3A%20Maurf%20Hassan%20and%20Steven%20Davy%20and%20Muhammad%20Zawish%20and%20Owais%20Bin%20Zuber%20and%20Nouman%20Ashraf%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20offer%20significant%20potential%20for%20enabling%0Aenergy-efficient%20intelligence%20at%20the%20edge.%20However%2C%20performing%20full%20SNN%0Ainference%20at%20the%20edge%20can%20be%20challenging%20due%20to%20the%20latency%20and%20energy%0Aconstraints%20arising%20from%20fixed%20and%20high%20timestep%20overheads.%20Edge-cloud%0Aco-inference%20systems%20present%20a%20promising%20solution%2C%20but%20their%20deployment%20is%0Aoften%20hindered%20by%20high%20latency%20and%20feature%20transmission%20costs.%20To%20address%20these%0Aissues%2C%20we%20introduce%20NeuCODEX%2C%20a%20neuromorphic%20co-inference%20architecture%20that%0Ajointly%20optimizes%20both%20spatial%20and%20temporal%20redundancy.%20NeuCODEX%20incorporates%20a%0Alearned%20spike-driven%20compression%20module%20to%20reduce%20data%20transmission%20and%20employs%0Aa%20dynamic%20early-exit%20mechanism%20to%20adaptively%20terminate%20inference%20based%20on%0Aoutput%20confidence.%20We%20evaluated%20NeuCODEX%20on%20both%20static%20images%20%28CIFAR10%20and%0ACaltech%29%20and%20neuromorphic%20event%20streams%20%28CIFAR10-DVS%20and%20N-Caltech%29.%20To%0Ademonstrate%20practicality%2C%20we%20prototyped%20NeuCODEX%20on%20ResNet-18%20and%20VGG-16%0Abackbones%20in%20a%20real%20edge-to-cloud%20testbed.%20Our%20proposed%20system%20reduces%20data%0Atransfer%20by%20up%20to%202048x%20and%20edge%20energy%20consumption%20by%20over%2090%25%2C%20while%20reducing%0Aend-to-end%20latency%20by%20up%20to%203x%20compared%20to%20edge-only%20inference%2C%20all%20with%20a%0Anegligible%20accuracy%20drop%20of%20less%20than%202%25.%20In%20doing%20so%2C%20NeuCODEX%20enables%0Apractical%2C%20high-performance%20SNN%20deployment%20in%20resource-constrained%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuCODEX%253A%2520Edge-Cloud%2520Co-Inference%2520with%2520Spike-Driven%2520Compression%2520and%250A%2520%2520Dynamic%2520Early-Exit%26entry.906535625%3DMaurf%2520Hassan%2520and%2520Steven%2520Davy%2520and%2520Muhammad%2520Zawish%2520and%2520Owais%2520Bin%2520Zuber%2520and%2520Nouman%2520Ashraf%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520offer%2520significant%2520potential%2520for%2520enabling%250Aenergy-efficient%2520intelligence%2520at%2520the%2520edge.%2520However%252C%2520performing%2520full%2520SNN%250Ainference%2520at%2520the%2520edge%2520can%2520be%2520challenging%2520due%2520to%2520the%2520latency%2520and%2520energy%250Aconstraints%2520arising%2520from%2520fixed%2520and%2520high%2520timestep%2520overheads.%2520Edge-cloud%250Aco-inference%2520systems%2520present%2520a%2520promising%2520solution%252C%2520but%2520their%2520deployment%2520is%250Aoften%2520hindered%2520by%2520high%2520latency%2520and%2520feature%2520transmission%2520costs.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520NeuCODEX%252C%2520a%2520neuromorphic%2520co-inference%2520architecture%2520that%250Ajointly%2520optimizes%2520both%2520spatial%2520and%2520temporal%2520redundancy.%2520NeuCODEX%2520incorporates%2520a%250Alearned%2520spike-driven%2520compression%2520module%2520to%2520reduce%2520data%2520transmission%2520and%2520employs%250Aa%2520dynamic%2520early-exit%2520mechanism%2520to%2520adaptively%2520terminate%2520inference%2520based%2520on%250Aoutput%2520confidence.%2520We%2520evaluated%2520NeuCODEX%2520on%2520both%2520static%2520images%2520%2528CIFAR10%2520and%250ACaltech%2529%2520and%2520neuromorphic%2520event%2520streams%2520%2528CIFAR10-DVS%2520and%2520N-Caltech%2529.%2520To%250Ademonstrate%2520practicality%252C%2520we%2520prototyped%2520NeuCODEX%2520on%2520ResNet-18%2520and%2520VGG-16%250Abackbones%2520in%2520a%2520real%2520edge-to-cloud%2520testbed.%2520Our%2520proposed%2520system%2520reduces%2520data%250Atransfer%2520by%2520up%2520to%25202048x%2520and%2520edge%2520energy%2520consumption%2520by%2520over%252090%2525%252C%2520while%2520reducing%250Aend-to-end%2520latency%2520by%2520up%2520to%25203x%2520compared%2520to%2520edge-only%2520inference%252C%2520all%2520with%2520a%250Anegligible%2520accuracy%2520drop%2520of%2520less%2520than%25202%2525.%2520In%2520doing%2520so%252C%2520NeuCODEX%2520enables%250Apractical%252C%2520high-performance%2520SNN%2520deployment%2520in%2520resource-constrained%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuCODEX%3A%20Edge-Cloud%20Co-Inference%20with%20Spike-Driven%20Compression%20and%0A%20%20Dynamic%20Early-Exit&entry.906535625=Maurf%20Hassan%20and%20Steven%20Davy%20and%20Muhammad%20Zawish%20and%20Owais%20Bin%20Zuber%20and%20Nouman%20Ashraf&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20offer%20significant%20potential%20for%20enabling%0Aenergy-efficient%20intelligence%20at%20the%20edge.%20However%2C%20performing%20full%20SNN%0Ainference%20at%20the%20edge%20can%20be%20challenging%20due%20to%20the%20latency%20and%20energy%0Aconstraints%20arising%20from%20fixed%20and%20high%20timestep%20overheads.%20Edge-cloud%0Aco-inference%20systems%20present%20a%20promising%20solution%2C%20but%20their%20deployment%20is%0Aoften%20hindered%20by%20high%20latency%20and%20feature%20transmission%20costs.%20To%20address%20these%0Aissues%2C%20we%20introduce%20NeuCODEX%2C%20a%20neuromorphic%20co-inference%20architecture%20that%0Ajointly%20optimizes%20both%20spatial%20and%20temporal%20redundancy.%20NeuCODEX%20incorporates%20a%0Alearned%20spike-driven%20compression%20module%20to%20reduce%20data%20transmission%20and%20employs%0Aa%20dynamic%20early-exit%20mechanism%20to%20adaptively%20terminate%20inference%20based%20on%0Aoutput%20confidence.%20We%20evaluated%20NeuCODEX%20on%20both%20static%20images%20%28CIFAR10%20and%0ACaltech%29%20and%20neuromorphic%20event%20streams%20%28CIFAR10-DVS%20and%20N-Caltech%29.%20To%0Ademonstrate%20practicality%2C%20we%20prototyped%20NeuCODEX%20on%20ResNet-18%20and%20VGG-16%0Abackbones%20in%20a%20real%20edge-to-cloud%20testbed.%20Our%20proposed%20system%20reduces%20data%0Atransfer%20by%20up%20to%202048x%20and%20edge%20energy%20consumption%20by%20over%2090%25%2C%20while%20reducing%0Aend-to-end%20latency%20by%20up%20to%203x%20compared%20to%20edge-only%20inference%2C%20all%20with%20a%0Anegligible%20accuracy%20drop%20of%20less%20than%202%25.%20In%20doing%20so%2C%20NeuCODEX%20enables%0Apractical%2C%20high-performance%20SNN%20deployment%20in%20resource-constrained%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19156v1&entry.124074799=Read"},
{"title": "A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and\n  Opportunities for Further Improvement", "author": "Tiany Peng and George Gui and Daniel J. Merlau and Grace Jiarui Fan and Malek Ben Sliman and Melanie Brucks and Eric J. Johnson and Vicki Morwitz and Abdullah Althenayyan and Silvia Bellezza and Dante Donati and Hortense Fong and Elizabeth Friedman and Ariana Guevara and Mohamed Hussein and Kinshuk Jerath and Bruce Kogut and Kristen Lane and Hannah Li and Patryk Perkowski and Oded Netzer and Olivier Toubia", "abstract": "  Do \"digital twins\" capture individual responses in surveys and experiments?\nWe run 19 pre-registered studies on a national U.S. panel and their LLM-powered\ndigital twins (constructed based on previously-collected extensive\nindividual-level data) and compare twin and human answers across 164 outcomes.\nThe correlation between twin and human answers is modest (approximately 0.2 on\naverage) and twin responses are less variable than human responses. While\nconstructing digital twins based on rich individual-level data improves our\nability to capture heterogeneity across participants and predict relative\ndifferences between them, it does not substantially improve our ability to\npredict the exact answers given by specific participants or enhance predictions\nof population means. Twin performance varies by domain and is higher among more\neducated, higher-income, and ideologically moderate participants. These results\nsuggest current digital twins can capture some degree of relative differences\nbut are unreliable for individual-level predictions and sample mean and\nvariance estimation, underscoring the need for careful validation before use.\nOur data and code are publicly available for researchers and practitioners\ninterested in optimizing digital twin pipelines.\n", "link": "http://arxiv.org/abs/2509.19088v1", "date": "2025-09-23", "relevancy": 2.1923, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4416}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mega-Study%20of%20Digital%20Twins%20Reveals%20Strengths%2C%20Weaknesses%20and%0A%20%20Opportunities%20for%20Further%20Improvement&body=Title%3A%20A%20Mega-Study%20of%20Digital%20Twins%20Reveals%20Strengths%2C%20Weaknesses%20and%0A%20%20Opportunities%20for%20Further%20Improvement%0AAuthor%3A%20Tiany%20Peng%20and%20George%20Gui%20and%20Daniel%20J.%20Merlau%20and%20Grace%20Jiarui%20Fan%20and%20Malek%20Ben%20Sliman%20and%20Melanie%20Brucks%20and%20Eric%20J.%20Johnson%20and%20Vicki%20Morwitz%20and%20Abdullah%20Althenayyan%20and%20Silvia%20Bellezza%20and%20Dante%20Donati%20and%20Hortense%20Fong%20and%20Elizabeth%20Friedman%20and%20Ariana%20Guevara%20and%20Mohamed%20Hussein%20and%20Kinshuk%20Jerath%20and%20Bruce%20Kogut%20and%20Kristen%20Lane%20and%20Hannah%20Li%20and%20Patryk%20Perkowski%20and%20Oded%20Netzer%20and%20Olivier%20Toubia%0AAbstract%3A%20%20%20Do%20%22digital%20twins%22%20capture%20individual%20responses%20in%20surveys%20and%20experiments%3F%0AWe%20run%2019%20pre-registered%20studies%20on%20a%20national%20U.S.%20panel%20and%20their%20LLM-powered%0Adigital%20twins%20%28constructed%20based%20on%20previously-collected%20extensive%0Aindividual-level%20data%29%20and%20compare%20twin%20and%20human%20answers%20across%20164%20outcomes.%0AThe%20correlation%20between%20twin%20and%20human%20answers%20is%20modest%20%28approximately%200.2%20on%0Aaverage%29%20and%20twin%20responses%20are%20less%20variable%20than%20human%20responses.%20While%0Aconstructing%20digital%20twins%20based%20on%20rich%20individual-level%20data%20improves%20our%0Aability%20to%20capture%20heterogeneity%20across%20participants%20and%20predict%20relative%0Adifferences%20between%20them%2C%20it%20does%20not%20substantially%20improve%20our%20ability%20to%0Apredict%20the%20exact%20answers%20given%20by%20specific%20participants%20or%20enhance%20predictions%0Aof%20population%20means.%20Twin%20performance%20varies%20by%20domain%20and%20is%20higher%20among%20more%0Aeducated%2C%20higher-income%2C%20and%20ideologically%20moderate%20participants.%20These%20results%0Asuggest%20current%20digital%20twins%20can%20capture%20some%20degree%20of%20relative%20differences%0Abut%20are%20unreliable%20for%20individual-level%20predictions%20and%20sample%20mean%20and%0Avariance%20estimation%2C%20underscoring%20the%20need%20for%20careful%20validation%20before%20use.%0AOur%20data%20and%20code%20are%20publicly%20available%20for%20researchers%20and%20practitioners%0Ainterested%20in%20optimizing%20digital%20twin%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mega-Study%2520of%2520Digital%2520Twins%2520Reveals%2520Strengths%252C%2520Weaknesses%2520and%250A%2520%2520Opportunities%2520for%2520Further%2520Improvement%26entry.906535625%3DTiany%2520Peng%2520and%2520George%2520Gui%2520and%2520Daniel%2520J.%2520Merlau%2520and%2520Grace%2520Jiarui%2520Fan%2520and%2520Malek%2520Ben%2520Sliman%2520and%2520Melanie%2520Brucks%2520and%2520Eric%2520J.%2520Johnson%2520and%2520Vicki%2520Morwitz%2520and%2520Abdullah%2520Althenayyan%2520and%2520Silvia%2520Bellezza%2520and%2520Dante%2520Donati%2520and%2520Hortense%2520Fong%2520and%2520Elizabeth%2520Friedman%2520and%2520Ariana%2520Guevara%2520and%2520Mohamed%2520Hussein%2520and%2520Kinshuk%2520Jerath%2520and%2520Bruce%2520Kogut%2520and%2520Kristen%2520Lane%2520and%2520Hannah%2520Li%2520and%2520Patryk%2520Perkowski%2520and%2520Oded%2520Netzer%2520and%2520Olivier%2520Toubia%26entry.1292438233%3D%2520%2520Do%2520%2522digital%2520twins%2522%2520capture%2520individual%2520responses%2520in%2520surveys%2520and%2520experiments%253F%250AWe%2520run%252019%2520pre-registered%2520studies%2520on%2520a%2520national%2520U.S.%2520panel%2520and%2520their%2520LLM-powered%250Adigital%2520twins%2520%2528constructed%2520based%2520on%2520previously-collected%2520extensive%250Aindividual-level%2520data%2529%2520and%2520compare%2520twin%2520and%2520human%2520answers%2520across%2520164%2520outcomes.%250AThe%2520correlation%2520between%2520twin%2520and%2520human%2520answers%2520is%2520modest%2520%2528approximately%25200.2%2520on%250Aaverage%2529%2520and%2520twin%2520responses%2520are%2520less%2520variable%2520than%2520human%2520responses.%2520While%250Aconstructing%2520digital%2520twins%2520based%2520on%2520rich%2520individual-level%2520data%2520improves%2520our%250Aability%2520to%2520capture%2520heterogeneity%2520across%2520participants%2520and%2520predict%2520relative%250Adifferences%2520between%2520them%252C%2520it%2520does%2520not%2520substantially%2520improve%2520our%2520ability%2520to%250Apredict%2520the%2520exact%2520answers%2520given%2520by%2520specific%2520participants%2520or%2520enhance%2520predictions%250Aof%2520population%2520means.%2520Twin%2520performance%2520varies%2520by%2520domain%2520and%2520is%2520higher%2520among%2520more%250Aeducated%252C%2520higher-income%252C%2520and%2520ideologically%2520moderate%2520participants.%2520These%2520results%250Asuggest%2520current%2520digital%2520twins%2520can%2520capture%2520some%2520degree%2520of%2520relative%2520differences%250Abut%2520are%2520unreliable%2520for%2520individual-level%2520predictions%2520and%2520sample%2520mean%2520and%250Avariance%2520estimation%252C%2520underscoring%2520the%2520need%2520for%2520careful%2520validation%2520before%2520use.%250AOur%2520data%2520and%2520code%2520are%2520publicly%2520available%2520for%2520researchers%2520and%2520practitioners%250Ainterested%2520in%2520optimizing%2520digital%2520twin%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mega-Study%20of%20Digital%20Twins%20Reveals%20Strengths%2C%20Weaknesses%20and%0A%20%20Opportunities%20for%20Further%20Improvement&entry.906535625=Tiany%20Peng%20and%20George%20Gui%20and%20Daniel%20J.%20Merlau%20and%20Grace%20Jiarui%20Fan%20and%20Malek%20Ben%20Sliman%20and%20Melanie%20Brucks%20and%20Eric%20J.%20Johnson%20and%20Vicki%20Morwitz%20and%20Abdullah%20Althenayyan%20and%20Silvia%20Bellezza%20and%20Dante%20Donati%20and%20Hortense%20Fong%20and%20Elizabeth%20Friedman%20and%20Ariana%20Guevara%20and%20Mohamed%20Hussein%20and%20Kinshuk%20Jerath%20and%20Bruce%20Kogut%20and%20Kristen%20Lane%20and%20Hannah%20Li%20and%20Patryk%20Perkowski%20and%20Oded%20Netzer%20and%20Olivier%20Toubia&entry.1292438233=%20%20Do%20%22digital%20twins%22%20capture%20individual%20responses%20in%20surveys%20and%20experiments%3F%0AWe%20run%2019%20pre-registered%20studies%20on%20a%20national%20U.S.%20panel%20and%20their%20LLM-powered%0Adigital%20twins%20%28constructed%20based%20on%20previously-collected%20extensive%0Aindividual-level%20data%29%20and%20compare%20twin%20and%20human%20answers%20across%20164%20outcomes.%0AThe%20correlation%20between%20twin%20and%20human%20answers%20is%20modest%20%28approximately%200.2%20on%0Aaverage%29%20and%20twin%20responses%20are%20less%20variable%20than%20human%20responses.%20While%0Aconstructing%20digital%20twins%20based%20on%20rich%20individual-level%20data%20improves%20our%0Aability%20to%20capture%20heterogeneity%20across%20participants%20and%20predict%20relative%0Adifferences%20between%20them%2C%20it%20does%20not%20substantially%20improve%20our%20ability%20to%0Apredict%20the%20exact%20answers%20given%20by%20specific%20participants%20or%20enhance%20predictions%0Aof%20population%20means.%20Twin%20performance%20varies%20by%20domain%20and%20is%20higher%20among%20more%0Aeducated%2C%20higher-income%2C%20and%20ideologically%20moderate%20participants.%20These%20results%0Asuggest%20current%20digital%20twins%20can%20capture%20some%20degree%20of%20relative%20differences%0Abut%20are%20unreliable%20for%20individual-level%20predictions%20and%20sample%20mean%20and%0Avariance%20estimation%2C%20underscoring%20the%20need%20for%20careful%20validation%20before%20use.%0AOur%20data%20and%20code%20are%20publicly%20available%20for%20researchers%20and%20practitioners%0Ainterested%20in%20optimizing%20digital%20twin%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19088v1&entry.124074799=Read"},
{"title": "Exploring Heterophily in Graph-level Tasks", "author": "Qinhan Hou and Yilun Zheng and Xichun Zhang and Sitao Luan and Jing Tang", "abstract": "  While heterophily has been widely studied in node-level tasks, its impact on\ngraph-level tasks remains unclear. We present the first analysis of heterophily\nin graph-level learning, combining theoretical insights with empirical\nvalidation. We first introduce a taxonomy of graph-level labeling schemes, and\nfocus on motif-based tasks within local structure labeling, which is a popular\nlabeling scheme. Using energy-based gradient flow analysis, we reveal a key\ninsight: unlike frequency-dominated regimes in node-level tasks, motif\ndetection requires mixed-frequency dynamics to remain flexible across multiple\nspectral components. Our theory shows that motif objectives are inherently\nmisaligned with global frequency dominance, demanding distinct architectural\nconsiderations. Experiments on synthetic datasets with controlled heterophily\nand real-world molecular property prediction support our findings, showing that\nfrequency-adaptive model outperform frequency-dominated models. This work\nestablishes a new theoretical understanding of heterophily in graph-level\nlearning and offers guidance for designing effective GNN architectures.\n", "link": "http://arxiv.org/abs/2509.18893v1", "date": "2025-09-23", "relevancy": 2.1904, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Heterophily%20in%20Graph-level%20Tasks&body=Title%3A%20Exploring%20Heterophily%20in%20Graph-level%20Tasks%0AAuthor%3A%20Qinhan%20Hou%20and%20Yilun%20Zheng%20and%20Xichun%20Zhang%20and%20Sitao%20Luan%20and%20Jing%20Tang%0AAbstract%3A%20%20%20While%20heterophily%20has%20been%20widely%20studied%20in%20node-level%20tasks%2C%20its%20impact%20on%0Agraph-level%20tasks%20remains%20unclear.%20We%20present%20the%20first%20analysis%20of%20heterophily%0Ain%20graph-level%20learning%2C%20combining%20theoretical%20insights%20with%20empirical%0Avalidation.%20We%20first%20introduce%20a%20taxonomy%20of%20graph-level%20labeling%20schemes%2C%20and%0Afocus%20on%20motif-based%20tasks%20within%20local%20structure%20labeling%2C%20which%20is%20a%20popular%0Alabeling%20scheme.%20Using%20energy-based%20gradient%20flow%20analysis%2C%20we%20reveal%20a%20key%0Ainsight%3A%20unlike%20frequency-dominated%20regimes%20in%20node-level%20tasks%2C%20motif%0Adetection%20requires%20mixed-frequency%20dynamics%20to%20remain%20flexible%20across%20multiple%0Aspectral%20components.%20Our%20theory%20shows%20that%20motif%20objectives%20are%20inherently%0Amisaligned%20with%20global%20frequency%20dominance%2C%20demanding%20distinct%20architectural%0Aconsiderations.%20Experiments%20on%20synthetic%20datasets%20with%20controlled%20heterophily%0Aand%20real-world%20molecular%20property%20prediction%20support%20our%20findings%2C%20showing%20that%0Afrequency-adaptive%20model%20outperform%20frequency-dominated%20models.%20This%20work%0Aestablishes%20a%20new%20theoretical%20understanding%20of%20heterophily%20in%20graph-level%0Alearning%20and%20offers%20guidance%20for%20designing%20effective%20GNN%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Heterophily%2520in%2520Graph-level%2520Tasks%26entry.906535625%3DQinhan%2520Hou%2520and%2520Yilun%2520Zheng%2520and%2520Xichun%2520Zhang%2520and%2520Sitao%2520Luan%2520and%2520Jing%2520Tang%26entry.1292438233%3D%2520%2520While%2520heterophily%2520has%2520been%2520widely%2520studied%2520in%2520node-level%2520tasks%252C%2520its%2520impact%2520on%250Agraph-level%2520tasks%2520remains%2520unclear.%2520We%2520present%2520the%2520first%2520analysis%2520of%2520heterophily%250Ain%2520graph-level%2520learning%252C%2520combining%2520theoretical%2520insights%2520with%2520empirical%250Avalidation.%2520We%2520first%2520introduce%2520a%2520taxonomy%2520of%2520graph-level%2520labeling%2520schemes%252C%2520and%250Afocus%2520on%2520motif-based%2520tasks%2520within%2520local%2520structure%2520labeling%252C%2520which%2520is%2520a%2520popular%250Alabeling%2520scheme.%2520Using%2520energy-based%2520gradient%2520flow%2520analysis%252C%2520we%2520reveal%2520a%2520key%250Ainsight%253A%2520unlike%2520frequency-dominated%2520regimes%2520in%2520node-level%2520tasks%252C%2520motif%250Adetection%2520requires%2520mixed-frequency%2520dynamics%2520to%2520remain%2520flexible%2520across%2520multiple%250Aspectral%2520components.%2520Our%2520theory%2520shows%2520that%2520motif%2520objectives%2520are%2520inherently%250Amisaligned%2520with%2520global%2520frequency%2520dominance%252C%2520demanding%2520distinct%2520architectural%250Aconsiderations.%2520Experiments%2520on%2520synthetic%2520datasets%2520with%2520controlled%2520heterophily%250Aand%2520real-world%2520molecular%2520property%2520prediction%2520support%2520our%2520findings%252C%2520showing%2520that%250Afrequency-adaptive%2520model%2520outperform%2520frequency-dominated%2520models.%2520This%2520work%250Aestablishes%2520a%2520new%2520theoretical%2520understanding%2520of%2520heterophily%2520in%2520graph-level%250Alearning%2520and%2520offers%2520guidance%2520for%2520designing%2520effective%2520GNN%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Heterophily%20in%20Graph-level%20Tasks&entry.906535625=Qinhan%20Hou%20and%20Yilun%20Zheng%20and%20Xichun%20Zhang%20and%20Sitao%20Luan%20and%20Jing%20Tang&entry.1292438233=%20%20While%20heterophily%20has%20been%20widely%20studied%20in%20node-level%20tasks%2C%20its%20impact%20on%0Agraph-level%20tasks%20remains%20unclear.%20We%20present%20the%20first%20analysis%20of%20heterophily%0Ain%20graph-level%20learning%2C%20combining%20theoretical%20insights%20with%20empirical%0Avalidation.%20We%20first%20introduce%20a%20taxonomy%20of%20graph-level%20labeling%20schemes%2C%20and%0Afocus%20on%20motif-based%20tasks%20within%20local%20structure%20labeling%2C%20which%20is%20a%20popular%0Alabeling%20scheme.%20Using%20energy-based%20gradient%20flow%20analysis%2C%20we%20reveal%20a%20key%0Ainsight%3A%20unlike%20frequency-dominated%20regimes%20in%20node-level%20tasks%2C%20motif%0Adetection%20requires%20mixed-frequency%20dynamics%20to%20remain%20flexible%20across%20multiple%0Aspectral%20components.%20Our%20theory%20shows%20that%20motif%20objectives%20are%20inherently%0Amisaligned%20with%20global%20frequency%20dominance%2C%20demanding%20distinct%20architectural%0Aconsiderations.%20Experiments%20on%20synthetic%20datasets%20with%20controlled%20heterophily%0Aand%20real-world%20molecular%20property%20prediction%20support%20our%20findings%2C%20showing%20that%0Afrequency-adaptive%20model%20outperform%20frequency-dominated%20models.%20This%20work%0Aestablishes%20a%20new%20theoretical%20understanding%20of%20heterophily%20in%20graph-level%0Alearning%20and%20offers%20guidance%20for%20designing%20effective%20GNN%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18893v1&entry.124074799=Read"},
{"title": "THFlow: A Temporally Hierarchical Flow Matching Framework for 3D Peptide\n  Design", "author": "Dengdeng Huang and Shikui Tu", "abstract": "  Deep generative models provide a promising approach to de novo 3D peptide\ndesign. Most of them jointly model the distributions of peptide's position,\norientation, and conformation, attempting to simultaneously converge to the\ntarget pocket. However, in the early stage of docking, optimizing\nconformation-only modalities such as rotation and torsion can be physically\nmeaningless, as the peptide is initialized far from the protein pocket and no\ninteraction field is present. We define this problem as the multimodal temporal\ninconsistency problem and claim it is a key factor contributing to low binding\naffinity in generated peptides. To address this challenge, we propose THFlow, a\nnovel flow matching-based multimodal generative model that explicitly models\nthe temporal hierarchy between peptide position and conformation. It employs a\npolynomial based conditional flow to accelerate positional convergence early\non, and later aligns it with rotation and torsion for coordinated conformation\nrefinement under the emerging interaction field. Additionally, we incorporate\ninteraction-related features, such as polarity, to further enhance the model's\nunderstanding of peptide-protein binding. Extensive experiments demonstrate\nthat THFlow outperforms existing methods in generating peptides with superior\nstability, affinity, and diversity, offering an effective and accurate solution\nfor advancing peptide-based therapeutic development.\n", "link": "http://arxiv.org/abs/2502.15855v2", "date": "2025-09-23", "relevancy": 2.1898, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5625}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5386}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20THFlow%3A%20A%20Temporally%20Hierarchical%20Flow%20Matching%20Framework%20for%203D%20Peptide%0A%20%20Design&body=Title%3A%20THFlow%3A%20A%20Temporally%20Hierarchical%20Flow%20Matching%20Framework%20for%203D%20Peptide%0A%20%20Design%0AAuthor%3A%20Dengdeng%20Huang%20and%20Shikui%20Tu%0AAbstract%3A%20%20%20Deep%20generative%20models%20provide%20a%20promising%20approach%20to%20de%20novo%203D%20peptide%0Adesign.%20Most%20of%20them%20jointly%20model%20the%20distributions%20of%20peptide%27s%20position%2C%0Aorientation%2C%20and%20conformation%2C%20attempting%20to%20simultaneously%20converge%20to%20the%0Atarget%20pocket.%20However%2C%20in%20the%20early%20stage%20of%20docking%2C%20optimizing%0Aconformation-only%20modalities%20such%20as%20rotation%20and%20torsion%20can%20be%20physically%0Ameaningless%2C%20as%20the%20peptide%20is%20initialized%20far%20from%20the%20protein%20pocket%20and%20no%0Ainteraction%20field%20is%20present.%20We%20define%20this%20problem%20as%20the%20multimodal%20temporal%0Ainconsistency%20problem%20and%20claim%20it%20is%20a%20key%20factor%20contributing%20to%20low%20binding%0Aaffinity%20in%20generated%20peptides.%20To%20address%20this%20challenge%2C%20we%20propose%20THFlow%2C%20a%0Anovel%20flow%20matching-based%20multimodal%20generative%20model%20that%20explicitly%20models%0Athe%20temporal%20hierarchy%20between%20peptide%20position%20and%20conformation.%20It%20employs%20a%0Apolynomial%20based%20conditional%20flow%20to%20accelerate%20positional%20convergence%20early%0Aon%2C%20and%20later%20aligns%20it%20with%20rotation%20and%20torsion%20for%20coordinated%20conformation%0Arefinement%20under%20the%20emerging%20interaction%20field.%20Additionally%2C%20we%20incorporate%0Ainteraction-related%20features%2C%20such%20as%20polarity%2C%20to%20further%20enhance%20the%20model%27s%0Aunderstanding%20of%20peptide-protein%20binding.%20Extensive%20experiments%20demonstrate%0Athat%20THFlow%20outperforms%20existing%20methods%20in%20generating%20peptides%20with%20superior%0Astability%2C%20affinity%2C%20and%20diversity%2C%20offering%20an%20effective%20and%20accurate%20solution%0Afor%20advancing%20peptide-based%20therapeutic%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTHFlow%253A%2520A%2520Temporally%2520Hierarchical%2520Flow%2520Matching%2520Framework%2520for%25203D%2520Peptide%250A%2520%2520Design%26entry.906535625%3DDengdeng%2520Huang%2520and%2520Shikui%2520Tu%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%2520provide%2520a%2520promising%2520approach%2520to%2520de%2520novo%25203D%2520peptide%250Adesign.%2520Most%2520of%2520them%2520jointly%2520model%2520the%2520distributions%2520of%2520peptide%2527s%2520position%252C%250Aorientation%252C%2520and%2520conformation%252C%2520attempting%2520to%2520simultaneously%2520converge%2520to%2520the%250Atarget%2520pocket.%2520However%252C%2520in%2520the%2520early%2520stage%2520of%2520docking%252C%2520optimizing%250Aconformation-only%2520modalities%2520such%2520as%2520rotation%2520and%2520torsion%2520can%2520be%2520physically%250Ameaningless%252C%2520as%2520the%2520peptide%2520is%2520initialized%2520far%2520from%2520the%2520protein%2520pocket%2520and%2520no%250Ainteraction%2520field%2520is%2520present.%2520We%2520define%2520this%2520problem%2520as%2520the%2520multimodal%2520temporal%250Ainconsistency%2520problem%2520and%2520claim%2520it%2520is%2520a%2520key%2520factor%2520contributing%2520to%2520low%2520binding%250Aaffinity%2520in%2520generated%2520peptides.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520THFlow%252C%2520a%250Anovel%2520flow%2520matching-based%2520multimodal%2520generative%2520model%2520that%2520explicitly%2520models%250Athe%2520temporal%2520hierarchy%2520between%2520peptide%2520position%2520and%2520conformation.%2520It%2520employs%2520a%250Apolynomial%2520based%2520conditional%2520flow%2520to%2520accelerate%2520positional%2520convergence%2520early%250Aon%252C%2520and%2520later%2520aligns%2520it%2520with%2520rotation%2520and%2520torsion%2520for%2520coordinated%2520conformation%250Arefinement%2520under%2520the%2520emerging%2520interaction%2520field.%2520Additionally%252C%2520we%2520incorporate%250Ainteraction-related%2520features%252C%2520such%2520as%2520polarity%252C%2520to%2520further%2520enhance%2520the%2520model%2527s%250Aunderstanding%2520of%2520peptide-protein%2520binding.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520THFlow%2520outperforms%2520existing%2520methods%2520in%2520generating%2520peptides%2520with%2520superior%250Astability%252C%2520affinity%252C%2520and%2520diversity%252C%2520offering%2520an%2520effective%2520and%2520accurate%2520solution%250Afor%2520advancing%2520peptide-based%2520therapeutic%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=THFlow%3A%20A%20Temporally%20Hierarchical%20Flow%20Matching%20Framework%20for%203D%20Peptide%0A%20%20Design&entry.906535625=Dengdeng%20Huang%20and%20Shikui%20Tu&entry.1292438233=%20%20Deep%20generative%20models%20provide%20a%20promising%20approach%20to%20de%20novo%203D%20peptide%0Adesign.%20Most%20of%20them%20jointly%20model%20the%20distributions%20of%20peptide%27s%20position%2C%0Aorientation%2C%20and%20conformation%2C%20attempting%20to%20simultaneously%20converge%20to%20the%0Atarget%20pocket.%20However%2C%20in%20the%20early%20stage%20of%20docking%2C%20optimizing%0Aconformation-only%20modalities%20such%20as%20rotation%20and%20torsion%20can%20be%20physically%0Ameaningless%2C%20as%20the%20peptide%20is%20initialized%20far%20from%20the%20protein%20pocket%20and%20no%0Ainteraction%20field%20is%20present.%20We%20define%20this%20problem%20as%20the%20multimodal%20temporal%0Ainconsistency%20problem%20and%20claim%20it%20is%20a%20key%20factor%20contributing%20to%20low%20binding%0Aaffinity%20in%20generated%20peptides.%20To%20address%20this%20challenge%2C%20we%20propose%20THFlow%2C%20a%0Anovel%20flow%20matching-based%20multimodal%20generative%20model%20that%20explicitly%20models%0Athe%20temporal%20hierarchy%20between%20peptide%20position%20and%20conformation.%20It%20employs%20a%0Apolynomial%20based%20conditional%20flow%20to%20accelerate%20positional%20convergence%20early%0Aon%2C%20and%20later%20aligns%20it%20with%20rotation%20and%20torsion%20for%20coordinated%20conformation%0Arefinement%20under%20the%20emerging%20interaction%20field.%20Additionally%2C%20we%20incorporate%0Ainteraction-related%20features%2C%20such%20as%20polarity%2C%20to%20further%20enhance%20the%20model%27s%0Aunderstanding%20of%20peptide-protein%20binding.%20Extensive%20experiments%20demonstrate%0Athat%20THFlow%20outperforms%20existing%20methods%20in%20generating%20peptides%20with%20superior%0Astability%2C%20affinity%2C%20and%20diversity%2C%20offering%20an%20effective%20and%20accurate%20solution%0Afor%20advancing%20peptide-based%20therapeutic%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15855v2&entry.124074799=Read"},
{"title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language\n  Fusion for Action Generation", "author": "Masato Kobayashi and Thanpimon Buamanee", "abstract": "  We propose Bilateral Control-Based Imitation Learning via Vision-Language\nFusion for Action Generation (Bi-VLA), a novel framework that extends bilateral\ncontrol-based imitation learning to handle more than one task within a single\nmodel. Conventional bilateral control methods exploit joint angle, velocity,\ntorque, and vision for precise manipulation but require task-specific models,\nlimiting their generality. Bi-VLA overcomes this limitation by utilizing robot\njoint angle, velocity, and torque data from leader-follower bilateral control\nwith visual features and natural language instructions through SigLIP and\nFiLM-based fusion. We validated Bi-VLA on two task types: one requiring\nsupplementary language cues and another distinguishable solely by vision.\nReal-robot experiments showed that Bi-VLA successfully interprets\nvision-language combinations and improves task success rates compared to\nconventional bilateral control-based imitation learning. Our Bi-VLA addresses\nthe single-task limitation of prior bilateral approaches and provides empirical\nevidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website:\nhttps://mertcookimg.github.io/bi-vla/\n", "link": "http://arxiv.org/abs/2509.18865v1", "date": "2025-09-23", "relevancy": 2.1885, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bi-VLA%3A%20Bilateral%20Control-Based%20Imitation%20Learning%20via%20Vision-Language%0A%20%20Fusion%20for%20Action%20Generation&body=Title%3A%20Bi-VLA%3A%20Bilateral%20Control-Based%20Imitation%20Learning%20via%20Vision-Language%0A%20%20Fusion%20for%20Action%20Generation%0AAuthor%3A%20Masato%20Kobayashi%20and%20Thanpimon%20Buamanee%0AAbstract%3A%20%20%20We%20propose%20Bilateral%20Control-Based%20Imitation%20Learning%20via%20Vision-Language%0AFusion%20for%20Action%20Generation%20%28Bi-VLA%29%2C%20a%20novel%20framework%20that%20extends%20bilateral%0Acontrol-based%20imitation%20learning%20to%20handle%20more%20than%20one%20task%20within%20a%20single%0Amodel.%20Conventional%20bilateral%20control%20methods%20exploit%20joint%20angle%2C%20velocity%2C%0Atorque%2C%20and%20vision%20for%20precise%20manipulation%20but%20require%20task-specific%20models%2C%0Alimiting%20their%20generality.%20Bi-VLA%20overcomes%20this%20limitation%20by%20utilizing%20robot%0Ajoint%20angle%2C%20velocity%2C%20and%20torque%20data%20from%20leader-follower%20bilateral%20control%0Awith%20visual%20features%20and%20natural%20language%20instructions%20through%20SigLIP%20and%0AFiLM-based%20fusion.%20We%20validated%20Bi-VLA%20on%20two%20task%20types%3A%20one%20requiring%0Asupplementary%20language%20cues%20and%20another%20distinguishable%20solely%20by%20vision.%0AReal-robot%20experiments%20showed%20that%20Bi-VLA%20successfully%20interprets%0Avision-language%20combinations%20and%20improves%20task%20success%20rates%20compared%20to%0Aconventional%20bilateral%20control-based%20imitation%20learning.%20Our%20Bi-VLA%20addresses%0Athe%20single-task%20limitation%20of%20prior%20bilateral%20approaches%20and%20provides%20empirical%0Aevidence%20that%20combining%20vision%20and%20language%20significantly%20enhances%20versatility.%0AExperimental%20results%20validate%20the%20effectiveness%20of%20Bi-VLA%20in%20real-world%20tasks.%0AFor%20additional%20material%2C%20please%20visit%20the%20website%3A%0Ahttps%3A//mertcookimg.github.io/bi-vla/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBi-VLA%253A%2520Bilateral%2520Control-Based%2520Imitation%2520Learning%2520via%2520Vision-Language%250A%2520%2520Fusion%2520for%2520Action%2520Generation%26entry.906535625%3DMasato%2520Kobayashi%2520and%2520Thanpimon%2520Buamanee%26entry.1292438233%3D%2520%2520We%2520propose%2520Bilateral%2520Control-Based%2520Imitation%2520Learning%2520via%2520Vision-Language%250AFusion%2520for%2520Action%2520Generation%2520%2528Bi-VLA%2529%252C%2520a%2520novel%2520framework%2520that%2520extends%2520bilateral%250Acontrol-based%2520imitation%2520learning%2520to%2520handle%2520more%2520than%2520one%2520task%2520within%2520a%2520single%250Amodel.%2520Conventional%2520bilateral%2520control%2520methods%2520exploit%2520joint%2520angle%252C%2520velocity%252C%250Atorque%252C%2520and%2520vision%2520for%2520precise%2520manipulation%2520but%2520require%2520task-specific%2520models%252C%250Alimiting%2520their%2520generality.%2520Bi-VLA%2520overcomes%2520this%2520limitation%2520by%2520utilizing%2520robot%250Ajoint%2520angle%252C%2520velocity%252C%2520and%2520torque%2520data%2520from%2520leader-follower%2520bilateral%2520control%250Awith%2520visual%2520features%2520and%2520natural%2520language%2520instructions%2520through%2520SigLIP%2520and%250AFiLM-based%2520fusion.%2520We%2520validated%2520Bi-VLA%2520on%2520two%2520task%2520types%253A%2520one%2520requiring%250Asupplementary%2520language%2520cues%2520and%2520another%2520distinguishable%2520solely%2520by%2520vision.%250AReal-robot%2520experiments%2520showed%2520that%2520Bi-VLA%2520successfully%2520interprets%250Avision-language%2520combinations%2520and%2520improves%2520task%2520success%2520rates%2520compared%2520to%250Aconventional%2520bilateral%2520control-based%2520imitation%2520learning.%2520Our%2520Bi-VLA%2520addresses%250Athe%2520single-task%2520limitation%2520of%2520prior%2520bilateral%2520approaches%2520and%2520provides%2520empirical%250Aevidence%2520that%2520combining%2520vision%2520and%2520language%2520significantly%2520enhances%2520versatility.%250AExperimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520Bi-VLA%2520in%2520real-world%2520tasks.%250AFor%2520additional%2520material%252C%2520please%2520visit%2520the%2520website%253A%250Ahttps%253A//mertcookimg.github.io/bi-vla/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bi-VLA%3A%20Bilateral%20Control-Based%20Imitation%20Learning%20via%20Vision-Language%0A%20%20Fusion%20for%20Action%20Generation&entry.906535625=Masato%20Kobayashi%20and%20Thanpimon%20Buamanee&entry.1292438233=%20%20We%20propose%20Bilateral%20Control-Based%20Imitation%20Learning%20via%20Vision-Language%0AFusion%20for%20Action%20Generation%20%28Bi-VLA%29%2C%20a%20novel%20framework%20that%20extends%20bilateral%0Acontrol-based%20imitation%20learning%20to%20handle%20more%20than%20one%20task%20within%20a%20single%0Amodel.%20Conventional%20bilateral%20control%20methods%20exploit%20joint%20angle%2C%20velocity%2C%0Atorque%2C%20and%20vision%20for%20precise%20manipulation%20but%20require%20task-specific%20models%2C%0Alimiting%20their%20generality.%20Bi-VLA%20overcomes%20this%20limitation%20by%20utilizing%20robot%0Ajoint%20angle%2C%20velocity%2C%20and%20torque%20data%20from%20leader-follower%20bilateral%20control%0Awith%20visual%20features%20and%20natural%20language%20instructions%20through%20SigLIP%20and%0AFiLM-based%20fusion.%20We%20validated%20Bi-VLA%20on%20two%20task%20types%3A%20one%20requiring%0Asupplementary%20language%20cues%20and%20another%20distinguishable%20solely%20by%20vision.%0AReal-robot%20experiments%20showed%20that%20Bi-VLA%20successfully%20interprets%0Avision-language%20combinations%20and%20improves%20task%20success%20rates%20compared%20to%0Aconventional%20bilateral%20control-based%20imitation%20learning.%20Our%20Bi-VLA%20addresses%0Athe%20single-task%20limitation%20of%20prior%20bilateral%20approaches%20and%20provides%20empirical%0Aevidence%20that%20combining%20vision%20and%20language%20significantly%20enhances%20versatility.%0AExperimental%20results%20validate%20the%20effectiveness%20of%20Bi-VLA%20in%20real-world%20tasks.%0AFor%20additional%20material%2C%20please%20visit%20the%20website%3A%0Ahttps%3A//mertcookimg.github.io/bi-vla/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18865v1&entry.124074799=Read"},
{"title": "Steering Multimodal Large Language Models Decoding for Context-Aware\n  Safety", "author": "Zheyuan Liu and Zhangchen Xu and Guangyao Dou and Xiangchi Yuan and Zhaoxuan Tan and Radha Poovendran and Meng Jiang", "abstract": "  Multimodal Large Language Models (MLLMs) are increasingly deployed in\nreal-world applications, yet their ability to make context-aware safety\ndecisions remains limited. Existing methods often fail to balance\noversensitivity (unjustified refusals of benign queries) and undersensitivity\n(missed detection of visually grounded risks), leaving a persistent gap in\nsafety alignment. To address this issue, we introduce Safety-aware Contrastive\nDecoding (SafeCoDe), a lightweight and model-agnostic decoding framework that\ndynamically adjusts token generation based on multimodal context. SafeCoDe\noperates in two stages: (1) a contrastive decoding mechanism that highlights\ntokens sensitive to visual context by contrasting real and Gaussian-noised\nimages, and (2) a global-aware token modulation strategy that integrates\nscene-level reasoning with token-level adjustment to adapt refusals according\nto the predicted safety verdict. Extensive experiments across diverse MLLM\narchitectures and safety benchmarks, covering undersensitivity,\noversensitivity, and general safety evaluations, show that SafeCoDe\nconsistently improves context-sensitive refusal behaviors while preserving\nmodel helpfulness.\n", "link": "http://arxiv.org/abs/2509.19212v1", "date": "2025-09-23", "relevancy": 2.1875, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20Multimodal%20Large%20Language%20Models%20Decoding%20for%20Context-Aware%0A%20%20Safety&body=Title%3A%20Steering%20Multimodal%20Large%20Language%20Models%20Decoding%20for%20Context-Aware%0A%20%20Safety%0AAuthor%3A%20Zheyuan%20Liu%20and%20Zhangchen%20Xu%20and%20Guangyao%20Dou%20and%20Xiangchi%20Yuan%20and%20Zhaoxuan%20Tan%20and%20Radha%20Poovendran%20and%20Meng%20Jiang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20increasingly%20deployed%20in%0Areal-world%20applications%2C%20yet%20their%20ability%20to%20make%20context-aware%20safety%0Adecisions%20remains%20limited.%20Existing%20methods%20often%20fail%20to%20balance%0Aoversensitivity%20%28unjustified%20refusals%20of%20benign%20queries%29%20and%20undersensitivity%0A%28missed%20detection%20of%20visually%20grounded%20risks%29%2C%20leaving%20a%20persistent%20gap%20in%0Asafety%20alignment.%20To%20address%20this%20issue%2C%20we%20introduce%20Safety-aware%20Contrastive%0ADecoding%20%28SafeCoDe%29%2C%20a%20lightweight%20and%20model-agnostic%20decoding%20framework%20that%0Adynamically%20adjusts%20token%20generation%20based%20on%20multimodal%20context.%20SafeCoDe%0Aoperates%20in%20two%20stages%3A%20%281%29%20a%20contrastive%20decoding%20mechanism%20that%20highlights%0Atokens%20sensitive%20to%20visual%20context%20by%20contrasting%20real%20and%20Gaussian-noised%0Aimages%2C%20and%20%282%29%20a%20global-aware%20token%20modulation%20strategy%20that%20integrates%0Ascene-level%20reasoning%20with%20token-level%20adjustment%20to%20adapt%20refusals%20according%0Ato%20the%20predicted%20safety%20verdict.%20Extensive%20experiments%20across%20diverse%20MLLM%0Aarchitectures%20and%20safety%20benchmarks%2C%20covering%20undersensitivity%2C%0Aoversensitivity%2C%20and%20general%20safety%20evaluations%2C%20show%20that%20SafeCoDe%0Aconsistently%20improves%20context-sensitive%20refusal%20behaviors%20while%20preserving%0Amodel%20helpfulness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520Multimodal%2520Large%2520Language%2520Models%2520Decoding%2520for%2520Context-Aware%250A%2520%2520Safety%26entry.906535625%3DZheyuan%2520Liu%2520and%2520Zhangchen%2520Xu%2520and%2520Guangyao%2520Dou%2520and%2520Xiangchi%2520Yuan%2520and%2520Zhaoxuan%2520Tan%2520and%2520Radha%2520Poovendran%2520and%2520Meng%2520Jiang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520increasingly%2520deployed%2520in%250Areal-world%2520applications%252C%2520yet%2520their%2520ability%2520to%2520make%2520context-aware%2520safety%250Adecisions%2520remains%2520limited.%2520Existing%2520methods%2520often%2520fail%2520to%2520balance%250Aoversensitivity%2520%2528unjustified%2520refusals%2520of%2520benign%2520queries%2529%2520and%2520undersensitivity%250A%2528missed%2520detection%2520of%2520visually%2520grounded%2520risks%2529%252C%2520leaving%2520a%2520persistent%2520gap%2520in%250Asafety%2520alignment.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Safety-aware%2520Contrastive%250ADecoding%2520%2528SafeCoDe%2529%252C%2520a%2520lightweight%2520and%2520model-agnostic%2520decoding%2520framework%2520that%250Adynamically%2520adjusts%2520token%2520generation%2520based%2520on%2520multimodal%2520context.%2520SafeCoDe%250Aoperates%2520in%2520two%2520stages%253A%2520%25281%2529%2520a%2520contrastive%2520decoding%2520mechanism%2520that%2520highlights%250Atokens%2520sensitive%2520to%2520visual%2520context%2520by%2520contrasting%2520real%2520and%2520Gaussian-noised%250Aimages%252C%2520and%2520%25282%2529%2520a%2520global-aware%2520token%2520modulation%2520strategy%2520that%2520integrates%250Ascene-level%2520reasoning%2520with%2520token-level%2520adjustment%2520to%2520adapt%2520refusals%2520according%250Ato%2520the%2520predicted%2520safety%2520verdict.%2520Extensive%2520experiments%2520across%2520diverse%2520MLLM%250Aarchitectures%2520and%2520safety%2520benchmarks%252C%2520covering%2520undersensitivity%252C%250Aoversensitivity%252C%2520and%2520general%2520safety%2520evaluations%252C%2520show%2520that%2520SafeCoDe%250Aconsistently%2520improves%2520context-sensitive%2520refusal%2520behaviors%2520while%2520preserving%250Amodel%2520helpfulness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20Multimodal%20Large%20Language%20Models%20Decoding%20for%20Context-Aware%0A%20%20Safety&entry.906535625=Zheyuan%20Liu%20and%20Zhangchen%20Xu%20and%20Guangyao%20Dou%20and%20Xiangchi%20Yuan%20and%20Zhaoxuan%20Tan%20and%20Radha%20Poovendran%20and%20Meng%20Jiang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20increasingly%20deployed%20in%0Areal-world%20applications%2C%20yet%20their%20ability%20to%20make%20context-aware%20safety%0Adecisions%20remains%20limited.%20Existing%20methods%20often%20fail%20to%20balance%0Aoversensitivity%20%28unjustified%20refusals%20of%20benign%20queries%29%20and%20undersensitivity%0A%28missed%20detection%20of%20visually%20grounded%20risks%29%2C%20leaving%20a%20persistent%20gap%20in%0Asafety%20alignment.%20To%20address%20this%20issue%2C%20we%20introduce%20Safety-aware%20Contrastive%0ADecoding%20%28SafeCoDe%29%2C%20a%20lightweight%20and%20model-agnostic%20decoding%20framework%20that%0Adynamically%20adjusts%20token%20generation%20based%20on%20multimodal%20context.%20SafeCoDe%0Aoperates%20in%20two%20stages%3A%20%281%29%20a%20contrastive%20decoding%20mechanism%20that%20highlights%0Atokens%20sensitive%20to%20visual%20context%20by%20contrasting%20real%20and%20Gaussian-noised%0Aimages%2C%20and%20%282%29%20a%20global-aware%20token%20modulation%20strategy%20that%20integrates%0Ascene-level%20reasoning%20with%20token-level%20adjustment%20to%20adapt%20refusals%20according%0Ato%20the%20predicted%20safety%20verdict.%20Extensive%20experiments%20across%20diverse%20MLLM%0Aarchitectures%20and%20safety%20benchmarks%2C%20covering%20undersensitivity%2C%0Aoversensitivity%2C%20and%20general%20safety%20evaluations%2C%20show%20that%20SafeCoDe%0Aconsistently%20improves%20context-sensitive%20refusal%20behaviors%20while%20preserving%0Amodel%20helpfulness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19212v1&entry.124074799=Read"},
{"title": "Learning coordinated badminton skills for legged manipulators", "author": "Yuntao Ma and Andrei Cramariuc and Farbod Farshidian and Marco Hutter", "abstract": "  Coordinating the motion between lower and upper limbs and aligning limb\ncontrol with perception are substantial challenges in robotics, particularly in\ndynamic environments. To this end, we introduce an approach for enabling legged\nmobile manipulators to play badminton, a task that requires precise\ncoordination of perception, locomotion, and arm swinging. We propose a unified\nreinforcement learning-based control policy for whole-body visuomotor skills\ninvolving all degrees of freedom to achieve effective shuttlecock tracking and\nstriking. This policy is informed by a perception noise model that utilizes\nreal-world camera data, allowing for consistent perception error levels between\nsimulation and deployment and encouraging learned active perception behaviors.\nOur method includes a shuttlecock prediction model, constrained reinforcement\nlearning for robust motion control, and integrated system identification\ntechniques to enhance deployment readiness. Extensive experimental results in a\nvariety of environments validate the robot's capability to predict shuttlecock\ntrajectories, navigate the service area effectively, and execute precise\nstrikes against human players, demonstrating the feasibility of using legged\nmobile manipulators in complex and dynamic sports scenarios.\n", "link": "http://arxiv.org/abs/2505.22974v2", "date": "2025-09-23", "relevancy": 2.1858, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.589}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5449}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20coordinated%20badminton%20skills%20for%20legged%20manipulators&body=Title%3A%20Learning%20coordinated%20badminton%20skills%20for%20legged%20manipulators%0AAuthor%3A%20Yuntao%20Ma%20and%20Andrei%20Cramariuc%20and%20Farbod%20Farshidian%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Coordinating%20the%20motion%20between%20lower%20and%20upper%20limbs%20and%20aligning%20limb%0Acontrol%20with%20perception%20are%20substantial%20challenges%20in%20robotics%2C%20particularly%20in%0Adynamic%20environments.%20To%20this%20end%2C%20we%20introduce%20an%20approach%20for%20enabling%20legged%0Amobile%20manipulators%20to%20play%20badminton%2C%20a%20task%20that%20requires%20precise%0Acoordination%20of%20perception%2C%20locomotion%2C%20and%20arm%20swinging.%20We%20propose%20a%20unified%0Areinforcement%20learning-based%20control%20policy%20for%20whole-body%20visuomotor%20skills%0Ainvolving%20all%20degrees%20of%20freedom%20to%20achieve%20effective%20shuttlecock%20tracking%20and%0Astriking.%20This%20policy%20is%20informed%20by%20a%20perception%20noise%20model%20that%20utilizes%0Areal-world%20camera%20data%2C%20allowing%20for%20consistent%20perception%20error%20levels%20between%0Asimulation%20and%20deployment%20and%20encouraging%20learned%20active%20perception%20behaviors.%0AOur%20method%20includes%20a%20shuttlecock%20prediction%20model%2C%20constrained%20reinforcement%0Alearning%20for%20robust%20motion%20control%2C%20and%20integrated%20system%20identification%0Atechniques%20to%20enhance%20deployment%20readiness.%20Extensive%20experimental%20results%20in%20a%0Avariety%20of%20environments%20validate%20the%20robot%27s%20capability%20to%20predict%20shuttlecock%0Atrajectories%2C%20navigate%20the%20service%20area%20effectively%2C%20and%20execute%20precise%0Astrikes%20against%20human%20players%2C%20demonstrating%20the%20feasibility%20of%20using%20legged%0Amobile%20manipulators%20in%20complex%20and%20dynamic%20sports%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22974v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520coordinated%2520badminton%2520skills%2520for%2520legged%2520manipulators%26entry.906535625%3DYuntao%2520Ma%2520and%2520Andrei%2520Cramariuc%2520and%2520Farbod%2520Farshidian%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Coordinating%2520the%2520motion%2520between%2520lower%2520and%2520upper%2520limbs%2520and%2520aligning%2520limb%250Acontrol%2520with%2520perception%2520are%2520substantial%2520challenges%2520in%2520robotics%252C%2520particularly%2520in%250Adynamic%2520environments.%2520To%2520this%2520end%252C%2520we%2520introduce%2520an%2520approach%2520for%2520enabling%2520legged%250Amobile%2520manipulators%2520to%2520play%2520badminton%252C%2520a%2520task%2520that%2520requires%2520precise%250Acoordination%2520of%2520perception%252C%2520locomotion%252C%2520and%2520arm%2520swinging.%2520We%2520propose%2520a%2520unified%250Areinforcement%2520learning-based%2520control%2520policy%2520for%2520whole-body%2520visuomotor%2520skills%250Ainvolving%2520all%2520degrees%2520of%2520freedom%2520to%2520achieve%2520effective%2520shuttlecock%2520tracking%2520and%250Astriking.%2520This%2520policy%2520is%2520informed%2520by%2520a%2520perception%2520noise%2520model%2520that%2520utilizes%250Areal-world%2520camera%2520data%252C%2520allowing%2520for%2520consistent%2520perception%2520error%2520levels%2520between%250Asimulation%2520and%2520deployment%2520and%2520encouraging%2520learned%2520active%2520perception%2520behaviors.%250AOur%2520method%2520includes%2520a%2520shuttlecock%2520prediction%2520model%252C%2520constrained%2520reinforcement%250Alearning%2520for%2520robust%2520motion%2520control%252C%2520and%2520integrated%2520system%2520identification%250Atechniques%2520to%2520enhance%2520deployment%2520readiness.%2520Extensive%2520experimental%2520results%2520in%2520a%250Avariety%2520of%2520environments%2520validate%2520the%2520robot%2527s%2520capability%2520to%2520predict%2520shuttlecock%250Atrajectories%252C%2520navigate%2520the%2520service%2520area%2520effectively%252C%2520and%2520execute%2520precise%250Astrikes%2520against%2520human%2520players%252C%2520demonstrating%2520the%2520feasibility%2520of%2520using%2520legged%250Amobile%2520manipulators%2520in%2520complex%2520and%2520dynamic%2520sports%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22974v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20coordinated%20badminton%20skills%20for%20legged%20manipulators&entry.906535625=Yuntao%20Ma%20and%20Andrei%20Cramariuc%20and%20Farbod%20Farshidian%20and%20Marco%20Hutter&entry.1292438233=%20%20Coordinating%20the%20motion%20between%20lower%20and%20upper%20limbs%20and%20aligning%20limb%0Acontrol%20with%20perception%20are%20substantial%20challenges%20in%20robotics%2C%20particularly%20in%0Adynamic%20environments.%20To%20this%20end%2C%20we%20introduce%20an%20approach%20for%20enabling%20legged%0Amobile%20manipulators%20to%20play%20badminton%2C%20a%20task%20that%20requires%20precise%0Acoordination%20of%20perception%2C%20locomotion%2C%20and%20arm%20swinging.%20We%20propose%20a%20unified%0Areinforcement%20learning-based%20control%20policy%20for%20whole-body%20visuomotor%20skills%0Ainvolving%20all%20degrees%20of%20freedom%20to%20achieve%20effective%20shuttlecock%20tracking%20and%0Astriking.%20This%20policy%20is%20informed%20by%20a%20perception%20noise%20model%20that%20utilizes%0Areal-world%20camera%20data%2C%20allowing%20for%20consistent%20perception%20error%20levels%20between%0Asimulation%20and%20deployment%20and%20encouraging%20learned%20active%20perception%20behaviors.%0AOur%20method%20includes%20a%20shuttlecock%20prediction%20model%2C%20constrained%20reinforcement%0Alearning%20for%20robust%20motion%20control%2C%20and%20integrated%20system%20identification%0Atechniques%20to%20enhance%20deployment%20readiness.%20Extensive%20experimental%20results%20in%20a%0Avariety%20of%20environments%20validate%20the%20robot%27s%20capability%20to%20predict%20shuttlecock%0Atrajectories%2C%20navigate%20the%20service%20area%20effectively%2C%20and%20execute%20precise%0Astrikes%20against%20human%20players%2C%20demonstrating%20the%20feasibility%20of%20using%20legged%0Amobile%20manipulators%20in%20complex%20and%20dynamic%20sports%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22974v2&entry.124074799=Read"},
{"title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather\n  Conditions", "author": "Yun Wang and Junjie Hu and Junhui Hou and Chenghao Zhang and Renwei Yang and Dapeng Oliver Wu", "abstract": "  Recent self-supervised stereo matching methods have made significant\nprogress, but their performance significantly degrades under adverse weather\nconditions such as night, rain, and fog. We identify two primary weaknesses\ncontributing to this performance degradation. First, adverse weather introduces\nnoise and reduces visibility, making CNN-based feature extractors struggle with\ndegraded regions like reflective and textureless areas. Second, these degraded\nregions can disrupt accurate pixel correspondences, leading to ineffective\nsupervision based on the photometric consistency assumption. To address these\nchallenges, we propose injecting robust priors derived from the visual\nfoundation model into the CNN-based feature extractor to improve feature\nrepresentation under adverse weather conditions. We then introduce scene\ncorrespondence priors to construct robust supervisory signals rather than\nrelying solely on the photometric consistency assumption. Specifically, we\ncreate synthetic stereo datasets with realistic weather degradations. These\ndatasets feature clear and adverse image pairs that maintain the same semantic\ncontext and disparity, preserving the scene correspondence property. With this\nknowledge, we propose a robust self-supervised training paradigm, consisting of\ntwo key steps: robust self-supervised scene correspondence learning and adverse\nweather distillation. Both steps aim to align underlying scene results from\nclean and adverse image pairs, thus improving model disparity estimation under\nadverse weather effects. Extensive experiments demonstrate the effectiveness\nand versatility of our proposed solution, which outperforms existing\nstate-of-the-art self-supervised methods. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.\n", "link": "http://arxiv.org/abs/2509.19165v1", "date": "2025-09-23", "relevancy": 2.1839, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5508}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoSe%3A%20Robust%20Self-supervised%20Stereo%20Matching%20under%20Adverse%20Weather%0A%20%20Conditions&body=Title%3A%20RoSe%3A%20Robust%20Self-supervised%20Stereo%20Matching%20under%20Adverse%20Weather%0A%20%20Conditions%0AAuthor%3A%20Yun%20Wang%20and%20Junjie%20Hu%20and%20Junhui%20Hou%20and%20Chenghao%20Zhang%20and%20Renwei%20Yang%20and%20Dapeng%20Oliver%20Wu%0AAbstract%3A%20%20%20Recent%20self-supervised%20stereo%20matching%20methods%20have%20made%20significant%0Aprogress%2C%20but%20their%20performance%20significantly%20degrades%20under%20adverse%20weather%0Aconditions%20such%20as%20night%2C%20rain%2C%20and%20fog.%20We%20identify%20two%20primary%20weaknesses%0Acontributing%20to%20this%20performance%20degradation.%20First%2C%20adverse%20weather%20introduces%0Anoise%20and%20reduces%20visibility%2C%20making%20CNN-based%20feature%20extractors%20struggle%20with%0Adegraded%20regions%20like%20reflective%20and%20textureless%20areas.%20Second%2C%20these%20degraded%0Aregions%20can%20disrupt%20accurate%20pixel%20correspondences%2C%20leading%20to%20ineffective%0Asupervision%20based%20on%20the%20photometric%20consistency%20assumption.%20To%20address%20these%0Achallenges%2C%20we%20propose%20injecting%20robust%20priors%20derived%20from%20the%20visual%0Afoundation%20model%20into%20the%20CNN-based%20feature%20extractor%20to%20improve%20feature%0Arepresentation%20under%20adverse%20weather%20conditions.%20We%20then%20introduce%20scene%0Acorrespondence%20priors%20to%20construct%20robust%20supervisory%20signals%20rather%20than%0Arelying%20solely%20on%20the%20photometric%20consistency%20assumption.%20Specifically%2C%20we%0Acreate%20synthetic%20stereo%20datasets%20with%20realistic%20weather%20degradations.%20These%0Adatasets%20feature%20clear%20and%20adverse%20image%20pairs%20that%20maintain%20the%20same%20semantic%0Acontext%20and%20disparity%2C%20preserving%20the%20scene%20correspondence%20property.%20With%20this%0Aknowledge%2C%20we%20propose%20a%20robust%20self-supervised%20training%20paradigm%2C%20consisting%20of%0Atwo%20key%20steps%3A%20robust%20self-supervised%20scene%20correspondence%20learning%20and%20adverse%0Aweather%20distillation.%20Both%20steps%20aim%20to%20align%20underlying%20scene%20results%20from%0Aclean%20and%20adverse%20image%20pairs%2C%20thus%20improving%20model%20disparity%20estimation%20under%0Aadverse%20weather%20effects.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aand%20versatility%20of%20our%20proposed%20solution%2C%20which%20outperforms%20existing%0Astate-of-the-art%20self-supervised%20methods.%20Codes%20are%20available%20at%0A%5Ctextcolor%7Bblue%7D%7Bhttps%3A//github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoSe%253A%2520Robust%2520Self-supervised%2520Stereo%2520Matching%2520under%2520Adverse%2520Weather%250A%2520%2520Conditions%26entry.906535625%3DYun%2520Wang%2520and%2520Junjie%2520Hu%2520and%2520Junhui%2520Hou%2520and%2520Chenghao%2520Zhang%2520and%2520Renwei%2520Yang%2520and%2520Dapeng%2520Oliver%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520self-supervised%2520stereo%2520matching%2520methods%2520have%2520made%2520significant%250Aprogress%252C%2520but%2520their%2520performance%2520significantly%2520degrades%2520under%2520adverse%2520weather%250Aconditions%2520such%2520as%2520night%252C%2520rain%252C%2520and%2520fog.%2520We%2520identify%2520two%2520primary%2520weaknesses%250Acontributing%2520to%2520this%2520performance%2520degradation.%2520First%252C%2520adverse%2520weather%2520introduces%250Anoise%2520and%2520reduces%2520visibility%252C%2520making%2520CNN-based%2520feature%2520extractors%2520struggle%2520with%250Adegraded%2520regions%2520like%2520reflective%2520and%2520textureless%2520areas.%2520Second%252C%2520these%2520degraded%250Aregions%2520can%2520disrupt%2520accurate%2520pixel%2520correspondences%252C%2520leading%2520to%2520ineffective%250Asupervision%2520based%2520on%2520the%2520photometric%2520consistency%2520assumption.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520injecting%2520robust%2520priors%2520derived%2520from%2520the%2520visual%250Afoundation%2520model%2520into%2520the%2520CNN-based%2520feature%2520extractor%2520to%2520improve%2520feature%250Arepresentation%2520under%2520adverse%2520weather%2520conditions.%2520We%2520then%2520introduce%2520scene%250Acorrespondence%2520priors%2520to%2520construct%2520robust%2520supervisory%2520signals%2520rather%2520than%250Arelying%2520solely%2520on%2520the%2520photometric%2520consistency%2520assumption.%2520Specifically%252C%2520we%250Acreate%2520synthetic%2520stereo%2520datasets%2520with%2520realistic%2520weather%2520degradations.%2520These%250Adatasets%2520feature%2520clear%2520and%2520adverse%2520image%2520pairs%2520that%2520maintain%2520the%2520same%2520semantic%250Acontext%2520and%2520disparity%252C%2520preserving%2520the%2520scene%2520correspondence%2520property.%2520With%2520this%250Aknowledge%252C%2520we%2520propose%2520a%2520robust%2520self-supervised%2520training%2520paradigm%252C%2520consisting%2520of%250Atwo%2520key%2520steps%253A%2520robust%2520self-supervised%2520scene%2520correspondence%2520learning%2520and%2520adverse%250Aweather%2520distillation.%2520Both%2520steps%2520aim%2520to%2520align%2520underlying%2520scene%2520results%2520from%250Aclean%2520and%2520adverse%2520image%2520pairs%252C%2520thus%2520improving%2520model%2520disparity%2520estimation%2520under%250Aadverse%2520weather%2520effects.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%250Aand%2520versatility%2520of%2520our%2520proposed%2520solution%252C%2520which%2520outperforms%2520existing%250Astate-of-the-art%2520self-supervised%2520methods.%2520Codes%2520are%2520available%2520at%250A%255Ctextcolor%257Bblue%257D%257Bhttps%253A//github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoSe%3A%20Robust%20Self-supervised%20Stereo%20Matching%20under%20Adverse%20Weather%0A%20%20Conditions&entry.906535625=Yun%20Wang%20and%20Junjie%20Hu%20and%20Junhui%20Hou%20and%20Chenghao%20Zhang%20and%20Renwei%20Yang%20and%20Dapeng%20Oliver%20Wu&entry.1292438233=%20%20Recent%20self-supervised%20stereo%20matching%20methods%20have%20made%20significant%0Aprogress%2C%20but%20their%20performance%20significantly%20degrades%20under%20adverse%20weather%0Aconditions%20such%20as%20night%2C%20rain%2C%20and%20fog.%20We%20identify%20two%20primary%20weaknesses%0Acontributing%20to%20this%20performance%20degradation.%20First%2C%20adverse%20weather%20introduces%0Anoise%20and%20reduces%20visibility%2C%20making%20CNN-based%20feature%20extractors%20struggle%20with%0Adegraded%20regions%20like%20reflective%20and%20textureless%20areas.%20Second%2C%20these%20degraded%0Aregions%20can%20disrupt%20accurate%20pixel%20correspondences%2C%20leading%20to%20ineffective%0Asupervision%20based%20on%20the%20photometric%20consistency%20assumption.%20To%20address%20these%0Achallenges%2C%20we%20propose%20injecting%20robust%20priors%20derived%20from%20the%20visual%0Afoundation%20model%20into%20the%20CNN-based%20feature%20extractor%20to%20improve%20feature%0Arepresentation%20under%20adverse%20weather%20conditions.%20We%20then%20introduce%20scene%0Acorrespondence%20priors%20to%20construct%20robust%20supervisory%20signals%20rather%20than%0Arelying%20solely%20on%20the%20photometric%20consistency%20assumption.%20Specifically%2C%20we%0Acreate%20synthetic%20stereo%20datasets%20with%20realistic%20weather%20degradations.%20These%0Adatasets%20feature%20clear%20and%20adverse%20image%20pairs%20that%20maintain%20the%20same%20semantic%0Acontext%20and%20disparity%2C%20preserving%20the%20scene%20correspondence%20property.%20With%20this%0Aknowledge%2C%20we%20propose%20a%20robust%20self-supervised%20training%20paradigm%2C%20consisting%20of%0Atwo%20key%20steps%3A%20robust%20self-supervised%20scene%20correspondence%20learning%20and%20adverse%0Aweather%20distillation.%20Both%20steps%20aim%20to%20align%20underlying%20scene%20results%20from%0Aclean%20and%20adverse%20image%20pairs%2C%20thus%20improving%20model%20disparity%20estimation%20under%0Aadverse%20weather%20effects.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aand%20versatility%20of%20our%20proposed%20solution%2C%20which%20outperforms%20existing%0Astate-of-the-art%20self-supervised%20methods.%20Codes%20are%20available%20at%0A%5Ctextcolor%7Bblue%7D%7Bhttps%3A//github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19165v1&entry.124074799=Read"},
{"title": "Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation\n  Platform for Embodied AI", "author": "Fei Ni and Min Zhang and Pengyi Li and Yifu Yuan and Lingfeng Zhang and Yuecheng Liu and Peilong Han and Longxin Kou and Shaojin Ma and Jinbin Qiao and David Gamaliel Arcos Bravo and Yuening Wang and Xiao Hu and Zhanguang Zhang and Xianze Yao and Yutong Li and Zhao Zhang and Ying Wen and Ying-Cong Chen and Xiaodan Liang and Liang Lin and Bin He and Haitham Bou-Ammar and He Wang and Huazhe Xu and Jiankang Deng and Shan Luo and Shuqiang Jiang and Wei Pan and Yang Gao and Stefanos Zafeiriou and Jan Peters and Yuzheng Zhuang and Yingxue Zhang and Yan Zheng and Hongyao Tang and Jianye Hao", "abstract": "  Embodied AI development significantly lags behind large foundation models due\nto three critical challenges: (1) lack of systematic understanding of core\ncapabilities needed for Embodied AI, making research lack clear objectives; (2)\nabsence of unified and standardized evaluation systems, rendering\ncross-benchmark evaluation infeasible; and (3) underdeveloped automated and\nscalable acquisition methods for embodied data, creating critical bottlenecks\nfor model scaling. To address these obstacles, we present Embodied Arena, a\ncomprehensive, unified, and evolving evaluation platform for Embodied AI. Our\nplatform establishes a systematic embodied capability taxonomy spanning three\nlevels (perception, reasoning, task execution), seven core capabilities, and 25\nfine-grained dimensions, enabling unified evaluation with systematic research\nobjectives. We introduce a standardized evaluation system built upon unified\ninfrastructure supporting flexible integration of 22 diverse benchmarks across\nthree domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced\nmodels from 20+ worldwide institutes. Additionally, we develop a novel\nLLM-driven automated generation pipeline ensuring scalable embodied evaluation\ndata with continuous evolution for diversity and comprehensiveness. Embodied\nArena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task\nPlanning) with dual perspectives (benchmark view and capability view),\nproviding comprehensive overviews of advanced model capabilities. Especially,\nwe present nine findings summarized from the evaluation results on the\nleaderboards of Embodied Arena. This helps to establish clear research veins\nand pinpoint critical research problems, thereby driving forward progress in\nthe field of Embodied AI.\n", "link": "http://arxiv.org/abs/2509.15273v2", "date": "2025-09-23", "relevancy": 2.1805, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied%20Arena%3A%20A%20Comprehensive%2C%20Unified%2C%20and%20Evolving%20Evaluation%0A%20%20Platform%20for%20Embodied%20AI&body=Title%3A%20Embodied%20Arena%3A%20A%20Comprehensive%2C%20Unified%2C%20and%20Evolving%20Evaluation%0A%20%20Platform%20for%20Embodied%20AI%0AAuthor%3A%20Fei%20Ni%20and%20Min%20Zhang%20and%20Pengyi%20Li%20and%20Yifu%20Yuan%20and%20Lingfeng%20Zhang%20and%20Yuecheng%20Liu%20and%20Peilong%20Han%20and%20Longxin%20Kou%20and%20Shaojin%20Ma%20and%20Jinbin%20Qiao%20and%20David%20Gamaliel%20Arcos%20Bravo%20and%20Yuening%20Wang%20and%20Xiao%20Hu%20and%20Zhanguang%20Zhang%20and%20Xianze%20Yao%20and%20Yutong%20Li%20and%20Zhao%20Zhang%20and%20Ying%20Wen%20and%20Ying-Cong%20Chen%20and%20Xiaodan%20Liang%20and%20Liang%20Lin%20and%20Bin%20He%20and%20Haitham%20Bou-Ammar%20and%20He%20Wang%20and%20Huazhe%20Xu%20and%20Jiankang%20Deng%20and%20Shan%20Luo%20and%20Shuqiang%20Jiang%20and%20Wei%20Pan%20and%20Yang%20Gao%20and%20Stefanos%20Zafeiriou%20and%20Jan%20Peters%20and%20Yuzheng%20Zhuang%20and%20Yingxue%20Zhang%20and%20Yan%20Zheng%20and%20Hongyao%20Tang%20and%20Jianye%20Hao%0AAbstract%3A%20%20%20Embodied%20AI%20development%20significantly%20lags%20behind%20large%20foundation%20models%20due%0Ato%20three%20critical%20challenges%3A%20%281%29%20lack%20of%20systematic%20understanding%20of%20core%0Acapabilities%20needed%20for%20Embodied%20AI%2C%20making%20research%20lack%20clear%20objectives%3B%20%282%29%0Aabsence%20of%20unified%20and%20standardized%20evaluation%20systems%2C%20rendering%0Across-benchmark%20evaluation%20infeasible%3B%20and%20%283%29%20underdeveloped%20automated%20and%0Ascalable%20acquisition%20methods%20for%20embodied%20data%2C%20creating%20critical%20bottlenecks%0Afor%20model%20scaling.%20To%20address%20these%20obstacles%2C%20we%20present%20Embodied%20Arena%2C%20a%0Acomprehensive%2C%20unified%2C%20and%20evolving%20evaluation%20platform%20for%20Embodied%20AI.%20Our%0Aplatform%20establishes%20a%20systematic%20embodied%20capability%20taxonomy%20spanning%20three%0Alevels%20%28perception%2C%20reasoning%2C%20task%20execution%29%2C%20seven%20core%20capabilities%2C%20and%2025%0Afine-grained%20dimensions%2C%20enabling%20unified%20evaluation%20with%20systematic%20research%0Aobjectives.%20We%20introduce%20a%20standardized%20evaluation%20system%20built%20upon%20unified%0Ainfrastructure%20supporting%20flexible%20integration%20of%2022%20diverse%20benchmarks%20across%0Athree%20domains%20%282D/3D%20Embodied%20Q%26A%2C%20Navigation%2C%20Task%20Planning%29%20and%2030%2B%20advanced%0Amodels%20from%2020%2B%20worldwide%20institutes.%20Additionally%2C%20we%20develop%20a%20novel%0ALLM-driven%20automated%20generation%20pipeline%20ensuring%20scalable%20embodied%20evaluation%0Adata%20with%20continuous%20evolution%20for%20diversity%20and%20comprehensiveness.%20Embodied%0AArena%20publishes%20three%20real-time%20leaderboards%20%28Embodied%20Q%26A%2C%20Navigation%2C%20Task%0APlanning%29%20with%20dual%20perspectives%20%28benchmark%20view%20and%20capability%20view%29%2C%0Aproviding%20comprehensive%20overviews%20of%20advanced%20model%20capabilities.%20Especially%2C%0Awe%20present%20nine%20findings%20summarized%20from%20the%20evaluation%20results%20on%20the%0Aleaderboards%20of%20Embodied%20Arena.%20This%20helps%20to%20establish%20clear%20research%20veins%0Aand%20pinpoint%20critical%20research%20problems%2C%20thereby%20driving%20forward%20progress%20in%0Athe%20field%20of%20Embodied%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied%2520Arena%253A%2520A%2520Comprehensive%252C%2520Unified%252C%2520and%2520Evolving%2520Evaluation%250A%2520%2520Platform%2520for%2520Embodied%2520AI%26entry.906535625%3DFei%2520Ni%2520and%2520Min%2520Zhang%2520and%2520Pengyi%2520Li%2520and%2520Yifu%2520Yuan%2520and%2520Lingfeng%2520Zhang%2520and%2520Yuecheng%2520Liu%2520and%2520Peilong%2520Han%2520and%2520Longxin%2520Kou%2520and%2520Shaojin%2520Ma%2520and%2520Jinbin%2520Qiao%2520and%2520David%2520Gamaliel%2520Arcos%2520Bravo%2520and%2520Yuening%2520Wang%2520and%2520Xiao%2520Hu%2520and%2520Zhanguang%2520Zhang%2520and%2520Xianze%2520Yao%2520and%2520Yutong%2520Li%2520and%2520Zhao%2520Zhang%2520and%2520Ying%2520Wen%2520and%2520Ying-Cong%2520Chen%2520and%2520Xiaodan%2520Liang%2520and%2520Liang%2520Lin%2520and%2520Bin%2520He%2520and%2520Haitham%2520Bou-Ammar%2520and%2520He%2520Wang%2520and%2520Huazhe%2520Xu%2520and%2520Jiankang%2520Deng%2520and%2520Shan%2520Luo%2520and%2520Shuqiang%2520Jiang%2520and%2520Wei%2520Pan%2520and%2520Yang%2520Gao%2520and%2520Stefanos%2520Zafeiriou%2520and%2520Jan%2520Peters%2520and%2520Yuzheng%2520Zhuang%2520and%2520Yingxue%2520Zhang%2520and%2520Yan%2520Zheng%2520and%2520Hongyao%2520Tang%2520and%2520Jianye%2520Hao%26entry.1292438233%3D%2520%2520Embodied%2520AI%2520development%2520significantly%2520lags%2520behind%2520large%2520foundation%2520models%2520due%250Ato%2520three%2520critical%2520challenges%253A%2520%25281%2529%2520lack%2520of%2520systematic%2520understanding%2520of%2520core%250Acapabilities%2520needed%2520for%2520Embodied%2520AI%252C%2520making%2520research%2520lack%2520clear%2520objectives%253B%2520%25282%2529%250Aabsence%2520of%2520unified%2520and%2520standardized%2520evaluation%2520systems%252C%2520rendering%250Across-benchmark%2520evaluation%2520infeasible%253B%2520and%2520%25283%2529%2520underdeveloped%2520automated%2520and%250Ascalable%2520acquisition%2520methods%2520for%2520embodied%2520data%252C%2520creating%2520critical%2520bottlenecks%250Afor%2520model%2520scaling.%2520To%2520address%2520these%2520obstacles%252C%2520we%2520present%2520Embodied%2520Arena%252C%2520a%250Acomprehensive%252C%2520unified%252C%2520and%2520evolving%2520evaluation%2520platform%2520for%2520Embodied%2520AI.%2520Our%250Aplatform%2520establishes%2520a%2520systematic%2520embodied%2520capability%2520taxonomy%2520spanning%2520three%250Alevels%2520%2528perception%252C%2520reasoning%252C%2520task%2520execution%2529%252C%2520seven%2520core%2520capabilities%252C%2520and%252025%250Afine-grained%2520dimensions%252C%2520enabling%2520unified%2520evaluation%2520with%2520systematic%2520research%250Aobjectives.%2520We%2520introduce%2520a%2520standardized%2520evaluation%2520system%2520built%2520upon%2520unified%250Ainfrastructure%2520supporting%2520flexible%2520integration%2520of%252022%2520diverse%2520benchmarks%2520across%250Athree%2520domains%2520%25282D/3D%2520Embodied%2520Q%2526A%252C%2520Navigation%252C%2520Task%2520Planning%2529%2520and%252030%252B%2520advanced%250Amodels%2520from%252020%252B%2520worldwide%2520institutes.%2520Additionally%252C%2520we%2520develop%2520a%2520novel%250ALLM-driven%2520automated%2520generation%2520pipeline%2520ensuring%2520scalable%2520embodied%2520evaluation%250Adata%2520with%2520continuous%2520evolution%2520for%2520diversity%2520and%2520comprehensiveness.%2520Embodied%250AArena%2520publishes%2520three%2520real-time%2520leaderboards%2520%2528Embodied%2520Q%2526A%252C%2520Navigation%252C%2520Task%250APlanning%2529%2520with%2520dual%2520perspectives%2520%2528benchmark%2520view%2520and%2520capability%2520view%2529%252C%250Aproviding%2520comprehensive%2520overviews%2520of%2520advanced%2520model%2520capabilities.%2520Especially%252C%250Awe%2520present%2520nine%2520findings%2520summarized%2520from%2520the%2520evaluation%2520results%2520on%2520the%250Aleaderboards%2520of%2520Embodied%2520Arena.%2520This%2520helps%2520to%2520establish%2520clear%2520research%2520veins%250Aand%2520pinpoint%2520critical%2520research%2520problems%252C%2520thereby%2520driving%2520forward%2520progress%2520in%250Athe%2520field%2520of%2520Embodied%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20Arena%3A%20A%20Comprehensive%2C%20Unified%2C%20and%20Evolving%20Evaluation%0A%20%20Platform%20for%20Embodied%20AI&entry.906535625=Fei%20Ni%20and%20Min%20Zhang%20and%20Pengyi%20Li%20and%20Yifu%20Yuan%20and%20Lingfeng%20Zhang%20and%20Yuecheng%20Liu%20and%20Peilong%20Han%20and%20Longxin%20Kou%20and%20Shaojin%20Ma%20and%20Jinbin%20Qiao%20and%20David%20Gamaliel%20Arcos%20Bravo%20and%20Yuening%20Wang%20and%20Xiao%20Hu%20and%20Zhanguang%20Zhang%20and%20Xianze%20Yao%20and%20Yutong%20Li%20and%20Zhao%20Zhang%20and%20Ying%20Wen%20and%20Ying-Cong%20Chen%20and%20Xiaodan%20Liang%20and%20Liang%20Lin%20and%20Bin%20He%20and%20Haitham%20Bou-Ammar%20and%20He%20Wang%20and%20Huazhe%20Xu%20and%20Jiankang%20Deng%20and%20Shan%20Luo%20and%20Shuqiang%20Jiang%20and%20Wei%20Pan%20and%20Yang%20Gao%20and%20Stefanos%20Zafeiriou%20and%20Jan%20Peters%20and%20Yuzheng%20Zhuang%20and%20Yingxue%20Zhang%20and%20Yan%20Zheng%20and%20Hongyao%20Tang%20and%20Jianye%20Hao&entry.1292438233=%20%20Embodied%20AI%20development%20significantly%20lags%20behind%20large%20foundation%20models%20due%0Ato%20three%20critical%20challenges%3A%20%281%29%20lack%20of%20systematic%20understanding%20of%20core%0Acapabilities%20needed%20for%20Embodied%20AI%2C%20making%20research%20lack%20clear%20objectives%3B%20%282%29%0Aabsence%20of%20unified%20and%20standardized%20evaluation%20systems%2C%20rendering%0Across-benchmark%20evaluation%20infeasible%3B%20and%20%283%29%20underdeveloped%20automated%20and%0Ascalable%20acquisition%20methods%20for%20embodied%20data%2C%20creating%20critical%20bottlenecks%0Afor%20model%20scaling.%20To%20address%20these%20obstacles%2C%20we%20present%20Embodied%20Arena%2C%20a%0Acomprehensive%2C%20unified%2C%20and%20evolving%20evaluation%20platform%20for%20Embodied%20AI.%20Our%0Aplatform%20establishes%20a%20systematic%20embodied%20capability%20taxonomy%20spanning%20three%0Alevels%20%28perception%2C%20reasoning%2C%20task%20execution%29%2C%20seven%20core%20capabilities%2C%20and%2025%0Afine-grained%20dimensions%2C%20enabling%20unified%20evaluation%20with%20systematic%20research%0Aobjectives.%20We%20introduce%20a%20standardized%20evaluation%20system%20built%20upon%20unified%0Ainfrastructure%20supporting%20flexible%20integration%20of%2022%20diverse%20benchmarks%20across%0Athree%20domains%20%282D/3D%20Embodied%20Q%26A%2C%20Navigation%2C%20Task%20Planning%29%20and%2030%2B%20advanced%0Amodels%20from%2020%2B%20worldwide%20institutes.%20Additionally%2C%20we%20develop%20a%20novel%0ALLM-driven%20automated%20generation%20pipeline%20ensuring%20scalable%20embodied%20evaluation%0Adata%20with%20continuous%20evolution%20for%20diversity%20and%20comprehensiveness.%20Embodied%0AArena%20publishes%20three%20real-time%20leaderboards%20%28Embodied%20Q%26A%2C%20Navigation%2C%20Task%0APlanning%29%20with%20dual%20perspectives%20%28benchmark%20view%20and%20capability%20view%29%2C%0Aproviding%20comprehensive%20overviews%20of%20advanced%20model%20capabilities.%20Especially%2C%0Awe%20present%20nine%20findings%20summarized%20from%20the%20evaluation%20results%20on%20the%0Aleaderboards%20of%20Embodied%20Arena.%20This%20helps%20to%20establish%20clear%20research%20veins%0Aand%20pinpoint%20critical%20research%20problems%2C%20thereby%20driving%20forward%20progress%20in%0Athe%20field%20of%20Embodied%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15273v2&entry.124074799=Read"},
{"title": "Moir\u00e9Net: A Compact Dual-Domain Network for Image Demoir\u00e9ing", "author": "Shuwei Guo and Simin Luan and Yan Ke and Zeyd Boukhers and John See and Cong Yang", "abstract": "  Moir\\'e patterns arise from spectral aliasing between display pixel lattices\nand camera sensor grids, manifesting as anisotropic, multi-scale artifacts that\npose significant challenges for digital image demoir\\'eing. We propose\nMoir\\'eNet, a convolutional neural U-Net-based framework that synergistically\nintegrates frequency and spatial domain features for effective artifact\nremoval. Moir\\'eNet introduces two key components: a Directional\nFrequency-Spatial Encoder (DFSE) that discerns moir\\'e orientation via\ndirectional difference convolution, and a Frequency-Spatial Adaptive Selector\n(FSAS) that enables precise, feature-adaptive suppression. Extensive\nexperiments demonstrate that Moir\\'eNet achieves state-of-the-art performance\non public and actively used datasets while being highly parameter-efficient.\nWith only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,\nMoir\\'eNet combines superior restoration quality with parameter efficiency,\nmaking it well-suited for resource-constrained applications including\nsmartphone photography, industrial imaging, and augmented reality.\n", "link": "http://arxiv.org/abs/2509.18910v1", "date": "2025-09-23", "relevancy": 2.1723, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.567}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5391}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moir%C3%A9Net%3A%20A%20Compact%20Dual-Domain%20Network%20for%20Image%20Demoir%C3%A9ing&body=Title%3A%20Moir%C3%A9Net%3A%20A%20Compact%20Dual-Domain%20Network%20for%20Image%20Demoir%C3%A9ing%0AAuthor%3A%20Shuwei%20Guo%20and%20Simin%20Luan%20and%20Yan%20Ke%20and%20Zeyd%20Boukhers%20and%20John%20See%20and%20Cong%20Yang%0AAbstract%3A%20%20%20Moir%5C%27e%20patterns%20arise%20from%20spectral%20aliasing%20between%20display%20pixel%20lattices%0Aand%20camera%20sensor%20grids%2C%20manifesting%20as%20anisotropic%2C%20multi-scale%20artifacts%20that%0Apose%20significant%20challenges%20for%20digital%20image%20demoir%5C%27eing.%20We%20propose%0AMoir%5C%27eNet%2C%20a%20convolutional%20neural%20U-Net-based%20framework%20that%20synergistically%0Aintegrates%20frequency%20and%20spatial%20domain%20features%20for%20effective%20artifact%0Aremoval.%20Moir%5C%27eNet%20introduces%20two%20key%20components%3A%20a%20Directional%0AFrequency-Spatial%20Encoder%20%28DFSE%29%20that%20discerns%20moir%5C%27e%20orientation%20via%0Adirectional%20difference%20convolution%2C%20and%20a%20Frequency-Spatial%20Adaptive%20Selector%0A%28FSAS%29%20that%20enables%20precise%2C%20feature-adaptive%20suppression.%20Extensive%0Aexperiments%20demonstrate%20that%20Moir%5C%27eNet%20achieves%20state-of-the-art%20performance%0Aon%20public%20and%20actively%20used%20datasets%20while%20being%20highly%20parameter-efficient.%0AWith%20only%205.513M%20parameters%2C%20representing%20a%2048%25%20reduction%20compared%20to%20ESDNet-L%2C%0AMoir%5C%27eNet%20combines%20superior%20restoration%20quality%20with%20parameter%20efficiency%2C%0Amaking%20it%20well-suited%20for%20resource-constrained%20applications%20including%0Asmartphone%20photography%2C%20industrial%20imaging%2C%20and%20augmented%20reality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoir%25C3%25A9Net%253A%2520A%2520Compact%2520Dual-Domain%2520Network%2520for%2520Image%2520Demoir%25C3%25A9ing%26entry.906535625%3DShuwei%2520Guo%2520and%2520Simin%2520Luan%2520and%2520Yan%2520Ke%2520and%2520Zeyd%2520Boukhers%2520and%2520John%2520See%2520and%2520Cong%2520Yang%26entry.1292438233%3D%2520%2520Moir%255C%2527e%2520patterns%2520arise%2520from%2520spectral%2520aliasing%2520between%2520display%2520pixel%2520lattices%250Aand%2520camera%2520sensor%2520grids%252C%2520manifesting%2520as%2520anisotropic%252C%2520multi-scale%2520artifacts%2520that%250Apose%2520significant%2520challenges%2520for%2520digital%2520image%2520demoir%255C%2527eing.%2520We%2520propose%250AMoir%255C%2527eNet%252C%2520a%2520convolutional%2520neural%2520U-Net-based%2520framework%2520that%2520synergistically%250Aintegrates%2520frequency%2520and%2520spatial%2520domain%2520features%2520for%2520effective%2520artifact%250Aremoval.%2520Moir%255C%2527eNet%2520introduces%2520two%2520key%2520components%253A%2520a%2520Directional%250AFrequency-Spatial%2520Encoder%2520%2528DFSE%2529%2520that%2520discerns%2520moir%255C%2527e%2520orientation%2520via%250Adirectional%2520difference%2520convolution%252C%2520and%2520a%2520Frequency-Spatial%2520Adaptive%2520Selector%250A%2528FSAS%2529%2520that%2520enables%2520precise%252C%2520feature-adaptive%2520suppression.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520Moir%255C%2527eNet%2520achieves%2520state-of-the-art%2520performance%250Aon%2520public%2520and%2520actively%2520used%2520datasets%2520while%2520being%2520highly%2520parameter-efficient.%250AWith%2520only%25205.513M%2520parameters%252C%2520representing%2520a%252048%2525%2520reduction%2520compared%2520to%2520ESDNet-L%252C%250AMoir%255C%2527eNet%2520combines%2520superior%2520restoration%2520quality%2520with%2520parameter%2520efficiency%252C%250Amaking%2520it%2520well-suited%2520for%2520resource-constrained%2520applications%2520including%250Asmartphone%2520photography%252C%2520industrial%2520imaging%252C%2520and%2520augmented%2520reality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moir%C3%A9Net%3A%20A%20Compact%20Dual-Domain%20Network%20for%20Image%20Demoir%C3%A9ing&entry.906535625=Shuwei%20Guo%20and%20Simin%20Luan%20and%20Yan%20Ke%20and%20Zeyd%20Boukhers%20and%20John%20See%20and%20Cong%20Yang&entry.1292438233=%20%20Moir%5C%27e%20patterns%20arise%20from%20spectral%20aliasing%20between%20display%20pixel%20lattices%0Aand%20camera%20sensor%20grids%2C%20manifesting%20as%20anisotropic%2C%20multi-scale%20artifacts%20that%0Apose%20significant%20challenges%20for%20digital%20image%20demoir%5C%27eing.%20We%20propose%0AMoir%5C%27eNet%2C%20a%20convolutional%20neural%20U-Net-based%20framework%20that%20synergistically%0Aintegrates%20frequency%20and%20spatial%20domain%20features%20for%20effective%20artifact%0Aremoval.%20Moir%5C%27eNet%20introduces%20two%20key%20components%3A%20a%20Directional%0AFrequency-Spatial%20Encoder%20%28DFSE%29%20that%20discerns%20moir%5C%27e%20orientation%20via%0Adirectional%20difference%20convolution%2C%20and%20a%20Frequency-Spatial%20Adaptive%20Selector%0A%28FSAS%29%20that%20enables%20precise%2C%20feature-adaptive%20suppression.%20Extensive%0Aexperiments%20demonstrate%20that%20Moir%5C%27eNet%20achieves%20state-of-the-art%20performance%0Aon%20public%20and%20actively%20used%20datasets%20while%20being%20highly%20parameter-efficient.%0AWith%20only%205.513M%20parameters%2C%20representing%20a%2048%25%20reduction%20compared%20to%20ESDNet-L%2C%0AMoir%5C%27eNet%20combines%20superior%20restoration%20quality%20with%20parameter%20efficiency%2C%0Amaking%20it%20well-suited%20for%20resource-constrained%20applications%20including%0Asmartphone%20photography%2C%20industrial%20imaging%2C%20and%20augmented%20reality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18910v1&entry.124074799=Read"},
{"title": "Towards Interpretable and Efficient Attention: Compressing All by\n  Contracting a Few", "author": "Qishuai Wen and Zhiyuan Huang and Chun-Guang Li", "abstract": "  Attention mechanisms in Transformers have gained significant empirical\nsuccess. Nonetheless, the optimization objectives underlying their forward pass\nare still unclear. Additionally, the quadratic complexity of self-attention is\nincreasingly prohibitive. Unlike the prior work on addressing the\ninterpretability or efficiency issue separately, we propose a unified\noptimization objective to alleviate both issues simultaneously. By unrolling\nthe optimization over the objective, we derive an inherently interpretable and\nefficient attention mechanism, which compresses all tokens into low-dimensional\nstructures by contracting a few representative tokens and then broadcasting the\ncontractions back. This Contract-and-Broadcast Self-Attention (CBSA) mechanism\ncan not only scale linearly but also generalize existing attention mechanisms\nas its special cases. Experiments further demonstrate comparable performance\nand even superior advantages of CBSA on several visual tasks. Code is available\nat this https URL.\n", "link": "http://arxiv.org/abs/2509.16875v2", "date": "2025-09-23", "relevancy": 2.1669, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6033}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5174}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interpretable%20and%20Efficient%20Attention%3A%20Compressing%20All%20by%0A%20%20Contracting%20a%20Few&body=Title%3A%20Towards%20Interpretable%20and%20Efficient%20Attention%3A%20Compressing%20All%20by%0A%20%20Contracting%20a%20Few%0AAuthor%3A%20Qishuai%20Wen%20and%20Zhiyuan%20Huang%20and%20Chun-Guang%20Li%0AAbstract%3A%20%20%20Attention%20mechanisms%20in%20Transformers%20have%20gained%20significant%20empirical%0Asuccess.%20Nonetheless%2C%20the%20optimization%20objectives%20underlying%20their%20forward%20pass%0Aare%20still%20unclear.%20Additionally%2C%20the%20quadratic%20complexity%20of%20self-attention%20is%0Aincreasingly%20prohibitive.%20Unlike%20the%20prior%20work%20on%20addressing%20the%0Ainterpretability%20or%20efficiency%20issue%20separately%2C%20we%20propose%20a%20unified%0Aoptimization%20objective%20to%20alleviate%20both%20issues%20simultaneously.%20By%20unrolling%0Athe%20optimization%20over%20the%20objective%2C%20we%20derive%20an%20inherently%20interpretable%20and%0Aefficient%20attention%20mechanism%2C%20which%20compresses%20all%20tokens%20into%20low-dimensional%0Astructures%20by%20contracting%20a%20few%20representative%20tokens%20and%20then%20broadcasting%20the%0Acontractions%20back.%20This%20Contract-and-Broadcast%20Self-Attention%20%28CBSA%29%20mechanism%0Acan%20not%20only%20scale%20linearly%20but%20also%20generalize%20existing%20attention%20mechanisms%0Aas%20its%20special%20cases.%20Experiments%20further%20demonstrate%20comparable%20performance%0Aand%20even%20superior%20advantages%20of%20CBSA%20on%20several%20visual%20tasks.%20Code%20is%20available%0Aat%20this%20https%20URL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16875v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interpretable%2520and%2520Efficient%2520Attention%253A%2520Compressing%2520All%2520by%250A%2520%2520Contracting%2520a%2520Few%26entry.906535625%3DQishuai%2520Wen%2520and%2520Zhiyuan%2520Huang%2520and%2520Chun-Guang%2520Li%26entry.1292438233%3D%2520%2520Attention%2520mechanisms%2520in%2520Transformers%2520have%2520gained%2520significant%2520empirical%250Asuccess.%2520Nonetheless%252C%2520the%2520optimization%2520objectives%2520underlying%2520their%2520forward%2520pass%250Aare%2520still%2520unclear.%2520Additionally%252C%2520the%2520quadratic%2520complexity%2520of%2520self-attention%2520is%250Aincreasingly%2520prohibitive.%2520Unlike%2520the%2520prior%2520work%2520on%2520addressing%2520the%250Ainterpretability%2520or%2520efficiency%2520issue%2520separately%252C%2520we%2520propose%2520a%2520unified%250Aoptimization%2520objective%2520to%2520alleviate%2520both%2520issues%2520simultaneously.%2520By%2520unrolling%250Athe%2520optimization%2520over%2520the%2520objective%252C%2520we%2520derive%2520an%2520inherently%2520interpretable%2520and%250Aefficient%2520attention%2520mechanism%252C%2520which%2520compresses%2520all%2520tokens%2520into%2520low-dimensional%250Astructures%2520by%2520contracting%2520a%2520few%2520representative%2520tokens%2520and%2520then%2520broadcasting%2520the%250Acontractions%2520back.%2520This%2520Contract-and-Broadcast%2520Self-Attention%2520%2528CBSA%2529%2520mechanism%250Acan%2520not%2520only%2520scale%2520linearly%2520but%2520also%2520generalize%2520existing%2520attention%2520mechanisms%250Aas%2520its%2520special%2520cases.%2520Experiments%2520further%2520demonstrate%2520comparable%2520performance%250Aand%2520even%2520superior%2520advantages%2520of%2520CBSA%2520on%2520several%2520visual%2520tasks.%2520Code%2520is%2520available%250Aat%2520this%2520https%2520URL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16875v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interpretable%20and%20Efficient%20Attention%3A%20Compressing%20All%20by%0A%20%20Contracting%20a%20Few&entry.906535625=Qishuai%20Wen%20and%20Zhiyuan%20Huang%20and%20Chun-Guang%20Li&entry.1292438233=%20%20Attention%20mechanisms%20in%20Transformers%20have%20gained%20significant%20empirical%0Asuccess.%20Nonetheless%2C%20the%20optimization%20objectives%20underlying%20their%20forward%20pass%0Aare%20still%20unclear.%20Additionally%2C%20the%20quadratic%20complexity%20of%20self-attention%20is%0Aincreasingly%20prohibitive.%20Unlike%20the%20prior%20work%20on%20addressing%20the%0Ainterpretability%20or%20efficiency%20issue%20separately%2C%20we%20propose%20a%20unified%0Aoptimization%20objective%20to%20alleviate%20both%20issues%20simultaneously.%20By%20unrolling%0Athe%20optimization%20over%20the%20objective%2C%20we%20derive%20an%20inherently%20interpretable%20and%0Aefficient%20attention%20mechanism%2C%20which%20compresses%20all%20tokens%20into%20low-dimensional%0Astructures%20by%20contracting%20a%20few%20representative%20tokens%20and%20then%20broadcasting%20the%0Acontractions%20back.%20This%20Contract-and-Broadcast%20Self-Attention%20%28CBSA%29%20mechanism%0Acan%20not%20only%20scale%20linearly%20but%20also%20generalize%20existing%20attention%20mechanisms%0Aas%20its%20special%20cases.%20Experiments%20further%20demonstrate%20comparable%20performance%0Aand%20even%20superior%20advantages%20of%20CBSA%20on%20several%20visual%20tasks.%20Code%20is%20available%0Aat%20this%20https%20URL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16875v2&entry.124074799=Read"},
{"title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level\n  RL", "author": "Hanyi Mao and Quanjia Xiao and Lei Pang and Haixiao Liu", "abstract": "  We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping on\nthe importance-sampling (IS) weight. We study RL methods with sequence-level IS\nand identify a mismatch when PPO/GRPO-style clipping is transplanted to\nsequences: a fixed clip range systematically reweights short vs.\\ long\nresponses, distorting the optimization direction. FSPO introduces a simple\nremedy: we clip the sequence log-IS ratio with a band that scales as\n$\\sqrt{L}$. Theoretically, we formalize length fairness via a Length\nReweighting Error (LRE) and prove that small LRE yields a cosine directional\nguarantee between the clipped and true updates. Empirically, FSPO flattens clip\nrates across length bins, stabilizes training, and outperforms all baselines\nacross multiple evaluation datasets on Qwen3-8B-Base model.\n", "link": "http://arxiv.org/abs/2509.09177v2", "date": "2025-09-23", "relevancy": 2.1658, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4594}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4241}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clip%20Your%20Sequences%20Fairly%3A%20Enforcing%20Length%20Fairness%20for%20Sequence-Level%0A%20%20RL&body=Title%3A%20Clip%20Your%20Sequences%20Fairly%3A%20Enforcing%20Length%20Fairness%20for%20Sequence-Level%0A%20%20RL%0AAuthor%3A%20Hanyi%20Mao%20and%20Quanjia%20Xiao%20and%20Lei%20Pang%20and%20Haixiao%20Liu%0AAbstract%3A%20%20%20We%20propose%20FSPO%20%28Fair%20Sequence%20Policy%20Optimization%29%2C%20a%20sequence-level%0Areinforcement%20learning%20method%20for%20LLMs%20that%20enforces%20length-fair%20clipping%20on%0Athe%20importance-sampling%20%28IS%29%20weight.%20We%20study%20RL%20methods%20with%20sequence-level%20IS%0Aand%20identify%20a%20mismatch%20when%20PPO/GRPO-style%20clipping%20is%20transplanted%20to%0Asequences%3A%20a%20fixed%20clip%20range%20systematically%20reweights%20short%20vs.%5C%20long%0Aresponses%2C%20distorting%20the%20optimization%20direction.%20FSPO%20introduces%20a%20simple%0Aremedy%3A%20we%20clip%20the%20sequence%20log-IS%20ratio%20with%20a%20band%20that%20scales%20as%0A%24%5Csqrt%7BL%7D%24.%20Theoretically%2C%20we%20formalize%20length%20fairness%20via%20a%20Length%0AReweighting%20Error%20%28LRE%29%20and%20prove%20that%20small%20LRE%20yields%20a%20cosine%20directional%0Aguarantee%20between%20the%20clipped%20and%20true%20updates.%20Empirically%2C%20FSPO%20flattens%20clip%0Arates%20across%20length%20bins%2C%20stabilizes%20training%2C%20and%20outperforms%20all%20baselines%0Aacross%20multiple%20evaluation%20datasets%20on%20Qwen3-8B-Base%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09177v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClip%2520Your%2520Sequences%2520Fairly%253A%2520Enforcing%2520Length%2520Fairness%2520for%2520Sequence-Level%250A%2520%2520RL%26entry.906535625%3DHanyi%2520Mao%2520and%2520Quanjia%2520Xiao%2520and%2520Lei%2520Pang%2520and%2520Haixiao%2520Liu%26entry.1292438233%3D%2520%2520We%2520propose%2520FSPO%2520%2528Fair%2520Sequence%2520Policy%2520Optimization%2529%252C%2520a%2520sequence-level%250Areinforcement%2520learning%2520method%2520for%2520LLMs%2520that%2520enforces%2520length-fair%2520clipping%2520on%250Athe%2520importance-sampling%2520%2528IS%2529%2520weight.%2520We%2520study%2520RL%2520methods%2520with%2520sequence-level%2520IS%250Aand%2520identify%2520a%2520mismatch%2520when%2520PPO/GRPO-style%2520clipping%2520is%2520transplanted%2520to%250Asequences%253A%2520a%2520fixed%2520clip%2520range%2520systematically%2520reweights%2520short%2520vs.%255C%2520long%250Aresponses%252C%2520distorting%2520the%2520optimization%2520direction.%2520FSPO%2520introduces%2520a%2520simple%250Aremedy%253A%2520we%2520clip%2520the%2520sequence%2520log-IS%2520ratio%2520with%2520a%2520band%2520that%2520scales%2520as%250A%2524%255Csqrt%257BL%257D%2524.%2520Theoretically%252C%2520we%2520formalize%2520length%2520fairness%2520via%2520a%2520Length%250AReweighting%2520Error%2520%2528LRE%2529%2520and%2520prove%2520that%2520small%2520LRE%2520yields%2520a%2520cosine%2520directional%250Aguarantee%2520between%2520the%2520clipped%2520and%2520true%2520updates.%2520Empirically%252C%2520FSPO%2520flattens%2520clip%250Arates%2520across%2520length%2520bins%252C%2520stabilizes%2520training%252C%2520and%2520outperforms%2520all%2520baselines%250Aacross%2520multiple%2520evaluation%2520datasets%2520on%2520Qwen3-8B-Base%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09177v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clip%20Your%20Sequences%20Fairly%3A%20Enforcing%20Length%20Fairness%20for%20Sequence-Level%0A%20%20RL&entry.906535625=Hanyi%20Mao%20and%20Quanjia%20Xiao%20and%20Lei%20Pang%20and%20Haixiao%20Liu&entry.1292438233=%20%20We%20propose%20FSPO%20%28Fair%20Sequence%20Policy%20Optimization%29%2C%20a%20sequence-level%0Areinforcement%20learning%20method%20for%20LLMs%20that%20enforces%20length-fair%20clipping%20on%0Athe%20importance-sampling%20%28IS%29%20weight.%20We%20study%20RL%20methods%20with%20sequence-level%20IS%0Aand%20identify%20a%20mismatch%20when%20PPO/GRPO-style%20clipping%20is%20transplanted%20to%0Asequences%3A%20a%20fixed%20clip%20range%20systematically%20reweights%20short%20vs.%5C%20long%0Aresponses%2C%20distorting%20the%20optimization%20direction.%20FSPO%20introduces%20a%20simple%0Aremedy%3A%20we%20clip%20the%20sequence%20log-IS%20ratio%20with%20a%20band%20that%20scales%20as%0A%24%5Csqrt%7BL%7D%24.%20Theoretically%2C%20we%20formalize%20length%20fairness%20via%20a%20Length%0AReweighting%20Error%20%28LRE%29%20and%20prove%20that%20small%20LRE%20yields%20a%20cosine%20directional%0Aguarantee%20between%20the%20clipped%20and%20true%20updates.%20Empirically%2C%20FSPO%20flattens%20clip%0Arates%20across%20length%20bins%2C%20stabilizes%20training%2C%20and%20outperforms%20all%20baselines%0Aacross%20multiple%20evaluation%20datasets%20on%20Qwen3-8B-Base%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09177v2&entry.124074799=Read"},
{"title": "Investigating Traffic Accident Detection Using Multimodal Large Language\n  Models", "author": "Ilhan Skender and Kailin Tong and Selim Solmaz and Daniel Watzenig", "abstract": "  Traffic safety remains a critical global concern, with timely and accurate\naccident detection essential for hazard reduction and rapid emergency response.\nInfrastructure-based vision sensors offer scalable and efficient solutions for\ncontinuous real-time monitoring, facilitating automated detection of acci-\ndents directly from captured images. This research investigates the zero-shot\ncapabilities of multimodal large language models (MLLMs) for detecting and\ndescribing traffic accidents using images from infrastructure cameras, thus\nminimizing reliance on extensive labeled datasets. Main contributions include:\n(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,\nexplicitly addressing the scarcity of diverse, realistic, infrastructure-based\naccident data through controlled simulations; (2) Comparative performance\nanalysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent\nidentification and descriptive capabilities without prior fine-tuning; and (3)\nIntegration of advanced visual analytics, specifically YOLO for object\ndetection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for\ninstance segmentation, into enhanced prompts to improve model accuracy and\nexplainability. Key numerical results show Pixtral as the top performer with an\nF1-score of 0.71 and 83% recall, while Gemini models gained precision with\nenhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and\nrecall losses. Gemma 3 offered the most balanced performance with minimal\nmetric fluctuation. These findings demonstrate the substantial potential of\nintegrating MLLMs with advanced visual analytics techniques, enhancing their\napplicability in real-world automated traffic monitoring systems.\n", "link": "http://arxiv.org/abs/2509.19096v1", "date": "2025-09-23", "relevancy": 2.1629, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5553}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5334}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Traffic%20Accident%20Detection%20Using%20Multimodal%20Large%20Language%0A%20%20Models&body=Title%3A%20Investigating%20Traffic%20Accident%20Detection%20Using%20Multimodal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Ilhan%20Skender%20and%20Kailin%20Tong%20and%20Selim%20Solmaz%20and%20Daniel%20Watzenig%0AAbstract%3A%20%20%20Traffic%20safety%20remains%20a%20critical%20global%20concern%2C%20with%20timely%20and%20accurate%0Aaccident%20detection%20essential%20for%20hazard%20reduction%20and%20rapid%20emergency%20response.%0AInfrastructure-based%20vision%20sensors%20offer%20scalable%20and%20efficient%20solutions%20for%0Acontinuous%20real-time%20monitoring%2C%20facilitating%20automated%20detection%20of%20acci-%0Adents%20directly%20from%20captured%20images.%20This%20research%20investigates%20the%20zero-shot%0Acapabilities%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20for%20detecting%20and%0Adescribing%20traffic%20accidents%20using%20images%20from%20infrastructure%20cameras%2C%20thus%0Aminimizing%20reliance%20on%20extensive%20labeled%20datasets.%20Main%20contributions%20include%3A%0A%281%29%20Evaluation%20of%20MLLMs%20using%20the%20simulated%20DeepAccident%20dataset%20from%20CARLA%2C%0Aexplicitly%20addressing%20the%20scarcity%20of%20diverse%2C%20realistic%2C%20infrastructure-based%0Aaccident%20data%20through%20controlled%20simulations%3B%20%282%29%20Comparative%20performance%0Aanalysis%20between%20Gemini%201.5%20and%202.0%2C%20Gemma%203%20and%20Pixtral%20models%20in%20acci-%20dent%0Aidentification%20and%20descriptive%20capabilities%20without%20prior%20fine-tuning%3B%20and%20%283%29%0AIntegration%20of%20advanced%20visual%20analytics%2C%20specifically%20YOLO%20for%20object%0Adetection%2C%20Deep%20SORT%20for%20multi-%20object%20tracking%2C%20and%20Segment%20Anything%20%28SAM%29%20for%0Ainstance%20segmentation%2C%20into%20enhanced%20prompts%20to%20improve%20model%20accuracy%20and%0Aexplainability.%20Key%20numerical%20results%20show%20Pixtral%20as%20the%20top%20performer%20with%20an%0AF1-score%20of%200.71%20and%2083%25%20recall%2C%20while%20Gemini%20models%20gained%20precision%20with%0Aenhanced%20prompts%20%28e.g.%2C%20Gemini%201.5%20rose%20to%2090%25%29%20but%20suffered%20notable%20F1%20and%0Arecall%20losses.%20Gemma%203%20offered%20the%20most%20balanced%20performance%20with%20minimal%0Ametric%20fluctuation.%20These%20findings%20demonstrate%20the%20substantial%20potential%20of%0Aintegrating%20MLLMs%20with%20advanced%20visual%20analytics%20techniques%2C%20enhancing%20their%0Aapplicability%20in%20real-world%20automated%20traffic%20monitoring%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Traffic%2520Accident%2520Detection%2520Using%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DIlhan%2520Skender%2520and%2520Kailin%2520Tong%2520and%2520Selim%2520Solmaz%2520and%2520Daniel%2520Watzenig%26entry.1292438233%3D%2520%2520Traffic%2520safety%2520remains%2520a%2520critical%2520global%2520concern%252C%2520with%2520timely%2520and%2520accurate%250Aaccident%2520detection%2520essential%2520for%2520hazard%2520reduction%2520and%2520rapid%2520emergency%2520response.%250AInfrastructure-based%2520vision%2520sensors%2520offer%2520scalable%2520and%2520efficient%2520solutions%2520for%250Acontinuous%2520real-time%2520monitoring%252C%2520facilitating%2520automated%2520detection%2520of%2520acci-%250Adents%2520directly%2520from%2520captured%2520images.%2520This%2520research%2520investigates%2520the%2520zero-shot%250Acapabilities%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520for%2520detecting%2520and%250Adescribing%2520traffic%2520accidents%2520using%2520images%2520from%2520infrastructure%2520cameras%252C%2520thus%250Aminimizing%2520reliance%2520on%2520extensive%2520labeled%2520datasets.%2520Main%2520contributions%2520include%253A%250A%25281%2529%2520Evaluation%2520of%2520MLLMs%2520using%2520the%2520simulated%2520DeepAccident%2520dataset%2520from%2520CARLA%252C%250Aexplicitly%2520addressing%2520the%2520scarcity%2520of%2520diverse%252C%2520realistic%252C%2520infrastructure-based%250Aaccident%2520data%2520through%2520controlled%2520simulations%253B%2520%25282%2529%2520Comparative%2520performance%250Aanalysis%2520between%2520Gemini%25201.5%2520and%25202.0%252C%2520Gemma%25203%2520and%2520Pixtral%2520models%2520in%2520acci-%2520dent%250Aidentification%2520and%2520descriptive%2520capabilities%2520without%2520prior%2520fine-tuning%253B%2520and%2520%25283%2529%250AIntegration%2520of%2520advanced%2520visual%2520analytics%252C%2520specifically%2520YOLO%2520for%2520object%250Adetection%252C%2520Deep%2520SORT%2520for%2520multi-%2520object%2520tracking%252C%2520and%2520Segment%2520Anything%2520%2528SAM%2529%2520for%250Ainstance%2520segmentation%252C%2520into%2520enhanced%2520prompts%2520to%2520improve%2520model%2520accuracy%2520and%250Aexplainability.%2520Key%2520numerical%2520results%2520show%2520Pixtral%2520as%2520the%2520top%2520performer%2520with%2520an%250AF1-score%2520of%25200.71%2520and%252083%2525%2520recall%252C%2520while%2520Gemini%2520models%2520gained%2520precision%2520with%250Aenhanced%2520prompts%2520%2528e.g.%252C%2520Gemini%25201.5%2520rose%2520to%252090%2525%2529%2520but%2520suffered%2520notable%2520F1%2520and%250Arecall%2520losses.%2520Gemma%25203%2520offered%2520the%2520most%2520balanced%2520performance%2520with%2520minimal%250Ametric%2520fluctuation.%2520These%2520findings%2520demonstrate%2520the%2520substantial%2520potential%2520of%250Aintegrating%2520MLLMs%2520with%2520advanced%2520visual%2520analytics%2520techniques%252C%2520enhancing%2520their%250Aapplicability%2520in%2520real-world%2520automated%2520traffic%2520monitoring%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Traffic%20Accident%20Detection%20Using%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Ilhan%20Skender%20and%20Kailin%20Tong%20and%20Selim%20Solmaz%20and%20Daniel%20Watzenig&entry.1292438233=%20%20Traffic%20safety%20remains%20a%20critical%20global%20concern%2C%20with%20timely%20and%20accurate%0Aaccident%20detection%20essential%20for%20hazard%20reduction%20and%20rapid%20emergency%20response.%0AInfrastructure-based%20vision%20sensors%20offer%20scalable%20and%20efficient%20solutions%20for%0Acontinuous%20real-time%20monitoring%2C%20facilitating%20automated%20detection%20of%20acci-%0Adents%20directly%20from%20captured%20images.%20This%20research%20investigates%20the%20zero-shot%0Acapabilities%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20for%20detecting%20and%0Adescribing%20traffic%20accidents%20using%20images%20from%20infrastructure%20cameras%2C%20thus%0Aminimizing%20reliance%20on%20extensive%20labeled%20datasets.%20Main%20contributions%20include%3A%0A%281%29%20Evaluation%20of%20MLLMs%20using%20the%20simulated%20DeepAccident%20dataset%20from%20CARLA%2C%0Aexplicitly%20addressing%20the%20scarcity%20of%20diverse%2C%20realistic%2C%20infrastructure-based%0Aaccident%20data%20through%20controlled%20simulations%3B%20%282%29%20Comparative%20performance%0Aanalysis%20between%20Gemini%201.5%20and%202.0%2C%20Gemma%203%20and%20Pixtral%20models%20in%20acci-%20dent%0Aidentification%20and%20descriptive%20capabilities%20without%20prior%20fine-tuning%3B%20and%20%283%29%0AIntegration%20of%20advanced%20visual%20analytics%2C%20specifically%20YOLO%20for%20object%0Adetection%2C%20Deep%20SORT%20for%20multi-%20object%20tracking%2C%20and%20Segment%20Anything%20%28SAM%29%20for%0Ainstance%20segmentation%2C%20into%20enhanced%20prompts%20to%20improve%20model%20accuracy%20and%0Aexplainability.%20Key%20numerical%20results%20show%20Pixtral%20as%20the%20top%20performer%20with%20an%0AF1-score%20of%200.71%20and%2083%25%20recall%2C%20while%20Gemini%20models%20gained%20precision%20with%0Aenhanced%20prompts%20%28e.g.%2C%20Gemini%201.5%20rose%20to%2090%25%29%20but%20suffered%20notable%20F1%20and%0Arecall%20losses.%20Gemma%203%20offered%20the%20most%20balanced%20performance%20with%20minimal%0Ametric%20fluctuation.%20These%20findings%20demonstrate%20the%20substantial%20potential%20of%0Aintegrating%20MLLMs%20with%20advanced%20visual%20analytics%20techniques%2C%20enhancing%20their%0Aapplicability%20in%20real-world%20automated%20traffic%20monitoring%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19096v1&entry.124074799=Read"},
{"title": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding", "author": "Yun-Shiuan Chuang and Sameer Narendran and Nikunj Harlalka and Alexander Cheung and Sizhe Gao and Siddharth Suresh and Junjie Hu and Timothy T. Rogers", "abstract": "  Guesstimation--the task of making approximate quantitative estimates about\nobjects or events-is a common real--world skill, yet remains underexplored in\nlarge language model (LLM) research. We introduce three guesstimation datasets:\nMARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many\nmarbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential\nelection). Inspired by the social science concept of Wisdom of Crowds (WOC)-\nwhere the median of multiple estimates improves accuracy-we propose WOC\ndecoding for LLMs. We replicate WOC effects in human participants and find that\nLLMs exhibit similar benefits: median aggregation across sampled responses\nconsistently improves accuracy over greedy decoding, self-consistency decoding,\nand mean decoding. This suggests that LLMs encode a world model that supports\napproximate reasoning. Our results position guesstimation as a useful probe of\nLLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM\nguesstimation performance on real-world tasks.\n", "link": "http://arxiv.org/abs/2501.17310v4", "date": "2025-09-23", "relevancy": 2.1616, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20LLM%20World%20Models%3A%20Enhancing%20Guesstimation%20with%20Wisdom%20of%20Crowds%0A%20%20Decoding&body=Title%3A%20Probing%20LLM%20World%20Models%3A%20Enhancing%20Guesstimation%20with%20Wisdom%20of%20Crowds%0A%20%20Decoding%0AAuthor%3A%20Yun-Shiuan%20Chuang%20and%20Sameer%20Narendran%20and%20Nikunj%20Harlalka%20and%20Alexander%20Cheung%20and%20Sizhe%20Gao%20and%20Siddharth%20Suresh%20and%20Junjie%20Hu%20and%20Timothy%20T.%20Rogers%0AAbstract%3A%20%20%20Guesstimation--the%20task%20of%20making%20approximate%20quantitative%20estimates%20about%0Aobjects%20or%20events-is%20a%20common%20real--world%20skill%2C%20yet%20remains%20underexplored%20in%0Alarge%20language%20model%20%28LLM%29%20research.%20We%20introduce%20three%20guesstimation%20datasets%3A%0AMARBLES%2C%20FUTURE%2C%20and%20ELECPRED%2C%20spanning%20physical%20estimation%20%28e.g.%2C%20how%20many%0Amarbles%20fit%20in%20a%20cup%29%20to%20abstract%20predictions%20%28e.g.%2C%20the%202024%20U.S.%20presidential%0Aelection%29.%20Inspired%20by%20the%20social%20science%20concept%20of%20Wisdom%20of%20Crowds%20%28WOC%29-%0Awhere%20the%20median%20of%20multiple%20estimates%20improves%20accuracy-we%20propose%20WOC%0Adecoding%20for%20LLMs.%20We%20replicate%20WOC%20effects%20in%20human%20participants%20and%20find%20that%0ALLMs%20exhibit%20similar%20benefits%3A%20median%20aggregation%20across%20sampled%20responses%0Aconsistently%20improves%20accuracy%20over%20greedy%20decoding%2C%20self-consistency%20decoding%2C%0Aand%20mean%20decoding.%20This%20suggests%20that%20LLMs%20encode%20a%20world%20model%20that%20supports%0Aapproximate%20reasoning.%20Our%20results%20position%20guesstimation%20as%20a%20useful%20probe%20of%0ALLM%20world%20knowledge%20and%20highlight%20WOC%20decoding%20as%20a%20strategy%20for%20enhancing%20LLM%0Aguesstimation%20performance%20on%20real-world%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17310v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520LLM%2520World%2520Models%253A%2520Enhancing%2520Guesstimation%2520with%2520Wisdom%2520of%2520Crowds%250A%2520%2520Decoding%26entry.906535625%3DYun-Shiuan%2520Chuang%2520and%2520Sameer%2520Narendran%2520and%2520Nikunj%2520Harlalka%2520and%2520Alexander%2520Cheung%2520and%2520Sizhe%2520Gao%2520and%2520Siddharth%2520Suresh%2520and%2520Junjie%2520Hu%2520and%2520Timothy%2520T.%2520Rogers%26entry.1292438233%3D%2520%2520Guesstimation--the%2520task%2520of%2520making%2520approximate%2520quantitative%2520estimates%2520about%250Aobjects%2520or%2520events-is%2520a%2520common%2520real--world%2520skill%252C%2520yet%2520remains%2520underexplored%2520in%250Alarge%2520language%2520model%2520%2528LLM%2529%2520research.%2520We%2520introduce%2520three%2520guesstimation%2520datasets%253A%250AMARBLES%252C%2520FUTURE%252C%2520and%2520ELECPRED%252C%2520spanning%2520physical%2520estimation%2520%2528e.g.%252C%2520how%2520many%250Amarbles%2520fit%2520in%2520a%2520cup%2529%2520to%2520abstract%2520predictions%2520%2528e.g.%252C%2520the%25202024%2520U.S.%2520presidential%250Aelection%2529.%2520Inspired%2520by%2520the%2520social%2520science%2520concept%2520of%2520Wisdom%2520of%2520Crowds%2520%2528WOC%2529-%250Awhere%2520the%2520median%2520of%2520multiple%2520estimates%2520improves%2520accuracy-we%2520propose%2520WOC%250Adecoding%2520for%2520LLMs.%2520We%2520replicate%2520WOC%2520effects%2520in%2520human%2520participants%2520and%2520find%2520that%250ALLMs%2520exhibit%2520similar%2520benefits%253A%2520median%2520aggregation%2520across%2520sampled%2520responses%250Aconsistently%2520improves%2520accuracy%2520over%2520greedy%2520decoding%252C%2520self-consistency%2520decoding%252C%250Aand%2520mean%2520decoding.%2520This%2520suggests%2520that%2520LLMs%2520encode%2520a%2520world%2520model%2520that%2520supports%250Aapproximate%2520reasoning.%2520Our%2520results%2520position%2520guesstimation%2520as%2520a%2520useful%2520probe%2520of%250ALLM%2520world%2520knowledge%2520and%2520highlight%2520WOC%2520decoding%2520as%2520a%2520strategy%2520for%2520enhancing%2520LLM%250Aguesstimation%2520performance%2520on%2520real-world%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17310v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20LLM%20World%20Models%3A%20Enhancing%20Guesstimation%20with%20Wisdom%20of%20Crowds%0A%20%20Decoding&entry.906535625=Yun-Shiuan%20Chuang%20and%20Sameer%20Narendran%20and%20Nikunj%20Harlalka%20and%20Alexander%20Cheung%20and%20Sizhe%20Gao%20and%20Siddharth%20Suresh%20and%20Junjie%20Hu%20and%20Timothy%20T.%20Rogers&entry.1292438233=%20%20Guesstimation--the%20task%20of%20making%20approximate%20quantitative%20estimates%20about%0Aobjects%20or%20events-is%20a%20common%20real--world%20skill%2C%20yet%20remains%20underexplored%20in%0Alarge%20language%20model%20%28LLM%29%20research.%20We%20introduce%20three%20guesstimation%20datasets%3A%0AMARBLES%2C%20FUTURE%2C%20and%20ELECPRED%2C%20spanning%20physical%20estimation%20%28e.g.%2C%20how%20many%0Amarbles%20fit%20in%20a%20cup%29%20to%20abstract%20predictions%20%28e.g.%2C%20the%202024%20U.S.%20presidential%0Aelection%29.%20Inspired%20by%20the%20social%20science%20concept%20of%20Wisdom%20of%20Crowds%20%28WOC%29-%0Awhere%20the%20median%20of%20multiple%20estimates%20improves%20accuracy-we%20propose%20WOC%0Adecoding%20for%20LLMs.%20We%20replicate%20WOC%20effects%20in%20human%20participants%20and%20find%20that%0ALLMs%20exhibit%20similar%20benefits%3A%20median%20aggregation%20across%20sampled%20responses%0Aconsistently%20improves%20accuracy%20over%20greedy%20decoding%2C%20self-consistency%20decoding%2C%0Aand%20mean%20decoding.%20This%20suggests%20that%20LLMs%20encode%20a%20world%20model%20that%20supports%0Aapproximate%20reasoning.%20Our%20results%20position%20guesstimation%20as%20a%20useful%20probe%20of%0ALLM%20world%20knowledge%20and%20highlight%20WOC%20decoding%20as%20a%20strategy%20for%20enhancing%20LLM%0Aguesstimation%20performance%20on%20real-world%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17310v4&entry.124074799=Read"},
{"title": "Socially Pertinent Robots in Gerontological Healthcare", "author": "Xavier Alameda-Pineda and Angus Addlesee and Daniel Hern\u00e1ndez Garc\u00eda and Chris Reinke and Soraya Arias and Federica Arrigoni and Alex Auternaud and Lauriane Blavette and Cigdem Beyan and Luis Gomez Camara and Ohad Cohen and Alessandro Conti and S\u00e9bastien Dacunha and Christian Dondrup and Yoav Ellinson and Francesco Ferro and Sharon Gannot and Florian Gras and Nancie Gunson and Radu Horaud and Moreno D'Inc\u00e0 and Imad Kimouche and S\u00e9verin Lemaignan and Oliver Lemon and Cyril Liotard and Luca Marchionni and Mordehay Moradi and Tomas Pajdla and Maribel Pino and Michal Polic and Matthieu Py and Ariel Rado and Bin Ren and Elisa Ricci and Anne-Sophie Rigaud and Paolo Rota and Marta Romeo and Nicu Sebe and Weronika Siei\u0144ska and Pinchas Tandeitnik and Francesco Tonini and Nicolas Turro and Timoth\u00e9e Wintz and Yanchao Yu", "abstract": "  Despite the many recent achievements in developing and deploying social\nrobotics, there are still many underexplored environments and applications for\nwhich systematic evaluation of such systems by end-users is necessary. While\nseveral robotic platforms have been used in gerontological healthcare, the\nquestion of whether or not a social interactive robot with multi-modal\nconversational capabilities will be useful and accepted in real-life facilities\nis yet to be answered. This paper is an attempt to partially answer this\nquestion, via two waves of experiments with patients and companions in a\nday-care gerontological facility in Paris with a full-sized humanoid robot\nendowed with social and conversational interaction capabilities. The software\narchitecture, developed during the H2020 SPRING project, together with the\nexperimental protocol, allowed us to evaluate the acceptability (AES) and\nusability (SUS) with more than 60 end-users. Overall, the users are receptive\nto this technology, especially when the robot perception and action skills are\nrobust to environmental clutter and flexible to handle a plethora of different\ninteractions.\n", "link": "http://arxiv.org/abs/2404.07560v3", "date": "2025-09-23", "relevancy": 1.6946, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5752}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.571}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Socially%20Pertinent%20Robots%20in%20Gerontological%20Healthcare&body=Title%3A%20Socially%20Pertinent%20Robots%20in%20Gerontological%20Healthcare%0AAuthor%3A%20Xavier%20Alameda-Pineda%20and%20Angus%20Addlesee%20and%20Daniel%20Hern%C3%A1ndez%20Garc%C3%ADa%20and%20Chris%20Reinke%20and%20Soraya%20Arias%20and%20Federica%20Arrigoni%20and%20Alex%20Auternaud%20and%20Lauriane%20Blavette%20and%20Cigdem%20Beyan%20and%20Luis%20Gomez%20Camara%20and%20Ohad%20Cohen%20and%20Alessandro%20Conti%20and%20S%C3%A9bastien%20Dacunha%20and%20Christian%20Dondrup%20and%20Yoav%20Ellinson%20and%20Francesco%20Ferro%20and%20Sharon%20Gannot%20and%20Florian%20Gras%20and%20Nancie%20Gunson%20and%20Radu%20Horaud%20and%20Moreno%20D%27Inc%C3%A0%20and%20Imad%20Kimouche%20and%20S%C3%A9verin%20Lemaignan%20and%20Oliver%20Lemon%20and%20Cyril%20Liotard%20and%20Luca%20Marchionni%20and%20Mordehay%20Moradi%20and%20Tomas%20Pajdla%20and%20Maribel%20Pino%20and%20Michal%20Polic%20and%20Matthieu%20Py%20and%20Ariel%20Rado%20and%20Bin%20Ren%20and%20Elisa%20Ricci%20and%20Anne-Sophie%20Rigaud%20and%20Paolo%20Rota%20and%20Marta%20Romeo%20and%20Nicu%20Sebe%20and%20Weronika%20Siei%C5%84ska%20and%20Pinchas%20Tandeitnik%20and%20Francesco%20Tonini%20and%20Nicolas%20Turro%20and%20Timoth%C3%A9e%20Wintz%20and%20Yanchao%20Yu%0AAbstract%3A%20%20%20Despite%20the%20many%20recent%20achievements%20in%20developing%20and%20deploying%20social%0Arobotics%2C%20there%20are%20still%20many%20underexplored%20environments%20and%20applications%20for%0Awhich%20systematic%20evaluation%20of%20such%20systems%20by%20end-users%20is%20necessary.%20While%0Aseveral%20robotic%20platforms%20have%20been%20used%20in%20gerontological%20healthcare%2C%20the%0Aquestion%20of%20whether%20or%20not%20a%20social%20interactive%20robot%20with%20multi-modal%0Aconversational%20capabilities%20will%20be%20useful%20and%20accepted%20in%20real-life%20facilities%0Ais%20yet%20to%20be%20answered.%20This%20paper%20is%20an%20attempt%20to%20partially%20answer%20this%0Aquestion%2C%20via%20two%20waves%20of%20experiments%20with%20patients%20and%20companions%20in%20a%0Aday-care%20gerontological%20facility%20in%20Paris%20with%20a%20full-sized%20humanoid%20robot%0Aendowed%20with%20social%20and%20conversational%20interaction%20capabilities.%20The%20software%0Aarchitecture%2C%20developed%20during%20the%20H2020%20SPRING%20project%2C%20together%20with%20the%0Aexperimental%20protocol%2C%20allowed%20us%20to%20evaluate%20the%20acceptability%20%28AES%29%20and%0Ausability%20%28SUS%29%20with%20more%20than%2060%20end-users.%20Overall%2C%20the%20users%20are%20receptive%0Ato%20this%20technology%2C%20especially%20when%20the%20robot%20perception%20and%20action%20skills%20are%0Arobust%20to%20environmental%20clutter%20and%20flexible%20to%20handle%20a%20plethora%20of%20different%0Ainteractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07560v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocially%2520Pertinent%2520Robots%2520in%2520Gerontological%2520Healthcare%26entry.906535625%3DXavier%2520Alameda-Pineda%2520and%2520Angus%2520Addlesee%2520and%2520Daniel%2520Hern%25C3%25A1ndez%2520Garc%25C3%25ADa%2520and%2520Chris%2520Reinke%2520and%2520Soraya%2520Arias%2520and%2520Federica%2520Arrigoni%2520and%2520Alex%2520Auternaud%2520and%2520Lauriane%2520Blavette%2520and%2520Cigdem%2520Beyan%2520and%2520Luis%2520Gomez%2520Camara%2520and%2520Ohad%2520Cohen%2520and%2520Alessandro%2520Conti%2520and%2520S%25C3%25A9bastien%2520Dacunha%2520and%2520Christian%2520Dondrup%2520and%2520Yoav%2520Ellinson%2520and%2520Francesco%2520Ferro%2520and%2520Sharon%2520Gannot%2520and%2520Florian%2520Gras%2520and%2520Nancie%2520Gunson%2520and%2520Radu%2520Horaud%2520and%2520Moreno%2520D%2527Inc%25C3%25A0%2520and%2520Imad%2520Kimouche%2520and%2520S%25C3%25A9verin%2520Lemaignan%2520and%2520Oliver%2520Lemon%2520and%2520Cyril%2520Liotard%2520and%2520Luca%2520Marchionni%2520and%2520Mordehay%2520Moradi%2520and%2520Tomas%2520Pajdla%2520and%2520Maribel%2520Pino%2520and%2520Michal%2520Polic%2520and%2520Matthieu%2520Py%2520and%2520Ariel%2520Rado%2520and%2520Bin%2520Ren%2520and%2520Elisa%2520Ricci%2520and%2520Anne-Sophie%2520Rigaud%2520and%2520Paolo%2520Rota%2520and%2520Marta%2520Romeo%2520and%2520Nicu%2520Sebe%2520and%2520Weronika%2520Siei%25C5%2584ska%2520and%2520Pinchas%2520Tandeitnik%2520and%2520Francesco%2520Tonini%2520and%2520Nicolas%2520Turro%2520and%2520Timoth%25C3%25A9e%2520Wintz%2520and%2520Yanchao%2520Yu%26entry.1292438233%3D%2520%2520Despite%2520the%2520many%2520recent%2520achievements%2520in%2520developing%2520and%2520deploying%2520social%250Arobotics%252C%2520there%2520are%2520still%2520many%2520underexplored%2520environments%2520and%2520applications%2520for%250Awhich%2520systematic%2520evaluation%2520of%2520such%2520systems%2520by%2520end-users%2520is%2520necessary.%2520While%250Aseveral%2520robotic%2520platforms%2520have%2520been%2520used%2520in%2520gerontological%2520healthcare%252C%2520the%250Aquestion%2520of%2520whether%2520or%2520not%2520a%2520social%2520interactive%2520robot%2520with%2520multi-modal%250Aconversational%2520capabilities%2520will%2520be%2520useful%2520and%2520accepted%2520in%2520real-life%2520facilities%250Ais%2520yet%2520to%2520be%2520answered.%2520This%2520paper%2520is%2520an%2520attempt%2520to%2520partially%2520answer%2520this%250Aquestion%252C%2520via%2520two%2520waves%2520of%2520experiments%2520with%2520patients%2520and%2520companions%2520in%2520a%250Aday-care%2520gerontological%2520facility%2520in%2520Paris%2520with%2520a%2520full-sized%2520humanoid%2520robot%250Aendowed%2520with%2520social%2520and%2520conversational%2520interaction%2520capabilities.%2520The%2520software%250Aarchitecture%252C%2520developed%2520during%2520the%2520H2020%2520SPRING%2520project%252C%2520together%2520with%2520the%250Aexperimental%2520protocol%252C%2520allowed%2520us%2520to%2520evaluate%2520the%2520acceptability%2520%2528AES%2529%2520and%250Ausability%2520%2528SUS%2529%2520with%2520more%2520than%252060%2520end-users.%2520Overall%252C%2520the%2520users%2520are%2520receptive%250Ato%2520this%2520technology%252C%2520especially%2520when%2520the%2520robot%2520perception%2520and%2520action%2520skills%2520are%250Arobust%2520to%2520environmental%2520clutter%2520and%2520flexible%2520to%2520handle%2520a%2520plethora%2520of%2520different%250Ainteractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07560v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Socially%20Pertinent%20Robots%20in%20Gerontological%20Healthcare&entry.906535625=Xavier%20Alameda-Pineda%20and%20Angus%20Addlesee%20and%20Daniel%20Hern%C3%A1ndez%20Garc%C3%ADa%20and%20Chris%20Reinke%20and%20Soraya%20Arias%20and%20Federica%20Arrigoni%20and%20Alex%20Auternaud%20and%20Lauriane%20Blavette%20and%20Cigdem%20Beyan%20and%20Luis%20Gomez%20Camara%20and%20Ohad%20Cohen%20and%20Alessandro%20Conti%20and%20S%C3%A9bastien%20Dacunha%20and%20Christian%20Dondrup%20and%20Yoav%20Ellinson%20and%20Francesco%20Ferro%20and%20Sharon%20Gannot%20and%20Florian%20Gras%20and%20Nancie%20Gunson%20and%20Radu%20Horaud%20and%20Moreno%20D%27Inc%C3%A0%20and%20Imad%20Kimouche%20and%20S%C3%A9verin%20Lemaignan%20and%20Oliver%20Lemon%20and%20Cyril%20Liotard%20and%20Luca%20Marchionni%20and%20Mordehay%20Moradi%20and%20Tomas%20Pajdla%20and%20Maribel%20Pino%20and%20Michal%20Polic%20and%20Matthieu%20Py%20and%20Ariel%20Rado%20and%20Bin%20Ren%20and%20Elisa%20Ricci%20and%20Anne-Sophie%20Rigaud%20and%20Paolo%20Rota%20and%20Marta%20Romeo%20and%20Nicu%20Sebe%20and%20Weronika%20Siei%C5%84ska%20and%20Pinchas%20Tandeitnik%20and%20Francesco%20Tonini%20and%20Nicolas%20Turro%20and%20Timoth%C3%A9e%20Wintz%20and%20Yanchao%20Yu&entry.1292438233=%20%20Despite%20the%20many%20recent%20achievements%20in%20developing%20and%20deploying%20social%0Arobotics%2C%20there%20are%20still%20many%20underexplored%20environments%20and%20applications%20for%0Awhich%20systematic%20evaluation%20of%20such%20systems%20by%20end-users%20is%20necessary.%20While%0Aseveral%20robotic%20platforms%20have%20been%20used%20in%20gerontological%20healthcare%2C%20the%0Aquestion%20of%20whether%20or%20not%20a%20social%20interactive%20robot%20with%20multi-modal%0Aconversational%20capabilities%20will%20be%20useful%20and%20accepted%20in%20real-life%20facilities%0Ais%20yet%20to%20be%20answered.%20This%20paper%20is%20an%20attempt%20to%20partially%20answer%20this%0Aquestion%2C%20via%20two%20waves%20of%20experiments%20with%20patients%20and%20companions%20in%20a%0Aday-care%20gerontological%20facility%20in%20Paris%20with%20a%20full-sized%20humanoid%20robot%0Aendowed%20with%20social%20and%20conversational%20interaction%20capabilities.%20The%20software%0Aarchitecture%2C%20developed%20during%20the%20H2020%20SPRING%20project%2C%20together%20with%20the%0Aexperimental%20protocol%2C%20allowed%20us%20to%20evaluate%20the%20acceptability%20%28AES%29%20and%0Ausability%20%28SUS%29%20with%20more%20than%2060%20end-users.%20Overall%2C%20the%20users%20are%20receptive%0Ato%20this%20technology%2C%20especially%20when%20the%20robot%20perception%20and%20action%20skills%20are%0Arobust%20to%20environmental%20clutter%20and%20flexible%20to%20handle%20a%20plethora%20of%20different%0Ainteractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07560v3&entry.124074799=Read"},
{"title": "Operator Splitting Covariance Steering for Safe Stochastic Nonlinear\n  Control", "author": "Akash Ratheesh and Vincent Pacelli and Augustinos D. Saravanos and Evangelos A. Theodorou", "abstract": "  This paper presents a novel algorithm for solving distribution steering\nproblems featuring nonlinear dynamics and chance constraints. Covariance\nsteering (CS) is an emerging methodology in stochastic optimal control that\nposes constraints on the first two moments of the state distribution -- thereby\nbeing more tractable than full distributional control. Nevertheless, a\nsignificant limitation of current approaches for solving nonlinear CS problems,\nsuch as sequential convex programming (SCP), is that they often generate\ninfeasible or poor results due to the large number of constraints. In this\npaper, we address these challenges, by proposing an operator splitting CS\napproach that temporarily decouples the full problem into subproblems that can\nbe solved in parallel. This relaxation does not require intermediate iterates\nto satisfy all constraints simultaneously prior to convergence, which enhances\nexploration and improves feasibility in such non-convex settings. Simulation\nresults across a variety of robotics applications verify the ability of the\nproposed method to find better solutions even under stricter safety constraints\nthan standard SCP. Finally, the applicability of our framework on real systems\nis also confirmed through hardware demonstrations\n", "link": "http://arxiv.org/abs/2411.11211v2", "date": "2025-09-23", "relevancy": 1.5262, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5182}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Operator%20Splitting%20Covariance%20Steering%20for%20Safe%20Stochastic%20Nonlinear%0A%20%20Control&body=Title%3A%20Operator%20Splitting%20Covariance%20Steering%20for%20Safe%20Stochastic%20Nonlinear%0A%20%20Control%0AAuthor%3A%20Akash%20Ratheesh%20and%20Vincent%20Pacelli%20and%20Augustinos%20D.%20Saravanos%20and%20Evangelos%20A.%20Theodorou%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20algorithm%20for%20solving%20distribution%20steering%0Aproblems%20featuring%20nonlinear%20dynamics%20and%20chance%20constraints.%20Covariance%0Asteering%20%28CS%29%20is%20an%20emerging%20methodology%20in%20stochastic%20optimal%20control%20that%0Aposes%20constraints%20on%20the%20first%20two%20moments%20of%20the%20state%20distribution%20--%20thereby%0Abeing%20more%20tractable%20than%20full%20distributional%20control.%20Nevertheless%2C%20a%0Asignificant%20limitation%20of%20current%20approaches%20for%20solving%20nonlinear%20CS%20problems%2C%0Asuch%20as%20sequential%20convex%20programming%20%28SCP%29%2C%20is%20that%20they%20often%20generate%0Ainfeasible%20or%20poor%20results%20due%20to%20the%20large%20number%20of%20constraints.%20In%20this%0Apaper%2C%20we%20address%20these%20challenges%2C%20by%20proposing%20an%20operator%20splitting%20CS%0Aapproach%20that%20temporarily%20decouples%20the%20full%20problem%20into%20subproblems%20that%20can%0Abe%20solved%20in%20parallel.%20This%20relaxation%20does%20not%20require%20intermediate%20iterates%0Ato%20satisfy%20all%20constraints%20simultaneously%20prior%20to%20convergence%2C%20which%20enhances%0Aexploration%20and%20improves%20feasibility%20in%20such%20non-convex%20settings.%20Simulation%0Aresults%20across%20a%20variety%20of%20robotics%20applications%20verify%20the%20ability%20of%20the%0Aproposed%20method%20to%20find%20better%20solutions%20even%20under%20stricter%20safety%20constraints%0Athan%20standard%20SCP.%20Finally%2C%20the%20applicability%20of%20our%20framework%20on%20real%20systems%0Ais%20also%20confirmed%20through%20hardware%20demonstrations%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOperator%2520Splitting%2520Covariance%2520Steering%2520for%2520Safe%2520Stochastic%2520Nonlinear%250A%2520%2520Control%26entry.906535625%3DAkash%2520Ratheesh%2520and%2520Vincent%2520Pacelli%2520and%2520Augustinos%2520D.%2520Saravanos%2520and%2520Evangelos%2520A.%2520Theodorou%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520algorithm%2520for%2520solving%2520distribution%2520steering%250Aproblems%2520featuring%2520nonlinear%2520dynamics%2520and%2520chance%2520constraints.%2520Covariance%250Asteering%2520%2528CS%2529%2520is%2520an%2520emerging%2520methodology%2520in%2520stochastic%2520optimal%2520control%2520that%250Aposes%2520constraints%2520on%2520the%2520first%2520two%2520moments%2520of%2520the%2520state%2520distribution%2520--%2520thereby%250Abeing%2520more%2520tractable%2520than%2520full%2520distributional%2520control.%2520Nevertheless%252C%2520a%250Asignificant%2520limitation%2520of%2520current%2520approaches%2520for%2520solving%2520nonlinear%2520CS%2520problems%252C%250Asuch%2520as%2520sequential%2520convex%2520programming%2520%2528SCP%2529%252C%2520is%2520that%2520they%2520often%2520generate%250Ainfeasible%2520or%2520poor%2520results%2520due%2520to%2520the%2520large%2520number%2520of%2520constraints.%2520In%2520this%250Apaper%252C%2520we%2520address%2520these%2520challenges%252C%2520by%2520proposing%2520an%2520operator%2520splitting%2520CS%250Aapproach%2520that%2520temporarily%2520decouples%2520the%2520full%2520problem%2520into%2520subproblems%2520that%2520can%250Abe%2520solved%2520in%2520parallel.%2520This%2520relaxation%2520does%2520not%2520require%2520intermediate%2520iterates%250Ato%2520satisfy%2520all%2520constraints%2520simultaneously%2520prior%2520to%2520convergence%252C%2520which%2520enhances%250Aexploration%2520and%2520improves%2520feasibility%2520in%2520such%2520non-convex%2520settings.%2520Simulation%250Aresults%2520across%2520a%2520variety%2520of%2520robotics%2520applications%2520verify%2520the%2520ability%2520of%2520the%250Aproposed%2520method%2520to%2520find%2520better%2520solutions%2520even%2520under%2520stricter%2520safety%2520constraints%250Athan%2520standard%2520SCP.%2520Finally%252C%2520the%2520applicability%2520of%2520our%2520framework%2520on%2520real%2520systems%250Ais%2520also%2520confirmed%2520through%2520hardware%2520demonstrations%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Operator%20Splitting%20Covariance%20Steering%20for%20Safe%20Stochastic%20Nonlinear%0A%20%20Control&entry.906535625=Akash%20Ratheesh%20and%20Vincent%20Pacelli%20and%20Augustinos%20D.%20Saravanos%20and%20Evangelos%20A.%20Theodorou&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20algorithm%20for%20solving%20distribution%20steering%0Aproblems%20featuring%20nonlinear%20dynamics%20and%20chance%20constraints.%20Covariance%0Asteering%20%28CS%29%20is%20an%20emerging%20methodology%20in%20stochastic%20optimal%20control%20that%0Aposes%20constraints%20on%20the%20first%20two%20moments%20of%20the%20state%20distribution%20--%20thereby%0Abeing%20more%20tractable%20than%20full%20distributional%20control.%20Nevertheless%2C%20a%0Asignificant%20limitation%20of%20current%20approaches%20for%20solving%20nonlinear%20CS%20problems%2C%0Asuch%20as%20sequential%20convex%20programming%20%28SCP%29%2C%20is%20that%20they%20often%20generate%0Ainfeasible%20or%20poor%20results%20due%20to%20the%20large%20number%20of%20constraints.%20In%20this%0Apaper%2C%20we%20address%20these%20challenges%2C%20by%20proposing%20an%20operator%20splitting%20CS%0Aapproach%20that%20temporarily%20decouples%20the%20full%20problem%20into%20subproblems%20that%20can%0Abe%20solved%20in%20parallel.%20This%20relaxation%20does%20not%20require%20intermediate%20iterates%0Ato%20satisfy%20all%20constraints%20simultaneously%20prior%20to%20convergence%2C%20which%20enhances%0Aexploration%20and%20improves%20feasibility%20in%20such%20non-convex%20settings.%20Simulation%0Aresults%20across%20a%20variety%20of%20robotics%20applications%20verify%20the%20ability%20of%20the%0Aproposed%20method%20to%20find%20better%20solutions%20even%20under%20stricter%20safety%20constraints%0Athan%20standard%20SCP.%20Finally%2C%20the%20applicability%20of%20our%20framework%20on%20real%20systems%0Ais%20also%20confirmed%20through%20hardware%20demonstrations%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11211v2&entry.124074799=Read"},
{"title": "Proactive-reactive detection and mitigation of intermittent faults in\n  robot swarms", "author": "Sinan O\u011fuz and Emanuele Garone and Marco Dorigo and Mary Katherine Heinrich", "abstract": "  Intermittent faults are transient errors that sporadically appear and\ndisappear. Although intermittent faults pose substantial challenges to\nreliability and coordination, existing studies of fault tolerance in robot\nswarms focus instead on permanent faults. One reason for this is that\nintermittent faults are prohibitively difficult to detect in the fully\nself-organized ad-hoc networks typical of robot swarms, as their network\ntopologies are transient and often unpredictable. However, in the recently\nintroduced self-organizing nervous systems (SoNS) approach, robot swarms are\nable to self-organize persistent network structures for the first time, easing\nthe problem of detecting intermittent faults. To address intermittent faults in\nrobot swarms that have persistent networks, we propose a novel\nproactive-reactive strategy to detection and mitigation, based on\nself-organized backup layers and distributed consensus in a multiplex network.\nProactively, the robots self-organize dynamic backup paths before faults occur,\nadapting to changes in the primary network topology and the robots' relative\npositions. Reactively, robots use one-shot likelihood ratio tests to compare\ninformation received along different paths in the multiplex network, enabling\nearly fault detection. Upon detection, communication is temporarily rerouted in\na self-organized way, until the detected fault resolves. We validate the\napproach in representative scenarios of faulty positional data occurring during\nformation control, demonstrating that intermittent faults are prevented from\ndisrupting convergence to desired formations, with high fault detection\naccuracy and low rates of false positives.\n", "link": "http://arxiv.org/abs/2509.19246v1", "date": "2025-09-23", "relevancy": 1.4564, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5041}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4808}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proactive-reactive%20detection%20and%20mitigation%20of%20intermittent%20faults%20in%0A%20%20robot%20swarms&body=Title%3A%20Proactive-reactive%20detection%20and%20mitigation%20of%20intermittent%20faults%20in%0A%20%20robot%20swarms%0AAuthor%3A%20Sinan%20O%C4%9Fuz%20and%20Emanuele%20Garone%20and%20Marco%20Dorigo%20and%20Mary%20Katherine%20Heinrich%0AAbstract%3A%20%20%20Intermittent%20faults%20are%20transient%20errors%20that%20sporadically%20appear%20and%0Adisappear.%20Although%20intermittent%20faults%20pose%20substantial%20challenges%20to%0Areliability%20and%20coordination%2C%20existing%20studies%20of%20fault%20tolerance%20in%20robot%0Aswarms%20focus%20instead%20on%20permanent%20faults.%20One%20reason%20for%20this%20is%20that%0Aintermittent%20faults%20are%20prohibitively%20difficult%20to%20detect%20in%20the%20fully%0Aself-organized%20ad-hoc%20networks%20typical%20of%20robot%20swarms%2C%20as%20their%20network%0Atopologies%20are%20transient%20and%20often%20unpredictable.%20However%2C%20in%20the%20recently%0Aintroduced%20self-organizing%20nervous%20systems%20%28SoNS%29%20approach%2C%20robot%20swarms%20are%0Aable%20to%20self-organize%20persistent%20network%20structures%20for%20the%20first%20time%2C%20easing%0Athe%20problem%20of%20detecting%20intermittent%20faults.%20To%20address%20intermittent%20faults%20in%0Arobot%20swarms%20that%20have%20persistent%20networks%2C%20we%20propose%20a%20novel%0Aproactive-reactive%20strategy%20to%20detection%20and%20mitigation%2C%20based%20on%0Aself-organized%20backup%20layers%20and%20distributed%20consensus%20in%20a%20multiplex%20network.%0AProactively%2C%20the%20robots%20self-organize%20dynamic%20backup%20paths%20before%20faults%20occur%2C%0Aadapting%20to%20changes%20in%20the%20primary%20network%20topology%20and%20the%20robots%27%20relative%0Apositions.%20Reactively%2C%20robots%20use%20one-shot%20likelihood%20ratio%20tests%20to%20compare%0Ainformation%20received%20along%20different%20paths%20in%20the%20multiplex%20network%2C%20enabling%0Aearly%20fault%20detection.%20Upon%20detection%2C%20communication%20is%20temporarily%20rerouted%20in%0Aa%20self-organized%20way%2C%20until%20the%20detected%20fault%20resolves.%20We%20validate%20the%0Aapproach%20in%20representative%20scenarios%20of%20faulty%20positional%20data%20occurring%20during%0Aformation%20control%2C%20demonstrating%20that%20intermittent%20faults%20are%20prevented%20from%0Adisrupting%20convergence%20to%20desired%20formations%2C%20with%20high%20fault%20detection%0Aaccuracy%20and%20low%20rates%20of%20false%20positives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProactive-reactive%2520detection%2520and%2520mitigation%2520of%2520intermittent%2520faults%2520in%250A%2520%2520robot%2520swarms%26entry.906535625%3DSinan%2520O%25C4%259Fuz%2520and%2520Emanuele%2520Garone%2520and%2520Marco%2520Dorigo%2520and%2520Mary%2520Katherine%2520Heinrich%26entry.1292438233%3D%2520%2520Intermittent%2520faults%2520are%2520transient%2520errors%2520that%2520sporadically%2520appear%2520and%250Adisappear.%2520Although%2520intermittent%2520faults%2520pose%2520substantial%2520challenges%2520to%250Areliability%2520and%2520coordination%252C%2520existing%2520studies%2520of%2520fault%2520tolerance%2520in%2520robot%250Aswarms%2520focus%2520instead%2520on%2520permanent%2520faults.%2520One%2520reason%2520for%2520this%2520is%2520that%250Aintermittent%2520faults%2520are%2520prohibitively%2520difficult%2520to%2520detect%2520in%2520the%2520fully%250Aself-organized%2520ad-hoc%2520networks%2520typical%2520of%2520robot%2520swarms%252C%2520as%2520their%2520network%250Atopologies%2520are%2520transient%2520and%2520often%2520unpredictable.%2520However%252C%2520in%2520the%2520recently%250Aintroduced%2520self-organizing%2520nervous%2520systems%2520%2528SoNS%2529%2520approach%252C%2520robot%2520swarms%2520are%250Aable%2520to%2520self-organize%2520persistent%2520network%2520structures%2520for%2520the%2520first%2520time%252C%2520easing%250Athe%2520problem%2520of%2520detecting%2520intermittent%2520faults.%2520To%2520address%2520intermittent%2520faults%2520in%250Arobot%2520swarms%2520that%2520have%2520persistent%2520networks%252C%2520we%2520propose%2520a%2520novel%250Aproactive-reactive%2520strategy%2520to%2520detection%2520and%2520mitigation%252C%2520based%2520on%250Aself-organized%2520backup%2520layers%2520and%2520distributed%2520consensus%2520in%2520a%2520multiplex%2520network.%250AProactively%252C%2520the%2520robots%2520self-organize%2520dynamic%2520backup%2520paths%2520before%2520faults%2520occur%252C%250Aadapting%2520to%2520changes%2520in%2520the%2520primary%2520network%2520topology%2520and%2520the%2520robots%2527%2520relative%250Apositions.%2520Reactively%252C%2520robots%2520use%2520one-shot%2520likelihood%2520ratio%2520tests%2520to%2520compare%250Ainformation%2520received%2520along%2520different%2520paths%2520in%2520the%2520multiplex%2520network%252C%2520enabling%250Aearly%2520fault%2520detection.%2520Upon%2520detection%252C%2520communication%2520is%2520temporarily%2520rerouted%2520in%250Aa%2520self-organized%2520way%252C%2520until%2520the%2520detected%2520fault%2520resolves.%2520We%2520validate%2520the%250Aapproach%2520in%2520representative%2520scenarios%2520of%2520faulty%2520positional%2520data%2520occurring%2520during%250Aformation%2520control%252C%2520demonstrating%2520that%2520intermittent%2520faults%2520are%2520prevented%2520from%250Adisrupting%2520convergence%2520to%2520desired%2520formations%252C%2520with%2520high%2520fault%2520detection%250Aaccuracy%2520and%2520low%2520rates%2520of%2520false%2520positives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proactive-reactive%20detection%20and%20mitigation%20of%20intermittent%20faults%20in%0A%20%20robot%20swarms&entry.906535625=Sinan%20O%C4%9Fuz%20and%20Emanuele%20Garone%20and%20Marco%20Dorigo%20and%20Mary%20Katherine%20Heinrich&entry.1292438233=%20%20Intermittent%20faults%20are%20transient%20errors%20that%20sporadically%20appear%20and%0Adisappear.%20Although%20intermittent%20faults%20pose%20substantial%20challenges%20to%0Areliability%20and%20coordination%2C%20existing%20studies%20of%20fault%20tolerance%20in%20robot%0Aswarms%20focus%20instead%20on%20permanent%20faults.%20One%20reason%20for%20this%20is%20that%0Aintermittent%20faults%20are%20prohibitively%20difficult%20to%20detect%20in%20the%20fully%0Aself-organized%20ad-hoc%20networks%20typical%20of%20robot%20swarms%2C%20as%20their%20network%0Atopologies%20are%20transient%20and%20often%20unpredictable.%20However%2C%20in%20the%20recently%0Aintroduced%20self-organizing%20nervous%20systems%20%28SoNS%29%20approach%2C%20robot%20swarms%20are%0Aable%20to%20self-organize%20persistent%20network%20structures%20for%20the%20first%20time%2C%20easing%0Athe%20problem%20of%20detecting%20intermittent%20faults.%20To%20address%20intermittent%20faults%20in%0Arobot%20swarms%20that%20have%20persistent%20networks%2C%20we%20propose%20a%20novel%0Aproactive-reactive%20strategy%20to%20detection%20and%20mitigation%2C%20based%20on%0Aself-organized%20backup%20layers%20and%20distributed%20consensus%20in%20a%20multiplex%20network.%0AProactively%2C%20the%20robots%20self-organize%20dynamic%20backup%20paths%20before%20faults%20occur%2C%0Aadapting%20to%20changes%20in%20the%20primary%20network%20topology%20and%20the%20robots%27%20relative%0Apositions.%20Reactively%2C%20robots%20use%20one-shot%20likelihood%20ratio%20tests%20to%20compare%0Ainformation%20received%20along%20different%20paths%20in%20the%20multiplex%20network%2C%20enabling%0Aearly%20fault%20detection.%20Upon%20detection%2C%20communication%20is%20temporarily%20rerouted%20in%0Aa%20self-organized%20way%2C%20until%20the%20detected%20fault%20resolves.%20We%20validate%20the%0Aapproach%20in%20representative%20scenarios%20of%20faulty%20positional%20data%20occurring%20during%0Aformation%20control%2C%20demonstrating%20that%20intermittent%20faults%20are%20prevented%20from%0Adisrupting%20convergence%20to%20desired%20formations%2C%20with%20high%20fault%20detection%0Aaccuracy%20and%20low%20rates%20of%20false%20positives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19246v1&entry.124074799=Read"},
{"title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies", "author": "Lars Ankile and Zhenyu Jiang and Rocky Duan and Guanya Shi and Pieter Abbeel and Anusha Nagabandi", "abstract": "  Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io\n", "link": "http://arxiv.org/abs/2509.19301v1", "date": "2025-09-23", "relevancy": 1.6902, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5956}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5693}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Off-Policy%20RL%20for%20Finetuning%20Behavior%20Cloning%20Policies&body=Title%3A%20Residual%20Off-Policy%20RL%20for%20Finetuning%20Behavior%20Cloning%20Policies%0AAuthor%3A%20Lars%20Ankile%20and%20Zhenyu%20Jiang%20and%20Rocky%20Duan%20and%20Guanya%20Shi%20and%20Pieter%20Abbeel%20and%20Anusha%20Nagabandi%0AAbstract%3A%20%20%20Recent%20advances%20in%20behavior%20cloning%20%28BC%29%20have%20enabled%20impressive%20visuomotor%0Acontrol%20policies.%20However%2C%20these%20approaches%20are%20limited%20by%20the%20quality%20of%20human%0Ademonstrations%2C%20the%20manual%20effort%20required%20for%20data%20collection%2C%20and%20the%0Adiminishing%20returns%20from%20increasing%20offline%20data.%20In%20comparison%2C%20reinforcement%0Alearning%20%28RL%29%20trains%20an%20agent%20through%20autonomous%20interaction%20with%20the%0Aenvironment%20and%20has%20shown%20remarkable%20success%20in%20various%20domains.%20Still%2C%0Atraining%20RL%20policies%20directly%20on%20real-world%20robots%20remains%20challenging%20due%20to%0Asample%20inefficiency%2C%20safety%20concerns%2C%20and%20the%20difficulty%20of%20learning%20from%0Asparse%20rewards%20for%20long-horizon%20tasks%2C%20especially%20for%20high-degree-of-freedom%0A%28DoF%29%20systems.%20We%20present%20a%20recipe%20that%20combines%20the%20benefits%20of%20BC%20and%20RL%0Athrough%20a%20residual%20learning%20framework.%20Our%20approach%20leverages%20BC%20policies%20as%0Ablack-box%20bases%20and%20learns%20lightweight%20per-step%20residual%20corrections%20via%0Asample-efficient%20off-policy%20RL.%20We%20demonstrate%20that%20our%20method%20requires%20only%0Asparse%20binary%20reward%20signals%20and%20can%20effectively%20improve%20manipulation%20policies%0Aon%20high-degree-of-freedom%20%28DoF%29%20systems%20in%20both%20simulation%20and%20the%20real%20world.%0AIn%20particular%2C%20we%20demonstrate%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%0Asuccessful%20real-world%20RL%20training%20on%20a%20humanoid%20robot%20with%20dexterous%20hands.%20Our%0Aresults%20demonstrate%20state-of-the-art%20performance%20in%20various%20vision-based%20tasks%2C%0Apointing%20towards%20a%20practical%20pathway%20for%20deploying%20RL%20in%20the%20real%20world.%0AProject%20website%3A%20https%3A//residual-offpolicy-rl.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Off-Policy%2520RL%2520for%2520Finetuning%2520Behavior%2520Cloning%2520Policies%26entry.906535625%3DLars%2520Ankile%2520and%2520Zhenyu%2520Jiang%2520and%2520Rocky%2520Duan%2520and%2520Guanya%2520Shi%2520and%2520Pieter%2520Abbeel%2520and%2520Anusha%2520Nagabandi%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520behavior%2520cloning%2520%2528BC%2529%2520have%2520enabled%2520impressive%2520visuomotor%250Acontrol%2520policies.%2520However%252C%2520these%2520approaches%2520are%2520limited%2520by%2520the%2520quality%2520of%2520human%250Ademonstrations%252C%2520the%2520manual%2520effort%2520required%2520for%2520data%2520collection%252C%2520and%2520the%250Adiminishing%2520returns%2520from%2520increasing%2520offline%2520data.%2520In%2520comparison%252C%2520reinforcement%250Alearning%2520%2528RL%2529%2520trains%2520an%2520agent%2520through%2520autonomous%2520interaction%2520with%2520the%250Aenvironment%2520and%2520has%2520shown%2520remarkable%2520success%2520in%2520various%2520domains.%2520Still%252C%250Atraining%2520RL%2520policies%2520directly%2520on%2520real-world%2520robots%2520remains%2520challenging%2520due%2520to%250Asample%2520inefficiency%252C%2520safety%2520concerns%252C%2520and%2520the%2520difficulty%2520of%2520learning%2520from%250Asparse%2520rewards%2520for%2520long-horizon%2520tasks%252C%2520especially%2520for%2520high-degree-of-freedom%250A%2528DoF%2529%2520systems.%2520We%2520present%2520a%2520recipe%2520that%2520combines%2520the%2520benefits%2520of%2520BC%2520and%2520RL%250Athrough%2520a%2520residual%2520learning%2520framework.%2520Our%2520approach%2520leverages%2520BC%2520policies%2520as%250Ablack-box%2520bases%2520and%2520learns%2520lightweight%2520per-step%2520residual%2520corrections%2520via%250Asample-efficient%2520off-policy%2520RL.%2520We%2520demonstrate%2520that%2520our%2520method%2520requires%2520only%250Asparse%2520binary%2520reward%2520signals%2520and%2520can%2520effectively%2520improve%2520manipulation%2520policies%250Aon%2520high-degree-of-freedom%2520%2528DoF%2529%2520systems%2520in%2520both%2520simulation%2520and%2520the%2520real%2520world.%250AIn%2520particular%252C%2520we%2520demonstrate%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%250Asuccessful%2520real-world%2520RL%2520training%2520on%2520a%2520humanoid%2520robot%2520with%2520dexterous%2520hands.%2520Our%250Aresults%2520demonstrate%2520state-of-the-art%2520performance%2520in%2520various%2520vision-based%2520tasks%252C%250Apointing%2520towards%2520a%2520practical%2520pathway%2520for%2520deploying%2520RL%2520in%2520the%2520real%2520world.%250AProject%2520website%253A%2520https%253A//residual-offpolicy-rl.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Off-Policy%20RL%20for%20Finetuning%20Behavior%20Cloning%20Policies&entry.906535625=Lars%20Ankile%20and%20Zhenyu%20Jiang%20and%20Rocky%20Duan%20and%20Guanya%20Shi%20and%20Pieter%20Abbeel%20and%20Anusha%20Nagabandi&entry.1292438233=%20%20Recent%20advances%20in%20behavior%20cloning%20%28BC%29%20have%20enabled%20impressive%20visuomotor%0Acontrol%20policies.%20However%2C%20these%20approaches%20are%20limited%20by%20the%20quality%20of%20human%0Ademonstrations%2C%20the%20manual%20effort%20required%20for%20data%20collection%2C%20and%20the%0Adiminishing%20returns%20from%20increasing%20offline%20data.%20In%20comparison%2C%20reinforcement%0Alearning%20%28RL%29%20trains%20an%20agent%20through%20autonomous%20interaction%20with%20the%0Aenvironment%20and%20has%20shown%20remarkable%20success%20in%20various%20domains.%20Still%2C%0Atraining%20RL%20policies%20directly%20on%20real-world%20robots%20remains%20challenging%20due%20to%0Asample%20inefficiency%2C%20safety%20concerns%2C%20and%20the%20difficulty%20of%20learning%20from%0Asparse%20rewards%20for%20long-horizon%20tasks%2C%20especially%20for%20high-degree-of-freedom%0A%28DoF%29%20systems.%20We%20present%20a%20recipe%20that%20combines%20the%20benefits%20of%20BC%20and%20RL%0Athrough%20a%20residual%20learning%20framework.%20Our%20approach%20leverages%20BC%20policies%20as%0Ablack-box%20bases%20and%20learns%20lightweight%20per-step%20residual%20corrections%20via%0Asample-efficient%20off-policy%20RL.%20We%20demonstrate%20that%20our%20method%20requires%20only%0Asparse%20binary%20reward%20signals%20and%20can%20effectively%20improve%20manipulation%20policies%0Aon%20high-degree-of-freedom%20%28DoF%29%20systems%20in%20both%20simulation%20and%20the%20real%20world.%0AIn%20particular%2C%20we%20demonstrate%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%0Asuccessful%20real-world%20RL%20training%20on%20a%20humanoid%20robot%20with%20dexterous%20hands.%20Our%0Aresults%20demonstrate%20state-of-the-art%20performance%20in%20various%20vision-based%20tasks%2C%0Apointing%20towards%20a%20practical%20pathway%20for%20deploying%20RL%20in%20the%20real%20world.%0AProject%20website%3A%20https%3A//residual-offpolicy-rl.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19301v1&entry.124074799=Read"},
{"title": "LLM-based Vulnerability Discovery through the Lens of Code Metrics", "author": "Felix Weissberg and Lukas Pirch and Erik Imgrund and Jonas M\u00f6ller and Thorsten Eisenhofer and Konrad Rieck", "abstract": "  Large language models (LLMs) excel in many tasks of software engineering, yet\nprogress in leveraging them for vulnerability discovery has stalled in recent\nyears. To understand this phenomenon, we investigate LLMs through the lens of\nclassic code metrics. Surprisingly, we find that a classifier trained solely on\nthese metrics performs on par with state-of-the-art LLMs for vulnerability\ndiscovery. A root-cause analysis reveals a strong correlation and a causal\neffect between LLMs and code metrics: When the value of a metric is changed,\nLLM predictions tend to shift by a corresponding magnitude. This dependency\nsuggests that LLMs operate at a similarly shallow level as code metrics,\nlimiting their ability to grasp complex patterns and fully realize their\npotential in vulnerability discovery. Based on these findings, we derive\nrecommendations on how research should more effectively address this challenge.\n", "link": "http://arxiv.org/abs/2509.19117v1", "date": "2025-09-23", "relevancy": 1.8887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-based%20Vulnerability%20Discovery%20through%20the%20Lens%20of%20Code%20Metrics&body=Title%3A%20LLM-based%20Vulnerability%20Discovery%20through%20the%20Lens%20of%20Code%20Metrics%0AAuthor%3A%20Felix%20Weissberg%20and%20Lukas%20Pirch%20and%20Erik%20Imgrund%20and%20Jonas%20M%C3%B6ller%20and%20Thorsten%20Eisenhofer%20and%20Konrad%20Rieck%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20many%20tasks%20of%20software%20engineering%2C%20yet%0Aprogress%20in%20leveraging%20them%20for%20vulnerability%20discovery%20has%20stalled%20in%20recent%0Ayears.%20To%20understand%20this%20phenomenon%2C%20we%20investigate%20LLMs%20through%20the%20lens%20of%0Aclassic%20code%20metrics.%20Surprisingly%2C%20we%20find%20that%20a%20classifier%20trained%20solely%20on%0Athese%20metrics%20performs%20on%20par%20with%20state-of-the-art%20LLMs%20for%20vulnerability%0Adiscovery.%20A%20root-cause%20analysis%20reveals%20a%20strong%20correlation%20and%20a%20causal%0Aeffect%20between%20LLMs%20and%20code%20metrics%3A%20When%20the%20value%20of%20a%20metric%20is%20changed%2C%0ALLM%20predictions%20tend%20to%20shift%20by%20a%20corresponding%20magnitude.%20This%20dependency%0Asuggests%20that%20LLMs%20operate%20at%20a%20similarly%20shallow%20level%20as%20code%20metrics%2C%0Alimiting%20their%20ability%20to%20grasp%20complex%20patterns%20and%20fully%20realize%20their%0Apotential%20in%20vulnerability%20discovery.%20Based%20on%20these%20findings%2C%20we%20derive%0Arecommendations%20on%20how%20research%20should%20more%20effectively%20address%20this%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-based%2520Vulnerability%2520Discovery%2520through%2520the%2520Lens%2520of%2520Code%2520Metrics%26entry.906535625%3DFelix%2520Weissberg%2520and%2520Lukas%2520Pirch%2520and%2520Erik%2520Imgrund%2520and%2520Jonas%2520M%25C3%25B6ller%2520and%2520Thorsten%2520Eisenhofer%2520and%2520Konrad%2520Rieck%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520in%2520many%2520tasks%2520of%2520software%2520engineering%252C%2520yet%250Aprogress%2520in%2520leveraging%2520them%2520for%2520vulnerability%2520discovery%2520has%2520stalled%2520in%2520recent%250Ayears.%2520To%2520understand%2520this%2520phenomenon%252C%2520we%2520investigate%2520LLMs%2520through%2520the%2520lens%2520of%250Aclassic%2520code%2520metrics.%2520Surprisingly%252C%2520we%2520find%2520that%2520a%2520classifier%2520trained%2520solely%2520on%250Athese%2520metrics%2520performs%2520on%2520par%2520with%2520state-of-the-art%2520LLMs%2520for%2520vulnerability%250Adiscovery.%2520A%2520root-cause%2520analysis%2520reveals%2520a%2520strong%2520correlation%2520and%2520a%2520causal%250Aeffect%2520between%2520LLMs%2520and%2520code%2520metrics%253A%2520When%2520the%2520value%2520of%2520a%2520metric%2520is%2520changed%252C%250ALLM%2520predictions%2520tend%2520to%2520shift%2520by%2520a%2520corresponding%2520magnitude.%2520This%2520dependency%250Asuggests%2520that%2520LLMs%2520operate%2520at%2520a%2520similarly%2520shallow%2520level%2520as%2520code%2520metrics%252C%250Alimiting%2520their%2520ability%2520to%2520grasp%2520complex%2520patterns%2520and%2520fully%2520realize%2520their%250Apotential%2520in%2520vulnerability%2520discovery.%2520Based%2520on%2520these%2520findings%252C%2520we%2520derive%250Arecommendations%2520on%2520how%2520research%2520should%2520more%2520effectively%2520address%2520this%2520challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-based%20Vulnerability%20Discovery%20through%20the%20Lens%20of%20Code%20Metrics&entry.906535625=Felix%20Weissberg%20and%20Lukas%20Pirch%20and%20Erik%20Imgrund%20and%20Jonas%20M%C3%B6ller%20and%20Thorsten%20Eisenhofer%20and%20Konrad%20Rieck&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20many%20tasks%20of%20software%20engineering%2C%20yet%0Aprogress%20in%20leveraging%20them%20for%20vulnerability%20discovery%20has%20stalled%20in%20recent%0Ayears.%20To%20understand%20this%20phenomenon%2C%20we%20investigate%20LLMs%20through%20the%20lens%20of%0Aclassic%20code%20metrics.%20Surprisingly%2C%20we%20find%20that%20a%20classifier%20trained%20solely%20on%0Athese%20metrics%20performs%20on%20par%20with%20state-of-the-art%20LLMs%20for%20vulnerability%0Adiscovery.%20A%20root-cause%20analysis%20reveals%20a%20strong%20correlation%20and%20a%20causal%0Aeffect%20between%20LLMs%20and%20code%20metrics%3A%20When%20the%20value%20of%20a%20metric%20is%20changed%2C%0ALLM%20predictions%20tend%20to%20shift%20by%20a%20corresponding%20magnitude.%20This%20dependency%0Asuggests%20that%20LLMs%20operate%20at%20a%20similarly%20shallow%20level%20as%20code%20metrics%2C%0Alimiting%20their%20ability%20to%20grasp%20complex%20patterns%20and%20fully%20realize%20their%0Apotential%20in%20vulnerability%20discovery.%20Based%20on%20these%20findings%2C%20we%20derive%0Arecommendations%20on%20how%20research%20should%20more%20effectively%20address%20this%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19117v1&entry.124074799=Read"},
{"title": "LongCat-Flash-Thinking Technical Report", "author": " Meituan LongCat Team and Anchun Gui and Bei Li and Bingyang Tao and Bole Zhou and Borun Chen and Chao Zhang and Chao Zhang and Chengcheng Han and Chenhui Yang and Chi Zhang and Chong Peng and Chuyu Zhang and Cong Chen and Fengcun Li and Gang Xu and Guoyuan Lin and Hao Jiang and Hao Liang and Haomin Fu and Haoxiang Ma and Hong Liu and Hongyan Hao and Hongyin Tang and Hongyu Zang and Hongzhi Ni and Hui Su and Jiahao Liu and Jiahuan Li and Jialin Liu and Jianfei Zhang and Jianhao Xu and Jianing Wang and Jiaqi Sun and Jiaqi Zhang and Jiarong Shi and Jiawei Yang and Jingang Wang and Jinrui Ding and Jun Kuang and Jun Xu and Ke He and Kefeng Zhang and Keheng Wang and Keqing He and Li Wei and Liang Shi and Lin Qiu and Lingbin Kong and Lingchuan Liu and Linsen Guo and Longfei An and Mai Xia and Meng Zhou and Mengshen Zhu and Peng Pei and Pengcheng Jia and Qi Gu and Qi Guo and Qiong Huang and Quan Chen and Quanchi Weng and Rongxiang Weng and Ruichen Shao and Rumei Li and Shanglin Lei and Shuai Du and Shuaikang Liu and Shuang Zhou and Shuhao Hu and Siyu Xu and Songshan Gong and Tao Liang and Tianhao Hu and Wei He and Wei Shi and Wei Wang and Wei Wu and Wei Zhuo and Weifeng Tang and Wenjie Shi and Wenlong Zhu and Xi Su and Xiangcheng Liu and Xiangyu Xi and Xiangzhou Huang and Xiao Liu and Xiaochen Jiang and Xiaowei Shi and Xiaowen Shi and Xiaoyu Li and Xin Chen and Xinyue Zhao and Xuan Huang and Xuemiao Zhang and Xuezhi Cao and Xunliang Cai and Yajie Zhang and Yang Chen and Yang Liu and Yang Liu and Yang Zheng and Yaoming Wang and Yaqi Huo and Yerui Sun and Yifan Lu and Yiyang Li and Youshao Xiao and Yuanzhe Lei and Yuchen Xie and Yueqing Sun and Yufei Zhang and Yuhuai Wei and Yulei Qian and Yunke Zhao and Yuqing Ding and Yuwei Jiang and Zhaohua Yang and Zhengyu Chen and Zhijian Liu and Zhikang Xia and Zhongda Su and Ziran Li and Ziwen Wang and Ziyuan Zhuang and Zongyu Wang and Zunyuan Yang", "abstract": "  We present LongCat-Flash-Thinking, an efficient 560-billion-parameter\nopen-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities\nare cultivated through a meticulously crafted training process, beginning with\nlong Chain-of-Thought (CoT) data cold-start and culminating in large-scale\nReinforcement Learning (RL). We first employ a well-designed cold-start\ntraining strategy, which significantly enhances the reasoning potential and\nequips the model with specialized skills in both formal and agentic reasoning.\nThen, a core innovation is our domain-parallel training scheme, which decouples\noptimization across distinct domains (e.g., STEM, Code, Agentic) and\nsubsequently fuses the resulting expert models into a single, nearly\nPareto-optimal model. This entire process is powered by our Dynamic\nORchestration for Asynchronous rollout (DORA) system, a large-scale RL\nframework that delivers a greater than threefold training speedup over\nsynchronous methods on tens of thousands of accelerators. As a result,\nLongCat-Flash-Thinking achieves state-of-the-art performance among open-source\nmodels on a suite of complex reasoning tasks. The model exhibits exceptional\nefficiency in agentic reasoning, reducing average token consumption by 64.5%\n(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We\nrelease LongCat-Flash-Thinking to promote further advances in reasoning systems\nand agentic AI research.\n", "link": "http://arxiv.org/abs/2509.18883v1", "date": "2025-09-23", "relevancy": 2.0826, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongCat-Flash-Thinking%20Technical%20Report&body=Title%3A%20LongCat-Flash-Thinking%20Technical%20Report%0AAuthor%3A%20%20Meituan%20LongCat%20Team%20and%20Anchun%20Gui%20and%20Bei%20Li%20and%20Bingyang%20Tao%20and%20Bole%20Zhou%20and%20Borun%20Chen%20and%20Chao%20Zhang%20and%20Chao%20Zhang%20and%20Chengcheng%20Han%20and%20Chenhui%20Yang%20and%20Chi%20Zhang%20and%20Chong%20Peng%20and%20Chuyu%20Zhang%20and%20Cong%20Chen%20and%20Fengcun%20Li%20and%20Gang%20Xu%20and%20Guoyuan%20Lin%20and%20Hao%20Jiang%20and%20Hao%20Liang%20and%20Haomin%20Fu%20and%20Haoxiang%20Ma%20and%20Hong%20Liu%20and%20Hongyan%20Hao%20and%20Hongyin%20Tang%20and%20Hongyu%20Zang%20and%20Hongzhi%20Ni%20and%20Hui%20Su%20and%20Jiahao%20Liu%20and%20Jiahuan%20Li%20and%20Jialin%20Liu%20and%20Jianfei%20Zhang%20and%20Jianhao%20Xu%20and%20Jianing%20Wang%20and%20Jiaqi%20Sun%20and%20Jiaqi%20Zhang%20and%20Jiarong%20Shi%20and%20Jiawei%20Yang%20and%20Jingang%20Wang%20and%20Jinrui%20Ding%20and%20Jun%20Kuang%20and%20Jun%20Xu%20and%20Ke%20He%20and%20Kefeng%20Zhang%20and%20Keheng%20Wang%20and%20Keqing%20He%20and%20Li%20Wei%20and%20Liang%20Shi%20and%20Lin%20Qiu%20and%20Lingbin%20Kong%20and%20Lingchuan%20Liu%20and%20Linsen%20Guo%20and%20Longfei%20An%20and%20Mai%20Xia%20and%20Meng%20Zhou%20and%20Mengshen%20Zhu%20and%20Peng%20Pei%20and%20Pengcheng%20Jia%20and%20Qi%20Gu%20and%20Qi%20Guo%20and%20Qiong%20Huang%20and%20Quan%20Chen%20and%20Quanchi%20Weng%20and%20Rongxiang%20Weng%20and%20Ruichen%20Shao%20and%20Rumei%20Li%20and%20Shanglin%20Lei%20and%20Shuai%20Du%20and%20Shuaikang%20Liu%20and%20Shuang%20Zhou%20and%20Shuhao%20Hu%20and%20Siyu%20Xu%20and%20Songshan%20Gong%20and%20Tao%20Liang%20and%20Tianhao%20Hu%20and%20Wei%20He%20and%20Wei%20Shi%20and%20Wei%20Wang%20and%20Wei%20Wu%20and%20Wei%20Zhuo%20and%20Weifeng%20Tang%20and%20Wenjie%20Shi%20and%20Wenlong%20Zhu%20and%20Xi%20Su%20and%20Xiangcheng%20Liu%20and%20Xiangyu%20Xi%20and%20Xiangzhou%20Huang%20and%20Xiao%20Liu%20and%20Xiaochen%20Jiang%20and%20Xiaowei%20Shi%20and%20Xiaowen%20Shi%20and%20Xiaoyu%20Li%20and%20Xin%20Chen%20and%20Xinyue%20Zhao%20and%20Xuan%20Huang%20and%20Xuemiao%20Zhang%20and%20Xuezhi%20Cao%20and%20Xunliang%20Cai%20and%20Yajie%20Zhang%20and%20Yang%20Chen%20and%20Yang%20Liu%20and%20Yang%20Liu%20and%20Yang%20Zheng%20and%20Yaoming%20Wang%20and%20Yaqi%20Huo%20and%20Yerui%20Sun%20and%20Yifan%20Lu%20and%20Yiyang%20Li%20and%20Youshao%20Xiao%20and%20Yuanzhe%20Lei%20and%20Yuchen%20Xie%20and%20Yueqing%20Sun%20and%20Yufei%20Zhang%20and%20Yuhuai%20Wei%20and%20Yulei%20Qian%20and%20Yunke%20Zhao%20and%20Yuqing%20Ding%20and%20Yuwei%20Jiang%20and%20Zhaohua%20Yang%20and%20Zhengyu%20Chen%20and%20Zhijian%20Liu%20and%20Zhikang%20Xia%20and%20Zhongda%20Su%20and%20Ziran%20Li%20and%20Ziwen%20Wang%20and%20Ziyuan%20Zhuang%20and%20Zongyu%20Wang%20and%20Zunyuan%20Yang%0AAbstract%3A%20%20%20We%20present%20LongCat-Flash-Thinking%2C%20an%20efficient%20560-billion-parameter%0Aopen-source%20Mixture-of-Experts%20%28MoE%29%20reasoning%20model.%20Its%20advanced%20capabilities%0Aare%20cultivated%20through%20a%20meticulously%20crafted%20training%20process%2C%20beginning%20with%0Along%20Chain-of-Thought%20%28CoT%29%20data%20cold-start%20and%20culminating%20in%20large-scale%0AReinforcement%20Learning%20%28RL%29.%20We%20first%20employ%20a%20well-designed%20cold-start%0Atraining%20strategy%2C%20which%20significantly%20enhances%20the%20reasoning%20potential%20and%0Aequips%20the%20model%20with%20specialized%20skills%20in%20both%20formal%20and%20agentic%20reasoning.%0AThen%2C%20a%20core%20innovation%20is%20our%20domain-parallel%20training%20scheme%2C%20which%20decouples%0Aoptimization%20across%20distinct%20domains%20%28e.g.%2C%20STEM%2C%20Code%2C%20Agentic%29%20and%0Asubsequently%20fuses%20the%20resulting%20expert%20models%20into%20a%20single%2C%20nearly%0APareto-optimal%20model.%20This%20entire%20process%20is%20powered%20by%20our%20Dynamic%0AORchestration%20for%20Asynchronous%20rollout%20%28DORA%29%20system%2C%20a%20large-scale%20RL%0Aframework%20that%20delivers%20a%20greater%20than%20threefold%20training%20speedup%20over%0Asynchronous%20methods%20on%20tens%20of%20thousands%20of%20accelerators.%20As%20a%20result%2C%0ALongCat-Flash-Thinking%20achieves%20state-of-the-art%20performance%20among%20open-source%0Amodels%20on%20a%20suite%20of%20complex%20reasoning%20tasks.%20The%20model%20exhibits%20exceptional%0Aefficiency%20in%20agentic%20reasoning%2C%20reducing%20average%20token%20consumption%20by%2064.5%25%0A%28from%2019%2C%20653%20to%206%2C%20965%29%20on%20AIME-25%2C%20without%20degrading%20task%20accuracy.%20We%0Arelease%20LongCat-Flash-Thinking%20to%20promote%20further%20advances%20in%20reasoning%20systems%0Aand%20agentic%20AI%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongCat-Flash-Thinking%2520Technical%2520Report%26entry.906535625%3D%2520Meituan%2520LongCat%2520Team%2520and%2520Anchun%2520Gui%2520and%2520Bei%2520Li%2520and%2520Bingyang%2520Tao%2520and%2520Bole%2520Zhou%2520and%2520Borun%2520Chen%2520and%2520Chao%2520Zhang%2520and%2520Chao%2520Zhang%2520and%2520Chengcheng%2520Han%2520and%2520Chenhui%2520Yang%2520and%2520Chi%2520Zhang%2520and%2520Chong%2520Peng%2520and%2520Chuyu%2520Zhang%2520and%2520Cong%2520Chen%2520and%2520Fengcun%2520Li%2520and%2520Gang%2520Xu%2520and%2520Guoyuan%2520Lin%2520and%2520Hao%2520Jiang%2520and%2520Hao%2520Liang%2520and%2520Haomin%2520Fu%2520and%2520Haoxiang%2520Ma%2520and%2520Hong%2520Liu%2520and%2520Hongyan%2520Hao%2520and%2520Hongyin%2520Tang%2520and%2520Hongyu%2520Zang%2520and%2520Hongzhi%2520Ni%2520and%2520Hui%2520Su%2520and%2520Jiahao%2520Liu%2520and%2520Jiahuan%2520Li%2520and%2520Jialin%2520Liu%2520and%2520Jianfei%2520Zhang%2520and%2520Jianhao%2520Xu%2520and%2520Jianing%2520Wang%2520and%2520Jiaqi%2520Sun%2520and%2520Jiaqi%2520Zhang%2520and%2520Jiarong%2520Shi%2520and%2520Jiawei%2520Yang%2520and%2520Jingang%2520Wang%2520and%2520Jinrui%2520Ding%2520and%2520Jun%2520Kuang%2520and%2520Jun%2520Xu%2520and%2520Ke%2520He%2520and%2520Kefeng%2520Zhang%2520and%2520Keheng%2520Wang%2520and%2520Keqing%2520He%2520and%2520Li%2520Wei%2520and%2520Liang%2520Shi%2520and%2520Lin%2520Qiu%2520and%2520Lingbin%2520Kong%2520and%2520Lingchuan%2520Liu%2520and%2520Linsen%2520Guo%2520and%2520Longfei%2520An%2520and%2520Mai%2520Xia%2520and%2520Meng%2520Zhou%2520and%2520Mengshen%2520Zhu%2520and%2520Peng%2520Pei%2520and%2520Pengcheng%2520Jia%2520and%2520Qi%2520Gu%2520and%2520Qi%2520Guo%2520and%2520Qiong%2520Huang%2520and%2520Quan%2520Chen%2520and%2520Quanchi%2520Weng%2520and%2520Rongxiang%2520Weng%2520and%2520Ruichen%2520Shao%2520and%2520Rumei%2520Li%2520and%2520Shanglin%2520Lei%2520and%2520Shuai%2520Du%2520and%2520Shuaikang%2520Liu%2520and%2520Shuang%2520Zhou%2520and%2520Shuhao%2520Hu%2520and%2520Siyu%2520Xu%2520and%2520Songshan%2520Gong%2520and%2520Tao%2520Liang%2520and%2520Tianhao%2520Hu%2520and%2520Wei%2520He%2520and%2520Wei%2520Shi%2520and%2520Wei%2520Wang%2520and%2520Wei%2520Wu%2520and%2520Wei%2520Zhuo%2520and%2520Weifeng%2520Tang%2520and%2520Wenjie%2520Shi%2520and%2520Wenlong%2520Zhu%2520and%2520Xi%2520Su%2520and%2520Xiangcheng%2520Liu%2520and%2520Xiangyu%2520Xi%2520and%2520Xiangzhou%2520Huang%2520and%2520Xiao%2520Liu%2520and%2520Xiaochen%2520Jiang%2520and%2520Xiaowei%2520Shi%2520and%2520Xiaowen%2520Shi%2520and%2520Xiaoyu%2520Li%2520and%2520Xin%2520Chen%2520and%2520Xinyue%2520Zhao%2520and%2520Xuan%2520Huang%2520and%2520Xuemiao%2520Zhang%2520and%2520Xuezhi%2520Cao%2520and%2520Xunliang%2520Cai%2520and%2520Yajie%2520Zhang%2520and%2520Yang%2520Chen%2520and%2520Yang%2520Liu%2520and%2520Yang%2520Liu%2520and%2520Yang%2520Zheng%2520and%2520Yaoming%2520Wang%2520and%2520Yaqi%2520Huo%2520and%2520Yerui%2520Sun%2520and%2520Yifan%2520Lu%2520and%2520Yiyang%2520Li%2520and%2520Youshao%2520Xiao%2520and%2520Yuanzhe%2520Lei%2520and%2520Yuchen%2520Xie%2520and%2520Yueqing%2520Sun%2520and%2520Yufei%2520Zhang%2520and%2520Yuhuai%2520Wei%2520and%2520Yulei%2520Qian%2520and%2520Yunke%2520Zhao%2520and%2520Yuqing%2520Ding%2520and%2520Yuwei%2520Jiang%2520and%2520Zhaohua%2520Yang%2520and%2520Zhengyu%2520Chen%2520and%2520Zhijian%2520Liu%2520and%2520Zhikang%2520Xia%2520and%2520Zhongda%2520Su%2520and%2520Ziran%2520Li%2520and%2520Ziwen%2520Wang%2520and%2520Ziyuan%2520Zhuang%2520and%2520Zongyu%2520Wang%2520and%2520Zunyuan%2520Yang%26entry.1292438233%3D%2520%2520We%2520present%2520LongCat-Flash-Thinking%252C%2520an%2520efficient%2520560-billion-parameter%250Aopen-source%2520Mixture-of-Experts%2520%2528MoE%2529%2520reasoning%2520model.%2520Its%2520advanced%2520capabilities%250Aare%2520cultivated%2520through%2520a%2520meticulously%2520crafted%2520training%2520process%252C%2520beginning%2520with%250Along%2520Chain-of-Thought%2520%2528CoT%2529%2520data%2520cold-start%2520and%2520culminating%2520in%2520large-scale%250AReinforcement%2520Learning%2520%2528RL%2529.%2520We%2520first%2520employ%2520a%2520well-designed%2520cold-start%250Atraining%2520strategy%252C%2520which%2520significantly%2520enhances%2520the%2520reasoning%2520potential%2520and%250Aequips%2520the%2520model%2520with%2520specialized%2520skills%2520in%2520both%2520formal%2520and%2520agentic%2520reasoning.%250AThen%252C%2520a%2520core%2520innovation%2520is%2520our%2520domain-parallel%2520training%2520scheme%252C%2520which%2520decouples%250Aoptimization%2520across%2520distinct%2520domains%2520%2528e.g.%252C%2520STEM%252C%2520Code%252C%2520Agentic%2529%2520and%250Asubsequently%2520fuses%2520the%2520resulting%2520expert%2520models%2520into%2520a%2520single%252C%2520nearly%250APareto-optimal%2520model.%2520This%2520entire%2520process%2520is%2520powered%2520by%2520our%2520Dynamic%250AORchestration%2520for%2520Asynchronous%2520rollout%2520%2528DORA%2529%2520system%252C%2520a%2520large-scale%2520RL%250Aframework%2520that%2520delivers%2520a%2520greater%2520than%2520threefold%2520training%2520speedup%2520over%250Asynchronous%2520methods%2520on%2520tens%2520of%2520thousands%2520of%2520accelerators.%2520As%2520a%2520result%252C%250ALongCat-Flash-Thinking%2520achieves%2520state-of-the-art%2520performance%2520among%2520open-source%250Amodels%2520on%2520a%2520suite%2520of%2520complex%2520reasoning%2520tasks.%2520The%2520model%2520exhibits%2520exceptional%250Aefficiency%2520in%2520agentic%2520reasoning%252C%2520reducing%2520average%2520token%2520consumption%2520by%252064.5%2525%250A%2528from%252019%252C%2520653%2520to%25206%252C%2520965%2529%2520on%2520AIME-25%252C%2520without%2520degrading%2520task%2520accuracy.%2520We%250Arelease%2520LongCat-Flash-Thinking%2520to%2520promote%2520further%2520advances%2520in%2520reasoning%2520systems%250Aand%2520agentic%2520AI%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongCat-Flash-Thinking%20Technical%20Report&entry.906535625=%20Meituan%20LongCat%20Team%20and%20Anchun%20Gui%20and%20Bei%20Li%20and%20Bingyang%20Tao%20and%20Bole%20Zhou%20and%20Borun%20Chen%20and%20Chao%20Zhang%20and%20Chao%20Zhang%20and%20Chengcheng%20Han%20and%20Chenhui%20Yang%20and%20Chi%20Zhang%20and%20Chong%20Peng%20and%20Chuyu%20Zhang%20and%20Cong%20Chen%20and%20Fengcun%20Li%20and%20Gang%20Xu%20and%20Guoyuan%20Lin%20and%20Hao%20Jiang%20and%20Hao%20Liang%20and%20Haomin%20Fu%20and%20Haoxiang%20Ma%20and%20Hong%20Liu%20and%20Hongyan%20Hao%20and%20Hongyin%20Tang%20and%20Hongyu%20Zang%20and%20Hongzhi%20Ni%20and%20Hui%20Su%20and%20Jiahao%20Liu%20and%20Jiahuan%20Li%20and%20Jialin%20Liu%20and%20Jianfei%20Zhang%20and%20Jianhao%20Xu%20and%20Jianing%20Wang%20and%20Jiaqi%20Sun%20and%20Jiaqi%20Zhang%20and%20Jiarong%20Shi%20and%20Jiawei%20Yang%20and%20Jingang%20Wang%20and%20Jinrui%20Ding%20and%20Jun%20Kuang%20and%20Jun%20Xu%20and%20Ke%20He%20and%20Kefeng%20Zhang%20and%20Keheng%20Wang%20and%20Keqing%20He%20and%20Li%20Wei%20and%20Liang%20Shi%20and%20Lin%20Qiu%20and%20Lingbin%20Kong%20and%20Lingchuan%20Liu%20and%20Linsen%20Guo%20and%20Longfei%20An%20and%20Mai%20Xia%20and%20Meng%20Zhou%20and%20Mengshen%20Zhu%20and%20Peng%20Pei%20and%20Pengcheng%20Jia%20and%20Qi%20Gu%20and%20Qi%20Guo%20and%20Qiong%20Huang%20and%20Quan%20Chen%20and%20Quanchi%20Weng%20and%20Rongxiang%20Weng%20and%20Ruichen%20Shao%20and%20Rumei%20Li%20and%20Shanglin%20Lei%20and%20Shuai%20Du%20and%20Shuaikang%20Liu%20and%20Shuang%20Zhou%20and%20Shuhao%20Hu%20and%20Siyu%20Xu%20and%20Songshan%20Gong%20and%20Tao%20Liang%20and%20Tianhao%20Hu%20and%20Wei%20He%20and%20Wei%20Shi%20and%20Wei%20Wang%20and%20Wei%20Wu%20and%20Wei%20Zhuo%20and%20Weifeng%20Tang%20and%20Wenjie%20Shi%20and%20Wenlong%20Zhu%20and%20Xi%20Su%20and%20Xiangcheng%20Liu%20and%20Xiangyu%20Xi%20and%20Xiangzhou%20Huang%20and%20Xiao%20Liu%20and%20Xiaochen%20Jiang%20and%20Xiaowei%20Shi%20and%20Xiaowen%20Shi%20and%20Xiaoyu%20Li%20and%20Xin%20Chen%20and%20Xinyue%20Zhao%20and%20Xuan%20Huang%20and%20Xuemiao%20Zhang%20and%20Xuezhi%20Cao%20and%20Xunliang%20Cai%20and%20Yajie%20Zhang%20and%20Yang%20Chen%20and%20Yang%20Liu%20and%20Yang%20Liu%20and%20Yang%20Zheng%20and%20Yaoming%20Wang%20and%20Yaqi%20Huo%20and%20Yerui%20Sun%20and%20Yifan%20Lu%20and%20Yiyang%20Li%20and%20Youshao%20Xiao%20and%20Yuanzhe%20Lei%20and%20Yuchen%20Xie%20and%20Yueqing%20Sun%20and%20Yufei%20Zhang%20and%20Yuhuai%20Wei%20and%20Yulei%20Qian%20and%20Yunke%20Zhao%20and%20Yuqing%20Ding%20and%20Yuwei%20Jiang%20and%20Zhaohua%20Yang%20and%20Zhengyu%20Chen%20and%20Zhijian%20Liu%20and%20Zhikang%20Xia%20and%20Zhongda%20Su%20and%20Ziran%20Li%20and%20Ziwen%20Wang%20and%20Ziyuan%20Zhuang%20and%20Zongyu%20Wang%20and%20Zunyuan%20Yang&entry.1292438233=%20%20We%20present%20LongCat-Flash-Thinking%2C%20an%20efficient%20560-billion-parameter%0Aopen-source%20Mixture-of-Experts%20%28MoE%29%20reasoning%20model.%20Its%20advanced%20capabilities%0Aare%20cultivated%20through%20a%20meticulously%20crafted%20training%20process%2C%20beginning%20with%0Along%20Chain-of-Thought%20%28CoT%29%20data%20cold-start%20and%20culminating%20in%20large-scale%0AReinforcement%20Learning%20%28RL%29.%20We%20first%20employ%20a%20well-designed%20cold-start%0Atraining%20strategy%2C%20which%20significantly%20enhances%20the%20reasoning%20potential%20and%0Aequips%20the%20model%20with%20specialized%20skills%20in%20both%20formal%20and%20agentic%20reasoning.%0AThen%2C%20a%20core%20innovation%20is%20our%20domain-parallel%20training%20scheme%2C%20which%20decouples%0Aoptimization%20across%20distinct%20domains%20%28e.g.%2C%20STEM%2C%20Code%2C%20Agentic%29%20and%0Asubsequently%20fuses%20the%20resulting%20expert%20models%20into%20a%20single%2C%20nearly%0APareto-optimal%20model.%20This%20entire%20process%20is%20powered%20by%20our%20Dynamic%0AORchestration%20for%20Asynchronous%20rollout%20%28DORA%29%20system%2C%20a%20large-scale%20RL%0Aframework%20that%20delivers%20a%20greater%20than%20threefold%20training%20speedup%20over%0Asynchronous%20methods%20on%20tens%20of%20thousands%20of%20accelerators.%20As%20a%20result%2C%0ALongCat-Flash-Thinking%20achieves%20state-of-the-art%20performance%20among%20open-source%0Amodels%20on%20a%20suite%20of%20complex%20reasoning%20tasks.%20The%20model%20exhibits%20exceptional%0Aefficiency%20in%20agentic%20reasoning%2C%20reducing%20average%20token%20consumption%20by%2064.5%25%0A%28from%2019%2C%20653%20to%206%2C%20965%29%20on%20AIME-25%2C%20without%20degrading%20task%20accuracy.%20We%0Arelease%20LongCat-Flash-Thinking%20to%20promote%20further%20advances%20in%20reasoning%20systems%0Aand%20agentic%20AI%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18883v1&entry.124074799=Read"},
{"title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under\n  Real-World Physical Variations", "author": "Hanqing Liu and Jiahuan Long and Junqi Wu and Jiacheng Hou and Huili Tang and Tingsong Jiang and Weien Zhou and Wen Yao", "abstract": "  Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges.\n", "link": "http://arxiv.org/abs/2509.18953v1", "date": "2025-09-23", "relevancy": 1.6878, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5767}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5621}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eva-VLA%3A%20Evaluating%20Vision-Language-Action%20Models%27%20Robustness%20Under%0A%20%20Real-World%20Physical%20Variations&body=Title%3A%20Eva-VLA%3A%20Evaluating%20Vision-Language-Action%20Models%27%20Robustness%20Under%0A%20%20Real-World%20Physical%20Variations%0AAuthor%3A%20Hanqing%20Liu%20and%20Jiahuan%20Long%20and%20Junqi%20Wu%20and%20Jiacheng%20Hou%20and%20Huili%20Tang%20and%20Tingsong%20Jiang%20and%20Weien%20Zhou%20and%20Wen%20Yao%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20promising%20solutions%20for%0Arobotic%20manipulation%2C%20yet%20their%20robustness%20to%20real-world%20physical%20variations%0Aremains%20critically%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20propose%20Eva-VLA%2C%20the%0Afirst%20unified%20framework%20that%20systematically%20evaluates%20the%20robustness%20of%20VLA%0Amodels%20by%20transforming%20discrete%20physical%20variations%20into%20continuous%0Aoptimization%20problems.%20However%2C%20comprehensively%20assessing%20VLA%20robustness%0Apresents%20two%20key%20challenges%3A%20%281%29%20how%20to%20systematically%20characterize%20diverse%0Aphysical%20variations%20encountered%20in%20real-world%20deployments%20while%20maintaining%0Aevaluation%20reproducibility%2C%20and%20%282%29%20how%20to%20discover%20worst-case%20scenarios%0Awithout%20prohibitive%20real-world%20data%20collection%20costs%20efficiently.%20To%20address%0Athe%20first%20challenge%2C%20we%20decompose%20real-world%20variations%20into%20three%20critical%0Adomains%3A%20object%203D%20transformations%20that%20affect%20spatial%20reasoning%2C%20illumination%0Avariations%20that%20challenge%20visual%20perception%2C%20and%20adversarial%20patches%20that%0Adisrupt%20scene%20understanding.%20For%20the%20second%20challenge%2C%20we%20introduce%20a%0Acontinuous%20black-box%20optimization%20framework%20that%20transforms%20discrete%20physical%0Avariations%20into%20parameter%20optimization%2C%20enabling%20systematic%20exploration%20of%0Aworst-case%20scenarios.%20Extensive%20experiments%20on%20state-of-the-art%20OpenVLA%20models%0Aacross%20multiple%20benchmarks%20reveal%20alarming%20vulnerabilities%3A%20all%20variation%20types%0Atrigger%20failure%20rates%20exceeding%2060%25%2C%20with%20object%20transformations%20causing%20up%20to%0A97.8%25%20failure%20in%20long-horizon%20tasks.%20Our%20findings%20expose%20critical%20gaps%20between%0Acontrolled%20laboratory%20success%20and%20unpredictable%20deployment%20readiness%2C%20while%20the%0AEva-VLA%20framework%20provides%20a%20practical%20pathway%20for%20hardening%20VLA-based%20robotic%0Amanipulation%20models%20against%20real-world%20deployment%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEva-VLA%253A%2520Evaluating%2520Vision-Language-Action%2520Models%2527%2520Robustness%2520Under%250A%2520%2520Real-World%2520Physical%2520Variations%26entry.906535625%3DHanqing%2520Liu%2520and%2520Jiahuan%2520Long%2520and%2520Junqi%2520Wu%2520and%2520Jiacheng%2520Hou%2520and%2520Huili%2520Tang%2520and%2520Tingsong%2520Jiang%2520and%2520Weien%2520Zhou%2520and%2520Wen%2520Yao%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520emerged%2520as%2520promising%2520solutions%2520for%250Arobotic%2520manipulation%252C%2520yet%2520their%2520robustness%2520to%2520real-world%2520physical%2520variations%250Aremains%2520critically%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520Eva-VLA%252C%2520the%250Afirst%2520unified%2520framework%2520that%2520systematically%2520evaluates%2520the%2520robustness%2520of%2520VLA%250Amodels%2520by%2520transforming%2520discrete%2520physical%2520variations%2520into%2520continuous%250Aoptimization%2520problems.%2520However%252C%2520comprehensively%2520assessing%2520VLA%2520robustness%250Apresents%2520two%2520key%2520challenges%253A%2520%25281%2529%2520how%2520to%2520systematically%2520characterize%2520diverse%250Aphysical%2520variations%2520encountered%2520in%2520real-world%2520deployments%2520while%2520maintaining%250Aevaluation%2520reproducibility%252C%2520and%2520%25282%2529%2520how%2520to%2520discover%2520worst-case%2520scenarios%250Awithout%2520prohibitive%2520real-world%2520data%2520collection%2520costs%2520efficiently.%2520To%2520address%250Athe%2520first%2520challenge%252C%2520we%2520decompose%2520real-world%2520variations%2520into%2520three%2520critical%250Adomains%253A%2520object%25203D%2520transformations%2520that%2520affect%2520spatial%2520reasoning%252C%2520illumination%250Avariations%2520that%2520challenge%2520visual%2520perception%252C%2520and%2520adversarial%2520patches%2520that%250Adisrupt%2520scene%2520understanding.%2520For%2520the%2520second%2520challenge%252C%2520we%2520introduce%2520a%250Acontinuous%2520black-box%2520optimization%2520framework%2520that%2520transforms%2520discrete%2520physical%250Avariations%2520into%2520parameter%2520optimization%252C%2520enabling%2520systematic%2520exploration%2520of%250Aworst-case%2520scenarios.%2520Extensive%2520experiments%2520on%2520state-of-the-art%2520OpenVLA%2520models%250Aacross%2520multiple%2520benchmarks%2520reveal%2520alarming%2520vulnerabilities%253A%2520all%2520variation%2520types%250Atrigger%2520failure%2520rates%2520exceeding%252060%2525%252C%2520with%2520object%2520transformations%2520causing%2520up%2520to%250A97.8%2525%2520failure%2520in%2520long-horizon%2520tasks.%2520Our%2520findings%2520expose%2520critical%2520gaps%2520between%250Acontrolled%2520laboratory%2520success%2520and%2520unpredictable%2520deployment%2520readiness%252C%2520while%2520the%250AEva-VLA%2520framework%2520provides%2520a%2520practical%2520pathway%2520for%2520hardening%2520VLA-based%2520robotic%250Amanipulation%2520models%2520against%2520real-world%2520deployment%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eva-VLA%3A%20Evaluating%20Vision-Language-Action%20Models%27%20Robustness%20Under%0A%20%20Real-World%20Physical%20Variations&entry.906535625=Hanqing%20Liu%20and%20Jiahuan%20Long%20and%20Junqi%20Wu%20and%20Jiacheng%20Hou%20and%20Huili%20Tang%20and%20Tingsong%20Jiang%20and%20Weien%20Zhou%20and%20Wen%20Yao&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20promising%20solutions%20for%0Arobotic%20manipulation%2C%20yet%20their%20robustness%20to%20real-world%20physical%20variations%0Aremains%20critically%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20propose%20Eva-VLA%2C%20the%0Afirst%20unified%20framework%20that%20systematically%20evaluates%20the%20robustness%20of%20VLA%0Amodels%20by%20transforming%20discrete%20physical%20variations%20into%20continuous%0Aoptimization%20problems.%20However%2C%20comprehensively%20assessing%20VLA%20robustness%0Apresents%20two%20key%20challenges%3A%20%281%29%20how%20to%20systematically%20characterize%20diverse%0Aphysical%20variations%20encountered%20in%20real-world%20deployments%20while%20maintaining%0Aevaluation%20reproducibility%2C%20and%20%282%29%20how%20to%20discover%20worst-case%20scenarios%0Awithout%20prohibitive%20real-world%20data%20collection%20costs%20efficiently.%20To%20address%0Athe%20first%20challenge%2C%20we%20decompose%20real-world%20variations%20into%20three%20critical%0Adomains%3A%20object%203D%20transformations%20that%20affect%20spatial%20reasoning%2C%20illumination%0Avariations%20that%20challenge%20visual%20perception%2C%20and%20adversarial%20patches%20that%0Adisrupt%20scene%20understanding.%20For%20the%20second%20challenge%2C%20we%20introduce%20a%0Acontinuous%20black-box%20optimization%20framework%20that%20transforms%20discrete%20physical%0Avariations%20into%20parameter%20optimization%2C%20enabling%20systematic%20exploration%20of%0Aworst-case%20scenarios.%20Extensive%20experiments%20on%20state-of-the-art%20OpenVLA%20models%0Aacross%20multiple%20benchmarks%20reveal%20alarming%20vulnerabilities%3A%20all%20variation%20types%0Atrigger%20failure%20rates%20exceeding%2060%25%2C%20with%20object%20transformations%20causing%20up%20to%0A97.8%25%20failure%20in%20long-horizon%20tasks.%20Our%20findings%20expose%20critical%20gaps%20between%0Acontrolled%20laboratory%20success%20and%20unpredictable%20deployment%20readiness%2C%20while%20the%0AEva-VLA%20framework%20provides%20a%20practical%20pathway%20for%20hardening%20VLA-based%20robotic%0Amanipulation%20models%20against%20real-world%20deployment%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18953v1&entry.124074799=Read"},
{"title": "Analysis on distribution and clustering of weight", "author": "Chunming Ye and Wenquan Tian and Yalan Gao and Songzhou Li", "abstract": "  The study on architecture and parameter characteristics remains the hot topic\nin the research of large language models. In this paper we concern with the\ncharacteristics of weight which are used to analyze the correlations and\ndifferences between models. Two kinds of vectors-standard deviation vector and\nclustering vector-are proposed to describe features of models. In the first\ncase, the weights are assumed to follow normal distribution. The standard\ndeviation values of projection matrices are normalized to form\nStandard-Deviation Vector, representing the distribution characteristics of\nmodels. In the second case, the singular values from each weight projection\nmatrix are extracted and grouped by K-Means algorithm. The grouped data with\nthe same type matrix are combined as Clustering Vector to represent the\ncorrelation characteristics of models' weights. The study reveals that these\ntwo vectors can effectively distinguish between different models and clearly\nshow the similarities among models of the same family. Moreover, after\nconducting LoRA fine-tuning with different datasets and models, it is found\nthat the distribution of weights represented by standard deviation vector is\ndirectly influenced by the dataset, but the correlations between different\nweights represented by clustering vector remain unaffected and maintain a high\nconsistency with the pre-trained model.\n", "link": "http://arxiv.org/abs/2509.19122v1", "date": "2025-09-23", "relevancy": 1.872, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20on%20distribution%20and%20clustering%20of%20weight&body=Title%3A%20Analysis%20on%20distribution%20and%20clustering%20of%20weight%0AAuthor%3A%20Chunming%20Ye%20and%20Wenquan%20Tian%20and%20Yalan%20Gao%20and%20Songzhou%20Li%0AAbstract%3A%20%20%20The%20study%20on%20architecture%20and%20parameter%20characteristics%20remains%20the%20hot%20topic%0Ain%20the%20research%20of%20large%20language%20models.%20In%20this%20paper%20we%20concern%20with%20the%0Acharacteristics%20of%20weight%20which%20are%20used%20to%20analyze%20the%20correlations%20and%0Adifferences%20between%20models.%20Two%20kinds%20of%20vectors-standard%20deviation%20vector%20and%0Aclustering%20vector-are%20proposed%20to%20describe%20features%20of%20models.%20In%20the%20first%0Acase%2C%20the%20weights%20are%20assumed%20to%20follow%20normal%20distribution.%20The%20standard%0Adeviation%20values%20of%20projection%20matrices%20are%20normalized%20to%20form%0AStandard-Deviation%20Vector%2C%20representing%20the%20distribution%20characteristics%20of%0Amodels.%20In%20the%20second%20case%2C%20the%20singular%20values%20from%20each%20weight%20projection%0Amatrix%20are%20extracted%20and%20grouped%20by%20K-Means%20algorithm.%20The%20grouped%20data%20with%0Athe%20same%20type%20matrix%20are%20combined%20as%20Clustering%20Vector%20to%20represent%20the%0Acorrelation%20characteristics%20of%20models%27%20weights.%20The%20study%20reveals%20that%20these%0Atwo%20vectors%20can%20effectively%20distinguish%20between%20different%20models%20and%20clearly%0Ashow%20the%20similarities%20among%20models%20of%20the%20same%20family.%20Moreover%2C%20after%0Aconducting%20LoRA%20fine-tuning%20with%20different%20datasets%20and%20models%2C%20it%20is%20found%0Athat%20the%20distribution%20of%20weights%20represented%20by%20standard%20deviation%20vector%20is%0Adirectly%20influenced%20by%20the%20dataset%2C%20but%20the%20correlations%20between%20different%0Aweights%20represented%20by%20clustering%20vector%20remain%20unaffected%20and%20maintain%20a%20high%0Aconsistency%20with%20the%20pre-trained%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520on%2520distribution%2520and%2520clustering%2520of%2520weight%26entry.906535625%3DChunming%2520Ye%2520and%2520Wenquan%2520Tian%2520and%2520Yalan%2520Gao%2520and%2520Songzhou%2520Li%26entry.1292438233%3D%2520%2520The%2520study%2520on%2520architecture%2520and%2520parameter%2520characteristics%2520remains%2520the%2520hot%2520topic%250Ain%2520the%2520research%2520of%2520large%2520language%2520models.%2520In%2520this%2520paper%2520we%2520concern%2520with%2520the%250Acharacteristics%2520of%2520weight%2520which%2520are%2520used%2520to%2520analyze%2520the%2520correlations%2520and%250Adifferences%2520between%2520models.%2520Two%2520kinds%2520of%2520vectors-standard%2520deviation%2520vector%2520and%250Aclustering%2520vector-are%2520proposed%2520to%2520describe%2520features%2520of%2520models.%2520In%2520the%2520first%250Acase%252C%2520the%2520weights%2520are%2520assumed%2520to%2520follow%2520normal%2520distribution.%2520The%2520standard%250Adeviation%2520values%2520of%2520projection%2520matrices%2520are%2520normalized%2520to%2520form%250AStandard-Deviation%2520Vector%252C%2520representing%2520the%2520distribution%2520characteristics%2520of%250Amodels.%2520In%2520the%2520second%2520case%252C%2520the%2520singular%2520values%2520from%2520each%2520weight%2520projection%250Amatrix%2520are%2520extracted%2520and%2520grouped%2520by%2520K-Means%2520algorithm.%2520The%2520grouped%2520data%2520with%250Athe%2520same%2520type%2520matrix%2520are%2520combined%2520as%2520Clustering%2520Vector%2520to%2520represent%2520the%250Acorrelation%2520characteristics%2520of%2520models%2527%2520weights.%2520The%2520study%2520reveals%2520that%2520these%250Atwo%2520vectors%2520can%2520effectively%2520distinguish%2520between%2520different%2520models%2520and%2520clearly%250Ashow%2520the%2520similarities%2520among%2520models%2520of%2520the%2520same%2520family.%2520Moreover%252C%2520after%250Aconducting%2520LoRA%2520fine-tuning%2520with%2520different%2520datasets%2520and%2520models%252C%2520it%2520is%2520found%250Athat%2520the%2520distribution%2520of%2520weights%2520represented%2520by%2520standard%2520deviation%2520vector%2520is%250Adirectly%2520influenced%2520by%2520the%2520dataset%252C%2520but%2520the%2520correlations%2520between%2520different%250Aweights%2520represented%2520by%2520clustering%2520vector%2520remain%2520unaffected%2520and%2520maintain%2520a%2520high%250Aconsistency%2520with%2520the%2520pre-trained%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20on%20distribution%20and%20clustering%20of%20weight&entry.906535625=Chunming%20Ye%20and%20Wenquan%20Tian%20and%20Yalan%20Gao%20and%20Songzhou%20Li&entry.1292438233=%20%20The%20study%20on%20architecture%20and%20parameter%20characteristics%20remains%20the%20hot%20topic%0Ain%20the%20research%20of%20large%20language%20models.%20In%20this%20paper%20we%20concern%20with%20the%0Acharacteristics%20of%20weight%20which%20are%20used%20to%20analyze%20the%20correlations%20and%0Adifferences%20between%20models.%20Two%20kinds%20of%20vectors-standard%20deviation%20vector%20and%0Aclustering%20vector-are%20proposed%20to%20describe%20features%20of%20models.%20In%20the%20first%0Acase%2C%20the%20weights%20are%20assumed%20to%20follow%20normal%20distribution.%20The%20standard%0Adeviation%20values%20of%20projection%20matrices%20are%20normalized%20to%20form%0AStandard-Deviation%20Vector%2C%20representing%20the%20distribution%20characteristics%20of%0Amodels.%20In%20the%20second%20case%2C%20the%20singular%20values%20from%20each%20weight%20projection%0Amatrix%20are%20extracted%20and%20grouped%20by%20K-Means%20algorithm.%20The%20grouped%20data%20with%0Athe%20same%20type%20matrix%20are%20combined%20as%20Clustering%20Vector%20to%20represent%20the%0Acorrelation%20characteristics%20of%20models%27%20weights.%20The%20study%20reveals%20that%20these%0Atwo%20vectors%20can%20effectively%20distinguish%20between%20different%20models%20and%20clearly%0Ashow%20the%20similarities%20among%20models%20of%20the%20same%20family.%20Moreover%2C%20after%0Aconducting%20LoRA%20fine-tuning%20with%20different%20datasets%20and%20models%2C%20it%20is%20found%0Athat%20the%20distribution%20of%20weights%20represented%20by%20standard%20deviation%20vector%20is%0Adirectly%20influenced%20by%20the%20dataset%2C%20but%20the%20correlations%20between%20different%0Aweights%20represented%20by%20clustering%20vector%20remain%20unaffected%20and%20maintain%20a%20high%0Aconsistency%20with%20the%20pre-trained%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19122v1&entry.124074799=Read"},
{"title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs", "author": "Richard Cornelius Suwandi and Feng Yin and Juntao Wang and Renjie Li and Tsung-Hui Chang and Sergios Theodoridis", "abstract": "  The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/richardcsuwandi/cake.\n", "link": "http://arxiv.org/abs/2509.17998v2", "date": "2025-09-23", "relevancy": 1.5001, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4995}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Kernel%20Design%20for%20Bayesian%20Optimization%20Is%20a%20Piece%20of%20CAKE%20with%0A%20%20LLMs&body=Title%3A%20Adaptive%20Kernel%20Design%20for%20Bayesian%20Optimization%20Is%20a%20Piece%20of%20CAKE%20with%0A%20%20LLMs%0AAuthor%3A%20Richard%20Cornelius%20Suwandi%20and%20Feng%20Yin%20and%20Juntao%20Wang%20and%20Renjie%20Li%20and%20Tsung-Hui%20Chang%20and%20Sergios%20Theodoridis%0AAbstract%3A%20%20%20The%20efficiency%20of%20Bayesian%20optimization%20%28BO%29%20relies%20heavily%20on%20the%20choice%20of%0Athe%20Gaussian%20process%20%28GP%29%20kernel%2C%20which%20plays%20a%20central%20role%20in%20balancing%0Aexploration%20and%20exploitation%20under%20limited%20evaluation%20budgets.%20Traditional%20BO%0Amethods%20often%20rely%20on%20fixed%20or%20heuristic%20kernel%20selection%20strategies%2C%20which%20can%0Aresult%20in%20slow%20convergence%20or%20suboptimal%20solutions%20when%20the%20chosen%20kernel%20is%0Apoorly%20suited%20to%20the%20underlying%20objective%20function.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20freshly-baked%20Context-Aware%20Kernel%20Evolution%20%28CAKE%29%20to%20enhance%20BO%0Awith%20large%20language%20models%20%28LLMs%29.%20Concretely%2C%20CAKE%20leverages%20LLMs%20as%20the%0Acrossover%20and%20mutation%20operators%20to%20adaptively%20generate%20and%20refine%20GP%20kernels%0Abased%20on%20the%20observed%20data%20throughout%20the%20optimization%20process.%20To%20maximize%20the%0Apower%20of%20CAKE%2C%20we%20further%20propose%20BIC-Acquisition%20Kernel%20Ranking%20%28BAKER%29%20to%0Aselect%20the%20most%20effective%20kernel%20through%20balancing%20the%20model%20fit%20measured%20by%0Athe%20Bayesian%20information%20criterion%20%28BIC%29%20with%20the%20expected%20improvement%20at%20each%0Aiteration%20of%20BO.%20Extensive%20experiments%20demonstrate%20that%20our%20fresh%20CAKE-based%20BO%0Amethod%20consistently%20outperforms%20established%20baselines%20across%20a%20range%20of%0Areal-world%20tasks%2C%20including%20hyperparameter%20optimization%2C%20controller%20tuning%2C%20and%0Aphotonic%20chip%20design.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/richardcsuwandi/cake.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Kernel%2520Design%2520for%2520Bayesian%2520Optimization%2520Is%2520a%2520Piece%2520of%2520CAKE%2520with%250A%2520%2520LLMs%26entry.906535625%3DRichard%2520Cornelius%2520Suwandi%2520and%2520Feng%2520Yin%2520and%2520Juntao%2520Wang%2520and%2520Renjie%2520Li%2520and%2520Tsung-Hui%2520Chang%2520and%2520Sergios%2520Theodoridis%26entry.1292438233%3D%2520%2520The%2520efficiency%2520of%2520Bayesian%2520optimization%2520%2528BO%2529%2520relies%2520heavily%2520on%2520the%2520choice%2520of%250Athe%2520Gaussian%2520process%2520%2528GP%2529%2520kernel%252C%2520which%2520plays%2520a%2520central%2520role%2520in%2520balancing%250Aexploration%2520and%2520exploitation%2520under%2520limited%2520evaluation%2520budgets.%2520Traditional%2520BO%250Amethods%2520often%2520rely%2520on%2520fixed%2520or%2520heuristic%2520kernel%2520selection%2520strategies%252C%2520which%2520can%250Aresult%2520in%2520slow%2520convergence%2520or%2520suboptimal%2520solutions%2520when%2520the%2520chosen%2520kernel%2520is%250Apoorly%2520suited%2520to%2520the%2520underlying%2520objective%2520function.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520a%2520freshly-baked%2520Context-Aware%2520Kernel%2520Evolution%2520%2528CAKE%2529%2520to%2520enhance%2520BO%250Awith%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Concretely%252C%2520CAKE%2520leverages%2520LLMs%2520as%2520the%250Acrossover%2520and%2520mutation%2520operators%2520to%2520adaptively%2520generate%2520and%2520refine%2520GP%2520kernels%250Abased%2520on%2520the%2520observed%2520data%2520throughout%2520the%2520optimization%2520process.%2520To%2520maximize%2520the%250Apower%2520of%2520CAKE%252C%2520we%2520further%2520propose%2520BIC-Acquisition%2520Kernel%2520Ranking%2520%2528BAKER%2529%2520to%250Aselect%2520the%2520most%2520effective%2520kernel%2520through%2520balancing%2520the%2520model%2520fit%2520measured%2520by%250Athe%2520Bayesian%2520information%2520criterion%2520%2528BIC%2529%2520with%2520the%2520expected%2520improvement%2520at%2520each%250Aiteration%2520of%2520BO.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520fresh%2520CAKE-based%2520BO%250Amethod%2520consistently%2520outperforms%2520established%2520baselines%2520across%2520a%2520range%2520of%250Areal-world%2520tasks%252C%2520including%2520hyperparameter%2520optimization%252C%2520controller%2520tuning%252C%2520and%250Aphotonic%2520chip%2520design.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/richardcsuwandi/cake.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Kernel%20Design%20for%20Bayesian%20Optimization%20Is%20a%20Piece%20of%20CAKE%20with%0A%20%20LLMs&entry.906535625=Richard%20Cornelius%20Suwandi%20and%20Feng%20Yin%20and%20Juntao%20Wang%20and%20Renjie%20Li%20and%20Tsung-Hui%20Chang%20and%20Sergios%20Theodoridis&entry.1292438233=%20%20The%20efficiency%20of%20Bayesian%20optimization%20%28BO%29%20relies%20heavily%20on%20the%20choice%20of%0Athe%20Gaussian%20process%20%28GP%29%20kernel%2C%20which%20plays%20a%20central%20role%20in%20balancing%0Aexploration%20and%20exploitation%20under%20limited%20evaluation%20budgets.%20Traditional%20BO%0Amethods%20often%20rely%20on%20fixed%20or%20heuristic%20kernel%20selection%20strategies%2C%20which%20can%0Aresult%20in%20slow%20convergence%20or%20suboptimal%20solutions%20when%20the%20chosen%20kernel%20is%0Apoorly%20suited%20to%20the%20underlying%20objective%20function.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20freshly-baked%20Context-Aware%20Kernel%20Evolution%20%28CAKE%29%20to%20enhance%20BO%0Awith%20large%20language%20models%20%28LLMs%29.%20Concretely%2C%20CAKE%20leverages%20LLMs%20as%20the%0Acrossover%20and%20mutation%20operators%20to%20adaptively%20generate%20and%20refine%20GP%20kernels%0Abased%20on%20the%20observed%20data%20throughout%20the%20optimization%20process.%20To%20maximize%20the%0Apower%20of%20CAKE%2C%20we%20further%20propose%20BIC-Acquisition%20Kernel%20Ranking%20%28BAKER%29%20to%0Aselect%20the%20most%20effective%20kernel%20through%20balancing%20the%20model%20fit%20measured%20by%0Athe%20Bayesian%20information%20criterion%20%28BIC%29%20with%20the%20expected%20improvement%20at%20each%0Aiteration%20of%20BO.%20Extensive%20experiments%20demonstrate%20that%20our%20fresh%20CAKE-based%20BO%0Amethod%20consistently%20outperforms%20established%20baselines%20across%20a%20range%20of%0Areal-world%20tasks%2C%20including%20hyperparameter%20optimization%2C%20controller%20tuning%2C%20and%0Aphotonic%20chip%20design.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/richardcsuwandi/cake.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17998v2&entry.124074799=Read"},
{"title": "PPG-Distill: Efficient Photoplethysmography Signals Analysis via\n  Foundation Model Distillation", "author": "Juntong Ni and Saurabh Kataria and Shengpu Tang and Carl Yang and Xiao Hu and Wei Jin", "abstract": "  Photoplethysmography (PPG) is widely used in wearable health monitoring, yet\nlarge PPG foundation models remain difficult to deploy on resource-limited\ndevices. We present PPG-Distill, a knowledge distillation framework that\ntransfers both global and local knowledge through prediction-, feature-, and\npatch-level distillation. PPG-Distill incorporates morphology distillation to\npreserve local waveform patterns and rhythm distillation to capture inter-patch\ntemporal structures. On heart rate estimation and atrial fibrillation\ndetection, PPG-Distill improves student performance by up to 21.8% while\nachieving 7X faster inference and reducing memory usage by 19X, enabling\nefficient PPG analysis on wearables\n", "link": "http://arxiv.org/abs/2509.19215v1", "date": "2025-09-23", "relevancy": 2.0404, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.51}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PPG-Distill%3A%20Efficient%20Photoplethysmography%20Signals%20Analysis%20via%0A%20%20Foundation%20Model%20Distillation&body=Title%3A%20PPG-Distill%3A%20Efficient%20Photoplethysmography%20Signals%20Analysis%20via%0A%20%20Foundation%20Model%20Distillation%0AAuthor%3A%20Juntong%20Ni%20and%20Saurabh%20Kataria%20and%20Shengpu%20Tang%20and%20Carl%20Yang%20and%20Xiao%20Hu%20and%20Wei%20Jin%0AAbstract%3A%20%20%20Photoplethysmography%20%28PPG%29%20is%20widely%20used%20in%20wearable%20health%20monitoring%2C%20yet%0Alarge%20PPG%20foundation%20models%20remain%20difficult%20to%20deploy%20on%20resource-limited%0Adevices.%20We%20present%20PPG-Distill%2C%20a%20knowledge%20distillation%20framework%20that%0Atransfers%20both%20global%20and%20local%20knowledge%20through%20prediction-%2C%20feature-%2C%20and%0Apatch-level%20distillation.%20PPG-Distill%20incorporates%20morphology%20distillation%20to%0Apreserve%20local%20waveform%20patterns%20and%20rhythm%20distillation%20to%20capture%20inter-patch%0Atemporal%20structures.%20On%20heart%20rate%20estimation%20and%20atrial%20fibrillation%0Adetection%2C%20PPG-Distill%20improves%20student%20performance%20by%20up%20to%2021.8%25%20while%0Aachieving%207X%20faster%20inference%20and%20reducing%20memory%20usage%20by%2019X%2C%20enabling%0Aefficient%20PPG%20analysis%20on%20wearables%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPPG-Distill%253A%2520Efficient%2520Photoplethysmography%2520Signals%2520Analysis%2520via%250A%2520%2520Foundation%2520Model%2520Distillation%26entry.906535625%3DJuntong%2520Ni%2520and%2520Saurabh%2520Kataria%2520and%2520Shengpu%2520Tang%2520and%2520Carl%2520Yang%2520and%2520Xiao%2520Hu%2520and%2520Wei%2520Jin%26entry.1292438233%3D%2520%2520Photoplethysmography%2520%2528PPG%2529%2520is%2520widely%2520used%2520in%2520wearable%2520health%2520monitoring%252C%2520yet%250Alarge%2520PPG%2520foundation%2520models%2520remain%2520difficult%2520to%2520deploy%2520on%2520resource-limited%250Adevices.%2520We%2520present%2520PPG-Distill%252C%2520a%2520knowledge%2520distillation%2520framework%2520that%250Atransfers%2520both%2520global%2520and%2520local%2520knowledge%2520through%2520prediction-%252C%2520feature-%252C%2520and%250Apatch-level%2520distillation.%2520PPG-Distill%2520incorporates%2520morphology%2520distillation%2520to%250Apreserve%2520local%2520waveform%2520patterns%2520and%2520rhythm%2520distillation%2520to%2520capture%2520inter-patch%250Atemporal%2520structures.%2520On%2520heart%2520rate%2520estimation%2520and%2520atrial%2520fibrillation%250Adetection%252C%2520PPG-Distill%2520improves%2520student%2520performance%2520by%2520up%2520to%252021.8%2525%2520while%250Aachieving%25207X%2520faster%2520inference%2520and%2520reducing%2520memory%2520usage%2520by%252019X%252C%2520enabling%250Aefficient%2520PPG%2520analysis%2520on%2520wearables%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PPG-Distill%3A%20Efficient%20Photoplethysmography%20Signals%20Analysis%20via%0A%20%20Foundation%20Model%20Distillation&entry.906535625=Juntong%20Ni%20and%20Saurabh%20Kataria%20and%20Shengpu%20Tang%20and%20Carl%20Yang%20and%20Xiao%20Hu%20and%20Wei%20Jin&entry.1292438233=%20%20Photoplethysmography%20%28PPG%29%20is%20widely%20used%20in%20wearable%20health%20monitoring%2C%20yet%0Alarge%20PPG%20foundation%20models%20remain%20difficult%20to%20deploy%20on%20resource-limited%0Adevices.%20We%20present%20PPG-Distill%2C%20a%20knowledge%20distillation%20framework%20that%0Atransfers%20both%20global%20and%20local%20knowledge%20through%20prediction-%2C%20feature-%2C%20and%0Apatch-level%20distillation.%20PPG-Distill%20incorporates%20morphology%20distillation%20to%0Apreserve%20local%20waveform%20patterns%20and%20rhythm%20distillation%20to%20capture%20inter-patch%0Atemporal%20structures.%20On%20heart%20rate%20estimation%20and%20atrial%20fibrillation%0Adetection%2C%20PPG-Distill%20improves%20student%20performance%20by%20up%20to%2021.8%25%20while%0Aachieving%207X%20faster%20inference%20and%20reducing%20memory%20usage%20by%2019X%2C%20enabling%0Aefficient%20PPG%20analysis%20on%20wearables%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19215v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


