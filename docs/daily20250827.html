<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250826.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MCGS: Multiview Consistency Enhancement for Sparse-View 3D Gaussian\n  Radiance Fields", "author": "Yuru Xiao and Deming Zhai and Wenbo Zhao and Kui Jiang and Junjun Jiang and Xianming Liu", "abstract": "  Radiance fields represented by 3D Gaussians excel at synthesizing novel\nviews, offering both high training efficiency and fast rendering. However, with\nsparse input views, the lack of multi-view consistency constraints results in\npoorly initialized Gaussians and unreliable heuristics for optimization,\nleading to suboptimal performance. Existing methods often incorporate depth\npriors from dense estimation networks but overlook the inherent multi-view\nconsistency in input images. Additionally, they rely on dense initialization,\nwhich limits the efficiency of scene representation. To overcome these\nchallenges, we propose a view synthesis framework based on 3D Gaussian\nSplatting, named MCGS, enabling photorealistic scene reconstruction from sparse\nviews. The key innovations of MCGS in enhancing multi-view consistency are as\nfollows: i) We leverage matching priors from a sparse matcher to initialize\nGaussians primarily on textured regions, while low-texture areas are populated\nwith randomly distributed Gaussians. This yields a compact yet sufficient set\nof initial Gaussians. ii) We propose a multi-view consistency-guided\nprogressive pruning strategy to dynamically eliminate inconsistent Gaussians.\nThis approach confines their optimization to a consistency-constrained space,\nwhich ensures robust and coherent scene reconstruction. These strategies\nenhance robustness to sparse views, accelerate rendering, and reduce memory\nconsumption, making MCGS a practical framework for 3D Gaussian Splatting.\n", "link": "http://arxiv.org/abs/2410.11394v2", "date": "2025-08-26", "relevancy": 3.3962, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7016}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6859}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCGS%3A%20Multiview%20Consistency%20Enhancement%20for%20Sparse-View%203D%20Gaussian%0A%20%20Radiance%20Fields&body=Title%3A%20MCGS%3A%20Multiview%20Consistency%20Enhancement%20for%20Sparse-View%203D%20Gaussian%0A%20%20Radiance%20Fields%0AAuthor%3A%20Yuru%20Xiao%20and%20Deming%20Zhai%20and%20Wenbo%20Zhao%20and%20Kui%20Jiang%20and%20Junjun%20Jiang%20and%20Xianming%20Liu%0AAbstract%3A%20%20%20Radiance%20fields%20represented%20by%203D%20Gaussians%20excel%20at%20synthesizing%20novel%0Aviews%2C%20offering%20both%20high%20training%20efficiency%20and%20fast%20rendering.%20However%2C%20with%0Asparse%20input%20views%2C%20the%20lack%20of%20multi-view%20consistency%20constraints%20results%20in%0Apoorly%20initialized%20Gaussians%20and%20unreliable%20heuristics%20for%20optimization%2C%0Aleading%20to%20suboptimal%20performance.%20Existing%20methods%20often%20incorporate%20depth%0Apriors%20from%20dense%20estimation%20networks%20but%20overlook%20the%20inherent%20multi-view%0Aconsistency%20in%20input%20images.%20Additionally%2C%20they%20rely%20on%20dense%20initialization%2C%0Awhich%20limits%20the%20efficiency%20of%20scene%20representation.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20a%20view%20synthesis%20framework%20based%20on%203D%20Gaussian%0ASplatting%2C%20named%20MCGS%2C%20enabling%20photorealistic%20scene%20reconstruction%20from%20sparse%0Aviews.%20The%20key%20innovations%20of%20MCGS%20in%20enhancing%20multi-view%20consistency%20are%20as%0Afollows%3A%20i%29%20We%20leverage%20matching%20priors%20from%20a%20sparse%20matcher%20to%20initialize%0AGaussians%20primarily%20on%20textured%20regions%2C%20while%20low-texture%20areas%20are%20populated%0Awith%20randomly%20distributed%20Gaussians.%20This%20yields%20a%20compact%20yet%20sufficient%20set%0Aof%20initial%20Gaussians.%20ii%29%20We%20propose%20a%20multi-view%20consistency-guided%0Aprogressive%20pruning%20strategy%20to%20dynamically%20eliminate%20inconsistent%20Gaussians.%0AThis%20approach%20confines%20their%20optimization%20to%20a%20consistency-constrained%20space%2C%0Awhich%20ensures%20robust%20and%20coherent%20scene%20reconstruction.%20These%20strategies%0Aenhance%20robustness%20to%20sparse%20views%2C%20accelerate%20rendering%2C%20and%20reduce%20memory%0Aconsumption%2C%20making%20MCGS%20a%20practical%20framework%20for%203D%20Gaussian%20Splatting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCGS%253A%2520Multiview%2520Consistency%2520Enhancement%2520for%2520Sparse-View%25203D%2520Gaussian%250A%2520%2520Radiance%2520Fields%26entry.906535625%3DYuru%2520Xiao%2520and%2520Deming%2520Zhai%2520and%2520Wenbo%2520Zhao%2520and%2520Kui%2520Jiang%2520and%2520Junjun%2520Jiang%2520and%2520Xianming%2520Liu%26entry.1292438233%3D%2520%2520Radiance%2520fields%2520represented%2520by%25203D%2520Gaussians%2520excel%2520at%2520synthesizing%2520novel%250Aviews%252C%2520offering%2520both%2520high%2520training%2520efficiency%2520and%2520fast%2520rendering.%2520However%252C%2520with%250Asparse%2520input%2520views%252C%2520the%2520lack%2520of%2520multi-view%2520consistency%2520constraints%2520results%2520in%250Apoorly%2520initialized%2520Gaussians%2520and%2520unreliable%2520heuristics%2520for%2520optimization%252C%250Aleading%2520to%2520suboptimal%2520performance.%2520Existing%2520methods%2520often%2520incorporate%2520depth%250Apriors%2520from%2520dense%2520estimation%2520networks%2520but%2520overlook%2520the%2520inherent%2520multi-view%250Aconsistency%2520in%2520input%2520images.%2520Additionally%252C%2520they%2520rely%2520on%2520dense%2520initialization%252C%250Awhich%2520limits%2520the%2520efficiency%2520of%2520scene%2520representation.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520view%2520synthesis%2520framework%2520based%2520on%25203D%2520Gaussian%250ASplatting%252C%2520named%2520MCGS%252C%2520enabling%2520photorealistic%2520scene%2520reconstruction%2520from%2520sparse%250Aviews.%2520The%2520key%2520innovations%2520of%2520MCGS%2520in%2520enhancing%2520multi-view%2520consistency%2520are%2520as%250Afollows%253A%2520i%2529%2520We%2520leverage%2520matching%2520priors%2520from%2520a%2520sparse%2520matcher%2520to%2520initialize%250AGaussians%2520primarily%2520on%2520textured%2520regions%252C%2520while%2520low-texture%2520areas%2520are%2520populated%250Awith%2520randomly%2520distributed%2520Gaussians.%2520This%2520yields%2520a%2520compact%2520yet%2520sufficient%2520set%250Aof%2520initial%2520Gaussians.%2520ii%2529%2520We%2520propose%2520a%2520multi-view%2520consistency-guided%250Aprogressive%2520pruning%2520strategy%2520to%2520dynamically%2520eliminate%2520inconsistent%2520Gaussians.%250AThis%2520approach%2520confines%2520their%2520optimization%2520to%2520a%2520consistency-constrained%2520space%252C%250Awhich%2520ensures%2520robust%2520and%2520coherent%2520scene%2520reconstruction.%2520These%2520strategies%250Aenhance%2520robustness%2520to%2520sparse%2520views%252C%2520accelerate%2520rendering%252C%2520and%2520reduce%2520memory%250Aconsumption%252C%2520making%2520MCGS%2520a%2520practical%2520framework%2520for%25203D%2520Gaussian%2520Splatting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCGS%3A%20Multiview%20Consistency%20Enhancement%20for%20Sparse-View%203D%20Gaussian%0A%20%20Radiance%20Fields&entry.906535625=Yuru%20Xiao%20and%20Deming%20Zhai%20and%20Wenbo%20Zhao%20and%20Kui%20Jiang%20and%20Junjun%20Jiang%20and%20Xianming%20Liu&entry.1292438233=%20%20Radiance%20fields%20represented%20by%203D%20Gaussians%20excel%20at%20synthesizing%20novel%0Aviews%2C%20offering%20both%20high%20training%20efficiency%20and%20fast%20rendering.%20However%2C%20with%0Asparse%20input%20views%2C%20the%20lack%20of%20multi-view%20consistency%20constraints%20results%20in%0Apoorly%20initialized%20Gaussians%20and%20unreliable%20heuristics%20for%20optimization%2C%0Aleading%20to%20suboptimal%20performance.%20Existing%20methods%20often%20incorporate%20depth%0Apriors%20from%20dense%20estimation%20networks%20but%20overlook%20the%20inherent%20multi-view%0Aconsistency%20in%20input%20images.%20Additionally%2C%20they%20rely%20on%20dense%20initialization%2C%0Awhich%20limits%20the%20efficiency%20of%20scene%20representation.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20a%20view%20synthesis%20framework%20based%20on%203D%20Gaussian%0ASplatting%2C%20named%20MCGS%2C%20enabling%20photorealistic%20scene%20reconstruction%20from%20sparse%0Aviews.%20The%20key%20innovations%20of%20MCGS%20in%20enhancing%20multi-view%20consistency%20are%20as%0Afollows%3A%20i%29%20We%20leverage%20matching%20priors%20from%20a%20sparse%20matcher%20to%20initialize%0AGaussians%20primarily%20on%20textured%20regions%2C%20while%20low-texture%20areas%20are%20populated%0Awith%20randomly%20distributed%20Gaussians.%20This%20yields%20a%20compact%20yet%20sufficient%20set%0Aof%20initial%20Gaussians.%20ii%29%20We%20propose%20a%20multi-view%20consistency-guided%0Aprogressive%20pruning%20strategy%20to%20dynamically%20eliminate%20inconsistent%20Gaussians.%0AThis%20approach%20confines%20their%20optimization%20to%20a%20consistency-constrained%20space%2C%0Awhich%20ensures%20robust%20and%20coherent%20scene%20reconstruction.%20These%20strategies%0Aenhance%20robustness%20to%20sparse%20views%2C%20accelerate%20rendering%2C%20and%20reduce%20memory%0Aconsumption%2C%20making%20MCGS%20a%20practical%20framework%20for%203D%20Gaussian%20Splatting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11394v2&entry.124074799=Read"},
{"title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual\n  Localization", "author": "Maxime Pietrantoni and Gabriela Csurka and Torsten Sattler", "abstract": "  Visual localization is the task of estimating a camera pose in a known\nenvironment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based\nrepresentations for accurate and privacy-preserving visual localization. We\npropose Gaussian Splatting Feature Fields (GSFFs), a scene representation for\nvisual localization that combines an explicit geometry model (3DGS) with an\nimplicit feature field. We leverage the dense geometric information and\ndifferentiable rasterization algorithm from 3DGS to learn robust feature\nrepresentations grounded in 3D. In particular, we align a 3D scale-aware\nfeature field and a 2D feature encoder in a common embedding space through a\ncontrastive framework. Using a 3D structure-informed clustering procedure, we\nfurther regularize the representation learning and seamlessly convert the\nfeatures to segmentations, which can be used for privacy-preserving visual\nlocalization. Pose refinement, which involves aligning either feature maps or\nsegmentations from a query image with those rendered from the GSFFs scene\nrepresentation, is used to achieve localization. The resulting privacy- and\nnon-privacy-preserving localization pipelines, evaluated on multiple real-world\ndatasets, show state-of-the-art performances.\n", "link": "http://arxiv.org/abs/2507.23569v2", "date": "2025-08-26", "relevancy": 3.3127, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6938}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6761}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20Feature%20Fields%20for%20Privacy-Preserving%20Visual%0A%20%20Localization&body=Title%3A%20Gaussian%20Splatting%20Feature%20Fields%20for%20Privacy-Preserving%20Visual%0A%20%20Localization%0AAuthor%3A%20Maxime%20Pietrantoni%20and%20Gabriela%20Csurka%20and%20Torsten%20Sattler%0AAbstract%3A%20%20%20Visual%20localization%20is%20the%20task%20of%20estimating%20a%20camera%20pose%20in%20a%20known%0Aenvironment.%20In%20this%20paper%2C%20we%20utilize%203D%20Gaussian%20Splatting%20%283DGS%29-based%0Arepresentations%20for%20accurate%20and%20privacy-preserving%20visual%20localization.%20We%0Apropose%20Gaussian%20Splatting%20Feature%20Fields%20%28GSFFs%29%2C%20a%20scene%20representation%20for%0Avisual%20localization%20that%20combines%20an%20explicit%20geometry%20model%20%283DGS%29%20with%20an%0Aimplicit%20feature%20field.%20We%20leverage%20the%20dense%20geometric%20information%20and%0Adifferentiable%20rasterization%20algorithm%20from%203DGS%20to%20learn%20robust%20feature%0Arepresentations%20grounded%20in%203D.%20In%20particular%2C%20we%20align%20a%203D%20scale-aware%0Afeature%20field%20and%20a%202D%20feature%20encoder%20in%20a%20common%20embedding%20space%20through%20a%0Acontrastive%20framework.%20Using%20a%203D%20structure-informed%20clustering%20procedure%2C%20we%0Afurther%20regularize%20the%20representation%20learning%20and%20seamlessly%20convert%20the%0Afeatures%20to%20segmentations%2C%20which%20can%20be%20used%20for%20privacy-preserving%20visual%0Alocalization.%20Pose%20refinement%2C%20which%20involves%20aligning%20either%20feature%20maps%20or%0Asegmentations%20from%20a%20query%20image%20with%20those%20rendered%20from%20the%20GSFFs%20scene%0Arepresentation%2C%20is%20used%20to%20achieve%20localization.%20The%20resulting%20privacy-%20and%0Anon-privacy-preserving%20localization%20pipelines%2C%20evaluated%20on%20multiple%20real-world%0Adatasets%2C%20show%20state-of-the-art%20performances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520Feature%2520Fields%2520for%2520Privacy-Preserving%2520Visual%250A%2520%2520Localization%26entry.906535625%3DMaxime%2520Pietrantoni%2520and%2520Gabriela%2520Csurka%2520and%2520Torsten%2520Sattler%26entry.1292438233%3D%2520%2520Visual%2520localization%2520is%2520the%2520task%2520of%2520estimating%2520a%2520camera%2520pose%2520in%2520a%2520known%250Aenvironment.%2520In%2520this%2520paper%252C%2520we%2520utilize%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529-based%250Arepresentations%2520for%2520accurate%2520and%2520privacy-preserving%2520visual%2520localization.%2520We%250Apropose%2520Gaussian%2520Splatting%2520Feature%2520Fields%2520%2528GSFFs%2529%252C%2520a%2520scene%2520representation%2520for%250Avisual%2520localization%2520that%2520combines%2520an%2520explicit%2520geometry%2520model%2520%25283DGS%2529%2520with%2520an%250Aimplicit%2520feature%2520field.%2520We%2520leverage%2520the%2520dense%2520geometric%2520information%2520and%250Adifferentiable%2520rasterization%2520algorithm%2520from%25203DGS%2520to%2520learn%2520robust%2520feature%250Arepresentations%2520grounded%2520in%25203D.%2520In%2520particular%252C%2520we%2520align%2520a%25203D%2520scale-aware%250Afeature%2520field%2520and%2520a%25202D%2520feature%2520encoder%2520in%2520a%2520common%2520embedding%2520space%2520through%2520a%250Acontrastive%2520framework.%2520Using%2520a%25203D%2520structure-informed%2520clustering%2520procedure%252C%2520we%250Afurther%2520regularize%2520the%2520representation%2520learning%2520and%2520seamlessly%2520convert%2520the%250Afeatures%2520to%2520segmentations%252C%2520which%2520can%2520be%2520used%2520for%2520privacy-preserving%2520visual%250Alocalization.%2520Pose%2520refinement%252C%2520which%2520involves%2520aligning%2520either%2520feature%2520maps%2520or%250Asegmentations%2520from%2520a%2520query%2520image%2520with%2520those%2520rendered%2520from%2520the%2520GSFFs%2520scene%250Arepresentation%252C%2520is%2520used%2520to%2520achieve%2520localization.%2520The%2520resulting%2520privacy-%2520and%250Anon-privacy-preserving%2520localization%2520pipelines%252C%2520evaluated%2520on%2520multiple%2520real-world%250Adatasets%252C%2520show%2520state-of-the-art%2520performances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20Feature%20Fields%20for%20Privacy-Preserving%20Visual%0A%20%20Localization&entry.906535625=Maxime%20Pietrantoni%20and%20Gabriela%20Csurka%20and%20Torsten%20Sattler&entry.1292438233=%20%20Visual%20localization%20is%20the%20task%20of%20estimating%20a%20camera%20pose%20in%20a%20known%0Aenvironment.%20In%20this%20paper%2C%20we%20utilize%203D%20Gaussian%20Splatting%20%283DGS%29-based%0Arepresentations%20for%20accurate%20and%20privacy-preserving%20visual%20localization.%20We%0Apropose%20Gaussian%20Splatting%20Feature%20Fields%20%28GSFFs%29%2C%20a%20scene%20representation%20for%0Avisual%20localization%20that%20combines%20an%20explicit%20geometry%20model%20%283DGS%29%20with%20an%0Aimplicit%20feature%20field.%20We%20leverage%20the%20dense%20geometric%20information%20and%0Adifferentiable%20rasterization%20algorithm%20from%203DGS%20to%20learn%20robust%20feature%0Arepresentations%20grounded%20in%203D.%20In%20particular%2C%20we%20align%20a%203D%20scale-aware%0Afeature%20field%20and%20a%202D%20feature%20encoder%20in%20a%20common%20embedding%20space%20through%20a%0Acontrastive%20framework.%20Using%20a%203D%20structure-informed%20clustering%20procedure%2C%20we%0Afurther%20regularize%20the%20representation%20learning%20and%20seamlessly%20convert%20the%0Afeatures%20to%20segmentations%2C%20which%20can%20be%20used%20for%20privacy-preserving%20visual%0Alocalization.%20Pose%20refinement%2C%20which%20involves%20aligning%20either%20feature%20maps%20or%0Asegmentations%20from%20a%20query%20image%20with%20those%20rendered%20from%20the%20GSFFs%20scene%0Arepresentation%2C%20is%20used%20to%20achieve%20localization.%20The%20resulting%20privacy-%20and%0Anon-privacy-preserving%20localization%20pipelines%2C%20evaluated%20on%20multiple%20real-world%0Adatasets%2C%20show%20state-of-the-art%20performances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23569v2&entry.124074799=Read"},
{"title": "Dual Enhancement on 3D Vision-Language Perception for Monocular 3D\n  Visual Grounding", "author": "Yuzhen Li and Min Liu and Yuan Bian and Xueping Wang and Zhaoyang Li and Gen Li and Yaonan Wang", "abstract": "  Monocular 3D visual grounding is a novel task that aims to locate 3D objects\nin RGB images using text descriptions with explicit geometry information.\nDespite the inclusion of geometry details in the text, we observe that the text\nembeddings are sensitive to the magnitude of numerical values but largely\nignore the associated measurement units. For example, simply equidistant\nmapping the length with unit \"meter\" to \"decimeters\" or \"centimeters\" leads to\nsevere performance degradation, even though the physical length remains\nequivalent. This observation signifies the weak 3D comprehension of pre-trained\nlanguage model, which generates misguiding text features to hinder 3D\nperception. Therefore, we propose to enhance the 3D perception of model on text\nembeddings and geometry features with two simple and effective methods.\nFirstly, we introduce a pre-processing method named 3D-text Enhancement (3DTE),\nwhich enhances the comprehension of mapping relationships between different\nunits by augmenting the diversity of distance descriptors in text queries.\nNext, we propose a Text-Guided Geometry Enhancement (TGE) module to further\nenhance the 3D-text information by projecting the basic text features into\ngeometrically consistent space. These 3D-enhanced text features are then\nleveraged to precisely guide the attention of geometry features. We evaluate\nthe proposed method through extensive comparisons and ablation studies on the\nMono3DRefer dataset. Experimental results demonstrate substantial improvements\nover previous methods, achieving new state-of-the-art results with a notable\naccuracy gain of 11.94\\% in the \"Far\" scenario. Our code will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2508.19165v1", "date": "2025-08-26", "relevancy": 3.2189, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6499}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Enhancement%20on%203D%20Vision-Language%20Perception%20for%20Monocular%203D%0A%20%20Visual%20Grounding&body=Title%3A%20Dual%20Enhancement%20on%203D%20Vision-Language%20Perception%20for%20Monocular%203D%0A%20%20Visual%20Grounding%0AAuthor%3A%20Yuzhen%20Li%20and%20Min%20Liu%20and%20Yuan%20Bian%20and%20Xueping%20Wang%20and%20Zhaoyang%20Li%20and%20Gen%20Li%20and%20Yaonan%20Wang%0AAbstract%3A%20%20%20Monocular%203D%20visual%20grounding%20is%20a%20novel%20task%20that%20aims%20to%20locate%203D%20objects%0Ain%20RGB%20images%20using%20text%20descriptions%20with%20explicit%20geometry%20information.%0ADespite%20the%20inclusion%20of%20geometry%20details%20in%20the%20text%2C%20we%20observe%20that%20the%20text%0Aembeddings%20are%20sensitive%20to%20the%20magnitude%20of%20numerical%20values%20but%20largely%0Aignore%20the%20associated%20measurement%20units.%20For%20example%2C%20simply%20equidistant%0Amapping%20the%20length%20with%20unit%20%22meter%22%20to%20%22decimeters%22%20or%20%22centimeters%22%20leads%20to%0Asevere%20performance%20degradation%2C%20even%20though%20the%20physical%20length%20remains%0Aequivalent.%20This%20observation%20signifies%20the%20weak%203D%20comprehension%20of%20pre-trained%0Alanguage%20model%2C%20which%20generates%20misguiding%20text%20features%20to%20hinder%203D%0Aperception.%20Therefore%2C%20we%20propose%20to%20enhance%20the%203D%20perception%20of%20model%20on%20text%0Aembeddings%20and%20geometry%20features%20with%20two%20simple%20and%20effective%20methods.%0AFirstly%2C%20we%20introduce%20a%20pre-processing%20method%20named%203D-text%20Enhancement%20%283DTE%29%2C%0Awhich%20enhances%20the%20comprehension%20of%20mapping%20relationships%20between%20different%0Aunits%20by%20augmenting%20the%20diversity%20of%20distance%20descriptors%20in%20text%20queries.%0ANext%2C%20we%20propose%20a%20Text-Guided%20Geometry%20Enhancement%20%28TGE%29%20module%20to%20further%0Aenhance%20the%203D-text%20information%20by%20projecting%20the%20basic%20text%20features%20into%0Ageometrically%20consistent%20space.%20These%203D-enhanced%20text%20features%20are%20then%0Aleveraged%20to%20precisely%20guide%20the%20attention%20of%20geometry%20features.%20We%20evaluate%0Athe%20proposed%20method%20through%20extensive%20comparisons%20and%20ablation%20studies%20on%20the%0AMono3DRefer%20dataset.%20Experimental%20results%20demonstrate%20substantial%20improvements%0Aover%20previous%20methods%2C%20achieving%20new%20state-of-the-art%20results%20with%20a%20notable%0Aaccuracy%20gain%20of%2011.94%5C%25%20in%20the%20%22Far%22%20scenario.%20Our%20code%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Enhancement%2520on%25203D%2520Vision-Language%2520Perception%2520for%2520Monocular%25203D%250A%2520%2520Visual%2520Grounding%26entry.906535625%3DYuzhen%2520Li%2520and%2520Min%2520Liu%2520and%2520Yuan%2520Bian%2520and%2520Xueping%2520Wang%2520and%2520Zhaoyang%2520Li%2520and%2520Gen%2520Li%2520and%2520Yaonan%2520Wang%26entry.1292438233%3D%2520%2520Monocular%25203D%2520visual%2520grounding%2520is%2520a%2520novel%2520task%2520that%2520aims%2520to%2520locate%25203D%2520objects%250Ain%2520RGB%2520images%2520using%2520text%2520descriptions%2520with%2520explicit%2520geometry%2520information.%250ADespite%2520the%2520inclusion%2520of%2520geometry%2520details%2520in%2520the%2520text%252C%2520we%2520observe%2520that%2520the%2520text%250Aembeddings%2520are%2520sensitive%2520to%2520the%2520magnitude%2520of%2520numerical%2520values%2520but%2520largely%250Aignore%2520the%2520associated%2520measurement%2520units.%2520For%2520example%252C%2520simply%2520equidistant%250Amapping%2520the%2520length%2520with%2520unit%2520%2522meter%2522%2520to%2520%2522decimeters%2522%2520or%2520%2522centimeters%2522%2520leads%2520to%250Asevere%2520performance%2520degradation%252C%2520even%2520though%2520the%2520physical%2520length%2520remains%250Aequivalent.%2520This%2520observation%2520signifies%2520the%2520weak%25203D%2520comprehension%2520of%2520pre-trained%250Alanguage%2520model%252C%2520which%2520generates%2520misguiding%2520text%2520features%2520to%2520hinder%25203D%250Aperception.%2520Therefore%252C%2520we%2520propose%2520to%2520enhance%2520the%25203D%2520perception%2520of%2520model%2520on%2520text%250Aembeddings%2520and%2520geometry%2520features%2520with%2520two%2520simple%2520and%2520effective%2520methods.%250AFirstly%252C%2520we%2520introduce%2520a%2520pre-processing%2520method%2520named%25203D-text%2520Enhancement%2520%25283DTE%2529%252C%250Awhich%2520enhances%2520the%2520comprehension%2520of%2520mapping%2520relationships%2520between%2520different%250Aunits%2520by%2520augmenting%2520the%2520diversity%2520of%2520distance%2520descriptors%2520in%2520text%2520queries.%250ANext%252C%2520we%2520propose%2520a%2520Text-Guided%2520Geometry%2520Enhancement%2520%2528TGE%2529%2520module%2520to%2520further%250Aenhance%2520the%25203D-text%2520information%2520by%2520projecting%2520the%2520basic%2520text%2520features%2520into%250Ageometrically%2520consistent%2520space.%2520These%25203D-enhanced%2520text%2520features%2520are%2520then%250Aleveraged%2520to%2520precisely%2520guide%2520the%2520attention%2520of%2520geometry%2520features.%2520We%2520evaluate%250Athe%2520proposed%2520method%2520through%2520extensive%2520comparisons%2520and%2520ablation%2520studies%2520on%2520the%250AMono3DRefer%2520dataset.%2520Experimental%2520results%2520demonstrate%2520substantial%2520improvements%250Aover%2520previous%2520methods%252C%2520achieving%2520new%2520state-of-the-art%2520results%2520with%2520a%2520notable%250Aaccuracy%2520gain%2520of%252011.94%255C%2525%2520in%2520the%2520%2522Far%2522%2520scenario.%2520Our%2520code%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Enhancement%20on%203D%20Vision-Language%20Perception%20for%20Monocular%203D%0A%20%20Visual%20Grounding&entry.906535625=Yuzhen%20Li%20and%20Min%20Liu%20and%20Yuan%20Bian%20and%20Xueping%20Wang%20and%20Zhaoyang%20Li%20and%20Gen%20Li%20and%20Yaonan%20Wang&entry.1292438233=%20%20Monocular%203D%20visual%20grounding%20is%20a%20novel%20task%20that%20aims%20to%20locate%203D%20objects%0Ain%20RGB%20images%20using%20text%20descriptions%20with%20explicit%20geometry%20information.%0ADespite%20the%20inclusion%20of%20geometry%20details%20in%20the%20text%2C%20we%20observe%20that%20the%20text%0Aembeddings%20are%20sensitive%20to%20the%20magnitude%20of%20numerical%20values%20but%20largely%0Aignore%20the%20associated%20measurement%20units.%20For%20example%2C%20simply%20equidistant%0Amapping%20the%20length%20with%20unit%20%22meter%22%20to%20%22decimeters%22%20or%20%22centimeters%22%20leads%20to%0Asevere%20performance%20degradation%2C%20even%20though%20the%20physical%20length%20remains%0Aequivalent.%20This%20observation%20signifies%20the%20weak%203D%20comprehension%20of%20pre-trained%0Alanguage%20model%2C%20which%20generates%20misguiding%20text%20features%20to%20hinder%203D%0Aperception.%20Therefore%2C%20we%20propose%20to%20enhance%20the%203D%20perception%20of%20model%20on%20text%0Aembeddings%20and%20geometry%20features%20with%20two%20simple%20and%20effective%20methods.%0AFirstly%2C%20we%20introduce%20a%20pre-processing%20method%20named%203D-text%20Enhancement%20%283DTE%29%2C%0Awhich%20enhances%20the%20comprehension%20of%20mapping%20relationships%20between%20different%0Aunits%20by%20augmenting%20the%20diversity%20of%20distance%20descriptors%20in%20text%20queries.%0ANext%2C%20we%20propose%20a%20Text-Guided%20Geometry%20Enhancement%20%28TGE%29%20module%20to%20further%0Aenhance%20the%203D-text%20information%20by%20projecting%20the%20basic%20text%20features%20into%0Ageometrically%20consistent%20space.%20These%203D-enhanced%20text%20features%20are%20then%0Aleveraged%20to%20precisely%20guide%20the%20attention%20of%20geometry%20features.%20We%20evaluate%0Athe%20proposed%20method%20through%20extensive%20comparisons%20and%20ablation%20studies%20on%20the%0AMono3DRefer%20dataset.%20Experimental%20results%20demonstrate%20substantial%20improvements%0Aover%20previous%20methods%2C%20achieving%20new%20state-of-the-art%20results%20with%20a%20notable%0Aaccuracy%20gain%20of%2011.94%5C%25%20in%20the%20%22Far%22%20scenario.%20Our%20code%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19165v1&entry.124074799=Read"},
{"title": "Articulate3D: Zero-Shot Text-Driven 3D Object Posing", "author": "Oishi Deb and Anjun Hu and Ashkan Khakzar and Philip Torr and Christian Rupprecht", "abstract": "  We propose a training-free method, Articulate3D, to pose a 3D asset through\nlanguage control. Despite advances in vision and language models, this task\nremains surprisingly challenging. To achieve this goal, we decompose the\nproblem into two steps. We modify a powerful image-generator to create target\nimages conditioned on the input image and a text instruction. We then align the\nmesh to the target images through a multi-view pose optimisation step. In\ndetail, we introduce a self-attention rewiring mechanism (RSActrl) that\ndecouples the source structure from pose within an image generative model,\nallowing it to maintain a consistent structure across varying poses. We\nobserved that differentiable rendering is an unreliable signal for articulation\noptimisation; instead, we use keypoints to establish correspondences between\ninput and target images. The effectiveness of Articulate3D is demonstrated\nacross a diverse range of 3D objects and free-form text prompts, successfully\nmanipulating poses while maintaining the original identity of the mesh.\nQuantitative evaluations and a comparative user study, in which our method was\npreferred over 85\\% of the time, confirm its superiority over existing\napproaches. Project page:https://odeb1.github.io/articulate3d_page_deb/\n", "link": "http://arxiv.org/abs/2508.19244v1", "date": "2025-08-26", "relevancy": 3.1914, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6716}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6527}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Articulate3D%3A%20Zero-Shot%20Text-Driven%203D%20Object%20Posing&body=Title%3A%20Articulate3D%3A%20Zero-Shot%20Text-Driven%203D%20Object%20Posing%0AAuthor%3A%20Oishi%20Deb%20and%20Anjun%20Hu%20and%20Ashkan%20Khakzar%20and%20Philip%20Torr%20and%20Christian%20Rupprecht%0AAbstract%3A%20%20%20We%20propose%20a%20training-free%20method%2C%20Articulate3D%2C%20to%20pose%20a%203D%20asset%20through%0Alanguage%20control.%20Despite%20advances%20in%20vision%20and%20language%20models%2C%20this%20task%0Aremains%20surprisingly%20challenging.%20To%20achieve%20this%20goal%2C%20we%20decompose%20the%0Aproblem%20into%20two%20steps.%20We%20modify%20a%20powerful%20image-generator%20to%20create%20target%0Aimages%20conditioned%20on%20the%20input%20image%20and%20a%20text%20instruction.%20We%20then%20align%20the%0Amesh%20to%20the%20target%20images%20through%20a%20multi-view%20pose%20optimisation%20step.%20In%0Adetail%2C%20we%20introduce%20a%20self-attention%20rewiring%20mechanism%20%28RSActrl%29%20that%0Adecouples%20the%20source%20structure%20from%20pose%20within%20an%20image%20generative%20model%2C%0Aallowing%20it%20to%20maintain%20a%20consistent%20structure%20across%20varying%20poses.%20We%0Aobserved%20that%20differentiable%20rendering%20is%20an%20unreliable%20signal%20for%20articulation%0Aoptimisation%3B%20instead%2C%20we%20use%20keypoints%20to%20establish%20correspondences%20between%0Ainput%20and%20target%20images.%20The%20effectiveness%20of%20Articulate3D%20is%20demonstrated%0Aacross%20a%20diverse%20range%20of%203D%20objects%20and%20free-form%20text%20prompts%2C%20successfully%0Amanipulating%20poses%20while%20maintaining%20the%20original%20identity%20of%20the%20mesh.%0AQuantitative%20evaluations%20and%20a%20comparative%20user%20study%2C%20in%20which%20our%20method%20was%0Apreferred%20over%2085%5C%25%20of%20the%20time%2C%20confirm%20its%20superiority%20over%20existing%0Aapproaches.%20Project%20page%3Ahttps%3A//odeb1.github.io/articulate3d_page_deb/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArticulate3D%253A%2520Zero-Shot%2520Text-Driven%25203D%2520Object%2520Posing%26entry.906535625%3DOishi%2520Deb%2520and%2520Anjun%2520Hu%2520and%2520Ashkan%2520Khakzar%2520and%2520Philip%2520Torr%2520and%2520Christian%2520Rupprecht%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520training-free%2520method%252C%2520Articulate3D%252C%2520to%2520pose%2520a%25203D%2520asset%2520through%250Alanguage%2520control.%2520Despite%2520advances%2520in%2520vision%2520and%2520language%2520models%252C%2520this%2520task%250Aremains%2520surprisingly%2520challenging.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520decompose%2520the%250Aproblem%2520into%2520two%2520steps.%2520We%2520modify%2520a%2520powerful%2520image-generator%2520to%2520create%2520target%250Aimages%2520conditioned%2520on%2520the%2520input%2520image%2520and%2520a%2520text%2520instruction.%2520We%2520then%2520align%2520the%250Amesh%2520to%2520the%2520target%2520images%2520through%2520a%2520multi-view%2520pose%2520optimisation%2520step.%2520In%250Adetail%252C%2520we%2520introduce%2520a%2520self-attention%2520rewiring%2520mechanism%2520%2528RSActrl%2529%2520that%250Adecouples%2520the%2520source%2520structure%2520from%2520pose%2520within%2520an%2520image%2520generative%2520model%252C%250Aallowing%2520it%2520to%2520maintain%2520a%2520consistent%2520structure%2520across%2520varying%2520poses.%2520We%250Aobserved%2520that%2520differentiable%2520rendering%2520is%2520an%2520unreliable%2520signal%2520for%2520articulation%250Aoptimisation%253B%2520instead%252C%2520we%2520use%2520keypoints%2520to%2520establish%2520correspondences%2520between%250Ainput%2520and%2520target%2520images.%2520The%2520effectiveness%2520of%2520Articulate3D%2520is%2520demonstrated%250Aacross%2520a%2520diverse%2520range%2520of%25203D%2520objects%2520and%2520free-form%2520text%2520prompts%252C%2520successfully%250Amanipulating%2520poses%2520while%2520maintaining%2520the%2520original%2520identity%2520of%2520the%2520mesh.%250AQuantitative%2520evaluations%2520and%2520a%2520comparative%2520user%2520study%252C%2520in%2520which%2520our%2520method%2520was%250Apreferred%2520over%252085%255C%2525%2520of%2520the%2520time%252C%2520confirm%2520its%2520superiority%2520over%2520existing%250Aapproaches.%2520Project%2520page%253Ahttps%253A//odeb1.github.io/articulate3d_page_deb/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Articulate3D%3A%20Zero-Shot%20Text-Driven%203D%20Object%20Posing&entry.906535625=Oishi%20Deb%20and%20Anjun%20Hu%20and%20Ashkan%20Khakzar%20and%20Philip%20Torr%20and%20Christian%20Rupprecht&entry.1292438233=%20%20We%20propose%20a%20training-free%20method%2C%20Articulate3D%2C%20to%20pose%20a%203D%20asset%20through%0Alanguage%20control.%20Despite%20advances%20in%20vision%20and%20language%20models%2C%20this%20task%0Aremains%20surprisingly%20challenging.%20To%20achieve%20this%20goal%2C%20we%20decompose%20the%0Aproblem%20into%20two%20steps.%20We%20modify%20a%20powerful%20image-generator%20to%20create%20target%0Aimages%20conditioned%20on%20the%20input%20image%20and%20a%20text%20instruction.%20We%20then%20align%20the%0Amesh%20to%20the%20target%20images%20through%20a%20multi-view%20pose%20optimisation%20step.%20In%0Adetail%2C%20we%20introduce%20a%20self-attention%20rewiring%20mechanism%20%28RSActrl%29%20that%0Adecouples%20the%20source%20structure%20from%20pose%20within%20an%20image%20generative%20model%2C%0Aallowing%20it%20to%20maintain%20a%20consistent%20structure%20across%20varying%20poses.%20We%0Aobserved%20that%20differentiable%20rendering%20is%20an%20unreliable%20signal%20for%20articulation%0Aoptimisation%3B%20instead%2C%20we%20use%20keypoints%20to%20establish%20correspondences%20between%0Ainput%20and%20target%20images.%20The%20effectiveness%20of%20Articulate3D%20is%20demonstrated%0Aacross%20a%20diverse%20range%20of%203D%20objects%20and%20free-form%20text%20prompts%2C%20successfully%0Amanipulating%20poses%20while%20maintaining%20the%20original%20identity%20of%20the%20mesh.%0AQuantitative%20evaluations%20and%20a%20comparative%20user%20study%2C%20in%20which%20our%20method%20was%0Apreferred%20over%2085%5C%25%20of%20the%20time%2C%20confirm%20its%20superiority%20over%20existing%0Aapproaches.%20Project%20page%3Ahttps%3A//odeb1.github.io/articulate3d_page_deb/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19244v1&entry.124074799=Read"},
{"title": "PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads", "author": "Shashikant Verma and Shanmuganathan Raman", "abstract": "  Achieving realistic hair strand synthesis is essential for creating lifelike\ndigital humans, but producing high-fidelity hair strand geometry remains a\nsignificant challenge. Existing methods require a complex setup for data\nacquisition, involving multi-view images captured in constrained studio\nenvironments. Additionally, these methods have longer hair volume estimation\nand strand synthesis times, which hinder efficiency. We introduce PanoHair, a\nmodel that estimates head geometry as signed distance fields using knowledge\ndistillation from a pre-trained generative teacher model for head synthesis.\nOur approach enables the prediction of semantic segmentation masks and 3D\norientations specifically for the hair region of the estimated geometry. Our\nmethod is generative and can generate diverse hairstyles with latent space\nmanipulations. For real images, our approach involves an inversion process to\ninfer latent codes and produces visually appealing hair strands, offering a\nstreamlined alternative to complex multi-view data acquisition setups. Given\nthe latent code, PanoHair generates a clean manifold mesh for the hair region\nin under 5 seconds, along with semantic and orientation maps, marking a\nsignificant improvement over existing methods, as demonstrated in our\nexperiments.\n", "link": "http://arxiv.org/abs/2508.18944v1", "date": "2025-08-26", "relevancy": 3.0288, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6315}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5929}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanoHair%3A%20Detailed%20Hair%20Strand%20Synthesis%20on%20Volumetric%20Heads&body=Title%3A%20PanoHair%3A%20Detailed%20Hair%20Strand%20Synthesis%20on%20Volumetric%20Heads%0AAuthor%3A%20Shashikant%20Verma%20and%20Shanmuganathan%20Raman%0AAbstract%3A%20%20%20Achieving%20realistic%20hair%20strand%20synthesis%20is%20essential%20for%20creating%20lifelike%0Adigital%20humans%2C%20but%20producing%20high-fidelity%20hair%20strand%20geometry%20remains%20a%0Asignificant%20challenge.%20Existing%20methods%20require%20a%20complex%20setup%20for%20data%0Aacquisition%2C%20involving%20multi-view%20images%20captured%20in%20constrained%20studio%0Aenvironments.%20Additionally%2C%20these%20methods%20have%20longer%20hair%20volume%20estimation%0Aand%20strand%20synthesis%20times%2C%20which%20hinder%20efficiency.%20We%20introduce%20PanoHair%2C%20a%0Amodel%20that%20estimates%20head%20geometry%20as%20signed%20distance%20fields%20using%20knowledge%0Adistillation%20from%20a%20pre-trained%20generative%20teacher%20model%20for%20head%20synthesis.%0AOur%20approach%20enables%20the%20prediction%20of%20semantic%20segmentation%20masks%20and%203D%0Aorientations%20specifically%20for%20the%20hair%20region%20of%20the%20estimated%20geometry.%20Our%0Amethod%20is%20generative%20and%20can%20generate%20diverse%20hairstyles%20with%20latent%20space%0Amanipulations.%20For%20real%20images%2C%20our%20approach%20involves%20an%20inversion%20process%20to%0Ainfer%20latent%20codes%20and%20produces%20visually%20appealing%20hair%20strands%2C%20offering%20a%0Astreamlined%20alternative%20to%20complex%20multi-view%20data%20acquisition%20setups.%20Given%0Athe%20latent%20code%2C%20PanoHair%20generates%20a%20clean%20manifold%20mesh%20for%20the%20hair%20region%0Ain%20under%205%20seconds%2C%20along%20with%20semantic%20and%20orientation%20maps%2C%20marking%20a%0Asignificant%20improvement%20over%20existing%20methods%2C%20as%20demonstrated%20in%20our%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoHair%253A%2520Detailed%2520Hair%2520Strand%2520Synthesis%2520on%2520Volumetric%2520Heads%26entry.906535625%3DShashikant%2520Verma%2520and%2520Shanmuganathan%2520Raman%26entry.1292438233%3D%2520%2520Achieving%2520realistic%2520hair%2520strand%2520synthesis%2520is%2520essential%2520for%2520creating%2520lifelike%250Adigital%2520humans%252C%2520but%2520producing%2520high-fidelity%2520hair%2520strand%2520geometry%2520remains%2520a%250Asignificant%2520challenge.%2520Existing%2520methods%2520require%2520a%2520complex%2520setup%2520for%2520data%250Aacquisition%252C%2520involving%2520multi-view%2520images%2520captured%2520in%2520constrained%2520studio%250Aenvironments.%2520Additionally%252C%2520these%2520methods%2520have%2520longer%2520hair%2520volume%2520estimation%250Aand%2520strand%2520synthesis%2520times%252C%2520which%2520hinder%2520efficiency.%2520We%2520introduce%2520PanoHair%252C%2520a%250Amodel%2520that%2520estimates%2520head%2520geometry%2520as%2520signed%2520distance%2520fields%2520using%2520knowledge%250Adistillation%2520from%2520a%2520pre-trained%2520generative%2520teacher%2520model%2520for%2520head%2520synthesis.%250AOur%2520approach%2520enables%2520the%2520prediction%2520of%2520semantic%2520segmentation%2520masks%2520and%25203D%250Aorientations%2520specifically%2520for%2520the%2520hair%2520region%2520of%2520the%2520estimated%2520geometry.%2520Our%250Amethod%2520is%2520generative%2520and%2520can%2520generate%2520diverse%2520hairstyles%2520with%2520latent%2520space%250Amanipulations.%2520For%2520real%2520images%252C%2520our%2520approach%2520involves%2520an%2520inversion%2520process%2520to%250Ainfer%2520latent%2520codes%2520and%2520produces%2520visually%2520appealing%2520hair%2520strands%252C%2520offering%2520a%250Astreamlined%2520alternative%2520to%2520complex%2520multi-view%2520data%2520acquisition%2520setups.%2520Given%250Athe%2520latent%2520code%252C%2520PanoHair%2520generates%2520a%2520clean%2520manifold%2520mesh%2520for%2520the%2520hair%2520region%250Ain%2520under%25205%2520seconds%252C%2520along%2520with%2520semantic%2520and%2520orientation%2520maps%252C%2520marking%2520a%250Asignificant%2520improvement%2520over%2520existing%2520methods%252C%2520as%2520demonstrated%2520in%2520our%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanoHair%3A%20Detailed%20Hair%20Strand%20Synthesis%20on%20Volumetric%20Heads&entry.906535625=Shashikant%20Verma%20and%20Shanmuganathan%20Raman&entry.1292438233=%20%20Achieving%20realistic%20hair%20strand%20synthesis%20is%20essential%20for%20creating%20lifelike%0Adigital%20humans%2C%20but%20producing%20high-fidelity%20hair%20strand%20geometry%20remains%20a%0Asignificant%20challenge.%20Existing%20methods%20require%20a%20complex%20setup%20for%20data%0Aacquisition%2C%20involving%20multi-view%20images%20captured%20in%20constrained%20studio%0Aenvironments.%20Additionally%2C%20these%20methods%20have%20longer%20hair%20volume%20estimation%0Aand%20strand%20synthesis%20times%2C%20which%20hinder%20efficiency.%20We%20introduce%20PanoHair%2C%20a%0Amodel%20that%20estimates%20head%20geometry%20as%20signed%20distance%20fields%20using%20knowledge%0Adistillation%20from%20a%20pre-trained%20generative%20teacher%20model%20for%20head%20synthesis.%0AOur%20approach%20enables%20the%20prediction%20of%20semantic%20segmentation%20masks%20and%203D%0Aorientations%20specifically%20for%20the%20hair%20region%20of%20the%20estimated%20geometry.%20Our%0Amethod%20is%20generative%20and%20can%20generate%20diverse%20hairstyles%20with%20latent%20space%0Amanipulations.%20For%20real%20images%2C%20our%20approach%20involves%20an%20inversion%20process%20to%0Ainfer%20latent%20codes%20and%20produces%20visually%20appealing%20hair%20strands%2C%20offering%20a%0Astreamlined%20alternative%20to%20complex%20multi-view%20data%20acquisition%20setups.%20Given%0Athe%20latent%20code%2C%20PanoHair%20generates%20a%20clean%20manifold%20mesh%20for%20the%20hair%20region%0Ain%20under%205%20seconds%2C%20along%20with%20semantic%20and%20orientation%20maps%2C%20marking%20a%0Asignificant%20improvement%20over%20existing%20methods%2C%20as%20demonstrated%20in%20our%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18944v1&entry.124074799=Read"},
{"title": "Video CLIP Model for Multi-View Echocardiography Interpretation", "author": "Ryo Takizawa and Satoshi Kodera and Tempei Kabayama and Ryo Matsuoka and Yuta Ando and Yuto Nakamura and Haruki Settai and Norihiko Takeda", "abstract": "  Echocardiography records ultrasound videos of the heart, enabling clinicians\nto assess cardiac function. Recent advances in large-scale vision-language\nmodels (VLMs) have spurred interest in automating echocardiographic\ninterpretation. However, most existing medical VLMs rely on single-frame\n(image) inputs, which can reduce diagnostic accuracy for conditions\nidentifiable only through cardiac motion. In addition, echocardiographic videos\nare captured from multiple views, each varying in suitability for detecting\nspecific conditions. Leveraging multiple views may therefore improve diagnostic\nperformance. We developed a video-language model that processes full video\nsequences from five standard views, trained on 60,747 echocardiographic\nvideo-report pairs. We evaluated the gains in retrieval performance from video\ninput and multi-view support, including the contributions of various pretrained\nmodels.\n", "link": "http://arxiv.org/abs/2504.18800v2", "date": "2025-08-26", "relevancy": 2.8847, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5826}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20CLIP%20Model%20for%20Multi-View%20Echocardiography%20Interpretation&body=Title%3A%20Video%20CLIP%20Model%20for%20Multi-View%20Echocardiography%20Interpretation%0AAuthor%3A%20Ryo%20Takizawa%20and%20Satoshi%20Kodera%20and%20Tempei%20Kabayama%20and%20Ryo%20Matsuoka%20and%20Yuta%20Ando%20and%20Yuto%20Nakamura%20and%20Haruki%20Settai%20and%20Norihiko%20Takeda%0AAbstract%3A%20%20%20Echocardiography%20records%20ultrasound%20videos%20of%20the%20heart%2C%20enabling%20clinicians%0Ato%20assess%20cardiac%20function.%20Recent%20advances%20in%20large-scale%20vision-language%0Amodels%20%28VLMs%29%20have%20spurred%20interest%20in%20automating%20echocardiographic%0Ainterpretation.%20However%2C%20most%20existing%20medical%20VLMs%20rely%20on%20single-frame%0A%28image%29%20inputs%2C%20which%20can%20reduce%20diagnostic%20accuracy%20for%20conditions%0Aidentifiable%20only%20through%20cardiac%20motion.%20In%20addition%2C%20echocardiographic%20videos%0Aare%20captured%20from%20multiple%20views%2C%20each%20varying%20in%20suitability%20for%20detecting%0Aspecific%20conditions.%20Leveraging%20multiple%20views%20may%20therefore%20improve%20diagnostic%0Aperformance.%20We%20developed%20a%20video-language%20model%20that%20processes%20full%20video%0Asequences%20from%20five%20standard%20views%2C%20trained%20on%2060%2C747%20echocardiographic%0Avideo-report%20pairs.%20We%20evaluated%20the%20gains%20in%20retrieval%20performance%20from%20video%0Ainput%20and%20multi-view%20support%2C%20including%20the%20contributions%20of%20various%20pretrained%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520CLIP%2520Model%2520for%2520Multi-View%2520Echocardiography%2520Interpretation%26entry.906535625%3DRyo%2520Takizawa%2520and%2520Satoshi%2520Kodera%2520and%2520Tempei%2520Kabayama%2520and%2520Ryo%2520Matsuoka%2520and%2520Yuta%2520Ando%2520and%2520Yuto%2520Nakamura%2520and%2520Haruki%2520Settai%2520and%2520Norihiko%2520Takeda%26entry.1292438233%3D%2520%2520Echocardiography%2520records%2520ultrasound%2520videos%2520of%2520the%2520heart%252C%2520enabling%2520clinicians%250Ato%2520assess%2520cardiac%2520function.%2520Recent%2520advances%2520in%2520large-scale%2520vision-language%250Amodels%2520%2528VLMs%2529%2520have%2520spurred%2520interest%2520in%2520automating%2520echocardiographic%250Ainterpretation.%2520However%252C%2520most%2520existing%2520medical%2520VLMs%2520rely%2520on%2520single-frame%250A%2528image%2529%2520inputs%252C%2520which%2520can%2520reduce%2520diagnostic%2520accuracy%2520for%2520conditions%250Aidentifiable%2520only%2520through%2520cardiac%2520motion.%2520In%2520addition%252C%2520echocardiographic%2520videos%250Aare%2520captured%2520from%2520multiple%2520views%252C%2520each%2520varying%2520in%2520suitability%2520for%2520detecting%250Aspecific%2520conditions.%2520Leveraging%2520multiple%2520views%2520may%2520therefore%2520improve%2520diagnostic%250Aperformance.%2520We%2520developed%2520a%2520video-language%2520model%2520that%2520processes%2520full%2520video%250Asequences%2520from%2520five%2520standard%2520views%252C%2520trained%2520on%252060%252C747%2520echocardiographic%250Avideo-report%2520pairs.%2520We%2520evaluated%2520the%2520gains%2520in%2520retrieval%2520performance%2520from%2520video%250Ainput%2520and%2520multi-view%2520support%252C%2520including%2520the%2520contributions%2520of%2520various%2520pretrained%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20CLIP%20Model%20for%20Multi-View%20Echocardiography%20Interpretation&entry.906535625=Ryo%20Takizawa%20and%20Satoshi%20Kodera%20and%20Tempei%20Kabayama%20and%20Ryo%20Matsuoka%20and%20Yuta%20Ando%20and%20Yuto%20Nakamura%20and%20Haruki%20Settai%20and%20Norihiko%20Takeda&entry.1292438233=%20%20Echocardiography%20records%20ultrasound%20videos%20of%20the%20heart%2C%20enabling%20clinicians%0Ato%20assess%20cardiac%20function.%20Recent%20advances%20in%20large-scale%20vision-language%0Amodels%20%28VLMs%29%20have%20spurred%20interest%20in%20automating%20echocardiographic%0Ainterpretation.%20However%2C%20most%20existing%20medical%20VLMs%20rely%20on%20single-frame%0A%28image%29%20inputs%2C%20which%20can%20reduce%20diagnostic%20accuracy%20for%20conditions%0Aidentifiable%20only%20through%20cardiac%20motion.%20In%20addition%2C%20echocardiographic%20videos%0Aare%20captured%20from%20multiple%20views%2C%20each%20varying%20in%20suitability%20for%20detecting%0Aspecific%20conditions.%20Leveraging%20multiple%20views%20may%20therefore%20improve%20diagnostic%0Aperformance.%20We%20developed%20a%20video-language%20model%20that%20processes%20full%20video%0Asequences%20from%20five%20standard%20views%2C%20trained%20on%2060%2C747%20echocardiographic%0Avideo-report%20pairs.%20We%20evaluated%20the%20gains%20in%20retrieval%20performance%20from%20video%0Ainput%20and%20multi-view%20support%2C%20including%20the%20contributions%20of%20various%20pretrained%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18800v2&entry.124074799=Read"},
{"title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space", "author": "Lin Li and Zehuan Huang and Haoran Feng and Gengxiong Zhuang and Rui Chen and Chunchao Guo and Lu Sheng", "abstract": "  3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.\n", "link": "http://arxiv.org/abs/2508.19247v1", "date": "2025-08-26", "relevancy": 2.8759, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5864}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.57}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoxHammer%3A%20Training-Free%20Precise%20and%20Coherent%203D%20Editing%20in%20Native%203D%0A%20%20Space&body=Title%3A%20VoxHammer%3A%20Training-Free%20Precise%20and%20Coherent%203D%20Editing%20in%20Native%203D%0A%20%20Space%0AAuthor%3A%20Lin%20Li%20and%20Zehuan%20Huang%20and%20Haoran%20Feng%20and%20Gengxiong%20Zhuang%20and%20Rui%20Chen%20and%20Chunchao%20Guo%20and%20Lu%20Sheng%0AAbstract%3A%20%20%203D%20local%20editing%20of%20specified%20regions%20is%20crucial%20for%20game%20industry%20and%20robot%0Ainteraction.%20Recent%20methods%20typically%20edit%20rendered%20multi-view%20images%20and%20then%0Areconstruct%203D%20models%2C%20but%20they%20face%20challenges%20in%20precisely%20preserving%0Aunedited%20regions%20and%20overall%20coherence.%20Inspired%20by%20structured%203D%20generative%0Amodels%2C%20we%20propose%20VoxHammer%2C%20a%20novel%20training-free%20approach%20that%20performs%0Aprecise%20and%20coherent%20editing%20in%203D%20latent%20space.%20Given%20a%203D%20model%2C%20VoxHammer%0Afirst%20predicts%20its%20inversion%20trajectory%20and%20obtains%20its%20inverted%20latents%20and%0Akey-value%20tokens%20at%20each%20timestep.%20Subsequently%2C%20in%20the%20denoising%20and%20editing%0Aphase%2C%20we%20replace%20the%20denoising%20features%20of%20preserved%20regions%20with%20the%0Acorresponding%20inverted%20latents%20and%20cached%20key-value%20tokens.%20By%20retaining%20these%0Acontextual%20features%2C%20this%20approach%20ensures%20consistent%20reconstruction%20of%0Apreserved%20areas%20and%20coherent%20integration%20of%20edited%20parts.%20To%20evaluate%20the%0Aconsistency%20of%20preserved%20regions%2C%20we%20constructed%20Edit3D-Bench%2C%20a%0Ahuman-annotated%20dataset%20comprising%20hundreds%20of%20samples%2C%20each%20with%20carefully%0Alabeled%203D%20editing%20regions.%20Experiments%20demonstrate%20that%20VoxHammer%0Asignificantly%20outperforms%20existing%20methods%20in%20terms%20of%20both%203D%20consistency%20of%0Apreserved%20regions%20and%20overall%20quality.%20Our%20method%20holds%20promise%20for%0Asynthesizing%20high-quality%20edited%20paired%20data%2C%20thereby%20laying%20the%20data%0Afoundation%20for%20in-context%203D%20generation.%20See%20our%20project%20page%20at%0Ahttps%3A//huanngzh.github.io/VoxHammer-Page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxHammer%253A%2520Training-Free%2520Precise%2520and%2520Coherent%25203D%2520Editing%2520in%2520Native%25203D%250A%2520%2520Space%26entry.906535625%3DLin%2520Li%2520and%2520Zehuan%2520Huang%2520and%2520Haoran%2520Feng%2520and%2520Gengxiong%2520Zhuang%2520and%2520Rui%2520Chen%2520and%2520Chunchao%2520Guo%2520and%2520Lu%2520Sheng%26entry.1292438233%3D%2520%25203D%2520local%2520editing%2520of%2520specified%2520regions%2520is%2520crucial%2520for%2520game%2520industry%2520and%2520robot%250Ainteraction.%2520Recent%2520methods%2520typically%2520edit%2520rendered%2520multi-view%2520images%2520and%2520then%250Areconstruct%25203D%2520models%252C%2520but%2520they%2520face%2520challenges%2520in%2520precisely%2520preserving%250Aunedited%2520regions%2520and%2520overall%2520coherence.%2520Inspired%2520by%2520structured%25203D%2520generative%250Amodels%252C%2520we%2520propose%2520VoxHammer%252C%2520a%2520novel%2520training-free%2520approach%2520that%2520performs%250Aprecise%2520and%2520coherent%2520editing%2520in%25203D%2520latent%2520space.%2520Given%2520a%25203D%2520model%252C%2520VoxHammer%250Afirst%2520predicts%2520its%2520inversion%2520trajectory%2520and%2520obtains%2520its%2520inverted%2520latents%2520and%250Akey-value%2520tokens%2520at%2520each%2520timestep.%2520Subsequently%252C%2520in%2520the%2520denoising%2520and%2520editing%250Aphase%252C%2520we%2520replace%2520the%2520denoising%2520features%2520of%2520preserved%2520regions%2520with%2520the%250Acorresponding%2520inverted%2520latents%2520and%2520cached%2520key-value%2520tokens.%2520By%2520retaining%2520these%250Acontextual%2520features%252C%2520this%2520approach%2520ensures%2520consistent%2520reconstruction%2520of%250Apreserved%2520areas%2520and%2520coherent%2520integration%2520of%2520edited%2520parts.%2520To%2520evaluate%2520the%250Aconsistency%2520of%2520preserved%2520regions%252C%2520we%2520constructed%2520Edit3D-Bench%252C%2520a%250Ahuman-annotated%2520dataset%2520comprising%2520hundreds%2520of%2520samples%252C%2520each%2520with%2520carefully%250Alabeled%25203D%2520editing%2520regions.%2520Experiments%2520demonstrate%2520that%2520VoxHammer%250Asignificantly%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520both%25203D%2520consistency%2520of%250Apreserved%2520regions%2520and%2520overall%2520quality.%2520Our%2520method%2520holds%2520promise%2520for%250Asynthesizing%2520high-quality%2520edited%2520paired%2520data%252C%2520thereby%2520laying%2520the%2520data%250Afoundation%2520for%2520in-context%25203D%2520generation.%2520See%2520our%2520project%2520page%2520at%250Ahttps%253A//huanngzh.github.io/VoxHammer-Page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoxHammer%3A%20Training-Free%20Precise%20and%20Coherent%203D%20Editing%20in%20Native%203D%0A%20%20Space&entry.906535625=Lin%20Li%20and%20Zehuan%20Huang%20and%20Haoran%20Feng%20and%20Gengxiong%20Zhuang%20and%20Rui%20Chen%20and%20Chunchao%20Guo%20and%20Lu%20Sheng&entry.1292438233=%20%203D%20local%20editing%20of%20specified%20regions%20is%20crucial%20for%20game%20industry%20and%20robot%0Ainteraction.%20Recent%20methods%20typically%20edit%20rendered%20multi-view%20images%20and%20then%0Areconstruct%203D%20models%2C%20but%20they%20face%20challenges%20in%20precisely%20preserving%0Aunedited%20regions%20and%20overall%20coherence.%20Inspired%20by%20structured%203D%20generative%0Amodels%2C%20we%20propose%20VoxHammer%2C%20a%20novel%20training-free%20approach%20that%20performs%0Aprecise%20and%20coherent%20editing%20in%203D%20latent%20space.%20Given%20a%203D%20model%2C%20VoxHammer%0Afirst%20predicts%20its%20inversion%20trajectory%20and%20obtains%20its%20inverted%20latents%20and%0Akey-value%20tokens%20at%20each%20timestep.%20Subsequently%2C%20in%20the%20denoising%20and%20editing%0Aphase%2C%20we%20replace%20the%20denoising%20features%20of%20preserved%20regions%20with%20the%0Acorresponding%20inverted%20latents%20and%20cached%20key-value%20tokens.%20By%20retaining%20these%0Acontextual%20features%2C%20this%20approach%20ensures%20consistent%20reconstruction%20of%0Apreserved%20areas%20and%20coherent%20integration%20of%20edited%20parts.%20To%20evaluate%20the%0Aconsistency%20of%20preserved%20regions%2C%20we%20constructed%20Edit3D-Bench%2C%20a%0Ahuman-annotated%20dataset%20comprising%20hundreds%20of%20samples%2C%20each%20with%20carefully%0Alabeled%203D%20editing%20regions.%20Experiments%20demonstrate%20that%20VoxHammer%0Asignificantly%20outperforms%20existing%20methods%20in%20terms%20of%20both%203D%20consistency%20of%0Apreserved%20regions%20and%20overall%20quality.%20Our%20method%20holds%20promise%20for%0Asynthesizing%20high-quality%20edited%20paired%20data%2C%20thereby%20laying%20the%20data%0Afoundation%20for%20in-context%203D%20generation.%20See%20our%20project%20page%20at%0Ahttps%3A//huanngzh.github.io/VoxHammer-Page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19247v1&entry.124074799=Read"},
{"title": "Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual\n  Limits of LVLMs", "author": "Somraj Gautam and Abhirama Subramanyam Penamakuri and Abhishek Bhandari and Gaurav Harit", "abstract": "  We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA)\non cricket scorecards, designed to evaluate large vision-language models\n(LVLMs) on complex numerical and cross-lingual reasoning over semi-structured\ntabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated\nscorecard images from ODI, T20, and Test formats, accompanied by 1,500 English\nQA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English\nscorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi\nscorecards, with all questions and answers kept in English to enable controlled\ncross-script evaluation. The task demands reasoning over structured numerical\ndata, multi-image context, and implicit domain knowledge. Empirical results\nshow that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle\non the English subset despite it being their primary training language and\nexhibit a further drop in performance on the Hindi subset. This reveals key\nlimitations in structure-aware visual text understanding, numerical reasoning,\nand cross-lingual generalization. The dataset is publicly available via Hugging\nFace at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLM\nresearch in this direction.\n", "link": "http://arxiv.org/abs/2508.17334v2", "date": "2025-08-26", "relevancy": 2.8657, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20%28Language%29%20Gap%3A%20Towards%20Probing%20Numerical%20and%20Cross-Lingual%0A%20%20Limits%20of%20LVLMs&body=Title%3A%20Mind%20the%20%28Language%29%20Gap%3A%20Towards%20Probing%20Numerical%20and%20Cross-Lingual%0A%20%20Limits%20of%20LVLMs%0AAuthor%3A%20Somraj%20Gautam%20and%20Abhirama%20Subramanyam%20Penamakuri%20and%20Abhishek%20Bhandari%20and%20Gaurav%20Harit%0AAbstract%3A%20%20%20We%20introduce%20MMCRICBENCH-3K%2C%20a%20benchmark%20for%20Visual%20Question%20Answering%20%28VQA%29%0Aon%20cricket%20scorecards%2C%20designed%20to%20evaluate%20large%20vision-language%20models%0A%28LVLMs%29%20on%20complex%20numerical%20and%20cross-lingual%20reasoning%20over%20semi-structured%0Atabular%20images.%20MMCRICBENCH-3K%20comprises%201%2C463%20synthetically%20generated%0Ascorecard%20images%20from%20ODI%2C%20T20%2C%20and%20Test%20formats%2C%20accompanied%20by%201%2C500%20English%0AQA%20pairs.%20It%20includes%20two%20subsets%3A%20MMCRICBENCH-E-1.5K%2C%20featuring%20English%0Ascorecards%2C%20and%20MMCRICBENCH-H-1.5K%2C%20containing%20visually%20similar%20Hindi%0Ascorecards%2C%20with%20all%20questions%20and%20answers%20kept%20in%20English%20to%20enable%20controlled%0Across-script%20evaluation.%20The%20task%20demands%20reasoning%20over%20structured%20numerical%0Adata%2C%20multi-image%20context%2C%20and%20implicit%20domain%20knowledge.%20Empirical%20results%0Ashow%20that%20even%20state-of-the-art%20LVLMs%2C%20such%20as%20GPT-4o%20and%20Qwen2.5VL%2C%20struggle%0Aon%20the%20English%20subset%20despite%20it%20being%20their%20primary%20training%20language%20and%0Aexhibit%20a%20further%20drop%20in%20performance%20on%20the%20Hindi%20subset.%20This%20reveals%20key%0Alimitations%20in%20structure-aware%20visual%20text%20understanding%2C%20numerical%20reasoning%2C%0Aand%20cross-lingual%20generalization.%20The%20dataset%20is%20publicly%20available%20via%20Hugging%0AFace%20at%20https%3A//huggingface.co/datasets/DIALab/MMCricBench%2C%20to%20promote%20LVLM%0Aresearch%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520%2528Language%2529%2520Gap%253A%2520Towards%2520Probing%2520Numerical%2520and%2520Cross-Lingual%250A%2520%2520Limits%2520of%2520LVLMs%26entry.906535625%3DSomraj%2520Gautam%2520and%2520Abhirama%2520Subramanyam%2520Penamakuri%2520and%2520Abhishek%2520Bhandari%2520and%2520Gaurav%2520Harit%26entry.1292438233%3D%2520%2520We%2520introduce%2520MMCRICBENCH-3K%252C%2520a%2520benchmark%2520for%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%250Aon%2520cricket%2520scorecards%252C%2520designed%2520to%2520evaluate%2520large%2520vision-language%2520models%250A%2528LVLMs%2529%2520on%2520complex%2520numerical%2520and%2520cross-lingual%2520reasoning%2520over%2520semi-structured%250Atabular%2520images.%2520MMCRICBENCH-3K%2520comprises%25201%252C463%2520synthetically%2520generated%250Ascorecard%2520images%2520from%2520ODI%252C%2520T20%252C%2520and%2520Test%2520formats%252C%2520accompanied%2520by%25201%252C500%2520English%250AQA%2520pairs.%2520It%2520includes%2520two%2520subsets%253A%2520MMCRICBENCH-E-1.5K%252C%2520featuring%2520English%250Ascorecards%252C%2520and%2520MMCRICBENCH-H-1.5K%252C%2520containing%2520visually%2520similar%2520Hindi%250Ascorecards%252C%2520with%2520all%2520questions%2520and%2520answers%2520kept%2520in%2520English%2520to%2520enable%2520controlled%250Across-script%2520evaluation.%2520The%2520task%2520demands%2520reasoning%2520over%2520structured%2520numerical%250Adata%252C%2520multi-image%2520context%252C%2520and%2520implicit%2520domain%2520knowledge.%2520Empirical%2520results%250Ashow%2520that%2520even%2520state-of-the-art%2520LVLMs%252C%2520such%2520as%2520GPT-4o%2520and%2520Qwen2.5VL%252C%2520struggle%250Aon%2520the%2520English%2520subset%2520despite%2520it%2520being%2520their%2520primary%2520training%2520language%2520and%250Aexhibit%2520a%2520further%2520drop%2520in%2520performance%2520on%2520the%2520Hindi%2520subset.%2520This%2520reveals%2520key%250Alimitations%2520in%2520structure-aware%2520visual%2520text%2520understanding%252C%2520numerical%2520reasoning%252C%250Aand%2520cross-lingual%2520generalization.%2520The%2520dataset%2520is%2520publicly%2520available%2520via%2520Hugging%250AFace%2520at%2520https%253A//huggingface.co/datasets/DIALab/MMCricBench%252C%2520to%2520promote%2520LVLM%250Aresearch%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20%28Language%29%20Gap%3A%20Towards%20Probing%20Numerical%20and%20Cross-Lingual%0A%20%20Limits%20of%20LVLMs&entry.906535625=Somraj%20Gautam%20and%20Abhirama%20Subramanyam%20Penamakuri%20and%20Abhishek%20Bhandari%20and%20Gaurav%20Harit&entry.1292438233=%20%20We%20introduce%20MMCRICBENCH-3K%2C%20a%20benchmark%20for%20Visual%20Question%20Answering%20%28VQA%29%0Aon%20cricket%20scorecards%2C%20designed%20to%20evaluate%20large%20vision-language%20models%0A%28LVLMs%29%20on%20complex%20numerical%20and%20cross-lingual%20reasoning%20over%20semi-structured%0Atabular%20images.%20MMCRICBENCH-3K%20comprises%201%2C463%20synthetically%20generated%0Ascorecard%20images%20from%20ODI%2C%20T20%2C%20and%20Test%20formats%2C%20accompanied%20by%201%2C500%20English%0AQA%20pairs.%20It%20includes%20two%20subsets%3A%20MMCRICBENCH-E-1.5K%2C%20featuring%20English%0Ascorecards%2C%20and%20MMCRICBENCH-H-1.5K%2C%20containing%20visually%20similar%20Hindi%0Ascorecards%2C%20with%20all%20questions%20and%20answers%20kept%20in%20English%20to%20enable%20controlled%0Across-script%20evaluation.%20The%20task%20demands%20reasoning%20over%20structured%20numerical%0Adata%2C%20multi-image%20context%2C%20and%20implicit%20domain%20knowledge.%20Empirical%20results%0Ashow%20that%20even%20state-of-the-art%20LVLMs%2C%20such%20as%20GPT-4o%20and%20Qwen2.5VL%2C%20struggle%0Aon%20the%20English%20subset%20despite%20it%20being%20their%20primary%20training%20language%20and%0Aexhibit%20a%20further%20drop%20in%20performance%20on%20the%20Hindi%20subset.%20This%20reveals%20key%0Alimitations%20in%20structure-aware%20visual%20text%20understanding%2C%20numerical%20reasoning%2C%0Aand%20cross-lingual%20generalization.%20The%20dataset%20is%20publicly%20available%20via%20Hugging%0AFace%20at%20https%3A//huggingface.co/datasets/DIALab/MMCricBench%2C%20to%20promote%20LVLM%0Aresearch%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17334v2&entry.124074799=Read"},
{"title": "Egocentric Human-Object Interaction Detection: A New Benchmark and\n  Method", "author": "Kunyuan Deng and Yi Wang and Lap-Pui Chau", "abstract": "  Egocentric human-object interaction (Ego-HOI) detection is crucial for\nintelligent agents to understand and assist human activities from a\nfirst-person perspective. However, progress has been hindered by the lack of\nbenchmarks and methods tailored to egocentric challenges such as severe\nhand-object occlusion. In this paper, we introduce the real-world Ego-HOI\ndetection task and the accompanying Ego-HOIBench, a new dataset with over 27K\negocentric images and explicit, fine-grained hand-verb-object triplet\nannotations across 123 categories. Ego-HOIBench covers diverse daily scenarios,\nobject types, and both single- and two-hand interactions, offering a\ncomprehensive testbed for Ego-HOI research. Benchmarking existing third-person\nHOI detectors on Ego-HOIBench reveals significant performance gaps,\nhighlighting the need for egocentric-specific solutions. To this end, we\npropose Hand Geometry and Interactivity Refinement (HGIR), a lightweight,\nplug-and-play scheme that leverages hand pose and geometric cues to enhance\ninteraction representations. Specifically, HGIR explicitly extracts global hand\ngeometric features from the estimated hand pose proposals, and further refines\ninteraction features through pose-interaction attention, enabling the model to\nfocus on subtle hand-object relationship differences even under severe\nocclusion. HGIR significantly improves Ego-HOI detection performance across\nmultiple baselines, achieving new state-of-the-art results on Ego-HOIBench. Our\ndataset and method establish a solid foundation for future research in\negocentric vision and human-object interaction understanding. Project page:\nhttps://dengkunyuan.github.io/EgoHOIBench/\n", "link": "http://arxiv.org/abs/2506.14189v2", "date": "2025-08-26", "relevancy": 2.8233, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5906}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5587}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Egocentric%20Human-Object%20Interaction%20Detection%3A%20A%20New%20Benchmark%20and%0A%20%20Method&body=Title%3A%20Egocentric%20Human-Object%20Interaction%20Detection%3A%20A%20New%20Benchmark%20and%0A%20%20Method%0AAuthor%3A%20Kunyuan%20Deng%20and%20Yi%20Wang%20and%20Lap-Pui%20Chau%0AAbstract%3A%20%20%20Egocentric%20human-object%20interaction%20%28Ego-HOI%29%20detection%20is%20crucial%20for%0Aintelligent%20agents%20to%20understand%20and%20assist%20human%20activities%20from%20a%0Afirst-person%20perspective.%20However%2C%20progress%20has%20been%20hindered%20by%20the%20lack%20of%0Abenchmarks%20and%20methods%20tailored%20to%20egocentric%20challenges%20such%20as%20severe%0Ahand-object%20occlusion.%20In%20this%20paper%2C%20we%20introduce%20the%20real-world%20Ego-HOI%0Adetection%20task%20and%20the%20accompanying%20Ego-HOIBench%2C%20a%20new%20dataset%20with%20over%2027K%0Aegocentric%20images%20and%20explicit%2C%20fine-grained%20hand-verb-object%20triplet%0Aannotations%20across%20123%20categories.%20Ego-HOIBench%20covers%20diverse%20daily%20scenarios%2C%0Aobject%20types%2C%20and%20both%20single-%20and%20two-hand%20interactions%2C%20offering%20a%0Acomprehensive%20testbed%20for%20Ego-HOI%20research.%20Benchmarking%20existing%20third-person%0AHOI%20detectors%20on%20Ego-HOIBench%20reveals%20significant%20performance%20gaps%2C%0Ahighlighting%20the%20need%20for%20egocentric-specific%20solutions.%20To%20this%20end%2C%20we%0Apropose%20Hand%20Geometry%20and%20Interactivity%20Refinement%20%28HGIR%29%2C%20a%20lightweight%2C%0Aplug-and-play%20scheme%20that%20leverages%20hand%20pose%20and%20geometric%20cues%20to%20enhance%0Ainteraction%20representations.%20Specifically%2C%20HGIR%20explicitly%20extracts%20global%20hand%0Ageometric%20features%20from%20the%20estimated%20hand%20pose%20proposals%2C%20and%20further%20refines%0Ainteraction%20features%20through%20pose-interaction%20attention%2C%20enabling%20the%20model%20to%0Afocus%20on%20subtle%20hand-object%20relationship%20differences%20even%20under%20severe%0Aocclusion.%20HGIR%20significantly%20improves%20Ego-HOI%20detection%20performance%20across%0Amultiple%20baselines%2C%20achieving%20new%20state-of-the-art%20results%20on%20Ego-HOIBench.%20Our%0Adataset%20and%20method%20establish%20a%20solid%20foundation%20for%20future%20research%20in%0Aegocentric%20vision%20and%20human-object%20interaction%20understanding.%20Project%20page%3A%0Ahttps%3A//dengkunyuan.github.io/EgoHOIBench/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgocentric%2520Human-Object%2520Interaction%2520Detection%253A%2520A%2520New%2520Benchmark%2520and%250A%2520%2520Method%26entry.906535625%3DKunyuan%2520Deng%2520and%2520Yi%2520Wang%2520and%2520Lap-Pui%2520Chau%26entry.1292438233%3D%2520%2520Egocentric%2520human-object%2520interaction%2520%2528Ego-HOI%2529%2520detection%2520is%2520crucial%2520for%250Aintelligent%2520agents%2520to%2520understand%2520and%2520assist%2520human%2520activities%2520from%2520a%250Afirst-person%2520perspective.%2520However%252C%2520progress%2520has%2520been%2520hindered%2520by%2520the%2520lack%2520of%250Abenchmarks%2520and%2520methods%2520tailored%2520to%2520egocentric%2520challenges%2520such%2520as%2520severe%250Ahand-object%2520occlusion.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520real-world%2520Ego-HOI%250Adetection%2520task%2520and%2520the%2520accompanying%2520Ego-HOIBench%252C%2520a%2520new%2520dataset%2520with%2520over%252027K%250Aegocentric%2520images%2520and%2520explicit%252C%2520fine-grained%2520hand-verb-object%2520triplet%250Aannotations%2520across%2520123%2520categories.%2520Ego-HOIBench%2520covers%2520diverse%2520daily%2520scenarios%252C%250Aobject%2520types%252C%2520and%2520both%2520single-%2520and%2520two-hand%2520interactions%252C%2520offering%2520a%250Acomprehensive%2520testbed%2520for%2520Ego-HOI%2520research.%2520Benchmarking%2520existing%2520third-person%250AHOI%2520detectors%2520on%2520Ego-HOIBench%2520reveals%2520significant%2520performance%2520gaps%252C%250Ahighlighting%2520the%2520need%2520for%2520egocentric-specific%2520solutions.%2520To%2520this%2520end%252C%2520we%250Apropose%2520Hand%2520Geometry%2520and%2520Interactivity%2520Refinement%2520%2528HGIR%2529%252C%2520a%2520lightweight%252C%250Aplug-and-play%2520scheme%2520that%2520leverages%2520hand%2520pose%2520and%2520geometric%2520cues%2520to%2520enhance%250Ainteraction%2520representations.%2520Specifically%252C%2520HGIR%2520explicitly%2520extracts%2520global%2520hand%250Ageometric%2520features%2520from%2520the%2520estimated%2520hand%2520pose%2520proposals%252C%2520and%2520further%2520refines%250Ainteraction%2520features%2520through%2520pose-interaction%2520attention%252C%2520enabling%2520the%2520model%2520to%250Afocus%2520on%2520subtle%2520hand-object%2520relationship%2520differences%2520even%2520under%2520severe%250Aocclusion.%2520HGIR%2520significantly%2520improves%2520Ego-HOI%2520detection%2520performance%2520across%250Amultiple%2520baselines%252C%2520achieving%2520new%2520state-of-the-art%2520results%2520on%2520Ego-HOIBench.%2520Our%250Adataset%2520and%2520method%2520establish%2520a%2520solid%2520foundation%2520for%2520future%2520research%2520in%250Aegocentric%2520vision%2520and%2520human-object%2520interaction%2520understanding.%2520Project%2520page%253A%250Ahttps%253A//dengkunyuan.github.io/EgoHOIBench/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Egocentric%20Human-Object%20Interaction%20Detection%3A%20A%20New%20Benchmark%20and%0A%20%20Method&entry.906535625=Kunyuan%20Deng%20and%20Yi%20Wang%20and%20Lap-Pui%20Chau&entry.1292438233=%20%20Egocentric%20human-object%20interaction%20%28Ego-HOI%29%20detection%20is%20crucial%20for%0Aintelligent%20agents%20to%20understand%20and%20assist%20human%20activities%20from%20a%0Afirst-person%20perspective.%20However%2C%20progress%20has%20been%20hindered%20by%20the%20lack%20of%0Abenchmarks%20and%20methods%20tailored%20to%20egocentric%20challenges%20such%20as%20severe%0Ahand-object%20occlusion.%20In%20this%20paper%2C%20we%20introduce%20the%20real-world%20Ego-HOI%0Adetection%20task%20and%20the%20accompanying%20Ego-HOIBench%2C%20a%20new%20dataset%20with%20over%2027K%0Aegocentric%20images%20and%20explicit%2C%20fine-grained%20hand-verb-object%20triplet%0Aannotations%20across%20123%20categories.%20Ego-HOIBench%20covers%20diverse%20daily%20scenarios%2C%0Aobject%20types%2C%20and%20both%20single-%20and%20two-hand%20interactions%2C%20offering%20a%0Acomprehensive%20testbed%20for%20Ego-HOI%20research.%20Benchmarking%20existing%20third-person%0AHOI%20detectors%20on%20Ego-HOIBench%20reveals%20significant%20performance%20gaps%2C%0Ahighlighting%20the%20need%20for%20egocentric-specific%20solutions.%20To%20this%20end%2C%20we%0Apropose%20Hand%20Geometry%20and%20Interactivity%20Refinement%20%28HGIR%29%2C%20a%20lightweight%2C%0Aplug-and-play%20scheme%20that%20leverages%20hand%20pose%20and%20geometric%20cues%20to%20enhance%0Ainteraction%20representations.%20Specifically%2C%20HGIR%20explicitly%20extracts%20global%20hand%0Ageometric%20features%20from%20the%20estimated%20hand%20pose%20proposals%2C%20and%20further%20refines%0Ainteraction%20features%20through%20pose-interaction%20attention%2C%20enabling%20the%20model%20to%0Afocus%20on%20subtle%20hand-object%20relationship%20differences%20even%20under%20severe%0Aocclusion.%20HGIR%20significantly%20improves%20Ego-HOI%20detection%20performance%20across%0Amultiple%20baselines%2C%20achieving%20new%20state-of-the-art%20results%20on%20Ego-HOIBench.%20Our%0Adataset%20and%20method%20establish%20a%20solid%20foundation%20for%20future%20research%20in%0Aegocentric%20vision%20and%20human-object%20interaction%20understanding.%20Project%20page%3A%0Ahttps%3A//dengkunyuan.github.io/EgoHOIBench/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14189v2&entry.124074799=Read"},
{"title": "Can we make NeRF-based visual localization privacy-preserving?", "author": "Maxime Pietrantoni and Martin Humenberger and Torsten Sattler and Gabriela Csurka", "abstract": "  Visual localization (VL) is the task of estimating the camera pose in a known\nscene. VL methods, a.o., can be distinguished based on how they represent the\nscene, e.g., explicitly through a (sparse) point cloud or a collection of\nimages or implicitly through the weights of a neural network. Recently,\nNeRF-based methods have become popular for VL. While NeRFs offer high-quality\nnovel view synthesis, they inadvertently encode fine scene details, raising\nprivacy concerns when deployed in cloud-based localization services as\nsensitive information could be recovered. In this paper, we tackle this\nchallenge on two ends. We first propose a new protocol to assess\nprivacy-preservation of NeRF-based representations. We show that NeRFs trained\nwith photometric losses store fine-grained details in their geometry\nrepresentations, making them vulnerable to privacy attacks, even if the head\nthat predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving\nNeural Segmentation Field), a NeRF variant trained with segmentation\nsupervision instead of RGB images. These segmentation labels are learned in a\nself-supervised manner, ensuring they are coarse enough to obscure identifiable\nscene details while remaining discriminativeness in 3D. The segmentation space\nof ppNeSF can be used for accurate visual localization, yielding\nstate-of-the-art results.\n", "link": "http://arxiv.org/abs/2508.18971v1", "date": "2025-08-26", "relevancy": 2.818, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5788}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5719}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20we%20make%20NeRF-based%20visual%20localization%20privacy-preserving%3F&body=Title%3A%20Can%20we%20make%20NeRF-based%20visual%20localization%20privacy-preserving%3F%0AAuthor%3A%20Maxime%20Pietrantoni%20and%20Martin%20Humenberger%20and%20Torsten%20Sattler%20and%20Gabriela%20Csurka%0AAbstract%3A%20%20%20Visual%20localization%20%28VL%29%20is%20the%20task%20of%20estimating%20the%20camera%20pose%20in%20a%20known%0Ascene.%20VL%20methods%2C%20a.o.%2C%20can%20be%20distinguished%20based%20on%20how%20they%20represent%20the%0Ascene%2C%20e.g.%2C%20explicitly%20through%20a%20%28sparse%29%20point%20cloud%20or%20a%20collection%20of%0Aimages%20or%20implicitly%20through%20the%20weights%20of%20a%20neural%20network.%20Recently%2C%0ANeRF-based%20methods%20have%20become%20popular%20for%20VL.%20While%20NeRFs%20offer%20high-quality%0Anovel%20view%20synthesis%2C%20they%20inadvertently%20encode%20fine%20scene%20details%2C%20raising%0Aprivacy%20concerns%20when%20deployed%20in%20cloud-based%20localization%20services%20as%0Asensitive%20information%20could%20be%20recovered.%20In%20this%20paper%2C%20we%20tackle%20this%0Achallenge%20on%20two%20ends.%20We%20first%20propose%20a%20new%20protocol%20to%20assess%0Aprivacy-preservation%20of%20NeRF-based%20representations.%20We%20show%20that%20NeRFs%20trained%0Awith%20photometric%20losses%20store%20fine-grained%20details%20in%20their%20geometry%0Arepresentations%2C%20making%20them%20vulnerable%20to%20privacy%20attacks%2C%20even%20if%20the%20head%0Athat%20predicts%20colors%20is%20removed.%20Second%2C%20we%20propose%20ppNeSF%20%28Privacy-Preserving%0ANeural%20Segmentation%20Field%29%2C%20a%20NeRF%20variant%20trained%20with%20segmentation%0Asupervision%20instead%20of%20RGB%20images.%20These%20segmentation%20labels%20are%20learned%20in%20a%0Aself-supervised%20manner%2C%20ensuring%20they%20are%20coarse%20enough%20to%20obscure%20identifiable%0Ascene%20details%20while%20remaining%20discriminativeness%20in%203D.%20The%20segmentation%20space%0Aof%20ppNeSF%20can%20be%20used%20for%20accurate%20visual%20localization%2C%20yielding%0Astate-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520we%2520make%2520NeRF-based%2520visual%2520localization%2520privacy-preserving%253F%26entry.906535625%3DMaxime%2520Pietrantoni%2520and%2520Martin%2520Humenberger%2520and%2520Torsten%2520Sattler%2520and%2520Gabriela%2520Csurka%26entry.1292438233%3D%2520%2520Visual%2520localization%2520%2528VL%2529%2520is%2520the%2520task%2520of%2520estimating%2520the%2520camera%2520pose%2520in%2520a%2520known%250Ascene.%2520VL%2520methods%252C%2520a.o.%252C%2520can%2520be%2520distinguished%2520based%2520on%2520how%2520they%2520represent%2520the%250Ascene%252C%2520e.g.%252C%2520explicitly%2520through%2520a%2520%2528sparse%2529%2520point%2520cloud%2520or%2520a%2520collection%2520of%250Aimages%2520or%2520implicitly%2520through%2520the%2520weights%2520of%2520a%2520neural%2520network.%2520Recently%252C%250ANeRF-based%2520methods%2520have%2520become%2520popular%2520for%2520VL.%2520While%2520NeRFs%2520offer%2520high-quality%250Anovel%2520view%2520synthesis%252C%2520they%2520inadvertently%2520encode%2520fine%2520scene%2520details%252C%2520raising%250Aprivacy%2520concerns%2520when%2520deployed%2520in%2520cloud-based%2520localization%2520services%2520as%250Asensitive%2520information%2520could%2520be%2520recovered.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520this%250Achallenge%2520on%2520two%2520ends.%2520We%2520first%2520propose%2520a%2520new%2520protocol%2520to%2520assess%250Aprivacy-preservation%2520of%2520NeRF-based%2520representations.%2520We%2520show%2520that%2520NeRFs%2520trained%250Awith%2520photometric%2520losses%2520store%2520fine-grained%2520details%2520in%2520their%2520geometry%250Arepresentations%252C%2520making%2520them%2520vulnerable%2520to%2520privacy%2520attacks%252C%2520even%2520if%2520the%2520head%250Athat%2520predicts%2520colors%2520is%2520removed.%2520Second%252C%2520we%2520propose%2520ppNeSF%2520%2528Privacy-Preserving%250ANeural%2520Segmentation%2520Field%2529%252C%2520a%2520NeRF%2520variant%2520trained%2520with%2520segmentation%250Asupervision%2520instead%2520of%2520RGB%2520images.%2520These%2520segmentation%2520labels%2520are%2520learned%2520in%2520a%250Aself-supervised%2520manner%252C%2520ensuring%2520they%2520are%2520coarse%2520enough%2520to%2520obscure%2520identifiable%250Ascene%2520details%2520while%2520remaining%2520discriminativeness%2520in%25203D.%2520The%2520segmentation%2520space%250Aof%2520ppNeSF%2520can%2520be%2520used%2520for%2520accurate%2520visual%2520localization%252C%2520yielding%250Astate-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20we%20make%20NeRF-based%20visual%20localization%20privacy-preserving%3F&entry.906535625=Maxime%20Pietrantoni%20and%20Martin%20Humenberger%20and%20Torsten%20Sattler%20and%20Gabriela%20Csurka&entry.1292438233=%20%20Visual%20localization%20%28VL%29%20is%20the%20task%20of%20estimating%20the%20camera%20pose%20in%20a%20known%0Ascene.%20VL%20methods%2C%20a.o.%2C%20can%20be%20distinguished%20based%20on%20how%20they%20represent%20the%0Ascene%2C%20e.g.%2C%20explicitly%20through%20a%20%28sparse%29%20point%20cloud%20or%20a%20collection%20of%0Aimages%20or%20implicitly%20through%20the%20weights%20of%20a%20neural%20network.%20Recently%2C%0ANeRF-based%20methods%20have%20become%20popular%20for%20VL.%20While%20NeRFs%20offer%20high-quality%0Anovel%20view%20synthesis%2C%20they%20inadvertently%20encode%20fine%20scene%20details%2C%20raising%0Aprivacy%20concerns%20when%20deployed%20in%20cloud-based%20localization%20services%20as%0Asensitive%20information%20could%20be%20recovered.%20In%20this%20paper%2C%20we%20tackle%20this%0Achallenge%20on%20two%20ends.%20We%20first%20propose%20a%20new%20protocol%20to%20assess%0Aprivacy-preservation%20of%20NeRF-based%20representations.%20We%20show%20that%20NeRFs%20trained%0Awith%20photometric%20losses%20store%20fine-grained%20details%20in%20their%20geometry%0Arepresentations%2C%20making%20them%20vulnerable%20to%20privacy%20attacks%2C%20even%20if%20the%20head%0Athat%20predicts%20colors%20is%20removed.%20Second%2C%20we%20propose%20ppNeSF%20%28Privacy-Preserving%0ANeural%20Segmentation%20Field%29%2C%20a%20NeRF%20variant%20trained%20with%20segmentation%0Asupervision%20instead%20of%20RGB%20images.%20These%20segmentation%20labels%20are%20learned%20in%20a%0Aself-supervised%20manner%2C%20ensuring%20they%20are%20coarse%20enough%20to%20obscure%20identifiable%0Ascene%20details%20while%20remaining%20discriminativeness%20in%203D.%20The%20segmentation%20space%0Aof%20ppNeSF%20can%20be%20used%20for%20accurate%20visual%20localization%2C%20yielding%0Astate-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18971v1&entry.124074799=Read"},
{"title": "M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context\n  Learning via Representation Engineering", "author": "Yanshu Li and Yi Cao and Hongyang He and Qisen Cheng and Xiang Fu and Xi Xiao and Tianyang Wang and Ruixiang Tang", "abstract": "  Multimodal in-context learning (ICL) equips Large Vision-language Models\n(LVLMs) with the ability to adapt to new tasks via multiple user-provided\ndemonstrations, without requiring any model parameter updates. However, its\neffectiveness is constrained by the token-intensive nature of multimodal inputs\nand the complexity of cross-modal few-shot reasoning, which together hinder\nLVLMs from extracting useful patterns from demonstrations. To address these\nchallenges, we propose \\textbf{M$^2$IV}, a novel representation engineering\napproach that replaces explicit token-level demonstrations with a set of\nlearnable Multimodal In-context Vectors directly injected into the residual\nstreams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA)\nand multi-layer perceptrons (MLP) in the ICL process, we design a training\nstrategy that enables M$^2$IV to perform fine-grained semantic distillation and\nrobust cross-modal representation learning. M$^2$IV not only improves\nperformance across diverse tasks and LVLMs but also significantly reduces token\noverhead, enabling graceful scaling to many-shot scenarios. To further enhance\nusability, we introduce \\textbf{VLibrary}, a repository that stores trained\nM$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer\npre-trained LVLMs in a customized manner that meets diverse requirements.\nExtensive experiments demonstrate that M$^2$IV consistently outperforms vanilla\nICL and prior representation engineering baselines, achieving an average\naccuracy gain of 3.74\\% with substantial improvements in overall efficiency.\n", "link": "http://arxiv.org/abs/2504.04633v3", "date": "2025-08-26", "relevancy": 2.7822, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M%24%5E2%24IV%3A%20Towards%20Efficient%20and%20Fine-grained%20Multimodal%20In-Context%0A%20%20Learning%20via%20Representation%20Engineering&body=Title%3A%20M%24%5E2%24IV%3A%20Towards%20Efficient%20and%20Fine-grained%20Multimodal%20In-Context%0A%20%20Learning%20via%20Representation%20Engineering%0AAuthor%3A%20Yanshu%20Li%20and%20Yi%20Cao%20and%20Hongyang%20He%20and%20Qisen%20Cheng%20and%20Xiang%20Fu%20and%20Xi%20Xiao%20and%20Tianyang%20Wang%20and%20Ruixiang%20Tang%0AAbstract%3A%20%20%20Multimodal%20in-context%20learning%20%28ICL%29%20equips%20Large%20Vision-language%20Models%0A%28LVLMs%29%20with%20the%20ability%20to%20adapt%20to%20new%20tasks%20via%20multiple%20user-provided%0Ademonstrations%2C%20without%20requiring%20any%20model%20parameter%20updates.%20However%2C%20its%0Aeffectiveness%20is%20constrained%20by%20the%20token-intensive%20nature%20of%20multimodal%20inputs%0Aand%20the%20complexity%20of%20cross-modal%20few-shot%20reasoning%2C%20which%20together%20hinder%0ALVLMs%20from%20extracting%20useful%20patterns%20from%20demonstrations.%20To%20address%20these%0Achallenges%2C%20we%20propose%20%5Ctextbf%7BM%24%5E2%24IV%7D%2C%20a%20novel%20representation%20engineering%0Aapproach%20that%20replaces%20explicit%20token-level%20demonstrations%20with%20a%20set%20of%0Alearnable%20Multimodal%20In-context%20Vectors%20directly%20injected%20into%20the%20residual%0Astreams%20of%20LVLMs.%20By%20analyzing%20the%20distinct%20roles%20of%20multi-head%20attention%20%28MHA%29%0Aand%20multi-layer%20perceptrons%20%28MLP%29%20in%20the%20ICL%20process%2C%20we%20design%20a%20training%0Astrategy%20that%20enables%20M%24%5E2%24IV%20to%20perform%20fine-grained%20semantic%20distillation%20and%0Arobust%20cross-modal%20representation%20learning.%20M%24%5E2%24IV%20not%20only%20improves%0Aperformance%20across%20diverse%20tasks%20and%20LVLMs%20but%20also%20significantly%20reduces%20token%0Aoverhead%2C%20enabling%20graceful%20scaling%20to%20many-shot%20scenarios.%20To%20further%20enhance%0Ausability%2C%20we%20introduce%20%5Ctextbf%7BVLibrary%7D%2C%20a%20repository%20that%20stores%20trained%0AM%24%5E2%24IVs%20for%20flexible%20retrieval%20and%20injection.%20With%20VLibrary%2C%20users%20can%20steer%0Apre-trained%20LVLMs%20in%20a%20customized%20manner%20that%20meets%20diverse%20requirements.%0AExtensive%20experiments%20demonstrate%20that%20M%24%5E2%24IV%20consistently%20outperforms%20vanilla%0AICL%20and%20prior%20representation%20engineering%20baselines%2C%20achieving%20an%20average%0Aaccuracy%20gain%20of%203.74%5C%25%20with%20substantial%20improvements%20in%20overall%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04633v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM%2524%255E2%2524IV%253A%2520Towards%2520Efficient%2520and%2520Fine-grained%2520Multimodal%2520In-Context%250A%2520%2520Learning%2520via%2520Representation%2520Engineering%26entry.906535625%3DYanshu%2520Li%2520and%2520Yi%2520Cao%2520and%2520Hongyang%2520He%2520and%2520Qisen%2520Cheng%2520and%2520Xiang%2520Fu%2520and%2520Xi%2520Xiao%2520and%2520Tianyang%2520Wang%2520and%2520Ruixiang%2520Tang%26entry.1292438233%3D%2520%2520Multimodal%2520in-context%2520learning%2520%2528ICL%2529%2520equips%2520Large%2520Vision-language%2520Models%250A%2528LVLMs%2529%2520with%2520the%2520ability%2520to%2520adapt%2520to%2520new%2520tasks%2520via%2520multiple%2520user-provided%250Ademonstrations%252C%2520without%2520requiring%2520any%2520model%2520parameter%2520updates.%2520However%252C%2520its%250Aeffectiveness%2520is%2520constrained%2520by%2520the%2520token-intensive%2520nature%2520of%2520multimodal%2520inputs%250Aand%2520the%2520complexity%2520of%2520cross-modal%2520few-shot%2520reasoning%252C%2520which%2520together%2520hinder%250ALVLMs%2520from%2520extracting%2520useful%2520patterns%2520from%2520demonstrations.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520%255Ctextbf%257BM%2524%255E2%2524IV%257D%252C%2520a%2520novel%2520representation%2520engineering%250Aapproach%2520that%2520replaces%2520explicit%2520token-level%2520demonstrations%2520with%2520a%2520set%2520of%250Alearnable%2520Multimodal%2520In-context%2520Vectors%2520directly%2520injected%2520into%2520the%2520residual%250Astreams%2520of%2520LVLMs.%2520By%2520analyzing%2520the%2520distinct%2520roles%2520of%2520multi-head%2520attention%2520%2528MHA%2529%250Aand%2520multi-layer%2520perceptrons%2520%2528MLP%2529%2520in%2520the%2520ICL%2520process%252C%2520we%2520design%2520a%2520training%250Astrategy%2520that%2520enables%2520M%2524%255E2%2524IV%2520to%2520perform%2520fine-grained%2520semantic%2520distillation%2520and%250Arobust%2520cross-modal%2520representation%2520learning.%2520M%2524%255E2%2524IV%2520not%2520only%2520improves%250Aperformance%2520across%2520diverse%2520tasks%2520and%2520LVLMs%2520but%2520also%2520significantly%2520reduces%2520token%250Aoverhead%252C%2520enabling%2520graceful%2520scaling%2520to%2520many-shot%2520scenarios.%2520To%2520further%2520enhance%250Ausability%252C%2520we%2520introduce%2520%255Ctextbf%257BVLibrary%257D%252C%2520a%2520repository%2520that%2520stores%2520trained%250AM%2524%255E2%2524IVs%2520for%2520flexible%2520retrieval%2520and%2520injection.%2520With%2520VLibrary%252C%2520users%2520can%2520steer%250Apre-trained%2520LVLMs%2520in%2520a%2520customized%2520manner%2520that%2520meets%2520diverse%2520requirements.%250AExtensive%2520experiments%2520demonstrate%2520that%2520M%2524%255E2%2524IV%2520consistently%2520outperforms%2520vanilla%250AICL%2520and%2520prior%2520representation%2520engineering%2520baselines%252C%2520achieving%2520an%2520average%250Aaccuracy%2520gain%2520of%25203.74%255C%2525%2520with%2520substantial%2520improvements%2520in%2520overall%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04633v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%24%5E2%24IV%3A%20Towards%20Efficient%20and%20Fine-grained%20Multimodal%20In-Context%0A%20%20Learning%20via%20Representation%20Engineering&entry.906535625=Yanshu%20Li%20and%20Yi%20Cao%20and%20Hongyang%20He%20and%20Qisen%20Cheng%20and%20Xiang%20Fu%20and%20Xi%20Xiao%20and%20Tianyang%20Wang%20and%20Ruixiang%20Tang&entry.1292438233=%20%20Multimodal%20in-context%20learning%20%28ICL%29%20equips%20Large%20Vision-language%20Models%0A%28LVLMs%29%20with%20the%20ability%20to%20adapt%20to%20new%20tasks%20via%20multiple%20user-provided%0Ademonstrations%2C%20without%20requiring%20any%20model%20parameter%20updates.%20However%2C%20its%0Aeffectiveness%20is%20constrained%20by%20the%20token-intensive%20nature%20of%20multimodal%20inputs%0Aand%20the%20complexity%20of%20cross-modal%20few-shot%20reasoning%2C%20which%20together%20hinder%0ALVLMs%20from%20extracting%20useful%20patterns%20from%20demonstrations.%20To%20address%20these%0Achallenges%2C%20we%20propose%20%5Ctextbf%7BM%24%5E2%24IV%7D%2C%20a%20novel%20representation%20engineering%0Aapproach%20that%20replaces%20explicit%20token-level%20demonstrations%20with%20a%20set%20of%0Alearnable%20Multimodal%20In-context%20Vectors%20directly%20injected%20into%20the%20residual%0Astreams%20of%20LVLMs.%20By%20analyzing%20the%20distinct%20roles%20of%20multi-head%20attention%20%28MHA%29%0Aand%20multi-layer%20perceptrons%20%28MLP%29%20in%20the%20ICL%20process%2C%20we%20design%20a%20training%0Astrategy%20that%20enables%20M%24%5E2%24IV%20to%20perform%20fine-grained%20semantic%20distillation%20and%0Arobust%20cross-modal%20representation%20learning.%20M%24%5E2%24IV%20not%20only%20improves%0Aperformance%20across%20diverse%20tasks%20and%20LVLMs%20but%20also%20significantly%20reduces%20token%0Aoverhead%2C%20enabling%20graceful%20scaling%20to%20many-shot%20scenarios.%20To%20further%20enhance%0Ausability%2C%20we%20introduce%20%5Ctextbf%7BVLibrary%7D%2C%20a%20repository%20that%20stores%20trained%0AM%24%5E2%24IVs%20for%20flexible%20retrieval%20and%20injection.%20With%20VLibrary%2C%20users%20can%20steer%0Apre-trained%20LVLMs%20in%20a%20customized%20manner%20that%20meets%20diverse%20requirements.%0AExtensive%20experiments%20demonstrate%20that%20M%24%5E2%24IV%20consistently%20outperforms%20vanilla%0AICL%20and%20prior%20representation%20engineering%20baselines%2C%20achieving%20an%20average%0Aaccuracy%20gain%20of%203.74%5C%25%20with%20substantial%20improvements%20in%20overall%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04633v3&entry.124074799=Read"},
{"title": "Generative Interfaces for Language Models", "author": "Jiaqi Chen and Yanzhe Zhang and Yutong Zhang and Yijia Shao and Diyi Yang", "abstract": "  Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.\n", "link": "http://arxiv.org/abs/2508.19227v1", "date": "2025-08-26", "relevancy": 2.7683, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6123}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5282}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Interfaces%20for%20Language%20Models&body=Title%3A%20Generative%20Interfaces%20for%20Language%20Models%0AAuthor%3A%20Jiaqi%20Chen%20and%20Yanzhe%20Zhang%20and%20Yutong%20Zhang%20and%20Yijia%20Shao%20and%20Diyi%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20seen%20as%20assistants%2C%20copilots%2C%0Aand%20consultants%2C%20capable%20of%20supporting%20a%20wide%20range%20of%20tasks%20through%20natural%0Aconversation.%20However%2C%20most%20systems%20remain%20constrained%20by%20a%20linear%0Arequest-response%20format%20that%20often%20makes%20interactions%20inefficient%20in%0Amulti-turn%2C%20information-dense%2C%20and%20exploratory%20tasks.%20To%20address%20these%0Alimitations%2C%20we%20propose%20Generative%20Interfaces%20for%20Language%20Models%2C%20a%20paradigm%0Ain%20which%20LLMs%20respond%20to%20user%20queries%20by%20proactively%20generating%20user%20interfaces%0A%28UIs%29%20that%20enable%20more%20adaptive%20and%20interactive%20engagement.%20Our%20framework%0Aleverages%20structured%20interface-specific%20representations%20and%20iterative%0Arefinements%20to%20translate%20user%20queries%20into%20task-specific%20UIs.%20For%20systematic%0Aevaluation%2C%20we%20introduce%20a%20multidimensional%20assessment%20framework%20that%20compares%0Agenerative%20interfaces%20with%20traditional%20chat-based%20ones%20across%20diverse%20tasks%2C%0Ainteraction%20patterns%2C%20and%20query%20types%2C%20capturing%20functional%2C%20interactive%2C%20and%0Aemotional%20aspects%20of%20user%20experience.%20Results%20show%20that%20generative%20interfaces%0Aconsistently%20outperform%20conversational%20ones%2C%20with%20humans%20preferring%20them%20in%0Aover%2070%25%20of%20cases.%20These%20findings%20clarify%20when%20and%20why%20users%20favor%20generative%0Ainterfaces%2C%20paving%20the%20way%20for%20future%20advancements%20in%20human-AI%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Interfaces%2520for%2520Language%2520Models%26entry.906535625%3DJiaqi%2520Chen%2520and%2520Yanzhe%2520Zhang%2520and%2520Yutong%2520Zhang%2520and%2520Yijia%2520Shao%2520and%2520Diyi%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520seen%2520as%2520assistants%252C%2520copilots%252C%250Aand%2520consultants%252C%2520capable%2520of%2520supporting%2520a%2520wide%2520range%2520of%2520tasks%2520through%2520natural%250Aconversation.%2520However%252C%2520most%2520systems%2520remain%2520constrained%2520by%2520a%2520linear%250Arequest-response%2520format%2520that%2520often%2520makes%2520interactions%2520inefficient%2520in%250Amulti-turn%252C%2520information-dense%252C%2520and%2520exploratory%2520tasks.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520Generative%2520Interfaces%2520for%2520Language%2520Models%252C%2520a%2520paradigm%250Ain%2520which%2520LLMs%2520respond%2520to%2520user%2520queries%2520by%2520proactively%2520generating%2520user%2520interfaces%250A%2528UIs%2529%2520that%2520enable%2520more%2520adaptive%2520and%2520interactive%2520engagement.%2520Our%2520framework%250Aleverages%2520structured%2520interface-specific%2520representations%2520and%2520iterative%250Arefinements%2520to%2520translate%2520user%2520queries%2520into%2520task-specific%2520UIs.%2520For%2520systematic%250Aevaluation%252C%2520we%2520introduce%2520a%2520multidimensional%2520assessment%2520framework%2520that%2520compares%250Agenerative%2520interfaces%2520with%2520traditional%2520chat-based%2520ones%2520across%2520diverse%2520tasks%252C%250Ainteraction%2520patterns%252C%2520and%2520query%2520types%252C%2520capturing%2520functional%252C%2520interactive%252C%2520and%250Aemotional%2520aspects%2520of%2520user%2520experience.%2520Results%2520show%2520that%2520generative%2520interfaces%250Aconsistently%2520outperform%2520conversational%2520ones%252C%2520with%2520humans%2520preferring%2520them%2520in%250Aover%252070%2525%2520of%2520cases.%2520These%2520findings%2520clarify%2520when%2520and%2520why%2520users%2520favor%2520generative%250Ainterfaces%252C%2520paving%2520the%2520way%2520for%2520future%2520advancements%2520in%2520human-AI%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Interfaces%20for%20Language%20Models&entry.906535625=Jiaqi%20Chen%20and%20Yanzhe%20Zhang%20and%20Yutong%20Zhang%20and%20Yijia%20Shao%20and%20Diyi%20Yang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20seen%20as%20assistants%2C%20copilots%2C%0Aand%20consultants%2C%20capable%20of%20supporting%20a%20wide%20range%20of%20tasks%20through%20natural%0Aconversation.%20However%2C%20most%20systems%20remain%20constrained%20by%20a%20linear%0Arequest-response%20format%20that%20often%20makes%20interactions%20inefficient%20in%0Amulti-turn%2C%20information-dense%2C%20and%20exploratory%20tasks.%20To%20address%20these%0Alimitations%2C%20we%20propose%20Generative%20Interfaces%20for%20Language%20Models%2C%20a%20paradigm%0Ain%20which%20LLMs%20respond%20to%20user%20queries%20by%20proactively%20generating%20user%20interfaces%0A%28UIs%29%20that%20enable%20more%20adaptive%20and%20interactive%20engagement.%20Our%20framework%0Aleverages%20structured%20interface-specific%20representations%20and%20iterative%0Arefinements%20to%20translate%20user%20queries%20into%20task-specific%20UIs.%20For%20systematic%0Aevaluation%2C%20we%20introduce%20a%20multidimensional%20assessment%20framework%20that%20compares%0Agenerative%20interfaces%20with%20traditional%20chat-based%20ones%20across%20diverse%20tasks%2C%0Ainteraction%20patterns%2C%20and%20query%20types%2C%20capturing%20functional%2C%20interactive%2C%20and%0Aemotional%20aspects%20of%20user%20experience.%20Results%20show%20that%20generative%20interfaces%0Aconsistently%20outperform%20conversational%20ones%2C%20with%20humans%20preferring%20them%20in%0Aover%2070%25%20of%20cases.%20These%20findings%20clarify%20when%20and%20why%20users%20favor%20generative%0Ainterfaces%2C%20paving%20the%20way%20for%20future%20advancements%20in%20human-AI%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19227v1&entry.124074799=Read"},
{"title": "PointFix: Learning to Fix Domain Bias for Robust Online Stereo\n  Adaptation", "author": "Kwonyoung Kim and Jungin Park and Jiyoung Lee and Dongbo Min and Kwanghoon Sohn", "abstract": "  Online stereo adaptation tackles the domain shift problem, caused by\ndifferent environments between synthetic (training) and real (test) datasets,\nto promptly adapt stereo models in dynamic real-world applications such as\nautonomous driving. However, previous methods often fail to counteract\nparticular regions related to dynamic objects with more severe environmental\nchanges. To mitigate this issue, we propose to incorporate an auxiliary\npoint-selective network into a meta-learning framework, called PointFix, to\nprovide a robust initialization of stereo models for online stereo adaptation.\nIn a nutshell, our auxiliary network learns to fix local variants intensively\nby effectively back-propagating local information through the meta-gradient for\nthe robust initialization of the baseline model. This network is\nmodel-agnostic, so can be used in any kind of architectures in a plug-and-play\nmanner. We conduct extensive experiments to verify the effectiveness of our\nmethod under three adaptation settings such as short-, mid-, and long-term\nsequences. Experimental results show that the proper initialization of the base\nstereo model by the auxiliary network enables our learning paradigm to achieve\nstate-of-the-art performance at inference.\n", "link": "http://arxiv.org/abs/2207.13340v2", "date": "2025-08-26", "relevancy": 2.7663, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5384}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointFix%3A%20Learning%20to%20Fix%20Domain%20Bias%20for%20Robust%20Online%20Stereo%0A%20%20Adaptation&body=Title%3A%20PointFix%3A%20Learning%20to%20Fix%20Domain%20Bias%20for%20Robust%20Online%20Stereo%0A%20%20Adaptation%0AAuthor%3A%20Kwonyoung%20Kim%20and%20Jungin%20Park%20and%20Jiyoung%20Lee%20and%20Dongbo%20Min%20and%20Kwanghoon%20Sohn%0AAbstract%3A%20%20%20Online%20stereo%20adaptation%20tackles%20the%20domain%20shift%20problem%2C%20caused%20by%0Adifferent%20environments%20between%20synthetic%20%28training%29%20and%20real%20%28test%29%20datasets%2C%0Ato%20promptly%20adapt%20stereo%20models%20in%20dynamic%20real-world%20applications%20such%20as%0Aautonomous%20driving.%20However%2C%20previous%20methods%20often%20fail%20to%20counteract%0Aparticular%20regions%20related%20to%20dynamic%20objects%20with%20more%20severe%20environmental%0Achanges.%20To%20mitigate%20this%20issue%2C%20we%20propose%20to%20incorporate%20an%20auxiliary%0Apoint-selective%20network%20into%20a%20meta-learning%20framework%2C%20called%20PointFix%2C%20to%0Aprovide%20a%20robust%20initialization%20of%20stereo%20models%20for%20online%20stereo%20adaptation.%0AIn%20a%20nutshell%2C%20our%20auxiliary%20network%20learns%20to%20fix%20local%20variants%20intensively%0Aby%20effectively%20back-propagating%20local%20information%20through%20the%20meta-gradient%20for%0Athe%20robust%20initialization%20of%20the%20baseline%20model.%20This%20network%20is%0Amodel-agnostic%2C%20so%20can%20be%20used%20in%20any%20kind%20of%20architectures%20in%20a%20plug-and-play%0Amanner.%20We%20conduct%20extensive%20experiments%20to%20verify%20the%20effectiveness%20of%20our%0Amethod%20under%20three%20adaptation%20settings%20such%20as%20short-%2C%20mid-%2C%20and%20long-term%0Asequences.%20Experimental%20results%20show%20that%20the%20proper%20initialization%20of%20the%20base%0Astereo%20model%20by%20the%20auxiliary%20network%20enables%20our%20learning%20paradigm%20to%20achieve%0Astate-of-the-art%20performance%20at%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.13340v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointFix%253A%2520Learning%2520to%2520Fix%2520Domain%2520Bias%2520for%2520Robust%2520Online%2520Stereo%250A%2520%2520Adaptation%26entry.906535625%3DKwonyoung%2520Kim%2520and%2520Jungin%2520Park%2520and%2520Jiyoung%2520Lee%2520and%2520Dongbo%2520Min%2520and%2520Kwanghoon%2520Sohn%26entry.1292438233%3D%2520%2520Online%2520stereo%2520adaptation%2520tackles%2520the%2520domain%2520shift%2520problem%252C%2520caused%2520by%250Adifferent%2520environments%2520between%2520synthetic%2520%2528training%2529%2520and%2520real%2520%2528test%2529%2520datasets%252C%250Ato%2520promptly%2520adapt%2520stereo%2520models%2520in%2520dynamic%2520real-world%2520applications%2520such%2520as%250Aautonomous%2520driving.%2520However%252C%2520previous%2520methods%2520often%2520fail%2520to%2520counteract%250Aparticular%2520regions%2520related%2520to%2520dynamic%2520objects%2520with%2520more%2520severe%2520environmental%250Achanges.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520propose%2520to%2520incorporate%2520an%2520auxiliary%250Apoint-selective%2520network%2520into%2520a%2520meta-learning%2520framework%252C%2520called%2520PointFix%252C%2520to%250Aprovide%2520a%2520robust%2520initialization%2520of%2520stereo%2520models%2520for%2520online%2520stereo%2520adaptation.%250AIn%2520a%2520nutshell%252C%2520our%2520auxiliary%2520network%2520learns%2520to%2520fix%2520local%2520variants%2520intensively%250Aby%2520effectively%2520back-propagating%2520local%2520information%2520through%2520the%2520meta-gradient%2520for%250Athe%2520robust%2520initialization%2520of%2520the%2520baseline%2520model.%2520This%2520network%2520is%250Amodel-agnostic%252C%2520so%2520can%2520be%2520used%2520in%2520any%2520kind%2520of%2520architectures%2520in%2520a%2520plug-and-play%250Amanner.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520verify%2520the%2520effectiveness%2520of%2520our%250Amethod%2520under%2520three%2520adaptation%2520settings%2520such%2520as%2520short-%252C%2520mid-%252C%2520and%2520long-term%250Asequences.%2520Experimental%2520results%2520show%2520that%2520the%2520proper%2520initialization%2520of%2520the%2520base%250Astereo%2520model%2520by%2520the%2520auxiliary%2520network%2520enables%2520our%2520learning%2520paradigm%2520to%2520achieve%250Astate-of-the-art%2520performance%2520at%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.13340v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointFix%3A%20Learning%20to%20Fix%20Domain%20Bias%20for%20Robust%20Online%20Stereo%0A%20%20Adaptation&entry.906535625=Kwonyoung%20Kim%20and%20Jungin%20Park%20and%20Jiyoung%20Lee%20and%20Dongbo%20Min%20and%20Kwanghoon%20Sohn&entry.1292438233=%20%20Online%20stereo%20adaptation%20tackles%20the%20domain%20shift%20problem%2C%20caused%20by%0Adifferent%20environments%20between%20synthetic%20%28training%29%20and%20real%20%28test%29%20datasets%2C%0Ato%20promptly%20adapt%20stereo%20models%20in%20dynamic%20real-world%20applications%20such%20as%0Aautonomous%20driving.%20However%2C%20previous%20methods%20often%20fail%20to%20counteract%0Aparticular%20regions%20related%20to%20dynamic%20objects%20with%20more%20severe%20environmental%0Achanges.%20To%20mitigate%20this%20issue%2C%20we%20propose%20to%20incorporate%20an%20auxiliary%0Apoint-selective%20network%20into%20a%20meta-learning%20framework%2C%20called%20PointFix%2C%20to%0Aprovide%20a%20robust%20initialization%20of%20stereo%20models%20for%20online%20stereo%20adaptation.%0AIn%20a%20nutshell%2C%20our%20auxiliary%20network%20learns%20to%20fix%20local%20variants%20intensively%0Aby%20effectively%20back-propagating%20local%20information%20through%20the%20meta-gradient%20for%0Athe%20robust%20initialization%20of%20the%20baseline%20model.%20This%20network%20is%0Amodel-agnostic%2C%20so%20can%20be%20used%20in%20any%20kind%20of%20architectures%20in%20a%20plug-and-play%0Amanner.%20We%20conduct%20extensive%20experiments%20to%20verify%20the%20effectiveness%20of%20our%0Amethod%20under%20three%20adaptation%20settings%20such%20as%20short-%2C%20mid-%2C%20and%20long-term%0Asequences.%20Experimental%20results%20show%20that%20the%20proper%20initialization%20of%20the%20base%0Astereo%20model%20by%20the%20auxiliary%20network%20enables%20our%20learning%20paradigm%20to%20achieve%0Astate-of-the-art%20performance%20at%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.13340v2&entry.124074799=Read"},
{"title": "TL-Training: A Task-Feature-Based Framework for Training Large Language\n  Models in Tool Use", "author": "Junjie Ye and Yilong Wu and Sixian Li and Yuming Yang and Zhiheng Xi and Tao Gui and Qi Zhang and Xuanjing Huang and Peng Wang and Zhongchao Shi and Jianping Fan and Zhengyin Du", "abstract": "  Large language models (LLMs) achieve remarkable advancements by leveraging\ntools to interact with environments, a critical step toward generalized AI.\nHowever, the standard supervised fine-tuning (SFT) approach, which relies on\nlarge-scale datasets, often overlooks task-specific characteristics in tool\nuse, leading to performance bottlenecks. To address this issue, we analyze\nthree existing LLMs and uncover key insights: training data can inadvertently\nimpede tool-use behavior, token importance is distributed unevenly, and errors\nin tool calls fall into a small set of categories. Building on these findings,\nwe propose~\\emph{TL-Training}, a task-feature-based framework that mitigates\nthe effects of suboptimal training data, dynamically adjusts token weights to\nprioritize key tokens during SFT, and incorporates a robust reward mechanism\ntailored to error categories, optimized through proximal policy optimization.\nWe validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four\nopen-source test sets. Our results demonstrate that the LLM trained by our\nmethod matches or surpasses both open- and closed-source LLMs in tool-use\nperformance using only 1,217 training data points. Additionally, our method\nenhances robustness in noisy environments and improves general task\nperformance, offering a scalable and efficient paradigm for tool-use training\nin LLMs. Code and data are available at\nhttps://github.com/Junjie-Ye/TL-Training.\n", "link": "http://arxiv.org/abs/2412.15495v2", "date": "2025-08-26", "relevancy": 2.7356, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TL-Training%3A%20A%20Task-Feature-Based%20Framework%20for%20Training%20Large%20Language%0A%20%20Models%20in%20Tool%20Use&body=Title%3A%20TL-Training%3A%20A%20Task-Feature-Based%20Framework%20for%20Training%20Large%20Language%0A%20%20Models%20in%20Tool%20Use%0AAuthor%3A%20Junjie%20Ye%20and%20Yilong%20Wu%20and%20Sixian%20Li%20and%20Yuming%20Yang%20and%20Zhiheng%20Xi%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%20and%20Peng%20Wang%20and%20Zhongchao%20Shi%20and%20Jianping%20Fan%20and%20Zhengyin%20Du%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20achieve%20remarkable%20advancements%20by%20leveraging%0Atools%20to%20interact%20with%20environments%2C%20a%20critical%20step%20toward%20generalized%20AI.%0AHowever%2C%20the%20standard%20supervised%20fine-tuning%20%28SFT%29%20approach%2C%20which%20relies%20on%0Alarge-scale%20datasets%2C%20often%20overlooks%20task-specific%20characteristics%20in%20tool%0Ause%2C%20leading%20to%20performance%20bottlenecks.%20To%20address%20this%20issue%2C%20we%20analyze%0Athree%20existing%20LLMs%20and%20uncover%20key%20insights%3A%20training%20data%20can%20inadvertently%0Aimpede%20tool-use%20behavior%2C%20token%20importance%20is%20distributed%20unevenly%2C%20and%20errors%0Ain%20tool%20calls%20fall%20into%20a%20small%20set%20of%20categories.%20Building%20on%20these%20findings%2C%0Awe%20propose~%5Cemph%7BTL-Training%7D%2C%20a%20task-feature-based%20framework%20that%20mitigates%0Athe%20effects%20of%20suboptimal%20training%20data%2C%20dynamically%20adjusts%20token%20weights%20to%0Aprioritize%20key%20tokens%20during%20SFT%2C%20and%20incorporates%20a%20robust%20reward%20mechanism%0Atailored%20to%20error%20categories%2C%20optimized%20through%20proximal%20policy%20optimization.%0AWe%20validate%20TL-Training%20by%20training%20CodeLLaMA-2-7B%20and%20evaluating%20it%20on%20four%0Aopen-source%20test%20sets.%20Our%20results%20demonstrate%20that%20the%20LLM%20trained%20by%20our%0Amethod%20matches%20or%20surpasses%20both%20open-%20and%20closed-source%20LLMs%20in%20tool-use%0Aperformance%20using%20only%201%2C217%20training%20data%20points.%20Additionally%2C%20our%20method%0Aenhances%20robustness%20in%20noisy%20environments%20and%20improves%20general%20task%0Aperformance%2C%20offering%20a%20scalable%20and%20efficient%20paradigm%20for%20tool-use%20training%0Ain%20LLMs.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Junjie-Ye/TL-Training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTL-Training%253A%2520A%2520Task-Feature-Based%2520Framework%2520for%2520Training%2520Large%2520Language%250A%2520%2520Models%2520in%2520Tool%2520Use%26entry.906535625%3DJunjie%2520Ye%2520and%2520Yilong%2520Wu%2520and%2520Sixian%2520Li%2520and%2520Yuming%2520Yang%2520and%2520Zhiheng%2520Xi%2520and%2520Tao%2520Gui%2520and%2520Qi%2520Zhang%2520and%2520Xuanjing%2520Huang%2520and%2520Peng%2520Wang%2520and%2520Zhongchao%2520Shi%2520and%2520Jianping%2520Fan%2520and%2520Zhengyin%2520Du%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520remarkable%2520advancements%2520by%2520leveraging%250Atools%2520to%2520interact%2520with%2520environments%252C%2520a%2520critical%2520step%2520toward%2520generalized%2520AI.%250AHowever%252C%2520the%2520standard%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520approach%252C%2520which%2520relies%2520on%250Alarge-scale%2520datasets%252C%2520often%2520overlooks%2520task-specific%2520characteristics%2520in%2520tool%250Ause%252C%2520leading%2520to%2520performance%2520bottlenecks.%2520To%2520address%2520this%2520issue%252C%2520we%2520analyze%250Athree%2520existing%2520LLMs%2520and%2520uncover%2520key%2520insights%253A%2520training%2520data%2520can%2520inadvertently%250Aimpede%2520tool-use%2520behavior%252C%2520token%2520importance%2520is%2520distributed%2520unevenly%252C%2520and%2520errors%250Ain%2520tool%2520calls%2520fall%2520into%2520a%2520small%2520set%2520of%2520categories.%2520Building%2520on%2520these%2520findings%252C%250Awe%2520propose~%255Cemph%257BTL-Training%257D%252C%2520a%2520task-feature-based%2520framework%2520that%2520mitigates%250Athe%2520effects%2520of%2520suboptimal%2520training%2520data%252C%2520dynamically%2520adjusts%2520token%2520weights%2520to%250Aprioritize%2520key%2520tokens%2520during%2520SFT%252C%2520and%2520incorporates%2520a%2520robust%2520reward%2520mechanism%250Atailored%2520to%2520error%2520categories%252C%2520optimized%2520through%2520proximal%2520policy%2520optimization.%250AWe%2520validate%2520TL-Training%2520by%2520training%2520CodeLLaMA-2-7B%2520and%2520evaluating%2520it%2520on%2520four%250Aopen-source%2520test%2520sets.%2520Our%2520results%2520demonstrate%2520that%2520the%2520LLM%2520trained%2520by%2520our%250Amethod%2520matches%2520or%2520surpasses%2520both%2520open-%2520and%2520closed-source%2520LLMs%2520in%2520tool-use%250Aperformance%2520using%2520only%25201%252C217%2520training%2520data%2520points.%2520Additionally%252C%2520our%2520method%250Aenhances%2520robustness%2520in%2520noisy%2520environments%2520and%2520improves%2520general%2520task%250Aperformance%252C%2520offering%2520a%2520scalable%2520and%2520efficient%2520paradigm%2520for%2520tool-use%2520training%250Ain%2520LLMs.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/Junjie-Ye/TL-Training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TL-Training%3A%20A%20Task-Feature-Based%20Framework%20for%20Training%20Large%20Language%0A%20%20Models%20in%20Tool%20Use&entry.906535625=Junjie%20Ye%20and%20Yilong%20Wu%20and%20Sixian%20Li%20and%20Yuming%20Yang%20and%20Zhiheng%20Xi%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%20and%20Peng%20Wang%20and%20Zhongchao%20Shi%20and%20Jianping%20Fan%20and%20Zhengyin%20Du&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20achieve%20remarkable%20advancements%20by%20leveraging%0Atools%20to%20interact%20with%20environments%2C%20a%20critical%20step%20toward%20generalized%20AI.%0AHowever%2C%20the%20standard%20supervised%20fine-tuning%20%28SFT%29%20approach%2C%20which%20relies%20on%0Alarge-scale%20datasets%2C%20often%20overlooks%20task-specific%20characteristics%20in%20tool%0Ause%2C%20leading%20to%20performance%20bottlenecks.%20To%20address%20this%20issue%2C%20we%20analyze%0Athree%20existing%20LLMs%20and%20uncover%20key%20insights%3A%20training%20data%20can%20inadvertently%0Aimpede%20tool-use%20behavior%2C%20token%20importance%20is%20distributed%20unevenly%2C%20and%20errors%0Ain%20tool%20calls%20fall%20into%20a%20small%20set%20of%20categories.%20Building%20on%20these%20findings%2C%0Awe%20propose~%5Cemph%7BTL-Training%7D%2C%20a%20task-feature-based%20framework%20that%20mitigates%0Athe%20effects%20of%20suboptimal%20training%20data%2C%20dynamically%20adjusts%20token%20weights%20to%0Aprioritize%20key%20tokens%20during%20SFT%2C%20and%20incorporates%20a%20robust%20reward%20mechanism%0Atailored%20to%20error%20categories%2C%20optimized%20through%20proximal%20policy%20optimization.%0AWe%20validate%20TL-Training%20by%20training%20CodeLLaMA-2-7B%20and%20evaluating%20it%20on%20four%0Aopen-source%20test%20sets.%20Our%20results%20demonstrate%20that%20the%20LLM%20trained%20by%20our%0Amethod%20matches%20or%20surpasses%20both%20open-%20and%20closed-source%20LLMs%20in%20tool-use%0Aperformance%20using%20only%201%2C217%20training%20data%20points.%20Additionally%2C%20our%20method%0Aenhances%20robustness%20in%20noisy%20environments%20and%20improves%20general%20task%0Aperformance%2C%20offering%20a%20scalable%20and%20efficient%20paradigm%20for%20tool-use%20training%0Ain%20LLMs.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Junjie-Ye/TL-Training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15495v2&entry.124074799=Read"},
{"title": "Enhancing Document VQA Models via Retrieval-Augmented Generation", "author": "Eric L\u00f3pez and Artemis Llabr\u00e9s and Ernest Valveny", "abstract": "  Document Visual Question Answering (Document VQA) must cope with documents\nthat span dozens of pages, yet leading systems still concatenate every page or\nrely on very large vision-language models, both of which are memory-hungry.\nRetrieval-Augmented Generation (RAG) offers an attractive alternative, first\nretrieving a concise set of relevant segments before generating answers from\nthis selected evidence. In this paper, we systematically evaluate the impact of\nincorporating RAG into Document VQA through different retrieval variants -\ntext-based retrieval using OCR tokens and purely visual retrieval without OCR -\nacross multiple models and benchmarks. Evaluated on the multi-page datasets\nMP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the\n\"concatenate-all-pages\" baseline by up to +22.5 ANLS, while the visual variant\nachieves +5.0 ANLS improvement without requiring any text extraction. An\nablation confirms that retrieval and reranking components drive most of the\ngain, whereas the layout-guided chunking strategy - proposed in several recent\nworks to leverage page structure - fails to help on these datasets. Our\nexperiments demonstrate that careful evidence selection consistently boosts\naccuracy across multiple model sizes and multi-page benchmarks, underscoring\nits practical value for real-world Document VQA.\n", "link": "http://arxiv.org/abs/2508.18984v1", "date": "2025-08-26", "relevancy": 2.6907, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Document%20VQA%20Models%20via%20Retrieval-Augmented%20Generation&body=Title%3A%20Enhancing%20Document%20VQA%20Models%20via%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Eric%20L%C3%B3pez%20and%20Artemis%20Llabr%C3%A9s%20and%20Ernest%20Valveny%0AAbstract%3A%20%20%20Document%20Visual%20Question%20Answering%20%28Document%20VQA%29%20must%20cope%20with%20documents%0Athat%20span%20dozens%20of%20pages%2C%20yet%20leading%20systems%20still%20concatenate%20every%20page%20or%0Arely%20on%20very%20large%20vision-language%20models%2C%20both%20of%20which%20are%20memory-hungry.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20offers%20an%20attractive%20alternative%2C%20first%0Aretrieving%20a%20concise%20set%20of%20relevant%20segments%20before%20generating%20answers%20from%0Athis%20selected%20evidence.%20In%20this%20paper%2C%20we%20systematically%20evaluate%20the%20impact%20of%0Aincorporating%20RAG%20into%20Document%20VQA%20through%20different%20retrieval%20variants%20-%0Atext-based%20retrieval%20using%20OCR%20tokens%20and%20purely%20visual%20retrieval%20without%20OCR%20-%0Aacross%20multiple%20models%20and%20benchmarks.%20Evaluated%20on%20the%20multi-page%20datasets%0AMP-DocVQA%2C%20DUDE%2C%20and%20InfographicVQA%2C%20the%20text-centric%20variant%20improves%20the%0A%22concatenate-all-pages%22%20baseline%20by%20up%20to%20%2B22.5%20ANLS%2C%20while%20the%20visual%20variant%0Aachieves%20%2B5.0%20ANLS%20improvement%20without%20requiring%20any%20text%20extraction.%20An%0Aablation%20confirms%20that%20retrieval%20and%20reranking%20components%20drive%20most%20of%20the%0Again%2C%20whereas%20the%20layout-guided%20chunking%20strategy%20-%20proposed%20in%20several%20recent%0Aworks%20to%20leverage%20page%20structure%20-%20fails%20to%20help%20on%20these%20datasets.%20Our%0Aexperiments%20demonstrate%20that%20careful%20evidence%20selection%20consistently%20boosts%0Aaccuracy%20across%20multiple%20model%20sizes%20and%20multi-page%20benchmarks%2C%20underscoring%0Aits%20practical%20value%20for%20real-world%20Document%20VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Document%2520VQA%2520Models%2520via%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DEric%2520L%25C3%25B3pez%2520and%2520Artemis%2520Llabr%25C3%25A9s%2520and%2520Ernest%2520Valveny%26entry.1292438233%3D%2520%2520Document%2520Visual%2520Question%2520Answering%2520%2528Document%2520VQA%2529%2520must%2520cope%2520with%2520documents%250Athat%2520span%2520dozens%2520of%2520pages%252C%2520yet%2520leading%2520systems%2520still%2520concatenate%2520every%2520page%2520or%250Arely%2520on%2520very%2520large%2520vision-language%2520models%252C%2520both%2520of%2520which%2520are%2520memory-hungry.%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520offers%2520an%2520attractive%2520alternative%252C%2520first%250Aretrieving%2520a%2520concise%2520set%2520of%2520relevant%2520segments%2520before%2520generating%2520answers%2520from%250Athis%2520selected%2520evidence.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520evaluate%2520the%2520impact%2520of%250Aincorporating%2520RAG%2520into%2520Document%2520VQA%2520through%2520different%2520retrieval%2520variants%2520-%250Atext-based%2520retrieval%2520using%2520OCR%2520tokens%2520and%2520purely%2520visual%2520retrieval%2520without%2520OCR%2520-%250Aacross%2520multiple%2520models%2520and%2520benchmarks.%2520Evaluated%2520on%2520the%2520multi-page%2520datasets%250AMP-DocVQA%252C%2520DUDE%252C%2520and%2520InfographicVQA%252C%2520the%2520text-centric%2520variant%2520improves%2520the%250A%2522concatenate-all-pages%2522%2520baseline%2520by%2520up%2520to%2520%252B22.5%2520ANLS%252C%2520while%2520the%2520visual%2520variant%250Aachieves%2520%252B5.0%2520ANLS%2520improvement%2520without%2520requiring%2520any%2520text%2520extraction.%2520An%250Aablation%2520confirms%2520that%2520retrieval%2520and%2520reranking%2520components%2520drive%2520most%2520of%2520the%250Again%252C%2520whereas%2520the%2520layout-guided%2520chunking%2520strategy%2520-%2520proposed%2520in%2520several%2520recent%250Aworks%2520to%2520leverage%2520page%2520structure%2520-%2520fails%2520to%2520help%2520on%2520these%2520datasets.%2520Our%250Aexperiments%2520demonstrate%2520that%2520careful%2520evidence%2520selection%2520consistently%2520boosts%250Aaccuracy%2520across%2520multiple%2520model%2520sizes%2520and%2520multi-page%2520benchmarks%252C%2520underscoring%250Aits%2520practical%2520value%2520for%2520real-world%2520Document%2520VQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Document%20VQA%20Models%20via%20Retrieval-Augmented%20Generation&entry.906535625=Eric%20L%C3%B3pez%20and%20Artemis%20Llabr%C3%A9s%20and%20Ernest%20Valveny&entry.1292438233=%20%20Document%20Visual%20Question%20Answering%20%28Document%20VQA%29%20must%20cope%20with%20documents%0Athat%20span%20dozens%20of%20pages%2C%20yet%20leading%20systems%20still%20concatenate%20every%20page%20or%0Arely%20on%20very%20large%20vision-language%20models%2C%20both%20of%20which%20are%20memory-hungry.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20offers%20an%20attractive%20alternative%2C%20first%0Aretrieving%20a%20concise%20set%20of%20relevant%20segments%20before%20generating%20answers%20from%0Athis%20selected%20evidence.%20In%20this%20paper%2C%20we%20systematically%20evaluate%20the%20impact%20of%0Aincorporating%20RAG%20into%20Document%20VQA%20through%20different%20retrieval%20variants%20-%0Atext-based%20retrieval%20using%20OCR%20tokens%20and%20purely%20visual%20retrieval%20without%20OCR%20-%0Aacross%20multiple%20models%20and%20benchmarks.%20Evaluated%20on%20the%20multi-page%20datasets%0AMP-DocVQA%2C%20DUDE%2C%20and%20InfographicVQA%2C%20the%20text-centric%20variant%20improves%20the%0A%22concatenate-all-pages%22%20baseline%20by%20up%20to%20%2B22.5%20ANLS%2C%20while%20the%20visual%20variant%0Aachieves%20%2B5.0%20ANLS%20improvement%20without%20requiring%20any%20text%20extraction.%20An%0Aablation%20confirms%20that%20retrieval%20and%20reranking%20components%20drive%20most%20of%20the%0Again%2C%20whereas%20the%20layout-guided%20chunking%20strategy%20-%20proposed%20in%20several%20recent%0Aworks%20to%20leverage%20page%20structure%20-%20fails%20to%20help%20on%20these%20datasets.%20Our%0Aexperiments%20demonstrate%20that%20careful%20evidence%20selection%20consistently%20boosts%0Aaccuracy%20across%20multiple%20model%20sizes%20and%20multi-page%20benchmarks%2C%20underscoring%0Aits%20practical%20value%20for%20real-world%20Document%20VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18984v1&entry.124074799=Read"},
{"title": "Understanding Tool-Integrated Reasoning", "author": "Heng Lin and Zhongwen Xu", "abstract": "  We study why Tool-Integrated Reasoning (TIR) makes Large Language Models\n(LLMs) more capable. While LLMs integrated with tools like Python code\ninterpreters show great promise, a principled theory explaining why this\nparadigm is effective has been missing. This work provides the first formal\nproof that TIR fundamentally expands an LLM's capabilities. We demonstrate that\ntools enable a strict expansion of the model's empirical and feasible support,\nbreaking the capability ceiling of pure-text models by unlocking\nproblem-solving strategies that are otherwise impossible or intractably\nverbose. To guide model behavior without compromising training stability and\nperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), a\nnovel algorithm that directly modifies the advantage function to guide the\npolicy behavior. We conduct comprehensive experiments on challenging\nmathematical benchmarks, leveraging a Python interpreter as the external tool.\nOur results show that the TIR model decisively outperforms its pure-text\ncounterpart on the pass@k metric. Crucially, this advantage is not confined to\ncomputationally-intensive problems but extends to those requiring significant\nabstract insight. We further identify the emergent cognitive patterns that\nillustrate how models learn to think with tools. Finally, we report improved\ntool usage behavior with early code invocation and much more interactive turns\nwith ASPO. Overall, our work provides the first principled explanation for\nTIR's success, shifting the focus from the mere fact that tools work to why and\nhow they enable more powerful reasoning.\n", "link": "http://arxiv.org/abs/2508.19201v1", "date": "2025-08-26", "relevancy": 2.6631, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Tool-Integrated%20Reasoning&body=Title%3A%20Understanding%20Tool-Integrated%20Reasoning%0AAuthor%3A%20Heng%20Lin%20and%20Zhongwen%20Xu%0AAbstract%3A%20%20%20We%20study%20why%20Tool-Integrated%20Reasoning%20%28TIR%29%20makes%20Large%20Language%20Models%0A%28LLMs%29%20more%20capable.%20While%20LLMs%20integrated%20with%20tools%20like%20Python%20code%0Ainterpreters%20show%20great%20promise%2C%20a%20principled%20theory%20explaining%20why%20this%0Aparadigm%20is%20effective%20has%20been%20missing.%20This%20work%20provides%20the%20first%20formal%0Aproof%20that%20TIR%20fundamentally%20expands%20an%20LLM%27s%20capabilities.%20We%20demonstrate%20that%0Atools%20enable%20a%20strict%20expansion%20of%20the%20model%27s%20empirical%20and%20feasible%20support%2C%0Abreaking%20the%20capability%20ceiling%20of%20pure-text%20models%20by%20unlocking%0Aproblem-solving%20strategies%20that%20are%20otherwise%20impossible%20or%20intractably%0Averbose.%20To%20guide%20model%20behavior%20without%20compromising%20training%20stability%20and%0Aperformance%2C%20we%20also%20introduce%20Advantage%20Shaping%20Policy%20Optimization%20%28ASPO%29%2C%20a%0Anovel%20algorithm%20that%20directly%20modifies%20the%20advantage%20function%20to%20guide%20the%0Apolicy%20behavior.%20We%20conduct%20comprehensive%20experiments%20on%20challenging%0Amathematical%20benchmarks%2C%20leveraging%20a%20Python%20interpreter%20as%20the%20external%20tool.%0AOur%20results%20show%20that%20the%20TIR%20model%20decisively%20outperforms%20its%20pure-text%0Acounterpart%20on%20the%20pass%40k%20metric.%20Crucially%2C%20this%20advantage%20is%20not%20confined%20to%0Acomputationally-intensive%20problems%20but%20extends%20to%20those%20requiring%20significant%0Aabstract%20insight.%20We%20further%20identify%20the%20emergent%20cognitive%20patterns%20that%0Aillustrate%20how%20models%20learn%20to%20think%20with%20tools.%20Finally%2C%20we%20report%20improved%0Atool%20usage%20behavior%20with%20early%20code%20invocation%20and%20much%20more%20interactive%20turns%0Awith%20ASPO.%20Overall%2C%20our%20work%20provides%20the%20first%20principled%20explanation%20for%0ATIR%27s%20success%2C%20shifting%20the%20focus%20from%20the%20mere%20fact%20that%20tools%20work%20to%20why%20and%0Ahow%20they%20enable%20more%20powerful%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Tool-Integrated%2520Reasoning%26entry.906535625%3DHeng%2520Lin%2520and%2520Zhongwen%2520Xu%26entry.1292438233%3D%2520%2520We%2520study%2520why%2520Tool-Integrated%2520Reasoning%2520%2528TIR%2529%2520makes%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520more%2520capable.%2520While%2520LLMs%2520integrated%2520with%2520tools%2520like%2520Python%2520code%250Ainterpreters%2520show%2520great%2520promise%252C%2520a%2520principled%2520theory%2520explaining%2520why%2520this%250Aparadigm%2520is%2520effective%2520has%2520been%2520missing.%2520This%2520work%2520provides%2520the%2520first%2520formal%250Aproof%2520that%2520TIR%2520fundamentally%2520expands%2520an%2520LLM%2527s%2520capabilities.%2520We%2520demonstrate%2520that%250Atools%2520enable%2520a%2520strict%2520expansion%2520of%2520the%2520model%2527s%2520empirical%2520and%2520feasible%2520support%252C%250Abreaking%2520the%2520capability%2520ceiling%2520of%2520pure-text%2520models%2520by%2520unlocking%250Aproblem-solving%2520strategies%2520that%2520are%2520otherwise%2520impossible%2520or%2520intractably%250Averbose.%2520To%2520guide%2520model%2520behavior%2520without%2520compromising%2520training%2520stability%2520and%250Aperformance%252C%2520we%2520also%2520introduce%2520Advantage%2520Shaping%2520Policy%2520Optimization%2520%2528ASPO%2529%252C%2520a%250Anovel%2520algorithm%2520that%2520directly%2520modifies%2520the%2520advantage%2520function%2520to%2520guide%2520the%250Apolicy%2520behavior.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520challenging%250Amathematical%2520benchmarks%252C%2520leveraging%2520a%2520Python%2520interpreter%2520as%2520the%2520external%2520tool.%250AOur%2520results%2520show%2520that%2520the%2520TIR%2520model%2520decisively%2520outperforms%2520its%2520pure-text%250Acounterpart%2520on%2520the%2520pass%2540k%2520metric.%2520Crucially%252C%2520this%2520advantage%2520is%2520not%2520confined%2520to%250Acomputationally-intensive%2520problems%2520but%2520extends%2520to%2520those%2520requiring%2520significant%250Aabstract%2520insight.%2520We%2520further%2520identify%2520the%2520emergent%2520cognitive%2520patterns%2520that%250Aillustrate%2520how%2520models%2520learn%2520to%2520think%2520with%2520tools.%2520Finally%252C%2520we%2520report%2520improved%250Atool%2520usage%2520behavior%2520with%2520early%2520code%2520invocation%2520and%2520much%2520more%2520interactive%2520turns%250Awith%2520ASPO.%2520Overall%252C%2520our%2520work%2520provides%2520the%2520first%2520principled%2520explanation%2520for%250ATIR%2527s%2520success%252C%2520shifting%2520the%2520focus%2520from%2520the%2520mere%2520fact%2520that%2520tools%2520work%2520to%2520why%2520and%250Ahow%2520they%2520enable%2520more%2520powerful%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Tool-Integrated%20Reasoning&entry.906535625=Heng%20Lin%20and%20Zhongwen%20Xu&entry.1292438233=%20%20We%20study%20why%20Tool-Integrated%20Reasoning%20%28TIR%29%20makes%20Large%20Language%20Models%0A%28LLMs%29%20more%20capable.%20While%20LLMs%20integrated%20with%20tools%20like%20Python%20code%0Ainterpreters%20show%20great%20promise%2C%20a%20principled%20theory%20explaining%20why%20this%0Aparadigm%20is%20effective%20has%20been%20missing.%20This%20work%20provides%20the%20first%20formal%0Aproof%20that%20TIR%20fundamentally%20expands%20an%20LLM%27s%20capabilities.%20We%20demonstrate%20that%0Atools%20enable%20a%20strict%20expansion%20of%20the%20model%27s%20empirical%20and%20feasible%20support%2C%0Abreaking%20the%20capability%20ceiling%20of%20pure-text%20models%20by%20unlocking%0Aproblem-solving%20strategies%20that%20are%20otherwise%20impossible%20or%20intractably%0Averbose.%20To%20guide%20model%20behavior%20without%20compromising%20training%20stability%20and%0Aperformance%2C%20we%20also%20introduce%20Advantage%20Shaping%20Policy%20Optimization%20%28ASPO%29%2C%20a%0Anovel%20algorithm%20that%20directly%20modifies%20the%20advantage%20function%20to%20guide%20the%0Apolicy%20behavior.%20We%20conduct%20comprehensive%20experiments%20on%20challenging%0Amathematical%20benchmarks%2C%20leveraging%20a%20Python%20interpreter%20as%20the%20external%20tool.%0AOur%20results%20show%20that%20the%20TIR%20model%20decisively%20outperforms%20its%20pure-text%0Acounterpart%20on%20the%20pass%40k%20metric.%20Crucially%2C%20this%20advantage%20is%20not%20confined%20to%0Acomputationally-intensive%20problems%20but%20extends%20to%20those%20requiring%20significant%0Aabstract%20insight.%20We%20further%20identify%20the%20emergent%20cognitive%20patterns%20that%0Aillustrate%20how%20models%20learn%20to%20think%20with%20tools.%20Finally%2C%20we%20report%20improved%0Atool%20usage%20behavior%20with%20early%20code%20invocation%20and%20much%20more%20interactive%20turns%0Awith%20ASPO.%20Overall%2C%20our%20work%20provides%20the%20first%20principled%20explanation%20for%0ATIR%27s%20success%2C%20shifting%20the%20focus%20from%20the%20mere%20fact%20that%20tools%20work%20to%20why%20and%0Ahow%20they%20enable%20more%20powerful%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19201v1&entry.124074799=Read"},
{"title": "RoofSeg: An edge-aware transformer-based network for end-to-end roof\n  plane segmentation", "author": "Siyuan You and Guozheng Xu and Pengwei Zhou and Qiwen Jin and Jian Yao and Li Li", "abstract": "  Roof plane segmentation is one of the key procedures for reconstructing\nthree-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from\nairborne light detection and ranging (LiDAR) point clouds. The majority of\ncurrent approaches for roof plane segmentation rely on the manually designed or\nlearned features followed by some specifically designed geometric clustering\nstrategies. Because the learned features are more powerful than the manually\ndesigned features, the deep learning-based approaches usually perform better\nthan the traditional approaches. However, the current deep learning-based\napproaches have three unsolved problems. The first is that most of them are not\ntruly end-to-end, the plane segmentation results may be not optimal. The second\nis that the point feature discriminability near the edges is relatively low,\nleading to inaccurate planar edges. The third is that the planar geometric\ncharacteristics are not sufficiently considered to constrain the network\ntraining. To solve these issues, a novel edge-aware transformer-based network,\nnamed RoofSeg, is developed for segmenting roof planes from LiDAR point clouds\nin a truly end-to-end manner. In the RoofSeg, we leverage a transformer\nencoder-decoder-based framework to hierarchically predict the plane instance\nmasks with the use of a set of learnable plane queries. To further improve the\nsegmentation accuracy of edge regions, we also design an Edge-Aware Mask Module\n(EAMM) that sufficiently incorporates planar geometric prior of edges to\nenhance its discriminability for plane instance mask refinement. In addition,\nwe propose an adaptive weighting strategy in the mask loss to reduce the\ninfluence of misclassified points, and also propose a new plane geometric loss\nto constrain the network training.\n", "link": "http://arxiv.org/abs/2508.19003v1", "date": "2025-08-26", "relevancy": 2.6596, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5383}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoofSeg%3A%20An%20edge-aware%20transformer-based%20network%20for%20end-to-end%20roof%0A%20%20plane%20segmentation&body=Title%3A%20RoofSeg%3A%20An%20edge-aware%20transformer-based%20network%20for%20end-to-end%20roof%0A%20%20plane%20segmentation%0AAuthor%3A%20Siyuan%20You%20and%20Guozheng%20Xu%20and%20Pengwei%20Zhou%20and%20Qiwen%20Jin%20and%20Jian%20Yao%20and%20Li%20Li%0AAbstract%3A%20%20%20Roof%20plane%20segmentation%20is%20one%20of%20the%20key%20procedures%20for%20reconstructing%0Athree-dimensional%20%283D%29%20building%20models%20at%20levels%20of%20detail%20%28LoD%29%202%20and%203%20from%0Aairborne%20light%20detection%20and%20ranging%20%28LiDAR%29%20point%20clouds.%20The%20majority%20of%0Acurrent%20approaches%20for%20roof%20plane%20segmentation%20rely%20on%20the%20manually%20designed%20or%0Alearned%20features%20followed%20by%20some%20specifically%20designed%20geometric%20clustering%0Astrategies.%20Because%20the%20learned%20features%20are%20more%20powerful%20than%20the%20manually%0Adesigned%20features%2C%20the%20deep%20learning-based%20approaches%20usually%20perform%20better%0Athan%20the%20traditional%20approaches.%20However%2C%20the%20current%20deep%20learning-based%0Aapproaches%20have%20three%20unsolved%20problems.%20The%20first%20is%20that%20most%20of%20them%20are%20not%0Atruly%20end-to-end%2C%20the%20plane%20segmentation%20results%20may%20be%20not%20optimal.%20The%20second%0Ais%20that%20the%20point%20feature%20discriminability%20near%20the%20edges%20is%20relatively%20low%2C%0Aleading%20to%20inaccurate%20planar%20edges.%20The%20third%20is%20that%20the%20planar%20geometric%0Acharacteristics%20are%20not%20sufficiently%20considered%20to%20constrain%20the%20network%0Atraining.%20To%20solve%20these%20issues%2C%20a%20novel%20edge-aware%20transformer-based%20network%2C%0Anamed%20RoofSeg%2C%20is%20developed%20for%20segmenting%20roof%20planes%20from%20LiDAR%20point%20clouds%0Ain%20a%20truly%20end-to-end%20manner.%20In%20the%20RoofSeg%2C%20we%20leverage%20a%20transformer%0Aencoder-decoder-based%20framework%20to%20hierarchically%20predict%20the%20plane%20instance%0Amasks%20with%20the%20use%20of%20a%20set%20of%20learnable%20plane%20queries.%20To%20further%20improve%20the%0Asegmentation%20accuracy%20of%20edge%20regions%2C%20we%20also%20design%20an%20Edge-Aware%20Mask%20Module%0A%28EAMM%29%20that%20sufficiently%20incorporates%20planar%20geometric%20prior%20of%20edges%20to%0Aenhance%20its%20discriminability%20for%20plane%20instance%20mask%20refinement.%20In%20addition%2C%0Awe%20propose%20an%20adaptive%20weighting%20strategy%20in%20the%20mask%20loss%20to%20reduce%20the%0Ainfluence%20of%20misclassified%20points%2C%20and%20also%20propose%20a%20new%20plane%20geometric%20loss%0Ato%20constrain%20the%20network%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoofSeg%253A%2520An%2520edge-aware%2520transformer-based%2520network%2520for%2520end-to-end%2520roof%250A%2520%2520plane%2520segmentation%26entry.906535625%3DSiyuan%2520You%2520and%2520Guozheng%2520Xu%2520and%2520Pengwei%2520Zhou%2520and%2520Qiwen%2520Jin%2520and%2520Jian%2520Yao%2520and%2520Li%2520Li%26entry.1292438233%3D%2520%2520Roof%2520plane%2520segmentation%2520is%2520one%2520of%2520the%2520key%2520procedures%2520for%2520reconstructing%250Athree-dimensional%2520%25283D%2529%2520building%2520models%2520at%2520levels%2520of%2520detail%2520%2528LoD%2529%25202%2520and%25203%2520from%250Aairborne%2520light%2520detection%2520and%2520ranging%2520%2528LiDAR%2529%2520point%2520clouds.%2520The%2520majority%2520of%250Acurrent%2520approaches%2520for%2520roof%2520plane%2520segmentation%2520rely%2520on%2520the%2520manually%2520designed%2520or%250Alearned%2520features%2520followed%2520by%2520some%2520specifically%2520designed%2520geometric%2520clustering%250Astrategies.%2520Because%2520the%2520learned%2520features%2520are%2520more%2520powerful%2520than%2520the%2520manually%250Adesigned%2520features%252C%2520the%2520deep%2520learning-based%2520approaches%2520usually%2520perform%2520better%250Athan%2520the%2520traditional%2520approaches.%2520However%252C%2520the%2520current%2520deep%2520learning-based%250Aapproaches%2520have%2520three%2520unsolved%2520problems.%2520The%2520first%2520is%2520that%2520most%2520of%2520them%2520are%2520not%250Atruly%2520end-to-end%252C%2520the%2520plane%2520segmentation%2520results%2520may%2520be%2520not%2520optimal.%2520The%2520second%250Ais%2520that%2520the%2520point%2520feature%2520discriminability%2520near%2520the%2520edges%2520is%2520relatively%2520low%252C%250Aleading%2520to%2520inaccurate%2520planar%2520edges.%2520The%2520third%2520is%2520that%2520the%2520planar%2520geometric%250Acharacteristics%2520are%2520not%2520sufficiently%2520considered%2520to%2520constrain%2520the%2520network%250Atraining.%2520To%2520solve%2520these%2520issues%252C%2520a%2520novel%2520edge-aware%2520transformer-based%2520network%252C%250Anamed%2520RoofSeg%252C%2520is%2520developed%2520for%2520segmenting%2520roof%2520planes%2520from%2520LiDAR%2520point%2520clouds%250Ain%2520a%2520truly%2520end-to-end%2520manner.%2520In%2520the%2520RoofSeg%252C%2520we%2520leverage%2520a%2520transformer%250Aencoder-decoder-based%2520framework%2520to%2520hierarchically%2520predict%2520the%2520plane%2520instance%250Amasks%2520with%2520the%2520use%2520of%2520a%2520set%2520of%2520learnable%2520plane%2520queries.%2520To%2520further%2520improve%2520the%250Asegmentation%2520accuracy%2520of%2520edge%2520regions%252C%2520we%2520also%2520design%2520an%2520Edge-Aware%2520Mask%2520Module%250A%2528EAMM%2529%2520that%2520sufficiently%2520incorporates%2520planar%2520geometric%2520prior%2520of%2520edges%2520to%250Aenhance%2520its%2520discriminability%2520for%2520plane%2520instance%2520mask%2520refinement.%2520In%2520addition%252C%250Awe%2520propose%2520an%2520adaptive%2520weighting%2520strategy%2520in%2520the%2520mask%2520loss%2520to%2520reduce%2520the%250Ainfluence%2520of%2520misclassified%2520points%252C%2520and%2520also%2520propose%2520a%2520new%2520plane%2520geometric%2520loss%250Ato%2520constrain%2520the%2520network%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoofSeg%3A%20An%20edge-aware%20transformer-based%20network%20for%20end-to-end%20roof%0A%20%20plane%20segmentation&entry.906535625=Siyuan%20You%20and%20Guozheng%20Xu%20and%20Pengwei%20Zhou%20and%20Qiwen%20Jin%20and%20Jian%20Yao%20and%20Li%20Li&entry.1292438233=%20%20Roof%20plane%20segmentation%20is%20one%20of%20the%20key%20procedures%20for%20reconstructing%0Athree-dimensional%20%283D%29%20building%20models%20at%20levels%20of%20detail%20%28LoD%29%202%20and%203%20from%0Aairborne%20light%20detection%20and%20ranging%20%28LiDAR%29%20point%20clouds.%20The%20majority%20of%0Acurrent%20approaches%20for%20roof%20plane%20segmentation%20rely%20on%20the%20manually%20designed%20or%0Alearned%20features%20followed%20by%20some%20specifically%20designed%20geometric%20clustering%0Astrategies.%20Because%20the%20learned%20features%20are%20more%20powerful%20than%20the%20manually%0Adesigned%20features%2C%20the%20deep%20learning-based%20approaches%20usually%20perform%20better%0Athan%20the%20traditional%20approaches.%20However%2C%20the%20current%20deep%20learning-based%0Aapproaches%20have%20three%20unsolved%20problems.%20The%20first%20is%20that%20most%20of%20them%20are%20not%0Atruly%20end-to-end%2C%20the%20plane%20segmentation%20results%20may%20be%20not%20optimal.%20The%20second%0Ais%20that%20the%20point%20feature%20discriminability%20near%20the%20edges%20is%20relatively%20low%2C%0Aleading%20to%20inaccurate%20planar%20edges.%20The%20third%20is%20that%20the%20planar%20geometric%0Acharacteristics%20are%20not%20sufficiently%20considered%20to%20constrain%20the%20network%0Atraining.%20To%20solve%20these%20issues%2C%20a%20novel%20edge-aware%20transformer-based%20network%2C%0Anamed%20RoofSeg%2C%20is%20developed%20for%20segmenting%20roof%20planes%20from%20LiDAR%20point%20clouds%0Ain%20a%20truly%20end-to-end%20manner.%20In%20the%20RoofSeg%2C%20we%20leverage%20a%20transformer%0Aencoder-decoder-based%20framework%20to%20hierarchically%20predict%20the%20plane%20instance%0Amasks%20with%20the%20use%20of%20a%20set%20of%20learnable%20plane%20queries.%20To%20further%20improve%20the%0Asegmentation%20accuracy%20of%20edge%20regions%2C%20we%20also%20design%20an%20Edge-Aware%20Mask%20Module%0A%28EAMM%29%20that%20sufficiently%20incorporates%20planar%20geometric%20prior%20of%20edges%20to%0Aenhance%20its%20discriminability%20for%20plane%20instance%20mask%20refinement.%20In%20addition%2C%0Awe%20propose%20an%20adaptive%20weighting%20strategy%20in%20the%20mask%20loss%20to%20reduce%20the%0Ainfluence%20of%20misclassified%20points%2C%20and%20also%20propose%20a%20new%20plane%20geometric%20loss%0Ato%20constrain%20the%20network%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19003v1&entry.124074799=Read"},
{"title": "Style4D-Bench: A Benchmark Suite for 4D Stylization", "author": "Beiqi Chen and Shuai Shao and Haitang Feng and Jianhuang Lai and Jianlou Si and Guangcong Wang", "abstract": "  We introduce Style4D-Bench, the first benchmark suite specifically designed\nfor 4D stylization, with the goal of standardizing evaluation and facilitating\nprogress in this emerging area. Style4D-Bench comprises: 1) a comprehensive\nevaluation protocol measuring spatial fidelity, temporal coherence, and\nmulti-view consistency through both perceptual and quantitative metrics, 2) a\nstrong baseline that make an initial attempt for 4D stylization, and 3) a\ncurated collection of high-resolution dynamic 4D scenes with diverse motions\nand complex backgrounds. To establish a strong baseline, we present Style4D, a\nnovel framework built upon 4D Gaussian Splatting. It consists of three key\ncomponents: a basic 4DGS scene representation to capture reliable geometry, a\nStyle Gaussian Representation that leverages lightweight per-Gaussian MLPs for\ntemporally and spatially aware appearance control, and a Holistic\nGeometry-Preserved Style Transfer module designed to enhance spatio-temporal\nconsistency via contrastive coherence learning and structural content\npreservation. Extensive experiments on Style4D-Bench demonstrate that Style4D\nachieves state-of-the-art performance in 4D stylization, producing fine-grained\nstylistic details with stable temporal dynamics and consistent multi-view\nrendering. We expect Style4D-Bench to become a valuable resource for\nbenchmarking and advancing research in stylized rendering of dynamic 3D scenes.\nProject page: https://becky-catherine.github.io/Style4D . Code:\nhttps://github.com/Becky-catherine/Style4D-Bench .\n", "link": "http://arxiv.org/abs/2508.19243v1", "date": "2025-08-26", "relevancy": 2.6374, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5359}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5233}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Style4D-Bench%3A%20A%20Benchmark%20Suite%20for%204D%20Stylization&body=Title%3A%20Style4D-Bench%3A%20A%20Benchmark%20Suite%20for%204D%20Stylization%0AAuthor%3A%20Beiqi%20Chen%20and%20Shuai%20Shao%20and%20Haitang%20Feng%20and%20Jianhuang%20Lai%20and%20Jianlou%20Si%20and%20Guangcong%20Wang%0AAbstract%3A%20%20%20We%20introduce%20Style4D-Bench%2C%20the%20first%20benchmark%20suite%20specifically%20designed%0Afor%204D%20stylization%2C%20with%20the%20goal%20of%20standardizing%20evaluation%20and%20facilitating%0Aprogress%20in%20this%20emerging%20area.%20Style4D-Bench%20comprises%3A%201%29%20a%20comprehensive%0Aevaluation%20protocol%20measuring%20spatial%20fidelity%2C%20temporal%20coherence%2C%20and%0Amulti-view%20consistency%20through%20both%20perceptual%20and%20quantitative%20metrics%2C%202%29%20a%0Astrong%20baseline%20that%20make%20an%20initial%20attempt%20for%204D%20stylization%2C%20and%203%29%20a%0Acurated%20collection%20of%20high-resolution%20dynamic%204D%20scenes%20with%20diverse%20motions%0Aand%20complex%20backgrounds.%20To%20establish%20a%20strong%20baseline%2C%20we%20present%20Style4D%2C%20a%0Anovel%20framework%20built%20upon%204D%20Gaussian%20Splatting.%20It%20consists%20of%20three%20key%0Acomponents%3A%20a%20basic%204DGS%20scene%20representation%20to%20capture%20reliable%20geometry%2C%20a%0AStyle%20Gaussian%20Representation%20that%20leverages%20lightweight%20per-Gaussian%20MLPs%20for%0Atemporally%20and%20spatially%20aware%20appearance%20control%2C%20and%20a%20Holistic%0AGeometry-Preserved%20Style%20Transfer%20module%20designed%20to%20enhance%20spatio-temporal%0Aconsistency%20via%20contrastive%20coherence%20learning%20and%20structural%20content%0Apreservation.%20Extensive%20experiments%20on%20Style4D-Bench%20demonstrate%20that%20Style4D%0Aachieves%20state-of-the-art%20performance%20in%204D%20stylization%2C%20producing%20fine-grained%0Astylistic%20details%20with%20stable%20temporal%20dynamics%20and%20consistent%20multi-view%0Arendering.%20We%20expect%20Style4D-Bench%20to%20become%20a%20valuable%20resource%20for%0Abenchmarking%20and%20advancing%20research%20in%20stylized%20rendering%20of%20dynamic%203D%20scenes.%0AProject%20page%3A%20https%3A//becky-catherine.github.io/Style4D%20.%20Code%3A%0Ahttps%3A//github.com/Becky-catherine/Style4D-Bench%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyle4D-Bench%253A%2520A%2520Benchmark%2520Suite%2520for%25204D%2520Stylization%26entry.906535625%3DBeiqi%2520Chen%2520and%2520Shuai%2520Shao%2520and%2520Haitang%2520Feng%2520and%2520Jianhuang%2520Lai%2520and%2520Jianlou%2520Si%2520and%2520Guangcong%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Style4D-Bench%252C%2520the%2520first%2520benchmark%2520suite%2520specifically%2520designed%250Afor%25204D%2520stylization%252C%2520with%2520the%2520goal%2520of%2520standardizing%2520evaluation%2520and%2520facilitating%250Aprogress%2520in%2520this%2520emerging%2520area.%2520Style4D-Bench%2520comprises%253A%25201%2529%2520a%2520comprehensive%250Aevaluation%2520protocol%2520measuring%2520spatial%2520fidelity%252C%2520temporal%2520coherence%252C%2520and%250Amulti-view%2520consistency%2520through%2520both%2520perceptual%2520and%2520quantitative%2520metrics%252C%25202%2529%2520a%250Astrong%2520baseline%2520that%2520make%2520an%2520initial%2520attempt%2520for%25204D%2520stylization%252C%2520and%25203%2529%2520a%250Acurated%2520collection%2520of%2520high-resolution%2520dynamic%25204D%2520scenes%2520with%2520diverse%2520motions%250Aand%2520complex%2520backgrounds.%2520To%2520establish%2520a%2520strong%2520baseline%252C%2520we%2520present%2520Style4D%252C%2520a%250Anovel%2520framework%2520built%2520upon%25204D%2520Gaussian%2520Splatting.%2520It%2520consists%2520of%2520three%2520key%250Acomponents%253A%2520a%2520basic%25204DGS%2520scene%2520representation%2520to%2520capture%2520reliable%2520geometry%252C%2520a%250AStyle%2520Gaussian%2520Representation%2520that%2520leverages%2520lightweight%2520per-Gaussian%2520MLPs%2520for%250Atemporally%2520and%2520spatially%2520aware%2520appearance%2520control%252C%2520and%2520a%2520Holistic%250AGeometry-Preserved%2520Style%2520Transfer%2520module%2520designed%2520to%2520enhance%2520spatio-temporal%250Aconsistency%2520via%2520contrastive%2520coherence%2520learning%2520and%2520structural%2520content%250Apreservation.%2520Extensive%2520experiments%2520on%2520Style4D-Bench%2520demonstrate%2520that%2520Style4D%250Aachieves%2520state-of-the-art%2520performance%2520in%25204D%2520stylization%252C%2520producing%2520fine-grained%250Astylistic%2520details%2520with%2520stable%2520temporal%2520dynamics%2520and%2520consistent%2520multi-view%250Arendering.%2520We%2520expect%2520Style4D-Bench%2520to%2520become%2520a%2520valuable%2520resource%2520for%250Abenchmarking%2520and%2520advancing%2520research%2520in%2520stylized%2520rendering%2520of%2520dynamic%25203D%2520scenes.%250AProject%2520page%253A%2520https%253A//becky-catherine.github.io/Style4D%2520.%2520Code%253A%250Ahttps%253A//github.com/Becky-catherine/Style4D-Bench%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Style4D-Bench%3A%20A%20Benchmark%20Suite%20for%204D%20Stylization&entry.906535625=Beiqi%20Chen%20and%20Shuai%20Shao%20and%20Haitang%20Feng%20and%20Jianhuang%20Lai%20and%20Jianlou%20Si%20and%20Guangcong%20Wang&entry.1292438233=%20%20We%20introduce%20Style4D-Bench%2C%20the%20first%20benchmark%20suite%20specifically%20designed%0Afor%204D%20stylization%2C%20with%20the%20goal%20of%20standardizing%20evaluation%20and%20facilitating%0Aprogress%20in%20this%20emerging%20area.%20Style4D-Bench%20comprises%3A%201%29%20a%20comprehensive%0Aevaluation%20protocol%20measuring%20spatial%20fidelity%2C%20temporal%20coherence%2C%20and%0Amulti-view%20consistency%20through%20both%20perceptual%20and%20quantitative%20metrics%2C%202%29%20a%0Astrong%20baseline%20that%20make%20an%20initial%20attempt%20for%204D%20stylization%2C%20and%203%29%20a%0Acurated%20collection%20of%20high-resolution%20dynamic%204D%20scenes%20with%20diverse%20motions%0Aand%20complex%20backgrounds.%20To%20establish%20a%20strong%20baseline%2C%20we%20present%20Style4D%2C%20a%0Anovel%20framework%20built%20upon%204D%20Gaussian%20Splatting.%20It%20consists%20of%20three%20key%0Acomponents%3A%20a%20basic%204DGS%20scene%20representation%20to%20capture%20reliable%20geometry%2C%20a%0AStyle%20Gaussian%20Representation%20that%20leverages%20lightweight%20per-Gaussian%20MLPs%20for%0Atemporally%20and%20spatially%20aware%20appearance%20control%2C%20and%20a%20Holistic%0AGeometry-Preserved%20Style%20Transfer%20module%20designed%20to%20enhance%20spatio-temporal%0Aconsistency%20via%20contrastive%20coherence%20learning%20and%20structural%20content%0Apreservation.%20Extensive%20experiments%20on%20Style4D-Bench%20demonstrate%20that%20Style4D%0Aachieves%20state-of-the-art%20performance%20in%204D%20stylization%2C%20producing%20fine-grained%0Astylistic%20details%20with%20stable%20temporal%20dynamics%20and%20consistent%20multi-view%0Arendering.%20We%20expect%20Style4D-Bench%20to%20become%20a%20valuable%20resource%20for%0Abenchmarking%20and%20advancing%20research%20in%20stylized%20rendering%20of%20dynamic%203D%20scenes.%0AProject%20page%3A%20https%3A//becky-catherine.github.io/Style4D%20.%20Code%3A%0Ahttps%3A//github.com/Becky-catherine/Style4D-Bench%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19243v1&entry.124074799=Read"},
{"title": "OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive\n  Simulation", "author": "Jianwen Jiang and Weihong Zeng and Zerong Zheng and Jiaqi Yang and Chao Liang and Wang Liao and Han Liang and Yuan Zhang and Mingyuan Gao", "abstract": "  Existing video avatar models can produce fluid human animations, yet they\nstruggle to move beyond mere physical likeness to capture a character's\nauthentic essence. Their motions typically synchronize with low-level cues like\naudio rhythm, lacking a deeper semantic understanding of emotion, intent, or\ncontext. To bridge this gap, \\textbf{we propose a framework designed to\ngenerate character animations that are not only physically plausible but also\nsemantically coherent and expressive.} Our model, \\textbf{OmniHuman-1.5}, is\nbuilt upon two key technical contributions. First, we leverage Multimodal Large\nLanguage Models to synthesize a structured textual representation of conditions\nthat provides high-level semantic guidance. This guidance steers our motion\ngenerator beyond simplistic rhythmic synchronization, enabling the production\nof actions that are contextually and emotionally resonant. Second, to ensure\nthe effective fusion of these multimodal inputs and mitigate inter-modality\nconflicts, we introduce a specialized Multimodal DiT architecture with a novel\nPseudo Last Frame design. The synergy of these components allows our model to\naccurately interpret the joint semantics of audio, images, and text, thereby\ngenerating motions that are deeply coherent with the character, scene, and\nlinguistic content. Extensive experiments demonstrate that our model achieves\nleading performance across a comprehensive set of metrics, including lip-sync\naccuracy, video quality, motion naturalness and semantic consistency with\ntextual prompts. Furthermore, our approach shows remarkable extensibility to\ncomplex scenarios, such as those involving multi-person and non-human subjects.\nHomepage: \\href{https://omnihuman-lab.github.io/v1_5/}\n", "link": "http://arxiv.org/abs/2508.19209v1", "date": "2025-08-26", "relevancy": 2.6062, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6707}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6479}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniHuman-1.5%3A%20Instilling%20an%20Active%20Mind%20in%20Avatars%20via%20Cognitive%0A%20%20Simulation&body=Title%3A%20OmniHuman-1.5%3A%20Instilling%20an%20Active%20Mind%20in%20Avatars%20via%20Cognitive%0A%20%20Simulation%0AAuthor%3A%20Jianwen%20Jiang%20and%20Weihong%20Zeng%20and%20Zerong%20Zheng%20and%20Jiaqi%20Yang%20and%20Chao%20Liang%20and%20Wang%20Liao%20and%20Han%20Liang%20and%20Yuan%20Zhang%20and%20Mingyuan%20Gao%0AAbstract%3A%20%20%20Existing%20video%20avatar%20models%20can%20produce%20fluid%20human%20animations%2C%20yet%20they%0Astruggle%20to%20move%20beyond%20mere%20physical%20likeness%20to%20capture%20a%20character%27s%0Aauthentic%20essence.%20Their%20motions%20typically%20synchronize%20with%20low-level%20cues%20like%0Aaudio%20rhythm%2C%20lacking%20a%20deeper%20semantic%20understanding%20of%20emotion%2C%20intent%2C%20or%0Acontext.%20To%20bridge%20this%20gap%2C%20%5Ctextbf%7Bwe%20propose%20a%20framework%20designed%20to%0Agenerate%20character%20animations%20that%20are%20not%20only%20physically%20plausible%20but%20also%0Asemantically%20coherent%20and%20expressive.%7D%20Our%20model%2C%20%5Ctextbf%7BOmniHuman-1.5%7D%2C%20is%0Abuilt%20upon%20two%20key%20technical%20contributions.%20First%2C%20we%20leverage%20Multimodal%20Large%0ALanguage%20Models%20to%20synthesize%20a%20structured%20textual%20representation%20of%20conditions%0Athat%20provides%20high-level%20semantic%20guidance.%20This%20guidance%20steers%20our%20motion%0Agenerator%20beyond%20simplistic%20rhythmic%20synchronization%2C%20enabling%20the%20production%0Aof%20actions%20that%20are%20contextually%20and%20emotionally%20resonant.%20Second%2C%20to%20ensure%0Athe%20effective%20fusion%20of%20these%20multimodal%20inputs%20and%20mitigate%20inter-modality%0Aconflicts%2C%20we%20introduce%20a%20specialized%20Multimodal%20DiT%20architecture%20with%20a%20novel%0APseudo%20Last%20Frame%20design.%20The%20synergy%20of%20these%20components%20allows%20our%20model%20to%0Aaccurately%20interpret%20the%20joint%20semantics%20of%20audio%2C%20images%2C%20and%20text%2C%20thereby%0Agenerating%20motions%20that%20are%20deeply%20coherent%20with%20the%20character%2C%20scene%2C%20and%0Alinguistic%20content.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20achieves%0Aleading%20performance%20across%20a%20comprehensive%20set%20of%20metrics%2C%20including%20lip-sync%0Aaccuracy%2C%20video%20quality%2C%20motion%20naturalness%20and%20semantic%20consistency%20with%0Atextual%20prompts.%20Furthermore%2C%20our%20approach%20shows%20remarkable%20extensibility%20to%0Acomplex%20scenarios%2C%20such%20as%20those%20involving%20multi-person%20and%20non-human%20subjects.%0AHomepage%3A%20%5Chref%7Bhttps%3A//omnihuman-lab.github.io/v1_5/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniHuman-1.5%253A%2520Instilling%2520an%2520Active%2520Mind%2520in%2520Avatars%2520via%2520Cognitive%250A%2520%2520Simulation%26entry.906535625%3DJianwen%2520Jiang%2520and%2520Weihong%2520Zeng%2520and%2520Zerong%2520Zheng%2520and%2520Jiaqi%2520Yang%2520and%2520Chao%2520Liang%2520and%2520Wang%2520Liao%2520and%2520Han%2520Liang%2520and%2520Yuan%2520Zhang%2520and%2520Mingyuan%2520Gao%26entry.1292438233%3D%2520%2520Existing%2520video%2520avatar%2520models%2520can%2520produce%2520fluid%2520human%2520animations%252C%2520yet%2520they%250Astruggle%2520to%2520move%2520beyond%2520mere%2520physical%2520likeness%2520to%2520capture%2520a%2520character%2527s%250Aauthentic%2520essence.%2520Their%2520motions%2520typically%2520synchronize%2520with%2520low-level%2520cues%2520like%250Aaudio%2520rhythm%252C%2520lacking%2520a%2520deeper%2520semantic%2520understanding%2520of%2520emotion%252C%2520intent%252C%2520or%250Acontext.%2520To%2520bridge%2520this%2520gap%252C%2520%255Ctextbf%257Bwe%2520propose%2520a%2520framework%2520designed%2520to%250Agenerate%2520character%2520animations%2520that%2520are%2520not%2520only%2520physically%2520plausible%2520but%2520also%250Asemantically%2520coherent%2520and%2520expressive.%257D%2520Our%2520model%252C%2520%255Ctextbf%257BOmniHuman-1.5%257D%252C%2520is%250Abuilt%2520upon%2520two%2520key%2520technical%2520contributions.%2520First%252C%2520we%2520leverage%2520Multimodal%2520Large%250ALanguage%2520Models%2520to%2520synthesize%2520a%2520structured%2520textual%2520representation%2520of%2520conditions%250Athat%2520provides%2520high-level%2520semantic%2520guidance.%2520This%2520guidance%2520steers%2520our%2520motion%250Agenerator%2520beyond%2520simplistic%2520rhythmic%2520synchronization%252C%2520enabling%2520the%2520production%250Aof%2520actions%2520that%2520are%2520contextually%2520and%2520emotionally%2520resonant.%2520Second%252C%2520to%2520ensure%250Athe%2520effective%2520fusion%2520of%2520these%2520multimodal%2520inputs%2520and%2520mitigate%2520inter-modality%250Aconflicts%252C%2520we%2520introduce%2520a%2520specialized%2520Multimodal%2520DiT%2520architecture%2520with%2520a%2520novel%250APseudo%2520Last%2520Frame%2520design.%2520The%2520synergy%2520of%2520these%2520components%2520allows%2520our%2520model%2520to%250Aaccurately%2520interpret%2520the%2520joint%2520semantics%2520of%2520audio%252C%2520images%252C%2520and%2520text%252C%2520thereby%250Agenerating%2520motions%2520that%2520are%2520deeply%2520coherent%2520with%2520the%2520character%252C%2520scene%252C%2520and%250Alinguistic%2520content.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520model%2520achieves%250Aleading%2520performance%2520across%2520a%2520comprehensive%2520set%2520of%2520metrics%252C%2520including%2520lip-sync%250Aaccuracy%252C%2520video%2520quality%252C%2520motion%2520naturalness%2520and%2520semantic%2520consistency%2520with%250Atextual%2520prompts.%2520Furthermore%252C%2520our%2520approach%2520shows%2520remarkable%2520extensibility%2520to%250Acomplex%2520scenarios%252C%2520such%2520as%2520those%2520involving%2520multi-person%2520and%2520non-human%2520subjects.%250AHomepage%253A%2520%255Chref%257Bhttps%253A//omnihuman-lab.github.io/v1_5/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniHuman-1.5%3A%20Instilling%20an%20Active%20Mind%20in%20Avatars%20via%20Cognitive%0A%20%20Simulation&entry.906535625=Jianwen%20Jiang%20and%20Weihong%20Zeng%20and%20Zerong%20Zheng%20and%20Jiaqi%20Yang%20and%20Chao%20Liang%20and%20Wang%20Liao%20and%20Han%20Liang%20and%20Yuan%20Zhang%20and%20Mingyuan%20Gao&entry.1292438233=%20%20Existing%20video%20avatar%20models%20can%20produce%20fluid%20human%20animations%2C%20yet%20they%0Astruggle%20to%20move%20beyond%20mere%20physical%20likeness%20to%20capture%20a%20character%27s%0Aauthentic%20essence.%20Their%20motions%20typically%20synchronize%20with%20low-level%20cues%20like%0Aaudio%20rhythm%2C%20lacking%20a%20deeper%20semantic%20understanding%20of%20emotion%2C%20intent%2C%20or%0Acontext.%20To%20bridge%20this%20gap%2C%20%5Ctextbf%7Bwe%20propose%20a%20framework%20designed%20to%0Agenerate%20character%20animations%20that%20are%20not%20only%20physically%20plausible%20but%20also%0Asemantically%20coherent%20and%20expressive.%7D%20Our%20model%2C%20%5Ctextbf%7BOmniHuman-1.5%7D%2C%20is%0Abuilt%20upon%20two%20key%20technical%20contributions.%20First%2C%20we%20leverage%20Multimodal%20Large%0ALanguage%20Models%20to%20synthesize%20a%20structured%20textual%20representation%20of%20conditions%0Athat%20provides%20high-level%20semantic%20guidance.%20This%20guidance%20steers%20our%20motion%0Agenerator%20beyond%20simplistic%20rhythmic%20synchronization%2C%20enabling%20the%20production%0Aof%20actions%20that%20are%20contextually%20and%20emotionally%20resonant.%20Second%2C%20to%20ensure%0Athe%20effective%20fusion%20of%20these%20multimodal%20inputs%20and%20mitigate%20inter-modality%0Aconflicts%2C%20we%20introduce%20a%20specialized%20Multimodal%20DiT%20architecture%20with%20a%20novel%0APseudo%20Last%20Frame%20design.%20The%20synergy%20of%20these%20components%20allows%20our%20model%20to%0Aaccurately%20interpret%20the%20joint%20semantics%20of%20audio%2C%20images%2C%20and%20text%2C%20thereby%0Agenerating%20motions%20that%20are%20deeply%20coherent%20with%20the%20character%2C%20scene%2C%20and%0Alinguistic%20content.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20achieves%0Aleading%20performance%20across%20a%20comprehensive%20set%20of%20metrics%2C%20including%20lip-sync%0Aaccuracy%2C%20video%20quality%2C%20motion%20naturalness%20and%20semantic%20consistency%20with%0Atextual%20prompts.%20Furthermore%2C%20our%20approach%20shows%20remarkable%20extensibility%20to%0Acomplex%20scenarios%2C%20such%20as%20those%20involving%20multi-person%20and%20non-human%20subjects.%0AHomepage%3A%20%5Chref%7Bhttps%3A//omnihuman-lab.github.io/v1_5/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19209v1&entry.124074799=Read"},
{"title": "Few-Shot Connectivity-Aware Text Line Segmentation in Historical\n  Documents", "author": "Rafael Sterzinger and Tingyu Lin and Robert Sablatnig", "abstract": "  A foundational task for the digital analysis of documents is text line\nsegmentation. However, automating this process with deep learning models is\nchallenging because it requires large, annotated datasets that are often\nunavailable for historical documents. Additionally, the annotation process is a\nlabor- and cost-intensive task that requires expert knowledge, which makes\nfew-shot learning a promising direction for reducing data requirements. In this\nwork, we demonstrate that small and simple architectures, coupled with a\ntopology-aware loss function, are more accurate and data-efficient than more\ncomplex alternatives. We pair a lightweight UNet++ with a connectivity-aware\nloss, initially developed for neuron morphology, which explicitly penalizes\nstructural errors like line fragmentation and unintended line merges. To\nincrease our limited data, we train on small patches extracted from a mere\nthree annotated pages per manuscript. Our methodology significantly improves\nupon the current state-of-the-art on the U-DIADS-TL dataset, with a 200%\nincrease in Recognition Accuracy and a 75% increase in Line Intersection over\nUnion. Our method also achieves an F-Measure score on par with or even\nexceeding that of the competition winner of the DIVA-HisDB baseline detection\ntask, all while requiring only three annotated pages, exemplifying the efficacy\nof our approach. Our implementation is publicly available at:\nhttps://github.com/RafaelSterzinger/acpr_few_shot_hist.\n", "link": "http://arxiv.org/abs/2508.19162v1", "date": "2025-08-26", "relevancy": 2.5735, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.526}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5095}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Connectivity-Aware%20Text%20Line%20Segmentation%20in%20Historical%0A%20%20Documents&body=Title%3A%20Few-Shot%20Connectivity-Aware%20Text%20Line%20Segmentation%20in%20Historical%0A%20%20Documents%0AAuthor%3A%20Rafael%20Sterzinger%20and%20Tingyu%20Lin%20and%20Robert%20Sablatnig%0AAbstract%3A%20%20%20A%20foundational%20task%20for%20the%20digital%20analysis%20of%20documents%20is%20text%20line%0Asegmentation.%20However%2C%20automating%20this%20process%20with%20deep%20learning%20models%20is%0Achallenging%20because%20it%20requires%20large%2C%20annotated%20datasets%20that%20are%20often%0Aunavailable%20for%20historical%20documents.%20Additionally%2C%20the%20annotation%20process%20is%20a%0Alabor-%20and%20cost-intensive%20task%20that%20requires%20expert%20knowledge%2C%20which%20makes%0Afew-shot%20learning%20a%20promising%20direction%20for%20reducing%20data%20requirements.%20In%20this%0Awork%2C%20we%20demonstrate%20that%20small%20and%20simple%20architectures%2C%20coupled%20with%20a%0Atopology-aware%20loss%20function%2C%20are%20more%20accurate%20and%20data-efficient%20than%20more%0Acomplex%20alternatives.%20We%20pair%20a%20lightweight%20UNet%2B%2B%20with%20a%20connectivity-aware%0Aloss%2C%20initially%20developed%20for%20neuron%20morphology%2C%20which%20explicitly%20penalizes%0Astructural%20errors%20like%20line%20fragmentation%20and%20unintended%20line%20merges.%20To%0Aincrease%20our%20limited%20data%2C%20we%20train%20on%20small%20patches%20extracted%20from%20a%20mere%0Athree%20annotated%20pages%20per%20manuscript.%20Our%20methodology%20significantly%20improves%0Aupon%20the%20current%20state-of-the-art%20on%20the%20U-DIADS-TL%20dataset%2C%20with%20a%20200%25%0Aincrease%20in%20Recognition%20Accuracy%20and%20a%2075%25%20increase%20in%20Line%20Intersection%20over%0AUnion.%20Our%20method%20also%20achieves%20an%20F-Measure%20score%20on%20par%20with%20or%20even%0Aexceeding%20that%20of%20the%20competition%20winner%20of%20the%20DIVA-HisDB%20baseline%20detection%0Atask%2C%20all%20while%20requiring%20only%20three%20annotated%20pages%2C%20exemplifying%20the%20efficacy%0Aof%20our%20approach.%20Our%20implementation%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/RafaelSterzinger/acpr_few_shot_hist.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Connectivity-Aware%2520Text%2520Line%2520Segmentation%2520in%2520Historical%250A%2520%2520Documents%26entry.906535625%3DRafael%2520Sterzinger%2520and%2520Tingyu%2520Lin%2520and%2520Robert%2520Sablatnig%26entry.1292438233%3D%2520%2520A%2520foundational%2520task%2520for%2520the%2520digital%2520analysis%2520of%2520documents%2520is%2520text%2520line%250Asegmentation.%2520However%252C%2520automating%2520this%2520process%2520with%2520deep%2520learning%2520models%2520is%250Achallenging%2520because%2520it%2520requires%2520large%252C%2520annotated%2520datasets%2520that%2520are%2520often%250Aunavailable%2520for%2520historical%2520documents.%2520Additionally%252C%2520the%2520annotation%2520process%2520is%2520a%250Alabor-%2520and%2520cost-intensive%2520task%2520that%2520requires%2520expert%2520knowledge%252C%2520which%2520makes%250Afew-shot%2520learning%2520a%2520promising%2520direction%2520for%2520reducing%2520data%2520requirements.%2520In%2520this%250Awork%252C%2520we%2520demonstrate%2520that%2520small%2520and%2520simple%2520architectures%252C%2520coupled%2520with%2520a%250Atopology-aware%2520loss%2520function%252C%2520are%2520more%2520accurate%2520and%2520data-efficient%2520than%2520more%250Acomplex%2520alternatives.%2520We%2520pair%2520a%2520lightweight%2520UNet%252B%252B%2520with%2520a%2520connectivity-aware%250Aloss%252C%2520initially%2520developed%2520for%2520neuron%2520morphology%252C%2520which%2520explicitly%2520penalizes%250Astructural%2520errors%2520like%2520line%2520fragmentation%2520and%2520unintended%2520line%2520merges.%2520To%250Aincrease%2520our%2520limited%2520data%252C%2520we%2520train%2520on%2520small%2520patches%2520extracted%2520from%2520a%2520mere%250Athree%2520annotated%2520pages%2520per%2520manuscript.%2520Our%2520methodology%2520significantly%2520improves%250Aupon%2520the%2520current%2520state-of-the-art%2520on%2520the%2520U-DIADS-TL%2520dataset%252C%2520with%2520a%2520200%2525%250Aincrease%2520in%2520Recognition%2520Accuracy%2520and%2520a%252075%2525%2520increase%2520in%2520Line%2520Intersection%2520over%250AUnion.%2520Our%2520method%2520also%2520achieves%2520an%2520F-Measure%2520score%2520on%2520par%2520with%2520or%2520even%250Aexceeding%2520that%2520of%2520the%2520competition%2520winner%2520of%2520the%2520DIVA-HisDB%2520baseline%2520detection%250Atask%252C%2520all%2520while%2520requiring%2520only%2520three%2520annotated%2520pages%252C%2520exemplifying%2520the%2520efficacy%250Aof%2520our%2520approach.%2520Our%2520implementation%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/RafaelSterzinger/acpr_few_shot_hist.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Connectivity-Aware%20Text%20Line%20Segmentation%20in%20Historical%0A%20%20Documents&entry.906535625=Rafael%20Sterzinger%20and%20Tingyu%20Lin%20and%20Robert%20Sablatnig&entry.1292438233=%20%20A%20foundational%20task%20for%20the%20digital%20analysis%20of%20documents%20is%20text%20line%0Asegmentation.%20However%2C%20automating%20this%20process%20with%20deep%20learning%20models%20is%0Achallenging%20because%20it%20requires%20large%2C%20annotated%20datasets%20that%20are%20often%0Aunavailable%20for%20historical%20documents.%20Additionally%2C%20the%20annotation%20process%20is%20a%0Alabor-%20and%20cost-intensive%20task%20that%20requires%20expert%20knowledge%2C%20which%20makes%0Afew-shot%20learning%20a%20promising%20direction%20for%20reducing%20data%20requirements.%20In%20this%0Awork%2C%20we%20demonstrate%20that%20small%20and%20simple%20architectures%2C%20coupled%20with%20a%0Atopology-aware%20loss%20function%2C%20are%20more%20accurate%20and%20data-efficient%20than%20more%0Acomplex%20alternatives.%20We%20pair%20a%20lightweight%20UNet%2B%2B%20with%20a%20connectivity-aware%0Aloss%2C%20initially%20developed%20for%20neuron%20morphology%2C%20which%20explicitly%20penalizes%0Astructural%20errors%20like%20line%20fragmentation%20and%20unintended%20line%20merges.%20To%0Aincrease%20our%20limited%20data%2C%20we%20train%20on%20small%20patches%20extracted%20from%20a%20mere%0Athree%20annotated%20pages%20per%20manuscript.%20Our%20methodology%20significantly%20improves%0Aupon%20the%20current%20state-of-the-art%20on%20the%20U-DIADS-TL%20dataset%2C%20with%20a%20200%25%0Aincrease%20in%20Recognition%20Accuracy%20and%20a%2075%25%20increase%20in%20Line%20Intersection%20over%0AUnion.%20Our%20method%20also%20achieves%20an%20F-Measure%20score%20on%20par%20with%20or%20even%0Aexceeding%20that%20of%20the%20competition%20winner%20of%20the%20DIVA-HisDB%20baseline%20detection%0Atask%2C%20all%20while%20requiring%20only%20three%20annotated%20pages%2C%20exemplifying%20the%20efficacy%0Aof%20our%20approach.%20Our%20implementation%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/RafaelSterzinger/acpr_few_shot_hist.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19162v1&entry.124074799=Read"},
{"title": "General Intelligence Requires Reward-based Pretraining", "author": "Seungwook Han and Jyothish Pari and Samuel J. Gershman and Pulkit Agrawal", "abstract": "  Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a reasoning\nprior for RL that can then be transferred to natural language tasks, and (3)\nlearning more generalizable reasoning functions using a small context window to\nreduce exploiting spurious correlations between tokens. Such a reasoning system\ncoupled with a trained retrieval system and a large external memory bank as a\nknowledge store can overcome several limitations of existing architectures at\nlearning to reason in novel scenarios.\n", "link": "http://arxiv.org/abs/2502.19402v3", "date": "2025-08-26", "relevancy": 2.5567, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Intelligence%20Requires%20Reward-based%20Pretraining&body=Title%3A%20General%20Intelligence%20Requires%20Reward-based%20Pretraining%0AAuthor%3A%20Seungwook%20Han%20and%20Jyothish%20Pari%20and%20Samuel%20J.%20Gershman%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20real-world%20utility%2C%0Aexemplifying%20artificial%20useful%20intelligence%20%28AUI%29.%20However%2C%20their%20ability%20to%0Areason%20adaptively%20and%20robustly%20--%20the%20hallmarks%20of%20artificial%20general%0Aintelligence%20%28AGI%29%20--%20remains%20fragile.%20While%20LLMs%20seemingly%20succeed%20in%0Acommonsense%20reasoning%2C%20programming%2C%20and%20mathematics%2C%20they%20struggle%20to%0Ageneralize%20algorithmic%20understanding%20across%20novel%20contexts.%20Our%20experiments%0Awith%20algorithmic%20tasks%20in%20esoteric%20programming%20languages%20reveal%20that%20LLM%27s%0Areasoning%20overfits%20to%20the%20training%20data%20and%20is%20limited%20in%20its%20transferability.%0AWe%20hypothesize%20that%20the%20core%20issue%20underlying%20such%20limited%20transferability%20is%0Athe%20coupling%20of%20reasoning%20and%20knowledge%20in%20LLMs.%0A%20%20To%20transition%20from%20AUI%20to%20AGI%2C%20we%20propose%20disentangling%20knowledge%20and%0Areasoning%20through%20three%20key%20directions%3A%20%281%29%20pretaining%20to%20reason%20using%20RL%20from%0Ascratch%20as%20an%20alternative%20to%20the%20widely%20used%20next-token%20prediction%20pretraining%2C%0A%282%29%20using%20a%20curriculum%20of%20synthetic%20tasks%20to%20ease%20the%20learning%20of%20a%20reasoning%0Aprior%20for%20RL%20that%20can%20then%20be%20transferred%20to%20natural%20language%20tasks%2C%20and%20%283%29%0Alearning%20more%20generalizable%20reasoning%20functions%20using%20a%20small%20context%20window%20to%0Areduce%20exploiting%20spurious%20correlations%20between%20tokens.%20Such%20a%20reasoning%20system%0Acoupled%20with%20a%20trained%20retrieval%20system%20and%20a%20large%20external%20memory%20bank%20as%20a%0Aknowledge%20store%20can%20overcome%20several%20limitations%20of%20existing%20architectures%20at%0Alearning%20to%20reason%20in%20novel%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19402v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Intelligence%2520Requires%2520Reward-based%2520Pretraining%26entry.906535625%3DSeungwook%2520Han%2520and%2520Jyothish%2520Pari%2520and%2520Samuel%2520J.%2520Gershman%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520real-world%2520utility%252C%250Aexemplifying%2520artificial%2520useful%2520intelligence%2520%2528AUI%2529.%2520However%252C%2520their%2520ability%2520to%250Areason%2520adaptively%2520and%2520robustly%2520--%2520the%2520hallmarks%2520of%2520artificial%2520general%250Aintelligence%2520%2528AGI%2529%2520--%2520remains%2520fragile.%2520While%2520LLMs%2520seemingly%2520succeed%2520in%250Acommonsense%2520reasoning%252C%2520programming%252C%2520and%2520mathematics%252C%2520they%2520struggle%2520to%250Ageneralize%2520algorithmic%2520understanding%2520across%2520novel%2520contexts.%2520Our%2520experiments%250Awith%2520algorithmic%2520tasks%2520in%2520esoteric%2520programming%2520languages%2520reveal%2520that%2520LLM%2527s%250Areasoning%2520overfits%2520to%2520the%2520training%2520data%2520and%2520is%2520limited%2520in%2520its%2520transferability.%250AWe%2520hypothesize%2520that%2520the%2520core%2520issue%2520underlying%2520such%2520limited%2520transferability%2520is%250Athe%2520coupling%2520of%2520reasoning%2520and%2520knowledge%2520in%2520LLMs.%250A%2520%2520To%2520transition%2520from%2520AUI%2520to%2520AGI%252C%2520we%2520propose%2520disentangling%2520knowledge%2520and%250Areasoning%2520through%2520three%2520key%2520directions%253A%2520%25281%2529%2520pretaining%2520to%2520reason%2520using%2520RL%2520from%250Ascratch%2520as%2520an%2520alternative%2520to%2520the%2520widely%2520used%2520next-token%2520prediction%2520pretraining%252C%250A%25282%2529%2520using%2520a%2520curriculum%2520of%2520synthetic%2520tasks%2520to%2520ease%2520the%2520learning%2520of%2520a%2520reasoning%250Aprior%2520for%2520RL%2520that%2520can%2520then%2520be%2520transferred%2520to%2520natural%2520language%2520tasks%252C%2520and%2520%25283%2529%250Alearning%2520more%2520generalizable%2520reasoning%2520functions%2520using%2520a%2520small%2520context%2520window%2520to%250Areduce%2520exploiting%2520spurious%2520correlations%2520between%2520tokens.%2520Such%2520a%2520reasoning%2520system%250Acoupled%2520with%2520a%2520trained%2520retrieval%2520system%2520and%2520a%2520large%2520external%2520memory%2520bank%2520as%2520a%250Aknowledge%2520store%2520can%2520overcome%2520several%2520limitations%2520of%2520existing%2520architectures%2520at%250Alearning%2520to%2520reason%2520in%2520novel%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19402v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Intelligence%20Requires%20Reward-based%20Pretraining&entry.906535625=Seungwook%20Han%20and%20Jyothish%20Pari%20and%20Samuel%20J.%20Gershman%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20real-world%20utility%2C%0Aexemplifying%20artificial%20useful%20intelligence%20%28AUI%29.%20However%2C%20their%20ability%20to%0Areason%20adaptively%20and%20robustly%20--%20the%20hallmarks%20of%20artificial%20general%0Aintelligence%20%28AGI%29%20--%20remains%20fragile.%20While%20LLMs%20seemingly%20succeed%20in%0Acommonsense%20reasoning%2C%20programming%2C%20and%20mathematics%2C%20they%20struggle%20to%0Ageneralize%20algorithmic%20understanding%20across%20novel%20contexts.%20Our%20experiments%0Awith%20algorithmic%20tasks%20in%20esoteric%20programming%20languages%20reveal%20that%20LLM%27s%0Areasoning%20overfits%20to%20the%20training%20data%20and%20is%20limited%20in%20its%20transferability.%0AWe%20hypothesize%20that%20the%20core%20issue%20underlying%20such%20limited%20transferability%20is%0Athe%20coupling%20of%20reasoning%20and%20knowledge%20in%20LLMs.%0A%20%20To%20transition%20from%20AUI%20to%20AGI%2C%20we%20propose%20disentangling%20knowledge%20and%0Areasoning%20through%20three%20key%20directions%3A%20%281%29%20pretaining%20to%20reason%20using%20RL%20from%0Ascratch%20as%20an%20alternative%20to%20the%20widely%20used%20next-token%20prediction%20pretraining%2C%0A%282%29%20using%20a%20curriculum%20of%20synthetic%20tasks%20to%20ease%20the%20learning%20of%20a%20reasoning%0Aprior%20for%20RL%20that%20can%20then%20be%20transferred%20to%20natural%20language%20tasks%2C%20and%20%283%29%0Alearning%20more%20generalizable%20reasoning%20functions%20using%20a%20small%20context%20window%20to%0Areduce%20exploiting%20spurious%20correlations%20between%20tokens.%20Such%20a%20reasoning%20system%0Acoupled%20with%20a%20trained%20retrieval%20system%20and%20a%20large%20external%20memory%20bank%20as%20a%0Aknowledge%20store%20can%20overcome%20several%20limitations%20of%20existing%20architectures%20at%0Alearning%20to%20reason%20in%20novel%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19402v3&entry.124074799=Read"},
{"title": "GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling", "author": "Arash Jamshidi and Lauri Sepp\u00e4l\u00e4inen and Katsiaryna Haitsiukevich and Hoang Phuc Hau Luu and Anton Bj\u00f6rklund and Kai Puolam\u00e4ki", "abstract": "  Machine learning models are often learned by minimising a loss function on\nthe training data using a gradient descent algorithm. These models often suffer\nfrom overfitting, leading to a decline in predictive performance on unseen\ndata. A standard solution is early stopping using a hold-out validation set,\nwhich halts the minimisation when the validation loss stops decreasing.\nHowever, this hold-out set reduces the data available for training. This paper\npresents {\\sc gradstop}, a novel stochastic early stopping method that only\nuses information in the gradients, which are produced by the gradient descent\nalgorithm ``for free.'' Our main contributions are that we estimate the\nBayesian posterior by the gradient information, define the early stopping\nproblem as drawing sample from this posterior, and use the approximated\nposterior to obtain a stopping criterion. Our empirical evaluation shows that\n{\\sc gradstop} achieves a small loss on test data and compares favourably to a\nvalidation-set-based stopping criterion. By leveraging the entire dataset for\ntraining, our method is particularly advantageous in data-limited settings,\nsuch as transfer learning. It can be incorporated as an optional feature in\ngradient descent libraries with only a small computational overhead. The source\ncode is available at https://github.com/edahelsinki/gradstop.\n", "link": "http://arxiv.org/abs/2508.19028v1", "date": "2025-08-26", "relevancy": 2.5324, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5201}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5135}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRADSTOP%3A%20Early%20Stopping%20of%20Gradient%20Descent%20via%20Posterior%20Sampling&body=Title%3A%20GRADSTOP%3A%20Early%20Stopping%20of%20Gradient%20Descent%20via%20Posterior%20Sampling%0AAuthor%3A%20Arash%20Jamshidi%20and%20Lauri%20Sepp%C3%A4l%C3%A4inen%20and%20Katsiaryna%20Haitsiukevich%20and%20Hoang%20Phuc%20Hau%20Luu%20and%20Anton%20Bj%C3%B6rklund%20and%20Kai%20Puolam%C3%A4ki%0AAbstract%3A%20%20%20Machine%20learning%20models%20are%20often%20learned%20by%20minimising%20a%20loss%20function%20on%0Athe%20training%20data%20using%20a%20gradient%20descent%20algorithm.%20These%20models%20often%20suffer%0Afrom%20overfitting%2C%20leading%20to%20a%20decline%20in%20predictive%20performance%20on%20unseen%0Adata.%20A%20standard%20solution%20is%20early%20stopping%20using%20a%20hold-out%20validation%20set%2C%0Awhich%20halts%20the%20minimisation%20when%20the%20validation%20loss%20stops%20decreasing.%0AHowever%2C%20this%20hold-out%20set%20reduces%20the%20data%20available%20for%20training.%20This%20paper%0Apresents%20%7B%5Csc%20gradstop%7D%2C%20a%20novel%20stochastic%20early%20stopping%20method%20that%20only%0Auses%20information%20in%20the%20gradients%2C%20which%20are%20produced%20by%20the%20gradient%20descent%0Aalgorithm%20%60%60for%20free.%27%27%20Our%20main%20contributions%20are%20that%20we%20estimate%20the%0ABayesian%20posterior%20by%20the%20gradient%20information%2C%20define%20the%20early%20stopping%0Aproblem%20as%20drawing%20sample%20from%20this%20posterior%2C%20and%20use%20the%20approximated%0Aposterior%20to%20obtain%20a%20stopping%20criterion.%20Our%20empirical%20evaluation%20shows%20that%0A%7B%5Csc%20gradstop%7D%20achieves%20a%20small%20loss%20on%20test%20data%20and%20compares%20favourably%20to%20a%0Avalidation-set-based%20stopping%20criterion.%20By%20leveraging%20the%20entire%20dataset%20for%0Atraining%2C%20our%20method%20is%20particularly%20advantageous%20in%20data-limited%20settings%2C%0Asuch%20as%20transfer%20learning.%20It%20can%20be%20incorporated%20as%20an%20optional%20feature%20in%0Agradient%20descent%20libraries%20with%20only%20a%20small%20computational%20overhead.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/edahelsinki/gradstop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRADSTOP%253A%2520Early%2520Stopping%2520of%2520Gradient%2520Descent%2520via%2520Posterior%2520Sampling%26entry.906535625%3DArash%2520Jamshidi%2520and%2520Lauri%2520Sepp%25C3%25A4l%25C3%25A4inen%2520and%2520Katsiaryna%2520Haitsiukevich%2520and%2520Hoang%2520Phuc%2520Hau%2520Luu%2520and%2520Anton%2520Bj%25C3%25B6rklund%2520and%2520Kai%2520Puolam%25C3%25A4ki%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520are%2520often%2520learned%2520by%2520minimising%2520a%2520loss%2520function%2520on%250Athe%2520training%2520data%2520using%2520a%2520gradient%2520descent%2520algorithm.%2520These%2520models%2520often%2520suffer%250Afrom%2520overfitting%252C%2520leading%2520to%2520a%2520decline%2520in%2520predictive%2520performance%2520on%2520unseen%250Adata.%2520A%2520standard%2520solution%2520is%2520early%2520stopping%2520using%2520a%2520hold-out%2520validation%2520set%252C%250Awhich%2520halts%2520the%2520minimisation%2520when%2520the%2520validation%2520loss%2520stops%2520decreasing.%250AHowever%252C%2520this%2520hold-out%2520set%2520reduces%2520the%2520data%2520available%2520for%2520training.%2520This%2520paper%250Apresents%2520%257B%255Csc%2520gradstop%257D%252C%2520a%2520novel%2520stochastic%2520early%2520stopping%2520method%2520that%2520only%250Auses%2520information%2520in%2520the%2520gradients%252C%2520which%2520are%2520produced%2520by%2520the%2520gradient%2520descent%250Aalgorithm%2520%2560%2560for%2520free.%2527%2527%2520Our%2520main%2520contributions%2520are%2520that%2520we%2520estimate%2520the%250ABayesian%2520posterior%2520by%2520the%2520gradient%2520information%252C%2520define%2520the%2520early%2520stopping%250Aproblem%2520as%2520drawing%2520sample%2520from%2520this%2520posterior%252C%2520and%2520use%2520the%2520approximated%250Aposterior%2520to%2520obtain%2520a%2520stopping%2520criterion.%2520Our%2520empirical%2520evaluation%2520shows%2520that%250A%257B%255Csc%2520gradstop%257D%2520achieves%2520a%2520small%2520loss%2520on%2520test%2520data%2520and%2520compares%2520favourably%2520to%2520a%250Avalidation-set-based%2520stopping%2520criterion.%2520By%2520leveraging%2520the%2520entire%2520dataset%2520for%250Atraining%252C%2520our%2520method%2520is%2520particularly%2520advantageous%2520in%2520data-limited%2520settings%252C%250Asuch%2520as%2520transfer%2520learning.%2520It%2520can%2520be%2520incorporated%2520as%2520an%2520optional%2520feature%2520in%250Agradient%2520descent%2520libraries%2520with%2520only%2520a%2520small%2520computational%2520overhead.%2520The%2520source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/edahelsinki/gradstop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRADSTOP%3A%20Early%20Stopping%20of%20Gradient%20Descent%20via%20Posterior%20Sampling&entry.906535625=Arash%20Jamshidi%20and%20Lauri%20Sepp%C3%A4l%C3%A4inen%20and%20Katsiaryna%20Haitsiukevich%20and%20Hoang%20Phuc%20Hau%20Luu%20and%20Anton%20Bj%C3%B6rklund%20and%20Kai%20Puolam%C3%A4ki&entry.1292438233=%20%20Machine%20learning%20models%20are%20often%20learned%20by%20minimising%20a%20loss%20function%20on%0Athe%20training%20data%20using%20a%20gradient%20descent%20algorithm.%20These%20models%20often%20suffer%0Afrom%20overfitting%2C%20leading%20to%20a%20decline%20in%20predictive%20performance%20on%20unseen%0Adata.%20A%20standard%20solution%20is%20early%20stopping%20using%20a%20hold-out%20validation%20set%2C%0Awhich%20halts%20the%20minimisation%20when%20the%20validation%20loss%20stops%20decreasing.%0AHowever%2C%20this%20hold-out%20set%20reduces%20the%20data%20available%20for%20training.%20This%20paper%0Apresents%20%7B%5Csc%20gradstop%7D%2C%20a%20novel%20stochastic%20early%20stopping%20method%20that%20only%0Auses%20information%20in%20the%20gradients%2C%20which%20are%20produced%20by%20the%20gradient%20descent%0Aalgorithm%20%60%60for%20free.%27%27%20Our%20main%20contributions%20are%20that%20we%20estimate%20the%0ABayesian%20posterior%20by%20the%20gradient%20information%2C%20define%20the%20early%20stopping%0Aproblem%20as%20drawing%20sample%20from%20this%20posterior%2C%20and%20use%20the%20approximated%0Aposterior%20to%20obtain%20a%20stopping%20criterion.%20Our%20empirical%20evaluation%20shows%20that%0A%7B%5Csc%20gradstop%7D%20achieves%20a%20small%20loss%20on%20test%20data%20and%20compares%20favourably%20to%20a%0Avalidation-set-based%20stopping%20criterion.%20By%20leveraging%20the%20entire%20dataset%20for%0Atraining%2C%20our%20method%20is%20particularly%20advantageous%20in%20data-limited%20settings%2C%0Asuch%20as%20transfer%20learning.%20It%20can%20be%20incorporated%20as%20an%20optional%20feature%20in%0Agradient%20descent%20libraries%20with%20only%20a%20small%20computational%20overhead.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/edahelsinki/gradstop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19028v1&entry.124074799=Read"},
{"title": "Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An\n  Exploration of Scaling Laws by Difficulty", "author": "Zhichao Yang and Zhaoxin Fan and Gen Li and Yuanze Hu and Xinyu Wang and Ye Qiu and Xin Wang and Yifan Sun and Wenjun Wu", "abstract": "  Structured, procedural reasoning is essential for Large Language Models\n(LLMs), especially in mathematics. While post-training methods have improved\nLLM performance, they still fall short in capturing deep procedural logic on\ncomplex tasks. To tackle the issue, in this paper, we first investigate this\nlimitation and uncover a novel finding: a Scaling Law by Difficulty, which\nreveals that model performance follows a U-shaped curve with respect to\ntraining data complexity -- excessive low-difficulty data impedes abstraction,\nwhile high-difficulty data significantly enhances reasoning ability. Motivated\nby this, we propose the Structured Solution Template (SST) framework, which\nuses solution templates and a curriculum of varied difficulty to explicitly\nteach procedural reasoning. Specifically, SST comprises (1) fine-tuning with\nstructured solution-template chains and dynamically weighted loss to prioritize\nprocedural logic, (2) prompt-time injection of solution templates as cognitive\nscaffolds to guide inference, and (3) integrated curriculum fine-tuning that\nexplicitly teaches the model to self-plan - execute - self-correct. Experiments\non GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly\nimproves both accuracy and efficiency, especially on harder problems.\n", "link": "http://arxiv.org/abs/2508.19069v1", "date": "2025-08-26", "relevancy": 2.5261, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Structured%20Templates%20Facilitate%20LLMs%20in%20Tackling%20Harder%20Tasks%3F%20%3A%20An%0A%20%20Exploration%20of%20Scaling%20Laws%20by%20Difficulty&body=Title%3A%20Can%20Structured%20Templates%20Facilitate%20LLMs%20in%20Tackling%20Harder%20Tasks%3F%20%3A%20An%0A%20%20Exploration%20of%20Scaling%20Laws%20by%20Difficulty%0AAuthor%3A%20Zhichao%20Yang%20and%20Zhaoxin%20Fan%20and%20Gen%20Li%20and%20Yuanze%20Hu%20and%20Xinyu%20Wang%20and%20Ye%20Qiu%20and%20Xin%20Wang%20and%20Yifan%20Sun%20and%20Wenjun%20Wu%0AAbstract%3A%20%20%20Structured%2C%20procedural%20reasoning%20is%20essential%20for%20Large%20Language%20Models%0A%28LLMs%29%2C%20especially%20in%20mathematics.%20While%20post-training%20methods%20have%20improved%0ALLM%20performance%2C%20they%20still%20fall%20short%20in%20capturing%20deep%20procedural%20logic%20on%0Acomplex%20tasks.%20To%20tackle%20the%20issue%2C%20in%20this%20paper%2C%20we%20first%20investigate%20this%0Alimitation%20and%20uncover%20a%20novel%20finding%3A%20a%20Scaling%20Law%20by%20Difficulty%2C%20which%0Areveals%20that%20model%20performance%20follows%20a%20U-shaped%20curve%20with%20respect%20to%0Atraining%20data%20complexity%20--%20excessive%20low-difficulty%20data%20impedes%20abstraction%2C%0Awhile%20high-difficulty%20data%20significantly%20enhances%20reasoning%20ability.%20Motivated%0Aby%20this%2C%20we%20propose%20the%20Structured%20Solution%20Template%20%28SST%29%20framework%2C%20which%0Auses%20solution%20templates%20and%20a%20curriculum%20of%20varied%20difficulty%20to%20explicitly%0Ateach%20procedural%20reasoning.%20Specifically%2C%20SST%20comprises%20%281%29%20fine-tuning%20with%0Astructured%20solution-template%20chains%20and%20dynamically%20weighted%20loss%20to%20prioritize%0Aprocedural%20logic%2C%20%282%29%20prompt-time%20injection%20of%20solution%20templates%20as%20cognitive%0Ascaffolds%20to%20guide%20inference%2C%20and%20%283%29%20integrated%20curriculum%20fine-tuning%20that%0Aexplicitly%20teaches%20the%20model%20to%20self-plan%20-%20execute%20-%20self-correct.%20Experiments%0Aon%20GSM8K%2C%20AIME24%2C%20and%20new%20Dynamic%20En%20benchmark%20show%20that%20SST%20significantly%0Aimproves%20both%20accuracy%20and%20efficiency%2C%20especially%20on%20harder%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Structured%2520Templates%2520Facilitate%2520LLMs%2520in%2520Tackling%2520Harder%2520Tasks%253F%2520%253A%2520An%250A%2520%2520Exploration%2520of%2520Scaling%2520Laws%2520by%2520Difficulty%26entry.906535625%3DZhichao%2520Yang%2520and%2520Zhaoxin%2520Fan%2520and%2520Gen%2520Li%2520and%2520Yuanze%2520Hu%2520and%2520Xinyu%2520Wang%2520and%2520Ye%2520Qiu%2520and%2520Xin%2520Wang%2520and%2520Yifan%2520Sun%2520and%2520Wenjun%2520Wu%26entry.1292438233%3D%2520%2520Structured%252C%2520procedural%2520reasoning%2520is%2520essential%2520for%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520especially%2520in%2520mathematics.%2520While%2520post-training%2520methods%2520have%2520improved%250ALLM%2520performance%252C%2520they%2520still%2520fall%2520short%2520in%2520capturing%2520deep%2520procedural%2520logic%2520on%250Acomplex%2520tasks.%2520To%2520tackle%2520the%2520issue%252C%2520in%2520this%2520paper%252C%2520we%2520first%2520investigate%2520this%250Alimitation%2520and%2520uncover%2520a%2520novel%2520finding%253A%2520a%2520Scaling%2520Law%2520by%2520Difficulty%252C%2520which%250Areveals%2520that%2520model%2520performance%2520follows%2520a%2520U-shaped%2520curve%2520with%2520respect%2520to%250Atraining%2520data%2520complexity%2520--%2520excessive%2520low-difficulty%2520data%2520impedes%2520abstraction%252C%250Awhile%2520high-difficulty%2520data%2520significantly%2520enhances%2520reasoning%2520ability.%2520Motivated%250Aby%2520this%252C%2520we%2520propose%2520the%2520Structured%2520Solution%2520Template%2520%2528SST%2529%2520framework%252C%2520which%250Auses%2520solution%2520templates%2520and%2520a%2520curriculum%2520of%2520varied%2520difficulty%2520to%2520explicitly%250Ateach%2520procedural%2520reasoning.%2520Specifically%252C%2520SST%2520comprises%2520%25281%2529%2520fine-tuning%2520with%250Astructured%2520solution-template%2520chains%2520and%2520dynamically%2520weighted%2520loss%2520to%2520prioritize%250Aprocedural%2520logic%252C%2520%25282%2529%2520prompt-time%2520injection%2520of%2520solution%2520templates%2520as%2520cognitive%250Ascaffolds%2520to%2520guide%2520inference%252C%2520and%2520%25283%2529%2520integrated%2520curriculum%2520fine-tuning%2520that%250Aexplicitly%2520teaches%2520the%2520model%2520to%2520self-plan%2520-%2520execute%2520-%2520self-correct.%2520Experiments%250Aon%2520GSM8K%252C%2520AIME24%252C%2520and%2520new%2520Dynamic%2520En%2520benchmark%2520show%2520that%2520SST%2520significantly%250Aimproves%2520both%2520accuracy%2520and%2520efficiency%252C%2520especially%2520on%2520harder%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Structured%20Templates%20Facilitate%20LLMs%20in%20Tackling%20Harder%20Tasks%3F%20%3A%20An%0A%20%20Exploration%20of%20Scaling%20Laws%20by%20Difficulty&entry.906535625=Zhichao%20Yang%20and%20Zhaoxin%20Fan%20and%20Gen%20Li%20and%20Yuanze%20Hu%20and%20Xinyu%20Wang%20and%20Ye%20Qiu%20and%20Xin%20Wang%20and%20Yifan%20Sun%20and%20Wenjun%20Wu&entry.1292438233=%20%20Structured%2C%20procedural%20reasoning%20is%20essential%20for%20Large%20Language%20Models%0A%28LLMs%29%2C%20especially%20in%20mathematics.%20While%20post-training%20methods%20have%20improved%0ALLM%20performance%2C%20they%20still%20fall%20short%20in%20capturing%20deep%20procedural%20logic%20on%0Acomplex%20tasks.%20To%20tackle%20the%20issue%2C%20in%20this%20paper%2C%20we%20first%20investigate%20this%0Alimitation%20and%20uncover%20a%20novel%20finding%3A%20a%20Scaling%20Law%20by%20Difficulty%2C%20which%0Areveals%20that%20model%20performance%20follows%20a%20U-shaped%20curve%20with%20respect%20to%0Atraining%20data%20complexity%20--%20excessive%20low-difficulty%20data%20impedes%20abstraction%2C%0Awhile%20high-difficulty%20data%20significantly%20enhances%20reasoning%20ability.%20Motivated%0Aby%20this%2C%20we%20propose%20the%20Structured%20Solution%20Template%20%28SST%29%20framework%2C%20which%0Auses%20solution%20templates%20and%20a%20curriculum%20of%20varied%20difficulty%20to%20explicitly%0Ateach%20procedural%20reasoning.%20Specifically%2C%20SST%20comprises%20%281%29%20fine-tuning%20with%0Astructured%20solution-template%20chains%20and%20dynamically%20weighted%20loss%20to%20prioritize%0Aprocedural%20logic%2C%20%282%29%20prompt-time%20injection%20of%20solution%20templates%20as%20cognitive%0Ascaffolds%20to%20guide%20inference%2C%20and%20%283%29%20integrated%20curriculum%20fine-tuning%20that%0Aexplicitly%20teaches%20the%20model%20to%20self-plan%20-%20execute%20-%20self-correct.%20Experiments%0Aon%20GSM8K%2C%20AIME24%2C%20and%20new%20Dynamic%20En%20benchmark%20show%20that%20SST%20significantly%0Aimproves%20both%20accuracy%20and%20efficiency%2C%20especially%20on%20harder%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19069v1&entry.124074799=Read"},
{"title": "Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis", "author": "Kushal Raj Bhandari and Sixue Xing and Soham Dan and Jianxi Gao", "abstract": "  Large Language Models (LLMs), already shown to ace various unstructured text\ncomprehension tasks, have also remarkably been shown to tackle table\n(structured) comprehension tasks without specific training. Building on earlier\nstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),\nmodel scale, instruction tuning, and domain bias affect Tabular QA (TQA)\nrobustness by testing LLMs, under diverse augmentations and perturbations, on\ndiverse domains: Wikipedia-based $\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$,\nand scientific $\\textbf{SCITAB}$. Although instruction tuning and larger, newer\nLLMs deliver stronger, more robust TQA performance, data contamination and\nreliability issues, especially on $\\textbf{WTQ}$, remain unresolved. Through an\nin-depth attention analysis, we reveal a strong correlation between\nperturbation-induced shifts in attention dispersion and the drops in\nperformance, with sensitivity peaking in the model's middle layers. We\nhighlight the need for improved interpretable methodologies to develop more\nreliable LLMs for table comprehension. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and performance drops, with sensitivity peaking in the model's\nmiddle layers. Based on these findings, we argue for the development of\nstructure-aware self-attention mechanisms and domain-adaptive processing\ntechniques to improve the transparency, generalization, and real-world\nreliability of LLMs on tabular data.\n", "link": "http://arxiv.org/abs/2406.12719v4", "date": "2025-08-26", "relevancy": 2.5202, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Robustness%20of%20Language%20Models%20for%20Tabular%20Question%0A%20%20Answering%20via%20Attention%20Analysis&body=Title%3A%20Exploring%20the%20Robustness%20of%20Language%20Models%20for%20Tabular%20Question%0A%20%20Answering%20via%20Attention%20Analysis%0AAuthor%3A%20Kushal%20Raj%20Bhandari%20and%20Sixue%20Xing%20and%20Soham%20Dan%20and%20Jianxi%20Gao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%2C%20already%20shown%20to%20ace%20various%20unstructured%20text%0Acomprehension%20tasks%2C%20have%20also%20remarkably%20been%20shown%20to%20tackle%20table%0A%28structured%29%20comprehension%20tasks%20without%20specific%20training.%20Building%20on%20earlier%0Astudies%20of%20LLMs%20for%20tabular%20tasks%2C%20we%20probe%20how%20in-context%20learning%20%28ICL%29%2C%0Amodel%20scale%2C%20instruction%20tuning%2C%20and%20domain%20bias%20affect%20Tabular%20QA%20%28TQA%29%0Arobustness%20by%20testing%20LLMs%2C%20under%20diverse%20augmentations%20and%20perturbations%2C%20on%0Adiverse%20domains%3A%20Wikipedia-based%20%24%5Ctextbf%7BWTQ%7D%24%2C%20financial%20%24%5Ctextbf%7BTAT-QA%7D%24%2C%0Aand%20scientific%20%24%5Ctextbf%7BSCITAB%7D%24.%20Although%20instruction%20tuning%20and%20larger%2C%20newer%0ALLMs%20deliver%20stronger%2C%20more%20robust%20TQA%20performance%2C%20data%20contamination%20and%0Areliability%20issues%2C%20especially%20on%20%24%5Ctextbf%7BWTQ%7D%24%2C%20remain%20unresolved.%20Through%20an%0Ain-depth%20attention%20analysis%2C%20we%20reveal%20a%20strong%20correlation%20between%0Aperturbation-induced%20shifts%20in%20attention%20dispersion%20and%20the%20drops%20in%0Aperformance%2C%20with%20sensitivity%20peaking%20in%20the%20model%27s%20middle%20layers.%20We%0Ahighlight%20the%20need%20for%20improved%20interpretable%20methodologies%20to%20develop%20more%0Areliable%20LLMs%20for%20table%20comprehension.%20Through%20an%20in-depth%20attention%20analysis%2C%0Awe%20reveal%20a%20strong%20correlation%20between%20perturbation-induced%20shifts%20in%20attention%0Adispersion%20and%20performance%20drops%2C%20with%20sensitivity%20peaking%20in%20the%20model%27s%0Amiddle%20layers.%20Based%20on%20these%20findings%2C%20we%20argue%20for%20the%20development%20of%0Astructure-aware%20self-attention%20mechanisms%20and%20domain-adaptive%20processing%0Atechniques%20to%20improve%20the%20transparency%2C%20generalization%2C%20and%20real-world%0Areliability%20of%20LLMs%20on%20tabular%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12719v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Robustness%2520of%2520Language%2520Models%2520for%2520Tabular%2520Question%250A%2520%2520Answering%2520via%2520Attention%2520Analysis%26entry.906535625%3DKushal%2520Raj%2520Bhandari%2520and%2520Sixue%2520Xing%2520and%2520Soham%2520Dan%2520and%2520Jianxi%2520Gao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520already%2520shown%2520to%2520ace%2520various%2520unstructured%2520text%250Acomprehension%2520tasks%252C%2520have%2520also%2520remarkably%2520been%2520shown%2520to%2520tackle%2520table%250A%2528structured%2529%2520comprehension%2520tasks%2520without%2520specific%2520training.%2520Building%2520on%2520earlier%250Astudies%2520of%2520LLMs%2520for%2520tabular%2520tasks%252C%2520we%2520probe%2520how%2520in-context%2520learning%2520%2528ICL%2529%252C%250Amodel%2520scale%252C%2520instruction%2520tuning%252C%2520and%2520domain%2520bias%2520affect%2520Tabular%2520QA%2520%2528TQA%2529%250Arobustness%2520by%2520testing%2520LLMs%252C%2520under%2520diverse%2520augmentations%2520and%2520perturbations%252C%2520on%250Adiverse%2520domains%253A%2520Wikipedia-based%2520%2524%255Ctextbf%257BWTQ%257D%2524%252C%2520financial%2520%2524%255Ctextbf%257BTAT-QA%257D%2524%252C%250Aand%2520scientific%2520%2524%255Ctextbf%257BSCITAB%257D%2524.%2520Although%2520instruction%2520tuning%2520and%2520larger%252C%2520newer%250ALLMs%2520deliver%2520stronger%252C%2520more%2520robust%2520TQA%2520performance%252C%2520data%2520contamination%2520and%250Areliability%2520issues%252C%2520especially%2520on%2520%2524%255Ctextbf%257BWTQ%257D%2524%252C%2520remain%2520unresolved.%2520Through%2520an%250Ain-depth%2520attention%2520analysis%252C%2520we%2520reveal%2520a%2520strong%2520correlation%2520between%250Aperturbation-induced%2520shifts%2520in%2520attention%2520dispersion%2520and%2520the%2520drops%2520in%250Aperformance%252C%2520with%2520sensitivity%2520peaking%2520in%2520the%2520model%2527s%2520middle%2520layers.%2520We%250Ahighlight%2520the%2520need%2520for%2520improved%2520interpretable%2520methodologies%2520to%2520develop%2520more%250Areliable%2520LLMs%2520for%2520table%2520comprehension.%2520Through%2520an%2520in-depth%2520attention%2520analysis%252C%250Awe%2520reveal%2520a%2520strong%2520correlation%2520between%2520perturbation-induced%2520shifts%2520in%2520attention%250Adispersion%2520and%2520performance%2520drops%252C%2520with%2520sensitivity%2520peaking%2520in%2520the%2520model%2527s%250Amiddle%2520layers.%2520Based%2520on%2520these%2520findings%252C%2520we%2520argue%2520for%2520the%2520development%2520of%250Astructure-aware%2520self-attention%2520mechanisms%2520and%2520domain-adaptive%2520processing%250Atechniques%2520to%2520improve%2520the%2520transparency%252C%2520generalization%252C%2520and%2520real-world%250Areliability%2520of%2520LLMs%2520on%2520tabular%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12719v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Robustness%20of%20Language%20Models%20for%20Tabular%20Question%0A%20%20Answering%20via%20Attention%20Analysis&entry.906535625=Kushal%20Raj%20Bhandari%20and%20Sixue%20Xing%20and%20Soham%20Dan%20and%20Jianxi%20Gao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%2C%20already%20shown%20to%20ace%20various%20unstructured%20text%0Acomprehension%20tasks%2C%20have%20also%20remarkably%20been%20shown%20to%20tackle%20table%0A%28structured%29%20comprehension%20tasks%20without%20specific%20training.%20Building%20on%20earlier%0Astudies%20of%20LLMs%20for%20tabular%20tasks%2C%20we%20probe%20how%20in-context%20learning%20%28ICL%29%2C%0Amodel%20scale%2C%20instruction%20tuning%2C%20and%20domain%20bias%20affect%20Tabular%20QA%20%28TQA%29%0Arobustness%20by%20testing%20LLMs%2C%20under%20diverse%20augmentations%20and%20perturbations%2C%20on%0Adiverse%20domains%3A%20Wikipedia-based%20%24%5Ctextbf%7BWTQ%7D%24%2C%20financial%20%24%5Ctextbf%7BTAT-QA%7D%24%2C%0Aand%20scientific%20%24%5Ctextbf%7BSCITAB%7D%24.%20Although%20instruction%20tuning%20and%20larger%2C%20newer%0ALLMs%20deliver%20stronger%2C%20more%20robust%20TQA%20performance%2C%20data%20contamination%20and%0Areliability%20issues%2C%20especially%20on%20%24%5Ctextbf%7BWTQ%7D%24%2C%20remain%20unresolved.%20Through%20an%0Ain-depth%20attention%20analysis%2C%20we%20reveal%20a%20strong%20correlation%20between%0Aperturbation-induced%20shifts%20in%20attention%20dispersion%20and%20the%20drops%20in%0Aperformance%2C%20with%20sensitivity%20peaking%20in%20the%20model%27s%20middle%20layers.%20We%0Ahighlight%20the%20need%20for%20improved%20interpretable%20methodologies%20to%20develop%20more%0Areliable%20LLMs%20for%20table%20comprehension.%20Through%20an%20in-depth%20attention%20analysis%2C%0Awe%20reveal%20a%20strong%20correlation%20between%20perturbation-induced%20shifts%20in%20attention%0Adispersion%20and%20performance%20drops%2C%20with%20sensitivity%20peaking%20in%20the%20model%27s%0Amiddle%20layers.%20Based%20on%20these%20findings%2C%20we%20argue%20for%20the%20development%20of%0Astructure-aware%20self-attention%20mechanisms%20and%20domain-adaptive%20processing%0Atechniques%20to%20improve%20the%20transparency%2C%20generalization%2C%20and%20real-world%0Areliability%20of%20LLMs%20on%20tabular%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12719v4&entry.124074799=Read"},
{"title": "LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding", "author": "Julian Ost and Andrea Ramazzina and Amogh Joshi and Maximilian B\u00f6mer and Mario Bijelic and Felix Heide", "abstract": "  Large-scale scene data is essential for training and testing in robot\nlearning. Neural reconstruction methods have promised the capability of\nreconstructing large physically-grounded outdoor scenes from captured sensor\ndata. However, these methods have baked-in static environments and only allow\nfor limited scene control -- they are functionally constrained in scene and\ntrajectory diversity by the captures from which they are reconstructed. In\ncontrast, generating driving data with recent image or video diffusion models\noffers control, however, at the cost of geometry grounding and causality. In\nthis work, we aim to bridge this gap and present a method that directly\ngenerates large-scale 3D driving scenes with accurate geometry, allowing for\ncausal novel view synthesis with object permanence and explicit 3D geometry\nestimation. The proposed method combines the generation of a proxy geometry and\nenvironment representation with score distillation from learned 2D image\npriors. We find that this approach allows for high controllability, enabling\nthe prompt-guided geometry and high-fidelity texture and structure that can be\nconditioned on map layouts -- producing realistic and geometrically consistent\n3D generations of complex driving scenes.\n", "link": "http://arxiv.org/abs/2508.19204v1", "date": "2025-08-26", "relevancy": 2.5199, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.637}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6286}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LSD-3D%3A%20Large-Scale%203D%20Driving%20Scene%20Generation%20with%20Geometry%20Grounding&body=Title%3A%20LSD-3D%3A%20Large-Scale%203D%20Driving%20Scene%20Generation%20with%20Geometry%20Grounding%0AAuthor%3A%20Julian%20Ost%20and%20Andrea%20Ramazzina%20and%20Amogh%20Joshi%20and%20Maximilian%20B%C3%B6mer%20and%20Mario%20Bijelic%20and%20Felix%20Heide%0AAbstract%3A%20%20%20Large-scale%20scene%20data%20is%20essential%20for%20training%20and%20testing%20in%20robot%0Alearning.%20Neural%20reconstruction%20methods%20have%20promised%20the%20capability%20of%0Areconstructing%20large%20physically-grounded%20outdoor%20scenes%20from%20captured%20sensor%0Adata.%20However%2C%20these%20methods%20have%20baked-in%20static%20environments%20and%20only%20allow%0Afor%20limited%20scene%20control%20--%20they%20are%20functionally%20constrained%20in%20scene%20and%0Atrajectory%20diversity%20by%20the%20captures%20from%20which%20they%20are%20reconstructed.%20In%0Acontrast%2C%20generating%20driving%20data%20with%20recent%20image%20or%20video%20diffusion%20models%0Aoffers%20control%2C%20however%2C%20at%20the%20cost%20of%20geometry%20grounding%20and%20causality.%20In%0Athis%20work%2C%20we%20aim%20to%20bridge%20this%20gap%20and%20present%20a%20method%20that%20directly%0Agenerates%20large-scale%203D%20driving%20scenes%20with%20accurate%20geometry%2C%20allowing%20for%0Acausal%20novel%20view%20synthesis%20with%20object%20permanence%20and%20explicit%203D%20geometry%0Aestimation.%20The%20proposed%20method%20combines%20the%20generation%20of%20a%20proxy%20geometry%20and%0Aenvironment%20representation%20with%20score%20distillation%20from%20learned%202D%20image%0Apriors.%20We%20find%20that%20this%20approach%20allows%20for%20high%20controllability%2C%20enabling%0Athe%20prompt-guided%20geometry%20and%20high-fidelity%20texture%20and%20structure%20that%20can%20be%0Aconditioned%20on%20map%20layouts%20--%20producing%20realistic%20and%20geometrically%20consistent%0A3D%20generations%20of%20complex%20driving%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLSD-3D%253A%2520Large-Scale%25203D%2520Driving%2520Scene%2520Generation%2520with%2520Geometry%2520Grounding%26entry.906535625%3DJulian%2520Ost%2520and%2520Andrea%2520Ramazzina%2520and%2520Amogh%2520Joshi%2520and%2520Maximilian%2520B%25C3%25B6mer%2520and%2520Mario%2520Bijelic%2520and%2520Felix%2520Heide%26entry.1292438233%3D%2520%2520Large-scale%2520scene%2520data%2520is%2520essential%2520for%2520training%2520and%2520testing%2520in%2520robot%250Alearning.%2520Neural%2520reconstruction%2520methods%2520have%2520promised%2520the%2520capability%2520of%250Areconstructing%2520large%2520physically-grounded%2520outdoor%2520scenes%2520from%2520captured%2520sensor%250Adata.%2520However%252C%2520these%2520methods%2520have%2520baked-in%2520static%2520environments%2520and%2520only%2520allow%250Afor%2520limited%2520scene%2520control%2520--%2520they%2520are%2520functionally%2520constrained%2520in%2520scene%2520and%250Atrajectory%2520diversity%2520by%2520the%2520captures%2520from%2520which%2520they%2520are%2520reconstructed.%2520In%250Acontrast%252C%2520generating%2520driving%2520data%2520with%2520recent%2520image%2520or%2520video%2520diffusion%2520models%250Aoffers%2520control%252C%2520however%252C%2520at%2520the%2520cost%2520of%2520geometry%2520grounding%2520and%2520causality.%2520In%250Athis%2520work%252C%2520we%2520aim%2520to%2520bridge%2520this%2520gap%2520and%2520present%2520a%2520method%2520that%2520directly%250Agenerates%2520large-scale%25203D%2520driving%2520scenes%2520with%2520accurate%2520geometry%252C%2520allowing%2520for%250Acausal%2520novel%2520view%2520synthesis%2520with%2520object%2520permanence%2520and%2520explicit%25203D%2520geometry%250Aestimation.%2520The%2520proposed%2520method%2520combines%2520the%2520generation%2520of%2520a%2520proxy%2520geometry%2520and%250Aenvironment%2520representation%2520with%2520score%2520distillation%2520from%2520learned%25202D%2520image%250Apriors.%2520We%2520find%2520that%2520this%2520approach%2520allows%2520for%2520high%2520controllability%252C%2520enabling%250Athe%2520prompt-guided%2520geometry%2520and%2520high-fidelity%2520texture%2520and%2520structure%2520that%2520can%2520be%250Aconditioned%2520on%2520map%2520layouts%2520--%2520producing%2520realistic%2520and%2520geometrically%2520consistent%250A3D%2520generations%2520of%2520complex%2520driving%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSD-3D%3A%20Large-Scale%203D%20Driving%20Scene%20Generation%20with%20Geometry%20Grounding&entry.906535625=Julian%20Ost%20and%20Andrea%20Ramazzina%20and%20Amogh%20Joshi%20and%20Maximilian%20B%C3%B6mer%20and%20Mario%20Bijelic%20and%20Felix%20Heide&entry.1292438233=%20%20Large-scale%20scene%20data%20is%20essential%20for%20training%20and%20testing%20in%20robot%0Alearning.%20Neural%20reconstruction%20methods%20have%20promised%20the%20capability%20of%0Areconstructing%20large%20physically-grounded%20outdoor%20scenes%20from%20captured%20sensor%0Adata.%20However%2C%20these%20methods%20have%20baked-in%20static%20environments%20and%20only%20allow%0Afor%20limited%20scene%20control%20--%20they%20are%20functionally%20constrained%20in%20scene%20and%0Atrajectory%20diversity%20by%20the%20captures%20from%20which%20they%20are%20reconstructed.%20In%0Acontrast%2C%20generating%20driving%20data%20with%20recent%20image%20or%20video%20diffusion%20models%0Aoffers%20control%2C%20however%2C%20at%20the%20cost%20of%20geometry%20grounding%20and%20causality.%20In%0Athis%20work%2C%20we%20aim%20to%20bridge%20this%20gap%20and%20present%20a%20method%20that%20directly%0Agenerates%20large-scale%203D%20driving%20scenes%20with%20accurate%20geometry%2C%20allowing%20for%0Acausal%20novel%20view%20synthesis%20with%20object%20permanence%20and%20explicit%203D%20geometry%0Aestimation.%20The%20proposed%20method%20combines%20the%20generation%20of%20a%20proxy%20geometry%20and%0Aenvironment%20representation%20with%20score%20distillation%20from%20learned%202D%20image%0Apriors.%20We%20find%20that%20this%20approach%20allows%20for%20high%20controllability%2C%20enabling%0Athe%20prompt-guided%20geometry%20and%20high-fidelity%20texture%20and%20structure%20that%20can%20be%0Aconditioned%20on%20map%20layouts%20--%20producing%20realistic%20and%20geometrically%20consistent%0A3D%20generations%20of%20complex%20driving%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19204v1&entry.124074799=Read"},
{"title": "Ask Me Again Differently: GRAS for Measuring Bias in Vision Language\n  Models on Gender, Race, Age, and Skin Tone", "author": "Shaivi Malik and Hasnat Md Abdullah and Sriparna Saha and Amit Sheth", "abstract": "  As Vision Language Models (VLMs) become integral to real-world applications,\nunderstanding their demographic biases is critical. We introduce GRAS, a\nbenchmark for uncovering demographic biases in VLMs across gender, race, age,\nand skin tone, offering the most diverse coverage to date. We further propose\nthe GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark\nfive state-of-the-art VLMs and reveal concerning bias levels, with the least\nbiased model attaining a GRAS Bias Score of only 2 out of 100. Our findings\nalso reveal a methodological insight: evaluating bias in VLMs with visual\nquestion answering (VQA) requires considering multiple formulations of a\nquestion. Our code, data, and evaluation results are publicly available.\n", "link": "http://arxiv.org/abs/2508.18989v1", "date": "2025-08-26", "relevancy": 2.5177, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5189}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ask%20Me%20Again%20Differently%3A%20GRAS%20for%20Measuring%20Bias%20in%20Vision%20Language%0A%20%20Models%20on%20Gender%2C%20Race%2C%20Age%2C%20and%20Skin%20Tone&body=Title%3A%20Ask%20Me%20Again%20Differently%3A%20GRAS%20for%20Measuring%20Bias%20in%20Vision%20Language%0A%20%20Models%20on%20Gender%2C%20Race%2C%20Age%2C%20and%20Skin%20Tone%0AAuthor%3A%20Shaivi%20Malik%20and%20Hasnat%20Md%20Abdullah%20and%20Sriparna%20Saha%20and%20Amit%20Sheth%0AAbstract%3A%20%20%20As%20Vision%20Language%20Models%20%28VLMs%29%20become%20integral%20to%20real-world%20applications%2C%0Aunderstanding%20their%20demographic%20biases%20is%20critical.%20We%20introduce%20GRAS%2C%20a%0Abenchmark%20for%20uncovering%20demographic%20biases%20in%20VLMs%20across%20gender%2C%20race%2C%20age%2C%0Aand%20skin%20tone%2C%20offering%20the%20most%20diverse%20coverage%20to%20date.%20We%20further%20propose%0Athe%20GRAS%20Bias%20Score%2C%20an%20interpretable%20metric%20for%20quantifying%20bias.%20We%20benchmark%0Afive%20state-of-the-art%20VLMs%20and%20reveal%20concerning%20bias%20levels%2C%20with%20the%20least%0Abiased%20model%20attaining%20a%20GRAS%20Bias%20Score%20of%20only%202%20out%20of%20100.%20Our%20findings%0Aalso%20reveal%20a%20methodological%20insight%3A%20evaluating%20bias%20in%20VLMs%20with%20visual%0Aquestion%20answering%20%28VQA%29%20requires%20considering%20multiple%20formulations%20of%20a%0Aquestion.%20Our%20code%2C%20data%2C%20and%20evaluation%20results%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsk%2520Me%2520Again%2520Differently%253A%2520GRAS%2520for%2520Measuring%2520Bias%2520in%2520Vision%2520Language%250A%2520%2520Models%2520on%2520Gender%252C%2520Race%252C%2520Age%252C%2520and%2520Skin%2520Tone%26entry.906535625%3DShaivi%2520Malik%2520and%2520Hasnat%2520Md%2520Abdullah%2520and%2520Sriparna%2520Saha%2520and%2520Amit%2520Sheth%26entry.1292438233%3D%2520%2520As%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520become%2520integral%2520to%2520real-world%2520applications%252C%250Aunderstanding%2520their%2520demographic%2520biases%2520is%2520critical.%2520We%2520introduce%2520GRAS%252C%2520a%250Abenchmark%2520for%2520uncovering%2520demographic%2520biases%2520in%2520VLMs%2520across%2520gender%252C%2520race%252C%2520age%252C%250Aand%2520skin%2520tone%252C%2520offering%2520the%2520most%2520diverse%2520coverage%2520to%2520date.%2520We%2520further%2520propose%250Athe%2520GRAS%2520Bias%2520Score%252C%2520an%2520interpretable%2520metric%2520for%2520quantifying%2520bias.%2520We%2520benchmark%250Afive%2520state-of-the-art%2520VLMs%2520and%2520reveal%2520concerning%2520bias%2520levels%252C%2520with%2520the%2520least%250Abiased%2520model%2520attaining%2520a%2520GRAS%2520Bias%2520Score%2520of%2520only%25202%2520out%2520of%2520100.%2520Our%2520findings%250Aalso%2520reveal%2520a%2520methodological%2520insight%253A%2520evaluating%2520bias%2520in%2520VLMs%2520with%2520visual%250Aquestion%2520answering%2520%2528VQA%2529%2520requires%2520considering%2520multiple%2520formulations%2520of%2520a%250Aquestion.%2520Our%2520code%252C%2520data%252C%2520and%2520evaluation%2520results%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ask%20Me%20Again%20Differently%3A%20GRAS%20for%20Measuring%20Bias%20in%20Vision%20Language%0A%20%20Models%20on%20Gender%2C%20Race%2C%20Age%2C%20and%20Skin%20Tone&entry.906535625=Shaivi%20Malik%20and%20Hasnat%20Md%20Abdullah%20and%20Sriparna%20Saha%20and%20Amit%20Sheth&entry.1292438233=%20%20As%20Vision%20Language%20Models%20%28VLMs%29%20become%20integral%20to%20real-world%20applications%2C%0Aunderstanding%20their%20demographic%20biases%20is%20critical.%20We%20introduce%20GRAS%2C%20a%0Abenchmark%20for%20uncovering%20demographic%20biases%20in%20VLMs%20across%20gender%2C%20race%2C%20age%2C%0Aand%20skin%20tone%2C%20offering%20the%20most%20diverse%20coverage%20to%20date.%20We%20further%20propose%0Athe%20GRAS%20Bias%20Score%2C%20an%20interpretable%20metric%20for%20quantifying%20bias.%20We%20benchmark%0Afive%20state-of-the-art%20VLMs%20and%20reveal%20concerning%20bias%20levels%2C%20with%20the%20least%0Abiased%20model%20attaining%20a%20GRAS%20Bias%20Score%20of%20only%202%20out%20of%20100.%20Our%20findings%0Aalso%20reveal%20a%20methodological%20insight%3A%20evaluating%20bias%20in%20VLMs%20with%20visual%0Aquestion%20answering%20%28VQA%29%20requires%20considering%20multiple%20formulations%20of%20a%0Aquestion.%20Our%20code%2C%20data%2C%20and%20evaluation%20results%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18989v1&entry.124074799=Read"},
{"title": "Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural\n  Models", "author": "Hung Ming Liu", "abstract": "  We present a framework where neural models develop an AI Mother Tongue, a\nnative symbolic language that simultaneously supports intuitive reasoning,\ncompositional symbol chains, and inherent interpretability. Unlike post-hoc\nexplanation methods, our approach embeds reasoning directly into the model's\nrepresentations: symbols capture meaningful semantic patterns, chains trace\ndecision paths, and gated induction mechanisms guide selective focus, yielding\ntransparent yet flexible reasoning. We introduce complementary training\nobjectives to enhance symbol purity and decision sparsity, and employ a\nsequential specialization strategy to first build broad symbolic competence and\nthen refine intuitive judgments. Experiments on AI tasks demonstrate\ncompetitive accuracy alongside verifiable reasoning traces, showing that AI\nMother Tongue can serve as a unified mechanism for interpretability, intuition,\nand symbolic reasoning in neural models.\n", "link": "http://arxiv.org/abs/2508.18988v1", "date": "2025-08-26", "relevancy": 2.5137, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5164}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20by%20AI%20Mother%20Tongue%3A%20Native%20Symbolic%20Reasoning%20in%20Neural%0A%20%20Models&body=Title%3A%20Interpretable%20by%20AI%20Mother%20Tongue%3A%20Native%20Symbolic%20Reasoning%20in%20Neural%0A%20%20Models%0AAuthor%3A%20Hung%20Ming%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20framework%20where%20neural%20models%20develop%20an%20AI%20Mother%20Tongue%2C%20a%0Anative%20symbolic%20language%20that%20simultaneously%20supports%20intuitive%20reasoning%2C%0Acompositional%20symbol%20chains%2C%20and%20inherent%20interpretability.%20Unlike%20post-hoc%0Aexplanation%20methods%2C%20our%20approach%20embeds%20reasoning%20directly%20into%20the%20model%27s%0Arepresentations%3A%20symbols%20capture%20meaningful%20semantic%20patterns%2C%20chains%20trace%0Adecision%20paths%2C%20and%20gated%20induction%20mechanisms%20guide%20selective%20focus%2C%20yielding%0Atransparent%20yet%20flexible%20reasoning.%20We%20introduce%20complementary%20training%0Aobjectives%20to%20enhance%20symbol%20purity%20and%20decision%20sparsity%2C%20and%20employ%20a%0Asequential%20specialization%20strategy%20to%20first%20build%20broad%20symbolic%20competence%20and%0Athen%20refine%20intuitive%20judgments.%20Experiments%20on%20AI%20tasks%20demonstrate%0Acompetitive%20accuracy%20alongside%20verifiable%20reasoning%20traces%2C%20showing%20that%20AI%0AMother%20Tongue%20can%20serve%20as%20a%20unified%20mechanism%20for%20interpretability%2C%20intuition%2C%0Aand%20symbolic%20reasoning%20in%20neural%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520by%2520AI%2520Mother%2520Tongue%253A%2520Native%2520Symbolic%2520Reasoning%2520in%2520Neural%250A%2520%2520Models%26entry.906535625%3DHung%2520Ming%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520framework%2520where%2520neural%2520models%2520develop%2520an%2520AI%2520Mother%2520Tongue%252C%2520a%250Anative%2520symbolic%2520language%2520that%2520simultaneously%2520supports%2520intuitive%2520reasoning%252C%250Acompositional%2520symbol%2520chains%252C%2520and%2520inherent%2520interpretability.%2520Unlike%2520post-hoc%250Aexplanation%2520methods%252C%2520our%2520approach%2520embeds%2520reasoning%2520directly%2520into%2520the%2520model%2527s%250Arepresentations%253A%2520symbols%2520capture%2520meaningful%2520semantic%2520patterns%252C%2520chains%2520trace%250Adecision%2520paths%252C%2520and%2520gated%2520induction%2520mechanisms%2520guide%2520selective%2520focus%252C%2520yielding%250Atransparent%2520yet%2520flexible%2520reasoning.%2520We%2520introduce%2520complementary%2520training%250Aobjectives%2520to%2520enhance%2520symbol%2520purity%2520and%2520decision%2520sparsity%252C%2520and%2520employ%2520a%250Asequential%2520specialization%2520strategy%2520to%2520first%2520build%2520broad%2520symbolic%2520competence%2520and%250Athen%2520refine%2520intuitive%2520judgments.%2520Experiments%2520on%2520AI%2520tasks%2520demonstrate%250Acompetitive%2520accuracy%2520alongside%2520verifiable%2520reasoning%2520traces%252C%2520showing%2520that%2520AI%250AMother%2520Tongue%2520can%2520serve%2520as%2520a%2520unified%2520mechanism%2520for%2520interpretability%252C%2520intuition%252C%250Aand%2520symbolic%2520reasoning%2520in%2520neural%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20by%20AI%20Mother%20Tongue%3A%20Native%20Symbolic%20Reasoning%20in%20Neural%0A%20%20Models&entry.906535625=Hung%20Ming%20Liu&entry.1292438233=%20%20We%20present%20a%20framework%20where%20neural%20models%20develop%20an%20AI%20Mother%20Tongue%2C%20a%0Anative%20symbolic%20language%20that%20simultaneously%20supports%20intuitive%20reasoning%2C%0Acompositional%20symbol%20chains%2C%20and%20inherent%20interpretability.%20Unlike%20post-hoc%0Aexplanation%20methods%2C%20our%20approach%20embeds%20reasoning%20directly%20into%20the%20model%27s%0Arepresentations%3A%20symbols%20capture%20meaningful%20semantic%20patterns%2C%20chains%20trace%0Adecision%20paths%2C%20and%20gated%20induction%20mechanisms%20guide%20selective%20focus%2C%20yielding%0Atransparent%20yet%20flexible%20reasoning.%20We%20introduce%20complementary%20training%0Aobjectives%20to%20enhance%20symbol%20purity%20and%20decision%20sparsity%2C%20and%20employ%20a%0Asequential%20specialization%20strategy%20to%20first%20build%20broad%20symbolic%20competence%20and%0Athen%20refine%20intuitive%20judgments.%20Experiments%20on%20AI%20tasks%20demonstrate%0Acompetitive%20accuracy%20alongside%20verifiable%20reasoning%20traces%2C%20showing%20that%20AI%0AMother%20Tongue%20can%20serve%20as%20a%20unified%20mechanism%20for%20interpretability%2C%20intuition%2C%0Aand%20symbolic%20reasoning%20in%20neural%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18988v1&entry.124074799=Read"},
{"title": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging\n  Counterfactual Augmentation", "author": "David Egea and Barproda Halder and Sanghamitra Dutta", "abstract": "  Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis.\n", "link": "http://arxiv.org/abs/2508.18933v1", "date": "2025-08-26", "relevancy": 2.5122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VISION%3A%20Robust%20and%20Interpretable%20Code%20Vulnerability%20Detection%20Leveraging%0A%20%20Counterfactual%20Augmentation&body=Title%3A%20VISION%3A%20Robust%20and%20Interpretable%20Code%20Vulnerability%20Detection%20Leveraging%0A%20%20Counterfactual%20Augmentation%0AAuthor%3A%20David%20Egea%20and%20Barproda%20Halder%20and%20Sanghamitra%20Dutta%0AAbstract%3A%20%20%20Automated%20detection%20of%20vulnerabilities%20in%20source%20code%20is%20an%20essential%0Acybersecurity%20challenge%2C%20underpinning%20trust%20in%20digital%20systems%20and%20services.%0AGraph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20a%20promising%20approach%20as%20they%20can%0Alearn%20structural%20and%20logical%20code%20relationships%20in%20a%20data-driven%20manner.%0AHowever%2C%20their%20performance%20is%20severely%20constrained%20by%20training%20data%20imbalances%0Aand%20label%20noise.%20GNNs%20often%20learn%20%27spurious%27%20correlations%20from%20superficial%20code%0Asimilarities%2C%20producing%20detectors%20that%20fail%20to%20generalize%20well%20to%20unseen%0Areal-world%20data.%20In%20this%20work%2C%20we%20propose%20a%20unified%20framework%20for%20robust%20and%0Ainterpretable%20vulnerability%20detection%2C%20called%20VISION%2C%20to%20mitigate%20spurious%0Acorrelations%20by%20systematically%20augmenting%20a%20counterfactual%20training%20dataset.%0ACounterfactuals%20are%20samples%20with%20minimal%20semantic%20modifications%20but%20opposite%0Alabels.%20Our%20framework%20includes%3A%20%28i%29%20generating%20counterfactuals%20by%20prompting%20a%0ALarge%20Language%20Model%20%28LLM%29%3B%20%28ii%29%20targeted%20GNN%20training%20on%20paired%20code%20examples%0Awith%20opposite%20labels%3B%20and%20%28iii%29%20graph-based%20interpretability%20to%20identify%20the%0Acrucial%20code%20statements%20relevant%20for%20vulnerability%20predictions%20while%20ignoring%0Aspurious%20ones.%20We%20find%20that%20VISION%20reduces%20spurious%20learning%20and%20enables%20more%0Arobust%2C%20generalizable%20detection%2C%20improving%20overall%20accuracy%20%28from%2051.8%25%20to%0A97.8%25%29%2C%20pairwise%20contrast%20accuracy%20%28from%204.5%25%20to%2095.8%25%29%2C%20and%20worst-group%0Aaccuracy%20%28from%200.7%25%20to%2085.5%25%29%20on%20the%20Common%20Weakness%20Enumeration%20%28CWE%29-20%0Avulnerability.%20We%20further%20demonstrate%20gains%20using%20proposed%20metrics%3A%20intra-class%0Aattribution%20variance%2C%20inter-class%20attribution%20distance%2C%20and%20node%20score%0Adependency.%20We%20also%20release%20CWE-20-CFA%2C%20a%20benchmark%20of%2027%2C556%20functions%20%28real%0Aand%20counterfactual%29%20from%20the%20high-impact%20CWE-20%20category.%20Finally%2C%20VISION%0Aadvances%20transparent%20and%20trustworthy%20AI-based%20cybersecurity%20systems%20through%0Ainteractive%20visualization%20for%20human-in-the-loop%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVISION%253A%2520Robust%2520and%2520Interpretable%2520Code%2520Vulnerability%2520Detection%2520Leveraging%250A%2520%2520Counterfactual%2520Augmentation%26entry.906535625%3DDavid%2520Egea%2520and%2520Barproda%2520Halder%2520and%2520Sanghamitra%2520Dutta%26entry.1292438233%3D%2520%2520Automated%2520detection%2520of%2520vulnerabilities%2520in%2520source%2520code%2520is%2520an%2520essential%250Acybersecurity%2520challenge%252C%2520underpinning%2520trust%2520in%2520digital%2520systems%2520and%2520services.%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520as%2520they%2520can%250Alearn%2520structural%2520and%2520logical%2520code%2520relationships%2520in%2520a%2520data-driven%2520manner.%250AHowever%252C%2520their%2520performance%2520is%2520severely%2520constrained%2520by%2520training%2520data%2520imbalances%250Aand%2520label%2520noise.%2520GNNs%2520often%2520learn%2520%2527spurious%2527%2520correlations%2520from%2520superficial%2520code%250Asimilarities%252C%2520producing%2520detectors%2520that%2520fail%2520to%2520generalize%2520well%2520to%2520unseen%250Areal-world%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520unified%2520framework%2520for%2520robust%2520and%250Ainterpretable%2520vulnerability%2520detection%252C%2520called%2520VISION%252C%2520to%2520mitigate%2520spurious%250Acorrelations%2520by%2520systematically%2520augmenting%2520a%2520counterfactual%2520training%2520dataset.%250ACounterfactuals%2520are%2520samples%2520with%2520minimal%2520semantic%2520modifications%2520but%2520opposite%250Alabels.%2520Our%2520framework%2520includes%253A%2520%2528i%2529%2520generating%2520counterfactuals%2520by%2520prompting%2520a%250ALarge%2520Language%2520Model%2520%2528LLM%2529%253B%2520%2528ii%2529%2520targeted%2520GNN%2520training%2520on%2520paired%2520code%2520examples%250Awith%2520opposite%2520labels%253B%2520and%2520%2528iii%2529%2520graph-based%2520interpretability%2520to%2520identify%2520the%250Acrucial%2520code%2520statements%2520relevant%2520for%2520vulnerability%2520predictions%2520while%2520ignoring%250Aspurious%2520ones.%2520We%2520find%2520that%2520VISION%2520reduces%2520spurious%2520learning%2520and%2520enables%2520more%250Arobust%252C%2520generalizable%2520detection%252C%2520improving%2520overall%2520accuracy%2520%2528from%252051.8%2525%2520to%250A97.8%2525%2529%252C%2520pairwise%2520contrast%2520accuracy%2520%2528from%25204.5%2525%2520to%252095.8%2525%2529%252C%2520and%2520worst-group%250Aaccuracy%2520%2528from%25200.7%2525%2520to%252085.5%2525%2529%2520on%2520the%2520Common%2520Weakness%2520Enumeration%2520%2528CWE%2529-20%250Avulnerability.%2520We%2520further%2520demonstrate%2520gains%2520using%2520proposed%2520metrics%253A%2520intra-class%250Aattribution%2520variance%252C%2520inter-class%2520attribution%2520distance%252C%2520and%2520node%2520score%250Adependency.%2520We%2520also%2520release%2520CWE-20-CFA%252C%2520a%2520benchmark%2520of%252027%252C556%2520functions%2520%2528real%250Aand%2520counterfactual%2529%2520from%2520the%2520high-impact%2520CWE-20%2520category.%2520Finally%252C%2520VISION%250Aadvances%2520transparent%2520and%2520trustworthy%2520AI-based%2520cybersecurity%2520systems%2520through%250Ainteractive%2520visualization%2520for%2520human-in-the-loop%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VISION%3A%20Robust%20and%20Interpretable%20Code%20Vulnerability%20Detection%20Leveraging%0A%20%20Counterfactual%20Augmentation&entry.906535625=David%20Egea%20and%20Barproda%20Halder%20and%20Sanghamitra%20Dutta&entry.1292438233=%20%20Automated%20detection%20of%20vulnerabilities%20in%20source%20code%20is%20an%20essential%0Acybersecurity%20challenge%2C%20underpinning%20trust%20in%20digital%20systems%20and%20services.%0AGraph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20a%20promising%20approach%20as%20they%20can%0Alearn%20structural%20and%20logical%20code%20relationships%20in%20a%20data-driven%20manner.%0AHowever%2C%20their%20performance%20is%20severely%20constrained%20by%20training%20data%20imbalances%0Aand%20label%20noise.%20GNNs%20often%20learn%20%27spurious%27%20correlations%20from%20superficial%20code%0Asimilarities%2C%20producing%20detectors%20that%20fail%20to%20generalize%20well%20to%20unseen%0Areal-world%20data.%20In%20this%20work%2C%20we%20propose%20a%20unified%20framework%20for%20robust%20and%0Ainterpretable%20vulnerability%20detection%2C%20called%20VISION%2C%20to%20mitigate%20spurious%0Acorrelations%20by%20systematically%20augmenting%20a%20counterfactual%20training%20dataset.%0ACounterfactuals%20are%20samples%20with%20minimal%20semantic%20modifications%20but%20opposite%0Alabels.%20Our%20framework%20includes%3A%20%28i%29%20generating%20counterfactuals%20by%20prompting%20a%0ALarge%20Language%20Model%20%28LLM%29%3B%20%28ii%29%20targeted%20GNN%20training%20on%20paired%20code%20examples%0Awith%20opposite%20labels%3B%20and%20%28iii%29%20graph-based%20interpretability%20to%20identify%20the%0Acrucial%20code%20statements%20relevant%20for%20vulnerability%20predictions%20while%20ignoring%0Aspurious%20ones.%20We%20find%20that%20VISION%20reduces%20spurious%20learning%20and%20enables%20more%0Arobust%2C%20generalizable%20detection%2C%20improving%20overall%20accuracy%20%28from%2051.8%25%20to%0A97.8%25%29%2C%20pairwise%20contrast%20accuracy%20%28from%204.5%25%20to%2095.8%25%29%2C%20and%20worst-group%0Aaccuracy%20%28from%200.7%25%20to%2085.5%25%29%20on%20the%20Common%20Weakness%20Enumeration%20%28CWE%29-20%0Avulnerability.%20We%20further%20demonstrate%20gains%20using%20proposed%20metrics%3A%20intra-class%0Aattribution%20variance%2C%20inter-class%20attribution%20distance%2C%20and%20node%20score%0Adependency.%20We%20also%20release%20CWE-20-CFA%2C%20a%20benchmark%20of%2027%2C556%20functions%20%28real%0Aand%20counterfactual%29%20from%20the%20high-impact%20CWE-20%20category.%20Finally%2C%20VISION%0Aadvances%20transparent%20and%20trustworthy%20AI-based%20cybersecurity%20systems%20through%0Ainteractive%20visualization%20for%20human-in-the-loop%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18933v1&entry.124074799=Read"},
{"title": "Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025", "author": "Thien-Phuc Tran and Minh-Quang Nguyen and Minh-Triet Tran and Tam V. Nguyen and Trong-Le Do and Duy-Nam Ly and Viet-Tham Huynh and Khanh-Duy Le and Mai-Khiem Tran and Trung-Nghia Le", "abstract": "  The Event-Enriched Image Analysis (EVENTA) Grand Challenge, hosted at ACM\nMultimedia 2025, introduces the first large-scale benchmark for event-level\nmultimodal understanding. Traditional captioning and retrieval tasks largely\nfocus on surface-level recognition of people, objects, and scenes, often\noverlooking the contextual and semantic dimensions that define real-world\nevents. EVENTA addresses this gap by integrating contextual, temporal, and\nsemantic information to capture the who, when, where, what, and why behind an\nimage. Built upon the OpenEvents V1 dataset, the challenge features two tracks:\nEvent-Enriched Image Retrieval and Captioning, and Event-Based Image Retrieval.\nA total of 45 teams from six countries participated, with evaluation conducted\nthrough Public and Private Test phases to ensure fairness and reproducibility.\nThe top three teams were invited to present their solutions at ACM Multimedia\n2025. EVENTA establishes a foundation for context-aware, narrative-driven\nmultimedia AI, with applications in journalism, media analysis, cultural\narchiving, and accessibility. Further details about the challenge are available\nat the official homepage: https://ltnghia.github.io/eventa/eventa-2025.\n", "link": "http://arxiv.org/abs/2508.18904v1", "date": "2025-08-26", "relevancy": 2.5021, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Enriched%20Image%20Analysis%20Grand%20Challenge%20at%20ACM%20Multimedia%202025&body=Title%3A%20Event-Enriched%20Image%20Analysis%20Grand%20Challenge%20at%20ACM%20Multimedia%202025%0AAuthor%3A%20Thien-Phuc%20Tran%20and%20Minh-Quang%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Tam%20V.%20Nguyen%20and%20Trong-Le%20Do%20and%20Duy-Nam%20Ly%20and%20Viet-Tham%20Huynh%20and%20Khanh-Duy%20Le%20and%20Mai-Khiem%20Tran%20and%20Trung-Nghia%20Le%0AAbstract%3A%20%20%20The%20Event-Enriched%20Image%20Analysis%20%28EVENTA%29%20Grand%20Challenge%2C%20hosted%20at%20ACM%0AMultimedia%202025%2C%20introduces%20the%20first%20large-scale%20benchmark%20for%20event-level%0Amultimodal%20understanding.%20Traditional%20captioning%20and%20retrieval%20tasks%20largely%0Afocus%20on%20surface-level%20recognition%20of%20people%2C%20objects%2C%20and%20scenes%2C%20often%0Aoverlooking%20the%20contextual%20and%20semantic%20dimensions%20that%20define%20real-world%0Aevents.%20EVENTA%20addresses%20this%20gap%20by%20integrating%20contextual%2C%20temporal%2C%20and%0Asemantic%20information%20to%20capture%20the%20who%2C%20when%2C%20where%2C%20what%2C%20and%20why%20behind%20an%0Aimage.%20Built%20upon%20the%20OpenEvents%20V1%20dataset%2C%20the%20challenge%20features%20two%20tracks%3A%0AEvent-Enriched%20Image%20Retrieval%20and%20Captioning%2C%20and%20Event-Based%20Image%20Retrieval.%0AA%20total%20of%2045%20teams%20from%20six%20countries%20participated%2C%20with%20evaluation%20conducted%0Athrough%20Public%20and%20Private%20Test%20phases%20to%20ensure%20fairness%20and%20reproducibility.%0AThe%20top%20three%20teams%20were%20invited%20to%20present%20their%20solutions%20at%20ACM%20Multimedia%0A2025.%20EVENTA%20establishes%20a%20foundation%20for%20context-aware%2C%20narrative-driven%0Amultimedia%20AI%2C%20with%20applications%20in%20journalism%2C%20media%20analysis%2C%20cultural%0Aarchiving%2C%20and%20accessibility.%20Further%20details%20about%20the%20challenge%20are%20available%0Aat%20the%20official%20homepage%3A%20https%3A//ltnghia.github.io/eventa/eventa-2025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Enriched%2520Image%2520Analysis%2520Grand%2520Challenge%2520at%2520ACM%2520Multimedia%25202025%26entry.906535625%3DThien-Phuc%2520Tran%2520and%2520Minh-Quang%2520Nguyen%2520and%2520Minh-Triet%2520Tran%2520and%2520Tam%2520V.%2520Nguyen%2520and%2520Trong-Le%2520Do%2520and%2520Duy-Nam%2520Ly%2520and%2520Viet-Tham%2520Huynh%2520and%2520Khanh-Duy%2520Le%2520and%2520Mai-Khiem%2520Tran%2520and%2520Trung-Nghia%2520Le%26entry.1292438233%3D%2520%2520The%2520Event-Enriched%2520Image%2520Analysis%2520%2528EVENTA%2529%2520Grand%2520Challenge%252C%2520hosted%2520at%2520ACM%250AMultimedia%25202025%252C%2520introduces%2520the%2520first%2520large-scale%2520benchmark%2520for%2520event-level%250Amultimodal%2520understanding.%2520Traditional%2520captioning%2520and%2520retrieval%2520tasks%2520largely%250Afocus%2520on%2520surface-level%2520recognition%2520of%2520people%252C%2520objects%252C%2520and%2520scenes%252C%2520often%250Aoverlooking%2520the%2520contextual%2520and%2520semantic%2520dimensions%2520that%2520define%2520real-world%250Aevents.%2520EVENTA%2520addresses%2520this%2520gap%2520by%2520integrating%2520contextual%252C%2520temporal%252C%2520and%250Asemantic%2520information%2520to%2520capture%2520the%2520who%252C%2520when%252C%2520where%252C%2520what%252C%2520and%2520why%2520behind%2520an%250Aimage.%2520Built%2520upon%2520the%2520OpenEvents%2520V1%2520dataset%252C%2520the%2520challenge%2520features%2520two%2520tracks%253A%250AEvent-Enriched%2520Image%2520Retrieval%2520and%2520Captioning%252C%2520and%2520Event-Based%2520Image%2520Retrieval.%250AA%2520total%2520of%252045%2520teams%2520from%2520six%2520countries%2520participated%252C%2520with%2520evaluation%2520conducted%250Athrough%2520Public%2520and%2520Private%2520Test%2520phases%2520to%2520ensure%2520fairness%2520and%2520reproducibility.%250AThe%2520top%2520three%2520teams%2520were%2520invited%2520to%2520present%2520their%2520solutions%2520at%2520ACM%2520Multimedia%250A2025.%2520EVENTA%2520establishes%2520a%2520foundation%2520for%2520context-aware%252C%2520narrative-driven%250Amultimedia%2520AI%252C%2520with%2520applications%2520in%2520journalism%252C%2520media%2520analysis%252C%2520cultural%250Aarchiving%252C%2520and%2520accessibility.%2520Further%2520details%2520about%2520the%2520challenge%2520are%2520available%250Aat%2520the%2520official%2520homepage%253A%2520https%253A//ltnghia.github.io/eventa/eventa-2025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Enriched%20Image%20Analysis%20Grand%20Challenge%20at%20ACM%20Multimedia%202025&entry.906535625=Thien-Phuc%20Tran%20and%20Minh-Quang%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Tam%20V.%20Nguyen%20and%20Trong-Le%20Do%20and%20Duy-Nam%20Ly%20and%20Viet-Tham%20Huynh%20and%20Khanh-Duy%20Le%20and%20Mai-Khiem%20Tran%20and%20Trung-Nghia%20Le&entry.1292438233=%20%20The%20Event-Enriched%20Image%20Analysis%20%28EVENTA%29%20Grand%20Challenge%2C%20hosted%20at%20ACM%0AMultimedia%202025%2C%20introduces%20the%20first%20large-scale%20benchmark%20for%20event-level%0Amultimodal%20understanding.%20Traditional%20captioning%20and%20retrieval%20tasks%20largely%0Afocus%20on%20surface-level%20recognition%20of%20people%2C%20objects%2C%20and%20scenes%2C%20often%0Aoverlooking%20the%20contextual%20and%20semantic%20dimensions%20that%20define%20real-world%0Aevents.%20EVENTA%20addresses%20this%20gap%20by%20integrating%20contextual%2C%20temporal%2C%20and%0Asemantic%20information%20to%20capture%20the%20who%2C%20when%2C%20where%2C%20what%2C%20and%20why%20behind%20an%0Aimage.%20Built%20upon%20the%20OpenEvents%20V1%20dataset%2C%20the%20challenge%20features%20two%20tracks%3A%0AEvent-Enriched%20Image%20Retrieval%20and%20Captioning%2C%20and%20Event-Based%20Image%20Retrieval.%0AA%20total%20of%2045%20teams%20from%20six%20countries%20participated%2C%20with%20evaluation%20conducted%0Athrough%20Public%20and%20Private%20Test%20phases%20to%20ensure%20fairness%20and%20reproducibility.%0AThe%20top%20three%20teams%20were%20invited%20to%20present%20their%20solutions%20at%20ACM%20Multimedia%0A2025.%20EVENTA%20establishes%20a%20foundation%20for%20context-aware%2C%20narrative-driven%0Amultimedia%20AI%2C%20with%20applications%20in%20journalism%2C%20media%20analysis%2C%20cultural%0Aarchiving%2C%20and%20accessibility.%20Further%20details%20about%20the%20challenge%20are%20available%0Aat%20the%20official%20homepage%3A%20https%3A//ltnghia.github.io/eventa/eventa-2025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18904v1&entry.124074799=Read"},
{"title": "AI Models Exceed Individual Human Accuracy in Predicting Everyday Social\n  Norms", "author": "Pontus Strimling and Simon Karlsson and Irina Vartanova and Kimmo Eriksson", "abstract": "  A fundamental question in cognitive science concerns how social norms are\nacquired and represented. While humans typically learn norms through embodied\nsocial experience, we investigated whether large language models can achieve\nsophisticated norm understanding through statistical learning alone. Across two\nstudies, we systematically evaluated multiple AI systems' ability to predict\nhuman social appropriateness judgments for 555 everyday scenarios by examining\nhow closely they predicted the average judgment compared to each human\nparticipant. In Study 1, GPT-4.5's accuracy in predicting the collective\njudgment on a continuous scale exceeded that of every human participant (100th\npercentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%\nof humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive\npower, all models showed systematic, correlated errors. These findings\ndemonstrate that sophisticated models of social cognition can emerge from\nstatistical learning over linguistic data alone, challenging strong versions of\ntheories emphasizing the exclusive necessity of embodied experience for\ncultural competence. The systematic nature of AI limitations across different\narchitectures indicates potential boundaries of pattern-based social\nunderstanding, while the models' ability to outperform nearly all individual\nhumans in this predictive task suggests that language serves as a remarkably\nrich repository for cultural knowledge transmission.\n", "link": "http://arxiv.org/abs/2508.19004v1", "date": "2025-08-26", "relevancy": 2.4404, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Models%20Exceed%20Individual%20Human%20Accuracy%20in%20Predicting%20Everyday%20Social%0A%20%20Norms&body=Title%3A%20AI%20Models%20Exceed%20Individual%20Human%20Accuracy%20in%20Predicting%20Everyday%20Social%0A%20%20Norms%0AAuthor%3A%20Pontus%20Strimling%20and%20Simon%20Karlsson%20and%20Irina%20Vartanova%20and%20Kimmo%20Eriksson%0AAbstract%3A%20%20%20A%20fundamental%20question%20in%20cognitive%20science%20concerns%20how%20social%20norms%20are%0Aacquired%20and%20represented.%20While%20humans%20typically%20learn%20norms%20through%20embodied%0Asocial%20experience%2C%20we%20investigated%20whether%20large%20language%20models%20can%20achieve%0Asophisticated%20norm%20understanding%20through%20statistical%20learning%20alone.%20Across%20two%0Astudies%2C%20we%20systematically%20evaluated%20multiple%20AI%20systems%27%20ability%20to%20predict%0Ahuman%20social%20appropriateness%20judgments%20for%20555%20everyday%20scenarios%20by%20examining%0Ahow%20closely%20they%20predicted%20the%20average%20judgment%20compared%20to%20each%20human%0Aparticipant.%20In%20Study%201%2C%20GPT-4.5%27s%20accuracy%20in%20predicting%20the%20collective%0Ajudgment%20on%20a%20continuous%20scale%20exceeded%20that%20of%20every%20human%20participant%20%28100th%0Apercentile%29.%20Study%202%20replicated%20this%2C%20with%20Gemini%202.5%20Pro%20outperforming%2098.7%25%0Aof%20humans%2C%20GPT-5%2097.8%25%2C%20and%20Claude%20Sonnet%204%2096.0%25.%20Despite%20this%20predictive%0Apower%2C%20all%20models%20showed%20systematic%2C%20correlated%20errors.%20These%20findings%0Ademonstrate%20that%20sophisticated%20models%20of%20social%20cognition%20can%20emerge%20from%0Astatistical%20learning%20over%20linguistic%20data%20alone%2C%20challenging%20strong%20versions%20of%0Atheories%20emphasizing%20the%20exclusive%20necessity%20of%20embodied%20experience%20for%0Acultural%20competence.%20The%20systematic%20nature%20of%20AI%20limitations%20across%20different%0Aarchitectures%20indicates%20potential%20boundaries%20of%20pattern-based%20social%0Aunderstanding%2C%20while%20the%20models%27%20ability%20to%20outperform%20nearly%20all%20individual%0Ahumans%20in%20this%20predictive%20task%20suggests%20that%20language%20serves%20as%20a%20remarkably%0Arich%20repository%20for%20cultural%20knowledge%20transmission.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Models%2520Exceed%2520Individual%2520Human%2520Accuracy%2520in%2520Predicting%2520Everyday%2520Social%250A%2520%2520Norms%26entry.906535625%3DPontus%2520Strimling%2520and%2520Simon%2520Karlsson%2520and%2520Irina%2520Vartanova%2520and%2520Kimmo%2520Eriksson%26entry.1292438233%3D%2520%2520A%2520fundamental%2520question%2520in%2520cognitive%2520science%2520concerns%2520how%2520social%2520norms%2520are%250Aacquired%2520and%2520represented.%2520While%2520humans%2520typically%2520learn%2520norms%2520through%2520embodied%250Asocial%2520experience%252C%2520we%2520investigated%2520whether%2520large%2520language%2520models%2520can%2520achieve%250Asophisticated%2520norm%2520understanding%2520through%2520statistical%2520learning%2520alone.%2520Across%2520two%250Astudies%252C%2520we%2520systematically%2520evaluated%2520multiple%2520AI%2520systems%2527%2520ability%2520to%2520predict%250Ahuman%2520social%2520appropriateness%2520judgments%2520for%2520555%2520everyday%2520scenarios%2520by%2520examining%250Ahow%2520closely%2520they%2520predicted%2520the%2520average%2520judgment%2520compared%2520to%2520each%2520human%250Aparticipant.%2520In%2520Study%25201%252C%2520GPT-4.5%2527s%2520accuracy%2520in%2520predicting%2520the%2520collective%250Ajudgment%2520on%2520a%2520continuous%2520scale%2520exceeded%2520that%2520of%2520every%2520human%2520participant%2520%2528100th%250Apercentile%2529.%2520Study%25202%2520replicated%2520this%252C%2520with%2520Gemini%25202.5%2520Pro%2520outperforming%252098.7%2525%250Aof%2520humans%252C%2520GPT-5%252097.8%2525%252C%2520and%2520Claude%2520Sonnet%25204%252096.0%2525.%2520Despite%2520this%2520predictive%250Apower%252C%2520all%2520models%2520showed%2520systematic%252C%2520correlated%2520errors.%2520These%2520findings%250Ademonstrate%2520that%2520sophisticated%2520models%2520of%2520social%2520cognition%2520can%2520emerge%2520from%250Astatistical%2520learning%2520over%2520linguistic%2520data%2520alone%252C%2520challenging%2520strong%2520versions%2520of%250Atheories%2520emphasizing%2520the%2520exclusive%2520necessity%2520of%2520embodied%2520experience%2520for%250Acultural%2520competence.%2520The%2520systematic%2520nature%2520of%2520AI%2520limitations%2520across%2520different%250Aarchitectures%2520indicates%2520potential%2520boundaries%2520of%2520pattern-based%2520social%250Aunderstanding%252C%2520while%2520the%2520models%2527%2520ability%2520to%2520outperform%2520nearly%2520all%2520individual%250Ahumans%2520in%2520this%2520predictive%2520task%2520suggests%2520that%2520language%2520serves%2520as%2520a%2520remarkably%250Arich%2520repository%2520for%2520cultural%2520knowledge%2520transmission.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Models%20Exceed%20Individual%20Human%20Accuracy%20in%20Predicting%20Everyday%20Social%0A%20%20Norms&entry.906535625=Pontus%20Strimling%20and%20Simon%20Karlsson%20and%20Irina%20Vartanova%20and%20Kimmo%20Eriksson&entry.1292438233=%20%20A%20fundamental%20question%20in%20cognitive%20science%20concerns%20how%20social%20norms%20are%0Aacquired%20and%20represented.%20While%20humans%20typically%20learn%20norms%20through%20embodied%0Asocial%20experience%2C%20we%20investigated%20whether%20large%20language%20models%20can%20achieve%0Asophisticated%20norm%20understanding%20through%20statistical%20learning%20alone.%20Across%20two%0Astudies%2C%20we%20systematically%20evaluated%20multiple%20AI%20systems%27%20ability%20to%20predict%0Ahuman%20social%20appropriateness%20judgments%20for%20555%20everyday%20scenarios%20by%20examining%0Ahow%20closely%20they%20predicted%20the%20average%20judgment%20compared%20to%20each%20human%0Aparticipant.%20In%20Study%201%2C%20GPT-4.5%27s%20accuracy%20in%20predicting%20the%20collective%0Ajudgment%20on%20a%20continuous%20scale%20exceeded%20that%20of%20every%20human%20participant%20%28100th%0Apercentile%29.%20Study%202%20replicated%20this%2C%20with%20Gemini%202.5%20Pro%20outperforming%2098.7%25%0Aof%20humans%2C%20GPT-5%2097.8%25%2C%20and%20Claude%20Sonnet%204%2096.0%25.%20Despite%20this%20predictive%0Apower%2C%20all%20models%20showed%20systematic%2C%20correlated%20errors.%20These%20findings%0Ademonstrate%20that%20sophisticated%20models%20of%20social%20cognition%20can%20emerge%20from%0Astatistical%20learning%20over%20linguistic%20data%20alone%2C%20challenging%20strong%20versions%20of%0Atheories%20emphasizing%20the%20exclusive%20necessity%20of%20embodied%20experience%20for%0Acultural%20competence.%20The%20systematic%20nature%20of%20AI%20limitations%20across%20different%0Aarchitectures%20indicates%20potential%20boundaries%20of%20pattern-based%20social%0Aunderstanding%2C%20while%20the%20models%27%20ability%20to%20outperform%20nearly%20all%20individual%0Ahumans%20in%20this%20predictive%20task%20suggests%20that%20language%20serves%20as%20a%20remarkably%0Arich%20repository%20for%20cultural%20knowledge%20transmission.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19004v1&entry.124074799=Read"},
{"title": "Revisiting SSL for sound event detection: complementary fusion and\n  adaptive post-processing", "author": "Hanfang Cui and Longfei Song and Li Li and Dongxing Xu and Yanhua Long", "abstract": "  Self-supervised learning (SSL) models offer powerful representations for\nsound event detection (SED), yet their synergistic potential remains\nunderexplored. This study systematically evaluates state-of-the-art SSL models\nto guide optimal model selection and integration for SED. We propose a\nframework that combines heterogeneous SSL representations (e.g., BEATs, HuBERT,\nWavLM) through three fusion strategies: individual SSL embedding integration,\ndual-modal fusion, and full aggregation. Experiments on the DCASE 2023 Task 4\nChallenge reveal that dual-modal fusion (e.g., CRNN+BEATs+WavLM) achieves\ncomplementary performance gains, while CRNN+BEATs alone delivers the best\nresults among individual SSL models. We further introduce normalized sound\nevent bounding boxes (nSEBBs), an adaptive post-processing method that\ndynamically adjusts event boundary predictions, improving PSDS1 by up to 4% for\nstandalone SSL models. These findings highlight the compatibility and\ncomplementarity of SSL architectures, providing guidance for task-specific\nfusion and robust SED system design.\n", "link": "http://arxiv.org/abs/2505.11889v2", "date": "2025-08-26", "relevancy": 2.419, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20SSL%20for%20sound%20event%20detection%3A%20complementary%20fusion%20and%0A%20%20adaptive%20post-processing&body=Title%3A%20Revisiting%20SSL%20for%20sound%20event%20detection%3A%20complementary%20fusion%20and%0A%20%20adaptive%20post-processing%0AAuthor%3A%20Hanfang%20Cui%20and%20Longfei%20Song%20and%20Li%20Li%20and%20Dongxing%20Xu%20and%20Yanhua%20Long%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20models%20offer%20powerful%20representations%20for%0Asound%20event%20detection%20%28SED%29%2C%20yet%20their%20synergistic%20potential%20remains%0Aunderexplored.%20This%20study%20systematically%20evaluates%20state-of-the-art%20SSL%20models%0Ato%20guide%20optimal%20model%20selection%20and%20integration%20for%20SED.%20We%20propose%20a%0Aframework%20that%20combines%20heterogeneous%20SSL%20representations%20%28e.g.%2C%20BEATs%2C%20HuBERT%2C%0AWavLM%29%20through%20three%20fusion%20strategies%3A%20individual%20SSL%20embedding%20integration%2C%0Adual-modal%20fusion%2C%20and%20full%20aggregation.%20Experiments%20on%20the%20DCASE%202023%20Task%204%0AChallenge%20reveal%20that%20dual-modal%20fusion%20%28e.g.%2C%20CRNN%2BBEATs%2BWavLM%29%20achieves%0Acomplementary%20performance%20gains%2C%20while%20CRNN%2BBEATs%20alone%20delivers%20the%20best%0Aresults%20among%20individual%20SSL%20models.%20We%20further%20introduce%20normalized%20sound%0Aevent%20bounding%20boxes%20%28nSEBBs%29%2C%20an%20adaptive%20post-processing%20method%20that%0Adynamically%20adjusts%20event%20boundary%20predictions%2C%20improving%20PSDS1%20by%20up%20to%204%25%20for%0Astandalone%20SSL%20models.%20These%20findings%20highlight%20the%20compatibility%20and%0Acomplementarity%20of%20SSL%20architectures%2C%20providing%20guidance%20for%20task-specific%0Afusion%20and%20robust%20SED%20system%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520SSL%2520for%2520sound%2520event%2520detection%253A%2520complementary%2520fusion%2520and%250A%2520%2520adaptive%2520post-processing%26entry.906535625%3DHanfang%2520Cui%2520and%2520Longfei%2520Song%2520and%2520Li%2520Li%2520and%2520Dongxing%2520Xu%2520and%2520Yanhua%2520Long%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520models%2520offer%2520powerful%2520representations%2520for%250Asound%2520event%2520detection%2520%2528SED%2529%252C%2520yet%2520their%2520synergistic%2520potential%2520remains%250Aunderexplored.%2520This%2520study%2520systematically%2520evaluates%2520state-of-the-art%2520SSL%2520models%250Ato%2520guide%2520optimal%2520model%2520selection%2520and%2520integration%2520for%2520SED.%2520We%2520propose%2520a%250Aframework%2520that%2520combines%2520heterogeneous%2520SSL%2520representations%2520%2528e.g.%252C%2520BEATs%252C%2520HuBERT%252C%250AWavLM%2529%2520through%2520three%2520fusion%2520strategies%253A%2520individual%2520SSL%2520embedding%2520integration%252C%250Adual-modal%2520fusion%252C%2520and%2520full%2520aggregation.%2520Experiments%2520on%2520the%2520DCASE%25202023%2520Task%25204%250AChallenge%2520reveal%2520that%2520dual-modal%2520fusion%2520%2528e.g.%252C%2520CRNN%252BBEATs%252BWavLM%2529%2520achieves%250Acomplementary%2520performance%2520gains%252C%2520while%2520CRNN%252BBEATs%2520alone%2520delivers%2520the%2520best%250Aresults%2520among%2520individual%2520SSL%2520models.%2520We%2520further%2520introduce%2520normalized%2520sound%250Aevent%2520bounding%2520boxes%2520%2528nSEBBs%2529%252C%2520an%2520adaptive%2520post-processing%2520method%2520that%250Adynamically%2520adjusts%2520event%2520boundary%2520predictions%252C%2520improving%2520PSDS1%2520by%2520up%2520to%25204%2525%2520for%250Astandalone%2520SSL%2520models.%2520These%2520findings%2520highlight%2520the%2520compatibility%2520and%250Acomplementarity%2520of%2520SSL%2520architectures%252C%2520providing%2520guidance%2520for%2520task-specific%250Afusion%2520and%2520robust%2520SED%2520system%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20SSL%20for%20sound%20event%20detection%3A%20complementary%20fusion%20and%0A%20%20adaptive%20post-processing&entry.906535625=Hanfang%20Cui%20and%20Longfei%20Song%20and%20Li%20Li%20and%20Dongxing%20Xu%20and%20Yanhua%20Long&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20models%20offer%20powerful%20representations%20for%0Asound%20event%20detection%20%28SED%29%2C%20yet%20their%20synergistic%20potential%20remains%0Aunderexplored.%20This%20study%20systematically%20evaluates%20state-of-the-art%20SSL%20models%0Ato%20guide%20optimal%20model%20selection%20and%20integration%20for%20SED.%20We%20propose%20a%0Aframework%20that%20combines%20heterogeneous%20SSL%20representations%20%28e.g.%2C%20BEATs%2C%20HuBERT%2C%0AWavLM%29%20through%20three%20fusion%20strategies%3A%20individual%20SSL%20embedding%20integration%2C%0Adual-modal%20fusion%2C%20and%20full%20aggregation.%20Experiments%20on%20the%20DCASE%202023%20Task%204%0AChallenge%20reveal%20that%20dual-modal%20fusion%20%28e.g.%2C%20CRNN%2BBEATs%2BWavLM%29%20achieves%0Acomplementary%20performance%20gains%2C%20while%20CRNN%2BBEATs%20alone%20delivers%20the%20best%0Aresults%20among%20individual%20SSL%20models.%20We%20further%20introduce%20normalized%20sound%0Aevent%20bounding%20boxes%20%28nSEBBs%29%2C%20an%20adaptive%20post-processing%20method%20that%0Adynamically%20adjusts%20event%20boundary%20predictions%2C%20improving%20PSDS1%20by%20up%20to%204%25%20for%0Astandalone%20SSL%20models.%20These%20findings%20highlight%20the%20compatibility%20and%0Acomplementarity%20of%20SSL%20architectures%2C%20providing%20guidance%20for%20task-specific%0Afusion%20and%20robust%20SED%20system%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11889v2&entry.124074799=Read"},
{"title": "Waver: Wave Your Way to Lifelike Video Generation", "author": "Yifu Zhang and Hao Yang and Yuqi Zhang and Yifei Hu and Fengda Zhu and Chuang Lin and Xiaofeng Mei and Yi Jiang and Bingyue Peng and Zehuan Yuan", "abstract": "  We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.\n", "link": "http://arxiv.org/abs/2508.15761v2", "date": "2025-08-26", "relevancy": 2.4051, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6114}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5985}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Waver%3A%20Wave%20Your%20Way%20to%20Lifelike%20Video%20Generation&body=Title%3A%20Waver%3A%20Wave%20Your%20Way%20to%20Lifelike%20Video%20Generation%0AAuthor%3A%20Yifu%20Zhang%20and%20Hao%20Yang%20and%20Yuqi%20Zhang%20and%20Yifei%20Hu%20and%20Fengda%20Zhu%20and%20Chuang%20Lin%20and%20Xiaofeng%20Mei%20and%20Yi%20Jiang%20and%20Bingyue%20Peng%20and%20Zehuan%20Yuan%0AAbstract%3A%20%20%20We%20present%20Waver%2C%20a%20high-performance%20foundation%20model%20for%20unified%20image%20and%0Avideo%20generation.%20Waver%20can%20directly%20generate%20videos%20with%20durations%20ranging%0Afrom%205%20to%2010%20seconds%20at%20a%20native%20resolution%20of%20720p%2C%20which%20are%20subsequently%0Aupscaled%20to%201080p.%20The%20model%20simultaneously%20supports%20text-to-video%20%28T2V%29%2C%0Aimage-to-video%20%28I2V%29%2C%20and%20text-to-image%20%28T2I%29%20generation%20within%20a%20single%2C%0Aintegrated%20framework.%20We%20introduce%20a%20Hybrid%20Stream%20DiT%20architecture%20to%20enhance%0Amodality%20alignment%20and%20accelerate%20training%20convergence.%20To%20ensure%20training%20data%0Aquality%2C%20we%20establish%20a%20comprehensive%20data%20curation%20pipeline%20and%20manually%0Aannotate%20and%20train%20an%20MLLM-based%20video%20quality%20model%20to%20filter%20for%20the%0Ahighest-quality%20samples.%20Furthermore%2C%20we%20provide%20detailed%20training%20and%0Ainference%20recipes%20to%20facilitate%20the%20generation%20of%20high-quality%20videos.%20Building%0Aon%20these%20contributions%2C%20Waver%20excels%20at%20capturing%20complex%20motion%2C%20achieving%0Asuperior%20motion%20amplitude%20and%20temporal%20consistency%20in%20video%20synthesis.%20Notably%2C%0Ait%20ranks%20among%20the%20Top%203%20on%20both%20the%20T2V%20and%20I2V%20leaderboards%20at%20Artificial%0AAnalysis%20%28data%20as%20of%202025-07-30%2010%3A00%20GMT%2B8%29%2C%20consistently%20outperforming%0Aexisting%20open-source%20models%20and%20matching%20or%20surpassing%20state-of-the-art%0Acommercial%20solutions.%20We%20hope%20this%20technical%20report%20will%20help%20the%20community%0Amore%20efficiently%20train%20high-quality%20video%20generation%20models%20and%20accelerate%0Aprogress%20in%20video%20generation%20technologies.%20Official%20page%3A%0Ahttps%3A//github.com/FoundationVision/Waver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaver%253A%2520Wave%2520Your%2520Way%2520to%2520Lifelike%2520Video%2520Generation%26entry.906535625%3DYifu%2520Zhang%2520and%2520Hao%2520Yang%2520and%2520Yuqi%2520Zhang%2520and%2520Yifei%2520Hu%2520and%2520Fengda%2520Zhu%2520and%2520Chuang%2520Lin%2520and%2520Xiaofeng%2520Mei%2520and%2520Yi%2520Jiang%2520and%2520Bingyue%2520Peng%2520and%2520Zehuan%2520Yuan%26entry.1292438233%3D%2520%2520We%2520present%2520Waver%252C%2520a%2520high-performance%2520foundation%2520model%2520for%2520unified%2520image%2520and%250Avideo%2520generation.%2520Waver%2520can%2520directly%2520generate%2520videos%2520with%2520durations%2520ranging%250Afrom%25205%2520to%252010%2520seconds%2520at%2520a%2520native%2520resolution%2520of%2520720p%252C%2520which%2520are%2520subsequently%250Aupscaled%2520to%25201080p.%2520The%2520model%2520simultaneously%2520supports%2520text-to-video%2520%2528T2V%2529%252C%250Aimage-to-video%2520%2528I2V%2529%252C%2520and%2520text-to-image%2520%2528T2I%2529%2520generation%2520within%2520a%2520single%252C%250Aintegrated%2520framework.%2520We%2520introduce%2520a%2520Hybrid%2520Stream%2520DiT%2520architecture%2520to%2520enhance%250Amodality%2520alignment%2520and%2520accelerate%2520training%2520convergence.%2520To%2520ensure%2520training%2520data%250Aquality%252C%2520we%2520establish%2520a%2520comprehensive%2520data%2520curation%2520pipeline%2520and%2520manually%250Aannotate%2520and%2520train%2520an%2520MLLM-based%2520video%2520quality%2520model%2520to%2520filter%2520for%2520the%250Ahighest-quality%2520samples.%2520Furthermore%252C%2520we%2520provide%2520detailed%2520training%2520and%250Ainference%2520recipes%2520to%2520facilitate%2520the%2520generation%2520of%2520high-quality%2520videos.%2520Building%250Aon%2520these%2520contributions%252C%2520Waver%2520excels%2520at%2520capturing%2520complex%2520motion%252C%2520achieving%250Asuperior%2520motion%2520amplitude%2520and%2520temporal%2520consistency%2520in%2520video%2520synthesis.%2520Notably%252C%250Ait%2520ranks%2520among%2520the%2520Top%25203%2520on%2520both%2520the%2520T2V%2520and%2520I2V%2520leaderboards%2520at%2520Artificial%250AAnalysis%2520%2528data%2520as%2520of%25202025-07-30%252010%253A00%2520GMT%252B8%2529%252C%2520consistently%2520outperforming%250Aexisting%2520open-source%2520models%2520and%2520matching%2520or%2520surpassing%2520state-of-the-art%250Acommercial%2520solutions.%2520We%2520hope%2520this%2520technical%2520report%2520will%2520help%2520the%2520community%250Amore%2520efficiently%2520train%2520high-quality%2520video%2520generation%2520models%2520and%2520accelerate%250Aprogress%2520in%2520video%2520generation%2520technologies.%2520Official%2520page%253A%250Ahttps%253A//github.com/FoundationVision/Waver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Waver%3A%20Wave%20Your%20Way%20to%20Lifelike%20Video%20Generation&entry.906535625=Yifu%20Zhang%20and%20Hao%20Yang%20and%20Yuqi%20Zhang%20and%20Yifei%20Hu%20and%20Fengda%20Zhu%20and%20Chuang%20Lin%20and%20Xiaofeng%20Mei%20and%20Yi%20Jiang%20and%20Bingyue%20Peng%20and%20Zehuan%20Yuan&entry.1292438233=%20%20We%20present%20Waver%2C%20a%20high-performance%20foundation%20model%20for%20unified%20image%20and%0Avideo%20generation.%20Waver%20can%20directly%20generate%20videos%20with%20durations%20ranging%0Afrom%205%20to%2010%20seconds%20at%20a%20native%20resolution%20of%20720p%2C%20which%20are%20subsequently%0Aupscaled%20to%201080p.%20The%20model%20simultaneously%20supports%20text-to-video%20%28T2V%29%2C%0Aimage-to-video%20%28I2V%29%2C%20and%20text-to-image%20%28T2I%29%20generation%20within%20a%20single%2C%0Aintegrated%20framework.%20We%20introduce%20a%20Hybrid%20Stream%20DiT%20architecture%20to%20enhance%0Amodality%20alignment%20and%20accelerate%20training%20convergence.%20To%20ensure%20training%20data%0Aquality%2C%20we%20establish%20a%20comprehensive%20data%20curation%20pipeline%20and%20manually%0Aannotate%20and%20train%20an%20MLLM-based%20video%20quality%20model%20to%20filter%20for%20the%0Ahighest-quality%20samples.%20Furthermore%2C%20we%20provide%20detailed%20training%20and%0Ainference%20recipes%20to%20facilitate%20the%20generation%20of%20high-quality%20videos.%20Building%0Aon%20these%20contributions%2C%20Waver%20excels%20at%20capturing%20complex%20motion%2C%20achieving%0Asuperior%20motion%20amplitude%20and%20temporal%20consistency%20in%20video%20synthesis.%20Notably%2C%0Ait%20ranks%20among%20the%20Top%203%20on%20both%20the%20T2V%20and%20I2V%20leaderboards%20at%20Artificial%0AAnalysis%20%28data%20as%20of%202025-07-30%2010%3A00%20GMT%2B8%29%2C%20consistently%20outperforming%0Aexisting%20open-source%20models%20and%20matching%20or%20surpassing%20state-of-the-art%0Acommercial%20solutions.%20We%20hope%20this%20technical%20report%20will%20help%20the%20community%0Amore%20efficiently%20train%20high-quality%20video%20generation%20models%20and%20accelerate%0Aprogress%20in%20video%20generation%20technologies.%20Official%20page%3A%0Ahttps%3A//github.com/FoundationVision/Waver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15761v2&entry.124074799=Read"},
{"title": "UniGenX: a unified generative foundation model that couples sequence,\n  structure and function to accelerate scientific design across proteins,\n  molecules and materials", "author": "Gongbo Zhang and Yanting Li and Renqian Luo and Pipi Hu and Yang Yang and Zeru Zhao and Lingbo Li and Guoqing Liu and Zun Wang and Ran Bi and Kaiyuan Gao and Liya Guo and Yu Xie and Chang Liu and Jia Zhang and Tian Xie and Robert Pinsler and Claudio Zeni and Ziheng Lu and Hongxia Hao and Yingce Xia and Marwin Segler and Maik Riechert and Wei Yang and Hao Jiang and Wen-Bin Zhang and Zhijun Zeng and Yi Zhu and Li Dong and Xiuyuan Hu and Li Yuan and Lei Chen and Haiguang Liu and Tao Qin", "abstract": "  Function in natural systems arises from one-dimensional sequences forming\nthree-dimensional structures with specific properties. However, current\ngenerative models suffer from critical limitations: training objectives seldom\ntarget function directly, discrete sequences and continuous coordinates are\noptimized in isolation, and conformational ensembles are under-modeled. We\npresent UniGenX, a unified generative foundation model that addresses these\ngaps by co-generating sequences and coordinates under direct functional and\nproperty objectives across proteins, molecules, and materials. UniGenX\nrepresents heterogeneous inputs as a mixed stream of symbolic and numeric\ntokens, where a decoder-only autoregressive transformer provides global context\nand a conditional diffusion head generates numeric fields steered by\ntask-specific tokens. Besides the new high SOTAs on structure prediction tasks,\nthe model demonstrates state-of-the-art or competitive performance for the\nfunction-aware generation across domains: in materials, it achieves\n\"conflicted\" multi-property conditional generation, yielding 436 crystal\ncandidates meeting triple constraints, including 11 with novel compositions; in\nchemistry, it sets new benchmarks on five property targets and conformer\nensemble generation on GEOM; and in biology, it improves success in modeling\nprotein induced fit (RMSD < 2 {\\AA}) by over 23-fold and enhances\nEC-conditioned enzyme design. Ablation studies and cross-domain transfer\nsubstantiate the benefits of joint discrete-continuous training, establishing\nUniGenX as a significant advance from prediction to controllable,\nfunction-aware generation.\n", "link": "http://arxiv.org/abs/2503.06687v2", "date": "2025-08-26", "relevancy": 2.3998, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.619}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5908}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGenX%3A%20a%20unified%20generative%20foundation%20model%20that%20couples%20sequence%2C%0A%20%20structure%20and%20function%20to%20accelerate%20scientific%20design%20across%20proteins%2C%0A%20%20molecules%20and%20materials&body=Title%3A%20UniGenX%3A%20a%20unified%20generative%20foundation%20model%20that%20couples%20sequence%2C%0A%20%20structure%20and%20function%20to%20accelerate%20scientific%20design%20across%20proteins%2C%0A%20%20molecules%20and%20materials%0AAuthor%3A%20Gongbo%20Zhang%20and%20Yanting%20Li%20and%20Renqian%20Luo%20and%20Pipi%20Hu%20and%20Yang%20Yang%20and%20Zeru%20Zhao%20and%20Lingbo%20Li%20and%20Guoqing%20Liu%20and%20Zun%20Wang%20and%20Ran%20Bi%20and%20Kaiyuan%20Gao%20and%20Liya%20Guo%20and%20Yu%20Xie%20and%20Chang%20Liu%20and%20Jia%20Zhang%20and%20Tian%20Xie%20and%20Robert%20Pinsler%20and%20Claudio%20Zeni%20and%20Ziheng%20Lu%20and%20Hongxia%20Hao%20and%20Yingce%20Xia%20and%20Marwin%20Segler%20and%20Maik%20Riechert%20and%20Wei%20Yang%20and%20Hao%20Jiang%20and%20Wen-Bin%20Zhang%20and%20Zhijun%20Zeng%20and%20Yi%20Zhu%20and%20Li%20Dong%20and%20Xiuyuan%20Hu%20and%20Li%20Yuan%20and%20Lei%20Chen%20and%20Haiguang%20Liu%20and%20Tao%20Qin%0AAbstract%3A%20%20%20Function%20in%20natural%20systems%20arises%20from%20one-dimensional%20sequences%20forming%0Athree-dimensional%20structures%20with%20specific%20properties.%20However%2C%20current%0Agenerative%20models%20suffer%20from%20critical%20limitations%3A%20training%20objectives%20seldom%0Atarget%20function%20directly%2C%20discrete%20sequences%20and%20continuous%20coordinates%20are%0Aoptimized%20in%20isolation%2C%20and%20conformational%20ensembles%20are%20under-modeled.%20We%0Apresent%20UniGenX%2C%20a%20unified%20generative%20foundation%20model%20that%20addresses%20these%0Agaps%20by%20co-generating%20sequences%20and%20coordinates%20under%20direct%20functional%20and%0Aproperty%20objectives%20across%20proteins%2C%20molecules%2C%20and%20materials.%20UniGenX%0Arepresents%20heterogeneous%20inputs%20as%20a%20mixed%20stream%20of%20symbolic%20and%20numeric%0Atokens%2C%20where%20a%20decoder-only%20autoregressive%20transformer%20provides%20global%20context%0Aand%20a%20conditional%20diffusion%20head%20generates%20numeric%20fields%20steered%20by%0Atask-specific%20tokens.%20Besides%20the%20new%20high%20SOTAs%20on%20structure%20prediction%20tasks%2C%0Athe%20model%20demonstrates%20state-of-the-art%20or%20competitive%20performance%20for%20the%0Afunction-aware%20generation%20across%20domains%3A%20in%20materials%2C%20it%20achieves%0A%22conflicted%22%20multi-property%20conditional%20generation%2C%20yielding%20436%20crystal%0Acandidates%20meeting%20triple%20constraints%2C%20including%2011%20with%20novel%20compositions%3B%20in%0Achemistry%2C%20it%20sets%20new%20benchmarks%20on%20five%20property%20targets%20and%20conformer%0Aensemble%20generation%20on%20GEOM%3B%20and%20in%20biology%2C%20it%20improves%20success%20in%20modeling%0Aprotein%20induced%20fit%20%28RMSD%20%3C%202%20%7B%5CAA%7D%29%20by%20over%2023-fold%20and%20enhances%0AEC-conditioned%20enzyme%20design.%20Ablation%20studies%20and%20cross-domain%20transfer%0Asubstantiate%20the%20benefits%20of%20joint%20discrete-continuous%20training%2C%20establishing%0AUniGenX%20as%20a%20significant%20advance%20from%20prediction%20to%20controllable%2C%0Afunction-aware%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGenX%253A%2520a%2520unified%2520generative%2520foundation%2520model%2520that%2520couples%2520sequence%252C%250A%2520%2520structure%2520and%2520function%2520to%2520accelerate%2520scientific%2520design%2520across%2520proteins%252C%250A%2520%2520molecules%2520and%2520materials%26entry.906535625%3DGongbo%2520Zhang%2520and%2520Yanting%2520Li%2520and%2520Renqian%2520Luo%2520and%2520Pipi%2520Hu%2520and%2520Yang%2520Yang%2520and%2520Zeru%2520Zhao%2520and%2520Lingbo%2520Li%2520and%2520Guoqing%2520Liu%2520and%2520Zun%2520Wang%2520and%2520Ran%2520Bi%2520and%2520Kaiyuan%2520Gao%2520and%2520Liya%2520Guo%2520and%2520Yu%2520Xie%2520and%2520Chang%2520Liu%2520and%2520Jia%2520Zhang%2520and%2520Tian%2520Xie%2520and%2520Robert%2520Pinsler%2520and%2520Claudio%2520Zeni%2520and%2520Ziheng%2520Lu%2520and%2520Hongxia%2520Hao%2520and%2520Yingce%2520Xia%2520and%2520Marwin%2520Segler%2520and%2520Maik%2520Riechert%2520and%2520Wei%2520Yang%2520and%2520Hao%2520Jiang%2520and%2520Wen-Bin%2520Zhang%2520and%2520Zhijun%2520Zeng%2520and%2520Yi%2520Zhu%2520and%2520Li%2520Dong%2520and%2520Xiuyuan%2520Hu%2520and%2520Li%2520Yuan%2520and%2520Lei%2520Chen%2520and%2520Haiguang%2520Liu%2520and%2520Tao%2520Qin%26entry.1292438233%3D%2520%2520Function%2520in%2520natural%2520systems%2520arises%2520from%2520one-dimensional%2520sequences%2520forming%250Athree-dimensional%2520structures%2520with%2520specific%2520properties.%2520However%252C%2520current%250Agenerative%2520models%2520suffer%2520from%2520critical%2520limitations%253A%2520training%2520objectives%2520seldom%250Atarget%2520function%2520directly%252C%2520discrete%2520sequences%2520and%2520continuous%2520coordinates%2520are%250Aoptimized%2520in%2520isolation%252C%2520and%2520conformational%2520ensembles%2520are%2520under-modeled.%2520We%250Apresent%2520UniGenX%252C%2520a%2520unified%2520generative%2520foundation%2520model%2520that%2520addresses%2520these%250Agaps%2520by%2520co-generating%2520sequences%2520and%2520coordinates%2520under%2520direct%2520functional%2520and%250Aproperty%2520objectives%2520across%2520proteins%252C%2520molecules%252C%2520and%2520materials.%2520UniGenX%250Arepresents%2520heterogeneous%2520inputs%2520as%2520a%2520mixed%2520stream%2520of%2520symbolic%2520and%2520numeric%250Atokens%252C%2520where%2520a%2520decoder-only%2520autoregressive%2520transformer%2520provides%2520global%2520context%250Aand%2520a%2520conditional%2520diffusion%2520head%2520generates%2520numeric%2520fields%2520steered%2520by%250Atask-specific%2520tokens.%2520Besides%2520the%2520new%2520high%2520SOTAs%2520on%2520structure%2520prediction%2520tasks%252C%250Athe%2520model%2520demonstrates%2520state-of-the-art%2520or%2520competitive%2520performance%2520for%2520the%250Afunction-aware%2520generation%2520across%2520domains%253A%2520in%2520materials%252C%2520it%2520achieves%250A%2522conflicted%2522%2520multi-property%2520conditional%2520generation%252C%2520yielding%2520436%2520crystal%250Acandidates%2520meeting%2520triple%2520constraints%252C%2520including%252011%2520with%2520novel%2520compositions%253B%2520in%250Achemistry%252C%2520it%2520sets%2520new%2520benchmarks%2520on%2520five%2520property%2520targets%2520and%2520conformer%250Aensemble%2520generation%2520on%2520GEOM%253B%2520and%2520in%2520biology%252C%2520it%2520improves%2520success%2520in%2520modeling%250Aprotein%2520induced%2520fit%2520%2528RMSD%2520%253C%25202%2520%257B%255CAA%257D%2529%2520by%2520over%252023-fold%2520and%2520enhances%250AEC-conditioned%2520enzyme%2520design.%2520Ablation%2520studies%2520and%2520cross-domain%2520transfer%250Asubstantiate%2520the%2520benefits%2520of%2520joint%2520discrete-continuous%2520training%252C%2520establishing%250AUniGenX%2520as%2520a%2520significant%2520advance%2520from%2520prediction%2520to%2520controllable%252C%250Afunction-aware%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGenX%3A%20a%20unified%20generative%20foundation%20model%20that%20couples%20sequence%2C%0A%20%20structure%20and%20function%20to%20accelerate%20scientific%20design%20across%20proteins%2C%0A%20%20molecules%20and%20materials&entry.906535625=Gongbo%20Zhang%20and%20Yanting%20Li%20and%20Renqian%20Luo%20and%20Pipi%20Hu%20and%20Yang%20Yang%20and%20Zeru%20Zhao%20and%20Lingbo%20Li%20and%20Guoqing%20Liu%20and%20Zun%20Wang%20and%20Ran%20Bi%20and%20Kaiyuan%20Gao%20and%20Liya%20Guo%20and%20Yu%20Xie%20and%20Chang%20Liu%20and%20Jia%20Zhang%20and%20Tian%20Xie%20and%20Robert%20Pinsler%20and%20Claudio%20Zeni%20and%20Ziheng%20Lu%20and%20Hongxia%20Hao%20and%20Yingce%20Xia%20and%20Marwin%20Segler%20and%20Maik%20Riechert%20and%20Wei%20Yang%20and%20Hao%20Jiang%20and%20Wen-Bin%20Zhang%20and%20Zhijun%20Zeng%20and%20Yi%20Zhu%20and%20Li%20Dong%20and%20Xiuyuan%20Hu%20and%20Li%20Yuan%20and%20Lei%20Chen%20and%20Haiguang%20Liu%20and%20Tao%20Qin&entry.1292438233=%20%20Function%20in%20natural%20systems%20arises%20from%20one-dimensional%20sequences%20forming%0Athree-dimensional%20structures%20with%20specific%20properties.%20However%2C%20current%0Agenerative%20models%20suffer%20from%20critical%20limitations%3A%20training%20objectives%20seldom%0Atarget%20function%20directly%2C%20discrete%20sequences%20and%20continuous%20coordinates%20are%0Aoptimized%20in%20isolation%2C%20and%20conformational%20ensembles%20are%20under-modeled.%20We%0Apresent%20UniGenX%2C%20a%20unified%20generative%20foundation%20model%20that%20addresses%20these%0Agaps%20by%20co-generating%20sequences%20and%20coordinates%20under%20direct%20functional%20and%0Aproperty%20objectives%20across%20proteins%2C%20molecules%2C%20and%20materials.%20UniGenX%0Arepresents%20heterogeneous%20inputs%20as%20a%20mixed%20stream%20of%20symbolic%20and%20numeric%0Atokens%2C%20where%20a%20decoder-only%20autoregressive%20transformer%20provides%20global%20context%0Aand%20a%20conditional%20diffusion%20head%20generates%20numeric%20fields%20steered%20by%0Atask-specific%20tokens.%20Besides%20the%20new%20high%20SOTAs%20on%20structure%20prediction%20tasks%2C%0Athe%20model%20demonstrates%20state-of-the-art%20or%20competitive%20performance%20for%20the%0Afunction-aware%20generation%20across%20domains%3A%20in%20materials%2C%20it%20achieves%0A%22conflicted%22%20multi-property%20conditional%20generation%2C%20yielding%20436%20crystal%0Acandidates%20meeting%20triple%20constraints%2C%20including%2011%20with%20novel%20compositions%3B%20in%0Achemistry%2C%20it%20sets%20new%20benchmarks%20on%20five%20property%20targets%20and%20conformer%0Aensemble%20generation%20on%20GEOM%3B%20and%20in%20biology%2C%20it%20improves%20success%20in%20modeling%0Aprotein%20induced%20fit%20%28RMSD%20%3C%202%20%7B%5CAA%7D%29%20by%20over%2023-fold%20and%20enhances%0AEC-conditioned%20enzyme%20design.%20Ablation%20studies%20and%20cross-domain%20transfer%0Asubstantiate%20the%20benefits%20of%20joint%20discrete-continuous%20training%2C%20establishing%0AUniGenX%20as%20a%20significant%20advance%20from%20prediction%20to%20controllable%2C%0Afunction-aware%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06687v2&entry.124074799=Read"},
{"title": "Planning-Query-Guided Model Generation for Model-Based Deformable Object\n  Manipulation", "author": "Alex LaGrassa and Zixuan Huang and Dmitry Berenson and Oliver Kroemer", "abstract": "  Efficient planning in high-dimensional spaces, such as those involving\ndeformable objects, requires computationally tractable yet sufficiently\nexpressive dynamics models. This paper introduces a method that automatically\ngenerates task-specific, spatially adaptive dynamics models by learning which\nregions of the object require high-resolution modeling to achieve good task\nperformance for a given planning query. Task performance depends on the complex\ninterplay between the dynamics model, world dynamics, control, and task\nrequirements. Our proposed diffusion-based model generator predicts per-region\nmodel resolutions based on start and goal pointclouds that define the planning\nquery. To efficiently collect the data for learning this mapping, a two-stage\nprocess optimizes resolution using predictive dynamics as a prior before\ndirectly optimizing using closed-loop performance. On a tree-manipulation task,\nour method doubles planning speed with only a small decrease in task\nperformance over using a full-resolution model. This approach informs a path\ntowards using previous planning and control data to generate computationally\nefficient yet sufficiently expressive dynamics models for new tasks.\n", "link": "http://arxiv.org/abs/2508.19199v1", "date": "2025-08-26", "relevancy": 2.3956, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6114}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6017}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planning-Query-Guided%20Model%20Generation%20for%20Model-Based%20Deformable%20Object%0A%20%20Manipulation&body=Title%3A%20Planning-Query-Guided%20Model%20Generation%20for%20Model-Based%20Deformable%20Object%0A%20%20Manipulation%0AAuthor%3A%20Alex%20LaGrassa%20and%20Zixuan%20Huang%20and%20Dmitry%20Berenson%20and%20Oliver%20Kroemer%0AAbstract%3A%20%20%20Efficient%20planning%20in%20high-dimensional%20spaces%2C%20such%20as%20those%20involving%0Adeformable%20objects%2C%20requires%20computationally%20tractable%20yet%20sufficiently%0Aexpressive%20dynamics%20models.%20This%20paper%20introduces%20a%20method%20that%20automatically%0Agenerates%20task-specific%2C%20spatially%20adaptive%20dynamics%20models%20by%20learning%20which%0Aregions%20of%20the%20object%20require%20high-resolution%20modeling%20to%20achieve%20good%20task%0Aperformance%20for%20a%20given%20planning%20query.%20Task%20performance%20depends%20on%20the%20complex%0Ainterplay%20between%20the%20dynamics%20model%2C%20world%20dynamics%2C%20control%2C%20and%20task%0Arequirements.%20Our%20proposed%20diffusion-based%20model%20generator%20predicts%20per-region%0Amodel%20resolutions%20based%20on%20start%20and%20goal%20pointclouds%20that%20define%20the%20planning%0Aquery.%20To%20efficiently%20collect%20the%20data%20for%20learning%20this%20mapping%2C%20a%20two-stage%0Aprocess%20optimizes%20resolution%20using%20predictive%20dynamics%20as%20a%20prior%20before%0Adirectly%20optimizing%20using%20closed-loop%20performance.%20On%20a%20tree-manipulation%20task%2C%0Aour%20method%20doubles%20planning%20speed%20with%20only%20a%20small%20decrease%20in%20task%0Aperformance%20over%20using%20a%20full-resolution%20model.%20This%20approach%20informs%20a%20path%0Atowards%20using%20previous%20planning%20and%20control%20data%20to%20generate%20computationally%0Aefficient%20yet%20sufficiently%20expressive%20dynamics%20models%20for%20new%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanning-Query-Guided%2520Model%2520Generation%2520for%2520Model-Based%2520Deformable%2520Object%250A%2520%2520Manipulation%26entry.906535625%3DAlex%2520LaGrassa%2520and%2520Zixuan%2520Huang%2520and%2520Dmitry%2520Berenson%2520and%2520Oliver%2520Kroemer%26entry.1292438233%3D%2520%2520Efficient%2520planning%2520in%2520high-dimensional%2520spaces%252C%2520such%2520as%2520those%2520involving%250Adeformable%2520objects%252C%2520requires%2520computationally%2520tractable%2520yet%2520sufficiently%250Aexpressive%2520dynamics%2520models.%2520This%2520paper%2520introduces%2520a%2520method%2520that%2520automatically%250Agenerates%2520task-specific%252C%2520spatially%2520adaptive%2520dynamics%2520models%2520by%2520learning%2520which%250Aregions%2520of%2520the%2520object%2520require%2520high-resolution%2520modeling%2520to%2520achieve%2520good%2520task%250Aperformance%2520for%2520a%2520given%2520planning%2520query.%2520Task%2520performance%2520depends%2520on%2520the%2520complex%250Ainterplay%2520between%2520the%2520dynamics%2520model%252C%2520world%2520dynamics%252C%2520control%252C%2520and%2520task%250Arequirements.%2520Our%2520proposed%2520diffusion-based%2520model%2520generator%2520predicts%2520per-region%250Amodel%2520resolutions%2520based%2520on%2520start%2520and%2520goal%2520pointclouds%2520that%2520define%2520the%2520planning%250Aquery.%2520To%2520efficiently%2520collect%2520the%2520data%2520for%2520learning%2520this%2520mapping%252C%2520a%2520two-stage%250Aprocess%2520optimizes%2520resolution%2520using%2520predictive%2520dynamics%2520as%2520a%2520prior%2520before%250Adirectly%2520optimizing%2520using%2520closed-loop%2520performance.%2520On%2520a%2520tree-manipulation%2520task%252C%250Aour%2520method%2520doubles%2520planning%2520speed%2520with%2520only%2520a%2520small%2520decrease%2520in%2520task%250Aperformance%2520over%2520using%2520a%2520full-resolution%2520model.%2520This%2520approach%2520informs%2520a%2520path%250Atowards%2520using%2520previous%2520planning%2520and%2520control%2520data%2520to%2520generate%2520computationally%250Aefficient%2520yet%2520sufficiently%2520expressive%2520dynamics%2520models%2520for%2520new%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planning-Query-Guided%20Model%20Generation%20for%20Model-Based%20Deformable%20Object%0A%20%20Manipulation&entry.906535625=Alex%20LaGrassa%20and%20Zixuan%20Huang%20and%20Dmitry%20Berenson%20and%20Oliver%20Kroemer&entry.1292438233=%20%20Efficient%20planning%20in%20high-dimensional%20spaces%2C%20such%20as%20those%20involving%0Adeformable%20objects%2C%20requires%20computationally%20tractable%20yet%20sufficiently%0Aexpressive%20dynamics%20models.%20This%20paper%20introduces%20a%20method%20that%20automatically%0Agenerates%20task-specific%2C%20spatially%20adaptive%20dynamics%20models%20by%20learning%20which%0Aregions%20of%20the%20object%20require%20high-resolution%20modeling%20to%20achieve%20good%20task%0Aperformance%20for%20a%20given%20planning%20query.%20Task%20performance%20depends%20on%20the%20complex%0Ainterplay%20between%20the%20dynamics%20model%2C%20world%20dynamics%2C%20control%2C%20and%20task%0Arequirements.%20Our%20proposed%20diffusion-based%20model%20generator%20predicts%20per-region%0Amodel%20resolutions%20based%20on%20start%20and%20goal%20pointclouds%20that%20define%20the%20planning%0Aquery.%20To%20efficiently%20collect%20the%20data%20for%20learning%20this%20mapping%2C%20a%20two-stage%0Aprocess%20optimizes%20resolution%20using%20predictive%20dynamics%20as%20a%20prior%20before%0Adirectly%20optimizing%20using%20closed-loop%20performance.%20On%20a%20tree-manipulation%20task%2C%0Aour%20method%20doubles%20planning%20speed%20with%20only%20a%20small%20decrease%20in%20task%0Aperformance%20over%20using%20a%20full-resolution%20model.%20This%20approach%20informs%20a%20path%0Atowards%20using%20previous%20planning%20and%20control%20data%20to%20generate%20computationally%0Aefficient%20yet%20sufficiently%20expressive%20dynamics%20models%20for%20new%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19199v1&entry.124074799=Read"},
{"title": "Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention\n  Mechanism for Graph Learning", "author": "An Ning and Tai Yue Li and Nan Yow Chen", "abstract": "  We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neural\nnetwork that integrates variational quantum circuits into the attention\nmechanism. At its core, QGAT employs strongly entangling quantum circuits with\namplitude-encoded node features to enable expressive nonlinear interactions.\nDistinct from classical multi-head attention that separately computes each\nhead, QGAT leverages a single quantum circuit to simultaneously generate\nmultiple attention coefficients. This quantum parallelism facilitates parameter\nsharing across heads, substantially reducing computational overhead and model\ncomplexity. Classical projection weights and quantum circuit parameters are\noptimized jointly in an end-to-end manner, ensuring flexible adaptation to\nlearning tasks. Empirical results demonstrate QGAT's effectiveness in capturing\ncomplex structural dependencies and improved generalization in inductive\nscenarios, highlighting its potential for scalable quantum-enhanced learning\nacross domains such as chemistry, biology, and network analysis. Furthermore,\nexperiments confirm that quantum embedding enhances robustness against feature\nand structural noise, suggesting advantages in handling real-world noisy data.\nThe modularity of QGAT also ensures straightforward integration into existing\narchitectures, allowing it to easily augment classical attention-based models.\n", "link": "http://arxiv.org/abs/2508.17630v2", "date": "2025-08-26", "relevancy": 2.3956, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4913}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4782}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Graph%20Attention%20Network%3A%20A%20Novel%20Quantum%20Multi-Head%20Attention%0A%20%20Mechanism%20for%20Graph%20Learning&body=Title%3A%20Quantum%20Graph%20Attention%20Network%3A%20A%20Novel%20Quantum%20Multi-Head%20Attention%0A%20%20Mechanism%20for%20Graph%20Learning%0AAuthor%3A%20An%20Ning%20and%20Tai%20Yue%20Li%20and%20Nan%20Yow%20Chen%0AAbstract%3A%20%20%20We%20propose%20the%20Quantum%20Graph%20Attention%20Network%20%28QGAT%29%2C%20a%20hybrid%20graph%20neural%0Anetwork%20that%20integrates%20variational%20quantum%20circuits%20into%20the%20attention%0Amechanism.%20At%20its%20core%2C%20QGAT%20employs%20strongly%20entangling%20quantum%20circuits%20with%0Aamplitude-encoded%20node%20features%20to%20enable%20expressive%20nonlinear%20interactions.%0ADistinct%20from%20classical%20multi-head%20attention%20that%20separately%20computes%20each%0Ahead%2C%20QGAT%20leverages%20a%20single%20quantum%20circuit%20to%20simultaneously%20generate%0Amultiple%20attention%20coefficients.%20This%20quantum%20parallelism%20facilitates%20parameter%0Asharing%20across%20heads%2C%20substantially%20reducing%20computational%20overhead%20and%20model%0Acomplexity.%20Classical%20projection%20weights%20and%20quantum%20circuit%20parameters%20are%0Aoptimized%20jointly%20in%20an%20end-to-end%20manner%2C%20ensuring%20flexible%20adaptation%20to%0Alearning%20tasks.%20Empirical%20results%20demonstrate%20QGAT%27s%20effectiveness%20in%20capturing%0Acomplex%20structural%20dependencies%20and%20improved%20generalization%20in%20inductive%0Ascenarios%2C%20highlighting%20its%20potential%20for%20scalable%20quantum-enhanced%20learning%0Aacross%20domains%20such%20as%20chemistry%2C%20biology%2C%20and%20network%20analysis.%20Furthermore%2C%0Aexperiments%20confirm%20that%20quantum%20embedding%20enhances%20robustness%20against%20feature%0Aand%20structural%20noise%2C%20suggesting%20advantages%20in%20handling%20real-world%20noisy%20data.%0AThe%20modularity%20of%20QGAT%20also%20ensures%20straightforward%20integration%20into%20existing%0Aarchitectures%2C%20allowing%20it%20to%20easily%20augment%20classical%20attention-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Graph%2520Attention%2520Network%253A%2520A%2520Novel%2520Quantum%2520Multi-Head%2520Attention%250A%2520%2520Mechanism%2520for%2520Graph%2520Learning%26entry.906535625%3DAn%2520Ning%2520and%2520Tai%2520Yue%2520Li%2520and%2520Nan%2520Yow%2520Chen%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520Quantum%2520Graph%2520Attention%2520Network%2520%2528QGAT%2529%252C%2520a%2520hybrid%2520graph%2520neural%250Anetwork%2520that%2520integrates%2520variational%2520quantum%2520circuits%2520into%2520the%2520attention%250Amechanism.%2520At%2520its%2520core%252C%2520QGAT%2520employs%2520strongly%2520entangling%2520quantum%2520circuits%2520with%250Aamplitude-encoded%2520node%2520features%2520to%2520enable%2520expressive%2520nonlinear%2520interactions.%250ADistinct%2520from%2520classical%2520multi-head%2520attention%2520that%2520separately%2520computes%2520each%250Ahead%252C%2520QGAT%2520leverages%2520a%2520single%2520quantum%2520circuit%2520to%2520simultaneously%2520generate%250Amultiple%2520attention%2520coefficients.%2520This%2520quantum%2520parallelism%2520facilitates%2520parameter%250Asharing%2520across%2520heads%252C%2520substantially%2520reducing%2520computational%2520overhead%2520and%2520model%250Acomplexity.%2520Classical%2520projection%2520weights%2520and%2520quantum%2520circuit%2520parameters%2520are%250Aoptimized%2520jointly%2520in%2520an%2520end-to-end%2520manner%252C%2520ensuring%2520flexible%2520adaptation%2520to%250Alearning%2520tasks.%2520Empirical%2520results%2520demonstrate%2520QGAT%2527s%2520effectiveness%2520in%2520capturing%250Acomplex%2520structural%2520dependencies%2520and%2520improved%2520generalization%2520in%2520inductive%250Ascenarios%252C%2520highlighting%2520its%2520potential%2520for%2520scalable%2520quantum-enhanced%2520learning%250Aacross%2520domains%2520such%2520as%2520chemistry%252C%2520biology%252C%2520and%2520network%2520analysis.%2520Furthermore%252C%250Aexperiments%2520confirm%2520that%2520quantum%2520embedding%2520enhances%2520robustness%2520against%2520feature%250Aand%2520structural%2520noise%252C%2520suggesting%2520advantages%2520in%2520handling%2520real-world%2520noisy%2520data.%250AThe%2520modularity%2520of%2520QGAT%2520also%2520ensures%2520straightforward%2520integration%2520into%2520existing%250Aarchitectures%252C%2520allowing%2520it%2520to%2520easily%2520augment%2520classical%2520attention-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Graph%20Attention%20Network%3A%20A%20Novel%20Quantum%20Multi-Head%20Attention%0A%20%20Mechanism%20for%20Graph%20Learning&entry.906535625=An%20Ning%20and%20Tai%20Yue%20Li%20and%20Nan%20Yow%20Chen&entry.1292438233=%20%20We%20propose%20the%20Quantum%20Graph%20Attention%20Network%20%28QGAT%29%2C%20a%20hybrid%20graph%20neural%0Anetwork%20that%20integrates%20variational%20quantum%20circuits%20into%20the%20attention%0Amechanism.%20At%20its%20core%2C%20QGAT%20employs%20strongly%20entangling%20quantum%20circuits%20with%0Aamplitude-encoded%20node%20features%20to%20enable%20expressive%20nonlinear%20interactions.%0ADistinct%20from%20classical%20multi-head%20attention%20that%20separately%20computes%20each%0Ahead%2C%20QGAT%20leverages%20a%20single%20quantum%20circuit%20to%20simultaneously%20generate%0Amultiple%20attention%20coefficients.%20This%20quantum%20parallelism%20facilitates%20parameter%0Asharing%20across%20heads%2C%20substantially%20reducing%20computational%20overhead%20and%20model%0Acomplexity.%20Classical%20projection%20weights%20and%20quantum%20circuit%20parameters%20are%0Aoptimized%20jointly%20in%20an%20end-to-end%20manner%2C%20ensuring%20flexible%20adaptation%20to%0Alearning%20tasks.%20Empirical%20results%20demonstrate%20QGAT%27s%20effectiveness%20in%20capturing%0Acomplex%20structural%20dependencies%20and%20improved%20generalization%20in%20inductive%0Ascenarios%2C%20highlighting%20its%20potential%20for%20scalable%20quantum-enhanced%20learning%0Aacross%20domains%20such%20as%20chemistry%2C%20biology%2C%20and%20network%20analysis.%20Furthermore%2C%0Aexperiments%20confirm%20that%20quantum%20embedding%20enhances%20robustness%20against%20feature%0Aand%20structural%20noise%2C%20suggesting%20advantages%20in%20handling%20real-world%20noisy%20data.%0AThe%20modularity%20of%20QGAT%20also%20ensures%20straightforward%20integration%20into%20existing%0Aarchitectures%2C%20allowing%20it%20to%20easily%20augment%20classical%20attention-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17630v2&entry.124074799=Read"},
{"title": "Interpolating Speaker Identities in Embedding Space for Data Expansion", "author": "Tianchi Liu and Ruijie Tao and Qiongqiong Wang and Yidi Jiang and Hardik B. Sailor and Ke Zhang and Jingru Lin and Haizhou Li", "abstract": "  The success of deep learning-based speaker verification systems is largely\nattributed to access to large-scale and diverse speaker identity data. However,\ncollecting data from more identities is expensive, challenging, and often\nlimited by privacy concerns. To address this limitation, we propose INSIDE\n(Interpolating Speaker Identities in Embedding Space), a novel data expansion\nmethod that synthesizes new speaker identities by interpolating between\nexisting speaker embeddings. Specifically, we select pairs of nearby speaker\nembeddings from a pretrained speaker embedding space and compute intermediate\nembeddings using spherical linear interpolation. These interpolated embeddings\nare then fed to a text-to-speech system to generate corresponding speech\nwaveforms. The resulting data is combined with the original dataset to train\ndownstream models. Experiments show that models trained with INSIDE-expanded\ndata outperform those trained only on real data, achieving 3.06\\% to 5.24\\%\nrelative improvements. While INSIDE is primarily designed for speaker\nverification, we also validate its effectiveness on gender classification,\nwhere it yields a 13.44\\% relative improvement. Moreover, INSIDE is compatible\nwith other augmentation techniques and can serve as a flexible, scalable\naddition to existing training pipelines.\n", "link": "http://arxiv.org/abs/2508.19210v1", "date": "2025-08-26", "relevancy": 2.3934, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5166}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpolating%20Speaker%20Identities%20in%20Embedding%20Space%20for%20Data%20Expansion&body=Title%3A%20Interpolating%20Speaker%20Identities%20in%20Embedding%20Space%20for%20Data%20Expansion%0AAuthor%3A%20Tianchi%20Liu%20and%20Ruijie%20Tao%20and%20Qiongqiong%20Wang%20and%20Yidi%20Jiang%20and%20Hardik%20B.%20Sailor%20and%20Ke%20Zhang%20and%20Jingru%20Lin%20and%20Haizhou%20Li%0AAbstract%3A%20%20%20The%20success%20of%20deep%20learning-based%20speaker%20verification%20systems%20is%20largely%0Aattributed%20to%20access%20to%20large-scale%20and%20diverse%20speaker%20identity%20data.%20However%2C%0Acollecting%20data%20from%20more%20identities%20is%20expensive%2C%20challenging%2C%20and%20often%0Alimited%20by%20privacy%20concerns.%20To%20address%20this%20limitation%2C%20we%20propose%20INSIDE%0A%28Interpolating%20Speaker%20Identities%20in%20Embedding%20Space%29%2C%20a%20novel%20data%20expansion%0Amethod%20that%20synthesizes%20new%20speaker%20identities%20by%20interpolating%20between%0Aexisting%20speaker%20embeddings.%20Specifically%2C%20we%20select%20pairs%20of%20nearby%20speaker%0Aembeddings%20from%20a%20pretrained%20speaker%20embedding%20space%20and%20compute%20intermediate%0Aembeddings%20using%20spherical%20linear%20interpolation.%20These%20interpolated%20embeddings%0Aare%20then%20fed%20to%20a%20text-to-speech%20system%20to%20generate%20corresponding%20speech%0Awaveforms.%20The%20resulting%20data%20is%20combined%20with%20the%20original%20dataset%20to%20train%0Adownstream%20models.%20Experiments%20show%20that%20models%20trained%20with%20INSIDE-expanded%0Adata%20outperform%20those%20trained%20only%20on%20real%20data%2C%20achieving%203.06%5C%25%20to%205.24%5C%25%0Arelative%20improvements.%20While%20INSIDE%20is%20primarily%20designed%20for%20speaker%0Averification%2C%20we%20also%20validate%20its%20effectiveness%20on%20gender%20classification%2C%0Awhere%20it%20yields%20a%2013.44%5C%25%20relative%20improvement.%20Moreover%2C%20INSIDE%20is%20compatible%0Awith%20other%20augmentation%20techniques%20and%20can%20serve%20as%20a%20flexible%2C%20scalable%0Aaddition%20to%20existing%20training%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpolating%2520Speaker%2520Identities%2520in%2520Embedding%2520Space%2520for%2520Data%2520Expansion%26entry.906535625%3DTianchi%2520Liu%2520and%2520Ruijie%2520Tao%2520and%2520Qiongqiong%2520Wang%2520and%2520Yidi%2520Jiang%2520and%2520Hardik%2520B.%2520Sailor%2520and%2520Ke%2520Zhang%2520and%2520Jingru%2520Lin%2520and%2520Haizhou%2520Li%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520deep%2520learning-based%2520speaker%2520verification%2520systems%2520is%2520largely%250Aattributed%2520to%2520access%2520to%2520large-scale%2520and%2520diverse%2520speaker%2520identity%2520data.%2520However%252C%250Acollecting%2520data%2520from%2520more%2520identities%2520is%2520expensive%252C%2520challenging%252C%2520and%2520often%250Alimited%2520by%2520privacy%2520concerns.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520INSIDE%250A%2528Interpolating%2520Speaker%2520Identities%2520in%2520Embedding%2520Space%2529%252C%2520a%2520novel%2520data%2520expansion%250Amethod%2520that%2520synthesizes%2520new%2520speaker%2520identities%2520by%2520interpolating%2520between%250Aexisting%2520speaker%2520embeddings.%2520Specifically%252C%2520we%2520select%2520pairs%2520of%2520nearby%2520speaker%250Aembeddings%2520from%2520a%2520pretrained%2520speaker%2520embedding%2520space%2520and%2520compute%2520intermediate%250Aembeddings%2520using%2520spherical%2520linear%2520interpolation.%2520These%2520interpolated%2520embeddings%250Aare%2520then%2520fed%2520to%2520a%2520text-to-speech%2520system%2520to%2520generate%2520corresponding%2520speech%250Awaveforms.%2520The%2520resulting%2520data%2520is%2520combined%2520with%2520the%2520original%2520dataset%2520to%2520train%250Adownstream%2520models.%2520Experiments%2520show%2520that%2520models%2520trained%2520with%2520INSIDE-expanded%250Adata%2520outperform%2520those%2520trained%2520only%2520on%2520real%2520data%252C%2520achieving%25203.06%255C%2525%2520to%25205.24%255C%2525%250Arelative%2520improvements.%2520While%2520INSIDE%2520is%2520primarily%2520designed%2520for%2520speaker%250Averification%252C%2520we%2520also%2520validate%2520its%2520effectiveness%2520on%2520gender%2520classification%252C%250Awhere%2520it%2520yields%2520a%252013.44%255C%2525%2520relative%2520improvement.%2520Moreover%252C%2520INSIDE%2520is%2520compatible%250Awith%2520other%2520augmentation%2520techniques%2520and%2520can%2520serve%2520as%2520a%2520flexible%252C%2520scalable%250Aaddition%2520to%2520existing%2520training%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpolating%20Speaker%20Identities%20in%20Embedding%20Space%20for%20Data%20Expansion&entry.906535625=Tianchi%20Liu%20and%20Ruijie%20Tao%20and%20Qiongqiong%20Wang%20and%20Yidi%20Jiang%20and%20Hardik%20B.%20Sailor%20and%20Ke%20Zhang%20and%20Jingru%20Lin%20and%20Haizhou%20Li&entry.1292438233=%20%20The%20success%20of%20deep%20learning-based%20speaker%20verification%20systems%20is%20largely%0Aattributed%20to%20access%20to%20large-scale%20and%20diverse%20speaker%20identity%20data.%20However%2C%0Acollecting%20data%20from%20more%20identities%20is%20expensive%2C%20challenging%2C%20and%20often%0Alimited%20by%20privacy%20concerns.%20To%20address%20this%20limitation%2C%20we%20propose%20INSIDE%0A%28Interpolating%20Speaker%20Identities%20in%20Embedding%20Space%29%2C%20a%20novel%20data%20expansion%0Amethod%20that%20synthesizes%20new%20speaker%20identities%20by%20interpolating%20between%0Aexisting%20speaker%20embeddings.%20Specifically%2C%20we%20select%20pairs%20of%20nearby%20speaker%0Aembeddings%20from%20a%20pretrained%20speaker%20embedding%20space%20and%20compute%20intermediate%0Aembeddings%20using%20spherical%20linear%20interpolation.%20These%20interpolated%20embeddings%0Aare%20then%20fed%20to%20a%20text-to-speech%20system%20to%20generate%20corresponding%20speech%0Awaveforms.%20The%20resulting%20data%20is%20combined%20with%20the%20original%20dataset%20to%20train%0Adownstream%20models.%20Experiments%20show%20that%20models%20trained%20with%20INSIDE-expanded%0Adata%20outperform%20those%20trained%20only%20on%20real%20data%2C%20achieving%203.06%5C%25%20to%205.24%5C%25%0Arelative%20improvements.%20While%20INSIDE%20is%20primarily%20designed%20for%20speaker%0Averification%2C%20we%20also%20validate%20its%20effectiveness%20on%20gender%20classification%2C%0Awhere%20it%20yields%20a%2013.44%5C%25%20relative%20improvement.%20Moreover%2C%20INSIDE%20is%20compatible%0Awith%20other%20augmentation%20techniques%20and%20can%20serve%20as%20a%20flexible%2C%20scalable%0Aaddition%20to%20existing%20training%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19210v1&entry.124074799=Read"},
{"title": "VibeVoice Technical Report", "author": "Zhiliang Peng and Jianwei Yu and Wenhui Wang and Yaoyao Chang and Yutao Sun and Li Dong and Yi Zhu and Weijiang Xu and Hangbo Bao and Zehua Wang and Shaohan Huang and Yan Xia and Furu Wei", "abstract": "  This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models.\n", "link": "http://arxiv.org/abs/2508.19205v1", "date": "2025-08-26", "relevancy": 2.3926, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VibeVoice%20Technical%20Report&body=Title%3A%20VibeVoice%20Technical%20Report%0AAuthor%3A%20Zhiliang%20Peng%20and%20Jianwei%20Yu%20and%20Wenhui%20Wang%20and%20Yaoyao%20Chang%20and%20Yutao%20Sun%20and%20Li%20Dong%20and%20Yi%20Zhu%20and%20Weijiang%20Xu%20and%20Hangbo%20Bao%20and%20Zehua%20Wang%20and%20Shaohan%20Huang%20and%20Yan%20Xia%20and%20Furu%20Wei%0AAbstract%3A%20%20%20This%20report%20presents%20VibeVoice%2C%20a%20novel%20model%20designed%20to%20synthesize%0Along-form%20speech%20with%20multiple%20speakers%20by%20employing%20next-token%20diffusion%2C%0Awhich%20is%20a%20unified%20method%20for%20modeling%20continuous%20data%20by%20autoregressively%0Agenerating%20latent%20vectors%20via%20diffusion.%20To%20enable%20this%2C%20we%20introduce%20a%20novel%0Acontinuous%20speech%20tokenizer%20that%2C%20when%20compared%20to%20the%20popular%20Encodec%20model%2C%0Aimproves%20data%20compression%20by%2080%20times%20while%20maintaining%20comparable%20performance.%0AThe%20tokenizer%20effectively%20preserves%20audio%20fidelity%20while%20significantly%20boosting%0Acomputational%20efficiency%20for%20processing%20long%20sequences.%20Thus%2C%20VibeVoice%20can%0Asynthesize%20long-form%20speech%20for%20up%20to%2090%20minutes%20%28in%20a%2064K%20context%20window%0Alength%29%20with%20a%20maximum%20of%204%20speakers%2C%20capturing%20the%20authentic%20conversational%0A%60%60vibe%27%27%20and%20surpassing%20open-source%20and%20proprietary%20dialogue%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVibeVoice%2520Technical%2520Report%26entry.906535625%3DZhiliang%2520Peng%2520and%2520Jianwei%2520Yu%2520and%2520Wenhui%2520Wang%2520and%2520Yaoyao%2520Chang%2520and%2520Yutao%2520Sun%2520and%2520Li%2520Dong%2520and%2520Yi%2520Zhu%2520and%2520Weijiang%2520Xu%2520and%2520Hangbo%2520Bao%2520and%2520Zehua%2520Wang%2520and%2520Shaohan%2520Huang%2520and%2520Yan%2520Xia%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520This%2520report%2520presents%2520VibeVoice%252C%2520a%2520novel%2520model%2520designed%2520to%2520synthesize%250Along-form%2520speech%2520with%2520multiple%2520speakers%2520by%2520employing%2520next-token%2520diffusion%252C%250Awhich%2520is%2520a%2520unified%2520method%2520for%2520modeling%2520continuous%2520data%2520by%2520autoregressively%250Agenerating%2520latent%2520vectors%2520via%2520diffusion.%2520To%2520enable%2520this%252C%2520we%2520introduce%2520a%2520novel%250Acontinuous%2520speech%2520tokenizer%2520that%252C%2520when%2520compared%2520to%2520the%2520popular%2520Encodec%2520model%252C%250Aimproves%2520data%2520compression%2520by%252080%2520times%2520while%2520maintaining%2520comparable%2520performance.%250AThe%2520tokenizer%2520effectively%2520preserves%2520audio%2520fidelity%2520while%2520significantly%2520boosting%250Acomputational%2520efficiency%2520for%2520processing%2520long%2520sequences.%2520Thus%252C%2520VibeVoice%2520can%250Asynthesize%2520long-form%2520speech%2520for%2520up%2520to%252090%2520minutes%2520%2528in%2520a%252064K%2520context%2520window%250Alength%2529%2520with%2520a%2520maximum%2520of%25204%2520speakers%252C%2520capturing%2520the%2520authentic%2520conversational%250A%2560%2560vibe%2527%2527%2520and%2520surpassing%2520open-source%2520and%2520proprietary%2520dialogue%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VibeVoice%20Technical%20Report&entry.906535625=Zhiliang%20Peng%20and%20Jianwei%20Yu%20and%20Wenhui%20Wang%20and%20Yaoyao%20Chang%20and%20Yutao%20Sun%20and%20Li%20Dong%20and%20Yi%20Zhu%20and%20Weijiang%20Xu%20and%20Hangbo%20Bao%20and%20Zehua%20Wang%20and%20Shaohan%20Huang%20and%20Yan%20Xia%20and%20Furu%20Wei&entry.1292438233=%20%20This%20report%20presents%20VibeVoice%2C%20a%20novel%20model%20designed%20to%20synthesize%0Along-form%20speech%20with%20multiple%20speakers%20by%20employing%20next-token%20diffusion%2C%0Awhich%20is%20a%20unified%20method%20for%20modeling%20continuous%20data%20by%20autoregressively%0Agenerating%20latent%20vectors%20via%20diffusion.%20To%20enable%20this%2C%20we%20introduce%20a%20novel%0Acontinuous%20speech%20tokenizer%20that%2C%20when%20compared%20to%20the%20popular%20Encodec%20model%2C%0Aimproves%20data%20compression%20by%2080%20times%20while%20maintaining%20comparable%20performance.%0AThe%20tokenizer%20effectively%20preserves%20audio%20fidelity%20while%20significantly%20boosting%0Acomputational%20efficiency%20for%20processing%20long%20sequences.%20Thus%2C%20VibeVoice%20can%0Asynthesize%20long-form%20speech%20for%20up%20to%2090%20minutes%20%28in%20a%2064K%20context%20window%0Alength%29%20with%20a%20maximum%20of%204%20speakers%2C%20capturing%20the%20authentic%20conversational%0A%60%60vibe%27%27%20and%20surpassing%20open-source%20and%20proprietary%20dialogue%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19205v1&entry.124074799=Read"},
{"title": "Generative Data Augmentation for Object Point Cloud Segmentation", "author": "Dekai Zhu and Stefan Gavranovic and Flavien Boussuge and Benjamin Busam and Slobodan Ilic", "abstract": "  Data augmentation is widely used to train deep learning models to address\ndata scarcity. However, traditional data augmentation (TDA) typically relies on\nsimple geometric transformation, such as random rotation and rescaling,\nresulting in minimal data diversity enrichment and limited model performance\nimprovement. State-of-the-art generative models for 3D shape generation rely on\nthe denoising diffusion probabilistic models and manage to generate realistic\nnovel point clouds for 3D content creation and manipulation. Nevertheless, the\ngenerated 3D shapes lack associated point-wise semantic labels, restricting\ntheir usage in enlarging the training data for point cloud segmentation tasks.\nTo bridge the gap between data augmentation techniques and the advanced\ndiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a\npart-aware generative model that can generate high-quality point clouds\nconditioned on given segmentation masks. Leveraging the novel generative model,\nwe introduce a 3-step generative data augmentation (GDA) pipeline for point\ncloud segmentation training. Our GDA approach requires only a small amount of\nlabeled samples but enriches the training data with generated variants and\npseudo-labeled samples, which are validated by a novel diffusion-based\npseudo-label filtering method. Extensive experiments on two large-scale\nsynthetic datasets and a real-world medical dataset demonstrate that our GDA\nmethod outperforms TDA approach and related semi-supervised and self-supervised\nmethods.\n", "link": "http://arxiv.org/abs/2505.17783v2", "date": "2025-08-26", "relevancy": 2.3852, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.634}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5889}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Data%20Augmentation%20for%20Object%20Point%20Cloud%20Segmentation&body=Title%3A%20Generative%20Data%20Augmentation%20for%20Object%20Point%20Cloud%20Segmentation%0AAuthor%3A%20Dekai%20Zhu%20and%20Stefan%20Gavranovic%20and%20Flavien%20Boussuge%20and%20Benjamin%20Busam%20and%20Slobodan%20Ilic%0AAbstract%3A%20%20%20Data%20augmentation%20is%20widely%20used%20to%20train%20deep%20learning%20models%20to%20address%0Adata%20scarcity.%20However%2C%20traditional%20data%20augmentation%20%28TDA%29%20typically%20relies%20on%0Asimple%20geometric%20transformation%2C%20such%20as%20random%20rotation%20and%20rescaling%2C%0Aresulting%20in%20minimal%20data%20diversity%20enrichment%20and%20limited%20model%20performance%0Aimprovement.%20State-of-the-art%20generative%20models%20for%203D%20shape%20generation%20rely%20on%0Athe%20denoising%20diffusion%20probabilistic%20models%20and%20manage%20to%20generate%20realistic%0Anovel%20point%20clouds%20for%203D%20content%20creation%20and%20manipulation.%20Nevertheless%2C%20the%0Agenerated%203D%20shapes%20lack%20associated%20point-wise%20semantic%20labels%2C%20restricting%0Atheir%20usage%20in%20enlarging%20the%20training%20data%20for%20point%20cloud%20segmentation%20tasks.%0ATo%20bridge%20the%20gap%20between%20data%20augmentation%20techniques%20and%20the%20advanced%0Adiffusion%20models%2C%20we%20extend%20the%20state-of-the-art%203D%20diffusion%20model%2C%20Lion%2C%20to%20a%0Apart-aware%20generative%20model%20that%20can%20generate%20high-quality%20point%20clouds%0Aconditioned%20on%20given%20segmentation%20masks.%20Leveraging%20the%20novel%20generative%20model%2C%0Awe%20introduce%20a%203-step%20generative%20data%20augmentation%20%28GDA%29%20pipeline%20for%20point%0Acloud%20segmentation%20training.%20Our%20GDA%20approach%20requires%20only%20a%20small%20amount%20of%0Alabeled%20samples%20but%20enriches%20the%20training%20data%20with%20generated%20variants%20and%0Apseudo-labeled%20samples%2C%20which%20are%20validated%20by%20a%20novel%20diffusion-based%0Apseudo-label%20filtering%20method.%20Extensive%20experiments%20on%20two%20large-scale%0Asynthetic%20datasets%20and%20a%20real-world%20medical%20dataset%20demonstrate%20that%20our%20GDA%0Amethod%20outperforms%20TDA%20approach%20and%20related%20semi-supervised%20and%20self-supervised%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Data%2520Augmentation%2520for%2520Object%2520Point%2520Cloud%2520Segmentation%26entry.906535625%3DDekai%2520Zhu%2520and%2520Stefan%2520Gavranovic%2520and%2520Flavien%2520Boussuge%2520and%2520Benjamin%2520Busam%2520and%2520Slobodan%2520Ilic%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520widely%2520used%2520to%2520train%2520deep%2520learning%2520models%2520to%2520address%250Adata%2520scarcity.%2520However%252C%2520traditional%2520data%2520augmentation%2520%2528TDA%2529%2520typically%2520relies%2520on%250Asimple%2520geometric%2520transformation%252C%2520such%2520as%2520random%2520rotation%2520and%2520rescaling%252C%250Aresulting%2520in%2520minimal%2520data%2520diversity%2520enrichment%2520and%2520limited%2520model%2520performance%250Aimprovement.%2520State-of-the-art%2520generative%2520models%2520for%25203D%2520shape%2520generation%2520rely%2520on%250Athe%2520denoising%2520diffusion%2520probabilistic%2520models%2520and%2520manage%2520to%2520generate%2520realistic%250Anovel%2520point%2520clouds%2520for%25203D%2520content%2520creation%2520and%2520manipulation.%2520Nevertheless%252C%2520the%250Agenerated%25203D%2520shapes%2520lack%2520associated%2520point-wise%2520semantic%2520labels%252C%2520restricting%250Atheir%2520usage%2520in%2520enlarging%2520the%2520training%2520data%2520for%2520point%2520cloud%2520segmentation%2520tasks.%250ATo%2520bridge%2520the%2520gap%2520between%2520data%2520augmentation%2520techniques%2520and%2520the%2520advanced%250Adiffusion%2520models%252C%2520we%2520extend%2520the%2520state-of-the-art%25203D%2520diffusion%2520model%252C%2520Lion%252C%2520to%2520a%250Apart-aware%2520generative%2520model%2520that%2520can%2520generate%2520high-quality%2520point%2520clouds%250Aconditioned%2520on%2520given%2520segmentation%2520masks.%2520Leveraging%2520the%2520novel%2520generative%2520model%252C%250Awe%2520introduce%2520a%25203-step%2520generative%2520data%2520augmentation%2520%2528GDA%2529%2520pipeline%2520for%2520point%250Acloud%2520segmentation%2520training.%2520Our%2520GDA%2520approach%2520requires%2520only%2520a%2520small%2520amount%2520of%250Alabeled%2520samples%2520but%2520enriches%2520the%2520training%2520data%2520with%2520generated%2520variants%2520and%250Apseudo-labeled%2520samples%252C%2520which%2520are%2520validated%2520by%2520a%2520novel%2520diffusion-based%250Apseudo-label%2520filtering%2520method.%2520Extensive%2520experiments%2520on%2520two%2520large-scale%250Asynthetic%2520datasets%2520and%2520a%2520real-world%2520medical%2520dataset%2520demonstrate%2520that%2520our%2520GDA%250Amethod%2520outperforms%2520TDA%2520approach%2520and%2520related%2520semi-supervised%2520and%2520self-supervised%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Data%20Augmentation%20for%20Object%20Point%20Cloud%20Segmentation&entry.906535625=Dekai%20Zhu%20and%20Stefan%20Gavranovic%20and%20Flavien%20Boussuge%20and%20Benjamin%20Busam%20and%20Slobodan%20Ilic&entry.1292438233=%20%20Data%20augmentation%20is%20widely%20used%20to%20train%20deep%20learning%20models%20to%20address%0Adata%20scarcity.%20However%2C%20traditional%20data%20augmentation%20%28TDA%29%20typically%20relies%20on%0Asimple%20geometric%20transformation%2C%20such%20as%20random%20rotation%20and%20rescaling%2C%0Aresulting%20in%20minimal%20data%20diversity%20enrichment%20and%20limited%20model%20performance%0Aimprovement.%20State-of-the-art%20generative%20models%20for%203D%20shape%20generation%20rely%20on%0Athe%20denoising%20diffusion%20probabilistic%20models%20and%20manage%20to%20generate%20realistic%0Anovel%20point%20clouds%20for%203D%20content%20creation%20and%20manipulation.%20Nevertheless%2C%20the%0Agenerated%203D%20shapes%20lack%20associated%20point-wise%20semantic%20labels%2C%20restricting%0Atheir%20usage%20in%20enlarging%20the%20training%20data%20for%20point%20cloud%20segmentation%20tasks.%0ATo%20bridge%20the%20gap%20between%20data%20augmentation%20techniques%20and%20the%20advanced%0Adiffusion%20models%2C%20we%20extend%20the%20state-of-the-art%203D%20diffusion%20model%2C%20Lion%2C%20to%20a%0Apart-aware%20generative%20model%20that%20can%20generate%20high-quality%20point%20clouds%0Aconditioned%20on%20given%20segmentation%20masks.%20Leveraging%20the%20novel%20generative%20model%2C%0Awe%20introduce%20a%203-step%20generative%20data%20augmentation%20%28GDA%29%20pipeline%20for%20point%0Acloud%20segmentation%20training.%20Our%20GDA%20approach%20requires%20only%20a%20small%20amount%20of%0Alabeled%20samples%20but%20enriches%20the%20training%20data%20with%20generated%20variants%20and%0Apseudo-labeled%20samples%2C%20which%20are%20validated%20by%20a%20novel%20diffusion-based%0Apseudo-label%20filtering%20method.%20Extensive%20experiments%20on%20two%20large-scale%0Asynthetic%20datasets%20and%20a%20real-world%20medical%20dataset%20demonstrate%20that%20our%20GDA%0Amethod%20outperforms%20TDA%20approach%20and%20related%20semi-supervised%20and%20self-supervised%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17783v2&entry.124074799=Read"},
{"title": "VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility\n  Constraints under Uncertainty for Autonomous Robotic Surgery", "author": "Wang Jiayin and Wei Yanran and Jiang Lei and Guo Xiaoyu and Zheng Ayong and Zhao Weidong and Li Zhongkui", "abstract": "  Autonomous control of the laparoscope in robot-assisted Minimally Invasive\nSurgery (MIS) has received considerable research interest due to its potential\nto improve surgical safety. Despite progress in pixel-level Image-Based Visual\nServoing (IBVS) control, the requirement of continuous visibility and the\nexistence of complex disturbances, such as parameterization error, measurement\nnoise, and uncertainties of payloads, could degrade the surgeon's visual\nexperience and compromise procedural safety. To address these limitations, this\npaper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and\nuncertainty-adaptive framework for autonomous laparoscope control that\nguarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian\nProcess Regression (GPR) is utilized to perform hybrid (deterministic +\nstochastic) quantification of operational uncertainties including residual\nmodel uncertainties, stochastic uncertainties, and external disturbances. Based\non uncertainty quantification, a novel safety aware trajectory optimization\nframework with probabilistic guarantees is proposed, where a\nuncertainty-adaptive safety Control Barrier Function (CBF) condition is given\nbased on uncertainty propagation, and chance constraints are simultaneously\nformulated based on probabilistic approximation. This uncertainty aware\nformulation enables adaptive control effort allocation, minimizing unnecessary\ncamera motion while maintaining robustness. The proposed method is validated\nthrough comparative simulations and experiments on a commercial surgical robot\nplatform (MicroPort MedBot Toumai) performing a sequential multi-target lymph\nnode dissection. Compared with baseline methods, the framework maintains\nnear-perfect target visibility (>99.9%), reduces tracking e\n", "link": "http://arxiv.org/abs/2508.18937v1", "date": "2025-08-26", "relevancy": 2.3753, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6072}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5944}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionSafeEnhanced%20VPC%3A%20Cautious%20Predictive%20Control%20with%20Visibility%0A%20%20Constraints%20under%20Uncertainty%20for%20Autonomous%20Robotic%20Surgery&body=Title%3A%20VisionSafeEnhanced%20VPC%3A%20Cautious%20Predictive%20Control%20with%20Visibility%0A%20%20Constraints%20under%20Uncertainty%20for%20Autonomous%20Robotic%20Surgery%0AAuthor%3A%20Wang%20Jiayin%20and%20Wei%20Yanran%20and%20Jiang%20Lei%20and%20Guo%20Xiaoyu%20and%20Zheng%20Ayong%20and%20Zhao%20Weidong%20and%20Li%20Zhongkui%0AAbstract%3A%20%20%20Autonomous%20control%20of%20the%20laparoscope%20in%20robot-assisted%20Minimally%20Invasive%0ASurgery%20%28MIS%29%20has%20received%20considerable%20research%20interest%20due%20to%20its%20potential%0Ato%20improve%20surgical%20safety.%20Despite%20progress%20in%20pixel-level%20Image-Based%20Visual%0AServoing%20%28IBVS%29%20control%2C%20the%20requirement%20of%20continuous%20visibility%20and%20the%0Aexistence%20of%20complex%20disturbances%2C%20such%20as%20parameterization%20error%2C%20measurement%0Anoise%2C%20and%20uncertainties%20of%20payloads%2C%20could%20degrade%20the%20surgeon%27s%20visual%0Aexperience%20and%20compromise%20procedural%20safety.%20To%20address%20these%20limitations%2C%20this%0Apaper%20proposes%20VisionSafeEnhanced%20Visual%20Predictive%20Control%20%28VPC%29%2C%20a%20robust%20and%0Auncertainty-adaptive%20framework%20for%20autonomous%20laparoscope%20control%20that%0Aguarantees%20Field%20of%20View%20%28FoV%29%20safety%20under%20uncertainty.%20Firstly%2C%20Gaussian%0AProcess%20Regression%20%28GPR%29%20is%20utilized%20to%20perform%20hybrid%20%28deterministic%20%2B%0Astochastic%29%20quantification%20of%20operational%20uncertainties%20including%20residual%0Amodel%20uncertainties%2C%20stochastic%20uncertainties%2C%20and%20external%20disturbances.%20Based%0Aon%20uncertainty%20quantification%2C%20a%20novel%20safety%20aware%20trajectory%20optimization%0Aframework%20with%20probabilistic%20guarantees%20is%20proposed%2C%20where%20a%0Auncertainty-adaptive%20safety%20Control%20Barrier%20Function%20%28CBF%29%20condition%20is%20given%0Abased%20on%20uncertainty%20propagation%2C%20and%20chance%20constraints%20are%20simultaneously%0Aformulated%20based%20on%20probabilistic%20approximation.%20This%20uncertainty%20aware%0Aformulation%20enables%20adaptive%20control%20effort%20allocation%2C%20minimizing%20unnecessary%0Acamera%20motion%20while%20maintaining%20robustness.%20The%20proposed%20method%20is%20validated%0Athrough%20comparative%20simulations%20and%20experiments%20on%20a%20commercial%20surgical%20robot%0Aplatform%20%28MicroPort%20MedBot%20Toumai%29%20performing%20a%20sequential%20multi-target%20lymph%0Anode%20dissection.%20Compared%20with%20baseline%20methods%2C%20the%20framework%20maintains%0Anear-perfect%20target%20visibility%20%28%3E99.9%25%29%2C%20reduces%20tracking%20e%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionSafeEnhanced%2520VPC%253A%2520Cautious%2520Predictive%2520Control%2520with%2520Visibility%250A%2520%2520Constraints%2520under%2520Uncertainty%2520for%2520Autonomous%2520Robotic%2520Surgery%26entry.906535625%3DWang%2520Jiayin%2520and%2520Wei%2520Yanran%2520and%2520Jiang%2520Lei%2520and%2520Guo%2520Xiaoyu%2520and%2520Zheng%2520Ayong%2520and%2520Zhao%2520Weidong%2520and%2520Li%2520Zhongkui%26entry.1292438233%3D%2520%2520Autonomous%2520control%2520of%2520the%2520laparoscope%2520in%2520robot-assisted%2520Minimally%2520Invasive%250ASurgery%2520%2528MIS%2529%2520has%2520received%2520considerable%2520research%2520interest%2520due%2520to%2520its%2520potential%250Ato%2520improve%2520surgical%2520safety.%2520Despite%2520progress%2520in%2520pixel-level%2520Image-Based%2520Visual%250AServoing%2520%2528IBVS%2529%2520control%252C%2520the%2520requirement%2520of%2520continuous%2520visibility%2520and%2520the%250Aexistence%2520of%2520complex%2520disturbances%252C%2520such%2520as%2520parameterization%2520error%252C%2520measurement%250Anoise%252C%2520and%2520uncertainties%2520of%2520payloads%252C%2520could%2520degrade%2520the%2520surgeon%2527s%2520visual%250Aexperience%2520and%2520compromise%2520procedural%2520safety.%2520To%2520address%2520these%2520limitations%252C%2520this%250Apaper%2520proposes%2520VisionSafeEnhanced%2520Visual%2520Predictive%2520Control%2520%2528VPC%2529%252C%2520a%2520robust%2520and%250Auncertainty-adaptive%2520framework%2520for%2520autonomous%2520laparoscope%2520control%2520that%250Aguarantees%2520Field%2520of%2520View%2520%2528FoV%2529%2520safety%2520under%2520uncertainty.%2520Firstly%252C%2520Gaussian%250AProcess%2520Regression%2520%2528GPR%2529%2520is%2520utilized%2520to%2520perform%2520hybrid%2520%2528deterministic%2520%252B%250Astochastic%2529%2520quantification%2520of%2520operational%2520uncertainties%2520including%2520residual%250Amodel%2520uncertainties%252C%2520stochastic%2520uncertainties%252C%2520and%2520external%2520disturbances.%2520Based%250Aon%2520uncertainty%2520quantification%252C%2520a%2520novel%2520safety%2520aware%2520trajectory%2520optimization%250Aframework%2520with%2520probabilistic%2520guarantees%2520is%2520proposed%252C%2520where%2520a%250Auncertainty-adaptive%2520safety%2520Control%2520Barrier%2520Function%2520%2528CBF%2529%2520condition%2520is%2520given%250Abased%2520on%2520uncertainty%2520propagation%252C%2520and%2520chance%2520constraints%2520are%2520simultaneously%250Aformulated%2520based%2520on%2520probabilistic%2520approximation.%2520This%2520uncertainty%2520aware%250Aformulation%2520enables%2520adaptive%2520control%2520effort%2520allocation%252C%2520minimizing%2520unnecessary%250Acamera%2520motion%2520while%2520maintaining%2520robustness.%2520The%2520proposed%2520method%2520is%2520validated%250Athrough%2520comparative%2520simulations%2520and%2520experiments%2520on%2520a%2520commercial%2520surgical%2520robot%250Aplatform%2520%2528MicroPort%2520MedBot%2520Toumai%2529%2520performing%2520a%2520sequential%2520multi-target%2520lymph%250Anode%2520dissection.%2520Compared%2520with%2520baseline%2520methods%252C%2520the%2520framework%2520maintains%250Anear-perfect%2520target%2520visibility%2520%2528%253E99.9%2525%2529%252C%2520reduces%2520tracking%2520e%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionSafeEnhanced%20VPC%3A%20Cautious%20Predictive%20Control%20with%20Visibility%0A%20%20Constraints%20under%20Uncertainty%20for%20Autonomous%20Robotic%20Surgery&entry.906535625=Wang%20Jiayin%20and%20Wei%20Yanran%20and%20Jiang%20Lei%20and%20Guo%20Xiaoyu%20and%20Zheng%20Ayong%20and%20Zhao%20Weidong%20and%20Li%20Zhongkui&entry.1292438233=%20%20Autonomous%20control%20of%20the%20laparoscope%20in%20robot-assisted%20Minimally%20Invasive%0ASurgery%20%28MIS%29%20has%20received%20considerable%20research%20interest%20due%20to%20its%20potential%0Ato%20improve%20surgical%20safety.%20Despite%20progress%20in%20pixel-level%20Image-Based%20Visual%0AServoing%20%28IBVS%29%20control%2C%20the%20requirement%20of%20continuous%20visibility%20and%20the%0Aexistence%20of%20complex%20disturbances%2C%20such%20as%20parameterization%20error%2C%20measurement%0Anoise%2C%20and%20uncertainties%20of%20payloads%2C%20could%20degrade%20the%20surgeon%27s%20visual%0Aexperience%20and%20compromise%20procedural%20safety.%20To%20address%20these%20limitations%2C%20this%0Apaper%20proposes%20VisionSafeEnhanced%20Visual%20Predictive%20Control%20%28VPC%29%2C%20a%20robust%20and%0Auncertainty-adaptive%20framework%20for%20autonomous%20laparoscope%20control%20that%0Aguarantees%20Field%20of%20View%20%28FoV%29%20safety%20under%20uncertainty.%20Firstly%2C%20Gaussian%0AProcess%20Regression%20%28GPR%29%20is%20utilized%20to%20perform%20hybrid%20%28deterministic%20%2B%0Astochastic%29%20quantification%20of%20operational%20uncertainties%20including%20residual%0Amodel%20uncertainties%2C%20stochastic%20uncertainties%2C%20and%20external%20disturbances.%20Based%0Aon%20uncertainty%20quantification%2C%20a%20novel%20safety%20aware%20trajectory%20optimization%0Aframework%20with%20probabilistic%20guarantees%20is%20proposed%2C%20where%20a%0Auncertainty-adaptive%20safety%20Control%20Barrier%20Function%20%28CBF%29%20condition%20is%20given%0Abased%20on%20uncertainty%20propagation%2C%20and%20chance%20constraints%20are%20simultaneously%0Aformulated%20based%20on%20probabilistic%20approximation.%20This%20uncertainty%20aware%0Aformulation%20enables%20adaptive%20control%20effort%20allocation%2C%20minimizing%20unnecessary%0Acamera%20motion%20while%20maintaining%20robustness.%20The%20proposed%20method%20is%20validated%0Athrough%20comparative%20simulations%20and%20experiments%20on%20a%20commercial%20surgical%20robot%0Aplatform%20%28MicroPort%20MedBot%20Toumai%29%20performing%20a%20sequential%20multi-target%20lymph%0Anode%20dissection.%20Compared%20with%20baseline%20methods%2C%20the%20framework%20maintains%0Anear-perfect%20target%20visibility%20%28%3E99.9%25%29%2C%20reduces%20tracking%20e%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18937v1&entry.124074799=Read"},
{"title": "Integrating Large Language Model for Improved Causal Discovery", "author": "Taiyu Ban and Lyuzhou Chen and Derui Lyu and Xiangyu Wang and Qinrui Zhu and Qiang Tu and Huanhuan Chen", "abstract": "  Recovering the structure of causal graphical models from observational data\nis an essential yet challenging task for causal discovery in scientific\nscenarios. Domain-specific causal discovery usually relies on expert validation\nor prior analysis to improve the reliability of recovered causality, which is\nyet limited by the scarcity of expert resources. Recently, Large Language\nModels (LLM) have been used for causal analysis across various domain-specific\nscenarios, suggesting its potential as autonomous expert roles in guiding\ndata-based structure learning. However, integrating LLMs into causal discovery\nfaces challenges due to inaccuracies in LLM-based reasoning on revealing the\nactual causal structure. To address this challenge, we propose an\nerror-tolerant LLM-driven causal discovery framework. The error-tolerant\nmechanism is designed three-fold with sufficient consideration on potential\ninaccuracies. In the LLM-based reasoning process, an accuracy-oriented\nprompting strategy restricts causal analysis to a reliable range. Next, a\nknowledge-to-structure transition aligns LLM-derived causal statements with\nstructural causal interactions. In the structure learning process, the\ngoodness-of-fit to data and adherence to LLM-derived priors are balanced to\nfurther address prior inaccuracies. Evaluation of eight real-world causal\nstructures demonstrates the efficacy of our LLM-driven approach in improving\ndata-based causal discovery, along with its robustness to inaccurate\nLLM-derived priors. Codes are available at https://github.com/tyMadara/LLM-CD.\n", "link": "http://arxiv.org/abs/2306.16902v2", "date": "2025-08-26", "relevancy": 2.3701, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Large%20Language%20Model%20for%20Improved%20Causal%20Discovery&body=Title%3A%20Integrating%20Large%20Language%20Model%20for%20Improved%20Causal%20Discovery%0AAuthor%3A%20Taiyu%20Ban%20and%20Lyuzhou%20Chen%20and%20Derui%20Lyu%20and%20Xiangyu%20Wang%20and%20Qinrui%20Zhu%20and%20Qiang%20Tu%20and%20Huanhuan%20Chen%0AAbstract%3A%20%20%20Recovering%20the%20structure%20of%20causal%20graphical%20models%20from%20observational%20data%0Ais%20an%20essential%20yet%20challenging%20task%20for%20causal%20discovery%20in%20scientific%0Ascenarios.%20Domain-specific%20causal%20discovery%20usually%20relies%20on%20expert%20validation%0Aor%20prior%20analysis%20to%20improve%20the%20reliability%20of%20recovered%20causality%2C%20which%20is%0Ayet%20limited%20by%20the%20scarcity%20of%20expert%20resources.%20Recently%2C%20Large%20Language%0AModels%20%28LLM%29%20have%20been%20used%20for%20causal%20analysis%20across%20various%20domain-specific%0Ascenarios%2C%20suggesting%20its%20potential%20as%20autonomous%20expert%20roles%20in%20guiding%0Adata-based%20structure%20learning.%20However%2C%20integrating%20LLMs%20into%20causal%20discovery%0Afaces%20challenges%20due%20to%20inaccuracies%20in%20LLM-based%20reasoning%20on%20revealing%20the%0Aactual%20causal%20structure.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0Aerror-tolerant%20LLM-driven%20causal%20discovery%20framework.%20The%20error-tolerant%0Amechanism%20is%20designed%20three-fold%20with%20sufficient%20consideration%20on%20potential%0Ainaccuracies.%20In%20the%20LLM-based%20reasoning%20process%2C%20an%20accuracy-oriented%0Aprompting%20strategy%20restricts%20causal%20analysis%20to%20a%20reliable%20range.%20Next%2C%20a%0Aknowledge-to-structure%20transition%20aligns%20LLM-derived%20causal%20statements%20with%0Astructural%20causal%20interactions.%20In%20the%20structure%20learning%20process%2C%20the%0Agoodness-of-fit%20to%20data%20and%20adherence%20to%20LLM-derived%20priors%20are%20balanced%20to%0Afurther%20address%20prior%20inaccuracies.%20Evaluation%20of%20eight%20real-world%20causal%0Astructures%20demonstrates%20the%20efficacy%20of%20our%20LLM-driven%20approach%20in%20improving%0Adata-based%20causal%20discovery%2C%20along%20with%20its%20robustness%20to%20inaccurate%0ALLM-derived%20priors.%20Codes%20are%20available%20at%20https%3A//github.com/tyMadara/LLM-CD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.16902v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Large%2520Language%2520Model%2520for%2520Improved%2520Causal%2520Discovery%26entry.906535625%3DTaiyu%2520Ban%2520and%2520Lyuzhou%2520Chen%2520and%2520Derui%2520Lyu%2520and%2520Xiangyu%2520Wang%2520and%2520Qinrui%2520Zhu%2520and%2520Qiang%2520Tu%2520and%2520Huanhuan%2520Chen%26entry.1292438233%3D%2520%2520Recovering%2520the%2520structure%2520of%2520causal%2520graphical%2520models%2520from%2520observational%2520data%250Ais%2520an%2520essential%2520yet%2520challenging%2520task%2520for%2520causal%2520discovery%2520in%2520scientific%250Ascenarios.%2520Domain-specific%2520causal%2520discovery%2520usually%2520relies%2520on%2520expert%2520validation%250Aor%2520prior%2520analysis%2520to%2520improve%2520the%2520reliability%2520of%2520recovered%2520causality%252C%2520which%2520is%250Ayet%2520limited%2520by%2520the%2520scarcity%2520of%2520expert%2520resources.%2520Recently%252C%2520Large%2520Language%250AModels%2520%2528LLM%2529%2520have%2520been%2520used%2520for%2520causal%2520analysis%2520across%2520various%2520domain-specific%250Ascenarios%252C%2520suggesting%2520its%2520potential%2520as%2520autonomous%2520expert%2520roles%2520in%2520guiding%250Adata-based%2520structure%2520learning.%2520However%252C%2520integrating%2520LLMs%2520into%2520causal%2520discovery%250Afaces%2520challenges%2520due%2520to%2520inaccuracies%2520in%2520LLM-based%2520reasoning%2520on%2520revealing%2520the%250Aactual%2520causal%2520structure.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%250Aerror-tolerant%2520LLM-driven%2520causal%2520discovery%2520framework.%2520The%2520error-tolerant%250Amechanism%2520is%2520designed%2520three-fold%2520with%2520sufficient%2520consideration%2520on%2520potential%250Ainaccuracies.%2520In%2520the%2520LLM-based%2520reasoning%2520process%252C%2520an%2520accuracy-oriented%250Aprompting%2520strategy%2520restricts%2520causal%2520analysis%2520to%2520a%2520reliable%2520range.%2520Next%252C%2520a%250Aknowledge-to-structure%2520transition%2520aligns%2520LLM-derived%2520causal%2520statements%2520with%250Astructural%2520causal%2520interactions.%2520In%2520the%2520structure%2520learning%2520process%252C%2520the%250Agoodness-of-fit%2520to%2520data%2520and%2520adherence%2520to%2520LLM-derived%2520priors%2520are%2520balanced%2520to%250Afurther%2520address%2520prior%2520inaccuracies.%2520Evaluation%2520of%2520eight%2520real-world%2520causal%250Astructures%2520demonstrates%2520the%2520efficacy%2520of%2520our%2520LLM-driven%2520approach%2520in%2520improving%250Adata-based%2520causal%2520discovery%252C%2520along%2520with%2520its%2520robustness%2520to%2520inaccurate%250ALLM-derived%2520priors.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/tyMadara/LLM-CD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.16902v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Large%20Language%20Model%20for%20Improved%20Causal%20Discovery&entry.906535625=Taiyu%20Ban%20and%20Lyuzhou%20Chen%20and%20Derui%20Lyu%20and%20Xiangyu%20Wang%20and%20Qinrui%20Zhu%20and%20Qiang%20Tu%20and%20Huanhuan%20Chen&entry.1292438233=%20%20Recovering%20the%20structure%20of%20causal%20graphical%20models%20from%20observational%20data%0Ais%20an%20essential%20yet%20challenging%20task%20for%20causal%20discovery%20in%20scientific%0Ascenarios.%20Domain-specific%20causal%20discovery%20usually%20relies%20on%20expert%20validation%0Aor%20prior%20analysis%20to%20improve%20the%20reliability%20of%20recovered%20causality%2C%20which%20is%0Ayet%20limited%20by%20the%20scarcity%20of%20expert%20resources.%20Recently%2C%20Large%20Language%0AModels%20%28LLM%29%20have%20been%20used%20for%20causal%20analysis%20across%20various%20domain-specific%0Ascenarios%2C%20suggesting%20its%20potential%20as%20autonomous%20expert%20roles%20in%20guiding%0Adata-based%20structure%20learning.%20However%2C%20integrating%20LLMs%20into%20causal%20discovery%0Afaces%20challenges%20due%20to%20inaccuracies%20in%20LLM-based%20reasoning%20on%20revealing%20the%0Aactual%20causal%20structure.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0Aerror-tolerant%20LLM-driven%20causal%20discovery%20framework.%20The%20error-tolerant%0Amechanism%20is%20designed%20three-fold%20with%20sufficient%20consideration%20on%20potential%0Ainaccuracies.%20In%20the%20LLM-based%20reasoning%20process%2C%20an%20accuracy-oriented%0Aprompting%20strategy%20restricts%20causal%20analysis%20to%20a%20reliable%20range.%20Next%2C%20a%0Aknowledge-to-structure%20transition%20aligns%20LLM-derived%20causal%20statements%20with%0Astructural%20causal%20interactions.%20In%20the%20structure%20learning%20process%2C%20the%0Agoodness-of-fit%20to%20data%20and%20adherence%20to%20LLM-derived%20priors%20are%20balanced%20to%0Afurther%20address%20prior%20inaccuracies.%20Evaluation%20of%20eight%20real-world%20causal%0Astructures%20demonstrates%20the%20efficacy%20of%20our%20LLM-driven%20approach%20in%20improving%0Adata-based%20causal%20discovery%2C%20along%20with%20its%20robustness%20to%20inaccurate%0ALLM-derived%20priors.%20Codes%20are%20available%20at%20https%3A//github.com/tyMadara/LLM-CD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.16902v2&entry.124074799=Read"},
{"title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning", "author": "Wei Xiong and Wenting Zhao and Weizhe Yuan and Olga Golovneva and Tong Zhang and Jason Weston and Sainbayar Sukhbaatar", "abstract": "  As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search.\n", "link": "http://arxiv.org/abs/2508.19229v1", "date": "2025-08-26", "relevancy": 2.3621, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4908}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4646}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StepWiser%3A%20Stepwise%20Generative%20Judges%20for%20Wiser%20Reasoning&body=Title%3A%20StepWiser%3A%20Stepwise%20Generative%20Judges%20for%20Wiser%20Reasoning%0AAuthor%3A%20Wei%20Xiong%20and%20Wenting%20Zhao%20and%20Weizhe%20Yuan%20and%20Olga%20Golovneva%20and%20Tong%20Zhang%20and%20Jason%20Weston%20and%20Sainbayar%20Sukhbaatar%0AAbstract%3A%20%20%20As%20models%20increasingly%20leverage%20multi-step%20reasoning%20strategies%20to%20solve%0Acomplex%20problems%2C%20supervising%20the%20logical%20validity%20of%20these%20intermediate%20steps%0Ahas%20become%20a%20critical%20research%20challenge.%20Process%20reward%20models%20address%20this%20by%0Aproviding%20step-by-step%20feedback%2C%20but%20current%20approaches%20have%20two%20major%0Adrawbacks%3A%20they%20typically%20function%20as%20classifiers%20without%20providing%0Aexplanations%2C%20and%20their%20reliance%20on%20supervised%20fine-tuning%20with%20static%20datasets%0Alimits%20generalization.%20Inspired%20by%20recent%20advances%2C%20we%20reframe%20stepwise%20reward%0Amodeling%20from%20a%20classification%20task%20to%20a%20reasoning%20task%20itself.%20We%20thus%20propose%0Aa%20generative%20judge%20that%20reasons%20about%20the%20policy%20model%27s%20reasoning%20steps%20%28i.e.%2C%0Ameta-reasons%29%2C%20outputting%20thinking%20tokens%20before%20delivering%20a%20final%20verdict.%0AOur%20model%2C%20StepWiser%2C%20is%20trained%20by%20reinforcement%20learning%20using%20relative%0Aoutcomes%20of%20rollouts.%20We%20show%20it%20provides%20%28i%29%20better%20judgment%20accuracy%20on%0Aintermediate%20steps%20than%20existing%20methods%3B%20%28ii%29%20can%20be%20used%20to%20improve%20the%0Apolicy%20model%20at%20training%20time%3B%20and%20%28iii%29%20improves%20inference-time%20search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepWiser%253A%2520Stepwise%2520Generative%2520Judges%2520for%2520Wiser%2520Reasoning%26entry.906535625%3DWei%2520Xiong%2520and%2520Wenting%2520Zhao%2520and%2520Weizhe%2520Yuan%2520and%2520Olga%2520Golovneva%2520and%2520Tong%2520Zhang%2520and%2520Jason%2520Weston%2520and%2520Sainbayar%2520Sukhbaatar%26entry.1292438233%3D%2520%2520As%2520models%2520increasingly%2520leverage%2520multi-step%2520reasoning%2520strategies%2520to%2520solve%250Acomplex%2520problems%252C%2520supervising%2520the%2520logical%2520validity%2520of%2520these%2520intermediate%2520steps%250Ahas%2520become%2520a%2520critical%2520research%2520challenge.%2520Process%2520reward%2520models%2520address%2520this%2520by%250Aproviding%2520step-by-step%2520feedback%252C%2520but%2520current%2520approaches%2520have%2520two%2520major%250Adrawbacks%253A%2520they%2520typically%2520function%2520as%2520classifiers%2520without%2520providing%250Aexplanations%252C%2520and%2520their%2520reliance%2520on%2520supervised%2520fine-tuning%2520with%2520static%2520datasets%250Alimits%2520generalization.%2520Inspired%2520by%2520recent%2520advances%252C%2520we%2520reframe%2520stepwise%2520reward%250Amodeling%2520from%2520a%2520classification%2520task%2520to%2520a%2520reasoning%2520task%2520itself.%2520We%2520thus%2520propose%250Aa%2520generative%2520judge%2520that%2520reasons%2520about%2520the%2520policy%2520model%2527s%2520reasoning%2520steps%2520%2528i.e.%252C%250Ameta-reasons%2529%252C%2520outputting%2520thinking%2520tokens%2520before%2520delivering%2520a%2520final%2520verdict.%250AOur%2520model%252C%2520StepWiser%252C%2520is%2520trained%2520by%2520reinforcement%2520learning%2520using%2520relative%250Aoutcomes%2520of%2520rollouts.%2520We%2520show%2520it%2520provides%2520%2528i%2529%2520better%2520judgment%2520accuracy%2520on%250Aintermediate%2520steps%2520than%2520existing%2520methods%253B%2520%2528ii%2529%2520can%2520be%2520used%2520to%2520improve%2520the%250Apolicy%2520model%2520at%2520training%2520time%253B%2520and%2520%2528iii%2529%2520improves%2520inference-time%2520search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StepWiser%3A%20Stepwise%20Generative%20Judges%20for%20Wiser%20Reasoning&entry.906535625=Wei%20Xiong%20and%20Wenting%20Zhao%20and%20Weizhe%20Yuan%20and%20Olga%20Golovneva%20and%20Tong%20Zhang%20and%20Jason%20Weston%20and%20Sainbayar%20Sukhbaatar&entry.1292438233=%20%20As%20models%20increasingly%20leverage%20multi-step%20reasoning%20strategies%20to%20solve%0Acomplex%20problems%2C%20supervising%20the%20logical%20validity%20of%20these%20intermediate%20steps%0Ahas%20become%20a%20critical%20research%20challenge.%20Process%20reward%20models%20address%20this%20by%0Aproviding%20step-by-step%20feedback%2C%20but%20current%20approaches%20have%20two%20major%0Adrawbacks%3A%20they%20typically%20function%20as%20classifiers%20without%20providing%0Aexplanations%2C%20and%20their%20reliance%20on%20supervised%20fine-tuning%20with%20static%20datasets%0Alimits%20generalization.%20Inspired%20by%20recent%20advances%2C%20we%20reframe%20stepwise%20reward%0Amodeling%20from%20a%20classification%20task%20to%20a%20reasoning%20task%20itself.%20We%20thus%20propose%0Aa%20generative%20judge%20that%20reasons%20about%20the%20policy%20model%27s%20reasoning%20steps%20%28i.e.%2C%0Ameta-reasons%29%2C%20outputting%20thinking%20tokens%20before%20delivering%20a%20final%20verdict.%0AOur%20model%2C%20StepWiser%2C%20is%20trained%20by%20reinforcement%20learning%20using%20relative%0Aoutcomes%20of%20rollouts.%20We%20show%20it%20provides%20%28i%29%20better%20judgment%20accuracy%20on%0Aintermediate%20steps%20than%20existing%20methods%3B%20%28ii%29%20can%20be%20used%20to%20improve%20the%0Apolicy%20model%20at%20training%20time%3B%20and%20%28iii%29%20improves%20inference-time%20search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19229v1&entry.124074799=Read"},
{"title": "Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level\n  Encoding in Curriculum-Based Online Learning Systems", "author": "Qian Xiao and Conn Breathnach and Ioana Ghergulescu and Conor O'Sullivan and Keith Johnston and Vincent Wade", "abstract": "  The surge in the adoption of Intelligent Tutoring Systems (ITSs) in\neducation, while being integral to curriculum-based learning, can inadvertently\nexacerbate performance gaps. To address this problem, student profiling becomes\ncrucial for tracking progress, identifying struggling students, and alleviating\ndisparities among students. Such profiling requires measuring student behaviors\nand performance across different aspects, such as content coverage, learning\nintensity, and proficiency in different concepts within a learning topic.\n  In this study, we introduce CTGraph, a graph-level representation learning\napproach to profile learner behaviors and performance in a self-supervised\nmanner. Our experiments demonstrate that CTGraph can provide a holistic view of\nstudent learning journeys, accounting for different aspects of student\nbehaviors and performance, as well as variations in their learning paths as\naligned to the curriculum structure. We also show that our approach can\nidentify struggling students and provide comparative analysis of diverse groups\nto pinpoint when and where students are struggling. As such, our approach opens\nmore opportunities to empower educators with rich insights into student\nlearning journeys and paves the way for more targeted interventions.\n", "link": "http://arxiv.org/abs/2508.18925v1", "date": "2025-08-26", "relevancy": 2.3587, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20Is%20Lagging%20Behind%3A%20Profiling%20Student%20Behaviors%20with%20Graph-Level%0A%20%20Encoding%20in%20Curriculum-Based%20Online%20Learning%20Systems&body=Title%3A%20Who%20Is%20Lagging%20Behind%3A%20Profiling%20Student%20Behaviors%20with%20Graph-Level%0A%20%20Encoding%20in%20Curriculum-Based%20Online%20Learning%20Systems%0AAuthor%3A%20Qian%20Xiao%20and%20Conn%20Breathnach%20and%20Ioana%20Ghergulescu%20and%20Conor%20O%27Sullivan%20and%20Keith%20Johnston%20and%20Vincent%20Wade%0AAbstract%3A%20%20%20The%20surge%20in%20the%20adoption%20of%20Intelligent%20Tutoring%20Systems%20%28ITSs%29%20in%0Aeducation%2C%20while%20being%20integral%20to%20curriculum-based%20learning%2C%20can%20inadvertently%0Aexacerbate%20performance%20gaps.%20To%20address%20this%20problem%2C%20student%20profiling%20becomes%0Acrucial%20for%20tracking%20progress%2C%20identifying%20struggling%20students%2C%20and%20alleviating%0Adisparities%20among%20students.%20Such%20profiling%20requires%20measuring%20student%20behaviors%0Aand%20performance%20across%20different%20aspects%2C%20such%20as%20content%20coverage%2C%20learning%0Aintensity%2C%20and%20proficiency%20in%20different%20concepts%20within%20a%20learning%20topic.%0A%20%20In%20this%20study%2C%20we%20introduce%20CTGraph%2C%20a%20graph-level%20representation%20learning%0Aapproach%20to%20profile%20learner%20behaviors%20and%20performance%20in%20a%20self-supervised%0Amanner.%20Our%20experiments%20demonstrate%20that%20CTGraph%20can%20provide%20a%20holistic%20view%20of%0Astudent%20learning%20journeys%2C%20accounting%20for%20different%20aspects%20of%20student%0Abehaviors%20and%20performance%2C%20as%20well%20as%20variations%20in%20their%20learning%20paths%20as%0Aaligned%20to%20the%20curriculum%20structure.%20We%20also%20show%20that%20our%20approach%20can%0Aidentify%20struggling%20students%20and%20provide%20comparative%20analysis%20of%20diverse%20groups%0Ato%20pinpoint%20when%20and%20where%20students%20are%20struggling.%20As%20such%2C%20our%20approach%20opens%0Amore%20opportunities%20to%20empower%20educators%20with%20rich%20insights%20into%20student%0Alearning%20journeys%20and%20paves%20the%20way%20for%20more%20targeted%20interventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520Is%2520Lagging%2520Behind%253A%2520Profiling%2520Student%2520Behaviors%2520with%2520Graph-Level%250A%2520%2520Encoding%2520in%2520Curriculum-Based%2520Online%2520Learning%2520Systems%26entry.906535625%3DQian%2520Xiao%2520and%2520Conn%2520Breathnach%2520and%2520Ioana%2520Ghergulescu%2520and%2520Conor%2520O%2527Sullivan%2520and%2520Keith%2520Johnston%2520and%2520Vincent%2520Wade%26entry.1292438233%3D%2520%2520The%2520surge%2520in%2520the%2520adoption%2520of%2520Intelligent%2520Tutoring%2520Systems%2520%2528ITSs%2529%2520in%250Aeducation%252C%2520while%2520being%2520integral%2520to%2520curriculum-based%2520learning%252C%2520can%2520inadvertently%250Aexacerbate%2520performance%2520gaps.%2520To%2520address%2520this%2520problem%252C%2520student%2520profiling%2520becomes%250Acrucial%2520for%2520tracking%2520progress%252C%2520identifying%2520struggling%2520students%252C%2520and%2520alleviating%250Adisparities%2520among%2520students.%2520Such%2520profiling%2520requires%2520measuring%2520student%2520behaviors%250Aand%2520performance%2520across%2520different%2520aspects%252C%2520such%2520as%2520content%2520coverage%252C%2520learning%250Aintensity%252C%2520and%2520proficiency%2520in%2520different%2520concepts%2520within%2520a%2520learning%2520topic.%250A%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520CTGraph%252C%2520a%2520graph-level%2520representation%2520learning%250Aapproach%2520to%2520profile%2520learner%2520behaviors%2520and%2520performance%2520in%2520a%2520self-supervised%250Amanner.%2520Our%2520experiments%2520demonstrate%2520that%2520CTGraph%2520can%2520provide%2520a%2520holistic%2520view%2520of%250Astudent%2520learning%2520journeys%252C%2520accounting%2520for%2520different%2520aspects%2520of%2520student%250Abehaviors%2520and%2520performance%252C%2520as%2520well%2520as%2520variations%2520in%2520their%2520learning%2520paths%2520as%250Aaligned%2520to%2520the%2520curriculum%2520structure.%2520We%2520also%2520show%2520that%2520our%2520approach%2520can%250Aidentify%2520struggling%2520students%2520and%2520provide%2520comparative%2520analysis%2520of%2520diverse%2520groups%250Ato%2520pinpoint%2520when%2520and%2520where%2520students%2520are%2520struggling.%2520As%2520such%252C%2520our%2520approach%2520opens%250Amore%2520opportunities%2520to%2520empower%2520educators%2520with%2520rich%2520insights%2520into%2520student%250Alearning%2520journeys%2520and%2520paves%2520the%2520way%2520for%2520more%2520targeted%2520interventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20Is%20Lagging%20Behind%3A%20Profiling%20Student%20Behaviors%20with%20Graph-Level%0A%20%20Encoding%20in%20Curriculum-Based%20Online%20Learning%20Systems&entry.906535625=Qian%20Xiao%20and%20Conn%20Breathnach%20and%20Ioana%20Ghergulescu%20and%20Conor%20O%27Sullivan%20and%20Keith%20Johnston%20and%20Vincent%20Wade&entry.1292438233=%20%20The%20surge%20in%20the%20adoption%20of%20Intelligent%20Tutoring%20Systems%20%28ITSs%29%20in%0Aeducation%2C%20while%20being%20integral%20to%20curriculum-based%20learning%2C%20can%20inadvertently%0Aexacerbate%20performance%20gaps.%20To%20address%20this%20problem%2C%20student%20profiling%20becomes%0Acrucial%20for%20tracking%20progress%2C%20identifying%20struggling%20students%2C%20and%20alleviating%0Adisparities%20among%20students.%20Such%20profiling%20requires%20measuring%20student%20behaviors%0Aand%20performance%20across%20different%20aspects%2C%20such%20as%20content%20coverage%2C%20learning%0Aintensity%2C%20and%20proficiency%20in%20different%20concepts%20within%20a%20learning%20topic.%0A%20%20In%20this%20study%2C%20we%20introduce%20CTGraph%2C%20a%20graph-level%20representation%20learning%0Aapproach%20to%20profile%20learner%20behaviors%20and%20performance%20in%20a%20self-supervised%0Amanner.%20Our%20experiments%20demonstrate%20that%20CTGraph%20can%20provide%20a%20holistic%20view%20of%0Astudent%20learning%20journeys%2C%20accounting%20for%20different%20aspects%20of%20student%0Abehaviors%20and%20performance%2C%20as%20well%20as%20variations%20in%20their%20learning%20paths%20as%0Aaligned%20to%20the%20curriculum%20structure.%20We%20also%20show%20that%20our%20approach%20can%0Aidentify%20struggling%20students%20and%20provide%20comparative%20analysis%20of%20diverse%20groups%0Ato%20pinpoint%20when%20and%20where%20students%20are%20struggling.%20As%20such%2C%20our%20approach%20opens%0Amore%20opportunities%20to%20empower%20educators%20with%20rich%20insights%20into%20student%0Alearning%20journeys%20and%20paves%20the%20way%20for%20more%20targeted%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18925v1&entry.124074799=Read"},
{"title": "Novel Approaches to Artificial Intelligence Development Based on the\n  Nearest Neighbor Method", "author": "I. I. Priezzhev and D. A. Danko and A. V. Shubin", "abstract": "  Modern neural network technologies, including large language models, have\nachieved remarkable success in various applied artificial intelligence\napplications, however, they face a range of fundamental limitations. Among them\nare hallucination effects, high computational complexity of training and\ninference, costly fine-tuning, and catastrophic forgetting issues. These\nlimitations significantly hinder the use of neural networks in critical areas\nsuch as medicine, industrial process management, and scientific research. This\narticle proposes an alternative approach based on the nearest neighbors method\nwith hierarchical clustering structures. Employing the k-nearest neighbors\nalgorithm significantly reduces or completely eliminates hallucination effects\nwhile simplifying model expansion and fine-tuning without the need for\nretraining the entire network. To overcome the high computational load of the\nk-nearest neighbors method, the paper proposes using tree-like data structures\nbased on Kohonen self-organizing maps, thereby greatly accelerating nearest\nneighbor searches. Tests conducted on handwritten digit recognition and simple\nsubtitle translation tasks confirmed the effectiveness of the proposed\napproach. With only a slight reduction in accuracy, the nearest neighbor search\ntime was reduced hundreds of times compared to exhaustive search methods. The\nproposed method features transparency and interpretability, closely aligns with\nhuman cognitive mechanisms, and demonstrates potential for extensive use in\ntasks requiring high reliability and explainable results.\n", "link": "http://arxiv.org/abs/2508.18953v1", "date": "2025-08-26", "relevancy": 2.3566, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4748}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Approaches%20to%20Artificial%20Intelligence%20Development%20Based%20on%20the%0A%20%20Nearest%20Neighbor%20Method&body=Title%3A%20Novel%20Approaches%20to%20Artificial%20Intelligence%20Development%20Based%20on%20the%0A%20%20Nearest%20Neighbor%20Method%0AAuthor%3A%20I.%20I.%20Priezzhev%20and%20D.%20A.%20Danko%20and%20A.%20V.%20Shubin%0AAbstract%3A%20%20%20Modern%20neural%20network%20technologies%2C%20including%20large%20language%20models%2C%20have%0Aachieved%20remarkable%20success%20in%20various%20applied%20artificial%20intelligence%0Aapplications%2C%20however%2C%20they%20face%20a%20range%20of%20fundamental%20limitations.%20Among%20them%0Aare%20hallucination%20effects%2C%20high%20computational%20complexity%20of%20training%20and%0Ainference%2C%20costly%20fine-tuning%2C%20and%20catastrophic%20forgetting%20issues.%20These%0Alimitations%20significantly%20hinder%20the%20use%20of%20neural%20networks%20in%20critical%20areas%0Asuch%20as%20medicine%2C%20industrial%20process%20management%2C%20and%20scientific%20research.%20This%0Aarticle%20proposes%20an%20alternative%20approach%20based%20on%20the%20nearest%20neighbors%20method%0Awith%20hierarchical%20clustering%20structures.%20Employing%20the%20k-nearest%20neighbors%0Aalgorithm%20significantly%20reduces%20or%20completely%20eliminates%20hallucination%20effects%0Awhile%20simplifying%20model%20expansion%20and%20fine-tuning%20without%20the%20need%20for%0Aretraining%20the%20entire%20network.%20To%20overcome%20the%20high%20computational%20load%20of%20the%0Ak-nearest%20neighbors%20method%2C%20the%20paper%20proposes%20using%20tree-like%20data%20structures%0Abased%20on%20Kohonen%20self-organizing%20maps%2C%20thereby%20greatly%20accelerating%20nearest%0Aneighbor%20searches.%20Tests%20conducted%20on%20handwritten%20digit%20recognition%20and%20simple%0Asubtitle%20translation%20tasks%20confirmed%20the%20effectiveness%20of%20the%20proposed%0Aapproach.%20With%20only%20a%20slight%20reduction%20in%20accuracy%2C%20the%20nearest%20neighbor%20search%0Atime%20was%20reduced%20hundreds%20of%20times%20compared%20to%20exhaustive%20search%20methods.%20The%0Aproposed%20method%20features%20transparency%20and%20interpretability%2C%20closely%20aligns%20with%0Ahuman%20cognitive%20mechanisms%2C%20and%20demonstrates%20potential%20for%20extensive%20use%20in%0Atasks%20requiring%20high%20reliability%20and%20explainable%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Approaches%2520to%2520Artificial%2520Intelligence%2520Development%2520Based%2520on%2520the%250A%2520%2520Nearest%2520Neighbor%2520Method%26entry.906535625%3DI.%2520I.%2520Priezzhev%2520and%2520D.%2520A.%2520Danko%2520and%2520A.%2520V.%2520Shubin%26entry.1292438233%3D%2520%2520Modern%2520neural%2520network%2520technologies%252C%2520including%2520large%2520language%2520models%252C%2520have%250Aachieved%2520remarkable%2520success%2520in%2520various%2520applied%2520artificial%2520intelligence%250Aapplications%252C%2520however%252C%2520they%2520face%2520a%2520range%2520of%2520fundamental%2520limitations.%2520Among%2520them%250Aare%2520hallucination%2520effects%252C%2520high%2520computational%2520complexity%2520of%2520training%2520and%250Ainference%252C%2520costly%2520fine-tuning%252C%2520and%2520catastrophic%2520forgetting%2520issues.%2520These%250Alimitations%2520significantly%2520hinder%2520the%2520use%2520of%2520neural%2520networks%2520in%2520critical%2520areas%250Asuch%2520as%2520medicine%252C%2520industrial%2520process%2520management%252C%2520and%2520scientific%2520research.%2520This%250Aarticle%2520proposes%2520an%2520alternative%2520approach%2520based%2520on%2520the%2520nearest%2520neighbors%2520method%250Awith%2520hierarchical%2520clustering%2520structures.%2520Employing%2520the%2520k-nearest%2520neighbors%250Aalgorithm%2520significantly%2520reduces%2520or%2520completely%2520eliminates%2520hallucination%2520effects%250Awhile%2520simplifying%2520model%2520expansion%2520and%2520fine-tuning%2520without%2520the%2520need%2520for%250Aretraining%2520the%2520entire%2520network.%2520To%2520overcome%2520the%2520high%2520computational%2520load%2520of%2520the%250Ak-nearest%2520neighbors%2520method%252C%2520the%2520paper%2520proposes%2520using%2520tree-like%2520data%2520structures%250Abased%2520on%2520Kohonen%2520self-organizing%2520maps%252C%2520thereby%2520greatly%2520accelerating%2520nearest%250Aneighbor%2520searches.%2520Tests%2520conducted%2520on%2520handwritten%2520digit%2520recognition%2520and%2520simple%250Asubtitle%2520translation%2520tasks%2520confirmed%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aapproach.%2520With%2520only%2520a%2520slight%2520reduction%2520in%2520accuracy%252C%2520the%2520nearest%2520neighbor%2520search%250Atime%2520was%2520reduced%2520hundreds%2520of%2520times%2520compared%2520to%2520exhaustive%2520search%2520methods.%2520The%250Aproposed%2520method%2520features%2520transparency%2520and%2520interpretability%252C%2520closely%2520aligns%2520with%250Ahuman%2520cognitive%2520mechanisms%252C%2520and%2520demonstrates%2520potential%2520for%2520extensive%2520use%2520in%250Atasks%2520requiring%2520high%2520reliability%2520and%2520explainable%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Approaches%20to%20Artificial%20Intelligence%20Development%20Based%20on%20the%0A%20%20Nearest%20Neighbor%20Method&entry.906535625=I.%20I.%20Priezzhev%20and%20D.%20A.%20Danko%20and%20A.%20V.%20Shubin&entry.1292438233=%20%20Modern%20neural%20network%20technologies%2C%20including%20large%20language%20models%2C%20have%0Aachieved%20remarkable%20success%20in%20various%20applied%20artificial%20intelligence%0Aapplications%2C%20however%2C%20they%20face%20a%20range%20of%20fundamental%20limitations.%20Among%20them%0Aare%20hallucination%20effects%2C%20high%20computational%20complexity%20of%20training%20and%0Ainference%2C%20costly%20fine-tuning%2C%20and%20catastrophic%20forgetting%20issues.%20These%0Alimitations%20significantly%20hinder%20the%20use%20of%20neural%20networks%20in%20critical%20areas%0Asuch%20as%20medicine%2C%20industrial%20process%20management%2C%20and%20scientific%20research.%20This%0Aarticle%20proposes%20an%20alternative%20approach%20based%20on%20the%20nearest%20neighbors%20method%0Awith%20hierarchical%20clustering%20structures.%20Employing%20the%20k-nearest%20neighbors%0Aalgorithm%20significantly%20reduces%20or%20completely%20eliminates%20hallucination%20effects%0Awhile%20simplifying%20model%20expansion%20and%20fine-tuning%20without%20the%20need%20for%0Aretraining%20the%20entire%20network.%20To%20overcome%20the%20high%20computational%20load%20of%20the%0Ak-nearest%20neighbors%20method%2C%20the%20paper%20proposes%20using%20tree-like%20data%20structures%0Abased%20on%20Kohonen%20self-organizing%20maps%2C%20thereby%20greatly%20accelerating%20nearest%0Aneighbor%20searches.%20Tests%20conducted%20on%20handwritten%20digit%20recognition%20and%20simple%0Asubtitle%20translation%20tasks%20confirmed%20the%20effectiveness%20of%20the%20proposed%0Aapproach.%20With%20only%20a%20slight%20reduction%20in%20accuracy%2C%20the%20nearest%20neighbor%20search%0Atime%20was%20reduced%20hundreds%20of%20times%20compared%20to%20exhaustive%20search%20methods.%20The%0Aproposed%20method%20features%20transparency%20and%20interpretability%2C%20closely%20aligns%20with%0Ahuman%20cognitive%20mechanisms%2C%20and%20demonstrates%20potential%20for%20extensive%20use%20in%0Atasks%20requiring%20high%20reliability%20and%20explainable%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18953v1&entry.124074799=Read"},
{"title": "CARMA: Collocation-Aware Resource Manager with GPU Memory Estimator", "author": "Ehsan Yousefzadeh-Asl-Miandoab and Reza Karimzadeh and Bulat Ibragimov and Florina M. Ciorba and P\u0131nar T\u00f6z\u00fcn", "abstract": "  Studies conducted on enterprise-scale infrastructure have shown that GPUs --\nthe core computational resource for deep learning (DL) training -- are often\nsignificantly underutilized. DL task collocation on GPUs is an opportunity to\naddress this challenge. However, it may result in (1) out-of-memory crashes for\nthe subsequently arriving task and (2) slowdowns for all tasks sharing the GPU\ndue to resource interference. The former challenge poses a threat to\nrobustness, while the latter affects the quality of service and energy\nefficiency.\n  We propose CARMA, a server-scale task-level collocation-aware resource\nmanagement system that handles both collocation challenges. CARMA encompasses\nGPUMemNet, a novel ML-based GPU memory estimator framework for DL training\ntasks, to minimize out-of-memory errors and introduces collocation policies\nthat cap GPU utilization to minimize interference. Furthermore, CARMA\nintroduces a recovery method to ensure robust restart of tasks that crash. Our\nevaluation on traces modeled after real-world DL training task traces shows\nthat CARMA increases the GPU utilization over time by 39.3\\%, decreases the\nend-to-end execution time by $\\sim$26.7\\%, and reduces the GPU energy use by\n$\\sim$14.2\\%.\n", "link": "http://arxiv.org/abs/2508.19073v1", "date": "2025-08-26", "relevancy": 2.3506, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4778}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4732}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARMA%3A%20Collocation-Aware%20Resource%20Manager%20with%20GPU%20Memory%20Estimator&body=Title%3A%20CARMA%3A%20Collocation-Aware%20Resource%20Manager%20with%20GPU%20Memory%20Estimator%0AAuthor%3A%20Ehsan%20Yousefzadeh-Asl-Miandoab%20and%20Reza%20Karimzadeh%20and%20Bulat%20Ibragimov%20and%20Florina%20M.%20Ciorba%20and%20P%C4%B1nar%20T%C3%B6z%C3%BCn%0AAbstract%3A%20%20%20Studies%20conducted%20on%20enterprise-scale%20infrastructure%20have%20shown%20that%20GPUs%20--%0Athe%20core%20computational%20resource%20for%20deep%20learning%20%28DL%29%20training%20--%20are%20often%0Asignificantly%20underutilized.%20DL%20task%20collocation%20on%20GPUs%20is%20an%20opportunity%20to%0Aaddress%20this%20challenge.%20However%2C%20it%20may%20result%20in%20%281%29%20out-of-memory%20crashes%20for%0Athe%20subsequently%20arriving%20task%20and%20%282%29%20slowdowns%20for%20all%20tasks%20sharing%20the%20GPU%0Adue%20to%20resource%20interference.%20The%20former%20challenge%20poses%20a%20threat%20to%0Arobustness%2C%20while%20the%20latter%20affects%20the%20quality%20of%20service%20and%20energy%0Aefficiency.%0A%20%20We%20propose%20CARMA%2C%20a%20server-scale%20task-level%20collocation-aware%20resource%0Amanagement%20system%20that%20handles%20both%20collocation%20challenges.%20CARMA%20encompasses%0AGPUMemNet%2C%20a%20novel%20ML-based%20GPU%20memory%20estimator%20framework%20for%20DL%20training%0Atasks%2C%20to%20minimize%20out-of-memory%20errors%20and%20introduces%20collocation%20policies%0Athat%20cap%20GPU%20utilization%20to%20minimize%20interference.%20Furthermore%2C%20CARMA%0Aintroduces%20a%20recovery%20method%20to%20ensure%20robust%20restart%20of%20tasks%20that%20crash.%20Our%0Aevaluation%20on%20traces%20modeled%20after%20real-world%20DL%20training%20task%20traces%20shows%0Athat%20CARMA%20increases%20the%20GPU%20utilization%20over%20time%20by%2039.3%5C%25%2C%20decreases%20the%0Aend-to-end%20execution%20time%20by%20%24%5Csim%2426.7%5C%25%2C%20and%20reduces%20the%20GPU%20energy%20use%20by%0A%24%5Csim%2414.2%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARMA%253A%2520Collocation-Aware%2520Resource%2520Manager%2520with%2520GPU%2520Memory%2520Estimator%26entry.906535625%3DEhsan%2520Yousefzadeh-Asl-Miandoab%2520and%2520Reza%2520Karimzadeh%2520and%2520Bulat%2520Ibragimov%2520and%2520Florina%2520M.%2520Ciorba%2520and%2520P%25C4%25B1nar%2520T%25C3%25B6z%25C3%25BCn%26entry.1292438233%3D%2520%2520Studies%2520conducted%2520on%2520enterprise-scale%2520infrastructure%2520have%2520shown%2520that%2520GPUs%2520--%250Athe%2520core%2520computational%2520resource%2520for%2520deep%2520learning%2520%2528DL%2529%2520training%2520--%2520are%2520often%250Asignificantly%2520underutilized.%2520DL%2520task%2520collocation%2520on%2520GPUs%2520is%2520an%2520opportunity%2520to%250Aaddress%2520this%2520challenge.%2520However%252C%2520it%2520may%2520result%2520in%2520%25281%2529%2520out-of-memory%2520crashes%2520for%250Athe%2520subsequently%2520arriving%2520task%2520and%2520%25282%2529%2520slowdowns%2520for%2520all%2520tasks%2520sharing%2520the%2520GPU%250Adue%2520to%2520resource%2520interference.%2520The%2520former%2520challenge%2520poses%2520a%2520threat%2520to%250Arobustness%252C%2520while%2520the%2520latter%2520affects%2520the%2520quality%2520of%2520service%2520and%2520energy%250Aefficiency.%250A%2520%2520We%2520propose%2520CARMA%252C%2520a%2520server-scale%2520task-level%2520collocation-aware%2520resource%250Amanagement%2520system%2520that%2520handles%2520both%2520collocation%2520challenges.%2520CARMA%2520encompasses%250AGPUMemNet%252C%2520a%2520novel%2520ML-based%2520GPU%2520memory%2520estimator%2520framework%2520for%2520DL%2520training%250Atasks%252C%2520to%2520minimize%2520out-of-memory%2520errors%2520and%2520introduces%2520collocation%2520policies%250Athat%2520cap%2520GPU%2520utilization%2520to%2520minimize%2520interference.%2520Furthermore%252C%2520CARMA%250Aintroduces%2520a%2520recovery%2520method%2520to%2520ensure%2520robust%2520restart%2520of%2520tasks%2520that%2520crash.%2520Our%250Aevaluation%2520on%2520traces%2520modeled%2520after%2520real-world%2520DL%2520training%2520task%2520traces%2520shows%250Athat%2520CARMA%2520increases%2520the%2520GPU%2520utilization%2520over%2520time%2520by%252039.3%255C%2525%252C%2520decreases%2520the%250Aend-to-end%2520execution%2520time%2520by%2520%2524%255Csim%252426.7%255C%2525%252C%2520and%2520reduces%2520the%2520GPU%2520energy%2520use%2520by%250A%2524%255Csim%252414.2%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARMA%3A%20Collocation-Aware%20Resource%20Manager%20with%20GPU%20Memory%20Estimator&entry.906535625=Ehsan%20Yousefzadeh-Asl-Miandoab%20and%20Reza%20Karimzadeh%20and%20Bulat%20Ibragimov%20and%20Florina%20M.%20Ciorba%20and%20P%C4%B1nar%20T%C3%B6z%C3%BCn&entry.1292438233=%20%20Studies%20conducted%20on%20enterprise-scale%20infrastructure%20have%20shown%20that%20GPUs%20--%0Athe%20core%20computational%20resource%20for%20deep%20learning%20%28DL%29%20training%20--%20are%20often%0Asignificantly%20underutilized.%20DL%20task%20collocation%20on%20GPUs%20is%20an%20opportunity%20to%0Aaddress%20this%20challenge.%20However%2C%20it%20may%20result%20in%20%281%29%20out-of-memory%20crashes%20for%0Athe%20subsequently%20arriving%20task%20and%20%282%29%20slowdowns%20for%20all%20tasks%20sharing%20the%20GPU%0Adue%20to%20resource%20interference.%20The%20former%20challenge%20poses%20a%20threat%20to%0Arobustness%2C%20while%20the%20latter%20affects%20the%20quality%20of%20service%20and%20energy%0Aefficiency.%0A%20%20We%20propose%20CARMA%2C%20a%20server-scale%20task-level%20collocation-aware%20resource%0Amanagement%20system%20that%20handles%20both%20collocation%20challenges.%20CARMA%20encompasses%0AGPUMemNet%2C%20a%20novel%20ML-based%20GPU%20memory%20estimator%20framework%20for%20DL%20training%0Atasks%2C%20to%20minimize%20out-of-memory%20errors%20and%20introduces%20collocation%20policies%0Athat%20cap%20GPU%20utilization%20to%20minimize%20interference.%20Furthermore%2C%20CARMA%0Aintroduces%20a%20recovery%20method%20to%20ensure%20robust%20restart%20of%20tasks%20that%20crash.%20Our%0Aevaluation%20on%20traces%20modeled%20after%20real-world%20DL%20training%20task%20traces%20shows%0Athat%20CARMA%20increases%20the%20GPU%20utilization%20over%20time%20by%2039.3%5C%25%2C%20decreases%20the%0Aend-to-end%20execution%20time%20by%20%24%5Csim%2426.7%5C%25%2C%20and%20reduces%20the%20GPU%20energy%20use%20by%0A%24%5Csim%2414.2%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19073v1&entry.124074799=Read"},
{"title": "Attackers Strike Back? Not Anymore -- An Ensemble of RL Defenders\n  Awakens for APT Detection", "author": "Sidahmed Benabderrahmane and Talal Rahwan", "abstract": "  Advanced Persistent Threats (APTs) represent a growing menace to modern\ndigital infrastructure. Unlike traditional cyberattacks, APTs are stealthy,\nadaptive, and long-lasting, often bypassing signature-based detection systems.\nThis paper introduces a novel framework for APT detection that unites deep\nlearning, reinforcement learning (RL), and active learning into a cohesive,\nadaptive defense system. Our system combines auto-encoders for latent\nbehavioral encoding with a multi-agent ensemble of RL-based defenders, each\ntrained to distinguish between benign and malicious process behaviors. We\nidentify a critical challenge in existing detection systems: their static\nnature and inability to adapt to evolving attack strategies. To this end, our\narchitecture includes multiple RL agents (Q-Learning, PPO, DQN, adversarial\ndefenders), each analyzing latent vectors generated by an auto-encoder. When\nany agent is uncertain about its decision, the system triggers an active\nlearning loop to simulate expert feedback, thus refining decision boundaries.\nAn ensemble voting mechanism, weighted by each agent's performance, ensures\nrobust final predictions.\n", "link": "http://arxiv.org/abs/2508.19072v1", "date": "2025-08-26", "relevancy": 2.348, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4901}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4594}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attackers%20Strike%20Back%3F%20Not%20Anymore%20--%20An%20Ensemble%20of%20RL%20Defenders%0A%20%20Awakens%20for%20APT%20Detection&body=Title%3A%20Attackers%20Strike%20Back%3F%20Not%20Anymore%20--%20An%20Ensemble%20of%20RL%20Defenders%0A%20%20Awakens%20for%20APT%20Detection%0AAuthor%3A%20Sidahmed%20Benabderrahmane%20and%20Talal%20Rahwan%0AAbstract%3A%20%20%20Advanced%20Persistent%20Threats%20%28APTs%29%20represent%20a%20growing%20menace%20to%20modern%0Adigital%20infrastructure.%20Unlike%20traditional%20cyberattacks%2C%20APTs%20are%20stealthy%2C%0Aadaptive%2C%20and%20long-lasting%2C%20often%20bypassing%20signature-based%20detection%20systems.%0AThis%20paper%20introduces%20a%20novel%20framework%20for%20APT%20detection%20that%20unites%20deep%0Alearning%2C%20reinforcement%20learning%20%28RL%29%2C%20and%20active%20learning%20into%20a%20cohesive%2C%0Aadaptive%20defense%20system.%20Our%20system%20combines%20auto-encoders%20for%20latent%0Abehavioral%20encoding%20with%20a%20multi-agent%20ensemble%20of%20RL-based%20defenders%2C%20each%0Atrained%20to%20distinguish%20between%20benign%20and%20malicious%20process%20behaviors.%20We%0Aidentify%20a%20critical%20challenge%20in%20existing%20detection%20systems%3A%20their%20static%0Anature%20and%20inability%20to%20adapt%20to%20evolving%20attack%20strategies.%20To%20this%20end%2C%20our%0Aarchitecture%20includes%20multiple%20RL%20agents%20%28Q-Learning%2C%20PPO%2C%20DQN%2C%20adversarial%0Adefenders%29%2C%20each%20analyzing%20latent%20vectors%20generated%20by%20an%20auto-encoder.%20When%0Aany%20agent%20is%20uncertain%20about%20its%20decision%2C%20the%20system%20triggers%20an%20active%0Alearning%20loop%20to%20simulate%20expert%20feedback%2C%20thus%20refining%20decision%20boundaries.%0AAn%20ensemble%20voting%20mechanism%2C%20weighted%20by%20each%20agent%27s%20performance%2C%20ensures%0Arobust%20final%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttackers%2520Strike%2520Back%253F%2520Not%2520Anymore%2520--%2520An%2520Ensemble%2520of%2520RL%2520Defenders%250A%2520%2520Awakens%2520for%2520APT%2520Detection%26entry.906535625%3DSidahmed%2520Benabderrahmane%2520and%2520Talal%2520Rahwan%26entry.1292438233%3D%2520%2520Advanced%2520Persistent%2520Threats%2520%2528APTs%2529%2520represent%2520a%2520growing%2520menace%2520to%2520modern%250Adigital%2520infrastructure.%2520Unlike%2520traditional%2520cyberattacks%252C%2520APTs%2520are%2520stealthy%252C%250Aadaptive%252C%2520and%2520long-lasting%252C%2520often%2520bypassing%2520signature-based%2520detection%2520systems.%250AThis%2520paper%2520introduces%2520a%2520novel%2520framework%2520for%2520APT%2520detection%2520that%2520unites%2520deep%250Alearning%252C%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520and%2520active%2520learning%2520into%2520a%2520cohesive%252C%250Aadaptive%2520defense%2520system.%2520Our%2520system%2520combines%2520auto-encoders%2520for%2520latent%250Abehavioral%2520encoding%2520with%2520a%2520multi-agent%2520ensemble%2520of%2520RL-based%2520defenders%252C%2520each%250Atrained%2520to%2520distinguish%2520between%2520benign%2520and%2520malicious%2520process%2520behaviors.%2520We%250Aidentify%2520a%2520critical%2520challenge%2520in%2520existing%2520detection%2520systems%253A%2520their%2520static%250Anature%2520and%2520inability%2520to%2520adapt%2520to%2520evolving%2520attack%2520strategies.%2520To%2520this%2520end%252C%2520our%250Aarchitecture%2520includes%2520multiple%2520RL%2520agents%2520%2528Q-Learning%252C%2520PPO%252C%2520DQN%252C%2520adversarial%250Adefenders%2529%252C%2520each%2520analyzing%2520latent%2520vectors%2520generated%2520by%2520an%2520auto-encoder.%2520When%250Aany%2520agent%2520is%2520uncertain%2520about%2520its%2520decision%252C%2520the%2520system%2520triggers%2520an%2520active%250Alearning%2520loop%2520to%2520simulate%2520expert%2520feedback%252C%2520thus%2520refining%2520decision%2520boundaries.%250AAn%2520ensemble%2520voting%2520mechanism%252C%2520weighted%2520by%2520each%2520agent%2527s%2520performance%252C%2520ensures%250Arobust%2520final%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attackers%20Strike%20Back%3F%20Not%20Anymore%20--%20An%20Ensemble%20of%20RL%20Defenders%0A%20%20Awakens%20for%20APT%20Detection&entry.906535625=Sidahmed%20Benabderrahmane%20and%20Talal%20Rahwan&entry.1292438233=%20%20Advanced%20Persistent%20Threats%20%28APTs%29%20represent%20a%20growing%20menace%20to%20modern%0Adigital%20infrastructure.%20Unlike%20traditional%20cyberattacks%2C%20APTs%20are%20stealthy%2C%0Aadaptive%2C%20and%20long-lasting%2C%20often%20bypassing%20signature-based%20detection%20systems.%0AThis%20paper%20introduces%20a%20novel%20framework%20for%20APT%20detection%20that%20unites%20deep%0Alearning%2C%20reinforcement%20learning%20%28RL%29%2C%20and%20active%20learning%20into%20a%20cohesive%2C%0Aadaptive%20defense%20system.%20Our%20system%20combines%20auto-encoders%20for%20latent%0Abehavioral%20encoding%20with%20a%20multi-agent%20ensemble%20of%20RL-based%20defenders%2C%20each%0Atrained%20to%20distinguish%20between%20benign%20and%20malicious%20process%20behaviors.%20We%0Aidentify%20a%20critical%20challenge%20in%20existing%20detection%20systems%3A%20their%20static%0Anature%20and%20inability%20to%20adapt%20to%20evolving%20attack%20strategies.%20To%20this%20end%2C%20our%0Aarchitecture%20includes%20multiple%20RL%20agents%20%28Q-Learning%2C%20PPO%2C%20DQN%2C%20adversarial%0Adefenders%29%2C%20each%20analyzing%20latent%20vectors%20generated%20by%20an%20auto-encoder.%20When%0Aany%20agent%20is%20uncertain%20about%20its%20decision%2C%20the%20system%20triggers%20an%20active%0Alearning%20loop%20to%20simulate%20expert%20feedback%2C%20thus%20refining%20decision%20boundaries.%0AAn%20ensemble%20voting%20mechanism%2C%20weighted%20by%20each%20agent%27s%20performance%2C%20ensures%0Arobust%20final%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19072v1&entry.124074799=Read"},
{"title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from\n  Pixels", "author": "Long Le and Ryan Lucas and Chen Wang and Chuhao Chen and Dinesh Jayaraman and Eric Eaton and Lingjie Liu", "abstract": "  Inferring the physical properties of 3D scenes from visual information is a\ncritical yet challenging task for creating interactive and realistic virtual\nworlds. While humans intuitively grasp material characteristics such as\nelasticity or stiffness, existing methods often rely on slow, per-scene\noptimization, limiting their generalizability and application. To address this\nproblem, we introduce PIXIE, a novel method that trains a generalizable neural\nnetwork to predict physical properties across multiple scenes from 3D visual\nfeatures purely using supervised losses. Once trained, our feed-forward network\ncan perform fast inference of plausible material fields, which coupled with a\nlearned static scene representation like Gaussian Splatting enables realistic\nphysics simulation under external forces. To facilitate this research, we also\ncollected PIXIEVERSE, one of the largest known datasets of paired 3D assets and\nphysic material annotations. Extensive evaluations demonstrate that PIXIE is\nabout 1.46-4.39x better and orders of magnitude faster than test-time\noptimization methods. By leveraging pretrained visual features like CLIP, our\nmethod can also zero-shot generalize to real-world scenes despite only ever\nbeen trained on synthetic data. https://pixie-3d.github.io/\n", "link": "http://arxiv.org/abs/2508.17437v2", "date": "2025-08-26", "relevancy": 2.3364, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6026}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5735}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixie%3A%20Fast%20and%20Generalizable%20Supervised%20Learning%20of%203D%20Physics%20from%0A%20%20Pixels&body=Title%3A%20Pixie%3A%20Fast%20and%20Generalizable%20Supervised%20Learning%20of%203D%20Physics%20from%0A%20%20Pixels%0AAuthor%3A%20Long%20Le%20and%20Ryan%20Lucas%20and%20Chen%20Wang%20and%20Chuhao%20Chen%20and%20Dinesh%20Jayaraman%20and%20Eric%20Eaton%20and%20Lingjie%20Liu%0AAbstract%3A%20%20%20Inferring%20the%20physical%20properties%20of%203D%20scenes%20from%20visual%20information%20is%20a%0Acritical%20yet%20challenging%20task%20for%20creating%20interactive%20and%20realistic%20virtual%0Aworlds.%20While%20humans%20intuitively%20grasp%20material%20characteristics%20such%20as%0Aelasticity%20or%20stiffness%2C%20existing%20methods%20often%20rely%20on%20slow%2C%20per-scene%0Aoptimization%2C%20limiting%20their%20generalizability%20and%20application.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20PIXIE%2C%20a%20novel%20method%20that%20trains%20a%20generalizable%20neural%0Anetwork%20to%20predict%20physical%20properties%20across%20multiple%20scenes%20from%203D%20visual%0Afeatures%20purely%20using%20supervised%20losses.%20Once%20trained%2C%20our%20feed-forward%20network%0Acan%20perform%20fast%20inference%20of%20plausible%20material%20fields%2C%20which%20coupled%20with%20a%0Alearned%20static%20scene%20representation%20like%20Gaussian%20Splatting%20enables%20realistic%0Aphysics%20simulation%20under%20external%20forces.%20To%20facilitate%20this%20research%2C%20we%20also%0Acollected%20PIXIEVERSE%2C%20one%20of%20the%20largest%20known%20datasets%20of%20paired%203D%20assets%20and%0Aphysic%20material%20annotations.%20Extensive%20evaluations%20demonstrate%20that%20PIXIE%20is%0Aabout%201.46-4.39x%20better%20and%20orders%20of%20magnitude%20faster%20than%20test-time%0Aoptimization%20methods.%20By%20leveraging%20pretrained%20visual%20features%20like%20CLIP%2C%20our%0Amethod%20can%20also%20zero-shot%20generalize%20to%20real-world%20scenes%20despite%20only%20ever%0Abeen%20trained%20on%20synthetic%20data.%20https%3A//pixie-3d.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17437v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixie%253A%2520Fast%2520and%2520Generalizable%2520Supervised%2520Learning%2520of%25203D%2520Physics%2520from%250A%2520%2520Pixels%26entry.906535625%3DLong%2520Le%2520and%2520Ryan%2520Lucas%2520and%2520Chen%2520Wang%2520and%2520Chuhao%2520Chen%2520and%2520Dinesh%2520Jayaraman%2520and%2520Eric%2520Eaton%2520and%2520Lingjie%2520Liu%26entry.1292438233%3D%2520%2520Inferring%2520the%2520physical%2520properties%2520of%25203D%2520scenes%2520from%2520visual%2520information%2520is%2520a%250Acritical%2520yet%2520challenging%2520task%2520for%2520creating%2520interactive%2520and%2520realistic%2520virtual%250Aworlds.%2520While%2520humans%2520intuitively%2520grasp%2520material%2520characteristics%2520such%2520as%250Aelasticity%2520or%2520stiffness%252C%2520existing%2520methods%2520often%2520rely%2520on%2520slow%252C%2520per-scene%250Aoptimization%252C%2520limiting%2520their%2520generalizability%2520and%2520application.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520introduce%2520PIXIE%252C%2520a%2520novel%2520method%2520that%2520trains%2520a%2520generalizable%2520neural%250Anetwork%2520to%2520predict%2520physical%2520properties%2520across%2520multiple%2520scenes%2520from%25203D%2520visual%250Afeatures%2520purely%2520using%2520supervised%2520losses.%2520Once%2520trained%252C%2520our%2520feed-forward%2520network%250Acan%2520perform%2520fast%2520inference%2520of%2520plausible%2520material%2520fields%252C%2520which%2520coupled%2520with%2520a%250Alearned%2520static%2520scene%2520representation%2520like%2520Gaussian%2520Splatting%2520enables%2520realistic%250Aphysics%2520simulation%2520under%2520external%2520forces.%2520To%2520facilitate%2520this%2520research%252C%2520we%2520also%250Acollected%2520PIXIEVERSE%252C%2520one%2520of%2520the%2520largest%2520known%2520datasets%2520of%2520paired%25203D%2520assets%2520and%250Aphysic%2520material%2520annotations.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520PIXIE%2520is%250Aabout%25201.46-4.39x%2520better%2520and%2520orders%2520of%2520magnitude%2520faster%2520than%2520test-time%250Aoptimization%2520methods.%2520By%2520leveraging%2520pretrained%2520visual%2520features%2520like%2520CLIP%252C%2520our%250Amethod%2520can%2520also%2520zero-shot%2520generalize%2520to%2520real-world%2520scenes%2520despite%2520only%2520ever%250Abeen%2520trained%2520on%2520synthetic%2520data.%2520https%253A//pixie-3d.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17437v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixie%3A%20Fast%20and%20Generalizable%20Supervised%20Learning%20of%203D%20Physics%20from%0A%20%20Pixels&entry.906535625=Long%20Le%20and%20Ryan%20Lucas%20and%20Chen%20Wang%20and%20Chuhao%20Chen%20and%20Dinesh%20Jayaraman%20and%20Eric%20Eaton%20and%20Lingjie%20Liu&entry.1292438233=%20%20Inferring%20the%20physical%20properties%20of%203D%20scenes%20from%20visual%20information%20is%20a%0Acritical%20yet%20challenging%20task%20for%20creating%20interactive%20and%20realistic%20virtual%0Aworlds.%20While%20humans%20intuitively%20grasp%20material%20characteristics%20such%20as%0Aelasticity%20or%20stiffness%2C%20existing%20methods%20often%20rely%20on%20slow%2C%20per-scene%0Aoptimization%2C%20limiting%20their%20generalizability%20and%20application.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20PIXIE%2C%20a%20novel%20method%20that%20trains%20a%20generalizable%20neural%0Anetwork%20to%20predict%20physical%20properties%20across%20multiple%20scenes%20from%203D%20visual%0Afeatures%20purely%20using%20supervised%20losses.%20Once%20trained%2C%20our%20feed-forward%20network%0Acan%20perform%20fast%20inference%20of%20plausible%20material%20fields%2C%20which%20coupled%20with%20a%0Alearned%20static%20scene%20representation%20like%20Gaussian%20Splatting%20enables%20realistic%0Aphysics%20simulation%20under%20external%20forces.%20To%20facilitate%20this%20research%2C%20we%20also%0Acollected%20PIXIEVERSE%2C%20one%20of%20the%20largest%20known%20datasets%20of%20paired%203D%20assets%20and%0Aphysic%20material%20annotations.%20Extensive%20evaluations%20demonstrate%20that%20PIXIE%20is%0Aabout%201.46-4.39x%20better%20and%20orders%20of%20magnitude%20faster%20than%20test-time%0Aoptimization%20methods.%20By%20leveraging%20pretrained%20visual%20features%20like%20CLIP%2C%20our%0Amethod%20can%20also%20zero-shot%20generalize%20to%20real-world%20scenes%20despite%20only%20ever%0Abeen%20trained%20on%20synthetic%20data.%20https%3A//pixie-3d.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17437v2&entry.124074799=Read"},
{"title": "MultiRef: Controllable Image Generation with Multiple Visual References", "author": "Ruoxi Chen and Dongping Chen and Siyuan Wu and Sinan Wang and Shiyun Lang and Petr Sushko and Gaoyang Jiang and Yao Wan and Ranjay Krishna", "abstract": "  Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.\n", "link": "http://arxiv.org/abs/2508.06905v3", "date": "2025-08-26", "relevancy": 2.3353, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6101}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5786}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiRef%3A%20Controllable%20Image%20Generation%20with%20Multiple%20Visual%20References&body=Title%3A%20MultiRef%3A%20Controllable%20Image%20Generation%20with%20Multiple%20Visual%20References%0AAuthor%3A%20Ruoxi%20Chen%20and%20Dongping%20Chen%20and%20Siyuan%20Wu%20and%20Sinan%20Wang%20and%20Shiyun%20Lang%20and%20Petr%20Sushko%20and%20Gaoyang%20Jiang%20and%20Yao%20Wan%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20Visual%20designers%20naturally%20draw%20inspiration%20from%20multiple%20visual%20references%2C%0Acombining%20diverse%20elements%20and%20aesthetic%20principles%20to%20create%20artwork.%20However%2C%0Acurrent%20image%20generative%20frameworks%20predominantly%20rely%20on%20single-source%20inputs%0A--%20either%20text%20prompts%20or%20individual%20reference%20images.%20In%20this%20paper%2C%20we%20focus%0Aon%20the%20task%20of%20controllable%20image%20generation%20using%20multiple%20visual%20references.%0AWe%20introduce%20MultiRef-bench%2C%20a%20rigorous%20evaluation%20framework%20comprising%20990%0Asynthetic%20and%201%2C000%20real-world%20samples%20that%20require%20incorporating%20visual%0Acontent%20from%20multiple%20reference%20images.%20The%20synthetic%20samples%20are%20synthetically%0Agenerated%20through%20our%20data%20engine%20RefBlend%2C%20with%2010%20reference%20types%20and%2033%0Areference%20combinations.%20Based%20on%20RefBlend%2C%20we%20further%20construct%20a%20dataset%0AMultiRef%20containing%2038k%20high-quality%20images%20to%20facilitate%20further%20research.%20Our%0Aexperiments%20across%20three%20interleaved%20image-text%20models%20%28i.e.%2C%20OmniGen%2C%20ACE%2C%20and%0AShow-o%29%20and%20six%20agentic%20frameworks%20%28e.g.%2C%20ChatDiT%20and%20LLM%20%2B%20SD%29%20reveal%20that%0Aeven%20state-of-the-art%20systems%20struggle%20with%20multi-reference%20conditioning%2C%20with%0Athe%20best%20model%20OmniGen%20achieving%20only%2066.6%25%20in%20synthetic%20samples%20and%2079.0%25%20in%0Areal-world%20cases%20on%20average%20compared%20to%20the%20golden%20answer.%20These%20findings%0Aprovide%20valuable%20directions%20for%20developing%20more%20flexible%20and%20human-like%0Acreative%20tools%20that%20can%20effectively%20integrate%20multiple%20sources%20of%20visual%0Ainspiration.%20The%20dataset%20is%20publicly%20available%20at%3A%20https%3A//multiref.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06905v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiRef%253A%2520Controllable%2520Image%2520Generation%2520with%2520Multiple%2520Visual%2520References%26entry.906535625%3DRuoxi%2520Chen%2520and%2520Dongping%2520Chen%2520and%2520Siyuan%2520Wu%2520and%2520Sinan%2520Wang%2520and%2520Shiyun%2520Lang%2520and%2520Petr%2520Sushko%2520and%2520Gaoyang%2520Jiang%2520and%2520Yao%2520Wan%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3D%2520%2520Visual%2520designers%2520naturally%2520draw%2520inspiration%2520from%2520multiple%2520visual%2520references%252C%250Acombining%2520diverse%2520elements%2520and%2520aesthetic%2520principles%2520to%2520create%2520artwork.%2520However%252C%250Acurrent%2520image%2520generative%2520frameworks%2520predominantly%2520rely%2520on%2520single-source%2520inputs%250A--%2520either%2520text%2520prompts%2520or%2520individual%2520reference%2520images.%2520In%2520this%2520paper%252C%2520we%2520focus%250Aon%2520the%2520task%2520of%2520controllable%2520image%2520generation%2520using%2520multiple%2520visual%2520references.%250AWe%2520introduce%2520MultiRef-bench%252C%2520a%2520rigorous%2520evaluation%2520framework%2520comprising%2520990%250Asynthetic%2520and%25201%252C000%2520real-world%2520samples%2520that%2520require%2520incorporating%2520visual%250Acontent%2520from%2520multiple%2520reference%2520images.%2520The%2520synthetic%2520samples%2520are%2520synthetically%250Agenerated%2520through%2520our%2520data%2520engine%2520RefBlend%252C%2520with%252010%2520reference%2520types%2520and%252033%250Areference%2520combinations.%2520Based%2520on%2520RefBlend%252C%2520we%2520further%2520construct%2520a%2520dataset%250AMultiRef%2520containing%252038k%2520high-quality%2520images%2520to%2520facilitate%2520further%2520research.%2520Our%250Aexperiments%2520across%2520three%2520interleaved%2520image-text%2520models%2520%2528i.e.%252C%2520OmniGen%252C%2520ACE%252C%2520and%250AShow-o%2529%2520and%2520six%2520agentic%2520frameworks%2520%2528e.g.%252C%2520ChatDiT%2520and%2520LLM%2520%252B%2520SD%2529%2520reveal%2520that%250Aeven%2520state-of-the-art%2520systems%2520struggle%2520with%2520multi-reference%2520conditioning%252C%2520with%250Athe%2520best%2520model%2520OmniGen%2520achieving%2520only%252066.6%2525%2520in%2520synthetic%2520samples%2520and%252079.0%2525%2520in%250Areal-world%2520cases%2520on%2520average%2520compared%2520to%2520the%2520golden%2520answer.%2520These%2520findings%250Aprovide%2520valuable%2520directions%2520for%2520developing%2520more%2520flexible%2520and%2520human-like%250Acreative%2520tools%2520that%2520can%2520effectively%2520integrate%2520multiple%2520sources%2520of%2520visual%250Ainspiration.%2520The%2520dataset%2520is%2520publicly%2520available%2520at%253A%2520https%253A//multiref.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06905v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiRef%3A%20Controllable%20Image%20Generation%20with%20Multiple%20Visual%20References&entry.906535625=Ruoxi%20Chen%20and%20Dongping%20Chen%20and%20Siyuan%20Wu%20and%20Sinan%20Wang%20and%20Shiyun%20Lang%20and%20Petr%20Sushko%20and%20Gaoyang%20Jiang%20and%20Yao%20Wan%20and%20Ranjay%20Krishna&entry.1292438233=%20%20Visual%20designers%20naturally%20draw%20inspiration%20from%20multiple%20visual%20references%2C%0Acombining%20diverse%20elements%20and%20aesthetic%20principles%20to%20create%20artwork.%20However%2C%0Acurrent%20image%20generative%20frameworks%20predominantly%20rely%20on%20single-source%20inputs%0A--%20either%20text%20prompts%20or%20individual%20reference%20images.%20In%20this%20paper%2C%20we%20focus%0Aon%20the%20task%20of%20controllable%20image%20generation%20using%20multiple%20visual%20references.%0AWe%20introduce%20MultiRef-bench%2C%20a%20rigorous%20evaluation%20framework%20comprising%20990%0Asynthetic%20and%201%2C000%20real-world%20samples%20that%20require%20incorporating%20visual%0Acontent%20from%20multiple%20reference%20images.%20The%20synthetic%20samples%20are%20synthetically%0Agenerated%20through%20our%20data%20engine%20RefBlend%2C%20with%2010%20reference%20types%20and%2033%0Areference%20combinations.%20Based%20on%20RefBlend%2C%20we%20further%20construct%20a%20dataset%0AMultiRef%20containing%2038k%20high-quality%20images%20to%20facilitate%20further%20research.%20Our%0Aexperiments%20across%20three%20interleaved%20image-text%20models%20%28i.e.%2C%20OmniGen%2C%20ACE%2C%20and%0AShow-o%29%20and%20six%20agentic%20frameworks%20%28e.g.%2C%20ChatDiT%20and%20LLM%20%2B%20SD%29%20reveal%20that%0Aeven%20state-of-the-art%20systems%20struggle%20with%20multi-reference%20conditioning%2C%20with%0Athe%20best%20model%20OmniGen%20achieving%20only%2066.6%25%20in%20synthetic%20samples%20and%2079.0%25%20in%0Areal-world%20cases%20on%20average%20compared%20to%20the%20golden%20answer.%20These%20findings%0Aprovide%20valuable%20directions%20for%20developing%20more%20flexible%20and%20human-like%0Acreative%20tools%20that%20can%20effectively%20integrate%20multiple%20sources%20of%20visual%0Ainspiration.%20The%20dataset%20is%20publicly%20available%20at%3A%20https%3A//multiref.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06905v3&entry.124074799=Read"},
{"title": "Generalization, Expressivity, and Universality of Graph Neural Networks\n  on Attributed Graphs", "author": "Levi Rauchwerger and Stefanie Jegelka and Ron Levie", "abstract": "  We analyze the universality and generalization of graph neural networks\n(GNNs) on attributed graphs, i.e., with node attributes. To this end, we\npropose pseudometrics over the space of all attributed graphs that describe the\nfine-grained expressivity of GNNs. Namely, GNNs are both Lipschitz continuous\nwith respect to our pseudometrics and can separate attributed graphs that are\ndistant in the metric. Moreover, we prove that the space of all attributed\ngraphs is relatively compact with respect to our metrics. Based on these\nproperties, we prove a universal approximation theorem for GNNs and\ngeneralization bounds for GNNs on any data distribution of attributed graphs.\nThe proposed metrics compute the similarity between the structures of\nattributed graphs via a hierarchical optimal transport between computation\ntrees. Our work extends and unites previous approaches which either derived\ntheory only for graphs with no attributes, derived compact metrics under which\nGNNs are continuous but without separation power, or derived metrics under\nwhich GNNs are continuous and separate points but the space of graphs is not\nrelatively compact, which prevents universal approximation and generalization\nanalysis.\n", "link": "http://arxiv.org/abs/2411.05464v3", "date": "2025-08-26", "relevancy": 2.3199, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4674}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4655}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%2C%20Expressivity%2C%20and%20Universality%20of%20Graph%20Neural%20Networks%0A%20%20on%20Attributed%20Graphs&body=Title%3A%20Generalization%2C%20Expressivity%2C%20and%20Universality%20of%20Graph%20Neural%20Networks%0A%20%20on%20Attributed%20Graphs%0AAuthor%3A%20Levi%20Rauchwerger%20and%20Stefanie%20Jegelka%20and%20Ron%20Levie%0AAbstract%3A%20%20%20We%20analyze%20the%20universality%20and%20generalization%20of%20graph%20neural%20networks%0A%28GNNs%29%20on%20attributed%20graphs%2C%20i.e.%2C%20with%20node%20attributes.%20To%20this%20end%2C%20we%0Apropose%20pseudometrics%20over%20the%20space%20of%20all%20attributed%20graphs%20that%20describe%20the%0Afine-grained%20expressivity%20of%20GNNs.%20Namely%2C%20GNNs%20are%20both%20Lipschitz%20continuous%0Awith%20respect%20to%20our%20pseudometrics%20and%20can%20separate%20attributed%20graphs%20that%20are%0Adistant%20in%20the%20metric.%20Moreover%2C%20we%20prove%20that%20the%20space%20of%20all%20attributed%0Agraphs%20is%20relatively%20compact%20with%20respect%20to%20our%20metrics.%20Based%20on%20these%0Aproperties%2C%20we%20prove%20a%20universal%20approximation%20theorem%20for%20GNNs%20and%0Ageneralization%20bounds%20for%20GNNs%20on%20any%20data%20distribution%20of%20attributed%20graphs.%0AThe%20proposed%20metrics%20compute%20the%20similarity%20between%20the%20structures%20of%0Aattributed%20graphs%20via%20a%20hierarchical%20optimal%20transport%20between%20computation%0Atrees.%20Our%20work%20extends%20and%20unites%20previous%20approaches%20which%20either%20derived%0Atheory%20only%20for%20graphs%20with%20no%20attributes%2C%20derived%20compact%20metrics%20under%20which%0AGNNs%20are%20continuous%20but%20without%20separation%20power%2C%20or%20derived%20metrics%20under%0Awhich%20GNNs%20are%20continuous%20and%20separate%20points%20but%20the%20space%20of%20graphs%20is%20not%0Arelatively%20compact%2C%20which%20prevents%20universal%20approximation%20and%20generalization%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05464v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%252C%2520Expressivity%252C%2520and%2520Universality%2520of%2520Graph%2520Neural%2520Networks%250A%2520%2520on%2520Attributed%2520Graphs%26entry.906535625%3DLevi%2520Rauchwerger%2520and%2520Stefanie%2520Jegelka%2520and%2520Ron%2520Levie%26entry.1292438233%3D%2520%2520We%2520analyze%2520the%2520universality%2520and%2520generalization%2520of%2520graph%2520neural%2520networks%250A%2528GNNs%2529%2520on%2520attributed%2520graphs%252C%2520i.e.%252C%2520with%2520node%2520attributes.%2520To%2520this%2520end%252C%2520we%250Apropose%2520pseudometrics%2520over%2520the%2520space%2520of%2520all%2520attributed%2520graphs%2520that%2520describe%2520the%250Afine-grained%2520expressivity%2520of%2520GNNs.%2520Namely%252C%2520GNNs%2520are%2520both%2520Lipschitz%2520continuous%250Awith%2520respect%2520to%2520our%2520pseudometrics%2520and%2520can%2520separate%2520attributed%2520graphs%2520that%2520are%250Adistant%2520in%2520the%2520metric.%2520Moreover%252C%2520we%2520prove%2520that%2520the%2520space%2520of%2520all%2520attributed%250Agraphs%2520is%2520relatively%2520compact%2520with%2520respect%2520to%2520our%2520metrics.%2520Based%2520on%2520these%250Aproperties%252C%2520we%2520prove%2520a%2520universal%2520approximation%2520theorem%2520for%2520GNNs%2520and%250Ageneralization%2520bounds%2520for%2520GNNs%2520on%2520any%2520data%2520distribution%2520of%2520attributed%2520graphs.%250AThe%2520proposed%2520metrics%2520compute%2520the%2520similarity%2520between%2520the%2520structures%2520of%250Aattributed%2520graphs%2520via%2520a%2520hierarchical%2520optimal%2520transport%2520between%2520computation%250Atrees.%2520Our%2520work%2520extends%2520and%2520unites%2520previous%2520approaches%2520which%2520either%2520derived%250Atheory%2520only%2520for%2520graphs%2520with%2520no%2520attributes%252C%2520derived%2520compact%2520metrics%2520under%2520which%250AGNNs%2520are%2520continuous%2520but%2520without%2520separation%2520power%252C%2520or%2520derived%2520metrics%2520under%250Awhich%2520GNNs%2520are%2520continuous%2520and%2520separate%2520points%2520but%2520the%2520space%2520of%2520graphs%2520is%2520not%250Arelatively%2520compact%252C%2520which%2520prevents%2520universal%2520approximation%2520and%2520generalization%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05464v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%2C%20Expressivity%2C%20and%20Universality%20of%20Graph%20Neural%20Networks%0A%20%20on%20Attributed%20Graphs&entry.906535625=Levi%20Rauchwerger%20and%20Stefanie%20Jegelka%20and%20Ron%20Levie&entry.1292438233=%20%20We%20analyze%20the%20universality%20and%20generalization%20of%20graph%20neural%20networks%0A%28GNNs%29%20on%20attributed%20graphs%2C%20i.e.%2C%20with%20node%20attributes.%20To%20this%20end%2C%20we%0Apropose%20pseudometrics%20over%20the%20space%20of%20all%20attributed%20graphs%20that%20describe%20the%0Afine-grained%20expressivity%20of%20GNNs.%20Namely%2C%20GNNs%20are%20both%20Lipschitz%20continuous%0Awith%20respect%20to%20our%20pseudometrics%20and%20can%20separate%20attributed%20graphs%20that%20are%0Adistant%20in%20the%20metric.%20Moreover%2C%20we%20prove%20that%20the%20space%20of%20all%20attributed%0Agraphs%20is%20relatively%20compact%20with%20respect%20to%20our%20metrics.%20Based%20on%20these%0Aproperties%2C%20we%20prove%20a%20universal%20approximation%20theorem%20for%20GNNs%20and%0Ageneralization%20bounds%20for%20GNNs%20on%20any%20data%20distribution%20of%20attributed%20graphs.%0AThe%20proposed%20metrics%20compute%20the%20similarity%20between%20the%20structures%20of%0Aattributed%20graphs%20via%20a%20hierarchical%20optimal%20transport%20between%20computation%0Atrees.%20Our%20work%20extends%20and%20unites%20previous%20approaches%20which%20either%20derived%0Atheory%20only%20for%20graphs%20with%20no%20attributes%2C%20derived%20compact%20metrics%20under%20which%0AGNNs%20are%20continuous%20but%20without%20separation%20power%2C%20or%20derived%20metrics%20under%0Awhich%20GNNs%20are%20continuous%20and%20separate%20points%20but%20the%20space%20of%20graphs%20is%20not%0Arelatively%20compact%2C%20which%20prevents%20universal%20approximation%20and%20generalization%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05464v3&entry.124074799=Read"},
{"title": "Is attention truly all we need? An empirical study of asset pricing in\n  pretrained RNN sparse and global attention models", "author": "Shanyan Lai", "abstract": "  This study investigates the pretrained RNN attention models with the\nmainstream attention mechanisms such as additive attention, Luong's three\nattentions, global self-attention (Self-att) and sliding window sparse\nattention (Sparse-att) for the empirical asset pricing research on top 420\nlarge-cap US stocks. This is the first paper on the large-scale\nstate-of-the-art (SOTA) attention mechanisms applied in the asset pricing\ncontext. They overcome the limitations of the traditional machine learning (ML)\nbased asset pricing, such as mis-capturing the temporal dependency and short\nmemory. Moreover, the enforced causal masks in the attention mechanisms address\nthe future data leaking issue ignored by the more advanced attention-based\nmodels, such as the classic Transformer. The proposed attention models also\nconsider the temporal sparsity characteristic of asset pricing data and\nmitigate potential overfitting issues by deploying the simplified model\nstructures. This provides some insights for future empirical economic research.\nAll models are examined in three periods, which cover pre-COVID-19 (mild\nuptrend), COVID-19 (steep uptrend with a large drawdown) and one year\npost-COVID-19 (sideways movement with high fluctuations), for testing the\nstability of these models under extreme market conditions. The study finds that\nin value-weighted portfolio back testing, Model Self-att and Model Sparse-att\nexhibit great capabilities in deriving the absolute returns and hedging\ndownside risks, while they achieve an annualized Sortino ratio of 2.0 and 1.80\nrespectively in the period with COVID-19. And Model Sparse-att performs more\nstably than Model Self-att from the perspective of absolute portfolio returns\nwith respect to the size of stocks' market capitalization.\n", "link": "http://arxiv.org/abs/2508.19006v1", "date": "2025-08-26", "relevancy": 2.3075, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5119}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4385}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20attention%20truly%20all%20we%20need%3F%20An%20empirical%20study%20of%20asset%20pricing%20in%0A%20%20pretrained%20RNN%20sparse%20and%20global%20attention%20models&body=Title%3A%20Is%20attention%20truly%20all%20we%20need%3F%20An%20empirical%20study%20of%20asset%20pricing%20in%0A%20%20pretrained%20RNN%20sparse%20and%20global%20attention%20models%0AAuthor%3A%20Shanyan%20Lai%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20pretrained%20RNN%20attention%20models%20with%20the%0Amainstream%20attention%20mechanisms%20such%20as%20additive%20attention%2C%20Luong%27s%20three%0Aattentions%2C%20global%20self-attention%20%28Self-att%29%20and%20sliding%20window%20sparse%0Aattention%20%28Sparse-att%29%20for%20the%20empirical%20asset%20pricing%20research%20on%20top%20420%0Alarge-cap%20US%20stocks.%20This%20is%20the%20first%20paper%20on%20the%20large-scale%0Astate-of-the-art%20%28SOTA%29%20attention%20mechanisms%20applied%20in%20the%20asset%20pricing%0Acontext.%20They%20overcome%20the%20limitations%20of%20the%20traditional%20machine%20learning%20%28ML%29%0Abased%20asset%20pricing%2C%20such%20as%20mis-capturing%20the%20temporal%20dependency%20and%20short%0Amemory.%20Moreover%2C%20the%20enforced%20causal%20masks%20in%20the%20attention%20mechanisms%20address%0Athe%20future%20data%20leaking%20issue%20ignored%20by%20the%20more%20advanced%20attention-based%0Amodels%2C%20such%20as%20the%20classic%20Transformer.%20The%20proposed%20attention%20models%20also%0Aconsider%20the%20temporal%20sparsity%20characteristic%20of%20asset%20pricing%20data%20and%0Amitigate%20potential%20overfitting%20issues%20by%20deploying%20the%20simplified%20model%0Astructures.%20This%20provides%20some%20insights%20for%20future%20empirical%20economic%20research.%0AAll%20models%20are%20examined%20in%20three%20periods%2C%20which%20cover%20pre-COVID-19%20%28mild%0Auptrend%29%2C%20COVID-19%20%28steep%20uptrend%20with%20a%20large%20drawdown%29%20and%20one%20year%0Apost-COVID-19%20%28sideways%20movement%20with%20high%20fluctuations%29%2C%20for%20testing%20the%0Astability%20of%20these%20models%20under%20extreme%20market%20conditions.%20The%20study%20finds%20that%0Ain%20value-weighted%20portfolio%20back%20testing%2C%20Model%20Self-att%20and%20Model%20Sparse-att%0Aexhibit%20great%20capabilities%20in%20deriving%20the%20absolute%20returns%20and%20hedging%0Adownside%20risks%2C%20while%20they%20achieve%20an%20annualized%20Sortino%20ratio%20of%202.0%20and%201.80%0Arespectively%20in%20the%20period%20with%20COVID-19.%20And%20Model%20Sparse-att%20performs%20more%0Astably%20than%20Model%20Self-att%20from%20the%20perspective%20of%20absolute%20portfolio%20returns%0Awith%20respect%20to%20the%20size%20of%20stocks%27%20market%20capitalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520attention%2520truly%2520all%2520we%2520need%253F%2520An%2520empirical%2520study%2520of%2520asset%2520pricing%2520in%250A%2520%2520pretrained%2520RNN%2520sparse%2520and%2520global%2520attention%2520models%26entry.906535625%3DShanyan%2520Lai%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520pretrained%2520RNN%2520attention%2520models%2520with%2520the%250Amainstream%2520attention%2520mechanisms%2520such%2520as%2520additive%2520attention%252C%2520Luong%2527s%2520three%250Aattentions%252C%2520global%2520self-attention%2520%2528Self-att%2529%2520and%2520sliding%2520window%2520sparse%250Aattention%2520%2528Sparse-att%2529%2520for%2520the%2520empirical%2520asset%2520pricing%2520research%2520on%2520top%2520420%250Alarge-cap%2520US%2520stocks.%2520This%2520is%2520the%2520first%2520paper%2520on%2520the%2520large-scale%250Astate-of-the-art%2520%2528SOTA%2529%2520attention%2520mechanisms%2520applied%2520in%2520the%2520asset%2520pricing%250Acontext.%2520They%2520overcome%2520the%2520limitations%2520of%2520the%2520traditional%2520machine%2520learning%2520%2528ML%2529%250Abased%2520asset%2520pricing%252C%2520such%2520as%2520mis-capturing%2520the%2520temporal%2520dependency%2520and%2520short%250Amemory.%2520Moreover%252C%2520the%2520enforced%2520causal%2520masks%2520in%2520the%2520attention%2520mechanisms%2520address%250Athe%2520future%2520data%2520leaking%2520issue%2520ignored%2520by%2520the%2520more%2520advanced%2520attention-based%250Amodels%252C%2520such%2520as%2520the%2520classic%2520Transformer.%2520The%2520proposed%2520attention%2520models%2520also%250Aconsider%2520the%2520temporal%2520sparsity%2520characteristic%2520of%2520asset%2520pricing%2520data%2520and%250Amitigate%2520potential%2520overfitting%2520issues%2520by%2520deploying%2520the%2520simplified%2520model%250Astructures.%2520This%2520provides%2520some%2520insights%2520for%2520future%2520empirical%2520economic%2520research.%250AAll%2520models%2520are%2520examined%2520in%2520three%2520periods%252C%2520which%2520cover%2520pre-COVID-19%2520%2528mild%250Auptrend%2529%252C%2520COVID-19%2520%2528steep%2520uptrend%2520with%2520a%2520large%2520drawdown%2529%2520and%2520one%2520year%250Apost-COVID-19%2520%2528sideways%2520movement%2520with%2520high%2520fluctuations%2529%252C%2520for%2520testing%2520the%250Astability%2520of%2520these%2520models%2520under%2520extreme%2520market%2520conditions.%2520The%2520study%2520finds%2520that%250Ain%2520value-weighted%2520portfolio%2520back%2520testing%252C%2520Model%2520Self-att%2520and%2520Model%2520Sparse-att%250Aexhibit%2520great%2520capabilities%2520in%2520deriving%2520the%2520absolute%2520returns%2520and%2520hedging%250Adownside%2520risks%252C%2520while%2520they%2520achieve%2520an%2520annualized%2520Sortino%2520ratio%2520of%25202.0%2520and%25201.80%250Arespectively%2520in%2520the%2520period%2520with%2520COVID-19.%2520And%2520Model%2520Sparse-att%2520performs%2520more%250Astably%2520than%2520Model%2520Self-att%2520from%2520the%2520perspective%2520of%2520absolute%2520portfolio%2520returns%250Awith%2520respect%2520to%2520the%2520size%2520of%2520stocks%2527%2520market%2520capitalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20attention%20truly%20all%20we%20need%3F%20An%20empirical%20study%20of%20asset%20pricing%20in%0A%20%20pretrained%20RNN%20sparse%20and%20global%20attention%20models&entry.906535625=Shanyan%20Lai&entry.1292438233=%20%20This%20study%20investigates%20the%20pretrained%20RNN%20attention%20models%20with%20the%0Amainstream%20attention%20mechanisms%20such%20as%20additive%20attention%2C%20Luong%27s%20three%0Aattentions%2C%20global%20self-attention%20%28Self-att%29%20and%20sliding%20window%20sparse%0Aattention%20%28Sparse-att%29%20for%20the%20empirical%20asset%20pricing%20research%20on%20top%20420%0Alarge-cap%20US%20stocks.%20This%20is%20the%20first%20paper%20on%20the%20large-scale%0Astate-of-the-art%20%28SOTA%29%20attention%20mechanisms%20applied%20in%20the%20asset%20pricing%0Acontext.%20They%20overcome%20the%20limitations%20of%20the%20traditional%20machine%20learning%20%28ML%29%0Abased%20asset%20pricing%2C%20such%20as%20mis-capturing%20the%20temporal%20dependency%20and%20short%0Amemory.%20Moreover%2C%20the%20enforced%20causal%20masks%20in%20the%20attention%20mechanisms%20address%0Athe%20future%20data%20leaking%20issue%20ignored%20by%20the%20more%20advanced%20attention-based%0Amodels%2C%20such%20as%20the%20classic%20Transformer.%20The%20proposed%20attention%20models%20also%0Aconsider%20the%20temporal%20sparsity%20characteristic%20of%20asset%20pricing%20data%20and%0Amitigate%20potential%20overfitting%20issues%20by%20deploying%20the%20simplified%20model%0Astructures.%20This%20provides%20some%20insights%20for%20future%20empirical%20economic%20research.%0AAll%20models%20are%20examined%20in%20three%20periods%2C%20which%20cover%20pre-COVID-19%20%28mild%0Auptrend%29%2C%20COVID-19%20%28steep%20uptrend%20with%20a%20large%20drawdown%29%20and%20one%20year%0Apost-COVID-19%20%28sideways%20movement%20with%20high%20fluctuations%29%2C%20for%20testing%20the%0Astability%20of%20these%20models%20under%20extreme%20market%20conditions.%20The%20study%20finds%20that%0Ain%20value-weighted%20portfolio%20back%20testing%2C%20Model%20Self-att%20and%20Model%20Sparse-att%0Aexhibit%20great%20capabilities%20in%20deriving%20the%20absolute%20returns%20and%20hedging%0Adownside%20risks%2C%20while%20they%20achieve%20an%20annualized%20Sortino%20ratio%20of%202.0%20and%201.80%0Arespectively%20in%20the%20period%20with%20COVID-19.%20And%20Model%20Sparse-att%20performs%20more%0Astably%20than%20Model%20Self-att%20from%20the%20perspective%20of%20absolute%20portfolio%20returns%0Awith%20respect%20to%20the%20size%20of%20stocks%27%20market%20capitalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19006v1&entry.124074799=Read"},
{"title": "Seal Your Backdoor with Variational Defense", "author": "Ivan Saboli\u0107 and Matej Grci\u0107 and Sini\u0161a \u0160egvi\u0107", "abstract": "  We propose VIBE, a model-agnostic framework that trains classifiers resilient\nto backdoor attacks. The key concept behind our approach is to treat malicious\ninputs and corrupted labels from the training dataset as observed random\nvariables, while the actual clean labels are latent. VIBE then recovers the\ncorresponding latent clean label posterior through variational inference. The\nresulting training procedure follows the expectation-maximization (EM)\nalgorithm. The E-step infers the clean pseudolabels by solving an\nentropy-regularized optimal transport problem, while the M-step updates the\nclassifier parameters via gradient descent. Being modular, VIBE can seamlessly\nintegrate with recent advancements in self-supervised representation learning,\nwhich enhance its ability to resist backdoor attacks. We experimentally\nvalidate the method effectiveness against contemporary backdoor attacks on\nstandard datasets, a large-scale setup with 1$k$ classes, and a dataset\npoisoned with multiple attacks. VIBE consistently outperforms previous defenses\nacross all tested scenarios.\n", "link": "http://arxiv.org/abs/2503.08829v3", "date": "2025-08-26", "relevancy": 2.3048, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4714}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.457}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seal%20Your%20Backdoor%20with%20Variational%20Defense&body=Title%3A%20Seal%20Your%20Backdoor%20with%20Variational%20Defense%0AAuthor%3A%20Ivan%20Saboli%C4%87%20and%20Matej%20Grci%C4%87%20and%20Sini%C5%A1a%20%C5%A0egvi%C4%87%0AAbstract%3A%20%20%20We%20propose%20VIBE%2C%20a%20model-agnostic%20framework%20that%20trains%20classifiers%20resilient%0Ato%20backdoor%20attacks.%20The%20key%20concept%20behind%20our%20approach%20is%20to%20treat%20malicious%0Ainputs%20and%20corrupted%20labels%20from%20the%20training%20dataset%20as%20observed%20random%0Avariables%2C%20while%20the%20actual%20clean%20labels%20are%20latent.%20VIBE%20then%20recovers%20the%0Acorresponding%20latent%20clean%20label%20posterior%20through%20variational%20inference.%20The%0Aresulting%20training%20procedure%20follows%20the%20expectation-maximization%20%28EM%29%0Aalgorithm.%20The%20E-step%20infers%20the%20clean%20pseudolabels%20by%20solving%20an%0Aentropy-regularized%20optimal%20transport%20problem%2C%20while%20the%20M-step%20updates%20the%0Aclassifier%20parameters%20via%20gradient%20descent.%20Being%20modular%2C%20VIBE%20can%20seamlessly%0Aintegrate%20with%20recent%20advancements%20in%20self-supervised%20representation%20learning%2C%0Awhich%20enhance%20its%20ability%20to%20resist%20backdoor%20attacks.%20We%20experimentally%0Avalidate%20the%20method%20effectiveness%20against%20contemporary%20backdoor%20attacks%20on%0Astandard%20datasets%2C%20a%20large-scale%20setup%20with%201%24k%24%20classes%2C%20and%20a%20dataset%0Apoisoned%20with%20multiple%20attacks.%20VIBE%20consistently%20outperforms%20previous%20defenses%0Aacross%20all%20tested%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08829v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeal%2520Your%2520Backdoor%2520with%2520Variational%2520Defense%26entry.906535625%3DIvan%2520Saboli%25C4%2587%2520and%2520Matej%2520Grci%25C4%2587%2520and%2520Sini%25C5%25A1a%2520%25C5%25A0egvi%25C4%2587%26entry.1292438233%3D%2520%2520We%2520propose%2520VIBE%252C%2520a%2520model-agnostic%2520framework%2520that%2520trains%2520classifiers%2520resilient%250Ato%2520backdoor%2520attacks.%2520The%2520key%2520concept%2520behind%2520our%2520approach%2520is%2520to%2520treat%2520malicious%250Ainputs%2520and%2520corrupted%2520labels%2520from%2520the%2520training%2520dataset%2520as%2520observed%2520random%250Avariables%252C%2520while%2520the%2520actual%2520clean%2520labels%2520are%2520latent.%2520VIBE%2520then%2520recovers%2520the%250Acorresponding%2520latent%2520clean%2520label%2520posterior%2520through%2520variational%2520inference.%2520The%250Aresulting%2520training%2520procedure%2520follows%2520the%2520expectation-maximization%2520%2528EM%2529%250Aalgorithm.%2520The%2520E-step%2520infers%2520the%2520clean%2520pseudolabels%2520by%2520solving%2520an%250Aentropy-regularized%2520optimal%2520transport%2520problem%252C%2520while%2520the%2520M-step%2520updates%2520the%250Aclassifier%2520parameters%2520via%2520gradient%2520descent.%2520Being%2520modular%252C%2520VIBE%2520can%2520seamlessly%250Aintegrate%2520with%2520recent%2520advancements%2520in%2520self-supervised%2520representation%2520learning%252C%250Awhich%2520enhance%2520its%2520ability%2520to%2520resist%2520backdoor%2520attacks.%2520We%2520experimentally%250Avalidate%2520the%2520method%2520effectiveness%2520against%2520contemporary%2520backdoor%2520attacks%2520on%250Astandard%2520datasets%252C%2520a%2520large-scale%2520setup%2520with%25201%2524k%2524%2520classes%252C%2520and%2520a%2520dataset%250Apoisoned%2520with%2520multiple%2520attacks.%2520VIBE%2520consistently%2520outperforms%2520previous%2520defenses%250Aacross%2520all%2520tested%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08829v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seal%20Your%20Backdoor%20with%20Variational%20Defense&entry.906535625=Ivan%20Saboli%C4%87%20and%20Matej%20Grci%C4%87%20and%20Sini%C5%A1a%20%C5%A0egvi%C4%87&entry.1292438233=%20%20We%20propose%20VIBE%2C%20a%20model-agnostic%20framework%20that%20trains%20classifiers%20resilient%0Ato%20backdoor%20attacks.%20The%20key%20concept%20behind%20our%20approach%20is%20to%20treat%20malicious%0Ainputs%20and%20corrupted%20labels%20from%20the%20training%20dataset%20as%20observed%20random%0Avariables%2C%20while%20the%20actual%20clean%20labels%20are%20latent.%20VIBE%20then%20recovers%20the%0Acorresponding%20latent%20clean%20label%20posterior%20through%20variational%20inference.%20The%0Aresulting%20training%20procedure%20follows%20the%20expectation-maximization%20%28EM%29%0Aalgorithm.%20The%20E-step%20infers%20the%20clean%20pseudolabels%20by%20solving%20an%0Aentropy-regularized%20optimal%20transport%20problem%2C%20while%20the%20M-step%20updates%20the%0Aclassifier%20parameters%20via%20gradient%20descent.%20Being%20modular%2C%20VIBE%20can%20seamlessly%0Aintegrate%20with%20recent%20advancements%20in%20self-supervised%20representation%20learning%2C%0Awhich%20enhance%20its%20ability%20to%20resist%20backdoor%20attacks.%20We%20experimentally%0Avalidate%20the%20method%20effectiveness%20against%20contemporary%20backdoor%20attacks%20on%0Astandard%20datasets%2C%20a%20large-scale%20setup%20with%201%24k%24%20classes%2C%20and%20a%20dataset%0Apoisoned%20with%20multiple%20attacks.%20VIBE%20consistently%20outperforms%20previous%20defenses%0Aacross%20all%20tested%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08829v3&entry.124074799=Read"},
{"title": "A Bag of Tricks for Efficient Implicit Neural Point Clouds", "author": "Florian Hahlbohm and Linus Franke and Leon Overk\u00e4mping and Paula Wespe and Susana Castillo and Martin Eisemann and Marcus Magnor", "abstract": "  Implicit Neural Point Cloud (INPC) is a recent hybrid representation that\ncombines the expressiveness of neural fields with the efficiency of point-based\nrendering, achieving state-of-the-art image quality in novel view synthesis.\nHowever, as with other high-quality approaches that query neural networks\nduring rendering, the practical usability of INPC is limited by comparatively\nslow rendering. In this work, we present a collection of optimizations that\nsignificantly improve both the training and inference performance of INPC\nwithout sacrificing visual fidelity. The most significant modifications are an\nimproved rasterizer implementation, more effective sampling techniques, and the\nincorporation of pre-training for the convolutional neural network used for\nhole-filling. Furthermore, we demonstrate that points can be modeled as small\nGaussians during inference to further improve quality in extrapolated, e.g.,\nclose-up views of the scene. We design our implementations to be broadly\napplicable beyond INPC and systematically evaluate each modification in a\nseries of experiments. Our optimized INPC pipeline achieves up to 25% faster\ntraining, 2x faster rendering, and 20% reduced VRAM usage paired with slight\nimage quality improvements.\n", "link": "http://arxiv.org/abs/2508.19140v1", "date": "2025-08-26", "relevancy": 2.2863, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6096}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5479}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bag%20of%20Tricks%20for%20Efficient%20Implicit%20Neural%20Point%20Clouds&body=Title%3A%20A%20Bag%20of%20Tricks%20for%20Efficient%20Implicit%20Neural%20Point%20Clouds%0AAuthor%3A%20Florian%20Hahlbohm%20and%20Linus%20Franke%20and%20Leon%20Overk%C3%A4mping%20and%20Paula%20Wespe%20and%20Susana%20Castillo%20and%20Martin%20Eisemann%20and%20Marcus%20Magnor%0AAbstract%3A%20%20%20Implicit%20Neural%20Point%20Cloud%20%28INPC%29%20is%20a%20recent%20hybrid%20representation%20that%0Acombines%20the%20expressiveness%20of%20neural%20fields%20with%20the%20efficiency%20of%20point-based%0Arendering%2C%20achieving%20state-of-the-art%20image%20quality%20in%20novel%20view%20synthesis.%0AHowever%2C%20as%20with%20other%20high-quality%20approaches%20that%20query%20neural%20networks%0Aduring%20rendering%2C%20the%20practical%20usability%20of%20INPC%20is%20limited%20by%20comparatively%0Aslow%20rendering.%20In%20this%20work%2C%20we%20present%20a%20collection%20of%20optimizations%20that%0Asignificantly%20improve%20both%20the%20training%20and%20inference%20performance%20of%20INPC%0Awithout%20sacrificing%20visual%20fidelity.%20The%20most%20significant%20modifications%20are%20an%0Aimproved%20rasterizer%20implementation%2C%20more%20effective%20sampling%20techniques%2C%20and%20the%0Aincorporation%20of%20pre-training%20for%20the%20convolutional%20neural%20network%20used%20for%0Ahole-filling.%20Furthermore%2C%20we%20demonstrate%20that%20points%20can%20be%20modeled%20as%20small%0AGaussians%20during%20inference%20to%20further%20improve%20quality%20in%20extrapolated%2C%20e.g.%2C%0Aclose-up%20views%20of%20the%20scene.%20We%20design%20our%20implementations%20to%20be%20broadly%0Aapplicable%20beyond%20INPC%20and%20systematically%20evaluate%20each%20modification%20in%20a%0Aseries%20of%20experiments.%20Our%20optimized%20INPC%20pipeline%20achieves%20up%20to%2025%25%20faster%0Atraining%2C%202x%20faster%20rendering%2C%20and%2020%25%20reduced%20VRAM%20usage%20paired%20with%20slight%0Aimage%20quality%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bag%2520of%2520Tricks%2520for%2520Efficient%2520Implicit%2520Neural%2520Point%2520Clouds%26entry.906535625%3DFlorian%2520Hahlbohm%2520and%2520Linus%2520Franke%2520and%2520Leon%2520Overk%25C3%25A4mping%2520and%2520Paula%2520Wespe%2520and%2520Susana%2520Castillo%2520and%2520Martin%2520Eisemann%2520and%2520Marcus%2520Magnor%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Point%2520Cloud%2520%2528INPC%2529%2520is%2520a%2520recent%2520hybrid%2520representation%2520that%250Acombines%2520the%2520expressiveness%2520of%2520neural%2520fields%2520with%2520the%2520efficiency%2520of%2520point-based%250Arendering%252C%2520achieving%2520state-of-the-art%2520image%2520quality%2520in%2520novel%2520view%2520synthesis.%250AHowever%252C%2520as%2520with%2520other%2520high-quality%2520approaches%2520that%2520query%2520neural%2520networks%250Aduring%2520rendering%252C%2520the%2520practical%2520usability%2520of%2520INPC%2520is%2520limited%2520by%2520comparatively%250Aslow%2520rendering.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520collection%2520of%2520optimizations%2520that%250Asignificantly%2520improve%2520both%2520the%2520training%2520and%2520inference%2520performance%2520of%2520INPC%250Awithout%2520sacrificing%2520visual%2520fidelity.%2520The%2520most%2520significant%2520modifications%2520are%2520an%250Aimproved%2520rasterizer%2520implementation%252C%2520more%2520effective%2520sampling%2520techniques%252C%2520and%2520the%250Aincorporation%2520of%2520pre-training%2520for%2520the%2520convolutional%2520neural%2520network%2520used%2520for%250Ahole-filling.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520points%2520can%2520be%2520modeled%2520as%2520small%250AGaussians%2520during%2520inference%2520to%2520further%2520improve%2520quality%2520in%2520extrapolated%252C%2520e.g.%252C%250Aclose-up%2520views%2520of%2520the%2520scene.%2520We%2520design%2520our%2520implementations%2520to%2520be%2520broadly%250Aapplicable%2520beyond%2520INPC%2520and%2520systematically%2520evaluate%2520each%2520modification%2520in%2520a%250Aseries%2520of%2520experiments.%2520Our%2520optimized%2520INPC%2520pipeline%2520achieves%2520up%2520to%252025%2525%2520faster%250Atraining%252C%25202x%2520faster%2520rendering%252C%2520and%252020%2525%2520reduced%2520VRAM%2520usage%2520paired%2520with%2520slight%250Aimage%2520quality%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bag%20of%20Tricks%20for%20Efficient%20Implicit%20Neural%20Point%20Clouds&entry.906535625=Florian%20Hahlbohm%20and%20Linus%20Franke%20and%20Leon%20Overk%C3%A4mping%20and%20Paula%20Wespe%20and%20Susana%20Castillo%20and%20Martin%20Eisemann%20and%20Marcus%20Magnor&entry.1292438233=%20%20Implicit%20Neural%20Point%20Cloud%20%28INPC%29%20is%20a%20recent%20hybrid%20representation%20that%0Acombines%20the%20expressiveness%20of%20neural%20fields%20with%20the%20efficiency%20of%20point-based%0Arendering%2C%20achieving%20state-of-the-art%20image%20quality%20in%20novel%20view%20synthesis.%0AHowever%2C%20as%20with%20other%20high-quality%20approaches%20that%20query%20neural%20networks%0Aduring%20rendering%2C%20the%20practical%20usability%20of%20INPC%20is%20limited%20by%20comparatively%0Aslow%20rendering.%20In%20this%20work%2C%20we%20present%20a%20collection%20of%20optimizations%20that%0Asignificantly%20improve%20both%20the%20training%20and%20inference%20performance%20of%20INPC%0Awithout%20sacrificing%20visual%20fidelity.%20The%20most%20significant%20modifications%20are%20an%0Aimproved%20rasterizer%20implementation%2C%20more%20effective%20sampling%20techniques%2C%20and%20the%0Aincorporation%20of%20pre-training%20for%20the%20convolutional%20neural%20network%20used%20for%0Ahole-filling.%20Furthermore%2C%20we%20demonstrate%20that%20points%20can%20be%20modeled%20as%20small%0AGaussians%20during%20inference%20to%20further%20improve%20quality%20in%20extrapolated%2C%20e.g.%2C%0Aclose-up%20views%20of%20the%20scene.%20We%20design%20our%20implementations%20to%20be%20broadly%0Aapplicable%20beyond%20INPC%20and%20systematically%20evaluate%20each%20modification%20in%20a%0Aseries%20of%20experiments.%20Our%20optimized%20INPC%20pipeline%20achieves%20up%20to%2025%25%20faster%0Atraining%2C%202x%20faster%20rendering%2C%20and%2020%25%20reduced%20VRAM%20usage%20paired%20with%20slight%0Aimage%20quality%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19140v1&entry.124074799=Read"},
{"title": "HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots", "author": "Shipeng Lyu and Fangyuan Wang and Weiwei Lin and Luhao Zhu and David Navarro-Alarcon and Guodong Guo", "abstract": "  Achieving both behavioral similarity and appropriateness in human-like motion\ngeneration for humanoid robot remains an open challenge, further compounded by\nthe lack of cross-embodiment adaptability. To address this problem, we propose\nHuBE, a bi-level closed-loop framework that integrates robot state, goal poses,\nand contextual situations to generate human-like behaviors, ensuring both\nbehavioral similarity and appropriateness, and eliminating structural\nmismatches between motion generation and execution. To support this framework,\nwe construct HPose, a context-enriched dataset featuring fine-grained\nsituational annotations. Furthermore, we introduce a bone scaling-based data\naugmentation strategy that ensures millimeter-level compatibility across\nheterogeneous humanoid robots. Comprehensive evaluations on multiple commercial\nplatforms demonstrate that HuBE significantly improves motion similarity,\nbehavioral appropriateness, and computational efficiency over state-of-the-art\nbaselines, establishing a solid foundation for transferable and human-like\nbehavior execution across diverse humanoid robots.\n", "link": "http://arxiv.org/abs/2508.19002v1", "date": "2025-08-26", "relevancy": 2.2779, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5862}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5621}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HuBE%3A%20Cross-Embodiment%20Human-like%20Behavior%20Execution%20for%20Humanoid%20Robots&body=Title%3A%20HuBE%3A%20Cross-Embodiment%20Human-like%20Behavior%20Execution%20for%20Humanoid%20Robots%0AAuthor%3A%20Shipeng%20Lyu%20and%20Fangyuan%20Wang%20and%20Weiwei%20Lin%20and%20Luhao%20Zhu%20and%20David%20Navarro-Alarcon%20and%20Guodong%20Guo%0AAbstract%3A%20%20%20Achieving%20both%20behavioral%20similarity%20and%20appropriateness%20in%20human-like%20motion%0Ageneration%20for%20humanoid%20robot%20remains%20an%20open%20challenge%2C%20further%20compounded%20by%0Athe%20lack%20of%20cross-embodiment%20adaptability.%20To%20address%20this%20problem%2C%20we%20propose%0AHuBE%2C%20a%20bi-level%20closed-loop%20framework%20that%20integrates%20robot%20state%2C%20goal%20poses%2C%0Aand%20contextual%20situations%20to%20generate%20human-like%20behaviors%2C%20ensuring%20both%0Abehavioral%20similarity%20and%20appropriateness%2C%20and%20eliminating%20structural%0Amismatches%20between%20motion%20generation%20and%20execution.%20To%20support%20this%20framework%2C%0Awe%20construct%20HPose%2C%20a%20context-enriched%20dataset%20featuring%20fine-grained%0Asituational%20annotations.%20Furthermore%2C%20we%20introduce%20a%20bone%20scaling-based%20data%0Aaugmentation%20strategy%20that%20ensures%20millimeter-level%20compatibility%20across%0Aheterogeneous%20humanoid%20robots.%20Comprehensive%20evaluations%20on%20multiple%20commercial%0Aplatforms%20demonstrate%20that%20HuBE%20significantly%20improves%20motion%20similarity%2C%0Abehavioral%20appropriateness%2C%20and%20computational%20efficiency%20over%20state-of-the-art%0Abaselines%2C%20establishing%20a%20solid%20foundation%20for%20transferable%20and%20human-like%0Abehavior%20execution%20across%20diverse%20humanoid%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuBE%253A%2520Cross-Embodiment%2520Human-like%2520Behavior%2520Execution%2520for%2520Humanoid%2520Robots%26entry.906535625%3DShipeng%2520Lyu%2520and%2520Fangyuan%2520Wang%2520and%2520Weiwei%2520Lin%2520and%2520Luhao%2520Zhu%2520and%2520David%2520Navarro-Alarcon%2520and%2520Guodong%2520Guo%26entry.1292438233%3D%2520%2520Achieving%2520both%2520behavioral%2520similarity%2520and%2520appropriateness%2520in%2520human-like%2520motion%250Ageneration%2520for%2520humanoid%2520robot%2520remains%2520an%2520open%2520challenge%252C%2520further%2520compounded%2520by%250Athe%2520lack%2520of%2520cross-embodiment%2520adaptability.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%250AHuBE%252C%2520a%2520bi-level%2520closed-loop%2520framework%2520that%2520integrates%2520robot%2520state%252C%2520goal%2520poses%252C%250Aand%2520contextual%2520situations%2520to%2520generate%2520human-like%2520behaviors%252C%2520ensuring%2520both%250Abehavioral%2520similarity%2520and%2520appropriateness%252C%2520and%2520eliminating%2520structural%250Amismatches%2520between%2520motion%2520generation%2520and%2520execution.%2520To%2520support%2520this%2520framework%252C%250Awe%2520construct%2520HPose%252C%2520a%2520context-enriched%2520dataset%2520featuring%2520fine-grained%250Asituational%2520annotations.%2520Furthermore%252C%2520we%2520introduce%2520a%2520bone%2520scaling-based%2520data%250Aaugmentation%2520strategy%2520that%2520ensures%2520millimeter-level%2520compatibility%2520across%250Aheterogeneous%2520humanoid%2520robots.%2520Comprehensive%2520evaluations%2520on%2520multiple%2520commercial%250Aplatforms%2520demonstrate%2520that%2520HuBE%2520significantly%2520improves%2520motion%2520similarity%252C%250Abehavioral%2520appropriateness%252C%2520and%2520computational%2520efficiency%2520over%2520state-of-the-art%250Abaselines%252C%2520establishing%2520a%2520solid%2520foundation%2520for%2520transferable%2520and%2520human-like%250Abehavior%2520execution%2520across%2520diverse%2520humanoid%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HuBE%3A%20Cross-Embodiment%20Human-like%20Behavior%20Execution%20for%20Humanoid%20Robots&entry.906535625=Shipeng%20Lyu%20and%20Fangyuan%20Wang%20and%20Weiwei%20Lin%20and%20Luhao%20Zhu%20and%20David%20Navarro-Alarcon%20and%20Guodong%20Guo&entry.1292438233=%20%20Achieving%20both%20behavioral%20similarity%20and%20appropriateness%20in%20human-like%20motion%0Ageneration%20for%20humanoid%20robot%20remains%20an%20open%20challenge%2C%20further%20compounded%20by%0Athe%20lack%20of%20cross-embodiment%20adaptability.%20To%20address%20this%20problem%2C%20we%20propose%0AHuBE%2C%20a%20bi-level%20closed-loop%20framework%20that%20integrates%20robot%20state%2C%20goal%20poses%2C%0Aand%20contextual%20situations%20to%20generate%20human-like%20behaviors%2C%20ensuring%20both%0Abehavioral%20similarity%20and%20appropriateness%2C%20and%20eliminating%20structural%0Amismatches%20between%20motion%20generation%20and%20execution.%20To%20support%20this%20framework%2C%0Awe%20construct%20HPose%2C%20a%20context-enriched%20dataset%20featuring%20fine-grained%0Asituational%20annotations.%20Furthermore%2C%20we%20introduce%20a%20bone%20scaling-based%20data%0Aaugmentation%20strategy%20that%20ensures%20millimeter-level%20compatibility%20across%0Aheterogeneous%20humanoid%20robots.%20Comprehensive%20evaluations%20on%20multiple%20commercial%0Aplatforms%20demonstrate%20that%20HuBE%20significantly%20improves%20motion%20similarity%2C%0Abehavioral%20appropriateness%2C%20and%20computational%20efficiency%20over%20state-of-the-art%0Abaselines%2C%20establishing%20a%20solid%20foundation%20for%20transferable%20and%20human-like%0Abehavior%20execution%20across%20diverse%20humanoid%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19002v1&entry.124074799=Read"},
{"title": "ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially\n  Relevant Video Retrieval", "author": "Yi Pan and Yujia Zhang and Michael Kampffmeyer and Xiaoguang Zhao", "abstract": "  Partially Relevant Video Retrieval (PRVR) is a practical yet challenging task\nthat involves retrieving videos based on queries relevant to only specific\nsegments. While existing works follow the paradigm of developing models to\nprocess unimodal features, powerful pretrained vision-language models like CLIP\nremain underexplored in this field. To bridge this gap, we propose ProPy, a\nmodel with systematic architectural adaption of CLIP specifically designed for\nPRVR. Drawing insights from the semantic relevance of multi-granularity events,\nProPy introduces two key innovations: (1) A Prompt Pyramid structure that\norganizes event prompts to capture semantics at multiple granularity levels,\nand (2) An Ancestor-Descendant Interaction Mechanism built on the pyramid that\nenables dynamic semantic interaction among events. With these designs, ProPy\nachieves SOTA performance on three public datasets, outperforming previous\nmodels by significant margins. Code is available at\nhttps://github.com/BUAAPY/ProPy.\n", "link": "http://arxiv.org/abs/2508.19024v1", "date": "2025-08-26", "relevancy": 2.2537, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5666}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProPy%3A%20Building%20Interactive%20Prompt%20Pyramids%20upon%20CLIP%20for%20Partially%0A%20%20Relevant%20Video%20Retrieval&body=Title%3A%20ProPy%3A%20Building%20Interactive%20Prompt%20Pyramids%20upon%20CLIP%20for%20Partially%0A%20%20Relevant%20Video%20Retrieval%0AAuthor%3A%20Yi%20Pan%20and%20Yujia%20Zhang%20and%20Michael%20Kampffmeyer%20and%20Xiaoguang%20Zhao%0AAbstract%3A%20%20%20Partially%20Relevant%20Video%20Retrieval%20%28PRVR%29%20is%20a%20practical%20yet%20challenging%20task%0Athat%20involves%20retrieving%20videos%20based%20on%20queries%20relevant%20to%20only%20specific%0Asegments.%20While%20existing%20works%20follow%20the%20paradigm%20of%20developing%20models%20to%0Aprocess%20unimodal%20features%2C%20powerful%20pretrained%20vision-language%20models%20like%20CLIP%0Aremain%20underexplored%20in%20this%20field.%20To%20bridge%20this%20gap%2C%20we%20propose%20ProPy%2C%20a%0Amodel%20with%20systematic%20architectural%20adaption%20of%20CLIP%20specifically%20designed%20for%0APRVR.%20Drawing%20insights%20from%20the%20semantic%20relevance%20of%20multi-granularity%20events%2C%0AProPy%20introduces%20two%20key%20innovations%3A%20%281%29%20A%20Prompt%20Pyramid%20structure%20that%0Aorganizes%20event%20prompts%20to%20capture%20semantics%20at%20multiple%20granularity%20levels%2C%0Aand%20%282%29%20An%20Ancestor-Descendant%20Interaction%20Mechanism%20built%20on%20the%20pyramid%20that%0Aenables%20dynamic%20semantic%20interaction%20among%20events.%20With%20these%20designs%2C%20ProPy%0Aachieves%20SOTA%20performance%20on%20three%20public%20datasets%2C%20outperforming%20previous%0Amodels%20by%20significant%20margins.%20Code%20is%20available%20at%0Ahttps%3A//github.com/BUAAPY/ProPy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProPy%253A%2520Building%2520Interactive%2520Prompt%2520Pyramids%2520upon%2520CLIP%2520for%2520Partially%250A%2520%2520Relevant%2520Video%2520Retrieval%26entry.906535625%3DYi%2520Pan%2520and%2520Yujia%2520Zhang%2520and%2520Michael%2520Kampffmeyer%2520and%2520Xiaoguang%2520Zhao%26entry.1292438233%3D%2520%2520Partially%2520Relevant%2520Video%2520Retrieval%2520%2528PRVR%2529%2520is%2520a%2520practical%2520yet%2520challenging%2520task%250Athat%2520involves%2520retrieving%2520videos%2520based%2520on%2520queries%2520relevant%2520to%2520only%2520specific%250Asegments.%2520While%2520existing%2520works%2520follow%2520the%2520paradigm%2520of%2520developing%2520models%2520to%250Aprocess%2520unimodal%2520features%252C%2520powerful%2520pretrained%2520vision-language%2520models%2520like%2520CLIP%250Aremain%2520underexplored%2520in%2520this%2520field.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520ProPy%252C%2520a%250Amodel%2520with%2520systematic%2520architectural%2520adaption%2520of%2520CLIP%2520specifically%2520designed%2520for%250APRVR.%2520Drawing%2520insights%2520from%2520the%2520semantic%2520relevance%2520of%2520multi-granularity%2520events%252C%250AProPy%2520introduces%2520two%2520key%2520innovations%253A%2520%25281%2529%2520A%2520Prompt%2520Pyramid%2520structure%2520that%250Aorganizes%2520event%2520prompts%2520to%2520capture%2520semantics%2520at%2520multiple%2520granularity%2520levels%252C%250Aand%2520%25282%2529%2520An%2520Ancestor-Descendant%2520Interaction%2520Mechanism%2520built%2520on%2520the%2520pyramid%2520that%250Aenables%2520dynamic%2520semantic%2520interaction%2520among%2520events.%2520With%2520these%2520designs%252C%2520ProPy%250Aachieves%2520SOTA%2520performance%2520on%2520three%2520public%2520datasets%252C%2520outperforming%2520previous%250Amodels%2520by%2520significant%2520margins.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/BUAAPY/ProPy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProPy%3A%20Building%20Interactive%20Prompt%20Pyramids%20upon%20CLIP%20for%20Partially%0A%20%20Relevant%20Video%20Retrieval&entry.906535625=Yi%20Pan%20and%20Yujia%20Zhang%20and%20Michael%20Kampffmeyer%20and%20Xiaoguang%20Zhao&entry.1292438233=%20%20Partially%20Relevant%20Video%20Retrieval%20%28PRVR%29%20is%20a%20practical%20yet%20challenging%20task%0Athat%20involves%20retrieving%20videos%20based%20on%20queries%20relevant%20to%20only%20specific%0Asegments.%20While%20existing%20works%20follow%20the%20paradigm%20of%20developing%20models%20to%0Aprocess%20unimodal%20features%2C%20powerful%20pretrained%20vision-language%20models%20like%20CLIP%0Aremain%20underexplored%20in%20this%20field.%20To%20bridge%20this%20gap%2C%20we%20propose%20ProPy%2C%20a%0Amodel%20with%20systematic%20architectural%20adaption%20of%20CLIP%20specifically%20designed%20for%0APRVR.%20Drawing%20insights%20from%20the%20semantic%20relevance%20of%20multi-granularity%20events%2C%0AProPy%20introduces%20two%20key%20innovations%3A%20%281%29%20A%20Prompt%20Pyramid%20structure%20that%0Aorganizes%20event%20prompts%20to%20capture%20semantics%20at%20multiple%20granularity%20levels%2C%0Aand%20%282%29%20An%20Ancestor-Descendant%20Interaction%20Mechanism%20built%20on%20the%20pyramid%20that%0Aenables%20dynamic%20semantic%20interaction%20among%20events.%20With%20these%20designs%2C%20ProPy%0Aachieves%20SOTA%20performance%20on%20three%20public%20datasets%2C%20outperforming%20previous%0Amodels%20by%20significant%20margins.%20Code%20is%20available%20at%0Ahttps%3A//github.com/BUAAPY/ProPy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19024v1&entry.124074799=Read"},
{"title": "Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG)\n  Algorithm", "author": "Hichem Cheriet and Khellat Kihel Badra and Chouraqui Samira", "abstract": "  Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical\nfor various applications, including combat support, package delivery and Search\nand Rescue Operations. This paper introduces the Tangent Intersection Guidance\n(TIG) algorithm, an advanced approach for UAV path planning in both static and\ndynamic environments. The algorithm uses the elliptic tangent intersection\nmethod to generate feasible paths. It generates two sub-paths for each threat,\nselects the optimal route based on a heuristic rule, and iteratively refines\nthe path until the target is reached. Considering the UAV kinematic and dynamic\nconstraints, a modified smoothing technique based on quadratic B\\'ezier curves\nis adopted to generate a smooth and efficient route. Experimental results show\nthat the TIG algorithm can generate the shortest path in less time, starting\nfrom 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent\nGraph, and Static APPATT algorithms in static environments. Furthermore, in\ncompletely unknown and partially known environments, TIG demonstrates efficient\nreal-time path planning capabilities for collision avoidance, outperforming APF\nand Dynamic APPATT algorithms.\n", "link": "http://arxiv.org/abs/2508.18967v1", "date": "2025-08-26", "relevancy": 2.2477, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.468}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4556}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20UAV%20Path%20Planning%20Using%20the%20Tangent%20Intersection%20Guidance%20%28TIG%29%0A%20%20Algorithm&body=Title%3A%20Enhanced%20UAV%20Path%20Planning%20Using%20the%20Tangent%20Intersection%20Guidance%20%28TIG%29%0A%20%20Algorithm%0AAuthor%3A%20Hichem%20Cheriet%20and%20Khellat%20Kihel%20Badra%20and%20Chouraqui%20Samira%0AAbstract%3A%20%20%20Efficient%20and%20safe%20navigation%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20is%20critical%0Afor%20various%20applications%2C%20including%20combat%20support%2C%20package%20delivery%20and%20Search%0Aand%20Rescue%20Operations.%20This%20paper%20introduces%20the%20Tangent%20Intersection%20Guidance%0A%28TIG%29%20algorithm%2C%20an%20advanced%20approach%20for%20UAV%20path%20planning%20in%20both%20static%20and%0Adynamic%20environments.%20The%20algorithm%20uses%20the%20elliptic%20tangent%20intersection%0Amethod%20to%20generate%20feasible%20paths.%20It%20generates%20two%20sub-paths%20for%20each%20threat%2C%0Aselects%20the%20optimal%20route%20based%20on%20a%20heuristic%20rule%2C%20and%20iteratively%20refines%0Athe%20path%20until%20the%20target%20is%20reached.%20Considering%20the%20UAV%20kinematic%20and%20dynamic%0Aconstraints%2C%20a%20modified%20smoothing%20technique%20based%20on%20quadratic%20B%5C%27ezier%20curves%0Ais%20adopted%20to%20generate%20a%20smooth%20and%20efficient%20route.%20Experimental%20results%20show%0Athat%20the%20TIG%20algorithm%20can%20generate%20the%20shortest%20path%20in%20less%20time%2C%20starting%0Afrom%200.01%20seconds%2C%20with%20fewer%20turning%20angles%20compared%20to%20A%2A%2C%20PRM%2C%20RRT%2A%2C%20Tangent%0AGraph%2C%20and%20Static%20APPATT%20algorithms%20in%20static%20environments.%20Furthermore%2C%20in%0Acompletely%20unknown%20and%20partially%20known%20environments%2C%20TIG%20demonstrates%20efficient%0Areal-time%20path%20planning%20capabilities%20for%20collision%20avoidance%2C%20outperforming%20APF%0Aand%20Dynamic%20APPATT%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520UAV%2520Path%2520Planning%2520Using%2520the%2520Tangent%2520Intersection%2520Guidance%2520%2528TIG%2529%250A%2520%2520Algorithm%26entry.906535625%3DHichem%2520Cheriet%2520and%2520Khellat%2520Kihel%2520Badra%2520and%2520Chouraqui%2520Samira%26entry.1292438233%3D%2520%2520Efficient%2520and%2520safe%2520navigation%2520of%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520is%2520critical%250Afor%2520various%2520applications%252C%2520including%2520combat%2520support%252C%2520package%2520delivery%2520and%2520Search%250Aand%2520Rescue%2520Operations.%2520This%2520paper%2520introduces%2520the%2520Tangent%2520Intersection%2520Guidance%250A%2528TIG%2529%2520algorithm%252C%2520an%2520advanced%2520approach%2520for%2520UAV%2520path%2520planning%2520in%2520both%2520static%2520and%250Adynamic%2520environments.%2520The%2520algorithm%2520uses%2520the%2520elliptic%2520tangent%2520intersection%250Amethod%2520to%2520generate%2520feasible%2520paths.%2520It%2520generates%2520two%2520sub-paths%2520for%2520each%2520threat%252C%250Aselects%2520the%2520optimal%2520route%2520based%2520on%2520a%2520heuristic%2520rule%252C%2520and%2520iteratively%2520refines%250Athe%2520path%2520until%2520the%2520target%2520is%2520reached.%2520Considering%2520the%2520UAV%2520kinematic%2520and%2520dynamic%250Aconstraints%252C%2520a%2520modified%2520smoothing%2520technique%2520based%2520on%2520quadratic%2520B%255C%2527ezier%2520curves%250Ais%2520adopted%2520to%2520generate%2520a%2520smooth%2520and%2520efficient%2520route.%2520Experimental%2520results%2520show%250Athat%2520the%2520TIG%2520algorithm%2520can%2520generate%2520the%2520shortest%2520path%2520in%2520less%2520time%252C%2520starting%250Afrom%25200.01%2520seconds%252C%2520with%2520fewer%2520turning%2520angles%2520compared%2520to%2520A%252A%252C%2520PRM%252C%2520RRT%252A%252C%2520Tangent%250AGraph%252C%2520and%2520Static%2520APPATT%2520algorithms%2520in%2520static%2520environments.%2520Furthermore%252C%2520in%250Acompletely%2520unknown%2520and%2520partially%2520known%2520environments%252C%2520TIG%2520demonstrates%2520efficient%250Areal-time%2520path%2520planning%2520capabilities%2520for%2520collision%2520avoidance%252C%2520outperforming%2520APF%250Aand%2520Dynamic%2520APPATT%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20UAV%20Path%20Planning%20Using%20the%20Tangent%20Intersection%20Guidance%20%28TIG%29%0A%20%20Algorithm&entry.906535625=Hichem%20Cheriet%20and%20Khellat%20Kihel%20Badra%20and%20Chouraqui%20Samira&entry.1292438233=%20%20Efficient%20and%20safe%20navigation%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20is%20critical%0Afor%20various%20applications%2C%20including%20combat%20support%2C%20package%20delivery%20and%20Search%0Aand%20Rescue%20Operations.%20This%20paper%20introduces%20the%20Tangent%20Intersection%20Guidance%0A%28TIG%29%20algorithm%2C%20an%20advanced%20approach%20for%20UAV%20path%20planning%20in%20both%20static%20and%0Adynamic%20environments.%20The%20algorithm%20uses%20the%20elliptic%20tangent%20intersection%0Amethod%20to%20generate%20feasible%20paths.%20It%20generates%20two%20sub-paths%20for%20each%20threat%2C%0Aselects%20the%20optimal%20route%20based%20on%20a%20heuristic%20rule%2C%20and%20iteratively%20refines%0Athe%20path%20until%20the%20target%20is%20reached.%20Considering%20the%20UAV%20kinematic%20and%20dynamic%0Aconstraints%2C%20a%20modified%20smoothing%20technique%20based%20on%20quadratic%20B%5C%27ezier%20curves%0Ais%20adopted%20to%20generate%20a%20smooth%20and%20efficient%20route.%20Experimental%20results%20show%0Athat%20the%20TIG%20algorithm%20can%20generate%20the%20shortest%20path%20in%20less%20time%2C%20starting%0Afrom%200.01%20seconds%2C%20with%20fewer%20turning%20angles%20compared%20to%20A%2A%2C%20PRM%2C%20RRT%2A%2C%20Tangent%0AGraph%2C%20and%20Static%20APPATT%20algorithms%20in%20static%20environments.%20Furthermore%2C%20in%0Acompletely%20unknown%20and%20partially%20known%20environments%2C%20TIG%20demonstrates%20efficient%0Areal-time%20path%20planning%20capabilities%20for%20collision%20avoidance%2C%20outperforming%20APF%0Aand%20Dynamic%20APPATT%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18967v1&entry.124074799=Read"},
{"title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented\n  Generation", "author": "Chan-Wei Hu and Yueqi Wang and Shuo Xing and Chia-Ju Chen and Suofei Feng and Ryan Rossi and Zhengzhong Tu", "abstract": "  Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning.\n", "link": "http://arxiv.org/abs/2505.24073v2", "date": "2025-08-26", "relevancy": 2.2414, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mRAG%3A%20Elucidating%20the%20Design%20Space%20of%20Multi-modal%20Retrieval-Augmented%0A%20%20Generation&body=Title%3A%20mRAG%3A%20Elucidating%20the%20Design%20Space%20of%20Multi-modal%20Retrieval-Augmented%0A%20%20Generation%0AAuthor%3A%20Chan-Wei%20Hu%20and%20Yueqi%20Wang%20and%20Shuo%20Xing%20and%20Chia-Ju%20Chen%20and%20Suofei%20Feng%20and%20Ryan%20Rossi%20and%20Zhengzhong%20Tu%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20remarkable%20strides%20in%0Amultimodal%20tasks%20such%20as%20visual%20question%20answering%2C%20visual%20grounding%2C%20and%0Acomplex%20reasoning.%20However%2C%20they%20remain%20limited%20by%20static%20training%20data%2C%0Asusceptibility%20to%20hallucinations%2C%20and%20inability%20to%20verify%20claims%20against%0Aup-to-date%2C%20external%20evidence%2C%20compromising%20their%20performance%20in%20dynamic%0Areal-world%20applications.%20Retrieval-Augmented%20Generation%20%28RAG%29%20offers%20a%0Apractical%20solution%20to%20mitigate%20these%20challenges%20by%20allowing%20the%20LVLMs%20to%20access%0Alarge-scale%20knowledge%20databases%20via%20retrieval%20mechanisms%2C%20thereby%20grounding%0Amodel%20outputs%20in%20factual%2C%20contextually%20relevant%20information.%20Here%20in%20this%0Apaper%2C%20we%20conduct%20the%20first%20systematic%20dissection%20of%20the%20multimodal%20RAG%0Apipeline%20for%20LVLMs%2C%20explicitly%20investigating%20%281%29%20the%20retrieval%20phase%3A%20on%20the%0Amodality%20configurations%20and%20retrieval%20strategies%2C%20%282%29%20the%20re-ranking%20stage%3A%20on%0Astrategies%20to%20mitigate%20positional%20biases%20and%20improve%20the%20relevance%20of%20retrieved%0Aevidence%2C%20and%20%283%29%20the%20generation%20phase%3A%20we%20further%20investigate%20how%20to%20best%0Aintegrate%20retrieved%20candidates%20into%20the%20final%20generation%20process.%20Finally%2C%20we%0Aextend%20to%20explore%20a%20unified%20agentic%20framework%20that%20integrates%20re-ranking%20and%0Ageneration%20through%20self-reflection%2C%20enabling%20LVLMs%20to%20select%20relevant%20evidence%0Aand%20suppress%20irrelevant%20context%20dynamically.%20Our%20full-stack%20exploration%20of%20RAG%0Afor%20LVLMs%20yields%20substantial%20insights%2C%20resulting%20in%20an%20average%20performance%0Aboost%20of%205%25%20without%20any%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmRAG%253A%2520Elucidating%2520the%2520Design%2520Space%2520of%2520Multi-modal%2520Retrieval-Augmented%250A%2520%2520Generation%26entry.906535625%3DChan-Wei%2520Hu%2520and%2520Yueqi%2520Wang%2520and%2520Shuo%2520Xing%2520and%2520Chia-Ju%2520Chen%2520and%2520Suofei%2520Feng%2520and%2520Ryan%2520Rossi%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520made%2520remarkable%2520strides%2520in%250Amultimodal%2520tasks%2520such%2520as%2520visual%2520question%2520answering%252C%2520visual%2520grounding%252C%2520and%250Acomplex%2520reasoning.%2520However%252C%2520they%2520remain%2520limited%2520by%2520static%2520training%2520data%252C%250Asusceptibility%2520to%2520hallucinations%252C%2520and%2520inability%2520to%2520verify%2520claims%2520against%250Aup-to-date%252C%2520external%2520evidence%252C%2520compromising%2520their%2520performance%2520in%2520dynamic%250Areal-world%2520applications.%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520offers%2520a%250Apractical%2520solution%2520to%2520mitigate%2520these%2520challenges%2520by%2520allowing%2520the%2520LVLMs%2520to%2520access%250Alarge-scale%2520knowledge%2520databases%2520via%2520retrieval%2520mechanisms%252C%2520thereby%2520grounding%250Amodel%2520outputs%2520in%2520factual%252C%2520contextually%2520relevant%2520information.%2520Here%2520in%2520this%250Apaper%252C%2520we%2520conduct%2520the%2520first%2520systematic%2520dissection%2520of%2520the%2520multimodal%2520RAG%250Apipeline%2520for%2520LVLMs%252C%2520explicitly%2520investigating%2520%25281%2529%2520the%2520retrieval%2520phase%253A%2520on%2520the%250Amodality%2520configurations%2520and%2520retrieval%2520strategies%252C%2520%25282%2529%2520the%2520re-ranking%2520stage%253A%2520on%250Astrategies%2520to%2520mitigate%2520positional%2520biases%2520and%2520improve%2520the%2520relevance%2520of%2520retrieved%250Aevidence%252C%2520and%2520%25283%2529%2520the%2520generation%2520phase%253A%2520we%2520further%2520investigate%2520how%2520to%2520best%250Aintegrate%2520retrieved%2520candidates%2520into%2520the%2520final%2520generation%2520process.%2520Finally%252C%2520we%250Aextend%2520to%2520explore%2520a%2520unified%2520agentic%2520framework%2520that%2520integrates%2520re-ranking%2520and%250Ageneration%2520through%2520self-reflection%252C%2520enabling%2520LVLMs%2520to%2520select%2520relevant%2520evidence%250Aand%2520suppress%2520irrelevant%2520context%2520dynamically.%2520Our%2520full-stack%2520exploration%2520of%2520RAG%250Afor%2520LVLMs%2520yields%2520substantial%2520insights%252C%2520resulting%2520in%2520an%2520average%2520performance%250Aboost%2520of%25205%2525%2520without%2520any%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mRAG%3A%20Elucidating%20the%20Design%20Space%20of%20Multi-modal%20Retrieval-Augmented%0A%20%20Generation&entry.906535625=Chan-Wei%20Hu%20and%20Yueqi%20Wang%20and%20Shuo%20Xing%20and%20Chia-Ju%20Chen%20and%20Suofei%20Feng%20and%20Ryan%20Rossi%20and%20Zhengzhong%20Tu&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20remarkable%20strides%20in%0Amultimodal%20tasks%20such%20as%20visual%20question%20answering%2C%20visual%20grounding%2C%20and%0Acomplex%20reasoning.%20However%2C%20they%20remain%20limited%20by%20static%20training%20data%2C%0Asusceptibility%20to%20hallucinations%2C%20and%20inability%20to%20verify%20claims%20against%0Aup-to-date%2C%20external%20evidence%2C%20compromising%20their%20performance%20in%20dynamic%0Areal-world%20applications.%20Retrieval-Augmented%20Generation%20%28RAG%29%20offers%20a%0Apractical%20solution%20to%20mitigate%20these%20challenges%20by%20allowing%20the%20LVLMs%20to%20access%0Alarge-scale%20knowledge%20databases%20via%20retrieval%20mechanisms%2C%20thereby%20grounding%0Amodel%20outputs%20in%20factual%2C%20contextually%20relevant%20information.%20Here%20in%20this%0Apaper%2C%20we%20conduct%20the%20first%20systematic%20dissection%20of%20the%20multimodal%20RAG%0Apipeline%20for%20LVLMs%2C%20explicitly%20investigating%20%281%29%20the%20retrieval%20phase%3A%20on%20the%0Amodality%20configurations%20and%20retrieval%20strategies%2C%20%282%29%20the%20re-ranking%20stage%3A%20on%0Astrategies%20to%20mitigate%20positional%20biases%20and%20improve%20the%20relevance%20of%20retrieved%0Aevidence%2C%20and%20%283%29%20the%20generation%20phase%3A%20we%20further%20investigate%20how%20to%20best%0Aintegrate%20retrieved%20candidates%20into%20the%20final%20generation%20process.%20Finally%2C%20we%0Aextend%20to%20explore%20a%20unified%20agentic%20framework%20that%20integrates%20re-ranking%20and%0Ageneration%20through%20self-reflection%2C%20enabling%20LVLMs%20to%20select%20relevant%20evidence%0Aand%20suppress%20irrelevant%20context%20dynamically.%20Our%20full-stack%20exploration%20of%20RAG%0Afor%20LVLMs%20yields%20substantial%20insights%2C%20resulting%20in%20an%20average%20performance%0Aboost%20of%205%25%20without%20any%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24073v2&entry.124074799=Read"},
{"title": "FastMesh:Efficient Artistic Mesh Generation via Component Decoupling", "author": "Jeonghwan Kim and Yushi Lan and Armando Fortes and Yongwei Chen and Xingang Pan", "abstract": "  Recent mesh generation approaches typically tokenize triangle meshes into\nsequences of tokens and train autoregressive models to generate these tokens\nsequentially. Despite substantial progress, such token sequences inevitably\nreuse vertices multiple times to fully represent manifold meshes, as each\nvertex is shared by multiple faces. This redundancy leads to excessively long\ntoken sequences and inefficient generation processes. In this paper, we propose\nan efficient framework that generates artistic meshes by treating vertices and\nfaces separately, significantly reducing redundancy. We employ an\nautoregressive model solely for vertex generation, decreasing the token count\nto approximately 23\\% of that required by the most compact existing tokenizer.\nNext, we leverage a bidirectional transformer to complete the mesh in a single\nstep by capturing inter-vertex relationships and constructing the adjacency\nmatrix that defines the mesh faces. To further improve the generation quality,\nwe introduce a fidelity enhancer to refine vertex positioning into more natural\narrangements and propose a post-processing framework to remove undesirable edge\nconnections. Experimental results show that our method achieves more than\n8$\\times$ faster speed on mesh generation compared to state-of-the-art\napproaches, while producing higher mesh quality.\n", "link": "http://arxiv.org/abs/2508.19188v1", "date": "2025-08-26", "relevancy": 2.2342, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5715}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5576}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastMesh%3AEfficient%20Artistic%20Mesh%20Generation%20via%20Component%20Decoupling&body=Title%3A%20FastMesh%3AEfficient%20Artistic%20Mesh%20Generation%20via%20Component%20Decoupling%0AAuthor%3A%20Jeonghwan%20Kim%20and%20Yushi%20Lan%20and%20Armando%20Fortes%20and%20Yongwei%20Chen%20and%20Xingang%20Pan%0AAbstract%3A%20%20%20Recent%20mesh%20generation%20approaches%20typically%20tokenize%20triangle%20meshes%20into%0Asequences%20of%20tokens%20and%20train%20autoregressive%20models%20to%20generate%20these%20tokens%0Asequentially.%20Despite%20substantial%20progress%2C%20such%20token%20sequences%20inevitably%0Areuse%20vertices%20multiple%20times%20to%20fully%20represent%20manifold%20meshes%2C%20as%20each%0Avertex%20is%20shared%20by%20multiple%20faces.%20This%20redundancy%20leads%20to%20excessively%20long%0Atoken%20sequences%20and%20inefficient%20generation%20processes.%20In%20this%20paper%2C%20we%20propose%0Aan%20efficient%20framework%20that%20generates%20artistic%20meshes%20by%20treating%20vertices%20and%0Afaces%20separately%2C%20significantly%20reducing%20redundancy.%20We%20employ%20an%0Aautoregressive%20model%20solely%20for%20vertex%20generation%2C%20decreasing%20the%20token%20count%0Ato%20approximately%2023%5C%25%20of%20that%20required%20by%20the%20most%20compact%20existing%20tokenizer.%0ANext%2C%20we%20leverage%20a%20bidirectional%20transformer%20to%20complete%20the%20mesh%20in%20a%20single%0Astep%20by%20capturing%20inter-vertex%20relationships%20and%20constructing%20the%20adjacency%0Amatrix%20that%20defines%20the%20mesh%20faces.%20To%20further%20improve%20the%20generation%20quality%2C%0Awe%20introduce%20a%20fidelity%20enhancer%20to%20refine%20vertex%20positioning%20into%20more%20natural%0Aarrangements%20and%20propose%20a%20post-processing%20framework%20to%20remove%20undesirable%20edge%0Aconnections.%20Experimental%20results%20show%20that%20our%20method%20achieves%20more%20than%0A8%24%5Ctimes%24%20faster%20speed%20on%20mesh%20generation%20compared%20to%20state-of-the-art%0Aapproaches%2C%20while%20producing%20higher%20mesh%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastMesh%253AEfficient%2520Artistic%2520Mesh%2520Generation%2520via%2520Component%2520Decoupling%26entry.906535625%3DJeonghwan%2520Kim%2520and%2520Yushi%2520Lan%2520and%2520Armando%2520Fortes%2520and%2520Yongwei%2520Chen%2520and%2520Xingang%2520Pan%26entry.1292438233%3D%2520%2520Recent%2520mesh%2520generation%2520approaches%2520typically%2520tokenize%2520triangle%2520meshes%2520into%250Asequences%2520of%2520tokens%2520and%2520train%2520autoregressive%2520models%2520to%2520generate%2520these%2520tokens%250Asequentially.%2520Despite%2520substantial%2520progress%252C%2520such%2520token%2520sequences%2520inevitably%250Areuse%2520vertices%2520multiple%2520times%2520to%2520fully%2520represent%2520manifold%2520meshes%252C%2520as%2520each%250Avertex%2520is%2520shared%2520by%2520multiple%2520faces.%2520This%2520redundancy%2520leads%2520to%2520excessively%2520long%250Atoken%2520sequences%2520and%2520inefficient%2520generation%2520processes.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aan%2520efficient%2520framework%2520that%2520generates%2520artistic%2520meshes%2520by%2520treating%2520vertices%2520and%250Afaces%2520separately%252C%2520significantly%2520reducing%2520redundancy.%2520We%2520employ%2520an%250Aautoregressive%2520model%2520solely%2520for%2520vertex%2520generation%252C%2520decreasing%2520the%2520token%2520count%250Ato%2520approximately%252023%255C%2525%2520of%2520that%2520required%2520by%2520the%2520most%2520compact%2520existing%2520tokenizer.%250ANext%252C%2520we%2520leverage%2520a%2520bidirectional%2520transformer%2520to%2520complete%2520the%2520mesh%2520in%2520a%2520single%250Astep%2520by%2520capturing%2520inter-vertex%2520relationships%2520and%2520constructing%2520the%2520adjacency%250Amatrix%2520that%2520defines%2520the%2520mesh%2520faces.%2520To%2520further%2520improve%2520the%2520generation%2520quality%252C%250Awe%2520introduce%2520a%2520fidelity%2520enhancer%2520to%2520refine%2520vertex%2520positioning%2520into%2520more%2520natural%250Aarrangements%2520and%2520propose%2520a%2520post-processing%2520framework%2520to%2520remove%2520undesirable%2520edge%250Aconnections.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520achieves%2520more%2520than%250A8%2524%255Ctimes%2524%2520faster%2520speed%2520on%2520mesh%2520generation%2520compared%2520to%2520state-of-the-art%250Aapproaches%252C%2520while%2520producing%2520higher%2520mesh%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastMesh%3AEfficient%20Artistic%20Mesh%20Generation%20via%20Component%20Decoupling&entry.906535625=Jeonghwan%20Kim%20and%20Yushi%20Lan%20and%20Armando%20Fortes%20and%20Yongwei%20Chen%20and%20Xingang%20Pan&entry.1292438233=%20%20Recent%20mesh%20generation%20approaches%20typically%20tokenize%20triangle%20meshes%20into%0Asequences%20of%20tokens%20and%20train%20autoregressive%20models%20to%20generate%20these%20tokens%0Asequentially.%20Despite%20substantial%20progress%2C%20such%20token%20sequences%20inevitably%0Areuse%20vertices%20multiple%20times%20to%20fully%20represent%20manifold%20meshes%2C%20as%20each%0Avertex%20is%20shared%20by%20multiple%20faces.%20This%20redundancy%20leads%20to%20excessively%20long%0Atoken%20sequences%20and%20inefficient%20generation%20processes.%20In%20this%20paper%2C%20we%20propose%0Aan%20efficient%20framework%20that%20generates%20artistic%20meshes%20by%20treating%20vertices%20and%0Afaces%20separately%2C%20significantly%20reducing%20redundancy.%20We%20employ%20an%0Aautoregressive%20model%20solely%20for%20vertex%20generation%2C%20decreasing%20the%20token%20count%0Ato%20approximately%2023%5C%25%20of%20that%20required%20by%20the%20most%20compact%20existing%20tokenizer.%0ANext%2C%20we%20leverage%20a%20bidirectional%20transformer%20to%20complete%20the%20mesh%20in%20a%20single%0Astep%20by%20capturing%20inter-vertex%20relationships%20and%20constructing%20the%20adjacency%0Amatrix%20that%20defines%20the%20mesh%20faces.%20To%20further%20improve%20the%20generation%20quality%2C%0Awe%20introduce%20a%20fidelity%20enhancer%20to%20refine%20vertex%20positioning%20into%20more%20natural%0Aarrangements%20and%20propose%20a%20post-processing%20framework%20to%20remove%20undesirable%20edge%0Aconnections.%20Experimental%20results%20show%20that%20our%20method%20achieves%20more%20than%0A8%24%5Ctimes%24%20faster%20speed%20on%20mesh%20generation%20compared%20to%20state-of-the-art%0Aapproaches%2C%20while%20producing%20higher%20mesh%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19188v1&entry.124074799=Read"},
{"title": "Investigating Advanced Reasoning of Large Language Models via Black-Box\n  Interaction", "author": "Congchi Yin and Tianyi Wu and Yankai Shu and Alex Gu and Yunhan Wang and Jun Shao and Xun Jiang and Piji Li", "abstract": "  Existing tasks fall short in evaluating reasoning ability of Large Language\nModels (LLMs) in an interactive, unknown environment. This deficiency leads to\nthe isolated assessment of deductive, inductive, and abductive reasoning,\nneglecting the integrated reasoning process that is indispensable for humans\ndiscovery of real world. We introduce a novel evaluation paradigm,\n\\textit{black-box interaction}, to tackle this challenge. A black-box is\ndefined by a hidden function that maps a specific set of inputs to outputs.\nLLMs are required to unravel the hidden function behind the black-box by\ninteracting with it in given exploration turns, and reasoning over observed\ninput-output pairs. Leveraging this idea, we build the \\textsc{Oracle}\nbenchmark which comprises 6 types of black-box task and 96 black-boxes. 19\nmodern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over\n70\\% accuracy on most easy black-boxes. But it still struggles with some hard\nblack-box tasks, where its average performance drops below 40\\%. Further\nanalysis indicates a universal difficulty among LLMs: They lack the high-level\nplanning capability to develop efficient and adaptive exploration strategies\nfor hypothesis refinement.\n", "link": "http://arxiv.org/abs/2508.19035v1", "date": "2025-08-26", "relevancy": 2.2234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5615}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Advanced%20Reasoning%20of%20Large%20Language%20Models%20via%20Black-Box%0A%20%20Interaction&body=Title%3A%20Investigating%20Advanced%20Reasoning%20of%20Large%20Language%20Models%20via%20Black-Box%0A%20%20Interaction%0AAuthor%3A%20Congchi%20Yin%20and%20Tianyi%20Wu%20and%20Yankai%20Shu%20and%20Alex%20Gu%20and%20Yunhan%20Wang%20and%20Jun%20Shao%20and%20Xun%20Jiang%20and%20Piji%20Li%0AAbstract%3A%20%20%20Existing%20tasks%20fall%20short%20in%20evaluating%20reasoning%20ability%20of%20Large%20Language%0AModels%20%28LLMs%29%20in%20an%20interactive%2C%20unknown%20environment.%20This%20deficiency%20leads%20to%0Athe%20isolated%20assessment%20of%20deductive%2C%20inductive%2C%20and%20abductive%20reasoning%2C%0Aneglecting%20the%20integrated%20reasoning%20process%20that%20is%20indispensable%20for%20humans%0Adiscovery%20of%20real%20world.%20We%20introduce%20a%20novel%20evaluation%20paradigm%2C%0A%5Ctextit%7Bblack-box%20interaction%7D%2C%20to%20tackle%20this%20challenge.%20A%20black-box%20is%0Adefined%20by%20a%20hidden%20function%20that%20maps%20a%20specific%20set%20of%20inputs%20to%20outputs.%0ALLMs%20are%20required%20to%20unravel%20the%20hidden%20function%20behind%20the%20black-box%20by%0Ainteracting%20with%20it%20in%20given%20exploration%20turns%2C%20and%20reasoning%20over%20observed%0Ainput-output%20pairs.%20Leveraging%20this%20idea%2C%20we%20build%20the%20%5Ctextsc%7BOracle%7D%0Abenchmark%20which%20comprises%206%20types%20of%20black-box%20task%20and%2096%20black-boxes.%2019%0Amodern%20LLMs%20are%20benchmarked.%20o3%20ranks%20first%20in%205%20of%20the%206%20tasks%2C%20achieving%20over%0A70%5C%25%20accuracy%20on%20most%20easy%20black-boxes.%20But%20it%20still%20struggles%20with%20some%20hard%0Ablack-box%20tasks%2C%20where%20its%20average%20performance%20drops%20below%2040%5C%25.%20Further%0Aanalysis%20indicates%20a%20universal%20difficulty%20among%20LLMs%3A%20They%20lack%20the%20high-level%0Aplanning%20capability%20to%20develop%20efficient%20and%20adaptive%20exploration%20strategies%0Afor%20hypothesis%20refinement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Advanced%2520Reasoning%2520of%2520Large%2520Language%2520Models%2520via%2520Black-Box%250A%2520%2520Interaction%26entry.906535625%3DCongchi%2520Yin%2520and%2520Tianyi%2520Wu%2520and%2520Yankai%2520Shu%2520and%2520Alex%2520Gu%2520and%2520Yunhan%2520Wang%2520and%2520Jun%2520Shao%2520and%2520Xun%2520Jiang%2520and%2520Piji%2520Li%26entry.1292438233%3D%2520%2520Existing%2520tasks%2520fall%2520short%2520in%2520evaluating%2520reasoning%2520ability%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520in%2520an%2520interactive%252C%2520unknown%2520environment.%2520This%2520deficiency%2520leads%2520to%250Athe%2520isolated%2520assessment%2520of%2520deductive%252C%2520inductive%252C%2520and%2520abductive%2520reasoning%252C%250Aneglecting%2520the%2520integrated%2520reasoning%2520process%2520that%2520is%2520indispensable%2520for%2520humans%250Adiscovery%2520of%2520real%2520world.%2520We%2520introduce%2520a%2520novel%2520evaluation%2520paradigm%252C%250A%255Ctextit%257Bblack-box%2520interaction%257D%252C%2520to%2520tackle%2520this%2520challenge.%2520A%2520black-box%2520is%250Adefined%2520by%2520a%2520hidden%2520function%2520that%2520maps%2520a%2520specific%2520set%2520of%2520inputs%2520to%2520outputs.%250ALLMs%2520are%2520required%2520to%2520unravel%2520the%2520hidden%2520function%2520behind%2520the%2520black-box%2520by%250Ainteracting%2520with%2520it%2520in%2520given%2520exploration%2520turns%252C%2520and%2520reasoning%2520over%2520observed%250Ainput-output%2520pairs.%2520Leveraging%2520this%2520idea%252C%2520we%2520build%2520the%2520%255Ctextsc%257BOracle%257D%250Abenchmark%2520which%2520comprises%25206%2520types%2520of%2520black-box%2520task%2520and%252096%2520black-boxes.%252019%250Amodern%2520LLMs%2520are%2520benchmarked.%2520o3%2520ranks%2520first%2520in%25205%2520of%2520the%25206%2520tasks%252C%2520achieving%2520over%250A70%255C%2525%2520accuracy%2520on%2520most%2520easy%2520black-boxes.%2520But%2520it%2520still%2520struggles%2520with%2520some%2520hard%250Ablack-box%2520tasks%252C%2520where%2520its%2520average%2520performance%2520drops%2520below%252040%255C%2525.%2520Further%250Aanalysis%2520indicates%2520a%2520universal%2520difficulty%2520among%2520LLMs%253A%2520They%2520lack%2520the%2520high-level%250Aplanning%2520capability%2520to%2520develop%2520efficient%2520and%2520adaptive%2520exploration%2520strategies%250Afor%2520hypothesis%2520refinement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Advanced%20Reasoning%20of%20Large%20Language%20Models%20via%20Black-Box%0A%20%20Interaction&entry.906535625=Congchi%20Yin%20and%20Tianyi%20Wu%20and%20Yankai%20Shu%20and%20Alex%20Gu%20and%20Yunhan%20Wang%20and%20Jun%20Shao%20and%20Xun%20Jiang%20and%20Piji%20Li&entry.1292438233=%20%20Existing%20tasks%20fall%20short%20in%20evaluating%20reasoning%20ability%20of%20Large%20Language%0AModels%20%28LLMs%29%20in%20an%20interactive%2C%20unknown%20environment.%20This%20deficiency%20leads%20to%0Athe%20isolated%20assessment%20of%20deductive%2C%20inductive%2C%20and%20abductive%20reasoning%2C%0Aneglecting%20the%20integrated%20reasoning%20process%20that%20is%20indispensable%20for%20humans%0Adiscovery%20of%20real%20world.%20We%20introduce%20a%20novel%20evaluation%20paradigm%2C%0A%5Ctextit%7Bblack-box%20interaction%7D%2C%20to%20tackle%20this%20challenge.%20A%20black-box%20is%0Adefined%20by%20a%20hidden%20function%20that%20maps%20a%20specific%20set%20of%20inputs%20to%20outputs.%0ALLMs%20are%20required%20to%20unravel%20the%20hidden%20function%20behind%20the%20black-box%20by%0Ainteracting%20with%20it%20in%20given%20exploration%20turns%2C%20and%20reasoning%20over%20observed%0Ainput-output%20pairs.%20Leveraging%20this%20idea%2C%20we%20build%20the%20%5Ctextsc%7BOracle%7D%0Abenchmark%20which%20comprises%206%20types%20of%20black-box%20task%20and%2096%20black-boxes.%2019%0Amodern%20LLMs%20are%20benchmarked.%20o3%20ranks%20first%20in%205%20of%20the%206%20tasks%2C%20achieving%20over%0A70%5C%25%20accuracy%20on%20most%20easy%20black-boxes.%20But%20it%20still%20struggles%20with%20some%20hard%0Ablack-box%20tasks%2C%20where%20its%20average%20performance%20drops%20below%2040%5C%25.%20Further%0Aanalysis%20indicates%20a%20universal%20difficulty%20among%20LLMs%3A%20They%20lack%20the%20high-level%0Aplanning%20capability%20to%20develop%20efficient%20and%20adaptive%20exploration%20strategies%0Afor%20hypothesis%20refinement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19035v1&entry.124074799=Read"},
{"title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation\n  and Language Generation for Explainable QA Hallucination Detection", "author": "Yiming Huang and Junyan Zhang and Zihao Wang and Biquan Bie and Yunzhong Qiu and Yi R. Fung and Xinlei He", "abstract": "  Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage.\n", "link": "http://arxiv.org/abs/2505.15386v2", "date": "2025-08-26", "relevancy": 2.22, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5872}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5665}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RePPL%3A%20Recalibrating%20Perplexity%20by%20Uncertainty%20in%20Semantic%20Propagation%0A%20%20and%20Language%20Generation%20for%20Explainable%20QA%20Hallucination%20Detection&body=Title%3A%20RePPL%3A%20Recalibrating%20Perplexity%20by%20Uncertainty%20in%20Semantic%20Propagation%0A%20%20and%20Language%20Generation%20for%20Explainable%20QA%20Hallucination%20Detection%0AAuthor%3A%20Yiming%20Huang%20and%20Junyan%20Zhang%20and%20Zihao%20Wang%20and%20Biquan%20Bie%20and%20Yunzhong%20Qiu%20and%20Yi%20R.%20Fung%20and%20Xinlei%20He%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20powerful%2C%20but%20hallucinations%20remain%0Aa%20vital%20obstacle%20to%20their%20trustworthy%20use.%20While%20previous%20works%20improved%20the%0Acapability%20of%20hallucination%20detection%20by%20measuring%20uncertainty%2C%20they%20all%20lack%0Athe%20ability%20to%20explain%20the%20provenance%20behind%20why%20hallucinations%20occur%2C%20i.e.%2C%0Awhich%20part%20of%20the%20inputs%20tends%20to%20trigger%20hallucinations.%20Recent%20works%20on%20the%0Aprompt%20attack%20indicate%20that%20uncertainty%20exists%20in%20semantic%20propagation%2C%20where%0Aattention%20mechanisms%20gradually%20fuse%20local%20token%20information%20into%20high-level%0Asemantics%20across%20layers.%20Meanwhile%2C%20uncertainty%20also%20emerges%20in%20language%0Ageneration%2C%20due%20to%20its%20probability-based%20selection%20of%20high-level%20semantics%20for%0Asampled%20generations.%20Based%20on%20that%2C%20we%20propose%20RePPL%20to%20recalibrate%20uncertainty%0Ameasurement%20by%20these%20two%20aspects%2C%20which%20dispatches%20explainable%20uncertainty%0Ascores%20to%20each%20token%20and%20aggregates%20in%20Perplexity-style%20Log-Average%20form%20as%0Atotal%20score.%20Experiments%20show%20that%20our%20method%20achieves%20the%20best%20comprehensive%0Adetection%20performance%20across%20various%20QA%20datasets%20on%20advanced%20models%20%28average%0AAUC%20of%200.833%29%2C%20and%20our%20method%20is%20capable%20of%20producing%20token-level%20uncertainty%0Ascores%20as%20explanations%20for%20the%20hallucination.%20Leveraging%20these%20scores%2C%20we%0Apreliminarily%20find%20the%20chaotic%20pattern%20of%20hallucination%20and%20showcase%20its%0Apromising%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRePPL%253A%2520Recalibrating%2520Perplexity%2520by%2520Uncertainty%2520in%2520Semantic%2520Propagation%250A%2520%2520and%2520Language%2520Generation%2520for%2520Explainable%2520QA%2520Hallucination%2520Detection%26entry.906535625%3DYiming%2520Huang%2520and%2520Junyan%2520Zhang%2520and%2520Zihao%2520Wang%2520and%2520Biquan%2520Bie%2520and%2520Yunzhong%2520Qiu%2520and%2520Yi%2520R.%2520Fung%2520and%2520Xinlei%2520He%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520powerful%252C%2520but%2520hallucinations%2520remain%250Aa%2520vital%2520obstacle%2520to%2520their%2520trustworthy%2520use.%2520While%2520previous%2520works%2520improved%2520the%250Acapability%2520of%2520hallucination%2520detection%2520by%2520measuring%2520uncertainty%252C%2520they%2520all%2520lack%250Athe%2520ability%2520to%2520explain%2520the%2520provenance%2520behind%2520why%2520hallucinations%2520occur%252C%2520i.e.%252C%250Awhich%2520part%2520of%2520the%2520inputs%2520tends%2520to%2520trigger%2520hallucinations.%2520Recent%2520works%2520on%2520the%250Aprompt%2520attack%2520indicate%2520that%2520uncertainty%2520exists%2520in%2520semantic%2520propagation%252C%2520where%250Aattention%2520mechanisms%2520gradually%2520fuse%2520local%2520token%2520information%2520into%2520high-level%250Asemantics%2520across%2520layers.%2520Meanwhile%252C%2520uncertainty%2520also%2520emerges%2520in%2520language%250Ageneration%252C%2520due%2520to%2520its%2520probability-based%2520selection%2520of%2520high-level%2520semantics%2520for%250Asampled%2520generations.%2520Based%2520on%2520that%252C%2520we%2520propose%2520RePPL%2520to%2520recalibrate%2520uncertainty%250Ameasurement%2520by%2520these%2520two%2520aspects%252C%2520which%2520dispatches%2520explainable%2520uncertainty%250Ascores%2520to%2520each%2520token%2520and%2520aggregates%2520in%2520Perplexity-style%2520Log-Average%2520form%2520as%250Atotal%2520score.%2520Experiments%2520show%2520that%2520our%2520method%2520achieves%2520the%2520best%2520comprehensive%250Adetection%2520performance%2520across%2520various%2520QA%2520datasets%2520on%2520advanced%2520models%2520%2528average%250AAUC%2520of%25200.833%2529%252C%2520and%2520our%2520method%2520is%2520capable%2520of%2520producing%2520token-level%2520uncertainty%250Ascores%2520as%2520explanations%2520for%2520the%2520hallucination.%2520Leveraging%2520these%2520scores%252C%2520we%250Apreliminarily%2520find%2520the%2520chaotic%2520pattern%2520of%2520hallucination%2520and%2520showcase%2520its%250Apromising%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RePPL%3A%20Recalibrating%20Perplexity%20by%20Uncertainty%20in%20Semantic%20Propagation%0A%20%20and%20Language%20Generation%20for%20Explainable%20QA%20Hallucination%20Detection&entry.906535625=Yiming%20Huang%20and%20Junyan%20Zhang%20and%20Zihao%20Wang%20and%20Biquan%20Bie%20and%20Yunzhong%20Qiu%20and%20Yi%20R.%20Fung%20and%20Xinlei%20He&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20powerful%2C%20but%20hallucinations%20remain%0Aa%20vital%20obstacle%20to%20their%20trustworthy%20use.%20While%20previous%20works%20improved%20the%0Acapability%20of%20hallucination%20detection%20by%20measuring%20uncertainty%2C%20they%20all%20lack%0Athe%20ability%20to%20explain%20the%20provenance%20behind%20why%20hallucinations%20occur%2C%20i.e.%2C%0Awhich%20part%20of%20the%20inputs%20tends%20to%20trigger%20hallucinations.%20Recent%20works%20on%20the%0Aprompt%20attack%20indicate%20that%20uncertainty%20exists%20in%20semantic%20propagation%2C%20where%0Aattention%20mechanisms%20gradually%20fuse%20local%20token%20information%20into%20high-level%0Asemantics%20across%20layers.%20Meanwhile%2C%20uncertainty%20also%20emerges%20in%20language%0Ageneration%2C%20due%20to%20its%20probability-based%20selection%20of%20high-level%20semantics%20for%0Asampled%20generations.%20Based%20on%20that%2C%20we%20propose%20RePPL%20to%20recalibrate%20uncertainty%0Ameasurement%20by%20these%20two%20aspects%2C%20which%20dispatches%20explainable%20uncertainty%0Ascores%20to%20each%20token%20and%20aggregates%20in%20Perplexity-style%20Log-Average%20form%20as%0Atotal%20score.%20Experiments%20show%20that%20our%20method%20achieves%20the%20best%20comprehensive%0Adetection%20performance%20across%20various%20QA%20datasets%20on%20advanced%20models%20%28average%0AAUC%20of%200.833%29%2C%20and%20our%20method%20is%20capable%20of%20producing%20token-level%20uncertainty%0Ascores%20as%20explanations%20for%20the%20hallucination.%20Leveraging%20these%20scores%2C%20we%0Apreliminarily%20find%20the%20chaotic%20pattern%20of%20hallucination%20and%20showcase%20its%0Apromising%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15386v2&entry.124074799=Read"},
{"title": "Generative Artificial Intelligence and Agents in Research and Teaching", "author": "Jussi S. Jauhiainen and Aurora Toppari", "abstract": "  This study provides a comprehensive analysis of the development, functioning,\nand application of generative artificial intelligence (GenAI) and large\nlanguage models (LLMs), with an emphasis on their implications for research and\neducation. It traces the conceptual evolution from artificial intelligence (AI)\nthrough machine learning (ML) and deep learning (DL) to transformer\narchitectures, which constitute the foundation of contemporary generative\nsystems. Technical aspects, including prompting strategies, word embeddings,\nand probabilistic sampling methods (temperature, top-k, and top-p), are\nexamined alongside the emergence of autonomous agents. These elements are\nconsidered in relation to both the opportunities they create and the\nlimitations and risks they entail.\n  The work critically evaluates the integration of GenAI across the research\nprocess, from ideation and literature review to research design, data\ncollection, analysis, interpretation, and dissemination. While particular\nattention is given to geographical research, the discussion extends to wider\nacademic contexts. A parallel strand addresses the pedagogical applications of\nGenAI, encompassing course and lesson design, teaching delivery, assessment,\nand feedback, with geography education serving as a case example.\n  Central to the analysis are the ethical, social, and environmental challenges\nposed by GenAI. Issues of bias, intellectual property, governance, and\naccountability are assessed, alongside the ecological footprint of LLMs and\nemerging technological strategies for mitigation. The concluding section\nconsiders near- and long-term futures of GenAI, including scenarios of\nsustained adoption, regulation, and potential decline. By situating GenAI\nwithin both scholarly practice and educational contexts, the study contributes\nto critical debates on its transformative potential and societal\nresponsibilities.\n", "link": "http://arxiv.org/abs/2508.16701v2", "date": "2025-08-26", "relevancy": 2.2041, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.627}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4976}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Artificial%20Intelligence%20and%20Agents%20in%20Research%20and%20Teaching&body=Title%3A%20Generative%20Artificial%20Intelligence%20and%20Agents%20in%20Research%20and%20Teaching%0AAuthor%3A%20Jussi%20S.%20Jauhiainen%20and%20Aurora%20Toppari%0AAbstract%3A%20%20%20This%20study%20provides%20a%20comprehensive%20analysis%20of%20the%20development%2C%20functioning%2C%0Aand%20application%20of%20generative%20artificial%20intelligence%20%28GenAI%29%20and%20large%0Alanguage%20models%20%28LLMs%29%2C%20with%20an%20emphasis%20on%20their%20implications%20for%20research%20and%0Aeducation.%20It%20traces%20the%20conceptual%20evolution%20from%20artificial%20intelligence%20%28AI%29%0Athrough%20machine%20learning%20%28ML%29%20and%20deep%20learning%20%28DL%29%20to%20transformer%0Aarchitectures%2C%20which%20constitute%20the%20foundation%20of%20contemporary%20generative%0Asystems.%20Technical%20aspects%2C%20including%20prompting%20strategies%2C%20word%20embeddings%2C%0Aand%20probabilistic%20sampling%20methods%20%28temperature%2C%20top-k%2C%20and%20top-p%29%2C%20are%0Aexamined%20alongside%20the%20emergence%20of%20autonomous%20agents.%20These%20elements%20are%0Aconsidered%20in%20relation%20to%20both%20the%20opportunities%20they%20create%20and%20the%0Alimitations%20and%20risks%20they%20entail.%0A%20%20The%20work%20critically%20evaluates%20the%20integration%20of%20GenAI%20across%20the%20research%0Aprocess%2C%20from%20ideation%20and%20literature%20review%20to%20research%20design%2C%20data%0Acollection%2C%20analysis%2C%20interpretation%2C%20and%20dissemination.%20While%20particular%0Aattention%20is%20given%20to%20geographical%20research%2C%20the%20discussion%20extends%20to%20wider%0Aacademic%20contexts.%20A%20parallel%20strand%20addresses%20the%20pedagogical%20applications%20of%0AGenAI%2C%20encompassing%20course%20and%20lesson%20design%2C%20teaching%20delivery%2C%20assessment%2C%0Aand%20feedback%2C%20with%20geography%20education%20serving%20as%20a%20case%20example.%0A%20%20Central%20to%20the%20analysis%20are%20the%20ethical%2C%20social%2C%20and%20environmental%20challenges%0Aposed%20by%20GenAI.%20Issues%20of%20bias%2C%20intellectual%20property%2C%20governance%2C%20and%0Aaccountability%20are%20assessed%2C%20alongside%20the%20ecological%20footprint%20of%20LLMs%20and%0Aemerging%20technological%20strategies%20for%20mitigation.%20The%20concluding%20section%0Aconsiders%20near-%20and%20long-term%20futures%20of%20GenAI%2C%20including%20scenarios%20of%0Asustained%20adoption%2C%20regulation%2C%20and%20potential%20decline.%20By%20situating%20GenAI%0Awithin%20both%20scholarly%20practice%20and%20educational%20contexts%2C%20the%20study%20contributes%0Ato%20critical%20debates%20on%20its%20transformative%20potential%20and%20societal%0Aresponsibilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Artificial%2520Intelligence%2520and%2520Agents%2520in%2520Research%2520and%2520Teaching%26entry.906535625%3DJussi%2520S.%2520Jauhiainen%2520and%2520Aurora%2520Toppari%26entry.1292438233%3D%2520%2520This%2520study%2520provides%2520a%2520comprehensive%2520analysis%2520of%2520the%2520development%252C%2520functioning%252C%250Aand%2520application%2520of%2520generative%2520artificial%2520intelligence%2520%2528GenAI%2529%2520and%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520with%2520an%2520emphasis%2520on%2520their%2520implications%2520for%2520research%2520and%250Aeducation.%2520It%2520traces%2520the%2520conceptual%2520evolution%2520from%2520artificial%2520intelligence%2520%2528AI%2529%250Athrough%2520machine%2520learning%2520%2528ML%2529%2520and%2520deep%2520learning%2520%2528DL%2529%2520to%2520transformer%250Aarchitectures%252C%2520which%2520constitute%2520the%2520foundation%2520of%2520contemporary%2520generative%250Asystems.%2520Technical%2520aspects%252C%2520including%2520prompting%2520strategies%252C%2520word%2520embeddings%252C%250Aand%2520probabilistic%2520sampling%2520methods%2520%2528temperature%252C%2520top-k%252C%2520and%2520top-p%2529%252C%2520are%250Aexamined%2520alongside%2520the%2520emergence%2520of%2520autonomous%2520agents.%2520These%2520elements%2520are%250Aconsidered%2520in%2520relation%2520to%2520both%2520the%2520opportunities%2520they%2520create%2520and%2520the%250Alimitations%2520and%2520risks%2520they%2520entail.%250A%2520%2520The%2520work%2520critically%2520evaluates%2520the%2520integration%2520of%2520GenAI%2520across%2520the%2520research%250Aprocess%252C%2520from%2520ideation%2520and%2520literature%2520review%2520to%2520research%2520design%252C%2520data%250Acollection%252C%2520analysis%252C%2520interpretation%252C%2520and%2520dissemination.%2520While%2520particular%250Aattention%2520is%2520given%2520to%2520geographical%2520research%252C%2520the%2520discussion%2520extends%2520to%2520wider%250Aacademic%2520contexts.%2520A%2520parallel%2520strand%2520addresses%2520the%2520pedagogical%2520applications%2520of%250AGenAI%252C%2520encompassing%2520course%2520and%2520lesson%2520design%252C%2520teaching%2520delivery%252C%2520assessment%252C%250Aand%2520feedback%252C%2520with%2520geography%2520education%2520serving%2520as%2520a%2520case%2520example.%250A%2520%2520Central%2520to%2520the%2520analysis%2520are%2520the%2520ethical%252C%2520social%252C%2520and%2520environmental%2520challenges%250Aposed%2520by%2520GenAI.%2520Issues%2520of%2520bias%252C%2520intellectual%2520property%252C%2520governance%252C%2520and%250Aaccountability%2520are%2520assessed%252C%2520alongside%2520the%2520ecological%2520footprint%2520of%2520LLMs%2520and%250Aemerging%2520technological%2520strategies%2520for%2520mitigation.%2520The%2520concluding%2520section%250Aconsiders%2520near-%2520and%2520long-term%2520futures%2520of%2520GenAI%252C%2520including%2520scenarios%2520of%250Asustained%2520adoption%252C%2520regulation%252C%2520and%2520potential%2520decline.%2520By%2520situating%2520GenAI%250Awithin%2520both%2520scholarly%2520practice%2520and%2520educational%2520contexts%252C%2520the%2520study%2520contributes%250Ato%2520critical%2520debates%2520on%2520its%2520transformative%2520potential%2520and%2520societal%250Aresponsibilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Artificial%20Intelligence%20and%20Agents%20in%20Research%20and%20Teaching&entry.906535625=Jussi%20S.%20Jauhiainen%20and%20Aurora%20Toppari&entry.1292438233=%20%20This%20study%20provides%20a%20comprehensive%20analysis%20of%20the%20development%2C%20functioning%2C%0Aand%20application%20of%20generative%20artificial%20intelligence%20%28GenAI%29%20and%20large%0Alanguage%20models%20%28LLMs%29%2C%20with%20an%20emphasis%20on%20their%20implications%20for%20research%20and%0Aeducation.%20It%20traces%20the%20conceptual%20evolution%20from%20artificial%20intelligence%20%28AI%29%0Athrough%20machine%20learning%20%28ML%29%20and%20deep%20learning%20%28DL%29%20to%20transformer%0Aarchitectures%2C%20which%20constitute%20the%20foundation%20of%20contemporary%20generative%0Asystems.%20Technical%20aspects%2C%20including%20prompting%20strategies%2C%20word%20embeddings%2C%0Aand%20probabilistic%20sampling%20methods%20%28temperature%2C%20top-k%2C%20and%20top-p%29%2C%20are%0Aexamined%20alongside%20the%20emergence%20of%20autonomous%20agents.%20These%20elements%20are%0Aconsidered%20in%20relation%20to%20both%20the%20opportunities%20they%20create%20and%20the%0Alimitations%20and%20risks%20they%20entail.%0A%20%20The%20work%20critically%20evaluates%20the%20integration%20of%20GenAI%20across%20the%20research%0Aprocess%2C%20from%20ideation%20and%20literature%20review%20to%20research%20design%2C%20data%0Acollection%2C%20analysis%2C%20interpretation%2C%20and%20dissemination.%20While%20particular%0Aattention%20is%20given%20to%20geographical%20research%2C%20the%20discussion%20extends%20to%20wider%0Aacademic%20contexts.%20A%20parallel%20strand%20addresses%20the%20pedagogical%20applications%20of%0AGenAI%2C%20encompassing%20course%20and%20lesson%20design%2C%20teaching%20delivery%2C%20assessment%2C%0Aand%20feedback%2C%20with%20geography%20education%20serving%20as%20a%20case%20example.%0A%20%20Central%20to%20the%20analysis%20are%20the%20ethical%2C%20social%2C%20and%20environmental%20challenges%0Aposed%20by%20GenAI.%20Issues%20of%20bias%2C%20intellectual%20property%2C%20governance%2C%20and%0Aaccountability%20are%20assessed%2C%20alongside%20the%20ecological%20footprint%20of%20LLMs%20and%0Aemerging%20technological%20strategies%20for%20mitigation.%20The%20concluding%20section%0Aconsiders%20near-%20and%20long-term%20futures%20of%20GenAI%2C%20including%20scenarios%20of%0Asustained%20adoption%2C%20regulation%2C%20and%20potential%20decline.%20By%20situating%20GenAI%0Awithin%20both%20scholarly%20practice%20and%20educational%20contexts%2C%20the%20study%20contributes%0Ato%20critical%20debates%20on%20its%20transformative%20potential%20and%20societal%0Aresponsibilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16701v2&entry.124074799=Read"},
{"title": "QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End\n  Reinforcement Learning", "author": "Allen Wang and Gavin Tao", "abstract": "  We address vision-guided quadruped motion control with reinforcement learning\n(RL) and highlight the necessity of combining proprioception with vision for\nrobust control. We propose QuadKAN, a spline-parameterized cross-modal policy\ninstantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates\na spline encoder for proprioception and a spline fusion head for\nproprioception-vision inputs. This structured function class aligns the\nstate-to-action mapping with the piecewise-smooth nature of gait, improving\nsample efficiency, reducing action jitter and energy consumption, and providing\ninterpretable posture-action sensitivities. We adopt Multi-Modal Delay\nRandomization (MMDR) and perform end-to-end training with Proximal Policy\nOptimization (PPO). Evaluations across diverse terrains, including both even\nand uneven surfaces and scenarios with static or dynamic obstacles, demonstrate\nthat QuadKAN achieves consistently higher returns, greater distances, and fewer\ncollisions than state-of-the-art (SOTA) baselines. These results show that\nspline-parameterized policies offer a simple, effective, and interpretable\nalternative for robust vision-guided locomotion. A repository will be made\navailable upon acceptance.\n", "link": "http://arxiv.org/abs/2508.19153v1", "date": "2025-08-26", "relevancy": 2.2034, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6184}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5471}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuadKAN%3A%20KAN-Enhanced%20Quadruped%20Motion%20Control%20via%20End-to-End%0A%20%20Reinforcement%20Learning&body=Title%3A%20QuadKAN%3A%20KAN-Enhanced%20Quadruped%20Motion%20Control%20via%20End-to-End%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Allen%20Wang%20and%20Gavin%20Tao%0AAbstract%3A%20%20%20We%20address%20vision-guided%20quadruped%20motion%20control%20with%20reinforcement%20learning%0A%28RL%29%20and%20highlight%20the%20necessity%20of%20combining%20proprioception%20with%20vision%20for%0Arobust%20control.%20We%20propose%20QuadKAN%2C%20a%20spline-parameterized%20cross-modal%20policy%0Ainstantiated%20with%20Kolmogorov-Arnold%20Networks%20%28KANs%29.%20The%20framework%20incorporates%0Aa%20spline%20encoder%20for%20proprioception%20and%20a%20spline%20fusion%20head%20for%0Aproprioception-vision%20inputs.%20This%20structured%20function%20class%20aligns%20the%0Astate-to-action%20mapping%20with%20the%20piecewise-smooth%20nature%20of%20gait%2C%20improving%0Asample%20efficiency%2C%20reducing%20action%20jitter%20and%20energy%20consumption%2C%20and%20providing%0Ainterpretable%20posture-action%20sensitivities.%20We%20adopt%20Multi-Modal%20Delay%0ARandomization%20%28MMDR%29%20and%20perform%20end-to-end%20training%20with%20Proximal%20Policy%0AOptimization%20%28PPO%29.%20Evaluations%20across%20diverse%20terrains%2C%20including%20both%20even%0Aand%20uneven%20surfaces%20and%20scenarios%20with%20static%20or%20dynamic%20obstacles%2C%20demonstrate%0Athat%20QuadKAN%20achieves%20consistently%20higher%20returns%2C%20greater%20distances%2C%20and%20fewer%0Acollisions%20than%20state-of-the-art%20%28SOTA%29%20baselines.%20These%20results%20show%20that%0Aspline-parameterized%20policies%20offer%20a%20simple%2C%20effective%2C%20and%20interpretable%0Aalternative%20for%20robust%20vision-guided%20locomotion.%20A%20repository%20will%20be%20made%0Aavailable%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuadKAN%253A%2520KAN-Enhanced%2520Quadruped%2520Motion%2520Control%2520via%2520End-to-End%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DAllen%2520Wang%2520and%2520Gavin%2520Tao%26entry.1292438233%3D%2520%2520We%2520address%2520vision-guided%2520quadruped%2520motion%2520control%2520with%2520reinforcement%2520learning%250A%2528RL%2529%2520and%2520highlight%2520the%2520necessity%2520of%2520combining%2520proprioception%2520with%2520vision%2520for%250Arobust%2520control.%2520We%2520propose%2520QuadKAN%252C%2520a%2520spline-parameterized%2520cross-modal%2520policy%250Ainstantiated%2520with%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529.%2520The%2520framework%2520incorporates%250Aa%2520spline%2520encoder%2520for%2520proprioception%2520and%2520a%2520spline%2520fusion%2520head%2520for%250Aproprioception-vision%2520inputs.%2520This%2520structured%2520function%2520class%2520aligns%2520the%250Astate-to-action%2520mapping%2520with%2520the%2520piecewise-smooth%2520nature%2520of%2520gait%252C%2520improving%250Asample%2520efficiency%252C%2520reducing%2520action%2520jitter%2520and%2520energy%2520consumption%252C%2520and%2520providing%250Ainterpretable%2520posture-action%2520sensitivities.%2520We%2520adopt%2520Multi-Modal%2520Delay%250ARandomization%2520%2528MMDR%2529%2520and%2520perform%2520end-to-end%2520training%2520with%2520Proximal%2520Policy%250AOptimization%2520%2528PPO%2529.%2520Evaluations%2520across%2520diverse%2520terrains%252C%2520including%2520both%2520even%250Aand%2520uneven%2520surfaces%2520and%2520scenarios%2520with%2520static%2520or%2520dynamic%2520obstacles%252C%2520demonstrate%250Athat%2520QuadKAN%2520achieves%2520consistently%2520higher%2520returns%252C%2520greater%2520distances%252C%2520and%2520fewer%250Acollisions%2520than%2520state-of-the-art%2520%2528SOTA%2529%2520baselines.%2520These%2520results%2520show%2520that%250Aspline-parameterized%2520policies%2520offer%2520a%2520simple%252C%2520effective%252C%2520and%2520interpretable%250Aalternative%2520for%2520robust%2520vision-guided%2520locomotion.%2520A%2520repository%2520will%2520be%2520made%250Aavailable%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuadKAN%3A%20KAN-Enhanced%20Quadruped%20Motion%20Control%20via%20End-to-End%0A%20%20Reinforcement%20Learning&entry.906535625=Allen%20Wang%20and%20Gavin%20Tao&entry.1292438233=%20%20We%20address%20vision-guided%20quadruped%20motion%20control%20with%20reinforcement%20learning%0A%28RL%29%20and%20highlight%20the%20necessity%20of%20combining%20proprioception%20with%20vision%20for%0Arobust%20control.%20We%20propose%20QuadKAN%2C%20a%20spline-parameterized%20cross-modal%20policy%0Ainstantiated%20with%20Kolmogorov-Arnold%20Networks%20%28KANs%29.%20The%20framework%20incorporates%0Aa%20spline%20encoder%20for%20proprioception%20and%20a%20spline%20fusion%20head%20for%0Aproprioception-vision%20inputs.%20This%20structured%20function%20class%20aligns%20the%0Astate-to-action%20mapping%20with%20the%20piecewise-smooth%20nature%20of%20gait%2C%20improving%0Asample%20efficiency%2C%20reducing%20action%20jitter%20and%20energy%20consumption%2C%20and%20providing%0Ainterpretable%20posture-action%20sensitivities.%20We%20adopt%20Multi-Modal%20Delay%0ARandomization%20%28MMDR%29%20and%20perform%20end-to-end%20training%20with%20Proximal%20Policy%0AOptimization%20%28PPO%29.%20Evaluations%20across%20diverse%20terrains%2C%20including%20both%20even%0Aand%20uneven%20surfaces%20and%20scenarios%20with%20static%20or%20dynamic%20obstacles%2C%20demonstrate%0Athat%20QuadKAN%20achieves%20consistently%20higher%20returns%2C%20greater%20distances%2C%20and%20fewer%0Acollisions%20than%20state-of-the-art%20%28SOTA%29%20baselines.%20These%20results%20show%20that%0Aspline-parameterized%20policies%20offer%20a%20simple%2C%20effective%2C%20and%20interpretable%0Aalternative%20for%20robust%20vision-guided%20locomotion.%20A%20repository%20will%20be%20made%0Aavailable%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19153v1&entry.124074799=Read"},
{"title": "Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and\n  Semantic Graph Integration", "author": "Shaoguang Wang and Ziyang Chen and Yijie Xu and Weiyu Guo and Hui Xiong", "abstract": "  The practical application of Multimodal Large Language Models (MLLMs) to\nVideo Question Answering (Video-QA) is severely hindered by the high token cost\nof processing numerous video frames. While increasing the number of sampled\nframes is a common strategy, we observe a \"less is more\" phenomenon where\nexcessive frames can paradoxically degrade performance due to context dilution.\nConcurrently, state-of-the-art keyframe selection methods, while effective,\nstill yield significant temporal redundancy, which we term 'visual echoes'. To\naddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel\npost-processing method that intelligently prunes the selected keyframes. AFP\nemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 and\nCLIP feature space to identify and merge these echoes into single\nrepresentatives. To compensate for information loss, we then introduce a\nlightweight, text-based semantic graph that provides critical context with\nminimal token overhead. Conducting extensive experiments on the LongVideoBench\nand VideoMME benchmarks across multiple leading MLLMs, our full approach\ndemonstrates a drastic reduction in required frames by up to 86.9% and total\ninput tokens by up to 83.2%. Crucially, by providing a concise, high-quality\nset of frames, our method not only enhances efficiency but often improves\naccuracy over baselines that use more frames. The code will be released upon\npublication.\n", "link": "http://arxiv.org/abs/2508.03337v4", "date": "2025-08-26", "relevancy": 2.1963, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.564}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Token-Efficient%20Video-QA%20via%20Adaptive%20Frame-Pruning%20and%0A%20%20Semantic%20Graph%20Integration&body=Title%3A%20Less%20is%20More%3A%20Token-Efficient%20Video-QA%20via%20Adaptive%20Frame-Pruning%20and%0A%20%20Semantic%20Graph%20Integration%0AAuthor%3A%20Shaoguang%20Wang%20and%20Ziyang%20Chen%20and%20Yijie%20Xu%20and%20Weiyu%20Guo%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20The%20practical%20application%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%0AVideo%20Question%20Answering%20%28Video-QA%29%20is%20severely%20hindered%20by%20the%20high%20token%20cost%0Aof%20processing%20numerous%20video%20frames.%20While%20increasing%20the%20number%20of%20sampled%0Aframes%20is%20a%20common%20strategy%2C%20we%20observe%20a%20%22less%20is%20more%22%20phenomenon%20where%0Aexcessive%20frames%20can%20paradoxically%20degrade%20performance%20due%20to%20context%20dilution.%0AConcurrently%2C%20state-of-the-art%20keyframe%20selection%20methods%2C%20while%20effective%2C%0Astill%20yield%20significant%20temporal%20redundancy%2C%20which%20we%20term%20%27visual%20echoes%27.%20To%0Aaddress%20these%20dual%20challenges%2C%20we%20propose%20Adaptive%20Frame-Pruning%20%28AFP%29%2C%20a%20novel%0Apost-processing%20method%20that%20intelligently%20prunes%20the%20selected%20keyframes.%20AFP%0Aemploys%20an%20adaptive%20hierarchical%20clustering%20algorithm%20on%20a%20fused%20ResNet-50%20and%0ACLIP%20feature%20space%20to%20identify%20and%20merge%20these%20echoes%20into%20single%0Arepresentatives.%20To%20compensate%20for%20information%20loss%2C%20we%20then%20introduce%20a%0Alightweight%2C%20text-based%20semantic%20graph%20that%20provides%20critical%20context%20with%0Aminimal%20token%20overhead.%20Conducting%20extensive%20experiments%20on%20the%20LongVideoBench%0Aand%20VideoMME%20benchmarks%20across%20multiple%20leading%20MLLMs%2C%20our%20full%20approach%0Ademonstrates%20a%20drastic%20reduction%20in%20required%20frames%20by%20up%20to%2086.9%25%20and%20total%0Ainput%20tokens%20by%20up%20to%2083.2%25.%20Crucially%2C%20by%20providing%20a%20concise%2C%20high-quality%0Aset%20of%20frames%2C%20our%20method%20not%20only%20enhances%20efficiency%20but%20often%20improves%0Aaccuracy%20over%20baselines%20that%20use%20more%20frames.%20The%20code%20will%20be%20released%20upon%0Apublication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03337v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Token-Efficient%2520Video-QA%2520via%2520Adaptive%2520Frame-Pruning%2520and%250A%2520%2520Semantic%2520Graph%2520Integration%26entry.906535625%3DShaoguang%2520Wang%2520and%2520Ziyang%2520Chen%2520and%2520Yijie%2520Xu%2520and%2520Weiyu%2520Guo%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520The%2520practical%2520application%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%250AVideo%2520Question%2520Answering%2520%2528Video-QA%2529%2520is%2520severely%2520hindered%2520by%2520the%2520high%2520token%2520cost%250Aof%2520processing%2520numerous%2520video%2520frames.%2520While%2520increasing%2520the%2520number%2520of%2520sampled%250Aframes%2520is%2520a%2520common%2520strategy%252C%2520we%2520observe%2520a%2520%2522less%2520is%2520more%2522%2520phenomenon%2520where%250Aexcessive%2520frames%2520can%2520paradoxically%2520degrade%2520performance%2520due%2520to%2520context%2520dilution.%250AConcurrently%252C%2520state-of-the-art%2520keyframe%2520selection%2520methods%252C%2520while%2520effective%252C%250Astill%2520yield%2520significant%2520temporal%2520redundancy%252C%2520which%2520we%2520term%2520%2527visual%2520echoes%2527.%2520To%250Aaddress%2520these%2520dual%2520challenges%252C%2520we%2520propose%2520Adaptive%2520Frame-Pruning%2520%2528AFP%2529%252C%2520a%2520novel%250Apost-processing%2520method%2520that%2520intelligently%2520prunes%2520the%2520selected%2520keyframes.%2520AFP%250Aemploys%2520an%2520adaptive%2520hierarchical%2520clustering%2520algorithm%2520on%2520a%2520fused%2520ResNet-50%2520and%250ACLIP%2520feature%2520space%2520to%2520identify%2520and%2520merge%2520these%2520echoes%2520into%2520single%250Arepresentatives.%2520To%2520compensate%2520for%2520information%2520loss%252C%2520we%2520then%2520introduce%2520a%250Alightweight%252C%2520text-based%2520semantic%2520graph%2520that%2520provides%2520critical%2520context%2520with%250Aminimal%2520token%2520overhead.%2520Conducting%2520extensive%2520experiments%2520on%2520the%2520LongVideoBench%250Aand%2520VideoMME%2520benchmarks%2520across%2520multiple%2520leading%2520MLLMs%252C%2520our%2520full%2520approach%250Ademonstrates%2520a%2520drastic%2520reduction%2520in%2520required%2520frames%2520by%2520up%2520to%252086.9%2525%2520and%2520total%250Ainput%2520tokens%2520by%2520up%2520to%252083.2%2525.%2520Crucially%252C%2520by%2520providing%2520a%2520concise%252C%2520high-quality%250Aset%2520of%2520frames%252C%2520our%2520method%2520not%2520only%2520enhances%2520efficiency%2520but%2520often%2520improves%250Aaccuracy%2520over%2520baselines%2520that%2520use%2520more%2520frames.%2520The%2520code%2520will%2520be%2520released%2520upon%250Apublication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03337v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Token-Efficient%20Video-QA%20via%20Adaptive%20Frame-Pruning%20and%0A%20%20Semantic%20Graph%20Integration&entry.906535625=Shaoguang%20Wang%20and%20Ziyang%20Chen%20and%20Yijie%20Xu%20and%20Weiyu%20Guo%20and%20Hui%20Xiong&entry.1292438233=%20%20The%20practical%20application%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%0AVideo%20Question%20Answering%20%28Video-QA%29%20is%20severely%20hindered%20by%20the%20high%20token%20cost%0Aof%20processing%20numerous%20video%20frames.%20While%20increasing%20the%20number%20of%20sampled%0Aframes%20is%20a%20common%20strategy%2C%20we%20observe%20a%20%22less%20is%20more%22%20phenomenon%20where%0Aexcessive%20frames%20can%20paradoxically%20degrade%20performance%20due%20to%20context%20dilution.%0AConcurrently%2C%20state-of-the-art%20keyframe%20selection%20methods%2C%20while%20effective%2C%0Astill%20yield%20significant%20temporal%20redundancy%2C%20which%20we%20term%20%27visual%20echoes%27.%20To%0Aaddress%20these%20dual%20challenges%2C%20we%20propose%20Adaptive%20Frame-Pruning%20%28AFP%29%2C%20a%20novel%0Apost-processing%20method%20that%20intelligently%20prunes%20the%20selected%20keyframes.%20AFP%0Aemploys%20an%20adaptive%20hierarchical%20clustering%20algorithm%20on%20a%20fused%20ResNet-50%20and%0ACLIP%20feature%20space%20to%20identify%20and%20merge%20these%20echoes%20into%20single%0Arepresentatives.%20To%20compensate%20for%20information%20loss%2C%20we%20then%20introduce%20a%0Alightweight%2C%20text-based%20semantic%20graph%20that%20provides%20critical%20context%20with%0Aminimal%20token%20overhead.%20Conducting%20extensive%20experiments%20on%20the%20LongVideoBench%0Aand%20VideoMME%20benchmarks%20across%20multiple%20leading%20MLLMs%2C%20our%20full%20approach%0Ademonstrates%20a%20drastic%20reduction%20in%20required%20frames%20by%20up%20to%2086.9%25%20and%20total%0Ainput%20tokens%20by%20up%20to%2083.2%25.%20Crucially%2C%20by%20providing%20a%20concise%2C%20high-quality%0Aset%20of%20frames%2C%20our%20method%20not%20only%20enhances%20efficiency%20but%20often%20improves%0Aaccuracy%20over%20baselines%20that%20use%20more%20frames.%20The%20code%20will%20be%20released%20upon%0Apublication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03337v4&entry.124074799=Read"},
{"title": "Image Coding for Machines via Feature-Preserving Rate-Distortion\n  Optimization", "author": "Samuel Fern\u00e1ndez-Mendui\u00f1a and Eduardo Pavez and Antonio Ortega", "abstract": "  Many images and videos are primarily processed by computer vision algorithms,\ninvolving only occasional human inspection. When this content requires\ncompression before processing, e.g., in distributed applications, coding\nmethods must optimize for both visual quality and downstream task performance.\nWe first show theoretically that an approach to reduce the effect of\ncompression for a given task loss is to perform rate-distortion optimization\n(RDO) using the distance between features, obtained from the original and the\ndecoded images, as a distortion metric. However, optimizing directly such a\nrate-distortion objective is computationally impractical because it requires\niteratively encoding and decoding the entire image-plus feature evaluation-for\neach possible coding configuration. We address this problem by simplifying the\nRDO formulation to make the distortion term computable using block-based\nencoders. We first apply Taylor's expansion to the feature extractor, recasting\nthe feature distance as a quadratic metric involving the Jacobian matrix of the\nneural network. Then, we replace the linearized metric with a block-wise\napproximation, which we call input-dependent squared error (IDSE). To make the\nmetric computable, we approximate IDSE using sketches of the Jacobian. The\nresulting loss can be evaluated block-wise in the transform domain and combined\nwith the sum of squared errors (SSE) to address both visual quality and\ncomputer vision performance. Simulations with AVC and HEVC across multiple\nfeature extractors and downstream networks show up to 17 % bit-rate savings for\nthe same task accuracy compared to RDO based on SSE, with no decoder complexity\noverhead and a small (7.86 %) encoder complexity increase.\n", "link": "http://arxiv.org/abs/2504.02216v2", "date": "2025-08-26", "relevancy": 2.1888, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5749}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5437}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Coding%20for%20Machines%20via%20Feature-Preserving%20Rate-Distortion%0A%20%20Optimization&body=Title%3A%20Image%20Coding%20for%20Machines%20via%20Feature-Preserving%20Rate-Distortion%0A%20%20Optimization%0AAuthor%3A%20Samuel%20Fern%C3%A1ndez-Mendui%C3%B1a%20and%20Eduardo%20Pavez%20and%20Antonio%20Ortega%0AAbstract%3A%20%20%20Many%20images%20and%20videos%20are%20primarily%20processed%20by%20computer%20vision%20algorithms%2C%0Ainvolving%20only%20occasional%20human%20inspection.%20When%20this%20content%20requires%0Acompression%20before%20processing%2C%20e.g.%2C%20in%20distributed%20applications%2C%20coding%0Amethods%20must%20optimize%20for%20both%20visual%20quality%20and%20downstream%20task%20performance.%0AWe%20first%20show%20theoretically%20that%20an%20approach%20to%20reduce%20the%20effect%20of%0Acompression%20for%20a%20given%20task%20loss%20is%20to%20perform%20rate-distortion%20optimization%0A%28RDO%29%20using%20the%20distance%20between%20features%2C%20obtained%20from%20the%20original%20and%20the%0Adecoded%20images%2C%20as%20a%20distortion%20metric.%20However%2C%20optimizing%20directly%20such%20a%0Arate-distortion%20objective%20is%20computationally%20impractical%20because%20it%20requires%0Aiteratively%20encoding%20and%20decoding%20the%20entire%20image-plus%20feature%20evaluation-for%0Aeach%20possible%20coding%20configuration.%20We%20address%20this%20problem%20by%20simplifying%20the%0ARDO%20formulation%20to%20make%20the%20distortion%20term%20computable%20using%20block-based%0Aencoders.%20We%20first%20apply%20Taylor%27s%20expansion%20to%20the%20feature%20extractor%2C%20recasting%0Athe%20feature%20distance%20as%20a%20quadratic%20metric%20involving%20the%20Jacobian%20matrix%20of%20the%0Aneural%20network.%20Then%2C%20we%20replace%20the%20linearized%20metric%20with%20a%20block-wise%0Aapproximation%2C%20which%20we%20call%20input-dependent%20squared%20error%20%28IDSE%29.%20To%20make%20the%0Ametric%20computable%2C%20we%20approximate%20IDSE%20using%20sketches%20of%20the%20Jacobian.%20The%0Aresulting%20loss%20can%20be%20evaluated%20block-wise%20in%20the%20transform%20domain%20and%20combined%0Awith%20the%20sum%20of%20squared%20errors%20%28SSE%29%20to%20address%20both%20visual%20quality%20and%0Acomputer%20vision%20performance.%20Simulations%20with%20AVC%20and%20HEVC%20across%20multiple%0Afeature%20extractors%20and%20downstream%20networks%20show%20up%20to%2017%20%25%20bit-rate%20savings%20for%0Athe%20same%20task%20accuracy%20compared%20to%20RDO%20based%20on%20SSE%2C%20with%20no%20decoder%20complexity%0Aoverhead%20and%20a%20small%20%287.86%20%25%29%20encoder%20complexity%20increase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Coding%2520for%2520Machines%2520via%2520Feature-Preserving%2520Rate-Distortion%250A%2520%2520Optimization%26entry.906535625%3DSamuel%2520Fern%25C3%25A1ndez-Mendui%25C3%25B1a%2520and%2520Eduardo%2520Pavez%2520and%2520Antonio%2520Ortega%26entry.1292438233%3D%2520%2520Many%2520images%2520and%2520videos%2520are%2520primarily%2520processed%2520by%2520computer%2520vision%2520algorithms%252C%250Ainvolving%2520only%2520occasional%2520human%2520inspection.%2520When%2520this%2520content%2520requires%250Acompression%2520before%2520processing%252C%2520e.g.%252C%2520in%2520distributed%2520applications%252C%2520coding%250Amethods%2520must%2520optimize%2520for%2520both%2520visual%2520quality%2520and%2520downstream%2520task%2520performance.%250AWe%2520first%2520show%2520theoretically%2520that%2520an%2520approach%2520to%2520reduce%2520the%2520effect%2520of%250Acompression%2520for%2520a%2520given%2520task%2520loss%2520is%2520to%2520perform%2520rate-distortion%2520optimization%250A%2528RDO%2529%2520using%2520the%2520distance%2520between%2520features%252C%2520obtained%2520from%2520the%2520original%2520and%2520the%250Adecoded%2520images%252C%2520as%2520a%2520distortion%2520metric.%2520However%252C%2520optimizing%2520directly%2520such%2520a%250Arate-distortion%2520objective%2520is%2520computationally%2520impractical%2520because%2520it%2520requires%250Aiteratively%2520encoding%2520and%2520decoding%2520the%2520entire%2520image-plus%2520feature%2520evaluation-for%250Aeach%2520possible%2520coding%2520configuration.%2520We%2520address%2520this%2520problem%2520by%2520simplifying%2520the%250ARDO%2520formulation%2520to%2520make%2520the%2520distortion%2520term%2520computable%2520using%2520block-based%250Aencoders.%2520We%2520first%2520apply%2520Taylor%2527s%2520expansion%2520to%2520the%2520feature%2520extractor%252C%2520recasting%250Athe%2520feature%2520distance%2520as%2520a%2520quadratic%2520metric%2520involving%2520the%2520Jacobian%2520matrix%2520of%2520the%250Aneural%2520network.%2520Then%252C%2520we%2520replace%2520the%2520linearized%2520metric%2520with%2520a%2520block-wise%250Aapproximation%252C%2520which%2520we%2520call%2520input-dependent%2520squared%2520error%2520%2528IDSE%2529.%2520To%2520make%2520the%250Ametric%2520computable%252C%2520we%2520approximate%2520IDSE%2520using%2520sketches%2520of%2520the%2520Jacobian.%2520The%250Aresulting%2520loss%2520can%2520be%2520evaluated%2520block-wise%2520in%2520the%2520transform%2520domain%2520and%2520combined%250Awith%2520the%2520sum%2520of%2520squared%2520errors%2520%2528SSE%2529%2520to%2520address%2520both%2520visual%2520quality%2520and%250Acomputer%2520vision%2520performance.%2520Simulations%2520with%2520AVC%2520and%2520HEVC%2520across%2520multiple%250Afeature%2520extractors%2520and%2520downstream%2520networks%2520show%2520up%2520to%252017%2520%2525%2520bit-rate%2520savings%2520for%250Athe%2520same%2520task%2520accuracy%2520compared%2520to%2520RDO%2520based%2520on%2520SSE%252C%2520with%2520no%2520decoder%2520complexity%250Aoverhead%2520and%2520a%2520small%2520%25287.86%2520%2525%2529%2520encoder%2520complexity%2520increase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Coding%20for%20Machines%20via%20Feature-Preserving%20Rate-Distortion%0A%20%20Optimization&entry.906535625=Samuel%20Fern%C3%A1ndez-Mendui%C3%B1a%20and%20Eduardo%20Pavez%20and%20Antonio%20Ortega&entry.1292438233=%20%20Many%20images%20and%20videos%20are%20primarily%20processed%20by%20computer%20vision%20algorithms%2C%0Ainvolving%20only%20occasional%20human%20inspection.%20When%20this%20content%20requires%0Acompression%20before%20processing%2C%20e.g.%2C%20in%20distributed%20applications%2C%20coding%0Amethods%20must%20optimize%20for%20both%20visual%20quality%20and%20downstream%20task%20performance.%0AWe%20first%20show%20theoretically%20that%20an%20approach%20to%20reduce%20the%20effect%20of%0Acompression%20for%20a%20given%20task%20loss%20is%20to%20perform%20rate-distortion%20optimization%0A%28RDO%29%20using%20the%20distance%20between%20features%2C%20obtained%20from%20the%20original%20and%20the%0Adecoded%20images%2C%20as%20a%20distortion%20metric.%20However%2C%20optimizing%20directly%20such%20a%0Arate-distortion%20objective%20is%20computationally%20impractical%20because%20it%20requires%0Aiteratively%20encoding%20and%20decoding%20the%20entire%20image-plus%20feature%20evaluation-for%0Aeach%20possible%20coding%20configuration.%20We%20address%20this%20problem%20by%20simplifying%20the%0ARDO%20formulation%20to%20make%20the%20distortion%20term%20computable%20using%20block-based%0Aencoders.%20We%20first%20apply%20Taylor%27s%20expansion%20to%20the%20feature%20extractor%2C%20recasting%0Athe%20feature%20distance%20as%20a%20quadratic%20metric%20involving%20the%20Jacobian%20matrix%20of%20the%0Aneural%20network.%20Then%2C%20we%20replace%20the%20linearized%20metric%20with%20a%20block-wise%0Aapproximation%2C%20which%20we%20call%20input-dependent%20squared%20error%20%28IDSE%29.%20To%20make%20the%0Ametric%20computable%2C%20we%20approximate%20IDSE%20using%20sketches%20of%20the%20Jacobian.%20The%0Aresulting%20loss%20can%20be%20evaluated%20block-wise%20in%20the%20transform%20domain%20and%20combined%0Awith%20the%20sum%20of%20squared%20errors%20%28SSE%29%20to%20address%20both%20visual%20quality%20and%0Acomputer%20vision%20performance.%20Simulations%20with%20AVC%20and%20HEVC%20across%20multiple%0Afeature%20extractors%20and%20downstream%20networks%20show%20up%20to%2017%20%25%20bit-rate%20savings%20for%0Athe%20same%20task%20accuracy%20compared%20to%20RDO%20based%20on%20SSE%2C%20with%20no%20decoder%20complexity%0Aoverhead%20and%20a%20small%20%287.86%20%25%29%20encoder%20complexity%20increase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02216v2&entry.124074799=Read"},
{"title": "OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event\n  Grounding", "author": "Hieu Nguyen and Phuc-Tan Nguyen and Thien-Phuc Tran and Minh-Quang Nguyen and Tam V. Nguyen and Minh-Triet Tran and Trung-Nghia Le", "abstract": "  We introduce OpenEvents V1a large-scale benchmark dataset designed to advance\nevent-centric vision-language understanding. Unlike conventional image\ncaptioning and retrieval datasets that focus on surface-level descriptions,\nOpenEvents V1 dataset emphasizes contextual and temporal grounding through\nthree primary tasks: (1) generating rich, event-aware image captions, (2)\nretrieving event-relevant news articles from image queries, and (3) retrieving\nevent-relevant images from narrative-style textual queries. The dataset\ncomprises over 200,000 news articles and 400,000 associated images sourced from\nCNN and The Guardian, spanning diverse domains and time periods. We provide\nextensive baseline results and standardized evaluation protocols for all tasks.\nOpenEvents V1 establishes a robust foundation for developing multimodal AI\nsystems capable of deep reasoning over complex real-world events. The dataset\nis publicly available at https://ltnghia.github.io/eventa/openevents-v1.\n", "link": "http://arxiv.org/abs/2506.18372v2", "date": "2025-08-26", "relevancy": 2.1805, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenEvents%20V1%3A%20Large-Scale%20Benchmark%20Dataset%20for%20Multimodal%20Event%0A%20%20Grounding&body=Title%3A%20OpenEvents%20V1%3A%20Large-Scale%20Benchmark%20Dataset%20for%20Multimodal%20Event%0A%20%20Grounding%0AAuthor%3A%20Hieu%20Nguyen%20and%20Phuc-Tan%20Nguyen%20and%20Thien-Phuc%20Tran%20and%20Minh-Quang%20Nguyen%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Trung-Nghia%20Le%0AAbstract%3A%20%20%20We%20introduce%20OpenEvents%20V1a%20large-scale%20benchmark%20dataset%20designed%20to%20advance%0Aevent-centric%20vision-language%20understanding.%20Unlike%20conventional%20image%0Acaptioning%20and%20retrieval%20datasets%20that%20focus%20on%20surface-level%20descriptions%2C%0AOpenEvents%20V1%20dataset%20emphasizes%20contextual%20and%20temporal%20grounding%20through%0Athree%20primary%20tasks%3A%20%281%29%20generating%20rich%2C%20event-aware%20image%20captions%2C%20%282%29%0Aretrieving%20event-relevant%20news%20articles%20from%20image%20queries%2C%20and%20%283%29%20retrieving%0Aevent-relevant%20images%20from%20narrative-style%20textual%20queries.%20The%20dataset%0Acomprises%20over%20200%2C000%20news%20articles%20and%20400%2C000%20associated%20images%20sourced%20from%0ACNN%20and%20The%20Guardian%2C%20spanning%20diverse%20domains%20and%20time%20periods.%20We%20provide%0Aextensive%20baseline%20results%20and%20standardized%20evaluation%20protocols%20for%20all%20tasks.%0AOpenEvents%20V1%20establishes%20a%20robust%20foundation%20for%20developing%20multimodal%20AI%0Asystems%20capable%20of%20deep%20reasoning%20over%20complex%20real-world%20events.%20The%20dataset%0Ais%20publicly%20available%20at%20https%3A//ltnghia.github.io/eventa/openevents-v1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenEvents%2520V1%253A%2520Large-Scale%2520Benchmark%2520Dataset%2520for%2520Multimodal%2520Event%250A%2520%2520Grounding%26entry.906535625%3DHieu%2520Nguyen%2520and%2520Phuc-Tan%2520Nguyen%2520and%2520Thien-Phuc%2520Tran%2520and%2520Minh-Quang%2520Nguyen%2520and%2520Tam%2520V.%2520Nguyen%2520and%2520Minh-Triet%2520Tran%2520and%2520Trung-Nghia%2520Le%26entry.1292438233%3D%2520%2520We%2520introduce%2520OpenEvents%2520V1a%2520large-scale%2520benchmark%2520dataset%2520designed%2520to%2520advance%250Aevent-centric%2520vision-language%2520understanding.%2520Unlike%2520conventional%2520image%250Acaptioning%2520and%2520retrieval%2520datasets%2520that%2520focus%2520on%2520surface-level%2520descriptions%252C%250AOpenEvents%2520V1%2520dataset%2520emphasizes%2520contextual%2520and%2520temporal%2520grounding%2520through%250Athree%2520primary%2520tasks%253A%2520%25281%2529%2520generating%2520rich%252C%2520event-aware%2520image%2520captions%252C%2520%25282%2529%250Aretrieving%2520event-relevant%2520news%2520articles%2520from%2520image%2520queries%252C%2520and%2520%25283%2529%2520retrieving%250Aevent-relevant%2520images%2520from%2520narrative-style%2520textual%2520queries.%2520The%2520dataset%250Acomprises%2520over%2520200%252C000%2520news%2520articles%2520and%2520400%252C000%2520associated%2520images%2520sourced%2520from%250ACNN%2520and%2520The%2520Guardian%252C%2520spanning%2520diverse%2520domains%2520and%2520time%2520periods.%2520We%2520provide%250Aextensive%2520baseline%2520results%2520and%2520standardized%2520evaluation%2520protocols%2520for%2520all%2520tasks.%250AOpenEvents%2520V1%2520establishes%2520a%2520robust%2520foundation%2520for%2520developing%2520multimodal%2520AI%250Asystems%2520capable%2520of%2520deep%2520reasoning%2520over%2520complex%2520real-world%2520events.%2520The%2520dataset%250Ais%2520publicly%2520available%2520at%2520https%253A//ltnghia.github.io/eventa/openevents-v1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenEvents%20V1%3A%20Large-Scale%20Benchmark%20Dataset%20for%20Multimodal%20Event%0A%20%20Grounding&entry.906535625=Hieu%20Nguyen%20and%20Phuc-Tan%20Nguyen%20and%20Thien-Phuc%20Tran%20and%20Minh-Quang%20Nguyen%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Trung-Nghia%20Le&entry.1292438233=%20%20We%20introduce%20OpenEvents%20V1a%20large-scale%20benchmark%20dataset%20designed%20to%20advance%0Aevent-centric%20vision-language%20understanding.%20Unlike%20conventional%20image%0Acaptioning%20and%20retrieval%20datasets%20that%20focus%20on%20surface-level%20descriptions%2C%0AOpenEvents%20V1%20dataset%20emphasizes%20contextual%20and%20temporal%20grounding%20through%0Athree%20primary%20tasks%3A%20%281%29%20generating%20rich%2C%20event-aware%20image%20captions%2C%20%282%29%0Aretrieving%20event-relevant%20news%20articles%20from%20image%20queries%2C%20and%20%283%29%20retrieving%0Aevent-relevant%20images%20from%20narrative-style%20textual%20queries.%20The%20dataset%0Acomprises%20over%20200%2C000%20news%20articles%20and%20400%2C000%20associated%20images%20sourced%20from%0ACNN%20and%20The%20Guardian%2C%20spanning%20diverse%20domains%20and%20time%20periods.%20We%20provide%0Aextensive%20baseline%20results%20and%20standardized%20evaluation%20protocols%20for%20all%20tasks.%0AOpenEvents%20V1%20establishes%20a%20robust%20foundation%20for%20developing%20multimodal%20AI%0Asystems%20capable%20of%20deep%20reasoning%20over%20complex%20real-world%20events.%20The%20dataset%0Ais%20publicly%20available%20at%20https%3A//ltnghia.github.io/eventa/openevents-v1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18372v2&entry.124074799=Read"},
{"title": "Safe Navigation under State Uncertainty: Online Adaptation for Robust\n  Control Barrier Functions", "author": "Ersin Das and Rahal Nanayakkara and Xiao Tan and Ryan M. Bena and Joel W. Burdick and Paulo Tabuada and Aaron D. Ames", "abstract": "  Measurements and state estimates are often imperfect in control practice,\nposing challenges for safety-critical applications, where safety guarantees\nrely on accurate state information. In the presence of estimation errors,\nseveral prior robust control barrier function (R-CBF) formulations have imposed\nstrict conditions on the input. These methods can be overly conservative and\ncan introduce issues such as infeasibility, high control effort, etc. This work\nproposes a systematic method to improve R-CBFs, and demonstrates its advantages\non a tracked vehicle that navigates among multiple obstacles. A primary\ncontribution is a new optimization-based online parameter adaptation scheme\nthat reduces the conservativeness of existing R-CBFs. In order to reduce the\ncomplexity of the parameter optimization, we merge several safety constraints\ninto one unified numerical CBF via Poisson's equation. We further address the\ndual relative degree issue that typically causes difficulty in vehicle\ntracking. Experimental trials demonstrate the overall performance improvement\nof our approach over existing formulations.\n", "link": "http://arxiv.org/abs/2508.19159v1", "date": "2025-08-26", "relevancy": 2.1797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.58}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5457}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Navigation%20under%20State%20Uncertainty%3A%20Online%20Adaptation%20for%20Robust%0A%20%20Control%20Barrier%20Functions&body=Title%3A%20Safe%20Navigation%20under%20State%20Uncertainty%3A%20Online%20Adaptation%20for%20Robust%0A%20%20Control%20Barrier%20Functions%0AAuthor%3A%20Ersin%20Das%20and%20Rahal%20Nanayakkara%20and%20Xiao%20Tan%20and%20Ryan%20M.%20Bena%20and%20Joel%20W.%20Burdick%20and%20Paulo%20Tabuada%20and%20Aaron%20D.%20Ames%0AAbstract%3A%20%20%20Measurements%20and%20state%20estimates%20are%20often%20imperfect%20in%20control%20practice%2C%0Aposing%20challenges%20for%20safety-critical%20applications%2C%20where%20safety%20guarantees%0Arely%20on%20accurate%20state%20information.%20In%20the%20presence%20of%20estimation%20errors%2C%0Aseveral%20prior%20robust%20control%20barrier%20function%20%28R-CBF%29%20formulations%20have%20imposed%0Astrict%20conditions%20on%20the%20input.%20These%20methods%20can%20be%20overly%20conservative%20and%0Acan%20introduce%20issues%20such%20as%20infeasibility%2C%20high%20control%20effort%2C%20etc.%20This%20work%0Aproposes%20a%20systematic%20method%20to%20improve%20R-CBFs%2C%20and%20demonstrates%20its%20advantages%0Aon%20a%20tracked%20vehicle%20that%20navigates%20among%20multiple%20obstacles.%20A%20primary%0Acontribution%20is%20a%20new%20optimization-based%20online%20parameter%20adaptation%20scheme%0Athat%20reduces%20the%20conservativeness%20of%20existing%20R-CBFs.%20In%20order%20to%20reduce%20the%0Acomplexity%20of%20the%20parameter%20optimization%2C%20we%20merge%20several%20safety%20constraints%0Ainto%20one%20unified%20numerical%20CBF%20via%20Poisson%27s%20equation.%20We%20further%20address%20the%0Adual%20relative%20degree%20issue%20that%20typically%20causes%20difficulty%20in%20vehicle%0Atracking.%20Experimental%20trials%20demonstrate%20the%20overall%20performance%20improvement%0Aof%20our%20approach%20over%20existing%20formulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Navigation%2520under%2520State%2520Uncertainty%253A%2520Online%2520Adaptation%2520for%2520Robust%250A%2520%2520Control%2520Barrier%2520Functions%26entry.906535625%3DErsin%2520Das%2520and%2520Rahal%2520Nanayakkara%2520and%2520Xiao%2520Tan%2520and%2520Ryan%2520M.%2520Bena%2520and%2520Joel%2520W.%2520Burdick%2520and%2520Paulo%2520Tabuada%2520and%2520Aaron%2520D.%2520Ames%26entry.1292438233%3D%2520%2520Measurements%2520and%2520state%2520estimates%2520are%2520often%2520imperfect%2520in%2520control%2520practice%252C%250Aposing%2520challenges%2520for%2520safety-critical%2520applications%252C%2520where%2520safety%2520guarantees%250Arely%2520on%2520accurate%2520state%2520information.%2520In%2520the%2520presence%2520of%2520estimation%2520errors%252C%250Aseveral%2520prior%2520robust%2520control%2520barrier%2520function%2520%2528R-CBF%2529%2520formulations%2520have%2520imposed%250Astrict%2520conditions%2520on%2520the%2520input.%2520These%2520methods%2520can%2520be%2520overly%2520conservative%2520and%250Acan%2520introduce%2520issues%2520such%2520as%2520infeasibility%252C%2520high%2520control%2520effort%252C%2520etc.%2520This%2520work%250Aproposes%2520a%2520systematic%2520method%2520to%2520improve%2520R-CBFs%252C%2520and%2520demonstrates%2520its%2520advantages%250Aon%2520a%2520tracked%2520vehicle%2520that%2520navigates%2520among%2520multiple%2520obstacles.%2520A%2520primary%250Acontribution%2520is%2520a%2520new%2520optimization-based%2520online%2520parameter%2520adaptation%2520scheme%250Athat%2520reduces%2520the%2520conservativeness%2520of%2520existing%2520R-CBFs.%2520In%2520order%2520to%2520reduce%2520the%250Acomplexity%2520of%2520the%2520parameter%2520optimization%252C%2520we%2520merge%2520several%2520safety%2520constraints%250Ainto%2520one%2520unified%2520numerical%2520CBF%2520via%2520Poisson%2527s%2520equation.%2520We%2520further%2520address%2520the%250Adual%2520relative%2520degree%2520issue%2520that%2520typically%2520causes%2520difficulty%2520in%2520vehicle%250Atracking.%2520Experimental%2520trials%2520demonstrate%2520the%2520overall%2520performance%2520improvement%250Aof%2520our%2520approach%2520over%2520existing%2520formulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Navigation%20under%20State%20Uncertainty%3A%20Online%20Adaptation%20for%20Robust%0A%20%20Control%20Barrier%20Functions&entry.906535625=Ersin%20Das%20and%20Rahal%20Nanayakkara%20and%20Xiao%20Tan%20and%20Ryan%20M.%20Bena%20and%20Joel%20W.%20Burdick%20and%20Paulo%20Tabuada%20and%20Aaron%20D.%20Ames&entry.1292438233=%20%20Measurements%20and%20state%20estimates%20are%20often%20imperfect%20in%20control%20practice%2C%0Aposing%20challenges%20for%20safety-critical%20applications%2C%20where%20safety%20guarantees%0Arely%20on%20accurate%20state%20information.%20In%20the%20presence%20of%20estimation%20errors%2C%0Aseveral%20prior%20robust%20control%20barrier%20function%20%28R-CBF%29%20formulations%20have%20imposed%0Astrict%20conditions%20on%20the%20input.%20These%20methods%20can%20be%20overly%20conservative%20and%0Acan%20introduce%20issues%20such%20as%20infeasibility%2C%20high%20control%20effort%2C%20etc.%20This%20work%0Aproposes%20a%20systematic%20method%20to%20improve%20R-CBFs%2C%20and%20demonstrates%20its%20advantages%0Aon%20a%20tracked%20vehicle%20that%20navigates%20among%20multiple%20obstacles.%20A%20primary%0Acontribution%20is%20a%20new%20optimization-based%20online%20parameter%20adaptation%20scheme%0Athat%20reduces%20the%20conservativeness%20of%20existing%20R-CBFs.%20In%20order%20to%20reduce%20the%0Acomplexity%20of%20the%20parameter%20optimization%2C%20we%20merge%20several%20safety%20constraints%0Ainto%20one%20unified%20numerical%20CBF%20via%20Poisson%27s%20equation.%20We%20further%20address%20the%0Adual%20relative%20degree%20issue%20that%20typically%20causes%20difficulty%20in%20vehicle%0Atracking.%20Experimental%20trials%20demonstrate%20the%20overall%20performance%20improvement%0Aof%20our%20approach%20over%20existing%20formulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19159v1&entry.124074799=Read"},
{"title": "Generative AI in Map-Making: A Technical Exploration and Its\n  Implications for Cartographers", "author": "Claudio Affolter and Sidi Wu and Yizi Chen and Lorenz Hurni", "abstract": "  Traditional map-making relies heavily on Geographic Information Systems\n(GIS), requiring domain expertise and being time-consuming, especially for\nrepetitive tasks. Recent advances in generative AI (GenAI), particularly image\ndiffusion models, offer new opportunities for automating and democratizing the\nmap-making process. However, these models struggle with accurate map creation\ndue to limited control over spatial composition and semantic layout. To address\nthis, we integrate vector data to guide map generation in different styles,\nspecified by the textual prompts. Our model is the first to generate accurate\nmaps in controlled styles, and we have integrated it into a web application to\nimprove its usability and accessibility. We conducted a user study with\nprofessional cartographers to assess the fidelity of generated maps, the\nusability of the web application, and the implications of ever-emerging GenAI\nin map-making. The findings have suggested the potential of our developed\napplication and, more generally, the GenAI models in helping both non-expert\nusers and professionals in creating maps more efficiently. We have also\noutlined further technical improvements and emphasized the new role of\ncartographers to advance the paradigm of AI-assisted map-making.\n", "link": "http://arxiv.org/abs/2508.18959v1", "date": "2025-08-26", "relevancy": 2.1693, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5671}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5407}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20in%20Map-Making%3A%20A%20Technical%20Exploration%20and%20Its%0A%20%20Implications%20for%20Cartographers&body=Title%3A%20Generative%20AI%20in%20Map-Making%3A%20A%20Technical%20Exploration%20and%20Its%0A%20%20Implications%20for%20Cartographers%0AAuthor%3A%20Claudio%20Affolter%20and%20Sidi%20Wu%20and%20Yizi%20Chen%20and%20Lorenz%20Hurni%0AAbstract%3A%20%20%20Traditional%20map-making%20relies%20heavily%20on%20Geographic%20Information%20Systems%0A%28GIS%29%2C%20requiring%20domain%20expertise%20and%20being%20time-consuming%2C%20especially%20for%0Arepetitive%20tasks.%20Recent%20advances%20in%20generative%20AI%20%28GenAI%29%2C%20particularly%20image%0Adiffusion%20models%2C%20offer%20new%20opportunities%20for%20automating%20and%20democratizing%20the%0Amap-making%20process.%20However%2C%20these%20models%20struggle%20with%20accurate%20map%20creation%0Adue%20to%20limited%20control%20over%20spatial%20composition%20and%20semantic%20layout.%20To%20address%0Athis%2C%20we%20integrate%20vector%20data%20to%20guide%20map%20generation%20in%20different%20styles%2C%0Aspecified%20by%20the%20textual%20prompts.%20Our%20model%20is%20the%20first%20to%20generate%20accurate%0Amaps%20in%20controlled%20styles%2C%20and%20we%20have%20integrated%20it%20into%20a%20web%20application%20to%0Aimprove%20its%20usability%20and%20accessibility.%20We%20conducted%20a%20user%20study%20with%0Aprofessional%20cartographers%20to%20assess%20the%20fidelity%20of%20generated%20maps%2C%20the%0Ausability%20of%20the%20web%20application%2C%20and%20the%20implications%20of%20ever-emerging%20GenAI%0Ain%20map-making.%20The%20findings%20have%20suggested%20the%20potential%20of%20our%20developed%0Aapplication%20and%2C%20more%20generally%2C%20the%20GenAI%20models%20in%20helping%20both%20non-expert%0Ausers%20and%20professionals%20in%20creating%20maps%20more%20efficiently.%20We%20have%20also%0Aoutlined%20further%20technical%20improvements%20and%20emphasized%20the%20new%20role%20of%0Acartographers%20to%20advance%20the%20paradigm%20of%20AI-assisted%20map-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520in%2520Map-Making%253A%2520A%2520Technical%2520Exploration%2520and%2520Its%250A%2520%2520Implications%2520for%2520Cartographers%26entry.906535625%3DClaudio%2520Affolter%2520and%2520Sidi%2520Wu%2520and%2520Yizi%2520Chen%2520and%2520Lorenz%2520Hurni%26entry.1292438233%3D%2520%2520Traditional%2520map-making%2520relies%2520heavily%2520on%2520Geographic%2520Information%2520Systems%250A%2528GIS%2529%252C%2520requiring%2520domain%2520expertise%2520and%2520being%2520time-consuming%252C%2520especially%2520for%250Arepetitive%2520tasks.%2520Recent%2520advances%2520in%2520generative%2520AI%2520%2528GenAI%2529%252C%2520particularly%2520image%250Adiffusion%2520models%252C%2520offer%2520new%2520opportunities%2520for%2520automating%2520and%2520democratizing%2520the%250Amap-making%2520process.%2520However%252C%2520these%2520models%2520struggle%2520with%2520accurate%2520map%2520creation%250Adue%2520to%2520limited%2520control%2520over%2520spatial%2520composition%2520and%2520semantic%2520layout.%2520To%2520address%250Athis%252C%2520we%2520integrate%2520vector%2520data%2520to%2520guide%2520map%2520generation%2520in%2520different%2520styles%252C%250Aspecified%2520by%2520the%2520textual%2520prompts.%2520Our%2520model%2520is%2520the%2520first%2520to%2520generate%2520accurate%250Amaps%2520in%2520controlled%2520styles%252C%2520and%2520we%2520have%2520integrated%2520it%2520into%2520a%2520web%2520application%2520to%250Aimprove%2520its%2520usability%2520and%2520accessibility.%2520We%2520conducted%2520a%2520user%2520study%2520with%250Aprofessional%2520cartographers%2520to%2520assess%2520the%2520fidelity%2520of%2520generated%2520maps%252C%2520the%250Ausability%2520of%2520the%2520web%2520application%252C%2520and%2520the%2520implications%2520of%2520ever-emerging%2520GenAI%250Ain%2520map-making.%2520The%2520findings%2520have%2520suggested%2520the%2520potential%2520of%2520our%2520developed%250Aapplication%2520and%252C%2520more%2520generally%252C%2520the%2520GenAI%2520models%2520in%2520helping%2520both%2520non-expert%250Ausers%2520and%2520professionals%2520in%2520creating%2520maps%2520more%2520efficiently.%2520We%2520have%2520also%250Aoutlined%2520further%2520technical%2520improvements%2520and%2520emphasized%2520the%2520new%2520role%2520of%250Acartographers%2520to%2520advance%2520the%2520paradigm%2520of%2520AI-assisted%2520map-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20in%20Map-Making%3A%20A%20Technical%20Exploration%20and%20Its%0A%20%20Implications%20for%20Cartographers&entry.906535625=Claudio%20Affolter%20and%20Sidi%20Wu%20and%20Yizi%20Chen%20and%20Lorenz%20Hurni&entry.1292438233=%20%20Traditional%20map-making%20relies%20heavily%20on%20Geographic%20Information%20Systems%0A%28GIS%29%2C%20requiring%20domain%20expertise%20and%20being%20time-consuming%2C%20especially%20for%0Arepetitive%20tasks.%20Recent%20advances%20in%20generative%20AI%20%28GenAI%29%2C%20particularly%20image%0Adiffusion%20models%2C%20offer%20new%20opportunities%20for%20automating%20and%20democratizing%20the%0Amap-making%20process.%20However%2C%20these%20models%20struggle%20with%20accurate%20map%20creation%0Adue%20to%20limited%20control%20over%20spatial%20composition%20and%20semantic%20layout.%20To%20address%0Athis%2C%20we%20integrate%20vector%20data%20to%20guide%20map%20generation%20in%20different%20styles%2C%0Aspecified%20by%20the%20textual%20prompts.%20Our%20model%20is%20the%20first%20to%20generate%20accurate%0Amaps%20in%20controlled%20styles%2C%20and%20we%20have%20integrated%20it%20into%20a%20web%20application%20to%0Aimprove%20its%20usability%20and%20accessibility.%20We%20conducted%20a%20user%20study%20with%0Aprofessional%20cartographers%20to%20assess%20the%20fidelity%20of%20generated%20maps%2C%20the%0Ausability%20of%20the%20web%20application%2C%20and%20the%20implications%20of%20ever-emerging%20GenAI%0Ain%20map-making.%20The%20findings%20have%20suggested%20the%20potential%20of%20our%20developed%0Aapplication%20and%2C%20more%20generally%2C%20the%20GenAI%20models%20in%20helping%20both%20non-expert%0Ausers%20and%20professionals%20in%20creating%20maps%20more%20efficiently.%20We%20have%20also%0Aoutlined%20further%20technical%20improvements%20and%20emphasized%20the%20new%20role%20of%0Acartographers%20to%20advance%20the%20paradigm%20of%20AI-assisted%20map-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18959v1&entry.124074799=Read"},
{"title": "Enhancing Underwater Images via Deep Learning: A Comparative Study of\n  VGG19 and ResNet50-Based Approaches", "author": "Aoqi Li and Yanghui Song and Jichao Dao and Chengfu Yang", "abstract": "  This paper addresses the challenging problem of image enhancement in complex\nunderwater scenes by proposing a solution based on deep learning. The proposed\nmethod skillfully integrates two deep convolutional neural network models,\nVGG19 and ResNet50, leveraging their powerful feature extraction capabilities\nto perform multi-scale and multi-level deep feature analysis of underwater\nimages. By constructing a unified model, the complementary advantages of the\ntwo models are effectively integrated, achieving a more comprehensive and\naccurate image enhancement effect.To objectively evaluate the enhancement\neffect, this paper introduces image quality assessment metrics such as PSNR,\nUCIQE, and UIQM to quantitatively compare images before and after enhancement\nand deeply analyzes the performance of different models in different\nscenarios.Furthermore, to improve the practicality and stability of the\nunderwater visual enhancement system, this paper also provides practical\nsuggestions from aspects such as model optimization, multi-model fusion, and\nhardware selection, aiming to provide strong technical support for visual\nenhancement tasks in complex underwater environments.\n", "link": "http://arxiv.org/abs/2508.17397v2", "date": "2025-08-26", "relevancy": 2.1651, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Underwater%20Images%20via%20Deep%20Learning%3A%20A%20Comparative%20Study%20of%0A%20%20VGG19%20and%20ResNet50-Based%20Approaches&body=Title%3A%20Enhancing%20Underwater%20Images%20via%20Deep%20Learning%3A%20A%20Comparative%20Study%20of%0A%20%20VGG19%20and%20ResNet50-Based%20Approaches%0AAuthor%3A%20Aoqi%20Li%20and%20Yanghui%20Song%20and%20Jichao%20Dao%20and%20Chengfu%20Yang%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenging%20problem%20of%20image%20enhancement%20in%20complex%0Aunderwater%20scenes%20by%20proposing%20a%20solution%20based%20on%20deep%20learning.%20The%20proposed%0Amethod%20skillfully%20integrates%20two%20deep%20convolutional%20neural%20network%20models%2C%0AVGG19%20and%20ResNet50%2C%20leveraging%20their%20powerful%20feature%20extraction%20capabilities%0Ato%20perform%20multi-scale%20and%20multi-level%20deep%20feature%20analysis%20of%20underwater%0Aimages.%20By%20constructing%20a%20unified%20model%2C%20the%20complementary%20advantages%20of%20the%0Atwo%20models%20are%20effectively%20integrated%2C%20achieving%20a%20more%20comprehensive%20and%0Aaccurate%20image%20enhancement%20effect.To%20objectively%20evaluate%20the%20enhancement%0Aeffect%2C%20this%20paper%20introduces%20image%20quality%20assessment%20metrics%20such%20as%20PSNR%2C%0AUCIQE%2C%20and%20UIQM%20to%20quantitatively%20compare%20images%20before%20and%20after%20enhancement%0Aand%20deeply%20analyzes%20the%20performance%20of%20different%20models%20in%20different%0Ascenarios.Furthermore%2C%20to%20improve%20the%20practicality%20and%20stability%20of%20the%0Aunderwater%20visual%20enhancement%20system%2C%20this%20paper%20also%20provides%20practical%0Asuggestions%20from%20aspects%20such%20as%20model%20optimization%2C%20multi-model%20fusion%2C%20and%0Ahardware%20selection%2C%20aiming%20to%20provide%20strong%20technical%20support%20for%20visual%0Aenhancement%20tasks%20in%20complex%20underwater%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Underwater%2520Images%2520via%2520Deep%2520Learning%253A%2520A%2520Comparative%2520Study%2520of%250A%2520%2520VGG19%2520and%2520ResNet50-Based%2520Approaches%26entry.906535625%3DAoqi%2520Li%2520and%2520Yanghui%2520Song%2520and%2520Jichao%2520Dao%2520and%2520Chengfu%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenging%2520problem%2520of%2520image%2520enhancement%2520in%2520complex%250Aunderwater%2520scenes%2520by%2520proposing%2520a%2520solution%2520based%2520on%2520deep%2520learning.%2520The%2520proposed%250Amethod%2520skillfully%2520integrates%2520two%2520deep%2520convolutional%2520neural%2520network%2520models%252C%250AVGG19%2520and%2520ResNet50%252C%2520leveraging%2520their%2520powerful%2520feature%2520extraction%2520capabilities%250Ato%2520perform%2520multi-scale%2520and%2520multi-level%2520deep%2520feature%2520analysis%2520of%2520underwater%250Aimages.%2520By%2520constructing%2520a%2520unified%2520model%252C%2520the%2520complementary%2520advantages%2520of%2520the%250Atwo%2520models%2520are%2520effectively%2520integrated%252C%2520achieving%2520a%2520more%2520comprehensive%2520and%250Aaccurate%2520image%2520enhancement%2520effect.To%2520objectively%2520evaluate%2520the%2520enhancement%250Aeffect%252C%2520this%2520paper%2520introduces%2520image%2520quality%2520assessment%2520metrics%2520such%2520as%2520PSNR%252C%250AUCIQE%252C%2520and%2520UIQM%2520to%2520quantitatively%2520compare%2520images%2520before%2520and%2520after%2520enhancement%250Aand%2520deeply%2520analyzes%2520the%2520performance%2520of%2520different%2520models%2520in%2520different%250Ascenarios.Furthermore%252C%2520to%2520improve%2520the%2520practicality%2520and%2520stability%2520of%2520the%250Aunderwater%2520visual%2520enhancement%2520system%252C%2520this%2520paper%2520also%2520provides%2520practical%250Asuggestions%2520from%2520aspects%2520such%2520as%2520model%2520optimization%252C%2520multi-model%2520fusion%252C%2520and%250Ahardware%2520selection%252C%2520aiming%2520to%2520provide%2520strong%2520technical%2520support%2520for%2520visual%250Aenhancement%2520tasks%2520in%2520complex%2520underwater%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Underwater%20Images%20via%20Deep%20Learning%3A%20A%20Comparative%20Study%20of%0A%20%20VGG19%20and%20ResNet50-Based%20Approaches&entry.906535625=Aoqi%20Li%20and%20Yanghui%20Song%20and%20Jichao%20Dao%20and%20Chengfu%20Yang&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenging%20problem%20of%20image%20enhancement%20in%20complex%0Aunderwater%20scenes%20by%20proposing%20a%20solution%20based%20on%20deep%20learning.%20The%20proposed%0Amethod%20skillfully%20integrates%20two%20deep%20convolutional%20neural%20network%20models%2C%0AVGG19%20and%20ResNet50%2C%20leveraging%20their%20powerful%20feature%20extraction%20capabilities%0Ato%20perform%20multi-scale%20and%20multi-level%20deep%20feature%20analysis%20of%20underwater%0Aimages.%20By%20constructing%20a%20unified%20model%2C%20the%20complementary%20advantages%20of%20the%0Atwo%20models%20are%20effectively%20integrated%2C%20achieving%20a%20more%20comprehensive%20and%0Aaccurate%20image%20enhancement%20effect.To%20objectively%20evaluate%20the%20enhancement%0Aeffect%2C%20this%20paper%20introduces%20image%20quality%20assessment%20metrics%20such%20as%20PSNR%2C%0AUCIQE%2C%20and%20UIQM%20to%20quantitatively%20compare%20images%20before%20and%20after%20enhancement%0Aand%20deeply%20analyzes%20the%20performance%20of%20different%20models%20in%20different%0Ascenarios.Furthermore%2C%20to%20improve%20the%20practicality%20and%20stability%20of%20the%0Aunderwater%20visual%20enhancement%20system%2C%20this%20paper%20also%20provides%20practical%0Asuggestions%20from%20aspects%20such%20as%20model%20optimization%2C%20multi-model%20fusion%2C%20and%0Ahardware%20selection%2C%20aiming%20to%20provide%20strong%20technical%20support%20for%20visual%0Aenhancement%20tasks%20in%20complex%20underwater%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17397v2&entry.124074799=Read"},
{"title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A\n  Framework and Benchmark", "author": "Yuxuan Cai and Yipeng Hao and Jie Zhou and Hang Yan and Zhikai Lei and Rui Zhen and Zhenhua Han and Yutao Yang and Junsong Li and Qianjun Pan and Tianyu Huai and Qin Chen and Xin Li and Kai Chen and Bo Zhang and Xipeng Qiu and Liang He", "abstract": "  As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI.\n", "link": "http://arxiv.org/abs/2508.19005v1", "date": "2025-08-26", "relevancy": 2.162, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5634}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5458}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Self-Evolving%20Agents%20via%20Experience-Driven%20Lifelong%20Learning%3A%20A%0A%20%20Framework%20and%20Benchmark&body=Title%3A%20Building%20Self-Evolving%20Agents%20via%20Experience-Driven%20Lifelong%20Learning%3A%20A%0A%20%20Framework%20and%20Benchmark%0AAuthor%3A%20Yuxuan%20Cai%20and%20Yipeng%20Hao%20and%20Jie%20Zhou%20and%20Hang%20Yan%20and%20Zhikai%20Lei%20and%20Rui%20Zhen%20and%20Zhenhua%20Han%20and%20Yutao%20Yang%20and%20Junsong%20Li%20and%20Qianjun%20Pan%20and%20Tianyu%20Huai%20and%20Qin%20Chen%20and%20Xin%20Li%20and%20Kai%20Chen%20and%20Bo%20Zhang%20and%20Xipeng%20Qiu%20and%20Liang%20He%0AAbstract%3A%20%20%20As%20AI%20advances%20toward%20general%20intelligence%2C%20the%20focus%20is%20shifting%20from%0Asystems%20optimized%20for%20static%20tasks%20to%20creating%20open-ended%20agents%20that%20learn%0Acontinuously.%20In%20this%20paper%2C%20we%20introduce%20Experience-driven%20Lifelong%20Learning%0A%28ELL%29%2C%20a%20framework%20for%20building%20self-evolving%20agents%20capable%20of%20continuous%0Agrowth%20through%20real-world%20interaction.%20The%20framework%20is%20built%20on%20four%20core%0Aprinciples%3A%20%281%29%20Experience%20Exploration%3A%20Agents%20learn%20through%20continuous%2C%0Aself-motivated%20interaction%20with%20dynamic%20environments%2C%20navigating%20interdependent%0Atasks%20and%20generating%20rich%20experiential%20trajectories.%20%282%29%20Long-term%20Memory%3A%0AAgents%20preserve%20and%20structure%20historical%20knowledge%2C%20including%20personal%0Aexperiences%2C%20domain%20expertise%2C%20and%20commonsense%20reasoning%2C%20into%20a%20persistent%0Amemory%20system.%20%283%29%20Skill%20Learning%3A%20Agents%20autonomously%20improve%20by%20abstracting%0Arecurring%20patterns%20from%20experience%20into%20reusable%20skills%2C%20which%20are%20actively%0Arefined%20and%20validated%20for%20application%20in%20new%20tasks.%20%284%29%20Knowledge%0AInternalization%3A%20Agents%20internalize%20explicit%20and%20discrete%20experiences%20into%0Aimplicit%20and%20intuitive%20capabilities%20as%20%22second%20nature%22.%0A%20%20We%20also%20introduce%20StuLife%2C%20a%20benchmark%20dataset%20for%20ELL%20that%20simulates%20a%0Astudent%27s%20holistic%20college%20journey%2C%20from%20enrollment%20to%20academic%20and%20personal%0Adevelopment%2C%20across%20three%20core%20phases%20and%20ten%20detailed%20sub-scenarios.%20StuLife%0Ais%20designed%20around%20three%20key%20paradigm%20shifts%3A%20From%20Passive%20to%20Proactive%2C%20From%0AContext%20to%20Memory%2C%20and%20From%20Imitation%20to%20Learning.%20In%20this%20dynamic%20environment%2C%0Aagents%20must%20acquire%20and%20distill%20practical%20skills%20and%20maintain%20persistent%20memory%0Ato%20make%20decisions%20based%20on%20evolving%20state%20variables.%20StuLife%20provides%20a%0Acomprehensive%20platform%20for%20evaluating%20lifelong%20learning%20capabilities%2C%20including%0Amemory%20retention%2C%20skill%20transfer%2C%20and%20self-motivated%20behavior.%20Beyond%0Aevaluating%20SOTA%20LLMs%20on%20the%20StuLife%20benchmark%2C%20we%20also%20explore%20the%20role%20of%0Acontext%20engineering%20in%20advancing%20AGI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Self-Evolving%2520Agents%2520via%2520Experience-Driven%2520Lifelong%2520Learning%253A%2520A%250A%2520%2520Framework%2520and%2520Benchmark%26entry.906535625%3DYuxuan%2520Cai%2520and%2520Yipeng%2520Hao%2520and%2520Jie%2520Zhou%2520and%2520Hang%2520Yan%2520and%2520Zhikai%2520Lei%2520and%2520Rui%2520Zhen%2520and%2520Zhenhua%2520Han%2520and%2520Yutao%2520Yang%2520and%2520Junsong%2520Li%2520and%2520Qianjun%2520Pan%2520and%2520Tianyu%2520Huai%2520and%2520Qin%2520Chen%2520and%2520Xin%2520Li%2520and%2520Kai%2520Chen%2520and%2520Bo%2520Zhang%2520and%2520Xipeng%2520Qiu%2520and%2520Liang%2520He%26entry.1292438233%3D%2520%2520As%2520AI%2520advances%2520toward%2520general%2520intelligence%252C%2520the%2520focus%2520is%2520shifting%2520from%250Asystems%2520optimized%2520for%2520static%2520tasks%2520to%2520creating%2520open-ended%2520agents%2520that%2520learn%250Acontinuously.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Experience-driven%2520Lifelong%2520Learning%250A%2528ELL%2529%252C%2520a%2520framework%2520for%2520building%2520self-evolving%2520agents%2520capable%2520of%2520continuous%250Agrowth%2520through%2520real-world%2520interaction.%2520The%2520framework%2520is%2520built%2520on%2520four%2520core%250Aprinciples%253A%2520%25281%2529%2520Experience%2520Exploration%253A%2520Agents%2520learn%2520through%2520continuous%252C%250Aself-motivated%2520interaction%2520with%2520dynamic%2520environments%252C%2520navigating%2520interdependent%250Atasks%2520and%2520generating%2520rich%2520experiential%2520trajectories.%2520%25282%2529%2520Long-term%2520Memory%253A%250AAgents%2520preserve%2520and%2520structure%2520historical%2520knowledge%252C%2520including%2520personal%250Aexperiences%252C%2520domain%2520expertise%252C%2520and%2520commonsense%2520reasoning%252C%2520into%2520a%2520persistent%250Amemory%2520system.%2520%25283%2529%2520Skill%2520Learning%253A%2520Agents%2520autonomously%2520improve%2520by%2520abstracting%250Arecurring%2520patterns%2520from%2520experience%2520into%2520reusable%2520skills%252C%2520which%2520are%2520actively%250Arefined%2520and%2520validated%2520for%2520application%2520in%2520new%2520tasks.%2520%25284%2529%2520Knowledge%250AInternalization%253A%2520Agents%2520internalize%2520explicit%2520and%2520discrete%2520experiences%2520into%250Aimplicit%2520and%2520intuitive%2520capabilities%2520as%2520%2522second%2520nature%2522.%250A%2520%2520We%2520also%2520introduce%2520StuLife%252C%2520a%2520benchmark%2520dataset%2520for%2520ELL%2520that%2520simulates%2520a%250Astudent%2527s%2520holistic%2520college%2520journey%252C%2520from%2520enrollment%2520to%2520academic%2520and%2520personal%250Adevelopment%252C%2520across%2520three%2520core%2520phases%2520and%2520ten%2520detailed%2520sub-scenarios.%2520StuLife%250Ais%2520designed%2520around%2520three%2520key%2520paradigm%2520shifts%253A%2520From%2520Passive%2520to%2520Proactive%252C%2520From%250AContext%2520to%2520Memory%252C%2520and%2520From%2520Imitation%2520to%2520Learning.%2520In%2520this%2520dynamic%2520environment%252C%250Aagents%2520must%2520acquire%2520and%2520distill%2520practical%2520skills%2520and%2520maintain%2520persistent%2520memory%250Ato%2520make%2520decisions%2520based%2520on%2520evolving%2520state%2520variables.%2520StuLife%2520provides%2520a%250Acomprehensive%2520platform%2520for%2520evaluating%2520lifelong%2520learning%2520capabilities%252C%2520including%250Amemory%2520retention%252C%2520skill%2520transfer%252C%2520and%2520self-motivated%2520behavior.%2520Beyond%250Aevaluating%2520SOTA%2520LLMs%2520on%2520the%2520StuLife%2520benchmark%252C%2520we%2520also%2520explore%2520the%2520role%2520of%250Acontext%2520engineering%2520in%2520advancing%2520AGI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Self-Evolving%20Agents%20via%20Experience-Driven%20Lifelong%20Learning%3A%20A%0A%20%20Framework%20and%20Benchmark&entry.906535625=Yuxuan%20Cai%20and%20Yipeng%20Hao%20and%20Jie%20Zhou%20and%20Hang%20Yan%20and%20Zhikai%20Lei%20and%20Rui%20Zhen%20and%20Zhenhua%20Han%20and%20Yutao%20Yang%20and%20Junsong%20Li%20and%20Qianjun%20Pan%20and%20Tianyu%20Huai%20and%20Qin%20Chen%20and%20Xin%20Li%20and%20Kai%20Chen%20and%20Bo%20Zhang%20and%20Xipeng%20Qiu%20and%20Liang%20He&entry.1292438233=%20%20As%20AI%20advances%20toward%20general%20intelligence%2C%20the%20focus%20is%20shifting%20from%0Asystems%20optimized%20for%20static%20tasks%20to%20creating%20open-ended%20agents%20that%20learn%0Acontinuously.%20In%20this%20paper%2C%20we%20introduce%20Experience-driven%20Lifelong%20Learning%0A%28ELL%29%2C%20a%20framework%20for%20building%20self-evolving%20agents%20capable%20of%20continuous%0Agrowth%20through%20real-world%20interaction.%20The%20framework%20is%20built%20on%20four%20core%0Aprinciples%3A%20%281%29%20Experience%20Exploration%3A%20Agents%20learn%20through%20continuous%2C%0Aself-motivated%20interaction%20with%20dynamic%20environments%2C%20navigating%20interdependent%0Atasks%20and%20generating%20rich%20experiential%20trajectories.%20%282%29%20Long-term%20Memory%3A%0AAgents%20preserve%20and%20structure%20historical%20knowledge%2C%20including%20personal%0Aexperiences%2C%20domain%20expertise%2C%20and%20commonsense%20reasoning%2C%20into%20a%20persistent%0Amemory%20system.%20%283%29%20Skill%20Learning%3A%20Agents%20autonomously%20improve%20by%20abstracting%0Arecurring%20patterns%20from%20experience%20into%20reusable%20skills%2C%20which%20are%20actively%0Arefined%20and%20validated%20for%20application%20in%20new%20tasks.%20%284%29%20Knowledge%0AInternalization%3A%20Agents%20internalize%20explicit%20and%20discrete%20experiences%20into%0Aimplicit%20and%20intuitive%20capabilities%20as%20%22second%20nature%22.%0A%20%20We%20also%20introduce%20StuLife%2C%20a%20benchmark%20dataset%20for%20ELL%20that%20simulates%20a%0Astudent%27s%20holistic%20college%20journey%2C%20from%20enrollment%20to%20academic%20and%20personal%0Adevelopment%2C%20across%20three%20core%20phases%20and%20ten%20detailed%20sub-scenarios.%20StuLife%0Ais%20designed%20around%20three%20key%20paradigm%20shifts%3A%20From%20Passive%20to%20Proactive%2C%20From%0AContext%20to%20Memory%2C%20and%20From%20Imitation%20to%20Learning.%20In%20this%20dynamic%20environment%2C%0Aagents%20must%20acquire%20and%20distill%20practical%20skills%20and%20maintain%20persistent%20memory%0Ato%20make%20decisions%20based%20on%20evolving%20state%20variables.%20StuLife%20provides%20a%0Acomprehensive%20platform%20for%20evaluating%20lifelong%20learning%20capabilities%2C%20including%0Amemory%20retention%2C%20skill%20transfer%2C%20and%20self-motivated%20behavior.%20Beyond%0Aevaluating%20SOTA%20LLMs%20on%20the%20StuLife%20benchmark%2C%20we%20also%20explore%20the%20role%20of%0Acontext%20engineering%20in%20advancing%20AGI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19005v1&entry.124074799=Read"},
{"title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge\n  Understanding of LLMs", "author": "Zhiqiang Liu and Enpei Niu and Yin Hua and Mengshu Sun and Lei Liang and Huajun Chen and Wen Zhang", "abstract": "  Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.\n", "link": "http://arxiv.org/abs/2507.17178v2", "date": "2025-08-26", "relevancy": 2.1566, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKA-Bench%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Structured%20Knowledge%0A%20%20Understanding%20of%20LLMs&body=Title%3A%20SKA-Bench%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Structured%20Knowledge%0A%20%20Understanding%20of%20LLMs%0AAuthor%3A%20Zhiqiang%20Liu%20and%20Enpei%20Niu%20and%20Yin%20Hua%20and%20Mengshu%20Sun%20and%20Lei%20Liang%20and%20Huajun%20Chen%20and%20Wen%20Zhang%0AAbstract%3A%20%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20made%20significant%20progress%20in%0Aunderstanding%20Structured%20Knowledge%20%28SK%29%20like%20KG%20and%20Table%2C%20existing%20evaluations%0Afor%20SK%20understanding%20are%20non-rigorous%20%28i.e.%2C%20lacking%20evaluations%20of%20specific%0Acapabilities%29%20and%20focus%20on%20a%20single%20type%20of%20SK.%20Therefore%2C%20we%20aim%20to%20propose%20a%0Amore%20comprehensive%20and%20rigorous%20structured%20knowledge%20understanding%20benchmark%20to%0Adiagnose%20the%20shortcomings%20of%20LLMs.%20In%20this%20paper%2C%20we%20introduce%20SKA-Bench%2C%20a%0AStructured%20Knowledge%20Augmented%20QA%20Benchmark%20that%20encompasses%20four%20widely%20used%0Astructured%20knowledge%20forms%3A%20KG%2C%20Table%2C%20KG%2BText%2C%20and%20Table%2BText.%20We%20utilize%20a%0Athree-stage%20pipeline%20to%20construct%20SKA-Bench%20instances%2C%20which%20includes%20a%0Aquestion%2C%20an%20answer%2C%20positive%20knowledge%20units%2C%20and%20noisy%20knowledge%20units.%20To%0Aevaluate%20the%20SK%20understanding%20capabilities%20of%20LLMs%20in%20a%20fine-grained%20manner%2C%20we%0Aexpand%20the%20instances%20into%20four%20fundamental%20ability%20testbeds%3A%20Noise%20Robustness%2C%0AOrder%20Insensitivity%2C%20Information%20Integration%2C%20and%20Negative%20Rejection.%20Empirical%0Aevaluations%20on%208%20representative%20LLMs%2C%20including%20the%20advanced%20DeepSeek-R1%2C%0Aindicate%20that%20existing%20LLMs%20still%20face%20significant%20challenges%20in%20understanding%0Astructured%20knowledge%2C%20and%20their%20performance%20is%20influenced%20by%20factors%20such%20as%0Athe%20amount%20of%20noise%2C%20the%20order%20of%20knowledge%20units%2C%20and%20hallucination%0Aphenomenon.%20Our%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Lza12a/SKA-Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKA-Bench%253A%2520A%2520Fine-Grained%2520Benchmark%2520for%2520Evaluating%2520Structured%2520Knowledge%250A%2520%2520Understanding%2520of%2520LLMs%26entry.906535625%3DZhiqiang%2520Liu%2520and%2520Enpei%2520Niu%2520and%2520Yin%2520Hua%2520and%2520Mengshu%2520Sun%2520and%2520Lei%2520Liang%2520and%2520Huajun%2520Chen%2520and%2520Wen%2520Zhang%26entry.1292438233%3D%2520%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520progress%2520in%250Aunderstanding%2520Structured%2520Knowledge%2520%2528SK%2529%2520like%2520KG%2520and%2520Table%252C%2520existing%2520evaluations%250Afor%2520SK%2520understanding%2520are%2520non-rigorous%2520%2528i.e.%252C%2520lacking%2520evaluations%2520of%2520specific%250Acapabilities%2529%2520and%2520focus%2520on%2520a%2520single%2520type%2520of%2520SK.%2520Therefore%252C%2520we%2520aim%2520to%2520propose%2520a%250Amore%2520comprehensive%2520and%2520rigorous%2520structured%2520knowledge%2520understanding%2520benchmark%2520to%250Adiagnose%2520the%2520shortcomings%2520of%2520LLMs.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SKA-Bench%252C%2520a%250AStructured%2520Knowledge%2520Augmented%2520QA%2520Benchmark%2520that%2520encompasses%2520four%2520widely%2520used%250Astructured%2520knowledge%2520forms%253A%2520KG%252C%2520Table%252C%2520KG%252BText%252C%2520and%2520Table%252BText.%2520We%2520utilize%2520a%250Athree-stage%2520pipeline%2520to%2520construct%2520SKA-Bench%2520instances%252C%2520which%2520includes%2520a%250Aquestion%252C%2520an%2520answer%252C%2520positive%2520knowledge%2520units%252C%2520and%2520noisy%2520knowledge%2520units.%2520To%250Aevaluate%2520the%2520SK%2520understanding%2520capabilities%2520of%2520LLMs%2520in%2520a%2520fine-grained%2520manner%252C%2520we%250Aexpand%2520the%2520instances%2520into%2520four%2520fundamental%2520ability%2520testbeds%253A%2520Noise%2520Robustness%252C%250AOrder%2520Insensitivity%252C%2520Information%2520Integration%252C%2520and%2520Negative%2520Rejection.%2520Empirical%250Aevaluations%2520on%25208%2520representative%2520LLMs%252C%2520including%2520the%2520advanced%2520DeepSeek-R1%252C%250Aindicate%2520that%2520existing%2520LLMs%2520still%2520face%2520significant%2520challenges%2520in%2520understanding%250Astructured%2520knowledge%252C%2520and%2520their%2520performance%2520is%2520influenced%2520by%2520factors%2520such%2520as%250Athe%2520amount%2520of%2520noise%252C%2520the%2520order%2520of%2520knowledge%2520units%252C%2520and%2520hallucination%250Aphenomenon.%2520Our%2520dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/Lza12a/SKA-Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKA-Bench%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Structured%20Knowledge%0A%20%20Understanding%20of%20LLMs&entry.906535625=Zhiqiang%20Liu%20and%20Enpei%20Niu%20and%20Yin%20Hua%20and%20Mengshu%20Sun%20and%20Lei%20Liang%20and%20Huajun%20Chen%20and%20Wen%20Zhang&entry.1292438233=%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20made%20significant%20progress%20in%0Aunderstanding%20Structured%20Knowledge%20%28SK%29%20like%20KG%20and%20Table%2C%20existing%20evaluations%0Afor%20SK%20understanding%20are%20non-rigorous%20%28i.e.%2C%20lacking%20evaluations%20of%20specific%0Acapabilities%29%20and%20focus%20on%20a%20single%20type%20of%20SK.%20Therefore%2C%20we%20aim%20to%20propose%20a%0Amore%20comprehensive%20and%20rigorous%20structured%20knowledge%20understanding%20benchmark%20to%0Adiagnose%20the%20shortcomings%20of%20LLMs.%20In%20this%20paper%2C%20we%20introduce%20SKA-Bench%2C%20a%0AStructured%20Knowledge%20Augmented%20QA%20Benchmark%20that%20encompasses%20four%20widely%20used%0Astructured%20knowledge%20forms%3A%20KG%2C%20Table%2C%20KG%2BText%2C%20and%20Table%2BText.%20We%20utilize%20a%0Athree-stage%20pipeline%20to%20construct%20SKA-Bench%20instances%2C%20which%20includes%20a%0Aquestion%2C%20an%20answer%2C%20positive%20knowledge%20units%2C%20and%20noisy%20knowledge%20units.%20To%0Aevaluate%20the%20SK%20understanding%20capabilities%20of%20LLMs%20in%20a%20fine-grained%20manner%2C%20we%0Aexpand%20the%20instances%20into%20four%20fundamental%20ability%20testbeds%3A%20Noise%20Robustness%2C%0AOrder%20Insensitivity%2C%20Information%20Integration%2C%20and%20Negative%20Rejection.%20Empirical%0Aevaluations%20on%208%20representative%20LLMs%2C%20including%20the%20advanced%20DeepSeek-R1%2C%0Aindicate%20that%20existing%20LLMs%20still%20face%20significant%20challenges%20in%20understanding%0Astructured%20knowledge%2C%20and%20their%20performance%20is%20influenced%20by%20factors%20such%20as%0Athe%20amount%20of%20noise%2C%20the%20order%20of%20knowledge%20units%2C%20and%20hallucination%0Aphenomenon.%20Our%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Lza12a/SKA-Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17178v2&entry.124074799=Read"},
{"title": "DQEN: Dual Query Enhancement Network for DETR-based HOI Detection", "author": "Zhehao Li and Chong Wang and Yi Chen and Yinghao Lu and Jiangbo Qian and Jiong Wang and Jiafei Wu", "abstract": "  Human-Object Interaction (HOI) detection focuses on localizing human-object\npairs and recognizing their interactions. Recently, the DETR-based framework\nhas been widely adopted in HOI detection. In DETR-based HOI models, queries\nwith clear meaning are crucial for accurately detecting HOIs. However, prior\nworks have typically relied on randomly initialized queries, leading to vague\nrepresentations that limit the model's effectiveness. Meanwhile, humans in the\nHOI categories are fixed, while objects and their interactions are variable.\nTherefore, we propose a Dual Query Enhancement Network (DQEN) to enhance object\nand interaction queries. Specifically, object queries are enhanced with\nobject-aware encoder features, enabling the model to focus more effectively on\nhumans interacting with objects in an object-aware way. On the other hand, we\ndesign a novel Interaction Semantic Fusion module to exploit the HOI candidates\nthat are promoted by the CLIP model. Semantic features are extracted to enhance\nthe initialization of interaction queries, thereby improving the model's\nability to understand interactions. Furthermore, we introduce an Auxiliary\nPrediction Unit aimed at improving the representation of interaction features.\nOur proposed method achieves competitive performance on both the HICO-Det and\nthe V-COCO datasets. The source code is available at\nhttps://github.com/lzzhhh1019/DQEN.\n", "link": "http://arxiv.org/abs/2508.18896v1", "date": "2025-08-26", "relevancy": 2.1507, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5399}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5376}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DQEN%3A%20Dual%20Query%20Enhancement%20Network%20for%20DETR-based%20HOI%20Detection&body=Title%3A%20DQEN%3A%20Dual%20Query%20Enhancement%20Network%20for%20DETR-based%20HOI%20Detection%0AAuthor%3A%20Zhehao%20Li%20and%20Chong%20Wang%20and%20Yi%20Chen%20and%20Yinghao%20Lu%20and%20Jiangbo%20Qian%20and%20Jiong%20Wang%20and%20Jiafei%20Wu%0AAbstract%3A%20%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20focuses%20on%20localizing%20human-object%0Apairs%20and%20recognizing%20their%20interactions.%20Recently%2C%20the%20DETR-based%20framework%0Ahas%20been%20widely%20adopted%20in%20HOI%20detection.%20In%20DETR-based%20HOI%20models%2C%20queries%0Awith%20clear%20meaning%20are%20crucial%20for%20accurately%20detecting%20HOIs.%20However%2C%20prior%0Aworks%20have%20typically%20relied%20on%20randomly%20initialized%20queries%2C%20leading%20to%20vague%0Arepresentations%20that%20limit%20the%20model%27s%20effectiveness.%20Meanwhile%2C%20humans%20in%20the%0AHOI%20categories%20are%20fixed%2C%20while%20objects%20and%20their%20interactions%20are%20variable.%0ATherefore%2C%20we%20propose%20a%20Dual%20Query%20Enhancement%20Network%20%28DQEN%29%20to%20enhance%20object%0Aand%20interaction%20queries.%20Specifically%2C%20object%20queries%20are%20enhanced%20with%0Aobject-aware%20encoder%20features%2C%20enabling%20the%20model%20to%20focus%20more%20effectively%20on%0Ahumans%20interacting%20with%20objects%20in%20an%20object-aware%20way.%20On%20the%20other%20hand%2C%20we%0Adesign%20a%20novel%20Interaction%20Semantic%20Fusion%20module%20to%20exploit%20the%20HOI%20candidates%0Athat%20are%20promoted%20by%20the%20CLIP%20model.%20Semantic%20features%20are%20extracted%20to%20enhance%0Athe%20initialization%20of%20interaction%20queries%2C%20thereby%20improving%20the%20model%27s%0Aability%20to%20understand%20interactions.%20Furthermore%2C%20we%20introduce%20an%20Auxiliary%0APrediction%20Unit%20aimed%20at%20improving%20the%20representation%20of%20interaction%20features.%0AOur%20proposed%20method%20achieves%20competitive%20performance%20on%20both%20the%20HICO-Det%20and%0Athe%20V-COCO%20datasets.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/lzzhhh1019/DQEN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDQEN%253A%2520Dual%2520Query%2520Enhancement%2520Network%2520for%2520DETR-based%2520HOI%2520Detection%26entry.906535625%3DZhehao%2520Li%2520and%2520Chong%2520Wang%2520and%2520Yi%2520Chen%2520and%2520Yinghao%2520Lu%2520and%2520Jiangbo%2520Qian%2520and%2520Jiong%2520Wang%2520and%2520Jiafei%2520Wu%26entry.1292438233%3D%2520%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520detection%2520focuses%2520on%2520localizing%2520human-object%250Apairs%2520and%2520recognizing%2520their%2520interactions.%2520Recently%252C%2520the%2520DETR-based%2520framework%250Ahas%2520been%2520widely%2520adopted%2520in%2520HOI%2520detection.%2520In%2520DETR-based%2520HOI%2520models%252C%2520queries%250Awith%2520clear%2520meaning%2520are%2520crucial%2520for%2520accurately%2520detecting%2520HOIs.%2520However%252C%2520prior%250Aworks%2520have%2520typically%2520relied%2520on%2520randomly%2520initialized%2520queries%252C%2520leading%2520to%2520vague%250Arepresentations%2520that%2520limit%2520the%2520model%2527s%2520effectiveness.%2520Meanwhile%252C%2520humans%2520in%2520the%250AHOI%2520categories%2520are%2520fixed%252C%2520while%2520objects%2520and%2520their%2520interactions%2520are%2520variable.%250ATherefore%252C%2520we%2520propose%2520a%2520Dual%2520Query%2520Enhancement%2520Network%2520%2528DQEN%2529%2520to%2520enhance%2520object%250Aand%2520interaction%2520queries.%2520Specifically%252C%2520object%2520queries%2520are%2520enhanced%2520with%250Aobject-aware%2520encoder%2520features%252C%2520enabling%2520the%2520model%2520to%2520focus%2520more%2520effectively%2520on%250Ahumans%2520interacting%2520with%2520objects%2520in%2520an%2520object-aware%2520way.%2520On%2520the%2520other%2520hand%252C%2520we%250Adesign%2520a%2520novel%2520Interaction%2520Semantic%2520Fusion%2520module%2520to%2520exploit%2520the%2520HOI%2520candidates%250Athat%2520are%2520promoted%2520by%2520the%2520CLIP%2520model.%2520Semantic%2520features%2520are%2520extracted%2520to%2520enhance%250Athe%2520initialization%2520of%2520interaction%2520queries%252C%2520thereby%2520improving%2520the%2520model%2527s%250Aability%2520to%2520understand%2520interactions.%2520Furthermore%252C%2520we%2520introduce%2520an%2520Auxiliary%250APrediction%2520Unit%2520aimed%2520at%2520improving%2520the%2520representation%2520of%2520interaction%2520features.%250AOur%2520proposed%2520method%2520achieves%2520competitive%2520performance%2520on%2520both%2520the%2520HICO-Det%2520and%250Athe%2520V-COCO%2520datasets.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lzzhhh1019/DQEN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DQEN%3A%20Dual%20Query%20Enhancement%20Network%20for%20DETR-based%20HOI%20Detection&entry.906535625=Zhehao%20Li%20and%20Chong%20Wang%20and%20Yi%20Chen%20and%20Yinghao%20Lu%20and%20Jiangbo%20Qian%20and%20Jiong%20Wang%20and%20Jiafei%20Wu&entry.1292438233=%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20focuses%20on%20localizing%20human-object%0Apairs%20and%20recognizing%20their%20interactions.%20Recently%2C%20the%20DETR-based%20framework%0Ahas%20been%20widely%20adopted%20in%20HOI%20detection.%20In%20DETR-based%20HOI%20models%2C%20queries%0Awith%20clear%20meaning%20are%20crucial%20for%20accurately%20detecting%20HOIs.%20However%2C%20prior%0Aworks%20have%20typically%20relied%20on%20randomly%20initialized%20queries%2C%20leading%20to%20vague%0Arepresentations%20that%20limit%20the%20model%27s%20effectiveness.%20Meanwhile%2C%20humans%20in%20the%0AHOI%20categories%20are%20fixed%2C%20while%20objects%20and%20their%20interactions%20are%20variable.%0ATherefore%2C%20we%20propose%20a%20Dual%20Query%20Enhancement%20Network%20%28DQEN%29%20to%20enhance%20object%0Aand%20interaction%20queries.%20Specifically%2C%20object%20queries%20are%20enhanced%20with%0Aobject-aware%20encoder%20features%2C%20enabling%20the%20model%20to%20focus%20more%20effectively%20on%0Ahumans%20interacting%20with%20objects%20in%20an%20object-aware%20way.%20On%20the%20other%20hand%2C%20we%0Adesign%20a%20novel%20Interaction%20Semantic%20Fusion%20module%20to%20exploit%20the%20HOI%20candidates%0Athat%20are%20promoted%20by%20the%20CLIP%20model.%20Semantic%20features%20are%20extracted%20to%20enhance%0Athe%20initialization%20of%20interaction%20queries%2C%20thereby%20improving%20the%20model%27s%0Aability%20to%20understand%20interactions.%20Furthermore%2C%20we%20introduce%20an%20Auxiliary%0APrediction%20Unit%20aimed%20at%20improving%20the%20representation%20of%20interaction%20features.%0AOur%20proposed%20method%20achieves%20competitive%20performance%20on%20both%20the%20HICO-Det%20and%0Athe%20V-COCO%20datasets.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/lzzhhh1019/DQEN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18896v1&entry.124074799=Read"},
{"title": "Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for\n  Multimodal Medical VQA", "author": "Karishma Thakrar and Shreyas Basavatia and Akshay Daftardar", "abstract": "  Dermatological care via telemedicine often lacks the rich context of\nin-person visits. Clinicians must make diagnoses based on a handful of images\nand brief descriptions, without the benefit of physical exams, second opinions,\nor reference materials. While many medical AI systems attempt to bridge these\ngaps with domain-specific fine-tuning, this work hypothesized that mimicking\nclinical reasoning processes could offer a more effective path forward. This\nstudy tested seven vision-language models on medical visual question answering\nacross six configurations: baseline models, fine-tuned variants, and both\naugmented with either reasoning layers that combine multiple model\nperspectives, analogous to peer consultation, or retrieval-augmented generation\nthat incorporates medical literature at inference time, serving a role similar\nto reference-checking. While fine-tuning degraded performance in four of seven\nmodels with an average 30% decrease, baseline models collapsed on test data.\nClinical-inspired architectures, meanwhile, achieved up to 70% accuracy,\nmaintaining performance on unseen data while generating explainable,\nliterature-grounded outputs critical for clinical adoption. These findings\ndemonstrate that medical AI succeeds by reconstructing the collaborative and\nevidence-based practices fundamental to clinical diagnosis.\n", "link": "http://arxiv.org/abs/2507.05520v3", "date": "2025-08-26", "relevancy": 2.1443, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Architecting%20Clinical%20Collaboration%3A%20Multi-Agent%20Reasoning%20Systems%20for%0A%20%20Multimodal%20Medical%20VQA&body=Title%3A%20Architecting%20Clinical%20Collaboration%3A%20Multi-Agent%20Reasoning%20Systems%20for%0A%20%20Multimodal%20Medical%20VQA%0AAuthor%3A%20Karishma%20Thakrar%20and%20Shreyas%20Basavatia%20and%20Akshay%20Daftardar%0AAbstract%3A%20%20%20Dermatological%20care%20via%20telemedicine%20often%20lacks%20the%20rich%20context%20of%0Ain-person%20visits.%20Clinicians%20must%20make%20diagnoses%20based%20on%20a%20handful%20of%20images%0Aand%20brief%20descriptions%2C%20without%20the%20benefit%20of%20physical%20exams%2C%20second%20opinions%2C%0Aor%20reference%20materials.%20While%20many%20medical%20AI%20systems%20attempt%20to%20bridge%20these%0Agaps%20with%20domain-specific%20fine-tuning%2C%20this%20work%20hypothesized%20that%20mimicking%0Aclinical%20reasoning%20processes%20could%20offer%20a%20more%20effective%20path%20forward.%20This%0Astudy%20tested%20seven%20vision-language%20models%20on%20medical%20visual%20question%20answering%0Aacross%20six%20configurations%3A%20baseline%20models%2C%20fine-tuned%20variants%2C%20and%20both%0Aaugmented%20with%20either%20reasoning%20layers%20that%20combine%20multiple%20model%0Aperspectives%2C%20analogous%20to%20peer%20consultation%2C%20or%20retrieval-augmented%20generation%0Athat%20incorporates%20medical%20literature%20at%20inference%20time%2C%20serving%20a%20role%20similar%0Ato%20reference-checking.%20While%20fine-tuning%20degraded%20performance%20in%20four%20of%20seven%0Amodels%20with%20an%20average%2030%25%20decrease%2C%20baseline%20models%20collapsed%20on%20test%20data.%0AClinical-inspired%20architectures%2C%20meanwhile%2C%20achieved%20up%20to%2070%25%20accuracy%2C%0Amaintaining%20performance%20on%20unseen%20data%20while%20generating%20explainable%2C%0Aliterature-grounded%20outputs%20critical%20for%20clinical%20adoption.%20These%20findings%0Ademonstrate%20that%20medical%20AI%20succeeds%20by%20reconstructing%20the%20collaborative%20and%0Aevidence-based%20practices%20fundamental%20to%20clinical%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05520v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchitecting%2520Clinical%2520Collaboration%253A%2520Multi-Agent%2520Reasoning%2520Systems%2520for%250A%2520%2520Multimodal%2520Medical%2520VQA%26entry.906535625%3DKarishma%2520Thakrar%2520and%2520Shreyas%2520Basavatia%2520and%2520Akshay%2520Daftardar%26entry.1292438233%3D%2520%2520Dermatological%2520care%2520via%2520telemedicine%2520often%2520lacks%2520the%2520rich%2520context%2520of%250Ain-person%2520visits.%2520Clinicians%2520must%2520make%2520diagnoses%2520based%2520on%2520a%2520handful%2520of%2520images%250Aand%2520brief%2520descriptions%252C%2520without%2520the%2520benefit%2520of%2520physical%2520exams%252C%2520second%2520opinions%252C%250Aor%2520reference%2520materials.%2520While%2520many%2520medical%2520AI%2520systems%2520attempt%2520to%2520bridge%2520these%250Agaps%2520with%2520domain-specific%2520fine-tuning%252C%2520this%2520work%2520hypothesized%2520that%2520mimicking%250Aclinical%2520reasoning%2520processes%2520could%2520offer%2520a%2520more%2520effective%2520path%2520forward.%2520This%250Astudy%2520tested%2520seven%2520vision-language%2520models%2520on%2520medical%2520visual%2520question%2520answering%250Aacross%2520six%2520configurations%253A%2520baseline%2520models%252C%2520fine-tuned%2520variants%252C%2520and%2520both%250Aaugmented%2520with%2520either%2520reasoning%2520layers%2520that%2520combine%2520multiple%2520model%250Aperspectives%252C%2520analogous%2520to%2520peer%2520consultation%252C%2520or%2520retrieval-augmented%2520generation%250Athat%2520incorporates%2520medical%2520literature%2520at%2520inference%2520time%252C%2520serving%2520a%2520role%2520similar%250Ato%2520reference-checking.%2520While%2520fine-tuning%2520degraded%2520performance%2520in%2520four%2520of%2520seven%250Amodels%2520with%2520an%2520average%252030%2525%2520decrease%252C%2520baseline%2520models%2520collapsed%2520on%2520test%2520data.%250AClinical-inspired%2520architectures%252C%2520meanwhile%252C%2520achieved%2520up%2520to%252070%2525%2520accuracy%252C%250Amaintaining%2520performance%2520on%2520unseen%2520data%2520while%2520generating%2520explainable%252C%250Aliterature-grounded%2520outputs%2520critical%2520for%2520clinical%2520adoption.%2520These%2520findings%250Ademonstrate%2520that%2520medical%2520AI%2520succeeds%2520by%2520reconstructing%2520the%2520collaborative%2520and%250Aevidence-based%2520practices%2520fundamental%2520to%2520clinical%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05520v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Architecting%20Clinical%20Collaboration%3A%20Multi-Agent%20Reasoning%20Systems%20for%0A%20%20Multimodal%20Medical%20VQA&entry.906535625=Karishma%20Thakrar%20and%20Shreyas%20Basavatia%20and%20Akshay%20Daftardar&entry.1292438233=%20%20Dermatological%20care%20via%20telemedicine%20often%20lacks%20the%20rich%20context%20of%0Ain-person%20visits.%20Clinicians%20must%20make%20diagnoses%20based%20on%20a%20handful%20of%20images%0Aand%20brief%20descriptions%2C%20without%20the%20benefit%20of%20physical%20exams%2C%20second%20opinions%2C%0Aor%20reference%20materials.%20While%20many%20medical%20AI%20systems%20attempt%20to%20bridge%20these%0Agaps%20with%20domain-specific%20fine-tuning%2C%20this%20work%20hypothesized%20that%20mimicking%0Aclinical%20reasoning%20processes%20could%20offer%20a%20more%20effective%20path%20forward.%20This%0Astudy%20tested%20seven%20vision-language%20models%20on%20medical%20visual%20question%20answering%0Aacross%20six%20configurations%3A%20baseline%20models%2C%20fine-tuned%20variants%2C%20and%20both%0Aaugmented%20with%20either%20reasoning%20layers%20that%20combine%20multiple%20model%0Aperspectives%2C%20analogous%20to%20peer%20consultation%2C%20or%20retrieval-augmented%20generation%0Athat%20incorporates%20medical%20literature%20at%20inference%20time%2C%20serving%20a%20role%20similar%0Ato%20reference-checking.%20While%20fine-tuning%20degraded%20performance%20in%20four%20of%20seven%0Amodels%20with%20an%20average%2030%25%20decrease%2C%20baseline%20models%20collapsed%20on%20test%20data.%0AClinical-inspired%20architectures%2C%20meanwhile%2C%20achieved%20up%20to%2070%25%20accuracy%2C%0Amaintaining%20performance%20on%20unseen%20data%20while%20generating%20explainable%2C%0Aliterature-grounded%20outputs%20critical%20for%20clinical%20adoption.%20These%20findings%0Ademonstrate%20that%20medical%20AI%20succeeds%20by%20reconstructing%20the%20collaborative%20and%0Aevidence-based%20practices%20fundamental%20to%20clinical%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05520v3&entry.124074799=Read"},
{"title": "Energy-Based Flow Matching for Generating 3D Molecular Structure", "author": "Wenyin Zhou and Christopher Iliffe Sprague and Vsevolod Viliuga and Matteo Tadiello and Arne Elofsson and Hossein Azizpour", "abstract": "  Molecular structure generation is a fundamental problem that involves\ndetermining the 3D positions of molecules' constituents. It has crucial\nbiological applications, such as molecular docking, protein folding, and\nmolecular design. Recent advances in generative modeling, such as diffusion\nmodels and flow matching, have made great progress on these tasks by modeling\nmolecular conformations as a distribution. In this work, we focus on flow\nmatching and adopt an energy-based perspective to improve training and\ninference of structure generation models. Our view results in a mapping\nfunction, represented by a deep network, that is directly learned to\n\\textit{iteratively} map random configurations, i.e. samples from the source\ndistribution, to target structures, i.e. points in the data manifold. This\nyields a conceptually simple and empirically effective flow matching setup that\nis theoretically justified and has interesting connections to fundamental\nproperties such as idempotency and stability, as well as the empirically useful\ntechniques such as structure refinement in AlphaFold. Experiments on protein\ndocking as well as protein backbone generation consistently demonstrate the\nmethod's effectiveness, where it outperforms recent baselines of\ntask-associated flow matching and diffusion models, using a similar\ncomputational budget.\n", "link": "http://arxiv.org/abs/2508.18949v1", "date": "2025-08-26", "relevancy": 2.1318, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5674}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5311}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Based%20Flow%20Matching%20for%20Generating%203D%20Molecular%20Structure&body=Title%3A%20Energy-Based%20Flow%20Matching%20for%20Generating%203D%20Molecular%20Structure%0AAuthor%3A%20Wenyin%20Zhou%20and%20Christopher%20Iliffe%20Sprague%20and%20Vsevolod%20Viliuga%20and%20Matteo%20Tadiello%20and%20Arne%20Elofsson%20and%20Hossein%20Azizpour%0AAbstract%3A%20%20%20Molecular%20structure%20generation%20is%20a%20fundamental%20problem%20that%20involves%0Adetermining%20the%203D%20positions%20of%20molecules%27%20constituents.%20It%20has%20crucial%0Abiological%20applications%2C%20such%20as%20molecular%20docking%2C%20protein%20folding%2C%20and%0Amolecular%20design.%20Recent%20advances%20in%20generative%20modeling%2C%20such%20as%20diffusion%0Amodels%20and%20flow%20matching%2C%20have%20made%20great%20progress%20on%20these%20tasks%20by%20modeling%0Amolecular%20conformations%20as%20a%20distribution.%20In%20this%20work%2C%20we%20focus%20on%20flow%0Amatching%20and%20adopt%20an%20energy-based%20perspective%20to%20improve%20training%20and%0Ainference%20of%20structure%20generation%20models.%20Our%20view%20results%20in%20a%20mapping%0Afunction%2C%20represented%20by%20a%20deep%20network%2C%20that%20is%20directly%20learned%20to%0A%5Ctextit%7Biteratively%7D%20map%20random%20configurations%2C%20i.e.%20samples%20from%20the%20source%0Adistribution%2C%20to%20target%20structures%2C%20i.e.%20points%20in%20the%20data%20manifold.%20This%0Ayields%20a%20conceptually%20simple%20and%20empirically%20effective%20flow%20matching%20setup%20that%0Ais%20theoretically%20justified%20and%20has%20interesting%20connections%20to%20fundamental%0Aproperties%20such%20as%20idempotency%20and%20stability%2C%20as%20well%20as%20the%20empirically%20useful%0Atechniques%20such%20as%20structure%20refinement%20in%20AlphaFold.%20Experiments%20on%20protein%0Adocking%20as%20well%20as%20protein%20backbone%20generation%20consistently%20demonstrate%20the%0Amethod%27s%20effectiveness%2C%20where%20it%20outperforms%20recent%20baselines%20of%0Atask-associated%20flow%20matching%20and%20diffusion%20models%2C%20using%20a%20similar%0Acomputational%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Based%2520Flow%2520Matching%2520for%2520Generating%25203D%2520Molecular%2520Structure%26entry.906535625%3DWenyin%2520Zhou%2520and%2520Christopher%2520Iliffe%2520Sprague%2520and%2520Vsevolod%2520Viliuga%2520and%2520Matteo%2520Tadiello%2520and%2520Arne%2520Elofsson%2520and%2520Hossein%2520Azizpour%26entry.1292438233%3D%2520%2520Molecular%2520structure%2520generation%2520is%2520a%2520fundamental%2520problem%2520that%2520involves%250Adetermining%2520the%25203D%2520positions%2520of%2520molecules%2527%2520constituents.%2520It%2520has%2520crucial%250Abiological%2520applications%252C%2520such%2520as%2520molecular%2520docking%252C%2520protein%2520folding%252C%2520and%250Amolecular%2520design.%2520Recent%2520advances%2520in%2520generative%2520modeling%252C%2520such%2520as%2520diffusion%250Amodels%2520and%2520flow%2520matching%252C%2520have%2520made%2520great%2520progress%2520on%2520these%2520tasks%2520by%2520modeling%250Amolecular%2520conformations%2520as%2520a%2520distribution.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520flow%250Amatching%2520and%2520adopt%2520an%2520energy-based%2520perspective%2520to%2520improve%2520training%2520and%250Ainference%2520of%2520structure%2520generation%2520models.%2520Our%2520view%2520results%2520in%2520a%2520mapping%250Afunction%252C%2520represented%2520by%2520a%2520deep%2520network%252C%2520that%2520is%2520directly%2520learned%2520to%250A%255Ctextit%257Biteratively%257D%2520map%2520random%2520configurations%252C%2520i.e.%2520samples%2520from%2520the%2520source%250Adistribution%252C%2520to%2520target%2520structures%252C%2520i.e.%2520points%2520in%2520the%2520data%2520manifold.%2520This%250Ayields%2520a%2520conceptually%2520simple%2520and%2520empirically%2520effective%2520flow%2520matching%2520setup%2520that%250Ais%2520theoretically%2520justified%2520and%2520has%2520interesting%2520connections%2520to%2520fundamental%250Aproperties%2520such%2520as%2520idempotency%2520and%2520stability%252C%2520as%2520well%2520as%2520the%2520empirically%2520useful%250Atechniques%2520such%2520as%2520structure%2520refinement%2520in%2520AlphaFold.%2520Experiments%2520on%2520protein%250Adocking%2520as%2520well%2520as%2520protein%2520backbone%2520generation%2520consistently%2520demonstrate%2520the%250Amethod%2527s%2520effectiveness%252C%2520where%2520it%2520outperforms%2520recent%2520baselines%2520of%250Atask-associated%2520flow%2520matching%2520and%2520diffusion%2520models%252C%2520using%2520a%2520similar%250Acomputational%2520budget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Based%20Flow%20Matching%20for%20Generating%203D%20Molecular%20Structure&entry.906535625=Wenyin%20Zhou%20and%20Christopher%20Iliffe%20Sprague%20and%20Vsevolod%20Viliuga%20and%20Matteo%20Tadiello%20and%20Arne%20Elofsson%20and%20Hossein%20Azizpour&entry.1292438233=%20%20Molecular%20structure%20generation%20is%20a%20fundamental%20problem%20that%20involves%0Adetermining%20the%203D%20positions%20of%20molecules%27%20constituents.%20It%20has%20crucial%0Abiological%20applications%2C%20such%20as%20molecular%20docking%2C%20protein%20folding%2C%20and%0Amolecular%20design.%20Recent%20advances%20in%20generative%20modeling%2C%20such%20as%20diffusion%0Amodels%20and%20flow%20matching%2C%20have%20made%20great%20progress%20on%20these%20tasks%20by%20modeling%0Amolecular%20conformations%20as%20a%20distribution.%20In%20this%20work%2C%20we%20focus%20on%20flow%0Amatching%20and%20adopt%20an%20energy-based%20perspective%20to%20improve%20training%20and%0Ainference%20of%20structure%20generation%20models.%20Our%20view%20results%20in%20a%20mapping%0Afunction%2C%20represented%20by%20a%20deep%20network%2C%20that%20is%20directly%20learned%20to%0A%5Ctextit%7Biteratively%7D%20map%20random%20configurations%2C%20i.e.%20samples%20from%20the%20source%0Adistribution%2C%20to%20target%20structures%2C%20i.e.%20points%20in%20the%20data%20manifold.%20This%0Ayields%20a%20conceptually%20simple%20and%20empirically%20effective%20flow%20matching%20setup%20that%0Ais%20theoretically%20justified%20and%20has%20interesting%20connections%20to%20fundamental%0Aproperties%20such%20as%20idempotency%20and%20stability%2C%20as%20well%20as%20the%20empirically%20useful%0Atechniques%20such%20as%20structure%20refinement%20in%20AlphaFold.%20Experiments%20on%20protein%0Adocking%20as%20well%20as%20protein%20backbone%20generation%20consistently%20demonstrate%20the%0Amethod%27s%20effectiveness%2C%20where%20it%20outperforms%20recent%20baselines%20of%0Atask-associated%20flow%20matching%20and%20diffusion%20models%2C%20using%20a%20similar%0Acomputational%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18949v1&entry.124074799=Read"},
{"title": "Interactive Evaluation of Large Language Models for Multi-Requirement\n  Software Engineering Tasks", "author": "Dimitrios Rontogiannis and Maxime Peyrard and Nicolas Baldwin and Martin Josifoski and Robert West and Dimitrios Gunopulos", "abstract": "  Standard single-turn, static benchmarks fall short in evaluating the nuanced\ncapabilities of Large Language Models (LLMs) on complex tasks such as software\nengineering. In this work, we propose a novel interactive evaluation framework\nthat assesses LLMs on multi-requirement programming tasks through structured,\nfeedback-driven dialogue. Each task is modeled as a requirement dependency\ngraph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides\nminimal, targeted hints to an ``interviewee'' model to help correct errors and\nfulfill target constraints. This dynamic protocol enables fine-grained\ndiagnostic insights into model behavior, uncovering strengths and systematic\nweaknesses that static benchmarks fail to measure. We build on DevAI, a\nbenchmark of 55 curated programming tasks, by adding ground-truth solutions and\nevaluating the relevance and utility of interviewer hints through expert\nannotation. Our results highlight the importance of dynamic evaluation in\nadvancing the development of collaborative code-generating agents.\n", "link": "http://arxiv.org/abs/2508.18905v1", "date": "2025-08-26", "relevancy": 2.1293, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Evaluation%20of%20Large%20Language%20Models%20for%20Multi-Requirement%0A%20%20Software%20Engineering%20Tasks&body=Title%3A%20Interactive%20Evaluation%20of%20Large%20Language%20Models%20for%20Multi-Requirement%0A%20%20Software%20Engineering%20Tasks%0AAuthor%3A%20Dimitrios%20Rontogiannis%20and%20Maxime%20Peyrard%20and%20Nicolas%20Baldwin%20and%20Martin%20Josifoski%20and%20Robert%20West%20and%20Dimitrios%20Gunopulos%0AAbstract%3A%20%20%20Standard%20single-turn%2C%20static%20benchmarks%20fall%20short%20in%20evaluating%20the%20nuanced%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20on%20complex%20tasks%20such%20as%20software%0Aengineering.%20In%20this%20work%2C%20we%20propose%20a%20novel%20interactive%20evaluation%20framework%0Athat%20assesses%20LLMs%20on%20multi-requirement%20programming%20tasks%20through%20structured%2C%0Afeedback-driven%20dialogue.%20Each%20task%20is%20modeled%20as%20a%20requirement%20dependency%0Agraph%2C%20and%20an%20%60%60interviewer%27%27%20LLM%2C%20aware%20of%20the%20ground-truth%20solution%2C%20provides%0Aminimal%2C%20targeted%20hints%20to%20an%20%60%60interviewee%27%27%20model%20to%20help%20correct%20errors%20and%0Afulfill%20target%20constraints.%20This%20dynamic%20protocol%20enables%20fine-grained%0Adiagnostic%20insights%20into%20model%20behavior%2C%20uncovering%20strengths%20and%20systematic%0Aweaknesses%20that%20static%20benchmarks%20fail%20to%20measure.%20We%20build%20on%20DevAI%2C%20a%0Abenchmark%20of%2055%20curated%20programming%20tasks%2C%20by%20adding%20ground-truth%20solutions%20and%0Aevaluating%20the%20relevance%20and%20utility%20of%20interviewer%20hints%20through%20expert%0Aannotation.%20Our%20results%20highlight%20the%20importance%20of%20dynamic%20evaluation%20in%0Aadvancing%20the%20development%20of%20collaborative%20code-generating%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Evaluation%2520of%2520Large%2520Language%2520Models%2520for%2520Multi-Requirement%250A%2520%2520Software%2520Engineering%2520Tasks%26entry.906535625%3DDimitrios%2520Rontogiannis%2520and%2520Maxime%2520Peyrard%2520and%2520Nicolas%2520Baldwin%2520and%2520Martin%2520Josifoski%2520and%2520Robert%2520West%2520and%2520Dimitrios%2520Gunopulos%26entry.1292438233%3D%2520%2520Standard%2520single-turn%252C%2520static%2520benchmarks%2520fall%2520short%2520in%2520evaluating%2520the%2520nuanced%250Acapabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520complex%2520tasks%2520such%2520as%2520software%250Aengineering.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520interactive%2520evaluation%2520framework%250Athat%2520assesses%2520LLMs%2520on%2520multi-requirement%2520programming%2520tasks%2520through%2520structured%252C%250Afeedback-driven%2520dialogue.%2520Each%2520task%2520is%2520modeled%2520as%2520a%2520requirement%2520dependency%250Agraph%252C%2520and%2520an%2520%2560%2560interviewer%2527%2527%2520LLM%252C%2520aware%2520of%2520the%2520ground-truth%2520solution%252C%2520provides%250Aminimal%252C%2520targeted%2520hints%2520to%2520an%2520%2560%2560interviewee%2527%2527%2520model%2520to%2520help%2520correct%2520errors%2520and%250Afulfill%2520target%2520constraints.%2520This%2520dynamic%2520protocol%2520enables%2520fine-grained%250Adiagnostic%2520insights%2520into%2520model%2520behavior%252C%2520uncovering%2520strengths%2520and%2520systematic%250Aweaknesses%2520that%2520static%2520benchmarks%2520fail%2520to%2520measure.%2520We%2520build%2520on%2520DevAI%252C%2520a%250Abenchmark%2520of%252055%2520curated%2520programming%2520tasks%252C%2520by%2520adding%2520ground-truth%2520solutions%2520and%250Aevaluating%2520the%2520relevance%2520and%2520utility%2520of%2520interviewer%2520hints%2520through%2520expert%250Aannotation.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520dynamic%2520evaluation%2520in%250Aadvancing%2520the%2520development%2520of%2520collaborative%2520code-generating%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Evaluation%20of%20Large%20Language%20Models%20for%20Multi-Requirement%0A%20%20Software%20Engineering%20Tasks&entry.906535625=Dimitrios%20Rontogiannis%20and%20Maxime%20Peyrard%20and%20Nicolas%20Baldwin%20and%20Martin%20Josifoski%20and%20Robert%20West%20and%20Dimitrios%20Gunopulos&entry.1292438233=%20%20Standard%20single-turn%2C%20static%20benchmarks%20fall%20short%20in%20evaluating%20the%20nuanced%0Acapabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20on%20complex%20tasks%20such%20as%20software%0Aengineering.%20In%20this%20work%2C%20we%20propose%20a%20novel%20interactive%20evaluation%20framework%0Athat%20assesses%20LLMs%20on%20multi-requirement%20programming%20tasks%20through%20structured%2C%0Afeedback-driven%20dialogue.%20Each%20task%20is%20modeled%20as%20a%20requirement%20dependency%0Agraph%2C%20and%20an%20%60%60interviewer%27%27%20LLM%2C%20aware%20of%20the%20ground-truth%20solution%2C%20provides%0Aminimal%2C%20targeted%20hints%20to%20an%20%60%60interviewee%27%27%20model%20to%20help%20correct%20errors%20and%0Afulfill%20target%20constraints.%20This%20dynamic%20protocol%20enables%20fine-grained%0Adiagnostic%20insights%20into%20model%20behavior%2C%20uncovering%20strengths%20and%20systematic%0Aweaknesses%20that%20static%20benchmarks%20fail%20to%20measure.%20We%20build%20on%20DevAI%2C%20a%0Abenchmark%20of%2055%20curated%20programming%20tasks%2C%20by%20adding%20ground-truth%20solutions%20and%0Aevaluating%20the%20relevance%20and%20utility%20of%20interviewer%20hints%20through%20expert%0Aannotation.%20Our%20results%20highlight%20the%20importance%20of%20dynamic%20evaluation%20in%0Aadvancing%20the%20development%20of%20collaborative%20code-generating%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18905v1&entry.124074799=Read"},
{"title": "Automatic Prompt Optimization with Prompt Distillation", "author": "Viktor N. Zhuravlev and Artur R. Khairullin and Ernest A. Dyagin and Alena N. Sitkina and Nikita I. Kulin", "abstract": "  Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.\n", "link": "http://arxiv.org/abs/2508.18992v1", "date": "2025-08-26", "relevancy": 2.1218, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4276}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4231}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Prompt%20Optimization%20with%20Prompt%20Distillation&body=Title%3A%20Automatic%20Prompt%20Optimization%20with%20Prompt%20Distillation%0AAuthor%3A%20Viktor%20N.%20Zhuravlev%20and%20Artur%20R.%20Khairullin%20and%20Ernest%20A.%20Dyagin%20and%20Alena%20N.%20Sitkina%20and%20Nikita%20I.%20Kulin%0AAbstract%3A%20%20%20Autoprompting%20is%20the%20process%20of%20automatically%20selecting%20optimized%20prompts%20for%0Alanguage%20models%2C%20which%20is%20gaining%20popularity%20due%20to%20the%20rapid%20development%20of%0Aprompt%20engineering%20driven%20by%20extensive%20research%20in%20the%20field%20of%20large%20language%0Amodels%20%28LLMs%29.%20This%20paper%20presents%20DistillPrompt%20--%20a%20novel%20autoprompting%0Amethod%20based%20on%20large%20language%20models%20that%20employs%20a%20multi-stage%20integration%20of%0Atask-specific%20information%20into%20prompts%20using%20training%20data.%20DistillPrompt%0Autilizes%20distillation%2C%20compression%2C%20and%20aggregation%20operations%20to%20explore%20the%0Aprompt%20space%20more%20thoroughly.%20The%20method%20was%20tested%20on%20different%20datasets%20for%0Atext%20classification%20and%20generation%20tasks%20using%20the%20t-lite-instruct-0.1%20language%0Amodel.%20The%20results%20demonstrate%20a%20significant%20average%20improvement%20%28e.g.%2C%2020.12%25%0Aacross%20the%20entire%20dataset%20compared%20to%20Grips%29%20in%20key%20metrics%20over%20existing%0Amethods%20in%20the%20field%2C%20establishing%20DistillPrompt%20as%20one%20of%20the%20most%20effective%0Anon-gradient%20approaches%20in%20autoprompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Prompt%2520Optimization%2520with%2520Prompt%2520Distillation%26entry.906535625%3DViktor%2520N.%2520Zhuravlev%2520and%2520Artur%2520R.%2520Khairullin%2520and%2520Ernest%2520A.%2520Dyagin%2520and%2520Alena%2520N.%2520Sitkina%2520and%2520Nikita%2520I.%2520Kulin%26entry.1292438233%3D%2520%2520Autoprompting%2520is%2520the%2520process%2520of%2520automatically%2520selecting%2520optimized%2520prompts%2520for%250Alanguage%2520models%252C%2520which%2520is%2520gaining%2520popularity%2520due%2520to%2520the%2520rapid%2520development%2520of%250Aprompt%2520engineering%2520driven%2520by%2520extensive%2520research%2520in%2520the%2520field%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529.%2520This%2520paper%2520presents%2520DistillPrompt%2520--%2520a%2520novel%2520autoprompting%250Amethod%2520based%2520on%2520large%2520language%2520models%2520that%2520employs%2520a%2520multi-stage%2520integration%2520of%250Atask-specific%2520information%2520into%2520prompts%2520using%2520training%2520data.%2520DistillPrompt%250Autilizes%2520distillation%252C%2520compression%252C%2520and%2520aggregation%2520operations%2520to%2520explore%2520the%250Aprompt%2520space%2520more%2520thoroughly.%2520The%2520method%2520was%2520tested%2520on%2520different%2520datasets%2520for%250Atext%2520classification%2520and%2520generation%2520tasks%2520using%2520the%2520t-lite-instruct-0.1%2520language%250Amodel.%2520The%2520results%2520demonstrate%2520a%2520significant%2520average%2520improvement%2520%2528e.g.%252C%252020.12%2525%250Aacross%2520the%2520entire%2520dataset%2520compared%2520to%2520Grips%2529%2520in%2520key%2520metrics%2520over%2520existing%250Amethods%2520in%2520the%2520field%252C%2520establishing%2520DistillPrompt%2520as%2520one%2520of%2520the%2520most%2520effective%250Anon-gradient%2520approaches%2520in%2520autoprompting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Prompt%20Optimization%20with%20Prompt%20Distillation&entry.906535625=Viktor%20N.%20Zhuravlev%20and%20Artur%20R.%20Khairullin%20and%20Ernest%20A.%20Dyagin%20and%20Alena%20N.%20Sitkina%20and%20Nikita%20I.%20Kulin&entry.1292438233=%20%20Autoprompting%20is%20the%20process%20of%20automatically%20selecting%20optimized%20prompts%20for%0Alanguage%20models%2C%20which%20is%20gaining%20popularity%20due%20to%20the%20rapid%20development%20of%0Aprompt%20engineering%20driven%20by%20extensive%20research%20in%20the%20field%20of%20large%20language%0Amodels%20%28LLMs%29.%20This%20paper%20presents%20DistillPrompt%20--%20a%20novel%20autoprompting%0Amethod%20based%20on%20large%20language%20models%20that%20employs%20a%20multi-stage%20integration%20of%0Atask-specific%20information%20into%20prompts%20using%20training%20data.%20DistillPrompt%0Autilizes%20distillation%2C%20compression%2C%20and%20aggregation%20operations%20to%20explore%20the%0Aprompt%20space%20more%20thoroughly.%20The%20method%20was%20tested%20on%20different%20datasets%20for%0Atext%20classification%20and%20generation%20tasks%20using%20the%20t-lite-instruct-0.1%20language%0Amodel.%20The%20results%20demonstrate%20a%20significant%20average%20improvement%20%28e.g.%2C%2020.12%25%0Aacross%20the%20entire%20dataset%20compared%20to%20Grips%29%20in%20key%20metrics%20over%20existing%0Amethods%20in%20the%20field%2C%20establishing%20DistillPrompt%20as%20one%20of%20the%20most%20effective%0Anon-gradient%20approaches%20in%20autoprompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18992v1&entry.124074799=Read"},
{"title": "LLM-Enhanced Linear Autoencoders for Recommendation", "author": "Jaewan Moon and Seongmin Park and Jongwuk Lee", "abstract": "  Large language models (LLMs) have been widely adopted to enrich the semantic\nrepresentation of textual item information in recommender systems. However,\nexisting linear autoencoders (LAEs) that incorporate textual information rely\non sparse word co-occurrence patterns, limiting their ability to capture rich\ntextual semantics. To address this, we propose L3AE, the first integration of\nLLMs into the LAE framework. L3AE effectively integrates the heterogeneous\nknowledge of textual semantics and user-item interactions through a two-phase\noptimization strategy. (i) L3AE first constructs a semantic item-to-item\ncorrelation matrix from LLM-derived item representations. (ii) It then learns\nan item-to-item weight matrix from collaborative signals while distilling\nsemantic item correlations as regularization. Notably, each phase of L3AE is\noptimized through closed-form solutions, ensuring global optimality and\ncomputational efficiency. Extensive experiments demonstrate that L3AE\nconsistently outperforms state-of-the-art LLM-enhanced models on three\nbenchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.\nThe source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.\n", "link": "http://arxiv.org/abs/2508.13500v2", "date": "2025-08-26", "relevancy": 2.1163, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5422}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5303}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Enhanced%20Linear%20Autoencoders%20for%20Recommendation&body=Title%3A%20LLM-Enhanced%20Linear%20Autoencoders%20for%20Recommendation%0AAuthor%3A%20Jaewan%20Moon%20and%20Seongmin%20Park%20and%20Jongwuk%20Lee%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20adopted%20to%20enrich%20the%20semantic%0Arepresentation%20of%20textual%20item%20information%20in%20recommender%20systems.%20However%2C%0Aexisting%20linear%20autoencoders%20%28LAEs%29%20that%20incorporate%20textual%20information%20rely%0Aon%20sparse%20word%20co-occurrence%20patterns%2C%20limiting%20their%20ability%20to%20capture%20rich%0Atextual%20semantics.%20To%20address%20this%2C%20we%20propose%20L3AE%2C%20the%20first%20integration%20of%0ALLMs%20into%20the%20LAE%20framework.%20L3AE%20effectively%20integrates%20the%20heterogeneous%0Aknowledge%20of%20textual%20semantics%20and%20user-item%20interactions%20through%20a%20two-phase%0Aoptimization%20strategy.%20%28i%29%20L3AE%20first%20constructs%20a%20semantic%20item-to-item%0Acorrelation%20matrix%20from%20LLM-derived%20item%20representations.%20%28ii%29%20It%20then%20learns%0Aan%20item-to-item%20weight%20matrix%20from%20collaborative%20signals%20while%20distilling%0Asemantic%20item%20correlations%20as%20regularization.%20Notably%2C%20each%20phase%20of%20L3AE%20is%0Aoptimized%20through%20closed-form%20solutions%2C%20ensuring%20global%20optimality%20and%0Acomputational%20efficiency.%20Extensive%20experiments%20demonstrate%20that%20L3AE%0Aconsistently%20outperforms%20state-of-the-art%20LLM-enhanced%20models%20on%20three%0Abenchmark%20datasets%2C%20achieving%20gains%20of%2027.6%25%20in%20Recall%4020%20and%2039.3%25%20in%20NDCG%4020.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/jaewan7599/L3AE_CIKM2025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Enhanced%2520Linear%2520Autoencoders%2520for%2520Recommendation%26entry.906535625%3DJaewan%2520Moon%2520and%2520Seongmin%2520Park%2520and%2520Jongwuk%2520Lee%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520widely%2520adopted%2520to%2520enrich%2520the%2520semantic%250Arepresentation%2520of%2520textual%2520item%2520information%2520in%2520recommender%2520systems.%2520However%252C%250Aexisting%2520linear%2520autoencoders%2520%2528LAEs%2529%2520that%2520incorporate%2520textual%2520information%2520rely%250Aon%2520sparse%2520word%2520co-occurrence%2520patterns%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520rich%250Atextual%2520semantics.%2520To%2520address%2520this%252C%2520we%2520propose%2520L3AE%252C%2520the%2520first%2520integration%2520of%250ALLMs%2520into%2520the%2520LAE%2520framework.%2520L3AE%2520effectively%2520integrates%2520the%2520heterogeneous%250Aknowledge%2520of%2520textual%2520semantics%2520and%2520user-item%2520interactions%2520through%2520a%2520two-phase%250Aoptimization%2520strategy.%2520%2528i%2529%2520L3AE%2520first%2520constructs%2520a%2520semantic%2520item-to-item%250Acorrelation%2520matrix%2520from%2520LLM-derived%2520item%2520representations.%2520%2528ii%2529%2520It%2520then%2520learns%250Aan%2520item-to-item%2520weight%2520matrix%2520from%2520collaborative%2520signals%2520while%2520distilling%250Asemantic%2520item%2520correlations%2520as%2520regularization.%2520Notably%252C%2520each%2520phase%2520of%2520L3AE%2520is%250Aoptimized%2520through%2520closed-form%2520solutions%252C%2520ensuring%2520global%2520optimality%2520and%250Acomputational%2520efficiency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520L3AE%250Aconsistently%2520outperforms%2520state-of-the-art%2520LLM-enhanced%2520models%2520on%2520three%250Abenchmark%2520datasets%252C%2520achieving%2520gains%2520of%252027.6%2525%2520in%2520Recall%254020%2520and%252039.3%2525%2520in%2520NDCG%254020.%250AThe%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/jaewan7599/L3AE_CIKM2025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Enhanced%20Linear%20Autoencoders%20for%20Recommendation&entry.906535625=Jaewan%20Moon%20and%20Seongmin%20Park%20and%20Jongwuk%20Lee&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20adopted%20to%20enrich%20the%20semantic%0Arepresentation%20of%20textual%20item%20information%20in%20recommender%20systems.%20However%2C%0Aexisting%20linear%20autoencoders%20%28LAEs%29%20that%20incorporate%20textual%20information%20rely%0Aon%20sparse%20word%20co-occurrence%20patterns%2C%20limiting%20their%20ability%20to%20capture%20rich%0Atextual%20semantics.%20To%20address%20this%2C%20we%20propose%20L3AE%2C%20the%20first%20integration%20of%0ALLMs%20into%20the%20LAE%20framework.%20L3AE%20effectively%20integrates%20the%20heterogeneous%0Aknowledge%20of%20textual%20semantics%20and%20user-item%20interactions%20through%20a%20two-phase%0Aoptimization%20strategy.%20%28i%29%20L3AE%20first%20constructs%20a%20semantic%20item-to-item%0Acorrelation%20matrix%20from%20LLM-derived%20item%20representations.%20%28ii%29%20It%20then%20learns%0Aan%20item-to-item%20weight%20matrix%20from%20collaborative%20signals%20while%20distilling%0Asemantic%20item%20correlations%20as%20regularization.%20Notably%2C%20each%20phase%20of%20L3AE%20is%0Aoptimized%20through%20closed-form%20solutions%2C%20ensuring%20global%20optimality%20and%0Acomputational%20efficiency.%20Extensive%20experiments%20demonstrate%20that%20L3AE%0Aconsistently%20outperforms%20state-of-the-art%20LLM-enhanced%20models%20on%20three%0Abenchmark%20datasets%2C%20achieving%20gains%20of%2027.6%25%20in%20Recall%4020%20and%2039.3%25%20in%20NDCG%4020.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/jaewan7599/L3AE_CIKM2025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13500v2&entry.124074799=Read"},
{"title": "VibES: Induced Vibration for Persistent Event-Based Sensing", "author": "Vincenzo Polizzi and Stephen Yang and Quentin Clark and Jonathan Kelly and Igor Gilitschenski and David B. Lindell", "abstract": "  Event cameras are a bio-inspired class of sensors that asynchronously measure\nper-pixel intensity changes. Under fixed illumination conditions in static or\nlow-motion scenes, rigidly mounted event cameras are unable to generate any\nevents, becoming unsuitable for most computer vision tasks. To address this\nlimitation, recent work has investigated motion-induced event stimulation that\noften requires complex hardware or additional optical components. In contrast,\nwe introduce a lightweight approach to sustain persistent event generation by\nemploying a simple rotating unbalanced mass to induce periodic vibrational\nmotion. This is combined with a motion-compensation pipeline that removes the\ninjected motion and yields clean, motion-corrected events for downstream\nperception tasks. We demonstrate our approach with a hardware prototype and\nevaluate it on real-world captured datasets. Our method reliably recovers\nmotion parameters and improves both image reconstruction and edge detection\nover event-based sensing without motion induction.\n", "link": "http://arxiv.org/abs/2508.19094v1", "date": "2025-08-26", "relevancy": 2.1125, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5495}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5332}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VibES%3A%20Induced%20Vibration%20for%20Persistent%20Event-Based%20Sensing&body=Title%3A%20VibES%3A%20Induced%20Vibration%20for%20Persistent%20Event-Based%20Sensing%0AAuthor%3A%20Vincenzo%20Polizzi%20and%20Stephen%20Yang%20and%20Quentin%20Clark%20and%20Jonathan%20Kelly%20and%20Igor%20Gilitschenski%20and%20David%20B.%20Lindell%0AAbstract%3A%20%20%20Event%20cameras%20are%20a%20bio-inspired%20class%20of%20sensors%20that%20asynchronously%20measure%0Aper-pixel%20intensity%20changes.%20Under%20fixed%20illumination%20conditions%20in%20static%20or%0Alow-motion%20scenes%2C%20rigidly%20mounted%20event%20cameras%20are%20unable%20to%20generate%20any%0Aevents%2C%20becoming%20unsuitable%20for%20most%20computer%20vision%20tasks.%20To%20address%20this%0Alimitation%2C%20recent%20work%20has%20investigated%20motion-induced%20event%20stimulation%20that%0Aoften%20requires%20complex%20hardware%20or%20additional%20optical%20components.%20In%20contrast%2C%0Awe%20introduce%20a%20lightweight%20approach%20to%20sustain%20persistent%20event%20generation%20by%0Aemploying%20a%20simple%20rotating%20unbalanced%20mass%20to%20induce%20periodic%20vibrational%0Amotion.%20This%20is%20combined%20with%20a%20motion-compensation%20pipeline%20that%20removes%20the%0Ainjected%20motion%20and%20yields%20clean%2C%20motion-corrected%20events%20for%20downstream%0Aperception%20tasks.%20We%20demonstrate%20our%20approach%20with%20a%20hardware%20prototype%20and%0Aevaluate%20it%20on%20real-world%20captured%20datasets.%20Our%20method%20reliably%20recovers%0Amotion%20parameters%20and%20improves%20both%20image%20reconstruction%20and%20edge%20detection%0Aover%20event-based%20sensing%20without%20motion%20induction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVibES%253A%2520Induced%2520Vibration%2520for%2520Persistent%2520Event-Based%2520Sensing%26entry.906535625%3DVincenzo%2520Polizzi%2520and%2520Stephen%2520Yang%2520and%2520Quentin%2520Clark%2520and%2520Jonathan%2520Kelly%2520and%2520Igor%2520Gilitschenski%2520and%2520David%2520B.%2520Lindell%26entry.1292438233%3D%2520%2520Event%2520cameras%2520are%2520a%2520bio-inspired%2520class%2520of%2520sensors%2520that%2520asynchronously%2520measure%250Aper-pixel%2520intensity%2520changes.%2520Under%2520fixed%2520illumination%2520conditions%2520in%2520static%2520or%250Alow-motion%2520scenes%252C%2520rigidly%2520mounted%2520event%2520cameras%2520are%2520unable%2520to%2520generate%2520any%250Aevents%252C%2520becoming%2520unsuitable%2520for%2520most%2520computer%2520vision%2520tasks.%2520To%2520address%2520this%250Alimitation%252C%2520recent%2520work%2520has%2520investigated%2520motion-induced%2520event%2520stimulation%2520that%250Aoften%2520requires%2520complex%2520hardware%2520or%2520additional%2520optical%2520components.%2520In%2520contrast%252C%250Awe%2520introduce%2520a%2520lightweight%2520approach%2520to%2520sustain%2520persistent%2520event%2520generation%2520by%250Aemploying%2520a%2520simple%2520rotating%2520unbalanced%2520mass%2520to%2520induce%2520periodic%2520vibrational%250Amotion.%2520This%2520is%2520combined%2520with%2520a%2520motion-compensation%2520pipeline%2520that%2520removes%2520the%250Ainjected%2520motion%2520and%2520yields%2520clean%252C%2520motion-corrected%2520events%2520for%2520downstream%250Aperception%2520tasks.%2520We%2520demonstrate%2520our%2520approach%2520with%2520a%2520hardware%2520prototype%2520and%250Aevaluate%2520it%2520on%2520real-world%2520captured%2520datasets.%2520Our%2520method%2520reliably%2520recovers%250Amotion%2520parameters%2520and%2520improves%2520both%2520image%2520reconstruction%2520and%2520edge%2520detection%250Aover%2520event-based%2520sensing%2520without%2520motion%2520induction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VibES%3A%20Induced%20Vibration%20for%20Persistent%20Event-Based%20Sensing&entry.906535625=Vincenzo%20Polizzi%20and%20Stephen%20Yang%20and%20Quentin%20Clark%20and%20Jonathan%20Kelly%20and%20Igor%20Gilitschenski%20and%20David%20B.%20Lindell&entry.1292438233=%20%20Event%20cameras%20are%20a%20bio-inspired%20class%20of%20sensors%20that%20asynchronously%20measure%0Aper-pixel%20intensity%20changes.%20Under%20fixed%20illumination%20conditions%20in%20static%20or%0Alow-motion%20scenes%2C%20rigidly%20mounted%20event%20cameras%20are%20unable%20to%20generate%20any%0Aevents%2C%20becoming%20unsuitable%20for%20most%20computer%20vision%20tasks.%20To%20address%20this%0Alimitation%2C%20recent%20work%20has%20investigated%20motion-induced%20event%20stimulation%20that%0Aoften%20requires%20complex%20hardware%20or%20additional%20optical%20components.%20In%20contrast%2C%0Awe%20introduce%20a%20lightweight%20approach%20to%20sustain%20persistent%20event%20generation%20by%0Aemploying%20a%20simple%20rotating%20unbalanced%20mass%20to%20induce%20periodic%20vibrational%0Amotion.%20This%20is%20combined%20with%20a%20motion-compensation%20pipeline%20that%20removes%20the%0Ainjected%20motion%20and%20yields%20clean%2C%20motion-corrected%20events%20for%20downstream%0Aperception%20tasks.%20We%20demonstrate%20our%20approach%20with%20a%20hardware%20prototype%20and%0Aevaluate%20it%20on%20real-world%20captured%20datasets.%20Our%20method%20reliably%20recovers%0Amotion%20parameters%20and%20improves%20both%20image%20reconstruction%20and%20edge%20detection%0Aover%20event-based%20sensing%20without%20motion%20induction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19094v1&entry.124074799=Read"},
{"title": "Sparse minimum Redundancy Maximum Relevance for feature selection", "author": "Peter Naylor and Benjamin Poignard and H\u00e9ctor Climente-Gonz\u00e1lez and Makoto Yamada", "abstract": "  We propose a feature screening method that integrates both feature-feature\nand feature-target relationships. Inactive features are identified via a\npenalized minimum Redundancy Maximum Relevance (mRMR) procedure, which is the\ncontinuous version of the classic mRMR penalized by a non-convex regularizer,\nand where the parameters estimated as zero coefficients represent the set of\ninactive features. We establish the conditions under which zero coefficients\nare correctly identified to guarantee accurate recovery of inactive features.\nWe introduce a multi-stage procedure based on the knockoff filter enabling the\npenalized mRMR to discard inactive features while controlling the false\ndiscovery rate (FDR). Our method performs comparably to HSIC-LASSO but is more\nconservative in the number of selected features. It only requires setting an\nFDR threshold, rather than specifying the number of features to retain. The\neffectiveness of the method is illustrated through simulations and real-world\ndatasets. The code to reproduce this work is available on the following GitHub:\nhttps://github.com/PeterJackNaylor/SmRMR.\n", "link": "http://arxiv.org/abs/2508.18901v1", "date": "2025-08-26", "relevancy": 2.1092, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4615}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.41}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20minimum%20Redundancy%20Maximum%20Relevance%20for%20feature%20selection&body=Title%3A%20Sparse%20minimum%20Redundancy%20Maximum%20Relevance%20for%20feature%20selection%0AAuthor%3A%20Peter%20Naylor%20and%20Benjamin%20Poignard%20and%20H%C3%A9ctor%20Climente-Gonz%C3%A1lez%20and%20Makoto%20Yamada%0AAbstract%3A%20%20%20We%20propose%20a%20feature%20screening%20method%20that%20integrates%20both%20feature-feature%0Aand%20feature-target%20relationships.%20Inactive%20features%20are%20identified%20via%20a%0Apenalized%20minimum%20Redundancy%20Maximum%20Relevance%20%28mRMR%29%20procedure%2C%20which%20is%20the%0Acontinuous%20version%20of%20the%20classic%20mRMR%20penalized%20by%20a%20non-convex%20regularizer%2C%0Aand%20where%20the%20parameters%20estimated%20as%20zero%20coefficients%20represent%20the%20set%20of%0Ainactive%20features.%20We%20establish%20the%20conditions%20under%20which%20zero%20coefficients%0Aare%20correctly%20identified%20to%20guarantee%20accurate%20recovery%20of%20inactive%20features.%0AWe%20introduce%20a%20multi-stage%20procedure%20based%20on%20the%20knockoff%20filter%20enabling%20the%0Apenalized%20mRMR%20to%20discard%20inactive%20features%20while%20controlling%20the%20false%0Adiscovery%20rate%20%28FDR%29.%20Our%20method%20performs%20comparably%20to%20HSIC-LASSO%20but%20is%20more%0Aconservative%20in%20the%20number%20of%20selected%20features.%20It%20only%20requires%20setting%20an%0AFDR%20threshold%2C%20rather%20than%20specifying%20the%20number%20of%20features%20to%20retain.%20The%0Aeffectiveness%20of%20the%20method%20is%20illustrated%20through%20simulations%20and%20real-world%0Adatasets.%20The%20code%20to%20reproduce%20this%20work%20is%20available%20on%20the%20following%20GitHub%3A%0Ahttps%3A//github.com/PeterJackNaylor/SmRMR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520minimum%2520Redundancy%2520Maximum%2520Relevance%2520for%2520feature%2520selection%26entry.906535625%3DPeter%2520Naylor%2520and%2520Benjamin%2520Poignard%2520and%2520H%25C3%25A9ctor%2520Climente-Gonz%25C3%25A1lez%2520and%2520Makoto%2520Yamada%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520feature%2520screening%2520method%2520that%2520integrates%2520both%2520feature-feature%250Aand%2520feature-target%2520relationships.%2520Inactive%2520features%2520are%2520identified%2520via%2520a%250Apenalized%2520minimum%2520Redundancy%2520Maximum%2520Relevance%2520%2528mRMR%2529%2520procedure%252C%2520which%2520is%2520the%250Acontinuous%2520version%2520of%2520the%2520classic%2520mRMR%2520penalized%2520by%2520a%2520non-convex%2520regularizer%252C%250Aand%2520where%2520the%2520parameters%2520estimated%2520as%2520zero%2520coefficients%2520represent%2520the%2520set%2520of%250Ainactive%2520features.%2520We%2520establish%2520the%2520conditions%2520under%2520which%2520zero%2520coefficients%250Aare%2520correctly%2520identified%2520to%2520guarantee%2520accurate%2520recovery%2520of%2520inactive%2520features.%250AWe%2520introduce%2520a%2520multi-stage%2520procedure%2520based%2520on%2520the%2520knockoff%2520filter%2520enabling%2520the%250Apenalized%2520mRMR%2520to%2520discard%2520inactive%2520features%2520while%2520controlling%2520the%2520false%250Adiscovery%2520rate%2520%2528FDR%2529.%2520Our%2520method%2520performs%2520comparably%2520to%2520HSIC-LASSO%2520but%2520is%2520more%250Aconservative%2520in%2520the%2520number%2520of%2520selected%2520features.%2520It%2520only%2520requires%2520setting%2520an%250AFDR%2520threshold%252C%2520rather%2520than%2520specifying%2520the%2520number%2520of%2520features%2520to%2520retain.%2520The%250Aeffectiveness%2520of%2520the%2520method%2520is%2520illustrated%2520through%2520simulations%2520and%2520real-world%250Adatasets.%2520The%2520code%2520to%2520reproduce%2520this%2520work%2520is%2520available%2520on%2520the%2520following%2520GitHub%253A%250Ahttps%253A//github.com/PeterJackNaylor/SmRMR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20minimum%20Redundancy%20Maximum%20Relevance%20for%20feature%20selection&entry.906535625=Peter%20Naylor%20and%20Benjamin%20Poignard%20and%20H%C3%A9ctor%20Climente-Gonz%C3%A1lez%20and%20Makoto%20Yamada&entry.1292438233=%20%20We%20propose%20a%20feature%20screening%20method%20that%20integrates%20both%20feature-feature%0Aand%20feature-target%20relationships.%20Inactive%20features%20are%20identified%20via%20a%0Apenalized%20minimum%20Redundancy%20Maximum%20Relevance%20%28mRMR%29%20procedure%2C%20which%20is%20the%0Acontinuous%20version%20of%20the%20classic%20mRMR%20penalized%20by%20a%20non-convex%20regularizer%2C%0Aand%20where%20the%20parameters%20estimated%20as%20zero%20coefficients%20represent%20the%20set%20of%0Ainactive%20features.%20We%20establish%20the%20conditions%20under%20which%20zero%20coefficients%0Aare%20correctly%20identified%20to%20guarantee%20accurate%20recovery%20of%20inactive%20features.%0AWe%20introduce%20a%20multi-stage%20procedure%20based%20on%20the%20knockoff%20filter%20enabling%20the%0Apenalized%20mRMR%20to%20discard%20inactive%20features%20while%20controlling%20the%20false%0Adiscovery%20rate%20%28FDR%29.%20Our%20method%20performs%20comparably%20to%20HSIC-LASSO%20but%20is%20more%0Aconservative%20in%20the%20number%20of%20selected%20features.%20It%20only%20requires%20setting%20an%0AFDR%20threshold%2C%20rather%20than%20specifying%20the%20number%20of%20features%20to%20retain.%20The%0Aeffectiveness%20of%20the%20method%20is%20illustrated%20through%20simulations%20and%20real-world%0Adatasets.%20The%20code%20to%20reproduce%20this%20work%20is%20available%20on%20the%20following%20GitHub%3A%0Ahttps%3A//github.com/PeterJackNaylor/SmRMR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18901v1&entry.124074799=Read"},
{"title": "Beyond flattening: a geometrically principled positional encoding for\n  vision transformers with Weierstrass elliptic functions", "author": "Zhihang Xin and Xitong Hu and Rui Wang", "abstract": "  Vision Transformers have demonstrated remarkable success in computer vision\ntasks, yet their reliance on learnable one-dimensional positional embeddings\nfundamentally disrupts the inherent two-dimensional spatial structure of images\nthrough patch flattening procedures. Traditional positional encoding approaches\nlack geometric constraints and fail to establish monotonic correspondence\nbetween Euclidean spatial distances and sequential index distances, thereby\nlimiting the model's capacity to leverage spatial proximity priors effectively.\nWe propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a\nmathematically principled approach that directly addresses two-dimensional\ncoordinates through natural complex domain representation, where the doubly\nperiodic properties of elliptic functions align remarkably with translational\ninvariance patterns commonly observed in visual data. Our method exploits the\nnon-linear geometric nature of elliptic functions to encode spatial distance\nrelationships naturally, while the algebraic addition formula enables direct\nderivation of relative positional information between arbitrary patch pairs\nfrom their absolute encodings. Comprehensive experiments demonstrate that\nWEF-PE achieves superior performance across diverse scenarios, including\n63.78\\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture,\n93.28\\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on\nVTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay\nproperty through rigorous mathematical proof, while attention visualization\nreveals enhanced geometric inductive bias and more coherent semantic focus\ncompared to conventional approaches.The source code implementing the methods\ndescribed in this paper is publicly available on GitHub.\n", "link": "http://arxiv.org/abs/2508.19167v1", "date": "2025-08-26", "relevancy": 2.1056, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5597}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5208}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20flattening%3A%20a%20geometrically%20principled%20positional%20encoding%20for%0A%20%20vision%20transformers%20with%20Weierstrass%20elliptic%20functions&body=Title%3A%20Beyond%20flattening%3A%20a%20geometrically%20principled%20positional%20encoding%20for%0A%20%20vision%20transformers%20with%20Weierstrass%20elliptic%20functions%0AAuthor%3A%20Zhihang%20Xin%20and%20Xitong%20Hu%20and%20Rui%20Wang%0AAbstract%3A%20%20%20Vision%20Transformers%20have%20demonstrated%20remarkable%20success%20in%20computer%20vision%0Atasks%2C%20yet%20their%20reliance%20on%20learnable%20one-dimensional%20positional%20embeddings%0Afundamentally%20disrupts%20the%20inherent%20two-dimensional%20spatial%20structure%20of%20images%0Athrough%20patch%20flattening%20procedures.%20Traditional%20positional%20encoding%20approaches%0Alack%20geometric%20constraints%20and%20fail%20to%20establish%20monotonic%20correspondence%0Abetween%20Euclidean%20spatial%20distances%20and%20sequential%20index%20distances%2C%20thereby%0Alimiting%20the%20model%27s%20capacity%20to%20leverage%20spatial%20proximity%20priors%20effectively.%0AWe%20propose%20Weierstrass%20Elliptic%20Function%20Positional%20Encoding%20%28WEF-PE%29%2C%20a%0Amathematically%20principled%20approach%20that%20directly%20addresses%20two-dimensional%0Acoordinates%20through%20natural%20complex%20domain%20representation%2C%20where%20the%20doubly%0Aperiodic%20properties%20of%20elliptic%20functions%20align%20remarkably%20with%20translational%0Ainvariance%20patterns%20commonly%20observed%20in%20visual%20data.%20Our%20method%20exploits%20the%0Anon-linear%20geometric%20nature%20of%20elliptic%20functions%20to%20encode%20spatial%20distance%0Arelationships%20naturally%2C%20while%20the%20algebraic%20addition%20formula%20enables%20direct%0Aderivation%20of%20relative%20positional%20information%20between%20arbitrary%20patch%20pairs%0Afrom%20their%20absolute%20encodings.%20Comprehensive%20experiments%20demonstrate%20that%0AWEF-PE%20achieves%20superior%20performance%20across%20diverse%20scenarios%2C%20including%0A63.78%5C%25%20accuracy%20on%20CIFAR-100%20from-scratch%20training%20with%20ViT-Tiny%20architecture%2C%0A93.28%5C%25%20on%20CIFAR-100%20fine-tuning%20with%20ViT-Base%2C%20and%20consistent%20improvements%20on%0AVTAB-1k%20benchmark%20tasks.%20Theoretical%20analysis%20confirms%20the%20distance-decay%0Aproperty%20through%20rigorous%20mathematical%20proof%2C%20while%20attention%20visualization%0Areveals%20enhanced%20geometric%20inductive%20bias%20and%20more%20coherent%20semantic%20focus%0Acompared%20to%20conventional%20approaches.The%20source%20code%20implementing%20the%20methods%0Adescribed%20in%20this%20paper%20is%20publicly%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520flattening%253A%2520a%2520geometrically%2520principled%2520positional%2520encoding%2520for%250A%2520%2520vision%2520transformers%2520with%2520Weierstrass%2520elliptic%2520functions%26entry.906535625%3DZhihang%2520Xin%2520and%2520Xitong%2520Hu%2520and%2520Rui%2520Wang%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520computer%2520vision%250Atasks%252C%2520yet%2520their%2520reliance%2520on%2520learnable%2520one-dimensional%2520positional%2520embeddings%250Afundamentally%2520disrupts%2520the%2520inherent%2520two-dimensional%2520spatial%2520structure%2520of%2520images%250Athrough%2520patch%2520flattening%2520procedures.%2520Traditional%2520positional%2520encoding%2520approaches%250Alack%2520geometric%2520constraints%2520and%2520fail%2520to%2520establish%2520monotonic%2520correspondence%250Abetween%2520Euclidean%2520spatial%2520distances%2520and%2520sequential%2520index%2520distances%252C%2520thereby%250Alimiting%2520the%2520model%2527s%2520capacity%2520to%2520leverage%2520spatial%2520proximity%2520priors%2520effectively.%250AWe%2520propose%2520Weierstrass%2520Elliptic%2520Function%2520Positional%2520Encoding%2520%2528WEF-PE%2529%252C%2520a%250Amathematically%2520principled%2520approach%2520that%2520directly%2520addresses%2520two-dimensional%250Acoordinates%2520through%2520natural%2520complex%2520domain%2520representation%252C%2520where%2520the%2520doubly%250Aperiodic%2520properties%2520of%2520elliptic%2520functions%2520align%2520remarkably%2520with%2520translational%250Ainvariance%2520patterns%2520commonly%2520observed%2520in%2520visual%2520data.%2520Our%2520method%2520exploits%2520the%250Anon-linear%2520geometric%2520nature%2520of%2520elliptic%2520functions%2520to%2520encode%2520spatial%2520distance%250Arelationships%2520naturally%252C%2520while%2520the%2520algebraic%2520addition%2520formula%2520enables%2520direct%250Aderivation%2520of%2520relative%2520positional%2520information%2520between%2520arbitrary%2520patch%2520pairs%250Afrom%2520their%2520absolute%2520encodings.%2520Comprehensive%2520experiments%2520demonstrate%2520that%250AWEF-PE%2520achieves%2520superior%2520performance%2520across%2520diverse%2520scenarios%252C%2520including%250A63.78%255C%2525%2520accuracy%2520on%2520CIFAR-100%2520from-scratch%2520training%2520with%2520ViT-Tiny%2520architecture%252C%250A93.28%255C%2525%2520on%2520CIFAR-100%2520fine-tuning%2520with%2520ViT-Base%252C%2520and%2520consistent%2520improvements%2520on%250AVTAB-1k%2520benchmark%2520tasks.%2520Theoretical%2520analysis%2520confirms%2520the%2520distance-decay%250Aproperty%2520through%2520rigorous%2520mathematical%2520proof%252C%2520while%2520attention%2520visualization%250Areveals%2520enhanced%2520geometric%2520inductive%2520bias%2520and%2520more%2520coherent%2520semantic%2520focus%250Acompared%2520to%2520conventional%2520approaches.The%2520source%2520code%2520implementing%2520the%2520methods%250Adescribed%2520in%2520this%2520paper%2520is%2520publicly%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20flattening%3A%20a%20geometrically%20principled%20positional%20encoding%20for%0A%20%20vision%20transformers%20with%20Weierstrass%20elliptic%20functions&entry.906535625=Zhihang%20Xin%20and%20Xitong%20Hu%20and%20Rui%20Wang&entry.1292438233=%20%20Vision%20Transformers%20have%20demonstrated%20remarkable%20success%20in%20computer%20vision%0Atasks%2C%20yet%20their%20reliance%20on%20learnable%20one-dimensional%20positional%20embeddings%0Afundamentally%20disrupts%20the%20inherent%20two-dimensional%20spatial%20structure%20of%20images%0Athrough%20patch%20flattening%20procedures.%20Traditional%20positional%20encoding%20approaches%0Alack%20geometric%20constraints%20and%20fail%20to%20establish%20monotonic%20correspondence%0Abetween%20Euclidean%20spatial%20distances%20and%20sequential%20index%20distances%2C%20thereby%0Alimiting%20the%20model%27s%20capacity%20to%20leverage%20spatial%20proximity%20priors%20effectively.%0AWe%20propose%20Weierstrass%20Elliptic%20Function%20Positional%20Encoding%20%28WEF-PE%29%2C%20a%0Amathematically%20principled%20approach%20that%20directly%20addresses%20two-dimensional%0Acoordinates%20through%20natural%20complex%20domain%20representation%2C%20where%20the%20doubly%0Aperiodic%20properties%20of%20elliptic%20functions%20align%20remarkably%20with%20translational%0Ainvariance%20patterns%20commonly%20observed%20in%20visual%20data.%20Our%20method%20exploits%20the%0Anon-linear%20geometric%20nature%20of%20elliptic%20functions%20to%20encode%20spatial%20distance%0Arelationships%20naturally%2C%20while%20the%20algebraic%20addition%20formula%20enables%20direct%0Aderivation%20of%20relative%20positional%20information%20between%20arbitrary%20patch%20pairs%0Afrom%20their%20absolute%20encodings.%20Comprehensive%20experiments%20demonstrate%20that%0AWEF-PE%20achieves%20superior%20performance%20across%20diverse%20scenarios%2C%20including%0A63.78%5C%25%20accuracy%20on%20CIFAR-100%20from-scratch%20training%20with%20ViT-Tiny%20architecture%2C%0A93.28%5C%25%20on%20CIFAR-100%20fine-tuning%20with%20ViT-Base%2C%20and%20consistent%20improvements%20on%0AVTAB-1k%20benchmark%20tasks.%20Theoretical%20analysis%20confirms%20the%20distance-decay%0Aproperty%20through%20rigorous%20mathematical%20proof%2C%20while%20attention%20visualization%0Areveals%20enhanced%20geometric%20inductive%20bias%20and%20more%20coherent%20semantic%20focus%0Acompared%20to%20conventional%20approaches.The%20source%20code%20implementing%20the%20methods%0Adescribed%20in%20this%20paper%20is%20publicly%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19167v1&entry.124074799=Read"},
{"title": "USO: Unified Style and Subject-Driven Generation via Disentangled and\n  Reward Learning", "author": "Shaojin Wu and Mengqi Huang and Yufeng Cheng and Wenxu Wu and Jiahe Tian and Yiming Luo and Fei Ding and Qian He", "abstract": "  Existing literature typically treats style-driven and subject-driven\ngeneration as two disjoint tasks: the former prioritizes stylistic similarity,\nwhereas the latter insists on subject consistency, resulting in an apparent\nantagonism. We argue that both objectives can be unified under a single\nframework because they ultimately concern the disentanglement and\nre-composition of content and style, a long-standing theme in style-driven\nresearch. To this end, we present USO, a Unified Style-Subject Optimized\ncustomization model. First, we construct a large-scale triplet dataset\nconsisting of content images, style images, and their corresponding stylized\ncontent images. Second, we introduce a disentangled learning scheme that\nsimultaneously aligns style features and disentangles content from style\nthrough two complementary objectives, style-alignment training and\ncontent-style disentanglement training. Third, we incorporate a style\nreward-learning paradigm denoted as SRL to further enhance the model's\nperformance. Finally, we release USO-Bench, the first benchmark that jointly\nevaluates style similarity and subject fidelity across multiple metrics.\nExtensive experiments demonstrate that USO achieves state-of-the-art\nperformance among open-source models along both dimensions of subject\nconsistency and style similarity. Code and model:\nhttps://github.com/bytedance/USO\n", "link": "http://arxiv.org/abs/2508.18966v1", "date": "2025-08-26", "relevancy": 2.0871, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5331}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5308}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USO%3A%20Unified%20Style%20and%20Subject-Driven%20Generation%20via%20Disentangled%20and%0A%20%20Reward%20Learning&body=Title%3A%20USO%3A%20Unified%20Style%20and%20Subject-Driven%20Generation%20via%20Disentangled%20and%0A%20%20Reward%20Learning%0AAuthor%3A%20Shaojin%20Wu%20and%20Mengqi%20Huang%20and%20Yufeng%20Cheng%20and%20Wenxu%20Wu%20and%20Jiahe%20Tian%20and%20Yiming%20Luo%20and%20Fei%20Ding%20and%20Qian%20He%0AAbstract%3A%20%20%20Existing%20literature%20typically%20treats%20style-driven%20and%20subject-driven%0Ageneration%20as%20two%20disjoint%20tasks%3A%20the%20former%20prioritizes%20stylistic%20similarity%2C%0Awhereas%20the%20latter%20insists%20on%20subject%20consistency%2C%20resulting%20in%20an%20apparent%0Aantagonism.%20We%20argue%20that%20both%20objectives%20can%20be%20unified%20under%20a%20single%0Aframework%20because%20they%20ultimately%20concern%20the%20disentanglement%20and%0Are-composition%20of%20content%20and%20style%2C%20a%20long-standing%20theme%20in%20style-driven%0Aresearch.%20To%20this%20end%2C%20we%20present%20USO%2C%20a%20Unified%20Style-Subject%20Optimized%0Acustomization%20model.%20First%2C%20we%20construct%20a%20large-scale%20triplet%20dataset%0Aconsisting%20of%20content%20images%2C%20style%20images%2C%20and%20their%20corresponding%20stylized%0Acontent%20images.%20Second%2C%20we%20introduce%20a%20disentangled%20learning%20scheme%20that%0Asimultaneously%20aligns%20style%20features%20and%20disentangles%20content%20from%20style%0Athrough%20two%20complementary%20objectives%2C%20style-alignment%20training%20and%0Acontent-style%20disentanglement%20training.%20Third%2C%20we%20incorporate%20a%20style%0Areward-learning%20paradigm%20denoted%20as%20SRL%20to%20further%20enhance%20the%20model%27s%0Aperformance.%20Finally%2C%20we%20release%20USO-Bench%2C%20the%20first%20benchmark%20that%20jointly%0Aevaluates%20style%20similarity%20and%20subject%20fidelity%20across%20multiple%20metrics.%0AExtensive%20experiments%20demonstrate%20that%20USO%20achieves%20state-of-the-art%0Aperformance%20among%20open-source%20models%20along%20both%20dimensions%20of%20subject%0Aconsistency%20and%20style%20similarity.%20Code%20and%20model%3A%0Ahttps%3A//github.com/bytedance/USO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSO%253A%2520Unified%2520Style%2520and%2520Subject-Driven%2520Generation%2520via%2520Disentangled%2520and%250A%2520%2520Reward%2520Learning%26entry.906535625%3DShaojin%2520Wu%2520and%2520Mengqi%2520Huang%2520and%2520Yufeng%2520Cheng%2520and%2520Wenxu%2520Wu%2520and%2520Jiahe%2520Tian%2520and%2520Yiming%2520Luo%2520and%2520Fei%2520Ding%2520and%2520Qian%2520He%26entry.1292438233%3D%2520%2520Existing%2520literature%2520typically%2520treats%2520style-driven%2520and%2520subject-driven%250Ageneration%2520as%2520two%2520disjoint%2520tasks%253A%2520the%2520former%2520prioritizes%2520stylistic%2520similarity%252C%250Awhereas%2520the%2520latter%2520insists%2520on%2520subject%2520consistency%252C%2520resulting%2520in%2520an%2520apparent%250Aantagonism.%2520We%2520argue%2520that%2520both%2520objectives%2520can%2520be%2520unified%2520under%2520a%2520single%250Aframework%2520because%2520they%2520ultimately%2520concern%2520the%2520disentanglement%2520and%250Are-composition%2520of%2520content%2520and%2520style%252C%2520a%2520long-standing%2520theme%2520in%2520style-driven%250Aresearch.%2520To%2520this%2520end%252C%2520we%2520present%2520USO%252C%2520a%2520Unified%2520Style-Subject%2520Optimized%250Acustomization%2520model.%2520First%252C%2520we%2520construct%2520a%2520large-scale%2520triplet%2520dataset%250Aconsisting%2520of%2520content%2520images%252C%2520style%2520images%252C%2520and%2520their%2520corresponding%2520stylized%250Acontent%2520images.%2520Second%252C%2520we%2520introduce%2520a%2520disentangled%2520learning%2520scheme%2520that%250Asimultaneously%2520aligns%2520style%2520features%2520and%2520disentangles%2520content%2520from%2520style%250Athrough%2520two%2520complementary%2520objectives%252C%2520style-alignment%2520training%2520and%250Acontent-style%2520disentanglement%2520training.%2520Third%252C%2520we%2520incorporate%2520a%2520style%250Areward-learning%2520paradigm%2520denoted%2520as%2520SRL%2520to%2520further%2520enhance%2520the%2520model%2527s%250Aperformance.%2520Finally%252C%2520we%2520release%2520USO-Bench%252C%2520the%2520first%2520benchmark%2520that%2520jointly%250Aevaluates%2520style%2520similarity%2520and%2520subject%2520fidelity%2520across%2520multiple%2520metrics.%250AExtensive%2520experiments%2520demonstrate%2520that%2520USO%2520achieves%2520state-of-the-art%250Aperformance%2520among%2520open-source%2520models%2520along%2520both%2520dimensions%2520of%2520subject%250Aconsistency%2520and%2520style%2520similarity.%2520Code%2520and%2520model%253A%250Ahttps%253A//github.com/bytedance/USO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USO%3A%20Unified%20Style%20and%20Subject-Driven%20Generation%20via%20Disentangled%20and%0A%20%20Reward%20Learning&entry.906535625=Shaojin%20Wu%20and%20Mengqi%20Huang%20and%20Yufeng%20Cheng%20and%20Wenxu%20Wu%20and%20Jiahe%20Tian%20and%20Yiming%20Luo%20and%20Fei%20Ding%20and%20Qian%20He&entry.1292438233=%20%20Existing%20literature%20typically%20treats%20style-driven%20and%20subject-driven%0Ageneration%20as%20two%20disjoint%20tasks%3A%20the%20former%20prioritizes%20stylistic%20similarity%2C%0Awhereas%20the%20latter%20insists%20on%20subject%20consistency%2C%20resulting%20in%20an%20apparent%0Aantagonism.%20We%20argue%20that%20both%20objectives%20can%20be%20unified%20under%20a%20single%0Aframework%20because%20they%20ultimately%20concern%20the%20disentanglement%20and%0Are-composition%20of%20content%20and%20style%2C%20a%20long-standing%20theme%20in%20style-driven%0Aresearch.%20To%20this%20end%2C%20we%20present%20USO%2C%20a%20Unified%20Style-Subject%20Optimized%0Acustomization%20model.%20First%2C%20we%20construct%20a%20large-scale%20triplet%20dataset%0Aconsisting%20of%20content%20images%2C%20style%20images%2C%20and%20their%20corresponding%20stylized%0Acontent%20images.%20Second%2C%20we%20introduce%20a%20disentangled%20learning%20scheme%20that%0Asimultaneously%20aligns%20style%20features%20and%20disentangles%20content%20from%20style%0Athrough%20two%20complementary%20objectives%2C%20style-alignment%20training%20and%0Acontent-style%20disentanglement%20training.%20Third%2C%20we%20incorporate%20a%20style%0Areward-learning%20paradigm%20denoted%20as%20SRL%20to%20further%20enhance%20the%20model%27s%0Aperformance.%20Finally%2C%20we%20release%20USO-Bench%2C%20the%20first%20benchmark%20that%20jointly%0Aevaluates%20style%20similarity%20and%20subject%20fidelity%20across%20multiple%20metrics.%0AExtensive%20experiments%20demonstrate%20that%20USO%20achieves%20state-of-the-art%0Aperformance%20among%20open-source%20models%20along%20both%20dimensions%20of%20subject%0Aconsistency%20and%20style%20similarity.%20Code%20and%20model%3A%0Ahttps%3A//github.com/bytedance/USO%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18966v1&entry.124074799=Read"},
{"title": "The point is the mask: scaling coral reef segmentation with weak\n  supervision", "author": "Matteo Contini and Victor Illien and Sylvain Poulain and Serge Bernard and Julien Barde and Sylvain Bonhommeau and Alexis Joly", "abstract": "  Monitoring coral reefs at large spatial scales remains an open challenge,\nessential for assessing ecosystem health and informing conservation efforts.\nWhile drone-based aerial imagery offers broad spatial coverage, its limited\nresolution makes it difficult to reliably distinguish fine-scale classes, such\nas coral morphotypes. At the same time, obtaining pixel-level annotations over\nlarge spatial extents is costly and labor-intensive, limiting the scalability\nof deep learning-based segmentation methods for aerial imagery. We present a\nmulti-scale weakly supervised semantic segmentation framework that addresses\nthis challenge by transferring fine-scale ecological information from\nunderwater imagery to aerial data. Our method enables large-scale coral reef\nmapping from drone imagery with minimal manual annotation, combining\nclassification-based supervision, spatial interpolation and self-distillation\ntechniques. We demonstrate the efficacy of the approach, enabling large-area\nsegmentation of coral morphotypes and demonstrating flexibility for integrating\nnew classes. This study presents a scalable, cost-effective methodology for\nhigh-resolution reef monitoring, combining low-cost data collection, weakly\nsupervised deep learning and multi-scale remote sensing.\n", "link": "http://arxiv.org/abs/2508.18958v1", "date": "2025-08-26", "relevancy": 2.0741, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5359}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5077}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20point%20is%20the%20mask%3A%20scaling%20coral%20reef%20segmentation%20with%20weak%0A%20%20supervision&body=Title%3A%20The%20point%20is%20the%20mask%3A%20scaling%20coral%20reef%20segmentation%20with%20weak%0A%20%20supervision%0AAuthor%3A%20Matteo%20Contini%20and%20Victor%20Illien%20and%20Sylvain%20Poulain%20and%20Serge%20Bernard%20and%20Julien%20Barde%20and%20Sylvain%20Bonhommeau%20and%20Alexis%20Joly%0AAbstract%3A%20%20%20Monitoring%20coral%20reefs%20at%20large%20spatial%20scales%20remains%20an%20open%20challenge%2C%0Aessential%20for%20assessing%20ecosystem%20health%20and%20informing%20conservation%20efforts.%0AWhile%20drone-based%20aerial%20imagery%20offers%20broad%20spatial%20coverage%2C%20its%20limited%0Aresolution%20makes%20it%20difficult%20to%20reliably%20distinguish%20fine-scale%20classes%2C%20such%0Aas%20coral%20morphotypes.%20At%20the%20same%20time%2C%20obtaining%20pixel-level%20annotations%20over%0Alarge%20spatial%20extents%20is%20costly%20and%20labor-intensive%2C%20limiting%20the%20scalability%0Aof%20deep%20learning-based%20segmentation%20methods%20for%20aerial%20imagery.%20We%20present%20a%0Amulti-scale%20weakly%20supervised%20semantic%20segmentation%20framework%20that%20addresses%0Athis%20challenge%20by%20transferring%20fine-scale%20ecological%20information%20from%0Aunderwater%20imagery%20to%20aerial%20data.%20Our%20method%20enables%20large-scale%20coral%20reef%0Amapping%20from%20drone%20imagery%20with%20minimal%20manual%20annotation%2C%20combining%0Aclassification-based%20supervision%2C%20spatial%20interpolation%20and%20self-distillation%0Atechniques.%20We%20demonstrate%20the%20efficacy%20of%20the%20approach%2C%20enabling%20large-area%0Asegmentation%20of%20coral%20morphotypes%20and%20demonstrating%20flexibility%20for%20integrating%0Anew%20classes.%20This%20study%20presents%20a%20scalable%2C%20cost-effective%20methodology%20for%0Ahigh-resolution%20reef%20monitoring%2C%20combining%20low-cost%20data%20collection%2C%20weakly%0Asupervised%20deep%20learning%20and%20multi-scale%20remote%20sensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520point%2520is%2520the%2520mask%253A%2520scaling%2520coral%2520reef%2520segmentation%2520with%2520weak%250A%2520%2520supervision%26entry.906535625%3DMatteo%2520Contini%2520and%2520Victor%2520Illien%2520and%2520Sylvain%2520Poulain%2520and%2520Serge%2520Bernard%2520and%2520Julien%2520Barde%2520and%2520Sylvain%2520Bonhommeau%2520and%2520Alexis%2520Joly%26entry.1292438233%3D%2520%2520Monitoring%2520coral%2520reefs%2520at%2520large%2520spatial%2520scales%2520remains%2520an%2520open%2520challenge%252C%250Aessential%2520for%2520assessing%2520ecosystem%2520health%2520and%2520informing%2520conservation%2520efforts.%250AWhile%2520drone-based%2520aerial%2520imagery%2520offers%2520broad%2520spatial%2520coverage%252C%2520its%2520limited%250Aresolution%2520makes%2520it%2520difficult%2520to%2520reliably%2520distinguish%2520fine-scale%2520classes%252C%2520such%250Aas%2520coral%2520morphotypes.%2520At%2520the%2520same%2520time%252C%2520obtaining%2520pixel-level%2520annotations%2520over%250Alarge%2520spatial%2520extents%2520is%2520costly%2520and%2520labor-intensive%252C%2520limiting%2520the%2520scalability%250Aof%2520deep%2520learning-based%2520segmentation%2520methods%2520for%2520aerial%2520imagery.%2520We%2520present%2520a%250Amulti-scale%2520weakly%2520supervised%2520semantic%2520segmentation%2520framework%2520that%2520addresses%250Athis%2520challenge%2520by%2520transferring%2520fine-scale%2520ecological%2520information%2520from%250Aunderwater%2520imagery%2520to%2520aerial%2520data.%2520Our%2520method%2520enables%2520large-scale%2520coral%2520reef%250Amapping%2520from%2520drone%2520imagery%2520with%2520minimal%2520manual%2520annotation%252C%2520combining%250Aclassification-based%2520supervision%252C%2520spatial%2520interpolation%2520and%2520self-distillation%250Atechniques.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520approach%252C%2520enabling%2520large-area%250Asegmentation%2520of%2520coral%2520morphotypes%2520and%2520demonstrating%2520flexibility%2520for%2520integrating%250Anew%2520classes.%2520This%2520study%2520presents%2520a%2520scalable%252C%2520cost-effective%2520methodology%2520for%250Ahigh-resolution%2520reef%2520monitoring%252C%2520combining%2520low-cost%2520data%2520collection%252C%2520weakly%250Asupervised%2520deep%2520learning%2520and%2520multi-scale%2520remote%2520sensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20point%20is%20the%20mask%3A%20scaling%20coral%20reef%20segmentation%20with%20weak%0A%20%20supervision&entry.906535625=Matteo%20Contini%20and%20Victor%20Illien%20and%20Sylvain%20Poulain%20and%20Serge%20Bernard%20and%20Julien%20Barde%20and%20Sylvain%20Bonhommeau%20and%20Alexis%20Joly&entry.1292438233=%20%20Monitoring%20coral%20reefs%20at%20large%20spatial%20scales%20remains%20an%20open%20challenge%2C%0Aessential%20for%20assessing%20ecosystem%20health%20and%20informing%20conservation%20efforts.%0AWhile%20drone-based%20aerial%20imagery%20offers%20broad%20spatial%20coverage%2C%20its%20limited%0Aresolution%20makes%20it%20difficult%20to%20reliably%20distinguish%20fine-scale%20classes%2C%20such%0Aas%20coral%20morphotypes.%20At%20the%20same%20time%2C%20obtaining%20pixel-level%20annotations%20over%0Alarge%20spatial%20extents%20is%20costly%20and%20labor-intensive%2C%20limiting%20the%20scalability%0Aof%20deep%20learning-based%20segmentation%20methods%20for%20aerial%20imagery.%20We%20present%20a%0Amulti-scale%20weakly%20supervised%20semantic%20segmentation%20framework%20that%20addresses%0Athis%20challenge%20by%20transferring%20fine-scale%20ecological%20information%20from%0Aunderwater%20imagery%20to%20aerial%20data.%20Our%20method%20enables%20large-scale%20coral%20reef%0Amapping%20from%20drone%20imagery%20with%20minimal%20manual%20annotation%2C%20combining%0Aclassification-based%20supervision%2C%20spatial%20interpolation%20and%20self-distillation%0Atechniques.%20We%20demonstrate%20the%20efficacy%20of%20the%20approach%2C%20enabling%20large-area%0Asegmentation%20of%20coral%20morphotypes%20and%20demonstrating%20flexibility%20for%20integrating%0Anew%20classes.%20This%20study%20presents%20a%20scalable%2C%20cost-effective%20methodology%20for%0Ahigh-resolution%20reef%20monitoring%2C%20combining%20low-cost%20data%20collection%2C%20weakly%0Asupervised%20deep%20learning%20and%20multi-scale%20remote%20sensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18958v1&entry.124074799=Read"},
{"title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness", "author": "Beier Zhu and Jiequan Cui and Hanwang Zhang and Chi Zhang", "abstract": "  While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels.\n", "link": "http://arxiv.org/abs/2503.09487v3", "date": "2025-08-26", "relevancy": 2.0543, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Project-Probe-Aggregate%3A%20Efficient%20Fine-Tuning%20for%20Group%20Robustness&body=Title%3A%20Project-Probe-Aggregate%3A%20Efficient%20Fine-Tuning%20for%20Group%20Robustness%0AAuthor%3A%20Beier%20Zhu%20and%20Jiequan%20Cui%20and%20Hanwang%20Zhang%20and%20Chi%20Zhang%0AAbstract%3A%20%20%20While%20image-text%20foundation%20models%20have%20succeeded%20across%20diverse%20downstream%0Atasks%2C%20they%20still%20face%20challenges%20in%20the%20presence%20of%20spurious%20correlations%0Abetween%20the%20input%20and%20label.%20To%20address%20this%20issue%2C%20we%20propose%20a%20simple%0Athree-step%20approach%2CProject-Probe-Aggregate%20%28PPA%29%2C%20that%20enables%0Aparameter-efficient%20fine-tuning%20for%20foundation%20models%20without%20relying%20on%20group%0Aannotations.%20Building%20upon%20the%20failure-based%20debiasing%20scheme%2C%20our%20method%2C%20PPA%2C%0Aimproves%20its%20two%20key%20components%3A%20minority%20samples%20identification%20and%20the%20robust%0Atraining%20algorithm.%20Specifically%2C%20we%20first%20train%20biased%20classifiers%20by%0Aprojecting%20image%20features%20onto%20the%20nullspace%20of%20class%20proxies%20from%20text%0Aencoders.%20Next%2C%20we%20infer%20group%20labels%20using%20the%20biased%20classifier%20and%20probe%0Agroup%20targets%20with%20prior%20correction.%20Finally%2C%20we%20aggregate%20group%20weights%20of%0Aeach%20class%20to%20produce%20the%20debiased%20classifier.%20Our%20theoretical%20analysis%20shows%0Athat%20our%20PPA%20enhances%20minority%20group%20identification%20and%20is%20Bayes%20optimal%20for%0Aminimizing%20the%20balanced%20group%20error%2C%20mitigating%20spurious%20correlations.%0AExtensive%20experimental%20results%20confirm%20the%20effectiveness%20of%20our%20PPA%3A%20it%0Aoutperforms%20the%20state-of-the-art%20by%20an%20average%20worst-group%20accuracy%20while%0Arequiring%20less%20than%200.01%25%20tunable%20parameters%20without%20training%20group%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09487v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProject-Probe-Aggregate%253A%2520Efficient%2520Fine-Tuning%2520for%2520Group%2520Robustness%26entry.906535625%3DBeier%2520Zhu%2520and%2520Jiequan%2520Cui%2520and%2520Hanwang%2520Zhang%2520and%2520Chi%2520Zhang%26entry.1292438233%3D%2520%2520While%2520image-text%2520foundation%2520models%2520have%2520succeeded%2520across%2520diverse%2520downstream%250Atasks%252C%2520they%2520still%2520face%2520challenges%2520in%2520the%2520presence%2520of%2520spurious%2520correlations%250Abetween%2520the%2520input%2520and%2520label.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520simple%250Athree-step%2520approach%252CProject-Probe-Aggregate%2520%2528PPA%2529%252C%2520that%2520enables%250Aparameter-efficient%2520fine-tuning%2520for%2520foundation%2520models%2520without%2520relying%2520on%2520group%250Aannotations.%2520Building%2520upon%2520the%2520failure-based%2520debiasing%2520scheme%252C%2520our%2520method%252C%2520PPA%252C%250Aimproves%2520its%2520two%2520key%2520components%253A%2520minority%2520samples%2520identification%2520and%2520the%2520robust%250Atraining%2520algorithm.%2520Specifically%252C%2520we%2520first%2520train%2520biased%2520classifiers%2520by%250Aprojecting%2520image%2520features%2520onto%2520the%2520nullspace%2520of%2520class%2520proxies%2520from%2520text%250Aencoders.%2520Next%252C%2520we%2520infer%2520group%2520labels%2520using%2520the%2520biased%2520classifier%2520and%2520probe%250Agroup%2520targets%2520with%2520prior%2520correction.%2520Finally%252C%2520we%2520aggregate%2520group%2520weights%2520of%250Aeach%2520class%2520to%2520produce%2520the%2520debiased%2520classifier.%2520Our%2520theoretical%2520analysis%2520shows%250Athat%2520our%2520PPA%2520enhances%2520minority%2520group%2520identification%2520and%2520is%2520Bayes%2520optimal%2520for%250Aminimizing%2520the%2520balanced%2520group%2520error%252C%2520mitigating%2520spurious%2520correlations.%250AExtensive%2520experimental%2520results%2520confirm%2520the%2520effectiveness%2520of%2520our%2520PPA%253A%2520it%250Aoutperforms%2520the%2520state-of-the-art%2520by%2520an%2520average%2520worst-group%2520accuracy%2520while%250Arequiring%2520less%2520than%25200.01%2525%2520tunable%2520parameters%2520without%2520training%2520group%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09487v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Project-Probe-Aggregate%3A%20Efficient%20Fine-Tuning%20for%20Group%20Robustness&entry.906535625=Beier%20Zhu%20and%20Jiequan%20Cui%20and%20Hanwang%20Zhang%20and%20Chi%20Zhang&entry.1292438233=%20%20While%20image-text%20foundation%20models%20have%20succeeded%20across%20diverse%20downstream%0Atasks%2C%20they%20still%20face%20challenges%20in%20the%20presence%20of%20spurious%20correlations%0Abetween%20the%20input%20and%20label.%20To%20address%20this%20issue%2C%20we%20propose%20a%20simple%0Athree-step%20approach%2CProject-Probe-Aggregate%20%28PPA%29%2C%20that%20enables%0Aparameter-efficient%20fine-tuning%20for%20foundation%20models%20without%20relying%20on%20group%0Aannotations.%20Building%20upon%20the%20failure-based%20debiasing%20scheme%2C%20our%20method%2C%20PPA%2C%0Aimproves%20its%20two%20key%20components%3A%20minority%20samples%20identification%20and%20the%20robust%0Atraining%20algorithm.%20Specifically%2C%20we%20first%20train%20biased%20classifiers%20by%0Aprojecting%20image%20features%20onto%20the%20nullspace%20of%20class%20proxies%20from%20text%0Aencoders.%20Next%2C%20we%20infer%20group%20labels%20using%20the%20biased%20classifier%20and%20probe%0Agroup%20targets%20with%20prior%20correction.%20Finally%2C%20we%20aggregate%20group%20weights%20of%0Aeach%20class%20to%20produce%20the%20debiased%20classifier.%20Our%20theoretical%20analysis%20shows%0Athat%20our%20PPA%20enhances%20minority%20group%20identification%20and%20is%20Bayes%20optimal%20for%0Aminimizing%20the%20balanced%20group%20error%2C%20mitigating%20spurious%20correlations.%0AExtensive%20experimental%20results%20confirm%20the%20effectiveness%20of%20our%20PPA%3A%20it%0Aoutperforms%20the%20state-of-the-art%20by%20an%20average%20worst-group%20accuracy%20while%0Arequiring%20less%20than%200.01%25%20tunable%20parameters%20without%20training%20group%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09487v3&entry.124074799=Read"},
{"title": "SoccerNet 2025 Challenges Results", "author": "Silvio Giancola and Anthony Cioppa and Marc Guti\u00e9rrez-P\u00e9rez and Jan Held and Carlos Hinojosa and Victor Joos and Arnaud Leduc and Floriane Magera and Karen Sanchez and Vladimir Somers and Artur Xarles and Antonio Agudo and Alexandre Alahi and Olivier Barnich and Albert Clap\u00e9s and Christophe De Vleeschouwer and Sergio Escalera and Bernard Ghanem and Thomas B. Moeslund and Marc Van Droogenbroeck and Tomoki Abe and Saad Alotaibi and Faisal Altawijri and Steven Araujo and Xiang Bai and Xiaoyang Bi and Jiawang Cao and Vanyi Chao and Kamil Czarnog\u00f3rski and Fabian Deuser and Mingyang Du and Tianrui Feng and Patrick Frenzel and Mirco Fuchs and Jorge Garc\u00eda and Konrad Habel and Takaya Hashiguchi and Sadao Hirose and Xinting Hu and Yewon Hwang and Ririko Inoue and Riku Itsuji and Kazuto Iwai and Hongwei Ji and Yangguang Ji and Licheng Jiao and Yuto Kageyama and Yuta Kamikawa and Yuuki Kanasugi and Hyungjung Kim and Jinwook Kim and Takuya Kurihara and Bozheng Li and Lingling Li and Xian Li and Youxing Lian and Dingkang Liang and Hongkai Lin and Jiadong Lin and Jian Liu and Liang Liu and Shuaikun Liu and Zhaohong Liu and Yi Lu and Federico M\u00e9ndez and Huadong Ma and Wenping Ma and Jacek Maksymiuk and Henry Mantilla and Ismail Mathkour and Daniel Matthes and Ayaha Motomochi and Amrulloh Robbani Muhammad and Haruto Nakayama and Joohyung Oh and Yin May Oo and Marcelo Ortega and Norbert Oswald and Rintaro Otsubo and Fabian Perez and Mengshi Qi and Cristian Rey and Abel Reyes-Angulo and Oliver Rose and Hoover Rueda-Chac\u00f3n and Hideo Saito and Jose Sarmiento and Kanta Sawafuji and Atom Scott and Xi Shen and Pragyan Shrestha and Jae-Young Sim and Long Sun and Yuyang Sun and Tomohiro Suzuki and Licheng Tang and Masato Tonouchi and Ikuma Uchida and Henry O. Velesaca and Tiancheng Wang and Rio Watanabe and Jay Wu and Yongliang Wu and Shunzo Yamagishi and Di Yang and Xu Yang and Yuxin Yang and Hao Ye and Xinyu Ye and Calvin Yeung and Xuanlong Yu and Chao Zhang and Dingyuan Zhang and Kexing Zhang and Zhe Zhao and Xin Zhou and Wenbo Zhu and Julian Ziegler", "abstract": "  The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNet\nopen benchmarking effort, dedicated to advancing computer vision research in\nfootball video understanding. This year's challenges span four vision-based\ntasks: (1) Team Ball Action Spotting, focused on detecting ball-related actions\nin football broadcasts and assigning actions to teams; (2) Monocular Depth\nEstimation, targeting the recovery of scene geometry from single-camera\nbroadcast clips through relative depth estimation for each pixel; (3)\nMulti-View Foul Recognition, requiring the analysis of multiple synchronized\ncamera views to classify fouls and their severity; and (4) Game State\nReconstruction, aimed at localizing and identifying all players from a\nbroadcast video to reconstruct the game state on a 2D top-view of the field.\nAcross all tasks, participants were provided with large-scale annotated\ndatasets, unified evaluation protocols, and strong baselines as starting\npoints. This report presents the results of each challenge, highlights the\ntop-performing solutions, and provides insights into the progress made by the\ncommunity. The SoccerNet Challenges continue to serve as a driving force for\nreproducible, open research at the intersection of computer vision, artificial\nintelligence, and sports. Detailed information about the tasks, challenges, and\nleaderboards can be found at https://www.soccer-net.org, with baselines and\ndevelopment kits available at https://github.com/SoccerNet.\n", "link": "http://arxiv.org/abs/2508.19182v1", "date": "2025-08-26", "relevancy": 2.0504, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoccerNet%202025%20Challenges%20Results&body=Title%3A%20SoccerNet%202025%20Challenges%20Results%0AAuthor%3A%20Silvio%20Giancola%20and%20Anthony%20Cioppa%20and%20Marc%20Guti%C3%A9rrez-P%C3%A9rez%20and%20Jan%20Held%20and%20Carlos%20Hinojosa%20and%20Victor%20Joos%20and%20Arnaud%20Leduc%20and%20Floriane%20Magera%20and%20Karen%20Sanchez%20and%20Vladimir%20Somers%20and%20Artur%20Xarles%20and%20Antonio%20Agudo%20and%20Alexandre%20Alahi%20and%20Olivier%20Barnich%20and%20Albert%20Clap%C3%A9s%20and%20Christophe%20De%20Vleeschouwer%20and%20Sergio%20Escalera%20and%20Bernard%20Ghanem%20and%20Thomas%20B.%20Moeslund%20and%20Marc%20Van%20Droogenbroeck%20and%20Tomoki%20Abe%20and%20Saad%20Alotaibi%20and%20Faisal%20Altawijri%20and%20Steven%20Araujo%20and%20Xiang%20Bai%20and%20Xiaoyang%20Bi%20and%20Jiawang%20Cao%20and%20Vanyi%20Chao%20and%20Kamil%20Czarnog%C3%B3rski%20and%20Fabian%20Deuser%20and%20Mingyang%20Du%20and%20Tianrui%20Feng%20and%20Patrick%20Frenzel%20and%20Mirco%20Fuchs%20and%20Jorge%20Garc%C3%ADa%20and%20Konrad%20Habel%20and%20Takaya%20Hashiguchi%20and%20Sadao%20Hirose%20and%20Xinting%20Hu%20and%20Yewon%20Hwang%20and%20Ririko%20Inoue%20and%20Riku%20Itsuji%20and%20Kazuto%20Iwai%20and%20Hongwei%20Ji%20and%20Yangguang%20Ji%20and%20Licheng%20Jiao%20and%20Yuto%20Kageyama%20and%20Yuta%20Kamikawa%20and%20Yuuki%20Kanasugi%20and%20Hyungjung%20Kim%20and%20Jinwook%20Kim%20and%20Takuya%20Kurihara%20and%20Bozheng%20Li%20and%20Lingling%20Li%20and%20Xian%20Li%20and%20Youxing%20Lian%20and%20Dingkang%20Liang%20and%20Hongkai%20Lin%20and%20Jiadong%20Lin%20and%20Jian%20Liu%20and%20Liang%20Liu%20and%20Shuaikun%20Liu%20and%20Zhaohong%20Liu%20and%20Yi%20Lu%20and%20Federico%20M%C3%A9ndez%20and%20Huadong%20Ma%20and%20Wenping%20Ma%20and%20Jacek%20Maksymiuk%20and%20Henry%20Mantilla%20and%20Ismail%20Mathkour%20and%20Daniel%20Matthes%20and%20Ayaha%20Motomochi%20and%20Amrulloh%20Robbani%20Muhammad%20and%20Haruto%20Nakayama%20and%20Joohyung%20Oh%20and%20Yin%20May%20Oo%20and%20Marcelo%20Ortega%20and%20Norbert%20Oswald%20and%20Rintaro%20Otsubo%20and%20Fabian%20Perez%20and%20Mengshi%20Qi%20and%20Cristian%20Rey%20and%20Abel%20Reyes-Angulo%20and%20Oliver%20Rose%20and%20Hoover%20Rueda-Chac%C3%B3n%20and%20Hideo%20Saito%20and%20Jose%20Sarmiento%20and%20Kanta%20Sawafuji%20and%20Atom%20Scott%20and%20Xi%20Shen%20and%20Pragyan%20Shrestha%20and%20Jae-Young%20Sim%20and%20Long%20Sun%20and%20Yuyang%20Sun%20and%20Tomohiro%20Suzuki%20and%20Licheng%20Tang%20and%20Masato%20Tonouchi%20and%20Ikuma%20Uchida%20and%20Henry%20O.%20Velesaca%20and%20Tiancheng%20Wang%20and%20Rio%20Watanabe%20and%20Jay%20Wu%20and%20Yongliang%20Wu%20and%20Shunzo%20Yamagishi%20and%20Di%20Yang%20and%20Xu%20Yang%20and%20Yuxin%20Yang%20and%20Hao%20Ye%20and%20Xinyu%20Ye%20and%20Calvin%20Yeung%20and%20Xuanlong%20Yu%20and%20Chao%20Zhang%20and%20Dingyuan%20Zhang%20and%20Kexing%20Zhang%20and%20Zhe%20Zhao%20and%20Xin%20Zhou%20and%20Wenbo%20Zhu%20and%20Julian%20Ziegler%0AAbstract%3A%20%20%20The%20SoccerNet%202025%20Challenges%20mark%20the%20fifth%20annual%20edition%20of%20the%20SoccerNet%0Aopen%20benchmarking%20effort%2C%20dedicated%20to%20advancing%20computer%20vision%20research%20in%0Afootball%20video%20understanding.%20This%20year%27s%20challenges%20span%20four%20vision-based%0Atasks%3A%20%281%29%20Team%20Ball%20Action%20Spotting%2C%20focused%20on%20detecting%20ball-related%20actions%0Ain%20football%20broadcasts%20and%20assigning%20actions%20to%20teams%3B%20%282%29%20Monocular%20Depth%0AEstimation%2C%20targeting%20the%20recovery%20of%20scene%20geometry%20from%20single-camera%0Abroadcast%20clips%20through%20relative%20depth%20estimation%20for%20each%20pixel%3B%20%283%29%0AMulti-View%20Foul%20Recognition%2C%20requiring%20the%20analysis%20of%20multiple%20synchronized%0Acamera%20views%20to%20classify%20fouls%20and%20their%20severity%3B%20and%20%284%29%20Game%20State%0AReconstruction%2C%20aimed%20at%20localizing%20and%20identifying%20all%20players%20from%20a%0Abroadcast%20video%20to%20reconstruct%20the%20game%20state%20on%20a%202D%20top-view%20of%20the%20field.%0AAcross%20all%20tasks%2C%20participants%20were%20provided%20with%20large-scale%20annotated%0Adatasets%2C%20unified%20evaluation%20protocols%2C%20and%20strong%20baselines%20as%20starting%0Apoints.%20This%20report%20presents%20the%20results%20of%20each%20challenge%2C%20highlights%20the%0Atop-performing%20solutions%2C%20and%20provides%20insights%20into%20the%20progress%20made%20by%20the%0Acommunity.%20The%20SoccerNet%20Challenges%20continue%20to%20serve%20as%20a%20driving%20force%20for%0Areproducible%2C%20open%20research%20at%20the%20intersection%20of%20computer%20vision%2C%20artificial%0Aintelligence%2C%20and%20sports.%20Detailed%20information%20about%20the%20tasks%2C%20challenges%2C%20and%0Aleaderboards%20can%20be%20found%20at%20https%3A//www.soccer-net.org%2C%20with%20baselines%20and%0Adevelopment%20kits%20available%20at%20https%3A//github.com/SoccerNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoccerNet%25202025%2520Challenges%2520Results%26entry.906535625%3DSilvio%2520Giancola%2520and%2520Anthony%2520Cioppa%2520and%2520Marc%2520Guti%25C3%25A9rrez-P%25C3%25A9rez%2520and%2520Jan%2520Held%2520and%2520Carlos%2520Hinojosa%2520and%2520Victor%2520Joos%2520and%2520Arnaud%2520Leduc%2520and%2520Floriane%2520Magera%2520and%2520Karen%2520Sanchez%2520and%2520Vladimir%2520Somers%2520and%2520Artur%2520Xarles%2520and%2520Antonio%2520Agudo%2520and%2520Alexandre%2520Alahi%2520and%2520Olivier%2520Barnich%2520and%2520Albert%2520Clap%25C3%25A9s%2520and%2520Christophe%2520De%2520Vleeschouwer%2520and%2520Sergio%2520Escalera%2520and%2520Bernard%2520Ghanem%2520and%2520Thomas%2520B.%2520Moeslund%2520and%2520Marc%2520Van%2520Droogenbroeck%2520and%2520Tomoki%2520Abe%2520and%2520Saad%2520Alotaibi%2520and%2520Faisal%2520Altawijri%2520and%2520Steven%2520Araujo%2520and%2520Xiang%2520Bai%2520and%2520Xiaoyang%2520Bi%2520and%2520Jiawang%2520Cao%2520and%2520Vanyi%2520Chao%2520and%2520Kamil%2520Czarnog%25C3%25B3rski%2520and%2520Fabian%2520Deuser%2520and%2520Mingyang%2520Du%2520and%2520Tianrui%2520Feng%2520and%2520Patrick%2520Frenzel%2520and%2520Mirco%2520Fuchs%2520and%2520Jorge%2520Garc%25C3%25ADa%2520and%2520Konrad%2520Habel%2520and%2520Takaya%2520Hashiguchi%2520and%2520Sadao%2520Hirose%2520and%2520Xinting%2520Hu%2520and%2520Yewon%2520Hwang%2520and%2520Ririko%2520Inoue%2520and%2520Riku%2520Itsuji%2520and%2520Kazuto%2520Iwai%2520and%2520Hongwei%2520Ji%2520and%2520Yangguang%2520Ji%2520and%2520Licheng%2520Jiao%2520and%2520Yuto%2520Kageyama%2520and%2520Yuta%2520Kamikawa%2520and%2520Yuuki%2520Kanasugi%2520and%2520Hyungjung%2520Kim%2520and%2520Jinwook%2520Kim%2520and%2520Takuya%2520Kurihara%2520and%2520Bozheng%2520Li%2520and%2520Lingling%2520Li%2520and%2520Xian%2520Li%2520and%2520Youxing%2520Lian%2520and%2520Dingkang%2520Liang%2520and%2520Hongkai%2520Lin%2520and%2520Jiadong%2520Lin%2520and%2520Jian%2520Liu%2520and%2520Liang%2520Liu%2520and%2520Shuaikun%2520Liu%2520and%2520Zhaohong%2520Liu%2520and%2520Yi%2520Lu%2520and%2520Federico%2520M%25C3%25A9ndez%2520and%2520Huadong%2520Ma%2520and%2520Wenping%2520Ma%2520and%2520Jacek%2520Maksymiuk%2520and%2520Henry%2520Mantilla%2520and%2520Ismail%2520Mathkour%2520and%2520Daniel%2520Matthes%2520and%2520Ayaha%2520Motomochi%2520and%2520Amrulloh%2520Robbani%2520Muhammad%2520and%2520Haruto%2520Nakayama%2520and%2520Joohyung%2520Oh%2520and%2520Yin%2520May%2520Oo%2520and%2520Marcelo%2520Ortega%2520and%2520Norbert%2520Oswald%2520and%2520Rintaro%2520Otsubo%2520and%2520Fabian%2520Perez%2520and%2520Mengshi%2520Qi%2520and%2520Cristian%2520Rey%2520and%2520Abel%2520Reyes-Angulo%2520and%2520Oliver%2520Rose%2520and%2520Hoover%2520Rueda-Chac%25C3%25B3n%2520and%2520Hideo%2520Saito%2520and%2520Jose%2520Sarmiento%2520and%2520Kanta%2520Sawafuji%2520and%2520Atom%2520Scott%2520and%2520Xi%2520Shen%2520and%2520Pragyan%2520Shrestha%2520and%2520Jae-Young%2520Sim%2520and%2520Long%2520Sun%2520and%2520Yuyang%2520Sun%2520and%2520Tomohiro%2520Suzuki%2520and%2520Licheng%2520Tang%2520and%2520Masato%2520Tonouchi%2520and%2520Ikuma%2520Uchida%2520and%2520Henry%2520O.%2520Velesaca%2520and%2520Tiancheng%2520Wang%2520and%2520Rio%2520Watanabe%2520and%2520Jay%2520Wu%2520and%2520Yongliang%2520Wu%2520and%2520Shunzo%2520Yamagishi%2520and%2520Di%2520Yang%2520and%2520Xu%2520Yang%2520and%2520Yuxin%2520Yang%2520and%2520Hao%2520Ye%2520and%2520Xinyu%2520Ye%2520and%2520Calvin%2520Yeung%2520and%2520Xuanlong%2520Yu%2520and%2520Chao%2520Zhang%2520and%2520Dingyuan%2520Zhang%2520and%2520Kexing%2520Zhang%2520and%2520Zhe%2520Zhao%2520and%2520Xin%2520Zhou%2520and%2520Wenbo%2520Zhu%2520and%2520Julian%2520Ziegler%26entry.1292438233%3D%2520%2520The%2520SoccerNet%25202025%2520Challenges%2520mark%2520the%2520fifth%2520annual%2520edition%2520of%2520the%2520SoccerNet%250Aopen%2520benchmarking%2520effort%252C%2520dedicated%2520to%2520advancing%2520computer%2520vision%2520research%2520in%250Afootball%2520video%2520understanding.%2520This%2520year%2527s%2520challenges%2520span%2520four%2520vision-based%250Atasks%253A%2520%25281%2529%2520Team%2520Ball%2520Action%2520Spotting%252C%2520focused%2520on%2520detecting%2520ball-related%2520actions%250Ain%2520football%2520broadcasts%2520and%2520assigning%2520actions%2520to%2520teams%253B%2520%25282%2529%2520Monocular%2520Depth%250AEstimation%252C%2520targeting%2520the%2520recovery%2520of%2520scene%2520geometry%2520from%2520single-camera%250Abroadcast%2520clips%2520through%2520relative%2520depth%2520estimation%2520for%2520each%2520pixel%253B%2520%25283%2529%250AMulti-View%2520Foul%2520Recognition%252C%2520requiring%2520the%2520analysis%2520of%2520multiple%2520synchronized%250Acamera%2520views%2520to%2520classify%2520fouls%2520and%2520their%2520severity%253B%2520and%2520%25284%2529%2520Game%2520State%250AReconstruction%252C%2520aimed%2520at%2520localizing%2520and%2520identifying%2520all%2520players%2520from%2520a%250Abroadcast%2520video%2520to%2520reconstruct%2520the%2520game%2520state%2520on%2520a%25202D%2520top-view%2520of%2520the%2520field.%250AAcross%2520all%2520tasks%252C%2520participants%2520were%2520provided%2520with%2520large-scale%2520annotated%250Adatasets%252C%2520unified%2520evaluation%2520protocols%252C%2520and%2520strong%2520baselines%2520as%2520starting%250Apoints.%2520This%2520report%2520presents%2520the%2520results%2520of%2520each%2520challenge%252C%2520highlights%2520the%250Atop-performing%2520solutions%252C%2520and%2520provides%2520insights%2520into%2520the%2520progress%2520made%2520by%2520the%250Acommunity.%2520The%2520SoccerNet%2520Challenges%2520continue%2520to%2520serve%2520as%2520a%2520driving%2520force%2520for%250Areproducible%252C%2520open%2520research%2520at%2520the%2520intersection%2520of%2520computer%2520vision%252C%2520artificial%250Aintelligence%252C%2520and%2520sports.%2520Detailed%2520information%2520about%2520the%2520tasks%252C%2520challenges%252C%2520and%250Aleaderboards%2520can%2520be%2520found%2520at%2520https%253A//www.soccer-net.org%252C%2520with%2520baselines%2520and%250Adevelopment%2520kits%2520available%2520at%2520https%253A//github.com/SoccerNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoccerNet%202025%20Challenges%20Results&entry.906535625=Silvio%20Giancola%20and%20Anthony%20Cioppa%20and%20Marc%20Guti%C3%A9rrez-P%C3%A9rez%20and%20Jan%20Held%20and%20Carlos%20Hinojosa%20and%20Victor%20Joos%20and%20Arnaud%20Leduc%20and%20Floriane%20Magera%20and%20Karen%20Sanchez%20and%20Vladimir%20Somers%20and%20Artur%20Xarles%20and%20Antonio%20Agudo%20and%20Alexandre%20Alahi%20and%20Olivier%20Barnich%20and%20Albert%20Clap%C3%A9s%20and%20Christophe%20De%20Vleeschouwer%20and%20Sergio%20Escalera%20and%20Bernard%20Ghanem%20and%20Thomas%20B.%20Moeslund%20and%20Marc%20Van%20Droogenbroeck%20and%20Tomoki%20Abe%20and%20Saad%20Alotaibi%20and%20Faisal%20Altawijri%20and%20Steven%20Araujo%20and%20Xiang%20Bai%20and%20Xiaoyang%20Bi%20and%20Jiawang%20Cao%20and%20Vanyi%20Chao%20and%20Kamil%20Czarnog%C3%B3rski%20and%20Fabian%20Deuser%20and%20Mingyang%20Du%20and%20Tianrui%20Feng%20and%20Patrick%20Frenzel%20and%20Mirco%20Fuchs%20and%20Jorge%20Garc%C3%ADa%20and%20Konrad%20Habel%20and%20Takaya%20Hashiguchi%20and%20Sadao%20Hirose%20and%20Xinting%20Hu%20and%20Yewon%20Hwang%20and%20Ririko%20Inoue%20and%20Riku%20Itsuji%20and%20Kazuto%20Iwai%20and%20Hongwei%20Ji%20and%20Yangguang%20Ji%20and%20Licheng%20Jiao%20and%20Yuto%20Kageyama%20and%20Yuta%20Kamikawa%20and%20Yuuki%20Kanasugi%20and%20Hyungjung%20Kim%20and%20Jinwook%20Kim%20and%20Takuya%20Kurihara%20and%20Bozheng%20Li%20and%20Lingling%20Li%20and%20Xian%20Li%20and%20Youxing%20Lian%20and%20Dingkang%20Liang%20and%20Hongkai%20Lin%20and%20Jiadong%20Lin%20and%20Jian%20Liu%20and%20Liang%20Liu%20and%20Shuaikun%20Liu%20and%20Zhaohong%20Liu%20and%20Yi%20Lu%20and%20Federico%20M%C3%A9ndez%20and%20Huadong%20Ma%20and%20Wenping%20Ma%20and%20Jacek%20Maksymiuk%20and%20Henry%20Mantilla%20and%20Ismail%20Mathkour%20and%20Daniel%20Matthes%20and%20Ayaha%20Motomochi%20and%20Amrulloh%20Robbani%20Muhammad%20and%20Haruto%20Nakayama%20and%20Joohyung%20Oh%20and%20Yin%20May%20Oo%20and%20Marcelo%20Ortega%20and%20Norbert%20Oswald%20and%20Rintaro%20Otsubo%20and%20Fabian%20Perez%20and%20Mengshi%20Qi%20and%20Cristian%20Rey%20and%20Abel%20Reyes-Angulo%20and%20Oliver%20Rose%20and%20Hoover%20Rueda-Chac%C3%B3n%20and%20Hideo%20Saito%20and%20Jose%20Sarmiento%20and%20Kanta%20Sawafuji%20and%20Atom%20Scott%20and%20Xi%20Shen%20and%20Pragyan%20Shrestha%20and%20Jae-Young%20Sim%20and%20Long%20Sun%20and%20Yuyang%20Sun%20and%20Tomohiro%20Suzuki%20and%20Licheng%20Tang%20and%20Masato%20Tonouchi%20and%20Ikuma%20Uchida%20and%20Henry%20O.%20Velesaca%20and%20Tiancheng%20Wang%20and%20Rio%20Watanabe%20and%20Jay%20Wu%20and%20Yongliang%20Wu%20and%20Shunzo%20Yamagishi%20and%20Di%20Yang%20and%20Xu%20Yang%20and%20Yuxin%20Yang%20and%20Hao%20Ye%20and%20Xinyu%20Ye%20and%20Calvin%20Yeung%20and%20Xuanlong%20Yu%20and%20Chao%20Zhang%20and%20Dingyuan%20Zhang%20and%20Kexing%20Zhang%20and%20Zhe%20Zhao%20and%20Xin%20Zhou%20and%20Wenbo%20Zhu%20and%20Julian%20Ziegler&entry.1292438233=%20%20The%20SoccerNet%202025%20Challenges%20mark%20the%20fifth%20annual%20edition%20of%20the%20SoccerNet%0Aopen%20benchmarking%20effort%2C%20dedicated%20to%20advancing%20computer%20vision%20research%20in%0Afootball%20video%20understanding.%20This%20year%27s%20challenges%20span%20four%20vision-based%0Atasks%3A%20%281%29%20Team%20Ball%20Action%20Spotting%2C%20focused%20on%20detecting%20ball-related%20actions%0Ain%20football%20broadcasts%20and%20assigning%20actions%20to%20teams%3B%20%282%29%20Monocular%20Depth%0AEstimation%2C%20targeting%20the%20recovery%20of%20scene%20geometry%20from%20single-camera%0Abroadcast%20clips%20through%20relative%20depth%20estimation%20for%20each%20pixel%3B%20%283%29%0AMulti-View%20Foul%20Recognition%2C%20requiring%20the%20analysis%20of%20multiple%20synchronized%0Acamera%20views%20to%20classify%20fouls%20and%20their%20severity%3B%20and%20%284%29%20Game%20State%0AReconstruction%2C%20aimed%20at%20localizing%20and%20identifying%20all%20players%20from%20a%0Abroadcast%20video%20to%20reconstruct%20the%20game%20state%20on%20a%202D%20top-view%20of%20the%20field.%0AAcross%20all%20tasks%2C%20participants%20were%20provided%20with%20large-scale%20annotated%0Adatasets%2C%20unified%20evaluation%20protocols%2C%20and%20strong%20baselines%20as%20starting%0Apoints.%20This%20report%20presents%20the%20results%20of%20each%20challenge%2C%20highlights%20the%0Atop-performing%20solutions%2C%20and%20provides%20insights%20into%20the%20progress%20made%20by%20the%0Acommunity.%20The%20SoccerNet%20Challenges%20continue%20to%20serve%20as%20a%20driving%20force%20for%0Areproducible%2C%20open%20research%20at%20the%20intersection%20of%20computer%20vision%2C%20artificial%0Aintelligence%2C%20and%20sports.%20Detailed%20information%20about%20the%20tasks%2C%20challenges%2C%20and%0Aleaderboards%20can%20be%20found%20at%20https%3A//www.soccer-net.org%2C%20with%20baselines%20and%0Adevelopment%20kits%20available%20at%20https%3A//github.com/SoccerNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19182v1&entry.124074799=Read"},
{"title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning", "author": "Yang Zhou and Sunzhu Li and Shunyu Liu and Wenkai Fang and Jiale Zhao and Jingwen Yang and Jianwei Lv and Kongcheng Zhang and Yihe Zhou and Hengtong Lu and Wei Chen and Yan Xie and Mingli Song", "abstract": "  Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. This work is still in progress, and we will release\nthe code, the models, and the datasets soon.\n", "link": "http://arxiv.org/abs/2508.16949v2", "date": "2025-08-26", "relevancy": 2.05, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning&body=Title%3A%20Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning%0AAuthor%3A%20Yang%20Zhou%20and%20Sunzhu%20Li%20and%20Shunyu%20Liu%20and%20Wenkai%20Fang%20and%20Jiale%20Zhao%20and%20Jingwen%20Yang%20and%20Jianwei%20Lv%20and%20Kongcheng%20Zhang%20and%20Yihe%20Zhou%20and%20Hengtong%20Lu%20and%20Wei%20Chen%20and%20Yan%20Xie%20and%20Mingli%20Song%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20underscored%20the%0Apotential%20of%20Reinforcement%20Learning%20%28RL%29%20to%20facilitate%20the%20emergence%20of%0Areasoning%20capabilities.%20Despite%20the%20encouraging%20results%2C%20a%20fundamental%20dilemma%0Apersists%20as%20RL%20improvement%20relies%20on%20learning%20from%20high-quality%20samples%2C%20yet%0Athe%20exploration%20for%20such%20samples%20remains%20bounded%20by%20the%20inherent%20limitations%20of%0ALLMs.%20This%2C%20in%20effect%2C%20creates%20an%20undesirable%20cycle%20in%20which%20what%20cannot%20be%0Aexplored%20cannot%20be%20learned.%20In%20this%20work%2C%20we%20propose%20Rubric-Scaffolded%0AReinforcement%20Learning%20%28RuscaRL%29%2C%20a%20novel%20instructional%20scaffolding%20framework%0Adesigned%20to%20break%20the%20exploration%20bottleneck%20for%20general%20LLM%20reasoning.%0ASpecifically%2C%20RuscaRL%20introduces%20checklist-style%20rubrics%20as%20%281%29%20explicit%0Ascaffolding%20for%20exploration%20during%20rollout%20generation%2C%20where%20different%20rubrics%0Aare%20provided%20as%20external%20guidance%20within%20task%20instructions%20to%20steer%20diverse%0Ahigh-quality%20responses.%20This%20guidance%20is%20gradually%20decayed%20over%20time%2C%0Aencouraging%20the%20model%20to%20internalize%20the%20underlying%20reasoning%20patterns%3B%20%282%29%0Averifiable%20rewards%20for%20exploitation%20during%20model%20training%2C%20where%20we%20can%20obtain%0Arobust%20LLM-as-a-Judge%20scores%20using%20rubrics%20as%20references%2C%20enabling%20effective%20RL%0Aon%20general%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20RuscaRL%20across%20various%20benchmarks%2C%20effectively%20expanding%0Areasoning%20boundaries%20under%20the%20best-of-N%20evaluation.%20Notably%2C%20RuscaRL%0Asignificantly%20boosts%20Qwen2.5-7B-Instruct%20from%2023.6%20to%2050.3%20on%20HealthBench-500%2C%0Asurpassing%20GPT-4.1.%20Furthermore%2C%20our%20fine-tuned%20variant%20on%0AQwen3-30B-A3B-Instruct%20achieves%2061.1%20on%20HealthBench-500%2C%20outperforming%20leading%0ALLMs%20including%20OpenAI-o3.%20This%20work%20is%20still%20in%20progress%2C%20and%20we%20will%20release%0Athe%20code%2C%20the%20models%2C%20and%20the%20datasets%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16949v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Exploration%2520Bottleneck%253A%2520Rubric-Scaffolded%2520Reinforcement%250A%2520%2520Learning%2520for%2520General%2520LLM%2520Reasoning%26entry.906535625%3DYang%2520Zhou%2520and%2520Sunzhu%2520Li%2520and%2520Shunyu%2520Liu%2520and%2520Wenkai%2520Fang%2520and%2520Jiale%2520Zhao%2520and%2520Jingwen%2520Yang%2520and%2520Jianwei%2520Lv%2520and%2520Kongcheng%2520Zhang%2520and%2520Yihe%2520Zhou%2520and%2520Hengtong%2520Lu%2520and%2520Wei%2520Chen%2520and%2520Yan%2520Xie%2520and%2520Mingli%2520Song%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520underscored%2520the%250Apotential%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%2520facilitate%2520the%2520emergence%2520of%250Areasoning%2520capabilities.%2520Despite%2520the%2520encouraging%2520results%252C%2520a%2520fundamental%2520dilemma%250Apersists%2520as%2520RL%2520improvement%2520relies%2520on%2520learning%2520from%2520high-quality%2520samples%252C%2520yet%250Athe%2520exploration%2520for%2520such%2520samples%2520remains%2520bounded%2520by%2520the%2520inherent%2520limitations%2520of%250ALLMs.%2520This%252C%2520in%2520effect%252C%2520creates%2520an%2520undesirable%2520cycle%2520in%2520which%2520what%2520cannot%2520be%250Aexplored%2520cannot%2520be%2520learned.%2520In%2520this%2520work%252C%2520we%2520propose%2520Rubric-Scaffolded%250AReinforcement%2520Learning%2520%2528RuscaRL%2529%252C%2520a%2520novel%2520instructional%2520scaffolding%2520framework%250Adesigned%2520to%2520break%2520the%2520exploration%2520bottleneck%2520for%2520general%2520LLM%2520reasoning.%250ASpecifically%252C%2520RuscaRL%2520introduces%2520checklist-style%2520rubrics%2520as%2520%25281%2529%2520explicit%250Ascaffolding%2520for%2520exploration%2520during%2520rollout%2520generation%252C%2520where%2520different%2520rubrics%250Aare%2520provided%2520as%2520external%2520guidance%2520within%2520task%2520instructions%2520to%2520steer%2520diverse%250Ahigh-quality%2520responses.%2520This%2520guidance%2520is%2520gradually%2520decayed%2520over%2520time%252C%250Aencouraging%2520the%2520model%2520to%2520internalize%2520the%2520underlying%2520reasoning%2520patterns%253B%2520%25282%2529%250Averifiable%2520rewards%2520for%2520exploitation%2520during%2520model%2520training%252C%2520where%2520we%2520can%2520obtain%250Arobust%2520LLM-as-a-Judge%2520scores%2520using%2520rubrics%2520as%2520references%252C%2520enabling%2520effective%2520RL%250Aon%2520general%2520reasoning%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%250Aof%2520the%2520proposed%2520RuscaRL%2520across%2520various%2520benchmarks%252C%2520effectively%2520expanding%250Areasoning%2520boundaries%2520under%2520the%2520best-of-N%2520evaluation.%2520Notably%252C%2520RuscaRL%250Asignificantly%2520boosts%2520Qwen2.5-7B-Instruct%2520from%252023.6%2520to%252050.3%2520on%2520HealthBench-500%252C%250Asurpassing%2520GPT-4.1.%2520Furthermore%252C%2520our%2520fine-tuned%2520variant%2520on%250AQwen3-30B-A3B-Instruct%2520achieves%252061.1%2520on%2520HealthBench-500%252C%2520outperforming%2520leading%250ALLMs%2520including%2520OpenAI-o3.%2520This%2520work%2520is%2520still%2520in%2520progress%252C%2520and%2520we%2520will%2520release%250Athe%2520code%252C%2520the%2520models%252C%2520and%2520the%2520datasets%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16949v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Exploration%20Bottleneck%3A%20Rubric-Scaffolded%20Reinforcement%0A%20%20Learning%20for%20General%20LLM%20Reasoning&entry.906535625=Yang%20Zhou%20and%20Sunzhu%20Li%20and%20Shunyu%20Liu%20and%20Wenkai%20Fang%20and%20Jiale%20Zhao%20and%20Jingwen%20Yang%20and%20Jianwei%20Lv%20and%20Kongcheng%20Zhang%20and%20Yihe%20Zhou%20and%20Hengtong%20Lu%20and%20Wei%20Chen%20and%20Yan%20Xie%20and%20Mingli%20Song&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20underscored%20the%0Apotential%20of%20Reinforcement%20Learning%20%28RL%29%20to%20facilitate%20the%20emergence%20of%0Areasoning%20capabilities.%20Despite%20the%20encouraging%20results%2C%20a%20fundamental%20dilemma%0Apersists%20as%20RL%20improvement%20relies%20on%20learning%20from%20high-quality%20samples%2C%20yet%0Athe%20exploration%20for%20such%20samples%20remains%20bounded%20by%20the%20inherent%20limitations%20of%0ALLMs.%20This%2C%20in%20effect%2C%20creates%20an%20undesirable%20cycle%20in%20which%20what%20cannot%20be%0Aexplored%20cannot%20be%20learned.%20In%20this%20work%2C%20we%20propose%20Rubric-Scaffolded%0AReinforcement%20Learning%20%28RuscaRL%29%2C%20a%20novel%20instructional%20scaffolding%20framework%0Adesigned%20to%20break%20the%20exploration%20bottleneck%20for%20general%20LLM%20reasoning.%0ASpecifically%2C%20RuscaRL%20introduces%20checklist-style%20rubrics%20as%20%281%29%20explicit%0Ascaffolding%20for%20exploration%20during%20rollout%20generation%2C%20where%20different%20rubrics%0Aare%20provided%20as%20external%20guidance%20within%20task%20instructions%20to%20steer%20diverse%0Ahigh-quality%20responses.%20This%20guidance%20is%20gradually%20decayed%20over%20time%2C%0Aencouraging%20the%20model%20to%20internalize%20the%20underlying%20reasoning%20patterns%3B%20%282%29%0Averifiable%20rewards%20for%20exploitation%20during%20model%20training%2C%20where%20we%20can%20obtain%0Arobust%20LLM-as-a-Judge%20scores%20using%20rubrics%20as%20references%2C%20enabling%20effective%20RL%0Aon%20general%20reasoning%20tasks.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20the%20proposed%20RuscaRL%20across%20various%20benchmarks%2C%20effectively%20expanding%0Areasoning%20boundaries%20under%20the%20best-of-N%20evaluation.%20Notably%2C%20RuscaRL%0Asignificantly%20boosts%20Qwen2.5-7B-Instruct%20from%2023.6%20to%2050.3%20on%20HealthBench-500%2C%0Asurpassing%20GPT-4.1.%20Furthermore%2C%20our%20fine-tuned%20variant%20on%0AQwen3-30B-A3B-Instruct%20achieves%2061.1%20on%20HealthBench-500%2C%20outperforming%20leading%0ALLMs%20including%20OpenAI-o3.%20This%20work%20is%20still%20in%20progress%2C%20and%20we%20will%20release%0Athe%20code%2C%20the%20models%2C%20and%20the%20datasets%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16949v2&entry.124074799=Read"},
{"title": "WetCat: Enabling Automated Skill Assessment in Wet-Lab Cataract Surgery\n  Videos", "author": "Negin Ghamsarian and Raphael Sznitman and Klaus Schoeffmann and Jens Kowal", "abstract": "  To meet the growing demand for systematic surgical training, wetlab\nenvironments have become indispensable platforms for hands-on practice in\nophthalmology. Yet, traditional wetlab training depends heavily on manual\nperformance evaluations, which are labor-intensive, time-consuming, and often\nsubject to variability. Recent advances in computer vision offer promising\navenues for automated skill assessment, enhancing both the efficiency and\nobjectivity of surgical education. Despite notable progress in ophthalmic\nsurgical datasets, existing resources predominantly focus on real surgeries or\nisolated tasks, falling short of supporting comprehensive skill evaluation in\ncontrolled wetlab settings. To address these limitations, we introduce WetCat,\nthe first dataset of wetlab cataract surgery videos specifically curated for\nautomated skill assessment. WetCat comprises high-resolution recordings of\nsurgeries performed by trainees on artificial eyes, featuring comprehensive\nphase annotations and semantic segmentations of key anatomical structures.\nThese annotations are meticulously designed to facilitate skill assessment\nduring the critical capsulorhexis and phacoemulsification phases, adhering to\nstandardized surgical skill assessment frameworks. By focusing on these\nessential phases, WetCat enables the development of interpretable, AI-driven\nevaluation tools aligned with established clinical metrics. This dataset lays a\nstrong foundation for advancing objective, scalable surgical education and sets\na new benchmark for automated workflow analysis and skill assessment in\nophthalmology training. The dataset and annotations are publicly available in\nSynapse https://www.synapse.org/Synapse:syn66401174/files.\n", "link": "http://arxiv.org/abs/2506.08896v3", "date": "2025-08-26", "relevancy": 2.0466, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5144}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5144}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WetCat%3A%20Enabling%20Automated%20Skill%20Assessment%20in%20Wet-Lab%20Cataract%20Surgery%0A%20%20Videos&body=Title%3A%20WetCat%3A%20Enabling%20Automated%20Skill%20Assessment%20in%20Wet-Lab%20Cataract%20Surgery%0A%20%20Videos%0AAuthor%3A%20Negin%20Ghamsarian%20and%20Raphael%20Sznitman%20and%20Klaus%20Schoeffmann%20and%20Jens%20Kowal%0AAbstract%3A%20%20%20To%20meet%20the%20growing%20demand%20for%20systematic%20surgical%20training%2C%20wetlab%0Aenvironments%20have%20become%20indispensable%20platforms%20for%20hands-on%20practice%20in%0Aophthalmology.%20Yet%2C%20traditional%20wetlab%20training%20depends%20heavily%20on%20manual%0Aperformance%20evaluations%2C%20which%20are%20labor-intensive%2C%20time-consuming%2C%20and%20often%0Asubject%20to%20variability.%20Recent%20advances%20in%20computer%20vision%20offer%20promising%0Aavenues%20for%20automated%20skill%20assessment%2C%20enhancing%20both%20the%20efficiency%20and%0Aobjectivity%20of%20surgical%20education.%20Despite%20notable%20progress%20in%20ophthalmic%0Asurgical%20datasets%2C%20existing%20resources%20predominantly%20focus%20on%20real%20surgeries%20or%0Aisolated%20tasks%2C%20falling%20short%20of%20supporting%20comprehensive%20skill%20evaluation%20in%0Acontrolled%20wetlab%20settings.%20To%20address%20these%20limitations%2C%20we%20introduce%20WetCat%2C%0Athe%20first%20dataset%20of%20wetlab%20cataract%20surgery%20videos%20specifically%20curated%20for%0Aautomated%20skill%20assessment.%20WetCat%20comprises%20high-resolution%20recordings%20of%0Asurgeries%20performed%20by%20trainees%20on%20artificial%20eyes%2C%20featuring%20comprehensive%0Aphase%20annotations%20and%20semantic%20segmentations%20of%20key%20anatomical%20structures.%0AThese%20annotations%20are%20meticulously%20designed%20to%20facilitate%20skill%20assessment%0Aduring%20the%20critical%20capsulorhexis%20and%20phacoemulsification%20phases%2C%20adhering%20to%0Astandardized%20surgical%20skill%20assessment%20frameworks.%20By%20focusing%20on%20these%0Aessential%20phases%2C%20WetCat%20enables%20the%20development%20of%20interpretable%2C%20AI-driven%0Aevaluation%20tools%20aligned%20with%20established%20clinical%20metrics.%20This%20dataset%20lays%20a%0Astrong%20foundation%20for%20advancing%20objective%2C%20scalable%20surgical%20education%20and%20sets%0Aa%20new%20benchmark%20for%20automated%20workflow%20analysis%20and%20skill%20assessment%20in%0Aophthalmology%20training.%20The%20dataset%20and%20annotations%20are%20publicly%20available%20in%0ASynapse%20https%3A//www.synapse.org/Synapse%3Asyn66401174/files.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08896v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWetCat%253A%2520Enabling%2520Automated%2520Skill%2520Assessment%2520in%2520Wet-Lab%2520Cataract%2520Surgery%250A%2520%2520Videos%26entry.906535625%3DNegin%2520Ghamsarian%2520and%2520Raphael%2520Sznitman%2520and%2520Klaus%2520Schoeffmann%2520and%2520Jens%2520Kowal%26entry.1292438233%3D%2520%2520To%2520meet%2520the%2520growing%2520demand%2520for%2520systematic%2520surgical%2520training%252C%2520wetlab%250Aenvironments%2520have%2520become%2520indispensable%2520platforms%2520for%2520hands-on%2520practice%2520in%250Aophthalmology.%2520Yet%252C%2520traditional%2520wetlab%2520training%2520depends%2520heavily%2520on%2520manual%250Aperformance%2520evaluations%252C%2520which%2520are%2520labor-intensive%252C%2520time-consuming%252C%2520and%2520often%250Asubject%2520to%2520variability.%2520Recent%2520advances%2520in%2520computer%2520vision%2520offer%2520promising%250Aavenues%2520for%2520automated%2520skill%2520assessment%252C%2520enhancing%2520both%2520the%2520efficiency%2520and%250Aobjectivity%2520of%2520surgical%2520education.%2520Despite%2520notable%2520progress%2520in%2520ophthalmic%250Asurgical%2520datasets%252C%2520existing%2520resources%2520predominantly%2520focus%2520on%2520real%2520surgeries%2520or%250Aisolated%2520tasks%252C%2520falling%2520short%2520of%2520supporting%2520comprehensive%2520skill%2520evaluation%2520in%250Acontrolled%2520wetlab%2520settings.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520WetCat%252C%250Athe%2520first%2520dataset%2520of%2520wetlab%2520cataract%2520surgery%2520videos%2520specifically%2520curated%2520for%250Aautomated%2520skill%2520assessment.%2520WetCat%2520comprises%2520high-resolution%2520recordings%2520of%250Asurgeries%2520performed%2520by%2520trainees%2520on%2520artificial%2520eyes%252C%2520featuring%2520comprehensive%250Aphase%2520annotations%2520and%2520semantic%2520segmentations%2520of%2520key%2520anatomical%2520structures.%250AThese%2520annotations%2520are%2520meticulously%2520designed%2520to%2520facilitate%2520skill%2520assessment%250Aduring%2520the%2520critical%2520capsulorhexis%2520and%2520phacoemulsification%2520phases%252C%2520adhering%2520to%250Astandardized%2520surgical%2520skill%2520assessment%2520frameworks.%2520By%2520focusing%2520on%2520these%250Aessential%2520phases%252C%2520WetCat%2520enables%2520the%2520development%2520of%2520interpretable%252C%2520AI-driven%250Aevaluation%2520tools%2520aligned%2520with%2520established%2520clinical%2520metrics.%2520This%2520dataset%2520lays%2520a%250Astrong%2520foundation%2520for%2520advancing%2520objective%252C%2520scalable%2520surgical%2520education%2520and%2520sets%250Aa%2520new%2520benchmark%2520for%2520automated%2520workflow%2520analysis%2520and%2520skill%2520assessment%2520in%250Aophthalmology%2520training.%2520The%2520dataset%2520and%2520annotations%2520are%2520publicly%2520available%2520in%250ASynapse%2520https%253A//www.synapse.org/Synapse%253Asyn66401174/files.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08896v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WetCat%3A%20Enabling%20Automated%20Skill%20Assessment%20in%20Wet-Lab%20Cataract%20Surgery%0A%20%20Videos&entry.906535625=Negin%20Ghamsarian%20and%20Raphael%20Sznitman%20and%20Klaus%20Schoeffmann%20and%20Jens%20Kowal&entry.1292438233=%20%20To%20meet%20the%20growing%20demand%20for%20systematic%20surgical%20training%2C%20wetlab%0Aenvironments%20have%20become%20indispensable%20platforms%20for%20hands-on%20practice%20in%0Aophthalmology.%20Yet%2C%20traditional%20wetlab%20training%20depends%20heavily%20on%20manual%0Aperformance%20evaluations%2C%20which%20are%20labor-intensive%2C%20time-consuming%2C%20and%20often%0Asubject%20to%20variability.%20Recent%20advances%20in%20computer%20vision%20offer%20promising%0Aavenues%20for%20automated%20skill%20assessment%2C%20enhancing%20both%20the%20efficiency%20and%0Aobjectivity%20of%20surgical%20education.%20Despite%20notable%20progress%20in%20ophthalmic%0Asurgical%20datasets%2C%20existing%20resources%20predominantly%20focus%20on%20real%20surgeries%20or%0Aisolated%20tasks%2C%20falling%20short%20of%20supporting%20comprehensive%20skill%20evaluation%20in%0Acontrolled%20wetlab%20settings.%20To%20address%20these%20limitations%2C%20we%20introduce%20WetCat%2C%0Athe%20first%20dataset%20of%20wetlab%20cataract%20surgery%20videos%20specifically%20curated%20for%0Aautomated%20skill%20assessment.%20WetCat%20comprises%20high-resolution%20recordings%20of%0Asurgeries%20performed%20by%20trainees%20on%20artificial%20eyes%2C%20featuring%20comprehensive%0Aphase%20annotations%20and%20semantic%20segmentations%20of%20key%20anatomical%20structures.%0AThese%20annotations%20are%20meticulously%20designed%20to%20facilitate%20skill%20assessment%0Aduring%20the%20critical%20capsulorhexis%20and%20phacoemulsification%20phases%2C%20adhering%20to%0Astandardized%20surgical%20skill%20assessment%20frameworks.%20By%20focusing%20on%20these%0Aessential%20phases%2C%20WetCat%20enables%20the%20development%20of%20interpretable%2C%20AI-driven%0Aevaluation%20tools%20aligned%20with%20established%20clinical%20metrics.%20This%20dataset%20lays%20a%0Astrong%20foundation%20for%20advancing%20objective%2C%20scalable%20surgical%20education%20and%20sets%0Aa%20new%20benchmark%20for%20automated%20workflow%20analysis%20and%20skill%20assessment%20in%0Aophthalmology%20training.%20The%20dataset%20and%20annotations%20are%20publicly%20available%20in%0ASynapse%20https%3A//www.synapse.org/Synapse%3Asyn66401174/files.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08896v3&entry.124074799=Read"},
{"title": "When recalling in-context, Transformers are not SSMs", "author": "Destiny Okpekpe and Antonio Orvieto", "abstract": "  Despite the advantageous subquadratic complexity of modern recurrent deep\nlearning models -- such as state-space models (SSMs) -- recent studies have\nhighlighted their potential shortcomings compared to transformers on reasoning\nand memorization tasks. In this paper, we dive deeper into one of such\nbenchmarks: associative recall (AR), which has been shown to correlate well\nwith language modeling performance, and inspect in detail the effects of\nscaling and optimization issues in recently proposed token mixing strategies.\nWe first demonstrate that, unlike standard transformers, the choice of learning\nrate plays a critical role in the performance of modern recurrent models: an\nissue that can severely affect reported performance in previous works and\nsuggests further research is needed to stabilize training. Next, we show that\nrecurrent and attention-based models exhibit contrasting benefits when scaling\nin width as opposed to depth, with attention being notably unable to solve AR\nwhen limited to a single layer. We then further inspect 1-layer transformers,\nrevealing that despite their poor performance, their training dynamics\nsurprisingly resemble the formation of induction heads, a phenomenon previously\nobserved only in their 2-layer counterparts. Finally, through architectural\nablations, we study how components affects Transformer and Mamba's performance\nand optimization stability.\n", "link": "http://arxiv.org/abs/2508.19029v1", "date": "2025-08-26", "relevancy": 2.0456, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5757}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5124}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20recalling%20in-context%2C%20Transformers%20are%20not%20SSMs&body=Title%3A%20When%20recalling%20in-context%2C%20Transformers%20are%20not%20SSMs%0AAuthor%3A%20Destiny%20Okpekpe%20and%20Antonio%20Orvieto%0AAbstract%3A%20%20%20Despite%20the%20advantageous%20subquadratic%20complexity%20of%20modern%20recurrent%20deep%0Alearning%20models%20--%20such%20as%20state-space%20models%20%28SSMs%29%20--%20recent%20studies%20have%0Ahighlighted%20their%20potential%20shortcomings%20compared%20to%20transformers%20on%20reasoning%0Aand%20memorization%20tasks.%20In%20this%20paper%2C%20we%20dive%20deeper%20into%20one%20of%20such%0Abenchmarks%3A%20associative%20recall%20%28AR%29%2C%20which%20has%20been%20shown%20to%20correlate%20well%0Awith%20language%20modeling%20performance%2C%20and%20inspect%20in%20detail%20the%20effects%20of%0Ascaling%20and%20optimization%20issues%20in%20recently%20proposed%20token%20mixing%20strategies.%0AWe%20first%20demonstrate%20that%2C%20unlike%20standard%20transformers%2C%20the%20choice%20of%20learning%0Arate%20plays%20a%20critical%20role%20in%20the%20performance%20of%20modern%20recurrent%20models%3A%20an%0Aissue%20that%20can%20severely%20affect%20reported%20performance%20in%20previous%20works%20and%0Asuggests%20further%20research%20is%20needed%20to%20stabilize%20training.%20Next%2C%20we%20show%20that%0Arecurrent%20and%20attention-based%20models%20exhibit%20contrasting%20benefits%20when%20scaling%0Ain%20width%20as%20opposed%20to%20depth%2C%20with%20attention%20being%20notably%20unable%20to%20solve%20AR%0Awhen%20limited%20to%20a%20single%20layer.%20We%20then%20further%20inspect%201-layer%20transformers%2C%0Arevealing%20that%20despite%20their%20poor%20performance%2C%20their%20training%20dynamics%0Asurprisingly%20resemble%20the%20formation%20of%20induction%20heads%2C%20a%20phenomenon%20previously%0Aobserved%20only%20in%20their%202-layer%20counterparts.%20Finally%2C%20through%20architectural%0Aablations%2C%20we%20study%20how%20components%20affects%20Transformer%20and%20Mamba%27s%20performance%0Aand%20optimization%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520recalling%2520in-context%252C%2520Transformers%2520are%2520not%2520SSMs%26entry.906535625%3DDestiny%2520Okpekpe%2520and%2520Antonio%2520Orvieto%26entry.1292438233%3D%2520%2520Despite%2520the%2520advantageous%2520subquadratic%2520complexity%2520of%2520modern%2520recurrent%2520deep%250Alearning%2520models%2520--%2520such%2520as%2520state-space%2520models%2520%2528SSMs%2529%2520--%2520recent%2520studies%2520have%250Ahighlighted%2520their%2520potential%2520shortcomings%2520compared%2520to%2520transformers%2520on%2520reasoning%250Aand%2520memorization%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520dive%2520deeper%2520into%2520one%2520of%2520such%250Abenchmarks%253A%2520associative%2520recall%2520%2528AR%2529%252C%2520which%2520has%2520been%2520shown%2520to%2520correlate%2520well%250Awith%2520language%2520modeling%2520performance%252C%2520and%2520inspect%2520in%2520detail%2520the%2520effects%2520of%250Ascaling%2520and%2520optimization%2520issues%2520in%2520recently%2520proposed%2520token%2520mixing%2520strategies.%250AWe%2520first%2520demonstrate%2520that%252C%2520unlike%2520standard%2520transformers%252C%2520the%2520choice%2520of%2520learning%250Arate%2520plays%2520a%2520critical%2520role%2520in%2520the%2520performance%2520of%2520modern%2520recurrent%2520models%253A%2520an%250Aissue%2520that%2520can%2520severely%2520affect%2520reported%2520performance%2520in%2520previous%2520works%2520and%250Asuggests%2520further%2520research%2520is%2520needed%2520to%2520stabilize%2520training.%2520Next%252C%2520we%2520show%2520that%250Arecurrent%2520and%2520attention-based%2520models%2520exhibit%2520contrasting%2520benefits%2520when%2520scaling%250Ain%2520width%2520as%2520opposed%2520to%2520depth%252C%2520with%2520attention%2520being%2520notably%2520unable%2520to%2520solve%2520AR%250Awhen%2520limited%2520to%2520a%2520single%2520layer.%2520We%2520then%2520further%2520inspect%25201-layer%2520transformers%252C%250Arevealing%2520that%2520despite%2520their%2520poor%2520performance%252C%2520their%2520training%2520dynamics%250Asurprisingly%2520resemble%2520the%2520formation%2520of%2520induction%2520heads%252C%2520a%2520phenomenon%2520previously%250Aobserved%2520only%2520in%2520their%25202-layer%2520counterparts.%2520Finally%252C%2520through%2520architectural%250Aablations%252C%2520we%2520study%2520how%2520components%2520affects%2520Transformer%2520and%2520Mamba%2527s%2520performance%250Aand%2520optimization%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20recalling%20in-context%2C%20Transformers%20are%20not%20SSMs&entry.906535625=Destiny%20Okpekpe%20and%20Antonio%20Orvieto&entry.1292438233=%20%20Despite%20the%20advantageous%20subquadratic%20complexity%20of%20modern%20recurrent%20deep%0Alearning%20models%20--%20such%20as%20state-space%20models%20%28SSMs%29%20--%20recent%20studies%20have%0Ahighlighted%20their%20potential%20shortcomings%20compared%20to%20transformers%20on%20reasoning%0Aand%20memorization%20tasks.%20In%20this%20paper%2C%20we%20dive%20deeper%20into%20one%20of%20such%0Abenchmarks%3A%20associative%20recall%20%28AR%29%2C%20which%20has%20been%20shown%20to%20correlate%20well%0Awith%20language%20modeling%20performance%2C%20and%20inspect%20in%20detail%20the%20effects%20of%0Ascaling%20and%20optimization%20issues%20in%20recently%20proposed%20token%20mixing%20strategies.%0AWe%20first%20demonstrate%20that%2C%20unlike%20standard%20transformers%2C%20the%20choice%20of%20learning%0Arate%20plays%20a%20critical%20role%20in%20the%20performance%20of%20modern%20recurrent%20models%3A%20an%0Aissue%20that%20can%20severely%20affect%20reported%20performance%20in%20previous%20works%20and%0Asuggests%20further%20research%20is%20needed%20to%20stabilize%20training.%20Next%2C%20we%20show%20that%0Arecurrent%20and%20attention-based%20models%20exhibit%20contrasting%20benefits%20when%20scaling%0Ain%20width%20as%20opposed%20to%20depth%2C%20with%20attention%20being%20notably%20unable%20to%20solve%20AR%0Awhen%20limited%20to%20a%20single%20layer.%20We%20then%20further%20inspect%201-layer%20transformers%2C%0Arevealing%20that%20despite%20their%20poor%20performance%2C%20their%20training%20dynamics%0Asurprisingly%20resemble%20the%20formation%20of%20induction%20heads%2C%20a%20phenomenon%20previously%0Aobserved%20only%20in%20their%202-layer%20counterparts.%20Finally%2C%20through%20architectural%0Aablations%2C%20we%20study%20how%20components%20affects%20Transformer%20and%20Mamba%27s%20performance%0Aand%20optimization%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19029v1&entry.124074799=Read"},
{"title": "GReAT: leveraging geometric artery data to improve wall shear stress\n  assessment", "author": "Julian Suk and Jolanda J. Wentzel and Patryk Rygiel and Joost Daemen and Daniel Rueckert and Jelmer M. Wolterink", "abstract": "  Leveraging big data for patient care is promising in many medical fields such\nas cardiovascular health. For example, hemodynamic biomarkers like wall shear\nstress could be assessed from patient-specific medical images via machine\nlearning algorithms, bypassing the need for time-intensive computational fluid\nsimulation. However, it is extremely challenging to amass large-enough datasets\nto effectively train such models. We could address this data scarcity by means\nof self-supervised pre-training and foundations models given large datasets of\ngeometric artery models. In the context of coronary arteries, leveraging\nlearned representations to improve hemodynamic biomarker assessment has not yet\nbeen well studied. In this work, we address this gap by investigating whether a\nlarge dataset (8449 shapes) consisting of geometric models of 3D blood vessels\ncan benefit wall shear stress assessment in coronary artery models from a\nsmall-scale clinical trial (49 patients). We create a self-supervised target\nfor the 3D blood vessels by computing the heat kernel signature, a quantity\nobtained via Laplacian eigenvectors, which captures the very essence of the\nshapes. We show how geometric representations learned from this datasets can\nboost segmentation of coronary arteries into regions of low, mid and high\n(time-averaged) wall shear stress even when trained on limited data.\n", "link": "http://arxiv.org/abs/2508.19030v1", "date": "2025-08-26", "relevancy": 2.0378, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5133}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5115}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GReAT%3A%20leveraging%20geometric%20artery%20data%20to%20improve%20wall%20shear%20stress%0A%20%20assessment&body=Title%3A%20GReAT%3A%20leveraging%20geometric%20artery%20data%20to%20improve%20wall%20shear%20stress%0A%20%20assessment%0AAuthor%3A%20Julian%20Suk%20and%20Jolanda%20J.%20Wentzel%20and%20Patryk%20Rygiel%20and%20Joost%20Daemen%20and%20Daniel%20Rueckert%20and%20Jelmer%20M.%20Wolterink%0AAbstract%3A%20%20%20Leveraging%20big%20data%20for%20patient%20care%20is%20promising%20in%20many%20medical%20fields%20such%0Aas%20cardiovascular%20health.%20For%20example%2C%20hemodynamic%20biomarkers%20like%20wall%20shear%0Astress%20could%20be%20assessed%20from%20patient-specific%20medical%20images%20via%20machine%0Alearning%20algorithms%2C%20bypassing%20the%20need%20for%20time-intensive%20computational%20fluid%0Asimulation.%20However%2C%20it%20is%20extremely%20challenging%20to%20amass%20large-enough%20datasets%0Ato%20effectively%20train%20such%20models.%20We%20could%20address%20this%20data%20scarcity%20by%20means%0Aof%20self-supervised%20pre-training%20and%20foundations%20models%20given%20large%20datasets%20of%0Ageometric%20artery%20models.%20In%20the%20context%20of%20coronary%20arteries%2C%20leveraging%0Alearned%20representations%20to%20improve%20hemodynamic%20biomarker%20assessment%20has%20not%20yet%0Abeen%20well%20studied.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20investigating%20whether%20a%0Alarge%20dataset%20%288449%20shapes%29%20consisting%20of%20geometric%20models%20of%203D%20blood%20vessels%0Acan%20benefit%20wall%20shear%20stress%20assessment%20in%20coronary%20artery%20models%20from%20a%0Asmall-scale%20clinical%20trial%20%2849%20patients%29.%20We%20create%20a%20self-supervised%20target%0Afor%20the%203D%20blood%20vessels%20by%20computing%20the%20heat%20kernel%20signature%2C%20a%20quantity%0Aobtained%20via%20Laplacian%20eigenvectors%2C%20which%20captures%20the%20very%20essence%20of%20the%0Ashapes.%20We%20show%20how%20geometric%20representations%20learned%20from%20this%20datasets%20can%0Aboost%20segmentation%20of%20coronary%20arteries%20into%20regions%20of%20low%2C%20mid%20and%20high%0A%28time-averaged%29%20wall%20shear%20stress%20even%20when%20trained%20on%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGReAT%253A%2520leveraging%2520geometric%2520artery%2520data%2520to%2520improve%2520wall%2520shear%2520stress%250A%2520%2520assessment%26entry.906535625%3DJulian%2520Suk%2520and%2520Jolanda%2520J.%2520Wentzel%2520and%2520Patryk%2520Rygiel%2520and%2520Joost%2520Daemen%2520and%2520Daniel%2520Rueckert%2520and%2520Jelmer%2520M.%2520Wolterink%26entry.1292438233%3D%2520%2520Leveraging%2520big%2520data%2520for%2520patient%2520care%2520is%2520promising%2520in%2520many%2520medical%2520fields%2520such%250Aas%2520cardiovascular%2520health.%2520For%2520example%252C%2520hemodynamic%2520biomarkers%2520like%2520wall%2520shear%250Astress%2520could%2520be%2520assessed%2520from%2520patient-specific%2520medical%2520images%2520via%2520machine%250Alearning%2520algorithms%252C%2520bypassing%2520the%2520need%2520for%2520time-intensive%2520computational%2520fluid%250Asimulation.%2520However%252C%2520it%2520is%2520extremely%2520challenging%2520to%2520amass%2520large-enough%2520datasets%250Ato%2520effectively%2520train%2520such%2520models.%2520We%2520could%2520address%2520this%2520data%2520scarcity%2520by%2520means%250Aof%2520self-supervised%2520pre-training%2520and%2520foundations%2520models%2520given%2520large%2520datasets%2520of%250Ageometric%2520artery%2520models.%2520In%2520the%2520context%2520of%2520coronary%2520arteries%252C%2520leveraging%250Alearned%2520representations%2520to%2520improve%2520hemodynamic%2520biomarker%2520assessment%2520has%2520not%2520yet%250Abeen%2520well%2520studied.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520investigating%2520whether%2520a%250Alarge%2520dataset%2520%25288449%2520shapes%2529%2520consisting%2520of%2520geometric%2520models%2520of%25203D%2520blood%2520vessels%250Acan%2520benefit%2520wall%2520shear%2520stress%2520assessment%2520in%2520coronary%2520artery%2520models%2520from%2520a%250Asmall-scale%2520clinical%2520trial%2520%252849%2520patients%2529.%2520We%2520create%2520a%2520self-supervised%2520target%250Afor%2520the%25203D%2520blood%2520vessels%2520by%2520computing%2520the%2520heat%2520kernel%2520signature%252C%2520a%2520quantity%250Aobtained%2520via%2520Laplacian%2520eigenvectors%252C%2520which%2520captures%2520the%2520very%2520essence%2520of%2520the%250Ashapes.%2520We%2520show%2520how%2520geometric%2520representations%2520learned%2520from%2520this%2520datasets%2520can%250Aboost%2520segmentation%2520of%2520coronary%2520arteries%2520into%2520regions%2520of%2520low%252C%2520mid%2520and%2520high%250A%2528time-averaged%2529%2520wall%2520shear%2520stress%2520even%2520when%2520trained%2520on%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GReAT%3A%20leveraging%20geometric%20artery%20data%20to%20improve%20wall%20shear%20stress%0A%20%20assessment&entry.906535625=Julian%20Suk%20and%20Jolanda%20J.%20Wentzel%20and%20Patryk%20Rygiel%20and%20Joost%20Daemen%20and%20Daniel%20Rueckert%20and%20Jelmer%20M.%20Wolterink&entry.1292438233=%20%20Leveraging%20big%20data%20for%20patient%20care%20is%20promising%20in%20many%20medical%20fields%20such%0Aas%20cardiovascular%20health.%20For%20example%2C%20hemodynamic%20biomarkers%20like%20wall%20shear%0Astress%20could%20be%20assessed%20from%20patient-specific%20medical%20images%20via%20machine%0Alearning%20algorithms%2C%20bypassing%20the%20need%20for%20time-intensive%20computational%20fluid%0Asimulation.%20However%2C%20it%20is%20extremely%20challenging%20to%20amass%20large-enough%20datasets%0Ato%20effectively%20train%20such%20models.%20We%20could%20address%20this%20data%20scarcity%20by%20means%0Aof%20self-supervised%20pre-training%20and%20foundations%20models%20given%20large%20datasets%20of%0Ageometric%20artery%20models.%20In%20the%20context%20of%20coronary%20arteries%2C%20leveraging%0Alearned%20representations%20to%20improve%20hemodynamic%20biomarker%20assessment%20has%20not%20yet%0Abeen%20well%20studied.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20investigating%20whether%20a%0Alarge%20dataset%20%288449%20shapes%29%20consisting%20of%20geometric%20models%20of%203D%20blood%20vessels%0Acan%20benefit%20wall%20shear%20stress%20assessment%20in%20coronary%20artery%20models%20from%20a%0Asmall-scale%20clinical%20trial%20%2849%20patients%29.%20We%20create%20a%20self-supervised%20target%0Afor%20the%203D%20blood%20vessels%20by%20computing%20the%20heat%20kernel%20signature%2C%20a%20quantity%0Aobtained%20via%20Laplacian%20eigenvectors%2C%20which%20captures%20the%20very%20essence%20of%20the%0Ashapes.%20We%20show%20how%20geometric%20representations%20learned%20from%20this%20datasets%20can%0Aboost%20segmentation%20of%20coronary%20arteries%20into%20regions%20of%20low%2C%20mid%20and%20high%0A%28time-averaged%29%20wall%20shear%20stress%20even%20when%20trained%20on%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19030v1&entry.124074799=Read"},
{"title": "Hybrid Deep Searcher: Integrating Parallel and Sequential Search\n  Reasoning", "author": "Dayoon Ko and Jihyuk Kim and Haeju Park and Sohyeon Kim and Dahyun Lee and Yongrae Jo and Gunhee Kim and Moontae Lee and Kyungjae Lee", "abstract": "  Large reasoning models (LRMs) have demonstrated strong performance in\ncomplex, multi-step reasoning tasks. Existing methods enhance LRMs by\nsequentially integrating external knowledge retrieval; models iteratively\ngenerate queries, retrieve external information, and progressively reason over\nthis information. However, purely sequential querying increases inference\nlatency and context length, diminishing coherence and potentially reducing\naccuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search\nQA), a synthetic dataset automatically generated from Natural Questions,\nexplicitly designed to train LRMs to distinguish parallelizable from sequential\nqueries. HDS-QA comprises hybrid-hop questions that combine parallelizable\nindependent subqueries (executable simultaneously) and sequentially dependent\nsubqueries (requiring step-by-step resolution), along with synthetic\nreasoning-querying-retrieval paths involving parallel queries. We fine-tune an\nLRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms\nstate-of-the-art baselines across multiple benchmarks, notably achieving +15.9\nand +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both\nrequiring comprehensive and exhaustive search. Experimental results highlight\ntwo key advantages: HybridDeepSearcher reaches comparable accuracy with fewer\nsearch turns, significantly reducing inference latency, and it effectively\nscales as more turns are permitted. These results demonstrate the efficiency,\nscalability, and effectiveness of explicitly training LRMs to leverage hybrid\nparallel and sequential querying.\n", "link": "http://arxiv.org/abs/2508.19113v1", "date": "2025-08-26", "relevancy": 2.0353, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5189}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Deep%20Searcher%3A%20Integrating%20Parallel%20and%20Sequential%20Search%0A%20%20Reasoning&body=Title%3A%20Hybrid%20Deep%20Searcher%3A%20Integrating%20Parallel%20and%20Sequential%20Search%0A%20%20Reasoning%0AAuthor%3A%20Dayoon%20Ko%20and%20Jihyuk%20Kim%20and%20Haeju%20Park%20and%20Sohyeon%20Kim%20and%20Dahyun%20Lee%20and%20Yongrae%20Jo%20and%20Gunhee%20Kim%20and%20Moontae%20Lee%20and%20Kyungjae%20Lee%0AAbstract%3A%20%20%20Large%20reasoning%20models%20%28LRMs%29%20have%20demonstrated%20strong%20performance%20in%0Acomplex%2C%20multi-step%20reasoning%20tasks.%20Existing%20methods%20enhance%20LRMs%20by%0Asequentially%20integrating%20external%20knowledge%20retrieval%3B%20models%20iteratively%0Agenerate%20queries%2C%20retrieve%20external%20information%2C%20and%20progressively%20reason%20over%0Athis%20information.%20However%2C%20purely%20sequential%20querying%20increases%20inference%0Alatency%20and%20context%20length%2C%20diminishing%20coherence%20and%20potentially%20reducing%0Aaccuracy.%20To%20address%20these%20limitations%2C%20we%20introduce%20HDS-QA%20%28Hybrid%20Deep%20Search%0AQA%29%2C%20a%20synthetic%20dataset%20automatically%20generated%20from%20Natural%20Questions%2C%0Aexplicitly%20designed%20to%20train%20LRMs%20to%20distinguish%20parallelizable%20from%20sequential%0Aqueries.%20HDS-QA%20comprises%20hybrid-hop%20questions%20that%20combine%20parallelizable%0Aindependent%20subqueries%20%28executable%20simultaneously%29%20and%20sequentially%20dependent%0Asubqueries%20%28requiring%20step-by-step%20resolution%29%2C%20along%20with%20synthetic%0Areasoning-querying-retrieval%20paths%20involving%20parallel%20queries.%20We%20fine-tune%20an%0ALRM%20using%20HDS-QA%2C%20naming%20the%20model%20HybridDeepSearcher%2C%20which%20outperforms%0Astate-of-the-art%20baselines%20across%20multiple%20benchmarks%2C%20notably%20achieving%20%2B15.9%0Aand%20%2B11.5%20F1%20on%20FanOutQA%20and%20a%20subset%20of%20BrowseComp%2C%20respectively%2C%20both%0Arequiring%20comprehensive%20and%20exhaustive%20search.%20Experimental%20results%20highlight%0Atwo%20key%20advantages%3A%20HybridDeepSearcher%20reaches%20comparable%20accuracy%20with%20fewer%0Asearch%20turns%2C%20significantly%20reducing%20inference%20latency%2C%20and%20it%20effectively%0Ascales%20as%20more%20turns%20are%20permitted.%20These%20results%20demonstrate%20the%20efficiency%2C%0Ascalability%2C%20and%20effectiveness%20of%20explicitly%20training%20LRMs%20to%20leverage%20hybrid%0Aparallel%20and%20sequential%20querying.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Deep%2520Searcher%253A%2520Integrating%2520Parallel%2520and%2520Sequential%2520Search%250A%2520%2520Reasoning%26entry.906535625%3DDayoon%2520Ko%2520and%2520Jihyuk%2520Kim%2520and%2520Haeju%2520Park%2520and%2520Sohyeon%2520Kim%2520and%2520Dahyun%2520Lee%2520and%2520Yongrae%2520Jo%2520and%2520Gunhee%2520Kim%2520and%2520Moontae%2520Lee%2520and%2520Kyungjae%2520Lee%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520%2528LRMs%2529%2520have%2520demonstrated%2520strong%2520performance%2520in%250Acomplex%252C%2520multi-step%2520reasoning%2520tasks.%2520Existing%2520methods%2520enhance%2520LRMs%2520by%250Asequentially%2520integrating%2520external%2520knowledge%2520retrieval%253B%2520models%2520iteratively%250Agenerate%2520queries%252C%2520retrieve%2520external%2520information%252C%2520and%2520progressively%2520reason%2520over%250Athis%2520information.%2520However%252C%2520purely%2520sequential%2520querying%2520increases%2520inference%250Alatency%2520and%2520context%2520length%252C%2520diminishing%2520coherence%2520and%2520potentially%2520reducing%250Aaccuracy.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520HDS-QA%2520%2528Hybrid%2520Deep%2520Search%250AQA%2529%252C%2520a%2520synthetic%2520dataset%2520automatically%2520generated%2520from%2520Natural%2520Questions%252C%250Aexplicitly%2520designed%2520to%2520train%2520LRMs%2520to%2520distinguish%2520parallelizable%2520from%2520sequential%250Aqueries.%2520HDS-QA%2520comprises%2520hybrid-hop%2520questions%2520that%2520combine%2520parallelizable%250Aindependent%2520subqueries%2520%2528executable%2520simultaneously%2529%2520and%2520sequentially%2520dependent%250Asubqueries%2520%2528requiring%2520step-by-step%2520resolution%2529%252C%2520along%2520with%2520synthetic%250Areasoning-querying-retrieval%2520paths%2520involving%2520parallel%2520queries.%2520We%2520fine-tune%2520an%250ALRM%2520using%2520HDS-QA%252C%2520naming%2520the%2520model%2520HybridDeepSearcher%252C%2520which%2520outperforms%250Astate-of-the-art%2520baselines%2520across%2520multiple%2520benchmarks%252C%2520notably%2520achieving%2520%252B15.9%250Aand%2520%252B11.5%2520F1%2520on%2520FanOutQA%2520and%2520a%2520subset%2520of%2520BrowseComp%252C%2520respectively%252C%2520both%250Arequiring%2520comprehensive%2520and%2520exhaustive%2520search.%2520Experimental%2520results%2520highlight%250Atwo%2520key%2520advantages%253A%2520HybridDeepSearcher%2520reaches%2520comparable%2520accuracy%2520with%2520fewer%250Asearch%2520turns%252C%2520significantly%2520reducing%2520inference%2520latency%252C%2520and%2520it%2520effectively%250Ascales%2520as%2520more%2520turns%2520are%2520permitted.%2520These%2520results%2520demonstrate%2520the%2520efficiency%252C%250Ascalability%252C%2520and%2520effectiveness%2520of%2520explicitly%2520training%2520LRMs%2520to%2520leverage%2520hybrid%250Aparallel%2520and%2520sequential%2520querying.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Deep%20Searcher%3A%20Integrating%20Parallel%20and%20Sequential%20Search%0A%20%20Reasoning&entry.906535625=Dayoon%20Ko%20and%20Jihyuk%20Kim%20and%20Haeju%20Park%20and%20Sohyeon%20Kim%20and%20Dahyun%20Lee%20and%20Yongrae%20Jo%20and%20Gunhee%20Kim%20and%20Moontae%20Lee%20and%20Kyungjae%20Lee&entry.1292438233=%20%20Large%20reasoning%20models%20%28LRMs%29%20have%20demonstrated%20strong%20performance%20in%0Acomplex%2C%20multi-step%20reasoning%20tasks.%20Existing%20methods%20enhance%20LRMs%20by%0Asequentially%20integrating%20external%20knowledge%20retrieval%3B%20models%20iteratively%0Agenerate%20queries%2C%20retrieve%20external%20information%2C%20and%20progressively%20reason%20over%0Athis%20information.%20However%2C%20purely%20sequential%20querying%20increases%20inference%0Alatency%20and%20context%20length%2C%20diminishing%20coherence%20and%20potentially%20reducing%0Aaccuracy.%20To%20address%20these%20limitations%2C%20we%20introduce%20HDS-QA%20%28Hybrid%20Deep%20Search%0AQA%29%2C%20a%20synthetic%20dataset%20automatically%20generated%20from%20Natural%20Questions%2C%0Aexplicitly%20designed%20to%20train%20LRMs%20to%20distinguish%20parallelizable%20from%20sequential%0Aqueries.%20HDS-QA%20comprises%20hybrid-hop%20questions%20that%20combine%20parallelizable%0Aindependent%20subqueries%20%28executable%20simultaneously%29%20and%20sequentially%20dependent%0Asubqueries%20%28requiring%20step-by-step%20resolution%29%2C%20along%20with%20synthetic%0Areasoning-querying-retrieval%20paths%20involving%20parallel%20queries.%20We%20fine-tune%20an%0ALRM%20using%20HDS-QA%2C%20naming%20the%20model%20HybridDeepSearcher%2C%20which%20outperforms%0Astate-of-the-art%20baselines%20across%20multiple%20benchmarks%2C%20notably%20achieving%20%2B15.9%0Aand%20%2B11.5%20F1%20on%20FanOutQA%20and%20a%20subset%20of%20BrowseComp%2C%20respectively%2C%20both%0Arequiring%20comprehensive%20and%20exhaustive%20search.%20Experimental%20results%20highlight%0Atwo%20key%20advantages%3A%20HybridDeepSearcher%20reaches%20comparable%20accuracy%20with%20fewer%0Asearch%20turns%2C%20significantly%20reducing%20inference%20latency%2C%20and%20it%20effectively%0Ascales%20as%20more%20turns%20are%20permitted.%20These%20results%20demonstrate%20the%20efficiency%2C%0Ascalability%2C%20and%20effectiveness%20of%20explicitly%20training%20LRMs%20to%20leverage%20hybrid%0Aparallel%20and%20sequential%20querying.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19113v1&entry.124074799=Read"},
{"title": "CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers", "author": "Dimitrios Mallis and Ahmet Serdar Karadeniz and Sebastian Cavada and Danila Rukhovich and Niki Foteinopoulou and Kseniya Cherenkova and Anis Kacem and Djamila Aouada", "abstract": "  We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design.\nOur approach is based on a powerful Vision and Large Language Model (VLLM) as a\nplanner and a tool-augmentation paradigm using CAD-specific tools.\nCAD-Assistant addresses multimodal user queries by generating actions that are\niteratively executed on a Python interpreter equipped with the FreeCAD\nsoftware, accessed via its Python API. Our framework is able to assess the\nimpact of generated CAD commands on geometry and adapts subsequent actions\nbased on the evolving state of the CAD design. We consider a wide range of\nCAD-specific tools including a sketch image parameterizer, rendering modules, a\n2D cross-section generator, and other specialized routines. CAD-Assistant is\nevaluated on multiple CAD benchmarks, where it outperforms VLLM baselines and\nsupervised task-specific methods. Beyond existing benchmarks, we qualitatively\ndemonstrate the potential of tool-augmented VLLMs as general-purpose CAD\nsolvers across diverse workflows.\n", "link": "http://arxiv.org/abs/2412.13810v3", "date": "2025-08-26", "relevancy": 2.0183, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5159}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-Assistant%3A%20Tool-Augmented%20VLLMs%20as%20Generic%20CAD%20Task%20Solvers&body=Title%3A%20CAD-Assistant%3A%20Tool-Augmented%20VLLMs%20as%20Generic%20CAD%20Task%20Solvers%0AAuthor%3A%20Dimitrios%20Mallis%20and%20Ahmet%20Serdar%20Karadeniz%20and%20Sebastian%20Cavada%20and%20Danila%20Rukhovich%20and%20Niki%20Foteinopoulou%20and%20Kseniya%20Cherenkova%20and%20Anis%20Kacem%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20We%20propose%20CAD-Assistant%2C%20a%20general-purpose%20CAD%20agent%20for%20AI-assisted%20design.%0AOur%20approach%20is%20based%20on%20a%20powerful%20Vision%20and%20Large%20Language%20Model%20%28VLLM%29%20as%20a%0Aplanner%20and%20a%20tool-augmentation%20paradigm%20using%20CAD-specific%20tools.%0ACAD-Assistant%20addresses%20multimodal%20user%20queries%20by%20generating%20actions%20that%20are%0Aiteratively%20executed%20on%20a%20Python%20interpreter%20equipped%20with%20the%20FreeCAD%0Asoftware%2C%20accessed%20via%20its%20Python%20API.%20Our%20framework%20is%20able%20to%20assess%20the%0Aimpact%20of%20generated%20CAD%20commands%20on%20geometry%20and%20adapts%20subsequent%20actions%0Abased%20on%20the%20evolving%20state%20of%20the%20CAD%20design.%20We%20consider%20a%20wide%20range%20of%0ACAD-specific%20tools%20including%20a%20sketch%20image%20parameterizer%2C%20rendering%20modules%2C%20a%0A2D%20cross-section%20generator%2C%20and%20other%20specialized%20routines.%20CAD-Assistant%20is%0Aevaluated%20on%20multiple%20CAD%20benchmarks%2C%20where%20it%20outperforms%20VLLM%20baselines%20and%0Asupervised%20task-specific%20methods.%20Beyond%20existing%20benchmarks%2C%20we%20qualitatively%0Ademonstrate%20the%20potential%20of%20tool-augmented%20VLLMs%20as%20general-purpose%20CAD%0Asolvers%20across%20diverse%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13810v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-Assistant%253A%2520Tool-Augmented%2520VLLMs%2520as%2520Generic%2520CAD%2520Task%2520Solvers%26entry.906535625%3DDimitrios%2520Mallis%2520and%2520Ahmet%2520Serdar%2520Karadeniz%2520and%2520Sebastian%2520Cavada%2520and%2520Danila%2520Rukhovich%2520and%2520Niki%2520Foteinopoulou%2520and%2520Kseniya%2520Cherenkova%2520and%2520Anis%2520Kacem%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520We%2520propose%2520CAD-Assistant%252C%2520a%2520general-purpose%2520CAD%2520agent%2520for%2520AI-assisted%2520design.%250AOur%2520approach%2520is%2520based%2520on%2520a%2520powerful%2520Vision%2520and%2520Large%2520Language%2520Model%2520%2528VLLM%2529%2520as%2520a%250Aplanner%2520and%2520a%2520tool-augmentation%2520paradigm%2520using%2520CAD-specific%2520tools.%250ACAD-Assistant%2520addresses%2520multimodal%2520user%2520queries%2520by%2520generating%2520actions%2520that%2520are%250Aiteratively%2520executed%2520on%2520a%2520Python%2520interpreter%2520equipped%2520with%2520the%2520FreeCAD%250Asoftware%252C%2520accessed%2520via%2520its%2520Python%2520API.%2520Our%2520framework%2520is%2520able%2520to%2520assess%2520the%250Aimpact%2520of%2520generated%2520CAD%2520commands%2520on%2520geometry%2520and%2520adapts%2520subsequent%2520actions%250Abased%2520on%2520the%2520evolving%2520state%2520of%2520the%2520CAD%2520design.%2520We%2520consider%2520a%2520wide%2520range%2520of%250ACAD-specific%2520tools%2520including%2520a%2520sketch%2520image%2520parameterizer%252C%2520rendering%2520modules%252C%2520a%250A2D%2520cross-section%2520generator%252C%2520and%2520other%2520specialized%2520routines.%2520CAD-Assistant%2520is%250Aevaluated%2520on%2520multiple%2520CAD%2520benchmarks%252C%2520where%2520it%2520outperforms%2520VLLM%2520baselines%2520and%250Asupervised%2520task-specific%2520methods.%2520Beyond%2520existing%2520benchmarks%252C%2520we%2520qualitatively%250Ademonstrate%2520the%2520potential%2520of%2520tool-augmented%2520VLLMs%2520as%2520general-purpose%2520CAD%250Asolvers%2520across%2520diverse%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13810v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-Assistant%3A%20Tool-Augmented%20VLLMs%20as%20Generic%20CAD%20Task%20Solvers&entry.906535625=Dimitrios%20Mallis%20and%20Ahmet%20Serdar%20Karadeniz%20and%20Sebastian%20Cavada%20and%20Danila%20Rukhovich%20and%20Niki%20Foteinopoulou%20and%20Kseniya%20Cherenkova%20and%20Anis%20Kacem%20and%20Djamila%20Aouada&entry.1292438233=%20%20We%20propose%20CAD-Assistant%2C%20a%20general-purpose%20CAD%20agent%20for%20AI-assisted%20design.%0AOur%20approach%20is%20based%20on%20a%20powerful%20Vision%20and%20Large%20Language%20Model%20%28VLLM%29%20as%20a%0Aplanner%20and%20a%20tool-augmentation%20paradigm%20using%20CAD-specific%20tools.%0ACAD-Assistant%20addresses%20multimodal%20user%20queries%20by%20generating%20actions%20that%20are%0Aiteratively%20executed%20on%20a%20Python%20interpreter%20equipped%20with%20the%20FreeCAD%0Asoftware%2C%20accessed%20via%20its%20Python%20API.%20Our%20framework%20is%20able%20to%20assess%20the%0Aimpact%20of%20generated%20CAD%20commands%20on%20geometry%20and%20adapts%20subsequent%20actions%0Abased%20on%20the%20evolving%20state%20of%20the%20CAD%20design.%20We%20consider%20a%20wide%20range%20of%0ACAD-specific%20tools%20including%20a%20sketch%20image%20parameterizer%2C%20rendering%20modules%2C%20a%0A2D%20cross-section%20generator%2C%20and%20other%20specialized%20routines.%20CAD-Assistant%20is%0Aevaluated%20on%20multiple%20CAD%20benchmarks%2C%20where%20it%20outperforms%20VLLM%20baselines%20and%0Asupervised%20task-specific%20methods.%20Beyond%20existing%20benchmarks%2C%20we%20qualitatively%0Ademonstrate%20the%20potential%20of%20tool-augmented%20VLLMs%20as%20general-purpose%20CAD%0Asolvers%20across%20diverse%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13810v3&entry.124074799=Read"},
{"title": "SecureV2X: An Efficient and Privacy-Preserving System for\n  Vehicle-to-Everything (V2X) Applications", "author": "Joshua Lee and Ali Arastehfard and Weiran Liu and Xuegang Ban and Yuan Hong", "abstract": "  Autonomous driving and V2X technologies have developed rapidly in the past\ndecade, leading to improved safety and efficiency in modern transportation.\nThese systems interact with extensive networks of vehicles, roadside\ninfrastructure, and cloud resources to support their machine learning\ncapabilities. However, the widespread use of machine learning in V2X systems\nraises issues over the privacy of the data involved. This is particularly\nconcerning for smart-transit and driver safety applications which can\nimplicitly reveal user locations or explicitly disclose medical data such as\nEEG signals. To resolve these issues, we propose SecureV2X, a scalable,\nmulti-agent system for secure neural network inferences deployed between the\nserver and each vehicle. Under this setting, we study two multi-agent V2X\napplications: secure drowsiness detection, and secure red-light violation\ndetection. Our system achieves strong performance relative to baselines, and\nscales efficiently to support a large number of secure computation interactions\nsimultaneously. For instance, SecureV2X is $9.4 \\times$ faster, requires\n$143\\times$ fewer computational rounds, and involves $16.6\\times$ less\ncommunication on drowsiness detection compared to other secure systems.\nMoreover, it achieves a runtime nearly $100\\times$ faster than state-of-the-art\nbenchmarks in object detection tasks for red light violation detection.\n", "link": "http://arxiv.org/abs/2508.19115v1", "date": "2025-08-26", "relevancy": 2.0176, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5179}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5116}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SecureV2X%3A%20An%20Efficient%20and%20Privacy-Preserving%20System%20for%0A%20%20Vehicle-to-Everything%20%28V2X%29%20Applications&body=Title%3A%20SecureV2X%3A%20An%20Efficient%20and%20Privacy-Preserving%20System%20for%0A%20%20Vehicle-to-Everything%20%28V2X%29%20Applications%0AAuthor%3A%20Joshua%20Lee%20and%20Ali%20Arastehfard%20and%20Weiran%20Liu%20and%20Xuegang%20Ban%20and%20Yuan%20Hong%0AAbstract%3A%20%20%20Autonomous%20driving%20and%20V2X%20technologies%20have%20developed%20rapidly%20in%20the%20past%0Adecade%2C%20leading%20to%20improved%20safety%20and%20efficiency%20in%20modern%20transportation.%0AThese%20systems%20interact%20with%20extensive%20networks%20of%20vehicles%2C%20roadside%0Ainfrastructure%2C%20and%20cloud%20resources%20to%20support%20their%20machine%20learning%0Acapabilities.%20However%2C%20the%20widespread%20use%20of%20machine%20learning%20in%20V2X%20systems%0Araises%20issues%20over%20the%20privacy%20of%20the%20data%20involved.%20This%20is%20particularly%0Aconcerning%20for%20smart-transit%20and%20driver%20safety%20applications%20which%20can%0Aimplicitly%20reveal%20user%20locations%20or%20explicitly%20disclose%20medical%20data%20such%20as%0AEEG%20signals.%20To%20resolve%20these%20issues%2C%20we%20propose%20SecureV2X%2C%20a%20scalable%2C%0Amulti-agent%20system%20for%20secure%20neural%20network%20inferences%20deployed%20between%20the%0Aserver%20and%20each%20vehicle.%20Under%20this%20setting%2C%20we%20study%20two%20multi-agent%20V2X%0Aapplications%3A%20secure%20drowsiness%20detection%2C%20and%20secure%20red-light%20violation%0Adetection.%20Our%20system%20achieves%20strong%20performance%20relative%20to%20baselines%2C%20and%0Ascales%20efficiently%20to%20support%20a%20large%20number%20of%20secure%20computation%20interactions%0Asimultaneously.%20For%20instance%2C%20SecureV2X%20is%20%249.4%20%5Ctimes%24%20faster%2C%20requires%0A%24143%5Ctimes%24%20fewer%20computational%20rounds%2C%20and%20involves%20%2416.6%5Ctimes%24%20less%0Acommunication%20on%20drowsiness%20detection%20compared%20to%20other%20secure%20systems.%0AMoreover%2C%20it%20achieves%20a%20runtime%20nearly%20%24100%5Ctimes%24%20faster%20than%20state-of-the-art%0Abenchmarks%20in%20object%20detection%20tasks%20for%20red%20light%20violation%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecureV2X%253A%2520An%2520Efficient%2520and%2520Privacy-Preserving%2520System%2520for%250A%2520%2520Vehicle-to-Everything%2520%2528V2X%2529%2520Applications%26entry.906535625%3DJoshua%2520Lee%2520and%2520Ali%2520Arastehfard%2520and%2520Weiran%2520Liu%2520and%2520Xuegang%2520Ban%2520and%2520Yuan%2520Hong%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520and%2520V2X%2520technologies%2520have%2520developed%2520rapidly%2520in%2520the%2520past%250Adecade%252C%2520leading%2520to%2520improved%2520safety%2520and%2520efficiency%2520in%2520modern%2520transportation.%250AThese%2520systems%2520interact%2520with%2520extensive%2520networks%2520of%2520vehicles%252C%2520roadside%250Ainfrastructure%252C%2520and%2520cloud%2520resources%2520to%2520support%2520their%2520machine%2520learning%250Acapabilities.%2520However%252C%2520the%2520widespread%2520use%2520of%2520machine%2520learning%2520in%2520V2X%2520systems%250Araises%2520issues%2520over%2520the%2520privacy%2520of%2520the%2520data%2520involved.%2520This%2520is%2520particularly%250Aconcerning%2520for%2520smart-transit%2520and%2520driver%2520safety%2520applications%2520which%2520can%250Aimplicitly%2520reveal%2520user%2520locations%2520or%2520explicitly%2520disclose%2520medical%2520data%2520such%2520as%250AEEG%2520signals.%2520To%2520resolve%2520these%2520issues%252C%2520we%2520propose%2520SecureV2X%252C%2520a%2520scalable%252C%250Amulti-agent%2520system%2520for%2520secure%2520neural%2520network%2520inferences%2520deployed%2520between%2520the%250Aserver%2520and%2520each%2520vehicle.%2520Under%2520this%2520setting%252C%2520we%2520study%2520two%2520multi-agent%2520V2X%250Aapplications%253A%2520secure%2520drowsiness%2520detection%252C%2520and%2520secure%2520red-light%2520violation%250Adetection.%2520Our%2520system%2520achieves%2520strong%2520performance%2520relative%2520to%2520baselines%252C%2520and%250Ascales%2520efficiently%2520to%2520support%2520a%2520large%2520number%2520of%2520secure%2520computation%2520interactions%250Asimultaneously.%2520For%2520instance%252C%2520SecureV2X%2520is%2520%25249.4%2520%255Ctimes%2524%2520faster%252C%2520requires%250A%2524143%255Ctimes%2524%2520fewer%2520computational%2520rounds%252C%2520and%2520involves%2520%252416.6%255Ctimes%2524%2520less%250Acommunication%2520on%2520drowsiness%2520detection%2520compared%2520to%2520other%2520secure%2520systems.%250AMoreover%252C%2520it%2520achieves%2520a%2520runtime%2520nearly%2520%2524100%255Ctimes%2524%2520faster%2520than%2520state-of-the-art%250Abenchmarks%2520in%2520object%2520detection%2520tasks%2520for%2520red%2520light%2520violation%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SecureV2X%3A%20An%20Efficient%20and%20Privacy-Preserving%20System%20for%0A%20%20Vehicle-to-Everything%20%28V2X%29%20Applications&entry.906535625=Joshua%20Lee%20and%20Ali%20Arastehfard%20and%20Weiran%20Liu%20and%20Xuegang%20Ban%20and%20Yuan%20Hong&entry.1292438233=%20%20Autonomous%20driving%20and%20V2X%20technologies%20have%20developed%20rapidly%20in%20the%20past%0Adecade%2C%20leading%20to%20improved%20safety%20and%20efficiency%20in%20modern%20transportation.%0AThese%20systems%20interact%20with%20extensive%20networks%20of%20vehicles%2C%20roadside%0Ainfrastructure%2C%20and%20cloud%20resources%20to%20support%20their%20machine%20learning%0Acapabilities.%20However%2C%20the%20widespread%20use%20of%20machine%20learning%20in%20V2X%20systems%0Araises%20issues%20over%20the%20privacy%20of%20the%20data%20involved.%20This%20is%20particularly%0Aconcerning%20for%20smart-transit%20and%20driver%20safety%20applications%20which%20can%0Aimplicitly%20reveal%20user%20locations%20or%20explicitly%20disclose%20medical%20data%20such%20as%0AEEG%20signals.%20To%20resolve%20these%20issues%2C%20we%20propose%20SecureV2X%2C%20a%20scalable%2C%0Amulti-agent%20system%20for%20secure%20neural%20network%20inferences%20deployed%20between%20the%0Aserver%20and%20each%20vehicle.%20Under%20this%20setting%2C%20we%20study%20two%20multi-agent%20V2X%0Aapplications%3A%20secure%20drowsiness%20detection%2C%20and%20secure%20red-light%20violation%0Adetection.%20Our%20system%20achieves%20strong%20performance%20relative%20to%20baselines%2C%20and%0Ascales%20efficiently%20to%20support%20a%20large%20number%20of%20secure%20computation%20interactions%0Asimultaneously.%20For%20instance%2C%20SecureV2X%20is%20%249.4%20%5Ctimes%24%20faster%2C%20requires%0A%24143%5Ctimes%24%20fewer%20computational%20rounds%2C%20and%20involves%20%2416.6%5Ctimes%24%20less%0Acommunication%20on%20drowsiness%20detection%20compared%20to%20other%20secure%20systems.%0AMoreover%2C%20it%20achieves%20a%20runtime%20nearly%20%24100%5Ctimes%24%20faster%20than%20state-of-the-art%0Abenchmarks%20in%20object%20detection%20tasks%20for%20red%20light%20violation%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19115v1&entry.124074799=Read"},
{"title": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and\n  Improved GRU", "author": "Peng Zhu and Yuante Li and Yifan Hu and Sheng Xiang and Qinyuan Liu and Dawei Cheng and Yuqi Liang", "abstract": "  As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality.\n", "link": "http://arxiv.org/abs/2410.20679v3", "date": "2025-08-26", "relevancy": 2.0161, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5226}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCI-GRU%3A%20Stock%20Prediction%20Model%20Based%20on%20Multi-Head%20Cross-Attention%20and%0A%20%20Improved%20GRU&body=Title%3A%20MCI-GRU%3A%20Stock%20Prediction%20Model%20Based%20on%20Multi-Head%20Cross-Attention%20and%0A%20%20Improved%20GRU%0AAuthor%3A%20Peng%20Zhu%20and%20Yuante%20Li%20and%20Yifan%20Hu%20and%20Sheng%20Xiang%20and%20Qinyuan%20Liu%20and%20Dawei%20Cheng%20and%20Yuqi%20Liang%0AAbstract%3A%20%20%20As%20financial%20markets%20grow%20increasingly%20complex%20in%20the%20big%20data%20era%2C%20accurate%0Astock%20prediction%20has%20become%20more%20critical.%20Traditional%20time%20series%20models%2C%20such%0Aas%20GRUs%2C%20have%20been%20widely%20used%20but%20often%20struggle%20to%20capture%20the%20intricate%0Anonlinear%20dynamics%20of%20markets%2C%20particularly%20in%20the%20flexible%20selection%20and%0Aeffective%20utilization%20of%20key%20historical%20information.%20Recently%2C%20methods%20like%0AGraph%20Neural%20Networks%20and%20Reinforcement%20Learning%20have%20shown%20promise%20in%20stock%0Aprediction%20but%20require%20high%20data%20quality%20and%20quantity%2C%20and%20they%20tend%20to%20exhibit%0Ainstability%20when%20dealing%20with%20data%20sparsity%20and%20noise.%20Moreover%2C%20the%20training%0Aand%20inference%20processes%20for%20these%20models%20are%20typically%20complex%20and%0Acomputationally%20expensive%2C%20limiting%20their%20broad%20deployment%20in%20practical%0Aapplications.%20Existing%20approaches%20also%20generally%20struggle%20to%20capture%0Aunobservable%20latent%20market%20states%20effectively%2C%20such%20as%20market%20sentiment%20and%0Aexpectations%2C%20microstructural%20factors%2C%20and%20participant%20behavior%20patterns%2C%0Aleading%20to%20an%20inadequate%20understanding%20of%20market%20dynamics%20and%20subsequently%0Aimpact%20prediction%20accuracy.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%0Astock%20prediction%20model%2C%20MCI-GRU%2C%20based%20on%20a%20multi-head%20cross-attention%0Amechanism%20and%20an%20improved%20GRU.%20First%2C%20we%20enhance%20the%20GRU%20model%20by%20replacing%20the%0Areset%20gate%20with%20an%20attention%20mechanism%2C%20thereby%20increasing%20the%20model%27s%0Aflexibility%20in%20selecting%20and%20utilizing%20historical%20information.%20Second%2C%20we%0Adesign%20a%20multi-head%20cross-attention%20mechanism%20for%20learning%20unobservable%20latent%0Amarket%20state%20representations%2C%20which%20are%20further%20enriched%20through%20interactions%0Awith%20both%20temporal%20features%20and%20cross-sectional%20features.%20Finally%2C%20extensive%0Aexperiments%20on%20four%20main%20stock%20markets%20show%20that%20the%20proposed%20method%0Aoutperforms%20SOTA%20techniques%20across%20multiple%20metrics.%20Additionally%2C%20its%0Asuccessful%20application%20in%20real-world%20fund%20management%20operations%20confirms%20its%0Aeffectiveness%20and%20practicality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20679v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCI-GRU%253A%2520Stock%2520Prediction%2520Model%2520Based%2520on%2520Multi-Head%2520Cross-Attention%2520and%250A%2520%2520Improved%2520GRU%26entry.906535625%3DPeng%2520Zhu%2520and%2520Yuante%2520Li%2520and%2520Yifan%2520Hu%2520and%2520Sheng%2520Xiang%2520and%2520Qinyuan%2520Liu%2520and%2520Dawei%2520Cheng%2520and%2520Yuqi%2520Liang%26entry.1292438233%3D%2520%2520As%2520financial%2520markets%2520grow%2520increasingly%2520complex%2520in%2520the%2520big%2520data%2520era%252C%2520accurate%250Astock%2520prediction%2520has%2520become%2520more%2520critical.%2520Traditional%2520time%2520series%2520models%252C%2520such%250Aas%2520GRUs%252C%2520have%2520been%2520widely%2520used%2520but%2520often%2520struggle%2520to%2520capture%2520the%2520intricate%250Anonlinear%2520dynamics%2520of%2520markets%252C%2520particularly%2520in%2520the%2520flexible%2520selection%2520and%250Aeffective%2520utilization%2520of%2520key%2520historical%2520information.%2520Recently%252C%2520methods%2520like%250AGraph%2520Neural%2520Networks%2520and%2520Reinforcement%2520Learning%2520have%2520shown%2520promise%2520in%2520stock%250Aprediction%2520but%2520require%2520high%2520data%2520quality%2520and%2520quantity%252C%2520and%2520they%2520tend%2520to%2520exhibit%250Ainstability%2520when%2520dealing%2520with%2520data%2520sparsity%2520and%2520noise.%2520Moreover%252C%2520the%2520training%250Aand%2520inference%2520processes%2520for%2520these%2520models%2520are%2520typically%2520complex%2520and%250Acomputationally%2520expensive%252C%2520limiting%2520their%2520broad%2520deployment%2520in%2520practical%250Aapplications.%2520Existing%2520approaches%2520also%2520generally%2520struggle%2520to%2520capture%250Aunobservable%2520latent%2520market%2520states%2520effectively%252C%2520such%2520as%2520market%2520sentiment%2520and%250Aexpectations%252C%2520microstructural%2520factors%252C%2520and%2520participant%2520behavior%2520patterns%252C%250Aleading%2520to%2520an%2520inadequate%2520understanding%2520of%2520market%2520dynamics%2520and%2520subsequently%250Aimpact%2520prediction%2520accuracy.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%250Astock%2520prediction%2520model%252C%2520MCI-GRU%252C%2520based%2520on%2520a%2520multi-head%2520cross-attention%250Amechanism%2520and%2520an%2520improved%2520GRU.%2520First%252C%2520we%2520enhance%2520the%2520GRU%2520model%2520by%2520replacing%2520the%250Areset%2520gate%2520with%2520an%2520attention%2520mechanism%252C%2520thereby%2520increasing%2520the%2520model%2527s%250Aflexibility%2520in%2520selecting%2520and%2520utilizing%2520historical%2520information.%2520Second%252C%2520we%250Adesign%2520a%2520multi-head%2520cross-attention%2520mechanism%2520for%2520learning%2520unobservable%2520latent%250Amarket%2520state%2520representations%252C%2520which%2520are%2520further%2520enriched%2520through%2520interactions%250Awith%2520both%2520temporal%2520features%2520and%2520cross-sectional%2520features.%2520Finally%252C%2520extensive%250Aexperiments%2520on%2520four%2520main%2520stock%2520markets%2520show%2520that%2520the%2520proposed%2520method%250Aoutperforms%2520SOTA%2520techniques%2520across%2520multiple%2520metrics.%2520Additionally%252C%2520its%250Asuccessful%2520application%2520in%2520real-world%2520fund%2520management%2520operations%2520confirms%2520its%250Aeffectiveness%2520and%2520practicality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20679v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCI-GRU%3A%20Stock%20Prediction%20Model%20Based%20on%20Multi-Head%20Cross-Attention%20and%0A%20%20Improved%20GRU&entry.906535625=Peng%20Zhu%20and%20Yuante%20Li%20and%20Yifan%20Hu%20and%20Sheng%20Xiang%20and%20Qinyuan%20Liu%20and%20Dawei%20Cheng%20and%20Yuqi%20Liang&entry.1292438233=%20%20As%20financial%20markets%20grow%20increasingly%20complex%20in%20the%20big%20data%20era%2C%20accurate%0Astock%20prediction%20has%20become%20more%20critical.%20Traditional%20time%20series%20models%2C%20such%0Aas%20GRUs%2C%20have%20been%20widely%20used%20but%20often%20struggle%20to%20capture%20the%20intricate%0Anonlinear%20dynamics%20of%20markets%2C%20particularly%20in%20the%20flexible%20selection%20and%0Aeffective%20utilization%20of%20key%20historical%20information.%20Recently%2C%20methods%20like%0AGraph%20Neural%20Networks%20and%20Reinforcement%20Learning%20have%20shown%20promise%20in%20stock%0Aprediction%20but%20require%20high%20data%20quality%20and%20quantity%2C%20and%20they%20tend%20to%20exhibit%0Ainstability%20when%20dealing%20with%20data%20sparsity%20and%20noise.%20Moreover%2C%20the%20training%0Aand%20inference%20processes%20for%20these%20models%20are%20typically%20complex%20and%0Acomputationally%20expensive%2C%20limiting%20their%20broad%20deployment%20in%20practical%0Aapplications.%20Existing%20approaches%20also%20generally%20struggle%20to%20capture%0Aunobservable%20latent%20market%20states%20effectively%2C%20such%20as%20market%20sentiment%20and%0Aexpectations%2C%20microstructural%20factors%2C%20and%20participant%20behavior%20patterns%2C%0Aleading%20to%20an%20inadequate%20understanding%20of%20market%20dynamics%20and%20subsequently%0Aimpact%20prediction%20accuracy.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%0Astock%20prediction%20model%2C%20MCI-GRU%2C%20based%20on%20a%20multi-head%20cross-attention%0Amechanism%20and%20an%20improved%20GRU.%20First%2C%20we%20enhance%20the%20GRU%20model%20by%20replacing%20the%0Areset%20gate%20with%20an%20attention%20mechanism%2C%20thereby%20increasing%20the%20model%27s%0Aflexibility%20in%20selecting%20and%20utilizing%20historical%20information.%20Second%2C%20we%0Adesign%20a%20multi-head%20cross-attention%20mechanism%20for%20learning%20unobservable%20latent%0Amarket%20state%20representations%2C%20which%20are%20further%20enriched%20through%20interactions%0Awith%20both%20temporal%20features%20and%20cross-sectional%20features.%20Finally%2C%20extensive%0Aexperiments%20on%20four%20main%20stock%20markets%20show%20that%20the%20proposed%20method%0Aoutperforms%20SOTA%20techniques%20across%20multiple%20metrics.%20Additionally%2C%20its%0Asuccessful%20application%20in%20real-world%20fund%20management%20operations%20confirms%20its%0Aeffectiveness%20and%20practicality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20679v3&entry.124074799=Read"},
{"title": "From Intents to Conversations: Generating Intent-Driven Dialogues with\n  Contrastive Learning for Multi-Turn Classification", "author": "Junhua Liu and Yong Keat Tan and Bin Fu and Kwan Hui Lim", "abstract": "  In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We further propose MINT-CL, a multi-task contrastive learning\nframework for multi-turn intent classification, which improves performance\nwhile reducing dependence on large-scale annotated datasets. Empirical results\ndemonstrate that our approach outperforms competitive baselines in both\ndialogue generation quality and classification accuracy, particularly in\nmultilingual settings. To facilitate future research, we release MINT-E, a\ncomprehensive, multilingual, intent-aware multi-turn dialogue corpus derived\nfrom the e-commerce domain. The reproduced source code and dataset are\navailable at https://github.com/junhua/chain-of-intent.\n", "link": "http://arxiv.org/abs/2411.14252v2", "date": "2025-08-26", "relevancy": 2.0125, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Intents%20to%20Conversations%3A%20Generating%20Intent-Driven%20Dialogues%20with%0A%20%20Contrastive%20Learning%20for%20Multi-Turn%20Classification&body=Title%3A%20From%20Intents%20to%20Conversations%3A%20Generating%20Intent-Driven%20Dialogues%20with%0A%20%20Contrastive%20Learning%20for%20Multi-Turn%20Classification%0AAuthor%3A%20Junhua%20Liu%20and%20Yong%20Keat%20Tan%20and%20Bin%20Fu%20and%20Kwan%20Hui%20Lim%0AAbstract%3A%20%20%20In%20conversational%20AI%20systems%2C%20a%20critical%20challenge%20in%20training%20effective%0Amulti-turn%20intent%20classification%20models%20lies%20in%20the%20generation%20of%20large-scale%2C%0Adomain-specific%2C%20multilingual%20dialogue%20datasets.%20In%20this%20paper%2C%20we%20introduce%0AChain-of-Intent%2C%20a%20novel%20framework%20that%20integrates%20Hidden%20Markov%20Models%20%28HMMs%29%0Awith%20Large%20Language%20Models%20%28LLMs%29%20to%20generate%20intent-driven%2C%20context-aware%0Adialogues%20through%20self-play.%20Our%20method%20first%20extracts%20domain-specific%20intent%0Atransition%20patterns%20from%20real-world%20e-commerce%20chat%20logs%2C%20which%20guide%20the%0Amodeling%20of%20turn-level%20dynamics%20and%20intent%20sequences.%20LLMs%20are%20then%20employed%20to%0Aparameterize%20the%20emission%20probabilities%20of%20HMMs%2C%20enabling%20the%20generation%20of%0Anatural%2C%20coherent%20utterances%20aligned%20with%20predicted%20intents%20and%20dialogue%0Acontext.%20We%20further%20propose%20MINT-CL%2C%20a%20multi-task%20contrastive%20learning%0Aframework%20for%20multi-turn%20intent%20classification%2C%20which%20improves%20performance%0Awhile%20reducing%20dependence%20on%20large-scale%20annotated%20datasets.%20Empirical%20results%0Ademonstrate%20that%20our%20approach%20outperforms%20competitive%20baselines%20in%20both%0Adialogue%20generation%20quality%20and%20classification%20accuracy%2C%20particularly%20in%0Amultilingual%20settings.%20To%20facilitate%20future%20research%2C%20we%20release%20MINT-E%2C%20a%0Acomprehensive%2C%20multilingual%2C%20intent-aware%20multi-turn%20dialogue%20corpus%20derived%0Afrom%20the%20e-commerce%20domain.%20The%20reproduced%20source%20code%20and%20dataset%20are%0Aavailable%20at%20https%3A//github.com/junhua/chain-of-intent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Intents%2520to%2520Conversations%253A%2520Generating%2520Intent-Driven%2520Dialogues%2520with%250A%2520%2520Contrastive%2520Learning%2520for%2520Multi-Turn%2520Classification%26entry.906535625%3DJunhua%2520Liu%2520and%2520Yong%2520Keat%2520Tan%2520and%2520Bin%2520Fu%2520and%2520Kwan%2520Hui%2520Lim%26entry.1292438233%3D%2520%2520In%2520conversational%2520AI%2520systems%252C%2520a%2520critical%2520challenge%2520in%2520training%2520effective%250Amulti-turn%2520intent%2520classification%2520models%2520lies%2520in%2520the%2520generation%2520of%2520large-scale%252C%250Adomain-specific%252C%2520multilingual%2520dialogue%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AChain-of-Intent%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520Hidden%2520Markov%2520Models%2520%2528HMMs%2529%250Awith%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520generate%2520intent-driven%252C%2520context-aware%250Adialogues%2520through%2520self-play.%2520Our%2520method%2520first%2520extracts%2520domain-specific%2520intent%250Atransition%2520patterns%2520from%2520real-world%2520e-commerce%2520chat%2520logs%252C%2520which%2520guide%2520the%250Amodeling%2520of%2520turn-level%2520dynamics%2520and%2520intent%2520sequences.%2520LLMs%2520are%2520then%2520employed%2520to%250Aparameterize%2520the%2520emission%2520probabilities%2520of%2520HMMs%252C%2520enabling%2520the%2520generation%2520of%250Anatural%252C%2520coherent%2520utterances%2520aligned%2520with%2520predicted%2520intents%2520and%2520dialogue%250Acontext.%2520We%2520further%2520propose%2520MINT-CL%252C%2520a%2520multi-task%2520contrastive%2520learning%250Aframework%2520for%2520multi-turn%2520intent%2520classification%252C%2520which%2520improves%2520performance%250Awhile%2520reducing%2520dependence%2520on%2520large-scale%2520annotated%2520datasets.%2520Empirical%2520results%250Ademonstrate%2520that%2520our%2520approach%2520outperforms%2520competitive%2520baselines%2520in%2520both%250Adialogue%2520generation%2520quality%2520and%2520classification%2520accuracy%252C%2520particularly%2520in%250Amultilingual%2520settings.%2520To%2520facilitate%2520future%2520research%252C%2520we%2520release%2520MINT-E%252C%2520a%250Acomprehensive%252C%2520multilingual%252C%2520intent-aware%2520multi-turn%2520dialogue%2520corpus%2520derived%250Afrom%2520the%2520e-commerce%2520domain.%2520The%2520reproduced%2520source%2520code%2520and%2520dataset%2520are%250Aavailable%2520at%2520https%253A//github.com/junhua/chain-of-intent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Intents%20to%20Conversations%3A%20Generating%20Intent-Driven%20Dialogues%20with%0A%20%20Contrastive%20Learning%20for%20Multi-Turn%20Classification&entry.906535625=Junhua%20Liu%20and%20Yong%20Keat%20Tan%20and%20Bin%20Fu%20and%20Kwan%20Hui%20Lim&entry.1292438233=%20%20In%20conversational%20AI%20systems%2C%20a%20critical%20challenge%20in%20training%20effective%0Amulti-turn%20intent%20classification%20models%20lies%20in%20the%20generation%20of%20large-scale%2C%0Adomain-specific%2C%20multilingual%20dialogue%20datasets.%20In%20this%20paper%2C%20we%20introduce%0AChain-of-Intent%2C%20a%20novel%20framework%20that%20integrates%20Hidden%20Markov%20Models%20%28HMMs%29%0Awith%20Large%20Language%20Models%20%28LLMs%29%20to%20generate%20intent-driven%2C%20context-aware%0Adialogues%20through%20self-play.%20Our%20method%20first%20extracts%20domain-specific%20intent%0Atransition%20patterns%20from%20real-world%20e-commerce%20chat%20logs%2C%20which%20guide%20the%0Amodeling%20of%20turn-level%20dynamics%20and%20intent%20sequences.%20LLMs%20are%20then%20employed%20to%0Aparameterize%20the%20emission%20probabilities%20of%20HMMs%2C%20enabling%20the%20generation%20of%0Anatural%2C%20coherent%20utterances%20aligned%20with%20predicted%20intents%20and%20dialogue%0Acontext.%20We%20further%20propose%20MINT-CL%2C%20a%20multi-task%20contrastive%20learning%0Aframework%20for%20multi-turn%20intent%20classification%2C%20which%20improves%20performance%0Awhile%20reducing%20dependence%20on%20large-scale%20annotated%20datasets.%20Empirical%20results%0Ademonstrate%20that%20our%20approach%20outperforms%20competitive%20baselines%20in%20both%0Adialogue%20generation%20quality%20and%20classification%20accuracy%2C%20particularly%20in%0Amultilingual%20settings.%20To%20facilitate%20future%20research%2C%20we%20release%20MINT-E%2C%20a%0Acomprehensive%2C%20multilingual%2C%20intent-aware%20multi-turn%20dialogue%20corpus%20derived%0Afrom%20the%20e-commerce%20domain.%20The%20reproduced%20source%20code%20and%20dataset%20are%0Aavailable%20at%20https%3A//github.com/junhua/chain-of-intent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14252v2&entry.124074799=Read"},
{"title": "KNN and K-means in Gini Prametric Spaces", "author": "Cassandra Mussard and Arthur Charpentier and St\u00e9phane Mussard", "abstract": "  This paper introduces enhancements to the K-means and K-nearest neighbors\n(KNN) algorithms based on the concept of Gini prametric spaces, instead of\ntraditional metric spaces. Unlike standard distance metrics, Gini prametrics\nincorporate both value-based and rank-based measures, offering robustness to\nnoise and outliers. The main contributions include: (1) a Gini prametric that\ncaptures rank information alongside value distances; (2) a Gini K-means\nalgorithm that is provably convergent and resilient to noisy data; and (3) a\nGini KNN method that performs competitively with state-of-the-art approaches\nlike Hassanat's distance in noisy environments. Experimental evaluations on 16\nUCI datasets demonstrate the superior performance and efficiency of the\nGini-based algorithms in clustering and classification tasks. This work opens\nnew directions for rank-based prametrics in machine learning and statistical\nanalysis.\n", "link": "http://arxiv.org/abs/2501.18028v3", "date": "2025-08-26", "relevancy": 1.6493, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4207}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KNN%20and%20K-means%20in%20Gini%20Prametric%20Spaces&body=Title%3A%20KNN%20and%20K-means%20in%20Gini%20Prametric%20Spaces%0AAuthor%3A%20Cassandra%20Mussard%20and%20Arthur%20Charpentier%20and%20St%C3%A9phane%20Mussard%0AAbstract%3A%20%20%20This%20paper%20introduces%20enhancements%20to%20the%20K-means%20and%20K-nearest%20neighbors%0A%28KNN%29%20algorithms%20based%20on%20the%20concept%20of%20Gini%20prametric%20spaces%2C%20instead%20of%0Atraditional%20metric%20spaces.%20Unlike%20standard%20distance%20metrics%2C%20Gini%20prametrics%0Aincorporate%20both%20value-based%20and%20rank-based%20measures%2C%20offering%20robustness%20to%0Anoise%20and%20outliers.%20The%20main%20contributions%20include%3A%20%281%29%20a%20Gini%20prametric%20that%0Acaptures%20rank%20information%20alongside%20value%20distances%3B%20%282%29%20a%20Gini%20K-means%0Aalgorithm%20that%20is%20provably%20convergent%20and%20resilient%20to%20noisy%20data%3B%20and%20%283%29%20a%0AGini%20KNN%20method%20that%20performs%20competitively%20with%20state-of-the-art%20approaches%0Alike%20Hassanat%27s%20distance%20in%20noisy%20environments.%20Experimental%20evaluations%20on%2016%0AUCI%20datasets%20demonstrate%20the%20superior%20performance%20and%20efficiency%20of%20the%0AGini-based%20algorithms%20in%20clustering%20and%20classification%20tasks.%20This%20work%20opens%0Anew%20directions%20for%20rank-based%20prametrics%20in%20machine%20learning%20and%20statistical%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18028v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKNN%2520and%2520K-means%2520in%2520Gini%2520Prametric%2520Spaces%26entry.906535625%3DCassandra%2520Mussard%2520and%2520Arthur%2520Charpentier%2520and%2520St%25C3%25A9phane%2520Mussard%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520enhancements%2520to%2520the%2520K-means%2520and%2520K-nearest%2520neighbors%250A%2528KNN%2529%2520algorithms%2520based%2520on%2520the%2520concept%2520of%2520Gini%2520prametric%2520spaces%252C%2520instead%2520of%250Atraditional%2520metric%2520spaces.%2520Unlike%2520standard%2520distance%2520metrics%252C%2520Gini%2520prametrics%250Aincorporate%2520both%2520value-based%2520and%2520rank-based%2520measures%252C%2520offering%2520robustness%2520to%250Anoise%2520and%2520outliers.%2520The%2520main%2520contributions%2520include%253A%2520%25281%2529%2520a%2520Gini%2520prametric%2520that%250Acaptures%2520rank%2520information%2520alongside%2520value%2520distances%253B%2520%25282%2529%2520a%2520Gini%2520K-means%250Aalgorithm%2520that%2520is%2520provably%2520convergent%2520and%2520resilient%2520to%2520noisy%2520data%253B%2520and%2520%25283%2529%2520a%250AGini%2520KNN%2520method%2520that%2520performs%2520competitively%2520with%2520state-of-the-art%2520approaches%250Alike%2520Hassanat%2527s%2520distance%2520in%2520noisy%2520environments.%2520Experimental%2520evaluations%2520on%252016%250AUCI%2520datasets%2520demonstrate%2520the%2520superior%2520performance%2520and%2520efficiency%2520of%2520the%250AGini-based%2520algorithms%2520in%2520clustering%2520and%2520classification%2520tasks.%2520This%2520work%2520opens%250Anew%2520directions%2520for%2520rank-based%2520prametrics%2520in%2520machine%2520learning%2520and%2520statistical%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18028v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KNN%20and%20K-means%20in%20Gini%20Prametric%20Spaces&entry.906535625=Cassandra%20Mussard%20and%20Arthur%20Charpentier%20and%20St%C3%A9phane%20Mussard&entry.1292438233=%20%20This%20paper%20introduces%20enhancements%20to%20the%20K-means%20and%20K-nearest%20neighbors%0A%28KNN%29%20algorithms%20based%20on%20the%20concept%20of%20Gini%20prametric%20spaces%2C%20instead%20of%0Atraditional%20metric%20spaces.%20Unlike%20standard%20distance%20metrics%2C%20Gini%20prametrics%0Aincorporate%20both%20value-based%20and%20rank-based%20measures%2C%20offering%20robustness%20to%0Anoise%20and%20outliers.%20The%20main%20contributions%20include%3A%20%281%29%20a%20Gini%20prametric%20that%0Acaptures%20rank%20information%20alongside%20value%20distances%3B%20%282%29%20a%20Gini%20K-means%0Aalgorithm%20that%20is%20provably%20convergent%20and%20resilient%20to%20noisy%20data%3B%20and%20%283%29%20a%0AGini%20KNN%20method%20that%20performs%20competitively%20with%20state-of-the-art%20approaches%0Alike%20Hassanat%27s%20distance%20in%20noisy%20environments.%20Experimental%20evaluations%20on%2016%0AUCI%20datasets%20demonstrate%20the%20superior%20performance%20and%20efficiency%20of%20the%0AGini-based%20algorithms%20in%20clustering%20and%20classification%20tasks.%20This%20work%20opens%0Anew%20directions%20for%20rank-based%20prametrics%20in%20machine%20learning%20and%20statistical%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18028v3&entry.124074799=Read"},
{"title": "A Consolidated Volatility Prediction with Back Propagation Neural\n  Network and Genetic Algorithm", "author": "Zong Ke and Jingyu Xu and Zizhou Zhang and Yu Cheng and Wenjun Wu", "abstract": "  This paper provides a unique approach with AI algorithms to predict emerging\nstock markets volatility. Traditionally, stock volatility is derived from\nhistorical volatility,Monte Carlo simulation and implied volatility as well. In\nthis paper, the writer designs a consolidated model with back-propagation\nneural network and genetic algorithm to predict future volatility of emerging\nstock markets and found that the results are quite accurate with low errors.\n", "link": "http://arxiv.org/abs/2412.07223v7", "date": "2025-08-26", "relevancy": 1.7958, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4821}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4549}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Consolidated%20Volatility%20Prediction%20with%20Back%20Propagation%20Neural%0A%20%20Network%20and%20Genetic%20Algorithm&body=Title%3A%20A%20Consolidated%20Volatility%20Prediction%20with%20Back%20Propagation%20Neural%0A%20%20Network%20and%20Genetic%20Algorithm%0AAuthor%3A%20Zong%20Ke%20and%20Jingyu%20Xu%20and%20Zizhou%20Zhang%20and%20Yu%20Cheng%20and%20Wenjun%20Wu%0AAbstract%3A%20%20%20This%20paper%20provides%20a%20unique%20approach%20with%20AI%20algorithms%20to%20predict%20emerging%0Astock%20markets%20volatility.%20Traditionally%2C%20stock%20volatility%20is%20derived%20from%0Ahistorical%20volatility%2CMonte%20Carlo%20simulation%20and%20implied%20volatility%20as%20well.%20In%0Athis%20paper%2C%20the%20writer%20designs%20a%20consolidated%20model%20with%20back-propagation%0Aneural%20network%20and%20genetic%20algorithm%20to%20predict%20future%20volatility%20of%20emerging%0Astock%20markets%20and%20found%20that%20the%20results%20are%20quite%20accurate%20with%20low%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07223v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Consolidated%2520Volatility%2520Prediction%2520with%2520Back%2520Propagation%2520Neural%250A%2520%2520Network%2520and%2520Genetic%2520Algorithm%26entry.906535625%3DZong%2520Ke%2520and%2520Jingyu%2520Xu%2520and%2520Zizhou%2520Zhang%2520and%2520Yu%2520Cheng%2520and%2520Wenjun%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520a%2520unique%2520approach%2520with%2520AI%2520algorithms%2520to%2520predict%2520emerging%250Astock%2520markets%2520volatility.%2520Traditionally%252C%2520stock%2520volatility%2520is%2520derived%2520from%250Ahistorical%2520volatility%252CMonte%2520Carlo%2520simulation%2520and%2520implied%2520volatility%2520as%2520well.%2520In%250Athis%2520paper%252C%2520the%2520writer%2520designs%2520a%2520consolidated%2520model%2520with%2520back-propagation%250Aneural%2520network%2520and%2520genetic%2520algorithm%2520to%2520predict%2520future%2520volatility%2520of%2520emerging%250Astock%2520markets%2520and%2520found%2520that%2520the%2520results%2520are%2520quite%2520accurate%2520with%2520low%2520errors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07223v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Consolidated%20Volatility%20Prediction%20with%20Back%20Propagation%20Neural%0A%20%20Network%20and%20Genetic%20Algorithm&entry.906535625=Zong%20Ke%20and%20Jingyu%20Xu%20and%20Zizhou%20Zhang%20and%20Yu%20Cheng%20and%20Wenjun%20Wu&entry.1292438233=%20%20This%20paper%20provides%20a%20unique%20approach%20with%20AI%20algorithms%20to%20predict%20emerging%0Astock%20markets%20volatility.%20Traditionally%2C%20stock%20volatility%20is%20derived%20from%0Ahistorical%20volatility%2CMonte%20Carlo%20simulation%20and%20implied%20volatility%20as%20well.%20In%0Athis%20paper%2C%20the%20writer%20designs%20a%20consolidated%20model%20with%20back-propagation%0Aneural%20network%20and%20genetic%20algorithm%20to%20predict%20future%20volatility%20of%20emerging%0Astock%20markets%20and%20found%20that%20the%20results%20are%20quite%20accurate%20with%20low%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07223v7&entry.124074799=Read"},
{"title": "Uncertainty-Resilient Active Intention Recognition for Robotic\n  Assistants", "author": "Juan Carlos Sabor\u00edo and Marc Vinci and Oscar Lima and Sebastian Stock and Lennart Niecksch and Martin G\u00fcnther and Alexander Sung and Joachim Hertzberg and Martin Atzm\u00fcller", "abstract": "  Purposeful behavior in robotic assistants requires the integration of\nmultiple components and technological advances. Often, the problem is reduced\nto recognizing explicit prompts, which limits autonomy, or is oversimplified\nthrough assumptions such as near-perfect information. We argue that a critical\ngap remains unaddressed -- specifically, the challenge of reasoning about the\nuncertain outcomes and perception errors inherent to human intention\nrecognition. In response, we present a framework designed to be resilient to\nuncertainty and sensor noise, integrating real-time sensor data with a\ncombination of planners. Centered around an intention-recognition POMDP, our\napproach addresses cooperative planning and acting under uncertainty. Our\nintegrated framework has been successfully tested on a physical robot with\npromising results.\n", "link": "http://arxiv.org/abs/2508.19150v1", "date": "2025-08-26", "relevancy": 1.7893, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6137}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6028}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Resilient%20Active%20Intention%20Recognition%20for%20Robotic%0A%20%20Assistants&body=Title%3A%20Uncertainty-Resilient%20Active%20Intention%20Recognition%20for%20Robotic%0A%20%20Assistants%0AAuthor%3A%20Juan%20Carlos%20Sabor%C3%ADo%20and%20Marc%20Vinci%20and%20Oscar%20Lima%20and%20Sebastian%20Stock%20and%20Lennart%20Niecksch%20and%20Martin%20G%C3%BCnther%20and%20Alexander%20Sung%20and%20Joachim%20Hertzberg%20and%20Martin%20Atzm%C3%BCller%0AAbstract%3A%20%20%20Purposeful%20behavior%20in%20robotic%20assistants%20requires%20the%20integration%20of%0Amultiple%20components%20and%20technological%20advances.%20Often%2C%20the%20problem%20is%20reduced%0Ato%20recognizing%20explicit%20prompts%2C%20which%20limits%20autonomy%2C%20or%20is%20oversimplified%0Athrough%20assumptions%20such%20as%20near-perfect%20information.%20We%20argue%20that%20a%20critical%0Agap%20remains%20unaddressed%20--%20specifically%2C%20the%20challenge%20of%20reasoning%20about%20the%0Auncertain%20outcomes%20and%20perception%20errors%20inherent%20to%20human%20intention%0Arecognition.%20In%20response%2C%20we%20present%20a%20framework%20designed%20to%20be%20resilient%20to%0Auncertainty%20and%20sensor%20noise%2C%20integrating%20real-time%20sensor%20data%20with%20a%0Acombination%20of%20planners.%20Centered%20around%20an%20intention-recognition%20POMDP%2C%20our%0Aapproach%20addresses%20cooperative%20planning%20and%20acting%20under%20uncertainty.%20Our%0Aintegrated%20framework%20has%20been%20successfully%20tested%20on%20a%20physical%20robot%20with%0Apromising%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Resilient%2520Active%2520Intention%2520Recognition%2520for%2520Robotic%250A%2520%2520Assistants%26entry.906535625%3DJuan%2520Carlos%2520Sabor%25C3%25ADo%2520and%2520Marc%2520Vinci%2520and%2520Oscar%2520Lima%2520and%2520Sebastian%2520Stock%2520and%2520Lennart%2520Niecksch%2520and%2520Martin%2520G%25C3%25BCnther%2520and%2520Alexander%2520Sung%2520and%2520Joachim%2520Hertzberg%2520and%2520Martin%2520Atzm%25C3%25BCller%26entry.1292438233%3D%2520%2520Purposeful%2520behavior%2520in%2520robotic%2520assistants%2520requires%2520the%2520integration%2520of%250Amultiple%2520components%2520and%2520technological%2520advances.%2520Often%252C%2520the%2520problem%2520is%2520reduced%250Ato%2520recognizing%2520explicit%2520prompts%252C%2520which%2520limits%2520autonomy%252C%2520or%2520is%2520oversimplified%250Athrough%2520assumptions%2520such%2520as%2520near-perfect%2520information.%2520We%2520argue%2520that%2520a%2520critical%250Agap%2520remains%2520unaddressed%2520--%2520specifically%252C%2520the%2520challenge%2520of%2520reasoning%2520about%2520the%250Auncertain%2520outcomes%2520and%2520perception%2520errors%2520inherent%2520to%2520human%2520intention%250Arecognition.%2520In%2520response%252C%2520we%2520present%2520a%2520framework%2520designed%2520to%2520be%2520resilient%2520to%250Auncertainty%2520and%2520sensor%2520noise%252C%2520integrating%2520real-time%2520sensor%2520data%2520with%2520a%250Acombination%2520of%2520planners.%2520Centered%2520around%2520an%2520intention-recognition%2520POMDP%252C%2520our%250Aapproach%2520addresses%2520cooperative%2520planning%2520and%2520acting%2520under%2520uncertainty.%2520Our%250Aintegrated%2520framework%2520has%2520been%2520successfully%2520tested%2520on%2520a%2520physical%2520robot%2520with%250Apromising%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Resilient%20Active%20Intention%20Recognition%20for%20Robotic%0A%20%20Assistants&entry.906535625=Juan%20Carlos%20Sabor%C3%ADo%20and%20Marc%20Vinci%20and%20Oscar%20Lima%20and%20Sebastian%20Stock%20and%20Lennart%20Niecksch%20and%20Martin%20G%C3%BCnther%20and%20Alexander%20Sung%20and%20Joachim%20Hertzberg%20and%20Martin%20Atzm%C3%BCller&entry.1292438233=%20%20Purposeful%20behavior%20in%20robotic%20assistants%20requires%20the%20integration%20of%0Amultiple%20components%20and%20technological%20advances.%20Often%2C%20the%20problem%20is%20reduced%0Ato%20recognizing%20explicit%20prompts%2C%20which%20limits%20autonomy%2C%20or%20is%20oversimplified%0Athrough%20assumptions%20such%20as%20near-perfect%20information.%20We%20argue%20that%20a%20critical%0Agap%20remains%20unaddressed%20--%20specifically%2C%20the%20challenge%20of%20reasoning%20about%20the%0Auncertain%20outcomes%20and%20perception%20errors%20inherent%20to%20human%20intention%0Arecognition.%20In%20response%2C%20we%20present%20a%20framework%20designed%20to%20be%20resilient%20to%0Auncertainty%20and%20sensor%20noise%2C%20integrating%20real-time%20sensor%20data%20with%20a%0Acombination%20of%20planners.%20Centered%20around%20an%20intention-recognition%20POMDP%2C%20our%0Aapproach%20addresses%20cooperative%20planning%20and%20acting%20under%20uncertainty.%20Our%0Aintegrated%20framework%20has%20been%20successfully%20tested%20on%20a%20physical%20robot%20with%0Apromising%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19150v1&entry.124074799=Read"},
{"title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling", "author": "Guoying Zhu and Meng Li and Haipeng Dai and Xuechen Liu and Weijun Wang and Keran Li and Jun xiao and Ligeng Chen and Wei Wang", "abstract": "  The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.\n", "link": "http://arxiv.org/abs/2508.18983v1", "date": "2025-08-26", "relevancy": 1.4208, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4765}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4744}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20MoE%20on%20the%20Edge%20via%20Importance-Driven%20Expert%20Scheduling&body=Title%3A%20Enabling%20MoE%20on%20the%20Edge%20via%20Importance-Driven%20Expert%20Scheduling%0AAuthor%3A%20Guoying%20Zhu%20and%20Meng%20Li%20and%20Haipeng%20Dai%20and%20Xuechen%20Liu%20and%20Weijun%20Wang%20and%20Keran%20Li%20and%20Jun%20xiao%20and%20Ligeng%20Chen%20and%20Wei%20Wang%0AAbstract%3A%20%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20architecture%20has%20emerged%20as%20a%20key%20technique%20for%0Ascaling%20Large%20Language%20Models%20by%20activating%20only%20a%20subset%20of%20experts%20per%20query.%0ADeploying%20MoE%20on%20consumer-grade%20edge%20hardware%2C%20however%2C%20is%20constrained%20by%0Alimited%20device%20memory%2C%20making%20dynamic%20expert%20offloading%20essential.%20Unlike%20prior%0Awork%20that%20treats%20offloading%20purely%20as%20a%20scheduling%20problem%2C%20we%20leverage%20expert%0Aimportance%20to%20guide%20decisions%2C%20substituting%20low-importance%20activated%20experts%0Awith%20functionally%20similar%20ones%20already%20cached%20in%20GPU%20memory%2C%20thereby%20preserving%0Aaccuracy.%20As%20a%20result%2C%20this%20design%20reduces%20memory%20usage%20and%20data%20transfer%2C%0Awhile%20largely%20eliminating%20PCIe%20overhead.%20In%20addition%2C%20we%20introduce%20a%20scheduling%0Apolicy%20that%20maximizes%20the%20reuse%20ratio%20of%20GPU-cached%20experts%2C%20further%20boosting%0Aefficiency.%20Extensive%20evaluations%20show%20that%20our%20approach%20delivers%2048%25%20lower%0Adecoding%20latency%20with%20over%2060%25%20expert%20cache%20hit%20rate%2C%20while%20maintaining%20nearly%0Alossless%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520MoE%2520on%2520the%2520Edge%2520via%2520Importance-Driven%2520Expert%2520Scheduling%26entry.906535625%3DGuoying%2520Zhu%2520and%2520Meng%2520Li%2520and%2520Haipeng%2520Dai%2520and%2520Xuechen%2520Liu%2520and%2520Weijun%2520Wang%2520and%2520Keran%2520Li%2520and%2520Jun%2520xiao%2520and%2520Ligeng%2520Chen%2520and%2520Wei%2520Wang%26entry.1292438233%3D%2520%2520The%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520architecture%2520has%2520emerged%2520as%2520a%2520key%2520technique%2520for%250Ascaling%2520Large%2520Language%2520Models%2520by%2520activating%2520only%2520a%2520subset%2520of%2520experts%2520per%2520query.%250ADeploying%2520MoE%2520on%2520consumer-grade%2520edge%2520hardware%252C%2520however%252C%2520is%2520constrained%2520by%250Alimited%2520device%2520memory%252C%2520making%2520dynamic%2520expert%2520offloading%2520essential.%2520Unlike%2520prior%250Awork%2520that%2520treats%2520offloading%2520purely%2520as%2520a%2520scheduling%2520problem%252C%2520we%2520leverage%2520expert%250Aimportance%2520to%2520guide%2520decisions%252C%2520substituting%2520low-importance%2520activated%2520experts%250Awith%2520functionally%2520similar%2520ones%2520already%2520cached%2520in%2520GPU%2520memory%252C%2520thereby%2520preserving%250Aaccuracy.%2520As%2520a%2520result%252C%2520this%2520design%2520reduces%2520memory%2520usage%2520and%2520data%2520transfer%252C%250Awhile%2520largely%2520eliminating%2520PCIe%2520overhead.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520scheduling%250Apolicy%2520that%2520maximizes%2520the%2520reuse%2520ratio%2520of%2520GPU-cached%2520experts%252C%2520further%2520boosting%250Aefficiency.%2520Extensive%2520evaluations%2520show%2520that%2520our%2520approach%2520delivers%252048%2525%2520lower%250Adecoding%2520latency%2520with%2520over%252060%2525%2520expert%2520cache%2520hit%2520rate%252C%2520while%2520maintaining%2520nearly%250Alossless%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20MoE%20on%20the%20Edge%20via%20Importance-Driven%20Expert%20Scheduling&entry.906535625=Guoying%20Zhu%20and%20Meng%20Li%20and%20Haipeng%20Dai%20and%20Xuechen%20Liu%20and%20Weijun%20Wang%20and%20Keran%20Li%20and%20Jun%20xiao%20and%20Ligeng%20Chen%20and%20Wei%20Wang&entry.1292438233=%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20architecture%20has%20emerged%20as%20a%20key%20technique%20for%0Ascaling%20Large%20Language%20Models%20by%20activating%20only%20a%20subset%20of%20experts%20per%20query.%0ADeploying%20MoE%20on%20consumer-grade%20edge%20hardware%2C%20however%2C%20is%20constrained%20by%0Alimited%20device%20memory%2C%20making%20dynamic%20expert%20offloading%20essential.%20Unlike%20prior%0Awork%20that%20treats%20offloading%20purely%20as%20a%20scheduling%20problem%2C%20we%20leverage%20expert%0Aimportance%20to%20guide%20decisions%2C%20substituting%20low-importance%20activated%20experts%0Awith%20functionally%20similar%20ones%20already%20cached%20in%20GPU%20memory%2C%20thereby%20preserving%0Aaccuracy.%20As%20a%20result%2C%20this%20design%20reduces%20memory%20usage%20and%20data%20transfer%2C%0Awhile%20largely%20eliminating%20PCIe%20overhead.%20In%20addition%2C%20we%20introduce%20a%20scheduling%0Apolicy%20that%20maximizes%20the%20reuse%20ratio%20of%20GPU-cached%20experts%2C%20further%20boosting%0Aefficiency.%20Extensive%20evaluations%20show%20that%20our%20approach%20delivers%2048%25%20lower%0Adecoding%20latency%20with%20over%2060%25%20expert%20cache%20hit%20rate%2C%20while%20maintaining%20nearly%0Alossless%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18983v1&entry.124074799=Read"},
{"title": "RDDM: Practicing RAW Domain Diffusion Model for Real-world Image\n  Restoration", "author": "Yan Chen and Yi Wen and Wei Li and Junchao Liu and Yong Guo and Jie Hu and Xinghao Chen", "abstract": "  We present the RAW domain diffusion model (RDDM), an end-to-end diffusion\nmodel that restores photo-realistic images directly from the sensor RAW data.\nWhile recent sRGB-domain diffusion methods achieve impressive results, they are\ncaught in a dilemma between high fidelity and realistic generation. As these\nmodels process lossy sRGB inputs and neglect the accessibility of the sensor\nRAW images in many scenarios, e.g., in image and video capturing in edge\ndevices, resulting in sub-optimal performance. RDDM bypasses this limitation by\ndirectly restoring images in the RAW domain, replacing the conventional\ntwo-stage image signal processing (ISP) + IR pipeline. However, a simple\nadaptation of pre-trained diffusion models to the RAW domain confronts the\nout-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE\n(RVAE) learning optimal latent representations, (2) a differentiable Post Tone\nProcessing (PTP) module enabling joint RAW and sRGB space optimization. To\ncompensate for the deficiency in the dataset, we develop a scalable degradation\npipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for\nlarge-scale training. Furthermore, we devise a configurable multi-bayer (CMB)\nLoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive\nexperiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion\nmethods, yielding higher fidelity results with fewer artifacts.\n", "link": "http://arxiv.org/abs/2508.19154v1", "date": "2025-08-26", "relevancy": 1.2411, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6749}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5987}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RDDM%3A%20Practicing%20RAW%20Domain%20Diffusion%20Model%20for%20Real-world%20Image%0A%20%20Restoration&body=Title%3A%20RDDM%3A%20Practicing%20RAW%20Domain%20Diffusion%20Model%20for%20Real-world%20Image%0A%20%20Restoration%0AAuthor%3A%20Yan%20Chen%20and%20Yi%20Wen%20and%20Wei%20Li%20and%20Junchao%20Liu%20and%20Yong%20Guo%20and%20Jie%20Hu%20and%20Xinghao%20Chen%0AAbstract%3A%20%20%20We%20present%20the%20RAW%20domain%20diffusion%20model%20%28RDDM%29%2C%20an%20end-to-end%20diffusion%0Amodel%20that%20restores%20photo-realistic%20images%20directly%20from%20the%20sensor%20RAW%20data.%0AWhile%20recent%20sRGB-domain%20diffusion%20methods%20achieve%20impressive%20results%2C%20they%20are%0Acaught%20in%20a%20dilemma%20between%20high%20fidelity%20and%20realistic%20generation.%20As%20these%0Amodels%20process%20lossy%20sRGB%20inputs%20and%20neglect%20the%20accessibility%20of%20the%20sensor%0ARAW%20images%20in%20many%20scenarios%2C%20e.g.%2C%20in%20image%20and%20video%20capturing%20in%20edge%0Adevices%2C%20resulting%20in%20sub-optimal%20performance.%20RDDM%20bypasses%20this%20limitation%20by%0Adirectly%20restoring%20images%20in%20the%20RAW%20domain%2C%20replacing%20the%20conventional%0Atwo-stage%20image%20signal%20processing%20%28ISP%29%20%2B%20IR%20pipeline.%20However%2C%20a%20simple%0Aadaptation%20of%20pre-trained%20diffusion%20models%20to%20the%20RAW%20domain%20confronts%20the%0Aout-of-distribution%20%28OOD%29%20issues.%20To%20this%20end%2C%20we%20propose%3A%20%281%29%20a%20RAW-domain%20VAE%0A%28RVAE%29%20learning%20optimal%20latent%20representations%2C%20%282%29%20a%20differentiable%20Post%20Tone%0AProcessing%20%28PTP%29%20module%20enabling%20joint%20RAW%20and%20sRGB%20space%20optimization.%20To%0Acompensate%20for%20the%20deficiency%20in%20the%20dataset%2C%20we%20develop%20a%20scalable%20degradation%0Apipeline%20synthesizing%20RAW%20LQ-HQ%20pairs%20from%20existing%20sRGB%20datasets%20for%0Alarge-scale%20training.%20Furthermore%2C%20we%20devise%20a%20configurable%20multi-bayer%20%28CMB%29%0ALoRA%20module%20handling%20diverse%20RAW%20patterns%20such%20as%20RGGB%2C%20BGGR%2C%20etc.%20Extensive%0Aexperiments%20demonstrate%20RDDM%27s%20superiority%20over%20state-of-the-art%20sRGB%20diffusion%0Amethods%2C%20yielding%20higher%20fidelity%20results%20with%20fewer%20artifacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRDDM%253A%2520Practicing%2520RAW%2520Domain%2520Diffusion%2520Model%2520for%2520Real-world%2520Image%250A%2520%2520Restoration%26entry.906535625%3DYan%2520Chen%2520and%2520Yi%2520Wen%2520and%2520Wei%2520Li%2520and%2520Junchao%2520Liu%2520and%2520Yong%2520Guo%2520and%2520Jie%2520Hu%2520and%2520Xinghao%2520Chen%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520RAW%2520domain%2520diffusion%2520model%2520%2528RDDM%2529%252C%2520an%2520end-to-end%2520diffusion%250Amodel%2520that%2520restores%2520photo-realistic%2520images%2520directly%2520from%2520the%2520sensor%2520RAW%2520data.%250AWhile%2520recent%2520sRGB-domain%2520diffusion%2520methods%2520achieve%2520impressive%2520results%252C%2520they%2520are%250Acaught%2520in%2520a%2520dilemma%2520between%2520high%2520fidelity%2520and%2520realistic%2520generation.%2520As%2520these%250Amodels%2520process%2520lossy%2520sRGB%2520inputs%2520and%2520neglect%2520the%2520accessibility%2520of%2520the%2520sensor%250ARAW%2520images%2520in%2520many%2520scenarios%252C%2520e.g.%252C%2520in%2520image%2520and%2520video%2520capturing%2520in%2520edge%250Adevices%252C%2520resulting%2520in%2520sub-optimal%2520performance.%2520RDDM%2520bypasses%2520this%2520limitation%2520by%250Adirectly%2520restoring%2520images%2520in%2520the%2520RAW%2520domain%252C%2520replacing%2520the%2520conventional%250Atwo-stage%2520image%2520signal%2520processing%2520%2528ISP%2529%2520%252B%2520IR%2520pipeline.%2520However%252C%2520a%2520simple%250Aadaptation%2520of%2520pre-trained%2520diffusion%2520models%2520to%2520the%2520RAW%2520domain%2520confronts%2520the%250Aout-of-distribution%2520%2528OOD%2529%2520issues.%2520To%2520this%2520end%252C%2520we%2520propose%253A%2520%25281%2529%2520a%2520RAW-domain%2520VAE%250A%2528RVAE%2529%2520learning%2520optimal%2520latent%2520representations%252C%2520%25282%2529%2520a%2520differentiable%2520Post%2520Tone%250AProcessing%2520%2528PTP%2529%2520module%2520enabling%2520joint%2520RAW%2520and%2520sRGB%2520space%2520optimization.%2520To%250Acompensate%2520for%2520the%2520deficiency%2520in%2520the%2520dataset%252C%2520we%2520develop%2520a%2520scalable%2520degradation%250Apipeline%2520synthesizing%2520RAW%2520LQ-HQ%2520pairs%2520from%2520existing%2520sRGB%2520datasets%2520for%250Alarge-scale%2520training.%2520Furthermore%252C%2520we%2520devise%2520a%2520configurable%2520multi-bayer%2520%2528CMB%2529%250ALoRA%2520module%2520handling%2520diverse%2520RAW%2520patterns%2520such%2520as%2520RGGB%252C%2520BGGR%252C%2520etc.%2520Extensive%250Aexperiments%2520demonstrate%2520RDDM%2527s%2520superiority%2520over%2520state-of-the-art%2520sRGB%2520diffusion%250Amethods%252C%2520yielding%2520higher%2520fidelity%2520results%2520with%2520fewer%2520artifacts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RDDM%3A%20Practicing%20RAW%20Domain%20Diffusion%20Model%20for%20Real-world%20Image%0A%20%20Restoration&entry.906535625=Yan%20Chen%20and%20Yi%20Wen%20and%20Wei%20Li%20and%20Junchao%20Liu%20and%20Yong%20Guo%20and%20Jie%20Hu%20and%20Xinghao%20Chen&entry.1292438233=%20%20We%20present%20the%20RAW%20domain%20diffusion%20model%20%28RDDM%29%2C%20an%20end-to-end%20diffusion%0Amodel%20that%20restores%20photo-realistic%20images%20directly%20from%20the%20sensor%20RAW%20data.%0AWhile%20recent%20sRGB-domain%20diffusion%20methods%20achieve%20impressive%20results%2C%20they%20are%0Acaught%20in%20a%20dilemma%20between%20high%20fidelity%20and%20realistic%20generation.%20As%20these%0Amodels%20process%20lossy%20sRGB%20inputs%20and%20neglect%20the%20accessibility%20of%20the%20sensor%0ARAW%20images%20in%20many%20scenarios%2C%20e.g.%2C%20in%20image%20and%20video%20capturing%20in%20edge%0Adevices%2C%20resulting%20in%20sub-optimal%20performance.%20RDDM%20bypasses%20this%20limitation%20by%0Adirectly%20restoring%20images%20in%20the%20RAW%20domain%2C%20replacing%20the%20conventional%0Atwo-stage%20image%20signal%20processing%20%28ISP%29%20%2B%20IR%20pipeline.%20However%2C%20a%20simple%0Aadaptation%20of%20pre-trained%20diffusion%20models%20to%20the%20RAW%20domain%20confronts%20the%0Aout-of-distribution%20%28OOD%29%20issues.%20To%20this%20end%2C%20we%20propose%3A%20%281%29%20a%20RAW-domain%20VAE%0A%28RVAE%29%20learning%20optimal%20latent%20representations%2C%20%282%29%20a%20differentiable%20Post%20Tone%0AProcessing%20%28PTP%29%20module%20enabling%20joint%20RAW%20and%20sRGB%20space%20optimization.%20To%0Acompensate%20for%20the%20deficiency%20in%20the%20dataset%2C%20we%20develop%20a%20scalable%20degradation%0Apipeline%20synthesizing%20RAW%20LQ-HQ%20pairs%20from%20existing%20sRGB%20datasets%20for%0Alarge-scale%20training.%20Furthermore%2C%20we%20devise%20a%20configurable%20multi-bayer%20%28CMB%29%0ALoRA%20module%20handling%20diverse%20RAW%20patterns%20such%20as%20RGGB%2C%20BGGR%2C%20etc.%20Extensive%0Aexperiments%20demonstrate%20RDDM%27s%20superiority%20over%20state-of-the-art%20sRGB%20diffusion%0Amethods%2C%20yielding%20higher%20fidelity%20results%20with%20fewer%20artifacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19154v1&entry.124074799=Read"},
{"title": "The Subset Sum Matching Problem", "author": "Yufei Wu and Manuel R. Torres and Parisa Zehtabi and Alberto Pozanco Lancho and Michael Cashmore and Daniel Borrajo and Manuela Veloso", "abstract": "  This paper presents a new combinatorial optimisation task, the Subset Sum\nMatching Problem (SSMP), which is an abstraction of common financial\napplications such as trades reconciliation. We present three algorithms, two\nsuboptimal and one optimal, to solve this problem. We also generate a benchmark\nto cover different instances of SSMP varying in complexity, and carry out an\nexperimental evaluation to assess the performance of the approaches.\n", "link": "http://arxiv.org/abs/2508.19218v1", "date": "2025-08-26", "relevancy": 1.4987, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.3754}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3745}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Subset%20Sum%20Matching%20Problem&body=Title%3A%20The%20Subset%20Sum%20Matching%20Problem%0AAuthor%3A%20Yufei%20Wu%20and%20Manuel%20R.%20Torres%20and%20Parisa%20Zehtabi%20and%20Alberto%20Pozanco%20Lancho%20and%20Michael%20Cashmore%20and%20Daniel%20Borrajo%20and%20Manuela%20Veloso%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20combinatorial%20optimisation%20task%2C%20the%20Subset%20Sum%0AMatching%20Problem%20%28SSMP%29%2C%20which%20is%20an%20abstraction%20of%20common%20financial%0Aapplications%20such%20as%20trades%20reconciliation.%20We%20present%20three%20algorithms%2C%20two%0Asuboptimal%20and%20one%20optimal%2C%20to%20solve%20this%20problem.%20We%20also%20generate%20a%20benchmark%0Ato%20cover%20different%20instances%20of%20SSMP%20varying%20in%20complexity%2C%20and%20carry%20out%20an%0Aexperimental%20evaluation%20to%20assess%20the%20performance%20of%20the%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Subset%2520Sum%2520Matching%2520Problem%26entry.906535625%3DYufei%2520Wu%2520and%2520Manuel%2520R.%2520Torres%2520and%2520Parisa%2520Zehtabi%2520and%2520Alberto%2520Pozanco%2520Lancho%2520and%2520Michael%2520Cashmore%2520and%2520Daniel%2520Borrajo%2520and%2520Manuela%2520Veloso%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520new%2520combinatorial%2520optimisation%2520task%252C%2520the%2520Subset%2520Sum%250AMatching%2520Problem%2520%2528SSMP%2529%252C%2520which%2520is%2520an%2520abstraction%2520of%2520common%2520financial%250Aapplications%2520such%2520as%2520trades%2520reconciliation.%2520We%2520present%2520three%2520algorithms%252C%2520two%250Asuboptimal%2520and%2520one%2520optimal%252C%2520to%2520solve%2520this%2520problem.%2520We%2520also%2520generate%2520a%2520benchmark%250Ato%2520cover%2520different%2520instances%2520of%2520SSMP%2520varying%2520in%2520complexity%252C%2520and%2520carry%2520out%2520an%250Aexperimental%2520evaluation%2520to%2520assess%2520the%2520performance%2520of%2520the%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Subset%20Sum%20Matching%20Problem&entry.906535625=Yufei%20Wu%20and%20Manuel%20R.%20Torres%20and%20Parisa%20Zehtabi%20and%20Alberto%20Pozanco%20Lancho%20and%20Michael%20Cashmore%20and%20Daniel%20Borrajo%20and%20Manuela%20Veloso&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20combinatorial%20optimisation%20task%2C%20the%20Subset%20Sum%0AMatching%20Problem%20%28SSMP%29%2C%20which%20is%20an%20abstraction%20of%20common%20financial%0Aapplications%20such%20as%20trades%20reconciliation.%20We%20present%20three%20algorithms%2C%20two%0Asuboptimal%20and%20one%20optimal%2C%20to%20solve%20this%20problem.%20We%20also%20generate%20a%20benchmark%0Ato%20cover%20different%20instances%20of%20SSMP%20varying%20in%20complexity%2C%20and%20carry%20out%20an%0Aexperimental%20evaluation%20to%20assess%20the%20performance%20of%20the%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19218v1&entry.124074799=Read"},
{"title": "Real-Time Model Checking for Closed-Loop Robot Reactive Planning", "author": "Christopher Chandler and Bernd Porr and Giulia Lafratta and Alice Miller", "abstract": "  We present a new application of model checking which achieves real-time\nmulti-step planning and obstacle avoidance on a real autonomous robot. We have\ndeveloped a small, purpose-built model checking algorithm which generates plans\nin situ based on \"core\" knowledge and attention as found in biological agents.\nThis is achieved in real-time using no pre-computed data on a low-powered\ndevice. Our approach is based on chaining temporary control systems which are\nspawned to counteract disturbances in the local environment that disrupt an\nautonomous agent from its preferred action (or resting state). A novel\ndiscretization of 2D LiDAR data sensitive to bounded variations in the local\nenvironment is used. Multi-step planning using model checking by forward\ndepth-first search is applied to cul-de-sac and playground scenarios. Both\nempirical results and informal proofs of two fundamental properties of our\napproach demonstrate that model checking can be used to create efficient\nmulti-step plans for local obstacle avoidance, improving on the performance of\na reactive agent which can only plan one step. Our approach is an instructional\ncase study for the development of safe, reliable and explainable planning in\nthe context of autonomous vehicles.\n", "link": "http://arxiv.org/abs/2508.19186v1", "date": "2025-08-26", "relevancy": 1.6825, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5914}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5745}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Model%20Checking%20for%20Closed-Loop%20Robot%20Reactive%20Planning&body=Title%3A%20Real-Time%20Model%20Checking%20for%20Closed-Loop%20Robot%20Reactive%20Planning%0AAuthor%3A%20Christopher%20Chandler%20and%20Bernd%20Porr%20and%20Giulia%20Lafratta%20and%20Alice%20Miller%0AAbstract%3A%20%20%20We%20present%20a%20new%20application%20of%20model%20checking%20which%20achieves%20real-time%0Amulti-step%20planning%20and%20obstacle%20avoidance%20on%20a%20real%20autonomous%20robot.%20We%20have%0Adeveloped%20a%20small%2C%20purpose-built%20model%20checking%20algorithm%20which%20generates%20plans%0Ain%20situ%20based%20on%20%22core%22%20knowledge%20and%20attention%20as%20found%20in%20biological%20agents.%0AThis%20is%20achieved%20in%20real-time%20using%20no%20pre-computed%20data%20on%20a%20low-powered%0Adevice.%20Our%20approach%20is%20based%20on%20chaining%20temporary%20control%20systems%20which%20are%0Aspawned%20to%20counteract%20disturbances%20in%20the%20local%20environment%20that%20disrupt%20an%0Aautonomous%20agent%20from%20its%20preferred%20action%20%28or%20resting%20state%29.%20A%20novel%0Adiscretization%20of%202D%20LiDAR%20data%20sensitive%20to%20bounded%20variations%20in%20the%20local%0Aenvironment%20is%20used.%20Multi-step%20planning%20using%20model%20checking%20by%20forward%0Adepth-first%20search%20is%20applied%20to%20cul-de-sac%20and%20playground%20scenarios.%20Both%0Aempirical%20results%20and%20informal%20proofs%20of%20two%20fundamental%20properties%20of%20our%0Aapproach%20demonstrate%20that%20model%20checking%20can%20be%20used%20to%20create%20efficient%0Amulti-step%20plans%20for%20local%20obstacle%20avoidance%2C%20improving%20on%20the%20performance%20of%0Aa%20reactive%20agent%20which%20can%20only%20plan%20one%20step.%20Our%20approach%20is%20an%20instructional%0Acase%20study%20for%20the%20development%20of%20safe%2C%20reliable%20and%20explainable%20planning%20in%0Athe%20context%20of%20autonomous%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Model%2520Checking%2520for%2520Closed-Loop%2520Robot%2520Reactive%2520Planning%26entry.906535625%3DChristopher%2520Chandler%2520and%2520Bernd%2520Porr%2520and%2520Giulia%2520Lafratta%2520and%2520Alice%2520Miller%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520application%2520of%2520model%2520checking%2520which%2520achieves%2520real-time%250Amulti-step%2520planning%2520and%2520obstacle%2520avoidance%2520on%2520a%2520real%2520autonomous%2520robot.%2520We%2520have%250Adeveloped%2520a%2520small%252C%2520purpose-built%2520model%2520checking%2520algorithm%2520which%2520generates%2520plans%250Ain%2520situ%2520based%2520on%2520%2522core%2522%2520knowledge%2520and%2520attention%2520as%2520found%2520in%2520biological%2520agents.%250AThis%2520is%2520achieved%2520in%2520real-time%2520using%2520no%2520pre-computed%2520data%2520on%2520a%2520low-powered%250Adevice.%2520Our%2520approach%2520is%2520based%2520on%2520chaining%2520temporary%2520control%2520systems%2520which%2520are%250Aspawned%2520to%2520counteract%2520disturbances%2520in%2520the%2520local%2520environment%2520that%2520disrupt%2520an%250Aautonomous%2520agent%2520from%2520its%2520preferred%2520action%2520%2528or%2520resting%2520state%2529.%2520A%2520novel%250Adiscretization%2520of%25202D%2520LiDAR%2520data%2520sensitive%2520to%2520bounded%2520variations%2520in%2520the%2520local%250Aenvironment%2520is%2520used.%2520Multi-step%2520planning%2520using%2520model%2520checking%2520by%2520forward%250Adepth-first%2520search%2520is%2520applied%2520to%2520cul-de-sac%2520and%2520playground%2520scenarios.%2520Both%250Aempirical%2520results%2520and%2520informal%2520proofs%2520of%2520two%2520fundamental%2520properties%2520of%2520our%250Aapproach%2520demonstrate%2520that%2520model%2520checking%2520can%2520be%2520used%2520to%2520create%2520efficient%250Amulti-step%2520plans%2520for%2520local%2520obstacle%2520avoidance%252C%2520improving%2520on%2520the%2520performance%2520of%250Aa%2520reactive%2520agent%2520which%2520can%2520only%2520plan%2520one%2520step.%2520Our%2520approach%2520is%2520an%2520instructional%250Acase%2520study%2520for%2520the%2520development%2520of%2520safe%252C%2520reliable%2520and%2520explainable%2520planning%2520in%250Athe%2520context%2520of%2520autonomous%2520vehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Model%20Checking%20for%20Closed-Loop%20Robot%20Reactive%20Planning&entry.906535625=Christopher%20Chandler%20and%20Bernd%20Porr%20and%20Giulia%20Lafratta%20and%20Alice%20Miller&entry.1292438233=%20%20We%20present%20a%20new%20application%20of%20model%20checking%20which%20achieves%20real-time%0Amulti-step%20planning%20and%20obstacle%20avoidance%20on%20a%20real%20autonomous%20robot.%20We%20have%0Adeveloped%20a%20small%2C%20purpose-built%20model%20checking%20algorithm%20which%20generates%20plans%0Ain%20situ%20based%20on%20%22core%22%20knowledge%20and%20attention%20as%20found%20in%20biological%20agents.%0AThis%20is%20achieved%20in%20real-time%20using%20no%20pre-computed%20data%20on%20a%20low-powered%0Adevice.%20Our%20approach%20is%20based%20on%20chaining%20temporary%20control%20systems%20which%20are%0Aspawned%20to%20counteract%20disturbances%20in%20the%20local%20environment%20that%20disrupt%20an%0Aautonomous%20agent%20from%20its%20preferred%20action%20%28or%20resting%20state%29.%20A%20novel%0Adiscretization%20of%202D%20LiDAR%20data%20sensitive%20to%20bounded%20variations%20in%20the%20local%0Aenvironment%20is%20used.%20Multi-step%20planning%20using%20model%20checking%20by%20forward%0Adepth-first%20search%20is%20applied%20to%20cul-de-sac%20and%20playground%20scenarios.%20Both%0Aempirical%20results%20and%20informal%20proofs%20of%20two%20fundamental%20properties%20of%20our%0Aapproach%20demonstrate%20that%20model%20checking%20can%20be%20used%20to%20create%20efficient%0Amulti-step%20plans%20for%20local%20obstacle%20avoidance%2C%20improving%20on%20the%20performance%20of%0Aa%20reactive%20agent%20which%20can%20only%20plan%20one%20step.%20Our%20approach%20is%20an%20instructional%0Acase%20study%20for%20the%20development%20of%20safe%2C%20reliable%20and%20explainable%20planning%20in%0Athe%20context%20of%20autonomous%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19186v1&entry.124074799=Read"},
{"title": "Model Context Protocols in Adaptive Transport Systems: A Survey", "author": "Gaurab Chhetri and Shriyank Somvanshi and Md Monzurul Islam and Shamyo Brotee and Mahmuda Sultana Mimi and Dipti Koirala and Biplov Pandey and Subasish Das", "abstract": "  The rapid expansion of interconnected devices, autonomous systems, and AI\napplications has created severe fragmentation in adaptive transport systems,\nwhere diverse protocols and context sources remain isolated. This survey\nprovides the first systematic investigation of the Model Context Protocol (MCP)\nas a unifying paradigm, highlighting its ability to bridge protocol-level\nadaptation with context-aware decision making. Analyzing established\nliterature, we show that existing efforts have implicitly converged toward\nMCP-like architectures, signaling a natural evolution from fragmented solutions\nto standardized integration frameworks. We propose a five-category taxonomy\ncovering adaptive mechanisms, context-aware frameworks, unification models,\nintegration strategies, and MCP-enabled architectures. Our findings reveal\nthree key insights: traditional transport protocols have reached the limits of\nisolated adaptation, MCP's client-server and JSON-RPC structure enables\nsemantic interoperability, and AI-driven transport demands integration\nparadigms uniquely suited to MCP. Finally, we present a research roadmap\npositioning MCP as a foundation for next-generation adaptive, context-aware,\nand intelligent transport infrastructures.\n", "link": "http://arxiv.org/abs/2508.19239v1", "date": "2025-08-26", "relevancy": 1.7455, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Context%20Protocols%20in%20Adaptive%20Transport%20Systems%3A%20A%20Survey&body=Title%3A%20Model%20Context%20Protocols%20in%20Adaptive%20Transport%20Systems%3A%20A%20Survey%0AAuthor%3A%20Gaurab%20Chhetri%20and%20Shriyank%20Somvanshi%20and%20Md%20Monzurul%20Islam%20and%20Shamyo%20Brotee%20and%20Mahmuda%20Sultana%20Mimi%20and%20Dipti%20Koirala%20and%20Biplov%20Pandey%20and%20Subasish%20Das%0AAbstract%3A%20%20%20The%20rapid%20expansion%20of%20interconnected%20devices%2C%20autonomous%20systems%2C%20and%20AI%0Aapplications%20has%20created%20severe%20fragmentation%20in%20adaptive%20transport%20systems%2C%0Awhere%20diverse%20protocols%20and%20context%20sources%20remain%20isolated.%20This%20survey%0Aprovides%20the%20first%20systematic%20investigation%20of%20the%20Model%20Context%20Protocol%20%28MCP%29%0Aas%20a%20unifying%20paradigm%2C%20highlighting%20its%20ability%20to%20bridge%20protocol-level%0Aadaptation%20with%20context-aware%20decision%20making.%20Analyzing%20established%0Aliterature%2C%20we%20show%20that%20existing%20efforts%20have%20implicitly%20converged%20toward%0AMCP-like%20architectures%2C%20signaling%20a%20natural%20evolution%20from%20fragmented%20solutions%0Ato%20standardized%20integration%20frameworks.%20We%20propose%20a%20five-category%20taxonomy%0Acovering%20adaptive%20mechanisms%2C%20context-aware%20frameworks%2C%20unification%20models%2C%0Aintegration%20strategies%2C%20and%20MCP-enabled%20architectures.%20Our%20findings%20reveal%0Athree%20key%20insights%3A%20traditional%20transport%20protocols%20have%20reached%20the%20limits%20of%0Aisolated%20adaptation%2C%20MCP%27s%20client-server%20and%20JSON-RPC%20structure%20enables%0Asemantic%20interoperability%2C%20and%20AI-driven%20transport%20demands%20integration%0Aparadigms%20uniquely%20suited%20to%20MCP.%20Finally%2C%20we%20present%20a%20research%20roadmap%0Apositioning%20MCP%20as%20a%20foundation%20for%20next-generation%20adaptive%2C%20context-aware%2C%0Aand%20intelligent%20transport%20infrastructures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Context%2520Protocols%2520in%2520Adaptive%2520Transport%2520Systems%253A%2520A%2520Survey%26entry.906535625%3DGaurab%2520Chhetri%2520and%2520Shriyank%2520Somvanshi%2520and%2520Md%2520Monzurul%2520Islam%2520and%2520Shamyo%2520Brotee%2520and%2520Mahmuda%2520Sultana%2520Mimi%2520and%2520Dipti%2520Koirala%2520and%2520Biplov%2520Pandey%2520and%2520Subasish%2520Das%26entry.1292438233%3D%2520%2520The%2520rapid%2520expansion%2520of%2520interconnected%2520devices%252C%2520autonomous%2520systems%252C%2520and%2520AI%250Aapplications%2520has%2520created%2520severe%2520fragmentation%2520in%2520adaptive%2520transport%2520systems%252C%250Awhere%2520diverse%2520protocols%2520and%2520context%2520sources%2520remain%2520isolated.%2520This%2520survey%250Aprovides%2520the%2520first%2520systematic%2520investigation%2520of%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529%250Aas%2520a%2520unifying%2520paradigm%252C%2520highlighting%2520its%2520ability%2520to%2520bridge%2520protocol-level%250Aadaptation%2520with%2520context-aware%2520decision%2520making.%2520Analyzing%2520established%250Aliterature%252C%2520we%2520show%2520that%2520existing%2520efforts%2520have%2520implicitly%2520converged%2520toward%250AMCP-like%2520architectures%252C%2520signaling%2520a%2520natural%2520evolution%2520from%2520fragmented%2520solutions%250Ato%2520standardized%2520integration%2520frameworks.%2520We%2520propose%2520a%2520five-category%2520taxonomy%250Acovering%2520adaptive%2520mechanisms%252C%2520context-aware%2520frameworks%252C%2520unification%2520models%252C%250Aintegration%2520strategies%252C%2520and%2520MCP-enabled%2520architectures.%2520Our%2520findings%2520reveal%250Athree%2520key%2520insights%253A%2520traditional%2520transport%2520protocols%2520have%2520reached%2520the%2520limits%2520of%250Aisolated%2520adaptation%252C%2520MCP%2527s%2520client-server%2520and%2520JSON-RPC%2520structure%2520enables%250Asemantic%2520interoperability%252C%2520and%2520AI-driven%2520transport%2520demands%2520integration%250Aparadigms%2520uniquely%2520suited%2520to%2520MCP.%2520Finally%252C%2520we%2520present%2520a%2520research%2520roadmap%250Apositioning%2520MCP%2520as%2520a%2520foundation%2520for%2520next-generation%2520adaptive%252C%2520context-aware%252C%250Aand%2520intelligent%2520transport%2520infrastructures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Context%20Protocols%20in%20Adaptive%20Transport%20Systems%3A%20A%20Survey&entry.906535625=Gaurab%20Chhetri%20and%20Shriyank%20Somvanshi%20and%20Md%20Monzurul%20Islam%20and%20Shamyo%20Brotee%20and%20Mahmuda%20Sultana%20Mimi%20and%20Dipti%20Koirala%20and%20Biplov%20Pandey%20and%20Subasish%20Das&entry.1292438233=%20%20The%20rapid%20expansion%20of%20interconnected%20devices%2C%20autonomous%20systems%2C%20and%20AI%0Aapplications%20has%20created%20severe%20fragmentation%20in%20adaptive%20transport%20systems%2C%0Awhere%20diverse%20protocols%20and%20context%20sources%20remain%20isolated.%20This%20survey%0Aprovides%20the%20first%20systematic%20investigation%20of%20the%20Model%20Context%20Protocol%20%28MCP%29%0Aas%20a%20unifying%20paradigm%2C%20highlighting%20its%20ability%20to%20bridge%20protocol-level%0Aadaptation%20with%20context-aware%20decision%20making.%20Analyzing%20established%0Aliterature%2C%20we%20show%20that%20existing%20efforts%20have%20implicitly%20converged%20toward%0AMCP-like%20architectures%2C%20signaling%20a%20natural%20evolution%20from%20fragmented%20solutions%0Ato%20standardized%20integration%20frameworks.%20We%20propose%20a%20five-category%20taxonomy%0Acovering%20adaptive%20mechanisms%2C%20context-aware%20frameworks%2C%20unification%20models%2C%0Aintegration%20strategies%2C%20and%20MCP-enabled%20architectures.%20Our%20findings%20reveal%0Athree%20key%20insights%3A%20traditional%20transport%20protocols%20have%20reached%20the%20limits%20of%0Aisolated%20adaptation%2C%20MCP%27s%20client-server%20and%20JSON-RPC%20structure%20enables%0Asemantic%20interoperability%2C%20and%20AI-driven%20transport%20demands%20integration%0Aparadigms%20uniquely%20suited%20to%20MCP.%20Finally%2C%20we%20present%20a%20research%20roadmap%0Apositioning%20MCP%20as%20a%20foundation%20for%20next-generation%20adaptive%2C%20context-aware%2C%0Aand%20intelligent%20transport%20infrastructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19239v1&entry.124074799=Read"},
{"title": "Safe Reinforcement Learning in Black-Box Environments via Adaptive\n  Shielding", "author": "Daniel Bethell and Simos Gerasimou and Radu Calinescu and Calum Imrie", "abstract": "  Empowering safe exploration of reinforcement learning (RL) agents during\ntraining is a critical challenge towards their deployment in many real-world\nscenarios. When prior knowledge of the domain or task is unavailable, training\nRL agents in unknown, black-box environments presents an even greater safety\nrisk. We introduce ADVICE (Adaptive Shielding with a Contrastive Autoencoder),\na novel post-shielding technique that distinguishes safe and unsafe features of\nstate-action pairs during training, and uses this knowledge to protect the RL\nagent from executing actions that yield likely hazardous outcomes. Our\ncomprehensive experimental evaluation against state-of-the-art safe RL\nexploration techniques shows that ADVICE significantly reduces safety\nviolations (approx 50%) during training, with a competitive outcome reward\ncompared to other techniques.\n", "link": "http://arxiv.org/abs/2405.18180v3", "date": "2025-08-26", "relevancy": 1.5908, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5654}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Reinforcement%20Learning%20in%20Black-Box%20Environments%20via%20Adaptive%0A%20%20Shielding&body=Title%3A%20Safe%20Reinforcement%20Learning%20in%20Black-Box%20Environments%20via%20Adaptive%0A%20%20Shielding%0AAuthor%3A%20Daniel%20Bethell%20and%20Simos%20Gerasimou%20and%20Radu%20Calinescu%20and%20Calum%20Imrie%0AAbstract%3A%20%20%20Empowering%20safe%20exploration%20of%20reinforcement%20learning%20%28RL%29%20agents%20during%0Atraining%20is%20a%20critical%20challenge%20towards%20their%20deployment%20in%20many%20real-world%0Ascenarios.%20When%20prior%20knowledge%20of%20the%20domain%20or%20task%20is%20unavailable%2C%20training%0ARL%20agents%20in%20unknown%2C%20black-box%20environments%20presents%20an%20even%20greater%20safety%0Arisk.%20We%20introduce%20ADVICE%20%28Adaptive%20Shielding%20with%20a%20Contrastive%20Autoencoder%29%2C%0Aa%20novel%20post-shielding%20technique%20that%20distinguishes%20safe%20and%20unsafe%20features%20of%0Astate-action%20pairs%20during%20training%2C%20and%20uses%20this%20knowledge%20to%20protect%20the%20RL%0Aagent%20from%20executing%20actions%20that%20yield%20likely%20hazardous%20outcomes.%20Our%0Acomprehensive%20experimental%20evaluation%20against%20state-of-the-art%20safe%20RL%0Aexploration%20techniques%20shows%20that%20ADVICE%20significantly%20reduces%20safety%0Aviolations%20%28approx%2050%25%29%20during%20training%2C%20with%20a%20competitive%20outcome%20reward%0Acompared%20to%20other%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18180v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Reinforcement%2520Learning%2520in%2520Black-Box%2520Environments%2520via%2520Adaptive%250A%2520%2520Shielding%26entry.906535625%3DDaniel%2520Bethell%2520and%2520Simos%2520Gerasimou%2520and%2520Radu%2520Calinescu%2520and%2520Calum%2520Imrie%26entry.1292438233%3D%2520%2520Empowering%2520safe%2520exploration%2520of%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%2520during%250Atraining%2520is%2520a%2520critical%2520challenge%2520towards%2520their%2520deployment%2520in%2520many%2520real-world%250Ascenarios.%2520When%2520prior%2520knowledge%2520of%2520the%2520domain%2520or%2520task%2520is%2520unavailable%252C%2520training%250ARL%2520agents%2520in%2520unknown%252C%2520black-box%2520environments%2520presents%2520an%2520even%2520greater%2520safety%250Arisk.%2520We%2520introduce%2520ADVICE%2520%2528Adaptive%2520Shielding%2520with%2520a%2520Contrastive%2520Autoencoder%2529%252C%250Aa%2520novel%2520post-shielding%2520technique%2520that%2520distinguishes%2520safe%2520and%2520unsafe%2520features%2520of%250Astate-action%2520pairs%2520during%2520training%252C%2520and%2520uses%2520this%2520knowledge%2520to%2520protect%2520the%2520RL%250Aagent%2520from%2520executing%2520actions%2520that%2520yield%2520likely%2520hazardous%2520outcomes.%2520Our%250Acomprehensive%2520experimental%2520evaluation%2520against%2520state-of-the-art%2520safe%2520RL%250Aexploration%2520techniques%2520shows%2520that%2520ADVICE%2520significantly%2520reduces%2520safety%250Aviolations%2520%2528approx%252050%2525%2529%2520during%2520training%252C%2520with%2520a%2520competitive%2520outcome%2520reward%250Acompared%2520to%2520other%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18180v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Reinforcement%20Learning%20in%20Black-Box%20Environments%20via%20Adaptive%0A%20%20Shielding&entry.906535625=Daniel%20Bethell%20and%20Simos%20Gerasimou%20and%20Radu%20Calinescu%20and%20Calum%20Imrie&entry.1292438233=%20%20Empowering%20safe%20exploration%20of%20reinforcement%20learning%20%28RL%29%20agents%20during%0Atraining%20is%20a%20critical%20challenge%20towards%20their%20deployment%20in%20many%20real-world%0Ascenarios.%20When%20prior%20knowledge%20of%20the%20domain%20or%20task%20is%20unavailable%2C%20training%0ARL%20agents%20in%20unknown%2C%20black-box%20environments%20presents%20an%20even%20greater%20safety%0Arisk.%20We%20introduce%20ADVICE%20%28Adaptive%20Shielding%20with%20a%20Contrastive%20Autoencoder%29%2C%0Aa%20novel%20post-shielding%20technique%20that%20distinguishes%20safe%20and%20unsafe%20features%20of%0Astate-action%20pairs%20during%20training%2C%20and%20uses%20this%20knowledge%20to%20protect%20the%20RL%0Aagent%20from%20executing%20actions%20that%20yield%20likely%20hazardous%20outcomes.%20Our%0Acomprehensive%20experimental%20evaluation%20against%20state-of-the-art%20safe%20RL%0Aexploration%20techniques%20shows%20that%20ADVICE%20significantly%20reduces%20safety%0Aviolations%20%28approx%2050%25%29%20during%20training%2C%20with%20a%20competitive%20outcome%20reward%0Acompared%20to%20other%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18180v3&entry.124074799=Read"},
{"title": "STDiff: A State Transition Diffusion Framework for Time Series\n  Imputation in Industrial Systems", "author": "Gary Simethy and Daniel Ortiz-Arroyo and Petar Durdevic", "abstract": "  Most deep learning methods for imputing missing values treat the task as\ncompleting patterns within a fixed time window. This assumption often fails in\nindustrial systems, where dynamics are driven by control actions, are highly\nnon-stationary, and can experience long, uninterrupted gaps. We propose STDiff,\nwhich reframes imputation as learning how the system evolves from one state to\nthe next. STDiff uses a conditional denoising diffusion model with a causal\nbias aligned to control theory, generating missing values step-by-step based on\nthe most recent known state and relevant control or environmental inputs. On a\npublic wastewater treatment dataset with simulated missing blocks, STDiff\nconsistently achieves the lowest errors, with its advantage increasing for\nlonger gaps. On a raw industrial dataset with substantial real gaps, it\nproduces trajectories that remain dynamically plausible, in contrast to\nwindow-based models that tend to flatten or over-smooth. These results support\ndynamics-aware, explicitly conditioned imputation as a robust approach for\nindustrial time series, and we discuss computational trade-offs and extensions\nto broader domains.\n", "link": "http://arxiv.org/abs/2508.19011v1", "date": "2025-08-26", "relevancy": 1.0536, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5552}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5135}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STDiff%3A%20A%20State%20Transition%20Diffusion%20Framework%20for%20Time%20Series%0A%20%20Imputation%20in%20Industrial%20Systems&body=Title%3A%20STDiff%3A%20A%20State%20Transition%20Diffusion%20Framework%20for%20Time%20Series%0A%20%20Imputation%20in%20Industrial%20Systems%0AAuthor%3A%20Gary%20Simethy%20and%20Daniel%20Ortiz-Arroyo%20and%20Petar%20Durdevic%0AAbstract%3A%20%20%20Most%20deep%20learning%20methods%20for%20imputing%20missing%20values%20treat%20the%20task%20as%0Acompleting%20patterns%20within%20a%20fixed%20time%20window.%20This%20assumption%20often%20fails%20in%0Aindustrial%20systems%2C%20where%20dynamics%20are%20driven%20by%20control%20actions%2C%20are%20highly%0Anon-stationary%2C%20and%20can%20experience%20long%2C%20uninterrupted%20gaps.%20We%20propose%20STDiff%2C%0Awhich%20reframes%20imputation%20as%20learning%20how%20the%20system%20evolves%20from%20one%20state%20to%0Athe%20next.%20STDiff%20uses%20a%20conditional%20denoising%20diffusion%20model%20with%20a%20causal%0Abias%20aligned%20to%20control%20theory%2C%20generating%20missing%20values%20step-by-step%20based%20on%0Athe%20most%20recent%20known%20state%20and%20relevant%20control%20or%20environmental%20inputs.%20On%20a%0Apublic%20wastewater%20treatment%20dataset%20with%20simulated%20missing%20blocks%2C%20STDiff%0Aconsistently%20achieves%20the%20lowest%20errors%2C%20with%20its%20advantage%20increasing%20for%0Alonger%20gaps.%20On%20a%20raw%20industrial%20dataset%20with%20substantial%20real%20gaps%2C%20it%0Aproduces%20trajectories%20that%20remain%20dynamically%20plausible%2C%20in%20contrast%20to%0Awindow-based%20models%20that%20tend%20to%20flatten%20or%20over-smooth.%20These%20results%20support%0Adynamics-aware%2C%20explicitly%20conditioned%20imputation%20as%20a%20robust%20approach%20for%0Aindustrial%20time%20series%2C%20and%20we%20discuss%20computational%20trade-offs%20and%20extensions%0Ato%20broader%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTDiff%253A%2520A%2520State%2520Transition%2520Diffusion%2520Framework%2520for%2520Time%2520Series%250A%2520%2520Imputation%2520in%2520Industrial%2520Systems%26entry.906535625%3DGary%2520Simethy%2520and%2520Daniel%2520Ortiz-Arroyo%2520and%2520Petar%2520Durdevic%26entry.1292438233%3D%2520%2520Most%2520deep%2520learning%2520methods%2520for%2520imputing%2520missing%2520values%2520treat%2520the%2520task%2520as%250Acompleting%2520patterns%2520within%2520a%2520fixed%2520time%2520window.%2520This%2520assumption%2520often%2520fails%2520in%250Aindustrial%2520systems%252C%2520where%2520dynamics%2520are%2520driven%2520by%2520control%2520actions%252C%2520are%2520highly%250Anon-stationary%252C%2520and%2520can%2520experience%2520long%252C%2520uninterrupted%2520gaps.%2520We%2520propose%2520STDiff%252C%250Awhich%2520reframes%2520imputation%2520as%2520learning%2520how%2520the%2520system%2520evolves%2520from%2520one%2520state%2520to%250Athe%2520next.%2520STDiff%2520uses%2520a%2520conditional%2520denoising%2520diffusion%2520model%2520with%2520a%2520causal%250Abias%2520aligned%2520to%2520control%2520theory%252C%2520generating%2520missing%2520values%2520step-by-step%2520based%2520on%250Athe%2520most%2520recent%2520known%2520state%2520and%2520relevant%2520control%2520or%2520environmental%2520inputs.%2520On%2520a%250Apublic%2520wastewater%2520treatment%2520dataset%2520with%2520simulated%2520missing%2520blocks%252C%2520STDiff%250Aconsistently%2520achieves%2520the%2520lowest%2520errors%252C%2520with%2520its%2520advantage%2520increasing%2520for%250Alonger%2520gaps.%2520On%2520a%2520raw%2520industrial%2520dataset%2520with%2520substantial%2520real%2520gaps%252C%2520it%250Aproduces%2520trajectories%2520that%2520remain%2520dynamically%2520plausible%252C%2520in%2520contrast%2520to%250Awindow-based%2520models%2520that%2520tend%2520to%2520flatten%2520or%2520over-smooth.%2520These%2520results%2520support%250Adynamics-aware%252C%2520explicitly%2520conditioned%2520imputation%2520as%2520a%2520robust%2520approach%2520for%250Aindustrial%2520time%2520series%252C%2520and%2520we%2520discuss%2520computational%2520trade-offs%2520and%2520extensions%250Ato%2520broader%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STDiff%3A%20A%20State%20Transition%20Diffusion%20Framework%20for%20Time%20Series%0A%20%20Imputation%20in%20Industrial%20Systems&entry.906535625=Gary%20Simethy%20and%20Daniel%20Ortiz-Arroyo%20and%20Petar%20Durdevic&entry.1292438233=%20%20Most%20deep%20learning%20methods%20for%20imputing%20missing%20values%20treat%20the%20task%20as%0Acompleting%20patterns%20within%20a%20fixed%20time%20window.%20This%20assumption%20often%20fails%20in%0Aindustrial%20systems%2C%20where%20dynamics%20are%20driven%20by%20control%20actions%2C%20are%20highly%0Anon-stationary%2C%20and%20can%20experience%20long%2C%20uninterrupted%20gaps.%20We%20propose%20STDiff%2C%0Awhich%20reframes%20imputation%20as%20learning%20how%20the%20system%20evolves%20from%20one%20state%20to%0Athe%20next.%20STDiff%20uses%20a%20conditional%20denoising%20diffusion%20model%20with%20a%20causal%0Abias%20aligned%20to%20control%20theory%2C%20generating%20missing%20values%20step-by-step%20based%20on%0Athe%20most%20recent%20known%20state%20and%20relevant%20control%20or%20environmental%20inputs.%20On%20a%0Apublic%20wastewater%20treatment%20dataset%20with%20simulated%20missing%20blocks%2C%20STDiff%0Aconsistently%20achieves%20the%20lowest%20errors%2C%20with%20its%20advantage%20increasing%20for%0Alonger%20gaps.%20On%20a%20raw%20industrial%20dataset%20with%20substantial%20real%20gaps%2C%20it%0Aproduces%20trajectories%20that%20remain%20dynamically%20plausible%2C%20in%20contrast%20to%0Awindow-based%20models%20that%20tend%20to%20flatten%20or%20over-smooth.%20These%20results%20support%0Adynamics-aware%2C%20explicitly%20conditioned%20imputation%20as%20a%20robust%20approach%20for%0Aindustrial%20time%20series%2C%20and%20we%20discuss%20computational%20trade-offs%20and%20extensions%0Ato%20broader%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19011v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


