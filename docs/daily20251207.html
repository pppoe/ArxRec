<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251204.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS", "author": "Chuanyu Fu and Guanying Chen and Yuqi Zhang and Kunbin Yao and Yuan Xiong and Chuan Huang and Shuguang Cui and Yasuyuki Matsushita and Xiaochun Cao", "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.", "link": "http://arxiv.org/abs/2512.04815v1", "date": "2025-12-04", "relevancy": 3.485, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7256}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6898}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobustSplat%2B%2B%3A%20Decoupling%20Densification%2C%20Dynamics%2C%20and%20Illumination%20for%20In-the-Wild%203DGS&body=Title%3A%20RobustSplat%2B%2B%3A%20Decoupling%20Densification%2C%20Dynamics%2C%20and%20Illumination%20for%20In-the-Wild%203DGS%0AAuthor%3A%20Chuanyu%20Fu%20and%20Guanying%20Chen%20and%20Yuqi%20Zhang%20and%20Kunbin%20Yao%20and%20Yuan%20Xiong%20and%20Chuan%20Huang%20and%20Shuguang%20Cui%20and%20Yasuyuki%20Matsushita%20and%20Xiaochun%20Cao%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20gained%20significant%20attention%20for%20its%20real-time%2C%20photo-realistic%20rendering%20in%20novel-view%20synthesis%20and%203D%20modeling.%20However%2C%20existing%20methods%20struggle%20with%20accurately%20modeling%20in-the-wild%20scenes%20affected%20by%20transient%20objects%20and%20illuminations%2C%20leading%20to%20artifacts%20in%20the%20rendered%20images.%20We%20identify%20that%20the%20Gaussian%20densification%20process%2C%20while%20enhancing%20scene%20detail%20capture%2C%20unintentionally%20contributes%20to%20these%20artifacts%20by%20growing%20additional%20Gaussians%20that%20model%20transient%20disturbances%20and%20illumination%20variations.%20To%20address%20this%2C%20we%20propose%20RobustSplat%2B%2B%2C%20a%20robust%20solution%20based%20on%20several%20critical%20designs.%20First%2C%20we%20introduce%20a%20delayed%20Gaussian%20growth%20strategy%20that%20prioritizes%20optimizing%20static%20scene%20structure%20before%20allowing%20Gaussian%20splitting/cloning%2C%20mitigating%20overfitting%20to%20transient%20objects%20in%20early%20optimization.%20Second%2C%20we%20design%20a%20scale-cascaded%20mask%20bootstrapping%20approach%20that%20first%20leverages%20lower-resolution%20feature%20similarity%20supervision%20for%20reliable%20initial%20transient%20mask%20estimation%2C%20taking%20advantage%20of%20its%20stronger%20semantic%20consistency%20and%20robustness%20to%20noise%2C%20and%20then%20progresses%20to%20high-resolution%20supervision%20to%20achieve%20more%20precise%20mask%20prediction.%20Third%2C%20we%20incorporate%20the%20delayed%20Gaussian%20growth%20strategy%20and%20mask%20bootstrapping%20with%20appearance%20modeling%20to%20handling%20in-the-wild%20scenes%20including%20transients%20and%20illuminations.%20Extensive%20experiments%20on%20multiple%20challenging%20datasets%20show%20that%20our%20method%20outperforms%20existing%20methods%2C%20clearly%20demonstrating%20the%20robustness%20and%20effectiveness%20of%20our%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustSplat%252B%252B%253A%2520Decoupling%2520Densification%252C%2520Dynamics%252C%2520and%2520Illumination%2520for%2520In-the-Wild%25203DGS%26entry.906535625%3DChuanyu%2520Fu%2520and%2520Guanying%2520Chen%2520and%2520Yuqi%2520Zhang%2520and%2520Kunbin%2520Yao%2520and%2520Yuan%2520Xiong%2520and%2520Chuan%2520Huang%2520and%2520Shuguang%2520Cui%2520and%2520Yasuyuki%2520Matsushita%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520gained%2520significant%2520attention%2520for%2520its%2520real-time%252C%2520photo-realistic%2520rendering%2520in%2520novel-view%2520synthesis%2520and%25203D%2520modeling.%2520However%252C%2520existing%2520methods%2520struggle%2520with%2520accurately%2520modeling%2520in-the-wild%2520scenes%2520affected%2520by%2520transient%2520objects%2520and%2520illuminations%252C%2520leading%2520to%2520artifacts%2520in%2520the%2520rendered%2520images.%2520We%2520identify%2520that%2520the%2520Gaussian%2520densification%2520process%252C%2520while%2520enhancing%2520scene%2520detail%2520capture%252C%2520unintentionally%2520contributes%2520to%2520these%2520artifacts%2520by%2520growing%2520additional%2520Gaussians%2520that%2520model%2520transient%2520disturbances%2520and%2520illumination%2520variations.%2520To%2520address%2520this%252C%2520we%2520propose%2520RobustSplat%252B%252B%252C%2520a%2520robust%2520solution%2520based%2520on%2520several%2520critical%2520designs.%2520First%252C%2520we%2520introduce%2520a%2520delayed%2520Gaussian%2520growth%2520strategy%2520that%2520prioritizes%2520optimizing%2520static%2520scene%2520structure%2520before%2520allowing%2520Gaussian%2520splitting/cloning%252C%2520mitigating%2520overfitting%2520to%2520transient%2520objects%2520in%2520early%2520optimization.%2520Second%252C%2520we%2520design%2520a%2520scale-cascaded%2520mask%2520bootstrapping%2520approach%2520that%2520first%2520leverages%2520lower-resolution%2520feature%2520similarity%2520supervision%2520for%2520reliable%2520initial%2520transient%2520mask%2520estimation%252C%2520taking%2520advantage%2520of%2520its%2520stronger%2520semantic%2520consistency%2520and%2520robustness%2520to%2520noise%252C%2520and%2520then%2520progresses%2520to%2520high-resolution%2520supervision%2520to%2520achieve%2520more%2520precise%2520mask%2520prediction.%2520Third%252C%2520we%2520incorporate%2520the%2520delayed%2520Gaussian%2520growth%2520strategy%2520and%2520mask%2520bootstrapping%2520with%2520appearance%2520modeling%2520to%2520handling%2520in-the-wild%2520scenes%2520including%2520transients%2520and%2520illuminations.%2520Extensive%2520experiments%2520on%2520multiple%2520challenging%2520datasets%2520show%2520that%2520our%2520method%2520outperforms%2520existing%2520methods%252C%2520clearly%2520demonstrating%2520the%2520robustness%2520and%2520effectiveness%2520of%2520our%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobustSplat%2B%2B%3A%20Decoupling%20Densification%2C%20Dynamics%2C%20and%20Illumination%20for%20In-the-Wild%203DGS&entry.906535625=Chuanyu%20Fu%20and%20Guanying%20Chen%20and%20Yuqi%20Zhang%20and%20Kunbin%20Yao%20and%20Yuan%20Xiong%20and%20Chuan%20Huang%20and%20Shuguang%20Cui%20and%20Yasuyuki%20Matsushita%20and%20Xiaochun%20Cao&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20has%20gained%20significant%20attention%20for%20its%20real-time%2C%20photo-realistic%20rendering%20in%20novel-view%20synthesis%20and%203D%20modeling.%20However%2C%20existing%20methods%20struggle%20with%20accurately%20modeling%20in-the-wild%20scenes%20affected%20by%20transient%20objects%20and%20illuminations%2C%20leading%20to%20artifacts%20in%20the%20rendered%20images.%20We%20identify%20that%20the%20Gaussian%20densification%20process%2C%20while%20enhancing%20scene%20detail%20capture%2C%20unintentionally%20contributes%20to%20these%20artifacts%20by%20growing%20additional%20Gaussians%20that%20model%20transient%20disturbances%20and%20illumination%20variations.%20To%20address%20this%2C%20we%20propose%20RobustSplat%2B%2B%2C%20a%20robust%20solution%20based%20on%20several%20critical%20designs.%20First%2C%20we%20introduce%20a%20delayed%20Gaussian%20growth%20strategy%20that%20prioritizes%20optimizing%20static%20scene%20structure%20before%20allowing%20Gaussian%20splitting/cloning%2C%20mitigating%20overfitting%20to%20transient%20objects%20in%20early%20optimization.%20Second%2C%20we%20design%20a%20scale-cascaded%20mask%20bootstrapping%20approach%20that%20first%20leverages%20lower-resolution%20feature%20similarity%20supervision%20for%20reliable%20initial%20transient%20mask%20estimation%2C%20taking%20advantage%20of%20its%20stronger%20semantic%20consistency%20and%20robustness%20to%20noise%2C%20and%20then%20progresses%20to%20high-resolution%20supervision%20to%20achieve%20more%20precise%20mask%20prediction.%20Third%2C%20we%20incorporate%20the%20delayed%20Gaussian%20growth%20strategy%20and%20mask%20bootstrapping%20with%20appearance%20modeling%20to%20handling%20in-the-wild%20scenes%20including%20transients%20and%20illuminations.%20Extensive%20experiments%20on%20multiple%20challenging%20datasets%20show%20that%20our%20method%20outperforms%20existing%20methods%2C%20clearly%20demonstrating%20the%20robustness%20and%20effectiveness%20of%20our%20method.&entry.1838667208=http%3A//arxiv.org/abs/2512.04815v1&entry.124074799=Read"},
{"title": "Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields", "author": "Ankit Dhiman and Tao Lu and R Srinath and Emre Arslan and Angela Xing and Yuanbo Xiangli and R Venkatesh Babu and Srinath Sridhar", "abstract": "Novel-view synthesis is an important problem in computer vision with applications in 3D reconstruction, mixed reality, and robotics. Recent methods like 3D Gaussian Splatting (3DGS) have become the preferred method for this task, providing high-quality novel views in real time. However, the training time of a 3DGS model is slow, often taking 30 minutes for a scene with 200 views. In contrast, our goal is to reduce the optimization time by training for fewer steps while maintaining high rendering quality. Specifically, we combine the guidance from both the position error and the appearance error to achieve a more effective densification. To balance the rate between adding new Gaussians and fitting old Gaussians, we develop a convergence-aware budget control mechanism. Moreover, to make the densification process more reliable, we selectively add new Gaussians from mostly visited regions. With these designs, we reduce the Gaussian optimization steps to one-third of the previous approach while achieving a comparable or even better novel view rendering quality. To further facilitate the rapid fitting of 4K resolution images, we introduce a dilation-based rendering technique. Our method, Turbo-GS, speeds up optimization for typical scenes and scales well to high-resolution (4K) scenarios on standard datasets. Through extensive experiments, we show that our method is significantly faster in optimization than other methods while retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.", "link": "http://arxiv.org/abs/2412.13547v2", "date": "2025-12-04", "relevancy": 3.4108, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7212}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6674}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Turbo-GS%3A%20Accelerating%203D%20Gaussian%20Fitting%20for%20High-Quality%20Radiance%20Fields&body=Title%3A%20Turbo-GS%3A%20Accelerating%203D%20Gaussian%20Fitting%20for%20High-Quality%20Radiance%20Fields%0AAuthor%3A%20Ankit%20Dhiman%20and%20Tao%20Lu%20and%20R%20Srinath%20and%20Emre%20Arslan%20and%20Angela%20Xing%20and%20Yuanbo%20Xiangli%20and%20R%20Venkatesh%20Babu%20and%20Srinath%20Sridhar%0AAbstract%3A%20Novel-view%20synthesis%20is%20an%20important%20problem%20in%20computer%20vision%20with%20applications%20in%203D%20reconstruction%2C%20mixed%20reality%2C%20and%20robotics.%20Recent%20methods%20like%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20become%20the%20preferred%20method%20for%20this%20task%2C%20providing%20high-quality%20novel%20views%20in%20real%20time.%20However%2C%20the%20training%20time%20of%20a%203DGS%20model%20is%20slow%2C%20often%20taking%2030%20minutes%20for%20a%20scene%20with%20200%20views.%20In%20contrast%2C%20our%20goal%20is%20to%20reduce%20the%20optimization%20time%20by%20training%20for%20fewer%20steps%20while%20maintaining%20high%20rendering%20quality.%20Specifically%2C%20we%20combine%20the%20guidance%20from%20both%20the%20position%20error%20and%20the%20appearance%20error%20to%20achieve%20a%20more%20effective%20densification.%20To%20balance%20the%20rate%20between%20adding%20new%20Gaussians%20and%20fitting%20old%20Gaussians%2C%20we%20develop%20a%20convergence-aware%20budget%20control%20mechanism.%20Moreover%2C%20to%20make%20the%20densification%20process%20more%20reliable%2C%20we%20selectively%20add%20new%20Gaussians%20from%20mostly%20visited%20regions.%20With%20these%20designs%2C%20we%20reduce%20the%20Gaussian%20optimization%20steps%20to%20one-third%20of%20the%20previous%20approach%20while%20achieving%20a%20comparable%20or%20even%20better%20novel%20view%20rendering%20quality.%20To%20further%20facilitate%20the%20rapid%20fitting%20of%204K%20resolution%20images%2C%20we%20introduce%20a%20dilation-based%20rendering%20technique.%20Our%20method%2C%20Turbo-GS%2C%20speeds%20up%20optimization%20for%20typical%20scenes%20and%20scales%20well%20to%20high-resolution%20%284K%29%20scenarios%20on%20standard%20datasets.%20Through%20extensive%20experiments%2C%20we%20show%20that%20our%20method%20is%20significantly%20faster%20in%20optimization%20than%20other%20methods%20while%20retaining%20quality.%20Project%20page%3A%20https%3A//ivl.cs.brown.edu/research/turbo-gs.%0ALink%3A%20http%3A//arxiv.org/abs/2412.13547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurbo-GS%253A%2520Accelerating%25203D%2520Gaussian%2520Fitting%2520for%2520High-Quality%2520Radiance%2520Fields%26entry.906535625%3DAnkit%2520Dhiman%2520and%2520Tao%2520Lu%2520and%2520R%2520Srinath%2520and%2520Emre%2520Arslan%2520and%2520Angela%2520Xing%2520and%2520Yuanbo%2520Xiangli%2520and%2520R%2520Venkatesh%2520Babu%2520and%2520Srinath%2520Sridhar%26entry.1292438233%3DNovel-view%2520synthesis%2520is%2520an%2520important%2520problem%2520in%2520computer%2520vision%2520with%2520applications%2520in%25203D%2520reconstruction%252C%2520mixed%2520reality%252C%2520and%2520robotics.%2520Recent%2520methods%2520like%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520become%2520the%2520preferred%2520method%2520for%2520this%2520task%252C%2520providing%2520high-quality%2520novel%2520views%2520in%2520real%2520time.%2520However%252C%2520the%2520training%2520time%2520of%2520a%25203DGS%2520model%2520is%2520slow%252C%2520often%2520taking%252030%2520minutes%2520for%2520a%2520scene%2520with%2520200%2520views.%2520In%2520contrast%252C%2520our%2520goal%2520is%2520to%2520reduce%2520the%2520optimization%2520time%2520by%2520training%2520for%2520fewer%2520steps%2520while%2520maintaining%2520high%2520rendering%2520quality.%2520Specifically%252C%2520we%2520combine%2520the%2520guidance%2520from%2520both%2520the%2520position%2520error%2520and%2520the%2520appearance%2520error%2520to%2520achieve%2520a%2520more%2520effective%2520densification.%2520To%2520balance%2520the%2520rate%2520between%2520adding%2520new%2520Gaussians%2520and%2520fitting%2520old%2520Gaussians%252C%2520we%2520develop%2520a%2520convergence-aware%2520budget%2520control%2520mechanism.%2520Moreover%252C%2520to%2520make%2520the%2520densification%2520process%2520more%2520reliable%252C%2520we%2520selectively%2520add%2520new%2520Gaussians%2520from%2520mostly%2520visited%2520regions.%2520With%2520these%2520designs%252C%2520we%2520reduce%2520the%2520Gaussian%2520optimization%2520steps%2520to%2520one-third%2520of%2520the%2520previous%2520approach%2520while%2520achieving%2520a%2520comparable%2520or%2520even%2520better%2520novel%2520view%2520rendering%2520quality.%2520To%2520further%2520facilitate%2520the%2520rapid%2520fitting%2520of%25204K%2520resolution%2520images%252C%2520we%2520introduce%2520a%2520dilation-based%2520rendering%2520technique.%2520Our%2520method%252C%2520Turbo-GS%252C%2520speeds%2520up%2520optimization%2520for%2520typical%2520scenes%2520and%2520scales%2520well%2520to%2520high-resolution%2520%25284K%2529%2520scenarios%2520on%2520standard%2520datasets.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520our%2520method%2520is%2520significantly%2520faster%2520in%2520optimization%2520than%2520other%2520methods%2520while%2520retaining%2520quality.%2520Project%2520page%253A%2520https%253A//ivl.cs.brown.edu/research/turbo-gs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Turbo-GS%3A%20Accelerating%203D%20Gaussian%20Fitting%20for%20High-Quality%20Radiance%20Fields&entry.906535625=Ankit%20Dhiman%20and%20Tao%20Lu%20and%20R%20Srinath%20and%20Emre%20Arslan%20and%20Angela%20Xing%20and%20Yuanbo%20Xiangli%20and%20R%20Venkatesh%20Babu%20and%20Srinath%20Sridhar&entry.1292438233=Novel-view%20synthesis%20is%20an%20important%20problem%20in%20computer%20vision%20with%20applications%20in%203D%20reconstruction%2C%20mixed%20reality%2C%20and%20robotics.%20Recent%20methods%20like%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20become%20the%20preferred%20method%20for%20this%20task%2C%20providing%20high-quality%20novel%20views%20in%20real%20time.%20However%2C%20the%20training%20time%20of%20a%203DGS%20model%20is%20slow%2C%20often%20taking%2030%20minutes%20for%20a%20scene%20with%20200%20views.%20In%20contrast%2C%20our%20goal%20is%20to%20reduce%20the%20optimization%20time%20by%20training%20for%20fewer%20steps%20while%20maintaining%20high%20rendering%20quality.%20Specifically%2C%20we%20combine%20the%20guidance%20from%20both%20the%20position%20error%20and%20the%20appearance%20error%20to%20achieve%20a%20more%20effective%20densification.%20To%20balance%20the%20rate%20between%20adding%20new%20Gaussians%20and%20fitting%20old%20Gaussians%2C%20we%20develop%20a%20convergence-aware%20budget%20control%20mechanism.%20Moreover%2C%20to%20make%20the%20densification%20process%20more%20reliable%2C%20we%20selectively%20add%20new%20Gaussians%20from%20mostly%20visited%20regions.%20With%20these%20designs%2C%20we%20reduce%20the%20Gaussian%20optimization%20steps%20to%20one-third%20of%20the%20previous%20approach%20while%20achieving%20a%20comparable%20or%20even%20better%20novel%20view%20rendering%20quality.%20To%20further%20facilitate%20the%20rapid%20fitting%20of%204K%20resolution%20images%2C%20we%20introduce%20a%20dilation-based%20rendering%20technique.%20Our%20method%2C%20Turbo-GS%2C%20speeds%20up%20optimization%20for%20typical%20scenes%20and%20scales%20well%20to%20high-resolution%20%284K%29%20scenarios%20on%20standard%20datasets.%20Through%20extensive%20experiments%2C%20we%20show%20that%20our%20method%20is%20significantly%20faster%20in%20optimization%20than%20other%20methods%20while%20retaining%20quality.%20Project%20page%3A%20https%3A//ivl.cs.brown.edu/research/turbo-gs.&entry.1838667208=http%3A//arxiv.org/abs/2412.13547v2&entry.124074799=Read"},
{"title": "From Generated Human Videos to Physically Plausible Robot Trajectories", "author": "James Ni and Zekai Wang and Wei Lin and Amir Bar and Yann LeCun and Trevor Darrell and Jitendra Malik and Roei Herzig", "abstract": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.", "link": "http://arxiv.org/abs/2512.05094v1", "date": "2025-12-04", "relevancy": 3.3171, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6827}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6691}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Generated%20Human%20Videos%20to%20Physically%20Plausible%20Robot%20Trajectories&body=Title%3A%20From%20Generated%20Human%20Videos%20to%20Physically%20Plausible%20Robot%20Trajectories%0AAuthor%3A%20James%20Ni%20and%20Zekai%20Wang%20and%20Wei%20Lin%20and%20Amir%20Bar%20and%20Yann%20LeCun%20and%20Trevor%20Darrell%20and%20Jitendra%20Malik%20and%20Roei%20Herzig%0AAbstract%3A%20Video%20generation%20models%20are%20rapidly%20improving%20in%20their%20ability%20to%20synthesize%20human%20actions%20in%20novel%20contexts%2C%20holding%20the%20potential%20to%20serve%20as%20high-level%20planners%20for%20contextual%20robot%20control.%20To%20realize%20this%20potential%2C%20a%20key%20research%20question%20remains%20open%3A%20how%20can%20a%20humanoid%20execute%20the%20human%20actions%20from%20generated%20videos%20in%20a%20zero-shot%20manner%3F%20This%20challenge%20arises%20because%20generated%20videos%20are%20often%20noisy%20and%20exhibit%20morphological%20distortions%20that%20make%20direct%20imitation%20difficult%20compared%20to%20real%20video.%20To%20address%20this%2C%20we%20introduce%20a%20two-stage%20pipeline.%20First%2C%20we%20lift%20video%20pixels%20into%20a%204D%20human%20representation%20and%20then%20retarget%20to%20the%20humanoid%20morphology.%20Second%2C%20we%20propose%20GenMimic-a%20physics-aware%20reinforcement%20learning%20policy%20conditioned%20on%203D%20keypoints%2C%20and%20trained%20with%20symmetry%20regularization%20and%20keypoint-weighted%20tracking%20rewards.%20As%20a%20result%2C%20GenMimic%20can%20mimic%20human%20actions%20from%20noisy%2C%20generated%20videos.%20We%20curate%20GenMimicBench%2C%20a%20synthetic%20human-motion%20dataset%20generated%20using%20two%20video%20generation%20models%20across%20a%20spectrum%20of%20actions%20and%20contexts%2C%20establishing%20a%20benchmark%20for%20assessing%20zero-shot%20generalization%20and%20policy%20robustness.%20Extensive%20experiments%20demonstrate%20improvements%20over%20strong%20baselines%20in%20simulation%20and%20confirm%20coherent%2C%20physically%20stable%20motion%20tracking%20on%20a%20Unitree%20G1%20humanoid%20robot%20without%20fine-tuning.%20This%20work%20offers%20a%20promising%20path%20to%20realizing%20the%20potential%20of%20video%20generation%20models%20as%20high-level%20policies%20for%20robot%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Generated%2520Human%2520Videos%2520to%2520Physically%2520Plausible%2520Robot%2520Trajectories%26entry.906535625%3DJames%2520Ni%2520and%2520Zekai%2520Wang%2520and%2520Wei%2520Lin%2520and%2520Amir%2520Bar%2520and%2520Yann%2520LeCun%2520and%2520Trevor%2520Darrell%2520and%2520Jitendra%2520Malik%2520and%2520Roei%2520Herzig%26entry.1292438233%3DVideo%2520generation%2520models%2520are%2520rapidly%2520improving%2520in%2520their%2520ability%2520to%2520synthesize%2520human%2520actions%2520in%2520novel%2520contexts%252C%2520holding%2520the%2520potential%2520to%2520serve%2520as%2520high-level%2520planners%2520for%2520contextual%2520robot%2520control.%2520To%2520realize%2520this%2520potential%252C%2520a%2520key%2520research%2520question%2520remains%2520open%253A%2520how%2520can%2520a%2520humanoid%2520execute%2520the%2520human%2520actions%2520from%2520generated%2520videos%2520in%2520a%2520zero-shot%2520manner%253F%2520This%2520challenge%2520arises%2520because%2520generated%2520videos%2520are%2520often%2520noisy%2520and%2520exhibit%2520morphological%2520distortions%2520that%2520make%2520direct%2520imitation%2520difficult%2520compared%2520to%2520real%2520video.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520two-stage%2520pipeline.%2520First%252C%2520we%2520lift%2520video%2520pixels%2520into%2520a%25204D%2520human%2520representation%2520and%2520then%2520retarget%2520to%2520the%2520humanoid%2520morphology.%2520Second%252C%2520we%2520propose%2520GenMimic-a%2520physics-aware%2520reinforcement%2520learning%2520policy%2520conditioned%2520on%25203D%2520keypoints%252C%2520and%2520trained%2520with%2520symmetry%2520regularization%2520and%2520keypoint-weighted%2520tracking%2520rewards.%2520As%2520a%2520result%252C%2520GenMimic%2520can%2520mimic%2520human%2520actions%2520from%2520noisy%252C%2520generated%2520videos.%2520We%2520curate%2520GenMimicBench%252C%2520a%2520synthetic%2520human-motion%2520dataset%2520generated%2520using%2520two%2520video%2520generation%2520models%2520across%2520a%2520spectrum%2520of%2520actions%2520and%2520contexts%252C%2520establishing%2520a%2520benchmark%2520for%2520assessing%2520zero-shot%2520generalization%2520and%2520policy%2520robustness.%2520Extensive%2520experiments%2520demonstrate%2520improvements%2520over%2520strong%2520baselines%2520in%2520simulation%2520and%2520confirm%2520coherent%252C%2520physically%2520stable%2520motion%2520tracking%2520on%2520a%2520Unitree%2520G1%2520humanoid%2520robot%2520without%2520fine-tuning.%2520This%2520work%2520offers%2520a%2520promising%2520path%2520to%2520realizing%2520the%2520potential%2520of%2520video%2520generation%2520models%2520as%2520high-level%2520policies%2520for%2520robot%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Generated%20Human%20Videos%20to%20Physically%20Plausible%20Robot%20Trajectories&entry.906535625=James%20Ni%20and%20Zekai%20Wang%20and%20Wei%20Lin%20and%20Amir%20Bar%20and%20Yann%20LeCun%20and%20Trevor%20Darrell%20and%20Jitendra%20Malik%20and%20Roei%20Herzig&entry.1292438233=Video%20generation%20models%20are%20rapidly%20improving%20in%20their%20ability%20to%20synthesize%20human%20actions%20in%20novel%20contexts%2C%20holding%20the%20potential%20to%20serve%20as%20high-level%20planners%20for%20contextual%20robot%20control.%20To%20realize%20this%20potential%2C%20a%20key%20research%20question%20remains%20open%3A%20how%20can%20a%20humanoid%20execute%20the%20human%20actions%20from%20generated%20videos%20in%20a%20zero-shot%20manner%3F%20This%20challenge%20arises%20because%20generated%20videos%20are%20often%20noisy%20and%20exhibit%20morphological%20distortions%20that%20make%20direct%20imitation%20difficult%20compared%20to%20real%20video.%20To%20address%20this%2C%20we%20introduce%20a%20two-stage%20pipeline.%20First%2C%20we%20lift%20video%20pixels%20into%20a%204D%20human%20representation%20and%20then%20retarget%20to%20the%20humanoid%20morphology.%20Second%2C%20we%20propose%20GenMimic-a%20physics-aware%20reinforcement%20learning%20policy%20conditioned%20on%203D%20keypoints%2C%20and%20trained%20with%20symmetry%20regularization%20and%20keypoint-weighted%20tracking%20rewards.%20As%20a%20result%2C%20GenMimic%20can%20mimic%20human%20actions%20from%20noisy%2C%20generated%20videos.%20We%20curate%20GenMimicBench%2C%20a%20synthetic%20human-motion%20dataset%20generated%20using%20two%20video%20generation%20models%20across%20a%20spectrum%20of%20actions%20and%20contexts%2C%20establishing%20a%20benchmark%20for%20assessing%20zero-shot%20generalization%20and%20policy%20robustness.%20Extensive%20experiments%20demonstrate%20improvements%20over%20strong%20baselines%20in%20simulation%20and%20confirm%20coherent%2C%20physically%20stable%20motion%20tracking%20on%20a%20Unitree%20G1%20humanoid%20robot%20without%20fine-tuning.%20This%20work%20offers%20a%20promising%20path%20to%20realizing%20the%20potential%20of%20video%20generation%20models%20as%20high-level%20policies%20for%20robot%20control.&entry.1838667208=http%3A//arxiv.org/abs/2512.05094v1&entry.124074799=Read"},
{"title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting", "author": "Hao-Jen Chien and Yi-Chuan Huang and Chung-Ho Wu and Wei-Lun Chao and Yu-Lun Liu", "abstract": "Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/", "link": "http://arxiv.org/abs/2512.05113v1", "date": "2025-12-04", "relevancy": 3.2973, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.691}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6568}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splannequin%3A%20Freezing%20Monocular%20Mannequin-Challenge%20Footage%20with%20Dual-Detection%20Splatting&body=Title%3A%20Splannequin%3A%20Freezing%20Monocular%20Mannequin-Challenge%20Footage%20with%20Dual-Detection%20Splatting%0AAuthor%3A%20Hao-Jen%20Chien%20and%20Yi-Chuan%20Huang%20and%20Chung-Ho%20Wu%20and%20Wei-Lun%20Chao%20and%20Yu-Lun%20Liu%0AAbstract%3A%20Synthesizing%20high-fidelity%20frozen%203D%20scenes%20from%20monocular%20Mannequin-Challenge%20%28MC%29%20videos%20is%20a%20unique%20problem%20distinct%20from%20standard%20dynamic%20scene%20reconstruction.%20Instead%20of%20focusing%20on%20modeling%20motion%2C%20our%20goal%20is%20to%20create%20a%20frozen%20scene%20while%20strategically%20preserving%20subtle%20dynamics%20to%20enable%20user-controlled%20instant%20selection.%20To%20achieve%20this%2C%20we%20introduce%20a%20novel%20application%20of%20dynamic%20Gaussian%20splatting%3A%20the%20scene%20is%20modeled%20dynamically%2C%20which%20retains%20nearby%20temporal%20variation%2C%20and%20a%20static%20scene%20is%20rendered%20by%20fixing%20the%20model%27s%20time%20parameter.%20However%2C%20under%20this%20usage%2C%20monocular%20capture%20with%20sparse%20temporal%20supervision%20introduces%20artifacts%20like%20ghosting%20and%20blur%20for%20Gaussians%20that%20become%20unobserved%20or%20occluded%20at%20weakly%20supervised%20timestamps.%20We%20propose%20Splannequin%2C%20an%20architecture-agnostic%20regularization%20that%20detects%20two%20states%20of%20Gaussian%20primitives%2C%20hidden%20and%20defective%2C%20and%20applies%20temporal%20anchoring.%20Under%20predominantly%20forward%20camera%20motion%2C%20hidden%20states%20are%20anchored%20to%20their%20recent%20well-observed%20past%20states%2C%20while%20defective%20states%20are%20anchored%20to%20future%20states%20with%20stronger%20supervision.%20Our%20method%20integrates%20into%20existing%20dynamic%20Gaussian%20pipelines%20via%20simple%20loss%20terms%2C%20requires%20no%20architectural%20changes%2C%20and%20adds%20zero%20inference%20overhead.%20This%20results%20in%20markedly%20improved%20visual%20quality%2C%20enabling%20high-fidelity%2C%20user-selectable%20frozen-time%20renderings%2C%20validated%20by%20a%2096%25%20user%20preference.%20Project%20page%3A%20https%3A//chien90190.github.io/splannequin/%0ALink%3A%20http%3A//arxiv.org/abs/2512.05113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplannequin%253A%2520Freezing%2520Monocular%2520Mannequin-Challenge%2520Footage%2520with%2520Dual-Detection%2520Splatting%26entry.906535625%3DHao-Jen%2520Chien%2520and%2520Yi-Chuan%2520Huang%2520and%2520Chung-Ho%2520Wu%2520and%2520Wei-Lun%2520Chao%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3DSynthesizing%2520high-fidelity%2520frozen%25203D%2520scenes%2520from%2520monocular%2520Mannequin-Challenge%2520%2528MC%2529%2520videos%2520is%2520a%2520unique%2520problem%2520distinct%2520from%2520standard%2520dynamic%2520scene%2520reconstruction.%2520Instead%2520of%2520focusing%2520on%2520modeling%2520motion%252C%2520our%2520goal%2520is%2520to%2520create%2520a%2520frozen%2520scene%2520while%2520strategically%2520preserving%2520subtle%2520dynamics%2520to%2520enable%2520user-controlled%2520instant%2520selection.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520novel%2520application%2520of%2520dynamic%2520Gaussian%2520splatting%253A%2520the%2520scene%2520is%2520modeled%2520dynamically%252C%2520which%2520retains%2520nearby%2520temporal%2520variation%252C%2520and%2520a%2520static%2520scene%2520is%2520rendered%2520by%2520fixing%2520the%2520model%2527s%2520time%2520parameter.%2520However%252C%2520under%2520this%2520usage%252C%2520monocular%2520capture%2520with%2520sparse%2520temporal%2520supervision%2520introduces%2520artifacts%2520like%2520ghosting%2520and%2520blur%2520for%2520Gaussians%2520that%2520become%2520unobserved%2520or%2520occluded%2520at%2520weakly%2520supervised%2520timestamps.%2520We%2520propose%2520Splannequin%252C%2520an%2520architecture-agnostic%2520regularization%2520that%2520detects%2520two%2520states%2520of%2520Gaussian%2520primitives%252C%2520hidden%2520and%2520defective%252C%2520and%2520applies%2520temporal%2520anchoring.%2520Under%2520predominantly%2520forward%2520camera%2520motion%252C%2520hidden%2520states%2520are%2520anchored%2520to%2520their%2520recent%2520well-observed%2520past%2520states%252C%2520while%2520defective%2520states%2520are%2520anchored%2520to%2520future%2520states%2520with%2520stronger%2520supervision.%2520Our%2520method%2520integrates%2520into%2520existing%2520dynamic%2520Gaussian%2520pipelines%2520via%2520simple%2520loss%2520terms%252C%2520requires%2520no%2520architectural%2520changes%252C%2520and%2520adds%2520zero%2520inference%2520overhead.%2520This%2520results%2520in%2520markedly%2520improved%2520visual%2520quality%252C%2520enabling%2520high-fidelity%252C%2520user-selectable%2520frozen-time%2520renderings%252C%2520validated%2520by%2520a%252096%2525%2520user%2520preference.%2520Project%2520page%253A%2520https%253A//chien90190.github.io/splannequin/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splannequin%3A%20Freezing%20Monocular%20Mannequin-Challenge%20Footage%20with%20Dual-Detection%20Splatting&entry.906535625=Hao-Jen%20Chien%20and%20Yi-Chuan%20Huang%20and%20Chung-Ho%20Wu%20and%20Wei-Lun%20Chao%20and%20Yu-Lun%20Liu&entry.1292438233=Synthesizing%20high-fidelity%20frozen%203D%20scenes%20from%20monocular%20Mannequin-Challenge%20%28MC%29%20videos%20is%20a%20unique%20problem%20distinct%20from%20standard%20dynamic%20scene%20reconstruction.%20Instead%20of%20focusing%20on%20modeling%20motion%2C%20our%20goal%20is%20to%20create%20a%20frozen%20scene%20while%20strategically%20preserving%20subtle%20dynamics%20to%20enable%20user-controlled%20instant%20selection.%20To%20achieve%20this%2C%20we%20introduce%20a%20novel%20application%20of%20dynamic%20Gaussian%20splatting%3A%20the%20scene%20is%20modeled%20dynamically%2C%20which%20retains%20nearby%20temporal%20variation%2C%20and%20a%20static%20scene%20is%20rendered%20by%20fixing%20the%20model%27s%20time%20parameter.%20However%2C%20under%20this%20usage%2C%20monocular%20capture%20with%20sparse%20temporal%20supervision%20introduces%20artifacts%20like%20ghosting%20and%20blur%20for%20Gaussians%20that%20become%20unobserved%20or%20occluded%20at%20weakly%20supervised%20timestamps.%20We%20propose%20Splannequin%2C%20an%20architecture-agnostic%20regularization%20that%20detects%20two%20states%20of%20Gaussian%20primitives%2C%20hidden%20and%20defective%2C%20and%20applies%20temporal%20anchoring.%20Under%20predominantly%20forward%20camera%20motion%2C%20hidden%20states%20are%20anchored%20to%20their%20recent%20well-observed%20past%20states%2C%20while%20defective%20states%20are%20anchored%20to%20future%20states%20with%20stronger%20supervision.%20Our%20method%20integrates%20into%20existing%20dynamic%20Gaussian%20pipelines%20via%20simple%20loss%20terms%2C%20requires%20no%20architectural%20changes%2C%20and%20adds%20zero%20inference%20overhead.%20This%20results%20in%20markedly%20improved%20visual%20quality%2C%20enabling%20high-fidelity%2C%20user-selectable%20frozen-time%20renderings%2C%20validated%20by%20a%2096%25%20user%20preference.%20Project%20page%3A%20https%3A//chien90190.github.io/splannequin/&entry.1838667208=http%3A//arxiv.org/abs/2512.05113v1&entry.124074799=Read"},
{"title": "SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models", "author": "Ruosen Zhao and Zhikang Zhang and Jialei Xu and Jiahao Chang and Dong Chen and Lingyun Li and Weijian Sun and Zizhuang Wei", "abstract": "Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.", "link": "http://arxiv.org/abs/2511.23075v2", "date": "2025-12-04", "relevancy": 3.0778, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaceMind%3A%20Camera-Guided%20Modality%20Fusion%20for%20Spatial%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20SpaceMind%3A%20Camera-Guided%20Modality%20Fusion%20for%20Spatial%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Ruosen%20Zhao%20and%20Zhikang%20Zhang%20and%20Jialei%20Xu%20and%20Jiahao%20Chang%20and%20Dong%20Chen%20and%20Lingyun%20Li%20and%20Weijian%20Sun%20and%20Zizhuang%20Wei%0AAbstract%3A%20Large%20vision-language%20models%20%28VLMs%29%20show%20strong%20multimodal%20understanding%20but%20still%20struggle%20with%203D%20spatial%20reasoning%2C%20such%20as%20distance%20estimation%2C%20size%20comparison%2C%20and%20cross-view%20consistency.%20Existing%203D-aware%20methods%20either%20depend%20on%20auxiliary%203D%20information%20or%20enhance%20RGB-only%20VLMs%20with%20geometry%20encoders%20through%20shallow%20feature%20fusion.%20We%20propose%20SpaceMind%2C%20a%20multimodal%20large%20language%20model%20explicitly%20designed%20for%20spatial%20reasoning%20solely%20from%20RGB%20inputs.%20The%20model%20adopts%20a%20dual-encoder%20architecture%2C%20integrating%20VGGT%20as%20a%20spatial%20understanding%20encoder%20and%20InternViT%20as%20a%202D%20visual%20encoder.%20The%20key%20idea%20is%20to%20treat%20the%20camera%20representation%20as%20an%20active%20guiding%20modality%20rather%20than%20passive%20metadata.%20Specifically%2C%20SpaceMind%20introduces%20a%20lightweight%20Camera-Guided%20Modality%20Fusion%20module%20before%20the%20language%20model%20to%20replace%20shallow%20fusion.%20It%20applies%20camera-conditioned%20biasing%20to%20spatial%20tokens%2C%20assigns%20query-independent%20weights%20reflecting%20their%20geometric%20importance%2C%20and%20uses%20the%20camera%20embedding%20to%20gate%20the%20fused%20representation.%20Empirically%2C%20SpaceMind%20establishes%20new%20state-of-the-art%20results%20on%20VSI-Bench%2C%20SQA3D%20and%20SPBench%2C%20surpassing%20both%20open%20and%20proprietary%20systems%20on%20VSI-Bench%20and%20SPBench%20by%20large%20margins%20and%20achieving%20state-of-the-art%20performance%20on%20SQA3D.%20These%20results%20demonstrate%20that%20camera-guided%20modality%20fusion%20is%20an%20effective%20and%20practical%20inductive%20bias%20for%20equipping%20VLMs%20with%20genuinely%20spatially%20grounded%20intelligence.%20We%20will%20release%20code%20and%20model%20checkpoints%20to%20support%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaceMind%253A%2520Camera-Guided%2520Modality%2520Fusion%2520for%2520Spatial%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DRuosen%2520Zhao%2520and%2520Zhikang%2520Zhang%2520and%2520Jialei%2520Xu%2520and%2520Jiahao%2520Chang%2520and%2520Dong%2520Chen%2520and%2520Lingyun%2520Li%2520and%2520Weijian%2520Sun%2520and%2520Zizhuang%2520Wei%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528VLMs%2529%2520show%2520strong%2520multimodal%2520understanding%2520but%2520still%2520struggle%2520with%25203D%2520spatial%2520reasoning%252C%2520such%2520as%2520distance%2520estimation%252C%2520size%2520comparison%252C%2520and%2520cross-view%2520consistency.%2520Existing%25203D-aware%2520methods%2520either%2520depend%2520on%2520auxiliary%25203D%2520information%2520or%2520enhance%2520RGB-only%2520VLMs%2520with%2520geometry%2520encoders%2520through%2520shallow%2520feature%2520fusion.%2520We%2520propose%2520SpaceMind%252C%2520a%2520multimodal%2520large%2520language%2520model%2520explicitly%2520designed%2520for%2520spatial%2520reasoning%2520solely%2520from%2520RGB%2520inputs.%2520The%2520model%2520adopts%2520a%2520dual-encoder%2520architecture%252C%2520integrating%2520VGGT%2520as%2520a%2520spatial%2520understanding%2520encoder%2520and%2520InternViT%2520as%2520a%25202D%2520visual%2520encoder.%2520The%2520key%2520idea%2520is%2520to%2520treat%2520the%2520camera%2520representation%2520as%2520an%2520active%2520guiding%2520modality%2520rather%2520than%2520passive%2520metadata.%2520Specifically%252C%2520SpaceMind%2520introduces%2520a%2520lightweight%2520Camera-Guided%2520Modality%2520Fusion%2520module%2520before%2520the%2520language%2520model%2520to%2520replace%2520shallow%2520fusion.%2520It%2520applies%2520camera-conditioned%2520biasing%2520to%2520spatial%2520tokens%252C%2520assigns%2520query-independent%2520weights%2520reflecting%2520their%2520geometric%2520importance%252C%2520and%2520uses%2520the%2520camera%2520embedding%2520to%2520gate%2520the%2520fused%2520representation.%2520Empirically%252C%2520SpaceMind%2520establishes%2520new%2520state-of-the-art%2520results%2520on%2520VSI-Bench%252C%2520SQA3D%2520and%2520SPBench%252C%2520surpassing%2520both%2520open%2520and%2520proprietary%2520systems%2520on%2520VSI-Bench%2520and%2520SPBench%2520by%2520large%2520margins%2520and%2520achieving%2520state-of-the-art%2520performance%2520on%2520SQA3D.%2520These%2520results%2520demonstrate%2520that%2520camera-guided%2520modality%2520fusion%2520is%2520an%2520effective%2520and%2520practical%2520inductive%2520bias%2520for%2520equipping%2520VLMs%2520with%2520genuinely%2520spatially%2520grounded%2520intelligence.%2520We%2520will%2520release%2520code%2520and%2520model%2520checkpoints%2520to%2520support%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaceMind%3A%20Camera-Guided%20Modality%20Fusion%20for%20Spatial%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Ruosen%20Zhao%20and%20Zhikang%20Zhang%20and%20Jialei%20Xu%20and%20Jiahao%20Chang%20and%20Dong%20Chen%20and%20Lingyun%20Li%20and%20Weijian%20Sun%20and%20Zizhuang%20Wei&entry.1292438233=Large%20vision-language%20models%20%28VLMs%29%20show%20strong%20multimodal%20understanding%20but%20still%20struggle%20with%203D%20spatial%20reasoning%2C%20such%20as%20distance%20estimation%2C%20size%20comparison%2C%20and%20cross-view%20consistency.%20Existing%203D-aware%20methods%20either%20depend%20on%20auxiliary%203D%20information%20or%20enhance%20RGB-only%20VLMs%20with%20geometry%20encoders%20through%20shallow%20feature%20fusion.%20We%20propose%20SpaceMind%2C%20a%20multimodal%20large%20language%20model%20explicitly%20designed%20for%20spatial%20reasoning%20solely%20from%20RGB%20inputs.%20The%20model%20adopts%20a%20dual-encoder%20architecture%2C%20integrating%20VGGT%20as%20a%20spatial%20understanding%20encoder%20and%20InternViT%20as%20a%202D%20visual%20encoder.%20The%20key%20idea%20is%20to%20treat%20the%20camera%20representation%20as%20an%20active%20guiding%20modality%20rather%20than%20passive%20metadata.%20Specifically%2C%20SpaceMind%20introduces%20a%20lightweight%20Camera-Guided%20Modality%20Fusion%20module%20before%20the%20language%20model%20to%20replace%20shallow%20fusion.%20It%20applies%20camera-conditioned%20biasing%20to%20spatial%20tokens%2C%20assigns%20query-independent%20weights%20reflecting%20their%20geometric%20importance%2C%20and%20uses%20the%20camera%20embedding%20to%20gate%20the%20fused%20representation.%20Empirically%2C%20SpaceMind%20establishes%20new%20state-of-the-art%20results%20on%20VSI-Bench%2C%20SQA3D%20and%20SPBench%2C%20surpassing%20both%20open%20and%20proprietary%20systems%20on%20VSI-Bench%20and%20SPBench%20by%20large%20margins%20and%20achieving%20state-of-the-art%20performance%20on%20SQA3D.%20These%20results%20demonstrate%20that%20camera-guided%20modality%20fusion%20is%20an%20effective%20and%20practical%20inductive%20bias%20for%20equipping%20VLMs%20with%20genuinely%20spatially%20grounded%20intelligence.%20We%20will%20release%20code%20and%20model%20checkpoints%20to%20support%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.23075v2&entry.124074799=Read"},
{"title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "author": "Yubo Huang and Hailong Guo and Fangtai Wu and Shifeng Zhang and Shijie Huang and Qijun Gan and Lin Liu and Sirui Zhao and Enhong Chen and Jiaming Liu and Steven Hoi", "abstract": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "link": "http://arxiv.org/abs/2512.04677v1", "date": "2025-12-04", "relevancy": 3.0095, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6061}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6021}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Live%20Avatar%3A%20Streaming%20Real-time%20Audio-Driven%20Avatar%20Generation%20with%20Infinite%20Length&body=Title%3A%20Live%20Avatar%3A%20Streaming%20Real-time%20Audio-Driven%20Avatar%20Generation%20with%20Infinite%20Length%0AAuthor%3A%20Yubo%20Huang%20and%20Hailong%20Guo%20and%20Fangtai%20Wu%20and%20Shifeng%20Zhang%20and%20Shijie%20Huang%20and%20Qijun%20Gan%20and%20Lin%20Liu%20and%20Sirui%20Zhao%20and%20Enhong%20Chen%20and%20Jiaming%20Liu%20and%20Steven%20Hoi%0AAbstract%3A%20Existing%20diffusion-based%20video%20generation%20methods%20are%20fundamentally%20constrained%20by%20sequential%20computation%20and%20long-horizon%20inconsistency%2C%20limiting%20their%20practical%20adoption%20in%20real-time%2C%20streaming%20audio-driven%20avatar%20synthesis.%20We%20present%20Live%20Avatar%2C%20an%20algorithm-system%20co-designed%20framework%20that%20enables%20efficient%2C%20high-fidelity%2C%20and%20infinite-length%20avatar%20generation%20using%20a%2014-billion-parameter%20diffusion%20model.%20Our%20approach%20introduces%20Timestep-forcing%20Pipeline%20Parallelism%20%28TPP%29%2C%20a%20distributed%20inference%20paradigm%20that%20pipelines%20denoising%20steps%20across%20multiple%20GPUs%2C%20effectively%20breaking%20the%20autoregressive%20bottleneck%20and%20ensuring%20stable%2C%20low-latency%20real-time%20streaming.%20To%20further%20enhance%20temporal%20consistency%20and%20mitigate%20identity%20drift%20and%20color%20artifacts%2C%20we%20propose%20the%20Rolling%20Sink%20Frame%20Mechanism%20%28RSFM%29%2C%20which%20maintains%20sequence%20fidelity%20by%20dynamically%20recalibrating%20appearance%20using%20a%20cached%20reference%20image.%20Additionally%2C%20we%20leverage%20Self-Forcing%20Distribution%20Matching%20Distillation%20to%20facilitate%20causal%2C%20streamable%20adaptation%20of%20large-scale%20models%20without%20sacrificing%20visual%20quality.%20Live%20Avatar%20demonstrates%20state-of-the-art%20performance%2C%20reaching%2020%20FPS%20end-to-end%20generation%20on%205%20H800%20GPUs%2C%20and%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%20to%20achieve%20practical%2C%20real-time%2C%20high-fidelity%20avatar%20generation%20at%20this%20scale.%20Our%20work%20establishes%20a%20new%20paradigm%20for%20deploying%20advanced%20diffusion%20models%20in%20industrial%20long-form%20video%20synthesis%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLive%2520Avatar%253A%2520Streaming%2520Real-time%2520Audio-Driven%2520Avatar%2520Generation%2520with%2520Infinite%2520Length%26entry.906535625%3DYubo%2520Huang%2520and%2520Hailong%2520Guo%2520and%2520Fangtai%2520Wu%2520and%2520Shifeng%2520Zhang%2520and%2520Shijie%2520Huang%2520and%2520Qijun%2520Gan%2520and%2520Lin%2520Liu%2520and%2520Sirui%2520Zhao%2520and%2520Enhong%2520Chen%2520and%2520Jiaming%2520Liu%2520and%2520Steven%2520Hoi%26entry.1292438233%3DExisting%2520diffusion-based%2520video%2520generation%2520methods%2520are%2520fundamentally%2520constrained%2520by%2520sequential%2520computation%2520and%2520long-horizon%2520inconsistency%252C%2520limiting%2520their%2520practical%2520adoption%2520in%2520real-time%252C%2520streaming%2520audio-driven%2520avatar%2520synthesis.%2520We%2520present%2520Live%2520Avatar%252C%2520an%2520algorithm-system%2520co-designed%2520framework%2520that%2520enables%2520efficient%252C%2520high-fidelity%252C%2520and%2520infinite-length%2520avatar%2520generation%2520using%2520a%252014-billion-parameter%2520diffusion%2520model.%2520Our%2520approach%2520introduces%2520Timestep-forcing%2520Pipeline%2520Parallelism%2520%2528TPP%2529%252C%2520a%2520distributed%2520inference%2520paradigm%2520that%2520pipelines%2520denoising%2520steps%2520across%2520multiple%2520GPUs%252C%2520effectively%2520breaking%2520the%2520autoregressive%2520bottleneck%2520and%2520ensuring%2520stable%252C%2520low-latency%2520real-time%2520streaming.%2520To%2520further%2520enhance%2520temporal%2520consistency%2520and%2520mitigate%2520identity%2520drift%2520and%2520color%2520artifacts%252C%2520we%2520propose%2520the%2520Rolling%2520Sink%2520Frame%2520Mechanism%2520%2528RSFM%2529%252C%2520which%2520maintains%2520sequence%2520fidelity%2520by%2520dynamically%2520recalibrating%2520appearance%2520using%2520a%2520cached%2520reference%2520image.%2520Additionally%252C%2520we%2520leverage%2520Self-Forcing%2520Distribution%2520Matching%2520Distillation%2520to%2520facilitate%2520causal%252C%2520streamable%2520adaptation%2520of%2520large-scale%2520models%2520without%2520sacrificing%2520visual%2520quality.%2520Live%2520Avatar%2520demonstrates%2520state-of-the-art%2520performance%252C%2520reaching%252020%2520FPS%2520end-to-end%2520generation%2520on%25205%2520H800%2520GPUs%252C%2520and%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520is%2520the%2520first%2520to%2520achieve%2520practical%252C%2520real-time%252C%2520high-fidelity%2520avatar%2520generation%2520at%2520this%2520scale.%2520Our%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%2520deploying%2520advanced%2520diffusion%2520models%2520in%2520industrial%2520long-form%2520video%2520synthesis%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Live%20Avatar%3A%20Streaming%20Real-time%20Audio-Driven%20Avatar%20Generation%20with%20Infinite%20Length&entry.906535625=Yubo%20Huang%20and%20Hailong%20Guo%20and%20Fangtai%20Wu%20and%20Shifeng%20Zhang%20and%20Shijie%20Huang%20and%20Qijun%20Gan%20and%20Lin%20Liu%20and%20Sirui%20Zhao%20and%20Enhong%20Chen%20and%20Jiaming%20Liu%20and%20Steven%20Hoi&entry.1292438233=Existing%20diffusion-based%20video%20generation%20methods%20are%20fundamentally%20constrained%20by%20sequential%20computation%20and%20long-horizon%20inconsistency%2C%20limiting%20their%20practical%20adoption%20in%20real-time%2C%20streaming%20audio-driven%20avatar%20synthesis.%20We%20present%20Live%20Avatar%2C%20an%20algorithm-system%20co-designed%20framework%20that%20enables%20efficient%2C%20high-fidelity%2C%20and%20infinite-length%20avatar%20generation%20using%20a%2014-billion-parameter%20diffusion%20model.%20Our%20approach%20introduces%20Timestep-forcing%20Pipeline%20Parallelism%20%28TPP%29%2C%20a%20distributed%20inference%20paradigm%20that%20pipelines%20denoising%20steps%20across%20multiple%20GPUs%2C%20effectively%20breaking%20the%20autoregressive%20bottleneck%20and%20ensuring%20stable%2C%20low-latency%20real-time%20streaming.%20To%20further%20enhance%20temporal%20consistency%20and%20mitigate%20identity%20drift%20and%20color%20artifacts%2C%20we%20propose%20the%20Rolling%20Sink%20Frame%20Mechanism%20%28RSFM%29%2C%20which%20maintains%20sequence%20fidelity%20by%20dynamically%20recalibrating%20appearance%20using%20a%20cached%20reference%20image.%20Additionally%2C%20we%20leverage%20Self-Forcing%20Distribution%20Matching%20Distillation%20to%20facilitate%20causal%2C%20streamable%20adaptation%20of%20large-scale%20models%20without%20sacrificing%20visual%20quality.%20Live%20Avatar%20demonstrates%20state-of-the-art%20performance%2C%20reaching%2020%20FPS%20end-to-end%20generation%20on%205%20H800%20GPUs%2C%20and%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%20to%20achieve%20practical%2C%20real-time%2C%20high-fidelity%20avatar%20generation%20at%20this%20scale.%20Our%20work%20establishes%20a%20new%20paradigm%20for%20deploying%20advanced%20diffusion%20models%20in%20industrial%20long-form%20video%20synthesis%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.04677v1&entry.124074799=Read"},
{"title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer", "author": "Xianfeng Wu and Yajing Bai and Minghan Li and Xianzu Wu and Xueqi Zhao and Zhongyuan Lai and Wenyu Liu and Xinggang Wang", "abstract": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT", "link": "http://arxiv.org/abs/2512.05060v1", "date": "2025-12-04", "relevancy": 3.002, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6058}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DLangVGGT%3A%204D%20Language-Visual%20Geometry%20Grounded%20Transformer&body=Title%3A%204DLangVGGT%3A%204D%20Language-Visual%20Geometry%20Grounded%20Transformer%0AAuthor%3A%20Xianfeng%20Wu%20and%20Yajing%20Bai%20and%20Minghan%20Li%20and%20Xianzu%20Wu%20and%20Xueqi%20Zhao%20and%20Zhongyuan%20Lai%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20Constructing%204D%20language%20fields%20is%20crucial%20for%20embodied%20AI%2C%20augmented/virtual%20reality%2C%20and%204D%20scene%20understanding%2C%20as%20they%20provide%20enriched%20semantic%20representations%20of%20dynamic%20environments%20and%20enable%20open-vocabulary%20querying%20in%20complex%20scenarios.%20However%2C%20existing%20approaches%20to%204D%20semantic%20field%20construction%20primarily%20rely%20on%20scene-specific%20Gaussian%20splatting%2C%20which%20requires%20per-scene%20optimization%2C%20exhibits%20limited%20generalization%2C%20and%20is%20difficult%20to%20scale%20to%20real-world%20applications.%20To%20address%20these%20limitations%2C%20we%20propose%204DLangVGGT%2C%20the%20first%20Transformer-based%20feed-forward%20unified%20framework%20for%204D%20language%20grounding%2C%20that%20jointly%20integrates%20geometric%20perception%20and%20language%20alignment%20within%20a%20single%20architecture.%204DLangVGGT%20has%20two%20key%20components%3A%20the%204D%20Visual%20Geometry%20Transformer%2C%20StreamVGGT%2C%20which%20captures%20spatio-temporal%20geometric%20representations%20of%20dynamic%20scenes%3B%20and%20the%20Semantic%20Bridging%20Decoder%20%28SBD%29%2C%20which%20projects%20geometry-aware%20features%20into%20a%20language-aligned%20semantic%20space%2C%20thereby%20enhancing%20semantic%20interpretability%20while%20preserving%20structural%20fidelity.%20Unlike%20prior%20methods%20that%20depend%20on%20costly%20per-scene%20optimization%2C%204DLangVGGT%20can%20be%20jointly%20trained%20across%20multiple%20dynamic%20scenes%20and%20directly%20applied%20during%20inference%2C%20achieving%20both%20deployment%20efficiency%20and%20strong%20generalization.%20This%20design%20significantly%20improves%20the%20practicality%20of%20large-scale%20deployment%20and%20establishes%20a%20new%20paradigm%20for%20open-vocabulary%204D%20scene%20understanding.%20Experiments%20on%20HyperNeRF%20and%20Neu3D%20datasets%20demonstrate%20that%20our%20approach%20not%20only%20generalizes%20effectively%20but%20also%20achieves%20state-of-the-art%20performance%2C%20achieving%20up%20to%202%25%20gains%20under%20per-scene%20training%20and%201%25%20improvements%20under%20multi-scene%20training.%20Our%20code%20released%20in%20https%3A//github.com/hustvl/4DLangVGGT%0ALink%3A%20http%3A//arxiv.org/abs/2512.05060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DLangVGGT%253A%25204D%2520Language-Visual%2520Geometry%2520Grounded%2520Transformer%26entry.906535625%3DXianfeng%2520Wu%2520and%2520Yajing%2520Bai%2520and%2520Minghan%2520Li%2520and%2520Xianzu%2520Wu%2520and%2520Xueqi%2520Zhao%2520and%2520Zhongyuan%2520Lai%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3DConstructing%25204D%2520language%2520fields%2520is%2520crucial%2520for%2520embodied%2520AI%252C%2520augmented/virtual%2520reality%252C%2520and%25204D%2520scene%2520understanding%252C%2520as%2520they%2520provide%2520enriched%2520semantic%2520representations%2520of%2520dynamic%2520environments%2520and%2520enable%2520open-vocabulary%2520querying%2520in%2520complex%2520scenarios.%2520However%252C%2520existing%2520approaches%2520to%25204D%2520semantic%2520field%2520construction%2520primarily%2520rely%2520on%2520scene-specific%2520Gaussian%2520splatting%252C%2520which%2520requires%2520per-scene%2520optimization%252C%2520exhibits%2520limited%2520generalization%252C%2520and%2520is%2520difficult%2520to%2520scale%2520to%2520real-world%2520applications.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%25204DLangVGGT%252C%2520the%2520first%2520Transformer-based%2520feed-forward%2520unified%2520framework%2520for%25204D%2520language%2520grounding%252C%2520that%2520jointly%2520integrates%2520geometric%2520perception%2520and%2520language%2520alignment%2520within%2520a%2520single%2520architecture.%25204DLangVGGT%2520has%2520two%2520key%2520components%253A%2520the%25204D%2520Visual%2520Geometry%2520Transformer%252C%2520StreamVGGT%252C%2520which%2520captures%2520spatio-temporal%2520geometric%2520representations%2520of%2520dynamic%2520scenes%253B%2520and%2520the%2520Semantic%2520Bridging%2520Decoder%2520%2528SBD%2529%252C%2520which%2520projects%2520geometry-aware%2520features%2520into%2520a%2520language-aligned%2520semantic%2520space%252C%2520thereby%2520enhancing%2520semantic%2520interpretability%2520while%2520preserving%2520structural%2520fidelity.%2520Unlike%2520prior%2520methods%2520that%2520depend%2520on%2520costly%2520per-scene%2520optimization%252C%25204DLangVGGT%2520can%2520be%2520jointly%2520trained%2520across%2520multiple%2520dynamic%2520scenes%2520and%2520directly%2520applied%2520during%2520inference%252C%2520achieving%2520both%2520deployment%2520efficiency%2520and%2520strong%2520generalization.%2520This%2520design%2520significantly%2520improves%2520the%2520practicality%2520of%2520large-scale%2520deployment%2520and%2520establishes%2520a%2520new%2520paradigm%2520for%2520open-vocabulary%25204D%2520scene%2520understanding.%2520Experiments%2520on%2520HyperNeRF%2520and%2520Neu3D%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520not%2520only%2520generalizes%2520effectively%2520but%2520also%2520achieves%2520state-of-the-art%2520performance%252C%2520achieving%2520up%2520to%25202%2525%2520gains%2520under%2520per-scene%2520training%2520and%25201%2525%2520improvements%2520under%2520multi-scene%2520training.%2520Our%2520code%2520released%2520in%2520https%253A//github.com/hustvl/4DLangVGGT%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DLangVGGT%3A%204D%20Language-Visual%20Geometry%20Grounded%20Transformer&entry.906535625=Xianfeng%20Wu%20and%20Yajing%20Bai%20and%20Minghan%20Li%20and%20Xianzu%20Wu%20and%20Xueqi%20Zhao%20and%20Zhongyuan%20Lai%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=Constructing%204D%20language%20fields%20is%20crucial%20for%20embodied%20AI%2C%20augmented/virtual%20reality%2C%20and%204D%20scene%20understanding%2C%20as%20they%20provide%20enriched%20semantic%20representations%20of%20dynamic%20environments%20and%20enable%20open-vocabulary%20querying%20in%20complex%20scenarios.%20However%2C%20existing%20approaches%20to%204D%20semantic%20field%20construction%20primarily%20rely%20on%20scene-specific%20Gaussian%20splatting%2C%20which%20requires%20per-scene%20optimization%2C%20exhibits%20limited%20generalization%2C%20and%20is%20difficult%20to%20scale%20to%20real-world%20applications.%20To%20address%20these%20limitations%2C%20we%20propose%204DLangVGGT%2C%20the%20first%20Transformer-based%20feed-forward%20unified%20framework%20for%204D%20language%20grounding%2C%20that%20jointly%20integrates%20geometric%20perception%20and%20language%20alignment%20within%20a%20single%20architecture.%204DLangVGGT%20has%20two%20key%20components%3A%20the%204D%20Visual%20Geometry%20Transformer%2C%20StreamVGGT%2C%20which%20captures%20spatio-temporal%20geometric%20representations%20of%20dynamic%20scenes%3B%20and%20the%20Semantic%20Bridging%20Decoder%20%28SBD%29%2C%20which%20projects%20geometry-aware%20features%20into%20a%20language-aligned%20semantic%20space%2C%20thereby%20enhancing%20semantic%20interpretability%20while%20preserving%20structural%20fidelity.%20Unlike%20prior%20methods%20that%20depend%20on%20costly%20per-scene%20optimization%2C%204DLangVGGT%20can%20be%20jointly%20trained%20across%20multiple%20dynamic%20scenes%20and%20directly%20applied%20during%20inference%2C%20achieving%20both%20deployment%20efficiency%20and%20strong%20generalization.%20This%20design%20significantly%20improves%20the%20practicality%20of%20large-scale%20deployment%20and%20establishes%20a%20new%20paradigm%20for%20open-vocabulary%204D%20scene%20understanding.%20Experiments%20on%20HyperNeRF%20and%20Neu3D%20datasets%20demonstrate%20that%20our%20approach%20not%20only%20generalizes%20effectively%20but%20also%20achieves%20state-of-the-art%20performance%2C%20achieving%20up%20to%202%25%20gains%20under%20per-scene%20training%20and%201%25%20improvements%20under%20multi-scene%20training.%20Our%20code%20released%20in%20https%3A//github.com/hustvl/4DLangVGGT&entry.1838667208=http%3A//arxiv.org/abs/2512.05060v1&entry.124074799=Read"},
{"title": "Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs", "author": "Xintong Zhang and Zhi Gao and Bofei Zhang and Pengxiang Li and Xiaowen Zhang and Yang Liu and Tao Yuan and Yuwei Wu and Yunde Jia and Song-Chun Zhu and Qing Li", "abstract": "Vision language models (VLMs) have achieved impressive performance across a variety of computer vision tasks. However, the multimodal reasoning capability has not been fully explored in existing models. In this paper, we propose a Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and zooming in on key image regions based on obtained visual cues and the given questions, achieving efficient multimodal reasoning. To enable this CoF capability, we present a two-stage training pipeline, including supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct the MM-CoF dataset, comprising 3K samples derived from a visual agent designed to adaptively identify key regions to solve visual tasks with different image resolutions and questions. We use MM-CoF to fine-tune the Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome accuracies and formats as rewards to update the Qwen2.5-VL model, enabling further refining the search and reasoning strategy of models without human priors. Our model achieves significant improvements on multiple benchmarks. On the V* benchmark that requires strong visual reasoning capability, our model outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to 4K, demonstrating the effectiveness of the proposed CoF method and facilitating the more efficient deployment of VLMs in practical applications.", "link": "http://arxiv.org/abs/2505.15436v2", "date": "2025-12-04", "relevancy": 2.9441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Chain-of-Focus%20Reasoning%20via%20Dynamic%20Visual%20Search%20and%20Zooming%20for%20Efficient%20VLMs&body=Title%3A%20Adaptive%20Chain-of-Focus%20Reasoning%20via%20Dynamic%20Visual%20Search%20and%20Zooming%20for%20Efficient%20VLMs%0AAuthor%3A%20Xintong%20Zhang%20and%20Zhi%20Gao%20and%20Bofei%20Zhang%20and%20Pengxiang%20Li%20and%20Xiaowen%20Zhang%20and%20Yang%20Liu%20and%20Tao%20Yuan%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%20and%20Song-Chun%20Zhu%20and%20Qing%20Li%0AAbstract%3A%20Vision%20language%20models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20across%20a%20variety%20of%20computer%20vision%20tasks.%20However%2C%20the%20multimodal%20reasoning%20capability%20has%20not%20been%20fully%20explored%20in%20existing%20models.%20In%20this%20paper%2C%20we%20propose%20a%20Chain-of-Focus%20%28CoF%29%20method%20that%20allows%20VLMs%20to%20perform%20adaptive%20focusing%20and%20zooming%20in%20on%20key%20image%20regions%20based%20on%20obtained%20visual%20cues%20and%20the%20given%20questions%2C%20achieving%20efficient%20multimodal%20reasoning.%20To%20enable%20this%20CoF%20capability%2C%20we%20present%20a%20two-stage%20training%20pipeline%2C%20including%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20In%20the%20SFT%20stage%2C%20we%20construct%20the%20MM-CoF%20dataset%2C%20comprising%203K%20samples%20derived%20from%20a%20visual%20agent%20designed%20to%20adaptively%20identify%20key%20regions%20to%20solve%20visual%20tasks%20with%20different%20image%20resolutions%20and%20questions.%20We%20use%20MM-CoF%20to%20fine-tune%20the%20Qwen2.5-VL%20model%20for%20cold%20start.%20In%20the%20RL%20stage%2C%20we%20leverage%20the%20outcome%20accuracies%20and%20formats%20as%20rewards%20to%20update%20the%20Qwen2.5-VL%20model%2C%20enabling%20further%20refining%20the%20search%20and%20reasoning%20strategy%20of%20models%20without%20human%20priors.%20Our%20model%20achieves%20significant%20improvements%20on%20multiple%20benchmarks.%20On%20the%20V%2A%20benchmark%20that%20requires%20strong%20visual%20reasoning%20capability%2C%20our%20model%20outperforms%20existing%20VLMs%20by%205%25%20among%208%20image%20resolutions%20ranging%20from%20224%20to%204K%2C%20demonstrating%20the%20effectiveness%20of%20the%20proposed%20CoF%20method%20and%20facilitating%20the%20more%20efficient%20deployment%20of%20VLMs%20in%20practical%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2505.15436v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Chain-of-Focus%2520Reasoning%2520via%2520Dynamic%2520Visual%2520Search%2520and%2520Zooming%2520for%2520Efficient%2520VLMs%26entry.906535625%3DXintong%2520Zhang%2520and%2520Zhi%2520Gao%2520and%2520Bofei%2520Zhang%2520and%2520Pengxiang%2520Li%2520and%2520Xiaowen%2520Zhang%2520and%2520Yang%2520Liu%2520and%2520Tao%2520Yuan%2520and%2520Yuwei%2520Wu%2520and%2520Yunde%2520Jia%2520and%2520Song-Chun%2520Zhu%2520and%2520Qing%2520Li%26entry.1292438233%3DVision%2520language%2520models%2520%2528VLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520across%2520a%2520variety%2520of%2520computer%2520vision%2520tasks.%2520However%252C%2520the%2520multimodal%2520reasoning%2520capability%2520has%2520not%2520been%2520fully%2520explored%2520in%2520existing%2520models.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Chain-of-Focus%2520%2528CoF%2529%2520method%2520that%2520allows%2520VLMs%2520to%2520perform%2520adaptive%2520focusing%2520and%2520zooming%2520in%2520on%2520key%2520image%2520regions%2520based%2520on%2520obtained%2520visual%2520cues%2520and%2520the%2520given%2520questions%252C%2520achieving%2520efficient%2520multimodal%2520reasoning.%2520To%2520enable%2520this%2520CoF%2520capability%252C%2520we%2520present%2520a%2520two-stage%2520training%2520pipeline%252C%2520including%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%2520learning%2520%2528RL%2529.%2520In%2520the%2520SFT%2520stage%252C%2520we%2520construct%2520the%2520MM-CoF%2520dataset%252C%2520comprising%25203K%2520samples%2520derived%2520from%2520a%2520visual%2520agent%2520designed%2520to%2520adaptively%2520identify%2520key%2520regions%2520to%2520solve%2520visual%2520tasks%2520with%2520different%2520image%2520resolutions%2520and%2520questions.%2520We%2520use%2520MM-CoF%2520to%2520fine-tune%2520the%2520Qwen2.5-VL%2520model%2520for%2520cold%2520start.%2520In%2520the%2520RL%2520stage%252C%2520we%2520leverage%2520the%2520outcome%2520accuracies%2520and%2520formats%2520as%2520rewards%2520to%2520update%2520the%2520Qwen2.5-VL%2520model%252C%2520enabling%2520further%2520refining%2520the%2520search%2520and%2520reasoning%2520strategy%2520of%2520models%2520without%2520human%2520priors.%2520Our%2520model%2520achieves%2520significant%2520improvements%2520on%2520multiple%2520benchmarks.%2520On%2520the%2520V%252A%2520benchmark%2520that%2520requires%2520strong%2520visual%2520reasoning%2520capability%252C%2520our%2520model%2520outperforms%2520existing%2520VLMs%2520by%25205%2525%2520among%25208%2520image%2520resolutions%2520ranging%2520from%2520224%2520to%25204K%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520the%2520proposed%2520CoF%2520method%2520and%2520facilitating%2520the%2520more%2520efficient%2520deployment%2520of%2520VLMs%2520in%2520practical%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15436v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Chain-of-Focus%20Reasoning%20via%20Dynamic%20Visual%20Search%20and%20Zooming%20for%20Efficient%20VLMs&entry.906535625=Xintong%20Zhang%20and%20Zhi%20Gao%20and%20Bofei%20Zhang%20and%20Pengxiang%20Li%20and%20Xiaowen%20Zhang%20and%20Yang%20Liu%20and%20Tao%20Yuan%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%20and%20Song-Chun%20Zhu%20and%20Qing%20Li&entry.1292438233=Vision%20language%20models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20across%20a%20variety%20of%20computer%20vision%20tasks.%20However%2C%20the%20multimodal%20reasoning%20capability%20has%20not%20been%20fully%20explored%20in%20existing%20models.%20In%20this%20paper%2C%20we%20propose%20a%20Chain-of-Focus%20%28CoF%29%20method%20that%20allows%20VLMs%20to%20perform%20adaptive%20focusing%20and%20zooming%20in%20on%20key%20image%20regions%20based%20on%20obtained%20visual%20cues%20and%20the%20given%20questions%2C%20achieving%20efficient%20multimodal%20reasoning.%20To%20enable%20this%20CoF%20capability%2C%20we%20present%20a%20two-stage%20training%20pipeline%2C%20including%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20%28RL%29.%20In%20the%20SFT%20stage%2C%20we%20construct%20the%20MM-CoF%20dataset%2C%20comprising%203K%20samples%20derived%20from%20a%20visual%20agent%20designed%20to%20adaptively%20identify%20key%20regions%20to%20solve%20visual%20tasks%20with%20different%20image%20resolutions%20and%20questions.%20We%20use%20MM-CoF%20to%20fine-tune%20the%20Qwen2.5-VL%20model%20for%20cold%20start.%20In%20the%20RL%20stage%2C%20we%20leverage%20the%20outcome%20accuracies%20and%20formats%20as%20rewards%20to%20update%20the%20Qwen2.5-VL%20model%2C%20enabling%20further%20refining%20the%20search%20and%20reasoning%20strategy%20of%20models%20without%20human%20priors.%20Our%20model%20achieves%20significant%20improvements%20on%20multiple%20benchmarks.%20On%20the%20V%2A%20benchmark%20that%20requires%20strong%20visual%20reasoning%20capability%2C%20our%20model%20outperforms%20existing%20VLMs%20by%205%25%20among%208%20image%20resolutions%20ranging%20from%20224%20to%204K%2C%20demonstrating%20the%20effectiveness%20of%20the%20proposed%20CoF%20method%20and%20facilitating%20the%20more%20efficient%20deployment%20of%20VLMs%20in%20practical%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2505.15436v2&entry.124074799=Read"},
{"title": "MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions", "author": "Kaen Kogashi and Anoop Cherian and Meng-Yu Jennifer Kuo", "abstract": "Real-world scenes often feature multiple humans interacting with multiple objects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D human-object interaction (HOI) benchmarks consider only a fraction of these complex interactions. To close this gap, we present MMHOI -- a large-scale, Multi-human Multi-object Interaction dataset consisting of images from 12 everyday scenarios. MMHOI offers complete 3D shape and pose annotations for every person and object, along with labels for 78 action categories and 14 interaction-specific body parts, providing a comprehensive testbed for next-generation HOI research. Building on MMHOI, we present MMHOI-Net, an end-to-end transformer-based neural network for jointly estimating human-object 3D geometries, their interactions, and associated actions. A key innovation in our framework is a structured dual-patch representation for modeling objects and their interactions, combined with action recognition to enhance the interaction prediction. Experiments on MMHOI and the recently proposed CORE4D datasets demonstrate that our approach achieves state-of-the-art performance in multi-HOI modeling, excelling in both accuracy and reconstruction quality. The MMHOI dataset is publicly available at https://zenodo.org/records/17711786.", "link": "http://arxiv.org/abs/2510.07828v3", "date": "2025-12-04", "relevancy": 2.9248, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6209}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5988}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMHOI%3A%20Modeling%20Complex%203D%20Multi-Human%20Multi-Object%20Interactions&body=Title%3A%20MMHOI%3A%20Modeling%20Complex%203D%20Multi-Human%20Multi-Object%20Interactions%0AAuthor%3A%20Kaen%20Kogashi%20and%20Anoop%20Cherian%20and%20Meng-Yu%20Jennifer%20Kuo%0AAbstract%3A%20Real-world%20scenes%20often%20feature%20multiple%20humans%20interacting%20with%20multiple%20objects%20in%20ways%20that%20are%20causal%2C%20goal-oriented%2C%20or%20cooperative.%20Yet%20existing%203D%20human-object%20interaction%20%28HOI%29%20benchmarks%20consider%20only%20a%20fraction%20of%20these%20complex%20interactions.%20To%20close%20this%20gap%2C%20we%20present%20MMHOI%20--%20a%20large-scale%2C%20Multi-human%20Multi-object%20Interaction%20dataset%20consisting%20of%20images%20from%2012%20everyday%20scenarios.%20MMHOI%20offers%20complete%203D%20shape%20and%20pose%20annotations%20for%20every%20person%20and%20object%2C%20along%20with%20labels%20for%2078%20action%20categories%20and%2014%20interaction-specific%20body%20parts%2C%20providing%20a%20comprehensive%20testbed%20for%20next-generation%20HOI%20research.%20Building%20on%20MMHOI%2C%20we%20present%20MMHOI-Net%2C%20an%20end-to-end%20transformer-based%20neural%20network%20for%20jointly%20estimating%20human-object%203D%20geometries%2C%20their%20interactions%2C%20and%20associated%20actions.%20A%20key%20innovation%20in%20our%20framework%20is%20a%20structured%20dual-patch%20representation%20for%20modeling%20objects%20and%20their%20interactions%2C%20combined%20with%20action%20recognition%20to%20enhance%20the%20interaction%20prediction.%20Experiments%20on%20MMHOI%20and%20the%20recently%20proposed%20CORE4D%20datasets%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20in%20multi-HOI%20modeling%2C%20excelling%20in%20both%20accuracy%20and%20reconstruction%20quality.%20The%20MMHOI%20dataset%20is%20publicly%20available%20at%20https%3A//zenodo.org/records/17711786.%0ALink%3A%20http%3A//arxiv.org/abs/2510.07828v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMHOI%253A%2520Modeling%2520Complex%25203D%2520Multi-Human%2520Multi-Object%2520Interactions%26entry.906535625%3DKaen%2520Kogashi%2520and%2520Anoop%2520Cherian%2520and%2520Meng-Yu%2520Jennifer%2520Kuo%26entry.1292438233%3DReal-world%2520scenes%2520often%2520feature%2520multiple%2520humans%2520interacting%2520with%2520multiple%2520objects%2520in%2520ways%2520that%2520are%2520causal%252C%2520goal-oriented%252C%2520or%2520cooperative.%2520Yet%2520existing%25203D%2520human-object%2520interaction%2520%2528HOI%2529%2520benchmarks%2520consider%2520only%2520a%2520fraction%2520of%2520these%2520complex%2520interactions.%2520To%2520close%2520this%2520gap%252C%2520we%2520present%2520MMHOI%2520--%2520a%2520large-scale%252C%2520Multi-human%2520Multi-object%2520Interaction%2520dataset%2520consisting%2520of%2520images%2520from%252012%2520everyday%2520scenarios.%2520MMHOI%2520offers%2520complete%25203D%2520shape%2520and%2520pose%2520annotations%2520for%2520every%2520person%2520and%2520object%252C%2520along%2520with%2520labels%2520for%252078%2520action%2520categories%2520and%252014%2520interaction-specific%2520body%2520parts%252C%2520providing%2520a%2520comprehensive%2520testbed%2520for%2520next-generation%2520HOI%2520research.%2520Building%2520on%2520MMHOI%252C%2520we%2520present%2520MMHOI-Net%252C%2520an%2520end-to-end%2520transformer-based%2520neural%2520network%2520for%2520jointly%2520estimating%2520human-object%25203D%2520geometries%252C%2520their%2520interactions%252C%2520and%2520associated%2520actions.%2520A%2520key%2520innovation%2520in%2520our%2520framework%2520is%2520a%2520structured%2520dual-patch%2520representation%2520for%2520modeling%2520objects%2520and%2520their%2520interactions%252C%2520combined%2520with%2520action%2520recognition%2520to%2520enhance%2520the%2520interaction%2520prediction.%2520Experiments%2520on%2520MMHOI%2520and%2520the%2520recently%2520proposed%2520CORE4D%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520in%2520multi-HOI%2520modeling%252C%2520excelling%2520in%2520both%2520accuracy%2520and%2520reconstruction%2520quality.%2520The%2520MMHOI%2520dataset%2520is%2520publicly%2520available%2520at%2520https%253A//zenodo.org/records/17711786.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07828v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMHOI%3A%20Modeling%20Complex%203D%20Multi-Human%20Multi-Object%20Interactions&entry.906535625=Kaen%20Kogashi%20and%20Anoop%20Cherian%20and%20Meng-Yu%20Jennifer%20Kuo&entry.1292438233=Real-world%20scenes%20often%20feature%20multiple%20humans%20interacting%20with%20multiple%20objects%20in%20ways%20that%20are%20causal%2C%20goal-oriented%2C%20or%20cooperative.%20Yet%20existing%203D%20human-object%20interaction%20%28HOI%29%20benchmarks%20consider%20only%20a%20fraction%20of%20these%20complex%20interactions.%20To%20close%20this%20gap%2C%20we%20present%20MMHOI%20--%20a%20large-scale%2C%20Multi-human%20Multi-object%20Interaction%20dataset%20consisting%20of%20images%20from%2012%20everyday%20scenarios.%20MMHOI%20offers%20complete%203D%20shape%20and%20pose%20annotations%20for%20every%20person%20and%20object%2C%20along%20with%20labels%20for%2078%20action%20categories%20and%2014%20interaction-specific%20body%20parts%2C%20providing%20a%20comprehensive%20testbed%20for%20next-generation%20HOI%20research.%20Building%20on%20MMHOI%2C%20we%20present%20MMHOI-Net%2C%20an%20end-to-end%20transformer-based%20neural%20network%20for%20jointly%20estimating%20human-object%203D%20geometries%2C%20their%20interactions%2C%20and%20associated%20actions.%20A%20key%20innovation%20in%20our%20framework%20is%20a%20structured%20dual-patch%20representation%20for%20modeling%20objects%20and%20their%20interactions%2C%20combined%20with%20action%20recognition%20to%20enhance%20the%20interaction%20prediction.%20Experiments%20on%20MMHOI%20and%20the%20recently%20proposed%20CORE4D%20datasets%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20in%20multi-HOI%20modeling%2C%20excelling%20in%20both%20accuracy%20and%20reconstruction%20quality.%20The%20MMHOI%20dataset%20is%20publicly%20available%20at%20https%3A//zenodo.org/records/17711786.&entry.1838667208=http%3A//arxiv.org/abs/2510.07828v3&entry.124074799=Read"},
{"title": "Towards Cross-View Point Correspondence in Vision-Language Models", "author": "Yipu Wang and Yuheng Ji and Yuyang Liu and Enshen Zhou and Ziqiang Yang and Yuxuan Tian and Ziheng Qin and Yue Liu and Huajie Tan and Cheng Chi and Zhiyuan Ma and Daniel Dajun Zeng and Xiaolong Zheng", "abstract": "Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of \"perceive\", \"reason\", and \"correspond\". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.", "link": "http://arxiv.org/abs/2512.04686v1", "date": "2025-12-04", "relevancy": 2.8945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5964}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Cross-View%20Point%20Correspondence%20in%20Vision-Language%20Models&body=Title%3A%20Towards%20Cross-View%20Point%20Correspondence%20in%20Vision-Language%20Models%0AAuthor%3A%20Yipu%20Wang%20and%20Yuheng%20Ji%20and%20Yuyang%20Liu%20and%20Enshen%20Zhou%20and%20Ziqiang%20Yang%20and%20Yuxuan%20Tian%20and%20Ziheng%20Qin%20and%20Yue%20Liu%20and%20Huajie%20Tan%20and%20Cheng%20Chi%20and%20Zhiyuan%20Ma%20and%20Daniel%20Dajun%20Zeng%20and%20Xiaolong%20Zheng%0AAbstract%3A%20Cross-view%20correspondence%20is%20a%20fundamental%20capability%20for%20spatial%20understanding%20and%20embodied%20AI.%20However%2C%20it%20is%20still%20far%20from%20being%20realized%20in%20Vision-Language%20Models%20%28VLMs%29%2C%20especially%20in%20achieving%20precise%20point-level%20correspondence%2C%20which%20is%20crucial%20for%20precise%20affordance%20interaction.%20So%20we%20propose%20the%20Cross-View%20Point%20Correspondence%20%28CVPC%29%20task%20and%20CrossPoint-Bench%2C%20a%20comprehensive%20benchmark%20with%20hierarchical%20design%2C%20inspired%20by%20the%20human%20cognitive%20process%20of%20%22perceive%22%2C%20%22reason%22%2C%20and%20%22correspond%22.%20Our%20evaluation%20shows%20the%20state-of-the-art%20models%20%28e.g.%2C%20Gemini-2.5-Pro%29%20still%20fall%20far%20behind%20humans%2C%20with%20a%20gap%20of%20over%2054.65%25%20in%20overall%20accuracy%2C%20exposing%20a%20challenge%20in%20transitioning%20from%20coarse-grained%20judgement%20to%20fine-grained%20coordinate%20prediction.%20To%20address%20this%20problem%2C%20we%20construct%20CrossPoint-378K%2C%20a%20dataset%20with%20378K%20question-answering%20pairs%20across%20900%20scenes%2C%20focused%20on%20actionable%20affordance%20regions%20that%20better%20reflect%20real-world%20manipulation%20and%20interaction%20scenarios.%20Furthermore%2C%20we%20propose%20CroPond%20that%20trained%20on%20the%20CrossPoint-378K%20dataset.%20Our%20CroPond%20achieves%20state-of-the-art%20performance%20on%20CrossPoint-Bench%2C%20surpassing%20Gemini-2.5-Pro%20by%2039.7%25%20accuracy%2C%20which%20offers%20a%20foundation%20for%20advancing%20future%20work%20on%20cross-view%20correspondence.%20The%20benchmark%2C%20dataset%2C%20and%20model%20are%20publicly%20available%20at%20https%3A//github.com/WangYipu2002/CrossPoint.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Cross-View%2520Point%2520Correspondence%2520in%2520Vision-Language%2520Models%26entry.906535625%3DYipu%2520Wang%2520and%2520Yuheng%2520Ji%2520and%2520Yuyang%2520Liu%2520and%2520Enshen%2520Zhou%2520and%2520Ziqiang%2520Yang%2520and%2520Yuxuan%2520Tian%2520and%2520Ziheng%2520Qin%2520and%2520Yue%2520Liu%2520and%2520Huajie%2520Tan%2520and%2520Cheng%2520Chi%2520and%2520Zhiyuan%2520Ma%2520and%2520Daniel%2520Dajun%2520Zeng%2520and%2520Xiaolong%2520Zheng%26entry.1292438233%3DCross-view%2520correspondence%2520is%2520a%2520fundamental%2520capability%2520for%2520spatial%2520understanding%2520and%2520embodied%2520AI.%2520However%252C%2520it%2520is%2520still%2520far%2520from%2520being%2520realized%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520especially%2520in%2520achieving%2520precise%2520point-level%2520correspondence%252C%2520which%2520is%2520crucial%2520for%2520precise%2520affordance%2520interaction.%2520So%2520we%2520propose%2520the%2520Cross-View%2520Point%2520Correspondence%2520%2528CVPC%2529%2520task%2520and%2520CrossPoint-Bench%252C%2520a%2520comprehensive%2520benchmark%2520with%2520hierarchical%2520design%252C%2520inspired%2520by%2520the%2520human%2520cognitive%2520process%2520of%2520%2522perceive%2522%252C%2520%2522reason%2522%252C%2520and%2520%2522correspond%2522.%2520Our%2520evaluation%2520shows%2520the%2520state-of-the-art%2520models%2520%2528e.g.%252C%2520Gemini-2.5-Pro%2529%2520still%2520fall%2520far%2520behind%2520humans%252C%2520with%2520a%2520gap%2520of%2520over%252054.65%2525%2520in%2520overall%2520accuracy%252C%2520exposing%2520a%2520challenge%2520in%2520transitioning%2520from%2520coarse-grained%2520judgement%2520to%2520fine-grained%2520coordinate%2520prediction.%2520To%2520address%2520this%2520problem%252C%2520we%2520construct%2520CrossPoint-378K%252C%2520a%2520dataset%2520with%2520378K%2520question-answering%2520pairs%2520across%2520900%2520scenes%252C%2520focused%2520on%2520actionable%2520affordance%2520regions%2520that%2520better%2520reflect%2520real-world%2520manipulation%2520and%2520interaction%2520scenarios.%2520Furthermore%252C%2520we%2520propose%2520CroPond%2520that%2520trained%2520on%2520the%2520CrossPoint-378K%2520dataset.%2520Our%2520CroPond%2520achieves%2520state-of-the-art%2520performance%2520on%2520CrossPoint-Bench%252C%2520surpassing%2520Gemini-2.5-Pro%2520by%252039.7%2525%2520accuracy%252C%2520which%2520offers%2520a%2520foundation%2520for%2520advancing%2520future%2520work%2520on%2520cross-view%2520correspondence.%2520The%2520benchmark%252C%2520dataset%252C%2520and%2520model%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/WangYipu2002/CrossPoint.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Cross-View%20Point%20Correspondence%20in%20Vision-Language%20Models&entry.906535625=Yipu%20Wang%20and%20Yuheng%20Ji%20and%20Yuyang%20Liu%20and%20Enshen%20Zhou%20and%20Ziqiang%20Yang%20and%20Yuxuan%20Tian%20and%20Ziheng%20Qin%20and%20Yue%20Liu%20and%20Huajie%20Tan%20and%20Cheng%20Chi%20and%20Zhiyuan%20Ma%20and%20Daniel%20Dajun%20Zeng%20and%20Xiaolong%20Zheng&entry.1292438233=Cross-view%20correspondence%20is%20a%20fundamental%20capability%20for%20spatial%20understanding%20and%20embodied%20AI.%20However%2C%20it%20is%20still%20far%20from%20being%20realized%20in%20Vision-Language%20Models%20%28VLMs%29%2C%20especially%20in%20achieving%20precise%20point-level%20correspondence%2C%20which%20is%20crucial%20for%20precise%20affordance%20interaction.%20So%20we%20propose%20the%20Cross-View%20Point%20Correspondence%20%28CVPC%29%20task%20and%20CrossPoint-Bench%2C%20a%20comprehensive%20benchmark%20with%20hierarchical%20design%2C%20inspired%20by%20the%20human%20cognitive%20process%20of%20%22perceive%22%2C%20%22reason%22%2C%20and%20%22correspond%22.%20Our%20evaluation%20shows%20the%20state-of-the-art%20models%20%28e.g.%2C%20Gemini-2.5-Pro%29%20still%20fall%20far%20behind%20humans%2C%20with%20a%20gap%20of%20over%2054.65%25%20in%20overall%20accuracy%2C%20exposing%20a%20challenge%20in%20transitioning%20from%20coarse-grained%20judgement%20to%20fine-grained%20coordinate%20prediction.%20To%20address%20this%20problem%2C%20we%20construct%20CrossPoint-378K%2C%20a%20dataset%20with%20378K%20question-answering%20pairs%20across%20900%20scenes%2C%20focused%20on%20actionable%20affordance%20regions%20that%20better%20reflect%20real-world%20manipulation%20and%20interaction%20scenarios.%20Furthermore%2C%20we%20propose%20CroPond%20that%20trained%20on%20the%20CrossPoint-378K%20dataset.%20Our%20CroPond%20achieves%20state-of-the-art%20performance%20on%20CrossPoint-Bench%2C%20surpassing%20Gemini-2.5-Pro%20by%2039.7%25%20accuracy%2C%20which%20offers%20a%20foundation%20for%20advancing%20future%20work%20on%20cross-view%20correspondence.%20The%20benchmark%2C%20dataset%2C%20and%20model%20are%20publicly%20available%20at%20https%3A//github.com/WangYipu2002/CrossPoint.&entry.1838667208=http%3A//arxiv.org/abs/2512.04686v1&entry.124074799=Read"},
{"title": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging", "author": "Zhijian Shu and Cheng Lin and Tao Xie and Wei Yin and Ben Li and Zhiyuan Pu and Weize Li and Yao Yao and Xun Cao and Xiaoyang Guo and Xiao-Xiao Long", "abstract": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/", "link": "http://arxiv.org/abs/2512.04939v1", "date": "2025-12-04", "relevancy": 2.8776, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5883}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5745}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiteVGGT%3A%20Boosting%20Vanilla%20VGGT%20via%20Geometry-aware%20Cached%20Token%20Merging&body=Title%3A%20LiteVGGT%3A%20Boosting%20Vanilla%20VGGT%20via%20Geometry-aware%20Cached%20Token%20Merging%0AAuthor%3A%20Zhijian%20Shu%20and%20Cheng%20Lin%20and%20Tao%20Xie%20and%20Wei%20Yin%20and%20Ben%20Li%20and%20Zhiyuan%20Pu%20and%20Weize%20Li%20and%20Yao%20Yao%20and%20Xun%20Cao%20and%20Xiaoyang%20Guo%20and%20Xiao-Xiao%20Long%0AAbstract%3A%203D%20vision%20foundation%20models%20like%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20have%20advanced%20greatly%20in%20geometric%20perception.%20However%2C%20it%20is%20time-consuming%20and%20memory-intensive%20for%20long%20sequences%2C%20limiting%20application%20to%20large-scale%20scenes%20beyond%20hundreds%20of%20images.%20To%20address%20this%2C%20we%20propose%20LiteVGGT%2C%20achieving%20up%20to%2010x%20speedup%20and%20substantial%20memory%20reduction%2C%20enabling%20efficient%20processing%20of%201000-image%20scenes.%20We%20derive%20two%20key%20insights%20for%203D%20reconstruction%3A%20%281%29%20tokens%20from%20local%20image%20regions%20have%20inherent%20geometric%20correlations%2C%20leading%20to%20high%20similarity%20and%20computational%20redundancy%3B%20%282%29%20token%20similarity%20across%20adjacent%20network%20layers%20remains%20stable%2C%20allowing%20for%20reusable%20merge%20decisions.%20Guided%20by%20these%2C%20we%20design%20a%20simple%20yet%20efficient%20strategy%2C%20dubbed%20geometry-aware%20cached%20token%20merging.%20We%20analyze%20each%20token%27s%20geometric%20importance%2C%20optimizing%20anchor%20token%20selection%20to%20better%20preserve%20key%20information%20for%20reconstruction.%20We%20also%20cache%20and%20reuse%20merge%20indices%20across%20layers%2C%20substantially%20reducing%20latency%20with%20minimal%20accuracy%20impact.%20This%20strategy%20retains%20VGGT%27s%20core%20performance%2C%20enabling%20efficient%20fine-tuning%20and%20FP8%20quantization%20for%20further%20gains.%20Extensive%20experiments%20validate%20LiteVGGT%27s%20effectiveness%2C%20scalability%2C%20and%20robustness.%20Project%20page%3A%20https%3A//garlicba.github.io/LiteVGGT/%0ALink%3A%20http%3A//arxiv.org/abs/2512.04939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiteVGGT%253A%2520Boosting%2520Vanilla%2520VGGT%2520via%2520Geometry-aware%2520Cached%2520Token%2520Merging%26entry.906535625%3DZhijian%2520Shu%2520and%2520Cheng%2520Lin%2520and%2520Tao%2520Xie%2520and%2520Wei%2520Yin%2520and%2520Ben%2520Li%2520and%2520Zhiyuan%2520Pu%2520and%2520Weize%2520Li%2520and%2520Yao%2520Yao%2520and%2520Xun%2520Cao%2520and%2520Xiaoyang%2520Guo%2520and%2520Xiao-Xiao%2520Long%26entry.1292438233%3D3D%2520vision%2520foundation%2520models%2520like%2520Visual%2520Geometry%2520Grounded%2520Transformer%2520%2528VGGT%2529%2520have%2520advanced%2520greatly%2520in%2520geometric%2520perception.%2520However%252C%2520it%2520is%2520time-consuming%2520and%2520memory-intensive%2520for%2520long%2520sequences%252C%2520limiting%2520application%2520to%2520large-scale%2520scenes%2520beyond%2520hundreds%2520of%2520images.%2520To%2520address%2520this%252C%2520we%2520propose%2520LiteVGGT%252C%2520achieving%2520up%2520to%252010x%2520speedup%2520and%2520substantial%2520memory%2520reduction%252C%2520enabling%2520efficient%2520processing%2520of%25201000-image%2520scenes.%2520We%2520derive%2520two%2520key%2520insights%2520for%25203D%2520reconstruction%253A%2520%25281%2529%2520tokens%2520from%2520local%2520image%2520regions%2520have%2520inherent%2520geometric%2520correlations%252C%2520leading%2520to%2520high%2520similarity%2520and%2520computational%2520redundancy%253B%2520%25282%2529%2520token%2520similarity%2520across%2520adjacent%2520network%2520layers%2520remains%2520stable%252C%2520allowing%2520for%2520reusable%2520merge%2520decisions.%2520Guided%2520by%2520these%252C%2520we%2520design%2520a%2520simple%2520yet%2520efficient%2520strategy%252C%2520dubbed%2520geometry-aware%2520cached%2520token%2520merging.%2520We%2520analyze%2520each%2520token%2527s%2520geometric%2520importance%252C%2520optimizing%2520anchor%2520token%2520selection%2520to%2520better%2520preserve%2520key%2520information%2520for%2520reconstruction.%2520We%2520also%2520cache%2520and%2520reuse%2520merge%2520indices%2520across%2520layers%252C%2520substantially%2520reducing%2520latency%2520with%2520minimal%2520accuracy%2520impact.%2520This%2520strategy%2520retains%2520VGGT%2527s%2520core%2520performance%252C%2520enabling%2520efficient%2520fine-tuning%2520and%2520FP8%2520quantization%2520for%2520further%2520gains.%2520Extensive%2520experiments%2520validate%2520LiteVGGT%2527s%2520effectiveness%252C%2520scalability%252C%2520and%2520robustness.%2520Project%2520page%253A%2520https%253A//garlicba.github.io/LiteVGGT/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiteVGGT%3A%20Boosting%20Vanilla%20VGGT%20via%20Geometry-aware%20Cached%20Token%20Merging&entry.906535625=Zhijian%20Shu%20and%20Cheng%20Lin%20and%20Tao%20Xie%20and%20Wei%20Yin%20and%20Ben%20Li%20and%20Zhiyuan%20Pu%20and%20Weize%20Li%20and%20Yao%20Yao%20and%20Xun%20Cao%20and%20Xiaoyang%20Guo%20and%20Xiao-Xiao%20Long&entry.1292438233=3D%20vision%20foundation%20models%20like%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20have%20advanced%20greatly%20in%20geometric%20perception.%20However%2C%20it%20is%20time-consuming%20and%20memory-intensive%20for%20long%20sequences%2C%20limiting%20application%20to%20large-scale%20scenes%20beyond%20hundreds%20of%20images.%20To%20address%20this%2C%20we%20propose%20LiteVGGT%2C%20achieving%20up%20to%2010x%20speedup%20and%20substantial%20memory%20reduction%2C%20enabling%20efficient%20processing%20of%201000-image%20scenes.%20We%20derive%20two%20key%20insights%20for%203D%20reconstruction%3A%20%281%29%20tokens%20from%20local%20image%20regions%20have%20inherent%20geometric%20correlations%2C%20leading%20to%20high%20similarity%20and%20computational%20redundancy%3B%20%282%29%20token%20similarity%20across%20adjacent%20network%20layers%20remains%20stable%2C%20allowing%20for%20reusable%20merge%20decisions.%20Guided%20by%20these%2C%20we%20design%20a%20simple%20yet%20efficient%20strategy%2C%20dubbed%20geometry-aware%20cached%20token%20merging.%20We%20analyze%20each%20token%27s%20geometric%20importance%2C%20optimizing%20anchor%20token%20selection%20to%20better%20preserve%20key%20information%20for%20reconstruction.%20We%20also%20cache%20and%20reuse%20merge%20indices%20across%20layers%2C%20substantially%20reducing%20latency%20with%20minimal%20accuracy%20impact.%20This%20strategy%20retains%20VGGT%27s%20core%20performance%2C%20enabling%20efficient%20fine-tuning%20and%20FP8%20quantization%20for%20further%20gains.%20Extensive%20experiments%20validate%20LiteVGGT%27s%20effectiveness%2C%20scalability%2C%20and%20robustness.%20Project%20page%3A%20https%3A//garlicba.github.io/LiteVGGT/&entry.1838667208=http%3A//arxiv.org/abs/2512.04939v1&entry.124074799=Read"},
{"title": "Order Matters: 3D Shape Generation from Sequential VR Sketches", "author": "Yizi Chen and Sidi Wu and Tianyi Xiao and Nina Wiedemann and Loic Landrieu", "abstract": "VR sketching lets users explore and iterate on ideas directly in 3D, offering a faster and more intuitive alternative to conventional CAD tools. However, existing sketch-to-shape models ignore the temporal ordering of strokes, discarding crucial cues about structure and design intent. We introduce VRSketch2Shape, the first framework and multi-category dataset for generating 3D shapes from sequential VR sketches. Our contributions are threefold: (i) an automated pipeline that generates sequential VR sketches from arbitrary shapes, (ii) a dataset of over 20k synthetic and 900 hand-drawn sketch-shape pairs across four categories, and (iii) an order-aware sketch encoder coupled with a diffusion-based 3D generator. Our approach yields higher geometric fidelity than prior work, generalizes effectively from synthetic to real sketches with minimal supervision, and performs well even on partial sketches. All data and models will be released open-source at https://chenyizi086.github.io/VRSketch2Shape_website.", "link": "http://arxiv.org/abs/2512.04761v1", "date": "2025-12-04", "relevancy": 2.8752, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5768}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5768}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Order%20Matters%3A%203D%20Shape%20Generation%20from%20Sequential%20VR%20Sketches&body=Title%3A%20Order%20Matters%3A%203D%20Shape%20Generation%20from%20Sequential%20VR%20Sketches%0AAuthor%3A%20Yizi%20Chen%20and%20Sidi%20Wu%20and%20Tianyi%20Xiao%20and%20Nina%20Wiedemann%20and%20Loic%20Landrieu%0AAbstract%3A%20VR%20sketching%20lets%20users%20explore%20and%20iterate%20on%20ideas%20directly%20in%203D%2C%20offering%20a%20faster%20and%20more%20intuitive%20alternative%20to%20conventional%20CAD%20tools.%20However%2C%20existing%20sketch-to-shape%20models%20ignore%20the%20temporal%20ordering%20of%20strokes%2C%20discarding%20crucial%20cues%20about%20structure%20and%20design%20intent.%20We%20introduce%20VRSketch2Shape%2C%20the%20first%20framework%20and%20multi-category%20dataset%20for%20generating%203D%20shapes%20from%20sequential%20VR%20sketches.%20Our%20contributions%20are%20threefold%3A%20%28i%29%20an%20automated%20pipeline%20that%20generates%20sequential%20VR%20sketches%20from%20arbitrary%20shapes%2C%20%28ii%29%20a%20dataset%20of%20over%2020k%20synthetic%20and%20900%20hand-drawn%20sketch-shape%20pairs%20across%20four%20categories%2C%20and%20%28iii%29%20an%20order-aware%20sketch%20encoder%20coupled%20with%20a%20diffusion-based%203D%20generator.%20Our%20approach%20yields%20higher%20geometric%20fidelity%20than%20prior%20work%2C%20generalizes%20effectively%20from%20synthetic%20to%20real%20sketches%20with%20minimal%20supervision%2C%20and%20performs%20well%20even%20on%20partial%20sketches.%20All%20data%20and%20models%20will%20be%20released%20open-source%20at%20https%3A//chenyizi086.github.io/VRSketch2Shape_website.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrder%2520Matters%253A%25203D%2520Shape%2520Generation%2520from%2520Sequential%2520VR%2520Sketches%26entry.906535625%3DYizi%2520Chen%2520and%2520Sidi%2520Wu%2520and%2520Tianyi%2520Xiao%2520and%2520Nina%2520Wiedemann%2520and%2520Loic%2520Landrieu%26entry.1292438233%3DVR%2520sketching%2520lets%2520users%2520explore%2520and%2520iterate%2520on%2520ideas%2520directly%2520in%25203D%252C%2520offering%2520a%2520faster%2520and%2520more%2520intuitive%2520alternative%2520to%2520conventional%2520CAD%2520tools.%2520However%252C%2520existing%2520sketch-to-shape%2520models%2520ignore%2520the%2520temporal%2520ordering%2520of%2520strokes%252C%2520discarding%2520crucial%2520cues%2520about%2520structure%2520and%2520design%2520intent.%2520We%2520introduce%2520VRSketch2Shape%252C%2520the%2520first%2520framework%2520and%2520multi-category%2520dataset%2520for%2520generating%25203D%2520shapes%2520from%2520sequential%2520VR%2520sketches.%2520Our%2520contributions%2520are%2520threefold%253A%2520%2528i%2529%2520an%2520automated%2520pipeline%2520that%2520generates%2520sequential%2520VR%2520sketches%2520from%2520arbitrary%2520shapes%252C%2520%2528ii%2529%2520a%2520dataset%2520of%2520over%252020k%2520synthetic%2520and%2520900%2520hand-drawn%2520sketch-shape%2520pairs%2520across%2520four%2520categories%252C%2520and%2520%2528iii%2529%2520an%2520order-aware%2520sketch%2520encoder%2520coupled%2520with%2520a%2520diffusion-based%25203D%2520generator.%2520Our%2520approach%2520yields%2520higher%2520geometric%2520fidelity%2520than%2520prior%2520work%252C%2520generalizes%2520effectively%2520from%2520synthetic%2520to%2520real%2520sketches%2520with%2520minimal%2520supervision%252C%2520and%2520performs%2520well%2520even%2520on%2520partial%2520sketches.%2520All%2520data%2520and%2520models%2520will%2520be%2520released%2520open-source%2520at%2520https%253A//chenyizi086.github.io/VRSketch2Shape_website.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Order%20Matters%3A%203D%20Shape%20Generation%20from%20Sequential%20VR%20Sketches&entry.906535625=Yizi%20Chen%20and%20Sidi%20Wu%20and%20Tianyi%20Xiao%20and%20Nina%20Wiedemann%20and%20Loic%20Landrieu&entry.1292438233=VR%20sketching%20lets%20users%20explore%20and%20iterate%20on%20ideas%20directly%20in%203D%2C%20offering%20a%20faster%20and%20more%20intuitive%20alternative%20to%20conventional%20CAD%20tools.%20However%2C%20existing%20sketch-to-shape%20models%20ignore%20the%20temporal%20ordering%20of%20strokes%2C%20discarding%20crucial%20cues%20about%20structure%20and%20design%20intent.%20We%20introduce%20VRSketch2Shape%2C%20the%20first%20framework%20and%20multi-category%20dataset%20for%20generating%203D%20shapes%20from%20sequential%20VR%20sketches.%20Our%20contributions%20are%20threefold%3A%20%28i%29%20an%20automated%20pipeline%20that%20generates%20sequential%20VR%20sketches%20from%20arbitrary%20shapes%2C%20%28ii%29%20a%20dataset%20of%20over%2020k%20synthetic%20and%20900%20hand-drawn%20sketch-shape%20pairs%20across%20four%20categories%2C%20and%20%28iii%29%20an%20order-aware%20sketch%20encoder%20coupled%20with%20a%20diffusion-based%203D%20generator.%20Our%20approach%20yields%20higher%20geometric%20fidelity%20than%20prior%20work%2C%20generalizes%20effectively%20from%20synthetic%20to%20real%20sketches%20with%20minimal%20supervision%2C%20and%20performs%20well%20even%20on%20partial%20sketches.%20All%20data%20and%20models%20will%20be%20released%20open-source%20at%20https%3A//chenyizi086.github.io/VRSketch2Shape_website.&entry.1838667208=http%3A//arxiv.org/abs/2512.04761v1&entry.124074799=Read"},
{"title": "DAVE: Diagnostic benchmark for Audio Visual Evaluation", "author": "Gorjan Radevski and Teodora Popordanoska and Matthew B. Blaschko and Tinne Tuytelaars", "abstract": "Audio-visual understanding is a rapidly evolving field that seeks to integrate and interpret information from both auditory and visual modalities. Despite recent advances in multi-modal learning, existing benchmarks often suffer from strong visual bias -- when answers can be inferred from visual data alone -- and provide only aggregate scores that conflate multiple sources of error. This makes it difficult to determine whether models struggle with visual understanding, audio interpretation, or audio-visual alignment. In this work, we introduce DAVE: Diagnostic Audio Visual Evaluation, a novel benchmark dataset designed to systematically evaluate audio-visual models across controlled settings. DAVE alleviates existing limitations by (i) ensuring both modalities are necessary to answer correctly and (ii) decoupling evaluation into atomic subcategories. Our detailed analysis of state-of-the-art models reveals specific failure modes and provides targeted insights for improvement. By offering this standardized diagnostic framework, we aim to facilitate more robust development of audio-visual models.\n  Dataset: https://huggingface.co/datasets/gorjanradevski/dave\n  Code: https://github.com/gorjanradevski/dave", "link": "http://arxiv.org/abs/2503.09321v2", "date": "2025-12-04", "relevancy": 2.8577, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAVE%3A%20Diagnostic%20benchmark%20for%20Audio%20Visual%20Evaluation&body=Title%3A%20DAVE%3A%20Diagnostic%20benchmark%20for%20Audio%20Visual%20Evaluation%0AAuthor%3A%20Gorjan%20Radevski%20and%20Teodora%20Popordanoska%20and%20Matthew%20B.%20Blaschko%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20Audio-visual%20understanding%20is%20a%20rapidly%20evolving%20field%20that%20seeks%20to%20integrate%20and%20interpret%20information%20from%20both%20auditory%20and%20visual%20modalities.%20Despite%20recent%20advances%20in%20multi-modal%20learning%2C%20existing%20benchmarks%20often%20suffer%20from%20strong%20visual%20bias%20--%20when%20answers%20can%20be%20inferred%20from%20visual%20data%20alone%20--%20and%20provide%20only%20aggregate%20scores%20that%20conflate%20multiple%20sources%20of%20error.%20This%20makes%20it%20difficult%20to%20determine%20whether%20models%20struggle%20with%20visual%20understanding%2C%20audio%20interpretation%2C%20or%20audio-visual%20alignment.%20In%20this%20work%2C%20we%20introduce%20DAVE%3A%20Diagnostic%20Audio%20Visual%20Evaluation%2C%20a%20novel%20benchmark%20dataset%20designed%20to%20systematically%20evaluate%20audio-visual%20models%20across%20controlled%20settings.%20DAVE%20alleviates%20existing%20limitations%20by%20%28i%29%20ensuring%20both%20modalities%20are%20necessary%20to%20answer%20correctly%20and%20%28ii%29%20decoupling%20evaluation%20into%20atomic%20subcategories.%20Our%20detailed%20analysis%20of%20state-of-the-art%20models%20reveals%20specific%20failure%20modes%20and%20provides%20targeted%20insights%20for%20improvement.%20By%20offering%20this%20standardized%20diagnostic%20framework%2C%20we%20aim%20to%20facilitate%20more%20robust%20development%20of%20audio-visual%20models.%0A%20%20Dataset%3A%20https%3A//huggingface.co/datasets/gorjanradevski/dave%0A%20%20Code%3A%20https%3A//github.com/gorjanradevski/dave%0ALink%3A%20http%3A//arxiv.org/abs/2503.09321v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAVE%253A%2520Diagnostic%2520benchmark%2520for%2520Audio%2520Visual%2520Evaluation%26entry.906535625%3DGorjan%2520Radevski%2520and%2520Teodora%2520Popordanoska%2520and%2520Matthew%2520B.%2520Blaschko%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3DAudio-visual%2520understanding%2520is%2520a%2520rapidly%2520evolving%2520field%2520that%2520seeks%2520to%2520integrate%2520and%2520interpret%2520information%2520from%2520both%2520auditory%2520and%2520visual%2520modalities.%2520Despite%2520recent%2520advances%2520in%2520multi-modal%2520learning%252C%2520existing%2520benchmarks%2520often%2520suffer%2520from%2520strong%2520visual%2520bias%2520--%2520when%2520answers%2520can%2520be%2520inferred%2520from%2520visual%2520data%2520alone%2520--%2520and%2520provide%2520only%2520aggregate%2520scores%2520that%2520conflate%2520multiple%2520sources%2520of%2520error.%2520This%2520makes%2520it%2520difficult%2520to%2520determine%2520whether%2520models%2520struggle%2520with%2520visual%2520understanding%252C%2520audio%2520interpretation%252C%2520or%2520audio-visual%2520alignment.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DAVE%253A%2520Diagnostic%2520Audio%2520Visual%2520Evaluation%252C%2520a%2520novel%2520benchmark%2520dataset%2520designed%2520to%2520systematically%2520evaluate%2520audio-visual%2520models%2520across%2520controlled%2520settings.%2520DAVE%2520alleviates%2520existing%2520limitations%2520by%2520%2528i%2529%2520ensuring%2520both%2520modalities%2520are%2520necessary%2520to%2520answer%2520correctly%2520and%2520%2528ii%2529%2520decoupling%2520evaluation%2520into%2520atomic%2520subcategories.%2520Our%2520detailed%2520analysis%2520of%2520state-of-the-art%2520models%2520reveals%2520specific%2520failure%2520modes%2520and%2520provides%2520targeted%2520insights%2520for%2520improvement.%2520By%2520offering%2520this%2520standardized%2520diagnostic%2520framework%252C%2520we%2520aim%2520to%2520facilitate%2520more%2520robust%2520development%2520of%2520audio-visual%2520models.%250A%2520%2520Dataset%253A%2520https%253A//huggingface.co/datasets/gorjanradevski/dave%250A%2520%2520Code%253A%2520https%253A//github.com/gorjanradevski/dave%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09321v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAVE%3A%20Diagnostic%20benchmark%20for%20Audio%20Visual%20Evaluation&entry.906535625=Gorjan%20Radevski%20and%20Teodora%20Popordanoska%20and%20Matthew%20B.%20Blaschko%20and%20Tinne%20Tuytelaars&entry.1292438233=Audio-visual%20understanding%20is%20a%20rapidly%20evolving%20field%20that%20seeks%20to%20integrate%20and%20interpret%20information%20from%20both%20auditory%20and%20visual%20modalities.%20Despite%20recent%20advances%20in%20multi-modal%20learning%2C%20existing%20benchmarks%20often%20suffer%20from%20strong%20visual%20bias%20--%20when%20answers%20can%20be%20inferred%20from%20visual%20data%20alone%20--%20and%20provide%20only%20aggregate%20scores%20that%20conflate%20multiple%20sources%20of%20error.%20This%20makes%20it%20difficult%20to%20determine%20whether%20models%20struggle%20with%20visual%20understanding%2C%20audio%20interpretation%2C%20or%20audio-visual%20alignment.%20In%20this%20work%2C%20we%20introduce%20DAVE%3A%20Diagnostic%20Audio%20Visual%20Evaluation%2C%20a%20novel%20benchmark%20dataset%20designed%20to%20systematically%20evaluate%20audio-visual%20models%20across%20controlled%20settings.%20DAVE%20alleviates%20existing%20limitations%20by%20%28i%29%20ensuring%20both%20modalities%20are%20necessary%20to%20answer%20correctly%20and%20%28ii%29%20decoupling%20evaluation%20into%20atomic%20subcategories.%20Our%20detailed%20analysis%20of%20state-of-the-art%20models%20reveals%20specific%20failure%20modes%20and%20provides%20targeted%20insights%20for%20improvement.%20By%20offering%20this%20standardized%20diagnostic%20framework%2C%20we%20aim%20to%20facilitate%20more%20robust%20development%20of%20audio-visual%20models.%0A%20%20Dataset%3A%20https%3A//huggingface.co/datasets/gorjanradevski/dave%0A%20%20Code%3A%20https%3A//github.com/gorjanradevski/dave&entry.1838667208=http%3A//arxiv.org/abs/2503.09321v2&entry.124074799=Read"},
{"title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "author": "Zuhao Yang and Sudong Wang and Kaichen Zhang and Keming Wu and Sicong Leng and Yifan Zhang and Bo Li and Chengwei Qin and Shijian Lu and Xingxuan Li and Lidong Bing", "abstract": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "link": "http://arxiv.org/abs/2511.20785v2", "date": "2025-12-04", "relevancy": 2.8309, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVT%3A%20Incentivizing%20%22Thinking%20with%20Long%20Videos%22%20via%20Native%20Tool%20Calling&body=Title%3A%20LongVT%3A%20Incentivizing%20%22Thinking%20with%20Long%20Videos%22%20via%20Native%20Tool%20Calling%0AAuthor%3A%20Zuhao%20Yang%20and%20Sudong%20Wang%20and%20Kaichen%20Zhang%20and%20Keming%20Wu%20and%20Sicong%20Leng%20and%20Yifan%20Zhang%20and%20Bo%20Li%20and%20Chengwei%20Qin%20and%20Shijian%20Lu%20and%20Xingxuan%20Li%20and%20Lidong%20Bing%0AAbstract%3A%20Large%20multimodal%20models%20%28LMMs%29%20have%20shown%20great%20potential%20for%20video%20reasoning%20with%20textual%20Chain-of-Thought.%20However%2C%20they%20remain%20vulnerable%20to%20hallucinations%2C%20especially%20when%20processing%20long-form%20videos%20where%20evidence%20is%20sparse%20and%20temporally%20dispersed.%20Inspired%20by%20how%20humans%20comprehend%20long%20videos%20-%20by%20first%20skimming%20globally%20and%20then%20examining%20relevant%20clips%20for%20details%20-%20we%20introduce%20LongVT%2C%20an%20end-to-end%20agentic%20framework%20that%20enables%20%22Thinking%20with%20Long%20Videos%22%20via%20interleaved%20Multimodal%20Chain-of-Tool-Thought.%20Specifically%2C%20we%20exploit%20LMMs%27%20inherent%20temporal%20grounding%20ability%20as%20a%20native%20video%20cropping%20tool%20to%20zoom%20in%20on%20a%20specific%20video%20clip%20and%20resample%20finer-grained%20video%20frames.%20This%20global-to-local%20reasoning%20loop%20continues%20until%20answers%20are%20grounded%20in%20retrieved%20visual%20evidence.%20Given%20the%20scarcity%20of%20fine-grained%20question-answering%20%28QA%29%20data%20for%20the%20long%20video%20reasoning%20task%2C%20we%20curate%20and%20will%20release%20a%20data%20suite%20named%20VideoSIAH%20to%20facilitate%20both%20training%20and%20evaluation.%20Specifically%2C%20our%20training%20dataset%20consists%20of%20247.9K%20samples%20for%20tool-integrated%20cold-start%20supervised%20fine-tuning%2C%201.6K%20samples%20for%20agentic%20reinforcement%20learning%2C%20and%2015.4K%20samples%20for%20agentic%20reinforcement%20fine-tuning%2C%20respectively.%20Our%20evaluation%20benchmark%20consists%20of%201%2C280%20QA%20pairs%20that%20are%20carefully%20curated%20through%20a%20semi-automatic%20data%20pipeline%20with%20human-in-the-loop%20validation.%20With%20a%20meticulously%20designed%20three-stage%20training%20strategy%20and%20extensive%20empirical%20validation%2C%20LongVT%20consistently%20outperforms%20existing%20strong%20baselines%20across%20four%20challenging%20long-video%20understanding%20and%20reasoning%20benchmarks.%20Our%20codes%2C%20data%2C%20and%20model%20checkpoints%20are%20publicly%20available%20at%20https%3A//github.com/EvolvingLMMs-Lab/LongVT%20.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVT%253A%2520Incentivizing%2520%2522Thinking%2520with%2520Long%2520Videos%2522%2520via%2520Native%2520Tool%2520Calling%26entry.906535625%3DZuhao%2520Yang%2520and%2520Sudong%2520Wang%2520and%2520Kaichen%2520Zhang%2520and%2520Keming%2520Wu%2520and%2520Sicong%2520Leng%2520and%2520Yifan%2520Zhang%2520and%2520Bo%2520Li%2520and%2520Chengwei%2520Qin%2520and%2520Shijian%2520Lu%2520and%2520Xingxuan%2520Li%2520and%2520Lidong%2520Bing%26entry.1292438233%3DLarge%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520shown%2520great%2520potential%2520for%2520video%2520reasoning%2520with%2520textual%2520Chain-of-Thought.%2520However%252C%2520they%2520remain%2520vulnerable%2520to%2520hallucinations%252C%2520especially%2520when%2520processing%2520long-form%2520videos%2520where%2520evidence%2520is%2520sparse%2520and%2520temporally%2520dispersed.%2520Inspired%2520by%2520how%2520humans%2520comprehend%2520long%2520videos%2520-%2520by%2520first%2520skimming%2520globally%2520and%2520then%2520examining%2520relevant%2520clips%2520for%2520details%2520-%2520we%2520introduce%2520LongVT%252C%2520an%2520end-to-end%2520agentic%2520framework%2520that%2520enables%2520%2522Thinking%2520with%2520Long%2520Videos%2522%2520via%2520interleaved%2520Multimodal%2520Chain-of-Tool-Thought.%2520Specifically%252C%2520we%2520exploit%2520LMMs%2527%2520inherent%2520temporal%2520grounding%2520ability%2520as%2520a%2520native%2520video%2520cropping%2520tool%2520to%2520zoom%2520in%2520on%2520a%2520specific%2520video%2520clip%2520and%2520resample%2520finer-grained%2520video%2520frames.%2520This%2520global-to-local%2520reasoning%2520loop%2520continues%2520until%2520answers%2520are%2520grounded%2520in%2520retrieved%2520visual%2520evidence.%2520Given%2520the%2520scarcity%2520of%2520fine-grained%2520question-answering%2520%2528QA%2529%2520data%2520for%2520the%2520long%2520video%2520reasoning%2520task%252C%2520we%2520curate%2520and%2520will%2520release%2520a%2520data%2520suite%2520named%2520VideoSIAH%2520to%2520facilitate%2520both%2520training%2520and%2520evaluation.%2520Specifically%252C%2520our%2520training%2520dataset%2520consists%2520of%2520247.9K%2520samples%2520for%2520tool-integrated%2520cold-start%2520supervised%2520fine-tuning%252C%25201.6K%2520samples%2520for%2520agentic%2520reinforcement%2520learning%252C%2520and%252015.4K%2520samples%2520for%2520agentic%2520reinforcement%2520fine-tuning%252C%2520respectively.%2520Our%2520evaluation%2520benchmark%2520consists%2520of%25201%252C280%2520QA%2520pairs%2520that%2520are%2520carefully%2520curated%2520through%2520a%2520semi-automatic%2520data%2520pipeline%2520with%2520human-in-the-loop%2520validation.%2520With%2520a%2520meticulously%2520designed%2520three-stage%2520training%2520strategy%2520and%2520extensive%2520empirical%2520validation%252C%2520LongVT%2520consistently%2520outperforms%2520existing%2520strong%2520baselines%2520across%2520four%2520challenging%2520long-video%2520understanding%2520and%2520reasoning%2520benchmarks.%2520Our%2520codes%252C%2520data%252C%2520and%2520model%2520checkpoints%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/EvolvingLMMs-Lab/LongVT%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVT%3A%20Incentivizing%20%22Thinking%20with%20Long%20Videos%22%20via%20Native%20Tool%20Calling&entry.906535625=Zuhao%20Yang%20and%20Sudong%20Wang%20and%20Kaichen%20Zhang%20and%20Keming%20Wu%20and%20Sicong%20Leng%20and%20Yifan%20Zhang%20and%20Bo%20Li%20and%20Chengwei%20Qin%20and%20Shijian%20Lu%20and%20Xingxuan%20Li%20and%20Lidong%20Bing&entry.1292438233=Large%20multimodal%20models%20%28LMMs%29%20have%20shown%20great%20potential%20for%20video%20reasoning%20with%20textual%20Chain-of-Thought.%20However%2C%20they%20remain%20vulnerable%20to%20hallucinations%2C%20especially%20when%20processing%20long-form%20videos%20where%20evidence%20is%20sparse%20and%20temporally%20dispersed.%20Inspired%20by%20how%20humans%20comprehend%20long%20videos%20-%20by%20first%20skimming%20globally%20and%20then%20examining%20relevant%20clips%20for%20details%20-%20we%20introduce%20LongVT%2C%20an%20end-to-end%20agentic%20framework%20that%20enables%20%22Thinking%20with%20Long%20Videos%22%20via%20interleaved%20Multimodal%20Chain-of-Tool-Thought.%20Specifically%2C%20we%20exploit%20LMMs%27%20inherent%20temporal%20grounding%20ability%20as%20a%20native%20video%20cropping%20tool%20to%20zoom%20in%20on%20a%20specific%20video%20clip%20and%20resample%20finer-grained%20video%20frames.%20This%20global-to-local%20reasoning%20loop%20continues%20until%20answers%20are%20grounded%20in%20retrieved%20visual%20evidence.%20Given%20the%20scarcity%20of%20fine-grained%20question-answering%20%28QA%29%20data%20for%20the%20long%20video%20reasoning%20task%2C%20we%20curate%20and%20will%20release%20a%20data%20suite%20named%20VideoSIAH%20to%20facilitate%20both%20training%20and%20evaluation.%20Specifically%2C%20our%20training%20dataset%20consists%20of%20247.9K%20samples%20for%20tool-integrated%20cold-start%20supervised%20fine-tuning%2C%201.6K%20samples%20for%20agentic%20reinforcement%20learning%2C%20and%2015.4K%20samples%20for%20agentic%20reinforcement%20fine-tuning%2C%20respectively.%20Our%20evaluation%20benchmark%20consists%20of%201%2C280%20QA%20pairs%20that%20are%20carefully%20curated%20through%20a%20semi-automatic%20data%20pipeline%20with%20human-in-the-loop%20validation.%20With%20a%20meticulously%20designed%20three-stage%20training%20strategy%20and%20extensive%20empirical%20validation%2C%20LongVT%20consistently%20outperforms%20existing%20strong%20baselines%20across%20four%20challenging%20long-video%20understanding%20and%20reasoning%20benchmarks.%20Our%20codes%2C%20data%2C%20and%20model%20checkpoints%20are%20publicly%20available%20at%20https%3A//github.com/EvolvingLMMs-Lab/LongVT%20.&entry.1838667208=http%3A//arxiv.org/abs/2511.20785v2&entry.124074799=Read"},
{"title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture", "author": "Xin He and Longhui Wei and Jianbo Ouyang and Lingxi Xie and Qi Tian", "abstract": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.", "link": "http://arxiv.org/abs/2512.04810v1", "date": "2025-12-04", "relevancy": 2.8288, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5774}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture&body=Title%3A%20EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture%0AAuthor%3A%20Xin%20He%20and%20Longhui%20Wei%20and%20Jianbo%20Ouyang%20and%20Lingxi%20Xie%20and%20Qi%20Tian%0AAbstract%3A%20We%20propose%20EMMA%2C%20an%20efficient%20and%20unified%20architecture%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Specifically%2C%20EMMA%20primarily%20consists%20of%201%29%20An%20efficient%20autoencoder%20with%20a%2032x%20compression%20ratio%2C%20which%20significantly%20reduces%20the%20number%20of%20tokens%20required%20for%20generation.%20This%20also%20ensures%20the%20training%20balance%20between%20understanding%20and%20generation%20tasks%20by%20applying%20the%20same%20compression%20ratio%20to%20images.%202%29%20Channel-wise%20concatenation%20instead%20of%20token-wise%20concatenation%20among%20visual%20understanding%20and%20generation%20tokens%2C%20which%20further%20reduces%20the%20visual%20tokens%20in%20unified%20architectures.%203%29%20A%20shared-and-decoupled%20network%20that%20enables%20mutual%20improvements%20across%20tasks%20while%20meeting%20the%20task-specific%20modeling%20requirements.%204%29%20A%20mixture-of-experts%20mechanism%20adopted%20for%20visual%20understanding%20encoder%2C%20which%20substantially%20improves%20perceptual%20capabilities%20with%20a%20few%20parameters%20increase.%20Extensive%20experiments%20have%20shown%20that%20EMMA-4B%20can%20significantly%20outperform%20state-of-the-art%20unified%20multimodal%20approaches%20%28e.g.%2C%20BAGEL-7B%29%20in%20both%20efficiency%20and%20performance%2C%20while%20also%20achieving%20competitive%20results%20compared%20to%20recent%20multimodal%20understanding%20and%20generation%20experts%20%28e.g.%2C%20Qwen3-VL%20and%20Qwen-Image%29.%20We%20believe%20that%20EMMA%20lays%20a%20solid%20foundation%20for%20the%20future%20development%20of%20unified%20multimodal%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMMA%253A%2520Efficient%2520Multimodal%2520Understanding%252C%2520Generation%252C%2520and%2520Editing%2520with%2520a%2520Unified%2520Architecture%26entry.906535625%3DXin%2520He%2520and%2520Longhui%2520Wei%2520and%2520Jianbo%2520Ouyang%2520and%2520Lingxi%2520Xie%2520and%2520Qi%2520Tian%26entry.1292438233%3DWe%2520propose%2520EMMA%252C%2520an%2520efficient%2520and%2520unified%2520architecture%2520for%2520multimodal%2520understanding%252C%2520generation%2520and%2520editing.%2520Specifically%252C%2520EMMA%2520primarily%2520consists%2520of%25201%2529%2520An%2520efficient%2520autoencoder%2520with%2520a%252032x%2520compression%2520ratio%252C%2520which%2520significantly%2520reduces%2520the%2520number%2520of%2520tokens%2520required%2520for%2520generation.%2520This%2520also%2520ensures%2520the%2520training%2520balance%2520between%2520understanding%2520and%2520generation%2520tasks%2520by%2520applying%2520the%2520same%2520compression%2520ratio%2520to%2520images.%25202%2529%2520Channel-wise%2520concatenation%2520instead%2520of%2520token-wise%2520concatenation%2520among%2520visual%2520understanding%2520and%2520generation%2520tokens%252C%2520which%2520further%2520reduces%2520the%2520visual%2520tokens%2520in%2520unified%2520architectures.%25203%2529%2520A%2520shared-and-decoupled%2520network%2520that%2520enables%2520mutual%2520improvements%2520across%2520tasks%2520while%2520meeting%2520the%2520task-specific%2520modeling%2520requirements.%25204%2529%2520A%2520mixture-of-experts%2520mechanism%2520adopted%2520for%2520visual%2520understanding%2520encoder%252C%2520which%2520substantially%2520improves%2520perceptual%2520capabilities%2520with%2520a%2520few%2520parameters%2520increase.%2520Extensive%2520experiments%2520have%2520shown%2520that%2520EMMA-4B%2520can%2520significantly%2520outperform%2520state-of-the-art%2520unified%2520multimodal%2520approaches%2520%2528e.g.%252C%2520BAGEL-7B%2529%2520in%2520both%2520efficiency%2520and%2520performance%252C%2520while%2520also%2520achieving%2520competitive%2520results%2520compared%2520to%2520recent%2520multimodal%2520understanding%2520and%2520generation%2520experts%2520%2528e.g.%252C%2520Qwen3-VL%2520and%2520Qwen-Image%2529.%2520We%2520believe%2520that%2520EMMA%2520lays%2520a%2520solid%2520foundation%2520for%2520the%2520future%2520development%2520of%2520unified%2520multimodal%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture&entry.906535625=Xin%20He%20and%20Longhui%20Wei%20and%20Jianbo%20Ouyang%20and%20Lingxi%20Xie%20and%20Qi%20Tian&entry.1292438233=We%20propose%20EMMA%2C%20an%20efficient%20and%20unified%20architecture%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Specifically%2C%20EMMA%20primarily%20consists%20of%201%29%20An%20efficient%20autoencoder%20with%20a%2032x%20compression%20ratio%2C%20which%20significantly%20reduces%20the%20number%20of%20tokens%20required%20for%20generation.%20This%20also%20ensures%20the%20training%20balance%20between%20understanding%20and%20generation%20tasks%20by%20applying%20the%20same%20compression%20ratio%20to%20images.%202%29%20Channel-wise%20concatenation%20instead%20of%20token-wise%20concatenation%20among%20visual%20understanding%20and%20generation%20tokens%2C%20which%20further%20reduces%20the%20visual%20tokens%20in%20unified%20architectures.%203%29%20A%20shared-and-decoupled%20network%20that%20enables%20mutual%20improvements%20across%20tasks%20while%20meeting%20the%20task-specific%20modeling%20requirements.%204%29%20A%20mixture-of-experts%20mechanism%20adopted%20for%20visual%20understanding%20encoder%2C%20which%20substantially%20improves%20perceptual%20capabilities%20with%20a%20few%20parameters%20increase.%20Extensive%20experiments%20have%20shown%20that%20EMMA-4B%20can%20significantly%20outperform%20state-of-the-art%20unified%20multimodal%20approaches%20%28e.g.%2C%20BAGEL-7B%29%20in%20both%20efficiency%20and%20performance%2C%20while%20also%20achieving%20competitive%20results%20compared%20to%20recent%20multimodal%20understanding%20and%20generation%20experts%20%28e.g.%2C%20Qwen3-VL%20and%20Qwen-Image%29.%20We%20believe%20that%20EMMA%20lays%20a%20solid%20foundation%20for%20the%20future%20development%20of%20unified%20multimodal%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2512.04810v1&entry.124074799=Read"},
{"title": "EVE: Towards End-to-End Video Subtitle Extraction with Vision-Language Models", "author": "Haiyang Yu and Mengyang Zhao and Jinghui Lu and Ke Niu and Yanjie Wang and Weijie Yin and Weitao Jia and Teng Fu and Yang Liu and Jun Liu and Hong Chen", "abstract": "Video subtitles play a crucial role in short videos and movies, as they not only help models better understand video content but also support applications such as video translation and content retrieval. Existing video subtitle extraction methods typically rely on multi-stage frameworks, where errors accumulate across stages and temporal dependencies are underutilized due to frame-wise processing. Moreover, although some Large Vision-Language Models (LVLMs) possess strong OCR capabilities, predicting accurate timestamps for subtitle texts remains challenging. To this end, we propose an End-to-end Video subtitle Extraction framework based on LVLMs, named EVE, which can output subtitles and their timestamps simultaneously. Specifically, we introduce a dual-branch Spatiotemporal Subtitle-Salient (S\\textsuperscript{3}) Module that serves as an adapter for LVLMs, capable of representing subtitle-related content and considering inter-frame correlations using only a small number of tokens. Within this module, the Spatial Semantic Context Aggregate branch aggregates high-level global semantics to provide spatial visual contextual information, while the Temporal Subtitle Token Query branch explicitly queries subtitle-relevant tokens while considering temporal correlation across frames. The small number of tokens retained by the S\\textsuperscript{3} module are fed to the language model, which then directly outputs the subtitle text along with its timestamps. Furthermore, we construct the first large-scale dataset dedicated to video subtitle extraction, ViSa, containing over 2.5M videos with timestamped and bilingual annotation, thereby providing the community with a well-organized training and evaluation benchmark.", "link": "http://arxiv.org/abs/2503.04058v2", "date": "2025-12-04", "relevancy": 2.8286, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5734}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVE%3A%20Towards%20End-to-End%20Video%20Subtitle%20Extraction%20with%20Vision-Language%20Models&body=Title%3A%20EVE%3A%20Towards%20End-to-End%20Video%20Subtitle%20Extraction%20with%20Vision-Language%20Models%0AAuthor%3A%20Haiyang%20Yu%20and%20Mengyang%20Zhao%20and%20Jinghui%20Lu%20and%20Ke%20Niu%20and%20Yanjie%20Wang%20and%20Weijie%20Yin%20and%20Weitao%20Jia%20and%20Teng%20Fu%20and%20Yang%20Liu%20and%20Jun%20Liu%20and%20Hong%20Chen%0AAbstract%3A%20Video%20subtitles%20play%20a%20crucial%20role%20in%20short%20videos%20and%20movies%2C%20as%20they%20not%20only%20help%20models%20better%20understand%20video%20content%20but%20also%20support%20applications%20such%20as%20video%20translation%20and%20content%20retrieval.%20Existing%20video%20subtitle%20extraction%20methods%20typically%20rely%20on%20multi-stage%20frameworks%2C%20where%20errors%20accumulate%20across%20stages%20and%20temporal%20dependencies%20are%20underutilized%20due%20to%20frame-wise%20processing.%20Moreover%2C%20although%20some%20Large%20Vision-Language%20Models%20%28LVLMs%29%20possess%20strong%20OCR%20capabilities%2C%20predicting%20accurate%20timestamps%20for%20subtitle%20texts%20remains%20challenging.%20To%20this%20end%2C%20we%20propose%20an%20End-to-end%20Video%20subtitle%20Extraction%20framework%20based%20on%20LVLMs%2C%20named%20EVE%2C%20which%20can%20output%20subtitles%20and%20their%20timestamps%20simultaneously.%20Specifically%2C%20we%20introduce%20a%20dual-branch%20Spatiotemporal%20Subtitle-Salient%20%28S%5Ctextsuperscript%7B3%7D%29%20Module%20that%20serves%20as%20an%20adapter%20for%20LVLMs%2C%20capable%20of%20representing%20subtitle-related%20content%20and%20considering%20inter-frame%20correlations%20using%20only%20a%20small%20number%20of%20tokens.%20Within%20this%20module%2C%20the%20Spatial%20Semantic%20Context%20Aggregate%20branch%20aggregates%20high-level%20global%20semantics%20to%20provide%20spatial%20visual%20contextual%20information%2C%20while%20the%20Temporal%20Subtitle%20Token%20Query%20branch%20explicitly%20queries%20subtitle-relevant%20tokens%20while%20considering%20temporal%20correlation%20across%20frames.%20The%20small%20number%20of%20tokens%20retained%20by%20the%20S%5Ctextsuperscript%7B3%7D%20module%20are%20fed%20to%20the%20language%20model%2C%20which%20then%20directly%20outputs%20the%20subtitle%20text%20along%20with%20its%20timestamps.%20Furthermore%2C%20we%20construct%20the%20first%20large-scale%20dataset%20dedicated%20to%20video%20subtitle%20extraction%2C%20ViSa%2C%20containing%20over%202.5M%20videos%20with%20timestamped%20and%20bilingual%20annotation%2C%20thereby%20providing%20the%20community%20with%20a%20well-organized%20training%20and%20evaluation%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2503.04058v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVE%253A%2520Towards%2520End-to-End%2520Video%2520Subtitle%2520Extraction%2520with%2520Vision-Language%2520Models%26entry.906535625%3DHaiyang%2520Yu%2520and%2520Mengyang%2520Zhao%2520and%2520Jinghui%2520Lu%2520and%2520Ke%2520Niu%2520and%2520Yanjie%2520Wang%2520and%2520Weijie%2520Yin%2520and%2520Weitao%2520Jia%2520and%2520Teng%2520Fu%2520and%2520Yang%2520Liu%2520and%2520Jun%2520Liu%2520and%2520Hong%2520Chen%26entry.1292438233%3DVideo%2520subtitles%2520play%2520a%2520crucial%2520role%2520in%2520short%2520videos%2520and%2520movies%252C%2520as%2520they%2520not%2520only%2520help%2520models%2520better%2520understand%2520video%2520content%2520but%2520also%2520support%2520applications%2520such%2520as%2520video%2520translation%2520and%2520content%2520retrieval.%2520Existing%2520video%2520subtitle%2520extraction%2520methods%2520typically%2520rely%2520on%2520multi-stage%2520frameworks%252C%2520where%2520errors%2520accumulate%2520across%2520stages%2520and%2520temporal%2520dependencies%2520are%2520underutilized%2520due%2520to%2520frame-wise%2520processing.%2520Moreover%252C%2520although%2520some%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520possess%2520strong%2520OCR%2520capabilities%252C%2520predicting%2520accurate%2520timestamps%2520for%2520subtitle%2520texts%2520remains%2520challenging.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520End-to-end%2520Video%2520subtitle%2520Extraction%2520framework%2520based%2520on%2520LVLMs%252C%2520named%2520EVE%252C%2520which%2520can%2520output%2520subtitles%2520and%2520their%2520timestamps%2520simultaneously.%2520Specifically%252C%2520we%2520introduce%2520a%2520dual-branch%2520Spatiotemporal%2520Subtitle-Salient%2520%2528S%255Ctextsuperscript%257B3%257D%2529%2520Module%2520that%2520serves%2520as%2520an%2520adapter%2520for%2520LVLMs%252C%2520capable%2520of%2520representing%2520subtitle-related%2520content%2520and%2520considering%2520inter-frame%2520correlations%2520using%2520only%2520a%2520small%2520number%2520of%2520tokens.%2520Within%2520this%2520module%252C%2520the%2520Spatial%2520Semantic%2520Context%2520Aggregate%2520branch%2520aggregates%2520high-level%2520global%2520semantics%2520to%2520provide%2520spatial%2520visual%2520contextual%2520information%252C%2520while%2520the%2520Temporal%2520Subtitle%2520Token%2520Query%2520branch%2520explicitly%2520queries%2520subtitle-relevant%2520tokens%2520while%2520considering%2520temporal%2520correlation%2520across%2520frames.%2520The%2520small%2520number%2520of%2520tokens%2520retained%2520by%2520the%2520S%255Ctextsuperscript%257B3%257D%2520module%2520are%2520fed%2520to%2520the%2520language%2520model%252C%2520which%2520then%2520directly%2520outputs%2520the%2520subtitle%2520text%2520along%2520with%2520its%2520timestamps.%2520Furthermore%252C%2520we%2520construct%2520the%2520first%2520large-scale%2520dataset%2520dedicated%2520to%2520video%2520subtitle%2520extraction%252C%2520ViSa%252C%2520containing%2520over%25202.5M%2520videos%2520with%2520timestamped%2520and%2520bilingual%2520annotation%252C%2520thereby%2520providing%2520the%2520community%2520with%2520a%2520well-organized%2520training%2520and%2520evaluation%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04058v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVE%3A%20Towards%20End-to-End%20Video%20Subtitle%20Extraction%20with%20Vision-Language%20Models&entry.906535625=Haiyang%20Yu%20and%20Mengyang%20Zhao%20and%20Jinghui%20Lu%20and%20Ke%20Niu%20and%20Yanjie%20Wang%20and%20Weijie%20Yin%20and%20Weitao%20Jia%20and%20Teng%20Fu%20and%20Yang%20Liu%20and%20Jun%20Liu%20and%20Hong%20Chen&entry.1292438233=Video%20subtitles%20play%20a%20crucial%20role%20in%20short%20videos%20and%20movies%2C%20as%20they%20not%20only%20help%20models%20better%20understand%20video%20content%20but%20also%20support%20applications%20such%20as%20video%20translation%20and%20content%20retrieval.%20Existing%20video%20subtitle%20extraction%20methods%20typically%20rely%20on%20multi-stage%20frameworks%2C%20where%20errors%20accumulate%20across%20stages%20and%20temporal%20dependencies%20are%20underutilized%20due%20to%20frame-wise%20processing.%20Moreover%2C%20although%20some%20Large%20Vision-Language%20Models%20%28LVLMs%29%20possess%20strong%20OCR%20capabilities%2C%20predicting%20accurate%20timestamps%20for%20subtitle%20texts%20remains%20challenging.%20To%20this%20end%2C%20we%20propose%20an%20End-to-end%20Video%20subtitle%20Extraction%20framework%20based%20on%20LVLMs%2C%20named%20EVE%2C%20which%20can%20output%20subtitles%20and%20their%20timestamps%20simultaneously.%20Specifically%2C%20we%20introduce%20a%20dual-branch%20Spatiotemporal%20Subtitle-Salient%20%28S%5Ctextsuperscript%7B3%7D%29%20Module%20that%20serves%20as%20an%20adapter%20for%20LVLMs%2C%20capable%20of%20representing%20subtitle-related%20content%20and%20considering%20inter-frame%20correlations%20using%20only%20a%20small%20number%20of%20tokens.%20Within%20this%20module%2C%20the%20Spatial%20Semantic%20Context%20Aggregate%20branch%20aggregates%20high-level%20global%20semantics%20to%20provide%20spatial%20visual%20contextual%20information%2C%20while%20the%20Temporal%20Subtitle%20Token%20Query%20branch%20explicitly%20queries%20subtitle-relevant%20tokens%20while%20considering%20temporal%20correlation%20across%20frames.%20The%20small%20number%20of%20tokens%20retained%20by%20the%20S%5Ctextsuperscript%7B3%7D%20module%20are%20fed%20to%20the%20language%20model%2C%20which%20then%20directly%20outputs%20the%20subtitle%20text%20along%20with%20its%20timestamps.%20Furthermore%2C%20we%20construct%20the%20first%20large-scale%20dataset%20dedicated%20to%20video%20subtitle%20extraction%2C%20ViSa%2C%20containing%20over%202.5M%20videos%20with%20timestamped%20and%20bilingual%20annotation%2C%20thereby%20providing%20the%20community%20with%20a%20well-organized%20training%20and%20evaluation%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2503.04058v2&entry.124074799=Read"},
{"title": "GeoPE:A Unified Geometric Positional Embedding for Structured Tensors", "author": "Yupu Yao and Bowen Yang", "abstract": "Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.", "link": "http://arxiv.org/abs/2512.04963v1", "date": "2025-12-04", "relevancy": 2.8177, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5783}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5641}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoPE%3AA%20Unified%20Geometric%20Positional%20Embedding%20for%20Structured%20Tensors&body=Title%3A%20GeoPE%3AA%20Unified%20Geometric%20Positional%20Embedding%20for%20Structured%20Tensors%0AAuthor%3A%20Yupu%20Yao%20and%20Bowen%20Yang%0AAbstract%3A%20Standard%20Vision%20Transformers%20flatten%202D%20images%20into%201D%20sequences%2C%20disrupting%20the%20natural%20spatial%20topology.%20While%20Rotary%20Positional%20Embedding%20%28RoPE%29%20excels%20in%201D%2C%20it%20inherits%20this%20limitation%2C%20often%20treating%20spatially%20distant%20patches%20%28e.g.%2C%20at%20row%20edges%29%20as%20sequence%20neighbors.%20Existing%202D%20approaches%20typically%20treat%20spatial%20axes%20independently%2C%20failing%20to%20decouple%20this%20false%20sequential%20proximity%20from%20true%20spatial%20distance.%20To%20restore%20the%202D%20spatial%20manifold%2C%20we%20introduce%20Geometric%20Positional%20Embedding%20%28GeoPE%29%2C%20a%20framework%20that%20extends%20rotations%20to%203D%20Euclidean%20space%20using%20quaternions.%20To%20overcome%20non-commutativity%20and%20ensure%20symmetry%2C%20GeoPE%20constructs%20a%20unified%20rotational%20operator%20by%20computing%20the%20geometric%20mean%20in%20the%20Lie%20algebra.%20This%20creates%20a%20geometrically%20coupled%20encoding%20that%20effectively%20separates%20spatial%20dimensions.%20Extensive%20experiments%20on%20image%20classification%2C%20object%20detection%2C%20and%203D%20semantic%20segmentation%20demonstrate%20that%20GeoPE%20consistently%20outperforms%20existing%202D%20RoPE%20variants%20and%20significantly%20enhances%20shape%20bias%2C%20confirming%20its%20ability%20to%20capture%20true%20geometric%20structure.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoPE%253AA%2520Unified%2520Geometric%2520Positional%2520Embedding%2520for%2520Structured%2520Tensors%26entry.906535625%3DYupu%2520Yao%2520and%2520Bowen%2520Yang%26entry.1292438233%3DStandard%2520Vision%2520Transformers%2520flatten%25202D%2520images%2520into%25201D%2520sequences%252C%2520disrupting%2520the%2520natural%2520spatial%2520topology.%2520While%2520Rotary%2520Positional%2520Embedding%2520%2528RoPE%2529%2520excels%2520in%25201D%252C%2520it%2520inherits%2520this%2520limitation%252C%2520often%2520treating%2520spatially%2520distant%2520patches%2520%2528e.g.%252C%2520at%2520row%2520edges%2529%2520as%2520sequence%2520neighbors.%2520Existing%25202D%2520approaches%2520typically%2520treat%2520spatial%2520axes%2520independently%252C%2520failing%2520to%2520decouple%2520this%2520false%2520sequential%2520proximity%2520from%2520true%2520spatial%2520distance.%2520To%2520restore%2520the%25202D%2520spatial%2520manifold%252C%2520we%2520introduce%2520Geometric%2520Positional%2520Embedding%2520%2528GeoPE%2529%252C%2520a%2520framework%2520that%2520extends%2520rotations%2520to%25203D%2520Euclidean%2520space%2520using%2520quaternions.%2520To%2520overcome%2520non-commutativity%2520and%2520ensure%2520symmetry%252C%2520GeoPE%2520constructs%2520a%2520unified%2520rotational%2520operator%2520by%2520computing%2520the%2520geometric%2520mean%2520in%2520the%2520Lie%2520algebra.%2520This%2520creates%2520a%2520geometrically%2520coupled%2520encoding%2520that%2520effectively%2520separates%2520spatial%2520dimensions.%2520Extensive%2520experiments%2520on%2520image%2520classification%252C%2520object%2520detection%252C%2520and%25203D%2520semantic%2520segmentation%2520demonstrate%2520that%2520GeoPE%2520consistently%2520outperforms%2520existing%25202D%2520RoPE%2520variants%2520and%2520significantly%2520enhances%2520shape%2520bias%252C%2520confirming%2520its%2520ability%2520to%2520capture%2520true%2520geometric%2520structure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoPE%3AA%20Unified%20Geometric%20Positional%20Embedding%20for%20Structured%20Tensors&entry.906535625=Yupu%20Yao%20and%20Bowen%20Yang&entry.1292438233=Standard%20Vision%20Transformers%20flatten%202D%20images%20into%201D%20sequences%2C%20disrupting%20the%20natural%20spatial%20topology.%20While%20Rotary%20Positional%20Embedding%20%28RoPE%29%20excels%20in%201D%2C%20it%20inherits%20this%20limitation%2C%20often%20treating%20spatially%20distant%20patches%20%28e.g.%2C%20at%20row%20edges%29%20as%20sequence%20neighbors.%20Existing%202D%20approaches%20typically%20treat%20spatial%20axes%20independently%2C%20failing%20to%20decouple%20this%20false%20sequential%20proximity%20from%20true%20spatial%20distance.%20To%20restore%20the%202D%20spatial%20manifold%2C%20we%20introduce%20Geometric%20Positional%20Embedding%20%28GeoPE%29%2C%20a%20framework%20that%20extends%20rotations%20to%203D%20Euclidean%20space%20using%20quaternions.%20To%20overcome%20non-commutativity%20and%20ensure%20symmetry%2C%20GeoPE%20constructs%20a%20unified%20rotational%20operator%20by%20computing%20the%20geometric%20mean%20in%20the%20Lie%20algebra.%20This%20creates%20a%20geometrically%20coupled%20encoding%20that%20effectively%20separates%20spatial%20dimensions.%20Extensive%20experiments%20on%20image%20classification%2C%20object%20detection%2C%20and%203D%20semantic%20segmentation%20demonstrate%20that%20GeoPE%20consistently%20outperforms%20existing%202D%20RoPE%20variants%20and%20significantly%20enhances%20shape%20bias%2C%20confirming%20its%20ability%20to%20capture%20true%20geometric%20structure.&entry.1838667208=http%3A//arxiv.org/abs/2512.04963v1&entry.124074799=Read"},
{"title": "SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding", "author": "Chang-Hsun Wu and Kai-Po Chang and Yu-Yang Sheng and Hung-Kai Chung and Kuei-Chun Wang and Yu-Chiang Frank Wang", "abstract": "Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.", "link": "http://arxiv.org/abs/2512.04643v1", "date": "2025-12-04", "relevancy": 2.7911, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEASON%3A%20Mitigating%20Temporal%20Hallucination%20in%20Video%20Large%20Language%20Models%20via%20Self-Diagnostic%20Contrastive%20Decoding&body=Title%3A%20SEASON%3A%20Mitigating%20Temporal%20Hallucination%20in%20Video%20Large%20Language%20Models%20via%20Self-Diagnostic%20Contrastive%20Decoding%0AAuthor%3A%20Chang-Hsun%20Wu%20and%20Kai-Po%20Chang%20and%20Yu-Yang%20Sheng%20and%20Hung-Kai%20Chung%20and%20Kuei-Chun%20Wang%20and%20Yu-Chiang%20Frank%20Wang%0AAbstract%3A%20Video%20Large%20Language%20Models%20%28VideoLLMs%29%20have%20shown%20remarkable%20progress%20in%20video%20understanding.%20However%2C%20these%20models%20still%20struggle%20to%20effectively%20perceive%20and%20exploit%20rich%20temporal%20information%20in%20videos%20when%20responding%20to%20user%20queries.%20Therefore%2C%20they%20often%20generate%20descriptions%20of%20events%20that%20are%20temporal%20inconsistent%20or%20causally%20implausible%2C%20causing%20severe%20hallucination%20issues.%20While%20most%20prior%20studies%20have%20focused%20on%20spatial%20hallucinations%20%28e.g.%20object%20mismatches%29%2C%20temporal%20reasoning%20in%20video%20understanding%20remains%20relatively%20underexplored.%20To%20address%20this%20issue%2C%20we%20propose%20Self-Diagnostic%20Contrastive%20Decoding%20%28SEASON%29%2C%20a%20training-free%20method%20that%20adaptively%20enhances%20temporal%20and%20spatial%20faithfulness%20for%20each%20output%20token.%20It%20achieves%20this%20by%20dynamically%20diagnosing%20each%20token%27s%20hallucination%20tendency%20and%20applying%20adaptive%20contrastive%20decoding%20against%20its%20corresponding%20temporal%20and%20spatial%20negatives.%20Extensive%20experiments%20demonstrate%20that%20SEASON%20outperforms%20all%20existing%20training-free%20hallucination%20mitigation%20approaches%20on%20three%20hallucination%20examination%20benchmarks%2C%20while%20further%20improves%20VideoLLMs%20across%20four%20general%20video%20understanding%20benchmarks.%20The%20code%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEASON%253A%2520Mitigating%2520Temporal%2520Hallucination%2520in%2520Video%2520Large%2520Language%2520Models%2520via%2520Self-Diagnostic%2520Contrastive%2520Decoding%26entry.906535625%3DChang-Hsun%2520Wu%2520and%2520Kai-Po%2520Chang%2520and%2520Yu-Yang%2520Sheng%2520and%2520Hung-Kai%2520Chung%2520and%2520Kuei-Chun%2520Wang%2520and%2520Yu-Chiang%2520Frank%2520Wang%26entry.1292438233%3DVideo%2520Large%2520Language%2520Models%2520%2528VideoLLMs%2529%2520have%2520shown%2520remarkable%2520progress%2520in%2520video%2520understanding.%2520However%252C%2520these%2520models%2520still%2520struggle%2520to%2520effectively%2520perceive%2520and%2520exploit%2520rich%2520temporal%2520information%2520in%2520videos%2520when%2520responding%2520to%2520user%2520queries.%2520Therefore%252C%2520they%2520often%2520generate%2520descriptions%2520of%2520events%2520that%2520are%2520temporal%2520inconsistent%2520or%2520causally%2520implausible%252C%2520causing%2520severe%2520hallucination%2520issues.%2520While%2520most%2520prior%2520studies%2520have%2520focused%2520on%2520spatial%2520hallucinations%2520%2528e.g.%2520object%2520mismatches%2529%252C%2520temporal%2520reasoning%2520in%2520video%2520understanding%2520remains%2520relatively%2520underexplored.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Self-Diagnostic%2520Contrastive%2520Decoding%2520%2528SEASON%2529%252C%2520a%2520training-free%2520method%2520that%2520adaptively%2520enhances%2520temporal%2520and%2520spatial%2520faithfulness%2520for%2520each%2520output%2520token.%2520It%2520achieves%2520this%2520by%2520dynamically%2520diagnosing%2520each%2520token%2527s%2520hallucination%2520tendency%2520and%2520applying%2520adaptive%2520contrastive%2520decoding%2520against%2520its%2520corresponding%2520temporal%2520and%2520spatial%2520negatives.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SEASON%2520outperforms%2520all%2520existing%2520training-free%2520hallucination%2520mitigation%2520approaches%2520on%2520three%2520hallucination%2520examination%2520benchmarks%252C%2520while%2520further%2520improves%2520VideoLLMs%2520across%2520four%2520general%2520video%2520understanding%2520benchmarks.%2520The%2520code%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEASON%3A%20Mitigating%20Temporal%20Hallucination%20in%20Video%20Large%20Language%20Models%20via%20Self-Diagnostic%20Contrastive%20Decoding&entry.906535625=Chang-Hsun%20Wu%20and%20Kai-Po%20Chang%20and%20Yu-Yang%20Sheng%20and%20Hung-Kai%20Chung%20and%20Kuei-Chun%20Wang%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=Video%20Large%20Language%20Models%20%28VideoLLMs%29%20have%20shown%20remarkable%20progress%20in%20video%20understanding.%20However%2C%20these%20models%20still%20struggle%20to%20effectively%20perceive%20and%20exploit%20rich%20temporal%20information%20in%20videos%20when%20responding%20to%20user%20queries.%20Therefore%2C%20they%20often%20generate%20descriptions%20of%20events%20that%20are%20temporal%20inconsistent%20or%20causally%20implausible%2C%20causing%20severe%20hallucination%20issues.%20While%20most%20prior%20studies%20have%20focused%20on%20spatial%20hallucinations%20%28e.g.%20object%20mismatches%29%2C%20temporal%20reasoning%20in%20video%20understanding%20remains%20relatively%20underexplored.%20To%20address%20this%20issue%2C%20we%20propose%20Self-Diagnostic%20Contrastive%20Decoding%20%28SEASON%29%2C%20a%20training-free%20method%20that%20adaptively%20enhances%20temporal%20and%20spatial%20faithfulness%20for%20each%20output%20token.%20It%20achieves%20this%20by%20dynamically%20diagnosing%20each%20token%27s%20hallucination%20tendency%20and%20applying%20adaptive%20contrastive%20decoding%20against%20its%20corresponding%20temporal%20and%20spatial%20negatives.%20Extensive%20experiments%20demonstrate%20that%20SEASON%20outperforms%20all%20existing%20training-free%20hallucination%20mitigation%20approaches%20on%20three%20hallucination%20examination%20benchmarks%2C%20while%20further%20improves%20VideoLLMs%20across%20four%20general%20video%20understanding%20benchmarks.%20The%20code%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2512.04643v1&entry.124074799=Read"},
{"title": "\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments", "author": "Ziyi Zhang and Zhen Sun and Zongmin Zhang and Zifan Peng and Yuemeng Zhao and Zichun Wang and Zeren Luo and Ruiting Zuo and Xinlei He", "abstract": "The visually impaired population faces significant challenges in daily activities. While prior works employ vision language models for assistance, most focus on static content and cannot address real-time perception needs in complex environments. Recent VideoLLMs enable real-time vision and speech interaction, offering promising potential for assistive tasks. In this work, we conduct the first study evaluating their effectiveness in supporting daily life for visually impaired individuals. We first conducted a user survey with visually impaired participants to design the benchmark VisAssistDaily for daily life evaluation. Using VisAssistDaily, we evaluate popular VideoLLMs and find GPT-4o achieves the highest task success rate. We further conduct a user study to reveal concerns about hazard perception. To address this, we propose SafeVid, an environment-awareness dataset, and fine-tune VITA-1.5, improving risk recognition accuracy from 25.00% to 76.00%.We hope this work provides valuable insights and inspiration for future research in this field.", "link": "http://arxiv.org/abs/2505.04488v2", "date": "2025-12-04", "relevancy": 2.7744, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22I%20Can%20See%20Forever%21%22%3A%20Evaluating%20Real-time%20VideoLLMs%20for%20Assisting%20Individuals%20with%20Visual%20Impairments&body=Title%3A%20%22I%20Can%20See%20Forever%21%22%3A%20Evaluating%20Real-time%20VideoLLMs%20for%20Assisting%20Individuals%20with%20Visual%20Impairments%0AAuthor%3A%20Ziyi%20Zhang%20and%20Zhen%20Sun%20and%20Zongmin%20Zhang%20and%20Zifan%20Peng%20and%20Yuemeng%20Zhao%20and%20Zichun%20Wang%20and%20Zeren%20Luo%20and%20Ruiting%20Zuo%20and%20Xinlei%20He%0AAbstract%3A%20The%20visually%20impaired%20population%20faces%20significant%20challenges%20in%20daily%20activities.%20While%20prior%20works%20employ%20vision%20language%20models%20for%20assistance%2C%20most%20focus%20on%20static%20content%20and%20cannot%20address%20real-time%20perception%20needs%20in%20complex%20environments.%20Recent%20VideoLLMs%20enable%20real-time%20vision%20and%20speech%20interaction%2C%20offering%20promising%20potential%20for%20assistive%20tasks.%20In%20this%20work%2C%20we%20conduct%20the%20first%20study%20evaluating%20their%20effectiveness%20in%20supporting%20daily%20life%20for%20visually%20impaired%20individuals.%20We%20first%20conducted%20a%20user%20survey%20with%20visually%20impaired%20participants%20to%20design%20the%20benchmark%20VisAssistDaily%20for%20daily%20life%20evaluation.%20Using%20VisAssistDaily%2C%20we%20evaluate%20popular%20VideoLLMs%20and%20find%20GPT-4o%20achieves%20the%20highest%20task%20success%20rate.%20We%20further%20conduct%20a%20user%20study%20to%20reveal%20concerns%20about%20hazard%20perception.%20To%20address%20this%2C%20we%20propose%20SafeVid%2C%20an%20environment-awareness%20dataset%2C%20and%20fine-tune%20VITA-1.5%2C%20improving%20risk%20recognition%20accuracy%20from%2025.00%25%20to%2076.00%25.We%20hope%20this%20work%20provides%20valuable%20insights%20and%20inspiration%20for%20future%20research%20in%20this%20field.%0ALink%3A%20http%3A//arxiv.org/abs/2505.04488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522I%2520Can%2520See%2520Forever%2521%2522%253A%2520Evaluating%2520Real-time%2520VideoLLMs%2520for%2520Assisting%2520Individuals%2520with%2520Visual%2520Impairments%26entry.906535625%3DZiyi%2520Zhang%2520and%2520Zhen%2520Sun%2520and%2520Zongmin%2520Zhang%2520and%2520Zifan%2520Peng%2520and%2520Yuemeng%2520Zhao%2520and%2520Zichun%2520Wang%2520and%2520Zeren%2520Luo%2520and%2520Ruiting%2520Zuo%2520and%2520Xinlei%2520He%26entry.1292438233%3DThe%2520visually%2520impaired%2520population%2520faces%2520significant%2520challenges%2520in%2520daily%2520activities.%2520While%2520prior%2520works%2520employ%2520vision%2520language%2520models%2520for%2520assistance%252C%2520most%2520focus%2520on%2520static%2520content%2520and%2520cannot%2520address%2520real-time%2520perception%2520needs%2520in%2520complex%2520environments.%2520Recent%2520VideoLLMs%2520enable%2520real-time%2520vision%2520and%2520speech%2520interaction%252C%2520offering%2520promising%2520potential%2520for%2520assistive%2520tasks.%2520In%2520this%2520work%252C%2520we%2520conduct%2520the%2520first%2520study%2520evaluating%2520their%2520effectiveness%2520in%2520supporting%2520daily%2520life%2520for%2520visually%2520impaired%2520individuals.%2520We%2520first%2520conducted%2520a%2520user%2520survey%2520with%2520visually%2520impaired%2520participants%2520to%2520design%2520the%2520benchmark%2520VisAssistDaily%2520for%2520daily%2520life%2520evaluation.%2520Using%2520VisAssistDaily%252C%2520we%2520evaluate%2520popular%2520VideoLLMs%2520and%2520find%2520GPT-4o%2520achieves%2520the%2520highest%2520task%2520success%2520rate.%2520We%2520further%2520conduct%2520a%2520user%2520study%2520to%2520reveal%2520concerns%2520about%2520hazard%2520perception.%2520To%2520address%2520this%252C%2520we%2520propose%2520SafeVid%252C%2520an%2520environment-awareness%2520dataset%252C%2520and%2520fine-tune%2520VITA-1.5%252C%2520improving%2520risk%2520recognition%2520accuracy%2520from%252025.00%2525%2520to%252076.00%2525.We%2520hope%2520this%2520work%2520provides%2520valuable%2520insights%2520and%2520inspiration%2520for%2520future%2520research%2520in%2520this%2520field.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22I%20Can%20See%20Forever%21%22%3A%20Evaluating%20Real-time%20VideoLLMs%20for%20Assisting%20Individuals%20with%20Visual%20Impairments&entry.906535625=Ziyi%20Zhang%20and%20Zhen%20Sun%20and%20Zongmin%20Zhang%20and%20Zifan%20Peng%20and%20Yuemeng%20Zhao%20and%20Zichun%20Wang%20and%20Zeren%20Luo%20and%20Ruiting%20Zuo%20and%20Xinlei%20He&entry.1292438233=The%20visually%20impaired%20population%20faces%20significant%20challenges%20in%20daily%20activities.%20While%20prior%20works%20employ%20vision%20language%20models%20for%20assistance%2C%20most%20focus%20on%20static%20content%20and%20cannot%20address%20real-time%20perception%20needs%20in%20complex%20environments.%20Recent%20VideoLLMs%20enable%20real-time%20vision%20and%20speech%20interaction%2C%20offering%20promising%20potential%20for%20assistive%20tasks.%20In%20this%20work%2C%20we%20conduct%20the%20first%20study%20evaluating%20their%20effectiveness%20in%20supporting%20daily%20life%20for%20visually%20impaired%20individuals.%20We%20first%20conducted%20a%20user%20survey%20with%20visually%20impaired%20participants%20to%20design%20the%20benchmark%20VisAssistDaily%20for%20daily%20life%20evaluation.%20Using%20VisAssistDaily%2C%20we%20evaluate%20popular%20VideoLLMs%20and%20find%20GPT-4o%20achieves%20the%20highest%20task%20success%20rate.%20We%20further%20conduct%20a%20user%20study%20to%20reveal%20concerns%20about%20hazard%20perception.%20To%20address%20this%2C%20we%20propose%20SafeVid%2C%20an%20environment-awareness%20dataset%2C%20and%20fine-tune%20VITA-1.5%2C%20improving%20risk%20recognition%20accuracy%20from%2025.00%25%20to%2076.00%25.We%20hope%20this%20work%20provides%20valuable%20insights%20and%20inspiration%20for%20future%20research%20in%20this%20field.&entry.1838667208=http%3A//arxiv.org/abs/2505.04488v2&entry.124074799=Read"},
{"title": "SO-Bench: A Structural Output Evaluation of Multimodal LLMs", "author": "Di Feng and Kaixin Ma and Feng Nan and Haofeng Chen and Bohan Zhai and David Griffiths and Mingfei Gao and Zhe Gan and Eshan Verma and Yinfei Yang and Zhifeng Chen and Afshin Dehghan", "abstract": "Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.", "link": "http://arxiv.org/abs/2511.21750v2", "date": "2025-12-04", "relevancy": 2.7572, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SO-Bench%3A%20A%20Structural%20Output%20Evaluation%20of%20Multimodal%20LLMs&body=Title%3A%20SO-Bench%3A%20A%20Structural%20Output%20Evaluation%20of%20Multimodal%20LLMs%0AAuthor%3A%20Di%20Feng%20and%20Kaixin%20Ma%20and%20Feng%20Nan%20and%20Haofeng%20Chen%20and%20Bohan%20Zhai%20and%20David%20Griffiths%20and%20Mingfei%20Gao%20and%20Zhe%20Gan%20and%20Eshan%20Verma%20and%20Yinfei%20Yang%20and%20Zhifeng%20Chen%20and%20Afshin%20Dehghan%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20are%20increasingly%20deployed%20in%20real-world%2C%20agentic%20settings%20where%20outputs%20must%20not%20only%20be%20correct%2C%20but%20also%20conform%20to%20predefined%20data%20schemas.%20Despite%20recent%20progress%20in%20structured%20generation%20in%20textual%20domain%2C%20there%20is%20still%20no%20benchmark%20that%20systematically%20evaluates%20schema-grounded%20information%20extraction%20and%20reasoning%20over%20visual%20inputs.%20In%20this%20work%2C%20we%20conduct%20a%20comprehensive%20study%20of%20visual%20structural%20output%20capabilities%20for%20MLLMs%20with%20our%20carefully%20designed%20SO-Bench%20benchmark.%20Covering%20four%20visual%20domains%2C%20including%20UI%20screens%2C%20natural%20images%2C%20documents%2C%20and%20charts%2C%20SO-Bench%20is%20built%20from%20over%206.5K%20diverse%20JSON%20schemas%20and%201.8K%20curated%20image-schema%20pairs%20with%20human-verified%20quality.%20Benchmarking%20experiments%20on%20open-sourced%20and%20frontier%20proprietary%20models%20reveal%20persistent%20gaps%20in%20predicting%20accurate%2C%20schema%20compliant%20outputs%2C%20highlighting%20the%20need%20for%20better%20multimodal%20structured%20reasoning.%20Beyond%20benchmarking%2C%20we%20further%20conduct%20training%20experiments%20to%20largely%20improve%20the%20model%27s%20structured%20output%20capability.%20We%20plan%20to%20make%20the%20benchmark%20available%20to%20the%20community.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21750v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSO-Bench%253A%2520A%2520Structural%2520Output%2520Evaluation%2520of%2520Multimodal%2520LLMs%26entry.906535625%3DDi%2520Feng%2520and%2520Kaixin%2520Ma%2520and%2520Feng%2520Nan%2520and%2520Haofeng%2520Chen%2520and%2520Bohan%2520Zhai%2520and%2520David%2520Griffiths%2520and%2520Mingfei%2520Gao%2520and%2520Zhe%2520Gan%2520and%2520Eshan%2520Verma%2520and%2520Yinfei%2520Yang%2520and%2520Zhifeng%2520Chen%2520and%2520Afshin%2520Dehghan%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520real-world%252C%2520agentic%2520settings%2520where%2520outputs%2520must%2520not%2520only%2520be%2520correct%252C%2520but%2520also%2520conform%2520to%2520predefined%2520data%2520schemas.%2520Despite%2520recent%2520progress%2520in%2520structured%2520generation%2520in%2520textual%2520domain%252C%2520there%2520is%2520still%2520no%2520benchmark%2520that%2520systematically%2520evaluates%2520schema-grounded%2520information%2520extraction%2520and%2520reasoning%2520over%2520visual%2520inputs.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520comprehensive%2520study%2520of%2520visual%2520structural%2520output%2520capabilities%2520for%2520MLLMs%2520with%2520our%2520carefully%2520designed%2520SO-Bench%2520benchmark.%2520Covering%2520four%2520visual%2520domains%252C%2520including%2520UI%2520screens%252C%2520natural%2520images%252C%2520documents%252C%2520and%2520charts%252C%2520SO-Bench%2520is%2520built%2520from%2520over%25206.5K%2520diverse%2520JSON%2520schemas%2520and%25201.8K%2520curated%2520image-schema%2520pairs%2520with%2520human-verified%2520quality.%2520Benchmarking%2520experiments%2520on%2520open-sourced%2520and%2520frontier%2520proprietary%2520models%2520reveal%2520persistent%2520gaps%2520in%2520predicting%2520accurate%252C%2520schema%2520compliant%2520outputs%252C%2520highlighting%2520the%2520need%2520for%2520better%2520multimodal%2520structured%2520reasoning.%2520Beyond%2520benchmarking%252C%2520we%2520further%2520conduct%2520training%2520experiments%2520to%2520largely%2520improve%2520the%2520model%2527s%2520structured%2520output%2520capability.%2520We%2520plan%2520to%2520make%2520the%2520benchmark%2520available%2520to%2520the%2520community.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21750v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SO-Bench%3A%20A%20Structural%20Output%20Evaluation%20of%20Multimodal%20LLMs&entry.906535625=Di%20Feng%20and%20Kaixin%20Ma%20and%20Feng%20Nan%20and%20Haofeng%20Chen%20and%20Bohan%20Zhai%20and%20David%20Griffiths%20and%20Mingfei%20Gao%20and%20Zhe%20Gan%20and%20Eshan%20Verma%20and%20Yinfei%20Yang%20and%20Zhifeng%20Chen%20and%20Afshin%20Dehghan&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20are%20increasingly%20deployed%20in%20real-world%2C%20agentic%20settings%20where%20outputs%20must%20not%20only%20be%20correct%2C%20but%20also%20conform%20to%20predefined%20data%20schemas.%20Despite%20recent%20progress%20in%20structured%20generation%20in%20textual%20domain%2C%20there%20is%20still%20no%20benchmark%20that%20systematically%20evaluates%20schema-grounded%20information%20extraction%20and%20reasoning%20over%20visual%20inputs.%20In%20this%20work%2C%20we%20conduct%20a%20comprehensive%20study%20of%20visual%20structural%20output%20capabilities%20for%20MLLMs%20with%20our%20carefully%20designed%20SO-Bench%20benchmark.%20Covering%20four%20visual%20domains%2C%20including%20UI%20screens%2C%20natural%20images%2C%20documents%2C%20and%20charts%2C%20SO-Bench%20is%20built%20from%20over%206.5K%20diverse%20JSON%20schemas%20and%201.8K%20curated%20image-schema%20pairs%20with%20human-verified%20quality.%20Benchmarking%20experiments%20on%20open-sourced%20and%20frontier%20proprietary%20models%20reveal%20persistent%20gaps%20in%20predicting%20accurate%2C%20schema%20compliant%20outputs%2C%20highlighting%20the%20need%20for%20better%20multimodal%20structured%20reasoning.%20Beyond%20benchmarking%2C%20we%20further%20conduct%20training%20experiments%20to%20largely%20improve%20the%20model%27s%20structured%20output%20capability.%20We%20plan%20to%20make%20the%20benchmark%20available%20to%20the%20community.&entry.1838667208=http%3A//arxiv.org/abs/2511.21750v2&entry.124074799=Read"},
{"title": "You Only Train Once (YOTO): A Retraining-Free Object Detection Framework", "author": "Priyanto Hidayatullah and Nurjannah Syakrani and Yudi Widhiyasana and Muhammad Rizqi Sholahuddin and Refdinal Tubagus and Zahri Al Adzani Hidayat and Hanri Fajar Ramadhan and Dafa Alfarizki Pratama and Farhan Muhammad Yasin", "abstract": "Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.", "link": "http://arxiv.org/abs/2512.04888v1", "date": "2025-12-04", "relevancy": 2.7387, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5689}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5543}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20Only%20Train%20Once%20%28YOTO%29%3A%20A%20Retraining-Free%20Object%20Detection%20Framework&body=Title%3A%20You%20Only%20Train%20Once%20%28YOTO%29%3A%20A%20Retraining-Free%20Object%20Detection%20Framework%0AAuthor%3A%20Priyanto%20Hidayatullah%20and%20Nurjannah%20Syakrani%20and%20Yudi%20Widhiyasana%20and%20Muhammad%20Rizqi%20Sholahuddin%20and%20Refdinal%20Tubagus%20and%20Zahri%20Al%20Adzani%20Hidayat%20and%20Hanri%20Fajar%20Ramadhan%20and%20Dafa%20Alfarizki%20Pratama%20and%20Farhan%20Muhammad%20Yasin%0AAbstract%3A%20Object%20detection%20constitutes%20the%20primary%20task%20within%20the%20domain%20of%20computer%20vision.%20It%20is%20utilized%20in%20numerous%20domains.%20Nonetheless%2C%20object%20detection%20continues%20to%20encounter%20the%20issue%20of%20catastrophic%20forgetting.%20The%20model%20must%20be%20retrained%20whenever%20new%20products%20are%20introduced%2C%20utilizing%20not%20only%20the%20new%20products%20dataset%20but%20also%20the%20entirety%20of%20the%20previous%20dataset.%20The%20outcome%20is%20obvious%3A%20increasing%20model%20training%20expenses%20and%20significant%20time%20consumption.%20In%20numerous%20sectors%2C%20particularly%20retail%20checkout%2C%20the%20frequent%20introduction%20of%20new%20products%20presents%20a%20great%20challenge.%20This%20study%20introduces%20You%20Only%20Train%20Once%20%28YOTO%29%2C%20a%20methodology%20designed%20to%20address%20the%20issue%20of%20catastrophic%20forgetting%20by%20integrating%20YOLO11n%20for%20object%20localization%20with%20DeIT%20and%20Proxy%20Anchor%20Loss%20for%20feature%20extraction%20and%20metric%20learning.%20For%20classification%2C%20we%20utilize%20cosine%20similarity%20between%20the%20embedding%20features%20of%20the%20target%20product%20and%20those%20in%20the%20Qdrant%20vector%20database.%20In%20a%20case%20study%20conducted%20in%20a%20retail%20store%20with%20140%20products%2C%20the%20experimental%20results%20demonstrate%20that%20our%20proposed%20framework%20achieves%20encouraging%20accuracy%2C%20whether%20for%20detecting%20new%20or%20existing%20products.%20Furthermore%2C%20without%20retraining%2C%20the%20training%20duration%20difference%20is%20significant.%20We%20achieve%20almost%203%20times%20the%20training%20time%20efficiency%20compared%20to%20classical%20object%20detection%20approaches.%20This%20efficiency%20escalates%20as%20additional%20new%20products%20are%20added%20to%20the%20product%20database.%20The%20average%20inference%20time%20is%20580%20ms%20per%20image%20containing%20multiple%20products%2C%20on%20an%20edge%20device%2C%20validating%20the%20proposed%20framework%27s%20feasibility%20for%20practical%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520Only%2520Train%2520Once%2520%2528YOTO%2529%253A%2520A%2520Retraining-Free%2520Object%2520Detection%2520Framework%26entry.906535625%3DPriyanto%2520Hidayatullah%2520and%2520Nurjannah%2520Syakrani%2520and%2520Yudi%2520Widhiyasana%2520and%2520Muhammad%2520Rizqi%2520Sholahuddin%2520and%2520Refdinal%2520Tubagus%2520and%2520Zahri%2520Al%2520Adzani%2520Hidayat%2520and%2520Hanri%2520Fajar%2520Ramadhan%2520and%2520Dafa%2520Alfarizki%2520Pratama%2520and%2520Farhan%2520Muhammad%2520Yasin%26entry.1292438233%3DObject%2520detection%2520constitutes%2520the%2520primary%2520task%2520within%2520the%2520domain%2520of%2520computer%2520vision.%2520It%2520is%2520utilized%2520in%2520numerous%2520domains.%2520Nonetheless%252C%2520object%2520detection%2520continues%2520to%2520encounter%2520the%2520issue%2520of%2520catastrophic%2520forgetting.%2520The%2520model%2520must%2520be%2520retrained%2520whenever%2520new%2520products%2520are%2520introduced%252C%2520utilizing%2520not%2520only%2520the%2520new%2520products%2520dataset%2520but%2520also%2520the%2520entirety%2520of%2520the%2520previous%2520dataset.%2520The%2520outcome%2520is%2520obvious%253A%2520increasing%2520model%2520training%2520expenses%2520and%2520significant%2520time%2520consumption.%2520In%2520numerous%2520sectors%252C%2520particularly%2520retail%2520checkout%252C%2520the%2520frequent%2520introduction%2520of%2520new%2520products%2520presents%2520a%2520great%2520challenge.%2520This%2520study%2520introduces%2520You%2520Only%2520Train%2520Once%2520%2528YOTO%2529%252C%2520a%2520methodology%2520designed%2520to%2520address%2520the%2520issue%2520of%2520catastrophic%2520forgetting%2520by%2520integrating%2520YOLO11n%2520for%2520object%2520localization%2520with%2520DeIT%2520and%2520Proxy%2520Anchor%2520Loss%2520for%2520feature%2520extraction%2520and%2520metric%2520learning.%2520For%2520classification%252C%2520we%2520utilize%2520cosine%2520similarity%2520between%2520the%2520embedding%2520features%2520of%2520the%2520target%2520product%2520and%2520those%2520in%2520the%2520Qdrant%2520vector%2520database.%2520In%2520a%2520case%2520study%2520conducted%2520in%2520a%2520retail%2520store%2520with%2520140%2520products%252C%2520the%2520experimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520framework%2520achieves%2520encouraging%2520accuracy%252C%2520whether%2520for%2520detecting%2520new%2520or%2520existing%2520products.%2520Furthermore%252C%2520without%2520retraining%252C%2520the%2520training%2520duration%2520difference%2520is%2520significant.%2520We%2520achieve%2520almost%25203%2520times%2520the%2520training%2520time%2520efficiency%2520compared%2520to%2520classical%2520object%2520detection%2520approaches.%2520This%2520efficiency%2520escalates%2520as%2520additional%2520new%2520products%2520are%2520added%2520to%2520the%2520product%2520database.%2520The%2520average%2520inference%2520time%2520is%2520580%2520ms%2520per%2520image%2520containing%2520multiple%2520products%252C%2520on%2520an%2520edge%2520device%252C%2520validating%2520the%2520proposed%2520framework%2527s%2520feasibility%2520for%2520practical%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Only%20Train%20Once%20%28YOTO%29%3A%20A%20Retraining-Free%20Object%20Detection%20Framework&entry.906535625=Priyanto%20Hidayatullah%20and%20Nurjannah%20Syakrani%20and%20Yudi%20Widhiyasana%20and%20Muhammad%20Rizqi%20Sholahuddin%20and%20Refdinal%20Tubagus%20and%20Zahri%20Al%20Adzani%20Hidayat%20and%20Hanri%20Fajar%20Ramadhan%20and%20Dafa%20Alfarizki%20Pratama%20and%20Farhan%20Muhammad%20Yasin&entry.1292438233=Object%20detection%20constitutes%20the%20primary%20task%20within%20the%20domain%20of%20computer%20vision.%20It%20is%20utilized%20in%20numerous%20domains.%20Nonetheless%2C%20object%20detection%20continues%20to%20encounter%20the%20issue%20of%20catastrophic%20forgetting.%20The%20model%20must%20be%20retrained%20whenever%20new%20products%20are%20introduced%2C%20utilizing%20not%20only%20the%20new%20products%20dataset%20but%20also%20the%20entirety%20of%20the%20previous%20dataset.%20The%20outcome%20is%20obvious%3A%20increasing%20model%20training%20expenses%20and%20significant%20time%20consumption.%20In%20numerous%20sectors%2C%20particularly%20retail%20checkout%2C%20the%20frequent%20introduction%20of%20new%20products%20presents%20a%20great%20challenge.%20This%20study%20introduces%20You%20Only%20Train%20Once%20%28YOTO%29%2C%20a%20methodology%20designed%20to%20address%20the%20issue%20of%20catastrophic%20forgetting%20by%20integrating%20YOLO11n%20for%20object%20localization%20with%20DeIT%20and%20Proxy%20Anchor%20Loss%20for%20feature%20extraction%20and%20metric%20learning.%20For%20classification%2C%20we%20utilize%20cosine%20similarity%20between%20the%20embedding%20features%20of%20the%20target%20product%20and%20those%20in%20the%20Qdrant%20vector%20database.%20In%20a%20case%20study%20conducted%20in%20a%20retail%20store%20with%20140%20products%2C%20the%20experimental%20results%20demonstrate%20that%20our%20proposed%20framework%20achieves%20encouraging%20accuracy%2C%20whether%20for%20detecting%20new%20or%20existing%20products.%20Furthermore%2C%20without%20retraining%2C%20the%20training%20duration%20difference%20is%20significant.%20We%20achieve%20almost%203%20times%20the%20training%20time%20efficiency%20compared%20to%20classical%20object%20detection%20approaches.%20This%20efficiency%20escalates%20as%20additional%20new%20products%20are%20added%20to%20the%20product%20database.%20The%20average%20inference%20time%20is%20580%20ms%20per%20image%20containing%20multiple%20products%2C%20on%20an%20edge%20device%2C%20validating%20the%20proposed%20framework%27s%20feasibility%20for%20practical%20use.&entry.1838667208=http%3A//arxiv.org/abs/2512.04888v1&entry.124074799=Read"},
{"title": "E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving", "author": "Yihong Tang and Haicheng Liao and Tong Nie and Junlin He and Ao Qu and Kehua Chen and Wei Ma and Zhenning Li and Lijun Sun and Chengzhong Xu", "abstract": "End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.", "link": "http://arxiv.org/abs/2512.04733v1", "date": "2025-12-04", "relevancy": 2.7341, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E3AD%3A%20An%20Emotion-Aware%20Vision-Language-Action%20Model%20for%20Human-Centric%20End-to-End%20Autonomous%20Driving&body=Title%3A%20E3AD%3A%20An%20Emotion-Aware%20Vision-Language-Action%20Model%20for%20Human-Centric%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Yihong%20Tang%20and%20Haicheng%20Liao%20and%20Tong%20Nie%20and%20Junlin%20He%20and%20Ao%20Qu%20and%20Kehua%20Chen%20and%20Wei%20Ma%20and%20Zhenning%20Li%20and%20Lijun%20Sun%20and%20Chengzhong%20Xu%0AAbstract%3A%20End-to-end%20autonomous%20driving%20%28AD%29%20systems%20increasingly%20adopt%20vision-language-action%20%28VLA%29%20models%2C%20yet%20they%20typically%20ignore%20the%20passenger%27s%20emotional%20state%2C%20which%20is%20central%20to%20comfort%20and%20AD%20acceptance.%20We%20introduce%20Open-Domain%20End-to-End%20%28OD-E2E%29%20autonomous%20driving%2C%20where%20an%20autonomous%20vehicle%20%28AV%29%20must%20interpret%20free-form%20natural-language%20commands%2C%20infer%20the%20emotion%2C%20and%20plan%20a%20physically%20feasible%20trajectory.%20We%20propose%20E3AD%2C%20an%20emotion-aware%20VLA%20framework%20that%20augments%20semantic%20understanding%20with%20two%20cognitively%20inspired%20components%3A%20a%20continuous%20Valenc-Arousal-Dominance%20%28VAD%29%20emotion%20model%20that%20captures%20tone%20and%20urgency%20from%20language%2C%20and%20a%20dual-pathway%20spatial%20reasoning%20module%20that%20fuses%20egocentric%20and%20allocentric%20views%20for%20human-like%20spatial%20cognition.%20A%20consistency-oriented%20training%20scheme%2C%20combining%20modality%20pretraining%20with%20preference-based%20alignment%2C%20further%20enforces%20coherence%20between%20emotional%20intent%20and%20driving%20actions.%20Across%20real-world%20datasets%2C%20E3AD%20improves%20visual%20grounding%20and%20waypoint%20planning%20and%20achieves%20state-of-the-art%20%28SOTA%29%20VAD%20correlation%20for%20emotion%20estimation.%20These%20results%20show%20that%20injecting%20emotion%20into%20VLA-style%20driving%20yields%20more%20human-aligned%20grounding%2C%20planning%2C%20and%20human-centric%20feedback.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE3AD%253A%2520An%2520Emotion-Aware%2520Vision-Language-Action%2520Model%2520for%2520Human-Centric%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DYihong%2520Tang%2520and%2520Haicheng%2520Liao%2520and%2520Tong%2520Nie%2520and%2520Junlin%2520He%2520and%2520Ao%2520Qu%2520and%2520Kehua%2520Chen%2520and%2520Wei%2520Ma%2520and%2520Zhenning%2520Li%2520and%2520Lijun%2520Sun%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3DEnd-to-end%2520autonomous%2520driving%2520%2528AD%2529%2520systems%2520increasingly%2520adopt%2520vision-language-action%2520%2528VLA%2529%2520models%252C%2520yet%2520they%2520typically%2520ignore%2520the%2520passenger%2527s%2520emotional%2520state%252C%2520which%2520is%2520central%2520to%2520comfort%2520and%2520AD%2520acceptance.%2520We%2520introduce%2520Open-Domain%2520End-to-End%2520%2528OD-E2E%2529%2520autonomous%2520driving%252C%2520where%2520an%2520autonomous%2520vehicle%2520%2528AV%2529%2520must%2520interpret%2520free-form%2520natural-language%2520commands%252C%2520infer%2520the%2520emotion%252C%2520and%2520plan%2520a%2520physically%2520feasible%2520trajectory.%2520We%2520propose%2520E3AD%252C%2520an%2520emotion-aware%2520VLA%2520framework%2520that%2520augments%2520semantic%2520understanding%2520with%2520two%2520cognitively%2520inspired%2520components%253A%2520a%2520continuous%2520Valenc-Arousal-Dominance%2520%2528VAD%2529%2520emotion%2520model%2520that%2520captures%2520tone%2520and%2520urgency%2520from%2520language%252C%2520and%2520a%2520dual-pathway%2520spatial%2520reasoning%2520module%2520that%2520fuses%2520egocentric%2520and%2520allocentric%2520views%2520for%2520human-like%2520spatial%2520cognition.%2520A%2520consistency-oriented%2520training%2520scheme%252C%2520combining%2520modality%2520pretraining%2520with%2520preference-based%2520alignment%252C%2520further%2520enforces%2520coherence%2520between%2520emotional%2520intent%2520and%2520driving%2520actions.%2520Across%2520real-world%2520datasets%252C%2520E3AD%2520improves%2520visual%2520grounding%2520and%2520waypoint%2520planning%2520and%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520VAD%2520correlation%2520for%2520emotion%2520estimation.%2520These%2520results%2520show%2520that%2520injecting%2520emotion%2520into%2520VLA-style%2520driving%2520yields%2520more%2520human-aligned%2520grounding%252C%2520planning%252C%2520and%2520human-centric%2520feedback.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E3AD%3A%20An%20Emotion-Aware%20Vision-Language-Action%20Model%20for%20Human-Centric%20End-to-End%20Autonomous%20Driving&entry.906535625=Yihong%20Tang%20and%20Haicheng%20Liao%20and%20Tong%20Nie%20and%20Junlin%20He%20and%20Ao%20Qu%20and%20Kehua%20Chen%20and%20Wei%20Ma%20and%20Zhenning%20Li%20and%20Lijun%20Sun%20and%20Chengzhong%20Xu&entry.1292438233=End-to-end%20autonomous%20driving%20%28AD%29%20systems%20increasingly%20adopt%20vision-language-action%20%28VLA%29%20models%2C%20yet%20they%20typically%20ignore%20the%20passenger%27s%20emotional%20state%2C%20which%20is%20central%20to%20comfort%20and%20AD%20acceptance.%20We%20introduce%20Open-Domain%20End-to-End%20%28OD-E2E%29%20autonomous%20driving%2C%20where%20an%20autonomous%20vehicle%20%28AV%29%20must%20interpret%20free-form%20natural-language%20commands%2C%20infer%20the%20emotion%2C%20and%20plan%20a%20physically%20feasible%20trajectory.%20We%20propose%20E3AD%2C%20an%20emotion-aware%20VLA%20framework%20that%20augments%20semantic%20understanding%20with%20two%20cognitively%20inspired%20components%3A%20a%20continuous%20Valenc-Arousal-Dominance%20%28VAD%29%20emotion%20model%20that%20captures%20tone%20and%20urgency%20from%20language%2C%20and%20a%20dual-pathway%20spatial%20reasoning%20module%20that%20fuses%20egocentric%20and%20allocentric%20views%20for%20human-like%20spatial%20cognition.%20A%20consistency-oriented%20training%20scheme%2C%20combining%20modality%20pretraining%20with%20preference-based%20alignment%2C%20further%20enforces%20coherence%20between%20emotional%20intent%20and%20driving%20actions.%20Across%20real-world%20datasets%2C%20E3AD%20improves%20visual%20grounding%20and%20waypoint%20planning%20and%20achieves%20state-of-the-art%20%28SOTA%29%20VAD%20correlation%20for%20emotion%20estimation.%20These%20results%20show%20that%20injecting%20emotion%20into%20VLA-style%20driving%20yields%20more%20human-aligned%20grounding%2C%20planning%2C%20and%20human-centric%20feedback.&entry.1838667208=http%3A//arxiv.org/abs/2512.04733v1&entry.124074799=Read"},
{"title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems", "author": "Massimo Bini and Ondrej Bohdal and Umberto Michieli and Zeynep Akata and Mete Ozay and Taha Ceritli", "abstract": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.", "link": "http://arxiv.org/abs/2512.04763v1", "date": "2025-12-04", "relevancy": 2.7299, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MemLoRA%3A%20Distilling%20Expert%20Adapters%20for%20On-Device%20Memory%20Systems&body=Title%3A%20MemLoRA%3A%20Distilling%20Expert%20Adapters%20for%20On-Device%20Memory%20Systems%0AAuthor%3A%20Massimo%20Bini%20and%20Ondrej%20Bohdal%20and%20Umberto%20Michieli%20and%20Zeynep%20Akata%20and%20Mete%20Ozay%20and%20Taha%20Ceritli%0AAbstract%3A%20Memory-augmented%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20consistency%20during%20prolonged%20dialogues%20by%20storing%20relevant%20memories%20and%20incorporating%20them%20as%20context.%20Such%20memory-based%20personalization%20is%20also%20key%20in%20on-device%20settings%20that%20allow%20users%20to%20keep%20their%20conversations%20and%20data%20private.%20However%2C%20memory-augmented%20systems%20typically%20rely%20on%20LLMs%20that%20are%20too%20costly%20for%20local%20on-device%20deployment.%20Even%20though%20Small%20Language%20Models%20%28SLMs%29%20are%20more%20suitable%20for%20on-device%20inference%20than%20LLMs%2C%20they%20cannot%20achieve%20sufficient%20performance.%20Additionally%2C%20these%20LLM-based%20systems%20lack%20native%20visual%20capabilities%2C%20limiting%20their%20applicability%20in%20multimodal%20contexts.%20In%20this%20paper%2C%20we%20introduce%20%28i%29%20MemLoRA%2C%20a%20novel%20memory%20system%20that%20enables%20local%20deployment%20by%20equipping%20SLMs%20with%20specialized%20memory%20adapters%2C%20and%20%28ii%29%20its%20vision%20extension%20MemLoRA-V%2C%20which%20integrates%20small%20Vision-Language%20Models%20%28SVLMs%29%20to%20memory%20systems%2C%20enabling%20native%20visual%20understanding.%20Following%20knowledge%20distillation%20principles%2C%20each%20adapter%20is%20trained%20separately%20for%20specific%20memory%20operations%24%5Cunicode%7Bx2013%7D%24knowledge%20extraction%2C%20memory%20update%2C%20and%20memory-augmented%20generation.%20Equipped%20with%20memory%20adapters%2C%20small%20models%20enable%20accurate%20on-device%20memory%20operations%20without%20cloud%20dependency.%20On%20text-only%20operations%2C%20MemLoRA%20outperforms%2010%24%5Ctimes%24%20larger%20baseline%20models%20%28e.g.%2C%20Gemma2-27B%29%20and%20achieves%20performance%20comparable%20to%2060%24%5Ctimes%24%20larger%20models%20%28e.g.%2C%20GPT-OSS-120B%29%20on%20the%20LoCoMo%20benchmark.%20To%20evaluate%20visual%20understanding%20operations%20instead%2C%20we%20extend%20LoCoMo%20with%20challenging%20Visual%20Question%20Answering%20tasks%20that%20require%20direct%20visual%20reasoning.%20On%20this%2C%20our%20VLM-integrated%20MemLoRA-V%20shows%20massive%20improvements%20over%20caption-based%20approaches%20%2881.3%20vs.%2023.7%20accuracy%29%20while%20keeping%20strong%20performance%20in%20text-based%20tasks%2C%20demonstrating%20the%20efficacy%20of%20our%20method%20in%20multimodal%20contexts.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemLoRA%253A%2520Distilling%2520Expert%2520Adapters%2520for%2520On-Device%2520Memory%2520Systems%26entry.906535625%3DMassimo%2520Bini%2520and%2520Ondrej%2520Bohdal%2520and%2520Umberto%2520Michieli%2520and%2520Zeynep%2520Akata%2520and%2520Mete%2520Ozay%2520and%2520Taha%2520Ceritli%26entry.1292438233%3DMemory-augmented%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520consistency%2520during%2520prolonged%2520dialogues%2520by%2520storing%2520relevant%2520memories%2520and%2520incorporating%2520them%2520as%2520context.%2520Such%2520memory-based%2520personalization%2520is%2520also%2520key%2520in%2520on-device%2520settings%2520that%2520allow%2520users%2520to%2520keep%2520their%2520conversations%2520and%2520data%2520private.%2520However%252C%2520memory-augmented%2520systems%2520typically%2520rely%2520on%2520LLMs%2520that%2520are%2520too%2520costly%2520for%2520local%2520on-device%2520deployment.%2520Even%2520though%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520are%2520more%2520suitable%2520for%2520on-device%2520inference%2520than%2520LLMs%252C%2520they%2520cannot%2520achieve%2520sufficient%2520performance.%2520Additionally%252C%2520these%2520LLM-based%2520systems%2520lack%2520native%2520visual%2520capabilities%252C%2520limiting%2520their%2520applicability%2520in%2520multimodal%2520contexts.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%2528i%2529%2520MemLoRA%252C%2520a%2520novel%2520memory%2520system%2520that%2520enables%2520local%2520deployment%2520by%2520equipping%2520SLMs%2520with%2520specialized%2520memory%2520adapters%252C%2520and%2520%2528ii%2529%2520its%2520vision%2520extension%2520MemLoRA-V%252C%2520which%2520integrates%2520small%2520Vision-Language%2520Models%2520%2528SVLMs%2529%2520to%2520memory%2520systems%252C%2520enabling%2520native%2520visual%2520understanding.%2520Following%2520knowledge%2520distillation%2520principles%252C%2520each%2520adapter%2520is%2520trained%2520separately%2520for%2520specific%2520memory%2520operations%2524%255Cunicode%257Bx2013%257D%2524knowledge%2520extraction%252C%2520memory%2520update%252C%2520and%2520memory-augmented%2520generation.%2520Equipped%2520with%2520memory%2520adapters%252C%2520small%2520models%2520enable%2520accurate%2520on-device%2520memory%2520operations%2520without%2520cloud%2520dependency.%2520On%2520text-only%2520operations%252C%2520MemLoRA%2520outperforms%252010%2524%255Ctimes%2524%2520larger%2520baseline%2520models%2520%2528e.g.%252C%2520Gemma2-27B%2529%2520and%2520achieves%2520performance%2520comparable%2520to%252060%2524%255Ctimes%2524%2520larger%2520models%2520%2528e.g.%252C%2520GPT-OSS-120B%2529%2520on%2520the%2520LoCoMo%2520benchmark.%2520To%2520evaluate%2520visual%2520understanding%2520operations%2520instead%252C%2520we%2520extend%2520LoCoMo%2520with%2520challenging%2520Visual%2520Question%2520Answering%2520tasks%2520that%2520require%2520direct%2520visual%2520reasoning.%2520On%2520this%252C%2520our%2520VLM-integrated%2520MemLoRA-V%2520shows%2520massive%2520improvements%2520over%2520caption-based%2520approaches%2520%252881.3%2520vs.%252023.7%2520accuracy%2529%2520while%2520keeping%2520strong%2520performance%2520in%2520text-based%2520tasks%252C%2520demonstrating%2520the%2520efficacy%2520of%2520our%2520method%2520in%2520multimodal%2520contexts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MemLoRA%3A%20Distilling%20Expert%20Adapters%20for%20On-Device%20Memory%20Systems&entry.906535625=Massimo%20Bini%20and%20Ondrej%20Bohdal%20and%20Umberto%20Michieli%20and%20Zeynep%20Akata%20and%20Mete%20Ozay%20and%20Taha%20Ceritli&entry.1292438233=Memory-augmented%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20consistency%20during%20prolonged%20dialogues%20by%20storing%20relevant%20memories%20and%20incorporating%20them%20as%20context.%20Such%20memory-based%20personalization%20is%20also%20key%20in%20on-device%20settings%20that%20allow%20users%20to%20keep%20their%20conversations%20and%20data%20private.%20However%2C%20memory-augmented%20systems%20typically%20rely%20on%20LLMs%20that%20are%20too%20costly%20for%20local%20on-device%20deployment.%20Even%20though%20Small%20Language%20Models%20%28SLMs%29%20are%20more%20suitable%20for%20on-device%20inference%20than%20LLMs%2C%20they%20cannot%20achieve%20sufficient%20performance.%20Additionally%2C%20these%20LLM-based%20systems%20lack%20native%20visual%20capabilities%2C%20limiting%20their%20applicability%20in%20multimodal%20contexts.%20In%20this%20paper%2C%20we%20introduce%20%28i%29%20MemLoRA%2C%20a%20novel%20memory%20system%20that%20enables%20local%20deployment%20by%20equipping%20SLMs%20with%20specialized%20memory%20adapters%2C%20and%20%28ii%29%20its%20vision%20extension%20MemLoRA-V%2C%20which%20integrates%20small%20Vision-Language%20Models%20%28SVLMs%29%20to%20memory%20systems%2C%20enabling%20native%20visual%20understanding.%20Following%20knowledge%20distillation%20principles%2C%20each%20adapter%20is%20trained%20separately%20for%20specific%20memory%20operations%24%5Cunicode%7Bx2013%7D%24knowledge%20extraction%2C%20memory%20update%2C%20and%20memory-augmented%20generation.%20Equipped%20with%20memory%20adapters%2C%20small%20models%20enable%20accurate%20on-device%20memory%20operations%20without%20cloud%20dependency.%20On%20text-only%20operations%2C%20MemLoRA%20outperforms%2010%24%5Ctimes%24%20larger%20baseline%20models%20%28e.g.%2C%20Gemma2-27B%29%20and%20achieves%20performance%20comparable%20to%2060%24%5Ctimes%24%20larger%20models%20%28e.g.%2C%20GPT-OSS-120B%29%20on%20the%20LoCoMo%20benchmark.%20To%20evaluate%20visual%20understanding%20operations%20instead%2C%20we%20extend%20LoCoMo%20with%20challenging%20Visual%20Question%20Answering%20tasks%20that%20require%20direct%20visual%20reasoning.%20On%20this%2C%20our%20VLM-integrated%20MemLoRA-V%20shows%20massive%20improvements%20over%20caption-based%20approaches%20%2881.3%20vs.%2023.7%20accuracy%29%20while%20keeping%20strong%20performance%20in%20text-based%20tasks%2C%20demonstrating%20the%20efficacy%20of%20our%20method%20in%20multimodal%20contexts.&entry.1838667208=http%3A//arxiv.org/abs/2512.04763v1&entry.124074799=Read"},
{"title": "RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation", "author": "Nicolas Houdr\u00e9 and Diego Marcos and Hugo Riffaud de Turckheim and Dino Ienco and Laurent Wendling and Camille Kurtz and Sylvain Lobry", "abstract": "Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.", "link": "http://arxiv.org/abs/2512.05025v1", "date": "2025-12-04", "relevancy": 2.7239, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAMEN%3A%20Resolution-Adjustable%20Multimodal%20Encoder%20for%20Earth%20Observation&body=Title%3A%20RAMEN%3A%20Resolution-Adjustable%20Multimodal%20Encoder%20for%20Earth%20Observation%0AAuthor%3A%20Nicolas%20Houdr%C3%A9%20and%20Diego%20Marcos%20and%20Hugo%20Riffaud%20de%20Turckheim%20and%20Dino%20Ienco%20and%20Laurent%20Wendling%20and%20Camille%20Kurtz%20and%20Sylvain%20Lobry%0AAbstract%3A%20Earth%20observation%20%28EO%29%20data%20spans%20a%20wide%20range%20of%20spatial%2C%20spectral%2C%20and%20temporal%20resolutions%2C%20from%20high-resolution%20optical%20imagery%20to%20low%20resolution%20multispectral%20products%20or%20radar%20time%20series.%20While%20recent%20foundation%20models%20have%20improved%20multimodal%20integration%20for%20learning%20meaningful%20representations%2C%20they%20often%20expect%20fixed%20input%20resolutions%20or%20are%20based%20on%20sensor-specific%20encoders%20limiting%20generalization%20across%20heterogeneous%20EO%20modalities.%20To%20overcome%20these%20limitations%20we%20introduce%20RAMEN%2C%20a%20resolution-adjustable%20multimodal%20encoder%20that%20learns%20a%20shared%20visual%20representation%20across%20EO%20data%20in%20a%20fully%20sensor-agnostic%20manner.%20RAMEN%20treats%20the%20modality%20and%20spatial%20and%20temporal%20resolutions%20as%20key%20input%20data%20features%2C%20enabling%20coherent%20analysis%20across%20modalities%20within%20a%20unified%20latent%20space.%20Its%20main%20methodological%20contribution%20is%20to%20define%20spatial%20resolution%20as%20a%20controllable%20output%20parameter%2C%20giving%20users%20direct%20control%20over%20the%20desired%20level%20of%20detail%20at%20inference%20and%20allowing%20explicit%20trade-offs%20between%20spatial%20precision%20and%20computational%20cost.%20We%20train%20a%20single%2C%20unified%20transformer%20encoder%20reconstructing%20masked%20multimodal%20EO%20data%20drawn%20from%20diverse%20sources%2C%20ensuring%20generalization%20across%20sensors%20and%20resolutions.%20Once%20pretrained%2C%20RAMEN%20transfers%20effectively%20to%20both%20known%20and%20unseen%20sensor%20configurations%20and%20outperforms%20larger%20state-of-the-art%20models%20on%20the%20community-standard%20PANGAEA%20benchmark%2C%20containing%20various%20multi-sensor%20and%20multi-resolution%20downstream%20tasks.%20Our%20code%20and%20pretrained%20model%20are%20available%20at%20https%3A//github.com/nicolashoudre/RAMEN.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAMEN%253A%2520Resolution-Adjustable%2520Multimodal%2520Encoder%2520for%2520Earth%2520Observation%26entry.906535625%3DNicolas%2520Houdr%25C3%25A9%2520and%2520Diego%2520Marcos%2520and%2520Hugo%2520Riffaud%2520de%2520Turckheim%2520and%2520Dino%2520Ienco%2520and%2520Laurent%2520Wendling%2520and%2520Camille%2520Kurtz%2520and%2520Sylvain%2520Lobry%26entry.1292438233%3DEarth%2520observation%2520%2528EO%2529%2520data%2520spans%2520a%2520wide%2520range%2520of%2520spatial%252C%2520spectral%252C%2520and%2520temporal%2520resolutions%252C%2520from%2520high-resolution%2520optical%2520imagery%2520to%2520low%2520resolution%2520multispectral%2520products%2520or%2520radar%2520time%2520series.%2520While%2520recent%2520foundation%2520models%2520have%2520improved%2520multimodal%2520integration%2520for%2520learning%2520meaningful%2520representations%252C%2520they%2520often%2520expect%2520fixed%2520input%2520resolutions%2520or%2520are%2520based%2520on%2520sensor-specific%2520encoders%2520limiting%2520generalization%2520across%2520heterogeneous%2520EO%2520modalities.%2520To%2520overcome%2520these%2520limitations%2520we%2520introduce%2520RAMEN%252C%2520a%2520resolution-adjustable%2520multimodal%2520encoder%2520that%2520learns%2520a%2520shared%2520visual%2520representation%2520across%2520EO%2520data%2520in%2520a%2520fully%2520sensor-agnostic%2520manner.%2520RAMEN%2520treats%2520the%2520modality%2520and%2520spatial%2520and%2520temporal%2520resolutions%2520as%2520key%2520input%2520data%2520features%252C%2520enabling%2520coherent%2520analysis%2520across%2520modalities%2520within%2520a%2520unified%2520latent%2520space.%2520Its%2520main%2520methodological%2520contribution%2520is%2520to%2520define%2520spatial%2520resolution%2520as%2520a%2520controllable%2520output%2520parameter%252C%2520giving%2520users%2520direct%2520control%2520over%2520the%2520desired%2520level%2520of%2520detail%2520at%2520inference%2520and%2520allowing%2520explicit%2520trade-offs%2520between%2520spatial%2520precision%2520and%2520computational%2520cost.%2520We%2520train%2520a%2520single%252C%2520unified%2520transformer%2520encoder%2520reconstructing%2520masked%2520multimodal%2520EO%2520data%2520drawn%2520from%2520diverse%2520sources%252C%2520ensuring%2520generalization%2520across%2520sensors%2520and%2520resolutions.%2520Once%2520pretrained%252C%2520RAMEN%2520transfers%2520effectively%2520to%2520both%2520known%2520and%2520unseen%2520sensor%2520configurations%2520and%2520outperforms%2520larger%2520state-of-the-art%2520models%2520on%2520the%2520community-standard%2520PANGAEA%2520benchmark%252C%2520containing%2520various%2520multi-sensor%2520and%2520multi-resolution%2520downstream%2520tasks.%2520Our%2520code%2520and%2520pretrained%2520model%2520are%2520available%2520at%2520https%253A//github.com/nicolashoudre/RAMEN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAMEN%3A%20Resolution-Adjustable%20Multimodal%20Encoder%20for%20Earth%20Observation&entry.906535625=Nicolas%20Houdr%C3%A9%20and%20Diego%20Marcos%20and%20Hugo%20Riffaud%20de%20Turckheim%20and%20Dino%20Ienco%20and%20Laurent%20Wendling%20and%20Camille%20Kurtz%20and%20Sylvain%20Lobry&entry.1292438233=Earth%20observation%20%28EO%29%20data%20spans%20a%20wide%20range%20of%20spatial%2C%20spectral%2C%20and%20temporal%20resolutions%2C%20from%20high-resolution%20optical%20imagery%20to%20low%20resolution%20multispectral%20products%20or%20radar%20time%20series.%20While%20recent%20foundation%20models%20have%20improved%20multimodal%20integration%20for%20learning%20meaningful%20representations%2C%20they%20often%20expect%20fixed%20input%20resolutions%20or%20are%20based%20on%20sensor-specific%20encoders%20limiting%20generalization%20across%20heterogeneous%20EO%20modalities.%20To%20overcome%20these%20limitations%20we%20introduce%20RAMEN%2C%20a%20resolution-adjustable%20multimodal%20encoder%20that%20learns%20a%20shared%20visual%20representation%20across%20EO%20data%20in%20a%20fully%20sensor-agnostic%20manner.%20RAMEN%20treats%20the%20modality%20and%20spatial%20and%20temporal%20resolutions%20as%20key%20input%20data%20features%2C%20enabling%20coherent%20analysis%20across%20modalities%20within%20a%20unified%20latent%20space.%20Its%20main%20methodological%20contribution%20is%20to%20define%20spatial%20resolution%20as%20a%20controllable%20output%20parameter%2C%20giving%20users%20direct%20control%20over%20the%20desired%20level%20of%20detail%20at%20inference%20and%20allowing%20explicit%20trade-offs%20between%20spatial%20precision%20and%20computational%20cost.%20We%20train%20a%20single%2C%20unified%20transformer%20encoder%20reconstructing%20masked%20multimodal%20EO%20data%20drawn%20from%20diverse%20sources%2C%20ensuring%20generalization%20across%20sensors%20and%20resolutions.%20Once%20pretrained%2C%20RAMEN%20transfers%20effectively%20to%20both%20known%20and%20unseen%20sensor%20configurations%20and%20outperforms%20larger%20state-of-the-art%20models%20on%20the%20community-standard%20PANGAEA%20benchmark%2C%20containing%20various%20multi-sensor%20and%20multi-resolution%20downstream%20tasks.%20Our%20code%20and%20pretrained%20model%20are%20available%20at%20https%3A//github.com/nicolashoudre/RAMEN.&entry.1838667208=http%3A//arxiv.org/abs/2512.05025v1&entry.124074799=Read"},
{"title": "SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition", "author": "Shunpeng Chen and Changwei Wang and Rongtao Xu and Xingtian Pei and Yukun Song and Jinzhou Lin and Wenhao Xu and Jingyi Zhang and Li Guo and Shibiao Xu", "abstract": "Visual Place Recognition (VPR) requires robust retrieval of geotagged images despite large appearance, viewpoint, and environmental variation. Prior methods focus on descriptor fine-tuning or fixed sampling strategies yet neglect the dynamic interplay between spatial context and visual similarity during training. We present SAGE (Spatial-visual Adaptive Graph Exploration), a unified training pipeline that enhances granular spatial-visual discrimination by jointly improving local feature aggregation, organize samples during training, and hard sample mining. We introduce a lightweight Soft Probing module that learns residual weights from training data for patch descriptors before bilinear aggregation, boosting distinctive local cues. During training we reconstruct an online geo-visual graph that fuses geographic proximity and current visual similarity so that candidate neighborhoods reflect the evolving embedding landscape. To concentrate learning on the most informative place neighborhoods, we seed clusters from high-affinity anchors and iteratively expand them with a greedy weighted clique expansion sampler. Implemented with a frozen DINOv2 backbone and parameter-efficient fine-tuning, SAGE achieves SOTA across eight benchmarks. It attains 98.9%, 95.8%, 94.5%, and 96.0% Recall@1 on SPED, Pitts30k-test, MSLS-val, and Nordland, respectively. Notably, our method obtains 100% Recall@10 on SPED only using 4096D global descriptors. Code and models will be released upon acceptance.", "link": "http://arxiv.org/abs/2509.25723v2", "date": "2025-12-04", "relevancy": 2.7195, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAGE%3A%20Spatial-visual%20Adaptive%20Graph%20Exploration%20for%20Visual%20Place%20Recognition&body=Title%3A%20SAGE%3A%20Spatial-visual%20Adaptive%20Graph%20Exploration%20for%20Visual%20Place%20Recognition%0AAuthor%3A%20Shunpeng%20Chen%20and%20Changwei%20Wang%20and%20Rongtao%20Xu%20and%20Xingtian%20Pei%20and%20Yukun%20Song%20and%20Jinzhou%20Lin%20and%20Wenhao%20Xu%20and%20Jingyi%20Zhang%20and%20Li%20Guo%20and%20Shibiao%20Xu%0AAbstract%3A%20Visual%20Place%20Recognition%20%28VPR%29%20requires%20robust%20retrieval%20of%20geotagged%20images%20despite%20large%20appearance%2C%20viewpoint%2C%20and%20environmental%20variation.%20Prior%20methods%20focus%20on%20descriptor%20fine-tuning%20or%20fixed%20sampling%20strategies%20yet%20neglect%20the%20dynamic%20interplay%20between%20spatial%20context%20and%20visual%20similarity%20during%20training.%20We%20present%20SAGE%20%28Spatial-visual%20Adaptive%20Graph%20Exploration%29%2C%20a%20unified%20training%20pipeline%20that%20enhances%20granular%20spatial-visual%20discrimination%20by%20jointly%20improving%20local%20feature%20aggregation%2C%20organize%20samples%20during%20training%2C%20and%20hard%20sample%20mining.%20We%20introduce%20a%20lightweight%20Soft%20Probing%20module%20that%20learns%20residual%20weights%20from%20training%20data%20for%20patch%20descriptors%20before%20bilinear%20aggregation%2C%20boosting%20distinctive%20local%20cues.%20During%20training%20we%20reconstruct%20an%20online%20geo-visual%20graph%20that%20fuses%20geographic%20proximity%20and%20current%20visual%20similarity%20so%20that%20candidate%20neighborhoods%20reflect%20the%20evolving%20embedding%20landscape.%20To%20concentrate%20learning%20on%20the%20most%20informative%20place%20neighborhoods%2C%20we%20seed%20clusters%20from%20high-affinity%20anchors%20and%20iteratively%20expand%20them%20with%20a%20greedy%20weighted%20clique%20expansion%20sampler.%20Implemented%20with%20a%20frozen%20DINOv2%20backbone%20and%20parameter-efficient%20fine-tuning%2C%20SAGE%20achieves%20SOTA%20across%20eight%20benchmarks.%20It%20attains%2098.9%25%2C%2095.8%25%2C%2094.5%25%2C%20and%2096.0%25%20Recall%401%20on%20SPED%2C%20Pitts30k-test%2C%20MSLS-val%2C%20and%20Nordland%2C%20respectively.%20Notably%2C%20our%20method%20obtains%20100%25%20Recall%4010%20on%20SPED%20only%20using%204096D%20global%20descriptors.%20Code%20and%20models%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2509.25723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAGE%253A%2520Spatial-visual%2520Adaptive%2520Graph%2520Exploration%2520for%2520Visual%2520Place%2520Recognition%26entry.906535625%3DShunpeng%2520Chen%2520and%2520Changwei%2520Wang%2520and%2520Rongtao%2520Xu%2520and%2520Xingtian%2520Pei%2520and%2520Yukun%2520Song%2520and%2520Jinzhou%2520Lin%2520and%2520Wenhao%2520Xu%2520and%2520Jingyi%2520Zhang%2520and%2520Li%2520Guo%2520and%2520Shibiao%2520Xu%26entry.1292438233%3DVisual%2520Place%2520Recognition%2520%2528VPR%2529%2520requires%2520robust%2520retrieval%2520of%2520geotagged%2520images%2520despite%2520large%2520appearance%252C%2520viewpoint%252C%2520and%2520environmental%2520variation.%2520Prior%2520methods%2520focus%2520on%2520descriptor%2520fine-tuning%2520or%2520fixed%2520sampling%2520strategies%2520yet%2520neglect%2520the%2520dynamic%2520interplay%2520between%2520spatial%2520context%2520and%2520visual%2520similarity%2520during%2520training.%2520We%2520present%2520SAGE%2520%2528Spatial-visual%2520Adaptive%2520Graph%2520Exploration%2529%252C%2520a%2520unified%2520training%2520pipeline%2520that%2520enhances%2520granular%2520spatial-visual%2520discrimination%2520by%2520jointly%2520improving%2520local%2520feature%2520aggregation%252C%2520organize%2520samples%2520during%2520training%252C%2520and%2520hard%2520sample%2520mining.%2520We%2520introduce%2520a%2520lightweight%2520Soft%2520Probing%2520module%2520that%2520learns%2520residual%2520weights%2520from%2520training%2520data%2520for%2520patch%2520descriptors%2520before%2520bilinear%2520aggregation%252C%2520boosting%2520distinctive%2520local%2520cues.%2520During%2520training%2520we%2520reconstruct%2520an%2520online%2520geo-visual%2520graph%2520that%2520fuses%2520geographic%2520proximity%2520and%2520current%2520visual%2520similarity%2520so%2520that%2520candidate%2520neighborhoods%2520reflect%2520the%2520evolving%2520embedding%2520landscape.%2520To%2520concentrate%2520learning%2520on%2520the%2520most%2520informative%2520place%2520neighborhoods%252C%2520we%2520seed%2520clusters%2520from%2520high-affinity%2520anchors%2520and%2520iteratively%2520expand%2520them%2520with%2520a%2520greedy%2520weighted%2520clique%2520expansion%2520sampler.%2520Implemented%2520with%2520a%2520frozen%2520DINOv2%2520backbone%2520and%2520parameter-efficient%2520fine-tuning%252C%2520SAGE%2520achieves%2520SOTA%2520across%2520eight%2520benchmarks.%2520It%2520attains%252098.9%2525%252C%252095.8%2525%252C%252094.5%2525%252C%2520and%252096.0%2525%2520Recall%25401%2520on%2520SPED%252C%2520Pitts30k-test%252C%2520MSLS-val%252C%2520and%2520Nordland%252C%2520respectively.%2520Notably%252C%2520our%2520method%2520obtains%2520100%2525%2520Recall%254010%2520on%2520SPED%2520only%2520using%25204096D%2520global%2520descriptors.%2520Code%2520and%2520models%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAGE%3A%20Spatial-visual%20Adaptive%20Graph%20Exploration%20for%20Visual%20Place%20Recognition&entry.906535625=Shunpeng%20Chen%20and%20Changwei%20Wang%20and%20Rongtao%20Xu%20and%20Xingtian%20Pei%20and%20Yukun%20Song%20and%20Jinzhou%20Lin%20and%20Wenhao%20Xu%20and%20Jingyi%20Zhang%20and%20Li%20Guo%20and%20Shibiao%20Xu&entry.1292438233=Visual%20Place%20Recognition%20%28VPR%29%20requires%20robust%20retrieval%20of%20geotagged%20images%20despite%20large%20appearance%2C%20viewpoint%2C%20and%20environmental%20variation.%20Prior%20methods%20focus%20on%20descriptor%20fine-tuning%20or%20fixed%20sampling%20strategies%20yet%20neglect%20the%20dynamic%20interplay%20between%20spatial%20context%20and%20visual%20similarity%20during%20training.%20We%20present%20SAGE%20%28Spatial-visual%20Adaptive%20Graph%20Exploration%29%2C%20a%20unified%20training%20pipeline%20that%20enhances%20granular%20spatial-visual%20discrimination%20by%20jointly%20improving%20local%20feature%20aggregation%2C%20organize%20samples%20during%20training%2C%20and%20hard%20sample%20mining.%20We%20introduce%20a%20lightweight%20Soft%20Probing%20module%20that%20learns%20residual%20weights%20from%20training%20data%20for%20patch%20descriptors%20before%20bilinear%20aggregation%2C%20boosting%20distinctive%20local%20cues.%20During%20training%20we%20reconstruct%20an%20online%20geo-visual%20graph%20that%20fuses%20geographic%20proximity%20and%20current%20visual%20similarity%20so%20that%20candidate%20neighborhoods%20reflect%20the%20evolving%20embedding%20landscape.%20To%20concentrate%20learning%20on%20the%20most%20informative%20place%20neighborhoods%2C%20we%20seed%20clusters%20from%20high-affinity%20anchors%20and%20iteratively%20expand%20them%20with%20a%20greedy%20weighted%20clique%20expansion%20sampler.%20Implemented%20with%20a%20frozen%20DINOv2%20backbone%20and%20parameter-efficient%20fine-tuning%2C%20SAGE%20achieves%20SOTA%20across%20eight%20benchmarks.%20It%20attains%2098.9%25%2C%2095.8%25%2C%2094.5%25%2C%20and%2096.0%25%20Recall%401%20on%20SPED%2C%20Pitts30k-test%2C%20MSLS-val%2C%20and%20Nordland%2C%20respectively.%20Notably%2C%20our%20method%20obtains%20100%25%20Recall%4010%20on%20SPED%20only%20using%204096D%20global%20descriptors.%20Code%20and%20models%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2509.25723v2&entry.124074799=Read"},
{"title": "OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation", "author": "Yuchen Che and Ryo Furukawa and Asako Kanezaki", "abstract": "Category-level articulated object pose estimation focuses on the pose estimation of unknown articulated objects within known categories. Despite its significance, this task remains challenging due to the varying shapes and poses of objects, expensive dataset annotation costs, and complex real-world environments. In this paper, we propose a novel self-supervised approach that leverages a single-frame point cloud to solve this task. Our model consistently generates reconstruction with a canonical pose and joint state for the entire input object, and it estimates object-level poses that reduce overall pose variance and part-level poses that align each part of the input with its corresponding part of the reconstruction. Experimental results demonstrate that our approach significantly outperforms previous self-supervised methods and is comparable to the state-of-the-art supervised methods. To assess the performance of our model in real-world scenarios, we also introduce a new real-world articulated object benchmark dataset.", "link": "http://arxiv.org/abs/2408.16547v2", "date": "2025-12-04", "relevancy": 2.7132, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5506}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5464}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OP-Align%3A%20Object-level%20and%20Part-level%20Alignment%20for%20Self-supervised%20Category-level%20Articulated%20Object%20Pose%20Estimation&body=Title%3A%20OP-Align%3A%20Object-level%20and%20Part-level%20Alignment%20for%20Self-supervised%20Category-level%20Articulated%20Object%20Pose%20Estimation%0AAuthor%3A%20Yuchen%20Che%20and%20Ryo%20Furukawa%20and%20Asako%20Kanezaki%0AAbstract%3A%20Category-level%20articulated%20object%20pose%20estimation%20focuses%20on%20the%20pose%20estimation%20of%20unknown%20articulated%20objects%20within%20known%20categories.%20Despite%20its%20significance%2C%20this%20task%20remains%20challenging%20due%20to%20the%20varying%20shapes%20and%20poses%20of%20objects%2C%20expensive%20dataset%20annotation%20costs%2C%20and%20complex%20real-world%20environments.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20self-supervised%20approach%20that%20leverages%20a%20single-frame%20point%20cloud%20to%20solve%20this%20task.%20Our%20model%20consistently%20generates%20reconstruction%20with%20a%20canonical%20pose%20and%20joint%20state%20for%20the%20entire%20input%20object%2C%20and%20it%20estimates%20object-level%20poses%20that%20reduce%20overall%20pose%20variance%20and%20part-level%20poses%20that%20align%20each%20part%20of%20the%20input%20with%20its%20corresponding%20part%20of%20the%20reconstruction.%20Experimental%20results%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20previous%20self-supervised%20methods%20and%20is%20comparable%20to%20the%20state-of-the-art%20supervised%20methods.%20To%20assess%20the%20performance%20of%20our%20model%20in%20real-world%20scenarios%2C%20we%20also%20introduce%20a%20new%20real-world%20articulated%20object%20benchmark%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2408.16547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOP-Align%253A%2520Object-level%2520and%2520Part-level%2520Alignment%2520for%2520Self-supervised%2520Category-level%2520Articulated%2520Object%2520Pose%2520Estimation%26entry.906535625%3DYuchen%2520Che%2520and%2520Ryo%2520Furukawa%2520and%2520Asako%2520Kanezaki%26entry.1292438233%3DCategory-level%2520articulated%2520object%2520pose%2520estimation%2520focuses%2520on%2520the%2520pose%2520estimation%2520of%2520unknown%2520articulated%2520objects%2520within%2520known%2520categories.%2520Despite%2520its%2520significance%252C%2520this%2520task%2520remains%2520challenging%2520due%2520to%2520the%2520varying%2520shapes%2520and%2520poses%2520of%2520objects%252C%2520expensive%2520dataset%2520annotation%2520costs%252C%2520and%2520complex%2520real-world%2520environments.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520self-supervised%2520approach%2520that%2520leverages%2520a%2520single-frame%2520point%2520cloud%2520to%2520solve%2520this%2520task.%2520Our%2520model%2520consistently%2520generates%2520reconstruction%2520with%2520a%2520canonical%2520pose%2520and%2520joint%2520state%2520for%2520the%2520entire%2520input%2520object%252C%2520and%2520it%2520estimates%2520object-level%2520poses%2520that%2520reduce%2520overall%2520pose%2520variance%2520and%2520part-level%2520poses%2520that%2520align%2520each%2520part%2520of%2520the%2520input%2520with%2520its%2520corresponding%2520part%2520of%2520the%2520reconstruction.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520previous%2520self-supervised%2520methods%2520and%2520is%2520comparable%2520to%2520the%2520state-of-the-art%2520supervised%2520methods.%2520To%2520assess%2520the%2520performance%2520of%2520our%2520model%2520in%2520real-world%2520scenarios%252C%2520we%2520also%2520introduce%2520a%2520new%2520real-world%2520articulated%2520object%2520benchmark%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OP-Align%3A%20Object-level%20and%20Part-level%20Alignment%20for%20Self-supervised%20Category-level%20Articulated%20Object%20Pose%20Estimation&entry.906535625=Yuchen%20Che%20and%20Ryo%20Furukawa%20and%20Asako%20Kanezaki&entry.1292438233=Category-level%20articulated%20object%20pose%20estimation%20focuses%20on%20the%20pose%20estimation%20of%20unknown%20articulated%20objects%20within%20known%20categories.%20Despite%20its%20significance%2C%20this%20task%20remains%20challenging%20due%20to%20the%20varying%20shapes%20and%20poses%20of%20objects%2C%20expensive%20dataset%20annotation%20costs%2C%20and%20complex%20real-world%20environments.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20self-supervised%20approach%20that%20leverages%20a%20single-frame%20point%20cloud%20to%20solve%20this%20task.%20Our%20model%20consistently%20generates%20reconstruction%20with%20a%20canonical%20pose%20and%20joint%20state%20for%20the%20entire%20input%20object%2C%20and%20it%20estimates%20object-level%20poses%20that%20reduce%20overall%20pose%20variance%20and%20part-level%20poses%20that%20align%20each%20part%20of%20the%20input%20with%20its%20corresponding%20part%20of%20the%20reconstruction.%20Experimental%20results%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20previous%20self-supervised%20methods%20and%20is%20comparable%20to%20the%20state-of-the-art%20supervised%20methods.%20To%20assess%20the%20performance%20of%20our%20model%20in%20real-world%20scenarios%2C%20we%20also%20introduce%20a%20new%20real-world%20articulated%20object%20benchmark%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2408.16547v2&entry.124074799=Read"},
{"title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image", "author": "Yanran Zhang and Ziyi Wang and Wenzhao Zheng and Zheng Zhu and Jie Zhou and Jiwen Lu", "abstract": "Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.", "link": "http://arxiv.org/abs/2512.05044v1", "date": "2025-12-04", "relevancy": 2.7041, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6884}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6711}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%203D%20Geometry%20Reconstruction%20and%20Motion%20Generation%20for%204D%20Synthesis%20from%20a%20Single%20Image&body=Title%3A%20Joint%203D%20Geometry%20Reconstruction%20and%20Motion%20Generation%20for%204D%20Synthesis%20from%20a%20Single%20Image%0AAuthor%3A%20Yanran%20Zhang%20and%20Ziyi%20Wang%20and%20Wenzhao%20Zheng%20and%20Zheng%20Zhu%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20Generating%20interactive%20and%20dynamic%204D%20scenes%20from%20a%20single%20static%20image%20remains%20a%20core%20challenge.%20Most%20existing%20generate-then-reconstruct%20and%20reconstruct-then-generate%20methods%20decouple%20geometry%20from%20motion%2C%20causing%20spatiotemporal%20inconsistencies%20and%20poor%20generalization.%20To%20address%20these%2C%20we%20extend%20the%20reconstruct-then-generate%20framework%20to%20jointly%20perform%20Motion%20generation%20and%20geometric%20Reconstruction%20for%204D%20Synthesis%20%28MoRe4D%29.%20We%20first%20introduce%20TrajScene-60K%2C%20a%20large-scale%20dataset%20of%2060%2C000%20video%20samples%20with%20dense%20point%20trajectories%2C%20addressing%20the%20scarcity%20of%20high-quality%204D%20scene%20data.%20Based%20on%20this%2C%20we%20propose%20a%20diffusion-based%204D%20Scene%20Trajectory%20Generator%20%284D-STraG%29%20to%20jointly%20generate%20geometrically%20consistent%20and%20motion-plausible%204D%20point%20trajectories.%20To%20leverage%20single-view%20priors%2C%20we%20design%20a%20depth-guided%20motion%20normalization%20strategy%20and%20a%20motion-aware%20module%20for%20effective%20geometry%20and%20dynamics%20integration.%20We%20then%20propose%20a%204D%20View%20Synthesis%20Module%20%284D-ViSM%29%20to%20render%20videos%20with%20arbitrary%20camera%20trajectories%20from%204D%20point%20track%20representations.%20Experiments%20show%20that%20MoRe4D%20generates%20high-quality%204D%20scenes%20with%20multi-view%20consistency%20and%20rich%20dynamic%20details%20from%20a%20single%20image.%20Code%3A%20https%3A//github.com/Zhangyr2022/MoRe4D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%25203D%2520Geometry%2520Reconstruction%2520and%2520Motion%2520Generation%2520for%25204D%2520Synthesis%2520from%2520a%2520Single%2520Image%26entry.906535625%3DYanran%2520Zhang%2520and%2520Ziyi%2520Wang%2520and%2520Wenzhao%2520Zheng%2520and%2520Zheng%2520Zhu%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3DGenerating%2520interactive%2520and%2520dynamic%25204D%2520scenes%2520from%2520a%2520single%2520static%2520image%2520remains%2520a%2520core%2520challenge.%2520Most%2520existing%2520generate-then-reconstruct%2520and%2520reconstruct-then-generate%2520methods%2520decouple%2520geometry%2520from%2520motion%252C%2520causing%2520spatiotemporal%2520inconsistencies%2520and%2520poor%2520generalization.%2520To%2520address%2520these%252C%2520we%2520extend%2520the%2520reconstruct-then-generate%2520framework%2520to%2520jointly%2520perform%2520Motion%2520generation%2520and%2520geometric%2520Reconstruction%2520for%25204D%2520Synthesis%2520%2528MoRe4D%2529.%2520We%2520first%2520introduce%2520TrajScene-60K%252C%2520a%2520large-scale%2520dataset%2520of%252060%252C000%2520video%2520samples%2520with%2520dense%2520point%2520trajectories%252C%2520addressing%2520the%2520scarcity%2520of%2520high-quality%25204D%2520scene%2520data.%2520Based%2520on%2520this%252C%2520we%2520propose%2520a%2520diffusion-based%25204D%2520Scene%2520Trajectory%2520Generator%2520%25284D-STraG%2529%2520to%2520jointly%2520generate%2520geometrically%2520consistent%2520and%2520motion-plausible%25204D%2520point%2520trajectories.%2520To%2520leverage%2520single-view%2520priors%252C%2520we%2520design%2520a%2520depth-guided%2520motion%2520normalization%2520strategy%2520and%2520a%2520motion-aware%2520module%2520for%2520effective%2520geometry%2520and%2520dynamics%2520integration.%2520We%2520then%2520propose%2520a%25204D%2520View%2520Synthesis%2520Module%2520%25284D-ViSM%2529%2520to%2520render%2520videos%2520with%2520arbitrary%2520camera%2520trajectories%2520from%25204D%2520point%2520track%2520representations.%2520Experiments%2520show%2520that%2520MoRe4D%2520generates%2520high-quality%25204D%2520scenes%2520with%2520multi-view%2520consistency%2520and%2520rich%2520dynamic%2520details%2520from%2520a%2520single%2520image.%2520Code%253A%2520https%253A//github.com/Zhangyr2022/MoRe4D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%203D%20Geometry%20Reconstruction%20and%20Motion%20Generation%20for%204D%20Synthesis%20from%20a%20Single%20Image&entry.906535625=Yanran%20Zhang%20and%20Ziyi%20Wang%20and%20Wenzhao%20Zheng%20and%20Zheng%20Zhu%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=Generating%20interactive%20and%20dynamic%204D%20scenes%20from%20a%20single%20static%20image%20remains%20a%20core%20challenge.%20Most%20existing%20generate-then-reconstruct%20and%20reconstruct-then-generate%20methods%20decouple%20geometry%20from%20motion%2C%20causing%20spatiotemporal%20inconsistencies%20and%20poor%20generalization.%20To%20address%20these%2C%20we%20extend%20the%20reconstruct-then-generate%20framework%20to%20jointly%20perform%20Motion%20generation%20and%20geometric%20Reconstruction%20for%204D%20Synthesis%20%28MoRe4D%29.%20We%20first%20introduce%20TrajScene-60K%2C%20a%20large-scale%20dataset%20of%2060%2C000%20video%20samples%20with%20dense%20point%20trajectories%2C%20addressing%20the%20scarcity%20of%20high-quality%204D%20scene%20data.%20Based%20on%20this%2C%20we%20propose%20a%20diffusion-based%204D%20Scene%20Trajectory%20Generator%20%284D-STraG%29%20to%20jointly%20generate%20geometrically%20consistent%20and%20motion-plausible%204D%20point%20trajectories.%20To%20leverage%20single-view%20priors%2C%20we%20design%20a%20depth-guided%20motion%20normalization%20strategy%20and%20a%20motion-aware%20module%20for%20effective%20geometry%20and%20dynamics%20integration.%20We%20then%20propose%20a%204D%20View%20Synthesis%20Module%20%284D-ViSM%29%20to%20render%20videos%20with%20arbitrary%20camera%20trajectories%20from%204D%20point%20track%20representations.%20Experiments%20show%20that%20MoRe4D%20generates%20high-quality%204D%20scenes%20with%20multi-view%20consistency%20and%20rich%20dynamic%20details%20from%20a%20single%20image.%20Code%3A%20https%3A//github.com/Zhangyr2022/MoRe4D.&entry.1838667208=http%3A//arxiv.org/abs/2512.05044v1&entry.124074799=Read"},
{"title": "Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI", "author": "Ramya Muthukrishnan and Borjan Gagoski and Aryn Lee and P. Ellen Grant and Elfar Adalsteinsson and Polina Golland and Benjamin Billot", "abstract": "We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.", "link": "http://arxiv.org/abs/2512.04890v1", "date": "2025-12-04", "relevancy": 2.7013, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5532}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5377}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Symmetry-Aware%20Head%20Pose%20Estimation%20for%20Fetal%20MRI&body=Title%3A%20Equivariant%20Symmetry-Aware%20Head%20Pose%20Estimation%20for%20Fetal%20MRI%0AAuthor%3A%20Ramya%20Muthukrishnan%20and%20Borjan%20Gagoski%20and%20Aryn%20Lee%20and%20P.%20Ellen%20Grant%20and%20Elfar%20Adalsteinsson%20and%20Polina%20Golland%20and%20Benjamin%20Billot%0AAbstract%3A%20We%20present%20E%283%29-Pose%2C%20a%20novel%20fast%20pose%20estimation%20method%20that%20jointly%20and%20explicitly%20models%20rotation%20equivariance%20and%20object%20symmetry.%20Our%20work%20is%20motivated%20by%20the%20challenging%20problem%20of%20accounting%20for%20fetal%20head%20motion%20during%20a%20diagnostic%20MRI%20scan.%20We%20aim%20to%20enable%20automatic%20adaptive%20prescription%20of%202D%20diagnostic%20MRI%20slices%20with%206-DoF%20head%20pose%20estimation%2C%20supported%20by%203D%20MRI%20volumes%20rapidly%20acquired%20before%20each%202D%20slice.%20Existing%20methods%20struggle%20to%20generalize%20to%20clinical%20volumes%2C%20due%20to%20pose%20ambiguities%20induced%20by%20inherent%20anatomical%20symmetries%2C%20as%20well%20as%20low%20resolution%2C%20noise%2C%20and%20artifacts.%20In%20contrast%2C%20E%283%29-Pose%20captures%20anatomical%20symmetries%20and%20rigid%20pose%20equivariance%20by%20construction%2C%20and%20yields%20robust%20estimates%20of%20the%20fetal%20head%20pose.%20Our%20experiments%20on%20publicly%20available%20and%20representative%20clinical%20fetal%20MRI%20datasets%20demonstrate%20the%20superior%20robustness%20and%20generalization%20of%20our%20method%20across%20domains.%20Crucially%2C%20E%283%29-Pose%20achieves%20state-of-the-art%20accuracy%20on%20clinical%20MRI%20volumes%2C%20paving%20the%20way%20for%20clinical%20translation.%20Our%20implementation%20is%20available%20at%20github.com/ramyamut/E3-Pose.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Symmetry-Aware%2520Head%2520Pose%2520Estimation%2520for%2520Fetal%2520MRI%26entry.906535625%3DRamya%2520Muthukrishnan%2520and%2520Borjan%2520Gagoski%2520and%2520Aryn%2520Lee%2520and%2520P.%2520Ellen%2520Grant%2520and%2520Elfar%2520Adalsteinsson%2520and%2520Polina%2520Golland%2520and%2520Benjamin%2520Billot%26entry.1292438233%3DWe%2520present%2520E%25283%2529-Pose%252C%2520a%2520novel%2520fast%2520pose%2520estimation%2520method%2520that%2520jointly%2520and%2520explicitly%2520models%2520rotation%2520equivariance%2520and%2520object%2520symmetry.%2520Our%2520work%2520is%2520motivated%2520by%2520the%2520challenging%2520problem%2520of%2520accounting%2520for%2520fetal%2520head%2520motion%2520during%2520a%2520diagnostic%2520MRI%2520scan.%2520We%2520aim%2520to%2520enable%2520automatic%2520adaptive%2520prescription%2520of%25202D%2520diagnostic%2520MRI%2520slices%2520with%25206-DoF%2520head%2520pose%2520estimation%252C%2520supported%2520by%25203D%2520MRI%2520volumes%2520rapidly%2520acquired%2520before%2520each%25202D%2520slice.%2520Existing%2520methods%2520struggle%2520to%2520generalize%2520to%2520clinical%2520volumes%252C%2520due%2520to%2520pose%2520ambiguities%2520induced%2520by%2520inherent%2520anatomical%2520symmetries%252C%2520as%2520well%2520as%2520low%2520resolution%252C%2520noise%252C%2520and%2520artifacts.%2520In%2520contrast%252C%2520E%25283%2529-Pose%2520captures%2520anatomical%2520symmetries%2520and%2520rigid%2520pose%2520equivariance%2520by%2520construction%252C%2520and%2520yields%2520robust%2520estimates%2520of%2520the%2520fetal%2520head%2520pose.%2520Our%2520experiments%2520on%2520publicly%2520available%2520and%2520representative%2520clinical%2520fetal%2520MRI%2520datasets%2520demonstrate%2520the%2520superior%2520robustness%2520and%2520generalization%2520of%2520our%2520method%2520across%2520domains.%2520Crucially%252C%2520E%25283%2529-Pose%2520achieves%2520state-of-the-art%2520accuracy%2520on%2520clinical%2520MRI%2520volumes%252C%2520paving%2520the%2520way%2520for%2520clinical%2520translation.%2520Our%2520implementation%2520is%2520available%2520at%2520github.com/ramyamut/E3-Pose.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Symmetry-Aware%20Head%20Pose%20Estimation%20for%20Fetal%20MRI&entry.906535625=Ramya%20Muthukrishnan%20and%20Borjan%20Gagoski%20and%20Aryn%20Lee%20and%20P.%20Ellen%20Grant%20and%20Elfar%20Adalsteinsson%20and%20Polina%20Golland%20and%20Benjamin%20Billot&entry.1292438233=We%20present%20E%283%29-Pose%2C%20a%20novel%20fast%20pose%20estimation%20method%20that%20jointly%20and%20explicitly%20models%20rotation%20equivariance%20and%20object%20symmetry.%20Our%20work%20is%20motivated%20by%20the%20challenging%20problem%20of%20accounting%20for%20fetal%20head%20motion%20during%20a%20diagnostic%20MRI%20scan.%20We%20aim%20to%20enable%20automatic%20adaptive%20prescription%20of%202D%20diagnostic%20MRI%20slices%20with%206-DoF%20head%20pose%20estimation%2C%20supported%20by%203D%20MRI%20volumes%20rapidly%20acquired%20before%20each%202D%20slice.%20Existing%20methods%20struggle%20to%20generalize%20to%20clinical%20volumes%2C%20due%20to%20pose%20ambiguities%20induced%20by%20inherent%20anatomical%20symmetries%2C%20as%20well%20as%20low%20resolution%2C%20noise%2C%20and%20artifacts.%20In%20contrast%2C%20E%283%29-Pose%20captures%20anatomical%20symmetries%20and%20rigid%20pose%20equivariance%20by%20construction%2C%20and%20yields%20robust%20estimates%20of%20the%20fetal%20head%20pose.%20Our%20experiments%20on%20publicly%20available%20and%20representative%20clinical%20fetal%20MRI%20datasets%20demonstrate%20the%20superior%20robustness%20and%20generalization%20of%20our%20method%20across%20domains.%20Crucially%2C%20E%283%29-Pose%20achieves%20state-of-the-art%20accuracy%20on%20clinical%20MRI%20volumes%2C%20paving%20the%20way%20for%20clinical%20translation.%20Our%20implementation%20is%20available%20at%20github.com/ramyamut/E3-Pose.&entry.1838667208=http%3A//arxiv.org/abs/2512.04890v1&entry.124074799=Read"},
{"title": "Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks", "author": "Leonid Pogorelyuk and Niels Bracher and Aaron Verkleeren and Lars K\u00fchmichel and Stefan T. Radev", "abstract": "We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.", "link": "http://arxiv.org/abs/2512.04970v1", "date": "2025-12-04", "relevancy": 2.691, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5528}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Single-Pixel%20Contrastive%20Learning%20for%20Semantic%20and%20Geometric%20Tasks&body=Title%3A%20Stable%20Single-Pixel%20Contrastive%20Learning%20for%20Semantic%20and%20Geometric%20Tasks%0AAuthor%3A%20Leonid%20Pogorelyuk%20and%20Niels%20Bracher%20and%20Aaron%20Verkleeren%20and%20Lars%20K%C3%BChmichel%20and%20Stefan%20T.%20Radev%0AAbstract%3A%20We%20pilot%20a%20family%20of%20stable%20contrastive%20losses%20for%20learning%20pixel-level%20representations%20that%20jointly%20capture%20semantic%20and%20geometric%20information.%20Our%20approach%20maps%20each%20pixel%20of%20an%20image%20to%20an%20overcomplete%20descriptor%20that%20is%20both%20view-invariant%20and%20semantically%20meaningful.%20It%20enables%20precise%20point-correspondence%20across%20images%20without%20requiring%20momentum-based%20teacher-student%20training.%20Two%20experiments%20in%20synthetic%202D%20and%203D%20environments%20demonstrate%20the%20properties%20of%20our%20loss%20and%20the%20resulting%20overcomplete%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Single-Pixel%2520Contrastive%2520Learning%2520for%2520Semantic%2520and%2520Geometric%2520Tasks%26entry.906535625%3DLeonid%2520Pogorelyuk%2520and%2520Niels%2520Bracher%2520and%2520Aaron%2520Verkleeren%2520and%2520Lars%2520K%25C3%25BChmichel%2520and%2520Stefan%2520T.%2520Radev%26entry.1292438233%3DWe%2520pilot%2520a%2520family%2520of%2520stable%2520contrastive%2520losses%2520for%2520learning%2520pixel-level%2520representations%2520that%2520jointly%2520capture%2520semantic%2520and%2520geometric%2520information.%2520Our%2520approach%2520maps%2520each%2520pixel%2520of%2520an%2520image%2520to%2520an%2520overcomplete%2520descriptor%2520that%2520is%2520both%2520view-invariant%2520and%2520semantically%2520meaningful.%2520It%2520enables%2520precise%2520point-correspondence%2520across%2520images%2520without%2520requiring%2520momentum-based%2520teacher-student%2520training.%2520Two%2520experiments%2520in%2520synthetic%25202D%2520and%25203D%2520environments%2520demonstrate%2520the%2520properties%2520of%2520our%2520loss%2520and%2520the%2520resulting%2520overcomplete%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Single-Pixel%20Contrastive%20Learning%20for%20Semantic%20and%20Geometric%20Tasks&entry.906535625=Leonid%20Pogorelyuk%20and%20Niels%20Bracher%20and%20Aaron%20Verkleeren%20and%20Lars%20K%C3%BChmichel%20and%20Stefan%20T.%20Radev&entry.1292438233=We%20pilot%20a%20family%20of%20stable%20contrastive%20losses%20for%20learning%20pixel-level%20representations%20that%20jointly%20capture%20semantic%20and%20geometric%20information.%20Our%20approach%20maps%20each%20pixel%20of%20an%20image%20to%20an%20overcomplete%20descriptor%20that%20is%20both%20view-invariant%20and%20semantically%20meaningful.%20It%20enables%20precise%20point-correspondence%20across%20images%20without%20requiring%20momentum-based%20teacher-student%20training.%20Two%20experiments%20in%20synthetic%202D%20and%203D%20environments%20demonstrate%20the%20properties%20of%20our%20loss%20and%20the%20resulting%20overcomplete%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2512.04970v1&entry.124074799=Read"},
{"title": "TongUI: Internet-Scale Trajectories from Multimodal Web Tutorials for Generalized GUI Agents", "author": "Bofei Zhang and Zirui Shang and Zhi Gao and Wang Zhang and Rui Xie and Xiaojian Ma and Tao Yuan and Xinxiao Wu and Song-Chun Zhu and Qing Li", "abstract": "Building Graphical User Interface (GUI) agents is a promising research direction, which simulates human interaction with computers or mobile phones to perform diverse GUI tasks. However, a major challenge in developing generalized GUI agents is the lack of sufficient trajectory data across various operating systems and applications, mainly due to the high cost of manual annotations. In this paper, we propose the TongUI framework that builds generalized GUI agents by learning from rich multimodal web tutorials. Concretely, we crawl and process online GUI tutorials (such as videos and articles) into GUI agent trajectory data, through which we produce the GUI-Net dataset containing 143K trajectory data across five operating systems and more than 200 applications. We develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net, which show remarkable performance improvements on commonly used grounding and navigation benchmarks, outperforming baseline agents about 10\\% on multiple benchmarks, showing the effectiveness of the GUI-Net dataset and underscoring the significance of our TongUI framework. We will fully open-source the code, the GUI-Net dataset, and the trained models soon.", "link": "http://arxiv.org/abs/2504.12679v4", "date": "2025-12-04", "relevancy": 2.6785, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5574}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TongUI%3A%20Internet-Scale%20Trajectories%20from%20Multimodal%20Web%20Tutorials%20for%20Generalized%20GUI%20Agents&body=Title%3A%20TongUI%3A%20Internet-Scale%20Trajectories%20from%20Multimodal%20Web%20Tutorials%20for%20Generalized%20GUI%20Agents%0AAuthor%3A%20Bofei%20Zhang%20and%20Zirui%20Shang%20and%20Zhi%20Gao%20and%20Wang%20Zhang%20and%20Rui%20Xie%20and%20Xiaojian%20Ma%20and%20Tao%20Yuan%20and%20Xinxiao%20Wu%20and%20Song-Chun%20Zhu%20and%20Qing%20Li%0AAbstract%3A%20Building%20Graphical%20User%20Interface%20%28GUI%29%20agents%20is%20a%20promising%20research%20direction%2C%20which%20simulates%20human%20interaction%20with%20computers%20or%20mobile%20phones%20to%20perform%20diverse%20GUI%20tasks.%20However%2C%20a%20major%20challenge%20in%20developing%20generalized%20GUI%20agents%20is%20the%20lack%20of%20sufficient%20trajectory%20data%20across%20various%20operating%20systems%20and%20applications%2C%20mainly%20due%20to%20the%20high%20cost%20of%20manual%20annotations.%20In%20this%20paper%2C%20we%20propose%20the%20TongUI%20framework%20that%20builds%20generalized%20GUI%20agents%20by%20learning%20from%20rich%20multimodal%20web%20tutorials.%20Concretely%2C%20we%20crawl%20and%20process%20online%20GUI%20tutorials%20%28such%20as%20videos%20and%20articles%29%20into%20GUI%20agent%20trajectory%20data%2C%20through%20which%20we%20produce%20the%20GUI-Net%20dataset%20containing%20143K%20trajectory%20data%20across%20five%20operating%20systems%20and%20more%20than%20200%20applications.%20We%20develop%20the%20TongUI%20agent%20by%20fine-tuning%20Qwen2.5-VL-3B/7B%20models%20on%20GUI-Net%2C%20which%20show%20remarkable%20performance%20improvements%20on%20commonly%20used%20grounding%20and%20navigation%20benchmarks%2C%20outperforming%20baseline%20agents%20about%2010%5C%25%20on%20multiple%20benchmarks%2C%20showing%20the%20effectiveness%20of%20the%20GUI-Net%20dataset%20and%20underscoring%20the%20significance%20of%20our%20TongUI%20framework.%20We%20will%20fully%20open-source%20the%20code%2C%20the%20GUI-Net%20dataset%2C%20and%20the%20trained%20models%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2504.12679v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTongUI%253A%2520Internet-Scale%2520Trajectories%2520from%2520Multimodal%2520Web%2520Tutorials%2520for%2520Generalized%2520GUI%2520Agents%26entry.906535625%3DBofei%2520Zhang%2520and%2520Zirui%2520Shang%2520and%2520Zhi%2520Gao%2520and%2520Wang%2520Zhang%2520and%2520Rui%2520Xie%2520and%2520Xiaojian%2520Ma%2520and%2520Tao%2520Yuan%2520and%2520Xinxiao%2520Wu%2520and%2520Song-Chun%2520Zhu%2520and%2520Qing%2520Li%26entry.1292438233%3DBuilding%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520is%2520a%2520promising%2520research%2520direction%252C%2520which%2520simulates%2520human%2520interaction%2520with%2520computers%2520or%2520mobile%2520phones%2520to%2520perform%2520diverse%2520GUI%2520tasks.%2520However%252C%2520a%2520major%2520challenge%2520in%2520developing%2520generalized%2520GUI%2520agents%2520is%2520the%2520lack%2520of%2520sufficient%2520trajectory%2520data%2520across%2520various%2520operating%2520systems%2520and%2520applications%252C%2520mainly%2520due%2520to%2520the%2520high%2520cost%2520of%2520manual%2520annotations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520TongUI%2520framework%2520that%2520builds%2520generalized%2520GUI%2520agents%2520by%2520learning%2520from%2520rich%2520multimodal%2520web%2520tutorials.%2520Concretely%252C%2520we%2520crawl%2520and%2520process%2520online%2520GUI%2520tutorials%2520%2528such%2520as%2520videos%2520and%2520articles%2529%2520into%2520GUI%2520agent%2520trajectory%2520data%252C%2520through%2520which%2520we%2520produce%2520the%2520GUI-Net%2520dataset%2520containing%2520143K%2520trajectory%2520data%2520across%2520five%2520operating%2520systems%2520and%2520more%2520than%2520200%2520applications.%2520We%2520develop%2520the%2520TongUI%2520agent%2520by%2520fine-tuning%2520Qwen2.5-VL-3B/7B%2520models%2520on%2520GUI-Net%252C%2520which%2520show%2520remarkable%2520performance%2520improvements%2520on%2520commonly%2520used%2520grounding%2520and%2520navigation%2520benchmarks%252C%2520outperforming%2520baseline%2520agents%2520about%252010%255C%2525%2520on%2520multiple%2520benchmarks%252C%2520showing%2520the%2520effectiveness%2520of%2520the%2520GUI-Net%2520dataset%2520and%2520underscoring%2520the%2520significance%2520of%2520our%2520TongUI%2520framework.%2520We%2520will%2520fully%2520open-source%2520the%2520code%252C%2520the%2520GUI-Net%2520dataset%252C%2520and%2520the%2520trained%2520models%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12679v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TongUI%3A%20Internet-Scale%20Trajectories%20from%20Multimodal%20Web%20Tutorials%20for%20Generalized%20GUI%20Agents&entry.906535625=Bofei%20Zhang%20and%20Zirui%20Shang%20and%20Zhi%20Gao%20and%20Wang%20Zhang%20and%20Rui%20Xie%20and%20Xiaojian%20Ma%20and%20Tao%20Yuan%20and%20Xinxiao%20Wu%20and%20Song-Chun%20Zhu%20and%20Qing%20Li&entry.1292438233=Building%20Graphical%20User%20Interface%20%28GUI%29%20agents%20is%20a%20promising%20research%20direction%2C%20which%20simulates%20human%20interaction%20with%20computers%20or%20mobile%20phones%20to%20perform%20diverse%20GUI%20tasks.%20However%2C%20a%20major%20challenge%20in%20developing%20generalized%20GUI%20agents%20is%20the%20lack%20of%20sufficient%20trajectory%20data%20across%20various%20operating%20systems%20and%20applications%2C%20mainly%20due%20to%20the%20high%20cost%20of%20manual%20annotations.%20In%20this%20paper%2C%20we%20propose%20the%20TongUI%20framework%20that%20builds%20generalized%20GUI%20agents%20by%20learning%20from%20rich%20multimodal%20web%20tutorials.%20Concretely%2C%20we%20crawl%20and%20process%20online%20GUI%20tutorials%20%28such%20as%20videos%20and%20articles%29%20into%20GUI%20agent%20trajectory%20data%2C%20through%20which%20we%20produce%20the%20GUI-Net%20dataset%20containing%20143K%20trajectory%20data%20across%20five%20operating%20systems%20and%20more%20than%20200%20applications.%20We%20develop%20the%20TongUI%20agent%20by%20fine-tuning%20Qwen2.5-VL-3B/7B%20models%20on%20GUI-Net%2C%20which%20show%20remarkable%20performance%20improvements%20on%20commonly%20used%20grounding%20and%20navigation%20benchmarks%2C%20outperforming%20baseline%20agents%20about%2010%5C%25%20on%20multiple%20benchmarks%2C%20showing%20the%20effectiveness%20of%20the%20GUI-Net%20dataset%20and%20underscoring%20the%20significance%20of%20our%20TongUI%20framework.%20We%20will%20fully%20open-source%20the%20code%2C%20the%20GUI-Net%20dataset%2C%20and%20the%20trained%20models%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2504.12679v4&entry.124074799=Read"},
{"title": "EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?", "author": "Pierre Adorni and Minh-Tan Pham and St\u00e9phane May and S\u00e9bastien Lef\u00e8vre", "abstract": "Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs. All codes and pretrained models are available at https://github.com/pierreadorni/EoS-FM.", "link": "http://arxiv.org/abs/2511.21523v2", "date": "2025-12-04", "relevancy": 2.6382, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EoS-FM%3A%20Can%20an%20Ensemble%20of%20Specialist%20Models%20act%20as%20a%20Generalist%20Feature%20Extractor%3F&body=Title%3A%20EoS-FM%3A%20Can%20an%20Ensemble%20of%20Specialist%20Models%20act%20as%20a%20Generalist%20Feature%20Extractor%3F%0AAuthor%3A%20Pierre%20Adorni%20and%20Minh-Tan%20Pham%20and%20St%C3%A9phane%20May%20and%20S%C3%A9bastien%20Lef%C3%A8vre%0AAbstract%3A%20Recent%20advances%20in%20foundation%20models%20have%20shown%20great%20promise%20in%20domains%20such%20as%20natural%20language%20processing%20and%20computer%20vision%2C%20and%20similar%20efforts%20are%20now%20emerging%20in%20the%20Earth%20Observation%20community.%20These%20models%20aim%20to%20generalize%20across%20tasks%20with%20limited%20supervision%2C%20reducing%20the%20need%20for%20training%20separate%20models%20for%20each%20task.%20However%2C%20current%20strategies%2C%20which%20largely%20focus%20on%20scaling%20model%20size%20and%20dataset%20volume%2C%20require%20prohibitive%20computational%20and%20data%20resources%2C%20limiting%20accessibility%20to%20only%20a%20few%20large%20institutions.%20Moreover%2C%20this%20paradigm%20of%20ever-larger%20models%20stands%20in%20stark%20contrast%20with%20the%20principles%20of%20sustainable%20and%20environmentally%20responsible%20AI%2C%20as%20it%20leads%20to%20immense%20carbon%20footprints%20and%20resource%20inefficiency.%20In%20this%20work%2C%20we%20present%20a%20novel%20and%20efficient%20alternative%3A%20an%20Ensemble-of-Specialists%20framework%20for%20building%20Remote%20Sensing%20Foundation%20Models%20%28RSFMs%29.%20Our%20method%20decomposes%20the%20training%20process%20into%20lightweight%2C%20task-specific%20ConvNeXtV2%20specialists%20that%20can%20be%20frozen%20and%20reused.%20This%20modular%20approach%20offers%20strong%20advantages%20in%20efficiency%2C%20interpretability%2C%20and%20extensibility.%20Moreover%2C%20it%20naturally%20supports%20federated%20training%2C%20pruning%2C%20and%20continuous%20specialist%20integration%2C%20making%20it%20particularly%20well-suited%20for%20collaborative%20and%20resource-constrained%20settings.%20Our%20framework%20sets%20a%20new%20direction%20for%20building%20scalable%20and%20efficient%20RSFMs.%20All%20codes%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/pierreadorni/EoS-FM.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEoS-FM%253A%2520Can%2520an%2520Ensemble%2520of%2520Specialist%2520Models%2520act%2520as%2520a%2520Generalist%2520Feature%2520Extractor%253F%26entry.906535625%3DPierre%2520Adorni%2520and%2520Minh-Tan%2520Pham%2520and%2520St%25C3%25A9phane%2520May%2520and%2520S%25C3%25A9bastien%2520Lef%25C3%25A8vre%26entry.1292438233%3DRecent%2520advances%2520in%2520foundation%2520models%2520have%2520shown%2520great%2520promise%2520in%2520domains%2520such%2520as%2520natural%2520language%2520processing%2520and%2520computer%2520vision%252C%2520and%2520similar%2520efforts%2520are%2520now%2520emerging%2520in%2520the%2520Earth%2520Observation%2520community.%2520These%2520models%2520aim%2520to%2520generalize%2520across%2520tasks%2520with%2520limited%2520supervision%252C%2520reducing%2520the%2520need%2520for%2520training%2520separate%2520models%2520for%2520each%2520task.%2520However%252C%2520current%2520strategies%252C%2520which%2520largely%2520focus%2520on%2520scaling%2520model%2520size%2520and%2520dataset%2520volume%252C%2520require%2520prohibitive%2520computational%2520and%2520data%2520resources%252C%2520limiting%2520accessibility%2520to%2520only%2520a%2520few%2520large%2520institutions.%2520Moreover%252C%2520this%2520paradigm%2520of%2520ever-larger%2520models%2520stands%2520in%2520stark%2520contrast%2520with%2520the%2520principles%2520of%2520sustainable%2520and%2520environmentally%2520responsible%2520AI%252C%2520as%2520it%2520leads%2520to%2520immense%2520carbon%2520footprints%2520and%2520resource%2520inefficiency.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520and%2520efficient%2520alternative%253A%2520an%2520Ensemble-of-Specialists%2520framework%2520for%2520building%2520Remote%2520Sensing%2520Foundation%2520Models%2520%2528RSFMs%2529.%2520Our%2520method%2520decomposes%2520the%2520training%2520process%2520into%2520lightweight%252C%2520task-specific%2520ConvNeXtV2%2520specialists%2520that%2520can%2520be%2520frozen%2520and%2520reused.%2520This%2520modular%2520approach%2520offers%2520strong%2520advantages%2520in%2520efficiency%252C%2520interpretability%252C%2520and%2520extensibility.%2520Moreover%252C%2520it%2520naturally%2520supports%2520federated%2520training%252C%2520pruning%252C%2520and%2520continuous%2520specialist%2520integration%252C%2520making%2520it%2520particularly%2520well-suited%2520for%2520collaborative%2520and%2520resource-constrained%2520settings.%2520Our%2520framework%2520sets%2520a%2520new%2520direction%2520for%2520building%2520scalable%2520and%2520efficient%2520RSFMs.%2520All%2520codes%2520and%2520pretrained%2520models%2520are%2520available%2520at%2520https%253A//github.com/pierreadorni/EoS-FM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EoS-FM%3A%20Can%20an%20Ensemble%20of%20Specialist%20Models%20act%20as%20a%20Generalist%20Feature%20Extractor%3F&entry.906535625=Pierre%20Adorni%20and%20Minh-Tan%20Pham%20and%20St%C3%A9phane%20May%20and%20S%C3%A9bastien%20Lef%C3%A8vre&entry.1292438233=Recent%20advances%20in%20foundation%20models%20have%20shown%20great%20promise%20in%20domains%20such%20as%20natural%20language%20processing%20and%20computer%20vision%2C%20and%20similar%20efforts%20are%20now%20emerging%20in%20the%20Earth%20Observation%20community.%20These%20models%20aim%20to%20generalize%20across%20tasks%20with%20limited%20supervision%2C%20reducing%20the%20need%20for%20training%20separate%20models%20for%20each%20task.%20However%2C%20current%20strategies%2C%20which%20largely%20focus%20on%20scaling%20model%20size%20and%20dataset%20volume%2C%20require%20prohibitive%20computational%20and%20data%20resources%2C%20limiting%20accessibility%20to%20only%20a%20few%20large%20institutions.%20Moreover%2C%20this%20paradigm%20of%20ever-larger%20models%20stands%20in%20stark%20contrast%20with%20the%20principles%20of%20sustainable%20and%20environmentally%20responsible%20AI%2C%20as%20it%20leads%20to%20immense%20carbon%20footprints%20and%20resource%20inefficiency.%20In%20this%20work%2C%20we%20present%20a%20novel%20and%20efficient%20alternative%3A%20an%20Ensemble-of-Specialists%20framework%20for%20building%20Remote%20Sensing%20Foundation%20Models%20%28RSFMs%29.%20Our%20method%20decomposes%20the%20training%20process%20into%20lightweight%2C%20task-specific%20ConvNeXtV2%20specialists%20that%20can%20be%20frozen%20and%20reused.%20This%20modular%20approach%20offers%20strong%20advantages%20in%20efficiency%2C%20interpretability%2C%20and%20extensibility.%20Moreover%2C%20it%20naturally%20supports%20federated%20training%2C%20pruning%2C%20and%20continuous%20specialist%20integration%2C%20making%20it%20particularly%20well-suited%20for%20collaborative%20and%20resource-constrained%20settings.%20Our%20framework%20sets%20a%20new%20direction%20for%20building%20scalable%20and%20efficient%20RSFMs.%20All%20codes%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/pierreadorni/EoS-FM.&entry.1838667208=http%3A//arxiv.org/abs/2511.21523v2&entry.124074799=Read"},
{"title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models", "author": "Akshit Singh and Shyam Marjit and Wei Lin and Paul Gavrikov and Serena Yeung-Levy and Hilde Kuehne and Rogerio Feris and Sivan Doveh and James Glass and M. Jehanzeb Mirza", "abstract": "Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets. Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.", "link": "http://arxiv.org/abs/2510.06783v2", "date": "2025-12-04", "relevancy": 2.6325, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTRV%3A%20Test-Time%20Reinforcement%20Learning%20for%20Vision%20Language%20Models&body=Title%3A%20TTRV%3A%20Test-Time%20Reinforcement%20Learning%20for%20Vision%20Language%20Models%0AAuthor%3A%20Akshit%20Singh%20and%20Shyam%20Marjit%20and%20Wei%20Lin%20and%20Paul%20Gavrikov%20and%20Serena%20Yeung-Levy%20and%20Hilde%20Kuehne%20and%20Rogerio%20Feris%20and%20Sivan%20Doveh%20and%20James%20Glass%20and%20M.%20Jehanzeb%20Mirza%0AAbstract%3A%20Existing%20methods%20for%20extracting%20reward%20signals%20in%20Reinforcement%20Learning%20typically%20rely%20on%20labeled%20data%20and%20dedicated%20training%20splits%2C%20a%20setup%20that%20contrasts%20with%20how%20humans%20learn%20directly%20from%20their%20environment.%20In%20this%20work%2C%20we%20propose%20TTRV%20to%20enhance%20vision%20language%20understanding%20by%20adapting%20the%20model%20on%20the%20fly%20at%20inference%20time%2C%20without%20the%20need%20for%20any%20labeled%20data.%20Concretely%2C%20we%20enhance%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20framework%20by%20designing%20rewards%20based%20on%20the%20frequency%20of%20the%20base%20model%27s%20output%2C%20while%20inferring%20on%20each%20test%20sample%20multiple%20times.%20Further%2C%20we%20also%20propose%20to%20control%20the%20diversity%20of%20the%20model%27s%20output%20by%20simultaneously%20rewarding%20the%20model%20for%20obtaining%20low%20entropy%20of%20the%20output%20empirical%20distribution.%20Our%20approach%20delivers%20consistent%20gains%20across%20both%20object%20recognition%20and%20visual%20question%20answering%20%28VQA%29%2C%20with%20improvements%20of%20up%20to%2052.4%25%20and%2029.8%25%2C%20respectively%2C%20and%20average%20boosts%20of%2024.6%25%20and%2010.0%25%20across%2016%20datasets.%20Remarkably%2C%20on%20image%20recognition%2C%20TTRV%20applied%20to%20InternVL%208B%20surpasses%20GPT-4o%20by%20an%20average%20of%202.3%25%20over%208%20benchmarks%2C%20while%20remaining%20highly%20competitive%20on%20VQA%2C%20demonstrating%20that%20test-time%20reinforcement%20learning%20can%20match%20or%20exceed%20the%20strongest%20proprietary%20models.%20Finally%2C%20we%20find%20many%20interesting%20properties%20of%20test-time%20RL%20for%20VLMs%3A%20for%20example%2C%20even%20in%20extremely%20data-constrained%20scenarios%2C%20where%20adaptation%20is%20performed%20on%20a%20single%20randomly%20chosen%20unlabeled%20test%20example%2C%20TTRV%20still%20yields%20non-trivial%20improvements%20of%20up%20to%205.5%25%20in%20recognition%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2510.06783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTRV%253A%2520Test-Time%2520Reinforcement%2520Learning%2520for%2520Vision%2520Language%2520Models%26entry.906535625%3DAkshit%2520Singh%2520and%2520Shyam%2520Marjit%2520and%2520Wei%2520Lin%2520and%2520Paul%2520Gavrikov%2520and%2520Serena%2520Yeung-Levy%2520and%2520Hilde%2520Kuehne%2520and%2520Rogerio%2520Feris%2520and%2520Sivan%2520Doveh%2520and%2520James%2520Glass%2520and%2520M.%2520Jehanzeb%2520Mirza%26entry.1292438233%3DExisting%2520methods%2520for%2520extracting%2520reward%2520signals%2520in%2520Reinforcement%2520Learning%2520typically%2520rely%2520on%2520labeled%2520data%2520and%2520dedicated%2520training%2520splits%252C%2520a%2520setup%2520that%2520contrasts%2520with%2520how%2520humans%2520learn%2520directly%2520from%2520their%2520environment.%2520In%2520this%2520work%252C%2520we%2520propose%2520TTRV%2520to%2520enhance%2520vision%2520language%2520understanding%2520by%2520adapting%2520the%2520model%2520on%2520the%2520fly%2520at%2520inference%2520time%252C%2520without%2520the%2520need%2520for%2520any%2520labeled%2520data.%2520Concretely%252C%2520we%2520enhance%2520the%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520framework%2520by%2520designing%2520rewards%2520based%2520on%2520the%2520frequency%2520of%2520the%2520base%2520model%2527s%2520output%252C%2520while%2520inferring%2520on%2520each%2520test%2520sample%2520multiple%2520times.%2520Further%252C%2520we%2520also%2520propose%2520to%2520control%2520the%2520diversity%2520of%2520the%2520model%2527s%2520output%2520by%2520simultaneously%2520rewarding%2520the%2520model%2520for%2520obtaining%2520low%2520entropy%2520of%2520the%2520output%2520empirical%2520distribution.%2520Our%2520approach%2520delivers%2520consistent%2520gains%2520across%2520both%2520object%2520recognition%2520and%2520visual%2520question%2520answering%2520%2528VQA%2529%252C%2520with%2520improvements%2520of%2520up%2520to%252052.4%2525%2520and%252029.8%2525%252C%2520respectively%252C%2520and%2520average%2520boosts%2520of%252024.6%2525%2520and%252010.0%2525%2520across%252016%2520datasets.%2520Remarkably%252C%2520on%2520image%2520recognition%252C%2520TTRV%2520applied%2520to%2520InternVL%25208B%2520surpasses%2520GPT-4o%2520by%2520an%2520average%2520of%25202.3%2525%2520over%25208%2520benchmarks%252C%2520while%2520remaining%2520highly%2520competitive%2520on%2520VQA%252C%2520demonstrating%2520that%2520test-time%2520reinforcement%2520learning%2520can%2520match%2520or%2520exceed%2520the%2520strongest%2520proprietary%2520models.%2520Finally%252C%2520we%2520find%2520many%2520interesting%2520properties%2520of%2520test-time%2520RL%2520for%2520VLMs%253A%2520for%2520example%252C%2520even%2520in%2520extremely%2520data-constrained%2520scenarios%252C%2520where%2520adaptation%2520is%2520performed%2520on%2520a%2520single%2520randomly%2520chosen%2520unlabeled%2520test%2520example%252C%2520TTRV%2520still%2520yields%2520non-trivial%2520improvements%2520of%2520up%2520to%25205.5%2525%2520in%2520recognition%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTRV%3A%20Test-Time%20Reinforcement%20Learning%20for%20Vision%20Language%20Models&entry.906535625=Akshit%20Singh%20and%20Shyam%20Marjit%20and%20Wei%20Lin%20and%20Paul%20Gavrikov%20and%20Serena%20Yeung-Levy%20and%20Hilde%20Kuehne%20and%20Rogerio%20Feris%20and%20Sivan%20Doveh%20and%20James%20Glass%20and%20M.%20Jehanzeb%20Mirza&entry.1292438233=Existing%20methods%20for%20extracting%20reward%20signals%20in%20Reinforcement%20Learning%20typically%20rely%20on%20labeled%20data%20and%20dedicated%20training%20splits%2C%20a%20setup%20that%20contrasts%20with%20how%20humans%20learn%20directly%20from%20their%20environment.%20In%20this%20work%2C%20we%20propose%20TTRV%20to%20enhance%20vision%20language%20understanding%20by%20adapting%20the%20model%20on%20the%20fly%20at%20inference%20time%2C%20without%20the%20need%20for%20any%20labeled%20data.%20Concretely%2C%20we%20enhance%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20framework%20by%20designing%20rewards%20based%20on%20the%20frequency%20of%20the%20base%20model%27s%20output%2C%20while%20inferring%20on%20each%20test%20sample%20multiple%20times.%20Further%2C%20we%20also%20propose%20to%20control%20the%20diversity%20of%20the%20model%27s%20output%20by%20simultaneously%20rewarding%20the%20model%20for%20obtaining%20low%20entropy%20of%20the%20output%20empirical%20distribution.%20Our%20approach%20delivers%20consistent%20gains%20across%20both%20object%20recognition%20and%20visual%20question%20answering%20%28VQA%29%2C%20with%20improvements%20of%20up%20to%2052.4%25%20and%2029.8%25%2C%20respectively%2C%20and%20average%20boosts%20of%2024.6%25%20and%2010.0%25%20across%2016%20datasets.%20Remarkably%2C%20on%20image%20recognition%2C%20TTRV%20applied%20to%20InternVL%208B%20surpasses%20GPT-4o%20by%20an%20average%20of%202.3%25%20over%208%20benchmarks%2C%20while%20remaining%20highly%20competitive%20on%20VQA%2C%20demonstrating%20that%20test-time%20reinforcement%20learning%20can%20match%20or%20exceed%20the%20strongest%20proprietary%20models.%20Finally%2C%20we%20find%20many%20interesting%20properties%20of%20test-time%20RL%20for%20VLMs%3A%20for%20example%2C%20even%20in%20extremely%20data-constrained%20scenarios%2C%20where%20adaptation%20is%20performed%20on%20a%20single%20randomly%20chosen%20unlabeled%20test%20example%2C%20TTRV%20still%20yields%20non-trivial%20improvements%20of%20up%20to%205.5%25%20in%20recognition%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2510.06783v2&entry.124074799=Read"},
{"title": "SignBind-LLM: Multi-Stage Modality Fusion for Sign Language Translation", "author": "Marshall Thomas and Edward Fish and Richard Bowden", "abstract": "Despite progress in gloss-free Sign Language Translation (SLT), traditional single modality end-to-end approaches consistently fail on two critical components of natural signing: the precise recognition of high-speed fingerspelling and the integration of asynchronous non-manual cues from the face. Recent progress in SLT with Large Language Models has side stepped this challenge, forcing a single network to learn these simultaneously resulting in poor performance when tasked with translating crucial information such as names, places, and technical terms. We introduce SignBind-LLM, a modular framework designed to overcome these limitations. Our approach employs separate, specialized predictors for continuous signing, fingerspelling, and lipreading. Each expert network first decodes its specific modality into a sequence of tokens. These parallel streams are then fused by a lightweight transformer that resolves temporal misalignments before passing the combined representation to a Large Language Model (LLM) for final sentence generation. Our method establishes a new state-of-the-art on the How2Sign, ChicagoFSWildPlus, and BOBSL datasets with a BLEU-4 score of 22.1, 73.2% letter accuracy and BLEU-4 score of 6.8 respectively. These results validate our core hypothesis: isolating and solving distinct recognition tasks before fusion provides a more powerful and effective pathway to robust, high-fidelity sign language translation.", "link": "http://arxiv.org/abs/2509.00030v3", "date": "2025-12-04", "relevancy": 2.6324, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SignBind-LLM%3A%20Multi-Stage%20Modality%20Fusion%20for%20Sign%20Language%20Translation&body=Title%3A%20SignBind-LLM%3A%20Multi-Stage%20Modality%20Fusion%20for%20Sign%20Language%20Translation%0AAuthor%3A%20Marshall%20Thomas%20and%20Edward%20Fish%20and%20Richard%20Bowden%0AAbstract%3A%20Despite%20progress%20in%20gloss-free%20Sign%20Language%20Translation%20%28SLT%29%2C%20traditional%20single%20modality%20end-to-end%20approaches%20consistently%20fail%20on%20two%20critical%20components%20of%20natural%20signing%3A%20the%20precise%20recognition%20of%20high-speed%20fingerspelling%20and%20the%20integration%20of%20asynchronous%20non-manual%20cues%20from%20the%20face.%20Recent%20progress%20in%20SLT%20with%20Large%20Language%20Models%20has%20side%20stepped%20this%20challenge%2C%20forcing%20a%20single%20network%20to%20learn%20these%20simultaneously%20resulting%20in%20poor%20performance%20when%20tasked%20with%20translating%20crucial%20information%20such%20as%20names%2C%20places%2C%20and%20technical%20terms.%20We%20introduce%20SignBind-LLM%2C%20a%20modular%20framework%20designed%20to%20overcome%20these%20limitations.%20Our%20approach%20employs%20separate%2C%20specialized%20predictors%20for%20continuous%20signing%2C%20fingerspelling%2C%20and%20lipreading.%20Each%20expert%20network%20first%20decodes%20its%20specific%20modality%20into%20a%20sequence%20of%20tokens.%20These%20parallel%20streams%20are%20then%20fused%20by%20a%20lightweight%20transformer%20that%20resolves%20temporal%20misalignments%20before%20passing%20the%20combined%20representation%20to%20a%20Large%20Language%20Model%20%28LLM%29%20for%20final%20sentence%20generation.%20Our%20method%20establishes%20a%20new%20state-of-the-art%20on%20the%20How2Sign%2C%20ChicagoFSWildPlus%2C%20and%20BOBSL%20datasets%20with%20a%20BLEU-4%20score%20of%2022.1%2C%2073.2%25%20letter%20accuracy%20and%20BLEU-4%20score%20of%206.8%20respectively.%20These%20results%20validate%20our%20core%20hypothesis%3A%20isolating%20and%20solving%20distinct%20recognition%20tasks%20before%20fusion%20provides%20a%20more%20powerful%20and%20effective%20pathway%20to%20robust%2C%20high-fidelity%20sign%20language%20translation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.00030v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignBind-LLM%253A%2520Multi-Stage%2520Modality%2520Fusion%2520for%2520Sign%2520Language%2520Translation%26entry.906535625%3DMarshall%2520Thomas%2520and%2520Edward%2520Fish%2520and%2520Richard%2520Bowden%26entry.1292438233%3DDespite%2520progress%2520in%2520gloss-free%2520Sign%2520Language%2520Translation%2520%2528SLT%2529%252C%2520traditional%2520single%2520modality%2520end-to-end%2520approaches%2520consistently%2520fail%2520on%2520two%2520critical%2520components%2520of%2520natural%2520signing%253A%2520the%2520precise%2520recognition%2520of%2520high-speed%2520fingerspelling%2520and%2520the%2520integration%2520of%2520asynchronous%2520non-manual%2520cues%2520from%2520the%2520face.%2520Recent%2520progress%2520in%2520SLT%2520with%2520Large%2520Language%2520Models%2520has%2520side%2520stepped%2520this%2520challenge%252C%2520forcing%2520a%2520single%2520network%2520to%2520learn%2520these%2520simultaneously%2520resulting%2520in%2520poor%2520performance%2520when%2520tasked%2520with%2520translating%2520crucial%2520information%2520such%2520as%2520names%252C%2520places%252C%2520and%2520technical%2520terms.%2520We%2520introduce%2520SignBind-LLM%252C%2520a%2520modular%2520framework%2520designed%2520to%2520overcome%2520these%2520limitations.%2520Our%2520approach%2520employs%2520separate%252C%2520specialized%2520predictors%2520for%2520continuous%2520signing%252C%2520fingerspelling%252C%2520and%2520lipreading.%2520Each%2520expert%2520network%2520first%2520decodes%2520its%2520specific%2520modality%2520into%2520a%2520sequence%2520of%2520tokens.%2520These%2520parallel%2520streams%2520are%2520then%2520fused%2520by%2520a%2520lightweight%2520transformer%2520that%2520resolves%2520temporal%2520misalignments%2520before%2520passing%2520the%2520combined%2520representation%2520to%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520for%2520final%2520sentence%2520generation.%2520Our%2520method%2520establishes%2520a%2520new%2520state-of-the-art%2520on%2520the%2520How2Sign%252C%2520ChicagoFSWildPlus%252C%2520and%2520BOBSL%2520datasets%2520with%2520a%2520BLEU-4%2520score%2520of%252022.1%252C%252073.2%2525%2520letter%2520accuracy%2520and%2520BLEU-4%2520score%2520of%25206.8%2520respectively.%2520These%2520results%2520validate%2520our%2520core%2520hypothesis%253A%2520isolating%2520and%2520solving%2520distinct%2520recognition%2520tasks%2520before%2520fusion%2520provides%2520a%2520more%2520powerful%2520and%2520effective%2520pathway%2520to%2520robust%252C%2520high-fidelity%2520sign%2520language%2520translation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00030v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SignBind-LLM%3A%20Multi-Stage%20Modality%20Fusion%20for%20Sign%20Language%20Translation&entry.906535625=Marshall%20Thomas%20and%20Edward%20Fish%20and%20Richard%20Bowden&entry.1292438233=Despite%20progress%20in%20gloss-free%20Sign%20Language%20Translation%20%28SLT%29%2C%20traditional%20single%20modality%20end-to-end%20approaches%20consistently%20fail%20on%20two%20critical%20components%20of%20natural%20signing%3A%20the%20precise%20recognition%20of%20high-speed%20fingerspelling%20and%20the%20integration%20of%20asynchronous%20non-manual%20cues%20from%20the%20face.%20Recent%20progress%20in%20SLT%20with%20Large%20Language%20Models%20has%20side%20stepped%20this%20challenge%2C%20forcing%20a%20single%20network%20to%20learn%20these%20simultaneously%20resulting%20in%20poor%20performance%20when%20tasked%20with%20translating%20crucial%20information%20such%20as%20names%2C%20places%2C%20and%20technical%20terms.%20We%20introduce%20SignBind-LLM%2C%20a%20modular%20framework%20designed%20to%20overcome%20these%20limitations.%20Our%20approach%20employs%20separate%2C%20specialized%20predictors%20for%20continuous%20signing%2C%20fingerspelling%2C%20and%20lipreading.%20Each%20expert%20network%20first%20decodes%20its%20specific%20modality%20into%20a%20sequence%20of%20tokens.%20These%20parallel%20streams%20are%20then%20fused%20by%20a%20lightweight%20transformer%20that%20resolves%20temporal%20misalignments%20before%20passing%20the%20combined%20representation%20to%20a%20Large%20Language%20Model%20%28LLM%29%20for%20final%20sentence%20generation.%20Our%20method%20establishes%20a%20new%20state-of-the-art%20on%20the%20How2Sign%2C%20ChicagoFSWildPlus%2C%20and%20BOBSL%20datasets%20with%20a%20BLEU-4%20score%20of%2022.1%2C%2073.2%25%20letter%20accuracy%20and%20BLEU-4%20score%20of%206.8%20respectively.%20These%20results%20validate%20our%20core%20hypothesis%3A%20isolating%20and%20solving%20distinct%20recognition%20tasks%20before%20fusion%20provides%20a%20more%20powerful%20and%20effective%20pathway%20to%20robust%2C%20high-fidelity%20sign%20language%20translation.&entry.1838667208=http%3A//arxiv.org/abs/2509.00030v3&entry.124074799=Read"},
{"title": "Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning", "author": "Jasmine Shone and Zhening Li and Shaden Alshammari and Mark Hamilton and William Freeman", "abstract": "The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) supervised contrastive learning with Euclidean distance as the feature space metric is improved by replacing the standard loss function with Jenson-Shannon divergence (JSD); (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded $f$-divergence. Our results highlight the importance of considering divergence choices in representation learning optimization.", "link": "http://arxiv.org/abs/2509.04734v2", "date": "2025-12-04", "relevancy": 2.6226, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20I-Con%3A%20Exploring%20New%20Dimension%20of%20Distance%20Measures%20in%20Representation%20Learning&body=Title%3A%20Beyond%20I-Con%3A%20Exploring%20New%20Dimension%20of%20Distance%20Measures%20in%20Representation%20Learning%0AAuthor%3A%20Jasmine%20Shone%20and%20Zhening%20Li%20and%20Shaden%20Alshammari%20and%20Mark%20Hamilton%20and%20William%20Freeman%0AAbstract%3A%20The%20Information%20Contrastive%20%28I-Con%29%20framework%20revealed%20that%20over%2023%20representation%20learning%20methods%20implicitly%20minimize%20KL%20divergence%20between%20data%20and%20learned%20distributions%20that%20encode%20similarities%20between%20data%20points.%20However%2C%20a%20KL-based%20loss%20may%20be%20misaligned%20with%20the%20true%20objective%2C%20and%20properties%20of%20KL%20divergence%20such%20as%20asymmetry%20and%20unboundedness%20may%20create%20optimization%20challenges.%20We%20present%20Beyond%20I-Con%2C%20a%20framework%20that%20enables%20systematic%20discovery%20of%20novel%20loss%20functions%20by%20exploring%20alternative%20statistical%20divergences.%20Key%20findings%3A%20%281%29%20on%20unsupervised%20clustering%20of%20DINO-ViT%20embeddings%2C%20we%20achieve%20state-of-the-art%20results%20by%20modifying%20the%20PMI%20algorithm%20to%20use%20total%20variation%20%28TV%29%20distance%3B%20%282%29%20supervised%20contrastive%20learning%20with%20Euclidean%20distance%20as%20the%20feature%20space%20metric%20is%20improved%20by%20replacing%20the%20standard%20loss%20function%20with%20Jenson-Shannon%20divergence%20%28JSD%29%3B%20%283%29%20on%20dimensionality%20reduction%2C%20we%20achieve%20superior%20qualitative%20results%20and%20better%20performance%20on%20downstream%20tasks%20than%20SNE%20by%20replacing%20KL%20with%20a%20bounded%20%24f%24-divergence.%20Our%20results%20highlight%20the%20importance%20of%20considering%20divergence%20choices%20in%20representation%20learning%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2509.04734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520I-Con%253A%2520Exploring%2520New%2520Dimension%2520of%2520Distance%2520Measures%2520in%2520Representation%2520Learning%26entry.906535625%3DJasmine%2520Shone%2520and%2520Zhening%2520Li%2520and%2520Shaden%2520Alshammari%2520and%2520Mark%2520Hamilton%2520and%2520William%2520Freeman%26entry.1292438233%3DThe%2520Information%2520Contrastive%2520%2528I-Con%2529%2520framework%2520revealed%2520that%2520over%252023%2520representation%2520learning%2520methods%2520implicitly%2520minimize%2520KL%2520divergence%2520between%2520data%2520and%2520learned%2520distributions%2520that%2520encode%2520similarities%2520between%2520data%2520points.%2520However%252C%2520a%2520KL-based%2520loss%2520may%2520be%2520misaligned%2520with%2520the%2520true%2520objective%252C%2520and%2520properties%2520of%2520KL%2520divergence%2520such%2520as%2520asymmetry%2520and%2520unboundedness%2520may%2520create%2520optimization%2520challenges.%2520We%2520present%2520Beyond%2520I-Con%252C%2520a%2520framework%2520that%2520enables%2520systematic%2520discovery%2520of%2520novel%2520loss%2520functions%2520by%2520exploring%2520alternative%2520statistical%2520divergences.%2520Key%2520findings%253A%2520%25281%2529%2520on%2520unsupervised%2520clustering%2520of%2520DINO-ViT%2520embeddings%252C%2520we%2520achieve%2520state-of-the-art%2520results%2520by%2520modifying%2520the%2520PMI%2520algorithm%2520to%2520use%2520total%2520variation%2520%2528TV%2529%2520distance%253B%2520%25282%2529%2520supervised%2520contrastive%2520learning%2520with%2520Euclidean%2520distance%2520as%2520the%2520feature%2520space%2520metric%2520is%2520improved%2520by%2520replacing%2520the%2520standard%2520loss%2520function%2520with%2520Jenson-Shannon%2520divergence%2520%2528JSD%2529%253B%2520%25283%2529%2520on%2520dimensionality%2520reduction%252C%2520we%2520achieve%2520superior%2520qualitative%2520results%2520and%2520better%2520performance%2520on%2520downstream%2520tasks%2520than%2520SNE%2520by%2520replacing%2520KL%2520with%2520a%2520bounded%2520%2524f%2524-divergence.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520considering%2520divergence%2520choices%2520in%2520representation%2520learning%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20I-Con%3A%20Exploring%20New%20Dimension%20of%20Distance%20Measures%20in%20Representation%20Learning&entry.906535625=Jasmine%20Shone%20and%20Zhening%20Li%20and%20Shaden%20Alshammari%20and%20Mark%20Hamilton%20and%20William%20Freeman&entry.1292438233=The%20Information%20Contrastive%20%28I-Con%29%20framework%20revealed%20that%20over%2023%20representation%20learning%20methods%20implicitly%20minimize%20KL%20divergence%20between%20data%20and%20learned%20distributions%20that%20encode%20similarities%20between%20data%20points.%20However%2C%20a%20KL-based%20loss%20may%20be%20misaligned%20with%20the%20true%20objective%2C%20and%20properties%20of%20KL%20divergence%20such%20as%20asymmetry%20and%20unboundedness%20may%20create%20optimization%20challenges.%20We%20present%20Beyond%20I-Con%2C%20a%20framework%20that%20enables%20systematic%20discovery%20of%20novel%20loss%20functions%20by%20exploring%20alternative%20statistical%20divergences.%20Key%20findings%3A%20%281%29%20on%20unsupervised%20clustering%20of%20DINO-ViT%20embeddings%2C%20we%20achieve%20state-of-the-art%20results%20by%20modifying%20the%20PMI%20algorithm%20to%20use%20total%20variation%20%28TV%29%20distance%3B%20%282%29%20supervised%20contrastive%20learning%20with%20Euclidean%20distance%20as%20the%20feature%20space%20metric%20is%20improved%20by%20replacing%20the%20standard%20loss%20function%20with%20Jenson-Shannon%20divergence%20%28JSD%29%3B%20%283%29%20on%20dimensionality%20reduction%2C%20we%20achieve%20superior%20qualitative%20results%20and%20better%20performance%20on%20downstream%20tasks%20than%20SNE%20by%20replacing%20KL%20with%20a%20bounded%20%24f%24-divergence.%20Our%20results%20highlight%20the%20importance%20of%20considering%20divergence%20choices%20in%20representation%20learning%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2509.04734v2&entry.124074799=Read"},
{"title": "TRINITY: An Evolved LLM Coordinator", "author": "Jinglue Xu and Qi Sun and Peter Schwendeman and Stefan Nielsen and Edoardo Cetin and Yujin Tang", "abstract": "Combining diverse foundation models is promising, but weight-merging is limited by mismatched architectures and closed APIs. Trinity addresses this with a lightweight coordinator that orchestrates collaboration among large language models (LLMs). The coordinator, comprising a compact language model (approximately $0.6$B parameters) and a lightweight head (approximately $10$K parameters), is optimized with an evolutionary strategy for efficient and adaptive delegation. Trinity processes queries over multiple turns, where at each turn the coordinator assigns one of three roles (Thinker, Worker, or Verifier) to a selected LLM, effectively offloading complex skill acquisition from the coordinator itself. Experiments show that Trinity consistently outperforms individual models and existing methods across coding, math, reasoning, and domain knowledge tasks, and generalizes robustly to out-of-distribution tasks. On standard benchmarks, Trinity achieves state-of-the-art results, including a score of 86.2% on LiveCodeBench. Theoretical and empirical analyses identify two main factors behind this performance: (1) the coordinator's hidden-state representations provide rich contextualization of inputs, and (2) under high dimensionality and strict budget constraints, the separable Covariance Matrix Adaptation Evolution Strategy offers advantages over reinforcement learning, imitation learning, and random search by exploiting potential block-epsilon-separability.", "link": "http://arxiv.org/abs/2512.04695v1", "date": "2025-12-04", "relevancy": 2.6198, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5324}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5324}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRINITY%3A%20An%20Evolved%20LLM%20Coordinator&body=Title%3A%20TRINITY%3A%20An%20Evolved%20LLM%20Coordinator%0AAuthor%3A%20Jinglue%20Xu%20and%20Qi%20Sun%20and%20Peter%20Schwendeman%20and%20Stefan%20Nielsen%20and%20Edoardo%20Cetin%20and%20Yujin%20Tang%0AAbstract%3A%20Combining%20diverse%20foundation%20models%20is%20promising%2C%20but%20weight-merging%20is%20limited%20by%20mismatched%20architectures%20and%20closed%20APIs.%20Trinity%20addresses%20this%20with%20a%20lightweight%20coordinator%20that%20orchestrates%20collaboration%20among%20large%20language%20models%20%28LLMs%29.%20The%20coordinator%2C%20comprising%20a%20compact%20language%20model%20%28approximately%20%240.6%24B%20parameters%29%20and%20a%20lightweight%20head%20%28approximately%20%2410%24K%20parameters%29%2C%20is%20optimized%20with%20an%20evolutionary%20strategy%20for%20efficient%20and%20adaptive%20delegation.%20Trinity%20processes%20queries%20over%20multiple%20turns%2C%20where%20at%20each%20turn%20the%20coordinator%20assigns%20one%20of%20three%20roles%20%28Thinker%2C%20Worker%2C%20or%20Verifier%29%20to%20a%20selected%20LLM%2C%20effectively%20offloading%20complex%20skill%20acquisition%20from%20the%20coordinator%20itself.%20Experiments%20show%20that%20Trinity%20consistently%20outperforms%20individual%20models%20and%20existing%20methods%20across%20coding%2C%20math%2C%20reasoning%2C%20and%20domain%20knowledge%20tasks%2C%20and%20generalizes%20robustly%20to%20out-of-distribution%20tasks.%20On%20standard%20benchmarks%2C%20Trinity%20achieves%20state-of-the-art%20results%2C%20including%20a%20score%20of%2086.2%25%20on%20LiveCodeBench.%20Theoretical%20and%20empirical%20analyses%20identify%20two%20main%20factors%20behind%20this%20performance%3A%20%281%29%20the%20coordinator%27s%20hidden-state%20representations%20provide%20rich%20contextualization%20of%20inputs%2C%20and%20%282%29%20under%20high%20dimensionality%20and%20strict%20budget%20constraints%2C%20the%20separable%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20offers%20advantages%20over%20reinforcement%20learning%2C%20imitation%20learning%2C%20and%20random%20search%20by%20exploiting%20potential%20block-epsilon-separability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRINITY%253A%2520An%2520Evolved%2520LLM%2520Coordinator%26entry.906535625%3DJinglue%2520Xu%2520and%2520Qi%2520Sun%2520and%2520Peter%2520Schwendeman%2520and%2520Stefan%2520Nielsen%2520and%2520Edoardo%2520Cetin%2520and%2520Yujin%2520Tang%26entry.1292438233%3DCombining%2520diverse%2520foundation%2520models%2520is%2520promising%252C%2520but%2520weight-merging%2520is%2520limited%2520by%2520mismatched%2520architectures%2520and%2520closed%2520APIs.%2520Trinity%2520addresses%2520this%2520with%2520a%2520lightweight%2520coordinator%2520that%2520orchestrates%2520collaboration%2520among%2520large%2520language%2520models%2520%2528LLMs%2529.%2520The%2520coordinator%252C%2520comprising%2520a%2520compact%2520language%2520model%2520%2528approximately%2520%25240.6%2524B%2520parameters%2529%2520and%2520a%2520lightweight%2520head%2520%2528approximately%2520%252410%2524K%2520parameters%2529%252C%2520is%2520optimized%2520with%2520an%2520evolutionary%2520strategy%2520for%2520efficient%2520and%2520adaptive%2520delegation.%2520Trinity%2520processes%2520queries%2520over%2520multiple%2520turns%252C%2520where%2520at%2520each%2520turn%2520the%2520coordinator%2520assigns%2520one%2520of%2520three%2520roles%2520%2528Thinker%252C%2520Worker%252C%2520or%2520Verifier%2529%2520to%2520a%2520selected%2520LLM%252C%2520effectively%2520offloading%2520complex%2520skill%2520acquisition%2520from%2520the%2520coordinator%2520itself.%2520Experiments%2520show%2520that%2520Trinity%2520consistently%2520outperforms%2520individual%2520models%2520and%2520existing%2520methods%2520across%2520coding%252C%2520math%252C%2520reasoning%252C%2520and%2520domain%2520knowledge%2520tasks%252C%2520and%2520generalizes%2520robustly%2520to%2520out-of-distribution%2520tasks.%2520On%2520standard%2520benchmarks%252C%2520Trinity%2520achieves%2520state-of-the-art%2520results%252C%2520including%2520a%2520score%2520of%252086.2%2525%2520on%2520LiveCodeBench.%2520Theoretical%2520and%2520empirical%2520analyses%2520identify%2520two%2520main%2520factors%2520behind%2520this%2520performance%253A%2520%25281%2529%2520the%2520coordinator%2527s%2520hidden-state%2520representations%2520provide%2520rich%2520contextualization%2520of%2520inputs%252C%2520and%2520%25282%2529%2520under%2520high%2520dimensionality%2520and%2520strict%2520budget%2520constraints%252C%2520the%2520separable%2520Covariance%2520Matrix%2520Adaptation%2520Evolution%2520Strategy%2520offers%2520advantages%2520over%2520reinforcement%2520learning%252C%2520imitation%2520learning%252C%2520and%2520random%2520search%2520by%2520exploiting%2520potential%2520block-epsilon-separability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRINITY%3A%20An%20Evolved%20LLM%20Coordinator&entry.906535625=Jinglue%20Xu%20and%20Qi%20Sun%20and%20Peter%20Schwendeman%20and%20Stefan%20Nielsen%20and%20Edoardo%20Cetin%20and%20Yujin%20Tang&entry.1292438233=Combining%20diverse%20foundation%20models%20is%20promising%2C%20but%20weight-merging%20is%20limited%20by%20mismatched%20architectures%20and%20closed%20APIs.%20Trinity%20addresses%20this%20with%20a%20lightweight%20coordinator%20that%20orchestrates%20collaboration%20among%20large%20language%20models%20%28LLMs%29.%20The%20coordinator%2C%20comprising%20a%20compact%20language%20model%20%28approximately%20%240.6%24B%20parameters%29%20and%20a%20lightweight%20head%20%28approximately%20%2410%24K%20parameters%29%2C%20is%20optimized%20with%20an%20evolutionary%20strategy%20for%20efficient%20and%20adaptive%20delegation.%20Trinity%20processes%20queries%20over%20multiple%20turns%2C%20where%20at%20each%20turn%20the%20coordinator%20assigns%20one%20of%20three%20roles%20%28Thinker%2C%20Worker%2C%20or%20Verifier%29%20to%20a%20selected%20LLM%2C%20effectively%20offloading%20complex%20skill%20acquisition%20from%20the%20coordinator%20itself.%20Experiments%20show%20that%20Trinity%20consistently%20outperforms%20individual%20models%20and%20existing%20methods%20across%20coding%2C%20math%2C%20reasoning%2C%20and%20domain%20knowledge%20tasks%2C%20and%20generalizes%20robustly%20to%20out-of-distribution%20tasks.%20On%20standard%20benchmarks%2C%20Trinity%20achieves%20state-of-the-art%20results%2C%20including%20a%20score%20of%2086.2%25%20on%20LiveCodeBench.%20Theoretical%20and%20empirical%20analyses%20identify%20two%20main%20factors%20behind%20this%20performance%3A%20%281%29%20the%20coordinator%27s%20hidden-state%20representations%20provide%20rich%20contextualization%20of%20inputs%2C%20and%20%282%29%20under%20high%20dimensionality%20and%20strict%20budget%20constraints%2C%20the%20separable%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20offers%20advantages%20over%20reinforcement%20learning%2C%20imitation%20learning%2C%20and%20random%20search%20by%20exploiting%20potential%20block-epsilon-separability.&entry.1838667208=http%3A//arxiv.org/abs/2512.04695v1&entry.124074799=Read"},
{"title": "Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis", "author": "Jasmaine Khale and Ravi Prakash Srivastava", "abstract": "Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.", "link": "http://arxiv.org/abs/2512.04967v1", "date": "2025-12-04", "relevancy": 2.5844, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5136}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balanced%20Few-Shot%20Episodic%20Learning%20for%20Accurate%20Retinal%20Disease%20Diagnosis&body=Title%3A%20Balanced%20Few-Shot%20Episodic%20Learning%20for%20Accurate%20Retinal%20Disease%20Diagnosis%0AAuthor%3A%20Jasmaine%20Khale%20and%20Ravi%20Prakash%20Srivastava%0AAbstract%3A%20Automated%20retinal%20disease%20diagnosis%20is%20vital%20given%20the%20rising%20prevalence%20of%20conditions%20such%20as%20diabetic%20retinopathy%20and%20macular%20degeneration.%20Conventional%20deep%20learning%20approaches%20require%20large%20annotated%20datasets%2C%20which%20are%20costly%20and%20often%20imbalanced%20across%20disease%20categories%2C%20limiting%20their%20reliability%20in%20practice.%20Few-shot%20learning%20%28FSL%29%20addresses%20this%20challenge%20by%20enabling%20models%20to%20generalize%20from%20only%20a%20few%20labeled%20samples%20per%20class.%20In%20this%20study%2Cwe%20propose%20a%20balanced%20few-shot%20episodic%20learning%20framework%20tailored%20to%20the%20Retinal%20Fundus%20Multi-Disease%20Image%20Dataset%20%28RFMiD%29.%20Focusing%20on%20the%20ten%20most%20represented%20classes%2C%20which%20still%20show%20substantial%20imbalance%20between%20majority%20diseases%20%28e.g.%2C%20Diabetic%20Retinopathy%2C%20Macular%20Hole%29%20and%20minority%20ones%20%28e.g.%2C%20Optic%20Disc%20Edema%2C%20Branch%20Retinal%20Vein%20Occlusion%29%2C%20our%20method%20integrates%20three%20key%20components%3A%20%28i%29%20balanced%20episodic%20sampling%2C%20ensuring%20equal%20participation%20of%20all%20classes%20in%20each%205-way%205-shot%20episode%3B%20%28ii%29%20targeted%20augmentation%2C%20including%20Contrast%20Limited%20Adaptive%20Histogram%20Equalization%20%28CLAHE%29%20and%20color/geometry%20transformations%2C%20to%20improve%20minority-class%20di-%20versity%3B%20and%20%28iii%29%20a%20ResNet-50%20encoder%20pretrained%20on%20ImageNet%2C%20selected%20for%20its%20superior%20ability%20to%20capture%20fine-grained%20retinal%20features.%20Prototypes%20are%20computed%20in%20the%20embedding%20space%20and%20classification%20is%20performed%20with%20cosine%20similarity%20for%20improved%20stability.%20Trained%20on%20100%20episodes%20and%20evaluated%20on%201%2C000%20test%20episodes%2C%20our%20framework%20achieves%20substantial%20accuracy%20gains%20and%20reduces%20bias%20toward%20majority%20classes%2C%20with%20notable%20improvements%20for%20underrepresented%20diseases.%20These%20results%20demonstrate%20that%20dataset-aware%20few-shot%20pipelines%2C%20combined%20with%20balanced%20sampling%20and%20CLAHE-enhanced%20preprocessing%2C%20can%20deliver%20more%20robust%20and%20clinically%20fair%20retinal%20disease%20diagnosis%20under%20data-constrained%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalanced%2520Few-Shot%2520Episodic%2520Learning%2520for%2520Accurate%2520Retinal%2520Disease%2520Diagnosis%26entry.906535625%3DJasmaine%2520Khale%2520and%2520Ravi%2520Prakash%2520Srivastava%26entry.1292438233%3DAutomated%2520retinal%2520disease%2520diagnosis%2520is%2520vital%2520given%2520the%2520rising%2520prevalence%2520of%2520conditions%2520such%2520as%2520diabetic%2520retinopathy%2520and%2520macular%2520degeneration.%2520Conventional%2520deep%2520learning%2520approaches%2520require%2520large%2520annotated%2520datasets%252C%2520which%2520are%2520costly%2520and%2520often%2520imbalanced%2520across%2520disease%2520categories%252C%2520limiting%2520their%2520reliability%2520in%2520practice.%2520Few-shot%2520learning%2520%2528FSL%2529%2520addresses%2520this%2520challenge%2520by%2520enabling%2520models%2520to%2520generalize%2520from%2520only%2520a%2520few%2520labeled%2520samples%2520per%2520class.%2520In%2520this%2520study%252Cwe%2520propose%2520a%2520balanced%2520few-shot%2520episodic%2520learning%2520framework%2520tailored%2520to%2520the%2520Retinal%2520Fundus%2520Multi-Disease%2520Image%2520Dataset%2520%2528RFMiD%2529.%2520Focusing%2520on%2520the%2520ten%2520most%2520represented%2520classes%252C%2520which%2520still%2520show%2520substantial%2520imbalance%2520between%2520majority%2520diseases%2520%2528e.g.%252C%2520Diabetic%2520Retinopathy%252C%2520Macular%2520Hole%2529%2520and%2520minority%2520ones%2520%2528e.g.%252C%2520Optic%2520Disc%2520Edema%252C%2520Branch%2520Retinal%2520Vein%2520Occlusion%2529%252C%2520our%2520method%2520integrates%2520three%2520key%2520components%253A%2520%2528i%2529%2520balanced%2520episodic%2520sampling%252C%2520ensuring%2520equal%2520participation%2520of%2520all%2520classes%2520in%2520each%25205-way%25205-shot%2520episode%253B%2520%2528ii%2529%2520targeted%2520augmentation%252C%2520including%2520Contrast%2520Limited%2520Adaptive%2520Histogram%2520Equalization%2520%2528CLAHE%2529%2520and%2520color/geometry%2520transformations%252C%2520to%2520improve%2520minority-class%2520di-%2520versity%253B%2520and%2520%2528iii%2529%2520a%2520ResNet-50%2520encoder%2520pretrained%2520on%2520ImageNet%252C%2520selected%2520for%2520its%2520superior%2520ability%2520to%2520capture%2520fine-grained%2520retinal%2520features.%2520Prototypes%2520are%2520computed%2520in%2520the%2520embedding%2520space%2520and%2520classification%2520is%2520performed%2520with%2520cosine%2520similarity%2520for%2520improved%2520stability.%2520Trained%2520on%2520100%2520episodes%2520and%2520evaluated%2520on%25201%252C000%2520test%2520episodes%252C%2520our%2520framework%2520achieves%2520substantial%2520accuracy%2520gains%2520and%2520reduces%2520bias%2520toward%2520majority%2520classes%252C%2520with%2520notable%2520improvements%2520for%2520underrepresented%2520diseases.%2520These%2520results%2520demonstrate%2520that%2520dataset-aware%2520few-shot%2520pipelines%252C%2520combined%2520with%2520balanced%2520sampling%2520and%2520CLAHE-enhanced%2520preprocessing%252C%2520can%2520deliver%2520more%2520robust%2520and%2520clinically%2520fair%2520retinal%2520disease%2520diagnosis%2520under%2520data-constrained%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balanced%20Few-Shot%20Episodic%20Learning%20for%20Accurate%20Retinal%20Disease%20Diagnosis&entry.906535625=Jasmaine%20Khale%20and%20Ravi%20Prakash%20Srivastava&entry.1292438233=Automated%20retinal%20disease%20diagnosis%20is%20vital%20given%20the%20rising%20prevalence%20of%20conditions%20such%20as%20diabetic%20retinopathy%20and%20macular%20degeneration.%20Conventional%20deep%20learning%20approaches%20require%20large%20annotated%20datasets%2C%20which%20are%20costly%20and%20often%20imbalanced%20across%20disease%20categories%2C%20limiting%20their%20reliability%20in%20practice.%20Few-shot%20learning%20%28FSL%29%20addresses%20this%20challenge%20by%20enabling%20models%20to%20generalize%20from%20only%20a%20few%20labeled%20samples%20per%20class.%20In%20this%20study%2Cwe%20propose%20a%20balanced%20few-shot%20episodic%20learning%20framework%20tailored%20to%20the%20Retinal%20Fundus%20Multi-Disease%20Image%20Dataset%20%28RFMiD%29.%20Focusing%20on%20the%20ten%20most%20represented%20classes%2C%20which%20still%20show%20substantial%20imbalance%20between%20majority%20diseases%20%28e.g.%2C%20Diabetic%20Retinopathy%2C%20Macular%20Hole%29%20and%20minority%20ones%20%28e.g.%2C%20Optic%20Disc%20Edema%2C%20Branch%20Retinal%20Vein%20Occlusion%29%2C%20our%20method%20integrates%20three%20key%20components%3A%20%28i%29%20balanced%20episodic%20sampling%2C%20ensuring%20equal%20participation%20of%20all%20classes%20in%20each%205-way%205-shot%20episode%3B%20%28ii%29%20targeted%20augmentation%2C%20including%20Contrast%20Limited%20Adaptive%20Histogram%20Equalization%20%28CLAHE%29%20and%20color/geometry%20transformations%2C%20to%20improve%20minority-class%20di-%20versity%3B%20and%20%28iii%29%20a%20ResNet-50%20encoder%20pretrained%20on%20ImageNet%2C%20selected%20for%20its%20superior%20ability%20to%20capture%20fine-grained%20retinal%20features.%20Prototypes%20are%20computed%20in%20the%20embedding%20space%20and%20classification%20is%20performed%20with%20cosine%20similarity%20for%20improved%20stability.%20Trained%20on%20100%20episodes%20and%20evaluated%20on%201%2C000%20test%20episodes%2C%20our%20framework%20achieves%20substantial%20accuracy%20gains%20and%20reduces%20bias%20toward%20majority%20classes%2C%20with%20notable%20improvements%20for%20underrepresented%20diseases.%20These%20results%20demonstrate%20that%20dataset-aware%20few-shot%20pipelines%2C%20combined%20with%20balanced%20sampling%20and%20CLAHE-enhanced%20preprocessing%2C%20can%20deliver%20more%20robust%20and%20clinically%20fair%20retinal%20disease%20diagnosis%20under%20data-constrained%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2512.04967v1&entry.124074799=Read"},
{"title": "Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations", "author": "Zhendong Yang and Jie Wang and Liansong Zong and Xiaorong Liu and Quan Qian and Shiqian Chen", "abstract": "Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting of old knowledge and overfitting on scarce new data. To address these challenges, this paper proposes a novel framework built upon Dual-Granularity Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN explicitly decouples feature learning into two parallel streams: 1) a fine-grained representation stream, which utilizes a novel Multi-Order Interaction Aggregation module to capture discriminative, class-specific features from the limited new samples. 2) a coarse-grained representation stream, designed to model and preserve general, class-agnostic knowledge shared across all fault types. These two representations are dynamically fused by a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features, preventing overfitting and alleviating feature conflicts. To further mitigate catastrophic forgetting, we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a decoupled Balanced Random Forest classifier is employed to counter the decision boundary bias caused by data imbalance. Extensive experiments on the TEP benchmark and a real-world MFF dataset demonstrate that our proposed DGGN achieves superior diagnostic performance and stability compared to state-of-the-art FSC-FD approaches. Our code is publicly available at https://github.com/MentaY/DGGN", "link": "http://arxiv.org/abs/2508.16634v4", "date": "2025-12-04", "relevancy": 2.5838, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5355}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5219}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20Class-incremental%20Fault%20Diagnosis%20by%20Preserving%20Class-Agnostic%20Knowledge%20with%20Dual-Granularity%20Representations&body=Title%3A%20Few-shot%20Class-incremental%20Fault%20Diagnosis%20by%20Preserving%20Class-Agnostic%20Knowledge%20with%20Dual-Granularity%20Representations%0AAuthor%3A%20Zhendong%20Yang%20and%20Jie%20Wang%20and%20Liansong%20Zong%20and%20Xiaorong%20Liu%20and%20Quan%20Qian%20and%20Shiqian%20Chen%0AAbstract%3A%20Few-Shot%20Class-Incremental%20Fault%20Diagnosis%20%28FSC-FD%29%2C%20which%20aims%20to%20continuously%20learn%20from%20new%20fault%20classes%20with%20only%20a%20few%20samples%20without%20forgetting%20old%20ones%2C%20is%20critical%20for%20real-world%20industrial%20systems.%20However%2C%20this%20challenging%20task%20severely%20amplifies%20the%20issues%20of%20catastrophic%20forgetting%20of%20old%20knowledge%20and%20overfitting%20on%20scarce%20new%20data.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20novel%20framework%20built%20upon%20Dual-Granularity%20Representations%2C%20termed%20the%20Dual-Granularity%20Guidance%20Network%20%28DGGN%29.%20Our%20DGGN%20explicitly%20decouples%20feature%20learning%20into%20two%20parallel%20streams%3A%201%29%20a%20fine-grained%20representation%20stream%2C%20which%20utilizes%20a%20novel%20Multi-Order%20Interaction%20Aggregation%20module%20to%20capture%20discriminative%2C%20class-specific%20features%20from%20the%20limited%20new%20samples.%202%29%20a%20coarse-grained%20representation%20stream%2C%20designed%20to%20model%20and%20preserve%20general%2C%20class-agnostic%20knowledge%20shared%20across%20all%20fault%20types.%20These%20two%20representations%20are%20dynamically%20fused%20by%20a%20multi-semantic%20cross-attention%20mechanism%2C%20where%20the%20stable%20coarse-grained%20knowledge%20guides%20the%20learning%20of%20fine-grained%20features%2C%20preventing%20overfitting%20and%20alleviating%20feature%20conflicts.%20To%20further%20mitigate%20catastrophic%20forgetting%2C%20we%20design%20a%20Boundary-Aware%20Exemplar%20Prioritization%20strategy.%20Moreover%2C%20a%20decoupled%20Balanced%20Random%20Forest%20classifier%20is%20employed%20to%20counter%20the%20decision%20boundary%20bias%20caused%20by%20data%20imbalance.%20Extensive%20experiments%20on%20the%20TEP%20benchmark%20and%20a%20real-world%20MFF%20dataset%20demonstrate%20that%20our%20proposed%20DGGN%20achieves%20superior%20diagnostic%20performance%20and%20stability%20compared%20to%20state-of-the-art%20FSC-FD%20approaches.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/MentaY/DGGN%0ALink%3A%20http%3A//arxiv.org/abs/2508.16634v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520Class-incremental%2520Fault%2520Diagnosis%2520by%2520Preserving%2520Class-Agnostic%2520Knowledge%2520with%2520Dual-Granularity%2520Representations%26entry.906535625%3DZhendong%2520Yang%2520and%2520Jie%2520Wang%2520and%2520Liansong%2520Zong%2520and%2520Xiaorong%2520Liu%2520and%2520Quan%2520Qian%2520and%2520Shiqian%2520Chen%26entry.1292438233%3DFew-Shot%2520Class-Incremental%2520Fault%2520Diagnosis%2520%2528FSC-FD%2529%252C%2520which%2520aims%2520to%2520continuously%2520learn%2520from%2520new%2520fault%2520classes%2520with%2520only%2520a%2520few%2520samples%2520without%2520forgetting%2520old%2520ones%252C%2520is%2520critical%2520for%2520real-world%2520industrial%2520systems.%2520However%252C%2520this%2520challenging%2520task%2520severely%2520amplifies%2520the%2520issues%2520of%2520catastrophic%2520forgetting%2520of%2520old%2520knowledge%2520and%2520overfitting%2520on%2520scarce%2520new%2520data.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520framework%2520built%2520upon%2520Dual-Granularity%2520Representations%252C%2520termed%2520the%2520Dual-Granularity%2520Guidance%2520Network%2520%2528DGGN%2529.%2520Our%2520DGGN%2520explicitly%2520decouples%2520feature%2520learning%2520into%2520two%2520parallel%2520streams%253A%25201%2529%2520a%2520fine-grained%2520representation%2520stream%252C%2520which%2520utilizes%2520a%2520novel%2520Multi-Order%2520Interaction%2520Aggregation%2520module%2520to%2520capture%2520discriminative%252C%2520class-specific%2520features%2520from%2520the%2520limited%2520new%2520samples.%25202%2529%2520a%2520coarse-grained%2520representation%2520stream%252C%2520designed%2520to%2520model%2520and%2520preserve%2520general%252C%2520class-agnostic%2520knowledge%2520shared%2520across%2520all%2520fault%2520types.%2520These%2520two%2520representations%2520are%2520dynamically%2520fused%2520by%2520a%2520multi-semantic%2520cross-attention%2520mechanism%252C%2520where%2520the%2520stable%2520coarse-grained%2520knowledge%2520guides%2520the%2520learning%2520of%2520fine-grained%2520features%252C%2520preventing%2520overfitting%2520and%2520alleviating%2520feature%2520conflicts.%2520To%2520further%2520mitigate%2520catastrophic%2520forgetting%252C%2520we%2520design%2520a%2520Boundary-Aware%2520Exemplar%2520Prioritization%2520strategy.%2520Moreover%252C%2520a%2520decoupled%2520Balanced%2520Random%2520Forest%2520classifier%2520is%2520employed%2520to%2520counter%2520the%2520decision%2520boundary%2520bias%2520caused%2520by%2520data%2520imbalance.%2520Extensive%2520experiments%2520on%2520the%2520TEP%2520benchmark%2520and%2520a%2520real-world%2520MFF%2520dataset%2520demonstrate%2520that%2520our%2520proposed%2520DGGN%2520achieves%2520superior%2520diagnostic%2520performance%2520and%2520stability%2520compared%2520to%2520state-of-the-art%2520FSC-FD%2520approaches.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/MentaY/DGGN%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16634v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20Class-incremental%20Fault%20Diagnosis%20by%20Preserving%20Class-Agnostic%20Knowledge%20with%20Dual-Granularity%20Representations&entry.906535625=Zhendong%20Yang%20and%20Jie%20Wang%20and%20Liansong%20Zong%20and%20Xiaorong%20Liu%20and%20Quan%20Qian%20and%20Shiqian%20Chen&entry.1292438233=Few-Shot%20Class-Incremental%20Fault%20Diagnosis%20%28FSC-FD%29%2C%20which%20aims%20to%20continuously%20learn%20from%20new%20fault%20classes%20with%20only%20a%20few%20samples%20without%20forgetting%20old%20ones%2C%20is%20critical%20for%20real-world%20industrial%20systems.%20However%2C%20this%20challenging%20task%20severely%20amplifies%20the%20issues%20of%20catastrophic%20forgetting%20of%20old%20knowledge%20and%20overfitting%20on%20scarce%20new%20data.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20novel%20framework%20built%20upon%20Dual-Granularity%20Representations%2C%20termed%20the%20Dual-Granularity%20Guidance%20Network%20%28DGGN%29.%20Our%20DGGN%20explicitly%20decouples%20feature%20learning%20into%20two%20parallel%20streams%3A%201%29%20a%20fine-grained%20representation%20stream%2C%20which%20utilizes%20a%20novel%20Multi-Order%20Interaction%20Aggregation%20module%20to%20capture%20discriminative%2C%20class-specific%20features%20from%20the%20limited%20new%20samples.%202%29%20a%20coarse-grained%20representation%20stream%2C%20designed%20to%20model%20and%20preserve%20general%2C%20class-agnostic%20knowledge%20shared%20across%20all%20fault%20types.%20These%20two%20representations%20are%20dynamically%20fused%20by%20a%20multi-semantic%20cross-attention%20mechanism%2C%20where%20the%20stable%20coarse-grained%20knowledge%20guides%20the%20learning%20of%20fine-grained%20features%2C%20preventing%20overfitting%20and%20alleviating%20feature%20conflicts.%20To%20further%20mitigate%20catastrophic%20forgetting%2C%20we%20design%20a%20Boundary-Aware%20Exemplar%20Prioritization%20strategy.%20Moreover%2C%20a%20decoupled%20Balanced%20Random%20Forest%20classifier%20is%20employed%20to%20counter%20the%20decision%20boundary%20bias%20caused%20by%20data%20imbalance.%20Extensive%20experiments%20on%20the%20TEP%20benchmark%20and%20a%20real-world%20MFF%20dataset%20demonstrate%20that%20our%20proposed%20DGGN%20achieves%20superior%20diagnostic%20performance%20and%20stability%20compared%20to%20state-of-the-art%20FSC-FD%20approaches.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/MentaY/DGGN&entry.1838667208=http%3A//arxiv.org/abs/2508.16634v4&entry.124074799=Read"},
{"title": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing", "author": "Seungwon Choi and Dong-Gyu Park and Seo-Yeon Hwang and Tae-Wan Kim", "abstract": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.", "link": "http://arxiv.org/abs/2512.03397v2", "date": "2025-12-04", "relevancy": 2.574, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5344}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5249}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surfel-LIO%3A%20Fast%20LiDAR-Inertial%20Odometry%20with%20Pre-computed%20Surfels%20and%20Hierarchical%20Z-order%20Voxel%20Hashing&body=Title%3A%20Surfel-LIO%3A%20Fast%20LiDAR-Inertial%20Odometry%20with%20Pre-computed%20Surfels%20and%20Hierarchical%20Z-order%20Voxel%20Hashing%0AAuthor%3A%20Seungwon%20Choi%20and%20Dong-Gyu%20Park%20and%20Seo-Yeon%20Hwang%20and%20Tae-Wan%20Kim%0AAbstract%3A%20LiDAR-inertial%20odometry%20%28LIO%29%20is%20an%20active%20research%20area%2C%20as%20it%20enables%20accurate%20real-time%20state%20estimation%20in%20GPS-denied%20environments.%20Recent%20advances%20in%20map%20data%20structures%20and%20spatial%20indexing%20have%20significantly%20improved%20the%20efficiency%20of%20LIO%20systems.%20Nevertheless%2C%20we%20observe%20that%20two%20aspects%20may%20still%20leave%20room%20for%20improvement%3A%20%281%29%20nearest%20neighbor%20search%20often%20requires%20examining%20multiple%20spatial%20units%20to%20gather%20sufficient%20points%20for%20plane%20fitting%2C%20and%20%282%29%20plane%20parameters%20are%20typically%20recomputed%20at%20every%20iteration%20despite%20unchanged%20map%20geometry.%20Motivated%20by%20these%20observations%2C%20we%20propose%20Surfel-LIO%2C%20which%20employs%20a%20hierarchical%20voxel%20structure%20%28hVox%29%20with%20pre-computed%20surfel%20representation.%20This%20design%20enables%20O%281%29%20correspondence%20retrieval%20without%20runtime%20neighbor%20enumeration%20or%20plane%20fitting%2C%20combined%20with%20Z-order%20curve%20encoding%20for%20cache-friendly%20spatial%20indexing.%20Experimental%20results%20on%20the%20M3DGR%20dataset%20demonstrate%20that%20our%20method%20achieves%20significantly%20faster%20processing%20speed%20compared%20to%20recent%20state-of-the-art%20methods%20while%20maintaining%20comparable%20state%20estimation%20accuracy.%20Our%20implementation%20is%20publicly%20available%20at%20https%3A//github.com/93won/lidar_inertial_odometry.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurfel-LIO%253A%2520Fast%2520LiDAR-Inertial%2520Odometry%2520with%2520Pre-computed%2520Surfels%2520and%2520Hierarchical%2520Z-order%2520Voxel%2520Hashing%26entry.906535625%3DSeungwon%2520Choi%2520and%2520Dong-Gyu%2520Park%2520and%2520Seo-Yeon%2520Hwang%2520and%2520Tae-Wan%2520Kim%26entry.1292438233%3DLiDAR-inertial%2520odometry%2520%2528LIO%2529%2520is%2520an%2520active%2520research%2520area%252C%2520as%2520it%2520enables%2520accurate%2520real-time%2520state%2520estimation%2520in%2520GPS-denied%2520environments.%2520Recent%2520advances%2520in%2520map%2520data%2520structures%2520and%2520spatial%2520indexing%2520have%2520significantly%2520improved%2520the%2520efficiency%2520of%2520LIO%2520systems.%2520Nevertheless%252C%2520we%2520observe%2520that%2520two%2520aspects%2520may%2520still%2520leave%2520room%2520for%2520improvement%253A%2520%25281%2529%2520nearest%2520neighbor%2520search%2520often%2520requires%2520examining%2520multiple%2520spatial%2520units%2520to%2520gather%2520sufficient%2520points%2520for%2520plane%2520fitting%252C%2520and%2520%25282%2529%2520plane%2520parameters%2520are%2520typically%2520recomputed%2520at%2520every%2520iteration%2520despite%2520unchanged%2520map%2520geometry.%2520Motivated%2520by%2520these%2520observations%252C%2520we%2520propose%2520Surfel-LIO%252C%2520which%2520employs%2520a%2520hierarchical%2520voxel%2520structure%2520%2528hVox%2529%2520with%2520pre-computed%2520surfel%2520representation.%2520This%2520design%2520enables%2520O%25281%2529%2520correspondence%2520retrieval%2520without%2520runtime%2520neighbor%2520enumeration%2520or%2520plane%2520fitting%252C%2520combined%2520with%2520Z-order%2520curve%2520encoding%2520for%2520cache-friendly%2520spatial%2520indexing.%2520Experimental%2520results%2520on%2520the%2520M3DGR%2520dataset%2520demonstrate%2520that%2520our%2520method%2520achieves%2520significantly%2520faster%2520processing%2520speed%2520compared%2520to%2520recent%2520state-of-the-art%2520methods%2520while%2520maintaining%2520comparable%2520state%2520estimation%2520accuracy.%2520Our%2520implementation%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/93won/lidar_inertial_odometry.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surfel-LIO%3A%20Fast%20LiDAR-Inertial%20Odometry%20with%20Pre-computed%20Surfels%20and%20Hierarchical%20Z-order%20Voxel%20Hashing&entry.906535625=Seungwon%20Choi%20and%20Dong-Gyu%20Park%20and%20Seo-Yeon%20Hwang%20and%20Tae-Wan%20Kim&entry.1292438233=LiDAR-inertial%20odometry%20%28LIO%29%20is%20an%20active%20research%20area%2C%20as%20it%20enables%20accurate%20real-time%20state%20estimation%20in%20GPS-denied%20environments.%20Recent%20advances%20in%20map%20data%20structures%20and%20spatial%20indexing%20have%20significantly%20improved%20the%20efficiency%20of%20LIO%20systems.%20Nevertheless%2C%20we%20observe%20that%20two%20aspects%20may%20still%20leave%20room%20for%20improvement%3A%20%281%29%20nearest%20neighbor%20search%20often%20requires%20examining%20multiple%20spatial%20units%20to%20gather%20sufficient%20points%20for%20plane%20fitting%2C%20and%20%282%29%20plane%20parameters%20are%20typically%20recomputed%20at%20every%20iteration%20despite%20unchanged%20map%20geometry.%20Motivated%20by%20these%20observations%2C%20we%20propose%20Surfel-LIO%2C%20which%20employs%20a%20hierarchical%20voxel%20structure%20%28hVox%29%20with%20pre-computed%20surfel%20representation.%20This%20design%20enables%20O%281%29%20correspondence%20retrieval%20without%20runtime%20neighbor%20enumeration%20or%20plane%20fitting%2C%20combined%20with%20Z-order%20curve%20encoding%20for%20cache-friendly%20spatial%20indexing.%20Experimental%20results%20on%20the%20M3DGR%20dataset%20demonstrate%20that%20our%20method%20achieves%20significantly%20faster%20processing%20speed%20compared%20to%20recent%20state-of-the-art%20methods%20while%20maintaining%20comparable%20state%20estimation%20accuracy.%20Our%20implementation%20is%20publicly%20available%20at%20https%3A//github.com/93won/lidar_inertial_odometry.&entry.1838667208=http%3A//arxiv.org/abs/2512.03397v2&entry.124074799=Read"},
{"title": "SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards", "author": "Yuan Gao and Jin Song", "abstract": "In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.", "link": "http://arxiv.org/abs/2512.05098v1", "date": "2025-12-04", "relevancy": 2.5737, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5267}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5146}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SA-IQA%3A%20Redefining%20Image%20Quality%20Assessment%20for%20Spatial%20Aesthetics%20with%20Multi-Dimensional%20Rewards&body=Title%3A%20SA-IQA%3A%20Redefining%20Image%20Quality%20Assessment%20for%20Spatial%20Aesthetics%20with%20Multi-Dimensional%20Rewards%0AAuthor%3A%20Yuan%20Gao%20and%20Jin%20Song%0AAbstract%3A%20In%20recent%20years%2C%20Image%20Quality%20Assessment%20%28IQA%29%20for%20AI-generated%20images%20%28AIGI%29%20has%20advanced%20rapidly%3B%20however%2C%20existing%20methods%20primarily%20target%20portraits%20and%20artistic%20images%2C%20lacking%20a%20systematic%20evaluation%20of%20interior%20scenes.%20We%20introduce%20Spatial%20Aesthetics%2C%20a%20paradigm%20that%20assesses%20the%20aesthetic%20quality%20of%20interior%20images%20along%20four%20dimensions%3A%20layout%2C%20harmony%2C%20lighting%2C%20and%20distortion.%20We%20construct%20SA-BENCH%2C%20the%20first%20benchmark%20for%20spatial%20aesthetics%2C%20comprising%2018%2C000%20images%20and%2050%2C000%20precise%20annotations.%20Employing%20SA-BENCH%2C%20we%20systematically%20evaluate%20current%20IQA%20methodologies%20and%20develop%20SA-IQA%2C%20through%20MLLM%20fine-tuning%20and%20a%20multidimensional%20fusion%20approach%2C%20as%20a%20comprehensive%20reward%20framework%20for%20assessing%20spatial%20aesthetics.%20We%20apply%20SA-IQA%20to%20two%20downstream%20tasks%3A%20%281%29%20serving%20as%20a%20reward%20signal%20integrated%20with%20GRPO%20reinforcement%20learning%20to%20optimize%20the%20AIGC%20generation%20pipeline%2C%20and%20%282%29%20Best-of-N%20selection%20to%20filter%20high-quality%20images%20and%20improve%20generation%20quality.%20Experiments%20indicate%20that%20SA-IQA%20significantly%20outperforms%20existing%20methods%20on%20SA-BENCH%2C%20setting%20a%20new%20standard%20for%20spatial%20aesthetics%20evaluation.%20Code%20and%20dataset%20will%20be%20open-sourced%20to%20advance%20research%20and%20applications%20in%20this%20domain.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSA-IQA%253A%2520Redefining%2520Image%2520Quality%2520Assessment%2520for%2520Spatial%2520Aesthetics%2520with%2520Multi-Dimensional%2520Rewards%26entry.906535625%3DYuan%2520Gao%2520and%2520Jin%2520Song%26entry.1292438233%3DIn%2520recent%2520years%252C%2520Image%2520Quality%2520Assessment%2520%2528IQA%2529%2520for%2520AI-generated%2520images%2520%2528AIGI%2529%2520has%2520advanced%2520rapidly%253B%2520however%252C%2520existing%2520methods%2520primarily%2520target%2520portraits%2520and%2520artistic%2520images%252C%2520lacking%2520a%2520systematic%2520evaluation%2520of%2520interior%2520scenes.%2520We%2520introduce%2520Spatial%2520Aesthetics%252C%2520a%2520paradigm%2520that%2520assesses%2520the%2520aesthetic%2520quality%2520of%2520interior%2520images%2520along%2520four%2520dimensions%253A%2520layout%252C%2520harmony%252C%2520lighting%252C%2520and%2520distortion.%2520We%2520construct%2520SA-BENCH%252C%2520the%2520first%2520benchmark%2520for%2520spatial%2520aesthetics%252C%2520comprising%252018%252C000%2520images%2520and%252050%252C000%2520precise%2520annotations.%2520Employing%2520SA-BENCH%252C%2520we%2520systematically%2520evaluate%2520current%2520IQA%2520methodologies%2520and%2520develop%2520SA-IQA%252C%2520through%2520MLLM%2520fine-tuning%2520and%2520a%2520multidimensional%2520fusion%2520approach%252C%2520as%2520a%2520comprehensive%2520reward%2520framework%2520for%2520assessing%2520spatial%2520aesthetics.%2520We%2520apply%2520SA-IQA%2520to%2520two%2520downstream%2520tasks%253A%2520%25281%2529%2520serving%2520as%2520a%2520reward%2520signal%2520integrated%2520with%2520GRPO%2520reinforcement%2520learning%2520to%2520optimize%2520the%2520AIGC%2520generation%2520pipeline%252C%2520and%2520%25282%2529%2520Best-of-N%2520selection%2520to%2520filter%2520high-quality%2520images%2520and%2520improve%2520generation%2520quality.%2520Experiments%2520indicate%2520that%2520SA-IQA%2520significantly%2520outperforms%2520existing%2520methods%2520on%2520SA-BENCH%252C%2520setting%2520a%2520new%2520standard%2520for%2520spatial%2520aesthetics%2520evaluation.%2520Code%2520and%2520dataset%2520will%2520be%2520open-sourced%2520to%2520advance%2520research%2520and%2520applications%2520in%2520this%2520domain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SA-IQA%3A%20Redefining%20Image%20Quality%20Assessment%20for%20Spatial%20Aesthetics%20with%20Multi-Dimensional%20Rewards&entry.906535625=Yuan%20Gao%20and%20Jin%20Song&entry.1292438233=In%20recent%20years%2C%20Image%20Quality%20Assessment%20%28IQA%29%20for%20AI-generated%20images%20%28AIGI%29%20has%20advanced%20rapidly%3B%20however%2C%20existing%20methods%20primarily%20target%20portraits%20and%20artistic%20images%2C%20lacking%20a%20systematic%20evaluation%20of%20interior%20scenes.%20We%20introduce%20Spatial%20Aesthetics%2C%20a%20paradigm%20that%20assesses%20the%20aesthetic%20quality%20of%20interior%20images%20along%20four%20dimensions%3A%20layout%2C%20harmony%2C%20lighting%2C%20and%20distortion.%20We%20construct%20SA-BENCH%2C%20the%20first%20benchmark%20for%20spatial%20aesthetics%2C%20comprising%2018%2C000%20images%20and%2050%2C000%20precise%20annotations.%20Employing%20SA-BENCH%2C%20we%20systematically%20evaluate%20current%20IQA%20methodologies%20and%20develop%20SA-IQA%2C%20through%20MLLM%20fine-tuning%20and%20a%20multidimensional%20fusion%20approach%2C%20as%20a%20comprehensive%20reward%20framework%20for%20assessing%20spatial%20aesthetics.%20We%20apply%20SA-IQA%20to%20two%20downstream%20tasks%3A%20%281%29%20serving%20as%20a%20reward%20signal%20integrated%20with%20GRPO%20reinforcement%20learning%20to%20optimize%20the%20AIGC%20generation%20pipeline%2C%20and%20%282%29%20Best-of-N%20selection%20to%20filter%20high-quality%20images%20and%20improve%20generation%20quality.%20Experiments%20indicate%20that%20SA-IQA%20significantly%20outperforms%20existing%20methods%20on%20SA-BENCH%2C%20setting%20a%20new%20standard%20for%20spatial%20aesthetics%20evaluation.%20Code%20and%20dataset%20will%20be%20open-sourced%20to%20advance%20research%20and%20applications%20in%20this%20domain.&entry.1838667208=http%3A//arxiv.org/abs/2512.05098v1&entry.124074799=Read"},
{"title": "Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding", "author": "Tsai-Ning Wang and Lin-Lin Chen and Neil Zeghidour and Aaqib Saeed", "abstract": "Pre-trained audio models excel at detecting acoustic patterns in auscultation sounds but often fail to grasp their clinical significance, limiting their use and performance in diagnostic tasks. To bridge this gap, we introduce AcuLa (Audio-Clinical Understanding via Language Alignment), a lightweight post-training framework that instills semantic understanding into any audio encoder by aligning it with a medical language model, which acts as a \"semantic teacher.\" To enable alignment at scale, we construct a large-scale dataset by leveraging off-the-shelf large language models to translate the rich, structured metadata accompanying existing audio recordings into coherent clinical reports. Our alignment strategy combines a representation-level contrastive objective with a self-supervised modeling, ensuring that the model learns clinical semantics while preserving fine-grained temporal cues. AcuLa achieves state-of-the-art results across 18 diverse cardio-respiratory tasks from 10 different datasets, improving the mean AUROC on classification benchmarks from 0.68 to 0.79 and, on the most challenging COVID-19 cough detection task, boosting the AUROC from 0.55 to 0.89. Our work demonstrates that this audio-language alignment transforms purely acoustic models into clinically-aware diagnostic tools, establishing a novel paradigm for enhancing physiological understanding in audio-based health monitoring.", "link": "http://arxiv.org/abs/2512.04847v1", "date": "2025-12-04", "relevancy": 2.5523, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20as%20Semantic%20Teachers%3A%20Post-Training%20Alignment%20for%20Medical%20Audio%20Understanding&body=Title%3A%20Language%20Models%20as%20Semantic%20Teachers%3A%20Post-Training%20Alignment%20for%20Medical%20Audio%20Understanding%0AAuthor%3A%20Tsai-Ning%20Wang%20and%20Lin-Lin%20Chen%20and%20Neil%20Zeghidour%20and%20Aaqib%20Saeed%0AAbstract%3A%20Pre-trained%20audio%20models%20excel%20at%20detecting%20acoustic%20patterns%20in%20auscultation%20sounds%20but%20often%20fail%20to%20grasp%20their%20clinical%20significance%2C%20limiting%20their%20use%20and%20performance%20in%20diagnostic%20tasks.%20To%20bridge%20this%20gap%2C%20we%20introduce%20AcuLa%20%28Audio-Clinical%20Understanding%20via%20Language%20Alignment%29%2C%20a%20lightweight%20post-training%20framework%20that%20instills%20semantic%20understanding%20into%20any%20audio%20encoder%20by%20aligning%20it%20with%20a%20medical%20language%20model%2C%20which%20acts%20as%20a%20%22semantic%20teacher.%22%20To%20enable%20alignment%20at%20scale%2C%20we%20construct%20a%20large-scale%20dataset%20by%20leveraging%20off-the-shelf%20large%20language%20models%20to%20translate%20the%20rich%2C%20structured%20metadata%20accompanying%20existing%20audio%20recordings%20into%20coherent%20clinical%20reports.%20Our%20alignment%20strategy%20combines%20a%20representation-level%20contrastive%20objective%20with%20a%20self-supervised%20modeling%2C%20ensuring%20that%20the%20model%20learns%20clinical%20semantics%20while%20preserving%20fine-grained%20temporal%20cues.%20AcuLa%20achieves%20state-of-the-art%20results%20across%2018%20diverse%20cardio-respiratory%20tasks%20from%2010%20different%20datasets%2C%20improving%20the%20mean%20AUROC%20on%20classification%20benchmarks%20from%200.68%20to%200.79%20and%2C%20on%20the%20most%20challenging%20COVID-19%20cough%20detection%20task%2C%20boosting%20the%20AUROC%20from%200.55%20to%200.89.%20Our%20work%20demonstrates%20that%20this%20audio-language%20alignment%20transforms%20purely%20acoustic%20models%20into%20clinically-aware%20diagnostic%20tools%2C%20establishing%20a%20novel%20paradigm%20for%20enhancing%20physiological%20understanding%20in%20audio-based%20health%20monitoring.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520as%2520Semantic%2520Teachers%253A%2520Post-Training%2520Alignment%2520for%2520Medical%2520Audio%2520Understanding%26entry.906535625%3DTsai-Ning%2520Wang%2520and%2520Lin-Lin%2520Chen%2520and%2520Neil%2520Zeghidour%2520and%2520Aaqib%2520Saeed%26entry.1292438233%3DPre-trained%2520audio%2520models%2520excel%2520at%2520detecting%2520acoustic%2520patterns%2520in%2520auscultation%2520sounds%2520but%2520often%2520fail%2520to%2520grasp%2520their%2520clinical%2520significance%252C%2520limiting%2520their%2520use%2520and%2520performance%2520in%2520diagnostic%2520tasks.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520AcuLa%2520%2528Audio-Clinical%2520Understanding%2520via%2520Language%2520Alignment%2529%252C%2520a%2520lightweight%2520post-training%2520framework%2520that%2520instills%2520semantic%2520understanding%2520into%2520any%2520audio%2520encoder%2520by%2520aligning%2520it%2520with%2520a%2520medical%2520language%2520model%252C%2520which%2520acts%2520as%2520a%2520%2522semantic%2520teacher.%2522%2520To%2520enable%2520alignment%2520at%2520scale%252C%2520we%2520construct%2520a%2520large-scale%2520dataset%2520by%2520leveraging%2520off-the-shelf%2520large%2520language%2520models%2520to%2520translate%2520the%2520rich%252C%2520structured%2520metadata%2520accompanying%2520existing%2520audio%2520recordings%2520into%2520coherent%2520clinical%2520reports.%2520Our%2520alignment%2520strategy%2520combines%2520a%2520representation-level%2520contrastive%2520objective%2520with%2520a%2520self-supervised%2520modeling%252C%2520ensuring%2520that%2520the%2520model%2520learns%2520clinical%2520semantics%2520while%2520preserving%2520fine-grained%2520temporal%2520cues.%2520AcuLa%2520achieves%2520state-of-the-art%2520results%2520across%252018%2520diverse%2520cardio-respiratory%2520tasks%2520from%252010%2520different%2520datasets%252C%2520improving%2520the%2520mean%2520AUROC%2520on%2520classification%2520benchmarks%2520from%25200.68%2520to%25200.79%2520and%252C%2520on%2520the%2520most%2520challenging%2520COVID-19%2520cough%2520detection%2520task%252C%2520boosting%2520the%2520AUROC%2520from%25200.55%2520to%25200.89.%2520Our%2520work%2520demonstrates%2520that%2520this%2520audio-language%2520alignment%2520transforms%2520purely%2520acoustic%2520models%2520into%2520clinically-aware%2520diagnostic%2520tools%252C%2520establishing%2520a%2520novel%2520paradigm%2520for%2520enhancing%2520physiological%2520understanding%2520in%2520audio-based%2520health%2520monitoring.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20as%20Semantic%20Teachers%3A%20Post-Training%20Alignment%20for%20Medical%20Audio%20Understanding&entry.906535625=Tsai-Ning%20Wang%20and%20Lin-Lin%20Chen%20and%20Neil%20Zeghidour%20and%20Aaqib%20Saeed&entry.1292438233=Pre-trained%20audio%20models%20excel%20at%20detecting%20acoustic%20patterns%20in%20auscultation%20sounds%20but%20often%20fail%20to%20grasp%20their%20clinical%20significance%2C%20limiting%20their%20use%20and%20performance%20in%20diagnostic%20tasks.%20To%20bridge%20this%20gap%2C%20we%20introduce%20AcuLa%20%28Audio-Clinical%20Understanding%20via%20Language%20Alignment%29%2C%20a%20lightweight%20post-training%20framework%20that%20instills%20semantic%20understanding%20into%20any%20audio%20encoder%20by%20aligning%20it%20with%20a%20medical%20language%20model%2C%20which%20acts%20as%20a%20%22semantic%20teacher.%22%20To%20enable%20alignment%20at%20scale%2C%20we%20construct%20a%20large-scale%20dataset%20by%20leveraging%20off-the-shelf%20large%20language%20models%20to%20translate%20the%20rich%2C%20structured%20metadata%20accompanying%20existing%20audio%20recordings%20into%20coherent%20clinical%20reports.%20Our%20alignment%20strategy%20combines%20a%20representation-level%20contrastive%20objective%20with%20a%20self-supervised%20modeling%2C%20ensuring%20that%20the%20model%20learns%20clinical%20semantics%20while%20preserving%20fine-grained%20temporal%20cues.%20AcuLa%20achieves%20state-of-the-art%20results%20across%2018%20diverse%20cardio-respiratory%20tasks%20from%2010%20different%20datasets%2C%20improving%20the%20mean%20AUROC%20on%20classification%20benchmarks%20from%200.68%20to%200.79%20and%2C%20on%20the%20most%20challenging%20COVID-19%20cough%20detection%20task%2C%20boosting%20the%20AUROC%20from%200.55%20to%200.89.%20Our%20work%20demonstrates%20that%20this%20audio-language%20alignment%20transforms%20purely%20acoustic%20models%20into%20clinically-aware%20diagnostic%20tools%2C%20establishing%20a%20novel%20paradigm%20for%20enhancing%20physiological%20understanding%20in%20audio-based%20health%20monitoring.&entry.1838667208=http%3A//arxiv.org/abs/2512.04847v1&entry.124074799=Read"},
{"title": "Control Illusion: The Failure of Instruction Hierarchies in Large Language Models", "author": "Yilin Geng and Haonan Li and Honglin Mu and Xudong Han and Timothy Baldwin and Omri Abend and Eduard Hovy and Lea Frermann", "abstract": "Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. Interestingly, we also find that societal hierarchy framings (e.g., authority, expertise, consensus) show stronger influence on model behavior than system/user roles, suggesting that pretraining-derived social structures function as latent behavioral priors with potentially greater impact than post-training guardrails.", "link": "http://arxiv.org/abs/2502.15851v4", "date": "2025-12-04", "relevancy": 2.5494, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Control%20Illusion%3A%20The%20Failure%20of%20Instruction%20Hierarchies%20in%20Large%20Language%20Models&body=Title%3A%20Control%20Illusion%3A%20The%20Failure%20of%20Instruction%20Hierarchies%20in%20Large%20Language%20Models%0AAuthor%3A%20Yilin%20Geng%20and%20Haonan%20Li%20and%20Honglin%20Mu%20and%20Xudong%20Han%20and%20Timothy%20Baldwin%20and%20Omri%20Abend%20and%20Eduard%20Hovy%20and%20Lea%20Frermann%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20with%20hierarchical%20instruction%20schemes%2C%20where%20certain%20instructions%20%28e.g.%2C%20system-level%20directives%29%20are%20expected%20to%20take%20precedence%20over%20others%20%28e.g.%2C%20user%20messages%29.%20Yet%2C%20we%20lack%20a%20systematic%20understanding%20of%20how%20effectively%20these%20hierarchical%20control%20mechanisms%20work.%20We%20introduce%20a%20systematic%20evaluation%20framework%20based%20on%20constraint%20prioritization%20to%20assess%20how%20well%20LLMs%20enforce%20instruction%20hierarchies.%20Our%20experiments%20across%20six%20state-of-the-art%20LLMs%20reveal%20that%20models%20struggle%20with%20consistent%20instruction%20prioritization%2C%20even%20for%20simple%20formatting%20conflicts.%20We%20find%20that%20the%20widely-adopted%20system/user%20prompt%20separation%20fails%20to%20establish%20a%20reliable%20instruction%20hierarchy%2C%20and%20models%20exhibit%20strong%20inherent%20biases%20toward%20certain%20constraint%20types%20regardless%20of%20their%20priority%20designation.%20Interestingly%2C%20we%20also%20find%20that%20societal%20hierarchy%20framings%20%28e.g.%2C%20authority%2C%20expertise%2C%20consensus%29%20show%20stronger%20influence%20on%20model%20behavior%20than%20system/user%20roles%2C%20suggesting%20that%20pretraining-derived%20social%20structures%20function%20as%20latent%20behavioral%20priors%20with%20potentially%20greater%20impact%20than%20post-training%20guardrails.%0ALink%3A%20http%3A//arxiv.org/abs/2502.15851v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControl%2520Illusion%253A%2520The%2520Failure%2520of%2520Instruction%2520Hierarchies%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DYilin%2520Geng%2520and%2520Haonan%2520Li%2520and%2520Honglin%2520Mu%2520and%2520Xudong%2520Han%2520and%2520Timothy%2520Baldwin%2520and%2520Omri%2520Abend%2520and%2520Eduard%2520Hovy%2520and%2520Lea%2520Frermann%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520with%2520hierarchical%2520instruction%2520schemes%252C%2520where%2520certain%2520instructions%2520%2528e.g.%252C%2520system-level%2520directives%2529%2520are%2520expected%2520to%2520take%2520precedence%2520over%2520others%2520%2528e.g.%252C%2520user%2520messages%2529.%2520Yet%252C%2520we%2520lack%2520a%2520systematic%2520understanding%2520of%2520how%2520effectively%2520these%2520hierarchical%2520control%2520mechanisms%2520work.%2520We%2520introduce%2520a%2520systematic%2520evaluation%2520framework%2520based%2520on%2520constraint%2520prioritization%2520to%2520assess%2520how%2520well%2520LLMs%2520enforce%2520instruction%2520hierarchies.%2520Our%2520experiments%2520across%2520six%2520state-of-the-art%2520LLMs%2520reveal%2520that%2520models%2520struggle%2520with%2520consistent%2520instruction%2520prioritization%252C%2520even%2520for%2520simple%2520formatting%2520conflicts.%2520We%2520find%2520that%2520the%2520widely-adopted%2520system/user%2520prompt%2520separation%2520fails%2520to%2520establish%2520a%2520reliable%2520instruction%2520hierarchy%252C%2520and%2520models%2520exhibit%2520strong%2520inherent%2520biases%2520toward%2520certain%2520constraint%2520types%2520regardless%2520of%2520their%2520priority%2520designation.%2520Interestingly%252C%2520we%2520also%2520find%2520that%2520societal%2520hierarchy%2520framings%2520%2528e.g.%252C%2520authority%252C%2520expertise%252C%2520consensus%2529%2520show%2520stronger%2520influence%2520on%2520model%2520behavior%2520than%2520system/user%2520roles%252C%2520suggesting%2520that%2520pretraining-derived%2520social%2520structures%2520function%2520as%2520latent%2520behavioral%2520priors%2520with%2520potentially%2520greater%2520impact%2520than%2520post-training%2520guardrails.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15851v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Control%20Illusion%3A%20The%20Failure%20of%20Instruction%20Hierarchies%20in%20Large%20Language%20Models&entry.906535625=Yilin%20Geng%20and%20Haonan%20Li%20and%20Honglin%20Mu%20and%20Xudong%20Han%20and%20Timothy%20Baldwin%20and%20Omri%20Abend%20and%20Eduard%20Hovy%20and%20Lea%20Frermann&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20with%20hierarchical%20instruction%20schemes%2C%20where%20certain%20instructions%20%28e.g.%2C%20system-level%20directives%29%20are%20expected%20to%20take%20precedence%20over%20others%20%28e.g.%2C%20user%20messages%29.%20Yet%2C%20we%20lack%20a%20systematic%20understanding%20of%20how%20effectively%20these%20hierarchical%20control%20mechanisms%20work.%20We%20introduce%20a%20systematic%20evaluation%20framework%20based%20on%20constraint%20prioritization%20to%20assess%20how%20well%20LLMs%20enforce%20instruction%20hierarchies.%20Our%20experiments%20across%20six%20state-of-the-art%20LLMs%20reveal%20that%20models%20struggle%20with%20consistent%20instruction%20prioritization%2C%20even%20for%20simple%20formatting%20conflicts.%20We%20find%20that%20the%20widely-adopted%20system/user%20prompt%20separation%20fails%20to%20establish%20a%20reliable%20instruction%20hierarchy%2C%20and%20models%20exhibit%20strong%20inherent%20biases%20toward%20certain%20constraint%20types%20regardless%20of%20their%20priority%20designation.%20Interestingly%2C%20we%20also%20find%20that%20societal%20hierarchy%20framings%20%28e.g.%2C%20authority%2C%20expertise%2C%20consensus%29%20show%20stronger%20influence%20on%20model%20behavior%20than%20system/user%20roles%2C%20suggesting%20that%20pretraining-derived%20social%20structures%20function%20as%20latent%20behavioral%20priors%20with%20potentially%20greater%20impact%20than%20post-training%20guardrails.&entry.1838667208=http%3A//arxiv.org/abs/2502.15851v4&entry.124074799=Read"},
{"title": "LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics", "author": "Weiye Shi and Zhaowei Zhang and Shaoheng Yan and Yaodong Yang", "abstract": "Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.", "link": "http://arxiv.org/abs/2512.04957v1", "date": "2025-12-04", "relevancy": 2.5475, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Know%20More%20Than%20Words%3A%20A%20Genre%20Study%20with%20Syntax%2C%20Metaphor%20%26%20Phonetics&body=Title%3A%20LLMs%20Know%20More%20Than%20Words%3A%20A%20Genre%20Study%20with%20Syntax%2C%20Metaphor%20%26%20Phonetics%0AAuthor%3A%20Weiye%20Shi%20and%20Zhaowei%20Zhang%20and%20Shaoheng%20Yan%20and%20Yaodong%20Yang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20demonstrate%20remarkable%20potential%20across%20diverse%20language%20related%20tasks%2C%20yet%20whether%20they%20capture%20deeper%20linguistic%20properties%2C%20such%20as%20syntactic%20structure%2C%20phonetic%20cues%2C%20and%20metrical%20patterns%20from%20raw%20text%20remains%20unclear.%20To%20analysis%20whether%20LLMs%20can%20learn%20these%20features%20effectively%20and%20apply%20them%20to%20important%20nature%20language%20related%20tasks%2C%20we%20introduce%20a%20novel%20multilingual%20genre%20classification%20dataset%20derived%20from%20Project%20Gutenberg%2C%20a%20large-scale%20digital%20library%20offering%20free%20access%20to%20thousands%20of%20public%20domain%20literary%20works%2C%20comprising%20thousands%20of%20sentences%20per%20binary%20task%20%28poetry%20vs.%20novel%3Bdrama%20vs.%20poetry%3Bdrama%20vs.%20novel%29%20in%20six%20languages%20%28English%2C%20French%2C%20German%2C%20Italian%2C%20Spanish%2C%20and%20Portuguese%29.%20We%20augment%20each%20with%20three%20explicit%20linguistic%20feature%20sets%20%28syntactic%20tree%20structures%2C%20metaphor%20counts%2C%20and%20phonetic%20metrics%29%20to%20evaluate%20their%20impact%20on%20classification%20performance.%20Experiments%20demonstrate%20that%20although%20LLM%20classifiers%20can%20learn%20latent%20linguistic%20structures%20either%20from%20raw%20text%20or%20from%20explicitly%20provided%20features%2C%20different%20features%20contribute%20unevenly%20across%20tasks%2C%20which%20underscores%20the%20importance%20of%20incorporating%20more%20complex%20linguistic%20signals%20during%20model%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Know%2520More%2520Than%2520Words%253A%2520A%2520Genre%2520Study%2520with%2520Syntax%252C%2520Metaphor%2520%2526%2520Phonetics%26entry.906535625%3DWeiye%2520Shi%2520and%2520Zhaowei%2520Zhang%2520and%2520Shaoheng%2520Yan%2520and%2520Yaodong%2520Yang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520remarkable%2520potential%2520across%2520diverse%2520language%2520related%2520tasks%252C%2520yet%2520whether%2520they%2520capture%2520deeper%2520linguistic%2520properties%252C%2520such%2520as%2520syntactic%2520structure%252C%2520phonetic%2520cues%252C%2520and%2520metrical%2520patterns%2520from%2520raw%2520text%2520remains%2520unclear.%2520To%2520analysis%2520whether%2520LLMs%2520can%2520learn%2520these%2520features%2520effectively%2520and%2520apply%2520them%2520to%2520important%2520nature%2520language%2520related%2520tasks%252C%2520we%2520introduce%2520a%2520novel%2520multilingual%2520genre%2520classification%2520dataset%2520derived%2520from%2520Project%2520Gutenberg%252C%2520a%2520large-scale%2520digital%2520library%2520offering%2520free%2520access%2520to%2520thousands%2520of%2520public%2520domain%2520literary%2520works%252C%2520comprising%2520thousands%2520of%2520sentences%2520per%2520binary%2520task%2520%2528poetry%2520vs.%2520novel%253Bdrama%2520vs.%2520poetry%253Bdrama%2520vs.%2520novel%2529%2520in%2520six%2520languages%2520%2528English%252C%2520French%252C%2520German%252C%2520Italian%252C%2520Spanish%252C%2520and%2520Portuguese%2529.%2520We%2520augment%2520each%2520with%2520three%2520explicit%2520linguistic%2520feature%2520sets%2520%2528syntactic%2520tree%2520structures%252C%2520metaphor%2520counts%252C%2520and%2520phonetic%2520metrics%2529%2520to%2520evaluate%2520their%2520impact%2520on%2520classification%2520performance.%2520Experiments%2520demonstrate%2520that%2520although%2520LLM%2520classifiers%2520can%2520learn%2520latent%2520linguistic%2520structures%2520either%2520from%2520raw%2520text%2520or%2520from%2520explicitly%2520provided%2520features%252C%2520different%2520features%2520contribute%2520unevenly%2520across%2520tasks%252C%2520which%2520underscores%2520the%2520importance%2520of%2520incorporating%2520more%2520complex%2520linguistic%2520signals%2520during%2520model%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Know%20More%20Than%20Words%3A%20A%20Genre%20Study%20with%20Syntax%2C%20Metaphor%20%26%20Phonetics&entry.906535625=Weiye%20Shi%20and%20Zhaowei%20Zhang%20and%20Shaoheng%20Yan%20and%20Yaodong%20Yang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20demonstrate%20remarkable%20potential%20across%20diverse%20language%20related%20tasks%2C%20yet%20whether%20they%20capture%20deeper%20linguistic%20properties%2C%20such%20as%20syntactic%20structure%2C%20phonetic%20cues%2C%20and%20metrical%20patterns%20from%20raw%20text%20remains%20unclear.%20To%20analysis%20whether%20LLMs%20can%20learn%20these%20features%20effectively%20and%20apply%20them%20to%20important%20nature%20language%20related%20tasks%2C%20we%20introduce%20a%20novel%20multilingual%20genre%20classification%20dataset%20derived%20from%20Project%20Gutenberg%2C%20a%20large-scale%20digital%20library%20offering%20free%20access%20to%20thousands%20of%20public%20domain%20literary%20works%2C%20comprising%20thousands%20of%20sentences%20per%20binary%20task%20%28poetry%20vs.%20novel%3Bdrama%20vs.%20poetry%3Bdrama%20vs.%20novel%29%20in%20six%20languages%20%28English%2C%20French%2C%20German%2C%20Italian%2C%20Spanish%2C%20and%20Portuguese%29.%20We%20augment%20each%20with%20three%20explicit%20linguistic%20feature%20sets%20%28syntactic%20tree%20structures%2C%20metaphor%20counts%2C%20and%20phonetic%20metrics%29%20to%20evaluate%20their%20impact%20on%20classification%20performance.%20Experiments%20demonstrate%20that%20although%20LLM%20classifiers%20can%20learn%20latent%20linguistic%20structures%20either%20from%20raw%20text%20or%20from%20explicitly%20provided%20features%2C%20different%20features%20contribute%20unevenly%20across%20tasks%2C%20which%20underscores%20the%20importance%20of%20incorporating%20more%20complex%20linguistic%20signals%20during%20model%20training.&entry.1838667208=http%3A//arxiv.org/abs/2512.04957v1&entry.124074799=Read"},
{"title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation", "author": "Baris Yilmaz and Bevan Deniz Cilgin and Erdem Akag\u00fcnd\u00fcz and Salih Tileylioglu", "abstract": "Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency $f_0$ distributions between real and generated records per station, and summarize station specificity with a score based on the $f_0$ distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.", "link": "http://arxiv.org/abs/2512.04694v1", "date": "2025-12-04", "relevancy": 2.5467, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5294}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5078}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimesNet-Gen%3A%20Deep%20Learning-based%20Site%20Specific%20Strong%20Motion%20Generation&body=Title%3A%20TimesNet-Gen%3A%20Deep%20Learning-based%20Site%20Specific%20Strong%20Motion%20Generation%0AAuthor%3A%20Baris%20Yilmaz%20and%20Bevan%20Deniz%20Cilgin%20and%20Erdem%20Akag%C3%BCnd%C3%BCz%20and%20Salih%20Tileylioglu%0AAbstract%3A%20Effective%20earthquake%20risk%20reduction%20relies%20on%20accurate%20site-specific%20evaluations.%20This%20requires%20models%20that%20can%20represent%20the%20influence%20of%20local%20site%20conditions%20on%20ground%20motion%20characteristics.%20In%20this%20context%2C%20data%20driven%20approaches%20that%20learn%20site%20controlled%20signatures%20from%20recorded%20ground%20motions%20offer%20a%20promising%20direction.%20We%20address%20strong%20ground%20motion%20generation%20from%20time-domain%20accelerometer%20records%20and%20introduce%20the%20TimesNet-Gen%2C%20a%20time-domain%20conditional%20generator.%20The%20approach%20uses%20a%20station%20specific%20latent%20bottleneck.%20We%20evaluate%20generation%20by%20comparing%20HVSR%20curves%20and%20fundamental%20site-frequency%20%24f_0%24%20distributions%20between%20real%20and%20generated%20records%20per%20station%2C%20and%20summarize%20station%20specificity%20with%20a%20score%20based%20on%20the%20%24f_0%24%20distribution%20confusion%20matrices.%20TimesNet-Gen%20achieves%20strong%20station-wise%20alignment%20and%20compares%20favorably%20with%20a%20spectrogram-based%20conditional%20VAE%20baseline%20for%20site-specific%20strong%20motion%20synthesis.%20Our%20codes%20are%20available%20via%20https%3A//github.com/brsylmz23/TimesNet-Gen.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimesNet-Gen%253A%2520Deep%2520Learning-based%2520Site%2520Specific%2520Strong%2520Motion%2520Generation%26entry.906535625%3DBaris%2520Yilmaz%2520and%2520Bevan%2520Deniz%2520Cilgin%2520and%2520Erdem%2520Akag%25C3%25BCnd%25C3%25BCz%2520and%2520Salih%2520Tileylioglu%26entry.1292438233%3DEffective%2520earthquake%2520risk%2520reduction%2520relies%2520on%2520accurate%2520site-specific%2520evaluations.%2520This%2520requires%2520models%2520that%2520can%2520represent%2520the%2520influence%2520of%2520local%2520site%2520conditions%2520on%2520ground%2520motion%2520characteristics.%2520In%2520this%2520context%252C%2520data%2520driven%2520approaches%2520that%2520learn%2520site%2520controlled%2520signatures%2520from%2520recorded%2520ground%2520motions%2520offer%2520a%2520promising%2520direction.%2520We%2520address%2520strong%2520ground%2520motion%2520generation%2520from%2520time-domain%2520accelerometer%2520records%2520and%2520introduce%2520the%2520TimesNet-Gen%252C%2520a%2520time-domain%2520conditional%2520generator.%2520The%2520approach%2520uses%2520a%2520station%2520specific%2520latent%2520bottleneck.%2520We%2520evaluate%2520generation%2520by%2520comparing%2520HVSR%2520curves%2520and%2520fundamental%2520site-frequency%2520%2524f_0%2524%2520distributions%2520between%2520real%2520and%2520generated%2520records%2520per%2520station%252C%2520and%2520summarize%2520station%2520specificity%2520with%2520a%2520score%2520based%2520on%2520the%2520%2524f_0%2524%2520distribution%2520confusion%2520matrices.%2520TimesNet-Gen%2520achieves%2520strong%2520station-wise%2520alignment%2520and%2520compares%2520favorably%2520with%2520a%2520spectrogram-based%2520conditional%2520VAE%2520baseline%2520for%2520site-specific%2520strong%2520motion%2520synthesis.%2520Our%2520codes%2520are%2520available%2520via%2520https%253A//github.com/brsylmz23/TimesNet-Gen.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimesNet-Gen%3A%20Deep%20Learning-based%20Site%20Specific%20Strong%20Motion%20Generation&entry.906535625=Baris%20Yilmaz%20and%20Bevan%20Deniz%20Cilgin%20and%20Erdem%20Akag%C3%BCnd%C3%BCz%20and%20Salih%20Tileylioglu&entry.1292438233=Effective%20earthquake%20risk%20reduction%20relies%20on%20accurate%20site-specific%20evaluations.%20This%20requires%20models%20that%20can%20represent%20the%20influence%20of%20local%20site%20conditions%20on%20ground%20motion%20characteristics.%20In%20this%20context%2C%20data%20driven%20approaches%20that%20learn%20site%20controlled%20signatures%20from%20recorded%20ground%20motions%20offer%20a%20promising%20direction.%20We%20address%20strong%20ground%20motion%20generation%20from%20time-domain%20accelerometer%20records%20and%20introduce%20the%20TimesNet-Gen%2C%20a%20time-domain%20conditional%20generator.%20The%20approach%20uses%20a%20station%20specific%20latent%20bottleneck.%20We%20evaluate%20generation%20by%20comparing%20HVSR%20curves%20and%20fundamental%20site-frequency%20%24f_0%24%20distributions%20between%20real%20and%20generated%20records%20per%20station%2C%20and%20summarize%20station%20specificity%20with%20a%20score%20based%20on%20the%20%24f_0%24%20distribution%20confusion%20matrices.%20TimesNet-Gen%20achieves%20strong%20station-wise%20alignment%20and%20compares%20favorably%20with%20a%20spectrogram-based%20conditional%20VAE%20baseline%20for%20site-specific%20strong%20motion%20synthesis.%20Our%20codes%20are%20available%20via%20https%3A//github.com/brsylmz23/TimesNet-Gen.&entry.1838667208=http%3A//arxiv.org/abs/2512.04694v1&entry.124074799=Read"},
{"title": "Generative Neural Video Compression via Video Diffusion Prior", "author": "Qi Mao and Hao Cheng and Tinghan Yang and Libiao Jin and Siwei Ma", "abstract": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.", "link": "http://arxiv.org/abs/2512.05016v1", "date": "2025-12-04", "relevancy": 2.5435, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6594}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6331}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Neural%20Video%20Compression%20via%20Video%20Diffusion%20Prior&body=Title%3A%20Generative%20Neural%20Video%20Compression%20via%20Video%20Diffusion%20Prior%0AAuthor%3A%20Qi%20Mao%20and%20Hao%20Cheng%20and%20Tinghan%20Yang%20and%20Libiao%20Jin%20and%20Siwei%20Ma%0AAbstract%3A%20We%20present%20GNVC-VD%2C%20the%20first%20DiT-based%20generative%20neural%20video%20compression%20framework%20built%20upon%20an%20advanced%20video%20generation%20foundation%20model%2C%20where%20spatio-temporal%20latent%20compression%20and%20sequence-level%20generative%20refinement%20are%20unified%20within%20a%20single%20codec.%20Existing%20perceptual%20codecs%20primarily%20rely%20on%20pre-trained%20image%20generative%20priors%20to%20restore%20high-frequency%20details%2C%20but%20their%20frame-wise%20nature%20lacks%20temporal%20modeling%20and%20inevitably%20leads%20to%20perceptual%20flickering.%20To%20address%20this%2C%20GNVC-VD%20introduces%20a%20unified%20flow-matching%20latent%20refinement%20module%20that%20leverages%20a%20video%20diffusion%20transformer%20to%20jointly%20enhance%20intra-%20and%20inter-frame%20latents%20through%20sequence-level%20denoising%2C%20ensuring%20consistent%20spatio-temporal%20details.%20Instead%20of%20denoising%20from%20pure%20Gaussian%20noise%20as%20in%20video%20generation%2C%20GNVC-VD%20initializes%20refinement%20from%20decoded%20spatio-temporal%20latents%20and%20learns%20a%20correction%20term%20that%20adapts%20the%20diffusion%20prior%20to%20compression-induced%20degradation.%20A%20conditioning%20adaptor%20further%20injects%20compression-aware%20cues%20into%20intermediate%20DiT%20layers%2C%20enabling%20effective%20artifact%20removal%20while%20maintaining%20temporal%20coherence%20under%20extreme%20bitrate%20constraints.%20Extensive%20experiments%20show%20that%20GNVC-VD%20surpasses%20both%20traditional%20and%20learned%20codecs%20in%20perceptual%20quality%20and%20significantly%20reduces%20the%20flickering%20artifacts%20that%20persist%20in%20prior%20generative%20approaches%2C%20even%20below%200.01%20bpp%2C%20highlighting%20the%20promise%20of%20integrating%20video-native%20generative%20priors%20into%20neural%20codecs%20for%20next-generation%20perceptual%20video%20compression.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Neural%2520Video%2520Compression%2520via%2520Video%2520Diffusion%2520Prior%26entry.906535625%3DQi%2520Mao%2520and%2520Hao%2520Cheng%2520and%2520Tinghan%2520Yang%2520and%2520Libiao%2520Jin%2520and%2520Siwei%2520Ma%26entry.1292438233%3DWe%2520present%2520GNVC-VD%252C%2520the%2520first%2520DiT-based%2520generative%2520neural%2520video%2520compression%2520framework%2520built%2520upon%2520an%2520advanced%2520video%2520generation%2520foundation%2520model%252C%2520where%2520spatio-temporal%2520latent%2520compression%2520and%2520sequence-level%2520generative%2520refinement%2520are%2520unified%2520within%2520a%2520single%2520codec.%2520Existing%2520perceptual%2520codecs%2520primarily%2520rely%2520on%2520pre-trained%2520image%2520generative%2520priors%2520to%2520restore%2520high-frequency%2520details%252C%2520but%2520their%2520frame-wise%2520nature%2520lacks%2520temporal%2520modeling%2520and%2520inevitably%2520leads%2520to%2520perceptual%2520flickering.%2520To%2520address%2520this%252C%2520GNVC-VD%2520introduces%2520a%2520unified%2520flow-matching%2520latent%2520refinement%2520module%2520that%2520leverages%2520a%2520video%2520diffusion%2520transformer%2520to%2520jointly%2520enhance%2520intra-%2520and%2520inter-frame%2520latents%2520through%2520sequence-level%2520denoising%252C%2520ensuring%2520consistent%2520spatio-temporal%2520details.%2520Instead%2520of%2520denoising%2520from%2520pure%2520Gaussian%2520noise%2520as%2520in%2520video%2520generation%252C%2520GNVC-VD%2520initializes%2520refinement%2520from%2520decoded%2520spatio-temporal%2520latents%2520and%2520learns%2520a%2520correction%2520term%2520that%2520adapts%2520the%2520diffusion%2520prior%2520to%2520compression-induced%2520degradation.%2520A%2520conditioning%2520adaptor%2520further%2520injects%2520compression-aware%2520cues%2520into%2520intermediate%2520DiT%2520layers%252C%2520enabling%2520effective%2520artifact%2520removal%2520while%2520maintaining%2520temporal%2520coherence%2520under%2520extreme%2520bitrate%2520constraints.%2520Extensive%2520experiments%2520show%2520that%2520GNVC-VD%2520surpasses%2520both%2520traditional%2520and%2520learned%2520codecs%2520in%2520perceptual%2520quality%2520and%2520significantly%2520reduces%2520the%2520flickering%2520artifacts%2520that%2520persist%2520in%2520prior%2520generative%2520approaches%252C%2520even%2520below%25200.01%2520bpp%252C%2520highlighting%2520the%2520promise%2520of%2520integrating%2520video-native%2520generative%2520priors%2520into%2520neural%2520codecs%2520for%2520next-generation%2520perceptual%2520video%2520compression.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Neural%20Video%20Compression%20via%20Video%20Diffusion%20Prior&entry.906535625=Qi%20Mao%20and%20Hao%20Cheng%20and%20Tinghan%20Yang%20and%20Libiao%20Jin%20and%20Siwei%20Ma&entry.1292438233=We%20present%20GNVC-VD%2C%20the%20first%20DiT-based%20generative%20neural%20video%20compression%20framework%20built%20upon%20an%20advanced%20video%20generation%20foundation%20model%2C%20where%20spatio-temporal%20latent%20compression%20and%20sequence-level%20generative%20refinement%20are%20unified%20within%20a%20single%20codec.%20Existing%20perceptual%20codecs%20primarily%20rely%20on%20pre-trained%20image%20generative%20priors%20to%20restore%20high-frequency%20details%2C%20but%20their%20frame-wise%20nature%20lacks%20temporal%20modeling%20and%20inevitably%20leads%20to%20perceptual%20flickering.%20To%20address%20this%2C%20GNVC-VD%20introduces%20a%20unified%20flow-matching%20latent%20refinement%20module%20that%20leverages%20a%20video%20diffusion%20transformer%20to%20jointly%20enhance%20intra-%20and%20inter-frame%20latents%20through%20sequence-level%20denoising%2C%20ensuring%20consistent%20spatio-temporal%20details.%20Instead%20of%20denoising%20from%20pure%20Gaussian%20noise%20as%20in%20video%20generation%2C%20GNVC-VD%20initializes%20refinement%20from%20decoded%20spatio-temporal%20latents%20and%20learns%20a%20correction%20term%20that%20adapts%20the%20diffusion%20prior%20to%20compression-induced%20degradation.%20A%20conditioning%20adaptor%20further%20injects%20compression-aware%20cues%20into%20intermediate%20DiT%20layers%2C%20enabling%20effective%20artifact%20removal%20while%20maintaining%20temporal%20coherence%20under%20extreme%20bitrate%20constraints.%20Extensive%20experiments%20show%20that%20GNVC-VD%20surpasses%20both%20traditional%20and%20learned%20codecs%20in%20perceptual%20quality%20and%20significantly%20reduces%20the%20flickering%20artifacts%20that%20persist%20in%20prior%20generative%20approaches%2C%20even%20below%200.01%20bpp%2C%20highlighting%20the%20promise%20of%20integrating%20video-native%20generative%20priors%20into%20neural%20codecs%20for%20next-generation%20perceptual%20video%20compression.&entry.1838667208=http%3A//arxiv.org/abs/2512.05016v1&entry.124074799=Read"},
{"title": "A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs", "author": "Qiong Chang and Weimin Wang and Junpei Zhong and Jun Miyazaki", "abstract": "This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.", "link": "http://arxiv.org/abs/2512.04996v1", "date": "2025-12-04", "relevancy": 2.5369, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.522}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20dynamic%20memory%20assignment%20strategy%20for%20dilation-based%20ICP%20algorithm%20on%20embedded%20GPUs&body=Title%3A%20A%20dynamic%20memory%20assignment%20strategy%20for%20dilation-based%20ICP%20algorithm%20on%20embedded%20GPUs%0AAuthor%3A%20Qiong%20Chang%20and%20Weimin%20Wang%20and%20Junpei%20Zhong%20and%20Jun%20Miyazaki%0AAbstract%3A%20This%20paper%20proposes%20a%20memory-efficient%20optimization%20strategy%20for%20the%20high-performance%20point%20cloud%20registration%20algorithm%20VANICP%2C%20enabling%20lightweight%20execution%20on%20embedded%20GPUs%20with%20constrained%20hardware%20resources.%20VANICP%20is%20a%20recently%20published%20acceleration%20framework%20that%20significantly%20improves%20the%20computational%20efficiency%20of%20point-cloud-based%20applications.%20By%20transforming%20the%20global%20nearest%20neighbor%20search%20into%20a%20localized%20process%20through%20a%20dilation-based%20information%20propagation%20mechanism%2C%20VANICP%20greatly%20reduces%20the%20computational%20complexity%20of%20the%20NNS.%20However%2C%20its%20original%20implementation%20demands%20a%20considerable%20amount%20of%20memory%2C%20which%20restricts%20its%20deployment%20in%20resource-constrained%20environments%20such%20as%20embedded%20systems.%20To%20address%20this%20issue%2C%20we%20propose%20a%20GPU-oriented%20dynamic%20memory%20assignment%20strategy%20that%20optimizes%20the%20memory%20usage%20of%20the%20dilation%20operation.%20Furthermore%2C%20based%20on%20this%20strategy%2C%20we%20construct%20an%20enhanced%20version%20of%20the%20VANICP%20framework%20that%20achieves%20over%2097%25%20reduction%20in%20memory%20consumption%20while%20preserving%20the%20original%20performance.%20Source%20code%20is%20published%20on%3A%20https%3A//github.com/changqiong/VANICP4Em.git.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520dynamic%2520memory%2520assignment%2520strategy%2520for%2520dilation-based%2520ICP%2520algorithm%2520on%2520embedded%2520GPUs%26entry.906535625%3DQiong%2520Chang%2520and%2520Weimin%2520Wang%2520and%2520Junpei%2520Zhong%2520and%2520Jun%2520Miyazaki%26entry.1292438233%3DThis%2520paper%2520proposes%2520a%2520memory-efficient%2520optimization%2520strategy%2520for%2520the%2520high-performance%2520point%2520cloud%2520registration%2520algorithm%2520VANICP%252C%2520enabling%2520lightweight%2520execution%2520on%2520embedded%2520GPUs%2520with%2520constrained%2520hardware%2520resources.%2520VANICP%2520is%2520a%2520recently%2520published%2520acceleration%2520framework%2520that%2520significantly%2520improves%2520the%2520computational%2520efficiency%2520of%2520point-cloud-based%2520applications.%2520By%2520transforming%2520the%2520global%2520nearest%2520neighbor%2520search%2520into%2520a%2520localized%2520process%2520through%2520a%2520dilation-based%2520information%2520propagation%2520mechanism%252C%2520VANICP%2520greatly%2520reduces%2520the%2520computational%2520complexity%2520of%2520the%2520NNS.%2520However%252C%2520its%2520original%2520implementation%2520demands%2520a%2520considerable%2520amount%2520of%2520memory%252C%2520which%2520restricts%2520its%2520deployment%2520in%2520resource-constrained%2520environments%2520such%2520as%2520embedded%2520systems.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520GPU-oriented%2520dynamic%2520memory%2520assignment%2520strategy%2520that%2520optimizes%2520the%2520memory%2520usage%2520of%2520the%2520dilation%2520operation.%2520Furthermore%252C%2520based%2520on%2520this%2520strategy%252C%2520we%2520construct%2520an%2520enhanced%2520version%2520of%2520the%2520VANICP%2520framework%2520that%2520achieves%2520over%252097%2525%2520reduction%2520in%2520memory%2520consumption%2520while%2520preserving%2520the%2520original%2520performance.%2520Source%2520code%2520is%2520published%2520on%253A%2520https%253A//github.com/changqiong/VANICP4Em.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20dynamic%20memory%20assignment%20strategy%20for%20dilation-based%20ICP%20algorithm%20on%20embedded%20GPUs&entry.906535625=Qiong%20Chang%20and%20Weimin%20Wang%20and%20Junpei%20Zhong%20and%20Jun%20Miyazaki&entry.1292438233=This%20paper%20proposes%20a%20memory-efficient%20optimization%20strategy%20for%20the%20high-performance%20point%20cloud%20registration%20algorithm%20VANICP%2C%20enabling%20lightweight%20execution%20on%20embedded%20GPUs%20with%20constrained%20hardware%20resources.%20VANICP%20is%20a%20recently%20published%20acceleration%20framework%20that%20significantly%20improves%20the%20computational%20efficiency%20of%20point-cloud-based%20applications.%20By%20transforming%20the%20global%20nearest%20neighbor%20search%20into%20a%20localized%20process%20through%20a%20dilation-based%20information%20propagation%20mechanism%2C%20VANICP%20greatly%20reduces%20the%20computational%20complexity%20of%20the%20NNS.%20However%2C%20its%20original%20implementation%20demands%20a%20considerable%20amount%20of%20memory%2C%20which%20restricts%20its%20deployment%20in%20resource-constrained%20environments%20such%20as%20embedded%20systems.%20To%20address%20this%20issue%2C%20we%20propose%20a%20GPU-oriented%20dynamic%20memory%20assignment%20strategy%20that%20optimizes%20the%20memory%20usage%20of%20the%20dilation%20operation.%20Furthermore%2C%20based%20on%20this%20strategy%2C%20we%20construct%20an%20enhanced%20version%20of%20the%20VANICP%20framework%20that%20achieves%20over%2097%25%20reduction%20in%20memory%20consumption%20while%20preserving%20the%20original%20performance.%20Source%20code%20is%20published%20on%3A%20https%3A//github.com/changqiong/VANICP4Em.git.&entry.1838667208=http%3A//arxiv.org/abs/2512.04996v1&entry.124074799=Read"},
{"title": "Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models", "author": "Mohammed Mohiuddin and Syed Mohammod Minhaz Hossain and Sumaiya Khanam and Prionkar Barua and Aparup Barua and MD Tamim Hossain", "abstract": "Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception), using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross-validation analysis.", "link": "http://arxiv.org/abs/2512.00572v2", "date": "2025-12-04", "relevancy": 2.5278, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5201}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4986}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Skeleton%20Based%20Representations%20for%20Robust%20Yoga%20Pose%20Classification%20Using%20Deep%20Learning%20Models&body=Title%3A%20Integrating%20Skeleton%20Based%20Representations%20for%20Robust%20Yoga%20Pose%20Classification%20Using%20Deep%20Learning%20Models%0AAuthor%3A%20Mohammed%20Mohiuddin%20and%20Syed%20Mohammod%20Minhaz%20Hossain%20and%20Sumaiya%20Khanam%20and%20Prionkar%20Barua%20and%20Aparup%20Barua%20and%20MD%20Tamim%20Hossain%0AAbstract%3A%20Yoga%20is%20a%20popular%20form%20of%20exercise%20worldwide%20due%20to%20its%20spiritual%20and%20physical%20health%20benefits%2C%20but%20incorrect%20postures%20can%20lead%20to%20injuries.%20Automated%20yoga%20pose%20classification%20has%20therefore%20gained%20importance%20to%20reduce%20reliance%20on%20expert%20practitioners.%20While%20human%20pose%20keypoint%20extraction%20models%20have%20shown%20high%20potential%20in%20action%20recognition%2C%20systematic%20benchmarking%20for%20yoga%20pose%20recognition%20remains%20limited%2C%20as%20prior%20works%20often%20focus%20solely%20on%20raw%20images%20or%20a%20single%20pose%20extraction%20model.%20In%20this%20study%2C%20we%20introduce%20a%20curated%20dataset%2C%20%27Yoga-16%27%2C%20which%20addresses%20limitations%20of%20existing%20datasets%2C%20and%20systematically%20evaluate%20three%20deep%20learning%20architectures%20%28VGG16%2C%20ResNet50%2C%20and%20Xception%29%2C%20using%20three%20input%20modalities%20%28direct%20images%2C%20MediaPipe%20Pose%20skeleton%20images%2C%20and%20YOLOv8%20Pose%20skeleton%20images%29.%20Our%20experiments%20demonstrate%20that%20skeleton-based%20representations%20outperform%20raw%20image%20inputs%2C%20with%20the%20highest%20accuracy%20of%2096.09%25%20achieved%20by%20VGG16%20with%20MediaPipe%20Pose%20skeleton%20input.%20Additionally%2C%20we%20provide%20interpretability%20analysis%20using%20Grad-CAM%2C%20offering%20insights%20into%20model%20decision-making%20for%20yoga%20pose%20classification%20with%20cross-validation%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Skeleton%2520Based%2520Representations%2520for%2520Robust%2520Yoga%2520Pose%2520Classification%2520Using%2520Deep%2520Learning%2520Models%26entry.906535625%3DMohammed%2520Mohiuddin%2520and%2520Syed%2520Mohammod%2520Minhaz%2520Hossain%2520and%2520Sumaiya%2520Khanam%2520and%2520Prionkar%2520Barua%2520and%2520Aparup%2520Barua%2520and%2520MD%2520Tamim%2520Hossain%26entry.1292438233%3DYoga%2520is%2520a%2520popular%2520form%2520of%2520exercise%2520worldwide%2520due%2520to%2520its%2520spiritual%2520and%2520physical%2520health%2520benefits%252C%2520but%2520incorrect%2520postures%2520can%2520lead%2520to%2520injuries.%2520Automated%2520yoga%2520pose%2520classification%2520has%2520therefore%2520gained%2520importance%2520to%2520reduce%2520reliance%2520on%2520expert%2520practitioners.%2520While%2520human%2520pose%2520keypoint%2520extraction%2520models%2520have%2520shown%2520high%2520potential%2520in%2520action%2520recognition%252C%2520systematic%2520benchmarking%2520for%2520yoga%2520pose%2520recognition%2520remains%2520limited%252C%2520as%2520prior%2520works%2520often%2520focus%2520solely%2520on%2520raw%2520images%2520or%2520a%2520single%2520pose%2520extraction%2520model.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520curated%2520dataset%252C%2520%2527Yoga-16%2527%252C%2520which%2520addresses%2520limitations%2520of%2520existing%2520datasets%252C%2520and%2520systematically%2520evaluate%2520three%2520deep%2520learning%2520architectures%2520%2528VGG16%252C%2520ResNet50%252C%2520and%2520Xception%2529%252C%2520using%2520three%2520input%2520modalities%2520%2528direct%2520images%252C%2520MediaPipe%2520Pose%2520skeleton%2520images%252C%2520and%2520YOLOv8%2520Pose%2520skeleton%2520images%2529.%2520Our%2520experiments%2520demonstrate%2520that%2520skeleton-based%2520representations%2520outperform%2520raw%2520image%2520inputs%252C%2520with%2520the%2520highest%2520accuracy%2520of%252096.09%2525%2520achieved%2520by%2520VGG16%2520with%2520MediaPipe%2520Pose%2520skeleton%2520input.%2520Additionally%252C%2520we%2520provide%2520interpretability%2520analysis%2520using%2520Grad-CAM%252C%2520offering%2520insights%2520into%2520model%2520decision-making%2520for%2520yoga%2520pose%2520classification%2520with%2520cross-validation%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Skeleton%20Based%20Representations%20for%20Robust%20Yoga%20Pose%20Classification%20Using%20Deep%20Learning%20Models&entry.906535625=Mohammed%20Mohiuddin%20and%20Syed%20Mohammod%20Minhaz%20Hossain%20and%20Sumaiya%20Khanam%20and%20Prionkar%20Barua%20and%20Aparup%20Barua%20and%20MD%20Tamim%20Hossain&entry.1292438233=Yoga%20is%20a%20popular%20form%20of%20exercise%20worldwide%20due%20to%20its%20spiritual%20and%20physical%20health%20benefits%2C%20but%20incorrect%20postures%20can%20lead%20to%20injuries.%20Automated%20yoga%20pose%20classification%20has%20therefore%20gained%20importance%20to%20reduce%20reliance%20on%20expert%20practitioners.%20While%20human%20pose%20keypoint%20extraction%20models%20have%20shown%20high%20potential%20in%20action%20recognition%2C%20systematic%20benchmarking%20for%20yoga%20pose%20recognition%20remains%20limited%2C%20as%20prior%20works%20often%20focus%20solely%20on%20raw%20images%20or%20a%20single%20pose%20extraction%20model.%20In%20this%20study%2C%20we%20introduce%20a%20curated%20dataset%2C%20%27Yoga-16%27%2C%20which%20addresses%20limitations%20of%20existing%20datasets%2C%20and%20systematically%20evaluate%20three%20deep%20learning%20architectures%20%28VGG16%2C%20ResNet50%2C%20and%20Xception%29%2C%20using%20three%20input%20modalities%20%28direct%20images%2C%20MediaPipe%20Pose%20skeleton%20images%2C%20and%20YOLOv8%20Pose%20skeleton%20images%29.%20Our%20experiments%20demonstrate%20that%20skeleton-based%20representations%20outperform%20raw%20image%20inputs%2C%20with%20the%20highest%20accuracy%20of%2096.09%25%20achieved%20by%20VGG16%20with%20MediaPipe%20Pose%20skeleton%20input.%20Additionally%2C%20we%20provide%20interpretability%20analysis%20using%20Grad-CAM%2C%20offering%20insights%20into%20model%20decision-making%20for%20yoga%20pose%20classification%20with%20cross-validation%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2512.00572v2&entry.124074799=Read"},
{"title": "Convergence Analysis for Deep Sparse Coding via Convolutional Neural Networks", "author": "Jianfei Li and Han Feng and Ding-Xuan Zhou", "abstract": "In this work, we explore the intersection of sparse coding theory and deep learning to enhance our understanding of feature extraction capabilities in advanced neural network architectures. We begin by introducing a novel class of Deep Sparse Coding (DSC) models and establish a thorough theoretical analysis of their uniqueness and stability properties. By applying iterative algorithms to these DSC models, we derive convergence rates for convolutional neural networks (CNNs) in their ability to extract sparse features. This provides a strong theoretical foundation for the use of CNNs in sparse feature-learning tasks. We additionally extend this convergence analysis to more general neural network architectures, including those with diverse activation functions, as well as self-attention and transformer-based models. This broadens the applicability of our findings to a wide range of deep learning methods for the extraction of deep-sparse features. Inspired by the strong connection between sparse coding and CNNs, we also explore training strategies to encourage neural networks to learn sparser features. Through numerical experiments, we demonstrate the effectiveness of these approaches, providing valuable insight for the design of efficient and interpretable deep learning models.", "link": "http://arxiv.org/abs/2408.05540v3", "date": "2025-12-04", "relevancy": 2.5235, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20Analysis%20for%20Deep%20Sparse%20Coding%20via%20Convolutional%20Neural%20Networks&body=Title%3A%20Convergence%20Analysis%20for%20Deep%20Sparse%20Coding%20via%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Jianfei%20Li%20and%20Han%20Feng%20and%20Ding-Xuan%20Zhou%0AAbstract%3A%20In%20this%20work%2C%20we%20explore%20the%20intersection%20of%20sparse%20coding%20theory%20and%20deep%20learning%20to%20enhance%20our%20understanding%20of%20feature%20extraction%20capabilities%20in%20advanced%20neural%20network%20architectures.%20We%20begin%20by%20introducing%20a%20novel%20class%20of%20Deep%20Sparse%20Coding%20%28DSC%29%20models%20and%20establish%20a%20thorough%20theoretical%20analysis%20of%20their%20uniqueness%20and%20stability%20properties.%20By%20applying%20iterative%20algorithms%20to%20these%20DSC%20models%2C%20we%20derive%20convergence%20rates%20for%20convolutional%20neural%20networks%20%28CNNs%29%20in%20their%20ability%20to%20extract%20sparse%20features.%20This%20provides%20a%20strong%20theoretical%20foundation%20for%20the%20use%20of%20CNNs%20in%20sparse%20feature-learning%20tasks.%20We%20additionally%20extend%20this%20convergence%20analysis%20to%20more%20general%20neural%20network%20architectures%2C%20including%20those%20with%20diverse%20activation%20functions%2C%20as%20well%20as%20self-attention%20and%20transformer-based%20models.%20This%20broadens%20the%20applicability%20of%20our%20findings%20to%20a%20wide%20range%20of%20deep%20learning%20methods%20for%20the%20extraction%20of%20deep-sparse%20features.%20Inspired%20by%20the%20strong%20connection%20between%20sparse%20coding%20and%20CNNs%2C%20we%20also%20explore%20training%20strategies%20to%20encourage%20neural%20networks%20to%20learn%20sparser%20features.%20Through%20numerical%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20these%20approaches%2C%20providing%20valuable%20insight%20for%20the%20design%20of%20efficient%20and%20interpretable%20deep%20learning%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2408.05540v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520Analysis%2520for%2520Deep%2520Sparse%2520Coding%2520via%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DJianfei%2520Li%2520and%2520Han%2520Feng%2520and%2520Ding-Xuan%2520Zhou%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520explore%2520the%2520intersection%2520of%2520sparse%2520coding%2520theory%2520and%2520deep%2520learning%2520to%2520enhance%2520our%2520understanding%2520of%2520feature%2520extraction%2520capabilities%2520in%2520advanced%2520neural%2520network%2520architectures.%2520We%2520begin%2520by%2520introducing%2520a%2520novel%2520class%2520of%2520Deep%2520Sparse%2520Coding%2520%2528DSC%2529%2520models%2520and%2520establish%2520a%2520thorough%2520theoretical%2520analysis%2520of%2520their%2520uniqueness%2520and%2520stability%2520properties.%2520By%2520applying%2520iterative%2520algorithms%2520to%2520these%2520DSC%2520models%252C%2520we%2520derive%2520convergence%2520rates%2520for%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520in%2520their%2520ability%2520to%2520extract%2520sparse%2520features.%2520This%2520provides%2520a%2520strong%2520theoretical%2520foundation%2520for%2520the%2520use%2520of%2520CNNs%2520in%2520sparse%2520feature-learning%2520tasks.%2520We%2520additionally%2520extend%2520this%2520convergence%2520analysis%2520to%2520more%2520general%2520neural%2520network%2520architectures%252C%2520including%2520those%2520with%2520diverse%2520activation%2520functions%252C%2520as%2520well%2520as%2520self-attention%2520and%2520transformer-based%2520models.%2520This%2520broadens%2520the%2520applicability%2520of%2520our%2520findings%2520to%2520a%2520wide%2520range%2520of%2520deep%2520learning%2520methods%2520for%2520the%2520extraction%2520of%2520deep-sparse%2520features.%2520Inspired%2520by%2520the%2520strong%2520connection%2520between%2520sparse%2520coding%2520and%2520CNNs%252C%2520we%2520also%2520explore%2520training%2520strategies%2520to%2520encourage%2520neural%2520networks%2520to%2520learn%2520sparser%2520features.%2520Through%2520numerical%2520experiments%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520these%2520approaches%252C%2520providing%2520valuable%2520insight%2520for%2520the%2520design%2520of%2520efficient%2520and%2520interpretable%2520deep%2520learning%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05540v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20Analysis%20for%20Deep%20Sparse%20Coding%20via%20Convolutional%20Neural%20Networks&entry.906535625=Jianfei%20Li%20and%20Han%20Feng%20and%20Ding-Xuan%20Zhou&entry.1292438233=In%20this%20work%2C%20we%20explore%20the%20intersection%20of%20sparse%20coding%20theory%20and%20deep%20learning%20to%20enhance%20our%20understanding%20of%20feature%20extraction%20capabilities%20in%20advanced%20neural%20network%20architectures.%20We%20begin%20by%20introducing%20a%20novel%20class%20of%20Deep%20Sparse%20Coding%20%28DSC%29%20models%20and%20establish%20a%20thorough%20theoretical%20analysis%20of%20their%20uniqueness%20and%20stability%20properties.%20By%20applying%20iterative%20algorithms%20to%20these%20DSC%20models%2C%20we%20derive%20convergence%20rates%20for%20convolutional%20neural%20networks%20%28CNNs%29%20in%20their%20ability%20to%20extract%20sparse%20features.%20This%20provides%20a%20strong%20theoretical%20foundation%20for%20the%20use%20of%20CNNs%20in%20sparse%20feature-learning%20tasks.%20We%20additionally%20extend%20this%20convergence%20analysis%20to%20more%20general%20neural%20network%20architectures%2C%20including%20those%20with%20diverse%20activation%20functions%2C%20as%20well%20as%20self-attention%20and%20transformer-based%20models.%20This%20broadens%20the%20applicability%20of%20our%20findings%20to%20a%20wide%20range%20of%20deep%20learning%20methods%20for%20the%20extraction%20of%20deep-sparse%20features.%20Inspired%20by%20the%20strong%20connection%20between%20sparse%20coding%20and%20CNNs%2C%20we%20also%20explore%20training%20strategies%20to%20encourage%20neural%20networks%20to%20learn%20sparser%20features.%20Through%20numerical%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20these%20approaches%2C%20providing%20valuable%20insight%20for%20the%20design%20of%20efficient%20and%20interpretable%20deep%20learning%20models.&entry.1838667208=http%3A//arxiv.org/abs/2408.05540v3&entry.124074799=Read"},
{"title": "Improving Graph Neural Network Training, Defense, and Hypergraph Partitioning via Adversarial Robustness Evaluation", "author": "Yongyu Wang", "abstract": "Graph Neural Networks (GNNs) are a highly effective neural network architecture for processing graph-structured data. Unlike traditional neural networks that rely solely on the features of the data as input, GNNs leverage both the graph structure, which represents the relationships between data points, and the feature matrix of the data to optimize their feature representation. This unique capability enables GNNs to achieve superior performance across various tasks. However, it also makes GNNs more susceptible to noise from both the graph structure and data features, which can significantly increase the training difficulty and degrade their performance. To address this issue, this paper proposes a novel method for selecting noise-sensitive training samples from the original training set to construct a smaller yet more effective training set for model training. These samples are used to help improve the model's ability to correctly process data in noisy environments. We have evaluated our approach on three of the most classical GNN models GCN, GAT, and GraphSAGE as well as three widely used benchmark datasets: Cora, Citeseer, and PubMed. Our experiments demonstrate that the proposed method can substantially boost the training of Graph Neural Networks compared to using randomly sampled training sets of the same size from the original training set and the larger original full training set. We further proposed a robust-node based hypergraph partitioning method, an adversarial robustness based graph pruning method for GNN defenses and a related spectral edge attack method.", "link": "http://arxiv.org/abs/2412.14738v7", "date": "2025-12-04", "relevancy": 2.5037, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.536}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4863}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Graph%20Neural%20Network%20Training%2C%20Defense%2C%20and%20Hypergraph%20Partitioning%20via%20Adversarial%20Robustness%20Evaluation&body=Title%3A%20Improving%20Graph%20Neural%20Network%20Training%2C%20Defense%2C%20and%20Hypergraph%20Partitioning%20via%20Adversarial%20Robustness%20Evaluation%0AAuthor%3A%20Yongyu%20Wang%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20highly%20effective%20neural%20network%20architecture%20for%20processing%20graph-structured%20data.%20Unlike%20traditional%20neural%20networks%20that%20rely%20solely%20on%20the%20features%20of%20the%20data%20as%20input%2C%20GNNs%20leverage%20both%20the%20graph%20structure%2C%20which%20represents%20the%20relationships%20between%20data%20points%2C%20and%20the%20feature%20matrix%20of%20the%20data%20to%20optimize%20their%20feature%20representation.%20This%20unique%20capability%20enables%20GNNs%20to%20achieve%20superior%20performance%20across%20various%20tasks.%20However%2C%20it%20also%20makes%20GNNs%20more%20susceptible%20to%20noise%20from%20both%20the%20graph%20structure%20and%20data%20features%2C%20which%20can%20significantly%20increase%20the%20training%20difficulty%20and%20degrade%20their%20performance.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20method%20for%20selecting%20noise-sensitive%20training%20samples%20from%20the%20original%20training%20set%20to%20construct%20a%20smaller%20yet%20more%20effective%20training%20set%20for%20model%20training.%20These%20samples%20are%20used%20to%20help%20improve%20the%20model%27s%20ability%20to%20correctly%20process%20data%20in%20noisy%20environments.%20We%20have%20evaluated%20our%20approach%20on%20three%20of%20the%20most%20classical%20GNN%20models%20GCN%2C%20GAT%2C%20and%20GraphSAGE%20as%20well%20as%20three%20widely%20used%20benchmark%20datasets%3A%20Cora%2C%20Citeseer%2C%20and%20PubMed.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20method%20can%20substantially%20boost%20the%20training%20of%20Graph%20Neural%20Networks%20compared%20to%20using%20randomly%20sampled%20training%20sets%20of%20the%20same%20size%20from%20the%20original%20training%20set%20and%20the%20larger%20original%20full%20training%20set.%20We%20further%20proposed%20a%20robust-node%20based%20hypergraph%20partitioning%20method%2C%20an%20adversarial%20robustness%20based%20graph%20pruning%20method%20for%20GNN%20defenses%20and%20a%20related%20spectral%20edge%20attack%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2412.14738v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Graph%2520Neural%2520Network%2520Training%252C%2520Defense%252C%2520and%2520Hypergraph%2520Partitioning%2520via%2520Adversarial%2520Robustness%2520Evaluation%26entry.906535625%3DYongyu%2520Wang%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520a%2520highly%2520effective%2520neural%2520network%2520architecture%2520for%2520processing%2520graph-structured%2520data.%2520Unlike%2520traditional%2520neural%2520networks%2520that%2520rely%2520solely%2520on%2520the%2520features%2520of%2520the%2520data%2520as%2520input%252C%2520GNNs%2520leverage%2520both%2520the%2520graph%2520structure%252C%2520which%2520represents%2520the%2520relationships%2520between%2520data%2520points%252C%2520and%2520the%2520feature%2520matrix%2520of%2520the%2520data%2520to%2520optimize%2520their%2520feature%2520representation.%2520This%2520unique%2520capability%2520enables%2520GNNs%2520to%2520achieve%2520superior%2520performance%2520across%2520various%2520tasks.%2520However%252C%2520it%2520also%2520makes%2520GNNs%2520more%2520susceptible%2520to%2520noise%2520from%2520both%2520the%2520graph%2520structure%2520and%2520data%2520features%252C%2520which%2520can%2520significantly%2520increase%2520the%2520training%2520difficulty%2520and%2520degrade%2520their%2520performance.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520method%2520for%2520selecting%2520noise-sensitive%2520training%2520samples%2520from%2520the%2520original%2520training%2520set%2520to%2520construct%2520a%2520smaller%2520yet%2520more%2520effective%2520training%2520set%2520for%2520model%2520training.%2520These%2520samples%2520are%2520used%2520to%2520help%2520improve%2520the%2520model%2527s%2520ability%2520to%2520correctly%2520process%2520data%2520in%2520noisy%2520environments.%2520We%2520have%2520evaluated%2520our%2520approach%2520on%2520three%2520of%2520the%2520most%2520classical%2520GNN%2520models%2520GCN%252C%2520GAT%252C%2520and%2520GraphSAGE%2520as%2520well%2520as%2520three%2520widely%2520used%2520benchmark%2520datasets%253A%2520Cora%252C%2520Citeseer%252C%2520and%2520PubMed.%2520Our%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520can%2520substantially%2520boost%2520the%2520training%2520of%2520Graph%2520Neural%2520Networks%2520compared%2520to%2520using%2520randomly%2520sampled%2520training%2520sets%2520of%2520the%2520same%2520size%2520from%2520the%2520original%2520training%2520set%2520and%2520the%2520larger%2520original%2520full%2520training%2520set.%2520We%2520further%2520proposed%2520a%2520robust-node%2520based%2520hypergraph%2520partitioning%2520method%252C%2520an%2520adversarial%2520robustness%2520based%2520graph%2520pruning%2520method%2520for%2520GNN%2520defenses%2520and%2520a%2520related%2520spectral%2520edge%2520attack%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14738v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Graph%20Neural%20Network%20Training%2C%20Defense%2C%20and%20Hypergraph%20Partitioning%20via%20Adversarial%20Robustness%20Evaluation&entry.906535625=Yongyu%20Wang&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20highly%20effective%20neural%20network%20architecture%20for%20processing%20graph-structured%20data.%20Unlike%20traditional%20neural%20networks%20that%20rely%20solely%20on%20the%20features%20of%20the%20data%20as%20input%2C%20GNNs%20leverage%20both%20the%20graph%20structure%2C%20which%20represents%20the%20relationships%20between%20data%20points%2C%20and%20the%20feature%20matrix%20of%20the%20data%20to%20optimize%20their%20feature%20representation.%20This%20unique%20capability%20enables%20GNNs%20to%20achieve%20superior%20performance%20across%20various%20tasks.%20However%2C%20it%20also%20makes%20GNNs%20more%20susceptible%20to%20noise%20from%20both%20the%20graph%20structure%20and%20data%20features%2C%20which%20can%20significantly%20increase%20the%20training%20difficulty%20and%20degrade%20their%20performance.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20method%20for%20selecting%20noise-sensitive%20training%20samples%20from%20the%20original%20training%20set%20to%20construct%20a%20smaller%20yet%20more%20effective%20training%20set%20for%20model%20training.%20These%20samples%20are%20used%20to%20help%20improve%20the%20model%27s%20ability%20to%20correctly%20process%20data%20in%20noisy%20environments.%20We%20have%20evaluated%20our%20approach%20on%20three%20of%20the%20most%20classical%20GNN%20models%20GCN%2C%20GAT%2C%20and%20GraphSAGE%20as%20well%20as%20three%20widely%20used%20benchmark%20datasets%3A%20Cora%2C%20Citeseer%2C%20and%20PubMed.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20method%20can%20substantially%20boost%20the%20training%20of%20Graph%20Neural%20Networks%20compared%20to%20using%20randomly%20sampled%20training%20sets%20of%20the%20same%20size%20from%20the%20original%20training%20set%20and%20the%20larger%20original%20full%20training%20set.%20We%20further%20proposed%20a%20robust-node%20based%20hypergraph%20partitioning%20method%2C%20an%20adversarial%20robustness%20based%20graph%20pruning%20method%20for%20GNN%20defenses%20and%20a%20related%20spectral%20edge%20attack%20method.&entry.1838667208=http%3A//arxiv.org/abs/2412.14738v7&entry.124074799=Read"},
{"title": "OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models", "author": "Zhuoyue Wan and Wentao Hu and Chen Jason Zhang and Yuanfeng Song and Shuaimin Li and Ruiqiang Xiao and Xiao-Yong Wei and Raymond Chi-Wing Wong", "abstract": "Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.", "link": "http://arxiv.org/abs/2512.04738v1", "date": "2025-12-04", "relevancy": 2.4728, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OsmT%3A%20Bridging%20OpenStreetMap%20Queries%20and%20Natural%20Language%20with%20Open-source%20Tag-aware%20Language%20Models&body=Title%3A%20OsmT%3A%20Bridging%20OpenStreetMap%20Queries%20and%20Natural%20Language%20with%20Open-source%20Tag-aware%20Language%20Models%0AAuthor%3A%20Zhuoyue%20Wan%20and%20Wentao%20Hu%20and%20Chen%20Jason%20Zhang%20and%20Yuanfeng%20Song%20and%20Shuaimin%20Li%20and%20Ruiqiang%20Xiao%20and%20Xiao-Yong%20Wei%20and%20Raymond%20Chi-Wing%20Wong%0AAbstract%3A%20Bridging%20natural%20language%20and%20structured%20query%20languages%20is%20a%20long-standing%20challenge%20in%20the%20database%20community.%20While%20recent%20advances%20in%20language%20models%20have%20shown%20promise%20in%20this%20direction%2C%20existing%20solutions%20often%20rely%20on%20large-scale%20closed-source%20models%20that%20suffer%20from%20high%20inference%20costs%2C%20limited%20transparency%2C%20and%20lack%20of%20adaptability%20for%20lightweight%20deployment.%20In%20this%20paper%2C%20we%20present%20OsmT%2C%20an%20open-source%20tag-aware%20language%20model%20specifically%20designed%20to%20bridge%20natural%20language%20and%20Overpass%20Query%20Language%20%28OverpassQL%29%2C%20a%20structured%20query%20language%20for%20accessing%20large-scale%20OpenStreetMap%20%28OSM%29%20data.%20To%20enhance%20the%20accuracy%20and%20structural%20validity%20of%20generated%20queries%2C%20we%20introduce%20a%20Tag%20Retrieval%20Augmentation%20%28TRA%29%20mechanism%20that%20incorporates%20contextually%20relevant%20tag%20knowledge%20into%20the%20generation%20process.%20This%20mechanism%20is%20designed%20to%20capture%20the%20hierarchical%20and%20relational%20dependencies%20present%20in%20the%20OSM%20database%2C%20addressing%20the%20topological%20complexity%20inherent%20in%20geospatial%20query%20formulation.%20In%20addition%2C%20we%20define%20a%20reverse%20task%2C%20OverpassQL-to-Text%2C%20which%20translates%20structured%20queries%20into%20natural%20language%20explanations%20to%20support%20query%20interpretation%20and%20improve%20user%20accessibility.%20We%20evaluate%20OsmT%20on%20a%20public%20benchmark%20against%20strong%20baselines%20and%20observe%20consistent%20improvements%20in%20both%20query%20generation%20and%20interpretation.%20Despite%20using%20significantly%20fewer%20parameters%2C%20our%20model%20achieves%20competitive%20accuracy%2C%20demonstrating%20the%20effectiveness%20of%20open-source%20pre-trained%20language%20models%20in%20bridging%20natural%20language%20and%20structured%20query%20languages%20within%20schema-rich%20geospatial%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOsmT%253A%2520Bridging%2520OpenStreetMap%2520Queries%2520and%2520Natural%2520Language%2520with%2520Open-source%2520Tag-aware%2520Language%2520Models%26entry.906535625%3DZhuoyue%2520Wan%2520and%2520Wentao%2520Hu%2520and%2520Chen%2520Jason%2520Zhang%2520and%2520Yuanfeng%2520Song%2520and%2520Shuaimin%2520Li%2520and%2520Ruiqiang%2520Xiao%2520and%2520Xiao-Yong%2520Wei%2520and%2520Raymond%2520Chi-Wing%2520Wong%26entry.1292438233%3DBridging%2520natural%2520language%2520and%2520structured%2520query%2520languages%2520is%2520a%2520long-standing%2520challenge%2520in%2520the%2520database%2520community.%2520While%2520recent%2520advances%2520in%2520language%2520models%2520have%2520shown%2520promise%2520in%2520this%2520direction%252C%2520existing%2520solutions%2520often%2520rely%2520on%2520large-scale%2520closed-source%2520models%2520that%2520suffer%2520from%2520high%2520inference%2520costs%252C%2520limited%2520transparency%252C%2520and%2520lack%2520of%2520adaptability%2520for%2520lightweight%2520deployment.%2520In%2520this%2520paper%252C%2520we%2520present%2520OsmT%252C%2520an%2520open-source%2520tag-aware%2520language%2520model%2520specifically%2520designed%2520to%2520bridge%2520natural%2520language%2520and%2520Overpass%2520Query%2520Language%2520%2528OverpassQL%2529%252C%2520a%2520structured%2520query%2520language%2520for%2520accessing%2520large-scale%2520OpenStreetMap%2520%2528OSM%2529%2520data.%2520To%2520enhance%2520the%2520accuracy%2520and%2520structural%2520validity%2520of%2520generated%2520queries%252C%2520we%2520introduce%2520a%2520Tag%2520Retrieval%2520Augmentation%2520%2528TRA%2529%2520mechanism%2520that%2520incorporates%2520contextually%2520relevant%2520tag%2520knowledge%2520into%2520the%2520generation%2520process.%2520This%2520mechanism%2520is%2520designed%2520to%2520capture%2520the%2520hierarchical%2520and%2520relational%2520dependencies%2520present%2520in%2520the%2520OSM%2520database%252C%2520addressing%2520the%2520topological%2520complexity%2520inherent%2520in%2520geospatial%2520query%2520formulation.%2520In%2520addition%252C%2520we%2520define%2520a%2520reverse%2520task%252C%2520OverpassQL-to-Text%252C%2520which%2520translates%2520structured%2520queries%2520into%2520natural%2520language%2520explanations%2520to%2520support%2520query%2520interpretation%2520and%2520improve%2520user%2520accessibility.%2520We%2520evaluate%2520OsmT%2520on%2520a%2520public%2520benchmark%2520against%2520strong%2520baselines%2520and%2520observe%2520consistent%2520improvements%2520in%2520both%2520query%2520generation%2520and%2520interpretation.%2520Despite%2520using%2520significantly%2520fewer%2520parameters%252C%2520our%2520model%2520achieves%2520competitive%2520accuracy%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520open-source%2520pre-trained%2520language%2520models%2520in%2520bridging%2520natural%2520language%2520and%2520structured%2520query%2520languages%2520within%2520schema-rich%2520geospatial%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OsmT%3A%20Bridging%20OpenStreetMap%20Queries%20and%20Natural%20Language%20with%20Open-source%20Tag-aware%20Language%20Models&entry.906535625=Zhuoyue%20Wan%20and%20Wentao%20Hu%20and%20Chen%20Jason%20Zhang%20and%20Yuanfeng%20Song%20and%20Shuaimin%20Li%20and%20Ruiqiang%20Xiao%20and%20Xiao-Yong%20Wei%20and%20Raymond%20Chi-Wing%20Wong&entry.1292438233=Bridging%20natural%20language%20and%20structured%20query%20languages%20is%20a%20long-standing%20challenge%20in%20the%20database%20community.%20While%20recent%20advances%20in%20language%20models%20have%20shown%20promise%20in%20this%20direction%2C%20existing%20solutions%20often%20rely%20on%20large-scale%20closed-source%20models%20that%20suffer%20from%20high%20inference%20costs%2C%20limited%20transparency%2C%20and%20lack%20of%20adaptability%20for%20lightweight%20deployment.%20In%20this%20paper%2C%20we%20present%20OsmT%2C%20an%20open-source%20tag-aware%20language%20model%20specifically%20designed%20to%20bridge%20natural%20language%20and%20Overpass%20Query%20Language%20%28OverpassQL%29%2C%20a%20structured%20query%20language%20for%20accessing%20large-scale%20OpenStreetMap%20%28OSM%29%20data.%20To%20enhance%20the%20accuracy%20and%20structural%20validity%20of%20generated%20queries%2C%20we%20introduce%20a%20Tag%20Retrieval%20Augmentation%20%28TRA%29%20mechanism%20that%20incorporates%20contextually%20relevant%20tag%20knowledge%20into%20the%20generation%20process.%20This%20mechanism%20is%20designed%20to%20capture%20the%20hierarchical%20and%20relational%20dependencies%20present%20in%20the%20OSM%20database%2C%20addressing%20the%20topological%20complexity%20inherent%20in%20geospatial%20query%20formulation.%20In%20addition%2C%20we%20define%20a%20reverse%20task%2C%20OverpassQL-to-Text%2C%20which%20translates%20structured%20queries%20into%20natural%20language%20explanations%20to%20support%20query%20interpretation%20and%20improve%20user%20accessibility.%20We%20evaluate%20OsmT%20on%20a%20public%20benchmark%20against%20strong%20baselines%20and%20observe%20consistent%20improvements%20in%20both%20query%20generation%20and%20interpretation.%20Despite%20using%20significantly%20fewer%20parameters%2C%20our%20model%20achieves%20competitive%20accuracy%2C%20demonstrating%20the%20effectiveness%20of%20open-source%20pre-trained%20language%20models%20in%20bridging%20natural%20language%20and%20structured%20query%20languages%20within%20schema-rich%20geospatial%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.04738v1&entry.124074799=Read"},
{"title": "Shared Multi-modal Embedding Space for Face-Voice Association", "author": "Christopher Simic and Korbinian Riedhammer and Tobias Bocklet", "abstract": "The FAME 2026 challenge comprises two demanding tasks: training face-voice associations combined with a multilingual setting that includes testing on languages on which the model was not trained. Our approach consists of separate uni-modal processing pipelines with general face and voice feature extraction, complemented by additional age-gender feature extraction to support prediction. The resulting single-modal features are projected into a shared embedding space and trained with an Adaptive Angular Margin (AAM) loss. Our approach achieved first place in the FAME 2026 challenge, with an average Equal-Error Rate (EER) of 23.99%.", "link": "http://arxiv.org/abs/2512.04814v1", "date": "2025-12-04", "relevancy": 2.4716, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shared%20Multi-modal%20Embedding%20Space%20for%20Face-Voice%20Association&body=Title%3A%20Shared%20Multi-modal%20Embedding%20Space%20for%20Face-Voice%20Association%0AAuthor%3A%20Christopher%20Simic%20and%20Korbinian%20Riedhammer%20and%20Tobias%20Bocklet%0AAbstract%3A%20The%20FAME%202026%20challenge%20comprises%20two%20demanding%20tasks%3A%20training%20face-voice%20associations%20combined%20with%20a%20multilingual%20setting%20that%20includes%20testing%20on%20languages%20on%20which%20the%20model%20was%20not%20trained.%20Our%20approach%20consists%20of%20separate%20uni-modal%20processing%20pipelines%20with%20general%20face%20and%20voice%20feature%20extraction%2C%20complemented%20by%20additional%20age-gender%20feature%20extraction%20to%20support%20prediction.%20The%20resulting%20single-modal%20features%20are%20projected%20into%20a%20shared%20embedding%20space%20and%20trained%20with%20an%20Adaptive%20Angular%20Margin%20%28AAM%29%20loss.%20Our%20approach%20achieved%20first%20place%20in%20the%20FAME%202026%20challenge%2C%20with%20an%20average%20Equal-Error%20Rate%20%28EER%29%20of%2023.99%25.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShared%2520Multi-modal%2520Embedding%2520Space%2520for%2520Face-Voice%2520Association%26entry.906535625%3DChristopher%2520Simic%2520and%2520Korbinian%2520Riedhammer%2520and%2520Tobias%2520Bocklet%26entry.1292438233%3DThe%2520FAME%25202026%2520challenge%2520comprises%2520two%2520demanding%2520tasks%253A%2520training%2520face-voice%2520associations%2520combined%2520with%2520a%2520multilingual%2520setting%2520that%2520includes%2520testing%2520on%2520languages%2520on%2520which%2520the%2520model%2520was%2520not%2520trained.%2520Our%2520approach%2520consists%2520of%2520separate%2520uni-modal%2520processing%2520pipelines%2520with%2520general%2520face%2520and%2520voice%2520feature%2520extraction%252C%2520complemented%2520by%2520additional%2520age-gender%2520feature%2520extraction%2520to%2520support%2520prediction.%2520The%2520resulting%2520single-modal%2520features%2520are%2520projected%2520into%2520a%2520shared%2520embedding%2520space%2520and%2520trained%2520with%2520an%2520Adaptive%2520Angular%2520Margin%2520%2528AAM%2529%2520loss.%2520Our%2520approach%2520achieved%2520first%2520place%2520in%2520the%2520FAME%25202026%2520challenge%252C%2520with%2520an%2520average%2520Equal-Error%2520Rate%2520%2528EER%2529%2520of%252023.99%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shared%20Multi-modal%20Embedding%20Space%20for%20Face-Voice%20Association&entry.906535625=Christopher%20Simic%20and%20Korbinian%20Riedhammer%20and%20Tobias%20Bocklet&entry.1292438233=The%20FAME%202026%20challenge%20comprises%20two%20demanding%20tasks%3A%20training%20face-voice%20associations%20combined%20with%20a%20multilingual%20setting%20that%20includes%20testing%20on%20languages%20on%20which%20the%20model%20was%20not%20trained.%20Our%20approach%20consists%20of%20separate%20uni-modal%20processing%20pipelines%20with%20general%20face%20and%20voice%20feature%20extraction%2C%20complemented%20by%20additional%20age-gender%20feature%20extraction%20to%20support%20prediction.%20The%20resulting%20single-modal%20features%20are%20projected%20into%20a%20shared%20embedding%20space%20and%20trained%20with%20an%20Adaptive%20Angular%20Margin%20%28AAM%29%20loss.%20Our%20approach%20achieved%20first%20place%20in%20the%20FAME%202026%20challenge%2C%20with%20an%20average%20Equal-Error%20Rate%20%28EER%29%20of%2023.99%25.&entry.1838667208=http%3A//arxiv.org/abs/2512.04814v1&entry.124074799=Read"},
{"title": "YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance", "author": "Junjie Zheng and Chunbo Hao and Guobin Ma and Xiaoyu Zhang and Gongyu Chen and Chaofan Ding and Zihao Chen and Lei Xie", "abstract": "Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints.", "link": "http://arxiv.org/abs/2512.04779v1", "date": "2025-12-04", "relevancy": 2.4708, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4965}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4931}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YingMusic-Singer%3A%20Zero-shot%20Singing%20Voice%20Synthesis%20and%20Editing%20with%20Annotation-free%20Melody%20Guidance&body=Title%3A%20YingMusic-Singer%3A%20Zero-shot%20Singing%20Voice%20Synthesis%20and%20Editing%20with%20Annotation-free%20Melody%20Guidance%0AAuthor%3A%20Junjie%20Zheng%20and%20Chunbo%20Hao%20and%20Guobin%20Ma%20and%20Xiaoyu%20Zhang%20and%20Gongyu%20Chen%20and%20Chaofan%20Ding%20and%20Zihao%20Chen%20and%20Lei%20Xie%0AAbstract%3A%20Singing%20Voice%20Synthesis%20%28SVS%29%20remains%20constrained%20in%20practical%20deployment%20due%20to%20its%20strong%20dependence%20on%20accurate%20phoneme-level%20alignment%20and%20manually%20annotated%20melody%20contours%2C%20requirements%20that%20are%20resource-intensive%20and%20hinder%20scalability.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20melody-driven%20SVS%20framework%20capable%20of%20synthesizing%20arbitrary%20lyrics%20following%20any%20reference%20melody%2C%20without%20relying%20on%20phoneme-level%20alignment.%20Our%20method%20builds%20on%20a%20Diffusion%20Transformer%20%28DiT%29%20architecture%2C%20enhanced%20with%20a%20dedicated%20melody%20extraction%20module%20that%20derives%20melody%20representations%20directly%20from%20reference%20audio.%20To%20ensure%20robust%20melody%20encoding%2C%20we%20employ%20a%20teacher%20model%20to%20guide%20the%20optimization%20of%20the%20melody%20extractor%2C%20alongside%20an%20implicit%20alignment%20mechanism%20that%20enforces%20similarity%20distribution%20constraints%20for%20improved%20melodic%20stability%20and%20coherence.%20Additionally%2C%20we%20refine%20duration%20modeling%20using%20weakly%20annotated%20song%20data%20and%20introduce%20a%20Flow-GRPO%20reinforcement%20learning%20strategy%20with%20a%20multi-objective%20reward%20function%20to%20jointly%20enhance%20pronunciation%20clarity%20and%20melodic%20fidelity.%20Experiments%20show%20that%20our%20model%20achieves%20superior%20performance%20over%20existing%20approaches%20in%20both%20objective%20measures%20and%20subjective%20listening%20tests%2C%20especially%20in%20zero-shot%20and%20lyric%20adaptation%20settings%2C%20while%20maintaining%20high%20audio%20quality%20without%20manual%20annotation.%20This%20work%20offers%20a%20practical%20and%20scalable%20solution%20for%20advancing%20data-efficient%20singing%20voice%20synthesis.%20To%20support%20reproducibility%2C%20we%20release%20our%20inference%20code%20and%20model%20checkpoints.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYingMusic-Singer%253A%2520Zero-shot%2520Singing%2520Voice%2520Synthesis%2520and%2520Editing%2520with%2520Annotation-free%2520Melody%2520Guidance%26entry.906535625%3DJunjie%2520Zheng%2520and%2520Chunbo%2520Hao%2520and%2520Guobin%2520Ma%2520and%2520Xiaoyu%2520Zhang%2520and%2520Gongyu%2520Chen%2520and%2520Chaofan%2520Ding%2520and%2520Zihao%2520Chen%2520and%2520Lei%2520Xie%26entry.1292438233%3DSinging%2520Voice%2520Synthesis%2520%2528SVS%2529%2520remains%2520constrained%2520in%2520practical%2520deployment%2520due%2520to%2520its%2520strong%2520dependence%2520on%2520accurate%2520phoneme-level%2520alignment%2520and%2520manually%2520annotated%2520melody%2520contours%252C%2520requirements%2520that%2520are%2520resource-intensive%2520and%2520hinder%2520scalability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520melody-driven%2520SVS%2520framework%2520capable%2520of%2520synthesizing%2520arbitrary%2520lyrics%2520following%2520any%2520reference%2520melody%252C%2520without%2520relying%2520on%2520phoneme-level%2520alignment.%2520Our%2520method%2520builds%2520on%2520a%2520Diffusion%2520Transformer%2520%2528DiT%2529%2520architecture%252C%2520enhanced%2520with%2520a%2520dedicated%2520melody%2520extraction%2520module%2520that%2520derives%2520melody%2520representations%2520directly%2520from%2520reference%2520audio.%2520To%2520ensure%2520robust%2520melody%2520encoding%252C%2520we%2520employ%2520a%2520teacher%2520model%2520to%2520guide%2520the%2520optimization%2520of%2520the%2520melody%2520extractor%252C%2520alongside%2520an%2520implicit%2520alignment%2520mechanism%2520that%2520enforces%2520similarity%2520distribution%2520constraints%2520for%2520improved%2520melodic%2520stability%2520and%2520coherence.%2520Additionally%252C%2520we%2520refine%2520duration%2520modeling%2520using%2520weakly%2520annotated%2520song%2520data%2520and%2520introduce%2520a%2520Flow-GRPO%2520reinforcement%2520learning%2520strategy%2520with%2520a%2520multi-objective%2520reward%2520function%2520to%2520jointly%2520enhance%2520pronunciation%2520clarity%2520and%2520melodic%2520fidelity.%2520Experiments%2520show%2520that%2520our%2520model%2520achieves%2520superior%2520performance%2520over%2520existing%2520approaches%2520in%2520both%2520objective%2520measures%2520and%2520subjective%2520listening%2520tests%252C%2520especially%2520in%2520zero-shot%2520and%2520lyric%2520adaptation%2520settings%252C%2520while%2520maintaining%2520high%2520audio%2520quality%2520without%2520manual%2520annotation.%2520This%2520work%2520offers%2520a%2520practical%2520and%2520scalable%2520solution%2520for%2520advancing%2520data-efficient%2520singing%2520voice%2520synthesis.%2520To%2520support%2520reproducibility%252C%2520we%2520release%2520our%2520inference%2520code%2520and%2520model%2520checkpoints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YingMusic-Singer%3A%20Zero-shot%20Singing%20Voice%20Synthesis%20and%20Editing%20with%20Annotation-free%20Melody%20Guidance&entry.906535625=Junjie%20Zheng%20and%20Chunbo%20Hao%20and%20Guobin%20Ma%20and%20Xiaoyu%20Zhang%20and%20Gongyu%20Chen%20and%20Chaofan%20Ding%20and%20Zihao%20Chen%20and%20Lei%20Xie&entry.1292438233=Singing%20Voice%20Synthesis%20%28SVS%29%20remains%20constrained%20in%20practical%20deployment%20due%20to%20its%20strong%20dependence%20on%20accurate%20phoneme-level%20alignment%20and%20manually%20annotated%20melody%20contours%2C%20requirements%20that%20are%20resource-intensive%20and%20hinder%20scalability.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20melody-driven%20SVS%20framework%20capable%20of%20synthesizing%20arbitrary%20lyrics%20following%20any%20reference%20melody%2C%20without%20relying%20on%20phoneme-level%20alignment.%20Our%20method%20builds%20on%20a%20Diffusion%20Transformer%20%28DiT%29%20architecture%2C%20enhanced%20with%20a%20dedicated%20melody%20extraction%20module%20that%20derives%20melody%20representations%20directly%20from%20reference%20audio.%20To%20ensure%20robust%20melody%20encoding%2C%20we%20employ%20a%20teacher%20model%20to%20guide%20the%20optimization%20of%20the%20melody%20extractor%2C%20alongside%20an%20implicit%20alignment%20mechanism%20that%20enforces%20similarity%20distribution%20constraints%20for%20improved%20melodic%20stability%20and%20coherence.%20Additionally%2C%20we%20refine%20duration%20modeling%20using%20weakly%20annotated%20song%20data%20and%20introduce%20a%20Flow-GRPO%20reinforcement%20learning%20strategy%20with%20a%20multi-objective%20reward%20function%20to%20jointly%20enhance%20pronunciation%20clarity%20and%20melodic%20fidelity.%20Experiments%20show%20that%20our%20model%20achieves%20superior%20performance%20over%20existing%20approaches%20in%20both%20objective%20measures%20and%20subjective%20listening%20tests%2C%20especially%20in%20zero-shot%20and%20lyric%20adaptation%20settings%2C%20while%20maintaining%20high%20audio%20quality%20without%20manual%20annotation.%20This%20work%20offers%20a%20practical%20and%20scalable%20solution%20for%20advancing%20data-efficient%20singing%20voice%20synthesis.%20To%20support%20reproducibility%2C%20we%20release%20our%20inference%20code%20and%20model%20checkpoints.&entry.1838667208=http%3A//arxiv.org/abs/2512.04779v1&entry.124074799=Read"},
{"title": "Jina-VLM: Small Multilingual Vision Language Model", "author": "Andreas Koukounas and Georgios Mastrapas and Florian H\u00f6nicke and Sedigheh Eslami and Guillaume Roncari and Scott Martens and Han Xiao", "abstract": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. The model achieves leading results on standard VQA benchmarks and multilingual evaluations while preserving competitive text-only performance. Model weights and code are publicly released at https://huggingface.co/jinaai/jina-vlm .", "link": "http://arxiv.org/abs/2512.04032v2", "date": "2025-12-04", "relevancy": 2.4498, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jina-VLM%3A%20Small%20Multilingual%20Vision%20Language%20Model&body=Title%3A%20Jina-VLM%3A%20Small%20Multilingual%20Vision%20Language%20Model%0AAuthor%3A%20Andreas%20Koukounas%20and%20Georgios%20Mastrapas%20and%20Florian%20H%C3%B6nicke%20and%20Sedigheh%20Eslami%20and%20Guillaume%20Roncari%20and%20Scott%20Martens%20and%20Han%20Xiao%0AAbstract%3A%20We%20present%20Jina-VLM%2C%20a%202.4B%20parameter%20vision-language%20model%20that%20achieves%20state-of-the-art%20multilingual%20visual%20question%20answering%20among%20open%202B-scale%20VLMs.%20The%20model%20couples%20a%20SigLIP2%20vision%20encoder%20with%20a%20Qwen3%20language%20backbone%20through%20an%20attention-pooling%20connector%20that%20enables%20token-efficient%20processing%20of%20arbitrary-resolution%20images.%20The%20model%20achieves%20leading%20results%20on%20standard%20VQA%20benchmarks%20and%20multilingual%20evaluations%20while%20preserving%20competitive%20text-only%20performance.%20Model%20weights%20and%20code%20are%20publicly%20released%20at%20https%3A//huggingface.co/jinaai/jina-vlm%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04032v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJina-VLM%253A%2520Small%2520Multilingual%2520Vision%2520Language%2520Model%26entry.906535625%3DAndreas%2520Koukounas%2520and%2520Georgios%2520Mastrapas%2520and%2520Florian%2520H%25C3%25B6nicke%2520and%2520Sedigheh%2520Eslami%2520and%2520Guillaume%2520Roncari%2520and%2520Scott%2520Martens%2520and%2520Han%2520Xiao%26entry.1292438233%3DWe%2520present%2520Jina-VLM%252C%2520a%25202.4B%2520parameter%2520vision-language%2520model%2520that%2520achieves%2520state-of-the-art%2520multilingual%2520visual%2520question%2520answering%2520among%2520open%25202B-scale%2520VLMs.%2520The%2520model%2520couples%2520a%2520SigLIP2%2520vision%2520encoder%2520with%2520a%2520Qwen3%2520language%2520backbone%2520through%2520an%2520attention-pooling%2520connector%2520that%2520enables%2520token-efficient%2520processing%2520of%2520arbitrary-resolution%2520images.%2520The%2520model%2520achieves%2520leading%2520results%2520on%2520standard%2520VQA%2520benchmarks%2520and%2520multilingual%2520evaluations%2520while%2520preserving%2520competitive%2520text-only%2520performance.%2520Model%2520weights%2520and%2520code%2520are%2520publicly%2520released%2520at%2520https%253A//huggingface.co/jinaai/jina-vlm%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04032v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jina-VLM%3A%20Small%20Multilingual%20Vision%20Language%20Model&entry.906535625=Andreas%20Koukounas%20and%20Georgios%20Mastrapas%20and%20Florian%20H%C3%B6nicke%20and%20Sedigheh%20Eslami%20and%20Guillaume%20Roncari%20and%20Scott%20Martens%20and%20Han%20Xiao&entry.1292438233=We%20present%20Jina-VLM%2C%20a%202.4B%20parameter%20vision-language%20model%20that%20achieves%20state-of-the-art%20multilingual%20visual%20question%20answering%20among%20open%202B-scale%20VLMs.%20The%20model%20couples%20a%20SigLIP2%20vision%20encoder%20with%20a%20Qwen3%20language%20backbone%20through%20an%20attention-pooling%20connector%20that%20enables%20token-efficient%20processing%20of%20arbitrary-resolution%20images.%20The%20model%20achieves%20leading%20results%20on%20standard%20VQA%20benchmarks%20and%20multilingual%20evaluations%20while%20preserving%20competitive%20text-only%20performance.%20Model%20weights%20and%20code%20are%20publicly%20released%20at%20https%3A//huggingface.co/jinaai/jina-vlm%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.04032v2&entry.124074799=Read"},
{"title": "Large language models can learn and generalize steganographic chain-of-thought under process supervision", "author": "Joey Skaf and Luis Ibanez-Lissen and Robert McCarthy and Connor Watts and Vasil Georgiv and Hannes Whittingham and Lorena Gonzalez-Manzano and David Lindner and Cameron Tice and Edward James Young and Puria Radmard", "abstract": "Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. However, recent works have shown that banning the mention of a specific example of reward hacking causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior, threatening the reliability of CoT monitoring. We provide an extension to these results with regard to the ability of models to learn a specific type of obfuscated reasoning: steganography. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning.We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.", "link": "http://arxiv.org/abs/2506.01926v2", "date": "2025-12-04", "relevancy": 2.4344, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20language%20models%20can%20learn%20and%20generalize%20steganographic%20chain-of-thought%20under%20process%20supervision&body=Title%3A%20Large%20language%20models%20can%20learn%20and%20generalize%20steganographic%20chain-of-thought%20under%20process%20supervision%0AAuthor%3A%20Joey%20Skaf%20and%20Luis%20Ibanez-Lissen%20and%20Robert%20McCarthy%20and%20Connor%20Watts%20and%20Vasil%20Georgiv%20and%20Hannes%20Whittingham%20and%20Lorena%20Gonzalez-Manzano%20and%20David%20Lindner%20and%20Cameron%20Tice%20and%20Edward%20James%20Young%20and%20Puria%20Radmard%0AAbstract%3A%20Chain-of-thought%20%28CoT%29%20reasoning%20not%20only%20enhances%20large%20language%20model%20performance%20but%20also%20provides%20critical%20insights%20into%20decision-making%20processes%2C%20marking%20it%20as%20a%20useful%20tool%20for%20monitoring%20model%20intent%20and%20planning.%20However%2C%20recent%20works%20have%20shown%20that%20banning%20the%20mention%20of%20a%20specific%20example%20of%20reward%20hacking%20causes%20obfuscation%20of%20the%20undesired%20reasoning%20traces%20but%20the%20persistence%20of%20the%20undesired%20behavior%2C%20threatening%20the%20reliability%20of%20CoT%20monitoring.%20We%20provide%20an%20extension%20to%20these%20results%20with%20regard%20to%20the%20ability%20of%20models%20to%20learn%20a%20specific%20type%20of%20obfuscated%20reasoning%3A%20steganography.%20First%2C%20we%20show%20that%20penalizing%20the%20use%20of%20specific%20strings%20within%20load-bearing%20reasoning%20traces%20causes%20models%20to%20substitute%20alternative%20strings.%20Crucially%2C%20this%20does%20not%20alter%20the%20underlying%20method%20by%20which%20the%20model%20performs%20the%20task%2C%20demonstrating%20that%20the%20model%20can%20learn%20to%20steganographically%20encode%20its%20reasoning.We%20further%20demonstrate%20that%20models%20can%20generalize%20an%20encoding%20scheme.%20When%20the%20penalized%20strings%20belong%20to%20an%20overarching%20class%2C%20the%20model%20learns%20not%20only%20to%20substitute%20strings%20seen%20in%20training%2C%20but%20also%20develops%20a%20general%20encoding%20scheme%20for%20all%20members%20of%20the%20class%20which%20it%20can%20apply%20to%20held-out%20testing%20strings.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520language%2520models%2520can%2520learn%2520and%2520generalize%2520steganographic%2520chain-of-thought%2520under%2520process%2520supervision%26entry.906535625%3DJoey%2520Skaf%2520and%2520Luis%2520Ibanez-Lissen%2520and%2520Robert%2520McCarthy%2520and%2520Connor%2520Watts%2520and%2520Vasil%2520Georgiv%2520and%2520Hannes%2520Whittingham%2520and%2520Lorena%2520Gonzalez-Manzano%2520and%2520David%2520Lindner%2520and%2520Cameron%2520Tice%2520and%2520Edward%2520James%2520Young%2520and%2520Puria%2520Radmard%26entry.1292438233%3DChain-of-thought%2520%2528CoT%2529%2520reasoning%2520not%2520only%2520enhances%2520large%2520language%2520model%2520performance%2520but%2520also%2520provides%2520critical%2520insights%2520into%2520decision-making%2520processes%252C%2520marking%2520it%2520as%2520a%2520useful%2520tool%2520for%2520monitoring%2520model%2520intent%2520and%2520planning.%2520However%252C%2520recent%2520works%2520have%2520shown%2520that%2520banning%2520the%2520mention%2520of%2520a%2520specific%2520example%2520of%2520reward%2520hacking%2520causes%2520obfuscation%2520of%2520the%2520undesired%2520reasoning%2520traces%2520but%2520the%2520persistence%2520of%2520the%2520undesired%2520behavior%252C%2520threatening%2520the%2520reliability%2520of%2520CoT%2520monitoring.%2520We%2520provide%2520an%2520extension%2520to%2520these%2520results%2520with%2520regard%2520to%2520the%2520ability%2520of%2520models%2520to%2520learn%2520a%2520specific%2520type%2520of%2520obfuscated%2520reasoning%253A%2520steganography.%2520First%252C%2520we%2520show%2520that%2520penalizing%2520the%2520use%2520of%2520specific%2520strings%2520within%2520load-bearing%2520reasoning%2520traces%2520causes%2520models%2520to%2520substitute%2520alternative%2520strings.%2520Crucially%252C%2520this%2520does%2520not%2520alter%2520the%2520underlying%2520method%2520by%2520which%2520the%2520model%2520performs%2520the%2520task%252C%2520demonstrating%2520that%2520the%2520model%2520can%2520learn%2520to%2520steganographically%2520encode%2520its%2520reasoning.We%2520further%2520demonstrate%2520that%2520models%2520can%2520generalize%2520an%2520encoding%2520scheme.%2520When%2520the%2520penalized%2520strings%2520belong%2520to%2520an%2520overarching%2520class%252C%2520the%2520model%2520learns%2520not%2520only%2520to%2520substitute%2520strings%2520seen%2520in%2520training%252C%2520but%2520also%2520develops%2520a%2520general%2520encoding%2520scheme%2520for%2520all%2520members%2520of%2520the%2520class%2520which%2520it%2520can%2520apply%2520to%2520held-out%2520testing%2520strings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20language%20models%20can%20learn%20and%20generalize%20steganographic%20chain-of-thought%20under%20process%20supervision&entry.906535625=Joey%20Skaf%20and%20Luis%20Ibanez-Lissen%20and%20Robert%20McCarthy%20and%20Connor%20Watts%20and%20Vasil%20Georgiv%20and%20Hannes%20Whittingham%20and%20Lorena%20Gonzalez-Manzano%20and%20David%20Lindner%20and%20Cameron%20Tice%20and%20Edward%20James%20Young%20and%20Puria%20Radmard&entry.1292438233=Chain-of-thought%20%28CoT%29%20reasoning%20not%20only%20enhances%20large%20language%20model%20performance%20but%20also%20provides%20critical%20insights%20into%20decision-making%20processes%2C%20marking%20it%20as%20a%20useful%20tool%20for%20monitoring%20model%20intent%20and%20planning.%20However%2C%20recent%20works%20have%20shown%20that%20banning%20the%20mention%20of%20a%20specific%20example%20of%20reward%20hacking%20causes%20obfuscation%20of%20the%20undesired%20reasoning%20traces%20but%20the%20persistence%20of%20the%20undesired%20behavior%2C%20threatening%20the%20reliability%20of%20CoT%20monitoring.%20We%20provide%20an%20extension%20to%20these%20results%20with%20regard%20to%20the%20ability%20of%20models%20to%20learn%20a%20specific%20type%20of%20obfuscated%20reasoning%3A%20steganography.%20First%2C%20we%20show%20that%20penalizing%20the%20use%20of%20specific%20strings%20within%20load-bearing%20reasoning%20traces%20causes%20models%20to%20substitute%20alternative%20strings.%20Crucially%2C%20this%20does%20not%20alter%20the%20underlying%20method%20by%20which%20the%20model%20performs%20the%20task%2C%20demonstrating%20that%20the%20model%20can%20learn%20to%20steganographically%20encode%20its%20reasoning.We%20further%20demonstrate%20that%20models%20can%20generalize%20an%20encoding%20scheme.%20When%20the%20penalized%20strings%20belong%20to%20an%20overarching%20class%2C%20the%20model%20learns%20not%20only%20to%20substitute%20strings%20seen%20in%20training%2C%20but%20also%20develops%20a%20general%20encoding%20scheme%20for%20all%20members%20of%20the%20class%20which%20it%20can%20apply%20to%20held-out%20testing%20strings.&entry.1838667208=http%3A//arxiv.org/abs/2506.01926v2&entry.124074799=Read"},
{"title": "Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild", "author": "Yigui Feng and Qinglin Wang and Haotian Mo and Yang Liu and Ke Liu and Gencheng Liu and Xinhai Chen and Siqi Shen and Songzhu Mei and Jie Liu", "abstract": "Generative psychological analysis of in-the-wild conversations faces two fundamental challenges: (1) existing Vision-Language Models (VLMs) fail to resolve Articulatory-Affective Ambiguity, where visual patterns of speech mimic emotional expressions; and (2) progress is stifled by a lack of verifiable evaluation metrics capable of assessing visual grounding and reasoning depth. We propose a complete ecosystem to address these twin challenges. First, we introduce Multilevel Insight Network for Disentanglement(MIND), a novel hierarchical visual encoder that introduces a Status Judgment module to algorithmically suppress ambiguous lip features based on their temporal feature variance, achieving explicit visual disentanglement. Second, we construct ConvoInsight-DB, a new large-scale dataset with expert annotations for micro-expressions and deep psychological inference. Third, Third, we designed the Mental Reasoning Insight Rating Metric (PRISM), an automated dimensional framework that uses expert-guided LLM to measure the multidimensional performance of large mental vision models. On our PRISM benchmark, MIND significantly outperforms all baselines, achieving a +86.95% gain in micro-expression detection over prior SOTA. Ablation studies confirm that our Status Judgment disentanglement module is the most critical component for this performance leap. Our code has been opened.", "link": "http://arxiv.org/abs/2512.04728v1", "date": "2025-12-04", "relevancy": 2.426, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6171}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20the%20Unspoken%3A%20A%20Disentanglement%20Model%20and%20Benchmark%20for%20Psychological%20Analysis%20in%20the%20Wild&body=Title%3A%20Measuring%20the%20Unspoken%3A%20A%20Disentanglement%20Model%20and%20Benchmark%20for%20Psychological%20Analysis%20in%20the%20Wild%0AAuthor%3A%20Yigui%20Feng%20and%20Qinglin%20Wang%20and%20Haotian%20Mo%20and%20Yang%20Liu%20and%20Ke%20Liu%20and%20Gencheng%20Liu%20and%20Xinhai%20Chen%20and%20Siqi%20Shen%20and%20Songzhu%20Mei%20and%20Jie%20Liu%0AAbstract%3A%20Generative%20psychological%20analysis%20of%20in-the-wild%20conversations%20faces%20two%20fundamental%20challenges%3A%20%281%29%20existing%20Vision-Language%20Models%20%28VLMs%29%20fail%20to%20resolve%20Articulatory-Affective%20Ambiguity%2C%20where%20visual%20patterns%20of%20speech%20mimic%20emotional%20expressions%3B%20and%20%282%29%20progress%20is%20stifled%20by%20a%20lack%20of%20verifiable%20evaluation%20metrics%20capable%20of%20assessing%20visual%20grounding%20and%20reasoning%20depth.%20We%20propose%20a%20complete%20ecosystem%20to%20address%20these%20twin%20challenges.%20First%2C%20we%20introduce%20Multilevel%20Insight%20Network%20for%20Disentanglement%28MIND%29%2C%20a%20novel%20hierarchical%20visual%20encoder%20that%20introduces%20a%20Status%20Judgment%20module%20to%20algorithmically%20suppress%20ambiguous%20lip%20features%20based%20on%20their%20temporal%20feature%20variance%2C%20achieving%20explicit%20visual%20disentanglement.%20Second%2C%20we%20construct%20ConvoInsight-DB%2C%20a%20new%20large-scale%20dataset%20with%20expert%20annotations%20for%20micro-expressions%20and%20deep%20psychological%20inference.%20Third%2C%20Third%2C%20we%20designed%20the%20Mental%20Reasoning%20Insight%20Rating%20Metric%20%28PRISM%29%2C%20an%20automated%20dimensional%20framework%20that%20uses%20expert-guided%20LLM%20to%20measure%20the%20multidimensional%20performance%20of%20large%20mental%20vision%20models.%20On%20our%20PRISM%20benchmark%2C%20MIND%20significantly%20outperforms%20all%20baselines%2C%20achieving%20a%20%2B86.95%25%20gain%20in%20micro-expression%20detection%20over%20prior%20SOTA.%20Ablation%20studies%20confirm%20that%20our%20Status%20Judgment%20disentanglement%20module%20is%20the%20most%20critical%20component%20for%20this%20performance%20leap.%20Our%20code%20has%20been%20opened.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520the%2520Unspoken%253A%2520A%2520Disentanglement%2520Model%2520and%2520Benchmark%2520for%2520Psychological%2520Analysis%2520in%2520the%2520Wild%26entry.906535625%3DYigui%2520Feng%2520and%2520Qinglin%2520Wang%2520and%2520Haotian%2520Mo%2520and%2520Yang%2520Liu%2520and%2520Ke%2520Liu%2520and%2520Gencheng%2520Liu%2520and%2520Xinhai%2520Chen%2520and%2520Siqi%2520Shen%2520and%2520Songzhu%2520Mei%2520and%2520Jie%2520Liu%26entry.1292438233%3DGenerative%2520psychological%2520analysis%2520of%2520in-the-wild%2520conversations%2520faces%2520two%2520fundamental%2520challenges%253A%2520%25281%2529%2520existing%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520fail%2520to%2520resolve%2520Articulatory-Affective%2520Ambiguity%252C%2520where%2520visual%2520patterns%2520of%2520speech%2520mimic%2520emotional%2520expressions%253B%2520and%2520%25282%2529%2520progress%2520is%2520stifled%2520by%2520a%2520lack%2520of%2520verifiable%2520evaluation%2520metrics%2520capable%2520of%2520assessing%2520visual%2520grounding%2520and%2520reasoning%2520depth.%2520We%2520propose%2520a%2520complete%2520ecosystem%2520to%2520address%2520these%2520twin%2520challenges.%2520First%252C%2520we%2520introduce%2520Multilevel%2520Insight%2520Network%2520for%2520Disentanglement%2528MIND%2529%252C%2520a%2520novel%2520hierarchical%2520visual%2520encoder%2520that%2520introduces%2520a%2520Status%2520Judgment%2520module%2520to%2520algorithmically%2520suppress%2520ambiguous%2520lip%2520features%2520based%2520on%2520their%2520temporal%2520feature%2520variance%252C%2520achieving%2520explicit%2520visual%2520disentanglement.%2520Second%252C%2520we%2520construct%2520ConvoInsight-DB%252C%2520a%2520new%2520large-scale%2520dataset%2520with%2520expert%2520annotations%2520for%2520micro-expressions%2520and%2520deep%2520psychological%2520inference.%2520Third%252C%2520Third%252C%2520we%2520designed%2520the%2520Mental%2520Reasoning%2520Insight%2520Rating%2520Metric%2520%2528PRISM%2529%252C%2520an%2520automated%2520dimensional%2520framework%2520that%2520uses%2520expert-guided%2520LLM%2520to%2520measure%2520the%2520multidimensional%2520performance%2520of%2520large%2520mental%2520vision%2520models.%2520On%2520our%2520PRISM%2520benchmark%252C%2520MIND%2520significantly%2520outperforms%2520all%2520baselines%252C%2520achieving%2520a%2520%252B86.95%2525%2520gain%2520in%2520micro-expression%2520detection%2520over%2520prior%2520SOTA.%2520Ablation%2520studies%2520confirm%2520that%2520our%2520Status%2520Judgment%2520disentanglement%2520module%2520is%2520the%2520most%2520critical%2520component%2520for%2520this%2520performance%2520leap.%2520Our%2520code%2520has%2520been%2520opened.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20the%20Unspoken%3A%20A%20Disentanglement%20Model%20and%20Benchmark%20for%20Psychological%20Analysis%20in%20the%20Wild&entry.906535625=Yigui%20Feng%20and%20Qinglin%20Wang%20and%20Haotian%20Mo%20and%20Yang%20Liu%20and%20Ke%20Liu%20and%20Gencheng%20Liu%20and%20Xinhai%20Chen%20and%20Siqi%20Shen%20and%20Songzhu%20Mei%20and%20Jie%20Liu&entry.1292438233=Generative%20psychological%20analysis%20of%20in-the-wild%20conversations%20faces%20two%20fundamental%20challenges%3A%20%281%29%20existing%20Vision-Language%20Models%20%28VLMs%29%20fail%20to%20resolve%20Articulatory-Affective%20Ambiguity%2C%20where%20visual%20patterns%20of%20speech%20mimic%20emotional%20expressions%3B%20and%20%282%29%20progress%20is%20stifled%20by%20a%20lack%20of%20verifiable%20evaluation%20metrics%20capable%20of%20assessing%20visual%20grounding%20and%20reasoning%20depth.%20We%20propose%20a%20complete%20ecosystem%20to%20address%20these%20twin%20challenges.%20First%2C%20we%20introduce%20Multilevel%20Insight%20Network%20for%20Disentanglement%28MIND%29%2C%20a%20novel%20hierarchical%20visual%20encoder%20that%20introduces%20a%20Status%20Judgment%20module%20to%20algorithmically%20suppress%20ambiguous%20lip%20features%20based%20on%20their%20temporal%20feature%20variance%2C%20achieving%20explicit%20visual%20disentanglement.%20Second%2C%20we%20construct%20ConvoInsight-DB%2C%20a%20new%20large-scale%20dataset%20with%20expert%20annotations%20for%20micro-expressions%20and%20deep%20psychological%20inference.%20Third%2C%20Third%2C%20we%20designed%20the%20Mental%20Reasoning%20Insight%20Rating%20Metric%20%28PRISM%29%2C%20an%20automated%20dimensional%20framework%20that%20uses%20expert-guided%20LLM%20to%20measure%20the%20multidimensional%20performance%20of%20large%20mental%20vision%20models.%20On%20our%20PRISM%20benchmark%2C%20MIND%20significantly%20outperforms%20all%20baselines%2C%20achieving%20a%20%2B86.95%25%20gain%20in%20micro-expression%20detection%20over%20prior%20SOTA.%20Ablation%20studies%20confirm%20that%20our%20Status%20Judgment%20disentanglement%20module%20is%20the%20most%20critical%20component%20for%20this%20performance%20leap.%20Our%20code%20has%20been%20opened.&entry.1838667208=http%3A//arxiv.org/abs/2512.04728v1&entry.124074799=Read"},
{"title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation", "author": "Xiaochuang Han and Youssef Emad and Melissa Hall and John Nguyen and Karthik Padthe and Liam Robbins and Amir Bar and Delong Chen and Michal Drozdzal and Maha Elbayad and Yushi Hu and Shang-Wen Li and Sreya Dutta Roy and Jakob Verbeek and XuDong Wang and Marjan Ghazvininejad and Luke Zettlemoyer and Emily Dinan", "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.", "link": "http://arxiv.org/abs/2512.05103v1", "date": "2025-12-04", "relevancy": 2.4189, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6147}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6012}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TV2TV%3A%20A%20Unified%20Framework%20for%20Interleaved%20Language%20and%20Video%20Generation&body=Title%3A%20TV2TV%3A%20A%20Unified%20Framework%20for%20Interleaved%20Language%20and%20Video%20Generation%0AAuthor%3A%20Xiaochuang%20Han%20and%20Youssef%20Emad%20and%20Melissa%20Hall%20and%20John%20Nguyen%20and%20Karthik%20Padthe%20and%20Liam%20Robbins%20and%20Amir%20Bar%20and%20Delong%20Chen%20and%20Michal%20Drozdzal%20and%20Maha%20Elbayad%20and%20Yushi%20Hu%20and%20Shang-Wen%20Li%20and%20Sreya%20Dutta%20Roy%20and%20Jakob%20Verbeek%20and%20XuDong%20Wang%20and%20Marjan%20Ghazvininejad%20and%20Luke%20Zettlemoyer%20and%20Emily%20Dinan%0AAbstract%3A%20Video%20generation%20models%20are%20rapidly%20advancing%2C%20but%20can%20still%20struggle%20with%20complex%20video%20outputs%20that%20require%20significant%20semantic%20branching%20or%20repeated%20high-level%20reasoning%20about%20what%20should%20happen%20next.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20class%20of%20omni%20video-text%20models%20that%20integrate%20ideas%20from%20recent%20LM%20reasoning%20advances%20to%20address%20this%20challenge.%20More%20specifically%2C%20we%20present%20TV2TV%2C%20a%20unified%20generative%20modeling%20framework%20which%20decomposes%20video%20generation%20into%20an%20interleaved%20text%20and%20video%20generation%20process.%20TV2TV%20jointly%20learns%20language%20modeling%20%28next-token%20prediction%29%20and%20video%20flow%20matching%20%28next-frame%20prediction%29%20using%20a%20Mixture-of-Transformers%20%28MoT%29%20architecture.%20At%20inference%20time%2C%20TV2TV%20decides%20when%20to%20alternate%20between%20generating%20text%20and%20video%20frames%2C%20allowing%20the%20model%20to%20%22think%20in%20words%22%20about%20subsequent%20content%20before%20%60%60acting%20in%20pixels%27%27%20to%20produce%20frames.%20This%20design%20offloads%20much%20of%20the%20responsibility%20for%20deciding%20what%20should%20happen%20next%20to%20the%20language%20modeling%20tower%2C%20enabling%20improved%20visual%20quality%20and%20prompt%20alignment%20of%20generated%20videos.%20It%20also%20enables%20fine-grained%20controllability%2C%20allowing%20users%20to%20modify%20the%20video%20generation%20trajectory%20through%20text%20interventions%20at%20any%20point%20in%20the%20process.%20In%20controlled%20experiments%20on%20video%20game%20data%2C%20TV2TV%20demonstrates%20substantial%20improvements%20in%20both%20visual%20quality%20and%20controllability.%20TV2TV%20also%20scales%20to%20natural%20videos%2C%20as%20we%20show%20by%20augmenting%20sports%20videos%20with%20interleaved%20natural%20language%20action%20descriptions%20using%20vision-language%20models%20%28VLMs%29.%20Training%20TV2TV%20on%20this%20corpus%20yields%20strong%20visual%20quality%20and%20prompt%20alignment%2C%20showcasing%20the%20model%27s%20ability%20to%20reason%20about%20and%20generate%20complex%20real-world%20action%20sequences.%20Together%2C%20these%20results%20highlight%20TV2TV%20as%20a%20promising%20step%20toward%20video%20generation%20with%20open-ended%20textual%20reasoning%20and%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTV2TV%253A%2520A%2520Unified%2520Framework%2520for%2520Interleaved%2520Language%2520and%2520Video%2520Generation%26entry.906535625%3DXiaochuang%2520Han%2520and%2520Youssef%2520Emad%2520and%2520Melissa%2520Hall%2520and%2520John%2520Nguyen%2520and%2520Karthik%2520Padthe%2520and%2520Liam%2520Robbins%2520and%2520Amir%2520Bar%2520and%2520Delong%2520Chen%2520and%2520Michal%2520Drozdzal%2520and%2520Maha%2520Elbayad%2520and%2520Yushi%2520Hu%2520and%2520Shang-Wen%2520Li%2520and%2520Sreya%2520Dutta%2520Roy%2520and%2520Jakob%2520Verbeek%2520and%2520XuDong%2520Wang%2520and%2520Marjan%2520Ghazvininejad%2520and%2520Luke%2520Zettlemoyer%2520and%2520Emily%2520Dinan%26entry.1292438233%3DVideo%2520generation%2520models%2520are%2520rapidly%2520advancing%252C%2520but%2520can%2520still%2520struggle%2520with%2520complex%2520video%2520outputs%2520that%2520require%2520significant%2520semantic%2520branching%2520or%2520repeated%2520high-level%2520reasoning%2520about%2520what%2520should%2520happen%2520next.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520class%2520of%2520omni%2520video-text%2520models%2520that%2520integrate%2520ideas%2520from%2520recent%2520LM%2520reasoning%2520advances%2520to%2520address%2520this%2520challenge.%2520More%2520specifically%252C%2520we%2520present%2520TV2TV%252C%2520a%2520unified%2520generative%2520modeling%2520framework%2520which%2520decomposes%2520video%2520generation%2520into%2520an%2520interleaved%2520text%2520and%2520video%2520generation%2520process.%2520TV2TV%2520jointly%2520learns%2520language%2520modeling%2520%2528next-token%2520prediction%2529%2520and%2520video%2520flow%2520matching%2520%2528next-frame%2520prediction%2529%2520using%2520a%2520Mixture-of-Transformers%2520%2528MoT%2529%2520architecture.%2520At%2520inference%2520time%252C%2520TV2TV%2520decides%2520when%2520to%2520alternate%2520between%2520generating%2520text%2520and%2520video%2520frames%252C%2520allowing%2520the%2520model%2520to%2520%2522think%2520in%2520words%2522%2520about%2520subsequent%2520content%2520before%2520%2560%2560acting%2520in%2520pixels%2527%2527%2520to%2520produce%2520frames.%2520This%2520design%2520offloads%2520much%2520of%2520the%2520responsibility%2520for%2520deciding%2520what%2520should%2520happen%2520next%2520to%2520the%2520language%2520modeling%2520tower%252C%2520enabling%2520improved%2520visual%2520quality%2520and%2520prompt%2520alignment%2520of%2520generated%2520videos.%2520It%2520also%2520enables%2520fine-grained%2520controllability%252C%2520allowing%2520users%2520to%2520modify%2520the%2520video%2520generation%2520trajectory%2520through%2520text%2520interventions%2520at%2520any%2520point%2520in%2520the%2520process.%2520In%2520controlled%2520experiments%2520on%2520video%2520game%2520data%252C%2520TV2TV%2520demonstrates%2520substantial%2520improvements%2520in%2520both%2520visual%2520quality%2520and%2520controllability.%2520TV2TV%2520also%2520scales%2520to%2520natural%2520videos%252C%2520as%2520we%2520show%2520by%2520augmenting%2520sports%2520videos%2520with%2520interleaved%2520natural%2520language%2520action%2520descriptions%2520using%2520vision-language%2520models%2520%2528VLMs%2529.%2520Training%2520TV2TV%2520on%2520this%2520corpus%2520yields%2520strong%2520visual%2520quality%2520and%2520prompt%2520alignment%252C%2520showcasing%2520the%2520model%2527s%2520ability%2520to%2520reason%2520about%2520and%2520generate%2520complex%2520real-world%2520action%2520sequences.%2520Together%252C%2520these%2520results%2520highlight%2520TV2TV%2520as%2520a%2520promising%2520step%2520toward%2520video%2520generation%2520with%2520open-ended%2520textual%2520reasoning%2520and%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TV2TV%3A%20A%20Unified%20Framework%20for%20Interleaved%20Language%20and%20Video%20Generation&entry.906535625=Xiaochuang%20Han%20and%20Youssef%20Emad%20and%20Melissa%20Hall%20and%20John%20Nguyen%20and%20Karthik%20Padthe%20and%20Liam%20Robbins%20and%20Amir%20Bar%20and%20Delong%20Chen%20and%20Michal%20Drozdzal%20and%20Maha%20Elbayad%20and%20Yushi%20Hu%20and%20Shang-Wen%20Li%20and%20Sreya%20Dutta%20Roy%20and%20Jakob%20Verbeek%20and%20XuDong%20Wang%20and%20Marjan%20Ghazvininejad%20and%20Luke%20Zettlemoyer%20and%20Emily%20Dinan&entry.1292438233=Video%20generation%20models%20are%20rapidly%20advancing%2C%20but%20can%20still%20struggle%20with%20complex%20video%20outputs%20that%20require%20significant%20semantic%20branching%20or%20repeated%20high-level%20reasoning%20about%20what%20should%20happen%20next.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20class%20of%20omni%20video-text%20models%20that%20integrate%20ideas%20from%20recent%20LM%20reasoning%20advances%20to%20address%20this%20challenge.%20More%20specifically%2C%20we%20present%20TV2TV%2C%20a%20unified%20generative%20modeling%20framework%20which%20decomposes%20video%20generation%20into%20an%20interleaved%20text%20and%20video%20generation%20process.%20TV2TV%20jointly%20learns%20language%20modeling%20%28next-token%20prediction%29%20and%20video%20flow%20matching%20%28next-frame%20prediction%29%20using%20a%20Mixture-of-Transformers%20%28MoT%29%20architecture.%20At%20inference%20time%2C%20TV2TV%20decides%20when%20to%20alternate%20between%20generating%20text%20and%20video%20frames%2C%20allowing%20the%20model%20to%20%22think%20in%20words%22%20about%20subsequent%20content%20before%20%60%60acting%20in%20pixels%27%27%20to%20produce%20frames.%20This%20design%20offloads%20much%20of%20the%20responsibility%20for%20deciding%20what%20should%20happen%20next%20to%20the%20language%20modeling%20tower%2C%20enabling%20improved%20visual%20quality%20and%20prompt%20alignment%20of%20generated%20videos.%20It%20also%20enables%20fine-grained%20controllability%2C%20allowing%20users%20to%20modify%20the%20video%20generation%20trajectory%20through%20text%20interventions%20at%20any%20point%20in%20the%20process.%20In%20controlled%20experiments%20on%20video%20game%20data%2C%20TV2TV%20demonstrates%20substantial%20improvements%20in%20both%20visual%20quality%20and%20controllability.%20TV2TV%20also%20scales%20to%20natural%20videos%2C%20as%20we%20show%20by%20augmenting%20sports%20videos%20with%20interleaved%20natural%20language%20action%20descriptions%20using%20vision-language%20models%20%28VLMs%29.%20Training%20TV2TV%20on%20this%20corpus%20yields%20strong%20visual%20quality%20and%20prompt%20alignment%2C%20showcasing%20the%20model%27s%20ability%20to%20reason%20about%20and%20generate%20complex%20real-world%20action%20sequences.%20Together%2C%20these%20results%20highlight%20TV2TV%20as%20a%20promising%20step%20toward%20video%20generation%20with%20open-ended%20textual%20reasoning%20and%20control.&entry.1838667208=http%3A//arxiv.org/abs/2512.05103v1&entry.124074799=Read"},
{"title": "Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection", "author": "Mohammad Arif Rasyidi and Omar Alhussein and Sami Muhaidat and Ernesto Damiani", "abstract": "Unsupervised anomaly-based intrusion detection requires models that can generalize to attack patterns not observed during training. This work presents the first large-scale evaluation of hybrid quantum-classical (HQC) autoencoders for this task. We construct a unified experimental framework that iterates over key quantum design choices, including quantum-layer placement, measurement approach, variational and non-variational formulations, and latent-space regularization. Experiments across three benchmark NIDS datasets show that HQC autoencoders can match or exceed classical performance in their best configurations, although they exhibit higher sensitivity to architectural decisions. Under zero-day evaluation, well-configured HQC models provide stronger and more stable generalization than classical and supervised baselines. Simulated gate-noise experiments reveal early performance degradation, indicating the need for noise-aware HQC designs. These results provide the first data-driven characterization of HQC autoencoder behavior for network intrusion detection and outline key factors that govern their practical viability. All experiment code and configurations are available at https://github.com/arasyi/hqcae-network-intrusion-detection.", "link": "http://arxiv.org/abs/2512.05069v1", "date": "2025-12-04", "relevancy": 2.4174, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5142}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4742}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Quantum-Classical%20Autoencoders%20for%20Unsupervised%20Network%20Intrusion%20Detection&body=Title%3A%20Hybrid%20Quantum-Classical%20Autoencoders%20for%20Unsupervised%20Network%20Intrusion%20Detection%0AAuthor%3A%20Mohammad%20Arif%20Rasyidi%20and%20Omar%20Alhussein%20and%20Sami%20Muhaidat%20and%20Ernesto%20Damiani%0AAbstract%3A%20Unsupervised%20anomaly-based%20intrusion%20detection%20requires%20models%20that%20can%20generalize%20to%20attack%20patterns%20not%20observed%20during%20training.%20This%20work%20presents%20the%20first%20large-scale%20evaluation%20of%20hybrid%20quantum-classical%20%28HQC%29%20autoencoders%20for%20this%20task.%20We%20construct%20a%20unified%20experimental%20framework%20that%20iterates%20over%20key%20quantum%20design%20choices%2C%20including%20quantum-layer%20placement%2C%20measurement%20approach%2C%20variational%20and%20non-variational%20formulations%2C%20and%20latent-space%20regularization.%20Experiments%20across%20three%20benchmark%20NIDS%20datasets%20show%20that%20HQC%20autoencoders%20can%20match%20or%20exceed%20classical%20performance%20in%20their%20best%20configurations%2C%20although%20they%20exhibit%20higher%20sensitivity%20to%20architectural%20decisions.%20Under%20zero-day%20evaluation%2C%20well-configured%20HQC%20models%20provide%20stronger%20and%20more%20stable%20generalization%20than%20classical%20and%20supervised%20baselines.%20Simulated%20gate-noise%20experiments%20reveal%20early%20performance%20degradation%2C%20indicating%20the%20need%20for%20noise-aware%20HQC%20designs.%20These%20results%20provide%20the%20first%20data-driven%20characterization%20of%20HQC%20autoencoder%20behavior%20for%20network%20intrusion%20detection%20and%20outline%20key%20factors%20that%20govern%20their%20practical%20viability.%20All%20experiment%20code%20and%20configurations%20are%20available%20at%20https%3A//github.com/arasyi/hqcae-network-intrusion-detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Quantum-Classical%2520Autoencoders%2520for%2520Unsupervised%2520Network%2520Intrusion%2520Detection%26entry.906535625%3DMohammad%2520Arif%2520Rasyidi%2520and%2520Omar%2520Alhussein%2520and%2520Sami%2520Muhaidat%2520and%2520Ernesto%2520Damiani%26entry.1292438233%3DUnsupervised%2520anomaly-based%2520intrusion%2520detection%2520requires%2520models%2520that%2520can%2520generalize%2520to%2520attack%2520patterns%2520not%2520observed%2520during%2520training.%2520This%2520work%2520presents%2520the%2520first%2520large-scale%2520evaluation%2520of%2520hybrid%2520quantum-classical%2520%2528HQC%2529%2520autoencoders%2520for%2520this%2520task.%2520We%2520construct%2520a%2520unified%2520experimental%2520framework%2520that%2520iterates%2520over%2520key%2520quantum%2520design%2520choices%252C%2520including%2520quantum-layer%2520placement%252C%2520measurement%2520approach%252C%2520variational%2520and%2520non-variational%2520formulations%252C%2520and%2520latent-space%2520regularization.%2520Experiments%2520across%2520three%2520benchmark%2520NIDS%2520datasets%2520show%2520that%2520HQC%2520autoencoders%2520can%2520match%2520or%2520exceed%2520classical%2520performance%2520in%2520their%2520best%2520configurations%252C%2520although%2520they%2520exhibit%2520higher%2520sensitivity%2520to%2520architectural%2520decisions.%2520Under%2520zero-day%2520evaluation%252C%2520well-configured%2520HQC%2520models%2520provide%2520stronger%2520and%2520more%2520stable%2520generalization%2520than%2520classical%2520and%2520supervised%2520baselines.%2520Simulated%2520gate-noise%2520experiments%2520reveal%2520early%2520performance%2520degradation%252C%2520indicating%2520the%2520need%2520for%2520noise-aware%2520HQC%2520designs.%2520These%2520results%2520provide%2520the%2520first%2520data-driven%2520characterization%2520of%2520HQC%2520autoencoder%2520behavior%2520for%2520network%2520intrusion%2520detection%2520and%2520outline%2520key%2520factors%2520that%2520govern%2520their%2520practical%2520viability.%2520All%2520experiment%2520code%2520and%2520configurations%2520are%2520available%2520at%2520https%253A//github.com/arasyi/hqcae-network-intrusion-detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Quantum-Classical%20Autoencoders%20for%20Unsupervised%20Network%20Intrusion%20Detection&entry.906535625=Mohammad%20Arif%20Rasyidi%20and%20Omar%20Alhussein%20and%20Sami%20Muhaidat%20and%20Ernesto%20Damiani&entry.1292438233=Unsupervised%20anomaly-based%20intrusion%20detection%20requires%20models%20that%20can%20generalize%20to%20attack%20patterns%20not%20observed%20during%20training.%20This%20work%20presents%20the%20first%20large-scale%20evaluation%20of%20hybrid%20quantum-classical%20%28HQC%29%20autoencoders%20for%20this%20task.%20We%20construct%20a%20unified%20experimental%20framework%20that%20iterates%20over%20key%20quantum%20design%20choices%2C%20including%20quantum-layer%20placement%2C%20measurement%20approach%2C%20variational%20and%20non-variational%20formulations%2C%20and%20latent-space%20regularization.%20Experiments%20across%20three%20benchmark%20NIDS%20datasets%20show%20that%20HQC%20autoencoders%20can%20match%20or%20exceed%20classical%20performance%20in%20their%20best%20configurations%2C%20although%20they%20exhibit%20higher%20sensitivity%20to%20architectural%20decisions.%20Under%20zero-day%20evaluation%2C%20well-configured%20HQC%20models%20provide%20stronger%20and%20more%20stable%20generalization%20than%20classical%20and%20supervised%20baselines.%20Simulated%20gate-noise%20experiments%20reveal%20early%20performance%20degradation%2C%20indicating%20the%20need%20for%20noise-aware%20HQC%20designs.%20These%20results%20provide%20the%20first%20data-driven%20characterization%20of%20HQC%20autoencoder%20behavior%20for%20network%20intrusion%20detection%20and%20outline%20key%20factors%20that%20govern%20their%20practical%20viability.%20All%20experiment%20code%20and%20configurations%20are%20available%20at%20https%3A//github.com/arasyi/hqcae-network-intrusion-detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.05069v1&entry.124074799=Read"},
{"title": "Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints", "author": "Minghan Zhu and Zhiyi Wang and Qihang Sun and Maani Ghaffari and Michael Posa", "abstract": "Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.", "link": "http://arxiv.org/abs/2512.05079v1", "date": "2025-12-04", "relevancy": 2.4124, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6511}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6076}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Reconstruction%20under%20Occlusion%20with%20Generative%20Priors%20and%20Contact-induced%20Constraints&body=Title%3A%20Object%20Reconstruction%20under%20Occlusion%20with%20Generative%20Priors%20and%20Contact-induced%20Constraints%0AAuthor%3A%20Minghan%20Zhu%20and%20Zhiyi%20Wang%20and%20Qihang%20Sun%20and%20Maani%20Ghaffari%20and%20Michael%20Posa%0AAbstract%3A%20Object%20geometry%20is%20key%20information%20for%20robot%20manipulation.%20Yet%2C%20object%20reconstruction%20is%20a%20challenging%20task%20because%20cameras%20only%20capture%20partial%20observations%20of%20objects%2C%20especially%20when%20occlusion%20occurs.%20In%20this%20paper%2C%20we%20leverage%20two%20extra%20sources%20of%20information%20to%20reduce%20the%20ambiguity%20of%20vision%20signals.%20First%2C%20generative%20models%20learn%20priors%20of%20the%20shapes%20of%20commonly%20seen%20objects%2C%20allowing%20us%20to%20make%20reasonable%20guesses%20of%20the%20unseen%20part%20of%20geometry.%20Second%2C%20contact%20information%2C%20which%20can%20be%20obtained%20from%20videos%20and%20physical%20interactions%2C%20provides%20sparse%20constraints%20on%20the%20boundary%20of%20the%20geometry.%20We%20combine%20the%20two%20sources%20of%20information%20through%20contact-guided%203D%20generation.%20The%20guidance%20formulation%20is%20inspired%20by%20drag-based%20editing%20in%20generative%20models.%20Experiments%20on%20synthetic%20and%20real-world%20data%20show%20that%20our%20approach%20improves%20the%20reconstruction%20compared%20to%20pure%203D%20generation%20and%20contact-based%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Reconstruction%2520under%2520Occlusion%2520with%2520Generative%2520Priors%2520and%2520Contact-induced%2520Constraints%26entry.906535625%3DMinghan%2520Zhu%2520and%2520Zhiyi%2520Wang%2520and%2520Qihang%2520Sun%2520and%2520Maani%2520Ghaffari%2520and%2520Michael%2520Posa%26entry.1292438233%3DObject%2520geometry%2520is%2520key%2520information%2520for%2520robot%2520manipulation.%2520Yet%252C%2520object%2520reconstruction%2520is%2520a%2520challenging%2520task%2520because%2520cameras%2520only%2520capture%2520partial%2520observations%2520of%2520objects%252C%2520especially%2520when%2520occlusion%2520occurs.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520two%2520extra%2520sources%2520of%2520information%2520to%2520reduce%2520the%2520ambiguity%2520of%2520vision%2520signals.%2520First%252C%2520generative%2520models%2520learn%2520priors%2520of%2520the%2520shapes%2520of%2520commonly%2520seen%2520objects%252C%2520allowing%2520us%2520to%2520make%2520reasonable%2520guesses%2520of%2520the%2520unseen%2520part%2520of%2520geometry.%2520Second%252C%2520contact%2520information%252C%2520which%2520can%2520be%2520obtained%2520from%2520videos%2520and%2520physical%2520interactions%252C%2520provides%2520sparse%2520constraints%2520on%2520the%2520boundary%2520of%2520the%2520geometry.%2520We%2520combine%2520the%2520two%2520sources%2520of%2520information%2520through%2520contact-guided%25203D%2520generation.%2520The%2520guidance%2520formulation%2520is%2520inspired%2520by%2520drag-based%2520editing%2520in%2520generative%2520models.%2520Experiments%2520on%2520synthetic%2520and%2520real-world%2520data%2520show%2520that%2520our%2520approach%2520improves%2520the%2520reconstruction%2520compared%2520to%2520pure%25203D%2520generation%2520and%2520contact-based%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Reconstruction%20under%20Occlusion%20with%20Generative%20Priors%20and%20Contact-induced%20Constraints&entry.906535625=Minghan%20Zhu%20and%20Zhiyi%20Wang%20and%20Qihang%20Sun%20and%20Maani%20Ghaffari%20and%20Michael%20Posa&entry.1292438233=Object%20geometry%20is%20key%20information%20for%20robot%20manipulation.%20Yet%2C%20object%20reconstruction%20is%20a%20challenging%20task%20because%20cameras%20only%20capture%20partial%20observations%20of%20objects%2C%20especially%20when%20occlusion%20occurs.%20In%20this%20paper%2C%20we%20leverage%20two%20extra%20sources%20of%20information%20to%20reduce%20the%20ambiguity%20of%20vision%20signals.%20First%2C%20generative%20models%20learn%20priors%20of%20the%20shapes%20of%20commonly%20seen%20objects%2C%20allowing%20us%20to%20make%20reasonable%20guesses%20of%20the%20unseen%20part%20of%20geometry.%20Second%2C%20contact%20information%2C%20which%20can%20be%20obtained%20from%20videos%20and%20physical%20interactions%2C%20provides%20sparse%20constraints%20on%20the%20boundary%20of%20the%20geometry.%20We%20combine%20the%20two%20sources%20of%20information%20through%20contact-guided%203D%20generation.%20The%20guidance%20formulation%20is%20inspired%20by%20drag-based%20editing%20in%20generative%20models.%20Experiments%20on%20synthetic%20and%20real-world%20data%20show%20that%20our%20approach%20improves%20the%20reconstruction%20compared%20to%20pure%203D%20generation%20and%20contact-based%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2512.05079v1&entry.124074799=Read"},
{"title": "Triangle Multiplication Is All You Need For Biomolecular Structure Representations", "author": "Jeffrey Ouyang-Zhang and Pranav Murugan and Daniel J. Diaz and Gianluca Scarpellini and Richard Strong Bowen and Nate Gruver and Adam Klivans and Philipp Kr\u00e4henb\u00fchl and Aleksandra Faust and Maruan Al-Shedivat", "abstract": "AlphaFold has transformed protein structure prediction, but emerging applications such as virtual ligand screening, proteome-wide folding, and de novo binder design demand predictions at a massive scale, where runtime and memory costs become prohibitive. A major bottleneck lies in the Pairformer backbone of AlphaFold3-style models, which relies on computationally expensive triangular primitives-especially triangle attention-for pairwise reasoning. We introduce Pairmixer, a streamlined alternative that eliminates triangle attention while preserving higher-order geometric reasoning capabilities that are critical for structure prediction. Pairmixer substantially improves computational efficiency, matching state-of-the-art structure predictors across folding and docking benchmarks, delivering up to 4x faster inference on long sequences while reducing training cost by 34%. Its efficiency alleviates the computational burden of downstream applications such as modeling large protein complexes, high-throughput ligand and binder screening, and hallucination-based design. Within BoltzDesign, for example, Pairmixer delivers over 2x faster sampling and scales to sequences ~30% longer than the memory limits of Pairformer. Code is available at https://github.com/genesistherapeutics/pairmixer.", "link": "http://arxiv.org/abs/2510.18870v2", "date": "2025-12-04", "relevancy": 2.3881, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4808}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.476}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Triangle%20Multiplication%20Is%20All%20You%20Need%20For%20Biomolecular%20Structure%20Representations&body=Title%3A%20Triangle%20Multiplication%20Is%20All%20You%20Need%20For%20Biomolecular%20Structure%20Representations%0AAuthor%3A%20Jeffrey%20Ouyang-Zhang%20and%20Pranav%20Murugan%20and%20Daniel%20J.%20Diaz%20and%20Gianluca%20Scarpellini%20and%20Richard%20Strong%20Bowen%20and%20Nate%20Gruver%20and%20Adam%20Klivans%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Aleksandra%20Faust%20and%20Maruan%20Al-Shedivat%0AAbstract%3A%20AlphaFold%20has%20transformed%20protein%20structure%20prediction%2C%20but%20emerging%20applications%20such%20as%20virtual%20ligand%20screening%2C%20proteome-wide%20folding%2C%20and%20de%20novo%20binder%20design%20demand%20predictions%20at%20a%20massive%20scale%2C%20where%20runtime%20and%20memory%20costs%20become%20prohibitive.%20A%20major%20bottleneck%20lies%20in%20the%20Pairformer%20backbone%20of%20AlphaFold3-style%20models%2C%20which%20relies%20on%20computationally%20expensive%20triangular%20primitives-especially%20triangle%20attention-for%20pairwise%20reasoning.%20We%20introduce%20Pairmixer%2C%20a%20streamlined%20alternative%20that%20eliminates%20triangle%20attention%20while%20preserving%20higher-order%20geometric%20reasoning%20capabilities%20that%20are%20critical%20for%20structure%20prediction.%20Pairmixer%20substantially%20improves%20computational%20efficiency%2C%20matching%20state-of-the-art%20structure%20predictors%20across%20folding%20and%20docking%20benchmarks%2C%20delivering%20up%20to%204x%20faster%20inference%20on%20long%20sequences%20while%20reducing%20training%20cost%20by%2034%25.%20Its%20efficiency%20alleviates%20the%20computational%20burden%20of%20downstream%20applications%20such%20as%20modeling%20large%20protein%20complexes%2C%20high-throughput%20ligand%20and%20binder%20screening%2C%20and%20hallucination-based%20design.%20Within%20BoltzDesign%2C%20for%20example%2C%20Pairmixer%20delivers%20over%202x%20faster%20sampling%20and%20scales%20to%20sequences%20~30%25%20longer%20than%20the%20memory%20limits%20of%20Pairformer.%20Code%20is%20available%20at%20https%3A//github.com/genesistherapeutics/pairmixer.%0ALink%3A%20http%3A//arxiv.org/abs/2510.18870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriangle%2520Multiplication%2520Is%2520All%2520You%2520Need%2520For%2520Biomolecular%2520Structure%2520Representations%26entry.906535625%3DJeffrey%2520Ouyang-Zhang%2520and%2520Pranav%2520Murugan%2520and%2520Daniel%2520J.%2520Diaz%2520and%2520Gianluca%2520Scarpellini%2520and%2520Richard%2520Strong%2520Bowen%2520and%2520Nate%2520Gruver%2520and%2520Adam%2520Klivans%2520and%2520Philipp%2520Kr%25C3%25A4henb%25C3%25BChl%2520and%2520Aleksandra%2520Faust%2520and%2520Maruan%2520Al-Shedivat%26entry.1292438233%3DAlphaFold%2520has%2520transformed%2520protein%2520structure%2520prediction%252C%2520but%2520emerging%2520applications%2520such%2520as%2520virtual%2520ligand%2520screening%252C%2520proteome-wide%2520folding%252C%2520and%2520de%2520novo%2520binder%2520design%2520demand%2520predictions%2520at%2520a%2520massive%2520scale%252C%2520where%2520runtime%2520and%2520memory%2520costs%2520become%2520prohibitive.%2520A%2520major%2520bottleneck%2520lies%2520in%2520the%2520Pairformer%2520backbone%2520of%2520AlphaFold3-style%2520models%252C%2520which%2520relies%2520on%2520computationally%2520expensive%2520triangular%2520primitives-especially%2520triangle%2520attention-for%2520pairwise%2520reasoning.%2520We%2520introduce%2520Pairmixer%252C%2520a%2520streamlined%2520alternative%2520that%2520eliminates%2520triangle%2520attention%2520while%2520preserving%2520higher-order%2520geometric%2520reasoning%2520capabilities%2520that%2520are%2520critical%2520for%2520structure%2520prediction.%2520Pairmixer%2520substantially%2520improves%2520computational%2520efficiency%252C%2520matching%2520state-of-the-art%2520structure%2520predictors%2520across%2520folding%2520and%2520docking%2520benchmarks%252C%2520delivering%2520up%2520to%25204x%2520faster%2520inference%2520on%2520long%2520sequences%2520while%2520reducing%2520training%2520cost%2520by%252034%2525.%2520Its%2520efficiency%2520alleviates%2520the%2520computational%2520burden%2520of%2520downstream%2520applications%2520such%2520as%2520modeling%2520large%2520protein%2520complexes%252C%2520high-throughput%2520ligand%2520and%2520binder%2520screening%252C%2520and%2520hallucination-based%2520design.%2520Within%2520BoltzDesign%252C%2520for%2520example%252C%2520Pairmixer%2520delivers%2520over%25202x%2520faster%2520sampling%2520and%2520scales%2520to%2520sequences%2520~30%2525%2520longer%2520than%2520the%2520memory%2520limits%2520of%2520Pairformer.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/genesistherapeutics/pairmixer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Triangle%20Multiplication%20Is%20All%20You%20Need%20For%20Biomolecular%20Structure%20Representations&entry.906535625=Jeffrey%20Ouyang-Zhang%20and%20Pranav%20Murugan%20and%20Daniel%20J.%20Diaz%20and%20Gianluca%20Scarpellini%20and%20Richard%20Strong%20Bowen%20and%20Nate%20Gruver%20and%20Adam%20Klivans%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Aleksandra%20Faust%20and%20Maruan%20Al-Shedivat&entry.1292438233=AlphaFold%20has%20transformed%20protein%20structure%20prediction%2C%20but%20emerging%20applications%20such%20as%20virtual%20ligand%20screening%2C%20proteome-wide%20folding%2C%20and%20de%20novo%20binder%20design%20demand%20predictions%20at%20a%20massive%20scale%2C%20where%20runtime%20and%20memory%20costs%20become%20prohibitive.%20A%20major%20bottleneck%20lies%20in%20the%20Pairformer%20backbone%20of%20AlphaFold3-style%20models%2C%20which%20relies%20on%20computationally%20expensive%20triangular%20primitives-especially%20triangle%20attention-for%20pairwise%20reasoning.%20We%20introduce%20Pairmixer%2C%20a%20streamlined%20alternative%20that%20eliminates%20triangle%20attention%20while%20preserving%20higher-order%20geometric%20reasoning%20capabilities%20that%20are%20critical%20for%20structure%20prediction.%20Pairmixer%20substantially%20improves%20computational%20efficiency%2C%20matching%20state-of-the-art%20structure%20predictors%20across%20folding%20and%20docking%20benchmarks%2C%20delivering%20up%20to%204x%20faster%20inference%20on%20long%20sequences%20while%20reducing%20training%20cost%20by%2034%25.%20Its%20efficiency%20alleviates%20the%20computational%20burden%20of%20downstream%20applications%20such%20as%20modeling%20large%20protein%20complexes%2C%20high-throughput%20ligand%20and%20binder%20screening%2C%20and%20hallucination-based%20design.%20Within%20BoltzDesign%2C%20for%20example%2C%20Pairmixer%20delivers%20over%202x%20faster%20sampling%20and%20scales%20to%20sequences%20~30%25%20longer%20than%20the%20memory%20limits%20of%20Pairformer.%20Code%20is%20available%20at%20https%3A//github.com/genesistherapeutics/pairmixer.&entry.1838667208=http%3A//arxiv.org/abs/2510.18870v2&entry.124074799=Read"},
{"title": "Virtually Unrolling the Herculaneum Papyri by Diffeomorphic Spiral Fitting", "author": "Paul Henderson", "abstract": "The Herculaneum Papyri are a collection of rolled papyrus documents that were charred and buried by the famous eruption of Mount Vesuvius. They promise to contain a wealth of previously unseen Greek and Latin texts, but are extremely fragile and thus most cannot be unrolled physically. A solution to access these texts is virtual unrolling, where the papyrus surface is digitally traced out in a CT scan of the scroll, to create a flattened representation. This tracing is very laborious to do manually in gigavoxel-sized scans, so automated approaches are desirable. We present the first top-down method that automatically fits a surface model to a CT scan of a severely damaged scroll. We take a novel approach that globally fits an explicit parametric model of the deformed scroll to existing neural network predictions of where the rolled papyrus likely passes. Our method guarantees the resulting surface is a single continuous 2D sheet, even passing through regions where the surface is not detectable in the CT scan. We conduct comprehensive experiments on high-resolution CT scans of two scrolls, showing that our approach successfully unrolls large regions, and exceeds the performance of the only existing automated unrolling method suitable for this data.", "link": "http://arxiv.org/abs/2512.04927v1", "date": "2025-12-04", "relevancy": 2.3836, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4774}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4774}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virtually%20Unrolling%20the%20Herculaneum%20Papyri%20by%20Diffeomorphic%20Spiral%20Fitting&body=Title%3A%20Virtually%20Unrolling%20the%20Herculaneum%20Papyri%20by%20Diffeomorphic%20Spiral%20Fitting%0AAuthor%3A%20Paul%20Henderson%0AAbstract%3A%20The%20Herculaneum%20Papyri%20are%20a%20collection%20of%20rolled%20papyrus%20documents%20that%20were%20charred%20and%20buried%20by%20the%20famous%20eruption%20of%20Mount%20Vesuvius.%20They%20promise%20to%20contain%20a%20wealth%20of%20previously%20unseen%20Greek%20and%20Latin%20texts%2C%20but%20are%20extremely%20fragile%20and%20thus%20most%20cannot%20be%20unrolled%20physically.%20A%20solution%20to%20access%20these%20texts%20is%20virtual%20unrolling%2C%20where%20the%20papyrus%20surface%20is%20digitally%20traced%20out%20in%20a%20CT%20scan%20of%20the%20scroll%2C%20to%20create%20a%20flattened%20representation.%20This%20tracing%20is%20very%20laborious%20to%20do%20manually%20in%20gigavoxel-sized%20scans%2C%20so%20automated%20approaches%20are%20desirable.%20We%20present%20the%20first%20top-down%20method%20that%20automatically%20fits%20a%20surface%20model%20to%20a%20CT%20scan%20of%20a%20severely%20damaged%20scroll.%20We%20take%20a%20novel%20approach%20that%20globally%20fits%20an%20explicit%20parametric%20model%20of%20the%20deformed%20scroll%20to%20existing%20neural%20network%20predictions%20of%20where%20the%20rolled%20papyrus%20likely%20passes.%20Our%20method%20guarantees%20the%20resulting%20surface%20is%20a%20single%20continuous%202D%20sheet%2C%20even%20passing%20through%20regions%20where%20the%20surface%20is%20not%20detectable%20in%20the%20CT%20scan.%20We%20conduct%20comprehensive%20experiments%20on%20high-resolution%20CT%20scans%20of%20two%20scrolls%2C%20showing%20that%20our%20approach%20successfully%20unrolls%20large%20regions%2C%20and%20exceeds%20the%20performance%20of%20the%20only%20existing%20automated%20unrolling%20method%20suitable%20for%20this%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtually%2520Unrolling%2520the%2520Herculaneum%2520Papyri%2520by%2520Diffeomorphic%2520Spiral%2520Fitting%26entry.906535625%3DPaul%2520Henderson%26entry.1292438233%3DThe%2520Herculaneum%2520Papyri%2520are%2520a%2520collection%2520of%2520rolled%2520papyrus%2520documents%2520that%2520were%2520charred%2520and%2520buried%2520by%2520the%2520famous%2520eruption%2520of%2520Mount%2520Vesuvius.%2520They%2520promise%2520to%2520contain%2520a%2520wealth%2520of%2520previously%2520unseen%2520Greek%2520and%2520Latin%2520texts%252C%2520but%2520are%2520extremely%2520fragile%2520and%2520thus%2520most%2520cannot%2520be%2520unrolled%2520physically.%2520A%2520solution%2520to%2520access%2520these%2520texts%2520is%2520virtual%2520unrolling%252C%2520where%2520the%2520papyrus%2520surface%2520is%2520digitally%2520traced%2520out%2520in%2520a%2520CT%2520scan%2520of%2520the%2520scroll%252C%2520to%2520create%2520a%2520flattened%2520representation.%2520This%2520tracing%2520is%2520very%2520laborious%2520to%2520do%2520manually%2520in%2520gigavoxel-sized%2520scans%252C%2520so%2520automated%2520approaches%2520are%2520desirable.%2520We%2520present%2520the%2520first%2520top-down%2520method%2520that%2520automatically%2520fits%2520a%2520surface%2520model%2520to%2520a%2520CT%2520scan%2520of%2520a%2520severely%2520damaged%2520scroll.%2520We%2520take%2520a%2520novel%2520approach%2520that%2520globally%2520fits%2520an%2520explicit%2520parametric%2520model%2520of%2520the%2520deformed%2520scroll%2520to%2520existing%2520neural%2520network%2520predictions%2520of%2520where%2520the%2520rolled%2520papyrus%2520likely%2520passes.%2520Our%2520method%2520guarantees%2520the%2520resulting%2520surface%2520is%2520a%2520single%2520continuous%25202D%2520sheet%252C%2520even%2520passing%2520through%2520regions%2520where%2520the%2520surface%2520is%2520not%2520detectable%2520in%2520the%2520CT%2520scan.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520high-resolution%2520CT%2520scans%2520of%2520two%2520scrolls%252C%2520showing%2520that%2520our%2520approach%2520successfully%2520unrolls%2520large%2520regions%252C%2520and%2520exceeds%2520the%2520performance%2520of%2520the%2520only%2520existing%2520automated%2520unrolling%2520method%2520suitable%2520for%2520this%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virtually%20Unrolling%20the%20Herculaneum%20Papyri%20by%20Diffeomorphic%20Spiral%20Fitting&entry.906535625=Paul%20Henderson&entry.1292438233=The%20Herculaneum%20Papyri%20are%20a%20collection%20of%20rolled%20papyrus%20documents%20that%20were%20charred%20and%20buried%20by%20the%20famous%20eruption%20of%20Mount%20Vesuvius.%20They%20promise%20to%20contain%20a%20wealth%20of%20previously%20unseen%20Greek%20and%20Latin%20texts%2C%20but%20are%20extremely%20fragile%20and%20thus%20most%20cannot%20be%20unrolled%20physically.%20A%20solution%20to%20access%20these%20texts%20is%20virtual%20unrolling%2C%20where%20the%20papyrus%20surface%20is%20digitally%20traced%20out%20in%20a%20CT%20scan%20of%20the%20scroll%2C%20to%20create%20a%20flattened%20representation.%20This%20tracing%20is%20very%20laborious%20to%20do%20manually%20in%20gigavoxel-sized%20scans%2C%20so%20automated%20approaches%20are%20desirable.%20We%20present%20the%20first%20top-down%20method%20that%20automatically%20fits%20a%20surface%20model%20to%20a%20CT%20scan%20of%20a%20severely%20damaged%20scroll.%20We%20take%20a%20novel%20approach%20that%20globally%20fits%20an%20explicit%20parametric%20model%20of%20the%20deformed%20scroll%20to%20existing%20neural%20network%20predictions%20of%20where%20the%20rolled%20papyrus%20likely%20passes.%20Our%20method%20guarantees%20the%20resulting%20surface%20is%20a%20single%20continuous%202D%20sheet%2C%20even%20passing%20through%20regions%20where%20the%20surface%20is%20not%20detectable%20in%20the%20CT%20scan.%20We%20conduct%20comprehensive%20experiments%20on%20high-resolution%20CT%20scans%20of%20two%20scrolls%2C%20showing%20that%20our%20approach%20successfully%20unrolls%20large%20regions%2C%20and%20exceeds%20the%20performance%20of%20the%20only%20existing%20automated%20unrolling%20method%20suitable%20for%20this%20data.&entry.1838667208=http%3A//arxiv.org/abs/2512.04927v1&entry.124074799=Read"},
{"title": "Series of quasi-uniform scatterings with fast search, root systems and neural network classifications", "author": "Igor V. Netay", "abstract": "In this paper we describe an approach to construct large extendable collections of vectors in predefined spaces of given dimensions. These collections are useful for neural network latent space configuration and training. For classification problem with large or unknown number of classes this allows to construct classifiers without classification layer and extend the number of classes without retraining of network from the very beginning. The construction allows to create large well-spaced vector collections in spaces of minimal possible dimension. If the number of classes is known or approximately predictable, one can choose sufficient enough vector collection size. If one needs to significantly extend the number of classes, one can extend the collection in the same latent space, or to incorporate the collection into collection of higher dimensions with same spacing between vectors. Also, regular symmetric structure of constructed vector collections can significantly simplify problems of search for nearest cluster centers or embeddings in the latent space. Construction of vector collections is based on combinatorics and geometry of semi-simple Lie groups irreducible representations with highest weight.", "link": "http://arxiv.org/abs/2512.04865v1", "date": "2025-12-04", "relevancy": 2.3797, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4771}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Series%20of%20quasi-uniform%20scatterings%20with%20fast%20search%2C%20root%20systems%20and%20neural%20network%20classifications&body=Title%3A%20Series%20of%20quasi-uniform%20scatterings%20with%20fast%20search%2C%20root%20systems%20and%20neural%20network%20classifications%0AAuthor%3A%20Igor%20V.%20Netay%0AAbstract%3A%20In%20this%20paper%20we%20describe%20an%20approach%20to%20construct%20large%20extendable%20collections%20of%20vectors%20in%20predefined%20spaces%20of%20given%20dimensions.%20These%20collections%20are%20useful%20for%20neural%20network%20latent%20space%20configuration%20and%20training.%20For%20classification%20problem%20with%20large%20or%20unknown%20number%20of%20classes%20this%20allows%20to%20construct%20classifiers%20without%20classification%20layer%20and%20extend%20the%20number%20of%20classes%20without%20retraining%20of%20network%20from%20the%20very%20beginning.%20The%20construction%20allows%20to%20create%20large%20well-spaced%20vector%20collections%20in%20spaces%20of%20minimal%20possible%20dimension.%20If%20the%20number%20of%20classes%20is%20known%20or%20approximately%20predictable%2C%20one%20can%20choose%20sufficient%20enough%20vector%20collection%20size.%20If%20one%20needs%20to%20significantly%20extend%20the%20number%20of%20classes%2C%20one%20can%20extend%20the%20collection%20in%20the%20same%20latent%20space%2C%20or%20to%20incorporate%20the%20collection%20into%20collection%20of%20higher%20dimensions%20with%20same%20spacing%20between%20vectors.%20Also%2C%20regular%20symmetric%20structure%20of%20constructed%20vector%20collections%20can%20significantly%20simplify%20problems%20of%20search%20for%20nearest%20cluster%20centers%20or%20embeddings%20in%20the%20latent%20space.%20Construction%20of%20vector%20collections%20is%20based%20on%20combinatorics%20and%20geometry%20of%20semi-simple%20Lie%20groups%20irreducible%20representations%20with%20highest%20weight.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeries%2520of%2520quasi-uniform%2520scatterings%2520with%2520fast%2520search%252C%2520root%2520systems%2520and%2520neural%2520network%2520classifications%26entry.906535625%3DIgor%2520V.%2520Netay%26entry.1292438233%3DIn%2520this%2520paper%2520we%2520describe%2520an%2520approach%2520to%2520construct%2520large%2520extendable%2520collections%2520of%2520vectors%2520in%2520predefined%2520spaces%2520of%2520given%2520dimensions.%2520These%2520collections%2520are%2520useful%2520for%2520neural%2520network%2520latent%2520space%2520configuration%2520and%2520training.%2520For%2520classification%2520problem%2520with%2520large%2520or%2520unknown%2520number%2520of%2520classes%2520this%2520allows%2520to%2520construct%2520classifiers%2520without%2520classification%2520layer%2520and%2520extend%2520the%2520number%2520of%2520classes%2520without%2520retraining%2520of%2520network%2520from%2520the%2520very%2520beginning.%2520The%2520construction%2520allows%2520to%2520create%2520large%2520well-spaced%2520vector%2520collections%2520in%2520spaces%2520of%2520minimal%2520possible%2520dimension.%2520If%2520the%2520number%2520of%2520classes%2520is%2520known%2520or%2520approximately%2520predictable%252C%2520one%2520can%2520choose%2520sufficient%2520enough%2520vector%2520collection%2520size.%2520If%2520one%2520needs%2520to%2520significantly%2520extend%2520the%2520number%2520of%2520classes%252C%2520one%2520can%2520extend%2520the%2520collection%2520in%2520the%2520same%2520latent%2520space%252C%2520or%2520to%2520incorporate%2520the%2520collection%2520into%2520collection%2520of%2520higher%2520dimensions%2520with%2520same%2520spacing%2520between%2520vectors.%2520Also%252C%2520regular%2520symmetric%2520structure%2520of%2520constructed%2520vector%2520collections%2520can%2520significantly%2520simplify%2520problems%2520of%2520search%2520for%2520nearest%2520cluster%2520centers%2520or%2520embeddings%2520in%2520the%2520latent%2520space.%2520Construction%2520of%2520vector%2520collections%2520is%2520based%2520on%2520combinatorics%2520and%2520geometry%2520of%2520semi-simple%2520Lie%2520groups%2520irreducible%2520representations%2520with%2520highest%2520weight.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Series%20of%20quasi-uniform%20scatterings%20with%20fast%20search%2C%20root%20systems%20and%20neural%20network%20classifications&entry.906535625=Igor%20V.%20Netay&entry.1292438233=In%20this%20paper%20we%20describe%20an%20approach%20to%20construct%20large%20extendable%20collections%20of%20vectors%20in%20predefined%20spaces%20of%20given%20dimensions.%20These%20collections%20are%20useful%20for%20neural%20network%20latent%20space%20configuration%20and%20training.%20For%20classification%20problem%20with%20large%20or%20unknown%20number%20of%20classes%20this%20allows%20to%20construct%20classifiers%20without%20classification%20layer%20and%20extend%20the%20number%20of%20classes%20without%20retraining%20of%20network%20from%20the%20very%20beginning.%20The%20construction%20allows%20to%20create%20large%20well-spaced%20vector%20collections%20in%20spaces%20of%20minimal%20possible%20dimension.%20If%20the%20number%20of%20classes%20is%20known%20or%20approximately%20predictable%2C%20one%20can%20choose%20sufficient%20enough%20vector%20collection%20size.%20If%20one%20needs%20to%20significantly%20extend%20the%20number%20of%20classes%2C%20one%20can%20extend%20the%20collection%20in%20the%20same%20latent%20space%2C%20or%20to%20incorporate%20the%20collection%20into%20collection%20of%20higher%20dimensions%20with%20same%20spacing%20between%20vectors.%20Also%2C%20regular%20symmetric%20structure%20of%20constructed%20vector%20collections%20can%20significantly%20simplify%20problems%20of%20search%20for%20nearest%20cluster%20centers%20or%20embeddings%20in%20the%20latent%20space.%20Construction%20of%20vector%20collections%20is%20based%20on%20combinatorics%20and%20geometry%20of%20semi-simple%20Lie%20groups%20irreducible%20representations%20with%20highest%20weight.&entry.1838667208=http%3A//arxiv.org/abs/2512.04865v1&entry.124074799=Read"},
{"title": "Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing", "author": "Maria-Paola Forte and Nikos Athanasiou and Giulia Ballardini and Jan Ulrich Bartels and Katherine J. Kuchenbecker and Michael J. Black", "abstract": "Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de", "link": "http://arxiv.org/abs/2512.04862v1", "date": "2025-12-04", "relevancy": 2.3781, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6371}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5652}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contact-Aware%20Refinement%20of%20Human%20Pose%20Pseudo-Ground%20Truth%20via%20Bioimpedance%20Sensing&body=Title%3A%20Contact-Aware%20Refinement%20of%20Human%20Pose%20Pseudo-Ground%20Truth%20via%20Bioimpedance%20Sensing%0AAuthor%3A%20Maria-Paola%20Forte%20and%20Nikos%20Athanasiou%20and%20Giulia%20Ballardini%20and%20Jan%20Ulrich%20Bartels%20and%20Katherine%20J.%20Kuchenbecker%20and%20Michael%20J.%20Black%0AAbstract%3A%20Capturing%20accurate%203D%20human%20pose%20in%20the%20wild%20would%20provide%20valuable%20data%20for%20training%20pose%20estimation%20and%20motion%20generation%20methods.%20While%20video-based%20estimation%20approaches%20have%20become%20increasingly%20accurate%2C%20they%20often%20fail%20in%20common%20scenarios%20involving%20self-contact%2C%20such%20as%20a%20hand%20touching%20the%20face.%20In%20contrast%2C%20wearable%20bioimpedance%20sensing%20can%20cheaply%20and%20unobtrusively%20measure%20ground-truth%20skin-to-skin%20contact.%20Consequently%2C%20we%20propose%20a%20novel%20framework%20that%20combines%20visual%20pose%20estimators%20with%20bioimpedance%20sensing%20to%20capture%20the%203D%20pose%20of%20people%20by%20taking%20self-contact%20into%20account.%20Our%20method%2C%20BioTUCH%2C%20initializes%20the%20pose%20using%20an%20off-the-shelf%20estimator%20and%20introduces%20contact-aware%20pose%20optimization%20during%20measured%20self-contact%3A%20reprojection%20error%20and%20deviations%20from%20the%20input%20estimate%20are%20minimized%20while%20enforcing%20vertex%20proximity%20constraints.%20We%20validate%20our%20approach%20using%20a%20new%20dataset%20of%20synchronized%20RGB%20video%2C%20bioimpedance%20measurements%2C%20and%203D%20motion%20capture.%20Testing%20with%20three%20input%20pose%20estimators%2C%20we%20demonstrate%20an%20average%20of%2011.7%25%20improvement%20in%20reconstruction%20accuracy.%20We%20also%20present%20a%20miniature%20wearable%20bioimpedance%20sensor%20that%20enables%20efficient%20large-scale%20collection%20of%20contact-aware%20training%20data%20for%20improving%20pose%20estimation%20and%20generation%20using%20BioTUCH.%20Code%20and%20data%20are%20available%20at%20biotuch.is.tue.mpg.de%0ALink%3A%20http%3A//arxiv.org/abs/2512.04862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContact-Aware%2520Refinement%2520of%2520Human%2520Pose%2520Pseudo-Ground%2520Truth%2520via%2520Bioimpedance%2520Sensing%26entry.906535625%3DMaria-Paola%2520Forte%2520and%2520Nikos%2520Athanasiou%2520and%2520Giulia%2520Ballardini%2520and%2520Jan%2520Ulrich%2520Bartels%2520and%2520Katherine%2520J.%2520Kuchenbecker%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3DCapturing%2520accurate%25203D%2520human%2520pose%2520in%2520the%2520wild%2520would%2520provide%2520valuable%2520data%2520for%2520training%2520pose%2520estimation%2520and%2520motion%2520generation%2520methods.%2520While%2520video-based%2520estimation%2520approaches%2520have%2520become%2520increasingly%2520accurate%252C%2520they%2520often%2520fail%2520in%2520common%2520scenarios%2520involving%2520self-contact%252C%2520such%2520as%2520a%2520hand%2520touching%2520the%2520face.%2520In%2520contrast%252C%2520wearable%2520bioimpedance%2520sensing%2520can%2520cheaply%2520and%2520unobtrusively%2520measure%2520ground-truth%2520skin-to-skin%2520contact.%2520Consequently%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520combines%2520visual%2520pose%2520estimators%2520with%2520bioimpedance%2520sensing%2520to%2520capture%2520the%25203D%2520pose%2520of%2520people%2520by%2520taking%2520self-contact%2520into%2520account.%2520Our%2520method%252C%2520BioTUCH%252C%2520initializes%2520the%2520pose%2520using%2520an%2520off-the-shelf%2520estimator%2520and%2520introduces%2520contact-aware%2520pose%2520optimization%2520during%2520measured%2520self-contact%253A%2520reprojection%2520error%2520and%2520deviations%2520from%2520the%2520input%2520estimate%2520are%2520minimized%2520while%2520enforcing%2520vertex%2520proximity%2520constraints.%2520We%2520validate%2520our%2520approach%2520using%2520a%2520new%2520dataset%2520of%2520synchronized%2520RGB%2520video%252C%2520bioimpedance%2520measurements%252C%2520and%25203D%2520motion%2520capture.%2520Testing%2520with%2520three%2520input%2520pose%2520estimators%252C%2520we%2520demonstrate%2520an%2520average%2520of%252011.7%2525%2520improvement%2520in%2520reconstruction%2520accuracy.%2520We%2520also%2520present%2520a%2520miniature%2520wearable%2520bioimpedance%2520sensor%2520that%2520enables%2520efficient%2520large-scale%2520collection%2520of%2520contact-aware%2520training%2520data%2520for%2520improving%2520pose%2520estimation%2520and%2520generation%2520using%2520BioTUCH.%2520Code%2520and%2520data%2520are%2520available%2520at%2520biotuch.is.tue.mpg.de%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contact-Aware%20Refinement%20of%20Human%20Pose%20Pseudo-Ground%20Truth%20via%20Bioimpedance%20Sensing&entry.906535625=Maria-Paola%20Forte%20and%20Nikos%20Athanasiou%20and%20Giulia%20Ballardini%20and%20Jan%20Ulrich%20Bartels%20and%20Katherine%20J.%20Kuchenbecker%20and%20Michael%20J.%20Black&entry.1292438233=Capturing%20accurate%203D%20human%20pose%20in%20the%20wild%20would%20provide%20valuable%20data%20for%20training%20pose%20estimation%20and%20motion%20generation%20methods.%20While%20video-based%20estimation%20approaches%20have%20become%20increasingly%20accurate%2C%20they%20often%20fail%20in%20common%20scenarios%20involving%20self-contact%2C%20such%20as%20a%20hand%20touching%20the%20face.%20In%20contrast%2C%20wearable%20bioimpedance%20sensing%20can%20cheaply%20and%20unobtrusively%20measure%20ground-truth%20skin-to-skin%20contact.%20Consequently%2C%20we%20propose%20a%20novel%20framework%20that%20combines%20visual%20pose%20estimators%20with%20bioimpedance%20sensing%20to%20capture%20the%203D%20pose%20of%20people%20by%20taking%20self-contact%20into%20account.%20Our%20method%2C%20BioTUCH%2C%20initializes%20the%20pose%20using%20an%20off-the-shelf%20estimator%20and%20introduces%20contact-aware%20pose%20optimization%20during%20measured%20self-contact%3A%20reprojection%20error%20and%20deviations%20from%20the%20input%20estimate%20are%20minimized%20while%20enforcing%20vertex%20proximity%20constraints.%20We%20validate%20our%20approach%20using%20a%20new%20dataset%20of%20synchronized%20RGB%20video%2C%20bioimpedance%20measurements%2C%20and%203D%20motion%20capture.%20Testing%20with%20three%20input%20pose%20estimators%2C%20we%20demonstrate%20an%20average%20of%2011.7%25%20improvement%20in%20reconstruction%20accuracy.%20We%20also%20present%20a%20miniature%20wearable%20bioimpedance%20sensor%20that%20enables%20efficient%20large-scale%20collection%20of%20contact-aware%20training%20data%20for%20improving%20pose%20estimation%20and%20generation%20using%20BioTUCH.%20Code%20and%20data%20are%20available%20at%20biotuch.is.tue.mpg.de&entry.1838667208=http%3A//arxiv.org/abs/2512.04862v1&entry.124074799=Read"},
{"title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression", "author": "Jung Yi and Wooseok Jang and Paul Hyunbin Cho and Jisu Nam and Heeji Yoon and Seungryong Kim", "abstract": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.", "link": "http://arxiv.org/abs/2512.05081v1", "date": "2025-12-04", "relevancy": 2.3747, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6081}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5855}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Forcing%3A%20Training-Free%20Long%20Video%20Generation%20with%20Deep%20Sink%20and%20Participative%20Compression&body=Title%3A%20Deep%20Forcing%3A%20Training-Free%20Long%20Video%20Generation%20with%20Deep%20Sink%20and%20Participative%20Compression%0AAuthor%3A%20Jung%20Yi%20and%20Wooseok%20Jang%20and%20Paul%20Hyunbin%20Cho%20and%20Jisu%20Nam%20and%20Heeji%20Yoon%20and%20Seungryong%20Kim%0AAbstract%3A%20Recent%20advances%20in%20autoregressive%20video%20diffusion%20have%20enabled%20real-time%20frame%20streaming%2C%20yet%20existing%20solutions%20still%20suffer%20from%20temporal%20repetition%2C%20drift%2C%20and%20motion%20deceleration.%20We%20find%20that%20naively%20applying%20StreamingLLM-style%20attention%20sinks%20to%20video%20diffusion%20leads%20to%20fidelity%20degradation%20and%20motion%20stagnation.%20To%20overcome%20this%2C%20we%20introduce%20Deep%20Forcing%2C%20which%20consists%20of%20two%20training-free%20mechanisms%20that%20address%20this%20without%20any%20fine-tuning.%20Specifically%2C%201%29%20Deep%20Sink%20dedicates%20half%20of%20the%20sliding%20window%20to%20persistent%20sink%20tokens%20and%20re-aligns%20their%20temporal%20RoPE%20phase%20to%20the%20current%20timeline%2C%20stabilizing%20global%20context%20during%20long%20rollouts.%202%29%20Participative%20Compression%20performs%20importance-aware%20KV%20cache%20pruning%20that%20preserves%20only%20tokens%20actively%20participating%20in%20recent%20attention%20while%20safely%20discarding%20redundant%20and%20degraded%20history%2C%20minimizing%20error%20accumulation%20under%20out-of-distribution%20length%20generation.%20Together%2C%20these%20components%20enable%20over%2012x%20extrapolation%20%28e.g.%205s-trained%20to%2060s%2B%20generation%29%20with%20better%20imaging%20quality%20than%20LongLive%2C%20better%20aesthetic%20quality%20than%20RollingForcing%2C%20almost%20maintaining%20overall%20consistency%2C%20and%20substantial%20gains%20in%20dynamic%20degree%2C%20all%20while%20maintaining%20real-time%20generation.%20Our%20results%20demonstrate%20that%20training-free%20KV-cache%20management%20can%20match%20or%20exceed%20training-based%20approaches%20for%20autoregressively%20streaming%20long-video%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Forcing%253A%2520Training-Free%2520Long%2520Video%2520Generation%2520with%2520Deep%2520Sink%2520and%2520Participative%2520Compression%26entry.906535625%3DJung%2520Yi%2520and%2520Wooseok%2520Jang%2520and%2520Paul%2520Hyunbin%2520Cho%2520and%2520Jisu%2520Nam%2520and%2520Heeji%2520Yoon%2520and%2520Seungryong%2520Kim%26entry.1292438233%3DRecent%2520advances%2520in%2520autoregressive%2520video%2520diffusion%2520have%2520enabled%2520real-time%2520frame%2520streaming%252C%2520yet%2520existing%2520solutions%2520still%2520suffer%2520from%2520temporal%2520repetition%252C%2520drift%252C%2520and%2520motion%2520deceleration.%2520We%2520find%2520that%2520naively%2520applying%2520StreamingLLM-style%2520attention%2520sinks%2520to%2520video%2520diffusion%2520leads%2520to%2520fidelity%2520degradation%2520and%2520motion%2520stagnation.%2520To%2520overcome%2520this%252C%2520we%2520introduce%2520Deep%2520Forcing%252C%2520which%2520consists%2520of%2520two%2520training-free%2520mechanisms%2520that%2520address%2520this%2520without%2520any%2520fine-tuning.%2520Specifically%252C%25201%2529%2520Deep%2520Sink%2520dedicates%2520half%2520of%2520the%2520sliding%2520window%2520to%2520persistent%2520sink%2520tokens%2520and%2520re-aligns%2520their%2520temporal%2520RoPE%2520phase%2520to%2520the%2520current%2520timeline%252C%2520stabilizing%2520global%2520context%2520during%2520long%2520rollouts.%25202%2529%2520Participative%2520Compression%2520performs%2520importance-aware%2520KV%2520cache%2520pruning%2520that%2520preserves%2520only%2520tokens%2520actively%2520participating%2520in%2520recent%2520attention%2520while%2520safely%2520discarding%2520redundant%2520and%2520degraded%2520history%252C%2520minimizing%2520error%2520accumulation%2520under%2520out-of-distribution%2520length%2520generation.%2520Together%252C%2520these%2520components%2520enable%2520over%252012x%2520extrapolation%2520%2528e.g.%25205s-trained%2520to%252060s%252B%2520generation%2529%2520with%2520better%2520imaging%2520quality%2520than%2520LongLive%252C%2520better%2520aesthetic%2520quality%2520than%2520RollingForcing%252C%2520almost%2520maintaining%2520overall%2520consistency%252C%2520and%2520substantial%2520gains%2520in%2520dynamic%2520degree%252C%2520all%2520while%2520maintaining%2520real-time%2520generation.%2520Our%2520results%2520demonstrate%2520that%2520training-free%2520KV-cache%2520management%2520can%2520match%2520or%2520exceed%2520training-based%2520approaches%2520for%2520autoregressively%2520streaming%2520long-video%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Forcing%3A%20Training-Free%20Long%20Video%20Generation%20with%20Deep%20Sink%20and%20Participative%20Compression&entry.906535625=Jung%20Yi%20and%20Wooseok%20Jang%20and%20Paul%20Hyunbin%20Cho%20and%20Jisu%20Nam%20and%20Heeji%20Yoon%20and%20Seungryong%20Kim&entry.1292438233=Recent%20advances%20in%20autoregressive%20video%20diffusion%20have%20enabled%20real-time%20frame%20streaming%2C%20yet%20existing%20solutions%20still%20suffer%20from%20temporal%20repetition%2C%20drift%2C%20and%20motion%20deceleration.%20We%20find%20that%20naively%20applying%20StreamingLLM-style%20attention%20sinks%20to%20video%20diffusion%20leads%20to%20fidelity%20degradation%20and%20motion%20stagnation.%20To%20overcome%20this%2C%20we%20introduce%20Deep%20Forcing%2C%20which%20consists%20of%20two%20training-free%20mechanisms%20that%20address%20this%20without%20any%20fine-tuning.%20Specifically%2C%201%29%20Deep%20Sink%20dedicates%20half%20of%20the%20sliding%20window%20to%20persistent%20sink%20tokens%20and%20re-aligns%20their%20temporal%20RoPE%20phase%20to%20the%20current%20timeline%2C%20stabilizing%20global%20context%20during%20long%20rollouts.%202%29%20Participative%20Compression%20performs%20importance-aware%20KV%20cache%20pruning%20that%20preserves%20only%20tokens%20actively%20participating%20in%20recent%20attention%20while%20safely%20discarding%20redundant%20and%20degraded%20history%2C%20minimizing%20error%20accumulation%20under%20out-of-distribution%20length%20generation.%20Together%2C%20these%20components%20enable%20over%2012x%20extrapolation%20%28e.g.%205s-trained%20to%2060s%2B%20generation%29%20with%20better%20imaging%20quality%20than%20LongLive%2C%20better%20aesthetic%20quality%20than%20RollingForcing%2C%20almost%20maintaining%20overall%20consistency%2C%20and%20substantial%20gains%20in%20dynamic%20degree%2C%20all%20while%20maintaining%20real-time%20generation.%20Our%20results%20demonstrate%20that%20training-free%20KV-cache%20management%20can%20match%20or%20exceed%20training-based%20approaches%20for%20autoregressively%20streaming%20long-video%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.05081v1&entry.124074799=Read"},
{"title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model", "author": " GigaBrain Team and Angen Ye and Boyuan Wang and Chaojun Ni and Guan Huang and Guosheng Zhao and Haoyun Li and Jie Li and Jiagang Zhu and Lv Feng and Peng Li and Qiuping Deng and Runqi Ouyang and Wenkang Qin and Xinze Chen and Xiaofeng Wang and Yang Wang and Yifan Li and Yilong Li and Yiran Ding and Yuan Xu and Yun Ye and Yukun Zhou and Zhehao Dong and Zhenan Wang and Zhichao Liu and Zheng Zhu", "abstract": "Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.", "link": "http://arxiv.org/abs/2510.19430v3", "date": "2025-12-04", "relevancy": 2.3745, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6224}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5769}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GigaBrain-0%3A%20A%20World%20Model-Powered%20Vision-Language-Action%20Model&body=Title%3A%20GigaBrain-0%3A%20A%20World%20Model-Powered%20Vision-Language-Action%20Model%0AAuthor%3A%20%20GigaBrain%20Team%20and%20Angen%20Ye%20and%20Boyuan%20Wang%20and%20Chaojun%20Ni%20and%20Guan%20Huang%20and%20Guosheng%20Zhao%20and%20Haoyun%20Li%20and%20Jie%20Li%20and%20Jiagang%20Zhu%20and%20Lv%20Feng%20and%20Peng%20Li%20and%20Qiuping%20Deng%20and%20Runqi%20Ouyang%20and%20Wenkang%20Qin%20and%20Xinze%20Chen%20and%20Xiaofeng%20Wang%20and%20Yang%20Wang%20and%20Yifan%20Li%20and%20Yilong%20Li%20and%20Yiran%20Ding%20and%20Yuan%20Xu%20and%20Yun%20Ye%20and%20Yukun%20Zhou%20and%20Zhehao%20Dong%20and%20Zhenan%20Wang%20and%20Zhichao%20Liu%20and%20Zheng%20Zhu%0AAbstract%3A%20Training%20Vision-Language-Action%20%28VLA%29%20models%20for%20generalist%20robots%20typically%20requires%20large-scale%20real-world%20robot%20data%2C%20which%20is%20expensive%20and%20time-consuming%20to%20collect.%20The%20inefficiency%20of%20physical%20data%20collection%20severely%20limits%20the%20scalability%2C%20and%20generalization%20capacity%20of%20current%20VLA%20systems.%20To%20address%20this%20challenge%2C%20we%20introduce%20GigaBrain-0%2C%20a%20novel%20VLA%20foundation%20model%20empowered%20by%20world%20model-generated%20data%20%28e.g.%2C%20video%20generation%2C%20real2real%20transfer%2C%20human%20transfer%2C%20view%20transfer%2C%20sim2real%20transfer%20data%29.%20By%20leveraging%20world%20models%20to%20generate%20diverse%20data%20at%20scale%2C%20GigaBrain-0%20significantly%20reduces%20reliance%20on%20real%20robot%20data%20while%20improving%20cross-task%20generalization.%20Our%20approach%20further%20improves%20policy%20robustness%20through%20RGBD%20input%20modeling%20and%20embodied%20Chain-of-Thought%20%28CoT%29%20supervision%2C%20enabling%20the%20model%20to%20reason%20about%20spatial%20geometry%2C%20object%20states%2C%20and%20long-horizon%20dependencies%20during%20task%20execution.%20This%20leads%20to%20substantial%20gains%20in%20real-world%20performance%20on%20dexterous%2C%20long-horizon%2C%20and%20mobile%20manipulation%20tasks.%20Extensive%20experiments%20demonstrate%20that%20GigaBrain-0%20achieves%20superior%20generalization%20across%20variations%20in%20appearances%20%28e.g.%2C%20textures%2C%20colors%29%2C%20object%20placements%2C%20and%20camera%20viewpoints.%20Additionally%2C%20we%20present%20GigaBrain-0-Small%2C%20an%20optimized%20lightweight%20variant%20designed%20to%20run%20efficiently%20on%20devices%20such%20as%20the%20NVIDIA%20Jetson%20AGX%20Orin.%0ALink%3A%20http%3A//arxiv.org/abs/2510.19430v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGigaBrain-0%253A%2520A%2520World%2520Model-Powered%2520Vision-Language-Action%2520Model%26entry.906535625%3D%2520GigaBrain%2520Team%2520and%2520Angen%2520Ye%2520and%2520Boyuan%2520Wang%2520and%2520Chaojun%2520Ni%2520and%2520Guan%2520Huang%2520and%2520Guosheng%2520Zhao%2520and%2520Haoyun%2520Li%2520and%2520Jie%2520Li%2520and%2520Jiagang%2520Zhu%2520and%2520Lv%2520Feng%2520and%2520Peng%2520Li%2520and%2520Qiuping%2520Deng%2520and%2520Runqi%2520Ouyang%2520and%2520Wenkang%2520Qin%2520and%2520Xinze%2520Chen%2520and%2520Xiaofeng%2520Wang%2520and%2520Yang%2520Wang%2520and%2520Yifan%2520Li%2520and%2520Yilong%2520Li%2520and%2520Yiran%2520Ding%2520and%2520Yuan%2520Xu%2520and%2520Yun%2520Ye%2520and%2520Yukun%2520Zhou%2520and%2520Zhehao%2520Dong%2520and%2520Zhenan%2520Wang%2520and%2520Zhichao%2520Liu%2520and%2520Zheng%2520Zhu%26entry.1292438233%3DTraining%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520for%2520generalist%2520robots%2520typically%2520requires%2520large-scale%2520real-world%2520robot%2520data%252C%2520which%2520is%2520expensive%2520and%2520time-consuming%2520to%2520collect.%2520The%2520inefficiency%2520of%2520physical%2520data%2520collection%2520severely%2520limits%2520the%2520scalability%252C%2520and%2520generalization%2520capacity%2520of%2520current%2520VLA%2520systems.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520GigaBrain-0%252C%2520a%2520novel%2520VLA%2520foundation%2520model%2520empowered%2520by%2520world%2520model-generated%2520data%2520%2528e.g.%252C%2520video%2520generation%252C%2520real2real%2520transfer%252C%2520human%2520transfer%252C%2520view%2520transfer%252C%2520sim2real%2520transfer%2520data%2529.%2520By%2520leveraging%2520world%2520models%2520to%2520generate%2520diverse%2520data%2520at%2520scale%252C%2520GigaBrain-0%2520significantly%2520reduces%2520reliance%2520on%2520real%2520robot%2520data%2520while%2520improving%2520cross-task%2520generalization.%2520Our%2520approach%2520further%2520improves%2520policy%2520robustness%2520through%2520RGBD%2520input%2520modeling%2520and%2520embodied%2520Chain-of-Thought%2520%2528CoT%2529%2520supervision%252C%2520enabling%2520the%2520model%2520to%2520reason%2520about%2520spatial%2520geometry%252C%2520object%2520states%252C%2520and%2520long-horizon%2520dependencies%2520during%2520task%2520execution.%2520This%2520leads%2520to%2520substantial%2520gains%2520in%2520real-world%2520performance%2520on%2520dexterous%252C%2520long-horizon%252C%2520and%2520mobile%2520manipulation%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GigaBrain-0%2520achieves%2520superior%2520generalization%2520across%2520variations%2520in%2520appearances%2520%2528e.g.%252C%2520textures%252C%2520colors%2529%252C%2520object%2520placements%252C%2520and%2520camera%2520viewpoints.%2520Additionally%252C%2520we%2520present%2520GigaBrain-0-Small%252C%2520an%2520optimized%2520lightweight%2520variant%2520designed%2520to%2520run%2520efficiently%2520on%2520devices%2520such%2520as%2520the%2520NVIDIA%2520Jetson%2520AGX%2520Orin.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.19430v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GigaBrain-0%3A%20A%20World%20Model-Powered%20Vision-Language-Action%20Model&entry.906535625=%20GigaBrain%20Team%20and%20Angen%20Ye%20and%20Boyuan%20Wang%20and%20Chaojun%20Ni%20and%20Guan%20Huang%20and%20Guosheng%20Zhao%20and%20Haoyun%20Li%20and%20Jie%20Li%20and%20Jiagang%20Zhu%20and%20Lv%20Feng%20and%20Peng%20Li%20and%20Qiuping%20Deng%20and%20Runqi%20Ouyang%20and%20Wenkang%20Qin%20and%20Xinze%20Chen%20and%20Xiaofeng%20Wang%20and%20Yang%20Wang%20and%20Yifan%20Li%20and%20Yilong%20Li%20and%20Yiran%20Ding%20and%20Yuan%20Xu%20and%20Yun%20Ye%20and%20Yukun%20Zhou%20and%20Zhehao%20Dong%20and%20Zhenan%20Wang%20and%20Zhichao%20Liu%20and%20Zheng%20Zhu&entry.1292438233=Training%20Vision-Language-Action%20%28VLA%29%20models%20for%20generalist%20robots%20typically%20requires%20large-scale%20real-world%20robot%20data%2C%20which%20is%20expensive%20and%20time-consuming%20to%20collect.%20The%20inefficiency%20of%20physical%20data%20collection%20severely%20limits%20the%20scalability%2C%20and%20generalization%20capacity%20of%20current%20VLA%20systems.%20To%20address%20this%20challenge%2C%20we%20introduce%20GigaBrain-0%2C%20a%20novel%20VLA%20foundation%20model%20empowered%20by%20world%20model-generated%20data%20%28e.g.%2C%20video%20generation%2C%20real2real%20transfer%2C%20human%20transfer%2C%20view%20transfer%2C%20sim2real%20transfer%20data%29.%20By%20leveraging%20world%20models%20to%20generate%20diverse%20data%20at%20scale%2C%20GigaBrain-0%20significantly%20reduces%20reliance%20on%20real%20robot%20data%20while%20improving%20cross-task%20generalization.%20Our%20approach%20further%20improves%20policy%20robustness%20through%20RGBD%20input%20modeling%20and%20embodied%20Chain-of-Thought%20%28CoT%29%20supervision%2C%20enabling%20the%20model%20to%20reason%20about%20spatial%20geometry%2C%20object%20states%2C%20and%20long-horizon%20dependencies%20during%20task%20execution.%20This%20leads%20to%20substantial%20gains%20in%20real-world%20performance%20on%20dexterous%2C%20long-horizon%2C%20and%20mobile%20manipulation%20tasks.%20Extensive%20experiments%20demonstrate%20that%20GigaBrain-0%20achieves%20superior%20generalization%20across%20variations%20in%20appearances%20%28e.g.%2C%20textures%2C%20colors%29%2C%20object%20placements%2C%20and%20camera%20viewpoints.%20Additionally%2C%20we%20present%20GigaBrain-0-Small%2C%20an%20optimized%20lightweight%20variant%20designed%20to%20run%20efficiently%20on%20devices%20such%20as%20the%20NVIDIA%20Jetson%20AGX%20Orin.&entry.1838667208=http%3A//arxiv.org/abs/2510.19430v3&entry.124074799=Read"},
{"title": "Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting", "author": "Jian Tang and Pu Pang and Haowen Sun and Chengzhong Ma and Xingyu Chen and Hua Huang and Xuguang Lan", "abstract": "Cross-domain transfer in robotic manipulation remains a longstanding challenge due to the significant domain gap between simulated and real-world environments. Existing methods such as domain randomization, adaptation, and sim-real calibration often require extensive tuning or fail to generalize to unseen scenarios. To address this issue, we observe that if domain-invariant features are utilized during policy training in simulation, and the same features can be extracted and provided as the input to policy during real-world deployment, the domain gap can be effectively bridged, leading to significantly improved policy generalization. Accordingly, we propose Semantic 2D Gaussian Splatting (S2GS), a novel representation method that extracts object-centric, domain-invariant spatial features. S2GS constructs multi-view 2D semantic fields and projects them into a unified 3D space via feature-level Gaussian splatting. A semantic filtering mechanism removes irrelevant background content, ensuring clean and consistent inputs for policy learning. To evaluate the effectiveness of S2GS, we adopt Diffusion Policy as the downstream learning algorithm and conduct experiments in the ManiSkill simulation environment, followed by real-world deployment. Results demonstrate that S2GS significantly improves sim-to-real transferability, maintaining high and stable task performance in real-world scenarios.", "link": "http://arxiv.org/abs/2512.04731v1", "date": "2025-12-04", "relevancy": 2.3736, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6046}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Simulation%20and%20Reality%3A%20Cross-Domain%20Transfer%20with%20Semantic%202D%20Gaussian%20Splatting&body=Title%3A%20Bridging%20Simulation%20and%20Reality%3A%20Cross-Domain%20Transfer%20with%20Semantic%202D%20Gaussian%20Splatting%0AAuthor%3A%20Jian%20Tang%20and%20Pu%20Pang%20and%20Haowen%20Sun%20and%20Chengzhong%20Ma%20and%20Xingyu%20Chen%20and%20Hua%20Huang%20and%20Xuguang%20Lan%0AAbstract%3A%20Cross-domain%20transfer%20in%20robotic%20manipulation%20remains%20a%20longstanding%20challenge%20due%20to%20the%20significant%20domain%20gap%20between%20simulated%20and%20real-world%20environments.%20Existing%20methods%20such%20as%20domain%20randomization%2C%20adaptation%2C%20and%20sim-real%20calibration%20often%20require%20extensive%20tuning%20or%20fail%20to%20generalize%20to%20unseen%20scenarios.%20To%20address%20this%20issue%2C%20we%20observe%20that%20if%20domain-invariant%20features%20are%20utilized%20during%20policy%20training%20in%20simulation%2C%20and%20the%20same%20features%20can%20be%20extracted%20and%20provided%20as%20the%20input%20to%20policy%20during%20real-world%20deployment%2C%20the%20domain%20gap%20can%20be%20effectively%20bridged%2C%20leading%20to%20significantly%20improved%20policy%20generalization.%20Accordingly%2C%20we%20propose%20Semantic%202D%20Gaussian%20Splatting%20%28S2GS%29%2C%20a%20novel%20representation%20method%20that%20extracts%20object-centric%2C%20domain-invariant%20spatial%20features.%20S2GS%20constructs%20multi-view%202D%20semantic%20fields%20and%20projects%20them%20into%20a%20unified%203D%20space%20via%20feature-level%20Gaussian%20splatting.%20A%20semantic%20filtering%20mechanism%20removes%20irrelevant%20background%20content%2C%20ensuring%20clean%20and%20consistent%20inputs%20for%20policy%20learning.%20To%20evaluate%20the%20effectiveness%20of%20S2GS%2C%20we%20adopt%20Diffusion%20Policy%20as%20the%20downstream%20learning%20algorithm%20and%20conduct%20experiments%20in%20the%20ManiSkill%20simulation%20environment%2C%20followed%20by%20real-world%20deployment.%20Results%20demonstrate%20that%20S2GS%20significantly%20improves%20sim-to-real%20transferability%2C%20maintaining%20high%20and%20stable%20task%20performance%20in%20real-world%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Simulation%2520and%2520Reality%253A%2520Cross-Domain%2520Transfer%2520with%2520Semantic%25202D%2520Gaussian%2520Splatting%26entry.906535625%3DJian%2520Tang%2520and%2520Pu%2520Pang%2520and%2520Haowen%2520Sun%2520and%2520Chengzhong%2520Ma%2520and%2520Xingyu%2520Chen%2520and%2520Hua%2520Huang%2520and%2520Xuguang%2520Lan%26entry.1292438233%3DCross-domain%2520transfer%2520in%2520robotic%2520manipulation%2520remains%2520a%2520longstanding%2520challenge%2520due%2520to%2520the%2520significant%2520domain%2520gap%2520between%2520simulated%2520and%2520real-world%2520environments.%2520Existing%2520methods%2520such%2520as%2520domain%2520randomization%252C%2520adaptation%252C%2520and%2520sim-real%2520calibration%2520often%2520require%2520extensive%2520tuning%2520or%2520fail%2520to%2520generalize%2520to%2520unseen%2520scenarios.%2520To%2520address%2520this%2520issue%252C%2520we%2520observe%2520that%2520if%2520domain-invariant%2520features%2520are%2520utilized%2520during%2520policy%2520training%2520in%2520simulation%252C%2520and%2520the%2520same%2520features%2520can%2520be%2520extracted%2520and%2520provided%2520as%2520the%2520input%2520to%2520policy%2520during%2520real-world%2520deployment%252C%2520the%2520domain%2520gap%2520can%2520be%2520effectively%2520bridged%252C%2520leading%2520to%2520significantly%2520improved%2520policy%2520generalization.%2520Accordingly%252C%2520we%2520propose%2520Semantic%25202D%2520Gaussian%2520Splatting%2520%2528S2GS%2529%252C%2520a%2520novel%2520representation%2520method%2520that%2520extracts%2520object-centric%252C%2520domain-invariant%2520spatial%2520features.%2520S2GS%2520constructs%2520multi-view%25202D%2520semantic%2520fields%2520and%2520projects%2520them%2520into%2520a%2520unified%25203D%2520space%2520via%2520feature-level%2520Gaussian%2520splatting.%2520A%2520semantic%2520filtering%2520mechanism%2520removes%2520irrelevant%2520background%2520content%252C%2520ensuring%2520clean%2520and%2520consistent%2520inputs%2520for%2520policy%2520learning.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520S2GS%252C%2520we%2520adopt%2520Diffusion%2520Policy%2520as%2520the%2520downstream%2520learning%2520algorithm%2520and%2520conduct%2520experiments%2520in%2520the%2520ManiSkill%2520simulation%2520environment%252C%2520followed%2520by%2520real-world%2520deployment.%2520Results%2520demonstrate%2520that%2520S2GS%2520significantly%2520improves%2520sim-to-real%2520transferability%252C%2520maintaining%2520high%2520and%2520stable%2520task%2520performance%2520in%2520real-world%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Simulation%20and%20Reality%3A%20Cross-Domain%20Transfer%20with%20Semantic%202D%20Gaussian%20Splatting&entry.906535625=Jian%20Tang%20and%20Pu%20Pang%20and%20Haowen%20Sun%20and%20Chengzhong%20Ma%20and%20Xingyu%20Chen%20and%20Hua%20Huang%20and%20Xuguang%20Lan&entry.1292438233=Cross-domain%20transfer%20in%20robotic%20manipulation%20remains%20a%20longstanding%20challenge%20due%20to%20the%20significant%20domain%20gap%20between%20simulated%20and%20real-world%20environments.%20Existing%20methods%20such%20as%20domain%20randomization%2C%20adaptation%2C%20and%20sim-real%20calibration%20often%20require%20extensive%20tuning%20or%20fail%20to%20generalize%20to%20unseen%20scenarios.%20To%20address%20this%20issue%2C%20we%20observe%20that%20if%20domain-invariant%20features%20are%20utilized%20during%20policy%20training%20in%20simulation%2C%20and%20the%20same%20features%20can%20be%20extracted%20and%20provided%20as%20the%20input%20to%20policy%20during%20real-world%20deployment%2C%20the%20domain%20gap%20can%20be%20effectively%20bridged%2C%20leading%20to%20significantly%20improved%20policy%20generalization.%20Accordingly%2C%20we%20propose%20Semantic%202D%20Gaussian%20Splatting%20%28S2GS%29%2C%20a%20novel%20representation%20method%20that%20extracts%20object-centric%2C%20domain-invariant%20spatial%20features.%20S2GS%20constructs%20multi-view%202D%20semantic%20fields%20and%20projects%20them%20into%20a%20unified%203D%20space%20via%20feature-level%20Gaussian%20splatting.%20A%20semantic%20filtering%20mechanism%20removes%20irrelevant%20background%20content%2C%20ensuring%20clean%20and%20consistent%20inputs%20for%20policy%20learning.%20To%20evaluate%20the%20effectiveness%20of%20S2GS%2C%20we%20adopt%20Diffusion%20Policy%20as%20the%20downstream%20learning%20algorithm%20and%20conduct%20experiments%20in%20the%20ManiSkill%20simulation%20environment%2C%20followed%20by%20real-world%20deployment.%20Results%20demonstrate%20that%20S2GS%20significantly%20improves%20sim-to-real%20transferability%2C%20maintaining%20high%20and%20stable%20task%20performance%20in%20real-world%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.04731v1&entry.124074799=Read"},
{"title": "Random Feature Spiking Neural Networks", "author": "Maximilian Gollwitzer and Felix Dietrich", "abstract": "Spiking Neural Networks (SNNs) as Machine Learning (ML) models have recently received a lot of attention as a potentially more energy-efficient alternative to conventional Artificial Neural Networks. The non-differentiability and sparsity of the spiking mechanism can make these models very difficult to train with algorithms based on propagating gradients through the spiking non-linearity. We address this problem by adapting the paradigm of Random Feature Methods (RFMs) from Artificial Neural Networks (ANNs) to Spike Response Model (SRM) SNNs. This approach allows training of SNNs without approximation of the spike function gradient. Concretely, we propose a novel data-driven, fast, high-performance, and interpretable algorithm for end-to-end training of SNNs inspired by the SWIM algorithm for RFM-ANNs, which we coin S-SWIM. We provide a thorough theoretical discussion and supplementary numerical experiments showing that S-SWIM can reach high accuracies on time series forecasting as a standalone strategy and serve as an effective initialisation strategy before gradient-based training. Additional ablation studies show that our proposed method performs better than random sampling of network weights.", "link": "http://arxiv.org/abs/2510.01012v2", "date": "2025-12-04", "relevancy": 2.3491, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4989}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4561}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Feature%20Spiking%20Neural%20Networks&body=Title%3A%20Random%20Feature%20Spiking%20Neural%20Networks%0AAuthor%3A%20Maximilian%20Gollwitzer%20and%20Felix%20Dietrich%0AAbstract%3A%20Spiking%20Neural%20Networks%20%28SNNs%29%20as%20Machine%20Learning%20%28ML%29%20models%20have%20recently%20received%20a%20lot%20of%20attention%20as%20a%20potentially%20more%20energy-efficient%20alternative%20to%20conventional%20Artificial%20Neural%20Networks.%20The%20non-differentiability%20and%20sparsity%20of%20the%20spiking%20mechanism%20can%20make%20these%20models%20very%20difficult%20to%20train%20with%20algorithms%20based%20on%20propagating%20gradients%20through%20the%20spiking%20non-linearity.%20We%20address%20this%20problem%20by%20adapting%20the%20paradigm%20of%20Random%20Feature%20Methods%20%28RFMs%29%20from%20Artificial%20Neural%20Networks%20%28ANNs%29%20to%20Spike%20Response%20Model%20%28SRM%29%20SNNs.%20This%20approach%20allows%20training%20of%20SNNs%20without%20approximation%20of%20the%20spike%20function%20gradient.%20Concretely%2C%20we%20propose%20a%20novel%20data-driven%2C%20fast%2C%20high-performance%2C%20and%20interpretable%20algorithm%20for%20end-to-end%20training%20of%20SNNs%20inspired%20by%20the%20SWIM%20algorithm%20for%20RFM-ANNs%2C%20which%20we%20coin%20S-SWIM.%20We%20provide%20a%20thorough%20theoretical%20discussion%20and%20supplementary%20numerical%20experiments%20showing%20that%20S-SWIM%20can%20reach%20high%20accuracies%20on%20time%20series%20forecasting%20as%20a%20standalone%20strategy%20and%20serve%20as%20an%20effective%20initialisation%20strategy%20before%20gradient-based%20training.%20Additional%20ablation%20studies%20show%20that%20our%20proposed%20method%20performs%20better%20than%20random%20sampling%20of%20network%20weights.%0ALink%3A%20http%3A//arxiv.org/abs/2510.01012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Feature%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DMaximilian%2520Gollwitzer%2520and%2520Felix%2520Dietrich%26entry.1292438233%3DSpiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520as%2520Machine%2520Learning%2520%2528ML%2529%2520models%2520have%2520recently%2520received%2520a%2520lot%2520of%2520attention%2520as%2520a%2520potentially%2520more%2520energy-efficient%2520alternative%2520to%2520conventional%2520Artificial%2520Neural%2520Networks.%2520The%2520non-differentiability%2520and%2520sparsity%2520of%2520the%2520spiking%2520mechanism%2520can%2520make%2520these%2520models%2520very%2520difficult%2520to%2520train%2520with%2520algorithms%2520based%2520on%2520propagating%2520gradients%2520through%2520the%2520spiking%2520non-linearity.%2520We%2520address%2520this%2520problem%2520by%2520adapting%2520the%2520paradigm%2520of%2520Random%2520Feature%2520Methods%2520%2528RFMs%2529%2520from%2520Artificial%2520Neural%2520Networks%2520%2528ANNs%2529%2520to%2520Spike%2520Response%2520Model%2520%2528SRM%2529%2520SNNs.%2520This%2520approach%2520allows%2520training%2520of%2520SNNs%2520without%2520approximation%2520of%2520the%2520spike%2520function%2520gradient.%2520Concretely%252C%2520we%2520propose%2520a%2520novel%2520data-driven%252C%2520fast%252C%2520high-performance%252C%2520and%2520interpretable%2520algorithm%2520for%2520end-to-end%2520training%2520of%2520SNNs%2520inspired%2520by%2520the%2520SWIM%2520algorithm%2520for%2520RFM-ANNs%252C%2520which%2520we%2520coin%2520S-SWIM.%2520We%2520provide%2520a%2520thorough%2520theoretical%2520discussion%2520and%2520supplementary%2520numerical%2520experiments%2520showing%2520that%2520S-SWIM%2520can%2520reach%2520high%2520accuracies%2520on%2520time%2520series%2520forecasting%2520as%2520a%2520standalone%2520strategy%2520and%2520serve%2520as%2520an%2520effective%2520initialisation%2520strategy%2520before%2520gradient-based%2520training.%2520Additional%2520ablation%2520studies%2520show%2520that%2520our%2520proposed%2520method%2520performs%2520better%2520than%2520random%2520sampling%2520of%2520network%2520weights.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Feature%20Spiking%20Neural%20Networks&entry.906535625=Maximilian%20Gollwitzer%20and%20Felix%20Dietrich&entry.1292438233=Spiking%20Neural%20Networks%20%28SNNs%29%20as%20Machine%20Learning%20%28ML%29%20models%20have%20recently%20received%20a%20lot%20of%20attention%20as%20a%20potentially%20more%20energy-efficient%20alternative%20to%20conventional%20Artificial%20Neural%20Networks.%20The%20non-differentiability%20and%20sparsity%20of%20the%20spiking%20mechanism%20can%20make%20these%20models%20very%20difficult%20to%20train%20with%20algorithms%20based%20on%20propagating%20gradients%20through%20the%20spiking%20non-linearity.%20We%20address%20this%20problem%20by%20adapting%20the%20paradigm%20of%20Random%20Feature%20Methods%20%28RFMs%29%20from%20Artificial%20Neural%20Networks%20%28ANNs%29%20to%20Spike%20Response%20Model%20%28SRM%29%20SNNs.%20This%20approach%20allows%20training%20of%20SNNs%20without%20approximation%20of%20the%20spike%20function%20gradient.%20Concretely%2C%20we%20propose%20a%20novel%20data-driven%2C%20fast%2C%20high-performance%2C%20and%20interpretable%20algorithm%20for%20end-to-end%20training%20of%20SNNs%20inspired%20by%20the%20SWIM%20algorithm%20for%20RFM-ANNs%2C%20which%20we%20coin%20S-SWIM.%20We%20provide%20a%20thorough%20theoretical%20discussion%20and%20supplementary%20numerical%20experiments%20showing%20that%20S-SWIM%20can%20reach%20high%20accuracies%20on%20time%20series%20forecasting%20as%20a%20standalone%20strategy%20and%20serve%20as%20an%20effective%20initialisation%20strategy%20before%20gradient-based%20training.%20Additional%20ablation%20studies%20show%20that%20our%20proposed%20method%20performs%20better%20than%20random%20sampling%20of%20network%20weights.&entry.1838667208=http%3A//arxiv.org/abs/2510.01012v2&entry.124074799=Read"},
{"title": "From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders", "author": "Amy Winecoff and Kevin Klyman", "abstract": "Generative AI systems may pose serious risks to individuals vulnerable to eating disorders. Existing safeguards tend to overlook subtle but clinically significant cues, leaving many risks unaddressed. To better understand the nature of these risks, we conducted semi-structured interviews with 15 clinicians, researchers, and advocates with expertise in eating disorders. Using abductive qualitative analysis, we developed an expert-guided taxonomy of generative AI risks across seven categories: (1) providing generalized health advice; (2) encouraging disordered behaviors; (3) supporting symptom concealment; (4) creating thinspiration; (5) reinforcing negative self-beliefs; (6) promoting excessive focus on the body; and (7) perpetuating narrow views about eating disorders. Our results demonstrate how certain user interactions with generative AI systems intersect with clinical features of eating disorders in ways that may intensify risk. We discuss implications of our work, including approaches for risk assessment, safeguard design, and participatory evaluation practices with domain experts.", "link": "http://arxiv.org/abs/2512.04843v1", "date": "2025-12-04", "relevancy": 2.3431, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.471}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4707}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Symptoms%20to%20Systems%3A%20An%20Expert-Guided%20Approach%20to%20Understanding%20Risks%20of%20Generative%20AI%20for%20Eating%20Disorders&body=Title%3A%20From%20Symptoms%20to%20Systems%3A%20An%20Expert-Guided%20Approach%20to%20Understanding%20Risks%20of%20Generative%20AI%20for%20Eating%20Disorders%0AAuthor%3A%20Amy%20Winecoff%20and%20Kevin%20Klyman%0AAbstract%3A%20Generative%20AI%20systems%20may%20pose%20serious%20risks%20to%20individuals%20vulnerable%20to%20eating%20disorders.%20Existing%20safeguards%20tend%20to%20overlook%20subtle%20but%20clinically%20significant%20cues%2C%20leaving%20many%20risks%20unaddressed.%20To%20better%20understand%20the%20nature%20of%20these%20risks%2C%20we%20conducted%20semi-structured%20interviews%20with%2015%20clinicians%2C%20researchers%2C%20and%20advocates%20with%20expertise%20in%20eating%20disorders.%20Using%20abductive%20qualitative%20analysis%2C%20we%20developed%20an%20expert-guided%20taxonomy%20of%20generative%20AI%20risks%20across%20seven%20categories%3A%20%281%29%20providing%20generalized%20health%20advice%3B%20%282%29%20encouraging%20disordered%20behaviors%3B%20%283%29%20supporting%20symptom%20concealment%3B%20%284%29%20creating%20thinspiration%3B%20%285%29%20reinforcing%20negative%20self-beliefs%3B%20%286%29%20promoting%20excessive%20focus%20on%20the%20body%3B%20and%20%287%29%20perpetuating%20narrow%20views%20about%20eating%20disorders.%20Our%20results%20demonstrate%20how%20certain%20user%20interactions%20with%20generative%20AI%20systems%20intersect%20with%20clinical%20features%20of%20eating%20disorders%20in%20ways%20that%20may%20intensify%20risk.%20We%20discuss%20implications%20of%20our%20work%2C%20including%20approaches%20for%20risk%20assessment%2C%20safeguard%20design%2C%20and%20participatory%20evaluation%20practices%20with%20domain%20experts.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Symptoms%2520to%2520Systems%253A%2520An%2520Expert-Guided%2520Approach%2520to%2520Understanding%2520Risks%2520of%2520Generative%2520AI%2520for%2520Eating%2520Disorders%26entry.906535625%3DAmy%2520Winecoff%2520and%2520Kevin%2520Klyman%26entry.1292438233%3DGenerative%2520AI%2520systems%2520may%2520pose%2520serious%2520risks%2520to%2520individuals%2520vulnerable%2520to%2520eating%2520disorders.%2520Existing%2520safeguards%2520tend%2520to%2520overlook%2520subtle%2520but%2520clinically%2520significant%2520cues%252C%2520leaving%2520many%2520risks%2520unaddressed.%2520To%2520better%2520understand%2520the%2520nature%2520of%2520these%2520risks%252C%2520we%2520conducted%2520semi-structured%2520interviews%2520with%252015%2520clinicians%252C%2520researchers%252C%2520and%2520advocates%2520with%2520expertise%2520in%2520eating%2520disorders.%2520Using%2520abductive%2520qualitative%2520analysis%252C%2520we%2520developed%2520an%2520expert-guided%2520taxonomy%2520of%2520generative%2520AI%2520risks%2520across%2520seven%2520categories%253A%2520%25281%2529%2520providing%2520generalized%2520health%2520advice%253B%2520%25282%2529%2520encouraging%2520disordered%2520behaviors%253B%2520%25283%2529%2520supporting%2520symptom%2520concealment%253B%2520%25284%2529%2520creating%2520thinspiration%253B%2520%25285%2529%2520reinforcing%2520negative%2520self-beliefs%253B%2520%25286%2529%2520promoting%2520excessive%2520focus%2520on%2520the%2520body%253B%2520and%2520%25287%2529%2520perpetuating%2520narrow%2520views%2520about%2520eating%2520disorders.%2520Our%2520results%2520demonstrate%2520how%2520certain%2520user%2520interactions%2520with%2520generative%2520AI%2520systems%2520intersect%2520with%2520clinical%2520features%2520of%2520eating%2520disorders%2520in%2520ways%2520that%2520may%2520intensify%2520risk.%2520We%2520discuss%2520implications%2520of%2520our%2520work%252C%2520including%2520approaches%2520for%2520risk%2520assessment%252C%2520safeguard%2520design%252C%2520and%2520participatory%2520evaluation%2520practices%2520with%2520domain%2520experts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Symptoms%20to%20Systems%3A%20An%20Expert-Guided%20Approach%20to%20Understanding%20Risks%20of%20Generative%20AI%20for%20Eating%20Disorders&entry.906535625=Amy%20Winecoff%20and%20Kevin%20Klyman&entry.1292438233=Generative%20AI%20systems%20may%20pose%20serious%20risks%20to%20individuals%20vulnerable%20to%20eating%20disorders.%20Existing%20safeguards%20tend%20to%20overlook%20subtle%20but%20clinically%20significant%20cues%2C%20leaving%20many%20risks%20unaddressed.%20To%20better%20understand%20the%20nature%20of%20these%20risks%2C%20we%20conducted%20semi-structured%20interviews%20with%2015%20clinicians%2C%20researchers%2C%20and%20advocates%20with%20expertise%20in%20eating%20disorders.%20Using%20abductive%20qualitative%20analysis%2C%20we%20developed%20an%20expert-guided%20taxonomy%20of%20generative%20AI%20risks%20across%20seven%20categories%3A%20%281%29%20providing%20generalized%20health%20advice%3B%20%282%29%20encouraging%20disordered%20behaviors%3B%20%283%29%20supporting%20symptom%20concealment%3B%20%284%29%20creating%20thinspiration%3B%20%285%29%20reinforcing%20negative%20self-beliefs%3B%20%286%29%20promoting%20excessive%20focus%20on%20the%20body%3B%20and%20%287%29%20perpetuating%20narrow%20views%20about%20eating%20disorders.%20Our%20results%20demonstrate%20how%20certain%20user%20interactions%20with%20generative%20AI%20systems%20intersect%20with%20clinical%20features%20of%20eating%20disorders%20in%20ways%20that%20may%20intensify%20risk.%20We%20discuss%20implications%20of%20our%20work%2C%20including%20approaches%20for%20risk%20assessment%2C%20safeguard%20design%2C%20and%20participatory%20evaluation%20practices%20with%20domain%20experts.&entry.1838667208=http%3A//arxiv.org/abs/2512.04843v1&entry.124074799=Read"},
{"title": "Joint Discriminative-Generative Modeling via Dual Adversarial Training", "author": "Xuwang Yin and Claire Zhang and Julie Steele and Nir Shavit and Tony T. Wang", "abstract": "Simultaneously achieving robust classification and high-fidelity generative modeling within a single framework presents a significant challenge. Hybrid approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as EBMs but are often limited by the instability and poor sample quality inherent in Stochastic Gradient Langevin Dynamics (SGLD)-based training. We address these limitations by proposing a novel training framework that integrates adversarial training (AT) principles for both discriminative robustness and stable generative learning. The proposed method introduces three key innovations: (1) the replacement of SGLD-based JEM learning with a stable, AT-based approach that optimizes the energy function by discriminating between real data and Projected Gradient Descent (PGD)-generated contrastive samples using the BCE loss; (2) synergistic adversarial training for the discriminative component that enhances classification robustness while eliminating the need for explicit gradient penalties; and (3) a two-stage training strategy that addresses normalization-related instabilities and enables leveraging pretrained robust classifiers, generalizing effectively across diverse architectures. Experiments on CIFAR-10/100 and ImageNet demonstrate that our approach: (1) is the first EBM-based hybrid to scale to high-resolution datasets with high training stability, simultaneously achieving state-of-the-art discriminative and generative performance on ImageNet 256$\\times$256; (2) uniquely combines generative quality with adversarial robustness, enabling critical applications like robust counterfactual explanations; and (3) functions as a competitive standalone generative model, matching the generative quality of autoregressive methods (VAR-d16) and surpassing diffusion models while offering unique versatility.", "link": "http://arxiv.org/abs/2510.13872v2", "date": "2025-12-04", "relevancy": 2.3357, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5977}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5758}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Discriminative-Generative%20Modeling%20via%20Dual%20Adversarial%20Training&body=Title%3A%20Joint%20Discriminative-Generative%20Modeling%20via%20Dual%20Adversarial%20Training%0AAuthor%3A%20Xuwang%20Yin%20and%20Claire%20Zhang%20and%20Julie%20Steele%20and%20Nir%20Shavit%20and%20Tony%20T.%20Wang%0AAbstract%3A%20Simultaneously%20achieving%20robust%20classification%20and%20high-fidelity%20generative%20modeling%20within%20a%20single%20framework%20presents%20a%20significant%20challenge.%20Hybrid%20approaches%2C%20such%20as%20Joint%20Energy-Based%20Models%20%28JEM%29%2C%20interpret%20classifiers%20as%20EBMs%20but%20are%20often%20limited%20by%20the%20instability%20and%20poor%20sample%20quality%20inherent%20in%20Stochastic%20Gradient%20Langevin%20Dynamics%20%28SGLD%29-based%20training.%20We%20address%20these%20limitations%20by%20proposing%20a%20novel%20training%20framework%20that%20integrates%20adversarial%20training%20%28AT%29%20principles%20for%20both%20discriminative%20robustness%20and%20stable%20generative%20learning.%20The%20proposed%20method%20introduces%20three%20key%20innovations%3A%20%281%29%20the%20replacement%20of%20SGLD-based%20JEM%20learning%20with%20a%20stable%2C%20AT-based%20approach%20that%20optimizes%20the%20energy%20function%20by%20discriminating%20between%20real%20data%20and%20Projected%20Gradient%20Descent%20%28PGD%29-generated%20contrastive%20samples%20using%20the%20BCE%20loss%3B%20%282%29%20synergistic%20adversarial%20training%20for%20the%20discriminative%20component%20that%20enhances%20classification%20robustness%20while%20eliminating%20the%20need%20for%20explicit%20gradient%20penalties%3B%20and%20%283%29%20a%20two-stage%20training%20strategy%20that%20addresses%20normalization-related%20instabilities%20and%20enables%20leveraging%20pretrained%20robust%20classifiers%2C%20generalizing%20effectively%20across%20diverse%20architectures.%20Experiments%20on%20CIFAR-10/100%20and%20ImageNet%20demonstrate%20that%20our%20approach%3A%20%281%29%20is%20the%20first%20EBM-based%20hybrid%20to%20scale%20to%20high-resolution%20datasets%20with%20high%20training%20stability%2C%20simultaneously%20achieving%20state-of-the-art%20discriminative%20and%20generative%20performance%20on%20ImageNet%20256%24%5Ctimes%24256%3B%20%282%29%20uniquely%20combines%20generative%20quality%20with%20adversarial%20robustness%2C%20enabling%20critical%20applications%20like%20robust%20counterfactual%20explanations%3B%20and%20%283%29%20functions%20as%20a%20competitive%20standalone%20generative%20model%2C%20matching%20the%20generative%20quality%20of%20autoregressive%20methods%20%28VAR-d16%29%20and%20surpassing%20diffusion%20models%20while%20offering%20unique%20versatility.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13872v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Discriminative-Generative%2520Modeling%2520via%2520Dual%2520Adversarial%2520Training%26entry.906535625%3DXuwang%2520Yin%2520and%2520Claire%2520Zhang%2520and%2520Julie%2520Steele%2520and%2520Nir%2520Shavit%2520and%2520Tony%2520T.%2520Wang%26entry.1292438233%3DSimultaneously%2520achieving%2520robust%2520classification%2520and%2520high-fidelity%2520generative%2520modeling%2520within%2520a%2520single%2520framework%2520presents%2520a%2520significant%2520challenge.%2520Hybrid%2520approaches%252C%2520such%2520as%2520Joint%2520Energy-Based%2520Models%2520%2528JEM%2529%252C%2520interpret%2520classifiers%2520as%2520EBMs%2520but%2520are%2520often%2520limited%2520by%2520the%2520instability%2520and%2520poor%2520sample%2520quality%2520inherent%2520in%2520Stochastic%2520Gradient%2520Langevin%2520Dynamics%2520%2528SGLD%2529-based%2520training.%2520We%2520address%2520these%2520limitations%2520by%2520proposing%2520a%2520novel%2520training%2520framework%2520that%2520integrates%2520adversarial%2520training%2520%2528AT%2529%2520principles%2520for%2520both%2520discriminative%2520robustness%2520and%2520stable%2520generative%2520learning.%2520The%2520proposed%2520method%2520introduces%2520three%2520key%2520innovations%253A%2520%25281%2529%2520the%2520replacement%2520of%2520SGLD-based%2520JEM%2520learning%2520with%2520a%2520stable%252C%2520AT-based%2520approach%2520that%2520optimizes%2520the%2520energy%2520function%2520by%2520discriminating%2520between%2520real%2520data%2520and%2520Projected%2520Gradient%2520Descent%2520%2528PGD%2529-generated%2520contrastive%2520samples%2520using%2520the%2520BCE%2520loss%253B%2520%25282%2529%2520synergistic%2520adversarial%2520training%2520for%2520the%2520discriminative%2520component%2520that%2520enhances%2520classification%2520robustness%2520while%2520eliminating%2520the%2520need%2520for%2520explicit%2520gradient%2520penalties%253B%2520and%2520%25283%2529%2520a%2520two-stage%2520training%2520strategy%2520that%2520addresses%2520normalization-related%2520instabilities%2520and%2520enables%2520leveraging%2520pretrained%2520robust%2520classifiers%252C%2520generalizing%2520effectively%2520across%2520diverse%2520architectures.%2520Experiments%2520on%2520CIFAR-10/100%2520and%2520ImageNet%2520demonstrate%2520that%2520our%2520approach%253A%2520%25281%2529%2520is%2520the%2520first%2520EBM-based%2520hybrid%2520to%2520scale%2520to%2520high-resolution%2520datasets%2520with%2520high%2520training%2520stability%252C%2520simultaneously%2520achieving%2520state-of-the-art%2520discriminative%2520and%2520generative%2520performance%2520on%2520ImageNet%2520256%2524%255Ctimes%2524256%253B%2520%25282%2529%2520uniquely%2520combines%2520generative%2520quality%2520with%2520adversarial%2520robustness%252C%2520enabling%2520critical%2520applications%2520like%2520robust%2520counterfactual%2520explanations%253B%2520and%2520%25283%2529%2520functions%2520as%2520a%2520competitive%2520standalone%2520generative%2520model%252C%2520matching%2520the%2520generative%2520quality%2520of%2520autoregressive%2520methods%2520%2528VAR-d16%2529%2520and%2520surpassing%2520diffusion%2520models%2520while%2520offering%2520unique%2520versatility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13872v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Discriminative-Generative%20Modeling%20via%20Dual%20Adversarial%20Training&entry.906535625=Xuwang%20Yin%20and%20Claire%20Zhang%20and%20Julie%20Steele%20and%20Nir%20Shavit%20and%20Tony%20T.%20Wang&entry.1292438233=Simultaneously%20achieving%20robust%20classification%20and%20high-fidelity%20generative%20modeling%20within%20a%20single%20framework%20presents%20a%20significant%20challenge.%20Hybrid%20approaches%2C%20such%20as%20Joint%20Energy-Based%20Models%20%28JEM%29%2C%20interpret%20classifiers%20as%20EBMs%20but%20are%20often%20limited%20by%20the%20instability%20and%20poor%20sample%20quality%20inherent%20in%20Stochastic%20Gradient%20Langevin%20Dynamics%20%28SGLD%29-based%20training.%20We%20address%20these%20limitations%20by%20proposing%20a%20novel%20training%20framework%20that%20integrates%20adversarial%20training%20%28AT%29%20principles%20for%20both%20discriminative%20robustness%20and%20stable%20generative%20learning.%20The%20proposed%20method%20introduces%20three%20key%20innovations%3A%20%281%29%20the%20replacement%20of%20SGLD-based%20JEM%20learning%20with%20a%20stable%2C%20AT-based%20approach%20that%20optimizes%20the%20energy%20function%20by%20discriminating%20between%20real%20data%20and%20Projected%20Gradient%20Descent%20%28PGD%29-generated%20contrastive%20samples%20using%20the%20BCE%20loss%3B%20%282%29%20synergistic%20adversarial%20training%20for%20the%20discriminative%20component%20that%20enhances%20classification%20robustness%20while%20eliminating%20the%20need%20for%20explicit%20gradient%20penalties%3B%20and%20%283%29%20a%20two-stage%20training%20strategy%20that%20addresses%20normalization-related%20instabilities%20and%20enables%20leveraging%20pretrained%20robust%20classifiers%2C%20generalizing%20effectively%20across%20diverse%20architectures.%20Experiments%20on%20CIFAR-10/100%20and%20ImageNet%20demonstrate%20that%20our%20approach%3A%20%281%29%20is%20the%20first%20EBM-based%20hybrid%20to%20scale%20to%20high-resolution%20datasets%20with%20high%20training%20stability%2C%20simultaneously%20achieving%20state-of-the-art%20discriminative%20and%20generative%20performance%20on%20ImageNet%20256%24%5Ctimes%24256%3B%20%282%29%20uniquely%20combines%20generative%20quality%20with%20adversarial%20robustness%2C%20enabling%20critical%20applications%20like%20robust%20counterfactual%20explanations%3B%20and%20%283%29%20functions%20as%20a%20competitive%20standalone%20generative%20model%2C%20matching%20the%20generative%20quality%20of%20autoregressive%20methods%20%28VAR-d16%29%20and%20surpassing%20diffusion%20models%20while%20offering%20unique%20versatility.&entry.1838667208=http%3A//arxiv.org/abs/2510.13872v2&entry.124074799=Read"},
{"title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution Can Improve Robust Scene Graph Generation", "author": "Changsheng Lv and Zijian Fu and Mengshi Qi", "abstract": "In this paper, we propose Robo-SGG, a plug-and-play module for robust scene graph generation (SGG). Unlike standard SGG, the robust scene graph generation aims to perform inference on a diverse range of corrupted images, with the core challenge being the domain shift between the clean and corrupted images. Existing SGG methods suffer from degraded performance due to shifted visual features (e.g., corruption interference or occlusions). To obtain robust visual features, we leverage layout information, representing the global structure of an image, which is robust to domain shift, to enhance the robustness of SGG methods under corruption. Specifically, we employ Instance Normalization (IN) to alleviate the domain-specific variations and recover the robust structural features (i.e., the positional and semantic relationships among objects) by the proposed Layout-Oriented Restitution. Furthermore, under corrupted images, we introduce a Layout-Embedded Encoder (LEE) that adaptively fuses layout and visual features via a gating mechanism, enhancing the robustness of positional and semantic representations for objects and predicates. Note that our proposed Robo-SGG module is designed as a plug-and-play component, which can be easily integrated into any baseline SGG model. Extensive experiments demonstrate that by integrating the state-of-the-art method into our proposed Robo-SGG, we achieve relative improvements of 6.3%, 11.1%, and 8.0% in mR@50 for PredCls, SGCls, and SGDet tasks on the VG-C benchmark, respectively, and achieve new state-of-the-art performance in the corruption scene graph generation benchmark (VG-C and GQA-C). We will release our source code and model.", "link": "http://arxiv.org/abs/2504.12606v2", "date": "2025-12-04", "relevancy": 2.3075, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6097}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5746}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robo-SGG%3A%20Exploiting%20Layout-Oriented%20Normalization%20and%20Restitution%20Can%20Improve%20Robust%20Scene%20Graph%20Generation&body=Title%3A%20Robo-SGG%3A%20Exploiting%20Layout-Oriented%20Normalization%20and%20Restitution%20Can%20Improve%20Robust%20Scene%20Graph%20Generation%0AAuthor%3A%20Changsheng%20Lv%20and%20Zijian%20Fu%20and%20Mengshi%20Qi%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20Robo-SGG%2C%20a%20plug-and-play%20module%20for%20robust%20scene%20graph%20generation%20%28SGG%29.%20Unlike%20standard%20SGG%2C%20the%20robust%20scene%20graph%20generation%20aims%20to%20perform%20inference%20on%20a%20diverse%20range%20of%20corrupted%20images%2C%20with%20the%20core%20challenge%20being%20the%20domain%20shift%20between%20the%20clean%20and%20corrupted%20images.%20Existing%20SGG%20methods%20suffer%20from%20degraded%20performance%20due%20to%20shifted%20visual%20features%20%28e.g.%2C%20corruption%20interference%20or%20occlusions%29.%20To%20obtain%20robust%20visual%20features%2C%20we%20leverage%20layout%20information%2C%20representing%20the%20global%20structure%20of%20an%20image%2C%20which%20is%20robust%20to%20domain%20shift%2C%20to%20enhance%20the%20robustness%20of%20SGG%20methods%20under%20corruption.%20Specifically%2C%20we%20employ%20Instance%20Normalization%20%28IN%29%20to%20alleviate%20the%20domain-specific%20variations%20and%20recover%20the%20robust%20structural%20features%20%28i.e.%2C%20the%20positional%20and%20semantic%20relationships%20among%20objects%29%20by%20the%20proposed%20Layout-Oriented%20Restitution.%20Furthermore%2C%20under%20corrupted%20images%2C%20we%20introduce%20a%20Layout-Embedded%20Encoder%20%28LEE%29%20that%20adaptively%20fuses%20layout%20and%20visual%20features%20via%20a%20gating%20mechanism%2C%20enhancing%20the%20robustness%20of%20positional%20and%20semantic%20representations%20for%20objects%20and%20predicates.%20Note%20that%20our%20proposed%20Robo-SGG%20module%20is%20designed%20as%20a%20plug-and-play%20component%2C%20which%20can%20be%20easily%20integrated%20into%20any%20baseline%20SGG%20model.%20Extensive%20experiments%20demonstrate%20that%20by%20integrating%20the%20state-of-the-art%20method%20into%20our%20proposed%20Robo-SGG%2C%20we%20achieve%20relative%20improvements%20of%206.3%25%2C%2011.1%25%2C%20and%208.0%25%20in%20mR%4050%20for%20PredCls%2C%20SGCls%2C%20and%20SGDet%20tasks%20on%20the%20VG-C%20benchmark%2C%20respectively%2C%20and%20achieve%20new%20state-of-the-art%20performance%20in%20the%20corruption%20scene%20graph%20generation%20benchmark%20%28VG-C%20and%20GQA-C%29.%20We%20will%20release%20our%20source%20code%20and%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2504.12606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobo-SGG%253A%2520Exploiting%2520Layout-Oriented%2520Normalization%2520and%2520Restitution%2520Can%2520Improve%2520Robust%2520Scene%2520Graph%2520Generation%26entry.906535625%3DChangsheng%2520Lv%2520and%2520Zijian%2520Fu%2520and%2520Mengshi%2520Qi%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520Robo-SGG%252C%2520a%2520plug-and-play%2520module%2520for%2520robust%2520scene%2520graph%2520generation%2520%2528SGG%2529.%2520Unlike%2520standard%2520SGG%252C%2520the%2520robust%2520scene%2520graph%2520generation%2520aims%2520to%2520perform%2520inference%2520on%2520a%2520diverse%2520range%2520of%2520corrupted%2520images%252C%2520with%2520the%2520core%2520challenge%2520being%2520the%2520domain%2520shift%2520between%2520the%2520clean%2520and%2520corrupted%2520images.%2520Existing%2520SGG%2520methods%2520suffer%2520from%2520degraded%2520performance%2520due%2520to%2520shifted%2520visual%2520features%2520%2528e.g.%252C%2520corruption%2520interference%2520or%2520occlusions%2529.%2520To%2520obtain%2520robust%2520visual%2520features%252C%2520we%2520leverage%2520layout%2520information%252C%2520representing%2520the%2520global%2520structure%2520of%2520an%2520image%252C%2520which%2520is%2520robust%2520to%2520domain%2520shift%252C%2520to%2520enhance%2520the%2520robustness%2520of%2520SGG%2520methods%2520under%2520corruption.%2520Specifically%252C%2520we%2520employ%2520Instance%2520Normalization%2520%2528IN%2529%2520to%2520alleviate%2520the%2520domain-specific%2520variations%2520and%2520recover%2520the%2520robust%2520structural%2520features%2520%2528i.e.%252C%2520the%2520positional%2520and%2520semantic%2520relationships%2520among%2520objects%2529%2520by%2520the%2520proposed%2520Layout-Oriented%2520Restitution.%2520Furthermore%252C%2520under%2520corrupted%2520images%252C%2520we%2520introduce%2520a%2520Layout-Embedded%2520Encoder%2520%2528LEE%2529%2520that%2520adaptively%2520fuses%2520layout%2520and%2520visual%2520features%2520via%2520a%2520gating%2520mechanism%252C%2520enhancing%2520the%2520robustness%2520of%2520positional%2520and%2520semantic%2520representations%2520for%2520objects%2520and%2520predicates.%2520Note%2520that%2520our%2520proposed%2520Robo-SGG%2520module%2520is%2520designed%2520as%2520a%2520plug-and-play%2520component%252C%2520which%2520can%2520be%2520easily%2520integrated%2520into%2520any%2520baseline%2520SGG%2520model.%2520Extensive%2520experiments%2520demonstrate%2520that%2520by%2520integrating%2520the%2520state-of-the-art%2520method%2520into%2520our%2520proposed%2520Robo-SGG%252C%2520we%2520achieve%2520relative%2520improvements%2520of%25206.3%2525%252C%252011.1%2525%252C%2520and%25208.0%2525%2520in%2520mR%254050%2520for%2520PredCls%252C%2520SGCls%252C%2520and%2520SGDet%2520tasks%2520on%2520the%2520VG-C%2520benchmark%252C%2520respectively%252C%2520and%2520achieve%2520new%2520state-of-the-art%2520performance%2520in%2520the%2520corruption%2520scene%2520graph%2520generation%2520benchmark%2520%2528VG-C%2520and%2520GQA-C%2529.%2520We%2520will%2520release%2520our%2520source%2520code%2520and%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robo-SGG%3A%20Exploiting%20Layout-Oriented%20Normalization%20and%20Restitution%20Can%20Improve%20Robust%20Scene%20Graph%20Generation&entry.906535625=Changsheng%20Lv%20and%20Zijian%20Fu%20and%20Mengshi%20Qi&entry.1292438233=In%20this%20paper%2C%20we%20propose%20Robo-SGG%2C%20a%20plug-and-play%20module%20for%20robust%20scene%20graph%20generation%20%28SGG%29.%20Unlike%20standard%20SGG%2C%20the%20robust%20scene%20graph%20generation%20aims%20to%20perform%20inference%20on%20a%20diverse%20range%20of%20corrupted%20images%2C%20with%20the%20core%20challenge%20being%20the%20domain%20shift%20between%20the%20clean%20and%20corrupted%20images.%20Existing%20SGG%20methods%20suffer%20from%20degraded%20performance%20due%20to%20shifted%20visual%20features%20%28e.g.%2C%20corruption%20interference%20or%20occlusions%29.%20To%20obtain%20robust%20visual%20features%2C%20we%20leverage%20layout%20information%2C%20representing%20the%20global%20structure%20of%20an%20image%2C%20which%20is%20robust%20to%20domain%20shift%2C%20to%20enhance%20the%20robustness%20of%20SGG%20methods%20under%20corruption.%20Specifically%2C%20we%20employ%20Instance%20Normalization%20%28IN%29%20to%20alleviate%20the%20domain-specific%20variations%20and%20recover%20the%20robust%20structural%20features%20%28i.e.%2C%20the%20positional%20and%20semantic%20relationships%20among%20objects%29%20by%20the%20proposed%20Layout-Oriented%20Restitution.%20Furthermore%2C%20under%20corrupted%20images%2C%20we%20introduce%20a%20Layout-Embedded%20Encoder%20%28LEE%29%20that%20adaptively%20fuses%20layout%20and%20visual%20features%20via%20a%20gating%20mechanism%2C%20enhancing%20the%20robustness%20of%20positional%20and%20semantic%20representations%20for%20objects%20and%20predicates.%20Note%20that%20our%20proposed%20Robo-SGG%20module%20is%20designed%20as%20a%20plug-and-play%20component%2C%20which%20can%20be%20easily%20integrated%20into%20any%20baseline%20SGG%20model.%20Extensive%20experiments%20demonstrate%20that%20by%20integrating%20the%20state-of-the-art%20method%20into%20our%20proposed%20Robo-SGG%2C%20we%20achieve%20relative%20improvements%20of%206.3%25%2C%2011.1%25%2C%20and%208.0%25%20in%20mR%4050%20for%20PredCls%2C%20SGCls%2C%20and%20SGDet%20tasks%20on%20the%20VG-C%20benchmark%2C%20respectively%2C%20and%20achieve%20new%20state-of-the-art%20performance%20in%20the%20corruption%20scene%20graph%20generation%20benchmark%20%28VG-C%20and%20GQA-C%29.%20We%20will%20release%20our%20source%20code%20and%20model.&entry.1838667208=http%3A//arxiv.org/abs/2504.12606v2&entry.124074799=Read"},
{"title": "Rethinking the Use of Vision Transformers for AI-Generated Image Detection", "author": "NaHyeon Park and Kunhee Kim and Junsuk Choe and Hyunjung Shim", "abstract": "Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.", "link": "http://arxiv.org/abs/2512.04969v1", "date": "2025-12-04", "relevancy": 2.2981, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5963}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5765}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20Use%20of%20Vision%20Transformers%20for%20AI-Generated%20Image%20Detection&body=Title%3A%20Rethinking%20the%20Use%20of%20Vision%20Transformers%20for%20AI-Generated%20Image%20Detection%0AAuthor%3A%20NaHyeon%20Park%20and%20Kunhee%20Kim%20and%20Junsuk%20Choe%20and%20Hyunjung%20Shim%0AAbstract%3A%20Rich%20feature%20representations%20derived%20from%20CLIP-ViT%20have%20been%20widely%20utilized%20in%20AI-generated%20image%20detection.%20While%20most%20existing%20methods%20primarily%20leverage%20features%20from%20the%20final%20layer%2C%20we%20systematically%20analyze%20the%20contributions%20of%20layer-wise%20features%20to%20this%20task.%20Our%20study%20reveals%20that%20earlier%20layers%20provide%20more%20localized%20and%20generalizable%20features%2C%20often%20surpassing%20the%20performance%20of%20final-layer%20features%20in%20detection%20tasks.%20Moreover%2C%20we%20find%20that%20different%20layers%20capture%20distinct%20aspects%20of%20the%20data%2C%20each%20contributing%20uniquely%20to%20AI-generated%20image%20detection.%20Motivated%20by%20these%20findings%2C%20we%20introduce%20a%20novel%20adaptive%20method%2C%20termed%20MoLD%2C%20which%20dynamically%20integrates%20features%20from%20multiple%20ViT%20layers%20using%20a%20gating-based%20mechanism.%20Extensive%20experiments%20on%20both%20GAN-%20and%20diffusion-generated%20images%20demonstrate%20that%20MoLD%20significantly%20improves%20detection%20performance%2C%20enhances%20generalization%20across%20diverse%20generative%20models%2C%20and%20exhibits%20robustness%20in%20real-world%20scenarios.%20Finally%2C%20we%20illustrate%20the%20scalability%20and%20versatility%20of%20our%20approach%20by%20successfully%20applying%20it%20to%20other%20pre-trained%20ViTs%2C%20such%20as%20DINOv2.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520Use%2520of%2520Vision%2520Transformers%2520for%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DNaHyeon%2520Park%2520and%2520Kunhee%2520Kim%2520and%2520Junsuk%2520Choe%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3DRich%2520feature%2520representations%2520derived%2520from%2520CLIP-ViT%2520have%2520been%2520widely%2520utilized%2520in%2520AI-generated%2520image%2520detection.%2520While%2520most%2520existing%2520methods%2520primarily%2520leverage%2520features%2520from%2520the%2520final%2520layer%252C%2520we%2520systematically%2520analyze%2520the%2520contributions%2520of%2520layer-wise%2520features%2520to%2520this%2520task.%2520Our%2520study%2520reveals%2520that%2520earlier%2520layers%2520provide%2520more%2520localized%2520and%2520generalizable%2520features%252C%2520often%2520surpassing%2520the%2520performance%2520of%2520final-layer%2520features%2520in%2520detection%2520tasks.%2520Moreover%252C%2520we%2520find%2520that%2520different%2520layers%2520capture%2520distinct%2520aspects%2520of%2520the%2520data%252C%2520each%2520contributing%2520uniquely%2520to%2520AI-generated%2520image%2520detection.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520introduce%2520a%2520novel%2520adaptive%2520method%252C%2520termed%2520MoLD%252C%2520which%2520dynamically%2520integrates%2520features%2520from%2520multiple%2520ViT%2520layers%2520using%2520a%2520gating-based%2520mechanism.%2520Extensive%2520experiments%2520on%2520both%2520GAN-%2520and%2520diffusion-generated%2520images%2520demonstrate%2520that%2520MoLD%2520significantly%2520improves%2520detection%2520performance%252C%2520enhances%2520generalization%2520across%2520diverse%2520generative%2520models%252C%2520and%2520exhibits%2520robustness%2520in%2520real-world%2520scenarios.%2520Finally%252C%2520we%2520illustrate%2520the%2520scalability%2520and%2520versatility%2520of%2520our%2520approach%2520by%2520successfully%2520applying%2520it%2520to%2520other%2520pre-trained%2520ViTs%252C%2520such%2520as%2520DINOv2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20Use%20of%20Vision%20Transformers%20for%20AI-Generated%20Image%20Detection&entry.906535625=NaHyeon%20Park%20and%20Kunhee%20Kim%20and%20Junsuk%20Choe%20and%20Hyunjung%20Shim&entry.1292438233=Rich%20feature%20representations%20derived%20from%20CLIP-ViT%20have%20been%20widely%20utilized%20in%20AI-generated%20image%20detection.%20While%20most%20existing%20methods%20primarily%20leverage%20features%20from%20the%20final%20layer%2C%20we%20systematically%20analyze%20the%20contributions%20of%20layer-wise%20features%20to%20this%20task.%20Our%20study%20reveals%20that%20earlier%20layers%20provide%20more%20localized%20and%20generalizable%20features%2C%20often%20surpassing%20the%20performance%20of%20final-layer%20features%20in%20detection%20tasks.%20Moreover%2C%20we%20find%20that%20different%20layers%20capture%20distinct%20aspects%20of%20the%20data%2C%20each%20contributing%20uniquely%20to%20AI-generated%20image%20detection.%20Motivated%20by%20these%20findings%2C%20we%20introduce%20a%20novel%20adaptive%20method%2C%20termed%20MoLD%2C%20which%20dynamically%20integrates%20features%20from%20multiple%20ViT%20layers%20using%20a%20gating-based%20mechanism.%20Extensive%20experiments%20on%20both%20GAN-%20and%20diffusion-generated%20images%20demonstrate%20that%20MoLD%20significantly%20improves%20detection%20performance%2C%20enhances%20generalization%20across%20diverse%20generative%20models%2C%20and%20exhibits%20robustness%20in%20real-world%20scenarios.%20Finally%2C%20we%20illustrate%20the%20scalability%20and%20versatility%20of%20our%20approach%20by%20successfully%20applying%20it%20to%20other%20pre-trained%20ViTs%2C%20such%20as%20DINOv2.&entry.1838667208=http%3A//arxiv.org/abs/2512.04969v1&entry.124074799=Read"},
{"title": "MT-Depth: Multi-task Instance feature analysis for the Depth Completion", "author": "Abdul Haseeb Nizamani and Dandi Zhou and Xinhai Sun", "abstract": "Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.", "link": "http://arxiv.org/abs/2512.04734v1", "date": "2025-12-04", "relevancy": 2.2973, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5785}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5742}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MT-Depth%3A%20Multi-task%20Instance%20feature%20analysis%20for%20the%20Depth%20Completion&body=Title%3A%20MT-Depth%3A%20Multi-task%20Instance%20feature%20analysis%20for%20the%20Depth%20Completion%0AAuthor%3A%20Abdul%20Haseeb%20Nizamani%20and%20Dandi%20Zhou%20and%20Xinhai%20Sun%0AAbstract%3A%20Depth%20completion%20plays%20a%20vital%20role%20in%203D%20perception%20systems%2C%20especially%20in%20scenarios%20where%20sparse%20depth%20data%20must%20be%20densified%20for%20tasks%20such%20as%20autonomous%20driving%2C%20robotics%2C%20and%20augmented%20reality.%20While%20many%20existing%20approaches%20rely%20on%20semantic%20segmentation%20to%20guide%20depth%20completion%2C%20they%20often%20overlook%20the%20benefits%20of%20object-level%20understanding.%20In%20this%20work%2C%20we%20introduce%20an%20instance-aware%20depth%20completion%20framework%20that%20explicitly%20integrates%20binary%20instance%20masks%20as%20spatial%20priors%20to%20refine%20depth%20predictions.%20Our%20model%20combines%20four%20main%20components%3A%20a%20frozen%20YOLO%20V11%20instance%20segmentation%20branch%2C%20a%20U-Net-based%20depth%20completion%20backbone%2C%20a%20cross-attention%20fusion%20module%2C%20and%20an%20attention-guided%20prediction%20head.%20The%20instance%20segmentation%20branch%20generates%20per-image%20foreground%20masks%20that%20guide%20the%20depth%20branch%20via%20cross-attention%2C%20allowing%20the%20network%20to%20focus%20on%20object-centric%20regions%20during%20refinement.%20We%20validate%20our%20method%20on%20the%20Virtual%20KITTI%202%20dataset%2C%20showing%20that%20it%20achieves%20lower%20RMSE%20compared%20to%20both%20a%20U-Net-only%20baseline%20and%20previous%20semantic-guided%20methods%2C%20while%20maintaining%20competitive%20MAE.%20Qualitative%20and%20quantitative%20results%20demonstrate%20that%20the%20proposed%20model%20effectively%20enhances%20depth%20accuracy%20near%20object%20boundaries%2C%20occlusions%2C%20and%20thin%20structures.%20Our%20findings%20suggest%20that%20incorporating%20instance-aware%20cues%20offers%20a%20promising%20direction%20for%20improving%20depth%20completion%20without%20relying%20on%20dense%20semantic%20labels.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMT-Depth%253A%2520Multi-task%2520Instance%2520feature%2520analysis%2520for%2520the%2520Depth%2520Completion%26entry.906535625%3DAbdul%2520Haseeb%2520Nizamani%2520and%2520Dandi%2520Zhou%2520and%2520Xinhai%2520Sun%26entry.1292438233%3DDepth%2520completion%2520plays%2520a%2520vital%2520role%2520in%25203D%2520perception%2520systems%252C%2520especially%2520in%2520scenarios%2520where%2520sparse%2520depth%2520data%2520must%2520be%2520densified%2520for%2520tasks%2520such%2520as%2520autonomous%2520driving%252C%2520robotics%252C%2520and%2520augmented%2520reality.%2520While%2520many%2520existing%2520approaches%2520rely%2520on%2520semantic%2520segmentation%2520to%2520guide%2520depth%2520completion%252C%2520they%2520often%2520overlook%2520the%2520benefits%2520of%2520object-level%2520understanding.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520instance-aware%2520depth%2520completion%2520framework%2520that%2520explicitly%2520integrates%2520binary%2520instance%2520masks%2520as%2520spatial%2520priors%2520to%2520refine%2520depth%2520predictions.%2520Our%2520model%2520combines%2520four%2520main%2520components%253A%2520a%2520frozen%2520YOLO%2520V11%2520instance%2520segmentation%2520branch%252C%2520a%2520U-Net-based%2520depth%2520completion%2520backbone%252C%2520a%2520cross-attention%2520fusion%2520module%252C%2520and%2520an%2520attention-guided%2520prediction%2520head.%2520The%2520instance%2520segmentation%2520branch%2520generates%2520per-image%2520foreground%2520masks%2520that%2520guide%2520the%2520depth%2520branch%2520via%2520cross-attention%252C%2520allowing%2520the%2520network%2520to%2520focus%2520on%2520object-centric%2520regions%2520during%2520refinement.%2520We%2520validate%2520our%2520method%2520on%2520the%2520Virtual%2520KITTI%25202%2520dataset%252C%2520showing%2520that%2520it%2520achieves%2520lower%2520RMSE%2520compared%2520to%2520both%2520a%2520U-Net-only%2520baseline%2520and%2520previous%2520semantic-guided%2520methods%252C%2520while%2520maintaining%2520competitive%2520MAE.%2520Qualitative%2520and%2520quantitative%2520results%2520demonstrate%2520that%2520the%2520proposed%2520model%2520effectively%2520enhances%2520depth%2520accuracy%2520near%2520object%2520boundaries%252C%2520occlusions%252C%2520and%2520thin%2520structures.%2520Our%2520findings%2520suggest%2520that%2520incorporating%2520instance-aware%2520cues%2520offers%2520a%2520promising%2520direction%2520for%2520improving%2520depth%2520completion%2520without%2520relying%2520on%2520dense%2520semantic%2520labels.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MT-Depth%3A%20Multi-task%20Instance%20feature%20analysis%20for%20the%20Depth%20Completion&entry.906535625=Abdul%20Haseeb%20Nizamani%20and%20Dandi%20Zhou%20and%20Xinhai%20Sun&entry.1292438233=Depth%20completion%20plays%20a%20vital%20role%20in%203D%20perception%20systems%2C%20especially%20in%20scenarios%20where%20sparse%20depth%20data%20must%20be%20densified%20for%20tasks%20such%20as%20autonomous%20driving%2C%20robotics%2C%20and%20augmented%20reality.%20While%20many%20existing%20approaches%20rely%20on%20semantic%20segmentation%20to%20guide%20depth%20completion%2C%20they%20often%20overlook%20the%20benefits%20of%20object-level%20understanding.%20In%20this%20work%2C%20we%20introduce%20an%20instance-aware%20depth%20completion%20framework%20that%20explicitly%20integrates%20binary%20instance%20masks%20as%20spatial%20priors%20to%20refine%20depth%20predictions.%20Our%20model%20combines%20four%20main%20components%3A%20a%20frozen%20YOLO%20V11%20instance%20segmentation%20branch%2C%20a%20U-Net-based%20depth%20completion%20backbone%2C%20a%20cross-attention%20fusion%20module%2C%20and%20an%20attention-guided%20prediction%20head.%20The%20instance%20segmentation%20branch%20generates%20per-image%20foreground%20masks%20that%20guide%20the%20depth%20branch%20via%20cross-attention%2C%20allowing%20the%20network%20to%20focus%20on%20object-centric%20regions%20during%20refinement.%20We%20validate%20our%20method%20on%20the%20Virtual%20KITTI%202%20dataset%2C%20showing%20that%20it%20achieves%20lower%20RMSE%20compared%20to%20both%20a%20U-Net-only%20baseline%20and%20previous%20semantic-guided%20methods%2C%20while%20maintaining%20competitive%20MAE.%20Qualitative%20and%20quantitative%20results%20demonstrate%20that%20the%20proposed%20model%20effectively%20enhances%20depth%20accuracy%20near%20object%20boundaries%2C%20occlusions%2C%20and%20thin%20structures.%20Our%20findings%20suggest%20that%20incorporating%20instance-aware%20cues%20offers%20a%20promising%20direction%20for%20improving%20depth%20completion%20without%20relying%20on%20dense%20semantic%20labels.&entry.1838667208=http%3A//arxiv.org/abs/2512.04734v1&entry.124074799=Read"},
{"title": "Gradient Descent with Provably Tuned Learning-rate Schedules", "author": "Dravyansh Sharma", "abstract": "Gradient-based iterative optimization methods are the workhorse of modern machine learning. They crucially rely on careful tuning of parameters like learning rate and momentum. However, one typically sets them using heuristic approaches without formal near-optimality guarantees. Recent work by Gupta and Roughgarden studies how to learn a good step-size in gradient descent. However, like most of the literature with theoretical guarantees for gradient-based optimization, their results rely on strong assumptions on the function class including convexity and smoothness which do not hold in typical applications. In this work, we develop novel analytical tools for provably tuning hyperparameters in gradient-based algorithms that apply to non-convex and non-smooth functions. We obtain matching sample complexity bounds for learning the step-size in gradient descent shown for smooth, convex functions in prior work (up to logarithmic factors) but for a much broader class of functions. Our analysis applies to gradient descent on neural networks with commonly used activation functions (including ReLU, sigmoid and tanh). We extend our framework to tuning multiple hyperparameters, including tuning the learning rate schedule, simultaneously tuning momentum and step-size, and pre-training the initialization vector. Our approach can be used to bound the sample complexity for minimizing both the validation loss as well as the number of gradient descent iterations.", "link": "http://arxiv.org/abs/2512.05084v1", "date": "2025-12-04", "relevancy": 2.2913, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4814}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4501}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Descent%20with%20Provably%20Tuned%20Learning-rate%20Schedules&body=Title%3A%20Gradient%20Descent%20with%20Provably%20Tuned%20Learning-rate%20Schedules%0AAuthor%3A%20Dravyansh%20Sharma%0AAbstract%3A%20Gradient-based%20iterative%20optimization%20methods%20are%20the%20workhorse%20of%20modern%20machine%20learning.%20They%20crucially%20rely%20on%20careful%20tuning%20of%20parameters%20like%20learning%20rate%20and%20momentum.%20However%2C%20one%20typically%20sets%20them%20using%20heuristic%20approaches%20without%20formal%20near-optimality%20guarantees.%20Recent%20work%20by%20Gupta%20and%20Roughgarden%20studies%20how%20to%20learn%20a%20good%20step-size%20in%20gradient%20descent.%20However%2C%20like%20most%20of%20the%20literature%20with%20theoretical%20guarantees%20for%20gradient-based%20optimization%2C%20their%20results%20rely%20on%20strong%20assumptions%20on%20the%20function%20class%20including%20convexity%20and%20smoothness%20which%20do%20not%20hold%20in%20typical%20applications.%20In%20this%20work%2C%20we%20develop%20novel%20analytical%20tools%20for%20provably%20tuning%20hyperparameters%20in%20gradient-based%20algorithms%20that%20apply%20to%20non-convex%20and%20non-smooth%20functions.%20We%20obtain%20matching%20sample%20complexity%20bounds%20for%20learning%20the%20step-size%20in%20gradient%20descent%20shown%20for%20smooth%2C%20convex%20functions%20in%20prior%20work%20%28up%20to%20logarithmic%20factors%29%20but%20for%20a%20much%20broader%20class%20of%20functions.%20Our%20analysis%20applies%20to%20gradient%20descent%20on%20neural%20networks%20with%20commonly%20used%20activation%20functions%20%28including%20ReLU%2C%20sigmoid%20and%20tanh%29.%20We%20extend%20our%20framework%20to%20tuning%20multiple%20hyperparameters%2C%20including%20tuning%20the%20learning%20rate%20schedule%2C%20simultaneously%20tuning%20momentum%20and%20step-size%2C%20and%20pre-training%20the%20initialization%20vector.%20Our%20approach%20can%20be%20used%20to%20bound%20the%20sample%20complexity%20for%20minimizing%20both%20the%20validation%20loss%20as%20well%20as%20the%20number%20of%20gradient%20descent%20iterations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Descent%2520with%2520Provably%2520Tuned%2520Learning-rate%2520Schedules%26entry.906535625%3DDravyansh%2520Sharma%26entry.1292438233%3DGradient-based%2520iterative%2520optimization%2520methods%2520are%2520the%2520workhorse%2520of%2520modern%2520machine%2520learning.%2520They%2520crucially%2520rely%2520on%2520careful%2520tuning%2520of%2520parameters%2520like%2520learning%2520rate%2520and%2520momentum.%2520However%252C%2520one%2520typically%2520sets%2520them%2520using%2520heuristic%2520approaches%2520without%2520formal%2520near-optimality%2520guarantees.%2520Recent%2520work%2520by%2520Gupta%2520and%2520Roughgarden%2520studies%2520how%2520to%2520learn%2520a%2520good%2520step-size%2520in%2520gradient%2520descent.%2520However%252C%2520like%2520most%2520of%2520the%2520literature%2520with%2520theoretical%2520guarantees%2520for%2520gradient-based%2520optimization%252C%2520their%2520results%2520rely%2520on%2520strong%2520assumptions%2520on%2520the%2520function%2520class%2520including%2520convexity%2520and%2520smoothness%2520which%2520do%2520not%2520hold%2520in%2520typical%2520applications.%2520In%2520this%2520work%252C%2520we%2520develop%2520novel%2520analytical%2520tools%2520for%2520provably%2520tuning%2520hyperparameters%2520in%2520gradient-based%2520algorithms%2520that%2520apply%2520to%2520non-convex%2520and%2520non-smooth%2520functions.%2520We%2520obtain%2520matching%2520sample%2520complexity%2520bounds%2520for%2520learning%2520the%2520step-size%2520in%2520gradient%2520descent%2520shown%2520for%2520smooth%252C%2520convex%2520functions%2520in%2520prior%2520work%2520%2528up%2520to%2520logarithmic%2520factors%2529%2520but%2520for%2520a%2520much%2520broader%2520class%2520of%2520functions.%2520Our%2520analysis%2520applies%2520to%2520gradient%2520descent%2520on%2520neural%2520networks%2520with%2520commonly%2520used%2520activation%2520functions%2520%2528including%2520ReLU%252C%2520sigmoid%2520and%2520tanh%2529.%2520We%2520extend%2520our%2520framework%2520to%2520tuning%2520multiple%2520hyperparameters%252C%2520including%2520tuning%2520the%2520learning%2520rate%2520schedule%252C%2520simultaneously%2520tuning%2520momentum%2520and%2520step-size%252C%2520and%2520pre-training%2520the%2520initialization%2520vector.%2520Our%2520approach%2520can%2520be%2520used%2520to%2520bound%2520the%2520sample%2520complexity%2520for%2520minimizing%2520both%2520the%2520validation%2520loss%2520as%2520well%2520as%2520the%2520number%2520of%2520gradient%2520descent%2520iterations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Descent%20with%20Provably%20Tuned%20Learning-rate%20Schedules&entry.906535625=Dravyansh%20Sharma&entry.1292438233=Gradient-based%20iterative%20optimization%20methods%20are%20the%20workhorse%20of%20modern%20machine%20learning.%20They%20crucially%20rely%20on%20careful%20tuning%20of%20parameters%20like%20learning%20rate%20and%20momentum.%20However%2C%20one%20typically%20sets%20them%20using%20heuristic%20approaches%20without%20formal%20near-optimality%20guarantees.%20Recent%20work%20by%20Gupta%20and%20Roughgarden%20studies%20how%20to%20learn%20a%20good%20step-size%20in%20gradient%20descent.%20However%2C%20like%20most%20of%20the%20literature%20with%20theoretical%20guarantees%20for%20gradient-based%20optimization%2C%20their%20results%20rely%20on%20strong%20assumptions%20on%20the%20function%20class%20including%20convexity%20and%20smoothness%20which%20do%20not%20hold%20in%20typical%20applications.%20In%20this%20work%2C%20we%20develop%20novel%20analytical%20tools%20for%20provably%20tuning%20hyperparameters%20in%20gradient-based%20algorithms%20that%20apply%20to%20non-convex%20and%20non-smooth%20functions.%20We%20obtain%20matching%20sample%20complexity%20bounds%20for%20learning%20the%20step-size%20in%20gradient%20descent%20shown%20for%20smooth%2C%20convex%20functions%20in%20prior%20work%20%28up%20to%20logarithmic%20factors%29%20but%20for%20a%20much%20broader%20class%20of%20functions.%20Our%20analysis%20applies%20to%20gradient%20descent%20on%20neural%20networks%20with%20commonly%20used%20activation%20functions%20%28including%20ReLU%2C%20sigmoid%20and%20tanh%29.%20We%20extend%20our%20framework%20to%20tuning%20multiple%20hyperparameters%2C%20including%20tuning%20the%20learning%20rate%20schedule%2C%20simultaneously%20tuning%20momentum%20and%20step-size%2C%20and%20pre-training%20the%20initialization%20vector.%20Our%20approach%20can%20be%20used%20to%20bound%20the%20sample%20complexity%20for%20minimizing%20both%20the%20validation%20loss%20as%20well%20as%20the%20number%20of%20gradient%20descent%20iterations.&entry.1838667208=http%3A//arxiv.org/abs/2512.05084v1&entry.124074799=Read"},
{"title": "SuperActivators: Only the Tail of the Distribution Contains Reliable Concept Signals", "author": "Cassandra Goldberg and Chaehyeon Kim and Adam Stein and Eric Wong", "abstract": "Concept vectors aim to enhance model interpretability by linking internal representations with human-understandable semantics, but their utility is often limited by noisy and inconsistent activations. In this work, we uncover a clear pattern within the noise, which we term the SuperActivator Mechanism: while in-concept and out-of-concept activations overlap considerably, the token activations in the extreme high tail of the in-concept distribution provide a reliable signal of concept presence. We demonstrate the generality of this mechanism by showing that SuperActivator tokens consistently outperform standard vector-based and prompting concept detection approaches, achieving up to a 14% higher F1 score across image and text modalities, model architectures, model layers, and concept extraction techniques. Finally, we leverage SuperActivator tokens to improve feature attributions for concepts.", "link": "http://arxiv.org/abs/2512.05038v1", "date": "2025-12-04", "relevancy": 2.2875, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperActivators%3A%20Only%20the%20Tail%20of%20the%20Distribution%20Contains%20Reliable%20Concept%20Signals&body=Title%3A%20SuperActivators%3A%20Only%20the%20Tail%20of%20the%20Distribution%20Contains%20Reliable%20Concept%20Signals%0AAuthor%3A%20Cassandra%20Goldberg%20and%20Chaehyeon%20Kim%20and%20Adam%20Stein%20and%20Eric%20Wong%0AAbstract%3A%20Concept%20vectors%20aim%20to%20enhance%20model%20interpretability%20by%20linking%20internal%20representations%20with%20human-understandable%20semantics%2C%20but%20their%20utility%20is%20often%20limited%20by%20noisy%20and%20inconsistent%20activations.%20In%20this%20work%2C%20we%20uncover%20a%20clear%20pattern%20within%20the%20noise%2C%20which%20we%20term%20the%20SuperActivator%20Mechanism%3A%20while%20in-concept%20and%20out-of-concept%20activations%20overlap%20considerably%2C%20the%20token%20activations%20in%20the%20extreme%20high%20tail%20of%20the%20in-concept%20distribution%20provide%20a%20reliable%20signal%20of%20concept%20presence.%20We%20demonstrate%20the%20generality%20of%20this%20mechanism%20by%20showing%20that%20SuperActivator%20tokens%20consistently%20outperform%20standard%20vector-based%20and%20prompting%20concept%20detection%20approaches%2C%20achieving%20up%20to%20a%2014%25%20higher%20F1%20score%20across%20image%20and%20text%20modalities%2C%20model%20architectures%2C%20model%20layers%2C%20and%20concept%20extraction%20techniques.%20Finally%2C%20we%20leverage%20SuperActivator%20tokens%20to%20improve%20feature%20attributions%20for%20concepts.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperActivators%253A%2520Only%2520the%2520Tail%2520of%2520the%2520Distribution%2520Contains%2520Reliable%2520Concept%2520Signals%26entry.906535625%3DCassandra%2520Goldberg%2520and%2520Chaehyeon%2520Kim%2520and%2520Adam%2520Stein%2520and%2520Eric%2520Wong%26entry.1292438233%3DConcept%2520vectors%2520aim%2520to%2520enhance%2520model%2520interpretability%2520by%2520linking%2520internal%2520representations%2520with%2520human-understandable%2520semantics%252C%2520but%2520their%2520utility%2520is%2520often%2520limited%2520by%2520noisy%2520and%2520inconsistent%2520activations.%2520In%2520this%2520work%252C%2520we%2520uncover%2520a%2520clear%2520pattern%2520within%2520the%2520noise%252C%2520which%2520we%2520term%2520the%2520SuperActivator%2520Mechanism%253A%2520while%2520in-concept%2520and%2520out-of-concept%2520activations%2520overlap%2520considerably%252C%2520the%2520token%2520activations%2520in%2520the%2520extreme%2520high%2520tail%2520of%2520the%2520in-concept%2520distribution%2520provide%2520a%2520reliable%2520signal%2520of%2520concept%2520presence.%2520We%2520demonstrate%2520the%2520generality%2520of%2520this%2520mechanism%2520by%2520showing%2520that%2520SuperActivator%2520tokens%2520consistently%2520outperform%2520standard%2520vector-based%2520and%2520prompting%2520concept%2520detection%2520approaches%252C%2520achieving%2520up%2520to%2520a%252014%2525%2520higher%2520F1%2520score%2520across%2520image%2520and%2520text%2520modalities%252C%2520model%2520architectures%252C%2520model%2520layers%252C%2520and%2520concept%2520extraction%2520techniques.%2520Finally%252C%2520we%2520leverage%2520SuperActivator%2520tokens%2520to%2520improve%2520feature%2520attributions%2520for%2520concepts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperActivators%3A%20Only%20the%20Tail%20of%20the%20Distribution%20Contains%20Reliable%20Concept%20Signals&entry.906535625=Cassandra%20Goldberg%20and%20Chaehyeon%20Kim%20and%20Adam%20Stein%20and%20Eric%20Wong&entry.1292438233=Concept%20vectors%20aim%20to%20enhance%20model%20interpretability%20by%20linking%20internal%20representations%20with%20human-understandable%20semantics%2C%20but%20their%20utility%20is%20often%20limited%20by%20noisy%20and%20inconsistent%20activations.%20In%20this%20work%2C%20we%20uncover%20a%20clear%20pattern%20within%20the%20noise%2C%20which%20we%20term%20the%20SuperActivator%20Mechanism%3A%20while%20in-concept%20and%20out-of-concept%20activations%20overlap%20considerably%2C%20the%20token%20activations%20in%20the%20extreme%20high%20tail%20of%20the%20in-concept%20distribution%20provide%20a%20reliable%20signal%20of%20concept%20presence.%20We%20demonstrate%20the%20generality%20of%20this%20mechanism%20by%20showing%20that%20SuperActivator%20tokens%20consistently%20outperform%20standard%20vector-based%20and%20prompting%20concept%20detection%20approaches%2C%20achieving%20up%20to%20a%2014%25%20higher%20F1%20score%20across%20image%20and%20text%20modalities%2C%20model%20architectures%2C%20model%20layers%2C%20and%20concept%20extraction%20techniques.%20Finally%2C%20we%20leverage%20SuperActivator%20tokens%20to%20improve%20feature%20attributions%20for%20concepts.&entry.1838667208=http%3A//arxiv.org/abs/2512.05038v1&entry.124074799=Read"},
{"title": "MORPH: PDE Foundation Models with Arbitrary Data Modality", "author": "Mahindra Singh Rautela and Alexander Most and Siddharth Mansingh and Bradley C. Love and Ayan Biswas and Diane Oyen and Earl Lawrence", "abstract": "We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.", "link": "http://arxiv.org/abs/2509.21670v3", "date": "2025-12-04", "relevancy": 2.2813, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MORPH%3A%20PDE%20Foundation%20Models%20with%20Arbitrary%20Data%20Modality&body=Title%3A%20MORPH%3A%20PDE%20Foundation%20Models%20with%20Arbitrary%20Data%20Modality%0AAuthor%3A%20Mahindra%20Singh%20Rautela%20and%20Alexander%20Most%20and%20Siddharth%20Mansingh%20and%20Bradley%20C.%20Love%20and%20Ayan%20Biswas%20and%20Diane%20Oyen%20and%20Earl%20Lawrence%0AAbstract%3A%20We%20introduce%20MORPH%2C%20a%20modality-agnostic%2C%20autoregressive%20foundation%20model%20for%20partial%20differential%20equations%20%28PDEs%29.%20MORPH%20is%20built%20on%20a%20convolutional%20vision%20transformer%20backbone%20that%20seamlessly%20handles%20heterogeneous%20spatiotemporal%20datasets%20of%20varying%20data%20modality%20%281D--3D%29%20at%20different%20resolutions%2C%20and%20multiple%20fields%20with%20mixed%20scalar%20and%20vector%20components.%20The%20architecture%20combines%20%28i%29%20component-wise%20convolution%2C%20which%20jointly%20processes%20scalar%20and%20vector%20channels%20to%20capture%20local%20interactions%2C%20%28ii%29%20inter-field%20cross-attention%2C%20which%20models%20and%20selectively%20propagates%20information%20between%20different%20physical%20fields%2C%20%28iii%29%20axial%20attentions%2C%20which%20factorize%20full%20spatiotemporal%20self-attention%20along%20individual%20spatial%20and%20temporal%20axes%20to%20reduce%20computational%20burden%20while%20retaining%20expressivity.%20We%20pretrain%20multiple%20model%20variants%20on%20a%20diverse%20collection%20of%20heterogeneous%20PDE%20datasets%20and%20evaluate%20transfer%20to%20a%20range%20of%20downstream%20prediction%20tasks.%20Using%20both%20full-model%20fine-tuning%20and%20parameter-efficient%20low-rank%20adapters%20%28LoRA%29%2C%20MORPH%20outperforms%20models%20trained%20from%20scratch.%20Across%20extensive%20evaluations%2C%20MORPH%20matches%20or%20surpasses%20strong%20baselines%20and%20recent%20state-of-the-art%20models.%20Collectively%2C%20these%20capabilities%20present%20a%20flexible%20and%20powerful%20backbone%20for%20learning%20from%20the%20heterogeneous%20and%20multimodal%20nature%20of%20scientific%20observations%2C%20charting%20a%20path%20toward%20scalable%20and%20data-efficient%20scientific%20machine%20learning.%20The%20source%20code%2C%20datasets%2C%20and%20models%20are%20publicly%20available%20at%20https%3A//github.com/lanl/MORPH.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21670v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMORPH%253A%2520PDE%2520Foundation%2520Models%2520with%2520Arbitrary%2520Data%2520Modality%26entry.906535625%3DMahindra%2520Singh%2520Rautela%2520and%2520Alexander%2520Most%2520and%2520Siddharth%2520Mansingh%2520and%2520Bradley%2520C.%2520Love%2520and%2520Ayan%2520Biswas%2520and%2520Diane%2520Oyen%2520and%2520Earl%2520Lawrence%26entry.1292438233%3DWe%2520introduce%2520MORPH%252C%2520a%2520modality-agnostic%252C%2520autoregressive%2520foundation%2520model%2520for%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520MORPH%2520is%2520built%2520on%2520a%2520convolutional%2520vision%2520transformer%2520backbone%2520that%2520seamlessly%2520handles%2520heterogeneous%2520spatiotemporal%2520datasets%2520of%2520varying%2520data%2520modality%2520%25281D--3D%2529%2520at%2520different%2520resolutions%252C%2520and%2520multiple%2520fields%2520with%2520mixed%2520scalar%2520and%2520vector%2520components.%2520The%2520architecture%2520combines%2520%2528i%2529%2520component-wise%2520convolution%252C%2520which%2520jointly%2520processes%2520scalar%2520and%2520vector%2520channels%2520to%2520capture%2520local%2520interactions%252C%2520%2528ii%2529%2520inter-field%2520cross-attention%252C%2520which%2520models%2520and%2520selectively%2520propagates%2520information%2520between%2520different%2520physical%2520fields%252C%2520%2528iii%2529%2520axial%2520attentions%252C%2520which%2520factorize%2520full%2520spatiotemporal%2520self-attention%2520along%2520individual%2520spatial%2520and%2520temporal%2520axes%2520to%2520reduce%2520computational%2520burden%2520while%2520retaining%2520expressivity.%2520We%2520pretrain%2520multiple%2520model%2520variants%2520on%2520a%2520diverse%2520collection%2520of%2520heterogeneous%2520PDE%2520datasets%2520and%2520evaluate%2520transfer%2520to%2520a%2520range%2520of%2520downstream%2520prediction%2520tasks.%2520Using%2520both%2520full-model%2520fine-tuning%2520and%2520parameter-efficient%2520low-rank%2520adapters%2520%2528LoRA%2529%252C%2520MORPH%2520outperforms%2520models%2520trained%2520from%2520scratch.%2520Across%2520extensive%2520evaluations%252C%2520MORPH%2520matches%2520or%2520surpasses%2520strong%2520baselines%2520and%2520recent%2520state-of-the-art%2520models.%2520Collectively%252C%2520these%2520capabilities%2520present%2520a%2520flexible%2520and%2520powerful%2520backbone%2520for%2520learning%2520from%2520the%2520heterogeneous%2520and%2520multimodal%2520nature%2520of%2520scientific%2520observations%252C%2520charting%2520a%2520path%2520toward%2520scalable%2520and%2520data-efficient%2520scientific%2520machine%2520learning.%2520The%2520source%2520code%252C%2520datasets%252C%2520and%2520models%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/lanl/MORPH.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21670v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MORPH%3A%20PDE%20Foundation%20Models%20with%20Arbitrary%20Data%20Modality&entry.906535625=Mahindra%20Singh%20Rautela%20and%20Alexander%20Most%20and%20Siddharth%20Mansingh%20and%20Bradley%20C.%20Love%20and%20Ayan%20Biswas%20and%20Diane%20Oyen%20and%20Earl%20Lawrence&entry.1292438233=We%20introduce%20MORPH%2C%20a%20modality-agnostic%2C%20autoregressive%20foundation%20model%20for%20partial%20differential%20equations%20%28PDEs%29.%20MORPH%20is%20built%20on%20a%20convolutional%20vision%20transformer%20backbone%20that%20seamlessly%20handles%20heterogeneous%20spatiotemporal%20datasets%20of%20varying%20data%20modality%20%281D--3D%29%20at%20different%20resolutions%2C%20and%20multiple%20fields%20with%20mixed%20scalar%20and%20vector%20components.%20The%20architecture%20combines%20%28i%29%20component-wise%20convolution%2C%20which%20jointly%20processes%20scalar%20and%20vector%20channels%20to%20capture%20local%20interactions%2C%20%28ii%29%20inter-field%20cross-attention%2C%20which%20models%20and%20selectively%20propagates%20information%20between%20different%20physical%20fields%2C%20%28iii%29%20axial%20attentions%2C%20which%20factorize%20full%20spatiotemporal%20self-attention%20along%20individual%20spatial%20and%20temporal%20axes%20to%20reduce%20computational%20burden%20while%20retaining%20expressivity.%20We%20pretrain%20multiple%20model%20variants%20on%20a%20diverse%20collection%20of%20heterogeneous%20PDE%20datasets%20and%20evaluate%20transfer%20to%20a%20range%20of%20downstream%20prediction%20tasks.%20Using%20both%20full-model%20fine-tuning%20and%20parameter-efficient%20low-rank%20adapters%20%28LoRA%29%2C%20MORPH%20outperforms%20models%20trained%20from%20scratch.%20Across%20extensive%20evaluations%2C%20MORPH%20matches%20or%20surpasses%20strong%20baselines%20and%20recent%20state-of-the-art%20models.%20Collectively%2C%20these%20capabilities%20present%20a%20flexible%20and%20powerful%20backbone%20for%20learning%20from%20the%20heterogeneous%20and%20multimodal%20nature%20of%20scientific%20observations%2C%20charting%20a%20path%20toward%20scalable%20and%20data-efficient%20scientific%20machine%20learning.%20The%20source%20code%2C%20datasets%2C%20and%20models%20are%20publicly%20available%20at%20https%3A//github.com/lanl/MORPH.&entry.1838667208=http%3A//arxiv.org/abs/2509.21670v3&entry.124074799=Read"},
{"title": "LaFiTe: A Generative Latent Field for 3D Native Texturing", "author": "Chia-Hao Chen and Zi-Xin Zou and Yan-Pei Cao and Ze Yuan and Guan Luo and Xiaojuan Qi and Ding Liang and Song-Hai Zhang and Yuan-Chen Guo", "abstract": "Generating high-fidelity, seamless textures directly on 3D surfaces, what we term 3D-native texturing, remains a fundamental open challenge, with the potential to overcome long-standing limitations of UV-based and multi-view projection methods. However, existing native approaches are constrained by the absence of a powerful and versatile latent representation, which severely limits the fidelity and generality of their generated textures. We identify this representation gap as the principal barrier to further progress. We introduce LaFiTe, a framework that addresses this challenge by learning to generate textures as a 3D generative sparse latent color field. At its core, LaFiTe employs a variational autoencoder (VAE) to encode complex surface appearance into a sparse, structured latent space, which is subsequently decoded into a continuous color field. This representation achieves unprecedented fidelity, exceeding state-of-the-art methods by >10 dB PSNR in reconstruction, by effectively disentangling texture appearance from mesh topology and UV parameterization. Building upon this strong representation, a conditional rectified-flow model synthesizes high-quality, coherent textures across diverse styles and geometries. Extensive experiments demonstrate that LaFiTe not only sets a new benchmark for 3D-native texturing but also enables flexible downstream applications such as material synthesis and texture super-resolution, paving the way for the next generation of 3D content creation workflows.", "link": "http://arxiv.org/abs/2512.04786v1", "date": "2025-12-04", "relevancy": 2.2694, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6045}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5491}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaFiTe%3A%20A%20Generative%20Latent%20Field%20for%203D%20Native%20Texturing&body=Title%3A%20LaFiTe%3A%20A%20Generative%20Latent%20Field%20for%203D%20Native%20Texturing%0AAuthor%3A%20Chia-Hao%20Chen%20and%20Zi-Xin%20Zou%20and%20Yan-Pei%20Cao%20and%20Ze%20Yuan%20and%20Guan%20Luo%20and%20Xiaojuan%20Qi%20and%20Ding%20Liang%20and%20Song-Hai%20Zhang%20and%20Yuan-Chen%20Guo%0AAbstract%3A%20Generating%20high-fidelity%2C%20seamless%20textures%20directly%20on%203D%20surfaces%2C%20what%20we%20term%203D-native%20texturing%2C%20remains%20a%20fundamental%20open%20challenge%2C%20with%20the%20potential%20to%20overcome%20long-standing%20limitations%20of%20UV-based%20and%20multi-view%20projection%20methods.%20However%2C%20existing%20native%20approaches%20are%20constrained%20by%20the%20absence%20of%20a%20powerful%20and%20versatile%20latent%20representation%2C%20which%20severely%20limits%20the%20fidelity%20and%20generality%20of%20their%20generated%20textures.%20We%20identify%20this%20representation%20gap%20as%20the%20principal%20barrier%20to%20further%20progress.%20We%20introduce%20LaFiTe%2C%20a%20framework%20that%20addresses%20this%20challenge%20by%20learning%20to%20generate%20textures%20as%20a%203D%20generative%20sparse%20latent%20color%20field.%20At%20its%20core%2C%20LaFiTe%20employs%20a%20variational%20autoencoder%20%28VAE%29%20to%20encode%20complex%20surface%20appearance%20into%20a%20sparse%2C%20structured%20latent%20space%2C%20which%20is%20subsequently%20decoded%20into%20a%20continuous%20color%20field.%20This%20representation%20achieves%20unprecedented%20fidelity%2C%20exceeding%20state-of-the-art%20methods%20by%20%3E10%20dB%20PSNR%20in%20reconstruction%2C%20by%20effectively%20disentangling%20texture%20appearance%20from%20mesh%20topology%20and%20UV%20parameterization.%20Building%20upon%20this%20strong%20representation%2C%20a%20conditional%20rectified-flow%20model%20synthesizes%20high-quality%2C%20coherent%20textures%20across%20diverse%20styles%20and%20geometries.%20Extensive%20experiments%20demonstrate%20that%20LaFiTe%20not%20only%20sets%20a%20new%20benchmark%20for%203D-native%20texturing%20but%20also%20enables%20flexible%20downstream%20applications%20such%20as%20material%20synthesis%20and%20texture%20super-resolution%2C%20paving%20the%20way%20for%20the%20next%20generation%20of%203D%20content%20creation%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaFiTe%253A%2520A%2520Generative%2520Latent%2520Field%2520for%25203D%2520Native%2520Texturing%26entry.906535625%3DChia-Hao%2520Chen%2520and%2520Zi-Xin%2520Zou%2520and%2520Yan-Pei%2520Cao%2520and%2520Ze%2520Yuan%2520and%2520Guan%2520Luo%2520and%2520Xiaojuan%2520Qi%2520and%2520Ding%2520Liang%2520and%2520Song-Hai%2520Zhang%2520and%2520Yuan-Chen%2520Guo%26entry.1292438233%3DGenerating%2520high-fidelity%252C%2520seamless%2520textures%2520directly%2520on%25203D%2520surfaces%252C%2520what%2520we%2520term%25203D-native%2520texturing%252C%2520remains%2520a%2520fundamental%2520open%2520challenge%252C%2520with%2520the%2520potential%2520to%2520overcome%2520long-standing%2520limitations%2520of%2520UV-based%2520and%2520multi-view%2520projection%2520methods.%2520However%252C%2520existing%2520native%2520approaches%2520are%2520constrained%2520by%2520the%2520absence%2520of%2520a%2520powerful%2520and%2520versatile%2520latent%2520representation%252C%2520which%2520severely%2520limits%2520the%2520fidelity%2520and%2520generality%2520of%2520their%2520generated%2520textures.%2520We%2520identify%2520this%2520representation%2520gap%2520as%2520the%2520principal%2520barrier%2520to%2520further%2520progress.%2520We%2520introduce%2520LaFiTe%252C%2520a%2520framework%2520that%2520addresses%2520this%2520challenge%2520by%2520learning%2520to%2520generate%2520textures%2520as%2520a%25203D%2520generative%2520sparse%2520latent%2520color%2520field.%2520At%2520its%2520core%252C%2520LaFiTe%2520employs%2520a%2520variational%2520autoencoder%2520%2528VAE%2529%2520to%2520encode%2520complex%2520surface%2520appearance%2520into%2520a%2520sparse%252C%2520structured%2520latent%2520space%252C%2520which%2520is%2520subsequently%2520decoded%2520into%2520a%2520continuous%2520color%2520field.%2520This%2520representation%2520achieves%2520unprecedented%2520fidelity%252C%2520exceeding%2520state-of-the-art%2520methods%2520by%2520%253E10%2520dB%2520PSNR%2520in%2520reconstruction%252C%2520by%2520effectively%2520disentangling%2520texture%2520appearance%2520from%2520mesh%2520topology%2520and%2520UV%2520parameterization.%2520Building%2520upon%2520this%2520strong%2520representation%252C%2520a%2520conditional%2520rectified-flow%2520model%2520synthesizes%2520high-quality%252C%2520coherent%2520textures%2520across%2520diverse%2520styles%2520and%2520geometries.%2520Extensive%2520experiments%2520demonstrate%2520that%2520LaFiTe%2520not%2520only%2520sets%2520a%2520new%2520benchmark%2520for%25203D-native%2520texturing%2520but%2520also%2520enables%2520flexible%2520downstream%2520applications%2520such%2520as%2520material%2520synthesis%2520and%2520texture%2520super-resolution%252C%2520paving%2520the%2520way%2520for%2520the%2520next%2520generation%2520of%25203D%2520content%2520creation%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaFiTe%3A%20A%20Generative%20Latent%20Field%20for%203D%20Native%20Texturing&entry.906535625=Chia-Hao%20Chen%20and%20Zi-Xin%20Zou%20and%20Yan-Pei%20Cao%20and%20Ze%20Yuan%20and%20Guan%20Luo%20and%20Xiaojuan%20Qi%20and%20Ding%20Liang%20and%20Song-Hai%20Zhang%20and%20Yuan-Chen%20Guo&entry.1292438233=Generating%20high-fidelity%2C%20seamless%20textures%20directly%20on%203D%20surfaces%2C%20what%20we%20term%203D-native%20texturing%2C%20remains%20a%20fundamental%20open%20challenge%2C%20with%20the%20potential%20to%20overcome%20long-standing%20limitations%20of%20UV-based%20and%20multi-view%20projection%20methods.%20However%2C%20existing%20native%20approaches%20are%20constrained%20by%20the%20absence%20of%20a%20powerful%20and%20versatile%20latent%20representation%2C%20which%20severely%20limits%20the%20fidelity%20and%20generality%20of%20their%20generated%20textures.%20We%20identify%20this%20representation%20gap%20as%20the%20principal%20barrier%20to%20further%20progress.%20We%20introduce%20LaFiTe%2C%20a%20framework%20that%20addresses%20this%20challenge%20by%20learning%20to%20generate%20textures%20as%20a%203D%20generative%20sparse%20latent%20color%20field.%20At%20its%20core%2C%20LaFiTe%20employs%20a%20variational%20autoencoder%20%28VAE%29%20to%20encode%20complex%20surface%20appearance%20into%20a%20sparse%2C%20structured%20latent%20space%2C%20which%20is%20subsequently%20decoded%20into%20a%20continuous%20color%20field.%20This%20representation%20achieves%20unprecedented%20fidelity%2C%20exceeding%20state-of-the-art%20methods%20by%20%3E10%20dB%20PSNR%20in%20reconstruction%2C%20by%20effectively%20disentangling%20texture%20appearance%20from%20mesh%20topology%20and%20UV%20parameterization.%20Building%20upon%20this%20strong%20representation%2C%20a%20conditional%20rectified-flow%20model%20synthesizes%20high-quality%2C%20coherent%20textures%20across%20diverse%20styles%20and%20geometries.%20Extensive%20experiments%20demonstrate%20that%20LaFiTe%20not%20only%20sets%20a%20new%20benchmark%20for%203D-native%20texturing%20but%20also%20enables%20flexible%20downstream%20applications%20such%20as%20material%20synthesis%20and%20texture%20super-resolution%2C%20paving%20the%20way%20for%20the%20next%20generation%20of%203D%20content%20creation%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2512.04786v1&entry.124074799=Read"},
{"title": "CoCoIns: Consistent Subject Generation via Contrastive Instantiated Concepts", "author": "Lee Hsin-Ying and Kelvin C. K. Chan and Ming-Hsuan Yang", "abstract": "While text-to-image generative models can synthesize diverse and faithful content, subject variation across multiple generations limits their application to long-form content generation. Existing approaches require time-consuming fine-tuning, reference images for all subjects, or access to previously generated content. We introduce Contrastive Concept Instantiation (CoCoIns), a framework that effectively synthesizes consistent subjects across multiple independent generations. The framework consists of a generative model and a mapping network that transforms input latent codes into pseudo-words associated with specific concept instances. Users can generate consistent subjects by reusing the same latent codes. To construct such associations, we propose a contrastive learning approach that trains the network to distinguish between different combinations of prompts and latent codes. Extensive evaluations on human faces with a single subject show that CoCoIns performs comparably to existing methods while maintaining greater flexibility. We also demonstrate the potential for extending CoCoIns to multiple subjects and other object categories.", "link": "http://arxiv.org/abs/2503.24387v2", "date": "2025-12-04", "relevancy": 2.2584, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6012}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5921}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoCoIns%3A%20Consistent%20Subject%20Generation%20via%20Contrastive%20Instantiated%20Concepts&body=Title%3A%20CoCoIns%3A%20Consistent%20Subject%20Generation%20via%20Contrastive%20Instantiated%20Concepts%0AAuthor%3A%20Lee%20Hsin-Ying%20and%20Kelvin%20C.%20K.%20Chan%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20While%20text-to-image%20generative%20models%20can%20synthesize%20diverse%20and%20faithful%20content%2C%20subject%20variation%20across%20multiple%20generations%20limits%20their%20application%20to%20long-form%20content%20generation.%20Existing%20approaches%20require%20time-consuming%20fine-tuning%2C%20reference%20images%20for%20all%20subjects%2C%20or%20access%20to%20previously%20generated%20content.%20We%20introduce%20Contrastive%20Concept%20Instantiation%20%28CoCoIns%29%2C%20a%20framework%20that%20effectively%20synthesizes%20consistent%20subjects%20across%20multiple%20independent%20generations.%20The%20framework%20consists%20of%20a%20generative%20model%20and%20a%20mapping%20network%20that%20transforms%20input%20latent%20codes%20into%20pseudo-words%20associated%20with%20specific%20concept%20instances.%20Users%20can%20generate%20consistent%20subjects%20by%20reusing%20the%20same%20latent%20codes.%20To%20construct%20such%20associations%2C%20we%20propose%20a%20contrastive%20learning%20approach%20that%20trains%20the%20network%20to%20distinguish%20between%20different%20combinations%20of%20prompts%20and%20latent%20codes.%20Extensive%20evaluations%20on%20human%20faces%20with%20a%20single%20subject%20show%20that%20CoCoIns%20performs%20comparably%20to%20existing%20methods%20while%20maintaining%20greater%20flexibility.%20We%20also%20demonstrate%20the%20potential%20for%20extending%20CoCoIns%20to%20multiple%20subjects%20and%20other%20object%20categories.%0ALink%3A%20http%3A//arxiv.org/abs/2503.24387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoCoIns%253A%2520Consistent%2520Subject%2520Generation%2520via%2520Contrastive%2520Instantiated%2520Concepts%26entry.906535625%3DLee%2520Hsin-Ying%2520and%2520Kelvin%2520C.%2520K.%2520Chan%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3DWhile%2520text-to-image%2520generative%2520models%2520can%2520synthesize%2520diverse%2520and%2520faithful%2520content%252C%2520subject%2520variation%2520across%2520multiple%2520generations%2520limits%2520their%2520application%2520to%2520long-form%2520content%2520generation.%2520Existing%2520approaches%2520require%2520time-consuming%2520fine-tuning%252C%2520reference%2520images%2520for%2520all%2520subjects%252C%2520or%2520access%2520to%2520previously%2520generated%2520content.%2520We%2520introduce%2520Contrastive%2520Concept%2520Instantiation%2520%2528CoCoIns%2529%252C%2520a%2520framework%2520that%2520effectively%2520synthesizes%2520consistent%2520subjects%2520across%2520multiple%2520independent%2520generations.%2520The%2520framework%2520consists%2520of%2520a%2520generative%2520model%2520and%2520a%2520mapping%2520network%2520that%2520transforms%2520input%2520latent%2520codes%2520into%2520pseudo-words%2520associated%2520with%2520specific%2520concept%2520instances.%2520Users%2520can%2520generate%2520consistent%2520subjects%2520by%2520reusing%2520the%2520same%2520latent%2520codes.%2520To%2520construct%2520such%2520associations%252C%2520we%2520propose%2520a%2520contrastive%2520learning%2520approach%2520that%2520trains%2520the%2520network%2520to%2520distinguish%2520between%2520different%2520combinations%2520of%2520prompts%2520and%2520latent%2520codes.%2520Extensive%2520evaluations%2520on%2520human%2520faces%2520with%2520a%2520single%2520subject%2520show%2520that%2520CoCoIns%2520performs%2520comparably%2520to%2520existing%2520methods%2520while%2520maintaining%2520greater%2520flexibility.%2520We%2520also%2520demonstrate%2520the%2520potential%2520for%2520extending%2520CoCoIns%2520to%2520multiple%2520subjects%2520and%2520other%2520object%2520categories.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.24387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoCoIns%3A%20Consistent%20Subject%20Generation%20via%20Contrastive%20Instantiated%20Concepts&entry.906535625=Lee%20Hsin-Ying%20and%20Kelvin%20C.%20K.%20Chan%20and%20Ming-Hsuan%20Yang&entry.1292438233=While%20text-to-image%20generative%20models%20can%20synthesize%20diverse%20and%20faithful%20content%2C%20subject%20variation%20across%20multiple%20generations%20limits%20their%20application%20to%20long-form%20content%20generation.%20Existing%20approaches%20require%20time-consuming%20fine-tuning%2C%20reference%20images%20for%20all%20subjects%2C%20or%20access%20to%20previously%20generated%20content.%20We%20introduce%20Contrastive%20Concept%20Instantiation%20%28CoCoIns%29%2C%20a%20framework%20that%20effectively%20synthesizes%20consistent%20subjects%20across%20multiple%20independent%20generations.%20The%20framework%20consists%20of%20a%20generative%20model%20and%20a%20mapping%20network%20that%20transforms%20input%20latent%20codes%20into%20pseudo-words%20associated%20with%20specific%20concept%20instances.%20Users%20can%20generate%20consistent%20subjects%20by%20reusing%20the%20same%20latent%20codes.%20To%20construct%20such%20associations%2C%20we%20propose%20a%20contrastive%20learning%20approach%20that%20trains%20the%20network%20to%20distinguish%20between%20different%20combinations%20of%20prompts%20and%20latent%20codes.%20Extensive%20evaluations%20on%20human%20faces%20with%20a%20single%20subject%20show%20that%20CoCoIns%20performs%20comparably%20to%20existing%20methods%20while%20maintaining%20greater%20flexibility.%20We%20also%20demonstrate%20the%20potential%20for%20extending%20CoCoIns%20to%20multiple%20subjects%20and%20other%20object%20categories.&entry.1838667208=http%3A//arxiv.org/abs/2503.24387v2&entry.124074799=Read"},
{"title": "Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark", "author": "Haobo Yuan and Yueyi Sun and Yanwei Li and Tao Zhang and Xueqing Deng and Henghui Ding and Lu Qi and Anran Wang and Xiangtai Li and Ming-Hsuan Yang", "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.", "link": "http://arxiv.org/abs/2512.05091v1", "date": "2025-12-04", "relevancy": 2.2546, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5769}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5769}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Reasoning%20Tracer%3A%20Object-Level%20Grounded%20Reasoning%20Benchmark&body=Title%3A%20Visual%20Reasoning%20Tracer%3A%20Object-Level%20Grounded%20Reasoning%20Benchmark%0AAuthor%3A%20Haobo%20Yuan%20and%20Yueyi%20Sun%20and%20Yanwei%20Li%20and%20Tao%20Zhang%20and%20Xueqing%20Deng%20and%20Henghui%20Ding%20and%20Lu%20Qi%20and%20Anran%20Wang%20and%20Xiangtai%20Li%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20improved%20performance%20on%20tasks%20such%20as%20visual%20grounding%20and%20visual%20question%20answering.%20However%2C%20the%20reasoning%20processes%20of%20these%20models%20remain%20largely%20opaque%3B%20they%20typically%20output%20only%20final%20predictions%20without%20revealing%20the%20intermediate%20steps%20or%20fine-grained%20evidence%20%28e.g.%2C%20pixels%2C%20locations%29%20that%20lead%20to%20the%20result.%20This%20contrasts%20with%20human%20intelligence%2C%20which%20naturally%20operates%20through%20a%20chain%20of%20visual%20reasoning.%20To%20address%20this%20limitation%2C%20we%20introduce%20the%20Visual%20Reasoning%20Tracer%20%28VRT%29%20task%2C%20which%20requires%20models%20to%20not%20only%20localize%20the%20target%20object%20but%20also%20explicitly%20predict%20the%20intermediate%20objects%20that%20form%20the%20reasoning%20path.%20To%20advance%20research%20in%20this%20area%2C%20we%20contribute%3A%20%281%29%20VRT-Bench%2C%20a%20human-annotated%20benchmark%20for%20evaluating%20visual%20reasoning%3B%20%282%29%20a%20new%20metric%20for%20assessing%20the%20quality%20of%20reasoning%20traces%3B%20and%20%283%29%20VRT-80k%2C%20a%20large-scale%20dataset%20for%20reasoning%20model%20training.%20Our%20experiments%20reveal%20that%20while%20existing%20models%20often%20produce%20the%20correct%20final%20output%2C%20they%20struggle%20to%20ground%20their%20intermediate%20reasoning.%20In%20contrast%2C%20models%20trained%20on%20VRT-80k%20achieve%20substantial%20improvements%20in%20tracing%20the%20reasoning%20path.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Reasoning%2520Tracer%253A%2520Object-Level%2520Grounded%2520Reasoning%2520Benchmark%26entry.906535625%3DHaobo%2520Yuan%2520and%2520Yueyi%2520Sun%2520and%2520Yanwei%2520Li%2520and%2520Tao%2520Zhang%2520and%2520Xueqing%2520Deng%2520and%2520Henghui%2520Ding%2520and%2520Lu%2520Qi%2520and%2520Anran%2520Wang%2520and%2520Xiangtai%2520Li%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3DRecent%2520advances%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520significantly%2520improved%2520performance%2520on%2520tasks%2520such%2520as%2520visual%2520grounding%2520and%2520visual%2520question%2520answering.%2520However%252C%2520the%2520reasoning%2520processes%2520of%2520these%2520models%2520remain%2520largely%2520opaque%253B%2520they%2520typically%2520output%2520only%2520final%2520predictions%2520without%2520revealing%2520the%2520intermediate%2520steps%2520or%2520fine-grained%2520evidence%2520%2528e.g.%252C%2520pixels%252C%2520locations%2529%2520that%2520lead%2520to%2520the%2520result.%2520This%2520contrasts%2520with%2520human%2520intelligence%252C%2520which%2520naturally%2520operates%2520through%2520a%2520chain%2520of%2520visual%2520reasoning.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520Visual%2520Reasoning%2520Tracer%2520%2528VRT%2529%2520task%252C%2520which%2520requires%2520models%2520to%2520not%2520only%2520localize%2520the%2520target%2520object%2520but%2520also%2520explicitly%2520predict%2520the%2520intermediate%2520objects%2520that%2520form%2520the%2520reasoning%2520path.%2520To%2520advance%2520research%2520in%2520this%2520area%252C%2520we%2520contribute%253A%2520%25281%2529%2520VRT-Bench%252C%2520a%2520human-annotated%2520benchmark%2520for%2520evaluating%2520visual%2520reasoning%253B%2520%25282%2529%2520a%2520new%2520metric%2520for%2520assessing%2520the%2520quality%2520of%2520reasoning%2520traces%253B%2520and%2520%25283%2529%2520VRT-80k%252C%2520a%2520large-scale%2520dataset%2520for%2520reasoning%2520model%2520training.%2520Our%2520experiments%2520reveal%2520that%2520while%2520existing%2520models%2520often%2520produce%2520the%2520correct%2520final%2520output%252C%2520they%2520struggle%2520to%2520ground%2520their%2520intermediate%2520reasoning.%2520In%2520contrast%252C%2520models%2520trained%2520on%2520VRT-80k%2520achieve%2520substantial%2520improvements%2520in%2520tracing%2520the%2520reasoning%2520path.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Reasoning%20Tracer%3A%20Object-Level%20Grounded%20Reasoning%20Benchmark&entry.906535625=Haobo%20Yuan%20and%20Yueyi%20Sun%20and%20Yanwei%20Li%20and%20Tao%20Zhang%20and%20Xueqing%20Deng%20and%20Henghui%20Ding%20and%20Lu%20Qi%20and%20Anran%20Wang%20and%20Xiangtai%20Li%20and%20Ming-Hsuan%20Yang&entry.1292438233=Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20improved%20performance%20on%20tasks%20such%20as%20visual%20grounding%20and%20visual%20question%20answering.%20However%2C%20the%20reasoning%20processes%20of%20these%20models%20remain%20largely%20opaque%3B%20they%20typically%20output%20only%20final%20predictions%20without%20revealing%20the%20intermediate%20steps%20or%20fine-grained%20evidence%20%28e.g.%2C%20pixels%2C%20locations%29%20that%20lead%20to%20the%20result.%20This%20contrasts%20with%20human%20intelligence%2C%20which%20naturally%20operates%20through%20a%20chain%20of%20visual%20reasoning.%20To%20address%20this%20limitation%2C%20we%20introduce%20the%20Visual%20Reasoning%20Tracer%20%28VRT%29%20task%2C%20which%20requires%20models%20to%20not%20only%20localize%20the%20target%20object%20but%20also%20explicitly%20predict%20the%20intermediate%20objects%20that%20form%20the%20reasoning%20path.%20To%20advance%20research%20in%20this%20area%2C%20we%20contribute%3A%20%281%29%20VRT-Bench%2C%20a%20human-annotated%20benchmark%20for%20evaluating%20visual%20reasoning%3B%20%282%29%20a%20new%20metric%20for%20assessing%20the%20quality%20of%20reasoning%20traces%3B%20and%20%283%29%20VRT-80k%2C%20a%20large-scale%20dataset%20for%20reasoning%20model%20training.%20Our%20experiments%20reveal%20that%20while%20existing%20models%20often%20produce%20the%20correct%20final%20output%2C%20they%20struggle%20to%20ground%20their%20intermediate%20reasoning.%20In%20contrast%2C%20models%20trained%20on%20VRT-80k%20achieve%20substantial%20improvements%20in%20tracing%20the%20reasoning%20path.&entry.1838667208=http%3A//arxiv.org/abs/2512.05091v1&entry.124074799=Read"},
{"title": "Polygon Intersection-over-Union Loss for Viewpoint-Agnostic Monocular 3D Vehicle Detection", "author": "Xinxuan Lu and Derek Gloudemans and Shepard Xia and Daniel B. Work", "abstract": "Monocular 3D object detection is a challenging task because depth information is difficult to obtain from 2D images. A subset of viewpoint-agnostic monocular 3D detection methods also do not explicitly leverage scene homography or geometry during training, meaning that a model trained thusly can detect objects in images from arbitrary viewpoints. Such works predict the projections of the 3D bounding boxes on the image plane to estimate the location of the 3D boxes, but these projections are not rectangular so the calculation of IoU between these projected polygons is not straightforward. This work proposes an efficient, fully differentiable algorithm for the calculation of IoU between two convex polygons, which can be utilized to compute the IoU between two 3D bounding box footprints viewed from an arbitrary angle. We test the performance of the proposed polygon IoU loss (PIoU loss) on three state-of-the-art viewpoint-agnostic 3D detection models. Experiments demonstrate that the proposed PIoU loss converges faster than L1 loss and that in 3D detection models, a combination of PIoU loss and L1 loss gives better results than L1 loss alone (+1.64% AP70 for MonoCon on cars, +0.18% AP70 for RTM3D on cars, and +0.83%/+2.46% AP50/AP25 for MonoRCNN on cyclists).", "link": "http://arxiv.org/abs/2309.07104v2", "date": "2025-12-04", "relevancy": 2.2374, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5619}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5618}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polygon%20Intersection-over-Union%20Loss%20for%20Viewpoint-Agnostic%20Monocular%203D%20Vehicle%20Detection&body=Title%3A%20Polygon%20Intersection-over-Union%20Loss%20for%20Viewpoint-Agnostic%20Monocular%203D%20Vehicle%20Detection%0AAuthor%3A%20Xinxuan%20Lu%20and%20Derek%20Gloudemans%20and%20Shepard%20Xia%20and%20Daniel%20B.%20Work%0AAbstract%3A%20Monocular%203D%20object%20detection%20is%20a%20challenging%20task%20because%20depth%20information%20is%20difficult%20to%20obtain%20from%202D%20images.%20A%20subset%20of%20viewpoint-agnostic%20monocular%203D%20detection%20methods%20also%20do%20not%20explicitly%20leverage%20scene%20homography%20or%20geometry%20during%20training%2C%20meaning%20that%20a%20model%20trained%20thusly%20can%20detect%20objects%20in%20images%20from%20arbitrary%20viewpoints.%20Such%20works%20predict%20the%20projections%20of%20the%203D%20bounding%20boxes%20on%20the%20image%20plane%20to%20estimate%20the%20location%20of%20the%203D%20boxes%2C%20but%20these%20projections%20are%20not%20rectangular%20so%20the%20calculation%20of%20IoU%20between%20these%20projected%20polygons%20is%20not%20straightforward.%20This%20work%20proposes%20an%20efficient%2C%20fully%20differentiable%20algorithm%20for%20the%20calculation%20of%20IoU%20between%20two%20convex%20polygons%2C%20which%20can%20be%20utilized%20to%20compute%20the%20IoU%20between%20two%203D%20bounding%20box%20footprints%20viewed%20from%20an%20arbitrary%20angle.%20We%20test%20the%20performance%20of%20the%20proposed%20polygon%20IoU%20loss%20%28PIoU%20loss%29%20on%20three%20state-of-the-art%20viewpoint-agnostic%203D%20detection%20models.%20Experiments%20demonstrate%20that%20the%20proposed%20PIoU%20loss%20converges%20faster%20than%20L1%20loss%20and%20that%20in%203D%20detection%20models%2C%20a%20combination%20of%20PIoU%20loss%20and%20L1%20loss%20gives%20better%20results%20than%20L1%20loss%20alone%20%28%2B1.64%25%20AP70%20for%20MonoCon%20on%20cars%2C%20%2B0.18%25%20AP70%20for%20RTM3D%20on%20cars%2C%20and%20%2B0.83%25/%2B2.46%25%20AP50/AP25%20for%20MonoRCNN%20on%20cyclists%29.%0ALink%3A%20http%3A//arxiv.org/abs/2309.07104v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolygon%2520Intersection-over-Union%2520Loss%2520for%2520Viewpoint-Agnostic%2520Monocular%25203D%2520Vehicle%2520Detection%26entry.906535625%3DXinxuan%2520Lu%2520and%2520Derek%2520Gloudemans%2520and%2520Shepard%2520Xia%2520and%2520Daniel%2520B.%2520Work%26entry.1292438233%3DMonocular%25203D%2520object%2520detection%2520is%2520a%2520challenging%2520task%2520because%2520depth%2520information%2520is%2520difficult%2520to%2520obtain%2520from%25202D%2520images.%2520A%2520subset%2520of%2520viewpoint-agnostic%2520monocular%25203D%2520detection%2520methods%2520also%2520do%2520not%2520explicitly%2520leverage%2520scene%2520homography%2520or%2520geometry%2520during%2520training%252C%2520meaning%2520that%2520a%2520model%2520trained%2520thusly%2520can%2520detect%2520objects%2520in%2520images%2520from%2520arbitrary%2520viewpoints.%2520Such%2520works%2520predict%2520the%2520projections%2520of%2520the%25203D%2520bounding%2520boxes%2520on%2520the%2520image%2520plane%2520to%2520estimate%2520the%2520location%2520of%2520the%25203D%2520boxes%252C%2520but%2520these%2520projections%2520are%2520not%2520rectangular%2520so%2520the%2520calculation%2520of%2520IoU%2520between%2520these%2520projected%2520polygons%2520is%2520not%2520straightforward.%2520This%2520work%2520proposes%2520an%2520efficient%252C%2520fully%2520differentiable%2520algorithm%2520for%2520the%2520calculation%2520of%2520IoU%2520between%2520two%2520convex%2520polygons%252C%2520which%2520can%2520be%2520utilized%2520to%2520compute%2520the%2520IoU%2520between%2520two%25203D%2520bounding%2520box%2520footprints%2520viewed%2520from%2520an%2520arbitrary%2520angle.%2520We%2520test%2520the%2520performance%2520of%2520the%2520proposed%2520polygon%2520IoU%2520loss%2520%2528PIoU%2520loss%2529%2520on%2520three%2520state-of-the-art%2520viewpoint-agnostic%25203D%2520detection%2520models.%2520Experiments%2520demonstrate%2520that%2520the%2520proposed%2520PIoU%2520loss%2520converges%2520faster%2520than%2520L1%2520loss%2520and%2520that%2520in%25203D%2520detection%2520models%252C%2520a%2520combination%2520of%2520PIoU%2520loss%2520and%2520L1%2520loss%2520gives%2520better%2520results%2520than%2520L1%2520loss%2520alone%2520%2528%252B1.64%2525%2520AP70%2520for%2520MonoCon%2520on%2520cars%252C%2520%252B0.18%2525%2520AP70%2520for%2520RTM3D%2520on%2520cars%252C%2520and%2520%252B0.83%2525/%252B2.46%2525%2520AP50/AP25%2520for%2520MonoRCNN%2520on%2520cyclists%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.07104v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polygon%20Intersection-over-Union%20Loss%20for%20Viewpoint-Agnostic%20Monocular%203D%20Vehicle%20Detection&entry.906535625=Xinxuan%20Lu%20and%20Derek%20Gloudemans%20and%20Shepard%20Xia%20and%20Daniel%20B.%20Work&entry.1292438233=Monocular%203D%20object%20detection%20is%20a%20challenging%20task%20because%20depth%20information%20is%20difficult%20to%20obtain%20from%202D%20images.%20A%20subset%20of%20viewpoint-agnostic%20monocular%203D%20detection%20methods%20also%20do%20not%20explicitly%20leverage%20scene%20homography%20or%20geometry%20during%20training%2C%20meaning%20that%20a%20model%20trained%20thusly%20can%20detect%20objects%20in%20images%20from%20arbitrary%20viewpoints.%20Such%20works%20predict%20the%20projections%20of%20the%203D%20bounding%20boxes%20on%20the%20image%20plane%20to%20estimate%20the%20location%20of%20the%203D%20boxes%2C%20but%20these%20projections%20are%20not%20rectangular%20so%20the%20calculation%20of%20IoU%20between%20these%20projected%20polygons%20is%20not%20straightforward.%20This%20work%20proposes%20an%20efficient%2C%20fully%20differentiable%20algorithm%20for%20the%20calculation%20of%20IoU%20between%20two%20convex%20polygons%2C%20which%20can%20be%20utilized%20to%20compute%20the%20IoU%20between%20two%203D%20bounding%20box%20footprints%20viewed%20from%20an%20arbitrary%20angle.%20We%20test%20the%20performance%20of%20the%20proposed%20polygon%20IoU%20loss%20%28PIoU%20loss%29%20on%20three%20state-of-the-art%20viewpoint-agnostic%203D%20detection%20models.%20Experiments%20demonstrate%20that%20the%20proposed%20PIoU%20loss%20converges%20faster%20than%20L1%20loss%20and%20that%20in%203D%20detection%20models%2C%20a%20combination%20of%20PIoU%20loss%20and%20L1%20loss%20gives%20better%20results%20than%20L1%20loss%20alone%20%28%2B1.64%25%20AP70%20for%20MonoCon%20on%20cars%2C%20%2B0.18%25%20AP70%20for%20RTM3D%20on%20cars%2C%20and%20%2B0.83%25/%2B2.46%25%20AP50/AP25%20for%20MonoRCNN%20on%20cyclists%29.&entry.1838667208=http%3A//arxiv.org/abs/2309.07104v2&entry.124074799=Read"},
{"title": "A result relating convex n-widths to covering numbers with some applications to neural networks", "author": "Jonathan Baxter and Peter Bartlett", "abstract": "In general, approximating classes of functions defined over high-dimensional input spaces by linear combinations of a fixed set of basis functions or ``features'' is known to be hard. Typically, the worst-case error of the best basis set decays only as fast as $\u0398\\(n^{-1/d}\\)$, where $n$ is the number of basis functions and $d$ is the input dimension. However, there are many examples of high-dimensional pattern recognition problems (such as face recognition) where linear combinations of small sets of features do solve the problem well. Hence these function classes do not suffer from the ``curse of dimensionality'' associated with more general classes. It is natural then, to look for characterizations of high-dimensional function classes that nevertheless are approximated well by linear combinations of small sets of features. In this paper we give a general result relating the error of approximation of a function class to the covering number of its ``convex core''. For one-hidden-layer neural networks, covering numbers of the class of functions computed by a single hidden node upper bound the covering numbers of the convex core. Hence, using standard results we obtain upper bounds on the approximation rate of neural network classes.", "link": "http://arxiv.org/abs/2512.04912v1", "date": "2025-12-04", "relevancy": 2.2287, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4587}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4467}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20result%20relating%20convex%20n-widths%20to%20covering%20numbers%20with%20some%20applications%20to%20neural%20networks&body=Title%3A%20A%20result%20relating%20convex%20n-widths%20to%20covering%20numbers%20with%20some%20applications%20to%20neural%20networks%0AAuthor%3A%20Jonathan%20Baxter%20and%20Peter%20Bartlett%0AAbstract%3A%20In%20general%2C%20approximating%20classes%20of%20functions%20defined%20over%20high-dimensional%20input%20spaces%20by%20linear%20combinations%20of%20a%20fixed%20set%20of%20basis%20functions%20or%20%60%60features%27%27%20is%20known%20to%20be%20hard.%20Typically%2C%20the%20worst-case%20error%20of%20the%20best%20basis%20set%20decays%20only%20as%20fast%20as%20%24%CE%98%5C%28n%5E%7B-1/d%7D%5C%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20basis%20functions%20and%20%24d%24%20is%20the%20input%20dimension.%20However%2C%20there%20are%20many%20examples%20of%20high-dimensional%20pattern%20recognition%20problems%20%28such%20as%20face%20recognition%29%20where%20linear%20combinations%20of%20small%20sets%20of%20features%20do%20solve%20the%20problem%20well.%20Hence%20these%20function%20classes%20do%20not%20suffer%20from%20the%20%60%60curse%20of%20dimensionality%27%27%20associated%20with%20more%20general%20classes.%20It%20is%20natural%20then%2C%20to%20look%20for%20characterizations%20of%20high-dimensional%20function%20classes%20that%20nevertheless%20are%20approximated%20well%20by%20linear%20combinations%20of%20small%20sets%20of%20features.%20In%20this%20paper%20we%20give%20a%20general%20result%20relating%20the%20error%20of%20approximation%20of%20a%20function%20class%20to%20the%20covering%20number%20of%20its%20%60%60convex%20core%27%27.%20For%20one-hidden-layer%20neural%20networks%2C%20covering%20numbers%20of%20the%20class%20of%20functions%20computed%20by%20a%20single%20hidden%20node%20upper%20bound%20the%20covering%20numbers%20of%20the%20convex%20core.%20Hence%2C%20using%20standard%20results%20we%20obtain%20upper%20bounds%20on%20the%20approximation%20rate%20of%20neural%20network%20classes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520result%2520relating%2520convex%2520n-widths%2520to%2520covering%2520numbers%2520with%2520some%2520applications%2520to%2520neural%2520networks%26entry.906535625%3DJonathan%2520Baxter%2520and%2520Peter%2520Bartlett%26entry.1292438233%3DIn%2520general%252C%2520approximating%2520classes%2520of%2520functions%2520defined%2520over%2520high-dimensional%2520input%2520spaces%2520by%2520linear%2520combinations%2520of%2520a%2520fixed%2520set%2520of%2520basis%2520functions%2520or%2520%2560%2560features%2527%2527%2520is%2520known%2520to%2520be%2520hard.%2520Typically%252C%2520the%2520worst-case%2520error%2520of%2520the%2520best%2520basis%2520set%2520decays%2520only%2520as%2520fast%2520as%2520%2524%25CE%2598%255C%2528n%255E%257B-1/d%257D%255C%2529%2524%252C%2520where%2520%2524n%2524%2520is%2520the%2520number%2520of%2520basis%2520functions%2520and%2520%2524d%2524%2520is%2520the%2520input%2520dimension.%2520However%252C%2520there%2520are%2520many%2520examples%2520of%2520high-dimensional%2520pattern%2520recognition%2520problems%2520%2528such%2520as%2520face%2520recognition%2529%2520where%2520linear%2520combinations%2520of%2520small%2520sets%2520of%2520features%2520do%2520solve%2520the%2520problem%2520well.%2520Hence%2520these%2520function%2520classes%2520do%2520not%2520suffer%2520from%2520the%2520%2560%2560curse%2520of%2520dimensionality%2527%2527%2520associated%2520with%2520more%2520general%2520classes.%2520It%2520is%2520natural%2520then%252C%2520to%2520look%2520for%2520characterizations%2520of%2520high-dimensional%2520function%2520classes%2520that%2520nevertheless%2520are%2520approximated%2520well%2520by%2520linear%2520combinations%2520of%2520small%2520sets%2520of%2520features.%2520In%2520this%2520paper%2520we%2520give%2520a%2520general%2520result%2520relating%2520the%2520error%2520of%2520approximation%2520of%2520a%2520function%2520class%2520to%2520the%2520covering%2520number%2520of%2520its%2520%2560%2560convex%2520core%2527%2527.%2520For%2520one-hidden-layer%2520neural%2520networks%252C%2520covering%2520numbers%2520of%2520the%2520class%2520of%2520functions%2520computed%2520by%2520a%2520single%2520hidden%2520node%2520upper%2520bound%2520the%2520covering%2520numbers%2520of%2520the%2520convex%2520core.%2520Hence%252C%2520using%2520standard%2520results%2520we%2520obtain%2520upper%2520bounds%2520on%2520the%2520approximation%2520rate%2520of%2520neural%2520network%2520classes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20result%20relating%20convex%20n-widths%20to%20covering%20numbers%20with%20some%20applications%20to%20neural%20networks&entry.906535625=Jonathan%20Baxter%20and%20Peter%20Bartlett&entry.1292438233=In%20general%2C%20approximating%20classes%20of%20functions%20defined%20over%20high-dimensional%20input%20spaces%20by%20linear%20combinations%20of%20a%20fixed%20set%20of%20basis%20functions%20or%20%60%60features%27%27%20is%20known%20to%20be%20hard.%20Typically%2C%20the%20worst-case%20error%20of%20the%20best%20basis%20set%20decays%20only%20as%20fast%20as%20%24%CE%98%5C%28n%5E%7B-1/d%7D%5C%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20basis%20functions%20and%20%24d%24%20is%20the%20input%20dimension.%20However%2C%20there%20are%20many%20examples%20of%20high-dimensional%20pattern%20recognition%20problems%20%28such%20as%20face%20recognition%29%20where%20linear%20combinations%20of%20small%20sets%20of%20features%20do%20solve%20the%20problem%20well.%20Hence%20these%20function%20classes%20do%20not%20suffer%20from%20the%20%60%60curse%20of%20dimensionality%27%27%20associated%20with%20more%20general%20classes.%20It%20is%20natural%20then%2C%20to%20look%20for%20characterizations%20of%20high-dimensional%20function%20classes%20that%20nevertheless%20are%20approximated%20well%20by%20linear%20combinations%20of%20small%20sets%20of%20features.%20In%20this%20paper%20we%20give%20a%20general%20result%20relating%20the%20error%20of%20approximation%20of%20a%20function%20class%20to%20the%20covering%20number%20of%20its%20%60%60convex%20core%27%27.%20For%20one-hidden-layer%20neural%20networks%2C%20covering%20numbers%20of%20the%20class%20of%20functions%20computed%20by%20a%20single%20hidden%20node%20upper%20bound%20the%20covering%20numbers%20of%20the%20convex%20core.%20Hence%2C%20using%20standard%20results%20we%20obtain%20upper%20bounds%20on%20the%20approximation%20rate%20of%20neural%20network%20classes.&entry.1838667208=http%3A//arxiv.org/abs/2512.04912v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning for Transparent Object Depth Completion Using Depth from Non-Transparent Objects", "author": "Xianghui Fan and Zhaoyu Chen and Mengyang Pan and Anping Deng and Hang Yang", "abstract": "The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.", "link": "http://arxiv.org/abs/2512.05006v1", "date": "2025-12-04", "relevancy": 2.2283, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5677}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5506}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20for%20Transparent%20Object%20Depth%20Completion%20Using%20Depth%20from%20Non-Transparent%20Objects&body=Title%3A%20Self-Supervised%20Learning%20for%20Transparent%20Object%20Depth%20Completion%20Using%20Depth%20from%20Non-Transparent%20Objects%0AAuthor%3A%20Xianghui%20Fan%20and%20Zhaoyu%20Chen%20and%20Mengyang%20Pan%20and%20Anping%20Deng%20and%20Hang%20Yang%0AAbstract%3A%20The%20perception%20of%20transparent%20objects%20is%20one%20of%20the%20well-known%20challenges%20in%20computer%20vision.%20Conventional%20depth%20sensors%20have%20difficulty%20in%20sensing%20the%20depth%20of%20transparent%20objects%20due%20to%20refraction%20and%20reflection%20of%20light.%20Previous%20research%20has%20typically%20train%20a%20neural%20network%20to%20complete%20the%20depth%20acquired%20by%20the%20sensor%2C%20and%20this%20method%20can%20quickly%20and%20accurately%20acquire%20accurate%20depth%20maps%20of%20transparent%20objects.%20However%2C%20previous%20training%20relies%20on%20a%20large%20amount%20of%20annotation%20data%20for%20supervision%2C%20and%20the%20labeling%20of%20depth%20maps%20is%20costly.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20new%20self-supervised%20method%20for%20training%20depth%20completion%20networks.%20Our%20method%20simulates%20the%20depth%20deficits%20of%20transparent%20objects%20within%20non-transparent%20regions%20and%20utilizes%20the%20original%20depth%20map%20as%20ground%20truth%20for%20supervision.%20Experiments%20demonstrate%20that%20our%20method%20achieves%20performance%20comparable%20to%20supervised%20approach%2C%20and%20pre-training%20with%20our%20method%20can%20improve%20the%20model%20performance%20when%20the%20training%20samples%20are%20small.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520for%2520Transparent%2520Object%2520Depth%2520Completion%2520Using%2520Depth%2520from%2520Non-Transparent%2520Objects%26entry.906535625%3DXianghui%2520Fan%2520and%2520Zhaoyu%2520Chen%2520and%2520Mengyang%2520Pan%2520and%2520Anping%2520Deng%2520and%2520Hang%2520Yang%26entry.1292438233%3DThe%2520perception%2520of%2520transparent%2520objects%2520is%2520one%2520of%2520the%2520well-known%2520challenges%2520in%2520computer%2520vision.%2520Conventional%2520depth%2520sensors%2520have%2520difficulty%2520in%2520sensing%2520the%2520depth%2520of%2520transparent%2520objects%2520due%2520to%2520refraction%2520and%2520reflection%2520of%2520light.%2520Previous%2520research%2520has%2520typically%2520train%2520a%2520neural%2520network%2520to%2520complete%2520the%2520depth%2520acquired%2520by%2520the%2520sensor%252C%2520and%2520this%2520method%2520can%2520quickly%2520and%2520accurately%2520acquire%2520accurate%2520depth%2520maps%2520of%2520transparent%2520objects.%2520However%252C%2520previous%2520training%2520relies%2520on%2520a%2520large%2520amount%2520of%2520annotation%2520data%2520for%2520supervision%252C%2520and%2520the%2520labeling%2520of%2520depth%2520maps%2520is%2520costly.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520a%2520new%2520self-supervised%2520method%2520for%2520training%2520depth%2520completion%2520networks.%2520Our%2520method%2520simulates%2520the%2520depth%2520deficits%2520of%2520transparent%2520objects%2520within%2520non-transparent%2520regions%2520and%2520utilizes%2520the%2520original%2520depth%2520map%2520as%2520ground%2520truth%2520for%2520supervision.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520performance%2520comparable%2520to%2520supervised%2520approach%252C%2520and%2520pre-training%2520with%2520our%2520method%2520can%2520improve%2520the%2520model%2520performance%2520when%2520the%2520training%2520samples%2520are%2520small.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20for%20Transparent%20Object%20Depth%20Completion%20Using%20Depth%20from%20Non-Transparent%20Objects&entry.906535625=Xianghui%20Fan%20and%20Zhaoyu%20Chen%20and%20Mengyang%20Pan%20and%20Anping%20Deng%20and%20Hang%20Yang&entry.1292438233=The%20perception%20of%20transparent%20objects%20is%20one%20of%20the%20well-known%20challenges%20in%20computer%20vision.%20Conventional%20depth%20sensors%20have%20difficulty%20in%20sensing%20the%20depth%20of%20transparent%20objects%20due%20to%20refraction%20and%20reflection%20of%20light.%20Previous%20research%20has%20typically%20train%20a%20neural%20network%20to%20complete%20the%20depth%20acquired%20by%20the%20sensor%2C%20and%20this%20method%20can%20quickly%20and%20accurately%20acquire%20accurate%20depth%20maps%20of%20transparent%20objects.%20However%2C%20previous%20training%20relies%20on%20a%20large%20amount%20of%20annotation%20data%20for%20supervision%2C%20and%20the%20labeling%20of%20depth%20maps%20is%20costly.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20new%20self-supervised%20method%20for%20training%20depth%20completion%20networks.%20Our%20method%20simulates%20the%20depth%20deficits%20of%20transparent%20objects%20within%20non-transparent%20regions%20and%20utilizes%20the%20original%20depth%20map%20as%20ground%20truth%20for%20supervision.%20Experiments%20demonstrate%20that%20our%20method%20achieves%20performance%20comparable%20to%20supervised%20approach%2C%20and%20pre-training%20with%20our%20method%20can%20improve%20the%20model%20performance%20when%20the%20training%20samples%20are%20small.&entry.1838667208=http%3A//arxiv.org/abs/2512.05006v1&entry.124074799=Read"},
{"title": "The AI Consumer Index (ACE)", "author": "Julien Benchek and Rohit Shetty and Benjamin Hunsberger and Ajay Arun and Zach Richards and Brendan Foody and Osvald Nitski and Bertie Vidgen", "abstract": "We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.", "link": "http://arxiv.org/abs/2512.04921v1", "date": "2025-12-04", "relevancy": 2.2265, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4631}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4376}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20AI%20Consumer%20Index%20%28ACE%29&body=Title%3A%20The%20AI%20Consumer%20Index%20%28ACE%29%0AAuthor%3A%20Julien%20Benchek%20and%20Rohit%20Shetty%20and%20Benjamin%20Hunsberger%20and%20Ajay%20Arun%20and%20Zach%20Richards%20and%20Brendan%20Foody%20and%20Osvald%20Nitski%20and%20Bertie%20Vidgen%0AAbstract%3A%20We%20introduce%20the%20first%20version%20of%20the%20AI%20Consumer%20Index%20%28ACE%29%2C%20a%20benchmark%20for%20assessing%20whether%20frontier%20AI%20models%20can%20perform%20high-value%20consumer%20tasks.%20ACE%20contains%20a%20hidden%20heldout%20set%20of%20400%20test%20cases%2C%20split%20across%20four%20consumer%20activities%3A%20shopping%2C%20food%2C%20gaming%2C%20and%20DIY.%20We%20are%20also%20open%20sourcing%2080%20cases%20as%20a%20devset%20with%20a%20CC-BY%20license.%20For%20the%20ACE%20leaderboard%20we%20evaluated%2010%20frontier%20models%20%28with%20websearch%20turned%20on%29%20using%20a%20novel%20grading%20methodology%20that%20dynamically%20checks%20whether%20relevant%20parts%20of%20the%20response%20are%20grounded%20in%20the%20retrieved%20web%20sources.%20GPT%205%20%28Thinking%20%3D%20High%29%20is%20the%20top-performing%20model%2C%20scoring%2056.1%25%2C%20followed%20by%20o3%20Pro%20%28Thinking%20%3D%20On%29%20%2855.2%25%29%20and%20GPT%205.1%20%28Thinking%20%3D%20High%29%20%2855.1%25%29.%20Models%20differ%20across%20domains%2C%20and%20in%20Shopping%20the%20top%20model%20scores%20under%2050%25.%20For%20some%20requests%20%28such%20as%20giving%20the%20correct%20price%20or%20providing%20working%20links%29%2C%20models%20are%20highly%20prone%20to%20hallucination.%20Overall%2C%20ACE%20shows%20a%20substantial%20gap%20between%20the%20performance%20of%20even%20the%20best%20models%20and%20consumers%27%20AI%20needs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520AI%2520Consumer%2520Index%2520%2528ACE%2529%26entry.906535625%3DJulien%2520Benchek%2520and%2520Rohit%2520Shetty%2520and%2520Benjamin%2520Hunsberger%2520and%2520Ajay%2520Arun%2520and%2520Zach%2520Richards%2520and%2520Brendan%2520Foody%2520and%2520Osvald%2520Nitski%2520and%2520Bertie%2520Vidgen%26entry.1292438233%3DWe%2520introduce%2520the%2520first%2520version%2520of%2520the%2520AI%2520Consumer%2520Index%2520%2528ACE%2529%252C%2520a%2520benchmark%2520for%2520assessing%2520whether%2520frontier%2520AI%2520models%2520can%2520perform%2520high-value%2520consumer%2520tasks.%2520ACE%2520contains%2520a%2520hidden%2520heldout%2520set%2520of%2520400%2520test%2520cases%252C%2520split%2520across%2520four%2520consumer%2520activities%253A%2520shopping%252C%2520food%252C%2520gaming%252C%2520and%2520DIY.%2520We%2520are%2520also%2520open%2520sourcing%252080%2520cases%2520as%2520a%2520devset%2520with%2520a%2520CC-BY%2520license.%2520For%2520the%2520ACE%2520leaderboard%2520we%2520evaluated%252010%2520frontier%2520models%2520%2528with%2520websearch%2520turned%2520on%2529%2520using%2520a%2520novel%2520grading%2520methodology%2520that%2520dynamically%2520checks%2520whether%2520relevant%2520parts%2520of%2520the%2520response%2520are%2520grounded%2520in%2520the%2520retrieved%2520web%2520sources.%2520GPT%25205%2520%2528Thinking%2520%253D%2520High%2529%2520is%2520the%2520top-performing%2520model%252C%2520scoring%252056.1%2525%252C%2520followed%2520by%2520o3%2520Pro%2520%2528Thinking%2520%253D%2520On%2529%2520%252855.2%2525%2529%2520and%2520GPT%25205.1%2520%2528Thinking%2520%253D%2520High%2529%2520%252855.1%2525%2529.%2520Models%2520differ%2520across%2520domains%252C%2520and%2520in%2520Shopping%2520the%2520top%2520model%2520scores%2520under%252050%2525.%2520For%2520some%2520requests%2520%2528such%2520as%2520giving%2520the%2520correct%2520price%2520or%2520providing%2520working%2520links%2529%252C%2520models%2520are%2520highly%2520prone%2520to%2520hallucination.%2520Overall%252C%2520ACE%2520shows%2520a%2520substantial%2520gap%2520between%2520the%2520performance%2520of%2520even%2520the%2520best%2520models%2520and%2520consumers%2527%2520AI%2520needs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20AI%20Consumer%20Index%20%28ACE%29&entry.906535625=Julien%20Benchek%20and%20Rohit%20Shetty%20and%20Benjamin%20Hunsberger%20and%20Ajay%20Arun%20and%20Zach%20Richards%20and%20Brendan%20Foody%20and%20Osvald%20Nitski%20and%20Bertie%20Vidgen&entry.1292438233=We%20introduce%20the%20first%20version%20of%20the%20AI%20Consumer%20Index%20%28ACE%29%2C%20a%20benchmark%20for%20assessing%20whether%20frontier%20AI%20models%20can%20perform%20high-value%20consumer%20tasks.%20ACE%20contains%20a%20hidden%20heldout%20set%20of%20400%20test%20cases%2C%20split%20across%20four%20consumer%20activities%3A%20shopping%2C%20food%2C%20gaming%2C%20and%20DIY.%20We%20are%20also%20open%20sourcing%2080%20cases%20as%20a%20devset%20with%20a%20CC-BY%20license.%20For%20the%20ACE%20leaderboard%20we%20evaluated%2010%20frontier%20models%20%28with%20websearch%20turned%20on%29%20using%20a%20novel%20grading%20methodology%20that%20dynamically%20checks%20whether%20relevant%20parts%20of%20the%20response%20are%20grounded%20in%20the%20retrieved%20web%20sources.%20GPT%205%20%28Thinking%20%3D%20High%29%20is%20the%20top-performing%20model%2C%20scoring%2056.1%25%2C%20followed%20by%20o3%20Pro%20%28Thinking%20%3D%20On%29%20%2855.2%25%29%20and%20GPT%205.1%20%28Thinking%20%3D%20High%29%20%2855.1%25%29.%20Models%20differ%20across%20domains%2C%20and%20in%20Shopping%20the%20top%20model%20scores%20under%2050%25.%20For%20some%20requests%20%28such%20as%20giving%20the%20correct%20price%20or%20providing%20working%20links%29%2C%20models%20are%20highly%20prone%20to%20hallucination.%20Overall%2C%20ACE%20shows%20a%20substantial%20gap%20between%20the%20performance%20of%20even%20the%20best%20models%20and%20consumers%27%20AI%20needs.&entry.1838667208=http%3A//arxiv.org/abs/2512.04921v1&entry.124074799=Read"},
{"title": "Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation", "author": "Yongfu Xue", "abstract": "The rapid development of parameter-efficient fine-tuning methods has noticeably improved the efficiency of adapting large language models. Among these, LoRA has gained widespread popularity due to its strong balance of effectiveness and parameter efficiency. However, LoRA relies on initializing two low-rank matrices whose product is zero, which limits its ability to effectively activate and leverage the original model weights-creating a potential bottleneck for optimal performance. To address this limitation, we propose \\textbf{IniLoRA}, a novel initialization strategy that initializes the low-rank matrices to closely approximate the original model weights. Experimental results indicate that IniLoRA achieves better performance than LoRA across a range of models and tasks. Additionally, we introduce two variants, IniLoRA-$\u03b1$ and IniLoRA-$\u03b2$, both leveraging distinct initialization methods to enhance performance further.", "link": "http://arxiv.org/abs/2510.03731v2", "date": "2025-12-04", "relevancy": 2.2213, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4502}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4431}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Fine-Tuning%20through%20Advanced%20Initialization%20Strategies%20for%20Low-Rank%20Adaptation&body=Title%3A%20Optimizing%20Fine-Tuning%20through%20Advanced%20Initialization%20Strategies%20for%20Low-Rank%20Adaptation%0AAuthor%3A%20Yongfu%20Xue%0AAbstract%3A%20The%20rapid%20development%20of%20parameter-efficient%20fine-tuning%20methods%20has%20noticeably%20improved%20the%20efficiency%20of%20adapting%20large%20language%20models.%20Among%20these%2C%20LoRA%20has%20gained%20widespread%20popularity%20due%20to%20its%20strong%20balance%20of%20effectiveness%20and%20parameter%20efficiency.%20However%2C%20LoRA%20relies%20on%20initializing%20two%20low-rank%20matrices%20whose%20product%20is%20zero%2C%20which%20limits%20its%20ability%20to%20effectively%20activate%20and%20leverage%20the%20original%20model%20weights-creating%20a%20potential%20bottleneck%20for%20optimal%20performance.%20To%20address%20this%20limitation%2C%20we%20propose%20%5Ctextbf%7BIniLoRA%7D%2C%20a%20novel%20initialization%20strategy%20that%20initializes%20the%20low-rank%20matrices%20to%20closely%20approximate%20the%20original%20model%20weights.%20Experimental%20results%20indicate%20that%20IniLoRA%20achieves%20better%20performance%20than%20LoRA%20across%20a%20range%20of%20models%20and%20tasks.%20Additionally%2C%20we%20introduce%20two%20variants%2C%20IniLoRA-%24%CE%B1%24%20and%20IniLoRA-%24%CE%B2%24%2C%20both%20leveraging%20distinct%20initialization%20methods%20to%20enhance%20performance%20further.%0ALink%3A%20http%3A//arxiv.org/abs/2510.03731v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Fine-Tuning%2520through%2520Advanced%2520Initialization%2520Strategies%2520for%2520Low-Rank%2520Adaptation%26entry.906535625%3DYongfu%2520Xue%26entry.1292438233%3DThe%2520rapid%2520development%2520of%2520parameter-efficient%2520fine-tuning%2520methods%2520has%2520noticeably%2520improved%2520the%2520efficiency%2520of%2520adapting%2520large%2520language%2520models.%2520Among%2520these%252C%2520LoRA%2520has%2520gained%2520widespread%2520popularity%2520due%2520to%2520its%2520strong%2520balance%2520of%2520effectiveness%2520and%2520parameter%2520efficiency.%2520However%252C%2520LoRA%2520relies%2520on%2520initializing%2520two%2520low-rank%2520matrices%2520whose%2520product%2520is%2520zero%252C%2520which%2520limits%2520its%2520ability%2520to%2520effectively%2520activate%2520and%2520leverage%2520the%2520original%2520model%2520weights-creating%2520a%2520potential%2520bottleneck%2520for%2520optimal%2520performance.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520%255Ctextbf%257BIniLoRA%257D%252C%2520a%2520novel%2520initialization%2520strategy%2520that%2520initializes%2520the%2520low-rank%2520matrices%2520to%2520closely%2520approximate%2520the%2520original%2520model%2520weights.%2520Experimental%2520results%2520indicate%2520that%2520IniLoRA%2520achieves%2520better%2520performance%2520than%2520LoRA%2520across%2520a%2520range%2520of%2520models%2520and%2520tasks.%2520Additionally%252C%2520we%2520introduce%2520two%2520variants%252C%2520IniLoRA-%2524%25CE%25B1%2524%2520and%2520IniLoRA-%2524%25CE%25B2%2524%252C%2520both%2520leveraging%2520distinct%2520initialization%2520methods%2520to%2520enhance%2520performance%2520further.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03731v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Fine-Tuning%20through%20Advanced%20Initialization%20Strategies%20for%20Low-Rank%20Adaptation&entry.906535625=Yongfu%20Xue&entry.1292438233=The%20rapid%20development%20of%20parameter-efficient%20fine-tuning%20methods%20has%20noticeably%20improved%20the%20efficiency%20of%20adapting%20large%20language%20models.%20Among%20these%2C%20LoRA%20has%20gained%20widespread%20popularity%20due%20to%20its%20strong%20balance%20of%20effectiveness%20and%20parameter%20efficiency.%20However%2C%20LoRA%20relies%20on%20initializing%20two%20low-rank%20matrices%20whose%20product%20is%20zero%2C%20which%20limits%20its%20ability%20to%20effectively%20activate%20and%20leverage%20the%20original%20model%20weights-creating%20a%20potential%20bottleneck%20for%20optimal%20performance.%20To%20address%20this%20limitation%2C%20we%20propose%20%5Ctextbf%7BIniLoRA%7D%2C%20a%20novel%20initialization%20strategy%20that%20initializes%20the%20low-rank%20matrices%20to%20closely%20approximate%20the%20original%20model%20weights.%20Experimental%20results%20indicate%20that%20IniLoRA%20achieves%20better%20performance%20than%20LoRA%20across%20a%20range%20of%20models%20and%20tasks.%20Additionally%2C%20we%20introduce%20two%20variants%2C%20IniLoRA-%24%CE%B1%24%20and%20IniLoRA-%24%CE%B2%24%2C%20both%20leveraging%20distinct%20initialization%20methods%20to%20enhance%20performance%20further.&entry.1838667208=http%3A//arxiv.org/abs/2510.03731v2&entry.124074799=Read"},
{"title": "Hoi! -- A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation", "author": "Tim Engelbracht and Ren\u00e9 Zurbr\u00fcgg and Matteo Wohlrapp and Martin B\u00fcchner and Abhinav Valada and Marc Pollefeys and Hermann Blum and Zuria Bauer", "abstract": "We present a dataset for force-grounded, cross-view articulated manipulation that couples what is seen with what is done and what is felt during real human interaction. The dataset contains 3048 sequences across 381 articulated objects in 38 environments. Each object is operated under four embodiments - (i) human hand, (ii) human hand with a wrist-mounted camera, (iii) handheld UMI gripper, and (iv) a custom Hoi! gripper - where the tool embodiment provides synchronized end-effector forces and tactile sensing. Our dataset offers a holistic view of interaction understanding from video, enabling researchers to evaluate how well methods transfer between human and robotic viewpoints, but also investigate underexplored modalities such as force sensing and prediction.", "link": "http://arxiv.org/abs/2512.04884v1", "date": "2025-12-04", "relevancy": 2.2133, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5691}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5422}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hoi%21%20--%20A%20Multimodal%20Dataset%20for%20Force-Grounded%2C%20Cross-View%20Articulated%20Manipulation&body=Title%3A%20Hoi%21%20--%20A%20Multimodal%20Dataset%20for%20Force-Grounded%2C%20Cross-View%20Articulated%20Manipulation%0AAuthor%3A%20Tim%20Engelbracht%20and%20Ren%C3%A9%20Zurbr%C3%BCgg%20and%20Matteo%20Wohlrapp%20and%20Martin%20B%C3%BCchner%20and%20Abhinav%20Valada%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Zuria%20Bauer%0AAbstract%3A%20We%20present%20a%20dataset%20for%20force-grounded%2C%20cross-view%20articulated%20manipulation%20that%20couples%20what%20is%20seen%20with%20what%20is%20done%20and%20what%20is%20felt%20during%20real%20human%20interaction.%20The%20dataset%20contains%203048%20sequences%20across%20381%20articulated%20objects%20in%2038%20environments.%20Each%20object%20is%20operated%20under%20four%20embodiments%20-%20%28i%29%20human%20hand%2C%20%28ii%29%20human%20hand%20with%20a%20wrist-mounted%20camera%2C%20%28iii%29%20handheld%20UMI%20gripper%2C%20and%20%28iv%29%20a%20custom%20Hoi%21%20gripper%20-%20where%20the%20tool%20embodiment%20provides%20synchronized%20end-effector%20forces%20and%20tactile%20sensing.%20Our%20dataset%20offers%20a%20holistic%20view%20of%20interaction%20understanding%20from%20video%2C%20enabling%20researchers%20to%20evaluate%20how%20well%20methods%20transfer%20between%20human%20and%20robotic%20viewpoints%2C%20but%20also%20investigate%20underexplored%20modalities%20such%20as%20force%20sensing%20and%20prediction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoi%2521%2520--%2520A%2520Multimodal%2520Dataset%2520for%2520Force-Grounded%252C%2520Cross-View%2520Articulated%2520Manipulation%26entry.906535625%3DTim%2520Engelbracht%2520and%2520Ren%25C3%25A9%2520Zurbr%25C3%25BCgg%2520and%2520Matteo%2520Wohlrapp%2520and%2520Martin%2520B%25C3%25BCchner%2520and%2520Abhinav%2520Valada%2520and%2520Marc%2520Pollefeys%2520and%2520Hermann%2520Blum%2520and%2520Zuria%2520Bauer%26entry.1292438233%3DWe%2520present%2520a%2520dataset%2520for%2520force-grounded%252C%2520cross-view%2520articulated%2520manipulation%2520that%2520couples%2520what%2520is%2520seen%2520with%2520what%2520is%2520done%2520and%2520what%2520is%2520felt%2520during%2520real%2520human%2520interaction.%2520The%2520dataset%2520contains%25203048%2520sequences%2520across%2520381%2520articulated%2520objects%2520in%252038%2520environments.%2520Each%2520object%2520is%2520operated%2520under%2520four%2520embodiments%2520-%2520%2528i%2529%2520human%2520hand%252C%2520%2528ii%2529%2520human%2520hand%2520with%2520a%2520wrist-mounted%2520camera%252C%2520%2528iii%2529%2520handheld%2520UMI%2520gripper%252C%2520and%2520%2528iv%2529%2520a%2520custom%2520Hoi%2521%2520gripper%2520-%2520where%2520the%2520tool%2520embodiment%2520provides%2520synchronized%2520end-effector%2520forces%2520and%2520tactile%2520sensing.%2520Our%2520dataset%2520offers%2520a%2520holistic%2520view%2520of%2520interaction%2520understanding%2520from%2520video%252C%2520enabling%2520researchers%2520to%2520evaluate%2520how%2520well%2520methods%2520transfer%2520between%2520human%2520and%2520robotic%2520viewpoints%252C%2520but%2520also%2520investigate%2520underexplored%2520modalities%2520such%2520as%2520force%2520sensing%2520and%2520prediction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hoi%21%20--%20A%20Multimodal%20Dataset%20for%20Force-Grounded%2C%20Cross-View%20Articulated%20Manipulation&entry.906535625=Tim%20Engelbracht%20and%20Ren%C3%A9%20Zurbr%C3%BCgg%20and%20Matteo%20Wohlrapp%20and%20Martin%20B%C3%BCchner%20and%20Abhinav%20Valada%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Zuria%20Bauer&entry.1292438233=We%20present%20a%20dataset%20for%20force-grounded%2C%20cross-view%20articulated%20manipulation%20that%20couples%20what%20is%20seen%20with%20what%20is%20done%20and%20what%20is%20felt%20during%20real%20human%20interaction.%20The%20dataset%20contains%203048%20sequences%20across%20381%20articulated%20objects%20in%2038%20environments.%20Each%20object%20is%20operated%20under%20four%20embodiments%20-%20%28i%29%20human%20hand%2C%20%28ii%29%20human%20hand%20with%20a%20wrist-mounted%20camera%2C%20%28iii%29%20handheld%20UMI%20gripper%2C%20and%20%28iv%29%20a%20custom%20Hoi%21%20gripper%20-%20where%20the%20tool%20embodiment%20provides%20synchronized%20end-effector%20forces%20and%20tactile%20sensing.%20Our%20dataset%20offers%20a%20holistic%20view%20of%20interaction%20understanding%20from%20video%2C%20enabling%20researchers%20to%20evaluate%20how%20well%20methods%20transfer%20between%20human%20and%20robotic%20viewpoints%2C%20but%20also%20investigate%20underexplored%20modalities%20such%20as%20force%20sensing%20and%20prediction.&entry.1838667208=http%3A//arxiv.org/abs/2512.04884v1&entry.124074799=Read"},
{"title": "Consistent spectral clustering in sparse tensor block models", "author": "Ian V\u00e4limaa and Lasse Leskel\u00e4", "abstract": "High-order clustering aims to classify objects in multiway datasets that are prevalent in various fields such as bioinformatics, recommendation systems, and social network analysis. Such data are often sparse and high-dimensional, posing significant statistical and computational challenges. This paper introduces a tensor block model specifically designed for sparse integer-valued data tensors. We propose a simple spectral clustering algorithm augmented with a trimming step to mitigate noise fluctuations, and identify a density threshold that ensures the algorithm's consistency. Our approach models sparsity using a sub-Poisson noise concentration framework, accommodating heavier than sub-Gaussian tails. Remarkably, this natural class of tensor block models is closed under aggregation across arbitrary modes. Consequently, we obtain a comprehensive framework for evaluating the tradeoff between signal loss and noise reduction incurred by aggregating data. The analysis is based on a novel concentration bound for sparse random Gram matrices. The theoretical findings are illustrated through numerical experiments.", "link": "http://arxiv.org/abs/2501.13820v2", "date": "2025-12-04", "relevancy": 2.2076, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4493}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4443}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20spectral%20clustering%20in%20sparse%20tensor%20block%20models&body=Title%3A%20Consistent%20spectral%20clustering%20in%20sparse%20tensor%20block%20models%0AAuthor%3A%20Ian%20V%C3%A4limaa%20and%20Lasse%20Leskel%C3%A4%0AAbstract%3A%20High-order%20clustering%20aims%20to%20classify%20objects%20in%20multiway%20datasets%20that%20are%20prevalent%20in%20various%20fields%20such%20as%20bioinformatics%2C%20recommendation%20systems%2C%20and%20social%20network%20analysis.%20Such%20data%20are%20often%20sparse%20and%20high-dimensional%2C%20posing%20significant%20statistical%20and%20computational%20challenges.%20This%20paper%20introduces%20a%20tensor%20block%20model%20specifically%20designed%20for%20sparse%20integer-valued%20data%20tensors.%20We%20propose%20a%20simple%20spectral%20clustering%20algorithm%20augmented%20with%20a%20trimming%20step%20to%20mitigate%20noise%20fluctuations%2C%20and%20identify%20a%20density%20threshold%20that%20ensures%20the%20algorithm%27s%20consistency.%20Our%20approach%20models%20sparsity%20using%20a%20sub-Poisson%20noise%20concentration%20framework%2C%20accommodating%20heavier%20than%20sub-Gaussian%20tails.%20Remarkably%2C%20this%20natural%20class%20of%20tensor%20block%20models%20is%20closed%20under%20aggregation%20across%20arbitrary%20modes.%20Consequently%2C%20we%20obtain%20a%20comprehensive%20framework%20for%20evaluating%20the%20tradeoff%20between%20signal%20loss%20and%20noise%20reduction%20incurred%20by%20aggregating%20data.%20The%20analysis%20is%20based%20on%20a%20novel%20concentration%20bound%20for%20sparse%20random%20Gram%20matrices.%20The%20theoretical%20findings%20are%20illustrated%20through%20numerical%20experiments.%0ALink%3A%20http%3A//arxiv.org/abs/2501.13820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520spectral%2520clustering%2520in%2520sparse%2520tensor%2520block%2520models%26entry.906535625%3DIan%2520V%25C3%25A4limaa%2520and%2520Lasse%2520Leskel%25C3%25A4%26entry.1292438233%3DHigh-order%2520clustering%2520aims%2520to%2520classify%2520objects%2520in%2520multiway%2520datasets%2520that%2520are%2520prevalent%2520in%2520various%2520fields%2520such%2520as%2520bioinformatics%252C%2520recommendation%2520systems%252C%2520and%2520social%2520network%2520analysis.%2520Such%2520data%2520are%2520often%2520sparse%2520and%2520high-dimensional%252C%2520posing%2520significant%2520statistical%2520and%2520computational%2520challenges.%2520This%2520paper%2520introduces%2520a%2520tensor%2520block%2520model%2520specifically%2520designed%2520for%2520sparse%2520integer-valued%2520data%2520tensors.%2520We%2520propose%2520a%2520simple%2520spectral%2520clustering%2520algorithm%2520augmented%2520with%2520a%2520trimming%2520step%2520to%2520mitigate%2520noise%2520fluctuations%252C%2520and%2520identify%2520a%2520density%2520threshold%2520that%2520ensures%2520the%2520algorithm%2527s%2520consistency.%2520Our%2520approach%2520models%2520sparsity%2520using%2520a%2520sub-Poisson%2520noise%2520concentration%2520framework%252C%2520accommodating%2520heavier%2520than%2520sub-Gaussian%2520tails.%2520Remarkably%252C%2520this%2520natural%2520class%2520of%2520tensor%2520block%2520models%2520is%2520closed%2520under%2520aggregation%2520across%2520arbitrary%2520modes.%2520Consequently%252C%2520we%2520obtain%2520a%2520comprehensive%2520framework%2520for%2520evaluating%2520the%2520tradeoff%2520between%2520signal%2520loss%2520and%2520noise%2520reduction%2520incurred%2520by%2520aggregating%2520data.%2520The%2520analysis%2520is%2520based%2520on%2520a%2520novel%2520concentration%2520bound%2520for%2520sparse%2520random%2520Gram%2520matrices.%2520The%2520theoretical%2520findings%2520are%2520illustrated%2520through%2520numerical%2520experiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20spectral%20clustering%20in%20sparse%20tensor%20block%20models&entry.906535625=Ian%20V%C3%A4limaa%20and%20Lasse%20Leskel%C3%A4&entry.1292438233=High-order%20clustering%20aims%20to%20classify%20objects%20in%20multiway%20datasets%20that%20are%20prevalent%20in%20various%20fields%20such%20as%20bioinformatics%2C%20recommendation%20systems%2C%20and%20social%20network%20analysis.%20Such%20data%20are%20often%20sparse%20and%20high-dimensional%2C%20posing%20significant%20statistical%20and%20computational%20challenges.%20This%20paper%20introduces%20a%20tensor%20block%20model%20specifically%20designed%20for%20sparse%20integer-valued%20data%20tensors.%20We%20propose%20a%20simple%20spectral%20clustering%20algorithm%20augmented%20with%20a%20trimming%20step%20to%20mitigate%20noise%20fluctuations%2C%20and%20identify%20a%20density%20threshold%20that%20ensures%20the%20algorithm%27s%20consistency.%20Our%20approach%20models%20sparsity%20using%20a%20sub-Poisson%20noise%20concentration%20framework%2C%20accommodating%20heavier%20than%20sub-Gaussian%20tails.%20Remarkably%2C%20this%20natural%20class%20of%20tensor%20block%20models%20is%20closed%20under%20aggregation%20across%20arbitrary%20modes.%20Consequently%2C%20we%20obtain%20a%20comprehensive%20framework%20for%20evaluating%20the%20tradeoff%20between%20signal%20loss%20and%20noise%20reduction%20incurred%20by%20aggregating%20data.%20The%20analysis%20is%20based%20on%20a%20novel%20concentration%20bound%20for%20sparse%20random%20Gram%20matrices.%20The%20theoretical%20findings%20are%20illustrated%20through%20numerical%20experiments.&entry.1838667208=http%3A//arxiv.org/abs/2501.13820v2&entry.124074799=Read"},
{"title": "Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems", "author": "M Zeeshan and Saud Satti", "abstract": "Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.", "link": "http://arxiv.org/abs/2512.04895v1", "date": "2025-12-04", "relevancy": 2.1883, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5836}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5216}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chameleon%3A%20Adaptive%20Adversarial%20Agents%20for%20Scaling-Based%20Visual%20Prompt%20Injection%20in%20Multimodal%20AI%20Systems&body=Title%3A%20Chameleon%3A%20Adaptive%20Adversarial%20Agents%20for%20Scaling-Based%20Visual%20Prompt%20Injection%20in%20Multimodal%20AI%20Systems%0AAuthor%3A%20M%20Zeeshan%20and%20Saud%20Satti%0AAbstract%3A%20Multimodal%20Artificial%20Intelligence%20%28AI%29%20systems%2C%20particularly%20Vision-Language%20Models%20%28VLMs%29%2C%20have%20become%20integral%20to%20critical%20applications%20ranging%20from%20autonomous%20decision-making%20to%20automated%20document%20processing.%20As%20these%20systems%20scale%2C%20they%20rely%20heavily%20on%20preprocessing%20pipelines%20to%20handle%20diverse%20inputs%20efficiently.%20However%2C%20this%20dependency%20on%20standard%20preprocessing%20operations%2C%20specifically%20image%20downscaling%2C%20creates%20a%20significant%20yet%20often%20overlooked%20security%20vulnerability.%20While%20intended%20for%20computational%20optimization%2C%20scaling%20algorithms%20can%20be%20exploited%20to%20conceal%20malicious%20visual%20prompts%20that%20are%20invisible%20to%20human%20observers%20but%20become%20active%20semantic%20instructions%20once%20processed%20by%20the%20model.%20Current%20adversarial%20strategies%20remain%20largely%20static%2C%20failing%20to%20account%20for%20the%20dynamic%20nature%20of%20modern%20agentic%20workflows.%20To%20address%20this%20gap%2C%20we%20propose%20Chameleon%2C%20a%20novel%2C%20adaptive%20adversarial%20framework%20designed%20to%20expose%20and%20exploit%20scaling%20vulnerabilities%20in%20production%20VLMs.%20Unlike%20traditional%20static%20attacks%2C%20Chameleon%20employs%20an%20iterative%2C%20agent-based%20optimization%20mechanism%20that%20dynamically%20refines%20image%20perturbations%20based%20on%20the%20target%20model%27s%20real-time%20feedback.%20This%20allows%20the%20framework%20to%20craft%20highly%20robust%20adversarial%20examples%20that%20survive%20standard%20downscaling%20operations%20to%20hijack%20downstream%20execution.%20We%20evaluate%20Chameleon%20against%20Gemini%202.5%20Flash%20model.%20Our%20experiments%20demonstrate%20that%20Chameleon%20achieves%20an%20Attack%20Success%20Rate%20%28ASR%29%20of%2084.5%25%20across%20varying%20scaling%20factors%2C%20significantly%20outperforming%20static%20baseline%20attacks%20which%20average%20only%2032.1%25.%20Furthermore%2C%20we%20show%20that%20these%20attacks%20effectively%20compromise%20agentic%20pipelines%2C%20reducing%20decision-making%20accuracy%20by%20over%2045%25%20in%20multi-step%20tasks.%20Finally%2C%20we%20discuss%20the%20implications%20of%20these%20vulnerabilities%20and%20propose%20multi-scale%20consistency%20checks%20as%20a%20necessary%20defense%20mechanism.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChameleon%253A%2520Adaptive%2520Adversarial%2520Agents%2520for%2520Scaling-Based%2520Visual%2520Prompt%2520Injection%2520in%2520Multimodal%2520AI%2520Systems%26entry.906535625%3DM%2520Zeeshan%2520and%2520Saud%2520Satti%26entry.1292438233%3DMultimodal%2520Artificial%2520Intelligence%2520%2528AI%2529%2520systems%252C%2520particularly%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520have%2520become%2520integral%2520to%2520critical%2520applications%2520ranging%2520from%2520autonomous%2520decision-making%2520to%2520automated%2520document%2520processing.%2520As%2520these%2520systems%2520scale%252C%2520they%2520rely%2520heavily%2520on%2520preprocessing%2520pipelines%2520to%2520handle%2520diverse%2520inputs%2520efficiently.%2520However%252C%2520this%2520dependency%2520on%2520standard%2520preprocessing%2520operations%252C%2520specifically%2520image%2520downscaling%252C%2520creates%2520a%2520significant%2520yet%2520often%2520overlooked%2520security%2520vulnerability.%2520While%2520intended%2520for%2520computational%2520optimization%252C%2520scaling%2520algorithms%2520can%2520be%2520exploited%2520to%2520conceal%2520malicious%2520visual%2520prompts%2520that%2520are%2520invisible%2520to%2520human%2520observers%2520but%2520become%2520active%2520semantic%2520instructions%2520once%2520processed%2520by%2520the%2520model.%2520Current%2520adversarial%2520strategies%2520remain%2520largely%2520static%252C%2520failing%2520to%2520account%2520for%2520the%2520dynamic%2520nature%2520of%2520modern%2520agentic%2520workflows.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520Chameleon%252C%2520a%2520novel%252C%2520adaptive%2520adversarial%2520framework%2520designed%2520to%2520expose%2520and%2520exploit%2520scaling%2520vulnerabilities%2520in%2520production%2520VLMs.%2520Unlike%2520traditional%2520static%2520attacks%252C%2520Chameleon%2520employs%2520an%2520iterative%252C%2520agent-based%2520optimization%2520mechanism%2520that%2520dynamically%2520refines%2520image%2520perturbations%2520based%2520on%2520the%2520target%2520model%2527s%2520real-time%2520feedback.%2520This%2520allows%2520the%2520framework%2520to%2520craft%2520highly%2520robust%2520adversarial%2520examples%2520that%2520survive%2520standard%2520downscaling%2520operations%2520to%2520hijack%2520downstream%2520execution.%2520We%2520evaluate%2520Chameleon%2520against%2520Gemini%25202.5%2520Flash%2520model.%2520Our%2520experiments%2520demonstrate%2520that%2520Chameleon%2520achieves%2520an%2520Attack%2520Success%2520Rate%2520%2528ASR%2529%2520of%252084.5%2525%2520across%2520varying%2520scaling%2520factors%252C%2520significantly%2520outperforming%2520static%2520baseline%2520attacks%2520which%2520average%2520only%252032.1%2525.%2520Furthermore%252C%2520we%2520show%2520that%2520these%2520attacks%2520effectively%2520compromise%2520agentic%2520pipelines%252C%2520reducing%2520decision-making%2520accuracy%2520by%2520over%252045%2525%2520in%2520multi-step%2520tasks.%2520Finally%252C%2520we%2520discuss%2520the%2520implications%2520of%2520these%2520vulnerabilities%2520and%2520propose%2520multi-scale%2520consistency%2520checks%2520as%2520a%2520necessary%2520defense%2520mechanism.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chameleon%3A%20Adaptive%20Adversarial%20Agents%20for%20Scaling-Based%20Visual%20Prompt%20Injection%20in%20Multimodal%20AI%20Systems&entry.906535625=M%20Zeeshan%20and%20Saud%20Satti&entry.1292438233=Multimodal%20Artificial%20Intelligence%20%28AI%29%20systems%2C%20particularly%20Vision-Language%20Models%20%28VLMs%29%2C%20have%20become%20integral%20to%20critical%20applications%20ranging%20from%20autonomous%20decision-making%20to%20automated%20document%20processing.%20As%20these%20systems%20scale%2C%20they%20rely%20heavily%20on%20preprocessing%20pipelines%20to%20handle%20diverse%20inputs%20efficiently.%20However%2C%20this%20dependency%20on%20standard%20preprocessing%20operations%2C%20specifically%20image%20downscaling%2C%20creates%20a%20significant%20yet%20often%20overlooked%20security%20vulnerability.%20While%20intended%20for%20computational%20optimization%2C%20scaling%20algorithms%20can%20be%20exploited%20to%20conceal%20malicious%20visual%20prompts%20that%20are%20invisible%20to%20human%20observers%20but%20become%20active%20semantic%20instructions%20once%20processed%20by%20the%20model.%20Current%20adversarial%20strategies%20remain%20largely%20static%2C%20failing%20to%20account%20for%20the%20dynamic%20nature%20of%20modern%20agentic%20workflows.%20To%20address%20this%20gap%2C%20we%20propose%20Chameleon%2C%20a%20novel%2C%20adaptive%20adversarial%20framework%20designed%20to%20expose%20and%20exploit%20scaling%20vulnerabilities%20in%20production%20VLMs.%20Unlike%20traditional%20static%20attacks%2C%20Chameleon%20employs%20an%20iterative%2C%20agent-based%20optimization%20mechanism%20that%20dynamically%20refines%20image%20perturbations%20based%20on%20the%20target%20model%27s%20real-time%20feedback.%20This%20allows%20the%20framework%20to%20craft%20highly%20robust%20adversarial%20examples%20that%20survive%20standard%20downscaling%20operations%20to%20hijack%20downstream%20execution.%20We%20evaluate%20Chameleon%20against%20Gemini%202.5%20Flash%20model.%20Our%20experiments%20demonstrate%20that%20Chameleon%20achieves%20an%20Attack%20Success%20Rate%20%28ASR%29%20of%2084.5%25%20across%20varying%20scaling%20factors%2C%20significantly%20outperforming%20static%20baseline%20attacks%20which%20average%20only%2032.1%25.%20Furthermore%2C%20we%20show%20that%20these%20attacks%20effectively%20compromise%20agentic%20pipelines%2C%20reducing%20decision-making%20accuracy%20by%20over%2045%25%20in%20multi-step%20tasks.%20Finally%2C%20we%20discuss%20the%20implications%20of%20these%20vulnerabilities%20and%20propose%20multi-scale%20consistency%20checks%20as%20a%20necessary%20defense%20mechanism.&entry.1838667208=http%3A//arxiv.org/abs/2512.04895v1&entry.124074799=Read"},
{"title": "Visuospatial navigation from the bottom-up: without vestibular integration, distance prediction, or maps", "author": "Patrick Govoni and Pawel Romanczuk", "abstract": "Navigation is believed to be controlled by at least two partially dissociable systems in the brain. The cognitive map informs an organism of its location and bearing, updated by integrating vestibular self-motion or predicting distances to landmarks. Route-based navigation, on the other hand, directly evaluate sequential movement decisions from immediate percepts. Here we demonstrate the sufficiency of visual route-based decision-making in a classic open field navigation task often assumed to require a cognitive map. Three distinct strategies emerge to robustly navigate to a hidden goal, each conferring contextual tradeoffs analyzed at both neural and behavioral scales, as well as qualitatively aligning with behavior observed across the biological spectrum. We propose reframing navigation from the bottom-up, through an egocentric episodic perspective without assuming online access to computationally expensive top-down representations, to better explain behavior under energetic or attentional constraints.", "link": "http://arxiv.org/abs/2407.13535v4", "date": "2025-12-04", "relevancy": 2.1851, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5898}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5163}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visuospatial%20navigation%20from%20the%20bottom-up%3A%20without%20vestibular%20integration%2C%20distance%20prediction%2C%20or%20maps&body=Title%3A%20Visuospatial%20navigation%20from%20the%20bottom-up%3A%20without%20vestibular%20integration%2C%20distance%20prediction%2C%20or%20maps%0AAuthor%3A%20Patrick%20Govoni%20and%20Pawel%20Romanczuk%0AAbstract%3A%20Navigation%20is%20believed%20to%20be%20controlled%20by%20at%20least%20two%20partially%20dissociable%20systems%20in%20the%20brain.%20The%20cognitive%20map%20informs%20an%20organism%20of%20its%20location%20and%20bearing%2C%20updated%20by%20integrating%20vestibular%20self-motion%20or%20predicting%20distances%20to%20landmarks.%20Route-based%20navigation%2C%20on%20the%20other%20hand%2C%20directly%20evaluate%20sequential%20movement%20decisions%20from%20immediate%20percepts.%20Here%20we%20demonstrate%20the%20sufficiency%20of%20visual%20route-based%20decision-making%20in%20a%20classic%20open%20field%20navigation%20task%20often%20assumed%20to%20require%20a%20cognitive%20map.%20Three%20distinct%20strategies%20emerge%20to%20robustly%20navigate%20to%20a%20hidden%20goal%2C%20each%20conferring%20contextual%20tradeoffs%20analyzed%20at%20both%20neural%20and%20behavioral%20scales%2C%20as%20well%20as%20qualitatively%20aligning%20with%20behavior%20observed%20across%20the%20biological%20spectrum.%20We%20propose%20reframing%20navigation%20from%20the%20bottom-up%2C%20through%20an%20egocentric%20episodic%20perspective%20without%20assuming%20online%20access%20to%20computationally%20expensive%20top-down%20representations%2C%20to%20better%20explain%20behavior%20under%20energetic%20or%20attentional%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2407.13535v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisuospatial%2520navigation%2520from%2520the%2520bottom-up%253A%2520without%2520vestibular%2520integration%252C%2520distance%2520prediction%252C%2520or%2520maps%26entry.906535625%3DPatrick%2520Govoni%2520and%2520Pawel%2520Romanczuk%26entry.1292438233%3DNavigation%2520is%2520believed%2520to%2520be%2520controlled%2520by%2520at%2520least%2520two%2520partially%2520dissociable%2520systems%2520in%2520the%2520brain.%2520The%2520cognitive%2520map%2520informs%2520an%2520organism%2520of%2520its%2520location%2520and%2520bearing%252C%2520updated%2520by%2520integrating%2520vestibular%2520self-motion%2520or%2520predicting%2520distances%2520to%2520landmarks.%2520Route-based%2520navigation%252C%2520on%2520the%2520other%2520hand%252C%2520directly%2520evaluate%2520sequential%2520movement%2520decisions%2520from%2520immediate%2520percepts.%2520Here%2520we%2520demonstrate%2520the%2520sufficiency%2520of%2520visual%2520route-based%2520decision-making%2520in%2520a%2520classic%2520open%2520field%2520navigation%2520task%2520often%2520assumed%2520to%2520require%2520a%2520cognitive%2520map.%2520Three%2520distinct%2520strategies%2520emerge%2520to%2520robustly%2520navigate%2520to%2520a%2520hidden%2520goal%252C%2520each%2520conferring%2520contextual%2520tradeoffs%2520analyzed%2520at%2520both%2520neural%2520and%2520behavioral%2520scales%252C%2520as%2520well%2520as%2520qualitatively%2520aligning%2520with%2520behavior%2520observed%2520across%2520the%2520biological%2520spectrum.%2520We%2520propose%2520reframing%2520navigation%2520from%2520the%2520bottom-up%252C%2520through%2520an%2520egocentric%2520episodic%2520perspective%2520without%2520assuming%2520online%2520access%2520to%2520computationally%2520expensive%2520top-down%2520representations%252C%2520to%2520better%2520explain%2520behavior%2520under%2520energetic%2520or%2520attentional%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13535v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visuospatial%20navigation%20from%20the%20bottom-up%3A%20without%20vestibular%20integration%2C%20distance%20prediction%2C%20or%20maps&entry.906535625=Patrick%20Govoni%20and%20Pawel%20Romanczuk&entry.1292438233=Navigation%20is%20believed%20to%20be%20controlled%20by%20at%20least%20two%20partially%20dissociable%20systems%20in%20the%20brain.%20The%20cognitive%20map%20informs%20an%20organism%20of%20its%20location%20and%20bearing%2C%20updated%20by%20integrating%20vestibular%20self-motion%20or%20predicting%20distances%20to%20landmarks.%20Route-based%20navigation%2C%20on%20the%20other%20hand%2C%20directly%20evaluate%20sequential%20movement%20decisions%20from%20immediate%20percepts.%20Here%20we%20demonstrate%20the%20sufficiency%20of%20visual%20route-based%20decision-making%20in%20a%20classic%20open%20field%20navigation%20task%20often%20assumed%20to%20require%20a%20cognitive%20map.%20Three%20distinct%20strategies%20emerge%20to%20robustly%20navigate%20to%20a%20hidden%20goal%2C%20each%20conferring%20contextual%20tradeoffs%20analyzed%20at%20both%20neural%20and%20behavioral%20scales%2C%20as%20well%20as%20qualitatively%20aligning%20with%20behavior%20observed%20across%20the%20biological%20spectrum.%20We%20propose%20reframing%20navigation%20from%20the%20bottom-up%2C%20through%20an%20egocentric%20episodic%20perspective%20without%20assuming%20online%20access%20to%20computationally%20expensive%20top-down%20representations%2C%20to%20better%20explain%20behavior%20under%20energetic%20or%20attentional%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2407.13535v4&entry.124074799=Read"},
{"title": "OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design", "author": "Ian Dunn and Liv Toft and Tyler Katz and Juhi Gupta and Riya Shah and Ramith Hettiarachchi and David R. Koes", "abstract": "Structure-based drug design (SBDD) focuses on designing small-molecule ligands that bind to specific protein pockets. Computational methods are integral in modern SBDD workflows and often make use of virtual screening methods via docking or pharmacophore search. Modern generative modeling approaches have focused on improving novel ligand discovery by enabling de novo design. In this work, we recognize that these tasks share a common structure and can therefore be represented as different instantiations of a consistent generative modeling framework. We propose a unified approach in OMTRA, a multi-modal flow matching model that flexibly performs many tasks relevant to SBDD, including some with no analogue in conventional workflows. Additionally, we curate a dataset of 500M 3D molecular conformers, complementing protein-ligand data and expanding the chemical diversity available for training. OMTRA obtains state of the art performance on pocket-conditioned de novo design and docking; however, the effects of large-scale pretraining and multi-task training are modest. All code, trained models, and dataset for reproducing this work are available at https://github.com/gnina/OMTRA", "link": "http://arxiv.org/abs/2512.05080v1", "date": "2025-12-04", "relevancy": 2.1621, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5414}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMTRA%3A%20A%20Multi-Task%20Generative%20Model%20for%20Structure-Based%20Drug%20Design&body=Title%3A%20OMTRA%3A%20A%20Multi-Task%20Generative%20Model%20for%20Structure-Based%20Drug%20Design%0AAuthor%3A%20Ian%20Dunn%20and%20Liv%20Toft%20and%20Tyler%20Katz%20and%20Juhi%20Gupta%20and%20Riya%20Shah%20and%20Ramith%20Hettiarachchi%20and%20David%20R.%20Koes%0AAbstract%3A%20Structure-based%20drug%20design%20%28SBDD%29%20focuses%20on%20designing%20small-molecule%20ligands%20that%20bind%20to%20specific%20protein%20pockets.%20Computational%20methods%20are%20integral%20in%20modern%20SBDD%20workflows%20and%20often%20make%20use%20of%20virtual%20screening%20methods%20via%20docking%20or%20pharmacophore%20search.%20Modern%20generative%20modeling%20approaches%20have%20focused%20on%20improving%20novel%20ligand%20discovery%20by%20enabling%20de%20novo%20design.%20In%20this%20work%2C%20we%20recognize%20that%20these%20tasks%20share%20a%20common%20structure%20and%20can%20therefore%20be%20represented%20as%20different%20instantiations%20of%20a%20consistent%20generative%20modeling%20framework.%20We%20propose%20a%20unified%20approach%20in%20OMTRA%2C%20a%20multi-modal%20flow%20matching%20model%20that%20flexibly%20performs%20many%20tasks%20relevant%20to%20SBDD%2C%20including%20some%20with%20no%20analogue%20in%20conventional%20workflows.%20Additionally%2C%20we%20curate%20a%20dataset%20of%20500M%203D%20molecular%20conformers%2C%20complementing%20protein-ligand%20data%20and%20expanding%20the%20chemical%20diversity%20available%20for%20training.%20OMTRA%20obtains%20state%20of%20the%20art%20performance%20on%20pocket-conditioned%20de%20novo%20design%20and%20docking%3B%20however%2C%20the%20effects%20of%20large-scale%20pretraining%20and%20multi-task%20training%20are%20modest.%20All%20code%2C%20trained%20models%2C%20and%20dataset%20for%20reproducing%20this%20work%20are%20available%20at%20https%3A//github.com/gnina/OMTRA%0ALink%3A%20http%3A//arxiv.org/abs/2512.05080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMTRA%253A%2520A%2520Multi-Task%2520Generative%2520Model%2520for%2520Structure-Based%2520Drug%2520Design%26entry.906535625%3DIan%2520Dunn%2520and%2520Liv%2520Toft%2520and%2520Tyler%2520Katz%2520and%2520Juhi%2520Gupta%2520and%2520Riya%2520Shah%2520and%2520Ramith%2520Hettiarachchi%2520and%2520David%2520R.%2520Koes%26entry.1292438233%3DStructure-based%2520drug%2520design%2520%2528SBDD%2529%2520focuses%2520on%2520designing%2520small-molecule%2520ligands%2520that%2520bind%2520to%2520specific%2520protein%2520pockets.%2520Computational%2520methods%2520are%2520integral%2520in%2520modern%2520SBDD%2520workflows%2520and%2520often%2520make%2520use%2520of%2520virtual%2520screening%2520methods%2520via%2520docking%2520or%2520pharmacophore%2520search.%2520Modern%2520generative%2520modeling%2520approaches%2520have%2520focused%2520on%2520improving%2520novel%2520ligand%2520discovery%2520by%2520enabling%2520de%2520novo%2520design.%2520In%2520this%2520work%252C%2520we%2520recognize%2520that%2520these%2520tasks%2520share%2520a%2520common%2520structure%2520and%2520can%2520therefore%2520be%2520represented%2520as%2520different%2520instantiations%2520of%2520a%2520consistent%2520generative%2520modeling%2520framework.%2520We%2520propose%2520a%2520unified%2520approach%2520in%2520OMTRA%252C%2520a%2520multi-modal%2520flow%2520matching%2520model%2520that%2520flexibly%2520performs%2520many%2520tasks%2520relevant%2520to%2520SBDD%252C%2520including%2520some%2520with%2520no%2520analogue%2520in%2520conventional%2520workflows.%2520Additionally%252C%2520we%2520curate%2520a%2520dataset%2520of%2520500M%25203D%2520molecular%2520conformers%252C%2520complementing%2520protein-ligand%2520data%2520and%2520expanding%2520the%2520chemical%2520diversity%2520available%2520for%2520training.%2520OMTRA%2520obtains%2520state%2520of%2520the%2520art%2520performance%2520on%2520pocket-conditioned%2520de%2520novo%2520design%2520and%2520docking%253B%2520however%252C%2520the%2520effects%2520of%2520large-scale%2520pretraining%2520and%2520multi-task%2520training%2520are%2520modest.%2520All%2520code%252C%2520trained%2520models%252C%2520and%2520dataset%2520for%2520reproducing%2520this%2520work%2520are%2520available%2520at%2520https%253A//github.com/gnina/OMTRA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMTRA%3A%20A%20Multi-Task%20Generative%20Model%20for%20Structure-Based%20Drug%20Design&entry.906535625=Ian%20Dunn%20and%20Liv%20Toft%20and%20Tyler%20Katz%20and%20Juhi%20Gupta%20and%20Riya%20Shah%20and%20Ramith%20Hettiarachchi%20and%20David%20R.%20Koes&entry.1292438233=Structure-based%20drug%20design%20%28SBDD%29%20focuses%20on%20designing%20small-molecule%20ligands%20that%20bind%20to%20specific%20protein%20pockets.%20Computational%20methods%20are%20integral%20in%20modern%20SBDD%20workflows%20and%20often%20make%20use%20of%20virtual%20screening%20methods%20via%20docking%20or%20pharmacophore%20search.%20Modern%20generative%20modeling%20approaches%20have%20focused%20on%20improving%20novel%20ligand%20discovery%20by%20enabling%20de%20novo%20design.%20In%20this%20work%2C%20we%20recognize%20that%20these%20tasks%20share%20a%20common%20structure%20and%20can%20therefore%20be%20represented%20as%20different%20instantiations%20of%20a%20consistent%20generative%20modeling%20framework.%20We%20propose%20a%20unified%20approach%20in%20OMTRA%2C%20a%20multi-modal%20flow%20matching%20model%20that%20flexibly%20performs%20many%20tasks%20relevant%20to%20SBDD%2C%20including%20some%20with%20no%20analogue%20in%20conventional%20workflows.%20Additionally%2C%20we%20curate%20a%20dataset%20of%20500M%203D%20molecular%20conformers%2C%20complementing%20protein-ligand%20data%20and%20expanding%20the%20chemical%20diversity%20available%20for%20training.%20OMTRA%20obtains%20state%20of%20the%20art%20performance%20on%20pocket-conditioned%20de%20novo%20design%20and%20docking%3B%20however%2C%20the%20effects%20of%20large-scale%20pretraining%20and%20multi-task%20training%20are%20modest.%20All%20code%2C%20trained%20models%2C%20and%20dataset%20for%20reproducing%20this%20work%20are%20available%20at%20https%3A//github.com/gnina/OMTRA&entry.1838667208=http%3A//arxiv.org/abs/2512.05080v1&entry.124074799=Read"},
{"title": "Shorting Dynamics and Structured Kernel Regularization", "author": "James Tian", "abstract": "This paper develops a nonlinear operator dynamic that progressively removes the influence of a prescribed feature subspace while retaining maximal structure elsewhere. The induced sequence of positive operators is monotone, admits an exact residual decomposition, and converges to the classical shorted operator. Transporting this dynamic to reproducing kernel Hilbert spaces yields a corresponding family of kernels that converges to the largest kernel dominated by the original one and annihilating the given subspace. In the finite-sample setting, the associated Gram operators inherit a structured residual decomposition that leads to a canonical form of kernel ridge regression and a principled way to enforce nuisance invariance. This gives a unified operator-analytic approach to invariant kernel construction and structured regularization in data analysis.", "link": "http://arxiv.org/abs/2512.04874v1", "date": "2025-12-04", "relevancy": 2.1607, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4434}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4348}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shorting%20Dynamics%20and%20Structured%20Kernel%20Regularization&body=Title%3A%20Shorting%20Dynamics%20and%20Structured%20Kernel%20Regularization%0AAuthor%3A%20James%20Tian%0AAbstract%3A%20This%20paper%20develops%20a%20nonlinear%20operator%20dynamic%20that%20progressively%20removes%20the%20influence%20of%20a%20prescribed%20feature%20subspace%20while%20retaining%20maximal%20structure%20elsewhere.%20The%20induced%20sequence%20of%20positive%20operators%20is%20monotone%2C%20admits%20an%20exact%20residual%20decomposition%2C%20and%20converges%20to%20the%20classical%20shorted%20operator.%20Transporting%20this%20dynamic%20to%20reproducing%20kernel%20Hilbert%20spaces%20yields%20a%20corresponding%20family%20of%20kernels%20that%20converges%20to%20the%20largest%20kernel%20dominated%20by%20the%20original%20one%20and%20annihilating%20the%20given%20subspace.%20In%20the%20finite-sample%20setting%2C%20the%20associated%20Gram%20operators%20inherit%20a%20structured%20residual%20decomposition%20that%20leads%20to%20a%20canonical%20form%20of%20kernel%20ridge%20regression%20and%20a%20principled%20way%20to%20enforce%20nuisance%20invariance.%20This%20gives%20a%20unified%20operator-analytic%20approach%20to%20invariant%20kernel%20construction%20and%20structured%20regularization%20in%20data%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShorting%2520Dynamics%2520and%2520Structured%2520Kernel%2520Regularization%26entry.906535625%3DJames%2520Tian%26entry.1292438233%3DThis%2520paper%2520develops%2520a%2520nonlinear%2520operator%2520dynamic%2520that%2520progressively%2520removes%2520the%2520influence%2520of%2520a%2520prescribed%2520feature%2520subspace%2520while%2520retaining%2520maximal%2520structure%2520elsewhere.%2520The%2520induced%2520sequence%2520of%2520positive%2520operators%2520is%2520monotone%252C%2520admits%2520an%2520exact%2520residual%2520decomposition%252C%2520and%2520converges%2520to%2520the%2520classical%2520shorted%2520operator.%2520Transporting%2520this%2520dynamic%2520to%2520reproducing%2520kernel%2520Hilbert%2520spaces%2520yields%2520a%2520corresponding%2520family%2520of%2520kernels%2520that%2520converges%2520to%2520the%2520largest%2520kernel%2520dominated%2520by%2520the%2520original%2520one%2520and%2520annihilating%2520the%2520given%2520subspace.%2520In%2520the%2520finite-sample%2520setting%252C%2520the%2520associated%2520Gram%2520operators%2520inherit%2520a%2520structured%2520residual%2520decomposition%2520that%2520leads%2520to%2520a%2520canonical%2520form%2520of%2520kernel%2520ridge%2520regression%2520and%2520a%2520principled%2520way%2520to%2520enforce%2520nuisance%2520invariance.%2520This%2520gives%2520a%2520unified%2520operator-analytic%2520approach%2520to%2520invariant%2520kernel%2520construction%2520and%2520structured%2520regularization%2520in%2520data%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shorting%20Dynamics%20and%20Structured%20Kernel%20Regularization&entry.906535625=James%20Tian&entry.1292438233=This%20paper%20develops%20a%20nonlinear%20operator%20dynamic%20that%20progressively%20removes%20the%20influence%20of%20a%20prescribed%20feature%20subspace%20while%20retaining%20maximal%20structure%20elsewhere.%20The%20induced%20sequence%20of%20positive%20operators%20is%20monotone%2C%20admits%20an%20exact%20residual%20decomposition%2C%20and%20converges%20to%20the%20classical%20shorted%20operator.%20Transporting%20this%20dynamic%20to%20reproducing%20kernel%20Hilbert%20spaces%20yields%20a%20corresponding%20family%20of%20kernels%20that%20converges%20to%20the%20largest%20kernel%20dominated%20by%20the%20original%20one%20and%20annihilating%20the%20given%20subspace.%20In%20the%20finite-sample%20setting%2C%20the%20associated%20Gram%20operators%20inherit%20a%20structured%20residual%20decomposition%20that%20leads%20to%20a%20canonical%20form%20of%20kernel%20ridge%20regression%20and%20a%20principled%20way%20to%20enforce%20nuisance%20invariance.%20This%20gives%20a%20unified%20operator-analytic%20approach%20to%20invariant%20kernel%20construction%20and%20structured%20regularization%20in%20data%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2512.04874v1&entry.124074799=Read"},
{"title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models", "author": "NaHyeon Park and Namin An and Kunhee Kim and Soyeon Yoon and Jiahao Huo and Hyunjung Shim", "abstract": "Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.", "link": "http://arxiv.org/abs/2512.04981v1", "date": "2025-12-04", "relevancy": 2.1517, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligned%20but%20Stereotypical%3F%20The%20Hidden%20Influence%20of%20System%20Prompts%20on%20Social%20Bias%20in%20LVLM-Based%20Text-to-Image%20Models&body=Title%3A%20Aligned%20but%20Stereotypical%3F%20The%20Hidden%20Influence%20of%20System%20Prompts%20on%20Social%20Bias%20in%20LVLM-Based%20Text-to-Image%20Models%0AAuthor%3A%20NaHyeon%20Park%20and%20Namin%20An%20and%20Kunhee%20Kim%20and%20Soyeon%20Yoon%20and%20Jiahao%20Huo%20and%20Hyunjung%20Shim%0AAbstract%3A%20Large%20vision-language%20model%20%28LVLM%29%20based%20text-to-image%20%28T2I%29%20systems%20have%20become%20the%20dominant%20paradigm%20in%20image%20generation%2C%20yet%20whether%20they%20amplify%20social%20biases%20remains%20insufficiently%20understood.%20In%20this%20paper%2C%20we%20show%20that%20LVLM-based%20models%20produce%20markedly%20more%20socially%20biased%20images%20than%20non-LVLM-based%20models.%20We%20introduce%20a%201%2C024%20prompt%20benchmark%20spanning%20four%20levels%20of%20linguistic%20complexity%20and%20evaluate%20demographic%20bias%20across%20multiple%20attributes%20in%20a%20systematic%20manner.%20Our%20analysis%20identifies%20system%20prompts%2C%20the%20predefined%20instructions%20guiding%20LVLMs%2C%20as%20a%20primary%20driver%20of%20biased%20behavior.%20Through%20decoded%20intermediate%20representations%2C%20token-probability%20diagnostics%2C%20and%20embedding-association%20analyses%2C%20we%20reveal%20how%20system%20prompts%20encode%20demographic%20priors%20that%20propagate%20into%20image%20synthesis.%20To%20this%20end%2C%20we%20propose%20FairPro%2C%20a%20training-free%20meta-prompting%20framework%20that%20enables%20LVLMs%20to%20self-audit%20and%20construct%20fairness-aware%20system%20prompts%20at%20test%20time.%20Experiments%20on%20two%20LVLM-based%20T2I%20models%2C%20SANA%20and%20Qwen-Image%2C%20show%20that%20FairPro%20substantially%20reduces%20demographic%20bias%20while%20preserving%20text-image%20alignment.%20We%20believe%20our%20findings%20provide%20deeper%20insight%20into%20the%20central%20role%20of%20system%20prompts%20in%20bias%20propagation%20and%20offer%20a%20practical%2C%20deployable%20approach%20for%20building%20more%20socially%20responsible%20T2I%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligned%2520but%2520Stereotypical%253F%2520The%2520Hidden%2520Influence%2520of%2520System%2520Prompts%2520on%2520Social%2520Bias%2520in%2520LVLM-Based%2520Text-to-Image%2520Models%26entry.906535625%3DNaHyeon%2520Park%2520and%2520Namin%2520An%2520and%2520Kunhee%2520Kim%2520and%2520Soyeon%2520Yoon%2520and%2520Jiahao%2520Huo%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3DLarge%2520vision-language%2520model%2520%2528LVLM%2529%2520based%2520text-to-image%2520%2528T2I%2529%2520systems%2520have%2520become%2520the%2520dominant%2520paradigm%2520in%2520image%2520generation%252C%2520yet%2520whether%2520they%2520amplify%2520social%2520biases%2520remains%2520insufficiently%2520understood.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520LVLM-based%2520models%2520produce%2520markedly%2520more%2520socially%2520biased%2520images%2520than%2520non-LVLM-based%2520models.%2520We%2520introduce%2520a%25201%252C024%2520prompt%2520benchmark%2520spanning%2520four%2520levels%2520of%2520linguistic%2520complexity%2520and%2520evaluate%2520demographic%2520bias%2520across%2520multiple%2520attributes%2520in%2520a%2520systematic%2520manner.%2520Our%2520analysis%2520identifies%2520system%2520prompts%252C%2520the%2520predefined%2520instructions%2520guiding%2520LVLMs%252C%2520as%2520a%2520primary%2520driver%2520of%2520biased%2520behavior.%2520Through%2520decoded%2520intermediate%2520representations%252C%2520token-probability%2520diagnostics%252C%2520and%2520embedding-association%2520analyses%252C%2520we%2520reveal%2520how%2520system%2520prompts%2520encode%2520demographic%2520priors%2520that%2520propagate%2520into%2520image%2520synthesis.%2520To%2520this%2520end%252C%2520we%2520propose%2520FairPro%252C%2520a%2520training-free%2520meta-prompting%2520framework%2520that%2520enables%2520LVLMs%2520to%2520self-audit%2520and%2520construct%2520fairness-aware%2520system%2520prompts%2520at%2520test%2520time.%2520Experiments%2520on%2520two%2520LVLM-based%2520T2I%2520models%252C%2520SANA%2520and%2520Qwen-Image%252C%2520show%2520that%2520FairPro%2520substantially%2520reduces%2520demographic%2520bias%2520while%2520preserving%2520text-image%2520alignment.%2520We%2520believe%2520our%2520findings%2520provide%2520deeper%2520insight%2520into%2520the%2520central%2520role%2520of%2520system%2520prompts%2520in%2520bias%2520propagation%2520and%2520offer%2520a%2520practical%252C%2520deployable%2520approach%2520for%2520building%2520more%2520socially%2520responsible%2520T2I%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligned%20but%20Stereotypical%3F%20The%20Hidden%20Influence%20of%20System%20Prompts%20on%20Social%20Bias%20in%20LVLM-Based%20Text-to-Image%20Models&entry.906535625=NaHyeon%20Park%20and%20Namin%20An%20and%20Kunhee%20Kim%20and%20Soyeon%20Yoon%20and%20Jiahao%20Huo%20and%20Hyunjung%20Shim&entry.1292438233=Large%20vision-language%20model%20%28LVLM%29%20based%20text-to-image%20%28T2I%29%20systems%20have%20become%20the%20dominant%20paradigm%20in%20image%20generation%2C%20yet%20whether%20they%20amplify%20social%20biases%20remains%20insufficiently%20understood.%20In%20this%20paper%2C%20we%20show%20that%20LVLM-based%20models%20produce%20markedly%20more%20socially%20biased%20images%20than%20non-LVLM-based%20models.%20We%20introduce%20a%201%2C024%20prompt%20benchmark%20spanning%20four%20levels%20of%20linguistic%20complexity%20and%20evaluate%20demographic%20bias%20across%20multiple%20attributes%20in%20a%20systematic%20manner.%20Our%20analysis%20identifies%20system%20prompts%2C%20the%20predefined%20instructions%20guiding%20LVLMs%2C%20as%20a%20primary%20driver%20of%20biased%20behavior.%20Through%20decoded%20intermediate%20representations%2C%20token-probability%20diagnostics%2C%20and%20embedding-association%20analyses%2C%20we%20reveal%20how%20system%20prompts%20encode%20demographic%20priors%20that%20propagate%20into%20image%20synthesis.%20To%20this%20end%2C%20we%20propose%20FairPro%2C%20a%20training-free%20meta-prompting%20framework%20that%20enables%20LVLMs%20to%20self-audit%20and%20construct%20fairness-aware%20system%20prompts%20at%20test%20time.%20Experiments%20on%20two%20LVLM-based%20T2I%20models%2C%20SANA%20and%20Qwen-Image%2C%20show%20that%20FairPro%20substantially%20reduces%20demographic%20bias%20while%20preserving%20text-image%20alignment.%20We%20believe%20our%20findings%20provide%20deeper%20insight%20into%20the%20central%20role%20of%20system%20prompts%20in%20bias%20propagation%20and%20offer%20a%20practical%2C%20deployable%20approach%20for%20building%20more%20socially%20responsible%20T2I%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.04981v1&entry.124074799=Read"},
{"title": "Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding", "author": "Abhigyan Bhattacharya and Hiranmoy Roy", "abstract": "Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration. Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors. In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis. Our approach starts with a method that organizes and synthesizes information based on meaning, followed by refining the texture. This process gives clear insights into the facial structure before we move on to creating detailed images. In the first stage, we blend two techniques: one that focuses on local features with CNNs and global features with Vision Transformers. This helped us create clear and detailed semantic layouts. In the second stage, we use a Multi-Modal Texture Generator to refine these layouts by pulling in information from different scales, ensuring everything looks cohesive and consistent. The architecture naturally handles arbitrary mask configurations through dynamic attention without maskspecific training. Experiment on two datasets CelebA-HQ and FFHQ shows that our model outperforms other state-of-the-art methods, showing improvements in metrics like LPIPS, PSNR, and SSIM. It produces visually striking results with better semantic preservation, in challenging large-area inpainting situations.", "link": "http://arxiv.org/abs/2512.05039v1", "date": "2025-12-04", "relevancy": 2.1492, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5908}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5277}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-Guided%20Two-Stage%20GAN%20for%20Face%20Inpainting%20with%20Hybrid%20Perceptual%20Encoding&body=Title%3A%20Semantic-Guided%20Two-Stage%20GAN%20for%20Face%20Inpainting%20with%20Hybrid%20Perceptual%20Encoding%0AAuthor%3A%20Abhigyan%20Bhattacharya%20and%20Hiranmoy%20Roy%0AAbstract%3A%20Facial%20Image%20inpainting%20aim%20is%20to%20restore%20the%20missing%20or%20corrupted%20regions%20in%20face%20images%20while%20preserving%20identity%2C%20structural%20consistency%20and%20photorealistic%20image%20quality%2C%20a%20task%20specifically%20created%20for%20photo%20restoration.%20Though%20there%20are%20recent%20lot%20of%20advances%20in%20deep%20generative%20models%2C%20existing%20methods%20face%20problems%20with%20large%20irregular%20masks%2C%20often%20producing%20blurry%20textures%20on%20the%20edges%20of%20the%20masked%20region%2C%20semantic%20inconsistencies%2C%20or%20unconvincing%20facial%20structures%20due%20to%20direct%20pixel%20level%20synthesis%20approach%20and%20limited%20exploitation%20of%20facial%20priors.%20In%20this%20paper%20we%20propose%20a%20novel%20architecture%2C%20which%20address%20these%20above%20challenges%20through%20semantic-guided%20hierarchical%20synthesis.%20Our%20approach%20starts%20with%20a%20method%20that%20organizes%20and%20synthesizes%20information%20based%20on%20meaning%2C%20followed%20by%20refining%20the%20texture.%20This%20process%20gives%20clear%20insights%20into%20the%20facial%20structure%20before%20we%20move%20on%20to%20creating%20detailed%20images.%20In%20the%20first%20stage%2C%20we%20blend%20two%20techniques%3A%20one%20that%20focuses%20on%20local%20features%20with%20CNNs%20and%20global%20features%20with%20Vision%20Transformers.%20This%20helped%20us%20create%20clear%20and%20detailed%20semantic%20layouts.%20In%20the%20second%20stage%2C%20we%20use%20a%20Multi-Modal%20Texture%20Generator%20to%20refine%20these%20layouts%20by%20pulling%20in%20information%20from%20different%20scales%2C%20ensuring%20everything%20looks%20cohesive%20and%20consistent.%20The%20architecture%20naturally%20handles%20arbitrary%20mask%20configurations%20through%20dynamic%20attention%20without%20maskspecific%20training.%20Experiment%20on%20two%20datasets%20CelebA-HQ%20and%20FFHQ%20shows%20that%20our%20model%20outperforms%20other%20state-of-the-art%20methods%2C%20showing%20improvements%20in%20metrics%20like%20LPIPS%2C%20PSNR%2C%20and%20SSIM.%20It%20produces%20visually%20striking%20results%20with%20better%20semantic%20preservation%2C%20in%20challenging%20large-area%20inpainting%20situations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-Guided%2520Two-Stage%2520GAN%2520for%2520Face%2520Inpainting%2520with%2520Hybrid%2520Perceptual%2520Encoding%26entry.906535625%3DAbhigyan%2520Bhattacharya%2520and%2520Hiranmoy%2520Roy%26entry.1292438233%3DFacial%2520Image%2520inpainting%2520aim%2520is%2520to%2520restore%2520the%2520missing%2520or%2520corrupted%2520regions%2520in%2520face%2520images%2520while%2520preserving%2520identity%252C%2520structural%2520consistency%2520and%2520photorealistic%2520image%2520quality%252C%2520a%2520task%2520specifically%2520created%2520for%2520photo%2520restoration.%2520Though%2520there%2520are%2520recent%2520lot%2520of%2520advances%2520in%2520deep%2520generative%2520models%252C%2520existing%2520methods%2520face%2520problems%2520with%2520large%2520irregular%2520masks%252C%2520often%2520producing%2520blurry%2520textures%2520on%2520the%2520edges%2520of%2520the%2520masked%2520region%252C%2520semantic%2520inconsistencies%252C%2520or%2520unconvincing%2520facial%2520structures%2520due%2520to%2520direct%2520pixel%2520level%2520synthesis%2520approach%2520and%2520limited%2520exploitation%2520of%2520facial%2520priors.%2520In%2520this%2520paper%2520we%2520propose%2520a%2520novel%2520architecture%252C%2520which%2520address%2520these%2520above%2520challenges%2520through%2520semantic-guided%2520hierarchical%2520synthesis.%2520Our%2520approach%2520starts%2520with%2520a%2520method%2520that%2520organizes%2520and%2520synthesizes%2520information%2520based%2520on%2520meaning%252C%2520followed%2520by%2520refining%2520the%2520texture.%2520This%2520process%2520gives%2520clear%2520insights%2520into%2520the%2520facial%2520structure%2520before%2520we%2520move%2520on%2520to%2520creating%2520detailed%2520images.%2520In%2520the%2520first%2520stage%252C%2520we%2520blend%2520two%2520techniques%253A%2520one%2520that%2520focuses%2520on%2520local%2520features%2520with%2520CNNs%2520and%2520global%2520features%2520with%2520Vision%2520Transformers.%2520This%2520helped%2520us%2520create%2520clear%2520and%2520detailed%2520semantic%2520layouts.%2520In%2520the%2520second%2520stage%252C%2520we%2520use%2520a%2520Multi-Modal%2520Texture%2520Generator%2520to%2520refine%2520these%2520layouts%2520by%2520pulling%2520in%2520information%2520from%2520different%2520scales%252C%2520ensuring%2520everything%2520looks%2520cohesive%2520and%2520consistent.%2520The%2520architecture%2520naturally%2520handles%2520arbitrary%2520mask%2520configurations%2520through%2520dynamic%2520attention%2520without%2520maskspecific%2520training.%2520Experiment%2520on%2520two%2520datasets%2520CelebA-HQ%2520and%2520FFHQ%2520shows%2520that%2520our%2520model%2520outperforms%2520other%2520state-of-the-art%2520methods%252C%2520showing%2520improvements%2520in%2520metrics%2520like%2520LPIPS%252C%2520PSNR%252C%2520and%2520SSIM.%2520It%2520produces%2520visually%2520striking%2520results%2520with%2520better%2520semantic%2520preservation%252C%2520in%2520challenging%2520large-area%2520inpainting%2520situations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-Guided%20Two-Stage%20GAN%20for%20Face%20Inpainting%20with%20Hybrid%20Perceptual%20Encoding&entry.906535625=Abhigyan%20Bhattacharya%20and%20Hiranmoy%20Roy&entry.1292438233=Facial%20Image%20inpainting%20aim%20is%20to%20restore%20the%20missing%20or%20corrupted%20regions%20in%20face%20images%20while%20preserving%20identity%2C%20structural%20consistency%20and%20photorealistic%20image%20quality%2C%20a%20task%20specifically%20created%20for%20photo%20restoration.%20Though%20there%20are%20recent%20lot%20of%20advances%20in%20deep%20generative%20models%2C%20existing%20methods%20face%20problems%20with%20large%20irregular%20masks%2C%20often%20producing%20blurry%20textures%20on%20the%20edges%20of%20the%20masked%20region%2C%20semantic%20inconsistencies%2C%20or%20unconvincing%20facial%20structures%20due%20to%20direct%20pixel%20level%20synthesis%20approach%20and%20limited%20exploitation%20of%20facial%20priors.%20In%20this%20paper%20we%20propose%20a%20novel%20architecture%2C%20which%20address%20these%20above%20challenges%20through%20semantic-guided%20hierarchical%20synthesis.%20Our%20approach%20starts%20with%20a%20method%20that%20organizes%20and%20synthesizes%20information%20based%20on%20meaning%2C%20followed%20by%20refining%20the%20texture.%20This%20process%20gives%20clear%20insights%20into%20the%20facial%20structure%20before%20we%20move%20on%20to%20creating%20detailed%20images.%20In%20the%20first%20stage%2C%20we%20blend%20two%20techniques%3A%20one%20that%20focuses%20on%20local%20features%20with%20CNNs%20and%20global%20features%20with%20Vision%20Transformers.%20This%20helped%20us%20create%20clear%20and%20detailed%20semantic%20layouts.%20In%20the%20second%20stage%2C%20we%20use%20a%20Multi-Modal%20Texture%20Generator%20to%20refine%20these%20layouts%20by%20pulling%20in%20information%20from%20different%20scales%2C%20ensuring%20everything%20looks%20cohesive%20and%20consistent.%20The%20architecture%20naturally%20handles%20arbitrary%20mask%20configurations%20through%20dynamic%20attention%20without%20maskspecific%20training.%20Experiment%20on%20two%20datasets%20CelebA-HQ%20and%20FFHQ%20shows%20that%20our%20model%20outperforms%20other%20state-of-the-art%20methods%2C%20showing%20improvements%20in%20metrics%20like%20LPIPS%2C%20PSNR%2C%20and%20SSIM.%20It%20produces%20visually%20striking%20results%20with%20better%20semantic%20preservation%2C%20in%20challenging%20large-area%20inpainting%20situations.&entry.1838667208=http%3A//arxiv.org/abs/2512.05039v1&entry.124074799=Read"},
{"title": "Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition", "author": "Novanto Yudistira", "abstract": "This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.", "link": "http://arxiv.org/abs/2512.04943v1", "date": "2025-12-04", "relevancy": 2.1483, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5515}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5305}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Adaptive%20Fusion%20of%20Multimodal%20Deep%20Networks%20for%20Human%20Action%20Recognition&body=Title%3A%20Towards%20Adaptive%20Fusion%20of%20Multimodal%20Deep%20Networks%20for%20Human%20Action%20Recognition%0AAuthor%3A%20Novanto%20Yudistira%0AAbstract%3A%20This%20study%20introduces%20a%20pioneering%20methodology%20for%20human%20action%20recognition%20by%20harnessing%20deep%20neural%20network%20techniques%20and%20adaptive%20fusion%20strategies%20across%20multiple%20modalities%2C%20including%20RGB%2C%20optical%20flows%2C%20audio%2C%20and%20depth%20information.%20Employing%20gating%20mechanisms%20for%20multimodal%20fusion%2C%20we%20aim%20to%20surpass%20limitations%20inherent%20in%20traditional%20unimodal%20recognition%20methods%20while%20exploring%20novel%20possibilities%20for%20diverse%20applications.%20Through%20an%20exhaustive%20investigation%20of%20gating%20mechanisms%20and%20adaptive%20weighting-based%20fusion%20architectures%2C%20our%20methodology%20enables%20the%20selective%20integration%20of%20relevant%20information%20from%20various%20modalities%2C%20thereby%20bolstering%20both%20accuracy%20and%20robustness%20in%20action%20recognition%20tasks.%20We%20meticulously%20examine%20various%20gated%20fusion%20strategies%20to%20pinpoint%20the%20most%20effective%20approach%20for%20multimodal%20action%20recognition%2C%20showcasing%20its%20superiority%20over%20conventional%20unimodal%20methods.%20Gating%20mechanisms%20facilitate%20the%20extraction%20of%20pivotal%20features%2C%20resulting%20in%20a%20more%20holistic%20representation%20of%20actions%20and%20substantial%20enhancements%20in%20recognition%20performance.%20Our%20evaluations%20across%20human%20action%20recognition%2C%20violence%20action%20detection%2C%20and%20multiple%20self-supervised%20learning%20tasks%20on%20benchmark%20datasets%20demonstrate%20promising%20advancements%20in%20accuracy.%20The%20significance%20of%20this%20research%20lies%20in%20its%20potential%20to%20revolutionize%20action%20recognition%20systems%20across%20diverse%20fields.%20The%20fusion%20of%20multimodal%20information%20promises%20sophisticated%20applications%20in%20surveillance%20and%20human-computer%20interaction%2C%20especially%20in%20contexts%20related%20to%20active%20assisted%20living.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Adaptive%2520Fusion%2520of%2520Multimodal%2520Deep%2520Networks%2520for%2520Human%2520Action%2520Recognition%26entry.906535625%3DNovanto%2520Yudistira%26entry.1292438233%3DThis%2520study%2520introduces%2520a%2520pioneering%2520methodology%2520for%2520human%2520action%2520recognition%2520by%2520harnessing%2520deep%2520neural%2520network%2520techniques%2520and%2520adaptive%2520fusion%2520strategies%2520across%2520multiple%2520modalities%252C%2520including%2520RGB%252C%2520optical%2520flows%252C%2520audio%252C%2520and%2520depth%2520information.%2520Employing%2520gating%2520mechanisms%2520for%2520multimodal%2520fusion%252C%2520we%2520aim%2520to%2520surpass%2520limitations%2520inherent%2520in%2520traditional%2520unimodal%2520recognition%2520methods%2520while%2520exploring%2520novel%2520possibilities%2520for%2520diverse%2520applications.%2520Through%2520an%2520exhaustive%2520investigation%2520of%2520gating%2520mechanisms%2520and%2520adaptive%2520weighting-based%2520fusion%2520architectures%252C%2520our%2520methodology%2520enables%2520the%2520selective%2520integration%2520of%2520relevant%2520information%2520from%2520various%2520modalities%252C%2520thereby%2520bolstering%2520both%2520accuracy%2520and%2520robustness%2520in%2520action%2520recognition%2520tasks.%2520We%2520meticulously%2520examine%2520various%2520gated%2520fusion%2520strategies%2520to%2520pinpoint%2520the%2520most%2520effective%2520approach%2520for%2520multimodal%2520action%2520recognition%252C%2520showcasing%2520its%2520superiority%2520over%2520conventional%2520unimodal%2520methods.%2520Gating%2520mechanisms%2520facilitate%2520the%2520extraction%2520of%2520pivotal%2520features%252C%2520resulting%2520in%2520a%2520more%2520holistic%2520representation%2520of%2520actions%2520and%2520substantial%2520enhancements%2520in%2520recognition%2520performance.%2520Our%2520evaluations%2520across%2520human%2520action%2520recognition%252C%2520violence%2520action%2520detection%252C%2520and%2520multiple%2520self-supervised%2520learning%2520tasks%2520on%2520benchmark%2520datasets%2520demonstrate%2520promising%2520advancements%2520in%2520accuracy.%2520The%2520significance%2520of%2520this%2520research%2520lies%2520in%2520its%2520potential%2520to%2520revolutionize%2520action%2520recognition%2520systems%2520across%2520diverse%2520fields.%2520The%2520fusion%2520of%2520multimodal%2520information%2520promises%2520sophisticated%2520applications%2520in%2520surveillance%2520and%2520human-computer%2520interaction%252C%2520especially%2520in%2520contexts%2520related%2520to%2520active%2520assisted%2520living.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Adaptive%20Fusion%20of%20Multimodal%20Deep%20Networks%20for%20Human%20Action%20Recognition&entry.906535625=Novanto%20Yudistira&entry.1292438233=This%20study%20introduces%20a%20pioneering%20methodology%20for%20human%20action%20recognition%20by%20harnessing%20deep%20neural%20network%20techniques%20and%20adaptive%20fusion%20strategies%20across%20multiple%20modalities%2C%20including%20RGB%2C%20optical%20flows%2C%20audio%2C%20and%20depth%20information.%20Employing%20gating%20mechanisms%20for%20multimodal%20fusion%2C%20we%20aim%20to%20surpass%20limitations%20inherent%20in%20traditional%20unimodal%20recognition%20methods%20while%20exploring%20novel%20possibilities%20for%20diverse%20applications.%20Through%20an%20exhaustive%20investigation%20of%20gating%20mechanisms%20and%20adaptive%20weighting-based%20fusion%20architectures%2C%20our%20methodology%20enables%20the%20selective%20integration%20of%20relevant%20information%20from%20various%20modalities%2C%20thereby%20bolstering%20both%20accuracy%20and%20robustness%20in%20action%20recognition%20tasks.%20We%20meticulously%20examine%20various%20gated%20fusion%20strategies%20to%20pinpoint%20the%20most%20effective%20approach%20for%20multimodal%20action%20recognition%2C%20showcasing%20its%20superiority%20over%20conventional%20unimodal%20methods.%20Gating%20mechanisms%20facilitate%20the%20extraction%20of%20pivotal%20features%2C%20resulting%20in%20a%20more%20holistic%20representation%20of%20actions%20and%20substantial%20enhancements%20in%20recognition%20performance.%20Our%20evaluations%20across%20human%20action%20recognition%2C%20violence%20action%20detection%2C%20and%20multiple%20self-supervised%20learning%20tasks%20on%20benchmark%20datasets%20demonstrate%20promising%20advancements%20in%20accuracy.%20The%20significance%20of%20this%20research%20lies%20in%20its%20potential%20to%20revolutionize%20action%20recognition%20systems%20across%20diverse%20fields.%20The%20fusion%20of%20multimodal%20information%20promises%20sophisticated%20applications%20in%20surveillance%20and%20human-computer%20interaction%2C%20especially%20in%20contexts%20related%20to%20active%20assisted%20living.&entry.1838667208=http%3A//arxiv.org/abs/2512.04943v1&entry.124074799=Read"},
{"title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers", "author": "Daniyar Zakarin and Thiemo Wandel and Anton Obukhov and Dengxin Dai", "abstract": "We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web", "link": "http://arxiv.org/abs/2512.05000v1", "date": "2025-12-04", "relevancy": 1.7808, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6199}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5908}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reflection%20Removal%20through%20Efficient%20Adaptation%20of%20Diffusion%20Transformers&body=Title%3A%20Reflection%20Removal%20through%20Efficient%20Adaptation%20of%20Diffusion%20Transformers%0AAuthor%3A%20Daniyar%20Zakarin%20and%20Thiemo%20Wandel%20and%20Anton%20Obukhov%20and%20Dengxin%20Dai%0AAbstract%3A%20We%20introduce%20a%20diffusion-transformer%20%28DiT%29%20framework%20for%20single-image%20reflection%20removal%20that%20leverages%20the%20generalization%20strengths%20of%20foundation%20diffusion%20models%20in%20the%20restoration%20setting.%20Rather%20than%20relying%20on%20task-specific%20architectures%2C%20we%20repurpose%20a%20pre-trained%20DiT-based%20foundation%20model%20by%20conditioning%20it%20on%20reflection-contaminated%20inputs%20and%20guiding%20it%20toward%20clean%20transmission%20layers.%20We%20systematically%20analyze%20existing%20reflection%20removal%20data%20sources%20for%20diversity%2C%20scalability%2C%20and%20photorealism.%20To%20address%20the%20shortage%20of%20suitable%20data%2C%20we%20construct%20a%20physically%20based%20rendering%20%28PBR%29%20pipeline%20in%20Blender%2C%20built%20around%20the%20Principled%20BSDF%2C%20to%20synthesize%20realistic%20glass%20materials%20and%20reflection%20effects.%20Efficient%20LoRA-based%20adaptation%20of%20the%20foundation%20model%2C%20combined%20with%20the%20proposed%20synthetic%20data%2C%20achieves%20state-of-the-art%20performance%20on%20in-domain%20and%20zero-shot%20benchmarks.%20These%20results%20demonstrate%20that%20pretrained%20diffusion%20transformers%2C%20when%20paired%20with%20physically%20grounded%20data%20synthesis%20and%20efficient%20adaptation%2C%20offer%20a%20scalable%20and%20high-fidelity%20solution%20for%20reflection%20removal.%20Project%20page%3A%20https%3A//hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web%0ALink%3A%20http%3A//arxiv.org/abs/2512.05000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflection%2520Removal%2520through%2520Efficient%2520Adaptation%2520of%2520Diffusion%2520Transformers%26entry.906535625%3DDaniyar%2520Zakarin%2520and%2520Thiemo%2520Wandel%2520and%2520Anton%2520Obukhov%2520and%2520Dengxin%2520Dai%26entry.1292438233%3DWe%2520introduce%2520a%2520diffusion-transformer%2520%2528DiT%2529%2520framework%2520for%2520single-image%2520reflection%2520removal%2520that%2520leverages%2520the%2520generalization%2520strengths%2520of%2520foundation%2520diffusion%2520models%2520in%2520the%2520restoration%2520setting.%2520Rather%2520than%2520relying%2520on%2520task-specific%2520architectures%252C%2520we%2520repurpose%2520a%2520pre-trained%2520DiT-based%2520foundation%2520model%2520by%2520conditioning%2520it%2520on%2520reflection-contaminated%2520inputs%2520and%2520guiding%2520it%2520toward%2520clean%2520transmission%2520layers.%2520We%2520systematically%2520analyze%2520existing%2520reflection%2520removal%2520data%2520sources%2520for%2520diversity%252C%2520scalability%252C%2520and%2520photorealism.%2520To%2520address%2520the%2520shortage%2520of%2520suitable%2520data%252C%2520we%2520construct%2520a%2520physically%2520based%2520rendering%2520%2528PBR%2529%2520pipeline%2520in%2520Blender%252C%2520built%2520around%2520the%2520Principled%2520BSDF%252C%2520to%2520synthesize%2520realistic%2520glass%2520materials%2520and%2520reflection%2520effects.%2520Efficient%2520LoRA-based%2520adaptation%2520of%2520the%2520foundation%2520model%252C%2520combined%2520with%2520the%2520proposed%2520synthetic%2520data%252C%2520achieves%2520state-of-the-art%2520performance%2520on%2520in-domain%2520and%2520zero-shot%2520benchmarks.%2520These%2520results%2520demonstrate%2520that%2520pretrained%2520diffusion%2520transformers%252C%2520when%2520paired%2520with%2520physically%2520grounded%2520data%2520synthesis%2520and%2520efficient%2520adaptation%252C%2520offer%2520a%2520scalable%2520and%2520high-fidelity%2520solution%2520for%2520reflection%2520removal.%2520Project%2520page%253A%2520https%253A//hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflection%20Removal%20through%20Efficient%20Adaptation%20of%20Diffusion%20Transformers&entry.906535625=Daniyar%20Zakarin%20and%20Thiemo%20Wandel%20and%20Anton%20Obukhov%20and%20Dengxin%20Dai&entry.1292438233=We%20introduce%20a%20diffusion-transformer%20%28DiT%29%20framework%20for%20single-image%20reflection%20removal%20that%20leverages%20the%20generalization%20strengths%20of%20foundation%20diffusion%20models%20in%20the%20restoration%20setting.%20Rather%20than%20relying%20on%20task-specific%20architectures%2C%20we%20repurpose%20a%20pre-trained%20DiT-based%20foundation%20model%20by%20conditioning%20it%20on%20reflection-contaminated%20inputs%20and%20guiding%20it%20toward%20clean%20transmission%20layers.%20We%20systematically%20analyze%20existing%20reflection%20removal%20data%20sources%20for%20diversity%2C%20scalability%2C%20and%20photorealism.%20To%20address%20the%20shortage%20of%20suitable%20data%2C%20we%20construct%20a%20physically%20based%20rendering%20%28PBR%29%20pipeline%20in%20Blender%2C%20built%20around%20the%20Principled%20BSDF%2C%20to%20synthesize%20realistic%20glass%20materials%20and%20reflection%20effects.%20Efficient%20LoRA-based%20adaptation%20of%20the%20foundation%20model%2C%20combined%20with%20the%20proposed%20synthetic%20data%2C%20achieves%20state-of-the-art%20performance%20on%20in-domain%20and%20zero-shot%20benchmarks.%20These%20results%20demonstrate%20that%20pretrained%20diffusion%20transformers%2C%20when%20paired%20with%20physically%20grounded%20data%20synthesis%20and%20efficient%20adaptation%2C%20offer%20a%20scalable%20and%20high-fidelity%20solution%20for%20reflection%20removal.%20Project%20page%3A%20https%3A//hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web&entry.1838667208=http%3A//arxiv.org/abs/2512.05000v1&entry.124074799=Read"},
{"title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation", "author": "Monishwaran Maheswaran and Rishabh Tiwari and Yuezhou Hu and Kerem Dilmen and Coleman Hooper and Haocheng Xi and Nicholas Lee and Mehrdad Farajtabar and Michael W. Mahoney and Kurt Keutzer and Amir Gholami", "abstract": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\\sim2\\times$ at matched accuracy.", "link": "http://arxiv.org/abs/2512.05033v1", "date": "2025-12-04", "relevancy": 1.9276, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5155}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arbitrage%3A%20Efficient%20Reasoning%20via%20Advantage-Aware%20Speculation&body=Title%3A%20Arbitrage%3A%20Efficient%20Reasoning%20via%20Advantage-Aware%20Speculation%0AAuthor%3A%20Monishwaran%20Maheswaran%20and%20Rishabh%20Tiwari%20and%20Yuezhou%20Hu%20and%20Kerem%20Dilmen%20and%20Coleman%20Hooper%20and%20Haocheng%20Xi%20and%20Nicholas%20Lee%20and%20Mehrdad%20Farajtabar%20and%20Michael%20W.%20Mahoney%20and%20Kurt%20Keutzer%20and%20Amir%20Gholami%0AAbstract%3A%20Modern%20Large%20Language%20Models%20achieve%20impressive%20reasoning%20capabilities%20with%20long%20Chain%20of%20Thoughts%2C%20but%20they%20incur%20substantial%20computational%20cost%20during%20inference%2C%20and%20this%20motivates%20techniques%20to%20improve%20the%20performance-cost%20ratio.%20Among%20these%20techniques%2C%20Speculative%20Decoding%20accelerates%20inference%20by%20employing%20a%20fast%20but%20inaccurate%20draft%20model%20to%20autoregressively%20propose%20tokens%2C%20which%20are%20then%20verified%20in%20parallel%20by%20a%20more%20capable%20target%20model.%20However%2C%20due%20to%20unnecessary%20rejections%20caused%20by%20token%20mismatches%20in%20semantically%20equivalent%20steps%2C%20traditional%20token-level%20Speculative%20Decoding%20struggles%20in%20reasoning%20tasks.%20Although%20recent%20works%20have%20shifted%20to%20step-level%20semantic%20verification%2C%20which%20improve%20efficiency%20by%20accepting%20or%20rejecting%20entire%20reasoning%20steps%2C%20existing%20step-level%20methods%20still%20regenerate%20many%20rejected%20steps%20with%20little%20improvement%2C%20wasting%20valuable%20target%20compute.%20To%20address%20this%20challenge%2C%20we%20propose%20Arbitrage%2C%20a%20novel%20step-level%20speculative%20generation%20framework%20that%20routes%20generation%20dynamically%20based%20on%20the%20relative%20advantage%20between%20draft%20and%20target%20models.%20Instead%20of%20applying%20a%20fixed%20acceptance%20threshold%2C%20Arbitrage%20uses%20a%20lightweight%20router%20trained%20to%20predict%20when%20the%20target%20model%20is%20likely%20to%20produce%20a%20meaningfully%20better%20step.%20This%20routing%20approximates%20an%20ideal%20Arbitrage%20Oracle%20that%20always%20chooses%20the%20higher-quality%20step%2C%20achieving%20near-optimal%20efficiency-accuracy%20trade-offs.%20Across%20multiple%20mathematical%20reasoning%20benchmarks%2C%20Arbitrage%20consistently%20surpasses%20prior%20step-level%20Speculative%20Decoding%20baselines%2C%20reducing%20inference%20latency%20by%20up%20to%20%24%5Csim2%5Ctimes%24%20at%20matched%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArbitrage%253A%2520Efficient%2520Reasoning%2520via%2520Advantage-Aware%2520Speculation%26entry.906535625%3DMonishwaran%2520Maheswaran%2520and%2520Rishabh%2520Tiwari%2520and%2520Yuezhou%2520Hu%2520and%2520Kerem%2520Dilmen%2520and%2520Coleman%2520Hooper%2520and%2520Haocheng%2520Xi%2520and%2520Nicholas%2520Lee%2520and%2520Mehrdad%2520Farajtabar%2520and%2520Michael%2520W.%2520Mahoney%2520and%2520Kurt%2520Keutzer%2520and%2520Amir%2520Gholami%26entry.1292438233%3DModern%2520Large%2520Language%2520Models%2520achieve%2520impressive%2520reasoning%2520capabilities%2520with%2520long%2520Chain%2520of%2520Thoughts%252C%2520but%2520they%2520incur%2520substantial%2520computational%2520cost%2520during%2520inference%252C%2520and%2520this%2520motivates%2520techniques%2520to%2520improve%2520the%2520performance-cost%2520ratio.%2520Among%2520these%2520techniques%252C%2520Speculative%2520Decoding%2520accelerates%2520inference%2520by%2520employing%2520a%2520fast%2520but%2520inaccurate%2520draft%2520model%2520to%2520autoregressively%2520propose%2520tokens%252C%2520which%2520are%2520then%2520verified%2520in%2520parallel%2520by%2520a%2520more%2520capable%2520target%2520model.%2520However%252C%2520due%2520to%2520unnecessary%2520rejections%2520caused%2520by%2520token%2520mismatches%2520in%2520semantically%2520equivalent%2520steps%252C%2520traditional%2520token-level%2520Speculative%2520Decoding%2520struggles%2520in%2520reasoning%2520tasks.%2520Although%2520recent%2520works%2520have%2520shifted%2520to%2520step-level%2520semantic%2520verification%252C%2520which%2520improve%2520efficiency%2520by%2520accepting%2520or%2520rejecting%2520entire%2520reasoning%2520steps%252C%2520existing%2520step-level%2520methods%2520still%2520regenerate%2520many%2520rejected%2520steps%2520with%2520little%2520improvement%252C%2520wasting%2520valuable%2520target%2520compute.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Arbitrage%252C%2520a%2520novel%2520step-level%2520speculative%2520generation%2520framework%2520that%2520routes%2520generation%2520dynamically%2520based%2520on%2520the%2520relative%2520advantage%2520between%2520draft%2520and%2520target%2520models.%2520Instead%2520of%2520applying%2520a%2520fixed%2520acceptance%2520threshold%252C%2520Arbitrage%2520uses%2520a%2520lightweight%2520router%2520trained%2520to%2520predict%2520when%2520the%2520target%2520model%2520is%2520likely%2520to%2520produce%2520a%2520meaningfully%2520better%2520step.%2520This%2520routing%2520approximates%2520an%2520ideal%2520Arbitrage%2520Oracle%2520that%2520always%2520chooses%2520the%2520higher-quality%2520step%252C%2520achieving%2520near-optimal%2520efficiency-accuracy%2520trade-offs.%2520Across%2520multiple%2520mathematical%2520reasoning%2520benchmarks%252C%2520Arbitrage%2520consistently%2520surpasses%2520prior%2520step-level%2520Speculative%2520Decoding%2520baselines%252C%2520reducing%2520inference%2520latency%2520by%2520up%2520to%2520%2524%255Csim2%255Ctimes%2524%2520at%2520matched%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arbitrage%3A%20Efficient%20Reasoning%20via%20Advantage-Aware%20Speculation&entry.906535625=Monishwaran%20Maheswaran%20and%20Rishabh%20Tiwari%20and%20Yuezhou%20Hu%20and%20Kerem%20Dilmen%20and%20Coleman%20Hooper%20and%20Haocheng%20Xi%20and%20Nicholas%20Lee%20and%20Mehrdad%20Farajtabar%20and%20Michael%20W.%20Mahoney%20and%20Kurt%20Keutzer%20and%20Amir%20Gholami&entry.1292438233=Modern%20Large%20Language%20Models%20achieve%20impressive%20reasoning%20capabilities%20with%20long%20Chain%20of%20Thoughts%2C%20but%20they%20incur%20substantial%20computational%20cost%20during%20inference%2C%20and%20this%20motivates%20techniques%20to%20improve%20the%20performance-cost%20ratio.%20Among%20these%20techniques%2C%20Speculative%20Decoding%20accelerates%20inference%20by%20employing%20a%20fast%20but%20inaccurate%20draft%20model%20to%20autoregressively%20propose%20tokens%2C%20which%20are%20then%20verified%20in%20parallel%20by%20a%20more%20capable%20target%20model.%20However%2C%20due%20to%20unnecessary%20rejections%20caused%20by%20token%20mismatches%20in%20semantically%20equivalent%20steps%2C%20traditional%20token-level%20Speculative%20Decoding%20struggles%20in%20reasoning%20tasks.%20Although%20recent%20works%20have%20shifted%20to%20step-level%20semantic%20verification%2C%20which%20improve%20efficiency%20by%20accepting%20or%20rejecting%20entire%20reasoning%20steps%2C%20existing%20step-level%20methods%20still%20regenerate%20many%20rejected%20steps%20with%20little%20improvement%2C%20wasting%20valuable%20target%20compute.%20To%20address%20this%20challenge%2C%20we%20propose%20Arbitrage%2C%20a%20novel%20step-level%20speculative%20generation%20framework%20that%20routes%20generation%20dynamically%20based%20on%20the%20relative%20advantage%20between%20draft%20and%20target%20models.%20Instead%20of%20applying%20a%20fixed%20acceptance%20threshold%2C%20Arbitrage%20uses%20a%20lightweight%20router%20trained%20to%20predict%20when%20the%20target%20model%20is%20likely%20to%20produce%20a%20meaningfully%20better%20step.%20This%20routing%20approximates%20an%20ideal%20Arbitrage%20Oracle%20that%20always%20chooses%20the%20higher-quality%20step%2C%20achieving%20near-optimal%20efficiency-accuracy%20trade-offs.%20Across%20multiple%20mathematical%20reasoning%20benchmarks%2C%20Arbitrage%20consistently%20surpasses%20prior%20step-level%20Speculative%20Decoding%20baselines%2C%20reducing%20inference%20latency%20by%20up%20to%20%24%5Csim2%5Ctimes%24%20at%20matched%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2512.05033v1&entry.124074799=Read"},
{"title": "SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection", "author": "Qing Xu and Yanqian Wang and Xiangjian Hea and Yue Li and Yixuan Zhang and Rong Qu and Wenting Duan and Zhen Chen", "abstract": "Automated lesion detection in chest X-rays has demonstrated significant potential for improving clinical diagnosis by precisely localizing pathological abnormalities. While recent promptable detection frameworks have achieved remarkable accuracy in target localization, existing methods typically rely on manual annotations as prompts, which are labor-intensive and impractical for clinical applications. To address this limitation, we propose SP-Det, a novel self-prompted detection framework that automatically generates rich textual context to guide multi-label lesion detection without requiring expert annotations. Specifically, we introduce an expert-free dual-text prompt generator (DTPG) that leverages two complementary textual modalities: semantic context prompts that capture global pathological patterns and disease beacon prompts that focus on disease-specific manifestations. Moreover, we devise a bidirectional feature enhancer (BFE) that synergistically integrates comprehensive diagnostic context with disease-specific embeddings to significantly improve feature representation and detection accuracy. Extensive experiments on two chest X-ray datasets with diverse thoracic disease categories demonstrate that our SP-Det framework outperforms state-of-the-art detection methods while completely eliminating the dependency on expert-annotated prompts compared to existing promptable architectures.", "link": "http://arxiv.org/abs/2512.04875v1", "date": "2025-12-04", "relevancy": 2.1309, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5414}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5289}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SP-Det%3A%20Self-Prompted%20Dual-Text%20Fusion%20for%20Generalized%20Multi-Label%20Lesion%20Detection&body=Title%3A%20SP-Det%3A%20Self-Prompted%20Dual-Text%20Fusion%20for%20Generalized%20Multi-Label%20Lesion%20Detection%0AAuthor%3A%20Qing%20Xu%20and%20Yanqian%20Wang%20and%20Xiangjian%20Hea%20and%20Yue%20Li%20and%20Yixuan%20Zhang%20and%20Rong%20Qu%20and%20Wenting%20Duan%20and%20Zhen%20Chen%0AAbstract%3A%20Automated%20lesion%20detection%20in%20chest%20X-rays%20has%20demonstrated%20significant%20potential%20for%20improving%20clinical%20diagnosis%20by%20precisely%20localizing%20pathological%20abnormalities.%20While%20recent%20promptable%20detection%20frameworks%20have%20achieved%20remarkable%20accuracy%20in%20target%20localization%2C%20existing%20methods%20typically%20rely%20on%20manual%20annotations%20as%20prompts%2C%20which%20are%20labor-intensive%20and%20impractical%20for%20clinical%20applications.%20To%20address%20this%20limitation%2C%20we%20propose%20SP-Det%2C%20a%20novel%20self-prompted%20detection%20framework%20that%20automatically%20generates%20rich%20textual%20context%20to%20guide%20multi-label%20lesion%20detection%20without%20requiring%20expert%20annotations.%20Specifically%2C%20we%20introduce%20an%20expert-free%20dual-text%20prompt%20generator%20%28DTPG%29%20that%20leverages%20two%20complementary%20textual%20modalities%3A%20semantic%20context%20prompts%20that%20capture%20global%20pathological%20patterns%20and%20disease%20beacon%20prompts%20that%20focus%20on%20disease-specific%20manifestations.%20Moreover%2C%20we%20devise%20a%20bidirectional%20feature%20enhancer%20%28BFE%29%20that%20synergistically%20integrates%20comprehensive%20diagnostic%20context%20with%20disease-specific%20embeddings%20to%20significantly%20improve%20feature%20representation%20and%20detection%20accuracy.%20Extensive%20experiments%20on%20two%20chest%20X-ray%20datasets%20with%20diverse%20thoracic%20disease%20categories%20demonstrate%20that%20our%20SP-Det%20framework%20outperforms%20state-of-the-art%20detection%20methods%20while%20completely%20eliminating%20the%20dependency%20on%20expert-annotated%20prompts%20compared%20to%20existing%20promptable%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSP-Det%253A%2520Self-Prompted%2520Dual-Text%2520Fusion%2520for%2520Generalized%2520Multi-Label%2520Lesion%2520Detection%26entry.906535625%3DQing%2520Xu%2520and%2520Yanqian%2520Wang%2520and%2520Xiangjian%2520Hea%2520and%2520Yue%2520Li%2520and%2520Yixuan%2520Zhang%2520and%2520Rong%2520Qu%2520and%2520Wenting%2520Duan%2520and%2520Zhen%2520Chen%26entry.1292438233%3DAutomated%2520lesion%2520detection%2520in%2520chest%2520X-rays%2520has%2520demonstrated%2520significant%2520potential%2520for%2520improving%2520clinical%2520diagnosis%2520by%2520precisely%2520localizing%2520pathological%2520abnormalities.%2520While%2520recent%2520promptable%2520detection%2520frameworks%2520have%2520achieved%2520remarkable%2520accuracy%2520in%2520target%2520localization%252C%2520existing%2520methods%2520typically%2520rely%2520on%2520manual%2520annotations%2520as%2520prompts%252C%2520which%2520are%2520labor-intensive%2520and%2520impractical%2520for%2520clinical%2520applications.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520SP-Det%252C%2520a%2520novel%2520self-prompted%2520detection%2520framework%2520that%2520automatically%2520generates%2520rich%2520textual%2520context%2520to%2520guide%2520multi-label%2520lesion%2520detection%2520without%2520requiring%2520expert%2520annotations.%2520Specifically%252C%2520we%2520introduce%2520an%2520expert-free%2520dual-text%2520prompt%2520generator%2520%2528DTPG%2529%2520that%2520leverages%2520two%2520complementary%2520textual%2520modalities%253A%2520semantic%2520context%2520prompts%2520that%2520capture%2520global%2520pathological%2520patterns%2520and%2520disease%2520beacon%2520prompts%2520that%2520focus%2520on%2520disease-specific%2520manifestations.%2520Moreover%252C%2520we%2520devise%2520a%2520bidirectional%2520feature%2520enhancer%2520%2528BFE%2529%2520that%2520synergistically%2520integrates%2520comprehensive%2520diagnostic%2520context%2520with%2520disease-specific%2520embeddings%2520to%2520significantly%2520improve%2520feature%2520representation%2520and%2520detection%2520accuracy.%2520Extensive%2520experiments%2520on%2520two%2520chest%2520X-ray%2520datasets%2520with%2520diverse%2520thoracic%2520disease%2520categories%2520demonstrate%2520that%2520our%2520SP-Det%2520framework%2520outperforms%2520state-of-the-art%2520detection%2520methods%2520while%2520completely%2520eliminating%2520the%2520dependency%2520on%2520expert-annotated%2520prompts%2520compared%2520to%2520existing%2520promptable%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SP-Det%3A%20Self-Prompted%20Dual-Text%20Fusion%20for%20Generalized%20Multi-Label%20Lesion%20Detection&entry.906535625=Qing%20Xu%20and%20Yanqian%20Wang%20and%20Xiangjian%20Hea%20and%20Yue%20Li%20and%20Yixuan%20Zhang%20and%20Rong%20Qu%20and%20Wenting%20Duan%20and%20Zhen%20Chen&entry.1292438233=Automated%20lesion%20detection%20in%20chest%20X-rays%20has%20demonstrated%20significant%20potential%20for%20improving%20clinical%20diagnosis%20by%20precisely%20localizing%20pathological%20abnormalities.%20While%20recent%20promptable%20detection%20frameworks%20have%20achieved%20remarkable%20accuracy%20in%20target%20localization%2C%20existing%20methods%20typically%20rely%20on%20manual%20annotations%20as%20prompts%2C%20which%20are%20labor-intensive%20and%20impractical%20for%20clinical%20applications.%20To%20address%20this%20limitation%2C%20we%20propose%20SP-Det%2C%20a%20novel%20self-prompted%20detection%20framework%20that%20automatically%20generates%20rich%20textual%20context%20to%20guide%20multi-label%20lesion%20detection%20without%20requiring%20expert%20annotations.%20Specifically%2C%20we%20introduce%20an%20expert-free%20dual-text%20prompt%20generator%20%28DTPG%29%20that%20leverages%20two%20complementary%20textual%20modalities%3A%20semantic%20context%20prompts%20that%20capture%20global%20pathological%20patterns%20and%20disease%20beacon%20prompts%20that%20focus%20on%20disease-specific%20manifestations.%20Moreover%2C%20we%20devise%20a%20bidirectional%20feature%20enhancer%20%28BFE%29%20that%20synergistically%20integrates%20comprehensive%20diagnostic%20context%20with%20disease-specific%20embeddings%20to%20significantly%20improve%20feature%20representation%20and%20detection%20accuracy.%20Extensive%20experiments%20on%20two%20chest%20X-ray%20datasets%20with%20diverse%20thoracic%20disease%20categories%20demonstrate%20that%20our%20SP-Det%20framework%20outperforms%20state-of-the-art%20detection%20methods%20while%20completely%20eliminating%20the%20dependency%20on%20expert-annotated%20prompts%20compared%20to%20existing%20promptable%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2512.04875v1&entry.124074799=Read"},
{"title": "SoK: Decentralized AI (DeAI)", "author": "Zhipeng Wang and Rui Sun and Elizabeth Lui and Vatsal Shah and Xihan Xiong and Jiahao Sun and Davide Crapis and William Knottenbelt", "abstract": "Centralization enhances the efficiency of Artificial Intelligence (AI) but also introduces critical challenges, including single points of failure, inherent biases, data privacy risks, and scalability limitations. To address these issues, blockchain-based Decentralized Artificial Intelligence (DeAI) has emerged as a promising paradigm that leverages decentralization and transparency to improve the trustworthiness of AI systems. Despite rapid adoption in industry, the academic community lacks a systematic analysis of DeAI's technical foundations, opportunities, and challenges. This work presents the first Systematization of Knowledge (SoK) on DeAI, offering a formal definition, a taxonomy of existing solutions based on the AI lifecycle, and an in-depth investigation of the roles of blockchain in enabling secure and incentive-compatible collaboration. We further review security risks across the DeAI lifecycle and empirically evaluate representative mitigation techniques. Finally, we highlight open research challenges and future directions for advancing blockchain-based DeAI.", "link": "http://arxiv.org/abs/2411.17461v4", "date": "2025-12-04", "relevancy": 2.0207, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4075}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoK%3A%20Decentralized%20AI%20%28DeAI%29&body=Title%3A%20SoK%3A%20Decentralized%20AI%20%28DeAI%29%0AAuthor%3A%20Zhipeng%20Wang%20and%20Rui%20Sun%20and%20Elizabeth%20Lui%20and%20Vatsal%20Shah%20and%20Xihan%20Xiong%20and%20Jiahao%20Sun%20and%20Davide%20Crapis%20and%20William%20Knottenbelt%0AAbstract%3A%20Centralization%20enhances%20the%20efficiency%20of%20Artificial%20Intelligence%20%28AI%29%20but%20also%20introduces%20critical%20challenges%2C%20including%20single%20points%20of%20failure%2C%20inherent%20biases%2C%20data%20privacy%20risks%2C%20and%20scalability%20limitations.%20To%20address%20these%20issues%2C%20blockchain-based%20Decentralized%20Artificial%20Intelligence%20%28DeAI%29%20has%20emerged%20as%20a%20promising%20paradigm%20that%20leverages%20decentralization%20and%20transparency%20to%20improve%20the%20trustworthiness%20of%20AI%20systems.%20Despite%20rapid%20adoption%20in%20industry%2C%20the%20academic%20community%20lacks%20a%20systematic%20analysis%20of%20DeAI%27s%20technical%20foundations%2C%20opportunities%2C%20and%20challenges.%20This%20work%20presents%20the%20first%20Systematization%20of%20Knowledge%20%28SoK%29%20on%20DeAI%2C%20offering%20a%20formal%20definition%2C%20a%20taxonomy%20of%20existing%20solutions%20based%20on%20the%20AI%20lifecycle%2C%20and%20an%20in-depth%20investigation%20of%20the%20roles%20of%20blockchain%20in%20enabling%20secure%20and%20incentive-compatible%20collaboration.%20We%20further%20review%20security%20risks%20across%20the%20DeAI%20lifecycle%20and%20empirically%20evaluate%20representative%20mitigation%20techniques.%20Finally%2C%20we%20highlight%20open%20research%20challenges%20and%20future%20directions%20for%20advancing%20blockchain-based%20DeAI.%0ALink%3A%20http%3A//arxiv.org/abs/2411.17461v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoK%253A%2520Decentralized%2520AI%2520%2528DeAI%2529%26entry.906535625%3DZhipeng%2520Wang%2520and%2520Rui%2520Sun%2520and%2520Elizabeth%2520Lui%2520and%2520Vatsal%2520Shah%2520and%2520Xihan%2520Xiong%2520and%2520Jiahao%2520Sun%2520and%2520Davide%2520Crapis%2520and%2520William%2520Knottenbelt%26entry.1292438233%3DCentralization%2520enhances%2520the%2520efficiency%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520but%2520also%2520introduces%2520critical%2520challenges%252C%2520including%2520single%2520points%2520of%2520failure%252C%2520inherent%2520biases%252C%2520data%2520privacy%2520risks%252C%2520and%2520scalability%2520limitations.%2520To%2520address%2520these%2520issues%252C%2520blockchain-based%2520Decentralized%2520Artificial%2520Intelligence%2520%2528DeAI%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520that%2520leverages%2520decentralization%2520and%2520transparency%2520to%2520improve%2520the%2520trustworthiness%2520of%2520AI%2520systems.%2520Despite%2520rapid%2520adoption%2520in%2520industry%252C%2520the%2520academic%2520community%2520lacks%2520a%2520systematic%2520analysis%2520of%2520DeAI%2527s%2520technical%2520foundations%252C%2520opportunities%252C%2520and%2520challenges.%2520This%2520work%2520presents%2520the%2520first%2520Systematization%2520of%2520Knowledge%2520%2528SoK%2529%2520on%2520DeAI%252C%2520offering%2520a%2520formal%2520definition%252C%2520a%2520taxonomy%2520of%2520existing%2520solutions%2520based%2520on%2520the%2520AI%2520lifecycle%252C%2520and%2520an%2520in-depth%2520investigation%2520of%2520the%2520roles%2520of%2520blockchain%2520in%2520enabling%2520secure%2520and%2520incentive-compatible%2520collaboration.%2520We%2520further%2520review%2520security%2520risks%2520across%2520the%2520DeAI%2520lifecycle%2520and%2520empirically%2520evaluate%2520representative%2520mitigation%2520techniques.%2520Finally%252C%2520we%2520highlight%2520open%2520research%2520challenges%2520and%2520future%2520directions%2520for%2520advancing%2520blockchain-based%2520DeAI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17461v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoK%3A%20Decentralized%20AI%20%28DeAI%29&entry.906535625=Zhipeng%20Wang%20and%20Rui%20Sun%20and%20Elizabeth%20Lui%20and%20Vatsal%20Shah%20and%20Xihan%20Xiong%20and%20Jiahao%20Sun%20and%20Davide%20Crapis%20and%20William%20Knottenbelt&entry.1292438233=Centralization%20enhances%20the%20efficiency%20of%20Artificial%20Intelligence%20%28AI%29%20but%20also%20introduces%20critical%20challenges%2C%20including%20single%20points%20of%20failure%2C%20inherent%20biases%2C%20data%20privacy%20risks%2C%20and%20scalability%20limitations.%20To%20address%20these%20issues%2C%20blockchain-based%20Decentralized%20Artificial%20Intelligence%20%28DeAI%29%20has%20emerged%20as%20a%20promising%20paradigm%20that%20leverages%20decentralization%20and%20transparency%20to%20improve%20the%20trustworthiness%20of%20AI%20systems.%20Despite%20rapid%20adoption%20in%20industry%2C%20the%20academic%20community%20lacks%20a%20systematic%20analysis%20of%20DeAI%27s%20technical%20foundations%2C%20opportunities%2C%20and%20challenges.%20This%20work%20presents%20the%20first%20Systematization%20of%20Knowledge%20%28SoK%29%20on%20DeAI%2C%20offering%20a%20formal%20definition%2C%20a%20taxonomy%20of%20existing%20solutions%20based%20on%20the%20AI%20lifecycle%2C%20and%20an%20in-depth%20investigation%20of%20the%20roles%20of%20blockchain%20in%20enabling%20secure%20and%20incentive-compatible%20collaboration.%20We%20further%20review%20security%20risks%20across%20the%20DeAI%20lifecycle%20and%20empirically%20evaluate%20representative%20mitigation%20techniques.%20Finally%2C%20we%20highlight%20open%20research%20challenges%20and%20future%20directions%20for%20advancing%20blockchain-based%20DeAI.&entry.1838667208=http%3A//arxiv.org/abs/2411.17461v4&entry.124074799=Read"},
{"title": "Dual-Path Region-Guided Attention Network for Ground Reaction Force and Moment Regression", "author": "Xuan Li and Samuel Bello", "abstract": "Accurate estimation of three-dimensional ground reaction forces and moments (GRFs/GRMs) is crucial for both biomechanics research and clinical rehabilitation evaluation. In this study, we focus on insole-based GRF/GRM estimation and further validate our approach on a public walking dataset. We propose a Dual-Path Region-Guided Attention Network that integrates anatomy-inspired spatial priors and temporal priors into a region-level attention mechanism, while a complementary path captures context from the full sensor field. The two paths are trained jointly and their outputs are combined to produce the final GRF/GRM predictions. Conclusions: Our model outperforms strong baseline models, including CNN and CNN-LSTM architectures on two datasets, achieving the lowest six-component average NRMSE of 5.78% on the insole dataset and 1.42% for the vertical ground reaction force on the public dataset. This demonstrates robust performance for ground reaction force and moment estimation.", "link": "http://arxiv.org/abs/2512.05030v1", "date": "2025-12-04", "relevancy": 2.1345, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5913}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5233}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Path%20Region-Guided%20Attention%20Network%20for%20Ground%20Reaction%20Force%20and%20Moment%20Regression&body=Title%3A%20Dual-Path%20Region-Guided%20Attention%20Network%20for%20Ground%20Reaction%20Force%20and%20Moment%20Regression%0AAuthor%3A%20Xuan%20Li%20and%20Samuel%20Bello%0AAbstract%3A%20Accurate%20estimation%20of%20three-dimensional%20ground%20reaction%20forces%20and%20moments%20%28GRFs/GRMs%29%20is%20crucial%20for%20both%20biomechanics%20research%20and%20clinical%20rehabilitation%20evaluation.%20In%20this%20study%2C%20we%20focus%20on%20insole-based%20GRF/GRM%20estimation%20and%20further%20validate%20our%20approach%20on%20a%20public%20walking%20dataset.%20We%20propose%20a%20Dual-Path%20Region-Guided%20Attention%20Network%20that%20integrates%20anatomy-inspired%20spatial%20priors%20and%20temporal%20priors%20into%20a%20region-level%20attention%20mechanism%2C%20while%20a%20complementary%20path%20captures%20context%20from%20the%20full%20sensor%20field.%20The%20two%20paths%20are%20trained%20jointly%20and%20their%20outputs%20are%20combined%20to%20produce%20the%20final%20GRF/GRM%20predictions.%20Conclusions%3A%20Our%20model%20outperforms%20strong%20baseline%20models%2C%20including%20CNN%20and%20CNN-LSTM%20architectures%20on%20two%20datasets%2C%20achieving%20the%20lowest%20six-component%20average%20NRMSE%20of%205.78%25%20on%20the%20insole%20dataset%20and%201.42%25%20for%20the%20vertical%20ground%20reaction%20force%20on%20the%20public%20dataset.%20This%20demonstrates%20robust%20performance%20for%20ground%20reaction%20force%20and%20moment%20estimation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Path%2520Region-Guided%2520Attention%2520Network%2520for%2520Ground%2520Reaction%2520Force%2520and%2520Moment%2520Regression%26entry.906535625%3DXuan%2520Li%2520and%2520Samuel%2520Bello%26entry.1292438233%3DAccurate%2520estimation%2520of%2520three-dimensional%2520ground%2520reaction%2520forces%2520and%2520moments%2520%2528GRFs/GRMs%2529%2520is%2520crucial%2520for%2520both%2520biomechanics%2520research%2520and%2520clinical%2520rehabilitation%2520evaluation.%2520In%2520this%2520study%252C%2520we%2520focus%2520on%2520insole-based%2520GRF/GRM%2520estimation%2520and%2520further%2520validate%2520our%2520approach%2520on%2520a%2520public%2520walking%2520dataset.%2520We%2520propose%2520a%2520Dual-Path%2520Region-Guided%2520Attention%2520Network%2520that%2520integrates%2520anatomy-inspired%2520spatial%2520priors%2520and%2520temporal%2520priors%2520into%2520a%2520region-level%2520attention%2520mechanism%252C%2520while%2520a%2520complementary%2520path%2520captures%2520context%2520from%2520the%2520full%2520sensor%2520field.%2520The%2520two%2520paths%2520are%2520trained%2520jointly%2520and%2520their%2520outputs%2520are%2520combined%2520to%2520produce%2520the%2520final%2520GRF/GRM%2520predictions.%2520Conclusions%253A%2520Our%2520model%2520outperforms%2520strong%2520baseline%2520models%252C%2520including%2520CNN%2520and%2520CNN-LSTM%2520architectures%2520on%2520two%2520datasets%252C%2520achieving%2520the%2520lowest%2520six-component%2520average%2520NRMSE%2520of%25205.78%2525%2520on%2520the%2520insole%2520dataset%2520and%25201.42%2525%2520for%2520the%2520vertical%2520ground%2520reaction%2520force%2520on%2520the%2520public%2520dataset.%2520This%2520demonstrates%2520robust%2520performance%2520for%2520ground%2520reaction%2520force%2520and%2520moment%2520estimation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Path%20Region-Guided%20Attention%20Network%20for%20Ground%20Reaction%20Force%20and%20Moment%20Regression&entry.906535625=Xuan%20Li%20and%20Samuel%20Bello&entry.1292438233=Accurate%20estimation%20of%20three-dimensional%20ground%20reaction%20forces%20and%20moments%20%28GRFs/GRMs%29%20is%20crucial%20for%20both%20biomechanics%20research%20and%20clinical%20rehabilitation%20evaluation.%20In%20this%20study%2C%20we%20focus%20on%20insole-based%20GRF/GRM%20estimation%20and%20further%20validate%20our%20approach%20on%20a%20public%20walking%20dataset.%20We%20propose%20a%20Dual-Path%20Region-Guided%20Attention%20Network%20that%20integrates%20anatomy-inspired%20spatial%20priors%20and%20temporal%20priors%20into%20a%20region-level%20attention%20mechanism%2C%20while%20a%20complementary%20path%20captures%20context%20from%20the%20full%20sensor%20field.%20The%20two%20paths%20are%20trained%20jointly%20and%20their%20outputs%20are%20combined%20to%20produce%20the%20final%20GRF/GRM%20predictions.%20Conclusions%3A%20Our%20model%20outperforms%20strong%20baseline%20models%2C%20including%20CNN%20and%20CNN-LSTM%20architectures%20on%20two%20datasets%2C%20achieving%20the%20lowest%20six-component%20average%20NRMSE%20of%205.78%25%20on%20the%20insole%20dataset%20and%201.42%25%20for%20the%20vertical%20ground%20reaction%20force%20on%20the%20public%20dataset.%20This%20demonstrates%20robust%20performance%20for%20ground%20reaction%20force%20and%20moment%20estimation.&entry.1838667208=http%3A//arxiv.org/abs/2512.05030v1&entry.124074799=Read"},
{"title": "Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists", "author": "Thomas F. Eisenmann and Andres Karjus and Mar Canet Sola and Levin Brinkmann and Bramantyo Ibrahim Supriyatno and Iyad Rahwan", "abstract": "Generative AI's novel capacities raise questions about the future role of human expertise: does AI level the playing field between professional artists and laypeople, or does expertise enhance AI use? Do the cognitive skills experts make use of in analyzing and drawing visual art also transfer to using these new tools? This pre-registered study conducts experimental comparisons between 50 professional artists and a demographically matched sample of laypeople. Our interdisciplinary team developed two tasks involving image replication and creative image creation, assessing their copying accuracy and divergent thinking. We implemented a bespoke platform for the experiment, powered by a modern text-to-image AI. Results reveal artists produced more accurate copies and more divergent ideas than lay participants, highlighting a skill transfer of professional expertise - even to the confined space of generative AI. We also explored how well an exemplary vision-capable large language model (GPT-4o) would fare: on par in copying and slightly better on average than artists in the creative task, although never above best humans. These findings highlight the importance of integrating artistic skills with AI, suggesting a potential for collaborative synergy that could reshape creative industries and arts education.", "link": "http://arxiv.org/abs/2501.12374v2", "date": "2025-12-04", "relevancy": 1.8852, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4727}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4715}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expertise%20elevates%20AI%20usage%3A%20experimental%20evidence%20comparing%20laypeople%20and%20professional%20artists&body=Title%3A%20Expertise%20elevates%20AI%20usage%3A%20experimental%20evidence%20comparing%20laypeople%20and%20professional%20artists%0AAuthor%3A%20Thomas%20F.%20Eisenmann%20and%20Andres%20Karjus%20and%20Mar%20Canet%20Sola%20and%20Levin%20Brinkmann%20and%20Bramantyo%20Ibrahim%20Supriyatno%20and%20Iyad%20Rahwan%0AAbstract%3A%20Generative%20AI%27s%20novel%20capacities%20raise%20questions%20about%20the%20future%20role%20of%20human%20expertise%3A%20does%20AI%20level%20the%20playing%20field%20between%20professional%20artists%20and%20laypeople%2C%20or%20does%20expertise%20enhance%20AI%20use%3F%20Do%20the%20cognitive%20skills%20experts%20make%20use%20of%20in%20analyzing%20and%20drawing%20visual%20art%20also%20transfer%20to%20using%20these%20new%20tools%3F%20This%20pre-registered%20study%20conducts%20experimental%20comparisons%20between%2050%20professional%20artists%20and%20a%20demographically%20matched%20sample%20of%20laypeople.%20Our%20interdisciplinary%20team%20developed%20two%20tasks%20involving%20image%20replication%20and%20creative%20image%20creation%2C%20assessing%20their%20copying%20accuracy%20and%20divergent%20thinking.%20We%20implemented%20a%20bespoke%20platform%20for%20the%20experiment%2C%20powered%20by%20a%20modern%20text-to-image%20AI.%20Results%20reveal%20artists%20produced%20more%20accurate%20copies%20and%20more%20divergent%20ideas%20than%20lay%20participants%2C%20highlighting%20a%20skill%20transfer%20of%20professional%20expertise%20-%20even%20to%20the%20confined%20space%20of%20generative%20AI.%20We%20also%20explored%20how%20well%20an%20exemplary%20vision-capable%20large%20language%20model%20%28GPT-4o%29%20would%20fare%3A%20on%20par%20in%20copying%20and%20slightly%20better%20on%20average%20than%20artists%20in%20the%20creative%20task%2C%20although%20never%20above%20best%20humans.%20These%20findings%20highlight%20the%20importance%20of%20integrating%20artistic%20skills%20with%20AI%2C%20suggesting%20a%20potential%20for%20collaborative%20synergy%20that%20could%20reshape%20creative%20industries%20and%20arts%20education.%0ALink%3A%20http%3A//arxiv.org/abs/2501.12374v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpertise%2520elevates%2520AI%2520usage%253A%2520experimental%2520evidence%2520comparing%2520laypeople%2520and%2520professional%2520artists%26entry.906535625%3DThomas%2520F.%2520Eisenmann%2520and%2520Andres%2520Karjus%2520and%2520Mar%2520Canet%2520Sola%2520and%2520Levin%2520Brinkmann%2520and%2520Bramantyo%2520Ibrahim%2520Supriyatno%2520and%2520Iyad%2520Rahwan%26entry.1292438233%3DGenerative%2520AI%2527s%2520novel%2520capacities%2520raise%2520questions%2520about%2520the%2520future%2520role%2520of%2520human%2520expertise%253A%2520does%2520AI%2520level%2520the%2520playing%2520field%2520between%2520professional%2520artists%2520and%2520laypeople%252C%2520or%2520does%2520expertise%2520enhance%2520AI%2520use%253F%2520Do%2520the%2520cognitive%2520skills%2520experts%2520make%2520use%2520of%2520in%2520analyzing%2520and%2520drawing%2520visual%2520art%2520also%2520transfer%2520to%2520using%2520these%2520new%2520tools%253F%2520This%2520pre-registered%2520study%2520conducts%2520experimental%2520comparisons%2520between%252050%2520professional%2520artists%2520and%2520a%2520demographically%2520matched%2520sample%2520of%2520laypeople.%2520Our%2520interdisciplinary%2520team%2520developed%2520two%2520tasks%2520involving%2520image%2520replication%2520and%2520creative%2520image%2520creation%252C%2520assessing%2520their%2520copying%2520accuracy%2520and%2520divergent%2520thinking.%2520We%2520implemented%2520a%2520bespoke%2520platform%2520for%2520the%2520experiment%252C%2520powered%2520by%2520a%2520modern%2520text-to-image%2520AI.%2520Results%2520reveal%2520artists%2520produced%2520more%2520accurate%2520copies%2520and%2520more%2520divergent%2520ideas%2520than%2520lay%2520participants%252C%2520highlighting%2520a%2520skill%2520transfer%2520of%2520professional%2520expertise%2520-%2520even%2520to%2520the%2520confined%2520space%2520of%2520generative%2520AI.%2520We%2520also%2520explored%2520how%2520well%2520an%2520exemplary%2520vision-capable%2520large%2520language%2520model%2520%2528GPT-4o%2529%2520would%2520fare%253A%2520on%2520par%2520in%2520copying%2520and%2520slightly%2520better%2520on%2520average%2520than%2520artists%2520in%2520the%2520creative%2520task%252C%2520although%2520never%2520above%2520best%2520humans.%2520These%2520findings%2520highlight%2520the%2520importance%2520of%2520integrating%2520artistic%2520skills%2520with%2520AI%252C%2520suggesting%2520a%2520potential%2520for%2520collaborative%2520synergy%2520that%2520could%2520reshape%2520creative%2520industries%2520and%2520arts%2520education.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12374v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expertise%20elevates%20AI%20usage%3A%20experimental%20evidence%20comparing%20laypeople%20and%20professional%20artists&entry.906535625=Thomas%20F.%20Eisenmann%20and%20Andres%20Karjus%20and%20Mar%20Canet%20Sola%20and%20Levin%20Brinkmann%20and%20Bramantyo%20Ibrahim%20Supriyatno%20and%20Iyad%20Rahwan&entry.1292438233=Generative%20AI%27s%20novel%20capacities%20raise%20questions%20about%20the%20future%20role%20of%20human%20expertise%3A%20does%20AI%20level%20the%20playing%20field%20between%20professional%20artists%20and%20laypeople%2C%20or%20does%20expertise%20enhance%20AI%20use%3F%20Do%20the%20cognitive%20skills%20experts%20make%20use%20of%20in%20analyzing%20and%20drawing%20visual%20art%20also%20transfer%20to%20using%20these%20new%20tools%3F%20This%20pre-registered%20study%20conducts%20experimental%20comparisons%20between%2050%20professional%20artists%20and%20a%20demographically%20matched%20sample%20of%20laypeople.%20Our%20interdisciplinary%20team%20developed%20two%20tasks%20involving%20image%20replication%20and%20creative%20image%20creation%2C%20assessing%20their%20copying%20accuracy%20and%20divergent%20thinking.%20We%20implemented%20a%20bespoke%20platform%20for%20the%20experiment%2C%20powered%20by%20a%20modern%20text-to-image%20AI.%20Results%20reveal%20artists%20produced%20more%20accurate%20copies%20and%20more%20divergent%20ideas%20than%20lay%20participants%2C%20highlighting%20a%20skill%20transfer%20of%20professional%20expertise%20-%20even%20to%20the%20confined%20space%20of%20generative%20AI.%20We%20also%20explored%20how%20well%20an%20exemplary%20vision-capable%20large%20language%20model%20%28GPT-4o%29%20would%20fare%3A%20on%20par%20in%20copying%20and%20slightly%20better%20on%20average%20than%20artists%20in%20the%20creative%20task%2C%20although%20never%20above%20best%20humans.%20These%20findings%20highlight%20the%20importance%20of%20integrating%20artistic%20skills%20with%20AI%2C%20suggesting%20a%20potential%20for%20collaborative%20synergy%20that%20could%20reshape%20creative%20industries%20and%20arts%20education.&entry.1838667208=http%3A//arxiv.org/abs/2501.12374v2&entry.124074799=Read"},
{"title": "On the Transfer of Knowledge in Quantum Algorithms", "author": "Esther Villar-Rodriguez and Eneko Osaba and Izaskun Oregi and Sebasti\u00e1n V. Romero and Juli\u00e1n Ferreiro-V\u00e9lez", "abstract": "Quantum computing is poised to transform computational paradigms across science and industry. As the field evolves, it can benefit from established classical methodologies, including promising paradigms such as Transfer of Knowledge (ToK). This work serves as a brief, self-contained reference for ToK, unifying its core principles under a single formal framework. We introduce a joint notation that consolidates and extends prior work in Transfer Learning and Transfer Optimization, bridging traditionally separate research lines and enabling a common language for knowledge reuse. Building on this foundation, we classify existing ToK strategies and principles into a structured taxonomy that helps researchers position their methods within a broader conceptual map. We then extend key transfer protocols to quantum computing, introducing two novel use cases--reverse annealing and multitasking Quantum Approximate Optimization Algorithm (QAOA)--alongside a sequential Variational Quantum Eigensolver (VQE) approach that supports and validates prior findings. These examples highlight ToK's potential to improve performance and generalization in quantum algorithms. Finally, we outline challenges and opportunities for integrating ToK into quantum computing, emphasizing its role in reducing resource demands and accelerating problem-solving. This work lays the groundwork for future synergies between classical and quantum computing through a shared, transferable knowledge framework.", "link": "http://arxiv.org/abs/2501.14120v3", "date": "2025-12-04", "relevancy": 1.6381, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4132}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4122}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Transfer%20of%20Knowledge%20in%20Quantum%20Algorithms&body=Title%3A%20On%20the%20Transfer%20of%20Knowledge%20in%20Quantum%20Algorithms%0AAuthor%3A%20Esther%20Villar-Rodriguez%20and%20Eneko%20Osaba%20and%20Izaskun%20Oregi%20and%20Sebasti%C3%A1n%20V.%20Romero%20and%20Juli%C3%A1n%20Ferreiro-V%C3%A9lez%0AAbstract%3A%20Quantum%20computing%20is%20poised%20to%20transform%20computational%20paradigms%20across%20science%20and%20industry.%20As%20the%20field%20evolves%2C%20it%20can%20benefit%20from%20established%20classical%20methodologies%2C%20including%20promising%20paradigms%20such%20as%20Transfer%20of%20Knowledge%20%28ToK%29.%20This%20work%20serves%20as%20a%20brief%2C%20self-contained%20reference%20for%20ToK%2C%20unifying%20its%20core%20principles%20under%20a%20single%20formal%20framework.%20We%20introduce%20a%20joint%20notation%20that%20consolidates%20and%20extends%20prior%20work%20in%20Transfer%20Learning%20and%20Transfer%20Optimization%2C%20bridging%20traditionally%20separate%20research%20lines%20and%20enabling%20a%20common%20language%20for%20knowledge%20reuse.%20Building%20on%20this%20foundation%2C%20we%20classify%20existing%20ToK%20strategies%20and%20principles%20into%20a%20structured%20taxonomy%20that%20helps%20researchers%20position%20their%20methods%20within%20a%20broader%20conceptual%20map.%20We%20then%20extend%20key%20transfer%20protocols%20to%20quantum%20computing%2C%20introducing%20two%20novel%20use%20cases--reverse%20annealing%20and%20multitasking%20Quantum%20Approximate%20Optimization%20Algorithm%20%28QAOA%29--alongside%20a%20sequential%20Variational%20Quantum%20Eigensolver%20%28VQE%29%20approach%20that%20supports%20and%20validates%20prior%20findings.%20These%20examples%20highlight%20ToK%27s%20potential%20to%20improve%20performance%20and%20generalization%20in%20quantum%20algorithms.%20Finally%2C%20we%20outline%20challenges%20and%20opportunities%20for%20integrating%20ToK%20into%20quantum%20computing%2C%20emphasizing%20its%20role%20in%20reducing%20resource%20demands%20and%20accelerating%20problem-solving.%20This%20work%20lays%20the%20groundwork%20for%20future%20synergies%20between%20classical%20and%20quantum%20computing%20through%20a%20shared%2C%20transferable%20knowledge%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2501.14120v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Transfer%2520of%2520Knowledge%2520in%2520Quantum%2520Algorithms%26entry.906535625%3DEsther%2520Villar-Rodriguez%2520and%2520Eneko%2520Osaba%2520and%2520Izaskun%2520Oregi%2520and%2520Sebasti%25C3%25A1n%2520V.%2520Romero%2520and%2520Juli%25C3%25A1n%2520Ferreiro-V%25C3%25A9lez%26entry.1292438233%3DQuantum%2520computing%2520is%2520poised%2520to%2520transform%2520computational%2520paradigms%2520across%2520science%2520and%2520industry.%2520As%2520the%2520field%2520evolves%252C%2520it%2520can%2520benefit%2520from%2520established%2520classical%2520methodologies%252C%2520including%2520promising%2520paradigms%2520such%2520as%2520Transfer%2520of%2520Knowledge%2520%2528ToK%2529.%2520This%2520work%2520serves%2520as%2520a%2520brief%252C%2520self-contained%2520reference%2520for%2520ToK%252C%2520unifying%2520its%2520core%2520principles%2520under%2520a%2520single%2520formal%2520framework.%2520We%2520introduce%2520a%2520joint%2520notation%2520that%2520consolidates%2520and%2520extends%2520prior%2520work%2520in%2520Transfer%2520Learning%2520and%2520Transfer%2520Optimization%252C%2520bridging%2520traditionally%2520separate%2520research%2520lines%2520and%2520enabling%2520a%2520common%2520language%2520for%2520knowledge%2520reuse.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520classify%2520existing%2520ToK%2520strategies%2520and%2520principles%2520into%2520a%2520structured%2520taxonomy%2520that%2520helps%2520researchers%2520position%2520their%2520methods%2520within%2520a%2520broader%2520conceptual%2520map.%2520We%2520then%2520extend%2520key%2520transfer%2520protocols%2520to%2520quantum%2520computing%252C%2520introducing%2520two%2520novel%2520use%2520cases--reverse%2520annealing%2520and%2520multitasking%2520Quantum%2520Approximate%2520Optimization%2520Algorithm%2520%2528QAOA%2529--alongside%2520a%2520sequential%2520Variational%2520Quantum%2520Eigensolver%2520%2528VQE%2529%2520approach%2520that%2520supports%2520and%2520validates%2520prior%2520findings.%2520These%2520examples%2520highlight%2520ToK%2527s%2520potential%2520to%2520improve%2520performance%2520and%2520generalization%2520in%2520quantum%2520algorithms.%2520Finally%252C%2520we%2520outline%2520challenges%2520and%2520opportunities%2520for%2520integrating%2520ToK%2520into%2520quantum%2520computing%252C%2520emphasizing%2520its%2520role%2520in%2520reducing%2520resource%2520demands%2520and%2520accelerating%2520problem-solving.%2520This%2520work%2520lays%2520the%2520groundwork%2520for%2520future%2520synergies%2520between%2520classical%2520and%2520quantum%2520computing%2520through%2520a%2520shared%252C%2520transferable%2520knowledge%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14120v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Transfer%20of%20Knowledge%20in%20Quantum%20Algorithms&entry.906535625=Esther%20Villar-Rodriguez%20and%20Eneko%20Osaba%20and%20Izaskun%20Oregi%20and%20Sebasti%C3%A1n%20V.%20Romero%20and%20Juli%C3%A1n%20Ferreiro-V%C3%A9lez&entry.1292438233=Quantum%20computing%20is%20poised%20to%20transform%20computational%20paradigms%20across%20science%20and%20industry.%20As%20the%20field%20evolves%2C%20it%20can%20benefit%20from%20established%20classical%20methodologies%2C%20including%20promising%20paradigms%20such%20as%20Transfer%20of%20Knowledge%20%28ToK%29.%20This%20work%20serves%20as%20a%20brief%2C%20self-contained%20reference%20for%20ToK%2C%20unifying%20its%20core%20principles%20under%20a%20single%20formal%20framework.%20We%20introduce%20a%20joint%20notation%20that%20consolidates%20and%20extends%20prior%20work%20in%20Transfer%20Learning%20and%20Transfer%20Optimization%2C%20bridging%20traditionally%20separate%20research%20lines%20and%20enabling%20a%20common%20language%20for%20knowledge%20reuse.%20Building%20on%20this%20foundation%2C%20we%20classify%20existing%20ToK%20strategies%20and%20principles%20into%20a%20structured%20taxonomy%20that%20helps%20researchers%20position%20their%20methods%20within%20a%20broader%20conceptual%20map.%20We%20then%20extend%20key%20transfer%20protocols%20to%20quantum%20computing%2C%20introducing%20two%20novel%20use%20cases--reverse%20annealing%20and%20multitasking%20Quantum%20Approximate%20Optimization%20Algorithm%20%28QAOA%29--alongside%20a%20sequential%20Variational%20Quantum%20Eigensolver%20%28VQE%29%20approach%20that%20supports%20and%20validates%20prior%20findings.%20These%20examples%20highlight%20ToK%27s%20potential%20to%20improve%20performance%20and%20generalization%20in%20quantum%20algorithms.%20Finally%2C%20we%20outline%20challenges%20and%20opportunities%20for%20integrating%20ToK%20into%20quantum%20computing%2C%20emphasizing%20its%20role%20in%20reducing%20resource%20demands%20and%20accelerating%20problem-solving.%20This%20work%20lays%20the%20groundwork%20for%20future%20synergies%20between%20classical%20and%20quantum%20computing%20through%20a%20shared%2C%20transferable%20knowledge%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2501.14120v3&entry.124074799=Read"},
{"title": "The Autonomy-Alignment Problem in Open-Ended Learning Robots: Formalising the Purpose Framework", "author": "Gianluca Baldassarre and Richard J. Duro and Emilio Cartoni and Mehdi Khamassi and Alejandro Romero and Vieri Giuliano Santucci", "abstract": "The rapid advancement of artificial intelligence is enabling the development of increasingly autonomous robots capable of operating beyond engineered factory settings and into the unstructured environments of human life. This shift raises a critical autonomy-alignment problem: how to ensure that a robot's autonomous learning focuses on acquiring knowledge and behaviours that serve human practical objectives while remaining aligned with broader human values (e.g., safety and ethics). This problem remains largely underexplored and lacks a unifying conceptual and formal framework. Here, we address one of its most challenging instances of the problem: open-ended learning (OEL) robots, which autonomously acquire new knowledge and skills through interaction with the environment, guided by intrinsic motivations and self-generated goals. We propose a computational framework, introduced qualitatively and then formalised, to guide the design of OEL architectures that balance autonomy with human control. At its core is the novel concept of purpose, which specifies what humans (designers or users) want the robot to learn, do, or avoid, independently of specific task domains. The framework decomposes the autonomy-alignment problem into four tractable sub-problems: the alignment of robot purposes (hardwired or learnt) with human purposes; the arbitration between multiple purposes; the grounding of abstract purposes into domain-specific goals; and the acquisition of competence to achieve those goals. The framework supports formal definitions of alignment across multiple cases and proofs of necessary and sufficient conditions under which alignment holds. Illustrative hypothetical scenarios showcase the applicability of the framework for guiding the development of purpose-aligned autonomous robots.", "link": "http://arxiv.org/abs/2403.02514v3", "date": "2025-12-04", "relevancy": 1.6374, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5625}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Autonomy-Alignment%20Problem%20in%20Open-Ended%20Learning%20Robots%3A%20Formalising%20the%20Purpose%20Framework&body=Title%3A%20The%20Autonomy-Alignment%20Problem%20in%20Open-Ended%20Learning%20Robots%3A%20Formalising%20the%20Purpose%20Framework%0AAuthor%3A%20Gianluca%20Baldassarre%20and%20Richard%20J.%20Duro%20and%20Emilio%20Cartoni%20and%20Mehdi%20Khamassi%20and%20Alejandro%20Romero%20and%20Vieri%20Giuliano%20Santucci%0AAbstract%3A%20The%20rapid%20advancement%20of%20artificial%20intelligence%20is%20enabling%20the%20development%20of%20increasingly%20autonomous%20robots%20capable%20of%20operating%20beyond%20engineered%20factory%20settings%20and%20into%20the%20unstructured%20environments%20of%20human%20life.%20This%20shift%20raises%20a%20critical%20autonomy-alignment%20problem%3A%20how%20to%20ensure%20that%20a%20robot%27s%20autonomous%20learning%20focuses%20on%20acquiring%20knowledge%20and%20behaviours%20that%20serve%20human%20practical%20objectives%20while%20remaining%20aligned%20with%20broader%20human%20values%20%28e.g.%2C%20safety%20and%20ethics%29.%20This%20problem%20remains%20largely%20underexplored%20and%20lacks%20a%20unifying%20conceptual%20and%20formal%20framework.%20Here%2C%20we%20address%20one%20of%20its%20most%20challenging%20instances%20of%20the%20problem%3A%20open-ended%20learning%20%28OEL%29%20robots%2C%20which%20autonomously%20acquire%20new%20knowledge%20and%20skills%20through%20interaction%20with%20the%20environment%2C%20guided%20by%20intrinsic%20motivations%20and%20self-generated%20goals.%20We%20propose%20a%20computational%20framework%2C%20introduced%20qualitatively%20and%20then%20formalised%2C%20to%20guide%20the%20design%20of%20OEL%20architectures%20that%20balance%20autonomy%20with%20human%20control.%20At%20its%20core%20is%20the%20novel%20concept%20of%20purpose%2C%20which%20specifies%20what%20humans%20%28designers%20or%20users%29%20want%20the%20robot%20to%20learn%2C%20do%2C%20or%20avoid%2C%20independently%20of%20specific%20task%20domains.%20The%20framework%20decomposes%20the%20autonomy-alignment%20problem%20into%20four%20tractable%20sub-problems%3A%20the%20alignment%20of%20robot%20purposes%20%28hardwired%20or%20learnt%29%20with%20human%20purposes%3B%20the%20arbitration%20between%20multiple%20purposes%3B%20the%20grounding%20of%20abstract%20purposes%20into%20domain-specific%20goals%3B%20and%20the%20acquisition%20of%20competence%20to%20achieve%20those%20goals.%20The%20framework%20supports%20formal%20definitions%20of%20alignment%20across%20multiple%20cases%20and%20proofs%20of%20necessary%20and%20sufficient%20conditions%20under%20which%20alignment%20holds.%20Illustrative%20hypothetical%20scenarios%20showcase%20the%20applicability%20of%20the%20framework%20for%20guiding%20the%20development%20of%20purpose-aligned%20autonomous%20robots.%0ALink%3A%20http%3A//arxiv.org/abs/2403.02514v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Autonomy-Alignment%2520Problem%2520in%2520Open-Ended%2520Learning%2520Robots%253A%2520Formalising%2520the%2520Purpose%2520Framework%26entry.906535625%3DGianluca%2520Baldassarre%2520and%2520Richard%2520J.%2520Duro%2520and%2520Emilio%2520Cartoni%2520and%2520Mehdi%2520Khamassi%2520and%2520Alejandro%2520Romero%2520and%2520Vieri%2520Giuliano%2520Santucci%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520artificial%2520intelligence%2520is%2520enabling%2520the%2520development%2520of%2520increasingly%2520autonomous%2520robots%2520capable%2520of%2520operating%2520beyond%2520engineered%2520factory%2520settings%2520and%2520into%2520the%2520unstructured%2520environments%2520of%2520human%2520life.%2520This%2520shift%2520raises%2520a%2520critical%2520autonomy-alignment%2520problem%253A%2520how%2520to%2520ensure%2520that%2520a%2520robot%2527s%2520autonomous%2520learning%2520focuses%2520on%2520acquiring%2520knowledge%2520and%2520behaviours%2520that%2520serve%2520human%2520practical%2520objectives%2520while%2520remaining%2520aligned%2520with%2520broader%2520human%2520values%2520%2528e.g.%252C%2520safety%2520and%2520ethics%2529.%2520This%2520problem%2520remains%2520largely%2520underexplored%2520and%2520lacks%2520a%2520unifying%2520conceptual%2520and%2520formal%2520framework.%2520Here%252C%2520we%2520address%2520one%2520of%2520its%2520most%2520challenging%2520instances%2520of%2520the%2520problem%253A%2520open-ended%2520learning%2520%2528OEL%2529%2520robots%252C%2520which%2520autonomously%2520acquire%2520new%2520knowledge%2520and%2520skills%2520through%2520interaction%2520with%2520the%2520environment%252C%2520guided%2520by%2520intrinsic%2520motivations%2520and%2520self-generated%2520goals.%2520We%2520propose%2520a%2520computational%2520framework%252C%2520introduced%2520qualitatively%2520and%2520then%2520formalised%252C%2520to%2520guide%2520the%2520design%2520of%2520OEL%2520architectures%2520that%2520balance%2520autonomy%2520with%2520human%2520control.%2520At%2520its%2520core%2520is%2520the%2520novel%2520concept%2520of%2520purpose%252C%2520which%2520specifies%2520what%2520humans%2520%2528designers%2520or%2520users%2529%2520want%2520the%2520robot%2520to%2520learn%252C%2520do%252C%2520or%2520avoid%252C%2520independently%2520of%2520specific%2520task%2520domains.%2520The%2520framework%2520decomposes%2520the%2520autonomy-alignment%2520problem%2520into%2520four%2520tractable%2520sub-problems%253A%2520the%2520alignment%2520of%2520robot%2520purposes%2520%2528hardwired%2520or%2520learnt%2529%2520with%2520human%2520purposes%253B%2520the%2520arbitration%2520between%2520multiple%2520purposes%253B%2520the%2520grounding%2520of%2520abstract%2520purposes%2520into%2520domain-specific%2520goals%253B%2520and%2520the%2520acquisition%2520of%2520competence%2520to%2520achieve%2520those%2520goals.%2520The%2520framework%2520supports%2520formal%2520definitions%2520of%2520alignment%2520across%2520multiple%2520cases%2520and%2520proofs%2520of%2520necessary%2520and%2520sufficient%2520conditions%2520under%2520which%2520alignment%2520holds.%2520Illustrative%2520hypothetical%2520scenarios%2520showcase%2520the%2520applicability%2520of%2520the%2520framework%2520for%2520guiding%2520the%2520development%2520of%2520purpose-aligned%2520autonomous%2520robots.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02514v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Autonomy-Alignment%20Problem%20in%20Open-Ended%20Learning%20Robots%3A%20Formalising%20the%20Purpose%20Framework&entry.906535625=Gianluca%20Baldassarre%20and%20Richard%20J.%20Duro%20and%20Emilio%20Cartoni%20and%20Mehdi%20Khamassi%20and%20Alejandro%20Romero%20and%20Vieri%20Giuliano%20Santucci&entry.1292438233=The%20rapid%20advancement%20of%20artificial%20intelligence%20is%20enabling%20the%20development%20of%20increasingly%20autonomous%20robots%20capable%20of%20operating%20beyond%20engineered%20factory%20settings%20and%20into%20the%20unstructured%20environments%20of%20human%20life.%20This%20shift%20raises%20a%20critical%20autonomy-alignment%20problem%3A%20how%20to%20ensure%20that%20a%20robot%27s%20autonomous%20learning%20focuses%20on%20acquiring%20knowledge%20and%20behaviours%20that%20serve%20human%20practical%20objectives%20while%20remaining%20aligned%20with%20broader%20human%20values%20%28e.g.%2C%20safety%20and%20ethics%29.%20This%20problem%20remains%20largely%20underexplored%20and%20lacks%20a%20unifying%20conceptual%20and%20formal%20framework.%20Here%2C%20we%20address%20one%20of%20its%20most%20challenging%20instances%20of%20the%20problem%3A%20open-ended%20learning%20%28OEL%29%20robots%2C%20which%20autonomously%20acquire%20new%20knowledge%20and%20skills%20through%20interaction%20with%20the%20environment%2C%20guided%20by%20intrinsic%20motivations%20and%20self-generated%20goals.%20We%20propose%20a%20computational%20framework%2C%20introduced%20qualitatively%20and%20then%20formalised%2C%20to%20guide%20the%20design%20of%20OEL%20architectures%20that%20balance%20autonomy%20with%20human%20control.%20At%20its%20core%20is%20the%20novel%20concept%20of%20purpose%2C%20which%20specifies%20what%20humans%20%28designers%20or%20users%29%20want%20the%20robot%20to%20learn%2C%20do%2C%20or%20avoid%2C%20independently%20of%20specific%20task%20domains.%20The%20framework%20decomposes%20the%20autonomy-alignment%20problem%20into%20four%20tractable%20sub-problems%3A%20the%20alignment%20of%20robot%20purposes%20%28hardwired%20or%20learnt%29%20with%20human%20purposes%3B%20the%20arbitration%20between%20multiple%20purposes%3B%20the%20grounding%20of%20abstract%20purposes%20into%20domain-specific%20goals%3B%20and%20the%20acquisition%20of%20competence%20to%20achieve%20those%20goals.%20The%20framework%20supports%20formal%20definitions%20of%20alignment%20across%20multiple%20cases%20and%20proofs%20of%20necessary%20and%20sufficient%20conditions%20under%20which%20alignment%20holds.%20Illustrative%20hypothetical%20scenarios%20showcase%20the%20applicability%20of%20the%20framework%20for%20guiding%20the%20development%20of%20purpose-aligned%20autonomous%20robots.&entry.1838667208=http%3A//arxiv.org/abs/2403.02514v3&entry.124074799=Read"},
{"title": "Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting", "author": "Souhir Ben Amor and Florian Ziel", "abstract": "We present a novel recurrent neural network architecture designed explicitly for day-ahead electricity price forecasting, aimed at improving short-term decision-making and operational management in energy systems. Our combined forecasting model embeds linear structures, such as expert models and Kalman filters, into recurrent networks, enabling efficient computation and enhanced interpretability. The design leverages the strengths of both linear and non-linear model structures, allowing it to capture all relevant stylised price characteristics in power markets, including calendar and autoregressive effects, as well as influences from load, renewable energy, and related fuel and carbon markets. For empirical testing, we use hourly data from the largest European electricity market spanning 2018 to 2025 in a comprehensive forecasting study, comparing our model against state-of-the-art approaches, particularly high-dimensional linear and neural network models. The proposed model achieves approximately 12% higher accuracy than leading benchmarks. We evaluate the contributions of the interpretable model components and conclude on the impact of combining linear and non-linear structures.", "link": "http://arxiv.org/abs/2512.04690v1", "date": "2025-12-04", "relevancy": 1.777, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4831}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4334}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Neural%20Networks%20with%20Linear%20Structures%20for%20Electricity%20Price%20Forecasting&body=Title%3A%20Recurrent%20Neural%20Networks%20with%20Linear%20Structures%20for%20Electricity%20Price%20Forecasting%0AAuthor%3A%20Souhir%20Ben%20Amor%20and%20Florian%20Ziel%0AAbstract%3A%20We%20present%20a%20novel%20recurrent%20neural%20network%20architecture%20designed%20explicitly%20for%20day-ahead%20electricity%20price%20forecasting%2C%20aimed%20at%20improving%20short-term%20decision-making%20and%20operational%20management%20in%20energy%20systems.%20Our%20combined%20forecasting%20model%20embeds%20linear%20structures%2C%20such%20as%20expert%20models%20and%20Kalman%20filters%2C%20into%20recurrent%20networks%2C%20enabling%20efficient%20computation%20and%20enhanced%20interpretability.%20The%20design%20leverages%20the%20strengths%20of%20both%20linear%20and%20non-linear%20model%20structures%2C%20allowing%20it%20to%20capture%20all%20relevant%20stylised%20price%20characteristics%20in%20power%20markets%2C%20including%20calendar%20and%20autoregressive%20effects%2C%20as%20well%20as%20influences%20from%20load%2C%20renewable%20energy%2C%20and%20related%20fuel%20and%20carbon%20markets.%20For%20empirical%20testing%2C%20we%20use%20hourly%20data%20from%20the%20largest%20European%20electricity%20market%20spanning%202018%20to%202025%20in%20a%20comprehensive%20forecasting%20study%2C%20comparing%20our%20model%20against%20state-of-the-art%20approaches%2C%20particularly%20high-dimensional%20linear%20and%20neural%20network%20models.%20The%20proposed%20model%20achieves%20approximately%2012%25%20higher%20accuracy%20than%20leading%20benchmarks.%20We%20evaluate%20the%20contributions%20of%20the%20interpretable%20model%20components%20and%20conclude%20on%20the%20impact%20of%20combining%20linear%20and%20non-linear%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Neural%2520Networks%2520with%2520Linear%2520Structures%2520for%2520Electricity%2520Price%2520Forecasting%26entry.906535625%3DSouhir%2520Ben%2520Amor%2520and%2520Florian%2520Ziel%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520recurrent%2520neural%2520network%2520architecture%2520designed%2520explicitly%2520for%2520day-ahead%2520electricity%2520price%2520forecasting%252C%2520aimed%2520at%2520improving%2520short-term%2520decision-making%2520and%2520operational%2520management%2520in%2520energy%2520systems.%2520Our%2520combined%2520forecasting%2520model%2520embeds%2520linear%2520structures%252C%2520such%2520as%2520expert%2520models%2520and%2520Kalman%2520filters%252C%2520into%2520recurrent%2520networks%252C%2520enabling%2520efficient%2520computation%2520and%2520enhanced%2520interpretability.%2520The%2520design%2520leverages%2520the%2520strengths%2520of%2520both%2520linear%2520and%2520non-linear%2520model%2520structures%252C%2520allowing%2520it%2520to%2520capture%2520all%2520relevant%2520stylised%2520price%2520characteristics%2520in%2520power%2520markets%252C%2520including%2520calendar%2520and%2520autoregressive%2520effects%252C%2520as%2520well%2520as%2520influences%2520from%2520load%252C%2520renewable%2520energy%252C%2520and%2520related%2520fuel%2520and%2520carbon%2520markets.%2520For%2520empirical%2520testing%252C%2520we%2520use%2520hourly%2520data%2520from%2520the%2520largest%2520European%2520electricity%2520market%2520spanning%25202018%2520to%25202025%2520in%2520a%2520comprehensive%2520forecasting%2520study%252C%2520comparing%2520our%2520model%2520against%2520state-of-the-art%2520approaches%252C%2520particularly%2520high-dimensional%2520linear%2520and%2520neural%2520network%2520models.%2520The%2520proposed%2520model%2520achieves%2520approximately%252012%2525%2520higher%2520accuracy%2520than%2520leading%2520benchmarks.%2520We%2520evaluate%2520the%2520contributions%2520of%2520the%2520interpretable%2520model%2520components%2520and%2520conclude%2520on%2520the%2520impact%2520of%2520combining%2520linear%2520and%2520non-linear%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Neural%20Networks%20with%20Linear%20Structures%20for%20Electricity%20Price%20Forecasting&entry.906535625=Souhir%20Ben%20Amor%20and%20Florian%20Ziel&entry.1292438233=We%20present%20a%20novel%20recurrent%20neural%20network%20architecture%20designed%20explicitly%20for%20day-ahead%20electricity%20price%20forecasting%2C%20aimed%20at%20improving%20short-term%20decision-making%20and%20operational%20management%20in%20energy%20systems.%20Our%20combined%20forecasting%20model%20embeds%20linear%20structures%2C%20such%20as%20expert%20models%20and%20Kalman%20filters%2C%20into%20recurrent%20networks%2C%20enabling%20efficient%20computation%20and%20enhanced%20interpretability.%20The%20design%20leverages%20the%20strengths%20of%20both%20linear%20and%20non-linear%20model%20structures%2C%20allowing%20it%20to%20capture%20all%20relevant%20stylised%20price%20characteristics%20in%20power%20markets%2C%20including%20calendar%20and%20autoregressive%20effects%2C%20as%20well%20as%20influences%20from%20load%2C%20renewable%20energy%2C%20and%20related%20fuel%20and%20carbon%20markets.%20For%20empirical%20testing%2C%20we%20use%20hourly%20data%20from%20the%20largest%20European%20electricity%20market%20spanning%202018%20to%202025%20in%20a%20comprehensive%20forecasting%20study%2C%20comparing%20our%20model%20against%20state-of-the-art%20approaches%2C%20particularly%20high-dimensional%20linear%20and%20neural%20network%20models.%20The%20proposed%20model%20achieves%20approximately%2012%25%20higher%20accuracy%20than%20leading%20benchmarks.%20We%20evaluate%20the%20contributions%20of%20the%20interpretable%20model%20components%20and%20conclude%20on%20the%20impact%20of%20combining%20linear%20and%20non-linear%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2512.04690v1&entry.124074799=Read"},
{"title": "A lightweight detector for real-time detection of remote sensing images", "author": "Qianyi Wang and Guoqiang Ren", "abstract": "Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.", "link": "http://arxiv.org/abs/2511.17147v2", "date": "2025-12-04", "relevancy": 2.1448, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5734}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5195}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20lightweight%20detector%20for%20real-time%20detection%20of%20remote%20sensing%20images&body=Title%3A%20A%20lightweight%20detector%20for%20real-time%20detection%20of%20remote%20sensing%20images%0AAuthor%3A%20Qianyi%20Wang%20and%20Guoqiang%20Ren%0AAbstract%3A%20Remote%20sensing%20imagery%20is%20widely%20used%20across%20various%20fields%2C%20yet%20real-time%20detection%20remains%20challenging%20due%20to%20the%20prevalence%20of%20small%20objects%20and%20the%20need%20to%20balance%20accuracy%20with%20efficiency.%20To%20address%20this%2C%20we%20propose%20DMG-YOLO%2C%20a%20lightweight%20real-time%20detector%20tailored%20for%20small%20object%20detection%20in%20remote%20sensing%20images.%20Specifically%2C%20we%20design%20a%20Dual-branch%20Feature%20Extraction%20%28DFE%29%20module%20in%20the%20backbone%2C%20which%20partitions%20feature%20maps%20into%20two%20parallel%20branches%3A%20one%20extracts%20local%20features%20via%20depthwise%20separable%20convolutions%2C%20and%20the%20other%20captures%20global%20context%20using%20a%20vision%20transformer%20with%20a%20gating%20mechanism.%20Additionally%2C%20a%20Multi-scale%20Feature%20Fusion%20%28MFF%29%20module%20with%20dilated%20convolutions%20enhances%20multi-scale%20integration%20while%20preserving%20fine%20details.%20In%20the%20neck%2C%20we%20introduce%20the%20Global%20and%20Local%20Aggregate%20Feature%20Pyramid%20Network%20%28GLAFPN%29%20to%20further%20boost%20small%20object%20detection%20through%20global-local%20feature%20fusion.%20Extensive%20experiments%20on%20the%20VisDrone2019%20and%20NWPU%20VHR-10%20datasets%20show%20that%20DMG-YOLO%20achieves%20competitive%20performance%20in%20terms%20of%20mAP%2C%20model%20size%2C%20and%20other%20key%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520lightweight%2520detector%2520for%2520real-time%2520detection%2520of%2520remote%2520sensing%2520images%26entry.906535625%3DQianyi%2520Wang%2520and%2520Guoqiang%2520Ren%26entry.1292438233%3DRemote%2520sensing%2520imagery%2520is%2520widely%2520used%2520across%2520various%2520fields%252C%2520yet%2520real-time%2520detection%2520remains%2520challenging%2520due%2520to%2520the%2520prevalence%2520of%2520small%2520objects%2520and%2520the%2520need%2520to%2520balance%2520accuracy%2520with%2520efficiency.%2520To%2520address%2520this%252C%2520we%2520propose%2520DMG-YOLO%252C%2520a%2520lightweight%2520real-time%2520detector%2520tailored%2520for%2520small%2520object%2520detection%2520in%2520remote%2520sensing%2520images.%2520Specifically%252C%2520we%2520design%2520a%2520Dual-branch%2520Feature%2520Extraction%2520%2528DFE%2529%2520module%2520in%2520the%2520backbone%252C%2520which%2520partitions%2520feature%2520maps%2520into%2520two%2520parallel%2520branches%253A%2520one%2520extracts%2520local%2520features%2520via%2520depthwise%2520separable%2520convolutions%252C%2520and%2520the%2520other%2520captures%2520global%2520context%2520using%2520a%2520vision%2520transformer%2520with%2520a%2520gating%2520mechanism.%2520Additionally%252C%2520a%2520Multi-scale%2520Feature%2520Fusion%2520%2528MFF%2529%2520module%2520with%2520dilated%2520convolutions%2520enhances%2520multi-scale%2520integration%2520while%2520preserving%2520fine%2520details.%2520In%2520the%2520neck%252C%2520we%2520introduce%2520the%2520Global%2520and%2520Local%2520Aggregate%2520Feature%2520Pyramid%2520Network%2520%2528GLAFPN%2529%2520to%2520further%2520boost%2520small%2520object%2520detection%2520through%2520global-local%2520feature%2520fusion.%2520Extensive%2520experiments%2520on%2520the%2520VisDrone2019%2520and%2520NWPU%2520VHR-10%2520datasets%2520show%2520that%2520DMG-YOLO%2520achieves%2520competitive%2520performance%2520in%2520terms%2520of%2520mAP%252C%2520model%2520size%252C%2520and%2520other%2520key%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20lightweight%20detector%20for%20real-time%20detection%20of%20remote%20sensing%20images&entry.906535625=Qianyi%20Wang%20and%20Guoqiang%20Ren&entry.1292438233=Remote%20sensing%20imagery%20is%20widely%20used%20across%20various%20fields%2C%20yet%20real-time%20detection%20remains%20challenging%20due%20to%20the%20prevalence%20of%20small%20objects%20and%20the%20need%20to%20balance%20accuracy%20with%20efficiency.%20To%20address%20this%2C%20we%20propose%20DMG-YOLO%2C%20a%20lightweight%20real-time%20detector%20tailored%20for%20small%20object%20detection%20in%20remote%20sensing%20images.%20Specifically%2C%20we%20design%20a%20Dual-branch%20Feature%20Extraction%20%28DFE%29%20module%20in%20the%20backbone%2C%20which%20partitions%20feature%20maps%20into%20two%20parallel%20branches%3A%20one%20extracts%20local%20features%20via%20depthwise%20separable%20convolutions%2C%20and%20the%20other%20captures%20global%20context%20using%20a%20vision%20transformer%20with%20a%20gating%20mechanism.%20Additionally%2C%20a%20Multi-scale%20Feature%20Fusion%20%28MFF%29%20module%20with%20dilated%20convolutions%20enhances%20multi-scale%20integration%20while%20preserving%20fine%20details.%20In%20the%20neck%2C%20we%20introduce%20the%20Global%20and%20Local%20Aggregate%20Feature%20Pyramid%20Network%20%28GLAFPN%29%20to%20further%20boost%20small%20object%20detection%20through%20global-local%20feature%20fusion.%20Extensive%20experiments%20on%20the%20VisDrone2019%20and%20NWPU%20VHR-10%20datasets%20show%20that%20DMG-YOLO%20achieves%20competitive%20performance%20in%20terms%20of%20mAP%2C%20model%20size%2C%20and%20other%20key%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.17147v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


