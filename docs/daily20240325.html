<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization", "author": "Jialu Wang and Kaichen Zhou and Andrew Markham and Niki Trigoni", "abstract": "  Despite the advancements in deep learning for camera relocalization tasks,\nobtaining ground truth pose labels required for the training process remains a\ncostly endeavor. While current weakly supervised methods excel in lightweight\nlabel generation, their performance notably declines in scenarios with sparse\nviews. In response to this challenge, we introduce WSCLoc, a system capable of\nbeing customized to various deep learning-based relocalization models to\nenhance their performance under weakly-supervised and sparse view conditions.\nThis is realized with two stages. In the initial stage, WSCLoc employs a\nmultilayer perceptron-based structure called WFT-NeRF to co-optimize image\nreconstruction quality and initial pose information. To ensure a stable\nlearning process, we incorporate temporal information as input. Furthermore,\ninstead of optimizing SE(3), we opt for $\\mathfrak{sim}(3)$ optimization to\nexplicitly enforce a scale constraint. In the second stage, we co-optimize the\npre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by\nTime-Encoding based Random View Synthesis and supervised by inter-frame\ngeometric constraints that consider pose, depth, and RGB information. We\nvalidate our approaches on two publicly available datasets, one outdoor and one\nindoor. Our experimental results demonstrate that our weakly-supervised\nrelocalization solutions achieve superior pose estimation accuracy in\nsparse-view scenarios, comparable to state-of-the-art camera relocalization\nmethods. We will make our code publicly available.\n", "link": "http://arxiv.org/abs/2403.15272v1", "date": "2024-03-22", "relevancy": 2.9191, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5989}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5869}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5657}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WSCLoc%3A%20Weakly-Supervised%20Sparse-View%20Camera%20Relocalization&body=Title%3A%20WSCLoc%3A%20Weakly-Supervised%20Sparse-View%20Camera%20Relocalization%0AAuthor%3A%20Jialu%20Wang%20and%20Kaichen%20Zhou%20and%20Andrew%20Markham%20and%20Niki%20Trigoni%0AAbstract%3A%20%20%20Despite%20the%20advancements%20in%20deep%20learning%20for%20camera%20relocalization%20tasks%2C%0Aobtaining%20ground%20truth%20pose%20labels%20required%20for%20the%20training%20process%20remains%20a%0Acostly%20endeavor.%20While%20current%20weakly%20supervised%20methods%20excel%20in%20lightweight%0Alabel%20generation%2C%20their%20performance%20notably%20declines%20in%20scenarios%20with%20sparse%0Aviews.%20In%20response%20to%20this%20challenge%2C%20we%20introduce%20WSCLoc%2C%20a%20system%20capable%20of%0Abeing%20customized%20to%20various%20deep%20learning-based%20relocalization%20models%20to%0Aenhance%20their%20performance%20under%20weakly-supervised%20and%20sparse%20view%20conditions.%0AThis%20is%20realized%20with%20two%20stages.%20In%20the%20initial%20stage%2C%20WSCLoc%20employs%20a%0Amultilayer%20perceptron-based%20structure%20called%20WFT-NeRF%20to%20co-optimize%20image%0Areconstruction%20quality%20and%20initial%20pose%20information.%20To%20ensure%20a%20stable%0Alearning%20process%2C%20we%20incorporate%20temporal%20information%20as%20input.%20Furthermore%2C%0Ainstead%20of%20optimizing%20SE%283%29%2C%20we%20opt%20for%20%24%5Cmathfrak%7Bsim%7D%283%29%24%20optimization%20to%0Aexplicitly%20enforce%20a%20scale%20constraint.%20In%20the%20second%20stage%2C%20we%20co-optimize%20the%0Apre-trained%20WFT-NeRF%20and%20WFT-Pose.%20This%20optimization%20is%20enhanced%20by%0ATime-Encoding%20based%20Random%20View%20Synthesis%20and%20supervised%20by%20inter-frame%0Ageometric%20constraints%20that%20consider%20pose%2C%20depth%2C%20and%20RGB%20information.%20We%0Avalidate%20our%20approaches%20on%20two%20publicly%20available%20datasets%2C%20one%20outdoor%20and%20one%0Aindoor.%20Our%20experimental%20results%20demonstrate%20that%20our%20weakly-supervised%0Arelocalization%20solutions%20achieve%20superior%20pose%20estimation%20accuracy%20in%0Asparse-view%20scenarios%2C%20comparable%20to%20state-of-the-art%20camera%20relocalization%0Amethods.%20We%20will%20make%20our%20code%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15272v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WSCLoc%3A%20Weakly-Supervised%20Sparse-View%20Camera%20Relocalization&entry.906535625=Jialu%20Wang%20and%20Kaichen%20Zhou%20and%20Andrew%20Markham%20and%20Niki%20Trigoni&entry.1292438233=%20%20Despite%20the%20advancements%20in%20deep%20learning%20for%20camera%20relocalization%20tasks%2C%0Aobtaining%20ground%20truth%20pose%20labels%20required%20for%20the%20training%20process%20remains%20a%0Acostly%20endeavor.%20While%20current%20weakly%20supervised%20methods%20excel%20in%20lightweight%0Alabel%20generation%2C%20their%20performance%20notably%20declines%20in%20scenarios%20with%20sparse%0Aviews.%20In%20response%20to%20this%20challenge%2C%20we%20introduce%20WSCLoc%2C%20a%20system%20capable%20of%0Abeing%20customized%20to%20various%20deep%20learning-based%20relocalization%20models%20to%0Aenhance%20their%20performance%20under%20weakly-supervised%20and%20sparse%20view%20conditions.%0AThis%20is%20realized%20with%20two%20stages.%20In%20the%20initial%20stage%2C%20WSCLoc%20employs%20a%0Amultilayer%20perceptron-based%20structure%20called%20WFT-NeRF%20to%20co-optimize%20image%0Areconstruction%20quality%20and%20initial%20pose%20information.%20To%20ensure%20a%20stable%0Alearning%20process%2C%20we%20incorporate%20temporal%20information%20as%20input.%20Furthermore%2C%0Ainstead%20of%20optimizing%20SE%283%29%2C%20we%20opt%20for%20%24%5Cmathfrak%7Bsim%7D%283%29%24%20optimization%20to%0Aexplicitly%20enforce%20a%20scale%20constraint.%20In%20the%20second%20stage%2C%20we%20co-optimize%20the%0Apre-trained%20WFT-NeRF%20and%20WFT-Pose.%20This%20optimization%20is%20enhanced%20by%0ATime-Encoding%20based%20Random%20View%20Synthesis%20and%20supervised%20by%20inter-frame%0Ageometric%20constraints%20that%20consider%20pose%2C%20depth%2C%20and%20RGB%20information.%20We%0Avalidate%20our%20approaches%20on%20two%20publicly%20available%20datasets%2C%20one%20outdoor%20and%20one%0Aindoor.%20Our%20experimental%20results%20demonstrate%20that%20our%20weakly-supervised%0Arelocalization%20solutions%20achieve%20superior%20pose%20estimation%20accuracy%20in%0Asparse-view%20scenarios%2C%20comparable%20to%20state-of-the-art%20camera%20relocalization%0Amethods.%20We%20will%20make%20our%20code%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15272v1&entry.124074799=Read"},
{"title": "Learning Topological Representations for Deep Image Understanding", "author": "Xiaoling Hu", "abstract": "  In many scenarios, especially biomedical applications, the correct\ndelineation of complex fine-scaled structures such as neurons, tissues, and\nvessels is critical for downstream analysis. Despite the strong predictive\npower of deep learning methods, they do not provide a satisfactory\nrepresentation of these structures, thus creating significant barriers in\nscalable annotation and downstream analysis. In this dissertation, we tackle\nsuch challenges by proposing novel representations of these topological\nstructures in a deep learning framework. We leverage the mathematical tools\nfrom topological data analysis, i.e., persistent homology and discrete Morse\ntheory, to develop principled methods for better segmentation and uncertainty\nestimation, which will become powerful tools for scalable annotation.\n", "link": "http://arxiv.org/abs/2403.15361v1", "date": "2024-03-22", "relevancy": 2.7855, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5602}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5404}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Topological%20Representations%20for%20Deep%20Image%20Understanding&body=Title%3A%20Learning%20Topological%20Representations%20for%20Deep%20Image%20Understanding%0AAuthor%3A%20Xiaoling%20Hu%0AAbstract%3A%20%20%20In%20many%20scenarios%2C%20especially%20biomedical%20applications%2C%20the%20correct%0Adelineation%20of%20complex%20fine-scaled%20structures%20such%20as%20neurons%2C%20tissues%2C%20and%0Avessels%20is%20critical%20for%20downstream%20analysis.%20Despite%20the%20strong%20predictive%0Apower%20of%20deep%20learning%20methods%2C%20they%20do%20not%20provide%20a%20satisfactory%0Arepresentation%20of%20these%20structures%2C%20thus%20creating%20significant%20barriers%20in%0Ascalable%20annotation%20and%20downstream%20analysis.%20In%20this%20dissertation%2C%20we%20tackle%0Asuch%20challenges%20by%20proposing%20novel%20representations%20of%20these%20topological%0Astructures%20in%20a%20deep%20learning%20framework.%20We%20leverage%20the%20mathematical%20tools%0Afrom%20topological%20data%20analysis%2C%20i.e.%2C%20persistent%20homology%20and%20discrete%20Morse%0Atheory%2C%20to%20develop%20principled%20methods%20for%20better%20segmentation%20and%20uncertainty%0Aestimation%2C%20which%20will%20become%20powerful%20tools%20for%20scalable%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15361v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Topological%20Representations%20for%20Deep%20Image%20Understanding&entry.906535625=Xiaoling%20Hu&entry.1292438233=%20%20In%20many%20scenarios%2C%20especially%20biomedical%20applications%2C%20the%20correct%0Adelineation%20of%20complex%20fine-scaled%20structures%20such%20as%20neurons%2C%20tissues%2C%20and%0Avessels%20is%20critical%20for%20downstream%20analysis.%20Despite%20the%20strong%20predictive%0Apower%20of%20deep%20learning%20methods%2C%20they%20do%20not%20provide%20a%20satisfactory%0Arepresentation%20of%20these%20structures%2C%20thus%20creating%20significant%20barriers%20in%0Ascalable%20annotation%20and%20downstream%20analysis.%20In%20this%20dissertation%2C%20we%20tackle%0Asuch%20challenges%20by%20proposing%20novel%20representations%20of%20these%20topological%0Astructures%20in%20a%20deep%20learning%20framework.%20We%20leverage%20the%20mathematical%20tools%0Afrom%20topological%20data%20analysis%2C%20i.e.%2C%20persistent%20homology%20and%20discrete%20Morse%0Atheory%2C%20to%20develop%20principled%20methods%20for%20better%20segmentation%20and%20uncertainty%0Aestimation%2C%20which%20will%20become%20powerful%20tools%20for%20scalable%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15361v1&entry.124074799=Read"},
{"title": "A Multimodal Approach for Cross-Domain Image Retrieval", "author": "Lucas Iijima and Tania Stathaki", "abstract": "  Image generators are gaining vast amount of popularity and have rapidly\nchanged how digital content is created. With the latest AI technology, millions\nof high quality images are being generated by the public, which are constantly\nmotivating the research community to push the limits of generative models to\ncreate more complex and realistic images. This paper focuses on Cross-Domain\nImage Retrieval (CDIR) which can be used as an additional tool to inspect\ncollections of generated images by determining the level of similarity between\nimages in a dataset. An ideal retrieval system would be able to generalize to\nunseen complex images from multiple domains (e.g., photos, drawings and\npaintings). To address this goal, we propose a novel caption-matching approach\nthat leverages multimodal language-vision architectures pre-trained on large\ndatasets. The method is tested on DomainNet and Office-Home datasets and\nconsistently achieves state-of-the-art performance over the latest approaches\nin the literature for cross-domain image retrieval. In order to verify the\neffectiveness with AI-generated images, the method was also put to test with a\ndatabase composed by samples collected from Midjourney, which is a widely used\ngenerative platform for content creation.\n", "link": "http://arxiv.org/abs/2403.15152v1", "date": "2024-03-22", "relevancy": 2.7667, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5675}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5545}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.538}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Approach%20for%20Cross-Domain%20Image%20Retrieval&body=Title%3A%20A%20Multimodal%20Approach%20for%20Cross-Domain%20Image%20Retrieval%0AAuthor%3A%20Lucas%20Iijima%20and%20Tania%20Stathaki%0AAbstract%3A%20%20%20Image%20generators%20are%20gaining%20vast%20amount%20of%20popularity%20and%20have%20rapidly%0Achanged%20how%20digital%20content%20is%20created.%20With%20the%20latest%20AI%20technology%2C%20millions%0Aof%20high%20quality%20images%20are%20being%20generated%20by%20the%20public%2C%20which%20are%20constantly%0Amotivating%20the%20research%20community%20to%20push%20the%20limits%20of%20generative%20models%20to%0Acreate%20more%20complex%20and%20realistic%20images.%20This%20paper%20focuses%20on%20Cross-Domain%0AImage%20Retrieval%20%28CDIR%29%20which%20can%20be%20used%20as%20an%20additional%20tool%20to%20inspect%0Acollections%20of%20generated%20images%20by%20determining%20the%20level%20of%20similarity%20between%0Aimages%20in%20a%20dataset.%20An%20ideal%20retrieval%20system%20would%20be%20able%20to%20generalize%20to%0Aunseen%20complex%20images%20from%20multiple%20domains%20%28e.g.%2C%20photos%2C%20drawings%20and%0Apaintings%29.%20To%20address%20this%20goal%2C%20we%20propose%20a%20novel%20caption-matching%20approach%0Athat%20leverages%20multimodal%20language-vision%20architectures%20pre-trained%20on%20large%0Adatasets.%20The%20method%20is%20tested%20on%20DomainNet%20and%20Office-Home%20datasets%20and%0Aconsistently%20achieves%20state-of-the-art%20performance%20over%20the%20latest%20approaches%0Ain%20the%20literature%20for%20cross-domain%20image%20retrieval.%20In%20order%20to%20verify%20the%0Aeffectiveness%20with%20AI-generated%20images%2C%20the%20method%20was%20also%20put%20to%20test%20with%20a%0Adatabase%20composed%20by%20samples%20collected%20from%20Midjourney%2C%20which%20is%20a%20widely%20used%0Agenerative%20platform%20for%20content%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15152v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Approach%20for%20Cross-Domain%20Image%20Retrieval&entry.906535625=Lucas%20Iijima%20and%20Tania%20Stathaki&entry.1292438233=%20%20Image%20generators%20are%20gaining%20vast%20amount%20of%20popularity%20and%20have%20rapidly%0Achanged%20how%20digital%20content%20is%20created.%20With%20the%20latest%20AI%20technology%2C%20millions%0Aof%20high%20quality%20images%20are%20being%20generated%20by%20the%20public%2C%20which%20are%20constantly%0Amotivating%20the%20research%20community%20to%20push%20the%20limits%20of%20generative%20models%20to%0Acreate%20more%20complex%20and%20realistic%20images.%20This%20paper%20focuses%20on%20Cross-Domain%0AImage%20Retrieval%20%28CDIR%29%20which%20can%20be%20used%20as%20an%20additional%20tool%20to%20inspect%0Acollections%20of%20generated%20images%20by%20determining%20the%20level%20of%20similarity%20between%0Aimages%20in%20a%20dataset.%20An%20ideal%20retrieval%20system%20would%20be%20able%20to%20generalize%20to%0Aunseen%20complex%20images%20from%20multiple%20domains%20%28e.g.%2C%20photos%2C%20drawings%20and%0Apaintings%29.%20To%20address%20this%20goal%2C%20we%20propose%20a%20novel%20caption-matching%20approach%0Athat%20leverages%20multimodal%20language-vision%20architectures%20pre-trained%20on%20large%0Adatasets.%20The%20method%20is%20tested%20on%20DomainNet%20and%20Office-Home%20datasets%20and%0Aconsistently%20achieves%20state-of-the-art%20performance%20over%20the%20latest%20approaches%0Ain%20the%20literature%20for%20cross-domain%20image%20retrieval.%20In%20order%20to%20verify%20the%0Aeffectiveness%20with%20AI-generated%20images%2C%20the%20method%20was%20also%20put%20to%20test%20with%20a%0Adatabase%20composed%20by%20samples%20collected%20from%20Midjourney%2C%20which%20is%20a%20widely%20used%0Agenerative%20platform%20for%20content%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15152v1&entry.124074799=Read"},
{"title": "Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks", "author": "Sudhir Sornapudi and Rajhans Singh", "abstract": "  Computer vision in agriculture is game-changing with its ability to transform\nfarming into a data-driven, precise, and sustainable industry. Deep learning\nhas empowered agriculture vision to analyze vast, complex visual data, but\nheavily rely on the availability of large annotated datasets. This remains a\nbottleneck as manual labeling is error-prone, time-consuming, and expensive.\nThe lack of efficient labeling approaches inspired us to consider\nself-supervised learning as a paradigm shift, learning meaningful feature\nrepresentations from raw agricultural image data. In this work, we explore how\nself-supervised representation learning unlocks the potential applicability to\ndiverse agriculture vision tasks by eliminating the need for large-scale\nannotated datasets. We propose a lightweight framework utilizing SimCLR, a\ncontrastive learning approach, to pre-train a ResNet-50 backbone on a large,\nunannotated dataset of real-world agriculture field images. Our experimental\nanalysis and results indicate that the model learns robust features applicable\nto a broad range of downstream agriculture tasks discussed in the paper.\nAdditionally, the reduced reliance on annotated data makes our approach more\ncost-effective and accessible, paving the way for broader adoption of computer\nvision in agriculture.\n", "link": "http://arxiv.org/abs/2403.15248v1", "date": "2024-03-22", "relevancy": 2.7479, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5201}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5186}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Backbone%20Framework%20for%20Diverse%20Agricultural%20Vision%20Tasks&body=Title%3A%20Self-Supervised%20Backbone%20Framework%20for%20Diverse%20Agricultural%20Vision%20Tasks%0AAuthor%3A%20Sudhir%20Sornapudi%20and%20Rajhans%20Singh%0AAbstract%3A%20%20%20Computer%20vision%20in%20agriculture%20is%20game-changing%20with%20its%20ability%20to%20transform%0Afarming%20into%20a%20data-driven%2C%20precise%2C%20and%20sustainable%20industry.%20Deep%20learning%0Ahas%20empowered%20agriculture%20vision%20to%20analyze%20vast%2C%20complex%20visual%20data%2C%20but%0Aheavily%20rely%20on%20the%20availability%20of%20large%20annotated%20datasets.%20This%20remains%20a%0Abottleneck%20as%20manual%20labeling%20is%20error-prone%2C%20time-consuming%2C%20and%20expensive.%0AThe%20lack%20of%20efficient%20labeling%20approaches%20inspired%20us%20to%20consider%0Aself-supervised%20learning%20as%20a%20paradigm%20shift%2C%20learning%20meaningful%20feature%0Arepresentations%20from%20raw%20agricultural%20image%20data.%20In%20this%20work%2C%20we%20explore%20how%0Aself-supervised%20representation%20learning%20unlocks%20the%20potential%20applicability%20to%0Adiverse%20agriculture%20vision%20tasks%20by%20eliminating%20the%20need%20for%20large-scale%0Aannotated%20datasets.%20We%20propose%20a%20lightweight%20framework%20utilizing%20SimCLR%2C%20a%0Acontrastive%20learning%20approach%2C%20to%20pre-train%20a%20ResNet-50%20backbone%20on%20a%20large%2C%0Aunannotated%20dataset%20of%20real-world%20agriculture%20field%20images.%20Our%20experimental%0Aanalysis%20and%20results%20indicate%20that%20the%20model%20learns%20robust%20features%20applicable%0Ato%20a%20broad%20range%20of%20downstream%20agriculture%20tasks%20discussed%20in%20the%20paper.%0AAdditionally%2C%20the%20reduced%20reliance%20on%20annotated%20data%20makes%20our%20approach%20more%0Acost-effective%20and%20accessible%2C%20paving%20the%20way%20for%20broader%20adoption%20of%20computer%0Avision%20in%20agriculture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15248v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Backbone%20Framework%20for%20Diverse%20Agricultural%20Vision%20Tasks&entry.906535625=Sudhir%20Sornapudi%20and%20Rajhans%20Singh&entry.1292438233=%20%20Computer%20vision%20in%20agriculture%20is%20game-changing%20with%20its%20ability%20to%20transform%0Afarming%20into%20a%20data-driven%2C%20precise%2C%20and%20sustainable%20industry.%20Deep%20learning%0Ahas%20empowered%20agriculture%20vision%20to%20analyze%20vast%2C%20complex%20visual%20data%2C%20but%0Aheavily%20rely%20on%20the%20availability%20of%20large%20annotated%20datasets.%20This%20remains%20a%0Abottleneck%20as%20manual%20labeling%20is%20error-prone%2C%20time-consuming%2C%20and%20expensive.%0AThe%20lack%20of%20efficient%20labeling%20approaches%20inspired%20us%20to%20consider%0Aself-supervised%20learning%20as%20a%20paradigm%20shift%2C%20learning%20meaningful%20feature%0Arepresentations%20from%20raw%20agricultural%20image%20data.%20In%20this%20work%2C%20we%20explore%20how%0Aself-supervised%20representation%20learning%20unlocks%20the%20potential%20applicability%20to%0Adiverse%20agriculture%20vision%20tasks%20by%20eliminating%20the%20need%20for%20large-scale%0Aannotated%20datasets.%20We%20propose%20a%20lightweight%20framework%20utilizing%20SimCLR%2C%20a%0Acontrastive%20learning%20approach%2C%20to%20pre-train%20a%20ResNet-50%20backbone%20on%20a%20large%2C%0Aunannotated%20dataset%20of%20real-world%20agriculture%20field%20images.%20Our%20experimental%0Aanalysis%20and%20results%20indicate%20that%20the%20model%20learns%20robust%20features%20applicable%0Ato%20a%20broad%20range%20of%20downstream%20agriculture%20tasks%20discussed%20in%20the%20paper.%0AAdditionally%2C%20the%20reduced%20reliance%20on%20annotated%20data%20makes%20our%20approach%20more%0Acost-effective%20and%20accessible%2C%20paving%20the%20way%20for%20broader%20adoption%20of%20computer%0Avision%20in%20agriculture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15248v1&entry.124074799=Read"},
{"title": "Long-CLIP: Unlocking the Long-Text Capability of CLIP", "author": "Beichen Zhang and Pan Zhang and Xiaoyi Dong and Yuhang Zang and Jiaqi Wang", "abstract": "  Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for\nzero-shot classification, text-image retrieval, and text-image generation by\naligning image and text modalities. Despite its widespread adoption, a\nsignificant limitation of CLIP lies in the inadequate length of text input. The\nlength of the text token is restricted to 77, and an empirical study shows the\nactual effective length is even less than 20. This prevents CLIP from handling\ndetailed descriptions, limiting its applications for image retrieval and\ntext-to-image generation with extensive prerequisites. To this end, we propose\nLong-CLIP as a plug-and-play alternative to CLIP that supports long-text input,\nretains or even surpasses its zero-shot generalizability, and aligns the CLIP\nlatent space, making it readily replace CLIP without any further adaptation in\ndownstream frameworks. Nevertheless, achieving this goal is far from\nstraightforward, as simplistic fine-tuning can result in a significant\ndegradation of CLIP's performance. Moreover, substituting the text encoder with\na language model supporting longer contexts necessitates pretraining with vast\namounts of data, incurring significant expenses. Accordingly, Long-CLIP\nintroduces an efficient fine-tuning solution on CLIP with two novel strategies\ndesigned to maintain the original capabilities, including (1) a\nknowledge-preserved stretching of positional embedding and (2) a primary\ncomponent matching of CLIP features. With leveraging just one million extra\nlong text-image pairs, Long-CLIP has shown the superiority to CLIP for about\n20% in long caption text-image retrieval and 6% in traditional text-image\nretrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers\nenhanced capabilities for generating images from detailed text descriptions by\nreplacing CLIP in a plug-and-play manner.\n", "link": "http://arxiv.org/abs/2403.15378v1", "date": "2024-03-22", "relevancy": 2.7206, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6094}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5148}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5082}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Long-CLIP%3A%20Unlocking%20the%20Long-Text%20Capability%20of%20CLIP&body=Title%3A%20Long-CLIP%3A%20Unlocking%20the%20Long-Text%20Capability%20of%20CLIP%0AAuthor%3A%20Beichen%20Zhang%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20been%20the%20cornerstone%20for%0Azero-shot%20classification%2C%20text-image%20retrieval%2C%20and%20text-image%20generation%20by%0Aaligning%20image%20and%20text%20modalities.%20Despite%20its%20widespread%20adoption%2C%20a%0Asignificant%20limitation%20of%20CLIP%20lies%20in%20the%20inadequate%20length%20of%20text%20input.%20The%0Alength%20of%20the%20text%20token%20is%20restricted%20to%2077%2C%20and%20an%20empirical%20study%20shows%20the%0Aactual%20effective%20length%20is%20even%20less%20than%2020.%20This%20prevents%20CLIP%20from%20handling%0Adetailed%20descriptions%2C%20limiting%20its%20applications%20for%20image%20retrieval%20and%0Atext-to-image%20generation%20with%20extensive%20prerequisites.%20To%20this%20end%2C%20we%20propose%0ALong-CLIP%20as%20a%20plug-and-play%20alternative%20to%20CLIP%20that%20supports%20long-text%20input%2C%0Aretains%20or%20even%20surpasses%20its%20zero-shot%20generalizability%2C%20and%20aligns%20the%20CLIP%0Alatent%20space%2C%20making%20it%20readily%20replace%20CLIP%20without%20any%20further%20adaptation%20in%0Adownstream%20frameworks.%20Nevertheless%2C%20achieving%20this%20goal%20is%20far%20from%0Astraightforward%2C%20as%20simplistic%20fine-tuning%20can%20result%20in%20a%20significant%0Adegradation%20of%20CLIP%27s%20performance.%20Moreover%2C%20substituting%20the%20text%20encoder%20with%0Aa%20language%20model%20supporting%20longer%20contexts%20necessitates%20pretraining%20with%20vast%0Aamounts%20of%20data%2C%20incurring%20significant%20expenses.%20Accordingly%2C%20Long-CLIP%0Aintroduces%20an%20efficient%20fine-tuning%20solution%20on%20CLIP%20with%20two%20novel%20strategies%0Adesigned%20to%20maintain%20the%20original%20capabilities%2C%20including%20%281%29%20a%0Aknowledge-preserved%20stretching%20of%20positional%20embedding%20and%20%282%29%20a%20primary%0Acomponent%20matching%20of%20CLIP%20features.%20With%20leveraging%20just%20one%20million%20extra%0Along%20text-image%20pairs%2C%20Long-CLIP%20has%20shown%20the%20superiority%20to%20CLIP%20for%20about%0A20%25%20in%20long%20caption%20text-image%20retrieval%20and%206%25%20in%20traditional%20text-image%0Aretrieval%20tasks%2C%20e.g.%2C%20COCO%20and%20Flickr30k.%20Furthermore%2C%20Long-CLIP%20offers%0Aenhanced%20capabilities%20for%20generating%20images%20from%20detailed%20text%20descriptions%20by%0Areplacing%20CLIP%20in%20a%20plug-and-play%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15378v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-CLIP%3A%20Unlocking%20the%20Long-Text%20Capability%20of%20CLIP&entry.906535625=Beichen%20Zhang%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20been%20the%20cornerstone%20for%0Azero-shot%20classification%2C%20text-image%20retrieval%2C%20and%20text-image%20generation%20by%0Aaligning%20image%20and%20text%20modalities.%20Despite%20its%20widespread%20adoption%2C%20a%0Asignificant%20limitation%20of%20CLIP%20lies%20in%20the%20inadequate%20length%20of%20text%20input.%20The%0Alength%20of%20the%20text%20token%20is%20restricted%20to%2077%2C%20and%20an%20empirical%20study%20shows%20the%0Aactual%20effective%20length%20is%20even%20less%20than%2020.%20This%20prevents%20CLIP%20from%20handling%0Adetailed%20descriptions%2C%20limiting%20its%20applications%20for%20image%20retrieval%20and%0Atext-to-image%20generation%20with%20extensive%20prerequisites.%20To%20this%20end%2C%20we%20propose%0ALong-CLIP%20as%20a%20plug-and-play%20alternative%20to%20CLIP%20that%20supports%20long-text%20input%2C%0Aretains%20or%20even%20surpasses%20its%20zero-shot%20generalizability%2C%20and%20aligns%20the%20CLIP%0Alatent%20space%2C%20making%20it%20readily%20replace%20CLIP%20without%20any%20further%20adaptation%20in%0Adownstream%20frameworks.%20Nevertheless%2C%20achieving%20this%20goal%20is%20far%20from%0Astraightforward%2C%20as%20simplistic%20fine-tuning%20can%20result%20in%20a%20significant%0Adegradation%20of%20CLIP%27s%20performance.%20Moreover%2C%20substituting%20the%20text%20encoder%20with%0Aa%20language%20model%20supporting%20longer%20contexts%20necessitates%20pretraining%20with%20vast%0Aamounts%20of%20data%2C%20incurring%20significant%20expenses.%20Accordingly%2C%20Long-CLIP%0Aintroduces%20an%20efficient%20fine-tuning%20solution%20on%20CLIP%20with%20two%20novel%20strategies%0Adesigned%20to%20maintain%20the%20original%20capabilities%2C%20including%20%281%29%20a%0Aknowledge-preserved%20stretching%20of%20positional%20embedding%20and%20%282%29%20a%20primary%0Acomponent%20matching%20of%20CLIP%20features.%20With%20leveraging%20just%20one%20million%20extra%0Along%20text-image%20pairs%2C%20Long-CLIP%20has%20shown%20the%20superiority%20to%20CLIP%20for%20about%0A20%25%20in%20long%20caption%20text-image%20retrieval%20and%206%25%20in%20traditional%20text-image%0Aretrieval%20tasks%2C%20e.g.%2C%20COCO%20and%20Flickr30k.%20Furthermore%2C%20Long-CLIP%20offers%0Aenhanced%20capabilities%20for%20generating%20images%20from%20detailed%20text%20descriptions%20by%0Areplacing%20CLIP%20in%20a%20plug-and-play%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15378v1&entry.124074799=Read"},
{"title": "Beyond Inserting: Learning Identity Embedding for Semantic-Fidelity\n  Personalized Diffusion Generation", "author": "Yang Li and Songlin Yang and Wei Wang and Jing Dong", "abstract": "  Advanced diffusion-based Text-to-Image (T2I) models, such as the Stable\nDiffusion Model, have made significant progress in generating diverse and\nhigh-quality images using text prompts alone. However, when non-famous users\nrequire personalized image generation for their identities (IDs), the T2I\nmodels fail to accurately generate their ID-related images. The main problem is\nthat pre-trained T2I models do not learn the mapping between the new ID prompts\nand their corresponding visual content. The previous methods either failed to\naccurately fit the face region or lost the interactive generative ability with\nother existing concepts in T2I models. In other words, they are unable to\ngenerate T2I-aligned and semantic-fidelity images for the given prompts with\nother concepts such as scenes (``Eiffel Tower''), actions (``holding a\nbasketball''), and facial attributes (``eyes closed''). In this paper, we focus\non inserting accurate and interactive ID embedding into the Stable Diffusion\nModel for semantic-fidelity personalized generation. We address this challenge\nfrom two perspectives: face-wise region fitting and semantic-fidelity token\noptimization. Specifically, we first visualize the attention overfit problem\nand propose a face-wise attention loss to fit the face region instead of\nentangling ID-unrelated information, such as face layout and background. This\nkey trick significantly enhances the ID accuracy and interactive generative\nability with other existing concepts. Then, we optimize one ID representation\nas multiple per-stage tokens where each token contains two disentangled\nfeatures. This expansion of the textual conditioning space improves\nsemantic-fidelity control. Extensive experiments validate that our results\nexhibit superior ID accuracy, text-based manipulation ability, and\ngeneralization compared to previous methods.\n", "link": "http://arxiv.org/abs/2402.00631v2", "date": "2024-03-22", "relevancy": 2.6851, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7376}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6256}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6232}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Beyond%20Inserting%3A%20Learning%20Identity%20Embedding%20for%20Semantic-Fidelity%0A%20%20Personalized%20Diffusion%20Generation&body=Title%3A%20Beyond%20Inserting%3A%20Learning%20Identity%20Embedding%20for%20Semantic-Fidelity%0A%20%20Personalized%20Diffusion%20Generation%0AAuthor%3A%20Yang%20Li%20and%20Songlin%20Yang%20and%20Wei%20Wang%20and%20Jing%20Dong%0AAbstract%3A%20%20%20Advanced%20diffusion-based%20Text-to-Image%20%28T2I%29%20models%2C%20such%20as%20the%20Stable%0ADiffusion%20Model%2C%20have%20made%20significant%20progress%20in%20generating%20diverse%20and%0Ahigh-quality%20images%20using%20text%20prompts%20alone.%20However%2C%20when%20non-famous%20users%0Arequire%20personalized%20image%20generation%20for%20their%20identities%20%28IDs%29%2C%20the%20T2I%0Amodels%20fail%20to%20accurately%20generate%20their%20ID-related%20images.%20The%20main%20problem%20is%0Athat%20pre-trained%20T2I%20models%20do%20not%20learn%20the%20mapping%20between%20the%20new%20ID%20prompts%0Aand%20their%20corresponding%20visual%20content.%20The%20previous%20methods%20either%20failed%20to%0Aaccurately%20fit%20the%20face%20region%20or%20lost%20the%20interactive%20generative%20ability%20with%0Aother%20existing%20concepts%20in%20T2I%20models.%20In%20other%20words%2C%20they%20are%20unable%20to%0Agenerate%20T2I-aligned%20and%20semantic-fidelity%20images%20for%20the%20given%20prompts%20with%0Aother%20concepts%20such%20as%20scenes%20%28%60%60Eiffel%20Tower%27%27%29%2C%20actions%20%28%60%60holding%20a%0Abasketball%27%27%29%2C%20and%20facial%20attributes%20%28%60%60eyes%20closed%27%27%29.%20In%20this%20paper%2C%20we%20focus%0Aon%20inserting%20accurate%20and%20interactive%20ID%20embedding%20into%20the%20Stable%20Diffusion%0AModel%20for%20semantic-fidelity%20personalized%20generation.%20We%20address%20this%20challenge%0Afrom%20two%20perspectives%3A%20face-wise%20region%20fitting%20and%20semantic-fidelity%20token%0Aoptimization.%20Specifically%2C%20we%20first%20visualize%20the%20attention%20overfit%20problem%0Aand%20propose%20a%20face-wise%20attention%20loss%20to%20fit%20the%20face%20region%20instead%20of%0Aentangling%20ID-unrelated%20information%2C%20such%20as%20face%20layout%20and%20background.%20This%0Akey%20trick%20significantly%20enhances%20the%20ID%20accuracy%20and%20interactive%20generative%0Aability%20with%20other%20existing%20concepts.%20Then%2C%20we%20optimize%20one%20ID%20representation%0Aas%20multiple%20per-stage%20tokens%20where%20each%20token%20contains%20two%20disentangled%0Afeatures.%20This%20expansion%20of%20the%20textual%20conditioning%20space%20improves%0Asemantic-fidelity%20control.%20Extensive%20experiments%20validate%20that%20our%20results%0Aexhibit%20superior%20ID%20accuracy%2C%20text-based%20manipulation%20ability%2C%20and%0Ageneralization%20compared%20to%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00631v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Inserting%3A%20Learning%20Identity%20Embedding%20for%20Semantic-Fidelity%0A%20%20Personalized%20Diffusion%20Generation&entry.906535625=Yang%20Li%20and%20Songlin%20Yang%20and%20Wei%20Wang%20and%20Jing%20Dong&entry.1292438233=%20%20Advanced%20diffusion-based%20Text-to-Image%20%28T2I%29%20models%2C%20such%20as%20the%20Stable%0ADiffusion%20Model%2C%20have%20made%20significant%20progress%20in%20generating%20diverse%20and%0Ahigh-quality%20images%20using%20text%20prompts%20alone.%20However%2C%20when%20non-famous%20users%0Arequire%20personalized%20image%20generation%20for%20their%20identities%20%28IDs%29%2C%20the%20T2I%0Amodels%20fail%20to%20accurately%20generate%20their%20ID-related%20images.%20The%20main%20problem%20is%0Athat%20pre-trained%20T2I%20models%20do%20not%20learn%20the%20mapping%20between%20the%20new%20ID%20prompts%0Aand%20their%20corresponding%20visual%20content.%20The%20previous%20methods%20either%20failed%20to%0Aaccurately%20fit%20the%20face%20region%20or%20lost%20the%20interactive%20generative%20ability%20with%0Aother%20existing%20concepts%20in%20T2I%20models.%20In%20other%20words%2C%20they%20are%20unable%20to%0Agenerate%20T2I-aligned%20and%20semantic-fidelity%20images%20for%20the%20given%20prompts%20with%0Aother%20concepts%20such%20as%20scenes%20%28%60%60Eiffel%20Tower%27%27%29%2C%20actions%20%28%60%60holding%20a%0Abasketball%27%27%29%2C%20and%20facial%20attributes%20%28%60%60eyes%20closed%27%27%29.%20In%20this%20paper%2C%20we%20focus%0Aon%20inserting%20accurate%20and%20interactive%20ID%20embedding%20into%20the%20Stable%20Diffusion%0AModel%20for%20semantic-fidelity%20personalized%20generation.%20We%20address%20this%20challenge%0Afrom%20two%20perspectives%3A%20face-wise%20region%20fitting%20and%20semantic-fidelity%20token%0Aoptimization.%20Specifically%2C%20we%20first%20visualize%20the%20attention%20overfit%20problem%0Aand%20propose%20a%20face-wise%20attention%20loss%20to%20fit%20the%20face%20region%20instead%20of%0Aentangling%20ID-unrelated%20information%2C%20such%20as%20face%20layout%20and%20background.%20This%0Akey%20trick%20significantly%20enhances%20the%20ID%20accuracy%20and%20interactive%20generative%0Aability%20with%20other%20existing%20concepts.%20Then%2C%20we%20optimize%20one%20ID%20representation%0Aas%20multiple%20per-stage%20tokens%20where%20each%20token%20contains%20two%20disentangled%0Afeatures.%20This%20expansion%20of%20the%20textual%20conditioning%20space%20improves%0Asemantic-fidelity%20control.%20Extensive%20experiments%20validate%20that%20our%20results%0Aexhibit%20superior%20ID%20accuracy%2C%20text-based%20manipulation%20ability%2C%20and%0Ageneralization%20compared%20to%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00631v2&entry.124074799=Read"},
{"title": "EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic\n  Surgeries using Gaussian Splatting", "author": "Kailing Wang and Chen Yang and Yuehao Wang and Sikuang Li and Yan Wang and Qi Dou and Xiaokang Yang and Wei Shen", "abstract": "  Precise camera tracking, high-fidelity 3D tissue reconstruction, and\nreal-time online visualization are critical for intrabody medical imaging\ndevices such as endoscopes and capsule robots. However, existing SLAM\n(Simultaneous Localization and Mapping) methods often struggle to achieve both\ncomplete high-quality surgical field reconstruction and efficient computation,\nrestricting their intraoperative applications among endoscopic surgeries. In\nthis paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic\nsurgeries, which integrates streamlined Gaussian representation and\ndifferentiable rasterization to facilitate over 100 fps rendering speed during\nonline camera tracking and tissue reconstructing. Extensive experiments show\nthat EndoGSLAM achieves a better trade-off between intraoperative availability\nand reconstruction quality than traditional or neural SLAM approaches, showing\ntremendous potential for endoscopic surgeries. The project page is at\nhttps://EndoGSLAM.loping151.com\n", "link": "http://arxiv.org/abs/2403.15124v1", "date": "2024-03-22", "relevancy": 2.6806, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5642}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EndoGSLAM%3A%20Real-Time%20Dense%20Reconstruction%20and%20Tracking%20in%20Endoscopic%0A%20%20Surgeries%20using%20Gaussian%20Splatting&body=Title%3A%20EndoGSLAM%3A%20Real-Time%20Dense%20Reconstruction%20and%20Tracking%20in%20Endoscopic%0A%20%20Surgeries%20using%20Gaussian%20Splatting%0AAuthor%3A%20Kailing%20Wang%20and%20Chen%20Yang%20and%20Yuehao%20Wang%20and%20Sikuang%20Li%20and%20Yan%20Wang%20and%20Qi%20Dou%20and%20Xiaokang%20Yang%20and%20Wei%20Shen%0AAbstract%3A%20%20%20Precise%20camera%20tracking%2C%20high-fidelity%203D%20tissue%20reconstruction%2C%20and%0Areal-time%20online%20visualization%20are%20critical%20for%20intrabody%20medical%20imaging%0Adevices%20such%20as%20endoscopes%20and%20capsule%20robots.%20However%2C%20existing%20SLAM%0A%28Simultaneous%20Localization%20and%20Mapping%29%20methods%20often%20struggle%20to%20achieve%20both%0Acomplete%20high-quality%20surgical%20field%20reconstruction%20and%20efficient%20computation%2C%0Arestricting%20their%20intraoperative%20applications%20among%20endoscopic%20surgeries.%20In%0Athis%20paper%2C%20we%20introduce%20EndoGSLAM%2C%20an%20efficient%20SLAM%20approach%20for%20endoscopic%0Asurgeries%2C%20which%20integrates%20streamlined%20Gaussian%20representation%20and%0Adifferentiable%20rasterization%20to%20facilitate%20over%20100%20fps%20rendering%20speed%20during%0Aonline%20camera%20tracking%20and%20tissue%20reconstructing.%20Extensive%20experiments%20show%0Athat%20EndoGSLAM%20achieves%20a%20better%20trade-off%20between%20intraoperative%20availability%0Aand%20reconstruction%20quality%20than%20traditional%20or%20neural%20SLAM%20approaches%2C%20showing%0Atremendous%20potential%20for%20endoscopic%20surgeries.%20The%20project%20page%20is%20at%0Ahttps%3A//EndoGSLAM.loping151.com%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15124v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EndoGSLAM%3A%20Real-Time%20Dense%20Reconstruction%20and%20Tracking%20in%20Endoscopic%0A%20%20Surgeries%20using%20Gaussian%20Splatting&entry.906535625=Kailing%20Wang%20and%20Chen%20Yang%20and%20Yuehao%20Wang%20and%20Sikuang%20Li%20and%20Yan%20Wang%20and%20Qi%20Dou%20and%20Xiaokang%20Yang%20and%20Wei%20Shen&entry.1292438233=%20%20Precise%20camera%20tracking%2C%20high-fidelity%203D%20tissue%20reconstruction%2C%20and%0Areal-time%20online%20visualization%20are%20critical%20for%20intrabody%20medical%20imaging%0Adevices%20such%20as%20endoscopes%20and%20capsule%20robots.%20However%2C%20existing%20SLAM%0A%28Simultaneous%20Localization%20and%20Mapping%29%20methods%20often%20struggle%20to%20achieve%20both%0Acomplete%20high-quality%20surgical%20field%20reconstruction%20and%20efficient%20computation%2C%0Arestricting%20their%20intraoperative%20applications%20among%20endoscopic%20surgeries.%20In%0Athis%20paper%2C%20we%20introduce%20EndoGSLAM%2C%20an%20efficient%20SLAM%20approach%20for%20endoscopic%0Asurgeries%2C%20which%20integrates%20streamlined%20Gaussian%20representation%20and%0Adifferentiable%20rasterization%20to%20facilitate%20over%20100%20fps%20rendering%20speed%20during%0Aonline%20camera%20tracking%20and%20tissue%20reconstructing.%20Extensive%20experiments%20show%0Athat%20EndoGSLAM%20achieves%20a%20better%20trade-off%20between%20intraoperative%20availability%0Aand%20reconstruction%20quality%20than%20traditional%20or%20neural%20SLAM%20approaches%2C%20showing%0Atremendous%20potential%20for%20endoscopic%20surgeries.%20The%20project%20page%20is%20at%0Ahttps%3A//EndoGSLAM.loping151.com%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15124v1&entry.124074799=Read"},
{"title": "Shadow Generation for Composite Image Using Diffusion model", "author": "Qingyang Liu and Junqi You and Jianting Wang and Xinhao Tao and Bo Zhang and Li Niu", "abstract": "  In the realm of image composition, generating realistic shadow for the\ninserted foreground remains a formidable challenge. Previous works have\ndeveloped image-to-image translation models which are trained on paired\ntraining data. However, they are struggling to generate shadows with accurate\nshapes and intensities, hindered by data scarcity and inherent task complexity.\nIn this paper, we resort to foundation model with rich prior knowledge of\nnatural shadow images. Specifically, we first adapt ControlNet to our task and\nthen propose intensity modulation modules to improve the shadow intensity.\nMoreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel\ndata acquisition pipeline. Experimental results on both DESOBA and DESOBAv2\ndatasets as well as real composite images demonstrate the superior capability\nof our model for shadow generation task. The dataset, code, and model are\nreleased at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.\n", "link": "http://arxiv.org/abs/2403.15234v1", "date": "2024-03-22", "relevancy": 2.6277, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5355}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5328}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5084}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Shadow%20Generation%20for%20Composite%20Image%20Using%20Diffusion%20model&body=Title%3A%20Shadow%20Generation%20for%20Composite%20Image%20Using%20Diffusion%20model%0AAuthor%3A%20Qingyang%20Liu%20and%20Junqi%20You%20and%20Jianting%20Wang%20and%20Xinhao%20Tao%20and%20Bo%20Zhang%20and%20Li%20Niu%0AAbstract%3A%20%20%20In%20the%20realm%20of%20image%20composition%2C%20generating%20realistic%20shadow%20for%20the%0Ainserted%20foreground%20remains%20a%20formidable%20challenge.%20Previous%20works%20have%0Adeveloped%20image-to-image%20translation%20models%20which%20are%20trained%20on%20paired%0Atraining%20data.%20However%2C%20they%20are%20struggling%20to%20generate%20shadows%20with%20accurate%0Ashapes%20and%20intensities%2C%20hindered%20by%20data%20scarcity%20and%20inherent%20task%20complexity.%0AIn%20this%20paper%2C%20we%20resort%20to%20foundation%20model%20with%20rich%20prior%20knowledge%20of%0Anatural%20shadow%20images.%20Specifically%2C%20we%20first%20adapt%20ControlNet%20to%20our%20task%20and%0Athen%20propose%20intensity%20modulation%20modules%20to%20improve%20the%20shadow%20intensity.%0AMoreover%2C%20we%20extend%20the%20small-scale%20DESOBA%20dataset%20to%20DESOBAv2%20using%20a%20novel%0Adata%20acquisition%20pipeline.%20Experimental%20results%20on%20both%20DESOBA%20and%20DESOBAv2%0Adatasets%20as%20well%20as%20real%20composite%20images%20demonstrate%20the%20superior%20capability%0Aof%20our%20model%20for%20shadow%20generation%20task.%20The%20dataset%2C%20code%2C%20and%20model%20are%0Areleased%20at%20https%3A//github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15234v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shadow%20Generation%20for%20Composite%20Image%20Using%20Diffusion%20model&entry.906535625=Qingyang%20Liu%20and%20Junqi%20You%20and%20Jianting%20Wang%20and%20Xinhao%20Tao%20and%20Bo%20Zhang%20and%20Li%20Niu&entry.1292438233=%20%20In%20the%20realm%20of%20image%20composition%2C%20generating%20realistic%20shadow%20for%20the%0Ainserted%20foreground%20remains%20a%20formidable%20challenge.%20Previous%20works%20have%0Adeveloped%20image-to-image%20translation%20models%20which%20are%20trained%20on%20paired%0Atraining%20data.%20However%2C%20they%20are%20struggling%20to%20generate%20shadows%20with%20accurate%0Ashapes%20and%20intensities%2C%20hindered%20by%20data%20scarcity%20and%20inherent%20task%20complexity.%0AIn%20this%20paper%2C%20we%20resort%20to%20foundation%20model%20with%20rich%20prior%20knowledge%20of%0Anatural%20shadow%20images.%20Specifically%2C%20we%20first%20adapt%20ControlNet%20to%20our%20task%20and%0Athen%20propose%20intensity%20modulation%20modules%20to%20improve%20the%20shadow%20intensity.%0AMoreover%2C%20we%20extend%20the%20small-scale%20DESOBA%20dataset%20to%20DESOBAv2%20using%20a%20novel%0Adata%20acquisition%20pipeline.%20Experimental%20results%20on%20both%20DESOBA%20and%20DESOBAv2%0Adatasets%20as%20well%20as%20real%20composite%20images%20demonstrate%20the%20superior%20capability%0Aof%20our%20model%20for%20shadow%20generation%20task.%20The%20dataset%2C%20code%2C%20and%20model%20are%0Areleased%20at%20https%3A//github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15234v1&entry.124074799=Read"},
{"title": "Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for\n  Weakly Semi-supervised 3D Object Detection", "author": "Hongzhi Gao and Zheng Chen and Zehui Chen and Lin Chen and Jiaming Liu and Shanghang Zhang and Feng Zhao", "abstract": "  Training high-accuracy 3D detectors necessitates massive labeled 3D\nannotations with 7 degree-of-freedom, which is laborious and time-consuming.\nTherefore, the form of point annotations is proposed to offer significant\nprospects for practical applications in 3D detection, which is not only more\naccessible and less expensive but also provides strong spatial information for\nobject localization.In this paper, we empirically discover that it is\nnon-trivial to merely adapt Point-DETR to its 3D form, encountering two main\nbottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it\ngenerates low-quality pseudo labels in distant regions due to the extreme\nsparsity of LiDAR points. To overcome these challenges, we introduce\nPoint-DETR3D, a teacher-student framework for weakly semi-supervised 3D\ndetection, designed to fully capitalize on point-wise supervision within a\nconstrained instance-wise annotation budget.Different from Point-DETR which\nencodes 3D positional information solely through a point encoder, we propose an\nexplicit positional query initialization strategy to enhance the positional\nprior. Considering the low quality of pseudo labels at distant regions produced\nby the teacher model, we enhance the detector's perception by incorporating\ndense imagery data through a novel Cross-Modal Deformable RoI Fusion\n(D-RoI).Moreover, an innovative point-guided self-supervised learning technique\nis proposed to allow for fully exploiting point priors, even in student\nmodels.Extensive experiments on representative nuScenes dataset demonstrate our\nPoint-DETR3D obtains significant improvements compared to previous works.\nNotably, with only 5% of labeled data, Point-DETR3D achieves over 90%\nperformance of its fully supervised counterpart.\n", "link": "http://arxiv.org/abs/2403.15317v1", "date": "2024-03-22", "relevancy": 2.6033, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6972}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.648}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6056}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Point-DETR3D%3A%20Leveraging%20Imagery%20Data%20with%20Spatial%20Point%20Prior%20for%0A%20%20Weakly%20Semi-supervised%203D%20Object%20Detection&body=Title%3A%20Point-DETR3D%3A%20Leveraging%20Imagery%20Data%20with%20Spatial%20Point%20Prior%20for%0A%20%20Weakly%20Semi-supervised%203D%20Object%20Detection%0AAuthor%3A%20Hongzhi%20Gao%20and%20Zheng%20Chen%20and%20Zehui%20Chen%20and%20Lin%20Chen%20and%20Jiaming%20Liu%20and%20Shanghang%20Zhang%20and%20Feng%20Zhao%0AAbstract%3A%20%20%20Training%20high-accuracy%203D%20detectors%20necessitates%20massive%20labeled%203D%0Aannotations%20with%207%20degree-of-freedom%2C%20which%20is%20laborious%20and%20time-consuming.%0ATherefore%2C%20the%20form%20of%20point%20annotations%20is%20proposed%20to%20offer%20significant%0Aprospects%20for%20practical%20applications%20in%203D%20detection%2C%20which%20is%20not%20only%20more%0Aaccessible%20and%20less%20expensive%20but%20also%20provides%20strong%20spatial%20information%20for%0Aobject%20localization.In%20this%20paper%2C%20we%20empirically%20discover%20that%20it%20is%0Anon-trivial%20to%20merely%20adapt%20Point-DETR%20to%20its%203D%20form%2C%20encountering%20two%20main%0Abottlenecks%3A%201%29%20it%20fails%20to%20encode%20strong%203D%20prior%20into%20the%20model%2C%20and%202%29%20it%0Agenerates%20low-quality%20pseudo%20labels%20in%20distant%20regions%20due%20to%20the%20extreme%0Asparsity%20of%20LiDAR%20points.%20To%20overcome%20these%20challenges%2C%20we%20introduce%0APoint-DETR3D%2C%20a%20teacher-student%20framework%20for%20weakly%20semi-supervised%203D%0Adetection%2C%20designed%20to%20fully%20capitalize%20on%20point-wise%20supervision%20within%20a%0Aconstrained%20instance-wise%20annotation%20budget.Different%20from%20Point-DETR%20which%0Aencodes%203D%20positional%20information%20solely%20through%20a%20point%20encoder%2C%20we%20propose%20an%0Aexplicit%20positional%20query%20initialization%20strategy%20to%20enhance%20the%20positional%0Aprior.%20Considering%20the%20low%20quality%20of%20pseudo%20labels%20at%20distant%20regions%20produced%0Aby%20the%20teacher%20model%2C%20we%20enhance%20the%20detector%27s%20perception%20by%20incorporating%0Adense%20imagery%20data%20through%20a%20novel%20Cross-Modal%20Deformable%20RoI%20Fusion%0A%28D-RoI%29.Moreover%2C%20an%20innovative%20point-guided%20self-supervised%20learning%20technique%0Ais%20proposed%20to%20allow%20for%20fully%20exploiting%20point%20priors%2C%20even%20in%20student%0Amodels.Extensive%20experiments%20on%20representative%20nuScenes%20dataset%20demonstrate%20our%0APoint-DETR3D%20obtains%20significant%20improvements%20compared%20to%20previous%20works.%0ANotably%2C%20with%20only%205%25%20of%20labeled%20data%2C%20Point-DETR3D%20achieves%20over%2090%25%0Aperformance%20of%20its%20fully%20supervised%20counterpart.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15317v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-DETR3D%3A%20Leveraging%20Imagery%20Data%20with%20Spatial%20Point%20Prior%20for%0A%20%20Weakly%20Semi-supervised%203D%20Object%20Detection&entry.906535625=Hongzhi%20Gao%20and%20Zheng%20Chen%20and%20Zehui%20Chen%20and%20Lin%20Chen%20and%20Jiaming%20Liu%20and%20Shanghang%20Zhang%20and%20Feng%20Zhao&entry.1292438233=%20%20Training%20high-accuracy%203D%20detectors%20necessitates%20massive%20labeled%203D%0Aannotations%20with%207%20degree-of-freedom%2C%20which%20is%20laborious%20and%20time-consuming.%0ATherefore%2C%20the%20form%20of%20point%20annotations%20is%20proposed%20to%20offer%20significant%0Aprospects%20for%20practical%20applications%20in%203D%20detection%2C%20which%20is%20not%20only%20more%0Aaccessible%20and%20less%20expensive%20but%20also%20provides%20strong%20spatial%20information%20for%0Aobject%20localization.In%20this%20paper%2C%20we%20empirically%20discover%20that%20it%20is%0Anon-trivial%20to%20merely%20adapt%20Point-DETR%20to%20its%203D%20form%2C%20encountering%20two%20main%0Abottlenecks%3A%201%29%20it%20fails%20to%20encode%20strong%203D%20prior%20into%20the%20model%2C%20and%202%29%20it%0Agenerates%20low-quality%20pseudo%20labels%20in%20distant%20regions%20due%20to%20the%20extreme%0Asparsity%20of%20LiDAR%20points.%20To%20overcome%20these%20challenges%2C%20we%20introduce%0APoint-DETR3D%2C%20a%20teacher-student%20framework%20for%20weakly%20semi-supervised%203D%0Adetection%2C%20designed%20to%20fully%20capitalize%20on%20point-wise%20supervision%20within%20a%0Aconstrained%20instance-wise%20annotation%20budget.Different%20from%20Point-DETR%20which%0Aencodes%203D%20positional%20information%20solely%20through%20a%20point%20encoder%2C%20we%20propose%20an%0Aexplicit%20positional%20query%20initialization%20strategy%20to%20enhance%20the%20positional%0Aprior.%20Considering%20the%20low%20quality%20of%20pseudo%20labels%20at%20distant%20regions%20produced%0Aby%20the%20teacher%20model%2C%20we%20enhance%20the%20detector%27s%20perception%20by%20incorporating%0Adense%20imagery%20data%20through%20a%20novel%20Cross-Modal%20Deformable%20RoI%20Fusion%0A%28D-RoI%29.Moreover%2C%20an%20innovative%20point-guided%20self-supervised%20learning%20technique%0Ais%20proposed%20to%20allow%20for%20fully%20exploiting%20point%20priors%2C%20even%20in%20student%0Amodels.Extensive%20experiments%20on%20representative%20nuScenes%20dataset%20demonstrate%20our%0APoint-DETR3D%20obtains%20significant%20improvements%20compared%20to%20previous%20works.%0ANotably%2C%20with%20only%205%25%20of%20labeled%20data%2C%20Point-DETR3D%20achieves%20over%2090%25%0Aperformance%20of%20its%20fully%20supervised%20counterpart.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15317v1&entry.124074799=Read"},
{"title": "Global Control for Local SO(3)-Equivariant Scale-Invariant Vessel\n  Segmentation", "author": "Patryk Rygiel and Dieuwertje Alblas and Christoph Brune and Kak Khee Yeung and Jelmer M. Wolterink", "abstract": "  Personalized 3D vascular models can aid in a range of diagnostic, prognostic,\nand treatment-planning tasks relevant to cardiovascular disease management.\nDeep learning provides a means to automatically obtain such models. Ideally, a\nuser should have control over the exact region of interest (ROI) to be included\nin a vascular model, and the model should be watertight and highly accurate. To\nthis end, we propose a combination of a global controller leveraging voxel mask\nsegmentations to provide boundary conditions for vessels of interest to a\nlocal, iterative vessel segmentation model. We introduce the preservation of\nscale- and rotational symmetries in the local segmentation model, leading to\ngeneralisation to vessels of unseen sizes and orientations. Combined with the\nglobal controller, this enables flexible 3D vascular model building, without\nadditional retraining. We demonstrate the potential of our method on a dataset\ncontaining abdominal aortic aneurysms (AAAs). Our method performs on par with a\nstate-of-the-art segmentation model in the segmentation of AAAs, iliac arteries\nand renal arteries, while providing a watertight, smooth surface segmentation.\nMoreover, we demonstrate that by adapting the global controller, we can easily\nextend vessel sections in the 3D model.\n", "link": "http://arxiv.org/abs/2403.15314v1", "date": "2024-03-22", "relevancy": 2.5679, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5013}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Global%20Control%20for%20Local%20SO%283%29-Equivariant%20Scale-Invariant%20Vessel%0A%20%20Segmentation&body=Title%3A%20Global%20Control%20for%20Local%20SO%283%29-Equivariant%20Scale-Invariant%20Vessel%0A%20%20Segmentation%0AAuthor%3A%20Patryk%20Rygiel%20and%20Dieuwertje%20Alblas%20and%20Christoph%20Brune%20and%20Kak%20Khee%20Yeung%20and%20Jelmer%20M.%20Wolterink%0AAbstract%3A%20%20%20Personalized%203D%20vascular%20models%20can%20aid%20in%20a%20range%20of%20diagnostic%2C%20prognostic%2C%0Aand%20treatment-planning%20tasks%20relevant%20to%20cardiovascular%20disease%20management.%0ADeep%20learning%20provides%20a%20means%20to%20automatically%20obtain%20such%20models.%20Ideally%2C%20a%0Auser%20should%20have%20control%20over%20the%20exact%20region%20of%20interest%20%28ROI%29%20to%20be%20included%0Ain%20a%20vascular%20model%2C%20and%20the%20model%20should%20be%20watertight%20and%20highly%20accurate.%20To%0Athis%20end%2C%20we%20propose%20a%20combination%20of%20a%20global%20controller%20leveraging%20voxel%20mask%0Asegmentations%20to%20provide%20boundary%20conditions%20for%20vessels%20of%20interest%20to%20a%0Alocal%2C%20iterative%20vessel%20segmentation%20model.%20We%20introduce%20the%20preservation%20of%0Ascale-%20and%20rotational%20symmetries%20in%20the%20local%20segmentation%20model%2C%20leading%20to%0Ageneralisation%20to%20vessels%20of%20unseen%20sizes%20and%20orientations.%20Combined%20with%20the%0Aglobal%20controller%2C%20this%20enables%20flexible%203D%20vascular%20model%20building%2C%20without%0Aadditional%20retraining.%20We%20demonstrate%20the%20potential%20of%20our%20method%20on%20a%20dataset%0Acontaining%20abdominal%20aortic%20aneurysms%20%28AAAs%29.%20Our%20method%20performs%20on%20par%20with%20a%0Astate-of-the-art%20segmentation%20model%20in%20the%20segmentation%20of%20AAAs%2C%20iliac%20arteries%0Aand%20renal%20arteries%2C%20while%20providing%20a%20watertight%2C%20smooth%20surface%20segmentation.%0AMoreover%2C%20we%20demonstrate%20that%20by%20adapting%20the%20global%20controller%2C%20we%20can%20easily%0Aextend%20vessel%20sections%20in%20the%203D%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15314v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Control%20for%20Local%20SO%283%29-Equivariant%20Scale-Invariant%20Vessel%0A%20%20Segmentation&entry.906535625=Patryk%20Rygiel%20and%20Dieuwertje%20Alblas%20and%20Christoph%20Brune%20and%20Kak%20Khee%20Yeung%20and%20Jelmer%20M.%20Wolterink&entry.1292438233=%20%20Personalized%203D%20vascular%20models%20can%20aid%20in%20a%20range%20of%20diagnostic%2C%20prognostic%2C%0Aand%20treatment-planning%20tasks%20relevant%20to%20cardiovascular%20disease%20management.%0ADeep%20learning%20provides%20a%20means%20to%20automatically%20obtain%20such%20models.%20Ideally%2C%20a%0Auser%20should%20have%20control%20over%20the%20exact%20region%20of%20interest%20%28ROI%29%20to%20be%20included%0Ain%20a%20vascular%20model%2C%20and%20the%20model%20should%20be%20watertight%20and%20highly%20accurate.%20To%0Athis%20end%2C%20we%20propose%20a%20combination%20of%20a%20global%20controller%20leveraging%20voxel%20mask%0Asegmentations%20to%20provide%20boundary%20conditions%20for%20vessels%20of%20interest%20to%20a%0Alocal%2C%20iterative%20vessel%20segmentation%20model.%20We%20introduce%20the%20preservation%20of%0Ascale-%20and%20rotational%20symmetries%20in%20the%20local%20segmentation%20model%2C%20leading%20to%0Ageneralisation%20to%20vessels%20of%20unseen%20sizes%20and%20orientations.%20Combined%20with%20the%0Aglobal%20controller%2C%20this%20enables%20flexible%203D%20vascular%20model%20building%2C%20without%0Aadditional%20retraining.%20We%20demonstrate%20the%20potential%20of%20our%20method%20on%20a%20dataset%0Acontaining%20abdominal%20aortic%20aneurysms%20%28AAAs%29.%20Our%20method%20performs%20on%20par%20with%20a%0Astate-of-the-art%20segmentation%20model%20in%20the%20segmentation%20of%20AAAs%2C%20iliac%20arteries%0Aand%20renal%20arteries%2C%20while%20providing%20a%20watertight%2C%20smooth%20surface%20segmentation.%0AMoreover%2C%20we%20demonstrate%20that%20by%20adapting%20the%20global%20controller%2C%20we%20can%20easily%0Aextend%20vessel%20sections%20in%20the%203D%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15314v1&entry.124074799=Read"},
{"title": "GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks", "author": "Sukhdeep Singh and Anuj Sharma and Vinod Kumar Chauhan", "abstract": "  Graph Neural Networks (GNN) have emerged as a popular and standard approach\nfor learning from graph-structured data. The literature on GNN highlights the\npotential of this evolving research area and its widespread adoption in\nreal-life applications. However, most of the approaches are either new in\nconcept or derived from specific techniques. Therefore, the potential of more\nthan one approach in hybrid form has not been studied extensively, which can be\nwell utilized for sequenced data or static data together. We derive a hybrid\napproach based on two established techniques as generalized aggregation\nnetworks and topology adaptive graph convolution networks that solve our\npurpose to apply on both types of sequenced and static nature of data,\neffectively. The proposed method applies to both node and graph classification.\nOur empirical analysis reveals that the results are at par with literature\nresults and better for handwritten strokes as sequenced data, where graph\nstructures have not been explored.\n", "link": "http://arxiv.org/abs/2403.15077v1", "date": "2024-03-22", "relevancy": 2.5652, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5242}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5149}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GTAGCN%3A%20Generalized%20Topology%20Adaptive%20Graph%20Convolutional%20Networks&body=Title%3A%20GTAGCN%3A%20Generalized%20Topology%20Adaptive%20Graph%20Convolutional%20Networks%0AAuthor%3A%20Sukhdeep%20Singh%20and%20Anuj%20Sharma%20and%20Vinod%20Kumar%20Chauhan%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNN%29%20have%20emerged%20as%20a%20popular%20and%20standard%20approach%0Afor%20learning%20from%20graph-structured%20data.%20The%20literature%20on%20GNN%20highlights%20the%0Apotential%20of%20this%20evolving%20research%20area%20and%20its%20widespread%20adoption%20in%0Areal-life%20applications.%20However%2C%20most%20of%20the%20approaches%20are%20either%20new%20in%0Aconcept%20or%20derived%20from%20specific%20techniques.%20Therefore%2C%20the%20potential%20of%20more%0Athan%20one%20approach%20in%20hybrid%20form%20has%20not%20been%20studied%20extensively%2C%20which%20can%20be%0Awell%20utilized%20for%20sequenced%20data%20or%20static%20data%20together.%20We%20derive%20a%20hybrid%0Aapproach%20based%20on%20two%20established%20techniques%20as%20generalized%20aggregation%0Anetworks%20and%20topology%20adaptive%20graph%20convolution%20networks%20that%20solve%20our%0Apurpose%20to%20apply%20on%20both%20types%20of%20sequenced%20and%20static%20nature%20of%20data%2C%0Aeffectively.%20The%20proposed%20method%20applies%20to%20both%20node%20and%20graph%20classification.%0AOur%20empirical%20analysis%20reveals%20that%20the%20results%20are%20at%20par%20with%20literature%0Aresults%20and%20better%20for%20handwritten%20strokes%20as%20sequenced%20data%2C%20where%20graph%0Astructures%20have%20not%20been%20explored.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15077v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTAGCN%3A%20Generalized%20Topology%20Adaptive%20Graph%20Convolutional%20Networks&entry.906535625=Sukhdeep%20Singh%20and%20Anuj%20Sharma%20and%20Vinod%20Kumar%20Chauhan&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNN%29%20have%20emerged%20as%20a%20popular%20and%20standard%20approach%0Afor%20learning%20from%20graph-structured%20data.%20The%20literature%20on%20GNN%20highlights%20the%0Apotential%20of%20this%20evolving%20research%20area%20and%20its%20widespread%20adoption%20in%0Areal-life%20applications.%20However%2C%20most%20of%20the%20approaches%20are%20either%20new%20in%0Aconcept%20or%20derived%20from%20specific%20techniques.%20Therefore%2C%20the%20potential%20of%20more%0Athan%20one%20approach%20in%20hybrid%20form%20has%20not%20been%20studied%20extensively%2C%20which%20can%20be%0Awell%20utilized%20for%20sequenced%20data%20or%20static%20data%20together.%20We%20derive%20a%20hybrid%0Aapproach%20based%20on%20two%20established%20techniques%20as%20generalized%20aggregation%0Anetworks%20and%20topology%20adaptive%20graph%20convolution%20networks%20that%20solve%20our%0Apurpose%20to%20apply%20on%20both%20types%20of%20sequenced%20and%20static%20nature%20of%20data%2C%0Aeffectively.%20The%20proposed%20method%20applies%20to%20both%20node%20and%20graph%20classification.%0AOur%20empirical%20analysis%20reveals%20that%20the%20results%20are%20at%20par%20with%20literature%0Aresults%20and%20better%20for%20handwritten%20strokes%20as%20sequenced%20data%2C%20where%20graph%0Astructures%20have%20not%20been%20explored.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15077v1&entry.124074799=Read"},
{"title": "S2DM: Sector-Shaped Diffusion Models for Video Generation", "author": "Haoran Lang and Yuxuan Ge and Zheng Tian", "abstract": "  Diffusion models have achieved great success in image generation. However,\nwhen leveraging this idea for video generation, we face significant challenges\nin maintaining the consistency and continuity across video frames. This is\nmainly caused by the lack of an effective framework to align frames of videos\nwith desired temporal features while preserving consistent semantic and\nstochastic features. In this work, we propose a novel Sector-Shaped Diffusion\nModel (S2DM) whose sector-shaped diffusion region is formed by a set of\nray-shaped reverse diffusion processes starting at the same noise point. S2DM\ncan generate a group of intrinsically related data sharing the same semantic\nand stochastic features while varying on temporal features with appropriate\nguided conditions. We apply S2DM to video generation tasks, and explore the use\nof optical flow as temporal conditions. Our experimental results show that S2DM\noutperforms many existing methods in the task of video generation without any\ntemporal-feature modelling modules. For text-to-video generation tasks where\ntemporal conditions are not explicitly given, we propose a two-stage generation\nstrategy which can decouple the generation of temporal features from\nsemantic-content features. We show that, without additional training, our model\nintegrated with another temporal conditions generative model can still achieve\ncomparable performance with existing works. Our results can be viewd at\nhttps://s2dm.github.io/S2DM/.\n", "link": "http://arxiv.org/abs/2403.13408v2", "date": "2024-03-22", "relevancy": 2.5605, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6713}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6329}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6118}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20S2DM%3A%20Sector-Shaped%20Diffusion%20Models%20for%20Video%20Generation&body=Title%3A%20S2DM%3A%20Sector-Shaped%20Diffusion%20Models%20for%20Video%20Generation%0AAuthor%3A%20Haoran%20Lang%20and%20Yuxuan%20Ge%20and%20Zheng%20Tian%0AAbstract%3A%20%20%20Diffusion%20models%20have%20achieved%20great%20success%20in%20image%20generation.%20However%2C%0Awhen%20leveraging%20this%20idea%20for%20video%20generation%2C%20we%20face%20significant%20challenges%0Ain%20maintaining%20the%20consistency%20and%20continuity%20across%20video%20frames.%20This%20is%0Amainly%20caused%20by%20the%20lack%20of%20an%20effective%20framework%20to%20align%20frames%20of%20videos%0Awith%20desired%20temporal%20features%20while%20preserving%20consistent%20semantic%20and%0Astochastic%20features.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Sector-Shaped%20Diffusion%0AModel%20%28S2DM%29%20whose%20sector-shaped%20diffusion%20region%20is%20formed%20by%20a%20set%20of%0Aray-shaped%20reverse%20diffusion%20processes%20starting%20at%20the%20same%20noise%20point.%20S2DM%0Acan%20generate%20a%20group%20of%20intrinsically%20related%20data%20sharing%20the%20same%20semantic%0Aand%20stochastic%20features%20while%20varying%20on%20temporal%20features%20with%20appropriate%0Aguided%20conditions.%20We%20apply%20S2DM%20to%20video%20generation%20tasks%2C%20and%20explore%20the%20use%0Aof%20optical%20flow%20as%20temporal%20conditions.%20Our%20experimental%20results%20show%20that%20S2DM%0Aoutperforms%20many%20existing%20methods%20in%20the%20task%20of%20video%20generation%20without%20any%0Atemporal-feature%20modelling%20modules.%20For%20text-to-video%20generation%20tasks%20where%0Atemporal%20conditions%20are%20not%20explicitly%20given%2C%20we%20propose%20a%20two-stage%20generation%0Astrategy%20which%20can%20decouple%20the%20generation%20of%20temporal%20features%20from%0Asemantic-content%20features.%20We%20show%20that%2C%20without%20additional%20training%2C%20our%20model%0Aintegrated%20with%20another%20temporal%20conditions%20generative%20model%20can%20still%20achieve%0Acomparable%20performance%20with%20existing%20works.%20Our%20results%20can%20be%20viewd%20at%0Ahttps%3A//s2dm.github.io/S2DM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13408v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2DM%3A%20Sector-Shaped%20Diffusion%20Models%20for%20Video%20Generation&entry.906535625=Haoran%20Lang%20and%20Yuxuan%20Ge%20and%20Zheng%20Tian&entry.1292438233=%20%20Diffusion%20models%20have%20achieved%20great%20success%20in%20image%20generation.%20However%2C%0Awhen%20leveraging%20this%20idea%20for%20video%20generation%2C%20we%20face%20significant%20challenges%0Ain%20maintaining%20the%20consistency%20and%20continuity%20across%20video%20frames.%20This%20is%0Amainly%20caused%20by%20the%20lack%20of%20an%20effective%20framework%20to%20align%20frames%20of%20videos%0Awith%20desired%20temporal%20features%20while%20preserving%20consistent%20semantic%20and%0Astochastic%20features.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Sector-Shaped%20Diffusion%0AModel%20%28S2DM%29%20whose%20sector-shaped%20diffusion%20region%20is%20formed%20by%20a%20set%20of%0Aray-shaped%20reverse%20diffusion%20processes%20starting%20at%20the%20same%20noise%20point.%20S2DM%0Acan%20generate%20a%20group%20of%20intrinsically%20related%20data%20sharing%20the%20same%20semantic%0Aand%20stochastic%20features%20while%20varying%20on%20temporal%20features%20with%20appropriate%0Aguided%20conditions.%20We%20apply%20S2DM%20to%20video%20generation%20tasks%2C%20and%20explore%20the%20use%0Aof%20optical%20flow%20as%20temporal%20conditions.%20Our%20experimental%20results%20show%20that%20S2DM%0Aoutperforms%20many%20existing%20methods%20in%20the%20task%20of%20video%20generation%20without%20any%0Atemporal-feature%20modelling%20modules.%20For%20text-to-video%20generation%20tasks%20where%0Atemporal%20conditions%20are%20not%20explicitly%20given%2C%20we%20propose%20a%20two-stage%20generation%0Astrategy%20which%20can%20decouple%20the%20generation%20of%20temporal%20features%20from%0Asemantic-content%20features.%20We%20show%20that%2C%20without%20additional%20training%2C%20our%20model%0Aintegrated%20with%20another%20temporal%20conditions%20generative%20model%20can%20still%20achieve%0Acomparable%20performance%20with%20existing%20works.%20Our%20results%20can%20be%20viewd%20at%0Ahttps%3A//s2dm.github.io/S2DM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13408v2&entry.124074799=Read"},
{"title": "Toulouse Hyperspectral Data Set: a benchmark data set to assess\n  semi-supervised spectral representation learning and pixel-wise\n  classification techniques", "author": "Romain Thoreau and Laurent Risser and V\u00e9ronique Achard and B\u00e9atrice Berthelot and Xavier Briottet", "abstract": "  Airborne hyperspectral images can be used to map the land cover in large\nurban areas, thanks to their very high spatial and spectral resolutions on a\nwide spectral domain. While the spectral dimension of hyperspectral images is\nhighly informative of the chemical composition of the land surface, the use of\nstate-of-the-art machine learning algorithms to map the land cover has been\ndramatically limited by the availability of training data. To cope with the\nscarcity of annotations, semi-supervised and self-supervised techniques have\nlately raised a lot of interest in the community. Yet, the publicly available\nhyperspectral data sets commonly used to benchmark machine learning models are\nnot totally suited to evaluate their generalization performances due to one or\nseveral of the following properties: a limited geographical coverage (which\ndoes not reflect the spectral diversity in metropolitan areas), a small number\nof land cover classes and a lack of appropriate standard train / test splits\nfor semi-supervised and self-supervised learning. Therefore, we release in this\npaper the Toulouse Hyperspectral Data Set that stands out from other data sets\nin the above-mentioned respects in order to meet key issues in spectral\nrepresentation learning and classification over large-scale hyperspectral\nimages with very few labeled pixels. Besides, we discuss and experiment\nself-supervised techniques for spectral representation learning, including the\nMasked Autoencoder, and establish a baseline for pixel-wise classification\nachieving 85% overall accuracy and 77% F1 score. The Toulouse Hyperspectral\nData Set and our code are publicly available at\nhttps://www.toulouse-hyperspectral-data-set.com and\nhttps://www.github.com/Romain3Ch216/tlse-experiments, respectively.\n", "link": "http://arxiv.org/abs/2311.08863v2", "date": "2024-03-22", "relevancy": 2.5496, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5335}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5177}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4786}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Toulouse%20Hyperspectral%20Data%20Set%3A%20a%20benchmark%20data%20set%20to%20assess%0A%20%20semi-supervised%20spectral%20representation%20learning%20and%20pixel-wise%0A%20%20classification%20techniques&body=Title%3A%20Toulouse%20Hyperspectral%20Data%20Set%3A%20a%20benchmark%20data%20set%20to%20assess%0A%20%20semi-supervised%20spectral%20representation%20learning%20and%20pixel-wise%0A%20%20classification%20techniques%0AAuthor%3A%20Romain%20Thoreau%20and%20Laurent%20Risser%20and%20V%C3%A9ronique%20Achard%20and%20B%C3%A9atrice%20Berthelot%20and%20Xavier%20Briottet%0AAbstract%3A%20%20%20Airborne%20hyperspectral%20images%20can%20be%20used%20to%20map%20the%20land%20cover%20in%20large%0Aurban%20areas%2C%20thanks%20to%20their%20very%20high%20spatial%20and%20spectral%20resolutions%20on%20a%0Awide%20spectral%20domain.%20While%20the%20spectral%20dimension%20of%20hyperspectral%20images%20is%0Ahighly%20informative%20of%20the%20chemical%20composition%20of%20the%20land%20surface%2C%20the%20use%20of%0Astate-of-the-art%20machine%20learning%20algorithms%20to%20map%20the%20land%20cover%20has%20been%0Adramatically%20limited%20by%20the%20availability%20of%20training%20data.%20To%20cope%20with%20the%0Ascarcity%20of%20annotations%2C%20semi-supervised%20and%20self-supervised%20techniques%20have%0Alately%20raised%20a%20lot%20of%20interest%20in%20the%20community.%20Yet%2C%20the%20publicly%20available%0Ahyperspectral%20data%20sets%20commonly%20used%20to%20benchmark%20machine%20learning%20models%20are%0Anot%20totally%20suited%20to%20evaluate%20their%20generalization%20performances%20due%20to%20one%20or%0Aseveral%20of%20the%20following%20properties%3A%20a%20limited%20geographical%20coverage%20%28which%0Adoes%20not%20reflect%20the%20spectral%20diversity%20in%20metropolitan%20areas%29%2C%20a%20small%20number%0Aof%20land%20cover%20classes%20and%20a%20lack%20of%20appropriate%20standard%20train%20/%20test%20splits%0Afor%20semi-supervised%20and%20self-supervised%20learning.%20Therefore%2C%20we%20release%20in%20this%0Apaper%20the%20Toulouse%20Hyperspectral%20Data%20Set%20that%20stands%20out%20from%20other%20data%20sets%0Ain%20the%20above-mentioned%20respects%20in%20order%20to%20meet%20key%20issues%20in%20spectral%0Arepresentation%20learning%20and%20classification%20over%20large-scale%20hyperspectral%0Aimages%20with%20very%20few%20labeled%20pixels.%20Besides%2C%20we%20discuss%20and%20experiment%0Aself-supervised%20techniques%20for%20spectral%20representation%20learning%2C%20including%20the%0AMasked%20Autoencoder%2C%20and%20establish%20a%20baseline%20for%20pixel-wise%20classification%0Aachieving%2085%25%20overall%20accuracy%20and%2077%25%20F1%20score.%20The%20Toulouse%20Hyperspectral%0AData%20Set%20and%20our%20code%20are%20publicly%20available%20at%0Ahttps%3A//www.toulouse-hyperspectral-data-set.com%20and%0Ahttps%3A//www.github.com/Romain3Ch216/tlse-experiments%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08863v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toulouse%20Hyperspectral%20Data%20Set%3A%20a%20benchmark%20data%20set%20to%20assess%0A%20%20semi-supervised%20spectral%20representation%20learning%20and%20pixel-wise%0A%20%20classification%20techniques&entry.906535625=Romain%20Thoreau%20and%20Laurent%20Risser%20and%20V%C3%A9ronique%20Achard%20and%20B%C3%A9atrice%20Berthelot%20and%20Xavier%20Briottet&entry.1292438233=%20%20Airborne%20hyperspectral%20images%20can%20be%20used%20to%20map%20the%20land%20cover%20in%20large%0Aurban%20areas%2C%20thanks%20to%20their%20very%20high%20spatial%20and%20spectral%20resolutions%20on%20a%0Awide%20spectral%20domain.%20While%20the%20spectral%20dimension%20of%20hyperspectral%20images%20is%0Ahighly%20informative%20of%20the%20chemical%20composition%20of%20the%20land%20surface%2C%20the%20use%20of%0Astate-of-the-art%20machine%20learning%20algorithms%20to%20map%20the%20land%20cover%20has%20been%0Adramatically%20limited%20by%20the%20availability%20of%20training%20data.%20To%20cope%20with%20the%0Ascarcity%20of%20annotations%2C%20semi-supervised%20and%20self-supervised%20techniques%20have%0Alately%20raised%20a%20lot%20of%20interest%20in%20the%20community.%20Yet%2C%20the%20publicly%20available%0Ahyperspectral%20data%20sets%20commonly%20used%20to%20benchmark%20machine%20learning%20models%20are%0Anot%20totally%20suited%20to%20evaluate%20their%20generalization%20performances%20due%20to%20one%20or%0Aseveral%20of%20the%20following%20properties%3A%20a%20limited%20geographical%20coverage%20%28which%0Adoes%20not%20reflect%20the%20spectral%20diversity%20in%20metropolitan%20areas%29%2C%20a%20small%20number%0Aof%20land%20cover%20classes%20and%20a%20lack%20of%20appropriate%20standard%20train%20/%20test%20splits%0Afor%20semi-supervised%20and%20self-supervised%20learning.%20Therefore%2C%20we%20release%20in%20this%0Apaper%20the%20Toulouse%20Hyperspectral%20Data%20Set%20that%20stands%20out%20from%20other%20data%20sets%0Ain%20the%20above-mentioned%20respects%20in%20order%20to%20meet%20key%20issues%20in%20spectral%0Arepresentation%20learning%20and%20classification%20over%20large-scale%20hyperspectral%0Aimages%20with%20very%20few%20labeled%20pixels.%20Besides%2C%20we%20discuss%20and%20experiment%0Aself-supervised%20techniques%20for%20spectral%20representation%20learning%2C%20including%20the%0AMasked%20Autoencoder%2C%20and%20establish%20a%20baseline%20for%20pixel-wise%20classification%0Aachieving%2085%25%20overall%20accuracy%20and%2077%25%20F1%20score.%20The%20Toulouse%20Hyperspectral%0AData%20Set%20and%20our%20code%20are%20publicly%20available%20at%0Ahttps%3A//www.toulouse-hyperspectral-data-set.com%20and%0Ahttps%3A//www.github.com/Romain3Ch216/tlse-experiments%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08863v2&entry.124074799=Read"},
{"title": "IFSENet : Harnessing Sparse Iterations for Interactive Few-shot\n  Segmentation Excellence", "author": "Shreyas Chandgothia and Ardhendu Sekhar and Amit Sethi", "abstract": "  Training a computer vision system to segment a novel class typically requires\ncollecting and painstakingly annotating lots of images with objects from that\nclass. Few-shot segmentation techniques reduce the required number of images to\nlearn to segment a new class, but careful annotations of object boundaries are\nstill required. On the other hand, interactive segmentation techniques only\nfocus on incrementally improving the segmentation of one object at a time\n(typically, using clicks given by an expert) in a class-agnostic manner. We\ncombine the two concepts to drastically reduce the effort required to train\nsegmentation models for novel classes. Instead of trivially feeding interactive\nsegmentation masks as ground truth to a few-shot segmentation model, we propose\nIFSENet, which can accept sparse supervision on a single or few support images\nin the form of clicks to generate masks on support (training, at least clicked\nupon once) as well as query (test, never clicked upon) images. To trade-off\neffort for accuracy flexibly, the number of images and clicks can be\nincrementally added to the support set to further improve the segmentation of\nsupport as well as query images. The proposed model approaches the accuracy of\nprevious state-of-the-art few-shot segmentation models with considerably lower\nannotation effort (clicks instead of maps), when tested on Pascal and SBD\ndatasets on query images. It also works well as an interactive segmentation\nmethod on support images.\n", "link": "http://arxiv.org/abs/2403.15089v1", "date": "2024-03-22", "relevancy": 2.5489, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5426}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4975}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4892}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IFSENet%20%3A%20Harnessing%20Sparse%20Iterations%20for%20Interactive%20Few-shot%0A%20%20Segmentation%20Excellence&body=Title%3A%20IFSENet%20%3A%20Harnessing%20Sparse%20Iterations%20for%20Interactive%20Few-shot%0A%20%20Segmentation%20Excellence%0AAuthor%3A%20Shreyas%20Chandgothia%20and%20Ardhendu%20Sekhar%20and%20Amit%20Sethi%0AAbstract%3A%20%20%20Training%20a%20computer%20vision%20system%20to%20segment%20a%20novel%20class%20typically%20requires%0Acollecting%20and%20painstakingly%20annotating%20lots%20of%20images%20with%20objects%20from%20that%0Aclass.%20Few-shot%20segmentation%20techniques%20reduce%20the%20required%20number%20of%20images%20to%0Alearn%20to%20segment%20a%20new%20class%2C%20but%20careful%20annotations%20of%20object%20boundaries%20are%0Astill%20required.%20On%20the%20other%20hand%2C%20interactive%20segmentation%20techniques%20only%0Afocus%20on%20incrementally%20improving%20the%20segmentation%20of%20one%20object%20at%20a%20time%0A%28typically%2C%20using%20clicks%20given%20by%20an%20expert%29%20in%20a%20class-agnostic%20manner.%20We%0Acombine%20the%20two%20concepts%20to%20drastically%20reduce%20the%20effort%20required%20to%20train%0Asegmentation%20models%20for%20novel%20classes.%20Instead%20of%20trivially%20feeding%20interactive%0Asegmentation%20masks%20as%20ground%20truth%20to%20a%20few-shot%20segmentation%20model%2C%20we%20propose%0AIFSENet%2C%20which%20can%20accept%20sparse%20supervision%20on%20a%20single%20or%20few%20support%20images%0Ain%20the%20form%20of%20clicks%20to%20generate%20masks%20on%20support%20%28training%2C%20at%20least%20clicked%0Aupon%20once%29%20as%20well%20as%20query%20%28test%2C%20never%20clicked%20upon%29%20images.%20To%20trade-off%0Aeffort%20for%20accuracy%20flexibly%2C%20the%20number%20of%20images%20and%20clicks%20can%20be%0Aincrementally%20added%20to%20the%20support%20set%20to%20further%20improve%20the%20segmentation%20of%0Asupport%20as%20well%20as%20query%20images.%20The%20proposed%20model%20approaches%20the%20accuracy%20of%0Aprevious%20state-of-the-art%20few-shot%20segmentation%20models%20with%20considerably%20lower%0Aannotation%20effort%20%28clicks%20instead%20of%20maps%29%2C%20when%20tested%20on%20Pascal%20and%20SBD%0Adatasets%20on%20query%20images.%20It%20also%20works%20well%20as%20an%20interactive%20segmentation%0Amethod%20on%20support%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15089v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IFSENet%20%3A%20Harnessing%20Sparse%20Iterations%20for%20Interactive%20Few-shot%0A%20%20Segmentation%20Excellence&entry.906535625=Shreyas%20Chandgothia%20and%20Ardhendu%20Sekhar%20and%20Amit%20Sethi&entry.1292438233=%20%20Training%20a%20computer%20vision%20system%20to%20segment%20a%20novel%20class%20typically%20requires%0Acollecting%20and%20painstakingly%20annotating%20lots%20of%20images%20with%20objects%20from%20that%0Aclass.%20Few-shot%20segmentation%20techniques%20reduce%20the%20required%20number%20of%20images%20to%0Alearn%20to%20segment%20a%20new%20class%2C%20but%20careful%20annotations%20of%20object%20boundaries%20are%0Astill%20required.%20On%20the%20other%20hand%2C%20interactive%20segmentation%20techniques%20only%0Afocus%20on%20incrementally%20improving%20the%20segmentation%20of%20one%20object%20at%20a%20time%0A%28typically%2C%20using%20clicks%20given%20by%20an%20expert%29%20in%20a%20class-agnostic%20manner.%20We%0Acombine%20the%20two%20concepts%20to%20drastically%20reduce%20the%20effort%20required%20to%20train%0Asegmentation%20models%20for%20novel%20classes.%20Instead%20of%20trivially%20feeding%20interactive%0Asegmentation%20masks%20as%20ground%20truth%20to%20a%20few-shot%20segmentation%20model%2C%20we%20propose%0AIFSENet%2C%20which%20can%20accept%20sparse%20supervision%20on%20a%20single%20or%20few%20support%20images%0Ain%20the%20form%20of%20clicks%20to%20generate%20masks%20on%20support%20%28training%2C%20at%20least%20clicked%0Aupon%20once%29%20as%20well%20as%20query%20%28test%2C%20never%20clicked%20upon%29%20images.%20To%20trade-off%0Aeffort%20for%20accuracy%20flexibly%2C%20the%20number%20of%20images%20and%20clicks%20can%20be%0Aincrementally%20added%20to%20the%20support%20set%20to%20further%20improve%20the%20segmentation%20of%0Asupport%20as%20well%20as%20query%20images.%20The%20proposed%20model%20approaches%20the%20accuracy%20of%0Aprevious%20state-of-the-art%20few-shot%20segmentation%20models%20with%20considerably%20lower%0Aannotation%20effort%20%28clicks%20instead%20of%20maps%29%2C%20when%20tested%20on%20Pascal%20and%20SBD%0Adatasets%20on%20query%20images.%20It%20also%20works%20well%20as%20an%20interactive%20segmentation%0Amethod%20on%20support%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15089v1&entry.124074799=Read"},
{"title": "Robust Utility Optimization via a GAN Approach", "author": "Florian Krach and Josef Teichmann and Hanna Wutte", "abstract": "  Robust utility optimization enables an investor to deal with market\nuncertainty in a structured way, with the goal of maximizing the worst-case\noutcome. In this work, we propose a generative adversarial network (GAN)\napproach to (approximately) solve robust utility optimization problems in\ngeneral and realistic settings. In particular, we model both the investor and\nthe market by neural networks (NN) and train them in a mini-max zero-sum game.\nThis approach is applicable for any continuous utility function and in\nrealistic market settings with trading costs, where only observable information\nof the market can be used. A large empirical study shows the versatile\nusability of our method. Whenever an optimal reference strategy is available,\nour method performs on par with it and in the (many) settings without known\noptimal strategy, our method outperforms all other reference strategies.\nMoreover, we can conclude from our study that the trained path-dependent\nstrategies do not outperform Markovian ones. Lastly, we uncover that our\ngenerative approach for learning optimal, (non-) robust investments under\ntrading costs generates universally applicable alternatives to well known\nasymptotic strategies of idealized settings.\n", "link": "http://arxiv.org/abs/2403.15243v1", "date": "2024-03-22", "relevancy": 2.5219, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5184}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5061}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Utility%20Optimization%20via%20a%20GAN%20Approach&body=Title%3A%20Robust%20Utility%20Optimization%20via%20a%20GAN%20Approach%0AAuthor%3A%20Florian%20Krach%20and%20Josef%20Teichmann%20and%20Hanna%20Wutte%0AAbstract%3A%20%20%20Robust%20utility%20optimization%20enables%20an%20investor%20to%20deal%20with%20market%0Auncertainty%20in%20a%20structured%20way%2C%20with%20the%20goal%20of%20maximizing%20the%20worst-case%0Aoutcome.%20In%20this%20work%2C%20we%20propose%20a%20generative%20adversarial%20network%20%28GAN%29%0Aapproach%20to%20%28approximately%29%20solve%20robust%20utility%20optimization%20problems%20in%0Ageneral%20and%20realistic%20settings.%20In%20particular%2C%20we%20model%20both%20the%20investor%20and%0Athe%20market%20by%20neural%20networks%20%28NN%29%20and%20train%20them%20in%20a%20mini-max%20zero-sum%20game.%0AThis%20approach%20is%20applicable%20for%20any%20continuous%20utility%20function%20and%20in%0Arealistic%20market%20settings%20with%20trading%20costs%2C%20where%20only%20observable%20information%0Aof%20the%20market%20can%20be%20used.%20A%20large%20empirical%20study%20shows%20the%20versatile%0Ausability%20of%20our%20method.%20Whenever%20an%20optimal%20reference%20strategy%20is%20available%2C%0Aour%20method%20performs%20on%20par%20with%20it%20and%20in%20the%20%28many%29%20settings%20without%20known%0Aoptimal%20strategy%2C%20our%20method%20outperforms%20all%20other%20reference%20strategies.%0AMoreover%2C%20we%20can%20conclude%20from%20our%20study%20that%20the%20trained%20path-dependent%0Astrategies%20do%20not%20outperform%20Markovian%20ones.%20Lastly%2C%20we%20uncover%20that%20our%0Agenerative%20approach%20for%20learning%20optimal%2C%20%28non-%29%20robust%20investments%20under%0Atrading%20costs%20generates%20universally%20applicable%20alternatives%20to%20well%20known%0Aasymptotic%20strategies%20of%20idealized%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15243v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Utility%20Optimization%20via%20a%20GAN%20Approach&entry.906535625=Florian%20Krach%20and%20Josef%20Teichmann%20and%20Hanna%20Wutte&entry.1292438233=%20%20Robust%20utility%20optimization%20enables%20an%20investor%20to%20deal%20with%20market%0Auncertainty%20in%20a%20structured%20way%2C%20with%20the%20goal%20of%20maximizing%20the%20worst-case%0Aoutcome.%20In%20this%20work%2C%20we%20propose%20a%20generative%20adversarial%20network%20%28GAN%29%0Aapproach%20to%20%28approximately%29%20solve%20robust%20utility%20optimization%20problems%20in%0Ageneral%20and%20realistic%20settings.%20In%20particular%2C%20we%20model%20both%20the%20investor%20and%0Athe%20market%20by%20neural%20networks%20%28NN%29%20and%20train%20them%20in%20a%20mini-max%20zero-sum%20game.%0AThis%20approach%20is%20applicable%20for%20any%20continuous%20utility%20function%20and%20in%0Arealistic%20market%20settings%20with%20trading%20costs%2C%20where%20only%20observable%20information%0Aof%20the%20market%20can%20be%20used.%20A%20large%20empirical%20study%20shows%20the%20versatile%0Ausability%20of%20our%20method.%20Whenever%20an%20optimal%20reference%20strategy%20is%20available%2C%0Aour%20method%20performs%20on%20par%20with%20it%20and%20in%20the%20%28many%29%20settings%20without%20known%0Aoptimal%20strategy%2C%20our%20method%20outperforms%20all%20other%20reference%20strategies.%0AMoreover%2C%20we%20can%20conclude%20from%20our%20study%20that%20the%20trained%20path-dependent%0Astrategies%20do%20not%20outperform%20Markovian%20ones.%20Lastly%2C%20we%20uncover%20that%20our%0Agenerative%20approach%20for%20learning%20optimal%2C%20%28non-%29%20robust%20investments%20under%0Atrading%20costs%20generates%20universally%20applicable%20alternatives%20to%20well%20known%0Aasymptotic%20strategies%20of%20idealized%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15243v1&entry.124074799=Read"},
{"title": "Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level", "author": "Ali Hassani and Wen-Mei Hwu and Humphrey Shi", "abstract": "  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\nfirst show that neighborhood attention can be represented as a batched GEMM\nproblem, similar to standard attention, and implement it for 1-D and 2-D\nneighborhood attention. These kernels on average provide 895% and 272%\nimprovement in full precision latency compared to existing naive kernels for\n1-D and 2-D neighborhood attention respectively. We find certain inherent\ninefficiencies in all unfused neighborhood attention kernels that bound their\nperformance and lower-precision scalability. We also developed fused\nneighborhood attention; an adaptation of fused dot-product attention kernels\nthat allow fine-grained control over attention across different spatial axes.\nKnown for reducing the quadratic time complexity of self attention to a linear\ncomplexity, neighborhood attention can now enjoy a reduced and constant memory\nfootprint, and record-breaking half precision latency. We observe that our\nfused kernels successfully circumvent some of the unavoidable inefficiencies in\nunfused implementations. While our unfused GEMM-based kernels only improve half\nprecision performance compared to naive kernels by an average of 496% and 113%\nin 1-D and 2-D problems respectively, our fused kernels improve naive kernels\nby an average of 1607% and 581% in 1-D and 2-D problems respectively.\n", "link": "http://arxiv.org/abs/2403.04690v2", "date": "2024-03-22", "relevancy": 2.4918, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5834}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4562}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4555}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Faster%20Neighborhood%20Attention%3A%20Reducing%20the%20O%28n%5E2%29%20Cost%20of%20Self%0A%20%20Attention%20at%20the%20Threadblock%20Level&body=Title%3A%20Faster%20Neighborhood%20Attention%3A%20Reducing%20the%20O%28n%5E2%29%20Cost%20of%20Self%0A%20%20Attention%20at%20the%20Threadblock%20Level%0AAuthor%3A%20Ali%20Hassani%20and%20Wen-Mei%20Hwu%20and%20Humphrey%20Shi%0AAbstract%3A%20%20%20Neighborhood%20attention%20reduces%20the%20cost%20of%20self%20attention%20by%20restricting%20each%0Atoken%27s%20attention%20span%20to%20its%20nearest%20neighbors.%20This%20restriction%2C%0Aparameterized%20by%20a%20window%20size%20and%20dilation%20factor%2C%20draws%20a%20spectrum%20of%0Apossible%20attention%20patterns%20between%20linear%20projection%20and%20self%20attention.%0ANeighborhood%20attention%2C%20and%20more%20generally%20sliding%20window%20attention%20patterns%2C%0Ahave%20long%20been%20bounded%20by%20infrastructure%2C%20particularly%20in%20higher-rank%20spaces%0A%282-D%20and%203-D%29%2C%20calling%20for%20the%20development%20of%20custom%20kernels%2C%20which%20have%20been%0Alimited%20in%20either%20functionality%2C%20or%20performance%2C%20if%20not%20both.%20In%20this%20work%2C%20we%0Afirst%20show%20that%20neighborhood%20attention%20can%20be%20represented%20as%20a%20batched%20GEMM%0Aproblem%2C%20similar%20to%20standard%20attention%2C%20and%20implement%20it%20for%201-D%20and%202-D%0Aneighborhood%20attention.%20These%20kernels%20on%20average%20provide%20895%25%20and%20272%25%0Aimprovement%20in%20full%20precision%20latency%20compared%20to%20existing%20naive%20kernels%20for%0A1-D%20and%202-D%20neighborhood%20attention%20respectively.%20We%20find%20certain%20inherent%0Ainefficiencies%20in%20all%20unfused%20neighborhood%20attention%20kernels%20that%20bound%20their%0Aperformance%20and%20lower-precision%20scalability.%20We%20also%20developed%20fused%0Aneighborhood%20attention%3B%20an%20adaptation%20of%20fused%20dot-product%20attention%20kernels%0Athat%20allow%20fine-grained%20control%20over%20attention%20across%20different%20spatial%20axes.%0AKnown%20for%20reducing%20the%20quadratic%20time%20complexity%20of%20self%20attention%20to%20a%20linear%0Acomplexity%2C%20neighborhood%20attention%20can%20now%20enjoy%20a%20reduced%20and%20constant%20memory%0Afootprint%2C%20and%20record-breaking%20half%20precision%20latency.%20We%20observe%20that%20our%0Afused%20kernels%20successfully%20circumvent%20some%20of%20the%20unavoidable%20inefficiencies%20in%0Aunfused%20implementations.%20While%20our%20unfused%20GEMM-based%20kernels%20only%20improve%20half%0Aprecision%20performance%20compared%20to%20naive%20kernels%20by%20an%20average%20of%20496%25%20and%20113%25%0Ain%201-D%20and%202-D%20problems%20respectively%2C%20our%20fused%20kernels%20improve%20naive%20kernels%0Aby%20an%20average%20of%201607%25%20and%20581%25%20in%201-D%20and%202-D%20problems%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04690v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Neighborhood%20Attention%3A%20Reducing%20the%20O%28n%5E2%29%20Cost%20of%20Self%0A%20%20Attention%20at%20the%20Threadblock%20Level&entry.906535625=Ali%20Hassani%20and%20Wen-Mei%20Hwu%20and%20Humphrey%20Shi&entry.1292438233=%20%20Neighborhood%20attention%20reduces%20the%20cost%20of%20self%20attention%20by%20restricting%20each%0Atoken%27s%20attention%20span%20to%20its%20nearest%20neighbors.%20This%20restriction%2C%0Aparameterized%20by%20a%20window%20size%20and%20dilation%20factor%2C%20draws%20a%20spectrum%20of%0Apossible%20attention%20patterns%20between%20linear%20projection%20and%20self%20attention.%0ANeighborhood%20attention%2C%20and%20more%20generally%20sliding%20window%20attention%20patterns%2C%0Ahave%20long%20been%20bounded%20by%20infrastructure%2C%20particularly%20in%20higher-rank%20spaces%0A%282-D%20and%203-D%29%2C%20calling%20for%20the%20development%20of%20custom%20kernels%2C%20which%20have%20been%0Alimited%20in%20either%20functionality%2C%20or%20performance%2C%20if%20not%20both.%20In%20this%20work%2C%20we%0Afirst%20show%20that%20neighborhood%20attention%20can%20be%20represented%20as%20a%20batched%20GEMM%0Aproblem%2C%20similar%20to%20standard%20attention%2C%20and%20implement%20it%20for%201-D%20and%202-D%0Aneighborhood%20attention.%20These%20kernels%20on%20average%20provide%20895%25%20and%20272%25%0Aimprovement%20in%20full%20precision%20latency%20compared%20to%20existing%20naive%20kernels%20for%0A1-D%20and%202-D%20neighborhood%20attention%20respectively.%20We%20find%20certain%20inherent%0Ainefficiencies%20in%20all%20unfused%20neighborhood%20attention%20kernels%20that%20bound%20their%0Aperformance%20and%20lower-precision%20scalability.%20We%20also%20developed%20fused%0Aneighborhood%20attention%3B%20an%20adaptation%20of%20fused%20dot-product%20attention%20kernels%0Athat%20allow%20fine-grained%20control%20over%20attention%20across%20different%20spatial%20axes.%0AKnown%20for%20reducing%20the%20quadratic%20time%20complexity%20of%20self%20attention%20to%20a%20linear%0Acomplexity%2C%20neighborhood%20attention%20can%20now%20enjoy%20a%20reduced%20and%20constant%20memory%0Afootprint%2C%20and%20record-breaking%20half%20precision%20latency.%20We%20observe%20that%20our%0Afused%20kernels%20successfully%20circumvent%20some%20of%20the%20unavoidable%20inefficiencies%20in%0Aunfused%20implementations.%20While%20our%20unfused%20GEMM-based%20kernels%20only%20improve%20half%0Aprecision%20performance%20compared%20to%20naive%20kernels%20by%20an%20average%20of%20496%25%20and%20113%25%0Ain%201-D%20and%202-D%20problems%20respectively%2C%20our%20fused%20kernels%20improve%20naive%20kernels%0Aby%20an%20average%20of%201607%25%20and%20581%25%20in%201-D%20and%202-D%20problems%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04690v2&entry.124074799=Read"},
{"title": "FSC: Few-point Shape Completion", "author": "Xianzu Wu and Xianfeng Wu and Tianyu Luan and Yajing Bai and Zhongyuan Lai and Junsong Yuan", "abstract": "  While previous studies have demonstrated successful 3D object shape\ncompletion with a sufficient number of points, they often fail in scenarios\nwhen a few points, e.g. tens of points, are observed. Surprisingly, via entropy\nanalysis, we find that even a few points, e.g. 64 points, could retain\nsubstantial information to help recover the 3D shape of the object. To address\nthe challenge of shape completion with very sparse point clouds, we then\npropose Few-point Shape Completion (FSC) model, which contains a novel\ndual-branch feature extractor for handling extremely sparse inputs, coupled\nwith an extensive branch for maximal point utilization with a saliency branch\nfor dynamic importance assignment. This model is further bolstered by a\ntwo-stage revision network that refines both the extracted features and the\ndecoder output, enhancing the detail and authenticity of the completed point\ncloud. Our experiments demonstrate the feasibility of recovering 3D shapes from\na few points. The proposed Few-point Shape Completion (FSC) model outperforms\nprevious methods on both few-point inputs and many-point inputs, and shows good\ngeneralizability to different object categories.\n", "link": "http://arxiv.org/abs/2403.07359v3", "date": "2024-03-22", "relevancy": 2.4891, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.503}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4996}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4909}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FSC%3A%20Few-point%20Shape%20Completion&body=Title%3A%20FSC%3A%20Few-point%20Shape%20Completion%0AAuthor%3A%20Xianzu%20Wu%20and%20Xianfeng%20Wu%20and%20Tianyu%20Luan%20and%20Yajing%20Bai%20and%20Zhongyuan%20Lai%20and%20Junsong%20Yuan%0AAbstract%3A%20%20%20While%20previous%20studies%20have%20demonstrated%20successful%203D%20object%20shape%0Acompletion%20with%20a%20sufficient%20number%20of%20points%2C%20they%20often%20fail%20in%20scenarios%0Awhen%20a%20few%20points%2C%20e.g.%20tens%20of%20points%2C%20are%20observed.%20Surprisingly%2C%20via%20entropy%0Aanalysis%2C%20we%20find%20that%20even%20a%20few%20points%2C%20e.g.%2064%20points%2C%20could%20retain%0Asubstantial%20information%20to%20help%20recover%20the%203D%20shape%20of%20the%20object.%20To%20address%0Athe%20challenge%20of%20shape%20completion%20with%20very%20sparse%20point%20clouds%2C%20we%20then%0Apropose%20Few-point%20Shape%20Completion%20%28FSC%29%20model%2C%20which%20contains%20a%20novel%0Adual-branch%20feature%20extractor%20for%20handling%20extremely%20sparse%20inputs%2C%20coupled%0Awith%20an%20extensive%20branch%20for%20maximal%20point%20utilization%20with%20a%20saliency%20branch%0Afor%20dynamic%20importance%20assignment.%20This%20model%20is%20further%20bolstered%20by%20a%0Atwo-stage%20revision%20network%20that%20refines%20both%20the%20extracted%20features%20and%20the%0Adecoder%20output%2C%20enhancing%20the%20detail%20and%20authenticity%20of%20the%20completed%20point%0Acloud.%20Our%20experiments%20demonstrate%20the%20feasibility%20of%20recovering%203D%20shapes%20from%0Aa%20few%20points.%20The%20proposed%20Few-point%20Shape%20Completion%20%28FSC%29%20model%20outperforms%0Aprevious%20methods%20on%20both%20few-point%20inputs%20and%20many-point%20inputs%2C%20and%20shows%20good%0Ageneralizability%20to%20different%20object%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07359v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FSC%3A%20Few-point%20Shape%20Completion&entry.906535625=Xianzu%20Wu%20and%20Xianfeng%20Wu%20and%20Tianyu%20Luan%20and%20Yajing%20Bai%20and%20Zhongyuan%20Lai%20and%20Junsong%20Yuan&entry.1292438233=%20%20While%20previous%20studies%20have%20demonstrated%20successful%203D%20object%20shape%0Acompletion%20with%20a%20sufficient%20number%20of%20points%2C%20they%20often%20fail%20in%20scenarios%0Awhen%20a%20few%20points%2C%20e.g.%20tens%20of%20points%2C%20are%20observed.%20Surprisingly%2C%20via%20entropy%0Aanalysis%2C%20we%20find%20that%20even%20a%20few%20points%2C%20e.g.%2064%20points%2C%20could%20retain%0Asubstantial%20information%20to%20help%20recover%20the%203D%20shape%20of%20the%20object.%20To%20address%0Athe%20challenge%20of%20shape%20completion%20with%20very%20sparse%20point%20clouds%2C%20we%20then%0Apropose%20Few-point%20Shape%20Completion%20%28FSC%29%20model%2C%20which%20contains%20a%20novel%0Adual-branch%20feature%20extractor%20for%20handling%20extremely%20sparse%20inputs%2C%20coupled%0Awith%20an%20extensive%20branch%20for%20maximal%20point%20utilization%20with%20a%20saliency%20branch%0Afor%20dynamic%20importance%20assignment.%20This%20model%20is%20further%20bolstered%20by%20a%0Atwo-stage%20revision%20network%20that%20refines%20both%20the%20extracted%20features%20and%20the%0Adecoder%20output%2C%20enhancing%20the%20detail%20and%20authenticity%20of%20the%20completed%20point%0Acloud.%20Our%20experiments%20demonstrate%20the%20feasibility%20of%20recovering%203D%20shapes%20from%0Aa%20few%20points.%20The%20proposed%20Few-point%20Shape%20Completion%20%28FSC%29%20model%20outperforms%0Aprevious%20methods%20on%20both%20few-point%20inputs%20and%20many-point%20inputs%2C%20and%20shows%20good%0Ageneralizability%20to%20different%20object%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07359v3&entry.124074799=Read"},
{"title": "Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation", "author": "Jiaheng Yu and Jing Li and Yue He and Kai Zhu and Shuyi Zhang and Wen Hu", "abstract": "  Recent methods utilize graph contrastive Learning within graph-structured\nuser-item interaction data for collaborative filtering and have demonstrated\ntheir efficacy in recommendation tasks. However, they ignore that the\ndifference relation density of nodes between the user- and item-side causes the\nadaptability of graphs on bilateral nodes to be different after multi-hop graph\ninteraction calculation, which limits existing models to achieve ideal results.\nTo solve this issue, we propose a novel framework for recommendation tasks\ncalled Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that\nconsider the bilateral unsymmetry on user-item node relation density for sliced\nuser and item graph reasoning better with bilateral slicing contrastive\ntraining. Especially, taking into account the aggregation ability of\nhypergraph-based graph convolutional network (GCN) in digging implicit\nsimilarities is more suitable for user nodes, embeddings generated from three\ndifferent modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into\ntwo subviews by the user- and item-side respectively, and selectively combined\ninto subview pairs bilaterally based on the characteristics of inter-node\nrelation structure. Furthermore, to align the distribution of user and item\nembeddings after aggregation, a dispersing loss is leveraged to adjust the\nmutual distance between all embeddings for maintaining learning ability.\nComprehensive experiments on two public datasets have proved the superiority of\nBusGCL in comparison to various recommendation methods. Other models can simply\nutilize our bilateral slicing contrastive learning to enhance recommending\nperformance without incurring extra expenses.\n", "link": "http://arxiv.org/abs/2403.15075v1", "date": "2024-03-22", "relevancy": 2.4682, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5075}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4663}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bilateral%20Unsymmetrical%20Graph%20Contrastive%20Learning%20for%20Recommendation&body=Title%3A%20Bilateral%20Unsymmetrical%20Graph%20Contrastive%20Learning%20for%20Recommendation%0AAuthor%3A%20Jiaheng%20Yu%20and%20Jing%20Li%20and%20Yue%20He%20and%20Kai%20Zhu%20and%20Shuyi%20Zhang%20and%20Wen%20Hu%0AAbstract%3A%20%20%20Recent%20methods%20utilize%20graph%20contrastive%20Learning%20within%20graph-structured%0Auser-item%20interaction%20data%20for%20collaborative%20filtering%20and%20have%20demonstrated%0Atheir%20efficacy%20in%20recommendation%20tasks.%20However%2C%20they%20ignore%20that%20the%0Adifference%20relation%20density%20of%20nodes%20between%20the%20user-%20and%20item-side%20causes%20the%0Aadaptability%20of%20graphs%20on%20bilateral%20nodes%20to%20be%20different%20after%20multi-hop%20graph%0Ainteraction%20calculation%2C%20which%20limits%20existing%20models%20to%20achieve%20ideal%20results.%0ATo%20solve%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20for%20recommendation%20tasks%0Acalled%20Bilateral%20Unsymmetrical%20Graph%20Contrastive%20Learning%20%28BusGCL%29%20that%0Aconsider%20the%20bilateral%20unsymmetry%20on%20user-item%20node%20relation%20density%20for%20sliced%0Auser%20and%20item%20graph%20reasoning%20better%20with%20bilateral%20slicing%20contrastive%0Atraining.%20Especially%2C%20taking%20into%20account%20the%20aggregation%20ability%20of%0Ahypergraph-based%20graph%20convolutional%20network%20%28GCN%29%20in%20digging%20implicit%0Asimilarities%20is%20more%20suitable%20for%20user%20nodes%2C%20embeddings%20generated%20from%20three%0Adifferent%20modules%3A%20hypergraph-based%20GCN%2C%20GCN%20and%20perturbed%20GCN%2C%20are%20sliced%20into%0Atwo%20subviews%20by%20the%20user-%20and%20item-side%20respectively%2C%20and%20selectively%20combined%0Ainto%20subview%20pairs%20bilaterally%20based%20on%20the%20characteristics%20of%20inter-node%0Arelation%20structure.%20Furthermore%2C%20to%20align%20the%20distribution%20of%20user%20and%20item%0Aembeddings%20after%20aggregation%2C%20a%20dispersing%20loss%20is%20leveraged%20to%20adjust%20the%0Amutual%20distance%20between%20all%20embeddings%20for%20maintaining%20learning%20ability.%0AComprehensive%20experiments%20on%20two%20public%20datasets%20have%20proved%20the%20superiority%20of%0ABusGCL%20in%20comparison%20to%20various%20recommendation%20methods.%20Other%20models%20can%20simply%0Autilize%20our%20bilateral%20slicing%20contrastive%20learning%20to%20enhance%20recommending%0Aperformance%20without%20incurring%20extra%20expenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15075v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bilateral%20Unsymmetrical%20Graph%20Contrastive%20Learning%20for%20Recommendation&entry.906535625=Jiaheng%20Yu%20and%20Jing%20Li%20and%20Yue%20He%20and%20Kai%20Zhu%20and%20Shuyi%20Zhang%20and%20Wen%20Hu&entry.1292438233=%20%20Recent%20methods%20utilize%20graph%20contrastive%20Learning%20within%20graph-structured%0Auser-item%20interaction%20data%20for%20collaborative%20filtering%20and%20have%20demonstrated%0Atheir%20efficacy%20in%20recommendation%20tasks.%20However%2C%20they%20ignore%20that%20the%0Adifference%20relation%20density%20of%20nodes%20between%20the%20user-%20and%20item-side%20causes%20the%0Aadaptability%20of%20graphs%20on%20bilateral%20nodes%20to%20be%20different%20after%20multi-hop%20graph%0Ainteraction%20calculation%2C%20which%20limits%20existing%20models%20to%20achieve%20ideal%20results.%0ATo%20solve%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20for%20recommendation%20tasks%0Acalled%20Bilateral%20Unsymmetrical%20Graph%20Contrastive%20Learning%20%28BusGCL%29%20that%0Aconsider%20the%20bilateral%20unsymmetry%20on%20user-item%20node%20relation%20density%20for%20sliced%0Auser%20and%20item%20graph%20reasoning%20better%20with%20bilateral%20slicing%20contrastive%0Atraining.%20Especially%2C%20taking%20into%20account%20the%20aggregation%20ability%20of%0Ahypergraph-based%20graph%20convolutional%20network%20%28GCN%29%20in%20digging%20implicit%0Asimilarities%20is%20more%20suitable%20for%20user%20nodes%2C%20embeddings%20generated%20from%20three%0Adifferent%20modules%3A%20hypergraph-based%20GCN%2C%20GCN%20and%20perturbed%20GCN%2C%20are%20sliced%20into%0Atwo%20subviews%20by%20the%20user-%20and%20item-side%20respectively%2C%20and%20selectively%20combined%0Ainto%20subview%20pairs%20bilaterally%20based%20on%20the%20characteristics%20of%20inter-node%0Arelation%20structure.%20Furthermore%2C%20to%20align%20the%20distribution%20of%20user%20and%20item%0Aembeddings%20after%20aggregation%2C%20a%20dispersing%20loss%20is%20leveraged%20to%20adjust%20the%0Amutual%20distance%20between%20all%20embeddings%20for%20maintaining%20learning%20ability.%0AComprehensive%20experiments%20on%20two%20public%20datasets%20have%20proved%20the%20superiority%20of%0ABusGCL%20in%20comparison%20to%20various%20recommendation%20methods.%20Other%20models%20can%20simply%0Autilize%20our%20bilateral%20slicing%20contrastive%20learning%20to%20enhance%20recommending%0Aperformance%20without%20incurring%20extra%20expenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15075v1&entry.124074799=Read"},
{"title": "CoLLEGe: Concept Embedding Generation for Large Language Models", "author": "Ryan Teehan and Brenden Lake and Mengye Ren", "abstract": "  Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training.\n", "link": "http://arxiv.org/abs/2403.15362v1", "date": "2024-03-22", "relevancy": 2.4553, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5097}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4823}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4811}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CoLLEGe%3A%20Concept%20Embedding%20Generation%20for%20Large%20Language%20Models&body=Title%3A%20CoLLEGe%3A%20Concept%20Embedding%20Generation%20for%20Large%20Language%20Models%0AAuthor%3A%20Ryan%20Teehan%20and%20Brenden%20Lake%20and%20Mengye%20Ren%0AAbstract%3A%20%20%20Current%20language%20models%20are%20unable%20to%20quickly%20learn%20new%20concepts%20on%20the%20fly%2C%0Aoften%20requiring%20a%20more%20involved%20finetuning%20process%20to%20learn%20robustly.%20Prompting%0Ain-context%20is%20not%20robust%20to%20context%20distractions%2C%20and%20often%20fails%20to%20confer%0Amuch%20information%20about%20the%20new%20concepts.%20Classic%20methods%20for%20few-shot%20word%0Alearning%20in%20NLP%2C%20relying%20on%20global%20word%20vectors%2C%20are%20less%20applicable%20to%20large%0Alanguage%20models.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20named%20CoLLEGe%0A%28Concept%20Learning%20with%20Language%20Embedding%20Generation%29%20to%20modernize%20few-shot%0Aconcept%20learning.%20CoLLEGe%20is%20a%20meta-learning%20framework%20capable%20of%20generating%0Aflexible%20embeddings%20for%20new%20concepts%20using%20a%20small%20number%20of%20example%20sentences%0Aor%20definitions.%20Our%20primary%20meta-learning%20objective%20is%20simply%20to%20facilitate%20a%0Alanguage%20model%20to%20make%20next%20word%20predictions%20in%20forthcoming%20sentences%2C%20making%0Ait%20compatible%20with%20language%20model%20pretraining.%20We%20design%20a%20series%20of%20tasks%20to%0Atest%20new%20concept%20learning%20in%20challenging%20real-world%20scenarios%2C%20including%20new%0Aword%20acquisition%2C%20definition%20inference%2C%20and%20verbal%20reasoning%2C%20and%20demonstrate%0Athat%20our%20method%20succeeds%20in%20each%20setting%20without%20task-specific%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15362v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoLLEGe%3A%20Concept%20Embedding%20Generation%20for%20Large%20Language%20Models&entry.906535625=Ryan%20Teehan%20and%20Brenden%20Lake%20and%20Mengye%20Ren&entry.1292438233=%20%20Current%20language%20models%20are%20unable%20to%20quickly%20learn%20new%20concepts%20on%20the%20fly%2C%0Aoften%20requiring%20a%20more%20involved%20finetuning%20process%20to%20learn%20robustly.%20Prompting%0Ain-context%20is%20not%20robust%20to%20context%20distractions%2C%20and%20often%20fails%20to%20confer%0Amuch%20information%20about%20the%20new%20concepts.%20Classic%20methods%20for%20few-shot%20word%0Alearning%20in%20NLP%2C%20relying%20on%20global%20word%20vectors%2C%20are%20less%20applicable%20to%20large%0Alanguage%20models.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%20named%20CoLLEGe%0A%28Concept%20Learning%20with%20Language%20Embedding%20Generation%29%20to%20modernize%20few-shot%0Aconcept%20learning.%20CoLLEGe%20is%20a%20meta-learning%20framework%20capable%20of%20generating%0Aflexible%20embeddings%20for%20new%20concepts%20using%20a%20small%20number%20of%20example%20sentences%0Aor%20definitions.%20Our%20primary%20meta-learning%20objective%20is%20simply%20to%20facilitate%20a%0Alanguage%20model%20to%20make%20next%20word%20predictions%20in%20forthcoming%20sentences%2C%20making%0Ait%20compatible%20with%20language%20model%20pretraining.%20We%20design%20a%20series%20of%20tasks%20to%0Atest%20new%20concept%20learning%20in%20challenging%20real-world%20scenarios%2C%20including%20new%0Aword%20acquisition%2C%20definition%20inference%2C%20and%20verbal%20reasoning%2C%20and%20demonstrate%0Athat%20our%20method%20succeeds%20in%20each%20setting%20without%20task-specific%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15362v1&entry.124074799=Read"},
{"title": "WEEP: A method for spatial interpretation of weakly supervised CNN\n  models in computational pathology", "author": "Abhinav Sharma and Bojing Liu and Mattias Rantalainen", "abstract": "  Deep learning enables the modelling of high-resolution histopathology\nwhole-slide images (WSI). Weakly supervised learning of tile-level data is\ntypically applied for tasks where labels only exist on the patient or WSI level\n(e.g. patient outcomes or histological grading). In this context, there is a\nneed for improved spatial interpretability of predictions from such models. We\npropose a novel method, Wsi rEgion sElection aPproach (WEEP), for model\ninterpretation. It provides a principled yet straightforward way to establish\nthe spatial area of WSI required for assigning a particular prediction label.\nWe demonstrate WEEP on a binary classification task in the area of breast\ncancer computational pathology. WEEP is easy to implement, is directly\nconnected to the model-based decision process, and offers information relevant\nto both research and diagnostic applications.\n", "link": "http://arxiv.org/abs/2403.15238v1", "date": "2024-03-22", "relevancy": 2.4504, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4792}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WEEP%3A%20A%20method%20for%20spatial%20interpretation%20of%20weakly%20supervised%20CNN%0A%20%20models%20in%20computational%20pathology&body=Title%3A%20WEEP%3A%20A%20method%20for%20spatial%20interpretation%20of%20weakly%20supervised%20CNN%0A%20%20models%20in%20computational%20pathology%0AAuthor%3A%20Abhinav%20Sharma%20and%20Bojing%20Liu%20and%20Mattias%20Rantalainen%0AAbstract%3A%20%20%20Deep%20learning%20enables%20the%20modelling%20of%20high-resolution%20histopathology%0Awhole-slide%20images%20%28WSI%29.%20Weakly%20supervised%20learning%20of%20tile-level%20data%20is%0Atypically%20applied%20for%20tasks%20where%20labels%20only%20exist%20on%20the%20patient%20or%20WSI%20level%0A%28e.g.%20patient%20outcomes%20or%20histological%20grading%29.%20In%20this%20context%2C%20there%20is%20a%0Aneed%20for%20improved%20spatial%20interpretability%20of%20predictions%20from%20such%20models.%20We%0Apropose%20a%20novel%20method%2C%20Wsi%20rEgion%20sElection%20aPproach%20%28WEEP%29%2C%20for%20model%0Ainterpretation.%20It%20provides%20a%20principled%20yet%20straightforward%20way%20to%20establish%0Athe%20spatial%20area%20of%20WSI%20required%20for%20assigning%20a%20particular%20prediction%20label.%0AWe%20demonstrate%20WEEP%20on%20a%20binary%20classification%20task%20in%20the%20area%20of%20breast%0Acancer%20computational%20pathology.%20WEEP%20is%20easy%20to%20implement%2C%20is%20directly%0Aconnected%20to%20the%20model-based%20decision%20process%2C%20and%20offers%20information%20relevant%0Ato%20both%20research%20and%20diagnostic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15238v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WEEP%3A%20A%20method%20for%20spatial%20interpretation%20of%20weakly%20supervised%20CNN%0A%20%20models%20in%20computational%20pathology&entry.906535625=Abhinav%20Sharma%20and%20Bojing%20Liu%20and%20Mattias%20Rantalainen&entry.1292438233=%20%20Deep%20learning%20enables%20the%20modelling%20of%20high-resolution%20histopathology%0Awhole-slide%20images%20%28WSI%29.%20Weakly%20supervised%20learning%20of%20tile-level%20data%20is%0Atypically%20applied%20for%20tasks%20where%20labels%20only%20exist%20on%20the%20patient%20or%20WSI%20level%0A%28e.g.%20patient%20outcomes%20or%20histological%20grading%29.%20In%20this%20context%2C%20there%20is%20a%0Aneed%20for%20improved%20spatial%20interpretability%20of%20predictions%20from%20such%20models.%20We%0Apropose%20a%20novel%20method%2C%20Wsi%20rEgion%20sElection%20aPproach%20%28WEEP%29%2C%20for%20model%0Ainterpretation.%20It%20provides%20a%20principled%20yet%20straightforward%20way%20to%20establish%0Athe%20spatial%20area%20of%20WSI%20required%20for%20assigning%20a%20particular%20prediction%20label.%0AWe%20demonstrate%20WEEP%20on%20a%20binary%20classification%20task%20in%20the%20area%20of%20breast%0Acancer%20computational%20pathology.%20WEEP%20is%20easy%20to%20implement%2C%20is%20directly%0Aconnected%20to%20the%20model-based%20decision%20process%2C%20and%20offers%20information%20relevant%0Ato%20both%20research%20and%20diagnostic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15238v1&entry.124074799=Read"},
{"title": "Spectral Motion Alignment for Video Motion Transfer using Diffusion\n  Models", "author": "Geon Yeong Park and Hyeonho Jeong and Sang Wan Lee and Jong Chul Ye", "abstract": "  The evolution of diffusion models has greatly impacted video generation and\nunderstanding. Particularly, text-to-video diffusion models (VDMs) have\nsignificantly facilitated the customization of input video with target\nappearance, motion, etc. Despite these advances, challenges persist in\naccurately distilling motion information from video frames. While existing\nworks leverage the consecutive frame residual as the target motion vector, they\ninherently lack global motion context and are vulnerable to frame-wise\ndistortions. To address this, we present Spectral Motion Alignment (SMA), a\nnovel framework that refines and aligns motion vectors using Fourier and\nwavelet transforms. SMA learns motion patterns by incorporating\nfrequency-domain regularization, facilitating the learning of whole-frame\nglobal motion dynamics, and mitigating spatial artifacts. Extensive experiments\ndemonstrate SMA's efficacy in improving motion transfer while maintaining\ncomputational efficiency and compatibility across various video customization\nframeworks.\n", "link": "http://arxiv.org/abs/2403.15249v1", "date": "2024-03-22", "relevancy": 2.4443, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6986}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5983}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5889}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spectral%20Motion%20Alignment%20for%20Video%20Motion%20Transfer%20using%20Diffusion%0A%20%20Models&body=Title%3A%20Spectral%20Motion%20Alignment%20for%20Video%20Motion%20Transfer%20using%20Diffusion%0A%20%20Models%0AAuthor%3A%20Geon%20Yeong%20Park%20and%20Hyeonho%20Jeong%20and%20Sang%20Wan%20Lee%20and%20Jong%20Chul%20Ye%0AAbstract%3A%20%20%20The%20evolution%20of%20diffusion%20models%20has%20greatly%20impacted%20video%20generation%20and%0Aunderstanding.%20Particularly%2C%20text-to-video%20diffusion%20models%20%28VDMs%29%20have%0Asignificantly%20facilitated%20the%20customization%20of%20input%20video%20with%20target%0Aappearance%2C%20motion%2C%20etc.%20Despite%20these%20advances%2C%20challenges%20persist%20in%0Aaccurately%20distilling%20motion%20information%20from%20video%20frames.%20While%20existing%0Aworks%20leverage%20the%20consecutive%20frame%20residual%20as%20the%20target%20motion%20vector%2C%20they%0Ainherently%20lack%20global%20motion%20context%20and%20are%20vulnerable%20to%20frame-wise%0Adistortions.%20To%20address%20this%2C%20we%20present%20Spectral%20Motion%20Alignment%20%28SMA%29%2C%20a%0Anovel%20framework%20that%20refines%20and%20aligns%20motion%20vectors%20using%20Fourier%20and%0Awavelet%20transforms.%20SMA%20learns%20motion%20patterns%20by%20incorporating%0Afrequency-domain%20regularization%2C%20facilitating%20the%20learning%20of%20whole-frame%0Aglobal%20motion%20dynamics%2C%20and%20mitigating%20spatial%20artifacts.%20Extensive%20experiments%0Ademonstrate%20SMA%27s%20efficacy%20in%20improving%20motion%20transfer%20while%20maintaining%0Acomputational%20efficiency%20and%20compatibility%20across%20various%20video%20customization%0Aframeworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15249v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Motion%20Alignment%20for%20Video%20Motion%20Transfer%20using%20Diffusion%0A%20%20Models&entry.906535625=Geon%20Yeong%20Park%20and%20Hyeonho%20Jeong%20and%20Sang%20Wan%20Lee%20and%20Jong%20Chul%20Ye&entry.1292438233=%20%20The%20evolution%20of%20diffusion%20models%20has%20greatly%20impacted%20video%20generation%20and%0Aunderstanding.%20Particularly%2C%20text-to-video%20diffusion%20models%20%28VDMs%29%20have%0Asignificantly%20facilitated%20the%20customization%20of%20input%20video%20with%20target%0Aappearance%2C%20motion%2C%20etc.%20Despite%20these%20advances%2C%20challenges%20persist%20in%0Aaccurately%20distilling%20motion%20information%20from%20video%20frames.%20While%20existing%0Aworks%20leverage%20the%20consecutive%20frame%20residual%20as%20the%20target%20motion%20vector%2C%20they%0Ainherently%20lack%20global%20motion%20context%20and%20are%20vulnerable%20to%20frame-wise%0Adistortions.%20To%20address%20this%2C%20we%20present%20Spectral%20Motion%20Alignment%20%28SMA%29%2C%20a%0Anovel%20framework%20that%20refines%20and%20aligns%20motion%20vectors%20using%20Fourier%20and%0Awavelet%20transforms.%20SMA%20learns%20motion%20patterns%20by%20incorporating%0Afrequency-domain%20regularization%2C%20facilitating%20the%20learning%20of%20whole-frame%0Aglobal%20motion%20dynamics%2C%20and%20mitigating%20spatial%20artifacts.%20Extensive%20experiments%0Ademonstrate%20SMA%27s%20efficacy%20in%20improving%20motion%20transfer%20while%20maintaining%0Acomputational%20efficiency%20and%20compatibility%20across%20various%20video%20customization%0Aframeworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15249v1&entry.124074799=Read"},
{"title": "Cell Variational Information Bottleneck Network", "author": "Zhonghua Zhai and Chen Ju and Jinsong Lan and Shuai Xiao", "abstract": "  In this work, we propose Cell Variational Information Bottleneck Network\n(cellVIB), a convolutional neural network using information bottleneck\nmechanism, which can be combined with the latest feedforward network\narchitecture in an end-to-end training method. Our Cell Variational Information\nBottleneck Network is constructed by stacking VIB cells, which generate feature\nmaps with uncertainty. As layers going deeper, the regularization effect will\ngradually increase, instead of directly adding excessive regular constraints to\nthe output layer of the model as in Deep VIB. Under each VIB cell, the\nfeedforward process learns an independent mean term and an standard deviation\nterm, and predicts the Gaussian distribution based on them. The feedback\nprocess is based on reparameterization trick for effective training. This work\nperforms an extensive analysis on MNIST dataset to verify the effectiveness of\neach VIB cells, and provides an insightful analysis on how the VIB cells affect\nmutual information. Experiments conducted on CIFAR-10 also prove that our\ncellVIB is robust against noisy labels during training and against corrupted\nimages during testing. Then, we validate our method on PACS dataset, whose\nresults show that the VIB cells can significantly improve the generalization\nperformance of the basic model. Finally, in a more complex representation\nlearning task, face recognition, our network structure has also achieved very\ncompetitive results.\n", "link": "http://arxiv.org/abs/2403.15082v1", "date": "2024-03-22", "relevancy": 2.4311, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4983}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4811}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4793}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cell%20Variational%20Information%20Bottleneck%20Network&body=Title%3A%20Cell%20Variational%20Information%20Bottleneck%20Network%0AAuthor%3A%20Zhonghua%20Zhai%20and%20Chen%20Ju%20and%20Jinsong%20Lan%20and%20Shuai%20Xiao%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20Cell%20Variational%20Information%20Bottleneck%20Network%0A%28cellVIB%29%2C%20a%20convolutional%20neural%20network%20using%20information%20bottleneck%0Amechanism%2C%20which%20can%20be%20combined%20with%20the%20latest%20feedforward%20network%0Aarchitecture%20in%20an%20end-to-end%20training%20method.%20Our%20Cell%20Variational%20Information%0ABottleneck%20Network%20is%20constructed%20by%20stacking%20VIB%20cells%2C%20which%20generate%20feature%0Amaps%20with%20uncertainty.%20As%20layers%20going%20deeper%2C%20the%20regularization%20effect%20will%0Agradually%20increase%2C%20instead%20of%20directly%20adding%20excessive%20regular%20constraints%20to%0Athe%20output%20layer%20of%20the%20model%20as%20in%20Deep%20VIB.%20Under%20each%20VIB%20cell%2C%20the%0Afeedforward%20process%20learns%20an%20independent%20mean%20term%20and%20an%20standard%20deviation%0Aterm%2C%20and%20predicts%20the%20Gaussian%20distribution%20based%20on%20them.%20The%20feedback%0Aprocess%20is%20based%20on%20reparameterization%20trick%20for%20effective%20training.%20This%20work%0Aperforms%20an%20extensive%20analysis%20on%20MNIST%20dataset%20to%20verify%20the%20effectiveness%20of%0Aeach%20VIB%20cells%2C%20and%20provides%20an%20insightful%20analysis%20on%20how%20the%20VIB%20cells%20affect%0Amutual%20information.%20Experiments%20conducted%20on%20CIFAR-10%20also%20prove%20that%20our%0AcellVIB%20is%20robust%20against%20noisy%20labels%20during%20training%20and%20against%20corrupted%0Aimages%20during%20testing.%20Then%2C%20we%20validate%20our%20method%20on%20PACS%20dataset%2C%20whose%0Aresults%20show%20that%20the%20VIB%20cells%20can%20significantly%20improve%20the%20generalization%0Aperformance%20of%20the%20basic%20model.%20Finally%2C%20in%20a%20more%20complex%20representation%0Alearning%20task%2C%20face%20recognition%2C%20our%20network%20structure%20has%20also%20achieved%20very%0Acompetitive%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15082v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cell%20Variational%20Information%20Bottleneck%20Network&entry.906535625=Zhonghua%20Zhai%20and%20Chen%20Ju%20and%20Jinsong%20Lan%20and%20Shuai%20Xiao&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20Cell%20Variational%20Information%20Bottleneck%20Network%0A%28cellVIB%29%2C%20a%20convolutional%20neural%20network%20using%20information%20bottleneck%0Amechanism%2C%20which%20can%20be%20combined%20with%20the%20latest%20feedforward%20network%0Aarchitecture%20in%20an%20end-to-end%20training%20method.%20Our%20Cell%20Variational%20Information%0ABottleneck%20Network%20is%20constructed%20by%20stacking%20VIB%20cells%2C%20which%20generate%20feature%0Amaps%20with%20uncertainty.%20As%20layers%20going%20deeper%2C%20the%20regularization%20effect%20will%0Agradually%20increase%2C%20instead%20of%20directly%20adding%20excessive%20regular%20constraints%20to%0Athe%20output%20layer%20of%20the%20model%20as%20in%20Deep%20VIB.%20Under%20each%20VIB%20cell%2C%20the%0Afeedforward%20process%20learns%20an%20independent%20mean%20term%20and%20an%20standard%20deviation%0Aterm%2C%20and%20predicts%20the%20Gaussian%20distribution%20based%20on%20them.%20The%20feedback%0Aprocess%20is%20based%20on%20reparameterization%20trick%20for%20effective%20training.%20This%20work%0Aperforms%20an%20extensive%20analysis%20on%20MNIST%20dataset%20to%20verify%20the%20effectiveness%20of%0Aeach%20VIB%20cells%2C%20and%20provides%20an%20insightful%20analysis%20on%20how%20the%20VIB%20cells%20affect%0Amutual%20information.%20Experiments%20conducted%20on%20CIFAR-10%20also%20prove%20that%20our%0AcellVIB%20is%20robust%20against%20noisy%20labels%20during%20training%20and%20against%20corrupted%0Aimages%20during%20testing.%20Then%2C%20we%20validate%20our%20method%20on%20PACS%20dataset%2C%20whose%0Aresults%20show%20that%20the%20VIB%20cells%20can%20significantly%20improve%20the%20generalization%0Aperformance%20of%20the%20basic%20model.%20Finally%2C%20in%20a%20more%20complex%20representation%0Alearning%20task%2C%20face%20recognition%2C%20our%20network%20structure%20has%20also%20achieved%20very%0Acompetitive%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15082v1&entry.124074799=Read"},
{"title": "Learning to Embed Time Series Patches Independently", "author": "Seunghan Lee and Taeyoung Park and Kibok Lee", "abstract": "  Masked time series modeling has recently gained much attention as a\nself-supervised representation learning strategy for time series. Inspired by\nmasked image modeling in computer vision, recent works first patchify and\npartially mask out time series, and then train Transformers to capture the\ndependencies between patches by predicting masked patches from unmasked\npatches. However, we argue that capturing such patch dependencies might not be\nan optimal strategy for time series representation learning; rather, learning\nto embed patches independently results in better time series representations.\nSpecifically, we propose to use 1) the simple patch reconstruction task, which\nautoencode each patch without looking at other patches, and 2) the simple\npatch-wise MLP that embeds each patch independently. In addition, we introduce\ncomplementary contrastive learning to hierarchically capture adjacent time\nseries information efficiently. Our proposed method improves time series\nforecasting and classification performance compared to state-of-the-art\nTransformer-based models, while it is more efficient in terms of the number of\nparameters and training/inference time. Code is available at this repository:\nhttps://github.com/seunghan96/pits.\n", "link": "http://arxiv.org/abs/2312.16427v3", "date": "2024-03-22", "relevancy": 2.4284, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5064}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4815}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4691}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Embed%20Time%20Series%20Patches%20Independently&body=Title%3A%20Learning%20to%20Embed%20Time%20Series%20Patches%20Independently%0AAuthor%3A%20Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee%0AAbstract%3A%20%20%20Masked%20time%20series%20modeling%20has%20recently%20gained%20much%20attention%20as%20a%0Aself-supervised%20representation%20learning%20strategy%20for%20time%20series.%20Inspired%20by%0Amasked%20image%20modeling%20in%20computer%20vision%2C%20recent%20works%20first%20patchify%20and%0Apartially%20mask%20out%20time%20series%2C%20and%20then%20train%20Transformers%20to%20capture%20the%0Adependencies%20between%20patches%20by%20predicting%20masked%20patches%20from%20unmasked%0Apatches.%20However%2C%20we%20argue%20that%20capturing%20such%20patch%20dependencies%20might%20not%20be%0Aan%20optimal%20strategy%20for%20time%20series%20representation%20learning%3B%20rather%2C%20learning%0Ato%20embed%20patches%20independently%20results%20in%20better%20time%20series%20representations.%0ASpecifically%2C%20we%20propose%20to%20use%201%29%20the%20simple%20patch%20reconstruction%20task%2C%20which%0Aautoencode%20each%20patch%20without%20looking%20at%20other%20patches%2C%20and%202%29%20the%20simple%0Apatch-wise%20MLP%20that%20embeds%20each%20patch%20independently.%20In%20addition%2C%20we%20introduce%0Acomplementary%20contrastive%20learning%20to%20hierarchically%20capture%20adjacent%20time%0Aseries%20information%20efficiently.%20Our%20proposed%20method%20improves%20time%20series%0Aforecasting%20and%20classification%20performance%20compared%20to%20state-of-the-art%0ATransformer-based%20models%2C%20while%20it%20is%20more%20efficient%20in%20terms%20of%20the%20number%20of%0Aparameters%20and%20training/inference%20time.%20Code%20is%20available%20at%20this%20repository%3A%0Ahttps%3A//github.com/seunghan96/pits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16427v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Embed%20Time%20Series%20Patches%20Independently&entry.906535625=Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee&entry.1292438233=%20%20Masked%20time%20series%20modeling%20has%20recently%20gained%20much%20attention%20as%20a%0Aself-supervised%20representation%20learning%20strategy%20for%20time%20series.%20Inspired%20by%0Amasked%20image%20modeling%20in%20computer%20vision%2C%20recent%20works%20first%20patchify%20and%0Apartially%20mask%20out%20time%20series%2C%20and%20then%20train%20Transformers%20to%20capture%20the%0Adependencies%20between%20patches%20by%20predicting%20masked%20patches%20from%20unmasked%0Apatches.%20However%2C%20we%20argue%20that%20capturing%20such%20patch%20dependencies%20might%20not%20be%0Aan%20optimal%20strategy%20for%20time%20series%20representation%20learning%3B%20rather%2C%20learning%0Ato%20embed%20patches%20independently%20results%20in%20better%20time%20series%20representations.%0ASpecifically%2C%20we%20propose%20to%20use%201%29%20the%20simple%20patch%20reconstruction%20task%2C%20which%0Aautoencode%20each%20patch%20without%20looking%20at%20other%20patches%2C%20and%202%29%20the%20simple%0Apatch-wise%20MLP%20that%20embeds%20each%20patch%20independently.%20In%20addition%2C%20we%20introduce%0Acomplementary%20contrastive%20learning%20to%20hierarchically%20capture%20adjacent%20time%0Aseries%20information%20efficiently.%20Our%20proposed%20method%20improves%20time%20series%0Aforecasting%20and%20classification%20performance%20compared%20to%20state-of-the-art%0ATransformer-based%20models%2C%20while%20it%20is%20more%20efficient%20in%20terms%20of%20the%20number%20of%0Aparameters%20and%20training/inference%20time.%20Code%20is%20available%20at%20this%20repository%3A%0Ahttps%3A//github.com/seunghan96/pits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16427v3&entry.124074799=Read"},
{"title": "Controlled Training Data Generation with Diffusion Models", "author": "Teresa Yeo and Andrei Atanov and Harold Benoit and Aleksandr Alekseev and Ruchira Ray and Pooya Esmaeil Akhoondi and Amir Zamir", "abstract": "  In this work, we present a method to control a text-to-image generative model\nto produce training data specifically \"useful\" for supervised learning. Unlike\nprevious works that employ an open-loop approach and pre-define prompts to\ngenerate new data using either a language model or human expertise, we develop\nan automated closed-loop system which involves two feedback mechanisms. The\nfirst mechanism uses feedback from a given supervised model and finds\nadversarial prompts that result in image generations that maximize the model\nloss. While these adversarial prompts result in diverse data informed by the\nmodel, they are not informed of the target distribution, which can be\ninefficient. Therefore, we introduce the second feedback mechanism that guides\nthe generation process towards a certain target distribution. We call the\nmethod combining these two mechanisms Guided Adversarial Prompts. We perform\nour evaluations on different tasks, datasets and architectures, with different\ntypes of distribution shifts (spuriously correlated data, unseen domains) and\ndemonstrate the efficiency of the proposed feedback mechanisms compared to\nopen-loop approaches.\n", "link": "http://arxiv.org/abs/2403.15309v1", "date": "2024-03-22", "relevancy": 2.4234, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6418}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6124}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5849}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Controlled%20Training%20Data%20Generation%20with%20Diffusion%20Models&body=Title%3A%20Controlled%20Training%20Data%20Generation%20with%20Diffusion%20Models%0AAuthor%3A%20Teresa%20Yeo%20and%20Andrei%20Atanov%20and%20Harold%20Benoit%20and%20Aleksandr%20Alekseev%20and%20Ruchira%20Ray%20and%20Pooya%20Esmaeil%20Akhoondi%20and%20Amir%20Zamir%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20method%20to%20control%20a%20text-to-image%20generative%20model%0Ato%20produce%20training%20data%20specifically%20%22useful%22%20for%20supervised%20learning.%20Unlike%0Aprevious%20works%20that%20employ%20an%20open-loop%20approach%20and%20pre-define%20prompts%20to%0Agenerate%20new%20data%20using%20either%20a%20language%20model%20or%20human%20expertise%2C%20we%20develop%0Aan%20automated%20closed-loop%20system%20which%20involves%20two%20feedback%20mechanisms.%20The%0Afirst%20mechanism%20uses%20feedback%20from%20a%20given%20supervised%20model%20and%20finds%0Aadversarial%20prompts%20that%20result%20in%20image%20generations%20that%20maximize%20the%20model%0Aloss.%20While%20these%20adversarial%20prompts%20result%20in%20diverse%20data%20informed%20by%20the%0Amodel%2C%20they%20are%20not%20informed%20of%20the%20target%20distribution%2C%20which%20can%20be%0Ainefficient.%20Therefore%2C%20we%20introduce%20the%20second%20feedback%20mechanism%20that%20guides%0Athe%20generation%20process%20towards%20a%20certain%20target%20distribution.%20We%20call%20the%0Amethod%20combining%20these%20two%20mechanisms%20Guided%20Adversarial%20Prompts.%20We%20perform%0Aour%20evaluations%20on%20different%20tasks%2C%20datasets%20and%20architectures%2C%20with%20different%0Atypes%20of%20distribution%20shifts%20%28spuriously%20correlated%20data%2C%20unseen%20domains%29%20and%0Ademonstrate%20the%20efficiency%20of%20the%20proposed%20feedback%20mechanisms%20compared%20to%0Aopen-loop%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15309v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlled%20Training%20Data%20Generation%20with%20Diffusion%20Models&entry.906535625=Teresa%20Yeo%20and%20Andrei%20Atanov%20and%20Harold%20Benoit%20and%20Aleksandr%20Alekseev%20and%20Ruchira%20Ray%20and%20Pooya%20Esmaeil%20Akhoondi%20and%20Amir%20Zamir&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20method%20to%20control%20a%20text-to-image%20generative%20model%0Ato%20produce%20training%20data%20specifically%20%22useful%22%20for%20supervised%20learning.%20Unlike%0Aprevious%20works%20that%20employ%20an%20open-loop%20approach%20and%20pre-define%20prompts%20to%0Agenerate%20new%20data%20using%20either%20a%20language%20model%20or%20human%20expertise%2C%20we%20develop%0Aan%20automated%20closed-loop%20system%20which%20involves%20two%20feedback%20mechanisms.%20The%0Afirst%20mechanism%20uses%20feedback%20from%20a%20given%20supervised%20model%20and%20finds%0Aadversarial%20prompts%20that%20result%20in%20image%20generations%20that%20maximize%20the%20model%0Aloss.%20While%20these%20adversarial%20prompts%20result%20in%20diverse%20data%20informed%20by%20the%0Amodel%2C%20they%20are%20not%20informed%20of%20the%20target%20distribution%2C%20which%20can%20be%0Ainefficient.%20Therefore%2C%20we%20introduce%20the%20second%20feedback%20mechanism%20that%20guides%0Athe%20generation%20process%20towards%20a%20certain%20target%20distribution.%20We%20call%20the%0Amethod%20combining%20these%20two%20mechanisms%20Guided%20Adversarial%20Prompts.%20We%20perform%0Aour%20evaluations%20on%20different%20tasks%2C%20datasets%20and%20architectures%2C%20with%20different%0Atypes%20of%20distribution%20shifts%20%28spuriously%20correlated%20data%2C%20unseen%20domains%29%20and%0Ademonstrate%20the%20efficiency%20of%20the%20proposed%20feedback%20mechanisms%20compared%20to%0Aopen-loop%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15309v1&entry.124074799=Read"},
{"title": "You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object\n  Tracking", "author": "Xiyang Wang and Chunyun Fu and Jiawei He and Mingguang Huang and Ting Meng and Siyu Zhang and Hangning Zhou and Ziyao Xu and Chi Zhang", "abstract": "  In the classical tracking-by-detection (TBD) paradigm, detection and tracking\nare separately and sequentially conducted, and data association must be\nproperly performed to achieve satisfactory tracking performance. In this paper,\na new end-to-end multi-object tracking framework is proposed, which integrates\nobject detection and multi-object tracking into a single model. The proposed\ntracking framework eliminates the complex data association process in the\nclassical TBD paradigm, and requires no additional training. Secondly, the\nregression confidence of historical trajectories is investigated, and the\npossible states of a trajectory (weak object or strong object) in the current\nframe are predicted. Then, a confidence fusion module is designed to guide\nnon-maximum suppression for trajectories and detections to achieve ordered and\nrobust tracking. Thirdly, by integrating historical trajectory features, the\nregression performance of the detector is enhanced, which better reflects the\nocclusion and disappearance patterns of objects in real world. Lastly,\nextensive experiments are conducted on the commonly used KITTI and Waymo\ndatasets. The results show that the proposed framework can achieve robust\ntracking by using only a 2D detector and a 3D detector, and it is proven more\naccurate than many of the state-of-the-art TBD-based multi-modal tracking\nmethods. The source codes of the proposed method are available at\nhttps://github.com/wangxiyang2022/YONTD-MOT.\n", "link": "http://arxiv.org/abs/2304.08709v2", "date": "2024-03-22", "relevancy": 2.4196, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6335}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5904}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5821}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20You%20Only%20Need%20Two%20Detectors%20to%20Achieve%20Multi-Modal%203D%20Multi-Object%0A%20%20Tracking&body=Title%3A%20You%20Only%20Need%20Two%20Detectors%20to%20Achieve%20Multi-Modal%203D%20Multi-Object%0A%20%20Tracking%0AAuthor%3A%20Xiyang%20Wang%20and%20Chunyun%20Fu%20and%20Jiawei%20He%20and%20Mingguang%20Huang%20and%20Ting%20Meng%20and%20Siyu%20Zhang%20and%20Hangning%20Zhou%20and%20Ziyao%20Xu%20and%20Chi%20Zhang%0AAbstract%3A%20%20%20In%20the%20classical%20tracking-by-detection%20%28TBD%29%20paradigm%2C%20detection%20and%20tracking%0Aare%20separately%20and%20sequentially%20conducted%2C%20and%20data%20association%20must%20be%0Aproperly%20performed%20to%20achieve%20satisfactory%20tracking%20performance.%20In%20this%20paper%2C%0Aa%20new%20end-to-end%20multi-object%20tracking%20framework%20is%20proposed%2C%20which%20integrates%0Aobject%20detection%20and%20multi-object%20tracking%20into%20a%20single%20model.%20The%20proposed%0Atracking%20framework%20eliminates%20the%20complex%20data%20association%20process%20in%20the%0Aclassical%20TBD%20paradigm%2C%20and%20requires%20no%20additional%20training.%20Secondly%2C%20the%0Aregression%20confidence%20of%20historical%20trajectories%20is%20investigated%2C%20and%20the%0Apossible%20states%20of%20a%20trajectory%20%28weak%20object%20or%20strong%20object%29%20in%20the%20current%0Aframe%20are%20predicted.%20Then%2C%20a%20confidence%20fusion%20module%20is%20designed%20to%20guide%0Anon-maximum%20suppression%20for%20trajectories%20and%20detections%20to%20achieve%20ordered%20and%0Arobust%20tracking.%20Thirdly%2C%20by%20integrating%20historical%20trajectory%20features%2C%20the%0Aregression%20performance%20of%20the%20detector%20is%20enhanced%2C%20which%20better%20reflects%20the%0Aocclusion%20and%20disappearance%20patterns%20of%20objects%20in%20real%20world.%20Lastly%2C%0Aextensive%20experiments%20are%20conducted%20on%20the%20commonly%20used%20KITTI%20and%20Waymo%0Adatasets.%20The%20results%20show%20that%20the%20proposed%20framework%20can%20achieve%20robust%0Atracking%20by%20using%20only%20a%202D%20detector%20and%20a%203D%20detector%2C%20and%20it%20is%20proven%20more%0Aaccurate%20than%20many%20of%20the%20state-of-the-art%20TBD-based%20multi-modal%20tracking%0Amethods.%20The%20source%20codes%20of%20the%20proposed%20method%20are%20available%20at%0Ahttps%3A//github.com/wangxiyang2022/YONTD-MOT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.08709v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Only%20Need%20Two%20Detectors%20to%20Achieve%20Multi-Modal%203D%20Multi-Object%0A%20%20Tracking&entry.906535625=Xiyang%20Wang%20and%20Chunyun%20Fu%20and%20Jiawei%20He%20and%20Mingguang%20Huang%20and%20Ting%20Meng%20and%20Siyu%20Zhang%20and%20Hangning%20Zhou%20and%20Ziyao%20Xu%20and%20Chi%20Zhang&entry.1292438233=%20%20In%20the%20classical%20tracking-by-detection%20%28TBD%29%20paradigm%2C%20detection%20and%20tracking%0Aare%20separately%20and%20sequentially%20conducted%2C%20and%20data%20association%20must%20be%0Aproperly%20performed%20to%20achieve%20satisfactory%20tracking%20performance.%20In%20this%20paper%2C%0Aa%20new%20end-to-end%20multi-object%20tracking%20framework%20is%20proposed%2C%20which%20integrates%0Aobject%20detection%20and%20multi-object%20tracking%20into%20a%20single%20model.%20The%20proposed%0Atracking%20framework%20eliminates%20the%20complex%20data%20association%20process%20in%20the%0Aclassical%20TBD%20paradigm%2C%20and%20requires%20no%20additional%20training.%20Secondly%2C%20the%0Aregression%20confidence%20of%20historical%20trajectories%20is%20investigated%2C%20and%20the%0Apossible%20states%20of%20a%20trajectory%20%28weak%20object%20or%20strong%20object%29%20in%20the%20current%0Aframe%20are%20predicted.%20Then%2C%20a%20confidence%20fusion%20module%20is%20designed%20to%20guide%0Anon-maximum%20suppression%20for%20trajectories%20and%20detections%20to%20achieve%20ordered%20and%0Arobust%20tracking.%20Thirdly%2C%20by%20integrating%20historical%20trajectory%20features%2C%20the%0Aregression%20performance%20of%20the%20detector%20is%20enhanced%2C%20which%20better%20reflects%20the%0Aocclusion%20and%20disappearance%20patterns%20of%20objects%20in%20real%20world.%20Lastly%2C%0Aextensive%20experiments%20are%20conducted%20on%20the%20commonly%20used%20KITTI%20and%20Waymo%0Adatasets.%20The%20results%20show%20that%20the%20proposed%20framework%20can%20achieve%20robust%0Atracking%20by%20using%20only%20a%202D%20detector%20and%20a%203D%20detector%2C%20and%20it%20is%20proven%20more%0Aaccurate%20than%20many%20of%20the%20state-of-the-art%20TBD-based%20multi-modal%20tracking%0Amethods.%20The%20source%20codes%20of%20the%20proposed%20method%20are%20available%20at%0Ahttps%3A//github.com/wangxiyang2022/YONTD-MOT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.08709v2&entry.124074799=Read"},
{"title": "Semantics, Distortion, and Style Matter: Towards Source-free UDA for\n  Panoramic Segmentation", "author": "Xu Zheng and Pengyuan Zhou and Athanasios V. Vasilakos and Lin Wang", "abstract": "  This paper addresses an interesting yet challenging problem -- source-free\nunsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic\nsegmentation -- given only a pinhole image-trained model (i.e., source) and\nunlabeled panoramic images (i.e., target). Tackling this problem is nontrivial\ndue to the semantic mismatches, style discrepancies, and inevitable distortion\nof panoramic images. To this end, we propose a novel method that utilizes\nTangent Projection (TP) as it has less distortion and meanwhile slits the\nequirectangular projection (ERP) with a fixed FoV to mimic the pinhole images.\nBoth projections are shown effective in extracting knowledge from the source\nmodel. However, the distinct projection discrepancies between source and target\ndomains impede the direct knowledge transfer; thus, we propose a panoramic\nprototype adaptation module (PPAM) to integrate panoramic prototypes from the\nextracted knowledge for adaptation. We then impose the loss constraints on both\npredictions and prototypes and propose a cross-dual attention module (CDAM) at\nthe feature level to better align the spatial and channel characteristics\nacross the domains and projections. Both knowledge extraction and transfer\nprocesses are synchronously updated to reach the best performance. Extensive\nexperiments on the synthetic and real-world benchmarks, including outdoor and\nindoor scenarios, demonstrate that our method achieves significantly better\nperformance than prior SFUDA methods for pinhole-to-panoramic adaptation.\n", "link": "http://arxiv.org/abs/2403.12505v2", "date": "2024-03-22", "relevancy": 2.4091, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6054}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5553}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantics%2C%20Distortion%2C%20and%20Style%20Matter%3A%20Towards%20Source-free%20UDA%20for%0A%20%20Panoramic%20Segmentation&body=Title%3A%20Semantics%2C%20Distortion%2C%20and%20Style%20Matter%3A%20Towards%20Source-free%20UDA%20for%0A%20%20Panoramic%20Segmentation%0AAuthor%3A%20Xu%20Zheng%20and%20Pengyuan%20Zhou%20and%20Athanasios%20V.%20Vasilakos%20and%20Lin%20Wang%0AAbstract%3A%20%20%20This%20paper%20addresses%20an%20interesting%20yet%20challenging%20problem%20--%20source-free%0Aunsupervised%20domain%20adaptation%20%28SFUDA%29%20for%20pinhole-to-panoramic%20semantic%0Asegmentation%20--%20given%20only%20a%20pinhole%20image-trained%20model%20%28i.e.%2C%20source%29%20and%0Aunlabeled%20panoramic%20images%20%28i.e.%2C%20target%29.%20Tackling%20this%20problem%20is%20nontrivial%0Adue%20to%20the%20semantic%20mismatches%2C%20style%20discrepancies%2C%20and%20inevitable%20distortion%0Aof%20panoramic%20images.%20To%20this%20end%2C%20we%20propose%20a%20novel%20method%20that%20utilizes%0ATangent%20Projection%20%28TP%29%20as%20it%20has%20less%20distortion%20and%20meanwhile%20slits%20the%0Aequirectangular%20projection%20%28ERP%29%20with%20a%20fixed%20FoV%20to%20mimic%20the%20pinhole%20images.%0ABoth%20projections%20are%20shown%20effective%20in%20extracting%20knowledge%20from%20the%20source%0Amodel.%20However%2C%20the%20distinct%20projection%20discrepancies%20between%20source%20and%20target%0Adomains%20impede%20the%20direct%20knowledge%20transfer%3B%20thus%2C%20we%20propose%20a%20panoramic%0Aprototype%20adaptation%20module%20%28PPAM%29%20to%20integrate%20panoramic%20prototypes%20from%20the%0Aextracted%20knowledge%20for%20adaptation.%20We%20then%20impose%20the%20loss%20constraints%20on%20both%0Apredictions%20and%20prototypes%20and%20propose%20a%20cross-dual%20attention%20module%20%28CDAM%29%20at%0Athe%20feature%20level%20to%20better%20align%20the%20spatial%20and%20channel%20characteristics%0Aacross%20the%20domains%20and%20projections.%20Both%20knowledge%20extraction%20and%20transfer%0Aprocesses%20are%20synchronously%20updated%20to%20reach%20the%20best%20performance.%20Extensive%0Aexperiments%20on%20the%20synthetic%20and%20real-world%20benchmarks%2C%20including%20outdoor%20and%0Aindoor%20scenarios%2C%20demonstrate%20that%20our%20method%20achieves%20significantly%20better%0Aperformance%20than%20prior%20SFUDA%20methods%20for%20pinhole-to-panoramic%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12505v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantics%2C%20Distortion%2C%20and%20Style%20Matter%3A%20Towards%20Source-free%20UDA%20for%0A%20%20Panoramic%20Segmentation&entry.906535625=Xu%20Zheng%20and%20Pengyuan%20Zhou%20and%20Athanasios%20V.%20Vasilakos%20and%20Lin%20Wang&entry.1292438233=%20%20This%20paper%20addresses%20an%20interesting%20yet%20challenging%20problem%20--%20source-free%0Aunsupervised%20domain%20adaptation%20%28SFUDA%29%20for%20pinhole-to-panoramic%20semantic%0Asegmentation%20--%20given%20only%20a%20pinhole%20image-trained%20model%20%28i.e.%2C%20source%29%20and%0Aunlabeled%20panoramic%20images%20%28i.e.%2C%20target%29.%20Tackling%20this%20problem%20is%20nontrivial%0Adue%20to%20the%20semantic%20mismatches%2C%20style%20discrepancies%2C%20and%20inevitable%20distortion%0Aof%20panoramic%20images.%20To%20this%20end%2C%20we%20propose%20a%20novel%20method%20that%20utilizes%0ATangent%20Projection%20%28TP%29%20as%20it%20has%20less%20distortion%20and%20meanwhile%20slits%20the%0Aequirectangular%20projection%20%28ERP%29%20with%20a%20fixed%20FoV%20to%20mimic%20the%20pinhole%20images.%0ABoth%20projections%20are%20shown%20effective%20in%20extracting%20knowledge%20from%20the%20source%0Amodel.%20However%2C%20the%20distinct%20projection%20discrepancies%20between%20source%20and%20target%0Adomains%20impede%20the%20direct%20knowledge%20transfer%3B%20thus%2C%20we%20propose%20a%20panoramic%0Aprototype%20adaptation%20module%20%28PPAM%29%20to%20integrate%20panoramic%20prototypes%20from%20the%0Aextracted%20knowledge%20for%20adaptation.%20We%20then%20impose%20the%20loss%20constraints%20on%20both%0Apredictions%20and%20prototypes%20and%20propose%20a%20cross-dual%20attention%20module%20%28CDAM%29%20at%0Athe%20feature%20level%20to%20better%20align%20the%20spatial%20and%20channel%20characteristics%0Aacross%20the%20domains%20and%20projections.%20Both%20knowledge%20extraction%20and%20transfer%0Aprocesses%20are%20synchronously%20updated%20to%20reach%20the%20best%20performance.%20Extensive%0Aexperiments%20on%20the%20synthetic%20and%20real-world%20benchmarks%2C%20including%20outdoor%20and%0Aindoor%20scenarios%2C%20demonstrate%20that%20our%20method%20achieves%20significantly%20better%0Aperformance%20than%20prior%20SFUDA%20methods%20for%20pinhole-to-panoramic%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12505v2&entry.124074799=Read"},
{"title": "InternVideo2: Scaling Video Foundation Models for Multimodal Video\n  Understanding", "author": "Yi Wang and Kunchang Li and Xinhao Li and Jiashuo Yu and Yinan He and Guo Chen and Baoqi Pei and Rongkun Zheng and Jilan Xu and Zun Wang and Yansong Shi and Tianxiang Jiang and Songze Li and Hongjie Zhang and Yifei Huang and Yu Qiao and Yali Wang and Limin Wang", "abstract": "  We introduce InternVideo2, a new video foundation model (ViFM) that achieves\nthe state-of-the-art performance in action recognition, video-text tasks, and\nvideo-centric dialogue. Our approach employs a progressive training paradigm\nthat unifies the different self- or weakly-supervised learning frameworks of\nmasked video token reconstruction, cross-modal contrastive learning, and next\ntoken prediction. Different training stages would guide our model to capture\ndifferent levels of structure and semantic information through different\npretext tasks. At the data level, we prioritize the spatiotemporal consistency\nby semantically segmenting videos and generating video-audio-speech captions.\nThis improves the alignment between video and text. We scale both data and\nmodel size for our InternVideo2. Through extensive experiments, we validate our\ndesigns and demonstrate the state-of-the-art performance on over 60 video and\naudio tasks. Notably, our model outperforms others on various video-related\ncaptioning, dialogue, and long video understanding benchmarks, highlighting its\nability to reason and comprehend long temporal contexts. Code and models are\navailable at https://github.com/OpenGVLab/InternVideo2/.\n", "link": "http://arxiv.org/abs/2403.15377v1", "date": "2024-03-22", "relevancy": 2.4073, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6138}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6082}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5873}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20InternVideo2%3A%20Scaling%20Video%20Foundation%20Models%20for%20Multimodal%20Video%0A%20%20Understanding&body=Title%3A%20InternVideo2%3A%20Scaling%20Video%20Foundation%20Models%20for%20Multimodal%20Video%0A%20%20Understanding%0AAuthor%3A%20Yi%20Wang%20and%20Kunchang%20Li%20and%20Xinhao%20Li%20and%20Jiashuo%20Yu%20and%20Yinan%20He%20and%20Guo%20Chen%20and%20Baoqi%20Pei%20and%20Rongkun%20Zheng%20and%20Jilan%20Xu%20and%20Zun%20Wang%20and%20Yansong%20Shi%20and%20Tianxiang%20Jiang%20and%20Songze%20Li%20and%20Hongjie%20Zhang%20and%20Yifei%20Huang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20%20%20We%20introduce%20InternVideo2%2C%20a%20new%20video%20foundation%20model%20%28ViFM%29%20that%20achieves%0Athe%20state-of-the-art%20performance%20in%20action%20recognition%2C%20video-text%20tasks%2C%20and%0Avideo-centric%20dialogue.%20Our%20approach%20employs%20a%20progressive%20training%20paradigm%0Athat%20unifies%20the%20different%20self-%20or%20weakly-supervised%20learning%20frameworks%20of%0Amasked%20video%20token%20reconstruction%2C%20cross-modal%20contrastive%20learning%2C%20and%20next%0Atoken%20prediction.%20Different%20training%20stages%20would%20guide%20our%20model%20to%20capture%0Adifferent%20levels%20of%20structure%20and%20semantic%20information%20through%20different%0Apretext%20tasks.%20At%20the%20data%20level%2C%20we%20prioritize%20the%20spatiotemporal%20consistency%0Aby%20semantically%20segmenting%20videos%20and%20generating%20video-audio-speech%20captions.%0AThis%20improves%20the%20alignment%20between%20video%20and%20text.%20We%20scale%20both%20data%20and%0Amodel%20size%20for%20our%20InternVideo2.%20Through%20extensive%20experiments%2C%20we%20validate%20our%0Adesigns%20and%20demonstrate%20the%20state-of-the-art%20performance%20on%20over%2060%20video%20and%0Aaudio%20tasks.%20Notably%2C%20our%20model%20outperforms%20others%20on%20various%20video-related%0Acaptioning%2C%20dialogue%2C%20and%20long%20video%20understanding%20benchmarks%2C%20highlighting%20its%0Aability%20to%20reason%20and%20comprehend%20long%20temporal%20contexts.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/OpenGVLab/InternVideo2/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15377v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternVideo2%3A%20Scaling%20Video%20Foundation%20Models%20for%20Multimodal%20Video%0A%20%20Understanding&entry.906535625=Yi%20Wang%20and%20Kunchang%20Li%20and%20Xinhao%20Li%20and%20Jiashuo%20Yu%20and%20Yinan%20He%20and%20Guo%20Chen%20and%20Baoqi%20Pei%20and%20Rongkun%20Zheng%20and%20Jilan%20Xu%20and%20Zun%20Wang%20and%20Yansong%20Shi%20and%20Tianxiang%20Jiang%20and%20Songze%20Li%20and%20Hongjie%20Zhang%20and%20Yifei%20Huang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang&entry.1292438233=%20%20We%20introduce%20InternVideo2%2C%20a%20new%20video%20foundation%20model%20%28ViFM%29%20that%20achieves%0Athe%20state-of-the-art%20performance%20in%20action%20recognition%2C%20video-text%20tasks%2C%20and%0Avideo-centric%20dialogue.%20Our%20approach%20employs%20a%20progressive%20training%20paradigm%0Athat%20unifies%20the%20different%20self-%20or%20weakly-supervised%20learning%20frameworks%20of%0Amasked%20video%20token%20reconstruction%2C%20cross-modal%20contrastive%20learning%2C%20and%20next%0Atoken%20prediction.%20Different%20training%20stages%20would%20guide%20our%20model%20to%20capture%0Adifferent%20levels%20of%20structure%20and%20semantic%20information%20through%20different%0Apretext%20tasks.%20At%20the%20data%20level%2C%20we%20prioritize%20the%20spatiotemporal%20consistency%0Aby%20semantically%20segmenting%20videos%20and%20generating%20video-audio-speech%20captions.%0AThis%20improves%20the%20alignment%20between%20video%20and%20text.%20We%20scale%20both%20data%20and%0Amodel%20size%20for%20our%20InternVideo2.%20Through%20extensive%20experiments%2C%20we%20validate%20our%0Adesigns%20and%20demonstrate%20the%20state-of-the-art%20performance%20on%20over%2060%20video%20and%0Aaudio%20tasks.%20Notably%2C%20our%20model%20outperforms%20others%20on%20various%20video-related%0Acaptioning%2C%20dialogue%2C%20and%20long%20video%20understanding%20benchmarks%2C%20highlighting%20its%0Aability%20to%20reason%20and%20comprehend%20long%20temporal%20contexts.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/OpenGVLab/InternVideo2/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15377v1&entry.124074799=Read"},
{"title": "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding", "author": "Chris Kelly and Luhui Hu and Jiayin Hu and Yu Tian and Deshun Yang and Bang Yang and Cindy Yang and Zihao Li and Zaoshan Huang and Yuexian Zou", "abstract": "  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n", "link": "http://arxiv.org/abs/2403.09530v2", "date": "2024-03-22", "relevancy": 2.4034, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6291}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5808}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VisionGPT-3D%3A%20A%20Generalized%20Multimodal%20Agent%20for%20Enhanced%203D%20Vision%0A%20%20Understanding&body=Title%3A%20VisionGPT-3D%3A%20A%20Generalized%20Multimodal%20Agent%20for%20Enhanced%203D%20Vision%0A%20%20Understanding%0AAuthor%3A%20Chris%20Kelly%20and%20Luhui%20Hu%20and%20Jiayin%20Hu%20and%20Yu%20Tian%20and%20Deshun%20Yang%20and%20Bang%20Yang%20and%20Cindy%20Yang%20and%20Zihao%20Li%20and%20Zaoshan%20Huang%20and%20Yuexian%20Zou%0AAbstract%3A%20%20%20The%20evolution%20of%20text%20to%20visual%20components%20facilitates%20people%27s%20daily%20lives%2C%0Asuch%20as%20generating%20image%2C%20videos%20from%20text%20and%20identifying%20the%20desired%20elements%0Awithin%20the%20images.%20Computer%20vision%20models%20involving%20the%20multimodal%20abilities%20in%0Athe%20previous%20days%20are%20focused%20on%20image%20detection%2C%20classification%20based%20on%0Awell-defined%20objects.%20Large%20language%20models%20%28LLMs%29%20introduces%20the%0Atransformation%20from%20nature%20language%20to%20visual%20objects%2C%20which%20present%20the%20visual%0Alayout%20for%20text%20contexts.%20OpenAI%20GPT-4%20has%20emerged%20as%20the%20pinnacle%20in%20LLMs%2C%0Awhile%20the%20computer%20vision%20%28CV%29%20domain%20boasts%20a%20plethora%20of%20state-of-the-art%0A%28SOTA%29%20models%20and%20algorithms%20to%20convert%202D%20images%20to%20their%203D%20representations.%0AHowever%2C%20the%20mismatching%20between%20the%20algorithms%20with%20the%20problem%20could%20lead%20to%0Aundesired%20results.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20an%20unified%0AVisionGPT-3D%20framework%20to%20consolidate%20the%20state-of-the-art%20vision%20models%2C%0Athereby%20facilitating%20the%20development%20of%20vision-oriented%20AI.%20VisionGPT-3D%0Aprovides%20a%20versatile%20multimodal%20framework%20building%20upon%20the%20strengths%20of%0Amultimodal%20foundation%20models.%20It%20seamlessly%20integrates%20various%20SOTA%20vision%0Amodels%20and%20brings%20the%20automation%20in%20the%20selection%20of%20SOTA%20vision%20models%2C%0Aidentifies%20the%20suitable%203D%20mesh%20creation%20algorithms%20corresponding%20to%202D%20depth%0Amaps%20analysis%2C%20generates%20optimal%20results%20based%20on%20diverse%20multimodal%20inputs%0Asuch%20as%20text%20prompts.%0A%20%20Keywords%3A%20VisionGPT-3D%2C%203D%20vision%20understanding%2C%20Multimodal%20agent%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09530v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionGPT-3D%3A%20A%20Generalized%20Multimodal%20Agent%20for%20Enhanced%203D%20Vision%0A%20%20Understanding&entry.906535625=Chris%20Kelly%20and%20Luhui%20Hu%20and%20Jiayin%20Hu%20and%20Yu%20Tian%20and%20Deshun%20Yang%20and%20Bang%20Yang%20and%20Cindy%20Yang%20and%20Zihao%20Li%20and%20Zaoshan%20Huang%20and%20Yuexian%20Zou&entry.1292438233=%20%20The%20evolution%20of%20text%20to%20visual%20components%20facilitates%20people%27s%20daily%20lives%2C%0Asuch%20as%20generating%20image%2C%20videos%20from%20text%20and%20identifying%20the%20desired%20elements%0Awithin%20the%20images.%20Computer%20vision%20models%20involving%20the%20multimodal%20abilities%20in%0Athe%20previous%20days%20are%20focused%20on%20image%20detection%2C%20classification%20based%20on%0Awell-defined%20objects.%20Large%20language%20models%20%28LLMs%29%20introduces%20the%0Atransformation%20from%20nature%20language%20to%20visual%20objects%2C%20which%20present%20the%20visual%0Alayout%20for%20text%20contexts.%20OpenAI%20GPT-4%20has%20emerged%20as%20the%20pinnacle%20in%20LLMs%2C%0Awhile%20the%20computer%20vision%20%28CV%29%20domain%20boasts%20a%20plethora%20of%20state-of-the-art%0A%28SOTA%29%20models%20and%20algorithms%20to%20convert%202D%20images%20to%20their%203D%20representations.%0AHowever%2C%20the%20mismatching%20between%20the%20algorithms%20with%20the%20problem%20could%20lead%20to%0Aundesired%20results.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20an%20unified%0AVisionGPT-3D%20framework%20to%20consolidate%20the%20state-of-the-art%20vision%20models%2C%0Athereby%20facilitating%20the%20development%20of%20vision-oriented%20AI.%20VisionGPT-3D%0Aprovides%20a%20versatile%20multimodal%20framework%20building%20upon%20the%20strengths%20of%0Amultimodal%20foundation%20models.%20It%20seamlessly%20integrates%20various%20SOTA%20vision%0Amodels%20and%20brings%20the%20automation%20in%20the%20selection%20of%20SOTA%20vision%20models%2C%0Aidentifies%20the%20suitable%203D%20mesh%20creation%20algorithms%20corresponding%20to%202D%20depth%0Amaps%20analysis%2C%20generates%20optimal%20results%20based%20on%20diverse%20multimodal%20inputs%0Asuch%20as%20text%20prompts.%0A%20%20Keywords%3A%20VisionGPT-3D%2C%203D%20vision%20understanding%2C%20Multimodal%20agent%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09530v2&entry.124074799=Read"},
{"title": "TriHelper: Zero-Shot Object Navigation with Dynamic Assistance", "author": "Lingfeng Zhang and Qiang Zhang and Hao Wang and Erjia Xiao and Zixuan Jiang and Honglei Chen and Renjing Xu", "abstract": "  Navigating toward specific objects in unknown environments without additional\ntraining, known as Zero-Shot object navigation, poses a significant challenge\nin the field of robotics, which demands high levels of auxiliary information\nand strategic planning. Traditional works have focused on holistic solutions,\noverlooking the specific challenges agents encounter during navigation such as\ncollision, low exploration efficiency, and misidentification of targets. To\naddress these challenges, our work proposes TriHelper, a novel framework\ndesigned to assist agents dynamically through three primary navigation\nchallenges: collision, exploration, and detection. Specifically, our framework\nconsists of three innovative components: (i) Collision Helper, (ii) Exploration\nHelper, and (iii) Detection Helper. These components work collaboratively to\nsolve these challenges throughout the navigation process. Experiments on the\nHabitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper\nsignificantly outperforms all existing baseline methods in Zero-Shot object\nnavigation, showcasing superior success rates and exploration efficiency. Our\nablation studies further underscore the effectiveness of each helper in\naddressing their respective challenges, notably enhancing the agent's\nnavigation capabilities. By proposing TriHelper, we offer a fresh perspective\non advancing the object navigation task, paving the way for future research in\nthe domain of Embodied AI and visual-based navigation.\n", "link": "http://arxiv.org/abs/2403.15223v1", "date": "2024-03-22", "relevancy": 2.3994, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6118}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5924}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5884}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TriHelper%3A%20Zero-Shot%20Object%20Navigation%20with%20Dynamic%20Assistance&body=Title%3A%20TriHelper%3A%20Zero-Shot%20Object%20Navigation%20with%20Dynamic%20Assistance%0AAuthor%3A%20Lingfeng%20Zhang%20and%20Qiang%20Zhang%20and%20Hao%20Wang%20and%20Erjia%20Xiao%20and%20Zixuan%20Jiang%20and%20Honglei%20Chen%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Navigating%20toward%20specific%20objects%20in%20unknown%20environments%20without%20additional%0Atraining%2C%20known%20as%20Zero-Shot%20object%20navigation%2C%20poses%20a%20significant%20challenge%0Ain%20the%20field%20of%20robotics%2C%20which%20demands%20high%20levels%20of%20auxiliary%20information%0Aand%20strategic%20planning.%20Traditional%20works%20have%20focused%20on%20holistic%20solutions%2C%0Aoverlooking%20the%20specific%20challenges%20agents%20encounter%20during%20navigation%20such%20as%0Acollision%2C%20low%20exploration%20efficiency%2C%20and%20misidentification%20of%20targets.%20To%0Aaddress%20these%20challenges%2C%20our%20work%20proposes%20TriHelper%2C%20a%20novel%20framework%0Adesigned%20to%20assist%20agents%20dynamically%20through%20three%20primary%20navigation%0Achallenges%3A%20collision%2C%20exploration%2C%20and%20detection.%20Specifically%2C%20our%20framework%0Aconsists%20of%20three%20innovative%20components%3A%20%28i%29%20Collision%20Helper%2C%20%28ii%29%20Exploration%0AHelper%2C%20and%20%28iii%29%20Detection%20Helper.%20These%20components%20work%20collaboratively%20to%0Asolve%20these%20challenges%20throughout%20the%20navigation%20process.%20Experiments%20on%20the%0AHabitat-Matterport%203D%20%28HM3D%29%20and%20Gibson%20datasets%20demonstrate%20that%20TriHelper%0Asignificantly%20outperforms%20all%20existing%20baseline%20methods%20in%20Zero-Shot%20object%0Anavigation%2C%20showcasing%20superior%20success%20rates%20and%20exploration%20efficiency.%20Our%0Aablation%20studies%20further%20underscore%20the%20effectiveness%20of%20each%20helper%20in%0Aaddressing%20their%20respective%20challenges%2C%20notably%20enhancing%20the%20agent%27s%0Anavigation%20capabilities.%20By%20proposing%20TriHelper%2C%20we%20offer%20a%20fresh%20perspective%0Aon%20advancing%20the%20object%20navigation%20task%2C%20paving%20the%20way%20for%20future%20research%20in%0Athe%20domain%20of%20Embodied%20AI%20and%20visual-based%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15223v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TriHelper%3A%20Zero-Shot%20Object%20Navigation%20with%20Dynamic%20Assistance&entry.906535625=Lingfeng%20Zhang%20and%20Qiang%20Zhang%20and%20Hao%20Wang%20and%20Erjia%20Xiao%20and%20Zixuan%20Jiang%20and%20Honglei%20Chen%20and%20Renjing%20Xu&entry.1292438233=%20%20Navigating%20toward%20specific%20objects%20in%20unknown%20environments%20without%20additional%0Atraining%2C%20known%20as%20Zero-Shot%20object%20navigation%2C%20poses%20a%20significant%20challenge%0Ain%20the%20field%20of%20robotics%2C%20which%20demands%20high%20levels%20of%20auxiliary%20information%0Aand%20strategic%20planning.%20Traditional%20works%20have%20focused%20on%20holistic%20solutions%2C%0Aoverlooking%20the%20specific%20challenges%20agents%20encounter%20during%20navigation%20such%20as%0Acollision%2C%20low%20exploration%20efficiency%2C%20and%20misidentification%20of%20targets.%20To%0Aaddress%20these%20challenges%2C%20our%20work%20proposes%20TriHelper%2C%20a%20novel%20framework%0Adesigned%20to%20assist%20agents%20dynamically%20through%20three%20primary%20navigation%0Achallenges%3A%20collision%2C%20exploration%2C%20and%20detection.%20Specifically%2C%20our%20framework%0Aconsists%20of%20three%20innovative%20components%3A%20%28i%29%20Collision%20Helper%2C%20%28ii%29%20Exploration%0AHelper%2C%20and%20%28iii%29%20Detection%20Helper.%20These%20components%20work%20collaboratively%20to%0Asolve%20these%20challenges%20throughout%20the%20navigation%20process.%20Experiments%20on%20the%0AHabitat-Matterport%203D%20%28HM3D%29%20and%20Gibson%20datasets%20demonstrate%20that%20TriHelper%0Asignificantly%20outperforms%20all%20existing%20baseline%20methods%20in%20Zero-Shot%20object%0Anavigation%2C%20showcasing%20superior%20success%20rates%20and%20exploration%20efficiency.%20Our%0Aablation%20studies%20further%20underscore%20the%20effectiveness%20of%20each%20helper%20in%0Aaddressing%20their%20respective%20challenges%2C%20notably%20enhancing%20the%20agent%27s%0Anavigation%20capabilities.%20By%20proposing%20TriHelper%2C%20we%20offer%20a%20fresh%20perspective%0Aon%20advancing%20the%20object%20navigation%20task%2C%20paving%20the%20way%20for%20future%20research%20in%0Athe%20domain%20of%20Embodied%20AI%20and%20visual-based%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15223v1&entry.124074799=Read"},
{"title": "SyncTweedies: A General Generative Framework Based on Synchronized\n  Diffusions", "author": "Jaihoon Kim and Juil Koo and Kyeongmin Yeo and Minhyuk Sung", "abstract": "  We introduce a general framework for generating diverse visual content,\nincluding ambiguous images, panorama images, mesh textures, and Gaussian splat\ntextures, by synchronizing multiple diffusion processes. We present exhaustive\ninvestigation into all possible scenarios for synchronizing multiple diffusion\nprocesses through a canonical space and analyze their characteristics across\napplications. In doing so, we reveal a previously unexplored case: averaging\nthe outputs of Tweedie's formula while conducting denoising in multiple\ninstance spaces. This case also provides the best quality with the widest\napplicability to downstream tasks. We name this case SyncTweedies. In our\nexperiments generating visual content aforementioned, we demonstrate the\nsuperior quality of generation by SyncTweedies compared to other\nsynchronization methods, optimization-based and iterative-update-based methods.\n", "link": "http://arxiv.org/abs/2403.14370v2", "date": "2024-03-22", "relevancy": 2.3796, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6103}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5921}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5806}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SyncTweedies%3A%20A%20General%20Generative%20Framework%20Based%20on%20Synchronized%0A%20%20Diffusions&body=Title%3A%20SyncTweedies%3A%20A%20General%20Generative%20Framework%20Based%20on%20Synchronized%0A%20%20Diffusions%0AAuthor%3A%20Jaihoon%20Kim%20and%20Juil%20Koo%20and%20Kyeongmin%20Yeo%20and%20Minhyuk%20Sung%0AAbstract%3A%20%20%20We%20introduce%20a%20general%20framework%20for%20generating%20diverse%20visual%20content%2C%0Aincluding%20ambiguous%20images%2C%20panorama%20images%2C%20mesh%20textures%2C%20and%20Gaussian%20splat%0Atextures%2C%20by%20synchronizing%20multiple%20diffusion%20processes.%20We%20present%20exhaustive%0Ainvestigation%20into%20all%20possible%20scenarios%20for%20synchronizing%20multiple%20diffusion%0Aprocesses%20through%20a%20canonical%20space%20and%20analyze%20their%20characteristics%20across%0Aapplications.%20In%20doing%20so%2C%20we%20reveal%20a%20previously%20unexplored%20case%3A%20averaging%0Athe%20outputs%20of%20Tweedie%27s%20formula%20while%20conducting%20denoising%20in%20multiple%0Ainstance%20spaces.%20This%20case%20also%20provides%20the%20best%20quality%20with%20the%20widest%0Aapplicability%20to%20downstream%20tasks.%20We%20name%20this%20case%20SyncTweedies.%20In%20our%0Aexperiments%20generating%20visual%20content%20aforementioned%2C%20we%20demonstrate%20the%0Asuperior%20quality%20of%20generation%20by%20SyncTweedies%20compared%20to%20other%0Asynchronization%20methods%2C%20optimization-based%20and%20iterative-update-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14370v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyncTweedies%3A%20A%20General%20Generative%20Framework%20Based%20on%20Synchronized%0A%20%20Diffusions&entry.906535625=Jaihoon%20Kim%20and%20Juil%20Koo%20and%20Kyeongmin%20Yeo%20and%20Minhyuk%20Sung&entry.1292438233=%20%20We%20introduce%20a%20general%20framework%20for%20generating%20diverse%20visual%20content%2C%0Aincluding%20ambiguous%20images%2C%20panorama%20images%2C%20mesh%20textures%2C%20and%20Gaussian%20splat%0Atextures%2C%20by%20synchronizing%20multiple%20diffusion%20processes.%20We%20present%20exhaustive%0Ainvestigation%20into%20all%20possible%20scenarios%20for%20synchronizing%20multiple%20diffusion%0Aprocesses%20through%20a%20canonical%20space%20and%20analyze%20their%20characteristics%20across%0Aapplications.%20In%20doing%20so%2C%20we%20reveal%20a%20previously%20unexplored%20case%3A%20averaging%0Athe%20outputs%20of%20Tweedie%27s%20formula%20while%20conducting%20denoising%20in%20multiple%0Ainstance%20spaces.%20This%20case%20also%20provides%20the%20best%20quality%20with%20the%20widest%0Aapplicability%20to%20downstream%20tasks.%20We%20name%20this%20case%20SyncTweedies.%20In%20our%0Aexperiments%20generating%20visual%20content%20aforementioned%2C%20we%20demonstrate%20the%0Asuperior%20quality%20of%20generation%20by%20SyncTweedies%20compared%20to%20other%0Asynchronization%20methods%2C%20optimization-based%20and%20iterative-update-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14370v2&entry.124074799=Read"},
{"title": "SIMAP: A simplicial-map layer for neural networks", "author": "Rocio Gonzalez-Diaz and Miguel A. Guti\u00e9rrez-Naranjo and Eduardo Paluzo-Hidalgo", "abstract": "  In this paper, we present SIMAP, a novel layer integrated into deep learning\nmodels, aimed at enhancing the interpretability of the output. The SIMAP layer\nis an enhanced version of Simplicial-Map Neural Networks (SMNNs), an\nexplainable neural network based on support sets and simplicial maps (functions\nused in topology to transform shapes while preserving their structural\nconnectivity). The novelty of the methodology proposed in this paper is\ntwo-fold: Firstly, SIMAP layers work in combination with other deep learning\narchitectures as an interpretable layer substituting classic dense final\nlayers. Secondly, unlike SMNNs, the support set is based on a fixed maximal\nsimplex, the barycentric subdivision being efficiently computed with a\nmatrix-based multiplication algorithm.\n", "link": "http://arxiv.org/abs/2403.15083v1", "date": "2024-03-22", "relevancy": 2.3724, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5045}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4603}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4587}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SIMAP%3A%20A%20simplicial-map%20layer%20for%20neural%20networks&body=Title%3A%20SIMAP%3A%20A%20simplicial-map%20layer%20for%20neural%20networks%0AAuthor%3A%20Rocio%20Gonzalez-Diaz%20and%20Miguel%20A.%20Guti%C3%A9rrez-Naranjo%20and%20Eduardo%20Paluzo-Hidalgo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20SIMAP%2C%20a%20novel%20layer%20integrated%20into%20deep%20learning%0Amodels%2C%20aimed%20at%20enhancing%20the%20interpretability%20of%20the%20output.%20The%20SIMAP%20layer%0Ais%20an%20enhanced%20version%20of%20Simplicial-Map%20Neural%20Networks%20%28SMNNs%29%2C%20an%0Aexplainable%20neural%20network%20based%20on%20support%20sets%20and%20simplicial%20maps%20%28functions%0Aused%20in%20topology%20to%20transform%20shapes%20while%20preserving%20their%20structural%0Aconnectivity%29.%20The%20novelty%20of%20the%20methodology%20proposed%20in%20this%20paper%20is%0Atwo-fold%3A%20Firstly%2C%20SIMAP%20layers%20work%20in%20combination%20with%20other%20deep%20learning%0Aarchitectures%20as%20an%20interpretable%20layer%20substituting%20classic%20dense%20final%0Alayers.%20Secondly%2C%20unlike%20SMNNs%2C%20the%20support%20set%20is%20based%20on%20a%20fixed%20maximal%0Asimplex%2C%20the%20barycentric%20subdivision%20being%20efficiently%20computed%20with%20a%0Amatrix-based%20multiplication%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15083v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIMAP%3A%20A%20simplicial-map%20layer%20for%20neural%20networks&entry.906535625=Rocio%20Gonzalez-Diaz%20and%20Miguel%20A.%20Guti%C3%A9rrez-Naranjo%20and%20Eduardo%20Paluzo-Hidalgo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20SIMAP%2C%20a%20novel%20layer%20integrated%20into%20deep%20learning%0Amodels%2C%20aimed%20at%20enhancing%20the%20interpretability%20of%20the%20output.%20The%20SIMAP%20layer%0Ais%20an%20enhanced%20version%20of%20Simplicial-Map%20Neural%20Networks%20%28SMNNs%29%2C%20an%0Aexplainable%20neural%20network%20based%20on%20support%20sets%20and%20simplicial%20maps%20%28functions%0Aused%20in%20topology%20to%20transform%20shapes%20while%20preserving%20their%20structural%0Aconnectivity%29.%20The%20novelty%20of%20the%20methodology%20proposed%20in%20this%20paper%20is%0Atwo-fold%3A%20Firstly%2C%20SIMAP%20layers%20work%20in%20combination%20with%20other%20deep%20learning%0Aarchitectures%20as%20an%20interpretable%20layer%20substituting%20classic%20dense%20final%0Alayers.%20Secondly%2C%20unlike%20SMNNs%2C%20the%20support%20set%20is%20based%20on%20a%20fixed%20maximal%0Asimplex%2C%20the%20barycentric%20subdivision%20being%20efficiently%20computed%20with%20a%0Amatrix-based%20multiplication%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15083v1&entry.124074799=Read"},
{"title": "Residual Denoising Diffusion Models", "author": "Jiawei Liu and Qiang Wang and Huijie Fan and Yinong Wang and Yandong Tang and Liangqiong Qu", "abstract": "  We propose residual denoising diffusion models (RDDM), a novel dual diffusion\nprocess that decouples the traditional single denoising diffusion process into\nresidual diffusion and noise diffusion. This dual diffusion framework expands\nthe denoising-based diffusion models, initially uninterpretable for image\nrestoration, into a unified and interpretable model for both image generation\nand restoration by introducing residuals. Specifically, our residual diffusion\nrepresents directional diffusion from the target image to the degraded input\nimage and explicitly guides the reverse generation process for image\nrestoration, while noise diffusion represents random perturbations in the\ndiffusion process. The residual prioritizes certainty, while the noise\nemphasizes diversity, enabling RDDM to effectively unify tasks with varying\ncertainty or diversity requirements, such as image generation and restoration.\nWe demonstrate that our sampling process is consistent with that of DDPM and\nDDIM through coefficient transformation, and propose a partially\npath-independent generation process to better understand the reverse process.\nNotably, our RDDM enables a generic UNet, trained with only an L1 loss and a\nbatch size of 1, to compete with state-of-the-art image restoration methods. We\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/nachifur/RDDM).\n", "link": "http://arxiv.org/abs/2308.13712v3", "date": "2024-03-22", "relevancy": 2.3584, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6017}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5944}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.58}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Residual%20Denoising%20Diffusion%20Models&body=Title%3A%20Residual%20Denoising%20Diffusion%20Models%0AAuthor%3A%20Jiawei%20Liu%20and%20Qiang%20Wang%20and%20Huijie%20Fan%20and%20Yinong%20Wang%20and%20Yandong%20Tang%20and%20Liangqiong%20Qu%0AAbstract%3A%20%20%20We%20propose%20residual%20denoising%20diffusion%20models%20%28RDDM%29%2C%20a%20novel%20dual%20diffusion%0Aprocess%20that%20decouples%20the%20traditional%20single%20denoising%20diffusion%20process%20into%0Aresidual%20diffusion%20and%20noise%20diffusion.%20This%20dual%20diffusion%20framework%20expands%0Athe%20denoising-based%20diffusion%20models%2C%20initially%20uninterpretable%20for%20image%0Arestoration%2C%20into%20a%20unified%20and%20interpretable%20model%20for%20both%20image%20generation%0Aand%20restoration%20by%20introducing%20residuals.%20Specifically%2C%20our%20residual%20diffusion%0Arepresents%20directional%20diffusion%20from%20the%20target%20image%20to%20the%20degraded%20input%0Aimage%20and%20explicitly%20guides%20the%20reverse%20generation%20process%20for%20image%0Arestoration%2C%20while%20noise%20diffusion%20represents%20random%20perturbations%20in%20the%0Adiffusion%20process.%20The%20residual%20prioritizes%20certainty%2C%20while%20the%20noise%0Aemphasizes%20diversity%2C%20enabling%20RDDM%20to%20effectively%20unify%20tasks%20with%20varying%0Acertainty%20or%20diversity%20requirements%2C%20such%20as%20image%20generation%20and%20restoration.%0AWe%20demonstrate%20that%20our%20sampling%20process%20is%20consistent%20with%20that%20of%20DDPM%20and%0ADDIM%20through%20coefficient%20transformation%2C%20and%20propose%20a%20partially%0Apath-independent%20generation%20process%20to%20better%20understand%20the%20reverse%20process.%0ANotably%2C%20our%20RDDM%20enables%20a%20generic%20UNet%2C%20trained%20with%20only%20an%20L1%20loss%20and%20a%0Abatch%20size%20of%201%2C%20to%20compete%20with%20state-of-the-art%20image%20restoration%20methods.%20We%0Aprovide%20code%20and%20pre-trained%20models%20to%20encourage%20further%20exploration%2C%0Aapplication%2C%20and%20development%20of%20our%20innovative%20framework%0A%28https%3A//github.com/nachifur/RDDM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13712v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Denoising%20Diffusion%20Models&entry.906535625=Jiawei%20Liu%20and%20Qiang%20Wang%20and%20Huijie%20Fan%20and%20Yinong%20Wang%20and%20Yandong%20Tang%20and%20Liangqiong%20Qu&entry.1292438233=%20%20We%20propose%20residual%20denoising%20diffusion%20models%20%28RDDM%29%2C%20a%20novel%20dual%20diffusion%0Aprocess%20that%20decouples%20the%20traditional%20single%20denoising%20diffusion%20process%20into%0Aresidual%20diffusion%20and%20noise%20diffusion.%20This%20dual%20diffusion%20framework%20expands%0Athe%20denoising-based%20diffusion%20models%2C%20initially%20uninterpretable%20for%20image%0Arestoration%2C%20into%20a%20unified%20and%20interpretable%20model%20for%20both%20image%20generation%0Aand%20restoration%20by%20introducing%20residuals.%20Specifically%2C%20our%20residual%20diffusion%0Arepresents%20directional%20diffusion%20from%20the%20target%20image%20to%20the%20degraded%20input%0Aimage%20and%20explicitly%20guides%20the%20reverse%20generation%20process%20for%20image%0Arestoration%2C%20while%20noise%20diffusion%20represents%20random%20perturbations%20in%20the%0Adiffusion%20process.%20The%20residual%20prioritizes%20certainty%2C%20while%20the%20noise%0Aemphasizes%20diversity%2C%20enabling%20RDDM%20to%20effectively%20unify%20tasks%20with%20varying%0Acertainty%20or%20diversity%20requirements%2C%20such%20as%20image%20generation%20and%20restoration.%0AWe%20demonstrate%20that%20our%20sampling%20process%20is%20consistent%20with%20that%20of%20DDPM%20and%0ADDIM%20through%20coefficient%20transformation%2C%20and%20propose%20a%20partially%0Apath-independent%20generation%20process%20to%20better%20understand%20the%20reverse%20process.%0ANotably%2C%20our%20RDDM%20enables%20a%20generic%20UNet%2C%20trained%20with%20only%20an%20L1%20loss%20and%20a%0Abatch%20size%20of%201%2C%20to%20compete%20with%20state-of-the-art%20image%20restoration%20methods.%20We%0Aprovide%20code%20and%20pre-trained%20models%20to%20encourage%20further%20exploration%2C%0Aapplication%2C%20and%20development%20of%20our%20innovative%20framework%0A%28https%3A//github.com/nachifur/RDDM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13712v3&entry.124074799=Read"},
{"title": "Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion", "author": "Xiang Fan and Anand Bhattad and Ranjay Krishna", "abstract": "  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n", "link": "http://arxiv.org/abs/2403.14617v2", "date": "2024-03-22", "relevancy": 2.3538, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6334}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5919}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.567}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Videoshop%3A%20Localized%20Semantic%20Video%20Editing%20with%20Noise-Extrapolated%0A%20%20Diffusion%20Inversion&body=Title%3A%20Videoshop%3A%20Localized%20Semantic%20Video%20Editing%20with%20Noise-Extrapolated%0A%20%20Diffusion%20Inversion%0AAuthor%3A%20Xiang%20Fan%20and%20Anand%20Bhattad%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20We%20introduce%20Videoshop%2C%20a%20training-free%20video%20editing%20algorithm%20for%20localized%0Asemantic%20edits.%20Videoshop%20allows%20users%20to%20use%20any%20editing%20software%2C%20including%0APhotoshop%20and%20generative%20inpainting%2C%20to%20modify%20the%20first%20frame%3B%20it%0Aautomatically%20propagates%20those%20changes%2C%20with%20semantic%2C%20spatial%2C%20and%20temporally%0Aconsistent%20motion%2C%20to%20the%20remaining%20frames.%20Unlike%20existing%20methods%20that%20enable%0Aedits%20only%20through%20imprecise%20textual%20instructions%2C%20Videoshop%20allows%20users%20to%0Aadd%20or%20remove%20objects%2C%20semantically%20change%20objects%2C%20insert%20stock%20photos%20into%0Avideos%2C%20etc.%20with%20fine-grained%20control%20over%20locations%20and%20appearance.%20We%0Aachieve%20this%20through%20image-based%20video%20editing%20by%20inverting%20latents%20with%20noise%0Aextrapolation%2C%20from%20which%20we%20generate%20videos%20conditioned%20on%20the%20edited%20image.%0AVideoshop%20produces%20higher%20quality%20edits%20against%206%20baselines%20on%202%20editing%0Abenchmarks%20using%2010%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14617v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Videoshop%3A%20Localized%20Semantic%20Video%20Editing%20with%20Noise-Extrapolated%0A%20%20Diffusion%20Inversion&entry.906535625=Xiang%20Fan%20and%20Anand%20Bhattad%20and%20Ranjay%20Krishna&entry.1292438233=%20%20We%20introduce%20Videoshop%2C%20a%20training-free%20video%20editing%20algorithm%20for%20localized%0Asemantic%20edits.%20Videoshop%20allows%20users%20to%20use%20any%20editing%20software%2C%20including%0APhotoshop%20and%20generative%20inpainting%2C%20to%20modify%20the%20first%20frame%3B%20it%0Aautomatically%20propagates%20those%20changes%2C%20with%20semantic%2C%20spatial%2C%20and%20temporally%0Aconsistent%20motion%2C%20to%20the%20remaining%20frames.%20Unlike%20existing%20methods%20that%20enable%0Aedits%20only%20through%20imprecise%20textual%20instructions%2C%20Videoshop%20allows%20users%20to%0Aadd%20or%20remove%20objects%2C%20semantically%20change%20objects%2C%20insert%20stock%20photos%20into%0Avideos%2C%20etc.%20with%20fine-grained%20control%20over%20locations%20and%20appearance.%20We%0Aachieve%20this%20through%20image-based%20video%20editing%20by%20inverting%20latents%20with%20noise%0Aextrapolation%2C%20from%20which%20we%20generate%20videos%20conditioned%20on%20the%20edited%20image.%0AVideoshop%20produces%20higher%20quality%20edits%20against%206%20baselines%20on%202%20editing%0Abenchmarks%20using%2010%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14617v2&entry.124074799=Read"},
{"title": "Your Image is My Video: Reshaping the Receptive Field via Image-To-Video\n  Differentiable AutoAugmentation and Fusion", "author": "Sofia Casarin and Cynthia I. Ugwu and Sergio Escalera and Oswald Lanz", "abstract": "  The landscape of deep learning research is moving towards innovative\nstrategies to harness the true potential of data. Traditionally, emphasis has\nbeen on scaling model architectures, resulting in large and complex neural\nnetworks, which can be difficult to train with limited computational resources.\nHowever, independently of the model size, data quality (i.e. amount and\nvariability) is still a major factor that affects model generalization. In this\nwork, we propose a novel technique to exploit available data through the use of\nautomatic data augmentation for the tasks of image classification and semantic\nsegmentation. We introduce the first Differentiable Augmentation Search method\n(DAS) to generate variations of images that can be processed as videos.\nCompared to previous approaches, DAS is extremely fast and flexible, allowing\nthe search on very large search spaces in less than a GPU day. Our intuition is\nthat the increased receptive field in the temporal dimension provided by DAS\ncould lead to benefits also to the spatial receptive field. More specifically,\nwe leverage DAS to guide the reshaping of the spatial receptive field by\nselecting task-dependant transformations. As a result, compared to standard\naugmentation alternatives, we improve in terms of accuracy on ImageNet,\nCifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when\nplugging-in our DAS over different light-weight video backbones.\n", "link": "http://arxiv.org/abs/2403.15194v1", "date": "2024-03-22", "relevancy": 2.3499, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.608}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5839}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5829}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Your%20Image%20is%20My%20Video%3A%20Reshaping%20the%20Receptive%20Field%20via%20Image-To-Video%0A%20%20Differentiable%20AutoAugmentation%20and%20Fusion&body=Title%3A%20Your%20Image%20is%20My%20Video%3A%20Reshaping%20the%20Receptive%20Field%20via%20Image-To-Video%0A%20%20Differentiable%20AutoAugmentation%20and%20Fusion%0AAuthor%3A%20Sofia%20Casarin%20and%20Cynthia%20I.%20Ugwu%20and%20Sergio%20Escalera%20and%20Oswald%20Lanz%0AAbstract%3A%20%20%20The%20landscape%20of%20deep%20learning%20research%20is%20moving%20towards%20innovative%0Astrategies%20to%20harness%20the%20true%20potential%20of%20data.%20Traditionally%2C%20emphasis%20has%0Abeen%20on%20scaling%20model%20architectures%2C%20resulting%20in%20large%20and%20complex%20neural%0Anetworks%2C%20which%20can%20be%20difficult%20to%20train%20with%20limited%20computational%20resources.%0AHowever%2C%20independently%20of%20the%20model%20size%2C%20data%20quality%20%28i.e.%20amount%20and%0Avariability%29%20is%20still%20a%20major%20factor%20that%20affects%20model%20generalization.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20technique%20to%20exploit%20available%20data%20through%20the%20use%20of%0Aautomatic%20data%20augmentation%20for%20the%20tasks%20of%20image%20classification%20and%20semantic%0Asegmentation.%20We%20introduce%20the%20first%20Differentiable%20Augmentation%20Search%20method%0A%28DAS%29%20to%20generate%20variations%20of%20images%20that%20can%20be%20processed%20as%20videos.%0ACompared%20to%20previous%20approaches%2C%20DAS%20is%20extremely%20fast%20and%20flexible%2C%20allowing%0Athe%20search%20on%20very%20large%20search%20spaces%20in%20less%20than%20a%20GPU%20day.%20Our%20intuition%20is%0Athat%20the%20increased%20receptive%20field%20in%20the%20temporal%20dimension%20provided%20by%20DAS%0Acould%20lead%20to%20benefits%20also%20to%20the%20spatial%20receptive%20field.%20More%20specifically%2C%0Awe%20leverage%20DAS%20to%20guide%20the%20reshaping%20of%20the%20spatial%20receptive%20field%20by%0Aselecting%20task-dependant%20transformations.%20As%20a%20result%2C%20compared%20to%20standard%0Aaugmentation%20alternatives%2C%20we%20improve%20in%20terms%20of%20accuracy%20on%20ImageNet%2C%0ACifar10%2C%20Cifar100%2C%20Tiny-ImageNet%2C%20Pascal-VOC-2012%20and%20CityScapes%20datasets%20when%0Aplugging-in%20our%20DAS%20over%20different%20light-weight%20video%20backbones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15194v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Your%20Image%20is%20My%20Video%3A%20Reshaping%20the%20Receptive%20Field%20via%20Image-To-Video%0A%20%20Differentiable%20AutoAugmentation%20and%20Fusion&entry.906535625=Sofia%20Casarin%20and%20Cynthia%20I.%20Ugwu%20and%20Sergio%20Escalera%20and%20Oswald%20Lanz&entry.1292438233=%20%20The%20landscape%20of%20deep%20learning%20research%20is%20moving%20towards%20innovative%0Astrategies%20to%20harness%20the%20true%20potential%20of%20data.%20Traditionally%2C%20emphasis%20has%0Abeen%20on%20scaling%20model%20architectures%2C%20resulting%20in%20large%20and%20complex%20neural%0Anetworks%2C%20which%20can%20be%20difficult%20to%20train%20with%20limited%20computational%20resources.%0AHowever%2C%20independently%20of%20the%20model%20size%2C%20data%20quality%20%28i.e.%20amount%20and%0Avariability%29%20is%20still%20a%20major%20factor%20that%20affects%20model%20generalization.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20technique%20to%20exploit%20available%20data%20through%20the%20use%20of%0Aautomatic%20data%20augmentation%20for%20the%20tasks%20of%20image%20classification%20and%20semantic%0Asegmentation.%20We%20introduce%20the%20first%20Differentiable%20Augmentation%20Search%20method%0A%28DAS%29%20to%20generate%20variations%20of%20images%20that%20can%20be%20processed%20as%20videos.%0ACompared%20to%20previous%20approaches%2C%20DAS%20is%20extremely%20fast%20and%20flexible%2C%20allowing%0Athe%20search%20on%20very%20large%20search%20spaces%20in%20less%20than%20a%20GPU%20day.%20Our%20intuition%20is%0Athat%20the%20increased%20receptive%20field%20in%20the%20temporal%20dimension%20provided%20by%20DAS%0Acould%20lead%20to%20benefits%20also%20to%20the%20spatial%20receptive%20field.%20More%20specifically%2C%0Awe%20leverage%20DAS%20to%20guide%20the%20reshaping%20of%20the%20spatial%20receptive%20field%20by%0Aselecting%20task-dependant%20transformations.%20As%20a%20result%2C%20compared%20to%20standard%0Aaugmentation%20alternatives%2C%20we%20improve%20in%20terms%20of%20accuracy%20on%20ImageNet%2C%0ACifar10%2C%20Cifar100%2C%20Tiny-ImageNet%2C%20Pascal-VOC-2012%20and%20CityScapes%20datasets%20when%0Aplugging-in%20our%20DAS%20over%20different%20light-weight%20video%20backbones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15194v1&entry.124074799=Read"},
{"title": "RGBD GS-ICP SLAM", "author": "Seongbo Ha and Jiung Yeon and Hyeonwoo Yu", "abstract": "  Simultaneous Localization and Mapping (SLAM) with dense representation plays\na key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)\napplications. Recent advancements in dense representation SLAM have highlighted\nthe potential of leveraging neural scene representation and 3D Gaussian\nrepresentation for high-fidelity spatial representation. In this paper, we\npropose a novel dense representation SLAM approach with a fusion of Generalized\nIterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast\nto existing methods, we utilize a single Gaussian map for both tracking and\nmapping, resulting in mutual benefits. Through the exchange of covariances\nbetween tracking and mapping processes with scale alignment techniques, we\nminimize redundant computations and achieve an efficient system. Additionally,\nwe enhance tracking accuracy and mapping quality through our keyframe selection\nmethods. Experimental results demonstrate the effectiveness of our approach,\nshowing an incredibly fast speed up to 107 FPS (for the entire system) and\nsuperior quality of the reconstructed map.\n", "link": "http://arxiv.org/abs/2403.12550v2", "date": "2024-03-22", "relevancy": 2.3436, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6133}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5708}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5552}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RGBD%20GS-ICP%20SLAM&body=Title%3A%20RGBD%20GS-ICP%20SLAM%0AAuthor%3A%20Seongbo%20Ha%20and%20Jiung%20Yeon%20and%20Hyeonwoo%20Yu%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20with%20dense%20representation%20plays%0Aa%20key%20role%20in%20robotics%2C%20Virtual%20Reality%20%28VR%29%2C%20and%20Augmented%20Reality%20%28AR%29%0Aapplications.%20Recent%20advancements%20in%20dense%20representation%20SLAM%20have%20highlighted%0Athe%20potential%20of%20leveraging%20neural%20scene%20representation%20and%203D%20Gaussian%0Arepresentation%20for%20high-fidelity%20spatial%20representation.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20dense%20representation%20SLAM%20approach%20with%20a%20fusion%20of%20Generalized%0AIterative%20Closest%20Point%20%28G-ICP%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29.%20In%20contrast%0Ato%20existing%20methods%2C%20we%20utilize%20a%20single%20Gaussian%20map%20for%20both%20tracking%20and%0Amapping%2C%20resulting%20in%20mutual%20benefits.%20Through%20the%20exchange%20of%20covariances%0Abetween%20tracking%20and%20mapping%20processes%20with%20scale%20alignment%20techniques%2C%20we%0Aminimize%20redundant%20computations%20and%20achieve%20an%20efficient%20system.%20Additionally%2C%0Awe%20enhance%20tracking%20accuracy%20and%20mapping%20quality%20through%20our%20keyframe%20selection%0Amethods.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Ashowing%20an%20incredibly%20fast%20speed%20up%20to%20107%20FPS%20%28for%20the%20entire%20system%29%20and%0Asuperior%20quality%20of%20the%20reconstructed%20map.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12550v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGBD%20GS-ICP%20SLAM&entry.906535625=Seongbo%20Ha%20and%20Jiung%20Yeon%20and%20Hyeonwoo%20Yu&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20with%20dense%20representation%20plays%0Aa%20key%20role%20in%20robotics%2C%20Virtual%20Reality%20%28VR%29%2C%20and%20Augmented%20Reality%20%28AR%29%0Aapplications.%20Recent%20advancements%20in%20dense%20representation%20SLAM%20have%20highlighted%0Athe%20potential%20of%20leveraging%20neural%20scene%20representation%20and%203D%20Gaussian%0Arepresentation%20for%20high-fidelity%20spatial%20representation.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20dense%20representation%20SLAM%20approach%20with%20a%20fusion%20of%20Generalized%0AIterative%20Closest%20Point%20%28G-ICP%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29.%20In%20contrast%0Ato%20existing%20methods%2C%20we%20utilize%20a%20single%20Gaussian%20map%20for%20both%20tracking%20and%0Amapping%2C%20resulting%20in%20mutual%20benefits.%20Through%20the%20exchange%20of%20covariances%0Abetween%20tracking%20and%20mapping%20processes%20with%20scale%20alignment%20techniques%2C%20we%0Aminimize%20redundant%20computations%20and%20achieve%20an%20efficient%20system.%20Additionally%2C%0Awe%20enhance%20tracking%20accuracy%20and%20mapping%20quality%20through%20our%20keyframe%20selection%0Amethods.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Ashowing%20an%20incredibly%20fast%20speed%20up%20to%20107%20FPS%20%28for%20the%20entire%20system%29%20and%0Asuperior%20quality%20of%20the%20reconstructed%20map.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12550v2&entry.124074799=Read"},
{"title": "Win-Win: Training High-Resolution Vision Transformers from Two Windows", "author": "Vincent Leroy and Jerome Revaud and Thomas Lucas and Philippe Weinzaepfel", "abstract": "  Transformers have become the standard in state-of-the-art vision\narchitectures, achieving impressive performance on both image-level and dense\npixelwise tasks. However, training vision transformers for high-resolution\npixelwise tasks has a prohibitive cost. Typical solutions boil down to\nhierarchical architectures, fast and approximate attention, or training on\nlow-resolution crops. This latter solution does not constrain architectural\nchoices, but it leads to a clear performance drop when testing at resolutions\nsignificantly higher than that used for training, thus requiring ad-hoc and\nslow post-processing schemes. In this paper, we propose a novel strategy for\nefficient training and inference of high-resolution vision transformers. The\nkey principle is to mask out most of the high-resolution inputs during\ntraining, keeping only N random windows. This allows the model to learn local\ninteractions between tokens inside each window, and global interactions between\ntokens from different windows. As a result, the model can directly process the\nhigh-resolution input at test time without any special trick. We show that this\nstrategy is effective when using relative positional embedding such as rotary\nembeddings. It is 4 times faster to train than a full-resolution network, and\nit is straightforward to use at test time compared to existing approaches. We\napply this strategy to three dense prediction tasks with high-resolution data.\nFirst, we show on the task of semantic segmentation that a simple setting with\n2 windows performs best, hence the name of our method: Win-Win. Second, we\nconfirm this result on the task of monocular depth prediction. Third, we\nfurther extend it to the binocular task of optical flow, reaching\nstate-of-the-art performance on the Spring benchmark that contains Full-HD\nimages with an order of magnitude faster inference than the best competitor.\n", "link": "http://arxiv.org/abs/2310.00632v2", "date": "2024-03-22", "relevancy": 2.3365, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6241}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6053}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Win-Win%3A%20Training%20High-Resolution%20Vision%20Transformers%20from%20Two%20Windows&body=Title%3A%20Win-Win%3A%20Training%20High-Resolution%20Vision%20Transformers%20from%20Two%20Windows%0AAuthor%3A%20Vincent%20Leroy%20and%20Jerome%20Revaud%20and%20Thomas%20Lucas%20and%20Philippe%20Weinzaepfel%0AAbstract%3A%20%20%20Transformers%20have%20become%20the%20standard%20in%20state-of-the-art%20vision%0Aarchitectures%2C%20achieving%20impressive%20performance%20on%20both%20image-level%20and%20dense%0Apixelwise%20tasks.%20However%2C%20training%20vision%20transformers%20for%20high-resolution%0Apixelwise%20tasks%20has%20a%20prohibitive%20cost.%20Typical%20solutions%20boil%20down%20to%0Ahierarchical%20architectures%2C%20fast%20and%20approximate%20attention%2C%20or%20training%20on%0Alow-resolution%20crops.%20This%20latter%20solution%20does%20not%20constrain%20architectural%0Achoices%2C%20but%20it%20leads%20to%20a%20clear%20performance%20drop%20when%20testing%20at%20resolutions%0Asignificantly%20higher%20than%20that%20used%20for%20training%2C%20thus%20requiring%20ad-hoc%20and%0Aslow%20post-processing%20schemes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20strategy%20for%0Aefficient%20training%20and%20inference%20of%20high-resolution%20vision%20transformers.%20The%0Akey%20principle%20is%20to%20mask%20out%20most%20of%20the%20high-resolution%20inputs%20during%0Atraining%2C%20keeping%20only%20N%20random%20windows.%20This%20allows%20the%20model%20to%20learn%20local%0Ainteractions%20between%20tokens%20inside%20each%20window%2C%20and%20global%20interactions%20between%0Atokens%20from%20different%20windows.%20As%20a%20result%2C%20the%20model%20can%20directly%20process%20the%0Ahigh-resolution%20input%20at%20test%20time%20without%20any%20special%20trick.%20We%20show%20that%20this%0Astrategy%20is%20effective%20when%20using%20relative%20positional%20embedding%20such%20as%20rotary%0Aembeddings.%20It%20is%204%20times%20faster%20to%20train%20than%20a%20full-resolution%20network%2C%20and%0Ait%20is%20straightforward%20to%20use%20at%20test%20time%20compared%20to%20existing%20approaches.%20We%0Aapply%20this%20strategy%20to%20three%20dense%20prediction%20tasks%20with%20high-resolution%20data.%0AFirst%2C%20we%20show%20on%20the%20task%20of%20semantic%20segmentation%20that%20a%20simple%20setting%20with%0A2%20windows%20performs%20best%2C%20hence%20the%20name%20of%20our%20method%3A%20Win-Win.%20Second%2C%20we%0Aconfirm%20this%20result%20on%20the%20task%20of%20monocular%20depth%20prediction.%20Third%2C%20we%0Afurther%20extend%20it%20to%20the%20binocular%20task%20of%20optical%20flow%2C%20reaching%0Astate-of-the-art%20performance%20on%20the%20Spring%20benchmark%20that%20contains%20Full-HD%0Aimages%20with%20an%20order%20of%20magnitude%20faster%20inference%20than%20the%20best%20competitor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00632v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Win-Win%3A%20Training%20High-Resolution%20Vision%20Transformers%20from%20Two%20Windows&entry.906535625=Vincent%20Leroy%20and%20Jerome%20Revaud%20and%20Thomas%20Lucas%20and%20Philippe%20Weinzaepfel&entry.1292438233=%20%20Transformers%20have%20become%20the%20standard%20in%20state-of-the-art%20vision%0Aarchitectures%2C%20achieving%20impressive%20performance%20on%20both%20image-level%20and%20dense%0Apixelwise%20tasks.%20However%2C%20training%20vision%20transformers%20for%20high-resolution%0Apixelwise%20tasks%20has%20a%20prohibitive%20cost.%20Typical%20solutions%20boil%20down%20to%0Ahierarchical%20architectures%2C%20fast%20and%20approximate%20attention%2C%20or%20training%20on%0Alow-resolution%20crops.%20This%20latter%20solution%20does%20not%20constrain%20architectural%0Achoices%2C%20but%20it%20leads%20to%20a%20clear%20performance%20drop%20when%20testing%20at%20resolutions%0Asignificantly%20higher%20than%20that%20used%20for%20training%2C%20thus%20requiring%20ad-hoc%20and%0Aslow%20post-processing%20schemes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20strategy%20for%0Aefficient%20training%20and%20inference%20of%20high-resolution%20vision%20transformers.%20The%0Akey%20principle%20is%20to%20mask%20out%20most%20of%20the%20high-resolution%20inputs%20during%0Atraining%2C%20keeping%20only%20N%20random%20windows.%20This%20allows%20the%20model%20to%20learn%20local%0Ainteractions%20between%20tokens%20inside%20each%20window%2C%20and%20global%20interactions%20between%0Atokens%20from%20different%20windows.%20As%20a%20result%2C%20the%20model%20can%20directly%20process%20the%0Ahigh-resolution%20input%20at%20test%20time%20without%20any%20special%20trick.%20We%20show%20that%20this%0Astrategy%20is%20effective%20when%20using%20relative%20positional%20embedding%20such%20as%20rotary%0Aembeddings.%20It%20is%204%20times%20faster%20to%20train%20than%20a%20full-resolution%20network%2C%20and%0Ait%20is%20straightforward%20to%20use%20at%20test%20time%20compared%20to%20existing%20approaches.%20We%0Aapply%20this%20strategy%20to%20three%20dense%20prediction%20tasks%20with%20high-resolution%20data.%0AFirst%2C%20we%20show%20on%20the%20task%20of%20semantic%20segmentation%20that%20a%20simple%20setting%20with%0A2%20windows%20performs%20best%2C%20hence%20the%20name%20of%20our%20method%3A%20Win-Win.%20Second%2C%20we%0Aconfirm%20this%20result%20on%20the%20task%20of%20monocular%20depth%20prediction.%20Third%2C%20we%0Afurther%20extend%20it%20to%20the%20binocular%20task%20of%20optical%20flow%2C%20reaching%0Astate-of-the-art%20performance%20on%20the%20Spring%20benchmark%20that%20contains%20Full-HD%0Aimages%20with%20an%20order%20of%20magnitude%20faster%20inference%20than%20the%20best%20competitor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00632v2&entry.124074799=Read"},
{"title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation", "author": "Dan Kondratyuk and Lijun Yu and Xiuye Gu and Jos\u00e9 Lezama and Jonathan Huang and Grant Schindler and Rachel Hornung and Vighnesh Birodkar and Jimmy Yan and Ming-Chang Chiu and Krishna Somandepalli and Hassan Akbari and Yair Alon and Yong Cheng and Josh Dillon and Agrim Gupta and Meera Hahn and Anja Hauth and David Hendon and Alonso Martinez and David Minnen and Mikhail Sirotenko and Kihyuk Sohn and Xuan Yang and Hartwig Adam and Ming-Hsuan Yang and Irfan Essa and Huisheng Wang and David A. Ross and Bryan Seybold and Lu Jiang", "abstract": "  We present VideoPoet, a language model capable of synthesizing high-quality\nvideo, with matching audio, from a large variety of conditioning signals.\nVideoPoet employs a decoder-only transformer architecture that processes\nmultimodal inputs -- including images, videos, text, and audio. The training\nprotocol follows that of Large Language Models (LLMs), consisting of two\nstages: pretraining and task-specific adaptation. During pretraining, VideoPoet\nincorporates a mixture of multimodal generative objectives within an\nautoregressive Transformer framework. The pretrained LLM serves as a foundation\nthat can be adapted for a range of video generation tasks. We present empirical\nresults demonstrating the model's state-of-the-art capabilities in zero-shot\nvideo generation, specifically highlighting VideoPoet's ability to generate\nhigh-fidelity motions. Project page: http://sites.research.google/videopoet/\n", "link": "http://arxiv.org/abs/2312.14125v3", "date": "2024-03-22", "relevancy": 2.3225, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5818}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5806}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5794}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VideoPoet%3A%20A%20Large%20Language%20Model%20for%20Zero-Shot%20Video%20Generation&body=Title%3A%20VideoPoet%3A%20A%20Large%20Language%20Model%20for%20Zero-Shot%20Video%20Generation%0AAuthor%3A%20Dan%20Kondratyuk%20and%20Lijun%20Yu%20and%20Xiuye%20Gu%20and%20Jos%C3%A9%20Lezama%20and%20Jonathan%20Huang%20and%20Grant%20Schindler%20and%20Rachel%20Hornung%20and%20Vighnesh%20Birodkar%20and%20Jimmy%20Yan%20and%20Ming-Chang%20Chiu%20and%20Krishna%20Somandepalli%20and%20Hassan%20Akbari%20and%20Yair%20Alon%20and%20Yong%20Cheng%20and%20Josh%20Dillon%20and%20Agrim%20Gupta%20and%20Meera%20Hahn%20and%20Anja%20Hauth%20and%20David%20Hendon%20and%20Alonso%20Martinez%20and%20David%20Minnen%20and%20Mikhail%20Sirotenko%20and%20Kihyuk%20Sohn%20and%20Xuan%20Yang%20and%20Hartwig%20Adam%20and%20Ming-Hsuan%20Yang%20and%20Irfan%20Essa%20and%20Huisheng%20Wang%20and%20David%20A.%20Ross%20and%20Bryan%20Seybold%20and%20Lu%20Jiang%0AAbstract%3A%20%20%20We%20present%20VideoPoet%2C%20a%20language%20model%20capable%20of%20synthesizing%20high-quality%0Avideo%2C%20with%20matching%20audio%2C%20from%20a%20large%20variety%20of%20conditioning%20signals.%0AVideoPoet%20employs%20a%20decoder-only%20transformer%20architecture%20that%20processes%0Amultimodal%20inputs%20--%20including%20images%2C%20videos%2C%20text%2C%20and%20audio.%20The%20training%0Aprotocol%20follows%20that%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20consisting%20of%20two%0Astages%3A%20pretraining%20and%20task-specific%20adaptation.%20During%20pretraining%2C%20VideoPoet%0Aincorporates%20a%20mixture%20of%20multimodal%20generative%20objectives%20within%20an%0Aautoregressive%20Transformer%20framework.%20The%20pretrained%20LLM%20serves%20as%20a%20foundation%0Athat%20can%20be%20adapted%20for%20a%20range%20of%20video%20generation%20tasks.%20We%20present%20empirical%0Aresults%20demonstrating%20the%20model%27s%20state-of-the-art%20capabilities%20in%20zero-shot%0Avideo%20generation%2C%20specifically%20highlighting%20VideoPoet%27s%20ability%20to%20generate%0Ahigh-fidelity%20motions.%20Project%20page%3A%20http%3A//sites.research.google/videopoet/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14125v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoPoet%3A%20A%20Large%20Language%20Model%20for%20Zero-Shot%20Video%20Generation&entry.906535625=Dan%20Kondratyuk%20and%20Lijun%20Yu%20and%20Xiuye%20Gu%20and%20Jos%C3%A9%20Lezama%20and%20Jonathan%20Huang%20and%20Grant%20Schindler%20and%20Rachel%20Hornung%20and%20Vighnesh%20Birodkar%20and%20Jimmy%20Yan%20and%20Ming-Chang%20Chiu%20and%20Krishna%20Somandepalli%20and%20Hassan%20Akbari%20and%20Yair%20Alon%20and%20Yong%20Cheng%20and%20Josh%20Dillon%20and%20Agrim%20Gupta%20and%20Meera%20Hahn%20and%20Anja%20Hauth%20and%20David%20Hendon%20and%20Alonso%20Martinez%20and%20David%20Minnen%20and%20Mikhail%20Sirotenko%20and%20Kihyuk%20Sohn%20and%20Xuan%20Yang%20and%20Hartwig%20Adam%20and%20Ming-Hsuan%20Yang%20and%20Irfan%20Essa%20and%20Huisheng%20Wang%20and%20David%20A.%20Ross%20and%20Bryan%20Seybold%20and%20Lu%20Jiang&entry.1292438233=%20%20We%20present%20VideoPoet%2C%20a%20language%20model%20capable%20of%20synthesizing%20high-quality%0Avideo%2C%20with%20matching%20audio%2C%20from%20a%20large%20variety%20of%20conditioning%20signals.%0AVideoPoet%20employs%20a%20decoder-only%20transformer%20architecture%20that%20processes%0Amultimodal%20inputs%20--%20including%20images%2C%20videos%2C%20text%2C%20and%20audio.%20The%20training%0Aprotocol%20follows%20that%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20consisting%20of%20two%0Astages%3A%20pretraining%20and%20task-specific%20adaptation.%20During%20pretraining%2C%20VideoPoet%0Aincorporates%20a%20mixture%20of%20multimodal%20generative%20objectives%20within%20an%0Aautoregressive%20Transformer%20framework.%20The%20pretrained%20LLM%20serves%20as%20a%20foundation%0Athat%20can%20be%20adapted%20for%20a%20range%20of%20video%20generation%20tasks.%20We%20present%20empirical%0Aresults%20demonstrating%20the%20model%27s%20state-of-the-art%20capabilities%20in%20zero-shot%0Avideo%20generation%2C%20specifically%20highlighting%20VideoPoet%27s%20ability%20to%20generate%0Ahigh-fidelity%20motions.%20Project%20page%3A%20http%3A//sites.research.google/videopoet/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14125v3&entry.124074799=Read"},
{"title": "Soft Contrastive Learning for Time Series", "author": "Seunghan Lee and Taeyoung Park and Kibok Lee", "abstract": "  Contrastive learning has shown to be effective to learn representations from\ntime series in a self-supervised way. However, contrasting similar time series\ninstances or values from adjacent timestamps within a time series leads to\nignore their inherent correlations, which results in deteriorating the quality\nof learned representations. To address this issue, we propose SoftCLT, a simple\nyet effective soft contrastive learning strategy for time series. This is\nachieved by introducing instance-wise and temporal contrastive loss with soft\nassignments ranging from zero to one. Specifically, we define soft assignments\nfor 1) instance-wise contrastive loss by the distance between time series on\nthe data space, and 2) temporal contrastive loss by the difference of\ntimestamps. SoftCLT is a plug-and-play method for time series contrastive\nlearning that improves the quality of learned representations without bells and\nwhistles. In experiments, we demonstrate that SoftCLT consistently improves the\nperformance in various downstream tasks including classification,\nsemi-supervised learning, transfer learning, and anomaly detection, showing\nstate-of-the-art performance. Code is available at this repository:\nhttps://github.com/seunghan96/softclt.\n", "link": "http://arxiv.org/abs/2312.16424v3", "date": "2024-03-22", "relevancy": 2.3124, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4642}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4542}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Soft%20Contrastive%20Learning%20for%20Time%20Series&body=Title%3A%20Soft%20Contrastive%20Learning%20for%20Time%20Series%0AAuthor%3A%20Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee%0AAbstract%3A%20%20%20Contrastive%20learning%20has%20shown%20to%20be%20effective%20to%20learn%20representations%20from%0Atime%20series%20in%20a%20self-supervised%20way.%20However%2C%20contrasting%20similar%20time%20series%0Ainstances%20or%20values%20from%20adjacent%20timestamps%20within%20a%20time%20series%20leads%20to%0Aignore%20their%20inherent%20correlations%2C%20which%20results%20in%20deteriorating%20the%20quality%0Aof%20learned%20representations.%20To%20address%20this%20issue%2C%20we%20propose%20SoftCLT%2C%20a%20simple%0Ayet%20effective%20soft%20contrastive%20learning%20strategy%20for%20time%20series.%20This%20is%0Aachieved%20by%20introducing%20instance-wise%20and%20temporal%20contrastive%20loss%20with%20soft%0Aassignments%20ranging%20from%20zero%20to%20one.%20Specifically%2C%20we%20define%20soft%20assignments%0Afor%201%29%20instance-wise%20contrastive%20loss%20by%20the%20distance%20between%20time%20series%20on%0Athe%20data%20space%2C%20and%202%29%20temporal%20contrastive%20loss%20by%20the%20difference%20of%0Atimestamps.%20SoftCLT%20is%20a%20plug-and-play%20method%20for%20time%20series%20contrastive%0Alearning%20that%20improves%20the%20quality%20of%20learned%20representations%20without%20bells%20and%0Awhistles.%20In%20experiments%2C%20we%20demonstrate%20that%20SoftCLT%20consistently%20improves%20the%0Aperformance%20in%20various%20downstream%20tasks%20including%20classification%2C%0Asemi-supervised%20learning%2C%20transfer%20learning%2C%20and%20anomaly%20detection%2C%20showing%0Astate-of-the-art%20performance.%20Code%20is%20available%20at%20this%20repository%3A%0Ahttps%3A//github.com/seunghan96/softclt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16424v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Contrastive%20Learning%20for%20Time%20Series&entry.906535625=Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee&entry.1292438233=%20%20Contrastive%20learning%20has%20shown%20to%20be%20effective%20to%20learn%20representations%20from%0Atime%20series%20in%20a%20self-supervised%20way.%20However%2C%20contrasting%20similar%20time%20series%0Ainstances%20or%20values%20from%20adjacent%20timestamps%20within%20a%20time%20series%20leads%20to%0Aignore%20their%20inherent%20correlations%2C%20which%20results%20in%20deteriorating%20the%20quality%0Aof%20learned%20representations.%20To%20address%20this%20issue%2C%20we%20propose%20SoftCLT%2C%20a%20simple%0Ayet%20effective%20soft%20contrastive%20learning%20strategy%20for%20time%20series.%20This%20is%0Aachieved%20by%20introducing%20instance-wise%20and%20temporal%20contrastive%20loss%20with%20soft%0Aassignments%20ranging%20from%20zero%20to%20one.%20Specifically%2C%20we%20define%20soft%20assignments%0Afor%201%29%20instance-wise%20contrastive%20loss%20by%20the%20distance%20between%20time%20series%20on%0Athe%20data%20space%2C%20and%202%29%20temporal%20contrastive%20loss%20by%20the%20difference%20of%0Atimestamps.%20SoftCLT%20is%20a%20plug-and-play%20method%20for%20time%20series%20contrastive%0Alearning%20that%20improves%20the%20quality%20of%20learned%20representations%20without%20bells%20and%0Awhistles.%20In%20experiments%2C%20we%20demonstrate%20that%20SoftCLT%20consistently%20improves%20the%0Aperformance%20in%20various%20downstream%20tasks%20including%20classification%2C%0Asemi-supervised%20learning%2C%20transfer%20learning%2C%20and%20anomaly%20detection%2C%20showing%0Astate-of-the-art%20performance.%20Code%20is%20available%20at%20this%20repository%3A%0Ahttps%3A//github.com/seunghan96/softclt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16424v3&entry.124074799=Read"},
{"title": "Detection Is Tracking: Point Cloud Multi-Sweep Deep Learning Models\n  Revisited", "author": "Lingji Chen", "abstract": "  Conventional tracking paradigm takes in instantaneous measurements such as\nrange and bearing, and produces object tracks across time. In applications such\nas autonomous driving, lidar measurements in the form of point clouds are\nusually passed through a \"virtual sensor\" realized by a deep learning model, to\nproduce \"measurements\" such as bounding boxes, which are in turn ingested by a\ntracking module to produce object tracks. Very often multiple lidar sweeps are\naccumulated in a buffer to merge and become the input to the virtual sensor. We\nargue in this paper that such an input already contains temporal information,\nand therefore the virtual sensor output should also contain temporal\ninformation, not just instantaneous values for the time corresponding to the\nend of the buffer. In particular, we present the deep learning model called\nMULti-Sweep PAired Detector (MULSPAD) that produces, for each detected object,\na pair of bounding boxes at both the end time and the beginning time of the\ninput buffer. This is achieved with fairly straightforward changes in commonly\nused lidar detection models, and with only marginal extra processing, but the\nresulting symmetry is satisfying. Such paired detections make it possible not\nonly to construct rudimentary trackers fairly easily, but also to construct\nmore sophisticated trackers that can exploit the extra information conveyed by\nthe pair and be robust to choices of motion models and object birth/death\nmodels. We have conducted preliminary training and experimentation using Waymo\nOpen Dataset, which shows the efficacy of our proposed method.\n", "link": "http://arxiv.org/abs/2402.15756v2", "date": "2024-03-22", "relevancy": 2.3062, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.584}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5751}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5697}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Detection%20Is%20Tracking%3A%20Point%20Cloud%20Multi-Sweep%20Deep%20Learning%20Models%0A%20%20Revisited&body=Title%3A%20Detection%20Is%20Tracking%3A%20Point%20Cloud%20Multi-Sweep%20Deep%20Learning%20Models%0A%20%20Revisited%0AAuthor%3A%20Lingji%20Chen%0AAbstract%3A%20%20%20Conventional%20tracking%20paradigm%20takes%20in%20instantaneous%20measurements%20such%20as%0Arange%20and%20bearing%2C%20and%20produces%20object%20tracks%20across%20time.%20In%20applications%20such%0Aas%20autonomous%20driving%2C%20lidar%20measurements%20in%20the%20form%20of%20point%20clouds%20are%0Ausually%20passed%20through%20a%20%22virtual%20sensor%22%20realized%20by%20a%20deep%20learning%20model%2C%20to%0Aproduce%20%22measurements%22%20such%20as%20bounding%20boxes%2C%20which%20are%20in%20turn%20ingested%20by%20a%0Atracking%20module%20to%20produce%20object%20tracks.%20Very%20often%20multiple%20lidar%20sweeps%20are%0Aaccumulated%20in%20a%20buffer%20to%20merge%20and%20become%20the%20input%20to%20the%20virtual%20sensor.%20We%0Aargue%20in%20this%20paper%20that%20such%20an%20input%20already%20contains%20temporal%20information%2C%0Aand%20therefore%20the%20virtual%20sensor%20output%20should%20also%20contain%20temporal%0Ainformation%2C%20not%20just%20instantaneous%20values%20for%20the%20time%20corresponding%20to%20the%0Aend%20of%20the%20buffer.%20In%20particular%2C%20we%20present%20the%20deep%20learning%20model%20called%0AMULti-Sweep%20PAired%20Detector%20%28MULSPAD%29%20that%20produces%2C%20for%20each%20detected%20object%2C%0Aa%20pair%20of%20bounding%20boxes%20at%20both%20the%20end%20time%20and%20the%20beginning%20time%20of%20the%0Ainput%20buffer.%20This%20is%20achieved%20with%20fairly%20straightforward%20changes%20in%20commonly%0Aused%20lidar%20detection%20models%2C%20and%20with%20only%20marginal%20extra%20processing%2C%20but%20the%0Aresulting%20symmetry%20is%20satisfying.%20Such%20paired%20detections%20make%20it%20possible%20not%0Aonly%20to%20construct%20rudimentary%20trackers%20fairly%20easily%2C%20but%20also%20to%20construct%0Amore%20sophisticated%20trackers%20that%20can%20exploit%20the%20extra%20information%20conveyed%20by%0Athe%20pair%20and%20be%20robust%20to%20choices%20of%20motion%20models%20and%20object%20birth/death%0Amodels.%20We%20have%20conducted%20preliminary%20training%20and%20experimentation%20using%20Waymo%0AOpen%20Dataset%2C%20which%20shows%20the%20efficacy%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15756v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20Is%20Tracking%3A%20Point%20Cloud%20Multi-Sweep%20Deep%20Learning%20Models%0A%20%20Revisited&entry.906535625=Lingji%20Chen&entry.1292438233=%20%20Conventional%20tracking%20paradigm%20takes%20in%20instantaneous%20measurements%20such%20as%0Arange%20and%20bearing%2C%20and%20produces%20object%20tracks%20across%20time.%20In%20applications%20such%0Aas%20autonomous%20driving%2C%20lidar%20measurements%20in%20the%20form%20of%20point%20clouds%20are%0Ausually%20passed%20through%20a%20%22virtual%20sensor%22%20realized%20by%20a%20deep%20learning%20model%2C%20to%0Aproduce%20%22measurements%22%20such%20as%20bounding%20boxes%2C%20which%20are%20in%20turn%20ingested%20by%20a%0Atracking%20module%20to%20produce%20object%20tracks.%20Very%20often%20multiple%20lidar%20sweeps%20are%0Aaccumulated%20in%20a%20buffer%20to%20merge%20and%20become%20the%20input%20to%20the%20virtual%20sensor.%20We%0Aargue%20in%20this%20paper%20that%20such%20an%20input%20already%20contains%20temporal%20information%2C%0Aand%20therefore%20the%20virtual%20sensor%20output%20should%20also%20contain%20temporal%0Ainformation%2C%20not%20just%20instantaneous%20values%20for%20the%20time%20corresponding%20to%20the%0Aend%20of%20the%20buffer.%20In%20particular%2C%20we%20present%20the%20deep%20learning%20model%20called%0AMULti-Sweep%20PAired%20Detector%20%28MULSPAD%29%20that%20produces%2C%20for%20each%20detected%20object%2C%0Aa%20pair%20of%20bounding%20boxes%20at%20both%20the%20end%20time%20and%20the%20beginning%20time%20of%20the%0Ainput%20buffer.%20This%20is%20achieved%20with%20fairly%20straightforward%20changes%20in%20commonly%0Aused%20lidar%20detection%20models%2C%20and%20with%20only%20marginal%20extra%20processing%2C%20but%20the%0Aresulting%20symmetry%20is%20satisfying.%20Such%20paired%20detections%20make%20it%20possible%20not%0Aonly%20to%20construct%20rudimentary%20trackers%20fairly%20easily%2C%20but%20also%20to%20construct%0Amore%20sophisticated%20trackers%20that%20can%20exploit%20the%20extra%20information%20conveyed%20by%0Athe%20pair%20and%20be%20robust%20to%20choices%20of%20motion%20models%20and%20object%20birth/death%0Amodels.%20We%20have%20conducted%20preliminary%20training%20and%20experimentation%20using%20Waymo%0AOpen%20Dataset%2C%20which%20shows%20the%20efficacy%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15756v2&entry.124074799=Read"},
{"title": "PDE-CNNs: Axiomatic Derivations and Applications", "author": "Gijs Bellaard and Sei Sakata and Bart M. N. Smets and Remco Duits", "abstract": "  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of\ngeometrically meaningful evolution PDEs as substitutes for the conventional\ncomponents in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer\nparameters, inherent equivariance, better performance, data efficiency, and\ngeometric interpretability. In this article we focus on Euclidean equivariant\nPDE-G-CNNs where the feature maps are two dimensional throughout. We call this\nvariant of the framework a PDE-CNN. We list several practically desirable\naxioms and derive from these which PDEs should be used in a PDE-CNN. Here our\napproach to geometric learning via PDEs is inspired by the axioms of classical\nlinear and morphological scale-space theory, which we generalize by introducing\nsemifield-valued signals. Furthermore, we experimentally confirm for small\nnetworks that PDE-CNNs offer fewer parameters, better performance, and data\nefficiency in comparison to CNNs. We also investigate what effect the use of\ndifferent semifields has on the performance of the models.\n", "link": "http://arxiv.org/abs/2403.15182v1", "date": "2024-03-22", "relevancy": 2.3059, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4832}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4511}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4492}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PDE-CNNs%3A%20Axiomatic%20Derivations%20and%20Applications&body=Title%3A%20PDE-CNNs%3A%20Axiomatic%20Derivations%20and%20Applications%0AAuthor%3A%20Gijs%20Bellaard%20and%20Sei%20Sakata%20and%20Bart%20M.%20N.%20Smets%20and%20Remco%20Duits%0AAbstract%3A%20%20%20PDE-based%20Group%20Convolutional%20Neural%20Networks%20%28PDE-G-CNNs%29%20utilize%20solvers%20of%0Ageometrically%20meaningful%20evolution%20PDEs%20as%20substitutes%20for%20the%20conventional%0Acomponents%20in%20G-CNNs.%20PDE-G-CNNs%20offer%20several%20key%20benefits%20all%20at%20once%3A%20fewer%0Aparameters%2C%20inherent%20equivariance%2C%20better%20performance%2C%20data%20efficiency%2C%20and%0Ageometric%20interpretability.%20In%20this%20article%20we%20focus%20on%20Euclidean%20equivariant%0APDE-G-CNNs%20where%20the%20feature%20maps%20are%20two%20dimensional%20throughout.%20We%20call%20this%0Avariant%20of%20the%20framework%20a%20PDE-CNN.%20We%20list%20several%20practically%20desirable%0Aaxioms%20and%20derive%20from%20these%20which%20PDEs%20should%20be%20used%20in%20a%20PDE-CNN.%20Here%20our%0Aapproach%20to%20geometric%20learning%20via%20PDEs%20is%20inspired%20by%20the%20axioms%20of%20classical%0Alinear%20and%20morphological%20scale-space%20theory%2C%20which%20we%20generalize%20by%20introducing%0Asemifield-valued%20signals.%20Furthermore%2C%20we%20experimentally%20confirm%20for%20small%0Anetworks%20that%20PDE-CNNs%20offer%20fewer%20parameters%2C%20better%20performance%2C%20and%20data%0Aefficiency%20in%20comparison%20to%20CNNs.%20We%20also%20investigate%20what%20effect%20the%20use%20of%0Adifferent%20semifields%20has%20on%20the%20performance%20of%20the%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15182v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PDE-CNNs%3A%20Axiomatic%20Derivations%20and%20Applications&entry.906535625=Gijs%20Bellaard%20and%20Sei%20Sakata%20and%20Bart%20M.%20N.%20Smets%20and%20Remco%20Duits&entry.1292438233=%20%20PDE-based%20Group%20Convolutional%20Neural%20Networks%20%28PDE-G-CNNs%29%20utilize%20solvers%20of%0Ageometrically%20meaningful%20evolution%20PDEs%20as%20substitutes%20for%20the%20conventional%0Acomponents%20in%20G-CNNs.%20PDE-G-CNNs%20offer%20several%20key%20benefits%20all%20at%20once%3A%20fewer%0Aparameters%2C%20inherent%20equivariance%2C%20better%20performance%2C%20data%20efficiency%2C%20and%0Ageometric%20interpretability.%20In%20this%20article%20we%20focus%20on%20Euclidean%20equivariant%0APDE-G-CNNs%20where%20the%20feature%20maps%20are%20two%20dimensional%20throughout.%20We%20call%20this%0Avariant%20of%20the%20framework%20a%20PDE-CNN.%20We%20list%20several%20practically%20desirable%0Aaxioms%20and%20derive%20from%20these%20which%20PDEs%20should%20be%20used%20in%20a%20PDE-CNN.%20Here%20our%0Aapproach%20to%20geometric%20learning%20via%20PDEs%20is%20inspired%20by%20the%20axioms%20of%20classical%0Alinear%20and%20morphological%20scale-space%20theory%2C%20which%20we%20generalize%20by%20introducing%0Asemifield-valued%20signals.%20Furthermore%2C%20we%20experimentally%20confirm%20for%20small%0Anetworks%20that%20PDE-CNNs%20offer%20fewer%20parameters%2C%20better%20performance%2C%20and%20data%0Aefficiency%20in%20comparison%20to%20CNNs.%20We%20also%20investigate%20what%20effect%20the%20use%20of%0Adifferent%20semifields%20has%20on%20the%20performance%20of%20the%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15182v1&entry.124074799=Read"},
{"title": "Infrastructure-Assisted Collaborative Perception in Automated Valet\n  Parking: A Safety Perspective", "author": "Yukuan Jia and Jiawen Zhang and Shimeng Lu and Baokang Fan and Ruiqing Mao and Sheng Zhou and Zhisheng Niu", "abstract": "  Environmental perception in Automated Valet Parking (AVP) has been a\nchallenging task due to severe occlusions in parking garages. Although\nCollaborative Perception (CP) can be applied to broaden the field of view of\nconnected vehicles, the limited bandwidth of vehicular communications restricts\nits application. In this work, we propose a BEV feature-based CP network\narchitecture for infrastructure-assisted AVP systems. The model takes the\nroadside camera and LiDAR as optional inputs and adaptively fuses them with\nonboard sensors in a unified BEV representation. Autoencoder and downsampling\nare applied for channel-wise and spatial-wise dimension reduction, while\nsparsification and quantization further compress the feature map with little\nloss in data precision. Combining these techniques, the size of a BEV feature\nmap is effectively compressed to fit in the feasible data rate of the NR-V2X\nnetwork. With the synthetic AVP dataset, we observe that CP can effectively\nincrease perception performance, especially for pedestrians. Moreover, the\nadvantage of infrastructure-assisted CP is demonstrated in two typical\nsafety-critical scenarios in the AVP setting, increasing the maximum safe\ncruising speed by up to 3m/s in both scenarios.\n", "link": "http://arxiv.org/abs/2403.15156v1", "date": "2024-03-22", "relevancy": 2.2831, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5813}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5676}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5525}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Infrastructure-Assisted%20Collaborative%20Perception%20in%20Automated%20Valet%0A%20%20Parking%3A%20A%20Safety%20Perspective&body=Title%3A%20Infrastructure-Assisted%20Collaborative%20Perception%20in%20Automated%20Valet%0A%20%20Parking%3A%20A%20Safety%20Perspective%0AAuthor%3A%20Yukuan%20Jia%20and%20Jiawen%20Zhang%20and%20Shimeng%20Lu%20and%20Baokang%20Fan%20and%20Ruiqing%20Mao%20and%20Sheng%20Zhou%20and%20Zhisheng%20Niu%0AAbstract%3A%20%20%20Environmental%20perception%20in%20Automated%20Valet%20Parking%20%28AVP%29%20has%20been%20a%0Achallenging%20task%20due%20to%20severe%20occlusions%20in%20parking%20garages.%20Although%0ACollaborative%20Perception%20%28CP%29%20can%20be%20applied%20to%20broaden%20the%20field%20of%20view%20of%0Aconnected%20vehicles%2C%20the%20limited%20bandwidth%20of%20vehicular%20communications%20restricts%0Aits%20application.%20In%20this%20work%2C%20we%20propose%20a%20BEV%20feature-based%20CP%20network%0Aarchitecture%20for%20infrastructure-assisted%20AVP%20systems.%20The%20model%20takes%20the%0Aroadside%20camera%20and%20LiDAR%20as%20optional%20inputs%20and%20adaptively%20fuses%20them%20with%0Aonboard%20sensors%20in%20a%20unified%20BEV%20representation.%20Autoencoder%20and%20downsampling%0Aare%20applied%20for%20channel-wise%20and%20spatial-wise%20dimension%20reduction%2C%20while%0Asparsification%20and%20quantization%20further%20compress%20the%20feature%20map%20with%20little%0Aloss%20in%20data%20precision.%20Combining%20these%20techniques%2C%20the%20size%20of%20a%20BEV%20feature%0Amap%20is%20effectively%20compressed%20to%20fit%20in%20the%20feasible%20data%20rate%20of%20the%20NR-V2X%0Anetwork.%20With%20the%20synthetic%20AVP%20dataset%2C%20we%20observe%20that%20CP%20can%20effectively%0Aincrease%20perception%20performance%2C%20especially%20for%20pedestrians.%20Moreover%2C%20the%0Aadvantage%20of%20infrastructure-assisted%20CP%20is%20demonstrated%20in%20two%20typical%0Asafety-critical%20scenarios%20in%20the%20AVP%20setting%2C%20increasing%20the%20maximum%20safe%0Acruising%20speed%20by%20up%20to%203m/s%20in%20both%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15156v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infrastructure-Assisted%20Collaborative%20Perception%20in%20Automated%20Valet%0A%20%20Parking%3A%20A%20Safety%20Perspective&entry.906535625=Yukuan%20Jia%20and%20Jiawen%20Zhang%20and%20Shimeng%20Lu%20and%20Baokang%20Fan%20and%20Ruiqing%20Mao%20and%20Sheng%20Zhou%20and%20Zhisheng%20Niu&entry.1292438233=%20%20Environmental%20perception%20in%20Automated%20Valet%20Parking%20%28AVP%29%20has%20been%20a%0Achallenging%20task%20due%20to%20severe%20occlusions%20in%20parking%20garages.%20Although%0ACollaborative%20Perception%20%28CP%29%20can%20be%20applied%20to%20broaden%20the%20field%20of%20view%20of%0Aconnected%20vehicles%2C%20the%20limited%20bandwidth%20of%20vehicular%20communications%20restricts%0Aits%20application.%20In%20this%20work%2C%20we%20propose%20a%20BEV%20feature-based%20CP%20network%0Aarchitecture%20for%20infrastructure-assisted%20AVP%20systems.%20The%20model%20takes%20the%0Aroadside%20camera%20and%20LiDAR%20as%20optional%20inputs%20and%20adaptively%20fuses%20them%20with%0Aonboard%20sensors%20in%20a%20unified%20BEV%20representation.%20Autoencoder%20and%20downsampling%0Aare%20applied%20for%20channel-wise%20and%20spatial-wise%20dimension%20reduction%2C%20while%0Asparsification%20and%20quantization%20further%20compress%20the%20feature%20map%20with%20little%0Aloss%20in%20data%20precision.%20Combining%20these%20techniques%2C%20the%20size%20of%20a%20BEV%20feature%0Amap%20is%20effectively%20compressed%20to%20fit%20in%20the%20feasible%20data%20rate%20of%20the%20NR-V2X%0Anetwork.%20With%20the%20synthetic%20AVP%20dataset%2C%20we%20observe%20that%20CP%20can%20effectively%0Aincrease%20perception%20performance%2C%20especially%20for%20pedestrians.%20Moreover%2C%20the%0Aadvantage%20of%20infrastructure-assisted%20CP%20is%20demonstrated%20in%20two%20typical%0Asafety-critical%20scenarios%20in%20the%20AVP%20setting%2C%20increasing%20the%20maximum%20safe%0Acruising%20speed%20by%20up%20to%203m/s%20in%20both%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15156v1&entry.124074799=Read"},
{"title": "Deep Generative Model based Rate-Distortion for Image Downscaling\n  Assessment", "author": "Yuanbang Liang and Bhavesh Garg and Paul L Rosin and Yipeng Qin", "abstract": "  In this paper, we propose Image Downscaling Assessment by Rate-Distortion\n(IDA-RD), a novel measure to quantitatively evaluate image downscaling\nalgorithms. In contrast to image-based methods that measure the quality of\ndownscaled images, ours is process-based that draws ideas from rate-distortion\ntheory to measure the distortion incurred during downscaling. Our main idea is\nthat downscaling and super-resolution (SR) can be viewed as the encoding and\ndecoding processes in the rate-distortion model, respectively, and that a\ndownscaling algorithm that preserves more details in the resulting\nlow-resolution (LR) images should lead to less distorted high-resolution (HR)\nimages in SR. In other words, the distortion should increase as the downscaling\nalgorithm deteriorates. However, it is non-trivial to measure this distortion\nas it requires the SR algorithm to be blind and stochastic. Our key insight is\nthat such requirements can be met by recent SR algorithms based on deep\ngenerative models that can find all matching HR images for a given LR image on\ntheir learned image manifolds. Extensive experimental results show the\neffectiveness of our IDA-RD measure.\n", "link": "http://arxiv.org/abs/2403.15139v1", "date": "2024-03-22", "relevancy": 2.281, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5925}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5869}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5413}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Generative%20Model%20based%20Rate-Distortion%20for%20Image%20Downscaling%0A%20%20Assessment&body=Title%3A%20Deep%20Generative%20Model%20based%20Rate-Distortion%20for%20Image%20Downscaling%0A%20%20Assessment%0AAuthor%3A%20Yuanbang%20Liang%20and%20Bhavesh%20Garg%20and%20Paul%20L%20Rosin%20and%20Yipeng%20Qin%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Image%20Downscaling%20Assessment%20by%20Rate-Distortion%0A%28IDA-RD%29%2C%20a%20novel%20measure%20to%20quantitatively%20evaluate%20image%20downscaling%0Aalgorithms.%20In%20contrast%20to%20image-based%20methods%20that%20measure%20the%20quality%20of%0Adownscaled%20images%2C%20ours%20is%20process-based%20that%20draws%20ideas%20from%20rate-distortion%0Atheory%20to%20measure%20the%20distortion%20incurred%20during%20downscaling.%20Our%20main%20idea%20is%0Athat%20downscaling%20and%20super-resolution%20%28SR%29%20can%20be%20viewed%20as%20the%20encoding%20and%0Adecoding%20processes%20in%20the%20rate-distortion%20model%2C%20respectively%2C%20and%20that%20a%0Adownscaling%20algorithm%20that%20preserves%20more%20details%20in%20the%20resulting%0Alow-resolution%20%28LR%29%20images%20should%20lead%20to%20less%20distorted%20high-resolution%20%28HR%29%0Aimages%20in%20SR.%20In%20other%20words%2C%20the%20distortion%20should%20increase%20as%20the%20downscaling%0Aalgorithm%20deteriorates.%20However%2C%20it%20is%20non-trivial%20to%20measure%20this%20distortion%0Aas%20it%20requires%20the%20SR%20algorithm%20to%20be%20blind%20and%20stochastic.%20Our%20key%20insight%20is%0Athat%20such%20requirements%20can%20be%20met%20by%20recent%20SR%20algorithms%20based%20on%20deep%0Agenerative%20models%20that%20can%20find%20all%20matching%20HR%20images%20for%20a%20given%20LR%20image%20on%0Atheir%20learned%20image%20manifolds.%20Extensive%20experimental%20results%20show%20the%0Aeffectiveness%20of%20our%20IDA-RD%20measure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15139v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Generative%20Model%20based%20Rate-Distortion%20for%20Image%20Downscaling%0A%20%20Assessment&entry.906535625=Yuanbang%20Liang%20and%20Bhavesh%20Garg%20and%20Paul%20L%20Rosin%20and%20Yipeng%20Qin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Image%20Downscaling%20Assessment%20by%20Rate-Distortion%0A%28IDA-RD%29%2C%20a%20novel%20measure%20to%20quantitatively%20evaluate%20image%20downscaling%0Aalgorithms.%20In%20contrast%20to%20image-based%20methods%20that%20measure%20the%20quality%20of%0Adownscaled%20images%2C%20ours%20is%20process-based%20that%20draws%20ideas%20from%20rate-distortion%0Atheory%20to%20measure%20the%20distortion%20incurred%20during%20downscaling.%20Our%20main%20idea%20is%0Athat%20downscaling%20and%20super-resolution%20%28SR%29%20can%20be%20viewed%20as%20the%20encoding%20and%0Adecoding%20processes%20in%20the%20rate-distortion%20model%2C%20respectively%2C%20and%20that%20a%0Adownscaling%20algorithm%20that%20preserves%20more%20details%20in%20the%20resulting%0Alow-resolution%20%28LR%29%20images%20should%20lead%20to%20less%20distorted%20high-resolution%20%28HR%29%0Aimages%20in%20SR.%20In%20other%20words%2C%20the%20distortion%20should%20increase%20as%20the%20downscaling%0Aalgorithm%20deteriorates.%20However%2C%20it%20is%20non-trivial%20to%20measure%20this%20distortion%0Aas%20it%20requires%20the%20SR%20algorithm%20to%20be%20blind%20and%20stochastic.%20Our%20key%20insight%20is%0Athat%20such%20requirements%20can%20be%20met%20by%20recent%20SR%20algorithms%20based%20on%20deep%0Agenerative%20models%20that%20can%20find%20all%20matching%20HR%20images%20for%20a%20given%20LR%20image%20on%0Atheir%20learned%20image%20manifolds.%20Extensive%20experimental%20results%20show%20the%0Aeffectiveness%20of%20our%20IDA-RD%20measure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15139v1&entry.124074799=Read"},
{"title": "Event-based Simultaneous Localization and Mapping: A Comprehensive\n  Survey", "author": "Kunping Huang and Sen Zhang and Jing Zhang and Dacheng Tao", "abstract": "  In recent decades, visual simultaneous localization and mapping (vSLAM) has\ngained significant interest in both academia and industry. It estimates camera\nmotion and reconstructs the environment concurrently using visual sensors on a\nmoving robot. However, conventional cameras are limited by hardware, including\nmotion blur and low dynamic range, which can negatively impact performance in\nchallenging scenarios like high-speed motion and high dynamic range\nillumination. Recent studies have demonstrated that event cameras, a new type\nof bio-inspired visual sensor, offer advantages such as high temporal\nresolution, dynamic range, low power consumption, and low latency. This paper\npresents a timely and comprehensive review of event-based vSLAM algorithms that\nexploit the benefits of asynchronous and irregular event streams for\nlocalization and mapping tasks. The review covers the working principle of\nevent cameras and various event representations for preprocessing event data.\nIt also categorizes event-based vSLAM methods into four main categories:\nfeature-based, direct, motion-compensation, and deep learning methods, with\ndetailed discussions and practical guidance for each approach. Furthermore, the\npaper evaluates the state-of-the-art methods on various benchmarks,\nhighlighting current challenges and future opportunities in this emerging\nresearch area. A public repository will be maintained to keep track of the\nrapid developments in this field at\n{\\url{https://github.com/kun150kun/ESLAM-survey}}.\n", "link": "http://arxiv.org/abs/2304.09793v2", "date": "2024-03-22", "relevancy": 2.2743, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6272}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5763}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5068}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Event-based%20Simultaneous%20Localization%20and%20Mapping%3A%20A%20Comprehensive%0A%20%20Survey&body=Title%3A%20Event-based%20Simultaneous%20Localization%20and%20Mapping%3A%20A%20Comprehensive%0A%20%20Survey%0AAuthor%3A%20Kunping%20Huang%20and%20Sen%20Zhang%20and%20Jing%20Zhang%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20In%20recent%20decades%2C%20visual%20simultaneous%20localization%20and%20mapping%20%28vSLAM%29%20has%0Agained%20significant%20interest%20in%20both%20academia%20and%20industry.%20It%20estimates%20camera%0Amotion%20and%20reconstructs%20the%20environment%20concurrently%20using%20visual%20sensors%20on%20a%0Amoving%20robot.%20However%2C%20conventional%20cameras%20are%20limited%20by%20hardware%2C%20including%0Amotion%20blur%20and%20low%20dynamic%20range%2C%20which%20can%20negatively%20impact%20performance%20in%0Achallenging%20scenarios%20like%20high-speed%20motion%20and%20high%20dynamic%20range%0Aillumination.%20Recent%20studies%20have%20demonstrated%20that%20event%20cameras%2C%20a%20new%20type%0Aof%20bio-inspired%20visual%20sensor%2C%20offer%20advantages%20such%20as%20high%20temporal%0Aresolution%2C%20dynamic%20range%2C%20low%20power%20consumption%2C%20and%20low%20latency.%20This%20paper%0Apresents%20a%20timely%20and%20comprehensive%20review%20of%20event-based%20vSLAM%20algorithms%20that%0Aexploit%20the%20benefits%20of%20asynchronous%20and%20irregular%20event%20streams%20for%0Alocalization%20and%20mapping%20tasks.%20The%20review%20covers%20the%20working%20principle%20of%0Aevent%20cameras%20and%20various%20event%20representations%20for%20preprocessing%20event%20data.%0AIt%20also%20categorizes%20event-based%20vSLAM%20methods%20into%20four%20main%20categories%3A%0Afeature-based%2C%20direct%2C%20motion-compensation%2C%20and%20deep%20learning%20methods%2C%20with%0Adetailed%20discussions%20and%20practical%20guidance%20for%20each%20approach.%20Furthermore%2C%20the%0Apaper%20evaluates%20the%20state-of-the-art%20methods%20on%20various%20benchmarks%2C%0Ahighlighting%20current%20challenges%20and%20future%20opportunities%20in%20this%20emerging%0Aresearch%20area.%20A%20public%20repository%20will%20be%20maintained%20to%20keep%20track%20of%20the%0Arapid%20developments%20in%20this%20field%20at%0A%7B%5Curl%7Bhttps%3A//github.com/kun150kun/ESLAM-survey%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.09793v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-based%20Simultaneous%20Localization%20and%20Mapping%3A%20A%20Comprehensive%0A%20%20Survey&entry.906535625=Kunping%20Huang%20and%20Sen%20Zhang%20and%20Jing%20Zhang%20and%20Dacheng%20Tao&entry.1292438233=%20%20In%20recent%20decades%2C%20visual%20simultaneous%20localization%20and%20mapping%20%28vSLAM%29%20has%0Agained%20significant%20interest%20in%20both%20academia%20and%20industry.%20It%20estimates%20camera%0Amotion%20and%20reconstructs%20the%20environment%20concurrently%20using%20visual%20sensors%20on%20a%0Amoving%20robot.%20However%2C%20conventional%20cameras%20are%20limited%20by%20hardware%2C%20including%0Amotion%20blur%20and%20low%20dynamic%20range%2C%20which%20can%20negatively%20impact%20performance%20in%0Achallenging%20scenarios%20like%20high-speed%20motion%20and%20high%20dynamic%20range%0Aillumination.%20Recent%20studies%20have%20demonstrated%20that%20event%20cameras%2C%20a%20new%20type%0Aof%20bio-inspired%20visual%20sensor%2C%20offer%20advantages%20such%20as%20high%20temporal%0Aresolution%2C%20dynamic%20range%2C%20low%20power%20consumption%2C%20and%20low%20latency.%20This%20paper%0Apresents%20a%20timely%20and%20comprehensive%20review%20of%20event-based%20vSLAM%20algorithms%20that%0Aexploit%20the%20benefits%20of%20asynchronous%20and%20irregular%20event%20streams%20for%0Alocalization%20and%20mapping%20tasks.%20The%20review%20covers%20the%20working%20principle%20of%0Aevent%20cameras%20and%20various%20event%20representations%20for%20preprocessing%20event%20data.%0AIt%20also%20categorizes%20event-based%20vSLAM%20methods%20into%20four%20main%20categories%3A%0Afeature-based%2C%20direct%2C%20motion-compensation%2C%20and%20deep%20learning%20methods%2C%20with%0Adetailed%20discussions%20and%20practical%20guidance%20for%20each%20approach.%20Furthermore%2C%20the%0Apaper%20evaluates%20the%20state-of-the-art%20methods%20on%20various%20benchmarks%2C%0Ahighlighting%20current%20challenges%20and%20future%20opportunities%20in%20this%20emerging%0Aresearch%20area.%20A%20public%20repository%20will%20be%20maintained%20to%20keep%20track%20of%20the%0Arapid%20developments%20in%20this%20field%20at%0A%7B%5Curl%7Bhttps%3A//github.com/kun150kun/ESLAM-survey%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.09793v2&entry.124074799=Read"},
{"title": "Mora: Enabling Generalist Video Generation via A Multi-Agent Framework", "author": "Zhengqing Yuan and Ruoxi Chen and Zhaoxu Li and Haolong Jia and Lifang He and Chi Wang and Lichao Sun", "abstract": "  Sora is the first large-scale generalist video generation model that garnered\nsignificant attention across society. Since its launch by OpenAI in February\n2024, no other video generation models have paralleled {Sora}'s performance or\nits capacity to support a broad spectrum of video generation tasks.\nAdditionally, there are only a few fully published video generation models,\nwith the majority being closed-source. To address this gap, this paper proposes\na new multi-agent framework Mora, which incorporates several advanced visual AI\nagents to replicate generalist video generation demonstrated by Sora. In\nparticular, Mora can utilize multiple visual agents and successfully mimic\nSora's video generation capabilities in various tasks, such as (1)\ntext-to-video generation, (2) text-conditional image-to-video generation, (3)\nextend generated videos, (4) video-to-video editing, (5) connect videos and (6)\nsimulate digital worlds. Our extensive experimental results show that Mora\nachieves performance that is proximate to that of Sora in various tasks.\nHowever, there exists an obvious performance gap between our work and Sora when\nassessed holistically. In summary, we hope this project can guide the future\ntrajectory of video generation through collaborative AI agents.\n", "link": "http://arxiv.org/abs/2403.13248v2", "date": "2024-03-22", "relevancy": 2.2693, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6069}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5459}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5222}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mora%3A%20Enabling%20Generalist%20Video%20Generation%20via%20A%20Multi-Agent%20Framework&body=Title%3A%20Mora%3A%20Enabling%20Generalist%20Video%20Generation%20via%20A%20Multi-Agent%20Framework%0AAuthor%3A%20Zhengqing%20Yuan%20and%20Ruoxi%20Chen%20and%20Zhaoxu%20Li%20and%20Haolong%20Jia%20and%20Lifang%20He%20and%20Chi%20Wang%20and%20Lichao%20Sun%0AAbstract%3A%20%20%20Sora%20is%20the%20first%20large-scale%20generalist%20video%20generation%20model%20that%20garnered%0Asignificant%20attention%20across%20society.%20Since%20its%20launch%20by%20OpenAI%20in%20February%0A2024%2C%20no%20other%20video%20generation%20models%20have%20paralleled%20%7BSora%7D%27s%20performance%20or%0Aits%20capacity%20to%20support%20a%20broad%20spectrum%20of%20video%20generation%20tasks.%0AAdditionally%2C%20there%20are%20only%20a%20few%20fully%20published%20video%20generation%20models%2C%0Awith%20the%20majority%20being%20closed-source.%20To%20address%20this%20gap%2C%20this%20paper%20proposes%0Aa%20new%20multi-agent%20framework%20Mora%2C%20which%20incorporates%20several%20advanced%20visual%20AI%0Aagents%20to%20replicate%20generalist%20video%20generation%20demonstrated%20by%20Sora.%20In%0Aparticular%2C%20Mora%20can%20utilize%20multiple%20visual%20agents%20and%20successfully%20mimic%0ASora%27s%20video%20generation%20capabilities%20in%20various%20tasks%2C%20such%20as%20%281%29%0Atext-to-video%20generation%2C%20%282%29%20text-conditional%20image-to-video%20generation%2C%20%283%29%0Aextend%20generated%20videos%2C%20%284%29%20video-to-video%20editing%2C%20%285%29%20connect%20videos%20and%20%286%29%0Asimulate%20digital%20worlds.%20Our%20extensive%20experimental%20results%20show%20that%20Mora%0Aachieves%20performance%20that%20is%20proximate%20to%20that%20of%20Sora%20in%20various%20tasks.%0AHowever%2C%20there%20exists%20an%20obvious%20performance%20gap%20between%20our%20work%20and%20Sora%20when%0Aassessed%20holistically.%20In%20summary%2C%20we%20hope%20this%20project%20can%20guide%20the%20future%0Atrajectory%20of%20video%20generation%20through%20collaborative%20AI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13248v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mora%3A%20Enabling%20Generalist%20Video%20Generation%20via%20A%20Multi-Agent%20Framework&entry.906535625=Zhengqing%20Yuan%20and%20Ruoxi%20Chen%20and%20Zhaoxu%20Li%20and%20Haolong%20Jia%20and%20Lifang%20He%20and%20Chi%20Wang%20and%20Lichao%20Sun&entry.1292438233=%20%20Sora%20is%20the%20first%20large-scale%20generalist%20video%20generation%20model%20that%20garnered%0Asignificant%20attention%20across%20society.%20Since%20its%20launch%20by%20OpenAI%20in%20February%0A2024%2C%20no%20other%20video%20generation%20models%20have%20paralleled%20%7BSora%7D%27s%20performance%20or%0Aits%20capacity%20to%20support%20a%20broad%20spectrum%20of%20video%20generation%20tasks.%0AAdditionally%2C%20there%20are%20only%20a%20few%20fully%20published%20video%20generation%20models%2C%0Awith%20the%20majority%20being%20closed-source.%20To%20address%20this%20gap%2C%20this%20paper%20proposes%0Aa%20new%20multi-agent%20framework%20Mora%2C%20which%20incorporates%20several%20advanced%20visual%20AI%0Aagents%20to%20replicate%20generalist%20video%20generation%20demonstrated%20by%20Sora.%20In%0Aparticular%2C%20Mora%20can%20utilize%20multiple%20visual%20agents%20and%20successfully%20mimic%0ASora%27s%20video%20generation%20capabilities%20in%20various%20tasks%2C%20such%20as%20%281%29%0Atext-to-video%20generation%2C%20%282%29%20text-conditional%20image-to-video%20generation%2C%20%283%29%0Aextend%20generated%20videos%2C%20%284%29%20video-to-video%20editing%2C%20%285%29%20connect%20videos%20and%20%286%29%0Asimulate%20digital%20worlds.%20Our%20extensive%20experimental%20results%20show%20that%20Mora%0Aachieves%20performance%20that%20is%20proximate%20to%20that%20of%20Sora%20in%20various%20tasks.%0AHowever%2C%20there%20exists%20an%20obvious%20performance%20gap%20between%20our%20work%20and%20Sora%20when%0Aassessed%20holistically.%20In%20summary%2C%20we%20hope%20this%20project%20can%20guide%20the%20future%0Atrajectory%20of%20video%20generation%20through%20collaborative%20AI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13248v2&entry.124074799=Read"},
{"title": "Multi-perspective Memory Enhanced Network for Identifying Key Nodes in\n  Social Networks", "author": "Qiang Zhang and Jiawei Liu and Fanrui Zhang and Xiaoling Zhu and Zheng-Jun Zha", "abstract": "  Identifying key nodes in social networks plays a crucial role in timely\nblocking false information. Existing key node identification methods usually\nconsider node influence only from the propagation structure perspective and\nhave insufficient generalization ability to unknown scenarios. In this paper,\nwe propose a novel Multi-perspective Memory Enhanced Network (MMEN) for\nidentifying key nodes in social networks, which mines key nodes from multiple\nperspectives and utilizes memory networks to store historical information.\nSpecifically, MMEN first constructs two propagation networks from the\nperspectives of user attributes and propagation structure and updates node\nfeature representations using graph attention networks. Meanwhile, the memory\nnetwork is employed to store information of similar subgraphs, enhancing the\nmodel's generalization performance in unknown scenarios. Finally, MMEN applies\nadaptive weights to combine the node influence of the two propagation networks\nto select the ultimate key nodes. Extensive experiments demonstrate that our\nmethod significantly outperforms previous methods.\n", "link": "http://arxiv.org/abs/2403.15235v1", "date": "2024-03-22", "relevancy": 2.2674, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4799}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4322}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-perspective%20Memory%20Enhanced%20Network%20for%20Identifying%20Key%20Nodes%20in%0A%20%20Social%20Networks&body=Title%3A%20Multi-perspective%20Memory%20Enhanced%20Network%20for%20Identifying%20Key%20Nodes%20in%0A%20%20Social%20Networks%0AAuthor%3A%20Qiang%20Zhang%20and%20Jiawei%20Liu%20and%20Fanrui%20Zhang%20and%20Xiaoling%20Zhu%20and%20Zheng-Jun%20Zha%0AAbstract%3A%20%20%20Identifying%20key%20nodes%20in%20social%20networks%20plays%20a%20crucial%20role%20in%20timely%0Ablocking%20false%20information.%20Existing%20key%20node%20identification%20methods%20usually%0Aconsider%20node%20influence%20only%20from%20the%20propagation%20structure%20perspective%20and%0Ahave%20insufficient%20generalization%20ability%20to%20unknown%20scenarios.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20Multi-perspective%20Memory%20Enhanced%20Network%20%28MMEN%29%20for%0Aidentifying%20key%20nodes%20in%20social%20networks%2C%20which%20mines%20key%20nodes%20from%20multiple%0Aperspectives%20and%20utilizes%20memory%20networks%20to%20store%20historical%20information.%0ASpecifically%2C%20MMEN%20first%20constructs%20two%20propagation%20networks%20from%20the%0Aperspectives%20of%20user%20attributes%20and%20propagation%20structure%20and%20updates%20node%0Afeature%20representations%20using%20graph%20attention%20networks.%20Meanwhile%2C%20the%20memory%0Anetwork%20is%20employed%20to%20store%20information%20of%20similar%20subgraphs%2C%20enhancing%20the%0Amodel%27s%20generalization%20performance%20in%20unknown%20scenarios.%20Finally%2C%20MMEN%20applies%0Aadaptive%20weights%20to%20combine%20the%20node%20influence%20of%20the%20two%20propagation%20networks%0Ato%20select%20the%20ultimate%20key%20nodes.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15235v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-perspective%20Memory%20Enhanced%20Network%20for%20Identifying%20Key%20Nodes%20in%0A%20%20Social%20Networks&entry.906535625=Qiang%20Zhang%20and%20Jiawei%20Liu%20and%20Fanrui%20Zhang%20and%20Xiaoling%20Zhu%20and%20Zheng-Jun%20Zha&entry.1292438233=%20%20Identifying%20key%20nodes%20in%20social%20networks%20plays%20a%20crucial%20role%20in%20timely%0Ablocking%20false%20information.%20Existing%20key%20node%20identification%20methods%20usually%0Aconsider%20node%20influence%20only%20from%20the%20propagation%20structure%20perspective%20and%0Ahave%20insufficient%20generalization%20ability%20to%20unknown%20scenarios.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20Multi-perspective%20Memory%20Enhanced%20Network%20%28MMEN%29%20for%0Aidentifying%20key%20nodes%20in%20social%20networks%2C%20which%20mines%20key%20nodes%20from%20multiple%0Aperspectives%20and%20utilizes%20memory%20networks%20to%20store%20historical%20information.%0ASpecifically%2C%20MMEN%20first%20constructs%20two%20propagation%20networks%20from%20the%0Aperspectives%20of%20user%20attributes%20and%20propagation%20structure%20and%20updates%20node%0Afeature%20representations%20using%20graph%20attention%20networks.%20Meanwhile%2C%20the%20memory%0Anetwork%20is%20employed%20to%20store%20information%20of%20similar%20subgraphs%2C%20enhancing%20the%0Amodel%27s%20generalization%20performance%20in%20unknown%20scenarios.%20Finally%2C%20MMEN%20applies%0Aadaptive%20weights%20to%20combine%20the%20node%20influence%20of%20the%20two%20propagation%20networks%0Ato%20select%20the%20ultimate%20key%20nodes.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15235v1&entry.124074799=Read"},
{"title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training", "author": "Brandon McKinzie and Zhe Gan and Jean-Philippe Fauconnier and Sam Dodge and Bowen Zhang and Philipp Dufter and Dhruti Shah and Xianzhi Du and Futang Peng and Floris Weers and Anton Belyi and Haotian Zhang and Karanjeet Singh and Doug Kang and Ankur Jain and Hongyu H\u00e8 and Max Schwarzer and Tom Gunter and Xiang Kong and Aonan Zhang and Jianyu Wang and Chong Wang and Nan Du and Tao Lei and Sam Wiseman and Guoli Yin and Mark Lee and Zirui Wang and Ruoming Pang and Peter Grasch and Alexander Toshev and Yinfei Yang", "abstract": "  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n", "link": "http://arxiv.org/abs/2403.09611v3", "date": "2024-03-22", "relevancy": 2.2627, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6237}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5266}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5233}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MM1%3A%20Methods%2C%20Analysis%20%26%20Insights%20from%20Multimodal%20LLM%20Pre-training&body=Title%3A%20MM1%3A%20Methods%2C%20Analysis%20%26%20Insights%20from%20Multimodal%20LLM%20Pre-training%0AAuthor%3A%20Brandon%20McKinzie%20and%20Zhe%20Gan%20and%20Jean-Philippe%20Fauconnier%20and%20Sam%20Dodge%20and%20Bowen%20Zhang%20and%20Philipp%20Dufter%20and%20Dhruti%20Shah%20and%20Xianzhi%20Du%20and%20Futang%20Peng%20and%20Floris%20Weers%20and%20Anton%20Belyi%20and%20Haotian%20Zhang%20and%20Karanjeet%20Singh%20and%20Doug%20Kang%20and%20Ankur%20Jain%20and%20Hongyu%20H%C3%A8%20and%20Max%20Schwarzer%20and%20Tom%20Gunter%20and%20Xiang%20Kong%20and%20Aonan%20Zhang%20and%20Jianyu%20Wang%20and%20Chong%20Wang%20and%20Nan%20Du%20and%20Tao%20Lei%20and%20Sam%20Wiseman%20and%20Guoli%20Yin%20and%20Mark%20Lee%20and%20Zirui%20Wang%20and%20Ruoming%20Pang%20and%20Peter%20Grasch%20and%20Alexander%20Toshev%20and%20Yinfei%20Yang%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20discuss%20building%20performant%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29.%20In%20particular%2C%20we%20study%20the%20importance%20of%20various%20architecture%0Acomponents%20and%20data%20choices.%20Through%20careful%20and%20comprehensive%20ablations%20of%20the%0Aimage%20encoder%2C%20the%20vision%20language%20connector%2C%20and%20various%20pre-training%20data%0Achoices%2C%20we%20identified%20several%20crucial%20design%20lessons.%20For%20example%2C%20we%0Ademonstrate%20that%20for%20large-scale%20multimodal%20pre-training%20using%20a%20careful%20mix%20of%0Aimage-caption%2C%20interleaved%20image-text%2C%20and%20text-only%20data%20is%20crucial%20for%0Aachieving%20state-of-the-art%20%28SOTA%29%20few-shot%20results%20across%20multiple%20benchmarks%2C%0Acompared%20to%20other%20published%20pre-training%20results.%20Further%2C%20we%20show%20that%20the%0Aimage%20encoder%20together%20with%20image%20resolution%20and%20the%20image%20token%20count%20has%0Asubstantial%20impact%2C%20while%20the%20vision-language%20connector%20design%20is%20of%0Acomparatively%20negligible%20importance.%20By%20scaling%20up%20the%20presented%20recipe%2C%20we%0Abuild%20MM1%2C%20a%20family%20of%20multimodal%20models%20up%20to%2030B%20parameters%2C%20including%20both%0Adense%20models%20and%20mixture-of-experts%20%28MoE%29%20variants%2C%20that%20are%20SOTA%20in%0Apre-training%20metrics%20and%20achieve%20competitive%20performance%20after%20supervised%0Afine-tuning%20on%20a%20range%20of%20established%20multimodal%20benchmarks.%20Thanks%20to%0Alarge-scale%20pre-training%2C%20MM1%20enjoys%20appealing%20properties%20such%20as%20enhanced%0Ain-context%20learning%2C%20and%20multi-image%20reasoning%2C%20enabling%20few-shot%0Achain-of-thought%20prompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09611v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM1%3A%20Methods%2C%20Analysis%20%26%20Insights%20from%20Multimodal%20LLM%20Pre-training&entry.906535625=Brandon%20McKinzie%20and%20Zhe%20Gan%20and%20Jean-Philippe%20Fauconnier%20and%20Sam%20Dodge%20and%20Bowen%20Zhang%20and%20Philipp%20Dufter%20and%20Dhruti%20Shah%20and%20Xianzhi%20Du%20and%20Futang%20Peng%20and%20Floris%20Weers%20and%20Anton%20Belyi%20and%20Haotian%20Zhang%20and%20Karanjeet%20Singh%20and%20Doug%20Kang%20and%20Ankur%20Jain%20and%20Hongyu%20H%C3%A8%20and%20Max%20Schwarzer%20and%20Tom%20Gunter%20and%20Xiang%20Kong%20and%20Aonan%20Zhang%20and%20Jianyu%20Wang%20and%20Chong%20Wang%20and%20Nan%20Du%20and%20Tao%20Lei%20and%20Sam%20Wiseman%20and%20Guoli%20Yin%20and%20Mark%20Lee%20and%20Zirui%20Wang%20and%20Ruoming%20Pang%20and%20Peter%20Grasch%20and%20Alexander%20Toshev%20and%20Yinfei%20Yang&entry.1292438233=%20%20In%20this%20work%2C%20we%20discuss%20building%20performant%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29.%20In%20particular%2C%20we%20study%20the%20importance%20of%20various%20architecture%0Acomponents%20and%20data%20choices.%20Through%20careful%20and%20comprehensive%20ablations%20of%20the%0Aimage%20encoder%2C%20the%20vision%20language%20connector%2C%20and%20various%20pre-training%20data%0Achoices%2C%20we%20identified%20several%20crucial%20design%20lessons.%20For%20example%2C%20we%0Ademonstrate%20that%20for%20large-scale%20multimodal%20pre-training%20using%20a%20careful%20mix%20of%0Aimage-caption%2C%20interleaved%20image-text%2C%20and%20text-only%20data%20is%20crucial%20for%0Aachieving%20state-of-the-art%20%28SOTA%29%20few-shot%20results%20across%20multiple%20benchmarks%2C%0Acompared%20to%20other%20published%20pre-training%20results.%20Further%2C%20we%20show%20that%20the%0Aimage%20encoder%20together%20with%20image%20resolution%20and%20the%20image%20token%20count%20has%0Asubstantial%20impact%2C%20while%20the%20vision-language%20connector%20design%20is%20of%0Acomparatively%20negligible%20importance.%20By%20scaling%20up%20the%20presented%20recipe%2C%20we%0Abuild%20MM1%2C%20a%20family%20of%20multimodal%20models%20up%20to%2030B%20parameters%2C%20including%20both%0Adense%20models%20and%20mixture-of-experts%20%28MoE%29%20variants%2C%20that%20are%20SOTA%20in%0Apre-training%20metrics%20and%20achieve%20competitive%20performance%20after%20supervised%0Afine-tuning%20on%20a%20range%20of%20established%20multimodal%20benchmarks.%20Thanks%20to%0Alarge-scale%20pre-training%2C%20MM1%20enjoys%20appealing%20properties%20such%20as%20enhanced%0Ain-context%20learning%2C%20and%20multi-image%20reasoning%2C%20enabling%20few-shot%0Achain-of-thought%20prompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09611v3&entry.124074799=Read"},
{"title": "Learning from Visual Demonstrations through Differentiable Nonlinear MPC\n  for Personalized Autonomous Driving", "author": "Flavia Sofia Acerbo and Jan Swevers and Tinne Tuytelaars and Tong Duy Son", "abstract": "  Human-like autonomous driving controllers have the potential to enhance\npassenger perception of autonomous vehicles. This paper proposes DriViDOC: a\nmodel for Driving from Vision through Differentiable Optimal Control, and its\napplication to learn personalized autonomous driving controllers from human\ndemonstrations. DriViDOC combines the automatic inference of relevant features\nfrom camera frames with the properties of nonlinear model predictive control\n(NMPC), such as constraint satisfaction. Our approach leverages the\ndifferentiability of parametric NMPC, allowing for end-to-end learning of the\ndriving model from images to control. The model is trained on an offline\ndataset comprising various driving styles collected on a motion-base driving\nsimulator. During online testing, the model demonstrates successful imitation\nof different driving styles, and the interpreted NMPC parameters provide\ninsights into the achievement of specific driving behaviors. Our experimental\nresults show that DriViDOC outperforms other methods involving NMPC and neural\nnetworks, exhibiting an average improvement of 20% in imitation scores.\n", "link": "http://arxiv.org/abs/2403.15102v1", "date": "2024-03-22", "relevancy": 2.2619, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5998}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5831}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5241}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Visual%20Demonstrations%20through%20Differentiable%20Nonlinear%20MPC%0A%20%20for%20Personalized%20Autonomous%20Driving&body=Title%3A%20Learning%20from%20Visual%20Demonstrations%20through%20Differentiable%20Nonlinear%20MPC%0A%20%20for%20Personalized%20Autonomous%20Driving%0AAuthor%3A%20Flavia%20Sofia%20Acerbo%20and%20Jan%20Swevers%20and%20Tinne%20Tuytelaars%20and%20Tong%20Duy%20Son%0AAbstract%3A%20%20%20Human-like%20autonomous%20driving%20controllers%20have%20the%20potential%20to%20enhance%0Apassenger%20perception%20of%20autonomous%20vehicles.%20This%20paper%20proposes%20DriViDOC%3A%20a%0Amodel%20for%20Driving%20from%20Vision%20through%20Differentiable%20Optimal%20Control%2C%20and%20its%0Aapplication%20to%20learn%20personalized%20autonomous%20driving%20controllers%20from%20human%0Ademonstrations.%20DriViDOC%20combines%20the%20automatic%20inference%20of%20relevant%20features%0Afrom%20camera%20frames%20with%20the%20properties%20of%20nonlinear%20model%20predictive%20control%0A%28NMPC%29%2C%20such%20as%20constraint%20satisfaction.%20Our%20approach%20leverages%20the%0Adifferentiability%20of%20parametric%20NMPC%2C%20allowing%20for%20end-to-end%20learning%20of%20the%0Adriving%20model%20from%20images%20to%20control.%20The%20model%20is%20trained%20on%20an%20offline%0Adataset%20comprising%20various%20driving%20styles%20collected%20on%20a%20motion-base%20driving%0Asimulator.%20During%20online%20testing%2C%20the%20model%20demonstrates%20successful%20imitation%0Aof%20different%20driving%20styles%2C%20and%20the%20interpreted%20NMPC%20parameters%20provide%0Ainsights%20into%20the%20achievement%20of%20specific%20driving%20behaviors.%20Our%20experimental%0Aresults%20show%20that%20DriViDOC%20outperforms%20other%20methods%20involving%20NMPC%20and%20neural%0Anetworks%2C%20exhibiting%20an%20average%20improvement%20of%2020%25%20in%20imitation%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15102v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Visual%20Demonstrations%20through%20Differentiable%20Nonlinear%20MPC%0A%20%20for%20Personalized%20Autonomous%20Driving&entry.906535625=Flavia%20Sofia%20Acerbo%20and%20Jan%20Swevers%20and%20Tinne%20Tuytelaars%20and%20Tong%20Duy%20Son&entry.1292438233=%20%20Human-like%20autonomous%20driving%20controllers%20have%20the%20potential%20to%20enhance%0Apassenger%20perception%20of%20autonomous%20vehicles.%20This%20paper%20proposes%20DriViDOC%3A%20a%0Amodel%20for%20Driving%20from%20Vision%20through%20Differentiable%20Optimal%20Control%2C%20and%20its%0Aapplication%20to%20learn%20personalized%20autonomous%20driving%20controllers%20from%20human%0Ademonstrations.%20DriViDOC%20combines%20the%20automatic%20inference%20of%20relevant%20features%0Afrom%20camera%20frames%20with%20the%20properties%20of%20nonlinear%20model%20predictive%20control%0A%28NMPC%29%2C%20such%20as%20constraint%20satisfaction.%20Our%20approach%20leverages%20the%0Adifferentiability%20of%20parametric%20NMPC%2C%20allowing%20for%20end-to-end%20learning%20of%20the%0Adriving%20model%20from%20images%20to%20control.%20The%20model%20is%20trained%20on%20an%20offline%0Adataset%20comprising%20various%20driving%20styles%20collected%20on%20a%20motion-base%20driving%0Asimulator.%20During%20online%20testing%2C%20the%20model%20demonstrates%20successful%20imitation%0Aof%20different%20driving%20styles%2C%20and%20the%20interpreted%20NMPC%20parameters%20provide%0Ainsights%20into%20the%20achievement%20of%20specific%20driving%20behaviors.%20Our%20experimental%0Aresults%20show%20that%20DriViDOC%20outperforms%20other%20methods%20involving%20NMPC%20and%20neural%0Anetworks%2C%20exhibiting%20an%20average%20improvement%20of%2020%25%20in%20imitation%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15102v1&entry.124074799=Read"},
{"title": "PIA: Your Personalized Image Animator via Plug-and-Play Modules in\n  Text-to-Image Models", "author": "Yiming Zhang and Zhening Xing and Yanhong Zeng and Youqing Fang and Kai Chen", "abstract": "  Recent advancements in personalized text-to-image (T2I) models have\nrevolutionized content creation, empowering non-experts to generate stunning\nimages with unique styles. While promising, adding realistic motions into these\npersonalized images by text poses significant challenges in preserving distinct\nstyles, high-fidelity details, and achieving motion controllability by text. In\nthis paper, we present PIA, a Personalized Image Animator that excels in\naligning with condition images, achieving motion controllability by text, and\nthe compatibility with various personalized T2I models without specific tuning.\nTo achieve these goals, PIA builds upon a base T2I model with well-trained\ntemporal alignment layers, allowing for the seamless transformation of any\npersonalized T2I model into an image animation model. A key component of PIA is\nthe introduction of the condition module, which utilizes the condition frame\nand inter-frame affinity as input to transfer appearance information guided by\nthe affinity hint for individual frame synthesis in the latent space. This\ndesign mitigates the challenges of appearance-related image alignment within\nand allows for a stronger focus on aligning with motion-related guidance.\n", "link": "http://arxiv.org/abs/2312.13964v2", "date": "2024-03-22", "relevancy": 2.2606, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6391}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5287}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PIA%3A%20Your%20Personalized%20Image%20Animator%20via%20Plug-and-Play%20Modules%20in%0A%20%20Text-to-Image%20Models&body=Title%3A%20PIA%3A%20Your%20Personalized%20Image%20Animator%20via%20Plug-and-Play%20Modules%20in%0A%20%20Text-to-Image%20Models%0AAuthor%3A%20Yiming%20Zhang%20and%20Zhening%20Xing%20and%20Yanhong%20Zeng%20and%20Youqing%20Fang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20personalized%20text-to-image%20%28T2I%29%20models%20have%0Arevolutionized%20content%20creation%2C%20empowering%20non-experts%20to%20generate%20stunning%0Aimages%20with%20unique%20styles.%20While%20promising%2C%20adding%20realistic%20motions%20into%20these%0Apersonalized%20images%20by%20text%20poses%20significant%20challenges%20in%20preserving%20distinct%0Astyles%2C%20high-fidelity%20details%2C%20and%20achieving%20motion%20controllability%20by%20text.%20In%0Athis%20paper%2C%20we%20present%20PIA%2C%20a%20Personalized%20Image%20Animator%20that%20excels%20in%0Aaligning%20with%20condition%20images%2C%20achieving%20motion%20controllability%20by%20text%2C%20and%0Athe%20compatibility%20with%20various%20personalized%20T2I%20models%20without%20specific%20tuning.%0ATo%20achieve%20these%20goals%2C%20PIA%20builds%20upon%20a%20base%20T2I%20model%20with%20well-trained%0Atemporal%20alignment%20layers%2C%20allowing%20for%20the%20seamless%20transformation%20of%20any%0Apersonalized%20T2I%20model%20into%20an%20image%20animation%20model.%20A%20key%20component%20of%20PIA%20is%0Athe%20introduction%20of%20the%20condition%20module%2C%20which%20utilizes%20the%20condition%20frame%0Aand%20inter-frame%20affinity%20as%20input%20to%20transfer%20appearance%20information%20guided%20by%0Athe%20affinity%20hint%20for%20individual%20frame%20synthesis%20in%20the%20latent%20space.%20This%0Adesign%20mitigates%20the%20challenges%20of%20appearance-related%20image%20alignment%20within%0Aand%20allows%20for%20a%20stronger%20focus%20on%20aligning%20with%20motion-related%20guidance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13964v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIA%3A%20Your%20Personalized%20Image%20Animator%20via%20Plug-and-Play%20Modules%20in%0A%20%20Text-to-Image%20Models&entry.906535625=Yiming%20Zhang%20and%20Zhening%20Xing%20and%20Yanhong%20Zeng%20and%20Youqing%20Fang%20and%20Kai%20Chen&entry.1292438233=%20%20Recent%20advancements%20in%20personalized%20text-to-image%20%28T2I%29%20models%20have%0Arevolutionized%20content%20creation%2C%20empowering%20non-experts%20to%20generate%20stunning%0Aimages%20with%20unique%20styles.%20While%20promising%2C%20adding%20realistic%20motions%20into%20these%0Apersonalized%20images%20by%20text%20poses%20significant%20challenges%20in%20preserving%20distinct%0Astyles%2C%20high-fidelity%20details%2C%20and%20achieving%20motion%20controllability%20by%20text.%20In%0Athis%20paper%2C%20we%20present%20PIA%2C%20a%20Personalized%20Image%20Animator%20that%20excels%20in%0Aaligning%20with%20condition%20images%2C%20achieving%20motion%20controllability%20by%20text%2C%20and%0Athe%20compatibility%20with%20various%20personalized%20T2I%20models%20without%20specific%20tuning.%0ATo%20achieve%20these%20goals%2C%20PIA%20builds%20upon%20a%20base%20T2I%20model%20with%20well-trained%0Atemporal%20alignment%20layers%2C%20allowing%20for%20the%20seamless%20transformation%20of%20any%0Apersonalized%20T2I%20model%20into%20an%20image%20animation%20model.%20A%20key%20component%20of%20PIA%20is%0Athe%20introduction%20of%20the%20condition%20module%2C%20which%20utilizes%20the%20condition%20frame%0Aand%20inter-frame%20affinity%20as%20input%20to%20transfer%20appearance%20information%20guided%20by%0Athe%20affinity%20hint%20for%20individual%20frame%20synthesis%20in%20the%20latent%20space.%20This%0Adesign%20mitigates%20the%20challenges%20of%20appearance-related%20image%20alignment%20within%0Aand%20allows%20for%20a%20stronger%20focus%20on%20aligning%20with%20motion-related%20guidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13964v2&entry.124074799=Read"},
{"title": "LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers", "author": "Massinissa Merouani and Khaled Afif Boudaoud and Iheb Nassim Aouadj and Nassim Tchoulak and Islem Kara Bernou and Hamza Benyamina and Fatima Benbouzid-Si Tayeb and Karima Benatchba and Hugh Leather and Riyadh Baghdadi", "abstract": "  While polyhedral compilers have shown success in implementing advanced code\ntransformations, they still have challenges in selecting the most profitable\ntransformations that lead to the best speedups. This has motivated the use of\nmachine learning to build cost models to guide the search for polyhedral\noptimizations. State-of-the-art polyhedral compilers have demonstrated a viable\nproof-of-concept of this approach. While such a proof-of-concept has shown\npromise, it still has significant limitations. State-of-the-art polyhedral\ncompilers that use a deep-learning cost model only support a small subset of\naffine transformations, limiting their ability to apply complex code\ntransformations. They also only support simple programs that have a single loop\nnest and a rectangular iteration domain, limiting their applicability to many\nprograms. These limitations significantly impact the generality of such\ncompilers and autoschedulers and put into question the whole approach. In this\npaper, we introduce LOOPer, the first polyhedral autoscheduler that uses a\ndeep-learning based cost model and covers a large set of affine transformations\nand programs. It supports the exploration of a large set of affine\ntransformations, allowing the application of complex sequences of polyhedral\ntransformations. It also supports the optimization of programs with multiple\nloop nests and with rectangular and non-rectangular iteration domains, allowing\nthe optimization of an extensive set of programs. We implement and evaluate\nLOOPer and show that it achieves speedups over the state-of-the-art. On the\nPolybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over\nTiramisu. LOOPer also achieves competitive speedups with a geometric mean\nspeedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does\nnot use a machine-learning based cost model.\n", "link": "http://arxiv.org/abs/2403.11522v2", "date": "2024-03-22", "relevancy": 2.2559, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4683}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4613}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.424}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LOOPer%3A%20A%20Learned%20Automatic%20Code%20Optimizer%20For%20Polyhedral%20Compilers&body=Title%3A%20LOOPer%3A%20A%20Learned%20Automatic%20Code%20Optimizer%20For%20Polyhedral%20Compilers%0AAuthor%3A%20Massinissa%20Merouani%20and%20Khaled%20Afif%20Boudaoud%20and%20Iheb%20Nassim%20Aouadj%20and%20Nassim%20Tchoulak%20and%20Islem%20Kara%20Bernou%20and%20Hamza%20Benyamina%20and%20Fatima%20Benbouzid-Si%20Tayeb%20and%20Karima%20Benatchba%20and%20Hugh%20Leather%20and%20Riyadh%20Baghdadi%0AAbstract%3A%20%20%20While%20polyhedral%20compilers%20have%20shown%20success%20in%20implementing%20advanced%20code%0Atransformations%2C%20they%20still%20have%20challenges%20in%20selecting%20the%20most%20profitable%0Atransformations%20that%20lead%20to%20the%20best%20speedups.%20This%20has%20motivated%20the%20use%20of%0Amachine%20learning%20to%20build%20cost%20models%20to%20guide%20the%20search%20for%20polyhedral%0Aoptimizations.%20State-of-the-art%20polyhedral%20compilers%20have%20demonstrated%20a%20viable%0Aproof-of-concept%20of%20this%20approach.%20While%20such%20a%20proof-of-concept%20has%20shown%0Apromise%2C%20it%20still%20has%20significant%20limitations.%20State-of-the-art%20polyhedral%0Acompilers%20that%20use%20a%20deep-learning%20cost%20model%20only%20support%20a%20small%20subset%20of%0Aaffine%20transformations%2C%20limiting%20their%20ability%20to%20apply%20complex%20code%0Atransformations.%20They%20also%20only%20support%20simple%20programs%20that%20have%20a%20single%20loop%0Anest%20and%20a%20rectangular%20iteration%20domain%2C%20limiting%20their%20applicability%20to%20many%0Aprograms.%20These%20limitations%20significantly%20impact%20the%20generality%20of%20such%0Acompilers%20and%20autoschedulers%20and%20put%20into%20question%20the%20whole%20approach.%20In%20this%0Apaper%2C%20we%20introduce%20LOOPer%2C%20the%20first%20polyhedral%20autoscheduler%20that%20uses%20a%0Adeep-learning%20based%20cost%20model%20and%20covers%20a%20large%20set%20of%20affine%20transformations%0Aand%20programs.%20It%20supports%20the%20exploration%20of%20a%20large%20set%20of%20affine%0Atransformations%2C%20allowing%20the%20application%20of%20complex%20sequences%20of%20polyhedral%0Atransformations.%20It%20also%20supports%20the%20optimization%20of%20programs%20with%20multiple%0Aloop%20nests%20and%20with%20rectangular%20and%20non-rectangular%20iteration%20domains%2C%20allowing%0Athe%20optimization%20of%20an%20extensive%20set%20of%20programs.%20We%20implement%20and%20evaluate%0ALOOPer%20and%20show%20that%20it%20achieves%20speedups%20over%20the%20state-of-the-art.%20On%20the%0APolybench%20benchmark%2C%20LOOPer%20achieves%20a%20geometric%20mean%20speedup%20of%201.59x%20over%0ATiramisu.%20LOOPer%20also%20achieves%20competitive%20speedups%20with%20a%20geometric%20mean%0Aspeedup%20of%201.34x%20over%20Pluto%2C%20a%20state-of-the-art%20polyhedral%20compiler%20that%20does%0Anot%20use%20a%20machine-learning%20based%20cost%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11522v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOOPer%3A%20A%20Learned%20Automatic%20Code%20Optimizer%20For%20Polyhedral%20Compilers&entry.906535625=Massinissa%20Merouani%20and%20Khaled%20Afif%20Boudaoud%20and%20Iheb%20Nassim%20Aouadj%20and%20Nassim%20Tchoulak%20and%20Islem%20Kara%20Bernou%20and%20Hamza%20Benyamina%20and%20Fatima%20Benbouzid-Si%20Tayeb%20and%20Karima%20Benatchba%20and%20Hugh%20Leather%20and%20Riyadh%20Baghdadi&entry.1292438233=%20%20While%20polyhedral%20compilers%20have%20shown%20success%20in%20implementing%20advanced%20code%0Atransformations%2C%20they%20still%20have%20challenges%20in%20selecting%20the%20most%20profitable%0Atransformations%20that%20lead%20to%20the%20best%20speedups.%20This%20has%20motivated%20the%20use%20of%0Amachine%20learning%20to%20build%20cost%20models%20to%20guide%20the%20search%20for%20polyhedral%0Aoptimizations.%20State-of-the-art%20polyhedral%20compilers%20have%20demonstrated%20a%20viable%0Aproof-of-concept%20of%20this%20approach.%20While%20such%20a%20proof-of-concept%20has%20shown%0Apromise%2C%20it%20still%20has%20significant%20limitations.%20State-of-the-art%20polyhedral%0Acompilers%20that%20use%20a%20deep-learning%20cost%20model%20only%20support%20a%20small%20subset%20of%0Aaffine%20transformations%2C%20limiting%20their%20ability%20to%20apply%20complex%20code%0Atransformations.%20They%20also%20only%20support%20simple%20programs%20that%20have%20a%20single%20loop%0Anest%20and%20a%20rectangular%20iteration%20domain%2C%20limiting%20their%20applicability%20to%20many%0Aprograms.%20These%20limitations%20significantly%20impact%20the%20generality%20of%20such%0Acompilers%20and%20autoschedulers%20and%20put%20into%20question%20the%20whole%20approach.%20In%20this%0Apaper%2C%20we%20introduce%20LOOPer%2C%20the%20first%20polyhedral%20autoscheduler%20that%20uses%20a%0Adeep-learning%20based%20cost%20model%20and%20covers%20a%20large%20set%20of%20affine%20transformations%0Aand%20programs.%20It%20supports%20the%20exploration%20of%20a%20large%20set%20of%20affine%0Atransformations%2C%20allowing%20the%20application%20of%20complex%20sequences%20of%20polyhedral%0Atransformations.%20It%20also%20supports%20the%20optimization%20of%20programs%20with%20multiple%0Aloop%20nests%20and%20with%20rectangular%20and%20non-rectangular%20iteration%20domains%2C%20allowing%0Athe%20optimization%20of%20an%20extensive%20set%20of%20programs.%20We%20implement%20and%20evaluate%0ALOOPer%20and%20show%20that%20it%20achieves%20speedups%20over%20the%20state-of-the-art.%20On%20the%0APolybench%20benchmark%2C%20LOOPer%20achieves%20a%20geometric%20mean%20speedup%20of%201.59x%20over%0ATiramisu.%20LOOPer%20also%20achieves%20competitive%20speedups%20with%20a%20geometric%20mean%0Aspeedup%20of%201.34x%20over%20Pluto%2C%20a%20state-of-the-art%20polyhedral%20compiler%20that%20does%0Anot%20use%20a%20machine-learning%20based%20cost%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11522v2&entry.124074799=Read"},
{"title": "Ultrasound Imaging based on the Variance of a Diffusion Restoration\n  Model", "author": "Yuxin Zhang and Cl\u00e9ment Huneau and J\u00e9r\u00f4me Idier and Diana Mateus", "abstract": "  Despite today's prevalence of ultrasound imaging in medicine, ultrasound\nsignal-to-noise ratio is still affected by several sources of noise and\nartefacts. Moreover, enhancing ultrasound image quality involves balancing\nconcurrent factors like contrast, resolution, and speckle preservation.\nRecently, there has been progress in both model-based and learning-based\napproaches addressing the problem of ultrasound image reconstruction. Bringing\nthe best from both worlds, we propose a hybrid reconstruction method combining\nan ultrasound linear direct model with a learning-based prior coming from a\ngenerative Denoising Diffusion model. More specifically, we rely on the\nunsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model\n(DDRM). Given the nature of multiplicative noise inherent to ultrasound, this\npaper proposes an empirical model to characterize the stochasticity of\ndiffusion reconstruction of ultrasound images, and shows the interest of its\nvariance as an echogenicity map estimator. We conduct experiments on synthetic,\nin-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging\napproach in achieving high-quality image reconstructions from single plane-wave\nacquisitions and in comparison to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.15316v1", "date": "2024-03-22", "relevancy": 2.2471, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5992}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.576}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5186}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ultrasound%20Imaging%20based%20on%20the%20Variance%20of%20a%20Diffusion%20Restoration%0A%20%20Model&body=Title%3A%20Ultrasound%20Imaging%20based%20on%20the%20Variance%20of%20a%20Diffusion%20Restoration%0A%20%20Model%0AAuthor%3A%20Yuxin%20Zhang%20and%20Cl%C3%A9ment%20Huneau%20and%20J%C3%A9r%C3%B4me%20Idier%20and%20Diana%20Mateus%0AAbstract%3A%20%20%20Despite%20today%27s%20prevalence%20of%20ultrasound%20imaging%20in%20medicine%2C%20ultrasound%0Asignal-to-noise%20ratio%20is%20still%20affected%20by%20several%20sources%20of%20noise%20and%0Aartefacts.%20Moreover%2C%20enhancing%20ultrasound%20image%20quality%20involves%20balancing%0Aconcurrent%20factors%20like%20contrast%2C%20resolution%2C%20and%20speckle%20preservation.%0ARecently%2C%20there%20has%20been%20progress%20in%20both%20model-based%20and%20learning-based%0Aapproaches%20addressing%20the%20problem%20of%20ultrasound%20image%20reconstruction.%20Bringing%0Athe%20best%20from%20both%20worlds%2C%20we%20propose%20a%20hybrid%20reconstruction%20method%20combining%0Aan%20ultrasound%20linear%20direct%20model%20with%20a%20learning-based%20prior%20coming%20from%20a%0Agenerative%20Denoising%20Diffusion%20model.%20More%20specifically%2C%20we%20rely%20on%20the%0Aunsupervised%20fine-tuning%20of%20a%20pre-trained%20Denoising%20Diffusion%20Restoration%20Model%0A%28DDRM%29.%20Given%20the%20nature%20of%20multiplicative%20noise%20inherent%20to%20ultrasound%2C%20this%0Apaper%20proposes%20an%20empirical%20model%20to%20characterize%20the%20stochasticity%20of%0Adiffusion%20reconstruction%20of%20ultrasound%20images%2C%20and%20shows%20the%20interest%20of%20its%0Avariance%20as%20an%20echogenicity%20map%20estimator.%20We%20conduct%20experiments%20on%20synthetic%2C%0Ain-vitro%2C%20and%20in-vivo%20data%2C%20demonstrating%20the%20efficacy%20of%20our%20variance%20imaging%0Aapproach%20in%20achieving%20high-quality%20image%20reconstructions%20from%20single%20plane-wave%0Aacquisitions%20and%20in%20comparison%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15316v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultrasound%20Imaging%20based%20on%20the%20Variance%20of%20a%20Diffusion%20Restoration%0A%20%20Model&entry.906535625=Yuxin%20Zhang%20and%20Cl%C3%A9ment%20Huneau%20and%20J%C3%A9r%C3%B4me%20Idier%20and%20Diana%20Mateus&entry.1292438233=%20%20Despite%20today%27s%20prevalence%20of%20ultrasound%20imaging%20in%20medicine%2C%20ultrasound%0Asignal-to-noise%20ratio%20is%20still%20affected%20by%20several%20sources%20of%20noise%20and%0Aartefacts.%20Moreover%2C%20enhancing%20ultrasound%20image%20quality%20involves%20balancing%0Aconcurrent%20factors%20like%20contrast%2C%20resolution%2C%20and%20speckle%20preservation.%0ARecently%2C%20there%20has%20been%20progress%20in%20both%20model-based%20and%20learning-based%0Aapproaches%20addressing%20the%20problem%20of%20ultrasound%20image%20reconstruction.%20Bringing%0Athe%20best%20from%20both%20worlds%2C%20we%20propose%20a%20hybrid%20reconstruction%20method%20combining%0Aan%20ultrasound%20linear%20direct%20model%20with%20a%20learning-based%20prior%20coming%20from%20a%0Agenerative%20Denoising%20Diffusion%20model.%20More%20specifically%2C%20we%20rely%20on%20the%0Aunsupervised%20fine-tuning%20of%20a%20pre-trained%20Denoising%20Diffusion%20Restoration%20Model%0A%28DDRM%29.%20Given%20the%20nature%20of%20multiplicative%20noise%20inherent%20to%20ultrasound%2C%20this%0Apaper%20proposes%20an%20empirical%20model%20to%20characterize%20the%20stochasticity%20of%0Adiffusion%20reconstruction%20of%20ultrasound%20images%2C%20and%20shows%20the%20interest%20of%20its%0Avariance%20as%20an%20echogenicity%20map%20estimator.%20We%20conduct%20experiments%20on%20synthetic%2C%0Ain-vitro%2C%20and%20in-vivo%20data%2C%20demonstrating%20the%20efficacy%20of%20our%20variance%20imaging%0Aapproach%20in%20achieving%20high-quality%20image%20reconstructions%20from%20single%20plane-wave%0Aacquisitions%20and%20in%20comparison%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15316v1&entry.124074799=Read"},
{"title": "DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from\n  Partially Annotated Data", "author": "Hanrong Ye and Dan Xu", "abstract": "  Recently, there has been an increased interest in the practical problem of\nlearning multiple dense scene understanding tasks from partially annotated\ndata, where each training sample is only labeled for a subset of the tasks. The\nmissing of task labels in training leads to low-quality and noisy predictions,\nas can be observed from state-of-the-art methods. To tackle this issue, we\nreformulate the partially-labeled multi-task dense prediction as a pixel-level\ndenoising problem, and propose a novel multi-task denoising diffusion framework\ncoined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to\nmodel a potential noisy distribution in the task prediction or feature maps and\ngenerate rectified outputs for different tasks. To exploit multi-task\nconsistency in denoising, we further introduce a Multi-Task Conditioning\nstrategy, which can implicitly utilize the complementary nature of the tasks to\nhelp learn the unlabeled tasks, leading to an improvement in the denoising\nperformance of the different tasks. Extensive quantitative and qualitative\nexperiments demonstrate that the proposed multi-task denoising diffusion model\ncan significantly improve multi-task prediction maps, and outperform the\nstate-of-the-art methods on three challenging multi-task benchmarks, under two\ndifferent partial-labeling evaluation settings. The code is available at\nhttps://prismformore.github.io/diffusionmtl/.\n", "link": "http://arxiv.org/abs/2403.15389v1", "date": "2024-03-22", "relevancy": 2.2435, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5573}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5571}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DiffusionMTL%3A%20Learning%20Multi-Task%20Denoising%20Diffusion%20Model%20from%0A%20%20Partially%20Annotated%20Data&body=Title%3A%20DiffusionMTL%3A%20Learning%20Multi-Task%20Denoising%20Diffusion%20Model%20from%0A%20%20Partially%20Annotated%20Data%0AAuthor%3A%20Hanrong%20Ye%20and%20Dan%20Xu%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20an%20increased%20interest%20in%20the%20practical%20problem%20of%0Alearning%20multiple%20dense%20scene%20understanding%20tasks%20from%20partially%20annotated%0Adata%2C%20where%20each%20training%20sample%20is%20only%20labeled%20for%20a%20subset%20of%20the%20tasks.%20The%0Amissing%20of%20task%20labels%20in%20training%20leads%20to%20low-quality%20and%20noisy%20predictions%2C%0Aas%20can%20be%20observed%20from%20state-of-the-art%20methods.%20To%20tackle%20this%20issue%2C%20we%0Areformulate%20the%20partially-labeled%20multi-task%20dense%20prediction%20as%20a%20pixel-level%0Adenoising%20problem%2C%20and%20propose%20a%20novel%20multi-task%20denoising%20diffusion%20framework%0Acoined%20as%20DiffusionMTL.%20It%20designs%20a%20joint%20diffusion%20and%20denoising%20paradigm%20to%0Amodel%20a%20potential%20noisy%20distribution%20in%20the%20task%20prediction%20or%20feature%20maps%20and%0Agenerate%20rectified%20outputs%20for%20different%20tasks.%20To%20exploit%20multi-task%0Aconsistency%20in%20denoising%2C%20we%20further%20introduce%20a%20Multi-Task%20Conditioning%0Astrategy%2C%20which%20can%20implicitly%20utilize%20the%20complementary%20nature%20of%20the%20tasks%20to%0Ahelp%20learn%20the%20unlabeled%20tasks%2C%20leading%20to%20an%20improvement%20in%20the%20denoising%0Aperformance%20of%20the%20different%20tasks.%20Extensive%20quantitative%20and%20qualitative%0Aexperiments%20demonstrate%20that%20the%20proposed%20multi-task%20denoising%20diffusion%20model%0Acan%20significantly%20improve%20multi-task%20prediction%20maps%2C%20and%20outperform%20the%0Astate-of-the-art%20methods%20on%20three%20challenging%20multi-task%20benchmarks%2C%20under%20two%0Adifferent%20partial-labeling%20evaluation%20settings.%20The%20code%20is%20available%20at%0Ahttps%3A//prismformore.github.io/diffusionmtl/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15389v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffusionMTL%3A%20Learning%20Multi-Task%20Denoising%20Diffusion%20Model%20from%0A%20%20Partially%20Annotated%20Data&entry.906535625=Hanrong%20Ye%20and%20Dan%20Xu&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20an%20increased%20interest%20in%20the%20practical%20problem%20of%0Alearning%20multiple%20dense%20scene%20understanding%20tasks%20from%20partially%20annotated%0Adata%2C%20where%20each%20training%20sample%20is%20only%20labeled%20for%20a%20subset%20of%20the%20tasks.%20The%0Amissing%20of%20task%20labels%20in%20training%20leads%20to%20low-quality%20and%20noisy%20predictions%2C%0Aas%20can%20be%20observed%20from%20state-of-the-art%20methods.%20To%20tackle%20this%20issue%2C%20we%0Areformulate%20the%20partially-labeled%20multi-task%20dense%20prediction%20as%20a%20pixel-level%0Adenoising%20problem%2C%20and%20propose%20a%20novel%20multi-task%20denoising%20diffusion%20framework%0Acoined%20as%20DiffusionMTL.%20It%20designs%20a%20joint%20diffusion%20and%20denoising%20paradigm%20to%0Amodel%20a%20potential%20noisy%20distribution%20in%20the%20task%20prediction%20or%20feature%20maps%20and%0Agenerate%20rectified%20outputs%20for%20different%20tasks.%20To%20exploit%20multi-task%0Aconsistency%20in%20denoising%2C%20we%20further%20introduce%20a%20Multi-Task%20Conditioning%0Astrategy%2C%20which%20can%20implicitly%20utilize%20the%20complementary%20nature%20of%20the%20tasks%20to%0Ahelp%20learn%20the%20unlabeled%20tasks%2C%20leading%20to%20an%20improvement%20in%20the%20denoising%0Aperformance%20of%20the%20different%20tasks.%20Extensive%20quantitative%20and%20qualitative%0Aexperiments%20demonstrate%20that%20the%20proposed%20multi-task%20denoising%20diffusion%20model%0Acan%20significantly%20improve%20multi-task%20prediction%20maps%2C%20and%20outperform%20the%0Astate-of-the-art%20methods%20on%20three%20challenging%20multi-task%20benchmarks%2C%20under%20two%0Adifferent%20partial-labeling%20evaluation%20settings.%20The%20code%20is%20available%20at%0Ahttps%3A//prismformore.github.io/diffusionmtl/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15389v1&entry.124074799=Read"},
{"title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis", "author": "Kevin Xie and Jonathan Lorraine and Tianshi Cao and Jun Gao and James Lucas and Antonio Torralba and Sanja Fidler and Xiaohui Zeng", "abstract": "  Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.\n", "link": "http://arxiv.org/abs/2403.15385v1", "date": "2024-03-22", "relevancy": 2.2397, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5758}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5557}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5457}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LATTE3D%3A%20Large-scale%20Amortized%20Text-To-Enhanced3D%20Synthesis&body=Title%3A%20LATTE3D%3A%20Large-scale%20Amortized%20Text-To-Enhanced3D%20Synthesis%0AAuthor%3A%20Kevin%20Xie%20and%20Jonathan%20Lorraine%20and%20Tianshi%20Cao%20and%20Jun%20Gao%20and%20James%20Lucas%20and%20Antonio%20Torralba%20and%20Sanja%20Fidler%20and%20Xiaohui%20Zeng%0AAbstract%3A%20%20%20Recent%20text-to-3D%20generation%20approaches%20produce%20impressive%203D%20results%20but%0Arequire%20time-consuming%20optimization%20that%20can%20take%20up%20to%20an%20hour%20per%20prompt.%0AAmortized%20methods%20like%20ATT3D%20optimize%20multiple%20prompts%20simultaneously%20to%0Aimprove%20efficiency%2C%20enabling%20fast%20text-to-3D%20synthesis.%20However%2C%20they%20cannot%0Acapture%20high-frequency%20geometry%20and%20texture%20details%20and%20struggle%20to%20scale%20to%0Alarge%20prompt%20sets%2C%20so%20they%20generalize%20poorly.%20We%20introduce%20LATTE3D%2C%20addressing%0Athese%20limitations%20to%20achieve%20fast%2C%20high-quality%20generation%20on%20a%20significantly%0Alarger%20prompt%20set.%20Key%20to%20our%20method%20is%201%29%20building%20a%20scalable%20architecture%20and%0A2%29%20leveraging%203D%20data%20during%20optimization%20through%203D-aware%20diffusion%20priors%2C%0Ashape%20regularization%2C%20and%20model%20initialization%20to%20achieve%20robustness%20to%20diverse%0Aand%20complex%20training%20prompts.%20LATTE3D%20amortizes%20both%20neural%20field%20and%20textured%0Asurface%20generation%20to%20produce%20highly%20detailed%20textured%20meshes%20in%20a%20single%0Aforward%20pass.%20LATTE3D%20generates%203D%20objects%20in%20400ms%2C%20and%20can%20be%20further%0Aenhanced%20with%20fast%20test-time%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15385v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LATTE3D%3A%20Large-scale%20Amortized%20Text-To-Enhanced3D%20Synthesis&entry.906535625=Kevin%20Xie%20and%20Jonathan%20Lorraine%20and%20Tianshi%20Cao%20and%20Jun%20Gao%20and%20James%20Lucas%20and%20Antonio%20Torralba%20and%20Sanja%20Fidler%20and%20Xiaohui%20Zeng&entry.1292438233=%20%20Recent%20text-to-3D%20generation%20approaches%20produce%20impressive%203D%20results%20but%0Arequire%20time-consuming%20optimization%20that%20can%20take%20up%20to%20an%20hour%20per%20prompt.%0AAmortized%20methods%20like%20ATT3D%20optimize%20multiple%20prompts%20simultaneously%20to%0Aimprove%20efficiency%2C%20enabling%20fast%20text-to-3D%20synthesis.%20However%2C%20they%20cannot%0Acapture%20high-frequency%20geometry%20and%20texture%20details%20and%20struggle%20to%20scale%20to%0Alarge%20prompt%20sets%2C%20so%20they%20generalize%20poorly.%20We%20introduce%20LATTE3D%2C%20addressing%0Athese%20limitations%20to%20achieve%20fast%2C%20high-quality%20generation%20on%20a%20significantly%0Alarger%20prompt%20set.%20Key%20to%20our%20method%20is%201%29%20building%20a%20scalable%20architecture%20and%0A2%29%20leveraging%203D%20data%20during%20optimization%20through%203D-aware%20diffusion%20priors%2C%0Ashape%20regularization%2C%20and%20model%20initialization%20to%20achieve%20robustness%20to%20diverse%0Aand%20complex%20training%20prompts.%20LATTE3D%20amortizes%20both%20neural%20field%20and%20textured%0Asurface%20generation%20to%20produce%20highly%20detailed%20textured%20meshes%20in%20a%20single%0Aforward%20pass.%20LATTE3D%20generates%203D%20objects%20in%20400ms%2C%20and%20can%20be%20further%0Aenhanced%20with%20fast%20test-time%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15385v1&entry.124074799=Read"},
{"title": "Selectively Informative Description can Reduce Undesired Embedding\n  Entanglements in Text-to-Image Personalization", "author": "Jimyeong Kim and Jungwon Park and Wonjong Rhee", "abstract": "  In text-to-image personalization, a timely and crucial challenge is the\ntendency of generated images overfitting to the biases present in the reference\nimages. We initiate our study with a comprehensive categorization of the biases\ninto background, nearby-object, tied-object, substance (in style\nre-contextualization), and pose biases. These biases manifest in the generated\nimages due to their entanglement into the subject embedding. This undesired\nembedding entanglement not only results in the reflection of biases from the\nreference images into the generated images but also notably diminishes the\nalignment of the generated images with the given generation prompt. To address\nthis challenge, we propose SID~(Selectively Informative Description), a text\ndescription strategy that deviates from the prevalent approach of only\ncharacterizing the subject's class identification. SID is generated utilizing\nmultimodal GPT-4 and can be seamlessly integrated into optimization-based\nmodels. We present comprehensive experimental results along with analyses of\ncross-attention maps, subject-alignment, non-subject-disentanglement, and\ntext-alignment.\n", "link": "http://arxiv.org/abs/2403.15330v1", "date": "2024-03-22", "relevancy": 2.2356, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5857}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5594}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5319}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Selectively%20Informative%20Description%20can%20Reduce%20Undesired%20Embedding%0A%20%20Entanglements%20in%20Text-to-Image%20Personalization&body=Title%3A%20Selectively%20Informative%20Description%20can%20Reduce%20Undesired%20Embedding%0A%20%20Entanglements%20in%20Text-to-Image%20Personalization%0AAuthor%3A%20Jimyeong%20Kim%20and%20Jungwon%20Park%20and%20Wonjong%20Rhee%0AAbstract%3A%20%20%20In%20text-to-image%20personalization%2C%20a%20timely%20and%20crucial%20challenge%20is%20the%0Atendency%20of%20generated%20images%20overfitting%20to%20the%20biases%20present%20in%20the%20reference%0Aimages.%20We%20initiate%20our%20study%20with%20a%20comprehensive%20categorization%20of%20the%20biases%0Ainto%20background%2C%20nearby-object%2C%20tied-object%2C%20substance%20%28in%20style%0Are-contextualization%29%2C%20and%20pose%20biases.%20These%20biases%20manifest%20in%20the%20generated%0Aimages%20due%20to%20their%20entanglement%20into%20the%20subject%20embedding.%20This%20undesired%0Aembedding%20entanglement%20not%20only%20results%20in%20the%20reflection%20of%20biases%20from%20the%0Areference%20images%20into%20the%20generated%20images%20but%20also%20notably%20diminishes%20the%0Aalignment%20of%20the%20generated%20images%20with%20the%20given%20generation%20prompt.%20To%20address%0Athis%20challenge%2C%20we%20propose%20SID~%28Selectively%20Informative%20Description%29%2C%20a%20text%0Adescription%20strategy%20that%20deviates%20from%20the%20prevalent%20approach%20of%20only%0Acharacterizing%20the%20subject%27s%20class%20identification.%20SID%20is%20generated%20utilizing%0Amultimodal%20GPT-4%20and%20can%20be%20seamlessly%20integrated%20into%20optimization-based%0Amodels.%20We%20present%20comprehensive%20experimental%20results%20along%20with%20analyses%20of%0Across-attention%20maps%2C%20subject-alignment%2C%20non-subject-disentanglement%2C%20and%0Atext-alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15330v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selectively%20Informative%20Description%20can%20Reduce%20Undesired%20Embedding%0A%20%20Entanglements%20in%20Text-to-Image%20Personalization&entry.906535625=Jimyeong%20Kim%20and%20Jungwon%20Park%20and%20Wonjong%20Rhee&entry.1292438233=%20%20In%20text-to-image%20personalization%2C%20a%20timely%20and%20crucial%20challenge%20is%20the%0Atendency%20of%20generated%20images%20overfitting%20to%20the%20biases%20present%20in%20the%20reference%0Aimages.%20We%20initiate%20our%20study%20with%20a%20comprehensive%20categorization%20of%20the%20biases%0Ainto%20background%2C%20nearby-object%2C%20tied-object%2C%20substance%20%28in%20style%0Are-contextualization%29%2C%20and%20pose%20biases.%20These%20biases%20manifest%20in%20the%20generated%0Aimages%20due%20to%20their%20entanglement%20into%20the%20subject%20embedding.%20This%20undesired%0Aembedding%20entanglement%20not%20only%20results%20in%20the%20reflection%20of%20biases%20from%20the%0Areference%20images%20into%20the%20generated%20images%20but%20also%20notably%20diminishes%20the%0Aalignment%20of%20the%20generated%20images%20with%20the%20given%20generation%20prompt.%20To%20address%0Athis%20challenge%2C%20we%20propose%20SID~%28Selectively%20Informative%20Description%29%2C%20a%20text%0Adescription%20strategy%20that%20deviates%20from%20the%20prevalent%20approach%20of%20only%0Acharacterizing%20the%20subject%27s%20class%20identification.%20SID%20is%20generated%20utilizing%0Amultimodal%20GPT-4%20and%20can%20be%20seamlessly%20integrated%20into%20optimization-based%0Amodels.%20We%20present%20comprehensive%20experimental%20results%20along%20with%20analyses%20of%0Across-attention%20maps%2C%20subject-alignment%2C%20non-subject-disentanglement%2C%20and%0Atext-alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15330v1&entry.124074799=Read"},
{"title": "Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting", "author": "Vladimir Yugay and Yue Li and Theo Gevers and Martin R. Oswald", "abstract": "  We present a dense simultaneous localization and mapping (SLAM) method that\nuses 3D Gaussians as a scene representation. Our approach enables\ninteractive-time reconstruction and photo-realistic rendering from real-world\nsingle-camera RGBD videos. To this end, we propose a novel effective strategy\nfor seeding new Gaussians for newly explored areas and their effective online\noptimization that is independent of the scene size and thus scalable to larger\nscenes. This is achieved by organizing the scene into sub-maps which are\nindependently optimized and do not need to be kept in memory. We further\naccomplish frame-to-model camera tracking by minimizing photometric and\ngeometric losses between the input and rendered frames. The Gaussian\nrepresentation allows for high-quality photo-realistic real-time rendering of\nreal-world scenes. Evaluation on synthetic and real-world datasets demonstrates\ncompetitive or superior performance in mapping, tracking, and rendering\ncompared to existing neural dense SLAM methods.\n", "link": "http://arxiv.org/abs/2312.10070v2", "date": "2024-03-22", "relevancy": 2.2347, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5926}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5411}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gaussian-SLAM%3A%20Photo-realistic%20Dense%20SLAM%20with%20Gaussian%20Splatting&body=Title%3A%20Gaussian-SLAM%3A%20Photo-realistic%20Dense%20SLAM%20with%20Gaussian%20Splatting%0AAuthor%3A%20Vladimir%20Yugay%20and%20Yue%20Li%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20We%20present%20a%20dense%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20method%20that%0Auses%203D%20Gaussians%20as%20a%20scene%20representation.%20Our%20approach%20enables%0Ainteractive-time%20reconstruction%20and%20photo-realistic%20rendering%20from%20real-world%0Asingle-camera%20RGBD%20videos.%20To%20this%20end%2C%20we%20propose%20a%20novel%20effective%20strategy%0Afor%20seeding%20new%20Gaussians%20for%20newly%20explored%20areas%20and%20their%20effective%20online%0Aoptimization%20that%20is%20independent%20of%20the%20scene%20size%20and%20thus%20scalable%20to%20larger%0Ascenes.%20This%20is%20achieved%20by%20organizing%20the%20scene%20into%20sub-maps%20which%20are%0Aindependently%20optimized%20and%20do%20not%20need%20to%20be%20kept%20in%20memory.%20We%20further%0Aaccomplish%20frame-to-model%20camera%20tracking%20by%20minimizing%20photometric%20and%0Ageometric%20losses%20between%20the%20input%20and%20rendered%20frames.%20The%20Gaussian%0Arepresentation%20allows%20for%20high-quality%20photo-realistic%20real-time%20rendering%20of%0Areal-world%20scenes.%20Evaluation%20on%20synthetic%20and%20real-world%20datasets%20demonstrates%0Acompetitive%20or%20superior%20performance%20in%20mapping%2C%20tracking%2C%20and%20rendering%0Acompared%20to%20existing%20neural%20dense%20SLAM%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10070v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian-SLAM%3A%20Photo-realistic%20Dense%20SLAM%20with%20Gaussian%20Splatting&entry.906535625=Vladimir%20Yugay%20and%20Yue%20Li%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20We%20present%20a%20dense%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20method%20that%0Auses%203D%20Gaussians%20as%20a%20scene%20representation.%20Our%20approach%20enables%0Ainteractive-time%20reconstruction%20and%20photo-realistic%20rendering%20from%20real-world%0Asingle-camera%20RGBD%20videos.%20To%20this%20end%2C%20we%20propose%20a%20novel%20effective%20strategy%0Afor%20seeding%20new%20Gaussians%20for%20newly%20explored%20areas%20and%20their%20effective%20online%0Aoptimization%20that%20is%20independent%20of%20the%20scene%20size%20and%20thus%20scalable%20to%20larger%0Ascenes.%20This%20is%20achieved%20by%20organizing%20the%20scene%20into%20sub-maps%20which%20are%0Aindependently%20optimized%20and%20do%20not%20need%20to%20be%20kept%20in%20memory.%20We%20further%0Aaccomplish%20frame-to-model%20camera%20tracking%20by%20minimizing%20photometric%20and%0Ageometric%20losses%20between%20the%20input%20and%20rendered%20frames.%20The%20Gaussian%0Arepresentation%20allows%20for%20high-quality%20photo-realistic%20real-time%20rendering%20of%0Areal-world%20scenes.%20Evaluation%20on%20synthetic%20and%20real-world%20datasets%20demonstrates%0Acompetitive%20or%20superior%20performance%20in%20mapping%2C%20tracking%2C%20and%20rendering%0Acompared%20to%20existing%20neural%20dense%20SLAM%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10070v2&entry.124074799=Read"},
{"title": "Transfer CLIP for Generalizable Image Denoising", "author": "Jun Cheng and Dong Liang and Shan Tan", "abstract": "  Image denoising is a fundamental task in computer vision. While prevailing\ndeep learning-based supervised and self-supervised methods have excelled in\neliminating in-distribution noise, their susceptibility to out-of-distribution\n(OOD) noise remains a significant challenge. The recent emergence of\ncontrastive language-image pre-training (CLIP) model has showcased exceptional\ncapabilities in open-world image recognition and segmentation. Yet, the\npotential for leveraging CLIP to enhance the robustness of low-level tasks\nremains largely unexplored. This paper uncovers that certain dense features\nextracted from the frozen ResNet image encoder of CLIP exhibit\ndistortion-invariant and content-related properties, which are highly desirable\nfor generalizable denoising. Leveraging these properties, we devise an\nasymmetrical encoder-decoder denoising network, which incorporates dense\nfeatures including the noisy image and its multi-scale features from the frozen\nResNet encoder of CLIP into a learnable image decoder to achieve generalizable\ndenoising. The progressive feature augmentation strategy is further proposed to\nmitigate feature overfitting and improve the robustness of the learnable\ndecoder. Extensive experiments and comparisons conducted across diverse OOD\nnoises, including synthetic noise, real-world sRGB noise, and low-dose CT image\nnoise, demonstrate the superior generalization ability of our method.\n", "link": "http://arxiv.org/abs/2403.15132v1", "date": "2024-03-22", "relevancy": 2.2266, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5818}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5438}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5366}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Transfer%20CLIP%20for%20Generalizable%20Image%20Denoising&body=Title%3A%20Transfer%20CLIP%20for%20Generalizable%20Image%20Denoising%0AAuthor%3A%20Jun%20Cheng%20and%20Dong%20Liang%20and%20Shan%20Tan%0AAbstract%3A%20%20%20Image%20denoising%20is%20a%20fundamental%20task%20in%20computer%20vision.%20While%20prevailing%0Adeep%20learning-based%20supervised%20and%20self-supervised%20methods%20have%20excelled%20in%0Aeliminating%20in-distribution%20noise%2C%20their%20susceptibility%20to%20out-of-distribution%0A%28OOD%29%20noise%20remains%20a%20significant%20challenge.%20The%20recent%20emergence%20of%0Acontrastive%20language-image%20pre-training%20%28CLIP%29%20model%20has%20showcased%20exceptional%0Acapabilities%20in%20open-world%20image%20recognition%20and%20segmentation.%20Yet%2C%20the%0Apotential%20for%20leveraging%20CLIP%20to%20enhance%20the%20robustness%20of%20low-level%20tasks%0Aremains%20largely%20unexplored.%20This%20paper%20uncovers%20that%20certain%20dense%20features%0Aextracted%20from%20the%20frozen%20ResNet%20image%20encoder%20of%20CLIP%20exhibit%0Adistortion-invariant%20and%20content-related%20properties%2C%20which%20are%20highly%20desirable%0Afor%20generalizable%20denoising.%20Leveraging%20these%20properties%2C%20we%20devise%20an%0Aasymmetrical%20encoder-decoder%20denoising%20network%2C%20which%20incorporates%20dense%0Afeatures%20including%20the%20noisy%20image%20and%20its%20multi-scale%20features%20from%20the%20frozen%0AResNet%20encoder%20of%20CLIP%20into%20a%20learnable%20image%20decoder%20to%20achieve%20generalizable%0Adenoising.%20The%20progressive%20feature%20augmentation%20strategy%20is%20further%20proposed%20to%0Amitigate%20feature%20overfitting%20and%20improve%20the%20robustness%20of%20the%20learnable%0Adecoder.%20Extensive%20experiments%20and%20comparisons%20conducted%20across%20diverse%20OOD%0Anoises%2C%20including%20synthetic%20noise%2C%20real-world%20sRGB%20noise%2C%20and%20low-dose%20CT%20image%0Anoise%2C%20demonstrate%20the%20superior%20generalization%20ability%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15132v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20CLIP%20for%20Generalizable%20Image%20Denoising&entry.906535625=Jun%20Cheng%20and%20Dong%20Liang%20and%20Shan%20Tan&entry.1292438233=%20%20Image%20denoising%20is%20a%20fundamental%20task%20in%20computer%20vision.%20While%20prevailing%0Adeep%20learning-based%20supervised%20and%20self-supervised%20methods%20have%20excelled%20in%0Aeliminating%20in-distribution%20noise%2C%20their%20susceptibility%20to%20out-of-distribution%0A%28OOD%29%20noise%20remains%20a%20significant%20challenge.%20The%20recent%20emergence%20of%0Acontrastive%20language-image%20pre-training%20%28CLIP%29%20model%20has%20showcased%20exceptional%0Acapabilities%20in%20open-world%20image%20recognition%20and%20segmentation.%20Yet%2C%20the%0Apotential%20for%20leveraging%20CLIP%20to%20enhance%20the%20robustness%20of%20low-level%20tasks%0Aremains%20largely%20unexplored.%20This%20paper%20uncovers%20that%20certain%20dense%20features%0Aextracted%20from%20the%20frozen%20ResNet%20image%20encoder%20of%20CLIP%20exhibit%0Adistortion-invariant%20and%20content-related%20properties%2C%20which%20are%20highly%20desirable%0Afor%20generalizable%20denoising.%20Leveraging%20these%20properties%2C%20we%20devise%20an%0Aasymmetrical%20encoder-decoder%20denoising%20network%2C%20which%20incorporates%20dense%0Afeatures%20including%20the%20noisy%20image%20and%20its%20multi-scale%20features%20from%20the%20frozen%0AResNet%20encoder%20of%20CLIP%20into%20a%20learnable%20image%20decoder%20to%20achieve%20generalizable%0Adenoising.%20The%20progressive%20feature%20augmentation%20strategy%20is%20further%20proposed%20to%0Amitigate%20feature%20overfitting%20and%20improve%20the%20robustness%20of%20the%20learnable%0Adecoder.%20Extensive%20experiments%20and%20comparisons%20conducted%20across%20diverse%20OOD%0Anoises%2C%20including%20synthetic%20noise%2C%20real-world%20sRGB%20noise%2C%20and%20low-dose%20CT%20image%0Anoise%2C%20demonstrate%20the%20superior%20generalization%20ability%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15132v1&entry.124074799=Read"},
{"title": "DragAPart: Learning a Part-Level Motion Prior for Articulated Objects", "author": "Ruining Li and Chuanxia Zheng and Christian Rupprecht and Andrea Vedaldi", "abstract": "  We introduce DragAPart, a method that, given an image and a set of drags as\ninput, can generate a new image of the same object in a new state, compatible\nwith the action of the drags. Differently from prior works that focused on\nrepositioning objects, DragAPart predicts part-level interactions, such as\nopening and closing a drawer. We study this problem as a proxy for learning a\ngeneralist motion model, not restricted to a specific kinematic structure or\nobject category. To this end, we start from a pre-trained image generator and\nfine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.\nCombined with a new encoding for the drags and dataset randomization, the new\nmodel generalizes well to real images and different categories. Compared to\nprior motion-controlled generators, we demonstrate much better part-level\nmotion understanding.\n", "link": "http://arxiv.org/abs/2403.15382v1", "date": "2024-03-22", "relevancy": 2.2238, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.58}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DragAPart%3A%20Learning%20a%20Part-Level%20Motion%20Prior%20for%20Articulated%20Objects&body=Title%3A%20DragAPart%3A%20Learning%20a%20Part-Level%20Motion%20Prior%20for%20Articulated%20Objects%0AAuthor%3A%20Ruining%20Li%20and%20Chuanxia%20Zheng%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20We%20introduce%20DragAPart%2C%20a%20method%20that%2C%20given%20an%20image%20and%20a%20set%20of%20drags%20as%0Ainput%2C%20can%20generate%20a%20new%20image%20of%20the%20same%20object%20in%20a%20new%20state%2C%20compatible%0Awith%20the%20action%20of%20the%20drags.%20Differently%20from%20prior%20works%20that%20focused%20on%0Arepositioning%20objects%2C%20DragAPart%20predicts%20part-level%20interactions%2C%20such%20as%0Aopening%20and%20closing%20a%20drawer.%20We%20study%20this%20problem%20as%20a%20proxy%20for%20learning%20a%0Ageneralist%20motion%20model%2C%20not%20restricted%20to%20a%20specific%20kinematic%20structure%20or%0Aobject%20category.%20To%20this%20end%2C%20we%20start%20from%20a%20pre-trained%20image%20generator%20and%0Afine-tune%20it%20on%20a%20new%20synthetic%20dataset%2C%20Drag-a-Move%2C%20which%20we%20introduce.%0ACombined%20with%20a%20new%20encoding%20for%20the%20drags%20and%20dataset%20randomization%2C%20the%20new%0Amodel%20generalizes%20well%20to%20real%20images%20and%20different%20categories.%20Compared%20to%0Aprior%20motion-controlled%20generators%2C%20we%20demonstrate%20much%20better%20part-level%0Amotion%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15382v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DragAPart%3A%20Learning%20a%20Part-Level%20Motion%20Prior%20for%20Articulated%20Objects&entry.906535625=Ruining%20Li%20and%20Chuanxia%20Zheng%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20We%20introduce%20DragAPart%2C%20a%20method%20that%2C%20given%20an%20image%20and%20a%20set%20of%20drags%20as%0Ainput%2C%20can%20generate%20a%20new%20image%20of%20the%20same%20object%20in%20a%20new%20state%2C%20compatible%0Awith%20the%20action%20of%20the%20drags.%20Differently%20from%20prior%20works%20that%20focused%20on%0Arepositioning%20objects%2C%20DragAPart%20predicts%20part-level%20interactions%2C%20such%20as%0Aopening%20and%20closing%20a%20drawer.%20We%20study%20this%20problem%20as%20a%20proxy%20for%20learning%20a%0Ageneralist%20motion%20model%2C%20not%20restricted%20to%20a%20specific%20kinematic%20structure%20or%0Aobject%20category.%20To%20this%20end%2C%20we%20start%20from%20a%20pre-trained%20image%20generator%20and%0Afine-tune%20it%20on%20a%20new%20synthetic%20dataset%2C%20Drag-a-Move%2C%20which%20we%20introduce.%0ACombined%20with%20a%20new%20encoding%20for%20the%20drags%20and%20dataset%20randomization%2C%20the%20new%0Amodel%20generalizes%20well%20to%20real%20images%20and%20different%20categories.%20Compared%20to%0Aprior%20motion-controlled%20generators%2C%20we%20demonstrate%20much%20better%20part-level%0Amotion%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15382v1&entry.124074799=Read"},
{"title": "LSK3DNet: Towards Effective and Efficient 3D Perception with Large\n  Sparse Kernels", "author": "Tuo Feng and Wenguan Wang and Fan Ma and Yi Yang", "abstract": "  Autonomous systems need to process large-scale, sparse, and irregular point\nclouds with limited compute resources. Consequently, it is essential to develop\nLiDAR perception methods that are both efficient and effective. Although\nnaively enlarging 3D kernel size can enhance performance, it will also lead to\na cubically-increasing overhead. Therefore, it is crucial to develop\nstreamlined 3D large kernel designs that eliminate redundant weights and work\neffectively with larger kernels. In this paper, we propose an efficient and\neffective Large Sparse Kernel 3D Neural Network (LSK3DNet) that leverages\ndynamic pruning to amplify the 3D kernel size. Our method comprises two core\ncomponents: Spatial-wise Dynamic Sparsity (SDS) and Channel-wise Weight\nSelection (CWS). SDS dynamically prunes and regrows volumetric weights from the\nbeginning to learn a large sparse 3D kernel. It not only boosts performance but\nalso significantly reduces model size and computational cost. Moreover, CWS\nselects the most important channels for 3D convolution during training and\nsubsequently prunes the redundant channels to accelerate inference for 3D\nvision tasks. We demonstrate the effectiveness of LSK3DNet on three benchmark\ndatasets and five tracks compared with classical models and large kernel\ndesigns. Notably, LSK3DNet achieves the state-of-the-art performance on\nSemanticKITTI (i.e., 75.6% on single-scan and 63.4% on multi-scan), with\nroughly 40% model size reduction and 60% computing operations reduction\ncompared to the naive large 3D kernel model.\n", "link": "http://arxiv.org/abs/2403.15173v1", "date": "2024-03-22", "relevancy": 2.2158, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5346}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5323}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LSK3DNet%3A%20Towards%20Effective%20and%20Efficient%203D%20Perception%20with%20Large%0A%20%20Sparse%20Kernels&body=Title%3A%20LSK3DNet%3A%20Towards%20Effective%20and%20Efficient%203D%20Perception%20with%20Large%0A%20%20Sparse%20Kernels%0AAuthor%3A%20Tuo%20Feng%20and%20Wenguan%20Wang%20and%20Fan%20Ma%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Autonomous%20systems%20need%20to%20process%20large-scale%2C%20sparse%2C%20and%20irregular%20point%0Aclouds%20with%20limited%20compute%20resources.%20Consequently%2C%20it%20is%20essential%20to%20develop%0ALiDAR%20perception%20methods%20that%20are%20both%20efficient%20and%20effective.%20Although%0Anaively%20enlarging%203D%20kernel%20size%20can%20enhance%20performance%2C%20it%20will%20also%20lead%20to%0Aa%20cubically-increasing%20overhead.%20Therefore%2C%20it%20is%20crucial%20to%20develop%0Astreamlined%203D%20large%20kernel%20designs%20that%20eliminate%20redundant%20weights%20and%20work%0Aeffectively%20with%20larger%20kernels.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20and%0Aeffective%20Large%20Sparse%20Kernel%203D%20Neural%20Network%20%28LSK3DNet%29%20that%20leverages%0Adynamic%20pruning%20to%20amplify%20the%203D%20kernel%20size.%20Our%20method%20comprises%20two%20core%0Acomponents%3A%20Spatial-wise%20Dynamic%20Sparsity%20%28SDS%29%20and%20Channel-wise%20Weight%0ASelection%20%28CWS%29.%20SDS%20dynamically%20prunes%20and%20regrows%20volumetric%20weights%20from%20the%0Abeginning%20to%20learn%20a%20large%20sparse%203D%20kernel.%20It%20not%20only%20boosts%20performance%20but%0Aalso%20significantly%20reduces%20model%20size%20and%20computational%20cost.%20Moreover%2C%20CWS%0Aselects%20the%20most%20important%20channels%20for%203D%20convolution%20during%20training%20and%0Asubsequently%20prunes%20the%20redundant%20channels%20to%20accelerate%20inference%20for%203D%0Avision%20tasks.%20We%20demonstrate%20the%20effectiveness%20of%20LSK3DNet%20on%20three%20benchmark%0Adatasets%20and%20five%20tracks%20compared%20with%20classical%20models%20and%20large%20kernel%0Adesigns.%20Notably%2C%20LSK3DNet%20achieves%20the%20state-of-the-art%20performance%20on%0ASemanticKITTI%20%28i.e.%2C%2075.6%25%20on%20single-scan%20and%2063.4%25%20on%20multi-scan%29%2C%20with%0Aroughly%2040%25%20model%20size%20reduction%20and%2060%25%20computing%20operations%20reduction%0Acompared%20to%20the%20naive%20large%203D%20kernel%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15173v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSK3DNet%3A%20Towards%20Effective%20and%20Efficient%203D%20Perception%20with%20Large%0A%20%20Sparse%20Kernels&entry.906535625=Tuo%20Feng%20and%20Wenguan%20Wang%20and%20Fan%20Ma%20and%20Yi%20Yang&entry.1292438233=%20%20Autonomous%20systems%20need%20to%20process%20large-scale%2C%20sparse%2C%20and%20irregular%20point%0Aclouds%20with%20limited%20compute%20resources.%20Consequently%2C%20it%20is%20essential%20to%20develop%0ALiDAR%20perception%20methods%20that%20are%20both%20efficient%20and%20effective.%20Although%0Anaively%20enlarging%203D%20kernel%20size%20can%20enhance%20performance%2C%20it%20will%20also%20lead%20to%0Aa%20cubically-increasing%20overhead.%20Therefore%2C%20it%20is%20crucial%20to%20develop%0Astreamlined%203D%20large%20kernel%20designs%20that%20eliminate%20redundant%20weights%20and%20work%0Aeffectively%20with%20larger%20kernels.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20and%0Aeffective%20Large%20Sparse%20Kernel%203D%20Neural%20Network%20%28LSK3DNet%29%20that%20leverages%0Adynamic%20pruning%20to%20amplify%20the%203D%20kernel%20size.%20Our%20method%20comprises%20two%20core%0Acomponents%3A%20Spatial-wise%20Dynamic%20Sparsity%20%28SDS%29%20and%20Channel-wise%20Weight%0ASelection%20%28CWS%29.%20SDS%20dynamically%20prunes%20and%20regrows%20volumetric%20weights%20from%20the%0Abeginning%20to%20learn%20a%20large%20sparse%203D%20kernel.%20It%20not%20only%20boosts%20performance%20but%0Aalso%20significantly%20reduces%20model%20size%20and%20computational%20cost.%20Moreover%2C%20CWS%0Aselects%20the%20most%20important%20channels%20for%203D%20convolution%20during%20training%20and%0Asubsequently%20prunes%20the%20redundant%20channels%20to%20accelerate%20inference%20for%203D%0Avision%20tasks.%20We%20demonstrate%20the%20effectiveness%20of%20LSK3DNet%20on%20three%20benchmark%0Adatasets%20and%20five%20tracks%20compared%20with%20classical%20models%20and%20large%20kernel%0Adesigns.%20Notably%2C%20LSK3DNet%20achieves%20the%20state-of-the-art%20performance%20on%0ASemanticKITTI%20%28i.e.%2C%2075.6%25%20on%20single-scan%20and%2063.4%25%20on%20multi-scan%29%2C%20with%0Aroughly%2040%25%20model%20size%20reduction%20and%2060%25%20computing%20operations%20reduction%0Acompared%20to%20the%20naive%20large%203D%20kernel%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15173v1&entry.124074799=Read"},
{"title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models", "author": "Yuzhang Shang and Mu Cai and Bingxin Xu and Yong Jae Lee and Yan Yan", "abstract": "  Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.\n", "link": "http://arxiv.org/abs/2403.15388v1", "date": "2024-03-22", "relevancy": 2.211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.551}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4897}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLaVA-PruMerge%3A%20Adaptive%20Token%20Reduction%20for%20Efficient%20Large%20Multimodal%0A%20%20Models&body=Title%3A%20LLaVA-PruMerge%3A%20Adaptive%20Token%20Reduction%20for%20Efficient%20Large%20Multimodal%0A%20%20Models%0AAuthor%3A%20Yuzhang%20Shang%20and%20Mu%20Cai%20and%20Bingxin%20Xu%20and%20Yong%20Jae%20Lee%20and%20Yan%20Yan%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20significant%20reasoning%20capabilities%0Aby%20connecting%20a%20visual%20encoder%20and%20a%20large%20language%20model.%20LMMs%20typically%20use%20a%0Afixed%20amount%20of%20visual%20tokens%2C%20such%20as%20the%20penultimate%20layer%20features%20in%20the%0ACLIP%20visual%20encoder%2C%20as%20the%20prefix%20content.%20Recent%20LMMs%20incorporate%20more%0Acomplex%20visual%20inputs%2C%20such%20as%20high-resolution%20images%20and%20videos%2C%20which%0Aincrease%20the%20number%20of%20visual%20tokens%20significantly.%20However%2C%20due%20to%20the%20design%0Aof%20the%20Transformer%20architecture%2C%20computational%20costs%20associated%20with%20these%0Amodels%20tend%20to%20increase%20quadratically%20with%20the%20number%20of%20input%20tokens.%20To%0Atackle%20this%20problem%2C%20we%20explore%20a%20token%20reduction%20mechanism%20and%20find%2C%20similar%0Ato%20prior%20work%2C%20that%20many%20visual%20tokens%20are%20spatially%20redundant.%20Based%20on%20this%2C%0Awe%20propose%20PruMerge%2C%20a%20novel%20adaptive%20visual%20token%20reduction%20approach%2C%20which%0Alargely%20reduces%20the%20number%20of%20visual%20tokens%20while%20maintaining%20comparable%20model%0Aperformance.%20We%20first%20select%20the%20unpruned%20visual%20tokens%20based%20on%20their%0Asimilarity%20to%20class%20tokens%20and%20spatial%20tokens.%20We%20then%20cluster%20the%20pruned%0Atokens%20based%20on%20key%20similarity%20and%20merge%20the%20clustered%20tokens%20with%20the%20unpruned%0Atokens%20to%20supplement%20their%20information.%20Empirically%2C%20when%20applied%20to%20LLaVA-1.5%2C%0Aour%20approach%20can%20compress%20the%20visual%20tokens%20by%2014.4%20times%20on%20average%2C%20and%0Aachieve%20comparable%20performance%20across%20diverse%20visual%20question-answering%20and%0Areasoning%20tasks.%20Code%20and%20checkpoints%20are%20at%20https%3A//llava-prumerge.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15388v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-PruMerge%3A%20Adaptive%20Token%20Reduction%20for%20Efficient%20Large%20Multimodal%0A%20%20Models&entry.906535625=Yuzhang%20Shang%20and%20Mu%20Cai%20and%20Bingxin%20Xu%20and%20Yong%20Jae%20Lee%20and%20Yan%20Yan&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20significant%20reasoning%20capabilities%0Aby%20connecting%20a%20visual%20encoder%20and%20a%20large%20language%20model.%20LMMs%20typically%20use%20a%0Afixed%20amount%20of%20visual%20tokens%2C%20such%20as%20the%20penultimate%20layer%20features%20in%20the%0ACLIP%20visual%20encoder%2C%20as%20the%20prefix%20content.%20Recent%20LMMs%20incorporate%20more%0Acomplex%20visual%20inputs%2C%20such%20as%20high-resolution%20images%20and%20videos%2C%20which%0Aincrease%20the%20number%20of%20visual%20tokens%20significantly.%20However%2C%20due%20to%20the%20design%0Aof%20the%20Transformer%20architecture%2C%20computational%20costs%20associated%20with%20these%0Amodels%20tend%20to%20increase%20quadratically%20with%20the%20number%20of%20input%20tokens.%20To%0Atackle%20this%20problem%2C%20we%20explore%20a%20token%20reduction%20mechanism%20and%20find%2C%20similar%0Ato%20prior%20work%2C%20that%20many%20visual%20tokens%20are%20spatially%20redundant.%20Based%20on%20this%2C%0Awe%20propose%20PruMerge%2C%20a%20novel%20adaptive%20visual%20token%20reduction%20approach%2C%20which%0Alargely%20reduces%20the%20number%20of%20visual%20tokens%20while%20maintaining%20comparable%20model%0Aperformance.%20We%20first%20select%20the%20unpruned%20visual%20tokens%20based%20on%20their%0Asimilarity%20to%20class%20tokens%20and%20spatial%20tokens.%20We%20then%20cluster%20the%20pruned%0Atokens%20based%20on%20key%20similarity%20and%20merge%20the%20clustered%20tokens%20with%20the%20unpruned%0Atokens%20to%20supplement%20their%20information.%20Empirically%2C%20when%20applied%20to%20LLaVA-1.5%2C%0Aour%20approach%20can%20compress%20the%20visual%20tokens%20by%2014.4%20times%20on%20average%2C%20and%0Aachieve%20comparable%20performance%20across%20diverse%20visual%20question-answering%20and%0Areasoning%20tasks.%20Code%20and%20checkpoints%20are%20at%20https%3A//llava-prumerge.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15388v1&entry.124074799=Read"},
{"title": "CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking", "author": "Nicolas Baumann and Michael Baumgartner and Edoardo Ghignone and Jonas K\u00fchne and Tobias Fischer and Yung-Hsu Yang and Marc Pollefeys and Michele Magno", "abstract": "  Accurate detection and tracking of surrounding objects is essential to enable\nself-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have\nset the benchmark for high performance, the appeal of camera-only solutions\nlies in their cost-effectiveness. Notably, despite the prevalent use of Radio\nDetection and Ranging (RADAR) sensors in automotive systems, their potential in\n3D detection and tracking has been largely disregarded due to data sparsity and\nmeasurement noise. As a recent development, the combination of RADARs and\ncameras is emerging as a promising solution. This paper presents Camera-RADAR\n3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object\ndetection, and Multi-Object Tracking (MOT). Building upon the foundations of\nthe State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates\nsubstantial improvements in both detection and tracking capabilities, by\nincorporating the spatial and velocity information of the RADAR sensor.\nExperimental results demonstrate an absolute improvement in detection\nperformance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in\nAverage Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when\nleveraging both modalities. CR3DT bridges the gap between high-performance and\ncost-effective perception systems in autonomous driving, by capitalizing on the\nubiquitous presence of RADAR in automotive applications.\n", "link": "http://arxiv.org/abs/2403.15313v1", "date": "2024-03-22", "relevancy": 2.2106, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5532}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5511}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CR3DT%3A%20Camera-RADAR%20Fusion%20for%203D%20Detection%20and%20Tracking&body=Title%3A%20CR3DT%3A%20Camera-RADAR%20Fusion%20for%203D%20Detection%20and%20Tracking%0AAuthor%3A%20Nicolas%20Baumann%20and%20Michael%20Baumgartner%20and%20Edoardo%20Ghignone%20and%20Jonas%20K%C3%BChne%20and%20Tobias%20Fischer%20and%20Yung-Hsu%20Yang%20and%20Marc%20Pollefeys%20and%20Michele%20Magno%0AAbstract%3A%20%20%20Accurate%20detection%20and%20tracking%20of%20surrounding%20objects%20is%20essential%20to%20enable%0Aself-driving%20vehicles.%20While%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20sensors%20have%0Aset%20the%20benchmark%20for%20high%20performance%2C%20the%20appeal%20of%20camera-only%20solutions%0Alies%20in%20their%20cost-effectiveness.%20Notably%2C%20despite%20the%20prevalent%20use%20of%20Radio%0ADetection%20and%20Ranging%20%28RADAR%29%20sensors%20in%20automotive%20systems%2C%20their%20potential%20in%0A3D%20detection%20and%20tracking%20has%20been%20largely%20disregarded%20due%20to%20data%20sparsity%20and%0Ameasurement%20noise.%20As%20a%20recent%20development%2C%20the%20combination%20of%20RADARs%20and%0Acameras%20is%20emerging%20as%20a%20promising%20solution.%20This%20paper%20presents%20Camera-RADAR%0A3D%20Detection%20and%20Tracking%20%28CR3DT%29%2C%20a%20camera-RADAR%20fusion%20model%20for%203D%20object%0Adetection%2C%20and%20Multi-Object%20Tracking%20%28MOT%29.%20Building%20upon%20the%20foundations%20of%0Athe%20State-of-the-Art%20%28SotA%29%20camera-only%20BEVDet%20architecture%2C%20CR3DT%20demonstrates%0Asubstantial%20improvements%20in%20both%20detection%20and%20tracking%20capabilities%2C%20by%0Aincorporating%20the%20spatial%20and%20velocity%20information%20of%20the%20RADAR%20sensor.%0AExperimental%20results%20demonstrate%20an%20absolute%20improvement%20in%20detection%0Aperformance%20of%205.3%25%20in%20mean%20Average%20Precision%20%28mAP%29%20and%20a%2014.9%25%20increase%20in%0AAverage%20Multi-Object%20Tracking%20Accuracy%20%28AMOTA%29%20on%20the%20nuScenes%20dataset%20when%0Aleveraging%20both%20modalities.%20CR3DT%20bridges%20the%20gap%20between%20high-performance%20and%0Acost-effective%20perception%20systems%20in%20autonomous%20driving%2C%20by%20capitalizing%20on%20the%0Aubiquitous%20presence%20of%20RADAR%20in%20automotive%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15313v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CR3DT%3A%20Camera-RADAR%20Fusion%20for%203D%20Detection%20and%20Tracking&entry.906535625=Nicolas%20Baumann%20and%20Michael%20Baumgartner%20and%20Edoardo%20Ghignone%20and%20Jonas%20K%C3%BChne%20and%20Tobias%20Fischer%20and%20Yung-Hsu%20Yang%20and%20Marc%20Pollefeys%20and%20Michele%20Magno&entry.1292438233=%20%20Accurate%20detection%20and%20tracking%20of%20surrounding%20objects%20is%20essential%20to%20enable%0Aself-driving%20vehicles.%20While%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20sensors%20have%0Aset%20the%20benchmark%20for%20high%20performance%2C%20the%20appeal%20of%20camera-only%20solutions%0Alies%20in%20their%20cost-effectiveness.%20Notably%2C%20despite%20the%20prevalent%20use%20of%20Radio%0ADetection%20and%20Ranging%20%28RADAR%29%20sensors%20in%20automotive%20systems%2C%20their%20potential%20in%0A3D%20detection%20and%20tracking%20has%20been%20largely%20disregarded%20due%20to%20data%20sparsity%20and%0Ameasurement%20noise.%20As%20a%20recent%20development%2C%20the%20combination%20of%20RADARs%20and%0Acameras%20is%20emerging%20as%20a%20promising%20solution.%20This%20paper%20presents%20Camera-RADAR%0A3D%20Detection%20and%20Tracking%20%28CR3DT%29%2C%20a%20camera-RADAR%20fusion%20model%20for%203D%20object%0Adetection%2C%20and%20Multi-Object%20Tracking%20%28MOT%29.%20Building%20upon%20the%20foundations%20of%0Athe%20State-of-the-Art%20%28SotA%29%20camera-only%20BEVDet%20architecture%2C%20CR3DT%20demonstrates%0Asubstantial%20improvements%20in%20both%20detection%20and%20tracking%20capabilities%2C%20by%0Aincorporating%20the%20spatial%20and%20velocity%20information%20of%20the%20RADAR%20sensor.%0AExperimental%20results%20demonstrate%20an%20absolute%20improvement%20in%20detection%0Aperformance%20of%205.3%25%20in%20mean%20Average%20Precision%20%28mAP%29%20and%20a%2014.9%25%20increase%20in%0AAverage%20Multi-Object%20Tracking%20Accuracy%20%28AMOTA%29%20on%20the%20nuScenes%20dataset%20when%0Aleveraging%20both%20modalities.%20CR3DT%20bridges%20the%20gap%20between%20high-performance%20and%0Acost-effective%20perception%20systems%20in%20autonomous%20driving%2C%20by%20capitalizing%20on%20the%0Aubiquitous%20presence%20of%20RADAR%20in%20automotive%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15313v1&entry.124074799=Read"},
{"title": "Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared\n  Autonomy", "author": "Dawei Zhang and Roberto Tron", "abstract": "  We present a novel approach that aims to address both safety and stability of\na haptic teleoperation system within a framework of Haptic Shared Autonomy\n(HSA). We use Control Barrier Functions (CBFs) to generate the control input\nthat follows the user's input as closely as possible while guaranteeing safety.\nIn the context of stability of the human-in-the-loop system, we limit the force\nfeedback perceived by the user via a small $L_2$-gain, which is achieved by\nlimiting the control and the force feedback via a differential constraint.\nSpecifically, with the property of HSA, we propose two pathways to design the\ncontrol and the force feedback: Sequential Control Force (SCF) and Joint\nControl Force (JCF). Both designs can achieve safety and stability but with\ndifferent responses to the user's commands. We conducted experimental\nsimulations to evaluate and investigate the properties of the designed methods.\nWe also tested the proposed method on a physical quadrotor UAV and a haptic\ninterface.\n", "link": "http://arxiv.org/abs/2403.15335v1", "date": "2024-03-22", "relevancy": 2.1938, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5773}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5372}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5241}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Safe%20and%20Stable%20Teleoperation%20of%20Quadrotor%20UAVs%20under%20Haptic%20Shared%0A%20%20Autonomy&body=Title%3A%20Safe%20and%20Stable%20Teleoperation%20of%20Quadrotor%20UAVs%20under%20Haptic%20Shared%0A%20%20Autonomy%0AAuthor%3A%20Dawei%20Zhang%20and%20Roberto%20Tron%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20that%20aims%20to%20address%20both%20safety%20and%20stability%20of%0Aa%20haptic%20teleoperation%20system%20within%20a%20framework%20of%20Haptic%20Shared%20Autonomy%0A%28HSA%29.%20We%20use%20Control%20Barrier%20Functions%20%28CBFs%29%20to%20generate%20the%20control%20input%0Athat%20follows%20the%20user%27s%20input%20as%20closely%20as%20possible%20while%20guaranteeing%20safety.%0AIn%20the%20context%20of%20stability%20of%20the%20human-in-the-loop%20system%2C%20we%20limit%20the%20force%0Afeedback%20perceived%20by%20the%20user%20via%20a%20small%20%24L_2%24-gain%2C%20which%20is%20achieved%20by%0Alimiting%20the%20control%20and%20the%20force%20feedback%20via%20a%20differential%20constraint.%0ASpecifically%2C%20with%20the%20property%20of%20HSA%2C%20we%20propose%20two%20pathways%20to%20design%20the%0Acontrol%20and%20the%20force%20feedback%3A%20Sequential%20Control%20Force%20%28SCF%29%20and%20Joint%0AControl%20Force%20%28JCF%29.%20Both%20designs%20can%20achieve%20safety%20and%20stability%20but%20with%0Adifferent%20responses%20to%20the%20user%27s%20commands.%20We%20conducted%20experimental%0Asimulations%20to%20evaluate%20and%20investigate%20the%20properties%20of%20the%20designed%20methods.%0AWe%20also%20tested%20the%20proposed%20method%20on%20a%20physical%20quadrotor%20UAV%20and%20a%20haptic%0Ainterface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15335v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20and%20Stable%20Teleoperation%20of%20Quadrotor%20UAVs%20under%20Haptic%20Shared%0A%20%20Autonomy&entry.906535625=Dawei%20Zhang%20and%20Roberto%20Tron&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20that%20aims%20to%20address%20both%20safety%20and%20stability%20of%0Aa%20haptic%20teleoperation%20system%20within%20a%20framework%20of%20Haptic%20Shared%20Autonomy%0A%28HSA%29.%20We%20use%20Control%20Barrier%20Functions%20%28CBFs%29%20to%20generate%20the%20control%20input%0Athat%20follows%20the%20user%27s%20input%20as%20closely%20as%20possible%20while%20guaranteeing%20safety.%0AIn%20the%20context%20of%20stability%20of%20the%20human-in-the-loop%20system%2C%20we%20limit%20the%20force%0Afeedback%20perceived%20by%20the%20user%20via%20a%20small%20%24L_2%24-gain%2C%20which%20is%20achieved%20by%0Alimiting%20the%20control%20and%20the%20force%20feedback%20via%20a%20differential%20constraint.%0ASpecifically%2C%20with%20the%20property%20of%20HSA%2C%20we%20propose%20two%20pathways%20to%20design%20the%0Acontrol%20and%20the%20force%20feedback%3A%20Sequential%20Control%20Force%20%28SCF%29%20and%20Joint%0AControl%20Force%20%28JCF%29.%20Both%20designs%20can%20achieve%20safety%20and%20stability%20but%20with%0Adifferent%20responses%20to%20the%20user%27s%20commands.%20We%20conducted%20experimental%0Asimulations%20to%20evaluate%20and%20investigate%20the%20properties%20of%20the%20designed%20methods.%0AWe%20also%20tested%20the%20proposed%20method%20on%20a%20physical%20quadrotor%20UAV%20and%20a%20haptic%0Ainterface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15335v1&entry.124074799=Read"},
{"title": "Modular Deep Active Learning Framework for Image Annotation: A Technical\n  Report for the Ophthalmo-AI Project", "author": "Md Abdul Kadir and Hasan Md Tusfiqur Alam and Pascale Maul and Hans-J\u00fcrgen Profitlich and Moritz Wolf and Daniel Sonntag", "abstract": "  Image annotation is one of the most essential tasks for guaranteeing proper\ntreatment for patients and tracking progress over the course of therapy in the\nfield of medical imaging and disease diagnosis. However, manually annotating a\nlot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL)\nbased segmentation algorithms have completely transformed this process and made\nit possible to automate image segmentation. By accurately segmenting medical\nimages, these algorithms can greatly minimize the time and effort necessary for\nmanual annotation. Additionally, by incorporating Active Learning (AL) methods,\nthese segmentation algorithms can perform far more effectively with a smaller\namount of ground truth data. We introduce MedDeepCyleAL, an end-to-end\nframework implementing the complete AL cycle. It provides researchers with the\nflexibility to choose the type of deep learning model they wish to employ and\nincludes an annotation tool that supports the classification and segmentation\nof medical images. The user-friendly interface allows for easy alteration of\nthe AL and DL model settings through a configuration file, requiring no prior\nprogramming experience. While MedDeepCyleAL can be applied to any kind of image\ndata, we have specifically applied it to ophthalmology data in this project.\n", "link": "http://arxiv.org/abs/2403.15143v1", "date": "2024-03-22", "relevancy": 2.1913, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5856}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5627}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5179}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Modular%20Deep%20Active%20Learning%20Framework%20for%20Image%20Annotation%3A%20A%20Technical%0A%20%20Report%20for%20the%20Ophthalmo-AI%20Project&body=Title%3A%20Modular%20Deep%20Active%20Learning%20Framework%20for%20Image%20Annotation%3A%20A%20Technical%0A%20%20Report%20for%20the%20Ophthalmo-AI%20Project%0AAuthor%3A%20Md%20Abdul%20Kadir%20and%20Hasan%20Md%20Tusfiqur%20Alam%20and%20Pascale%20Maul%20and%20Hans-J%C3%BCrgen%20Profitlich%20and%20Moritz%20Wolf%20and%20Daniel%20Sonntag%0AAbstract%3A%20%20%20Image%20annotation%20is%20one%20of%20the%20most%20essential%20tasks%20for%20guaranteeing%20proper%0Atreatment%20for%20patients%20and%20tracking%20progress%20over%20the%20course%20of%20therapy%20in%20the%0Afield%20of%20medical%20imaging%20and%20disease%20diagnosis.%20However%2C%20manually%20annotating%20a%0Alot%20of%202D%20and%203D%20imaging%20data%20can%20be%20extremely%20tedious.%20Deep%20Learning%20%28DL%29%0Abased%20segmentation%20algorithms%20have%20completely%20transformed%20this%20process%20and%20made%0Ait%20possible%20to%20automate%20image%20segmentation.%20By%20accurately%20segmenting%20medical%0Aimages%2C%20these%20algorithms%20can%20greatly%20minimize%20the%20time%20and%20effort%20necessary%20for%0Amanual%20annotation.%20Additionally%2C%20by%20incorporating%20Active%20Learning%20%28AL%29%20methods%2C%0Athese%20segmentation%20algorithms%20can%20perform%20far%20more%20effectively%20with%20a%20smaller%0Aamount%20of%20ground%20truth%20data.%20We%20introduce%20MedDeepCyleAL%2C%20an%20end-to-end%0Aframework%20implementing%20the%20complete%20AL%20cycle.%20It%20provides%20researchers%20with%20the%0Aflexibility%20to%20choose%20the%20type%20of%20deep%20learning%20model%20they%20wish%20to%20employ%20and%0Aincludes%20an%20annotation%20tool%20that%20supports%20the%20classification%20and%20segmentation%0Aof%20medical%20images.%20The%20user-friendly%20interface%20allows%20for%20easy%20alteration%20of%0Athe%20AL%20and%20DL%20model%20settings%20through%20a%20configuration%20file%2C%20requiring%20no%20prior%0Aprogramming%20experience.%20While%20MedDeepCyleAL%20can%20be%20applied%20to%20any%20kind%20of%20image%0Adata%2C%20we%20have%20specifically%20applied%20it%20to%20ophthalmology%20data%20in%20this%20project.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15143v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Deep%20Active%20Learning%20Framework%20for%20Image%20Annotation%3A%20A%20Technical%0A%20%20Report%20for%20the%20Ophthalmo-AI%20Project&entry.906535625=Md%20Abdul%20Kadir%20and%20Hasan%20Md%20Tusfiqur%20Alam%20and%20Pascale%20Maul%20and%20Hans-J%C3%BCrgen%20Profitlich%20and%20Moritz%20Wolf%20and%20Daniel%20Sonntag&entry.1292438233=%20%20Image%20annotation%20is%20one%20of%20the%20most%20essential%20tasks%20for%20guaranteeing%20proper%0Atreatment%20for%20patients%20and%20tracking%20progress%20over%20the%20course%20of%20therapy%20in%20the%0Afield%20of%20medical%20imaging%20and%20disease%20diagnosis.%20However%2C%20manually%20annotating%20a%0Alot%20of%202D%20and%203D%20imaging%20data%20can%20be%20extremely%20tedious.%20Deep%20Learning%20%28DL%29%0Abased%20segmentation%20algorithms%20have%20completely%20transformed%20this%20process%20and%20made%0Ait%20possible%20to%20automate%20image%20segmentation.%20By%20accurately%20segmenting%20medical%0Aimages%2C%20these%20algorithms%20can%20greatly%20minimize%20the%20time%20and%20effort%20necessary%20for%0Amanual%20annotation.%20Additionally%2C%20by%20incorporating%20Active%20Learning%20%28AL%29%20methods%2C%0Athese%20segmentation%20algorithms%20can%20perform%20far%20more%20effectively%20with%20a%20smaller%0Aamount%20of%20ground%20truth%20data.%20We%20introduce%20MedDeepCyleAL%2C%20an%20end-to-end%0Aframework%20implementing%20the%20complete%20AL%20cycle.%20It%20provides%20researchers%20with%20the%0Aflexibility%20to%20choose%20the%20type%20of%20deep%20learning%20model%20they%20wish%20to%20employ%20and%0Aincludes%20an%20annotation%20tool%20that%20supports%20the%20classification%20and%20segmentation%0Aof%20medical%20images.%20The%20user-friendly%20interface%20allows%20for%20easy%20alteration%20of%0Athe%20AL%20and%20DL%20model%20settings%20through%20a%20configuration%20file%2C%20requiring%20no%20prior%0Aprogramming%20experience.%20While%20MedDeepCyleAL%20can%20be%20applied%20to%20any%20kind%20of%20image%0Adata%2C%20we%20have%20specifically%20applied%20it%20to%20ophthalmology%20data%20in%20this%20project.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15143v1&entry.124074799=Read"},
{"title": "ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal\n  Instance Segmentation", "author": "Minh Tran and Winston Bounsavy and Khoa Vo and Anh Nguyen and Tri Nguyen and Ngan Le", "abstract": "  Amodal Instance Segmentation (AIS) presents a challenging task as it involves\npredicting both visible and occluded parts of objects within images. Existing\nAIS methods rely on a bidirectional approach, encompassing both the transition\nfrom amodal features to visible features (amodal-to-visible) and from visible\nfeatures to amodal features (visible-to-amodal). Our observation shows that the\nutilization of amodal features through the amodal-to-visible can confuse the\nvisible features due to the extra information of occluded/hidden segments not\npresented in visible display. Consequently, this compromised quality of visible\nfeatures during the subsequent visible-to-amodal transition. To tackle this\nissue, we introduce ShapeFormer, a decoupled Transformer-based model with a\nvisible-to-amodal transition. It facilitates the explicit relationship between\noutput segmentations and avoids the need for amodal-to-visible transitions.\nShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head for\npredicting visible segmentation with occlusion awareness, (ii) Shape-Prior\nAmodal Mask Head for predicting amodal and occluded masks, and (iii)\nCategory-Specific Shape Prior Retriever aims to provide shape prior knowledge.\nComprehensive experiments and extensive ablation studies across various AIS\nbenchmarks demonstrate the effectiveness of our ShapeFormer. The code is\navailable at: https://github.com/UARK-AICV/ShapeFormer\n", "link": "http://arxiv.org/abs/2403.11376v2", "date": "2024-03-22", "relevancy": 2.1858, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6146}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4959}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ShapeFormer%3A%20Shape%20Prior%20Visible-to-Amodal%20Transformer-based%20Amodal%0A%20%20Instance%20Segmentation&body=Title%3A%20ShapeFormer%3A%20Shape%20Prior%20Visible-to-Amodal%20Transformer-based%20Amodal%0A%20%20Instance%20Segmentation%0AAuthor%3A%20Minh%20Tran%20and%20Winston%20Bounsavy%20and%20Khoa%20Vo%20and%20Anh%20Nguyen%20and%20Tri%20Nguyen%20and%20Ngan%20Le%0AAbstract%3A%20%20%20Amodal%20Instance%20Segmentation%20%28AIS%29%20presents%20a%20challenging%20task%20as%20it%20involves%0Apredicting%20both%20visible%20and%20occluded%20parts%20of%20objects%20within%20images.%20Existing%0AAIS%20methods%20rely%20on%20a%20bidirectional%20approach%2C%20encompassing%20both%20the%20transition%0Afrom%20amodal%20features%20to%20visible%20features%20%28amodal-to-visible%29%20and%20from%20visible%0Afeatures%20to%20amodal%20features%20%28visible-to-amodal%29.%20Our%20observation%20shows%20that%20the%0Autilization%20of%20amodal%20features%20through%20the%20amodal-to-visible%20can%20confuse%20the%0Avisible%20features%20due%20to%20the%20extra%20information%20of%20occluded/hidden%20segments%20not%0Apresented%20in%20visible%20display.%20Consequently%2C%20this%20compromised%20quality%20of%20visible%0Afeatures%20during%20the%20subsequent%20visible-to-amodal%20transition.%20To%20tackle%20this%0Aissue%2C%20we%20introduce%20ShapeFormer%2C%20a%20decoupled%20Transformer-based%20model%20with%20a%0Avisible-to-amodal%20transition.%20It%20facilitates%20the%20explicit%20relationship%20between%0Aoutput%20segmentations%20and%20avoids%20the%20need%20for%20amodal-to-visible%20transitions.%0AShapeFormer%20comprises%20three%20key%20modules%3A%20%28i%29%20Visible-Occluding%20Mask%20Head%20for%0Apredicting%20visible%20segmentation%20with%20occlusion%20awareness%2C%20%28ii%29%20Shape-Prior%0AAmodal%20Mask%20Head%20for%20predicting%20amodal%20and%20occluded%20masks%2C%20and%20%28iii%29%0ACategory-Specific%20Shape%20Prior%20Retriever%20aims%20to%20provide%20shape%20prior%20knowledge.%0AComprehensive%20experiments%20and%20extensive%20ablation%20studies%20across%20various%20AIS%0Abenchmarks%20demonstrate%20the%20effectiveness%20of%20our%20ShapeFormer.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/UARK-AICV/ShapeFormer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11376v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeFormer%3A%20Shape%20Prior%20Visible-to-Amodal%20Transformer-based%20Amodal%0A%20%20Instance%20Segmentation&entry.906535625=Minh%20Tran%20and%20Winston%20Bounsavy%20and%20Khoa%20Vo%20and%20Anh%20Nguyen%20and%20Tri%20Nguyen%20and%20Ngan%20Le&entry.1292438233=%20%20Amodal%20Instance%20Segmentation%20%28AIS%29%20presents%20a%20challenging%20task%20as%20it%20involves%0Apredicting%20both%20visible%20and%20occluded%20parts%20of%20objects%20within%20images.%20Existing%0AAIS%20methods%20rely%20on%20a%20bidirectional%20approach%2C%20encompassing%20both%20the%20transition%0Afrom%20amodal%20features%20to%20visible%20features%20%28amodal-to-visible%29%20and%20from%20visible%0Afeatures%20to%20amodal%20features%20%28visible-to-amodal%29.%20Our%20observation%20shows%20that%20the%0Autilization%20of%20amodal%20features%20through%20the%20amodal-to-visible%20can%20confuse%20the%0Avisible%20features%20due%20to%20the%20extra%20information%20of%20occluded/hidden%20segments%20not%0Apresented%20in%20visible%20display.%20Consequently%2C%20this%20compromised%20quality%20of%20visible%0Afeatures%20during%20the%20subsequent%20visible-to-amodal%20transition.%20To%20tackle%20this%0Aissue%2C%20we%20introduce%20ShapeFormer%2C%20a%20decoupled%20Transformer-based%20model%20with%20a%0Avisible-to-amodal%20transition.%20It%20facilitates%20the%20explicit%20relationship%20between%0Aoutput%20segmentations%20and%20avoids%20the%20need%20for%20amodal-to-visible%20transitions.%0AShapeFormer%20comprises%20three%20key%20modules%3A%20%28i%29%20Visible-Occluding%20Mask%20Head%20for%0Apredicting%20visible%20segmentation%20with%20occlusion%20awareness%2C%20%28ii%29%20Shape-Prior%0AAmodal%20Mask%20Head%20for%20predicting%20amodal%20and%20occluded%20masks%2C%20and%20%28iii%29%0ACategory-Specific%20Shape%20Prior%20Retriever%20aims%20to%20provide%20shape%20prior%20knowledge.%0AComprehensive%20experiments%20and%20extensive%20ablation%20studies%20across%20various%20AIS%0Abenchmarks%20demonstrate%20the%20effectiveness%20of%20our%20ShapeFormer.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/UARK-AICV/ShapeFormer%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11376v2&entry.124074799=Read"},
{"title": "MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image\n  Acquisition Systems", "author": "Yu Gao and Lutong Su and Hao Liang and Yufeng Yue and Yi Yang and Mengyin Fu", "abstract": "  Neural Radiance Fields (NeRF) use multi-view images for 3D scene\nrepresentation, demonstrating remarkable performance. As one of the primary\nsources of multi-view images, multi-camera systems encounter challenges such as\nvarying intrinsic parameters and frequent pose changes. Most previous\nNeRF-based methods assume a unique camera and rarely consider multi-camera\nscenarios. Besides, some NeRF methods that can optimize intrinsic and extrinsic\nparameters still remain susceptible to suboptimal solutions when these\nparameters are poor initialized. In this paper, we propose MC-NeRF, a method\nthat enables joint optimization of both intrinsic and extrinsic parameters\nalongside NeRF. The method also supports each image corresponding to\nindependent camera parameters. First, we tackle coupling issue and the\ndegenerate case that arise from the joint optimization between intrinsic and\nextrinsic parameters. Second, based on the proposed solutions, we introduce an\nefficient calibration image acquisition scheme for multi-camera systems,\nincluding the design of calibration object. Finally, we present an end-to-end\nnetwork with training sequence that enables the estimation of intrinsic and\nextrinsic parameters, along with the rendering network. Furthermore,\nrecognizing that most existing datasets are designed for a unique camera, we\nconstruct a real multi-camera image acquisition system and create a\ncorresponding new dataset, which includes both simulated data and real-world\ncaptured images. Experiments confirm the effectiveness of our method when each\nimage corresponds to different camera parameters. Specifically, we use\nmulti-cameras, each with different intrinsic and extrinsic parameters in\nreal-world system, to achieve 3D scene representation without providing initial\nposes.\n", "link": "http://arxiv.org/abs/2309.07846v3", "date": "2024-03-22", "relevancy": 2.1853, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.56}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5413}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5247}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MC-NeRF%3A%20Multi-Camera%20Neural%20Radiance%20Fields%20for%20Multi-Camera%20Image%0A%20%20Acquisition%20Systems&body=Title%3A%20MC-NeRF%3A%20Multi-Camera%20Neural%20Radiance%20Fields%20for%20Multi-Camera%20Image%0A%20%20Acquisition%20Systems%0AAuthor%3A%20Yu%20Gao%20and%20Lutong%20Su%20and%20Hao%20Liang%20and%20Yufeng%20Yue%20and%20Yi%20Yang%20and%20Mengyin%20Fu%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20use%20multi-view%20images%20for%203D%20scene%0Arepresentation%2C%20demonstrating%20remarkable%20performance.%20As%20one%20of%20the%20primary%0Asources%20of%20multi-view%20images%2C%20multi-camera%20systems%20encounter%20challenges%20such%20as%0Avarying%20intrinsic%20parameters%20and%20frequent%20pose%20changes.%20Most%20previous%0ANeRF-based%20methods%20assume%20a%20unique%20camera%20and%20rarely%20consider%20multi-camera%0Ascenarios.%20Besides%2C%20some%20NeRF%20methods%20that%20can%20optimize%20intrinsic%20and%20extrinsic%0Aparameters%20still%20remain%20susceptible%20to%20suboptimal%20solutions%20when%20these%0Aparameters%20are%20poor%20initialized.%20In%20this%20paper%2C%20we%20propose%20MC-NeRF%2C%20a%20method%0Athat%20enables%20joint%20optimization%20of%20both%20intrinsic%20and%20extrinsic%20parameters%0Aalongside%20NeRF.%20The%20method%20also%20supports%20each%20image%20corresponding%20to%0Aindependent%20camera%20parameters.%20First%2C%20we%20tackle%20coupling%20issue%20and%20the%0Adegenerate%20case%20that%20arise%20from%20the%20joint%20optimization%20between%20intrinsic%20and%0Aextrinsic%20parameters.%20Second%2C%20based%20on%20the%20proposed%20solutions%2C%20we%20introduce%20an%0Aefficient%20calibration%20image%20acquisition%20scheme%20for%20multi-camera%20systems%2C%0Aincluding%20the%20design%20of%20calibration%20object.%20Finally%2C%20we%20present%20an%20end-to-end%0Anetwork%20with%20training%20sequence%20that%20enables%20the%20estimation%20of%20intrinsic%20and%0Aextrinsic%20parameters%2C%20along%20with%20the%20rendering%20network.%20Furthermore%2C%0Arecognizing%20that%20most%20existing%20datasets%20are%20designed%20for%20a%20unique%20camera%2C%20we%0Aconstruct%20a%20real%20multi-camera%20image%20acquisition%20system%20and%20create%20a%0Acorresponding%20new%20dataset%2C%20which%20includes%20both%20simulated%20data%20and%20real-world%0Acaptured%20images.%20Experiments%20confirm%20the%20effectiveness%20of%20our%20method%20when%20each%0Aimage%20corresponds%20to%20different%20camera%20parameters.%20Specifically%2C%20we%20use%0Amulti-cameras%2C%20each%20with%20different%20intrinsic%20and%20extrinsic%20parameters%20in%0Areal-world%20system%2C%20to%20achieve%203D%20scene%20representation%20without%20providing%20initial%0Aposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.07846v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MC-NeRF%3A%20Multi-Camera%20Neural%20Radiance%20Fields%20for%20Multi-Camera%20Image%0A%20%20Acquisition%20Systems&entry.906535625=Yu%20Gao%20and%20Lutong%20Su%20and%20Hao%20Liang%20and%20Yufeng%20Yue%20and%20Yi%20Yang%20and%20Mengyin%20Fu&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20use%20multi-view%20images%20for%203D%20scene%0Arepresentation%2C%20demonstrating%20remarkable%20performance.%20As%20one%20of%20the%20primary%0Asources%20of%20multi-view%20images%2C%20multi-camera%20systems%20encounter%20challenges%20such%20as%0Avarying%20intrinsic%20parameters%20and%20frequent%20pose%20changes.%20Most%20previous%0ANeRF-based%20methods%20assume%20a%20unique%20camera%20and%20rarely%20consider%20multi-camera%0Ascenarios.%20Besides%2C%20some%20NeRF%20methods%20that%20can%20optimize%20intrinsic%20and%20extrinsic%0Aparameters%20still%20remain%20susceptible%20to%20suboptimal%20solutions%20when%20these%0Aparameters%20are%20poor%20initialized.%20In%20this%20paper%2C%20we%20propose%20MC-NeRF%2C%20a%20method%0Athat%20enables%20joint%20optimization%20of%20both%20intrinsic%20and%20extrinsic%20parameters%0Aalongside%20NeRF.%20The%20method%20also%20supports%20each%20image%20corresponding%20to%0Aindependent%20camera%20parameters.%20First%2C%20we%20tackle%20coupling%20issue%20and%20the%0Adegenerate%20case%20that%20arise%20from%20the%20joint%20optimization%20between%20intrinsic%20and%0Aextrinsic%20parameters.%20Second%2C%20based%20on%20the%20proposed%20solutions%2C%20we%20introduce%20an%0Aefficient%20calibration%20image%20acquisition%20scheme%20for%20multi-camera%20systems%2C%0Aincluding%20the%20design%20of%20calibration%20object.%20Finally%2C%20we%20present%20an%20end-to-end%0Anetwork%20with%20training%20sequence%20that%20enables%20the%20estimation%20of%20intrinsic%20and%0Aextrinsic%20parameters%2C%20along%20with%20the%20rendering%20network.%20Furthermore%2C%0Arecognizing%20that%20most%20existing%20datasets%20are%20designed%20for%20a%20unique%20camera%2C%20we%0Aconstruct%20a%20real%20multi-camera%20image%20acquisition%20system%20and%20create%20a%0Acorresponding%20new%20dataset%2C%20which%20includes%20both%20simulated%20data%20and%20real-world%0Acaptured%20images.%20Experiments%20confirm%20the%20effectiveness%20of%20our%20method%20when%20each%0Aimage%20corresponds%20to%20different%20camera%20parameters.%20Specifically%2C%20we%20use%0Amulti-cameras%2C%20each%20with%20different%20intrinsic%20and%20extrinsic%20parameters%20in%0Areal-world%20system%2C%20to%20achieve%203D%20scene%20representation%20without%20providing%20initial%0Aposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07846v3&entry.124074799=Read"},
{"title": "Robust Direct Data-Driven Control for Probabilistic Systems", "author": "Alexander von Rohr and Dmitrii Likhachev and Sebastian Trimpe", "abstract": "  We propose a data-driven control method for systems with aleatoric\nuncertainty, for example, robot fleets with variations between agents. Our\nmethod leverages shared trajectory data to increase the robustness of the\ndesigned controller and thus facilitate transfer to new variations without the\nneed for prior parameter and uncertainty estimations. In contrast to existing\nwork on experience transfer for performance, our approach focuses on robustness\nand uses data collected from multiple realizations to guarantee generalization\nto unseen ones. Our method is based on scenario optimization combined with\nrecent formulations for direct data-driven control. We derive lower bounds on\nthe amount of data required to achieve quadratic stability for probabilistic\nsystems with aleatoric uncertainty and demonstrate the benefits of our\ndata-driven method through a numerical example. We find that the learned\ncontrollers generalize well to high variations in the dynamics even when based\non only a few short open-loop trajectories. Robust experience transfer enables\nthe design of safe and robust controllers that work out of the box without any\nadditional learning during deployment.\n", "link": "http://arxiv.org/abs/2306.16973v2", "date": "2024-03-22", "relevancy": 2.1841, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5945}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5419}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5308}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Direct%20Data-Driven%20Control%20for%20Probabilistic%20Systems&body=Title%3A%20Robust%20Direct%20Data-Driven%20Control%20for%20Probabilistic%20Systems%0AAuthor%3A%20Alexander%20von%20Rohr%20and%20Dmitrii%20Likhachev%20and%20Sebastian%20Trimpe%0AAbstract%3A%20%20%20We%20propose%20a%20data-driven%20control%20method%20for%20systems%20with%20aleatoric%0Auncertainty%2C%20for%20example%2C%20robot%20fleets%20with%20variations%20between%20agents.%20Our%0Amethod%20leverages%20shared%20trajectory%20data%20to%20increase%20the%20robustness%20of%20the%0Adesigned%20controller%20and%20thus%20facilitate%20transfer%20to%20new%20variations%20without%20the%0Aneed%20for%20prior%20parameter%20and%20uncertainty%20estimations.%20In%20contrast%20to%20existing%0Awork%20on%20experience%20transfer%20for%20performance%2C%20our%20approach%20focuses%20on%20robustness%0Aand%20uses%20data%20collected%20from%20multiple%20realizations%20to%20guarantee%20generalization%0Ato%20unseen%20ones.%20Our%20method%20is%20based%20on%20scenario%20optimization%20combined%20with%0Arecent%20formulations%20for%20direct%20data-driven%20control.%20We%20derive%20lower%20bounds%20on%0Athe%20amount%20of%20data%20required%20to%20achieve%20quadratic%20stability%20for%20probabilistic%0Asystems%20with%20aleatoric%20uncertainty%20and%20demonstrate%20the%20benefits%20of%20our%0Adata-driven%20method%20through%20a%20numerical%20example.%20We%20find%20that%20the%20learned%0Acontrollers%20generalize%20well%20to%20high%20variations%20in%20the%20dynamics%20even%20when%20based%0Aon%20only%20a%20few%20short%20open-loop%20trajectories.%20Robust%20experience%20transfer%20enables%0Athe%20design%20of%20safe%20and%20robust%20controllers%20that%20work%20out%20of%20the%20box%20without%20any%0Aadditional%20learning%20during%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.16973v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Direct%20Data-Driven%20Control%20for%20Probabilistic%20Systems&entry.906535625=Alexander%20von%20Rohr%20and%20Dmitrii%20Likhachev%20and%20Sebastian%20Trimpe&entry.1292438233=%20%20We%20propose%20a%20data-driven%20control%20method%20for%20systems%20with%20aleatoric%0Auncertainty%2C%20for%20example%2C%20robot%20fleets%20with%20variations%20between%20agents.%20Our%0Amethod%20leverages%20shared%20trajectory%20data%20to%20increase%20the%20robustness%20of%20the%0Adesigned%20controller%20and%20thus%20facilitate%20transfer%20to%20new%20variations%20without%20the%0Aneed%20for%20prior%20parameter%20and%20uncertainty%20estimations.%20In%20contrast%20to%20existing%0Awork%20on%20experience%20transfer%20for%20performance%2C%20our%20approach%20focuses%20on%20robustness%0Aand%20uses%20data%20collected%20from%20multiple%20realizations%20to%20guarantee%20generalization%0Ato%20unseen%20ones.%20Our%20method%20is%20based%20on%20scenario%20optimization%20combined%20with%0Arecent%20formulations%20for%20direct%20data-driven%20control.%20We%20derive%20lower%20bounds%20on%0Athe%20amount%20of%20data%20required%20to%20achieve%20quadratic%20stability%20for%20probabilistic%0Asystems%20with%20aleatoric%20uncertainty%20and%20demonstrate%20the%20benefits%20of%20our%0Adata-driven%20method%20through%20a%20numerical%20example.%20We%20find%20that%20the%20learned%0Acontrollers%20generalize%20well%20to%20high%20variations%20in%20the%20dynamics%20even%20when%20based%0Aon%20only%20a%20few%20short%20open-loop%20trajectories.%20Robust%20experience%20transfer%20enables%0Athe%20design%20of%20safe%20and%20robust%20controllers%20that%20work%20out%20of%20the%20box%20without%20any%0Aadditional%20learning%20during%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.16973v2&entry.124074799=Read"},
{"title": "MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral\n  Pedestrian Detection", "author": "Taeheon Kim and Sangyun Chung and Damin Yeom and Youngjoon Yu and Hak Gu Kim and Yong Man Ro", "abstract": "  Multispectral pedestrian detection is attractive for around-the-clock\napplications due to the complementary information between RGB and thermal\nmodalities. However, current models often fail to detect pedestrians in obvious\ncases, especially due to the modality bias learned from statistically biased\ndatasets. From these problems, we anticipate that maybe understanding the\ncomplementary information itself is difficult to achieve from vision-only\nmodels. Accordingly, we propose a novel Multispectral Chain-of-Thought\nDetection (MSCoTDet) framework, which incorporates Large Language Models (LLMs)\nto understand the complementary information at the semantic level and further\nenhance the fusion process. Specifically, we generate text descriptions of the\npedestrian in each RGB and thermal modality and design a Multispectral\nChain-of-Thought (MSCoT) prompting, which models a step-by-step process to\nfacilitate cross-modal reasoning at the semantic level and perform accurate\ndetection. Moreover, we design a Language-driven Multi-modal Fusion (LMF)\nstrategy that enables fusing vision-driven and language-driven detections.\nExtensive experiments validate that MSCoTDet improves multispectral pedestrian\ndetection.\n", "link": "http://arxiv.org/abs/2403.15209v1", "date": "2024-03-22", "relevancy": 2.1694, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5769}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5514}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5195}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MSCoTDet%3A%20Language-driven%20Multi-modal%20Fusion%20for%20Improved%20Multispectral%0A%20%20Pedestrian%20Detection&body=Title%3A%20MSCoTDet%3A%20Language-driven%20Multi-modal%20Fusion%20for%20Improved%20Multispectral%0A%20%20Pedestrian%20Detection%0AAuthor%3A%20Taeheon%20Kim%20and%20Sangyun%20Chung%20and%20Damin%20Yeom%20and%20Youngjoon%20Yu%20and%20Hak%20Gu%20Kim%20and%20Yong%20Man%20Ro%0AAbstract%3A%20%20%20Multispectral%20pedestrian%20detection%20is%20attractive%20for%20around-the-clock%0Aapplications%20due%20to%20the%20complementary%20information%20between%20RGB%20and%20thermal%0Amodalities.%20However%2C%20current%20models%20often%20fail%20to%20detect%20pedestrians%20in%20obvious%0Acases%2C%20especially%20due%20to%20the%20modality%20bias%20learned%20from%20statistically%20biased%0Adatasets.%20From%20these%20problems%2C%20we%20anticipate%20that%20maybe%20understanding%20the%0Acomplementary%20information%20itself%20is%20difficult%20to%20achieve%20from%20vision-only%0Amodels.%20Accordingly%2C%20we%20propose%20a%20novel%20Multispectral%20Chain-of-Thought%0ADetection%20%28MSCoTDet%29%20framework%2C%20which%20incorporates%20Large%20Language%20Models%20%28LLMs%29%0Ato%20understand%20the%20complementary%20information%20at%20the%20semantic%20level%20and%20further%0Aenhance%20the%20fusion%20process.%20Specifically%2C%20we%20generate%20text%20descriptions%20of%20the%0Apedestrian%20in%20each%20RGB%20and%20thermal%20modality%20and%20design%20a%20Multispectral%0AChain-of-Thought%20%28MSCoT%29%20prompting%2C%20which%20models%20a%20step-by-step%20process%20to%0Afacilitate%20cross-modal%20reasoning%20at%20the%20semantic%20level%20and%20perform%20accurate%0Adetection.%20Moreover%2C%20we%20design%20a%20Language-driven%20Multi-modal%20Fusion%20%28LMF%29%0Astrategy%20that%20enables%20fusing%20vision-driven%20and%20language-driven%20detections.%0AExtensive%20experiments%20validate%20that%20MSCoTDet%20improves%20multispectral%20pedestrian%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15209v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSCoTDet%3A%20Language-driven%20Multi-modal%20Fusion%20for%20Improved%20Multispectral%0A%20%20Pedestrian%20Detection&entry.906535625=Taeheon%20Kim%20and%20Sangyun%20Chung%20and%20Damin%20Yeom%20and%20Youngjoon%20Yu%20and%20Hak%20Gu%20Kim%20and%20Yong%20Man%20Ro&entry.1292438233=%20%20Multispectral%20pedestrian%20detection%20is%20attractive%20for%20around-the-clock%0Aapplications%20due%20to%20the%20complementary%20information%20between%20RGB%20and%20thermal%0Amodalities.%20However%2C%20current%20models%20often%20fail%20to%20detect%20pedestrians%20in%20obvious%0Acases%2C%20especially%20due%20to%20the%20modality%20bias%20learned%20from%20statistically%20biased%0Adatasets.%20From%20these%20problems%2C%20we%20anticipate%20that%20maybe%20understanding%20the%0Acomplementary%20information%20itself%20is%20difficult%20to%20achieve%20from%20vision-only%0Amodels.%20Accordingly%2C%20we%20propose%20a%20novel%20Multispectral%20Chain-of-Thought%0ADetection%20%28MSCoTDet%29%20framework%2C%20which%20incorporates%20Large%20Language%20Models%20%28LLMs%29%0Ato%20understand%20the%20complementary%20information%20at%20the%20semantic%20level%20and%20further%0Aenhance%20the%20fusion%20process.%20Specifically%2C%20we%20generate%20text%20descriptions%20of%20the%0Apedestrian%20in%20each%20RGB%20and%20thermal%20modality%20and%20design%20a%20Multispectral%0AChain-of-Thought%20%28MSCoT%29%20prompting%2C%20which%20models%20a%20step-by-step%20process%20to%0Afacilitate%20cross-modal%20reasoning%20at%20the%20semantic%20level%20and%20perform%20accurate%0Adetection.%20Moreover%2C%20we%20design%20a%20Language-driven%20Multi-modal%20Fusion%20%28LMF%29%0Astrategy%20that%20enables%20fusing%20vision-driven%20and%20language-driven%20detections.%0AExtensive%20experiments%20validate%20that%20MSCoTDet%20improves%20multispectral%20pedestrian%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15209v1&entry.124074799=Read"},
{"title": "Collaborative AI Teaming in Unknown Environments via Active Goal\n  Deduction", "author": "Zuyuan Zhang and Hanhan Zhou and Mahdi Imani and Taeyoung Lee and Tian Lan", "abstract": "  With the advancements of artificial intelligence (AI), we're seeing more\nscenarios that require AI to work closely with other agents, whose goals and\nstrategies might not be known beforehand. However, existing approaches for\ntraining collaborative agents often require defined and known reward signals\nand cannot address the problem of teaming with unknown agents that often have\nlatent objectives/rewards. In response to this challenge, we propose teaming\nwith unknown agents framework, which leverages kernel density Bayesian inverse\nlearning method for active goal deduction and utilizes pre-trained,\ngoal-conditioned policies to enable zero-shot policy adaptation. We prove that\nunbiased reward estimates in our framework are sufficient for optimal teaming\nwith unknown agents. We further evaluate the framework of redesigned\nmulti-agent particle and StarCraft II micromanagement environments with diverse\nunknown agents of different behaviors/rewards. Empirical results demonstrate\nthat our framework significantly advances the teaming performance of AI and\nunknown agents in a wide range of collaborative scenarios.\n", "link": "http://arxiv.org/abs/2403.15341v1", "date": "2024-03-22", "relevancy": 2.1624, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5475}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5433}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5326}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Collaborative%20AI%20Teaming%20in%20Unknown%20Environments%20via%20Active%20Goal%0A%20%20Deduction&body=Title%3A%20Collaborative%20AI%20Teaming%20in%20Unknown%20Environments%20via%20Active%20Goal%0A%20%20Deduction%0AAuthor%3A%20Zuyuan%20Zhang%20and%20Hanhan%20Zhou%20and%20Mahdi%20Imani%20and%20Taeyoung%20Lee%20and%20Tian%20Lan%0AAbstract%3A%20%20%20With%20the%20advancements%20of%20artificial%20intelligence%20%28AI%29%2C%20we%27re%20seeing%20more%0Ascenarios%20that%20require%20AI%20to%20work%20closely%20with%20other%20agents%2C%20whose%20goals%20and%0Astrategies%20might%20not%20be%20known%20beforehand.%20However%2C%20existing%20approaches%20for%0Atraining%20collaborative%20agents%20often%20require%20defined%20and%20known%20reward%20signals%0Aand%20cannot%20address%20the%20problem%20of%20teaming%20with%20unknown%20agents%20that%20often%20have%0Alatent%20objectives/rewards.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20teaming%0Awith%20unknown%20agents%20framework%2C%20which%20leverages%20kernel%20density%20Bayesian%20inverse%0Alearning%20method%20for%20active%20goal%20deduction%20and%20utilizes%20pre-trained%2C%0Agoal-conditioned%20policies%20to%20enable%20zero-shot%20policy%20adaptation.%20We%20prove%20that%0Aunbiased%20reward%20estimates%20in%20our%20framework%20are%20sufficient%20for%20optimal%20teaming%0Awith%20unknown%20agents.%20We%20further%20evaluate%20the%20framework%20of%20redesigned%0Amulti-agent%20particle%20and%20StarCraft%20II%20micromanagement%20environments%20with%20diverse%0Aunknown%20agents%20of%20different%20behaviors/rewards.%20Empirical%20results%20demonstrate%0Athat%20our%20framework%20significantly%20advances%20the%20teaming%20performance%20of%20AI%20and%0Aunknown%20agents%20in%20a%20wide%20range%20of%20collaborative%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15341v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20AI%20Teaming%20in%20Unknown%20Environments%20via%20Active%20Goal%0A%20%20Deduction&entry.906535625=Zuyuan%20Zhang%20and%20Hanhan%20Zhou%20and%20Mahdi%20Imani%20and%20Taeyoung%20Lee%20and%20Tian%20Lan&entry.1292438233=%20%20With%20the%20advancements%20of%20artificial%20intelligence%20%28AI%29%2C%20we%27re%20seeing%20more%0Ascenarios%20that%20require%20AI%20to%20work%20closely%20with%20other%20agents%2C%20whose%20goals%20and%0Astrategies%20might%20not%20be%20known%20beforehand.%20However%2C%20existing%20approaches%20for%0Atraining%20collaborative%20agents%20often%20require%20defined%20and%20known%20reward%20signals%0Aand%20cannot%20address%20the%20problem%20of%20teaming%20with%20unknown%20agents%20that%20often%20have%0Alatent%20objectives/rewards.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20teaming%0Awith%20unknown%20agents%20framework%2C%20which%20leverages%20kernel%20density%20Bayesian%20inverse%0Alearning%20method%20for%20active%20goal%20deduction%20and%20utilizes%20pre-trained%2C%0Agoal-conditioned%20policies%20to%20enable%20zero-shot%20policy%20adaptation.%20We%20prove%20that%0Aunbiased%20reward%20estimates%20in%20our%20framework%20are%20sufficient%20for%20optimal%20teaming%0Awith%20unknown%20agents.%20We%20further%20evaluate%20the%20framework%20of%20redesigned%0Amulti-agent%20particle%20and%20StarCraft%20II%20micromanagement%20environments%20with%20diverse%0Aunknown%20agents%20of%20different%20behaviors/rewards.%20Empirical%20results%20demonstrate%0Athat%20our%20framework%20significantly%20advances%20the%20teaming%20performance%20of%20AI%20and%0Aunknown%20agents%20in%20a%20wide%20range%20of%20collaborative%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15341v1&entry.124074799=Read"},
{"title": "IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object\n  Detection", "author": "Junbo Yin and Jianbing Shen and Runnan Chen and Wei Li and Ruigang Yang and Pascal Frossard and Wenguan Wang", "abstract": "  Bird's eye view (BEV) representation has emerged as a dominant solution for\ndescribing 3D space in autonomous driving scenarios. However, objects in the\nBEV representation typically exhibit small sizes, and the associated point\ncloud context is inherently sparse, which leads to great challenges for\nreliable 3D perception. In this paper, we propose IS-Fusion, an innovative\nmultimodal fusion framework that jointly captures the Instance- and Scene-level\ncontextual information. IS-Fusion essentially differs from existing approaches\nthat only focus on the BEV scene-level fusion by explicitly incorporating\ninstance-level multimodal information, thus facilitating the instance-centric\ntasks like 3D object detection. It comprises a Hierarchical Scene Fusion (HSF)\nmodule and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid\nand Grid-to-Region transformers to capture the multimodal scene context at\ndifferent granularities. IGF mines instance candidates, explores their\nrelationships, and aggregates the local multimodal context for each instance.\nThese instances then serve as guidance to enhance the scene feature and yield\nan instance-aware BEV representation. On the challenging nuScenes benchmark,\nIS-Fusion outperforms all the published multimodal works to date. Code is\navailable at: https://github.com/yinjunbo/IS-Fusion.\n", "link": "http://arxiv.org/abs/2403.15241v1", "date": "2024-03-22", "relevancy": 2.1556, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5476}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5401}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5297}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IS-Fusion%3A%20Instance-Scene%20Collaborative%20Fusion%20for%20Multimodal%203D%20Object%0A%20%20Detection&body=Title%3A%20IS-Fusion%3A%20Instance-Scene%20Collaborative%20Fusion%20for%20Multimodal%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Junbo%20Yin%20and%20Jianbing%20Shen%20and%20Runnan%20Chen%20and%20Wei%20Li%20and%20Ruigang%20Yang%20and%20Pascal%20Frossard%20and%20Wenguan%20Wang%0AAbstract%3A%20%20%20Bird%27s%20eye%20view%20%28BEV%29%20representation%20has%20emerged%20as%20a%20dominant%20solution%20for%0Adescribing%203D%20space%20in%20autonomous%20driving%20scenarios.%20However%2C%20objects%20in%20the%0ABEV%20representation%20typically%20exhibit%20small%20sizes%2C%20and%20the%20associated%20point%0Acloud%20context%20is%20inherently%20sparse%2C%20which%20leads%20to%20great%20challenges%20for%0Areliable%203D%20perception.%20In%20this%20paper%2C%20we%20propose%20IS-Fusion%2C%20an%20innovative%0Amultimodal%20fusion%20framework%20that%20jointly%20captures%20the%20Instance-%20and%20Scene-level%0Acontextual%20information.%20IS-Fusion%20essentially%20differs%20from%20existing%20approaches%0Athat%20only%20focus%20on%20the%20BEV%20scene-level%20fusion%20by%20explicitly%20incorporating%0Ainstance-level%20multimodal%20information%2C%20thus%20facilitating%20the%20instance-centric%0Atasks%20like%203D%20object%20detection.%20It%20comprises%20a%20Hierarchical%20Scene%20Fusion%20%28HSF%29%0Amodule%20and%20an%20Instance-Guided%20Fusion%20%28IGF%29%20module.%20HSF%20applies%20Point-to-Grid%0Aand%20Grid-to-Region%20transformers%20to%20capture%20the%20multimodal%20scene%20context%20at%0Adifferent%20granularities.%20IGF%20mines%20instance%20candidates%2C%20explores%20their%0Arelationships%2C%20and%20aggregates%20the%20local%20multimodal%20context%20for%20each%20instance.%0AThese%20instances%20then%20serve%20as%20guidance%20to%20enhance%20the%20scene%20feature%20and%20yield%0Aan%20instance-aware%20BEV%20representation.%20On%20the%20challenging%20nuScenes%20benchmark%2C%0AIS-Fusion%20outperforms%20all%20the%20published%20multimodal%20works%20to%20date.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/yinjunbo/IS-Fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15241v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IS-Fusion%3A%20Instance-Scene%20Collaborative%20Fusion%20for%20Multimodal%203D%20Object%0A%20%20Detection&entry.906535625=Junbo%20Yin%20and%20Jianbing%20Shen%20and%20Runnan%20Chen%20and%20Wei%20Li%20and%20Ruigang%20Yang%20and%20Pascal%20Frossard%20and%20Wenguan%20Wang&entry.1292438233=%20%20Bird%27s%20eye%20view%20%28BEV%29%20representation%20has%20emerged%20as%20a%20dominant%20solution%20for%0Adescribing%203D%20space%20in%20autonomous%20driving%20scenarios.%20However%2C%20objects%20in%20the%0ABEV%20representation%20typically%20exhibit%20small%20sizes%2C%20and%20the%20associated%20point%0Acloud%20context%20is%20inherently%20sparse%2C%20which%20leads%20to%20great%20challenges%20for%0Areliable%203D%20perception.%20In%20this%20paper%2C%20we%20propose%20IS-Fusion%2C%20an%20innovative%0Amultimodal%20fusion%20framework%20that%20jointly%20captures%20the%20Instance-%20and%20Scene-level%0Acontextual%20information.%20IS-Fusion%20essentially%20differs%20from%20existing%20approaches%0Athat%20only%20focus%20on%20the%20BEV%20scene-level%20fusion%20by%20explicitly%20incorporating%0Ainstance-level%20multimodal%20information%2C%20thus%20facilitating%20the%20instance-centric%0Atasks%20like%203D%20object%20detection.%20It%20comprises%20a%20Hierarchical%20Scene%20Fusion%20%28HSF%29%0Amodule%20and%20an%20Instance-Guided%20Fusion%20%28IGF%29%20module.%20HSF%20applies%20Point-to-Grid%0Aand%20Grid-to-Region%20transformers%20to%20capture%20the%20multimodal%20scene%20context%20at%0Adifferent%20granularities.%20IGF%20mines%20instance%20candidates%2C%20explores%20their%0Arelationships%2C%20and%20aggregates%20the%20local%20multimodal%20context%20for%20each%20instance.%0AThese%20instances%20then%20serve%20as%20guidance%20to%20enhance%20the%20scene%20feature%20and%20yield%0Aan%20instance-aware%20BEV%20representation.%20On%20the%20challenging%20nuScenes%20benchmark%2C%0AIS-Fusion%20outperforms%20all%20the%20published%20multimodal%20works%20to%20date.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/yinjunbo/IS-Fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15241v1&entry.124074799=Read"},
{"title": "Gradient-based Sampling for Class Imbalanced Semi-supervised Object\n  Detection", "author": "Jiaming Li and Xiangru Lin and Wei Zhang and Xiao Tan and Yingying Li and Junyu Han and Errui Ding and Jingdong Wang and Guanbin Li", "abstract": "  Current semi-supervised object detection (SSOD) algorithms typically assume\nclass balanced datasets (PASCAL VOC etc.) or slightly class imbalanced datasets\n(MS-COCO, etc). This assumption can be easily violated since real world\ndatasets can be extremely class imbalanced in nature, thus making the\nperformance of semi-supervised object detectors far from satisfactory. Besides,\nthe research for this problem in SSOD is severely under-explored. To bridge\nthis research gap, we comprehensively study the class imbalance problem for\nSSOD under more challenging scenarios, thus forming the first experimental\nsetting for class imbalanced SSOD (CI-SSOD). Moreover, we propose a simple yet\neffective gradient-based sampling framework that tackles the class imbalance\nproblem from the perspective of two types of confirmation biases. To tackle\nconfirmation bias towards majority classes, the gradient-based reweighting and\ngradient-based thresholding modules leverage the gradients from each class to\nfully balance the influence of the majority and minority classes. To tackle the\nconfirmation bias from incorrect pseudo labels of minority classes, the\nclass-rebalancing sampling module resamples unlabeled data following the\nguidance of the gradient-based reweighting module. Experiments on three\nproposed sub-tasks, namely MS-COCO, MS-COCO to Object365 and LVIS, suggest that\nour method outperforms current class imbalanced object detectors by clear\nmargins, serving as a baseline for future research in CI-SSOD. Code will be\navailable at https://github.com/nightkeepers/CI-SSOD.\n", "link": "http://arxiv.org/abs/2403.15127v1", "date": "2024-03-22", "relevancy": 2.1482, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5734}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5112}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5108}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gradient-based%20Sampling%20for%20Class%20Imbalanced%20Semi-supervised%20Object%0A%20%20Detection&body=Title%3A%20Gradient-based%20Sampling%20for%20Class%20Imbalanced%20Semi-supervised%20Object%0A%20%20Detection%0AAuthor%3A%20Jiaming%20Li%20and%20Xiangru%20Lin%20and%20Wei%20Zhang%20and%20Xiao%20Tan%20and%20Yingying%20Li%20and%20Junyu%20Han%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Guanbin%20Li%0AAbstract%3A%20%20%20Current%20semi-supervised%20object%20detection%20%28SSOD%29%20algorithms%20typically%20assume%0Aclass%20balanced%20datasets%20%28PASCAL%20VOC%20etc.%29%20or%20slightly%20class%20imbalanced%20datasets%0A%28MS-COCO%2C%20etc%29.%20This%20assumption%20can%20be%20easily%20violated%20since%20real%20world%0Adatasets%20can%20be%20extremely%20class%20imbalanced%20in%20nature%2C%20thus%20making%20the%0Aperformance%20of%20semi-supervised%20object%20detectors%20far%20from%20satisfactory.%20Besides%2C%0Athe%20research%20for%20this%20problem%20in%20SSOD%20is%20severely%20under-explored.%20To%20bridge%0Athis%20research%20gap%2C%20we%20comprehensively%20study%20the%20class%20imbalance%20problem%20for%0ASSOD%20under%20more%20challenging%20scenarios%2C%20thus%20forming%20the%20first%20experimental%0Asetting%20for%20class%20imbalanced%20SSOD%20%28CI-SSOD%29.%20Moreover%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20gradient-based%20sampling%20framework%20that%20tackles%20the%20class%20imbalance%0Aproblem%20from%20the%20perspective%20of%20two%20types%20of%20confirmation%20biases.%20To%20tackle%0Aconfirmation%20bias%20towards%20majority%20classes%2C%20the%20gradient-based%20reweighting%20and%0Agradient-based%20thresholding%20modules%20leverage%20the%20gradients%20from%20each%20class%20to%0Afully%20balance%20the%20influence%20of%20the%20majority%20and%20minority%20classes.%20To%20tackle%20the%0Aconfirmation%20bias%20from%20incorrect%20pseudo%20labels%20of%20minority%20classes%2C%20the%0Aclass-rebalancing%20sampling%20module%20resamples%20unlabeled%20data%20following%20the%0Aguidance%20of%20the%20gradient-based%20reweighting%20module.%20Experiments%20on%20three%0Aproposed%20sub-tasks%2C%20namely%20MS-COCO%2C%20MS-COCO%20to%20Object365%20and%20LVIS%2C%20suggest%20that%0Aour%20method%20outperforms%20current%20class%20imbalanced%20object%20detectors%20by%20clear%0Amargins%2C%20serving%20as%20a%20baseline%20for%20future%20research%20in%20CI-SSOD.%20Code%20will%20be%0Aavailable%20at%20https%3A//github.com/nightkeepers/CI-SSOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15127v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-based%20Sampling%20for%20Class%20Imbalanced%20Semi-supervised%20Object%0A%20%20Detection&entry.906535625=Jiaming%20Li%20and%20Xiangru%20Lin%20and%20Wei%20Zhang%20and%20Xiao%20Tan%20and%20Yingying%20Li%20and%20Junyu%20Han%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Guanbin%20Li&entry.1292438233=%20%20Current%20semi-supervised%20object%20detection%20%28SSOD%29%20algorithms%20typically%20assume%0Aclass%20balanced%20datasets%20%28PASCAL%20VOC%20etc.%29%20or%20slightly%20class%20imbalanced%20datasets%0A%28MS-COCO%2C%20etc%29.%20This%20assumption%20can%20be%20easily%20violated%20since%20real%20world%0Adatasets%20can%20be%20extremely%20class%20imbalanced%20in%20nature%2C%20thus%20making%20the%0Aperformance%20of%20semi-supervised%20object%20detectors%20far%20from%20satisfactory.%20Besides%2C%0Athe%20research%20for%20this%20problem%20in%20SSOD%20is%20severely%20under-explored.%20To%20bridge%0Athis%20research%20gap%2C%20we%20comprehensively%20study%20the%20class%20imbalance%20problem%20for%0ASSOD%20under%20more%20challenging%20scenarios%2C%20thus%20forming%20the%20first%20experimental%0Asetting%20for%20class%20imbalanced%20SSOD%20%28CI-SSOD%29.%20Moreover%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20gradient-based%20sampling%20framework%20that%20tackles%20the%20class%20imbalance%0Aproblem%20from%20the%20perspective%20of%20two%20types%20of%20confirmation%20biases.%20To%20tackle%0Aconfirmation%20bias%20towards%20majority%20classes%2C%20the%20gradient-based%20reweighting%20and%0Agradient-based%20thresholding%20modules%20leverage%20the%20gradients%20from%20each%20class%20to%0Afully%20balance%20the%20influence%20of%20the%20majority%20and%20minority%20classes.%20To%20tackle%20the%0Aconfirmation%20bias%20from%20incorrect%20pseudo%20labels%20of%20minority%20classes%2C%20the%0Aclass-rebalancing%20sampling%20module%20resamples%20unlabeled%20data%20following%20the%0Aguidance%20of%20the%20gradient-based%20reweighting%20module.%20Experiments%20on%20three%0Aproposed%20sub-tasks%2C%20namely%20MS-COCO%2C%20MS-COCO%20to%20Object365%20and%20LVIS%2C%20suggest%20that%0Aour%20method%20outperforms%20current%20class%20imbalanced%20object%20detectors%20by%20clear%0Amargins%2C%20serving%20as%20a%20baseline%20for%20future%20research%20in%20CI-SSOD.%20Code%20will%20be%0Aavailable%20at%20https%3A//github.com/nightkeepers/CI-SSOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15127v1&entry.124074799=Read"},
{"title": "UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction", "author": "Lan Feng and Mohammadhossein Bahari and Kaouther Messaoud Ben Amor and \u00c9loi Zablocki and Matthieu Cord and Alexandre Alahi", "abstract": "  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, \\textit{e.g.,} in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\n\\hyperlink{https://github.com/vita-epfl/UniTraj}{https://github.com/vita-epfl/UniTraj}.\n", "link": "http://arxiv.org/abs/2403.15098v1", "date": "2024-03-22", "relevancy": 2.1462, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5236}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UniTraj%3A%20A%20Unified%20Framework%20for%20Scalable%20Vehicle%20Trajectory%20Prediction&body=Title%3A%20UniTraj%3A%20A%20Unified%20Framework%20for%20Scalable%20Vehicle%20Trajectory%20Prediction%0AAuthor%3A%20Lan%20Feng%20and%20Mohammadhossein%20Bahari%20and%20Kaouther%20Messaoud%20Ben%20Amor%20and%20%C3%89loi%20Zablocki%20and%20Matthieu%20Cord%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Vehicle%20trajectory%20prediction%20has%20increasingly%20relied%20on%20data-driven%0Asolutions%2C%20but%20their%20ability%20to%20scale%20to%20different%20data%20domains%20and%20the%20impact%0Aof%20larger%20dataset%20sizes%20on%20their%20generalization%20remain%20under-explored.%20While%0Athese%20questions%20can%20be%20studied%20by%20employing%20multiple%20datasets%2C%20it%20is%0Achallenging%20due%20to%20several%20discrepancies%2C%20%5Ctextit%7Be.g.%2C%7D%20in%20data%20formats%2C%20map%0Aresolution%2C%20and%20semantic%20annotation%20types.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20UniTraj%2C%20a%20comprehensive%20framework%20that%20unifies%20various%20datasets%2C%0Amodels%2C%20and%20evaluation%20criteria%2C%20presenting%20new%20opportunities%20for%20the%20vehicle%0Atrajectory%20prediction%20field.%20In%20particular%2C%20using%20UniTraj%2C%20we%20conduct%20extensive%0Aexperiments%20and%20find%20that%20model%20performance%20significantly%20drops%20when%0Atransferred%20to%20other%20datasets.%20However%2C%20enlarging%20data%20size%20and%20diversity%20can%0Asubstantially%20improve%20performance%2C%20leading%20to%20a%20new%20state-of-the-art%20result%20for%0Athe%20nuScenes%20dataset.%20We%20provide%20insights%20into%20dataset%20characteristics%20to%0Aexplain%20these%20findings.%20The%20code%20can%20be%20found%20here%3A%0A%5Chyperlink%7Bhttps%3A//github.com/vita-epfl/UniTraj%7D%7Bhttps%3A//github.com/vita-epfl/UniTraj%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15098v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTraj%3A%20A%20Unified%20Framework%20for%20Scalable%20Vehicle%20Trajectory%20Prediction&entry.906535625=Lan%20Feng%20and%20Mohammadhossein%20Bahari%20and%20Kaouther%20Messaoud%20Ben%20Amor%20and%20%C3%89loi%20Zablocki%20and%20Matthieu%20Cord%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Vehicle%20trajectory%20prediction%20has%20increasingly%20relied%20on%20data-driven%0Asolutions%2C%20but%20their%20ability%20to%20scale%20to%20different%20data%20domains%20and%20the%20impact%0Aof%20larger%20dataset%20sizes%20on%20their%20generalization%20remain%20under-explored.%20While%0Athese%20questions%20can%20be%20studied%20by%20employing%20multiple%20datasets%2C%20it%20is%0Achallenging%20due%20to%20several%20discrepancies%2C%20%5Ctextit%7Be.g.%2C%7D%20in%20data%20formats%2C%20map%0Aresolution%2C%20and%20semantic%20annotation%20types.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20UniTraj%2C%20a%20comprehensive%20framework%20that%20unifies%20various%20datasets%2C%0Amodels%2C%20and%20evaluation%20criteria%2C%20presenting%20new%20opportunities%20for%20the%20vehicle%0Atrajectory%20prediction%20field.%20In%20particular%2C%20using%20UniTraj%2C%20we%20conduct%20extensive%0Aexperiments%20and%20find%20that%20model%20performance%20significantly%20drops%20when%0Atransferred%20to%20other%20datasets.%20However%2C%20enlarging%20data%20size%20and%20diversity%20can%0Asubstantially%20improve%20performance%2C%20leading%20to%20a%20new%20state-of-the-art%20result%20for%0Athe%20nuScenes%20dataset.%20We%20provide%20insights%20into%20dataset%20characteristics%20to%0Aexplain%20these%20findings.%20The%20code%20can%20be%20found%20here%3A%0A%5Chyperlink%7Bhttps%3A//github.com/vita-epfl/UniTraj%7D%7Bhttps%3A//github.com/vita-epfl/UniTraj%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15098v1&entry.124074799=Read"},
{"title": "LeGO: Leveraging a Surface Deformation Network for Animatable Stylized\n  Face Generation with One Example", "author": "Soyeon Yoon and Kwan Yun and Kwanggyoon Seo and Sihun Cha and Jung Eun Yoo and Junyong Noh", "abstract": "  Recent advances in 3D face stylization have made significant strides in few\nto zero-shot settings. However, the degree of stylization achieved by existing\nmethods is often not sufficient for practical applications because they are\nmostly based on statistical 3D Morphable Models (3DMM) with limited variations.\nTo this end, we propose a method that can produce a highly stylized 3D face\nmodel with desired topology. Our methods train a surface deformation network\nwith 3DMM and translate its domain to the target style using a paired exemplar.\nThe network achieves stylization of the 3D face mesh by mimicking the style of\nthe target using a differentiable renderer and directional CLIP losses.\nAdditionally, during the inference process, we utilize a Mesh Agnostic Encoder\n(MAGE) that takes deformation target, a mesh of diverse topologies as input to\nthe stylization process and encodes its shape into our latent space. The\nresulting stylized face model can be animated by commonly used 3DMM blend\nshapes. A set of quantitative and qualitative evaluations demonstrate that our\nmethod can produce highly stylized face meshes according to a given style and\noutput them in a desired topology. We also demonstrate example applications of\nour method including image-based stylized avatar generation, linear\ninterpolation of geometric styles, and facial animation of stylized avatars.\n", "link": "http://arxiv.org/abs/2403.15227v1", "date": "2024-03-22", "relevancy": 2.1436, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5405}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5328}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5321}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LeGO%3A%20Leveraging%20a%20Surface%20Deformation%20Network%20for%20Animatable%20Stylized%0A%20%20Face%20Generation%20with%20One%20Example&body=Title%3A%20LeGO%3A%20Leveraging%20a%20Surface%20Deformation%20Network%20for%20Animatable%20Stylized%0A%20%20Face%20Generation%20with%20One%20Example%0AAuthor%3A%20Soyeon%20Yoon%20and%20Kwan%20Yun%20and%20Kwanggyoon%20Seo%20and%20Sihun%20Cha%20and%20Jung%20Eun%20Yoo%20and%20Junyong%20Noh%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20face%20stylization%20have%20made%20significant%20strides%20in%20few%0Ato%20zero-shot%20settings.%20However%2C%20the%20degree%20of%20stylization%20achieved%20by%20existing%0Amethods%20is%20often%20not%20sufficient%20for%20practical%20applications%20because%20they%20are%0Amostly%20based%20on%20statistical%203D%20Morphable%20Models%20%283DMM%29%20with%20limited%20variations.%0ATo%20this%20end%2C%20we%20propose%20a%20method%20that%20can%20produce%20a%20highly%20stylized%203D%20face%0Amodel%20with%20desired%20topology.%20Our%20methods%20train%20a%20surface%20deformation%20network%0Awith%203DMM%20and%20translate%20its%20domain%20to%20the%20target%20style%20using%20a%20paired%20exemplar.%0AThe%20network%20achieves%20stylization%20of%20the%203D%20face%20mesh%20by%20mimicking%20the%20style%20of%0Athe%20target%20using%20a%20differentiable%20renderer%20and%20directional%20CLIP%20losses.%0AAdditionally%2C%20during%20the%20inference%20process%2C%20we%20utilize%20a%20Mesh%20Agnostic%20Encoder%0A%28MAGE%29%20that%20takes%20deformation%20target%2C%20a%20mesh%20of%20diverse%20topologies%20as%20input%20to%0Athe%20stylization%20process%20and%20encodes%20its%20shape%20into%20our%20latent%20space.%20The%0Aresulting%20stylized%20face%20model%20can%20be%20animated%20by%20commonly%20used%203DMM%20blend%0Ashapes.%20A%20set%20of%20quantitative%20and%20qualitative%20evaluations%20demonstrate%20that%20our%0Amethod%20can%20produce%20highly%20stylized%20face%20meshes%20according%20to%20a%20given%20style%20and%0Aoutput%20them%20in%20a%20desired%20topology.%20We%20also%20demonstrate%20example%20applications%20of%0Aour%20method%20including%20image-based%20stylized%20avatar%20generation%2C%20linear%0Ainterpolation%20of%20geometric%20styles%2C%20and%20facial%20animation%20of%20stylized%20avatars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15227v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeGO%3A%20Leveraging%20a%20Surface%20Deformation%20Network%20for%20Animatable%20Stylized%0A%20%20Face%20Generation%20with%20One%20Example&entry.906535625=Soyeon%20Yoon%20and%20Kwan%20Yun%20and%20Kwanggyoon%20Seo%20and%20Sihun%20Cha%20and%20Jung%20Eun%20Yoo%20and%20Junyong%20Noh&entry.1292438233=%20%20Recent%20advances%20in%203D%20face%20stylization%20have%20made%20significant%20strides%20in%20few%0Ato%20zero-shot%20settings.%20However%2C%20the%20degree%20of%20stylization%20achieved%20by%20existing%0Amethods%20is%20often%20not%20sufficient%20for%20practical%20applications%20because%20they%20are%0Amostly%20based%20on%20statistical%203D%20Morphable%20Models%20%283DMM%29%20with%20limited%20variations.%0ATo%20this%20end%2C%20we%20propose%20a%20method%20that%20can%20produce%20a%20highly%20stylized%203D%20face%0Amodel%20with%20desired%20topology.%20Our%20methods%20train%20a%20surface%20deformation%20network%0Awith%203DMM%20and%20translate%20its%20domain%20to%20the%20target%20style%20using%20a%20paired%20exemplar.%0AThe%20network%20achieves%20stylization%20of%20the%203D%20face%20mesh%20by%20mimicking%20the%20style%20of%0Athe%20target%20using%20a%20differentiable%20renderer%20and%20directional%20CLIP%20losses.%0AAdditionally%2C%20during%20the%20inference%20process%2C%20we%20utilize%20a%20Mesh%20Agnostic%20Encoder%0A%28MAGE%29%20that%20takes%20deformation%20target%2C%20a%20mesh%20of%20diverse%20topologies%20as%20input%20to%0Athe%20stylization%20process%20and%20encodes%20its%20shape%20into%20our%20latent%20space.%20The%0Aresulting%20stylized%20face%20model%20can%20be%20animated%20by%20commonly%20used%203DMM%20blend%0Ashapes.%20A%20set%20of%20quantitative%20and%20qualitative%20evaluations%20demonstrate%20that%20our%0Amethod%20can%20produce%20highly%20stylized%20face%20meshes%20according%20to%20a%20given%20style%20and%0Aoutput%20them%20in%20a%20desired%20topology.%20We%20also%20demonstrate%20example%20applications%20of%0Aour%20method%20including%20image-based%20stylized%20avatar%20generation%2C%20linear%0Ainterpolation%20of%20geometric%20styles%2C%20and%20facial%20animation%20of%20stylized%20avatars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15227v1&entry.124074799=Read"},
{"title": "Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment\n  Anything Model for Crowd-Sourcing Medical Image Annotations", "author": "Pranav Kulkarni and Adway Kanhere and Dharmam Savani and Andrew Chan and Devina Chatterjee and Paul H. Yi and Vishwa S. Parekh", "abstract": "  Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Model (SAM) have revolutionized\nsemantic segmentation with exceptional zero-shot generalizability across\nvarious domains, including medical imaging, and hold a lot of promise for\nstreamlining the annotation process. However, SAM has yet to be evaluated in a\ncrowd-sourced setting to curate annotations for training 3D DL segmentation\nmodels. In this work, we explore the potential of SAM for crowd-sourcing\n\"sparse\" annotations from non-experts to generate \"dense\" segmentation masks\nfor training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our\nresults indicate that while SAM-generated annotations exhibit high mean Dice\nscores compared to ground-truth annotations, nnU-Net models trained on\nSAM-generated annotations perform significantly worse than nnU-Net models\ntrained on ground-truth annotations ($p<0.001$, all).\n", "link": "http://arxiv.org/abs/2403.15218v1", "date": "2024-03-22", "relevancy": 2.1383, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.606}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5143}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Anytime%2C%20Anywhere%2C%20Anyone%3A%20Investigating%20the%20Feasibility%20of%20Segment%0A%20%20Anything%20Model%20for%20Crowd-Sourcing%20Medical%20Image%20Annotations&body=Title%3A%20Anytime%2C%20Anywhere%2C%20Anyone%3A%20Investigating%20the%20Feasibility%20of%20Segment%0A%20%20Anything%20Model%20for%20Crowd-Sourcing%20Medical%20Image%20Annotations%0AAuthor%3A%20Pranav%20Kulkarni%20and%20Adway%20Kanhere%20and%20Dharmam%20Savani%20and%20Andrew%20Chan%20and%20Devina%20Chatterjee%20and%20Paul%20H.%20Yi%20and%20Vishwa%20S.%20Parekh%0AAbstract%3A%20%20%20Curating%20annotations%20for%20medical%20image%20segmentation%20is%20a%20labor-intensive%20and%0Atime-consuming%20task%20that%20requires%20domain%20expertise%2C%20resulting%20in%20%22narrowly%22%0Afocused%20deep%20learning%20%28DL%29%20models%20with%20limited%20translational%20utility.%20Recently%2C%0Afoundation%20models%20like%20the%20Segment%20Anything%20Model%20%28SAM%29%20have%20revolutionized%0Asemantic%20segmentation%20with%20exceptional%20zero-shot%20generalizability%20across%0Avarious%20domains%2C%20including%20medical%20imaging%2C%20and%20hold%20a%20lot%20of%20promise%20for%0Astreamlining%20the%20annotation%20process.%20However%2C%20SAM%20has%20yet%20to%20be%20evaluated%20in%20a%0Acrowd-sourced%20setting%20to%20curate%20annotations%20for%20training%203D%20DL%20segmentation%0Amodels.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20SAM%20for%20crowd-sourcing%0A%22sparse%22%20annotations%20from%20non-experts%20to%20generate%20%22dense%22%20segmentation%20masks%0Afor%20training%203D%20nnU-Net%20models%2C%20a%20state-of-the-art%20DL%20segmentation%20model.%20Our%0Aresults%20indicate%20that%20while%20SAM-generated%20annotations%20exhibit%20high%20mean%20Dice%0Ascores%20compared%20to%20ground-truth%20annotations%2C%20nnU-Net%20models%20trained%20on%0ASAM-generated%20annotations%20perform%20significantly%20worse%20than%20nnU-Net%20models%0Atrained%20on%20ground-truth%20annotations%20%28%24p%3C0.001%24%2C%20all%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15218v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anytime%2C%20Anywhere%2C%20Anyone%3A%20Investigating%20the%20Feasibility%20of%20Segment%0A%20%20Anything%20Model%20for%20Crowd-Sourcing%20Medical%20Image%20Annotations&entry.906535625=Pranav%20Kulkarni%20and%20Adway%20Kanhere%20and%20Dharmam%20Savani%20and%20Andrew%20Chan%20and%20Devina%20Chatterjee%20and%20Paul%20H.%20Yi%20and%20Vishwa%20S.%20Parekh&entry.1292438233=%20%20Curating%20annotations%20for%20medical%20image%20segmentation%20is%20a%20labor-intensive%20and%0Atime-consuming%20task%20that%20requires%20domain%20expertise%2C%20resulting%20in%20%22narrowly%22%0Afocused%20deep%20learning%20%28DL%29%20models%20with%20limited%20translational%20utility.%20Recently%2C%0Afoundation%20models%20like%20the%20Segment%20Anything%20Model%20%28SAM%29%20have%20revolutionized%0Asemantic%20segmentation%20with%20exceptional%20zero-shot%20generalizability%20across%0Avarious%20domains%2C%20including%20medical%20imaging%2C%20and%20hold%20a%20lot%20of%20promise%20for%0Astreamlining%20the%20annotation%20process.%20However%2C%20SAM%20has%20yet%20to%20be%20evaluated%20in%20a%0Acrowd-sourced%20setting%20to%20curate%20annotations%20for%20training%203D%20DL%20segmentation%0Amodels.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20SAM%20for%20crowd-sourcing%0A%22sparse%22%20annotations%20from%20non-experts%20to%20generate%20%22dense%22%20segmentation%20masks%0Afor%20training%203D%20nnU-Net%20models%2C%20a%20state-of-the-art%20DL%20segmentation%20model.%20Our%0Aresults%20indicate%20that%20while%20SAM-generated%20annotations%20exhibit%20high%20mean%20Dice%0Ascores%20compared%20to%20ground-truth%20annotations%2C%20nnU-Net%20models%20trained%20on%0ASAM-generated%20annotations%20perform%20significantly%20worse%20than%20nnU-Net%20models%0Atrained%20on%20ground-truth%20annotations%20%28%24p%3C0.001%24%2C%20all%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15218v1&entry.124074799=Read"},
{"title": "Inducing High Energy-Latency of Large Vision-Language Models with\n  Verbose Images", "author": "Kuofeng Gao and Yang Bai and Jindong Gu and Shu-Tao Xia and Philip Torr and Zhifeng Li and Wei Liu", "abstract": "  Large vision-language models (VLMs) such as GPT-4 have achieved exceptional\nperformance across various multi-modal tasks. However, the deployment of VLMs\nnecessitates substantial energy consumption and computational resources. Once\nattackers maliciously induce high energy consumption and latency time\n(energy-latency cost) during inference of VLMs, it will exhaust computational\nresources. In this paper, we explore this attack surface about availability of\nVLMs and aim to induce high energy-latency cost during inference of VLMs. We\nfind that high energy-latency cost during inference of VLMs can be manipulated\nby maximizing the length of generated sequences. To this end, we propose\nverbose images, with the goal of crafting an imperceptible perturbation to\ninduce VLMs to generate long sentences during inference. Concretely, we design\nthree loss objectives. First, a loss is proposed to delay the occurrence of\nend-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop\ngenerating further tokens. Moreover, an uncertainty loss and a token diversity\nloss are proposed to increase the uncertainty over each generated token and the\ndiversity among all tokens of the whole generated sequence, respectively, which\ncan break output dependency at token-level and sequence-level. Furthermore, a\ntemporal weight adjustment algorithm is proposed, which can effectively balance\nthese losses. Extensive experiments demonstrate that our verbose images can\nincrease the length of generated sequences by 7.87 times and 8.56 times\ncompared to original images on MS-COCO and ImageNet datasets, which presents\npotential challenges for various applications. Our code is available at\nhttps://github.com/KuofengGao/Verbose_Images.\n", "link": "http://arxiv.org/abs/2401.11170v2", "date": "2024-03-22", "relevancy": 2.1381, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5445}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5287}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5241}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Inducing%20High%20Energy-Latency%20of%20Large%20Vision-Language%20Models%20with%0A%20%20Verbose%20Images&body=Title%3A%20Inducing%20High%20Energy-Latency%20of%20Large%20Vision-Language%20Models%20with%0A%20%20Verbose%20Images%0AAuthor%3A%20Kuofeng%20Gao%20and%20Yang%20Bai%20and%20Jindong%20Gu%20and%20Shu-Tao%20Xia%20and%20Philip%20Torr%20and%20Zhifeng%20Li%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28VLMs%29%20such%20as%20GPT-4%20have%20achieved%20exceptional%0Aperformance%20across%20various%20multi-modal%20tasks.%20However%2C%20the%20deployment%20of%20VLMs%0Anecessitates%20substantial%20energy%20consumption%20and%20computational%20resources.%20Once%0Aattackers%20maliciously%20induce%20high%20energy%20consumption%20and%20latency%20time%0A%28energy-latency%20cost%29%20during%20inference%20of%20VLMs%2C%20it%20will%20exhaust%20computational%0Aresources.%20In%20this%20paper%2C%20we%20explore%20this%20attack%20surface%20about%20availability%20of%0AVLMs%20and%20aim%20to%20induce%20high%20energy-latency%20cost%20during%20inference%20of%20VLMs.%20We%0Afind%20that%20high%20energy-latency%20cost%20during%20inference%20of%20VLMs%20can%20be%20manipulated%0Aby%20maximizing%20the%20length%20of%20generated%20sequences.%20To%20this%20end%2C%20we%20propose%0Averbose%20images%2C%20with%20the%20goal%20of%20crafting%20an%20imperceptible%20perturbation%20to%0Ainduce%20VLMs%20to%20generate%20long%20sentences%20during%20inference.%20Concretely%2C%20we%20design%0Athree%20loss%20objectives.%20First%2C%20a%20loss%20is%20proposed%20to%20delay%20the%20occurrence%20of%0Aend-of-sequence%20%28EOS%29%20token%2C%20where%20EOS%20token%20is%20a%20signal%20for%20VLMs%20to%20stop%0Agenerating%20further%20tokens.%20Moreover%2C%20an%20uncertainty%20loss%20and%20a%20token%20diversity%0Aloss%20are%20proposed%20to%20increase%20the%20uncertainty%20over%20each%20generated%20token%20and%20the%0Adiversity%20among%20all%20tokens%20of%20the%20whole%20generated%20sequence%2C%20respectively%2C%20which%0Acan%20break%20output%20dependency%20at%20token-level%20and%20sequence-level.%20Furthermore%2C%20a%0Atemporal%20weight%20adjustment%20algorithm%20is%20proposed%2C%20which%20can%20effectively%20balance%0Athese%20losses.%20Extensive%20experiments%20demonstrate%20that%20our%20verbose%20images%20can%0Aincrease%20the%20length%20of%20generated%20sequences%20by%207.87%20times%20and%208.56%20times%0Acompared%20to%20original%20images%20on%20MS-COCO%20and%20ImageNet%20datasets%2C%20which%20presents%0Apotential%20challenges%20for%20various%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/KuofengGao/Verbose_Images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11170v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inducing%20High%20Energy-Latency%20of%20Large%20Vision-Language%20Models%20with%0A%20%20Verbose%20Images&entry.906535625=Kuofeng%20Gao%20and%20Yang%20Bai%20and%20Jindong%20Gu%20and%20Shu-Tao%20Xia%20and%20Philip%20Torr%20and%20Zhifeng%20Li%20and%20Wei%20Liu&entry.1292438233=%20%20Large%20vision-language%20models%20%28VLMs%29%20such%20as%20GPT-4%20have%20achieved%20exceptional%0Aperformance%20across%20various%20multi-modal%20tasks.%20However%2C%20the%20deployment%20of%20VLMs%0Anecessitates%20substantial%20energy%20consumption%20and%20computational%20resources.%20Once%0Aattackers%20maliciously%20induce%20high%20energy%20consumption%20and%20latency%20time%0A%28energy-latency%20cost%29%20during%20inference%20of%20VLMs%2C%20it%20will%20exhaust%20computational%0Aresources.%20In%20this%20paper%2C%20we%20explore%20this%20attack%20surface%20about%20availability%20of%0AVLMs%20and%20aim%20to%20induce%20high%20energy-latency%20cost%20during%20inference%20of%20VLMs.%20We%0Afind%20that%20high%20energy-latency%20cost%20during%20inference%20of%20VLMs%20can%20be%20manipulated%0Aby%20maximizing%20the%20length%20of%20generated%20sequences.%20To%20this%20end%2C%20we%20propose%0Averbose%20images%2C%20with%20the%20goal%20of%20crafting%20an%20imperceptible%20perturbation%20to%0Ainduce%20VLMs%20to%20generate%20long%20sentences%20during%20inference.%20Concretely%2C%20we%20design%0Athree%20loss%20objectives.%20First%2C%20a%20loss%20is%20proposed%20to%20delay%20the%20occurrence%20of%0Aend-of-sequence%20%28EOS%29%20token%2C%20where%20EOS%20token%20is%20a%20signal%20for%20VLMs%20to%20stop%0Agenerating%20further%20tokens.%20Moreover%2C%20an%20uncertainty%20loss%20and%20a%20token%20diversity%0Aloss%20are%20proposed%20to%20increase%20the%20uncertainty%20over%20each%20generated%20token%20and%20the%0Adiversity%20among%20all%20tokens%20of%20the%20whole%20generated%20sequence%2C%20respectively%2C%20which%0Acan%20break%20output%20dependency%20at%20token-level%20and%20sequence-level.%20Furthermore%2C%20a%0Atemporal%20weight%20adjustment%20algorithm%20is%20proposed%2C%20which%20can%20effectively%20balance%0Athese%20losses.%20Extensive%20experiments%20demonstrate%20that%20our%20verbose%20images%20can%0Aincrease%20the%20length%20of%20generated%20sequences%20by%207.87%20times%20and%208.56%20times%0Acompared%20to%20original%20images%20on%20MS-COCO%20and%20ImageNet%20datasets%2C%20which%20presents%0Apotential%20challenges%20for%20various%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/KuofengGao/Verbose_Images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11170v2&entry.124074799=Read"},
{"title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference", "author": "Han Zhao and Min Zhang and Wei Zhao and Pengxiang Ding and Siteng Huang and Donglin Wang", "abstract": "  In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due\nto Cobra's linear sequential modeling. (2) Interestingly, the results of\nclosed-set challenging prediction benchmarks show that Cobra performs well in\novercoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.\n", "link": "http://arxiv.org/abs/2403.14520v2", "date": "2024-03-22", "relevancy": 2.1089, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5062}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference&body=Title%3A%20Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference%0AAuthor%3A%20Han%20Zhao%20and%20Min%20Zhang%20and%20Wei%20Zhao%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20application%20of%20multimodal%20large%20language%20models%20%28MLLM%29%0Ain%20various%20fields%20has%20achieved%20remarkable%20success.%20However%2C%20as%20the%20foundation%0Amodel%20for%20many%20downstream%20tasks%2C%20current%20MLLMs%20are%20composed%20of%20the%20well-known%0ATransformer%20network%2C%20which%20has%20a%20less%20efficient%20quadratic%20computation%0Acomplexity.%20To%20improve%20the%20efficiency%20of%20such%20basic%20models%2C%20we%20propose%20Cobra%2C%20a%0Alinear%20computational%20complexity%20MLLM.%20Specifically%2C%20Cobra%20integrates%20the%0Aefficient%20Mamba%20language%20model%20into%20the%20visual%20modality.%20Moreover%2C%20we%20explore%0Aand%20study%20various%20modal%20fusion%20schemes%20to%20create%20an%20effective%20multi-modal%0AMamba.%20Extensive%20experiments%20demonstrate%20that%20%281%29%20Cobra%20achieves%20extremely%0Acompetitive%20performance%20with%20current%20computationally%20efficient%20state-of-the-art%0Amethods%2C%20e.g.%2C%20LLaVA-Phi%2C%20TinyLLaVA%2C%20and%20MobileVLM%20v2%2C%20and%20has%20faster%20speed%20due%0Ato%20Cobra%27s%20linear%20sequential%20modeling.%20%282%29%20Interestingly%2C%20the%20results%20of%0Aclosed-set%20challenging%20prediction%20benchmarks%20show%20that%20Cobra%20performs%20well%20in%0Aovercoming%20visual%20illusions%20and%20spatial%20relationship%20judgments.%20%283%29%20Notably%2C%0ACobra%20even%20achieves%20comparable%20performance%20to%20LLaVA%20with%20about%2043%25%20of%20the%0Anumber%20of%20parameters.%20We%20will%20make%20all%20codes%20of%20Cobra%20open-source%20and%20hope%20that%0Athe%20proposed%20method%20can%20facilitate%20future%20research%20on%20complexity%20problems%20in%0AMLLM.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//sites.google.com/view/cobravlm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14520v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cobra%3A%20Extending%20Mamba%20to%20Multi-Modal%20Large%20Language%20Model%20for%20Efficient%0A%20%20Inference&entry.906535625=Han%20Zhao%20and%20Min%20Zhang%20and%20Wei%20Zhao%20and%20Pengxiang%20Ding%20and%20Siteng%20Huang%20and%20Donglin%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20application%20of%20multimodal%20large%20language%20models%20%28MLLM%29%0Ain%20various%20fields%20has%20achieved%20remarkable%20success.%20However%2C%20as%20the%20foundation%0Amodel%20for%20many%20downstream%20tasks%2C%20current%20MLLMs%20are%20composed%20of%20the%20well-known%0ATransformer%20network%2C%20which%20has%20a%20less%20efficient%20quadratic%20computation%0Acomplexity.%20To%20improve%20the%20efficiency%20of%20such%20basic%20models%2C%20we%20propose%20Cobra%2C%20a%0Alinear%20computational%20complexity%20MLLM.%20Specifically%2C%20Cobra%20integrates%20the%0Aefficient%20Mamba%20language%20model%20into%20the%20visual%20modality.%20Moreover%2C%20we%20explore%0Aand%20study%20various%20modal%20fusion%20schemes%20to%20create%20an%20effective%20multi-modal%0AMamba.%20Extensive%20experiments%20demonstrate%20that%20%281%29%20Cobra%20achieves%20extremely%0Acompetitive%20performance%20with%20current%20computationally%20efficient%20state-of-the-art%0Amethods%2C%20e.g.%2C%20LLaVA-Phi%2C%20TinyLLaVA%2C%20and%20MobileVLM%20v2%2C%20and%20has%20faster%20speed%20due%0Ato%20Cobra%27s%20linear%20sequential%20modeling.%20%282%29%20Interestingly%2C%20the%20results%20of%0Aclosed-set%20challenging%20prediction%20benchmarks%20show%20that%20Cobra%20performs%20well%20in%0Aovercoming%20visual%20illusions%20and%20spatial%20relationship%20judgments.%20%283%29%20Notably%2C%0ACobra%20even%20achieves%20comparable%20performance%20to%20LLaVA%20with%20about%2043%25%20of%20the%0Anumber%20of%20parameters.%20We%20will%20make%20all%20codes%20of%20Cobra%20open-source%20and%20hope%20that%0Athe%20proposed%20method%20can%20facilitate%20future%20research%20on%20complexity%20problems%20in%0AMLLM.%20Our%20project%20page%20is%20available%20at%3A%20https%3A//sites.google.com/view/cobravlm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14520v2&entry.124074799=Read"},
{"title": "GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition", "author": "Lei Jiang and Weixin Yang and Xin Zhang and Hao Ni", "abstract": "  Skeleton-based action recognition (SAR) in videos is an important but\nchallenging task in computer vision. The recent state-of-the-art models for SAR\nare primarily based on graph convolutional neural networks (GCNs), which are\npowerful in extracting the spatial information of skeleton data. However, it is\nyet clear that such GCN-based models can effectively capture the temporal\ndynamics of human action sequences. To this end, we propose the DevLSTM module,\nwhich exploits the path development -- a principled and parsimonious\nrepresentation for sequential data by leveraging the Lie group structure. The\npath development, originated from Rough path theory, can effectively capture\nthe order of events in high-dimensional stream data with massive dimension\nreduction and consequently enhance the LSTM module substantially. Our proposed\nG-DevLSTM module can be conveniently plugged into the temporal graph,\ncomplementing existing advanced GCN-based models. Our empirical studies on the\nNTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybrid\nmodel significantly outperforms the current best-performing methods in SAR\ntasks. The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM.\n", "link": "http://arxiv.org/abs/2403.15212v1", "date": "2024-03-22", "relevancy": 2.1071, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5528}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5089}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5079}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GCN-DevLSTM%3A%20Path%20Development%20for%20Skeleton-Based%20Action%20Recognition&body=Title%3A%20GCN-DevLSTM%3A%20Path%20Development%20for%20Skeleton-Based%20Action%20Recognition%0AAuthor%3A%20Lei%20Jiang%20and%20Weixin%20Yang%20and%20Xin%20Zhang%20and%20Hao%20Ni%0AAbstract%3A%20%20%20Skeleton-based%20action%20recognition%20%28SAR%29%20in%20videos%20is%20an%20important%20but%0Achallenging%20task%20in%20computer%20vision.%20The%20recent%20state-of-the-art%20models%20for%20SAR%0Aare%20primarily%20based%20on%20graph%20convolutional%20neural%20networks%20%28GCNs%29%2C%20which%20are%0Apowerful%20in%20extracting%20the%20spatial%20information%20of%20skeleton%20data.%20However%2C%20it%20is%0Ayet%20clear%20that%20such%20GCN-based%20models%20can%20effectively%20capture%20the%20temporal%0Adynamics%20of%20human%20action%20sequences.%20To%20this%20end%2C%20we%20propose%20the%20DevLSTM%20module%2C%0Awhich%20exploits%20the%20path%20development%20--%20a%20principled%20and%20parsimonious%0Arepresentation%20for%20sequential%20data%20by%20leveraging%20the%20Lie%20group%20structure.%20The%0Apath%20development%2C%20originated%20from%20Rough%20path%20theory%2C%20can%20effectively%20capture%0Athe%20order%20of%20events%20in%20high-dimensional%20stream%20data%20with%20massive%20dimension%0Areduction%20and%20consequently%20enhance%20the%20LSTM%20module%20substantially.%20Our%20proposed%0AG-DevLSTM%20module%20can%20be%20conveniently%20plugged%20into%20the%20temporal%20graph%2C%0Acomplementing%20existing%20advanced%20GCN-based%20models.%20Our%20empirical%20studies%20on%20the%0ANTU60%2C%20NTU120%20and%20Chalearn2013%20datasets%20demonstrate%20that%20our%20proposed%20hybrid%0Amodel%20significantly%20outperforms%20the%20current%20best-performing%20methods%20in%20SAR%0Atasks.%20The%20code%20is%20available%20at%20https%3A//github.com/DeepIntoStreams/GCN-DevLSTM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15212v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCN-DevLSTM%3A%20Path%20Development%20for%20Skeleton-Based%20Action%20Recognition&entry.906535625=Lei%20Jiang%20and%20Weixin%20Yang%20and%20Xin%20Zhang%20and%20Hao%20Ni&entry.1292438233=%20%20Skeleton-based%20action%20recognition%20%28SAR%29%20in%20videos%20is%20an%20important%20but%0Achallenging%20task%20in%20computer%20vision.%20The%20recent%20state-of-the-art%20models%20for%20SAR%0Aare%20primarily%20based%20on%20graph%20convolutional%20neural%20networks%20%28GCNs%29%2C%20which%20are%0Apowerful%20in%20extracting%20the%20spatial%20information%20of%20skeleton%20data.%20However%2C%20it%20is%0Ayet%20clear%20that%20such%20GCN-based%20models%20can%20effectively%20capture%20the%20temporal%0Adynamics%20of%20human%20action%20sequences.%20To%20this%20end%2C%20we%20propose%20the%20DevLSTM%20module%2C%0Awhich%20exploits%20the%20path%20development%20--%20a%20principled%20and%20parsimonious%0Arepresentation%20for%20sequential%20data%20by%20leveraging%20the%20Lie%20group%20structure.%20The%0Apath%20development%2C%20originated%20from%20Rough%20path%20theory%2C%20can%20effectively%20capture%0Athe%20order%20of%20events%20in%20high-dimensional%20stream%20data%20with%20massive%20dimension%0Areduction%20and%20consequently%20enhance%20the%20LSTM%20module%20substantially.%20Our%20proposed%0AG-DevLSTM%20module%20can%20be%20conveniently%20plugged%20into%20the%20temporal%20graph%2C%0Acomplementing%20existing%20advanced%20GCN-based%20models.%20Our%20empirical%20studies%20on%20the%0ANTU60%2C%20NTU120%20and%20Chalearn2013%20datasets%20demonstrate%20that%20our%20proposed%20hybrid%0Amodel%20significantly%20outperforms%20the%20current%20best-performing%20methods%20in%20SAR%0Atasks.%20The%20code%20is%20available%20at%20https%3A//github.com/DeepIntoStreams/GCN-DevLSTM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15212v1&entry.124074799=Read"},
{"title": "Augmented Reality based Simulated Data (ARSim) with multi-view\n  consistency for AV perception networks", "author": "Aqeel Anwar and Tae Eun Choe and Zian Wang and Sanja Fidler and Minwoo Park", "abstract": "  Detecting a diverse range of objects under various driving scenarios is\nessential for the effectiveness of autonomous driving systems. However, the\nreal-world data collected often lacks the necessary diversity presenting a\nlong-tail distribution. Although synthetic data has been utilized to overcome\nthis issue by generating virtual scenes, it faces hurdles such as a significant\ndomain gap and the substantial efforts required from 3D artists to create\nrealistic environments. To overcome these challenges, we present ARSim, a fully\nautomated, comprehensive, modular framework designed to enhance real multi-view\nimage data with 3D synthetic objects of interest. The proposed method\nintegrates domain adaptation and randomization strategies to address covariate\nshift between real and simulated data by inferring essential domain attributes\nfrom real data and employing simulation-based randomization for other\nattributes. We construct a simplified virtual scene using real data and\nstrategically place 3D synthetic assets within it. Illumination is achieved by\nestimating light distribution from multiple images capturing the surroundings\nof the vehicle. Camera parameters from real data are employed to render\nsynthetic assets in each frame. The resulting augmented multi-view consistent\ndataset is used to train a multi-camera perception network for autonomous\nvehicles. Experimental results on various AV perception tasks demonstrate the\nsuperior performance of networks trained on the augmented dataset.\n", "link": "http://arxiv.org/abs/2403.15370v1", "date": "2024-03-22", "relevancy": 2.1023, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5391}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Augmented%20Reality%20based%20Simulated%20Data%20%28ARSim%29%20with%20multi-view%0A%20%20consistency%20for%20AV%20perception%20networks&body=Title%3A%20Augmented%20Reality%20based%20Simulated%20Data%20%28ARSim%29%20with%20multi-view%0A%20%20consistency%20for%20AV%20perception%20networks%0AAuthor%3A%20Aqeel%20Anwar%20and%20Tae%20Eun%20Choe%20and%20Zian%20Wang%20and%20Sanja%20Fidler%20and%20Minwoo%20Park%0AAbstract%3A%20%20%20Detecting%20a%20diverse%20range%20of%20objects%20under%20various%20driving%20scenarios%20is%0Aessential%20for%20the%20effectiveness%20of%20autonomous%20driving%20systems.%20However%2C%20the%0Areal-world%20data%20collected%20often%20lacks%20the%20necessary%20diversity%20presenting%20a%0Along-tail%20distribution.%20Although%20synthetic%20data%20has%20been%20utilized%20to%20overcome%0Athis%20issue%20by%20generating%20virtual%20scenes%2C%20it%20faces%20hurdles%20such%20as%20a%20significant%0Adomain%20gap%20and%20the%20substantial%20efforts%20required%20from%203D%20artists%20to%20create%0Arealistic%20environments.%20To%20overcome%20these%20challenges%2C%20we%20present%20ARSim%2C%20a%20fully%0Aautomated%2C%20comprehensive%2C%20modular%20framework%20designed%20to%20enhance%20real%20multi-view%0Aimage%20data%20with%203D%20synthetic%20objects%20of%20interest.%20The%20proposed%20method%0Aintegrates%20domain%20adaptation%20and%20randomization%20strategies%20to%20address%20covariate%0Ashift%20between%20real%20and%20simulated%20data%20by%20inferring%20essential%20domain%20attributes%0Afrom%20real%20data%20and%20employing%20simulation-based%20randomization%20for%20other%0Aattributes.%20We%20construct%20a%20simplified%20virtual%20scene%20using%20real%20data%20and%0Astrategically%20place%203D%20synthetic%20assets%20within%20it.%20Illumination%20is%20achieved%20by%0Aestimating%20light%20distribution%20from%20multiple%20images%20capturing%20the%20surroundings%0Aof%20the%20vehicle.%20Camera%20parameters%20from%20real%20data%20are%20employed%20to%20render%0Asynthetic%20assets%20in%20each%20frame.%20The%20resulting%20augmented%20multi-view%20consistent%0Adataset%20is%20used%20to%20train%20a%20multi-camera%20perception%20network%20for%20autonomous%0Avehicles.%20Experimental%20results%20on%20various%20AV%20perception%20tasks%20demonstrate%20the%0Asuperior%20performance%20of%20networks%20trained%20on%20the%20augmented%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15370v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmented%20Reality%20based%20Simulated%20Data%20%28ARSim%29%20with%20multi-view%0A%20%20consistency%20for%20AV%20perception%20networks&entry.906535625=Aqeel%20Anwar%20and%20Tae%20Eun%20Choe%20and%20Zian%20Wang%20and%20Sanja%20Fidler%20and%20Minwoo%20Park&entry.1292438233=%20%20Detecting%20a%20diverse%20range%20of%20objects%20under%20various%20driving%20scenarios%20is%0Aessential%20for%20the%20effectiveness%20of%20autonomous%20driving%20systems.%20However%2C%20the%0Areal-world%20data%20collected%20often%20lacks%20the%20necessary%20diversity%20presenting%20a%0Along-tail%20distribution.%20Although%20synthetic%20data%20has%20been%20utilized%20to%20overcome%0Athis%20issue%20by%20generating%20virtual%20scenes%2C%20it%20faces%20hurdles%20such%20as%20a%20significant%0Adomain%20gap%20and%20the%20substantial%20efforts%20required%20from%203D%20artists%20to%20create%0Arealistic%20environments.%20To%20overcome%20these%20challenges%2C%20we%20present%20ARSim%2C%20a%20fully%0Aautomated%2C%20comprehensive%2C%20modular%20framework%20designed%20to%20enhance%20real%20multi-view%0Aimage%20data%20with%203D%20synthetic%20objects%20of%20interest.%20The%20proposed%20method%0Aintegrates%20domain%20adaptation%20and%20randomization%20strategies%20to%20address%20covariate%0Ashift%20between%20real%20and%20simulated%20data%20by%20inferring%20essential%20domain%20attributes%0Afrom%20real%20data%20and%20employing%20simulation-based%20randomization%20for%20other%0Aattributes.%20We%20construct%20a%20simplified%20virtual%20scene%20using%20real%20data%20and%0Astrategically%20place%203D%20synthetic%20assets%20within%20it.%20Illumination%20is%20achieved%20by%0Aestimating%20light%20distribution%20from%20multiple%20images%20capturing%20the%20surroundings%0Aof%20the%20vehicle.%20Camera%20parameters%20from%20real%20data%20are%20employed%20to%20render%0Asynthetic%20assets%20in%20each%20frame.%20The%20resulting%20augmented%20multi-view%20consistent%0Adataset%20is%20used%20to%20train%20a%20multi-camera%20perception%20network%20for%20autonomous%0Avehicles.%20Experimental%20results%20on%20various%20AV%20perception%20tasks%20demonstrate%20the%0Asuperior%20performance%20of%20networks%20trained%20on%20the%20augmented%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15370v1&entry.124074799=Read"},
{"title": "ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and\n  Self-Prompting", "author": "Yankai Jiang and Zhongzhen Huang and Rongzhao Zhang and Xiaofan Zhang and Shaoting Zhang", "abstract": "  The long-tailed distribution problem in medical image analysis reflects a\nhigh prevalence of common conditions and a low prevalence of rare ones, which\nposes a significant challenge in developing a unified model capable of\nidentifying rare or novel tumor categories not encountered during training. In\nthis paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT)\nbased on query-disentangling and self-prompting to segment unseen tumor\ncategories beyond the training set. ZePT disentangles the object queries into\ntwo subsets and trains them in two stages. Initially, it learns a set of\nfundamental queries for organ segmentation through an object-aware feature\ngrouping strategy, which gathers organ-level visual features. Subsequently, it\nrefines the other set of advanced queries that focus on the auto-generated\nvisual prompts for unseen tumor segmentation. Moreover, we introduce\nquery-knowledge alignment at the feature level to enhance each query's\ndiscriminative representation and generalizability. Extensive experiments on\nvarious tumor segmentation tasks demonstrate the performance superiority of\nZePT, which surpasses the previous counterparts and evidence the promising\nability for zero-shot tumor segmentation in real-world settings.\n", "link": "http://arxiv.org/abs/2312.04964v2", "date": "2024-03-22", "relevancy": 2.0996, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5257}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5139}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ZePT%3A%20Zero-Shot%20Pan-Tumor%20Segmentation%20via%20Query-Disentangling%20and%0A%20%20Self-Prompting&body=Title%3A%20ZePT%3A%20Zero-Shot%20Pan-Tumor%20Segmentation%20via%20Query-Disentangling%20and%0A%20%20Self-Prompting%0AAuthor%3A%20Yankai%20Jiang%20and%20Zhongzhen%20Huang%20and%20Rongzhao%20Zhang%20and%20Xiaofan%20Zhang%20and%20Shaoting%20Zhang%0AAbstract%3A%20%20%20The%20long-tailed%20distribution%20problem%20in%20medical%20image%20analysis%20reflects%20a%0Ahigh%20prevalence%20of%20common%20conditions%20and%20a%20low%20prevalence%20of%20rare%20ones%2C%20which%0Aposes%20a%20significant%20challenge%20in%20developing%20a%20unified%20model%20capable%20of%0Aidentifying%20rare%20or%20novel%20tumor%20categories%20not%20encountered%20during%20training.%20In%0Athis%20paper%2C%20we%20propose%20a%20new%20zero-shot%20pan-tumor%20segmentation%20framework%20%28ZePT%29%0Abased%20on%20query-disentangling%20and%20self-prompting%20to%20segment%20unseen%20tumor%0Acategories%20beyond%20the%20training%20set.%20ZePT%20disentangles%20the%20object%20queries%20into%0Atwo%20subsets%20and%20trains%20them%20in%20two%20stages.%20Initially%2C%20it%20learns%20a%20set%20of%0Afundamental%20queries%20for%20organ%20segmentation%20through%20an%20object-aware%20feature%0Agrouping%20strategy%2C%20which%20gathers%20organ-level%20visual%20features.%20Subsequently%2C%20it%0Arefines%20the%20other%20set%20of%20advanced%20queries%20that%20focus%20on%20the%20auto-generated%0Avisual%20prompts%20for%20unseen%20tumor%20segmentation.%20Moreover%2C%20we%20introduce%0Aquery-knowledge%20alignment%20at%20the%20feature%20level%20to%20enhance%20each%20query%27s%0Adiscriminative%20representation%20and%20generalizability.%20Extensive%20experiments%20on%0Avarious%20tumor%20segmentation%20tasks%20demonstrate%20the%20performance%20superiority%20of%0AZePT%2C%20which%20surpasses%20the%20previous%20counterparts%20and%20evidence%20the%20promising%0Aability%20for%20zero-shot%20tumor%20segmentation%20in%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04964v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZePT%3A%20Zero-Shot%20Pan-Tumor%20Segmentation%20via%20Query-Disentangling%20and%0A%20%20Self-Prompting&entry.906535625=Yankai%20Jiang%20and%20Zhongzhen%20Huang%20and%20Rongzhao%20Zhang%20and%20Xiaofan%20Zhang%20and%20Shaoting%20Zhang&entry.1292438233=%20%20The%20long-tailed%20distribution%20problem%20in%20medical%20image%20analysis%20reflects%20a%0Ahigh%20prevalence%20of%20common%20conditions%20and%20a%20low%20prevalence%20of%20rare%20ones%2C%20which%0Aposes%20a%20significant%20challenge%20in%20developing%20a%20unified%20model%20capable%20of%0Aidentifying%20rare%20or%20novel%20tumor%20categories%20not%20encountered%20during%20training.%20In%0Athis%20paper%2C%20we%20propose%20a%20new%20zero-shot%20pan-tumor%20segmentation%20framework%20%28ZePT%29%0Abased%20on%20query-disentangling%20and%20self-prompting%20to%20segment%20unseen%20tumor%0Acategories%20beyond%20the%20training%20set.%20ZePT%20disentangles%20the%20object%20queries%20into%0Atwo%20subsets%20and%20trains%20them%20in%20two%20stages.%20Initially%2C%20it%20learns%20a%20set%20of%0Afundamental%20queries%20for%20organ%20segmentation%20through%20an%20object-aware%20feature%0Agrouping%20strategy%2C%20which%20gathers%20organ-level%20visual%20features.%20Subsequently%2C%20it%0Arefines%20the%20other%20set%20of%20advanced%20queries%20that%20focus%20on%20the%20auto-generated%0Avisual%20prompts%20for%20unseen%20tumor%20segmentation.%20Moreover%2C%20we%20introduce%0Aquery-knowledge%20alignment%20at%20the%20feature%20level%20to%20enhance%20each%20query%27s%0Adiscriminative%20representation%20and%20generalizability.%20Extensive%20experiments%20on%0Avarious%20tumor%20segmentation%20tasks%20demonstrate%20the%20performance%20superiority%20of%0AZePT%2C%20which%20surpasses%20the%20previous%20counterparts%20and%20evidence%20the%20promising%0Aability%20for%20zero-shot%20tumor%20segmentation%20in%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04964v2&entry.124074799=Read"},
{"title": "ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars", "author": "Zhenwei Wang and Tengfei Wang and Gerhard Hancke and Ziwei Liu and Rynson W. H. Lau", "abstract": "  Real-world applications often require a large gallery of 3D assets that share\na consistent theme. While remarkable advances have been made in general 3D\ncontent creation from text or image, synthesizing customized 3D assets\nfollowing the shared theme of input 3D exemplars remains an open and\nchallenging problem. In this work, we present ThemeStation, a novel approach\nfor theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D\nassets based on given few exemplars with two goals: 1) unity for generating 3D\nassets that thematically align with the given exemplars and 2) diversity for\ngenerating 3D assets with a high degree of variations. To this end, we design a\ntwo-stage framework that draws a concept image first, followed by a\nreference-informed 3D modeling stage. We propose a novel dual score\ndistillation (DSD) loss to jointly leverage priors from both the input\nexemplars and the synthesized concept image. Extensive experiments and user\nstudies confirm that ThemeStation surpasses prior works in producing diverse\ntheme-aware 3D models with impressive quality. ThemeStation also enables\nvarious applications such as controllable 3D-to-3D generation.\n", "link": "http://arxiv.org/abs/2403.15383v1", "date": "2024-03-22", "relevancy": 2.0984, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5321}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5235}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5086}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ThemeStation%3A%20Generating%20Theme-Aware%203D%20Assets%20from%20Few%20Exemplars&body=Title%3A%20ThemeStation%3A%20Generating%20Theme-Aware%203D%20Assets%20from%20Few%20Exemplars%0AAuthor%3A%20Zhenwei%20Wang%20and%20Tengfei%20Wang%20and%20Gerhard%20Hancke%20and%20Ziwei%20Liu%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Real-world%20applications%20often%20require%20a%20large%20gallery%20of%203D%20assets%20that%20share%0Aa%20consistent%20theme.%20While%20remarkable%20advances%20have%20been%20made%20in%20general%203D%0Acontent%20creation%20from%20text%20or%20image%2C%20synthesizing%20customized%203D%20assets%0Afollowing%20the%20shared%20theme%20of%20input%203D%20exemplars%20remains%20an%20open%20and%0Achallenging%20problem.%20In%20this%20work%2C%20we%20present%20ThemeStation%2C%20a%20novel%20approach%0Afor%20theme-aware%203D-to-3D%20generation.%20ThemeStation%20synthesizes%20customized%203D%0Aassets%20based%20on%20given%20few%20exemplars%20with%20two%20goals%3A%201%29%20unity%20for%20generating%203D%0Aassets%20that%20thematically%20align%20with%20the%20given%20exemplars%20and%202%29%20diversity%20for%0Agenerating%203D%20assets%20with%20a%20high%20degree%20of%20variations.%20To%20this%20end%2C%20we%20design%20a%0Atwo-stage%20framework%20that%20draws%20a%20concept%20image%20first%2C%20followed%20by%20a%0Areference-informed%203D%20modeling%20stage.%20We%20propose%20a%20novel%20dual%20score%0Adistillation%20%28DSD%29%20loss%20to%20jointly%20leverage%20priors%20from%20both%20the%20input%0Aexemplars%20and%20the%20synthesized%20concept%20image.%20Extensive%20experiments%20and%20user%0Astudies%20confirm%20that%20ThemeStation%20surpasses%20prior%20works%20in%20producing%20diverse%0Atheme-aware%203D%20models%20with%20impressive%20quality.%20ThemeStation%20also%20enables%0Avarious%20applications%20such%20as%20controllable%203D-to-3D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15383v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThemeStation%3A%20Generating%20Theme-Aware%203D%20Assets%20from%20Few%20Exemplars&entry.906535625=Zhenwei%20Wang%20and%20Tengfei%20Wang%20and%20Gerhard%20Hancke%20and%20Ziwei%20Liu%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Real-world%20applications%20often%20require%20a%20large%20gallery%20of%203D%20assets%20that%20share%0Aa%20consistent%20theme.%20While%20remarkable%20advances%20have%20been%20made%20in%20general%203D%0Acontent%20creation%20from%20text%20or%20image%2C%20synthesizing%20customized%203D%20assets%0Afollowing%20the%20shared%20theme%20of%20input%203D%20exemplars%20remains%20an%20open%20and%0Achallenging%20problem.%20In%20this%20work%2C%20we%20present%20ThemeStation%2C%20a%20novel%20approach%0Afor%20theme-aware%203D-to-3D%20generation.%20ThemeStation%20synthesizes%20customized%203D%0Aassets%20based%20on%20given%20few%20exemplars%20with%20two%20goals%3A%201%29%20unity%20for%20generating%203D%0Aassets%20that%20thematically%20align%20with%20the%20given%20exemplars%20and%202%29%20diversity%20for%0Agenerating%203D%20assets%20with%20a%20high%20degree%20of%20variations.%20To%20this%20end%2C%20we%20design%20a%0Atwo-stage%20framework%20that%20draws%20a%20concept%20image%20first%2C%20followed%20by%20a%0Areference-informed%203D%20modeling%20stage.%20We%20propose%20a%20novel%20dual%20score%0Adistillation%20%28DSD%29%20loss%20to%20jointly%20leverage%20priors%20from%20both%20the%20input%0Aexemplars%20and%20the%20synthesized%20concept%20image.%20Extensive%20experiments%20and%20user%0Astudies%20confirm%20that%20ThemeStation%20surpasses%20prior%20works%20in%20producing%20diverse%0Atheme-aware%203D%20models%20with%20impressive%20quality.%20ThemeStation%20also%20enables%0Avarious%20applications%20such%20as%20controllable%203D-to-3D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15383v1&entry.124074799=Read"},
{"title": "FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos", "author": "Florian Langer and Jihong Ju and Georgi Dikov and Gerhard Reitmayr and Mohsen Ghafoorian", "abstract": "  Digitising the 3D world into a clean, CAD model-based representation has\nimportant applications for augmented reality and robotics. Current\nstate-of-the-art methods are computationally intensive as they individually\nencode each detected object and optimise CAD alignments in a second stage. In\nthis work, we propose FastCAD, a real-time method that simultaneously retrieves\nand aligns CAD models for all objects in a given scene. In contrast to previous\nworks, we directly predict alignment parameters and shape embeddings. We\nachieve high-quality shape retrievals by learning CAD embeddings in a\ncontrastive learning framework and distilling those into FastCAD. Our\nsingle-stage method accelerates the inference time by a factor of 50 compared\nto other methods operating on RGB-D scans while outperforming them on the\nchallenging Scan2CAD alignment benchmark. Further, our approach collaborates\nseamlessly with online 3D reconstruction techniques. This enables the real-time\ngeneration of precise CAD model-based reconstructions from videos at 10 FPS.\nDoing so, we significantly improve the Scan2CAD alignment accuracy in the video\nsetting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to\n29.6%.\n", "link": "http://arxiv.org/abs/2403.15161v1", "date": "2024-03-22", "relevancy": 2.098, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5406}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5238}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4861}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FastCAD%3A%20Real-Time%20CAD%20Retrieval%20and%20Alignment%20from%20Scans%20and%20Videos&body=Title%3A%20FastCAD%3A%20Real-Time%20CAD%20Retrieval%20and%20Alignment%20from%20Scans%20and%20Videos%0AAuthor%3A%20Florian%20Langer%20and%20Jihong%20Ju%20and%20Georgi%20Dikov%20and%20Gerhard%20Reitmayr%20and%20Mohsen%20Ghafoorian%0AAbstract%3A%20%20%20Digitising%20the%203D%20world%20into%20a%20clean%2C%20CAD%20model-based%20representation%20has%0Aimportant%20applications%20for%20augmented%20reality%20and%20robotics.%20Current%0Astate-of-the-art%20methods%20are%20computationally%20intensive%20as%20they%20individually%0Aencode%20each%20detected%20object%20and%20optimise%20CAD%20alignments%20in%20a%20second%20stage.%20In%0Athis%20work%2C%20we%20propose%20FastCAD%2C%20a%20real-time%20method%20that%20simultaneously%20retrieves%0Aand%20aligns%20CAD%20models%20for%20all%20objects%20in%20a%20given%20scene.%20In%20contrast%20to%20previous%0Aworks%2C%20we%20directly%20predict%20alignment%20parameters%20and%20shape%20embeddings.%20We%0Aachieve%20high-quality%20shape%20retrievals%20by%20learning%20CAD%20embeddings%20in%20a%0Acontrastive%20learning%20framework%20and%20distilling%20those%20into%20FastCAD.%20Our%0Asingle-stage%20method%20accelerates%20the%20inference%20time%20by%20a%20factor%20of%2050%20compared%0Ato%20other%20methods%20operating%20on%20RGB-D%20scans%20while%20outperforming%20them%20on%20the%0Achallenging%20Scan2CAD%20alignment%20benchmark.%20Further%2C%20our%20approach%20collaborates%0Aseamlessly%20with%20online%203D%20reconstruction%20techniques.%20This%20enables%20the%20real-time%0Ageneration%20of%20precise%20CAD%20model-based%20reconstructions%20from%20videos%20at%2010%20FPS.%0ADoing%20so%2C%20we%20significantly%20improve%20the%20Scan2CAD%20alignment%20accuracy%20in%20the%20video%0Asetting%20from%2043.0%25%20to%2048.2%25%20and%20the%20reconstruction%20accuracy%20from%2022.9%25%20to%0A29.6%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15161v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastCAD%3A%20Real-Time%20CAD%20Retrieval%20and%20Alignment%20from%20Scans%20and%20Videos&entry.906535625=Florian%20Langer%20and%20Jihong%20Ju%20and%20Georgi%20Dikov%20and%20Gerhard%20Reitmayr%20and%20Mohsen%20Ghafoorian&entry.1292438233=%20%20Digitising%20the%203D%20world%20into%20a%20clean%2C%20CAD%20model-based%20representation%20has%0Aimportant%20applications%20for%20augmented%20reality%20and%20robotics.%20Current%0Astate-of-the-art%20methods%20are%20computationally%20intensive%20as%20they%20individually%0Aencode%20each%20detected%20object%20and%20optimise%20CAD%20alignments%20in%20a%20second%20stage.%20In%0Athis%20work%2C%20we%20propose%20FastCAD%2C%20a%20real-time%20method%20that%20simultaneously%20retrieves%0Aand%20aligns%20CAD%20models%20for%20all%20objects%20in%20a%20given%20scene.%20In%20contrast%20to%20previous%0Aworks%2C%20we%20directly%20predict%20alignment%20parameters%20and%20shape%20embeddings.%20We%0Aachieve%20high-quality%20shape%20retrievals%20by%20learning%20CAD%20embeddings%20in%20a%0Acontrastive%20learning%20framework%20and%20distilling%20those%20into%20FastCAD.%20Our%0Asingle-stage%20method%20accelerates%20the%20inference%20time%20by%20a%20factor%20of%2050%20compared%0Ato%20other%20methods%20operating%20on%20RGB-D%20scans%20while%20outperforming%20them%20on%20the%0Achallenging%20Scan2CAD%20alignment%20benchmark.%20Further%2C%20our%20approach%20collaborates%0Aseamlessly%20with%20online%203D%20reconstruction%20techniques.%20This%20enables%20the%20real-time%0Ageneration%20of%20precise%20CAD%20model-based%20reconstructions%20from%20videos%20at%2010%20FPS.%0ADoing%20so%2C%20we%20significantly%20improve%20the%20Scan2CAD%20alignment%20accuracy%20in%20the%20video%0Asetting%20from%2043.0%25%20to%2048.2%25%20and%20the%20reconstruction%20accuracy%20from%2022.9%25%20to%0A29.6%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15161v1&entry.124074799=Read"},
{"title": "An In-Depth Analysis of Data Reduction Methods for Sustainable Deep\n  Learning", "author": "V\u00edctor Toscano-Dur\u00e1n and Javier Perera-Lago and Eduardo Paluzo-Hidalgo and Roc\u00edo Gonzalez-Diaz and Miguel \u00c1ngel Gutierrez-Naranjo and Matteo Rucco", "abstract": "  In recent years, Deep Learning has gained popularity for its ability to solve\ncomplex classification tasks, increasingly delivering better results thanks to\nthe development of more accurate models, the availability of huge volumes of\ndata and the improved computational capabilities of modern computers. However,\nthese improvements in performance also bring efficiency problems, related to\nthe storage of datasets and models, and to the waste of energy and time\ninvolved in both the training and inference processes. In this context, data\nreduction can help reduce energy consumption when training a deep learning\nmodel. In this paper, we present up to eight different methods to reduce the\nsize of a tabular training dataset, and we develop a Python package to apply\nthem. We also introduce a representativeness metric based on topology to\nmeasure how similar are the reduced datasets and the full training dataset.\nAdditionally, we develop a methodology to apply these data reduction methods to\nimage datasets for object detection tasks. Finally, we experimentally compare\nhow these data reduction methods affect the representativeness of the reduced\ndataset, the energy consumption and the predictive performance of the model.\n", "link": "http://arxiv.org/abs/2403.15150v1", "date": "2024-03-22", "relevancy": 2.0848, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5391}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5124}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4986}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20In-Depth%20Analysis%20of%20Data%20Reduction%20Methods%20for%20Sustainable%20Deep%0A%20%20Learning&body=Title%3A%20An%20In-Depth%20Analysis%20of%20Data%20Reduction%20Methods%20for%20Sustainable%20Deep%0A%20%20Learning%0AAuthor%3A%20V%C3%ADctor%20Toscano-Dur%C3%A1n%20and%20Javier%20Perera-Lago%20and%20Eduardo%20Paluzo-Hidalgo%20and%20Roc%C3%ADo%20Gonzalez-Diaz%20and%20Miguel%20%C3%81ngel%20Gutierrez-Naranjo%20and%20Matteo%20Rucco%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Deep%20Learning%20has%20gained%20popularity%20for%20its%20ability%20to%20solve%0Acomplex%20classification%20tasks%2C%20increasingly%20delivering%20better%20results%20thanks%20to%0Athe%20development%20of%20more%20accurate%20models%2C%20the%20availability%20of%20huge%20volumes%20of%0Adata%20and%20the%20improved%20computational%20capabilities%20of%20modern%20computers.%20However%2C%0Athese%20improvements%20in%20performance%20also%20bring%20efficiency%20problems%2C%20related%20to%0Athe%20storage%20of%20datasets%20and%20models%2C%20and%20to%20the%20waste%20of%20energy%20and%20time%0Ainvolved%20in%20both%20the%20training%20and%20inference%20processes.%20In%20this%20context%2C%20data%0Areduction%20can%20help%20reduce%20energy%20consumption%20when%20training%20a%20deep%20learning%0Amodel.%20In%20this%20paper%2C%20we%20present%20up%20to%20eight%20different%20methods%20to%20reduce%20the%0Asize%20of%20a%20tabular%20training%20dataset%2C%20and%20we%20develop%20a%20Python%20package%20to%20apply%0Athem.%20We%20also%20introduce%20a%20representativeness%20metric%20based%20on%20topology%20to%0Ameasure%20how%20similar%20are%20the%20reduced%20datasets%20and%20the%20full%20training%20dataset.%0AAdditionally%2C%20we%20develop%20a%20methodology%20to%20apply%20these%20data%20reduction%20methods%20to%0Aimage%20datasets%20for%20object%20detection%20tasks.%20Finally%2C%20we%20experimentally%20compare%0Ahow%20these%20data%20reduction%20methods%20affect%20the%20representativeness%20of%20the%20reduced%0Adataset%2C%20the%20energy%20consumption%20and%20the%20predictive%20performance%20of%20the%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15150v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20In-Depth%20Analysis%20of%20Data%20Reduction%20Methods%20for%20Sustainable%20Deep%0A%20%20Learning&entry.906535625=V%C3%ADctor%20Toscano-Dur%C3%A1n%20and%20Javier%20Perera-Lago%20and%20Eduardo%20Paluzo-Hidalgo%20and%20Roc%C3%ADo%20Gonzalez-Diaz%20and%20Miguel%20%C3%81ngel%20Gutierrez-Naranjo%20and%20Matteo%20Rucco&entry.1292438233=%20%20In%20recent%20years%2C%20Deep%20Learning%20has%20gained%20popularity%20for%20its%20ability%20to%20solve%0Acomplex%20classification%20tasks%2C%20increasingly%20delivering%20better%20results%20thanks%20to%0Athe%20development%20of%20more%20accurate%20models%2C%20the%20availability%20of%20huge%20volumes%20of%0Adata%20and%20the%20improved%20computational%20capabilities%20of%20modern%20computers.%20However%2C%0Athese%20improvements%20in%20performance%20also%20bring%20efficiency%20problems%2C%20related%20to%0Athe%20storage%20of%20datasets%20and%20models%2C%20and%20to%20the%20waste%20of%20energy%20and%20time%0Ainvolved%20in%20both%20the%20training%20and%20inference%20processes.%20In%20this%20context%2C%20data%0Areduction%20can%20help%20reduce%20energy%20consumption%20when%20training%20a%20deep%20learning%0Amodel.%20In%20this%20paper%2C%20we%20present%20up%20to%20eight%20different%20methods%20to%20reduce%20the%0Asize%20of%20a%20tabular%20training%20dataset%2C%20and%20we%20develop%20a%20Python%20package%20to%20apply%0Athem.%20We%20also%20introduce%20a%20representativeness%20metric%20based%20on%20topology%20to%0Ameasure%20how%20similar%20are%20the%20reduced%20datasets%20and%20the%20full%20training%20dataset.%0AAdditionally%2C%20we%20develop%20a%20methodology%20to%20apply%20these%20data%20reduction%20methods%20to%0Aimage%20datasets%20for%20object%20detection%20tasks.%20Finally%2C%20we%20experimentally%20compare%0Ahow%20these%20data%20reduction%20methods%20affect%20the%20representativeness%20of%20the%20reduced%0Adataset%2C%20the%20energy%20consumption%20and%20the%20predictive%20performance%20of%20the%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15150v1&entry.124074799=Read"},
{"title": "CRPlace: Camera-Radar Fusion with BEV Representation for Place\n  Recognition", "author": "Shaowei Fu and Yifan Duan and Yao Li and Chengzhen Meng and Yingjie Wang and Jianmin Ji and Yanyong Zhang", "abstract": "  The integration of complementary characteristics from camera and radar data\nhas emerged as an effective approach in 3D object detection. However, such\nfusion-based methods remain unexplored for place recognition, an equally\nimportant task for autonomous systems. Given that place recognition relies on\nthe similarity between a query scene and the corresponding candidate scene, the\nstationary background of a scene is expected to play a crucial role in the\ntask. As such, current well-designed camera-radar fusion methods for 3D object\ndetection can hardly take effect in place recognition because they mainly focus\non dynamic foreground objects. In this paper, a background-attentive\ncamera-radar fusion-based method, named CRPlace, is proposed to generate\nbackground-attentive global descriptors from multi-view images and radar point\nclouds for accurate place recognition. To extract stationary background\nfeatures effectively, we design an adaptive module that generates the\nbackground-attentive mask by utilizing the camera BEV feature and radar dynamic\npoints. With the guidance of a background mask, we devise a bidirectional\ncross-attention-based spatial fusion strategy to facilitate comprehensive\nspatial interaction between the background information of the camera BEV\nfeature and the radar BEV feature. As the first camera-radar fusion-based place\nrecognition network, CRPlace has been evaluated thoroughly on the nuScenes\ndataset. The results show that our algorithm outperforms a variety of baseline\nmethods across a comprehensive set of metrics (recall@1 reaches 91.2%).\n", "link": "http://arxiv.org/abs/2403.15183v1", "date": "2024-03-22", "relevancy": 2.0836, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.554}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4992}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4922}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CRPlace%3A%20Camera-Radar%20Fusion%20with%20BEV%20Representation%20for%20Place%0A%20%20Recognition&body=Title%3A%20CRPlace%3A%20Camera-Radar%20Fusion%20with%20BEV%20Representation%20for%20Place%0A%20%20Recognition%0AAuthor%3A%20Shaowei%20Fu%20and%20Yifan%20Duan%20and%20Yao%20Li%20and%20Chengzhen%20Meng%20and%20Yingjie%20Wang%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang%0AAbstract%3A%20%20%20The%20integration%20of%20complementary%20characteristics%20from%20camera%20and%20radar%20data%0Ahas%20emerged%20as%20an%20effective%20approach%20in%203D%20object%20detection.%20However%2C%20such%0Afusion-based%20methods%20remain%20unexplored%20for%20place%20recognition%2C%20an%20equally%0Aimportant%20task%20for%20autonomous%20systems.%20Given%20that%20place%20recognition%20relies%20on%0Athe%20similarity%20between%20a%20query%20scene%20and%20the%20corresponding%20candidate%20scene%2C%20the%0Astationary%20background%20of%20a%20scene%20is%20expected%20to%20play%20a%20crucial%20role%20in%20the%0Atask.%20As%20such%2C%20current%20well-designed%20camera-radar%20fusion%20methods%20for%203D%20object%0Adetection%20can%20hardly%20take%20effect%20in%20place%20recognition%20because%20they%20mainly%20focus%0Aon%20dynamic%20foreground%20objects.%20In%20this%20paper%2C%20a%20background-attentive%0Acamera-radar%20fusion-based%20method%2C%20named%20CRPlace%2C%20is%20proposed%20to%20generate%0Abackground-attentive%20global%20descriptors%20from%20multi-view%20images%20and%20radar%20point%0Aclouds%20for%20accurate%20place%20recognition.%20To%20extract%20stationary%20background%0Afeatures%20effectively%2C%20we%20design%20an%20adaptive%20module%20that%20generates%20the%0Abackground-attentive%20mask%20by%20utilizing%20the%20camera%20BEV%20feature%20and%20radar%20dynamic%0Apoints.%20With%20the%20guidance%20of%20a%20background%20mask%2C%20we%20devise%20a%20bidirectional%0Across-attention-based%20spatial%20fusion%20strategy%20to%20facilitate%20comprehensive%0Aspatial%20interaction%20between%20the%20background%20information%20of%20the%20camera%20BEV%0Afeature%20and%20the%20radar%20BEV%20feature.%20As%20the%20first%20camera-radar%20fusion-based%20place%0Arecognition%20network%2C%20CRPlace%20has%20been%20evaluated%20thoroughly%20on%20the%20nuScenes%0Adataset.%20The%20results%20show%20that%20our%20algorithm%20outperforms%20a%20variety%20of%20baseline%0Amethods%20across%20a%20comprehensive%20set%20of%20metrics%20%28recall%401%20reaches%2091.2%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15183v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRPlace%3A%20Camera-Radar%20Fusion%20with%20BEV%20Representation%20for%20Place%0A%20%20Recognition&entry.906535625=Shaowei%20Fu%20and%20Yifan%20Duan%20and%20Yao%20Li%20and%20Chengzhen%20Meng%20and%20Yingjie%20Wang%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang&entry.1292438233=%20%20The%20integration%20of%20complementary%20characteristics%20from%20camera%20and%20radar%20data%0Ahas%20emerged%20as%20an%20effective%20approach%20in%203D%20object%20detection.%20However%2C%20such%0Afusion-based%20methods%20remain%20unexplored%20for%20place%20recognition%2C%20an%20equally%0Aimportant%20task%20for%20autonomous%20systems.%20Given%20that%20place%20recognition%20relies%20on%0Athe%20similarity%20between%20a%20query%20scene%20and%20the%20corresponding%20candidate%20scene%2C%20the%0Astationary%20background%20of%20a%20scene%20is%20expected%20to%20play%20a%20crucial%20role%20in%20the%0Atask.%20As%20such%2C%20current%20well-designed%20camera-radar%20fusion%20methods%20for%203D%20object%0Adetection%20can%20hardly%20take%20effect%20in%20place%20recognition%20because%20they%20mainly%20focus%0Aon%20dynamic%20foreground%20objects.%20In%20this%20paper%2C%20a%20background-attentive%0Acamera-radar%20fusion-based%20method%2C%20named%20CRPlace%2C%20is%20proposed%20to%20generate%0Abackground-attentive%20global%20descriptors%20from%20multi-view%20images%20and%20radar%20point%0Aclouds%20for%20accurate%20place%20recognition.%20To%20extract%20stationary%20background%0Afeatures%20effectively%2C%20we%20design%20an%20adaptive%20module%20that%20generates%20the%0Abackground-attentive%20mask%20by%20utilizing%20the%20camera%20BEV%20feature%20and%20radar%20dynamic%0Apoints.%20With%20the%20guidance%20of%20a%20background%20mask%2C%20we%20devise%20a%20bidirectional%0Across-attention-based%20spatial%20fusion%20strategy%20to%20facilitate%20comprehensive%0Aspatial%20interaction%20between%20the%20background%20information%20of%20the%20camera%20BEV%0Afeature%20and%20the%20radar%20BEV%20feature.%20As%20the%20first%20camera-radar%20fusion-based%20place%0Arecognition%20network%2C%20CRPlace%20has%20been%20evaluated%20thoroughly%20on%20the%20nuScenes%0Adataset.%20The%20results%20show%20that%20our%20algorithm%20outperforms%20a%20variety%20of%20baseline%0Amethods%20across%20a%20comprehensive%20set%20of%20metrics%20%28recall%401%20reaches%2091.2%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15183v1&entry.124074799=Read"},
{"title": "Hyperbolic Metric Learning for Visual Outlier Detection", "author": "Alvaro Gonzalez-Jimenez and Simone Lionetti and Dena Bazazian and Philippe Gottfrois and Fabian Gr\u00f6ger and Marc Pouly and Alexander Navarini", "abstract": "  Out-Of-Distribution (OOD) detection is critical to deploy deep learning\nmodels in safety-critical applications. However, the inherent hierarchical\nconcept structure of visual data, which is instrumental to OOD detection, is\noften poorly captured by conventional methods based on Euclidean geometry. This\nwork proposes a metric framework that leverages the strengths of Hyperbolic\ngeometry for OOD detection. Inspired by previous works that refine the decision\nboundary for OOD data with synthetic outliers, we extend this method to\nHyperbolic space. Interestingly, we find that synthetic outliers do not benefit\nOOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we\nexplore the relationship between OOD detection performance and Hyperbolic\nembedding dimension, addressing practical concerns in resource-constrained\nenvironments. Extensive experiments show that our framework improves the FPR95\nfor OOD detection from 22\\% to 15\\% and from 49% to 28% on CIFAR-10 and\nCIFAR-100 respectively compared to Euclidean methods.\n", "link": "http://arxiv.org/abs/2403.15260v1", "date": "2024-03-22", "relevancy": 2.0684, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5303}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5089}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5045}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hyperbolic%20Metric%20Learning%20for%20Visual%20Outlier%20Detection&body=Title%3A%20Hyperbolic%20Metric%20Learning%20for%20Visual%20Outlier%20Detection%0AAuthor%3A%20Alvaro%20Gonzalez-Jimenez%20and%20Simone%20Lionetti%20and%20Dena%20Bazazian%20and%20Philippe%20Gottfrois%20and%20Fabian%20Gr%C3%B6ger%20and%20Marc%20Pouly%20and%20Alexander%20Navarini%0AAbstract%3A%20%20%20Out-Of-Distribution%20%28OOD%29%20detection%20is%20critical%20to%20deploy%20deep%20learning%0Amodels%20in%20safety-critical%20applications.%20However%2C%20the%20inherent%20hierarchical%0Aconcept%20structure%20of%20visual%20data%2C%20which%20is%20instrumental%20to%20OOD%20detection%2C%20is%0Aoften%20poorly%20captured%20by%20conventional%20methods%20based%20on%20Euclidean%20geometry.%20This%0Awork%20proposes%20a%20metric%20framework%20that%20leverages%20the%20strengths%20of%20Hyperbolic%0Ageometry%20for%20OOD%20detection.%20Inspired%20by%20previous%20works%20that%20refine%20the%20decision%0Aboundary%20for%20OOD%20data%20with%20synthetic%20outliers%2C%20we%20extend%20this%20method%20to%0AHyperbolic%20space.%20Interestingly%2C%20we%20find%20that%20synthetic%20outliers%20do%20not%20benefit%0AOOD%20detection%20in%20Hyperbolic%20space%20as%20they%20do%20in%20Euclidean%20space.%20Furthermore%20we%0Aexplore%20the%20relationship%20between%20OOD%20detection%20performance%20and%20Hyperbolic%0Aembedding%20dimension%2C%20addressing%20practical%20concerns%20in%20resource-constrained%0Aenvironments.%20Extensive%20experiments%20show%20that%20our%20framework%20improves%20the%20FPR95%0Afor%20OOD%20detection%20from%2022%5C%25%20to%2015%5C%25%20and%20from%2049%25%20to%2028%25%20on%20CIFAR-10%20and%0ACIFAR-100%20respectively%20compared%20to%20Euclidean%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15260v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperbolic%20Metric%20Learning%20for%20Visual%20Outlier%20Detection&entry.906535625=Alvaro%20Gonzalez-Jimenez%20and%20Simone%20Lionetti%20and%20Dena%20Bazazian%20and%20Philippe%20Gottfrois%20and%20Fabian%20Gr%C3%B6ger%20and%20Marc%20Pouly%20and%20Alexander%20Navarini&entry.1292438233=%20%20Out-Of-Distribution%20%28OOD%29%20detection%20is%20critical%20to%20deploy%20deep%20learning%0Amodels%20in%20safety-critical%20applications.%20However%2C%20the%20inherent%20hierarchical%0Aconcept%20structure%20of%20visual%20data%2C%20which%20is%20instrumental%20to%20OOD%20detection%2C%20is%0Aoften%20poorly%20captured%20by%20conventional%20methods%20based%20on%20Euclidean%20geometry.%20This%0Awork%20proposes%20a%20metric%20framework%20that%20leverages%20the%20strengths%20of%20Hyperbolic%0Ageometry%20for%20OOD%20detection.%20Inspired%20by%20previous%20works%20that%20refine%20the%20decision%0Aboundary%20for%20OOD%20data%20with%20synthetic%20outliers%2C%20we%20extend%20this%20method%20to%0AHyperbolic%20space.%20Interestingly%2C%20we%20find%20that%20synthetic%20outliers%20do%20not%20benefit%0AOOD%20detection%20in%20Hyperbolic%20space%20as%20they%20do%20in%20Euclidean%20space.%20Furthermore%20we%0Aexplore%20the%20relationship%20between%20OOD%20detection%20performance%20and%20Hyperbolic%0Aembedding%20dimension%2C%20addressing%20practical%20concerns%20in%20resource-constrained%0Aenvironments.%20Extensive%20experiments%20show%20that%20our%20framework%20improves%20the%20FPR95%0Afor%20OOD%20detection%20from%2022%5C%25%20to%2015%5C%25%20and%20from%2049%25%20to%2028%25%20on%20CIFAR-10%20and%0ACIFAR-100%20respectively%20compared%20to%20Euclidean%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15260v1&entry.124074799=Read"},
{"title": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models", "author": "Adam Ibrahim and Benjamin Th\u00e9rien and Kshitij Gupta and Mats L. Richter and Quentin Anthony and Timoth\u00e9e Lesort and Eugene Belilovsky and Irina Rish", "abstract": "  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n", "link": "http://arxiv.org/abs/2403.08763v2", "date": "2024-03-22", "relevancy": 2.0678, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5138}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models&body=Title%3A%20Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Adam%20Ibrahim%20and%20Benjamin%20Th%C3%A9rien%20and%20Kshitij%20Gupta%20and%20Mats%20L.%20Richter%20and%20Quentin%20Anthony%20and%20Timoth%C3%A9e%20Lesort%20and%20Eugene%20Belilovsky%20and%20Irina%20Rish%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20routinely%20pre-trained%20on%20billions%20of%20tokens%2C%0Aonly%20to%20start%20the%20process%20over%20again%20once%20new%20data%20becomes%20available.%20A%20much%0Amore%20efficient%20solution%20is%20to%20continually%20pre-train%20these%20models%2C%20saving%0Asignificant%20compute%20compared%20to%20re-training.%20However%2C%20the%20distribution%20shift%0Ainduced%20by%20new%20data%20typically%20results%20in%20degraded%20performance%20on%20previous%20data%0Aor%20poor%20adaptation%20to%20the%20new%20data.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20and%0Ascalable%20combination%20of%20learning%20rate%20%28LR%29%20re-warming%2C%20LR%20re-decaying%2C%20and%0Areplay%20of%20previous%20data%20is%20sufficient%20to%20match%20the%20performance%20of%20fully%0Are-training%20from%20scratch%20on%20all%20available%20data%2C%20as%20measured%20by%20final%20loss%20and%0Alanguage%20model%20%28LM%29%20evaluation%20benchmarks.%20Specifically%2C%20we%20show%20this%20for%20a%0Aweak%20but%20realistic%20distribution%20shift%20between%20two%20commonly%20used%20LLM%0Apre-training%20datasets%20%28English%24%5Crightarrow%24English%29%20and%20a%20stronger%20distribution%0Ashift%20%28English%24%5Crightarrow%24German%29%20at%20the%20%24405%24M%20parameter%20model%20scale%20with%0Alarge%20dataset%20sizes%20%28hundreds%20of%20billions%20of%20tokens%29.%20Selecting%20the%20weak%20but%0Arealistic%20shift%20for%20larger-scale%20experiments%2C%20we%20also%20find%20that%20our%20continual%0Alearning%20strategies%20match%20the%20re-training%20baseline%20for%20a%2010B%20parameter%20LLM.%20Our%0Aresults%20demonstrate%20that%20LLMs%20can%20be%20successfully%20updated%20via%20simple%20and%0Ascalable%20continual%20learning%20strategies%2C%20matching%20the%20re-training%20baseline%20using%0Aonly%20a%20fraction%20of%20the%20compute.%20Finally%2C%20inspired%20by%20previous%20work%2C%20we%20propose%0Aalternatives%20to%20the%20cosine%20learning%20rate%20schedule%20that%20help%20circumvent%0Aforgetting%20induced%20by%20LR%20re-warming%20and%20that%20are%20not%20bound%20to%20a%20fixed%20token%0Abudget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08763v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20and%20Scalable%20Strategies%20to%20Continually%20Pre-train%20Large%20Language%0A%20%20Models&entry.906535625=Adam%20Ibrahim%20and%20Benjamin%20Th%C3%A9rien%20and%20Kshitij%20Gupta%20and%20Mats%20L.%20Richter%20and%20Quentin%20Anthony%20and%20Timoth%C3%A9e%20Lesort%20and%20Eugene%20Belilovsky%20and%20Irina%20Rish&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20routinely%20pre-trained%20on%20billions%20of%20tokens%2C%0Aonly%20to%20start%20the%20process%20over%20again%20once%20new%20data%20becomes%20available.%20A%20much%0Amore%20efficient%20solution%20is%20to%20continually%20pre-train%20these%20models%2C%20saving%0Asignificant%20compute%20compared%20to%20re-training.%20However%2C%20the%20distribution%20shift%0Ainduced%20by%20new%20data%20typically%20results%20in%20degraded%20performance%20on%20previous%20data%0Aor%20poor%20adaptation%20to%20the%20new%20data.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20and%0Ascalable%20combination%20of%20learning%20rate%20%28LR%29%20re-warming%2C%20LR%20re-decaying%2C%20and%0Areplay%20of%20previous%20data%20is%20sufficient%20to%20match%20the%20performance%20of%20fully%0Are-training%20from%20scratch%20on%20all%20available%20data%2C%20as%20measured%20by%20final%20loss%20and%0Alanguage%20model%20%28LM%29%20evaluation%20benchmarks.%20Specifically%2C%20we%20show%20this%20for%20a%0Aweak%20but%20realistic%20distribution%20shift%20between%20two%20commonly%20used%20LLM%0Apre-training%20datasets%20%28English%24%5Crightarrow%24English%29%20and%20a%20stronger%20distribution%0Ashift%20%28English%24%5Crightarrow%24German%29%20at%20the%20%24405%24M%20parameter%20model%20scale%20with%0Alarge%20dataset%20sizes%20%28hundreds%20of%20billions%20of%20tokens%29.%20Selecting%20the%20weak%20but%0Arealistic%20shift%20for%20larger-scale%20experiments%2C%20we%20also%20find%20that%20our%20continual%0Alearning%20strategies%20match%20the%20re-training%20baseline%20for%20a%2010B%20parameter%20LLM.%20Our%0Aresults%20demonstrate%20that%20LLMs%20can%20be%20successfully%20updated%20via%20simple%20and%0Ascalable%20continual%20learning%20strategies%2C%20matching%20the%20re-training%20baseline%20using%0Aonly%20a%20fraction%20of%20the%20compute.%20Finally%2C%20inspired%20by%20previous%20work%2C%20we%20propose%0Aalternatives%20to%20the%20cosine%20learning%20rate%20schedule%20that%20help%20circumvent%0Aforgetting%20induced%20by%20LR%20re-warming%20and%20that%20are%20not%20bound%20to%20a%20fixed%20token%0Abudget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08763v2&entry.124074799=Read"},
{"title": "An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic\n  Wild Person Re-Identification", "author": "Lei Zhang and Xiaowei Fu and Fuxiang Huang and Yi Yang and Xinbo Gao", "abstract": "  Person re-identification (ReID) has made great strides thanks to the\ndata-driven deep learning techniques. However, the existing benchmark datasets\nlack diversity, and models trained on these data cannot generalize well to\ndynamic wild scenarios. To meet the goal of improving the explicit\ngeneralization of ReID models, we develop a new Open-World, Diverse,\nCross-Spatial-Temporal dataset named OWD with several distinct features. 1)\nDiverse collection scenes: multiple independent open-world and highly dynamic\ncollecting scenes, including streets, intersections, shopping malls, etc. 2)\nDiverse lighting variations: long time spans from daytime to nighttime with\nabundant illumination changes. 3) Diverse person status: multiple camera\nnetworks in all seasons with normal/adverse weather conditions and diverse\npedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4)\nProtected privacy: invisible faces for privacy critical applications. To\nimprove the implicit generalization of ReID, we further propose a Latent Domain\nExpansion (LDE) method to develop the potential of source data, which decouples\ndiscriminative identity-relevant and trustworthy domain-relevant features and\nimplicitly enforces domain-randomized identity feature space expansion with\nricher domain diversity to facilitate domain invariant representations. Our\ncomprehensive evaluations with most benchmark datasets in the community are\ncrucial for progress, although this work is far from the grand goal toward\nopen-world and dynamic wild applications.\n", "link": "http://arxiv.org/abs/2403.15119v1", "date": "2024-03-22", "relevancy": 2.0463, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5248}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5025}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5013}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Open-World%2C%20Diverse%2C%20Cross-Spatial-Temporal%20Benchmark%20for%20Dynamic%0A%20%20Wild%20Person%20Re-Identification&body=Title%3A%20An%20Open-World%2C%20Diverse%2C%20Cross-Spatial-Temporal%20Benchmark%20for%20Dynamic%0A%20%20Wild%20Person%20Re-Identification%0AAuthor%3A%20Lei%20Zhang%20and%20Xiaowei%20Fu%20and%20Fuxiang%20Huang%20and%20Yi%20Yang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Person%20re-identification%20%28ReID%29%20has%20made%20great%20strides%20thanks%20to%20the%0Adata-driven%20deep%20learning%20techniques.%20However%2C%20the%20existing%20benchmark%20datasets%0Alack%20diversity%2C%20and%20models%20trained%20on%20these%20data%20cannot%20generalize%20well%20to%0Adynamic%20wild%20scenarios.%20To%20meet%20the%20goal%20of%20improving%20the%20explicit%0Ageneralization%20of%20ReID%20models%2C%20we%20develop%20a%20new%20Open-World%2C%20Diverse%2C%0ACross-Spatial-Temporal%20dataset%20named%20OWD%20with%20several%20distinct%20features.%201%29%0ADiverse%20collection%20scenes%3A%20multiple%20independent%20open-world%20and%20highly%20dynamic%0Acollecting%20scenes%2C%20including%20streets%2C%20intersections%2C%20shopping%20malls%2C%20etc.%202%29%0ADiverse%20lighting%20variations%3A%20long%20time%20spans%20from%20daytime%20to%20nighttime%20with%0Aabundant%20illumination%20changes.%203%29%20Diverse%20person%20status%3A%20multiple%20camera%0Anetworks%20in%20all%20seasons%20with%20normal/adverse%20weather%20conditions%20and%20diverse%0Apedestrian%20appearances%20%28e.g.%2C%20clothes%2C%20personal%20belongings%2C%20poses%2C%20etc.%29.%204%29%0AProtected%20privacy%3A%20invisible%20faces%20for%20privacy%20critical%20applications.%20To%0Aimprove%20the%20implicit%20generalization%20of%20ReID%2C%20we%20further%20propose%20a%20Latent%20Domain%0AExpansion%20%28LDE%29%20method%20to%20develop%20the%20potential%20of%20source%20data%2C%20which%20decouples%0Adiscriminative%20identity-relevant%20and%20trustworthy%20domain-relevant%20features%20and%0Aimplicitly%20enforces%20domain-randomized%20identity%20feature%20space%20expansion%20with%0Aricher%20domain%20diversity%20to%20facilitate%20domain%20invariant%20representations.%20Our%0Acomprehensive%20evaluations%20with%20most%20benchmark%20datasets%20in%20the%20community%20are%0Acrucial%20for%20progress%2C%20although%20this%20work%20is%20far%20from%20the%20grand%20goal%20toward%0Aopen-world%20and%20dynamic%20wild%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15119v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Open-World%2C%20Diverse%2C%20Cross-Spatial-Temporal%20Benchmark%20for%20Dynamic%0A%20%20Wild%20Person%20Re-Identification&entry.906535625=Lei%20Zhang%20and%20Xiaowei%20Fu%20and%20Fuxiang%20Huang%20and%20Yi%20Yang%20and%20Xinbo%20Gao&entry.1292438233=%20%20Person%20re-identification%20%28ReID%29%20has%20made%20great%20strides%20thanks%20to%20the%0Adata-driven%20deep%20learning%20techniques.%20However%2C%20the%20existing%20benchmark%20datasets%0Alack%20diversity%2C%20and%20models%20trained%20on%20these%20data%20cannot%20generalize%20well%20to%0Adynamic%20wild%20scenarios.%20To%20meet%20the%20goal%20of%20improving%20the%20explicit%0Ageneralization%20of%20ReID%20models%2C%20we%20develop%20a%20new%20Open-World%2C%20Diverse%2C%0ACross-Spatial-Temporal%20dataset%20named%20OWD%20with%20several%20distinct%20features.%201%29%0ADiverse%20collection%20scenes%3A%20multiple%20independent%20open-world%20and%20highly%20dynamic%0Acollecting%20scenes%2C%20including%20streets%2C%20intersections%2C%20shopping%20malls%2C%20etc.%202%29%0ADiverse%20lighting%20variations%3A%20long%20time%20spans%20from%20daytime%20to%20nighttime%20with%0Aabundant%20illumination%20changes.%203%29%20Diverse%20person%20status%3A%20multiple%20camera%0Anetworks%20in%20all%20seasons%20with%20normal/adverse%20weather%20conditions%20and%20diverse%0Apedestrian%20appearances%20%28e.g.%2C%20clothes%2C%20personal%20belongings%2C%20poses%2C%20etc.%29.%204%29%0AProtected%20privacy%3A%20invisible%20faces%20for%20privacy%20critical%20applications.%20To%0Aimprove%20the%20implicit%20generalization%20of%20ReID%2C%20we%20further%20propose%20a%20Latent%20Domain%0AExpansion%20%28LDE%29%20method%20to%20develop%20the%20potential%20of%20source%20data%2C%20which%20decouples%0Adiscriminative%20identity-relevant%20and%20trustworthy%20domain-relevant%20features%20and%0Aimplicitly%20enforces%20domain-randomized%20identity%20feature%20space%20expansion%20with%0Aricher%20domain%20diversity%20to%20facilitate%20domain%20invariant%20representations.%20Our%0Acomprehensive%20evaluations%20with%20most%20benchmark%20datasets%20in%20the%20community%20are%0Acrucial%20for%20progress%2C%20although%20this%20work%20is%20far%20from%20the%20grand%20goal%20toward%0Aopen-world%20and%20dynamic%20wild%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15119v1&entry.124074799=Read"},
{"title": "Fast ODE-based Sampling for Diffusion Models in Around 5 Steps", "author": "Zhenyu Zhou and Defang Chen and Can Wang and Chun Chen", "abstract": "  Sampling from diffusion models can be treated as solving the corresponding\nordinary differential equations (ODEs), with the aim of obtaining an accurate\nsolution with as few number of function evaluations (NFE) as possible.\nRecently, various fast samplers utilizing higher-order ODE solvers have emerged\nand achieved better performance than the initial first-order one. However,\nthese numerical methods inherently result in certain approximation errors,\nwhich significantly degrades sample quality with extremely small NFE (e.g.,\naround 5). In contrast, based on the geometric observation that each sampling\ntrajectory almost lies in a two-dimensional subspace embedded in the ambient\nspace, we propose Approximate MEan-Direction Solver (AMED-Solver) that\neliminates truncation errors by directly learning the mean direction for fast\ndiffusion sampling. Besides, our method can be easily used as a plugin to\nfurther improve existing ODE-based samplers. Extensive experiments on image\nsynthesis with the resolution ranging from 32 to 512 demonstrate the\neffectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10,\n10.74 FID on ImageNet 64$\\times$64, and 13.20 FID on LSUN Bedroom. Our code is\navailable at https://github.com/zju-pi/diff-sampler.\n", "link": "http://arxiv.org/abs/2312.00094v2", "date": "2024-03-22", "relevancy": 2.0446, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5395}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.496}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4889}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fast%20ODE-based%20Sampling%20for%20Diffusion%20Models%20in%20Around%205%20Steps&body=Title%3A%20Fast%20ODE-based%20Sampling%20for%20Diffusion%20Models%20in%20Around%205%20Steps%0AAuthor%3A%20Zhenyu%20Zhou%20and%20Defang%20Chen%20and%20Can%20Wang%20and%20Chun%20Chen%0AAbstract%3A%20%20%20Sampling%20from%20diffusion%20models%20can%20be%20treated%20as%20solving%20the%20corresponding%0Aordinary%20differential%20equations%20%28ODEs%29%2C%20with%20the%20aim%20of%20obtaining%20an%20accurate%0Asolution%20with%20as%20few%20number%20of%20function%20evaluations%20%28NFE%29%20as%20possible.%0ARecently%2C%20various%20fast%20samplers%20utilizing%20higher-order%20ODE%20solvers%20have%20emerged%0Aand%20achieved%20better%20performance%20than%20the%20initial%20first-order%20one.%20However%2C%0Athese%20numerical%20methods%20inherently%20result%20in%20certain%20approximation%20errors%2C%0Awhich%20significantly%20degrades%20sample%20quality%20with%20extremely%20small%20NFE%20%28e.g.%2C%0Aaround%205%29.%20In%20contrast%2C%20based%20on%20the%20geometric%20observation%20that%20each%20sampling%0Atrajectory%20almost%20lies%20in%20a%20two-dimensional%20subspace%20embedded%20in%20the%20ambient%0Aspace%2C%20we%20propose%20Approximate%20MEan-Direction%20Solver%20%28AMED-Solver%29%20that%0Aeliminates%20truncation%20errors%20by%20directly%20learning%20the%20mean%20direction%20for%20fast%0Adiffusion%20sampling.%20Besides%2C%20our%20method%20can%20be%20easily%20used%20as%20a%20plugin%20to%0Afurther%20improve%20existing%20ODE-based%20samplers.%20Extensive%20experiments%20on%20image%0Asynthesis%20with%20the%20resolution%20ranging%20from%2032%20to%20512%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%20With%20only%205%20NFE%2C%20we%20achieve%206.61%20FID%20on%20CIFAR-10%2C%0A10.74%20FID%20on%20ImageNet%2064%24%5Ctimes%2464%2C%20and%2013.20%20FID%20on%20LSUN%20Bedroom.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/zju-pi/diff-sampler.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00094v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20ODE-based%20Sampling%20for%20Diffusion%20Models%20in%20Around%205%20Steps&entry.906535625=Zhenyu%20Zhou%20and%20Defang%20Chen%20and%20Can%20Wang%20and%20Chun%20Chen&entry.1292438233=%20%20Sampling%20from%20diffusion%20models%20can%20be%20treated%20as%20solving%20the%20corresponding%0Aordinary%20differential%20equations%20%28ODEs%29%2C%20with%20the%20aim%20of%20obtaining%20an%20accurate%0Asolution%20with%20as%20few%20number%20of%20function%20evaluations%20%28NFE%29%20as%20possible.%0ARecently%2C%20various%20fast%20samplers%20utilizing%20higher-order%20ODE%20solvers%20have%20emerged%0Aand%20achieved%20better%20performance%20than%20the%20initial%20first-order%20one.%20However%2C%0Athese%20numerical%20methods%20inherently%20result%20in%20certain%20approximation%20errors%2C%0Awhich%20significantly%20degrades%20sample%20quality%20with%20extremely%20small%20NFE%20%28e.g.%2C%0Aaround%205%29.%20In%20contrast%2C%20based%20on%20the%20geometric%20observation%20that%20each%20sampling%0Atrajectory%20almost%20lies%20in%20a%20two-dimensional%20subspace%20embedded%20in%20the%20ambient%0Aspace%2C%20we%20propose%20Approximate%20MEan-Direction%20Solver%20%28AMED-Solver%29%20that%0Aeliminates%20truncation%20errors%20by%20directly%20learning%20the%20mean%20direction%20for%20fast%0Adiffusion%20sampling.%20Besides%2C%20our%20method%20can%20be%20easily%20used%20as%20a%20plugin%20to%0Afurther%20improve%20existing%20ODE-based%20samplers.%20Extensive%20experiments%20on%20image%0Asynthesis%20with%20the%20resolution%20ranging%20from%2032%20to%20512%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%20With%20only%205%20NFE%2C%20we%20achieve%206.61%20FID%20on%20CIFAR-10%2C%0A10.74%20FID%20on%20ImageNet%2064%24%5Ctimes%2464%2C%20and%2013.20%20FID%20on%20LSUN%20Bedroom.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/zju-pi/diff-sampler.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00094v2&entry.124074799=Read"},
{"title": "Federated Bayesian Deep Learning: The Application of Statistical\n  Aggregation Methods to Bayesian Models", "author": "John Fischer and Marko Orescanin and Justin Loomis and Patrick McClure", "abstract": "  Federated learning (FL) is an approach to training machine learning models\nthat takes advantage of multiple distributed datasets while maintaining data\nprivacy and reducing communication costs associated with sharing local\ndatasets. Aggregation strategies have been developed to pool or fuse the\nweights and biases of distributed deterministic models; however, modern\ndeterministic deep learning (DL) models are often poorly calibrated and lack\nthe ability to communicate a measure of epistemic uncertainty in prediction,\nwhich is desirable for remote sensing platforms and safety-critical\napplications. Conversely, Bayesian DL models are often well calibrated and\ncapable of quantifying and communicating a measure of epistemic uncertainty\nalong with a competitive prediction accuracy. Unfortunately, because the\nweights and biases in Bayesian DL models are defined by a probability\ndistribution, simple application of the aggregation methods associated with FL\nschemes for deterministic models is either impossible or results in sub-optimal\nperformance. In this work, we use independent and identically distributed (IID)\nand non-IID partitions of the CIFAR-10 dataset and a fully variational\nResNet-20 architecture to analyze six different aggregation strategies for\nBayesian DL models. Additionally, we analyze the traditional federated\naveraging approach applied to an approximate Bayesian Monte Carlo dropout model\nas a lightweight alternative to more complex variational inference methods in\nFL. We show that aggregation strategy is a key hyperparameter in the design of\na Bayesian FL system with downstream effects on accuracy, calibration,\nuncertainty quantification, training stability, and client compute\nrequirements.\n", "link": "http://arxiv.org/abs/2403.15263v1", "date": "2024-03-22", "relevancy": 2.0422, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5162}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4917}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Federated%20Bayesian%20Deep%20Learning%3A%20The%20Application%20of%20Statistical%0A%20%20Aggregation%20Methods%20to%20Bayesian%20Models&body=Title%3A%20Federated%20Bayesian%20Deep%20Learning%3A%20The%20Application%20of%20Statistical%0A%20%20Aggregation%20Methods%20to%20Bayesian%20Models%0AAuthor%3A%20John%20Fischer%20and%20Marko%20Orescanin%20and%20Justin%20Loomis%20and%20Patrick%20McClure%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20an%20approach%20to%20training%20machine%20learning%20models%0Athat%20takes%20advantage%20of%20multiple%20distributed%20datasets%20while%20maintaining%20data%0Aprivacy%20and%20reducing%20communication%20costs%20associated%20with%20sharing%20local%0Adatasets.%20Aggregation%20strategies%20have%20been%20developed%20to%20pool%20or%20fuse%20the%0Aweights%20and%20biases%20of%20distributed%20deterministic%20models%3B%20however%2C%20modern%0Adeterministic%20deep%20learning%20%28DL%29%20models%20are%20often%20poorly%20calibrated%20and%20lack%0Athe%20ability%20to%20communicate%20a%20measure%20of%20epistemic%20uncertainty%20in%20prediction%2C%0Awhich%20is%20desirable%20for%20remote%20sensing%20platforms%20and%20safety-critical%0Aapplications.%20Conversely%2C%20Bayesian%20DL%20models%20are%20often%20well%20calibrated%20and%0Acapable%20of%20quantifying%20and%20communicating%20a%20measure%20of%20epistemic%20uncertainty%0Aalong%20with%20a%20competitive%20prediction%20accuracy.%20Unfortunately%2C%20because%20the%0Aweights%20and%20biases%20in%20Bayesian%20DL%20models%20are%20defined%20by%20a%20probability%0Adistribution%2C%20simple%20application%20of%20the%20aggregation%20methods%20associated%20with%20FL%0Aschemes%20for%20deterministic%20models%20is%20either%20impossible%20or%20results%20in%20sub-optimal%0Aperformance.%20In%20this%20work%2C%20we%20use%20independent%20and%20identically%20distributed%20%28IID%29%0Aand%20non-IID%20partitions%20of%20the%20CIFAR-10%20dataset%20and%20a%20fully%20variational%0AResNet-20%20architecture%20to%20analyze%20six%20different%20aggregation%20strategies%20for%0ABayesian%20DL%20models.%20Additionally%2C%20we%20analyze%20the%20traditional%20federated%0Aaveraging%20approach%20applied%20to%20an%20approximate%20Bayesian%20Monte%20Carlo%20dropout%20model%0Aas%20a%20lightweight%20alternative%20to%20more%20complex%20variational%20inference%20methods%20in%0AFL.%20We%20show%20that%20aggregation%20strategy%20is%20a%20key%20hyperparameter%20in%20the%20design%20of%0Aa%20Bayesian%20FL%20system%20with%20downstream%20effects%20on%20accuracy%2C%20calibration%2C%0Auncertainty%20quantification%2C%20training%20stability%2C%20and%20client%20compute%0Arequirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15263v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Bayesian%20Deep%20Learning%3A%20The%20Application%20of%20Statistical%0A%20%20Aggregation%20Methods%20to%20Bayesian%20Models&entry.906535625=John%20Fischer%20and%20Marko%20Orescanin%20and%20Justin%20Loomis%20and%20Patrick%20McClure&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20an%20approach%20to%20training%20machine%20learning%20models%0Athat%20takes%20advantage%20of%20multiple%20distributed%20datasets%20while%20maintaining%20data%0Aprivacy%20and%20reducing%20communication%20costs%20associated%20with%20sharing%20local%0Adatasets.%20Aggregation%20strategies%20have%20been%20developed%20to%20pool%20or%20fuse%20the%0Aweights%20and%20biases%20of%20distributed%20deterministic%20models%3B%20however%2C%20modern%0Adeterministic%20deep%20learning%20%28DL%29%20models%20are%20often%20poorly%20calibrated%20and%20lack%0Athe%20ability%20to%20communicate%20a%20measure%20of%20epistemic%20uncertainty%20in%20prediction%2C%0Awhich%20is%20desirable%20for%20remote%20sensing%20platforms%20and%20safety-critical%0Aapplications.%20Conversely%2C%20Bayesian%20DL%20models%20are%20often%20well%20calibrated%20and%0Acapable%20of%20quantifying%20and%20communicating%20a%20measure%20of%20epistemic%20uncertainty%0Aalong%20with%20a%20competitive%20prediction%20accuracy.%20Unfortunately%2C%20because%20the%0Aweights%20and%20biases%20in%20Bayesian%20DL%20models%20are%20defined%20by%20a%20probability%0Adistribution%2C%20simple%20application%20of%20the%20aggregation%20methods%20associated%20with%20FL%0Aschemes%20for%20deterministic%20models%20is%20either%20impossible%20or%20results%20in%20sub-optimal%0Aperformance.%20In%20this%20work%2C%20we%20use%20independent%20and%20identically%20distributed%20%28IID%29%0Aand%20non-IID%20partitions%20of%20the%20CIFAR-10%20dataset%20and%20a%20fully%20variational%0AResNet-20%20architecture%20to%20analyze%20six%20different%20aggregation%20strategies%20for%0ABayesian%20DL%20models.%20Additionally%2C%20we%20analyze%20the%20traditional%20federated%0Aaveraging%20approach%20applied%20to%20an%20approximate%20Bayesian%20Monte%20Carlo%20dropout%20model%0Aas%20a%20lightweight%20alternative%20to%20more%20complex%20variational%20inference%20methods%20in%0AFL.%20We%20show%20that%20aggregation%20strategy%20is%20a%20key%20hyperparameter%20in%20the%20design%20of%0Aa%20Bayesian%20FL%20system%20with%20downstream%20effects%20on%20accuracy%2C%20calibration%2C%0Auncertainty%20quantification%2C%20training%20stability%2C%20and%20client%20compute%0Arequirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15263v1&entry.124074799=Read"},
{"title": "CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object\n  Detection under Unknown Degradations", "author": "Yuwei Zhang and Yan Wu and Yanming Liu and Xinyue Peng", "abstract": "  Object detection methods under known single degradations have been\nextensively investigated. However, existing approaches require prior knowledge\nof the degradation type and train a separate model for each, limiting their\npractical applications in unpredictable environments. To address this\nchallenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,\nCPA-Enhancer, for object detection under unknown degradations. Specifically,\nCPA-Enhancer progressively adapts its enhancement strategy under the\nstep-by-step guidance of CoT prompts, that encode degradation-related\ninformation. To the best of our knowledge, it's the first work that exploits\nCoT prompting for object detection tasks. Overall, CPA-Enhancer is a\nplug-and-play enhancement model that can be integrated into any generic\ndetectors to achieve substantial gains on degraded images, without knowing the\ndegradation type priorly. Experimental results demonstrate that CPA-Enhancer\nnot only sets the new state of the art for object detection but also boosts the\nperformance of other downstream vision tasks under unknown degradations.\n", "link": "http://arxiv.org/abs/2403.11220v3", "date": "2024-03-22", "relevancy": 2.0313, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5106}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5065}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5042}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CPA-Enhancer%3A%20Chain-of-Thought%20Prompted%20Adaptive%20Enhancer%20for%20Object%0A%20%20Detection%20under%20Unknown%20Degradations&body=Title%3A%20CPA-Enhancer%3A%20Chain-of-Thought%20Prompted%20Adaptive%20Enhancer%20for%20Object%0A%20%20Detection%20under%20Unknown%20Degradations%0AAuthor%3A%20Yuwei%20Zhang%20and%20Yan%20Wu%20and%20Yanming%20Liu%20and%20Xinyue%20Peng%0AAbstract%3A%20%20%20Object%20detection%20methods%20under%20known%20single%20degradations%20have%20been%0Aextensively%20investigated.%20However%2C%20existing%20approaches%20require%20prior%20knowledge%0Aof%20the%20degradation%20type%20and%20train%20a%20separate%20model%20for%20each%2C%20limiting%20their%0Apractical%20applications%20in%20unpredictable%20environments.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20chain-of-thought%20%28CoT%29%20prompted%20adaptive%20enhancer%2C%0ACPA-Enhancer%2C%20for%20object%20detection%20under%20unknown%20degradations.%20Specifically%2C%0ACPA-Enhancer%20progressively%20adapts%20its%20enhancement%20strategy%20under%20the%0Astep-by-step%20guidance%20of%20CoT%20prompts%2C%20that%20encode%20degradation-related%0Ainformation.%20To%20the%20best%20of%20our%20knowledge%2C%20it%27s%20the%20first%20work%20that%20exploits%0ACoT%20prompting%20for%20object%20detection%20tasks.%20Overall%2C%20CPA-Enhancer%20is%20a%0Aplug-and-play%20enhancement%20model%20that%20can%20be%20integrated%20into%20any%20generic%0Adetectors%20to%20achieve%20substantial%20gains%20on%20degraded%20images%2C%20without%20knowing%20the%0Adegradation%20type%20priorly.%20Experimental%20results%20demonstrate%20that%20CPA-Enhancer%0Anot%20only%20sets%20the%20new%20state%20of%20the%20art%20for%20object%20detection%20but%20also%20boosts%20the%0Aperformance%20of%20other%20downstream%20vision%20tasks%20under%20unknown%20degradations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11220v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CPA-Enhancer%3A%20Chain-of-Thought%20Prompted%20Adaptive%20Enhancer%20for%20Object%0A%20%20Detection%20under%20Unknown%20Degradations&entry.906535625=Yuwei%20Zhang%20and%20Yan%20Wu%20and%20Yanming%20Liu%20and%20Xinyue%20Peng&entry.1292438233=%20%20Object%20detection%20methods%20under%20known%20single%20degradations%20have%20been%0Aextensively%20investigated.%20However%2C%20existing%20approaches%20require%20prior%20knowledge%0Aof%20the%20degradation%20type%20and%20train%20a%20separate%20model%20for%20each%2C%20limiting%20their%0Apractical%20applications%20in%20unpredictable%20environments.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20chain-of-thought%20%28CoT%29%20prompted%20adaptive%20enhancer%2C%0ACPA-Enhancer%2C%20for%20object%20detection%20under%20unknown%20degradations.%20Specifically%2C%0ACPA-Enhancer%20progressively%20adapts%20its%20enhancement%20strategy%20under%20the%0Astep-by-step%20guidance%20of%20CoT%20prompts%2C%20that%20encode%20degradation-related%0Ainformation.%20To%20the%20best%20of%20our%20knowledge%2C%20it%27s%20the%20first%20work%20that%20exploits%0ACoT%20prompting%20for%20object%20detection%20tasks.%20Overall%2C%20CPA-Enhancer%20is%20a%0Aplug-and-play%20enhancement%20model%20that%20can%20be%20integrated%20into%20any%20generic%0Adetectors%20to%20achieve%20substantial%20gains%20on%20degraded%20images%2C%20without%20knowing%20the%0Adegradation%20type%20priorly.%20Experimental%20results%20demonstrate%20that%20CPA-Enhancer%0Anot%20only%20sets%20the%20new%20state%20of%20the%20art%20for%20object%20detection%20but%20also%20boosts%20the%0Aperformance%20of%20other%20downstream%20vision%20tasks%20under%20unknown%20degradations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11220v3&entry.124074799=Read"},
{"title": "FunQA: Towards Surprising Video Comprehension", "author": "Binzhu Xie and Sicheng Zhang and Zitang Zhou and Bo Li and Yuanhan Zhang and Jack Hessel and Jingkang Yang and Ziwei Liu", "abstract": "  Surprising videos, such as funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question-answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Moreover, we propose\nFunMentor, an agent designed for Vision-Language Models (VLMs) that uses\nmulti-turn dialogues to enhance models' understanding of counter-intuitiveness.\nExtensive experiments with existing VLMs demonstrate the effectiveness of\nFunMentor and reveal significant performance gaps for the FunQA videos across\nspatial-temporal reasoning, visual-centered reasoning, and free-text\ngeneration.\n", "link": "http://arxiv.org/abs/2306.14899v2", "date": "2024-03-22", "relevancy": 2.0305, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5152}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5115}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4985}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FunQA%3A%20Towards%20Surprising%20Video%20Comprehension&body=Title%3A%20FunQA%3A%20Towards%20Surprising%20Video%20Comprehension%0AAuthor%3A%20Binzhu%20Xie%20and%20Sicheng%20Zhang%20and%20Zitang%20Zhou%20and%20Bo%20Li%20and%20Yuanhan%20Zhang%20and%20Jack%20Hessel%20and%20Jingkang%20Yang%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Surprising%20videos%2C%20such%20as%20funny%20clips%2C%20creative%20performances%2C%20or%20visual%0Aillusions%2C%20attract%20significant%20attention.%20Enjoyment%20of%20these%20videos%20is%20not%0Asimply%20a%20response%20to%20visual%20stimuli%3B%20rather%2C%20it%20hinges%20on%20the%20human%20capacity%20to%0Aunderstand%20%28and%20appreciate%29%20commonsense%20violations%20depicted%20in%20these%20videos.%20We%0Aintroduce%20FunQA%2C%20a%20challenging%20video%20question-answering%20%28QA%29%20dataset%0Aspecifically%20designed%20to%20evaluate%20and%20enhance%20the%20depth%20of%20video%20reasoning%0Abased%20on%20counter-intuitive%20and%20fun%20videos.%20Unlike%20most%20video%20QA%20benchmarks%0Awhich%20focus%20on%20less%20surprising%20contexts%2C%20e.g.%2C%20cooking%20or%20instructional%20videos%2C%0AFunQA%20covers%20three%20previously%20unexplored%20types%20of%20surprising%20videos%3A%201%29%0AHumorQA%2C%202%29%20CreativeQA%2C%20and%203%29%20MagicQA.%20For%20each%20subset%2C%20we%20establish%20rigorous%0AQA%20tasks%20designed%20to%20assess%20the%20model%27s%20capability%20in%20counter-intuitive%0Atimestamp%20localization%2C%20detailed%20video%20description%2C%20and%20reasoning%20around%0Acounter-intuitiveness.%20We%20also%20pose%20higher-level%20tasks%2C%20such%20as%20attributing%20a%0Afitting%20and%20vivid%20title%20to%20the%20video%20and%20scoring%20the%20video%20creativity.%20In%0Atotal%2C%20the%20FunQA%20benchmark%20consists%20of%20312K%20free-text%20QA%20pairs%20derived%20from%0A4.3K%20video%20clips%2C%20spanning%20a%20total%20of%2024%20video%20hours.%20Moreover%2C%20we%20propose%0AFunMentor%2C%20an%20agent%20designed%20for%20Vision-Language%20Models%20%28VLMs%29%20that%20uses%0Amulti-turn%20dialogues%20to%20enhance%20models%27%20understanding%20of%20counter-intuitiveness.%0AExtensive%20experiments%20with%20existing%20VLMs%20demonstrate%20the%20effectiveness%20of%0AFunMentor%20and%20reveal%20significant%20performance%20gaps%20for%20the%20FunQA%20videos%20across%0Aspatial-temporal%20reasoning%2C%20visual-centered%20reasoning%2C%20and%20free-text%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.14899v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FunQA%3A%20Towards%20Surprising%20Video%20Comprehension&entry.906535625=Binzhu%20Xie%20and%20Sicheng%20Zhang%20and%20Zitang%20Zhou%20and%20Bo%20Li%20and%20Yuanhan%20Zhang%20and%20Jack%20Hessel%20and%20Jingkang%20Yang%20and%20Ziwei%20Liu&entry.1292438233=%20%20Surprising%20videos%2C%20such%20as%20funny%20clips%2C%20creative%20performances%2C%20or%20visual%0Aillusions%2C%20attract%20significant%20attention.%20Enjoyment%20of%20these%20videos%20is%20not%0Asimply%20a%20response%20to%20visual%20stimuli%3B%20rather%2C%20it%20hinges%20on%20the%20human%20capacity%20to%0Aunderstand%20%28and%20appreciate%29%20commonsense%20violations%20depicted%20in%20these%20videos.%20We%0Aintroduce%20FunQA%2C%20a%20challenging%20video%20question-answering%20%28QA%29%20dataset%0Aspecifically%20designed%20to%20evaluate%20and%20enhance%20the%20depth%20of%20video%20reasoning%0Abased%20on%20counter-intuitive%20and%20fun%20videos.%20Unlike%20most%20video%20QA%20benchmarks%0Awhich%20focus%20on%20less%20surprising%20contexts%2C%20e.g.%2C%20cooking%20or%20instructional%20videos%2C%0AFunQA%20covers%20three%20previously%20unexplored%20types%20of%20surprising%20videos%3A%201%29%0AHumorQA%2C%202%29%20CreativeQA%2C%20and%203%29%20MagicQA.%20For%20each%20subset%2C%20we%20establish%20rigorous%0AQA%20tasks%20designed%20to%20assess%20the%20model%27s%20capability%20in%20counter-intuitive%0Atimestamp%20localization%2C%20detailed%20video%20description%2C%20and%20reasoning%20around%0Acounter-intuitiveness.%20We%20also%20pose%20higher-level%20tasks%2C%20such%20as%20attributing%20a%0Afitting%20and%20vivid%20title%20to%20the%20video%20and%20scoring%20the%20video%20creativity.%20In%0Atotal%2C%20the%20FunQA%20benchmark%20consists%20of%20312K%20free-text%20QA%20pairs%20derived%20from%0A4.3K%20video%20clips%2C%20spanning%20a%20total%20of%2024%20video%20hours.%20Moreover%2C%20we%20propose%0AFunMentor%2C%20an%20agent%20designed%20for%20Vision-Language%20Models%20%28VLMs%29%20that%20uses%0Amulti-turn%20dialogues%20to%20enhance%20models%27%20understanding%20of%20counter-intuitiveness.%0AExtensive%20experiments%20with%20existing%20VLMs%20demonstrate%20the%20effectiveness%20of%0AFunMentor%20and%20reveal%20significant%20performance%20gaps%20for%20the%20FunQA%20videos%20across%0Aspatial-temporal%20reasoning%2C%20visual-centered%20reasoning%2C%20and%20free-text%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.14899v2&entry.124074799=Read"},
{"title": "Robust optimization for adversarial learning with finite sample\n  complexity guarantees", "author": "Andr\u00e9 Bertolace and Konstatinos Gatsis and Kostas Margellos", "abstract": "  Decision making and learning in the presence of uncertainty has attracted\nsignificant attention in view of the increasing need to achieve robust and\nreliable operations. In the case where uncertainty stems from the presence of\nadversarial attacks this need is becoming more prominent. In this paper we\nfocus on linear and nonlinear classification problems and propose a novel\nadversarial training method for robust classifiers, inspired by Support Vector\nMachine (SVM) margins. We view robustness under a data driven lens, and derive\nfinite sample complexity bounds for both linear and non-linear classifiers in\nbinary and multi-class scenarios. Notably, our bounds match natural\nclassifiers' complexity. Our algorithm minimizes a worst-case surrogate loss\nusing Linear Programming (LP) and Second Order Cone Programming (SOCP) for\nlinear and non-linear models. Numerical experiments on the benchmark MNIST and\nCIFAR10 datasets show our approach's comparable performance to state-of-the-art\nmethods, without needing adversarial examples during training. Our work offers\na comprehensive framework for enhancing binary linear and non-linear classifier\nrobustness, embedding robustness in learning under the presence of adversaries.\n", "link": "http://arxiv.org/abs/2403.15207v1", "date": "2024-03-22", "relevancy": 2.0302, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.542}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4834}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4819}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20optimization%20for%20adversarial%20learning%20with%20finite%20sample%0A%20%20complexity%20guarantees&body=Title%3A%20Robust%20optimization%20for%20adversarial%20learning%20with%20finite%20sample%0A%20%20complexity%20guarantees%0AAuthor%3A%20Andr%C3%A9%20Bertolace%20and%20Konstatinos%20Gatsis%20and%20Kostas%20Margellos%0AAbstract%3A%20%20%20Decision%20making%20and%20learning%20in%20the%20presence%20of%20uncertainty%20has%20attracted%0Asignificant%20attention%20in%20view%20of%20the%20increasing%20need%20to%20achieve%20robust%20and%0Areliable%20operations.%20In%20the%20case%20where%20uncertainty%20stems%20from%20the%20presence%20of%0Aadversarial%20attacks%20this%20need%20is%20becoming%20more%20prominent.%20In%20this%20paper%20we%0Afocus%20on%20linear%20and%20nonlinear%20classification%20problems%20and%20propose%20a%20novel%0Aadversarial%20training%20method%20for%20robust%20classifiers%2C%20inspired%20by%20Support%20Vector%0AMachine%20%28SVM%29%20margins.%20We%20view%20robustness%20under%20a%20data%20driven%20lens%2C%20and%20derive%0Afinite%20sample%20complexity%20bounds%20for%20both%20linear%20and%20non-linear%20classifiers%20in%0Abinary%20and%20multi-class%20scenarios.%20Notably%2C%20our%20bounds%20match%20natural%0Aclassifiers%27%20complexity.%20Our%20algorithm%20minimizes%20a%20worst-case%20surrogate%20loss%0Ausing%20Linear%20Programming%20%28LP%29%20and%20Second%20Order%20Cone%20Programming%20%28SOCP%29%20for%0Alinear%20and%20non-linear%20models.%20Numerical%20experiments%20on%20the%20benchmark%20MNIST%20and%0ACIFAR10%20datasets%20show%20our%20approach%27s%20comparable%20performance%20to%20state-of-the-art%0Amethods%2C%20without%20needing%20adversarial%20examples%20during%20training.%20Our%20work%20offers%0Aa%20comprehensive%20framework%20for%20enhancing%20binary%20linear%20and%20non-linear%20classifier%0Arobustness%2C%20embedding%20robustness%20in%20learning%20under%20the%20presence%20of%20adversaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15207v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20optimization%20for%20adversarial%20learning%20with%20finite%20sample%0A%20%20complexity%20guarantees&entry.906535625=Andr%C3%A9%20Bertolace%20and%20Konstatinos%20Gatsis%20and%20Kostas%20Margellos&entry.1292438233=%20%20Decision%20making%20and%20learning%20in%20the%20presence%20of%20uncertainty%20has%20attracted%0Asignificant%20attention%20in%20view%20of%20the%20increasing%20need%20to%20achieve%20robust%20and%0Areliable%20operations.%20In%20the%20case%20where%20uncertainty%20stems%20from%20the%20presence%20of%0Aadversarial%20attacks%20this%20need%20is%20becoming%20more%20prominent.%20In%20this%20paper%20we%0Afocus%20on%20linear%20and%20nonlinear%20classification%20problems%20and%20propose%20a%20novel%0Aadversarial%20training%20method%20for%20robust%20classifiers%2C%20inspired%20by%20Support%20Vector%0AMachine%20%28SVM%29%20margins.%20We%20view%20robustness%20under%20a%20data%20driven%20lens%2C%20and%20derive%0Afinite%20sample%20complexity%20bounds%20for%20both%20linear%20and%20non-linear%20classifiers%20in%0Abinary%20and%20multi-class%20scenarios.%20Notably%2C%20our%20bounds%20match%20natural%0Aclassifiers%27%20complexity.%20Our%20algorithm%20minimizes%20a%20worst-case%20surrogate%20loss%0Ausing%20Linear%20Programming%20%28LP%29%20and%20Second%20Order%20Cone%20Programming%20%28SOCP%29%20for%0Alinear%20and%20non-linear%20models.%20Numerical%20experiments%20on%20the%20benchmark%20MNIST%20and%0ACIFAR10%20datasets%20show%20our%20approach%27s%20comparable%20performance%20to%20state-of-the-art%0Amethods%2C%20without%20needing%20adversarial%20examples%20during%20training.%20Our%20work%20offers%0Aa%20comprehensive%20framework%20for%20enhancing%20binary%20linear%20and%20non-linear%20classifier%0Arobustness%2C%20embedding%20robustness%20in%20learning%20under%20the%20presence%20of%20adversaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15207v1&entry.124074799=Read"},
{"title": "Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in\n  Safety Monitoring Applications", "author": "V\u00edt Kr\u00e1tk\u00fd and Giuseppe Silano and Matou\u0161 Vrba and Christos Papaioannidis and Ioannis Mademlis and Robert P\u011bni\u010dka and Ioannis Pitas and Martin Saska", "abstract": "  This paper presents a formation control approach for contactless\ngesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor\nUnmanned Aerial Vehicles (UAVs) and a human worker. The approach is intended\nfor monitoring the safety of human workers, especially those working at\nheights. In the proposed dynamic formation scheme, one UAV acts as the leader\nof the formation and is equipped with sensors for human worker detection and\ngesture recognition. The follower UAVs maintain a predetermined formation\nrelative to the worker's position, thereby providing additional perspectives of\nthe monitored scene. Hand gestures allow the human worker to specify movements\nand action commands for the UAV team and initiate other mission-related\ncommands without the need for an additional communication channel or specific\nmarkers. Together with a novel unified human detection and tracking algorithm,\nhuman pose estimation approach and gesture detection pipeline, the proposed\napproach forms a first instance of an HSI system incorporating all these\nmodules onboard real-world UAVs. Simulations and field experiments with three\nUAVs and a human worker in a mock-up scenario showcase the effectiveness and\nresponsiveness of the proposed approach.\n", "link": "http://arxiv.org/abs/2403.15333v1", "date": "2024-03-22", "relevancy": 2.0201, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5409}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4894}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4754}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gesture-Controlled%20Aerial%20Robot%20Formation%20for%20Human-Swarm%20Interaction%20in%0A%20%20Safety%20Monitoring%20Applications&body=Title%3A%20Gesture-Controlled%20Aerial%20Robot%20Formation%20for%20Human-Swarm%20Interaction%20in%0A%20%20Safety%20Monitoring%20Applications%0AAuthor%3A%20V%C3%ADt%20Kr%C3%A1tk%C3%BD%20and%20Giuseppe%20Silano%20and%20Matou%C5%A1%20Vrba%20and%20Christos%20Papaioannidis%20and%20Ioannis%20Mademlis%20and%20Robert%20P%C4%9Bni%C4%8Dka%20and%20Ioannis%20Pitas%20and%20Martin%20Saska%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20formation%20control%20approach%20for%20contactless%0Agesture-based%20Human-Swarm%20Interaction%20%28HSI%29%20between%20a%20team%20of%20multi-rotor%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20a%20human%20worker.%20The%20approach%20is%20intended%0Afor%20monitoring%20the%20safety%20of%20human%20workers%2C%20especially%20those%20working%20at%0Aheights.%20In%20the%20proposed%20dynamic%20formation%20scheme%2C%20one%20UAV%20acts%20as%20the%20leader%0Aof%20the%20formation%20and%20is%20equipped%20with%20sensors%20for%20human%20worker%20detection%20and%0Agesture%20recognition.%20The%20follower%20UAVs%20maintain%20a%20predetermined%20formation%0Arelative%20to%20the%20worker%27s%20position%2C%20thereby%20providing%20additional%20perspectives%20of%0Athe%20monitored%20scene.%20Hand%20gestures%20allow%20the%20human%20worker%20to%20specify%20movements%0Aand%20action%20commands%20for%20the%20UAV%20team%20and%20initiate%20other%20mission-related%0Acommands%20without%20the%20need%20for%20an%20additional%20communication%20channel%20or%20specific%0Amarkers.%20Together%20with%20a%20novel%20unified%20human%20detection%20and%20tracking%20algorithm%2C%0Ahuman%20pose%20estimation%20approach%20and%20gesture%20detection%20pipeline%2C%20the%20proposed%0Aapproach%20forms%20a%20first%20instance%20of%20an%20HSI%20system%20incorporating%20all%20these%0Amodules%20onboard%20real-world%20UAVs.%20Simulations%20and%20field%20experiments%20with%20three%0AUAVs%20and%20a%20human%20worker%20in%20a%20mock-up%20scenario%20showcase%20the%20effectiveness%20and%0Aresponsiveness%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15333v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gesture-Controlled%20Aerial%20Robot%20Formation%20for%20Human-Swarm%20Interaction%20in%0A%20%20Safety%20Monitoring%20Applications&entry.906535625=V%C3%ADt%20Kr%C3%A1tk%C3%BD%20and%20Giuseppe%20Silano%20and%20Matou%C5%A1%20Vrba%20and%20Christos%20Papaioannidis%20and%20Ioannis%20Mademlis%20and%20Robert%20P%C4%9Bni%C4%8Dka%20and%20Ioannis%20Pitas%20and%20Martin%20Saska&entry.1292438233=%20%20This%20paper%20presents%20a%20formation%20control%20approach%20for%20contactless%0Agesture-based%20Human-Swarm%20Interaction%20%28HSI%29%20between%20a%20team%20of%20multi-rotor%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20a%20human%20worker.%20The%20approach%20is%20intended%0Afor%20monitoring%20the%20safety%20of%20human%20workers%2C%20especially%20those%20working%20at%0Aheights.%20In%20the%20proposed%20dynamic%20formation%20scheme%2C%20one%20UAV%20acts%20as%20the%20leader%0Aof%20the%20formation%20and%20is%20equipped%20with%20sensors%20for%20human%20worker%20detection%20and%0Agesture%20recognition.%20The%20follower%20UAVs%20maintain%20a%20predetermined%20formation%0Arelative%20to%20the%20worker%27s%20position%2C%20thereby%20providing%20additional%20perspectives%20of%0Athe%20monitored%20scene.%20Hand%20gestures%20allow%20the%20human%20worker%20to%20specify%20movements%0Aand%20action%20commands%20for%20the%20UAV%20team%20and%20initiate%20other%20mission-related%0Acommands%20without%20the%20need%20for%20an%20additional%20communication%20channel%20or%20specific%0Amarkers.%20Together%20with%20a%20novel%20unified%20human%20detection%20and%20tracking%20algorithm%2C%0Ahuman%20pose%20estimation%20approach%20and%20gesture%20detection%20pipeline%2C%20the%20proposed%0Aapproach%20forms%20a%20first%20instance%20of%20an%20HSI%20system%20incorporating%20all%20these%0Amodules%20onboard%20real-world%20UAVs.%20Simulations%20and%20field%20experiments%20with%20three%0AUAVs%20and%20a%20human%20worker%20in%20a%20mock-up%20scenario%20showcase%20the%20effectiveness%20and%0Aresponsiveness%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15333v1&entry.124074799=Read"},
{"title": "SkySense: A Multi-Modal Remote Sensing Foundation Model Towards\n  Universal Interpretation for Earth Observation Imagery", "author": "Xin Guo and Jiangwei Lao and Bo Dang and Yingying Zhang and Lei Yu and Lixiang Ru and Liheng Zhong and Ziyuan Huang and Kang Wu and Dingxiang Hu and Huimei He and Jian Wang and Jingdong Chen and Ming Yang and Yongjun Zhang and Yansheng Li", "abstract": "  Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense\npotential towards a generic model for Earth Observation. Nevertheless, these\nworks primarily focus on a single modality without temporal and geo-context\nmodeling, hampering their capabilities for diverse tasks. In this study, we\npresent SkySense, a generic billion-scale model, pre-trained on a curated\nmulti-modal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal\nsequences. SkySense incorporates a factorized multi-modal spatiotemporal\nencoder taking temporal sequences of optical and Synthetic Aperture Radar (SAR)\ndata as input. This encoder is pre-trained by our proposed Multi-Granularity\nContrastive Learning to learn representations across different modal and\nspatial granularities. To further enhance the RSI representations by the\ngeo-context clue, we introduce Geo-Context Prototype Learning to learn\nregion-aware prototypes upon RSI's multi-modal spatiotemporal features. To our\nbest knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modules\ncan be flexibly combined or used individually to accommodate various tasks. It\ndemonstrates remarkable generalization capabilities on a thorough evaluation\nencompassing 16 datasets over 7 tasks, from single- to multi-modal, static to\ntemporal, and classification to localization. SkySense surpasses 18 recent\nRSFMs in all test scenarios. Specifically, it outperforms the latest models\nsuch as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and\n3.61% on average respectively. We will release the pre-trained weights to\nfacilitate future research and Earth Observation applications.\n", "link": "http://arxiv.org/abs/2312.10115v2", "date": "2024-03-22", "relevancy": 2.0148, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5023}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4873}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SkySense%3A%20A%20Multi-Modal%20Remote%20Sensing%20Foundation%20Model%20Towards%0A%20%20Universal%20Interpretation%20for%20Earth%20Observation%20Imagery&body=Title%3A%20SkySense%3A%20A%20Multi-Modal%20Remote%20Sensing%20Foundation%20Model%20Towards%0A%20%20Universal%20Interpretation%20for%20Earth%20Observation%20Imagery%0AAuthor%3A%20Xin%20Guo%20and%20Jiangwei%20Lao%20and%20Bo%20Dang%20and%20Yingying%20Zhang%20and%20Lei%20Yu%20and%20Lixiang%20Ru%20and%20Liheng%20Zhong%20and%20Ziyuan%20Huang%20and%20Kang%20Wu%20and%20Dingxiang%20Hu%20and%20Huimei%20He%20and%20Jian%20Wang%20and%20Jingdong%20Chen%20and%20Ming%20Yang%20and%20Yongjun%20Zhang%20and%20Yansheng%20Li%0AAbstract%3A%20%20%20Prior%20studies%20on%20Remote%20Sensing%20Foundation%20Model%20%28RSFM%29%20reveal%20immense%0Apotential%20towards%20a%20generic%20model%20for%20Earth%20Observation.%20Nevertheless%2C%20these%0Aworks%20primarily%20focus%20on%20a%20single%20modality%20without%20temporal%20and%20geo-context%0Amodeling%2C%20hampering%20their%20capabilities%20for%20diverse%20tasks.%20In%20this%20study%2C%20we%0Apresent%20SkySense%2C%20a%20generic%20billion-scale%20model%2C%20pre-trained%20on%20a%20curated%0Amulti-modal%20Remote%20Sensing%20Imagery%20%28RSI%29%20dataset%20with%2021.5%20million%20temporal%0Asequences.%20SkySense%20incorporates%20a%20factorized%20multi-modal%20spatiotemporal%0Aencoder%20taking%20temporal%20sequences%20of%20optical%20and%20Synthetic%20Aperture%20Radar%20%28SAR%29%0Adata%20as%20input.%20This%20encoder%20is%20pre-trained%20by%20our%20proposed%20Multi-Granularity%0AContrastive%20Learning%20to%20learn%20representations%20across%20different%20modal%20and%0Aspatial%20granularities.%20To%20further%20enhance%20the%20RSI%20representations%20by%20the%0Ageo-context%20clue%2C%20we%20introduce%20Geo-Context%20Prototype%20Learning%20to%20learn%0Aregion-aware%20prototypes%20upon%20RSI%27s%20multi-modal%20spatiotemporal%20features.%20To%20our%0Abest%20knowledge%2C%20SkySense%20is%20the%20largest%20Multi-Modal%20RSFM%20to%20date%2C%20whose%20modules%0Acan%20be%20flexibly%20combined%20or%20used%20individually%20to%20accommodate%20various%20tasks.%20It%0Ademonstrates%20remarkable%20generalization%20capabilities%20on%20a%20thorough%20evaluation%0Aencompassing%2016%20datasets%20over%207%20tasks%2C%20from%20single-%20to%20multi-modal%2C%20static%20to%0Atemporal%2C%20and%20classification%20to%20localization.%20SkySense%20surpasses%2018%20recent%0ARSFMs%20in%20all%20test%20scenarios.%20Specifically%2C%20it%20outperforms%20the%20latest%20models%0Asuch%20as%20GFM%2C%20SatLas%20and%20Scale-MAE%20by%20a%20large%20margin%2C%20i.e.%2C%202.76%25%2C%203.67%25%20and%0A3.61%25%20on%20average%20respectively.%20We%20will%20release%20the%20pre-trained%20weights%20to%0Afacilitate%20future%20research%20and%20Earth%20Observation%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10115v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkySense%3A%20A%20Multi-Modal%20Remote%20Sensing%20Foundation%20Model%20Towards%0A%20%20Universal%20Interpretation%20for%20Earth%20Observation%20Imagery&entry.906535625=Xin%20Guo%20and%20Jiangwei%20Lao%20and%20Bo%20Dang%20and%20Yingying%20Zhang%20and%20Lei%20Yu%20and%20Lixiang%20Ru%20and%20Liheng%20Zhong%20and%20Ziyuan%20Huang%20and%20Kang%20Wu%20and%20Dingxiang%20Hu%20and%20Huimei%20He%20and%20Jian%20Wang%20and%20Jingdong%20Chen%20and%20Ming%20Yang%20and%20Yongjun%20Zhang%20and%20Yansheng%20Li&entry.1292438233=%20%20Prior%20studies%20on%20Remote%20Sensing%20Foundation%20Model%20%28RSFM%29%20reveal%20immense%0Apotential%20towards%20a%20generic%20model%20for%20Earth%20Observation.%20Nevertheless%2C%20these%0Aworks%20primarily%20focus%20on%20a%20single%20modality%20without%20temporal%20and%20geo-context%0Amodeling%2C%20hampering%20their%20capabilities%20for%20diverse%20tasks.%20In%20this%20study%2C%20we%0Apresent%20SkySense%2C%20a%20generic%20billion-scale%20model%2C%20pre-trained%20on%20a%20curated%0Amulti-modal%20Remote%20Sensing%20Imagery%20%28RSI%29%20dataset%20with%2021.5%20million%20temporal%0Asequences.%20SkySense%20incorporates%20a%20factorized%20multi-modal%20spatiotemporal%0Aencoder%20taking%20temporal%20sequences%20of%20optical%20and%20Synthetic%20Aperture%20Radar%20%28SAR%29%0Adata%20as%20input.%20This%20encoder%20is%20pre-trained%20by%20our%20proposed%20Multi-Granularity%0AContrastive%20Learning%20to%20learn%20representations%20across%20different%20modal%20and%0Aspatial%20granularities.%20To%20further%20enhance%20the%20RSI%20representations%20by%20the%0Ageo-context%20clue%2C%20we%20introduce%20Geo-Context%20Prototype%20Learning%20to%20learn%0Aregion-aware%20prototypes%20upon%20RSI%27s%20multi-modal%20spatiotemporal%20features.%20To%20our%0Abest%20knowledge%2C%20SkySense%20is%20the%20largest%20Multi-Modal%20RSFM%20to%20date%2C%20whose%20modules%0Acan%20be%20flexibly%20combined%20or%20used%20individually%20to%20accommodate%20various%20tasks.%20It%0Ademonstrates%20remarkable%20generalization%20capabilities%20on%20a%20thorough%20evaluation%0Aencompassing%2016%20datasets%20over%207%20tasks%2C%20from%20single-%20to%20multi-modal%2C%20static%20to%0Atemporal%2C%20and%20classification%20to%20localization.%20SkySense%20surpasses%2018%20recent%0ARSFMs%20in%20all%20test%20scenarios.%20Specifically%2C%20it%20outperforms%20the%20latest%20models%0Asuch%20as%20GFM%2C%20SatLas%20and%20Scale-MAE%20by%20a%20large%20margin%2C%20i.e.%2C%202.76%25%2C%203.67%25%20and%0A3.61%25%20on%20average%20respectively.%20We%20will%20release%20the%20pre-trained%20weights%20to%0Afacilitate%20future%20research%20and%20Earth%20Observation%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10115v2&entry.124074799=Read"},
{"title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions", "author": "Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini", "abstract": "  Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.\n", "link": "http://arxiv.org/abs/2403.15246v1", "date": "2024-03-22", "relevancy": 1.7892, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4698}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4445}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4411}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FollowIR%3A%20Evaluating%20and%20Teaching%20Information%20Retrieval%20Models%20to%20Follow%0A%20%20Instructions&body=Title%3A%20FollowIR%3A%20Evaluating%20and%20Teaching%20Information%20Retrieval%20Models%20to%20Follow%0A%20%20Instructions%0AAuthor%3A%20Orion%20Weller%20and%20Benjamin%20Chang%20and%20Sean%20MacAvaney%20and%20Kyle%20Lo%20and%20Arman%20Cohan%20and%20Benjamin%20Van%20Durme%20and%20Dawn%20Lawrie%20and%20Luca%20Soldaini%0AAbstract%3A%20%20%20Modern%20Large%20Language%20Models%20%28LLMs%29%20are%20capable%20of%20following%20long%20and%20complex%0Ainstructions%20that%20enable%20a%20diverse%20amount%20of%20user%20tasks.%20However%2C%20despite%0AInformation%20Retrieval%20%28IR%29%20models%20using%20LLMs%20as%20the%20backbone%20of%20their%0Aarchitectures%2C%20nearly%20all%20of%20them%20still%20only%20take%20queries%20as%20input%2C%20with%20no%0Ainstructions.%20For%20the%20handful%20of%20recent%20models%20that%20do%20take%20instructions%2C%20it%27s%0Aunclear%20how%20they%20use%20them.%20We%20introduce%20our%20dataset%20FollowIR%2C%20which%20contains%20a%0Arigorous%20instruction%20evaluation%20benchmark%20as%20well%20as%20a%20training%20set%20for%20helping%0AIR%20models%20learn%20to%20better%20follow%20real-world%20instructions.%20FollowIR%20builds%20off%0Athe%20long%20history%20of%20the%20TREC%20conferences%3A%20as%20TREC%20provides%20human%20annotators%0Awith%20instructions%20%28also%20known%20as%20narratives%29%20to%20determine%20document%20relevance%2C%0Aso%20should%20IR%20models%20be%20able%20to%20understand%20and%20decide%20relevance%20based%20on%20these%0Adetailed%20instructions.%20Our%20evaluation%20benchmark%20starts%20with%20three%20deeply%20judged%0ATREC%20collections%20and%20alters%20the%20annotator%20instructions%2C%20re-annotating%20relevant%0Adocuments.%20Through%20this%20process%2C%20we%20can%20measure%20how%20well%20IR%20models%20follow%0Ainstructions%2C%20through%20a%20new%20pairwise%20evaluation%20framework.%20Our%20results%20indicate%0Athat%20existing%20retrieval%20models%20fail%20to%20correctly%20use%20instructions%2C%20using%20them%0Afor%20basic%20keywords%20and%20struggling%20to%20understand%20long-form%20information.%20However%2C%0Awe%20show%20that%20it%20is%20possible%20for%20IR%20models%20to%20learn%20to%20follow%20complex%0Ainstructions%3A%20our%20new%20FollowIR-7B%20model%20has%20significant%20improvements%20%28over%2013%25%29%0Aafter%20fine-tuning%20on%20our%20training%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15246v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FollowIR%3A%20Evaluating%20and%20Teaching%20Information%20Retrieval%20Models%20to%20Follow%0A%20%20Instructions&entry.906535625=Orion%20Weller%20and%20Benjamin%20Chang%20and%20Sean%20MacAvaney%20and%20Kyle%20Lo%20and%20Arman%20Cohan%20and%20Benjamin%20Van%20Durme%20and%20Dawn%20Lawrie%20and%20Luca%20Soldaini&entry.1292438233=%20%20Modern%20Large%20Language%20Models%20%28LLMs%29%20are%20capable%20of%20following%20long%20and%20complex%0Ainstructions%20that%20enable%20a%20diverse%20amount%20of%20user%20tasks.%20However%2C%20despite%0AInformation%20Retrieval%20%28IR%29%20models%20using%20LLMs%20as%20the%20backbone%20of%20their%0Aarchitectures%2C%20nearly%20all%20of%20them%20still%20only%20take%20queries%20as%20input%2C%20with%20no%0Ainstructions.%20For%20the%20handful%20of%20recent%20models%20that%20do%20take%20instructions%2C%20it%27s%0Aunclear%20how%20they%20use%20them.%20We%20introduce%20our%20dataset%20FollowIR%2C%20which%20contains%20a%0Arigorous%20instruction%20evaluation%20benchmark%20as%20well%20as%20a%20training%20set%20for%20helping%0AIR%20models%20learn%20to%20better%20follow%20real-world%20instructions.%20FollowIR%20builds%20off%0Athe%20long%20history%20of%20the%20TREC%20conferences%3A%20as%20TREC%20provides%20human%20annotators%0Awith%20instructions%20%28also%20known%20as%20narratives%29%20to%20determine%20document%20relevance%2C%0Aso%20should%20IR%20models%20be%20able%20to%20understand%20and%20decide%20relevance%20based%20on%20these%0Adetailed%20instructions.%20Our%20evaluation%20benchmark%20starts%20with%20three%20deeply%20judged%0ATREC%20collections%20and%20alters%20the%20annotator%20instructions%2C%20re-annotating%20relevant%0Adocuments.%20Through%20this%20process%2C%20we%20can%20measure%20how%20well%20IR%20models%20follow%0Ainstructions%2C%20through%20a%20new%20pairwise%20evaluation%20framework.%20Our%20results%20indicate%0Athat%20existing%20retrieval%20models%20fail%20to%20correctly%20use%20instructions%2C%20using%20them%0Afor%20basic%20keywords%20and%20struggling%20to%20understand%20long-form%20information.%20However%2C%0Awe%20show%20that%20it%20is%20possible%20for%20IR%20models%20to%20learn%20to%20follow%20complex%0Ainstructions%3A%20our%20new%20FollowIR-7B%20model%20has%20significant%20improvements%20%28over%2013%25%29%0Aafter%20fine-tuning%20on%20our%20training%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15246v1&entry.124074799=Read"},
{"title": "Solving a Real-World Package Delivery Routing Problem Using Quantum\n  Annealers", "author": "Eneko Osaba and Esther Villar-Rodriguez and Ant\u00f3n Asla", "abstract": "  Research focused on the conjunction between quantum computing and routing\nproblems has been very prolific in recent years. Most of the works revolve\naround classical problems such as the Traveling Salesman Problem or the Vehicle\nRouting Problem. Even though working on these problems is valuable, it is also\nundeniable that their academic-oriented nature falls short of real-world\nrequirements. The main objective of this research is to present a solving\nmethod for realistic instances, avoiding problem relaxations or technical\nshortcuts. Instead, a quantum-classical hybrid solver has been developed,\ncoined Q4RPD, that considers a set of real constraints such as a heterogeneous\nfleet of vehicles, priority deliveries, and capacities characterized by two\nvalues: weight and dimensions of the packages. Q4RPD resorts to the Leap\nConstrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the\napplication of Q4RPD, an experimentation composed of six different instances\nhas been conducted, aiming to serve as illustrative examples.\n", "link": "http://arxiv.org/abs/2403.15114v1", "date": "2024-03-22", "relevancy": 1.202, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4147}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.402}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3945}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Solving%20a%20Real-World%20Package%20Delivery%20Routing%20Problem%20Using%20Quantum%0A%20%20Annealers&body=Title%3A%20Solving%20a%20Real-World%20Package%20Delivery%20Routing%20Problem%20Using%20Quantum%0A%20%20Annealers%0AAuthor%3A%20Eneko%20Osaba%20and%20Esther%20Villar-Rodriguez%20and%20Ant%C3%B3n%20Asla%0AAbstract%3A%20%20%20Research%20focused%20on%20the%20conjunction%20between%20quantum%20computing%20and%20routing%0Aproblems%20has%20been%20very%20prolific%20in%20recent%20years.%20Most%20of%20the%20works%20revolve%0Aaround%20classical%20problems%20such%20as%20the%20Traveling%20Salesman%20Problem%20or%20the%20Vehicle%0ARouting%20Problem.%20Even%20though%20working%20on%20these%20problems%20is%20valuable%2C%20it%20is%20also%0Aundeniable%20that%20their%20academic-oriented%20nature%20falls%20short%20of%20real-world%0Arequirements.%20The%20main%20objective%20of%20this%20research%20is%20to%20present%20a%20solving%0Amethod%20for%20realistic%20instances%2C%20avoiding%20problem%20relaxations%20or%20technical%0Ashortcuts.%20Instead%2C%20a%20quantum-classical%20hybrid%20solver%20has%20been%20developed%2C%0Acoined%20Q4RPD%2C%20that%20considers%20a%20set%20of%20real%20constraints%20such%20as%20a%20heterogeneous%0Afleet%20of%20vehicles%2C%20priority%20deliveries%2C%20and%20capacities%20characterized%20by%20two%0Avalues%3A%20weight%20and%20dimensions%20of%20the%20packages.%20Q4RPD%20resorts%20to%20the%20Leap%0AConstrained%20Quadratic%20Model%20Hybrid%20Solver%20of%20D-Wave.%20To%20demonstrate%20the%0Aapplication%20of%20Q4RPD%2C%20an%20experimentation%20composed%20of%20six%20different%20instances%0Ahas%20been%20conducted%2C%20aiming%20to%20serve%20as%20illustrative%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15114v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20a%20Real-World%20Package%20Delivery%20Routing%20Problem%20Using%20Quantum%0A%20%20Annealers&entry.906535625=Eneko%20Osaba%20and%20Esther%20Villar-Rodriguez%20and%20Ant%C3%B3n%20Asla&entry.1292438233=%20%20Research%20focused%20on%20the%20conjunction%20between%20quantum%20computing%20and%20routing%0Aproblems%20has%20been%20very%20prolific%20in%20recent%20years.%20Most%20of%20the%20works%20revolve%0Aaround%20classical%20problems%20such%20as%20the%20Traveling%20Salesman%20Problem%20or%20the%20Vehicle%0ARouting%20Problem.%20Even%20though%20working%20on%20these%20problems%20is%20valuable%2C%20it%20is%20also%0Aundeniable%20that%20their%20academic-oriented%20nature%20falls%20short%20of%20real-world%0Arequirements.%20The%20main%20objective%20of%20this%20research%20is%20to%20present%20a%20solving%0Amethod%20for%20realistic%20instances%2C%20avoiding%20problem%20relaxations%20or%20technical%0Ashortcuts.%20Instead%2C%20a%20quantum-classical%20hybrid%20solver%20has%20been%20developed%2C%0Acoined%20Q4RPD%2C%20that%20considers%20a%20set%20of%20real%20constraints%20such%20as%20a%20heterogeneous%0Afleet%20of%20vehicles%2C%20priority%20deliveries%2C%20and%20capacities%20characterized%20by%20two%0Avalues%3A%20weight%20and%20dimensions%20of%20the%20packages.%20Q4RPD%20resorts%20to%20the%20Leap%0AConstrained%20Quadratic%20Model%20Hybrid%20Solver%20of%20D-Wave.%20To%20demonstrate%20the%0Aapplication%20of%20Q4RPD%2C%20an%20experimentation%20composed%20of%20six%20different%20instances%0Ahas%20been%20conducted%2C%20aiming%20to%20serve%20as%20illustrative%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15114v1&entry.124074799=Read"},
{"title": "Set-membership target search and tracking within an unknown cluttered\n  area using cooperating UAVs equipped with vision systems", "author": "Maxime Zagar and Luc Meyer and Michel Kieffer and H\u00e9l\u00e8ne Piet-Lahanier", "abstract": "  This paper addresses the problem of target search and tracking using a fleet\nof cooperating UAVs evolving in some unknown region of interest containing an a\npriori unknown number of moving ground targets. Each drone is equipped with an\nembedded Computer Vision System (CVS), providing an image with labeled pixels\nand a depth map of the observed part of its environment. Moreover, a box\ncontaining the corresponding pixels in the image frame is available when a UAV\nidentifies a target. Hypotheses regarding information provided by the pixel\nclassification, depth map construction, and target identification algorithms\nare proposed to allow its exploitation by set-membership approaches. A\nset-membership target location estimator is developed using the information\nprovided by the CVS. Each UAV evaluates sets guaranteed to contain the location\nof the identified targets and a set possibly containing the locations of\ntargets still to be identified. Then, each UAV uses these sets to search and\ntrack targets cooperatively.\n", "link": "http://arxiv.org/abs/2403.15113v1", "date": "2024-03-22", "relevancy": 2.0066, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5516}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4848}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4585}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Set-membership%20target%20search%20and%20tracking%20within%20an%20unknown%20cluttered%0A%20%20area%20using%20cooperating%20UAVs%20equipped%20with%20vision%20systems&body=Title%3A%20Set-membership%20target%20search%20and%20tracking%20within%20an%20unknown%20cluttered%0A%20%20area%20using%20cooperating%20UAVs%20equipped%20with%20vision%20systems%0AAuthor%3A%20Maxime%20Zagar%20and%20Luc%20Meyer%20and%20Michel%20Kieffer%20and%20H%C3%A9l%C3%A8ne%20Piet-Lahanier%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20target%20search%20and%20tracking%20using%20a%20fleet%0Aof%20cooperating%20UAVs%20evolving%20in%20some%20unknown%20region%20of%20interest%20containing%20an%20a%0Apriori%20unknown%20number%20of%20moving%20ground%20targets.%20Each%20drone%20is%20equipped%20with%20an%0Aembedded%20Computer%20Vision%20System%20%28CVS%29%2C%20providing%20an%20image%20with%20labeled%20pixels%0Aand%20a%20depth%20map%20of%20the%20observed%20part%20of%20its%20environment.%20Moreover%2C%20a%20box%0Acontaining%20the%20corresponding%20pixels%20in%20the%20image%20frame%20is%20available%20when%20a%20UAV%0Aidentifies%20a%20target.%20Hypotheses%20regarding%20information%20provided%20by%20the%20pixel%0Aclassification%2C%20depth%20map%20construction%2C%20and%20target%20identification%20algorithms%0Aare%20proposed%20to%20allow%20its%20exploitation%20by%20set-membership%20approaches.%20A%0Aset-membership%20target%20location%20estimator%20is%20developed%20using%20the%20information%0Aprovided%20by%20the%20CVS.%20Each%20UAV%20evaluates%20sets%20guaranteed%20to%20contain%20the%20location%0Aof%20the%20identified%20targets%20and%20a%20set%20possibly%20containing%20the%20locations%20of%0Atargets%20still%20to%20be%20identified.%20Then%2C%20each%20UAV%20uses%20these%20sets%20to%20search%20and%0Atrack%20targets%20cooperatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15113v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Set-membership%20target%20search%20and%20tracking%20within%20an%20unknown%20cluttered%0A%20%20area%20using%20cooperating%20UAVs%20equipped%20with%20vision%20systems&entry.906535625=Maxime%20Zagar%20and%20Luc%20Meyer%20and%20Michel%20Kieffer%20and%20H%C3%A9l%C3%A8ne%20Piet-Lahanier&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20target%20search%20and%20tracking%20using%20a%20fleet%0Aof%20cooperating%20UAVs%20evolving%20in%20some%20unknown%20region%20of%20interest%20containing%20an%20a%0Apriori%20unknown%20number%20of%20moving%20ground%20targets.%20Each%20drone%20is%20equipped%20with%20an%0Aembedded%20Computer%20Vision%20System%20%28CVS%29%2C%20providing%20an%20image%20with%20labeled%20pixels%0Aand%20a%20depth%20map%20of%20the%20observed%20part%20of%20its%20environment.%20Moreover%2C%20a%20box%0Acontaining%20the%20corresponding%20pixels%20in%20the%20image%20frame%20is%20available%20when%20a%20UAV%0Aidentifies%20a%20target.%20Hypotheses%20regarding%20information%20provided%20by%20the%20pixel%0Aclassification%2C%20depth%20map%20construction%2C%20and%20target%20identification%20algorithms%0Aare%20proposed%20to%20allow%20its%20exploitation%20by%20set-membership%20approaches.%20A%0Aset-membership%20target%20location%20estimator%20is%20developed%20using%20the%20information%0Aprovided%20by%20the%20CVS.%20Each%20UAV%20evaluates%20sets%20guaranteed%20to%20contain%20the%20location%0Aof%20the%20identified%20targets%20and%20a%20set%20possibly%20containing%20the%20locations%20of%0Atargets%20still%20to%20be%20identified.%20Then%2C%20each%20UAV%20uses%20these%20sets%20to%20search%20and%0Atrack%20targets%20cooperatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15113v1&entry.124074799=Read"},
{"title": "A Wasserstein perspective of Vanilla GANs", "author": "Lea Kunkel and Mathias Trabs", "abstract": "  The empirical success of Generative Adversarial Networks (GANs) caused an\nincreasing interest in theoretical research. The statistical literature is\nmainly focused on Wasserstein GANs and generalizations thereof, which\nespecially allow for good dimension reduction properties. Statistical results\nfor Vanilla GANs, the original optimization problem, are still rather limited\nand require assumptions such as smooth activation functions and equal\ndimensions of the latent space and the ambient space. To bridge this gap, we\ndraw a connection from Vanilla GANs to the Wasserstein distance. By doing so,\nexisting results for Wasserstein GANs can be extended to Vanilla GANs. In\nparticular, we obtain an oracle inequality for Vanilla GANs in Wasserstein\ndistance. The assumptions of this oracle inequality are designed to be\nsatisfied by network architectures commonly used in practice, such as\nfeedforward ReLU networks. By providing a quantitative result for the\napproximation of a Lipschitz function by a feedforward ReLU network with\nbounded H\\\"older norm, we conclude a rate of convergence for Vanilla GANs as\nwell as Wasserstein GANs as estimators of the unknown probability distribution.\n", "link": "http://arxiv.org/abs/2403.15312v1", "date": "2024-03-22", "relevancy": 1.9438, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5114}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4815}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4802}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Wasserstein%20perspective%20of%20Vanilla%20GANs&body=Title%3A%20A%20Wasserstein%20perspective%20of%20Vanilla%20GANs%0AAuthor%3A%20Lea%20Kunkel%20and%20Mathias%20Trabs%0AAbstract%3A%20%20%20The%20empirical%20success%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20caused%20an%0Aincreasing%20interest%20in%20theoretical%20research.%20The%20statistical%20literature%20is%0Amainly%20focused%20on%20Wasserstein%20GANs%20and%20generalizations%20thereof%2C%20which%0Aespecially%20allow%20for%20good%20dimension%20reduction%20properties.%20Statistical%20results%0Afor%20Vanilla%20GANs%2C%20the%20original%20optimization%20problem%2C%20are%20still%20rather%20limited%0Aand%20require%20assumptions%20such%20as%20smooth%20activation%20functions%20and%20equal%0Adimensions%20of%20the%20latent%20space%20and%20the%20ambient%20space.%20To%20bridge%20this%20gap%2C%20we%0Adraw%20a%20connection%20from%20Vanilla%20GANs%20to%20the%20Wasserstein%20distance.%20By%20doing%20so%2C%0Aexisting%20results%20for%20Wasserstein%20GANs%20can%20be%20extended%20to%20Vanilla%20GANs.%20In%0Aparticular%2C%20we%20obtain%20an%20oracle%20inequality%20for%20Vanilla%20GANs%20in%20Wasserstein%0Adistance.%20The%20assumptions%20of%20this%20oracle%20inequality%20are%20designed%20to%20be%0Asatisfied%20by%20network%20architectures%20commonly%20used%20in%20practice%2C%20such%20as%0Afeedforward%20ReLU%20networks.%20By%20providing%20a%20quantitative%20result%20for%20the%0Aapproximation%20of%20a%20Lipschitz%20function%20by%20a%20feedforward%20ReLU%20network%20with%0Abounded%20H%5C%22older%20norm%2C%20we%20conclude%20a%20rate%20of%20convergence%20for%20Vanilla%20GANs%20as%0Awell%20as%20Wasserstein%20GANs%20as%20estimators%20of%20the%20unknown%20probability%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15312v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Wasserstein%20perspective%20of%20Vanilla%20GANs&entry.906535625=Lea%20Kunkel%20and%20Mathias%20Trabs&entry.1292438233=%20%20The%20empirical%20success%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20caused%20an%0Aincreasing%20interest%20in%20theoretical%20research.%20The%20statistical%20literature%20is%0Amainly%20focused%20on%20Wasserstein%20GANs%20and%20generalizations%20thereof%2C%20which%0Aespecially%20allow%20for%20good%20dimension%20reduction%20properties.%20Statistical%20results%0Afor%20Vanilla%20GANs%2C%20the%20original%20optimization%20problem%2C%20are%20still%20rather%20limited%0Aand%20require%20assumptions%20such%20as%20smooth%20activation%20functions%20and%20equal%0Adimensions%20of%20the%20latent%20space%20and%20the%20ambient%20space.%20To%20bridge%20this%20gap%2C%20we%0Adraw%20a%20connection%20from%20Vanilla%20GANs%20to%20the%20Wasserstein%20distance.%20By%20doing%20so%2C%0Aexisting%20results%20for%20Wasserstein%20GANs%20can%20be%20extended%20to%20Vanilla%20GANs.%20In%0Aparticular%2C%20we%20obtain%20an%20oracle%20inequality%20for%20Vanilla%20GANs%20in%20Wasserstein%0Adistance.%20The%20assumptions%20of%20this%20oracle%20inequality%20are%20designed%20to%20be%0Asatisfied%20by%20network%20architectures%20commonly%20used%20in%20practice%2C%20such%20as%0Afeedforward%20ReLU%20networks.%20By%20providing%20a%20quantitative%20result%20for%20the%0Aapproximation%20of%20a%20Lipschitz%20function%20by%20a%20feedforward%20ReLU%20network%20with%0Abounded%20H%5C%22older%20norm%2C%20we%20conclude%20a%20rate%20of%20convergence%20for%20Vanilla%20GANs%20as%0Awell%20as%20Wasserstein%20GANs%20as%20estimators%20of%20the%20unknown%20probability%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15312v1&entry.124074799=Read"},
{"title": "EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural\n  Architecture Search", "author": "Pedram Bakhtiarifard and Christian Igel and Raghavendra Selvan", "abstract": "  Energy consumption from the selection, training, and deployment of deep\nlearning models has seen a significant uptick recently. This work aims to\nfacilitate the design of energy-efficient deep learning models that require\nless computational resources and prioritize environmental sustainability by\nfocusing on the energy consumption. Neural architecture search (NAS) benefits\nfrom tabular benchmarks, which evaluate NAS strategies cost-effectively through\nprecomputed performance statistics. We advocate for including energy efficiency\nas an additional performance criterion in NAS. To this end, we introduce an\nenhanced tabular benchmark encompassing data on energy consumption for varied\narchitectures. The benchmark, designated as EC-NAS, has been made available in\nan open-source format to advance research in energy-conscious NAS. EC-NAS\nincorporates a surrogate model to predict energy consumption, aiding in\ndiminishing the energy expenditure of the dataset creation. Our findings\nemphasize the potential of EC-NAS by leveraging multi-objective optimization\nalgorithms, revealing a balance between energy usage and accuracy. This\nsuggests the feasibility of identifying energy-lean architectures with little\nor no compromise in performance.\n", "link": "http://arxiv.org/abs/2210.06015v4", "date": "2024-03-22", "relevancy": 1.8063, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4822}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4328}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4284}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EC-NAS%3A%20Energy%20Consumption%20Aware%20Tabular%20Benchmarks%20for%20Neural%0A%20%20Architecture%20Search&body=Title%3A%20EC-NAS%3A%20Energy%20Consumption%20Aware%20Tabular%20Benchmarks%20for%20Neural%0A%20%20Architecture%20Search%0AAuthor%3A%20Pedram%20Bakhtiarifard%20and%20Christian%20Igel%20and%20Raghavendra%20Selvan%0AAbstract%3A%20%20%20Energy%20consumption%20from%20the%20selection%2C%20training%2C%20and%20deployment%20of%20deep%0Alearning%20models%20has%20seen%20a%20significant%20uptick%20recently.%20This%20work%20aims%20to%0Afacilitate%20the%20design%20of%20energy-efficient%20deep%20learning%20models%20that%20require%0Aless%20computational%20resources%20and%20prioritize%20environmental%20sustainability%20by%0Afocusing%20on%20the%20energy%20consumption.%20Neural%20architecture%20search%20%28NAS%29%20benefits%0Afrom%20tabular%20benchmarks%2C%20which%20evaluate%20NAS%20strategies%20cost-effectively%20through%0Aprecomputed%20performance%20statistics.%20We%20advocate%20for%20including%20energy%20efficiency%0Aas%20an%20additional%20performance%20criterion%20in%20NAS.%20To%20this%20end%2C%20we%20introduce%20an%0Aenhanced%20tabular%20benchmark%20encompassing%20data%20on%20energy%20consumption%20for%20varied%0Aarchitectures.%20The%20benchmark%2C%20designated%20as%20EC-NAS%2C%20has%20been%20made%20available%20in%0Aan%20open-source%20format%20to%20advance%20research%20in%20energy-conscious%20NAS.%20EC-NAS%0Aincorporates%20a%20surrogate%20model%20to%20predict%20energy%20consumption%2C%20aiding%20in%0Adiminishing%20the%20energy%20expenditure%20of%20the%20dataset%20creation.%20Our%20findings%0Aemphasize%20the%20potential%20of%20EC-NAS%20by%20leveraging%20multi-objective%20optimization%0Aalgorithms%2C%20revealing%20a%20balance%20between%20energy%20usage%20and%20accuracy.%20This%0Asuggests%20the%20feasibility%20of%20identifying%20energy-lean%20architectures%20with%20little%0Aor%20no%20compromise%20in%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.06015v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EC-NAS%3A%20Energy%20Consumption%20Aware%20Tabular%20Benchmarks%20for%20Neural%0A%20%20Architecture%20Search&entry.906535625=Pedram%20Bakhtiarifard%20and%20Christian%20Igel%20and%20Raghavendra%20Selvan&entry.1292438233=%20%20Energy%20consumption%20from%20the%20selection%2C%20training%2C%20and%20deployment%20of%20deep%0Alearning%20models%20has%20seen%20a%20significant%20uptick%20recently.%20This%20work%20aims%20to%0Afacilitate%20the%20design%20of%20energy-efficient%20deep%20learning%20models%20that%20require%0Aless%20computational%20resources%20and%20prioritize%20environmental%20sustainability%20by%0Afocusing%20on%20the%20energy%20consumption.%20Neural%20architecture%20search%20%28NAS%29%20benefits%0Afrom%20tabular%20benchmarks%2C%20which%20evaluate%20NAS%20strategies%20cost-effectively%20through%0Aprecomputed%20performance%20statistics.%20We%20advocate%20for%20including%20energy%20efficiency%0Aas%20an%20additional%20performance%20criterion%20in%20NAS.%20To%20this%20end%2C%20we%20introduce%20an%0Aenhanced%20tabular%20benchmark%20encompassing%20data%20on%20energy%20consumption%20for%20varied%0Aarchitectures.%20The%20benchmark%2C%20designated%20as%20EC-NAS%2C%20has%20been%20made%20available%20in%0Aan%20open-source%20format%20to%20advance%20research%20in%20energy-conscious%20NAS.%20EC-NAS%0Aincorporates%20a%20surrogate%20model%20to%20predict%20energy%20consumption%2C%20aiding%20in%0Adiminishing%20the%20energy%20expenditure%20of%20the%20dataset%20creation.%20Our%20findings%0Aemphasize%20the%20potential%20of%20EC-NAS%20by%20leveraging%20multi-objective%20optimization%0Aalgorithms%2C%20revealing%20a%20balance%20between%20energy%20usage%20and%20accuracy.%20This%0Asuggests%20the%20feasibility%20of%20identifying%20energy-lean%20architectures%20with%20little%0Aor%20no%20compromise%20in%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.06015v4&entry.124074799=Read"},
{"title": "Model Uncertainty in Evolutionary Optimization and Bayesian\n  Optimization: A Comparative Analysis", "author": "Hao Hao and Xiaoqun Zhang and Aimin Zhou", "abstract": "  Black-box optimization problems, which are common in many real-world\napplications, require optimization through input-output interactions without\naccess to internal workings. This often leads to significant computational\nresources being consumed for simulations. Bayesian Optimization (BO) and\nSurrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used\ngradient-free optimization techniques employed to address such challenges. Both\napproaches follow a similar iterative procedure that relies on surrogate models\nto guide the search process. This paper aims to elucidate the similarities and\ndifferences in the utilization of model uncertainty between these two methods,\nas well as the impact of model inaccuracies on algorithmic performance. A novel\nmodel-assisted strategy is introduced, which utilizes unevaluated solutions to\ngenerate offspring, leveraging the population-based search capabilities of\nevolutionary algorithm to enhance the effectiveness of model-assisted\noptimization. Experimental results demonstrate that the proposed approach\noutperforms mainstream Bayesian optimization algorithms in terms of accuracy\nand efficiency.\n", "link": "http://arxiv.org/abs/2403.14413v2", "date": "2024-03-22", "relevancy": 1.4514, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5186}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4754}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4732}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Model%20Uncertainty%20in%20Evolutionary%20Optimization%20and%20Bayesian%0A%20%20Optimization%3A%20A%20Comparative%20Analysis&body=Title%3A%20Model%20Uncertainty%20in%20Evolutionary%20Optimization%20and%20Bayesian%0A%20%20Optimization%3A%20A%20Comparative%20Analysis%0AAuthor%3A%20Hao%20Hao%20and%20Xiaoqun%20Zhang%20and%20Aimin%20Zhou%0AAbstract%3A%20%20%20Black-box%20optimization%20problems%2C%20which%20are%20common%20in%20many%20real-world%0Aapplications%2C%20require%20optimization%20through%20input-output%20interactions%20without%0Aaccess%20to%20internal%20workings.%20This%20often%20leads%20to%20significant%20computational%0Aresources%20being%20consumed%20for%20simulations.%20Bayesian%20Optimization%20%28BO%29%20and%0ASurrogate-Assisted%20Evolutionary%20Algorithm%20%28SAEA%29%20are%20two%20widely%20used%0Agradient-free%20optimization%20techniques%20employed%20to%20address%20such%20challenges.%20Both%0Aapproaches%20follow%20a%20similar%20iterative%20procedure%20that%20relies%20on%20surrogate%20models%0Ato%20guide%20the%20search%20process.%20This%20paper%20aims%20to%20elucidate%20the%20similarities%20and%0Adifferences%20in%20the%20utilization%20of%20model%20uncertainty%20between%20these%20two%20methods%2C%0Aas%20well%20as%20the%20impact%20of%20model%20inaccuracies%20on%20algorithmic%20performance.%20A%20novel%0Amodel-assisted%20strategy%20is%20introduced%2C%20which%20utilizes%20unevaluated%20solutions%20to%0Agenerate%20offspring%2C%20leveraging%20the%20population-based%20search%20capabilities%20of%0Aevolutionary%20algorithm%20to%20enhance%20the%20effectiveness%20of%20model-assisted%0Aoptimization.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20approach%0Aoutperforms%20mainstream%20Bayesian%20optimization%20algorithms%20in%20terms%20of%20accuracy%0Aand%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14413v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Uncertainty%20in%20Evolutionary%20Optimization%20and%20Bayesian%0A%20%20Optimization%3A%20A%20Comparative%20Analysis&entry.906535625=Hao%20Hao%20and%20Xiaoqun%20Zhang%20and%20Aimin%20Zhou&entry.1292438233=%20%20Black-box%20optimization%20problems%2C%20which%20are%20common%20in%20many%20real-world%0Aapplications%2C%20require%20optimization%20through%20input-output%20interactions%20without%0Aaccess%20to%20internal%20workings.%20This%20often%20leads%20to%20significant%20computational%0Aresources%20being%20consumed%20for%20simulations.%20Bayesian%20Optimization%20%28BO%29%20and%0ASurrogate-Assisted%20Evolutionary%20Algorithm%20%28SAEA%29%20are%20two%20widely%20used%0Agradient-free%20optimization%20techniques%20employed%20to%20address%20such%20challenges.%20Both%0Aapproaches%20follow%20a%20similar%20iterative%20procedure%20that%20relies%20on%20surrogate%20models%0Ato%20guide%20the%20search%20process.%20This%20paper%20aims%20to%20elucidate%20the%20similarities%20and%0Adifferences%20in%20the%20utilization%20of%20model%20uncertainty%20between%20these%20two%20methods%2C%0Aas%20well%20as%20the%20impact%20of%20model%20inaccuracies%20on%20algorithmic%20performance.%20A%20novel%0Amodel-assisted%20strategy%20is%20introduced%2C%20which%20utilizes%20unevaluated%20solutions%20to%0Agenerate%20offspring%2C%20leveraging%20the%20population-based%20search%20capabilities%20of%0Aevolutionary%20algorithm%20to%20enhance%20the%20effectiveness%20of%20model-assisted%0Aoptimization.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20approach%0Aoutperforms%20mainstream%20Bayesian%20optimization%20algorithms%20in%20terms%20of%20accuracy%0Aand%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14413v2&entry.124074799=Read"},
{"title": "Attractor reconstruction with reservoir computers: The effect of the\n  reservoir's conditional Lyapunov exponents on faithful attractor\n  reconstruction", "author": "Joseph D. Hart", "abstract": "  Reservoir computing is a machine learning framework that has been shown to be\nable to replicate the chaotic attractor, including the fractal dimension and\nthe entire Lyapunov spectrum, of the dynamical system on which it is trained.\nWe quantitatively relate the generalized synchronization dynamics of a driven\nreservoir during the training stage to the performance of the trained reservoir\ncomputer at the attractor reconstruction task. We show that, in order to obtain\nsuccessful attractor reconstruction and Lyapunov spectrum estimation, the\nlargest conditional Lyapunov exponent of the driven reservoir must be\nsignificantly more negative than the most negative Lyapunov exponent of the\ntarget system. We also find that the maximal conditional Lyapunov exponent of\nthe reservoir depends strongly on the spectral radius of the reservoir\nadjacency matrix, and therefore, for attractor reconstruction and Lyapunov\nspectrum estimation, small spectral radius reservoir computers perform better\nin general. Our arguments are supported by numerical examples on well-known\nchaotic systems.\n", "link": "http://arxiv.org/abs/2401.00885v2", "date": "2024-03-22", "relevancy": 1.6749, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4573}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4128}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4092}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Attractor%20reconstruction%20with%20reservoir%20computers%3A%20The%20effect%20of%20the%0A%20%20reservoir%27s%20conditional%20Lyapunov%20exponents%20on%20faithful%20attractor%0A%20%20reconstruction&body=Title%3A%20Attractor%20reconstruction%20with%20reservoir%20computers%3A%20The%20effect%20of%20the%0A%20%20reservoir%27s%20conditional%20Lyapunov%20exponents%20on%20faithful%20attractor%0A%20%20reconstruction%0AAuthor%3A%20Joseph%20D.%20Hart%0AAbstract%3A%20%20%20Reservoir%20computing%20is%20a%20machine%20learning%20framework%20that%20has%20been%20shown%20to%20be%0Aable%20to%20replicate%20the%20chaotic%20attractor%2C%20including%20the%20fractal%20dimension%20and%0Athe%20entire%20Lyapunov%20spectrum%2C%20of%20the%20dynamical%20system%20on%20which%20it%20is%20trained.%0AWe%20quantitatively%20relate%20the%20generalized%20synchronization%20dynamics%20of%20a%20driven%0Areservoir%20during%20the%20training%20stage%20to%20the%20performance%20of%20the%20trained%20reservoir%0Acomputer%20at%20the%20attractor%20reconstruction%20task.%20We%20show%20that%2C%20in%20order%20to%20obtain%0Asuccessful%20attractor%20reconstruction%20and%20Lyapunov%20spectrum%20estimation%2C%20the%0Alargest%20conditional%20Lyapunov%20exponent%20of%20the%20driven%20reservoir%20must%20be%0Asignificantly%20more%20negative%20than%20the%20most%20negative%20Lyapunov%20exponent%20of%20the%0Atarget%20system.%20We%20also%20find%20that%20the%20maximal%20conditional%20Lyapunov%20exponent%20of%0Athe%20reservoir%20depends%20strongly%20on%20the%20spectral%20radius%20of%20the%20reservoir%0Aadjacency%20matrix%2C%20and%20therefore%2C%20for%20attractor%20reconstruction%20and%20Lyapunov%0Aspectrum%20estimation%2C%20small%20spectral%20radius%20reservoir%20computers%20perform%20better%0Ain%20general.%20Our%20arguments%20are%20supported%20by%20numerical%20examples%20on%20well-known%0Achaotic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00885v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attractor%20reconstruction%20with%20reservoir%20computers%3A%20The%20effect%20of%20the%0A%20%20reservoir%27s%20conditional%20Lyapunov%20exponents%20on%20faithful%20attractor%0A%20%20reconstruction&entry.906535625=Joseph%20D.%20Hart&entry.1292438233=%20%20Reservoir%20computing%20is%20a%20machine%20learning%20framework%20that%20has%20been%20shown%20to%20be%0Aable%20to%20replicate%20the%20chaotic%20attractor%2C%20including%20the%20fractal%20dimension%20and%0Athe%20entire%20Lyapunov%20spectrum%2C%20of%20the%20dynamical%20system%20on%20which%20it%20is%20trained.%0AWe%20quantitatively%20relate%20the%20generalized%20synchronization%20dynamics%20of%20a%20driven%0Areservoir%20during%20the%20training%20stage%20to%20the%20performance%20of%20the%20trained%20reservoir%0Acomputer%20at%20the%20attractor%20reconstruction%20task.%20We%20show%20that%2C%20in%20order%20to%20obtain%0Asuccessful%20attractor%20reconstruction%20and%20Lyapunov%20spectrum%20estimation%2C%20the%0Alargest%20conditional%20Lyapunov%20exponent%20of%20the%20driven%20reservoir%20must%20be%0Asignificantly%20more%20negative%20than%20the%20most%20negative%20Lyapunov%20exponent%20of%20the%0Atarget%20system.%20We%20also%20find%20that%20the%20maximal%20conditional%20Lyapunov%20exponent%20of%0Athe%20reservoir%20depends%20strongly%20on%20the%20spectral%20radius%20of%20the%20reservoir%0Aadjacency%20matrix%2C%20and%20therefore%2C%20for%20attractor%20reconstruction%20and%20Lyapunov%0Aspectrum%20estimation%2C%20small%20spectral%20radius%20reservoir%20computers%20perform%20better%0Ain%20general.%20Our%20arguments%20are%20supported%20by%20numerical%20examples%20on%20well-known%0Achaotic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00885v2&entry.124074799=Read"},
{"title": "Multi-conditioned Graph Diffusion for Neural Architecture Search", "author": "Rohan Asthana and Joschua Conrad and Youssef Dawoud and Maurits Ortmanns and Vasileios Belagiannis", "abstract": "  Neural architecture search automates the design of neural network\narchitectures usually by exploring a large and thus complex architecture search\nspace. To advance the architecture search, we present a graph diffusion-based\nNAS approach that uses discrete conditional graph diffusion processes to\ngenerate high-performing neural network architectures. We then propose a\nmulti-conditioned classifier-free guidance approach applied to graph diffusion\nnetworks to jointly impose constraints such as high accuracy and low hardware\nlatency. Unlike the related work, our method is completely differentiable and\nrequires only a single model training. In our evaluations, we show promising\nresults on six standard benchmarks, yielding novel and unique architectures at\na fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we\ndemonstrate the generalisability and efficiency of our method through\nexperiments on ImageNet dataset.\n", "link": "http://arxiv.org/abs/2403.06020v2", "date": "2024-03-22", "relevancy": 1.9762, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5035}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4943}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4901}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-conditioned%20Graph%20Diffusion%20for%20Neural%20Architecture%20Search&body=Title%3A%20Multi-conditioned%20Graph%20Diffusion%20for%20Neural%20Architecture%20Search%0AAuthor%3A%20Rohan%20Asthana%20and%20Joschua%20Conrad%20and%20Youssef%20Dawoud%20and%20Maurits%20Ortmanns%20and%20Vasileios%20Belagiannis%0AAbstract%3A%20%20%20Neural%20architecture%20search%20automates%20the%20design%20of%20neural%20network%0Aarchitectures%20usually%20by%20exploring%20a%20large%20and%20thus%20complex%20architecture%20search%0Aspace.%20To%20advance%20the%20architecture%20search%2C%20we%20present%20a%20graph%20diffusion-based%0ANAS%20approach%20that%20uses%20discrete%20conditional%20graph%20diffusion%20processes%20to%0Agenerate%20high-performing%20neural%20network%20architectures.%20We%20then%20propose%20a%0Amulti-conditioned%20classifier-free%20guidance%20approach%20applied%20to%20graph%20diffusion%0Anetworks%20to%20jointly%20impose%20constraints%20such%20as%20high%20accuracy%20and%20low%20hardware%0Alatency.%20Unlike%20the%20related%20work%2C%20our%20method%20is%20completely%20differentiable%20and%0Arequires%20only%20a%20single%20model%20training.%20In%20our%20evaluations%2C%20we%20show%20promising%0Aresults%20on%20six%20standard%20benchmarks%2C%20yielding%20novel%20and%20unique%20architectures%20at%0Aa%20fast%20speed%2C%20i.e.%20less%20than%200.2%20seconds%20per%20architecture.%20Furthermore%2C%20we%0Ademonstrate%20the%20generalisability%20and%20efficiency%20of%20our%20method%20through%0Aexperiments%20on%20ImageNet%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06020v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-conditioned%20Graph%20Diffusion%20for%20Neural%20Architecture%20Search&entry.906535625=Rohan%20Asthana%20and%20Joschua%20Conrad%20and%20Youssef%20Dawoud%20and%20Maurits%20Ortmanns%20and%20Vasileios%20Belagiannis&entry.1292438233=%20%20Neural%20architecture%20search%20automates%20the%20design%20of%20neural%20network%0Aarchitectures%20usually%20by%20exploring%20a%20large%20and%20thus%20complex%20architecture%20search%0Aspace.%20To%20advance%20the%20architecture%20search%2C%20we%20present%20a%20graph%20diffusion-based%0ANAS%20approach%20that%20uses%20discrete%20conditional%20graph%20diffusion%20processes%20to%0Agenerate%20high-performing%20neural%20network%20architectures.%20We%20then%20propose%20a%0Amulti-conditioned%20classifier-free%20guidance%20approach%20applied%20to%20graph%20diffusion%0Anetworks%20to%20jointly%20impose%20constraints%20such%20as%20high%20accuracy%20and%20low%20hardware%0Alatency.%20Unlike%20the%20related%20work%2C%20our%20method%20is%20completely%20differentiable%20and%0Arequires%20only%20a%20single%20model%20training.%20In%20our%20evaluations%2C%20we%20show%20promising%0Aresults%20on%20six%20standard%20benchmarks%2C%20yielding%20novel%20and%20unique%20architectures%20at%0Aa%20fast%20speed%2C%20i.e.%20less%20than%200.2%20seconds%20per%20architecture.%20Furthermore%2C%20we%0Ademonstrate%20the%20generalisability%20and%20efficiency%20of%20our%20method%20through%0Aexperiments%20on%20ImageNet%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06020v2&entry.124074799=Read"},
{"title": "Exploring the Task-agnostic Trait of Self-supervised Learning in the\n  Context of Detecting Mental Disorders", "author": "Rohan Kumar Gupta and Rohit Sinha", "abstract": "  Self-supervised learning (SSL) has been investigated to generate\ntask-agnostic representations across various domains. However, such\ninvestigation has not been conducted for detecting multiple mental disorders.\nThe rationale behind the existence of a task-agnostic representation lies in\nthe overlapping symptoms among multiple mental disorders. Consequently, the\nbehavioural data collected for mental health assessment may carry a mixed bag\nof attributes related to multiple disorders. Motivated by that, in this study,\nwe explore a task-agnostic representation derived through SSL in the context of\ndetecting major depressive disorder (MDD) and post-traumatic stress disorder\n(PTSD) using audio and video data collected during interactive sessions. This\nstudy employs SSL models trained by predicting multiple fixed targets or masked\nframes. We propose a list of fixed targets to make the generated representation\nmore efficient for detecting MDD and PTSD. Furthermore, we modify the\nhyper-parameters of the SSL encoder predicting fixed targets to generate global\nrepresentations that capture varying temporal contexts. Both these innovations\nare noted to yield improved detection performances for considered mental\ndisorders and exhibit task-agnostic traits. In the context of the SSL model\npredicting masked frames, the generated global representations are also noted\nto exhibit task-agnostic traits.\n", "link": "http://arxiv.org/abs/2403.15170v1", "date": "2024-03-22", "relevancy": 1.9985, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5276}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4808}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4766}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Task-agnostic%20Trait%20of%20Self-supervised%20Learning%20in%20the%0A%20%20Context%20of%20Detecting%20Mental%20Disorders&body=Title%3A%20Exploring%20the%20Task-agnostic%20Trait%20of%20Self-supervised%20Learning%20in%20the%0A%20%20Context%20of%20Detecting%20Mental%20Disorders%0AAuthor%3A%20Rohan%20Kumar%20Gupta%20and%20Rohit%20Sinha%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20been%20investigated%20to%20generate%0Atask-agnostic%20representations%20across%20various%20domains.%20However%2C%20such%0Ainvestigation%20has%20not%20been%20conducted%20for%20detecting%20multiple%20mental%20disorders.%0AThe%20rationale%20behind%20the%20existence%20of%20a%20task-agnostic%20representation%20lies%20in%0Athe%20overlapping%20symptoms%20among%20multiple%20mental%20disorders.%20Consequently%2C%20the%0Abehavioural%20data%20collected%20for%20mental%20health%20assessment%20may%20carry%20a%20mixed%20bag%0Aof%20attributes%20related%20to%20multiple%20disorders.%20Motivated%20by%20that%2C%20in%20this%20study%2C%0Awe%20explore%20a%20task-agnostic%20representation%20derived%20through%20SSL%20in%20the%20context%20of%0Adetecting%20major%20depressive%20disorder%20%28MDD%29%20and%20post-traumatic%20stress%20disorder%0A%28PTSD%29%20using%20audio%20and%20video%20data%20collected%20during%20interactive%20sessions.%20This%0Astudy%20employs%20SSL%20models%20trained%20by%20predicting%20multiple%20fixed%20targets%20or%20masked%0Aframes.%20We%20propose%20a%20list%20of%20fixed%20targets%20to%20make%20the%20generated%20representation%0Amore%20efficient%20for%20detecting%20MDD%20and%20PTSD.%20Furthermore%2C%20we%20modify%20the%0Ahyper-parameters%20of%20the%20SSL%20encoder%20predicting%20fixed%20targets%20to%20generate%20global%0Arepresentations%20that%20capture%20varying%20temporal%20contexts.%20Both%20these%20innovations%0Aare%20noted%20to%20yield%20improved%20detection%20performances%20for%20considered%20mental%0Adisorders%20and%20exhibit%20task-agnostic%20traits.%20In%20the%20context%20of%20the%20SSL%20model%0Apredicting%20masked%20frames%2C%20the%20generated%20global%20representations%20are%20also%20noted%0Ato%20exhibit%20task-agnostic%20traits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15170v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Task-agnostic%20Trait%20of%20Self-supervised%20Learning%20in%20the%0A%20%20Context%20of%20Detecting%20Mental%20Disorders&entry.906535625=Rohan%20Kumar%20Gupta%20and%20Rohit%20Sinha&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20been%20investigated%20to%20generate%0Atask-agnostic%20representations%20across%20various%20domains.%20However%2C%20such%0Ainvestigation%20has%20not%20been%20conducted%20for%20detecting%20multiple%20mental%20disorders.%0AThe%20rationale%20behind%20the%20existence%20of%20a%20task-agnostic%20representation%20lies%20in%0Athe%20overlapping%20symptoms%20among%20multiple%20mental%20disorders.%20Consequently%2C%20the%0Abehavioural%20data%20collected%20for%20mental%20health%20assessment%20may%20carry%20a%20mixed%20bag%0Aof%20attributes%20related%20to%20multiple%20disorders.%20Motivated%20by%20that%2C%20in%20this%20study%2C%0Awe%20explore%20a%20task-agnostic%20representation%20derived%20through%20SSL%20in%20the%20context%20of%0Adetecting%20major%20depressive%20disorder%20%28MDD%29%20and%20post-traumatic%20stress%20disorder%0A%28PTSD%29%20using%20audio%20and%20video%20data%20collected%20during%20interactive%20sessions.%20This%0Astudy%20employs%20SSL%20models%20trained%20by%20predicting%20multiple%20fixed%20targets%20or%20masked%0Aframes.%20We%20propose%20a%20list%20of%20fixed%20targets%20to%20make%20the%20generated%20representation%0Amore%20efficient%20for%20detecting%20MDD%20and%20PTSD.%20Furthermore%2C%20we%20modify%20the%0Ahyper-parameters%20of%20the%20SSL%20encoder%20predicting%20fixed%20targets%20to%20generate%20global%0Arepresentations%20that%20capture%20varying%20temporal%20contexts.%20Both%20these%20innovations%0Aare%20noted%20to%20yield%20improved%20detection%20performances%20for%20considered%20mental%0Adisorders%20and%20exhibit%20task-agnostic%20traits.%20In%20the%20context%20of%20the%20SSL%20model%0Apredicting%20masked%20frames%2C%20the%20generated%20global%20representations%20are%20also%20noted%0Ato%20exhibit%20task-agnostic%20traits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15170v1&entry.124074799=Read"},
{"title": "Text clustering with LLM embeddings", "author": "Alina Petukhova and Joao P. Matos-Carvalho and Nuno Fachada", "abstract": "  Text clustering is an important approach for organising the growing amount of\ndigital content, helping to structure and find hidden patterns in uncategorised\ndata. In this research, we investigated how different textual embeddings -\nparticularly those used in large language models (LLMs) - and clustering\nalgorithms affect how text datasets are clustered. A series of experiments were\nconducted to assess how embeddings influence clustering results, the role\nplayed by dimensionality reduction through summarisation, and embedding size\nadjustment. Results reveal that LLM embeddings excel at capturing the nuances\nof structured language, while BERT leads the lightweight options in\nperformance. In addition, we find that increasing embedding dimensionality and\nsummarisation techniques do not uniformly improve clustering efficiency,\nsuggesting that these strategies require careful analysis to use in real-life\nmodels. These results highlight a complex balance between the need for nuanced\ntext representation and computational feasibility in text clustering\napplications. This study extends traditional text clustering frameworks by\nincorporating embeddings from LLMs, thereby paving the way for improved\nmethodologies and opening new avenues for future research in various types of\ntextual analysis.\n", "link": "http://arxiv.org/abs/2403.15112v1", "date": "2024-03-22", "relevancy": 1.8086, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.482}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4333}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4248}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Text%20clustering%20with%20LLM%20embeddings&body=Title%3A%20Text%20clustering%20with%20LLM%20embeddings%0AAuthor%3A%20Alina%20Petukhova%20and%20Joao%20P.%20Matos-Carvalho%20and%20Nuno%20Fachada%0AAbstract%3A%20%20%20Text%20clustering%20is%20an%20important%20approach%20for%20organising%20the%20growing%20amount%20of%0Adigital%20content%2C%20helping%20to%20structure%20and%20find%20hidden%20patterns%20in%20uncategorised%0Adata.%20In%20this%20research%2C%20we%20investigated%20how%20different%20textual%20embeddings%20-%0Aparticularly%20those%20used%20in%20large%20language%20models%20%28LLMs%29%20-%20and%20clustering%0Aalgorithms%20affect%20how%20text%20datasets%20are%20clustered.%20A%20series%20of%20experiments%20were%0Aconducted%20to%20assess%20how%20embeddings%20influence%20clustering%20results%2C%20the%20role%0Aplayed%20by%20dimensionality%20reduction%20through%20summarisation%2C%20and%20embedding%20size%0Aadjustment.%20Results%20reveal%20that%20LLM%20embeddings%20excel%20at%20capturing%20the%20nuances%0Aof%20structured%20language%2C%20while%20BERT%20leads%20the%20lightweight%20options%20in%0Aperformance.%20In%20addition%2C%20we%20find%20that%20increasing%20embedding%20dimensionality%20and%0Asummarisation%20techniques%20do%20not%20uniformly%20improve%20clustering%20efficiency%2C%0Asuggesting%20that%20these%20strategies%20require%20careful%20analysis%20to%20use%20in%20real-life%0Amodels.%20These%20results%20highlight%20a%20complex%20balance%20between%20the%20need%20for%20nuanced%0Atext%20representation%20and%20computational%20feasibility%20in%20text%20clustering%0Aapplications.%20This%20study%20extends%20traditional%20text%20clustering%20frameworks%20by%0Aincorporating%20embeddings%20from%20LLMs%2C%20thereby%20paving%20the%20way%20for%20improved%0Amethodologies%20and%20opening%20new%20avenues%20for%20future%20research%20in%20various%20types%20of%0Atextual%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15112v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20clustering%20with%20LLM%20embeddings&entry.906535625=Alina%20Petukhova%20and%20Joao%20P.%20Matos-Carvalho%20and%20Nuno%20Fachada&entry.1292438233=%20%20Text%20clustering%20is%20an%20important%20approach%20for%20organising%20the%20growing%20amount%20of%0Adigital%20content%2C%20helping%20to%20structure%20and%20find%20hidden%20patterns%20in%20uncategorised%0Adata.%20In%20this%20research%2C%20we%20investigated%20how%20different%20textual%20embeddings%20-%0Aparticularly%20those%20used%20in%20large%20language%20models%20%28LLMs%29%20-%20and%20clustering%0Aalgorithms%20affect%20how%20text%20datasets%20are%20clustered.%20A%20series%20of%20experiments%20were%0Aconducted%20to%20assess%20how%20embeddings%20influence%20clustering%20results%2C%20the%20role%0Aplayed%20by%20dimensionality%20reduction%20through%20summarisation%2C%20and%20embedding%20size%0Aadjustment.%20Results%20reveal%20that%20LLM%20embeddings%20excel%20at%20capturing%20the%20nuances%0Aof%20structured%20language%2C%20while%20BERT%20leads%20the%20lightweight%20options%20in%0Aperformance.%20In%20addition%2C%20we%20find%20that%20increasing%20embedding%20dimensionality%20and%0Asummarisation%20techniques%20do%20not%20uniformly%20improve%20clustering%20efficiency%2C%0Asuggesting%20that%20these%20strategies%20require%20careful%20analysis%20to%20use%20in%20real-life%0Amodels.%20These%20results%20highlight%20a%20complex%20balance%20between%20the%20need%20for%20nuanced%0Atext%20representation%20and%20computational%20feasibility%20in%20text%20clustering%0Aapplications.%20This%20study%20extends%20traditional%20text%20clustering%20frameworks%20by%0Aincorporating%20embeddings%20from%20LLMs%2C%20thereby%20paving%20the%20way%20for%20improved%0Amethodologies%20and%20opening%20new%20avenues%20for%20future%20research%20in%20various%20types%20of%0Atextual%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15112v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


