<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250422.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Text-based Animatable 3D Avatars with Morphable Model Alignment", "author": "Yiqian Wu and Malte Prinzler and Xiaogang Jin and Siyu Tang", "abstract": "  The generation of high-quality, animatable 3D head avatars from text has\nenormous potential in content creation applications such as games, movies, and\nembodied virtual assistants. Current text-to-3D generation methods typically\ncombine parametric head models with 2D diffusion models using score\ndistillation sampling to produce 3D-consistent results. However, they struggle\nto synthesize realistic details and suffer from misalignments between the\nappearance and the driving parametric model, resulting in unnatural animation\nresults. We discovered that these limitations stem from ambiguities in the 2D\ndiffusion predictions during 3D avatar distillation, specifically: i) the\navatar's appearance and geometry is underconstrained by the text input, and ii)\nthe semantic alignment between the predictions and the parametric head model is\ninsufficient because the diffusion model alone cannot incorporate information\nfrom the parametric model. In this work, we propose a novel framework,\nAnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with\nmorphable model alignment, and introduce two key strategies to address these\nchallenges. First, we tackle appearance and geometry ambiguities by utilizing\nprior information from a pretrained text-to-3D model to initialize a 3D avatar\nwith robust appearance, geometry, and rigging relationships to the morphable\nmodel. Second, we refine the initial 3D avatar for dynamic expressions using a\nControlNet that is conditioned on semantic and normal maps of the morphable\nmodel to ensure accurate alignment. As a result, our method outperforms\nexisting approaches in terms of synthesis quality, alignment, and animation\nfidelity. Our experiments show that the proposed method advances the state of\nthe art in text-based, animatable 3D head avatar generation.\n", "link": "http://arxiv.org/abs/2504.15835v1", "date": "2025-04-22", "relevancy": 3.4275, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6878}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6878}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-based%20Animatable%203D%20Avatars%20with%20Morphable%20Model%20Alignment&body=Title%3A%20Text-based%20Animatable%203D%20Avatars%20with%20Morphable%20Model%20Alignment%0AAuthor%3A%20Yiqian%20Wu%20and%20Malte%20Prinzler%20and%20Xiaogang%20Jin%20and%20Siyu%20Tang%0AAbstract%3A%20%20%20The%20generation%20of%20high-quality%2C%20animatable%203D%20head%20avatars%20from%20text%20has%0Aenormous%20potential%20in%20content%20creation%20applications%20such%20as%20games%2C%20movies%2C%20and%0Aembodied%20virtual%20assistants.%20Current%20text-to-3D%20generation%20methods%20typically%0Acombine%20parametric%20head%20models%20with%202D%20diffusion%20models%20using%20score%0Adistillation%20sampling%20to%20produce%203D-consistent%20results.%20However%2C%20they%20struggle%0Ato%20synthesize%20realistic%20details%20and%20suffer%20from%20misalignments%20between%20the%0Aappearance%20and%20the%20driving%20parametric%20model%2C%20resulting%20in%20unnatural%20animation%0Aresults.%20We%20discovered%20that%20these%20limitations%20stem%20from%20ambiguities%20in%20the%202D%0Adiffusion%20predictions%20during%203D%20avatar%20distillation%2C%20specifically%3A%20i%29%20the%0Aavatar%27s%20appearance%20and%20geometry%20is%20underconstrained%20by%20the%20text%20input%2C%20and%20ii%29%0Athe%20semantic%20alignment%20between%20the%20predictions%20and%20the%20parametric%20head%20model%20is%0Ainsufficient%20because%20the%20diffusion%20model%20alone%20cannot%20incorporate%20information%0Afrom%20the%20parametric%20model.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%2C%0AAnimPortrait3D%2C%20for%20text-based%20realistic%20animatable%203DGS%20avatar%20generation%20with%0Amorphable%20model%20alignment%2C%20and%20introduce%20two%20key%20strategies%20to%20address%20these%0Achallenges.%20First%2C%20we%20tackle%20appearance%20and%20geometry%20ambiguities%20by%20utilizing%0Aprior%20information%20from%20a%20pretrained%20text-to-3D%20model%20to%20initialize%20a%203D%20avatar%0Awith%20robust%20appearance%2C%20geometry%2C%20and%20rigging%20relationships%20to%20the%20morphable%0Amodel.%20Second%2C%20we%20refine%20the%20initial%203D%20avatar%20for%20dynamic%20expressions%20using%20a%0AControlNet%20that%20is%20conditioned%20on%20semantic%20and%20normal%20maps%20of%20the%20morphable%0Amodel%20to%20ensure%20accurate%20alignment.%20As%20a%20result%2C%20our%20method%20outperforms%0Aexisting%20approaches%20in%20terms%20of%20synthesis%20quality%2C%20alignment%2C%20and%20animation%0Afidelity.%20Our%20experiments%20show%20that%20the%20proposed%20method%20advances%20the%20state%20of%0Athe%20art%20in%20text-based%2C%20animatable%203D%20head%20avatar%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-based%2520Animatable%25203D%2520Avatars%2520with%2520Morphable%2520Model%2520Alignment%26entry.906535625%3DYiqian%2520Wu%2520and%2520Malte%2520Prinzler%2520and%2520Xiaogang%2520Jin%2520and%2520Siyu%2520Tang%26entry.1292438233%3D%2520%2520The%2520generation%2520of%2520high-quality%252C%2520animatable%25203D%2520head%2520avatars%2520from%2520text%2520has%250Aenormous%2520potential%2520in%2520content%2520creation%2520applications%2520such%2520as%2520games%252C%2520movies%252C%2520and%250Aembodied%2520virtual%2520assistants.%2520Current%2520text-to-3D%2520generation%2520methods%2520typically%250Acombine%2520parametric%2520head%2520models%2520with%25202D%2520diffusion%2520models%2520using%2520score%250Adistillation%2520sampling%2520to%2520produce%25203D-consistent%2520results.%2520However%252C%2520they%2520struggle%250Ato%2520synthesize%2520realistic%2520details%2520and%2520suffer%2520from%2520misalignments%2520between%2520the%250Aappearance%2520and%2520the%2520driving%2520parametric%2520model%252C%2520resulting%2520in%2520unnatural%2520animation%250Aresults.%2520We%2520discovered%2520that%2520these%2520limitations%2520stem%2520from%2520ambiguities%2520in%2520the%25202D%250Adiffusion%2520predictions%2520during%25203D%2520avatar%2520distillation%252C%2520specifically%253A%2520i%2529%2520the%250Aavatar%2527s%2520appearance%2520and%2520geometry%2520is%2520underconstrained%2520by%2520the%2520text%2520input%252C%2520and%2520ii%2529%250Athe%2520semantic%2520alignment%2520between%2520the%2520predictions%2520and%2520the%2520parametric%2520head%2520model%2520is%250Ainsufficient%2520because%2520the%2520diffusion%2520model%2520alone%2520cannot%2520incorporate%2520information%250Afrom%2520the%2520parametric%2520model.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%250AAnimPortrait3D%252C%2520for%2520text-based%2520realistic%2520animatable%25203DGS%2520avatar%2520generation%2520with%250Amorphable%2520model%2520alignment%252C%2520and%2520introduce%2520two%2520key%2520strategies%2520to%2520address%2520these%250Achallenges.%2520First%252C%2520we%2520tackle%2520appearance%2520and%2520geometry%2520ambiguities%2520by%2520utilizing%250Aprior%2520information%2520from%2520a%2520pretrained%2520text-to-3D%2520model%2520to%2520initialize%2520a%25203D%2520avatar%250Awith%2520robust%2520appearance%252C%2520geometry%252C%2520and%2520rigging%2520relationships%2520to%2520the%2520morphable%250Amodel.%2520Second%252C%2520we%2520refine%2520the%2520initial%25203D%2520avatar%2520for%2520dynamic%2520expressions%2520using%2520a%250AControlNet%2520that%2520is%2520conditioned%2520on%2520semantic%2520and%2520normal%2520maps%2520of%2520the%2520morphable%250Amodel%2520to%2520ensure%2520accurate%2520alignment.%2520As%2520a%2520result%252C%2520our%2520method%2520outperforms%250Aexisting%2520approaches%2520in%2520terms%2520of%2520synthesis%2520quality%252C%2520alignment%252C%2520and%2520animation%250Afidelity.%2520Our%2520experiments%2520show%2520that%2520the%2520proposed%2520method%2520advances%2520the%2520state%2520of%250Athe%2520art%2520in%2520text-based%252C%2520animatable%25203D%2520head%2520avatar%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-based%20Animatable%203D%20Avatars%20with%20Morphable%20Model%20Alignment&entry.906535625=Yiqian%20Wu%20and%20Malte%20Prinzler%20and%20Xiaogang%20Jin%20and%20Siyu%20Tang&entry.1292438233=%20%20The%20generation%20of%20high-quality%2C%20animatable%203D%20head%20avatars%20from%20text%20has%0Aenormous%20potential%20in%20content%20creation%20applications%20such%20as%20games%2C%20movies%2C%20and%0Aembodied%20virtual%20assistants.%20Current%20text-to-3D%20generation%20methods%20typically%0Acombine%20parametric%20head%20models%20with%202D%20diffusion%20models%20using%20score%0Adistillation%20sampling%20to%20produce%203D-consistent%20results.%20However%2C%20they%20struggle%0Ato%20synthesize%20realistic%20details%20and%20suffer%20from%20misalignments%20between%20the%0Aappearance%20and%20the%20driving%20parametric%20model%2C%20resulting%20in%20unnatural%20animation%0Aresults.%20We%20discovered%20that%20these%20limitations%20stem%20from%20ambiguities%20in%20the%202D%0Adiffusion%20predictions%20during%203D%20avatar%20distillation%2C%20specifically%3A%20i%29%20the%0Aavatar%27s%20appearance%20and%20geometry%20is%20underconstrained%20by%20the%20text%20input%2C%20and%20ii%29%0Athe%20semantic%20alignment%20between%20the%20predictions%20and%20the%20parametric%20head%20model%20is%0Ainsufficient%20because%20the%20diffusion%20model%20alone%20cannot%20incorporate%20information%0Afrom%20the%20parametric%20model.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%2C%0AAnimPortrait3D%2C%20for%20text-based%20realistic%20animatable%203DGS%20avatar%20generation%20with%0Amorphable%20model%20alignment%2C%20and%20introduce%20two%20key%20strategies%20to%20address%20these%0Achallenges.%20First%2C%20we%20tackle%20appearance%20and%20geometry%20ambiguities%20by%20utilizing%0Aprior%20information%20from%20a%20pretrained%20text-to-3D%20model%20to%20initialize%20a%203D%20avatar%0Awith%20robust%20appearance%2C%20geometry%2C%20and%20rigging%20relationships%20to%20the%20morphable%0Amodel.%20Second%2C%20we%20refine%20the%20initial%203D%20avatar%20for%20dynamic%20expressions%20using%20a%0AControlNet%20that%20is%20conditioned%20on%20semantic%20and%20normal%20maps%20of%20the%20morphable%0Amodel%20to%20ensure%20accurate%20alignment.%20As%20a%20result%2C%20our%20method%20outperforms%0Aexisting%20approaches%20in%20terms%20of%20synthesis%20quality%2C%20alignment%2C%20and%20animation%0Afidelity.%20Our%20experiments%20show%20that%20the%20proposed%20method%20advances%20the%20state%20of%0Athe%20art%20in%20text-based%2C%20animatable%203D%20head%20avatar%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15835v1&entry.124074799=Read"},
{"title": "HEMGS: A Hybrid Entropy Model for 3D Gaussian Splatting Data Compression", "author": "Lei Liu and Zhenghao Chen and Wei Jiang and Wei Wang and Dong Xu", "abstract": "  In this work, we propose a novel compression framework for 3D Gaussian\nSplatting (3DGS) data. Building on anchor-based 3DGS methodologies, our\napproach compresses all attributes within each anchor by introducing a novel\nHybrid Entropy Model for 3D Gaussian Splatting (HEMGS) to achieve hybrid\nlossy-lossless compression. It consists of three main components: a\nvariable-rate predictor, a hyperprior network, and an autoregressive network.\nFirst, unlike previous methods that adopt multiple models to achieve multi-rate\nlossy compression, thereby increasing training overhead, our variable-rate\npredictor enables variable-rate compression with a single model and a\nhyperparameter $\\lambda$ by producing a learned Quantization Step feature for\nversatile lossy compression. Second, to improve lossless compression, the\nhyperprior network captures both scene-agnostic and scene-specific features to\ngenerate a prior feature, while the autoregressive network employs an adaptive\ncontext selection algorithm with flexible receptive fields to produce a\ncontextual feature. By integrating these two features, HEMGS can accurately\nestimate the distribution of the current coding element within each attribute,\nenabling improved entropy coding and reduced storage. We integrate HEMGS into a\ncompression framework, and experimental results on four benchmarks indicate\nthat HEMGS achieves about a 40% average reduction in size while maintaining\nrendering quality over baseline methods and achieving state-of-the-art\ncompression results.\n", "link": "http://arxiv.org/abs/2411.18473v2", "date": "2025-04-22", "relevancy": 3.1876, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6701}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.623}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HEMGS%3A%20A%20Hybrid%20Entropy%20Model%20for%203D%20Gaussian%20Splatting%20Data%20Compression&body=Title%3A%20HEMGS%3A%20A%20Hybrid%20Entropy%20Model%20for%203D%20Gaussian%20Splatting%20Data%20Compression%0AAuthor%3A%20Lei%20Liu%20and%20Zhenghao%20Chen%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Dong%20Xu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20compression%20framework%20for%203D%20Gaussian%0ASplatting%20%283DGS%29%20data.%20Building%20on%20anchor-based%203DGS%20methodologies%2C%20our%0Aapproach%20compresses%20all%20attributes%20within%20each%20anchor%20by%20introducing%20a%20novel%0AHybrid%20Entropy%20Model%20for%203D%20Gaussian%20Splatting%20%28HEMGS%29%20to%20achieve%20hybrid%0Alossy-lossless%20compression.%20It%20consists%20of%20three%20main%20components%3A%20a%0Avariable-rate%20predictor%2C%20a%20hyperprior%20network%2C%20and%20an%20autoregressive%20network.%0AFirst%2C%20unlike%20previous%20methods%20that%20adopt%20multiple%20models%20to%20achieve%20multi-rate%0Alossy%20compression%2C%20thereby%20increasing%20training%20overhead%2C%20our%20variable-rate%0Apredictor%20enables%20variable-rate%20compression%20with%20a%20single%20model%20and%20a%0Ahyperparameter%20%24%5Clambda%24%20by%20producing%20a%20learned%20Quantization%20Step%20feature%20for%0Aversatile%20lossy%20compression.%20Second%2C%20to%20improve%20lossless%20compression%2C%20the%0Ahyperprior%20network%20captures%20both%20scene-agnostic%20and%20scene-specific%20features%20to%0Agenerate%20a%20prior%20feature%2C%20while%20the%20autoregressive%20network%20employs%20an%20adaptive%0Acontext%20selection%20algorithm%20with%20flexible%20receptive%20fields%20to%20produce%20a%0Acontextual%20feature.%20By%20integrating%20these%20two%20features%2C%20HEMGS%20can%20accurately%0Aestimate%20the%20distribution%20of%20the%20current%20coding%20element%20within%20each%20attribute%2C%0Aenabling%20improved%20entropy%20coding%20and%20reduced%20storage.%20We%20integrate%20HEMGS%20into%20a%0Acompression%20framework%2C%20and%20experimental%20results%20on%20four%20benchmarks%20indicate%0Athat%20HEMGS%20achieves%20about%20a%2040%25%20average%20reduction%20in%20size%20while%20maintaining%0Arendering%20quality%20over%20baseline%20methods%20and%20achieving%20state-of-the-art%0Acompression%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHEMGS%253A%2520A%2520Hybrid%2520Entropy%2520Model%2520for%25203D%2520Gaussian%2520Splatting%2520Data%2520Compression%26entry.906535625%3DLei%2520Liu%2520and%2520Zhenghao%2520Chen%2520and%2520Wei%2520Jiang%2520and%2520Wei%2520Wang%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520compression%2520framework%2520for%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520data.%2520Building%2520on%2520anchor-based%25203DGS%2520methodologies%252C%2520our%250Aapproach%2520compresses%2520all%2520attributes%2520within%2520each%2520anchor%2520by%2520introducing%2520a%2520novel%250AHybrid%2520Entropy%2520Model%2520for%25203D%2520Gaussian%2520Splatting%2520%2528HEMGS%2529%2520to%2520achieve%2520hybrid%250Alossy-lossless%2520compression.%2520It%2520consists%2520of%2520three%2520main%2520components%253A%2520a%250Avariable-rate%2520predictor%252C%2520a%2520hyperprior%2520network%252C%2520and%2520an%2520autoregressive%2520network.%250AFirst%252C%2520unlike%2520previous%2520methods%2520that%2520adopt%2520multiple%2520models%2520to%2520achieve%2520multi-rate%250Alossy%2520compression%252C%2520thereby%2520increasing%2520training%2520overhead%252C%2520our%2520variable-rate%250Apredictor%2520enables%2520variable-rate%2520compression%2520with%2520a%2520single%2520model%2520and%2520a%250Ahyperparameter%2520%2524%255Clambda%2524%2520by%2520producing%2520a%2520learned%2520Quantization%2520Step%2520feature%2520for%250Aversatile%2520lossy%2520compression.%2520Second%252C%2520to%2520improve%2520lossless%2520compression%252C%2520the%250Ahyperprior%2520network%2520captures%2520both%2520scene-agnostic%2520and%2520scene-specific%2520features%2520to%250Agenerate%2520a%2520prior%2520feature%252C%2520while%2520the%2520autoregressive%2520network%2520employs%2520an%2520adaptive%250Acontext%2520selection%2520algorithm%2520with%2520flexible%2520receptive%2520fields%2520to%2520produce%2520a%250Acontextual%2520feature.%2520By%2520integrating%2520these%2520two%2520features%252C%2520HEMGS%2520can%2520accurately%250Aestimate%2520the%2520distribution%2520of%2520the%2520current%2520coding%2520element%2520within%2520each%2520attribute%252C%250Aenabling%2520improved%2520entropy%2520coding%2520and%2520reduced%2520storage.%2520We%2520integrate%2520HEMGS%2520into%2520a%250Acompression%2520framework%252C%2520and%2520experimental%2520results%2520on%2520four%2520benchmarks%2520indicate%250Athat%2520HEMGS%2520achieves%2520about%2520a%252040%2525%2520average%2520reduction%2520in%2520size%2520while%2520maintaining%250Arendering%2520quality%2520over%2520baseline%2520methods%2520and%2520achieving%2520state-of-the-art%250Acompression%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEMGS%3A%20A%20Hybrid%20Entropy%20Model%20for%203D%20Gaussian%20Splatting%20Data%20Compression&entry.906535625=Lei%20Liu%20and%20Zhenghao%20Chen%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Dong%20Xu&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20compression%20framework%20for%203D%20Gaussian%0ASplatting%20%283DGS%29%20data.%20Building%20on%20anchor-based%203DGS%20methodologies%2C%20our%0Aapproach%20compresses%20all%20attributes%20within%20each%20anchor%20by%20introducing%20a%20novel%0AHybrid%20Entropy%20Model%20for%203D%20Gaussian%20Splatting%20%28HEMGS%29%20to%20achieve%20hybrid%0Alossy-lossless%20compression.%20It%20consists%20of%20three%20main%20components%3A%20a%0Avariable-rate%20predictor%2C%20a%20hyperprior%20network%2C%20and%20an%20autoregressive%20network.%0AFirst%2C%20unlike%20previous%20methods%20that%20adopt%20multiple%20models%20to%20achieve%20multi-rate%0Alossy%20compression%2C%20thereby%20increasing%20training%20overhead%2C%20our%20variable-rate%0Apredictor%20enables%20variable-rate%20compression%20with%20a%20single%20model%20and%20a%0Ahyperparameter%20%24%5Clambda%24%20by%20producing%20a%20learned%20Quantization%20Step%20feature%20for%0Aversatile%20lossy%20compression.%20Second%2C%20to%20improve%20lossless%20compression%2C%20the%0Ahyperprior%20network%20captures%20both%20scene-agnostic%20and%20scene-specific%20features%20to%0Agenerate%20a%20prior%20feature%2C%20while%20the%20autoregressive%20network%20employs%20an%20adaptive%0Acontext%20selection%20algorithm%20with%20flexible%20receptive%20fields%20to%20produce%20a%0Acontextual%20feature.%20By%20integrating%20these%20two%20features%2C%20HEMGS%20can%20accurately%0Aestimate%20the%20distribution%20of%20the%20current%20coding%20element%20within%20each%20attribute%2C%0Aenabling%20improved%20entropy%20coding%20and%20reduced%20storage.%20We%20integrate%20HEMGS%20into%20a%0Acompression%20framework%2C%20and%20experimental%20results%20on%20four%20benchmarks%20indicate%0Athat%20HEMGS%20achieves%20about%20a%2040%25%20average%20reduction%20in%20size%20while%20maintaining%0Arendering%20quality%20over%20baseline%20methods%20and%20achieving%20state-of-the-art%0Acompression%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18473v2&entry.124074799=Read"},
{"title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive\n  Analysis", "author": "Frank Li and Hari Trivedi and Bardia Khosravi and Theo Dapamede and Mohammadreza Chavoshi and Abdulhameed Dere and Rohan Satya Isaac and Aawez Mansuri and Janice Newsome and Saptarshi Purkayastha and Judy Gichoya", "abstract": "  Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology.\n", "link": "http://arxiv.org/abs/2504.16047v1", "date": "2025-04-22", "relevancy": 3.1329, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6708}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Vision%20Language%20Models%20%28VLMs%29%20for%20Radiology%3A%20A%20Comprehensive%0A%20%20Analysis&body=Title%3A%20Evaluating%20Vision%20Language%20Models%20%28VLMs%29%20for%20Radiology%3A%20A%20Comprehensive%0A%20%20Analysis%0AAuthor%3A%20Frank%20Li%20and%20Hari%20Trivedi%20and%20Bardia%20Khosravi%20and%20Theo%20Dapamede%20and%20Mohammadreza%20Chavoshi%20and%20Abdulhameed%20Dere%20and%20Rohan%20Satya%20Isaac%20and%20Aawez%20Mansuri%20and%20Janice%20Newsome%20and%20Saptarshi%20Purkayastha%20and%20Judy%20Gichoya%0AAbstract%3A%20%20%20Foundation%20models%2C%20trained%20on%20vast%20amounts%20of%20data%20using%20self-supervised%0Atechniques%2C%20have%20emerged%20as%20a%20promising%20frontier%20for%20advancing%20artificial%0Aintelligence%20%28AI%29%20applications%20in%20medicine.%20This%20study%20evaluates%20three%0Adifferent%20vision-language%20foundation%20models%20%28RAD-DINO%2C%20CheXagent%2C%20and%0ABiomedCLIP%29%20on%20their%20ability%20to%20capture%20fine-grained%20imaging%20features%20for%0Aradiology%20tasks.%20The%20models%20were%20assessed%20across%20classification%2C%20segmentation%2C%0Aand%20regression%20tasks%20for%20pneumothorax%20and%20cardiomegaly%20on%20chest%20radiographs.%0ASelf-supervised%20RAD-DINO%20consistently%20excelled%20in%20segmentation%20tasks%2C%20while%0Atext-supervised%20CheXagent%20demonstrated%20superior%20classification%20performance.%0ABiomedCLIP%20showed%20inconsistent%20performance%20across%20tasks.%20A%20custom%20segmentation%0Amodel%20that%20integrates%20global%20and%20local%20features%20substantially%20improved%0Aperformance%20for%20all%20foundation%20models%2C%20particularly%20for%20challenging%0Apneumothorax%20segmentation.%20The%20findings%20highlight%20that%20pre-training%20methodology%0Asignificantly%20influences%20model%20performance%20on%20specific%20downstream%20tasks.%20For%0Afine-grained%20segmentation%20tasks%2C%20models%20trained%20without%20text%20supervision%0Aperformed%20better%2C%20while%20text-supervised%20models%20offered%20advantages%20in%0Aclassification%20and%20interpretability.%20These%20insights%20provide%20guidance%20for%0Aselecting%20foundation%20models%20based%20on%20specific%20clinical%20applications%20in%0Aradiology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520for%2520Radiology%253A%2520A%2520Comprehensive%250A%2520%2520Analysis%26entry.906535625%3DFrank%2520Li%2520and%2520Hari%2520Trivedi%2520and%2520Bardia%2520Khosravi%2520and%2520Theo%2520Dapamede%2520and%2520Mohammadreza%2520Chavoshi%2520and%2520Abdulhameed%2520Dere%2520and%2520Rohan%2520Satya%2520Isaac%2520and%2520Aawez%2520Mansuri%2520and%2520Janice%2520Newsome%2520and%2520Saptarshi%2520Purkayastha%2520and%2520Judy%2520Gichoya%26entry.1292438233%3D%2520%2520Foundation%2520models%252C%2520trained%2520on%2520vast%2520amounts%2520of%2520data%2520using%2520self-supervised%250Atechniques%252C%2520have%2520emerged%2520as%2520a%2520promising%2520frontier%2520for%2520advancing%2520artificial%250Aintelligence%2520%2528AI%2529%2520applications%2520in%2520medicine.%2520This%2520study%2520evaluates%2520three%250Adifferent%2520vision-language%2520foundation%2520models%2520%2528RAD-DINO%252C%2520CheXagent%252C%2520and%250ABiomedCLIP%2529%2520on%2520their%2520ability%2520to%2520capture%2520fine-grained%2520imaging%2520features%2520for%250Aradiology%2520tasks.%2520The%2520models%2520were%2520assessed%2520across%2520classification%252C%2520segmentation%252C%250Aand%2520regression%2520tasks%2520for%2520pneumothorax%2520and%2520cardiomegaly%2520on%2520chest%2520radiographs.%250ASelf-supervised%2520RAD-DINO%2520consistently%2520excelled%2520in%2520segmentation%2520tasks%252C%2520while%250Atext-supervised%2520CheXagent%2520demonstrated%2520superior%2520classification%2520performance.%250ABiomedCLIP%2520showed%2520inconsistent%2520performance%2520across%2520tasks.%2520A%2520custom%2520segmentation%250Amodel%2520that%2520integrates%2520global%2520and%2520local%2520features%2520substantially%2520improved%250Aperformance%2520for%2520all%2520foundation%2520models%252C%2520particularly%2520for%2520challenging%250Apneumothorax%2520segmentation.%2520The%2520findings%2520highlight%2520that%2520pre-training%2520methodology%250Asignificantly%2520influences%2520model%2520performance%2520on%2520specific%2520downstream%2520tasks.%2520For%250Afine-grained%2520segmentation%2520tasks%252C%2520models%2520trained%2520without%2520text%2520supervision%250Aperformed%2520better%252C%2520while%2520text-supervised%2520models%2520offered%2520advantages%2520in%250Aclassification%2520and%2520interpretability.%2520These%2520insights%2520provide%2520guidance%2520for%250Aselecting%2520foundation%2520models%2520based%2520on%2520specific%2520clinical%2520applications%2520in%250Aradiology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Vision%20Language%20Models%20%28VLMs%29%20for%20Radiology%3A%20A%20Comprehensive%0A%20%20Analysis&entry.906535625=Frank%20Li%20and%20Hari%20Trivedi%20and%20Bardia%20Khosravi%20and%20Theo%20Dapamede%20and%20Mohammadreza%20Chavoshi%20and%20Abdulhameed%20Dere%20and%20Rohan%20Satya%20Isaac%20and%20Aawez%20Mansuri%20and%20Janice%20Newsome%20and%20Saptarshi%20Purkayastha%20and%20Judy%20Gichoya&entry.1292438233=%20%20Foundation%20models%2C%20trained%20on%20vast%20amounts%20of%20data%20using%20self-supervised%0Atechniques%2C%20have%20emerged%20as%20a%20promising%20frontier%20for%20advancing%20artificial%0Aintelligence%20%28AI%29%20applications%20in%20medicine.%20This%20study%20evaluates%20three%0Adifferent%20vision-language%20foundation%20models%20%28RAD-DINO%2C%20CheXagent%2C%20and%0ABiomedCLIP%29%20on%20their%20ability%20to%20capture%20fine-grained%20imaging%20features%20for%0Aradiology%20tasks.%20The%20models%20were%20assessed%20across%20classification%2C%20segmentation%2C%0Aand%20regression%20tasks%20for%20pneumothorax%20and%20cardiomegaly%20on%20chest%20radiographs.%0ASelf-supervised%20RAD-DINO%20consistently%20excelled%20in%20segmentation%20tasks%2C%20while%0Atext-supervised%20CheXagent%20demonstrated%20superior%20classification%20performance.%0ABiomedCLIP%20showed%20inconsistent%20performance%20across%20tasks.%20A%20custom%20segmentation%0Amodel%20that%20integrates%20global%20and%20local%20features%20substantially%20improved%0Aperformance%20for%20all%20foundation%20models%2C%20particularly%20for%20challenging%0Apneumothorax%20segmentation.%20The%20findings%20highlight%20that%20pre-training%20methodology%0Asignificantly%20influences%20model%20performance%20on%20specific%20downstream%20tasks.%20For%0Afine-grained%20segmentation%20tasks%2C%20models%20trained%20without%20text%20supervision%0Aperformed%20better%2C%20while%20text-supervised%20models%20offered%20advantages%20in%0Aclassification%20and%20interpretability.%20These%20insights%20provide%20guidance%20for%0Aselecting%20foundation%20models%20based%20on%20specific%20clinical%20applications%20in%0Aradiology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16047v1&entry.124074799=Read"},
{"title": "Vision language models are unreliable at trivial spatial cognition", "author": "Sangeet Khemlani and Tyler Tran and Nathaniel Gyory and Anthony M. Harrison and Wallace E. Lawson and Ravenna Thielstrom and Hunter Thompson and Taaren Singh and J. Gregory Trafton", "abstract": "  Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing.\n", "link": "http://arxiv.org/abs/2504.16061v1", "date": "2025-04-22", "relevancy": 3.1074, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6588}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20language%20models%20are%20unreliable%20at%20trivial%20spatial%20cognition&body=Title%3A%20Vision%20language%20models%20are%20unreliable%20at%20trivial%20spatial%20cognition%0AAuthor%3A%20Sangeet%20Khemlani%20and%20Tyler%20Tran%20and%20Nathaniel%20Gyory%20and%20Anthony%20M.%20Harrison%20and%20Wallace%20E.%20Lawson%20and%20Ravenna%20Thielstrom%20and%20Hunter%20Thompson%20and%20Taaren%20Singh%20and%20J.%20Gregory%20Trafton%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20are%20designed%20to%20extract%20relevant%20visuospatial%0Ainformation%20from%20images.%20Some%20research%20suggests%20that%20VLMs%20can%20exhibit%20humanlike%0Ascene%20understanding%2C%20while%20other%20investigations%20reveal%20difficulties%20in%20their%0Aability%20to%20process%20relational%20information.%20To%20achieve%20widespread%20applicability%2C%0AVLMs%20must%20perform%20reliably%2C%20yielding%20comparable%20competence%20across%20a%20wide%0Avariety%20of%20related%20tasks.%20We%20sought%20to%20test%20how%20reliable%20these%20architectures%0Aare%20at%20engaging%20in%20trivial%20spatial%20cognition%2C%20e.g.%2C%20recognizing%20whether%20one%0Aobject%20is%20left%20of%20another%20in%20an%20uncluttered%20scene.%20We%20developed%20a%20benchmark%0Adataset%20--%20TableTest%20--%20whose%20images%20depict%203D%20scenes%20of%20objects%20arranged%20on%20a%0Atable%2C%20and%20used%20it%20to%20evaluate%20state-of-the-art%20VLMs.%20Results%20show%20that%0Aperformance%20could%20be%20degraded%20by%20minor%20variations%20of%20prompts%20that%20use%20logically%0Aequivalent%20descriptions.%20These%20analyses%20suggest%20limitations%20in%20how%20VLMs%20may%0Areason%20about%20spatial%20relations%20in%20real-world%20applications.%20They%20also%20reveal%0Anovel%20opportunities%20for%20bolstering%20image%20caption%20corpora%20for%20more%20efficient%0Atraining%20and%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520language%2520models%2520are%2520unreliable%2520at%2520trivial%2520spatial%2520cognition%26entry.906535625%3DSangeet%2520Khemlani%2520and%2520Tyler%2520Tran%2520and%2520Nathaniel%2520Gyory%2520and%2520Anthony%2520M.%2520Harrison%2520and%2520Wallace%2520E.%2520Lawson%2520and%2520Ravenna%2520Thielstrom%2520and%2520Hunter%2520Thompson%2520and%2520Taaren%2520Singh%2520and%2520J.%2520Gregory%2520Trafton%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520are%2520designed%2520to%2520extract%2520relevant%2520visuospatial%250Ainformation%2520from%2520images.%2520Some%2520research%2520suggests%2520that%2520VLMs%2520can%2520exhibit%2520humanlike%250Ascene%2520understanding%252C%2520while%2520other%2520investigations%2520reveal%2520difficulties%2520in%2520their%250Aability%2520to%2520process%2520relational%2520information.%2520To%2520achieve%2520widespread%2520applicability%252C%250AVLMs%2520must%2520perform%2520reliably%252C%2520yielding%2520comparable%2520competence%2520across%2520a%2520wide%250Avariety%2520of%2520related%2520tasks.%2520We%2520sought%2520to%2520test%2520how%2520reliable%2520these%2520architectures%250Aare%2520at%2520engaging%2520in%2520trivial%2520spatial%2520cognition%252C%2520e.g.%252C%2520recognizing%2520whether%2520one%250Aobject%2520is%2520left%2520of%2520another%2520in%2520an%2520uncluttered%2520scene.%2520We%2520developed%2520a%2520benchmark%250Adataset%2520--%2520TableTest%2520--%2520whose%2520images%2520depict%25203D%2520scenes%2520of%2520objects%2520arranged%2520on%2520a%250Atable%252C%2520and%2520used%2520it%2520to%2520evaluate%2520state-of-the-art%2520VLMs.%2520Results%2520show%2520that%250Aperformance%2520could%2520be%2520degraded%2520by%2520minor%2520variations%2520of%2520prompts%2520that%2520use%2520logically%250Aequivalent%2520descriptions.%2520These%2520analyses%2520suggest%2520limitations%2520in%2520how%2520VLMs%2520may%250Areason%2520about%2520spatial%2520relations%2520in%2520real-world%2520applications.%2520They%2520also%2520reveal%250Anovel%2520opportunities%2520for%2520bolstering%2520image%2520caption%2520corpora%2520for%2520more%2520efficient%250Atraining%2520and%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20language%20models%20are%20unreliable%20at%20trivial%20spatial%20cognition&entry.906535625=Sangeet%20Khemlani%20and%20Tyler%20Tran%20and%20Nathaniel%20Gyory%20and%20Anthony%20M.%20Harrison%20and%20Wallace%20E.%20Lawson%20and%20Ravenna%20Thielstrom%20and%20Hunter%20Thompson%20and%20Taaren%20Singh%20and%20J.%20Gregory%20Trafton&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20are%20designed%20to%20extract%20relevant%20visuospatial%0Ainformation%20from%20images.%20Some%20research%20suggests%20that%20VLMs%20can%20exhibit%20humanlike%0Ascene%20understanding%2C%20while%20other%20investigations%20reveal%20difficulties%20in%20their%0Aability%20to%20process%20relational%20information.%20To%20achieve%20widespread%20applicability%2C%0AVLMs%20must%20perform%20reliably%2C%20yielding%20comparable%20competence%20across%20a%20wide%0Avariety%20of%20related%20tasks.%20We%20sought%20to%20test%20how%20reliable%20these%20architectures%0Aare%20at%20engaging%20in%20trivial%20spatial%20cognition%2C%20e.g.%2C%20recognizing%20whether%20one%0Aobject%20is%20left%20of%20another%20in%20an%20uncluttered%20scene.%20We%20developed%20a%20benchmark%0Adataset%20--%20TableTest%20--%20whose%20images%20depict%203D%20scenes%20of%20objects%20arranged%20on%20a%0Atable%2C%20and%20used%20it%20to%20evaluate%20state-of-the-art%20VLMs.%20Results%20show%20that%0Aperformance%20could%20be%20degraded%20by%20minor%20variations%20of%20prompts%20that%20use%20logically%0Aequivalent%20descriptions.%20These%20analyses%20suggest%20limitations%20in%20how%20VLMs%20may%0Areason%20about%20spatial%20relations%20in%20real-world%20applications.%20They%20also%20reveal%0Anovel%20opportunities%20for%20bolstering%20image%20caption%20corpora%20for%20more%20efficient%0Atraining%20and%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16061v1&entry.124074799=Read"},
{"title": "RaSCL: Radar to Satellite Crossview Localization", "author": "Blerim Abdullai and Tony Wang and Xinyuan Qiao and Florian Shkurti and Timothy D. Barfoot", "abstract": "  GNSS is unreliable, inaccurate, and insufficient in many real-time autonomous\nfield applications. In this work, we present a GNSS-free global localization\nsolution that contains a method of registering imaging radar on the ground with\noverhead RGB imagery, with joint optimization of relative poses from odometry\nand global poses from our overhead registration. Previous works have used\nvarious combinations of ground sensors and overhead imagery, and different\nfeature extraction and matching methods. These include various handcrafted and\ndeep-learning-based methods for extracting features from overhead imagery. Our\nwork presents insights on extracting essential features from RGB overhead\nimages for effective global localization against overhead imagery using only\nground radar and a single georeferenced initial guess. We motivate our method\nby evaluating it on datasets in diverse geographic conditions and robotic\nplatforms, including on an Unmanned Surface Vessel (USV) as well as urban and\nsuburban driving datasets.\n", "link": "http://arxiv.org/abs/2504.15899v1", "date": "2025-04-22", "relevancy": 3.0708, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6575}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6301}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaSCL%3A%20Radar%20to%20Satellite%20Crossview%20Localization&body=Title%3A%20RaSCL%3A%20Radar%20to%20Satellite%20Crossview%20Localization%0AAuthor%3A%20Blerim%20Abdullai%20and%20Tony%20Wang%20and%20Xinyuan%20Qiao%20and%20Florian%20Shkurti%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20GNSS%20is%20unreliable%2C%20inaccurate%2C%20and%20insufficient%20in%20many%20real-time%20autonomous%0Afield%20applications.%20In%20this%20work%2C%20we%20present%20a%20GNSS-free%20global%20localization%0Asolution%20that%20contains%20a%20method%20of%20registering%20imaging%20radar%20on%20the%20ground%20with%0Aoverhead%20RGB%20imagery%2C%20with%20joint%20optimization%20of%20relative%20poses%20from%20odometry%0Aand%20global%20poses%20from%20our%20overhead%20registration.%20Previous%20works%20have%20used%0Avarious%20combinations%20of%20ground%20sensors%20and%20overhead%20imagery%2C%20and%20different%0Afeature%20extraction%20and%20matching%20methods.%20These%20include%20various%20handcrafted%20and%0Adeep-learning-based%20methods%20for%20extracting%20features%20from%20overhead%20imagery.%20Our%0Awork%20presents%20insights%20on%20extracting%20essential%20features%20from%20RGB%20overhead%0Aimages%20for%20effective%20global%20localization%20against%20overhead%20imagery%20using%20only%0Aground%20radar%20and%20a%20single%20georeferenced%20initial%20guess.%20We%20motivate%20our%20method%0Aby%20evaluating%20it%20on%20datasets%20in%20diverse%20geographic%20conditions%20and%20robotic%0Aplatforms%2C%20including%20on%20an%20Unmanned%20Surface%20Vessel%20%28USV%29%20as%20well%20as%20urban%20and%0Asuburban%20driving%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaSCL%253A%2520Radar%2520to%2520Satellite%2520Crossview%2520Localization%26entry.906535625%3DBlerim%2520Abdullai%2520and%2520Tony%2520Wang%2520and%2520Xinyuan%2520Qiao%2520and%2520Florian%2520Shkurti%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520GNSS%2520is%2520unreliable%252C%2520inaccurate%252C%2520and%2520insufficient%2520in%2520many%2520real-time%2520autonomous%250Afield%2520applications.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520GNSS-free%2520global%2520localization%250Asolution%2520that%2520contains%2520a%2520method%2520of%2520registering%2520imaging%2520radar%2520on%2520the%2520ground%2520with%250Aoverhead%2520RGB%2520imagery%252C%2520with%2520joint%2520optimization%2520of%2520relative%2520poses%2520from%2520odometry%250Aand%2520global%2520poses%2520from%2520our%2520overhead%2520registration.%2520Previous%2520works%2520have%2520used%250Avarious%2520combinations%2520of%2520ground%2520sensors%2520and%2520overhead%2520imagery%252C%2520and%2520different%250Afeature%2520extraction%2520and%2520matching%2520methods.%2520These%2520include%2520various%2520handcrafted%2520and%250Adeep-learning-based%2520methods%2520for%2520extracting%2520features%2520from%2520overhead%2520imagery.%2520Our%250Awork%2520presents%2520insights%2520on%2520extracting%2520essential%2520features%2520from%2520RGB%2520overhead%250Aimages%2520for%2520effective%2520global%2520localization%2520against%2520overhead%2520imagery%2520using%2520only%250Aground%2520radar%2520and%2520a%2520single%2520georeferenced%2520initial%2520guess.%2520We%2520motivate%2520our%2520method%250Aby%2520evaluating%2520it%2520on%2520datasets%2520in%2520diverse%2520geographic%2520conditions%2520and%2520robotic%250Aplatforms%252C%2520including%2520on%2520an%2520Unmanned%2520Surface%2520Vessel%2520%2528USV%2529%2520as%2520well%2520as%2520urban%2520and%250Asuburban%2520driving%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaSCL%3A%20Radar%20to%20Satellite%20Crossview%20Localization&entry.906535625=Blerim%20Abdullai%20and%20Tony%20Wang%20and%20Xinyuan%20Qiao%20and%20Florian%20Shkurti%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20GNSS%20is%20unreliable%2C%20inaccurate%2C%20and%20insufficient%20in%20many%20real-time%20autonomous%0Afield%20applications.%20In%20this%20work%2C%20we%20present%20a%20GNSS-free%20global%20localization%0Asolution%20that%20contains%20a%20method%20of%20registering%20imaging%20radar%20on%20the%20ground%20with%0Aoverhead%20RGB%20imagery%2C%20with%20joint%20optimization%20of%20relative%20poses%20from%20odometry%0Aand%20global%20poses%20from%20our%20overhead%20registration.%20Previous%20works%20have%20used%0Avarious%20combinations%20of%20ground%20sensors%20and%20overhead%20imagery%2C%20and%20different%0Afeature%20extraction%20and%20matching%20methods.%20These%20include%20various%20handcrafted%20and%0Adeep-learning-based%20methods%20for%20extracting%20features%20from%20overhead%20imagery.%20Our%0Awork%20presents%20insights%20on%20extracting%20essential%20features%20from%20RGB%20overhead%0Aimages%20for%20effective%20global%20localization%20against%20overhead%20imagery%20using%20only%0Aground%20radar%20and%20a%20single%20georeferenced%20initial%20guess.%20We%20motivate%20our%20method%0Aby%20evaluating%20it%20on%20datasets%20in%20diverse%20geographic%20conditions%20and%20robotic%0Aplatforms%2C%20including%20on%20an%20Unmanned%20Surface%20Vessel%20%28USV%29%20as%20well%20as%20urban%20and%0Asuburban%20driving%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15899v1&entry.124074799=Read"},
{"title": "GADS: A Super Lightweight Model for Head Pose Estimation", "author": "Menan Velayuthan and Asiri Gawesha and Purushoth Velayuthan and Nuwan Kodagoda and Dharshana Kasthurirathna and Pradeepa Samarasinghe", "abstract": "  In human-computer interaction, head pose estimation profoundly influences\napplication functionality. Although utilizing facial landmarks is valuable for\nthis purpose, existing landmark-based methods prioritize precision over\nsimplicity and model size, limiting their deployment on edge devices and in\ncompute-poor environments. To bridge this gap, we propose \\textbf{Grouped\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\nframework. By grouping landmarks into regions and employing small Deep Set\nlayers, we reduce computational complexity. Our multihead attention mechanism\nextracts and combines inter-group information, resulting in a model that is\n$7.5\\times$ smaller and executes $25\\times$ faster than the current lightest\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\nbeing $4321\\times$ smaller than the best-performing model. We introduce vanilla\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\nas a robust baseline for resource-constrained head pose estimation methods.\n", "link": "http://arxiv.org/abs/2504.15751v1", "date": "2025-04-22", "relevancy": 2.9965, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6335}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5822}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GADS%3A%20A%20Super%20Lightweight%20Model%20for%20Head%20Pose%20Estimation&body=Title%3A%20GADS%3A%20A%20Super%20Lightweight%20Model%20for%20Head%20Pose%20Estimation%0AAuthor%3A%20Menan%20Velayuthan%20and%20Asiri%20Gawesha%20and%20Purushoth%20Velayuthan%20and%20Nuwan%20Kodagoda%20and%20Dharshana%20Kasthurirathna%20and%20Pradeepa%20Samarasinghe%0AAbstract%3A%20%20%20In%20human-computer%20interaction%2C%20head%20pose%20estimation%20profoundly%20influences%0Aapplication%20functionality.%20Although%20utilizing%20facial%20landmarks%20is%20valuable%20for%0Athis%20purpose%2C%20existing%20landmark-based%20methods%20prioritize%20precision%20over%0Asimplicity%20and%20model%20size%2C%20limiting%20their%20deployment%20on%20edge%20devices%20and%20in%0Acompute-poor%20environments.%20To%20bridge%20this%20gap%2C%20we%20propose%20%5Ctextbf%7BGrouped%0AAttention%20Deep%20Sets%20%28GADS%29%7D%2C%20a%20novel%20architecture%20based%20on%20the%20Deep%20Set%0Aframework.%20By%20grouping%20landmarks%20into%20regions%20and%20employing%20small%20Deep%20Set%0Alayers%2C%20we%20reduce%20computational%20complexity.%20Our%20multihead%20attention%20mechanism%0Aextracts%20and%20combines%20inter-group%20information%2C%20resulting%20in%20a%20model%20that%20is%0A%247.5%5Ctimes%24%20smaller%20and%20executes%20%2425%5Ctimes%24%20faster%20than%20the%20current%20lightest%0Astate-of-the-art%20model.%20Notably%2C%20our%20method%20achieves%20an%20impressive%20reduction%2C%0Abeing%20%244321%5Ctimes%24%20smaller%20than%20the%20best-performing%20model.%20We%20introduce%20vanilla%0AGADS%20and%20Hybrid-GADS%20%28landmarks%20%2B%20RGB%29%20and%20evaluate%20our%20models%20on%20three%0Abenchmark%20datasets%20--%20AFLW2000%2C%20BIWI%2C%20and%20300W-LP.%20We%20envision%20our%20architecture%0Aas%20a%20robust%20baseline%20for%20resource-constrained%20head%20pose%20estimation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGADS%253A%2520A%2520Super%2520Lightweight%2520Model%2520for%2520Head%2520Pose%2520Estimation%26entry.906535625%3DMenan%2520Velayuthan%2520and%2520Asiri%2520Gawesha%2520and%2520Purushoth%2520Velayuthan%2520and%2520Nuwan%2520Kodagoda%2520and%2520Dharshana%2520Kasthurirathna%2520and%2520Pradeepa%2520Samarasinghe%26entry.1292438233%3D%2520%2520In%2520human-computer%2520interaction%252C%2520head%2520pose%2520estimation%2520profoundly%2520influences%250Aapplication%2520functionality.%2520Although%2520utilizing%2520facial%2520landmarks%2520is%2520valuable%2520for%250Athis%2520purpose%252C%2520existing%2520landmark-based%2520methods%2520prioritize%2520precision%2520over%250Asimplicity%2520and%2520model%2520size%252C%2520limiting%2520their%2520deployment%2520on%2520edge%2520devices%2520and%2520in%250Acompute-poor%2520environments.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520%255Ctextbf%257BGrouped%250AAttention%2520Deep%2520Sets%2520%2528GADS%2529%257D%252C%2520a%2520novel%2520architecture%2520based%2520on%2520the%2520Deep%2520Set%250Aframework.%2520By%2520grouping%2520landmarks%2520into%2520regions%2520and%2520employing%2520small%2520Deep%2520Set%250Alayers%252C%2520we%2520reduce%2520computational%2520complexity.%2520Our%2520multihead%2520attention%2520mechanism%250Aextracts%2520and%2520combines%2520inter-group%2520information%252C%2520resulting%2520in%2520a%2520model%2520that%2520is%250A%25247.5%255Ctimes%2524%2520smaller%2520and%2520executes%2520%252425%255Ctimes%2524%2520faster%2520than%2520the%2520current%2520lightest%250Astate-of-the-art%2520model.%2520Notably%252C%2520our%2520method%2520achieves%2520an%2520impressive%2520reduction%252C%250Abeing%2520%25244321%255Ctimes%2524%2520smaller%2520than%2520the%2520best-performing%2520model.%2520We%2520introduce%2520vanilla%250AGADS%2520and%2520Hybrid-GADS%2520%2528landmarks%2520%252B%2520RGB%2529%2520and%2520evaluate%2520our%2520models%2520on%2520three%250Abenchmark%2520datasets%2520--%2520AFLW2000%252C%2520BIWI%252C%2520and%2520300W-LP.%2520We%2520envision%2520our%2520architecture%250Aas%2520a%2520robust%2520baseline%2520for%2520resource-constrained%2520head%2520pose%2520estimation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GADS%3A%20A%20Super%20Lightweight%20Model%20for%20Head%20Pose%20Estimation&entry.906535625=Menan%20Velayuthan%20and%20Asiri%20Gawesha%20and%20Purushoth%20Velayuthan%20and%20Nuwan%20Kodagoda%20and%20Dharshana%20Kasthurirathna%20and%20Pradeepa%20Samarasinghe&entry.1292438233=%20%20In%20human-computer%20interaction%2C%20head%20pose%20estimation%20profoundly%20influences%0Aapplication%20functionality.%20Although%20utilizing%20facial%20landmarks%20is%20valuable%20for%0Athis%20purpose%2C%20existing%20landmark-based%20methods%20prioritize%20precision%20over%0Asimplicity%20and%20model%20size%2C%20limiting%20their%20deployment%20on%20edge%20devices%20and%20in%0Acompute-poor%20environments.%20To%20bridge%20this%20gap%2C%20we%20propose%20%5Ctextbf%7BGrouped%0AAttention%20Deep%20Sets%20%28GADS%29%7D%2C%20a%20novel%20architecture%20based%20on%20the%20Deep%20Set%0Aframework.%20By%20grouping%20landmarks%20into%20regions%20and%20employing%20small%20Deep%20Set%0Alayers%2C%20we%20reduce%20computational%20complexity.%20Our%20multihead%20attention%20mechanism%0Aextracts%20and%20combines%20inter-group%20information%2C%20resulting%20in%20a%20model%20that%20is%0A%247.5%5Ctimes%24%20smaller%20and%20executes%20%2425%5Ctimes%24%20faster%20than%20the%20current%20lightest%0Astate-of-the-art%20model.%20Notably%2C%20our%20method%20achieves%20an%20impressive%20reduction%2C%0Abeing%20%244321%5Ctimes%24%20smaller%20than%20the%20best-performing%20model.%20We%20introduce%20vanilla%0AGADS%20and%20Hybrid-GADS%20%28landmarks%20%2B%20RGB%29%20and%20evaluate%20our%20models%20on%20three%0Abenchmark%20datasets%20--%20AFLW2000%2C%20BIWI%2C%20and%20300W-LP.%20We%20envision%20our%20architecture%0Aas%20a%20robust%20baseline%20for%20resource-constrained%20head%20pose%20estimation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15751v1&entry.124074799=Read"},
{"title": "Enhancing Features in Long-tailed Data Using Large Vision Model", "author": "Pengxiao Han and Changkun Ye and Jinguang Tong and Cuicui Jiang and Jie Hong and Li Fang and Xuesong Li", "abstract": "  Language-based foundation models, such as large language models (LLMs) or\nlarge vision-language models (LVLMs), have been widely studied in long-tailed\nrecognition. However, the need for linguistic data is not applicable to all\npractical tasks. In this study, we aim to explore using large vision models\n(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features\nwithout any language information. Specifically, we extract features from the\nLVM and fuse them with features in the baseline network's map and latent space\nto obtain the augmented features. Moreover, we design several prototype-based\nlosses in the latent space to further exploit the potential of the augmented\nfeatures. In the experimental section, we validate our approach on two\nbenchmark datasets: ImageNet-LT and iNaturalist2018.\n", "link": "http://arxiv.org/abs/2504.10852v2", "date": "2025-04-22", "relevancy": 2.9369, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5921}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Features%20in%20Long-tailed%20Data%20Using%20Large%20Vision%20Model&body=Title%3A%20Enhancing%20Features%20in%20Long-tailed%20Data%20Using%20Large%20Vision%20Model%0AAuthor%3A%20Pengxiao%20Han%20and%20Changkun%20Ye%20and%20Jinguang%20Tong%20and%20Cuicui%20Jiang%20and%20Jie%20Hong%20and%20Li%20Fang%20and%20Xuesong%20Li%0AAbstract%3A%20%20%20Language-based%20foundation%20models%2C%20such%20as%20large%20language%20models%20%28LLMs%29%20or%0Alarge%20vision-language%20models%20%28LVLMs%29%2C%20have%20been%20widely%20studied%20in%20long-tailed%0Arecognition.%20However%2C%20the%20need%20for%20linguistic%20data%20is%20not%20applicable%20to%20all%0Apractical%20tasks.%20In%20this%20study%2C%20we%20aim%20to%20explore%20using%20large%20vision%20models%0A%28LVMs%29%20or%20visual%20foundation%20models%20%28VFMs%29%20to%20enhance%20long-tailed%20data%20features%0Awithout%20any%20language%20information.%20Specifically%2C%20we%20extract%20features%20from%20the%0ALVM%20and%20fuse%20them%20with%20features%20in%20the%20baseline%20network%27s%20map%20and%20latent%20space%0Ato%20obtain%20the%20augmented%20features.%20Moreover%2C%20we%20design%20several%20prototype-based%0Alosses%20in%20the%20latent%20space%20to%20further%20exploit%20the%20potential%20of%20the%20augmented%0Afeatures.%20In%20the%20experimental%20section%2C%20we%20validate%20our%20approach%20on%20two%0Abenchmark%20datasets%3A%20ImageNet-LT%20and%20iNaturalist2018.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10852v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Features%2520in%2520Long-tailed%2520Data%2520Using%2520Large%2520Vision%2520Model%26entry.906535625%3DPengxiao%2520Han%2520and%2520Changkun%2520Ye%2520and%2520Jinguang%2520Tong%2520and%2520Cuicui%2520Jiang%2520and%2520Jie%2520Hong%2520and%2520Li%2520Fang%2520and%2520Xuesong%2520Li%26entry.1292438233%3D%2520%2520Language-based%2520foundation%2520models%252C%2520such%2520as%2520large%2520language%2520models%2520%2528LLMs%2529%2520or%250Alarge%2520vision-language%2520models%2520%2528LVLMs%2529%252C%2520have%2520been%2520widely%2520studied%2520in%2520long-tailed%250Arecognition.%2520However%252C%2520the%2520need%2520for%2520linguistic%2520data%2520is%2520not%2520applicable%2520to%2520all%250Apractical%2520tasks.%2520In%2520this%2520study%252C%2520we%2520aim%2520to%2520explore%2520using%2520large%2520vision%2520models%250A%2528LVMs%2529%2520or%2520visual%2520foundation%2520models%2520%2528VFMs%2529%2520to%2520enhance%2520long-tailed%2520data%2520features%250Awithout%2520any%2520language%2520information.%2520Specifically%252C%2520we%2520extract%2520features%2520from%2520the%250ALVM%2520and%2520fuse%2520them%2520with%2520features%2520in%2520the%2520baseline%2520network%2527s%2520map%2520and%2520latent%2520space%250Ato%2520obtain%2520the%2520augmented%2520features.%2520Moreover%252C%2520we%2520design%2520several%2520prototype-based%250Alosses%2520in%2520the%2520latent%2520space%2520to%2520further%2520exploit%2520the%2520potential%2520of%2520the%2520augmented%250Afeatures.%2520In%2520the%2520experimental%2520section%252C%2520we%2520validate%2520our%2520approach%2520on%2520two%250Abenchmark%2520datasets%253A%2520ImageNet-LT%2520and%2520iNaturalist2018.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10852v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Features%20in%20Long-tailed%20Data%20Using%20Large%20Vision%20Model&entry.906535625=Pengxiao%20Han%20and%20Changkun%20Ye%20and%20Jinguang%20Tong%20and%20Cuicui%20Jiang%20and%20Jie%20Hong%20and%20Li%20Fang%20and%20Xuesong%20Li&entry.1292438233=%20%20Language-based%20foundation%20models%2C%20such%20as%20large%20language%20models%20%28LLMs%29%20or%0Alarge%20vision-language%20models%20%28LVLMs%29%2C%20have%20been%20widely%20studied%20in%20long-tailed%0Arecognition.%20However%2C%20the%20need%20for%20linguistic%20data%20is%20not%20applicable%20to%20all%0Apractical%20tasks.%20In%20this%20study%2C%20we%20aim%20to%20explore%20using%20large%20vision%20models%0A%28LVMs%29%20or%20visual%20foundation%20models%20%28VFMs%29%20to%20enhance%20long-tailed%20data%20features%0Awithout%20any%20language%20information.%20Specifically%2C%20we%20extract%20features%20from%20the%0ALVM%20and%20fuse%20them%20with%20features%20in%20the%20baseline%20network%27s%20map%20and%20latent%20space%0Ato%20obtain%20the%20augmented%20features.%20Moreover%2C%20we%20design%20several%20prototype-based%0Alosses%20in%20the%20latent%20space%20to%20further%20exploit%20the%20potential%20of%20the%20augmented%0Afeatures.%20In%20the%20experimental%20section%2C%20we%20validate%20our%20approach%20on%20two%0Abenchmark%20datasets%3A%20ImageNet-LT%20and%20iNaturalist2018.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10852v2&entry.124074799=Read"},
{"title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention", "author": "Yucheng Li and Huiqiang Jiang and Chengruidong Zhang and Qianhui Wu and Xufang Luo and Surin Ahn and Amir H. Abdi and Dongsheng Li and Jianfeng Gao and Yuqing Yang and Lili Qiu", "abstract": "  The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.\n", "link": "http://arxiv.org/abs/2504.16083v1", "date": "2025-04-22", "relevancy": 2.9348, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMInference%3A%20Accelerating%20Pre-filling%20for%20Long-Context%20VLMs%20via%0A%20%20Modality-Aware%20Permutation%20Sparse%20Attention&body=Title%3A%20MMInference%3A%20Accelerating%20Pre-filling%20for%20Long-Context%20VLMs%20via%0A%20%20Modality-Aware%20Permutation%20Sparse%20Attention%0AAuthor%3A%20Yucheng%20Li%20and%20Huiqiang%20Jiang%20and%20Chengruidong%20Zhang%20and%20Qianhui%20Wu%20and%20Xufang%20Luo%20and%20Surin%20Ahn%20and%20Amir%20H.%20Abdi%20and%20Dongsheng%20Li%20and%20Jianfeng%20Gao%20and%20Yuqing%20Yang%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20The%20integration%20of%20long-context%20capabilities%20with%20visual%20understanding%0Aunlocks%20unprecedented%20potential%20for%20Vision%20Language%20Models%20%28VLMs%29.%20However%2C%20the%0Aquadratic%20attention%20complexity%20during%20the%20pre-filling%20phase%20remains%20a%0Asignificant%20obstacle%20to%20real-world%20deployment.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20MMInference%20%28Multimodality%20Million%20tokens%20Inference%29%2C%20a%20dynamic%0Asparse%20attention%20method%20that%20accelerates%20the%20prefilling%20stage%20for%20long-context%0Amulti-modal%20inputs.%20First%2C%20our%20analysis%20reveals%20that%20the%20temporal%20and%20spatial%0Alocality%20of%20video%20input%20leads%20to%20a%20unique%20sparse%20pattern%2C%20the%20Grid%20pattern.%0ASimultaneously%2C%20VLMs%20exhibit%20markedly%20different%20sparse%20distributions%20across%0Adifferent%20modalities.%20We%20introduce%20a%20permutation-based%20method%20to%20leverage%20the%0Aunique%20Grid%20pattern%20and%20handle%20modality%20boundary%20issues.%20By%20offline%20search%20the%0Aoptimal%20sparse%20patterns%20for%20each%20head%2C%20MMInference%20constructs%20the%20sparse%0Adistribution%20dynamically%20based%20on%20the%20input.%20We%20also%20provide%20optimized%20GPU%0Akernels%20for%20efficient%20sparse%20computations.%20Notably%2C%20MMInference%20integrates%0Aseamlessly%20into%20existing%20VLM%20pipelines%20without%20any%20model%20modifications%20or%0Afine-tuning.%20Experiments%20on%20multi-modal%20benchmarks-including%20Video%20QA%2C%0ACaptioning%2C%20VisionNIAH%2C%20and%20Mixed-Modality%20NIAH-with%20state-of-the-art%0Along-context%20VLMs%20%28LongVila%2C%20LlavaVideo%2C%20VideoChat-Flash%2C%20Qwen2.5-VL%29%20show%20that%0AMMInference%20accelerates%20the%20pre-filling%20stage%20by%20up%20to%208.3x%20at%201M%20tokens%20while%0Amaintaining%20accuracy.%20Our%20code%20is%20available%20at%20https%3A//aka.ms/MMInference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMInference%253A%2520Accelerating%2520Pre-filling%2520for%2520Long-Context%2520VLMs%2520via%250A%2520%2520Modality-Aware%2520Permutation%2520Sparse%2520Attention%26entry.906535625%3DYucheng%2520Li%2520and%2520Huiqiang%2520Jiang%2520and%2520Chengruidong%2520Zhang%2520and%2520Qianhui%2520Wu%2520and%2520Xufang%2520Luo%2520and%2520Surin%2520Ahn%2520and%2520Amir%2520H.%2520Abdi%2520and%2520Dongsheng%2520Li%2520and%2520Jianfeng%2520Gao%2520and%2520Yuqing%2520Yang%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520long-context%2520capabilities%2520with%2520visual%2520understanding%250Aunlocks%2520unprecedented%2520potential%2520for%2520Vision%2520Language%2520Models%2520%2528VLMs%2529.%2520However%252C%2520the%250Aquadratic%2520attention%2520complexity%2520during%2520the%2520pre-filling%2520phase%2520remains%2520a%250Asignificant%2520obstacle%2520to%2520real-world%2520deployment.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Aintroduce%2520MMInference%2520%2528Multimodality%2520Million%2520tokens%2520Inference%2529%252C%2520a%2520dynamic%250Asparse%2520attention%2520method%2520that%2520accelerates%2520the%2520prefilling%2520stage%2520for%2520long-context%250Amulti-modal%2520inputs.%2520First%252C%2520our%2520analysis%2520reveals%2520that%2520the%2520temporal%2520and%2520spatial%250Alocality%2520of%2520video%2520input%2520leads%2520to%2520a%2520unique%2520sparse%2520pattern%252C%2520the%2520Grid%2520pattern.%250ASimultaneously%252C%2520VLMs%2520exhibit%2520markedly%2520different%2520sparse%2520distributions%2520across%250Adifferent%2520modalities.%2520We%2520introduce%2520a%2520permutation-based%2520method%2520to%2520leverage%2520the%250Aunique%2520Grid%2520pattern%2520and%2520handle%2520modality%2520boundary%2520issues.%2520By%2520offline%2520search%2520the%250Aoptimal%2520sparse%2520patterns%2520for%2520each%2520head%252C%2520MMInference%2520constructs%2520the%2520sparse%250Adistribution%2520dynamically%2520based%2520on%2520the%2520input.%2520We%2520also%2520provide%2520optimized%2520GPU%250Akernels%2520for%2520efficient%2520sparse%2520computations.%2520Notably%252C%2520MMInference%2520integrates%250Aseamlessly%2520into%2520existing%2520VLM%2520pipelines%2520without%2520any%2520model%2520modifications%2520or%250Afine-tuning.%2520Experiments%2520on%2520multi-modal%2520benchmarks-including%2520Video%2520QA%252C%250ACaptioning%252C%2520VisionNIAH%252C%2520and%2520Mixed-Modality%2520NIAH-with%2520state-of-the-art%250Along-context%2520VLMs%2520%2528LongVila%252C%2520LlavaVideo%252C%2520VideoChat-Flash%252C%2520Qwen2.5-VL%2529%2520show%2520that%250AMMInference%2520accelerates%2520the%2520pre-filling%2520stage%2520by%2520up%2520to%25208.3x%2520at%25201M%2520tokens%2520while%250Amaintaining%2520accuracy.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//aka.ms/MMInference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMInference%3A%20Accelerating%20Pre-filling%20for%20Long-Context%20VLMs%20via%0A%20%20Modality-Aware%20Permutation%20Sparse%20Attention&entry.906535625=Yucheng%20Li%20and%20Huiqiang%20Jiang%20and%20Chengruidong%20Zhang%20and%20Qianhui%20Wu%20and%20Xufang%20Luo%20and%20Surin%20Ahn%20and%20Amir%20H.%20Abdi%20and%20Dongsheng%20Li%20and%20Jianfeng%20Gao%20and%20Yuqing%20Yang%20and%20Lili%20Qiu&entry.1292438233=%20%20The%20integration%20of%20long-context%20capabilities%20with%20visual%20understanding%0Aunlocks%20unprecedented%20potential%20for%20Vision%20Language%20Models%20%28VLMs%29.%20However%2C%20the%0Aquadratic%20attention%20complexity%20during%20the%20pre-filling%20phase%20remains%20a%0Asignificant%20obstacle%20to%20real-world%20deployment.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20MMInference%20%28Multimodality%20Million%20tokens%20Inference%29%2C%20a%20dynamic%0Asparse%20attention%20method%20that%20accelerates%20the%20prefilling%20stage%20for%20long-context%0Amulti-modal%20inputs.%20First%2C%20our%20analysis%20reveals%20that%20the%20temporal%20and%20spatial%0Alocality%20of%20video%20input%20leads%20to%20a%20unique%20sparse%20pattern%2C%20the%20Grid%20pattern.%0ASimultaneously%2C%20VLMs%20exhibit%20markedly%20different%20sparse%20distributions%20across%0Adifferent%20modalities.%20We%20introduce%20a%20permutation-based%20method%20to%20leverage%20the%0Aunique%20Grid%20pattern%20and%20handle%20modality%20boundary%20issues.%20By%20offline%20search%20the%0Aoptimal%20sparse%20patterns%20for%20each%20head%2C%20MMInference%20constructs%20the%20sparse%0Adistribution%20dynamically%20based%20on%20the%20input.%20We%20also%20provide%20optimized%20GPU%0Akernels%20for%20efficient%20sparse%20computations.%20Notably%2C%20MMInference%20integrates%0Aseamlessly%20into%20existing%20VLM%20pipelines%20without%20any%20model%20modifications%20or%0Afine-tuning.%20Experiments%20on%20multi-modal%20benchmarks-including%20Video%20QA%2C%0ACaptioning%2C%20VisionNIAH%2C%20and%20Mixed-Modality%20NIAH-with%20state-of-the-art%0Along-context%20VLMs%20%28LongVila%2C%20LlavaVideo%2C%20VideoChat-Flash%2C%20Qwen2.5-VL%29%20show%20that%0AMMInference%20accelerates%20the%20pre-filling%20stage%20by%20up%20to%208.3x%20at%201M%20tokens%20while%0Amaintaining%20accuracy.%20Our%20code%20is%20available%20at%20https%3A//aka.ms/MMInference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16083v1&entry.124074799=Read"},
{"title": "Normal-guided Detail-Preserving Neural Implicit Function for\n  High-Fidelity 3D Surface Reconstruction", "author": "Aarya Patel and Hamid Laga and Ojaswa Sharma", "abstract": "  Neural implicit representations have emerged as a powerful paradigm for 3D\nreconstruction. However, despite their success, existing methods fail to\ncapture fine geometric details and thin structures, especially in scenarios\nwhere only sparse multi-view RGB images of the objects of interest are\navailable. This paper shows that training neural representations with\nfirst-order differential properties (surface normals) leads to highly accurate\n3D surface reconstruction, even with as few as two RGB images. Using input RGB\nimages, we compute approximate ground-truth surface normals from depth maps\nproduced by an off-the-shelf monocular depth estimator. During training, we\ndirectly locate the surface point of the SDF network and supervise its normal\nwith the one estimated from the depth map. Extensive experiments demonstrate\nthat our method achieves state-of-the-art reconstruction accuracy with a\nminimal number of views, capturing intricate geometric details and thin\nstructures that were previously challenging to capture.\n", "link": "http://arxiv.org/abs/2406.04861v2", "date": "2025-04-22", "relevancy": 2.8916, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5897}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5854}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normal-guided%20Detail-Preserving%20Neural%20Implicit%20Function%20for%0A%20%20High-Fidelity%203D%20Surface%20Reconstruction&body=Title%3A%20Normal-guided%20Detail-Preserving%20Neural%20Implicit%20Function%20for%0A%20%20High-Fidelity%203D%20Surface%20Reconstruction%0AAuthor%3A%20Aarya%20Patel%20and%20Hamid%20Laga%20and%20Ojaswa%20Sharma%0AAbstract%3A%20%20%20Neural%20implicit%20representations%20have%20emerged%20as%20a%20powerful%20paradigm%20for%203D%0Areconstruction.%20However%2C%20despite%20their%20success%2C%20existing%20methods%20fail%20to%0Acapture%20fine%20geometric%20details%20and%20thin%20structures%2C%20especially%20in%20scenarios%0Awhere%20only%20sparse%20multi-view%20RGB%20images%20of%20the%20objects%20of%20interest%20are%0Aavailable.%20This%20paper%20shows%20that%20training%20neural%20representations%20with%0Afirst-order%20differential%20properties%20%28surface%20normals%29%20leads%20to%20highly%20accurate%0A3D%20surface%20reconstruction%2C%20even%20with%20as%20few%20as%20two%20RGB%20images.%20Using%20input%20RGB%0Aimages%2C%20we%20compute%20approximate%20ground-truth%20surface%20normals%20from%20depth%20maps%0Aproduced%20by%20an%20off-the-shelf%20monocular%20depth%20estimator.%20During%20training%2C%20we%0Adirectly%20locate%20the%20surface%20point%20of%20the%20SDF%20network%20and%20supervise%20its%20normal%0Awith%20the%20one%20estimated%20from%20the%20depth%20map.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20reconstruction%20accuracy%20with%20a%0Aminimal%20number%20of%20views%2C%20capturing%20intricate%20geometric%20details%20and%20thin%0Astructures%20that%20were%20previously%20challenging%20to%20capture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04861v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormal-guided%2520Detail-Preserving%2520Neural%2520Implicit%2520Function%2520for%250A%2520%2520High-Fidelity%25203D%2520Surface%2520Reconstruction%26entry.906535625%3DAarya%2520Patel%2520and%2520Hamid%2520Laga%2520and%2520Ojaswa%2520Sharma%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representations%2520have%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%25203D%250Areconstruction.%2520However%252C%2520despite%2520their%2520success%252C%2520existing%2520methods%2520fail%2520to%250Acapture%2520fine%2520geometric%2520details%2520and%2520thin%2520structures%252C%2520especially%2520in%2520scenarios%250Awhere%2520only%2520sparse%2520multi-view%2520RGB%2520images%2520of%2520the%2520objects%2520of%2520interest%2520are%250Aavailable.%2520This%2520paper%2520shows%2520that%2520training%2520neural%2520representations%2520with%250Afirst-order%2520differential%2520properties%2520%2528surface%2520normals%2529%2520leads%2520to%2520highly%2520accurate%250A3D%2520surface%2520reconstruction%252C%2520even%2520with%2520as%2520few%2520as%2520two%2520RGB%2520images.%2520Using%2520input%2520RGB%250Aimages%252C%2520we%2520compute%2520approximate%2520ground-truth%2520surface%2520normals%2520from%2520depth%2520maps%250Aproduced%2520by%2520an%2520off-the-shelf%2520monocular%2520depth%2520estimator.%2520During%2520training%252C%2520we%250Adirectly%2520locate%2520the%2520surface%2520point%2520of%2520the%2520SDF%2520network%2520and%2520supervise%2520its%2520normal%250Awith%2520the%2520one%2520estimated%2520from%2520the%2520depth%2520map.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520achieves%2520state-of-the-art%2520reconstruction%2520accuracy%2520with%2520a%250Aminimal%2520number%2520of%2520views%252C%2520capturing%2520intricate%2520geometric%2520details%2520and%2520thin%250Astructures%2520that%2520were%2520previously%2520challenging%2520to%2520capture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04861v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normal-guided%20Detail-Preserving%20Neural%20Implicit%20Function%20for%0A%20%20High-Fidelity%203D%20Surface%20Reconstruction&entry.906535625=Aarya%20Patel%20and%20Hamid%20Laga%20and%20Ojaswa%20Sharma&entry.1292438233=%20%20Neural%20implicit%20representations%20have%20emerged%20as%20a%20powerful%20paradigm%20for%203D%0Areconstruction.%20However%2C%20despite%20their%20success%2C%20existing%20methods%20fail%20to%0Acapture%20fine%20geometric%20details%20and%20thin%20structures%2C%20especially%20in%20scenarios%0Awhere%20only%20sparse%20multi-view%20RGB%20images%20of%20the%20objects%20of%20interest%20are%0Aavailable.%20This%20paper%20shows%20that%20training%20neural%20representations%20with%0Afirst-order%20differential%20properties%20%28surface%20normals%29%20leads%20to%20highly%20accurate%0A3D%20surface%20reconstruction%2C%20even%20with%20as%20few%20as%20two%20RGB%20images.%20Using%20input%20RGB%0Aimages%2C%20we%20compute%20approximate%20ground-truth%20surface%20normals%20from%20depth%20maps%0Aproduced%20by%20an%20off-the-shelf%20monocular%20depth%20estimator.%20During%20training%2C%20we%0Adirectly%20locate%20the%20surface%20point%20of%20the%20SDF%20network%20and%20supervise%20its%20normal%0Awith%20the%20one%20estimated%20from%20the%20depth%20map.%20Extensive%20experiments%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20reconstruction%20accuracy%20with%20a%0Aminimal%20number%20of%20views%2C%20capturing%20intricate%20geometric%20details%20and%20thin%0Astructures%20that%20were%20previously%20challenging%20to%20capture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04861v2&entry.124074799=Read"},
{"title": "Describe Anything: Detailed Localized Image and Video Captioning", "author": "Long Lian and Yifan Ding and Yunhao Ge and Sifei Liu and Hanzi Mao and Boyi Li and Marco Pavone and Ming-Yu Liu and Trevor Darrell and Adam Yala and Yin Cui", "abstract": "  Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.\n", "link": "http://arxiv.org/abs/2504.16072v1", "date": "2025-04-22", "relevancy": 2.8765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Describe%20Anything%3A%20Detailed%20Localized%20Image%20and%20Video%20Captioning&body=Title%3A%20Describe%20Anything%3A%20Detailed%20Localized%20Image%20and%20Video%20Captioning%0AAuthor%3A%20Long%20Lian%20and%20Yifan%20Ding%20and%20Yunhao%20Ge%20and%20Sifei%20Liu%20and%20Hanzi%20Mao%20and%20Boyi%20Li%20and%20Marco%20Pavone%20and%20Ming-Yu%20Liu%20and%20Trevor%20Darrell%20and%20Adam%20Yala%20and%20Yin%20Cui%0AAbstract%3A%20%20%20Generating%20detailed%20and%20accurate%20descriptions%20for%20specific%20regions%20in%20images%0Aand%20videos%20remains%20a%20fundamental%20challenge%20for%20vision-language%20models.%20We%0Aintroduce%20the%20Describe%20Anything%20Model%20%28DAM%29%2C%20a%20model%20designed%20for%20detailed%0Alocalized%20captioning%20%28DLC%29.%20DAM%20preserves%20both%20local%20details%20and%20global%20context%0Athrough%20two%20key%20innovations%3A%20a%20focal%20prompt%2C%20which%20ensures%20high-resolution%0Aencoding%20of%20targeted%20regions%2C%20and%20a%20localized%20vision%20backbone%2C%20which%20integrates%0Aprecise%20localization%20with%20its%20broader%20context.%20To%20tackle%20the%20scarcity%20of%0Ahigh-quality%20DLC%20data%2C%20we%20propose%20a%20Semi-supervised%20learning%20%28SSL%29-based%20Data%0APipeline%20%28DLC-SDP%29.%20DLC-SDP%20starts%20with%20existing%20segmentation%20datasets%20and%0Aexpands%20to%20unlabeled%20web%20images%20using%20SSL.%20We%20introduce%20DLC-Bench%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20DLC%20without%20relying%20on%20reference%20captions.%20DAM%20sets%20new%0Astate-of-the-art%20on%207%20benchmarks%20spanning%20keyword-level%2C%20phrase-level%2C%20and%0Adetailed%20multi-sentence%20localized%20image%20and%20video%20captioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDescribe%2520Anything%253A%2520Detailed%2520Localized%2520Image%2520and%2520Video%2520Captioning%26entry.906535625%3DLong%2520Lian%2520and%2520Yifan%2520Ding%2520and%2520Yunhao%2520Ge%2520and%2520Sifei%2520Liu%2520and%2520Hanzi%2520Mao%2520and%2520Boyi%2520Li%2520and%2520Marco%2520Pavone%2520and%2520Ming-Yu%2520Liu%2520and%2520Trevor%2520Darrell%2520and%2520Adam%2520Yala%2520and%2520Yin%2520Cui%26entry.1292438233%3D%2520%2520Generating%2520detailed%2520and%2520accurate%2520descriptions%2520for%2520specific%2520regions%2520in%2520images%250Aand%2520videos%2520remains%2520a%2520fundamental%2520challenge%2520for%2520vision-language%2520models.%2520We%250Aintroduce%2520the%2520Describe%2520Anything%2520Model%2520%2528DAM%2529%252C%2520a%2520model%2520designed%2520for%2520detailed%250Alocalized%2520captioning%2520%2528DLC%2529.%2520DAM%2520preserves%2520both%2520local%2520details%2520and%2520global%2520context%250Athrough%2520two%2520key%2520innovations%253A%2520a%2520focal%2520prompt%252C%2520which%2520ensures%2520high-resolution%250Aencoding%2520of%2520targeted%2520regions%252C%2520and%2520a%2520localized%2520vision%2520backbone%252C%2520which%2520integrates%250Aprecise%2520localization%2520with%2520its%2520broader%2520context.%2520To%2520tackle%2520the%2520scarcity%2520of%250Ahigh-quality%2520DLC%2520data%252C%2520we%2520propose%2520a%2520Semi-supervised%2520learning%2520%2528SSL%2529-based%2520Data%250APipeline%2520%2528DLC-SDP%2529.%2520DLC-SDP%2520starts%2520with%2520existing%2520segmentation%2520datasets%2520and%250Aexpands%2520to%2520unlabeled%2520web%2520images%2520using%2520SSL.%2520We%2520introduce%2520DLC-Bench%252C%2520a%2520benchmark%250Adesigned%2520to%2520evaluate%2520DLC%2520without%2520relying%2520on%2520reference%2520captions.%2520DAM%2520sets%2520new%250Astate-of-the-art%2520on%25207%2520benchmarks%2520spanning%2520keyword-level%252C%2520phrase-level%252C%2520and%250Adetailed%2520multi-sentence%2520localized%2520image%2520and%2520video%2520captioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Describe%20Anything%3A%20Detailed%20Localized%20Image%20and%20Video%20Captioning&entry.906535625=Long%20Lian%20and%20Yifan%20Ding%20and%20Yunhao%20Ge%20and%20Sifei%20Liu%20and%20Hanzi%20Mao%20and%20Boyi%20Li%20and%20Marco%20Pavone%20and%20Ming-Yu%20Liu%20and%20Trevor%20Darrell%20and%20Adam%20Yala%20and%20Yin%20Cui&entry.1292438233=%20%20Generating%20detailed%20and%20accurate%20descriptions%20for%20specific%20regions%20in%20images%0Aand%20videos%20remains%20a%20fundamental%20challenge%20for%20vision-language%20models.%20We%0Aintroduce%20the%20Describe%20Anything%20Model%20%28DAM%29%2C%20a%20model%20designed%20for%20detailed%0Alocalized%20captioning%20%28DLC%29.%20DAM%20preserves%20both%20local%20details%20and%20global%20context%0Athrough%20two%20key%20innovations%3A%20a%20focal%20prompt%2C%20which%20ensures%20high-resolution%0Aencoding%20of%20targeted%20regions%2C%20and%20a%20localized%20vision%20backbone%2C%20which%20integrates%0Aprecise%20localization%20with%20its%20broader%20context.%20To%20tackle%20the%20scarcity%20of%0Ahigh-quality%20DLC%20data%2C%20we%20propose%20a%20Semi-supervised%20learning%20%28SSL%29-based%20Data%0APipeline%20%28DLC-SDP%29.%20DLC-SDP%20starts%20with%20existing%20segmentation%20datasets%20and%0Aexpands%20to%20unlabeled%20web%20images%20using%20SSL.%20We%20introduce%20DLC-Bench%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20DLC%20without%20relying%20on%20reference%20captions.%20DAM%20sets%20new%0Astate-of-the-art%20on%207%20benchmarks%20spanning%20keyword-level%2C%20phrase-level%2C%20and%0Adetailed%20multi-sentence%20localized%20image%20and%20video%20captioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16072v1&entry.124074799=Read"},
{"title": "LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene\n  Graphs with Weak Supervision", "author": "Jiani Huang and Ziyang Li and Mayur Naik and Ser-Nam Lim", "abstract": "  Supervised approaches for learning spatio-temporal scene graphs (STSG) from\nvideo are greatly hindered due to their reliance on STSG-annotated videos,\nwhich are labor-intensive to construct at scale. Is it feasible to instead use\nreadily available video captions as weak supervision? To address this question,\nwe propose LASER, a neuro-symbolic framework to enable training STSG generators\nusing only video captions. LASER employs large language models to first extract\nlogical specifications with rich spatio-temporal semantic information from\nvideo captions. LASER then trains the underlying STSG generator to align the\npredicted STSG with the specification. The alignment algorithm overcomes the\nchallenges of weak supervision by leveraging a differentiable symbolic reasoner\nand using a combination of contrastive, temporal, and semantics losses. The\noverall approach efficiently trains low-level perception models to extract a\nfine-grained STSG that conforms to the video caption. In doing so, it enables a\nnovel methodology for learning STSGs without tedious annotations. We evaluate\nour method on three video datasets: OpenPVSG, 20BN, and MUGEN. Our approach\ndemonstrates substantial improvements over fully-supervised baselines,\nachieving a unary predicate prediction accuracy of 27.78% (+12.65%) and a\nbinary recall@5 of 0.42 (+0.22) on OpenPVSG. Additionally, LASER exceeds\nbaselines by 7% on 20BN and 5.2% on MUGEN in terms of overall predicate\nprediction accuracy.\n", "link": "http://arxiv.org/abs/2304.07647v5", "date": "2025-04-22", "relevancy": 2.8437, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LASER%3A%20A%20Neuro-Symbolic%20Framework%20for%20Learning%20Spatial-Temporal%20Scene%0A%20%20Graphs%20with%20Weak%20Supervision&body=Title%3A%20LASER%3A%20A%20Neuro-Symbolic%20Framework%20for%20Learning%20Spatial-Temporal%20Scene%0A%20%20Graphs%20with%20Weak%20Supervision%0AAuthor%3A%20Jiani%20Huang%20and%20Ziyang%20Li%20and%20Mayur%20Naik%20and%20Ser-Nam%20Lim%0AAbstract%3A%20%20%20Supervised%20approaches%20for%20learning%20spatio-temporal%20scene%20graphs%20%28STSG%29%20from%0Avideo%20are%20greatly%20hindered%20due%20to%20their%20reliance%20on%20STSG-annotated%20videos%2C%0Awhich%20are%20labor-intensive%20to%20construct%20at%20scale.%20Is%20it%20feasible%20to%20instead%20use%0Areadily%20available%20video%20captions%20as%20weak%20supervision%3F%20To%20address%20this%20question%2C%0Awe%20propose%20LASER%2C%20a%20neuro-symbolic%20framework%20to%20enable%20training%20STSG%20generators%0Ausing%20only%20video%20captions.%20LASER%20employs%20large%20language%20models%20to%20first%20extract%0Alogical%20specifications%20with%20rich%20spatio-temporal%20semantic%20information%20from%0Avideo%20captions.%20LASER%20then%20trains%20the%20underlying%20STSG%20generator%20to%20align%20the%0Apredicted%20STSG%20with%20the%20specification.%20The%20alignment%20algorithm%20overcomes%20the%0Achallenges%20of%20weak%20supervision%20by%20leveraging%20a%20differentiable%20symbolic%20reasoner%0Aand%20using%20a%20combination%20of%20contrastive%2C%20temporal%2C%20and%20semantics%20losses.%20The%0Aoverall%20approach%20efficiently%20trains%20low-level%20perception%20models%20to%20extract%20a%0Afine-grained%20STSG%20that%20conforms%20to%20the%20video%20caption.%20In%20doing%20so%2C%20it%20enables%20a%0Anovel%20methodology%20for%20learning%20STSGs%20without%20tedious%20annotations.%20We%20evaluate%0Aour%20method%20on%20three%20video%20datasets%3A%20OpenPVSG%2C%2020BN%2C%20and%20MUGEN.%20Our%20approach%0Ademonstrates%20substantial%20improvements%20over%20fully-supervised%20baselines%2C%0Aachieving%20a%20unary%20predicate%20prediction%20accuracy%20of%2027.78%25%20%28%2B12.65%25%29%20and%20a%0Abinary%20recall%405%20of%200.42%20%28%2B0.22%29%20on%20OpenPVSG.%20Additionally%2C%20LASER%20exceeds%0Abaselines%20by%207%25%20on%2020BN%20and%205.2%25%20on%20MUGEN%20in%20terms%20of%20overall%20predicate%0Aprediction%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.07647v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLASER%253A%2520A%2520Neuro-Symbolic%2520Framework%2520for%2520Learning%2520Spatial-Temporal%2520Scene%250A%2520%2520Graphs%2520with%2520Weak%2520Supervision%26entry.906535625%3DJiani%2520Huang%2520and%2520Ziyang%2520Li%2520and%2520Mayur%2520Naik%2520and%2520Ser-Nam%2520Lim%26entry.1292438233%3D%2520%2520Supervised%2520approaches%2520for%2520learning%2520spatio-temporal%2520scene%2520graphs%2520%2528STSG%2529%2520from%250Avideo%2520are%2520greatly%2520hindered%2520due%2520to%2520their%2520reliance%2520on%2520STSG-annotated%2520videos%252C%250Awhich%2520are%2520labor-intensive%2520to%2520construct%2520at%2520scale.%2520Is%2520it%2520feasible%2520to%2520instead%2520use%250Areadily%2520available%2520video%2520captions%2520as%2520weak%2520supervision%253F%2520To%2520address%2520this%2520question%252C%250Awe%2520propose%2520LASER%252C%2520a%2520neuro-symbolic%2520framework%2520to%2520enable%2520training%2520STSG%2520generators%250Ausing%2520only%2520video%2520captions.%2520LASER%2520employs%2520large%2520language%2520models%2520to%2520first%2520extract%250Alogical%2520specifications%2520with%2520rich%2520spatio-temporal%2520semantic%2520information%2520from%250Avideo%2520captions.%2520LASER%2520then%2520trains%2520the%2520underlying%2520STSG%2520generator%2520to%2520align%2520the%250Apredicted%2520STSG%2520with%2520the%2520specification.%2520The%2520alignment%2520algorithm%2520overcomes%2520the%250Achallenges%2520of%2520weak%2520supervision%2520by%2520leveraging%2520a%2520differentiable%2520symbolic%2520reasoner%250Aand%2520using%2520a%2520combination%2520of%2520contrastive%252C%2520temporal%252C%2520and%2520semantics%2520losses.%2520The%250Aoverall%2520approach%2520efficiently%2520trains%2520low-level%2520perception%2520models%2520to%2520extract%2520a%250Afine-grained%2520STSG%2520that%2520conforms%2520to%2520the%2520video%2520caption.%2520In%2520doing%2520so%252C%2520it%2520enables%2520a%250Anovel%2520methodology%2520for%2520learning%2520STSGs%2520without%2520tedious%2520annotations.%2520We%2520evaluate%250Aour%2520method%2520on%2520three%2520video%2520datasets%253A%2520OpenPVSG%252C%252020BN%252C%2520and%2520MUGEN.%2520Our%2520approach%250Ademonstrates%2520substantial%2520improvements%2520over%2520fully-supervised%2520baselines%252C%250Aachieving%2520a%2520unary%2520predicate%2520prediction%2520accuracy%2520of%252027.78%2525%2520%2528%252B12.65%2525%2529%2520and%2520a%250Abinary%2520recall%25405%2520of%25200.42%2520%2528%252B0.22%2529%2520on%2520OpenPVSG.%2520Additionally%252C%2520LASER%2520exceeds%250Abaselines%2520by%25207%2525%2520on%252020BN%2520and%25205.2%2525%2520on%2520MUGEN%2520in%2520terms%2520of%2520overall%2520predicate%250Aprediction%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.07647v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LASER%3A%20A%20Neuro-Symbolic%20Framework%20for%20Learning%20Spatial-Temporal%20Scene%0A%20%20Graphs%20with%20Weak%20Supervision&entry.906535625=Jiani%20Huang%20and%20Ziyang%20Li%20and%20Mayur%20Naik%20and%20Ser-Nam%20Lim&entry.1292438233=%20%20Supervised%20approaches%20for%20learning%20spatio-temporal%20scene%20graphs%20%28STSG%29%20from%0Avideo%20are%20greatly%20hindered%20due%20to%20their%20reliance%20on%20STSG-annotated%20videos%2C%0Awhich%20are%20labor-intensive%20to%20construct%20at%20scale.%20Is%20it%20feasible%20to%20instead%20use%0Areadily%20available%20video%20captions%20as%20weak%20supervision%3F%20To%20address%20this%20question%2C%0Awe%20propose%20LASER%2C%20a%20neuro-symbolic%20framework%20to%20enable%20training%20STSG%20generators%0Ausing%20only%20video%20captions.%20LASER%20employs%20large%20language%20models%20to%20first%20extract%0Alogical%20specifications%20with%20rich%20spatio-temporal%20semantic%20information%20from%0Avideo%20captions.%20LASER%20then%20trains%20the%20underlying%20STSG%20generator%20to%20align%20the%0Apredicted%20STSG%20with%20the%20specification.%20The%20alignment%20algorithm%20overcomes%20the%0Achallenges%20of%20weak%20supervision%20by%20leveraging%20a%20differentiable%20symbolic%20reasoner%0Aand%20using%20a%20combination%20of%20contrastive%2C%20temporal%2C%20and%20semantics%20losses.%20The%0Aoverall%20approach%20efficiently%20trains%20low-level%20perception%20models%20to%20extract%20a%0Afine-grained%20STSG%20that%20conforms%20to%20the%20video%20caption.%20In%20doing%20so%2C%20it%20enables%20a%0Anovel%20methodology%20for%20learning%20STSGs%20without%20tedious%20annotations.%20We%20evaluate%0Aour%20method%20on%20three%20video%20datasets%3A%20OpenPVSG%2C%2020BN%2C%20and%20MUGEN.%20Our%20approach%0Ademonstrates%20substantial%20improvements%20over%20fully-supervised%20baselines%2C%0Aachieving%20a%20unary%20predicate%20prediction%20accuracy%20of%2027.78%25%20%28%2B12.65%25%29%20and%20a%0Abinary%20recall%405%20of%200.42%20%28%2B0.22%29%20on%20OpenPVSG.%20Additionally%2C%20LASER%20exceeds%0Abaselines%20by%207%25%20on%2020BN%20and%205.2%25%20on%20MUGEN%20in%20terms%20of%20overall%20predicate%0Aprediction%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.07647v5&entry.124074799=Read"},
{"title": "Clifford Group Equivariant Diffusion Models for 3D Molecular Generation", "author": "Cong Liu and Sharvaree Vadgama and David Ruhe and Erik Bekkers and Patrick Forr\u00e8", "abstract": "  This paper explores leveraging the Clifford algebra's expressive power for\n$\\E(n)$-equivariant diffusion models. We utilize the geometric products between\nClifford multivectors and the rich geometric information encoded in Clifford\nsubspaces in \\emph{Clifford Diffusion Models} (CDMs). We extend the diffusion\nprocess beyond just Clifford one-vectors to incorporate all higher-grade\nmultivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us\nto apply latent diffusion across complete multivectors. This enables CDMs to\ncapture the joint distribution across different subspaces of the algebra,\nincorporating richer geometric information through higher-order features. We\nprovide empirical results for unconditional molecular generation on the QM9\ndataset, showing that CDMs provide a promising avenue for generative modeling.\n", "link": "http://arxiv.org/abs/2504.15773v1", "date": "2025-04-22", "relevancy": 2.8273, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5782}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5782}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clifford%20Group%20Equivariant%20Diffusion%20Models%20for%203D%20Molecular%20Generation&body=Title%3A%20Clifford%20Group%20Equivariant%20Diffusion%20Models%20for%203D%20Molecular%20Generation%0AAuthor%3A%20Cong%20Liu%20and%20Sharvaree%20Vadgama%20and%20David%20Ruhe%20and%20Erik%20Bekkers%20and%20Patrick%20Forr%C3%A8%0AAbstract%3A%20%20%20This%20paper%20explores%20leveraging%20the%20Clifford%20algebra%27s%20expressive%20power%20for%0A%24%5CE%28n%29%24-equivariant%20diffusion%20models.%20We%20utilize%20the%20geometric%20products%20between%0AClifford%20multivectors%20and%20the%20rich%20geometric%20information%20encoded%20in%20Clifford%0Asubspaces%20in%20%5Cemph%7BClifford%20Diffusion%20Models%7D%20%28CDMs%29.%20We%20extend%20the%20diffusion%0Aprocess%20beyond%20just%20Clifford%20one-vectors%20to%20incorporate%20all%20higher-grade%0Amultivector%20subspaces.%20The%20data%20is%20embedded%20in%20grade-%24k%24%20subspaces%2C%20allowing%20us%0Ato%20apply%20latent%20diffusion%20across%20complete%20multivectors.%20This%20enables%20CDMs%20to%0Acapture%20the%20joint%20distribution%20across%20different%20subspaces%20of%20the%20algebra%2C%0Aincorporating%20richer%20geometric%20information%20through%20higher-order%20features.%20We%0Aprovide%20empirical%20results%20for%20unconditional%20molecular%20generation%20on%20the%20QM9%0Adataset%2C%20showing%20that%20CDMs%20provide%20a%20promising%20avenue%20for%20generative%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClifford%2520Group%2520Equivariant%2520Diffusion%2520Models%2520for%25203D%2520Molecular%2520Generation%26entry.906535625%3DCong%2520Liu%2520and%2520Sharvaree%2520Vadgama%2520and%2520David%2520Ruhe%2520and%2520Erik%2520Bekkers%2520and%2520Patrick%2520Forr%25C3%25A8%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520leveraging%2520the%2520Clifford%2520algebra%2527s%2520expressive%2520power%2520for%250A%2524%255CE%2528n%2529%2524-equivariant%2520diffusion%2520models.%2520We%2520utilize%2520the%2520geometric%2520products%2520between%250AClifford%2520multivectors%2520and%2520the%2520rich%2520geometric%2520information%2520encoded%2520in%2520Clifford%250Asubspaces%2520in%2520%255Cemph%257BClifford%2520Diffusion%2520Models%257D%2520%2528CDMs%2529.%2520We%2520extend%2520the%2520diffusion%250Aprocess%2520beyond%2520just%2520Clifford%2520one-vectors%2520to%2520incorporate%2520all%2520higher-grade%250Amultivector%2520subspaces.%2520The%2520data%2520is%2520embedded%2520in%2520grade-%2524k%2524%2520subspaces%252C%2520allowing%2520us%250Ato%2520apply%2520latent%2520diffusion%2520across%2520complete%2520multivectors.%2520This%2520enables%2520CDMs%2520to%250Acapture%2520the%2520joint%2520distribution%2520across%2520different%2520subspaces%2520of%2520the%2520algebra%252C%250Aincorporating%2520richer%2520geometric%2520information%2520through%2520higher-order%2520features.%2520We%250Aprovide%2520empirical%2520results%2520for%2520unconditional%2520molecular%2520generation%2520on%2520the%2520QM9%250Adataset%252C%2520showing%2520that%2520CDMs%2520provide%2520a%2520promising%2520avenue%2520for%2520generative%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clifford%20Group%20Equivariant%20Diffusion%20Models%20for%203D%20Molecular%20Generation&entry.906535625=Cong%20Liu%20and%20Sharvaree%20Vadgama%20and%20David%20Ruhe%20and%20Erik%20Bekkers%20and%20Patrick%20Forr%C3%A8&entry.1292438233=%20%20This%20paper%20explores%20leveraging%20the%20Clifford%20algebra%27s%20expressive%20power%20for%0A%24%5CE%28n%29%24-equivariant%20diffusion%20models.%20We%20utilize%20the%20geometric%20products%20between%0AClifford%20multivectors%20and%20the%20rich%20geometric%20information%20encoded%20in%20Clifford%0Asubspaces%20in%20%5Cemph%7BClifford%20Diffusion%20Models%7D%20%28CDMs%29.%20We%20extend%20the%20diffusion%0Aprocess%20beyond%20just%20Clifford%20one-vectors%20to%20incorporate%20all%20higher-grade%0Amultivector%20subspaces.%20The%20data%20is%20embedded%20in%20grade-%24k%24%20subspaces%2C%20allowing%20us%0Ato%20apply%20latent%20diffusion%20across%20complete%20multivectors.%20This%20enables%20CDMs%20to%0Acapture%20the%20joint%20distribution%20across%20different%20subspaces%20of%20the%20algebra%2C%0Aincorporating%20richer%20geometric%20information%20through%20higher-order%20features.%20We%0Aprovide%20empirical%20results%20for%20unconditional%20molecular%20generation%20on%20the%20QM9%0Adataset%2C%20showing%20that%20CDMs%20provide%20a%20promising%20avenue%20for%20generative%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15773v1&entry.124074799=Read"},
{"title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding", "author": "Ziqi Pang and Yu-Xiong Wang", "abstract": "  We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video\n", "link": "http://arxiv.org/abs/2504.16082v1", "date": "2025-04-22", "relevancy": 2.8267, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MR.%20Video%3A%20%22MapReduce%22%20is%20the%20Principle%20for%20Long%20Video%20Understanding&body=Title%3A%20MR.%20Video%3A%20%22MapReduce%22%20is%20the%20Principle%20for%20Long%20Video%20Understanding%0AAuthor%3A%20Ziqi%20Pang%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20We%20propose%20MR.%20Video%2C%20an%20agentic%20long%20video%20understanding%20framework%20that%0Ademonstrates%20the%20simple%20yet%20effective%20MapReduce%20principle%20for%20processing%20long%0Avideos%3A%20%281%29%20Map%3A%20independently%20and%20densely%20perceiving%20short%20video%20clips%2C%20and%0A%282%29%20Reduce%3A%20jointly%20aggregating%20information%20from%20all%20clips.%20Compared%20with%0Asequence-to-sequence%20vision-language%20models%20%28VLMs%29%2C%20MR.%20Video%20performs%20detailed%0Ashort%20video%20perception%20without%20being%20limited%20by%20context%20length.%20Compared%20with%0Aexisting%20video%20agents%20that%20typically%20rely%20on%20sequential%20key%20segment%20selection%2C%0Athe%20Map%20operation%20enables%20simpler%20and%20more%20scalable%20sequence%20parallel%0Aperception%20of%20short%20video%20segments.%20Its%20Reduce%20step%20allows%20for%20more%0Acomprehensive%20context%20aggregation%20and%20reasoning%2C%20surpassing%20explicit%20key%0Asegment%20retrieval.%20This%20MapReduce%20principle%20is%20applicable%20to%20both%20VLMs%20and%0Avideo%20agents%2C%20and%20we%20use%20LLM%20agents%20to%20validate%20its%20effectiveness.%0A%20%20In%20practice%2C%20MR.%20Video%20employs%20two%20MapReduce%20stages%3A%20%28A%29%20Captioning%3A%0Agenerating%20captions%20for%20short%20video%20clips%20%28map%29%2C%20then%20standardizing%20repeated%0Acharacters%20and%20objects%20into%20shared%20names%20%28reduce%29%3B%20%28B%29%20Analysis%3A%20for%20each%20user%0Aquestion%2C%20analyzing%20relevant%20information%20from%20individual%20short%20videos%20%28map%29%2C%0Aand%20integrating%20them%20into%20a%20final%20answer%20%28reduce%29.%20MR.%20Video%20achieves%20over%2010%25%0Aaccuracy%20improvement%20on%20the%20challenging%20LVBench%20compared%20to%20state-of-the-art%0AVLMs%20and%20video%20agents.%0A%20%20Code%20is%20available%20at%3A%20https%3A//github.com/ziqipang/MR-Video%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMR.%2520Video%253A%2520%2522MapReduce%2522%2520is%2520the%2520Principle%2520for%2520Long%2520Video%2520Understanding%26entry.906535625%3DZiqi%2520Pang%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520MR.%2520Video%252C%2520an%2520agentic%2520long%2520video%2520understanding%2520framework%2520that%250Ademonstrates%2520the%2520simple%2520yet%2520effective%2520MapReduce%2520principle%2520for%2520processing%2520long%250Avideos%253A%2520%25281%2529%2520Map%253A%2520independently%2520and%2520densely%2520perceiving%2520short%2520video%2520clips%252C%2520and%250A%25282%2529%2520Reduce%253A%2520jointly%2520aggregating%2520information%2520from%2520all%2520clips.%2520Compared%2520with%250Asequence-to-sequence%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520MR.%2520Video%2520performs%2520detailed%250Ashort%2520video%2520perception%2520without%2520being%2520limited%2520by%2520context%2520length.%2520Compared%2520with%250Aexisting%2520video%2520agents%2520that%2520typically%2520rely%2520on%2520sequential%2520key%2520segment%2520selection%252C%250Athe%2520Map%2520operation%2520enables%2520simpler%2520and%2520more%2520scalable%2520sequence%2520parallel%250Aperception%2520of%2520short%2520video%2520segments.%2520Its%2520Reduce%2520step%2520allows%2520for%2520more%250Acomprehensive%2520context%2520aggregation%2520and%2520reasoning%252C%2520surpassing%2520explicit%2520key%250Asegment%2520retrieval.%2520This%2520MapReduce%2520principle%2520is%2520applicable%2520to%2520both%2520VLMs%2520and%250Avideo%2520agents%252C%2520and%2520we%2520use%2520LLM%2520agents%2520to%2520validate%2520its%2520effectiveness.%250A%2520%2520In%2520practice%252C%2520MR.%2520Video%2520employs%2520two%2520MapReduce%2520stages%253A%2520%2528A%2529%2520Captioning%253A%250Agenerating%2520captions%2520for%2520short%2520video%2520clips%2520%2528map%2529%252C%2520then%2520standardizing%2520repeated%250Acharacters%2520and%2520objects%2520into%2520shared%2520names%2520%2528reduce%2529%253B%2520%2528B%2529%2520Analysis%253A%2520for%2520each%2520user%250Aquestion%252C%2520analyzing%2520relevant%2520information%2520from%2520individual%2520short%2520videos%2520%2528map%2529%252C%250Aand%2520integrating%2520them%2520into%2520a%2520final%2520answer%2520%2528reduce%2529.%2520MR.%2520Video%2520achieves%2520over%252010%2525%250Aaccuracy%2520improvement%2520on%2520the%2520challenging%2520LVBench%2520compared%2520to%2520state-of-the-art%250AVLMs%2520and%2520video%2520agents.%250A%2520%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/ziqipang/MR-Video%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MR.%20Video%3A%20%22MapReduce%22%20is%20the%20Principle%20for%20Long%20Video%20Understanding&entry.906535625=Ziqi%20Pang%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20We%20propose%20MR.%20Video%2C%20an%20agentic%20long%20video%20understanding%20framework%20that%0Ademonstrates%20the%20simple%20yet%20effective%20MapReduce%20principle%20for%20processing%20long%0Avideos%3A%20%281%29%20Map%3A%20independently%20and%20densely%20perceiving%20short%20video%20clips%2C%20and%0A%282%29%20Reduce%3A%20jointly%20aggregating%20information%20from%20all%20clips.%20Compared%20with%0Asequence-to-sequence%20vision-language%20models%20%28VLMs%29%2C%20MR.%20Video%20performs%20detailed%0Ashort%20video%20perception%20without%20being%20limited%20by%20context%20length.%20Compared%20with%0Aexisting%20video%20agents%20that%20typically%20rely%20on%20sequential%20key%20segment%20selection%2C%0Athe%20Map%20operation%20enables%20simpler%20and%20more%20scalable%20sequence%20parallel%0Aperception%20of%20short%20video%20segments.%20Its%20Reduce%20step%20allows%20for%20more%0Acomprehensive%20context%20aggregation%20and%20reasoning%2C%20surpassing%20explicit%20key%0Asegment%20retrieval.%20This%20MapReduce%20principle%20is%20applicable%20to%20both%20VLMs%20and%0Avideo%20agents%2C%20and%20we%20use%20LLM%20agents%20to%20validate%20its%20effectiveness.%0A%20%20In%20practice%2C%20MR.%20Video%20employs%20two%20MapReduce%20stages%3A%20%28A%29%20Captioning%3A%0Agenerating%20captions%20for%20short%20video%20clips%20%28map%29%2C%20then%20standardizing%20repeated%0Acharacters%20and%20objects%20into%20shared%20names%20%28reduce%29%3B%20%28B%29%20Analysis%3A%20for%20each%20user%0Aquestion%2C%20analyzing%20relevant%20information%20from%20individual%20short%20videos%20%28map%29%2C%0Aand%20integrating%20them%20into%20a%20final%20answer%20%28reduce%29.%20MR.%20Video%20achieves%20over%2010%25%0Aaccuracy%20improvement%20on%20the%20challenging%20LVBench%20compared%20to%20state-of-the-art%0AVLMs%20and%20video%20agents.%0A%20%20Code%20is%20available%20at%3A%20https%3A//github.com/ziqipang/MR-Video%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16082v1&entry.124074799=Read"},
{"title": "Prompting Depth Anything for 4K Resolution Accurate Metric Depth\n  Estimation", "author": "Haotong Lin and Sida Peng and Jingxiao Chen and Songyou Peng and Jiaming Sun and Minghuan Liu and Hujun Bao and Jiashi Feng and Xiaowei Zhou and Bingyi Kang", "abstract": "  Prompts play a critical role in unleashing the power of language and vision\nfoundation models for specific tasks. For the first time, we introduce\nprompting into depth foundation models, creating a new paradigm for metric\ndepth estimation termed Prompt Depth Anything. Specifically, we use a low-cost\nLiDAR as the prompt to guide the Depth Anything model for accurate metric depth\noutput, achieving up to 4K resolution. Our approach centers on a concise prompt\nfusion design that integrates the LiDAR at multiple scales within the depth\ndecoder. To address training challenges posed by limited datasets containing\nboth LiDAR depth and precise GT depth, we propose a scalable data pipeline that\nincludes synthetic data LiDAR simulation and real data pseudo GT depth\ngeneration. Our approach sets new state-of-the-arts on the ARKitScenes and\nScanNet++ datasets and benefits downstream applications, including 3D\nreconstruction and generalized robotic grasping.\n", "link": "http://arxiv.org/abs/2412.14015v2", "date": "2025-04-22", "relevancy": 2.818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5809}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompting%20Depth%20Anything%20for%204K%20Resolution%20Accurate%20Metric%20Depth%0A%20%20Estimation&body=Title%3A%20Prompting%20Depth%20Anything%20for%204K%20Resolution%20Accurate%20Metric%20Depth%0A%20%20Estimation%0AAuthor%3A%20Haotong%20Lin%20and%20Sida%20Peng%20and%20Jingxiao%20Chen%20and%20Songyou%20Peng%20and%20Jiaming%20Sun%20and%20Minghuan%20Liu%20and%20Hujun%20Bao%20and%20Jiashi%20Feng%20and%20Xiaowei%20Zhou%20and%20Bingyi%20Kang%0AAbstract%3A%20%20%20Prompts%20play%20a%20critical%20role%20in%20unleashing%20the%20power%20of%20language%20and%20vision%0Afoundation%20models%20for%20specific%20tasks.%20For%20the%20first%20time%2C%20we%20introduce%0Aprompting%20into%20depth%20foundation%20models%2C%20creating%20a%20new%20paradigm%20for%20metric%0Adepth%20estimation%20termed%20Prompt%20Depth%20Anything.%20Specifically%2C%20we%20use%20a%20low-cost%0ALiDAR%20as%20the%20prompt%20to%20guide%20the%20Depth%20Anything%20model%20for%20accurate%20metric%20depth%0Aoutput%2C%20achieving%20up%20to%204K%20resolution.%20Our%20approach%20centers%20on%20a%20concise%20prompt%0Afusion%20design%20that%20integrates%20the%20LiDAR%20at%20multiple%20scales%20within%20the%20depth%0Adecoder.%20To%20address%20training%20challenges%20posed%20by%20limited%20datasets%20containing%0Aboth%20LiDAR%20depth%20and%20precise%20GT%20depth%2C%20we%20propose%20a%20scalable%20data%20pipeline%20that%0Aincludes%20synthetic%20data%20LiDAR%20simulation%20and%20real%20data%20pseudo%20GT%20depth%0Ageneration.%20Our%20approach%20sets%20new%20state-of-the-arts%20on%20the%20ARKitScenes%20and%0AScanNet%2B%2B%20datasets%20and%20benefits%20downstream%20applications%2C%20including%203D%0Areconstruction%20and%20generalized%20robotic%20grasping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompting%2520Depth%2520Anything%2520for%25204K%2520Resolution%2520Accurate%2520Metric%2520Depth%250A%2520%2520Estimation%26entry.906535625%3DHaotong%2520Lin%2520and%2520Sida%2520Peng%2520and%2520Jingxiao%2520Chen%2520and%2520Songyou%2520Peng%2520and%2520Jiaming%2520Sun%2520and%2520Minghuan%2520Liu%2520and%2520Hujun%2520Bao%2520and%2520Jiashi%2520Feng%2520and%2520Xiaowei%2520Zhou%2520and%2520Bingyi%2520Kang%26entry.1292438233%3D%2520%2520Prompts%2520play%2520a%2520critical%2520role%2520in%2520unleashing%2520the%2520power%2520of%2520language%2520and%2520vision%250Afoundation%2520models%2520for%2520specific%2520tasks.%2520For%2520the%2520first%2520time%252C%2520we%2520introduce%250Aprompting%2520into%2520depth%2520foundation%2520models%252C%2520creating%2520a%2520new%2520paradigm%2520for%2520metric%250Adepth%2520estimation%2520termed%2520Prompt%2520Depth%2520Anything.%2520Specifically%252C%2520we%2520use%2520a%2520low-cost%250ALiDAR%2520as%2520the%2520prompt%2520to%2520guide%2520the%2520Depth%2520Anything%2520model%2520for%2520accurate%2520metric%2520depth%250Aoutput%252C%2520achieving%2520up%2520to%25204K%2520resolution.%2520Our%2520approach%2520centers%2520on%2520a%2520concise%2520prompt%250Afusion%2520design%2520that%2520integrates%2520the%2520LiDAR%2520at%2520multiple%2520scales%2520within%2520the%2520depth%250Adecoder.%2520To%2520address%2520training%2520challenges%2520posed%2520by%2520limited%2520datasets%2520containing%250Aboth%2520LiDAR%2520depth%2520and%2520precise%2520GT%2520depth%252C%2520we%2520propose%2520a%2520scalable%2520data%2520pipeline%2520that%250Aincludes%2520synthetic%2520data%2520LiDAR%2520simulation%2520and%2520real%2520data%2520pseudo%2520GT%2520depth%250Ageneration.%2520Our%2520approach%2520sets%2520new%2520state-of-the-arts%2520on%2520the%2520ARKitScenes%2520and%250AScanNet%252B%252B%2520datasets%2520and%2520benefits%2520downstream%2520applications%252C%2520including%25203D%250Areconstruction%2520and%2520generalized%2520robotic%2520grasping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompting%20Depth%20Anything%20for%204K%20Resolution%20Accurate%20Metric%20Depth%0A%20%20Estimation&entry.906535625=Haotong%20Lin%20and%20Sida%20Peng%20and%20Jingxiao%20Chen%20and%20Songyou%20Peng%20and%20Jiaming%20Sun%20and%20Minghuan%20Liu%20and%20Hujun%20Bao%20and%20Jiashi%20Feng%20and%20Xiaowei%20Zhou%20and%20Bingyi%20Kang&entry.1292438233=%20%20Prompts%20play%20a%20critical%20role%20in%20unleashing%20the%20power%20of%20language%20and%20vision%0Afoundation%20models%20for%20specific%20tasks.%20For%20the%20first%20time%2C%20we%20introduce%0Aprompting%20into%20depth%20foundation%20models%2C%20creating%20a%20new%20paradigm%20for%20metric%0Adepth%20estimation%20termed%20Prompt%20Depth%20Anything.%20Specifically%2C%20we%20use%20a%20low-cost%0ALiDAR%20as%20the%20prompt%20to%20guide%20the%20Depth%20Anything%20model%20for%20accurate%20metric%20depth%0Aoutput%2C%20achieving%20up%20to%204K%20resolution.%20Our%20approach%20centers%20on%20a%20concise%20prompt%0Afusion%20design%20that%20integrates%20the%20LiDAR%20at%20multiple%20scales%20within%20the%20depth%0Adecoder.%20To%20address%20training%20challenges%20posed%20by%20limited%20datasets%20containing%0Aboth%20LiDAR%20depth%20and%20precise%20GT%20depth%2C%20we%20propose%20a%20scalable%20data%20pipeline%20that%0Aincludes%20synthetic%20data%20LiDAR%20simulation%20and%20real%20data%20pseudo%20GT%20depth%0Ageneration.%20Our%20approach%20sets%20new%20state-of-the-arts%20on%20the%20ARKitScenes%20and%0AScanNet%2B%2B%20datasets%20and%20benefits%20downstream%20applications%2C%20including%203D%0Areconstruction%20and%20generalized%20robotic%20grasping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14015v2&entry.124074799=Read"},
{"title": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment", "author": "Yachun Mi and Yu Li and Weicheng Meng and Chaofeng Chen and Chen Hui and Shaohui Liu", "abstract": "  The rapid growth of long-duration, high-definition videos has made efficient\nvideo quality assessment (VQA) a critical challenge. Existing research\ntypically tackles this problem through two main strategies: reducing model\nparameters and resampling inputs. However, light-weight Convolution Neural\nNetworks (CNN) and Transformers often struggle to balance efficiency with high\nperformance due to the requirement of long-range modeling capabilities.\nRecently, the state-space model, particularly Mamba, has emerged as a promising\nalternative, offering linear complexity with respect to sequence length.\nMeanwhile, efficient VQA heavily depends on resampling long sequences to\nminimize computational costs, yet current resampling methods are often weak in\npreserving essential semantic information. In this work, we present MVQA, a\nMamba-based model designed for efficient VQA along with a novel Unified\nSemantic and Distortion Sampling (USDS) approach. USDS combines semantic patch\nsampling from low-resolution videos and distortion patch sampling from\noriginal-resolution videos. The former captures semantically dense regions,\nwhile the latter retains critical distortion details. To prevent computation\nincrease from dual inputs, we propose a fusion mechanism using pre-defined\nmasks, enabling a unified sampling strategy that captures both semantic and\nquality information without additional computational burden. Experiments show\nthat the proposed MVQA, equipped with USDS, achieve comparable performance to\nstate-of-the-art methods while being $2\\times$ as fast and requiring only $1/5$\nGPU memory.\n", "link": "http://arxiv.org/abs/2504.16003v1", "date": "2025-04-22", "relevancy": 2.7818, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5667}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.554}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVQA%3A%20Mamba%20with%20Unified%20Sampling%20for%20Efficient%20Video%20Quality%20Assessment&body=Title%3A%20MVQA%3A%20Mamba%20with%20Unified%20Sampling%20for%20Efficient%20Video%20Quality%20Assessment%0AAuthor%3A%20Yachun%20Mi%20and%20Yu%20Li%20and%20Weicheng%20Meng%20and%20Chaofeng%20Chen%20and%20Chen%20Hui%20and%20Shaohui%20Liu%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20long-duration%2C%20high-definition%20videos%20has%20made%20efficient%0Avideo%20quality%20assessment%20%28VQA%29%20a%20critical%20challenge.%20Existing%20research%0Atypically%20tackles%20this%20problem%20through%20two%20main%20strategies%3A%20reducing%20model%0Aparameters%20and%20resampling%20inputs.%20However%2C%20light-weight%20Convolution%20Neural%0ANetworks%20%28CNN%29%20and%20Transformers%20often%20struggle%20to%20balance%20efficiency%20with%20high%0Aperformance%20due%20to%20the%20requirement%20of%20long-range%20modeling%20capabilities.%0ARecently%2C%20the%20state-space%20model%2C%20particularly%20Mamba%2C%20has%20emerged%20as%20a%20promising%0Aalternative%2C%20offering%20linear%20complexity%20with%20respect%20to%20sequence%20length.%0AMeanwhile%2C%20efficient%20VQA%20heavily%20depends%20on%20resampling%20long%20sequences%20to%0Aminimize%20computational%20costs%2C%20yet%20current%20resampling%20methods%20are%20often%20weak%20in%0Apreserving%20essential%20semantic%20information.%20In%20this%20work%2C%20we%20present%20MVQA%2C%20a%0AMamba-based%20model%20designed%20for%20efficient%20VQA%20along%20with%20a%20novel%20Unified%0ASemantic%20and%20Distortion%20Sampling%20%28USDS%29%20approach.%20USDS%20combines%20semantic%20patch%0Asampling%20from%20low-resolution%20videos%20and%20distortion%20patch%20sampling%20from%0Aoriginal-resolution%20videos.%20The%20former%20captures%20semantically%20dense%20regions%2C%0Awhile%20the%20latter%20retains%20critical%20distortion%20details.%20To%20prevent%20computation%0Aincrease%20from%20dual%20inputs%2C%20we%20propose%20a%20fusion%20mechanism%20using%20pre-defined%0Amasks%2C%20enabling%20a%20unified%20sampling%20strategy%20that%20captures%20both%20semantic%20and%0Aquality%20information%20without%20additional%20computational%20burden.%20Experiments%20show%0Athat%20the%20proposed%20MVQA%2C%20equipped%20with%20USDS%2C%20achieve%20comparable%20performance%20to%0Astate-of-the-art%20methods%20while%20being%20%242%5Ctimes%24%20as%20fast%20and%20requiring%20only%20%241/5%24%0AGPU%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVQA%253A%2520Mamba%2520with%2520Unified%2520Sampling%2520for%2520Efficient%2520Video%2520Quality%2520Assessment%26entry.906535625%3DYachun%2520Mi%2520and%2520Yu%2520Li%2520and%2520Weicheng%2520Meng%2520and%2520Chaofeng%2520Chen%2520and%2520Chen%2520Hui%2520and%2520Shaohui%2520Liu%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520long-duration%252C%2520high-definition%2520videos%2520has%2520made%2520efficient%250Avideo%2520quality%2520assessment%2520%2528VQA%2529%2520a%2520critical%2520challenge.%2520Existing%2520research%250Atypically%2520tackles%2520this%2520problem%2520through%2520two%2520main%2520strategies%253A%2520reducing%2520model%250Aparameters%2520and%2520resampling%2520inputs.%2520However%252C%2520light-weight%2520Convolution%2520Neural%250ANetworks%2520%2528CNN%2529%2520and%2520Transformers%2520often%2520struggle%2520to%2520balance%2520efficiency%2520with%2520high%250Aperformance%2520due%2520to%2520the%2520requirement%2520of%2520long-range%2520modeling%2520capabilities.%250ARecently%252C%2520the%2520state-space%2520model%252C%2520particularly%2520Mamba%252C%2520has%2520emerged%2520as%2520a%2520promising%250Aalternative%252C%2520offering%2520linear%2520complexity%2520with%2520respect%2520to%2520sequence%2520length.%250AMeanwhile%252C%2520efficient%2520VQA%2520heavily%2520depends%2520on%2520resampling%2520long%2520sequences%2520to%250Aminimize%2520computational%2520costs%252C%2520yet%2520current%2520resampling%2520methods%2520are%2520often%2520weak%2520in%250Apreserving%2520essential%2520semantic%2520information.%2520In%2520this%2520work%252C%2520we%2520present%2520MVQA%252C%2520a%250AMamba-based%2520model%2520designed%2520for%2520efficient%2520VQA%2520along%2520with%2520a%2520novel%2520Unified%250ASemantic%2520and%2520Distortion%2520Sampling%2520%2528USDS%2529%2520approach.%2520USDS%2520combines%2520semantic%2520patch%250Asampling%2520from%2520low-resolution%2520videos%2520and%2520distortion%2520patch%2520sampling%2520from%250Aoriginal-resolution%2520videos.%2520The%2520former%2520captures%2520semantically%2520dense%2520regions%252C%250Awhile%2520the%2520latter%2520retains%2520critical%2520distortion%2520details.%2520To%2520prevent%2520computation%250Aincrease%2520from%2520dual%2520inputs%252C%2520we%2520propose%2520a%2520fusion%2520mechanism%2520using%2520pre-defined%250Amasks%252C%2520enabling%2520a%2520unified%2520sampling%2520strategy%2520that%2520captures%2520both%2520semantic%2520and%250Aquality%2520information%2520without%2520additional%2520computational%2520burden.%2520Experiments%2520show%250Athat%2520the%2520proposed%2520MVQA%252C%2520equipped%2520with%2520USDS%252C%2520achieve%2520comparable%2520performance%2520to%250Astate-of-the-art%2520methods%2520while%2520being%2520%25242%255Ctimes%2524%2520as%2520fast%2520and%2520requiring%2520only%2520%25241/5%2524%250AGPU%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVQA%3A%20Mamba%20with%20Unified%20Sampling%20for%20Efficient%20Video%20Quality%20Assessment&entry.906535625=Yachun%20Mi%20and%20Yu%20Li%20and%20Weicheng%20Meng%20and%20Chaofeng%20Chen%20and%20Chen%20Hui%20and%20Shaohui%20Liu&entry.1292438233=%20%20The%20rapid%20growth%20of%20long-duration%2C%20high-definition%20videos%20has%20made%20efficient%0Avideo%20quality%20assessment%20%28VQA%29%20a%20critical%20challenge.%20Existing%20research%0Atypically%20tackles%20this%20problem%20through%20two%20main%20strategies%3A%20reducing%20model%0Aparameters%20and%20resampling%20inputs.%20However%2C%20light-weight%20Convolution%20Neural%0ANetworks%20%28CNN%29%20and%20Transformers%20often%20struggle%20to%20balance%20efficiency%20with%20high%0Aperformance%20due%20to%20the%20requirement%20of%20long-range%20modeling%20capabilities.%0ARecently%2C%20the%20state-space%20model%2C%20particularly%20Mamba%2C%20has%20emerged%20as%20a%20promising%0Aalternative%2C%20offering%20linear%20complexity%20with%20respect%20to%20sequence%20length.%0AMeanwhile%2C%20efficient%20VQA%20heavily%20depends%20on%20resampling%20long%20sequences%20to%0Aminimize%20computational%20costs%2C%20yet%20current%20resampling%20methods%20are%20often%20weak%20in%0Apreserving%20essential%20semantic%20information.%20In%20this%20work%2C%20we%20present%20MVQA%2C%20a%0AMamba-based%20model%20designed%20for%20efficient%20VQA%20along%20with%20a%20novel%20Unified%0ASemantic%20and%20Distortion%20Sampling%20%28USDS%29%20approach.%20USDS%20combines%20semantic%20patch%0Asampling%20from%20low-resolution%20videos%20and%20distortion%20patch%20sampling%20from%0Aoriginal-resolution%20videos.%20The%20former%20captures%20semantically%20dense%20regions%2C%0Awhile%20the%20latter%20retains%20critical%20distortion%20details.%20To%20prevent%20computation%0Aincrease%20from%20dual%20inputs%2C%20we%20propose%20a%20fusion%20mechanism%20using%20pre-defined%0Amasks%2C%20enabling%20a%20unified%20sampling%20strategy%20that%20captures%20both%20semantic%20and%0Aquality%20information%20without%20additional%20computational%20burden.%20Experiments%20show%0Athat%20the%20proposed%20MVQA%2C%20equipped%20with%20USDS%2C%20achieve%20comparable%20performance%20to%0Astate-of-the-art%20methods%20while%20being%20%242%5Ctimes%24%20as%20fast%20and%20requiring%20only%20%241/5%24%0AGPU%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16003v1&entry.124074799=Read"},
{"title": "Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution\n  Generalization", "author": "Lin Zhu and Xinbing Wang and Chenghu Zhou and Nanyang Ye", "abstract": "  Recent advances in large pre-trained models showed promising results in\nfew-shot learning. However, their generalization ability on two-dimensional\nOut-of-Distribution (OoD) data, i.e., correlation shift and diversity shift,\nhas not been thoroughly investigated. Researches have shown that even with a\nsignificant amount of training data, few methods can achieve better performance\nthan the standard empirical risk minimization method (ERM) in OoD\ngeneralization. This few-shot OoD generalization dilemma emerges as a\nchallenging direction in deep neural network generalization research, where the\nperformance suffers from overfitting on few-shot examples and OoD\ngeneralization errors. In this paper, leveraging a broader supervision source,\nwe explore a novel Bayesian cross-modal image-text alignment learning method\n(Bayes-CAL) to address this issue. Specifically, the model is designed as only\ntext representations are fine-tuned via a Bayesian modelling approach with\ngradient orthogonalization loss and invariant risk minimization (IRM) loss. The\nBayesian approach is essentially introduced to avoid overfitting the base\nclasses observed during training and improve generalization to broader unseen\nclasses. The dedicated loss is introduced to achieve better image-text\nalignment by disentangling the causal and non-casual parts of image features.\nNumerical experiments demonstrate that Bayes-CAL achieved state-of-the-art OoD\ngeneralization performances on two-dimensional distribution shifts. Moreover,\ncompared with CLIP-like models, Bayes-CAL yields more stable generalization\nperformances on unseen classes. Our code is available at\nhttps://github.com/LinLLLL/BayesCAL.\n", "link": "http://arxiv.org/abs/2504.09448v2", "date": "2025-04-22", "relevancy": 2.7782, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5551}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Cross-Modal%20Alignment%20Learning%20for%20Few-Shot%20Out-of-Distribution%0A%20%20Generalization&body=Title%3A%20Bayesian%20Cross-Modal%20Alignment%20Learning%20for%20Few-Shot%20Out-of-Distribution%0A%20%20Generalization%0AAuthor%3A%20Lin%20Zhu%20and%20Xinbing%20Wang%20and%20Chenghu%20Zhou%20and%20Nanyang%20Ye%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20pre-trained%20models%20showed%20promising%20results%20in%0Afew-shot%20learning.%20However%2C%20their%20generalization%20ability%20on%20two-dimensional%0AOut-of-Distribution%20%28OoD%29%20data%2C%20i.e.%2C%20correlation%20shift%20and%20diversity%20shift%2C%0Ahas%20not%20been%20thoroughly%20investigated.%20Researches%20have%20shown%20that%20even%20with%20a%0Asignificant%20amount%20of%20training%20data%2C%20few%20methods%20can%20achieve%20better%20performance%0Athan%20the%20standard%20empirical%20risk%20minimization%20method%20%28ERM%29%20in%20OoD%0Ageneralization.%20This%20few-shot%20OoD%20generalization%20dilemma%20emerges%20as%20a%0Achallenging%20direction%20in%20deep%20neural%20network%20generalization%20research%2C%20where%20the%0Aperformance%20suffers%20from%20overfitting%20on%20few-shot%20examples%20and%20OoD%0Ageneralization%20errors.%20In%20this%20paper%2C%20leveraging%20a%20broader%20supervision%20source%2C%0Awe%20explore%20a%20novel%20Bayesian%20cross-modal%20image-text%20alignment%20learning%20method%0A%28Bayes-CAL%29%20to%20address%20this%20issue.%20Specifically%2C%20the%20model%20is%20designed%20as%20only%0Atext%20representations%20are%20fine-tuned%20via%20a%20Bayesian%20modelling%20approach%20with%0Agradient%20orthogonalization%20loss%20and%20invariant%20risk%20minimization%20%28IRM%29%20loss.%20The%0ABayesian%20approach%20is%20essentially%20introduced%20to%20avoid%20overfitting%20the%20base%0Aclasses%20observed%20during%20training%20and%20improve%20generalization%20to%20broader%20unseen%0Aclasses.%20The%20dedicated%20loss%20is%20introduced%20to%20achieve%20better%20image-text%0Aalignment%20by%20disentangling%20the%20causal%20and%20non-casual%20parts%20of%20image%20features.%0ANumerical%20experiments%20demonstrate%20that%20Bayes-CAL%20achieved%20state-of-the-art%20OoD%0Ageneralization%20performances%20on%20two-dimensional%20distribution%20shifts.%20Moreover%2C%0Acompared%20with%20CLIP-like%20models%2C%20Bayes-CAL%20yields%20more%20stable%20generalization%0Aperformances%20on%20unseen%20classes.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LinLLLL/BayesCAL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09448v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Cross-Modal%2520Alignment%2520Learning%2520for%2520Few-Shot%2520Out-of-Distribution%250A%2520%2520Generalization%26entry.906535625%3DLin%2520Zhu%2520and%2520Xinbing%2520Wang%2520and%2520Chenghu%2520Zhou%2520and%2520Nanyang%2520Ye%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520pre-trained%2520models%2520showed%2520promising%2520results%2520in%250Afew-shot%2520learning.%2520However%252C%2520their%2520generalization%2520ability%2520on%2520two-dimensional%250AOut-of-Distribution%2520%2528OoD%2529%2520data%252C%2520i.e.%252C%2520correlation%2520shift%2520and%2520diversity%2520shift%252C%250Ahas%2520not%2520been%2520thoroughly%2520investigated.%2520Researches%2520have%2520shown%2520that%2520even%2520with%2520a%250Asignificant%2520amount%2520of%2520training%2520data%252C%2520few%2520methods%2520can%2520achieve%2520better%2520performance%250Athan%2520the%2520standard%2520empirical%2520risk%2520minimization%2520method%2520%2528ERM%2529%2520in%2520OoD%250Ageneralization.%2520This%2520few-shot%2520OoD%2520generalization%2520dilemma%2520emerges%2520as%2520a%250Achallenging%2520direction%2520in%2520deep%2520neural%2520network%2520generalization%2520research%252C%2520where%2520the%250Aperformance%2520suffers%2520from%2520overfitting%2520on%2520few-shot%2520examples%2520and%2520OoD%250Ageneralization%2520errors.%2520In%2520this%2520paper%252C%2520leveraging%2520a%2520broader%2520supervision%2520source%252C%250Awe%2520explore%2520a%2520novel%2520Bayesian%2520cross-modal%2520image-text%2520alignment%2520learning%2520method%250A%2528Bayes-CAL%2529%2520to%2520address%2520this%2520issue.%2520Specifically%252C%2520the%2520model%2520is%2520designed%2520as%2520only%250Atext%2520representations%2520are%2520fine-tuned%2520via%2520a%2520Bayesian%2520modelling%2520approach%2520with%250Agradient%2520orthogonalization%2520loss%2520and%2520invariant%2520risk%2520minimization%2520%2528IRM%2529%2520loss.%2520The%250ABayesian%2520approach%2520is%2520essentially%2520introduced%2520to%2520avoid%2520overfitting%2520the%2520base%250Aclasses%2520observed%2520during%2520training%2520and%2520improve%2520generalization%2520to%2520broader%2520unseen%250Aclasses.%2520The%2520dedicated%2520loss%2520is%2520introduced%2520to%2520achieve%2520better%2520image-text%250Aalignment%2520by%2520disentangling%2520the%2520causal%2520and%2520non-casual%2520parts%2520of%2520image%2520features.%250ANumerical%2520experiments%2520demonstrate%2520that%2520Bayes-CAL%2520achieved%2520state-of-the-art%2520OoD%250Ageneralization%2520performances%2520on%2520two-dimensional%2520distribution%2520shifts.%2520Moreover%252C%250Acompared%2520with%2520CLIP-like%2520models%252C%2520Bayes-CAL%2520yields%2520more%2520stable%2520generalization%250Aperformances%2520on%2520unseen%2520classes.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LinLLLL/BayesCAL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09448v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Cross-Modal%20Alignment%20Learning%20for%20Few-Shot%20Out-of-Distribution%0A%20%20Generalization&entry.906535625=Lin%20Zhu%20and%20Xinbing%20Wang%20and%20Chenghu%20Zhou%20and%20Nanyang%20Ye&entry.1292438233=%20%20Recent%20advances%20in%20large%20pre-trained%20models%20showed%20promising%20results%20in%0Afew-shot%20learning.%20However%2C%20their%20generalization%20ability%20on%20two-dimensional%0AOut-of-Distribution%20%28OoD%29%20data%2C%20i.e.%2C%20correlation%20shift%20and%20diversity%20shift%2C%0Ahas%20not%20been%20thoroughly%20investigated.%20Researches%20have%20shown%20that%20even%20with%20a%0Asignificant%20amount%20of%20training%20data%2C%20few%20methods%20can%20achieve%20better%20performance%0Athan%20the%20standard%20empirical%20risk%20minimization%20method%20%28ERM%29%20in%20OoD%0Ageneralization.%20This%20few-shot%20OoD%20generalization%20dilemma%20emerges%20as%20a%0Achallenging%20direction%20in%20deep%20neural%20network%20generalization%20research%2C%20where%20the%0Aperformance%20suffers%20from%20overfitting%20on%20few-shot%20examples%20and%20OoD%0Ageneralization%20errors.%20In%20this%20paper%2C%20leveraging%20a%20broader%20supervision%20source%2C%0Awe%20explore%20a%20novel%20Bayesian%20cross-modal%20image-text%20alignment%20learning%20method%0A%28Bayes-CAL%29%20to%20address%20this%20issue.%20Specifically%2C%20the%20model%20is%20designed%20as%20only%0Atext%20representations%20are%20fine-tuned%20via%20a%20Bayesian%20modelling%20approach%20with%0Agradient%20orthogonalization%20loss%20and%20invariant%20risk%20minimization%20%28IRM%29%20loss.%20The%0ABayesian%20approach%20is%20essentially%20introduced%20to%20avoid%20overfitting%20the%20base%0Aclasses%20observed%20during%20training%20and%20improve%20generalization%20to%20broader%20unseen%0Aclasses.%20The%20dedicated%20loss%20is%20introduced%20to%20achieve%20better%20image-text%0Aalignment%20by%20disentangling%20the%20causal%20and%20non-casual%20parts%20of%20image%20features.%0ANumerical%20experiments%20demonstrate%20that%20Bayes-CAL%20achieved%20state-of-the-art%20OoD%0Ageneralization%20performances%20on%20two-dimensional%20distribution%20shifts.%20Moreover%2C%0Acompared%20with%20CLIP-like%20models%2C%20Bayes-CAL%20yields%20more%20stable%20generalization%0Aperformances%20on%20unseen%20classes.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LinLLLL/BayesCAL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09448v2&entry.124074799=Read"},
{"title": "Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong\n  Person Re-identification", "author": "Shiben Liu and Huijie Fan and Qiang Wang and Baojie Fan and Yandong Tang and Liangqiong Qu", "abstract": "  Lifelong Person Re-identification (LReID) suffers from a key challenge in\npreserving old knowledge while adapting to new information. The existing\nsolutions include rehearsal-based and rehearsal-free methods to address this\nchallenge. Rehearsal-based approaches rely on knowledge distillation,\ncontinuously accumulating forgetting during the distillation process.\nRehearsal-free methods insufficiently learn the distribution of each domain,\nleading to forgetfulness over time. To solve these issues, we propose a novel\nDistribution-aware Forgetting Compensation (DAFC) model that explores\ncross-domain shared representation learning and domain-specific distribution\nintegration without using old exemplars or knowledge distillation. We propose a\nText-driven Prompt Aggregation (TPA) that utilizes text features to enrich\nprompt elements and guide the prompt model to learn fine-grained\nrepresentations for each instance. This can enhance the differentiation of\nidentity information and establish the foundation for domain distribution\nawareness. Then, Distribution-based Awareness and Integration (DAI) is designed\nto capture each domain-specific distribution by a dedicated expert network and\nadaptively consolidate them into a shared region in high-dimensional space. In\nthis manner, DAI can consolidate and enhance cross-domain shared representation\nlearning while alleviating catastrophic forgetting. Furthermore, we develop a\nKnowledge Consolidation Mechanism (KCM) that comprises instance-level\ndiscrimination and cross-domain consistency alignment strategies to facilitate\nmodel adaptive learning of new knowledge from the current domain and promote\nknowledge consolidation learning between acquired domain-specific\ndistributions, respectively. Experimental results show that our DAFC\noutperforms state-of-the-art methods. Our code is available at\nhttps://github.com/LiuShiBen/DAFC.\n", "link": "http://arxiv.org/abs/2504.15041v2", "date": "2025-04-22", "relevancy": 2.7112, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5451}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.543}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution-aware%20Forgetting%20Compensation%20for%20Exemplar-Free%20Lifelong%0A%20%20Person%20Re-identification&body=Title%3A%20Distribution-aware%20Forgetting%20Compensation%20for%20Exemplar-Free%20Lifelong%0A%20%20Person%20Re-identification%0AAuthor%3A%20Shiben%20Liu%20and%20Huijie%20Fan%20and%20Qiang%20Wang%20and%20Baojie%20Fan%20and%20Yandong%20Tang%20and%20Liangqiong%20Qu%0AAbstract%3A%20%20%20Lifelong%20Person%20Re-identification%20%28LReID%29%20suffers%20from%20a%20key%20challenge%20in%0Apreserving%20old%20knowledge%20while%20adapting%20to%20new%20information.%20The%20existing%0Asolutions%20include%20rehearsal-based%20and%20rehearsal-free%20methods%20to%20address%20this%0Achallenge.%20Rehearsal-based%20approaches%20rely%20on%20knowledge%20distillation%2C%0Acontinuously%20accumulating%20forgetting%20during%20the%20distillation%20process.%0ARehearsal-free%20methods%20insufficiently%20learn%20the%20distribution%20of%20each%20domain%2C%0Aleading%20to%20forgetfulness%20over%20time.%20To%20solve%20these%20issues%2C%20we%20propose%20a%20novel%0ADistribution-aware%20Forgetting%20Compensation%20%28DAFC%29%20model%20that%20explores%0Across-domain%20shared%20representation%20learning%20and%20domain-specific%20distribution%0Aintegration%20without%20using%20old%20exemplars%20or%20knowledge%20distillation.%20We%20propose%20a%0AText-driven%20Prompt%20Aggregation%20%28TPA%29%20that%20utilizes%20text%20features%20to%20enrich%0Aprompt%20elements%20and%20guide%20the%20prompt%20model%20to%20learn%20fine-grained%0Arepresentations%20for%20each%20instance.%20This%20can%20enhance%20the%20differentiation%20of%0Aidentity%20information%20and%20establish%20the%20foundation%20for%20domain%20distribution%0Aawareness.%20Then%2C%20Distribution-based%20Awareness%20and%20Integration%20%28DAI%29%20is%20designed%0Ato%20capture%20each%20domain-specific%20distribution%20by%20a%20dedicated%20expert%20network%20and%0Aadaptively%20consolidate%20them%20into%20a%20shared%20region%20in%20high-dimensional%20space.%20In%0Athis%20manner%2C%20DAI%20can%20consolidate%20and%20enhance%20cross-domain%20shared%20representation%0Alearning%20while%20alleviating%20catastrophic%20forgetting.%20Furthermore%2C%20we%20develop%20a%0AKnowledge%20Consolidation%20Mechanism%20%28KCM%29%20that%20comprises%20instance-level%0Adiscrimination%20and%20cross-domain%20consistency%20alignment%20strategies%20to%20facilitate%0Amodel%20adaptive%20learning%20of%20new%20knowledge%20from%20the%20current%20domain%20and%20promote%0Aknowledge%20consolidation%20learning%20between%20acquired%20domain-specific%0Adistributions%2C%20respectively.%20Experimental%20results%20show%20that%20our%20DAFC%0Aoutperforms%20state-of-the-art%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LiuShiBen/DAFC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15041v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution-aware%2520Forgetting%2520Compensation%2520for%2520Exemplar-Free%2520Lifelong%250A%2520%2520Person%2520Re-identification%26entry.906535625%3DShiben%2520Liu%2520and%2520Huijie%2520Fan%2520and%2520Qiang%2520Wang%2520and%2520Baojie%2520Fan%2520and%2520Yandong%2520Tang%2520and%2520Liangqiong%2520Qu%26entry.1292438233%3D%2520%2520Lifelong%2520Person%2520Re-identification%2520%2528LReID%2529%2520suffers%2520from%2520a%2520key%2520challenge%2520in%250Apreserving%2520old%2520knowledge%2520while%2520adapting%2520to%2520new%2520information.%2520The%2520existing%250Asolutions%2520include%2520rehearsal-based%2520and%2520rehearsal-free%2520methods%2520to%2520address%2520this%250Achallenge.%2520Rehearsal-based%2520approaches%2520rely%2520on%2520knowledge%2520distillation%252C%250Acontinuously%2520accumulating%2520forgetting%2520during%2520the%2520distillation%2520process.%250ARehearsal-free%2520methods%2520insufficiently%2520learn%2520the%2520distribution%2520of%2520each%2520domain%252C%250Aleading%2520to%2520forgetfulness%2520over%2520time.%2520To%2520solve%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%250ADistribution-aware%2520Forgetting%2520Compensation%2520%2528DAFC%2529%2520model%2520that%2520explores%250Across-domain%2520shared%2520representation%2520learning%2520and%2520domain-specific%2520distribution%250Aintegration%2520without%2520using%2520old%2520exemplars%2520or%2520knowledge%2520distillation.%2520We%2520propose%2520a%250AText-driven%2520Prompt%2520Aggregation%2520%2528TPA%2529%2520that%2520utilizes%2520text%2520features%2520to%2520enrich%250Aprompt%2520elements%2520and%2520guide%2520the%2520prompt%2520model%2520to%2520learn%2520fine-grained%250Arepresentations%2520for%2520each%2520instance.%2520This%2520can%2520enhance%2520the%2520differentiation%2520of%250Aidentity%2520information%2520and%2520establish%2520the%2520foundation%2520for%2520domain%2520distribution%250Aawareness.%2520Then%252C%2520Distribution-based%2520Awareness%2520and%2520Integration%2520%2528DAI%2529%2520is%2520designed%250Ato%2520capture%2520each%2520domain-specific%2520distribution%2520by%2520a%2520dedicated%2520expert%2520network%2520and%250Aadaptively%2520consolidate%2520them%2520into%2520a%2520shared%2520region%2520in%2520high-dimensional%2520space.%2520In%250Athis%2520manner%252C%2520DAI%2520can%2520consolidate%2520and%2520enhance%2520cross-domain%2520shared%2520representation%250Alearning%2520while%2520alleviating%2520catastrophic%2520forgetting.%2520Furthermore%252C%2520we%2520develop%2520a%250AKnowledge%2520Consolidation%2520Mechanism%2520%2528KCM%2529%2520that%2520comprises%2520instance-level%250Adiscrimination%2520and%2520cross-domain%2520consistency%2520alignment%2520strategies%2520to%2520facilitate%250Amodel%2520adaptive%2520learning%2520of%2520new%2520knowledge%2520from%2520the%2520current%2520domain%2520and%2520promote%250Aknowledge%2520consolidation%2520learning%2520between%2520acquired%2520domain-specific%250Adistributions%252C%2520respectively.%2520Experimental%2520results%2520show%2520that%2520our%2520DAFC%250Aoutperforms%2520state-of-the-art%2520methods.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LiuShiBen/DAFC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15041v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution-aware%20Forgetting%20Compensation%20for%20Exemplar-Free%20Lifelong%0A%20%20Person%20Re-identification&entry.906535625=Shiben%20Liu%20and%20Huijie%20Fan%20and%20Qiang%20Wang%20and%20Baojie%20Fan%20and%20Yandong%20Tang%20and%20Liangqiong%20Qu&entry.1292438233=%20%20Lifelong%20Person%20Re-identification%20%28LReID%29%20suffers%20from%20a%20key%20challenge%20in%0Apreserving%20old%20knowledge%20while%20adapting%20to%20new%20information.%20The%20existing%0Asolutions%20include%20rehearsal-based%20and%20rehearsal-free%20methods%20to%20address%20this%0Achallenge.%20Rehearsal-based%20approaches%20rely%20on%20knowledge%20distillation%2C%0Acontinuously%20accumulating%20forgetting%20during%20the%20distillation%20process.%0ARehearsal-free%20methods%20insufficiently%20learn%20the%20distribution%20of%20each%20domain%2C%0Aleading%20to%20forgetfulness%20over%20time.%20To%20solve%20these%20issues%2C%20we%20propose%20a%20novel%0ADistribution-aware%20Forgetting%20Compensation%20%28DAFC%29%20model%20that%20explores%0Across-domain%20shared%20representation%20learning%20and%20domain-specific%20distribution%0Aintegration%20without%20using%20old%20exemplars%20or%20knowledge%20distillation.%20We%20propose%20a%0AText-driven%20Prompt%20Aggregation%20%28TPA%29%20that%20utilizes%20text%20features%20to%20enrich%0Aprompt%20elements%20and%20guide%20the%20prompt%20model%20to%20learn%20fine-grained%0Arepresentations%20for%20each%20instance.%20This%20can%20enhance%20the%20differentiation%20of%0Aidentity%20information%20and%20establish%20the%20foundation%20for%20domain%20distribution%0Aawareness.%20Then%2C%20Distribution-based%20Awareness%20and%20Integration%20%28DAI%29%20is%20designed%0Ato%20capture%20each%20domain-specific%20distribution%20by%20a%20dedicated%20expert%20network%20and%0Aadaptively%20consolidate%20them%20into%20a%20shared%20region%20in%20high-dimensional%20space.%20In%0Athis%20manner%2C%20DAI%20can%20consolidate%20and%20enhance%20cross-domain%20shared%20representation%0Alearning%20while%20alleviating%20catastrophic%20forgetting.%20Furthermore%2C%20we%20develop%20a%0AKnowledge%20Consolidation%20Mechanism%20%28KCM%29%20that%20comprises%20instance-level%0Adiscrimination%20and%20cross-domain%20consistency%20alignment%20strategies%20to%20facilitate%0Amodel%20adaptive%20learning%20of%20new%20knowledge%20from%20the%20current%20domain%20and%20promote%0Aknowledge%20consolidation%20learning%20between%20acquired%20domain-specific%0Adistributions%2C%20respectively.%20Experimental%20results%20show%20that%20our%20DAFC%0Aoutperforms%20state-of-the-art%20methods.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LiuShiBen/DAFC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15041v2&entry.124074799=Read"},
{"title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud\n  Learning", "author": "Song Wang and Xiaolu Liu and Lingdong Kong and Jianyun Xu and Chunyong Hu and Gongfan Fang and Wentong Li and Jianke Zhu and Xinchao Wang", "abstract": "  Self-supervised representation learning for point cloud has demonstrated\neffectiveness in improving pre-trained model performance across diverse tasks.\nHowever, as pre-trained models grow in complexity, fully fine-tuning them for\ndownstream applications demands substantial computational and storage\nresources. Parameter-efficient fine-tuning (PEFT) methods offer a promising\nsolution to mitigate these resource requirements, yet most current approaches\nrely on complex adapter and prompt mechanisms that increase tunable parameters.\nIn this paper, we propose PointLoRA, a simple yet effective method that\ncombines low-rank adaptation (LoRA) with multi-scale token selection to\nefficiently fine-tune point cloud models. Our approach embeds LoRA layers\nwithin the most parameter-intensive components of point cloud transformers,\nreducing the need for tunable parameters while enhancing global feature\ncapture. Additionally, multi-scale token selection extracts critical local\ninformation to serve as prompts for downstream fine-tuning, effectively\ncomplementing the global context captured by LoRA. The experimental results\nacross various pre-trained models and three challenging public datasets\ndemonstrate that our approach achieves competitive performance with only 3.43%\nof the trainable parameters, making it highly effective for\nresource-constrained applications. Source code is available at:\nhttps://github.com/songw-zju/PointLoRA.\n", "link": "http://arxiv.org/abs/2504.16023v1", "date": "2025-04-22", "relevancy": 2.6982, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5517}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5436}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointLoRA%3A%20Low-Rank%20Adaptation%20with%20Token%20Selection%20for%20Point%20Cloud%0A%20%20Learning&body=Title%3A%20PointLoRA%3A%20Low-Rank%20Adaptation%20with%20Token%20Selection%20for%20Point%20Cloud%0A%20%20Learning%0AAuthor%3A%20Song%20Wang%20and%20Xiaolu%20Liu%20and%20Lingdong%20Kong%20and%20Jianyun%20Xu%20and%20Chunyong%20Hu%20and%20Gongfan%20Fang%20and%20Wentong%20Li%20and%20Jianke%20Zhu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Self-supervised%20representation%20learning%20for%20point%20cloud%20has%20demonstrated%0Aeffectiveness%20in%20improving%20pre-trained%20model%20performance%20across%20diverse%20tasks.%0AHowever%2C%20as%20pre-trained%20models%20grow%20in%20complexity%2C%20fully%20fine-tuning%20them%20for%0Adownstream%20applications%20demands%20substantial%20computational%20and%20storage%0Aresources.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20offer%20a%20promising%0Asolution%20to%20mitigate%20these%20resource%20requirements%2C%20yet%20most%20current%20approaches%0Arely%20on%20complex%20adapter%20and%20prompt%20mechanisms%20that%20increase%20tunable%20parameters.%0AIn%20this%20paper%2C%20we%20propose%20PointLoRA%2C%20a%20simple%20yet%20effective%20method%20that%0Acombines%20low-rank%20adaptation%20%28LoRA%29%20with%20multi-scale%20token%20selection%20to%0Aefficiently%20fine-tune%20point%20cloud%20models.%20Our%20approach%20embeds%20LoRA%20layers%0Awithin%20the%20most%20parameter-intensive%20components%20of%20point%20cloud%20transformers%2C%0Areducing%20the%20need%20for%20tunable%20parameters%20while%20enhancing%20global%20feature%0Acapture.%20Additionally%2C%20multi-scale%20token%20selection%20extracts%20critical%20local%0Ainformation%20to%20serve%20as%20prompts%20for%20downstream%20fine-tuning%2C%20effectively%0Acomplementing%20the%20global%20context%20captured%20by%20LoRA.%20The%20experimental%20results%0Aacross%20various%20pre-trained%20models%20and%20three%20challenging%20public%20datasets%0Ademonstrate%20that%20our%20approach%20achieves%20competitive%20performance%20with%20only%203.43%25%0Aof%20the%20trainable%20parameters%2C%20making%20it%20highly%20effective%20for%0Aresource-constrained%20applications.%20Source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/songw-zju/PointLoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointLoRA%253A%2520Low-Rank%2520Adaptation%2520with%2520Token%2520Selection%2520for%2520Point%2520Cloud%250A%2520%2520Learning%26entry.906535625%3DSong%2520Wang%2520and%2520Xiaolu%2520Liu%2520and%2520Lingdong%2520Kong%2520and%2520Jianyun%2520Xu%2520and%2520Chunyong%2520Hu%2520and%2520Gongfan%2520Fang%2520and%2520Wentong%2520Li%2520and%2520Jianke%2520Zhu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Self-supervised%2520representation%2520learning%2520for%2520point%2520cloud%2520has%2520demonstrated%250Aeffectiveness%2520in%2520improving%2520pre-trained%2520model%2520performance%2520across%2520diverse%2520tasks.%250AHowever%252C%2520as%2520pre-trained%2520models%2520grow%2520in%2520complexity%252C%2520fully%2520fine-tuning%2520them%2520for%250Adownstream%2520applications%2520demands%2520substantial%2520computational%2520and%2520storage%250Aresources.%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%2520offer%2520a%2520promising%250Asolution%2520to%2520mitigate%2520these%2520resource%2520requirements%252C%2520yet%2520most%2520current%2520approaches%250Arely%2520on%2520complex%2520adapter%2520and%2520prompt%2520mechanisms%2520that%2520increase%2520tunable%2520parameters.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520PointLoRA%252C%2520a%2520simple%2520yet%2520effective%2520method%2520that%250Acombines%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520with%2520multi-scale%2520token%2520selection%2520to%250Aefficiently%2520fine-tune%2520point%2520cloud%2520models.%2520Our%2520approach%2520embeds%2520LoRA%2520layers%250Awithin%2520the%2520most%2520parameter-intensive%2520components%2520of%2520point%2520cloud%2520transformers%252C%250Areducing%2520the%2520need%2520for%2520tunable%2520parameters%2520while%2520enhancing%2520global%2520feature%250Acapture.%2520Additionally%252C%2520multi-scale%2520token%2520selection%2520extracts%2520critical%2520local%250Ainformation%2520to%2520serve%2520as%2520prompts%2520for%2520downstream%2520fine-tuning%252C%2520effectively%250Acomplementing%2520the%2520global%2520context%2520captured%2520by%2520LoRA.%2520The%2520experimental%2520results%250Aacross%2520various%2520pre-trained%2520models%2520and%2520three%2520challenging%2520public%2520datasets%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520competitive%2520performance%2520with%2520only%25203.43%2525%250Aof%2520the%2520trainable%2520parameters%252C%2520making%2520it%2520highly%2520effective%2520for%250Aresource-constrained%2520applications.%2520Source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/songw-zju/PointLoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointLoRA%3A%20Low-Rank%20Adaptation%20with%20Token%20Selection%20for%20Point%20Cloud%0A%20%20Learning&entry.906535625=Song%20Wang%20and%20Xiaolu%20Liu%20and%20Lingdong%20Kong%20and%20Jianyun%20Xu%20and%20Chunyong%20Hu%20and%20Gongfan%20Fang%20and%20Wentong%20Li%20and%20Jianke%20Zhu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Self-supervised%20representation%20learning%20for%20point%20cloud%20has%20demonstrated%0Aeffectiveness%20in%20improving%20pre-trained%20model%20performance%20across%20diverse%20tasks.%0AHowever%2C%20as%20pre-trained%20models%20grow%20in%20complexity%2C%20fully%20fine-tuning%20them%20for%0Adownstream%20applications%20demands%20substantial%20computational%20and%20storage%0Aresources.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20offer%20a%20promising%0Asolution%20to%20mitigate%20these%20resource%20requirements%2C%20yet%20most%20current%20approaches%0Arely%20on%20complex%20adapter%20and%20prompt%20mechanisms%20that%20increase%20tunable%20parameters.%0AIn%20this%20paper%2C%20we%20propose%20PointLoRA%2C%20a%20simple%20yet%20effective%20method%20that%0Acombines%20low-rank%20adaptation%20%28LoRA%29%20with%20multi-scale%20token%20selection%20to%0Aefficiently%20fine-tune%20point%20cloud%20models.%20Our%20approach%20embeds%20LoRA%20layers%0Awithin%20the%20most%20parameter-intensive%20components%20of%20point%20cloud%20transformers%2C%0Areducing%20the%20need%20for%20tunable%20parameters%20while%20enhancing%20global%20feature%0Acapture.%20Additionally%2C%20multi-scale%20token%20selection%20extracts%20critical%20local%0Ainformation%20to%20serve%20as%20prompts%20for%20downstream%20fine-tuning%2C%20effectively%0Acomplementing%20the%20global%20context%20captured%20by%20LoRA.%20The%20experimental%20results%0Aacross%20various%20pre-trained%20models%20and%20three%20challenging%20public%20datasets%0Ademonstrate%20that%20our%20approach%20achieves%20competitive%20performance%20with%20only%203.43%25%0Aof%20the%20trainable%20parameters%2C%20making%20it%20highly%20effective%20for%0Aresource-constrained%20applications.%20Source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/songw-zju/PointLoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16023v1&entry.124074799=Read"},
{"title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws", "author": "Zhixuan Pan and Shaowen Wang and Jian Li", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.\n", "link": "http://arxiv.org/abs/2504.09597v4", "date": "2025-04-22", "relevancy": 2.6836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws&body=Title%3A%20Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws%0AAuthor%3A%20Zhixuan%20Pan%20and%20Shaowen%20Wang%20and%20Jian%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Anumerous%20tasks%2C%20yet%20principled%20explanations%20for%20their%20underlying%20mechanisms%20and%0Aseveral%20phenomena%2C%20such%20as%20scaling%20laws%2C%20hallucinations%2C%20and%20related%20behaviors%2C%0Aremain%20elusive.%20In%20this%20work%2C%20we%20revisit%20the%20classical%20relationship%20between%0Acompression%20and%20prediction%2C%20grounded%20in%20Kolmogorov%20complexity%20and%20Shannon%0Ainformation%20theory%2C%20to%20provide%20deeper%20insights%20into%20LLM%20behaviors.%20By%0Aleveraging%20the%20Kolmogorov%20Structure%20Function%20and%20interpreting%20LLM%20compression%0Aas%20a%20two-part%20coding%20process%2C%20we%20offer%20a%20detailed%20view%20of%20how%20LLMs%20acquire%20and%0Astore%20information%20across%20increasing%20model%20and%20data%20scales%20--%20from%20pervasive%0Asyntactic%20patterns%20to%20progressively%20rarer%20knowledge%20elements.%20Motivated%20by%20this%0Atheoretical%20perspective%20and%20natural%20assumptions%20inspired%20by%20Heap%27s%20and%20Zipf%27s%0Alaws%2C%20we%20introduce%20a%20simplified%20yet%20representative%20hierarchical%20data-generation%0Aframework%20called%20the%20Syntax-Knowledge%20model.%20Under%20the%20Bayesian%20setting%2C%20we%0Ashow%20that%20prediction%20and%20compression%20within%20this%20model%20naturally%20lead%20to%0Adiverse%20learning%20and%20scaling%20behaviors%20of%20LLMs.%20In%20particular%2C%20our%20theoretical%0Aanalysis%20offers%20intuitive%20and%20principled%20explanations%20for%20both%20data%20and%20model%0Ascaling%20laws%2C%20the%20dynamics%20of%20knowledge%20acquisition%20during%20training%20and%0Afine-tuning%2C%20factual%20knowledge%20hallucinations%20in%20LLMs.%20The%20experimental%20results%0Avalidate%20our%20theoretical%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09597v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520LLM%2520Behaviors%2520via%2520Compression%253A%2520Data%2520Generation%252C%2520Knowledge%250A%2520%2520Acquisition%2520and%2520Scaling%2520Laws%26entry.906535625%3DZhixuan%2520Pan%2520and%2520Shaowen%2520Wang%2520and%2520Jian%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Anumerous%2520tasks%252C%2520yet%2520principled%2520explanations%2520for%2520their%2520underlying%2520mechanisms%2520and%250Aseveral%2520phenomena%252C%2520such%2520as%2520scaling%2520laws%252C%2520hallucinations%252C%2520and%2520related%2520behaviors%252C%250Aremain%2520elusive.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520classical%2520relationship%2520between%250Acompression%2520and%2520prediction%252C%2520grounded%2520in%2520Kolmogorov%2520complexity%2520and%2520Shannon%250Ainformation%2520theory%252C%2520to%2520provide%2520deeper%2520insights%2520into%2520LLM%2520behaviors.%2520By%250Aleveraging%2520the%2520Kolmogorov%2520Structure%2520Function%2520and%2520interpreting%2520LLM%2520compression%250Aas%2520a%2520two-part%2520coding%2520process%252C%2520we%2520offer%2520a%2520detailed%2520view%2520of%2520how%2520LLMs%2520acquire%2520and%250Astore%2520information%2520across%2520increasing%2520model%2520and%2520data%2520scales%2520--%2520from%2520pervasive%250Asyntactic%2520patterns%2520to%2520progressively%2520rarer%2520knowledge%2520elements.%2520Motivated%2520by%2520this%250Atheoretical%2520perspective%2520and%2520natural%2520assumptions%2520inspired%2520by%2520Heap%2527s%2520and%2520Zipf%2527s%250Alaws%252C%2520we%2520introduce%2520a%2520simplified%2520yet%2520representative%2520hierarchical%2520data-generation%250Aframework%2520called%2520the%2520Syntax-Knowledge%2520model.%2520Under%2520the%2520Bayesian%2520setting%252C%2520we%250Ashow%2520that%2520prediction%2520and%2520compression%2520within%2520this%2520model%2520naturally%2520lead%2520to%250Adiverse%2520learning%2520and%2520scaling%2520behaviors%2520of%2520LLMs.%2520In%2520particular%252C%2520our%2520theoretical%250Aanalysis%2520offers%2520intuitive%2520and%2520principled%2520explanations%2520for%2520both%2520data%2520and%2520model%250Ascaling%2520laws%252C%2520the%2520dynamics%2520of%2520knowledge%2520acquisition%2520during%2520training%2520and%250Afine-tuning%252C%2520factual%2520knowledge%2520hallucinations%2520in%2520LLMs.%2520The%2520experimental%2520results%250Avalidate%2520our%2520theoretical%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09597v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws&entry.906535625=Zhixuan%20Pan%20and%20Shaowen%20Wang%20and%20Jian%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Anumerous%20tasks%2C%20yet%20principled%20explanations%20for%20their%20underlying%20mechanisms%20and%0Aseveral%20phenomena%2C%20such%20as%20scaling%20laws%2C%20hallucinations%2C%20and%20related%20behaviors%2C%0Aremain%20elusive.%20In%20this%20work%2C%20we%20revisit%20the%20classical%20relationship%20between%0Acompression%20and%20prediction%2C%20grounded%20in%20Kolmogorov%20complexity%20and%20Shannon%0Ainformation%20theory%2C%20to%20provide%20deeper%20insights%20into%20LLM%20behaviors.%20By%0Aleveraging%20the%20Kolmogorov%20Structure%20Function%20and%20interpreting%20LLM%20compression%0Aas%20a%20two-part%20coding%20process%2C%20we%20offer%20a%20detailed%20view%20of%20how%20LLMs%20acquire%20and%0Astore%20information%20across%20increasing%20model%20and%20data%20scales%20--%20from%20pervasive%0Asyntactic%20patterns%20to%20progressively%20rarer%20knowledge%20elements.%20Motivated%20by%20this%0Atheoretical%20perspective%20and%20natural%20assumptions%20inspired%20by%20Heap%27s%20and%20Zipf%27s%0Alaws%2C%20we%20introduce%20a%20simplified%20yet%20representative%20hierarchical%20data-generation%0Aframework%20called%20the%20Syntax-Knowledge%20model.%20Under%20the%20Bayesian%20setting%2C%20we%0Ashow%20that%20prediction%20and%20compression%20within%20this%20model%20naturally%20lead%20to%0Adiverse%20learning%20and%20scaling%20behaviors%20of%20LLMs.%20In%20particular%2C%20our%20theoretical%0Aanalysis%20offers%20intuitive%20and%20principled%20explanations%20for%20both%20data%20and%20model%0Ascaling%20laws%2C%20the%20dynamics%20of%20knowledge%20acquisition%20during%20training%20and%0Afine-tuning%2C%20factual%20knowledge%20hallucinations%20in%20LLMs.%20The%20experimental%20results%0Avalidate%20our%20theoretical%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09597v4&entry.124074799=Read"},
{"title": "LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free\n  Receptive Field Enlargement", "author": "Zhifan Ye and Kejing Xia and Yonggan Fu and Xin Dong and Jihoon Hong and Xiangchi Yuan and Shizhe Diao and Jan Kautz and Pavlo Molchanov and Yingyan Celine Lin", "abstract": "  State space models (SSMs) have emerged as an efficient alternative to\nTransformer models for language modeling, offering linear computational\ncomplexity and constant memory usage as context length increases. However,\ndespite their efficiency in handling long contexts, recent studies have shown\nthat SSMs, such as Mamba models, generally underperform compared to\nTransformers in long-context understanding tasks. To address this significant\nshortfall and achieve both efficient and accurate long-context understanding,\nwe propose LongMamba, a training-free technique that significantly enhances the\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\nthat the hidden channels in Mamba can be categorized into local and global\nchannels based on their receptive field lengths, with global channels primarily\nresponsible for long-context capability. These global channels can become the\nkey bottleneck as the input context lengthens. Specifically, when input lengths\nlargely exceed the training sequence length, global channels exhibit\nlimitations in adaptively extend their receptive fields, leading to Mamba's\npoor long-context performance. The key idea of LongMamba is to mitigate the\nhidden state memory decay in these global channels by preventing the\naccumulation of unimportant tokens in their memory. This is achieved by first\nidentifying critical tokens in the global channels and then applying token\nfiltering to accumulate only those critical tokens. Through extensive\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\nsets a new standard for Mamba's long-context performance, significantly\nextending its operational range without requiring additional training. Our code\nis available at https://github.com/GATECH-EIC/LongMamba.\n", "link": "http://arxiv.org/abs/2504.16053v1", "date": "2025-04-22", "relevancy": 2.6606, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.539}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongMamba%3A%20Enhancing%20Mamba%27s%20Long%20Context%20Capabilities%20via%20Training-Free%0A%20%20Receptive%20Field%20Enlargement&body=Title%3A%20LongMamba%3A%20Enhancing%20Mamba%27s%20Long%20Context%20Capabilities%20via%20Training-Free%0A%20%20Receptive%20Field%20Enlargement%0AAuthor%3A%20Zhifan%20Ye%20and%20Kejing%20Xia%20and%20Yonggan%20Fu%20and%20Xin%20Dong%20and%20Jihoon%20Hong%20and%20Xiangchi%20Yuan%20and%20Shizhe%20Diao%20and%20Jan%20Kautz%20and%20Pavlo%20Molchanov%20and%20Yingyan%20Celine%20Lin%0AAbstract%3A%20%20%20State%20space%20models%20%28SSMs%29%20have%20emerged%20as%20an%20efficient%20alternative%20to%0ATransformer%20models%20for%20language%20modeling%2C%20offering%20linear%20computational%0Acomplexity%20and%20constant%20memory%20usage%20as%20context%20length%20increases.%20However%2C%0Adespite%20their%20efficiency%20in%20handling%20long%20contexts%2C%20recent%20studies%20have%20shown%0Athat%20SSMs%2C%20such%20as%20Mamba%20models%2C%20generally%20underperform%20compared%20to%0ATransformers%20in%20long-context%20understanding%20tasks.%20To%20address%20this%20significant%0Ashortfall%20and%20achieve%20both%20efficient%20and%20accurate%20long-context%20understanding%2C%0Awe%20propose%20LongMamba%2C%20a%20training-free%20technique%20that%20significantly%20enhances%20the%0Along-context%20capabilities%20of%20Mamba%20models.%20LongMamba%20builds%20on%20our%20discovery%0Athat%20the%20hidden%20channels%20in%20Mamba%20can%20be%20categorized%20into%20local%20and%20global%0Achannels%20based%20on%20their%20receptive%20field%20lengths%2C%20with%20global%20channels%20primarily%0Aresponsible%20for%20long-context%20capability.%20These%20global%20channels%20can%20become%20the%0Akey%20bottleneck%20as%20the%20input%20context%20lengthens.%20Specifically%2C%20when%20input%20lengths%0Alargely%20exceed%20the%20training%20sequence%20length%2C%20global%20channels%20exhibit%0Alimitations%20in%20adaptively%20extend%20their%20receptive%20fields%2C%20leading%20to%20Mamba%27s%0Apoor%20long-context%20performance.%20The%20key%20idea%20of%20LongMamba%20is%20to%20mitigate%20the%0Ahidden%20state%20memory%20decay%20in%20these%20global%20channels%20by%20preventing%20the%0Aaccumulation%20of%20unimportant%20tokens%20in%20their%20memory.%20This%20is%20achieved%20by%20first%0Aidentifying%20critical%20tokens%20in%20the%20global%20channels%20and%20then%20applying%20token%0Afiltering%20to%20accumulate%20only%20those%20critical%20tokens.%20Through%20extensive%0Abenchmarking%20across%20synthetic%20and%20real-world%20long-context%20scenarios%2C%20LongMamba%0Asets%20a%20new%20standard%20for%20Mamba%27s%20long-context%20performance%2C%20significantly%0Aextending%20its%20operational%20range%20without%20requiring%20additional%20training.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/GATECH-EIC/LongMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongMamba%253A%2520Enhancing%2520Mamba%2527s%2520Long%2520Context%2520Capabilities%2520via%2520Training-Free%250A%2520%2520Receptive%2520Field%2520Enlargement%26entry.906535625%3DZhifan%2520Ye%2520and%2520Kejing%2520Xia%2520and%2520Yonggan%2520Fu%2520and%2520Xin%2520Dong%2520and%2520Jihoon%2520Hong%2520and%2520Xiangchi%2520Yuan%2520and%2520Shizhe%2520Diao%2520and%2520Jan%2520Kautz%2520and%2520Pavlo%2520Molchanov%2520and%2520Yingyan%2520Celine%2520Lin%26entry.1292438233%3D%2520%2520State%2520space%2520models%2520%2528SSMs%2529%2520have%2520emerged%2520as%2520an%2520efficient%2520alternative%2520to%250ATransformer%2520models%2520for%2520language%2520modeling%252C%2520offering%2520linear%2520computational%250Acomplexity%2520and%2520constant%2520memory%2520usage%2520as%2520context%2520length%2520increases.%2520However%252C%250Adespite%2520their%2520efficiency%2520in%2520handling%2520long%2520contexts%252C%2520recent%2520studies%2520have%2520shown%250Athat%2520SSMs%252C%2520such%2520as%2520Mamba%2520models%252C%2520generally%2520underperform%2520compared%2520to%250ATransformers%2520in%2520long-context%2520understanding%2520tasks.%2520To%2520address%2520this%2520significant%250Ashortfall%2520and%2520achieve%2520both%2520efficient%2520and%2520accurate%2520long-context%2520understanding%252C%250Awe%2520propose%2520LongMamba%252C%2520a%2520training-free%2520technique%2520that%2520significantly%2520enhances%2520the%250Along-context%2520capabilities%2520of%2520Mamba%2520models.%2520LongMamba%2520builds%2520on%2520our%2520discovery%250Athat%2520the%2520hidden%2520channels%2520in%2520Mamba%2520can%2520be%2520categorized%2520into%2520local%2520and%2520global%250Achannels%2520based%2520on%2520their%2520receptive%2520field%2520lengths%252C%2520with%2520global%2520channels%2520primarily%250Aresponsible%2520for%2520long-context%2520capability.%2520These%2520global%2520channels%2520can%2520become%2520the%250Akey%2520bottleneck%2520as%2520the%2520input%2520context%2520lengthens.%2520Specifically%252C%2520when%2520input%2520lengths%250Alargely%2520exceed%2520the%2520training%2520sequence%2520length%252C%2520global%2520channels%2520exhibit%250Alimitations%2520in%2520adaptively%2520extend%2520their%2520receptive%2520fields%252C%2520leading%2520to%2520Mamba%2527s%250Apoor%2520long-context%2520performance.%2520The%2520key%2520idea%2520of%2520LongMamba%2520is%2520to%2520mitigate%2520the%250Ahidden%2520state%2520memory%2520decay%2520in%2520these%2520global%2520channels%2520by%2520preventing%2520the%250Aaccumulation%2520of%2520unimportant%2520tokens%2520in%2520their%2520memory.%2520This%2520is%2520achieved%2520by%2520first%250Aidentifying%2520critical%2520tokens%2520in%2520the%2520global%2520channels%2520and%2520then%2520applying%2520token%250Afiltering%2520to%2520accumulate%2520only%2520those%2520critical%2520tokens.%2520Through%2520extensive%250Abenchmarking%2520across%2520synthetic%2520and%2520real-world%2520long-context%2520scenarios%252C%2520LongMamba%250Asets%2520a%2520new%2520standard%2520for%2520Mamba%2527s%2520long-context%2520performance%252C%2520significantly%250Aextending%2520its%2520operational%2520range%2520without%2520requiring%2520additional%2520training.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/GATECH-EIC/LongMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongMamba%3A%20Enhancing%20Mamba%27s%20Long%20Context%20Capabilities%20via%20Training-Free%0A%20%20Receptive%20Field%20Enlargement&entry.906535625=Zhifan%20Ye%20and%20Kejing%20Xia%20and%20Yonggan%20Fu%20and%20Xin%20Dong%20and%20Jihoon%20Hong%20and%20Xiangchi%20Yuan%20and%20Shizhe%20Diao%20and%20Jan%20Kautz%20and%20Pavlo%20Molchanov%20and%20Yingyan%20Celine%20Lin&entry.1292438233=%20%20State%20space%20models%20%28SSMs%29%20have%20emerged%20as%20an%20efficient%20alternative%20to%0ATransformer%20models%20for%20language%20modeling%2C%20offering%20linear%20computational%0Acomplexity%20and%20constant%20memory%20usage%20as%20context%20length%20increases.%20However%2C%0Adespite%20their%20efficiency%20in%20handling%20long%20contexts%2C%20recent%20studies%20have%20shown%0Athat%20SSMs%2C%20such%20as%20Mamba%20models%2C%20generally%20underperform%20compared%20to%0ATransformers%20in%20long-context%20understanding%20tasks.%20To%20address%20this%20significant%0Ashortfall%20and%20achieve%20both%20efficient%20and%20accurate%20long-context%20understanding%2C%0Awe%20propose%20LongMamba%2C%20a%20training-free%20technique%20that%20significantly%20enhances%20the%0Along-context%20capabilities%20of%20Mamba%20models.%20LongMamba%20builds%20on%20our%20discovery%0Athat%20the%20hidden%20channels%20in%20Mamba%20can%20be%20categorized%20into%20local%20and%20global%0Achannels%20based%20on%20their%20receptive%20field%20lengths%2C%20with%20global%20channels%20primarily%0Aresponsible%20for%20long-context%20capability.%20These%20global%20channels%20can%20become%20the%0Akey%20bottleneck%20as%20the%20input%20context%20lengthens.%20Specifically%2C%20when%20input%20lengths%0Alargely%20exceed%20the%20training%20sequence%20length%2C%20global%20channels%20exhibit%0Alimitations%20in%20adaptively%20extend%20their%20receptive%20fields%2C%20leading%20to%20Mamba%27s%0Apoor%20long-context%20performance.%20The%20key%20idea%20of%20LongMamba%20is%20to%20mitigate%20the%0Ahidden%20state%20memory%20decay%20in%20these%20global%20channels%20by%20preventing%20the%0Aaccumulation%20of%20unimportant%20tokens%20in%20their%20memory.%20This%20is%20achieved%20by%20first%0Aidentifying%20critical%20tokens%20in%20the%20global%20channels%20and%20then%20applying%20token%0Afiltering%20to%20accumulate%20only%20those%20critical%20tokens.%20Through%20extensive%0Abenchmarking%20across%20synthetic%20and%20real-world%20long-context%20scenarios%2C%20LongMamba%0Asets%20a%20new%20standard%20for%20Mamba%27s%20long-context%20performance%2C%20significantly%0Aextending%20its%20operational%20range%20without%20requiring%20additional%20training.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/GATECH-EIC/LongMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16053v1&entry.124074799=Read"},
{"title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy\n  Multi-modal Geometric Problem Solving", "author": "Daocheng Fu and Zijun Chen and Renqiu Xia and Qi Liu and Yuan Feng and Hongbin Zhou and Renrui Zhang and Shiyang Feng and Peng Gao and Junchi Yan and Botian Shi and Bo Zhang and Yu Qiao", "abstract": "  Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen\n", "link": "http://arxiv.org/abs/2504.15780v1", "date": "2025-04-22", "relevancy": 2.606, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5245}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5202}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrustGeoGen%3A%20Scalable%20and%20Formal-Verified%20Data%20Engine%20for%20Trustworthy%0A%20%20Multi-modal%20Geometric%20Problem%20Solving&body=Title%3A%20TrustGeoGen%3A%20Scalable%20and%20Formal-Verified%20Data%20Engine%20for%20Trustworthy%0A%20%20Multi-modal%20Geometric%20Problem%20Solving%0AAuthor%3A%20Daocheng%20Fu%20and%20Zijun%20Chen%20and%20Renqiu%20Xia%20and%20Qi%20Liu%20and%20Yuan%20Feng%20and%20Hongbin%20Zhou%20and%20Renrui%20Zhang%20and%20Shiyang%20Feng%20and%20Peng%20Gao%20and%20Junchi%20Yan%20and%20Botian%20Shi%20and%20Bo%20Zhang%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Mathematical%20geometric%20problem%20solving%20%28GPS%29%20often%20requires%20effective%0Aintegration%20of%20multimodal%20information%20and%20verifiable%20logical%20coherence.%20Despite%0Athe%20fast%20development%20of%20large%20language%20models%20in%20general%20problem%20solving%2C%20it%0Aremains%20unresolved%20regarding%20with%20both%20methodology%20and%20benchmarks%2C%20especially%0Agiven%20the%20fact%20that%20exiting%20synthetic%20GPS%20benchmarks%20are%20often%20not%0Aself-verified%20and%20contain%20noise%20and%20self-contradicted%20information%20due%20to%20the%0Aillusion%20of%20LLMs.%20In%20this%20paper%2C%20we%20propose%20a%20scalable%20data%20engine%20called%0ATrustGeoGen%20for%20problem%20generation%2C%20with%20formal%20verification%20to%20provide%20a%0Aprincipled%20benchmark%2C%20which%20we%20believe%20lays%20the%20foundation%20for%20the%20further%0Adevelopment%20of%20methods%20for%20GPS.%20The%20engine%20synthesizes%20geometric%20data%20through%0Afour%20key%20innovations%3A%201%29%20multimodal-aligned%20generation%20of%20diagrams%2C%20textual%0Adescriptions%2C%20and%20stepwise%20solutions%3B%202%29%20formal%20verification%20ensuring%0Arule-compliant%20reasoning%20paths%3B%203%29%20a%20bootstrapping%20mechanism%20enabling%0Acomplexity%20escalation%20via%20recursive%20state%20generation%20and%204%29%20our%20devised%0AGeoExplore%20series%20algorithms%20simultaneously%20produce%20multi-solution%20variants%20and%0Aself-reflective%20backtracking%20traces.%20By%20formal%20logical%20verification%2C%0ATrustGeoGen%20produces%20GeoTrust-200K%20dataset%20with%20guaranteed%20modality%20integrity%2C%0Aalong%20with%20GeoTrust-test%20testset.%20Experiments%20reveal%20the%20state-of-the-art%0Amodels%20achieve%20only%2049.17%5C%25%20accuracy%20on%20GeoTrust-test%2C%20demonstrating%20its%0Aevaluation%20stringency.%20Crucially%2C%20models%20trained%20on%20GeoTrust%20achieve%20OOD%0Ageneralization%20on%20GeoQA%2C%20significantly%20reducing%20logical%20inconsistencies%0Arelative%20to%20pseudo-label%20annotated%20by%20OpenAI-o1.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Alpha-Innovator/TrustGeoGen%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrustGeoGen%253A%2520Scalable%2520and%2520Formal-Verified%2520Data%2520Engine%2520for%2520Trustworthy%250A%2520%2520Multi-modal%2520Geometric%2520Problem%2520Solving%26entry.906535625%3DDaocheng%2520Fu%2520and%2520Zijun%2520Chen%2520and%2520Renqiu%2520Xia%2520and%2520Qi%2520Liu%2520and%2520Yuan%2520Feng%2520and%2520Hongbin%2520Zhou%2520and%2520Renrui%2520Zhang%2520and%2520Shiyang%2520Feng%2520and%2520Peng%2520Gao%2520and%2520Junchi%2520Yan%2520and%2520Botian%2520Shi%2520and%2520Bo%2520Zhang%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Mathematical%2520geometric%2520problem%2520solving%2520%2528GPS%2529%2520often%2520requires%2520effective%250Aintegration%2520of%2520multimodal%2520information%2520and%2520verifiable%2520logical%2520coherence.%2520Despite%250Athe%2520fast%2520development%2520of%2520large%2520language%2520models%2520in%2520general%2520problem%2520solving%252C%2520it%250Aremains%2520unresolved%2520regarding%2520with%2520both%2520methodology%2520and%2520benchmarks%252C%2520especially%250Agiven%2520the%2520fact%2520that%2520exiting%2520synthetic%2520GPS%2520benchmarks%2520are%2520often%2520not%250Aself-verified%2520and%2520contain%2520noise%2520and%2520self-contradicted%2520information%2520due%2520to%2520the%250Aillusion%2520of%2520LLMs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520scalable%2520data%2520engine%2520called%250ATrustGeoGen%2520for%2520problem%2520generation%252C%2520with%2520formal%2520verification%2520to%2520provide%2520a%250Aprincipled%2520benchmark%252C%2520which%2520we%2520believe%2520lays%2520the%2520foundation%2520for%2520the%2520further%250Adevelopment%2520of%2520methods%2520for%2520GPS.%2520The%2520engine%2520synthesizes%2520geometric%2520data%2520through%250Afour%2520key%2520innovations%253A%25201%2529%2520multimodal-aligned%2520generation%2520of%2520diagrams%252C%2520textual%250Adescriptions%252C%2520and%2520stepwise%2520solutions%253B%25202%2529%2520formal%2520verification%2520ensuring%250Arule-compliant%2520reasoning%2520paths%253B%25203%2529%2520a%2520bootstrapping%2520mechanism%2520enabling%250Acomplexity%2520escalation%2520via%2520recursive%2520state%2520generation%2520and%25204%2529%2520our%2520devised%250AGeoExplore%2520series%2520algorithms%2520simultaneously%2520produce%2520multi-solution%2520variants%2520and%250Aself-reflective%2520backtracking%2520traces.%2520By%2520formal%2520logical%2520verification%252C%250ATrustGeoGen%2520produces%2520GeoTrust-200K%2520dataset%2520with%2520guaranteed%2520modality%2520integrity%252C%250Aalong%2520with%2520GeoTrust-test%2520testset.%2520Experiments%2520reveal%2520the%2520state-of-the-art%250Amodels%2520achieve%2520only%252049.17%255C%2525%2520accuracy%2520on%2520GeoTrust-test%252C%2520demonstrating%2520its%250Aevaluation%2520stringency.%2520Crucially%252C%2520models%2520trained%2520on%2520GeoTrust%2520achieve%2520OOD%250Ageneralization%2520on%2520GeoQA%252C%2520significantly%2520reducing%2520logical%2520inconsistencies%250Arelative%2520to%2520pseudo-label%2520annotated%2520by%2520OpenAI-o1.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Alpha-Innovator/TrustGeoGen%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrustGeoGen%3A%20Scalable%20and%20Formal-Verified%20Data%20Engine%20for%20Trustworthy%0A%20%20Multi-modal%20Geometric%20Problem%20Solving&entry.906535625=Daocheng%20Fu%20and%20Zijun%20Chen%20and%20Renqiu%20Xia%20and%20Qi%20Liu%20and%20Yuan%20Feng%20and%20Hongbin%20Zhou%20and%20Renrui%20Zhang%20and%20Shiyang%20Feng%20and%20Peng%20Gao%20and%20Junchi%20Yan%20and%20Botian%20Shi%20and%20Bo%20Zhang%20and%20Yu%20Qiao&entry.1292438233=%20%20Mathematical%20geometric%20problem%20solving%20%28GPS%29%20often%20requires%20effective%0Aintegration%20of%20multimodal%20information%20and%20verifiable%20logical%20coherence.%20Despite%0Athe%20fast%20development%20of%20large%20language%20models%20in%20general%20problem%20solving%2C%20it%0Aremains%20unresolved%20regarding%20with%20both%20methodology%20and%20benchmarks%2C%20especially%0Agiven%20the%20fact%20that%20exiting%20synthetic%20GPS%20benchmarks%20are%20often%20not%0Aself-verified%20and%20contain%20noise%20and%20self-contradicted%20information%20due%20to%20the%0Aillusion%20of%20LLMs.%20In%20this%20paper%2C%20we%20propose%20a%20scalable%20data%20engine%20called%0ATrustGeoGen%20for%20problem%20generation%2C%20with%20formal%20verification%20to%20provide%20a%0Aprincipled%20benchmark%2C%20which%20we%20believe%20lays%20the%20foundation%20for%20the%20further%0Adevelopment%20of%20methods%20for%20GPS.%20The%20engine%20synthesizes%20geometric%20data%20through%0Afour%20key%20innovations%3A%201%29%20multimodal-aligned%20generation%20of%20diagrams%2C%20textual%0Adescriptions%2C%20and%20stepwise%20solutions%3B%202%29%20formal%20verification%20ensuring%0Arule-compliant%20reasoning%20paths%3B%203%29%20a%20bootstrapping%20mechanism%20enabling%0Acomplexity%20escalation%20via%20recursive%20state%20generation%20and%204%29%20our%20devised%0AGeoExplore%20series%20algorithms%20simultaneously%20produce%20multi-solution%20variants%20and%0Aself-reflective%20backtracking%20traces.%20By%20formal%20logical%20verification%2C%0ATrustGeoGen%20produces%20GeoTrust-200K%20dataset%20with%20guaranteed%20modality%20integrity%2C%0Aalong%20with%20GeoTrust-test%20testset.%20Experiments%20reveal%20the%20state-of-the-art%0Amodels%20achieve%20only%2049.17%5C%25%20accuracy%20on%20GeoTrust-test%2C%20demonstrating%20its%0Aevaluation%20stringency.%20Crucially%2C%20models%20trained%20on%20GeoTrust%20achieve%20OOD%0Ageneralization%20on%20GeoQA%2C%20significantly%20reducing%20logical%20inconsistencies%0Arelative%20to%20pseudo-label%20annotated%20by%20OpenAI-o1.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Alpha-Innovator/TrustGeoGen%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15780v1&entry.124074799=Read"},
{"title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting", "author": "Jian Hu and Dimitrios Korkinof and Shaogang Gong and Mariano Beguerisse-Diaz", "abstract": "  We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.\n", "link": "http://arxiv.org/abs/2504.15921v1", "date": "2025-04-22", "relevancy": 2.5906, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViSMaP%3A%20Unsupervised%20Hour-long%20Video%20Summarisation%20by%20Meta-Prompting&body=Title%3A%20ViSMaP%3A%20Unsupervised%20Hour-long%20Video%20Summarisation%20by%20Meta-Prompting%0AAuthor%3A%20Jian%20Hu%20and%20Dimitrios%20Korkinof%20and%20Shaogang%20Gong%20and%20Mariano%20Beguerisse-Diaz%0AAbstract%3A%20%20%20We%20introduce%20ViSMap%3A%20Unsupervised%20Video%20Summarisation%20by%20Meta%20Prompting%2C%20a%0Asystem%20to%20summarise%20hour%20long%20videos%20with%20no-supervision.%20Most%20existing%20video%0Aunderstanding%20models%20work%20well%20on%20short%20videos%20of%20pre-segmented%20events%2C%20yet%0Athey%20struggle%20to%20summarise%20longer%20videos%20where%20relevant%20events%20are%20sparsely%0Adistributed%20and%20not%20pre-segmented.%20Moreover%2C%20long-form%20video%20understanding%0Aoften%20relies%20on%20supervised%20hierarchical%20training%20that%20needs%20extensive%0Aannotations%20which%20are%20costly%2C%20slow%20and%20prone%20to%20inconsistency.%20With%20ViSMaP%20we%0Abridge%20the%20gap%20between%20short%20videos%20%28where%20annotated%20data%20is%20plentiful%29%20and%0Along%20ones%20%28where%20it%27s%20not%29.%20We%20rely%20on%20LLMs%20to%20create%20optimised%0Apseudo-summaries%20of%20long%20videos%20using%20segment%20descriptions%20from%20short%20ones.%0AThese%20pseudo-summaries%20are%20used%20as%20training%20data%20for%20a%20model%20that%20generates%0Along-form%20video%20summaries%2C%20bypassing%20the%20need%20for%20expensive%20annotations%20of%20long%0Avideos.%20Specifically%2C%20we%20adopt%20a%20meta-prompting%20strategy%20to%20iteratively%0Agenerate%20and%20refine%20creating%20pseudo-summaries%20of%20long%20videos.%20The%20strategy%0Aleverages%20short%20clip%20descriptions%20obtained%20from%20a%20supervised%20short%20video%20model%0Ato%20guide%20the%20summary.%20Each%20iteration%20uses%20three%20LLMs%20working%20in%20sequence%3A%20one%0Ato%20generate%20the%20pseudo-summary%20from%20clip%20descriptions%2C%20another%20to%20evaluate%20it%2C%0Aand%20a%20third%20to%20optimise%20the%20prompt%20of%20the%20generator.%20This%20iteration%20is%0Anecessary%20because%20the%20quality%20of%20the%20pseudo-summaries%20is%20highly%20dependent%20on%0Athe%20generator%20prompt%2C%20and%20varies%20widely%20among%20videos.%20We%20evaluate%20our%20summaries%0Aextensively%20on%20multiple%20datasets%3B%20our%20results%20show%20that%20ViSMaP%20achieves%0Aperformance%20comparable%20to%20fully%20supervised%20state-of-the-art%20models%20while%0Ageneralising%20across%20domains%20without%20sacrificing%20performance.%20Code%20will%20be%0Areleased%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViSMaP%253A%2520Unsupervised%2520Hour-long%2520Video%2520Summarisation%2520by%2520Meta-Prompting%26entry.906535625%3DJian%2520Hu%2520and%2520Dimitrios%2520Korkinof%2520and%2520Shaogang%2520Gong%2520and%2520Mariano%2520Beguerisse-Diaz%26entry.1292438233%3D%2520%2520We%2520introduce%2520ViSMap%253A%2520Unsupervised%2520Video%2520Summarisation%2520by%2520Meta%2520Prompting%252C%2520a%250Asystem%2520to%2520summarise%2520hour%2520long%2520videos%2520with%2520no-supervision.%2520Most%2520existing%2520video%250Aunderstanding%2520models%2520work%2520well%2520on%2520short%2520videos%2520of%2520pre-segmented%2520events%252C%2520yet%250Athey%2520struggle%2520to%2520summarise%2520longer%2520videos%2520where%2520relevant%2520events%2520are%2520sparsely%250Adistributed%2520and%2520not%2520pre-segmented.%2520Moreover%252C%2520long-form%2520video%2520understanding%250Aoften%2520relies%2520on%2520supervised%2520hierarchical%2520training%2520that%2520needs%2520extensive%250Aannotations%2520which%2520are%2520costly%252C%2520slow%2520and%2520prone%2520to%2520inconsistency.%2520With%2520ViSMaP%2520we%250Abridge%2520the%2520gap%2520between%2520short%2520videos%2520%2528where%2520annotated%2520data%2520is%2520plentiful%2529%2520and%250Along%2520ones%2520%2528where%2520it%2527s%2520not%2529.%2520We%2520rely%2520on%2520LLMs%2520to%2520create%2520optimised%250Apseudo-summaries%2520of%2520long%2520videos%2520using%2520segment%2520descriptions%2520from%2520short%2520ones.%250AThese%2520pseudo-summaries%2520are%2520used%2520as%2520training%2520data%2520for%2520a%2520model%2520that%2520generates%250Along-form%2520video%2520summaries%252C%2520bypassing%2520the%2520need%2520for%2520expensive%2520annotations%2520of%2520long%250Avideos.%2520Specifically%252C%2520we%2520adopt%2520a%2520meta-prompting%2520strategy%2520to%2520iteratively%250Agenerate%2520and%2520refine%2520creating%2520pseudo-summaries%2520of%2520long%2520videos.%2520The%2520strategy%250Aleverages%2520short%2520clip%2520descriptions%2520obtained%2520from%2520a%2520supervised%2520short%2520video%2520model%250Ato%2520guide%2520the%2520summary.%2520Each%2520iteration%2520uses%2520three%2520LLMs%2520working%2520in%2520sequence%253A%2520one%250Ato%2520generate%2520the%2520pseudo-summary%2520from%2520clip%2520descriptions%252C%2520another%2520to%2520evaluate%2520it%252C%250Aand%2520a%2520third%2520to%2520optimise%2520the%2520prompt%2520of%2520the%2520generator.%2520This%2520iteration%2520is%250Anecessary%2520because%2520the%2520quality%2520of%2520the%2520pseudo-summaries%2520is%2520highly%2520dependent%2520on%250Athe%2520generator%2520prompt%252C%2520and%2520varies%2520widely%2520among%2520videos.%2520We%2520evaluate%2520our%2520summaries%250Aextensively%2520on%2520multiple%2520datasets%253B%2520our%2520results%2520show%2520that%2520ViSMaP%2520achieves%250Aperformance%2520comparable%2520to%2520fully%2520supervised%2520state-of-the-art%2520models%2520while%250Ageneralising%2520across%2520domains%2520without%2520sacrificing%2520performance.%2520Code%2520will%2520be%250Areleased%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViSMaP%3A%20Unsupervised%20Hour-long%20Video%20Summarisation%20by%20Meta-Prompting&entry.906535625=Jian%20Hu%20and%20Dimitrios%20Korkinof%20and%20Shaogang%20Gong%20and%20Mariano%20Beguerisse-Diaz&entry.1292438233=%20%20We%20introduce%20ViSMap%3A%20Unsupervised%20Video%20Summarisation%20by%20Meta%20Prompting%2C%20a%0Asystem%20to%20summarise%20hour%20long%20videos%20with%20no-supervision.%20Most%20existing%20video%0Aunderstanding%20models%20work%20well%20on%20short%20videos%20of%20pre-segmented%20events%2C%20yet%0Athey%20struggle%20to%20summarise%20longer%20videos%20where%20relevant%20events%20are%20sparsely%0Adistributed%20and%20not%20pre-segmented.%20Moreover%2C%20long-form%20video%20understanding%0Aoften%20relies%20on%20supervised%20hierarchical%20training%20that%20needs%20extensive%0Aannotations%20which%20are%20costly%2C%20slow%20and%20prone%20to%20inconsistency.%20With%20ViSMaP%20we%0Abridge%20the%20gap%20between%20short%20videos%20%28where%20annotated%20data%20is%20plentiful%29%20and%0Along%20ones%20%28where%20it%27s%20not%29.%20We%20rely%20on%20LLMs%20to%20create%20optimised%0Apseudo-summaries%20of%20long%20videos%20using%20segment%20descriptions%20from%20short%20ones.%0AThese%20pseudo-summaries%20are%20used%20as%20training%20data%20for%20a%20model%20that%20generates%0Along-form%20video%20summaries%2C%20bypassing%20the%20need%20for%20expensive%20annotations%20of%20long%0Avideos.%20Specifically%2C%20we%20adopt%20a%20meta-prompting%20strategy%20to%20iteratively%0Agenerate%20and%20refine%20creating%20pseudo-summaries%20of%20long%20videos.%20The%20strategy%0Aleverages%20short%20clip%20descriptions%20obtained%20from%20a%20supervised%20short%20video%20model%0Ato%20guide%20the%20summary.%20Each%20iteration%20uses%20three%20LLMs%20working%20in%20sequence%3A%20one%0Ato%20generate%20the%20pseudo-summary%20from%20clip%20descriptions%2C%20another%20to%20evaluate%20it%2C%0Aand%20a%20third%20to%20optimise%20the%20prompt%20of%20the%20generator.%20This%20iteration%20is%0Anecessary%20because%20the%20quality%20of%20the%20pseudo-summaries%20is%20highly%20dependent%20on%0Athe%20generator%20prompt%2C%20and%20varies%20widely%20among%20videos.%20We%20evaluate%20our%20summaries%0Aextensively%20on%20multiple%20datasets%3B%20our%20results%20show%20that%20ViSMaP%20achieves%0Aperformance%20comparable%20to%20fully%20supervised%20state-of-the-art%20models%20while%0Ageneralising%20across%20domains%20without%20sacrificing%20performance.%20Code%20will%20be%0Areleased%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15921v1&entry.124074799=Read"},
{"title": "FocusedAD: Character-centric Movie Audio Description", "author": "Xiaojun Ye and Chun Wang and Yiren Song and Sheng Zhou and Liangcheng Li and Jiajun Bu", "abstract": "  Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD .\n", "link": "http://arxiv.org/abs/2504.12157v2", "date": "2025-04-22", "relevancy": 2.5624, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocusedAD%3A%20Character-centric%20Movie%20Audio%20Description&body=Title%3A%20FocusedAD%3A%20Character-centric%20Movie%20Audio%20Description%0AAuthor%3A%20Xiaojun%20Ye%20and%20Chun%20Wang%20and%20Yiren%20Song%20and%20Sheng%20Zhou%20and%20Liangcheng%20Li%20and%20Jiajun%20Bu%0AAbstract%3A%20%20%20Movie%20Audio%20Description%20%28AD%29%20aims%20to%20narrate%20visual%20content%20during%0Adialogue-free%20segments%2C%20particularly%20benefiting%20blind%20and%20visually%20impaired%0A%28BVI%29%20audiences.%20Compared%20with%20general%20video%20captioning%2C%20AD%20demands%0Aplot-relevant%20narration%20with%20explicit%20character%20name%20references%2C%20posing%20unique%0Achallenges%20in%20movie%20understanding.To%20identify%20active%20main%20characters%20and%20focus%0Aon%20storyline-relevant%20regions%2C%20we%20propose%20FocusedAD%2C%20a%20novel%20framework%20that%0Adelivers%20character-centric%20movie%20audio%20descriptions.%20It%20includes%3A%20%28i%29%20a%0ACharacter%20Perception%20Module%28CPM%29%20for%20tracking%20character%20regions%20and%20linking%0Athem%20to%20names%3B%20%28ii%29%20a%20Dynamic%20Prior%20Module%28DPM%29%20that%20injects%20contextual%20cues%0Afrom%20prior%20ADs%20and%20subtitles%20via%20learnable%20soft%20prompts%3B%20and%20%28iii%29%20a%20Focused%0ACaption%20Module%28FCM%29%20that%20generates%20narrations%20enriched%20with%20plot-relevant%0Adetails%20and%20named%20characters.%20To%20overcome%20limitations%20in%20character%0Aidentification%2C%20we%20also%20introduce%20an%20automated%20pipeline%20for%20building%20character%0Aquery%20banks.%20FocusedAD%20achieves%20state-of-the-art%20performance%20on%20multiple%0Abenchmarks%2C%20including%20strong%20zero-shot%20results%20on%20MAD-eval-Named%20and%20our%20newly%0Aproposed%20Cinepile-AD%20dataset.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//github.com/Thorin215/FocusedAD%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocusedAD%253A%2520Character-centric%2520Movie%2520Audio%2520Description%26entry.906535625%3DXiaojun%2520Ye%2520and%2520Chun%2520Wang%2520and%2520Yiren%2520Song%2520and%2520Sheng%2520Zhou%2520and%2520Liangcheng%2520Li%2520and%2520Jiajun%2520Bu%26entry.1292438233%3D%2520%2520Movie%2520Audio%2520Description%2520%2528AD%2529%2520aims%2520to%2520narrate%2520visual%2520content%2520during%250Adialogue-free%2520segments%252C%2520particularly%2520benefiting%2520blind%2520and%2520visually%2520impaired%250A%2528BVI%2529%2520audiences.%2520Compared%2520with%2520general%2520video%2520captioning%252C%2520AD%2520demands%250Aplot-relevant%2520narration%2520with%2520explicit%2520character%2520name%2520references%252C%2520posing%2520unique%250Achallenges%2520in%2520movie%2520understanding.To%2520identify%2520active%2520main%2520characters%2520and%2520focus%250Aon%2520storyline-relevant%2520regions%252C%2520we%2520propose%2520FocusedAD%252C%2520a%2520novel%2520framework%2520that%250Adelivers%2520character-centric%2520movie%2520audio%2520descriptions.%2520It%2520includes%253A%2520%2528i%2529%2520a%250ACharacter%2520Perception%2520Module%2528CPM%2529%2520for%2520tracking%2520character%2520regions%2520and%2520linking%250Athem%2520to%2520names%253B%2520%2528ii%2529%2520a%2520Dynamic%2520Prior%2520Module%2528DPM%2529%2520that%2520injects%2520contextual%2520cues%250Afrom%2520prior%2520ADs%2520and%2520subtitles%2520via%2520learnable%2520soft%2520prompts%253B%2520and%2520%2528iii%2529%2520a%2520Focused%250ACaption%2520Module%2528FCM%2529%2520that%2520generates%2520narrations%2520enriched%2520with%2520plot-relevant%250Adetails%2520and%2520named%2520characters.%2520To%2520overcome%2520limitations%2520in%2520character%250Aidentification%252C%2520we%2520also%2520introduce%2520an%2520automated%2520pipeline%2520for%2520building%2520character%250Aquery%2520banks.%2520FocusedAD%2520achieves%2520state-of-the-art%2520performance%2520on%2520multiple%250Abenchmarks%252C%2520including%2520strong%2520zero-shot%2520results%2520on%2520MAD-eval-Named%2520and%2520our%2520newly%250Aproposed%2520Cinepile-AD%2520dataset.%2520Code%2520and%2520data%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/Thorin215/FocusedAD%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocusedAD%3A%20Character-centric%20Movie%20Audio%20Description&entry.906535625=Xiaojun%20Ye%20and%20Chun%20Wang%20and%20Yiren%20Song%20and%20Sheng%20Zhou%20and%20Liangcheng%20Li%20and%20Jiajun%20Bu&entry.1292438233=%20%20Movie%20Audio%20Description%20%28AD%29%20aims%20to%20narrate%20visual%20content%20during%0Adialogue-free%20segments%2C%20particularly%20benefiting%20blind%20and%20visually%20impaired%0A%28BVI%29%20audiences.%20Compared%20with%20general%20video%20captioning%2C%20AD%20demands%0Aplot-relevant%20narration%20with%20explicit%20character%20name%20references%2C%20posing%20unique%0Achallenges%20in%20movie%20understanding.To%20identify%20active%20main%20characters%20and%20focus%0Aon%20storyline-relevant%20regions%2C%20we%20propose%20FocusedAD%2C%20a%20novel%20framework%20that%0Adelivers%20character-centric%20movie%20audio%20descriptions.%20It%20includes%3A%20%28i%29%20a%0ACharacter%20Perception%20Module%28CPM%29%20for%20tracking%20character%20regions%20and%20linking%0Athem%20to%20names%3B%20%28ii%29%20a%20Dynamic%20Prior%20Module%28DPM%29%20that%20injects%20contextual%20cues%0Afrom%20prior%20ADs%20and%20subtitles%20via%20learnable%20soft%20prompts%3B%20and%20%28iii%29%20a%20Focused%0ACaption%20Module%28FCM%29%20that%20generates%20narrations%20enriched%20with%20plot-relevant%0Adetails%20and%20named%20characters.%20To%20overcome%20limitations%20in%20character%0Aidentification%2C%20we%20also%20introduce%20an%20automated%20pipeline%20for%20building%20character%0Aquery%20banks.%20FocusedAD%20achieves%20state-of-the-art%20performance%20on%20multiple%0Abenchmarks%2C%20including%20strong%20zero-shot%20results%20on%20MAD-eval-Named%20and%20our%20newly%0Aproposed%20Cinepile-AD%20dataset.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//github.com/Thorin215/FocusedAD%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12157v2&entry.124074799=Read"},
{"title": "Low-Rank Adaptation of Neural Fields", "author": "Anh Truong and Ahmed H. Mahmoud and Mina Konakovi\u0107 Lukovi\u0107 and Justin Solomon", "abstract": "  Processing visual data often involves small adjustments or sequences of\nchanges, such as in image filtering, surface smoothing, and video storage.\nWhile established graphics techniques like normal mapping and video compression\nexploit redundancy to encode such small changes efficiently, the problem of\nencoding small changes to neural fields (NF) -- neural network\nparameterizations of visual or physical functions -- has received less\nattention.\n  We propose a parameter-efficient strategy for updating neural fields using\nlow-rank adaptations (LoRA). LoRA, a method from the parameter-efficient\nfine-tuning LLM community, encodes small updates to pre-trained models with\nminimal computational overhead. We adapt LoRA to instance-specific neural\nfields, avoiding the need for large pre-trained models yielding a pipeline\nsuitable for low-compute hardware.\n  We validate our approach with experiments in image filtering, video\ncompression, and geometry editing, demonstrating its effectiveness and\nversatility for representing neural field updates.\n", "link": "http://arxiv.org/abs/2504.15933v1", "date": "2025-04-22", "relevancy": 2.5384, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5254}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5024}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Adaptation%20of%20Neural%20Fields&body=Title%3A%20Low-Rank%20Adaptation%20of%20Neural%20Fields%0AAuthor%3A%20Anh%20Truong%20and%20Ahmed%20H.%20Mahmoud%20and%20Mina%20Konakovi%C4%87%20Lukovi%C4%87%20and%20Justin%20Solomon%0AAbstract%3A%20%20%20Processing%20visual%20data%20often%20involves%20small%20adjustments%20or%20sequences%20of%0Achanges%2C%20such%20as%20in%20image%20filtering%2C%20surface%20smoothing%2C%20and%20video%20storage.%0AWhile%20established%20graphics%20techniques%20like%20normal%20mapping%20and%20video%20compression%0Aexploit%20redundancy%20to%20encode%20such%20small%20changes%20efficiently%2C%20the%20problem%20of%0Aencoding%20small%20changes%20to%20neural%20fields%20%28NF%29%20--%20neural%20network%0Aparameterizations%20of%20visual%20or%20physical%20functions%20--%20has%20received%20less%0Aattention.%0A%20%20We%20propose%20a%20parameter-efficient%20strategy%20for%20updating%20neural%20fields%20using%0Alow-rank%20adaptations%20%28LoRA%29.%20LoRA%2C%20a%20method%20from%20the%20parameter-efficient%0Afine-tuning%20LLM%20community%2C%20encodes%20small%20updates%20to%20pre-trained%20models%20with%0Aminimal%20computational%20overhead.%20We%20adapt%20LoRA%20to%20instance-specific%20neural%0Afields%2C%20avoiding%20the%20need%20for%20large%20pre-trained%20models%20yielding%20a%20pipeline%0Asuitable%20for%20low-compute%20hardware.%0A%20%20We%20validate%20our%20approach%20with%20experiments%20in%20image%20filtering%2C%20video%0Acompression%2C%20and%20geometry%20editing%2C%20demonstrating%20its%20effectiveness%20and%0Aversatility%20for%20representing%20neural%20field%20updates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Adaptation%2520of%2520Neural%2520Fields%26entry.906535625%3DAnh%2520Truong%2520and%2520Ahmed%2520H.%2520Mahmoud%2520and%2520Mina%2520Konakovi%25C4%2587%2520Lukovi%25C4%2587%2520and%2520Justin%2520Solomon%26entry.1292438233%3D%2520%2520Processing%2520visual%2520data%2520often%2520involves%2520small%2520adjustments%2520or%2520sequences%2520of%250Achanges%252C%2520such%2520as%2520in%2520image%2520filtering%252C%2520surface%2520smoothing%252C%2520and%2520video%2520storage.%250AWhile%2520established%2520graphics%2520techniques%2520like%2520normal%2520mapping%2520and%2520video%2520compression%250Aexploit%2520redundancy%2520to%2520encode%2520such%2520small%2520changes%2520efficiently%252C%2520the%2520problem%2520of%250Aencoding%2520small%2520changes%2520to%2520neural%2520fields%2520%2528NF%2529%2520--%2520neural%2520network%250Aparameterizations%2520of%2520visual%2520or%2520physical%2520functions%2520--%2520has%2520received%2520less%250Aattention.%250A%2520%2520We%2520propose%2520a%2520parameter-efficient%2520strategy%2520for%2520updating%2520neural%2520fields%2520using%250Alow-rank%2520adaptations%2520%2528LoRA%2529.%2520LoRA%252C%2520a%2520method%2520from%2520the%2520parameter-efficient%250Afine-tuning%2520LLM%2520community%252C%2520encodes%2520small%2520updates%2520to%2520pre-trained%2520models%2520with%250Aminimal%2520computational%2520overhead.%2520We%2520adapt%2520LoRA%2520to%2520instance-specific%2520neural%250Afields%252C%2520avoiding%2520the%2520need%2520for%2520large%2520pre-trained%2520models%2520yielding%2520a%2520pipeline%250Asuitable%2520for%2520low-compute%2520hardware.%250A%2520%2520We%2520validate%2520our%2520approach%2520with%2520experiments%2520in%2520image%2520filtering%252C%2520video%250Acompression%252C%2520and%2520geometry%2520editing%252C%2520demonstrating%2520its%2520effectiveness%2520and%250Aversatility%2520for%2520representing%2520neural%2520field%2520updates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Adaptation%20of%20Neural%20Fields&entry.906535625=Anh%20Truong%20and%20Ahmed%20H.%20Mahmoud%20and%20Mina%20Konakovi%C4%87%20Lukovi%C4%87%20and%20Justin%20Solomon&entry.1292438233=%20%20Processing%20visual%20data%20often%20involves%20small%20adjustments%20or%20sequences%20of%0Achanges%2C%20such%20as%20in%20image%20filtering%2C%20surface%20smoothing%2C%20and%20video%20storage.%0AWhile%20established%20graphics%20techniques%20like%20normal%20mapping%20and%20video%20compression%0Aexploit%20redundancy%20to%20encode%20such%20small%20changes%20efficiently%2C%20the%20problem%20of%0Aencoding%20small%20changes%20to%20neural%20fields%20%28NF%29%20--%20neural%20network%0Aparameterizations%20of%20visual%20or%20physical%20functions%20--%20has%20received%20less%0Aattention.%0A%20%20We%20propose%20a%20parameter-efficient%20strategy%20for%20updating%20neural%20fields%20using%0Alow-rank%20adaptations%20%28LoRA%29.%20LoRA%2C%20a%20method%20from%20the%20parameter-efficient%0Afine-tuning%20LLM%20community%2C%20encodes%20small%20updates%20to%20pre-trained%20models%20with%0Aminimal%20computational%20overhead.%20We%20adapt%20LoRA%20to%20instance-specific%20neural%0Afields%2C%20avoiding%20the%20need%20for%20large%20pre-trained%20models%20yielding%20a%20pipeline%0Asuitable%20for%20low-compute%20hardware.%0A%20%20We%20validate%20our%20approach%20with%20experiments%20in%20image%20filtering%2C%20video%0Acompression%2C%20and%20geometry%20editing%2C%20demonstrating%20its%20effectiveness%20and%0Aversatility%20for%20representing%20neural%20field%20updates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15933v1&entry.124074799=Read"},
{"title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer", "author": "Soham Sane", "abstract": "  We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\naddressing the memory overhead and hyperparameter complexity of adaptive\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\ngradient normalization followed by a smooth hyperbolic tangent transformation,\n$g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness\nparameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\nformulation; (2) a formal non-convex convergence analysis guaranteeing\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\nperformance profile. While exhibiting instability in off-policy DQN, it\nprovides enhanced training stability with competitive results in TD3 (requiring\ncareful $\\alpha$ tuning) and achieves substantially superior performance in\non-policy PPO. These results underscore the critical importance of empirical\n$\\alpha$ selection, revealing strong interactions between the optimizer's\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\nalternative optimizer for memory-constrained scenarios and shows significant\npromise for on-policy learning regimes where its stability and efficiency\nadvantages can be particularly impactful.\n", "link": "http://arxiv.org/abs/2504.16020v1", "date": "2025-04-22", "relevancy": 2.4889, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5127}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5075}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaGrad%3A%20Non-Linear%20Gradient%20Normalization%20Optimizer&body=Title%3A%20AlphaGrad%3A%20Non-Linear%20Gradient%20Normalization%20Optimizer%0AAuthor%3A%20Soham%20Sane%0AAbstract%3A%20%20%20We%20introduce%20AlphaGrad%2C%20a%20memory-efficient%2C%20conditionally%20stateless%20optimizer%0Aaddressing%20the%20memory%20overhead%20and%20hyperparameter%20complexity%20of%20adaptive%0Amethods%20like%20Adam.%20AlphaGrad%20enforces%20scale%20invariance%20via%20tensor-wise%20L2%0Agradient%20normalization%20followed%20by%20a%20smooth%20hyperbolic%20tangent%20transformation%2C%0A%24g%27%20%3D%20%5Ctanh%28%5Calpha%20%5Ccdot%20%5Ctilde%7Bg%7D%29%24%2C%20controlled%20by%20a%20single%20steepness%0Aparameter%20%24%5Calpha%24.%20Our%20contributions%20include%3A%20%281%29%20the%20AlphaGrad%20algorithm%0Aformulation%3B%20%282%29%20a%20formal%20non-convex%20convergence%20analysis%20guaranteeing%0Astationarity%3B%20%283%29%20extensive%20empirical%20evaluation%20on%20diverse%20RL%20benchmarks%20%28DQN%2C%0ATD3%2C%20PPO%29.%20Compared%20to%20Adam%2C%20AlphaGrad%20demonstrates%20a%20highly%20context-dependent%0Aperformance%20profile.%20While%20exhibiting%20instability%20in%20off-policy%20DQN%2C%20it%0Aprovides%20enhanced%20training%20stability%20with%20competitive%20results%20in%20TD3%20%28requiring%0Acareful%20%24%5Calpha%24%20tuning%29%20and%20achieves%20substantially%20superior%20performance%20in%0Aon-policy%20PPO.%20These%20results%20underscore%20the%20critical%20importance%20of%20empirical%0A%24%5Calpha%24%20selection%2C%20revealing%20strong%20interactions%20between%20the%20optimizer%27s%0Adynamics%20and%20the%20underlying%20RL%20algorithm.%20AlphaGrad%20presents%20a%20compelling%0Aalternative%20optimizer%20for%20memory-constrained%20scenarios%20and%20shows%20significant%0Apromise%20for%20on-policy%20learning%20regimes%20where%20its%20stability%20and%20efficiency%0Aadvantages%20can%20be%20particularly%20impactful.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaGrad%253A%2520Non-Linear%2520Gradient%2520Normalization%2520Optimizer%26entry.906535625%3DSoham%2520Sane%26entry.1292438233%3D%2520%2520We%2520introduce%2520AlphaGrad%252C%2520a%2520memory-efficient%252C%2520conditionally%2520stateless%2520optimizer%250Aaddressing%2520the%2520memory%2520overhead%2520and%2520hyperparameter%2520complexity%2520of%2520adaptive%250Amethods%2520like%2520Adam.%2520AlphaGrad%2520enforces%2520scale%2520invariance%2520via%2520tensor-wise%2520L2%250Agradient%2520normalization%2520followed%2520by%2520a%2520smooth%2520hyperbolic%2520tangent%2520transformation%252C%250A%2524g%2527%2520%253D%2520%255Ctanh%2528%255Calpha%2520%255Ccdot%2520%255Ctilde%257Bg%257D%2529%2524%252C%2520controlled%2520by%2520a%2520single%2520steepness%250Aparameter%2520%2524%255Calpha%2524.%2520Our%2520contributions%2520include%253A%2520%25281%2529%2520the%2520AlphaGrad%2520algorithm%250Aformulation%253B%2520%25282%2529%2520a%2520formal%2520non-convex%2520convergence%2520analysis%2520guaranteeing%250Astationarity%253B%2520%25283%2529%2520extensive%2520empirical%2520evaluation%2520on%2520diverse%2520RL%2520benchmarks%2520%2528DQN%252C%250ATD3%252C%2520PPO%2529.%2520Compared%2520to%2520Adam%252C%2520AlphaGrad%2520demonstrates%2520a%2520highly%2520context-dependent%250Aperformance%2520profile.%2520While%2520exhibiting%2520instability%2520in%2520off-policy%2520DQN%252C%2520it%250Aprovides%2520enhanced%2520training%2520stability%2520with%2520competitive%2520results%2520in%2520TD3%2520%2528requiring%250Acareful%2520%2524%255Calpha%2524%2520tuning%2529%2520and%2520achieves%2520substantially%2520superior%2520performance%2520in%250Aon-policy%2520PPO.%2520These%2520results%2520underscore%2520the%2520critical%2520importance%2520of%2520empirical%250A%2524%255Calpha%2524%2520selection%252C%2520revealing%2520strong%2520interactions%2520between%2520the%2520optimizer%2527s%250Adynamics%2520and%2520the%2520underlying%2520RL%2520algorithm.%2520AlphaGrad%2520presents%2520a%2520compelling%250Aalternative%2520optimizer%2520for%2520memory-constrained%2520scenarios%2520and%2520shows%2520significant%250Apromise%2520for%2520on-policy%2520learning%2520regimes%2520where%2520its%2520stability%2520and%2520efficiency%250Aadvantages%2520can%2520be%2520particularly%2520impactful.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaGrad%3A%20Non-Linear%20Gradient%20Normalization%20Optimizer&entry.906535625=Soham%20Sane&entry.1292438233=%20%20We%20introduce%20AlphaGrad%2C%20a%20memory-efficient%2C%20conditionally%20stateless%20optimizer%0Aaddressing%20the%20memory%20overhead%20and%20hyperparameter%20complexity%20of%20adaptive%0Amethods%20like%20Adam.%20AlphaGrad%20enforces%20scale%20invariance%20via%20tensor-wise%20L2%0Agradient%20normalization%20followed%20by%20a%20smooth%20hyperbolic%20tangent%20transformation%2C%0A%24g%27%20%3D%20%5Ctanh%28%5Calpha%20%5Ccdot%20%5Ctilde%7Bg%7D%29%24%2C%20controlled%20by%20a%20single%20steepness%0Aparameter%20%24%5Calpha%24.%20Our%20contributions%20include%3A%20%281%29%20the%20AlphaGrad%20algorithm%0Aformulation%3B%20%282%29%20a%20formal%20non-convex%20convergence%20analysis%20guaranteeing%0Astationarity%3B%20%283%29%20extensive%20empirical%20evaluation%20on%20diverse%20RL%20benchmarks%20%28DQN%2C%0ATD3%2C%20PPO%29.%20Compared%20to%20Adam%2C%20AlphaGrad%20demonstrates%20a%20highly%20context-dependent%0Aperformance%20profile.%20While%20exhibiting%20instability%20in%20off-policy%20DQN%2C%20it%0Aprovides%20enhanced%20training%20stability%20with%20competitive%20results%20in%20TD3%20%28requiring%0Acareful%20%24%5Calpha%24%20tuning%29%20and%20achieves%20substantially%20superior%20performance%20in%0Aon-policy%20PPO.%20These%20results%20underscore%20the%20critical%20importance%20of%20empirical%0A%24%5Calpha%24%20selection%2C%20revealing%20strong%20interactions%20between%20the%20optimizer%27s%0Adynamics%20and%20the%20underlying%20RL%20algorithm.%20AlphaGrad%20presents%20a%20compelling%0Aalternative%20optimizer%20for%20memory-constrained%20scenarios%20and%20shows%20significant%0Apromise%20for%20on-policy%20learning%20regimes%20where%20its%20stability%20and%20efficiency%0Aadvantages%20can%20be%20particularly%20impactful.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16020v1&entry.124074799=Read"},
{"title": "Development and evaluation of a deep learning algorithm for German word\n  recognition from lip movements", "author": "Dinh Nam Pham and Torsten Rahne", "abstract": "  When reading lips, many people benefit from additional visual information\nfrom the lip movements of the speaker, which is, however, very error prone.\nAlgorithms for lip reading with artificial intelligence based on artificial\nneural networks significantly improve word recognition but are not available\nfor the German language. A total of 1806 video clips with only one\nGerman-speaking person each were selected, split into word segments, and\nassigned to word classes using speech-recognition software. In 38,391 video\nsegments with 32 speakers, 18 polysyllabic, visually distinguishable words were\nused to train and validate a neural network. The 3D Convolutional Neural\nNetwork and Gated Recurrent Units models and a combination of both models\n(GRUConv) were compared, as were different image sections and color spaces of\nthe videos. The accuracy was determined in 5000 training epochs. Comparison of\nthe color spaces did not reveal any relevant different correct classification\nrates in the range from 69% to 72%. With a cut to the lips, a significantly\nhigher accuracy of 70% was achieved than when cut to the entire speaker's face\n(34%). With the GRUConv model, the maximum accuracies were 87% with known\nspeakers and 63% in the validation with unknown speakers. The neural network\nfor lip reading, which was first developed for the German language, shows a\nvery high level of accuracy, comparable to English-language algorithms. It\nworks with unknown speakers as well and can be generalized with more word\nclasses.\n", "link": "http://arxiv.org/abs/2504.15792v1", "date": "2025-04-22", "relevancy": 2.4837, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Development%20and%20evaluation%20of%20a%20deep%20learning%20algorithm%20for%20German%20word%0A%20%20recognition%20from%20lip%20movements&body=Title%3A%20Development%20and%20evaluation%20of%20a%20deep%20learning%20algorithm%20for%20German%20word%0A%20%20recognition%20from%20lip%20movements%0AAuthor%3A%20Dinh%20Nam%20Pham%20and%20Torsten%20Rahne%0AAbstract%3A%20%20%20When%20reading%20lips%2C%20many%20people%20benefit%20from%20additional%20visual%20information%0Afrom%20the%20lip%20movements%20of%20the%20speaker%2C%20which%20is%2C%20however%2C%20very%20error%20prone.%0AAlgorithms%20for%20lip%20reading%20with%20artificial%20intelligence%20based%20on%20artificial%0Aneural%20networks%20significantly%20improve%20word%20recognition%20but%20are%20not%20available%0Afor%20the%20German%20language.%20A%20total%20of%201806%20video%20clips%20with%20only%20one%0AGerman-speaking%20person%20each%20were%20selected%2C%20split%20into%20word%20segments%2C%20and%0Aassigned%20to%20word%20classes%20using%20speech-recognition%20software.%20In%2038%2C391%20video%0Asegments%20with%2032%20speakers%2C%2018%20polysyllabic%2C%20visually%20distinguishable%20words%20were%0Aused%20to%20train%20and%20validate%20a%20neural%20network.%20The%203D%20Convolutional%20Neural%0ANetwork%20and%20Gated%20Recurrent%20Units%20models%20and%20a%20combination%20of%20both%20models%0A%28GRUConv%29%20were%20compared%2C%20as%20were%20different%20image%20sections%20and%20color%20spaces%20of%0Athe%20videos.%20The%20accuracy%20was%20determined%20in%205000%20training%20epochs.%20Comparison%20of%0Athe%20color%20spaces%20did%20not%20reveal%20any%20relevant%20different%20correct%20classification%0Arates%20in%20the%20range%20from%2069%25%20to%2072%25.%20With%20a%20cut%20to%20the%20lips%2C%20a%20significantly%0Ahigher%20accuracy%20of%2070%25%20was%20achieved%20than%20when%20cut%20to%20the%20entire%20speaker%27s%20face%0A%2834%25%29.%20With%20the%20GRUConv%20model%2C%20the%20maximum%20accuracies%20were%2087%25%20with%20known%0Aspeakers%20and%2063%25%20in%20the%20validation%20with%20unknown%20speakers.%20The%20neural%20network%0Afor%20lip%20reading%2C%20which%20was%20first%20developed%20for%20the%20German%20language%2C%20shows%20a%0Avery%20high%20level%20of%20accuracy%2C%20comparable%20to%20English-language%20algorithms.%20It%0Aworks%20with%20unknown%20speakers%20as%20well%20and%20can%20be%20generalized%20with%20more%20word%0Aclasses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevelopment%2520and%2520evaluation%2520of%2520a%2520deep%2520learning%2520algorithm%2520for%2520German%2520word%250A%2520%2520recognition%2520from%2520lip%2520movements%26entry.906535625%3DDinh%2520Nam%2520Pham%2520and%2520Torsten%2520Rahne%26entry.1292438233%3D%2520%2520When%2520reading%2520lips%252C%2520many%2520people%2520benefit%2520from%2520additional%2520visual%2520information%250Afrom%2520the%2520lip%2520movements%2520of%2520the%2520speaker%252C%2520which%2520is%252C%2520however%252C%2520very%2520error%2520prone.%250AAlgorithms%2520for%2520lip%2520reading%2520with%2520artificial%2520intelligence%2520based%2520on%2520artificial%250Aneural%2520networks%2520significantly%2520improve%2520word%2520recognition%2520but%2520are%2520not%2520available%250Afor%2520the%2520German%2520language.%2520A%2520total%2520of%25201806%2520video%2520clips%2520with%2520only%2520one%250AGerman-speaking%2520person%2520each%2520were%2520selected%252C%2520split%2520into%2520word%2520segments%252C%2520and%250Aassigned%2520to%2520word%2520classes%2520using%2520speech-recognition%2520software.%2520In%252038%252C391%2520video%250Asegments%2520with%252032%2520speakers%252C%252018%2520polysyllabic%252C%2520visually%2520distinguishable%2520words%2520were%250Aused%2520to%2520train%2520and%2520validate%2520a%2520neural%2520network.%2520The%25203D%2520Convolutional%2520Neural%250ANetwork%2520and%2520Gated%2520Recurrent%2520Units%2520models%2520and%2520a%2520combination%2520of%2520both%2520models%250A%2528GRUConv%2529%2520were%2520compared%252C%2520as%2520were%2520different%2520image%2520sections%2520and%2520color%2520spaces%2520of%250Athe%2520videos.%2520The%2520accuracy%2520was%2520determined%2520in%25205000%2520training%2520epochs.%2520Comparison%2520of%250Athe%2520color%2520spaces%2520did%2520not%2520reveal%2520any%2520relevant%2520different%2520correct%2520classification%250Arates%2520in%2520the%2520range%2520from%252069%2525%2520to%252072%2525.%2520With%2520a%2520cut%2520to%2520the%2520lips%252C%2520a%2520significantly%250Ahigher%2520accuracy%2520of%252070%2525%2520was%2520achieved%2520than%2520when%2520cut%2520to%2520the%2520entire%2520speaker%2527s%2520face%250A%252834%2525%2529.%2520With%2520the%2520GRUConv%2520model%252C%2520the%2520maximum%2520accuracies%2520were%252087%2525%2520with%2520known%250Aspeakers%2520and%252063%2525%2520in%2520the%2520validation%2520with%2520unknown%2520speakers.%2520The%2520neural%2520network%250Afor%2520lip%2520reading%252C%2520which%2520was%2520first%2520developed%2520for%2520the%2520German%2520language%252C%2520shows%2520a%250Avery%2520high%2520level%2520of%2520accuracy%252C%2520comparable%2520to%2520English-language%2520algorithms.%2520It%250Aworks%2520with%2520unknown%2520speakers%2520as%2520well%2520and%2520can%2520be%2520generalized%2520with%2520more%2520word%250Aclasses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Development%20and%20evaluation%20of%20a%20deep%20learning%20algorithm%20for%20German%20word%0A%20%20recognition%20from%20lip%20movements&entry.906535625=Dinh%20Nam%20Pham%20and%20Torsten%20Rahne&entry.1292438233=%20%20When%20reading%20lips%2C%20many%20people%20benefit%20from%20additional%20visual%20information%0Afrom%20the%20lip%20movements%20of%20the%20speaker%2C%20which%20is%2C%20however%2C%20very%20error%20prone.%0AAlgorithms%20for%20lip%20reading%20with%20artificial%20intelligence%20based%20on%20artificial%0Aneural%20networks%20significantly%20improve%20word%20recognition%20but%20are%20not%20available%0Afor%20the%20German%20language.%20A%20total%20of%201806%20video%20clips%20with%20only%20one%0AGerman-speaking%20person%20each%20were%20selected%2C%20split%20into%20word%20segments%2C%20and%0Aassigned%20to%20word%20classes%20using%20speech-recognition%20software.%20In%2038%2C391%20video%0Asegments%20with%2032%20speakers%2C%2018%20polysyllabic%2C%20visually%20distinguishable%20words%20were%0Aused%20to%20train%20and%20validate%20a%20neural%20network.%20The%203D%20Convolutional%20Neural%0ANetwork%20and%20Gated%20Recurrent%20Units%20models%20and%20a%20combination%20of%20both%20models%0A%28GRUConv%29%20were%20compared%2C%20as%20were%20different%20image%20sections%20and%20color%20spaces%20of%0Athe%20videos.%20The%20accuracy%20was%20determined%20in%205000%20training%20epochs.%20Comparison%20of%0Athe%20color%20spaces%20did%20not%20reveal%20any%20relevant%20different%20correct%20classification%0Arates%20in%20the%20range%20from%2069%25%20to%2072%25.%20With%20a%20cut%20to%20the%20lips%2C%20a%20significantly%0Ahigher%20accuracy%20of%2070%25%20was%20achieved%20than%20when%20cut%20to%20the%20entire%20speaker%27s%20face%0A%2834%25%29.%20With%20the%20GRUConv%20model%2C%20the%20maximum%20accuracies%20were%2087%25%20with%20known%0Aspeakers%20and%2063%25%20in%20the%20validation%20with%20unknown%20speakers.%20The%20neural%20network%0Afor%20lip%20reading%2C%20which%20was%20first%20developed%20for%20the%20German%20language%2C%20shows%20a%0Avery%20high%20level%20of%20accuracy%2C%20comparable%20to%20English-language%20algorithms.%20It%0Aworks%20with%20unknown%20speakers%20as%20well%20and%20can%20be%20generalized%20with%20more%20word%0Aclasses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15792v1&entry.124074799=Read"},
{"title": "SUPRA: Subspace Parameterized Attention for Neural Operator on General\n  Domains", "author": "Zherui Yang and Zhengyang Xue and Ligang Liu", "abstract": "  Neural operators are efficient surrogate models for solving partial\ndifferential equations (PDEs), but their key components face challenges: (1) in\norder to improve accuracy, attention mechanisms suffer from computational\ninefficiency on large-scale meshes, and (2) spectral convolutions rely on the\nFast Fourier Transform (FFT) on regular grids and assume a flat geometry, which\ncauses accuracy degradation on irregular domains. To tackle these problems, we\nregard the matrix-vector operations in the standard attention mechanism on\nvectors in Euclidean space as bilinear forms and linear operators in vector\nspaces and generalize the attention mechanism to function spaces. This new\nattention mechanism is fully equivalent to the standard attention but\nimpossible to compute due to the infinite dimensionality of function spaces. To\naddress this, inspired by model reduction techniques, we propose a Subspace\nParameterized Attention (SUPRA) neural operator, which approximates the\nattention mechanism within a finite-dimensional subspace. To construct a\nsubspace on irregular domains for SUPRA, we propose using the Laplacian\neigenfunctions, which naturally adapt to domains' geometry and guarantee the\noptimal approximation for smooth functions. Experiments show that the SUPRA\nneural operator reduces error rates by up to 33% on various PDE datasets while\nmaintaining state-of-the-art computational efficiency.\n", "link": "http://arxiv.org/abs/2504.15897v1", "date": "2025-04-22", "relevancy": 2.4701, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4977}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4968}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUPRA%3A%20Subspace%20Parameterized%20Attention%20for%20Neural%20Operator%20on%20General%0A%20%20Domains&body=Title%3A%20SUPRA%3A%20Subspace%20Parameterized%20Attention%20for%20Neural%20Operator%20on%20General%0A%20%20Domains%0AAuthor%3A%20Zherui%20Yang%20and%20Zhengyang%20Xue%20and%20Ligang%20Liu%0AAbstract%3A%20%20%20Neural%20operators%20are%20efficient%20surrogate%20models%20for%20solving%20partial%0Adifferential%20equations%20%28PDEs%29%2C%20but%20their%20key%20components%20face%20challenges%3A%20%281%29%20in%0Aorder%20to%20improve%20accuracy%2C%20attention%20mechanisms%20suffer%20from%20computational%0Ainefficiency%20on%20large-scale%20meshes%2C%20and%20%282%29%20spectral%20convolutions%20rely%20on%20the%0AFast%20Fourier%20Transform%20%28FFT%29%20on%20regular%20grids%20and%20assume%20a%20flat%20geometry%2C%20which%0Acauses%20accuracy%20degradation%20on%20irregular%20domains.%20To%20tackle%20these%20problems%2C%20we%0Aregard%20the%20matrix-vector%20operations%20in%20the%20standard%20attention%20mechanism%20on%0Avectors%20in%20Euclidean%20space%20as%20bilinear%20forms%20and%20linear%20operators%20in%20vector%0Aspaces%20and%20generalize%20the%20attention%20mechanism%20to%20function%20spaces.%20This%20new%0Aattention%20mechanism%20is%20fully%20equivalent%20to%20the%20standard%20attention%20but%0Aimpossible%20to%20compute%20due%20to%20the%20infinite%20dimensionality%20of%20function%20spaces.%20To%0Aaddress%20this%2C%20inspired%20by%20model%20reduction%20techniques%2C%20we%20propose%20a%20Subspace%0AParameterized%20Attention%20%28SUPRA%29%20neural%20operator%2C%20which%20approximates%20the%0Aattention%20mechanism%20within%20a%20finite-dimensional%20subspace.%20To%20construct%20a%0Asubspace%20on%20irregular%20domains%20for%20SUPRA%2C%20we%20propose%20using%20the%20Laplacian%0Aeigenfunctions%2C%20which%20naturally%20adapt%20to%20domains%27%20geometry%20and%20guarantee%20the%0Aoptimal%20approximation%20for%20smooth%20functions.%20Experiments%20show%20that%20the%20SUPRA%0Aneural%20operator%20reduces%20error%20rates%20by%20up%20to%2033%25%20on%20various%20PDE%20datasets%20while%0Amaintaining%20state-of-the-art%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUPRA%253A%2520Subspace%2520Parameterized%2520Attention%2520for%2520Neural%2520Operator%2520on%2520General%250A%2520%2520Domains%26entry.906535625%3DZherui%2520Yang%2520and%2520Zhengyang%2520Xue%2520and%2520Ligang%2520Liu%26entry.1292438233%3D%2520%2520Neural%2520operators%2520are%2520efficient%2520surrogate%2520models%2520for%2520solving%2520partial%250Adifferential%2520equations%2520%2528PDEs%2529%252C%2520but%2520their%2520key%2520components%2520face%2520challenges%253A%2520%25281%2529%2520in%250Aorder%2520to%2520improve%2520accuracy%252C%2520attention%2520mechanisms%2520suffer%2520from%2520computational%250Ainefficiency%2520on%2520large-scale%2520meshes%252C%2520and%2520%25282%2529%2520spectral%2520convolutions%2520rely%2520on%2520the%250AFast%2520Fourier%2520Transform%2520%2528FFT%2529%2520on%2520regular%2520grids%2520and%2520assume%2520a%2520flat%2520geometry%252C%2520which%250Acauses%2520accuracy%2520degradation%2520on%2520irregular%2520domains.%2520To%2520tackle%2520these%2520problems%252C%2520we%250Aregard%2520the%2520matrix-vector%2520operations%2520in%2520the%2520standard%2520attention%2520mechanism%2520on%250Avectors%2520in%2520Euclidean%2520space%2520as%2520bilinear%2520forms%2520and%2520linear%2520operators%2520in%2520vector%250Aspaces%2520and%2520generalize%2520the%2520attention%2520mechanism%2520to%2520function%2520spaces.%2520This%2520new%250Aattention%2520mechanism%2520is%2520fully%2520equivalent%2520to%2520the%2520standard%2520attention%2520but%250Aimpossible%2520to%2520compute%2520due%2520to%2520the%2520infinite%2520dimensionality%2520of%2520function%2520spaces.%2520To%250Aaddress%2520this%252C%2520inspired%2520by%2520model%2520reduction%2520techniques%252C%2520we%2520propose%2520a%2520Subspace%250AParameterized%2520Attention%2520%2528SUPRA%2529%2520neural%2520operator%252C%2520which%2520approximates%2520the%250Aattention%2520mechanism%2520within%2520a%2520finite-dimensional%2520subspace.%2520To%2520construct%2520a%250Asubspace%2520on%2520irregular%2520domains%2520for%2520SUPRA%252C%2520we%2520propose%2520using%2520the%2520Laplacian%250Aeigenfunctions%252C%2520which%2520naturally%2520adapt%2520to%2520domains%2527%2520geometry%2520and%2520guarantee%2520the%250Aoptimal%2520approximation%2520for%2520smooth%2520functions.%2520Experiments%2520show%2520that%2520the%2520SUPRA%250Aneural%2520operator%2520reduces%2520error%2520rates%2520by%2520up%2520to%252033%2525%2520on%2520various%2520PDE%2520datasets%2520while%250Amaintaining%2520state-of-the-art%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUPRA%3A%20Subspace%20Parameterized%20Attention%20for%20Neural%20Operator%20on%20General%0A%20%20Domains&entry.906535625=Zherui%20Yang%20and%20Zhengyang%20Xue%20and%20Ligang%20Liu&entry.1292438233=%20%20Neural%20operators%20are%20efficient%20surrogate%20models%20for%20solving%20partial%0Adifferential%20equations%20%28PDEs%29%2C%20but%20their%20key%20components%20face%20challenges%3A%20%281%29%20in%0Aorder%20to%20improve%20accuracy%2C%20attention%20mechanisms%20suffer%20from%20computational%0Ainefficiency%20on%20large-scale%20meshes%2C%20and%20%282%29%20spectral%20convolutions%20rely%20on%20the%0AFast%20Fourier%20Transform%20%28FFT%29%20on%20regular%20grids%20and%20assume%20a%20flat%20geometry%2C%20which%0Acauses%20accuracy%20degradation%20on%20irregular%20domains.%20To%20tackle%20these%20problems%2C%20we%0Aregard%20the%20matrix-vector%20operations%20in%20the%20standard%20attention%20mechanism%20on%0Avectors%20in%20Euclidean%20space%20as%20bilinear%20forms%20and%20linear%20operators%20in%20vector%0Aspaces%20and%20generalize%20the%20attention%20mechanism%20to%20function%20spaces.%20This%20new%0Aattention%20mechanism%20is%20fully%20equivalent%20to%20the%20standard%20attention%20but%0Aimpossible%20to%20compute%20due%20to%20the%20infinite%20dimensionality%20of%20function%20spaces.%20To%0Aaddress%20this%2C%20inspired%20by%20model%20reduction%20techniques%2C%20we%20propose%20a%20Subspace%0AParameterized%20Attention%20%28SUPRA%29%20neural%20operator%2C%20which%20approximates%20the%0Aattention%20mechanism%20within%20a%20finite-dimensional%20subspace.%20To%20construct%20a%0Asubspace%20on%20irregular%20domains%20for%20SUPRA%2C%20we%20propose%20using%20the%20Laplacian%0Aeigenfunctions%2C%20which%20naturally%20adapt%20to%20domains%27%20geometry%20and%20guarantee%20the%0Aoptimal%20approximation%20for%20smooth%20functions.%20Experiments%20show%20that%20the%20SUPRA%0Aneural%20operator%20reduces%20error%20rates%20by%20up%20to%2033%25%20on%20various%20PDE%20datasets%20while%0Amaintaining%20state-of-the-art%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15897v1&entry.124074799=Read"},
{"title": "Survey of Video Diffusion Models: Foundations, Implementations, and\n  Applications", "author": "Yimu Wang and Xuye Liu and Wei Pang and Li Ma and Shuai Yuan and Paul Debevec and Ning Yu", "abstract": "  Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion.\n", "link": "http://arxiv.org/abs/2504.16081v1", "date": "2025-04-22", "relevancy": 2.4695, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6338}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6211}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20of%20Video%20Diffusion%20Models%3A%20Foundations%2C%20Implementations%2C%20and%0A%20%20Applications&body=Title%3A%20Survey%20of%20Video%20Diffusion%20Models%3A%20Foundations%2C%20Implementations%2C%20and%0A%20%20Applications%0AAuthor%3A%20Yimu%20Wang%20and%20Xuye%20Liu%20and%20Wei%20Pang%20and%20Li%20Ma%20and%20Shuai%20Yuan%20and%20Paul%20Debevec%20and%20Ning%20Yu%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20revolutionized%20video%20generation%2C%0Aoffering%20superior%20temporal%20consistency%20and%20visual%20quality%20compared%20to%0Atraditional%20generative%20adversarial%20networks-based%20approaches.%20While%20this%0Aemerging%20field%20shows%20tremendous%20promise%20in%20applications%2C%20it%20faces%20significant%0Achallenges%20in%20motion%20consistency%2C%20computational%20efficiency%2C%20and%20ethical%0Aconsiderations.%20This%20survey%20provides%20a%20comprehensive%20review%20of%20diffusion-based%0Avideo%20generation%2C%20examining%20its%20evolution%2C%20technical%20foundations%2C%20and%20practical%0Aapplications.%20We%20present%20a%20systematic%20taxonomy%20of%20current%20methodologies%2C%0Aanalyze%20architectural%20innovations%20and%20optimization%20strategies%2C%20and%20investigate%0Aapplications%20across%20low-level%20vision%20tasks%20such%20as%20denoising%20and%0Asuper-resolution.%20Additionally%2C%20we%20explore%20the%20synergies%20between%20diffusionbased%0Avideo%20generation%20and%20related%20domains%2C%20including%20video%20representation%20learning%2C%0Aquestion%20answering%2C%20and%20retrieval.%20Compared%20to%20the%20existing%20surveys%20%28Lei%20et%0Aal.%2C%202024a%3Bb%3B%20Melnik%20et%20al.%2C%202024%3B%20Cao%20et%20al.%2C%202023%3B%20Xing%20et%20al.%2C%202024c%29%20which%0Afocus%20on%20specific%20aspects%20of%20video%20generation%2C%20such%20as%20human%20video%20synthesis%0A%28Lei%20et%20al.%2C%202024a%29%20or%20long-form%20content%20generation%20%28Lei%20et%20al.%2C%202024b%29%2C%20our%0Awork%20provides%20a%20broader%2C%20more%20updated%2C%20and%20more%20fine-grained%20perspective%20on%0Adiffusion-based%20approaches%20with%20a%20special%20section%20for%20evaluation%20metrics%2C%0Aindustry%20solutions%2C%20and%20training%20engineering%20techniques%20in%20video%20generation.%0AThis%20survey%20serves%20as%20a%20foundational%20resource%20for%20researchers%20and%20practitioners%0Aworking%20at%20the%20intersection%20of%20diffusion%20models%20and%20video%20generation%2C%20providing%0Ainsights%20into%20both%20the%20theoretical%20frameworks%20and%20practical%20implementations%0Athat%20drive%20this%20rapidly%20evolving%20field.%20A%20structured%20list%20of%20related%20works%0Ainvolved%20in%20this%20survey%20is%20also%20available%20on%0Ahttps%3A//github.com/Eyeline-Research/Survey-Video-Diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520of%2520Video%2520Diffusion%2520Models%253A%2520Foundations%252C%2520Implementations%252C%2520and%250A%2520%2520Applications%26entry.906535625%3DYimu%2520Wang%2520and%2520Xuye%2520Liu%2520and%2520Wei%2520Pang%2520and%2520Li%2520Ma%2520and%2520Shuai%2520Yuan%2520and%2520Paul%2520Debevec%2520and%2520Ning%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520revolutionized%2520video%2520generation%252C%250Aoffering%2520superior%2520temporal%2520consistency%2520and%2520visual%2520quality%2520compared%2520to%250Atraditional%2520generative%2520adversarial%2520networks-based%2520approaches.%2520While%2520this%250Aemerging%2520field%2520shows%2520tremendous%2520promise%2520in%2520applications%252C%2520it%2520faces%2520significant%250Achallenges%2520in%2520motion%2520consistency%252C%2520computational%2520efficiency%252C%2520and%2520ethical%250Aconsiderations.%2520This%2520survey%2520provides%2520a%2520comprehensive%2520review%2520of%2520diffusion-based%250Avideo%2520generation%252C%2520examining%2520its%2520evolution%252C%2520technical%2520foundations%252C%2520and%2520practical%250Aapplications.%2520We%2520present%2520a%2520systematic%2520taxonomy%2520of%2520current%2520methodologies%252C%250Aanalyze%2520architectural%2520innovations%2520and%2520optimization%2520strategies%252C%2520and%2520investigate%250Aapplications%2520across%2520low-level%2520vision%2520tasks%2520such%2520as%2520denoising%2520and%250Asuper-resolution.%2520Additionally%252C%2520we%2520explore%2520the%2520synergies%2520between%2520diffusionbased%250Avideo%2520generation%2520and%2520related%2520domains%252C%2520including%2520video%2520representation%2520learning%252C%250Aquestion%2520answering%252C%2520and%2520retrieval.%2520Compared%2520to%2520the%2520existing%2520surveys%2520%2528Lei%2520et%250Aal.%252C%25202024a%253Bb%253B%2520Melnik%2520et%2520al.%252C%25202024%253B%2520Cao%2520et%2520al.%252C%25202023%253B%2520Xing%2520et%2520al.%252C%25202024c%2529%2520which%250Afocus%2520on%2520specific%2520aspects%2520of%2520video%2520generation%252C%2520such%2520as%2520human%2520video%2520synthesis%250A%2528Lei%2520et%2520al.%252C%25202024a%2529%2520or%2520long-form%2520content%2520generation%2520%2528Lei%2520et%2520al.%252C%25202024b%2529%252C%2520our%250Awork%2520provides%2520a%2520broader%252C%2520more%2520updated%252C%2520and%2520more%2520fine-grained%2520perspective%2520on%250Adiffusion-based%2520approaches%2520with%2520a%2520special%2520section%2520for%2520evaluation%2520metrics%252C%250Aindustry%2520solutions%252C%2520and%2520training%2520engineering%2520techniques%2520in%2520video%2520generation.%250AThis%2520survey%2520serves%2520as%2520a%2520foundational%2520resource%2520for%2520researchers%2520and%2520practitioners%250Aworking%2520at%2520the%2520intersection%2520of%2520diffusion%2520models%2520and%2520video%2520generation%252C%2520providing%250Ainsights%2520into%2520both%2520the%2520theoretical%2520frameworks%2520and%2520practical%2520implementations%250Athat%2520drive%2520this%2520rapidly%2520evolving%2520field.%2520A%2520structured%2520list%2520of%2520related%2520works%250Ainvolved%2520in%2520this%2520survey%2520is%2520also%2520available%2520on%250Ahttps%253A//github.com/Eyeline-Research/Survey-Video-Diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20of%20Video%20Diffusion%20Models%3A%20Foundations%2C%20Implementations%2C%20and%0A%20%20Applications&entry.906535625=Yimu%20Wang%20and%20Xuye%20Liu%20and%20Wei%20Pang%20and%20Li%20Ma%20and%20Shuai%20Yuan%20and%20Paul%20Debevec%20and%20Ning%20Yu&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20revolutionized%20video%20generation%2C%0Aoffering%20superior%20temporal%20consistency%20and%20visual%20quality%20compared%20to%0Atraditional%20generative%20adversarial%20networks-based%20approaches.%20While%20this%0Aemerging%20field%20shows%20tremendous%20promise%20in%20applications%2C%20it%20faces%20significant%0Achallenges%20in%20motion%20consistency%2C%20computational%20efficiency%2C%20and%20ethical%0Aconsiderations.%20This%20survey%20provides%20a%20comprehensive%20review%20of%20diffusion-based%0Avideo%20generation%2C%20examining%20its%20evolution%2C%20technical%20foundations%2C%20and%20practical%0Aapplications.%20We%20present%20a%20systematic%20taxonomy%20of%20current%20methodologies%2C%0Aanalyze%20architectural%20innovations%20and%20optimization%20strategies%2C%20and%20investigate%0Aapplications%20across%20low-level%20vision%20tasks%20such%20as%20denoising%20and%0Asuper-resolution.%20Additionally%2C%20we%20explore%20the%20synergies%20between%20diffusionbased%0Avideo%20generation%20and%20related%20domains%2C%20including%20video%20representation%20learning%2C%0Aquestion%20answering%2C%20and%20retrieval.%20Compared%20to%20the%20existing%20surveys%20%28Lei%20et%0Aal.%2C%202024a%3Bb%3B%20Melnik%20et%20al.%2C%202024%3B%20Cao%20et%20al.%2C%202023%3B%20Xing%20et%20al.%2C%202024c%29%20which%0Afocus%20on%20specific%20aspects%20of%20video%20generation%2C%20such%20as%20human%20video%20synthesis%0A%28Lei%20et%20al.%2C%202024a%29%20or%20long-form%20content%20generation%20%28Lei%20et%20al.%2C%202024b%29%2C%20our%0Awork%20provides%20a%20broader%2C%20more%20updated%2C%20and%20more%20fine-grained%20perspective%20on%0Adiffusion-based%20approaches%20with%20a%20special%20section%20for%20evaluation%20metrics%2C%0Aindustry%20solutions%2C%20and%20training%20engineering%20techniques%20in%20video%20generation.%0AThis%20survey%20serves%20as%20a%20foundational%20resource%20for%20researchers%20and%20practitioners%0Aworking%20at%20the%20intersection%20of%20diffusion%20models%20and%20video%20generation%2C%20providing%0Ainsights%20into%20both%20the%20theoretical%20frameworks%20and%20practical%20implementations%0Athat%20drive%20this%20rapidly%20evolving%20field.%20A%20structured%20list%20of%20related%20works%0Ainvolved%20in%20this%20survey%20is%20also%20available%20on%0Ahttps%3A//github.com/Eyeline-Research/Survey-Video-Diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16081v1&entry.124074799=Read"},
{"title": "DERD-Net: Learning Depth from Event-based Ray Densities", "author": "Diego de Oliveira Hitzges and Suman Ghosh and Guillermo Gallego", "abstract": "  Event cameras offer a promising avenue for multi-view stereo depth estimation\nand Simultaneous Localization And Mapping (SLAM) due to their ability to detect\nblur-free 3D edges at high-speed and over broad illumination conditions.\nHowever, traditional deep learning frameworks designed for conventional cameras\nstruggle with the asynchronous, stream-like nature of event data, as their\narchitectures are optimized for discrete, image-like inputs. We propose a\nscalable, flexible and adaptable framework for pixel-wise depth estimation with\nevent cameras in both monocular and stereo setups. The 3D scene structure is\nencoded into disparity space images (DSIs), representing spatial densities of\nrays obtained by back-projecting events into space via known camera poses. Our\nneural network processes local subregions of the DSIs combining 3D convolutions\nand a recurrent structure to recognize valuable patterns for depth prediction.\nLocal processing enables fast inference with full parallelization and ensures\nconstant ultra-low model complexity and memory costs, regardless of camera\nresolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)\ndemonstrate unprecedented effectiveness: (i) using purely monocular data, our\nmethod achieves comparable results to existing stereo methods; (ii) when\napplied to stereo data, it strongly outperforms all state-of-the-art (SOTA)\napproaches, reducing the mean absolute error by at least 42%; (iii) our method\nalso allows for increases in depth completeness by more than 3-fold while still\nyielding a reduction in median absolute error of at least 30%. Given its\nremarkable performance and effective processing of event-data, our framework\nholds strong potential to become a standard approach for using deep learning\nfor event-based depth estimation and SLAM. Project page:\nhttps://github.com/tub-rip/DERD-Net\n", "link": "http://arxiv.org/abs/2504.15863v1", "date": "2025-04-22", "relevancy": 2.4657, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6204}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6174}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DERD-Net%3A%20Learning%20Depth%20from%20Event-based%20Ray%20Densities&body=Title%3A%20DERD-Net%3A%20Learning%20Depth%20from%20Event-based%20Ray%20Densities%0AAuthor%3A%20Diego%20de%20Oliveira%20Hitzges%20and%20Suman%20Ghosh%20and%20Guillermo%20Gallego%0AAbstract%3A%20%20%20Event%20cameras%20offer%20a%20promising%20avenue%20for%20multi-view%20stereo%20depth%20estimation%0Aand%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29%20due%20to%20their%20ability%20to%20detect%0Ablur-free%203D%20edges%20at%20high-speed%20and%20over%20broad%20illumination%20conditions.%0AHowever%2C%20traditional%20deep%20learning%20frameworks%20designed%20for%20conventional%20cameras%0Astruggle%20with%20the%20asynchronous%2C%20stream-like%20nature%20of%20event%20data%2C%20as%20their%0Aarchitectures%20are%20optimized%20for%20discrete%2C%20image-like%20inputs.%20We%20propose%20a%0Ascalable%2C%20flexible%20and%20adaptable%20framework%20for%20pixel-wise%20depth%20estimation%20with%0Aevent%20cameras%20in%20both%20monocular%20and%20stereo%20setups.%20The%203D%20scene%20structure%20is%0Aencoded%20into%20disparity%20space%20images%20%28DSIs%29%2C%20representing%20spatial%20densities%20of%0Arays%20obtained%20by%20back-projecting%20events%20into%20space%20via%20known%20camera%20poses.%20Our%0Aneural%20network%20processes%20local%20subregions%20of%20the%20DSIs%20combining%203D%20convolutions%0Aand%20a%20recurrent%20structure%20to%20recognize%20valuable%20patterns%20for%20depth%20prediction.%0ALocal%20processing%20enables%20fast%20inference%20with%20full%20parallelization%20and%20ensures%0Aconstant%20ultra-low%20model%20complexity%20and%20memory%20costs%2C%20regardless%20of%20camera%0Aresolution.%20Experiments%20on%20standard%20benchmarks%20%28MVSEC%20and%20DSEC%20datasets%29%0Ademonstrate%20unprecedented%20effectiveness%3A%20%28i%29%20using%20purely%20monocular%20data%2C%20our%0Amethod%20achieves%20comparable%20results%20to%20existing%20stereo%20methods%3B%20%28ii%29%20when%0Aapplied%20to%20stereo%20data%2C%20it%20strongly%20outperforms%20all%20state-of-the-art%20%28SOTA%29%0Aapproaches%2C%20reducing%20the%20mean%20absolute%20error%20by%20at%20least%2042%25%3B%20%28iii%29%20our%20method%0Aalso%20allows%20for%20increases%20in%20depth%20completeness%20by%20more%20than%203-fold%20while%20still%0Ayielding%20a%20reduction%20in%20median%20absolute%20error%20of%20at%20least%2030%25.%20Given%20its%0Aremarkable%20performance%20and%20effective%20processing%20of%20event-data%2C%20our%20framework%0Aholds%20strong%20potential%20to%20become%20a%20standard%20approach%20for%20using%20deep%20learning%0Afor%20event-based%20depth%20estimation%20and%20SLAM.%20Project%20page%3A%0Ahttps%3A//github.com/tub-rip/DERD-Net%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDERD-Net%253A%2520Learning%2520Depth%2520from%2520Event-based%2520Ray%2520Densities%26entry.906535625%3DDiego%2520de%2520Oliveira%2520Hitzges%2520and%2520Suman%2520Ghosh%2520and%2520Guillermo%2520Gallego%26entry.1292438233%3D%2520%2520Event%2520cameras%2520offer%2520a%2520promising%2520avenue%2520for%2520multi-view%2520stereo%2520depth%2520estimation%250Aand%2520Simultaneous%2520Localization%2520And%2520Mapping%2520%2528SLAM%2529%2520due%2520to%2520their%2520ability%2520to%2520detect%250Ablur-free%25203D%2520edges%2520at%2520high-speed%2520and%2520over%2520broad%2520illumination%2520conditions.%250AHowever%252C%2520traditional%2520deep%2520learning%2520frameworks%2520designed%2520for%2520conventional%2520cameras%250Astruggle%2520with%2520the%2520asynchronous%252C%2520stream-like%2520nature%2520of%2520event%2520data%252C%2520as%2520their%250Aarchitectures%2520are%2520optimized%2520for%2520discrete%252C%2520image-like%2520inputs.%2520We%2520propose%2520a%250Ascalable%252C%2520flexible%2520and%2520adaptable%2520framework%2520for%2520pixel-wise%2520depth%2520estimation%2520with%250Aevent%2520cameras%2520in%2520both%2520monocular%2520and%2520stereo%2520setups.%2520The%25203D%2520scene%2520structure%2520is%250Aencoded%2520into%2520disparity%2520space%2520images%2520%2528DSIs%2529%252C%2520representing%2520spatial%2520densities%2520of%250Arays%2520obtained%2520by%2520back-projecting%2520events%2520into%2520space%2520via%2520known%2520camera%2520poses.%2520Our%250Aneural%2520network%2520processes%2520local%2520subregions%2520of%2520the%2520DSIs%2520combining%25203D%2520convolutions%250Aand%2520a%2520recurrent%2520structure%2520to%2520recognize%2520valuable%2520patterns%2520for%2520depth%2520prediction.%250ALocal%2520processing%2520enables%2520fast%2520inference%2520with%2520full%2520parallelization%2520and%2520ensures%250Aconstant%2520ultra-low%2520model%2520complexity%2520and%2520memory%2520costs%252C%2520regardless%2520of%2520camera%250Aresolution.%2520Experiments%2520on%2520standard%2520benchmarks%2520%2528MVSEC%2520and%2520DSEC%2520datasets%2529%250Ademonstrate%2520unprecedented%2520effectiveness%253A%2520%2528i%2529%2520using%2520purely%2520monocular%2520data%252C%2520our%250Amethod%2520achieves%2520comparable%2520results%2520to%2520existing%2520stereo%2520methods%253B%2520%2528ii%2529%2520when%250Aapplied%2520to%2520stereo%2520data%252C%2520it%2520strongly%2520outperforms%2520all%2520state-of-the-art%2520%2528SOTA%2529%250Aapproaches%252C%2520reducing%2520the%2520mean%2520absolute%2520error%2520by%2520at%2520least%252042%2525%253B%2520%2528iii%2529%2520our%2520method%250Aalso%2520allows%2520for%2520increases%2520in%2520depth%2520completeness%2520by%2520more%2520than%25203-fold%2520while%2520still%250Ayielding%2520a%2520reduction%2520in%2520median%2520absolute%2520error%2520of%2520at%2520least%252030%2525.%2520Given%2520its%250Aremarkable%2520performance%2520and%2520effective%2520processing%2520of%2520event-data%252C%2520our%2520framework%250Aholds%2520strong%2520potential%2520to%2520become%2520a%2520standard%2520approach%2520for%2520using%2520deep%2520learning%250Afor%2520event-based%2520depth%2520estimation%2520and%2520SLAM.%2520Project%2520page%253A%250Ahttps%253A//github.com/tub-rip/DERD-Net%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DERD-Net%3A%20Learning%20Depth%20from%20Event-based%20Ray%20Densities&entry.906535625=Diego%20de%20Oliveira%20Hitzges%20and%20Suman%20Ghosh%20and%20Guillermo%20Gallego&entry.1292438233=%20%20Event%20cameras%20offer%20a%20promising%20avenue%20for%20multi-view%20stereo%20depth%20estimation%0Aand%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29%20due%20to%20their%20ability%20to%20detect%0Ablur-free%203D%20edges%20at%20high-speed%20and%20over%20broad%20illumination%20conditions.%0AHowever%2C%20traditional%20deep%20learning%20frameworks%20designed%20for%20conventional%20cameras%0Astruggle%20with%20the%20asynchronous%2C%20stream-like%20nature%20of%20event%20data%2C%20as%20their%0Aarchitectures%20are%20optimized%20for%20discrete%2C%20image-like%20inputs.%20We%20propose%20a%0Ascalable%2C%20flexible%20and%20adaptable%20framework%20for%20pixel-wise%20depth%20estimation%20with%0Aevent%20cameras%20in%20both%20monocular%20and%20stereo%20setups.%20The%203D%20scene%20structure%20is%0Aencoded%20into%20disparity%20space%20images%20%28DSIs%29%2C%20representing%20spatial%20densities%20of%0Arays%20obtained%20by%20back-projecting%20events%20into%20space%20via%20known%20camera%20poses.%20Our%0Aneural%20network%20processes%20local%20subregions%20of%20the%20DSIs%20combining%203D%20convolutions%0Aand%20a%20recurrent%20structure%20to%20recognize%20valuable%20patterns%20for%20depth%20prediction.%0ALocal%20processing%20enables%20fast%20inference%20with%20full%20parallelization%20and%20ensures%0Aconstant%20ultra-low%20model%20complexity%20and%20memory%20costs%2C%20regardless%20of%20camera%0Aresolution.%20Experiments%20on%20standard%20benchmarks%20%28MVSEC%20and%20DSEC%20datasets%29%0Ademonstrate%20unprecedented%20effectiveness%3A%20%28i%29%20using%20purely%20monocular%20data%2C%20our%0Amethod%20achieves%20comparable%20results%20to%20existing%20stereo%20methods%3B%20%28ii%29%20when%0Aapplied%20to%20stereo%20data%2C%20it%20strongly%20outperforms%20all%20state-of-the-art%20%28SOTA%29%0Aapproaches%2C%20reducing%20the%20mean%20absolute%20error%20by%20at%20least%2042%25%3B%20%28iii%29%20our%20method%0Aalso%20allows%20for%20increases%20in%20depth%20completeness%20by%20more%20than%203-fold%20while%20still%0Ayielding%20a%20reduction%20in%20median%20absolute%20error%20of%20at%20least%2030%25.%20Given%20its%0Aremarkable%20performance%20and%20effective%20processing%20of%20event-data%2C%20our%20framework%0Aholds%20strong%20potential%20to%20become%20a%20standard%20approach%20for%20using%20deep%20learning%0Afor%20event-based%20depth%20estimation%20and%20SLAM.%20Project%20page%3A%0Ahttps%3A//github.com/tub-rip/DERD-Net%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15863v1&entry.124074799=Read"},
{"title": "Selective Task Group Updates for Multi-Task Optimization", "author": "Wooseong Jeong and Kuk-Jin Yoon", "abstract": "  Multi-task learning enables the acquisition of task-generic knowledge by\ntraining multiple tasks within a unified architecture. However, training all\ntasks together in a single architecture can lead to performance degradation,\nknown as negative transfer, which is a main concern in multi-task learning.\nPrevious works have addressed this issue by optimizing the multi-task network\nthrough gradient manipulation or weighted loss adjustments. However, their\noptimization strategy focuses on addressing task imbalance in shared\nparameters, neglecting the learning of task-specific parameters. As a result,\nthey show limitations in mitigating negative transfer, since the learning of\nshared space and task-specific information influences each other during\noptimization. To address this, we propose a different approach to enhance\nmulti-task performance by selectively grouping tasks and updating them for each\nbatch during optimization. We introduce an algorithm that adaptively determines\nhow to effectively group tasks and update them during the learning process. To\ntrack inter-task relations and optimize multi-task networks simultaneously, we\npropose proximal inter-task affinity, which can be measured during the\noptimization process. We provide a theoretical analysis on how dividing tasks\ninto multiple groups and updating them sequentially significantly affects\nmulti-task performance by enhancing the learning of task-specific parameters.\nOur methods substantially outperform previous multi-task optimization\napproaches and are scalable to different architectures and various numbers of\ntasks.\n", "link": "http://arxiv.org/abs/2502.11986v2", "date": "2025-04-22", "relevancy": 2.4612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4896}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Task%20Group%20Updates%20for%20Multi-Task%20Optimization&body=Title%3A%20Selective%20Task%20Group%20Updates%20for%20Multi-Task%20Optimization%0AAuthor%3A%20Wooseong%20Jeong%20and%20Kuk-Jin%20Yoon%0AAbstract%3A%20%20%20Multi-task%20learning%20enables%20the%20acquisition%20of%20task-generic%20knowledge%20by%0Atraining%20multiple%20tasks%20within%20a%20unified%20architecture.%20However%2C%20training%20all%0Atasks%20together%20in%20a%20single%20architecture%20can%20lead%20to%20performance%20degradation%2C%0Aknown%20as%20negative%20transfer%2C%20which%20is%20a%20main%20concern%20in%20multi-task%20learning.%0APrevious%20works%20have%20addressed%20this%20issue%20by%20optimizing%20the%20multi-task%20network%0Athrough%20gradient%20manipulation%20or%20weighted%20loss%20adjustments.%20However%2C%20their%0Aoptimization%20strategy%20focuses%20on%20addressing%20task%20imbalance%20in%20shared%0Aparameters%2C%20neglecting%20the%20learning%20of%20task-specific%20parameters.%20As%20a%20result%2C%0Athey%20show%20limitations%20in%20mitigating%20negative%20transfer%2C%20since%20the%20learning%20of%0Ashared%20space%20and%20task-specific%20information%20influences%20each%20other%20during%0Aoptimization.%20To%20address%20this%2C%20we%20propose%20a%20different%20approach%20to%20enhance%0Amulti-task%20performance%20by%20selectively%20grouping%20tasks%20and%20updating%20them%20for%20each%0Abatch%20during%20optimization.%20We%20introduce%20an%20algorithm%20that%20adaptively%20determines%0Ahow%20to%20effectively%20group%20tasks%20and%20update%20them%20during%20the%20learning%20process.%20To%0Atrack%20inter-task%20relations%20and%20optimize%20multi-task%20networks%20simultaneously%2C%20we%0Apropose%20proximal%20inter-task%20affinity%2C%20which%20can%20be%20measured%20during%20the%0Aoptimization%20process.%20We%20provide%20a%20theoretical%20analysis%20on%20how%20dividing%20tasks%0Ainto%20multiple%20groups%20and%20updating%20them%20sequentially%20significantly%20affects%0Amulti-task%20performance%20by%20enhancing%20the%20learning%20of%20task-specific%20parameters.%0AOur%20methods%20substantially%20outperform%20previous%20multi-task%20optimization%0Aapproaches%20and%20are%20scalable%20to%20different%20architectures%20and%20various%20numbers%20of%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11986v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Task%2520Group%2520Updates%2520for%2520Multi-Task%2520Optimization%26entry.906535625%3DWooseong%2520Jeong%2520and%2520Kuk-Jin%2520Yoon%26entry.1292438233%3D%2520%2520Multi-task%2520learning%2520enables%2520the%2520acquisition%2520of%2520task-generic%2520knowledge%2520by%250Atraining%2520multiple%2520tasks%2520within%2520a%2520unified%2520architecture.%2520However%252C%2520training%2520all%250Atasks%2520together%2520in%2520a%2520single%2520architecture%2520can%2520lead%2520to%2520performance%2520degradation%252C%250Aknown%2520as%2520negative%2520transfer%252C%2520which%2520is%2520a%2520main%2520concern%2520in%2520multi-task%2520learning.%250APrevious%2520works%2520have%2520addressed%2520this%2520issue%2520by%2520optimizing%2520the%2520multi-task%2520network%250Athrough%2520gradient%2520manipulation%2520or%2520weighted%2520loss%2520adjustments.%2520However%252C%2520their%250Aoptimization%2520strategy%2520focuses%2520on%2520addressing%2520task%2520imbalance%2520in%2520shared%250Aparameters%252C%2520neglecting%2520the%2520learning%2520of%2520task-specific%2520parameters.%2520As%2520a%2520result%252C%250Athey%2520show%2520limitations%2520in%2520mitigating%2520negative%2520transfer%252C%2520since%2520the%2520learning%2520of%250Ashared%2520space%2520and%2520task-specific%2520information%2520influences%2520each%2520other%2520during%250Aoptimization.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520different%2520approach%2520to%2520enhance%250Amulti-task%2520performance%2520by%2520selectively%2520grouping%2520tasks%2520and%2520updating%2520them%2520for%2520each%250Abatch%2520during%2520optimization.%2520We%2520introduce%2520an%2520algorithm%2520that%2520adaptively%2520determines%250Ahow%2520to%2520effectively%2520group%2520tasks%2520and%2520update%2520them%2520during%2520the%2520learning%2520process.%2520To%250Atrack%2520inter-task%2520relations%2520and%2520optimize%2520multi-task%2520networks%2520simultaneously%252C%2520we%250Apropose%2520proximal%2520inter-task%2520affinity%252C%2520which%2520can%2520be%2520measured%2520during%2520the%250Aoptimization%2520process.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520on%2520how%2520dividing%2520tasks%250Ainto%2520multiple%2520groups%2520and%2520updating%2520them%2520sequentially%2520significantly%2520affects%250Amulti-task%2520performance%2520by%2520enhancing%2520the%2520learning%2520of%2520task-specific%2520parameters.%250AOur%2520methods%2520substantially%2520outperform%2520previous%2520multi-task%2520optimization%250Aapproaches%2520and%2520are%2520scalable%2520to%2520different%2520architectures%2520and%2520various%2520numbers%2520of%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11986v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Task%20Group%20Updates%20for%20Multi-Task%20Optimization&entry.906535625=Wooseong%20Jeong%20and%20Kuk-Jin%20Yoon&entry.1292438233=%20%20Multi-task%20learning%20enables%20the%20acquisition%20of%20task-generic%20knowledge%20by%0Atraining%20multiple%20tasks%20within%20a%20unified%20architecture.%20However%2C%20training%20all%0Atasks%20together%20in%20a%20single%20architecture%20can%20lead%20to%20performance%20degradation%2C%0Aknown%20as%20negative%20transfer%2C%20which%20is%20a%20main%20concern%20in%20multi-task%20learning.%0APrevious%20works%20have%20addressed%20this%20issue%20by%20optimizing%20the%20multi-task%20network%0Athrough%20gradient%20manipulation%20or%20weighted%20loss%20adjustments.%20However%2C%20their%0Aoptimization%20strategy%20focuses%20on%20addressing%20task%20imbalance%20in%20shared%0Aparameters%2C%20neglecting%20the%20learning%20of%20task-specific%20parameters.%20As%20a%20result%2C%0Athey%20show%20limitations%20in%20mitigating%20negative%20transfer%2C%20since%20the%20learning%20of%0Ashared%20space%20and%20task-specific%20information%20influences%20each%20other%20during%0Aoptimization.%20To%20address%20this%2C%20we%20propose%20a%20different%20approach%20to%20enhance%0Amulti-task%20performance%20by%20selectively%20grouping%20tasks%20and%20updating%20them%20for%20each%0Abatch%20during%20optimization.%20We%20introduce%20an%20algorithm%20that%20adaptively%20determines%0Ahow%20to%20effectively%20group%20tasks%20and%20update%20them%20during%20the%20learning%20process.%20To%0Atrack%20inter-task%20relations%20and%20optimize%20multi-task%20networks%20simultaneously%2C%20we%0Apropose%20proximal%20inter-task%20affinity%2C%20which%20can%20be%20measured%20during%20the%0Aoptimization%20process.%20We%20provide%20a%20theoretical%20analysis%20on%20how%20dividing%20tasks%0Ainto%20multiple%20groups%20and%20updating%20them%20sequentially%20significantly%20affects%0Amulti-task%20performance%20by%20enhancing%20the%20learning%20of%20task-specific%20parameters.%0AOur%20methods%20substantially%20outperform%20previous%20multi-task%20optimization%0Aapproaches%20and%20are%20scalable%20to%20different%20architectures%20and%20various%20numbers%20of%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11986v2&entry.124074799=Read"},
{"title": "Insights from Verification: Training a Verilog Generation LLM with\n  Reinforcement Learning with Testbench Feedback", "author": "Ning Wang and Bingkun Yao and Jie Zhou and Yuchen Hu and Xi Wang and Nan Guan and Zhe Jiang", "abstract": "  Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B.\n", "link": "http://arxiv.org/abs/2504.15804v1", "date": "2025-04-22", "relevancy": 2.452, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insights%20from%20Verification%3A%20Training%20a%20Verilog%20Generation%20LLM%20with%0A%20%20Reinforcement%20Learning%20with%20Testbench%20Feedback&body=Title%3A%20Insights%20from%20Verification%3A%20Training%20a%20Verilog%20Generation%20LLM%20with%0A%20%20Reinforcement%20Learning%20with%20Testbench%20Feedback%0AAuthor%3A%20Ning%20Wang%20and%20Bingkun%20Yao%20and%20Jie%20Zhou%20and%20Yuchen%20Hu%20and%20Xi%20Wang%20and%20Nan%20Guan%20and%20Zhe%20Jiang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20strong%20performance%20in%20Verilog%0Ageneration%20from%20natural%20language%20description.%20However%2C%20ensuring%20the%20functional%0Acorrectness%20of%20the%20generated%20code%20remains%20a%20significant%20challenge.%20This%20paper%0Aintroduces%20a%20method%20that%20integrates%20verification%20insights%20from%20testbench%20into%0Athe%20training%20of%20Verilog%20generation%20LLMs%2C%20aligning%20the%20training%20with%20the%0Afundamental%20goal%20of%20hardware%20design%3A%20functional%20correctness.%20The%20main%20obstacle%0Ain%20using%20LLMs%20for%20Verilog%20code%20generation%20is%20the%20lack%20of%20sufficient%20functional%0Averification%20data%2C%20particularly%20testbenches%20paired%20with%20design%20specifications%0Aand%20code.%20To%20address%20this%20problem%2C%20we%20introduce%20an%20automatic%20testbench%0Ageneration%20pipeline%20that%20decomposes%20the%20process%20and%20uses%20feedback%20from%20the%0AVerilog%20compiler%20simulator%20%28VCS%29%20to%20reduce%20hallucination%20and%20ensure%0Acorrectness.%20We%20then%20use%20the%20testbench%20to%20evaluate%20the%20generated%20codes%20and%0Acollect%20them%20for%20further%20training%2C%20where%20verification%20insights%20are%20introduced.%0AOur%20method%20applies%20reinforcement%20learning%20%28RL%29%2C%20specifically%20direct%20preference%0Aoptimization%20%28DPO%29%2C%20to%20align%20Verilog%20code%20generation%20with%20functional%0Acorrectness%20by%20training%20preference%20pairs%20based%20on%20testbench%20outcomes.%20In%0Aevaluations%20on%20VerilogEval-Machine%2C%20VerilogEval-Human%2C%20RTLLM%20v1.1%2C%20RTLLM%20v2%2C%0Aand%20VerilogEval%20v2%2C%20our%20approach%20consistently%20outperforms%20state-of-the-art%0Abaselines%20in%20generating%20functionally%20correct%20Verilog%20code.%20We%20open%20source%20all%0Atraining%20code%2C%20data%2C%20and%20models%20at%0Ahttps%3A//anonymous.4open.science/r/VeriPrefer-E88B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsights%2520from%2520Verification%253A%2520Training%2520a%2520Verilog%2520Generation%2520LLM%2520with%250A%2520%2520Reinforcement%2520Learning%2520with%2520Testbench%2520Feedback%26entry.906535625%3DNing%2520Wang%2520and%2520Bingkun%2520Yao%2520and%2520Jie%2520Zhou%2520and%2520Yuchen%2520Hu%2520and%2520Xi%2520Wang%2520and%2520Nan%2520Guan%2520and%2520Zhe%2520Jiang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520strong%2520performance%2520in%2520Verilog%250Ageneration%2520from%2520natural%2520language%2520description.%2520However%252C%2520ensuring%2520the%2520functional%250Acorrectness%2520of%2520the%2520generated%2520code%2520remains%2520a%2520significant%2520challenge.%2520This%2520paper%250Aintroduces%2520a%2520method%2520that%2520integrates%2520verification%2520insights%2520from%2520testbench%2520into%250Athe%2520training%2520of%2520Verilog%2520generation%2520LLMs%252C%2520aligning%2520the%2520training%2520with%2520the%250Afundamental%2520goal%2520of%2520hardware%2520design%253A%2520functional%2520correctness.%2520The%2520main%2520obstacle%250Ain%2520using%2520LLMs%2520for%2520Verilog%2520code%2520generation%2520is%2520the%2520lack%2520of%2520sufficient%2520functional%250Averification%2520data%252C%2520particularly%2520testbenches%2520paired%2520with%2520design%2520specifications%250Aand%2520code.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520an%2520automatic%2520testbench%250Ageneration%2520pipeline%2520that%2520decomposes%2520the%2520process%2520and%2520uses%2520feedback%2520from%2520the%250AVerilog%2520compiler%2520simulator%2520%2528VCS%2529%2520to%2520reduce%2520hallucination%2520and%2520ensure%250Acorrectness.%2520We%2520then%2520use%2520the%2520testbench%2520to%2520evaluate%2520the%2520generated%2520codes%2520and%250Acollect%2520them%2520for%2520further%2520training%252C%2520where%2520verification%2520insights%2520are%2520introduced.%250AOur%2520method%2520applies%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520specifically%2520direct%2520preference%250Aoptimization%2520%2528DPO%2529%252C%2520to%2520align%2520Verilog%2520code%2520generation%2520with%2520functional%250Acorrectness%2520by%2520training%2520preference%2520pairs%2520based%2520on%2520testbench%2520outcomes.%2520In%250Aevaluations%2520on%2520VerilogEval-Machine%252C%2520VerilogEval-Human%252C%2520RTLLM%2520v1.1%252C%2520RTLLM%2520v2%252C%250Aand%2520VerilogEval%2520v2%252C%2520our%2520approach%2520consistently%2520outperforms%2520state-of-the-art%250Abaselines%2520in%2520generating%2520functionally%2520correct%2520Verilog%2520code.%2520We%2520open%2520source%2520all%250Atraining%2520code%252C%2520data%252C%2520and%2520models%2520at%250Ahttps%253A//anonymous.4open.science/r/VeriPrefer-E88B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insights%20from%20Verification%3A%20Training%20a%20Verilog%20Generation%20LLM%20with%0A%20%20Reinforcement%20Learning%20with%20Testbench%20Feedback&entry.906535625=Ning%20Wang%20and%20Bingkun%20Yao%20and%20Jie%20Zhou%20and%20Yuchen%20Hu%20and%20Xi%20Wang%20and%20Nan%20Guan%20and%20Zhe%20Jiang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20strong%20performance%20in%20Verilog%0Ageneration%20from%20natural%20language%20description.%20However%2C%20ensuring%20the%20functional%0Acorrectness%20of%20the%20generated%20code%20remains%20a%20significant%20challenge.%20This%20paper%0Aintroduces%20a%20method%20that%20integrates%20verification%20insights%20from%20testbench%20into%0Athe%20training%20of%20Verilog%20generation%20LLMs%2C%20aligning%20the%20training%20with%20the%0Afundamental%20goal%20of%20hardware%20design%3A%20functional%20correctness.%20The%20main%20obstacle%0Ain%20using%20LLMs%20for%20Verilog%20code%20generation%20is%20the%20lack%20of%20sufficient%20functional%0Averification%20data%2C%20particularly%20testbenches%20paired%20with%20design%20specifications%0Aand%20code.%20To%20address%20this%20problem%2C%20we%20introduce%20an%20automatic%20testbench%0Ageneration%20pipeline%20that%20decomposes%20the%20process%20and%20uses%20feedback%20from%20the%0AVerilog%20compiler%20simulator%20%28VCS%29%20to%20reduce%20hallucination%20and%20ensure%0Acorrectness.%20We%20then%20use%20the%20testbench%20to%20evaluate%20the%20generated%20codes%20and%0Acollect%20them%20for%20further%20training%2C%20where%20verification%20insights%20are%20introduced.%0AOur%20method%20applies%20reinforcement%20learning%20%28RL%29%2C%20specifically%20direct%20preference%0Aoptimization%20%28DPO%29%2C%20to%20align%20Verilog%20code%20generation%20with%20functional%0Acorrectness%20by%20training%20preference%20pairs%20based%20on%20testbench%20outcomes.%20In%0Aevaluations%20on%20VerilogEval-Machine%2C%20VerilogEval-Human%2C%20RTLLM%20v1.1%2C%20RTLLM%20v2%2C%0Aand%20VerilogEval%20v2%2C%20our%20approach%20consistently%20outperforms%20state-of-the-art%0Abaselines%20in%20generating%20functionally%20correct%20Verilog%20code.%20We%20open%20source%20all%0Atraining%20code%2C%20data%2C%20and%20models%20at%0Ahttps%3A//anonymous.4open.science/r/VeriPrefer-E88B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15804v1&entry.124074799=Read"},
{"title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy\n  Prediction", "author": "Zhiqiang Wei and Lianqing Zheng and Jianan Liu and Tao Huang and Qing-Long Han and Wenwen Zhang and Fengdeng Zhang", "abstract": "  Accurate 3D semantic occupancy perception is essential for autonomous driving\nin complex environments with diverse and irregular objects. While\nvision-centric methods suffer from geometric inaccuracies, LiDAR-based\napproaches often lack rich semantic information. To address these limitations,\nMS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes\nmiddle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's\ngeometric fidelity with camera-based semantic richness via hierarchical\ncross-modal fusion. The framework introduces innovations at two critical\nstages: (1) In the middle-stage feature fusion, the Gaussian-Geo module\nleverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D\nimage features with dense geometric priors, and the Semantic-Aware module\nenriches LiDAR voxels with semantic context via deformable cross-attention; (2)\nIn the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically\nbalances voxel features across modalities, while the High Classification\nConfidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using\nself-attention-based refinement. Experiments on the nuScenes-OpenOccupancy\nbenchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%\nand a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU\nand +2.4% mIoU. Ablation studies further validate the contribution of each\nmodule, with substantial improvements in small-object perception, demonstrating\nthe practical value of MS-Occ for safety-critical autonomous driving scenarios.\n", "link": "http://arxiv.org/abs/2504.15888v1", "date": "2025-04-22", "relevancy": 2.4435, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6376}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-Occ%3A%20Multi-Stage%20LiDAR-Camera%20Fusion%20for%203D%20Semantic%20Occupancy%0A%20%20Prediction&body=Title%3A%20MS-Occ%3A%20Multi-Stage%20LiDAR-Camera%20Fusion%20for%203D%20Semantic%20Occupancy%0A%20%20Prediction%0AAuthor%3A%20Zhiqiang%20Wei%20and%20Lianqing%20Zheng%20and%20Jianan%20Liu%20and%20Tao%20Huang%20and%20Qing-Long%20Han%20and%20Wenwen%20Zhang%20and%20Fengdeng%20Zhang%0AAbstract%3A%20%20%20Accurate%203D%20semantic%20occupancy%20perception%20is%20essential%20for%20autonomous%20driving%0Ain%20complex%20environments%20with%20diverse%20and%20irregular%20objects.%20While%0Avision-centric%20methods%20suffer%20from%20geometric%20inaccuracies%2C%20LiDAR-based%0Aapproaches%20often%20lack%20rich%20semantic%20information.%20To%20address%20these%20limitations%2C%0AMS-Occ%2C%20a%20novel%20multi-stage%20LiDAR-camera%20fusion%20framework%20which%20includes%0Amiddle-stage%20fusion%20and%20late-stage%20fusion%2C%20is%20proposed%2C%20integrating%20LiDAR%27s%0Ageometric%20fidelity%20with%20camera-based%20semantic%20richness%20via%20hierarchical%0Across-modal%20fusion.%20The%20framework%20introduces%20innovations%20at%20two%20critical%0Astages%3A%20%281%29%20In%20the%20middle-stage%20feature%20fusion%2C%20the%20Gaussian-Geo%20module%0Aleverages%20Gaussian%20kernel%20rendering%20on%20sparse%20LiDAR%20depth%20maps%20to%20enhance%202D%0Aimage%20features%20with%20dense%20geometric%20priors%2C%20and%20the%20Semantic-Aware%20module%0Aenriches%20LiDAR%20voxels%20with%20semantic%20context%20via%20deformable%20cross-attention%3B%20%282%29%0AIn%20the%20late-stage%20voxel%20fusion%2C%20the%20Adaptive%20Fusion%20%28AF%29%20module%20dynamically%0Abalances%20voxel%20features%20across%20modalities%2C%20while%20the%20High%20Classification%0AConfidence%20Voxel%20Fusion%20%28HCCVF%29%20module%20resolves%20semantic%20inconsistencies%20using%0Aself-attention-based%20refinement.%20Experiments%20on%20the%20nuScenes-OpenOccupancy%0Abenchmark%20show%20that%20MS-Occ%20achieves%20an%20Intersection%20over%20Union%20%28IoU%29%20of%2032.1%25%0Aand%20a%20mean%20IoU%20%28mIoU%29%20of%2025.3%25%2C%20surpassing%20the%20state-of-the-art%20by%20%2B0.7%25%20IoU%0Aand%20%2B2.4%25%20mIoU.%20Ablation%20studies%20further%20validate%20the%20contribution%20of%20each%0Amodule%2C%20with%20substantial%20improvements%20in%20small-object%20perception%2C%20demonstrating%0Athe%20practical%20value%20of%20MS-Occ%20for%20safety-critical%20autonomous%20driving%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-Occ%253A%2520Multi-Stage%2520LiDAR-Camera%2520Fusion%2520for%25203D%2520Semantic%2520Occupancy%250A%2520%2520Prediction%26entry.906535625%3DZhiqiang%2520Wei%2520and%2520Lianqing%2520Zheng%2520and%2520Jianan%2520Liu%2520and%2520Tao%2520Huang%2520and%2520Qing-Long%2520Han%2520and%2520Wenwen%2520Zhang%2520and%2520Fengdeng%2520Zhang%26entry.1292438233%3D%2520%2520Accurate%25203D%2520semantic%2520occupancy%2520perception%2520is%2520essential%2520for%2520autonomous%2520driving%250Ain%2520complex%2520environments%2520with%2520diverse%2520and%2520irregular%2520objects.%2520While%250Avision-centric%2520methods%2520suffer%2520from%2520geometric%2520inaccuracies%252C%2520LiDAR-based%250Aapproaches%2520often%2520lack%2520rich%2520semantic%2520information.%2520To%2520address%2520these%2520limitations%252C%250AMS-Occ%252C%2520a%2520novel%2520multi-stage%2520LiDAR-camera%2520fusion%2520framework%2520which%2520includes%250Amiddle-stage%2520fusion%2520and%2520late-stage%2520fusion%252C%2520is%2520proposed%252C%2520integrating%2520LiDAR%2527s%250Ageometric%2520fidelity%2520with%2520camera-based%2520semantic%2520richness%2520via%2520hierarchical%250Across-modal%2520fusion.%2520The%2520framework%2520introduces%2520innovations%2520at%2520two%2520critical%250Astages%253A%2520%25281%2529%2520In%2520the%2520middle-stage%2520feature%2520fusion%252C%2520the%2520Gaussian-Geo%2520module%250Aleverages%2520Gaussian%2520kernel%2520rendering%2520on%2520sparse%2520LiDAR%2520depth%2520maps%2520to%2520enhance%25202D%250Aimage%2520features%2520with%2520dense%2520geometric%2520priors%252C%2520and%2520the%2520Semantic-Aware%2520module%250Aenriches%2520LiDAR%2520voxels%2520with%2520semantic%2520context%2520via%2520deformable%2520cross-attention%253B%2520%25282%2529%250AIn%2520the%2520late-stage%2520voxel%2520fusion%252C%2520the%2520Adaptive%2520Fusion%2520%2528AF%2529%2520module%2520dynamically%250Abalances%2520voxel%2520features%2520across%2520modalities%252C%2520while%2520the%2520High%2520Classification%250AConfidence%2520Voxel%2520Fusion%2520%2528HCCVF%2529%2520module%2520resolves%2520semantic%2520inconsistencies%2520using%250Aself-attention-based%2520refinement.%2520Experiments%2520on%2520the%2520nuScenes-OpenOccupancy%250Abenchmark%2520show%2520that%2520MS-Occ%2520achieves%2520an%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520of%252032.1%2525%250Aand%2520a%2520mean%2520IoU%2520%2528mIoU%2529%2520of%252025.3%2525%252C%2520surpassing%2520the%2520state-of-the-art%2520by%2520%252B0.7%2525%2520IoU%250Aand%2520%252B2.4%2525%2520mIoU.%2520Ablation%2520studies%2520further%2520validate%2520the%2520contribution%2520of%2520each%250Amodule%252C%2520with%2520substantial%2520improvements%2520in%2520small-object%2520perception%252C%2520demonstrating%250Athe%2520practical%2520value%2520of%2520MS-Occ%2520for%2520safety-critical%2520autonomous%2520driving%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-Occ%3A%20Multi-Stage%20LiDAR-Camera%20Fusion%20for%203D%20Semantic%20Occupancy%0A%20%20Prediction&entry.906535625=Zhiqiang%20Wei%20and%20Lianqing%20Zheng%20and%20Jianan%20Liu%20and%20Tao%20Huang%20and%20Qing-Long%20Han%20and%20Wenwen%20Zhang%20and%20Fengdeng%20Zhang&entry.1292438233=%20%20Accurate%203D%20semantic%20occupancy%20perception%20is%20essential%20for%20autonomous%20driving%0Ain%20complex%20environments%20with%20diverse%20and%20irregular%20objects.%20While%0Avision-centric%20methods%20suffer%20from%20geometric%20inaccuracies%2C%20LiDAR-based%0Aapproaches%20often%20lack%20rich%20semantic%20information.%20To%20address%20these%20limitations%2C%0AMS-Occ%2C%20a%20novel%20multi-stage%20LiDAR-camera%20fusion%20framework%20which%20includes%0Amiddle-stage%20fusion%20and%20late-stage%20fusion%2C%20is%20proposed%2C%20integrating%20LiDAR%27s%0Ageometric%20fidelity%20with%20camera-based%20semantic%20richness%20via%20hierarchical%0Across-modal%20fusion.%20The%20framework%20introduces%20innovations%20at%20two%20critical%0Astages%3A%20%281%29%20In%20the%20middle-stage%20feature%20fusion%2C%20the%20Gaussian-Geo%20module%0Aleverages%20Gaussian%20kernel%20rendering%20on%20sparse%20LiDAR%20depth%20maps%20to%20enhance%202D%0Aimage%20features%20with%20dense%20geometric%20priors%2C%20and%20the%20Semantic-Aware%20module%0Aenriches%20LiDAR%20voxels%20with%20semantic%20context%20via%20deformable%20cross-attention%3B%20%282%29%0AIn%20the%20late-stage%20voxel%20fusion%2C%20the%20Adaptive%20Fusion%20%28AF%29%20module%20dynamically%0Abalances%20voxel%20features%20across%20modalities%2C%20while%20the%20High%20Classification%0AConfidence%20Voxel%20Fusion%20%28HCCVF%29%20module%20resolves%20semantic%20inconsistencies%20using%0Aself-attention-based%20refinement.%20Experiments%20on%20the%20nuScenes-OpenOccupancy%0Abenchmark%20show%20that%20MS-Occ%20achieves%20an%20Intersection%20over%20Union%20%28IoU%29%20of%2032.1%25%0Aand%20a%20mean%20IoU%20%28mIoU%29%20of%2025.3%25%2C%20surpassing%20the%20state-of-the-art%20by%20%2B0.7%25%20IoU%0Aand%20%2B2.4%25%20mIoU.%20Ablation%20studies%20further%20validate%20the%20contribution%20of%20each%0Amodule%2C%20with%20substantial%20improvements%20in%20small-object%20perception%2C%20demonstrating%0Athe%20practical%20value%20of%20MS-Occ%20for%20safety-critical%20autonomous%20driving%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15888v1&entry.124074799=Read"},
{"title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via\n  Reinforcement Learning", "author": "Wang Lin and Liyu Jia and Wentao Hu and Kaihang Pan and Zhongqi Yue and Wei Zhao and Jingyuan Chen and Fei Wu and Hanwang Zhang", "abstract": "  Despite recent progress in video generation, producing videos that adhere to\nphysical laws remains a significant challenge. Traditional diffusion-based\nmethods struggle to extrapolate to unseen physical conditions (eg, velocity)\ndue to their reliance on data-driven approximations. To address this, we\npropose to integrate symbolic reasoning and reinforcement learning to enforce\nphysical consistency in video generation. We first introduce the Diffusion\nTimestep Tokenizer (DDT), which learns discrete, recursive visual tokens by\nrecovering visual attributes lost during the diffusion process. The recursive\nvisual tokens enable symbolic reasoning by a large language model. Based on it,\nwe propose the Phys-AR framework, which consists of two stages: The first stage\nuses supervised fine-tuning to transfer symbolic knowledge, while the second\nstage applies reinforcement learning to optimize the model's reasoning\nabilities through reward functions based on physical conditions. Our approach\nallows the model to dynamically adjust and improve the physical properties of\ngenerated videos, ensuring adherence to physical laws. Experimental results\ndemonstrate that PhysAR can generate videos that are physically consistent.\n", "link": "http://arxiv.org/abs/2504.15932v1", "date": "2025-04-22", "relevancy": 2.4405, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6475}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5867}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Physical%20Video%20Generation%20with%20Diffusion%20Timestep%20Tokens%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20Reasoning%20Physical%20Video%20Generation%20with%20Diffusion%20Timestep%20Tokens%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Wang%20Lin%20and%20Liyu%20Jia%20and%20Wentao%20Hu%20and%20Kaihang%20Pan%20and%20Zhongqi%20Yue%20and%20Wei%20Zhao%20and%20Jingyuan%20Chen%20and%20Fei%20Wu%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20Despite%20recent%20progress%20in%20video%20generation%2C%20producing%20videos%20that%20adhere%20to%0Aphysical%20laws%20remains%20a%20significant%20challenge.%20Traditional%20diffusion-based%0Amethods%20struggle%20to%20extrapolate%20to%20unseen%20physical%20conditions%20%28eg%2C%20velocity%29%0Adue%20to%20their%20reliance%20on%20data-driven%20approximations.%20To%20address%20this%2C%20we%0Apropose%20to%20integrate%20symbolic%20reasoning%20and%20reinforcement%20learning%20to%20enforce%0Aphysical%20consistency%20in%20video%20generation.%20We%20first%20introduce%20the%20Diffusion%0ATimestep%20Tokenizer%20%28DDT%29%2C%20which%20learns%20discrete%2C%20recursive%20visual%20tokens%20by%0Arecovering%20visual%20attributes%20lost%20during%20the%20diffusion%20process.%20The%20recursive%0Avisual%20tokens%20enable%20symbolic%20reasoning%20by%20a%20large%20language%20model.%20Based%20on%20it%2C%0Awe%20propose%20the%20Phys-AR%20framework%2C%20which%20consists%20of%20two%20stages%3A%20The%20first%20stage%0Auses%20supervised%20fine-tuning%20to%20transfer%20symbolic%20knowledge%2C%20while%20the%20second%0Astage%20applies%20reinforcement%20learning%20to%20optimize%20the%20model%27s%20reasoning%0Aabilities%20through%20reward%20functions%20based%20on%20physical%20conditions.%20Our%20approach%0Aallows%20the%20model%20to%20dynamically%20adjust%20and%20improve%20the%20physical%20properties%20of%0Agenerated%20videos%2C%20ensuring%20adherence%20to%20physical%20laws.%20Experimental%20results%0Ademonstrate%20that%20PhysAR%20can%20generate%20videos%20that%20are%20physically%20consistent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Physical%2520Video%2520Generation%2520with%2520Diffusion%2520Timestep%2520Tokens%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DWang%2520Lin%2520and%2520Liyu%2520Jia%2520and%2520Wentao%2520Hu%2520and%2520Kaihang%2520Pan%2520and%2520Zhongqi%2520Yue%2520and%2520Wei%2520Zhao%2520and%2520Jingyuan%2520Chen%2520and%2520Fei%2520Wu%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520recent%2520progress%2520in%2520video%2520generation%252C%2520producing%2520videos%2520that%2520adhere%2520to%250Aphysical%2520laws%2520remains%2520a%2520significant%2520challenge.%2520Traditional%2520diffusion-based%250Amethods%2520struggle%2520to%2520extrapolate%2520to%2520unseen%2520physical%2520conditions%2520%2528eg%252C%2520velocity%2529%250Adue%2520to%2520their%2520reliance%2520on%2520data-driven%2520approximations.%2520To%2520address%2520this%252C%2520we%250Apropose%2520to%2520integrate%2520symbolic%2520reasoning%2520and%2520reinforcement%2520learning%2520to%2520enforce%250Aphysical%2520consistency%2520in%2520video%2520generation.%2520We%2520first%2520introduce%2520the%2520Diffusion%250ATimestep%2520Tokenizer%2520%2528DDT%2529%252C%2520which%2520learns%2520discrete%252C%2520recursive%2520visual%2520tokens%2520by%250Arecovering%2520visual%2520attributes%2520lost%2520during%2520the%2520diffusion%2520process.%2520The%2520recursive%250Avisual%2520tokens%2520enable%2520symbolic%2520reasoning%2520by%2520a%2520large%2520language%2520model.%2520Based%2520on%2520it%252C%250Awe%2520propose%2520the%2520Phys-AR%2520framework%252C%2520which%2520consists%2520of%2520two%2520stages%253A%2520The%2520first%2520stage%250Auses%2520supervised%2520fine-tuning%2520to%2520transfer%2520symbolic%2520knowledge%252C%2520while%2520the%2520second%250Astage%2520applies%2520reinforcement%2520learning%2520to%2520optimize%2520the%2520model%2527s%2520reasoning%250Aabilities%2520through%2520reward%2520functions%2520based%2520on%2520physical%2520conditions.%2520Our%2520approach%250Aallows%2520the%2520model%2520to%2520dynamically%2520adjust%2520and%2520improve%2520the%2520physical%2520properties%2520of%250Agenerated%2520videos%252C%2520ensuring%2520adherence%2520to%2520physical%2520laws.%2520Experimental%2520results%250Ademonstrate%2520that%2520PhysAR%2520can%2520generate%2520videos%2520that%2520are%2520physically%2520consistent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Physical%20Video%20Generation%20with%20Diffusion%20Timestep%20Tokens%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Wang%20Lin%20and%20Liyu%20Jia%20and%20Wentao%20Hu%20and%20Kaihang%20Pan%20and%20Zhongqi%20Yue%20and%20Wei%20Zhao%20and%20Jingyuan%20Chen%20and%20Fei%20Wu%20and%20Hanwang%20Zhang&entry.1292438233=%20%20Despite%20recent%20progress%20in%20video%20generation%2C%20producing%20videos%20that%20adhere%20to%0Aphysical%20laws%20remains%20a%20significant%20challenge.%20Traditional%20diffusion-based%0Amethods%20struggle%20to%20extrapolate%20to%20unseen%20physical%20conditions%20%28eg%2C%20velocity%29%0Adue%20to%20their%20reliance%20on%20data-driven%20approximations.%20To%20address%20this%2C%20we%0Apropose%20to%20integrate%20symbolic%20reasoning%20and%20reinforcement%20learning%20to%20enforce%0Aphysical%20consistency%20in%20video%20generation.%20We%20first%20introduce%20the%20Diffusion%0ATimestep%20Tokenizer%20%28DDT%29%2C%20which%20learns%20discrete%2C%20recursive%20visual%20tokens%20by%0Arecovering%20visual%20attributes%20lost%20during%20the%20diffusion%20process.%20The%20recursive%0Avisual%20tokens%20enable%20symbolic%20reasoning%20by%20a%20large%20language%20model.%20Based%20on%20it%2C%0Awe%20propose%20the%20Phys-AR%20framework%2C%20which%20consists%20of%20two%20stages%3A%20The%20first%20stage%0Auses%20supervised%20fine-tuning%20to%20transfer%20symbolic%20knowledge%2C%20while%20the%20second%0Astage%20applies%20reinforcement%20learning%20to%20optimize%20the%20model%27s%20reasoning%0Aabilities%20through%20reward%20functions%20based%20on%20physical%20conditions.%20Our%20approach%0Aallows%20the%20model%20to%20dynamically%20adjust%20and%20improve%20the%20physical%20properties%20of%0Agenerated%20videos%2C%20ensuring%20adherence%20to%20physical%20laws.%20Experimental%20results%0Ademonstrate%20that%20PhysAR%20can%20generate%20videos%20that%20are%20physically%20consistent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15932v1&entry.124074799=Read"},
{"title": "Benchmarking the Reproducibility of Brain MRI Segmentation Across\n  Scanners and Time", "author": "Ekaterina Kondrateva and Sandzhi Barg and Mikhail Vasiliev", "abstract": "  Accurate and reproducible brain morphometry from structural MRI is critical\nfor monitoring neuroanatomical changes across time and across imaging domains.\nAlthough deep learning has accelerated segmentation workflows, scanner-induced\nvariability and reproducibility limitations remain-especially in longitudinal\nand multi-site settings. In this study, we benchmark two modern segmentation\npipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the\nmost widely adopted tools in neuroimaging.\n  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and\na 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation\nvariability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),\nand Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume\nvariation in small subcortical structures such as the amygdala and ventral\ndiencephalon, even under controlled test-retest conditions. This raises a key\nquestion: is it feasible to detect subtle longitudinal changes on the order of\n5-10% in pea-sized brain regions, given the magnitude of domain-induced\nmorphometric noise?\n  We further analyze the effects of registration templates and interpolation\nmodes, and propose surface-based quality filtering to improve segmentation\nreliability. This study provides a reproducible benchmark for morphometric\nreproducibility and emphasizes the need for harmonization strategies in\nreal-world neuroimaging studies.\n  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation\n", "link": "http://arxiv.org/abs/2504.15931v1", "date": "2025-04-22", "relevancy": 2.4148, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20Reproducibility%20of%20Brain%20MRI%20Segmentation%20Across%0A%20%20Scanners%20and%20Time&body=Title%3A%20Benchmarking%20the%20Reproducibility%20of%20Brain%20MRI%20Segmentation%20Across%0A%20%20Scanners%20and%20Time%0AAuthor%3A%20Ekaterina%20Kondrateva%20and%20Sandzhi%20Barg%20and%20Mikhail%20Vasiliev%0AAbstract%3A%20%20%20Accurate%20and%20reproducible%20brain%20morphometry%20from%20structural%20MRI%20is%20critical%0Afor%20monitoring%20neuroanatomical%20changes%20across%20time%20and%20across%20imaging%20domains.%0AAlthough%20deep%20learning%20has%20accelerated%20segmentation%20workflows%2C%20scanner-induced%0Avariability%20and%20reproducibility%20limitations%20remain-especially%20in%20longitudinal%0Aand%20multi-site%20settings.%20In%20this%20study%2C%20we%20benchmark%20two%20modern%20segmentation%0Apipelines%2C%20FastSurfer%20and%20SynthSeg%2C%20both%20integrated%20into%20FreeSurfer%2C%20one%20of%20the%0Amost%20widely%20adopted%20tools%20in%20neuroimaging.%0A%20%20Using%20two%20complementary%20datasets%20-%20a%2017-year%20longitudinal%20cohort%20%28SIMON%29%20and%0Aa%209-site%20test-retest%20cohort%20%28SRPBS%29-we%20quantify%20inter-scan%20segmentation%0Avariability%20using%20Dice%20coefficient%2C%20Surface%20Dice%2C%20Hausdorff%20Distance%20%28HD95%29%2C%0Aand%20Mean%20Absolute%20Percentage%20Error%20%28MAPE%29.%20Our%20results%20reveal%20up%20to%207-8%25%20volume%0Avariation%20in%20small%20subcortical%20structures%20such%20as%20the%20amygdala%20and%20ventral%0Adiencephalon%2C%20even%20under%20controlled%20test-retest%20conditions.%20This%20raises%20a%20key%0Aquestion%3A%20is%20it%20feasible%20to%20detect%20subtle%20longitudinal%20changes%20on%20the%20order%20of%0A5-10%25%20in%20pea-sized%20brain%20regions%2C%20given%20the%20magnitude%20of%20domain-induced%0Amorphometric%20noise%3F%0A%20%20We%20further%20analyze%20the%20effects%20of%20registration%20templates%20and%20interpolation%0Amodes%2C%20and%20propose%20surface-based%20quality%20filtering%20to%20improve%20segmentation%0Areliability.%20This%20study%20provides%20a%20reproducible%20benchmark%20for%20morphometric%0Areproducibility%20and%20emphasizes%20the%20need%20for%20harmonization%20strategies%20in%0Areal-world%20neuroimaging%20studies.%0A%20%20Code%20and%20figures%3A%20https%3A//github.com/kondratevakate/brain-mri-segmentation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520the%2520Reproducibility%2520of%2520Brain%2520MRI%2520Segmentation%2520Across%250A%2520%2520Scanners%2520and%2520Time%26entry.906535625%3DEkaterina%2520Kondrateva%2520and%2520Sandzhi%2520Barg%2520and%2520Mikhail%2520Vasiliev%26entry.1292438233%3D%2520%2520Accurate%2520and%2520reproducible%2520brain%2520morphometry%2520from%2520structural%2520MRI%2520is%2520critical%250Afor%2520monitoring%2520neuroanatomical%2520changes%2520across%2520time%2520and%2520across%2520imaging%2520domains.%250AAlthough%2520deep%2520learning%2520has%2520accelerated%2520segmentation%2520workflows%252C%2520scanner-induced%250Avariability%2520and%2520reproducibility%2520limitations%2520remain-especially%2520in%2520longitudinal%250Aand%2520multi-site%2520settings.%2520In%2520this%2520study%252C%2520we%2520benchmark%2520two%2520modern%2520segmentation%250Apipelines%252C%2520FastSurfer%2520and%2520SynthSeg%252C%2520both%2520integrated%2520into%2520FreeSurfer%252C%2520one%2520of%2520the%250Amost%2520widely%2520adopted%2520tools%2520in%2520neuroimaging.%250A%2520%2520Using%2520two%2520complementary%2520datasets%2520-%2520a%252017-year%2520longitudinal%2520cohort%2520%2528SIMON%2529%2520and%250Aa%25209-site%2520test-retest%2520cohort%2520%2528SRPBS%2529-we%2520quantify%2520inter-scan%2520segmentation%250Avariability%2520using%2520Dice%2520coefficient%252C%2520Surface%2520Dice%252C%2520Hausdorff%2520Distance%2520%2528HD95%2529%252C%250Aand%2520Mean%2520Absolute%2520Percentage%2520Error%2520%2528MAPE%2529.%2520Our%2520results%2520reveal%2520up%2520to%25207-8%2525%2520volume%250Avariation%2520in%2520small%2520subcortical%2520structures%2520such%2520as%2520the%2520amygdala%2520and%2520ventral%250Adiencephalon%252C%2520even%2520under%2520controlled%2520test-retest%2520conditions.%2520This%2520raises%2520a%2520key%250Aquestion%253A%2520is%2520it%2520feasible%2520to%2520detect%2520subtle%2520longitudinal%2520changes%2520on%2520the%2520order%2520of%250A5-10%2525%2520in%2520pea-sized%2520brain%2520regions%252C%2520given%2520the%2520magnitude%2520of%2520domain-induced%250Amorphometric%2520noise%253F%250A%2520%2520We%2520further%2520analyze%2520the%2520effects%2520of%2520registration%2520templates%2520and%2520interpolation%250Amodes%252C%2520and%2520propose%2520surface-based%2520quality%2520filtering%2520to%2520improve%2520segmentation%250Areliability.%2520This%2520study%2520provides%2520a%2520reproducible%2520benchmark%2520for%2520morphometric%250Areproducibility%2520and%2520emphasizes%2520the%2520need%2520for%2520harmonization%2520strategies%2520in%250Areal-world%2520neuroimaging%2520studies.%250A%2520%2520Code%2520and%2520figures%253A%2520https%253A//github.com/kondratevakate/brain-mri-segmentation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20Reproducibility%20of%20Brain%20MRI%20Segmentation%20Across%0A%20%20Scanners%20and%20Time&entry.906535625=Ekaterina%20Kondrateva%20and%20Sandzhi%20Barg%20and%20Mikhail%20Vasiliev&entry.1292438233=%20%20Accurate%20and%20reproducible%20brain%20morphometry%20from%20structural%20MRI%20is%20critical%0Afor%20monitoring%20neuroanatomical%20changes%20across%20time%20and%20across%20imaging%20domains.%0AAlthough%20deep%20learning%20has%20accelerated%20segmentation%20workflows%2C%20scanner-induced%0Avariability%20and%20reproducibility%20limitations%20remain-especially%20in%20longitudinal%0Aand%20multi-site%20settings.%20In%20this%20study%2C%20we%20benchmark%20two%20modern%20segmentation%0Apipelines%2C%20FastSurfer%20and%20SynthSeg%2C%20both%20integrated%20into%20FreeSurfer%2C%20one%20of%20the%0Amost%20widely%20adopted%20tools%20in%20neuroimaging.%0A%20%20Using%20two%20complementary%20datasets%20-%20a%2017-year%20longitudinal%20cohort%20%28SIMON%29%20and%0Aa%209-site%20test-retest%20cohort%20%28SRPBS%29-we%20quantify%20inter-scan%20segmentation%0Avariability%20using%20Dice%20coefficient%2C%20Surface%20Dice%2C%20Hausdorff%20Distance%20%28HD95%29%2C%0Aand%20Mean%20Absolute%20Percentage%20Error%20%28MAPE%29.%20Our%20results%20reveal%20up%20to%207-8%25%20volume%0Avariation%20in%20small%20subcortical%20structures%20such%20as%20the%20amygdala%20and%20ventral%0Adiencephalon%2C%20even%20under%20controlled%20test-retest%20conditions.%20This%20raises%20a%20key%0Aquestion%3A%20is%20it%20feasible%20to%20detect%20subtle%20longitudinal%20changes%20on%20the%20order%20of%0A5-10%25%20in%20pea-sized%20brain%20regions%2C%20given%20the%20magnitude%20of%20domain-induced%0Amorphometric%20noise%3F%0A%20%20We%20further%20analyze%20the%20effects%20of%20registration%20templates%20and%20interpolation%0Amodes%2C%20and%20propose%20surface-based%20quality%20filtering%20to%20improve%20segmentation%0Areliability.%20This%20study%20provides%20a%20reproducible%20benchmark%20for%20morphometric%0Areproducibility%20and%20emphasizes%20the%20need%20for%20harmonization%20strategies%20in%0Areal-world%20neuroimaging%20studies.%0A%20%20Code%20and%20figures%3A%20https%3A//github.com/kondratevakate/brain-mri-segmentation%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15931v1&entry.124074799=Read"},
{"title": "Satellite to GroundScape -- Large-scale Consistent Ground View\n  Generation from Satellite Views", "author": "Ningli Xu and Rongjun Qin", "abstract": "  Generating consistent ground-view images from satellite imagery is\nchallenging, primarily due to the large discrepancies in viewing angles and\nresolution between satellite and ground-level domains. Previous efforts mainly\nconcentrated on single-view generation, often resulting in inconsistencies\nacross neighboring ground views. In this work, we propose a novel cross-view\nsynthesis approach designed to overcome these challenges by ensuring\nconsistency across ground-view images generated from satellite views. Our\nmethod, based on a fixed latent diffusion model, introduces two conditioning\nmodules: satellite-guided denoising, which extracts high-level scene layout to\nguide the denoising process, and satellite-temporal denoising, which captures\ncamera motion to maintain consistency across multiple generated views. We\nfurther contribute a large-scale satellite-ground dataset containing over\n100,000 perspective pairs to facilitate extensive ground scene or video\ngeneration. Experimental results demonstrate that our approach outperforms\nexisting methods on perceptual and temporal metrics, achieving high\nphotorealism and consistency in multi-view outputs.\n", "link": "http://arxiv.org/abs/2504.15786v1", "date": "2025-04-22", "relevancy": 2.3859, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6021}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5954}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Satellite%20to%20GroundScape%20--%20Large-scale%20Consistent%20Ground%20View%0A%20%20Generation%20from%20Satellite%20Views&body=Title%3A%20Satellite%20to%20GroundScape%20--%20Large-scale%20Consistent%20Ground%20View%0A%20%20Generation%20from%20Satellite%20Views%0AAuthor%3A%20Ningli%20Xu%20and%20Rongjun%20Qin%0AAbstract%3A%20%20%20Generating%20consistent%20ground-view%20images%20from%20satellite%20imagery%20is%0Achallenging%2C%20primarily%20due%20to%20the%20large%20discrepancies%20in%20viewing%20angles%20and%0Aresolution%20between%20satellite%20and%20ground-level%20domains.%20Previous%20efforts%20mainly%0Aconcentrated%20on%20single-view%20generation%2C%20often%20resulting%20in%20inconsistencies%0Aacross%20neighboring%20ground%20views.%20In%20this%20work%2C%20we%20propose%20a%20novel%20cross-view%0Asynthesis%20approach%20designed%20to%20overcome%20these%20challenges%20by%20ensuring%0Aconsistency%20across%20ground-view%20images%20generated%20from%20satellite%20views.%20Our%0Amethod%2C%20based%20on%20a%20fixed%20latent%20diffusion%20model%2C%20introduces%20two%20conditioning%0Amodules%3A%20satellite-guided%20denoising%2C%20which%20extracts%20high-level%20scene%20layout%20to%0Aguide%20the%20denoising%20process%2C%20and%20satellite-temporal%20denoising%2C%20which%20captures%0Acamera%20motion%20to%20maintain%20consistency%20across%20multiple%20generated%20views.%20We%0Afurther%20contribute%20a%20large-scale%20satellite-ground%20dataset%20containing%20over%0A100%2C000%20perspective%20pairs%20to%20facilitate%20extensive%20ground%20scene%20or%20video%0Ageneration.%20Experimental%20results%20demonstrate%20that%20our%20approach%20outperforms%0Aexisting%20methods%20on%20perceptual%20and%20temporal%20metrics%2C%20achieving%20high%0Aphotorealism%20and%20consistency%20in%20multi-view%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSatellite%2520to%2520GroundScape%2520--%2520Large-scale%2520Consistent%2520Ground%2520View%250A%2520%2520Generation%2520from%2520Satellite%2520Views%26entry.906535625%3DNingli%2520Xu%2520and%2520Rongjun%2520Qin%26entry.1292438233%3D%2520%2520Generating%2520consistent%2520ground-view%2520images%2520from%2520satellite%2520imagery%2520is%250Achallenging%252C%2520primarily%2520due%2520to%2520the%2520large%2520discrepancies%2520in%2520viewing%2520angles%2520and%250Aresolution%2520between%2520satellite%2520and%2520ground-level%2520domains.%2520Previous%2520efforts%2520mainly%250Aconcentrated%2520on%2520single-view%2520generation%252C%2520often%2520resulting%2520in%2520inconsistencies%250Aacross%2520neighboring%2520ground%2520views.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520cross-view%250Asynthesis%2520approach%2520designed%2520to%2520overcome%2520these%2520challenges%2520by%2520ensuring%250Aconsistency%2520across%2520ground-view%2520images%2520generated%2520from%2520satellite%2520views.%2520Our%250Amethod%252C%2520based%2520on%2520a%2520fixed%2520latent%2520diffusion%2520model%252C%2520introduces%2520two%2520conditioning%250Amodules%253A%2520satellite-guided%2520denoising%252C%2520which%2520extracts%2520high-level%2520scene%2520layout%2520to%250Aguide%2520the%2520denoising%2520process%252C%2520and%2520satellite-temporal%2520denoising%252C%2520which%2520captures%250Acamera%2520motion%2520to%2520maintain%2520consistency%2520across%2520multiple%2520generated%2520views.%2520We%250Afurther%2520contribute%2520a%2520large-scale%2520satellite-ground%2520dataset%2520containing%2520over%250A100%252C000%2520perspective%2520pairs%2520to%2520facilitate%2520extensive%2520ground%2520scene%2520or%2520video%250Ageneration.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520outperforms%250Aexisting%2520methods%2520on%2520perceptual%2520and%2520temporal%2520metrics%252C%2520achieving%2520high%250Aphotorealism%2520and%2520consistency%2520in%2520multi-view%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Satellite%20to%20GroundScape%20--%20Large-scale%20Consistent%20Ground%20View%0A%20%20Generation%20from%20Satellite%20Views&entry.906535625=Ningli%20Xu%20and%20Rongjun%20Qin&entry.1292438233=%20%20Generating%20consistent%20ground-view%20images%20from%20satellite%20imagery%20is%0Achallenging%2C%20primarily%20due%20to%20the%20large%20discrepancies%20in%20viewing%20angles%20and%0Aresolution%20between%20satellite%20and%20ground-level%20domains.%20Previous%20efforts%20mainly%0Aconcentrated%20on%20single-view%20generation%2C%20often%20resulting%20in%20inconsistencies%0Aacross%20neighboring%20ground%20views.%20In%20this%20work%2C%20we%20propose%20a%20novel%20cross-view%0Asynthesis%20approach%20designed%20to%20overcome%20these%20challenges%20by%20ensuring%0Aconsistency%20across%20ground-view%20images%20generated%20from%20satellite%20views.%20Our%0Amethod%2C%20based%20on%20a%20fixed%20latent%20diffusion%20model%2C%20introduces%20two%20conditioning%0Amodules%3A%20satellite-guided%20denoising%2C%20which%20extracts%20high-level%20scene%20layout%20to%0Aguide%20the%20denoising%20process%2C%20and%20satellite-temporal%20denoising%2C%20which%20captures%0Acamera%20motion%20to%20maintain%20consistency%20across%20multiple%20generated%20views.%20We%0Afurther%20contribute%20a%20large-scale%20satellite-ground%20dataset%20containing%20over%0A100%2C000%20perspective%20pairs%20to%20facilitate%20extensive%20ground%20scene%20or%20video%0Ageneration.%20Experimental%20results%20demonstrate%20that%20our%20approach%20outperforms%0Aexisting%20methods%20on%20perceptual%20and%20temporal%20metrics%2C%20achieving%20high%0Aphotorealism%20and%20consistency%20in%20multi-view%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15786v1&entry.124074799=Read"},
{"title": "Muon Optimizer Accelerates Grokking", "author": "Amund Tveit and Bj\u00f8rn Remseth and Arve Skogvold", "abstract": "  This paper investigates the impact of different optimizers on the grokking\nphenomenon, where models exhibit delayed generalization. We conducted\nexperiments across seven numerical tasks (primarily modular arithmetic) using a\nmodern Transformer architecture. The experimental configuration systematically\nvaried the optimizer (Muon vs. AdamW) and the softmax activation function\n(standard softmax, stablemax, and sparsemax) to assess their combined effect on\nlearning dynamics. Our empirical evaluation reveals that the Muon optimizer,\ncharacterized by its use of spectral norm constraints and second-order\ninformation, significantly accelerates the onset of grokking compared to the\nwidely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch\nfrom 153.09 to 102.89 across all configurations, a statistically significant\ndifference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice\nplays a crucial role in facilitating the transition from memorization to\ngeneralization.\n", "link": "http://arxiv.org/abs/2504.16041v1", "date": "2025-04-22", "relevancy": 2.3535, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4877}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4695}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Muon%20Optimizer%20Accelerates%20Grokking&body=Title%3A%20Muon%20Optimizer%20Accelerates%20Grokking%0AAuthor%3A%20Amund%20Tveit%20and%20Bj%C3%B8rn%20Remseth%20and%20Arve%20Skogvold%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20impact%20of%20different%20optimizers%20on%20the%20grokking%0Aphenomenon%2C%20where%20models%20exhibit%20delayed%20generalization.%20We%20conducted%0Aexperiments%20across%20seven%20numerical%20tasks%20%28primarily%20modular%20arithmetic%29%20using%20a%0Amodern%20Transformer%20architecture.%20The%20experimental%20configuration%20systematically%0Avaried%20the%20optimizer%20%28Muon%20vs.%20AdamW%29%20and%20the%20softmax%20activation%20function%0A%28standard%20softmax%2C%20stablemax%2C%20and%20sparsemax%29%20to%20assess%20their%20combined%20effect%20on%0Alearning%20dynamics.%20Our%20empirical%20evaluation%20reveals%20that%20the%20Muon%20optimizer%2C%0Acharacterized%20by%20its%20use%20of%20spectral%20norm%20constraints%20and%20second-order%0Ainformation%2C%20significantly%20accelerates%20the%20onset%20of%20grokking%20compared%20to%20the%0Awidely%20used%20AdamW%20optimizer.%20Specifically%2C%20Muon%20reduced%20the%20mean%20grokking%20epoch%0Afrom%20153.09%20to%20102.89%20across%20all%20configurations%2C%20a%20statistically%20significant%0Adifference%20%28t%20%3D%205.0175%2C%20p%20%3D%206.33e-08%29.%20This%20suggests%20that%20the%20optimizer%20choice%0Aplays%20a%20crucial%20role%20in%20facilitating%20the%20transition%20from%20memorization%20to%0Ageneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuon%2520Optimizer%2520Accelerates%2520Grokking%26entry.906535625%3DAmund%2520Tveit%2520and%2520Bj%25C3%25B8rn%2520Remseth%2520and%2520Arve%2520Skogvold%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520impact%2520of%2520different%2520optimizers%2520on%2520the%2520grokking%250Aphenomenon%252C%2520where%2520models%2520exhibit%2520delayed%2520generalization.%2520We%2520conducted%250Aexperiments%2520across%2520seven%2520numerical%2520tasks%2520%2528primarily%2520modular%2520arithmetic%2529%2520using%2520a%250Amodern%2520Transformer%2520architecture.%2520The%2520experimental%2520configuration%2520systematically%250Avaried%2520the%2520optimizer%2520%2528Muon%2520vs.%2520AdamW%2529%2520and%2520the%2520softmax%2520activation%2520function%250A%2528standard%2520softmax%252C%2520stablemax%252C%2520and%2520sparsemax%2529%2520to%2520assess%2520their%2520combined%2520effect%2520on%250Alearning%2520dynamics.%2520Our%2520empirical%2520evaluation%2520reveals%2520that%2520the%2520Muon%2520optimizer%252C%250Acharacterized%2520by%2520its%2520use%2520of%2520spectral%2520norm%2520constraints%2520and%2520second-order%250Ainformation%252C%2520significantly%2520accelerates%2520the%2520onset%2520of%2520grokking%2520compared%2520to%2520the%250Awidely%2520used%2520AdamW%2520optimizer.%2520Specifically%252C%2520Muon%2520reduced%2520the%2520mean%2520grokking%2520epoch%250Afrom%2520153.09%2520to%2520102.89%2520across%2520all%2520configurations%252C%2520a%2520statistically%2520significant%250Adifference%2520%2528t%2520%253D%25205.0175%252C%2520p%2520%253D%25206.33e-08%2529.%2520This%2520suggests%2520that%2520the%2520optimizer%2520choice%250Aplays%2520a%2520crucial%2520role%2520in%2520facilitating%2520the%2520transition%2520from%2520memorization%2520to%250Ageneralization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Muon%20Optimizer%20Accelerates%20Grokking&entry.906535625=Amund%20Tveit%20and%20Bj%C3%B8rn%20Remseth%20and%20Arve%20Skogvold&entry.1292438233=%20%20This%20paper%20investigates%20the%20impact%20of%20different%20optimizers%20on%20the%20grokking%0Aphenomenon%2C%20where%20models%20exhibit%20delayed%20generalization.%20We%20conducted%0Aexperiments%20across%20seven%20numerical%20tasks%20%28primarily%20modular%20arithmetic%29%20using%20a%0Amodern%20Transformer%20architecture.%20The%20experimental%20configuration%20systematically%0Avaried%20the%20optimizer%20%28Muon%20vs.%20AdamW%29%20and%20the%20softmax%20activation%20function%0A%28standard%20softmax%2C%20stablemax%2C%20and%20sparsemax%29%20to%20assess%20their%20combined%20effect%20on%0Alearning%20dynamics.%20Our%20empirical%20evaluation%20reveals%20that%20the%20Muon%20optimizer%2C%0Acharacterized%20by%20its%20use%20of%20spectral%20norm%20constraints%20and%20second-order%0Ainformation%2C%20significantly%20accelerates%20the%20onset%20of%20grokking%20compared%20to%20the%0Awidely%20used%20AdamW%20optimizer.%20Specifically%2C%20Muon%20reduced%20the%20mean%20grokking%20epoch%0Afrom%20153.09%20to%20102.89%20across%20all%20configurations%2C%20a%20statistically%20significant%0Adifference%20%28t%20%3D%205.0175%2C%20p%20%3D%206.33e-08%29.%20This%20suggests%20that%20the%20optimizer%20choice%0Aplays%20a%20crucial%20role%20in%20facilitating%20the%20transition%20from%20memorization%20to%0Ageneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16041v1&entry.124074799=Read"},
{"title": "How Private is Your Attention? Bridging Privacy with In-Context Learning", "author": "Soham Bonnerjee and Zhen Wei and  Yeon and Anna Asch and Sagnik Nandy and Promit Ghosal", "abstract": "  In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings.\n", "link": "http://arxiv.org/abs/2504.16000v1", "date": "2025-04-22", "relevancy": 2.3479, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Private%20is%20Your%20Attention%3F%20Bridging%20Privacy%20with%20In-Context%20Learning&body=Title%3A%20How%20Private%20is%20Your%20Attention%3F%20Bridging%20Privacy%20with%20In-Context%20Learning%0AAuthor%3A%20Soham%20Bonnerjee%20and%20Zhen%20Wei%20and%20%20Yeon%20and%20Anna%20Asch%20and%20Sagnik%20Nandy%20and%20Promit%20Ghosal%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29-the%20ability%20of%20transformer-based%20models%20to%20perform%0Anew%20tasks%20from%20examples%20provided%20at%20inference%20time-has%20emerged%20as%20a%20hallmark%20of%0Amodern%20language%20models.%20While%20recent%20works%20have%20investigated%20the%20mechanisms%0Aunderlying%20ICL%2C%20its%20feasibility%20under%20formal%20privacy%20constraints%20remains%0Alargely%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20differentially%20private%0Apretraining%20algorithm%20for%20linear%20attention%20heads%20and%20present%20the%20first%0Atheoretical%20analysis%20of%20the%20privacy-accuracy%20trade-off%20for%20ICL%20in%20linear%0Aregression.%20Our%20results%20characterize%20the%20fundamental%20tension%20between%0Aoptimization%20and%20privacy-induced%20noise%2C%20formally%20capturing%20behaviors%20observed%0Ain%20private%20training%20via%20iterative%20methods.%20Additionally%2C%20we%20show%20that%20our%0Amethod%20is%20robust%20to%20adversarial%20perturbations%20of%20training%20prompts%2C%20unlike%0Astandard%20ridge%20regression.%20All%20theoretical%20findings%20are%20supported%20by%20extensive%0Asimulations%20across%20diverse%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Private%2520is%2520Your%2520Attention%253F%2520Bridging%2520Privacy%2520with%2520In-Context%2520Learning%26entry.906535625%3DSoham%2520Bonnerjee%2520and%2520Zhen%2520Wei%2520and%2520%2520Yeon%2520and%2520Anna%2520Asch%2520and%2520Sagnik%2520Nandy%2520and%2520Promit%2520Ghosal%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529-the%2520ability%2520of%2520transformer-based%2520models%2520to%2520perform%250Anew%2520tasks%2520from%2520examples%2520provided%2520at%2520inference%2520time-has%2520emerged%2520as%2520a%2520hallmark%2520of%250Amodern%2520language%2520models.%2520While%2520recent%2520works%2520have%2520investigated%2520the%2520mechanisms%250Aunderlying%2520ICL%252C%2520its%2520feasibility%2520under%2520formal%2520privacy%2520constraints%2520remains%250Alargely%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520differentially%2520private%250Apretraining%2520algorithm%2520for%2520linear%2520attention%2520heads%2520and%2520present%2520the%2520first%250Atheoretical%2520analysis%2520of%2520the%2520privacy-accuracy%2520trade-off%2520for%2520ICL%2520in%2520linear%250Aregression.%2520Our%2520results%2520characterize%2520the%2520fundamental%2520tension%2520between%250Aoptimization%2520and%2520privacy-induced%2520noise%252C%2520formally%2520capturing%2520behaviors%2520observed%250Ain%2520private%2520training%2520via%2520iterative%2520methods.%2520Additionally%252C%2520we%2520show%2520that%2520our%250Amethod%2520is%2520robust%2520to%2520adversarial%2520perturbations%2520of%2520training%2520prompts%252C%2520unlike%250Astandard%2520ridge%2520regression.%2520All%2520theoretical%2520findings%2520are%2520supported%2520by%2520extensive%250Asimulations%2520across%2520diverse%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Private%20is%20Your%20Attention%3F%20Bridging%20Privacy%20with%20In-Context%20Learning&entry.906535625=Soham%20Bonnerjee%20and%20Zhen%20Wei%20and%20%20Yeon%20and%20Anna%20Asch%20and%20Sagnik%20Nandy%20and%20Promit%20Ghosal&entry.1292438233=%20%20In-context%20learning%20%28ICL%29-the%20ability%20of%20transformer-based%20models%20to%20perform%0Anew%20tasks%20from%20examples%20provided%20at%20inference%20time-has%20emerged%20as%20a%20hallmark%20of%0Amodern%20language%20models.%20While%20recent%20works%20have%20investigated%20the%20mechanisms%0Aunderlying%20ICL%2C%20its%20feasibility%20under%20formal%20privacy%20constraints%20remains%0Alargely%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20differentially%20private%0Apretraining%20algorithm%20for%20linear%20attention%20heads%20and%20present%20the%20first%0Atheoretical%20analysis%20of%20the%20privacy-accuracy%20trade-off%20for%20ICL%20in%20linear%0Aregression.%20Our%20results%20characterize%20the%20fundamental%20tension%20between%0Aoptimization%20and%20privacy-induced%20noise%2C%20formally%20capturing%20behaviors%20observed%0Ain%20private%20training%20via%20iterative%20methods.%20Additionally%2C%20we%20show%20that%20our%0Amethod%20is%20robust%20to%20adversarial%20perturbations%20of%20training%20prompts%2C%20unlike%0Astandard%20ridge%20regression.%20All%20theoretical%20findings%20are%20supported%20by%20extensive%0Asimulations%20across%20diverse%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16000v1&entry.124074799=Read"},
{"title": "Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural\n  Network for Edge Detection", "author": "Lei Xu and Mehmet Yamac and Mete Ahishali and Moncef Gabbouj", "abstract": "  Edge detection has attracted considerable attention thanks to its exceptional\nability to enhance performance in downstream computer vision tasks. In recent\nyears, various deep learning methods have been explored for edge detection\ntasks resulting in a significant performance improvement compared to\nconventional computer vision algorithms. In neural networks, edge detection\ntasks require considerably large receptive fields to provide satisfactory\nperformance. In a typical convolutional operation, such a large receptive field\ncan be achieved by utilizing a significant number of consecutive layers, which\nyields deep network structures. Recently, a Multi-scale Tensorial Summation\n(MTS) factorization operator was presented, which can achieve very large\nreceptive fields even from the initial layers. In this paper, we propose a\nnovel MTS Dimensional Reduction (MTS-DR) module guided neural network,\nMTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and\ncorresponding MTS-DR blocks as a new backbone to remove redundant information\ninitially. Such a dimensional reduction module enables the neural network to\nfocus specifically on relevant information (i.e., necessary subspaces).\nFinally, a weight U-shaped refinement module follows MTS-DR blocks in the\nMTS-DR-Net. We conducted extensive experiments on two benchmark edge detection\ndatasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The\nimplementation of the proposed MTS-DR-Net can be found at\nhttps://github.com/LeiXuAI/MTS-DR-Net.git.\n", "link": "http://arxiv.org/abs/2504.15770v1", "date": "2025-04-22", "relevancy": 2.3467, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6016}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5766}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Tensorial%20Summation%20and%20Dimensional%20Reduction%20Guided%20Neural%0A%20%20Network%20for%20Edge%20Detection&body=Title%3A%20Multi-Scale%20Tensorial%20Summation%20and%20Dimensional%20Reduction%20Guided%20Neural%0A%20%20Network%20for%20Edge%20Detection%0AAuthor%3A%20Lei%20Xu%20and%20Mehmet%20Yamac%20and%20Mete%20Ahishali%20and%20Moncef%20Gabbouj%0AAbstract%3A%20%20%20Edge%20detection%20has%20attracted%20considerable%20attention%20thanks%20to%20its%20exceptional%0Aability%20to%20enhance%20performance%20in%20downstream%20computer%20vision%20tasks.%20In%20recent%0Ayears%2C%20various%20deep%20learning%20methods%20have%20been%20explored%20for%20edge%20detection%0Atasks%20resulting%20in%20a%20significant%20performance%20improvement%20compared%20to%0Aconventional%20computer%20vision%20algorithms.%20In%20neural%20networks%2C%20edge%20detection%0Atasks%20require%20considerably%20large%20receptive%20fields%20to%20provide%20satisfactory%0Aperformance.%20In%20a%20typical%20convolutional%20operation%2C%20such%20a%20large%20receptive%20field%0Acan%20be%20achieved%20by%20utilizing%20a%20significant%20number%20of%20consecutive%20layers%2C%20which%0Ayields%20deep%20network%20structures.%20Recently%2C%20a%20Multi-scale%20Tensorial%20Summation%0A%28MTS%29%20factorization%20operator%20was%20presented%2C%20which%20can%20achieve%20very%20large%0Areceptive%20fields%20even%20from%20the%20initial%20layers.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20MTS%20Dimensional%20Reduction%20%28MTS-DR%29%20module%20guided%20neural%20network%2C%0AMTS-DR-Net%2C%20for%20the%20edge%20detection%20task.%20The%20MTS-DR-Net%20uses%20MTS%20layers%2C%20and%0Acorresponding%20MTS-DR%20blocks%20as%20a%20new%20backbone%20to%20remove%20redundant%20information%0Ainitially.%20Such%20a%20dimensional%20reduction%20module%20enables%20the%20neural%20network%20to%0Afocus%20specifically%20on%20relevant%20information%20%28i.e.%2C%20necessary%20subspaces%29.%0AFinally%2C%20a%20weight%20U-shaped%20refinement%20module%20follows%20MTS-DR%20blocks%20in%20the%0AMTS-DR-Net.%20We%20conducted%20extensive%20experiments%20on%20two%20benchmark%20edge%20detection%0Adatasets%3A%20BSDS500%20and%20BIPEDv2%20to%20verify%20the%20effectiveness%20of%20our%20model.%20The%0Aimplementation%20of%20the%20proposed%20MTS-DR-Net%20can%20be%20found%20at%0Ahttps%3A//github.com/LeiXuAI/MTS-DR-Net.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Tensorial%2520Summation%2520and%2520Dimensional%2520Reduction%2520Guided%2520Neural%250A%2520%2520Network%2520for%2520Edge%2520Detection%26entry.906535625%3DLei%2520Xu%2520and%2520Mehmet%2520Yamac%2520and%2520Mete%2520Ahishali%2520and%2520Moncef%2520Gabbouj%26entry.1292438233%3D%2520%2520Edge%2520detection%2520has%2520attracted%2520considerable%2520attention%2520thanks%2520to%2520its%2520exceptional%250Aability%2520to%2520enhance%2520performance%2520in%2520downstream%2520computer%2520vision%2520tasks.%2520In%2520recent%250Ayears%252C%2520various%2520deep%2520learning%2520methods%2520have%2520been%2520explored%2520for%2520edge%2520detection%250Atasks%2520resulting%2520in%2520a%2520significant%2520performance%2520improvement%2520compared%2520to%250Aconventional%2520computer%2520vision%2520algorithms.%2520In%2520neural%2520networks%252C%2520edge%2520detection%250Atasks%2520require%2520considerably%2520large%2520receptive%2520fields%2520to%2520provide%2520satisfactory%250Aperformance.%2520In%2520a%2520typical%2520convolutional%2520operation%252C%2520such%2520a%2520large%2520receptive%2520field%250Acan%2520be%2520achieved%2520by%2520utilizing%2520a%2520significant%2520number%2520of%2520consecutive%2520layers%252C%2520which%250Ayields%2520deep%2520network%2520structures.%2520Recently%252C%2520a%2520Multi-scale%2520Tensorial%2520Summation%250A%2528MTS%2529%2520factorization%2520operator%2520was%2520presented%252C%2520which%2520can%2520achieve%2520very%2520large%250Areceptive%2520fields%2520even%2520from%2520the%2520initial%2520layers.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520MTS%2520Dimensional%2520Reduction%2520%2528MTS-DR%2529%2520module%2520guided%2520neural%2520network%252C%250AMTS-DR-Net%252C%2520for%2520the%2520edge%2520detection%2520task.%2520The%2520MTS-DR-Net%2520uses%2520MTS%2520layers%252C%2520and%250Acorresponding%2520MTS-DR%2520blocks%2520as%2520a%2520new%2520backbone%2520to%2520remove%2520redundant%2520information%250Ainitially.%2520Such%2520a%2520dimensional%2520reduction%2520module%2520enables%2520the%2520neural%2520network%2520to%250Afocus%2520specifically%2520on%2520relevant%2520information%2520%2528i.e.%252C%2520necessary%2520subspaces%2529.%250AFinally%252C%2520a%2520weight%2520U-shaped%2520refinement%2520module%2520follows%2520MTS-DR%2520blocks%2520in%2520the%250AMTS-DR-Net.%2520We%2520conducted%2520extensive%2520experiments%2520on%2520two%2520benchmark%2520edge%2520detection%250Adatasets%253A%2520BSDS500%2520and%2520BIPEDv2%2520to%2520verify%2520the%2520effectiveness%2520of%2520our%2520model.%2520The%250Aimplementation%2520of%2520the%2520proposed%2520MTS-DR-Net%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/LeiXuAI/MTS-DR-Net.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Tensorial%20Summation%20and%20Dimensional%20Reduction%20Guided%20Neural%0A%20%20Network%20for%20Edge%20Detection&entry.906535625=Lei%20Xu%20and%20Mehmet%20Yamac%20and%20Mete%20Ahishali%20and%20Moncef%20Gabbouj&entry.1292438233=%20%20Edge%20detection%20has%20attracted%20considerable%20attention%20thanks%20to%20its%20exceptional%0Aability%20to%20enhance%20performance%20in%20downstream%20computer%20vision%20tasks.%20In%20recent%0Ayears%2C%20various%20deep%20learning%20methods%20have%20been%20explored%20for%20edge%20detection%0Atasks%20resulting%20in%20a%20significant%20performance%20improvement%20compared%20to%0Aconventional%20computer%20vision%20algorithms.%20In%20neural%20networks%2C%20edge%20detection%0Atasks%20require%20considerably%20large%20receptive%20fields%20to%20provide%20satisfactory%0Aperformance.%20In%20a%20typical%20convolutional%20operation%2C%20such%20a%20large%20receptive%20field%0Acan%20be%20achieved%20by%20utilizing%20a%20significant%20number%20of%20consecutive%20layers%2C%20which%0Ayields%20deep%20network%20structures.%20Recently%2C%20a%20Multi-scale%20Tensorial%20Summation%0A%28MTS%29%20factorization%20operator%20was%20presented%2C%20which%20can%20achieve%20very%20large%0Areceptive%20fields%20even%20from%20the%20initial%20layers.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20MTS%20Dimensional%20Reduction%20%28MTS-DR%29%20module%20guided%20neural%20network%2C%0AMTS-DR-Net%2C%20for%20the%20edge%20detection%20task.%20The%20MTS-DR-Net%20uses%20MTS%20layers%2C%20and%0Acorresponding%20MTS-DR%20blocks%20as%20a%20new%20backbone%20to%20remove%20redundant%20information%0Ainitially.%20Such%20a%20dimensional%20reduction%20module%20enables%20the%20neural%20network%20to%0Afocus%20specifically%20on%20relevant%20information%20%28i.e.%2C%20necessary%20subspaces%29.%0AFinally%2C%20a%20weight%20U-shaped%20refinement%20module%20follows%20MTS-DR%20blocks%20in%20the%0AMTS-DR-Net.%20We%20conducted%20extensive%20experiments%20on%20two%20benchmark%20edge%20detection%0Adatasets%3A%20BSDS500%20and%20BIPEDv2%20to%20verify%20the%20effectiveness%20of%20our%20model.%20The%0Aimplementation%20of%20the%20proposed%20MTS-DR-Net%20can%20be%20found%20at%0Ahttps%3A//github.com/LeiXuAI/MTS-DR-Net.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15770v1&entry.124074799=Read"},
{"title": "Visual Place Cell Encoding: A Computational Model for Spatial\n  Representation and Cognitive Mapping", "author": "Chance J. Hamilton and Alfredo Weitzenfeld", "abstract": "  This paper presents the Visual Place Cell Encoding (VPCE) model, a\nbiologically inspired computational framework for simulating place cell-like\nactivation using visual input. Drawing on evidence that visual landmarks play a\ncentral role in spatial encoding, the proposed VPCE model activates visual\nplace cells by clustering high-dimensional appearance features extracted from\nimages captured by a robot-mounted camera. Each cluster center defines a\nreceptive field, and activation is computed based on visual similarity using a\nradial basis function. We evaluate whether the resulting activation patterns\ncorrelate with key properties of biological place cells, including spatial\nproximity, orientation alignment, and boundary differentiation. Experiments\ndemonstrate that the VPCE can distinguish between visually similar yet\nspatially distinct locations and adapt to environment changes such as the\ninsertion or removal of walls. These results suggest that structured visual\ninput, even in the absence of motion cues or reward-driven learning, is\nsufficient to generate place-cell-like spatial representations and support\nbiologically inspired cognitive mapping.\n", "link": "http://arxiv.org/abs/2504.15953v1", "date": "2025-04-22", "relevancy": 2.3241, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Place%20Cell%20Encoding%3A%20A%20Computational%20Model%20for%20Spatial%0A%20%20Representation%20and%20Cognitive%20Mapping&body=Title%3A%20Visual%20Place%20Cell%20Encoding%3A%20A%20Computational%20Model%20for%20Spatial%0A%20%20Representation%20and%20Cognitive%20Mapping%0AAuthor%3A%20Chance%20J.%20Hamilton%20and%20Alfredo%20Weitzenfeld%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20Visual%20Place%20Cell%20Encoding%20%28VPCE%29%20model%2C%20a%0Abiologically%20inspired%20computational%20framework%20for%20simulating%20place%20cell-like%0Aactivation%20using%20visual%20input.%20Drawing%20on%20evidence%20that%20visual%20landmarks%20play%20a%0Acentral%20role%20in%20spatial%20encoding%2C%20the%20proposed%20VPCE%20model%20activates%20visual%0Aplace%20cells%20by%20clustering%20high-dimensional%20appearance%20features%20extracted%20from%0Aimages%20captured%20by%20a%20robot-mounted%20camera.%20Each%20cluster%20center%20defines%20a%0Areceptive%20field%2C%20and%20activation%20is%20computed%20based%20on%20visual%20similarity%20using%20a%0Aradial%20basis%20function.%20We%20evaluate%20whether%20the%20resulting%20activation%20patterns%0Acorrelate%20with%20key%20properties%20of%20biological%20place%20cells%2C%20including%20spatial%0Aproximity%2C%20orientation%20alignment%2C%20and%20boundary%20differentiation.%20Experiments%0Ademonstrate%20that%20the%20VPCE%20can%20distinguish%20between%20visually%20similar%20yet%0Aspatially%20distinct%20locations%20and%20adapt%20to%20environment%20changes%20such%20as%20the%0Ainsertion%20or%20removal%20of%20walls.%20These%20results%20suggest%20that%20structured%20visual%0Ainput%2C%20even%20in%20the%20absence%20of%20motion%20cues%20or%20reward-driven%20learning%2C%20is%0Asufficient%20to%20generate%20place-cell-like%20spatial%20representations%20and%20support%0Abiologically%20inspired%20cognitive%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Place%2520Cell%2520Encoding%253A%2520A%2520Computational%2520Model%2520for%2520Spatial%250A%2520%2520Representation%2520and%2520Cognitive%2520Mapping%26entry.906535625%3DChance%2520J.%2520Hamilton%2520and%2520Alfredo%2520Weitzenfeld%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520Visual%2520Place%2520Cell%2520Encoding%2520%2528VPCE%2529%2520model%252C%2520a%250Abiologically%2520inspired%2520computational%2520framework%2520for%2520simulating%2520place%2520cell-like%250Aactivation%2520using%2520visual%2520input.%2520Drawing%2520on%2520evidence%2520that%2520visual%2520landmarks%2520play%2520a%250Acentral%2520role%2520in%2520spatial%2520encoding%252C%2520the%2520proposed%2520VPCE%2520model%2520activates%2520visual%250Aplace%2520cells%2520by%2520clustering%2520high-dimensional%2520appearance%2520features%2520extracted%2520from%250Aimages%2520captured%2520by%2520a%2520robot-mounted%2520camera.%2520Each%2520cluster%2520center%2520defines%2520a%250Areceptive%2520field%252C%2520and%2520activation%2520is%2520computed%2520based%2520on%2520visual%2520similarity%2520using%2520a%250Aradial%2520basis%2520function.%2520We%2520evaluate%2520whether%2520the%2520resulting%2520activation%2520patterns%250Acorrelate%2520with%2520key%2520properties%2520of%2520biological%2520place%2520cells%252C%2520including%2520spatial%250Aproximity%252C%2520orientation%2520alignment%252C%2520and%2520boundary%2520differentiation.%2520Experiments%250Ademonstrate%2520that%2520the%2520VPCE%2520can%2520distinguish%2520between%2520visually%2520similar%2520yet%250Aspatially%2520distinct%2520locations%2520and%2520adapt%2520to%2520environment%2520changes%2520such%2520as%2520the%250Ainsertion%2520or%2520removal%2520of%2520walls.%2520These%2520results%2520suggest%2520that%2520structured%2520visual%250Ainput%252C%2520even%2520in%2520the%2520absence%2520of%2520motion%2520cues%2520or%2520reward-driven%2520learning%252C%2520is%250Asufficient%2520to%2520generate%2520place-cell-like%2520spatial%2520representations%2520and%2520support%250Abiologically%2520inspired%2520cognitive%2520mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Place%20Cell%20Encoding%3A%20A%20Computational%20Model%20for%20Spatial%0A%20%20Representation%20and%20Cognitive%20Mapping&entry.906535625=Chance%20J.%20Hamilton%20and%20Alfredo%20Weitzenfeld&entry.1292438233=%20%20This%20paper%20presents%20the%20Visual%20Place%20Cell%20Encoding%20%28VPCE%29%20model%2C%20a%0Abiologically%20inspired%20computational%20framework%20for%20simulating%20place%20cell-like%0Aactivation%20using%20visual%20input.%20Drawing%20on%20evidence%20that%20visual%20landmarks%20play%20a%0Acentral%20role%20in%20spatial%20encoding%2C%20the%20proposed%20VPCE%20model%20activates%20visual%0Aplace%20cells%20by%20clustering%20high-dimensional%20appearance%20features%20extracted%20from%0Aimages%20captured%20by%20a%20robot-mounted%20camera.%20Each%20cluster%20center%20defines%20a%0Areceptive%20field%2C%20and%20activation%20is%20computed%20based%20on%20visual%20similarity%20using%20a%0Aradial%20basis%20function.%20We%20evaluate%20whether%20the%20resulting%20activation%20patterns%0Acorrelate%20with%20key%20properties%20of%20biological%20place%20cells%2C%20including%20spatial%0Aproximity%2C%20orientation%20alignment%2C%20and%20boundary%20differentiation.%20Experiments%0Ademonstrate%20that%20the%20VPCE%20can%20distinguish%20between%20visually%20similar%20yet%0Aspatially%20distinct%20locations%20and%20adapt%20to%20environment%20changes%20such%20as%20the%0Ainsertion%20or%20removal%20of%20walls.%20These%20results%20suggest%20that%20structured%20visual%0Ainput%2C%20even%20in%20the%20absence%20of%20motion%20cues%20or%20reward-driven%20learning%2C%20is%0Asufficient%20to%20generate%20place-cell-like%20spatial%20representations%20and%20support%0Abiologically%20inspired%20cognitive%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15953v1&entry.124074799=Read"},
{"title": "Automated Creativity Evaluation for Large Language Models: A\n  Reference-Based Approach", "author": "Ruizhe Li and Chiwei Zhu and Benfeng Xu and Xiaorui Wang and Zhendong Mao", "abstract": "  Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%).\n", "link": "http://arxiv.org/abs/2504.15784v1", "date": "2025-04-22", "relevancy": 2.3198, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4651}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Creativity%20Evaluation%20for%20Large%20Language%20Models%3A%20A%0A%20%20Reference-Based%20Approach&body=Title%3A%20Automated%20Creativity%20Evaluation%20for%20Large%20Language%20Models%3A%20A%0A%20%20Reference-Based%20Approach%0AAuthor%3A%20Ruizhe%20Li%20and%20Chiwei%20Zhu%20and%20Benfeng%20Xu%20and%20Xiaorui%20Wang%20and%20Zhendong%20Mao%0AAbstract%3A%20%20%20Creative%20writing%20is%20a%20key%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20with%0Apotential%20applications%20in%20literature%2C%20storytelling%2C%20and%20various%20creative%0Adomains.%20However%2C%20evaluating%20the%20creativity%20of%20machine-generated%20texts%20remains%0Aa%20significant%20challenge%2C%20as%20existing%20methods%20either%20rely%20on%20costly%20manual%0Aannotations%20or%20fail%20to%20align%20closely%20with%20human%20assessments.%20In%20this%20paper%2C%20we%0Apropose%20an%20effective%20automated%20evaluation%20method%20based%20on%20the%20Torrance%20Test%20of%0ACreative%20Writing%20%28TTCW%29%2C%20which%20evaluates%20creativity%20as%20product.%20Our%20method%0Aemploys%20a%20reference-based%20Likert-style%20approach%2C%20scoring%20generated%20creative%0Atexts%20relative%20to%20high-quality%20reference%20texts%20across%20various%20tests.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20improves%20the%0Aalignment%20between%20LLM%20evaluations%20and%20human%20assessments%2C%20achieving%20a%20pairwise%0Aaccuracy%20of%200.75%20%28%2B15%5C%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Creativity%2520Evaluation%2520for%2520Large%2520Language%2520Models%253A%2520A%250A%2520%2520Reference-Based%2520Approach%26entry.906535625%3DRuizhe%2520Li%2520and%2520Chiwei%2520Zhu%2520and%2520Benfeng%2520Xu%2520and%2520Xiaorui%2520Wang%2520and%2520Zhendong%2520Mao%26entry.1292438233%3D%2520%2520Creative%2520writing%2520is%2520a%2520key%2520capability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520with%250Apotential%2520applications%2520in%2520literature%252C%2520storytelling%252C%2520and%2520various%2520creative%250Adomains.%2520However%252C%2520evaluating%2520the%2520creativity%2520of%2520machine-generated%2520texts%2520remains%250Aa%2520significant%2520challenge%252C%2520as%2520existing%2520methods%2520either%2520rely%2520on%2520costly%2520manual%250Aannotations%2520or%2520fail%2520to%2520align%2520closely%2520with%2520human%2520assessments.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520an%2520effective%2520automated%2520evaluation%2520method%2520based%2520on%2520the%2520Torrance%2520Test%2520of%250ACreative%2520Writing%2520%2528TTCW%2529%252C%2520which%2520evaluates%2520creativity%2520as%2520product.%2520Our%2520method%250Aemploys%2520a%2520reference-based%2520Likert-style%2520approach%252C%2520scoring%2520generated%2520creative%250Atexts%2520relative%2520to%2520high-quality%2520reference%2520texts%2520across%2520various%2520tests.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520significantly%2520improves%2520the%250Aalignment%2520between%2520LLM%2520evaluations%2520and%2520human%2520assessments%252C%2520achieving%2520a%2520pairwise%250Aaccuracy%2520of%25200.75%2520%2528%252B15%255C%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Creativity%20Evaluation%20for%20Large%20Language%20Models%3A%20A%0A%20%20Reference-Based%20Approach&entry.906535625=Ruizhe%20Li%20and%20Chiwei%20Zhu%20and%20Benfeng%20Xu%20and%20Xiaorui%20Wang%20and%20Zhendong%20Mao&entry.1292438233=%20%20Creative%20writing%20is%20a%20key%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20with%0Apotential%20applications%20in%20literature%2C%20storytelling%2C%20and%20various%20creative%0Adomains.%20However%2C%20evaluating%20the%20creativity%20of%20machine-generated%20texts%20remains%0Aa%20significant%20challenge%2C%20as%20existing%20methods%20either%20rely%20on%20costly%20manual%0Aannotations%20or%20fail%20to%20align%20closely%20with%20human%20assessments.%20In%20this%20paper%2C%20we%0Apropose%20an%20effective%20automated%20evaluation%20method%20based%20on%20the%20Torrance%20Test%20of%0ACreative%20Writing%20%28TTCW%29%2C%20which%20evaluates%20creativity%20as%20product.%20Our%20method%0Aemploys%20a%20reference-based%20Likert-style%20approach%2C%20scoring%20generated%20creative%0Atexts%20relative%20to%20high-quality%20reference%20texts%20across%20various%20tests.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20improves%20the%0Aalignment%20between%20LLM%20evaluations%20and%20human%20assessments%2C%20achieving%20a%20pairwise%0Aaccuracy%20of%200.75%20%28%2B15%5C%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15784v1&entry.124074799=Read"},
{"title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for\n  Subject-Driven Text-to-Image Generation", "author": "Zebin Yao and Lei Ren and Huixing Jiang and Chen Wei and Xiaojie Wang and Ruifan Li and Fangxiang Feng", "abstract": "  Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor.\n", "link": "http://arxiv.org/abs/2504.15958v1", "date": "2025-04-22", "relevancy": 2.2847, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5927}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5672}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeGraftor%3A%20Training-Free%20Cross-Image%20Feature%20Grafting%20for%0A%20%20Subject-Driven%20Text-to-Image%20Generation&body=Title%3A%20FreeGraftor%3A%20Training-Free%20Cross-Image%20Feature%20Grafting%20for%0A%20%20Subject-Driven%20Text-to-Image%20Generation%0AAuthor%3A%20Zebin%20Yao%20and%20Lei%20Ren%20and%20Huixing%20Jiang%20and%20Chen%20Wei%20and%20Xiaojie%20Wang%20and%20Ruifan%20Li%20and%20Fangxiang%20Feng%0AAbstract%3A%20%20%20Subject-driven%20image%20generation%20aims%20to%20synthesize%20novel%20scenes%20that%0Afaithfully%20preserve%20subject%20identity%20from%20reference%20images%20while%20adhering%20to%0Atextual%20guidance%2C%20yet%20existing%20methods%20struggle%20with%20a%20critical%20trade-off%0Abetween%20fidelity%20and%20efficiency.%20Tuning-based%20approaches%20rely%20on%20time-consuming%0Aand%20resource-intensive%20subject-specific%20optimization%2C%20while%20zero-shot%20methods%0Afail%20to%20maintain%20adequate%20subject%20consistency.%20In%20this%20work%2C%20we%20propose%0AFreeGraftor%2C%20a%20training-free%20framework%20that%20addresses%20these%20limitations%20through%0Across-image%20feature%20grafting.%20Specifically%2C%20FreeGraftor%20employs%20semantic%0Amatching%20and%20position-constrained%20attention%20fusion%20to%20transfer%20visual%20details%0Afrom%20reference%20subjects%20to%20the%20generated%20image.%20Additionally%2C%20our%20framework%0Aincorporates%20a%20novel%20noise%20initialization%20strategy%20to%20preserve%20geometry%20priors%0Aof%20reference%20subjects%20for%20robust%20feature%20matching.%20Extensive%20qualitative%20and%0Aquantitative%20experiments%20demonstrate%20that%20our%20method%20enables%20precise%20subject%0Aidentity%20transfer%20while%20maintaining%20text-aligned%20scene%20synthesis.%20Without%0Arequiring%20model%20fine-tuning%20or%20additional%20training%2C%20FreeGraftor%20significantly%0Aoutperforms%20existing%20zero-shot%20and%20training-free%20approaches%20in%20both%20subject%0Afidelity%20and%20text%20alignment.%20Furthermore%2C%20our%20framework%20can%20seamlessly%20extend%0Ato%20multi-subject%20generation%2C%20making%20it%20practical%20for%20real-world%20deployment.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/Nihukat/FreeGraftor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeGraftor%253A%2520Training-Free%2520Cross-Image%2520Feature%2520Grafting%2520for%250A%2520%2520Subject-Driven%2520Text-to-Image%2520Generation%26entry.906535625%3DZebin%2520Yao%2520and%2520Lei%2520Ren%2520and%2520Huixing%2520Jiang%2520and%2520Chen%2520Wei%2520and%2520Xiaojie%2520Wang%2520and%2520Ruifan%2520Li%2520and%2520Fangxiang%2520Feng%26entry.1292438233%3D%2520%2520Subject-driven%2520image%2520generation%2520aims%2520to%2520synthesize%2520novel%2520scenes%2520that%250Afaithfully%2520preserve%2520subject%2520identity%2520from%2520reference%2520images%2520while%2520adhering%2520to%250Atextual%2520guidance%252C%2520yet%2520existing%2520methods%2520struggle%2520with%2520a%2520critical%2520trade-off%250Abetween%2520fidelity%2520and%2520efficiency.%2520Tuning-based%2520approaches%2520rely%2520on%2520time-consuming%250Aand%2520resource-intensive%2520subject-specific%2520optimization%252C%2520while%2520zero-shot%2520methods%250Afail%2520to%2520maintain%2520adequate%2520subject%2520consistency.%2520In%2520this%2520work%252C%2520we%2520propose%250AFreeGraftor%252C%2520a%2520training-free%2520framework%2520that%2520addresses%2520these%2520limitations%2520through%250Across-image%2520feature%2520grafting.%2520Specifically%252C%2520FreeGraftor%2520employs%2520semantic%250Amatching%2520and%2520position-constrained%2520attention%2520fusion%2520to%2520transfer%2520visual%2520details%250Afrom%2520reference%2520subjects%2520to%2520the%2520generated%2520image.%2520Additionally%252C%2520our%2520framework%250Aincorporates%2520a%2520novel%2520noise%2520initialization%2520strategy%2520to%2520preserve%2520geometry%2520priors%250Aof%2520reference%2520subjects%2520for%2520robust%2520feature%2520matching.%2520Extensive%2520qualitative%2520and%250Aquantitative%2520experiments%2520demonstrate%2520that%2520our%2520method%2520enables%2520precise%2520subject%250Aidentity%2520transfer%2520while%2520maintaining%2520text-aligned%2520scene%2520synthesis.%2520Without%250Arequiring%2520model%2520fine-tuning%2520or%2520additional%2520training%252C%2520FreeGraftor%2520significantly%250Aoutperforms%2520existing%2520zero-shot%2520and%2520training-free%2520approaches%2520in%2520both%2520subject%250Afidelity%2520and%2520text%2520alignment.%2520Furthermore%252C%2520our%2520framework%2520can%2520seamlessly%2520extend%250Ato%2520multi-subject%2520generation%252C%2520making%2520it%2520practical%2520for%2520real-world%2520deployment.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/Nihukat/FreeGraftor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeGraftor%3A%20Training-Free%20Cross-Image%20Feature%20Grafting%20for%0A%20%20Subject-Driven%20Text-to-Image%20Generation&entry.906535625=Zebin%20Yao%20and%20Lei%20Ren%20and%20Huixing%20Jiang%20and%20Chen%20Wei%20and%20Xiaojie%20Wang%20and%20Ruifan%20Li%20and%20Fangxiang%20Feng&entry.1292438233=%20%20Subject-driven%20image%20generation%20aims%20to%20synthesize%20novel%20scenes%20that%0Afaithfully%20preserve%20subject%20identity%20from%20reference%20images%20while%20adhering%20to%0Atextual%20guidance%2C%20yet%20existing%20methods%20struggle%20with%20a%20critical%20trade-off%0Abetween%20fidelity%20and%20efficiency.%20Tuning-based%20approaches%20rely%20on%20time-consuming%0Aand%20resource-intensive%20subject-specific%20optimization%2C%20while%20zero-shot%20methods%0Afail%20to%20maintain%20adequate%20subject%20consistency.%20In%20this%20work%2C%20we%20propose%0AFreeGraftor%2C%20a%20training-free%20framework%20that%20addresses%20these%20limitations%20through%0Across-image%20feature%20grafting.%20Specifically%2C%20FreeGraftor%20employs%20semantic%0Amatching%20and%20position-constrained%20attention%20fusion%20to%20transfer%20visual%20details%0Afrom%20reference%20subjects%20to%20the%20generated%20image.%20Additionally%2C%20our%20framework%0Aincorporates%20a%20novel%20noise%20initialization%20strategy%20to%20preserve%20geometry%20priors%0Aof%20reference%20subjects%20for%20robust%20feature%20matching.%20Extensive%20qualitative%20and%0Aquantitative%20experiments%20demonstrate%20that%20our%20method%20enables%20precise%20subject%0Aidentity%20transfer%20while%20maintaining%20text-aligned%20scene%20synthesis.%20Without%0Arequiring%20model%20fine-tuning%20or%20additional%20training%2C%20FreeGraftor%20significantly%0Aoutperforms%20existing%20zero-shot%20and%20training-free%20approaches%20in%20both%20subject%0Afidelity%20and%20text%20alignment.%20Furthermore%2C%20our%20framework%20can%20seamlessly%20extend%0Ato%20multi-subject%20generation%2C%20making%20it%20practical%20for%20real-world%20deployment.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/Nihukat/FreeGraftor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15958v1&entry.124074799=Read"},
{"title": "Locating and Mitigating Gradient Conflicts in Point Cloud Domain\n  Adaptation via Saliency Map Skewness", "author": "Jiaqi Tang and Yinsong Xu and Qingchao Chen", "abstract": "  Object classification models utilizing point cloud data are fundamental for\n3D media understanding, yet they often struggle with unseen or\nout-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain\nadaptation (UDA) methods typically employ a multi-task learning (MTL) framework\nthat combines primary classification tasks with auxiliary self-supervision\ntasks to bridge the gap between cross-domain feature distributions. However,\nour further experiments demonstrate that not all gradients from\nself-supervision tasks are beneficial and some may negatively impact the\nclassification performance. In this paper, we propose a novel solution, termed\nSaliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient\nconflicts. Specifically, our method designs a new scoring mechanism based on\nthe skewness of 3D saliency maps to estimate gradient conflicts without\nrequiring target labels. Leveraging this, we develop a sample selection\nstrategy that dynamically filters out samples whose self-supervision gradients\nare not beneficial for the classification. Our approach is scalable,\nintroducing modest computational overhead, and can be integrated into all the\npoint cloud UDA MTL frameworks. Extensive evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches. In addition, we provide a new\nperspective on understanding the UDA problem through back-propagation analysis.\n", "link": "http://arxiv.org/abs/2504.15796v1", "date": "2025-04-22", "relevancy": 2.2819, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5722}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5695}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locating%20and%20Mitigating%20Gradient%20Conflicts%20in%20Point%20Cloud%20Domain%0A%20%20Adaptation%20via%20Saliency%20Map%20Skewness&body=Title%3A%20Locating%20and%20Mitigating%20Gradient%20Conflicts%20in%20Point%20Cloud%20Domain%0A%20%20Adaptation%20via%20Saliency%20Map%20Skewness%0AAuthor%3A%20Jiaqi%20Tang%20and%20Yinsong%20Xu%20and%20Qingchao%20Chen%0AAbstract%3A%20%20%20Object%20classification%20models%20utilizing%20point%20cloud%20data%20are%20fundamental%20for%0A3D%20media%20understanding%2C%20yet%20they%20often%20struggle%20with%20unseen%20or%0Aout-of-distribution%20%28OOD%29%20scenarios.%20Existing%20point%20cloud%20unsupervised%20domain%0Aadaptation%20%28UDA%29%20methods%20typically%20employ%20a%20multi-task%20learning%20%28MTL%29%20framework%0Athat%20combines%20primary%20classification%20tasks%20with%20auxiliary%20self-supervision%0Atasks%20to%20bridge%20the%20gap%20between%20cross-domain%20feature%20distributions.%20However%2C%0Aour%20further%20experiments%20demonstrate%20that%20not%20all%20gradients%20from%0Aself-supervision%20tasks%20are%20beneficial%20and%20some%20may%20negatively%20impact%20the%0Aclassification%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20solution%2C%20termed%0ASaliency%20Map-based%20Data%20Sampling%20Block%20%28SM-DSB%29%2C%20to%20mitigate%20these%20gradient%0Aconflicts.%20Specifically%2C%20our%20method%20designs%20a%20new%20scoring%20mechanism%20based%20on%0Athe%20skewness%20of%203D%20saliency%20maps%20to%20estimate%20gradient%20conflicts%20without%0Arequiring%20target%20labels.%20Leveraging%20this%2C%20we%20develop%20a%20sample%20selection%0Astrategy%20that%20dynamically%20filters%20out%20samples%20whose%20self-supervision%20gradients%0Aare%20not%20beneficial%20for%20the%20classification.%20Our%20approach%20is%20scalable%2C%0Aintroducing%20modest%20computational%20overhead%2C%20and%20can%20be%20integrated%20into%20all%20the%0Apoint%20cloud%20UDA%20MTL%20frameworks.%20Extensive%20evaluations%20demonstrate%20that%20our%0Amethod%20outperforms%20state-of-the-art%20approaches.%20In%20addition%2C%20we%20provide%20a%20new%0Aperspective%20on%20understanding%20the%20UDA%20problem%20through%20back-propagation%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocating%2520and%2520Mitigating%2520Gradient%2520Conflicts%2520in%2520Point%2520Cloud%2520Domain%250A%2520%2520Adaptation%2520via%2520Saliency%2520Map%2520Skewness%26entry.906535625%3DJiaqi%2520Tang%2520and%2520Yinsong%2520Xu%2520and%2520Qingchao%2520Chen%26entry.1292438233%3D%2520%2520Object%2520classification%2520models%2520utilizing%2520point%2520cloud%2520data%2520are%2520fundamental%2520for%250A3D%2520media%2520understanding%252C%2520yet%2520they%2520often%2520struggle%2520with%2520unseen%2520or%250Aout-of-distribution%2520%2528OOD%2529%2520scenarios.%2520Existing%2520point%2520cloud%2520unsupervised%2520domain%250Aadaptation%2520%2528UDA%2529%2520methods%2520typically%2520employ%2520a%2520multi-task%2520learning%2520%2528MTL%2529%2520framework%250Athat%2520combines%2520primary%2520classification%2520tasks%2520with%2520auxiliary%2520self-supervision%250Atasks%2520to%2520bridge%2520the%2520gap%2520between%2520cross-domain%2520feature%2520distributions.%2520However%252C%250Aour%2520further%2520experiments%2520demonstrate%2520that%2520not%2520all%2520gradients%2520from%250Aself-supervision%2520tasks%2520are%2520beneficial%2520and%2520some%2520may%2520negatively%2520impact%2520the%250Aclassification%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520solution%252C%2520termed%250ASaliency%2520Map-based%2520Data%2520Sampling%2520Block%2520%2528SM-DSB%2529%252C%2520to%2520mitigate%2520these%2520gradient%250Aconflicts.%2520Specifically%252C%2520our%2520method%2520designs%2520a%2520new%2520scoring%2520mechanism%2520based%2520on%250Athe%2520skewness%2520of%25203D%2520saliency%2520maps%2520to%2520estimate%2520gradient%2520conflicts%2520without%250Arequiring%2520target%2520labels.%2520Leveraging%2520this%252C%2520we%2520develop%2520a%2520sample%2520selection%250Astrategy%2520that%2520dynamically%2520filters%2520out%2520samples%2520whose%2520self-supervision%2520gradients%250Aare%2520not%2520beneficial%2520for%2520the%2520classification.%2520Our%2520approach%2520is%2520scalable%252C%250Aintroducing%2520modest%2520computational%2520overhead%252C%2520and%2520can%2520be%2520integrated%2520into%2520all%2520the%250Apoint%2520cloud%2520UDA%2520MTL%2520frameworks.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520state-of-the-art%2520approaches.%2520In%2520addition%252C%2520we%2520provide%2520a%2520new%250Aperspective%2520on%2520understanding%2520the%2520UDA%2520problem%2520through%2520back-propagation%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locating%20and%20Mitigating%20Gradient%20Conflicts%20in%20Point%20Cloud%20Domain%0A%20%20Adaptation%20via%20Saliency%20Map%20Skewness&entry.906535625=Jiaqi%20Tang%20and%20Yinsong%20Xu%20and%20Qingchao%20Chen&entry.1292438233=%20%20Object%20classification%20models%20utilizing%20point%20cloud%20data%20are%20fundamental%20for%0A3D%20media%20understanding%2C%20yet%20they%20often%20struggle%20with%20unseen%20or%0Aout-of-distribution%20%28OOD%29%20scenarios.%20Existing%20point%20cloud%20unsupervised%20domain%0Aadaptation%20%28UDA%29%20methods%20typically%20employ%20a%20multi-task%20learning%20%28MTL%29%20framework%0Athat%20combines%20primary%20classification%20tasks%20with%20auxiliary%20self-supervision%0Atasks%20to%20bridge%20the%20gap%20between%20cross-domain%20feature%20distributions.%20However%2C%0Aour%20further%20experiments%20demonstrate%20that%20not%20all%20gradients%20from%0Aself-supervision%20tasks%20are%20beneficial%20and%20some%20may%20negatively%20impact%20the%0Aclassification%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20solution%2C%20termed%0ASaliency%20Map-based%20Data%20Sampling%20Block%20%28SM-DSB%29%2C%20to%20mitigate%20these%20gradient%0Aconflicts.%20Specifically%2C%20our%20method%20designs%20a%20new%20scoring%20mechanism%20based%20on%0Athe%20skewness%20of%203D%20saliency%20maps%20to%20estimate%20gradient%20conflicts%20without%0Arequiring%20target%20labels.%20Leveraging%20this%2C%20we%20develop%20a%20sample%20selection%0Astrategy%20that%20dynamically%20filters%20out%20samples%20whose%20self-supervision%20gradients%0Aare%20not%20beneficial%20for%20the%20classification.%20Our%20approach%20is%20scalable%2C%0Aintroducing%20modest%20computational%20overhead%2C%20and%20can%20be%20integrated%20into%20all%20the%0Apoint%20cloud%20UDA%20MTL%20frameworks.%20Extensive%20evaluations%20demonstrate%20that%20our%0Amethod%20outperforms%20state-of-the-art%20approaches.%20In%20addition%2C%20we%20provide%20a%20new%0Aperspective%20on%20understanding%20the%20UDA%20problem%20through%20back-propagation%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15796v1&entry.124074799=Read"},
{"title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language\n  Models", "author": "Saban Ozturk and Melih B. Yilmaz and Muti Kara and M. Talat Yavuz and Aykut Ko\u00e7 and Tolga \u00c7ukur", "abstract": "  Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods.\n", "link": "http://arxiv.org/abs/2504.15929v1", "date": "2025-04-22", "relevancy": 2.2729, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6077}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Entity%20Driven%20Triplet%20Mining%20for%20Aligning%20Medical%20Vision-Language%0A%20%20Models&body=Title%3A%20Meta-Entity%20Driven%20Triplet%20Mining%20for%20Aligning%20Medical%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Saban%20Ozturk%20and%20Melih%20B.%20Yilmaz%20and%20Muti%20Kara%20and%20M.%20Talat%20Yavuz%20and%20Aykut%20Ko%C3%A7%20and%20Tolga%20%C3%87ukur%0AAbstract%3A%20%20%20Diagnostic%20imaging%20relies%20on%20interpreting%20both%20images%20and%20radiology%20reports%2C%0Abut%20the%20growing%20data%20volumes%20place%20significant%20pressure%20on%20medical%20experts%2C%0Ayielding%20increased%20errors%20and%20workflow%20backlogs.%20Medical%20vision-language%20models%0A%28med-VLMs%29%20have%20emerged%20as%20a%20powerful%20framework%20to%20efficiently%20process%0Amultimodal%20imaging%20data%2C%20particularly%20in%20chest%20X-ray%20%28CXR%29%20evaluations%2C%20albeit%0Atheir%20performance%20hinges%20on%20how%20well%20image%20and%20text%20representations%20are%0Aaligned.%20Existing%20alignment%20methods%2C%20predominantly%20based%20on%20contrastive%0Alearning%2C%20prioritize%20separation%20between%20disease%20classes%20over%20segregation%20of%0Afine-grained%20pathology%20attributes%20like%20location%2C%20size%20or%20severity%2C%20leading%20to%0Asuboptimal%20representations.%20Here%2C%20we%20propose%20MedTrim%20%28Meta-entity-driven%0ATriplet%20mining%29%2C%20a%20novel%20method%20that%20enhances%20image-text%20alignment%20through%0Amultimodal%20triplet%20learning%20synergistically%20guided%20by%20disease%20class%20as%20well%20as%0Aadjectival%20and%20directional%20pathology%20descriptors.%20Unlike%20common%20alignment%0Amethods%20that%20separate%20broad%20disease%20classes%2C%20MedTrim%20leverages%20structured%0Ameta-entity%20information%20to%20preserve%20subtle%20but%20clinically%20significant%0Aintra-class%20variations.%20For%20this%20purpose%2C%20we%20first%20introduce%20an%20ontology-based%0Aentity%20recognition%20module%20that%20extracts%20pathology-specific%20meta-entities%20from%0ACXR%20reports%2C%20as%20annotations%20on%20pathology%20attributes%20are%20rare%20in%20public%0Adatasets.%20For%20refined%20sample%20selection%20in%20triplet%20mining%2C%20we%20then%20introduce%20a%0Anovel%20score%20function%20that%20captures%20an%20aggregate%20measure%20of%20inter-sample%0Asimilarity%20based%20on%20disease%20classes%20and%20adjectival/directional%20descriptors.%0ALastly%2C%20we%20introduce%20a%20multimodal%20triplet%20alignment%20objective%20for%20explicit%0Awithin-%20and%20cross-modal%20alignment%20between%20samples%20sharing%20detailed%20pathology%0Acharacteristics.%20Our%20demonstrations%20indicate%20that%20MedTrim%20improves%20performance%0Ain%20downstream%20retrieval%20and%20classification%20tasks%20compared%20to%20state-of-the-art%0Aalignment%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Entity%2520Driven%2520Triplet%2520Mining%2520for%2520Aligning%2520Medical%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DSaban%2520Ozturk%2520and%2520Melih%2520B.%2520Yilmaz%2520and%2520Muti%2520Kara%2520and%2520M.%2520Talat%2520Yavuz%2520and%2520Aykut%2520Ko%25C3%25A7%2520and%2520Tolga%2520%25C3%2587ukur%26entry.1292438233%3D%2520%2520Diagnostic%2520imaging%2520relies%2520on%2520interpreting%2520both%2520images%2520and%2520radiology%2520reports%252C%250Abut%2520the%2520growing%2520data%2520volumes%2520place%2520significant%2520pressure%2520on%2520medical%2520experts%252C%250Ayielding%2520increased%2520errors%2520and%2520workflow%2520backlogs.%2520Medical%2520vision-language%2520models%250A%2528med-VLMs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520framework%2520to%2520efficiently%2520process%250Amultimodal%2520imaging%2520data%252C%2520particularly%2520in%2520chest%2520X-ray%2520%2528CXR%2529%2520evaluations%252C%2520albeit%250Atheir%2520performance%2520hinges%2520on%2520how%2520well%2520image%2520and%2520text%2520representations%2520are%250Aaligned.%2520Existing%2520alignment%2520methods%252C%2520predominantly%2520based%2520on%2520contrastive%250Alearning%252C%2520prioritize%2520separation%2520between%2520disease%2520classes%2520over%2520segregation%2520of%250Afine-grained%2520pathology%2520attributes%2520like%2520location%252C%2520size%2520or%2520severity%252C%2520leading%2520to%250Asuboptimal%2520representations.%2520Here%252C%2520we%2520propose%2520MedTrim%2520%2528Meta-entity-driven%250ATriplet%2520mining%2529%252C%2520a%2520novel%2520method%2520that%2520enhances%2520image-text%2520alignment%2520through%250Amultimodal%2520triplet%2520learning%2520synergistically%2520guided%2520by%2520disease%2520class%2520as%2520well%2520as%250Aadjectival%2520and%2520directional%2520pathology%2520descriptors.%2520Unlike%2520common%2520alignment%250Amethods%2520that%2520separate%2520broad%2520disease%2520classes%252C%2520MedTrim%2520leverages%2520structured%250Ameta-entity%2520information%2520to%2520preserve%2520subtle%2520but%2520clinically%2520significant%250Aintra-class%2520variations.%2520For%2520this%2520purpose%252C%2520we%2520first%2520introduce%2520an%2520ontology-based%250Aentity%2520recognition%2520module%2520that%2520extracts%2520pathology-specific%2520meta-entities%2520from%250ACXR%2520reports%252C%2520as%2520annotations%2520on%2520pathology%2520attributes%2520are%2520rare%2520in%2520public%250Adatasets.%2520For%2520refined%2520sample%2520selection%2520in%2520triplet%2520mining%252C%2520we%2520then%2520introduce%2520a%250Anovel%2520score%2520function%2520that%2520captures%2520an%2520aggregate%2520measure%2520of%2520inter-sample%250Asimilarity%2520based%2520on%2520disease%2520classes%2520and%2520adjectival/directional%2520descriptors.%250ALastly%252C%2520we%2520introduce%2520a%2520multimodal%2520triplet%2520alignment%2520objective%2520for%2520explicit%250Awithin-%2520and%2520cross-modal%2520alignment%2520between%2520samples%2520sharing%2520detailed%2520pathology%250Acharacteristics.%2520Our%2520demonstrations%2520indicate%2520that%2520MedTrim%2520improves%2520performance%250Ain%2520downstream%2520retrieval%2520and%2520classification%2520tasks%2520compared%2520to%2520state-of-the-art%250Aalignment%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Entity%20Driven%20Triplet%20Mining%20for%20Aligning%20Medical%20Vision-Language%0A%20%20Models&entry.906535625=Saban%20Ozturk%20and%20Melih%20B.%20Yilmaz%20and%20Muti%20Kara%20and%20M.%20Talat%20Yavuz%20and%20Aykut%20Ko%C3%A7%20and%20Tolga%20%C3%87ukur&entry.1292438233=%20%20Diagnostic%20imaging%20relies%20on%20interpreting%20both%20images%20and%20radiology%20reports%2C%0Abut%20the%20growing%20data%20volumes%20place%20significant%20pressure%20on%20medical%20experts%2C%0Ayielding%20increased%20errors%20and%20workflow%20backlogs.%20Medical%20vision-language%20models%0A%28med-VLMs%29%20have%20emerged%20as%20a%20powerful%20framework%20to%20efficiently%20process%0Amultimodal%20imaging%20data%2C%20particularly%20in%20chest%20X-ray%20%28CXR%29%20evaluations%2C%20albeit%0Atheir%20performance%20hinges%20on%20how%20well%20image%20and%20text%20representations%20are%0Aaligned.%20Existing%20alignment%20methods%2C%20predominantly%20based%20on%20contrastive%0Alearning%2C%20prioritize%20separation%20between%20disease%20classes%20over%20segregation%20of%0Afine-grained%20pathology%20attributes%20like%20location%2C%20size%20or%20severity%2C%20leading%20to%0Asuboptimal%20representations.%20Here%2C%20we%20propose%20MedTrim%20%28Meta-entity-driven%0ATriplet%20mining%29%2C%20a%20novel%20method%20that%20enhances%20image-text%20alignment%20through%0Amultimodal%20triplet%20learning%20synergistically%20guided%20by%20disease%20class%20as%20well%20as%0Aadjectival%20and%20directional%20pathology%20descriptors.%20Unlike%20common%20alignment%0Amethods%20that%20separate%20broad%20disease%20classes%2C%20MedTrim%20leverages%20structured%0Ameta-entity%20information%20to%20preserve%20subtle%20but%20clinically%20significant%0Aintra-class%20variations.%20For%20this%20purpose%2C%20we%20first%20introduce%20an%20ontology-based%0Aentity%20recognition%20module%20that%20extracts%20pathology-specific%20meta-entities%20from%0ACXR%20reports%2C%20as%20annotations%20on%20pathology%20attributes%20are%20rare%20in%20public%0Adatasets.%20For%20refined%20sample%20selection%20in%20triplet%20mining%2C%20we%20then%20introduce%20a%0Anovel%20score%20function%20that%20captures%20an%20aggregate%20measure%20of%20inter-sample%0Asimilarity%20based%20on%20disease%20classes%20and%20adjectival/directional%20descriptors.%0ALastly%2C%20we%20introduce%20a%20multimodal%20triplet%20alignment%20objective%20for%20explicit%0Awithin-%20and%20cross-modal%20alignment%20between%20samples%20sharing%20detailed%20pathology%0Acharacteristics.%20Our%20demonstrations%20indicate%20that%20MedTrim%20improves%20performance%0Ain%20downstream%20retrieval%20and%20classification%20tasks%20compared%20to%20state-of-the-art%0Aalignment%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15929v1&entry.124074799=Read"},
{"title": "Efficient Discovery of Motif Transition Process for Large-Scale Temporal\n  Graphs", "author": "Zhiyuan Zheng and Jianpeng Qi and Jiantao Li and Guoqing Chao and Junyu Dong and Yanwei Yu", "abstract": "  Understanding the dynamic transition of motifs in temporal graphs is\nessential for revealing how graph structures evolve over time, identifying\ncritical patterns, and predicting future behaviors, yet existing methods often\nfocus on predefined motifs, limiting their ability to comprehensively capture\ntransitions and interrelationships. We propose a parallel motif transition\nprocess discovery algorithm, PTMT, a novel parallel method for discovering\nmotif transition processes in large-scale temporal graphs. PTMT integrates a\ntree-based framework with the temporal zone partitioning (TZP) strategy, which\npartitions temporal graphs by time and structure while preserving lossless\nmotif transitions and enabling massive parallelism. PTMT comprises three\nphases: growth zone parallel expansion, overlap-aware result aggregation, and\ndeterministic encoding of motif transitions, ensuring accurate tracking of\ndynamic transitions and interactions. Results on 10 real-world datasets\ndemonstrate that PTMT achieves speedups ranging from 12.0$\\times$ to\n50.3$\\times$ compared to the SOTA method.\n", "link": "http://arxiv.org/abs/2504.15979v1", "date": "2025-04-22", "relevancy": 2.2621, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4473}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Discovery%20of%20Motif%20Transition%20Process%20for%20Large-Scale%20Temporal%0A%20%20Graphs&body=Title%3A%20Efficient%20Discovery%20of%20Motif%20Transition%20Process%20for%20Large-Scale%20Temporal%0A%20%20Graphs%0AAuthor%3A%20Zhiyuan%20Zheng%20and%20Jianpeng%20Qi%20and%20Jiantao%20Li%20and%20Guoqing%20Chao%20and%20Junyu%20Dong%20and%20Yanwei%20Yu%0AAbstract%3A%20%20%20Understanding%20the%20dynamic%20transition%20of%20motifs%20in%20temporal%20graphs%20is%0Aessential%20for%20revealing%20how%20graph%20structures%20evolve%20over%20time%2C%20identifying%0Acritical%20patterns%2C%20and%20predicting%20future%20behaviors%2C%20yet%20existing%20methods%20often%0Afocus%20on%20predefined%20motifs%2C%20limiting%20their%20ability%20to%20comprehensively%20capture%0Atransitions%20and%20interrelationships.%20We%20propose%20a%20parallel%20motif%20transition%0Aprocess%20discovery%20algorithm%2C%20PTMT%2C%20a%20novel%20parallel%20method%20for%20discovering%0Amotif%20transition%20processes%20in%20large-scale%20temporal%20graphs.%20PTMT%20integrates%20a%0Atree-based%20framework%20with%20the%20temporal%20zone%20partitioning%20%28TZP%29%20strategy%2C%20which%0Apartitions%20temporal%20graphs%20by%20time%20and%20structure%20while%20preserving%20lossless%0Amotif%20transitions%20and%20enabling%20massive%20parallelism.%20PTMT%20comprises%20three%0Aphases%3A%20growth%20zone%20parallel%20expansion%2C%20overlap-aware%20result%20aggregation%2C%20and%0Adeterministic%20encoding%20of%20motif%20transitions%2C%20ensuring%20accurate%20tracking%20of%0Adynamic%20transitions%20and%20interactions.%20Results%20on%2010%20real-world%20datasets%0Ademonstrate%20that%20PTMT%20achieves%20speedups%20ranging%20from%2012.0%24%5Ctimes%24%20to%0A50.3%24%5Ctimes%24%20compared%20to%20the%20SOTA%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Discovery%2520of%2520Motif%2520Transition%2520Process%2520for%2520Large-Scale%2520Temporal%250A%2520%2520Graphs%26entry.906535625%3DZhiyuan%2520Zheng%2520and%2520Jianpeng%2520Qi%2520and%2520Jiantao%2520Li%2520and%2520Guoqing%2520Chao%2520and%2520Junyu%2520Dong%2520and%2520Yanwei%2520Yu%26entry.1292438233%3D%2520%2520Understanding%2520the%2520dynamic%2520transition%2520of%2520motifs%2520in%2520temporal%2520graphs%2520is%250Aessential%2520for%2520revealing%2520how%2520graph%2520structures%2520evolve%2520over%2520time%252C%2520identifying%250Acritical%2520patterns%252C%2520and%2520predicting%2520future%2520behaviors%252C%2520yet%2520existing%2520methods%2520often%250Afocus%2520on%2520predefined%2520motifs%252C%2520limiting%2520their%2520ability%2520to%2520comprehensively%2520capture%250Atransitions%2520and%2520interrelationships.%2520We%2520propose%2520a%2520parallel%2520motif%2520transition%250Aprocess%2520discovery%2520algorithm%252C%2520PTMT%252C%2520a%2520novel%2520parallel%2520method%2520for%2520discovering%250Amotif%2520transition%2520processes%2520in%2520large-scale%2520temporal%2520graphs.%2520PTMT%2520integrates%2520a%250Atree-based%2520framework%2520with%2520the%2520temporal%2520zone%2520partitioning%2520%2528TZP%2529%2520strategy%252C%2520which%250Apartitions%2520temporal%2520graphs%2520by%2520time%2520and%2520structure%2520while%2520preserving%2520lossless%250Amotif%2520transitions%2520and%2520enabling%2520massive%2520parallelism.%2520PTMT%2520comprises%2520three%250Aphases%253A%2520growth%2520zone%2520parallel%2520expansion%252C%2520overlap-aware%2520result%2520aggregation%252C%2520and%250Adeterministic%2520encoding%2520of%2520motif%2520transitions%252C%2520ensuring%2520accurate%2520tracking%2520of%250Adynamic%2520transitions%2520and%2520interactions.%2520Results%2520on%252010%2520real-world%2520datasets%250Ademonstrate%2520that%2520PTMT%2520achieves%2520speedups%2520ranging%2520from%252012.0%2524%255Ctimes%2524%2520to%250A50.3%2524%255Ctimes%2524%2520compared%2520to%2520the%2520SOTA%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Discovery%20of%20Motif%20Transition%20Process%20for%20Large-Scale%20Temporal%0A%20%20Graphs&entry.906535625=Zhiyuan%20Zheng%20and%20Jianpeng%20Qi%20and%20Jiantao%20Li%20and%20Guoqing%20Chao%20and%20Junyu%20Dong%20and%20Yanwei%20Yu&entry.1292438233=%20%20Understanding%20the%20dynamic%20transition%20of%20motifs%20in%20temporal%20graphs%20is%0Aessential%20for%20revealing%20how%20graph%20structures%20evolve%20over%20time%2C%20identifying%0Acritical%20patterns%2C%20and%20predicting%20future%20behaviors%2C%20yet%20existing%20methods%20often%0Afocus%20on%20predefined%20motifs%2C%20limiting%20their%20ability%20to%20comprehensively%20capture%0Atransitions%20and%20interrelationships.%20We%20propose%20a%20parallel%20motif%20transition%0Aprocess%20discovery%20algorithm%2C%20PTMT%2C%20a%20novel%20parallel%20method%20for%20discovering%0Amotif%20transition%20processes%20in%20large-scale%20temporal%20graphs.%20PTMT%20integrates%20a%0Atree-based%20framework%20with%20the%20temporal%20zone%20partitioning%20%28TZP%29%20strategy%2C%20which%0Apartitions%20temporal%20graphs%20by%20time%20and%20structure%20while%20preserving%20lossless%0Amotif%20transitions%20and%20enabling%20massive%20parallelism.%20PTMT%20comprises%20three%0Aphases%3A%20growth%20zone%20parallel%20expansion%2C%20overlap-aware%20result%20aggregation%2C%20and%0Adeterministic%20encoding%20of%20motif%20transitions%2C%20ensuring%20accurate%20tracking%20of%0Adynamic%20transitions%20and%20interactions.%20Results%20on%2010%20real-world%20datasets%0Ademonstrate%20that%20PTMT%20achieves%20speedups%20ranging%20from%2012.0%24%5Ctimes%24%20to%0A50.3%24%5Ctimes%24%20compared%20to%20the%20SOTA%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15979v1&entry.124074799=Read"},
{"title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering\n  Models", "author": "Quentin Herau and Nathan Piasco and Moussab Bennehar and Luis Rolado and Dzmitry Tsishkou and Bingbing Liu and Cyrille Migniot and Pascal Vasseur and C\u00e9dric Demonceaux", "abstract": "  Autonomous driving systems rely on accurate perception and localization of\nthe ego car to ensure safety and reliability in challenging real-world driving\nscenarios. Public datasets play a vital role in benchmarking and guiding\nadvancement in research by providing standardized resources for model\ndevelopment and evaluation. However, potential inaccuracies in sensor\ncalibration and vehicle poses within these datasets can lead to erroneous\nevaluations of downstream tasks, adversely impacting the reliability and\nperformance of the autonomous systems. To address this challenge, we propose a\nrobust optimization method based on Neural Radiance Fields (NeRF) to refine\nsensor poses and calibration parameters, enhancing the integrity of dataset\nbenchmarks. To validate improvement in accuracy of our optimized poses without\nground truth, we present a thorough evaluation process, relying on reprojection\nmetrics, Novel View Synthesis rendering quality, and geometric alignment. We\ndemonstrate that our method achieves significant improvements in sensor pose\naccuracy. By optimizing these critical parameters, our approach not only\nimproves the utility of existing datasets but also paves the way for more\nreliable autonomous driving models. To foster continued progress in this field,\nwe make the optimized sensor poses publicly available, providing a valuable\nresource for the research community.\n", "link": "http://arxiv.org/abs/2504.15776v1", "date": "2025-04-22", "relevancy": 2.2153, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5737}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5397}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose%20Optimization%20for%20Autonomous%20Driving%20Datasets%20using%20Neural%20Rendering%0A%20%20Models&body=Title%3A%20Pose%20Optimization%20for%20Autonomous%20Driving%20Datasets%20using%20Neural%20Rendering%0A%20%20Models%0AAuthor%3A%20Quentin%20Herau%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rolado%20and%20Dzmitry%20Tsishkou%20and%20Bingbing%20Liu%20and%20Cyrille%20Migniot%20and%20Pascal%20Vasseur%20and%20C%C3%A9dric%20Demonceaux%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20rely%20on%20accurate%20perception%20and%20localization%20of%0Athe%20ego%20car%20to%20ensure%20safety%20and%20reliability%20in%20challenging%20real-world%20driving%0Ascenarios.%20Public%20datasets%20play%20a%20vital%20role%20in%20benchmarking%20and%20guiding%0Aadvancement%20in%20research%20by%20providing%20standardized%20resources%20for%20model%0Adevelopment%20and%20evaluation.%20However%2C%20potential%20inaccuracies%20in%20sensor%0Acalibration%20and%20vehicle%20poses%20within%20these%20datasets%20can%20lead%20to%20erroneous%0Aevaluations%20of%20downstream%20tasks%2C%20adversely%20impacting%20the%20reliability%20and%0Aperformance%20of%20the%20autonomous%20systems.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Arobust%20optimization%20method%20based%20on%20Neural%20Radiance%20Fields%20%28NeRF%29%20to%20refine%0Asensor%20poses%20and%20calibration%20parameters%2C%20enhancing%20the%20integrity%20of%20dataset%0Abenchmarks.%20To%20validate%20improvement%20in%20accuracy%20of%20our%20optimized%20poses%20without%0Aground%20truth%2C%20we%20present%20a%20thorough%20evaluation%20process%2C%20relying%20on%20reprojection%0Ametrics%2C%20Novel%20View%20Synthesis%20rendering%20quality%2C%20and%20geometric%20alignment.%20We%0Ademonstrate%20that%20our%20method%20achieves%20significant%20improvements%20in%20sensor%20pose%0Aaccuracy.%20By%20optimizing%20these%20critical%20parameters%2C%20our%20approach%20not%20only%0Aimproves%20the%20utility%20of%20existing%20datasets%20but%20also%20paves%20the%20way%20for%20more%0Areliable%20autonomous%20driving%20models.%20To%20foster%20continued%20progress%20in%20this%20field%2C%0Awe%20make%20the%20optimized%20sensor%20poses%20publicly%20available%2C%20providing%20a%20valuable%0Aresource%20for%20the%20research%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose%2520Optimization%2520for%2520Autonomous%2520Driving%2520Datasets%2520using%2520Neural%2520Rendering%250A%2520%2520Models%26entry.906535625%3DQuentin%2520Herau%2520and%2520Nathan%2520Piasco%2520and%2520Moussab%2520Bennehar%2520and%2520Luis%2520Rolado%2520and%2520Dzmitry%2520Tsishkou%2520and%2520Bingbing%2520Liu%2520and%2520Cyrille%2520Migniot%2520and%2520Pascal%2520Vasseur%2520and%2520C%25C3%25A9dric%2520Demonceaux%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520systems%2520rely%2520on%2520accurate%2520perception%2520and%2520localization%2520of%250Athe%2520ego%2520car%2520to%2520ensure%2520safety%2520and%2520reliability%2520in%2520challenging%2520real-world%2520driving%250Ascenarios.%2520Public%2520datasets%2520play%2520a%2520vital%2520role%2520in%2520benchmarking%2520and%2520guiding%250Aadvancement%2520in%2520research%2520by%2520providing%2520standardized%2520resources%2520for%2520model%250Adevelopment%2520and%2520evaluation.%2520However%252C%2520potential%2520inaccuracies%2520in%2520sensor%250Acalibration%2520and%2520vehicle%2520poses%2520within%2520these%2520datasets%2520can%2520lead%2520to%2520erroneous%250Aevaluations%2520of%2520downstream%2520tasks%252C%2520adversely%2520impacting%2520the%2520reliability%2520and%250Aperformance%2520of%2520the%2520autonomous%2520systems.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%250Arobust%2520optimization%2520method%2520based%2520on%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520to%2520refine%250Asensor%2520poses%2520and%2520calibration%2520parameters%252C%2520enhancing%2520the%2520integrity%2520of%2520dataset%250Abenchmarks.%2520To%2520validate%2520improvement%2520in%2520accuracy%2520of%2520our%2520optimized%2520poses%2520without%250Aground%2520truth%252C%2520we%2520present%2520a%2520thorough%2520evaluation%2520process%252C%2520relying%2520on%2520reprojection%250Ametrics%252C%2520Novel%2520View%2520Synthesis%2520rendering%2520quality%252C%2520and%2520geometric%2520alignment.%2520We%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520significant%2520improvements%2520in%2520sensor%2520pose%250Aaccuracy.%2520By%2520optimizing%2520these%2520critical%2520parameters%252C%2520our%2520approach%2520not%2520only%250Aimproves%2520the%2520utility%2520of%2520existing%2520datasets%2520but%2520also%2520paves%2520the%2520way%2520for%2520more%250Areliable%2520autonomous%2520driving%2520models.%2520To%2520foster%2520continued%2520progress%2520in%2520this%2520field%252C%250Awe%2520make%2520the%2520optimized%2520sensor%2520poses%2520publicly%2520available%252C%2520providing%2520a%2520valuable%250Aresource%2520for%2520the%2520research%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose%20Optimization%20for%20Autonomous%20Driving%20Datasets%20using%20Neural%20Rendering%0A%20%20Models&entry.906535625=Quentin%20Herau%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rolado%20and%20Dzmitry%20Tsishkou%20and%20Bingbing%20Liu%20and%20Cyrille%20Migniot%20and%20Pascal%20Vasseur%20and%20C%C3%A9dric%20Demonceaux&entry.1292438233=%20%20Autonomous%20driving%20systems%20rely%20on%20accurate%20perception%20and%20localization%20of%0Athe%20ego%20car%20to%20ensure%20safety%20and%20reliability%20in%20challenging%20real-world%20driving%0Ascenarios.%20Public%20datasets%20play%20a%20vital%20role%20in%20benchmarking%20and%20guiding%0Aadvancement%20in%20research%20by%20providing%20standardized%20resources%20for%20model%0Adevelopment%20and%20evaluation.%20However%2C%20potential%20inaccuracies%20in%20sensor%0Acalibration%20and%20vehicle%20poses%20within%20these%20datasets%20can%20lead%20to%20erroneous%0Aevaluations%20of%20downstream%20tasks%2C%20adversely%20impacting%20the%20reliability%20and%0Aperformance%20of%20the%20autonomous%20systems.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Arobust%20optimization%20method%20based%20on%20Neural%20Radiance%20Fields%20%28NeRF%29%20to%20refine%0Asensor%20poses%20and%20calibration%20parameters%2C%20enhancing%20the%20integrity%20of%20dataset%0Abenchmarks.%20To%20validate%20improvement%20in%20accuracy%20of%20our%20optimized%20poses%20without%0Aground%20truth%2C%20we%20present%20a%20thorough%20evaluation%20process%2C%20relying%20on%20reprojection%0Ametrics%2C%20Novel%20View%20Synthesis%20rendering%20quality%2C%20and%20geometric%20alignment.%20We%0Ademonstrate%20that%20our%20method%20achieves%20significant%20improvements%20in%20sensor%20pose%0Aaccuracy.%20By%20optimizing%20these%20critical%20parameters%2C%20our%20approach%20not%20only%0Aimproves%20the%20utility%20of%20existing%20datasets%20but%20also%20paves%20the%20way%20for%20more%0Areliable%20autonomous%20driving%20models.%20To%20foster%20continued%20progress%20in%20this%20field%2C%0Awe%20make%20the%20optimized%20sensor%20poses%20publicly%20available%2C%20providing%20a%20valuable%0Aresource%20for%20the%20research%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15776v1&entry.124074799=Read"},
{"title": "An Operator Splitting View of Federated Learning", "author": "Saber Malekmohammadi and Kiarash Shaloudegi and Zeou Hu and Yaoliang Yu", "abstract": "  Over the past few years, the federated learning ($\\texttt{FL}$) community has\nwitnessed a proliferation of new $\\texttt{FL}$ algorithms. However, our\nunderstating of the theory of $\\texttt{FL}$ is still fragmented, and a\nthorough, formal comparison of these algorithms remains elusive. Motivated by\nthis gap, we show that many of the existing $\\texttt{FL}$ algorithms can be\nunderstood from an operator splitting point of view. This unification allows us\nto compare different algorithms with ease, to refine previous convergence\nresults and to uncover new algorithmic variants. In particular, our analysis\nreveals the vital role played by the step size in $\\texttt{FL}$ algorithms. The\nunification also leads to a streamlined and economic way to accelerate\n$\\texttt{FL}$ algorithms, without incurring any communication overhead. We\nperform numerical experiments on both convex and nonconvex models to validate\nour findings.\n", "link": "http://arxiv.org/abs/2108.05974v3", "date": "2025-04-22", "relevancy": 2.2124, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4393}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Operator%20Splitting%20View%20of%20Federated%20Learning&body=Title%3A%20An%20Operator%20Splitting%20View%20of%20Federated%20Learning%0AAuthor%3A%20Saber%20Malekmohammadi%20and%20Kiarash%20Shaloudegi%20and%20Zeou%20Hu%20and%20Yaoliang%20Yu%0AAbstract%3A%20%20%20Over%20the%20past%20few%20years%2C%20the%20federated%20learning%20%28%24%5Ctexttt%7BFL%7D%24%29%20community%20has%0Awitnessed%20a%20proliferation%20of%20new%20%24%5Ctexttt%7BFL%7D%24%20algorithms.%20However%2C%20our%0Aunderstating%20of%20the%20theory%20of%20%24%5Ctexttt%7BFL%7D%24%20is%20still%20fragmented%2C%20and%20a%0Athorough%2C%20formal%20comparison%20of%20these%20algorithms%20remains%20elusive.%20Motivated%20by%0Athis%20gap%2C%20we%20show%20that%20many%20of%20the%20existing%20%24%5Ctexttt%7BFL%7D%24%20algorithms%20can%20be%0Aunderstood%20from%20an%20operator%20splitting%20point%20of%20view.%20This%20unification%20allows%20us%0Ato%20compare%20different%20algorithms%20with%20ease%2C%20to%20refine%20previous%20convergence%0Aresults%20and%20to%20uncover%20new%20algorithmic%20variants.%20In%20particular%2C%20our%20analysis%0Areveals%20the%20vital%20role%20played%20by%20the%20step%20size%20in%20%24%5Ctexttt%7BFL%7D%24%20algorithms.%20The%0Aunification%20also%20leads%20to%20a%20streamlined%20and%20economic%20way%20to%20accelerate%0A%24%5Ctexttt%7BFL%7D%24%20algorithms%2C%20without%20incurring%20any%20communication%20overhead.%20We%0Aperform%20numerical%20experiments%20on%20both%20convex%20and%20nonconvex%20models%20to%20validate%0Aour%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.05974v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Operator%2520Splitting%2520View%2520of%2520Federated%2520Learning%26entry.906535625%3DSaber%2520Malekmohammadi%2520and%2520Kiarash%2520Shaloudegi%2520and%2520Zeou%2520Hu%2520and%2520Yaoliang%2520Yu%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520few%2520years%252C%2520the%2520federated%2520learning%2520%2528%2524%255Ctexttt%257BFL%257D%2524%2529%2520community%2520has%250Awitnessed%2520a%2520proliferation%2520of%2520new%2520%2524%255Ctexttt%257BFL%257D%2524%2520algorithms.%2520However%252C%2520our%250Aunderstating%2520of%2520the%2520theory%2520of%2520%2524%255Ctexttt%257BFL%257D%2524%2520is%2520still%2520fragmented%252C%2520and%2520a%250Athorough%252C%2520formal%2520comparison%2520of%2520these%2520algorithms%2520remains%2520elusive.%2520Motivated%2520by%250Athis%2520gap%252C%2520we%2520show%2520that%2520many%2520of%2520the%2520existing%2520%2524%255Ctexttt%257BFL%257D%2524%2520algorithms%2520can%2520be%250Aunderstood%2520from%2520an%2520operator%2520splitting%2520point%2520of%2520view.%2520This%2520unification%2520allows%2520us%250Ato%2520compare%2520different%2520algorithms%2520with%2520ease%252C%2520to%2520refine%2520previous%2520convergence%250Aresults%2520and%2520to%2520uncover%2520new%2520algorithmic%2520variants.%2520In%2520particular%252C%2520our%2520analysis%250Areveals%2520the%2520vital%2520role%2520played%2520by%2520the%2520step%2520size%2520in%2520%2524%255Ctexttt%257BFL%257D%2524%2520algorithms.%2520The%250Aunification%2520also%2520leads%2520to%2520a%2520streamlined%2520and%2520economic%2520way%2520to%2520accelerate%250A%2524%255Ctexttt%257BFL%257D%2524%2520algorithms%252C%2520without%2520incurring%2520any%2520communication%2520overhead.%2520We%250Aperform%2520numerical%2520experiments%2520on%2520both%2520convex%2520and%2520nonconvex%2520models%2520to%2520validate%250Aour%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2108.05974v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Operator%20Splitting%20View%20of%20Federated%20Learning&entry.906535625=Saber%20Malekmohammadi%20and%20Kiarash%20Shaloudegi%20and%20Zeou%20Hu%20and%20Yaoliang%20Yu&entry.1292438233=%20%20Over%20the%20past%20few%20years%2C%20the%20federated%20learning%20%28%24%5Ctexttt%7BFL%7D%24%29%20community%20has%0Awitnessed%20a%20proliferation%20of%20new%20%24%5Ctexttt%7BFL%7D%24%20algorithms.%20However%2C%20our%0Aunderstating%20of%20the%20theory%20of%20%24%5Ctexttt%7BFL%7D%24%20is%20still%20fragmented%2C%20and%20a%0Athorough%2C%20formal%20comparison%20of%20these%20algorithms%20remains%20elusive.%20Motivated%20by%0Athis%20gap%2C%20we%20show%20that%20many%20of%20the%20existing%20%24%5Ctexttt%7BFL%7D%24%20algorithms%20can%20be%0Aunderstood%20from%20an%20operator%20splitting%20point%20of%20view.%20This%20unification%20allows%20us%0Ato%20compare%20different%20algorithms%20with%20ease%2C%20to%20refine%20previous%20convergence%0Aresults%20and%20to%20uncover%20new%20algorithmic%20variants.%20In%20particular%2C%20our%20analysis%0Areveals%20the%20vital%20role%20played%20by%20the%20step%20size%20in%20%24%5Ctexttt%7BFL%7D%24%20algorithms.%20The%0Aunification%20also%20leads%20to%20a%20streamlined%20and%20economic%20way%20to%20accelerate%0A%24%5Ctexttt%7BFL%7D%24%20algorithms%2C%20without%20incurring%20any%20communication%20overhead.%20We%0Aperform%20numerical%20experiments%20on%20both%20convex%20and%20nonconvex%20models%20to%20validate%0Aour%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.05974v3&entry.124074799=Read"},
{"title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking\n  Questions", "author": "Chang Zong and Bin Li and Shoujun Zhou and Jian Wan and Lei Zhang", "abstract": "  Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc.\n", "link": "http://arxiv.org/abs/2504.15918v1", "date": "2025-04-22", "relevancy": 2.2103, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ask2Loc%3A%20Learning%20to%20Locate%20Instructional%20Visual%20Answers%20by%20Asking%0A%20%20Questions&body=Title%3A%20Ask2Loc%3A%20Learning%20to%20Locate%20Instructional%20Visual%20Answers%20by%20Asking%0A%20%20Questions%0AAuthor%3A%20Chang%20Zong%20and%20Bin%20Li%20and%20Shoujun%20Zhou%20and%20Jian%20Wan%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Locating%20specific%20segments%20within%20an%20instructional%20video%20is%20an%20efficient%20way%0Ato%20acquire%20guiding%20knowledge.%20Generally%2C%20the%20task%20of%20obtaining%20video%20segments%0Afor%20both%20verbal%20explanations%20and%20visual%20demonstrations%20is%20known%20as%20visual%0Aanswer%20localization%20%28VAL%29.%20However%2C%20users%20often%20need%20multiple%20interactions%20to%0Aobtain%20answers%20that%20align%20with%20their%20expectations%20when%20using%20the%20system.%20During%0Athese%20interactions%2C%20humans%20deepen%20their%20understanding%20of%20the%20video%20content%20by%0Aasking%20themselves%20questions%2C%20thereby%20accurately%20identifying%20the%20location.%0ATherefore%2C%20we%20propose%20a%20new%20task%2C%20named%20In-VAL%2C%20to%20simulate%20the%20multiple%0Ainteractions%20between%20humans%20and%20videos%20in%20the%20procedure%20of%20obtaining%20visual%0Aanswers.%20The%20In-VAL%20task%20requires%20interactively%20addressing%20several%20semantic%20gap%0Aissues%2C%20including%201%29%20the%20ambiguity%20of%20user%20intent%20in%20the%20input%20questions%2C%202%29%0Athe%20incompleteness%20of%20language%20in%20video%20subtitles%2C%20and%203%29%20the%20fragmentation%20of%0Acontent%20in%20video%20segments.%20To%20address%20these%20issues%2C%20we%20propose%20Ask2Loc%2C%20a%0Aframework%20for%20resolving%20In-VAL%20by%20asking%20questions.%20It%20includes%20three%20key%0Amodules%3A%201%29%20a%20chatting%20module%20to%20refine%20initial%20questions%20and%20uncover%20clear%0Aintentions%2C%202%29%20a%20rewriting%20module%20to%20generate%20fluent%20language%20and%20create%0Acomplete%20descriptions%2C%20and%203%29%20a%20searching%20module%20to%20broaden%20local%20context%20and%0Aprovide%20integrated%20content.%20We%20conduct%20extensive%20experiments%20on%20three%0Areconstructed%20In-VAL%20datasets.%20Compared%20to%20traditional%20end-to-end%20and%20two-stage%0Amethods%2C%20our%20proposed%20Ask2Loc%20can%20improve%20performance%20by%20up%20to%2014.91%20%28mIoU%29%20on%0Athe%20In-VAL%20task.%20Our%20code%20and%20datasets%20can%20be%20accessed%20at%0Ahttps%3A//github.com/changzong/Ask2Loc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsk2Loc%253A%2520Learning%2520to%2520Locate%2520Instructional%2520Visual%2520Answers%2520by%2520Asking%250A%2520%2520Questions%26entry.906535625%3DChang%2520Zong%2520and%2520Bin%2520Li%2520and%2520Shoujun%2520Zhou%2520and%2520Jian%2520Wan%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Locating%2520specific%2520segments%2520within%2520an%2520instructional%2520video%2520is%2520an%2520efficient%2520way%250Ato%2520acquire%2520guiding%2520knowledge.%2520Generally%252C%2520the%2520task%2520of%2520obtaining%2520video%2520segments%250Afor%2520both%2520verbal%2520explanations%2520and%2520visual%2520demonstrations%2520is%2520known%2520as%2520visual%250Aanswer%2520localization%2520%2528VAL%2529.%2520However%252C%2520users%2520often%2520need%2520multiple%2520interactions%2520to%250Aobtain%2520answers%2520that%2520align%2520with%2520their%2520expectations%2520when%2520using%2520the%2520system.%2520During%250Athese%2520interactions%252C%2520humans%2520deepen%2520their%2520understanding%2520of%2520the%2520video%2520content%2520by%250Aasking%2520themselves%2520questions%252C%2520thereby%2520accurately%2520identifying%2520the%2520location.%250ATherefore%252C%2520we%2520propose%2520a%2520new%2520task%252C%2520named%2520In-VAL%252C%2520to%2520simulate%2520the%2520multiple%250Ainteractions%2520between%2520humans%2520and%2520videos%2520in%2520the%2520procedure%2520of%2520obtaining%2520visual%250Aanswers.%2520The%2520In-VAL%2520task%2520requires%2520interactively%2520addressing%2520several%2520semantic%2520gap%250Aissues%252C%2520including%25201%2529%2520the%2520ambiguity%2520of%2520user%2520intent%2520in%2520the%2520input%2520questions%252C%25202%2529%250Athe%2520incompleteness%2520of%2520language%2520in%2520video%2520subtitles%252C%2520and%25203%2529%2520the%2520fragmentation%2520of%250Acontent%2520in%2520video%2520segments.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Ask2Loc%252C%2520a%250Aframework%2520for%2520resolving%2520In-VAL%2520by%2520asking%2520questions.%2520It%2520includes%2520three%2520key%250Amodules%253A%25201%2529%2520a%2520chatting%2520module%2520to%2520refine%2520initial%2520questions%2520and%2520uncover%2520clear%250Aintentions%252C%25202%2529%2520a%2520rewriting%2520module%2520to%2520generate%2520fluent%2520language%2520and%2520create%250Acomplete%2520descriptions%252C%2520and%25203%2529%2520a%2520searching%2520module%2520to%2520broaden%2520local%2520context%2520and%250Aprovide%2520integrated%2520content.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520three%250Areconstructed%2520In-VAL%2520datasets.%2520Compared%2520to%2520traditional%2520end-to-end%2520and%2520two-stage%250Amethods%252C%2520our%2520proposed%2520Ask2Loc%2520can%2520improve%2520performance%2520by%2520up%2520to%252014.91%2520%2528mIoU%2529%2520on%250Athe%2520In-VAL%2520task.%2520Our%2520code%2520and%2520datasets%2520can%2520be%2520accessed%2520at%250Ahttps%253A//github.com/changzong/Ask2Loc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ask2Loc%3A%20Learning%20to%20Locate%20Instructional%20Visual%20Answers%20by%20Asking%0A%20%20Questions&entry.906535625=Chang%20Zong%20and%20Bin%20Li%20and%20Shoujun%20Zhou%20and%20Jian%20Wan%20and%20Lei%20Zhang&entry.1292438233=%20%20Locating%20specific%20segments%20within%20an%20instructional%20video%20is%20an%20efficient%20way%0Ato%20acquire%20guiding%20knowledge.%20Generally%2C%20the%20task%20of%20obtaining%20video%20segments%0Afor%20both%20verbal%20explanations%20and%20visual%20demonstrations%20is%20known%20as%20visual%0Aanswer%20localization%20%28VAL%29.%20However%2C%20users%20often%20need%20multiple%20interactions%20to%0Aobtain%20answers%20that%20align%20with%20their%20expectations%20when%20using%20the%20system.%20During%0Athese%20interactions%2C%20humans%20deepen%20their%20understanding%20of%20the%20video%20content%20by%0Aasking%20themselves%20questions%2C%20thereby%20accurately%20identifying%20the%20location.%0ATherefore%2C%20we%20propose%20a%20new%20task%2C%20named%20In-VAL%2C%20to%20simulate%20the%20multiple%0Ainteractions%20between%20humans%20and%20videos%20in%20the%20procedure%20of%20obtaining%20visual%0Aanswers.%20The%20In-VAL%20task%20requires%20interactively%20addressing%20several%20semantic%20gap%0Aissues%2C%20including%201%29%20the%20ambiguity%20of%20user%20intent%20in%20the%20input%20questions%2C%202%29%0Athe%20incompleteness%20of%20language%20in%20video%20subtitles%2C%20and%203%29%20the%20fragmentation%20of%0Acontent%20in%20video%20segments.%20To%20address%20these%20issues%2C%20we%20propose%20Ask2Loc%2C%20a%0Aframework%20for%20resolving%20In-VAL%20by%20asking%20questions.%20It%20includes%20three%20key%0Amodules%3A%201%29%20a%20chatting%20module%20to%20refine%20initial%20questions%20and%20uncover%20clear%0Aintentions%2C%202%29%20a%20rewriting%20module%20to%20generate%20fluent%20language%20and%20create%0Acomplete%20descriptions%2C%20and%203%29%20a%20searching%20module%20to%20broaden%20local%20context%20and%0Aprovide%20integrated%20content.%20We%20conduct%20extensive%20experiments%20on%20three%0Areconstructed%20In-VAL%20datasets.%20Compared%20to%20traditional%20end-to-end%20and%20two-stage%0Amethods%2C%20our%20proposed%20Ask2Loc%20can%20improve%20performance%20by%20up%20to%2014.91%20%28mIoU%29%20on%0Athe%20In-VAL%20task.%20Our%20code%20and%20datasets%20can%20be%20accessed%20at%0Ahttps%3A//github.com/changzong/Ask2Loc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15918v1&entry.124074799=Read"},
{"title": "Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose\n  Dolphins in Drone-Shot Videos", "author": "Daniele Baieri and Riccardo Cicciarella and Michael Kr\u00fctzen and Emanuele Rodol\u00e0 and Silvia Zuffi", "abstract": "  We address the problem of estimating the metric 3D shape and motion of wild\ndolphins from monocular video, with the aim of assessing their body condition.\nWhile considerable progress has been made in reconstructing 3D models of\nterrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty\nof observing them in their natural underwater environment. To address this, we\npropose a model-based approach that incorporates a transmission model to\naccount for water-induced occlusion. We apply our method to video captured\nunder different sea conditions. We estimate mass and volume, and compare our\nresults to a manual 2D measurements-based method.\n", "link": "http://arxiv.org/abs/2504.15782v1", "date": "2025-04-22", "relevancy": 2.2088, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5593}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5503}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-based%20Metric%203D%20Shape%20and%20Motion%20Reconstruction%20of%20Wild%20Bottlenose%0A%20%20Dolphins%20in%20Drone-Shot%20Videos&body=Title%3A%20Model-based%20Metric%203D%20Shape%20and%20Motion%20Reconstruction%20of%20Wild%20Bottlenose%0A%20%20Dolphins%20in%20Drone-Shot%20Videos%0AAuthor%3A%20Daniele%20Baieri%20and%20Riccardo%20Cicciarella%20and%20Michael%20Kr%C3%BCtzen%20and%20Emanuele%20Rodol%C3%A0%20and%20Silvia%20Zuffi%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20estimating%20the%20metric%203D%20shape%20and%20motion%20of%20wild%0Adolphins%20from%20monocular%20video%2C%20with%20the%20aim%20of%20assessing%20their%20body%20condition.%0AWhile%20considerable%20progress%20has%20been%20made%20in%20reconstructing%203D%20models%20of%0Aterrestrial%20quadrupeds%2C%20aquatic%20animals%20remain%20unexplored%20due%20to%20the%20difficulty%0Aof%20observing%20them%20in%20their%20natural%20underwater%20environment.%20To%20address%20this%2C%20we%0Apropose%20a%20model-based%20approach%20that%20incorporates%20a%20transmission%20model%20to%0Aaccount%20for%20water-induced%20occlusion.%20We%20apply%20our%20method%20to%20video%20captured%0Aunder%20different%20sea%20conditions.%20We%20estimate%20mass%20and%20volume%2C%20and%20compare%20our%0Aresults%20to%20a%20manual%202D%20measurements-based%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-based%2520Metric%25203D%2520Shape%2520and%2520Motion%2520Reconstruction%2520of%2520Wild%2520Bottlenose%250A%2520%2520Dolphins%2520in%2520Drone-Shot%2520Videos%26entry.906535625%3DDaniele%2520Baieri%2520and%2520Riccardo%2520Cicciarella%2520and%2520Michael%2520Kr%25C3%25BCtzen%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Silvia%2520Zuffi%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520estimating%2520the%2520metric%25203D%2520shape%2520and%2520motion%2520of%2520wild%250Adolphins%2520from%2520monocular%2520video%252C%2520with%2520the%2520aim%2520of%2520assessing%2520their%2520body%2520condition.%250AWhile%2520considerable%2520progress%2520has%2520been%2520made%2520in%2520reconstructing%25203D%2520models%2520of%250Aterrestrial%2520quadrupeds%252C%2520aquatic%2520animals%2520remain%2520unexplored%2520due%2520to%2520the%2520difficulty%250Aof%2520observing%2520them%2520in%2520their%2520natural%2520underwater%2520environment.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520model-based%2520approach%2520that%2520incorporates%2520a%2520transmission%2520model%2520to%250Aaccount%2520for%2520water-induced%2520occlusion.%2520We%2520apply%2520our%2520method%2520to%2520video%2520captured%250Aunder%2520different%2520sea%2520conditions.%2520We%2520estimate%2520mass%2520and%2520volume%252C%2520and%2520compare%2520our%250Aresults%2520to%2520a%2520manual%25202D%2520measurements-based%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-based%20Metric%203D%20Shape%20and%20Motion%20Reconstruction%20of%20Wild%20Bottlenose%0A%20%20Dolphins%20in%20Drone-Shot%20Videos&entry.906535625=Daniele%20Baieri%20and%20Riccardo%20Cicciarella%20and%20Michael%20Kr%C3%BCtzen%20and%20Emanuele%20Rodol%C3%A0%20and%20Silvia%20Zuffi&entry.1292438233=%20%20We%20address%20the%20problem%20of%20estimating%20the%20metric%203D%20shape%20and%20motion%20of%20wild%0Adolphins%20from%20monocular%20video%2C%20with%20the%20aim%20of%20assessing%20their%20body%20condition.%0AWhile%20considerable%20progress%20has%20been%20made%20in%20reconstructing%203D%20models%20of%0Aterrestrial%20quadrupeds%2C%20aquatic%20animals%20remain%20unexplored%20due%20to%20the%20difficulty%0Aof%20observing%20them%20in%20their%20natural%20underwater%20environment.%20To%20address%20this%2C%20we%0Apropose%20a%20model-based%20approach%20that%20incorporates%20a%20transmission%20model%20to%0Aaccount%20for%20water-induced%20occlusion.%20We%20apply%20our%20method%20to%20video%20captured%0Aunder%20different%20sea%20conditions.%20We%20estimate%20mass%20and%20volume%2C%20and%20compare%20our%0Aresults%20to%20a%20manual%202D%20measurements-based%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15782v1&entry.124074799=Read"},
{"title": "Generative AI for Research Data Processing: Lessons Learnt From Three\n  Use Cases", "author": "Modhurita Mitra and Martine G. de Vos and Nicola Cortinovis and Dawa Ometto", "abstract": "  There has been enormous interest in generative AI since ChatGPT was launched\nin 2022. However, there are concerns about the accuracy and consistency of the\noutputs of generative AI. We have carried out an exploratory study on the\napplication of this new technology in research data processing. We identified\ntasks for which rule-based or traditional machine learning approaches were\ndifficult to apply, and then performed these tasks using generative AI.\n  We demonstrate the feasibility of using the generative AI model Claude 3 Opus\nin three research projects involving complex data processing tasks:\n  1) Information extraction: We extract plant species names from historical\nseedlists (catalogues of seeds) published by botanical gardens.\n  2) Natural language understanding: We extract certain data points (name of\ndrug, name of health indication, relative effectiveness, cost-effectiveness,\netc.) from documents published by Health Technology Assessment organisations in\nthe EU.\n  3) Text classification: We assign industry codes to projects on the\ncrowdfunding website Kickstarter.\n  We share the lessons we learnt from these use cases: How to determine if\ngenerative AI is an appropriate tool for a given data processing task, and if\nso, how to maximise the accuracy and consistency of the results obtained.\n", "link": "http://arxiv.org/abs/2504.15829v1", "date": "2025-04-22", "relevancy": 2.1918, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5577}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5491}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20for%20Research%20Data%20Processing%3A%20Lessons%20Learnt%20From%20Three%0A%20%20Use%20Cases&body=Title%3A%20Generative%20AI%20for%20Research%20Data%20Processing%3A%20Lessons%20Learnt%20From%20Three%0A%20%20Use%20Cases%0AAuthor%3A%20Modhurita%20Mitra%20and%20Martine%20G.%20de%20Vos%20and%20Nicola%20Cortinovis%20and%20Dawa%20Ometto%0AAbstract%3A%20%20%20There%20has%20been%20enormous%20interest%20in%20generative%20AI%20since%20ChatGPT%20was%20launched%0Ain%202022.%20However%2C%20there%20are%20concerns%20about%20the%20accuracy%20and%20consistency%20of%20the%0Aoutputs%20of%20generative%20AI.%20We%20have%20carried%20out%20an%20exploratory%20study%20on%20the%0Aapplication%20of%20this%20new%20technology%20in%20research%20data%20processing.%20We%20identified%0Atasks%20for%20which%20rule-based%20or%20traditional%20machine%20learning%20approaches%20were%0Adifficult%20to%20apply%2C%20and%20then%20performed%20these%20tasks%20using%20generative%20AI.%0A%20%20We%20demonstrate%20the%20feasibility%20of%20using%20the%20generative%20AI%20model%20Claude%203%20Opus%0Ain%20three%20research%20projects%20involving%20complex%20data%20processing%20tasks%3A%0A%20%201%29%20Information%20extraction%3A%20We%20extract%20plant%20species%20names%20from%20historical%0Aseedlists%20%28catalogues%20of%20seeds%29%20published%20by%20botanical%20gardens.%0A%20%202%29%20Natural%20language%20understanding%3A%20We%20extract%20certain%20data%20points%20%28name%20of%0Adrug%2C%20name%20of%20health%20indication%2C%20relative%20effectiveness%2C%20cost-effectiveness%2C%0Aetc.%29%20from%20documents%20published%20by%20Health%20Technology%20Assessment%20organisations%20in%0Athe%20EU.%0A%20%203%29%20Text%20classification%3A%20We%20assign%20industry%20codes%20to%20projects%20on%20the%0Acrowdfunding%20website%20Kickstarter.%0A%20%20We%20share%20the%20lessons%20we%20learnt%20from%20these%20use%20cases%3A%20How%20to%20determine%20if%0Agenerative%20AI%20is%20an%20appropriate%20tool%20for%20a%20given%20data%20processing%20task%2C%20and%20if%0Aso%2C%20how%20to%20maximise%20the%20accuracy%20and%20consistency%20of%20the%20results%20obtained.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520for%2520Research%2520Data%2520Processing%253A%2520Lessons%2520Learnt%2520From%2520Three%250A%2520%2520Use%2520Cases%26entry.906535625%3DModhurita%2520Mitra%2520and%2520Martine%2520G.%2520de%2520Vos%2520and%2520Nicola%2520Cortinovis%2520and%2520Dawa%2520Ometto%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520enormous%2520interest%2520in%2520generative%2520AI%2520since%2520ChatGPT%2520was%2520launched%250Ain%25202022.%2520However%252C%2520there%2520are%2520concerns%2520about%2520the%2520accuracy%2520and%2520consistency%2520of%2520the%250Aoutputs%2520of%2520generative%2520AI.%2520We%2520have%2520carried%2520out%2520an%2520exploratory%2520study%2520on%2520the%250Aapplication%2520of%2520this%2520new%2520technology%2520in%2520research%2520data%2520processing.%2520We%2520identified%250Atasks%2520for%2520which%2520rule-based%2520or%2520traditional%2520machine%2520learning%2520approaches%2520were%250Adifficult%2520to%2520apply%252C%2520and%2520then%2520performed%2520these%2520tasks%2520using%2520generative%2520AI.%250A%2520%2520We%2520demonstrate%2520the%2520feasibility%2520of%2520using%2520the%2520generative%2520AI%2520model%2520Claude%25203%2520Opus%250Ain%2520three%2520research%2520projects%2520involving%2520complex%2520data%2520processing%2520tasks%253A%250A%2520%25201%2529%2520Information%2520extraction%253A%2520We%2520extract%2520plant%2520species%2520names%2520from%2520historical%250Aseedlists%2520%2528catalogues%2520of%2520seeds%2529%2520published%2520by%2520botanical%2520gardens.%250A%2520%25202%2529%2520Natural%2520language%2520understanding%253A%2520We%2520extract%2520certain%2520data%2520points%2520%2528name%2520of%250Adrug%252C%2520name%2520of%2520health%2520indication%252C%2520relative%2520effectiveness%252C%2520cost-effectiveness%252C%250Aetc.%2529%2520from%2520documents%2520published%2520by%2520Health%2520Technology%2520Assessment%2520organisations%2520in%250Athe%2520EU.%250A%2520%25203%2529%2520Text%2520classification%253A%2520We%2520assign%2520industry%2520codes%2520to%2520projects%2520on%2520the%250Acrowdfunding%2520website%2520Kickstarter.%250A%2520%2520We%2520share%2520the%2520lessons%2520we%2520learnt%2520from%2520these%2520use%2520cases%253A%2520How%2520to%2520determine%2520if%250Agenerative%2520AI%2520is%2520an%2520appropriate%2520tool%2520for%2520a%2520given%2520data%2520processing%2520task%252C%2520and%2520if%250Aso%252C%2520how%2520to%2520maximise%2520the%2520accuracy%2520and%2520consistency%2520of%2520the%2520results%2520obtained.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20for%20Research%20Data%20Processing%3A%20Lessons%20Learnt%20From%20Three%0A%20%20Use%20Cases&entry.906535625=Modhurita%20Mitra%20and%20Martine%20G.%20de%20Vos%20and%20Nicola%20Cortinovis%20and%20Dawa%20Ometto&entry.1292438233=%20%20There%20has%20been%20enormous%20interest%20in%20generative%20AI%20since%20ChatGPT%20was%20launched%0Ain%202022.%20However%2C%20there%20are%20concerns%20about%20the%20accuracy%20and%20consistency%20of%20the%0Aoutputs%20of%20generative%20AI.%20We%20have%20carried%20out%20an%20exploratory%20study%20on%20the%0Aapplication%20of%20this%20new%20technology%20in%20research%20data%20processing.%20We%20identified%0Atasks%20for%20which%20rule-based%20or%20traditional%20machine%20learning%20approaches%20were%0Adifficult%20to%20apply%2C%20and%20then%20performed%20these%20tasks%20using%20generative%20AI.%0A%20%20We%20demonstrate%20the%20feasibility%20of%20using%20the%20generative%20AI%20model%20Claude%203%20Opus%0Ain%20three%20research%20projects%20involving%20complex%20data%20processing%20tasks%3A%0A%20%201%29%20Information%20extraction%3A%20We%20extract%20plant%20species%20names%20from%20historical%0Aseedlists%20%28catalogues%20of%20seeds%29%20published%20by%20botanical%20gardens.%0A%20%202%29%20Natural%20language%20understanding%3A%20We%20extract%20certain%20data%20points%20%28name%20of%0Adrug%2C%20name%20of%20health%20indication%2C%20relative%20effectiveness%2C%20cost-effectiveness%2C%0Aetc.%29%20from%20documents%20published%20by%20Health%20Technology%20Assessment%20organisations%20in%0Athe%20EU.%0A%20%203%29%20Text%20classification%3A%20We%20assign%20industry%20codes%20to%20projects%20on%20the%0Acrowdfunding%20website%20Kickstarter.%0A%20%20We%20share%20the%20lessons%20we%20learnt%20from%20these%20use%20cases%3A%20How%20to%20determine%20if%0Agenerative%20AI%20is%20an%20appropriate%20tool%20for%20a%20given%20data%20processing%20task%2C%20and%20if%0Aso%2C%20how%20to%20maximise%20the%20accuracy%20and%20consistency%20of%20the%20results%20obtained.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15829v1&entry.124074799=Read"},
{"title": "Explainable Unsupervised Anomaly Detection with Random Forest", "author": "Joshua S. Harvey and Joshua Rosaler and Mingshu Li and Dhruv Desai and Dhagash Mehta", "abstract": "  We describe the use of an unsupervised Random Forest for similarity learning\nand improved unsupervised anomaly detection. By training a Random Forest to\ndiscriminate between real data and synthetic data sampled from a uniform\ndistribution over the real data bounds, a distance measure is obtained that\nanisometrically transforms the data, expanding distances at the boundary of the\ndata manifold. We show that using distances recovered from this transformation\nimproves the accuracy of unsupervised anomaly detection, compared to other\ncommonly used detectors, demonstrated over a large number of benchmark\ndatasets. As well as improved performance, this method has advantages over\nother unsupervised anomaly detection methods, including minimal requirements\nfor data preprocessing, native handling of missing data, and potential for\nvisualizations. By relating outlier scores to partitions of the Random Forest,\nwe develop a method for locally explainable anomaly predictions in terms of\nfeature importance.\n", "link": "http://arxiv.org/abs/2504.16075v1", "date": "2025-04-22", "relevancy": 2.1909, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4628}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4272}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20Unsupervised%20Anomaly%20Detection%20with%20Random%20Forest&body=Title%3A%20Explainable%20Unsupervised%20Anomaly%20Detection%20with%20Random%20Forest%0AAuthor%3A%20Joshua%20S.%20Harvey%20and%20Joshua%20Rosaler%20and%20Mingshu%20Li%20and%20Dhruv%20Desai%20and%20Dhagash%20Mehta%0AAbstract%3A%20%20%20We%20describe%20the%20use%20of%20an%20unsupervised%20Random%20Forest%20for%20similarity%20learning%0Aand%20improved%20unsupervised%20anomaly%20detection.%20By%20training%20a%20Random%20Forest%20to%0Adiscriminate%20between%20real%20data%20and%20synthetic%20data%20sampled%20from%20a%20uniform%0Adistribution%20over%20the%20real%20data%20bounds%2C%20a%20distance%20measure%20is%20obtained%20that%0Aanisometrically%20transforms%20the%20data%2C%20expanding%20distances%20at%20the%20boundary%20of%20the%0Adata%20manifold.%20We%20show%20that%20using%20distances%20recovered%20from%20this%20transformation%0Aimproves%20the%20accuracy%20of%20unsupervised%20anomaly%20detection%2C%20compared%20to%20other%0Acommonly%20used%20detectors%2C%20demonstrated%20over%20a%20large%20number%20of%20benchmark%0Adatasets.%20As%20well%20as%20improved%20performance%2C%20this%20method%20has%20advantages%20over%0Aother%20unsupervised%20anomaly%20detection%20methods%2C%20including%20minimal%20requirements%0Afor%20data%20preprocessing%2C%20native%20handling%20of%20missing%20data%2C%20and%20potential%20for%0Avisualizations.%20By%20relating%20outlier%20scores%20to%20partitions%20of%20the%20Random%20Forest%2C%0Awe%20develop%20a%20method%20for%20locally%20explainable%20anomaly%20predictions%20in%20terms%20of%0Afeature%20importance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520Unsupervised%2520Anomaly%2520Detection%2520with%2520Random%2520Forest%26entry.906535625%3DJoshua%2520S.%2520Harvey%2520and%2520Joshua%2520Rosaler%2520and%2520Mingshu%2520Li%2520and%2520Dhruv%2520Desai%2520and%2520Dhagash%2520Mehta%26entry.1292438233%3D%2520%2520We%2520describe%2520the%2520use%2520of%2520an%2520unsupervised%2520Random%2520Forest%2520for%2520similarity%2520learning%250Aand%2520improved%2520unsupervised%2520anomaly%2520detection.%2520By%2520training%2520a%2520Random%2520Forest%2520to%250Adiscriminate%2520between%2520real%2520data%2520and%2520synthetic%2520data%2520sampled%2520from%2520a%2520uniform%250Adistribution%2520over%2520the%2520real%2520data%2520bounds%252C%2520a%2520distance%2520measure%2520is%2520obtained%2520that%250Aanisometrically%2520transforms%2520the%2520data%252C%2520expanding%2520distances%2520at%2520the%2520boundary%2520of%2520the%250Adata%2520manifold.%2520We%2520show%2520that%2520using%2520distances%2520recovered%2520from%2520this%2520transformation%250Aimproves%2520the%2520accuracy%2520of%2520unsupervised%2520anomaly%2520detection%252C%2520compared%2520to%2520other%250Acommonly%2520used%2520detectors%252C%2520demonstrated%2520over%2520a%2520large%2520number%2520of%2520benchmark%250Adatasets.%2520As%2520well%2520as%2520improved%2520performance%252C%2520this%2520method%2520has%2520advantages%2520over%250Aother%2520unsupervised%2520anomaly%2520detection%2520methods%252C%2520including%2520minimal%2520requirements%250Afor%2520data%2520preprocessing%252C%2520native%2520handling%2520of%2520missing%2520data%252C%2520and%2520potential%2520for%250Avisualizations.%2520By%2520relating%2520outlier%2520scores%2520to%2520partitions%2520of%2520the%2520Random%2520Forest%252C%250Awe%2520develop%2520a%2520method%2520for%2520locally%2520explainable%2520anomaly%2520predictions%2520in%2520terms%2520of%250Afeature%2520importance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Unsupervised%20Anomaly%20Detection%20with%20Random%20Forest&entry.906535625=Joshua%20S.%20Harvey%20and%20Joshua%20Rosaler%20and%20Mingshu%20Li%20and%20Dhruv%20Desai%20and%20Dhagash%20Mehta&entry.1292438233=%20%20We%20describe%20the%20use%20of%20an%20unsupervised%20Random%20Forest%20for%20similarity%20learning%0Aand%20improved%20unsupervised%20anomaly%20detection.%20By%20training%20a%20Random%20Forest%20to%0Adiscriminate%20between%20real%20data%20and%20synthetic%20data%20sampled%20from%20a%20uniform%0Adistribution%20over%20the%20real%20data%20bounds%2C%20a%20distance%20measure%20is%20obtained%20that%0Aanisometrically%20transforms%20the%20data%2C%20expanding%20distances%20at%20the%20boundary%20of%20the%0Adata%20manifold.%20We%20show%20that%20using%20distances%20recovered%20from%20this%20transformation%0Aimproves%20the%20accuracy%20of%20unsupervised%20anomaly%20detection%2C%20compared%20to%20other%0Acommonly%20used%20detectors%2C%20demonstrated%20over%20a%20large%20number%20of%20benchmark%0Adatasets.%20As%20well%20as%20improved%20performance%2C%20this%20method%20has%20advantages%20over%0Aother%20unsupervised%20anomaly%20detection%20methods%2C%20including%20minimal%20requirements%0Afor%20data%20preprocessing%2C%20native%20handling%20of%20missing%20data%2C%20and%20potential%20for%0Avisualizations.%20By%20relating%20outlier%20scores%20to%20partitions%20of%20the%20Random%20Forest%2C%0Awe%20develop%20a%20method%20for%20locally%20explainable%20anomaly%20predictions%20in%20terms%20of%0Afeature%20importance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16075v1&entry.124074799=Read"},
{"title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation", "author": "Van Nguyen Nguyen and Stephen Tyree and Andrew Guo and Mederic Fourmy and Anas Gouda and Taeyeop Lee and Sungphill Moon and Hyeontae Son and Lukas Ranftl and Jonathan Tremblay and Eric Brachmann and Bertram Drost and Vincent Lepetit and Carsten Rother and Stan Birchfield and Jiri Matas and Yann Labbe and Martin Sundermeyer and Tomas Hodan", "abstract": "  We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the 6th in a series of public competitions organized to capture\nthe state of the art in 6D object pose estimation and related tasks. In 2024,\nour goal was to transition BOP from lab-like setups to real-world scenarios.\nFirst, we introduced new model-free tasks, where no 3D object models are\navailable and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks. Notably, the best 2024 method\nfor model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22%\nhigher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is\nonly 4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 13% more accurate\nthan GenFlow. Methods have similar rankings on 6D detection as on 6D\nlocalization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21--29% relative improvement\ncompared to the best 2023 method (CNOS). However, the 2D detection accuracy for\nunseen objects is still -35% behind the accuracy for seen objects (GDet2023),\nand the 2D detection stage is consequently the main bottleneck of existing\npipelines for 6D localization/detection of unseen objects. The online\nevaluation system stays open and is available at http://bop.felk.cvut.cz/\n", "link": "http://arxiv.org/abs/2504.02812v3", "date": "2025-04-22", "relevancy": 2.1879, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BOP%20Challenge%202024%20on%20Model-Based%20and%20Model-Free%206D%20Object%20Pose%0A%20%20Estimation&body=Title%3A%20BOP%20Challenge%202024%20on%20Model-Based%20and%20Model-Free%206D%20Object%20Pose%0A%20%20Estimation%0AAuthor%3A%20Van%20Nguyen%20Nguyen%20and%20Stephen%20Tyree%20and%20Andrew%20Guo%20and%20Mederic%20Fourmy%20and%20Anas%20Gouda%20and%20Taeyeop%20Lee%20and%20Sungphill%20Moon%20and%20Hyeontae%20Son%20and%20Lukas%20Ranftl%20and%20Jonathan%20Tremblay%20and%20Eric%20Brachmann%20and%20Bertram%20Drost%20and%20Vincent%20Lepetit%20and%20Carsten%20Rother%20and%20Stan%20Birchfield%20and%20Jiri%20Matas%20and%20Yann%20Labbe%20and%20Martin%20Sundermeyer%20and%20Tomas%20Hodan%0AAbstract%3A%20%20%20We%20present%20the%20evaluation%20methodology%2C%20datasets%20and%20results%20of%20the%20BOP%0AChallenge%202024%2C%20the%206th%20in%20a%20series%20of%20public%20competitions%20organized%20to%20capture%0Athe%20state%20of%20the%20art%20in%206D%20object%20pose%20estimation%20and%20related%20tasks.%20In%202024%2C%0Aour%20goal%20was%20to%20transition%20BOP%20from%20lab-like%20setups%20to%20real-world%20scenarios.%0AFirst%2C%20we%20introduced%20new%20model-free%20tasks%2C%20where%20no%203D%20object%20models%20are%0Aavailable%20and%20methods%20need%20to%20onboard%20objects%20just%20from%20provided%20reference%0Avideos.%20Second%2C%20we%20defined%20a%20new%2C%20more%20practical%206D%20object%20detection%20task%20where%0Aidentities%20of%20objects%20visible%20in%20a%20test%20image%20are%20not%20provided%20as%20input.%20Third%2C%0Awe%20introduced%20new%20BOP-H3%20datasets%20recorded%20with%20high-resolution%20sensors%20and%0AAR/VR%20headsets%2C%20closely%20resembling%20real-world%20scenarios.%20BOP-H3%20include%203D%0Amodels%20and%20onboarding%20videos%20to%20support%20both%20model-based%20and%20model-free%20tasks.%0AParticipants%20competed%20on%20seven%20challenge%20tracks.%20Notably%2C%20the%20best%202024%20method%0Afor%20model-based%206D%20localization%20of%20unseen%20objects%20%28FreeZeV2.1%29%20achieves%2022%25%0Ahigher%20accuracy%20on%20BOP-Classic-Core%20than%20the%20best%202023%20method%20%28GenFlow%29%2C%20and%20is%0Aonly%204%25%20behind%20the%20best%202023%20method%20for%20seen%20objects%20%28GPose2023%29%20although%20being%0Asignificantly%20slower%20%2824.9%20vs%202.7s%20per%20image%29.%20A%20more%20practical%202024%20method%20for%0Athis%20task%20is%20Co-op%20which%20takes%20only%200.8s%20per%20image%20and%20is%2013%25%20more%20accurate%0Athan%20GenFlow.%20Methods%20have%20similar%20rankings%20on%206D%20detection%20as%20on%206D%0Alocalization%20but%20higher%20run%20time.%20On%20model-based%202D%20detection%20of%20unseen%0Aobjects%2C%20the%20best%202024%20method%20%28MUSE%29%20achieves%2021--29%25%20relative%20improvement%0Acompared%20to%20the%20best%202023%20method%20%28CNOS%29.%20However%2C%20the%202D%20detection%20accuracy%20for%0Aunseen%20objects%20is%20still%20-35%25%20behind%20the%20accuracy%20for%20seen%20objects%20%28GDet2023%29%2C%0Aand%20the%202D%20detection%20stage%20is%20consequently%20the%20main%20bottleneck%20of%20existing%0Apipelines%20for%206D%20localization/detection%20of%20unseen%20objects.%20The%20online%0Aevaluation%20system%20stays%20open%20and%20is%20available%20at%20http%3A//bop.felk.cvut.cz/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02812v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBOP%2520Challenge%25202024%2520on%2520Model-Based%2520and%2520Model-Free%25206D%2520Object%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DVan%2520Nguyen%2520Nguyen%2520and%2520Stephen%2520Tyree%2520and%2520Andrew%2520Guo%2520and%2520Mederic%2520Fourmy%2520and%2520Anas%2520Gouda%2520and%2520Taeyeop%2520Lee%2520and%2520Sungphill%2520Moon%2520and%2520Hyeontae%2520Son%2520and%2520Lukas%2520Ranftl%2520and%2520Jonathan%2520Tremblay%2520and%2520Eric%2520Brachmann%2520and%2520Bertram%2520Drost%2520and%2520Vincent%2520Lepetit%2520and%2520Carsten%2520Rother%2520and%2520Stan%2520Birchfield%2520and%2520Jiri%2520Matas%2520and%2520Yann%2520Labbe%2520and%2520Martin%2520Sundermeyer%2520and%2520Tomas%2520Hodan%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520evaluation%2520methodology%252C%2520datasets%2520and%2520results%2520of%2520the%2520BOP%250AChallenge%25202024%252C%2520the%25206th%2520in%2520a%2520series%2520of%2520public%2520competitions%2520organized%2520to%2520capture%250Athe%2520state%2520of%2520the%2520art%2520in%25206D%2520object%2520pose%2520estimation%2520and%2520related%2520tasks.%2520In%25202024%252C%250Aour%2520goal%2520was%2520to%2520transition%2520BOP%2520from%2520lab-like%2520setups%2520to%2520real-world%2520scenarios.%250AFirst%252C%2520we%2520introduced%2520new%2520model-free%2520tasks%252C%2520where%2520no%25203D%2520object%2520models%2520are%250Aavailable%2520and%2520methods%2520need%2520to%2520onboard%2520objects%2520just%2520from%2520provided%2520reference%250Avideos.%2520Second%252C%2520we%2520defined%2520a%2520new%252C%2520more%2520practical%25206D%2520object%2520detection%2520task%2520where%250Aidentities%2520of%2520objects%2520visible%2520in%2520a%2520test%2520image%2520are%2520not%2520provided%2520as%2520input.%2520Third%252C%250Awe%2520introduced%2520new%2520BOP-H3%2520datasets%2520recorded%2520with%2520high-resolution%2520sensors%2520and%250AAR/VR%2520headsets%252C%2520closely%2520resembling%2520real-world%2520scenarios.%2520BOP-H3%2520include%25203D%250Amodels%2520and%2520onboarding%2520videos%2520to%2520support%2520both%2520model-based%2520and%2520model-free%2520tasks.%250AParticipants%2520competed%2520on%2520seven%2520challenge%2520tracks.%2520Notably%252C%2520the%2520best%25202024%2520method%250Afor%2520model-based%25206D%2520localization%2520of%2520unseen%2520objects%2520%2528FreeZeV2.1%2529%2520achieves%252022%2525%250Ahigher%2520accuracy%2520on%2520BOP-Classic-Core%2520than%2520the%2520best%25202023%2520method%2520%2528GenFlow%2529%252C%2520and%2520is%250Aonly%25204%2525%2520behind%2520the%2520best%25202023%2520method%2520for%2520seen%2520objects%2520%2528GPose2023%2529%2520although%2520being%250Asignificantly%2520slower%2520%252824.9%2520vs%25202.7s%2520per%2520image%2529.%2520A%2520more%2520practical%25202024%2520method%2520for%250Athis%2520task%2520is%2520Co-op%2520which%2520takes%2520only%25200.8s%2520per%2520image%2520and%2520is%252013%2525%2520more%2520accurate%250Athan%2520GenFlow.%2520Methods%2520have%2520similar%2520rankings%2520on%25206D%2520detection%2520as%2520on%25206D%250Alocalization%2520but%2520higher%2520run%2520time.%2520On%2520model-based%25202D%2520detection%2520of%2520unseen%250Aobjects%252C%2520the%2520best%25202024%2520method%2520%2528MUSE%2529%2520achieves%252021--29%2525%2520relative%2520improvement%250Acompared%2520to%2520the%2520best%25202023%2520method%2520%2528CNOS%2529.%2520However%252C%2520the%25202D%2520detection%2520accuracy%2520for%250Aunseen%2520objects%2520is%2520still%2520-35%2525%2520behind%2520the%2520accuracy%2520for%2520seen%2520objects%2520%2528GDet2023%2529%252C%250Aand%2520the%25202D%2520detection%2520stage%2520is%2520consequently%2520the%2520main%2520bottleneck%2520of%2520existing%250Apipelines%2520for%25206D%2520localization/detection%2520of%2520unseen%2520objects.%2520The%2520online%250Aevaluation%2520system%2520stays%2520open%2520and%2520is%2520available%2520at%2520http%253A//bop.felk.cvut.cz/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02812v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BOP%20Challenge%202024%20on%20Model-Based%20and%20Model-Free%206D%20Object%20Pose%0A%20%20Estimation&entry.906535625=Van%20Nguyen%20Nguyen%20and%20Stephen%20Tyree%20and%20Andrew%20Guo%20and%20Mederic%20Fourmy%20and%20Anas%20Gouda%20and%20Taeyeop%20Lee%20and%20Sungphill%20Moon%20and%20Hyeontae%20Son%20and%20Lukas%20Ranftl%20and%20Jonathan%20Tremblay%20and%20Eric%20Brachmann%20and%20Bertram%20Drost%20and%20Vincent%20Lepetit%20and%20Carsten%20Rother%20and%20Stan%20Birchfield%20and%20Jiri%20Matas%20and%20Yann%20Labbe%20and%20Martin%20Sundermeyer%20and%20Tomas%20Hodan&entry.1292438233=%20%20We%20present%20the%20evaluation%20methodology%2C%20datasets%20and%20results%20of%20the%20BOP%0AChallenge%202024%2C%20the%206th%20in%20a%20series%20of%20public%20competitions%20organized%20to%20capture%0Athe%20state%20of%20the%20art%20in%206D%20object%20pose%20estimation%20and%20related%20tasks.%20In%202024%2C%0Aour%20goal%20was%20to%20transition%20BOP%20from%20lab-like%20setups%20to%20real-world%20scenarios.%0AFirst%2C%20we%20introduced%20new%20model-free%20tasks%2C%20where%20no%203D%20object%20models%20are%0Aavailable%20and%20methods%20need%20to%20onboard%20objects%20just%20from%20provided%20reference%0Avideos.%20Second%2C%20we%20defined%20a%20new%2C%20more%20practical%206D%20object%20detection%20task%20where%0Aidentities%20of%20objects%20visible%20in%20a%20test%20image%20are%20not%20provided%20as%20input.%20Third%2C%0Awe%20introduced%20new%20BOP-H3%20datasets%20recorded%20with%20high-resolution%20sensors%20and%0AAR/VR%20headsets%2C%20closely%20resembling%20real-world%20scenarios.%20BOP-H3%20include%203D%0Amodels%20and%20onboarding%20videos%20to%20support%20both%20model-based%20and%20model-free%20tasks.%0AParticipants%20competed%20on%20seven%20challenge%20tracks.%20Notably%2C%20the%20best%202024%20method%0Afor%20model-based%206D%20localization%20of%20unseen%20objects%20%28FreeZeV2.1%29%20achieves%2022%25%0Ahigher%20accuracy%20on%20BOP-Classic-Core%20than%20the%20best%202023%20method%20%28GenFlow%29%2C%20and%20is%0Aonly%204%25%20behind%20the%20best%202023%20method%20for%20seen%20objects%20%28GPose2023%29%20although%20being%0Asignificantly%20slower%20%2824.9%20vs%202.7s%20per%20image%29.%20A%20more%20practical%202024%20method%20for%0Athis%20task%20is%20Co-op%20which%20takes%20only%200.8s%20per%20image%20and%20is%2013%25%20more%20accurate%0Athan%20GenFlow.%20Methods%20have%20similar%20rankings%20on%206D%20detection%20as%20on%206D%0Alocalization%20but%20higher%20run%20time.%20On%20model-based%202D%20detection%20of%20unseen%0Aobjects%2C%20the%20best%202024%20method%20%28MUSE%29%20achieves%2021--29%25%20relative%20improvement%0Acompared%20to%20the%20best%202023%20method%20%28CNOS%29.%20However%2C%20the%202D%20detection%20accuracy%20for%0Aunseen%20objects%20is%20still%20-35%25%20behind%20the%20accuracy%20for%20seen%20objects%20%28GDet2023%29%2C%0Aand%20the%202D%20detection%20stage%20is%20consequently%20the%20main%20bottleneck%20of%20existing%0Apipelines%20for%206D%20localization/detection%20of%20unseen%20objects.%20The%20online%0Aevaluation%20system%20stays%20open%20and%20is%20available%20at%20http%3A//bop.felk.cvut.cz/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02812v3&entry.124074799=Read"},
{"title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "author": "Lotfi Abdelkrim Mecharbat and Ibrahim Elmakky and Martin Takac and Mohammed Yaqub", "abstract": "  Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.\n", "link": "http://arxiv.org/abs/2504.15865v1", "date": "2025-04-22", "relevancy": 2.1812, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5574}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5389}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedNNS%3A%20Supernet-based%20Medical%20Task-Adaptive%20Neural%20Network%20Search&body=Title%3A%20MedNNS%3A%20Supernet-based%20Medical%20Task-Adaptive%20Neural%20Network%20Search%0AAuthor%3A%20Lotfi%20Abdelkrim%20Mecharbat%20and%20Ibrahim%20Elmakky%20and%20Martin%20Takac%20and%20Mohammed%20Yaqub%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20has%20achieved%20remarkable%20progress%20in%20the%20field%20of%20medical%0Aimaging.%20However%2C%20adapting%20DL%20models%20to%20medical%20tasks%20remains%20a%20significant%0Achallenge%2C%20primarily%20due%20to%20two%20key%20factors%3A%20%281%29%20architecture%20selection%2C%20as%0Adifferent%20tasks%20necessitate%20specialized%20model%20designs%2C%20and%20%282%29%20weight%0Ainitialization%2C%20which%20directly%20impacts%20the%20convergence%20speed%20and%20final%0Aperformance%20of%20the%20models.%20Although%20transfer%20learning%20from%20ImageNet%20is%20a%20widely%0Aadopted%20strategy%2C%20its%20effectiveness%20is%20constrained%20by%20the%20substantial%0Adifferences%20between%20natural%20and%20medical%20images.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Medical%20Neural%20Network%20Search%20%28MedNNS%29%2C%20the%20first%20Neural%20Network%0ASearch%20framework%20for%20medical%20imaging%20applications.%20MedNNS%20jointly%20optimizes%0Aarchitecture%20selection%20and%20weight%20initialization%20by%20constructing%20a%20meta-space%0Athat%20encodes%20datasets%20and%20models%20based%20on%20how%20well%20they%20perform%20together.%20We%0Abuild%20this%20space%20using%20a%20Supernetwork-based%20approach%2C%20expanding%20the%20model%20zoo%0Asize%20by%2051x%20times%20over%20previous%20state-of-the-art%20%28SOTA%29%20methods.%20Moreover%2C%20we%0Aintroduce%20rank%20loss%20and%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20loss%20into%20the%0Aconstruction%20of%20the%20space%20to%20capture%20inter-model%20and%20inter-dataset%0Arelationships%2C%20thereby%20achieving%20more%20accurate%20alignment%20in%20the%20meta-space.%0AExperimental%20results%20across%20multiple%20datasets%20demonstrate%20that%20MedNNS%0Asignificantly%20outperforms%20both%20ImageNet%20pre-trained%20DL%20models%20and%20SOTA%20Neural%0AArchitecture%20Search%20%28NAS%29%20methods%2C%20achieving%20an%20average%20accuracy%20improvement%20of%0A1.7%25%20across%20datasets%20while%20converging%20substantially%20faster.%20The%20code%20and%20the%0Aprocessed%20meta-space%20is%20available%20at%20https%3A//github.com/BioMedIA-MBZUAI/MedNNS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedNNS%253A%2520Supernet-based%2520Medical%2520Task-Adaptive%2520Neural%2520Network%2520Search%26entry.906535625%3DLotfi%2520Abdelkrim%2520Mecharbat%2520and%2520Ibrahim%2520Elmakky%2520and%2520Martin%2520Takac%2520and%2520Mohammed%2520Yaqub%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520has%2520achieved%2520remarkable%2520progress%2520in%2520the%2520field%2520of%2520medical%250Aimaging.%2520However%252C%2520adapting%2520DL%2520models%2520to%2520medical%2520tasks%2520remains%2520a%2520significant%250Achallenge%252C%2520primarily%2520due%2520to%2520two%2520key%2520factors%253A%2520%25281%2529%2520architecture%2520selection%252C%2520as%250Adifferent%2520tasks%2520necessitate%2520specialized%2520model%2520designs%252C%2520and%2520%25282%2529%2520weight%250Ainitialization%252C%2520which%2520directly%2520impacts%2520the%2520convergence%2520speed%2520and%2520final%250Aperformance%2520of%2520the%2520models.%2520Although%2520transfer%2520learning%2520from%2520ImageNet%2520is%2520a%2520widely%250Aadopted%2520strategy%252C%2520its%2520effectiveness%2520is%2520constrained%2520by%2520the%2520substantial%250Adifferences%2520between%2520natural%2520and%2520medical%2520images.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520Medical%2520Neural%2520Network%2520Search%2520%2528MedNNS%2529%252C%2520the%2520first%2520Neural%2520Network%250ASearch%2520framework%2520for%2520medical%2520imaging%2520applications.%2520MedNNS%2520jointly%2520optimizes%250Aarchitecture%2520selection%2520and%2520weight%2520initialization%2520by%2520constructing%2520a%2520meta-space%250Athat%2520encodes%2520datasets%2520and%2520models%2520based%2520on%2520how%2520well%2520they%2520perform%2520together.%2520We%250Abuild%2520this%2520space%2520using%2520a%2520Supernetwork-based%2520approach%252C%2520expanding%2520the%2520model%2520zoo%250Asize%2520by%252051x%2520times%2520over%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520methods.%2520Moreover%252C%2520we%250Aintroduce%2520rank%2520loss%2520and%2520Fr%255C%2527echet%2520Inception%2520Distance%2520%2528FID%2529%2520loss%2520into%2520the%250Aconstruction%2520of%2520the%2520space%2520to%2520capture%2520inter-model%2520and%2520inter-dataset%250Arelationships%252C%2520thereby%2520achieving%2520more%2520accurate%2520alignment%2520in%2520the%2520meta-space.%250AExperimental%2520results%2520across%2520multiple%2520datasets%2520demonstrate%2520that%2520MedNNS%250Asignificantly%2520outperforms%2520both%2520ImageNet%2520pre-trained%2520DL%2520models%2520and%2520SOTA%2520Neural%250AArchitecture%2520Search%2520%2528NAS%2529%2520methods%252C%2520achieving%2520an%2520average%2520accuracy%2520improvement%2520of%250A1.7%2525%2520across%2520datasets%2520while%2520converging%2520substantially%2520faster.%2520The%2520code%2520and%2520the%250Aprocessed%2520meta-space%2520is%2520available%2520at%2520https%253A//github.com/BioMedIA-MBZUAI/MedNNS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedNNS%3A%20Supernet-based%20Medical%20Task-Adaptive%20Neural%20Network%20Search&entry.906535625=Lotfi%20Abdelkrim%20Mecharbat%20and%20Ibrahim%20Elmakky%20and%20Martin%20Takac%20and%20Mohammed%20Yaqub&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20has%20achieved%20remarkable%20progress%20in%20the%20field%20of%20medical%0Aimaging.%20However%2C%20adapting%20DL%20models%20to%20medical%20tasks%20remains%20a%20significant%0Achallenge%2C%20primarily%20due%20to%20two%20key%20factors%3A%20%281%29%20architecture%20selection%2C%20as%0Adifferent%20tasks%20necessitate%20specialized%20model%20designs%2C%20and%20%282%29%20weight%0Ainitialization%2C%20which%20directly%20impacts%20the%20convergence%20speed%20and%20final%0Aperformance%20of%20the%20models.%20Although%20transfer%20learning%20from%20ImageNet%20is%20a%20widely%0Aadopted%20strategy%2C%20its%20effectiveness%20is%20constrained%20by%20the%20substantial%0Adifferences%20between%20natural%20and%20medical%20images.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Medical%20Neural%20Network%20Search%20%28MedNNS%29%2C%20the%20first%20Neural%20Network%0ASearch%20framework%20for%20medical%20imaging%20applications.%20MedNNS%20jointly%20optimizes%0Aarchitecture%20selection%20and%20weight%20initialization%20by%20constructing%20a%20meta-space%0Athat%20encodes%20datasets%20and%20models%20based%20on%20how%20well%20they%20perform%20together.%20We%0Abuild%20this%20space%20using%20a%20Supernetwork-based%20approach%2C%20expanding%20the%20model%20zoo%0Asize%20by%2051x%20times%20over%20previous%20state-of-the-art%20%28SOTA%29%20methods.%20Moreover%2C%20we%0Aintroduce%20rank%20loss%20and%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20loss%20into%20the%0Aconstruction%20of%20the%20space%20to%20capture%20inter-model%20and%20inter-dataset%0Arelationships%2C%20thereby%20achieving%20more%20accurate%20alignment%20in%20the%20meta-space.%0AExperimental%20results%20across%20multiple%20datasets%20demonstrate%20that%20MedNNS%0Asignificantly%20outperforms%20both%20ImageNet%20pre-trained%20DL%20models%20and%20SOTA%20Neural%0AArchitecture%20Search%20%28NAS%29%20methods%2C%20achieving%20an%20average%20accuracy%20improvement%20of%0A1.7%25%20across%20datasets%20while%20converging%20substantially%20faster.%20The%20code%20and%20the%0Aprocessed%20meta-space%20is%20available%20at%20https%3A//github.com/BioMedIA-MBZUAI/MedNNS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15865v1&entry.124074799=Read"},
{"title": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments", "author": "Marharyta Domnich and Julius V\u00e4lja and Rasmus Moorits Veski and Giacomo Magnifico and Kadi Tulver and Eduard Barbu and Raul Vicente", "abstract": "  As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks.\n", "link": "http://arxiv.org/abs/2410.21131v3", "date": "2025-04-22", "relevancy": 2.169, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5414}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Unifying%20Evaluation%20of%20Counterfactual%20Explanations%3A%20Leveraging%0A%20%20Large%20Language%20Models%20for%20Human-Centric%20Assessments&body=Title%3A%20Towards%20Unifying%20Evaluation%20of%20Counterfactual%20Explanations%3A%20Leveraging%0A%20%20Large%20Language%20Models%20for%20Human-Centric%20Assessments%0AAuthor%3A%20Marharyta%20Domnich%20and%20Julius%20V%C3%A4lja%20and%20Rasmus%20Moorits%20Veski%20and%20Giacomo%20Magnifico%20and%20Kadi%20Tulver%20and%20Eduard%20Barbu%20and%20Raul%20Vicente%0AAbstract%3A%20%20%20As%20machine%20learning%20models%20evolve%2C%20maintaining%20transparency%20demands%20more%0Ahuman-centric%20explainable%20AI%20techniques.%20Counterfactual%20explanations%2C%20with%0Aroots%20in%20human%20reasoning%2C%20identify%20the%20minimal%20input%20changes%20needed%20to%20obtain%20a%0Agiven%20output%20and%2C%20hence%2C%20are%20crucial%20for%20supporting%20decision-making.%20Despite%0Atheir%20importance%2C%20the%20evaluation%20of%20these%20explanations%20often%20lacks%20grounding%20in%0Auser%20studies%20and%20remains%20fragmented%2C%20with%20existing%20metrics%20not%20fully%20capturing%0Ahuman%20perspectives.%20To%20address%20this%20challenge%2C%20we%20developed%20a%20diverse%20set%20of%2030%0Acounterfactual%20scenarios%20and%20collected%20ratings%20across%208%20evaluation%20metrics%20from%0A206%20respondents.%20Subsequently%2C%20we%20fine-tuned%20different%20Large%20Language%20Models%0A%28LLMs%29%20to%20predict%20average%20or%20individual%20human%20judgment%20across%20these%20metrics.%0AOur%20methodology%20allowed%20LLMs%20to%20achieve%20an%20accuracy%20of%20up%20to%2063%25%20in%20zero-shot%0Aevaluations%20and%2085%25%20%28over%20a%203-classes%20prediction%29%20with%20fine-tuning%20across%20all%0Ametrics.%20The%20fine-tuned%20models%20predicting%20human%20ratings%20offer%20better%0Acomparability%20and%20scalability%20in%20evaluating%20different%20counterfactual%0Aexplanation%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21131v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Unifying%2520Evaluation%2520of%2520Counterfactual%2520Explanations%253A%2520Leveraging%250A%2520%2520Large%2520Language%2520Models%2520for%2520Human-Centric%2520Assessments%26entry.906535625%3DMarharyta%2520Domnich%2520and%2520Julius%2520V%25C3%25A4lja%2520and%2520Rasmus%2520Moorits%2520Veski%2520and%2520Giacomo%2520Magnifico%2520and%2520Kadi%2520Tulver%2520and%2520Eduard%2520Barbu%2520and%2520Raul%2520Vicente%26entry.1292438233%3D%2520%2520As%2520machine%2520learning%2520models%2520evolve%252C%2520maintaining%2520transparency%2520demands%2520more%250Ahuman-centric%2520explainable%2520AI%2520techniques.%2520Counterfactual%2520explanations%252C%2520with%250Aroots%2520in%2520human%2520reasoning%252C%2520identify%2520the%2520minimal%2520input%2520changes%2520needed%2520to%2520obtain%2520a%250Agiven%2520output%2520and%252C%2520hence%252C%2520are%2520crucial%2520for%2520supporting%2520decision-making.%2520Despite%250Atheir%2520importance%252C%2520the%2520evaluation%2520of%2520these%2520explanations%2520often%2520lacks%2520grounding%2520in%250Auser%2520studies%2520and%2520remains%2520fragmented%252C%2520with%2520existing%2520metrics%2520not%2520fully%2520capturing%250Ahuman%2520perspectives.%2520To%2520address%2520this%2520challenge%252C%2520we%2520developed%2520a%2520diverse%2520set%2520of%252030%250Acounterfactual%2520scenarios%2520and%2520collected%2520ratings%2520across%25208%2520evaluation%2520metrics%2520from%250A206%2520respondents.%2520Subsequently%252C%2520we%2520fine-tuned%2520different%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520to%2520predict%2520average%2520or%2520individual%2520human%2520judgment%2520across%2520these%2520metrics.%250AOur%2520methodology%2520allowed%2520LLMs%2520to%2520achieve%2520an%2520accuracy%2520of%2520up%2520to%252063%2525%2520in%2520zero-shot%250Aevaluations%2520and%252085%2525%2520%2528over%2520a%25203-classes%2520prediction%2529%2520with%2520fine-tuning%2520across%2520all%250Ametrics.%2520The%2520fine-tuned%2520models%2520predicting%2520human%2520ratings%2520offer%2520better%250Acomparability%2520and%2520scalability%2520in%2520evaluating%2520different%2520counterfactual%250Aexplanation%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21131v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unifying%20Evaluation%20of%20Counterfactual%20Explanations%3A%20Leveraging%0A%20%20Large%20Language%20Models%20for%20Human-Centric%20Assessments&entry.906535625=Marharyta%20Domnich%20and%20Julius%20V%C3%A4lja%20and%20Rasmus%20Moorits%20Veski%20and%20Giacomo%20Magnifico%20and%20Kadi%20Tulver%20and%20Eduard%20Barbu%20and%20Raul%20Vicente&entry.1292438233=%20%20As%20machine%20learning%20models%20evolve%2C%20maintaining%20transparency%20demands%20more%0Ahuman-centric%20explainable%20AI%20techniques.%20Counterfactual%20explanations%2C%20with%0Aroots%20in%20human%20reasoning%2C%20identify%20the%20minimal%20input%20changes%20needed%20to%20obtain%20a%0Agiven%20output%20and%2C%20hence%2C%20are%20crucial%20for%20supporting%20decision-making.%20Despite%0Atheir%20importance%2C%20the%20evaluation%20of%20these%20explanations%20often%20lacks%20grounding%20in%0Auser%20studies%20and%20remains%20fragmented%2C%20with%20existing%20metrics%20not%20fully%20capturing%0Ahuman%20perspectives.%20To%20address%20this%20challenge%2C%20we%20developed%20a%20diverse%20set%20of%2030%0Acounterfactual%20scenarios%20and%20collected%20ratings%20across%208%20evaluation%20metrics%20from%0A206%20respondents.%20Subsequently%2C%20we%20fine-tuned%20different%20Large%20Language%20Models%0A%28LLMs%29%20to%20predict%20average%20or%20individual%20human%20judgment%20across%20these%20metrics.%0AOur%20methodology%20allowed%20LLMs%20to%20achieve%20an%20accuracy%20of%20up%20to%2063%25%20in%20zero-shot%0Aevaluations%20and%2085%25%20%28over%20a%203-classes%20prediction%29%20with%20fine-tuning%20across%20all%0Ametrics.%20The%20fine-tuned%20models%20predicting%20human%20ratings%20offer%20better%0Acomparability%20and%20scalability%20in%20evaluating%20different%20counterfactual%0Aexplanation%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21131v3&entry.124074799=Read"},
{"title": "New Recipe for Semi-supervised Community Detection: Clique Annealing\n  under Crystallization Kinetics", "author": "Ling Cheng and Jiashu Pu and Ruicheng Liang and Qian Shao and Hezhe Qiao and Feida Zhu", "abstract": "  Semi-supervised community detection methods are widely used for identifying\nspecific communities due to the label scarcity. Existing semi-supervised\ncommunity detection methods typically involve two learning stages learning in\nboth initial identification and subsequent adjustment, which often starts from\nan unreasonable community core candidate. Moreover, these methods encounter\nscalability issues because they depend on reinforcement learning and generative\nadversarial networks, leading to higher computational costs and restricting the\nselection of candidates. To address these limitations, we draw a parallel\nbetween crystallization kinetics and community detection to integrate the\nspontaneity of the annealing process into community detection. Specifically, we\nliken community detection to identifying a crystal subgrain (core) that expands\ninto a complete grain (community) through a process similar to annealing. Based\non this finding, we propose CLique ANNealing (CLANN), which applies kinetics\nconcepts to community detection by integrating these principles into the\noptimization process to strengthen the consistency of the community core.\nSubsequently, a learning-free Transitive Annealer was employed to refine the\nfirst-stage candidates by merging neighboring cliques and repositioning the\ncommunity core, enabling a spontaneous growth process that enhances\nscalability. Extensive experiments on \\textbf{43} different network settings\ndemonstrate that CLANN outperforms state-of-the-art methods across multiple\nreal-world datasets, showcasing its exceptional efficacy and efficiency in\ncommunity detection.\n", "link": "http://arxiv.org/abs/2504.15927v1", "date": "2025-04-22", "relevancy": 2.1511, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.438}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4265}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20New%20Recipe%20for%20Semi-supervised%20Community%20Detection%3A%20Clique%20Annealing%0A%20%20under%20Crystallization%20Kinetics&body=Title%3A%20New%20Recipe%20for%20Semi-supervised%20Community%20Detection%3A%20Clique%20Annealing%0A%20%20under%20Crystallization%20Kinetics%0AAuthor%3A%20Ling%20Cheng%20and%20Jiashu%20Pu%20and%20Ruicheng%20Liang%20and%20Qian%20Shao%20and%20Hezhe%20Qiao%20and%20Feida%20Zhu%0AAbstract%3A%20%20%20Semi-supervised%20community%20detection%20methods%20are%20widely%20used%20for%20identifying%0Aspecific%20communities%20due%20to%20the%20label%20scarcity.%20Existing%20semi-supervised%0Acommunity%20detection%20methods%20typically%20involve%20two%20learning%20stages%20learning%20in%0Aboth%20initial%20identification%20and%20subsequent%20adjustment%2C%20which%20often%20starts%20from%0Aan%20unreasonable%20community%20core%20candidate.%20Moreover%2C%20these%20methods%20encounter%0Ascalability%20issues%20because%20they%20depend%20on%20reinforcement%20learning%20and%20generative%0Aadversarial%20networks%2C%20leading%20to%20higher%20computational%20costs%20and%20restricting%20the%0Aselection%20of%20candidates.%20To%20address%20these%20limitations%2C%20we%20draw%20a%20parallel%0Abetween%20crystallization%20kinetics%20and%20community%20detection%20to%20integrate%20the%0Aspontaneity%20of%20the%20annealing%20process%20into%20community%20detection.%20Specifically%2C%20we%0Aliken%20community%20detection%20to%20identifying%20a%20crystal%20subgrain%20%28core%29%20that%20expands%0Ainto%20a%20complete%20grain%20%28community%29%20through%20a%20process%20similar%20to%20annealing.%20Based%0Aon%20this%20finding%2C%20we%20propose%20CLique%20ANNealing%20%28CLANN%29%2C%20which%20applies%20kinetics%0Aconcepts%20to%20community%20detection%20by%20integrating%20these%20principles%20into%20the%0Aoptimization%20process%20to%20strengthen%20the%20consistency%20of%20the%20community%20core.%0ASubsequently%2C%20a%20learning-free%20Transitive%20Annealer%20was%20employed%20to%20refine%20the%0Afirst-stage%20candidates%20by%20merging%20neighboring%20cliques%20and%20repositioning%20the%0Acommunity%20core%2C%20enabling%20a%20spontaneous%20growth%20process%20that%20enhances%0Ascalability.%20Extensive%20experiments%20on%20%5Ctextbf%7B43%7D%20different%20network%20settings%0Ademonstrate%20that%20CLANN%20outperforms%20state-of-the-art%20methods%20across%20multiple%0Areal-world%20datasets%2C%20showcasing%20its%20exceptional%20efficacy%20and%20efficiency%20in%0Acommunity%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNew%2520Recipe%2520for%2520Semi-supervised%2520Community%2520Detection%253A%2520Clique%2520Annealing%250A%2520%2520under%2520Crystallization%2520Kinetics%26entry.906535625%3DLing%2520Cheng%2520and%2520Jiashu%2520Pu%2520and%2520Ruicheng%2520Liang%2520and%2520Qian%2520Shao%2520and%2520Hezhe%2520Qiao%2520and%2520Feida%2520Zhu%26entry.1292438233%3D%2520%2520Semi-supervised%2520community%2520detection%2520methods%2520are%2520widely%2520used%2520for%2520identifying%250Aspecific%2520communities%2520due%2520to%2520the%2520label%2520scarcity.%2520Existing%2520semi-supervised%250Acommunity%2520detection%2520methods%2520typically%2520involve%2520two%2520learning%2520stages%2520learning%2520in%250Aboth%2520initial%2520identification%2520and%2520subsequent%2520adjustment%252C%2520which%2520often%2520starts%2520from%250Aan%2520unreasonable%2520community%2520core%2520candidate.%2520Moreover%252C%2520these%2520methods%2520encounter%250Ascalability%2520issues%2520because%2520they%2520depend%2520on%2520reinforcement%2520learning%2520and%2520generative%250Aadversarial%2520networks%252C%2520leading%2520to%2520higher%2520computational%2520costs%2520and%2520restricting%2520the%250Aselection%2520of%2520candidates.%2520To%2520address%2520these%2520limitations%252C%2520we%2520draw%2520a%2520parallel%250Abetween%2520crystallization%2520kinetics%2520and%2520community%2520detection%2520to%2520integrate%2520the%250Aspontaneity%2520of%2520the%2520annealing%2520process%2520into%2520community%2520detection.%2520Specifically%252C%2520we%250Aliken%2520community%2520detection%2520to%2520identifying%2520a%2520crystal%2520subgrain%2520%2528core%2529%2520that%2520expands%250Ainto%2520a%2520complete%2520grain%2520%2528community%2529%2520through%2520a%2520process%2520similar%2520to%2520annealing.%2520Based%250Aon%2520this%2520finding%252C%2520we%2520propose%2520CLique%2520ANNealing%2520%2528CLANN%2529%252C%2520which%2520applies%2520kinetics%250Aconcepts%2520to%2520community%2520detection%2520by%2520integrating%2520these%2520principles%2520into%2520the%250Aoptimization%2520process%2520to%2520strengthen%2520the%2520consistency%2520of%2520the%2520community%2520core.%250ASubsequently%252C%2520a%2520learning-free%2520Transitive%2520Annealer%2520was%2520employed%2520to%2520refine%2520the%250Afirst-stage%2520candidates%2520by%2520merging%2520neighboring%2520cliques%2520and%2520repositioning%2520the%250Acommunity%2520core%252C%2520enabling%2520a%2520spontaneous%2520growth%2520process%2520that%2520enhances%250Ascalability.%2520Extensive%2520experiments%2520on%2520%255Ctextbf%257B43%257D%2520different%2520network%2520settings%250Ademonstrate%2520that%2520CLANN%2520outperforms%2520state-of-the-art%2520methods%2520across%2520multiple%250Areal-world%2520datasets%252C%2520showcasing%2520its%2520exceptional%2520efficacy%2520and%2520efficiency%2520in%250Acommunity%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Recipe%20for%20Semi-supervised%20Community%20Detection%3A%20Clique%20Annealing%0A%20%20under%20Crystallization%20Kinetics&entry.906535625=Ling%20Cheng%20and%20Jiashu%20Pu%20and%20Ruicheng%20Liang%20and%20Qian%20Shao%20and%20Hezhe%20Qiao%20and%20Feida%20Zhu&entry.1292438233=%20%20Semi-supervised%20community%20detection%20methods%20are%20widely%20used%20for%20identifying%0Aspecific%20communities%20due%20to%20the%20label%20scarcity.%20Existing%20semi-supervised%0Acommunity%20detection%20methods%20typically%20involve%20two%20learning%20stages%20learning%20in%0Aboth%20initial%20identification%20and%20subsequent%20adjustment%2C%20which%20often%20starts%20from%0Aan%20unreasonable%20community%20core%20candidate.%20Moreover%2C%20these%20methods%20encounter%0Ascalability%20issues%20because%20they%20depend%20on%20reinforcement%20learning%20and%20generative%0Aadversarial%20networks%2C%20leading%20to%20higher%20computational%20costs%20and%20restricting%20the%0Aselection%20of%20candidates.%20To%20address%20these%20limitations%2C%20we%20draw%20a%20parallel%0Abetween%20crystallization%20kinetics%20and%20community%20detection%20to%20integrate%20the%0Aspontaneity%20of%20the%20annealing%20process%20into%20community%20detection.%20Specifically%2C%20we%0Aliken%20community%20detection%20to%20identifying%20a%20crystal%20subgrain%20%28core%29%20that%20expands%0Ainto%20a%20complete%20grain%20%28community%29%20through%20a%20process%20similar%20to%20annealing.%20Based%0Aon%20this%20finding%2C%20we%20propose%20CLique%20ANNealing%20%28CLANN%29%2C%20which%20applies%20kinetics%0Aconcepts%20to%20community%20detection%20by%20integrating%20these%20principles%20into%20the%0Aoptimization%20process%20to%20strengthen%20the%20consistency%20of%20the%20community%20core.%0ASubsequently%2C%20a%20learning-free%20Transitive%20Annealer%20was%20employed%20to%20refine%20the%0Afirst-stage%20candidates%20by%20merging%20neighboring%20cliques%20and%20repositioning%20the%0Acommunity%20core%2C%20enabling%20a%20spontaneous%20growth%20process%20that%20enhances%0Ascalability.%20Extensive%20experiments%20on%20%5Ctextbf%7B43%7D%20different%20network%20settings%0Ademonstrate%20that%20CLANN%20outperforms%20state-of-the-art%20methods%20across%20multiple%0Areal-world%20datasets%2C%20showcasing%20its%20exceptional%20efficacy%20and%20efficiency%20in%0Acommunity%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15927v1&entry.124074799=Read"},
{"title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale", "author": "Joya Chen and Ziyun Zeng and Yiqi Lin and Wei Li and Zejun Ma and Mike Zheng Shou", "abstract": "  Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.\n", "link": "http://arxiv.org/abs/2504.16030v1", "date": "2025-04-22", "relevancy": 2.1439, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiveCC%3A%20Learning%20Video%20LLM%20with%20Streaming%20Speech%20Transcription%20at%20Scale&body=Title%3A%20LiveCC%3A%20Learning%20Video%20LLM%20with%20Streaming%20Speech%20Transcription%20at%20Scale%0AAuthor%3A%20Joya%20Chen%20and%20Ziyun%20Zeng%20and%20Yiqi%20Lin%20and%20Wei%20Li%20and%20Zejun%20Ma%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Recent%20video%20large%20language%20models%20%28Video%20LLMs%29%20often%20depend%20on%20costly%20human%0Aannotations%20or%20proprietary%20model%20APIs%20%28e.g.%2C%20GPT-4o%29%20to%20produce%20training%20data%2C%0Awhich%20limits%20their%20training%20at%20scale.%20In%20this%20paper%2C%20we%20explore%20large-scale%0Atraining%20for%20Video%20LLM%20with%20cheap%20automatic%20speech%20recognition%20%28ASR%29%0Atranscripts.%20Specifically%2C%20we%20propose%20a%20novel%20streaming%20training%20approach%20that%0Adensely%20interleaves%20the%20ASR%20words%20and%20video%20frames%20according%20to%20their%0Atimestamps.%20Compared%20to%20previous%20studies%20in%20vision-language%20representation%20with%0AASR%2C%20our%20method%20naturally%20fits%20the%20streaming%20characteristics%20of%20ASR%2C%20thus%0Aenabling%20the%20model%20to%20learn%20temporally-aligned%2C%20fine-grained%20vision-language%0Amodeling.%20To%20support%20the%20training%20algorithm%2C%20we%20introduce%20a%20data%20production%0Apipeline%20to%20process%20YouTube%20videos%20and%20their%20closed%20captions%20%28CC%2C%20same%20as%20ASR%29%2C%0Aresulting%20in%20Live-CC-5M%20dataset%20for%20pre-training%20and%20Live-WhisperX-526K%20dataset%0Afor%20high-quality%20supervised%20fine-tuning%20%28SFT%29.%20Remarkably%2C%20even%20without%20SFT%2C%0Athe%20ASR-only%20pre-trained%20LiveCC-7B-Base%20model%20demonstrates%20competitive%20general%0Avideo%20QA%20performance%20and%20exhibits%20a%20new%20capability%20in%20real-time%20video%0Acommentary.%20To%20evaluate%20this%2C%20we%20carefully%20design%20a%20new%20LiveSports-3K%0Abenchmark%2C%20using%20LLM-as-a-judge%20to%20measure%20the%20free-form%20commentary.%0AExperiments%20show%20our%20final%20LiveCC-7B-Instruct%20model%20can%20surpass%20advanced%2072B%0Amodels%20%28Qwen2.5-VL-72B-Instruct%2C%20LLaVA-Video-72B%29%20in%20commentary%20quality%20even%0Aworking%20in%20a%20real-time%20mode.%20Meanwhile%2C%20it%20achieves%20state-of-the-art%20results%20at%0Athe%207B/8B%20scale%20on%20popular%20video%20QA%20benchmarks%20such%20as%20VideoMME%20and%20OVOBench%2C%0Ademonstrating%20the%20broad%20generalizability%20of%20our%20approach.%20All%20resources%20of%20this%0Apaper%20have%20been%20released%20at%20https%3A//showlab.github.io/livecc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiveCC%253A%2520Learning%2520Video%2520LLM%2520with%2520Streaming%2520Speech%2520Transcription%2520at%2520Scale%26entry.906535625%3DJoya%2520Chen%2520and%2520Ziyun%2520Zeng%2520and%2520Yiqi%2520Lin%2520and%2520Wei%2520Li%2520and%2520Zejun%2520Ma%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Recent%2520video%2520large%2520language%2520models%2520%2528Video%2520LLMs%2529%2520often%2520depend%2520on%2520costly%2520human%250Aannotations%2520or%2520proprietary%2520model%2520APIs%2520%2528e.g.%252C%2520GPT-4o%2529%2520to%2520produce%2520training%2520data%252C%250Awhich%2520limits%2520their%2520training%2520at%2520scale.%2520In%2520this%2520paper%252C%2520we%2520explore%2520large-scale%250Atraining%2520for%2520Video%2520LLM%2520with%2520cheap%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%250Atranscripts.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520streaming%2520training%2520approach%2520that%250Adensely%2520interleaves%2520the%2520ASR%2520words%2520and%2520video%2520frames%2520according%2520to%2520their%250Atimestamps.%2520Compared%2520to%2520previous%2520studies%2520in%2520vision-language%2520representation%2520with%250AASR%252C%2520our%2520method%2520naturally%2520fits%2520the%2520streaming%2520characteristics%2520of%2520ASR%252C%2520thus%250Aenabling%2520the%2520model%2520to%2520learn%2520temporally-aligned%252C%2520fine-grained%2520vision-language%250Amodeling.%2520To%2520support%2520the%2520training%2520algorithm%252C%2520we%2520introduce%2520a%2520data%2520production%250Apipeline%2520to%2520process%2520YouTube%2520videos%2520and%2520their%2520closed%2520captions%2520%2528CC%252C%2520same%2520as%2520ASR%2529%252C%250Aresulting%2520in%2520Live-CC-5M%2520dataset%2520for%2520pre-training%2520and%2520Live-WhisperX-526K%2520dataset%250Afor%2520high-quality%2520supervised%2520fine-tuning%2520%2528SFT%2529.%2520Remarkably%252C%2520even%2520without%2520SFT%252C%250Athe%2520ASR-only%2520pre-trained%2520LiveCC-7B-Base%2520model%2520demonstrates%2520competitive%2520general%250Avideo%2520QA%2520performance%2520and%2520exhibits%2520a%2520new%2520capability%2520in%2520real-time%2520video%250Acommentary.%2520To%2520evaluate%2520this%252C%2520we%2520carefully%2520design%2520a%2520new%2520LiveSports-3K%250Abenchmark%252C%2520using%2520LLM-as-a-judge%2520to%2520measure%2520the%2520free-form%2520commentary.%250AExperiments%2520show%2520our%2520final%2520LiveCC-7B-Instruct%2520model%2520can%2520surpass%2520advanced%252072B%250Amodels%2520%2528Qwen2.5-VL-72B-Instruct%252C%2520LLaVA-Video-72B%2529%2520in%2520commentary%2520quality%2520even%250Aworking%2520in%2520a%2520real-time%2520mode.%2520Meanwhile%252C%2520it%2520achieves%2520state-of-the-art%2520results%2520at%250Athe%25207B/8B%2520scale%2520on%2520popular%2520video%2520QA%2520benchmarks%2520such%2520as%2520VideoMME%2520and%2520OVOBench%252C%250Ademonstrating%2520the%2520broad%2520generalizability%2520of%2520our%2520approach.%2520All%2520resources%2520of%2520this%250Apaper%2520have%2520been%2520released%2520at%2520https%253A//showlab.github.io/livecc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiveCC%3A%20Learning%20Video%20LLM%20with%20Streaming%20Speech%20Transcription%20at%20Scale&entry.906535625=Joya%20Chen%20and%20Ziyun%20Zeng%20and%20Yiqi%20Lin%20and%20Wei%20Li%20and%20Zejun%20Ma%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Recent%20video%20large%20language%20models%20%28Video%20LLMs%29%20often%20depend%20on%20costly%20human%0Aannotations%20or%20proprietary%20model%20APIs%20%28e.g.%2C%20GPT-4o%29%20to%20produce%20training%20data%2C%0Awhich%20limits%20their%20training%20at%20scale.%20In%20this%20paper%2C%20we%20explore%20large-scale%0Atraining%20for%20Video%20LLM%20with%20cheap%20automatic%20speech%20recognition%20%28ASR%29%0Atranscripts.%20Specifically%2C%20we%20propose%20a%20novel%20streaming%20training%20approach%20that%0Adensely%20interleaves%20the%20ASR%20words%20and%20video%20frames%20according%20to%20their%0Atimestamps.%20Compared%20to%20previous%20studies%20in%20vision-language%20representation%20with%0AASR%2C%20our%20method%20naturally%20fits%20the%20streaming%20characteristics%20of%20ASR%2C%20thus%0Aenabling%20the%20model%20to%20learn%20temporally-aligned%2C%20fine-grained%20vision-language%0Amodeling.%20To%20support%20the%20training%20algorithm%2C%20we%20introduce%20a%20data%20production%0Apipeline%20to%20process%20YouTube%20videos%20and%20their%20closed%20captions%20%28CC%2C%20same%20as%20ASR%29%2C%0Aresulting%20in%20Live-CC-5M%20dataset%20for%20pre-training%20and%20Live-WhisperX-526K%20dataset%0Afor%20high-quality%20supervised%20fine-tuning%20%28SFT%29.%20Remarkably%2C%20even%20without%20SFT%2C%0Athe%20ASR-only%20pre-trained%20LiveCC-7B-Base%20model%20demonstrates%20competitive%20general%0Avideo%20QA%20performance%20and%20exhibits%20a%20new%20capability%20in%20real-time%20video%0Acommentary.%20To%20evaluate%20this%2C%20we%20carefully%20design%20a%20new%20LiveSports-3K%0Abenchmark%2C%20using%20LLM-as-a-judge%20to%20measure%20the%20free-form%20commentary.%0AExperiments%20show%20our%20final%20LiveCC-7B-Instruct%20model%20can%20surpass%20advanced%2072B%0Amodels%20%28Qwen2.5-VL-72B-Instruct%2C%20LLaVA-Video-72B%29%20in%20commentary%20quality%20even%0Aworking%20in%20a%20real-time%20mode.%20Meanwhile%2C%20it%20achieves%20state-of-the-art%20results%20at%0Athe%207B/8B%20scale%20on%20popular%20video%20QA%20benchmarks%20such%20as%20VideoMME%20and%20OVOBench%2C%0Ademonstrating%20the%20broad%20generalizability%20of%20our%20approach.%20All%20resources%20of%20this%0Apaper%20have%20been%20released%20at%20https%3A//showlab.github.io/livecc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16030v1&entry.124074799=Read"},
{"title": "ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order\n  Neighboring Feature Fusion", "author": "Xiang Li and Haobing Liu and Jianpeng Qi and Yuan Cao and Guoqing Chao and Yanwei Yu", "abstract": "  Graph Neural Networks (GNNs) have demonstrated strong performance across\nvarious graph-based tasks by effectively capturing relational information\nbetween nodes. These models rely on iterative message passing to propagate node\nfeatures, enabling nodes to aggregate information from their neighbors. Recent\nresearch has significantly improved the message-passing mechanism, enhancing\nGNN scalability on large-scale graphs. However, GNNs still face two main\nchallenges: over-smoothing, where excessive message passing results in\nindistinguishable node representations, especially in deep networks\nincorporating high-order neighbors; and scalability issues, as traditional\narchitectures suffer from high model complexity and increased inference time\ndue to redundant information aggregation. This paper proposes a novel framework\nfor large-scale graphs named ScaleGNN that simultaneously addresses both\nchallenges by adaptively fusing multi-level graph features. We first construct\nneighbor matrices for each order, learning their relative information through\ntrainable weights through an adaptive high-order feature fusion module. This\nallows the model to selectively emphasize informative high-order neighbors\nwhile reducing unnecessary computational costs. Additionally, we introduce a\nHigh-order redundant feature masking mechanism based on a Local Contribution\nScore (LCS), which enables the model to retain only the most relevant neighbors\nat each order, preventing redundant information propagation. Furthermore,\nlow-order enhanced feature aggregation adaptively integrates low-order and\nhigh-order features based on task relevance, ensuring effective capture of both\nlocal and global structural information without excessive complexity. Extensive\nexperiments on real-world datasets demonstrate that our approach consistently\noutperforms state-of-the-art GNN models in both accuracy and computational\nefficiency.\n", "link": "http://arxiv.org/abs/2504.15920v1", "date": "2025-04-22", "relevancy": 2.1215, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5339}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.531}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScaleGNN%3A%20Towards%20Scalable%20Graph%20Neural%20Networks%20via%20Adaptive%20High-order%0A%20%20Neighboring%20Feature%20Fusion&body=Title%3A%20ScaleGNN%3A%20Towards%20Scalable%20Graph%20Neural%20Networks%20via%20Adaptive%20High-order%0A%20%20Neighboring%20Feature%20Fusion%0AAuthor%3A%20Xiang%20Li%20and%20Haobing%20Liu%20and%20Jianpeng%20Qi%20and%20Yuan%20Cao%20and%20Guoqing%20Chao%20and%20Yanwei%20Yu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20strong%20performance%20across%0Avarious%20graph-based%20tasks%20by%20effectively%20capturing%20relational%20information%0Abetween%20nodes.%20These%20models%20rely%20on%20iterative%20message%20passing%20to%20propagate%20node%0Afeatures%2C%20enabling%20nodes%20to%20aggregate%20information%20from%20their%20neighbors.%20Recent%0Aresearch%20has%20significantly%20improved%20the%20message-passing%20mechanism%2C%20enhancing%0AGNN%20scalability%20on%20large-scale%20graphs.%20However%2C%20GNNs%20still%20face%20two%20main%0Achallenges%3A%20over-smoothing%2C%20where%20excessive%20message%20passing%20results%20in%0Aindistinguishable%20node%20representations%2C%20especially%20in%20deep%20networks%0Aincorporating%20high-order%20neighbors%3B%20and%20scalability%20issues%2C%20as%20traditional%0Aarchitectures%20suffer%20from%20high%20model%20complexity%20and%20increased%20inference%20time%0Adue%20to%20redundant%20information%20aggregation.%20This%20paper%20proposes%20a%20novel%20framework%0Afor%20large-scale%20graphs%20named%20ScaleGNN%20that%20simultaneously%20addresses%20both%0Achallenges%20by%20adaptively%20fusing%20multi-level%20graph%20features.%20We%20first%20construct%0Aneighbor%20matrices%20for%20each%20order%2C%20learning%20their%20relative%20information%20through%0Atrainable%20weights%20through%20an%20adaptive%20high-order%20feature%20fusion%20module.%20This%0Aallows%20the%20model%20to%20selectively%20emphasize%20informative%20high-order%20neighbors%0Awhile%20reducing%20unnecessary%20computational%20costs.%20Additionally%2C%20we%20introduce%20a%0AHigh-order%20redundant%20feature%20masking%20mechanism%20based%20on%20a%20Local%20Contribution%0AScore%20%28LCS%29%2C%20which%20enables%20the%20model%20to%20retain%20only%20the%20most%20relevant%20neighbors%0Aat%20each%20order%2C%20preventing%20redundant%20information%20propagation.%20Furthermore%2C%0Alow-order%20enhanced%20feature%20aggregation%20adaptively%20integrates%20low-order%20and%0Ahigh-order%20features%20based%20on%20task%20relevance%2C%20ensuring%20effective%20capture%20of%20both%0Alocal%20and%20global%20structural%20information%20without%20excessive%20complexity.%20Extensive%0Aexperiments%20on%20real-world%20datasets%20demonstrate%20that%20our%20approach%20consistently%0Aoutperforms%20state-of-the-art%20GNN%20models%20in%20both%20accuracy%20and%20computational%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaleGNN%253A%2520Towards%2520Scalable%2520Graph%2520Neural%2520Networks%2520via%2520Adaptive%2520High-order%250A%2520%2520Neighboring%2520Feature%2520Fusion%26entry.906535625%3DXiang%2520Li%2520and%2520Haobing%2520Liu%2520and%2520Jianpeng%2520Qi%2520and%2520Yuan%2520Cao%2520and%2520Guoqing%2520Chao%2520and%2520Yanwei%2520Yu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520strong%2520performance%2520across%250Avarious%2520graph-based%2520tasks%2520by%2520effectively%2520capturing%2520relational%2520information%250Abetween%2520nodes.%2520These%2520models%2520rely%2520on%2520iterative%2520message%2520passing%2520to%2520propagate%2520node%250Afeatures%252C%2520enabling%2520nodes%2520to%2520aggregate%2520information%2520from%2520their%2520neighbors.%2520Recent%250Aresearch%2520has%2520significantly%2520improved%2520the%2520message-passing%2520mechanism%252C%2520enhancing%250AGNN%2520scalability%2520on%2520large-scale%2520graphs.%2520However%252C%2520GNNs%2520still%2520face%2520two%2520main%250Achallenges%253A%2520over-smoothing%252C%2520where%2520excessive%2520message%2520passing%2520results%2520in%250Aindistinguishable%2520node%2520representations%252C%2520especially%2520in%2520deep%2520networks%250Aincorporating%2520high-order%2520neighbors%253B%2520and%2520scalability%2520issues%252C%2520as%2520traditional%250Aarchitectures%2520suffer%2520from%2520high%2520model%2520complexity%2520and%2520increased%2520inference%2520time%250Adue%2520to%2520redundant%2520information%2520aggregation.%2520This%2520paper%2520proposes%2520a%2520novel%2520framework%250Afor%2520large-scale%2520graphs%2520named%2520ScaleGNN%2520that%2520simultaneously%2520addresses%2520both%250Achallenges%2520by%2520adaptively%2520fusing%2520multi-level%2520graph%2520features.%2520We%2520first%2520construct%250Aneighbor%2520matrices%2520for%2520each%2520order%252C%2520learning%2520their%2520relative%2520information%2520through%250Atrainable%2520weights%2520through%2520an%2520adaptive%2520high-order%2520feature%2520fusion%2520module.%2520This%250Aallows%2520the%2520model%2520to%2520selectively%2520emphasize%2520informative%2520high-order%2520neighbors%250Awhile%2520reducing%2520unnecessary%2520computational%2520costs.%2520Additionally%252C%2520we%2520introduce%2520a%250AHigh-order%2520redundant%2520feature%2520masking%2520mechanism%2520based%2520on%2520a%2520Local%2520Contribution%250AScore%2520%2528LCS%2529%252C%2520which%2520enables%2520the%2520model%2520to%2520retain%2520only%2520the%2520most%2520relevant%2520neighbors%250Aat%2520each%2520order%252C%2520preventing%2520redundant%2520information%2520propagation.%2520Furthermore%252C%250Alow-order%2520enhanced%2520feature%2520aggregation%2520adaptively%2520integrates%2520low-order%2520and%250Ahigh-order%2520features%2520based%2520on%2520task%2520relevance%252C%2520ensuring%2520effective%2520capture%2520of%2520both%250Alocal%2520and%2520global%2520structural%2520information%2520without%2520excessive%2520complexity.%2520Extensive%250Aexperiments%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520consistently%250Aoutperforms%2520state-of-the-art%2520GNN%2520models%2520in%2520both%2520accuracy%2520and%2520computational%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScaleGNN%3A%20Towards%20Scalable%20Graph%20Neural%20Networks%20via%20Adaptive%20High-order%0A%20%20Neighboring%20Feature%20Fusion&entry.906535625=Xiang%20Li%20and%20Haobing%20Liu%20and%20Jianpeng%20Qi%20and%20Yuan%20Cao%20and%20Guoqing%20Chao%20and%20Yanwei%20Yu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20strong%20performance%20across%0Avarious%20graph-based%20tasks%20by%20effectively%20capturing%20relational%20information%0Abetween%20nodes.%20These%20models%20rely%20on%20iterative%20message%20passing%20to%20propagate%20node%0Afeatures%2C%20enabling%20nodes%20to%20aggregate%20information%20from%20their%20neighbors.%20Recent%0Aresearch%20has%20significantly%20improved%20the%20message-passing%20mechanism%2C%20enhancing%0AGNN%20scalability%20on%20large-scale%20graphs.%20However%2C%20GNNs%20still%20face%20two%20main%0Achallenges%3A%20over-smoothing%2C%20where%20excessive%20message%20passing%20results%20in%0Aindistinguishable%20node%20representations%2C%20especially%20in%20deep%20networks%0Aincorporating%20high-order%20neighbors%3B%20and%20scalability%20issues%2C%20as%20traditional%0Aarchitectures%20suffer%20from%20high%20model%20complexity%20and%20increased%20inference%20time%0Adue%20to%20redundant%20information%20aggregation.%20This%20paper%20proposes%20a%20novel%20framework%0Afor%20large-scale%20graphs%20named%20ScaleGNN%20that%20simultaneously%20addresses%20both%0Achallenges%20by%20adaptively%20fusing%20multi-level%20graph%20features.%20We%20first%20construct%0Aneighbor%20matrices%20for%20each%20order%2C%20learning%20their%20relative%20information%20through%0Atrainable%20weights%20through%20an%20adaptive%20high-order%20feature%20fusion%20module.%20This%0Aallows%20the%20model%20to%20selectively%20emphasize%20informative%20high-order%20neighbors%0Awhile%20reducing%20unnecessary%20computational%20costs.%20Additionally%2C%20we%20introduce%20a%0AHigh-order%20redundant%20feature%20masking%20mechanism%20based%20on%20a%20Local%20Contribution%0AScore%20%28LCS%29%2C%20which%20enables%20the%20model%20to%20retain%20only%20the%20most%20relevant%20neighbors%0Aat%20each%20order%2C%20preventing%20redundant%20information%20propagation.%20Furthermore%2C%0Alow-order%20enhanced%20feature%20aggregation%20adaptively%20integrates%20low-order%20and%0Ahigh-order%20features%20based%20on%20task%20relevance%2C%20ensuring%20effective%20capture%20of%20both%0Alocal%20and%20global%20structural%20information%20without%20excessive%20complexity.%20Extensive%0Aexperiments%20on%20real-world%20datasets%20demonstrate%20that%20our%20approach%20consistently%0Aoutperforms%20state-of-the-art%20GNN%20models%20in%20both%20accuracy%20and%20computational%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15920v1&entry.124074799=Read"},
{"title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning\n  Corpus (ARC) Tasks with Model Temperature Considerations", "author": "Nikhil Khandalkar and Pavan Yadav and Krishna Shinde and Lokesh B. Ramegowda and Rajarshi Das", "abstract": "  Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility.\n", "link": "http://arxiv.org/abs/2504.15903v1", "date": "2025-04-22", "relevancy": 2.114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20Noise%20on%20LLM-Models%20Performance%20in%20Abstraction%20and%20Reasoning%0A%20%20Corpus%20%28ARC%29%20Tasks%20with%20Model%20Temperature%20Considerations&body=Title%3A%20Impact%20of%20Noise%20on%20LLM-Models%20Performance%20in%20Abstraction%20and%20Reasoning%0A%20%20Corpus%20%28ARC%29%20Tasks%20with%20Model%20Temperature%20Considerations%0AAuthor%3A%20Nikhil%20Khandalkar%20and%20Pavan%20Yadav%20and%20Krishna%20Shinde%20and%20Lokesh%20B.%20Ramegowda%20and%20Rajarshi%20Das%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20generated%20growing%0Ainterest%20in%20their%20structured%20reasoning%20capabilities%2C%20particularly%20in%20tasks%0Ainvolving%20abstraction%20and%20pattern%20recognition.%20The%20Abstraction%20and%20Reasoning%0ACorpus%20%28ARC%29%20benchmark%20plays%20a%20crucial%20role%20in%20evaluating%20these%20capabilities%20by%0Atesting%20how%20well%20AI%20models%20generalize%20to%20novel%20problems.%20While%20GPT-4o%0Ademonstrates%20strong%20performance%20by%20solving%20all%20ARC%20tasks%20under%20zero-noise%0Aconditions%2C%20other%20models%20like%20DeepSeek%20R1%20and%20LLaMA%203.2%20fail%20to%20solve%20any%2C%0Asuggesting%20limitations%20in%20their%20ability%20to%20reason%20beyond%20simple%20pattern%0Amatching.%20To%20explore%20this%20gap%2C%20we%20systematically%20evaluate%20these%20models%20across%0Adifferent%20noise%20levels%20and%20temperature%20settings.%20Our%20results%20reveal%20that%20the%0Aintroduction%20of%20noise%20consistently%20impairs%20model%20performance%2C%20regardless%20of%0Aarchitecture.%20This%20decline%20highlights%20a%20shared%20vulnerability%3A%20current%20LLMs%2C%0Adespite%20showing%20signs%20of%20abstract%20reasoning%2C%20remain%20highly%20sensitive%20to%20input%0Aperturbations.%20Such%20fragility%20raises%20concerns%20about%20their%20real-world%0Aapplicability%2C%20where%20noise%20and%20uncertainty%20are%20common.%20By%20comparing%20how%0Adifferent%20model%20architectures%20respond%20to%20these%20challenges%2C%20we%20offer%20insights%0Ainto%20the%20structural%20weaknesses%20of%20modern%20LLMs%20in%20reasoning%20tasks.%20This%20work%0Aunderscores%20the%20need%20for%20developing%20more%20robust%20and%20adaptable%20AI%20systems%0Acapable%20of%20handling%20the%20ambiguity%20and%20variability%20inherent%20in%20real-world%0Ascenarios.%20Our%20findings%20aim%20to%20guide%20future%20research%20toward%20enhancing%20model%0Ageneralization%2C%20robustness%2C%20and%20alignment%20with%20human-like%20cognitive%0Aflexibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%2520Noise%2520on%2520LLM-Models%2520Performance%2520in%2520Abstraction%2520and%2520Reasoning%250A%2520%2520Corpus%2520%2528ARC%2529%2520Tasks%2520with%2520Model%2520Temperature%2520Considerations%26entry.906535625%3DNikhil%2520Khandalkar%2520and%2520Pavan%2520Yadav%2520and%2520Krishna%2520Shinde%2520and%2520Lokesh%2520B.%2520Ramegowda%2520and%2520Rajarshi%2520Das%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520generated%2520growing%250Ainterest%2520in%2520their%2520structured%2520reasoning%2520capabilities%252C%2520particularly%2520in%2520tasks%250Ainvolving%2520abstraction%2520and%2520pattern%2520recognition.%2520The%2520Abstraction%2520and%2520Reasoning%250ACorpus%2520%2528ARC%2529%2520benchmark%2520plays%2520a%2520crucial%2520role%2520in%2520evaluating%2520these%2520capabilities%2520by%250Atesting%2520how%2520well%2520AI%2520models%2520generalize%2520to%2520novel%2520problems.%2520While%2520GPT-4o%250Ademonstrates%2520strong%2520performance%2520by%2520solving%2520all%2520ARC%2520tasks%2520under%2520zero-noise%250Aconditions%252C%2520other%2520models%2520like%2520DeepSeek%2520R1%2520and%2520LLaMA%25203.2%2520fail%2520to%2520solve%2520any%252C%250Asuggesting%2520limitations%2520in%2520their%2520ability%2520to%2520reason%2520beyond%2520simple%2520pattern%250Amatching.%2520To%2520explore%2520this%2520gap%252C%2520we%2520systematically%2520evaluate%2520these%2520models%2520across%250Adifferent%2520noise%2520levels%2520and%2520temperature%2520settings.%2520Our%2520results%2520reveal%2520that%2520the%250Aintroduction%2520of%2520noise%2520consistently%2520impairs%2520model%2520performance%252C%2520regardless%2520of%250Aarchitecture.%2520This%2520decline%2520highlights%2520a%2520shared%2520vulnerability%253A%2520current%2520LLMs%252C%250Adespite%2520showing%2520signs%2520of%2520abstract%2520reasoning%252C%2520remain%2520highly%2520sensitive%2520to%2520input%250Aperturbations.%2520Such%2520fragility%2520raises%2520concerns%2520about%2520their%2520real-world%250Aapplicability%252C%2520where%2520noise%2520and%2520uncertainty%2520are%2520common.%2520By%2520comparing%2520how%250Adifferent%2520model%2520architectures%2520respond%2520to%2520these%2520challenges%252C%2520we%2520offer%2520insights%250Ainto%2520the%2520structural%2520weaknesses%2520of%2520modern%2520LLMs%2520in%2520reasoning%2520tasks.%2520This%2520work%250Aunderscores%2520the%2520need%2520for%2520developing%2520more%2520robust%2520and%2520adaptable%2520AI%2520systems%250Acapable%2520of%2520handling%2520the%2520ambiguity%2520and%2520variability%2520inherent%2520in%2520real-world%250Ascenarios.%2520Our%2520findings%2520aim%2520to%2520guide%2520future%2520research%2520toward%2520enhancing%2520model%250Ageneralization%252C%2520robustness%252C%2520and%2520alignment%2520with%2520human-like%2520cognitive%250Aflexibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20Noise%20on%20LLM-Models%20Performance%20in%20Abstraction%20and%20Reasoning%0A%20%20Corpus%20%28ARC%29%20Tasks%20with%20Model%20Temperature%20Considerations&entry.906535625=Nikhil%20Khandalkar%20and%20Pavan%20Yadav%20and%20Krishna%20Shinde%20and%20Lokesh%20B.%20Ramegowda%20and%20Rajarshi%20Das&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20generated%20growing%0Ainterest%20in%20their%20structured%20reasoning%20capabilities%2C%20particularly%20in%20tasks%0Ainvolving%20abstraction%20and%20pattern%20recognition.%20The%20Abstraction%20and%20Reasoning%0ACorpus%20%28ARC%29%20benchmark%20plays%20a%20crucial%20role%20in%20evaluating%20these%20capabilities%20by%0Atesting%20how%20well%20AI%20models%20generalize%20to%20novel%20problems.%20While%20GPT-4o%0Ademonstrates%20strong%20performance%20by%20solving%20all%20ARC%20tasks%20under%20zero-noise%0Aconditions%2C%20other%20models%20like%20DeepSeek%20R1%20and%20LLaMA%203.2%20fail%20to%20solve%20any%2C%0Asuggesting%20limitations%20in%20their%20ability%20to%20reason%20beyond%20simple%20pattern%0Amatching.%20To%20explore%20this%20gap%2C%20we%20systematically%20evaluate%20these%20models%20across%0Adifferent%20noise%20levels%20and%20temperature%20settings.%20Our%20results%20reveal%20that%20the%0Aintroduction%20of%20noise%20consistently%20impairs%20model%20performance%2C%20regardless%20of%0Aarchitecture.%20This%20decline%20highlights%20a%20shared%20vulnerability%3A%20current%20LLMs%2C%0Adespite%20showing%20signs%20of%20abstract%20reasoning%2C%20remain%20highly%20sensitive%20to%20input%0Aperturbations.%20Such%20fragility%20raises%20concerns%20about%20their%20real-world%0Aapplicability%2C%20where%20noise%20and%20uncertainty%20are%20common.%20By%20comparing%20how%0Adifferent%20model%20architectures%20respond%20to%20these%20challenges%2C%20we%20offer%20insights%0Ainto%20the%20structural%20weaknesses%20of%20modern%20LLMs%20in%20reasoning%20tasks.%20This%20work%0Aunderscores%20the%20need%20for%20developing%20more%20robust%20and%20adaptable%20AI%20systems%0Acapable%20of%20handling%20the%20ambiguity%20and%20variability%20inherent%20in%20real-world%0Ascenarios.%20Our%20findings%20aim%20to%20guide%20future%20research%20toward%20enhancing%20model%0Ageneralization%2C%20robustness%2C%20and%20alignment%20with%20human-like%20cognitive%0Aflexibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15903v1&entry.124074799=Read"},
{"title": "Time-Varying Soft-Maximum Barrier Functions for Safety in Unmapped and\n  Dynamic Environments", "author": "Amirsaeid Safari and Jesse B. Hoagg", "abstract": "  We present a closed-form optimal feedback control method that ensures safety\nin an a prior unknown and potentially dynamic environment. This article\nconsiders the scenario where local perception data (e.g., LiDAR) is obtained\nperiodically, and this data can be used to construct a local control barrier\nfunction (CBF) that models a local set that is safe for a period of time into\nthe future. Then, we use a smooth time-varying soft-maximum function to compose\nthe N most recently obtained local CBFs into a single barrier function that\nmodels an approximate union of the N most recently obtained local sets. This\ncomposite barrier function is used in a constrained quadratic optimization,\nwhich is solved in closed form to obtain a safe-and-optimal feedback control.\nWe also apply the time-varying soft-maximum barrier function control to 2\nrobotic systems (nonholonomic ground robot with nonnegligible inertia, and\nquadrotor robot), where the objective is to navigate an a priori unknown\nenvironment safely and reach a target destination. In these applications, we\npresent a simple approach to generate local CBFs from periodically obtained\nperception data.\n", "link": "http://arxiv.org/abs/2409.01458v2", "date": "2025-04-22", "relevancy": 2.1139, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.576}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5287}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-Varying%20Soft-Maximum%20Barrier%20Functions%20for%20Safety%20in%20Unmapped%20and%0A%20%20Dynamic%20Environments&body=Title%3A%20Time-Varying%20Soft-Maximum%20Barrier%20Functions%20for%20Safety%20in%20Unmapped%20and%0A%20%20Dynamic%20Environments%0AAuthor%3A%20Amirsaeid%20Safari%20and%20Jesse%20B.%20Hoagg%0AAbstract%3A%20%20%20We%20present%20a%20closed-form%20optimal%20feedback%20control%20method%20that%20ensures%20safety%0Ain%20an%20a%20prior%20unknown%20and%20potentially%20dynamic%20environment.%20This%20article%0Aconsiders%20the%20scenario%20where%20local%20perception%20data%20%28e.g.%2C%20LiDAR%29%20is%20obtained%0Aperiodically%2C%20and%20this%20data%20can%20be%20used%20to%20construct%20a%20local%20control%20barrier%0Afunction%20%28CBF%29%20that%20models%20a%20local%20set%20that%20is%20safe%20for%20a%20period%20of%20time%20into%0Athe%20future.%20Then%2C%20we%20use%20a%20smooth%20time-varying%20soft-maximum%20function%20to%20compose%0Athe%20N%20most%20recently%20obtained%20local%20CBFs%20into%20a%20single%20barrier%20function%20that%0Amodels%20an%20approximate%20union%20of%20the%20N%20most%20recently%20obtained%20local%20sets.%20This%0Acomposite%20barrier%20function%20is%20used%20in%20a%20constrained%20quadratic%20optimization%2C%0Awhich%20is%20solved%20in%20closed%20form%20to%20obtain%20a%20safe-and-optimal%20feedback%20control.%0AWe%20also%20apply%20the%20time-varying%20soft-maximum%20barrier%20function%20control%20to%202%0Arobotic%20systems%20%28nonholonomic%20ground%20robot%20with%20nonnegligible%20inertia%2C%20and%0Aquadrotor%20robot%29%2C%20where%20the%20objective%20is%20to%20navigate%20an%20a%20priori%20unknown%0Aenvironment%20safely%20and%20reach%20a%20target%20destination.%20In%20these%20applications%2C%20we%0Apresent%20a%20simple%20approach%20to%20generate%20local%20CBFs%20from%20periodically%20obtained%0Aperception%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-Varying%2520Soft-Maximum%2520Barrier%2520Functions%2520for%2520Safety%2520in%2520Unmapped%2520and%250A%2520%2520Dynamic%2520Environments%26entry.906535625%3DAmirsaeid%2520Safari%2520and%2520Jesse%2520B.%2520Hoagg%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520closed-form%2520optimal%2520feedback%2520control%2520method%2520that%2520ensures%2520safety%250Ain%2520an%2520a%2520prior%2520unknown%2520and%2520potentially%2520dynamic%2520environment.%2520This%2520article%250Aconsiders%2520the%2520scenario%2520where%2520local%2520perception%2520data%2520%2528e.g.%252C%2520LiDAR%2529%2520is%2520obtained%250Aperiodically%252C%2520and%2520this%2520data%2520can%2520be%2520used%2520to%2520construct%2520a%2520local%2520control%2520barrier%250Afunction%2520%2528CBF%2529%2520that%2520models%2520a%2520local%2520set%2520that%2520is%2520safe%2520for%2520a%2520period%2520of%2520time%2520into%250Athe%2520future.%2520Then%252C%2520we%2520use%2520a%2520smooth%2520time-varying%2520soft-maximum%2520function%2520to%2520compose%250Athe%2520N%2520most%2520recently%2520obtained%2520local%2520CBFs%2520into%2520a%2520single%2520barrier%2520function%2520that%250Amodels%2520an%2520approximate%2520union%2520of%2520the%2520N%2520most%2520recently%2520obtained%2520local%2520sets.%2520This%250Acomposite%2520barrier%2520function%2520is%2520used%2520in%2520a%2520constrained%2520quadratic%2520optimization%252C%250Awhich%2520is%2520solved%2520in%2520closed%2520form%2520to%2520obtain%2520a%2520safe-and-optimal%2520feedback%2520control.%250AWe%2520also%2520apply%2520the%2520time-varying%2520soft-maximum%2520barrier%2520function%2520control%2520to%25202%250Arobotic%2520systems%2520%2528nonholonomic%2520ground%2520robot%2520with%2520nonnegligible%2520inertia%252C%2520and%250Aquadrotor%2520robot%2529%252C%2520where%2520the%2520objective%2520is%2520to%2520navigate%2520an%2520a%2520priori%2520unknown%250Aenvironment%2520safely%2520and%2520reach%2520a%2520target%2520destination.%2520In%2520these%2520applications%252C%2520we%250Apresent%2520a%2520simple%2520approach%2520to%2520generate%2520local%2520CBFs%2520from%2520periodically%2520obtained%250Aperception%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-Varying%20Soft-Maximum%20Barrier%20Functions%20for%20Safety%20in%20Unmapped%20and%0A%20%20Dynamic%20Environments&entry.906535625=Amirsaeid%20Safari%20and%20Jesse%20B.%20Hoagg&entry.1292438233=%20%20We%20present%20a%20closed-form%20optimal%20feedback%20control%20method%20that%20ensures%20safety%0Ain%20an%20a%20prior%20unknown%20and%20potentially%20dynamic%20environment.%20This%20article%0Aconsiders%20the%20scenario%20where%20local%20perception%20data%20%28e.g.%2C%20LiDAR%29%20is%20obtained%0Aperiodically%2C%20and%20this%20data%20can%20be%20used%20to%20construct%20a%20local%20control%20barrier%0Afunction%20%28CBF%29%20that%20models%20a%20local%20set%20that%20is%20safe%20for%20a%20period%20of%20time%20into%0Athe%20future.%20Then%2C%20we%20use%20a%20smooth%20time-varying%20soft-maximum%20function%20to%20compose%0Athe%20N%20most%20recently%20obtained%20local%20CBFs%20into%20a%20single%20barrier%20function%20that%0Amodels%20an%20approximate%20union%20of%20the%20N%20most%20recently%20obtained%20local%20sets.%20This%0Acomposite%20barrier%20function%20is%20used%20in%20a%20constrained%20quadratic%20optimization%2C%0Awhich%20is%20solved%20in%20closed%20form%20to%20obtain%20a%20safe-and-optimal%20feedback%20control.%0AWe%20also%20apply%20the%20time-varying%20soft-maximum%20barrier%20function%20control%20to%202%0Arobotic%20systems%20%28nonholonomic%20ground%20robot%20with%20nonnegligible%20inertia%2C%20and%0Aquadrotor%20robot%29%2C%20where%20the%20objective%20is%20to%20navigate%20an%20a%20priori%20unknown%0Aenvironment%20safely%20and%20reach%20a%20target%20destination.%20In%20these%20applications%2C%20we%0Apresent%20a%20simple%20approach%20to%20generate%20local%20CBFs%20from%20periodically%20obtained%0Aperception%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01458v2&entry.124074799=Read"},
{"title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with\n  Dual Optimizers", "author": "Xuyang Zhong and Haochen Luo and Chen Liu", "abstract": "  Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms.\n", "link": "http://arxiv.org/abs/2504.15827v1", "date": "2025-04-22", "relevancy": 2.1099, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5824}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5256}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualOptim%3A%20Enhancing%20Efficacy%20and%20Stability%20in%20Machine%20Unlearning%20with%0A%20%20Dual%20Optimizers&body=Title%3A%20DualOptim%3A%20Enhancing%20Efficacy%20and%20Stability%20in%20Machine%20Unlearning%20with%0A%20%20Dual%20Optimizers%0AAuthor%3A%20Xuyang%20Zhong%20and%20Haochen%20Luo%20and%20Chen%20Liu%0AAbstract%3A%20%20%20Existing%20machine%20unlearning%20%28MU%29%20approaches%20exhibit%20significant%20sensitivity%0Ato%20hyperparameters%2C%20requiring%20meticulous%20tuning%20that%20limits%20practical%0Adeployment.%20In%20this%20work%2C%20we%20first%20empirically%20demonstrate%20the%20instability%20and%0Asuboptimal%20performance%20of%20existing%20popular%20MU%20methods%20when%20deployed%20in%0Adifferent%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20Dual%20Optimizer%0A%28DualOptim%29%2C%20which%20incorporates%20adaptive%20learning%20rate%20and%20decoupled%20momentum%0Afactors.%20Empirical%20and%20theoretical%20evidence%20demonstrates%20that%20DualOptim%0Acontributes%20to%20effective%20and%20stable%20unlearning.%20Through%20extensive%20experiments%2C%0Awe%20show%20that%20DualOptim%20can%20significantly%20boost%20MU%20efficacy%20and%20stability%20across%0Adiverse%20tasks%2C%20including%20image%20classification%2C%20image%20generation%2C%20and%20large%0Alanguage%20models%2C%20making%20it%20a%20versatile%20approach%20to%20empower%20existing%20MU%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualOptim%253A%2520Enhancing%2520Efficacy%2520and%2520Stability%2520in%2520Machine%2520Unlearning%2520with%250A%2520%2520Dual%2520Optimizers%26entry.906535625%3DXuyang%2520Zhong%2520and%2520Haochen%2520Luo%2520and%2520Chen%2520Liu%26entry.1292438233%3D%2520%2520Existing%2520machine%2520unlearning%2520%2528MU%2529%2520approaches%2520exhibit%2520significant%2520sensitivity%250Ato%2520hyperparameters%252C%2520requiring%2520meticulous%2520tuning%2520that%2520limits%2520practical%250Adeployment.%2520In%2520this%2520work%252C%2520we%2520first%2520empirically%2520demonstrate%2520the%2520instability%2520and%250Asuboptimal%2520performance%2520of%2520existing%2520popular%2520MU%2520methods%2520when%2520deployed%2520in%250Adifferent%2520scenarios.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Dual%2520Optimizer%250A%2528DualOptim%2529%252C%2520which%2520incorporates%2520adaptive%2520learning%2520rate%2520and%2520decoupled%2520momentum%250Afactors.%2520Empirical%2520and%2520theoretical%2520evidence%2520demonstrates%2520that%2520DualOptim%250Acontributes%2520to%2520effective%2520and%2520stable%2520unlearning.%2520Through%2520extensive%2520experiments%252C%250Awe%2520show%2520that%2520DualOptim%2520can%2520significantly%2520boost%2520MU%2520efficacy%2520and%2520stability%2520across%250Adiverse%2520tasks%252C%2520including%2520image%2520classification%252C%2520image%2520generation%252C%2520and%2520large%250Alanguage%2520models%252C%2520making%2520it%2520a%2520versatile%2520approach%2520to%2520empower%2520existing%2520MU%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualOptim%3A%20Enhancing%20Efficacy%20and%20Stability%20in%20Machine%20Unlearning%20with%0A%20%20Dual%20Optimizers&entry.906535625=Xuyang%20Zhong%20and%20Haochen%20Luo%20and%20Chen%20Liu&entry.1292438233=%20%20Existing%20machine%20unlearning%20%28MU%29%20approaches%20exhibit%20significant%20sensitivity%0Ato%20hyperparameters%2C%20requiring%20meticulous%20tuning%20that%20limits%20practical%0Adeployment.%20In%20this%20work%2C%20we%20first%20empirically%20demonstrate%20the%20instability%20and%0Asuboptimal%20performance%20of%20existing%20popular%20MU%20methods%20when%20deployed%20in%0Adifferent%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20Dual%20Optimizer%0A%28DualOptim%29%2C%20which%20incorporates%20adaptive%20learning%20rate%20and%20decoupled%20momentum%0Afactors.%20Empirical%20and%20theoretical%20evidence%20demonstrates%20that%20DualOptim%0Acontributes%20to%20effective%20and%20stable%20unlearning.%20Through%20extensive%20experiments%2C%0Awe%20show%20that%20DualOptim%20can%20significantly%20boost%20MU%20efficacy%20and%20stability%20across%0Adiverse%20tasks%2C%20including%20image%20classification%2C%20image%20generation%2C%20and%20large%0Alanguage%20models%2C%20making%20it%20a%20versatile%20approach%20to%20empower%20existing%20MU%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15827v1&entry.124074799=Read"},
{"title": "Histogram-based Parameter-efficient Tuning for Passive Sonar\n  Classification", "author": "Amirmohammad Mohammadi and Davelle Carreiro and Alexandra Van Dine and Joshua Peeples", "abstract": "  Parameter-efficient transfer learning (PETL) methods adapt large artificial\nneural networks to downstream tasks without fine-tuning the entire model.\nHowever, existing additive methods, such as adapters, sometimes struggle to\ncapture distributional shifts in intermediate feature embeddings. We propose a\nnovel histogram-based parameter-efficient tuning (HPT) technique that captures\nthe statistics of the target domain and modulates the embeddings. Experimental\nresults on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)\ndemonstrate that HPT outperforms conventional adapters. Notably, HPT achieves\n91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields\nfeature representations closer to those of fully fine-tuned models. Overall,\nHPT balances parameter savings and performance, providing a distribution-aware\nalternative to existing adapters and shows a promising direction for scalable\ntransfer learning in resource-constrained environments. The code is publicly\navailable:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.\n", "link": "http://arxiv.org/abs/2504.15214v2", "date": "2025-04-22", "relevancy": 2.0912, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5201}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Histogram-based%20Parameter-efficient%20Tuning%20for%20Passive%20Sonar%0A%20%20Classification&body=Title%3A%20Histogram-based%20Parameter-efficient%20Tuning%20for%20Passive%20Sonar%0A%20%20Classification%0AAuthor%3A%20Amirmohammad%20Mohammadi%20and%20Davelle%20Carreiro%20and%20Alexandra%20Van%20Dine%20and%20Joshua%20Peeples%0AAbstract%3A%20%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20methods%20adapt%20large%20artificial%0Aneural%20networks%20to%20downstream%20tasks%20without%20fine-tuning%20the%20entire%20model.%0AHowever%2C%20existing%20additive%20methods%2C%20such%20as%20adapters%2C%20sometimes%20struggle%20to%0Acapture%20distributional%20shifts%20in%20intermediate%20feature%20embeddings.%20We%20propose%20a%0Anovel%20histogram-based%20parameter-efficient%20tuning%20%28HPT%29%20technique%20that%20captures%0Athe%20statistics%20of%20the%20target%20domain%20and%20modulates%20the%20embeddings.%20Experimental%0Aresults%20on%20three%20downstream%20passive%20sonar%20datasets%20%28ShipsEar%2C%20DeepShip%2C%20VTUAD%29%0Ademonstrate%20that%20HPT%20outperforms%20conventional%20adapters.%20Notably%2C%20HPT%20achieves%0A91.8%25%20vs.%2089.8%25%20accuracy%20on%20VTUAD.%20Furthermore%2C%20HPT%20trains%20faster%20and%20yields%0Afeature%20representations%20closer%20to%20those%20of%20fully%20fine-tuned%20models.%20Overall%2C%0AHPT%20balances%20parameter%20savings%20and%20performance%2C%20providing%20a%20distribution-aware%0Aalternative%20to%20existing%20adapters%20and%20shows%20a%20promising%20direction%20for%20scalable%0Atransfer%20learning%20in%20resource-constrained%20environments.%20The%20code%20is%20publicly%0Aavailable%3A%0Ahttps%3A//github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistogram-based%2520Parameter-efficient%2520Tuning%2520for%2520Passive%2520Sonar%250A%2520%2520Classification%26entry.906535625%3DAmirmohammad%2520Mohammadi%2520and%2520Davelle%2520Carreiro%2520and%2520Alexandra%2520Van%2520Dine%2520and%2520Joshua%2520Peeples%26entry.1292438233%3D%2520%2520Parameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520methods%2520adapt%2520large%2520artificial%250Aneural%2520networks%2520to%2520downstream%2520tasks%2520without%2520fine-tuning%2520the%2520entire%2520model.%250AHowever%252C%2520existing%2520additive%2520methods%252C%2520such%2520as%2520adapters%252C%2520sometimes%2520struggle%2520to%250Acapture%2520distributional%2520shifts%2520in%2520intermediate%2520feature%2520embeddings.%2520We%2520propose%2520a%250Anovel%2520histogram-based%2520parameter-efficient%2520tuning%2520%2528HPT%2529%2520technique%2520that%2520captures%250Athe%2520statistics%2520of%2520the%2520target%2520domain%2520and%2520modulates%2520the%2520embeddings.%2520Experimental%250Aresults%2520on%2520three%2520downstream%2520passive%2520sonar%2520datasets%2520%2528ShipsEar%252C%2520DeepShip%252C%2520VTUAD%2529%250Ademonstrate%2520that%2520HPT%2520outperforms%2520conventional%2520adapters.%2520Notably%252C%2520HPT%2520achieves%250A91.8%2525%2520vs.%252089.8%2525%2520accuracy%2520on%2520VTUAD.%2520Furthermore%252C%2520HPT%2520trains%2520faster%2520and%2520yields%250Afeature%2520representations%2520closer%2520to%2520those%2520of%2520fully%2520fine-tuned%2520models.%2520Overall%252C%250AHPT%2520balances%2520parameter%2520savings%2520and%2520performance%252C%2520providing%2520a%2520distribution-aware%250Aalternative%2520to%2520existing%2520adapters%2520and%2520shows%2520a%2520promising%2520direction%2520for%2520scalable%250Atransfer%2520learning%2520in%2520resource-constrained%2520environments.%2520The%2520code%2520is%2520publicly%250Aavailable%253A%250Ahttps%253A//github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Histogram-based%20Parameter-efficient%20Tuning%20for%20Passive%20Sonar%0A%20%20Classification&entry.906535625=Amirmohammad%20Mohammadi%20and%20Davelle%20Carreiro%20and%20Alexandra%20Van%20Dine%20and%20Joshua%20Peeples&entry.1292438233=%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20methods%20adapt%20large%20artificial%0Aneural%20networks%20to%20downstream%20tasks%20without%20fine-tuning%20the%20entire%20model.%0AHowever%2C%20existing%20additive%20methods%2C%20such%20as%20adapters%2C%20sometimes%20struggle%20to%0Acapture%20distributional%20shifts%20in%20intermediate%20feature%20embeddings.%20We%20propose%20a%0Anovel%20histogram-based%20parameter-efficient%20tuning%20%28HPT%29%20technique%20that%20captures%0Athe%20statistics%20of%20the%20target%20domain%20and%20modulates%20the%20embeddings.%20Experimental%0Aresults%20on%20three%20downstream%20passive%20sonar%20datasets%20%28ShipsEar%2C%20DeepShip%2C%20VTUAD%29%0Ademonstrate%20that%20HPT%20outperforms%20conventional%20adapters.%20Notably%2C%20HPT%20achieves%0A91.8%25%20vs.%2089.8%25%20accuracy%20on%20VTUAD.%20Furthermore%2C%20HPT%20trains%20faster%20and%20yields%0Afeature%20representations%20closer%20to%20those%20of%20fully%20fine-tuned%20models.%20Overall%2C%0AHPT%20balances%20parameter%20savings%20and%20performance%2C%20providing%20a%20distribution-aware%0Aalternative%20to%20existing%20adapters%20and%20shows%20a%20promising%20direction%20for%20scalable%0Atransfer%20learning%20in%20resource-constrained%20environments.%20The%20code%20is%20publicly%0Aavailable%3A%0Ahttps%3A//github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15214v2&entry.124074799=Read"},
{"title": "HoLa: B-Rep Generation using a Holistic Latent Representation", "author": "Yilin Liu and Duoteng Xu and Xingyao Yu and Xiang Xu and Daniel Cohen-Or and Hao Zhang and Hui Huang", "abstract": "  We introduce a novel representation for learning and generating\nComputer-Aided Design (CAD) models in the form of $\\textit{boundary\nrepresentations}$ (B-Reps). Our representation unifies the continuous geometric\nproperties of B-Rep primitives in different orders (e.g., surfaces and curves)\nand their discrete topological relations in a $\\textit{holistic latent}$ (HoLa)\nspace. This is based on the simple observation that the topological connection\nbetween two surfaces is intrinsically tied to the geometry of their\nintersecting curve. Such a prior allows us to reformulate topology learning in\nB-Reps as a geometric reconstruction problem in Euclidean space. Specifically,\nwe eliminate the presence of curves, vertices, and all the topological\nconnections in the latent space by learning to distinguish and derive curve\ngeometries from a pair of surface primitives via a neural intersection network.\nTo this end, our holistic latent space is only defined on surfaces but encodes\na full B-Rep model, including the geometry of surfaces, curves, vertices, and\ntheir topological relations. Our compact and holistic latent space facilitates\nthe design of a first diffusion-based generator to take on a large variety of\ninputs including point clouds, single/multi-view images, 2D sketches, and text\nprompts. Our method significantly reduces ambiguities, redundancies, and\nincoherences among the generated B-Rep primitives, as well as training\ncomplexities inherent in prior multi-step B-Rep learning pipelines, while\nachieving greatly improved validity rate over current state of the art: 82% vs.\n$\\approx$50%.\n", "link": "http://arxiv.org/abs/2504.14257v2", "date": "2025-04-22", "relevancy": 2.0832, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5536}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5248}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoLa%3A%20B-Rep%20Generation%20using%20a%20Holistic%20Latent%20Representation&body=Title%3A%20HoLa%3A%20B-Rep%20Generation%20using%20a%20Holistic%20Latent%20Representation%0AAuthor%3A%20Yilin%20Liu%20and%20Duoteng%20Xu%20and%20Xingyao%20Yu%20and%20Xiang%20Xu%20and%20Daniel%20Cohen-Or%20and%20Hao%20Zhang%20and%20Hui%20Huang%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20representation%20for%20learning%20and%20generating%0AComputer-Aided%20Design%20%28CAD%29%20models%20in%20the%20form%20of%20%24%5Ctextit%7Bboundary%0Arepresentations%7D%24%20%28B-Reps%29.%20Our%20representation%20unifies%20the%20continuous%20geometric%0Aproperties%20of%20B-Rep%20primitives%20in%20different%20orders%20%28e.g.%2C%20surfaces%20and%20curves%29%0Aand%20their%20discrete%20topological%20relations%20in%20a%20%24%5Ctextit%7Bholistic%20latent%7D%24%20%28HoLa%29%0Aspace.%20This%20is%20based%20on%20the%20simple%20observation%20that%20the%20topological%20connection%0Abetween%20two%20surfaces%20is%20intrinsically%20tied%20to%20the%20geometry%20of%20their%0Aintersecting%20curve.%20Such%20a%20prior%20allows%20us%20to%20reformulate%20topology%20learning%20in%0AB-Reps%20as%20a%20geometric%20reconstruction%20problem%20in%20Euclidean%20space.%20Specifically%2C%0Awe%20eliminate%20the%20presence%20of%20curves%2C%20vertices%2C%20and%20all%20the%20topological%0Aconnections%20in%20the%20latent%20space%20by%20learning%20to%20distinguish%20and%20derive%20curve%0Ageometries%20from%20a%20pair%20of%20surface%20primitives%20via%20a%20neural%20intersection%20network.%0ATo%20this%20end%2C%20our%20holistic%20latent%20space%20is%20only%20defined%20on%20surfaces%20but%20encodes%0Aa%20full%20B-Rep%20model%2C%20including%20the%20geometry%20of%20surfaces%2C%20curves%2C%20vertices%2C%20and%0Atheir%20topological%20relations.%20Our%20compact%20and%20holistic%20latent%20space%20facilitates%0Athe%20design%20of%20a%20first%20diffusion-based%20generator%20to%20take%20on%20a%20large%20variety%20of%0Ainputs%20including%20point%20clouds%2C%20single/multi-view%20images%2C%202D%20sketches%2C%20and%20text%0Aprompts.%20Our%20method%20significantly%20reduces%20ambiguities%2C%20redundancies%2C%20and%0Aincoherences%20among%20the%20generated%20B-Rep%20primitives%2C%20as%20well%20as%20training%0Acomplexities%20inherent%20in%20prior%20multi-step%20B-Rep%20learning%20pipelines%2C%20while%0Aachieving%20greatly%20improved%20validity%20rate%20over%20current%20state%20of%20the%20art%3A%2082%25%20vs.%0A%24%5Capprox%2450%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14257v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoLa%253A%2520B-Rep%2520Generation%2520using%2520a%2520Holistic%2520Latent%2520Representation%26entry.906535625%3DYilin%2520Liu%2520and%2520Duoteng%2520Xu%2520and%2520Xingyao%2520Yu%2520and%2520Xiang%2520Xu%2520and%2520Daniel%2520Cohen-Or%2520and%2520Hao%2520Zhang%2520and%2520Hui%2520Huang%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520representation%2520for%2520learning%2520and%2520generating%250AComputer-Aided%2520Design%2520%2528CAD%2529%2520models%2520in%2520the%2520form%2520of%2520%2524%255Ctextit%257Bboundary%250Arepresentations%257D%2524%2520%2528B-Reps%2529.%2520Our%2520representation%2520unifies%2520the%2520continuous%2520geometric%250Aproperties%2520of%2520B-Rep%2520primitives%2520in%2520different%2520orders%2520%2528e.g.%252C%2520surfaces%2520and%2520curves%2529%250Aand%2520their%2520discrete%2520topological%2520relations%2520in%2520a%2520%2524%255Ctextit%257Bholistic%2520latent%257D%2524%2520%2528HoLa%2529%250Aspace.%2520This%2520is%2520based%2520on%2520the%2520simple%2520observation%2520that%2520the%2520topological%2520connection%250Abetween%2520two%2520surfaces%2520is%2520intrinsically%2520tied%2520to%2520the%2520geometry%2520of%2520their%250Aintersecting%2520curve.%2520Such%2520a%2520prior%2520allows%2520us%2520to%2520reformulate%2520topology%2520learning%2520in%250AB-Reps%2520as%2520a%2520geometric%2520reconstruction%2520problem%2520in%2520Euclidean%2520space.%2520Specifically%252C%250Awe%2520eliminate%2520the%2520presence%2520of%2520curves%252C%2520vertices%252C%2520and%2520all%2520the%2520topological%250Aconnections%2520in%2520the%2520latent%2520space%2520by%2520learning%2520to%2520distinguish%2520and%2520derive%2520curve%250Ageometries%2520from%2520a%2520pair%2520of%2520surface%2520primitives%2520via%2520a%2520neural%2520intersection%2520network.%250ATo%2520this%2520end%252C%2520our%2520holistic%2520latent%2520space%2520is%2520only%2520defined%2520on%2520surfaces%2520but%2520encodes%250Aa%2520full%2520B-Rep%2520model%252C%2520including%2520the%2520geometry%2520of%2520surfaces%252C%2520curves%252C%2520vertices%252C%2520and%250Atheir%2520topological%2520relations.%2520Our%2520compact%2520and%2520holistic%2520latent%2520space%2520facilitates%250Athe%2520design%2520of%2520a%2520first%2520diffusion-based%2520generator%2520to%2520take%2520on%2520a%2520large%2520variety%2520of%250Ainputs%2520including%2520point%2520clouds%252C%2520single/multi-view%2520images%252C%25202D%2520sketches%252C%2520and%2520text%250Aprompts.%2520Our%2520method%2520significantly%2520reduces%2520ambiguities%252C%2520redundancies%252C%2520and%250Aincoherences%2520among%2520the%2520generated%2520B-Rep%2520primitives%252C%2520as%2520well%2520as%2520training%250Acomplexities%2520inherent%2520in%2520prior%2520multi-step%2520B-Rep%2520learning%2520pipelines%252C%2520while%250Aachieving%2520greatly%2520improved%2520validity%2520rate%2520over%2520current%2520state%2520of%2520the%2520art%253A%252082%2525%2520vs.%250A%2524%255Capprox%252450%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14257v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoLa%3A%20B-Rep%20Generation%20using%20a%20Holistic%20Latent%20Representation&entry.906535625=Yilin%20Liu%20and%20Duoteng%20Xu%20and%20Xingyao%20Yu%20and%20Xiang%20Xu%20and%20Daniel%20Cohen-Or%20and%20Hao%20Zhang%20and%20Hui%20Huang&entry.1292438233=%20%20We%20introduce%20a%20novel%20representation%20for%20learning%20and%20generating%0AComputer-Aided%20Design%20%28CAD%29%20models%20in%20the%20form%20of%20%24%5Ctextit%7Bboundary%0Arepresentations%7D%24%20%28B-Reps%29.%20Our%20representation%20unifies%20the%20continuous%20geometric%0Aproperties%20of%20B-Rep%20primitives%20in%20different%20orders%20%28e.g.%2C%20surfaces%20and%20curves%29%0Aand%20their%20discrete%20topological%20relations%20in%20a%20%24%5Ctextit%7Bholistic%20latent%7D%24%20%28HoLa%29%0Aspace.%20This%20is%20based%20on%20the%20simple%20observation%20that%20the%20topological%20connection%0Abetween%20two%20surfaces%20is%20intrinsically%20tied%20to%20the%20geometry%20of%20their%0Aintersecting%20curve.%20Such%20a%20prior%20allows%20us%20to%20reformulate%20topology%20learning%20in%0AB-Reps%20as%20a%20geometric%20reconstruction%20problem%20in%20Euclidean%20space.%20Specifically%2C%0Awe%20eliminate%20the%20presence%20of%20curves%2C%20vertices%2C%20and%20all%20the%20topological%0Aconnections%20in%20the%20latent%20space%20by%20learning%20to%20distinguish%20and%20derive%20curve%0Ageometries%20from%20a%20pair%20of%20surface%20primitives%20via%20a%20neural%20intersection%20network.%0ATo%20this%20end%2C%20our%20holistic%20latent%20space%20is%20only%20defined%20on%20surfaces%20but%20encodes%0Aa%20full%20B-Rep%20model%2C%20including%20the%20geometry%20of%20surfaces%2C%20curves%2C%20vertices%2C%20and%0Atheir%20topological%20relations.%20Our%20compact%20and%20holistic%20latent%20space%20facilitates%0Athe%20design%20of%20a%20first%20diffusion-based%20generator%20to%20take%20on%20a%20large%20variety%20of%0Ainputs%20including%20point%20clouds%2C%20single/multi-view%20images%2C%202D%20sketches%2C%20and%20text%0Aprompts.%20Our%20method%20significantly%20reduces%20ambiguities%2C%20redundancies%2C%20and%0Aincoherences%20among%20the%20generated%20B-Rep%20primitives%2C%20as%20well%20as%20training%0Acomplexities%20inherent%20in%20prior%20multi-step%20B-Rep%20learning%20pipelines%2C%20while%0Aachieving%20greatly%20improved%20validity%20rate%20over%20current%20state%20of%20the%20art%3A%2082%25%20vs.%0A%24%5Capprox%2450%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14257v2&entry.124074799=Read"},
{"title": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal\n  Localization", "author": "Hye-Min Won and Jieun Lee and Jiyong Oh", "abstract": "  Reliable localization is critical for robot navigation in complex indoor\nenvironments. In this paper, we propose an uncertainty-aware localization\nmethod that enhances the reliability of localization outputs without modifying\nthe prediction model itself. This study introduces a percentile-based rejection\nstrategy that filters out unreliable 3-DoF pose predictions based on aleatoric\nand epistemic uncertainties the network estimates. We apply this approach to a\nmulti-modal end-to-end localization that fuses RGB images and 2D LiDAR data,\nand we evaluate it across three real-world datasets collected using a\ncommercialized serving robot. Experimental results show that applying stricter\nuncertainty thresholds consistently improves pose accuracy. Specifically, the\nmean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean\norientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70%\nthresholds, respectively. Furthermore, the rejection strategy effectively\nremoves extreme outliers, resulting in better alignment with ground truth\ntrajectories. To the best of our knowledge, this is the first study to\nquantitatively demonstrate the benefits of percentile-based uncertainty\nrejection in multi-modal end-to-end localization tasks. Our approach provides a\npractical means to enhance the reliability and accuracy of localization systems\nin real-world deployments.\n", "link": "http://arxiv.org/abs/2504.07677v2", "date": "2025-04-22", "relevancy": 2.0731, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.7262}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6634}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localization%20Meets%20Uncertainty%3A%20Uncertainty-Aware%20Multi-Modal%0A%20%20Localization&body=Title%3A%20Localization%20Meets%20Uncertainty%3A%20Uncertainty-Aware%20Multi-Modal%0A%20%20Localization%0AAuthor%3A%20Hye-Min%20Won%20and%20Jieun%20Lee%20and%20Jiyong%20Oh%0AAbstract%3A%20%20%20Reliable%20localization%20is%20critical%20for%20robot%20navigation%20in%20complex%20indoor%0Aenvironments.%20In%20this%20paper%2C%20we%20propose%20an%20uncertainty-aware%20localization%0Amethod%20that%20enhances%20the%20reliability%20of%20localization%20outputs%20without%20modifying%0Athe%20prediction%20model%20itself.%20This%20study%20introduces%20a%20percentile-based%20rejection%0Astrategy%20that%20filters%20out%20unreliable%203-DoF%20pose%20predictions%20based%20on%20aleatoric%0Aand%20epistemic%20uncertainties%20the%20network%20estimates.%20We%20apply%20this%20approach%20to%20a%0Amulti-modal%20end-to-end%20localization%20that%20fuses%20RGB%20images%20and%202D%20LiDAR%20data%2C%0Aand%20we%20evaluate%20it%20across%20three%20real-world%20datasets%20collected%20using%20a%0Acommercialized%20serving%20robot.%20Experimental%20results%20show%20that%20applying%20stricter%0Auncertainty%20thresholds%20consistently%20improves%20pose%20accuracy.%20Specifically%2C%20the%0Amean%20position%20error%20is%20reduced%20by%2041.0%25%2C%2056.7%25%2C%20and%2069.4%25%2C%20and%20the%20mean%0Aorientation%20error%20by%2055.6%25%2C%2065.7%25%2C%20and%2073.3%25%2C%20when%20applying%2090%25%2C%2080%25%2C%20and%2070%25%0Athresholds%2C%20respectively.%20Furthermore%2C%20the%20rejection%20strategy%20effectively%0Aremoves%20extreme%20outliers%2C%20resulting%20in%20better%20alignment%20with%20ground%20truth%0Atrajectories.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%20to%0Aquantitatively%20demonstrate%20the%20benefits%20of%20percentile-based%20uncertainty%0Arejection%20in%20multi-modal%20end-to-end%20localization%20tasks.%20Our%20approach%20provides%20a%0Apractical%20means%20to%20enhance%20the%20reliability%20and%20accuracy%20of%20localization%20systems%0Ain%20real-world%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalization%2520Meets%2520Uncertainty%253A%2520Uncertainty-Aware%2520Multi-Modal%250A%2520%2520Localization%26entry.906535625%3DHye-Min%2520Won%2520and%2520Jieun%2520Lee%2520and%2520Jiyong%2520Oh%26entry.1292438233%3D%2520%2520Reliable%2520localization%2520is%2520critical%2520for%2520robot%2520navigation%2520in%2520complex%2520indoor%250Aenvironments.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520uncertainty-aware%2520localization%250Amethod%2520that%2520enhances%2520the%2520reliability%2520of%2520localization%2520outputs%2520without%2520modifying%250Athe%2520prediction%2520model%2520itself.%2520This%2520study%2520introduces%2520a%2520percentile-based%2520rejection%250Astrategy%2520that%2520filters%2520out%2520unreliable%25203-DoF%2520pose%2520predictions%2520based%2520on%2520aleatoric%250Aand%2520epistemic%2520uncertainties%2520the%2520network%2520estimates.%2520We%2520apply%2520this%2520approach%2520to%2520a%250Amulti-modal%2520end-to-end%2520localization%2520that%2520fuses%2520RGB%2520images%2520and%25202D%2520LiDAR%2520data%252C%250Aand%2520we%2520evaluate%2520it%2520across%2520three%2520real-world%2520datasets%2520collected%2520using%2520a%250Acommercialized%2520serving%2520robot.%2520Experimental%2520results%2520show%2520that%2520applying%2520stricter%250Auncertainty%2520thresholds%2520consistently%2520improves%2520pose%2520accuracy.%2520Specifically%252C%2520the%250Amean%2520position%2520error%2520is%2520reduced%2520by%252041.0%2525%252C%252056.7%2525%252C%2520and%252069.4%2525%252C%2520and%2520the%2520mean%250Aorientation%2520error%2520by%252055.6%2525%252C%252065.7%2525%252C%2520and%252073.3%2525%252C%2520when%2520applying%252090%2525%252C%252080%2525%252C%2520and%252070%2525%250Athresholds%252C%2520respectively.%2520Furthermore%252C%2520the%2520rejection%2520strategy%2520effectively%250Aremoves%2520extreme%2520outliers%252C%2520resulting%2520in%2520better%2520alignment%2520with%2520ground%2520truth%250Atrajectories.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%250Aquantitatively%2520demonstrate%2520the%2520benefits%2520of%2520percentile-based%2520uncertainty%250Arejection%2520in%2520multi-modal%2520end-to-end%2520localization%2520tasks.%2520Our%2520approach%2520provides%2520a%250Apractical%2520means%2520to%2520enhance%2520the%2520reliability%2520and%2520accuracy%2520of%2520localization%2520systems%250Ain%2520real-world%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localization%20Meets%20Uncertainty%3A%20Uncertainty-Aware%20Multi-Modal%0A%20%20Localization&entry.906535625=Hye-Min%20Won%20and%20Jieun%20Lee%20and%20Jiyong%20Oh&entry.1292438233=%20%20Reliable%20localization%20is%20critical%20for%20robot%20navigation%20in%20complex%20indoor%0Aenvironments.%20In%20this%20paper%2C%20we%20propose%20an%20uncertainty-aware%20localization%0Amethod%20that%20enhances%20the%20reliability%20of%20localization%20outputs%20without%20modifying%0Athe%20prediction%20model%20itself.%20This%20study%20introduces%20a%20percentile-based%20rejection%0Astrategy%20that%20filters%20out%20unreliable%203-DoF%20pose%20predictions%20based%20on%20aleatoric%0Aand%20epistemic%20uncertainties%20the%20network%20estimates.%20We%20apply%20this%20approach%20to%20a%0Amulti-modal%20end-to-end%20localization%20that%20fuses%20RGB%20images%20and%202D%20LiDAR%20data%2C%0Aand%20we%20evaluate%20it%20across%20three%20real-world%20datasets%20collected%20using%20a%0Acommercialized%20serving%20robot.%20Experimental%20results%20show%20that%20applying%20stricter%0Auncertainty%20thresholds%20consistently%20improves%20pose%20accuracy.%20Specifically%2C%20the%0Amean%20position%20error%20is%20reduced%20by%2041.0%25%2C%2056.7%25%2C%20and%2069.4%25%2C%20and%20the%20mean%0Aorientation%20error%20by%2055.6%25%2C%2065.7%25%2C%20and%2073.3%25%2C%20when%20applying%2090%25%2C%2080%25%2C%20and%2070%25%0Athresholds%2C%20respectively.%20Furthermore%2C%20the%20rejection%20strategy%20effectively%0Aremoves%20extreme%20outliers%2C%20resulting%20in%20better%20alignment%20with%20ground%20truth%0Atrajectories.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%20to%0Aquantitatively%20demonstrate%20the%20benefits%20of%20percentile-based%20uncertainty%0Arejection%20in%20multi-modal%20end-to-end%20localization%20tasks.%20Our%20approach%20provides%20a%0Apractical%20means%20to%20enhance%20the%20reliability%20and%20accuracy%20of%20localization%20systems%0Ain%20real-world%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07677v2&entry.124074799=Read"},
{"title": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs\n  Computing in Edge Network", "author": "Wenjing Xiao and Chenglong Shi and Miaojiang Chen and Zhiquan Liu and Min Chen and H. Herbert Song", "abstract": "  With the exponential growth of Internet of Things (IoT) devices, edge\ncomputing (EC) is gradually playing an important role in providing\ncost-effective services. However, existing approaches struggle to perform well\nin graph-structured scenarios where user data is correlated, such as traffic\nflow prediction and social relationship recommender systems. In particular,\ngraph neural network (GNN)-based approaches lead to expensive server\ncommunication cost. To address this problem, we propose GraphEdge, an efficient\nGNN-based EC architecture. It considers the EC system of GNN tasks, where there\nare associations between users and it needs to take into account the task data\nof its neighbors when processing the tasks of a user. Specifically, the\narchitecture first perceives the user topology and represents their data\nassociations as a graph layout at each time step. Then the graph layout is\noptimized by calling our proposed hierarchical traversal graph cut algorithm\n(HiCut), which cuts the graph layout into multiple weakly associated subgraphs\nbased on the aggregation characteristics of GNN, and the communication cost\nbetween different subgraphs during GNN inference is minimized. Finally, based\non the optimized graph layout, our proposed deep reinforcement learning (DRL)\nbased graph offloading algorithm (DRLGO) is executed to obtain the optimal\noffloading strategy for the tasks of users, the offloading strategy is\nsubgraph-based, it tries to offload user tasks in a subgraph to the same edge\nserver as possible while minimizing the task processing time and energy\nconsumption of the EC system. Experimental results show the good effectiveness\nand dynamic adaptation of our proposed architecture and it also performs well\neven in dynamic scenarios.\n", "link": "http://arxiv.org/abs/2504.15905v1", "date": "2025-04-22", "relevancy": 2.0721, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5696}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.485}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphEdge%3A%20Dynamic%20Graph%20Partition%20and%20Task%20Scheduling%20for%20GNNs%0A%20%20Computing%20in%20Edge%20Network&body=Title%3A%20GraphEdge%3A%20Dynamic%20Graph%20Partition%20and%20Task%20Scheduling%20for%20GNNs%0A%20%20Computing%20in%20Edge%20Network%0AAuthor%3A%20Wenjing%20Xiao%20and%20Chenglong%20Shi%20and%20Miaojiang%20Chen%20and%20Zhiquan%20Liu%20and%20Min%20Chen%20and%20H.%20Herbert%20Song%0AAbstract%3A%20%20%20With%20the%20exponential%20growth%20of%20Internet%20of%20Things%20%28IoT%29%20devices%2C%20edge%0Acomputing%20%28EC%29%20is%20gradually%20playing%20an%20important%20role%20in%20providing%0Acost-effective%20services.%20However%2C%20existing%20approaches%20struggle%20to%20perform%20well%0Ain%20graph-structured%20scenarios%20where%20user%20data%20is%20correlated%2C%20such%20as%20traffic%0Aflow%20prediction%20and%20social%20relationship%20recommender%20systems.%20In%20particular%2C%0Agraph%20neural%20network%20%28GNN%29-based%20approaches%20lead%20to%20expensive%20server%0Acommunication%20cost.%20To%20address%20this%20problem%2C%20we%20propose%20GraphEdge%2C%20an%20efficient%0AGNN-based%20EC%20architecture.%20It%20considers%20the%20EC%20system%20of%20GNN%20tasks%2C%20where%20there%0Aare%20associations%20between%20users%20and%20it%20needs%20to%20take%20into%20account%20the%20task%20data%0Aof%20its%20neighbors%20when%20processing%20the%20tasks%20of%20a%20user.%20Specifically%2C%20the%0Aarchitecture%20first%20perceives%20the%20user%20topology%20and%20represents%20their%20data%0Aassociations%20as%20a%20graph%20layout%20at%20each%20time%20step.%20Then%20the%20graph%20layout%20is%0Aoptimized%20by%20calling%20our%20proposed%20hierarchical%20traversal%20graph%20cut%20algorithm%0A%28HiCut%29%2C%20which%20cuts%20the%20graph%20layout%20into%20multiple%20weakly%20associated%20subgraphs%0Abased%20on%20the%20aggregation%20characteristics%20of%20GNN%2C%20and%20the%20communication%20cost%0Abetween%20different%20subgraphs%20during%20GNN%20inference%20is%20minimized.%20Finally%2C%20based%0Aon%20the%20optimized%20graph%20layout%2C%20our%20proposed%20deep%20reinforcement%20learning%20%28DRL%29%0Abased%20graph%20offloading%20algorithm%20%28DRLGO%29%20is%20executed%20to%20obtain%20the%20optimal%0Aoffloading%20strategy%20for%20the%20tasks%20of%20users%2C%20the%20offloading%20strategy%20is%0Asubgraph-based%2C%20it%20tries%20to%20offload%20user%20tasks%20in%20a%20subgraph%20to%20the%20same%20edge%0Aserver%20as%20possible%20while%20minimizing%20the%20task%20processing%20time%20and%20energy%0Aconsumption%20of%20the%20EC%20system.%20Experimental%20results%20show%20the%20good%20effectiveness%0Aand%20dynamic%20adaptation%20of%20our%20proposed%20architecture%20and%20it%20also%20performs%20well%0Aeven%20in%20dynamic%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphEdge%253A%2520Dynamic%2520Graph%2520Partition%2520and%2520Task%2520Scheduling%2520for%2520GNNs%250A%2520%2520Computing%2520in%2520Edge%2520Network%26entry.906535625%3DWenjing%2520Xiao%2520and%2520Chenglong%2520Shi%2520and%2520Miaojiang%2520Chen%2520and%2520Zhiquan%2520Liu%2520and%2520Min%2520Chen%2520and%2520H.%2520Herbert%2520Song%26entry.1292438233%3D%2520%2520With%2520the%2520exponential%2520growth%2520of%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520devices%252C%2520edge%250Acomputing%2520%2528EC%2529%2520is%2520gradually%2520playing%2520an%2520important%2520role%2520in%2520providing%250Acost-effective%2520services.%2520However%252C%2520existing%2520approaches%2520struggle%2520to%2520perform%2520well%250Ain%2520graph-structured%2520scenarios%2520where%2520user%2520data%2520is%2520correlated%252C%2520such%2520as%2520traffic%250Aflow%2520prediction%2520and%2520social%2520relationship%2520recommender%2520systems.%2520In%2520particular%252C%250Agraph%2520neural%2520network%2520%2528GNN%2529-based%2520approaches%2520lead%2520to%2520expensive%2520server%250Acommunication%2520cost.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520GraphEdge%252C%2520an%2520efficient%250AGNN-based%2520EC%2520architecture.%2520It%2520considers%2520the%2520EC%2520system%2520of%2520GNN%2520tasks%252C%2520where%2520there%250Aare%2520associations%2520between%2520users%2520and%2520it%2520needs%2520to%2520take%2520into%2520account%2520the%2520task%2520data%250Aof%2520its%2520neighbors%2520when%2520processing%2520the%2520tasks%2520of%2520a%2520user.%2520Specifically%252C%2520the%250Aarchitecture%2520first%2520perceives%2520the%2520user%2520topology%2520and%2520represents%2520their%2520data%250Aassociations%2520as%2520a%2520graph%2520layout%2520at%2520each%2520time%2520step.%2520Then%2520the%2520graph%2520layout%2520is%250Aoptimized%2520by%2520calling%2520our%2520proposed%2520hierarchical%2520traversal%2520graph%2520cut%2520algorithm%250A%2528HiCut%2529%252C%2520which%2520cuts%2520the%2520graph%2520layout%2520into%2520multiple%2520weakly%2520associated%2520subgraphs%250Abased%2520on%2520the%2520aggregation%2520characteristics%2520of%2520GNN%252C%2520and%2520the%2520communication%2520cost%250Abetween%2520different%2520subgraphs%2520during%2520GNN%2520inference%2520is%2520minimized.%2520Finally%252C%2520based%250Aon%2520the%2520optimized%2520graph%2520layout%252C%2520our%2520proposed%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%250Abased%2520graph%2520offloading%2520algorithm%2520%2528DRLGO%2529%2520is%2520executed%2520to%2520obtain%2520the%2520optimal%250Aoffloading%2520strategy%2520for%2520the%2520tasks%2520of%2520users%252C%2520the%2520offloading%2520strategy%2520is%250Asubgraph-based%252C%2520it%2520tries%2520to%2520offload%2520user%2520tasks%2520in%2520a%2520subgraph%2520to%2520the%2520same%2520edge%250Aserver%2520as%2520possible%2520while%2520minimizing%2520the%2520task%2520processing%2520time%2520and%2520energy%250Aconsumption%2520of%2520the%2520EC%2520system.%2520Experimental%2520results%2520show%2520the%2520good%2520effectiveness%250Aand%2520dynamic%2520adaptation%2520of%2520our%2520proposed%2520architecture%2520and%2520it%2520also%2520performs%2520well%250Aeven%2520in%2520dynamic%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphEdge%3A%20Dynamic%20Graph%20Partition%20and%20Task%20Scheduling%20for%20GNNs%0A%20%20Computing%20in%20Edge%20Network&entry.906535625=Wenjing%20Xiao%20and%20Chenglong%20Shi%20and%20Miaojiang%20Chen%20and%20Zhiquan%20Liu%20and%20Min%20Chen%20and%20H.%20Herbert%20Song&entry.1292438233=%20%20With%20the%20exponential%20growth%20of%20Internet%20of%20Things%20%28IoT%29%20devices%2C%20edge%0Acomputing%20%28EC%29%20is%20gradually%20playing%20an%20important%20role%20in%20providing%0Acost-effective%20services.%20However%2C%20existing%20approaches%20struggle%20to%20perform%20well%0Ain%20graph-structured%20scenarios%20where%20user%20data%20is%20correlated%2C%20such%20as%20traffic%0Aflow%20prediction%20and%20social%20relationship%20recommender%20systems.%20In%20particular%2C%0Agraph%20neural%20network%20%28GNN%29-based%20approaches%20lead%20to%20expensive%20server%0Acommunication%20cost.%20To%20address%20this%20problem%2C%20we%20propose%20GraphEdge%2C%20an%20efficient%0AGNN-based%20EC%20architecture.%20It%20considers%20the%20EC%20system%20of%20GNN%20tasks%2C%20where%20there%0Aare%20associations%20between%20users%20and%20it%20needs%20to%20take%20into%20account%20the%20task%20data%0Aof%20its%20neighbors%20when%20processing%20the%20tasks%20of%20a%20user.%20Specifically%2C%20the%0Aarchitecture%20first%20perceives%20the%20user%20topology%20and%20represents%20their%20data%0Aassociations%20as%20a%20graph%20layout%20at%20each%20time%20step.%20Then%20the%20graph%20layout%20is%0Aoptimized%20by%20calling%20our%20proposed%20hierarchical%20traversal%20graph%20cut%20algorithm%0A%28HiCut%29%2C%20which%20cuts%20the%20graph%20layout%20into%20multiple%20weakly%20associated%20subgraphs%0Abased%20on%20the%20aggregation%20characteristics%20of%20GNN%2C%20and%20the%20communication%20cost%0Abetween%20different%20subgraphs%20during%20GNN%20inference%20is%20minimized.%20Finally%2C%20based%0Aon%20the%20optimized%20graph%20layout%2C%20our%20proposed%20deep%20reinforcement%20learning%20%28DRL%29%0Abased%20graph%20offloading%20algorithm%20%28DRLGO%29%20is%20executed%20to%20obtain%20the%20optimal%0Aoffloading%20strategy%20for%20the%20tasks%20of%20users%2C%20the%20offloading%20strategy%20is%0Asubgraph-based%2C%20it%20tries%20to%20offload%20user%20tasks%20in%20a%20subgraph%20to%20the%20same%20edge%0Aserver%20as%20possible%20while%20minimizing%20the%20task%20processing%20time%20and%20energy%0Aconsumption%20of%20the%20EC%20system.%20Experimental%20results%20show%20the%20good%20effectiveness%0Aand%20dynamic%20adaptation%20of%20our%20proposed%20architecture%20and%20it%20also%20performs%20well%0Aeven%20in%20dynamic%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15905v1&entry.124074799=Read"},
{"title": "Achieving Distributive Justice in Federated Learning via Uncertainty\n  Quantification", "author": "Alycia Carey and Xintao Wu", "abstract": "  Client-level fairness metrics for federated learning are used to ensure that\nall clients in a federation either: a) have similar final performance on their\nlocal data distributions (i.e., client parity), or b) obtain final performance\non their local data distributions relative to their contribution to the\nfederated learning process (i.e., contribution fairness). While a handful of\nworks that propose either client-parity or contribution-based fairness metrics\nground their definitions and decisions in social theories of equality -- such\nas distributive justice -- most works arbitrarily choose what notion of\nfairness to align with which makes it difficult for practitioners to choose\nwhich fairness metric aligns best with their fairness ethics. In this work, we\npropose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),\na flexible federated learning framework that can achieve multiple distributive\njustice-based client-level fairness metrics. Namely, by utilizing techniques\ninspired by fair resource allocation, in conjunction with performing aleatoric\nuncertainty-based client weighing, our UDJ-FL framework is able to achieve\negalitarian, utilitarian, Rawls' difference principle, or desert-based\nclient-level fairness. We empirically show the ability of UDJ-FL to achieve all\nfour defined distributive justice-based client-level fairness metrics in\naddition to providing fairness equivalent to (or surpassing) other popular fair\nfederated learning works. Further, we provide justification for why aleatoric\nuncertainty weighing is necessary to the construction of our UDJ-FL framework\nas well as derive theoretical guarantees for the generalization bounds of\nUDJ-FL. Our code is publicly available at\nhttps://github.com/alycia-noel/UDJ-FL.\n", "link": "http://arxiv.org/abs/2504.15924v1", "date": "2025-04-22", "relevancy": 2.0669, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.562}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5306}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Achieving%20Distributive%20Justice%20in%20Federated%20Learning%20via%20Uncertainty%0A%20%20Quantification&body=Title%3A%20Achieving%20Distributive%20Justice%20in%20Federated%20Learning%20via%20Uncertainty%0A%20%20Quantification%0AAuthor%3A%20Alycia%20Carey%20and%20Xintao%20Wu%0AAbstract%3A%20%20%20Client-level%20fairness%20metrics%20for%20federated%20learning%20are%20used%20to%20ensure%20that%0Aall%20clients%20in%20a%20federation%20either%3A%20a%29%20have%20similar%20final%20performance%20on%20their%0Alocal%20data%20distributions%20%28i.e.%2C%20client%20parity%29%2C%20or%20b%29%20obtain%20final%20performance%0Aon%20their%20local%20data%20distributions%20relative%20to%20their%20contribution%20to%20the%0Afederated%20learning%20process%20%28i.e.%2C%20contribution%20fairness%29.%20While%20a%20handful%20of%0Aworks%20that%20propose%20either%20client-parity%20or%20contribution-based%20fairness%20metrics%0Aground%20their%20definitions%20and%20decisions%20in%20social%20theories%20of%20equality%20--%20such%0Aas%20distributive%20justice%20--%20most%20works%20arbitrarily%20choose%20what%20notion%20of%0Afairness%20to%20align%20with%20which%20makes%20it%20difficult%20for%20practitioners%20to%20choose%0Awhich%20fairness%20metric%20aligns%20best%20with%20their%20fairness%20ethics.%20In%20this%20work%2C%20we%0Apropose%20UDJ-FL%20%28Uncertainty-based%20Distributive%20Justice%20for%20Federated%20Learning%29%2C%0Aa%20flexible%20federated%20learning%20framework%20that%20can%20achieve%20multiple%20distributive%0Ajustice-based%20client-level%20fairness%20metrics.%20Namely%2C%20by%20utilizing%20techniques%0Ainspired%20by%20fair%20resource%20allocation%2C%20in%20conjunction%20with%20performing%20aleatoric%0Auncertainty-based%20client%20weighing%2C%20our%20UDJ-FL%20framework%20is%20able%20to%20achieve%0Aegalitarian%2C%20utilitarian%2C%20Rawls%27%20difference%20principle%2C%20or%20desert-based%0Aclient-level%20fairness.%20We%20empirically%20show%20the%20ability%20of%20UDJ-FL%20to%20achieve%20all%0Afour%20defined%20distributive%20justice-based%20client-level%20fairness%20metrics%20in%0Aaddition%20to%20providing%20fairness%20equivalent%20to%20%28or%20surpassing%29%20other%20popular%20fair%0Afederated%20learning%20works.%20Further%2C%20we%20provide%20justification%20for%20why%20aleatoric%0Auncertainty%20weighing%20is%20necessary%20to%20the%20construction%20of%20our%20UDJ-FL%20framework%0Aas%20well%20as%20derive%20theoretical%20guarantees%20for%20the%20generalization%20bounds%20of%0AUDJ-FL.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/alycia-noel/UDJ-FL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAchieving%2520Distributive%2520Justice%2520in%2520Federated%2520Learning%2520via%2520Uncertainty%250A%2520%2520Quantification%26entry.906535625%3DAlycia%2520Carey%2520and%2520Xintao%2520Wu%26entry.1292438233%3D%2520%2520Client-level%2520fairness%2520metrics%2520for%2520federated%2520learning%2520are%2520used%2520to%2520ensure%2520that%250Aall%2520clients%2520in%2520a%2520federation%2520either%253A%2520a%2529%2520have%2520similar%2520final%2520performance%2520on%2520their%250Alocal%2520data%2520distributions%2520%2528i.e.%252C%2520client%2520parity%2529%252C%2520or%2520b%2529%2520obtain%2520final%2520performance%250Aon%2520their%2520local%2520data%2520distributions%2520relative%2520to%2520their%2520contribution%2520to%2520the%250Afederated%2520learning%2520process%2520%2528i.e.%252C%2520contribution%2520fairness%2529.%2520While%2520a%2520handful%2520of%250Aworks%2520that%2520propose%2520either%2520client-parity%2520or%2520contribution-based%2520fairness%2520metrics%250Aground%2520their%2520definitions%2520and%2520decisions%2520in%2520social%2520theories%2520of%2520equality%2520--%2520such%250Aas%2520distributive%2520justice%2520--%2520most%2520works%2520arbitrarily%2520choose%2520what%2520notion%2520of%250Afairness%2520to%2520align%2520with%2520which%2520makes%2520it%2520difficult%2520for%2520practitioners%2520to%2520choose%250Awhich%2520fairness%2520metric%2520aligns%2520best%2520with%2520their%2520fairness%2520ethics.%2520In%2520this%2520work%252C%2520we%250Apropose%2520UDJ-FL%2520%2528Uncertainty-based%2520Distributive%2520Justice%2520for%2520Federated%2520Learning%2529%252C%250Aa%2520flexible%2520federated%2520learning%2520framework%2520that%2520can%2520achieve%2520multiple%2520distributive%250Ajustice-based%2520client-level%2520fairness%2520metrics.%2520Namely%252C%2520by%2520utilizing%2520techniques%250Ainspired%2520by%2520fair%2520resource%2520allocation%252C%2520in%2520conjunction%2520with%2520performing%2520aleatoric%250Auncertainty-based%2520client%2520weighing%252C%2520our%2520UDJ-FL%2520framework%2520is%2520able%2520to%2520achieve%250Aegalitarian%252C%2520utilitarian%252C%2520Rawls%2527%2520difference%2520principle%252C%2520or%2520desert-based%250Aclient-level%2520fairness.%2520We%2520empirically%2520show%2520the%2520ability%2520of%2520UDJ-FL%2520to%2520achieve%2520all%250Afour%2520defined%2520distributive%2520justice-based%2520client-level%2520fairness%2520metrics%2520in%250Aaddition%2520to%2520providing%2520fairness%2520equivalent%2520to%2520%2528or%2520surpassing%2529%2520other%2520popular%2520fair%250Afederated%2520learning%2520works.%2520Further%252C%2520we%2520provide%2520justification%2520for%2520why%2520aleatoric%250Auncertainty%2520weighing%2520is%2520necessary%2520to%2520the%2520construction%2520of%2520our%2520UDJ-FL%2520framework%250Aas%2520well%2520as%2520derive%2520theoretical%2520guarantees%2520for%2520the%2520generalization%2520bounds%2520of%250AUDJ-FL.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/alycia-noel/UDJ-FL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Achieving%20Distributive%20Justice%20in%20Federated%20Learning%20via%20Uncertainty%0A%20%20Quantification&entry.906535625=Alycia%20Carey%20and%20Xintao%20Wu&entry.1292438233=%20%20Client-level%20fairness%20metrics%20for%20federated%20learning%20are%20used%20to%20ensure%20that%0Aall%20clients%20in%20a%20federation%20either%3A%20a%29%20have%20similar%20final%20performance%20on%20their%0Alocal%20data%20distributions%20%28i.e.%2C%20client%20parity%29%2C%20or%20b%29%20obtain%20final%20performance%0Aon%20their%20local%20data%20distributions%20relative%20to%20their%20contribution%20to%20the%0Afederated%20learning%20process%20%28i.e.%2C%20contribution%20fairness%29.%20While%20a%20handful%20of%0Aworks%20that%20propose%20either%20client-parity%20or%20contribution-based%20fairness%20metrics%0Aground%20their%20definitions%20and%20decisions%20in%20social%20theories%20of%20equality%20--%20such%0Aas%20distributive%20justice%20--%20most%20works%20arbitrarily%20choose%20what%20notion%20of%0Afairness%20to%20align%20with%20which%20makes%20it%20difficult%20for%20practitioners%20to%20choose%0Awhich%20fairness%20metric%20aligns%20best%20with%20their%20fairness%20ethics.%20In%20this%20work%2C%20we%0Apropose%20UDJ-FL%20%28Uncertainty-based%20Distributive%20Justice%20for%20Federated%20Learning%29%2C%0Aa%20flexible%20federated%20learning%20framework%20that%20can%20achieve%20multiple%20distributive%0Ajustice-based%20client-level%20fairness%20metrics.%20Namely%2C%20by%20utilizing%20techniques%0Ainspired%20by%20fair%20resource%20allocation%2C%20in%20conjunction%20with%20performing%20aleatoric%0Auncertainty-based%20client%20weighing%2C%20our%20UDJ-FL%20framework%20is%20able%20to%20achieve%0Aegalitarian%2C%20utilitarian%2C%20Rawls%27%20difference%20principle%2C%20or%20desert-based%0Aclient-level%20fairness.%20We%20empirically%20show%20the%20ability%20of%20UDJ-FL%20to%20achieve%20all%0Afour%20defined%20distributive%20justice-based%20client-level%20fairness%20metrics%20in%0Aaddition%20to%20providing%20fairness%20equivalent%20to%20%28or%20surpassing%29%20other%20popular%20fair%0Afederated%20learning%20works.%20Further%2C%20we%20provide%20justification%20for%20why%20aleatoric%0Auncertainty%20weighing%20is%20necessary%20to%20the%20construction%20of%20our%20UDJ-FL%20framework%0Aas%20well%20as%20derive%20theoretical%20guarantees%20for%20the%20generalization%20bounds%20of%0AUDJ-FL.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/alycia-noel/UDJ-FL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15924v1&entry.124074799=Read"},
{"title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement\n  Learning on LLM", "author": "Xiaojiang Zhang and Jinghui Wang and Zifei Cheng and Wenhao Zhuang and Zheng Lin and Minglei Zhang and Shaojie Wang and Yinghan Cui and Chao Wang and Junyi Peng and Shimiao Jiang and Shiqi Kuang and Shouyu Yin and Chaohang Wen and Haotian Zhang and Bin Chen and Bing Yu", "abstract": "  Recent advances of reasoning models, exemplified by OpenAI's o1 and\nDeepSeek's R1, highlight the significant potential of Reinforcement Learning\n(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).\nHowever, replicating these advancements across diverse domains remains\nchallenging due to limited methodological transparency. In this work, we\npresent two-Staged history-Resampling Policy Optimization (SRPO), which\nsurpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and\nLiveCodeBench benchmarks. SRPO achieves this using the same base model as\nDeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps\nrequired by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building\nupon Group Relative Policy Optimization (GRPO), we introduce two key\nmethodological innovations: (1) a two-stage cross-domain training paradigm\ndesigned to balance the development of mathematical reasoning and coding\nproficiency, and (2) History Resampling (HR), a technique to address\nineffective samples. Our comprehensive experiments validate the effectiveness\nof our approach, offering valuable insights into scaling LLM reasoning\ncapabilities across diverse tasks.\n", "link": "http://arxiv.org/abs/2504.14286v2", "date": "2025-04-22", "relevancy": 2.0625, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRPO%3A%20A%20Cross-Domain%20Implementation%20of%20Large-Scale%20Reinforcement%0A%20%20Learning%20on%20LLM&body=Title%3A%20SRPO%3A%20A%20Cross-Domain%20Implementation%20of%20Large-Scale%20Reinforcement%0A%20%20Learning%20on%20LLM%0AAuthor%3A%20Xiaojiang%20Zhang%20and%20Jinghui%20Wang%20and%20Zifei%20Cheng%20and%20Wenhao%20Zhuang%20and%20Zheng%20Lin%20and%20Minglei%20Zhang%20and%20Shaojie%20Wang%20and%20Yinghan%20Cui%20and%20Chao%20Wang%20and%20Junyi%20Peng%20and%20Shimiao%20Jiang%20and%20Shiqi%20Kuang%20and%20Shouyu%20Yin%20and%20Chaohang%20Wen%20and%20Haotian%20Zhang%20and%20Bin%20Chen%20and%20Bing%20Yu%0AAbstract%3A%20%20%20Recent%20advances%20of%20reasoning%20models%2C%20exemplified%20by%20OpenAI%27s%20o1%20and%0ADeepSeek%27s%20R1%2C%20highlight%20the%20significant%20potential%20of%20Reinforcement%20Learning%0A%28RL%29%20to%20enhance%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20replicating%20these%20advancements%20across%20diverse%20domains%20remains%0Achallenging%20due%20to%20limited%20methodological%20transparency.%20In%20this%20work%2C%20we%0Apresent%20two-Staged%20history-Resampling%20Policy%20Optimization%20%28SRPO%29%2C%20which%0Asurpasses%20the%20performance%20of%20DeepSeek-R1-Zero-32B%20on%20the%20AIME24%20and%0ALiveCodeBench%20benchmarks.%20SRPO%20achieves%20this%20using%20the%20same%20base%20model%20as%0ADeepSeek%20%28i.e.%20Qwen2.5-32B%29%2C%20using%20only%20about%201/10%20of%20the%20training%20steps%0Arequired%20by%20DeepSeek-R1-Zero-32B%2C%20demonstrating%20superior%20efficiency.%20Building%0Aupon%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20we%20introduce%20two%20key%0Amethodological%20innovations%3A%20%281%29%20a%20two-stage%20cross-domain%20training%20paradigm%0Adesigned%20to%20balance%20the%20development%20of%20mathematical%20reasoning%20and%20coding%0Aproficiency%2C%20and%20%282%29%20History%20Resampling%20%28HR%29%2C%20a%20technique%20to%20address%0Aineffective%20samples.%20Our%20comprehensive%20experiments%20validate%20the%20effectiveness%0Aof%20our%20approach%2C%20offering%20valuable%20insights%20into%20scaling%20LLM%20reasoning%0Acapabilities%20across%20diverse%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14286v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRPO%253A%2520A%2520Cross-Domain%2520Implementation%2520of%2520Large-Scale%2520Reinforcement%250A%2520%2520Learning%2520on%2520LLM%26entry.906535625%3DXiaojiang%2520Zhang%2520and%2520Jinghui%2520Wang%2520and%2520Zifei%2520Cheng%2520and%2520Wenhao%2520Zhuang%2520and%2520Zheng%2520Lin%2520and%2520Minglei%2520Zhang%2520and%2520Shaojie%2520Wang%2520and%2520Yinghan%2520Cui%2520and%2520Chao%2520Wang%2520and%2520Junyi%2520Peng%2520and%2520Shimiao%2520Jiang%2520and%2520Shiqi%2520Kuang%2520and%2520Shouyu%2520Yin%2520and%2520Chaohang%2520Wen%2520and%2520Haotian%2520Zhang%2520and%2520Bin%2520Chen%2520and%2520Bing%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520of%2520reasoning%2520models%252C%2520exemplified%2520by%2520OpenAI%2527s%2520o1%2520and%250ADeepSeek%2527s%2520R1%252C%2520highlight%2520the%2520significant%2520potential%2520of%2520Reinforcement%2520Learning%250A%2528RL%2529%2520to%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250AHowever%252C%2520replicating%2520these%2520advancements%2520across%2520diverse%2520domains%2520remains%250Achallenging%2520due%2520to%2520limited%2520methodological%2520transparency.%2520In%2520this%2520work%252C%2520we%250Apresent%2520two-Staged%2520history-Resampling%2520Policy%2520Optimization%2520%2528SRPO%2529%252C%2520which%250Asurpasses%2520the%2520performance%2520of%2520DeepSeek-R1-Zero-32B%2520on%2520the%2520AIME24%2520and%250ALiveCodeBench%2520benchmarks.%2520SRPO%2520achieves%2520this%2520using%2520the%2520same%2520base%2520model%2520as%250ADeepSeek%2520%2528i.e.%2520Qwen2.5-32B%2529%252C%2520using%2520only%2520about%25201/10%2520of%2520the%2520training%2520steps%250Arequired%2520by%2520DeepSeek-R1-Zero-32B%252C%2520demonstrating%2520superior%2520efficiency.%2520Building%250Aupon%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520we%2520introduce%2520two%2520key%250Amethodological%2520innovations%253A%2520%25281%2529%2520a%2520two-stage%2520cross-domain%2520training%2520paradigm%250Adesigned%2520to%2520balance%2520the%2520development%2520of%2520mathematical%2520reasoning%2520and%2520coding%250Aproficiency%252C%2520and%2520%25282%2529%2520History%2520Resampling%2520%2528HR%2529%252C%2520a%2520technique%2520to%2520address%250Aineffective%2520samples.%2520Our%2520comprehensive%2520experiments%2520validate%2520the%2520effectiveness%250Aof%2520our%2520approach%252C%2520offering%2520valuable%2520insights%2520into%2520scaling%2520LLM%2520reasoning%250Acapabilities%2520across%2520diverse%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14286v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRPO%3A%20A%20Cross-Domain%20Implementation%20of%20Large-Scale%20Reinforcement%0A%20%20Learning%20on%20LLM&entry.906535625=Xiaojiang%20Zhang%20and%20Jinghui%20Wang%20and%20Zifei%20Cheng%20and%20Wenhao%20Zhuang%20and%20Zheng%20Lin%20and%20Minglei%20Zhang%20and%20Shaojie%20Wang%20and%20Yinghan%20Cui%20and%20Chao%20Wang%20and%20Junyi%20Peng%20and%20Shimiao%20Jiang%20and%20Shiqi%20Kuang%20and%20Shouyu%20Yin%20and%20Chaohang%20Wen%20and%20Haotian%20Zhang%20and%20Bin%20Chen%20and%20Bing%20Yu&entry.1292438233=%20%20Recent%20advances%20of%20reasoning%20models%2C%20exemplified%20by%20OpenAI%27s%20o1%20and%0ADeepSeek%27s%20R1%2C%20highlight%20the%20significant%20potential%20of%20Reinforcement%20Learning%0A%28RL%29%20to%20enhance%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20replicating%20these%20advancements%20across%20diverse%20domains%20remains%0Achallenging%20due%20to%20limited%20methodological%20transparency.%20In%20this%20work%2C%20we%0Apresent%20two-Staged%20history-Resampling%20Policy%20Optimization%20%28SRPO%29%2C%20which%0Asurpasses%20the%20performance%20of%20DeepSeek-R1-Zero-32B%20on%20the%20AIME24%20and%0ALiveCodeBench%20benchmarks.%20SRPO%20achieves%20this%20using%20the%20same%20base%20model%20as%0ADeepSeek%20%28i.e.%20Qwen2.5-32B%29%2C%20using%20only%20about%201/10%20of%20the%20training%20steps%0Arequired%20by%20DeepSeek-R1-Zero-32B%2C%20demonstrating%20superior%20efficiency.%20Building%0Aupon%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20we%20introduce%20two%20key%0Amethodological%20innovations%3A%20%281%29%20a%20two-stage%20cross-domain%20training%20paradigm%0Adesigned%20to%20balance%20the%20development%20of%20mathematical%20reasoning%20and%20coding%0Aproficiency%2C%20and%20%282%29%20History%20Resampling%20%28HR%29%2C%20a%20technique%20to%20address%0Aineffective%20samples.%20Our%20comprehensive%20experiments%20validate%20the%20effectiveness%0Aof%20our%20approach%2C%20offering%20valuable%20insights%20into%20scaling%20LLM%20reasoning%0Acapabilities%20across%20diverse%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14286v2&entry.124074799=Read"},
{"title": "Mitigating Traffic Oscillations in Mixed Traffic Flow with Scalable Deep\n  Koopman Predictive Control", "author": "Hao Lyu and Yanyong Guo and Pan Liu and Nan Zheng and Ting Wang and Quansheng Yue", "abstract": "  The use of connected automated vehicle (CAV) is advocated to mitigate traffic\noscillations in mixed traffic flow consisting of CAVs and human driven vehicles\n(HDVs). This study proposes an adaptive deep Koopman predictive control\nframework (AdapKoopPC) for regulating mixed traffic flow. Firstly, a Koopman\ntheory-based adaptive trajectory prediction deep network (AdapKoopnet) is\ndesigned for modeling HDVs car-following behavior. AdapKoopnet enables the\nrepresentation of HDVs behavior by a linear model in a high-dimensional space.\nSecondly, the model predictive control is employed to smooth the mixed traffic\nflow, where the combination of the linear dynamic model of CAVs and linear\nprediction blocks from AdapKoopnet is embedded as the predictive model into the\nAdapKoopPC. Finally, the predictive performance of the prosed AdapKoopnet is\nverified using the HighD naturalistic driving dataset. Furthermore, the control\nperformance of AdapKoopPC is validated by the numerical simulations. Results\ndemonstrate that the AdapKoopnet provides more accuracy HDVs predicted\ntrajectories than the baseline nonlinear models. Moreover, the proposed\nAdapKoopPC exhibits more effective control performance with less computation\ncost compared with baselines in mitigating traffic oscillations, especially at\nthe low CAVs penetration rates. The code of proposed AdapKoopPC is open source.\n", "link": "http://arxiv.org/abs/2502.00043v2", "date": "2025-04-22", "relevancy": 2.057, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5299}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5135}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Traffic%20Oscillations%20in%20Mixed%20Traffic%20Flow%20with%20Scalable%20Deep%0A%20%20Koopman%20Predictive%20Control&body=Title%3A%20Mitigating%20Traffic%20Oscillations%20in%20Mixed%20Traffic%20Flow%20with%20Scalable%20Deep%0A%20%20Koopman%20Predictive%20Control%0AAuthor%3A%20Hao%20Lyu%20and%20Yanyong%20Guo%20and%20Pan%20Liu%20and%20Nan%20Zheng%20and%20Ting%20Wang%20and%20Quansheng%20Yue%0AAbstract%3A%20%20%20The%20use%20of%20connected%20automated%20vehicle%20%28CAV%29%20is%20advocated%20to%20mitigate%20traffic%0Aoscillations%20in%20mixed%20traffic%20flow%20consisting%20of%20CAVs%20and%20human%20driven%20vehicles%0A%28HDVs%29.%20This%20study%20proposes%20an%20adaptive%20deep%20Koopman%20predictive%20control%0Aframework%20%28AdapKoopPC%29%20for%20regulating%20mixed%20traffic%20flow.%20Firstly%2C%20a%20Koopman%0Atheory-based%20adaptive%20trajectory%20prediction%20deep%20network%20%28AdapKoopnet%29%20is%0Adesigned%20for%20modeling%20HDVs%20car-following%20behavior.%20AdapKoopnet%20enables%20the%0Arepresentation%20of%20HDVs%20behavior%20by%20a%20linear%20model%20in%20a%20high-dimensional%20space.%0ASecondly%2C%20the%20model%20predictive%20control%20is%20employed%20to%20smooth%20the%20mixed%20traffic%0Aflow%2C%20where%20the%20combination%20of%20the%20linear%20dynamic%20model%20of%20CAVs%20and%20linear%0Aprediction%20blocks%20from%20AdapKoopnet%20is%20embedded%20as%20the%20predictive%20model%20into%20the%0AAdapKoopPC.%20Finally%2C%20the%20predictive%20performance%20of%20the%20prosed%20AdapKoopnet%20is%0Averified%20using%20the%20HighD%20naturalistic%20driving%20dataset.%20Furthermore%2C%20the%20control%0Aperformance%20of%20AdapKoopPC%20is%20validated%20by%20the%20numerical%20simulations.%20Results%0Ademonstrate%20that%20the%20AdapKoopnet%20provides%20more%20accuracy%20HDVs%20predicted%0Atrajectories%20than%20the%20baseline%20nonlinear%20models.%20Moreover%2C%20the%20proposed%0AAdapKoopPC%20exhibits%20more%20effective%20control%20performance%20with%20less%20computation%0Acost%20compared%20with%20baselines%20in%20mitigating%20traffic%20oscillations%2C%20especially%20at%0Athe%20low%20CAVs%20penetration%20rates.%20The%20code%20of%20proposed%20AdapKoopPC%20is%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Traffic%2520Oscillations%2520in%2520Mixed%2520Traffic%2520Flow%2520with%2520Scalable%2520Deep%250A%2520%2520Koopman%2520Predictive%2520Control%26entry.906535625%3DHao%2520Lyu%2520and%2520Yanyong%2520Guo%2520and%2520Pan%2520Liu%2520and%2520Nan%2520Zheng%2520and%2520Ting%2520Wang%2520and%2520Quansheng%2520Yue%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520connected%2520automated%2520vehicle%2520%2528CAV%2529%2520is%2520advocated%2520to%2520mitigate%2520traffic%250Aoscillations%2520in%2520mixed%2520traffic%2520flow%2520consisting%2520of%2520CAVs%2520and%2520human%2520driven%2520vehicles%250A%2528HDVs%2529.%2520This%2520study%2520proposes%2520an%2520adaptive%2520deep%2520Koopman%2520predictive%2520control%250Aframework%2520%2528AdapKoopPC%2529%2520for%2520regulating%2520mixed%2520traffic%2520flow.%2520Firstly%252C%2520a%2520Koopman%250Atheory-based%2520adaptive%2520trajectory%2520prediction%2520deep%2520network%2520%2528AdapKoopnet%2529%2520is%250Adesigned%2520for%2520modeling%2520HDVs%2520car-following%2520behavior.%2520AdapKoopnet%2520enables%2520the%250Arepresentation%2520of%2520HDVs%2520behavior%2520by%2520a%2520linear%2520model%2520in%2520a%2520high-dimensional%2520space.%250ASecondly%252C%2520the%2520model%2520predictive%2520control%2520is%2520employed%2520to%2520smooth%2520the%2520mixed%2520traffic%250Aflow%252C%2520where%2520the%2520combination%2520of%2520the%2520linear%2520dynamic%2520model%2520of%2520CAVs%2520and%2520linear%250Aprediction%2520blocks%2520from%2520AdapKoopnet%2520is%2520embedded%2520as%2520the%2520predictive%2520model%2520into%2520the%250AAdapKoopPC.%2520Finally%252C%2520the%2520predictive%2520performance%2520of%2520the%2520prosed%2520AdapKoopnet%2520is%250Averified%2520using%2520the%2520HighD%2520naturalistic%2520driving%2520dataset.%2520Furthermore%252C%2520the%2520control%250Aperformance%2520of%2520AdapKoopPC%2520is%2520validated%2520by%2520the%2520numerical%2520simulations.%2520Results%250Ademonstrate%2520that%2520the%2520AdapKoopnet%2520provides%2520more%2520accuracy%2520HDVs%2520predicted%250Atrajectories%2520than%2520the%2520baseline%2520nonlinear%2520models.%2520Moreover%252C%2520the%2520proposed%250AAdapKoopPC%2520exhibits%2520more%2520effective%2520control%2520performance%2520with%2520less%2520computation%250Acost%2520compared%2520with%2520baselines%2520in%2520mitigating%2520traffic%2520oscillations%252C%2520especially%2520at%250Athe%2520low%2520CAVs%2520penetration%2520rates.%2520The%2520code%2520of%2520proposed%2520AdapKoopPC%2520is%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Traffic%20Oscillations%20in%20Mixed%20Traffic%20Flow%20with%20Scalable%20Deep%0A%20%20Koopman%20Predictive%20Control&entry.906535625=Hao%20Lyu%20and%20Yanyong%20Guo%20and%20Pan%20Liu%20and%20Nan%20Zheng%20and%20Ting%20Wang%20and%20Quansheng%20Yue&entry.1292438233=%20%20The%20use%20of%20connected%20automated%20vehicle%20%28CAV%29%20is%20advocated%20to%20mitigate%20traffic%0Aoscillations%20in%20mixed%20traffic%20flow%20consisting%20of%20CAVs%20and%20human%20driven%20vehicles%0A%28HDVs%29.%20This%20study%20proposes%20an%20adaptive%20deep%20Koopman%20predictive%20control%0Aframework%20%28AdapKoopPC%29%20for%20regulating%20mixed%20traffic%20flow.%20Firstly%2C%20a%20Koopman%0Atheory-based%20adaptive%20trajectory%20prediction%20deep%20network%20%28AdapKoopnet%29%20is%0Adesigned%20for%20modeling%20HDVs%20car-following%20behavior.%20AdapKoopnet%20enables%20the%0Arepresentation%20of%20HDVs%20behavior%20by%20a%20linear%20model%20in%20a%20high-dimensional%20space.%0ASecondly%2C%20the%20model%20predictive%20control%20is%20employed%20to%20smooth%20the%20mixed%20traffic%0Aflow%2C%20where%20the%20combination%20of%20the%20linear%20dynamic%20model%20of%20CAVs%20and%20linear%0Aprediction%20blocks%20from%20AdapKoopnet%20is%20embedded%20as%20the%20predictive%20model%20into%20the%0AAdapKoopPC.%20Finally%2C%20the%20predictive%20performance%20of%20the%20prosed%20AdapKoopnet%20is%0Averified%20using%20the%20HighD%20naturalistic%20driving%20dataset.%20Furthermore%2C%20the%20control%0Aperformance%20of%20AdapKoopPC%20is%20validated%20by%20the%20numerical%20simulations.%20Results%0Ademonstrate%20that%20the%20AdapKoopnet%20provides%20more%20accuracy%20HDVs%20predicted%0Atrajectories%20than%20the%20baseline%20nonlinear%20models.%20Moreover%2C%20the%20proposed%0AAdapKoopPC%20exhibits%20more%20effective%20control%20performance%20with%20less%20computation%0Acost%20compared%20with%20baselines%20in%20mitigating%20traffic%20oscillations%2C%20especially%20at%0Athe%20low%20CAVs%20penetration%20rates.%20The%20code%20of%20proposed%20AdapKoopPC%20is%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00043v2&entry.124074799=Read"},
{"title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities", "author": "Thomas Schmied and J\u00f6rg Bornschein and Jordi Grau-Moya and Markus Wulfmeier and Razvan Pascanu", "abstract": "  The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.\n", "link": "http://arxiv.org/abs/2504.16078v1", "date": "2025-04-22", "relevancy": 2.0496, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20are%20Greedy%20Agents%3A%20Effects%20of%20RL%20Fine-tuning%20on%20Decision-Making%0A%20%20Abilities&body=Title%3A%20LLMs%20are%20Greedy%20Agents%3A%20Effects%20of%20RL%20Fine-tuning%20on%20Decision-Making%0A%20%20Abilities%0AAuthor%3A%20Thomas%20Schmied%20and%20J%C3%B6rg%20Bornschein%20and%20Jordi%20Grau-Moya%20and%20Markus%20Wulfmeier%20and%20Razvan%20Pascanu%0AAbstract%3A%20%20%20The%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20sparked%20interest%20in%20various%0Aagentic%20applications.%20A%20key%20hypothesis%20is%20that%20LLMs%2C%20leveraging%20common%20sense%0Aand%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20can%20effectively%20explore%20and%20efficiently%0Asolve%20complex%20domains.%20However%2C%20LLM%20agents%20have%20been%20found%20to%20suffer%20from%0Asub-optimal%20exploration%20and%20the%20knowing-doing%20gap%2C%20the%20inability%20to%20effectively%0Aact%20on%20knowledge%20present%20in%20the%20model.%20In%20this%20work%2C%20we%20systematically%20study%0Awhy%20LLMs%20perform%20sub-optimally%20in%20decision-making%20scenarios.%20In%20particular%2C%20we%0Aclosely%20examine%20three%20prevalent%20failure%20modes%3A%20greediness%2C%20frequency%20bias%2C%20and%0Athe%20knowing-doing%20gap.%20We%20propose%20mitigation%20of%20these%20shortcomings%20by%0Afine-tuning%20via%20Reinforcement%20Learning%20%28RL%29%20on%20self-generated%20CoT%20rationales.%0AOur%20experiments%20across%20multi-armed%20bandits%2C%20contextual%20bandits%2C%20and%0ATic-tac-toe%2C%20demonstrate%20that%20RL%20fine-tuning%20enhances%20the%20decision-making%0Aabilities%20of%20LLMs%20by%20increasing%20exploration%20and%20narrowing%20the%20knowing-doing%0Agap.%20Finally%2C%20we%20study%20both%20classic%20exploration%20mechanisms%2C%20such%20as%0A%24%5Cepsilon%24-greedy%2C%20and%20LLM-specific%20approaches%2C%20such%20as%20self-correction%20and%0Aself-consistency%2C%20to%20enable%20more%20effective%20fine-tuning%20of%20LLMs%20for%0Adecision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520are%2520Greedy%2520Agents%253A%2520Effects%2520of%2520RL%2520Fine-tuning%2520on%2520Decision-Making%250A%2520%2520Abilities%26entry.906535625%3DThomas%2520Schmied%2520and%2520J%25C3%25B6rg%2520Bornschein%2520and%2520Jordi%2520Grau-Moya%2520and%2520Markus%2520Wulfmeier%2520and%2520Razvan%2520Pascanu%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520sparked%2520interest%2520in%2520various%250Aagentic%2520applications.%2520A%2520key%2520hypothesis%2520is%2520that%2520LLMs%252C%2520leveraging%2520common%2520sense%250Aand%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%252C%2520can%2520effectively%2520explore%2520and%2520efficiently%250Asolve%2520complex%2520domains.%2520However%252C%2520LLM%2520agents%2520have%2520been%2520found%2520to%2520suffer%2520from%250Asub-optimal%2520exploration%2520and%2520the%2520knowing-doing%2520gap%252C%2520the%2520inability%2520to%2520effectively%250Aact%2520on%2520knowledge%2520present%2520in%2520the%2520model.%2520In%2520this%2520work%252C%2520we%2520systematically%2520study%250Awhy%2520LLMs%2520perform%2520sub-optimally%2520in%2520decision-making%2520scenarios.%2520In%2520particular%252C%2520we%250Aclosely%2520examine%2520three%2520prevalent%2520failure%2520modes%253A%2520greediness%252C%2520frequency%2520bias%252C%2520and%250Athe%2520knowing-doing%2520gap.%2520We%2520propose%2520mitigation%2520of%2520these%2520shortcomings%2520by%250Afine-tuning%2520via%2520Reinforcement%2520Learning%2520%2528RL%2529%2520on%2520self-generated%2520CoT%2520rationales.%250AOur%2520experiments%2520across%2520multi-armed%2520bandits%252C%2520contextual%2520bandits%252C%2520and%250ATic-tac-toe%252C%2520demonstrate%2520that%2520RL%2520fine-tuning%2520enhances%2520the%2520decision-making%250Aabilities%2520of%2520LLMs%2520by%2520increasing%2520exploration%2520and%2520narrowing%2520the%2520knowing-doing%250Agap.%2520Finally%252C%2520we%2520study%2520both%2520classic%2520exploration%2520mechanisms%252C%2520such%2520as%250A%2524%255Cepsilon%2524-greedy%252C%2520and%2520LLM-specific%2520approaches%252C%2520such%2520as%2520self-correction%2520and%250Aself-consistency%252C%2520to%2520enable%2520more%2520effective%2520fine-tuning%2520of%2520LLMs%2520for%250Adecision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20are%20Greedy%20Agents%3A%20Effects%20of%20RL%20Fine-tuning%20on%20Decision-Making%0A%20%20Abilities&entry.906535625=Thomas%20Schmied%20and%20J%C3%B6rg%20Bornschein%20and%20Jordi%20Grau-Moya%20and%20Markus%20Wulfmeier%20and%20Razvan%20Pascanu&entry.1292438233=%20%20The%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20sparked%20interest%20in%20various%0Aagentic%20applications.%20A%20key%20hypothesis%20is%20that%20LLMs%2C%20leveraging%20common%20sense%0Aand%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20can%20effectively%20explore%20and%20efficiently%0Asolve%20complex%20domains.%20However%2C%20LLM%20agents%20have%20been%20found%20to%20suffer%20from%0Asub-optimal%20exploration%20and%20the%20knowing-doing%20gap%2C%20the%20inability%20to%20effectively%0Aact%20on%20knowledge%20present%20in%20the%20model.%20In%20this%20work%2C%20we%20systematically%20study%0Awhy%20LLMs%20perform%20sub-optimally%20in%20decision-making%20scenarios.%20In%20particular%2C%20we%0Aclosely%20examine%20three%20prevalent%20failure%20modes%3A%20greediness%2C%20frequency%20bias%2C%20and%0Athe%20knowing-doing%20gap.%20We%20propose%20mitigation%20of%20these%20shortcomings%20by%0Afine-tuning%20via%20Reinforcement%20Learning%20%28RL%29%20on%20self-generated%20CoT%20rationales.%0AOur%20experiments%20across%20multi-armed%20bandits%2C%20contextual%20bandits%2C%20and%0ATic-tac-toe%2C%20demonstrate%20that%20RL%20fine-tuning%20enhances%20the%20decision-making%0Aabilities%20of%20LLMs%20by%20increasing%20exploration%20and%20narrowing%20the%20knowing-doing%0Agap.%20Finally%2C%20we%20study%20both%20classic%20exploration%20mechanisms%2C%20such%20as%0A%24%5Cepsilon%24-greedy%2C%20and%20LLM-specific%20approaches%2C%20such%20as%20self-correction%20and%0Aself-consistency%2C%20to%20enable%20more%20effective%20fine-tuning%20of%20LLMs%20for%0Adecision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16078v1&entry.124074799=Read"},
{"title": "Harnessing Language for Coordination: A Framework and Benchmark for\n  LLM-Driven Multi-Agent Control", "author": "Timoth\u00e9e Anne and Noah Syrkis and Meriem Elhosni and Florian Turati and Franck Legendre and Alain Jaquier and Sebastian Risi", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. Their potential to facilitate human coordination with many\nagents is a promising but largely under-explored area. Such capabilities would\nbe helpful in disaster response, urban planning, and real-time strategy\nscenarios. In this work, we introduce (1) a real-time strategy game benchmark\ndesigned to evaluate these abilities and (2) a novel framework we term HIVE.\nHIVE empowers a single human to coordinate swarms of up to 2,000 agents through\na natural language dialog with an LLM. We present promising results on this\nmulti-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. Our findings also\nhighlight critical limitations of current models, including difficulties in\nprocessing spatial visual information and challenges in formulating long-term\nstrategic plans. This work sheds light on the potential and limitations of LLMs\nin human-swarm coordination, paving the way for future research in this area.\nThe HIVE project page, hive.syrkis.com, includes videos of the system in\naction.\n", "link": "http://arxiv.org/abs/2412.11761v2", "date": "2025-04-22", "relevancy": 2.0437, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Language%20for%20Coordination%3A%20A%20Framework%20and%20Benchmark%20for%0A%20%20LLM-Driven%20Multi-Agent%20Control&body=Title%3A%20Harnessing%20Language%20for%20Coordination%3A%20A%20Framework%20and%20Benchmark%20for%0A%20%20LLM-Driven%20Multi-Agent%20Control%0AAuthor%3A%20Timoth%C3%A9e%20Anne%20and%20Noah%20Syrkis%20and%20Meriem%20Elhosni%20and%20Florian%20Turati%20and%20Franck%20Legendre%20and%20Alain%20Jaquier%20and%20Sebastian%20Risi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Avarious%20tasks.%20Their%20potential%20to%20facilitate%20human%20coordination%20with%20many%0Aagents%20is%20a%20promising%20but%20largely%20under-explored%20area.%20Such%20capabilities%20would%0Abe%20helpful%20in%20disaster%20response%2C%20urban%20planning%2C%20and%20real-time%20strategy%0Ascenarios.%20In%20this%20work%2C%20we%20introduce%20%281%29%20a%20real-time%20strategy%20game%20benchmark%0Adesigned%20to%20evaluate%20these%20abilities%20and%20%282%29%20a%20novel%20framework%20we%20term%20HIVE.%0AHIVE%20empowers%20a%20single%20human%20to%20coordinate%20swarms%20of%20up%20to%202%2C000%20agents%20through%0Aa%20natural%20language%20dialog%20with%20an%20LLM.%20We%20present%20promising%20results%20on%20this%0Amulti-agent%20benchmark%2C%20with%20our%20hybrid%20approach%20solving%20tasks%20such%20as%0Acoordinating%20agent%20movements%2C%20exploiting%20unit%20weaknesses%2C%20leveraging%20human%0Aannotations%2C%20and%20understanding%20terrain%20and%20strategic%20points.%20Our%20findings%20also%0Ahighlight%20critical%20limitations%20of%20current%20models%2C%20including%20difficulties%20in%0Aprocessing%20spatial%20visual%20information%20and%20challenges%20in%20formulating%20long-term%0Astrategic%20plans.%20This%20work%20sheds%20light%20on%20the%20potential%20and%20limitations%20of%20LLMs%0Ain%20human-swarm%20coordination%2C%20paving%20the%20way%20for%20future%20research%20in%20this%20area.%0AThe%20HIVE%20project%20page%2C%20hive.syrkis.com%2C%20includes%20videos%20of%20the%20system%20in%0Aaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Language%2520for%2520Coordination%253A%2520A%2520Framework%2520and%2520Benchmark%2520for%250A%2520%2520LLM-Driven%2520Multi-Agent%2520Control%26entry.906535625%3DTimoth%25C3%25A9e%2520Anne%2520and%2520Noah%2520Syrkis%2520and%2520Meriem%2520Elhosni%2520and%2520Florian%2520Turati%2520and%2520Franck%2520Legendre%2520and%2520Alain%2520Jaquier%2520and%2520Sebastian%2520Risi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%250Avarious%2520tasks.%2520Their%2520potential%2520to%2520facilitate%2520human%2520coordination%2520with%2520many%250Aagents%2520is%2520a%2520promising%2520but%2520largely%2520under-explored%2520area.%2520Such%2520capabilities%2520would%250Abe%2520helpful%2520in%2520disaster%2520response%252C%2520urban%2520planning%252C%2520and%2520real-time%2520strategy%250Ascenarios.%2520In%2520this%2520work%252C%2520we%2520introduce%2520%25281%2529%2520a%2520real-time%2520strategy%2520game%2520benchmark%250Adesigned%2520to%2520evaluate%2520these%2520abilities%2520and%2520%25282%2529%2520a%2520novel%2520framework%2520we%2520term%2520HIVE.%250AHIVE%2520empowers%2520a%2520single%2520human%2520to%2520coordinate%2520swarms%2520of%2520up%2520to%25202%252C000%2520agents%2520through%250Aa%2520natural%2520language%2520dialog%2520with%2520an%2520LLM.%2520We%2520present%2520promising%2520results%2520on%2520this%250Amulti-agent%2520benchmark%252C%2520with%2520our%2520hybrid%2520approach%2520solving%2520tasks%2520such%2520as%250Acoordinating%2520agent%2520movements%252C%2520exploiting%2520unit%2520weaknesses%252C%2520leveraging%2520human%250Aannotations%252C%2520and%2520understanding%2520terrain%2520and%2520strategic%2520points.%2520Our%2520findings%2520also%250Ahighlight%2520critical%2520limitations%2520of%2520current%2520models%252C%2520including%2520difficulties%2520in%250Aprocessing%2520spatial%2520visual%2520information%2520and%2520challenges%2520in%2520formulating%2520long-term%250Astrategic%2520plans.%2520This%2520work%2520sheds%2520light%2520on%2520the%2520potential%2520and%2520limitations%2520of%2520LLMs%250Ain%2520human-swarm%2520coordination%252C%2520paving%2520the%2520way%2520for%2520future%2520research%2520in%2520this%2520area.%250AThe%2520HIVE%2520project%2520page%252C%2520hive.syrkis.com%252C%2520includes%2520videos%2520of%2520the%2520system%2520in%250Aaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Language%20for%20Coordination%3A%20A%20Framework%20and%20Benchmark%20for%0A%20%20LLM-Driven%20Multi-Agent%20Control&entry.906535625=Timoth%C3%A9e%20Anne%20and%20Noah%20Syrkis%20and%20Meriem%20Elhosni%20and%20Florian%20Turati%20and%20Franck%20Legendre%20and%20Alain%20Jaquier%20and%20Sebastian%20Risi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Avarious%20tasks.%20Their%20potential%20to%20facilitate%20human%20coordination%20with%20many%0Aagents%20is%20a%20promising%20but%20largely%20under-explored%20area.%20Such%20capabilities%20would%0Abe%20helpful%20in%20disaster%20response%2C%20urban%20planning%2C%20and%20real-time%20strategy%0Ascenarios.%20In%20this%20work%2C%20we%20introduce%20%281%29%20a%20real-time%20strategy%20game%20benchmark%0Adesigned%20to%20evaluate%20these%20abilities%20and%20%282%29%20a%20novel%20framework%20we%20term%20HIVE.%0AHIVE%20empowers%20a%20single%20human%20to%20coordinate%20swarms%20of%20up%20to%202%2C000%20agents%20through%0Aa%20natural%20language%20dialog%20with%20an%20LLM.%20We%20present%20promising%20results%20on%20this%0Amulti-agent%20benchmark%2C%20with%20our%20hybrid%20approach%20solving%20tasks%20such%20as%0Acoordinating%20agent%20movements%2C%20exploiting%20unit%20weaknesses%2C%20leveraging%20human%0Aannotations%2C%20and%20understanding%20terrain%20and%20strategic%20points.%20Our%20findings%20also%0Ahighlight%20critical%20limitations%20of%20current%20models%2C%20including%20difficulties%20in%0Aprocessing%20spatial%20visual%20information%20and%20challenges%20in%20formulating%20long-term%0Astrategic%20plans.%20This%20work%20sheds%20light%20on%20the%20potential%20and%20limitations%20of%20LLMs%0Ain%20human-swarm%20coordination%2C%20paving%20the%20way%20for%20future%20research%20in%20this%20area.%0AThe%20HIVE%20project%20page%2C%20hive.syrkis.com%2C%20includes%20videos%20of%20the%20system%20in%0Aaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11761v2&entry.124074799=Read"},
{"title": "What's the Difference? Supporting Users in Identifying the Effects of\n  Prompt and Model Changes Through Token Patterns", "author": "Michael A. Hedderich and Anyi Wang and Raoyuan Zhao and Florian Eichin and Barbara Plank", "abstract": "  Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods, either automated metrics or human\nevaluation, have limitations, such as providing limited insights or being\nlabor-intensive. We propose Spotlight, a new approach that combines both\nautomation and human analysis. Based on data mining techniques, we\nautomatically distinguish between random (decoding) variations and systematic\ndifferences in language model outputs. This process provides token patterns\nthat describe the systematic differences and guide the user in manually\nanalyzing the effects of their prompt and model changes efficiently. We create\nthree benchmarks to quantitatively test the reliability of token pattern\nextraction methods and demonstrate that our approach provides new insights into\nestablished prompt data. From a human-centric perspective, through\ndemonstration studies and a user study, we show that our token pattern approach\nhelps users understand the systematic differences of language model outputs,\nand we are able to discover relevant differences caused by prompt and model\nchanges (e.g. related to gender or culture), thus supporting the prompt\nengineering process and human-centric model behavior research.\n", "link": "http://arxiv.org/abs/2504.15815v1", "date": "2025-04-22", "relevancy": 2.032, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%27s%20the%20Difference%3F%20Supporting%20Users%20in%20Identifying%20the%20Effects%20of%0A%20%20Prompt%20and%20Model%20Changes%20Through%20Token%20Patterns&body=Title%3A%20What%27s%20the%20Difference%3F%20Supporting%20Users%20in%20Identifying%20the%20Effects%20of%0A%20%20Prompt%20and%20Model%20Changes%20Through%20Token%20Patterns%0AAuthor%3A%20Michael%20A.%20Hedderich%20and%20Anyi%20Wang%20and%20Raoyuan%20Zhao%20and%20Florian%20Eichin%20and%20Barbara%20Plank%0AAbstract%3A%20%20%20Prompt%20engineering%20for%20large%20language%20models%20is%20challenging%2C%20as%20even%20small%0Aprompt%20perturbations%20or%20model%20changes%20can%20significantly%20impact%20the%20generated%0Aoutput%20texts.%20Existing%20evaluation%20methods%2C%20either%20automated%20metrics%20or%20human%0Aevaluation%2C%20have%20limitations%2C%20such%20as%20providing%20limited%20insights%20or%20being%0Alabor-intensive.%20We%20propose%20Spotlight%2C%20a%20new%20approach%20that%20combines%20both%0Aautomation%20and%20human%20analysis.%20Based%20on%20data%20mining%20techniques%2C%20we%0Aautomatically%20distinguish%20between%20random%20%28decoding%29%20variations%20and%20systematic%0Adifferences%20in%20language%20model%20outputs.%20This%20process%20provides%20token%20patterns%0Athat%20describe%20the%20systematic%20differences%20and%20guide%20the%20user%20in%20manually%0Aanalyzing%20the%20effects%20of%20their%20prompt%20and%20model%20changes%20efficiently.%20We%20create%0Athree%20benchmarks%20to%20quantitatively%20test%20the%20reliability%20of%20token%20pattern%0Aextraction%20methods%20and%20demonstrate%20that%20our%20approach%20provides%20new%20insights%20into%0Aestablished%20prompt%20data.%20From%20a%20human-centric%20perspective%2C%20through%0Ademonstration%20studies%20and%20a%20user%20study%2C%20we%20show%20that%20our%20token%20pattern%20approach%0Ahelps%20users%20understand%20the%20systematic%20differences%20of%20language%20model%20outputs%2C%0Aand%20we%20are%20able%20to%20discover%20relevant%20differences%20caused%20by%20prompt%20and%20model%0Achanges%20%28e.g.%20related%20to%20gender%20or%20culture%29%2C%20thus%20supporting%20the%20prompt%0Aengineering%20process%20and%20human-centric%20model%20behavior%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2527s%2520the%2520Difference%253F%2520Supporting%2520Users%2520in%2520Identifying%2520the%2520Effects%2520of%250A%2520%2520Prompt%2520and%2520Model%2520Changes%2520Through%2520Token%2520Patterns%26entry.906535625%3DMichael%2520A.%2520Hedderich%2520and%2520Anyi%2520Wang%2520and%2520Raoyuan%2520Zhao%2520and%2520Florian%2520Eichin%2520and%2520Barbara%2520Plank%26entry.1292438233%3D%2520%2520Prompt%2520engineering%2520for%2520large%2520language%2520models%2520is%2520challenging%252C%2520as%2520even%2520small%250Aprompt%2520perturbations%2520or%2520model%2520changes%2520can%2520significantly%2520impact%2520the%2520generated%250Aoutput%2520texts.%2520Existing%2520evaluation%2520methods%252C%2520either%2520automated%2520metrics%2520or%2520human%250Aevaluation%252C%2520have%2520limitations%252C%2520such%2520as%2520providing%2520limited%2520insights%2520or%2520being%250Alabor-intensive.%2520We%2520propose%2520Spotlight%252C%2520a%2520new%2520approach%2520that%2520combines%2520both%250Aautomation%2520and%2520human%2520analysis.%2520Based%2520on%2520data%2520mining%2520techniques%252C%2520we%250Aautomatically%2520distinguish%2520between%2520random%2520%2528decoding%2529%2520variations%2520and%2520systematic%250Adifferences%2520in%2520language%2520model%2520outputs.%2520This%2520process%2520provides%2520token%2520patterns%250Athat%2520describe%2520the%2520systematic%2520differences%2520and%2520guide%2520the%2520user%2520in%2520manually%250Aanalyzing%2520the%2520effects%2520of%2520their%2520prompt%2520and%2520model%2520changes%2520efficiently.%2520We%2520create%250Athree%2520benchmarks%2520to%2520quantitatively%2520test%2520the%2520reliability%2520of%2520token%2520pattern%250Aextraction%2520methods%2520and%2520demonstrate%2520that%2520our%2520approach%2520provides%2520new%2520insights%2520into%250Aestablished%2520prompt%2520data.%2520From%2520a%2520human-centric%2520perspective%252C%2520through%250Ademonstration%2520studies%2520and%2520a%2520user%2520study%252C%2520we%2520show%2520that%2520our%2520token%2520pattern%2520approach%250Ahelps%2520users%2520understand%2520the%2520systematic%2520differences%2520of%2520language%2520model%2520outputs%252C%250Aand%2520we%2520are%2520able%2520to%2520discover%2520relevant%2520differences%2520caused%2520by%2520prompt%2520and%2520model%250Achanges%2520%2528e.g.%2520related%2520to%2520gender%2520or%2520culture%2529%252C%2520thus%2520supporting%2520the%2520prompt%250Aengineering%2520process%2520and%2520human-centric%2520model%2520behavior%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%27s%20the%20Difference%3F%20Supporting%20Users%20in%20Identifying%20the%20Effects%20of%0A%20%20Prompt%20and%20Model%20Changes%20Through%20Token%20Patterns&entry.906535625=Michael%20A.%20Hedderich%20and%20Anyi%20Wang%20and%20Raoyuan%20Zhao%20and%20Florian%20Eichin%20and%20Barbara%20Plank&entry.1292438233=%20%20Prompt%20engineering%20for%20large%20language%20models%20is%20challenging%2C%20as%20even%20small%0Aprompt%20perturbations%20or%20model%20changes%20can%20significantly%20impact%20the%20generated%0Aoutput%20texts.%20Existing%20evaluation%20methods%2C%20either%20automated%20metrics%20or%20human%0Aevaluation%2C%20have%20limitations%2C%20such%20as%20providing%20limited%20insights%20or%20being%0Alabor-intensive.%20We%20propose%20Spotlight%2C%20a%20new%20approach%20that%20combines%20both%0Aautomation%20and%20human%20analysis.%20Based%20on%20data%20mining%20techniques%2C%20we%0Aautomatically%20distinguish%20between%20random%20%28decoding%29%20variations%20and%20systematic%0Adifferences%20in%20language%20model%20outputs.%20This%20process%20provides%20token%20patterns%0Athat%20describe%20the%20systematic%20differences%20and%20guide%20the%20user%20in%20manually%0Aanalyzing%20the%20effects%20of%20their%20prompt%20and%20model%20changes%20efficiently.%20We%20create%0Athree%20benchmarks%20to%20quantitatively%20test%20the%20reliability%20of%20token%20pattern%0Aextraction%20methods%20and%20demonstrate%20that%20our%20approach%20provides%20new%20insights%20into%0Aestablished%20prompt%20data.%20From%20a%20human-centric%20perspective%2C%20through%0Ademonstration%20studies%20and%20a%20user%20study%2C%20we%20show%20that%20our%20token%20pattern%20approach%0Ahelps%20users%20understand%20the%20systematic%20differences%20of%20language%20model%20outputs%2C%0Aand%20we%20are%20able%20to%20discover%20relevant%20differences%20caused%20by%20prompt%20and%20model%0Achanges%20%28e.g.%20related%20to%20gender%20or%20culture%29%2C%20thus%20supporting%20the%20prompt%0Aengineering%20process%20and%20human-centric%20model%20behavior%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15815v1&entry.124074799=Read"},
{"title": "Grounded in Context: Retrieval-Based Method for Hallucination Detection", "author": "Assaf Gerner and Netta Madvil and Nadav Barak and Alex Zaikman and Jonatan Liberman and Liron Hamra and Rotem Brazilay and Shay Tsadok and Yaron Friedman and Neal Harow and Noam Bresler and Shir Chorev and Philip Tannor", "abstract": "  Despite advancements in grounded content generation, production Large\nLanguage Models (LLMs) based applications still suffer from hallucinated\nanswers. We present \"Grounded in Context\" - Deepchecks' hallucination detection\nframework, designed for production-scale long-context data and tailored to\ndiverse use cases, including summarization, data extraction, and RAG. Inspired\nby RAG architecture, our method integrates retrieval and Natural Language\nInference (NLI) models to predict factual consistency between premises and\nhypotheses using an encoder-based model with only a 512-token context window.\nOur framework identifies unsupported claims with an F1 score of 0.83 in\nRAGTruth's response-level classification task, matching methods that trained on\nthe dataset, and outperforming all comparable frameworks using similar-sized\nmodels.\n", "link": "http://arxiv.org/abs/2504.15771v1", "date": "2025-04-22", "relevancy": 2.0122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounded%20in%20Context%3A%20Retrieval-Based%20Method%20for%20Hallucination%20Detection&body=Title%3A%20Grounded%20in%20Context%3A%20Retrieval-Based%20Method%20for%20Hallucination%20Detection%0AAuthor%3A%20Assaf%20Gerner%20and%20Netta%20Madvil%20and%20Nadav%20Barak%20and%20Alex%20Zaikman%20and%20Jonatan%20Liberman%20and%20Liron%20Hamra%20and%20Rotem%20Brazilay%20and%20Shay%20Tsadok%20and%20Yaron%20Friedman%20and%20Neal%20Harow%20and%20Noam%20Bresler%20and%20Shir%20Chorev%20and%20Philip%20Tannor%0AAbstract%3A%20%20%20Despite%20advancements%20in%20grounded%20content%20generation%2C%20production%20Large%0ALanguage%20Models%20%28LLMs%29%20based%20applications%20still%20suffer%20from%20hallucinated%0Aanswers.%20We%20present%20%22Grounded%20in%20Context%22%20-%20Deepchecks%27%20hallucination%20detection%0Aframework%2C%20designed%20for%20production-scale%20long-context%20data%20and%20tailored%20to%0Adiverse%20use%20cases%2C%20including%20summarization%2C%20data%20extraction%2C%20and%20RAG.%20Inspired%0Aby%20RAG%20architecture%2C%20our%20method%20integrates%20retrieval%20and%20Natural%20Language%0AInference%20%28NLI%29%20models%20to%20predict%20factual%20consistency%20between%20premises%20and%0Ahypotheses%20using%20an%20encoder-based%20model%20with%20only%20a%20512-token%20context%20window.%0AOur%20framework%20identifies%20unsupported%20claims%20with%20an%20F1%20score%20of%200.83%20in%0ARAGTruth%27s%20response-level%20classification%20task%2C%20matching%20methods%20that%20trained%20on%0Athe%20dataset%2C%20and%20outperforming%20all%20comparable%20frameworks%20using%20similar-sized%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounded%2520in%2520Context%253A%2520Retrieval-Based%2520Method%2520for%2520Hallucination%2520Detection%26entry.906535625%3DAssaf%2520Gerner%2520and%2520Netta%2520Madvil%2520and%2520Nadav%2520Barak%2520and%2520Alex%2520Zaikman%2520and%2520Jonatan%2520Liberman%2520and%2520Liron%2520Hamra%2520and%2520Rotem%2520Brazilay%2520and%2520Shay%2520Tsadok%2520and%2520Yaron%2520Friedman%2520and%2520Neal%2520Harow%2520and%2520Noam%2520Bresler%2520and%2520Shir%2520Chorev%2520and%2520Philip%2520Tannor%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520grounded%2520content%2520generation%252C%2520production%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520based%2520applications%2520still%2520suffer%2520from%2520hallucinated%250Aanswers.%2520We%2520present%2520%2522Grounded%2520in%2520Context%2522%2520-%2520Deepchecks%2527%2520hallucination%2520detection%250Aframework%252C%2520designed%2520for%2520production-scale%2520long-context%2520data%2520and%2520tailored%2520to%250Adiverse%2520use%2520cases%252C%2520including%2520summarization%252C%2520data%2520extraction%252C%2520and%2520RAG.%2520Inspired%250Aby%2520RAG%2520architecture%252C%2520our%2520method%2520integrates%2520retrieval%2520and%2520Natural%2520Language%250AInference%2520%2528NLI%2529%2520models%2520to%2520predict%2520factual%2520consistency%2520between%2520premises%2520and%250Ahypotheses%2520using%2520an%2520encoder-based%2520model%2520with%2520only%2520a%2520512-token%2520context%2520window.%250AOur%2520framework%2520identifies%2520unsupported%2520claims%2520with%2520an%2520F1%2520score%2520of%25200.83%2520in%250ARAGTruth%2527s%2520response-level%2520classification%2520task%252C%2520matching%2520methods%2520that%2520trained%2520on%250Athe%2520dataset%252C%2520and%2520outperforming%2520all%2520comparable%2520frameworks%2520using%2520similar-sized%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounded%20in%20Context%3A%20Retrieval-Based%20Method%20for%20Hallucination%20Detection&entry.906535625=Assaf%20Gerner%20and%20Netta%20Madvil%20and%20Nadav%20Barak%20and%20Alex%20Zaikman%20and%20Jonatan%20Liberman%20and%20Liron%20Hamra%20and%20Rotem%20Brazilay%20and%20Shay%20Tsadok%20and%20Yaron%20Friedman%20and%20Neal%20Harow%20and%20Noam%20Bresler%20and%20Shir%20Chorev%20and%20Philip%20Tannor&entry.1292438233=%20%20Despite%20advancements%20in%20grounded%20content%20generation%2C%20production%20Large%0ALanguage%20Models%20%28LLMs%29%20based%20applications%20still%20suffer%20from%20hallucinated%0Aanswers.%20We%20present%20%22Grounded%20in%20Context%22%20-%20Deepchecks%27%20hallucination%20detection%0Aframework%2C%20designed%20for%20production-scale%20long-context%20data%20and%20tailored%20to%0Adiverse%20use%20cases%2C%20including%20summarization%2C%20data%20extraction%2C%20and%20RAG.%20Inspired%0Aby%20RAG%20architecture%2C%20our%20method%20integrates%20retrieval%20and%20Natural%20Language%0AInference%20%28NLI%29%20models%20to%20predict%20factual%20consistency%20between%20premises%20and%0Ahypotheses%20using%20an%20encoder-based%20model%20with%20only%20a%20512-token%20context%20window.%0AOur%20framework%20identifies%20unsupported%20claims%20with%20an%20F1%20score%20of%200.83%20in%0ARAGTruth%27s%20response-level%20classification%20task%2C%20matching%20methods%20that%20trained%20on%0Athe%20dataset%2C%20and%20outperforming%20all%20comparable%20frameworks%20using%20similar-sized%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15771v1&entry.124074799=Read"},
{"title": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical\n  Federated Learning", "author": "Sindhuja Madabushi and Ahmad Faraz Khan and Haider Ali and Jin-Hee Cho", "abstract": "  Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL.\n", "link": "http://arxiv.org/abs/2504.15995v1", "date": "2025-04-22", "relevancy": 2.0064, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5118}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4945}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPUS-VFL%3A%20Incentivizing%20Optimal%20Privacy-Utility%20Tradeoffs%20in%20Vertical%0A%20%20Federated%20Learning&body=Title%3A%20OPUS-VFL%3A%20Incentivizing%20Optimal%20Privacy-Utility%20Tradeoffs%20in%20Vertical%0A%20%20Federated%20Learning%0AAuthor%3A%20Sindhuja%20Madabushi%20and%20Ahmad%20Faraz%20Khan%20and%20Haider%20Ali%20and%20Jin-Hee%20Cho%0AAbstract%3A%20%20%20Vertical%20Federated%20Learning%20%28VFL%29%20enables%20organizations%20with%20disjoint%20feature%0Aspaces%20but%20shared%20user%20bases%20to%20collaboratively%20train%20models%20without%20sharing%0Araw%20data.%20However%2C%20existing%20VFL%20systems%20face%20critical%20limitations%3A%20they%20often%0Alack%20effective%20incentive%20mechanisms%2C%20struggle%20to%20balance%20privacy-utility%0Atradeoffs%2C%20and%20fail%20to%20accommodate%20clients%20with%20heterogeneous%20resource%0Acapabilities.%20These%20challenges%20hinder%20meaningful%20participation%2C%20degrade%20model%0Aperformance%2C%20and%20limit%20practical%20deployment.%20To%20address%20these%20issues%2C%20we%0Apropose%20OPUS-VFL%2C%20an%20Optimal%20Privacy-Utility%20tradeoff%20Strategy%20for%20VFL.%0AOPUS-VFL%20introduces%20a%20novel%2C%20privacy-aware%20incentive%20mechanism%20that%20rewards%0Aclients%20based%20on%20a%20principled%20combination%20of%20model%20contribution%2C%20privacy%0Apreservation%2C%20and%20resource%20investment.%20It%20employs%20a%20lightweight%20leave-one-out%0A%28LOO%29%20strategy%20to%20quantify%20feature%20importance%20per%20client%2C%20and%20integrates%20an%0Aadaptive%20differential%20privacy%20mechanism%20that%20enables%20clients%20to%20dynamically%0Acalibrate%20noise%20levels%20to%20optimize%20their%20individual%20utility.%20Our%20framework%20is%0Adesigned%20to%20be%20scalable%2C%20budget-balanced%2C%20and%20robust%20to%20inference%20and%20poisoning%0Aattacks.%20Extensive%20experiments%20on%20benchmark%20datasets%20%28MNIST%2C%20CIFAR-10%2C%20and%0ACIFAR-100%29%20demonstrate%20that%20OPUS-VFL%20significantly%20outperforms%20state-of-the-art%0AVFL%20baselines%20in%20both%20efficiency%20and%20robustness.%20It%20reduces%20label%20inference%0Aattack%20success%20rates%20by%20up%20to%2020%25%2C%20increases%20feature%20inference%20reconstruction%0Aerror%20%28MSE%29%20by%20over%2030%25%2C%20and%20achieves%20up%20to%2025%25%20higher%20incentives%20for%20clients%0Athat%20contribute%20meaningfully%20while%20respecting%20privacy%20and%20cost%20constraints.%0AThese%20results%20highlight%20the%20practicality%20and%20innovation%20of%20OPUS-VFL%20as%20a%0Asecure%2C%20fair%2C%20and%20performance-driven%20solution%20for%20real-world%20VFL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPUS-VFL%253A%2520Incentivizing%2520Optimal%2520Privacy-Utility%2520Tradeoffs%2520in%2520Vertical%250A%2520%2520Federated%2520Learning%26entry.906535625%3DSindhuja%2520Madabushi%2520and%2520Ahmad%2520Faraz%2520Khan%2520and%2520Haider%2520Ali%2520and%2520Jin-Hee%2520Cho%26entry.1292438233%3D%2520%2520Vertical%2520Federated%2520Learning%2520%2528VFL%2529%2520enables%2520organizations%2520with%2520disjoint%2520feature%250Aspaces%2520but%2520shared%2520user%2520bases%2520to%2520collaboratively%2520train%2520models%2520without%2520sharing%250Araw%2520data.%2520However%252C%2520existing%2520VFL%2520systems%2520face%2520critical%2520limitations%253A%2520they%2520often%250Alack%2520effective%2520incentive%2520mechanisms%252C%2520struggle%2520to%2520balance%2520privacy-utility%250Atradeoffs%252C%2520and%2520fail%2520to%2520accommodate%2520clients%2520with%2520heterogeneous%2520resource%250Acapabilities.%2520These%2520challenges%2520hinder%2520meaningful%2520participation%252C%2520degrade%2520model%250Aperformance%252C%2520and%2520limit%2520practical%2520deployment.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520OPUS-VFL%252C%2520an%2520Optimal%2520Privacy-Utility%2520tradeoff%2520Strategy%2520for%2520VFL.%250AOPUS-VFL%2520introduces%2520a%2520novel%252C%2520privacy-aware%2520incentive%2520mechanism%2520that%2520rewards%250Aclients%2520based%2520on%2520a%2520principled%2520combination%2520of%2520model%2520contribution%252C%2520privacy%250Apreservation%252C%2520and%2520resource%2520investment.%2520It%2520employs%2520a%2520lightweight%2520leave-one-out%250A%2528LOO%2529%2520strategy%2520to%2520quantify%2520feature%2520importance%2520per%2520client%252C%2520and%2520integrates%2520an%250Aadaptive%2520differential%2520privacy%2520mechanism%2520that%2520enables%2520clients%2520to%2520dynamically%250Acalibrate%2520noise%2520levels%2520to%2520optimize%2520their%2520individual%2520utility.%2520Our%2520framework%2520is%250Adesigned%2520to%2520be%2520scalable%252C%2520budget-balanced%252C%2520and%2520robust%2520to%2520inference%2520and%2520poisoning%250Aattacks.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520%2528MNIST%252C%2520CIFAR-10%252C%2520and%250ACIFAR-100%2529%2520demonstrate%2520that%2520OPUS-VFL%2520significantly%2520outperforms%2520state-of-the-art%250AVFL%2520baselines%2520in%2520both%2520efficiency%2520and%2520robustness.%2520It%2520reduces%2520label%2520inference%250Aattack%2520success%2520rates%2520by%2520up%2520to%252020%2525%252C%2520increases%2520feature%2520inference%2520reconstruction%250Aerror%2520%2528MSE%2529%2520by%2520over%252030%2525%252C%2520and%2520achieves%2520up%2520to%252025%2525%2520higher%2520incentives%2520for%2520clients%250Athat%2520contribute%2520meaningfully%2520while%2520respecting%2520privacy%2520and%2520cost%2520constraints.%250AThese%2520results%2520highlight%2520the%2520practicality%2520and%2520innovation%2520of%2520OPUS-VFL%2520as%2520a%250Asecure%252C%2520fair%252C%2520and%2520performance-driven%2520solution%2520for%2520real-world%2520VFL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPUS-VFL%3A%20Incentivizing%20Optimal%20Privacy-Utility%20Tradeoffs%20in%20Vertical%0A%20%20Federated%20Learning&entry.906535625=Sindhuja%20Madabushi%20and%20Ahmad%20Faraz%20Khan%20and%20Haider%20Ali%20and%20Jin-Hee%20Cho&entry.1292438233=%20%20Vertical%20Federated%20Learning%20%28VFL%29%20enables%20organizations%20with%20disjoint%20feature%0Aspaces%20but%20shared%20user%20bases%20to%20collaboratively%20train%20models%20without%20sharing%0Araw%20data.%20However%2C%20existing%20VFL%20systems%20face%20critical%20limitations%3A%20they%20often%0Alack%20effective%20incentive%20mechanisms%2C%20struggle%20to%20balance%20privacy-utility%0Atradeoffs%2C%20and%20fail%20to%20accommodate%20clients%20with%20heterogeneous%20resource%0Acapabilities.%20These%20challenges%20hinder%20meaningful%20participation%2C%20degrade%20model%0Aperformance%2C%20and%20limit%20practical%20deployment.%20To%20address%20these%20issues%2C%20we%0Apropose%20OPUS-VFL%2C%20an%20Optimal%20Privacy-Utility%20tradeoff%20Strategy%20for%20VFL.%0AOPUS-VFL%20introduces%20a%20novel%2C%20privacy-aware%20incentive%20mechanism%20that%20rewards%0Aclients%20based%20on%20a%20principled%20combination%20of%20model%20contribution%2C%20privacy%0Apreservation%2C%20and%20resource%20investment.%20It%20employs%20a%20lightweight%20leave-one-out%0A%28LOO%29%20strategy%20to%20quantify%20feature%20importance%20per%20client%2C%20and%20integrates%20an%0Aadaptive%20differential%20privacy%20mechanism%20that%20enables%20clients%20to%20dynamically%0Acalibrate%20noise%20levels%20to%20optimize%20their%20individual%20utility.%20Our%20framework%20is%0Adesigned%20to%20be%20scalable%2C%20budget-balanced%2C%20and%20robust%20to%20inference%20and%20poisoning%0Aattacks.%20Extensive%20experiments%20on%20benchmark%20datasets%20%28MNIST%2C%20CIFAR-10%2C%20and%0ACIFAR-100%29%20demonstrate%20that%20OPUS-VFL%20significantly%20outperforms%20state-of-the-art%0AVFL%20baselines%20in%20both%20efficiency%20and%20robustness.%20It%20reduces%20label%20inference%0Aattack%20success%20rates%20by%20up%20to%2020%25%2C%20increases%20feature%20inference%20reconstruction%0Aerror%20%28MSE%29%20by%20over%2030%25%2C%20and%20achieves%20up%20to%2025%25%20higher%20incentives%20for%20clients%0Athat%20contribute%20meaningfully%20while%20respecting%20privacy%20and%20cost%20constraints.%0AThese%20results%20highlight%20the%20practicality%20and%20innovation%20of%20OPUS-VFL%20as%20a%0Asecure%2C%20fair%2C%20and%20performance-driven%20solution%20for%20real-world%20VFL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15995v1&entry.124074799=Read"},
{"title": "SCMPPI: Supervised Contrastive Multimodal Framework for Predicting\n  Protein-Protein Interactions", "author": "Shengrui XU and Tianchi Lu and Zikun Wang and Jixiu Zhai and Jingwan Wang", "abstract": "  Protein-protein interaction (PPI) prediction plays a pivotal role in\ndeciphering cellular functions and disease mechanisms. To address the\nlimitations of traditional experimental methods and existing computational\napproaches in cross-modal feature fusion and false-negative suppression, we\npropose SCMPPI-a novel supervised contrastive multimodal framework. By\neffectively integrating sequence-based features (AAC, DPC, ESMC-CKSAAP) with\nnetwork topology (Node2Vec embeddings) and incorporating an enhanced\ncontrastive learning strategy with negative sample filtering, SCMPPI achieves\nsuperior prediction performance. Extensive experiments on eight benchmark\ndatasets demonstrate its state-of-the-art accuracy(98.13%) and AUC(99.69%),\nalong with excellent cross-species generalization (AUC>99%). Successful\napplications in CD9 networks, Wnt pathway analysis, and cancer-specific\nnetworks further highlight its potential for disease target discovery,\nestablishing SCMPPI as a powerful tool for multimodal biological data analysis.\n", "link": "http://arxiv.org/abs/2504.02698v2", "date": "2025-04-22", "relevancy": 2.0063, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCMPPI%3A%20Supervised%20Contrastive%20Multimodal%20Framework%20for%20Predicting%0A%20%20Protein-Protein%20Interactions&body=Title%3A%20SCMPPI%3A%20Supervised%20Contrastive%20Multimodal%20Framework%20for%20Predicting%0A%20%20Protein-Protein%20Interactions%0AAuthor%3A%20Shengrui%20XU%20and%20Tianchi%20Lu%20and%20Zikun%20Wang%20and%20Jixiu%20Zhai%20and%20Jingwan%20Wang%0AAbstract%3A%20%20%20Protein-protein%20interaction%20%28PPI%29%20prediction%20plays%20a%20pivotal%20role%20in%0Adeciphering%20cellular%20functions%20and%20disease%20mechanisms.%20To%20address%20the%0Alimitations%20of%20traditional%20experimental%20methods%20and%20existing%20computational%0Aapproaches%20in%20cross-modal%20feature%20fusion%20and%20false-negative%20suppression%2C%20we%0Apropose%20SCMPPI-a%20novel%20supervised%20contrastive%20multimodal%20framework.%20By%0Aeffectively%20integrating%20sequence-based%20features%20%28AAC%2C%20DPC%2C%20ESMC-CKSAAP%29%20with%0Anetwork%20topology%20%28Node2Vec%20embeddings%29%20and%20incorporating%20an%20enhanced%0Acontrastive%20learning%20strategy%20with%20negative%20sample%20filtering%2C%20SCMPPI%20achieves%0Asuperior%20prediction%20performance.%20Extensive%20experiments%20on%20eight%20benchmark%0Adatasets%20demonstrate%20its%20state-of-the-art%20accuracy%2898.13%25%29%20and%20AUC%2899.69%25%29%2C%0Aalong%20with%20excellent%20cross-species%20generalization%20%28AUC%3E99%25%29.%20Successful%0Aapplications%20in%20CD9%20networks%2C%20Wnt%20pathway%20analysis%2C%20and%20cancer-specific%0Anetworks%20further%20highlight%20its%20potential%20for%20disease%20target%20discovery%2C%0Aestablishing%20SCMPPI%20as%20a%20powerful%20tool%20for%20multimodal%20biological%20data%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCMPPI%253A%2520Supervised%2520Contrastive%2520Multimodal%2520Framework%2520for%2520Predicting%250A%2520%2520Protein-Protein%2520Interactions%26entry.906535625%3DShengrui%2520XU%2520and%2520Tianchi%2520Lu%2520and%2520Zikun%2520Wang%2520and%2520Jixiu%2520Zhai%2520and%2520Jingwan%2520Wang%26entry.1292438233%3D%2520%2520Protein-protein%2520interaction%2520%2528PPI%2529%2520prediction%2520plays%2520a%2520pivotal%2520role%2520in%250Adeciphering%2520cellular%2520functions%2520and%2520disease%2520mechanisms.%2520To%2520address%2520the%250Alimitations%2520of%2520traditional%2520experimental%2520methods%2520and%2520existing%2520computational%250Aapproaches%2520in%2520cross-modal%2520feature%2520fusion%2520and%2520false-negative%2520suppression%252C%2520we%250Apropose%2520SCMPPI-a%2520novel%2520supervised%2520contrastive%2520multimodal%2520framework.%2520By%250Aeffectively%2520integrating%2520sequence-based%2520features%2520%2528AAC%252C%2520DPC%252C%2520ESMC-CKSAAP%2529%2520with%250Anetwork%2520topology%2520%2528Node2Vec%2520embeddings%2529%2520and%2520incorporating%2520an%2520enhanced%250Acontrastive%2520learning%2520strategy%2520with%2520negative%2520sample%2520filtering%252C%2520SCMPPI%2520achieves%250Asuperior%2520prediction%2520performance.%2520Extensive%2520experiments%2520on%2520eight%2520benchmark%250Adatasets%2520demonstrate%2520its%2520state-of-the-art%2520accuracy%252898.13%2525%2529%2520and%2520AUC%252899.69%2525%2529%252C%250Aalong%2520with%2520excellent%2520cross-species%2520generalization%2520%2528AUC%253E99%2525%2529.%2520Successful%250Aapplications%2520in%2520CD9%2520networks%252C%2520Wnt%2520pathway%2520analysis%252C%2520and%2520cancer-specific%250Anetworks%2520further%2520highlight%2520its%2520potential%2520for%2520disease%2520target%2520discovery%252C%250Aestablishing%2520SCMPPI%2520as%2520a%2520powerful%2520tool%2520for%2520multimodal%2520biological%2520data%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCMPPI%3A%20Supervised%20Contrastive%20Multimodal%20Framework%20for%20Predicting%0A%20%20Protein-Protein%20Interactions&entry.906535625=Shengrui%20XU%20and%20Tianchi%20Lu%20and%20Zikun%20Wang%20and%20Jixiu%20Zhai%20and%20Jingwan%20Wang&entry.1292438233=%20%20Protein-protein%20interaction%20%28PPI%29%20prediction%20plays%20a%20pivotal%20role%20in%0Adeciphering%20cellular%20functions%20and%20disease%20mechanisms.%20To%20address%20the%0Alimitations%20of%20traditional%20experimental%20methods%20and%20existing%20computational%0Aapproaches%20in%20cross-modal%20feature%20fusion%20and%20false-negative%20suppression%2C%20we%0Apropose%20SCMPPI-a%20novel%20supervised%20contrastive%20multimodal%20framework.%20By%0Aeffectively%20integrating%20sequence-based%20features%20%28AAC%2C%20DPC%2C%20ESMC-CKSAAP%29%20with%0Anetwork%20topology%20%28Node2Vec%20embeddings%29%20and%20incorporating%20an%20enhanced%0Acontrastive%20learning%20strategy%20with%20negative%20sample%20filtering%2C%20SCMPPI%20achieves%0Asuperior%20prediction%20performance.%20Extensive%20experiments%20on%20eight%20benchmark%0Adatasets%20demonstrate%20its%20state-of-the-art%20accuracy%2898.13%25%29%20and%20AUC%2899.69%25%29%2C%0Aalong%20with%20excellent%20cross-species%20generalization%20%28AUC%3E99%25%29.%20Successful%0Aapplications%20in%20CD9%20networks%2C%20Wnt%20pathway%20analysis%2C%20and%20cancer-specific%0Anetworks%20further%20highlight%20its%20potential%20for%20disease%20target%20discovery%2C%0Aestablishing%20SCMPPI%20as%20a%20powerful%20tool%20for%20multimodal%20biological%20data%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02698v2&entry.124074799=Read"},
{"title": "Onboard Satellite Image Classification for Earth Observation: A\n  Comparative Study of ViT Models", "author": "Thanh-Dung Le and Vu Nguyen Ha and Ti Ti Nguyen and Geoffrey Eappen and Prabhu Thiruvasagam and Hong-fu Chou and Duc-Dung Tran and Hung Nguyen-Kha and Luis M. Garces-Socarras and Jorge L. Gonzalez-Rios and Juan Carlos Merlano-Duncan and Symeon Chatzinotas", "abstract": "  This study focuses on identifying the most effective pre-trained model for\nland use classification in onboard satellite processing, emphasizing achieving\nhigh accuracy, computational efficiency, and robustness against noisy data\nconditions commonly encountered during satellite-based inference. Through\nextensive experimentation, we compare the performance of traditional CNN-based,\nResNet-based, and various pre-trained vision Transformer models. Our findings\ndemonstrate that pre-trained Vision Transformer (ViT) models, particularly\nMobileViTV2 and EfficientViT-M2, outperform models trained from scratch in\nterms of accuracy and efficiency. These models achieve high performance with\nreduced computational requirements and exhibit greater resilience during\ninference under noisy conditions. While MobileViTV2 has excelled on clean\nvalidation data, EfficientViT-M2 has proved more robust when handling noise,\nmaking it the most suitable model for onboard satellite EO tasks. Our\nexperimental results demonstrate that EfficientViT-M2 is the optimal choice for\nreliable and efficient RS-IC in satellite operations, achieving 98.76 % of\naccuracy, precision, and recall. Precisely, EfficientViT-M2 delivers the\nhighest performance across all metrics, excels in training efficiency (1,000s)\nand inference time (10s), and demonstrates greater robustness (overall\nrobustness score of 0.79). Consequently, EfficientViT-M2 consumes 63.93 % less\npower than MobileViTV2 (79.23 W) and 73.26 % less power than SwinTransformer\n(108.90 W). This highlights its significant advantage in energy efficiency.\n", "link": "http://arxiv.org/abs/2409.03901v3", "date": "2025-04-22", "relevancy": 2.0059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Onboard%20Satellite%20Image%20Classification%20for%20Earth%20Observation%3A%20A%0A%20%20Comparative%20Study%20of%20ViT%20Models&body=Title%3A%20Onboard%20Satellite%20Image%20Classification%20for%20Earth%20Observation%3A%20A%0A%20%20Comparative%20Study%20of%20ViT%20Models%0AAuthor%3A%20Thanh-Dung%20Le%20and%20Vu%20Nguyen%20Ha%20and%20Ti%20Ti%20Nguyen%20and%20Geoffrey%20Eappen%20and%20Prabhu%20Thiruvasagam%20and%20Hong-fu%20Chou%20and%20Duc-Dung%20Tran%20and%20Hung%20Nguyen-Kha%20and%20Luis%20M.%20Garces-Socarras%20and%20Jorge%20L.%20Gonzalez-Rios%20and%20Juan%20Carlos%20Merlano-Duncan%20and%20Symeon%20Chatzinotas%0AAbstract%3A%20%20%20This%20study%20focuses%20on%20identifying%20the%20most%20effective%20pre-trained%20model%20for%0Aland%20use%20classification%20in%20onboard%20satellite%20processing%2C%20emphasizing%20achieving%0Ahigh%20accuracy%2C%20computational%20efficiency%2C%20and%20robustness%20against%20noisy%20data%0Aconditions%20commonly%20encountered%20during%20satellite-based%20inference.%20Through%0Aextensive%20experimentation%2C%20we%20compare%20the%20performance%20of%20traditional%20CNN-based%2C%0AResNet-based%2C%20and%20various%20pre-trained%20vision%20Transformer%20models.%20Our%20findings%0Ademonstrate%20that%20pre-trained%20Vision%20Transformer%20%28ViT%29%20models%2C%20particularly%0AMobileViTV2%20and%20EfficientViT-M2%2C%20outperform%20models%20trained%20from%20scratch%20in%0Aterms%20of%20accuracy%20and%20efficiency.%20These%20models%20achieve%20high%20performance%20with%0Areduced%20computational%20requirements%20and%20exhibit%20greater%20resilience%20during%0Ainference%20under%20noisy%20conditions.%20While%20MobileViTV2%20has%20excelled%20on%20clean%0Avalidation%20data%2C%20EfficientViT-M2%20has%20proved%20more%20robust%20when%20handling%20noise%2C%0Amaking%20it%20the%20most%20suitable%20model%20for%20onboard%20satellite%20EO%20tasks.%20Our%0Aexperimental%20results%20demonstrate%20that%20EfficientViT-M2%20is%20the%20optimal%20choice%20for%0Areliable%20and%20efficient%20RS-IC%20in%20satellite%20operations%2C%20achieving%2098.76%20%25%20of%0Aaccuracy%2C%20precision%2C%20and%20recall.%20Precisely%2C%20EfficientViT-M2%20delivers%20the%0Ahighest%20performance%20across%20all%20metrics%2C%20excels%20in%20training%20efficiency%20%281%2C000s%29%0Aand%20inference%20time%20%2810s%29%2C%20and%20demonstrates%20greater%20robustness%20%28overall%0Arobustness%20score%20of%200.79%29.%20Consequently%2C%20EfficientViT-M2%20consumes%2063.93%20%25%20less%0Apower%20than%20MobileViTV2%20%2879.23%20W%29%20and%2073.26%20%25%20less%20power%20than%20SwinTransformer%0A%28108.90%20W%29.%20This%20highlights%20its%20significant%20advantage%20in%20energy%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03901v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnboard%2520Satellite%2520Image%2520Classification%2520for%2520Earth%2520Observation%253A%2520A%250A%2520%2520Comparative%2520Study%2520of%2520ViT%2520Models%26entry.906535625%3DThanh-Dung%2520Le%2520and%2520Vu%2520Nguyen%2520Ha%2520and%2520Ti%2520Ti%2520Nguyen%2520and%2520Geoffrey%2520Eappen%2520and%2520Prabhu%2520Thiruvasagam%2520and%2520Hong-fu%2520Chou%2520and%2520Duc-Dung%2520Tran%2520and%2520Hung%2520Nguyen-Kha%2520and%2520Luis%2520M.%2520Garces-Socarras%2520and%2520Jorge%2520L.%2520Gonzalez-Rios%2520and%2520Juan%2520Carlos%2520Merlano-Duncan%2520and%2520Symeon%2520Chatzinotas%26entry.1292438233%3D%2520%2520This%2520study%2520focuses%2520on%2520identifying%2520the%2520most%2520effective%2520pre-trained%2520model%2520for%250Aland%2520use%2520classification%2520in%2520onboard%2520satellite%2520processing%252C%2520emphasizing%2520achieving%250Ahigh%2520accuracy%252C%2520computational%2520efficiency%252C%2520and%2520robustness%2520against%2520noisy%2520data%250Aconditions%2520commonly%2520encountered%2520during%2520satellite-based%2520inference.%2520Through%250Aextensive%2520experimentation%252C%2520we%2520compare%2520the%2520performance%2520of%2520traditional%2520CNN-based%252C%250AResNet-based%252C%2520and%2520various%2520pre-trained%2520vision%2520Transformer%2520models.%2520Our%2520findings%250Ademonstrate%2520that%2520pre-trained%2520Vision%2520Transformer%2520%2528ViT%2529%2520models%252C%2520particularly%250AMobileViTV2%2520and%2520EfficientViT-M2%252C%2520outperform%2520models%2520trained%2520from%2520scratch%2520in%250Aterms%2520of%2520accuracy%2520and%2520efficiency.%2520These%2520models%2520achieve%2520high%2520performance%2520with%250Areduced%2520computational%2520requirements%2520and%2520exhibit%2520greater%2520resilience%2520during%250Ainference%2520under%2520noisy%2520conditions.%2520While%2520MobileViTV2%2520has%2520excelled%2520on%2520clean%250Avalidation%2520data%252C%2520EfficientViT-M2%2520has%2520proved%2520more%2520robust%2520when%2520handling%2520noise%252C%250Amaking%2520it%2520the%2520most%2520suitable%2520model%2520for%2520onboard%2520satellite%2520EO%2520tasks.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520EfficientViT-M2%2520is%2520the%2520optimal%2520choice%2520for%250Areliable%2520and%2520efficient%2520RS-IC%2520in%2520satellite%2520operations%252C%2520achieving%252098.76%2520%2525%2520of%250Aaccuracy%252C%2520precision%252C%2520and%2520recall.%2520Precisely%252C%2520EfficientViT-M2%2520delivers%2520the%250Ahighest%2520performance%2520across%2520all%2520metrics%252C%2520excels%2520in%2520training%2520efficiency%2520%25281%252C000s%2529%250Aand%2520inference%2520time%2520%252810s%2529%252C%2520and%2520demonstrates%2520greater%2520robustness%2520%2528overall%250Arobustness%2520score%2520of%25200.79%2529.%2520Consequently%252C%2520EfficientViT-M2%2520consumes%252063.93%2520%2525%2520less%250Apower%2520than%2520MobileViTV2%2520%252879.23%2520W%2529%2520and%252073.26%2520%2525%2520less%2520power%2520than%2520SwinTransformer%250A%2528108.90%2520W%2529.%2520This%2520highlights%2520its%2520significant%2520advantage%2520in%2520energy%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03901v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Onboard%20Satellite%20Image%20Classification%20for%20Earth%20Observation%3A%20A%0A%20%20Comparative%20Study%20of%20ViT%20Models&entry.906535625=Thanh-Dung%20Le%20and%20Vu%20Nguyen%20Ha%20and%20Ti%20Ti%20Nguyen%20and%20Geoffrey%20Eappen%20and%20Prabhu%20Thiruvasagam%20and%20Hong-fu%20Chou%20and%20Duc-Dung%20Tran%20and%20Hung%20Nguyen-Kha%20and%20Luis%20M.%20Garces-Socarras%20and%20Jorge%20L.%20Gonzalez-Rios%20and%20Juan%20Carlos%20Merlano-Duncan%20and%20Symeon%20Chatzinotas&entry.1292438233=%20%20This%20study%20focuses%20on%20identifying%20the%20most%20effective%20pre-trained%20model%20for%0Aland%20use%20classification%20in%20onboard%20satellite%20processing%2C%20emphasizing%20achieving%0Ahigh%20accuracy%2C%20computational%20efficiency%2C%20and%20robustness%20against%20noisy%20data%0Aconditions%20commonly%20encountered%20during%20satellite-based%20inference.%20Through%0Aextensive%20experimentation%2C%20we%20compare%20the%20performance%20of%20traditional%20CNN-based%2C%0AResNet-based%2C%20and%20various%20pre-trained%20vision%20Transformer%20models.%20Our%20findings%0Ademonstrate%20that%20pre-trained%20Vision%20Transformer%20%28ViT%29%20models%2C%20particularly%0AMobileViTV2%20and%20EfficientViT-M2%2C%20outperform%20models%20trained%20from%20scratch%20in%0Aterms%20of%20accuracy%20and%20efficiency.%20These%20models%20achieve%20high%20performance%20with%0Areduced%20computational%20requirements%20and%20exhibit%20greater%20resilience%20during%0Ainference%20under%20noisy%20conditions.%20While%20MobileViTV2%20has%20excelled%20on%20clean%0Avalidation%20data%2C%20EfficientViT-M2%20has%20proved%20more%20robust%20when%20handling%20noise%2C%0Amaking%20it%20the%20most%20suitable%20model%20for%20onboard%20satellite%20EO%20tasks.%20Our%0Aexperimental%20results%20demonstrate%20that%20EfficientViT-M2%20is%20the%20optimal%20choice%20for%0Areliable%20and%20efficient%20RS-IC%20in%20satellite%20operations%2C%20achieving%2098.76%20%25%20of%0Aaccuracy%2C%20precision%2C%20and%20recall.%20Precisely%2C%20EfficientViT-M2%20delivers%20the%0Ahighest%20performance%20across%20all%20metrics%2C%20excels%20in%20training%20efficiency%20%281%2C000s%29%0Aand%20inference%20time%20%2810s%29%2C%20and%20demonstrates%20greater%20robustness%20%28overall%0Arobustness%20score%20of%200.79%29.%20Consequently%2C%20EfficientViT-M2%20consumes%2063.93%20%25%20less%0Apower%20than%20MobileViTV2%20%2879.23%20W%29%20and%2073.26%20%25%20less%20power%20than%20SwinTransformer%0A%28108.90%20W%29.%20This%20highlights%20its%20significant%20advantage%20in%20energy%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03901v3&entry.124074799=Read"},
{"title": "Embedded Safe Reactive Navigation for Multirotors Systems using Control\n  Barrier Functions", "author": "Nazar Misyats and Marvin Harms and Morten Nissov and Martin Jacquet and Kostas Alexis", "abstract": "  Aiming to promote the wide adoption of safety filters for autonomous aerial\nrobots, this paper presents a safe control architecture designed for seamless\nintegration into widely used open-source autopilots. Departing from methods\nthat require consistent localization and mapping, we formalize the obstacle\navoidance problem as a composite control barrier function constructed only from\nthe online onboard range measurements. The proposed framework acts as a safety\nfilter, modifying the acceleration references derived by the nominal\nposition/velocity control loops, and is integrated into the PX4 autopilot\nstack. Experimental studies using a small multirotor aerial robot demonstrate\nthe effectiveness and performance of the solution within dynamic maneuvering\nand unknown environments.\n", "link": "http://arxiv.org/abs/2504.15850v1", "date": "2025-04-22", "relevancy": 2.0059, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5199}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedded%20Safe%20Reactive%20Navigation%20for%20Multirotors%20Systems%20using%20Control%0A%20%20Barrier%20Functions&body=Title%3A%20Embedded%20Safe%20Reactive%20Navigation%20for%20Multirotors%20Systems%20using%20Control%0A%20%20Barrier%20Functions%0AAuthor%3A%20Nazar%20Misyats%20and%20Marvin%20Harms%20and%20Morten%20Nissov%20and%20Martin%20Jacquet%20and%20Kostas%20Alexis%0AAbstract%3A%20%20%20Aiming%20to%20promote%20the%20wide%20adoption%20of%20safety%20filters%20for%20autonomous%20aerial%0Arobots%2C%20this%20paper%20presents%20a%20safe%20control%20architecture%20designed%20for%20seamless%0Aintegration%20into%20widely%20used%20open-source%20autopilots.%20Departing%20from%20methods%0Athat%20require%20consistent%20localization%20and%20mapping%2C%20we%20formalize%20the%20obstacle%0Aavoidance%20problem%20as%20a%20composite%20control%20barrier%20function%20constructed%20only%20from%0Athe%20online%20onboard%20range%20measurements.%20The%20proposed%20framework%20acts%20as%20a%20safety%0Afilter%2C%20modifying%20the%20acceleration%20references%20derived%20by%20the%20nominal%0Aposition/velocity%20control%20loops%2C%20and%20is%20integrated%20into%20the%20PX4%20autopilot%0Astack.%20Experimental%20studies%20using%20a%20small%20multirotor%20aerial%20robot%20demonstrate%0Athe%20effectiveness%20and%20performance%20of%20the%20solution%20within%20dynamic%20maneuvering%0Aand%20unknown%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedded%2520Safe%2520Reactive%2520Navigation%2520for%2520Multirotors%2520Systems%2520using%2520Control%250A%2520%2520Barrier%2520Functions%26entry.906535625%3DNazar%2520Misyats%2520and%2520Marvin%2520Harms%2520and%2520Morten%2520Nissov%2520and%2520Martin%2520Jacquet%2520and%2520Kostas%2520Alexis%26entry.1292438233%3D%2520%2520Aiming%2520to%2520promote%2520the%2520wide%2520adoption%2520of%2520safety%2520filters%2520for%2520autonomous%2520aerial%250Arobots%252C%2520this%2520paper%2520presents%2520a%2520safe%2520control%2520architecture%2520designed%2520for%2520seamless%250Aintegration%2520into%2520widely%2520used%2520open-source%2520autopilots.%2520Departing%2520from%2520methods%250Athat%2520require%2520consistent%2520localization%2520and%2520mapping%252C%2520we%2520formalize%2520the%2520obstacle%250Aavoidance%2520problem%2520as%2520a%2520composite%2520control%2520barrier%2520function%2520constructed%2520only%2520from%250Athe%2520online%2520onboard%2520range%2520measurements.%2520The%2520proposed%2520framework%2520acts%2520as%2520a%2520safety%250Afilter%252C%2520modifying%2520the%2520acceleration%2520references%2520derived%2520by%2520the%2520nominal%250Aposition/velocity%2520control%2520loops%252C%2520and%2520is%2520integrated%2520into%2520the%2520PX4%2520autopilot%250Astack.%2520Experimental%2520studies%2520using%2520a%2520small%2520multirotor%2520aerial%2520robot%2520demonstrate%250Athe%2520effectiveness%2520and%2520performance%2520of%2520the%2520solution%2520within%2520dynamic%2520maneuvering%250Aand%2520unknown%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedded%20Safe%20Reactive%20Navigation%20for%20Multirotors%20Systems%20using%20Control%0A%20%20Barrier%20Functions&entry.906535625=Nazar%20Misyats%20and%20Marvin%20Harms%20and%20Morten%20Nissov%20and%20Martin%20Jacquet%20and%20Kostas%20Alexis&entry.1292438233=%20%20Aiming%20to%20promote%20the%20wide%20adoption%20of%20safety%20filters%20for%20autonomous%20aerial%0Arobots%2C%20this%20paper%20presents%20a%20safe%20control%20architecture%20designed%20for%20seamless%0Aintegration%20into%20widely%20used%20open-source%20autopilots.%20Departing%20from%20methods%0Athat%20require%20consistent%20localization%20and%20mapping%2C%20we%20formalize%20the%20obstacle%0Aavoidance%20problem%20as%20a%20composite%20control%20barrier%20function%20constructed%20only%20from%0Athe%20online%20onboard%20range%20measurements.%20The%20proposed%20framework%20acts%20as%20a%20safety%0Afilter%2C%20modifying%20the%20acceleration%20references%20derived%20by%20the%20nominal%0Aposition/velocity%20control%20loops%2C%20and%20is%20integrated%20into%20the%20PX4%20autopilot%0Astack.%20Experimental%20studies%20using%20a%20small%20multirotor%20aerial%20robot%20demonstrate%0Athe%20effectiveness%20and%20performance%20of%20the%20solution%20within%20dynamic%20maneuvering%0Aand%20unknown%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15850v1&entry.124074799=Read"},
{"title": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs\n  DeepSeek-V3", "author": "Ahmed R. Sadik and Siddhata Govind", "abstract": "  Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection\n", "link": "http://arxiv.org/abs/2504.16027v1", "date": "2025-04-22", "relevancy": 1.9998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20LLM%20for%20Code%20Smells%20Detection%3A%20OpenAI%20GPT-4.0%20vs%0A%20%20DeepSeek-V3&body=Title%3A%20Benchmarking%20LLM%20for%20Code%20Smells%20Detection%3A%20OpenAI%20GPT-4.0%20vs%0A%20%20DeepSeek-V3%0AAuthor%3A%20Ahmed%20R.%20Sadik%20and%20Siddhata%20Govind%0AAbstract%3A%20%20%20Determining%20the%20most%20effective%20Large%20Language%20Model%20for%20code%20smell%20detection%0Apresents%20a%20complex%20challenge.%20This%20study%20introduces%20a%20structured%20methodology%0Aand%20evaluation%20matrix%20to%20tackle%20this%20issue%2C%20leveraging%20a%20curated%20dataset%20of%0Acode%20samples%20consistently%20annotated%20with%20known%20smells.%20The%20dataset%20spans%20four%0Aprominent%20programming%20languages%20Java%2C%20Python%2C%20JavaScript%2C%20and%20C%2B%2B%3B%20allowing%20for%0Across%20language%20comparison.%20We%20benchmark%20two%20state%20of%20the%20art%20LLMs%2C%20OpenAI%20GPT%0A4.0%20and%20DeepSeek-V3%2C%20using%20precision%2C%20recall%2C%20and%20F1%20score%20as%20evaluation%0Ametrics.%20Our%20analysis%20covers%20three%20levels%20of%20detail%3A%20overall%20performance%2C%0Acategory%20level%20performance%2C%20and%20individual%20code%20smell%20type%20performance.%0AAdditionally%2C%20we%20explore%20cost%20effectiveness%20by%20comparing%20the%20token%20based%0Adetection%20approach%20of%20GPT%204.0%20with%20the%20pattern-matching%20techniques%20employed%20by%0ADeepSeek%20V3.%20The%20study%20also%20includes%20a%20cost%20analysis%20relative%20to%20traditional%0Astatic%20analysis%20tools%20such%20as%20SonarQube.%20The%20findings%20offer%20valuable%20guidance%0Afor%20practitioners%20in%20selecting%20an%20efficient%2C%20cost%20effective%20solution%20for%0Aautomated%20code%20smell%20detection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520LLM%2520for%2520Code%2520Smells%2520Detection%253A%2520OpenAI%2520GPT-4.0%2520vs%250A%2520%2520DeepSeek-V3%26entry.906535625%3DAhmed%2520R.%2520Sadik%2520and%2520Siddhata%2520Govind%26entry.1292438233%3D%2520%2520Determining%2520the%2520most%2520effective%2520Large%2520Language%2520Model%2520for%2520code%2520smell%2520detection%250Apresents%2520a%2520complex%2520challenge.%2520This%2520study%2520introduces%2520a%2520structured%2520methodology%250Aand%2520evaluation%2520matrix%2520to%2520tackle%2520this%2520issue%252C%2520leveraging%2520a%2520curated%2520dataset%2520of%250Acode%2520samples%2520consistently%2520annotated%2520with%2520known%2520smells.%2520The%2520dataset%2520spans%2520four%250Aprominent%2520programming%2520languages%2520Java%252C%2520Python%252C%2520JavaScript%252C%2520and%2520C%252B%252B%253B%2520allowing%2520for%250Across%2520language%2520comparison.%2520We%2520benchmark%2520two%2520state%2520of%2520the%2520art%2520LLMs%252C%2520OpenAI%2520GPT%250A4.0%2520and%2520DeepSeek-V3%252C%2520using%2520precision%252C%2520recall%252C%2520and%2520F1%2520score%2520as%2520evaluation%250Ametrics.%2520Our%2520analysis%2520covers%2520three%2520levels%2520of%2520detail%253A%2520overall%2520performance%252C%250Acategory%2520level%2520performance%252C%2520and%2520individual%2520code%2520smell%2520type%2520performance.%250AAdditionally%252C%2520we%2520explore%2520cost%2520effectiveness%2520by%2520comparing%2520the%2520token%2520based%250Adetection%2520approach%2520of%2520GPT%25204.0%2520with%2520the%2520pattern-matching%2520techniques%2520employed%2520by%250ADeepSeek%2520V3.%2520The%2520study%2520also%2520includes%2520a%2520cost%2520analysis%2520relative%2520to%2520traditional%250Astatic%2520analysis%2520tools%2520such%2520as%2520SonarQube.%2520The%2520findings%2520offer%2520valuable%2520guidance%250Afor%2520practitioners%2520in%2520selecting%2520an%2520efficient%252C%2520cost%2520effective%2520solution%2520for%250Aautomated%2520code%2520smell%2520detection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20LLM%20for%20Code%20Smells%20Detection%3A%20OpenAI%20GPT-4.0%20vs%0A%20%20DeepSeek-V3&entry.906535625=Ahmed%20R.%20Sadik%20and%20Siddhata%20Govind&entry.1292438233=%20%20Determining%20the%20most%20effective%20Large%20Language%20Model%20for%20code%20smell%20detection%0Apresents%20a%20complex%20challenge.%20This%20study%20introduces%20a%20structured%20methodology%0Aand%20evaluation%20matrix%20to%20tackle%20this%20issue%2C%20leveraging%20a%20curated%20dataset%20of%0Acode%20samples%20consistently%20annotated%20with%20known%20smells.%20The%20dataset%20spans%20four%0Aprominent%20programming%20languages%20Java%2C%20Python%2C%20JavaScript%2C%20and%20C%2B%2B%3B%20allowing%20for%0Across%20language%20comparison.%20We%20benchmark%20two%20state%20of%20the%20art%20LLMs%2C%20OpenAI%20GPT%0A4.0%20and%20DeepSeek-V3%2C%20using%20precision%2C%20recall%2C%20and%20F1%20score%20as%20evaluation%0Ametrics.%20Our%20analysis%20covers%20three%20levels%20of%20detail%3A%20overall%20performance%2C%0Acategory%20level%20performance%2C%20and%20individual%20code%20smell%20type%20performance.%0AAdditionally%2C%20we%20explore%20cost%20effectiveness%20by%20comparing%20the%20token%20based%0Adetection%20approach%20of%20GPT%204.0%20with%20the%20pattern-matching%20techniques%20employed%20by%0ADeepSeek%20V3.%20The%20study%20also%20includes%20a%20cost%20analysis%20relative%20to%20traditional%0Astatic%20analysis%20tools%20such%20as%20SonarQube.%20The%20findings%20offer%20valuable%20guidance%0Afor%20practitioners%20in%20selecting%20an%20efficient%2C%20cost%20effective%20solution%20for%0Aautomated%20code%20smell%20detection%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16027v1&entry.124074799=Read"},
{"title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight\n  Language Models", "author": "Shang Wang", "abstract": "  The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation.\n", "link": "http://arxiv.org/abs/2504.15983v1", "date": "2025-04-22", "relevancy": 1.9992, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20W-PCA%20Based%20Gradient-Free%20Proxy%20for%20Efficient%20Search%20of%20Lightweight%0A%20%20Language%20Models&body=Title%3A%20W-PCA%20Based%20Gradient-Free%20Proxy%20for%20Efficient%20Search%20of%20Lightweight%0A%20%20Language%20Models%0AAuthor%3A%20Shang%20Wang%0AAbstract%3A%20%20%20The%20demand%20for%20efficient%20natural%20language%20processing%20%28NLP%29%20systems%20has%20led%20to%0Athe%20development%20of%20lightweight%20language%20models.%20Previous%20work%20in%20this%20area%20has%0Aprimarily%20focused%20on%20manual%20design%20or%20training-based%20neural%20architecture%20search%0A%28NAS%29%20methods.%20Recently%2C%20zero-shot%20NAS%20methods%20have%20been%20proposed%20for%0Aevaluating%20language%20models%20without%20the%20need%20for%20training.%20However%2C%20prevailing%0Aapproaches%20to%20zero-shot%20NAS%20often%20face%20challenges%20such%20as%20biased%20evaluation%0Ametrics%20and%20computational%20inefficiencies.%20In%20this%20paper%2C%20we%20introduce%0Aweight-weighted%20PCA%20%28W-PCA%29%2C%20a%20novel%20zero-shot%20NAS%20method%20specifically%20tailored%0Afor%20lightweight%20language%20models.%20Our%20approach%20utilizes%20two%20evaluation%20proxies%3A%0Athe%20parameter%20count%20and%20the%20number%20of%20principal%20components%20with%20cumulative%0Acontribution%20exceeding%20%24%5Ceta%24%20in%20the%20feed-forward%20neural%20%28FFN%29%20layer.%0AAdditionally%2C%20by%20eliminating%20the%20need%20for%20gradient%20computations%2C%20we%20optimize%0Athe%20evaluation%20time%2C%20thus%20enhancing%20the%20efficiency%20of%20designing%20and%20evaluating%0Alightweight%20language%20models.%20We%20conduct%20a%20comparative%20analysis%20on%20the%20GLUE%20and%0ASQuAD%20datasets%20to%20evaluate%20our%20approach.%20The%20results%20demonstrate%20that%20our%0Amethod%20significantly%20reduces%20training%20time%20compared%20to%20one-shot%20NAS%20methods%20and%0Aachieves%20higher%20scores%20in%20the%20testing%20phase%20compared%20to%20previous%0Astate-of-the-art%20training-based%20methods.%20Furthermore%2C%20we%20perform%20ranking%0Aevaluations%20on%20a%20dataset%20sampled%20from%20the%20FlexiBERT%20search%20space.%20Our%20approach%0Aexhibits%20superior%20ranking%20correlation%20and%20further%20reduces%20solving%20time%20compared%0Ato%20other%20zero-shot%20NAS%20methods%20that%20require%20gradient%20computation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DW-PCA%2520Based%2520Gradient-Free%2520Proxy%2520for%2520Efficient%2520Search%2520of%2520Lightweight%250A%2520%2520Language%2520Models%26entry.906535625%3DShang%2520Wang%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520efficient%2520natural%2520language%2520processing%2520%2528NLP%2529%2520systems%2520has%2520led%2520to%250Athe%2520development%2520of%2520lightweight%2520language%2520models.%2520Previous%2520work%2520in%2520this%2520area%2520has%250Aprimarily%2520focused%2520on%2520manual%2520design%2520or%2520training-based%2520neural%2520architecture%2520search%250A%2528NAS%2529%2520methods.%2520Recently%252C%2520zero-shot%2520NAS%2520methods%2520have%2520been%2520proposed%2520for%250Aevaluating%2520language%2520models%2520without%2520the%2520need%2520for%2520training.%2520However%252C%2520prevailing%250Aapproaches%2520to%2520zero-shot%2520NAS%2520often%2520face%2520challenges%2520such%2520as%2520biased%2520evaluation%250Ametrics%2520and%2520computational%2520inefficiencies.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Aweight-weighted%2520PCA%2520%2528W-PCA%2529%252C%2520a%2520novel%2520zero-shot%2520NAS%2520method%2520specifically%2520tailored%250Afor%2520lightweight%2520language%2520models.%2520Our%2520approach%2520utilizes%2520two%2520evaluation%2520proxies%253A%250Athe%2520parameter%2520count%2520and%2520the%2520number%2520of%2520principal%2520components%2520with%2520cumulative%250Acontribution%2520exceeding%2520%2524%255Ceta%2524%2520in%2520the%2520feed-forward%2520neural%2520%2528FFN%2529%2520layer.%250AAdditionally%252C%2520by%2520eliminating%2520the%2520need%2520for%2520gradient%2520computations%252C%2520we%2520optimize%250Athe%2520evaluation%2520time%252C%2520thus%2520enhancing%2520the%2520efficiency%2520of%2520designing%2520and%2520evaluating%250Alightweight%2520language%2520models.%2520We%2520conduct%2520a%2520comparative%2520analysis%2520on%2520the%2520GLUE%2520and%250ASQuAD%2520datasets%2520to%2520evaluate%2520our%2520approach.%2520The%2520results%2520demonstrate%2520that%2520our%250Amethod%2520significantly%2520reduces%2520training%2520time%2520compared%2520to%2520one-shot%2520NAS%2520methods%2520and%250Aachieves%2520higher%2520scores%2520in%2520the%2520testing%2520phase%2520compared%2520to%2520previous%250Astate-of-the-art%2520training-based%2520methods.%2520Furthermore%252C%2520we%2520perform%2520ranking%250Aevaluations%2520on%2520a%2520dataset%2520sampled%2520from%2520the%2520FlexiBERT%2520search%2520space.%2520Our%2520approach%250Aexhibits%2520superior%2520ranking%2520correlation%2520and%2520further%2520reduces%2520solving%2520time%2520compared%250Ato%2520other%2520zero-shot%2520NAS%2520methods%2520that%2520require%2520gradient%2520computation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=W-PCA%20Based%20Gradient-Free%20Proxy%20for%20Efficient%20Search%20of%20Lightweight%0A%20%20Language%20Models&entry.906535625=Shang%20Wang&entry.1292438233=%20%20The%20demand%20for%20efficient%20natural%20language%20processing%20%28NLP%29%20systems%20has%20led%20to%0Athe%20development%20of%20lightweight%20language%20models.%20Previous%20work%20in%20this%20area%20has%0Aprimarily%20focused%20on%20manual%20design%20or%20training-based%20neural%20architecture%20search%0A%28NAS%29%20methods.%20Recently%2C%20zero-shot%20NAS%20methods%20have%20been%20proposed%20for%0Aevaluating%20language%20models%20without%20the%20need%20for%20training.%20However%2C%20prevailing%0Aapproaches%20to%20zero-shot%20NAS%20often%20face%20challenges%20such%20as%20biased%20evaluation%0Ametrics%20and%20computational%20inefficiencies.%20In%20this%20paper%2C%20we%20introduce%0Aweight-weighted%20PCA%20%28W-PCA%29%2C%20a%20novel%20zero-shot%20NAS%20method%20specifically%20tailored%0Afor%20lightweight%20language%20models.%20Our%20approach%20utilizes%20two%20evaluation%20proxies%3A%0Athe%20parameter%20count%20and%20the%20number%20of%20principal%20components%20with%20cumulative%0Acontribution%20exceeding%20%24%5Ceta%24%20in%20the%20feed-forward%20neural%20%28FFN%29%20layer.%0AAdditionally%2C%20by%20eliminating%20the%20need%20for%20gradient%20computations%2C%20we%20optimize%0Athe%20evaluation%20time%2C%20thus%20enhancing%20the%20efficiency%20of%20designing%20and%20evaluating%0Alightweight%20language%20models.%20We%20conduct%20a%20comparative%20analysis%20on%20the%20GLUE%20and%0ASQuAD%20datasets%20to%20evaluate%20our%20approach.%20The%20results%20demonstrate%20that%20our%0Amethod%20significantly%20reduces%20training%20time%20compared%20to%20one-shot%20NAS%20methods%20and%0Aachieves%20higher%20scores%20in%20the%20testing%20phase%20compared%20to%20previous%0Astate-of-the-art%20training-based%20methods.%20Furthermore%2C%20we%20perform%20ranking%0Aevaluations%20on%20a%20dataset%20sampled%20from%20the%20FlexiBERT%20search%20space.%20Our%20approach%0Aexhibits%20superior%20ranking%20correlation%20and%20further%20reduces%20solving%20time%20compared%0Ato%20other%20zero-shot%20NAS%20methods%20that%20require%20gradient%20computation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15983v1&entry.124074799=Read"},
{"title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models", "author": "Junfeng Fang and Houcheng Jiang and Kun Wang and Yunshan Ma and Shi Jie and Xiang Wang and Xiangnan He and Tat-seng Chua", "abstract": "  Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.7%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.\n", "link": "http://arxiv.org/abs/2410.02355v4", "date": "2025-04-22", "relevancy": 1.9976, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaEdit%3A%20Null-Space%20Constrained%20Knowledge%20Editing%20for%20Language%20Models&body=Title%3A%20AlphaEdit%3A%20Null-Space%20Constrained%20Knowledge%20Editing%20for%20Language%20Models%0AAuthor%3A%20Junfeng%20Fang%20and%20Houcheng%20Jiang%20and%20Kun%20Wang%20and%20Yunshan%20Ma%20and%20Shi%20Jie%20and%20Xiang%20Wang%20and%20Xiangnan%20He%20and%20Tat-seng%20Chua%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20exhibit%20hallucinations%20due%20to%20incorrect%20or%0Aoutdated%20knowledge.%20Hence%2C%20model%20editing%20methods%20have%20emerged%20to%20enable%0Atargeted%20knowledge%20updates.%20To%20achieve%20this%2C%20a%20prevailing%20paradigm%20is%20the%0Alocating-then-editing%20approach%2C%20which%20first%20locates%20influential%20parameters%20and%0Athen%20edits%20them%20by%20introducing%20a%20perturbation.%20While%20effective%2C%20current%20studies%0Ahave%20demonstrated%20that%20this%20perturbation%20inevitably%20disrupt%20the%20originally%0Apreserved%20knowledge%20within%20LLMs%2C%20especially%20in%20sequential%20editing%20scenarios.%20To%0Aaddress%20this%2C%20we%20introduce%20AlphaEdit%2C%20a%20novel%20solution%20that%20projects%0Aperturbation%20onto%20the%20null%20space%20of%20the%20preserved%20knowledge%20before%20applying%20it%0Ato%20the%20parameters.%20We%20theoretically%20prove%20that%20this%20projection%20ensures%20the%0Aoutput%20of%20post-edited%20LLMs%20remains%20unchanged%20when%20queried%20about%20the%20preserved%0Aknowledge%2C%20thereby%20mitigating%20the%20issue%20of%20disruption.%20Extensive%20experiments%20on%0Avarious%20LLMs%2C%20including%20LLaMA3%2C%20GPT2-XL%2C%20and%20GPT-J%2C%20show%20that%20AlphaEdit%20boosts%0Athe%20performance%20of%20most%20locating-then-editing%20methods%20by%20an%20average%20of%2036.7%25%0Awith%20a%20single%20line%20of%20additional%20code%20for%20projection%20solely.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/jianghoucheng/AlphaEdit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02355v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaEdit%253A%2520Null-Space%2520Constrained%2520Knowledge%2520Editing%2520for%2520Language%2520Models%26entry.906535625%3DJunfeng%2520Fang%2520and%2520Houcheng%2520Jiang%2520and%2520Kun%2520Wang%2520and%2520Yunshan%2520Ma%2520and%2520Shi%2520Jie%2520and%2520Xiang%2520Wang%2520and%2520Xiangnan%2520He%2520and%2520Tat-seng%2520Chua%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520exhibit%2520hallucinations%2520due%2520to%2520incorrect%2520or%250Aoutdated%2520knowledge.%2520Hence%252C%2520model%2520editing%2520methods%2520have%2520emerged%2520to%2520enable%250Atargeted%2520knowledge%2520updates.%2520To%2520achieve%2520this%252C%2520a%2520prevailing%2520paradigm%2520is%2520the%250Alocating-then-editing%2520approach%252C%2520which%2520first%2520locates%2520influential%2520parameters%2520and%250Athen%2520edits%2520them%2520by%2520introducing%2520a%2520perturbation.%2520While%2520effective%252C%2520current%2520studies%250Ahave%2520demonstrated%2520that%2520this%2520perturbation%2520inevitably%2520disrupt%2520the%2520originally%250Apreserved%2520knowledge%2520within%2520LLMs%252C%2520especially%2520in%2520sequential%2520editing%2520scenarios.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520AlphaEdit%252C%2520a%2520novel%2520solution%2520that%2520projects%250Aperturbation%2520onto%2520the%2520null%2520space%2520of%2520the%2520preserved%2520knowledge%2520before%2520applying%2520it%250Ato%2520the%2520parameters.%2520We%2520theoretically%2520prove%2520that%2520this%2520projection%2520ensures%2520the%250Aoutput%2520of%2520post-edited%2520LLMs%2520remains%2520unchanged%2520when%2520queried%2520about%2520the%2520preserved%250Aknowledge%252C%2520thereby%2520mitigating%2520the%2520issue%2520of%2520disruption.%2520Extensive%2520experiments%2520on%250Avarious%2520LLMs%252C%2520including%2520LLaMA3%252C%2520GPT2-XL%252C%2520and%2520GPT-J%252C%2520show%2520that%2520AlphaEdit%2520boosts%250Athe%2520performance%2520of%2520most%2520locating-then-editing%2520methods%2520by%2520an%2520average%2520of%252036.7%2525%250Awith%2520a%2520single%2520line%2520of%2520additional%2520code%2520for%2520projection%2520solely.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/jianghoucheng/AlphaEdit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02355v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaEdit%3A%20Null-Space%20Constrained%20Knowledge%20Editing%20for%20Language%20Models&entry.906535625=Junfeng%20Fang%20and%20Houcheng%20Jiang%20and%20Kun%20Wang%20and%20Yunshan%20Ma%20and%20Shi%20Jie%20and%20Xiang%20Wang%20and%20Xiangnan%20He%20and%20Tat-seng%20Chua&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20exhibit%20hallucinations%20due%20to%20incorrect%20or%0Aoutdated%20knowledge.%20Hence%2C%20model%20editing%20methods%20have%20emerged%20to%20enable%0Atargeted%20knowledge%20updates.%20To%20achieve%20this%2C%20a%20prevailing%20paradigm%20is%20the%0Alocating-then-editing%20approach%2C%20which%20first%20locates%20influential%20parameters%20and%0Athen%20edits%20them%20by%20introducing%20a%20perturbation.%20While%20effective%2C%20current%20studies%0Ahave%20demonstrated%20that%20this%20perturbation%20inevitably%20disrupt%20the%20originally%0Apreserved%20knowledge%20within%20LLMs%2C%20especially%20in%20sequential%20editing%20scenarios.%20To%0Aaddress%20this%2C%20we%20introduce%20AlphaEdit%2C%20a%20novel%20solution%20that%20projects%0Aperturbation%20onto%20the%20null%20space%20of%20the%20preserved%20knowledge%20before%20applying%20it%0Ato%20the%20parameters.%20We%20theoretically%20prove%20that%20this%20projection%20ensures%20the%0Aoutput%20of%20post-edited%20LLMs%20remains%20unchanged%20when%20queried%20about%20the%20preserved%0Aknowledge%2C%20thereby%20mitigating%20the%20issue%20of%20disruption.%20Extensive%20experiments%20on%0Avarious%20LLMs%2C%20including%20LLaMA3%2C%20GPT2-XL%2C%20and%20GPT-J%2C%20show%20that%20AlphaEdit%20boosts%0Athe%20performance%20of%20most%20locating-then-editing%20methods%20by%20an%20average%20of%2036.7%25%0Awith%20a%20single%20line%20of%20additional%20code%20for%20projection%20solely.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/jianghoucheng/AlphaEdit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02355v4&entry.124074799=Read"},
{"title": "Is Large-Scale Pretraining the Secret to Good Domain Generalization?", "author": "Piotr Teterwak and Kuniaki Saito and Theodoros Tsiligkaridis and Bryan A. Plummer and Kate Saenko", "abstract": "  Multi-Source Domain Generalization (DG) is the task of training on multiple\nsource domains and achieving high classification performance on unseen target\ndomains. Recent methods combine robust features from web-scale pretrained\nbackbones with new features learned from source data, and this has dramatically\nimproved benchmark results. However, it remains unclear if DG finetuning\nmethods are becoming better over time, or if improved benchmark performance is\nsimply an artifact of stronger pre-training. Prior studies have shown that\nperceptual similarity to pre-training data correlates with zero-shot\nperformance, but we find the effect limited in the DG setting. Instead, we\nposit that having perceptually similar data in pretraining is not enough; and\nthat it is how well these data were learned that determines performance. This\nleads us to introduce the Alignment Hypothesis, which states that the final DG\nperformance will be high if and only if alignment of image and class label text\nembeddings is high. Our experiments confirm the Alignment Hypothesis is true,\nand we use it as an analysis tool of existing DG methods evaluated on DomainBed\ndatasets by splitting evaluation data into In-pretraining (IP) and\nOut-of-pretraining (OOP). We show that all evaluated DG methods struggle on\nDomainBed-OOP, while recent methods excel on DomainBed-IP. Put together, our\nfindings highlight the need for DG methods which can generalize beyond\npretraining alignment.\n", "link": "http://arxiv.org/abs/2412.02856v3", "date": "2025-04-22", "relevancy": 1.9959, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5104}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4938}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Large-Scale%20Pretraining%20the%20Secret%20to%20Good%20Domain%20Generalization%3F&body=Title%3A%20Is%20Large-Scale%20Pretraining%20the%20Secret%20to%20Good%20Domain%20Generalization%3F%0AAuthor%3A%20Piotr%20Teterwak%20and%20Kuniaki%20Saito%20and%20Theodoros%20Tsiligkaridis%20and%20Bryan%20A.%20Plummer%20and%20Kate%20Saenko%0AAbstract%3A%20%20%20Multi-Source%20Domain%20Generalization%20%28DG%29%20is%20the%20task%20of%20training%20on%20multiple%0Asource%20domains%20and%20achieving%20high%20classification%20performance%20on%20unseen%20target%0Adomains.%20Recent%20methods%20combine%20robust%20features%20from%20web-scale%20pretrained%0Abackbones%20with%20new%20features%20learned%20from%20source%20data%2C%20and%20this%20has%20dramatically%0Aimproved%20benchmark%20results.%20However%2C%20it%20remains%20unclear%20if%20DG%20finetuning%0Amethods%20are%20becoming%20better%20over%20time%2C%20or%20if%20improved%20benchmark%20performance%20is%0Asimply%20an%20artifact%20of%20stronger%20pre-training.%20Prior%20studies%20have%20shown%20that%0Aperceptual%20similarity%20to%20pre-training%20data%20correlates%20with%20zero-shot%0Aperformance%2C%20but%20we%20find%20the%20effect%20limited%20in%20the%20DG%20setting.%20Instead%2C%20we%0Aposit%20that%20having%20perceptually%20similar%20data%20in%20pretraining%20is%20not%20enough%3B%20and%0Athat%20it%20is%20how%20well%20these%20data%20were%20learned%20that%20determines%20performance.%20This%0Aleads%20us%20to%20introduce%20the%20Alignment%20Hypothesis%2C%20which%20states%20that%20the%20final%20DG%0Aperformance%20will%20be%20high%20if%20and%20only%20if%20alignment%20of%20image%20and%20class%20label%20text%0Aembeddings%20is%20high.%20Our%20experiments%20confirm%20the%20Alignment%20Hypothesis%20is%20true%2C%0Aand%20we%20use%20it%20as%20an%20analysis%20tool%20of%20existing%20DG%20methods%20evaluated%20on%20DomainBed%0Adatasets%20by%20splitting%20evaluation%20data%20into%20In-pretraining%20%28IP%29%20and%0AOut-of-pretraining%20%28OOP%29.%20We%20show%20that%20all%20evaluated%20DG%20methods%20struggle%20on%0ADomainBed-OOP%2C%20while%20recent%20methods%20excel%20on%20DomainBed-IP.%20Put%20together%2C%20our%0Afindings%20highlight%20the%20need%20for%20DG%20methods%20which%20can%20generalize%20beyond%0Apretraining%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02856v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Large-Scale%2520Pretraining%2520the%2520Secret%2520to%2520Good%2520Domain%2520Generalization%253F%26entry.906535625%3DPiotr%2520Teterwak%2520and%2520Kuniaki%2520Saito%2520and%2520Theodoros%2520Tsiligkaridis%2520and%2520Bryan%2520A.%2520Plummer%2520and%2520Kate%2520Saenko%26entry.1292438233%3D%2520%2520Multi-Source%2520Domain%2520Generalization%2520%2528DG%2529%2520is%2520the%2520task%2520of%2520training%2520on%2520multiple%250Asource%2520domains%2520and%2520achieving%2520high%2520classification%2520performance%2520on%2520unseen%2520target%250Adomains.%2520Recent%2520methods%2520combine%2520robust%2520features%2520from%2520web-scale%2520pretrained%250Abackbones%2520with%2520new%2520features%2520learned%2520from%2520source%2520data%252C%2520and%2520this%2520has%2520dramatically%250Aimproved%2520benchmark%2520results.%2520However%252C%2520it%2520remains%2520unclear%2520if%2520DG%2520finetuning%250Amethods%2520are%2520becoming%2520better%2520over%2520time%252C%2520or%2520if%2520improved%2520benchmark%2520performance%2520is%250Asimply%2520an%2520artifact%2520of%2520stronger%2520pre-training.%2520Prior%2520studies%2520have%2520shown%2520that%250Aperceptual%2520similarity%2520to%2520pre-training%2520data%2520correlates%2520with%2520zero-shot%250Aperformance%252C%2520but%2520we%2520find%2520the%2520effect%2520limited%2520in%2520the%2520DG%2520setting.%2520Instead%252C%2520we%250Aposit%2520that%2520having%2520perceptually%2520similar%2520data%2520in%2520pretraining%2520is%2520not%2520enough%253B%2520and%250Athat%2520it%2520is%2520how%2520well%2520these%2520data%2520were%2520learned%2520that%2520determines%2520performance.%2520This%250Aleads%2520us%2520to%2520introduce%2520the%2520Alignment%2520Hypothesis%252C%2520which%2520states%2520that%2520the%2520final%2520DG%250Aperformance%2520will%2520be%2520high%2520if%2520and%2520only%2520if%2520alignment%2520of%2520image%2520and%2520class%2520label%2520text%250Aembeddings%2520is%2520high.%2520Our%2520experiments%2520confirm%2520the%2520Alignment%2520Hypothesis%2520is%2520true%252C%250Aand%2520we%2520use%2520it%2520as%2520an%2520analysis%2520tool%2520of%2520existing%2520DG%2520methods%2520evaluated%2520on%2520DomainBed%250Adatasets%2520by%2520splitting%2520evaluation%2520data%2520into%2520In-pretraining%2520%2528IP%2529%2520and%250AOut-of-pretraining%2520%2528OOP%2529.%2520We%2520show%2520that%2520all%2520evaluated%2520DG%2520methods%2520struggle%2520on%250ADomainBed-OOP%252C%2520while%2520recent%2520methods%2520excel%2520on%2520DomainBed-IP.%2520Put%2520together%252C%2520our%250Afindings%2520highlight%2520the%2520need%2520for%2520DG%2520methods%2520which%2520can%2520generalize%2520beyond%250Apretraining%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02856v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Large-Scale%20Pretraining%20the%20Secret%20to%20Good%20Domain%20Generalization%3F&entry.906535625=Piotr%20Teterwak%20and%20Kuniaki%20Saito%20and%20Theodoros%20Tsiligkaridis%20and%20Bryan%20A.%20Plummer%20and%20Kate%20Saenko&entry.1292438233=%20%20Multi-Source%20Domain%20Generalization%20%28DG%29%20is%20the%20task%20of%20training%20on%20multiple%0Asource%20domains%20and%20achieving%20high%20classification%20performance%20on%20unseen%20target%0Adomains.%20Recent%20methods%20combine%20robust%20features%20from%20web-scale%20pretrained%0Abackbones%20with%20new%20features%20learned%20from%20source%20data%2C%20and%20this%20has%20dramatically%0Aimproved%20benchmark%20results.%20However%2C%20it%20remains%20unclear%20if%20DG%20finetuning%0Amethods%20are%20becoming%20better%20over%20time%2C%20or%20if%20improved%20benchmark%20performance%20is%0Asimply%20an%20artifact%20of%20stronger%20pre-training.%20Prior%20studies%20have%20shown%20that%0Aperceptual%20similarity%20to%20pre-training%20data%20correlates%20with%20zero-shot%0Aperformance%2C%20but%20we%20find%20the%20effect%20limited%20in%20the%20DG%20setting.%20Instead%2C%20we%0Aposit%20that%20having%20perceptually%20similar%20data%20in%20pretraining%20is%20not%20enough%3B%20and%0Athat%20it%20is%20how%20well%20these%20data%20were%20learned%20that%20determines%20performance.%20This%0Aleads%20us%20to%20introduce%20the%20Alignment%20Hypothesis%2C%20which%20states%20that%20the%20final%20DG%0Aperformance%20will%20be%20high%20if%20and%20only%20if%20alignment%20of%20image%20and%20class%20label%20text%0Aembeddings%20is%20high.%20Our%20experiments%20confirm%20the%20Alignment%20Hypothesis%20is%20true%2C%0Aand%20we%20use%20it%20as%20an%20analysis%20tool%20of%20existing%20DG%20methods%20evaluated%20on%20DomainBed%0Adatasets%20by%20splitting%20evaluation%20data%20into%20In-pretraining%20%28IP%29%20and%0AOut-of-pretraining%20%28OOP%29.%20We%20show%20that%20all%20evaluated%20DG%20methods%20struggle%20on%0ADomainBed-OOP%2C%20while%20recent%20methods%20excel%20on%20DomainBed-IP.%20Put%20together%2C%20our%0Afindings%20highlight%20the%20need%20for%20DG%20methods%20which%20can%20generalize%20beyond%0Apretraining%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02856v3&entry.124074799=Read"},
{"title": "When resampling/reweighting improves feature learning in imbalanced\n  classification?: A toy-model study", "author": "Tomoyuki Obuchi and Toshiyuki Tanaka", "abstract": "  A toy model of binary classification is studied with the aim of clarifying\nthe class-wise resampling/reweighting effect on the feature learning\nperformance under the presence of class imbalance. In the analysis, a\nhigh-dimensional limit of the input space is taken while keeping the ratio of\nthe dataset size against the input dimension finite and the non-rigorous\nreplica method from statistical mechanics is employed. The result shows that\nthere exists a case in which the no resampling/reweighting situation gives the\nbest feature learning performance irrespectively of the choice of losses or\nclassifiers, supporting recent findings in Cao et al. (2019); Kang et al.\n(2019). It is also revealed that the key of the result is the symmetry of the\nloss and the problem setting. Inspired by this, we propose a further simplified\nmodel exhibiting the same property in the multiclass setting. These clarify\nwhen the class-wise resampling/reweighting becomes effective in imbalanced\nclassification.\n", "link": "http://arxiv.org/abs/2409.05598v2", "date": "2025-04-22", "relevancy": 1.9958, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5487}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4652}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20resampling/reweighting%20improves%20feature%20learning%20in%20imbalanced%0A%20%20classification%3F%3A%20A%20toy-model%20study&body=Title%3A%20When%20resampling/reweighting%20improves%20feature%20learning%20in%20imbalanced%0A%20%20classification%3F%3A%20A%20toy-model%20study%0AAuthor%3A%20Tomoyuki%20Obuchi%20and%20Toshiyuki%20Tanaka%0AAbstract%3A%20%20%20A%20toy%20model%20of%20binary%20classification%20is%20studied%20with%20the%20aim%20of%20clarifying%0Athe%20class-wise%20resampling/reweighting%20effect%20on%20the%20feature%20learning%0Aperformance%20under%20the%20presence%20of%20class%20imbalance.%20In%20the%20analysis%2C%20a%0Ahigh-dimensional%20limit%20of%20the%20input%20space%20is%20taken%20while%20keeping%20the%20ratio%20of%0Athe%20dataset%20size%20against%20the%20input%20dimension%20finite%20and%20the%20non-rigorous%0Areplica%20method%20from%20statistical%20mechanics%20is%20employed.%20The%20result%20shows%20that%0Athere%20exists%20a%20case%20in%20which%20the%20no%20resampling/reweighting%20situation%20gives%20the%0Abest%20feature%20learning%20performance%20irrespectively%20of%20the%20choice%20of%20losses%20or%0Aclassifiers%2C%20supporting%20recent%20findings%20in%20Cao%20et%20al.%20%282019%29%3B%20Kang%20et%20al.%0A%282019%29.%20It%20is%20also%20revealed%20that%20the%20key%20of%20the%20result%20is%20the%20symmetry%20of%20the%0Aloss%20and%20the%20problem%20setting.%20Inspired%20by%20this%2C%20we%20propose%20a%20further%20simplified%0Amodel%20exhibiting%20the%20same%20property%20in%20the%20multiclass%20setting.%20These%20clarify%0Awhen%20the%20class-wise%20resampling/reweighting%20becomes%20effective%20in%20imbalanced%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05598v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520resampling/reweighting%2520improves%2520feature%2520learning%2520in%2520imbalanced%250A%2520%2520classification%253F%253A%2520A%2520toy-model%2520study%26entry.906535625%3DTomoyuki%2520Obuchi%2520and%2520Toshiyuki%2520Tanaka%26entry.1292438233%3D%2520%2520A%2520toy%2520model%2520of%2520binary%2520classification%2520is%2520studied%2520with%2520the%2520aim%2520of%2520clarifying%250Athe%2520class-wise%2520resampling/reweighting%2520effect%2520on%2520the%2520feature%2520learning%250Aperformance%2520under%2520the%2520presence%2520of%2520class%2520imbalance.%2520In%2520the%2520analysis%252C%2520a%250Ahigh-dimensional%2520limit%2520of%2520the%2520input%2520space%2520is%2520taken%2520while%2520keeping%2520the%2520ratio%2520of%250Athe%2520dataset%2520size%2520against%2520the%2520input%2520dimension%2520finite%2520and%2520the%2520non-rigorous%250Areplica%2520method%2520from%2520statistical%2520mechanics%2520is%2520employed.%2520The%2520result%2520shows%2520that%250Athere%2520exists%2520a%2520case%2520in%2520which%2520the%2520no%2520resampling/reweighting%2520situation%2520gives%2520the%250Abest%2520feature%2520learning%2520performance%2520irrespectively%2520of%2520the%2520choice%2520of%2520losses%2520or%250Aclassifiers%252C%2520supporting%2520recent%2520findings%2520in%2520Cao%2520et%2520al.%2520%25282019%2529%253B%2520Kang%2520et%2520al.%250A%25282019%2529.%2520It%2520is%2520also%2520revealed%2520that%2520the%2520key%2520of%2520the%2520result%2520is%2520the%2520symmetry%2520of%2520the%250Aloss%2520and%2520the%2520problem%2520setting.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520further%2520simplified%250Amodel%2520exhibiting%2520the%2520same%2520property%2520in%2520the%2520multiclass%2520setting.%2520These%2520clarify%250Awhen%2520the%2520class-wise%2520resampling/reweighting%2520becomes%2520effective%2520in%2520imbalanced%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05598v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20resampling/reweighting%20improves%20feature%20learning%20in%20imbalanced%0A%20%20classification%3F%3A%20A%20toy-model%20study&entry.906535625=Tomoyuki%20Obuchi%20and%20Toshiyuki%20Tanaka&entry.1292438233=%20%20A%20toy%20model%20of%20binary%20classification%20is%20studied%20with%20the%20aim%20of%20clarifying%0Athe%20class-wise%20resampling/reweighting%20effect%20on%20the%20feature%20learning%0Aperformance%20under%20the%20presence%20of%20class%20imbalance.%20In%20the%20analysis%2C%20a%0Ahigh-dimensional%20limit%20of%20the%20input%20space%20is%20taken%20while%20keeping%20the%20ratio%20of%0Athe%20dataset%20size%20against%20the%20input%20dimension%20finite%20and%20the%20non-rigorous%0Areplica%20method%20from%20statistical%20mechanics%20is%20employed.%20The%20result%20shows%20that%0Athere%20exists%20a%20case%20in%20which%20the%20no%20resampling/reweighting%20situation%20gives%20the%0Abest%20feature%20learning%20performance%20irrespectively%20of%20the%20choice%20of%20losses%20or%0Aclassifiers%2C%20supporting%20recent%20findings%20in%20Cao%20et%20al.%20%282019%29%3B%20Kang%20et%20al.%0A%282019%29.%20It%20is%20also%20revealed%20that%20the%20key%20of%20the%20result%20is%20the%20symmetry%20of%20the%0Aloss%20and%20the%20problem%20setting.%20Inspired%20by%20this%2C%20we%20propose%20a%20further%20simplified%0Amodel%20exhibiting%20the%20same%20property%20in%20the%20multiclass%20setting.%20These%20clarify%0Awhen%20the%20class-wise%20resampling/reweighting%20becomes%20effective%20in%20imbalanced%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05598v2&entry.124074799=Read"},
{"title": "FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale\n  Neural Networks through Federated Learning", "author": "Yuanyuan Chen and Zichen Chen and Pengcheng Wu and Han Yu", "abstract": "  Large-scale neural networks possess considerable expressive power. They are\nwell-suited for complex learning tasks in industrial applications. However,\nlarge-scale models pose significant challenges for training under the current\nFederated Learning (FL) paradigm. Existing approaches for efficient FL training\noften leverage model parameter dropout. However, manipulating individual model\nparameters is not only inefficient in meaningfully reducing the communication\noverhead when training large-scale FL models, but may also be detrimental to\nthe scaling efforts and model performance as shown by recent research. To\naddress these issues, we propose the Federated Opportunistic Block Dropout\n(FedOBD) approach. The key novelty is that it decomposes large-scale models\ninto semantic blocks so that FL participants can opportunistically upload\nquantized blocks, which are deemed to be significant towards training the\nmodel, to the FL server for aggregation. Extensive experiments evaluating\nFedOBD against four state-of-the-art approaches based on multiple real-world\ndatasets show that it reduces the overall communication overhead by more than\n88% compared to the best performing baseline approach, while achieving the\nhighest test accuracy. To the best of our knowledge, FedOBD is the first\napproach to perform dropout on FL models at the block level rather than at the\nindividual parameter level.\n", "link": "http://arxiv.org/abs/2208.05174v6", "date": "2025-04-22", "relevancy": 1.994, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5013}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.501}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedOBD%3A%20Opportunistic%20Block%20Dropout%20for%20Efficiently%20Training%20Large-scale%0A%20%20Neural%20Networks%20through%20Federated%20Learning&body=Title%3A%20FedOBD%3A%20Opportunistic%20Block%20Dropout%20for%20Efficiently%20Training%20Large-scale%0A%20%20Neural%20Networks%20through%20Federated%20Learning%0AAuthor%3A%20Yuanyuan%20Chen%20and%20Zichen%20Chen%20and%20Pengcheng%20Wu%20and%20Han%20Yu%0AAbstract%3A%20%20%20Large-scale%20neural%20networks%20possess%20considerable%20expressive%20power.%20They%20are%0Awell-suited%20for%20complex%20learning%20tasks%20in%20industrial%20applications.%20However%2C%0Alarge-scale%20models%20pose%20significant%20challenges%20for%20training%20under%20the%20current%0AFederated%20Learning%20%28FL%29%20paradigm.%20Existing%20approaches%20for%20efficient%20FL%20training%0Aoften%20leverage%20model%20parameter%20dropout.%20However%2C%20manipulating%20individual%20model%0Aparameters%20is%20not%20only%20inefficient%20in%20meaningfully%20reducing%20the%20communication%0Aoverhead%20when%20training%20large-scale%20FL%20models%2C%20but%20may%20also%20be%20detrimental%20to%0Athe%20scaling%20efforts%20and%20model%20performance%20as%20shown%20by%20recent%20research.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20Federated%20Opportunistic%20Block%20Dropout%0A%28FedOBD%29%20approach.%20The%20key%20novelty%20is%20that%20it%20decomposes%20large-scale%20models%0Ainto%20semantic%20blocks%20so%20that%20FL%20participants%20can%20opportunistically%20upload%0Aquantized%20blocks%2C%20which%20are%20deemed%20to%20be%20significant%20towards%20training%20the%0Amodel%2C%20to%20the%20FL%20server%20for%20aggregation.%20Extensive%20experiments%20evaluating%0AFedOBD%20against%20four%20state-of-the-art%20approaches%20based%20on%20multiple%20real-world%0Adatasets%20show%20that%20it%20reduces%20the%20overall%20communication%20overhead%20by%20more%20than%0A88%25%20compared%20to%20the%20best%20performing%20baseline%20approach%2C%20while%20achieving%20the%0Ahighest%20test%20accuracy.%20To%20the%20best%20of%20our%20knowledge%2C%20FedOBD%20is%20the%20first%0Aapproach%20to%20perform%20dropout%20on%20FL%20models%20at%20the%20block%20level%20rather%20than%20at%20the%0Aindividual%20parameter%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.05174v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedOBD%253A%2520Opportunistic%2520Block%2520Dropout%2520for%2520Efficiently%2520Training%2520Large-scale%250A%2520%2520Neural%2520Networks%2520through%2520Federated%2520Learning%26entry.906535625%3DYuanyuan%2520Chen%2520and%2520Zichen%2520Chen%2520and%2520Pengcheng%2520Wu%2520and%2520Han%2520Yu%26entry.1292438233%3D%2520%2520Large-scale%2520neural%2520networks%2520possess%2520considerable%2520expressive%2520power.%2520They%2520are%250Awell-suited%2520for%2520complex%2520learning%2520tasks%2520in%2520industrial%2520applications.%2520However%252C%250Alarge-scale%2520models%2520pose%2520significant%2520challenges%2520for%2520training%2520under%2520the%2520current%250AFederated%2520Learning%2520%2528FL%2529%2520paradigm.%2520Existing%2520approaches%2520for%2520efficient%2520FL%2520training%250Aoften%2520leverage%2520model%2520parameter%2520dropout.%2520However%252C%2520manipulating%2520individual%2520model%250Aparameters%2520is%2520not%2520only%2520inefficient%2520in%2520meaningfully%2520reducing%2520the%2520communication%250Aoverhead%2520when%2520training%2520large-scale%2520FL%2520models%252C%2520but%2520may%2520also%2520be%2520detrimental%2520to%250Athe%2520scaling%2520efforts%2520and%2520model%2520performance%2520as%2520shown%2520by%2520recent%2520research.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520the%2520Federated%2520Opportunistic%2520Block%2520Dropout%250A%2528FedOBD%2529%2520approach.%2520The%2520key%2520novelty%2520is%2520that%2520it%2520decomposes%2520large-scale%2520models%250Ainto%2520semantic%2520blocks%2520so%2520that%2520FL%2520participants%2520can%2520opportunistically%2520upload%250Aquantized%2520blocks%252C%2520which%2520are%2520deemed%2520to%2520be%2520significant%2520towards%2520training%2520the%250Amodel%252C%2520to%2520the%2520FL%2520server%2520for%2520aggregation.%2520Extensive%2520experiments%2520evaluating%250AFedOBD%2520against%2520four%2520state-of-the-art%2520approaches%2520based%2520on%2520multiple%2520real-world%250Adatasets%2520show%2520that%2520it%2520reduces%2520the%2520overall%2520communication%2520overhead%2520by%2520more%2520than%250A88%2525%2520compared%2520to%2520the%2520best%2520performing%2520baseline%2520approach%252C%2520while%2520achieving%2520the%250Ahighest%2520test%2520accuracy.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520FedOBD%2520is%2520the%2520first%250Aapproach%2520to%2520perform%2520dropout%2520on%2520FL%2520models%2520at%2520the%2520block%2520level%2520rather%2520than%2520at%2520the%250Aindividual%2520parameter%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.05174v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedOBD%3A%20Opportunistic%20Block%20Dropout%20for%20Efficiently%20Training%20Large-scale%0A%20%20Neural%20Networks%20through%20Federated%20Learning&entry.906535625=Yuanyuan%20Chen%20and%20Zichen%20Chen%20and%20Pengcheng%20Wu%20and%20Han%20Yu&entry.1292438233=%20%20Large-scale%20neural%20networks%20possess%20considerable%20expressive%20power.%20They%20are%0Awell-suited%20for%20complex%20learning%20tasks%20in%20industrial%20applications.%20However%2C%0Alarge-scale%20models%20pose%20significant%20challenges%20for%20training%20under%20the%20current%0AFederated%20Learning%20%28FL%29%20paradigm.%20Existing%20approaches%20for%20efficient%20FL%20training%0Aoften%20leverage%20model%20parameter%20dropout.%20However%2C%20manipulating%20individual%20model%0Aparameters%20is%20not%20only%20inefficient%20in%20meaningfully%20reducing%20the%20communication%0Aoverhead%20when%20training%20large-scale%20FL%20models%2C%20but%20may%20also%20be%20detrimental%20to%0Athe%20scaling%20efforts%20and%20model%20performance%20as%20shown%20by%20recent%20research.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20Federated%20Opportunistic%20Block%20Dropout%0A%28FedOBD%29%20approach.%20The%20key%20novelty%20is%20that%20it%20decomposes%20large-scale%20models%0Ainto%20semantic%20blocks%20so%20that%20FL%20participants%20can%20opportunistically%20upload%0Aquantized%20blocks%2C%20which%20are%20deemed%20to%20be%20significant%20towards%20training%20the%0Amodel%2C%20to%20the%20FL%20server%20for%20aggregation.%20Extensive%20experiments%20evaluating%0AFedOBD%20against%20four%20state-of-the-art%20approaches%20based%20on%20multiple%20real-world%0Adatasets%20show%20that%20it%20reduces%20the%20overall%20communication%20overhead%20by%20more%20than%0A88%25%20compared%20to%20the%20best%20performing%20baseline%20approach%2C%20while%20achieving%20the%0Ahighest%20test%20accuracy.%20To%20the%20best%20of%20our%20knowledge%2C%20FedOBD%20is%20the%20first%0Aapproach%20to%20perform%20dropout%20on%20FL%20models%20at%20the%20block%20level%20rather%20than%20at%20the%0Aindividual%20parameter%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.05174v6&entry.124074799=Read"},
{"title": "Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text\n  Prompts", "author": "Zhenkui Yang and Zeyi Huang and Ge Wang and Han Ding and Tony Xiao Han and Fei Wang", "abstract": "  Wireless signal-based human sensing technologies, such as WiFi,\nmillimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID),\nenable the detection and interpretation of human presence, posture, and\nactivities, thereby providing critical support for applications in public\nsecurity, healthcare, and smart environments. These technologies exhibit\nnotable advantages due to their non-contact operation and environmental\nadaptability; however, existing systems often fail to leverage the textual\ninformation inherent in datasets. To address this, we propose an innovative\ntext-enhanced wireless sensing framework, WiTalk, that seamlessly integrates\nsemantic knowledge through three hierarchical prompt strategies-label-only,\nbrief description, and detailed action description-without requiring\narchitectural modifications or incurring additional data costs. We rigorously\nvalidate this framework across three public benchmark datasets: XRF55 for human\naction recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action\nlocalization (TAL). Experimental results demonstrate significant performance\nimprovements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%,\n2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD\nimproves by 4.98%; and on XRFV2, the mean average precision gains across\nvarious methods range from 4.02% to 13.68%. Our codes have been included in\nhttps://github.com/yangzhenkui/WiTalk.\n", "link": "http://arxiv.org/abs/2504.14621v2", "date": "2025-04-22", "relevancy": 1.9792, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5352}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4877}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Talk%20is%20Not%20Always%20Cheap%3A%20Promoting%20Wireless%20Sensing%20Models%20with%20Text%0A%20%20Prompts&body=Title%3A%20Talk%20is%20Not%20Always%20Cheap%3A%20Promoting%20Wireless%20Sensing%20Models%20with%20Text%0A%20%20Prompts%0AAuthor%3A%20Zhenkui%20Yang%20and%20Zeyi%20Huang%20and%20Ge%20Wang%20and%20Han%20Ding%20and%20Tony%20Xiao%20Han%20and%20Fei%20Wang%0AAbstract%3A%20%20%20Wireless%20signal-based%20human%20sensing%20technologies%2C%20such%20as%20WiFi%2C%0Amillimeter-wave%20%28mmWave%29%20radar%2C%20and%20Radio%20Frequency%20Identification%20%28RFID%29%2C%0Aenable%20the%20detection%20and%20interpretation%20of%20human%20presence%2C%20posture%2C%20and%0Aactivities%2C%20thereby%20providing%20critical%20support%20for%20applications%20in%20public%0Asecurity%2C%20healthcare%2C%20and%20smart%20environments.%20These%20technologies%20exhibit%0Anotable%20advantages%20due%20to%20their%20non-contact%20operation%20and%20environmental%0Aadaptability%3B%20however%2C%20existing%20systems%20often%20fail%20to%20leverage%20the%20textual%0Ainformation%20inherent%20in%20datasets.%20To%20address%20this%2C%20we%20propose%20an%20innovative%0Atext-enhanced%20wireless%20sensing%20framework%2C%20WiTalk%2C%20that%20seamlessly%20integrates%0Asemantic%20knowledge%20through%20three%20hierarchical%20prompt%20strategies-label-only%2C%0Abrief%20description%2C%20and%20detailed%20action%20description-without%20requiring%0Aarchitectural%20modifications%20or%20incurring%20additional%20data%20costs.%20We%20rigorously%0Avalidate%20this%20framework%20across%20three%20public%20benchmark%20datasets%3A%20XRF55%20for%20human%0Aaction%20recognition%20%28HAR%29%2C%20and%20WiFiTAL%20and%20XRFV2%20for%20WiFi%20temporal%20action%0Alocalization%20%28TAL%29.%20Experimental%20results%20demonstrate%20significant%20performance%0Aimprovements%3A%20on%20XRF55%2C%20accuracy%20for%20WiFi%2C%20RFID%2C%20and%20mmWave%20increases%20by%203.9%25%2C%0A2.59%25%2C%20and%200.46%25%2C%20respectively%3B%20on%20WiFiTAL%2C%20the%20average%20performance%20of%20WiFiTAD%0Aimproves%20by%204.98%25%3B%20and%20on%20XRFV2%2C%20the%20mean%20average%20precision%20gains%20across%0Avarious%20methods%20range%20from%204.02%25%20to%2013.68%25.%20Our%20codes%20have%20been%20included%20in%0Ahttps%3A//github.com/yangzhenkui/WiTalk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalk%2520is%2520Not%2520Always%2520Cheap%253A%2520Promoting%2520Wireless%2520Sensing%2520Models%2520with%2520Text%250A%2520%2520Prompts%26entry.906535625%3DZhenkui%2520Yang%2520and%2520Zeyi%2520Huang%2520and%2520Ge%2520Wang%2520and%2520Han%2520Ding%2520and%2520Tony%2520Xiao%2520Han%2520and%2520Fei%2520Wang%26entry.1292438233%3D%2520%2520Wireless%2520signal-based%2520human%2520sensing%2520technologies%252C%2520such%2520as%2520WiFi%252C%250Amillimeter-wave%2520%2528mmWave%2529%2520radar%252C%2520and%2520Radio%2520Frequency%2520Identification%2520%2528RFID%2529%252C%250Aenable%2520the%2520detection%2520and%2520interpretation%2520of%2520human%2520presence%252C%2520posture%252C%2520and%250Aactivities%252C%2520thereby%2520providing%2520critical%2520support%2520for%2520applications%2520in%2520public%250Asecurity%252C%2520healthcare%252C%2520and%2520smart%2520environments.%2520These%2520technologies%2520exhibit%250Anotable%2520advantages%2520due%2520to%2520their%2520non-contact%2520operation%2520and%2520environmental%250Aadaptability%253B%2520however%252C%2520existing%2520systems%2520often%2520fail%2520to%2520leverage%2520the%2520textual%250Ainformation%2520inherent%2520in%2520datasets.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520innovative%250Atext-enhanced%2520wireless%2520sensing%2520framework%252C%2520WiTalk%252C%2520that%2520seamlessly%2520integrates%250Asemantic%2520knowledge%2520through%2520three%2520hierarchical%2520prompt%2520strategies-label-only%252C%250Abrief%2520description%252C%2520and%2520detailed%2520action%2520description-without%2520requiring%250Aarchitectural%2520modifications%2520or%2520incurring%2520additional%2520data%2520costs.%2520We%2520rigorously%250Avalidate%2520this%2520framework%2520across%2520three%2520public%2520benchmark%2520datasets%253A%2520XRF55%2520for%2520human%250Aaction%2520recognition%2520%2528HAR%2529%252C%2520and%2520WiFiTAL%2520and%2520XRFV2%2520for%2520WiFi%2520temporal%2520action%250Alocalization%2520%2528TAL%2529.%2520Experimental%2520results%2520demonstrate%2520significant%2520performance%250Aimprovements%253A%2520on%2520XRF55%252C%2520accuracy%2520for%2520WiFi%252C%2520RFID%252C%2520and%2520mmWave%2520increases%2520by%25203.9%2525%252C%250A2.59%2525%252C%2520and%25200.46%2525%252C%2520respectively%253B%2520on%2520WiFiTAL%252C%2520the%2520average%2520performance%2520of%2520WiFiTAD%250Aimproves%2520by%25204.98%2525%253B%2520and%2520on%2520XRFV2%252C%2520the%2520mean%2520average%2520precision%2520gains%2520across%250Avarious%2520methods%2520range%2520from%25204.02%2525%2520to%252013.68%2525.%2520Our%2520codes%2520have%2520been%2520included%2520in%250Ahttps%253A//github.com/yangzhenkui/WiTalk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talk%20is%20Not%20Always%20Cheap%3A%20Promoting%20Wireless%20Sensing%20Models%20with%20Text%0A%20%20Prompts&entry.906535625=Zhenkui%20Yang%20and%20Zeyi%20Huang%20and%20Ge%20Wang%20and%20Han%20Ding%20and%20Tony%20Xiao%20Han%20and%20Fei%20Wang&entry.1292438233=%20%20Wireless%20signal-based%20human%20sensing%20technologies%2C%20such%20as%20WiFi%2C%0Amillimeter-wave%20%28mmWave%29%20radar%2C%20and%20Radio%20Frequency%20Identification%20%28RFID%29%2C%0Aenable%20the%20detection%20and%20interpretation%20of%20human%20presence%2C%20posture%2C%20and%0Aactivities%2C%20thereby%20providing%20critical%20support%20for%20applications%20in%20public%0Asecurity%2C%20healthcare%2C%20and%20smart%20environments.%20These%20technologies%20exhibit%0Anotable%20advantages%20due%20to%20their%20non-contact%20operation%20and%20environmental%0Aadaptability%3B%20however%2C%20existing%20systems%20often%20fail%20to%20leverage%20the%20textual%0Ainformation%20inherent%20in%20datasets.%20To%20address%20this%2C%20we%20propose%20an%20innovative%0Atext-enhanced%20wireless%20sensing%20framework%2C%20WiTalk%2C%20that%20seamlessly%20integrates%0Asemantic%20knowledge%20through%20three%20hierarchical%20prompt%20strategies-label-only%2C%0Abrief%20description%2C%20and%20detailed%20action%20description-without%20requiring%0Aarchitectural%20modifications%20or%20incurring%20additional%20data%20costs.%20We%20rigorously%0Avalidate%20this%20framework%20across%20three%20public%20benchmark%20datasets%3A%20XRF55%20for%20human%0Aaction%20recognition%20%28HAR%29%2C%20and%20WiFiTAL%20and%20XRFV2%20for%20WiFi%20temporal%20action%0Alocalization%20%28TAL%29.%20Experimental%20results%20demonstrate%20significant%20performance%0Aimprovements%3A%20on%20XRF55%2C%20accuracy%20for%20WiFi%2C%20RFID%2C%20and%20mmWave%20increases%20by%203.9%25%2C%0A2.59%25%2C%20and%200.46%25%2C%20respectively%3B%20on%20WiFiTAL%2C%20the%20average%20performance%20of%20WiFiTAD%0Aimproves%20by%204.98%25%3B%20and%20on%20XRFV2%2C%20the%20mean%20average%20precision%20gains%20across%0Avarious%20methods%20range%20from%204.02%25%20to%2013.68%25.%20Our%20codes%20have%20been%20included%20in%0Ahttps%3A//github.com/yangzhenkui/WiTalk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14621v2&entry.124074799=Read"},
{"title": "Towards Robust Infrared Small Target Detection: A Feature-Enhanced and\n  Sensitivity-Tunable Framework", "author": "Jinmiao Zhao and Zelin Shi and Chuang Yu and Yunpeng Liu and Yimian Dai", "abstract": "  Recently, single-frame infrared small target (SIRST) detection technology has\nattracted wide-spread attention. However, due to the intrinsic feature scarcity\nin infrared small targets, precise segmentation of small targets from complex\nbackgrounds remains a significant challenge. Different from most existing deep\nlearning-based methods that focus on improving network architectures, we\npropose a feature-enhanced and sensitivity-tunable (FEST) framework, which is\ncompatible with existing SIRST detection networks and further enhances their\ndetection performance. The FEST framework improves the model's robustness from\ntwo aspects: feature enhancement and target confidence regulation. For feature\nenhancement, on the one hand, we adopt a multi-scale fusion strategy, which can\neffectively improve the model's perception and adaptability to multi-scale\nfeatures of multi-size targets. On the other hand, we construct an edge\nenhancement difficulty mining (EEDM) loss based on the analysis of the task\ncharacteristics, which helps guide the network to continuously focus on\nchallenging target regions and edge features during training. For target\nconfidence regulation, we design an adjustable sensitivity (AS) strategy for\nnetwork post-processing. This strategy not only enhances the adaptability of\nthe network in complex scenarios, but also significantly improves the detection\nrate of infrared small targets while maintaining segmentation accuracy.\nExtensive experimental results show that our FEST framework can significantly\nenhance the performance of existing SIRST detection networks. Notably, the\nmulti-scale direction-aware network (MSDA-Net) equipped with the FEST framework\nwon the first prize in the PRCV 2024 wide-area infrared small target detection\ncompetition.\n", "link": "http://arxiv.org/abs/2407.20090v2", "date": "2025-04-22", "relevancy": 1.9733, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5074}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.485}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Infrared%20Small%20Target%20Detection%3A%20A%20Feature-Enhanced%20and%0A%20%20Sensitivity-Tunable%20Framework&body=Title%3A%20Towards%20Robust%20Infrared%20Small%20Target%20Detection%3A%20A%20Feature-Enhanced%20and%0A%20%20Sensitivity-Tunable%20Framework%0AAuthor%3A%20Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu%20and%20Yimian%20Dai%0AAbstract%3A%20%20%20Recently%2C%20single-frame%20infrared%20small%20target%20%28SIRST%29%20detection%20technology%20has%0Aattracted%20wide-spread%20attention.%20However%2C%20due%20to%20the%20intrinsic%20feature%20scarcity%0Ain%20infrared%20small%20targets%2C%20precise%20segmentation%20of%20small%20targets%20from%20complex%0Abackgrounds%20remains%20a%20significant%20challenge.%20Different%20from%20most%20existing%20deep%0Alearning-based%20methods%20that%20focus%20on%20improving%20network%20architectures%2C%20we%0Apropose%20a%20feature-enhanced%20and%20sensitivity-tunable%20%28FEST%29%20framework%2C%20which%20is%0Acompatible%20with%20existing%20SIRST%20detection%20networks%20and%20further%20enhances%20their%0Adetection%20performance.%20The%20FEST%20framework%20improves%20the%20model%27s%20robustness%20from%0Atwo%20aspects%3A%20feature%20enhancement%20and%20target%20confidence%20regulation.%20For%20feature%0Aenhancement%2C%20on%20the%20one%20hand%2C%20we%20adopt%20a%20multi-scale%20fusion%20strategy%2C%20which%20can%0Aeffectively%20improve%20the%20model%27s%20perception%20and%20adaptability%20to%20multi-scale%0Afeatures%20of%20multi-size%20targets.%20On%20the%20other%20hand%2C%20we%20construct%20an%20edge%0Aenhancement%20difficulty%20mining%20%28EEDM%29%20loss%20based%20on%20the%20analysis%20of%20the%20task%0Acharacteristics%2C%20which%20helps%20guide%20the%20network%20to%20continuously%20focus%20on%0Achallenging%20target%20regions%20and%20edge%20features%20during%20training.%20For%20target%0Aconfidence%20regulation%2C%20we%20design%20an%20adjustable%20sensitivity%20%28AS%29%20strategy%20for%0Anetwork%20post-processing.%20This%20strategy%20not%20only%20enhances%20the%20adaptability%20of%0Athe%20network%20in%20complex%20scenarios%2C%20but%20also%20significantly%20improves%20the%20detection%0Arate%20of%20infrared%20small%20targets%20while%20maintaining%20segmentation%20accuracy.%0AExtensive%20experimental%20results%20show%20that%20our%20FEST%20framework%20can%20significantly%0Aenhance%20the%20performance%20of%20existing%20SIRST%20detection%20networks.%20Notably%2C%20the%0Amulti-scale%20direction-aware%20network%20%28MSDA-Net%29%20equipped%20with%20the%20FEST%20framework%0Awon%20the%20first%20prize%20in%20the%20PRCV%202024%20wide-area%20infrared%20small%20target%20detection%0Acompetition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Infrared%2520Small%2520Target%2520Detection%253A%2520A%2520Feature-Enhanced%2520and%250A%2520%2520Sensitivity-Tunable%2520Framework%26entry.906535625%3DJinmiao%2520Zhao%2520and%2520Zelin%2520Shi%2520and%2520Chuang%2520Yu%2520and%2520Yunpeng%2520Liu%2520and%2520Yimian%2520Dai%26entry.1292438233%3D%2520%2520Recently%252C%2520single-frame%2520infrared%2520small%2520target%2520%2528SIRST%2529%2520detection%2520technology%2520has%250Aattracted%2520wide-spread%2520attention.%2520However%252C%2520due%2520to%2520the%2520intrinsic%2520feature%2520scarcity%250Ain%2520infrared%2520small%2520targets%252C%2520precise%2520segmentation%2520of%2520small%2520targets%2520from%2520complex%250Abackgrounds%2520remains%2520a%2520significant%2520challenge.%2520Different%2520from%2520most%2520existing%2520deep%250Alearning-based%2520methods%2520that%2520focus%2520on%2520improving%2520network%2520architectures%252C%2520we%250Apropose%2520a%2520feature-enhanced%2520and%2520sensitivity-tunable%2520%2528FEST%2529%2520framework%252C%2520which%2520is%250Acompatible%2520with%2520existing%2520SIRST%2520detection%2520networks%2520and%2520further%2520enhances%2520their%250Adetection%2520performance.%2520The%2520FEST%2520framework%2520improves%2520the%2520model%2527s%2520robustness%2520from%250Atwo%2520aspects%253A%2520feature%2520enhancement%2520and%2520target%2520confidence%2520regulation.%2520For%2520feature%250Aenhancement%252C%2520on%2520the%2520one%2520hand%252C%2520we%2520adopt%2520a%2520multi-scale%2520fusion%2520strategy%252C%2520which%2520can%250Aeffectively%2520improve%2520the%2520model%2527s%2520perception%2520and%2520adaptability%2520to%2520multi-scale%250Afeatures%2520of%2520multi-size%2520targets.%2520On%2520the%2520other%2520hand%252C%2520we%2520construct%2520an%2520edge%250Aenhancement%2520difficulty%2520mining%2520%2528EEDM%2529%2520loss%2520based%2520on%2520the%2520analysis%2520of%2520the%2520task%250Acharacteristics%252C%2520which%2520helps%2520guide%2520the%2520network%2520to%2520continuously%2520focus%2520on%250Achallenging%2520target%2520regions%2520and%2520edge%2520features%2520during%2520training.%2520For%2520target%250Aconfidence%2520regulation%252C%2520we%2520design%2520an%2520adjustable%2520sensitivity%2520%2528AS%2529%2520strategy%2520for%250Anetwork%2520post-processing.%2520This%2520strategy%2520not%2520only%2520enhances%2520the%2520adaptability%2520of%250Athe%2520network%2520in%2520complex%2520scenarios%252C%2520but%2520also%2520significantly%2520improves%2520the%2520detection%250Arate%2520of%2520infrared%2520small%2520targets%2520while%2520maintaining%2520segmentation%2520accuracy.%250AExtensive%2520experimental%2520results%2520show%2520that%2520our%2520FEST%2520framework%2520can%2520significantly%250Aenhance%2520the%2520performance%2520of%2520existing%2520SIRST%2520detection%2520networks.%2520Notably%252C%2520the%250Amulti-scale%2520direction-aware%2520network%2520%2528MSDA-Net%2529%2520equipped%2520with%2520the%2520FEST%2520framework%250Awon%2520the%2520first%2520prize%2520in%2520the%2520PRCV%25202024%2520wide-area%2520infrared%2520small%2520target%2520detection%250Acompetition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Infrared%20Small%20Target%20Detection%3A%20A%20Feature-Enhanced%20and%0A%20%20Sensitivity-Tunable%20Framework&entry.906535625=Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu%20and%20Yimian%20Dai&entry.1292438233=%20%20Recently%2C%20single-frame%20infrared%20small%20target%20%28SIRST%29%20detection%20technology%20has%0Aattracted%20wide-spread%20attention.%20However%2C%20due%20to%20the%20intrinsic%20feature%20scarcity%0Ain%20infrared%20small%20targets%2C%20precise%20segmentation%20of%20small%20targets%20from%20complex%0Abackgrounds%20remains%20a%20significant%20challenge.%20Different%20from%20most%20existing%20deep%0Alearning-based%20methods%20that%20focus%20on%20improving%20network%20architectures%2C%20we%0Apropose%20a%20feature-enhanced%20and%20sensitivity-tunable%20%28FEST%29%20framework%2C%20which%20is%0Acompatible%20with%20existing%20SIRST%20detection%20networks%20and%20further%20enhances%20their%0Adetection%20performance.%20The%20FEST%20framework%20improves%20the%20model%27s%20robustness%20from%0Atwo%20aspects%3A%20feature%20enhancement%20and%20target%20confidence%20regulation.%20For%20feature%0Aenhancement%2C%20on%20the%20one%20hand%2C%20we%20adopt%20a%20multi-scale%20fusion%20strategy%2C%20which%20can%0Aeffectively%20improve%20the%20model%27s%20perception%20and%20adaptability%20to%20multi-scale%0Afeatures%20of%20multi-size%20targets.%20On%20the%20other%20hand%2C%20we%20construct%20an%20edge%0Aenhancement%20difficulty%20mining%20%28EEDM%29%20loss%20based%20on%20the%20analysis%20of%20the%20task%0Acharacteristics%2C%20which%20helps%20guide%20the%20network%20to%20continuously%20focus%20on%0Achallenging%20target%20regions%20and%20edge%20features%20during%20training.%20For%20target%0Aconfidence%20regulation%2C%20we%20design%20an%20adjustable%20sensitivity%20%28AS%29%20strategy%20for%0Anetwork%20post-processing.%20This%20strategy%20not%20only%20enhances%20the%20adaptability%20of%0Athe%20network%20in%20complex%20scenarios%2C%20but%20also%20significantly%20improves%20the%20detection%0Arate%20of%20infrared%20small%20targets%20while%20maintaining%20segmentation%20accuracy.%0AExtensive%20experimental%20results%20show%20that%20our%20FEST%20framework%20can%20significantly%0Aenhance%20the%20performance%20of%20existing%20SIRST%20detection%20networks.%20Notably%2C%20the%0Amulti-scale%20direction-aware%20network%20%28MSDA-Net%29%20equipped%20with%20the%20FEST%20framework%0Awon%20the%20first%20prize%20in%20the%20PRCV%202024%20wide-area%20infrared%20small%20target%20detection%0Acompetition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20090v2&entry.124074799=Read"},
{"title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning", "author": "Le Zhuo and Liangbing Zhao and Sayak Paul and Yue Liao and Renrui Zhang and Yi Xin and Peng Gao and Mohamed Elhoseiny and Hongsheng Li", "abstract": "  Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.\n", "link": "http://arxiv.org/abs/2504.16080v1", "date": "2025-04-22", "relevancy": 1.9699, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7564}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6576}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Reflection%20to%20Perfection%3A%20Scaling%20Inference-Time%20Optimization%20for%0A%20%20Text-to-Image%20Diffusion%20Models%20via%20Reflection%20Tuning&body=Title%3A%20From%20Reflection%20to%20Perfection%3A%20Scaling%20Inference-Time%20Optimization%20for%0A%20%20Text-to-Image%20Diffusion%20Models%20via%20Reflection%20Tuning%0AAuthor%3A%20Le%20Zhuo%20and%20Liangbing%20Zhao%20and%20Sayak%20Paul%20and%20Yue%20Liao%20and%20Renrui%20Zhang%20and%20Yi%20Xin%20and%20Peng%20Gao%20and%20Mohamed%20Elhoseiny%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Recent%20text-to-image%20diffusion%20models%20achieve%20impressive%20visual%20quality%0Athrough%20extensive%20scaling%20of%20training%20data%20and%20model%20parameters%2C%20yet%20they%20often%0Astruggle%20with%20complex%20scenes%20and%20fine-grained%20details.%20Inspired%20by%20the%0Aself-reflection%20capabilities%20emergent%20in%20large%20language%20models%2C%20we%20propose%0AReflectionFlow%2C%20an%20inference-time%20framework%20enabling%20diffusion%20models%20to%0Aiteratively%20reflect%20upon%20and%20refine%20their%20outputs.%20ReflectionFlow%20introduces%0Athree%20complementary%20inference-time%20scaling%20axes%3A%20%281%29%20noise-level%20scaling%20to%0Aoptimize%20latent%20initialization%3B%20%282%29%20prompt-level%20scaling%20for%20precise%20semantic%0Aguidance%3B%20and%20most%20notably%2C%20%283%29%20reflection-level%20scaling%2C%20which%20explicitly%0Aprovides%20actionable%20reflections%20to%20iteratively%20assess%20and%20correct%20previous%0Agenerations.%20To%20facilitate%20reflection-level%20scaling%2C%20we%20construct%20GenRef%2C%20a%0Alarge-scale%20dataset%20comprising%201%20million%20triplets%2C%20each%20containing%20a%0Areflection%2C%20a%20flawed%20image%2C%20and%20an%20enhanced%20image.%20Leveraging%20this%20dataset%2C%20we%0Aefficiently%20perform%20reflection%20tuning%20on%20state-of-the-art%20diffusion%0Atransformer%2C%20FLUX.1-dev%2C%20by%20jointly%20modeling%20multimodal%20inputs%20within%20a%20unified%0Aframework.%20Experimental%20results%20show%20that%20ReflectionFlow%20significantly%0Aoutperforms%20naive%20noise-level%20scaling%20methods%2C%20offering%20a%20scalable%20and%0Acompute-efficient%20solution%20toward%20higher-quality%20image%20synthesis%20on%20challenging%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Reflection%2520to%2520Perfection%253A%2520Scaling%2520Inference-Time%2520Optimization%2520for%250A%2520%2520Text-to-Image%2520Diffusion%2520Models%2520via%2520Reflection%2520Tuning%26entry.906535625%3DLe%2520Zhuo%2520and%2520Liangbing%2520Zhao%2520and%2520Sayak%2520Paul%2520and%2520Yue%2520Liao%2520and%2520Renrui%2520Zhang%2520and%2520Yi%2520Xin%2520and%2520Peng%2520Gao%2520and%2520Mohamed%2520Elhoseiny%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Recent%2520text-to-image%2520diffusion%2520models%2520achieve%2520impressive%2520visual%2520quality%250Athrough%2520extensive%2520scaling%2520of%2520training%2520data%2520and%2520model%2520parameters%252C%2520yet%2520they%2520often%250Astruggle%2520with%2520complex%2520scenes%2520and%2520fine-grained%2520details.%2520Inspired%2520by%2520the%250Aself-reflection%2520capabilities%2520emergent%2520in%2520large%2520language%2520models%252C%2520we%2520propose%250AReflectionFlow%252C%2520an%2520inference-time%2520framework%2520enabling%2520diffusion%2520models%2520to%250Aiteratively%2520reflect%2520upon%2520and%2520refine%2520their%2520outputs.%2520ReflectionFlow%2520introduces%250Athree%2520complementary%2520inference-time%2520scaling%2520axes%253A%2520%25281%2529%2520noise-level%2520scaling%2520to%250Aoptimize%2520latent%2520initialization%253B%2520%25282%2529%2520prompt-level%2520scaling%2520for%2520precise%2520semantic%250Aguidance%253B%2520and%2520most%2520notably%252C%2520%25283%2529%2520reflection-level%2520scaling%252C%2520which%2520explicitly%250Aprovides%2520actionable%2520reflections%2520to%2520iteratively%2520assess%2520and%2520correct%2520previous%250Agenerations.%2520To%2520facilitate%2520reflection-level%2520scaling%252C%2520we%2520construct%2520GenRef%252C%2520a%250Alarge-scale%2520dataset%2520comprising%25201%2520million%2520triplets%252C%2520each%2520containing%2520a%250Areflection%252C%2520a%2520flawed%2520image%252C%2520and%2520an%2520enhanced%2520image.%2520Leveraging%2520this%2520dataset%252C%2520we%250Aefficiently%2520perform%2520reflection%2520tuning%2520on%2520state-of-the-art%2520diffusion%250Atransformer%252C%2520FLUX.1-dev%252C%2520by%2520jointly%2520modeling%2520multimodal%2520inputs%2520within%2520a%2520unified%250Aframework.%2520Experimental%2520results%2520show%2520that%2520ReflectionFlow%2520significantly%250Aoutperforms%2520naive%2520noise-level%2520scaling%2520methods%252C%2520offering%2520a%2520scalable%2520and%250Acompute-efficient%2520solution%2520toward%2520higher-quality%2520image%2520synthesis%2520on%2520challenging%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Reflection%20to%20Perfection%3A%20Scaling%20Inference-Time%20Optimization%20for%0A%20%20Text-to-Image%20Diffusion%20Models%20via%20Reflection%20Tuning&entry.906535625=Le%20Zhuo%20and%20Liangbing%20Zhao%20and%20Sayak%20Paul%20and%20Yue%20Liao%20and%20Renrui%20Zhang%20and%20Yi%20Xin%20and%20Peng%20Gao%20and%20Mohamed%20Elhoseiny%20and%20Hongsheng%20Li&entry.1292438233=%20%20Recent%20text-to-image%20diffusion%20models%20achieve%20impressive%20visual%20quality%0Athrough%20extensive%20scaling%20of%20training%20data%20and%20model%20parameters%2C%20yet%20they%20often%0Astruggle%20with%20complex%20scenes%20and%20fine-grained%20details.%20Inspired%20by%20the%0Aself-reflection%20capabilities%20emergent%20in%20large%20language%20models%2C%20we%20propose%0AReflectionFlow%2C%20an%20inference-time%20framework%20enabling%20diffusion%20models%20to%0Aiteratively%20reflect%20upon%20and%20refine%20their%20outputs.%20ReflectionFlow%20introduces%0Athree%20complementary%20inference-time%20scaling%20axes%3A%20%281%29%20noise-level%20scaling%20to%0Aoptimize%20latent%20initialization%3B%20%282%29%20prompt-level%20scaling%20for%20precise%20semantic%0Aguidance%3B%20and%20most%20notably%2C%20%283%29%20reflection-level%20scaling%2C%20which%20explicitly%0Aprovides%20actionable%20reflections%20to%20iteratively%20assess%20and%20correct%20previous%0Agenerations.%20To%20facilitate%20reflection-level%20scaling%2C%20we%20construct%20GenRef%2C%20a%0Alarge-scale%20dataset%20comprising%201%20million%20triplets%2C%20each%20containing%20a%0Areflection%2C%20a%20flawed%20image%2C%20and%20an%20enhanced%20image.%20Leveraging%20this%20dataset%2C%20we%0Aefficiently%20perform%20reflection%20tuning%20on%20state-of-the-art%20diffusion%0Atransformer%2C%20FLUX.1-dev%2C%20by%20jointly%20modeling%20multimodal%20inputs%20within%20a%20unified%0Aframework.%20Experimental%20results%20show%20that%20ReflectionFlow%20significantly%0Aoutperforms%20naive%20noise-level%20scaling%20methods%2C%20offering%20a%20scalable%20and%0Acompute-efficient%20solution%20toward%20higher-quality%20image%20synthesis%20on%20challenging%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16080v1&entry.124074799=Read"},
{"title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory", "author": "Alessio Buscemi and Daniele Proverbio and Alessandro Di Stefano and The Anh Han and German Castignani and Pietro Li\u00f2", "abstract": "  Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents.\n", "link": "http://arxiv.org/abs/2504.14325v2", "date": "2025-04-22", "relevancy": 1.9635, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5134}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4902}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAIRGAME%3A%20a%20Framework%20for%20AI%20Agents%20Bias%20Recognition%20using%20Game%20Theory&body=Title%3A%20FAIRGAME%3A%20a%20Framework%20for%20AI%20Agents%20Bias%20Recognition%20using%20Game%20Theory%0AAuthor%3A%20Alessio%20Buscemi%20and%20Daniele%20Proverbio%20and%20Alessandro%20Di%20Stefano%20and%20The%20Anh%20Han%20and%20German%20Castignani%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20%20%20Letting%20AI%20agents%20interact%20in%20multi-agent%20applications%20adds%20a%20layer%20of%0Acomplexity%20to%20the%20interpretability%20and%20prediction%20of%20AI%20outcomes%2C%20with%20profound%0Aimplications%20for%20their%20trustworthy%20adoption%20in%20research%20and%20society.%20Game%0Atheory%20offers%20powerful%20models%20to%20capture%20and%20interpret%20strategic%20interaction%0Aamong%20agents%2C%20but%20requires%20the%20support%20of%20reproducible%2C%20standardized%20and%0Auser-friendly%20IT%20frameworks%20to%20enable%20comparison%20and%20interpretation%20of%20results.%0ATo%20this%20end%2C%20we%20present%20FAIRGAME%2C%20a%20Framework%20for%20AI%20Agents%20Bias%20Recognition%0Ausing%20Game%20Theory.%20We%20describe%20its%20implementation%20and%20usage%2C%20and%20we%20employ%20it%0Ato%20uncover%20biased%20outcomes%20in%20popular%20games%20among%20AI%20agents%2C%20depending%20on%20the%0Aemployed%20Large%20Language%20Model%20%28LLM%29%20and%20used%20language%2C%20as%20well%20as%20on%20the%0Apersonality%20trait%20or%20strategic%20knowledge%20of%20the%20agents.%20Overall%2C%20FAIRGAME%0Aallows%20users%20to%20reliably%20and%20easily%20simulate%20their%20desired%20games%20and%20scenarios%0Aand%20compare%20the%20results%20across%20simulation%20campaigns%20and%20with%20game-theoretic%0Apredictions%2C%20enabling%20the%20systematic%20discovery%20of%20biases%2C%20the%20anticipation%20of%0Aemerging%20behavior%20out%20of%20strategic%20interplays%2C%20and%20empowering%20further%20research%0Ainto%20strategic%20decision-making%20using%20LLM%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAIRGAME%253A%2520a%2520Framework%2520for%2520AI%2520Agents%2520Bias%2520Recognition%2520using%2520Game%2520Theory%26entry.906535625%3DAlessio%2520Buscemi%2520and%2520Daniele%2520Proverbio%2520and%2520Alessandro%2520Di%2520Stefano%2520and%2520The%2520Anh%2520Han%2520and%2520German%2520Castignani%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520Letting%2520AI%2520agents%2520interact%2520in%2520multi-agent%2520applications%2520adds%2520a%2520layer%2520of%250Acomplexity%2520to%2520the%2520interpretability%2520and%2520prediction%2520of%2520AI%2520outcomes%252C%2520with%2520profound%250Aimplications%2520for%2520their%2520trustworthy%2520adoption%2520in%2520research%2520and%2520society.%2520Game%250Atheory%2520offers%2520powerful%2520models%2520to%2520capture%2520and%2520interpret%2520strategic%2520interaction%250Aamong%2520agents%252C%2520but%2520requires%2520the%2520support%2520of%2520reproducible%252C%2520standardized%2520and%250Auser-friendly%2520IT%2520frameworks%2520to%2520enable%2520comparison%2520and%2520interpretation%2520of%2520results.%250ATo%2520this%2520end%252C%2520we%2520present%2520FAIRGAME%252C%2520a%2520Framework%2520for%2520AI%2520Agents%2520Bias%2520Recognition%250Ausing%2520Game%2520Theory.%2520We%2520describe%2520its%2520implementation%2520and%2520usage%252C%2520and%2520we%2520employ%2520it%250Ato%2520uncover%2520biased%2520outcomes%2520in%2520popular%2520games%2520among%2520AI%2520agents%252C%2520depending%2520on%2520the%250Aemployed%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520and%2520used%2520language%252C%2520as%2520well%2520as%2520on%2520the%250Apersonality%2520trait%2520or%2520strategic%2520knowledge%2520of%2520the%2520agents.%2520Overall%252C%2520FAIRGAME%250Aallows%2520users%2520to%2520reliably%2520and%2520easily%2520simulate%2520their%2520desired%2520games%2520and%2520scenarios%250Aand%2520compare%2520the%2520results%2520across%2520simulation%2520campaigns%2520and%2520with%2520game-theoretic%250Apredictions%252C%2520enabling%2520the%2520systematic%2520discovery%2520of%2520biases%252C%2520the%2520anticipation%2520of%250Aemerging%2520behavior%2520out%2520of%2520strategic%2520interplays%252C%2520and%2520empowering%2520further%2520research%250Ainto%2520strategic%2520decision-making%2520using%2520LLM%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAIRGAME%3A%20a%20Framework%20for%20AI%20Agents%20Bias%20Recognition%20using%20Game%20Theory&entry.906535625=Alessio%20Buscemi%20and%20Daniele%20Proverbio%20and%20Alessandro%20Di%20Stefano%20and%20The%20Anh%20Han%20and%20German%20Castignani%20and%20Pietro%20Li%C3%B2&entry.1292438233=%20%20Letting%20AI%20agents%20interact%20in%20multi-agent%20applications%20adds%20a%20layer%20of%0Acomplexity%20to%20the%20interpretability%20and%20prediction%20of%20AI%20outcomes%2C%20with%20profound%0Aimplications%20for%20their%20trustworthy%20adoption%20in%20research%20and%20society.%20Game%0Atheory%20offers%20powerful%20models%20to%20capture%20and%20interpret%20strategic%20interaction%0Aamong%20agents%2C%20but%20requires%20the%20support%20of%20reproducible%2C%20standardized%20and%0Auser-friendly%20IT%20frameworks%20to%20enable%20comparison%20and%20interpretation%20of%20results.%0ATo%20this%20end%2C%20we%20present%20FAIRGAME%2C%20a%20Framework%20for%20AI%20Agents%20Bias%20Recognition%0Ausing%20Game%20Theory.%20We%20describe%20its%20implementation%20and%20usage%2C%20and%20we%20employ%20it%0Ato%20uncover%20biased%20outcomes%20in%20popular%20games%20among%20AI%20agents%2C%20depending%20on%20the%0Aemployed%20Large%20Language%20Model%20%28LLM%29%20and%20used%20language%2C%20as%20well%20as%20on%20the%0Apersonality%20trait%20or%20strategic%20knowledge%20of%20the%20agents.%20Overall%2C%20FAIRGAME%0Aallows%20users%20to%20reliably%20and%20easily%20simulate%20their%20desired%20games%20and%20scenarios%0Aand%20compare%20the%20results%20across%20simulation%20campaigns%20and%20with%20game-theoretic%0Apredictions%2C%20enabling%20the%20systematic%20discovery%20of%20biases%2C%20the%20anticipation%20of%0Aemerging%20behavior%20out%20of%20strategic%20interplays%2C%20and%20empowering%20further%20research%0Ainto%20strategic%20decision-making%20using%20LLM%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14325v2&entry.124074799=Read"},
{"title": "EEG Right & Left Voluntary Hand Movement-based Virtual Brain-Computer\n  Interfacing Keyboard Using Hybrid Deep Learning Approach", "author": "Biplov Paneru and Bipul Thapa and Bishwash Paneru and Sanjog Chhetri Sapkota", "abstract": "  Brain-machine interfaces (BMIs), particularly those based on\nelectroencephalography (EEG), offer promising solutions for assisting\nindividuals with motor disabilities. However, challenges in reliably\ninterpreting EEG signals for specific tasks, such as simulating keystrokes,\npersist due to the complexity and variability of brain activity. Current\nEEG-based BMIs face limitations in adaptability, usability, and robustness,\nespecially in applications like virtual keyboards, as traditional\nmachine-learning models struggle to handle high-dimensional EEG data\neffectively. To address these gaps, we developed an EEG-based BMI system\ncapable of accurately identifying voluntary keystrokes, specifically leveraging\nright and left voluntary hand movements. Using a publicly available EEG\ndataset, the signals were pre-processed with band-pass filtering, segmented\ninto 22-electrode arrays, and refined into event-related potential (ERP)\nwindows, resulting in a 19x200 feature array categorized into three classes:\nresting state (0), 'd' key press (1), and 'l' key press (2). Our approach\nemploys a hybrid neural network architecture with BiGRU-Attention as the\nproposed model for interpreting EEG signals, achieving superior test accuracy\nof 90% and a mean accuracy of 91% in 10-fold stratified cross-validation. This\nperformance outperforms traditional ML methods like Support Vector Machines\n(SVMs) and Naive Bayes, as well as advanced architectures such as Transformers,\nCNN-Transformer hybrids, and EEGNet. Finally, the BiGRU-Attention model is\nintegrated into a real-time graphical user interface (GUI) to simulate and\npredict keystrokes from brain activity. Our work demonstrates how deep learning\ncan advance EEG-based BMI systems by addressing the challenges of signal\ninterpretation and classification.\n", "link": "http://arxiv.org/abs/2409.00035v3", "date": "2025-04-22", "relevancy": 1.9484, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4929}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG%20Right%20%26%20Left%20Voluntary%20Hand%20Movement-based%20Virtual%20Brain-Computer%0A%20%20Interfacing%20Keyboard%20Using%20Hybrid%20Deep%20Learning%20Approach&body=Title%3A%20EEG%20Right%20%26%20Left%20Voluntary%20Hand%20Movement-based%20Virtual%20Brain-Computer%0A%20%20Interfacing%20Keyboard%20Using%20Hybrid%20Deep%20Learning%20Approach%0AAuthor%3A%20Biplov%20Paneru%20and%20Bipul%20Thapa%20and%20Bishwash%20Paneru%20and%20Sanjog%20Chhetri%20Sapkota%0AAbstract%3A%20%20%20Brain-machine%20interfaces%20%28BMIs%29%2C%20particularly%20those%20based%20on%0Aelectroencephalography%20%28EEG%29%2C%20offer%20promising%20solutions%20for%20assisting%0Aindividuals%20with%20motor%20disabilities.%20However%2C%20challenges%20in%20reliably%0Ainterpreting%20EEG%20signals%20for%20specific%20tasks%2C%20such%20as%20simulating%20keystrokes%2C%0Apersist%20due%20to%20the%20complexity%20and%20variability%20of%20brain%20activity.%20Current%0AEEG-based%20BMIs%20face%20limitations%20in%20adaptability%2C%20usability%2C%20and%20robustness%2C%0Aespecially%20in%20applications%20like%20virtual%20keyboards%2C%20as%20traditional%0Amachine-learning%20models%20struggle%20to%20handle%20high-dimensional%20EEG%20data%0Aeffectively.%20To%20address%20these%20gaps%2C%20we%20developed%20an%20EEG-based%20BMI%20system%0Acapable%20of%20accurately%20identifying%20voluntary%20keystrokes%2C%20specifically%20leveraging%0Aright%20and%20left%20voluntary%20hand%20movements.%20Using%20a%20publicly%20available%20EEG%0Adataset%2C%20the%20signals%20were%20pre-processed%20with%20band-pass%20filtering%2C%20segmented%0Ainto%2022-electrode%20arrays%2C%20and%20refined%20into%20event-related%20potential%20%28ERP%29%0Awindows%2C%20resulting%20in%20a%2019x200%20feature%20array%20categorized%20into%20three%20classes%3A%0Aresting%20state%20%280%29%2C%20%27d%27%20key%20press%20%281%29%2C%20and%20%27l%27%20key%20press%20%282%29.%20Our%20approach%0Aemploys%20a%20hybrid%20neural%20network%20architecture%20with%20BiGRU-Attention%20as%20the%0Aproposed%20model%20for%20interpreting%20EEG%20signals%2C%20achieving%20superior%20test%20accuracy%0Aof%2090%25%20and%20a%20mean%20accuracy%20of%2091%25%20in%2010-fold%20stratified%20cross-validation.%20This%0Aperformance%20outperforms%20traditional%20ML%20methods%20like%20Support%20Vector%20Machines%0A%28SVMs%29%20and%20Naive%20Bayes%2C%20as%20well%20as%20advanced%20architectures%20such%20as%20Transformers%2C%0ACNN-Transformer%20hybrids%2C%20and%20EEGNet.%20Finally%2C%20the%20BiGRU-Attention%20model%20is%0Aintegrated%20into%20a%20real-time%20graphical%20user%20interface%20%28GUI%29%20to%20simulate%20and%0Apredict%20keystrokes%20from%20brain%20activity.%20Our%20work%20demonstrates%20how%20deep%20learning%0Acan%20advance%20EEG-based%20BMI%20systems%20by%20addressing%20the%20challenges%20of%20signal%0Ainterpretation%20and%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00035v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG%2520Right%2520%2526%2520Left%2520Voluntary%2520Hand%2520Movement-based%2520Virtual%2520Brain-Computer%250A%2520%2520Interfacing%2520Keyboard%2520Using%2520Hybrid%2520Deep%2520Learning%2520Approach%26entry.906535625%3DBiplov%2520Paneru%2520and%2520Bipul%2520Thapa%2520and%2520Bishwash%2520Paneru%2520and%2520Sanjog%2520Chhetri%2520Sapkota%26entry.1292438233%3D%2520%2520Brain-machine%2520interfaces%2520%2528BMIs%2529%252C%2520particularly%2520those%2520based%2520on%250Aelectroencephalography%2520%2528EEG%2529%252C%2520offer%2520promising%2520solutions%2520for%2520assisting%250Aindividuals%2520with%2520motor%2520disabilities.%2520However%252C%2520challenges%2520in%2520reliably%250Ainterpreting%2520EEG%2520signals%2520for%2520specific%2520tasks%252C%2520such%2520as%2520simulating%2520keystrokes%252C%250Apersist%2520due%2520to%2520the%2520complexity%2520and%2520variability%2520of%2520brain%2520activity.%2520Current%250AEEG-based%2520BMIs%2520face%2520limitations%2520in%2520adaptability%252C%2520usability%252C%2520and%2520robustness%252C%250Aespecially%2520in%2520applications%2520like%2520virtual%2520keyboards%252C%2520as%2520traditional%250Amachine-learning%2520models%2520struggle%2520to%2520handle%2520high-dimensional%2520EEG%2520data%250Aeffectively.%2520To%2520address%2520these%2520gaps%252C%2520we%2520developed%2520an%2520EEG-based%2520BMI%2520system%250Acapable%2520of%2520accurately%2520identifying%2520voluntary%2520keystrokes%252C%2520specifically%2520leveraging%250Aright%2520and%2520left%2520voluntary%2520hand%2520movements.%2520Using%2520a%2520publicly%2520available%2520EEG%250Adataset%252C%2520the%2520signals%2520were%2520pre-processed%2520with%2520band-pass%2520filtering%252C%2520segmented%250Ainto%252022-electrode%2520arrays%252C%2520and%2520refined%2520into%2520event-related%2520potential%2520%2528ERP%2529%250Awindows%252C%2520resulting%2520in%2520a%252019x200%2520feature%2520array%2520categorized%2520into%2520three%2520classes%253A%250Aresting%2520state%2520%25280%2529%252C%2520%2527d%2527%2520key%2520press%2520%25281%2529%252C%2520and%2520%2527l%2527%2520key%2520press%2520%25282%2529.%2520Our%2520approach%250Aemploys%2520a%2520hybrid%2520neural%2520network%2520architecture%2520with%2520BiGRU-Attention%2520as%2520the%250Aproposed%2520model%2520for%2520interpreting%2520EEG%2520signals%252C%2520achieving%2520superior%2520test%2520accuracy%250Aof%252090%2525%2520and%2520a%2520mean%2520accuracy%2520of%252091%2525%2520in%252010-fold%2520stratified%2520cross-validation.%2520This%250Aperformance%2520outperforms%2520traditional%2520ML%2520methods%2520like%2520Support%2520Vector%2520Machines%250A%2528SVMs%2529%2520and%2520Naive%2520Bayes%252C%2520as%2520well%2520as%2520advanced%2520architectures%2520such%2520as%2520Transformers%252C%250ACNN-Transformer%2520hybrids%252C%2520and%2520EEGNet.%2520Finally%252C%2520the%2520BiGRU-Attention%2520model%2520is%250Aintegrated%2520into%2520a%2520real-time%2520graphical%2520user%2520interface%2520%2528GUI%2529%2520to%2520simulate%2520and%250Apredict%2520keystrokes%2520from%2520brain%2520activity.%2520Our%2520work%2520demonstrates%2520how%2520deep%2520learning%250Acan%2520advance%2520EEG-based%2520BMI%2520systems%2520by%2520addressing%2520the%2520challenges%2520of%2520signal%250Ainterpretation%2520and%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00035v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG%20Right%20%26%20Left%20Voluntary%20Hand%20Movement-based%20Virtual%20Brain-Computer%0A%20%20Interfacing%20Keyboard%20Using%20Hybrid%20Deep%20Learning%20Approach&entry.906535625=Biplov%20Paneru%20and%20Bipul%20Thapa%20and%20Bishwash%20Paneru%20and%20Sanjog%20Chhetri%20Sapkota&entry.1292438233=%20%20Brain-machine%20interfaces%20%28BMIs%29%2C%20particularly%20those%20based%20on%0Aelectroencephalography%20%28EEG%29%2C%20offer%20promising%20solutions%20for%20assisting%0Aindividuals%20with%20motor%20disabilities.%20However%2C%20challenges%20in%20reliably%0Ainterpreting%20EEG%20signals%20for%20specific%20tasks%2C%20such%20as%20simulating%20keystrokes%2C%0Apersist%20due%20to%20the%20complexity%20and%20variability%20of%20brain%20activity.%20Current%0AEEG-based%20BMIs%20face%20limitations%20in%20adaptability%2C%20usability%2C%20and%20robustness%2C%0Aespecially%20in%20applications%20like%20virtual%20keyboards%2C%20as%20traditional%0Amachine-learning%20models%20struggle%20to%20handle%20high-dimensional%20EEG%20data%0Aeffectively.%20To%20address%20these%20gaps%2C%20we%20developed%20an%20EEG-based%20BMI%20system%0Acapable%20of%20accurately%20identifying%20voluntary%20keystrokes%2C%20specifically%20leveraging%0Aright%20and%20left%20voluntary%20hand%20movements.%20Using%20a%20publicly%20available%20EEG%0Adataset%2C%20the%20signals%20were%20pre-processed%20with%20band-pass%20filtering%2C%20segmented%0Ainto%2022-electrode%20arrays%2C%20and%20refined%20into%20event-related%20potential%20%28ERP%29%0Awindows%2C%20resulting%20in%20a%2019x200%20feature%20array%20categorized%20into%20three%20classes%3A%0Aresting%20state%20%280%29%2C%20%27d%27%20key%20press%20%281%29%2C%20and%20%27l%27%20key%20press%20%282%29.%20Our%20approach%0Aemploys%20a%20hybrid%20neural%20network%20architecture%20with%20BiGRU-Attention%20as%20the%0Aproposed%20model%20for%20interpreting%20EEG%20signals%2C%20achieving%20superior%20test%20accuracy%0Aof%2090%25%20and%20a%20mean%20accuracy%20of%2091%25%20in%2010-fold%20stratified%20cross-validation.%20This%0Aperformance%20outperforms%20traditional%20ML%20methods%20like%20Support%20Vector%20Machines%0A%28SVMs%29%20and%20Naive%20Bayes%2C%20as%20well%20as%20advanced%20architectures%20such%20as%20Transformers%2C%0ACNN-Transformer%20hybrids%2C%20and%20EEGNet.%20Finally%2C%20the%20BiGRU-Attention%20model%20is%0Aintegrated%20into%20a%20real-time%20graphical%20user%20interface%20%28GUI%29%20to%20simulate%20and%0Apredict%20keystrokes%20from%20brain%20activity.%20Our%20work%20demonstrates%20how%20deep%20learning%0Acan%20advance%20EEG-based%20BMI%20systems%20by%20addressing%20the%20challenges%20of%20signal%0Ainterpretation%20and%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00035v3&entry.124074799=Read"},
{"title": "Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal\n  Fold Paralysis", "author": "Yucong Zhang and Xin Zou and Jinshan Yang and Wenjun Chen and Juan Liu and Faya Liang and Ming Li", "abstract": "  This paper presents the Multimodal Laryngoscopic Video Analyzing System\n(MLVAS), a novel system that leverages both audio and video data to\nautomatically extract key video segments and metrics from raw laryngeal\nvideostroboscopic videos for assisted clinical assessment. The system\nintegrates video-based glottis detection with an audio keyword spotting method\nto analyze both video and audio data, identifying patient vocalizations and\nrefining video highlights to ensure optimal inspection of vocal fold movements.\nBeyond key video segment extraction from the raw laryngeal videos, MLVAS is\nable to generate effective audio and visual features for Vocal Fold Paralysis\n(VFP) detection. Pre-trained audio encoders are utilized to encode the patient\nvoice to get the audio features. Visual features are generated by measuring the\nangle deviation of both the left and right vocal folds to the estimated glottal\nmidline on the segmented glottis masks. To get better masks, we introduce a\ndiffusion-based refinement that follows traditional U-Net segmentation to\nreduce false positives. We conducted several ablation studies to demonstrate\nthe effectiveness of each module and modalities in the proposed MLVAS. The\nexperimental results on a public segmentation dataset show the effectiveness of\nour proposed segmentation module. In addition, unilateral VFP classification\nresults on a real-world clinic dataset demonstrate MLVAS's ability of providing\nreliable and objective metrics as well as visualization for assisted clinical\ndiagnosis.\n", "link": "http://arxiv.org/abs/2409.03597v3", "date": "2025-04-22", "relevancy": 1.9373, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Laryngoscopic%20Video%20Analysis%20for%20Assisted%20Diagnosis%20of%20Vocal%0A%20%20Fold%20Paralysis&body=Title%3A%20Multimodal%20Laryngoscopic%20Video%20Analysis%20for%20Assisted%20Diagnosis%20of%20Vocal%0A%20%20Fold%20Paralysis%0AAuthor%3A%20Yucong%20Zhang%20and%20Xin%20Zou%20and%20Jinshan%20Yang%20and%20Wenjun%20Chen%20and%20Juan%20Liu%20and%20Faya%20Liang%20and%20Ming%20Li%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20Multimodal%20Laryngoscopic%20Video%20Analyzing%20System%0A%28MLVAS%29%2C%20a%20novel%20system%20that%20leverages%20both%20audio%20and%20video%20data%20to%0Aautomatically%20extract%20key%20video%20segments%20and%20metrics%20from%20raw%20laryngeal%0Avideostroboscopic%20videos%20for%20assisted%20clinical%20assessment.%20The%20system%0Aintegrates%20video-based%20glottis%20detection%20with%20an%20audio%20keyword%20spotting%20method%0Ato%20analyze%20both%20video%20and%20audio%20data%2C%20identifying%20patient%20vocalizations%20and%0Arefining%20video%20highlights%20to%20ensure%20optimal%20inspection%20of%20vocal%20fold%20movements.%0ABeyond%20key%20video%20segment%20extraction%20from%20the%20raw%20laryngeal%20videos%2C%20MLVAS%20is%0Aable%20to%20generate%20effective%20audio%20and%20visual%20features%20for%20Vocal%20Fold%20Paralysis%0A%28VFP%29%20detection.%20Pre-trained%20audio%20encoders%20are%20utilized%20to%20encode%20the%20patient%0Avoice%20to%20get%20the%20audio%20features.%20Visual%20features%20are%20generated%20by%20measuring%20the%0Aangle%20deviation%20of%20both%20the%20left%20and%20right%20vocal%20folds%20to%20the%20estimated%20glottal%0Amidline%20on%20the%20segmented%20glottis%20masks.%20To%20get%20better%20masks%2C%20we%20introduce%20a%0Adiffusion-based%20refinement%20that%20follows%20traditional%20U-Net%20segmentation%20to%0Areduce%20false%20positives.%20We%20conducted%20several%20ablation%20studies%20to%20demonstrate%0Athe%20effectiveness%20of%20each%20module%20and%20modalities%20in%20the%20proposed%20MLVAS.%20The%0Aexperimental%20results%20on%20a%20public%20segmentation%20dataset%20show%20the%20effectiveness%20of%0Aour%20proposed%20segmentation%20module.%20In%20addition%2C%20unilateral%20VFP%20classification%0Aresults%20on%20a%20real-world%20clinic%20dataset%20demonstrate%20MLVAS%27s%20ability%20of%20providing%0Areliable%20and%20objective%20metrics%20as%20well%20as%20visualization%20for%20assisted%20clinical%0Adiagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03597v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Laryngoscopic%2520Video%2520Analysis%2520for%2520Assisted%2520Diagnosis%2520of%2520Vocal%250A%2520%2520Fold%2520Paralysis%26entry.906535625%3DYucong%2520Zhang%2520and%2520Xin%2520Zou%2520and%2520Jinshan%2520Yang%2520and%2520Wenjun%2520Chen%2520and%2520Juan%2520Liu%2520and%2520Faya%2520Liang%2520and%2520Ming%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520Multimodal%2520Laryngoscopic%2520Video%2520Analyzing%2520System%250A%2528MLVAS%2529%252C%2520a%2520novel%2520system%2520that%2520leverages%2520both%2520audio%2520and%2520video%2520data%2520to%250Aautomatically%2520extract%2520key%2520video%2520segments%2520and%2520metrics%2520from%2520raw%2520laryngeal%250Avideostroboscopic%2520videos%2520for%2520assisted%2520clinical%2520assessment.%2520The%2520system%250Aintegrates%2520video-based%2520glottis%2520detection%2520with%2520an%2520audio%2520keyword%2520spotting%2520method%250Ato%2520analyze%2520both%2520video%2520and%2520audio%2520data%252C%2520identifying%2520patient%2520vocalizations%2520and%250Arefining%2520video%2520highlights%2520to%2520ensure%2520optimal%2520inspection%2520of%2520vocal%2520fold%2520movements.%250ABeyond%2520key%2520video%2520segment%2520extraction%2520from%2520the%2520raw%2520laryngeal%2520videos%252C%2520MLVAS%2520is%250Aable%2520to%2520generate%2520effective%2520audio%2520and%2520visual%2520features%2520for%2520Vocal%2520Fold%2520Paralysis%250A%2528VFP%2529%2520detection.%2520Pre-trained%2520audio%2520encoders%2520are%2520utilized%2520to%2520encode%2520the%2520patient%250Avoice%2520to%2520get%2520the%2520audio%2520features.%2520Visual%2520features%2520are%2520generated%2520by%2520measuring%2520the%250Aangle%2520deviation%2520of%2520both%2520the%2520left%2520and%2520right%2520vocal%2520folds%2520to%2520the%2520estimated%2520glottal%250Amidline%2520on%2520the%2520segmented%2520glottis%2520masks.%2520To%2520get%2520better%2520masks%252C%2520we%2520introduce%2520a%250Adiffusion-based%2520refinement%2520that%2520follows%2520traditional%2520U-Net%2520segmentation%2520to%250Areduce%2520false%2520positives.%2520We%2520conducted%2520several%2520ablation%2520studies%2520to%2520demonstrate%250Athe%2520effectiveness%2520of%2520each%2520module%2520and%2520modalities%2520in%2520the%2520proposed%2520MLVAS.%2520The%250Aexperimental%2520results%2520on%2520a%2520public%2520segmentation%2520dataset%2520show%2520the%2520effectiveness%2520of%250Aour%2520proposed%2520segmentation%2520module.%2520In%2520addition%252C%2520unilateral%2520VFP%2520classification%250Aresults%2520on%2520a%2520real-world%2520clinic%2520dataset%2520demonstrate%2520MLVAS%2527s%2520ability%2520of%2520providing%250Areliable%2520and%2520objective%2520metrics%2520as%2520well%2520as%2520visualization%2520for%2520assisted%2520clinical%250Adiagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03597v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Laryngoscopic%20Video%20Analysis%20for%20Assisted%20Diagnosis%20of%20Vocal%0A%20%20Fold%20Paralysis&entry.906535625=Yucong%20Zhang%20and%20Xin%20Zou%20and%20Jinshan%20Yang%20and%20Wenjun%20Chen%20and%20Juan%20Liu%20and%20Faya%20Liang%20and%20Ming%20Li&entry.1292438233=%20%20This%20paper%20presents%20the%20Multimodal%20Laryngoscopic%20Video%20Analyzing%20System%0A%28MLVAS%29%2C%20a%20novel%20system%20that%20leverages%20both%20audio%20and%20video%20data%20to%0Aautomatically%20extract%20key%20video%20segments%20and%20metrics%20from%20raw%20laryngeal%0Avideostroboscopic%20videos%20for%20assisted%20clinical%20assessment.%20The%20system%0Aintegrates%20video-based%20glottis%20detection%20with%20an%20audio%20keyword%20spotting%20method%0Ato%20analyze%20both%20video%20and%20audio%20data%2C%20identifying%20patient%20vocalizations%20and%0Arefining%20video%20highlights%20to%20ensure%20optimal%20inspection%20of%20vocal%20fold%20movements.%0ABeyond%20key%20video%20segment%20extraction%20from%20the%20raw%20laryngeal%20videos%2C%20MLVAS%20is%0Aable%20to%20generate%20effective%20audio%20and%20visual%20features%20for%20Vocal%20Fold%20Paralysis%0A%28VFP%29%20detection.%20Pre-trained%20audio%20encoders%20are%20utilized%20to%20encode%20the%20patient%0Avoice%20to%20get%20the%20audio%20features.%20Visual%20features%20are%20generated%20by%20measuring%20the%0Aangle%20deviation%20of%20both%20the%20left%20and%20right%20vocal%20folds%20to%20the%20estimated%20glottal%0Amidline%20on%20the%20segmented%20glottis%20masks.%20To%20get%20better%20masks%2C%20we%20introduce%20a%0Adiffusion-based%20refinement%20that%20follows%20traditional%20U-Net%20segmentation%20to%0Areduce%20false%20positives.%20We%20conducted%20several%20ablation%20studies%20to%20demonstrate%0Athe%20effectiveness%20of%20each%20module%20and%20modalities%20in%20the%20proposed%20MLVAS.%20The%0Aexperimental%20results%20on%20a%20public%20segmentation%20dataset%20show%20the%20effectiveness%20of%0Aour%20proposed%20segmentation%20module.%20In%20addition%2C%20unilateral%20VFP%20classification%0Aresults%20on%20a%20real-world%20clinic%20dataset%20demonstrate%20MLVAS%27s%20ability%20of%20providing%0Areliable%20and%20objective%20metrics%20as%20well%20as%20visualization%20for%20assisted%20clinical%0Adiagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03597v3&entry.124074799=Read"},
{"title": "PIDSR: Complementary Polarized Image Demosaicing and Super-Resolution", "author": "Shuangfan Zhou and Chu Zhou and Youwei Lyu and Heng Guo and Zhanyu Ma and Boxin Shi and Imari Sato", "abstract": "  Polarization cameras can capture multiple polarized images with different\npolarizer angles in a single shot, bringing convenience to polarization-based\ndownstream tasks. However, their direct outputs are color-polarization filter\narray (CPFA) raw images, requiring demosaicing to reconstruct full-resolution,\nfull-color polarized images; unfortunately, this necessary step introduces\nartifacts that make polarization-related parameters such as the degree of\npolarization (DoP) and angle of polarization (AoP) prone to error. Besides,\nlimited by the hardware design, the resolution of a polarization camera is\noften much lower than that of a conventional RGB camera. Existing polarized\nimage demosaicing (PID) methods are limited in that they cannot enhance\nresolution, while polarized image super-resolution (PISR) methods, though\ndesigned to obtain high-resolution (HR) polarized images from the demosaicing\nresults, tend to retain or even amplify errors in the DoP and AoP introduced by\ndemosaicing artifacts. In this paper, we propose PIDSR, a joint framework that\nperforms complementary Polarized Image Demosaicing and Super-Resolution,\nshowing the ability to robustly obtain high-quality HR polarized images with\nmore accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments\nshow our PIDSR not only achieves state-of-the-art performance on both synthetic\nand real data, but also facilitates downstream tasks.\n", "link": "http://arxiv.org/abs/2504.07758v2", "date": "2025-04-22", "relevancy": 1.9363, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5046}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4883}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIDSR%3A%20Complementary%20Polarized%20Image%20Demosaicing%20and%20Super-Resolution&body=Title%3A%20PIDSR%3A%20Complementary%20Polarized%20Image%20Demosaicing%20and%20Super-Resolution%0AAuthor%3A%20Shuangfan%20Zhou%20and%20Chu%20Zhou%20and%20Youwei%20Lyu%20and%20Heng%20Guo%20and%20Zhanyu%20Ma%20and%20Boxin%20Shi%20and%20Imari%20Sato%0AAbstract%3A%20%20%20Polarization%20cameras%20can%20capture%20multiple%20polarized%20images%20with%20different%0Apolarizer%20angles%20in%20a%20single%20shot%2C%20bringing%20convenience%20to%20polarization-based%0Adownstream%20tasks.%20However%2C%20their%20direct%20outputs%20are%20color-polarization%20filter%0Aarray%20%28CPFA%29%20raw%20images%2C%20requiring%20demosaicing%20to%20reconstruct%20full-resolution%2C%0Afull-color%20polarized%20images%3B%20unfortunately%2C%20this%20necessary%20step%20introduces%0Aartifacts%20that%20make%20polarization-related%20parameters%20such%20as%20the%20degree%20of%0Apolarization%20%28DoP%29%20and%20angle%20of%20polarization%20%28AoP%29%20prone%20to%20error.%20Besides%2C%0Alimited%20by%20the%20hardware%20design%2C%20the%20resolution%20of%20a%20polarization%20camera%20is%0Aoften%20much%20lower%20than%20that%20of%20a%20conventional%20RGB%20camera.%20Existing%20polarized%0Aimage%20demosaicing%20%28PID%29%20methods%20are%20limited%20in%20that%20they%20cannot%20enhance%0Aresolution%2C%20while%20polarized%20image%20super-resolution%20%28PISR%29%20methods%2C%20though%0Adesigned%20to%20obtain%20high-resolution%20%28HR%29%20polarized%20images%20from%20the%20demosaicing%0Aresults%2C%20tend%20to%20retain%20or%20even%20amplify%20errors%20in%20the%20DoP%20and%20AoP%20introduced%20by%0Ademosaicing%20artifacts.%20In%20this%20paper%2C%20we%20propose%20PIDSR%2C%20a%20joint%20framework%20that%0Aperforms%20complementary%20Polarized%20Image%20Demosaicing%20and%20Super-Resolution%2C%0Ashowing%20the%20ability%20to%20robustly%20obtain%20high-quality%20HR%20polarized%20images%20with%0Amore%20accurate%20DoP%20and%20AoP%20from%20a%20CPFA%20raw%20image%20in%20a%20direct%20manner.%20Experiments%0Ashow%20our%20PIDSR%20not%20only%20achieves%20state-of-the-art%20performance%20on%20both%20synthetic%0Aand%20real%20data%2C%20but%20also%20facilitates%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07758v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIDSR%253A%2520Complementary%2520Polarized%2520Image%2520Demosaicing%2520and%2520Super-Resolution%26entry.906535625%3DShuangfan%2520Zhou%2520and%2520Chu%2520Zhou%2520and%2520Youwei%2520Lyu%2520and%2520Heng%2520Guo%2520and%2520Zhanyu%2520Ma%2520and%2520Boxin%2520Shi%2520and%2520Imari%2520Sato%26entry.1292438233%3D%2520%2520Polarization%2520cameras%2520can%2520capture%2520multiple%2520polarized%2520images%2520with%2520different%250Apolarizer%2520angles%2520in%2520a%2520single%2520shot%252C%2520bringing%2520convenience%2520to%2520polarization-based%250Adownstream%2520tasks.%2520However%252C%2520their%2520direct%2520outputs%2520are%2520color-polarization%2520filter%250Aarray%2520%2528CPFA%2529%2520raw%2520images%252C%2520requiring%2520demosaicing%2520to%2520reconstruct%2520full-resolution%252C%250Afull-color%2520polarized%2520images%253B%2520unfortunately%252C%2520this%2520necessary%2520step%2520introduces%250Aartifacts%2520that%2520make%2520polarization-related%2520parameters%2520such%2520as%2520the%2520degree%2520of%250Apolarization%2520%2528DoP%2529%2520and%2520angle%2520of%2520polarization%2520%2528AoP%2529%2520prone%2520to%2520error.%2520Besides%252C%250Alimited%2520by%2520the%2520hardware%2520design%252C%2520the%2520resolution%2520of%2520a%2520polarization%2520camera%2520is%250Aoften%2520much%2520lower%2520than%2520that%2520of%2520a%2520conventional%2520RGB%2520camera.%2520Existing%2520polarized%250Aimage%2520demosaicing%2520%2528PID%2529%2520methods%2520are%2520limited%2520in%2520that%2520they%2520cannot%2520enhance%250Aresolution%252C%2520while%2520polarized%2520image%2520super-resolution%2520%2528PISR%2529%2520methods%252C%2520though%250Adesigned%2520to%2520obtain%2520high-resolution%2520%2528HR%2529%2520polarized%2520images%2520from%2520the%2520demosaicing%250Aresults%252C%2520tend%2520to%2520retain%2520or%2520even%2520amplify%2520errors%2520in%2520the%2520DoP%2520and%2520AoP%2520introduced%2520by%250Ademosaicing%2520artifacts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PIDSR%252C%2520a%2520joint%2520framework%2520that%250Aperforms%2520complementary%2520Polarized%2520Image%2520Demosaicing%2520and%2520Super-Resolution%252C%250Ashowing%2520the%2520ability%2520to%2520robustly%2520obtain%2520high-quality%2520HR%2520polarized%2520images%2520with%250Amore%2520accurate%2520DoP%2520and%2520AoP%2520from%2520a%2520CPFA%2520raw%2520image%2520in%2520a%2520direct%2520manner.%2520Experiments%250Ashow%2520our%2520PIDSR%2520not%2520only%2520achieves%2520state-of-the-art%2520performance%2520on%2520both%2520synthetic%250Aand%2520real%2520data%252C%2520but%2520also%2520facilitates%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07758v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIDSR%3A%20Complementary%20Polarized%20Image%20Demosaicing%20and%20Super-Resolution&entry.906535625=Shuangfan%20Zhou%20and%20Chu%20Zhou%20and%20Youwei%20Lyu%20and%20Heng%20Guo%20and%20Zhanyu%20Ma%20and%20Boxin%20Shi%20and%20Imari%20Sato&entry.1292438233=%20%20Polarization%20cameras%20can%20capture%20multiple%20polarized%20images%20with%20different%0Apolarizer%20angles%20in%20a%20single%20shot%2C%20bringing%20convenience%20to%20polarization-based%0Adownstream%20tasks.%20However%2C%20their%20direct%20outputs%20are%20color-polarization%20filter%0Aarray%20%28CPFA%29%20raw%20images%2C%20requiring%20demosaicing%20to%20reconstruct%20full-resolution%2C%0Afull-color%20polarized%20images%3B%20unfortunately%2C%20this%20necessary%20step%20introduces%0Aartifacts%20that%20make%20polarization-related%20parameters%20such%20as%20the%20degree%20of%0Apolarization%20%28DoP%29%20and%20angle%20of%20polarization%20%28AoP%29%20prone%20to%20error.%20Besides%2C%0Alimited%20by%20the%20hardware%20design%2C%20the%20resolution%20of%20a%20polarization%20camera%20is%0Aoften%20much%20lower%20than%20that%20of%20a%20conventional%20RGB%20camera.%20Existing%20polarized%0Aimage%20demosaicing%20%28PID%29%20methods%20are%20limited%20in%20that%20they%20cannot%20enhance%0Aresolution%2C%20while%20polarized%20image%20super-resolution%20%28PISR%29%20methods%2C%20though%0Adesigned%20to%20obtain%20high-resolution%20%28HR%29%20polarized%20images%20from%20the%20demosaicing%0Aresults%2C%20tend%20to%20retain%20or%20even%20amplify%20errors%20in%20the%20DoP%20and%20AoP%20introduced%20by%0Ademosaicing%20artifacts.%20In%20this%20paper%2C%20we%20propose%20PIDSR%2C%20a%20joint%20framework%20that%0Aperforms%20complementary%20Polarized%20Image%20Demosaicing%20and%20Super-Resolution%2C%0Ashowing%20the%20ability%20to%20robustly%20obtain%20high-quality%20HR%20polarized%20images%20with%0Amore%20accurate%20DoP%20and%20AoP%20from%20a%20CPFA%20raw%20image%20in%20a%20direct%20manner.%20Experiments%0Ashow%20our%20PIDSR%20not%20only%20achieves%20state-of-the-art%20performance%20on%20both%20synthetic%0Aand%20real%20data%2C%20but%20also%20facilitates%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07758v2&entry.124074799=Read"},
{"title": "Time's Up! An Empirical Study of LLM Reasoning Ability Under Output\n  Length Constraint", "author": "Yi Sun and Han Wang and Jiaqiang Li and Jiacheng Liu and Xiangyu Li and Hao Wen and Huiwen Zheng and Yan Liang and Yuanchun Li and Yunxin Liu", "abstract": "  Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making the models think before\nanswering, they are able to achieve much higher accuracy with extra inference\ncomputation. However, in many real-world scenarios, models are used under time\nconstraints, where an answer should be given to the user within a certain\noutput length. It is unclear whether and how the reasoning abilities of LLMs\nremain effective under such constraints. We take a first look at this problem\nby conducting an in-depth empirical study. Specifically, we test more than 25\nLLMs on common reasoning datasets under a wide range of output length budgets,\nand we analyze the correlation between the inference accuracy and various\nproperties including model type, model size, prompt style, etc. We also\nconsider the mappings between the token budgets and the actual on-device\nlatency budgets. The results have demonstrated several interesting findings\nregarding the budget-aware LLM reasoning that differ from the unconstrained\nsituation, e.g. the optimal choices of model sizes and prompts change under\ndifferent budgets. These findings offer practical guidance for users to deploy\nLLMs under real-world latency constraints.\n", "link": "http://arxiv.org/abs/2504.14350v2", "date": "2025-04-22", "relevancy": 1.9353, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%27s%20Up%21%20An%20Empirical%20Study%20of%20LLM%20Reasoning%20Ability%20Under%20Output%0A%20%20Length%20Constraint&body=Title%3A%20Time%27s%20Up%21%20An%20Empirical%20Study%20of%20LLM%20Reasoning%20Ability%20Under%20Output%0A%20%20Length%20Constraint%0AAuthor%3A%20Yi%20Sun%20and%20Han%20Wang%20and%20Jiaqiang%20Li%20and%20Jiacheng%20Liu%20and%20Xiangyu%20Li%20and%20Hao%20Wen%20and%20Huiwen%20Zheng%20and%20Yan%20Liang%20and%20Yuanchun%20Li%20and%20Yunxin%20Liu%0AAbstract%3A%20%20%20Recent%20work%20has%20demonstrated%20the%20remarkable%20potential%20of%20Large%20Language%0AModels%20%28LLMs%29%20in%20test-time%20scaling.%20By%20making%20the%20models%20think%20before%0Aanswering%2C%20they%20are%20able%20to%20achieve%20much%20higher%20accuracy%20with%20extra%20inference%0Acomputation.%20However%2C%20in%20many%20real-world%20scenarios%2C%20models%20are%20used%20under%20time%0Aconstraints%2C%20where%20an%20answer%20should%20be%20given%20to%20the%20user%20within%20a%20certain%0Aoutput%20length.%20It%20is%20unclear%20whether%20and%20how%20the%20reasoning%20abilities%20of%20LLMs%0Aremain%20effective%20under%20such%20constraints.%20We%20take%20a%20first%20look%20at%20this%20problem%0Aby%20conducting%20an%20in-depth%20empirical%20study.%20Specifically%2C%20we%20test%20more%20than%2025%0ALLMs%20on%20common%20reasoning%20datasets%20under%20a%20wide%20range%20of%20output%20length%20budgets%2C%0Aand%20we%20analyze%20the%20correlation%20between%20the%20inference%20accuracy%20and%20various%0Aproperties%20including%20model%20type%2C%20model%20size%2C%20prompt%20style%2C%20etc.%20We%20also%0Aconsider%20the%20mappings%20between%20the%20token%20budgets%20and%20the%20actual%20on-device%0Alatency%20budgets.%20The%20results%20have%20demonstrated%20several%20interesting%20findings%0Aregarding%20the%20budget-aware%20LLM%20reasoning%20that%20differ%20from%20the%20unconstrained%0Asituation%2C%20e.g.%20the%20optimal%20choices%20of%20model%20sizes%20and%20prompts%20change%20under%0Adifferent%20budgets.%20These%20findings%20offer%20practical%20guidance%20for%20users%20to%20deploy%0ALLMs%20under%20real-world%20latency%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14350v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2527s%2520Up%2521%2520An%2520Empirical%2520Study%2520of%2520LLM%2520Reasoning%2520Ability%2520Under%2520Output%250A%2520%2520Length%2520Constraint%26entry.906535625%3DYi%2520Sun%2520and%2520Han%2520Wang%2520and%2520Jiaqiang%2520Li%2520and%2520Jiacheng%2520Liu%2520and%2520Xiangyu%2520Li%2520and%2520Hao%2520Wen%2520and%2520Huiwen%2520Zheng%2520and%2520Yan%2520Liang%2520and%2520Yuanchun%2520Li%2520and%2520Yunxin%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520demonstrated%2520the%2520remarkable%2520potential%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520in%2520test-time%2520scaling.%2520By%2520making%2520the%2520models%2520think%2520before%250Aanswering%252C%2520they%2520are%2520able%2520to%2520achieve%2520much%2520higher%2520accuracy%2520with%2520extra%2520inference%250Acomputation.%2520However%252C%2520in%2520many%2520real-world%2520scenarios%252C%2520models%2520are%2520used%2520under%2520time%250Aconstraints%252C%2520where%2520an%2520answer%2520should%2520be%2520given%2520to%2520the%2520user%2520within%2520a%2520certain%250Aoutput%2520length.%2520It%2520is%2520unclear%2520whether%2520and%2520how%2520the%2520reasoning%2520abilities%2520of%2520LLMs%250Aremain%2520effective%2520under%2520such%2520constraints.%2520We%2520take%2520a%2520first%2520look%2520at%2520this%2520problem%250Aby%2520conducting%2520an%2520in-depth%2520empirical%2520study.%2520Specifically%252C%2520we%2520test%2520more%2520than%252025%250ALLMs%2520on%2520common%2520reasoning%2520datasets%2520under%2520a%2520wide%2520range%2520of%2520output%2520length%2520budgets%252C%250Aand%2520we%2520analyze%2520the%2520correlation%2520between%2520the%2520inference%2520accuracy%2520and%2520various%250Aproperties%2520including%2520model%2520type%252C%2520model%2520size%252C%2520prompt%2520style%252C%2520etc.%2520We%2520also%250Aconsider%2520the%2520mappings%2520between%2520the%2520token%2520budgets%2520and%2520the%2520actual%2520on-device%250Alatency%2520budgets.%2520The%2520results%2520have%2520demonstrated%2520several%2520interesting%2520findings%250Aregarding%2520the%2520budget-aware%2520LLM%2520reasoning%2520that%2520differ%2520from%2520the%2520unconstrained%250Asituation%252C%2520e.g.%2520the%2520optimal%2520choices%2520of%2520model%2520sizes%2520and%2520prompts%2520change%2520under%250Adifferent%2520budgets.%2520These%2520findings%2520offer%2520practical%2520guidance%2520for%2520users%2520to%2520deploy%250ALLMs%2520under%2520real-world%2520latency%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14350v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%27s%20Up%21%20An%20Empirical%20Study%20of%20LLM%20Reasoning%20Ability%20Under%20Output%0A%20%20Length%20Constraint&entry.906535625=Yi%20Sun%20and%20Han%20Wang%20and%20Jiaqiang%20Li%20and%20Jiacheng%20Liu%20and%20Xiangyu%20Li%20and%20Hao%20Wen%20and%20Huiwen%20Zheng%20and%20Yan%20Liang%20and%20Yuanchun%20Li%20and%20Yunxin%20Liu&entry.1292438233=%20%20Recent%20work%20has%20demonstrated%20the%20remarkable%20potential%20of%20Large%20Language%0AModels%20%28LLMs%29%20in%20test-time%20scaling.%20By%20making%20the%20models%20think%20before%0Aanswering%2C%20they%20are%20able%20to%20achieve%20much%20higher%20accuracy%20with%20extra%20inference%0Acomputation.%20However%2C%20in%20many%20real-world%20scenarios%2C%20models%20are%20used%20under%20time%0Aconstraints%2C%20where%20an%20answer%20should%20be%20given%20to%20the%20user%20within%20a%20certain%0Aoutput%20length.%20It%20is%20unclear%20whether%20and%20how%20the%20reasoning%20abilities%20of%20LLMs%0Aremain%20effective%20under%20such%20constraints.%20We%20take%20a%20first%20look%20at%20this%20problem%0Aby%20conducting%20an%20in-depth%20empirical%20study.%20Specifically%2C%20we%20test%20more%20than%2025%0ALLMs%20on%20common%20reasoning%20datasets%20under%20a%20wide%20range%20of%20output%20length%20budgets%2C%0Aand%20we%20analyze%20the%20correlation%20between%20the%20inference%20accuracy%20and%20various%0Aproperties%20including%20model%20type%2C%20model%20size%2C%20prompt%20style%2C%20etc.%20We%20also%0Aconsider%20the%20mappings%20between%20the%20token%20budgets%20and%20the%20actual%20on-device%0Alatency%20budgets.%20The%20results%20have%20demonstrated%20several%20interesting%20findings%0Aregarding%20the%20budget-aware%20LLM%20reasoning%20that%20differ%20from%20the%20unconstrained%0Asituation%2C%20e.g.%20the%20optimal%20choices%20of%20model%20sizes%20and%20prompts%20change%20under%0Adifferent%20budgets.%20These%20findings%20offer%20practical%20guidance%20for%20users%20to%20deploy%0ALLMs%20under%20real-world%20latency%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14350v2&entry.124074799=Read"},
{"title": "A Pontryagin Perspective on Reinforcement Learning", "author": "Onno Eberhard and Claire Vernade and Michael Muehlebach", "abstract": "  Reinforcement learning has traditionally focused on learning state-dependent\npolicies to solve optimal control problems in a closed-loop fashion. In this\nwork, we introduce the paradigm of open-loop reinforcement learning where a\nfixed action sequence is learned instead. We present three new algorithms: one\nrobust model-based method and two sample-efficient model-free methods. Rather\nthan basing our algorithms on Bellman's equation from dynamic programming, our\nwork builds on Pontryagin's principle from the theory of open-loop optimal\ncontrol. We provide convergence guarantees and evaluate all methods empirically\non a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks,\nsignificantly outperforming existing baselines.\n", "link": "http://arxiv.org/abs/2405.18100v3", "date": "2025-04-22", "relevancy": 1.9222, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5012}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4861}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Pontryagin%20Perspective%20on%20Reinforcement%20Learning&body=Title%3A%20A%20Pontryagin%20Perspective%20on%20Reinforcement%20Learning%0AAuthor%3A%20Onno%20Eberhard%20and%20Claire%20Vernade%20and%20Michael%20Muehlebach%0AAbstract%3A%20%20%20Reinforcement%20learning%20has%20traditionally%20focused%20on%20learning%20state-dependent%0Apolicies%20to%20solve%20optimal%20control%20problems%20in%20a%20closed-loop%20fashion.%20In%20this%0Awork%2C%20we%20introduce%20the%20paradigm%20of%20open-loop%20reinforcement%20learning%20where%20a%0Afixed%20action%20sequence%20is%20learned%20instead.%20We%20present%20three%20new%20algorithms%3A%20one%0Arobust%20model-based%20method%20and%20two%20sample-efficient%20model-free%20methods.%20Rather%0Athan%20basing%20our%20algorithms%20on%20Bellman%27s%20equation%20from%20dynamic%20programming%2C%20our%0Awork%20builds%20on%20Pontryagin%27s%20principle%20from%20the%20theory%20of%20open-loop%20optimal%0Acontrol.%20We%20provide%20convergence%20guarantees%20and%20evaluate%20all%20methods%20empirically%0Aon%20a%20pendulum%20swing-up%20task%2C%20as%20well%20as%20on%20two%20high-dimensional%20MuJoCo%20tasks%2C%0Asignificantly%20outperforming%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18100v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Pontryagin%2520Perspective%2520on%2520Reinforcement%2520Learning%26entry.906535625%3DOnno%2520Eberhard%2520and%2520Claire%2520Vernade%2520and%2520Michael%2520Muehlebach%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520has%2520traditionally%2520focused%2520on%2520learning%2520state-dependent%250Apolicies%2520to%2520solve%2520optimal%2520control%2520problems%2520in%2520a%2520closed-loop%2520fashion.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520the%2520paradigm%2520of%2520open-loop%2520reinforcement%2520learning%2520where%2520a%250Afixed%2520action%2520sequence%2520is%2520learned%2520instead.%2520We%2520present%2520three%2520new%2520algorithms%253A%2520one%250Arobust%2520model-based%2520method%2520and%2520two%2520sample-efficient%2520model-free%2520methods.%2520Rather%250Athan%2520basing%2520our%2520algorithms%2520on%2520Bellman%2527s%2520equation%2520from%2520dynamic%2520programming%252C%2520our%250Awork%2520builds%2520on%2520Pontryagin%2527s%2520principle%2520from%2520the%2520theory%2520of%2520open-loop%2520optimal%250Acontrol.%2520We%2520provide%2520convergence%2520guarantees%2520and%2520evaluate%2520all%2520methods%2520empirically%250Aon%2520a%2520pendulum%2520swing-up%2520task%252C%2520as%2520well%2520as%2520on%2520two%2520high-dimensional%2520MuJoCo%2520tasks%252C%250Asignificantly%2520outperforming%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18100v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Pontryagin%20Perspective%20on%20Reinforcement%20Learning&entry.906535625=Onno%20Eberhard%20and%20Claire%20Vernade%20and%20Michael%20Muehlebach&entry.1292438233=%20%20Reinforcement%20learning%20has%20traditionally%20focused%20on%20learning%20state-dependent%0Apolicies%20to%20solve%20optimal%20control%20problems%20in%20a%20closed-loop%20fashion.%20In%20this%0Awork%2C%20we%20introduce%20the%20paradigm%20of%20open-loop%20reinforcement%20learning%20where%20a%0Afixed%20action%20sequence%20is%20learned%20instead.%20We%20present%20three%20new%20algorithms%3A%20one%0Arobust%20model-based%20method%20and%20two%20sample-efficient%20model-free%20methods.%20Rather%0Athan%20basing%20our%20algorithms%20on%20Bellman%27s%20equation%20from%20dynamic%20programming%2C%20our%0Awork%20builds%20on%20Pontryagin%27s%20principle%20from%20the%20theory%20of%20open-loop%20optimal%0Acontrol.%20We%20provide%20convergence%20guarantees%20and%20evaluate%20all%20methods%20empirically%0Aon%20a%20pendulum%20swing-up%20task%2C%20as%20well%20as%20on%20two%20high-dimensional%20MuJoCo%20tasks%2C%0Asignificantly%20outperforming%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18100v3&entry.124074799=Read"},
{"title": "Tina: Tiny Reasoning Models via LoRA", "author": "Shangshang Wang and Julian Asilis and \u00d6mer Faruk Akg\u00fcl and Enes Burak Bilgin and Ollie Liu and Willie Neiswanger", "abstract": "  How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.\n", "link": "http://arxiv.org/abs/2504.15777v1", "date": "2025-04-22", "relevancy": 1.9198, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tina%3A%20Tiny%20Reasoning%20Models%20via%20LoRA&body=Title%3A%20Tina%3A%20Tiny%20Reasoning%20Models%20via%20LoRA%0AAuthor%3A%20Shangshang%20Wang%20and%20Julian%20Asilis%20and%20%C3%96mer%20Faruk%20Akg%C3%BCl%20and%20Enes%20Burak%20Bilgin%20and%20Ollie%20Liu%20and%20Willie%20Neiswanger%0AAbstract%3A%20%20%20How%20cost-effectively%20can%20strong%20reasoning%20abilities%20be%20achieved%20in%20language%0Amodels%3F%20Driven%20by%20this%20fundamental%20question%2C%20we%20present%20Tina%2C%20a%20family%20of%20tiny%0Areasoning%20models%20achieved%20with%20high%20cost-efficiency.%20Notably%2C%20Tina%20demonstrates%0Athat%20substantial%20reasoning%20performance%20can%20be%20developed%20using%20only%20minimal%0Aresources%2C%20by%20applying%20parameter-efficient%20updates%20during%20reinforcement%0Alearning%20%28RL%29%2C%20using%20low-rank%20adaptation%20%28LoRA%29%2C%20to%20an%20already%20tiny%201.5B%0Aparameter%20base%20model.%20This%20minimalist%20approach%20produces%20models%20that%20achieve%0Areasoning%20performance%20which%20is%20competitive%20with%2C%20and%20sometimes%20surpasses%2C%20SOTA%0ARL%20reasoning%20models%20built%20upon%20the%20same%20base%20model.%20Crucially%2C%20this%20is%20achieved%0Aat%20a%20tiny%20fraction%20of%20the%20computational%20post-training%20cost%20employed%20by%20existing%0ASOTA%20models.%20In%20fact%2C%20the%20best%20Tina%20model%20achieves%20a%20%3E20%5C%25%20reasoning%0Aperformance%20increase%20and%2043.33%5C%25%20Pass%401%20accuracy%20on%20AIME24%2C%20at%20only%20%5C%249%20USD%0Apost-training%20and%20evaluation%20cost%20%28i.e.%2C%20an%20estimated%20260x%20cost%20reduction%29.%20Our%0Awork%20reveals%20the%20surprising%20effectiveness%20of%20efficient%20RL%20reasoning%20via%20LoRA.%0AWe%20validate%20this%20across%20multiple%20open-source%20reasoning%20datasets%20and%20various%0Aablation%20settings%20starting%20with%20a%20single%2C%20fixed%20set%20of%20hyperparameters.%0AFurthermore%2C%20we%20hypothesize%20that%20this%20effectiveness%20and%20efficiency%20stem%20from%0ALoRA%20rapidly%20adapting%20the%20model%20to%20the%20structural%20format%20of%20reasoning%20rewarded%0Aby%20RL%2C%20while%20largely%20preserving%20the%20base%20model%27s%20underlying%20knowledge.%20In%0Aservice%20of%20accessibility%20and%20open%20research%2C%20we%20fully%20open-source%20all%20code%2C%0Atraining%20logs%2C%20and%20model%20weights%20%5C%26%20checkpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTina%253A%2520Tiny%2520Reasoning%2520Models%2520via%2520LoRA%26entry.906535625%3DShangshang%2520Wang%2520and%2520Julian%2520Asilis%2520and%2520%25C3%2596mer%2520Faruk%2520Akg%25C3%25BCl%2520and%2520Enes%2520Burak%2520Bilgin%2520and%2520Ollie%2520Liu%2520and%2520Willie%2520Neiswanger%26entry.1292438233%3D%2520%2520How%2520cost-effectively%2520can%2520strong%2520reasoning%2520abilities%2520be%2520achieved%2520in%2520language%250Amodels%253F%2520Driven%2520by%2520this%2520fundamental%2520question%252C%2520we%2520present%2520Tina%252C%2520a%2520family%2520of%2520tiny%250Areasoning%2520models%2520achieved%2520with%2520high%2520cost-efficiency.%2520Notably%252C%2520Tina%2520demonstrates%250Athat%2520substantial%2520reasoning%2520performance%2520can%2520be%2520developed%2520using%2520only%2520minimal%250Aresources%252C%2520by%2520applying%2520parameter-efficient%2520updates%2520during%2520reinforcement%250Alearning%2520%2528RL%2529%252C%2520using%2520low-rank%2520adaptation%2520%2528LoRA%2529%252C%2520to%2520an%2520already%2520tiny%25201.5B%250Aparameter%2520base%2520model.%2520This%2520minimalist%2520approach%2520produces%2520models%2520that%2520achieve%250Areasoning%2520performance%2520which%2520is%2520competitive%2520with%252C%2520and%2520sometimes%2520surpasses%252C%2520SOTA%250ARL%2520reasoning%2520models%2520built%2520upon%2520the%2520same%2520base%2520model.%2520Crucially%252C%2520this%2520is%2520achieved%250Aat%2520a%2520tiny%2520fraction%2520of%2520the%2520computational%2520post-training%2520cost%2520employed%2520by%2520existing%250ASOTA%2520models.%2520In%2520fact%252C%2520the%2520best%2520Tina%2520model%2520achieves%2520a%2520%253E20%255C%2525%2520reasoning%250Aperformance%2520increase%2520and%252043.33%255C%2525%2520Pass%25401%2520accuracy%2520on%2520AIME24%252C%2520at%2520only%2520%255C%25249%2520USD%250Apost-training%2520and%2520evaluation%2520cost%2520%2528i.e.%252C%2520an%2520estimated%2520260x%2520cost%2520reduction%2529.%2520Our%250Awork%2520reveals%2520the%2520surprising%2520effectiveness%2520of%2520efficient%2520RL%2520reasoning%2520via%2520LoRA.%250AWe%2520validate%2520this%2520across%2520multiple%2520open-source%2520reasoning%2520datasets%2520and%2520various%250Aablation%2520settings%2520starting%2520with%2520a%2520single%252C%2520fixed%2520set%2520of%2520hyperparameters.%250AFurthermore%252C%2520we%2520hypothesize%2520that%2520this%2520effectiveness%2520and%2520efficiency%2520stem%2520from%250ALoRA%2520rapidly%2520adapting%2520the%2520model%2520to%2520the%2520structural%2520format%2520of%2520reasoning%2520rewarded%250Aby%2520RL%252C%2520while%2520largely%2520preserving%2520the%2520base%2520model%2527s%2520underlying%2520knowledge.%2520In%250Aservice%2520of%2520accessibility%2520and%2520open%2520research%252C%2520we%2520fully%2520open-source%2520all%2520code%252C%250Atraining%2520logs%252C%2520and%2520model%2520weights%2520%255C%2526%2520checkpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tina%3A%20Tiny%20Reasoning%20Models%20via%20LoRA&entry.906535625=Shangshang%20Wang%20and%20Julian%20Asilis%20and%20%C3%96mer%20Faruk%20Akg%C3%BCl%20and%20Enes%20Burak%20Bilgin%20and%20Ollie%20Liu%20and%20Willie%20Neiswanger&entry.1292438233=%20%20How%20cost-effectively%20can%20strong%20reasoning%20abilities%20be%20achieved%20in%20language%0Amodels%3F%20Driven%20by%20this%20fundamental%20question%2C%20we%20present%20Tina%2C%20a%20family%20of%20tiny%0Areasoning%20models%20achieved%20with%20high%20cost-efficiency.%20Notably%2C%20Tina%20demonstrates%0Athat%20substantial%20reasoning%20performance%20can%20be%20developed%20using%20only%20minimal%0Aresources%2C%20by%20applying%20parameter-efficient%20updates%20during%20reinforcement%0Alearning%20%28RL%29%2C%20using%20low-rank%20adaptation%20%28LoRA%29%2C%20to%20an%20already%20tiny%201.5B%0Aparameter%20base%20model.%20This%20minimalist%20approach%20produces%20models%20that%20achieve%0Areasoning%20performance%20which%20is%20competitive%20with%2C%20and%20sometimes%20surpasses%2C%20SOTA%0ARL%20reasoning%20models%20built%20upon%20the%20same%20base%20model.%20Crucially%2C%20this%20is%20achieved%0Aat%20a%20tiny%20fraction%20of%20the%20computational%20post-training%20cost%20employed%20by%20existing%0ASOTA%20models.%20In%20fact%2C%20the%20best%20Tina%20model%20achieves%20a%20%3E20%5C%25%20reasoning%0Aperformance%20increase%20and%2043.33%5C%25%20Pass%401%20accuracy%20on%20AIME24%2C%20at%20only%20%5C%249%20USD%0Apost-training%20and%20evaluation%20cost%20%28i.e.%2C%20an%20estimated%20260x%20cost%20reduction%29.%20Our%0Awork%20reveals%20the%20surprising%20effectiveness%20of%20efficient%20RL%20reasoning%20via%20LoRA.%0AWe%20validate%20this%20across%20multiple%20open-source%20reasoning%20datasets%20and%20various%0Aablation%20settings%20starting%20with%20a%20single%2C%20fixed%20set%20of%20hyperparameters.%0AFurthermore%2C%20we%20hypothesize%20that%20this%20effectiveness%20and%20efficiency%20stem%20from%0ALoRA%20rapidly%20adapting%20the%20model%20to%20the%20structural%20format%20of%20reasoning%20rewarded%0Aby%20RL%2C%20while%20largely%20preserving%20the%20base%20model%27s%20underlying%20knowledge.%20In%0Aservice%20of%20accessibility%20and%20open%20research%2C%20we%20fully%20open-source%20all%20code%2C%0Atraining%20logs%2C%20and%20model%20weights%20%5C%26%20checkpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15777v1&entry.124074799=Read"},
{"title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient\n  Entanglement", "author": "Hui Yuan and Yifan Zeng and Yue Wu and Huazheng Wang and Mengdi Wang and Liu Leqi", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has become the predominant\napproach for language model (LM) alignment. At its core, RLHF uses a\nmargin-based loss for preference optimization, specifying ideal LM behavior\nonly by the difference between preferred and dispreferred responses. In this\npaper, we identify a common pitfall of margin-based methods -- the\nunder-specification of ideal LM behavior on preferred and dispreferred\nresponses individually, which leads to two unintended consequences as the\nmargin increases: (1) The probability of dispreferred (e.g., unsafe) responses\nmay increase, resulting in potential safety alignment failures. (2) The\nprobability of preferred responses may decrease, even when those responses are\nideal. We demystify the reasons behind these problematic behaviors:\nmargin-based losses couple the change in the preferred probability to the\ngradient of the dispreferred one, and vice versa, often preventing the\npreferred probability from increasing while the dispreferred one decreases, and\nthus causing a synchronized increase or decrease in both probabilities. We term\nthis effect, inherent in margin-based objectives, gradient entanglement.\nFormally, we derive conditions for general margin-based alignment objectives\nunder which gradient entanglement becomes concerning: the inner product of the\ngradients of preferred and dispreferred log-probabilities is large relative to\nthe individual gradient norms. We theoretically investigate why such inner\nproducts can be large when aligning language models and empirically validate\nour findings. Empirical implications of our framework extend to explaining\nimportant differences in the training dynamics of various preference\noptimization algorithms, and suggesting potential algorithm designs to mitigate\nthe under-specification issue of margin-based methods and thereby improving\nlanguage model alignment.\n", "link": "http://arxiv.org/abs/2410.13828v2", "date": "2025-04-22", "relevancy": 1.4126, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4733}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Common%20Pitfall%20of%20Margin-based%20Language%20Model%20Alignment%3A%20Gradient%0A%20%20Entanglement&body=Title%3A%20A%20Common%20Pitfall%20of%20Margin-based%20Language%20Model%20Alignment%3A%20Gradient%0A%20%20Entanglement%0AAuthor%3A%20Hui%20Yuan%20and%20Yifan%20Zeng%20and%20Yue%20Wu%20and%20Huazheng%20Wang%20and%20Mengdi%20Wang%20and%20Liu%20Leqi%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20become%20the%20predominant%0Aapproach%20for%20language%20model%20%28LM%29%20alignment.%20At%20its%20core%2C%20RLHF%20uses%20a%0Amargin-based%20loss%20for%20preference%20optimization%2C%20specifying%20ideal%20LM%20behavior%0Aonly%20by%20the%20difference%20between%20preferred%20and%20dispreferred%20responses.%20In%20this%0Apaper%2C%20we%20identify%20a%20common%20pitfall%20of%20margin-based%20methods%20--%20the%0Aunder-specification%20of%20ideal%20LM%20behavior%20on%20preferred%20and%20dispreferred%0Aresponses%20individually%2C%20which%20leads%20to%20two%20unintended%20consequences%20as%20the%0Amargin%20increases%3A%20%281%29%20The%20probability%20of%20dispreferred%20%28e.g.%2C%20unsafe%29%20responses%0Amay%20increase%2C%20resulting%20in%20potential%20safety%20alignment%20failures.%20%282%29%20The%0Aprobability%20of%20preferred%20responses%20may%20decrease%2C%20even%20when%20those%20responses%20are%0Aideal.%20We%20demystify%20the%20reasons%20behind%20these%20problematic%20behaviors%3A%0Amargin-based%20losses%20couple%20the%20change%20in%20the%20preferred%20probability%20to%20the%0Agradient%20of%20the%20dispreferred%20one%2C%20and%20vice%20versa%2C%20often%20preventing%20the%0Apreferred%20probability%20from%20increasing%20while%20the%20dispreferred%20one%20decreases%2C%20and%0Athus%20causing%20a%20synchronized%20increase%20or%20decrease%20in%20both%20probabilities.%20We%20term%0Athis%20effect%2C%20inherent%20in%20margin-based%20objectives%2C%20gradient%20entanglement.%0AFormally%2C%20we%20derive%20conditions%20for%20general%20margin-based%20alignment%20objectives%0Aunder%20which%20gradient%20entanglement%20becomes%20concerning%3A%20the%20inner%20product%20of%20the%0Agradients%20of%20preferred%20and%20dispreferred%20log-probabilities%20is%20large%20relative%20to%0Athe%20individual%20gradient%20norms.%20We%20theoretically%20investigate%20why%20such%20inner%0Aproducts%20can%20be%20large%20when%20aligning%20language%20models%20and%20empirically%20validate%0Aour%20findings.%20Empirical%20implications%20of%20our%20framework%20extend%20to%20explaining%0Aimportant%20differences%20in%20the%20training%20dynamics%20of%20various%20preference%0Aoptimization%20algorithms%2C%20and%20suggesting%20potential%20algorithm%20designs%20to%20mitigate%0Athe%20under-specification%20issue%20of%20margin-based%20methods%20and%20thereby%20improving%0Alanguage%20model%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Common%2520Pitfall%2520of%2520Margin-based%2520Language%2520Model%2520Alignment%253A%2520Gradient%250A%2520%2520Entanglement%26entry.906535625%3DHui%2520Yuan%2520and%2520Yifan%2520Zeng%2520and%2520Yue%2520Wu%2520and%2520Huazheng%2520Wang%2520and%2520Mengdi%2520Wang%2520and%2520Liu%2520Leqi%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520has%2520become%2520the%2520predominant%250Aapproach%2520for%2520language%2520model%2520%2528LM%2529%2520alignment.%2520At%2520its%2520core%252C%2520RLHF%2520uses%2520a%250Amargin-based%2520loss%2520for%2520preference%2520optimization%252C%2520specifying%2520ideal%2520LM%2520behavior%250Aonly%2520by%2520the%2520difference%2520between%2520preferred%2520and%2520dispreferred%2520responses.%2520In%2520this%250Apaper%252C%2520we%2520identify%2520a%2520common%2520pitfall%2520of%2520margin-based%2520methods%2520--%2520the%250Aunder-specification%2520of%2520ideal%2520LM%2520behavior%2520on%2520preferred%2520and%2520dispreferred%250Aresponses%2520individually%252C%2520which%2520leads%2520to%2520two%2520unintended%2520consequences%2520as%2520the%250Amargin%2520increases%253A%2520%25281%2529%2520The%2520probability%2520of%2520dispreferred%2520%2528e.g.%252C%2520unsafe%2529%2520responses%250Amay%2520increase%252C%2520resulting%2520in%2520potential%2520safety%2520alignment%2520failures.%2520%25282%2529%2520The%250Aprobability%2520of%2520preferred%2520responses%2520may%2520decrease%252C%2520even%2520when%2520those%2520responses%2520are%250Aideal.%2520We%2520demystify%2520the%2520reasons%2520behind%2520these%2520problematic%2520behaviors%253A%250Amargin-based%2520losses%2520couple%2520the%2520change%2520in%2520the%2520preferred%2520probability%2520to%2520the%250Agradient%2520of%2520the%2520dispreferred%2520one%252C%2520and%2520vice%2520versa%252C%2520often%2520preventing%2520the%250Apreferred%2520probability%2520from%2520increasing%2520while%2520the%2520dispreferred%2520one%2520decreases%252C%2520and%250Athus%2520causing%2520a%2520synchronized%2520increase%2520or%2520decrease%2520in%2520both%2520probabilities.%2520We%2520term%250Athis%2520effect%252C%2520inherent%2520in%2520margin-based%2520objectives%252C%2520gradient%2520entanglement.%250AFormally%252C%2520we%2520derive%2520conditions%2520for%2520general%2520margin-based%2520alignment%2520objectives%250Aunder%2520which%2520gradient%2520entanglement%2520becomes%2520concerning%253A%2520the%2520inner%2520product%2520of%2520the%250Agradients%2520of%2520preferred%2520and%2520dispreferred%2520log-probabilities%2520is%2520large%2520relative%2520to%250Athe%2520individual%2520gradient%2520norms.%2520We%2520theoretically%2520investigate%2520why%2520such%2520inner%250Aproducts%2520can%2520be%2520large%2520when%2520aligning%2520language%2520models%2520and%2520empirically%2520validate%250Aour%2520findings.%2520Empirical%2520implications%2520of%2520our%2520framework%2520extend%2520to%2520explaining%250Aimportant%2520differences%2520in%2520the%2520training%2520dynamics%2520of%2520various%2520preference%250Aoptimization%2520algorithms%252C%2520and%2520suggesting%2520potential%2520algorithm%2520designs%2520to%2520mitigate%250Athe%2520under-specification%2520issue%2520of%2520margin-based%2520methods%2520and%2520thereby%2520improving%250Alanguage%2520model%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Common%20Pitfall%20of%20Margin-based%20Language%20Model%20Alignment%3A%20Gradient%0A%20%20Entanglement&entry.906535625=Hui%20Yuan%20and%20Yifan%20Zeng%20and%20Yue%20Wu%20and%20Huazheng%20Wang%20and%20Mengdi%20Wang%20and%20Liu%20Leqi&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20become%20the%20predominant%0Aapproach%20for%20language%20model%20%28LM%29%20alignment.%20At%20its%20core%2C%20RLHF%20uses%20a%0Amargin-based%20loss%20for%20preference%20optimization%2C%20specifying%20ideal%20LM%20behavior%0Aonly%20by%20the%20difference%20between%20preferred%20and%20dispreferred%20responses.%20In%20this%0Apaper%2C%20we%20identify%20a%20common%20pitfall%20of%20margin-based%20methods%20--%20the%0Aunder-specification%20of%20ideal%20LM%20behavior%20on%20preferred%20and%20dispreferred%0Aresponses%20individually%2C%20which%20leads%20to%20two%20unintended%20consequences%20as%20the%0Amargin%20increases%3A%20%281%29%20The%20probability%20of%20dispreferred%20%28e.g.%2C%20unsafe%29%20responses%0Amay%20increase%2C%20resulting%20in%20potential%20safety%20alignment%20failures.%20%282%29%20The%0Aprobability%20of%20preferred%20responses%20may%20decrease%2C%20even%20when%20those%20responses%20are%0Aideal.%20We%20demystify%20the%20reasons%20behind%20these%20problematic%20behaviors%3A%0Amargin-based%20losses%20couple%20the%20change%20in%20the%20preferred%20probability%20to%20the%0Agradient%20of%20the%20dispreferred%20one%2C%20and%20vice%20versa%2C%20often%20preventing%20the%0Apreferred%20probability%20from%20increasing%20while%20the%20dispreferred%20one%20decreases%2C%20and%0Athus%20causing%20a%20synchronized%20increase%20or%20decrease%20in%20both%20probabilities.%20We%20term%0Athis%20effect%2C%20inherent%20in%20margin-based%20objectives%2C%20gradient%20entanglement.%0AFormally%2C%20we%20derive%20conditions%20for%20general%20margin-based%20alignment%20objectives%0Aunder%20which%20gradient%20entanglement%20becomes%20concerning%3A%20the%20inner%20product%20of%20the%0Agradients%20of%20preferred%20and%20dispreferred%20log-probabilities%20is%20large%20relative%20to%0Athe%20individual%20gradient%20norms.%20We%20theoretically%20investigate%20why%20such%20inner%0Aproducts%20can%20be%20large%20when%20aligning%20language%20models%20and%20empirically%20validate%0Aour%20findings.%20Empirical%20implications%20of%20our%20framework%20extend%20to%20explaining%0Aimportant%20differences%20in%20the%20training%20dynamics%20of%20various%20preference%0Aoptimization%20algorithms%2C%20and%20suggesting%20potential%20algorithm%20designs%20to%20mitigate%0Athe%20under-specification%20issue%20of%20margin-based%20methods%20and%20thereby%20improving%0Alanguage%20model%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13828v2&entry.124074799=Read"},
{"title": "A Mechanism-Learning Deeply Coupled Model for Remote Sensing Retrieval\n  of Global Land Surface Temperature", "author": "Tian Xie and Menghui Jiang and Huanfeng Shen and Huifang Li and Chao Zeng and Jun Ma and Guanhao Zhang and Liangpei Zhang", "abstract": "  Land surface temperature (LST) retrieval from remote sensing data is pivotal\nfor analyzing climate processes and surface energy budgets. However, LST\nretrieval is an ill-posed inverse problem, which becomes particularly severe\nwhen only a single band is available. In this paper, we propose a deeply\ncoupled framework integrating mechanistic modeling and machine learning to\nenhance the accuracy and generalizability of single-channel LST retrieval.\nTraining samples are generated using a physically-based radiative transfer\nmodel and a global collection of 5810 atmospheric profiles. A physics-informed\nmachine learning framework is proposed to systematically incorporate the first\nprinciples from classical physical inversion models into the learning workflow,\nwith optimization constrained by radiative transfer equations. Global\nvalidation demonstrated a 30% reduction in root-mean-square error versus\nstandalone methods. Under extreme humidity, the mean absolute error decreased\nfrom 4.87 K to 2.29 K (53% improvement). Continental-scale tests across five\ncontinents confirmed the superior generalizability of this model.\n", "link": "http://arxiv.org/abs/2504.07481v3", "date": "2025-04-22", "relevancy": 1.4326, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4833}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mechanism-Learning%20Deeply%20Coupled%20Model%20for%20Remote%20Sensing%20Retrieval%0A%20%20of%20Global%20Land%20Surface%20Temperature&body=Title%3A%20A%20Mechanism-Learning%20Deeply%20Coupled%20Model%20for%20Remote%20Sensing%20Retrieval%0A%20%20of%20Global%20Land%20Surface%20Temperature%0AAuthor%3A%20Tian%20Xie%20and%20Menghui%20Jiang%20and%20Huanfeng%20Shen%20and%20Huifang%20Li%20and%20Chao%20Zeng%20and%20Jun%20Ma%20and%20Guanhao%20Zhang%20and%20Liangpei%20Zhang%0AAbstract%3A%20%20%20Land%20surface%20temperature%20%28LST%29%20retrieval%20from%20remote%20sensing%20data%20is%20pivotal%0Afor%20analyzing%20climate%20processes%20and%20surface%20energy%20budgets.%20However%2C%20LST%0Aretrieval%20is%20an%20ill-posed%20inverse%20problem%2C%20which%20becomes%20particularly%20severe%0Awhen%20only%20a%20single%20band%20is%20available.%20In%20this%20paper%2C%20we%20propose%20a%20deeply%0Acoupled%20framework%20integrating%20mechanistic%20modeling%20and%20machine%20learning%20to%0Aenhance%20the%20accuracy%20and%20generalizability%20of%20single-channel%20LST%20retrieval.%0ATraining%20samples%20are%20generated%20using%20a%20physically-based%20radiative%20transfer%0Amodel%20and%20a%20global%20collection%20of%205810%20atmospheric%20profiles.%20A%20physics-informed%0Amachine%20learning%20framework%20is%20proposed%20to%20systematically%20incorporate%20the%20first%0Aprinciples%20from%20classical%20physical%20inversion%20models%20into%20the%20learning%20workflow%2C%0Awith%20optimization%20constrained%20by%20radiative%20transfer%20equations.%20Global%0Avalidation%20demonstrated%20a%2030%25%20reduction%20in%20root-mean-square%20error%20versus%0Astandalone%20methods.%20Under%20extreme%20humidity%2C%20the%20mean%20absolute%20error%20decreased%0Afrom%204.87%20K%20to%202.29%20K%20%2853%25%20improvement%29.%20Continental-scale%20tests%20across%20five%0Acontinents%20confirmed%20the%20superior%20generalizability%20of%20this%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07481v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mechanism-Learning%2520Deeply%2520Coupled%2520Model%2520for%2520Remote%2520Sensing%2520Retrieval%250A%2520%2520of%2520Global%2520Land%2520Surface%2520Temperature%26entry.906535625%3DTian%2520Xie%2520and%2520Menghui%2520Jiang%2520and%2520Huanfeng%2520Shen%2520and%2520Huifang%2520Li%2520and%2520Chao%2520Zeng%2520and%2520Jun%2520Ma%2520and%2520Guanhao%2520Zhang%2520and%2520Liangpei%2520Zhang%26entry.1292438233%3D%2520%2520Land%2520surface%2520temperature%2520%2528LST%2529%2520retrieval%2520from%2520remote%2520sensing%2520data%2520is%2520pivotal%250Afor%2520analyzing%2520climate%2520processes%2520and%2520surface%2520energy%2520budgets.%2520However%252C%2520LST%250Aretrieval%2520is%2520an%2520ill-posed%2520inverse%2520problem%252C%2520which%2520becomes%2520particularly%2520severe%250Awhen%2520only%2520a%2520single%2520band%2520is%2520available.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520deeply%250Acoupled%2520framework%2520integrating%2520mechanistic%2520modeling%2520and%2520machine%2520learning%2520to%250Aenhance%2520the%2520accuracy%2520and%2520generalizability%2520of%2520single-channel%2520LST%2520retrieval.%250ATraining%2520samples%2520are%2520generated%2520using%2520a%2520physically-based%2520radiative%2520transfer%250Amodel%2520and%2520a%2520global%2520collection%2520of%25205810%2520atmospheric%2520profiles.%2520A%2520physics-informed%250Amachine%2520learning%2520framework%2520is%2520proposed%2520to%2520systematically%2520incorporate%2520the%2520first%250Aprinciples%2520from%2520classical%2520physical%2520inversion%2520models%2520into%2520the%2520learning%2520workflow%252C%250Awith%2520optimization%2520constrained%2520by%2520radiative%2520transfer%2520equations.%2520Global%250Avalidation%2520demonstrated%2520a%252030%2525%2520reduction%2520in%2520root-mean-square%2520error%2520versus%250Astandalone%2520methods.%2520Under%2520extreme%2520humidity%252C%2520the%2520mean%2520absolute%2520error%2520decreased%250Afrom%25204.87%2520K%2520to%25202.29%2520K%2520%252853%2525%2520improvement%2529.%2520Continental-scale%2520tests%2520across%2520five%250Acontinents%2520confirmed%2520the%2520superior%2520generalizability%2520of%2520this%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07481v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mechanism-Learning%20Deeply%20Coupled%20Model%20for%20Remote%20Sensing%20Retrieval%0A%20%20of%20Global%20Land%20Surface%20Temperature&entry.906535625=Tian%20Xie%20and%20Menghui%20Jiang%20and%20Huanfeng%20Shen%20and%20Huifang%20Li%20and%20Chao%20Zeng%20and%20Jun%20Ma%20and%20Guanhao%20Zhang%20and%20Liangpei%20Zhang&entry.1292438233=%20%20Land%20surface%20temperature%20%28LST%29%20retrieval%20from%20remote%20sensing%20data%20is%20pivotal%0Afor%20analyzing%20climate%20processes%20and%20surface%20energy%20budgets.%20However%2C%20LST%0Aretrieval%20is%20an%20ill-posed%20inverse%20problem%2C%20which%20becomes%20particularly%20severe%0Awhen%20only%20a%20single%20band%20is%20available.%20In%20this%20paper%2C%20we%20propose%20a%20deeply%0Acoupled%20framework%20integrating%20mechanistic%20modeling%20and%20machine%20learning%20to%0Aenhance%20the%20accuracy%20and%20generalizability%20of%20single-channel%20LST%20retrieval.%0ATraining%20samples%20are%20generated%20using%20a%20physically-based%20radiative%20transfer%0Amodel%20and%20a%20global%20collection%20of%205810%20atmospheric%20profiles.%20A%20physics-informed%0Amachine%20learning%20framework%20is%20proposed%20to%20systematically%20incorporate%20the%20first%0Aprinciples%20from%20classical%20physical%20inversion%20models%20into%20the%20learning%20workflow%2C%0Awith%20optimization%20constrained%20by%20radiative%20transfer%20equations.%20Global%0Avalidation%20demonstrated%20a%2030%25%20reduction%20in%20root-mean-square%20error%20versus%0Astandalone%20methods.%20Under%20extreme%20humidity%2C%20the%20mean%20absolute%20error%20decreased%0Afrom%204.87%20K%20to%202.29%20K%20%2853%25%20improvement%29.%20Continental-scale%20tests%20across%20five%0Acontinents%20confirmed%20the%20superior%20generalizability%20of%20this%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07481v3&entry.124074799=Read"},
{"title": "A Conceptual Model for Attributions in Event-Centric Knowledge Graphs", "author": "Florian Pl\u00f6tzky and Katarina Britz and Wolf-Tilo Balke", "abstract": "  The use of narratives as a means of fusing information from knowledge graphs\n(KGs) into a coherent line of argumentation has been the subject of recent\ninvestigation. Narratives are especially useful in event-centric knowledge\ngraphs in that they provide a means to connect different real-world events and\ncategorize them by well-known narrations. However, specifically for\ncontroversial events, a problem in information fusion arises, namely, multiple\nviewpoints regarding the validity of certain event aspects, e.g., regarding the\nrole a participant takes in an event, may exist. Expressing those viewpoints in\nKGs is challenging because disputed information provided by different\nviewpoints may introduce inconsistencies. Hence, most KGs only feature a single\nview on the contained information, hampering the effectiveness of narrative\ninformation access. This paper is an extension of our original work and\nintroduces attributions, i.e., parameterized predicates that allow for the\nrepresentation of facts that are only valid in a specific viewpoint. For this,\nwe develop a conceptual model that allows for the representation of\nviewpoint-dependent information. As an extension, we enhance the model by a\nconception of viewpoint-compatibility. Based on this, we deepen our original\ndeliberations on the model's effects on information fusion and provide\nadditional grounding in the literature.\n", "link": "http://arxiv.org/abs/2503.03563v2", "date": "2025-04-22", "relevancy": 1.7138, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Conceptual%20Model%20for%20Attributions%20in%20Event-Centric%20Knowledge%20Graphs&body=Title%3A%20A%20Conceptual%20Model%20for%20Attributions%20in%20Event-Centric%20Knowledge%20Graphs%0AAuthor%3A%20Florian%20Pl%C3%B6tzky%20and%20Katarina%20Britz%20and%20Wolf-Tilo%20Balke%0AAbstract%3A%20%20%20The%20use%20of%20narratives%20as%20a%20means%20of%20fusing%20information%20from%20knowledge%20graphs%0A%28KGs%29%20into%20a%20coherent%20line%20of%20argumentation%20has%20been%20the%20subject%20of%20recent%0Ainvestigation.%20Narratives%20are%20especially%20useful%20in%20event-centric%20knowledge%0Agraphs%20in%20that%20they%20provide%20a%20means%20to%20connect%20different%20real-world%20events%20and%0Acategorize%20them%20by%20well-known%20narrations.%20However%2C%20specifically%20for%0Acontroversial%20events%2C%20a%20problem%20in%20information%20fusion%20arises%2C%20namely%2C%20multiple%0Aviewpoints%20regarding%20the%20validity%20of%20certain%20event%20aspects%2C%20e.g.%2C%20regarding%20the%0Arole%20a%20participant%20takes%20in%20an%20event%2C%20may%20exist.%20Expressing%20those%20viewpoints%20in%0AKGs%20is%20challenging%20because%20disputed%20information%20provided%20by%20different%0Aviewpoints%20may%20introduce%20inconsistencies.%20Hence%2C%20most%20KGs%20only%20feature%20a%20single%0Aview%20on%20the%20contained%20information%2C%20hampering%20the%20effectiveness%20of%20narrative%0Ainformation%20access.%20This%20paper%20is%20an%20extension%20of%20our%20original%20work%20and%0Aintroduces%20attributions%2C%20i.e.%2C%20parameterized%20predicates%20that%20allow%20for%20the%0Arepresentation%20of%20facts%20that%20are%20only%20valid%20in%20a%20specific%20viewpoint.%20For%20this%2C%0Awe%20develop%20a%20conceptual%20model%20that%20allows%20for%20the%20representation%20of%0Aviewpoint-dependent%20information.%20As%20an%20extension%2C%20we%20enhance%20the%20model%20by%20a%0Aconception%20of%20viewpoint-compatibility.%20Based%20on%20this%2C%20we%20deepen%20our%20original%0Adeliberations%20on%20the%20model%27s%20effects%20on%20information%20fusion%20and%20provide%0Aadditional%20grounding%20in%20the%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03563v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Conceptual%2520Model%2520for%2520Attributions%2520in%2520Event-Centric%2520Knowledge%2520Graphs%26entry.906535625%3DFlorian%2520Pl%25C3%25B6tzky%2520and%2520Katarina%2520Britz%2520and%2520Wolf-Tilo%2520Balke%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520narratives%2520as%2520a%2520means%2520of%2520fusing%2520information%2520from%2520knowledge%2520graphs%250A%2528KGs%2529%2520into%2520a%2520coherent%2520line%2520of%2520argumentation%2520has%2520been%2520the%2520subject%2520of%2520recent%250Ainvestigation.%2520Narratives%2520are%2520especially%2520useful%2520in%2520event-centric%2520knowledge%250Agraphs%2520in%2520that%2520they%2520provide%2520a%2520means%2520to%2520connect%2520different%2520real-world%2520events%2520and%250Acategorize%2520them%2520by%2520well-known%2520narrations.%2520However%252C%2520specifically%2520for%250Acontroversial%2520events%252C%2520a%2520problem%2520in%2520information%2520fusion%2520arises%252C%2520namely%252C%2520multiple%250Aviewpoints%2520regarding%2520the%2520validity%2520of%2520certain%2520event%2520aspects%252C%2520e.g.%252C%2520regarding%2520the%250Arole%2520a%2520participant%2520takes%2520in%2520an%2520event%252C%2520may%2520exist.%2520Expressing%2520those%2520viewpoints%2520in%250AKGs%2520is%2520challenging%2520because%2520disputed%2520information%2520provided%2520by%2520different%250Aviewpoints%2520may%2520introduce%2520inconsistencies.%2520Hence%252C%2520most%2520KGs%2520only%2520feature%2520a%2520single%250Aview%2520on%2520the%2520contained%2520information%252C%2520hampering%2520the%2520effectiveness%2520of%2520narrative%250Ainformation%2520access.%2520This%2520paper%2520is%2520an%2520extension%2520of%2520our%2520original%2520work%2520and%250Aintroduces%2520attributions%252C%2520i.e.%252C%2520parameterized%2520predicates%2520that%2520allow%2520for%2520the%250Arepresentation%2520of%2520facts%2520that%2520are%2520only%2520valid%2520in%2520a%2520specific%2520viewpoint.%2520For%2520this%252C%250Awe%2520develop%2520a%2520conceptual%2520model%2520that%2520allows%2520for%2520the%2520representation%2520of%250Aviewpoint-dependent%2520information.%2520As%2520an%2520extension%252C%2520we%2520enhance%2520the%2520model%2520by%2520a%250Aconception%2520of%2520viewpoint-compatibility.%2520Based%2520on%2520this%252C%2520we%2520deepen%2520our%2520original%250Adeliberations%2520on%2520the%2520model%2527s%2520effects%2520on%2520information%2520fusion%2520and%2520provide%250Aadditional%2520grounding%2520in%2520the%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03563v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Conceptual%20Model%20for%20Attributions%20in%20Event-Centric%20Knowledge%20Graphs&entry.906535625=Florian%20Pl%C3%B6tzky%20and%20Katarina%20Britz%20and%20Wolf-Tilo%20Balke&entry.1292438233=%20%20The%20use%20of%20narratives%20as%20a%20means%20of%20fusing%20information%20from%20knowledge%20graphs%0A%28KGs%29%20into%20a%20coherent%20line%20of%20argumentation%20has%20been%20the%20subject%20of%20recent%0Ainvestigation.%20Narratives%20are%20especially%20useful%20in%20event-centric%20knowledge%0Agraphs%20in%20that%20they%20provide%20a%20means%20to%20connect%20different%20real-world%20events%20and%0Acategorize%20them%20by%20well-known%20narrations.%20However%2C%20specifically%20for%0Acontroversial%20events%2C%20a%20problem%20in%20information%20fusion%20arises%2C%20namely%2C%20multiple%0Aviewpoints%20regarding%20the%20validity%20of%20certain%20event%20aspects%2C%20e.g.%2C%20regarding%20the%0Arole%20a%20participant%20takes%20in%20an%20event%2C%20may%20exist.%20Expressing%20those%20viewpoints%20in%0AKGs%20is%20challenging%20because%20disputed%20information%20provided%20by%20different%0Aviewpoints%20may%20introduce%20inconsistencies.%20Hence%2C%20most%20KGs%20only%20feature%20a%20single%0Aview%20on%20the%20contained%20information%2C%20hampering%20the%20effectiveness%20of%20narrative%0Ainformation%20access.%20This%20paper%20is%20an%20extension%20of%20our%20original%20work%20and%0Aintroduces%20attributions%2C%20i.e.%2C%20parameterized%20predicates%20that%20allow%20for%20the%0Arepresentation%20of%20facts%20that%20are%20only%20valid%20in%20a%20specific%20viewpoint.%20For%20this%2C%0Awe%20develop%20a%20conceptual%20model%20that%20allows%20for%20the%20representation%20of%0Aviewpoint-dependent%20information.%20As%20an%20extension%2C%20we%20enhance%20the%20model%20by%20a%0Aconception%20of%20viewpoint-compatibility.%20Based%20on%20this%2C%20we%20deepen%20our%20original%0Adeliberations%20on%20the%20model%27s%20effects%20on%20information%20fusion%20and%20provide%0Aadditional%20grounding%20in%20the%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03563v2&entry.124074799=Read"},
{"title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents", "author": "Siyu Zhou and Tianyi Zhou and Yijun Yang and Guodong Long and Deheng Ye and Jing Jiang and Chengqi Zhang", "abstract": "  Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.\n", "link": "http://arxiv.org/abs/2504.15785v1", "date": "2025-04-22", "relevancy": 1.6093, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5702}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5336}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WALL-E%202.0%3A%20World%20Alignment%20by%20NeuroSymbolic%20Learning%20improves%20World%0A%20%20Model-based%20LLM%20Agents&body=Title%3A%20WALL-E%202.0%3A%20World%20Alignment%20by%20NeuroSymbolic%20Learning%20improves%20World%0A%20%20Model-based%20LLM%20Agents%0AAuthor%3A%20Siyu%20Zhou%20and%20Tianyi%20Zhou%20and%20Yijun%20Yang%20and%20Guodong%20Long%20and%20Deheng%20Ye%20and%20Jing%20Jiang%20and%20Chengqi%20Zhang%0AAbstract%3A%20%20%20Can%20we%20build%20accurate%20world%20models%20out%20of%20large%20language%20models%20%28LLMs%29%3F%20How%0Acan%20world%20models%20benefit%20LLM%20agents%3F%20The%20gap%20between%20the%20prior%20knowledge%20of%0ALLMs%20and%20the%20specified%20environment%27s%20dynamics%20usually%20bottlenecks%20LLMs%27%0Aperformance%20as%20world%20models.%20To%20bridge%20the%20gap%2C%20we%20propose%20a%20training-free%0A%22world%20alignment%22%20that%20learns%20an%20environment%27s%20symbolic%20knowledge%20complementary%0Ato%20LLMs.%20The%20symbolic%20knowledge%20covers%20action%20rules%2C%20knowledge%20graphs%2C%20and%0Ascene%20graphs%2C%20which%20are%20extracted%20by%20LLMs%20from%20exploration%20trajectories%20and%0Aencoded%20into%20executable%20codes%20to%20regulate%20LLM%20agents%27%20policies.%20We%20further%0Apropose%20an%20RL-free%2C%20model-based%20agent%20%22WALL-E%202.0%22%20through%20the%20model-predictive%0Acontrol%20%28MPC%29%20framework.%20Unlike%20classical%20MPC%20requiring%20costly%20optimization%20on%0Athe%20fly%2C%20we%20adopt%20an%20LLM%20agent%20as%20an%20efficient%20look-ahead%20optimizer%20of%20future%0Asteps%27%20actions%20by%20interacting%20with%20the%20neurosymbolic%20world%20model.%20While%20the%20LLM%0Aagent%27s%20strong%20heuristics%20make%20it%20an%20efficient%20planner%20in%20MPC%2C%20the%20quality%20of%0Aits%20planned%20actions%20is%20also%20secured%20by%20the%20accurate%20predictions%20of%20the%20aligned%0Aworld%20model.%20They%20together%20considerably%20improve%20learning%20efficiency%20in%20a%20new%0Aenvironment.%20On%20open-world%20challenges%20in%20Mars%20%28Minecraft%20like%29%20and%20ALFWorld%0A%28embodied%20indoor%20environments%29%2C%20WALL-E%202.0%20significantly%20outperforms%20existing%0Amethods%2C%20e.g.%2C%20surpassing%20baselines%20in%20Mars%20by%2016.1%25-51.6%25%20of%20success%20rate%20and%0Aby%20at%20least%2061.7%25%20in%20score.%20In%20ALFWorld%2C%20it%20achieves%20a%20new%20record%2098%25%20success%0Arate%20after%20only%204%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWALL-E%25202.0%253A%2520World%2520Alignment%2520by%2520NeuroSymbolic%2520Learning%2520improves%2520World%250A%2520%2520Model-based%2520LLM%2520Agents%26entry.906535625%3DSiyu%2520Zhou%2520and%2520Tianyi%2520Zhou%2520and%2520Yijun%2520Yang%2520and%2520Guodong%2520Long%2520and%2520Deheng%2520Ye%2520and%2520Jing%2520Jiang%2520and%2520Chengqi%2520Zhang%26entry.1292438233%3D%2520%2520Can%2520we%2520build%2520accurate%2520world%2520models%2520out%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%253F%2520How%250Acan%2520world%2520models%2520benefit%2520LLM%2520agents%253F%2520The%2520gap%2520between%2520the%2520prior%2520knowledge%2520of%250ALLMs%2520and%2520the%2520specified%2520environment%2527s%2520dynamics%2520usually%2520bottlenecks%2520LLMs%2527%250Aperformance%2520as%2520world%2520models.%2520To%2520bridge%2520the%2520gap%252C%2520we%2520propose%2520a%2520training-free%250A%2522world%2520alignment%2522%2520that%2520learns%2520an%2520environment%2527s%2520symbolic%2520knowledge%2520complementary%250Ato%2520LLMs.%2520The%2520symbolic%2520knowledge%2520covers%2520action%2520rules%252C%2520knowledge%2520graphs%252C%2520and%250Ascene%2520graphs%252C%2520which%2520are%2520extracted%2520by%2520LLMs%2520from%2520exploration%2520trajectories%2520and%250Aencoded%2520into%2520executable%2520codes%2520to%2520regulate%2520LLM%2520agents%2527%2520policies.%2520We%2520further%250Apropose%2520an%2520RL-free%252C%2520model-based%2520agent%2520%2522WALL-E%25202.0%2522%2520through%2520the%2520model-predictive%250Acontrol%2520%2528MPC%2529%2520framework.%2520Unlike%2520classical%2520MPC%2520requiring%2520costly%2520optimization%2520on%250Athe%2520fly%252C%2520we%2520adopt%2520an%2520LLM%2520agent%2520as%2520an%2520efficient%2520look-ahead%2520optimizer%2520of%2520future%250Asteps%2527%2520actions%2520by%2520interacting%2520with%2520the%2520neurosymbolic%2520world%2520model.%2520While%2520the%2520LLM%250Aagent%2527s%2520strong%2520heuristics%2520make%2520it%2520an%2520efficient%2520planner%2520in%2520MPC%252C%2520the%2520quality%2520of%250Aits%2520planned%2520actions%2520is%2520also%2520secured%2520by%2520the%2520accurate%2520predictions%2520of%2520the%2520aligned%250Aworld%2520model.%2520They%2520together%2520considerably%2520improve%2520learning%2520efficiency%2520in%2520a%2520new%250Aenvironment.%2520On%2520open-world%2520challenges%2520in%2520Mars%2520%2528Minecraft%2520like%2529%2520and%2520ALFWorld%250A%2528embodied%2520indoor%2520environments%2529%252C%2520WALL-E%25202.0%2520significantly%2520outperforms%2520existing%250Amethods%252C%2520e.g.%252C%2520surpassing%2520baselines%2520in%2520Mars%2520by%252016.1%2525-51.6%2525%2520of%2520success%2520rate%2520and%250Aby%2520at%2520least%252061.7%2525%2520in%2520score.%2520In%2520ALFWorld%252C%2520it%2520achieves%2520a%2520new%2520record%252098%2525%2520success%250Arate%2520after%2520only%25204%2520iterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WALL-E%202.0%3A%20World%20Alignment%20by%20NeuroSymbolic%20Learning%20improves%20World%0A%20%20Model-based%20LLM%20Agents&entry.906535625=Siyu%20Zhou%20and%20Tianyi%20Zhou%20and%20Yijun%20Yang%20and%20Guodong%20Long%20and%20Deheng%20Ye%20and%20Jing%20Jiang%20and%20Chengqi%20Zhang&entry.1292438233=%20%20Can%20we%20build%20accurate%20world%20models%20out%20of%20large%20language%20models%20%28LLMs%29%3F%20How%0Acan%20world%20models%20benefit%20LLM%20agents%3F%20The%20gap%20between%20the%20prior%20knowledge%20of%0ALLMs%20and%20the%20specified%20environment%27s%20dynamics%20usually%20bottlenecks%20LLMs%27%0Aperformance%20as%20world%20models.%20To%20bridge%20the%20gap%2C%20we%20propose%20a%20training-free%0A%22world%20alignment%22%20that%20learns%20an%20environment%27s%20symbolic%20knowledge%20complementary%0Ato%20LLMs.%20The%20symbolic%20knowledge%20covers%20action%20rules%2C%20knowledge%20graphs%2C%20and%0Ascene%20graphs%2C%20which%20are%20extracted%20by%20LLMs%20from%20exploration%20trajectories%20and%0Aencoded%20into%20executable%20codes%20to%20regulate%20LLM%20agents%27%20policies.%20We%20further%0Apropose%20an%20RL-free%2C%20model-based%20agent%20%22WALL-E%202.0%22%20through%20the%20model-predictive%0Acontrol%20%28MPC%29%20framework.%20Unlike%20classical%20MPC%20requiring%20costly%20optimization%20on%0Athe%20fly%2C%20we%20adopt%20an%20LLM%20agent%20as%20an%20efficient%20look-ahead%20optimizer%20of%20future%0Asteps%27%20actions%20by%20interacting%20with%20the%20neurosymbolic%20world%20model.%20While%20the%20LLM%0Aagent%27s%20strong%20heuristics%20make%20it%20an%20efficient%20planner%20in%20MPC%2C%20the%20quality%20of%0Aits%20planned%20actions%20is%20also%20secured%20by%20the%20accurate%20predictions%20of%20the%20aligned%0Aworld%20model.%20They%20together%20considerably%20improve%20learning%20efficiency%20in%20a%20new%0Aenvironment.%20On%20open-world%20challenges%20in%20Mars%20%28Minecraft%20like%29%20and%20ALFWorld%0A%28embodied%20indoor%20environments%29%2C%20WALL-E%202.0%20significantly%20outperforms%20existing%0Amethods%2C%20e.g.%2C%20surpassing%20baselines%20in%20Mars%20by%2016.1%25-51.6%25%20of%20success%20rate%20and%0Aby%20at%20least%2061.7%25%20in%20score.%20In%20ALFWorld%2C%20it%20achieves%20a%20new%20record%2098%25%20success%0Arate%20after%20only%204%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15785v1&entry.124074799=Read"},
{"title": "Transition of $\u03b1$-mixing in Random Iterations with Applications in\n  Queuing Theory", "author": "Attila Lovas", "abstract": "  Nonlinear time series models with exogenous regressors are essential in\neconometrics, queuing theory, and machine learning, though their statistical\nanalysis remains incomplete. Key results, such as the law of large numbers and\nthe functional central limit theorem, are known for weakly dependent variables.\nWe demonstrate the transfer of mixing properties from the exogenous regressor\nto the response via coupling arguments. Additionally, we study Markov chains in\nrandom environments with drift and minorization conditions, even under\nnon-stationary environments with favorable mixing properties, and apply this\nframework to single-server queuing models.\n", "link": "http://arxiv.org/abs/2410.05056v3", "date": "2025-04-22", "relevancy": 0.7898, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3983}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3969}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transition%20of%20%24%CE%B1%24-mixing%20in%20Random%20Iterations%20with%20Applications%20in%0A%20%20Queuing%20Theory&body=Title%3A%20Transition%20of%20%24%CE%B1%24-mixing%20in%20Random%20Iterations%20with%20Applications%20in%0A%20%20Queuing%20Theory%0AAuthor%3A%20Attila%20Lovas%0AAbstract%3A%20%20%20Nonlinear%20time%20series%20models%20with%20exogenous%20regressors%20are%20essential%20in%0Aeconometrics%2C%20queuing%20theory%2C%20and%20machine%20learning%2C%20though%20their%20statistical%0Aanalysis%20remains%20incomplete.%20Key%20results%2C%20such%20as%20the%20law%20of%20large%20numbers%20and%0Athe%20functional%20central%20limit%20theorem%2C%20are%20known%20for%20weakly%20dependent%20variables.%0AWe%20demonstrate%20the%20transfer%20of%20mixing%20properties%20from%20the%20exogenous%20regressor%0Ato%20the%20response%20via%20coupling%20arguments.%20Additionally%2C%20we%20study%20Markov%20chains%20in%0Arandom%20environments%20with%20drift%20and%20minorization%20conditions%2C%20even%20under%0Anon-stationary%20environments%20with%20favorable%20mixing%20properties%2C%20and%20apply%20this%0Aframework%20to%20single-server%20queuing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05056v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransition%2520of%2520%2524%25CE%25B1%2524-mixing%2520in%2520Random%2520Iterations%2520with%2520Applications%2520in%250A%2520%2520Queuing%2520Theory%26entry.906535625%3DAttila%2520Lovas%26entry.1292438233%3D%2520%2520Nonlinear%2520time%2520series%2520models%2520with%2520exogenous%2520regressors%2520are%2520essential%2520in%250Aeconometrics%252C%2520queuing%2520theory%252C%2520and%2520machine%2520learning%252C%2520though%2520their%2520statistical%250Aanalysis%2520remains%2520incomplete.%2520Key%2520results%252C%2520such%2520as%2520the%2520law%2520of%2520large%2520numbers%2520and%250Athe%2520functional%2520central%2520limit%2520theorem%252C%2520are%2520known%2520for%2520weakly%2520dependent%2520variables.%250AWe%2520demonstrate%2520the%2520transfer%2520of%2520mixing%2520properties%2520from%2520the%2520exogenous%2520regressor%250Ato%2520the%2520response%2520via%2520coupling%2520arguments.%2520Additionally%252C%2520we%2520study%2520Markov%2520chains%2520in%250Arandom%2520environments%2520with%2520drift%2520and%2520minorization%2520conditions%252C%2520even%2520under%250Anon-stationary%2520environments%2520with%2520favorable%2520mixing%2520properties%252C%2520and%2520apply%2520this%250Aframework%2520to%2520single-server%2520queuing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05056v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transition%20of%20%24%CE%B1%24-mixing%20in%20Random%20Iterations%20with%20Applications%20in%0A%20%20Queuing%20Theory&entry.906535625=Attila%20Lovas&entry.1292438233=%20%20Nonlinear%20time%20series%20models%20with%20exogenous%20regressors%20are%20essential%20in%0Aeconometrics%2C%20queuing%20theory%2C%20and%20machine%20learning%2C%20though%20their%20statistical%0Aanalysis%20remains%20incomplete.%20Key%20results%2C%20such%20as%20the%20law%20of%20large%20numbers%20and%0Athe%20functional%20central%20limit%20theorem%2C%20are%20known%20for%20weakly%20dependent%20variables.%0AWe%20demonstrate%20the%20transfer%20of%20mixing%20properties%20from%20the%20exogenous%20regressor%0Ato%20the%20response%20via%20coupling%20arguments.%20Additionally%2C%20we%20study%20Markov%20chains%20in%0Arandom%20environments%20with%20drift%20and%20minorization%20conditions%2C%20even%20under%0Anon-stationary%20environments%20with%20favorable%20mixing%20properties%2C%20and%20apply%20this%0Aframework%20to%20single-server%20queuing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05056v3&entry.124074799=Read"},
{"title": "Deep-Learning Control of Lower-Limb Exoskeletons via simplified\n  Therapist Input", "author": "Lorenzo Vianello and Cl\u00e9ment Lhoste and Emek Bar\u0131\u015f K\u00fc\u00e7\u00fcktabak and Matthew Short and Levi Hargrove and Jose L. Pons", "abstract": "  Partial-assistance exoskeletons hold significant potential for gait\nrehabilitation by promoting active participation during (re)learning of\nnormative walking patterns. Typically, the control of interaction torques in\npartial-assistance exoskeletons relies on a hierarchical control structure.\nThese approaches require extensive calibration due to the complexity of the\ncontroller and user-specific parameter tuning, especially for activities like\nstair or ramp navigation. To address the limitations of hierarchical control in\nexoskeletons, this work proposes a three-step, data-driven approach: (1) using\nrecent sensor data to probabilistically infer locomotion states (landing step\nlength, landing step height, walking velocity, step clearance, gait phase), (2)\nallowing therapists to modify these features via a user interface, and (3)\nusing the adjusted locomotion features to predict the desired joint posture and\nmodel stiffness in a spring-damper system based on prediction uncertainty. We\nevaluated the proposed approach with two healthy participants engaging in\ntreadmill walking and stair ascent and descent at varying speeds, with and\nwithout external modification of the gait features through a user interface.\nResults showed a variation in kinematics according to the gait characteristics\nand a negative interaction power suggesting exoskeleton assistance across the\ndifferent conditions.\n", "link": "http://arxiv.org/abs/2412.07959v2", "date": "2025-04-22", "relevancy": 1.7093, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5921}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5701}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-Learning%20Control%20of%20Lower-Limb%20Exoskeletons%20via%20simplified%0A%20%20Therapist%20Input&body=Title%3A%20Deep-Learning%20Control%20of%20Lower-Limb%20Exoskeletons%20via%20simplified%0A%20%20Therapist%20Input%0AAuthor%3A%20Lorenzo%20Vianello%20and%20Cl%C3%A9ment%20Lhoste%20and%20Emek%20Bar%C4%B1%C5%9F%20K%C3%BC%C3%A7%C3%BCktabak%20and%20Matthew%20Short%20and%20Levi%20Hargrove%20and%20Jose%20L.%20Pons%0AAbstract%3A%20%20%20Partial-assistance%20exoskeletons%20hold%20significant%20potential%20for%20gait%0Arehabilitation%20by%20promoting%20active%20participation%20during%20%28re%29learning%20of%0Anormative%20walking%20patterns.%20Typically%2C%20the%20control%20of%20interaction%20torques%20in%0Apartial-assistance%20exoskeletons%20relies%20on%20a%20hierarchical%20control%20structure.%0AThese%20approaches%20require%20extensive%20calibration%20due%20to%20the%20complexity%20of%20the%0Acontroller%20and%20user-specific%20parameter%20tuning%2C%20especially%20for%20activities%20like%0Astair%20or%20ramp%20navigation.%20To%20address%20the%20limitations%20of%20hierarchical%20control%20in%0Aexoskeletons%2C%20this%20work%20proposes%20a%20three-step%2C%20data-driven%20approach%3A%20%281%29%20using%0Arecent%20sensor%20data%20to%20probabilistically%20infer%20locomotion%20states%20%28landing%20step%0Alength%2C%20landing%20step%20height%2C%20walking%20velocity%2C%20step%20clearance%2C%20gait%20phase%29%2C%20%282%29%0Aallowing%20therapists%20to%20modify%20these%20features%20via%20a%20user%20interface%2C%20and%20%283%29%0Ausing%20the%20adjusted%20locomotion%20features%20to%20predict%20the%20desired%20joint%20posture%20and%0Amodel%20stiffness%20in%20a%20spring-damper%20system%20based%20on%20prediction%20uncertainty.%20We%0Aevaluated%20the%20proposed%20approach%20with%20two%20healthy%20participants%20engaging%20in%0Atreadmill%20walking%20and%20stair%20ascent%20and%20descent%20at%20varying%20speeds%2C%20with%20and%0Awithout%20external%20modification%20of%20the%20gait%20features%20through%20a%20user%20interface.%0AResults%20showed%20a%20variation%20in%20kinematics%20according%20to%20the%20gait%20characteristics%0Aand%20a%20negative%20interaction%20power%20suggesting%20exoskeleton%20assistance%20across%20the%0Adifferent%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-Learning%2520Control%2520of%2520Lower-Limb%2520Exoskeletons%2520via%2520simplified%250A%2520%2520Therapist%2520Input%26entry.906535625%3DLorenzo%2520Vianello%2520and%2520Cl%25C3%25A9ment%2520Lhoste%2520and%2520Emek%2520Bar%25C4%25B1%25C5%259F%2520K%25C3%25BC%25C3%25A7%25C3%25BCktabak%2520and%2520Matthew%2520Short%2520and%2520Levi%2520Hargrove%2520and%2520Jose%2520L.%2520Pons%26entry.1292438233%3D%2520%2520Partial-assistance%2520exoskeletons%2520hold%2520significant%2520potential%2520for%2520gait%250Arehabilitation%2520by%2520promoting%2520active%2520participation%2520during%2520%2528re%2529learning%2520of%250Anormative%2520walking%2520patterns.%2520Typically%252C%2520the%2520control%2520of%2520interaction%2520torques%2520in%250Apartial-assistance%2520exoskeletons%2520relies%2520on%2520a%2520hierarchical%2520control%2520structure.%250AThese%2520approaches%2520require%2520extensive%2520calibration%2520due%2520to%2520the%2520complexity%2520of%2520the%250Acontroller%2520and%2520user-specific%2520parameter%2520tuning%252C%2520especially%2520for%2520activities%2520like%250Astair%2520or%2520ramp%2520navigation.%2520To%2520address%2520the%2520limitations%2520of%2520hierarchical%2520control%2520in%250Aexoskeletons%252C%2520this%2520work%2520proposes%2520a%2520three-step%252C%2520data-driven%2520approach%253A%2520%25281%2529%2520using%250Arecent%2520sensor%2520data%2520to%2520probabilistically%2520infer%2520locomotion%2520states%2520%2528landing%2520step%250Alength%252C%2520landing%2520step%2520height%252C%2520walking%2520velocity%252C%2520step%2520clearance%252C%2520gait%2520phase%2529%252C%2520%25282%2529%250Aallowing%2520therapists%2520to%2520modify%2520these%2520features%2520via%2520a%2520user%2520interface%252C%2520and%2520%25283%2529%250Ausing%2520the%2520adjusted%2520locomotion%2520features%2520to%2520predict%2520the%2520desired%2520joint%2520posture%2520and%250Amodel%2520stiffness%2520in%2520a%2520spring-damper%2520system%2520based%2520on%2520prediction%2520uncertainty.%2520We%250Aevaluated%2520the%2520proposed%2520approach%2520with%2520two%2520healthy%2520participants%2520engaging%2520in%250Atreadmill%2520walking%2520and%2520stair%2520ascent%2520and%2520descent%2520at%2520varying%2520speeds%252C%2520with%2520and%250Awithout%2520external%2520modification%2520of%2520the%2520gait%2520features%2520through%2520a%2520user%2520interface.%250AResults%2520showed%2520a%2520variation%2520in%2520kinematics%2520according%2520to%2520the%2520gait%2520characteristics%250Aand%2520a%2520negative%2520interaction%2520power%2520suggesting%2520exoskeleton%2520assistance%2520across%2520the%250Adifferent%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-Learning%20Control%20of%20Lower-Limb%20Exoskeletons%20via%20simplified%0A%20%20Therapist%20Input&entry.906535625=Lorenzo%20Vianello%20and%20Cl%C3%A9ment%20Lhoste%20and%20Emek%20Bar%C4%B1%C5%9F%20K%C3%BC%C3%A7%C3%BCktabak%20and%20Matthew%20Short%20and%20Levi%20Hargrove%20and%20Jose%20L.%20Pons&entry.1292438233=%20%20Partial-assistance%20exoskeletons%20hold%20significant%20potential%20for%20gait%0Arehabilitation%20by%20promoting%20active%20participation%20during%20%28re%29learning%20of%0Anormative%20walking%20patterns.%20Typically%2C%20the%20control%20of%20interaction%20torques%20in%0Apartial-assistance%20exoskeletons%20relies%20on%20a%20hierarchical%20control%20structure.%0AThese%20approaches%20require%20extensive%20calibration%20due%20to%20the%20complexity%20of%20the%0Acontroller%20and%20user-specific%20parameter%20tuning%2C%20especially%20for%20activities%20like%0Astair%20or%20ramp%20navigation.%20To%20address%20the%20limitations%20of%20hierarchical%20control%20in%0Aexoskeletons%2C%20this%20work%20proposes%20a%20three-step%2C%20data-driven%20approach%3A%20%281%29%20using%0Arecent%20sensor%20data%20to%20probabilistically%20infer%20locomotion%20states%20%28landing%20step%0Alength%2C%20landing%20step%20height%2C%20walking%20velocity%2C%20step%20clearance%2C%20gait%20phase%29%2C%20%282%29%0Aallowing%20therapists%20to%20modify%20these%20features%20via%20a%20user%20interface%2C%20and%20%283%29%0Ausing%20the%20adjusted%20locomotion%20features%20to%20predict%20the%20desired%20joint%20posture%20and%0Amodel%20stiffness%20in%20a%20spring-damper%20system%20based%20on%20prediction%20uncertainty.%20We%0Aevaluated%20the%20proposed%20approach%20with%20two%20healthy%20participants%20engaging%20in%0Atreadmill%20walking%20and%20stair%20ascent%20and%20descent%20at%20varying%20speeds%2C%20with%20and%0Awithout%20external%20modification%20of%20the%20gait%20features%20through%20a%20user%20interface.%0AResults%20showed%20a%20variation%20in%20kinematics%20according%20to%20the%20gait%20characteristics%0Aand%20a%20negative%20interaction%20power%20suggesting%20exoskeleton%20assistance%20across%20the%0Adifferent%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07959v2&entry.124074799=Read"},
{"title": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation\n  in Space Applications", "author": "Leonardo Olivi and Edoardo Santero Mormile and Enzo Tartaglione", "abstract": "  In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field.\n", "link": "http://arxiv.org/abs/2504.15991v1", "date": "2025-04-22", "relevancy": 1.7004, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5575}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Adaptation%20of%20Deep%20Neural%20Networks%20for%20Semantic%20Segmentation%0A%20%20in%20Space%20Applications&body=Title%3A%20Efficient%20Adaptation%20of%20Deep%20Neural%20Networks%20for%20Semantic%20Segmentation%0A%20%20in%20Space%20Applications%0AAuthor%3A%20Leonardo%20Olivi%20and%20Edoardo%20Santero%20Mormile%20and%20Enzo%20Tartaglione%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20application%20of%20Deep%20Learning%20techniques%20has%20shown%0Aremarkable%20success%20in%20various%20computer%20vision%20tasks%2C%20paving%20the%20way%20for%20their%0Adeployment%20in%20extraterrestrial%20exploration.%20Transfer%20learning%20has%20emerged%20as%20a%0Apowerful%20strategy%20for%20addressing%20the%20scarcity%20of%20labeled%20data%20in%20these%20novel%0Aenvironments.%20This%20paper%20represents%20one%20of%20the%20first%20efforts%20in%20evaluating%20the%0Afeasibility%20of%20employing%20adapters%20toward%20efficient%20transfer%20learning%20for%20rock%0Asegmentation%20in%20extraterrestrial%20landscapes%2C%20mainly%20focusing%20on%20lunar%20and%0Amartian%20terrains.%20Our%20work%20suggests%20that%20the%20use%20of%20adapters%2C%20strategically%0Aintegrated%20into%20a%20pre-trained%20backbone%20model%2C%20can%20be%20successful%20in%20reducing%0Aboth%20bandwidth%20and%20memory%20requirements%20for%20the%20target%20extraterrestrial%20device.%0AIn%20this%20study%2C%20we%20considered%20two%20memory-saving%20strategies%3A%20layer%20fusion%20%28to%0Areduce%20to%20zero%20the%20inference%20overhead%29%20and%20an%20%60%60adapter%20ranking%27%27%20%28to%20also%0Areduce%20the%20transmission%20cost%29.%20Finally%2C%20we%20evaluate%20these%20results%20in%20terms%20of%0Atask%20performance%2C%20memory%2C%20and%20computation%20on%20embedded%20devices%2C%20evidencing%0Atrade-offs%20that%20open%20the%20road%20to%20more%20research%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Adaptation%2520of%2520Deep%2520Neural%2520Networks%2520for%2520Semantic%2520Segmentation%250A%2520%2520in%2520Space%2520Applications%26entry.906535625%3DLeonardo%2520Olivi%2520and%2520Edoardo%2520Santero%2520Mormile%2520and%2520Enzo%2520Tartaglione%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520application%2520of%2520Deep%2520Learning%2520techniques%2520has%2520shown%250Aremarkable%2520success%2520in%2520various%2520computer%2520vision%2520tasks%252C%2520paving%2520the%2520way%2520for%2520their%250Adeployment%2520in%2520extraterrestrial%2520exploration.%2520Transfer%2520learning%2520has%2520emerged%2520as%2520a%250Apowerful%2520strategy%2520for%2520addressing%2520the%2520scarcity%2520of%2520labeled%2520data%2520in%2520these%2520novel%250Aenvironments.%2520This%2520paper%2520represents%2520one%2520of%2520the%2520first%2520efforts%2520in%2520evaluating%2520the%250Afeasibility%2520of%2520employing%2520adapters%2520toward%2520efficient%2520transfer%2520learning%2520for%2520rock%250Asegmentation%2520in%2520extraterrestrial%2520landscapes%252C%2520mainly%2520focusing%2520on%2520lunar%2520and%250Amartian%2520terrains.%2520Our%2520work%2520suggests%2520that%2520the%2520use%2520of%2520adapters%252C%2520strategically%250Aintegrated%2520into%2520a%2520pre-trained%2520backbone%2520model%252C%2520can%2520be%2520successful%2520in%2520reducing%250Aboth%2520bandwidth%2520and%2520memory%2520requirements%2520for%2520the%2520target%2520extraterrestrial%2520device.%250AIn%2520this%2520study%252C%2520we%2520considered%2520two%2520memory-saving%2520strategies%253A%2520layer%2520fusion%2520%2528to%250Areduce%2520to%2520zero%2520the%2520inference%2520overhead%2529%2520and%2520an%2520%2560%2560adapter%2520ranking%2527%2527%2520%2528to%2520also%250Areduce%2520the%2520transmission%2520cost%2529.%2520Finally%252C%2520we%2520evaluate%2520these%2520results%2520in%2520terms%2520of%250Atask%2520performance%252C%2520memory%252C%2520and%2520computation%2520on%2520embedded%2520devices%252C%2520evidencing%250Atrade-offs%2520that%2520open%2520the%2520road%2520to%2520more%2520research%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Adaptation%20of%20Deep%20Neural%20Networks%20for%20Semantic%20Segmentation%0A%20%20in%20Space%20Applications&entry.906535625=Leonardo%20Olivi%20and%20Edoardo%20Santero%20Mormile%20and%20Enzo%20Tartaglione&entry.1292438233=%20%20In%20recent%20years%2C%20the%20application%20of%20Deep%20Learning%20techniques%20has%20shown%0Aremarkable%20success%20in%20various%20computer%20vision%20tasks%2C%20paving%20the%20way%20for%20their%0Adeployment%20in%20extraterrestrial%20exploration.%20Transfer%20learning%20has%20emerged%20as%20a%0Apowerful%20strategy%20for%20addressing%20the%20scarcity%20of%20labeled%20data%20in%20these%20novel%0Aenvironments.%20This%20paper%20represents%20one%20of%20the%20first%20efforts%20in%20evaluating%20the%0Afeasibility%20of%20employing%20adapters%20toward%20efficient%20transfer%20learning%20for%20rock%0Asegmentation%20in%20extraterrestrial%20landscapes%2C%20mainly%20focusing%20on%20lunar%20and%0Amartian%20terrains.%20Our%20work%20suggests%20that%20the%20use%20of%20adapters%2C%20strategically%0Aintegrated%20into%20a%20pre-trained%20backbone%20model%2C%20can%20be%20successful%20in%20reducing%0Aboth%20bandwidth%20and%20memory%20requirements%20for%20the%20target%20extraterrestrial%20device.%0AIn%20this%20study%2C%20we%20considered%20two%20memory-saving%20strategies%3A%20layer%20fusion%20%28to%0Areduce%20to%20zero%20the%20inference%20overhead%29%20and%20an%20%60%60adapter%20ranking%27%27%20%28to%20also%0Areduce%20the%20transmission%20cost%29.%20Finally%2C%20we%20evaluate%20these%20results%20in%20terms%20of%0Atask%20performance%2C%20memory%2C%20and%20computation%20on%20embedded%20devices%2C%20evidencing%0Atrade-offs%20that%20open%20the%20road%20to%20more%20research%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15991v1&entry.124074799=Read"},
{"title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making", "author": "Chengbo Zheng and Tim Miller and Alina Bialkowski and H Peter Soyer and Monika Janda", "abstract": "  High stakes decision-making often requires a continuous interplay between\nevolving evidence and shifting hypotheses, a dynamic that is not well supported\nby current AI decision support systems. In this paper, we introduce a\nmixed-initiative framework for AI assisted decision making that is grounded in\nthe data-frame theory of sensemaking and the evaluative AI paradigm. Our\napproach enables both humans and AI to collaboratively construct, validate, and\nadapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer\ndiagnosis prototype that leverages a concept bottleneck model to facilitate\ninterpretable interactions and dynamic updates to diagnostic hypotheses.\n", "link": "http://arxiv.org/abs/2504.15894v1", "date": "2025-04-22", "relevancy": 1.447, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4855}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4831}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supporting%20Data-Frame%20Dynamics%20in%20AI-assisted%20Decision%20Making&body=Title%3A%20Supporting%20Data-Frame%20Dynamics%20in%20AI-assisted%20Decision%20Making%0AAuthor%3A%20Chengbo%20Zheng%20and%20Tim%20Miller%20and%20Alina%20Bialkowski%20and%20H%20Peter%20Soyer%20and%20Monika%20Janda%0AAbstract%3A%20%20%20High%20stakes%20decision-making%20often%20requires%20a%20continuous%20interplay%20between%0Aevolving%20evidence%20and%20shifting%20hypotheses%2C%20a%20dynamic%20that%20is%20not%20well%20supported%0Aby%20current%20AI%20decision%20support%20systems.%20In%20this%20paper%2C%20we%20introduce%20a%0Amixed-initiative%20framework%20for%20AI%20assisted%20decision%20making%20that%20is%20grounded%20in%0Athe%20data-frame%20theory%20of%20sensemaking%20and%20the%20evaluative%20AI%20paradigm.%20Our%0Aapproach%20enables%20both%20humans%20and%20AI%20to%20collaboratively%20construct%2C%20validate%2C%20and%0Aadapt%20hypotheses.%20We%20demonstrate%20our%20framework%20with%20an%20AI-assisted%20skin%20cancer%0Adiagnosis%20prototype%20that%20leverages%20a%20concept%20bottleneck%20model%20to%20facilitate%0Ainterpretable%20interactions%20and%20dynamic%20updates%20to%20diagnostic%20hypotheses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupporting%2520Data-Frame%2520Dynamics%2520in%2520AI-assisted%2520Decision%2520Making%26entry.906535625%3DChengbo%2520Zheng%2520and%2520Tim%2520Miller%2520and%2520Alina%2520Bialkowski%2520and%2520H%2520Peter%2520Soyer%2520and%2520Monika%2520Janda%26entry.1292438233%3D%2520%2520High%2520stakes%2520decision-making%2520often%2520requires%2520a%2520continuous%2520interplay%2520between%250Aevolving%2520evidence%2520and%2520shifting%2520hypotheses%252C%2520a%2520dynamic%2520that%2520is%2520not%2520well%2520supported%250Aby%2520current%2520AI%2520decision%2520support%2520systems.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Amixed-initiative%2520framework%2520for%2520AI%2520assisted%2520decision%2520making%2520that%2520is%2520grounded%2520in%250Athe%2520data-frame%2520theory%2520of%2520sensemaking%2520and%2520the%2520evaluative%2520AI%2520paradigm.%2520Our%250Aapproach%2520enables%2520both%2520humans%2520and%2520AI%2520to%2520collaboratively%2520construct%252C%2520validate%252C%2520and%250Aadapt%2520hypotheses.%2520We%2520demonstrate%2520our%2520framework%2520with%2520an%2520AI-assisted%2520skin%2520cancer%250Adiagnosis%2520prototype%2520that%2520leverages%2520a%2520concept%2520bottleneck%2520model%2520to%2520facilitate%250Ainterpretable%2520interactions%2520and%2520dynamic%2520updates%2520to%2520diagnostic%2520hypotheses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supporting%20Data-Frame%20Dynamics%20in%20AI-assisted%20Decision%20Making&entry.906535625=Chengbo%20Zheng%20and%20Tim%20Miller%20and%20Alina%20Bialkowski%20and%20H%20Peter%20Soyer%20and%20Monika%20Janda&entry.1292438233=%20%20High%20stakes%20decision-making%20often%20requires%20a%20continuous%20interplay%20between%0Aevolving%20evidence%20and%20shifting%20hypotheses%2C%20a%20dynamic%20that%20is%20not%20well%20supported%0Aby%20current%20AI%20decision%20support%20systems.%20In%20this%20paper%2C%20we%20introduce%20a%0Amixed-initiative%20framework%20for%20AI%20assisted%20decision%20making%20that%20is%20grounded%20in%0Athe%20data-frame%20theory%20of%20sensemaking%20and%20the%20evaluative%20AI%20paradigm.%20Our%0Aapproach%20enables%20both%20humans%20and%20AI%20to%20collaboratively%20construct%2C%20validate%2C%20and%0Aadapt%20hypotheses.%20We%20demonstrate%20our%20framework%20with%20an%20AI-assisted%20skin%20cancer%0Adiagnosis%20prototype%20that%20leverages%20a%20concept%20bottleneck%20model%20to%20facilitate%0Ainterpretable%20interactions%20and%20dynamic%20updates%20to%20diagnostic%20hypotheses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15894v1&entry.124074799=Read"},
{"title": "Facilitating Reinforcement Learning for Process Control Using Transfer\n  Learning: Overview and Perspectives", "author": "Runze Lin and Junghui Chen and Lei Xie and Hongye Su", "abstract": "  In the context of Industry 4.0 and smart manufacturing, the field of process\nindustry optimization and control is also undergoing a digital transformation.\nWith the rise of Deep Reinforcement Learning (DRL), its application in process\ncontrol has attracted widespread attention. However, the extremely low sample\nefficiency and the safety concerns caused by exploration in DRL hinder its\npractical implementation in industrial settings. Transfer learning offers an\neffective solution for DRL, enhancing its generalization and adaptability in\nmulti-mode control scenarios. This paper provides insights into the use of DRL\nfor process control from the perspective of transfer learning. We analyze the\nchallenges of applying DRL in the process industry and the necessity of\nintroducing transfer learning. Furthermore, recommendations and prospects are\nprovided for future research directions on how transfer learning can be\nintegrated with DRL to enhance process control. This paper aims to offer a set\nof promising, user-friendly, easy-to-implement, and scalable approaches to\nartificial intelligence-facilitated industrial control for scholars and\nengineers in the process industry.\n", "link": "http://arxiv.org/abs/2404.00247v3", "date": "2025-04-22", "relevancy": 1.4438, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5288}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4228}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facilitating%20Reinforcement%20Learning%20for%20Process%20Control%20Using%20Transfer%0A%20%20Learning%3A%20Overview%20and%20Perspectives&body=Title%3A%20Facilitating%20Reinforcement%20Learning%20for%20Process%20Control%20Using%20Transfer%0A%20%20Learning%3A%20Overview%20and%20Perspectives%0AAuthor%3A%20Runze%20Lin%20and%20Junghui%20Chen%20and%20Lei%20Xie%20and%20Hongye%20Su%0AAbstract%3A%20%20%20In%20the%20context%20of%20Industry%204.0%20and%20smart%20manufacturing%2C%20the%20field%20of%20process%0Aindustry%20optimization%20and%20control%20is%20also%20undergoing%20a%20digital%20transformation.%0AWith%20the%20rise%20of%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20its%20application%20in%20process%0Acontrol%20has%20attracted%20widespread%20attention.%20However%2C%20the%20extremely%20low%20sample%0Aefficiency%20and%20the%20safety%20concerns%20caused%20by%20exploration%20in%20DRL%20hinder%20its%0Apractical%20implementation%20in%20industrial%20settings.%20Transfer%20learning%20offers%20an%0Aeffective%20solution%20for%20DRL%2C%20enhancing%20its%20generalization%20and%20adaptability%20in%0Amulti-mode%20control%20scenarios.%20This%20paper%20provides%20insights%20into%20the%20use%20of%20DRL%0Afor%20process%20control%20from%20the%20perspective%20of%20transfer%20learning.%20We%20analyze%20the%0Achallenges%20of%20applying%20DRL%20in%20the%20process%20industry%20and%20the%20necessity%20of%0Aintroducing%20transfer%20learning.%20Furthermore%2C%20recommendations%20and%20prospects%20are%0Aprovided%20for%20future%20research%20directions%20on%20how%20transfer%20learning%20can%20be%0Aintegrated%20with%20DRL%20to%20enhance%20process%20control.%20This%20paper%20aims%20to%20offer%20a%20set%0Aof%20promising%2C%20user-friendly%2C%20easy-to-implement%2C%20and%20scalable%20approaches%20to%0Aartificial%20intelligence-facilitated%20industrial%20control%20for%20scholars%20and%0Aengineers%20in%20the%20process%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00247v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacilitating%2520Reinforcement%2520Learning%2520for%2520Process%2520Control%2520Using%2520Transfer%250A%2520%2520Learning%253A%2520Overview%2520and%2520Perspectives%26entry.906535625%3DRunze%2520Lin%2520and%2520Junghui%2520Chen%2520and%2520Lei%2520Xie%2520and%2520Hongye%2520Su%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520Industry%25204.0%2520and%2520smart%2520manufacturing%252C%2520the%2520field%2520of%2520process%250Aindustry%2520optimization%2520and%2520control%2520is%2520also%2520undergoing%2520a%2520digital%2520transformation.%250AWith%2520the%2520rise%2520of%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%252C%2520its%2520application%2520in%2520process%250Acontrol%2520has%2520attracted%2520widespread%2520attention.%2520However%252C%2520the%2520extremely%2520low%2520sample%250Aefficiency%2520and%2520the%2520safety%2520concerns%2520caused%2520by%2520exploration%2520in%2520DRL%2520hinder%2520its%250Apractical%2520implementation%2520in%2520industrial%2520settings.%2520Transfer%2520learning%2520offers%2520an%250Aeffective%2520solution%2520for%2520DRL%252C%2520enhancing%2520its%2520generalization%2520and%2520adaptability%2520in%250Amulti-mode%2520control%2520scenarios.%2520This%2520paper%2520provides%2520insights%2520into%2520the%2520use%2520of%2520DRL%250Afor%2520process%2520control%2520from%2520the%2520perspective%2520of%2520transfer%2520learning.%2520We%2520analyze%2520the%250Achallenges%2520of%2520applying%2520DRL%2520in%2520the%2520process%2520industry%2520and%2520the%2520necessity%2520of%250Aintroducing%2520transfer%2520learning.%2520Furthermore%252C%2520recommendations%2520and%2520prospects%2520are%250Aprovided%2520for%2520future%2520research%2520directions%2520on%2520how%2520transfer%2520learning%2520can%2520be%250Aintegrated%2520with%2520DRL%2520to%2520enhance%2520process%2520control.%2520This%2520paper%2520aims%2520to%2520offer%2520a%2520set%250Aof%2520promising%252C%2520user-friendly%252C%2520easy-to-implement%252C%2520and%2520scalable%2520approaches%2520to%250Aartificial%2520intelligence-facilitated%2520industrial%2520control%2520for%2520scholars%2520and%250Aengineers%2520in%2520the%2520process%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00247v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facilitating%20Reinforcement%20Learning%20for%20Process%20Control%20Using%20Transfer%0A%20%20Learning%3A%20Overview%20and%20Perspectives&entry.906535625=Runze%20Lin%20and%20Junghui%20Chen%20and%20Lei%20Xie%20and%20Hongye%20Su&entry.1292438233=%20%20In%20the%20context%20of%20Industry%204.0%20and%20smart%20manufacturing%2C%20the%20field%20of%20process%0Aindustry%20optimization%20and%20control%20is%20also%20undergoing%20a%20digital%20transformation.%0AWith%20the%20rise%20of%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20its%20application%20in%20process%0Acontrol%20has%20attracted%20widespread%20attention.%20However%2C%20the%20extremely%20low%20sample%0Aefficiency%20and%20the%20safety%20concerns%20caused%20by%20exploration%20in%20DRL%20hinder%20its%0Apractical%20implementation%20in%20industrial%20settings.%20Transfer%20learning%20offers%20an%0Aeffective%20solution%20for%20DRL%2C%20enhancing%20its%20generalization%20and%20adaptability%20in%0Amulti-mode%20control%20scenarios.%20This%20paper%20provides%20insights%20into%20the%20use%20of%20DRL%0Afor%20process%20control%20from%20the%20perspective%20of%20transfer%20learning.%20We%20analyze%20the%0Achallenges%20of%20applying%20DRL%20in%20the%20process%20industry%20and%20the%20necessity%20of%0Aintroducing%20transfer%20learning.%20Furthermore%2C%20recommendations%20and%20prospects%20are%0Aprovided%20for%20future%20research%20directions%20on%20how%20transfer%20learning%20can%20be%0Aintegrated%20with%20DRL%20to%20enhance%20process%20control.%20This%20paper%20aims%20to%20offer%20a%20set%0Aof%20promising%2C%20user-friendly%2C%20easy-to-implement%2C%20and%20scalable%20approaches%20to%0Aartificial%20intelligence-facilitated%20industrial%20control%20for%20scholars%20and%0Aengineers%20in%20the%20process%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00247v3&entry.124074799=Read"},
{"title": "Adaptive Student's t-distribution with method of moments moving\n  estimator for nonstationary time series", "author": "Jarek Duda", "abstract": "  The real life time series are usually nonstationary, bringing a difficult\nquestion of model adaptation. Classical approaches like ARMA-ARCH assume\narbitrary type of dependence. To avoid their bias, we will focus on recently\nproposed agnostic philosophy of moving estimator: in time $t$ finding\nparameters optimizing e.g. $F_t=\\sum_{\\tau<t} (1-\\eta)^{t-\\tau} \\ln(\\rho_\\theta\n(x_\\tau))$ moving log-likelihood, evolving in time. It allows for example to\nestimate parameters using inexpensive exponential moving averages (EMA), like\nabsolute central moments $m_p=E[|x-\\mu|^p]$ evolving for one or multiple powers\n$p\\in\\mathbb{R}^+$ using $m_{p,t+1} = m_{p,t} + \\eta (|x_t-\\mu_t|^p-m_{p,t})$.\nApplication of such general adaptive methods of moments will be presented on\nStudent's t-distribution, popular especially in economical applications, here\napplied to log-returns of DJIA companies. While standard ARMA-ARCH approaches\nprovide evolution of $\\mu$ and $\\sigma$, here we also get evolution of $\\nu$\ndescribing $\\rho(x)\\sim |x|^{-\\nu-1}$ tail shape, probability of extreme events\n- which might turn out catastrophic, destabilizing the market.\n", "link": "http://arxiv.org/abs/2304.03069v4", "date": "2025-04-22", "relevancy": 1.2344, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.42}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4135}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Student%27s%20t-distribution%20with%20method%20of%20moments%20moving%0A%20%20estimator%20for%20nonstationary%20time%20series&body=Title%3A%20Adaptive%20Student%27s%20t-distribution%20with%20method%20of%20moments%20moving%0A%20%20estimator%20for%20nonstationary%20time%20series%0AAuthor%3A%20Jarek%20Duda%0AAbstract%3A%20%20%20The%20real%20life%20time%20series%20are%20usually%20nonstationary%2C%20bringing%20a%20difficult%0Aquestion%20of%20model%20adaptation.%20Classical%20approaches%20like%20ARMA-ARCH%20assume%0Aarbitrary%20type%20of%20dependence.%20To%20avoid%20their%20bias%2C%20we%20will%20focus%20on%20recently%0Aproposed%20agnostic%20philosophy%20of%20moving%20estimator%3A%20in%20time%20%24t%24%20finding%0Aparameters%20optimizing%20e.g.%20%24F_t%3D%5Csum_%7B%5Ctau%3Ct%7D%20%281-%5Ceta%29%5E%7Bt-%5Ctau%7D%20%5Cln%28%5Crho_%5Ctheta%0A%28x_%5Ctau%29%29%24%20moving%20log-likelihood%2C%20evolving%20in%20time.%20It%20allows%20for%20example%20to%0Aestimate%20parameters%20using%20inexpensive%20exponential%20moving%20averages%20%28EMA%29%2C%20like%0Aabsolute%20central%20moments%20%24m_p%3DE%5B%7Cx-%5Cmu%7C%5Ep%5D%24%20evolving%20for%20one%20or%20multiple%20powers%0A%24p%5Cin%5Cmathbb%7BR%7D%5E%2B%24%20using%20%24m_%7Bp%2Ct%2B1%7D%20%3D%20m_%7Bp%2Ct%7D%20%2B%20%5Ceta%20%28%7Cx_t-%5Cmu_t%7C%5Ep-m_%7Bp%2Ct%7D%29%24.%0AApplication%20of%20such%20general%20adaptive%20methods%20of%20moments%20will%20be%20presented%20on%0AStudent%27s%20t-distribution%2C%20popular%20especially%20in%20economical%20applications%2C%20here%0Aapplied%20to%20log-returns%20of%20DJIA%20companies.%20While%20standard%20ARMA-ARCH%20approaches%0Aprovide%20evolution%20of%20%24%5Cmu%24%20and%20%24%5Csigma%24%2C%20here%20we%20also%20get%20evolution%20of%20%24%5Cnu%24%0Adescribing%20%24%5Crho%28x%29%5Csim%20%7Cx%7C%5E%7B-%5Cnu-1%7D%24%20tail%20shape%2C%20probability%20of%20extreme%20events%0A-%20which%20might%20turn%20out%20catastrophic%2C%20destabilizing%20the%20market.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03069v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Student%2527s%2520t-distribution%2520with%2520method%2520of%2520moments%2520moving%250A%2520%2520estimator%2520for%2520nonstationary%2520time%2520series%26entry.906535625%3DJarek%2520Duda%26entry.1292438233%3D%2520%2520The%2520real%2520life%2520time%2520series%2520are%2520usually%2520nonstationary%252C%2520bringing%2520a%2520difficult%250Aquestion%2520of%2520model%2520adaptation.%2520Classical%2520approaches%2520like%2520ARMA-ARCH%2520assume%250Aarbitrary%2520type%2520of%2520dependence.%2520To%2520avoid%2520their%2520bias%252C%2520we%2520will%2520focus%2520on%2520recently%250Aproposed%2520agnostic%2520philosophy%2520of%2520moving%2520estimator%253A%2520in%2520time%2520%2524t%2524%2520finding%250Aparameters%2520optimizing%2520e.g.%2520%2524F_t%253D%255Csum_%257B%255Ctau%253Ct%257D%2520%25281-%255Ceta%2529%255E%257Bt-%255Ctau%257D%2520%255Cln%2528%255Crho_%255Ctheta%250A%2528x_%255Ctau%2529%2529%2524%2520moving%2520log-likelihood%252C%2520evolving%2520in%2520time.%2520It%2520allows%2520for%2520example%2520to%250Aestimate%2520parameters%2520using%2520inexpensive%2520exponential%2520moving%2520averages%2520%2528EMA%2529%252C%2520like%250Aabsolute%2520central%2520moments%2520%2524m_p%253DE%255B%257Cx-%255Cmu%257C%255Ep%255D%2524%2520evolving%2520for%2520one%2520or%2520multiple%2520powers%250A%2524p%255Cin%255Cmathbb%257BR%257D%255E%252B%2524%2520using%2520%2524m_%257Bp%252Ct%252B1%257D%2520%253D%2520m_%257Bp%252Ct%257D%2520%252B%2520%255Ceta%2520%2528%257Cx_t-%255Cmu_t%257C%255Ep-m_%257Bp%252Ct%257D%2529%2524.%250AApplication%2520of%2520such%2520general%2520adaptive%2520methods%2520of%2520moments%2520will%2520be%2520presented%2520on%250AStudent%2527s%2520t-distribution%252C%2520popular%2520especially%2520in%2520economical%2520applications%252C%2520here%250Aapplied%2520to%2520log-returns%2520of%2520DJIA%2520companies.%2520While%2520standard%2520ARMA-ARCH%2520approaches%250Aprovide%2520evolution%2520of%2520%2524%255Cmu%2524%2520and%2520%2524%255Csigma%2524%252C%2520here%2520we%2520also%2520get%2520evolution%2520of%2520%2524%255Cnu%2524%250Adescribing%2520%2524%255Crho%2528x%2529%255Csim%2520%257Cx%257C%255E%257B-%255Cnu-1%257D%2524%2520tail%2520shape%252C%2520probability%2520of%2520extreme%2520events%250A-%2520which%2520might%2520turn%2520out%2520catastrophic%252C%2520destabilizing%2520the%2520market.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.03069v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Student%27s%20t-distribution%20with%20method%20of%20moments%20moving%0A%20%20estimator%20for%20nonstationary%20time%20series&entry.906535625=Jarek%20Duda&entry.1292438233=%20%20The%20real%20life%20time%20series%20are%20usually%20nonstationary%2C%20bringing%20a%20difficult%0Aquestion%20of%20model%20adaptation.%20Classical%20approaches%20like%20ARMA-ARCH%20assume%0Aarbitrary%20type%20of%20dependence.%20To%20avoid%20their%20bias%2C%20we%20will%20focus%20on%20recently%0Aproposed%20agnostic%20philosophy%20of%20moving%20estimator%3A%20in%20time%20%24t%24%20finding%0Aparameters%20optimizing%20e.g.%20%24F_t%3D%5Csum_%7B%5Ctau%3Ct%7D%20%281-%5Ceta%29%5E%7Bt-%5Ctau%7D%20%5Cln%28%5Crho_%5Ctheta%0A%28x_%5Ctau%29%29%24%20moving%20log-likelihood%2C%20evolving%20in%20time.%20It%20allows%20for%20example%20to%0Aestimate%20parameters%20using%20inexpensive%20exponential%20moving%20averages%20%28EMA%29%2C%20like%0Aabsolute%20central%20moments%20%24m_p%3DE%5B%7Cx-%5Cmu%7C%5Ep%5D%24%20evolving%20for%20one%20or%20multiple%20powers%0A%24p%5Cin%5Cmathbb%7BR%7D%5E%2B%24%20using%20%24m_%7Bp%2Ct%2B1%7D%20%3D%20m_%7Bp%2Ct%7D%20%2B%20%5Ceta%20%28%7Cx_t-%5Cmu_t%7C%5Ep-m_%7Bp%2Ct%7D%29%24.%0AApplication%20of%20such%20general%20adaptive%20methods%20of%20moments%20will%20be%20presented%20on%0AStudent%27s%20t-distribution%2C%20popular%20especially%20in%20economical%20applications%2C%20here%0Aapplied%20to%20log-returns%20of%20DJIA%20companies.%20While%20standard%20ARMA-ARCH%20approaches%0Aprovide%20evolution%20of%20%24%5Cmu%24%20and%20%24%5Csigma%24%2C%20here%20we%20also%20get%20evolution%20of%20%24%5Cnu%24%0Adescribing%20%24%5Crho%28x%29%5Csim%20%7Cx%7C%5E%7B-%5Cnu-1%7D%24%20tail%20shape%2C%20probability%20of%20extreme%20events%0A-%20which%20might%20turn%20out%20catastrophic%2C%20destabilizing%20the%20market.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03069v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


