<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250423.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Splatting is an Effective Data Generator for 3D Object\n  Detection", "author": "Farhad G. Zanjani and Davide Abati and Auke Wiggers and Dimitris Kalatzis and Jens Petersen and Hong Cai and Amirhossein Habibian", "abstract": "  We investigate data augmentation for 3D object detection in autonomous\ndriving. We utilize recent advancements in 3D reconstruction based on Gaussian\nSplatting for 3D object placement in driving scenes. Unlike existing\ndiffusion-based methods that synthesize images conditioned on BEV layouts, our\napproach places 3D objects directly in the reconstructed 3D space with\nexplicitly imposed geometric transformations. This ensures both the physical\nplausibility of object placement and highly accurate 3D pose and position\nannotations.\n  Our experiments demonstrate that even by integrating a limited number of\nexternal 3D objects into real scenes, the augmented data significantly enhances\n3D object detection performance and outperforms existing diffusion-based 3D\naugmentation for object detection. Extensive testing on the nuScenes dataset\nreveals that imposing high geometric diversity in object placement has a\ngreater impact compared to the appearance diversity of objects. Additionally,\nwe show that generating hard examples, either by maximizing detection loss or\nimposing high visual occlusion in camera images, does not lead to more\nefficient 3D data augmentation for camera-based 3D object detection in\nautonomous driving.\n", "link": "http://arxiv.org/abs/2504.16740v1", "date": "2025-04-23", "relevancy": 3.2951, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.691}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6672}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20is%20an%20Effective%20Data%20Generator%20for%203D%20Object%0A%20%20Detection&body=Title%3A%20Gaussian%20Splatting%20is%20an%20Effective%20Data%20Generator%20for%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Farhad%20G.%20Zanjani%20and%20Davide%20Abati%20and%20Auke%20Wiggers%20and%20Dimitris%20Kalatzis%20and%20Jens%20Petersen%20and%20Hong%20Cai%20and%20Amirhossein%20Habibian%0AAbstract%3A%20%20%20We%20investigate%20data%20augmentation%20for%203D%20object%20detection%20in%20autonomous%0Adriving.%20We%20utilize%20recent%20advancements%20in%203D%20reconstruction%20based%20on%20Gaussian%0ASplatting%20for%203D%20object%20placement%20in%20driving%20scenes.%20Unlike%20existing%0Adiffusion-based%20methods%20that%20synthesize%20images%20conditioned%20on%20BEV%20layouts%2C%20our%0Aapproach%20places%203D%20objects%20directly%20in%20the%20reconstructed%203D%20space%20with%0Aexplicitly%20imposed%20geometric%20transformations.%20This%20ensures%20both%20the%20physical%0Aplausibility%20of%20object%20placement%20and%20highly%20accurate%203D%20pose%20and%20position%0Aannotations.%0A%20%20Our%20experiments%20demonstrate%20that%20even%20by%20integrating%20a%20limited%20number%20of%0Aexternal%203D%20objects%20into%20real%20scenes%2C%20the%20augmented%20data%20significantly%20enhances%0A3D%20object%20detection%20performance%20and%20outperforms%20existing%20diffusion-based%203D%0Aaugmentation%20for%20object%20detection.%20Extensive%20testing%20on%20the%20nuScenes%20dataset%0Areveals%20that%20imposing%20high%20geometric%20diversity%20in%20object%20placement%20has%20a%0Agreater%20impact%20compared%20to%20the%20appearance%20diversity%20of%20objects.%20Additionally%2C%0Awe%20show%20that%20generating%20hard%20examples%2C%20either%20by%20maximizing%20detection%20loss%20or%0Aimposing%20high%20visual%20occlusion%20in%20camera%20images%2C%20does%20not%20lead%20to%20more%0Aefficient%203D%20data%20augmentation%20for%20camera-based%203D%20object%20detection%20in%0Aautonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520is%2520an%2520Effective%2520Data%2520Generator%2520for%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DFarhad%2520G.%2520Zanjani%2520and%2520Davide%2520Abati%2520and%2520Auke%2520Wiggers%2520and%2520Dimitris%2520Kalatzis%2520and%2520Jens%2520Petersen%2520and%2520Hong%2520Cai%2520and%2520Amirhossein%2520Habibian%26entry.1292438233%3D%2520%2520We%2520investigate%2520data%2520augmentation%2520for%25203D%2520object%2520detection%2520in%2520autonomous%250Adriving.%2520We%2520utilize%2520recent%2520advancements%2520in%25203D%2520reconstruction%2520based%2520on%2520Gaussian%250ASplatting%2520for%25203D%2520object%2520placement%2520in%2520driving%2520scenes.%2520Unlike%2520existing%250Adiffusion-based%2520methods%2520that%2520synthesize%2520images%2520conditioned%2520on%2520BEV%2520layouts%252C%2520our%250Aapproach%2520places%25203D%2520objects%2520directly%2520in%2520the%2520reconstructed%25203D%2520space%2520with%250Aexplicitly%2520imposed%2520geometric%2520transformations.%2520This%2520ensures%2520both%2520the%2520physical%250Aplausibility%2520of%2520object%2520placement%2520and%2520highly%2520accurate%25203D%2520pose%2520and%2520position%250Aannotations.%250A%2520%2520Our%2520experiments%2520demonstrate%2520that%2520even%2520by%2520integrating%2520a%2520limited%2520number%2520of%250Aexternal%25203D%2520objects%2520into%2520real%2520scenes%252C%2520the%2520augmented%2520data%2520significantly%2520enhances%250A3D%2520object%2520detection%2520performance%2520and%2520outperforms%2520existing%2520diffusion-based%25203D%250Aaugmentation%2520for%2520object%2520detection.%2520Extensive%2520testing%2520on%2520the%2520nuScenes%2520dataset%250Areveals%2520that%2520imposing%2520high%2520geometric%2520diversity%2520in%2520object%2520placement%2520has%2520a%250Agreater%2520impact%2520compared%2520to%2520the%2520appearance%2520diversity%2520of%2520objects.%2520Additionally%252C%250Awe%2520show%2520that%2520generating%2520hard%2520examples%252C%2520either%2520by%2520maximizing%2520detection%2520loss%2520or%250Aimposing%2520high%2520visual%2520occlusion%2520in%2520camera%2520images%252C%2520does%2520not%2520lead%2520to%2520more%250Aefficient%25203D%2520data%2520augmentation%2520for%2520camera-based%25203D%2520object%2520detection%2520in%250Aautonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20is%20an%20Effective%20Data%20Generator%20for%203D%20Object%0A%20%20Detection&entry.906535625=Farhad%20G.%20Zanjani%20and%20Davide%20Abati%20and%20Auke%20Wiggers%20and%20Dimitris%20Kalatzis%20and%20Jens%20Petersen%20and%20Hong%20Cai%20and%20Amirhossein%20Habibian&entry.1292438233=%20%20We%20investigate%20data%20augmentation%20for%203D%20object%20detection%20in%20autonomous%0Adriving.%20We%20utilize%20recent%20advancements%20in%203D%20reconstruction%20based%20on%20Gaussian%0ASplatting%20for%203D%20object%20placement%20in%20driving%20scenes.%20Unlike%20existing%0Adiffusion-based%20methods%20that%20synthesize%20images%20conditioned%20on%20BEV%20layouts%2C%20our%0Aapproach%20places%203D%20objects%20directly%20in%20the%20reconstructed%203D%20space%20with%0Aexplicitly%20imposed%20geometric%20transformations.%20This%20ensures%20both%20the%20physical%0Aplausibility%20of%20object%20placement%20and%20highly%20accurate%203D%20pose%20and%20position%0Aannotations.%0A%20%20Our%20experiments%20demonstrate%20that%20even%20by%20integrating%20a%20limited%20number%20of%0Aexternal%203D%20objects%20into%20real%20scenes%2C%20the%20augmented%20data%20significantly%20enhances%0A3D%20object%20detection%20performance%20and%20outperforms%20existing%20diffusion-based%203D%0Aaugmentation%20for%20object%20detection.%20Extensive%20testing%20on%20the%20nuScenes%20dataset%0Areveals%20that%20imposing%20high%20geometric%20diversity%20in%20object%20placement%20has%20a%0Agreater%20impact%20compared%20to%20the%20appearance%20diversity%20of%20objects.%20Additionally%2C%0Awe%20show%20that%20generating%20hard%20examples%2C%20either%20by%20maximizing%20detection%20loss%20or%0Aimposing%20high%20visual%20occlusion%20in%20camera%20images%2C%20does%20not%20lead%20to%20more%0Aefficient%203D%20data%20augmentation%20for%20camera-based%203D%20object%20detection%20in%0Aautonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16740v1&entry.124074799=Read"},
{"title": "Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting\n  Conditions with View-Adaptive Curve Adjustment", "author": "Ziteng Cui and Xuangeng Chu and Tatsuya Harada", "abstract": "  Capturing high-quality photographs under diverse real-world lighting\nconditions is challenging, as both natural lighting (e.g., low-light) and\ncamera exposure settings (e.g., exposure time) significantly impact image\nquality. This challenge becomes more pronounced in multi-view scenarios, where\nvariations in lighting and image signal processor (ISP) settings across\nviewpoints introduce photometric inconsistencies. Such lighting degradations\nand view-dependent variations pose substantial challenges to novel view\nsynthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel\napproach to achieving high-quality novel view synthesis results under diverse\nchallenging lighting conditions using 3DGS. By adopting per-view color matrix\nmapping and view-adaptive curve adjustments, Luminance-GS achieves\nstate-of-the-art (SOTA) results across various lighting conditions -- including\nlow-light, overexposure, and varying exposure -- while not altering the\noriginal 3DGS explicit representation. Compared to previous NeRF- and\n3DGS-based baselines, Luminance-GS provides real-time rendering speed with\nimproved reconstruction quality.\n", "link": "http://arxiv.org/abs/2504.01503v2", "date": "2025-04-23", "relevancy": 3.1451, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.672}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6089}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Luminance-GS%3A%20Adapting%203D%20Gaussian%20Splatting%20to%20Challenging%20Lighting%0A%20%20Conditions%20with%20View-Adaptive%20Curve%20Adjustment&body=Title%3A%20Luminance-GS%3A%20Adapting%203D%20Gaussian%20Splatting%20to%20Challenging%20Lighting%0A%20%20Conditions%20with%20View-Adaptive%20Curve%20Adjustment%0AAuthor%3A%20Ziteng%20Cui%20and%20Xuangeng%20Chu%20and%20Tatsuya%20Harada%0AAbstract%3A%20%20%20Capturing%20high-quality%20photographs%20under%20diverse%20real-world%20lighting%0Aconditions%20is%20challenging%2C%20as%20both%20natural%20lighting%20%28e.g.%2C%20low-light%29%20and%0Acamera%20exposure%20settings%20%28e.g.%2C%20exposure%20time%29%20significantly%20impact%20image%0Aquality.%20This%20challenge%20becomes%20more%20pronounced%20in%20multi-view%20scenarios%2C%20where%0Avariations%20in%20lighting%20and%20image%20signal%20processor%20%28ISP%29%20settings%20across%0Aviewpoints%20introduce%20photometric%20inconsistencies.%20Such%20lighting%20degradations%0Aand%20view-dependent%20variations%20pose%20substantial%20challenges%20to%20novel%20view%0Asynthesis%20%28NVS%29%20frameworks%20based%20on%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29.%20To%20address%20this%2C%20we%20introduce%20Luminance-GS%2C%20a%20novel%0Aapproach%20to%20achieving%20high-quality%20novel%20view%20synthesis%20results%20under%20diverse%0Achallenging%20lighting%20conditions%20using%203DGS.%20By%20adopting%20per-view%20color%20matrix%0Amapping%20and%20view-adaptive%20curve%20adjustments%2C%20Luminance-GS%20achieves%0Astate-of-the-art%20%28SOTA%29%20results%20across%20various%20lighting%20conditions%20--%20including%0Alow-light%2C%20overexposure%2C%20and%20varying%20exposure%20--%20while%20not%20altering%20the%0Aoriginal%203DGS%20explicit%20representation.%20Compared%20to%20previous%20NeRF-%20and%0A3DGS-based%20baselines%2C%20Luminance-GS%20provides%20real-time%20rendering%20speed%20with%0Aimproved%20reconstruction%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01503v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLuminance-GS%253A%2520Adapting%25203D%2520Gaussian%2520Splatting%2520to%2520Challenging%2520Lighting%250A%2520%2520Conditions%2520with%2520View-Adaptive%2520Curve%2520Adjustment%26entry.906535625%3DZiteng%2520Cui%2520and%2520Xuangeng%2520Chu%2520and%2520Tatsuya%2520Harada%26entry.1292438233%3D%2520%2520Capturing%2520high-quality%2520photographs%2520under%2520diverse%2520real-world%2520lighting%250Aconditions%2520is%2520challenging%252C%2520as%2520both%2520natural%2520lighting%2520%2528e.g.%252C%2520low-light%2529%2520and%250Acamera%2520exposure%2520settings%2520%2528e.g.%252C%2520exposure%2520time%2529%2520significantly%2520impact%2520image%250Aquality.%2520This%2520challenge%2520becomes%2520more%2520pronounced%2520in%2520multi-view%2520scenarios%252C%2520where%250Avariations%2520in%2520lighting%2520and%2520image%2520signal%2520processor%2520%2528ISP%2529%2520settings%2520across%250Aviewpoints%2520introduce%2520photometric%2520inconsistencies.%2520Such%2520lighting%2520degradations%250Aand%2520view-dependent%2520variations%2520pose%2520substantial%2520challenges%2520to%2520novel%2520view%250Asynthesis%2520%2528NVS%2529%2520frameworks%2520based%2520on%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520and%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Luminance-GS%252C%2520a%2520novel%250Aapproach%2520to%2520achieving%2520high-quality%2520novel%2520view%2520synthesis%2520results%2520under%2520diverse%250Achallenging%2520lighting%2520conditions%2520using%25203DGS.%2520By%2520adopting%2520per-view%2520color%2520matrix%250Amapping%2520and%2520view-adaptive%2520curve%2520adjustments%252C%2520Luminance-GS%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520results%2520across%2520various%2520lighting%2520conditions%2520--%2520including%250Alow-light%252C%2520overexposure%252C%2520and%2520varying%2520exposure%2520--%2520while%2520not%2520altering%2520the%250Aoriginal%25203DGS%2520explicit%2520representation.%2520Compared%2520to%2520previous%2520NeRF-%2520and%250A3DGS-based%2520baselines%252C%2520Luminance-GS%2520provides%2520real-time%2520rendering%2520speed%2520with%250Aimproved%2520reconstruction%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01503v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Luminance-GS%3A%20Adapting%203D%20Gaussian%20Splatting%20to%20Challenging%20Lighting%0A%20%20Conditions%20with%20View-Adaptive%20Curve%20Adjustment&entry.906535625=Ziteng%20Cui%20and%20Xuangeng%20Chu%20and%20Tatsuya%20Harada&entry.1292438233=%20%20Capturing%20high-quality%20photographs%20under%20diverse%20real-world%20lighting%0Aconditions%20is%20challenging%2C%20as%20both%20natural%20lighting%20%28e.g.%2C%20low-light%29%20and%0Acamera%20exposure%20settings%20%28e.g.%2C%20exposure%20time%29%20significantly%20impact%20image%0Aquality.%20This%20challenge%20becomes%20more%20pronounced%20in%20multi-view%20scenarios%2C%20where%0Avariations%20in%20lighting%20and%20image%20signal%20processor%20%28ISP%29%20settings%20across%0Aviewpoints%20introduce%20photometric%20inconsistencies.%20Such%20lighting%20degradations%0Aand%20view-dependent%20variations%20pose%20substantial%20challenges%20to%20novel%20view%0Asynthesis%20%28NVS%29%20frameworks%20based%20on%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%203D%0AGaussian%20Splatting%20%283DGS%29.%20To%20address%20this%2C%20we%20introduce%20Luminance-GS%2C%20a%20novel%0Aapproach%20to%20achieving%20high-quality%20novel%20view%20synthesis%20results%20under%20diverse%0Achallenging%20lighting%20conditions%20using%203DGS.%20By%20adopting%20per-view%20color%20matrix%0Amapping%20and%20view-adaptive%20curve%20adjustments%2C%20Luminance-GS%20achieves%0Astate-of-the-art%20%28SOTA%29%20results%20across%20various%20lighting%20conditions%20--%20including%0Alow-light%2C%20overexposure%2C%20and%20varying%20exposure%20--%20while%20not%20altering%20the%0Aoriginal%203DGS%20explicit%20representation.%20Compared%20to%20previous%20NeRF-%20and%0A3DGS-based%20baselines%2C%20Luminance-GS%20provides%20real-time%20rendering%20speed%20with%0Aimproved%20reconstruction%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01503v2&entry.124074799=Read"},
{"title": "PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum\n  Learning", "author": "Yingjie Xi and Jian Jun Zhang and Xiaosong Yang", "abstract": "  In computer animation, game design, and human-computer interaction,\nsynthesizing human motion that aligns with user intent remains a significant\nchallenge. Existing methods have notable limitations: textual approaches offer\nhigh-level semantic guidance but struggle to describe complex actions\naccurately; trajectory-based techniques provide intuitive global motion\ndirection yet often fall short in generating precise or customized character\nmovements; and anchor poses-guided methods are typically confined to synthesize\nonly simple motion patterns. To generate more controllable and precise human\nmotions, we propose \\textbf{ProMoGen (Progressive Motion Generation)}, a novel\nframework that integrates trajectory guidance with sparse anchor motion\ncontrol. Global trajectories ensure consistency in spatial direction and\ndisplacement, while sparse anchor motions only deliver precise action guidance\nwithout displacement. This decoupling enables independent refinement of both\naspects, resulting in a more controllable, high-fidelity, and sophisticated\nmotion synthesis. ProMoGen supports both dual and single control paradigms\nwithin a unified training process. Moreover, we recognize that direct learning\nfrom sparse motions is inherently unstable, we introduce \\textbf{SAP-CL (Sparse\nAnchor Posture Curriculum Learning)}, a curriculum learning strategy that\nprogressively adjusts the number of anchors used for guidance, thereby enabling\nmore precise and stable convergence. Extensive experiments demonstrate that\nProMoGen excels in synthesizing vivid and diverse motions guided by predefined\ntrajectory and arbitrary anchor frames. Our approach seamlessly integrates\npersonalized motion with structured guidance, significantly outperforming\nstate-of-the-art methods across multiple control scenarios.\n", "link": "http://arxiv.org/abs/2504.16722v1", "date": "2025-04-23", "relevancy": 3.0641, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6661}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.597}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PMG%3A%20Progressive%20Motion%20Generation%20via%20Sparse%20Anchor%20Postures%20Curriculum%0A%20%20Learning&body=Title%3A%20PMG%3A%20Progressive%20Motion%20Generation%20via%20Sparse%20Anchor%20Postures%20Curriculum%0A%20%20Learning%0AAuthor%3A%20Yingjie%20Xi%20and%20Jian%20Jun%20Zhang%20and%20Xiaosong%20Yang%0AAbstract%3A%20%20%20In%20computer%20animation%2C%20game%20design%2C%20and%20human-computer%20interaction%2C%0Asynthesizing%20human%20motion%20that%20aligns%20with%20user%20intent%20remains%20a%20significant%0Achallenge.%20Existing%20methods%20have%20notable%20limitations%3A%20textual%20approaches%20offer%0Ahigh-level%20semantic%20guidance%20but%20struggle%20to%20describe%20complex%20actions%0Aaccurately%3B%20trajectory-based%20techniques%20provide%20intuitive%20global%20motion%0Adirection%20yet%20often%20fall%20short%20in%20generating%20precise%20or%20customized%20character%0Amovements%3B%20and%20anchor%20poses-guided%20methods%20are%20typically%20confined%20to%20synthesize%0Aonly%20simple%20motion%20patterns.%20To%20generate%20more%20controllable%20and%20precise%20human%0Amotions%2C%20we%20propose%20%5Ctextbf%7BProMoGen%20%28Progressive%20Motion%20Generation%29%7D%2C%20a%20novel%0Aframework%20that%20integrates%20trajectory%20guidance%20with%20sparse%20anchor%20motion%0Acontrol.%20Global%20trajectories%20ensure%20consistency%20in%20spatial%20direction%20and%0Adisplacement%2C%20while%20sparse%20anchor%20motions%20only%20deliver%20precise%20action%20guidance%0Awithout%20displacement.%20This%20decoupling%20enables%20independent%20refinement%20of%20both%0Aaspects%2C%20resulting%20in%20a%20more%20controllable%2C%20high-fidelity%2C%20and%20sophisticated%0Amotion%20synthesis.%20ProMoGen%20supports%20both%20dual%20and%20single%20control%20paradigms%0Awithin%20a%20unified%20training%20process.%20Moreover%2C%20we%20recognize%20that%20direct%20learning%0Afrom%20sparse%20motions%20is%20inherently%20unstable%2C%20we%20introduce%20%5Ctextbf%7BSAP-CL%20%28Sparse%0AAnchor%20Posture%20Curriculum%20Learning%29%7D%2C%20a%20curriculum%20learning%20strategy%20that%0Aprogressively%20adjusts%20the%20number%20of%20anchors%20used%20for%20guidance%2C%20thereby%20enabling%0Amore%20precise%20and%20stable%20convergence.%20Extensive%20experiments%20demonstrate%20that%0AProMoGen%20excels%20in%20synthesizing%20vivid%20and%20diverse%20motions%20guided%20by%20predefined%0Atrajectory%20and%20arbitrary%20anchor%20frames.%20Our%20approach%20seamlessly%20integrates%0Apersonalized%20motion%20with%20structured%20guidance%2C%20significantly%20outperforming%0Astate-of-the-art%20methods%20across%20multiple%20control%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPMG%253A%2520Progressive%2520Motion%2520Generation%2520via%2520Sparse%2520Anchor%2520Postures%2520Curriculum%250A%2520%2520Learning%26entry.906535625%3DYingjie%2520Xi%2520and%2520Jian%2520Jun%2520Zhang%2520and%2520Xiaosong%2520Yang%26entry.1292438233%3D%2520%2520In%2520computer%2520animation%252C%2520game%2520design%252C%2520and%2520human-computer%2520interaction%252C%250Asynthesizing%2520human%2520motion%2520that%2520aligns%2520with%2520user%2520intent%2520remains%2520a%2520significant%250Achallenge.%2520Existing%2520methods%2520have%2520notable%2520limitations%253A%2520textual%2520approaches%2520offer%250Ahigh-level%2520semantic%2520guidance%2520but%2520struggle%2520to%2520describe%2520complex%2520actions%250Aaccurately%253B%2520trajectory-based%2520techniques%2520provide%2520intuitive%2520global%2520motion%250Adirection%2520yet%2520often%2520fall%2520short%2520in%2520generating%2520precise%2520or%2520customized%2520character%250Amovements%253B%2520and%2520anchor%2520poses-guided%2520methods%2520are%2520typically%2520confined%2520to%2520synthesize%250Aonly%2520simple%2520motion%2520patterns.%2520To%2520generate%2520more%2520controllable%2520and%2520precise%2520human%250Amotions%252C%2520we%2520propose%2520%255Ctextbf%257BProMoGen%2520%2528Progressive%2520Motion%2520Generation%2529%257D%252C%2520a%2520novel%250Aframework%2520that%2520integrates%2520trajectory%2520guidance%2520with%2520sparse%2520anchor%2520motion%250Acontrol.%2520Global%2520trajectories%2520ensure%2520consistency%2520in%2520spatial%2520direction%2520and%250Adisplacement%252C%2520while%2520sparse%2520anchor%2520motions%2520only%2520deliver%2520precise%2520action%2520guidance%250Awithout%2520displacement.%2520This%2520decoupling%2520enables%2520independent%2520refinement%2520of%2520both%250Aaspects%252C%2520resulting%2520in%2520a%2520more%2520controllable%252C%2520high-fidelity%252C%2520and%2520sophisticated%250Amotion%2520synthesis.%2520ProMoGen%2520supports%2520both%2520dual%2520and%2520single%2520control%2520paradigms%250Awithin%2520a%2520unified%2520training%2520process.%2520Moreover%252C%2520we%2520recognize%2520that%2520direct%2520learning%250Afrom%2520sparse%2520motions%2520is%2520inherently%2520unstable%252C%2520we%2520introduce%2520%255Ctextbf%257BSAP-CL%2520%2528Sparse%250AAnchor%2520Posture%2520Curriculum%2520Learning%2529%257D%252C%2520a%2520curriculum%2520learning%2520strategy%2520that%250Aprogressively%2520adjusts%2520the%2520number%2520of%2520anchors%2520used%2520for%2520guidance%252C%2520thereby%2520enabling%250Amore%2520precise%2520and%2520stable%2520convergence.%2520Extensive%2520experiments%2520demonstrate%2520that%250AProMoGen%2520excels%2520in%2520synthesizing%2520vivid%2520and%2520diverse%2520motions%2520guided%2520by%2520predefined%250Atrajectory%2520and%2520arbitrary%2520anchor%2520frames.%2520Our%2520approach%2520seamlessly%2520integrates%250Apersonalized%2520motion%2520with%2520structured%2520guidance%252C%2520significantly%2520outperforming%250Astate-of-the-art%2520methods%2520across%2520multiple%2520control%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PMG%3A%20Progressive%20Motion%20Generation%20via%20Sparse%20Anchor%20Postures%20Curriculum%0A%20%20Learning&entry.906535625=Yingjie%20Xi%20and%20Jian%20Jun%20Zhang%20and%20Xiaosong%20Yang&entry.1292438233=%20%20In%20computer%20animation%2C%20game%20design%2C%20and%20human-computer%20interaction%2C%0Asynthesizing%20human%20motion%20that%20aligns%20with%20user%20intent%20remains%20a%20significant%0Achallenge.%20Existing%20methods%20have%20notable%20limitations%3A%20textual%20approaches%20offer%0Ahigh-level%20semantic%20guidance%20but%20struggle%20to%20describe%20complex%20actions%0Aaccurately%3B%20trajectory-based%20techniques%20provide%20intuitive%20global%20motion%0Adirection%20yet%20often%20fall%20short%20in%20generating%20precise%20or%20customized%20character%0Amovements%3B%20and%20anchor%20poses-guided%20methods%20are%20typically%20confined%20to%20synthesize%0Aonly%20simple%20motion%20patterns.%20To%20generate%20more%20controllable%20and%20precise%20human%0Amotions%2C%20we%20propose%20%5Ctextbf%7BProMoGen%20%28Progressive%20Motion%20Generation%29%7D%2C%20a%20novel%0Aframework%20that%20integrates%20trajectory%20guidance%20with%20sparse%20anchor%20motion%0Acontrol.%20Global%20trajectories%20ensure%20consistency%20in%20spatial%20direction%20and%0Adisplacement%2C%20while%20sparse%20anchor%20motions%20only%20deliver%20precise%20action%20guidance%0Awithout%20displacement.%20This%20decoupling%20enables%20independent%20refinement%20of%20both%0Aaspects%2C%20resulting%20in%20a%20more%20controllable%2C%20high-fidelity%2C%20and%20sophisticated%0Amotion%20synthesis.%20ProMoGen%20supports%20both%20dual%20and%20single%20control%20paradigms%0Awithin%20a%20unified%20training%20process.%20Moreover%2C%20we%20recognize%20that%20direct%20learning%0Afrom%20sparse%20motions%20is%20inherently%20unstable%2C%20we%20introduce%20%5Ctextbf%7BSAP-CL%20%28Sparse%0AAnchor%20Posture%20Curriculum%20Learning%29%7D%2C%20a%20curriculum%20learning%20strategy%20that%0Aprogressively%20adjusts%20the%20number%20of%20anchors%20used%20for%20guidance%2C%20thereby%20enabling%0Amore%20precise%20and%20stable%20convergence.%20Extensive%20experiments%20demonstrate%20that%0AProMoGen%20excels%20in%20synthesizing%20vivid%20and%20diverse%20motions%20guided%20by%20predefined%0Atrajectory%20and%20arbitrary%20anchor%20frames.%20Our%20approach%20seamlessly%20integrates%0Apersonalized%20motion%20with%20structured%20guidance%2C%20significantly%20outperforming%0Astate-of-the-art%20methods%20across%20multiple%20control%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16722v1&entry.124074799=Read"},
{"title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding", "author": "Xiaoxing Hu and Kaicheng Yang and Jun Wang and Haoran Xu and Ziyong Feng and Yupei Wang", "abstract": "  Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA\n", "link": "http://arxiv.org/abs/2504.16801v1", "date": "2025-04-23", "relevancy": 2.955, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5979}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5964}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupled%20Global-Local%20Alignment%20for%20Improving%20Compositional%0A%20%20Understanding&body=Title%3A%20Decoupled%20Global-Local%20Alignment%20for%20Improving%20Compositional%0A%20%20Understanding%0AAuthor%3A%20Xiaoxing%20Hu%20and%20Kaicheng%20Yang%20and%20Jun%20Wang%20and%20Haoran%20Xu%20and%20Ziyong%20Feng%20and%20Yupei%20Wang%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20achieved%20success%20on%0Amultiple%20downstream%20tasks%20by%20aligning%20image%20and%20text%20modalities.%20However%2C%20the%0Anature%20of%20global%20contrastive%20learning%20limits%20CLIP%27s%20ability%20to%20comprehend%0Acompositional%20concepts%2C%20such%20as%20relations%20and%20attributes.%20Although%20recent%0Astudies%20employ%20global%20hard%20negative%20samples%20to%20improve%20compositional%0Aunderstanding%2C%20these%20methods%20significantly%20compromise%20the%20model%27s%20inherent%0Ageneral%20capabilities%20by%20forcibly%20distancing%20textual%20negative%20samples%20from%0Aimages%20in%20the%20embedding%20space.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%0ADecoupled%20Global-Local%20Alignment%20%28DeGLA%29%20framework%20that%20improves%20compositional%0Aunderstanding%20while%20substantially%20mitigating%20losses%20in%20general%20capabilities.%20To%0Aoptimize%20the%20retention%20of%20the%20model%27s%20inherent%20capabilities%2C%20we%20incorporate%20a%0Aself-distillation%20mechanism%20within%20the%20global%20alignment%20process%2C%20aligning%20the%0Alearnable%20image-text%20encoder%20with%20a%20frozen%20teacher%20model%20derived%20from%20an%0Aexponential%20moving%20average.%20Under%20the%20constraint%20of%20self-distillation%2C%20it%0Aeffectively%20mitigates%20the%20catastrophic%20forgetting%20of%20pretrained%20knowledge%0Aduring%20fine-tuning.%20To%20improve%20compositional%20understanding%2C%20we%20first%20leverage%0Athe%20in-context%20learning%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20construct%0Aabout%202M%20high-quality%20negative%20captions%20across%20five%20types.%20Subsequently%2C%20we%0Apropose%20the%20Image-Grounded%20Contrast%20%28IGC%29%20loss%20and%20Text-Grounded%20Contrast%20%28TGC%29%0Aloss%20to%20enhance%20vision-language%20compositionally.%20Extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20DeGLA%20framework.%20Compared%20to%20previous%0Astate-of-the-art%20methods%2C%20DeGLA%20achieves%20an%20average%20enhancement%20of%203.5%25%20across%0Athe%20VALSE%2C%20SugarCrepe%2C%20and%20ARO%20benchmarks.%20Concurrently%2C%20it%20obtains%20an%20average%0Aperformance%20improvement%20of%2013.0%25%20on%20zero-shot%20classification%20tasks%20across%0Aeleven%20datasets.%20Our%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/xiaoxing2001/DeGLA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupled%2520Global-Local%2520Alignment%2520for%2520Improving%2520Compositional%250A%2520%2520Understanding%26entry.906535625%3DXiaoxing%2520Hu%2520and%2520Kaicheng%2520Yang%2520and%2520Jun%2520Wang%2520and%2520Haoran%2520Xu%2520and%2520Ziyong%2520Feng%2520and%2520Yupei%2520Wang%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520achieved%2520success%2520on%250Amultiple%2520downstream%2520tasks%2520by%2520aligning%2520image%2520and%2520text%2520modalities.%2520However%252C%2520the%250Anature%2520of%2520global%2520contrastive%2520learning%2520limits%2520CLIP%2527s%2520ability%2520to%2520comprehend%250Acompositional%2520concepts%252C%2520such%2520as%2520relations%2520and%2520attributes.%2520Although%2520recent%250Astudies%2520employ%2520global%2520hard%2520negative%2520samples%2520to%2520improve%2520compositional%250Aunderstanding%252C%2520these%2520methods%2520significantly%2520compromise%2520the%2520model%2527s%2520inherent%250Ageneral%2520capabilities%2520by%2520forcibly%2520distancing%2520textual%2520negative%2520samples%2520from%250Aimages%2520in%2520the%2520embedding%2520space.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520a%250ADecoupled%2520Global-Local%2520Alignment%2520%2528DeGLA%2529%2520framework%2520that%2520improves%2520compositional%250Aunderstanding%2520while%2520substantially%2520mitigating%2520losses%2520in%2520general%2520capabilities.%2520To%250Aoptimize%2520the%2520retention%2520of%2520the%2520model%2527s%2520inherent%2520capabilities%252C%2520we%2520incorporate%2520a%250Aself-distillation%2520mechanism%2520within%2520the%2520global%2520alignment%2520process%252C%2520aligning%2520the%250Alearnable%2520image-text%2520encoder%2520with%2520a%2520frozen%2520teacher%2520model%2520derived%2520from%2520an%250Aexponential%2520moving%2520average.%2520Under%2520the%2520constraint%2520of%2520self-distillation%252C%2520it%250Aeffectively%2520mitigates%2520the%2520catastrophic%2520forgetting%2520of%2520pretrained%2520knowledge%250Aduring%2520fine-tuning.%2520To%2520improve%2520compositional%2520understanding%252C%2520we%2520first%2520leverage%250Athe%2520in-context%2520learning%2520capability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520construct%250Aabout%25202M%2520high-quality%2520negative%2520captions%2520across%2520five%2520types.%2520Subsequently%252C%2520we%250Apropose%2520the%2520Image-Grounded%2520Contrast%2520%2528IGC%2529%2520loss%2520and%2520Text-Grounded%2520Contrast%2520%2528TGC%2529%250Aloss%2520to%2520enhance%2520vision-language%2520compositionally.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520DeGLA%2520framework.%2520Compared%2520to%2520previous%250Astate-of-the-art%2520methods%252C%2520DeGLA%2520achieves%2520an%2520average%2520enhancement%2520of%25203.5%2525%2520across%250Athe%2520VALSE%252C%2520SugarCrepe%252C%2520and%2520ARO%2520benchmarks.%2520Concurrently%252C%2520it%2520obtains%2520an%2520average%250Aperformance%2520improvement%2520of%252013.0%2525%2520on%2520zero-shot%2520classification%2520tasks%2520across%250Aeleven%2520datasets.%2520Our%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/xiaoxing2001/DeGLA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20Global-Local%20Alignment%20for%20Improving%20Compositional%0A%20%20Understanding&entry.906535625=Xiaoxing%20Hu%20and%20Kaicheng%20Yang%20and%20Jun%20Wang%20and%20Haoran%20Xu%20and%20Ziyong%20Feng%20and%20Yupei%20Wang&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20achieved%20success%20on%0Amultiple%20downstream%20tasks%20by%20aligning%20image%20and%20text%20modalities.%20However%2C%20the%0Anature%20of%20global%20contrastive%20learning%20limits%20CLIP%27s%20ability%20to%20comprehend%0Acompositional%20concepts%2C%20such%20as%20relations%20and%20attributes.%20Although%20recent%0Astudies%20employ%20global%20hard%20negative%20samples%20to%20improve%20compositional%0Aunderstanding%2C%20these%20methods%20significantly%20compromise%20the%20model%27s%20inherent%0Ageneral%20capabilities%20by%20forcibly%20distancing%20textual%20negative%20samples%20from%0Aimages%20in%20the%20embedding%20space.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%0ADecoupled%20Global-Local%20Alignment%20%28DeGLA%29%20framework%20that%20improves%20compositional%0Aunderstanding%20while%20substantially%20mitigating%20losses%20in%20general%20capabilities.%20To%0Aoptimize%20the%20retention%20of%20the%20model%27s%20inherent%20capabilities%2C%20we%20incorporate%20a%0Aself-distillation%20mechanism%20within%20the%20global%20alignment%20process%2C%20aligning%20the%0Alearnable%20image-text%20encoder%20with%20a%20frozen%20teacher%20model%20derived%20from%20an%0Aexponential%20moving%20average.%20Under%20the%20constraint%20of%20self-distillation%2C%20it%0Aeffectively%20mitigates%20the%20catastrophic%20forgetting%20of%20pretrained%20knowledge%0Aduring%20fine-tuning.%20To%20improve%20compositional%20understanding%2C%20we%20first%20leverage%0Athe%20in-context%20learning%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20construct%0Aabout%202M%20high-quality%20negative%20captions%20across%20five%20types.%20Subsequently%2C%20we%0Apropose%20the%20Image-Grounded%20Contrast%20%28IGC%29%20loss%20and%20Text-Grounded%20Contrast%20%28TGC%29%0Aloss%20to%20enhance%20vision-language%20compositionally.%20Extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20DeGLA%20framework.%20Compared%20to%20previous%0Astate-of-the-art%20methods%2C%20DeGLA%20achieves%20an%20average%20enhancement%20of%203.5%25%20across%0Athe%20VALSE%2C%20SugarCrepe%2C%20and%20ARO%20benchmarks.%20Concurrently%2C%20it%20obtains%20an%20average%0Aperformance%20improvement%20of%2013.0%25%20on%20zero-shot%20classification%20tasks%20across%0Aeleven%20datasets.%20Our%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/xiaoxing2001/DeGLA%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16801v1&entry.124074799=Read"},
{"title": "V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental\n  Visual Variations", "author": "Zhiyuan Fan and Yumeng Wang and Sandeep Polisetty and Yi R. and  Fung", "abstract": "  Large Vision Language Models (LVLMs) excel in various vision-language tasks.\nYet, their robustness to visual variations in position, scale, orientation, and\ncontext that objects in natural scenes inevitably exhibit due to changes in\nviewpoint and environment remains largely underexplored. To bridge this gap, we\nintroduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating\nVisual Variation Robustness of LVLMs, which encompasses automated evaluation\ndataset generation and principled metrics for thorough robustness assessment.\nThrough extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability\nto visual variations, in which even advanced models that excel at complex\nvision-language tasks significantly underperform on simple tasks such as object\nrecognition. Interestingly, these models exhibit a distinct visual position\nbias that contradicts theories of effective receptive fields, and demonstrate a\nhuman-like visual acuity threshold. To identify the source of these\nvulnerabilities, we present a systematic framework for component-level\nanalysis, featuring a novel visualization approach for aligned visual features.\nResults show that these vulnerabilities stem from error accumulation in the\npipeline architecture and inadequate multimodal alignment. Complementary\nexperiments with synthetic data further demonstrate that these limitations are\nfundamentally architectural deficiencies, scoring the need for architectural\ninnovations in future LVLM designs.\n", "link": "http://arxiv.org/abs/2504.16727v1", "date": "2025-04-23", "relevancy": 2.879, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6041}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V%24%5E2%24R-Bench%3A%20Holistically%20Evaluating%20LVLM%20Robustness%20to%20Fundamental%0A%20%20Visual%20Variations&body=Title%3A%20V%24%5E2%24R-Bench%3A%20Holistically%20Evaluating%20LVLM%20Robustness%20to%20Fundamental%0A%20%20Visual%20Variations%0AAuthor%3A%20Zhiyuan%20Fan%20and%20Yumeng%20Wang%20and%20Sandeep%20Polisetty%20and%20Yi%20R.%20and%20%20Fung%0AAbstract%3A%20%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20excel%20in%20various%20vision-language%20tasks.%0AYet%2C%20their%20robustness%20to%20visual%20variations%20in%20position%2C%20scale%2C%20orientation%2C%20and%0Acontext%20that%20objects%20in%20natural%20scenes%20inevitably%20exhibit%20due%20to%20changes%20in%0Aviewpoint%20and%20environment%20remains%20largely%20underexplored.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20V%24%5E2%24R-Bench%2C%20a%20comprehensive%20benchmark%20framework%20for%20evaluating%0AVisual%20Variation%20Robustness%20of%20LVLMs%2C%20which%20encompasses%20automated%20evaluation%0Adataset%20generation%20and%20principled%20metrics%20for%20thorough%20robustness%20assessment.%0AThrough%20extensive%20evaluation%20on%2021%20LVLMs%2C%20we%20reveal%20a%20surprising%20vulnerability%0Ato%20visual%20variations%2C%20in%20which%20even%20advanced%20models%20that%20excel%20at%20complex%0Avision-language%20tasks%20significantly%20underperform%20on%20simple%20tasks%20such%20as%20object%0Arecognition.%20Interestingly%2C%20these%20models%20exhibit%20a%20distinct%20visual%20position%0Abias%20that%20contradicts%20theories%20of%20effective%20receptive%20fields%2C%20and%20demonstrate%20a%0Ahuman-like%20visual%20acuity%20threshold.%20To%20identify%20the%20source%20of%20these%0Avulnerabilities%2C%20we%20present%20a%20systematic%20framework%20for%20component-level%0Aanalysis%2C%20featuring%20a%20novel%20visualization%20approach%20for%20aligned%20visual%20features.%0AResults%20show%20that%20these%20vulnerabilities%20stem%20from%20error%20accumulation%20in%20the%0Apipeline%20architecture%20and%20inadequate%20multimodal%20alignment.%20Complementary%0Aexperiments%20with%20synthetic%20data%20further%20demonstrate%20that%20these%20limitations%20are%0Afundamentally%20architectural%20deficiencies%2C%20scoring%20the%20need%20for%20architectural%0Ainnovations%20in%20future%20LVLM%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV%2524%255E2%2524R-Bench%253A%2520Holistically%2520Evaluating%2520LVLM%2520Robustness%2520to%2520Fundamental%250A%2520%2520Visual%2520Variations%26entry.906535625%3DZhiyuan%2520Fan%2520and%2520Yumeng%2520Wang%2520and%2520Sandeep%2520Polisetty%2520and%2520Yi%2520R.%2520and%2520%2520Fung%26entry.1292438233%3D%2520%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520excel%2520in%2520various%2520vision-language%2520tasks.%250AYet%252C%2520their%2520robustness%2520to%2520visual%2520variations%2520in%2520position%252C%2520scale%252C%2520orientation%252C%2520and%250Acontext%2520that%2520objects%2520in%2520natural%2520scenes%2520inevitably%2520exhibit%2520due%2520to%2520changes%2520in%250Aviewpoint%2520and%2520environment%2520remains%2520largely%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520V%2524%255E2%2524R-Bench%252C%2520a%2520comprehensive%2520benchmark%2520framework%2520for%2520evaluating%250AVisual%2520Variation%2520Robustness%2520of%2520LVLMs%252C%2520which%2520encompasses%2520automated%2520evaluation%250Adataset%2520generation%2520and%2520principled%2520metrics%2520for%2520thorough%2520robustness%2520assessment.%250AThrough%2520extensive%2520evaluation%2520on%252021%2520LVLMs%252C%2520we%2520reveal%2520a%2520surprising%2520vulnerability%250Ato%2520visual%2520variations%252C%2520in%2520which%2520even%2520advanced%2520models%2520that%2520excel%2520at%2520complex%250Avision-language%2520tasks%2520significantly%2520underperform%2520on%2520simple%2520tasks%2520such%2520as%2520object%250Arecognition.%2520Interestingly%252C%2520these%2520models%2520exhibit%2520a%2520distinct%2520visual%2520position%250Abias%2520that%2520contradicts%2520theories%2520of%2520effective%2520receptive%2520fields%252C%2520and%2520demonstrate%2520a%250Ahuman-like%2520visual%2520acuity%2520threshold.%2520To%2520identify%2520the%2520source%2520of%2520these%250Avulnerabilities%252C%2520we%2520present%2520a%2520systematic%2520framework%2520for%2520component-level%250Aanalysis%252C%2520featuring%2520a%2520novel%2520visualization%2520approach%2520for%2520aligned%2520visual%2520features.%250AResults%2520show%2520that%2520these%2520vulnerabilities%2520stem%2520from%2520error%2520accumulation%2520in%2520the%250Apipeline%2520architecture%2520and%2520inadequate%2520multimodal%2520alignment.%2520Complementary%250Aexperiments%2520with%2520synthetic%2520data%2520further%2520demonstrate%2520that%2520these%2520limitations%2520are%250Afundamentally%2520architectural%2520deficiencies%252C%2520scoring%2520the%2520need%2520for%2520architectural%250Ainnovations%2520in%2520future%2520LVLM%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V%24%5E2%24R-Bench%3A%20Holistically%20Evaluating%20LVLM%20Robustness%20to%20Fundamental%0A%20%20Visual%20Variations&entry.906535625=Zhiyuan%20Fan%20and%20Yumeng%20Wang%20and%20Sandeep%20Polisetty%20and%20Yi%20R.%20and%20%20Fung&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20excel%20in%20various%20vision-language%20tasks.%0AYet%2C%20their%20robustness%20to%20visual%20variations%20in%20position%2C%20scale%2C%20orientation%2C%20and%0Acontext%20that%20objects%20in%20natural%20scenes%20inevitably%20exhibit%20due%20to%20changes%20in%0Aviewpoint%20and%20environment%20remains%20largely%20underexplored.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20V%24%5E2%24R-Bench%2C%20a%20comprehensive%20benchmark%20framework%20for%20evaluating%0AVisual%20Variation%20Robustness%20of%20LVLMs%2C%20which%20encompasses%20automated%20evaluation%0Adataset%20generation%20and%20principled%20metrics%20for%20thorough%20robustness%20assessment.%0AThrough%20extensive%20evaluation%20on%2021%20LVLMs%2C%20we%20reveal%20a%20surprising%20vulnerability%0Ato%20visual%20variations%2C%20in%20which%20even%20advanced%20models%20that%20excel%20at%20complex%0Avision-language%20tasks%20significantly%20underperform%20on%20simple%20tasks%20such%20as%20object%0Arecognition.%20Interestingly%2C%20these%20models%20exhibit%20a%20distinct%20visual%20position%0Abias%20that%20contradicts%20theories%20of%20effective%20receptive%20fields%2C%20and%20demonstrate%20a%0Ahuman-like%20visual%20acuity%20threshold.%20To%20identify%20the%20source%20of%20these%0Avulnerabilities%2C%20we%20present%20a%20systematic%20framework%20for%20component-level%0Aanalysis%2C%20featuring%20a%20novel%20visualization%20approach%20for%20aligned%20visual%20features.%0AResults%20show%20that%20these%20vulnerabilities%20stem%20from%20error%20accumulation%20in%20the%0Apipeline%20architecture%20and%20inadequate%20multimodal%20alignment.%20Complementary%0Aexperiments%20with%20synthetic%20data%20further%20demonstrate%20that%20these%20limitations%20are%0Afundamentally%20architectural%20deficiencies%2C%20scoring%20the%20need%20for%20architectural%0Ainnovations%20in%20future%20LVLM%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16727v1&entry.124074799=Read"},
{"title": "MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion", "author": "Saron Samuel and Dan DeGenaro and Jimena Guallar-Blasco and Kate Sanders and Oluwaseun Eisape and Arun Reddy and Alexander Martin and Andrew Yates and Eugene Yang and Cameron Carpenter and David Etter and Efsun Kayi and Matthew Wiesner and Kenton Murray and Reno Kriz", "abstract": "  Videos inherently contain multiple modalities, including visual events, text\noverlays, sounds, and speech, all of which are important for retrieval.\nHowever, state-of-the-art multimodal language models like VAST and LanguageBind\nare built on vision-language models (VLMs), and thus overly prioritize visual\nsignals. Retrieval benchmarks further reinforce this bias by focusing on visual\nqueries and neglecting other modalities. We create a search system MMMORRF that\nextracts text and features from both visual and audio modalities and integrates\nthem with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is\nboth effective and efficient, demonstrating practicality in searching videos\nbased on users' information needs instead of visual descriptive queries. We\nevaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed\nfor more targeted information needs, and find that it improves nDCG@20 by 81%\nover leading multimodal encoders and 37% over single-modality retrieval,\ndemonstrating the value of integrating diverse modalities.\n", "link": "http://arxiv.org/abs/2503.20698v3", "date": "2025-04-23", "relevancy": 2.8398, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMMORRF%3A%20Multimodal%20Multilingual%20Modularized%20Reciprocal%20Rank%20Fusion&body=Title%3A%20MMMORRF%3A%20Multimodal%20Multilingual%20Modularized%20Reciprocal%20Rank%20Fusion%0AAuthor%3A%20Saron%20Samuel%20and%20Dan%20DeGenaro%20and%20Jimena%20Guallar-Blasco%20and%20Kate%20Sanders%20and%20Oluwaseun%20Eisape%20and%20Arun%20Reddy%20and%20Alexander%20Martin%20and%20Andrew%20Yates%20and%20Eugene%20Yang%20and%20Cameron%20Carpenter%20and%20David%20Etter%20and%20Efsun%20Kayi%20and%20Matthew%20Wiesner%20and%20Kenton%20Murray%20and%20Reno%20Kriz%0AAbstract%3A%20%20%20Videos%20inherently%20contain%20multiple%20modalities%2C%20including%20visual%20events%2C%20text%0Aoverlays%2C%20sounds%2C%20and%20speech%2C%20all%20of%20which%20are%20important%20for%20retrieval.%0AHowever%2C%20state-of-the-art%20multimodal%20language%20models%20like%20VAST%20and%20LanguageBind%0Aare%20built%20on%20vision-language%20models%20%28VLMs%29%2C%20and%20thus%20overly%20prioritize%20visual%0Asignals.%20Retrieval%20benchmarks%20further%20reinforce%20this%20bias%20by%20focusing%20on%20visual%0Aqueries%20and%20neglecting%20other%20modalities.%20We%20create%20a%20search%20system%20MMMORRF%20that%0Aextracts%20text%20and%20features%20from%20both%20visual%20and%20audio%20modalities%20and%20integrates%0Athem%20with%20a%20novel%20modality-aware%20weighted%20reciprocal%20rank%20fusion.%20MMMORRF%20is%0Aboth%20effective%20and%20efficient%2C%20demonstrating%20practicality%20in%20searching%20videos%0Abased%20on%20users%27%20information%20needs%20instead%20of%20visual%20descriptive%20queries.%20We%0Aevaluate%20MMMORRF%20on%20MultiVENT%202.0%20and%20TVR%2C%20two%20multimodal%20benchmarks%20designed%0Afor%20more%20targeted%20information%20needs%2C%20and%20find%20that%20it%20improves%20nDCG%4020%20by%2081%25%0Aover%20leading%20multimodal%20encoders%20and%2037%25%20over%20single-modality%20retrieval%2C%0Ademonstrating%20the%20value%20of%20integrating%20diverse%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20698v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMMORRF%253A%2520Multimodal%2520Multilingual%2520Modularized%2520Reciprocal%2520Rank%2520Fusion%26entry.906535625%3DSaron%2520Samuel%2520and%2520Dan%2520DeGenaro%2520and%2520Jimena%2520Guallar-Blasco%2520and%2520Kate%2520Sanders%2520and%2520Oluwaseun%2520Eisape%2520and%2520Arun%2520Reddy%2520and%2520Alexander%2520Martin%2520and%2520Andrew%2520Yates%2520and%2520Eugene%2520Yang%2520and%2520Cameron%2520Carpenter%2520and%2520David%2520Etter%2520and%2520Efsun%2520Kayi%2520and%2520Matthew%2520Wiesner%2520and%2520Kenton%2520Murray%2520and%2520Reno%2520Kriz%26entry.1292438233%3D%2520%2520Videos%2520inherently%2520contain%2520multiple%2520modalities%252C%2520including%2520visual%2520events%252C%2520text%250Aoverlays%252C%2520sounds%252C%2520and%2520speech%252C%2520all%2520of%2520which%2520are%2520important%2520for%2520retrieval.%250AHowever%252C%2520state-of-the-art%2520multimodal%2520language%2520models%2520like%2520VAST%2520and%2520LanguageBind%250Aare%2520built%2520on%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520and%2520thus%2520overly%2520prioritize%2520visual%250Asignals.%2520Retrieval%2520benchmarks%2520further%2520reinforce%2520this%2520bias%2520by%2520focusing%2520on%2520visual%250Aqueries%2520and%2520neglecting%2520other%2520modalities.%2520We%2520create%2520a%2520search%2520system%2520MMMORRF%2520that%250Aextracts%2520text%2520and%2520features%2520from%2520both%2520visual%2520and%2520audio%2520modalities%2520and%2520integrates%250Athem%2520with%2520a%2520novel%2520modality-aware%2520weighted%2520reciprocal%2520rank%2520fusion.%2520MMMORRF%2520is%250Aboth%2520effective%2520and%2520efficient%252C%2520demonstrating%2520practicality%2520in%2520searching%2520videos%250Abased%2520on%2520users%2527%2520information%2520needs%2520instead%2520of%2520visual%2520descriptive%2520queries.%2520We%250Aevaluate%2520MMMORRF%2520on%2520MultiVENT%25202.0%2520and%2520TVR%252C%2520two%2520multimodal%2520benchmarks%2520designed%250Afor%2520more%2520targeted%2520information%2520needs%252C%2520and%2520find%2520that%2520it%2520improves%2520nDCG%254020%2520by%252081%2525%250Aover%2520leading%2520multimodal%2520encoders%2520and%252037%2525%2520over%2520single-modality%2520retrieval%252C%250Ademonstrating%2520the%2520value%2520of%2520integrating%2520diverse%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20698v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMMORRF%3A%20Multimodal%20Multilingual%20Modularized%20Reciprocal%20Rank%20Fusion&entry.906535625=Saron%20Samuel%20and%20Dan%20DeGenaro%20and%20Jimena%20Guallar-Blasco%20and%20Kate%20Sanders%20and%20Oluwaseun%20Eisape%20and%20Arun%20Reddy%20and%20Alexander%20Martin%20and%20Andrew%20Yates%20and%20Eugene%20Yang%20and%20Cameron%20Carpenter%20and%20David%20Etter%20and%20Efsun%20Kayi%20and%20Matthew%20Wiesner%20and%20Kenton%20Murray%20and%20Reno%20Kriz&entry.1292438233=%20%20Videos%20inherently%20contain%20multiple%20modalities%2C%20including%20visual%20events%2C%20text%0Aoverlays%2C%20sounds%2C%20and%20speech%2C%20all%20of%20which%20are%20important%20for%20retrieval.%0AHowever%2C%20state-of-the-art%20multimodal%20language%20models%20like%20VAST%20and%20LanguageBind%0Aare%20built%20on%20vision-language%20models%20%28VLMs%29%2C%20and%20thus%20overly%20prioritize%20visual%0Asignals.%20Retrieval%20benchmarks%20further%20reinforce%20this%20bias%20by%20focusing%20on%20visual%0Aqueries%20and%20neglecting%20other%20modalities.%20We%20create%20a%20search%20system%20MMMORRF%20that%0Aextracts%20text%20and%20features%20from%20both%20visual%20and%20audio%20modalities%20and%20integrates%0Athem%20with%20a%20novel%20modality-aware%20weighted%20reciprocal%20rank%20fusion.%20MMMORRF%20is%0Aboth%20effective%20and%20efficient%2C%20demonstrating%20practicality%20in%20searching%20videos%0Abased%20on%20users%27%20information%20needs%20instead%20of%20visual%20descriptive%20queries.%20We%0Aevaluate%20MMMORRF%20on%20MultiVENT%202.0%20and%20TVR%2C%20two%20multimodal%20benchmarks%20designed%0Afor%20more%20targeted%20information%20needs%2C%20and%20find%20that%20it%20improves%20nDCG%4020%20by%2081%25%0Aover%20leading%20multimodal%20encoders%20and%2037%25%20over%20single-modality%20retrieval%2C%0Ademonstrating%20the%20value%20of%20integrating%20diverse%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20698v3&entry.124074799=Read"},
{"title": "I-Con: A Unifying Framework for Representation Learning", "author": "Shaden Alshammari and John Hershey and Axel Feldmann and William T. Freeman and Mark Hamilton", "abstract": "  As the field of representation learning grows, there has been a proliferation\nof different loss functions to solve different classes of problems. We\nintroduce a single information-theoretic equation that generalizes a large\ncollection of modern loss functions in machine learning. In particular, we\nintroduce a framework that shows that several broad classes of machine learning\nmethods are precisely minimizing an integrated KL divergence between two\nconditional distributions: the supervisory and learned representations. This\nviewpoint exposes a hidden information geometry underlying clustering, spectral\nmethods, dimensionality reduction, contrastive learning, and supervised\nlearning. This framework enables the development of new loss functions by\ncombining successful techniques from across the literature. We not only present\na wide array of proofs, connecting over 23 different approaches, but we also\nleverage these theoretical results to create state-of-the-art unsupervised\nimage classifiers that achieve a +8% improvement over the prior\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\ndemonstrate that I-Con can be used to derive principled debiasing methods which\nimprove contrastive representation learners.\n", "link": "http://arxiv.org/abs/2504.16929v1", "date": "2025-04-23", "relevancy": 2.8313, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5827}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.57}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I-Con%3A%20A%20Unifying%20Framework%20for%20Representation%20Learning&body=Title%3A%20I-Con%3A%20A%20Unifying%20Framework%20for%20Representation%20Learning%0AAuthor%3A%20Shaden%20Alshammari%20and%20John%20Hershey%20and%20Axel%20Feldmann%20and%20William%20T.%20Freeman%20and%20Mark%20Hamilton%0AAbstract%3A%20%20%20As%20the%20field%20of%20representation%20learning%20grows%2C%20there%20has%20been%20a%20proliferation%0Aof%20different%20loss%20functions%20to%20solve%20different%20classes%20of%20problems.%20We%0Aintroduce%20a%20single%20information-theoretic%20equation%20that%20generalizes%20a%20large%0Acollection%20of%20modern%20loss%20functions%20in%20machine%20learning.%20In%20particular%2C%20we%0Aintroduce%20a%20framework%20that%20shows%20that%20several%20broad%20classes%20of%20machine%20learning%0Amethods%20are%20precisely%20minimizing%20an%20integrated%20KL%20divergence%20between%20two%0Aconditional%20distributions%3A%20the%20supervisory%20and%20learned%20representations.%20This%0Aviewpoint%20exposes%20a%20hidden%20information%20geometry%20underlying%20clustering%2C%20spectral%0Amethods%2C%20dimensionality%20reduction%2C%20contrastive%20learning%2C%20and%20supervised%0Alearning.%20This%20framework%20enables%20the%20development%20of%20new%20loss%20functions%20by%0Acombining%20successful%20techniques%20from%20across%20the%20literature.%20We%20not%20only%20present%0Aa%20wide%20array%20of%20proofs%2C%20connecting%20over%2023%20different%20approaches%2C%20but%20we%20also%0Aleverage%20these%20theoretical%20results%20to%20create%20state-of-the-art%20unsupervised%0Aimage%20classifiers%20that%20achieve%20a%20%2B8%25%20improvement%20over%20the%20prior%0Astate-of-the-art%20on%20unsupervised%20classification%20on%20ImageNet-1K.%20We%20also%0Ademonstrate%20that%20I-Con%20can%20be%20used%20to%20derive%20principled%20debiasing%20methods%20which%0Aimprove%20contrastive%20representation%20learners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI-Con%253A%2520A%2520Unifying%2520Framework%2520for%2520Representation%2520Learning%26entry.906535625%3DShaden%2520Alshammari%2520and%2520John%2520Hershey%2520and%2520Axel%2520Feldmann%2520and%2520William%2520T.%2520Freeman%2520and%2520Mark%2520Hamilton%26entry.1292438233%3D%2520%2520As%2520the%2520field%2520of%2520representation%2520learning%2520grows%252C%2520there%2520has%2520been%2520a%2520proliferation%250Aof%2520different%2520loss%2520functions%2520to%2520solve%2520different%2520classes%2520of%2520problems.%2520We%250Aintroduce%2520a%2520single%2520information-theoretic%2520equation%2520that%2520generalizes%2520a%2520large%250Acollection%2520of%2520modern%2520loss%2520functions%2520in%2520machine%2520learning.%2520In%2520particular%252C%2520we%250Aintroduce%2520a%2520framework%2520that%2520shows%2520that%2520several%2520broad%2520classes%2520of%2520machine%2520learning%250Amethods%2520are%2520precisely%2520minimizing%2520an%2520integrated%2520KL%2520divergence%2520between%2520two%250Aconditional%2520distributions%253A%2520the%2520supervisory%2520and%2520learned%2520representations.%2520This%250Aviewpoint%2520exposes%2520a%2520hidden%2520information%2520geometry%2520underlying%2520clustering%252C%2520spectral%250Amethods%252C%2520dimensionality%2520reduction%252C%2520contrastive%2520learning%252C%2520and%2520supervised%250Alearning.%2520This%2520framework%2520enables%2520the%2520development%2520of%2520new%2520loss%2520functions%2520by%250Acombining%2520successful%2520techniques%2520from%2520across%2520the%2520literature.%2520We%2520not%2520only%2520present%250Aa%2520wide%2520array%2520of%2520proofs%252C%2520connecting%2520over%252023%2520different%2520approaches%252C%2520but%2520we%2520also%250Aleverage%2520these%2520theoretical%2520results%2520to%2520create%2520state-of-the-art%2520unsupervised%250Aimage%2520classifiers%2520that%2520achieve%2520a%2520%252B8%2525%2520improvement%2520over%2520the%2520prior%250Astate-of-the-art%2520on%2520unsupervised%2520classification%2520on%2520ImageNet-1K.%2520We%2520also%250Ademonstrate%2520that%2520I-Con%2520can%2520be%2520used%2520to%2520derive%2520principled%2520debiasing%2520methods%2520which%250Aimprove%2520contrastive%2520representation%2520learners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I-Con%3A%20A%20Unifying%20Framework%20for%20Representation%20Learning&entry.906535625=Shaden%20Alshammari%20and%20John%20Hershey%20and%20Axel%20Feldmann%20and%20William%20T.%20Freeman%20and%20Mark%20Hamilton&entry.1292438233=%20%20As%20the%20field%20of%20representation%20learning%20grows%2C%20there%20has%20been%20a%20proliferation%0Aof%20different%20loss%20functions%20to%20solve%20different%20classes%20of%20problems.%20We%0Aintroduce%20a%20single%20information-theoretic%20equation%20that%20generalizes%20a%20large%0Acollection%20of%20modern%20loss%20functions%20in%20machine%20learning.%20In%20particular%2C%20we%0Aintroduce%20a%20framework%20that%20shows%20that%20several%20broad%20classes%20of%20machine%20learning%0Amethods%20are%20precisely%20minimizing%20an%20integrated%20KL%20divergence%20between%20two%0Aconditional%20distributions%3A%20the%20supervisory%20and%20learned%20representations.%20This%0Aviewpoint%20exposes%20a%20hidden%20information%20geometry%20underlying%20clustering%2C%20spectral%0Amethods%2C%20dimensionality%20reduction%2C%20contrastive%20learning%2C%20and%20supervised%0Alearning.%20This%20framework%20enables%20the%20development%20of%20new%20loss%20functions%20by%0Acombining%20successful%20techniques%20from%20across%20the%20literature.%20We%20not%20only%20present%0Aa%20wide%20array%20of%20proofs%2C%20connecting%20over%2023%20different%20approaches%2C%20but%20we%20also%0Aleverage%20these%20theoretical%20results%20to%20create%20state-of-the-art%20unsupervised%0Aimage%20classifiers%20that%20achieve%20a%20%2B8%25%20improvement%20over%20the%20prior%0Astate-of-the-art%20on%20unsupervised%20classification%20on%20ImageNet-1K.%20We%20also%0Ademonstrate%20that%20I-Con%20can%20be%20used%20to%20derive%20principled%20debiasing%20methods%20which%0Aimprove%20contrastive%20representation%20learners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16929v1&entry.124074799=Read"},
{"title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning", "author": "Di Zhang and Junxian Li and Jingdi Lei and Xunzhi Wang and Yujie Liu and Zonglin Yang and Jiatong Li and Weida Wang and Suorong Yang and Jianbo Wu and Peng Ye and Wanli Ouyang and Dongzhan Zhou", "abstract": "  Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.\n", "link": "http://arxiv.org/abs/2411.18203v5", "date": "2025-04-23", "relevancy": 2.7962, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Critic-V%3A%20VLM%20Critics%20Help%20Catch%20VLM%20Errors%20in%20Multimodal%20Reasoning&body=Title%3A%20Critic-V%3A%20VLM%20Critics%20Help%20Catch%20VLM%20Errors%20in%20Multimodal%20Reasoning%0AAuthor%3A%20Di%20Zhang%20and%20Junxian%20Li%20and%20Jingdi%20Lei%20and%20Xunzhi%20Wang%20and%20Yujie%20Liu%20and%20Zonglin%20Yang%20and%20Jiatong%20Li%20and%20Weida%20Wang%20and%20Suorong%20Yang%20and%20Jianbo%20Wu%20and%20Peng%20Ye%20and%20Wanli%20Ouyang%20and%20Dongzhan%20Zhou%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20remarkable%20advancements%20in%0Amultimodal%20reasoning%20tasks.%20However%2C%20they%20still%20often%20generate%20inaccurate%20or%0Airrelevant%20responses%20due%20to%20issues%20like%20hallucinated%20image%20understandings%20or%0Aunrefined%20reasoning%20paths.%20To%20address%20these%20challenges%2C%20we%20introduce%20Critic-V%2C%0Aa%20novel%20framework%20inspired%20by%20the%20Actor-Critic%20paradigm%20to%20boost%20the%20reasoning%0Acapability%20of%20VLMs.%20This%20framework%20decouples%20the%20reasoning%20process%20and%20critic%0Aprocess%20by%20integrating%20two%20independent%20components%3A%20the%20Reasoner%2C%20which%0Agenerates%20reasoning%20paths%20based%20on%20visual%20and%20textual%20inputs%2C%20and%20the%20Critic%2C%0Awhich%20provides%20constructive%20critique%20to%20refine%20these%20paths.%20In%20this%20approach%2C%0Athe%20Reasoner%20generates%20reasoning%20responses%20according%20to%20text%20prompts%2C%20which%20can%0Aevolve%20iteratively%20as%20a%20policy%20based%20on%20feedback%20from%20the%20Critic.%20This%0Ainteraction%20process%20was%20theoretically%20driven%20by%20a%20reinforcement%20learning%0Aframework%20where%20the%20Critic%20offers%20natural%20language%20critiques%20instead%20of%20scalar%0Arewards%2C%20enabling%20more%20nuanced%20feedback%20to%20boost%20the%20Reasoner%27s%20capability%20on%0Acomplex%20reasoning%20tasks.%20The%20Critic%20model%20is%20trained%20using%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20leveraging%20a%20preference%20dataset%20of%20critiques%20ranked%20by%0ARule-based%20Reward~%28RBR%29%20to%20enhance%20its%20critic%20capabilities.%20Evaluation%20results%0Ashow%20that%20the%20Critic-V%20framework%20significantly%20outperforms%20existing%20methods%2C%0Aincluding%20GPT-4V%2C%20on%205%20out%20of%208%20benchmarks%2C%20especially%20regarding%20reasoning%0Aaccuracy%20and%20efficiency.%20Combining%20a%20dynamic%20text-based%20policy%20for%20the%20Reasoner%0Aand%20constructive%20feedback%20from%20the%20preference-optimized%20Critic%20enables%20a%20more%0Areliable%20and%20context-sensitive%20multimodal%20reasoning%20process.%20Our%20approach%0Aprovides%20a%20promising%20solution%20to%20enhance%20the%20reliability%20of%20VLMs%2C%20improving%0Atheir%20performance%20in%20real-world%20reasoning-heavy%20multimodal%20applications%20such%20as%0Aautonomous%20driving%20and%20embodied%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18203v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCritic-V%253A%2520VLM%2520Critics%2520Help%2520Catch%2520VLM%2520Errors%2520in%2520Multimodal%2520Reasoning%26entry.906535625%3DDi%2520Zhang%2520and%2520Junxian%2520Li%2520and%2520Jingdi%2520Lei%2520and%2520Xunzhi%2520Wang%2520and%2520Yujie%2520Liu%2520and%2520Zonglin%2520Yang%2520and%2520Jiatong%2520Li%2520and%2520Weida%2520Wang%2520and%2520Suorong%2520Yang%2520and%2520Jianbo%2520Wu%2520and%2520Peng%2520Ye%2520and%2520Wanli%2520Ouyang%2520and%2520Dongzhan%2520Zhou%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520remarkable%2520advancements%2520in%250Amultimodal%2520reasoning%2520tasks.%2520However%252C%2520they%2520still%2520often%2520generate%2520inaccurate%2520or%250Airrelevant%2520responses%2520due%2520to%2520issues%2520like%2520hallucinated%2520image%2520understandings%2520or%250Aunrefined%2520reasoning%2520paths.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Critic-V%252C%250Aa%2520novel%2520framework%2520inspired%2520by%2520the%2520Actor-Critic%2520paradigm%2520to%2520boost%2520the%2520reasoning%250Acapability%2520of%2520VLMs.%2520This%2520framework%2520decouples%2520the%2520reasoning%2520process%2520and%2520critic%250Aprocess%2520by%2520integrating%2520two%2520independent%2520components%253A%2520the%2520Reasoner%252C%2520which%250Agenerates%2520reasoning%2520paths%2520based%2520on%2520visual%2520and%2520textual%2520inputs%252C%2520and%2520the%2520Critic%252C%250Awhich%2520provides%2520constructive%2520critique%2520to%2520refine%2520these%2520paths.%2520In%2520this%2520approach%252C%250Athe%2520Reasoner%2520generates%2520reasoning%2520responses%2520according%2520to%2520text%2520prompts%252C%2520which%2520can%250Aevolve%2520iteratively%2520as%2520a%2520policy%2520based%2520on%2520feedback%2520from%2520the%2520Critic.%2520This%250Ainteraction%2520process%2520was%2520theoretically%2520driven%2520by%2520a%2520reinforcement%2520learning%250Aframework%2520where%2520the%2520Critic%2520offers%2520natural%2520language%2520critiques%2520instead%2520of%2520scalar%250Arewards%252C%2520enabling%2520more%2520nuanced%2520feedback%2520to%2520boost%2520the%2520Reasoner%2527s%2520capability%2520on%250Acomplex%2520reasoning%2520tasks.%2520The%2520Critic%2520model%2520is%2520trained%2520using%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%252C%2520leveraging%2520a%2520preference%2520dataset%2520of%2520critiques%2520ranked%2520by%250ARule-based%2520Reward~%2528RBR%2529%2520to%2520enhance%2520its%2520critic%2520capabilities.%2520Evaluation%2520results%250Ashow%2520that%2520the%2520Critic-V%2520framework%2520significantly%2520outperforms%2520existing%2520methods%252C%250Aincluding%2520GPT-4V%252C%2520on%25205%2520out%2520of%25208%2520benchmarks%252C%2520especially%2520regarding%2520reasoning%250Aaccuracy%2520and%2520efficiency.%2520Combining%2520a%2520dynamic%2520text-based%2520policy%2520for%2520the%2520Reasoner%250Aand%2520constructive%2520feedback%2520from%2520the%2520preference-optimized%2520Critic%2520enables%2520a%2520more%250Areliable%2520and%2520context-sensitive%2520multimodal%2520reasoning%2520process.%2520Our%2520approach%250Aprovides%2520a%2520promising%2520solution%2520to%2520enhance%2520the%2520reliability%2520of%2520VLMs%252C%2520improving%250Atheir%2520performance%2520in%2520real-world%2520reasoning-heavy%2520multimodal%2520applications%2520such%2520as%250Aautonomous%2520driving%2520and%2520embodied%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18203v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Critic-V%3A%20VLM%20Critics%20Help%20Catch%20VLM%20Errors%20in%20Multimodal%20Reasoning&entry.906535625=Di%20Zhang%20and%20Junxian%20Li%20and%20Jingdi%20Lei%20and%20Xunzhi%20Wang%20and%20Yujie%20Liu%20and%20Zonglin%20Yang%20and%20Jiatong%20Li%20and%20Weida%20Wang%20and%20Suorong%20Yang%20and%20Jianbo%20Wu%20and%20Peng%20Ye%20and%20Wanli%20Ouyang%20and%20Dongzhan%20Zhou&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20remarkable%20advancements%20in%0Amultimodal%20reasoning%20tasks.%20However%2C%20they%20still%20often%20generate%20inaccurate%20or%0Airrelevant%20responses%20due%20to%20issues%20like%20hallucinated%20image%20understandings%20or%0Aunrefined%20reasoning%20paths.%20To%20address%20these%20challenges%2C%20we%20introduce%20Critic-V%2C%0Aa%20novel%20framework%20inspired%20by%20the%20Actor-Critic%20paradigm%20to%20boost%20the%20reasoning%0Acapability%20of%20VLMs.%20This%20framework%20decouples%20the%20reasoning%20process%20and%20critic%0Aprocess%20by%20integrating%20two%20independent%20components%3A%20the%20Reasoner%2C%20which%0Agenerates%20reasoning%20paths%20based%20on%20visual%20and%20textual%20inputs%2C%20and%20the%20Critic%2C%0Awhich%20provides%20constructive%20critique%20to%20refine%20these%20paths.%20In%20this%20approach%2C%0Athe%20Reasoner%20generates%20reasoning%20responses%20according%20to%20text%20prompts%2C%20which%20can%0Aevolve%20iteratively%20as%20a%20policy%20based%20on%20feedback%20from%20the%20Critic.%20This%0Ainteraction%20process%20was%20theoretically%20driven%20by%20a%20reinforcement%20learning%0Aframework%20where%20the%20Critic%20offers%20natural%20language%20critiques%20instead%20of%20scalar%0Arewards%2C%20enabling%20more%20nuanced%20feedback%20to%20boost%20the%20Reasoner%27s%20capability%20on%0Acomplex%20reasoning%20tasks.%20The%20Critic%20model%20is%20trained%20using%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20leveraging%20a%20preference%20dataset%20of%20critiques%20ranked%20by%0ARule-based%20Reward~%28RBR%29%20to%20enhance%20its%20critic%20capabilities.%20Evaluation%20results%0Ashow%20that%20the%20Critic-V%20framework%20significantly%20outperforms%20existing%20methods%2C%0Aincluding%20GPT-4V%2C%20on%205%20out%20of%208%20benchmarks%2C%20especially%20regarding%20reasoning%0Aaccuracy%20and%20efficiency.%20Combining%20a%20dynamic%20text-based%20policy%20for%20the%20Reasoner%0Aand%20constructive%20feedback%20from%20the%20preference-optimized%20Critic%20enables%20a%20more%0Areliable%20and%20context-sensitive%20multimodal%20reasoning%20process.%20Our%20approach%0Aprovides%20a%20promising%20solution%20to%20enhance%20the%20reliability%20of%20VLMs%2C%20improving%0Atheir%20performance%20in%20real-world%20reasoning-heavy%20multimodal%20applications%20such%20as%0Aautonomous%20driving%20and%20embodied%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18203v5&entry.124074799=Read"},
{"title": "Solving Inverse Problems in Protein Space Using Diffusion-Based Priors", "author": "Axel Levy and Eric R. Chan and Sara Fridovich-Keil and Fr\u00e9d\u00e9ric Poitevin and Ellen D. Zhong and Gordon Wetzstein", "abstract": "  The interaction of a protein with its environment can be understood and\ncontrolled via its 3D structure. Experimental methods for protein structure\ndetermination, such as X-ray crystallography or cryogenic electron microscopy,\nshed light on biological processes but introduce challenging inverse problems.\nLearning-based approaches have emerged as accurate and efficient methods to\nsolve these inverse problems for 3D structure determination, but are\nspecialized for a predefined type of measurement. Here, we introduce a\nversatile framework to turn biophysical measurements, such as cryo-EM density\nmaps, into 3D atomic models. Our method combines a physics-based forward model\nof the measurement process with a pretrained generative model providing a\ntask-agnostic, data-driven prior. Our method outperforms posterior sampling\nbaselines on linear and non-linear inverse problems. In particular, it is the\nfirst diffusion-based method for refining atomic models from cryo-EM maps and\nbuilding atomic models from sparse distance matrices.\n", "link": "http://arxiv.org/abs/2406.04239v2", "date": "2025-04-23", "relevancy": 2.7453, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5795}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5338}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Inverse%20Problems%20in%20Protein%20Space%20Using%20Diffusion-Based%20Priors&body=Title%3A%20Solving%20Inverse%20Problems%20in%20Protein%20Space%20Using%20Diffusion-Based%20Priors%0AAuthor%3A%20Axel%20Levy%20and%20Eric%20R.%20Chan%20and%20Sara%20Fridovich-Keil%20and%20Fr%C3%A9d%C3%A9ric%20Poitevin%20and%20Ellen%20D.%20Zhong%20and%20Gordon%20Wetzstein%0AAbstract%3A%20%20%20The%20interaction%20of%20a%20protein%20with%20its%20environment%20can%20be%20understood%20and%0Acontrolled%20via%20its%203D%20structure.%20Experimental%20methods%20for%20protein%20structure%0Adetermination%2C%20such%20as%20X-ray%20crystallography%20or%20cryogenic%20electron%20microscopy%2C%0Ashed%20light%20on%20biological%20processes%20but%20introduce%20challenging%20inverse%20problems.%0ALearning-based%20approaches%20have%20emerged%20as%20accurate%20and%20efficient%20methods%20to%0Asolve%20these%20inverse%20problems%20for%203D%20structure%20determination%2C%20but%20are%0Aspecialized%20for%20a%20predefined%20type%20of%20measurement.%20Here%2C%20we%20introduce%20a%0Aversatile%20framework%20to%20turn%20biophysical%20measurements%2C%20such%20as%20cryo-EM%20density%0Amaps%2C%20into%203D%20atomic%20models.%20Our%20method%20combines%20a%20physics-based%20forward%20model%0Aof%20the%20measurement%20process%20with%20a%20pretrained%20generative%20model%20providing%20a%0Atask-agnostic%2C%20data-driven%20prior.%20Our%20method%20outperforms%20posterior%20sampling%0Abaselines%20on%20linear%20and%20non-linear%20inverse%20problems.%20In%20particular%2C%20it%20is%20the%0Afirst%20diffusion-based%20method%20for%20refining%20atomic%20models%20from%20cryo-EM%20maps%20and%0Abuilding%20atomic%20models%20from%20sparse%20distance%20matrices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Inverse%2520Problems%2520in%2520Protein%2520Space%2520Using%2520Diffusion-Based%2520Priors%26entry.906535625%3DAxel%2520Levy%2520and%2520Eric%2520R.%2520Chan%2520and%2520Sara%2520Fridovich-Keil%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Poitevin%2520and%2520Ellen%2520D.%2520Zhong%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3D%2520%2520The%2520interaction%2520of%2520a%2520protein%2520with%2520its%2520environment%2520can%2520be%2520understood%2520and%250Acontrolled%2520via%2520its%25203D%2520structure.%2520Experimental%2520methods%2520for%2520protein%2520structure%250Adetermination%252C%2520such%2520as%2520X-ray%2520crystallography%2520or%2520cryogenic%2520electron%2520microscopy%252C%250Ashed%2520light%2520on%2520biological%2520processes%2520but%2520introduce%2520challenging%2520inverse%2520problems.%250ALearning-based%2520approaches%2520have%2520emerged%2520as%2520accurate%2520and%2520efficient%2520methods%2520to%250Asolve%2520these%2520inverse%2520problems%2520for%25203D%2520structure%2520determination%252C%2520but%2520are%250Aspecialized%2520for%2520a%2520predefined%2520type%2520of%2520measurement.%2520Here%252C%2520we%2520introduce%2520a%250Aversatile%2520framework%2520to%2520turn%2520biophysical%2520measurements%252C%2520such%2520as%2520cryo-EM%2520density%250Amaps%252C%2520into%25203D%2520atomic%2520models.%2520Our%2520method%2520combines%2520a%2520physics-based%2520forward%2520model%250Aof%2520the%2520measurement%2520process%2520with%2520a%2520pretrained%2520generative%2520model%2520providing%2520a%250Atask-agnostic%252C%2520data-driven%2520prior.%2520Our%2520method%2520outperforms%2520posterior%2520sampling%250Abaselines%2520on%2520linear%2520and%2520non-linear%2520inverse%2520problems.%2520In%2520particular%252C%2520it%2520is%2520the%250Afirst%2520diffusion-based%2520method%2520for%2520refining%2520atomic%2520models%2520from%2520cryo-EM%2520maps%2520and%250Abuilding%2520atomic%2520models%2520from%2520sparse%2520distance%2520matrices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Inverse%20Problems%20in%20Protein%20Space%20Using%20Diffusion-Based%20Priors&entry.906535625=Axel%20Levy%20and%20Eric%20R.%20Chan%20and%20Sara%20Fridovich-Keil%20and%20Fr%C3%A9d%C3%A9ric%20Poitevin%20and%20Ellen%20D.%20Zhong%20and%20Gordon%20Wetzstein&entry.1292438233=%20%20The%20interaction%20of%20a%20protein%20with%20its%20environment%20can%20be%20understood%20and%0Acontrolled%20via%20its%203D%20structure.%20Experimental%20methods%20for%20protein%20structure%0Adetermination%2C%20such%20as%20X-ray%20crystallography%20or%20cryogenic%20electron%20microscopy%2C%0Ashed%20light%20on%20biological%20processes%20but%20introduce%20challenging%20inverse%20problems.%0ALearning-based%20approaches%20have%20emerged%20as%20accurate%20and%20efficient%20methods%20to%0Asolve%20these%20inverse%20problems%20for%203D%20structure%20determination%2C%20but%20are%0Aspecialized%20for%20a%20predefined%20type%20of%20measurement.%20Here%2C%20we%20introduce%20a%0Aversatile%20framework%20to%20turn%20biophysical%20measurements%2C%20such%20as%20cryo-EM%20density%0Amaps%2C%20into%203D%20atomic%20models.%20Our%20method%20combines%20a%20physics-based%20forward%20model%0Aof%20the%20measurement%20process%20with%20a%20pretrained%20generative%20model%20providing%20a%0Atask-agnostic%2C%20data-driven%20prior.%20Our%20method%20outperforms%20posterior%20sampling%0Abaselines%20on%20linear%20and%20non-linear%20inverse%20problems.%20In%20particular%2C%20it%20is%20the%0Afirst%20diffusion-based%20method%20for%20refining%20atomic%20models%20from%20cryo-EM%20maps%20and%0Abuilding%20atomic%20models%20from%20sparse%20distance%20matrices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04239v2&entry.124074799=Read"},
{"title": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge", "author": "Mirian Hipolito Garcia and Camille Couturier and Daniel Madrigal Diaz and Ankur Mallick and Anastasios Kyrillidis and Robert Sim and Victor Ruhle and Saravan Rajmohan", "abstract": "  We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks\n", "link": "http://arxiv.org/abs/2504.16871v1", "date": "2025-04-23", "relevancy": 2.5781, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20How%20LLMs%20Capture%20and%20Represent%20Domain-Specific%20Knowledge&body=Title%3A%20Exploring%20How%20LLMs%20Capture%20and%20Represent%20Domain-Specific%20Knowledge%0AAuthor%3A%20Mirian%20Hipolito%20Garcia%20and%20Camille%20Couturier%20and%20Daniel%20Madrigal%20Diaz%20and%20Ankur%20Mallick%20and%20Anastasios%20Kyrillidis%20and%20Robert%20Sim%20and%20Victor%20Ruhle%20and%20Saravan%20Rajmohan%0AAbstract%3A%20%20%20We%20study%20whether%20Large%20Language%20Models%20%28LLMs%29%20inherently%20capture%0Adomain-specific%20nuances%20in%20natural%20language.%20Our%20experiments%20probe%20the%20domain%0Asensitivity%20of%20LLMs%20by%20examining%20their%20ability%20to%20distinguish%20queries%20from%0Adifferent%20domains%20using%20hidden%20states%20generated%20during%20the%20prefill%20phase.%20We%0Areveal%20latent%20domain-related%20trajectories%20that%20indicate%20the%20model%27s%20internal%0Arecognition%20of%20query%20domains.%20We%20also%20study%20the%20robustness%20of%20these%20domain%0Arepresentations%20to%20variations%20in%20prompt%20styles%20and%20sources.%20Our%20approach%0Aleverages%20these%20representations%20for%20model%20selection%2C%20mapping%20the%20LLM%20that%20best%0Amatches%20the%20domain%20trace%20of%20the%20input%20query%20%28i.e.%2C%20the%20model%20with%20the%20highest%0Aperformance%20on%20similar%20traces%29.%20Our%20findings%20show%20that%20LLMs%20can%20differentiate%0Aqueries%20for%20related%20domains%2C%20and%20that%20the%20fine-tuned%20model%20is%20not%20always%20the%0Amost%20accurate.%20Unlike%20previous%20work%2C%20our%20interpretations%20apply%20to%20both%20closed%0Aand%20open-ended%20generative%20tasks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520How%2520LLMs%2520Capture%2520and%2520Represent%2520Domain-Specific%2520Knowledge%26entry.906535625%3DMirian%2520Hipolito%2520Garcia%2520and%2520Camille%2520Couturier%2520and%2520Daniel%2520Madrigal%2520Diaz%2520and%2520Ankur%2520Mallick%2520and%2520Anastasios%2520Kyrillidis%2520and%2520Robert%2520Sim%2520and%2520Victor%2520Ruhle%2520and%2520Saravan%2520Rajmohan%26entry.1292438233%3D%2520%2520We%2520study%2520whether%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520inherently%2520capture%250Adomain-specific%2520nuances%2520in%2520natural%2520language.%2520Our%2520experiments%2520probe%2520the%2520domain%250Asensitivity%2520of%2520LLMs%2520by%2520examining%2520their%2520ability%2520to%2520distinguish%2520queries%2520from%250Adifferent%2520domains%2520using%2520hidden%2520states%2520generated%2520during%2520the%2520prefill%2520phase.%2520We%250Areveal%2520latent%2520domain-related%2520trajectories%2520that%2520indicate%2520the%2520model%2527s%2520internal%250Arecognition%2520of%2520query%2520domains.%2520We%2520also%2520study%2520the%2520robustness%2520of%2520these%2520domain%250Arepresentations%2520to%2520variations%2520in%2520prompt%2520styles%2520and%2520sources.%2520Our%2520approach%250Aleverages%2520these%2520representations%2520for%2520model%2520selection%252C%2520mapping%2520the%2520LLM%2520that%2520best%250Amatches%2520the%2520domain%2520trace%2520of%2520the%2520input%2520query%2520%2528i.e.%252C%2520the%2520model%2520with%2520the%2520highest%250Aperformance%2520on%2520similar%2520traces%2529.%2520Our%2520findings%2520show%2520that%2520LLMs%2520can%2520differentiate%250Aqueries%2520for%2520related%2520domains%252C%2520and%2520that%2520the%2520fine-tuned%2520model%2520is%2520not%2520always%2520the%250Amost%2520accurate.%2520Unlike%2520previous%2520work%252C%2520our%2520interpretations%2520apply%2520to%2520both%2520closed%250Aand%2520open-ended%2520generative%2520tasks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20How%20LLMs%20Capture%20and%20Represent%20Domain-Specific%20Knowledge&entry.906535625=Mirian%20Hipolito%20Garcia%20and%20Camille%20Couturier%20and%20Daniel%20Madrigal%20Diaz%20and%20Ankur%20Mallick%20and%20Anastasios%20Kyrillidis%20and%20Robert%20Sim%20and%20Victor%20Ruhle%20and%20Saravan%20Rajmohan&entry.1292438233=%20%20We%20study%20whether%20Large%20Language%20Models%20%28LLMs%29%20inherently%20capture%0Adomain-specific%20nuances%20in%20natural%20language.%20Our%20experiments%20probe%20the%20domain%0Asensitivity%20of%20LLMs%20by%20examining%20their%20ability%20to%20distinguish%20queries%20from%0Adifferent%20domains%20using%20hidden%20states%20generated%20during%20the%20prefill%20phase.%20We%0Areveal%20latent%20domain-related%20trajectories%20that%20indicate%20the%20model%27s%20internal%0Arecognition%20of%20query%20domains.%20We%20also%20study%20the%20robustness%20of%20these%20domain%0Arepresentations%20to%20variations%20in%20prompt%20styles%20and%20sources.%20Our%20approach%0Aleverages%20these%20representations%20for%20model%20selection%2C%20mapping%20the%20LLM%20that%20best%0Amatches%20the%20domain%20trace%20of%20the%20input%20query%20%28i.e.%2C%20the%20model%20with%20the%20highest%0Aperformance%20on%20similar%20traces%29.%20Our%20findings%20show%20that%20LLMs%20can%20differentiate%0Aqueries%20for%20related%20domains%2C%20and%20that%20the%20fine-tuned%20model%20is%20not%20always%20the%0Amost%20accurate.%20Unlike%20previous%20work%2C%20our%20interpretations%20apply%20to%20both%20closed%0Aand%20open-ended%20generative%20tasks%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16871v1&entry.124074799=Read"},
{"title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for\n  Long-Context AI Conversations", "author": "Kwangseob Ahn", "abstract": "  Large language models (LLMs) struggle with maintaining coherence in extended\nconversations spanning hundreds of turns, despite performing well within their\ncontext windows. This paper introduces HEMA (Hippocampus-Inspired Extended\nMemory Architecture), a dual-memory system inspired by human cognitive\nprocesses. HEMA combines Compact Memory - a continuously updated one-sentence\nsummary preserving global narrative coherence, and Vector Memory - an episodic\nstore of chunk embeddings queried via cosine similarity. When integrated with a\n6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns\nwhile keeping prompt length under 3,500 tokens. Experimental results show\nsubstantial improvements: factual recall accuracy increases from 41% to 87%,\nand human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K\nindexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling\nthe area under the precision-recall curve compared to summarization-only\napproaches. Ablation studies reveal two key insights: semantic forgetting\nthrough age-weighted pruning reduces retrieval latency by 34% with minimal\nrecall loss, and a two-level summary hierarchy prevents cascade errors in\nultra-long conversations exceeding 1,000 turns. HEMA demonstrates that\ncombining verbatim recall with semantic continuity provides a practical\nsolution for privacy-aware conversational AI capable of month-long dialogues\nwithout model retraining.\n", "link": "http://arxiv.org/abs/2504.16754v1", "date": "2025-04-23", "relevancy": 2.5035, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4984}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HEMA%20%3A%20A%20Hippocampus-Inspired%20Extended%20Memory%20Architecture%20for%0A%20%20Long-Context%20AI%20Conversations&body=Title%3A%20HEMA%20%3A%20A%20Hippocampus-Inspired%20Extended%20Memory%20Architecture%20for%0A%20%20Long-Context%20AI%20Conversations%0AAuthor%3A%20Kwangseob%20Ahn%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20struggle%20with%20maintaining%20coherence%20in%20extended%0Aconversations%20spanning%20hundreds%20of%20turns%2C%20despite%20performing%20well%20within%20their%0Acontext%20windows.%20This%20paper%20introduces%20HEMA%20%28Hippocampus-Inspired%20Extended%0AMemory%20Architecture%29%2C%20a%20dual-memory%20system%20inspired%20by%20human%20cognitive%0Aprocesses.%20HEMA%20combines%20Compact%20Memory%20-%20a%20continuously%20updated%20one-sentence%0Asummary%20preserving%20global%20narrative%20coherence%2C%20and%20Vector%20Memory%20-%20an%20episodic%0Astore%20of%20chunk%20embeddings%20queried%20via%20cosine%20similarity.%20When%20integrated%20with%20a%0A6B-parameter%20transformer%2C%20HEMA%20maintains%20coherent%20dialogues%20beyond%20300%20turns%0Awhile%20keeping%20prompt%20length%20under%203%2C500%20tokens.%20Experimental%20results%20show%0Asubstantial%20improvements%3A%20factual%20recall%20accuracy%20increases%20from%2041%25%20to%2087%25%2C%0Aand%20human-rated%20coherence%20improves%20from%202.7%20to%204.3%20on%20a%205-point%20scale.%20With%2010K%0Aindexed%20chunks%2C%20Vector%20Memory%20achieves%20P%405%20%3E%3D%200.80%20and%20R%4050%20%3E%3D%200.74%2C%20doubling%0Athe%20area%20under%20the%20precision-recall%20curve%20compared%20to%20summarization-only%0Aapproaches.%20Ablation%20studies%20reveal%20two%20key%20insights%3A%20semantic%20forgetting%0Athrough%20age-weighted%20pruning%20reduces%20retrieval%20latency%20by%2034%25%20with%20minimal%0Arecall%20loss%2C%20and%20a%20two-level%20summary%20hierarchy%20prevents%20cascade%20errors%20in%0Aultra-long%20conversations%20exceeding%201%2C000%20turns.%20HEMA%20demonstrates%20that%0Acombining%20verbatim%20recall%20with%20semantic%20continuity%20provides%20a%20practical%0Asolution%20for%20privacy-aware%20conversational%20AI%20capable%20of%20month-long%20dialogues%0Awithout%20model%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHEMA%2520%253A%2520A%2520Hippocampus-Inspired%2520Extended%2520Memory%2520Architecture%2520for%250A%2520%2520Long-Context%2520AI%2520Conversations%26entry.906535625%3DKwangseob%2520Ahn%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520struggle%2520with%2520maintaining%2520coherence%2520in%2520extended%250Aconversations%2520spanning%2520hundreds%2520of%2520turns%252C%2520despite%2520performing%2520well%2520within%2520their%250Acontext%2520windows.%2520This%2520paper%2520introduces%2520HEMA%2520%2528Hippocampus-Inspired%2520Extended%250AMemory%2520Architecture%2529%252C%2520a%2520dual-memory%2520system%2520inspired%2520by%2520human%2520cognitive%250Aprocesses.%2520HEMA%2520combines%2520Compact%2520Memory%2520-%2520a%2520continuously%2520updated%2520one-sentence%250Asummary%2520preserving%2520global%2520narrative%2520coherence%252C%2520and%2520Vector%2520Memory%2520-%2520an%2520episodic%250Astore%2520of%2520chunk%2520embeddings%2520queried%2520via%2520cosine%2520similarity.%2520When%2520integrated%2520with%2520a%250A6B-parameter%2520transformer%252C%2520HEMA%2520maintains%2520coherent%2520dialogues%2520beyond%2520300%2520turns%250Awhile%2520keeping%2520prompt%2520length%2520under%25203%252C500%2520tokens.%2520Experimental%2520results%2520show%250Asubstantial%2520improvements%253A%2520factual%2520recall%2520accuracy%2520increases%2520from%252041%2525%2520to%252087%2525%252C%250Aand%2520human-rated%2520coherence%2520improves%2520from%25202.7%2520to%25204.3%2520on%2520a%25205-point%2520scale.%2520With%252010K%250Aindexed%2520chunks%252C%2520Vector%2520Memory%2520achieves%2520P%25405%2520%253E%253D%25200.80%2520and%2520R%254050%2520%253E%253D%25200.74%252C%2520doubling%250Athe%2520area%2520under%2520the%2520precision-recall%2520curve%2520compared%2520to%2520summarization-only%250Aapproaches.%2520Ablation%2520studies%2520reveal%2520two%2520key%2520insights%253A%2520semantic%2520forgetting%250Athrough%2520age-weighted%2520pruning%2520reduces%2520retrieval%2520latency%2520by%252034%2525%2520with%2520minimal%250Arecall%2520loss%252C%2520and%2520a%2520two-level%2520summary%2520hierarchy%2520prevents%2520cascade%2520errors%2520in%250Aultra-long%2520conversations%2520exceeding%25201%252C000%2520turns.%2520HEMA%2520demonstrates%2520that%250Acombining%2520verbatim%2520recall%2520with%2520semantic%2520continuity%2520provides%2520a%2520practical%250Asolution%2520for%2520privacy-aware%2520conversational%2520AI%2520capable%2520of%2520month-long%2520dialogues%250Awithout%2520model%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEMA%20%3A%20A%20Hippocampus-Inspired%20Extended%20Memory%20Architecture%20for%0A%20%20Long-Context%20AI%20Conversations&entry.906535625=Kwangseob%20Ahn&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20struggle%20with%20maintaining%20coherence%20in%20extended%0Aconversations%20spanning%20hundreds%20of%20turns%2C%20despite%20performing%20well%20within%20their%0Acontext%20windows.%20This%20paper%20introduces%20HEMA%20%28Hippocampus-Inspired%20Extended%0AMemory%20Architecture%29%2C%20a%20dual-memory%20system%20inspired%20by%20human%20cognitive%0Aprocesses.%20HEMA%20combines%20Compact%20Memory%20-%20a%20continuously%20updated%20one-sentence%0Asummary%20preserving%20global%20narrative%20coherence%2C%20and%20Vector%20Memory%20-%20an%20episodic%0Astore%20of%20chunk%20embeddings%20queried%20via%20cosine%20similarity.%20When%20integrated%20with%20a%0A6B-parameter%20transformer%2C%20HEMA%20maintains%20coherent%20dialogues%20beyond%20300%20turns%0Awhile%20keeping%20prompt%20length%20under%203%2C500%20tokens.%20Experimental%20results%20show%0Asubstantial%20improvements%3A%20factual%20recall%20accuracy%20increases%20from%2041%25%20to%2087%25%2C%0Aand%20human-rated%20coherence%20improves%20from%202.7%20to%204.3%20on%20a%205-point%20scale.%20With%2010K%0Aindexed%20chunks%2C%20Vector%20Memory%20achieves%20P%405%20%3E%3D%200.80%20and%20R%4050%20%3E%3D%200.74%2C%20doubling%0Athe%20area%20under%20the%20precision-recall%20curve%20compared%20to%20summarization-only%0Aapproaches.%20Ablation%20studies%20reveal%20two%20key%20insights%3A%20semantic%20forgetting%0Athrough%20age-weighted%20pruning%20reduces%20retrieval%20latency%20by%2034%25%20with%20minimal%0Arecall%20loss%2C%20and%20a%20two-level%20summary%20hierarchy%20prevents%20cascade%20errors%20in%0Aultra-long%20conversations%20exceeding%201%2C000%20turns.%20HEMA%20demonstrates%20that%0Acombining%20verbatim%20recall%20with%20semantic%20continuity%20provides%20a%20practical%0Asolution%20for%20privacy-aware%20conversational%20AI%20capable%20of%20month-long%20dialogues%0Awithout%20model%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16754v1&entry.124074799=Read"},
{"title": "A Low-Cost Photogrammetry System for 3D Plant Modeling and Phenotyping", "author": "Joe Hrzich and Michael A. Beck and Christopher P. Bidinosti and Christopher J. Henry and Kalhari Manawasinghe and Karen Tanino", "abstract": "  We present an open-source, low-cost photogrammetry system for 3D plant\nmodeling and phenotyping. The system uses a structure-from-motion approach to\nreconstruct 3D representations of the plants via point clouds. Using wheat as\nan example, we demonstrate how various phenotypic traits can be computed easily\nfrom the point clouds. These include standard measurements such as plant height\nand radius, as well as features that would be more cumbersome to measure by\nhand, such as leaf angles and convex hull. We further demonstrate the utility\nof the system through the investigation of specific metrics that may yield\nobjective classifications of erectophile versus planophile wheat canopy\narchitectures.\n", "link": "http://arxiv.org/abs/2504.16840v1", "date": "2025-04-23", "relevancy": 2.4961, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5129}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5129}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Low-Cost%20Photogrammetry%20System%20for%203D%20Plant%20Modeling%20and%20Phenotyping&body=Title%3A%20A%20Low-Cost%20Photogrammetry%20System%20for%203D%20Plant%20Modeling%20and%20Phenotyping%0AAuthor%3A%20Joe%20Hrzich%20and%20Michael%20A.%20Beck%20and%20Christopher%20P.%20Bidinosti%20and%20Christopher%20J.%20Henry%20and%20Kalhari%20Manawasinghe%20and%20Karen%20Tanino%0AAbstract%3A%20%20%20We%20present%20an%20open-source%2C%20low-cost%20photogrammetry%20system%20for%203D%20plant%0Amodeling%20and%20phenotyping.%20The%20system%20uses%20a%20structure-from-motion%20approach%20to%0Areconstruct%203D%20representations%20of%20the%20plants%20via%20point%20clouds.%20Using%20wheat%20as%0Aan%20example%2C%20we%20demonstrate%20how%20various%20phenotypic%20traits%20can%20be%20computed%20easily%0Afrom%20the%20point%20clouds.%20These%20include%20standard%20measurements%20such%20as%20plant%20height%0Aand%20radius%2C%20as%20well%20as%20features%20that%20would%20be%20more%20cumbersome%20to%20measure%20by%0Ahand%2C%20such%20as%20leaf%20angles%20and%20convex%20hull.%20We%20further%20demonstrate%20the%20utility%0Aof%20the%20system%20through%20the%20investigation%20of%20specific%20metrics%20that%20may%20yield%0Aobjective%20classifications%20of%20erectophile%20versus%20planophile%20wheat%20canopy%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Low-Cost%2520Photogrammetry%2520System%2520for%25203D%2520Plant%2520Modeling%2520and%2520Phenotyping%26entry.906535625%3DJoe%2520Hrzich%2520and%2520Michael%2520A.%2520Beck%2520and%2520Christopher%2520P.%2520Bidinosti%2520and%2520Christopher%2520J.%2520Henry%2520and%2520Kalhari%2520Manawasinghe%2520and%2520Karen%2520Tanino%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520open-source%252C%2520low-cost%2520photogrammetry%2520system%2520for%25203D%2520plant%250Amodeling%2520and%2520phenotyping.%2520The%2520system%2520uses%2520a%2520structure-from-motion%2520approach%2520to%250Areconstruct%25203D%2520representations%2520of%2520the%2520plants%2520via%2520point%2520clouds.%2520Using%2520wheat%2520as%250Aan%2520example%252C%2520we%2520demonstrate%2520how%2520various%2520phenotypic%2520traits%2520can%2520be%2520computed%2520easily%250Afrom%2520the%2520point%2520clouds.%2520These%2520include%2520standard%2520measurements%2520such%2520as%2520plant%2520height%250Aand%2520radius%252C%2520as%2520well%2520as%2520features%2520that%2520would%2520be%2520more%2520cumbersome%2520to%2520measure%2520by%250Ahand%252C%2520such%2520as%2520leaf%2520angles%2520and%2520convex%2520hull.%2520We%2520further%2520demonstrate%2520the%2520utility%250Aof%2520the%2520system%2520through%2520the%2520investigation%2520of%2520specific%2520metrics%2520that%2520may%2520yield%250Aobjective%2520classifications%2520of%2520erectophile%2520versus%2520planophile%2520wheat%2520canopy%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Low-Cost%20Photogrammetry%20System%20for%203D%20Plant%20Modeling%20and%20Phenotyping&entry.906535625=Joe%20Hrzich%20and%20Michael%20A.%20Beck%20and%20Christopher%20P.%20Bidinosti%20and%20Christopher%20J.%20Henry%20and%20Kalhari%20Manawasinghe%20and%20Karen%20Tanino&entry.1292438233=%20%20We%20present%20an%20open-source%2C%20low-cost%20photogrammetry%20system%20for%203D%20plant%0Amodeling%20and%20phenotyping.%20The%20system%20uses%20a%20structure-from-motion%20approach%20to%0Areconstruct%203D%20representations%20of%20the%20plants%20via%20point%20clouds.%20Using%20wheat%20as%0Aan%20example%2C%20we%20demonstrate%20how%20various%20phenotypic%20traits%20can%20be%20computed%20easily%0Afrom%20the%20point%20clouds.%20These%20include%20standard%20measurements%20such%20as%20plant%20height%0Aand%20radius%2C%20as%20well%20as%20features%20that%20would%20be%20more%20cumbersome%20to%20measure%20by%0Ahand%2C%20such%20as%20leaf%20angles%20and%20convex%20hull.%20We%20further%20demonstrate%20the%20utility%0Aof%20the%20system%20through%20the%20investigation%20of%20specific%20metrics%20that%20may%20yield%0Aobjective%20classifications%20of%20erectophile%20versus%20planophile%20wheat%20canopy%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16840v1&entry.124074799=Read"},
{"title": "Detecting and Understanding Hateful Contents in Memes Through Captioning\n  and Visual Question-Answering", "author": "Ali Anaissi and Junaid Akram and Kunal Chaturvedi and Ali Braytee", "abstract": "  Memes are widely used for humor and cultural commentary, but they are\nincreasingly exploited to spread hateful content. Due to their multimodal\nnature, hateful memes often evade traditional text-only or image-only detection\nsystems, particularly when they employ subtle or coded references. To address\nthese challenges, we propose a multimodal hate detection framework that\nintegrates key components: OCR to extract embedded text, captioning to describe\nvisual content neutrally, sub-label classification for granular categorization\nof hateful content, RAG for contextually relevant retrieval, and VQA for\niterative analysis of symbolic and contextual cues. This enables the framework\nto uncover latent signals that simpler pipelines fail to detect. Experimental\nresults on the Facebook Hateful Memes dataset reveal that the proposed\nframework exceeds the performance of unimodal and conventional multimodal\nmodels in both accuracy and AUC-ROC.\n", "link": "http://arxiv.org/abs/2504.16723v1", "date": "2025-04-23", "relevancy": 2.4563, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Understanding%20Hateful%20Contents%20in%20Memes%20Through%20Captioning%0A%20%20and%20Visual%20Question-Answering&body=Title%3A%20Detecting%20and%20Understanding%20Hateful%20Contents%20in%20Memes%20Through%20Captioning%0A%20%20and%20Visual%20Question-Answering%0AAuthor%3A%20Ali%20Anaissi%20and%20Junaid%20Akram%20and%20Kunal%20Chaturvedi%20and%20Ali%20Braytee%0AAbstract%3A%20%20%20Memes%20are%20widely%20used%20for%20humor%20and%20cultural%20commentary%2C%20but%20they%20are%0Aincreasingly%20exploited%20to%20spread%20hateful%20content.%20Due%20to%20their%20multimodal%0Anature%2C%20hateful%20memes%20often%20evade%20traditional%20text-only%20or%20image-only%20detection%0Asystems%2C%20particularly%20when%20they%20employ%20subtle%20or%20coded%20references.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20multimodal%20hate%20detection%20framework%20that%0Aintegrates%20key%20components%3A%20OCR%20to%20extract%20embedded%20text%2C%20captioning%20to%20describe%0Avisual%20content%20neutrally%2C%20sub-label%20classification%20for%20granular%20categorization%0Aof%20hateful%20content%2C%20RAG%20for%20contextually%20relevant%20retrieval%2C%20and%20VQA%20for%0Aiterative%20analysis%20of%20symbolic%20and%20contextual%20cues.%20This%20enables%20the%20framework%0Ato%20uncover%20latent%20signals%20that%20simpler%20pipelines%20fail%20to%20detect.%20Experimental%0Aresults%20on%20the%20Facebook%20Hateful%20Memes%20dataset%20reveal%20that%20the%20proposed%0Aframework%20exceeds%20the%20performance%20of%20unimodal%20and%20conventional%20multimodal%0Amodels%20in%20both%20accuracy%20and%20AUC-ROC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Understanding%2520Hateful%2520Contents%2520in%2520Memes%2520Through%2520Captioning%250A%2520%2520and%2520Visual%2520Question-Answering%26entry.906535625%3DAli%2520Anaissi%2520and%2520Junaid%2520Akram%2520and%2520Kunal%2520Chaturvedi%2520and%2520Ali%2520Braytee%26entry.1292438233%3D%2520%2520Memes%2520are%2520widely%2520used%2520for%2520humor%2520and%2520cultural%2520commentary%252C%2520but%2520they%2520are%250Aincreasingly%2520exploited%2520to%2520spread%2520hateful%2520content.%2520Due%2520to%2520their%2520multimodal%250Anature%252C%2520hateful%2520memes%2520often%2520evade%2520traditional%2520text-only%2520or%2520image-only%2520detection%250Asystems%252C%2520particularly%2520when%2520they%2520employ%2520subtle%2520or%2520coded%2520references.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520multimodal%2520hate%2520detection%2520framework%2520that%250Aintegrates%2520key%2520components%253A%2520OCR%2520to%2520extract%2520embedded%2520text%252C%2520captioning%2520to%2520describe%250Avisual%2520content%2520neutrally%252C%2520sub-label%2520classification%2520for%2520granular%2520categorization%250Aof%2520hateful%2520content%252C%2520RAG%2520for%2520contextually%2520relevant%2520retrieval%252C%2520and%2520VQA%2520for%250Aiterative%2520analysis%2520of%2520symbolic%2520and%2520contextual%2520cues.%2520This%2520enables%2520the%2520framework%250Ato%2520uncover%2520latent%2520signals%2520that%2520simpler%2520pipelines%2520fail%2520to%2520detect.%2520Experimental%250Aresults%2520on%2520the%2520Facebook%2520Hateful%2520Memes%2520dataset%2520reveal%2520that%2520the%2520proposed%250Aframework%2520exceeds%2520the%2520performance%2520of%2520unimodal%2520and%2520conventional%2520multimodal%250Amodels%2520in%2520both%2520accuracy%2520and%2520AUC-ROC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Understanding%20Hateful%20Contents%20in%20Memes%20Through%20Captioning%0A%20%20and%20Visual%20Question-Answering&entry.906535625=Ali%20Anaissi%20and%20Junaid%20Akram%20and%20Kunal%20Chaturvedi%20and%20Ali%20Braytee&entry.1292438233=%20%20Memes%20are%20widely%20used%20for%20humor%20and%20cultural%20commentary%2C%20but%20they%20are%0Aincreasingly%20exploited%20to%20spread%20hateful%20content.%20Due%20to%20their%20multimodal%0Anature%2C%20hateful%20memes%20often%20evade%20traditional%20text-only%20or%20image-only%20detection%0Asystems%2C%20particularly%20when%20they%20employ%20subtle%20or%20coded%20references.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20multimodal%20hate%20detection%20framework%20that%0Aintegrates%20key%20components%3A%20OCR%20to%20extract%20embedded%20text%2C%20captioning%20to%20describe%0Avisual%20content%20neutrally%2C%20sub-label%20classification%20for%20granular%20categorization%0Aof%20hateful%20content%2C%20RAG%20for%20contextually%20relevant%20retrieval%2C%20and%20VQA%20for%0Aiterative%20analysis%20of%20symbolic%20and%20contextual%20cues.%20This%20enables%20the%20framework%0Ato%20uncover%20latent%20signals%20that%20simpler%20pipelines%20fail%20to%20detect.%20Experimental%0Aresults%20on%20the%20Facebook%20Hateful%20Memes%20dataset%20reveal%20that%20the%20proposed%0Aframework%20exceeds%20the%20performance%20of%20unimodal%20and%20conventional%20multimodal%0Amodels%20in%20both%20accuracy%20and%20AUC-ROC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16723v1&entry.124074799=Read"},
{"title": "OSDFace: One-Step Diffusion Model for Face Restoration", "author": "Jingkai Wang and Jue Gong and Lin Zhang and Zheng Chen and Xing Liu and Hong Gu and Yutong Liu and Yulun Zhang and Xiaokang Yang", "abstract": "  Diffusion models have demonstrated impressive performance in face\nrestoration. Yet, their multi-step inference process remains computationally\nintensive, limiting their applicability in real-world scenarios. Moreover,\nexisting methods often struggle to generate face images that are harmonious,\nrealistic, and consistent with the subject's identity. In this work, we propose\nOSDFace, a novel one-step diffusion model for face restoration. Specifically,\nwe propose a visual representation embedder (VRE) to better capture prior\ninformation and understand the input face. In VRE, low-quality faces are\nprocessed by a visual tokenizer and subsequently embedded with a\nvector-quantized dictionary to generate visual prompts. Additionally, we\nincorporate a facial identity loss derived from face recognition to further\nensure identity consistency. We further employ a generative adversarial network\n(GAN) as a guidance model to encourage distribution alignment between the\nrestored face and the ground truth. Experimental results demonstrate that\nOSDFace surpasses current state-of-the-art (SOTA) methods in both visual\nquality and quantitative metrics, generating high-fidelity, natural face images\nwith high identity consistency. The code and model will be released at\nhttps://github.com/jkwang28/OSDFace.\n", "link": "http://arxiv.org/abs/2411.17163v2", "date": "2025-04-23", "relevancy": 2.4511, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.64}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6198}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OSDFace%3A%20One-Step%20Diffusion%20Model%20for%20Face%20Restoration&body=Title%3A%20OSDFace%3A%20One-Step%20Diffusion%20Model%20for%20Face%20Restoration%0AAuthor%3A%20Jingkai%20Wang%20and%20Jue%20Gong%20and%20Lin%20Zhang%20and%20Zheng%20Chen%20and%20Xing%20Liu%20and%20Hong%20Gu%20and%20Yutong%20Liu%20and%20Yulun%20Zhang%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20face%0Arestoration.%20Yet%2C%20their%20multi-step%20inference%20process%20remains%20computationally%0Aintensive%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios.%20Moreover%2C%0Aexisting%20methods%20often%20struggle%20to%20generate%20face%20images%20that%20are%20harmonious%2C%0Arealistic%2C%20and%20consistent%20with%20the%20subject%27s%20identity.%20In%20this%20work%2C%20we%20propose%0AOSDFace%2C%20a%20novel%20one-step%20diffusion%20model%20for%20face%20restoration.%20Specifically%2C%0Awe%20propose%20a%20visual%20representation%20embedder%20%28VRE%29%20to%20better%20capture%20prior%0Ainformation%20and%20understand%20the%20input%20face.%20In%20VRE%2C%20low-quality%20faces%20are%0Aprocessed%20by%20a%20visual%20tokenizer%20and%20subsequently%20embedded%20with%20a%0Avector-quantized%20dictionary%20to%20generate%20visual%20prompts.%20Additionally%2C%20we%0Aincorporate%20a%20facial%20identity%20loss%20derived%20from%20face%20recognition%20to%20further%0Aensure%20identity%20consistency.%20We%20further%20employ%20a%20generative%20adversarial%20network%0A%28GAN%29%20as%20a%20guidance%20model%20to%20encourage%20distribution%20alignment%20between%20the%0Arestored%20face%20and%20the%20ground%20truth.%20Experimental%20results%20demonstrate%20that%0AOSDFace%20surpasses%20current%20state-of-the-art%20%28SOTA%29%20methods%20in%20both%20visual%0Aquality%20and%20quantitative%20metrics%2C%20generating%20high-fidelity%2C%20natural%20face%20images%0Awith%20high%20identity%20consistency.%20The%20code%20and%20model%20will%20be%20released%20at%0Ahttps%3A//github.com/jkwang28/OSDFace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17163v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOSDFace%253A%2520One-Step%2520Diffusion%2520Model%2520for%2520Face%2520Restoration%26entry.906535625%3DJingkai%2520Wang%2520and%2520Jue%2520Gong%2520and%2520Lin%2520Zhang%2520and%2520Zheng%2520Chen%2520and%2520Xing%2520Liu%2520and%2520Hong%2520Gu%2520and%2520Yutong%2520Liu%2520and%2520Yulun%2520Zhang%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520impressive%2520performance%2520in%2520face%250Arestoration.%2520Yet%252C%2520their%2520multi-step%2520inference%2520process%2520remains%2520computationally%250Aintensive%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%2520scenarios.%2520Moreover%252C%250Aexisting%2520methods%2520often%2520struggle%2520to%2520generate%2520face%2520images%2520that%2520are%2520harmonious%252C%250Arealistic%252C%2520and%2520consistent%2520with%2520the%2520subject%2527s%2520identity.%2520In%2520this%2520work%252C%2520we%2520propose%250AOSDFace%252C%2520a%2520novel%2520one-step%2520diffusion%2520model%2520for%2520face%2520restoration.%2520Specifically%252C%250Awe%2520propose%2520a%2520visual%2520representation%2520embedder%2520%2528VRE%2529%2520to%2520better%2520capture%2520prior%250Ainformation%2520and%2520understand%2520the%2520input%2520face.%2520In%2520VRE%252C%2520low-quality%2520faces%2520are%250Aprocessed%2520by%2520a%2520visual%2520tokenizer%2520and%2520subsequently%2520embedded%2520with%2520a%250Avector-quantized%2520dictionary%2520to%2520generate%2520visual%2520prompts.%2520Additionally%252C%2520we%250Aincorporate%2520a%2520facial%2520identity%2520loss%2520derived%2520from%2520face%2520recognition%2520to%2520further%250Aensure%2520identity%2520consistency.%2520We%2520further%2520employ%2520a%2520generative%2520adversarial%2520network%250A%2528GAN%2529%2520as%2520a%2520guidance%2520model%2520to%2520encourage%2520distribution%2520alignment%2520between%2520the%250Arestored%2520face%2520and%2520the%2520ground%2520truth.%2520Experimental%2520results%2520demonstrate%2520that%250AOSDFace%2520surpasses%2520current%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520in%2520both%2520visual%250Aquality%2520and%2520quantitative%2520metrics%252C%2520generating%2520high-fidelity%252C%2520natural%2520face%2520images%250Awith%2520high%2520identity%2520consistency.%2520The%2520code%2520and%2520model%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/jkwang28/OSDFace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17163v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OSDFace%3A%20One-Step%20Diffusion%20Model%20for%20Face%20Restoration&entry.906535625=Jingkai%20Wang%20and%20Jue%20Gong%20and%20Lin%20Zhang%20and%20Zheng%20Chen%20and%20Xing%20Liu%20and%20Hong%20Gu%20and%20Yutong%20Liu%20and%20Yulun%20Zhang%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20face%0Arestoration.%20Yet%2C%20their%20multi-step%20inference%20process%20remains%20computationally%0Aintensive%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios.%20Moreover%2C%0Aexisting%20methods%20often%20struggle%20to%20generate%20face%20images%20that%20are%20harmonious%2C%0Arealistic%2C%20and%20consistent%20with%20the%20subject%27s%20identity.%20In%20this%20work%2C%20we%20propose%0AOSDFace%2C%20a%20novel%20one-step%20diffusion%20model%20for%20face%20restoration.%20Specifically%2C%0Awe%20propose%20a%20visual%20representation%20embedder%20%28VRE%29%20to%20better%20capture%20prior%0Ainformation%20and%20understand%20the%20input%20face.%20In%20VRE%2C%20low-quality%20faces%20are%0Aprocessed%20by%20a%20visual%20tokenizer%20and%20subsequently%20embedded%20with%20a%0Avector-quantized%20dictionary%20to%20generate%20visual%20prompts.%20Additionally%2C%20we%0Aincorporate%20a%20facial%20identity%20loss%20derived%20from%20face%20recognition%20to%20further%0Aensure%20identity%20consistency.%20We%20further%20employ%20a%20generative%20adversarial%20network%0A%28GAN%29%20as%20a%20guidance%20model%20to%20encourage%20distribution%20alignment%20between%20the%0Arestored%20face%20and%20the%20ground%20truth.%20Experimental%20results%20demonstrate%20that%0AOSDFace%20surpasses%20current%20state-of-the-art%20%28SOTA%29%20methods%20in%20both%20visual%0Aquality%20and%20quantitative%20metrics%2C%20generating%20high-fidelity%2C%20natural%20face%20images%0Awith%20high%20identity%20consistency.%20The%20code%20and%20model%20will%20be%20released%20at%0Ahttps%3A//github.com/jkwang28/OSDFace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17163v2&entry.124074799=Read"},
{"title": "Towards Explainable AI: Multi-Modal Transformer for Video-based Image\n  Description Generation", "author": "Lakshita Agarwal and Bindu Verma", "abstract": "  Understanding and analyzing video actions are essential for producing\ninsightful and contextualized descriptions, especially for video-based\napplications like intelligent monitoring and autonomous systems. The proposed\nwork introduces a novel framework for generating natural language descriptions\nfrom video datasets by combining textual and visual modalities. The suggested\narchitecture makes use of ResNet50 to extract visual features from video frames\nthat are taken from the Microsoft Research Video Description Corpus (MSVD), and\nBerkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual\ncharacteristics are converted into patch embeddings and then run through an\nencoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In\norder to align textual and visual representations and guarantee high-quality\ndescription production, the system uses multi-head self-attention and\ncross-attention techniques. The model's efficacy is demonstrated by performance\nevaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested\nframework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)\nand 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores\nof 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and\n0.795 (MSVD). By producing human-like, contextually relevant descriptions,\nstrengthening interpretability, and improving real-world applications, this\nresearch advances explainable AI.\n", "link": "http://arxiv.org/abs/2504.16788v1", "date": "2025-04-23", "relevancy": 2.4471, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6259}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6088}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Explainable%20AI%3A%20Multi-Modal%20Transformer%20for%20Video-based%20Image%0A%20%20Description%20Generation&body=Title%3A%20Towards%20Explainable%20AI%3A%20Multi-Modal%20Transformer%20for%20Video-based%20Image%0A%20%20Description%20Generation%0AAuthor%3A%20Lakshita%20Agarwal%20and%20Bindu%20Verma%0AAbstract%3A%20%20%20Understanding%20and%20analyzing%20video%20actions%20are%20essential%20for%20producing%0Ainsightful%20and%20contextualized%20descriptions%2C%20especially%20for%20video-based%0Aapplications%20like%20intelligent%20monitoring%20and%20autonomous%20systems.%20The%20proposed%0Awork%20introduces%20a%20novel%20framework%20for%20generating%20natural%20language%20descriptions%0Afrom%20video%20datasets%20by%20combining%20textual%20and%20visual%20modalities.%20The%20suggested%0Aarchitecture%20makes%20use%20of%20ResNet50%20to%20extract%20visual%20features%20from%20video%20frames%0Athat%20are%20taken%20from%20the%20Microsoft%20Research%20Video%20Description%20Corpus%20%28MSVD%29%2C%20and%0ABerkeley%20DeepDrive%20eXplanation%20%28BDD-X%29%20datasets.%20The%20extracted%20visual%0Acharacteristics%20are%20converted%20into%20patch%20embeddings%20and%20then%20run%20through%20an%0Aencoder-decoder%20model%20based%20on%20Generative%20Pre-trained%20Transformer-2%20%28GPT-2%29.%20In%0Aorder%20to%20align%20textual%20and%20visual%20representations%20and%20guarantee%20high-quality%0Adescription%20production%2C%20the%20system%20uses%20multi-head%20self-attention%20and%0Across-attention%20techniques.%20The%20model%27s%20efficacy%20is%20demonstrated%20by%20performance%0Aevaluation%20using%20BLEU%20%281-4%29%2C%20CIDEr%2C%20METEOR%2C%20and%20ROUGE-L.%20The%20suggested%0Aframework%20outperforms%20traditional%20methods%20with%20BLEU-4%20scores%20of%200.755%20%28BDD-X%29%0Aand%200.778%20%28MSVD%29%2C%20CIDEr%20scores%20of%201.235%20%28BDD-X%29%20and%201.315%20%28MSVD%29%2C%20METEOR%20scores%0Aof%200.312%20%28BDD-X%29%20and%200.329%20%28MSVD%29%2C%20and%20ROUGE-L%20scores%20of%200.782%20%28BDD-X%29%20and%0A0.795%20%28MSVD%29.%20By%20producing%20human-like%2C%20contextually%20relevant%20descriptions%2C%0Astrengthening%20interpretability%2C%20and%20improving%20real-world%20applications%2C%20this%0Aresearch%20advances%20explainable%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Explainable%2520AI%253A%2520Multi-Modal%2520Transformer%2520for%2520Video-based%2520Image%250A%2520%2520Description%2520Generation%26entry.906535625%3DLakshita%2520Agarwal%2520and%2520Bindu%2520Verma%26entry.1292438233%3D%2520%2520Understanding%2520and%2520analyzing%2520video%2520actions%2520are%2520essential%2520for%2520producing%250Ainsightful%2520and%2520contextualized%2520descriptions%252C%2520especially%2520for%2520video-based%250Aapplications%2520like%2520intelligent%2520monitoring%2520and%2520autonomous%2520systems.%2520The%2520proposed%250Awork%2520introduces%2520a%2520novel%2520framework%2520for%2520generating%2520natural%2520language%2520descriptions%250Afrom%2520video%2520datasets%2520by%2520combining%2520textual%2520and%2520visual%2520modalities.%2520The%2520suggested%250Aarchitecture%2520makes%2520use%2520of%2520ResNet50%2520to%2520extract%2520visual%2520features%2520from%2520video%2520frames%250Athat%2520are%2520taken%2520from%2520the%2520Microsoft%2520Research%2520Video%2520Description%2520Corpus%2520%2528MSVD%2529%252C%2520and%250ABerkeley%2520DeepDrive%2520eXplanation%2520%2528BDD-X%2529%2520datasets.%2520The%2520extracted%2520visual%250Acharacteristics%2520are%2520converted%2520into%2520patch%2520embeddings%2520and%2520then%2520run%2520through%2520an%250Aencoder-decoder%2520model%2520based%2520on%2520Generative%2520Pre-trained%2520Transformer-2%2520%2528GPT-2%2529.%2520In%250Aorder%2520to%2520align%2520textual%2520and%2520visual%2520representations%2520and%2520guarantee%2520high-quality%250Adescription%2520production%252C%2520the%2520system%2520uses%2520multi-head%2520self-attention%2520and%250Across-attention%2520techniques.%2520The%2520model%2527s%2520efficacy%2520is%2520demonstrated%2520by%2520performance%250Aevaluation%2520using%2520BLEU%2520%25281-4%2529%252C%2520CIDEr%252C%2520METEOR%252C%2520and%2520ROUGE-L.%2520The%2520suggested%250Aframework%2520outperforms%2520traditional%2520methods%2520with%2520BLEU-4%2520scores%2520of%25200.755%2520%2528BDD-X%2529%250Aand%25200.778%2520%2528MSVD%2529%252C%2520CIDEr%2520scores%2520of%25201.235%2520%2528BDD-X%2529%2520and%25201.315%2520%2528MSVD%2529%252C%2520METEOR%2520scores%250Aof%25200.312%2520%2528BDD-X%2529%2520and%25200.329%2520%2528MSVD%2529%252C%2520and%2520ROUGE-L%2520scores%2520of%25200.782%2520%2528BDD-X%2529%2520and%250A0.795%2520%2528MSVD%2529.%2520By%2520producing%2520human-like%252C%2520contextually%2520relevant%2520descriptions%252C%250Astrengthening%2520interpretability%252C%2520and%2520improving%2520real-world%2520applications%252C%2520this%250Aresearch%2520advances%2520explainable%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Explainable%20AI%3A%20Multi-Modal%20Transformer%20for%20Video-based%20Image%0A%20%20Description%20Generation&entry.906535625=Lakshita%20Agarwal%20and%20Bindu%20Verma&entry.1292438233=%20%20Understanding%20and%20analyzing%20video%20actions%20are%20essential%20for%20producing%0Ainsightful%20and%20contextualized%20descriptions%2C%20especially%20for%20video-based%0Aapplications%20like%20intelligent%20monitoring%20and%20autonomous%20systems.%20The%20proposed%0Awork%20introduces%20a%20novel%20framework%20for%20generating%20natural%20language%20descriptions%0Afrom%20video%20datasets%20by%20combining%20textual%20and%20visual%20modalities.%20The%20suggested%0Aarchitecture%20makes%20use%20of%20ResNet50%20to%20extract%20visual%20features%20from%20video%20frames%0Athat%20are%20taken%20from%20the%20Microsoft%20Research%20Video%20Description%20Corpus%20%28MSVD%29%2C%20and%0ABerkeley%20DeepDrive%20eXplanation%20%28BDD-X%29%20datasets.%20The%20extracted%20visual%0Acharacteristics%20are%20converted%20into%20patch%20embeddings%20and%20then%20run%20through%20an%0Aencoder-decoder%20model%20based%20on%20Generative%20Pre-trained%20Transformer-2%20%28GPT-2%29.%20In%0Aorder%20to%20align%20textual%20and%20visual%20representations%20and%20guarantee%20high-quality%0Adescription%20production%2C%20the%20system%20uses%20multi-head%20self-attention%20and%0Across-attention%20techniques.%20The%20model%27s%20efficacy%20is%20demonstrated%20by%20performance%0Aevaluation%20using%20BLEU%20%281-4%29%2C%20CIDEr%2C%20METEOR%2C%20and%20ROUGE-L.%20The%20suggested%0Aframework%20outperforms%20traditional%20methods%20with%20BLEU-4%20scores%20of%200.755%20%28BDD-X%29%0Aand%200.778%20%28MSVD%29%2C%20CIDEr%20scores%20of%201.235%20%28BDD-X%29%20and%201.315%20%28MSVD%29%2C%20METEOR%20scores%0Aof%200.312%20%28BDD-X%29%20and%200.329%20%28MSVD%29%2C%20and%20ROUGE-L%20scores%20of%200.782%20%28BDD-X%29%20and%0A0.795%20%28MSVD%29.%20By%20producing%20human-like%2C%20contextually%20relevant%20descriptions%2C%0Astrengthening%20interpretability%2C%20and%20improving%20real-world%20applications%2C%20this%0Aresearch%20advances%20explainable%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16788v1&entry.124074799=Read"},
{"title": "Noise-Tolerant Coreset-Based Class Incremental Continual Learning", "author": "Edison Mucllari and Aswin Raghavan and Zachary Alan Daniels", "abstract": "  Many applications of computer vision require the ability to adapt to novel\ndata distributions after deployment. Adaptation requires algorithms capable of\ncontinual learning (CL). Continual learners must be plastic to adapt to novel\ntasks while minimizing forgetting of previous tasks.However, CL opens up\navenues for noise to enter the training pipeline and disrupt the CL. This work\nfocuses on label noise and instance noise in the context of class-incremental\nlearning (CIL), where new classes are added to a classifier over time, and\nthere is no access to external data from past classes. We aim to understand the\nsensitivity of CL methods that work by replaying items from a memory\nconstructed using the idea of Coresets. We derive a new bound for the\nrobustness of such a method to uncorrelated instance noise under a general\nadditive noise threat model, revealing several insights. Putting the theory\ninto practice, we create two continual learning algorithms to construct\nnoise-tolerant replay buffers. We empirically compare the effectiveness of\nprior memory-based continual learners and the proposed algorithms under label\nand uncorrelated instance noise on five diverse datasets. We show that existing\nmemory-based CL are not robust whereas the proposed methods exhibit significant\nimprovements in maximizing classification accuracy and minimizing forgetting in\nthe noisy CIL setting.\n", "link": "http://arxiv.org/abs/2504.16763v1", "date": "2025-04-23", "relevancy": 2.4462, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4955}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise-Tolerant%20Coreset-Based%20Class%20Incremental%20Continual%20Learning&body=Title%3A%20Noise-Tolerant%20Coreset-Based%20Class%20Incremental%20Continual%20Learning%0AAuthor%3A%20Edison%20Mucllari%20and%20Aswin%20Raghavan%20and%20Zachary%20Alan%20Daniels%0AAbstract%3A%20%20%20Many%20applications%20of%20computer%20vision%20require%20the%20ability%20to%20adapt%20to%20novel%0Adata%20distributions%20after%20deployment.%20Adaptation%20requires%20algorithms%20capable%20of%0Acontinual%20learning%20%28CL%29.%20Continual%20learners%20must%20be%20plastic%20to%20adapt%20to%20novel%0Atasks%20while%20minimizing%20forgetting%20of%20previous%20tasks.However%2C%20CL%20opens%20up%0Aavenues%20for%20noise%20to%20enter%20the%20training%20pipeline%20and%20disrupt%20the%20CL.%20This%20work%0Afocuses%20on%20label%20noise%20and%20instance%20noise%20in%20the%20context%20of%20class-incremental%0Alearning%20%28CIL%29%2C%20where%20new%20classes%20are%20added%20to%20a%20classifier%20over%20time%2C%20and%0Athere%20is%20no%20access%20to%20external%20data%20from%20past%20classes.%20We%20aim%20to%20understand%20the%0Asensitivity%20of%20CL%20methods%20that%20work%20by%20replaying%20items%20from%20a%20memory%0Aconstructed%20using%20the%20idea%20of%20Coresets.%20We%20derive%20a%20new%20bound%20for%20the%0Arobustness%20of%20such%20a%20method%20to%20uncorrelated%20instance%20noise%20under%20a%20general%0Aadditive%20noise%20threat%20model%2C%20revealing%20several%20insights.%20Putting%20the%20theory%0Ainto%20practice%2C%20we%20create%20two%20continual%20learning%20algorithms%20to%20construct%0Anoise-tolerant%20replay%20buffers.%20We%20empirically%20compare%20the%20effectiveness%20of%0Aprior%20memory-based%20continual%20learners%20and%20the%20proposed%20algorithms%20under%20label%0Aand%20uncorrelated%20instance%20noise%20on%20five%20diverse%20datasets.%20We%20show%20that%20existing%0Amemory-based%20CL%20are%20not%20robust%20whereas%20the%20proposed%20methods%20exhibit%20significant%0Aimprovements%20in%20maximizing%20classification%20accuracy%20and%20minimizing%20forgetting%20in%0Athe%20noisy%20CIL%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise-Tolerant%2520Coreset-Based%2520Class%2520Incremental%2520Continual%2520Learning%26entry.906535625%3DEdison%2520Mucllari%2520and%2520Aswin%2520Raghavan%2520and%2520Zachary%2520Alan%2520Daniels%26entry.1292438233%3D%2520%2520Many%2520applications%2520of%2520computer%2520vision%2520require%2520the%2520ability%2520to%2520adapt%2520to%2520novel%250Adata%2520distributions%2520after%2520deployment.%2520Adaptation%2520requires%2520algorithms%2520capable%2520of%250Acontinual%2520learning%2520%2528CL%2529.%2520Continual%2520learners%2520must%2520be%2520plastic%2520to%2520adapt%2520to%2520novel%250Atasks%2520while%2520minimizing%2520forgetting%2520of%2520previous%2520tasks.However%252C%2520CL%2520opens%2520up%250Aavenues%2520for%2520noise%2520to%2520enter%2520the%2520training%2520pipeline%2520and%2520disrupt%2520the%2520CL.%2520This%2520work%250Afocuses%2520on%2520label%2520noise%2520and%2520instance%2520noise%2520in%2520the%2520context%2520of%2520class-incremental%250Alearning%2520%2528CIL%2529%252C%2520where%2520new%2520classes%2520are%2520added%2520to%2520a%2520classifier%2520over%2520time%252C%2520and%250Athere%2520is%2520no%2520access%2520to%2520external%2520data%2520from%2520past%2520classes.%2520We%2520aim%2520to%2520understand%2520the%250Asensitivity%2520of%2520CL%2520methods%2520that%2520work%2520by%2520replaying%2520items%2520from%2520a%2520memory%250Aconstructed%2520using%2520the%2520idea%2520of%2520Coresets.%2520We%2520derive%2520a%2520new%2520bound%2520for%2520the%250Arobustness%2520of%2520such%2520a%2520method%2520to%2520uncorrelated%2520instance%2520noise%2520under%2520a%2520general%250Aadditive%2520noise%2520threat%2520model%252C%2520revealing%2520several%2520insights.%2520Putting%2520the%2520theory%250Ainto%2520practice%252C%2520we%2520create%2520two%2520continual%2520learning%2520algorithms%2520to%2520construct%250Anoise-tolerant%2520replay%2520buffers.%2520We%2520empirically%2520compare%2520the%2520effectiveness%2520of%250Aprior%2520memory-based%2520continual%2520learners%2520and%2520the%2520proposed%2520algorithms%2520under%2520label%250Aand%2520uncorrelated%2520instance%2520noise%2520on%2520five%2520diverse%2520datasets.%2520We%2520show%2520that%2520existing%250Amemory-based%2520CL%2520are%2520not%2520robust%2520whereas%2520the%2520proposed%2520methods%2520exhibit%2520significant%250Aimprovements%2520in%2520maximizing%2520classification%2520accuracy%2520and%2520minimizing%2520forgetting%2520in%250Athe%2520noisy%2520CIL%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise-Tolerant%20Coreset-Based%20Class%20Incremental%20Continual%20Learning&entry.906535625=Edison%20Mucllari%20and%20Aswin%20Raghavan%20and%20Zachary%20Alan%20Daniels&entry.1292438233=%20%20Many%20applications%20of%20computer%20vision%20require%20the%20ability%20to%20adapt%20to%20novel%0Adata%20distributions%20after%20deployment.%20Adaptation%20requires%20algorithms%20capable%20of%0Acontinual%20learning%20%28CL%29.%20Continual%20learners%20must%20be%20plastic%20to%20adapt%20to%20novel%0Atasks%20while%20minimizing%20forgetting%20of%20previous%20tasks.However%2C%20CL%20opens%20up%0Aavenues%20for%20noise%20to%20enter%20the%20training%20pipeline%20and%20disrupt%20the%20CL.%20This%20work%0Afocuses%20on%20label%20noise%20and%20instance%20noise%20in%20the%20context%20of%20class-incremental%0Alearning%20%28CIL%29%2C%20where%20new%20classes%20are%20added%20to%20a%20classifier%20over%20time%2C%20and%0Athere%20is%20no%20access%20to%20external%20data%20from%20past%20classes.%20We%20aim%20to%20understand%20the%0Asensitivity%20of%20CL%20methods%20that%20work%20by%20replaying%20items%20from%20a%20memory%0Aconstructed%20using%20the%20idea%20of%20Coresets.%20We%20derive%20a%20new%20bound%20for%20the%0Arobustness%20of%20such%20a%20method%20to%20uncorrelated%20instance%20noise%20under%20a%20general%0Aadditive%20noise%20threat%20model%2C%20revealing%20several%20insights.%20Putting%20the%20theory%0Ainto%20practice%2C%20we%20create%20two%20continual%20learning%20algorithms%20to%20construct%0Anoise-tolerant%20replay%20buffers.%20We%20empirically%20compare%20the%20effectiveness%20of%0Aprior%20memory-based%20continual%20learners%20and%20the%20proposed%20algorithms%20under%20label%0Aand%20uncorrelated%20instance%20noise%20on%20five%20diverse%20datasets.%20We%20show%20that%20existing%0Amemory-based%20CL%20are%20not%20robust%20whereas%20the%20proposed%20methods%20exhibit%20significant%0Aimprovements%20in%20maximizing%20classification%20accuracy%20and%20minimizing%20forgetting%20in%0Athe%20noisy%20CIL%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16763v1&entry.124074799=Read"},
{"title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation", "author": "Ruotong Wang and Mingli Zhu and Jiarong Ou and Rui Chen and Xin Tao and Pengfei Wan and Baoyuan Wu", "abstract": "  Text-to-video (T2V) generative models have rapidly advanced and found\nwidespread applications across fields like entertainment, education, and\nmarketing. However, the adversarial vulnerabilities of these models remain\nrarely explored. We observe that in T2V generation tasks, the generated videos\noften contain substantial redundant information not explicitly specified in the\ntext prompts, such as environmental elements, secondary objects, and additional\ndetails, providing opportunities for malicious attackers to embed hidden\nharmful content. Exploiting this inherent redundancy, we introduce BadVideo,\nthe first backdoor attack framework tailored for T2V generation. Our attack\nfocuses on designing target adversarial outputs through two key strategies: (1)\nSpatio-Temporal Composition, which combines different spatiotemporal features\nto encode malicious information; (2) Dynamic Element Transformation, which\nintroduces transformations in redundant elements over time to convey malicious\ninformation. Based on these strategies, the attacker's malicious target\nseamlessly integrates with the user's textual instructions, providing high\nstealthiness. Moreover, by exploiting the temporal dimension of videos, our\nattack successfully evades traditional content moderation systems that\nprimarily analyze spatial information within individual frames. Extensive\nexperiments demonstrate that BadVideo achieves high attack success rates while\npreserving original semantics and maintaining excellent performance on clean\ninputs. Overall, our work reveals the adversarial vulnerability of T2V models,\ncalling attention to potential risks and misuse. Our project page is at\nhttps://wrt2000.github.io/BadVideo2025/.\n", "link": "http://arxiv.org/abs/2504.16907v1", "date": "2025-04-23", "relevancy": 2.3614, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6365}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6238}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BadVideo%3A%20Stealthy%20Backdoor%20Attack%20against%20Text-to-Video%20Generation&body=Title%3A%20BadVideo%3A%20Stealthy%20Backdoor%20Attack%20against%20Text-to-Video%20Generation%0AAuthor%3A%20Ruotong%20Wang%20and%20Mingli%20Zhu%20and%20Jiarong%20Ou%20and%20Rui%20Chen%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Baoyuan%20Wu%0AAbstract%3A%20%20%20Text-to-video%20%28T2V%29%20generative%20models%20have%20rapidly%20advanced%20and%20found%0Awidespread%20applications%20across%20fields%20like%20entertainment%2C%20education%2C%20and%0Amarketing.%20However%2C%20the%20adversarial%20vulnerabilities%20of%20these%20models%20remain%0Ararely%20explored.%20We%20observe%20that%20in%20T2V%20generation%20tasks%2C%20the%20generated%20videos%0Aoften%20contain%20substantial%20redundant%20information%20not%20explicitly%20specified%20in%20the%0Atext%20prompts%2C%20such%20as%20environmental%20elements%2C%20secondary%20objects%2C%20and%20additional%0Adetails%2C%20providing%20opportunities%20for%20malicious%20attackers%20to%20embed%20hidden%0Aharmful%20content.%20Exploiting%20this%20inherent%20redundancy%2C%20we%20introduce%20BadVideo%2C%0Athe%20first%20backdoor%20attack%20framework%20tailored%20for%20T2V%20generation.%20Our%20attack%0Afocuses%20on%20designing%20target%20adversarial%20outputs%20through%20two%20key%20strategies%3A%20%281%29%0ASpatio-Temporal%20Composition%2C%20which%20combines%20different%20spatiotemporal%20features%0Ato%20encode%20malicious%20information%3B%20%282%29%20Dynamic%20Element%20Transformation%2C%20which%0Aintroduces%20transformations%20in%20redundant%20elements%20over%20time%20to%20convey%20malicious%0Ainformation.%20Based%20on%20these%20strategies%2C%20the%20attacker%27s%20malicious%20target%0Aseamlessly%20integrates%20with%20the%20user%27s%20textual%20instructions%2C%20providing%20high%0Astealthiness.%20Moreover%2C%20by%20exploiting%20the%20temporal%20dimension%20of%20videos%2C%20our%0Aattack%20successfully%20evades%20traditional%20content%20moderation%20systems%20that%0Aprimarily%20analyze%20spatial%20information%20within%20individual%20frames.%20Extensive%0Aexperiments%20demonstrate%20that%20BadVideo%20achieves%20high%20attack%20success%20rates%20while%0Apreserving%20original%20semantics%20and%20maintaining%20excellent%20performance%20on%20clean%0Ainputs.%20Overall%2C%20our%20work%20reveals%20the%20adversarial%20vulnerability%20of%20T2V%20models%2C%0Acalling%20attention%20to%20potential%20risks%20and%20misuse.%20Our%20project%20page%20is%20at%0Ahttps%3A//wrt2000.github.io/BadVideo2025/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBadVideo%253A%2520Stealthy%2520Backdoor%2520Attack%2520against%2520Text-to-Video%2520Generation%26entry.906535625%3DRuotong%2520Wang%2520and%2520Mingli%2520Zhu%2520and%2520Jiarong%2520Ou%2520and%2520Rui%2520Chen%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Baoyuan%2520Wu%26entry.1292438233%3D%2520%2520Text-to-video%2520%2528T2V%2529%2520generative%2520models%2520have%2520rapidly%2520advanced%2520and%2520found%250Awidespread%2520applications%2520across%2520fields%2520like%2520entertainment%252C%2520education%252C%2520and%250Amarketing.%2520However%252C%2520the%2520adversarial%2520vulnerabilities%2520of%2520these%2520models%2520remain%250Ararely%2520explored.%2520We%2520observe%2520that%2520in%2520T2V%2520generation%2520tasks%252C%2520the%2520generated%2520videos%250Aoften%2520contain%2520substantial%2520redundant%2520information%2520not%2520explicitly%2520specified%2520in%2520the%250Atext%2520prompts%252C%2520such%2520as%2520environmental%2520elements%252C%2520secondary%2520objects%252C%2520and%2520additional%250Adetails%252C%2520providing%2520opportunities%2520for%2520malicious%2520attackers%2520to%2520embed%2520hidden%250Aharmful%2520content.%2520Exploiting%2520this%2520inherent%2520redundancy%252C%2520we%2520introduce%2520BadVideo%252C%250Athe%2520first%2520backdoor%2520attack%2520framework%2520tailored%2520for%2520T2V%2520generation.%2520Our%2520attack%250Afocuses%2520on%2520designing%2520target%2520adversarial%2520outputs%2520through%2520two%2520key%2520strategies%253A%2520%25281%2529%250ASpatio-Temporal%2520Composition%252C%2520which%2520combines%2520different%2520spatiotemporal%2520features%250Ato%2520encode%2520malicious%2520information%253B%2520%25282%2529%2520Dynamic%2520Element%2520Transformation%252C%2520which%250Aintroduces%2520transformations%2520in%2520redundant%2520elements%2520over%2520time%2520to%2520convey%2520malicious%250Ainformation.%2520Based%2520on%2520these%2520strategies%252C%2520the%2520attacker%2527s%2520malicious%2520target%250Aseamlessly%2520integrates%2520with%2520the%2520user%2527s%2520textual%2520instructions%252C%2520providing%2520high%250Astealthiness.%2520Moreover%252C%2520by%2520exploiting%2520the%2520temporal%2520dimension%2520of%2520videos%252C%2520our%250Aattack%2520successfully%2520evades%2520traditional%2520content%2520moderation%2520systems%2520that%250Aprimarily%2520analyze%2520spatial%2520information%2520within%2520individual%2520frames.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520BadVideo%2520achieves%2520high%2520attack%2520success%2520rates%2520while%250Apreserving%2520original%2520semantics%2520and%2520maintaining%2520excellent%2520performance%2520on%2520clean%250Ainputs.%2520Overall%252C%2520our%2520work%2520reveals%2520the%2520adversarial%2520vulnerability%2520of%2520T2V%2520models%252C%250Acalling%2520attention%2520to%2520potential%2520risks%2520and%2520misuse.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//wrt2000.github.io/BadVideo2025/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BadVideo%3A%20Stealthy%20Backdoor%20Attack%20against%20Text-to-Video%20Generation&entry.906535625=Ruotong%20Wang%20and%20Mingli%20Zhu%20and%20Jiarong%20Ou%20and%20Rui%20Chen%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Baoyuan%20Wu&entry.1292438233=%20%20Text-to-video%20%28T2V%29%20generative%20models%20have%20rapidly%20advanced%20and%20found%0Awidespread%20applications%20across%20fields%20like%20entertainment%2C%20education%2C%20and%0Amarketing.%20However%2C%20the%20adversarial%20vulnerabilities%20of%20these%20models%20remain%0Ararely%20explored.%20We%20observe%20that%20in%20T2V%20generation%20tasks%2C%20the%20generated%20videos%0Aoften%20contain%20substantial%20redundant%20information%20not%20explicitly%20specified%20in%20the%0Atext%20prompts%2C%20such%20as%20environmental%20elements%2C%20secondary%20objects%2C%20and%20additional%0Adetails%2C%20providing%20opportunities%20for%20malicious%20attackers%20to%20embed%20hidden%0Aharmful%20content.%20Exploiting%20this%20inherent%20redundancy%2C%20we%20introduce%20BadVideo%2C%0Athe%20first%20backdoor%20attack%20framework%20tailored%20for%20T2V%20generation.%20Our%20attack%0Afocuses%20on%20designing%20target%20adversarial%20outputs%20through%20two%20key%20strategies%3A%20%281%29%0ASpatio-Temporal%20Composition%2C%20which%20combines%20different%20spatiotemporal%20features%0Ato%20encode%20malicious%20information%3B%20%282%29%20Dynamic%20Element%20Transformation%2C%20which%0Aintroduces%20transformations%20in%20redundant%20elements%20over%20time%20to%20convey%20malicious%0Ainformation.%20Based%20on%20these%20strategies%2C%20the%20attacker%27s%20malicious%20target%0Aseamlessly%20integrates%20with%20the%20user%27s%20textual%20instructions%2C%20providing%20high%0Astealthiness.%20Moreover%2C%20by%20exploiting%20the%20temporal%20dimension%20of%20videos%2C%20our%0Aattack%20successfully%20evades%20traditional%20content%20moderation%20systems%20that%0Aprimarily%20analyze%20spatial%20information%20within%20individual%20frames.%20Extensive%0Aexperiments%20demonstrate%20that%20BadVideo%20achieves%20high%20attack%20success%20rates%20while%0Apreserving%20original%20semantics%20and%20maintaining%20excellent%20performance%20on%20clean%0Ainputs.%20Overall%2C%20our%20work%20reveals%20the%20adversarial%20vulnerability%20of%20T2V%20models%2C%0Acalling%20attention%20to%20potential%20risks%20and%20misuse.%20Our%20project%20page%20is%20at%0Ahttps%3A//wrt2000.github.io/BadVideo2025/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16907v1&entry.124074799=Read"},
{"title": "Tri-FusionNet: Enhancing Image Description Generation with\n  Transformer-based Fusion Network and Dual Attention Mechanism", "author": "Lakshita Agarwal and Bindu Verma", "abstract": "  Image description generation is essential for accessibility and AI\nunderstanding of visual content. Recent advancements in deep learning have\nsignificantly improved natural language processing and computer vision. In this\nwork, we propose Tri-FusionNet, a novel image description generation model that\nintegrates transformer modules: a Vision Transformer (ViT) encoder module with\ndual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder\nmodule, and a Contrastive Language-Image Pre-Training (CLIP) integrating\nmodule. The ViT encoder, enhanced with dual attention, focuses on relevant\nspatial regions and linguistic context, improving image feature extraction. The\nRoBERTa decoder is employed to generate precise textual descriptions. CLIP's\nintegrating module aligns visual and textual data through contrastive learning,\nensuring effective combination of both modalities. This fusion of ViT, RoBERTa,\nand CLIP, along with dual attention, enables the model to produce more\naccurate, contextually rich, and flexible descriptions. The proposed framework\ndemonstrated competitive performance on the Flickr30k and Flickr8k datasets,\nwith BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores\nof 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of\n0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores\nof 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results\ndemonstrate the effectiveness of Tri-FusionNet in generating high-quality image\ndescriptions.\n", "link": "http://arxiv.org/abs/2504.16761v1", "date": "2025-04-23", "relevancy": 2.3286, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5878}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5793}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tri-FusionNet%3A%20Enhancing%20Image%20Description%20Generation%20with%0A%20%20Transformer-based%20Fusion%20Network%20and%20Dual%20Attention%20Mechanism&body=Title%3A%20Tri-FusionNet%3A%20Enhancing%20Image%20Description%20Generation%20with%0A%20%20Transformer-based%20Fusion%20Network%20and%20Dual%20Attention%20Mechanism%0AAuthor%3A%20Lakshita%20Agarwal%20and%20Bindu%20Verma%0AAbstract%3A%20%20%20Image%20description%20generation%20is%20essential%20for%20accessibility%20and%20AI%0Aunderstanding%20of%20visual%20content.%20Recent%20advancements%20in%20deep%20learning%20have%0Asignificantly%20improved%20natural%20language%20processing%20and%20computer%20vision.%20In%20this%0Awork%2C%20we%20propose%20Tri-FusionNet%2C%20a%20novel%20image%20description%20generation%20model%20that%0Aintegrates%20transformer%20modules%3A%20a%20Vision%20Transformer%20%28ViT%29%20encoder%20module%20with%0Adual-attention%20mechanism%2C%20a%20Robustly%20Optimized%20BERT%20Approach%20%28RoBERTa%29%20decoder%0Amodule%2C%20and%20a%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20integrating%0Amodule.%20The%20ViT%20encoder%2C%20enhanced%20with%20dual%20attention%2C%20focuses%20on%20relevant%0Aspatial%20regions%20and%20linguistic%20context%2C%20improving%20image%20feature%20extraction.%20The%0ARoBERTa%20decoder%20is%20employed%20to%20generate%20precise%20textual%20descriptions.%20CLIP%27s%0Aintegrating%20module%20aligns%20visual%20and%20textual%20data%20through%20contrastive%20learning%2C%0Aensuring%20effective%20combination%20of%20both%20modalities.%20This%20fusion%20of%20ViT%2C%20RoBERTa%2C%0Aand%20CLIP%2C%20along%20with%20dual%20attention%2C%20enables%20the%20model%20to%20produce%20more%0Aaccurate%2C%20contextually%20rich%2C%20and%20flexible%20descriptions.%20The%20proposed%20framework%0Ademonstrated%20competitive%20performance%20on%20the%20Flickr30k%20and%20Flickr8k%20datasets%2C%0Awith%20BLEU%20scores%20ranging%20from%200.767%20to%200.456%20and%200.784%20to%200.479%2C%20CIDEr%20scores%0Aof%201.679%20and%201.483%2C%20METEOR%20scores%20of%200.478%20and%200.358%2C%20and%20ROUGE-L%20scores%20of%0A0.567%20and%200.789%2C%20respectively.%20On%20MS-COCO%2C%20the%20framework%20obtained%20BLEU%20scores%0Aof%200.893%20%28B-1%29%2C%200.821%20%28B-2%29%2C%200.794%20%28B-3%29%2C%20and%200.725%20%28B-4%29.%20The%20results%0Ademonstrate%20the%20effectiveness%20of%20Tri-FusionNet%20in%20generating%20high-quality%20image%0Adescriptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTri-FusionNet%253A%2520Enhancing%2520Image%2520Description%2520Generation%2520with%250A%2520%2520Transformer-based%2520Fusion%2520Network%2520and%2520Dual%2520Attention%2520Mechanism%26entry.906535625%3DLakshita%2520Agarwal%2520and%2520Bindu%2520Verma%26entry.1292438233%3D%2520%2520Image%2520description%2520generation%2520is%2520essential%2520for%2520accessibility%2520and%2520AI%250Aunderstanding%2520of%2520visual%2520content.%2520Recent%2520advancements%2520in%2520deep%2520learning%2520have%250Asignificantly%2520improved%2520natural%2520language%2520processing%2520and%2520computer%2520vision.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Tri-FusionNet%252C%2520a%2520novel%2520image%2520description%2520generation%2520model%2520that%250Aintegrates%2520transformer%2520modules%253A%2520a%2520Vision%2520Transformer%2520%2528ViT%2529%2520encoder%2520module%2520with%250Adual-attention%2520mechanism%252C%2520a%2520Robustly%2520Optimized%2520BERT%2520Approach%2520%2528RoBERTa%2529%2520decoder%250Amodule%252C%2520and%2520a%2520Contrastive%2520Language-Image%2520Pre-Training%2520%2528CLIP%2529%2520integrating%250Amodule.%2520The%2520ViT%2520encoder%252C%2520enhanced%2520with%2520dual%2520attention%252C%2520focuses%2520on%2520relevant%250Aspatial%2520regions%2520and%2520linguistic%2520context%252C%2520improving%2520image%2520feature%2520extraction.%2520The%250ARoBERTa%2520decoder%2520is%2520employed%2520to%2520generate%2520precise%2520textual%2520descriptions.%2520CLIP%2527s%250Aintegrating%2520module%2520aligns%2520visual%2520and%2520textual%2520data%2520through%2520contrastive%2520learning%252C%250Aensuring%2520effective%2520combination%2520of%2520both%2520modalities.%2520This%2520fusion%2520of%2520ViT%252C%2520RoBERTa%252C%250Aand%2520CLIP%252C%2520along%2520with%2520dual%2520attention%252C%2520enables%2520the%2520model%2520to%2520produce%2520more%250Aaccurate%252C%2520contextually%2520rich%252C%2520and%2520flexible%2520descriptions.%2520The%2520proposed%2520framework%250Ademonstrated%2520competitive%2520performance%2520on%2520the%2520Flickr30k%2520and%2520Flickr8k%2520datasets%252C%250Awith%2520BLEU%2520scores%2520ranging%2520from%25200.767%2520to%25200.456%2520and%25200.784%2520to%25200.479%252C%2520CIDEr%2520scores%250Aof%25201.679%2520and%25201.483%252C%2520METEOR%2520scores%2520of%25200.478%2520and%25200.358%252C%2520and%2520ROUGE-L%2520scores%2520of%250A0.567%2520and%25200.789%252C%2520respectively.%2520On%2520MS-COCO%252C%2520the%2520framework%2520obtained%2520BLEU%2520scores%250Aof%25200.893%2520%2528B-1%2529%252C%25200.821%2520%2528B-2%2529%252C%25200.794%2520%2528B-3%2529%252C%2520and%25200.725%2520%2528B-4%2529.%2520The%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520Tri-FusionNet%2520in%2520generating%2520high-quality%2520image%250Adescriptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tri-FusionNet%3A%20Enhancing%20Image%20Description%20Generation%20with%0A%20%20Transformer-based%20Fusion%20Network%20and%20Dual%20Attention%20Mechanism&entry.906535625=Lakshita%20Agarwal%20and%20Bindu%20Verma&entry.1292438233=%20%20Image%20description%20generation%20is%20essential%20for%20accessibility%20and%20AI%0Aunderstanding%20of%20visual%20content.%20Recent%20advancements%20in%20deep%20learning%20have%0Asignificantly%20improved%20natural%20language%20processing%20and%20computer%20vision.%20In%20this%0Awork%2C%20we%20propose%20Tri-FusionNet%2C%20a%20novel%20image%20description%20generation%20model%20that%0Aintegrates%20transformer%20modules%3A%20a%20Vision%20Transformer%20%28ViT%29%20encoder%20module%20with%0Adual-attention%20mechanism%2C%20a%20Robustly%20Optimized%20BERT%20Approach%20%28RoBERTa%29%20decoder%0Amodule%2C%20and%20a%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20integrating%0Amodule.%20The%20ViT%20encoder%2C%20enhanced%20with%20dual%20attention%2C%20focuses%20on%20relevant%0Aspatial%20regions%20and%20linguistic%20context%2C%20improving%20image%20feature%20extraction.%20The%0ARoBERTa%20decoder%20is%20employed%20to%20generate%20precise%20textual%20descriptions.%20CLIP%27s%0Aintegrating%20module%20aligns%20visual%20and%20textual%20data%20through%20contrastive%20learning%2C%0Aensuring%20effective%20combination%20of%20both%20modalities.%20This%20fusion%20of%20ViT%2C%20RoBERTa%2C%0Aand%20CLIP%2C%20along%20with%20dual%20attention%2C%20enables%20the%20model%20to%20produce%20more%0Aaccurate%2C%20contextually%20rich%2C%20and%20flexible%20descriptions.%20The%20proposed%20framework%0Ademonstrated%20competitive%20performance%20on%20the%20Flickr30k%20and%20Flickr8k%20datasets%2C%0Awith%20BLEU%20scores%20ranging%20from%200.767%20to%200.456%20and%200.784%20to%200.479%2C%20CIDEr%20scores%0Aof%201.679%20and%201.483%2C%20METEOR%20scores%20of%200.478%20and%200.358%2C%20and%20ROUGE-L%20scores%20of%0A0.567%20and%200.789%2C%20respectively.%20On%20MS-COCO%2C%20the%20framework%20obtained%20BLEU%20scores%0Aof%200.893%20%28B-1%29%2C%200.821%20%28B-2%29%2C%200.794%20%28B-3%29%2C%20and%200.725%20%28B-4%29.%20The%20results%0Ademonstrate%20the%20effectiveness%20of%20Tri-FusionNet%20in%20generating%20high-quality%20image%0Adescriptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16761v1&entry.124074799=Read"},
{"title": "Exploring zero-shot structure-based protein fitness prediction", "author": "Arnav Sharma and Anthony Gitter", "abstract": "  The ability to make zero-shot predictions about the fitness consequences of\nprotein sequence changes with pre-trained machine learning models enables many\npractical applications. Such models can be applied for downstream tasks like\ngenetic variant interpretation and protein engineering without additional\nlabeled data. The advent of capable protein structure prediction tools has led\nto the availability of orders of magnitude more precomputed predicted\nstructures, giving rise to powerful structure-based fitness prediction models.\nThrough our experiments, we assess several modeling choices for structure-based\nmodels and their effects on downstream fitness prediction. Zero-shot fitness\nprediction models can struggle to assess the fitness landscape within\ndisordered regions of proteins, those that lack a fixed 3D structure. We\nconfirm the importance of matching protein structures to fitness assays and\nfind that predicted structures for disordered regions can be misleading and\naffect predictive performance. Lastly, we evaluate an additional\nstructure-based model on the ProteinGym substitution benchmark and show that\nsimple multi-modal ensembles are strong baselines.\n", "link": "http://arxiv.org/abs/2504.16886v1", "date": "2025-04-23", "relevancy": 2.3087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4659}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4659}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20zero-shot%20structure-based%20protein%20fitness%20prediction&body=Title%3A%20Exploring%20zero-shot%20structure-based%20protein%20fitness%20prediction%0AAuthor%3A%20Arnav%20Sharma%20and%20Anthony%20Gitter%0AAbstract%3A%20%20%20The%20ability%20to%20make%20zero-shot%20predictions%20about%20the%20fitness%20consequences%20of%0Aprotein%20sequence%20changes%20with%20pre-trained%20machine%20learning%20models%20enables%20many%0Apractical%20applications.%20Such%20models%20can%20be%20applied%20for%20downstream%20tasks%20like%0Agenetic%20variant%20interpretation%20and%20protein%20engineering%20without%20additional%0Alabeled%20data.%20The%20advent%20of%20capable%20protein%20structure%20prediction%20tools%20has%20led%0Ato%20the%20availability%20of%20orders%20of%20magnitude%20more%20precomputed%20predicted%0Astructures%2C%20giving%20rise%20to%20powerful%20structure-based%20fitness%20prediction%20models.%0AThrough%20our%20experiments%2C%20we%20assess%20several%20modeling%20choices%20for%20structure-based%0Amodels%20and%20their%20effects%20on%20downstream%20fitness%20prediction.%20Zero-shot%20fitness%0Aprediction%20models%20can%20struggle%20to%20assess%20the%20fitness%20landscape%20within%0Adisordered%20regions%20of%20proteins%2C%20those%20that%20lack%20a%20fixed%203D%20structure.%20We%0Aconfirm%20the%20importance%20of%20matching%20protein%20structures%20to%20fitness%20assays%20and%0Afind%20that%20predicted%20structures%20for%20disordered%20regions%20can%20be%20misleading%20and%0Aaffect%20predictive%20performance.%20Lastly%2C%20we%20evaluate%20an%20additional%0Astructure-based%20model%20on%20the%20ProteinGym%20substitution%20benchmark%20and%20show%20that%0Asimple%20multi-modal%20ensembles%20are%20strong%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520zero-shot%2520structure-based%2520protein%2520fitness%2520prediction%26entry.906535625%3DArnav%2520Sharma%2520and%2520Anthony%2520Gitter%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520make%2520zero-shot%2520predictions%2520about%2520the%2520fitness%2520consequences%2520of%250Aprotein%2520sequence%2520changes%2520with%2520pre-trained%2520machine%2520learning%2520models%2520enables%2520many%250Apractical%2520applications.%2520Such%2520models%2520can%2520be%2520applied%2520for%2520downstream%2520tasks%2520like%250Agenetic%2520variant%2520interpretation%2520and%2520protein%2520engineering%2520without%2520additional%250Alabeled%2520data.%2520The%2520advent%2520of%2520capable%2520protein%2520structure%2520prediction%2520tools%2520has%2520led%250Ato%2520the%2520availability%2520of%2520orders%2520of%2520magnitude%2520more%2520precomputed%2520predicted%250Astructures%252C%2520giving%2520rise%2520to%2520powerful%2520structure-based%2520fitness%2520prediction%2520models.%250AThrough%2520our%2520experiments%252C%2520we%2520assess%2520several%2520modeling%2520choices%2520for%2520structure-based%250Amodels%2520and%2520their%2520effects%2520on%2520downstream%2520fitness%2520prediction.%2520Zero-shot%2520fitness%250Aprediction%2520models%2520can%2520struggle%2520to%2520assess%2520the%2520fitness%2520landscape%2520within%250Adisordered%2520regions%2520of%2520proteins%252C%2520those%2520that%2520lack%2520a%2520fixed%25203D%2520structure.%2520We%250Aconfirm%2520the%2520importance%2520of%2520matching%2520protein%2520structures%2520to%2520fitness%2520assays%2520and%250Afind%2520that%2520predicted%2520structures%2520for%2520disordered%2520regions%2520can%2520be%2520misleading%2520and%250Aaffect%2520predictive%2520performance.%2520Lastly%252C%2520we%2520evaluate%2520an%2520additional%250Astructure-based%2520model%2520on%2520the%2520ProteinGym%2520substitution%2520benchmark%2520and%2520show%2520that%250Asimple%2520multi-modal%2520ensembles%2520are%2520strong%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20zero-shot%20structure-based%20protein%20fitness%20prediction&entry.906535625=Arnav%20Sharma%20and%20Anthony%20Gitter&entry.1292438233=%20%20The%20ability%20to%20make%20zero-shot%20predictions%20about%20the%20fitness%20consequences%20of%0Aprotein%20sequence%20changes%20with%20pre-trained%20machine%20learning%20models%20enables%20many%0Apractical%20applications.%20Such%20models%20can%20be%20applied%20for%20downstream%20tasks%20like%0Agenetic%20variant%20interpretation%20and%20protein%20engineering%20without%20additional%0Alabeled%20data.%20The%20advent%20of%20capable%20protein%20structure%20prediction%20tools%20has%20led%0Ato%20the%20availability%20of%20orders%20of%20magnitude%20more%20precomputed%20predicted%0Astructures%2C%20giving%20rise%20to%20powerful%20structure-based%20fitness%20prediction%20models.%0AThrough%20our%20experiments%2C%20we%20assess%20several%20modeling%20choices%20for%20structure-based%0Amodels%20and%20their%20effects%20on%20downstream%20fitness%20prediction.%20Zero-shot%20fitness%0Aprediction%20models%20can%20struggle%20to%20assess%20the%20fitness%20landscape%20within%0Adisordered%20regions%20of%20proteins%2C%20those%20that%20lack%20a%20fixed%203D%20structure.%20We%0Aconfirm%20the%20importance%20of%20matching%20protein%20structures%20to%20fitness%20assays%20and%0Afind%20that%20predicted%20structures%20for%20disordered%20regions%20can%20be%20misleading%20and%0Aaffect%20predictive%20performance.%20Lastly%2C%20we%20evaluate%20an%20additional%0Astructure-based%20model%20on%20the%20ProteinGym%20substitution%20benchmark%20and%20show%20that%0Asimple%20multi-modal%20ensembles%20are%20strong%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16886v1&entry.124074799=Read"},
{"title": "A Survey of AI Agent Protocols", "author": "Yingxuan Yang and Huacan Chai and Yuanyi Song and Siyuan Qi and Muning Wen and Ning Li and Junwei Liao and Haoyi Hu and Jianghao Lin and Gaowei Chang and Weiwen Liu and Ying Wen and Yong Yu and Weinan Zhang", "abstract": "  The rapid development of large language models (LLMs) has led to the\nwidespread deployment of LLM agents across diverse industries, including\ncustomer service, content generation, data analysis, and even healthcare.\nHowever, as more LLM agents are deployed, a major issue has emerged: there is\nno standard way for these agents to communicate with external tools or data\nsources. This lack of standardized protocols makes it difficult for agents to\nwork together or scale effectively, and it limits their ability to tackle\ncomplex, real-world tasks. A unified communication protocol for LLM agents\ncould change this. It would allow agents and tools to interact more smoothly,\nencourage collaboration, and triggering the formation of collective\nintelligence. In this paper, we provide a systematic overview of existing\ncommunication protocols for LLM agents. We classify them into four main\ncategories and make an analysis to help users and developers select the most\nsuitable protocols for specific applications. Additionally, we conduct a\ncomparative performance analysis of these protocols across key dimensions such\nas security, scalability, and latency. Finally, we explore future challenges,\nsuch as how protocols can adapt and survive in fast-evolving environments, and\nwhat qualities future protocols might need to support the next generation of\nLLM agent ecosystems. We expect this work to serve as a practical reference for\nboth researchers and engineers seeking to design, evaluate, or integrate robust\ncommunication infrastructures for intelligent agents.\n", "link": "http://arxiv.org/abs/2504.16736v1", "date": "2025-04-23", "relevancy": 2.304, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4791}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20AI%20Agent%20Protocols&body=Title%3A%20A%20Survey%20of%20AI%20Agent%20Protocols%0AAuthor%3A%20Yingxuan%20Yang%20and%20Huacan%20Chai%20and%20Yuanyi%20Song%20and%20Siyuan%20Qi%20and%20Muning%20Wen%20and%20Ning%20Li%20and%20Junwei%20Liao%20and%20Haoyi%20Hu%20and%20Jianghao%20Lin%20and%20Gaowei%20Chang%20and%20Weiwen%20Liu%20and%20Ying%20Wen%20and%20Yong%20Yu%20and%20Weinan%20Zhang%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%20led%20to%20the%0Awidespread%20deployment%20of%20LLM%20agents%20across%20diverse%20industries%2C%20including%0Acustomer%20service%2C%20content%20generation%2C%20data%20analysis%2C%20and%20even%20healthcare.%0AHowever%2C%20as%20more%20LLM%20agents%20are%20deployed%2C%20a%20major%20issue%20has%20emerged%3A%20there%20is%0Ano%20standard%20way%20for%20these%20agents%20to%20communicate%20with%20external%20tools%20or%20data%0Asources.%20This%20lack%20of%20standardized%20protocols%20makes%20it%20difficult%20for%20agents%20to%0Awork%20together%20or%20scale%20effectively%2C%20and%20it%20limits%20their%20ability%20to%20tackle%0Acomplex%2C%20real-world%20tasks.%20A%20unified%20communication%20protocol%20for%20LLM%20agents%0Acould%20change%20this.%20It%20would%20allow%20agents%20and%20tools%20to%20interact%20more%20smoothly%2C%0Aencourage%20collaboration%2C%20and%20triggering%20the%20formation%20of%20collective%0Aintelligence.%20In%20this%20paper%2C%20we%20provide%20a%20systematic%20overview%20of%20existing%0Acommunication%20protocols%20for%20LLM%20agents.%20We%20classify%20them%20into%20four%20main%0Acategories%20and%20make%20an%20analysis%20to%20help%20users%20and%20developers%20select%20the%20most%0Asuitable%20protocols%20for%20specific%20applications.%20Additionally%2C%20we%20conduct%20a%0Acomparative%20performance%20analysis%20of%20these%20protocols%20across%20key%20dimensions%20such%0Aas%20security%2C%20scalability%2C%20and%20latency.%20Finally%2C%20we%20explore%20future%20challenges%2C%0Asuch%20as%20how%20protocols%20can%20adapt%20and%20survive%20in%20fast-evolving%20environments%2C%20and%0Awhat%20qualities%20future%20protocols%20might%20need%20to%20support%20the%20next%20generation%20of%0ALLM%20agent%20ecosystems.%20We%20expect%20this%20work%20to%20serve%20as%20a%20practical%20reference%20for%0Aboth%20researchers%20and%20engineers%20seeking%20to%20design%2C%20evaluate%2C%20or%20integrate%20robust%0Acommunication%20infrastructures%20for%20intelligent%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520AI%2520Agent%2520Protocols%26entry.906535625%3DYingxuan%2520Yang%2520and%2520Huacan%2520Chai%2520and%2520Yuanyi%2520Song%2520and%2520Siyuan%2520Qi%2520and%2520Muning%2520Wen%2520and%2520Ning%2520Li%2520and%2520Junwei%2520Liao%2520and%2520Haoyi%2520Hu%2520and%2520Jianghao%2520Lin%2520and%2520Gaowei%2520Chang%2520and%2520Weiwen%2520Liu%2520and%2520Ying%2520Wen%2520and%2520Yong%2520Yu%2520and%2520Weinan%2520Zhang%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520led%2520to%2520the%250Awidespread%2520deployment%2520of%2520LLM%2520agents%2520across%2520diverse%2520industries%252C%2520including%250Acustomer%2520service%252C%2520content%2520generation%252C%2520data%2520analysis%252C%2520and%2520even%2520healthcare.%250AHowever%252C%2520as%2520more%2520LLM%2520agents%2520are%2520deployed%252C%2520a%2520major%2520issue%2520has%2520emerged%253A%2520there%2520is%250Ano%2520standard%2520way%2520for%2520these%2520agents%2520to%2520communicate%2520with%2520external%2520tools%2520or%2520data%250Asources.%2520This%2520lack%2520of%2520standardized%2520protocols%2520makes%2520it%2520difficult%2520for%2520agents%2520to%250Awork%2520together%2520or%2520scale%2520effectively%252C%2520and%2520it%2520limits%2520their%2520ability%2520to%2520tackle%250Acomplex%252C%2520real-world%2520tasks.%2520A%2520unified%2520communication%2520protocol%2520for%2520LLM%2520agents%250Acould%2520change%2520this.%2520It%2520would%2520allow%2520agents%2520and%2520tools%2520to%2520interact%2520more%2520smoothly%252C%250Aencourage%2520collaboration%252C%2520and%2520triggering%2520the%2520formation%2520of%2520collective%250Aintelligence.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%2520systematic%2520overview%2520of%2520existing%250Acommunication%2520protocols%2520for%2520LLM%2520agents.%2520We%2520classify%2520them%2520into%2520four%2520main%250Acategories%2520and%2520make%2520an%2520analysis%2520to%2520help%2520users%2520and%2520developers%2520select%2520the%2520most%250Asuitable%2520protocols%2520for%2520specific%2520applications.%2520Additionally%252C%2520we%2520conduct%2520a%250Acomparative%2520performance%2520analysis%2520of%2520these%2520protocols%2520across%2520key%2520dimensions%2520such%250Aas%2520security%252C%2520scalability%252C%2520and%2520latency.%2520Finally%252C%2520we%2520explore%2520future%2520challenges%252C%250Asuch%2520as%2520how%2520protocols%2520can%2520adapt%2520and%2520survive%2520in%2520fast-evolving%2520environments%252C%2520and%250Awhat%2520qualities%2520future%2520protocols%2520might%2520need%2520to%2520support%2520the%2520next%2520generation%2520of%250ALLM%2520agent%2520ecosystems.%2520We%2520expect%2520this%2520work%2520to%2520serve%2520as%2520a%2520practical%2520reference%2520for%250Aboth%2520researchers%2520and%2520engineers%2520seeking%2520to%2520design%252C%2520evaluate%252C%2520or%2520integrate%2520robust%250Acommunication%2520infrastructures%2520for%2520intelligent%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20AI%20Agent%20Protocols&entry.906535625=Yingxuan%20Yang%20and%20Huacan%20Chai%20and%20Yuanyi%20Song%20and%20Siyuan%20Qi%20and%20Muning%20Wen%20and%20Ning%20Li%20and%20Junwei%20Liao%20and%20Haoyi%20Hu%20and%20Jianghao%20Lin%20and%20Gaowei%20Chang%20and%20Weiwen%20Liu%20and%20Ying%20Wen%20and%20Yong%20Yu%20and%20Weinan%20Zhang&entry.1292438233=%20%20The%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%20led%20to%20the%0Awidespread%20deployment%20of%20LLM%20agents%20across%20diverse%20industries%2C%20including%0Acustomer%20service%2C%20content%20generation%2C%20data%20analysis%2C%20and%20even%20healthcare.%0AHowever%2C%20as%20more%20LLM%20agents%20are%20deployed%2C%20a%20major%20issue%20has%20emerged%3A%20there%20is%0Ano%20standard%20way%20for%20these%20agents%20to%20communicate%20with%20external%20tools%20or%20data%0Asources.%20This%20lack%20of%20standardized%20protocols%20makes%20it%20difficult%20for%20agents%20to%0Awork%20together%20or%20scale%20effectively%2C%20and%20it%20limits%20their%20ability%20to%20tackle%0Acomplex%2C%20real-world%20tasks.%20A%20unified%20communication%20protocol%20for%20LLM%20agents%0Acould%20change%20this.%20It%20would%20allow%20agents%20and%20tools%20to%20interact%20more%20smoothly%2C%0Aencourage%20collaboration%2C%20and%20triggering%20the%20formation%20of%20collective%0Aintelligence.%20In%20this%20paper%2C%20we%20provide%20a%20systematic%20overview%20of%20existing%0Acommunication%20protocols%20for%20LLM%20agents.%20We%20classify%20them%20into%20four%20main%0Acategories%20and%20make%20an%20analysis%20to%20help%20users%20and%20developers%20select%20the%20most%0Asuitable%20protocols%20for%20specific%20applications.%20Additionally%2C%20we%20conduct%20a%0Acomparative%20performance%20analysis%20of%20these%20protocols%20across%20key%20dimensions%20such%0Aas%20security%2C%20scalability%2C%20and%20latency.%20Finally%2C%20we%20explore%20future%20challenges%2C%0Asuch%20as%20how%20protocols%20can%20adapt%20and%20survive%20in%20fast-evolving%20environments%2C%20and%0Awhat%20qualities%20future%20protocols%20might%20need%20to%20support%20the%20next%20generation%20of%0ALLM%20agent%20ecosystems.%20We%20expect%20this%20work%20to%20serve%20as%20a%20practical%20reference%20for%0Aboth%20researchers%20and%20engineers%20seeking%20to%20design%2C%20evaluate%2C%20or%20integrate%20robust%0Acommunication%20infrastructures%20for%20intelligent%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16736v1&entry.124074799=Read"},
{"title": "Latent Diffusion Planning for Imitation Learning", "author": "Amber Xie and Oleh Rybkin and Dorsa Sadigh and Chelsea Finn", "abstract": "  Recent progress in imitation learning has been enabled by policy\narchitectures that scale to complex visuomotor tasks, multimodal distributions,\nand large datasets. However, these methods often rely on learning from large\namount of expert demonstrations. To address these shortcomings, we propose\nLatent Diffusion Planning (LDP), a modular approach consisting of a planner\nwhich can leverage action-free demonstrations, and an inverse dynamics model\nwhich can leverage suboptimal data, that both operate over a learned latent\nspace. First, we learn a compact latent space through a variational\nautoencoder, enabling effective forecasting of future states in image-based\ndomains. Then, we train a planner and an inverse dynamics model with diffusion\nobjectives. By separating planning from action prediction, LDP can benefit from\nthe denser supervision signals of suboptimal and action-free data. On simulated\nvisual robotic manipulation tasks, LDP outperforms state-of-the-art imitation\nlearning approaches, as they cannot leverage such additional data.\n", "link": "http://arxiv.org/abs/2504.16925v1", "date": "2025-04-23", "relevancy": 2.2966, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6045}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5813}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Diffusion%20Planning%20for%20Imitation%20Learning&body=Title%3A%20Latent%20Diffusion%20Planning%20for%20Imitation%20Learning%0AAuthor%3A%20Amber%20Xie%20and%20Oleh%20Rybkin%20and%20Dorsa%20Sadigh%20and%20Chelsea%20Finn%0AAbstract%3A%20%20%20Recent%20progress%20in%20imitation%20learning%20has%20been%20enabled%20by%20policy%0Aarchitectures%20that%20scale%20to%20complex%20visuomotor%20tasks%2C%20multimodal%20distributions%2C%0Aand%20large%20datasets.%20However%2C%20these%20methods%20often%20rely%20on%20learning%20from%20large%0Aamount%20of%20expert%20demonstrations.%20To%20address%20these%20shortcomings%2C%20we%20propose%0ALatent%20Diffusion%20Planning%20%28LDP%29%2C%20a%20modular%20approach%20consisting%20of%20a%20planner%0Awhich%20can%20leverage%20action-free%20demonstrations%2C%20and%20an%20inverse%20dynamics%20model%0Awhich%20can%20leverage%20suboptimal%20data%2C%20that%20both%20operate%20over%20a%20learned%20latent%0Aspace.%20First%2C%20we%20learn%20a%20compact%20latent%20space%20through%20a%20variational%0Aautoencoder%2C%20enabling%20effective%20forecasting%20of%20future%20states%20in%20image-based%0Adomains.%20Then%2C%20we%20train%20a%20planner%20and%20an%20inverse%20dynamics%20model%20with%20diffusion%0Aobjectives.%20By%20separating%20planning%20from%20action%20prediction%2C%20LDP%20can%20benefit%20from%0Athe%20denser%20supervision%20signals%20of%20suboptimal%20and%20action-free%20data.%20On%20simulated%0Avisual%20robotic%20manipulation%20tasks%2C%20LDP%20outperforms%20state-of-the-art%20imitation%0Alearning%20approaches%2C%20as%20they%20cannot%20leverage%20such%20additional%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Diffusion%2520Planning%2520for%2520Imitation%2520Learning%26entry.906535625%3DAmber%2520Xie%2520and%2520Oleh%2520Rybkin%2520and%2520Dorsa%2520Sadigh%2520and%2520Chelsea%2520Finn%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520imitation%2520learning%2520has%2520been%2520enabled%2520by%2520policy%250Aarchitectures%2520that%2520scale%2520to%2520complex%2520visuomotor%2520tasks%252C%2520multimodal%2520distributions%252C%250Aand%2520large%2520datasets.%2520However%252C%2520these%2520methods%2520often%2520rely%2520on%2520learning%2520from%2520large%250Aamount%2520of%2520expert%2520demonstrations.%2520To%2520address%2520these%2520shortcomings%252C%2520we%2520propose%250ALatent%2520Diffusion%2520Planning%2520%2528LDP%2529%252C%2520a%2520modular%2520approach%2520consisting%2520of%2520a%2520planner%250Awhich%2520can%2520leverage%2520action-free%2520demonstrations%252C%2520and%2520an%2520inverse%2520dynamics%2520model%250Awhich%2520can%2520leverage%2520suboptimal%2520data%252C%2520that%2520both%2520operate%2520over%2520a%2520learned%2520latent%250Aspace.%2520First%252C%2520we%2520learn%2520a%2520compact%2520latent%2520space%2520through%2520a%2520variational%250Aautoencoder%252C%2520enabling%2520effective%2520forecasting%2520of%2520future%2520states%2520in%2520image-based%250Adomains.%2520Then%252C%2520we%2520train%2520a%2520planner%2520and%2520an%2520inverse%2520dynamics%2520model%2520with%2520diffusion%250Aobjectives.%2520By%2520separating%2520planning%2520from%2520action%2520prediction%252C%2520LDP%2520can%2520benefit%2520from%250Athe%2520denser%2520supervision%2520signals%2520of%2520suboptimal%2520and%2520action-free%2520data.%2520On%2520simulated%250Avisual%2520robotic%2520manipulation%2520tasks%252C%2520LDP%2520outperforms%2520state-of-the-art%2520imitation%250Alearning%2520approaches%252C%2520as%2520they%2520cannot%2520leverage%2520such%2520additional%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Diffusion%20Planning%20for%20Imitation%20Learning&entry.906535625=Amber%20Xie%20and%20Oleh%20Rybkin%20and%20Dorsa%20Sadigh%20and%20Chelsea%20Finn&entry.1292438233=%20%20Recent%20progress%20in%20imitation%20learning%20has%20been%20enabled%20by%20policy%0Aarchitectures%20that%20scale%20to%20complex%20visuomotor%20tasks%2C%20multimodal%20distributions%2C%0Aand%20large%20datasets.%20However%2C%20these%20methods%20often%20rely%20on%20learning%20from%20large%0Aamount%20of%20expert%20demonstrations.%20To%20address%20these%20shortcomings%2C%20we%20propose%0ALatent%20Diffusion%20Planning%20%28LDP%29%2C%20a%20modular%20approach%20consisting%20of%20a%20planner%0Awhich%20can%20leverage%20action-free%20demonstrations%2C%20and%20an%20inverse%20dynamics%20model%0Awhich%20can%20leverage%20suboptimal%20data%2C%20that%20both%20operate%20over%20a%20learned%20latent%0Aspace.%20First%2C%20we%20learn%20a%20compact%20latent%20space%20through%20a%20variational%0Aautoencoder%2C%20enabling%20effective%20forecasting%20of%20future%20states%20in%20image-based%0Adomains.%20Then%2C%20we%20train%20a%20planner%20and%20an%20inverse%20dynamics%20model%20with%20diffusion%0Aobjectives.%20By%20separating%20planning%20from%20action%20prediction%2C%20LDP%20can%20benefit%20from%0Athe%20denser%20supervision%20signals%20of%20suboptimal%20and%20action-free%20data.%20On%20simulated%0Avisual%20robotic%20manipulation%20tasks%2C%20LDP%20outperforms%20state-of-the-art%20imitation%0Alearning%20approaches%2C%20as%20they%20cannot%20leverage%20such%20additional%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16925v1&entry.124074799=Read"},
{"title": "MorphoNavi: Aerial-Ground Robot Navigation with Object Oriented Mapping\n  in Digital Twin", "author": "Sausar Karaf and Mikhail Martynov and Oleg Sautenkov and Zhanibek Darush and Dzmitry Tsetserukou", "abstract": "  This paper presents a novel mapping approach for a universal aerial-ground\nrobotic system utilizing a single monocular camera. The proposed system is\ncapable of detecting a diverse range of objects and estimating their positions\nwithout requiring fine-tuning for specific environments. The system's\nperformance was evaluated through a simulated search-and-rescue scenario, where\nthe MorphoGear robot successfully located a robotic dog while an operator\nmonitored the process. This work contributes to the development of intelligent,\nmultimodal robotic systems capable of operating in unstructured environments.\n", "link": "http://arxiv.org/abs/2504.16914v1", "date": "2025-04-23", "relevancy": 2.2867, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5957}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5847}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MorphoNavi%3A%20Aerial-Ground%20Robot%20Navigation%20with%20Object%20Oriented%20Mapping%0A%20%20in%20Digital%20Twin&body=Title%3A%20MorphoNavi%3A%20Aerial-Ground%20Robot%20Navigation%20with%20Object%20Oriented%20Mapping%0A%20%20in%20Digital%20Twin%0AAuthor%3A%20Sausar%20Karaf%20and%20Mikhail%20Martynov%20and%20Oleg%20Sautenkov%20and%20Zhanibek%20Darush%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20mapping%20approach%20for%20a%20universal%20aerial-ground%0Arobotic%20system%20utilizing%20a%20single%20monocular%20camera.%20The%20proposed%20system%20is%0Acapable%20of%20detecting%20a%20diverse%20range%20of%20objects%20and%20estimating%20their%20positions%0Awithout%20requiring%20fine-tuning%20for%20specific%20environments.%20The%20system%27s%0Aperformance%20was%20evaluated%20through%20a%20simulated%20search-and-rescue%20scenario%2C%20where%0Athe%20MorphoGear%20robot%20successfully%20located%20a%20robotic%20dog%20while%20an%20operator%0Amonitored%20the%20process.%20This%20work%20contributes%20to%20the%20development%20of%20intelligent%2C%0Amultimodal%20robotic%20systems%20capable%20of%20operating%20in%20unstructured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphoNavi%253A%2520Aerial-Ground%2520Robot%2520Navigation%2520with%2520Object%2520Oriented%2520Mapping%250A%2520%2520in%2520Digital%2520Twin%26entry.906535625%3DSausar%2520Karaf%2520and%2520Mikhail%2520Martynov%2520and%2520Oleg%2520Sautenkov%2520and%2520Zhanibek%2520Darush%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520mapping%2520approach%2520for%2520a%2520universal%2520aerial-ground%250Arobotic%2520system%2520utilizing%2520a%2520single%2520monocular%2520camera.%2520The%2520proposed%2520system%2520is%250Acapable%2520of%2520detecting%2520a%2520diverse%2520range%2520of%2520objects%2520and%2520estimating%2520their%2520positions%250Awithout%2520requiring%2520fine-tuning%2520for%2520specific%2520environments.%2520The%2520system%2527s%250Aperformance%2520was%2520evaluated%2520through%2520a%2520simulated%2520search-and-rescue%2520scenario%252C%2520where%250Athe%2520MorphoGear%2520robot%2520successfully%2520located%2520a%2520robotic%2520dog%2520while%2520an%2520operator%250Amonitored%2520the%2520process.%2520This%2520work%2520contributes%2520to%2520the%2520development%2520of%2520intelligent%252C%250Amultimodal%2520robotic%2520systems%2520capable%2520of%2520operating%2520in%2520unstructured%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MorphoNavi%3A%20Aerial-Ground%20Robot%20Navigation%20with%20Object%20Oriented%20Mapping%0A%20%20in%20Digital%20Twin&entry.906535625=Sausar%20Karaf%20and%20Mikhail%20Martynov%20and%20Oleg%20Sautenkov%20and%20Zhanibek%20Darush%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20mapping%20approach%20for%20a%20universal%20aerial-ground%0Arobotic%20system%20utilizing%20a%20single%20monocular%20camera.%20The%20proposed%20system%20is%0Acapable%20of%20detecting%20a%20diverse%20range%20of%20objects%20and%20estimating%20their%20positions%0Awithout%20requiring%20fine-tuning%20for%20specific%20environments.%20The%20system%27s%0Aperformance%20was%20evaluated%20through%20a%20simulated%20search-and-rescue%20scenario%2C%20where%0Athe%20MorphoGear%20robot%20successfully%20located%20a%20robotic%20dog%20while%20an%20operator%0Amonitored%20the%20process.%20This%20work%20contributes%20to%20the%20development%20of%20intelligent%2C%0Amultimodal%20robotic%20systems%20capable%20of%20operating%20in%20unstructured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16914v1&entry.124074799=Read"},
{"title": "4D Multimodal Co-attention Fusion Network with Latent Contrastive\n  Alignment for Alzheimer's Diagnosis", "author": "Yuxiang Wei and Yanteng Zhang and Xi Xiao and Tianyang Wang and Xiao Wang and Vince D. Calhoun", "abstract": "  Multimodal neuroimaging provides complementary structural and functional\ninsights into both human brain organization and disease-related dynamics.\nRecent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's\ndisease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,\nfMRI) with behavioral cognitive scores tabular data biomarkers. However, the\nintrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI\ndynamics vs. 3D anatomical sMRI structure) presents critical challenges for\ndiscriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a\ngeometry-aware multimodal co-attention network with latent alignment for early\nAD diagnosis using sMRI and fMRI. At the core of our approach is a\nmulti-patch-to-multi-patch (M2M) contrastive loss function that quantifies and\nreduces representational discrepancies via geometry-weighted patch\ncorrespondence, explicitly aligning fMRI components across brain regions with\ntheir sMRI structural substrates without one-to-one constraints. Additionally,\nwe propose a latent-as-query co-attention module to autonomously discover\nfusion patterns, circumventing modality prioritization biases while minimizing\nfeature redundancy. We conduct extensive experiments to confirm the\neffectiveness of our method and highlight the correspondance between fMRI and\nsMRI as AD biomarkers.\n", "link": "http://arxiv.org/abs/2504.16798v1", "date": "2025-04-23", "relevancy": 2.2659, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5927}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5563}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D%20Multimodal%20Co-attention%20Fusion%20Network%20with%20Latent%20Contrastive%0A%20%20Alignment%20for%20Alzheimer%27s%20Diagnosis&body=Title%3A%204D%20Multimodal%20Co-attention%20Fusion%20Network%20with%20Latent%20Contrastive%0A%20%20Alignment%20for%20Alzheimer%27s%20Diagnosis%0AAuthor%3A%20Yuxiang%20Wei%20and%20Yanteng%20Zhang%20and%20Xi%20Xiao%20and%20Tianyang%20Wang%20and%20Xiao%20Wang%20and%20Vince%20D.%20Calhoun%0AAbstract%3A%20%20%20Multimodal%20neuroimaging%20provides%20complementary%20structural%20and%20functional%0Ainsights%20into%20both%20human%20brain%20organization%20and%20disease-related%20dynamics.%0ARecent%20studies%20demonstrate%20enhanced%20diagnostic%20sensitivity%20for%20Alzheimer%27s%0Adisease%20%28AD%29%20through%20synergistic%20integration%20of%20neuroimaging%20data%20%28e.g.%2C%20sMRI%2C%0AfMRI%29%20with%20behavioral%20cognitive%20scores%20tabular%20data%20biomarkers.%20However%2C%20the%0Aintrinsic%20heterogeneity%20across%20modalities%20%28e.g.%2C%204D%20spatiotemporal%20fMRI%0Adynamics%20vs.%203D%20anatomical%20sMRI%20structure%29%20presents%20critical%20challenges%20for%0Adiscriminative%20feature%20fusion.%20To%20bridge%20this%20gap%2C%20we%20propose%20M2M-AlignNet%3A%20a%0Ageometry-aware%20multimodal%20co-attention%20network%20with%20latent%20alignment%20for%20early%0AAD%20diagnosis%20using%20sMRI%20and%20fMRI.%20At%20the%20core%20of%20our%20approach%20is%20a%0Amulti-patch-to-multi-patch%20%28M2M%29%20contrastive%20loss%20function%20that%20quantifies%20and%0Areduces%20representational%20discrepancies%20via%20geometry-weighted%20patch%0Acorrespondence%2C%20explicitly%20aligning%20fMRI%20components%20across%20brain%20regions%20with%0Atheir%20sMRI%20structural%20substrates%20without%20one-to-one%20constraints.%20Additionally%2C%0Awe%20propose%20a%20latent-as-query%20co-attention%20module%20to%20autonomously%20discover%0Afusion%20patterns%2C%20circumventing%20modality%20prioritization%20biases%20while%20minimizing%0Afeature%20redundancy.%20We%20conduct%20extensive%20experiments%20to%20confirm%20the%0Aeffectiveness%20of%20our%20method%20and%20highlight%20the%20correspondance%20between%20fMRI%20and%0AsMRI%20as%20AD%20biomarkers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D%2520Multimodal%2520Co-attention%2520Fusion%2520Network%2520with%2520Latent%2520Contrastive%250A%2520%2520Alignment%2520for%2520Alzheimer%2527s%2520Diagnosis%26entry.906535625%3DYuxiang%2520Wei%2520and%2520Yanteng%2520Zhang%2520and%2520Xi%2520Xiao%2520and%2520Tianyang%2520Wang%2520and%2520Xiao%2520Wang%2520and%2520Vince%2520D.%2520Calhoun%26entry.1292438233%3D%2520%2520Multimodal%2520neuroimaging%2520provides%2520complementary%2520structural%2520and%2520functional%250Ainsights%2520into%2520both%2520human%2520brain%2520organization%2520and%2520disease-related%2520dynamics.%250ARecent%2520studies%2520demonstrate%2520enhanced%2520diagnostic%2520sensitivity%2520for%2520Alzheimer%2527s%250Adisease%2520%2528AD%2529%2520through%2520synergistic%2520integration%2520of%2520neuroimaging%2520data%2520%2528e.g.%252C%2520sMRI%252C%250AfMRI%2529%2520with%2520behavioral%2520cognitive%2520scores%2520tabular%2520data%2520biomarkers.%2520However%252C%2520the%250Aintrinsic%2520heterogeneity%2520across%2520modalities%2520%2528e.g.%252C%25204D%2520spatiotemporal%2520fMRI%250Adynamics%2520vs.%25203D%2520anatomical%2520sMRI%2520structure%2529%2520presents%2520critical%2520challenges%2520for%250Adiscriminative%2520feature%2520fusion.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520M2M-AlignNet%253A%2520a%250Ageometry-aware%2520multimodal%2520co-attention%2520network%2520with%2520latent%2520alignment%2520for%2520early%250AAD%2520diagnosis%2520using%2520sMRI%2520and%2520fMRI.%2520At%2520the%2520core%2520of%2520our%2520approach%2520is%2520a%250Amulti-patch-to-multi-patch%2520%2528M2M%2529%2520contrastive%2520loss%2520function%2520that%2520quantifies%2520and%250Areduces%2520representational%2520discrepancies%2520via%2520geometry-weighted%2520patch%250Acorrespondence%252C%2520explicitly%2520aligning%2520fMRI%2520components%2520across%2520brain%2520regions%2520with%250Atheir%2520sMRI%2520structural%2520substrates%2520without%2520one-to-one%2520constraints.%2520Additionally%252C%250Awe%2520propose%2520a%2520latent-as-query%2520co-attention%2520module%2520to%2520autonomously%2520discover%250Afusion%2520patterns%252C%2520circumventing%2520modality%2520prioritization%2520biases%2520while%2520minimizing%250Afeature%2520redundancy.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520confirm%2520the%250Aeffectiveness%2520of%2520our%2520method%2520and%2520highlight%2520the%2520correspondance%2520between%2520fMRI%2520and%250AsMRI%2520as%2520AD%2520biomarkers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D%20Multimodal%20Co-attention%20Fusion%20Network%20with%20Latent%20Contrastive%0A%20%20Alignment%20for%20Alzheimer%27s%20Diagnosis&entry.906535625=Yuxiang%20Wei%20and%20Yanteng%20Zhang%20and%20Xi%20Xiao%20and%20Tianyang%20Wang%20and%20Xiao%20Wang%20and%20Vince%20D.%20Calhoun&entry.1292438233=%20%20Multimodal%20neuroimaging%20provides%20complementary%20structural%20and%20functional%0Ainsights%20into%20both%20human%20brain%20organization%20and%20disease-related%20dynamics.%0ARecent%20studies%20demonstrate%20enhanced%20diagnostic%20sensitivity%20for%20Alzheimer%27s%0Adisease%20%28AD%29%20through%20synergistic%20integration%20of%20neuroimaging%20data%20%28e.g.%2C%20sMRI%2C%0AfMRI%29%20with%20behavioral%20cognitive%20scores%20tabular%20data%20biomarkers.%20However%2C%20the%0Aintrinsic%20heterogeneity%20across%20modalities%20%28e.g.%2C%204D%20spatiotemporal%20fMRI%0Adynamics%20vs.%203D%20anatomical%20sMRI%20structure%29%20presents%20critical%20challenges%20for%0Adiscriminative%20feature%20fusion.%20To%20bridge%20this%20gap%2C%20we%20propose%20M2M-AlignNet%3A%20a%0Ageometry-aware%20multimodal%20co-attention%20network%20with%20latent%20alignment%20for%20early%0AAD%20diagnosis%20using%20sMRI%20and%20fMRI.%20At%20the%20core%20of%20our%20approach%20is%20a%0Amulti-patch-to-multi-patch%20%28M2M%29%20contrastive%20loss%20function%20that%20quantifies%20and%0Areduces%20representational%20discrepancies%20via%20geometry-weighted%20patch%0Acorrespondence%2C%20explicitly%20aligning%20fMRI%20components%20across%20brain%20regions%20with%0Atheir%20sMRI%20structural%20substrates%20without%20one-to-one%20constraints.%20Additionally%2C%0Awe%20propose%20a%20latent-as-query%20co-attention%20module%20to%20autonomously%20discover%0Afusion%20patterns%2C%20circumventing%20modality%20prioritization%20biases%20while%20minimizing%0Afeature%20redundancy.%20We%20conduct%20extensive%20experiments%20to%20confirm%20the%0Aeffectiveness%20of%20our%20method%20and%20highlight%20the%20correspondance%20between%20fMRI%20and%0AsMRI%20as%20AD%20biomarkers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16798v1&entry.124074799=Read"},
{"title": "MediSee: Reasoning-based Pixel-level Perception in Medical Images", "author": "Qinyue Tong and Ziqian Lu and Jun Liu and Yangming Zheng and Zheming Lu", "abstract": "  Despite remarkable advancements in pixel-level medical image perception,\nexisting methods are either limited to specific tasks or heavily rely on\naccurate bounding boxes or text labels as input prompts. However, the medical\nknowledge required for input is a huge obstacle for general public, which\ngreatly reduces the universality of these methods. Compared with these\ndomain-specialized auxiliary information, general users tend to rely on oral\nqueries that require logical reasoning. In this paper, we introduce a novel\nmedical vision task: Medical Reasoning Segmentation and Detection (MedSD),\nwhich aims to comprehend implicit queries about medical images and generate the\ncorresponding segmentation mask and bounding box for the target object. To\naccomplish this task, we first introduce a Multi-perspective, Logic-driven\nMedical Reasoning Segmentation and Detection (MLMR-SD) dataset, which\nencompasses a substantial collection of medical entity targets along with their\ncorresponding reasoning. Furthermore, we propose MediSee, an effective baseline\nmodel designed for medical reasoning segmentation and detection. The\nexperimental results indicate that the proposed method can effectively address\nMedSD with implicit colloquial queries and outperform traditional medical\nreferring segmentation methods.\n", "link": "http://arxiv.org/abs/2504.11008v2", "date": "2025-04-23", "relevancy": 2.2507, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MediSee%3A%20Reasoning-based%20Pixel-level%20Perception%20in%20Medical%20Images&body=Title%3A%20MediSee%3A%20Reasoning-based%20Pixel-level%20Perception%20in%20Medical%20Images%0AAuthor%3A%20Qinyue%20Tong%20and%20Ziqian%20Lu%20and%20Jun%20Liu%20and%20Yangming%20Zheng%20and%20Zheming%20Lu%0AAbstract%3A%20%20%20Despite%20remarkable%20advancements%20in%20pixel-level%20medical%20image%20perception%2C%0Aexisting%20methods%20are%20either%20limited%20to%20specific%20tasks%20or%20heavily%20rely%20on%0Aaccurate%20bounding%20boxes%20or%20text%20labels%20as%20input%20prompts.%20However%2C%20the%20medical%0Aknowledge%20required%20for%20input%20is%20a%20huge%20obstacle%20for%20general%20public%2C%20which%0Agreatly%20reduces%20the%20universality%20of%20these%20methods.%20Compared%20with%20these%0Adomain-specialized%20auxiliary%20information%2C%20general%20users%20tend%20to%20rely%20on%20oral%0Aqueries%20that%20require%20logical%20reasoning.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Amedical%20vision%20task%3A%20Medical%20Reasoning%20Segmentation%20and%20Detection%20%28MedSD%29%2C%0Awhich%20aims%20to%20comprehend%20implicit%20queries%20about%20medical%20images%20and%20generate%20the%0Acorresponding%20segmentation%20mask%20and%20bounding%20box%20for%20the%20target%20object.%20To%0Aaccomplish%20this%20task%2C%20we%20first%20introduce%20a%20Multi-perspective%2C%20Logic-driven%0AMedical%20Reasoning%20Segmentation%20and%20Detection%20%28MLMR-SD%29%20dataset%2C%20which%0Aencompasses%20a%20substantial%20collection%20of%20medical%20entity%20targets%20along%20with%20their%0Acorresponding%20reasoning.%20Furthermore%2C%20we%20propose%20MediSee%2C%20an%20effective%20baseline%0Amodel%20designed%20for%20medical%20reasoning%20segmentation%20and%20detection.%20The%0Aexperimental%20results%20indicate%20that%20the%20proposed%20method%20can%20effectively%20address%0AMedSD%20with%20implicit%20colloquial%20queries%20and%20outperform%20traditional%20medical%0Areferring%20segmentation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11008v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMediSee%253A%2520Reasoning-based%2520Pixel-level%2520Perception%2520in%2520Medical%2520Images%26entry.906535625%3DQinyue%2520Tong%2520and%2520Ziqian%2520Lu%2520and%2520Jun%2520Liu%2520and%2520Yangming%2520Zheng%2520and%2520Zheming%2520Lu%26entry.1292438233%3D%2520%2520Despite%2520remarkable%2520advancements%2520in%2520pixel-level%2520medical%2520image%2520perception%252C%250Aexisting%2520methods%2520are%2520either%2520limited%2520to%2520specific%2520tasks%2520or%2520heavily%2520rely%2520on%250Aaccurate%2520bounding%2520boxes%2520or%2520text%2520labels%2520as%2520input%2520prompts.%2520However%252C%2520the%2520medical%250Aknowledge%2520required%2520for%2520input%2520is%2520a%2520huge%2520obstacle%2520for%2520general%2520public%252C%2520which%250Agreatly%2520reduces%2520the%2520universality%2520of%2520these%2520methods.%2520Compared%2520with%2520these%250Adomain-specialized%2520auxiliary%2520information%252C%2520general%2520users%2520tend%2520to%2520rely%2520on%2520oral%250Aqueries%2520that%2520require%2520logical%2520reasoning.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Amedical%2520vision%2520task%253A%2520Medical%2520Reasoning%2520Segmentation%2520and%2520Detection%2520%2528MedSD%2529%252C%250Awhich%2520aims%2520to%2520comprehend%2520implicit%2520queries%2520about%2520medical%2520images%2520and%2520generate%2520the%250Acorresponding%2520segmentation%2520mask%2520and%2520bounding%2520box%2520for%2520the%2520target%2520object.%2520To%250Aaccomplish%2520this%2520task%252C%2520we%2520first%2520introduce%2520a%2520Multi-perspective%252C%2520Logic-driven%250AMedical%2520Reasoning%2520Segmentation%2520and%2520Detection%2520%2528MLMR-SD%2529%2520dataset%252C%2520which%250Aencompasses%2520a%2520substantial%2520collection%2520of%2520medical%2520entity%2520targets%2520along%2520with%2520their%250Acorresponding%2520reasoning.%2520Furthermore%252C%2520we%2520propose%2520MediSee%252C%2520an%2520effective%2520baseline%250Amodel%2520designed%2520for%2520medical%2520reasoning%2520segmentation%2520and%2520detection.%2520The%250Aexperimental%2520results%2520indicate%2520that%2520the%2520proposed%2520method%2520can%2520effectively%2520address%250AMedSD%2520with%2520implicit%2520colloquial%2520queries%2520and%2520outperform%2520traditional%2520medical%250Areferring%2520segmentation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11008v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MediSee%3A%20Reasoning-based%20Pixel-level%20Perception%20in%20Medical%20Images&entry.906535625=Qinyue%20Tong%20and%20Ziqian%20Lu%20and%20Jun%20Liu%20and%20Yangming%20Zheng%20and%20Zheming%20Lu&entry.1292438233=%20%20Despite%20remarkable%20advancements%20in%20pixel-level%20medical%20image%20perception%2C%0Aexisting%20methods%20are%20either%20limited%20to%20specific%20tasks%20or%20heavily%20rely%20on%0Aaccurate%20bounding%20boxes%20or%20text%20labels%20as%20input%20prompts.%20However%2C%20the%20medical%0Aknowledge%20required%20for%20input%20is%20a%20huge%20obstacle%20for%20general%20public%2C%20which%0Agreatly%20reduces%20the%20universality%20of%20these%20methods.%20Compared%20with%20these%0Adomain-specialized%20auxiliary%20information%2C%20general%20users%20tend%20to%20rely%20on%20oral%0Aqueries%20that%20require%20logical%20reasoning.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Amedical%20vision%20task%3A%20Medical%20Reasoning%20Segmentation%20and%20Detection%20%28MedSD%29%2C%0Awhich%20aims%20to%20comprehend%20implicit%20queries%20about%20medical%20images%20and%20generate%20the%0Acorresponding%20segmentation%20mask%20and%20bounding%20box%20for%20the%20target%20object.%20To%0Aaccomplish%20this%20task%2C%20we%20first%20introduce%20a%20Multi-perspective%2C%20Logic-driven%0AMedical%20Reasoning%20Segmentation%20and%20Detection%20%28MLMR-SD%29%20dataset%2C%20which%0Aencompasses%20a%20substantial%20collection%20of%20medical%20entity%20targets%20along%20with%20their%0Acorresponding%20reasoning.%20Furthermore%2C%20we%20propose%20MediSee%2C%20an%20effective%20baseline%0Amodel%20designed%20for%20medical%20reasoning%20segmentation%20and%20detection.%20The%0Aexperimental%20results%20indicate%20that%20the%20proposed%20method%20can%20effectively%20address%0AMedSD%20with%20implicit%20colloquial%20queries%20and%20outperform%20traditional%20medical%0Areferring%20segmentation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11008v2&entry.124074799=Read"},
{"title": "Generalized Neighborhood Attention: Multi-dimensional Sparse Attention\n  at the Speed of Light", "author": "Ali Hassani and Fengzhe Zhou and Aditya Kane and Jiannan Huang and Chieh-Yun Chen and Min Shi and Steven Walton and Markus Hoehnerbach and Vijay Thakkar and Michael Isaev and Qinsheng Zhang and Bing Xu and Haicheng Wu and Wen-mei Hwu and Ming-Yu Liu and Humphrey Shi", "abstract": "  Many sparse attention mechanisms such as Neighborhood Attention have\ntypically failed to consistently deliver speedup over the self attention\nbaseline. This is largely due to the level of complexity in attention\ninfrastructure, and the rapid evolution of AI hardware architecture. At the\nsame time, many state-of-the-art foundational models, particularly in computer\nvision, are heavily bound by attention, and need reliable sparsity to escape\nthe O(n^2) complexity. In this paper, we study a class of promising sparse\nattention mechanisms that focus on locality, and aim to develop a better\nanalytical model of their performance improvements. We first introduce\nGeneralized Neighborhood Attention (GNA), which can describe sliding window,\nstrided sliding window, and blocked attention. We then consider possible design\nchoices in implementing these approaches, and create a simulator that can\nprovide much more realistic speedup upper bounds for any given setting.\nFinally, we implement GNA on top of a state-of-the-art fused multi-headed\nattention (FMHA) kernel designed for the NVIDIA Blackwell architecture in\nCUTLASS. Our implementation can fully realize the maximum speedup theoretically\npossible in many perfectly block-sparse cases, and achieves an effective\nutilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA\nconfigurations into off-the-shelf generative models, such as Cosmos-7B,\nHunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end\nspeedup on B200 without any fine-tuning. We will open source our simulator and\nBlackwell kernels directly through the NATTEN project.\n", "link": "http://arxiv.org/abs/2504.16922v1", "date": "2025-04-23", "relevancy": 2.2476, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5765}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5611}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Neighborhood%20Attention%3A%20Multi-dimensional%20Sparse%20Attention%0A%20%20at%20the%20Speed%20of%20Light&body=Title%3A%20Generalized%20Neighborhood%20Attention%3A%20Multi-dimensional%20Sparse%20Attention%0A%20%20at%20the%20Speed%20of%20Light%0AAuthor%3A%20Ali%20Hassani%20and%20Fengzhe%20Zhou%20and%20Aditya%20Kane%20and%20Jiannan%20Huang%20and%20Chieh-Yun%20Chen%20and%20Min%20Shi%20and%20Steven%20Walton%20and%20Markus%20Hoehnerbach%20and%20Vijay%20Thakkar%20and%20Michael%20Isaev%20and%20Qinsheng%20Zhang%20and%20Bing%20Xu%20and%20Haicheng%20Wu%20and%20Wen-mei%20Hwu%20and%20Ming-Yu%20Liu%20and%20Humphrey%20Shi%0AAbstract%3A%20%20%20Many%20sparse%20attention%20mechanisms%20such%20as%20Neighborhood%20Attention%20have%0Atypically%20failed%20to%20consistently%20deliver%20speedup%20over%20the%20self%20attention%0Abaseline.%20This%20is%20largely%20due%20to%20the%20level%20of%20complexity%20in%20attention%0Ainfrastructure%2C%20and%20the%20rapid%20evolution%20of%20AI%20hardware%20architecture.%20At%20the%0Asame%20time%2C%20many%20state-of-the-art%20foundational%20models%2C%20particularly%20in%20computer%0Avision%2C%20are%20heavily%20bound%20by%20attention%2C%20and%20need%20reliable%20sparsity%20to%20escape%0Athe%20O%28n%5E2%29%20complexity.%20In%20this%20paper%2C%20we%20study%20a%20class%20of%20promising%20sparse%0Aattention%20mechanisms%20that%20focus%20on%20locality%2C%20and%20aim%20to%20develop%20a%20better%0Aanalytical%20model%20of%20their%20performance%20improvements.%20We%20first%20introduce%0AGeneralized%20Neighborhood%20Attention%20%28GNA%29%2C%20which%20can%20describe%20sliding%20window%2C%0Astrided%20sliding%20window%2C%20and%20blocked%20attention.%20We%20then%20consider%20possible%20design%0Achoices%20in%20implementing%20these%20approaches%2C%20and%20create%20a%20simulator%20that%20can%0Aprovide%20much%20more%20realistic%20speedup%20upper%20bounds%20for%20any%20given%20setting.%0AFinally%2C%20we%20implement%20GNA%20on%20top%20of%20a%20state-of-the-art%20fused%20multi-headed%0Aattention%20%28FMHA%29%20kernel%20designed%20for%20the%20NVIDIA%20Blackwell%20architecture%20in%0ACUTLASS.%20Our%20implementation%20can%20fully%20realize%20the%20maximum%20speedup%20theoretically%0Apossible%20in%20many%20perfectly%20block-sparse%20cases%2C%20and%20achieves%20an%20effective%0Autilization%20of%201.3%20petaFLOPs/second%20in%20FP16.%20In%20addition%2C%20we%20plug%20various%20GNA%0Aconfigurations%20into%20off-the-shelf%20generative%20models%2C%20such%20as%20Cosmos-7B%2C%0AHunyuanVideo%2C%20and%20FLUX%2C%20and%20show%20that%20it%20can%20deliver%2028%25%20to%2046%25%20end-to-end%0Aspeedup%20on%20B200%20without%20any%20fine-tuning.%20We%20will%20open%20source%20our%20simulator%20and%0ABlackwell%20kernels%20directly%20through%20the%20NATTEN%20project.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Neighborhood%2520Attention%253A%2520Multi-dimensional%2520Sparse%2520Attention%250A%2520%2520at%2520the%2520Speed%2520of%2520Light%26entry.906535625%3DAli%2520Hassani%2520and%2520Fengzhe%2520Zhou%2520and%2520Aditya%2520Kane%2520and%2520Jiannan%2520Huang%2520and%2520Chieh-Yun%2520Chen%2520and%2520Min%2520Shi%2520and%2520Steven%2520Walton%2520and%2520Markus%2520Hoehnerbach%2520and%2520Vijay%2520Thakkar%2520and%2520Michael%2520Isaev%2520and%2520Qinsheng%2520Zhang%2520and%2520Bing%2520Xu%2520and%2520Haicheng%2520Wu%2520and%2520Wen-mei%2520Hwu%2520and%2520Ming-Yu%2520Liu%2520and%2520Humphrey%2520Shi%26entry.1292438233%3D%2520%2520Many%2520sparse%2520attention%2520mechanisms%2520such%2520as%2520Neighborhood%2520Attention%2520have%250Atypically%2520failed%2520to%2520consistently%2520deliver%2520speedup%2520over%2520the%2520self%2520attention%250Abaseline.%2520This%2520is%2520largely%2520due%2520to%2520the%2520level%2520of%2520complexity%2520in%2520attention%250Ainfrastructure%252C%2520and%2520the%2520rapid%2520evolution%2520of%2520AI%2520hardware%2520architecture.%2520At%2520the%250Asame%2520time%252C%2520many%2520state-of-the-art%2520foundational%2520models%252C%2520particularly%2520in%2520computer%250Avision%252C%2520are%2520heavily%2520bound%2520by%2520attention%252C%2520and%2520need%2520reliable%2520sparsity%2520to%2520escape%250Athe%2520O%2528n%255E2%2529%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520study%2520a%2520class%2520of%2520promising%2520sparse%250Aattention%2520mechanisms%2520that%2520focus%2520on%2520locality%252C%2520and%2520aim%2520to%2520develop%2520a%2520better%250Aanalytical%2520model%2520of%2520their%2520performance%2520improvements.%2520We%2520first%2520introduce%250AGeneralized%2520Neighborhood%2520Attention%2520%2528GNA%2529%252C%2520which%2520can%2520describe%2520sliding%2520window%252C%250Astrided%2520sliding%2520window%252C%2520and%2520blocked%2520attention.%2520We%2520then%2520consider%2520possible%2520design%250Achoices%2520in%2520implementing%2520these%2520approaches%252C%2520and%2520create%2520a%2520simulator%2520that%2520can%250Aprovide%2520much%2520more%2520realistic%2520speedup%2520upper%2520bounds%2520for%2520any%2520given%2520setting.%250AFinally%252C%2520we%2520implement%2520GNA%2520on%2520top%2520of%2520a%2520state-of-the-art%2520fused%2520multi-headed%250Aattention%2520%2528FMHA%2529%2520kernel%2520designed%2520for%2520the%2520NVIDIA%2520Blackwell%2520architecture%2520in%250ACUTLASS.%2520Our%2520implementation%2520can%2520fully%2520realize%2520the%2520maximum%2520speedup%2520theoretically%250Apossible%2520in%2520many%2520perfectly%2520block-sparse%2520cases%252C%2520and%2520achieves%2520an%2520effective%250Autilization%2520of%25201.3%2520petaFLOPs/second%2520in%2520FP16.%2520In%2520addition%252C%2520we%2520plug%2520various%2520GNA%250Aconfigurations%2520into%2520off-the-shelf%2520generative%2520models%252C%2520such%2520as%2520Cosmos-7B%252C%250AHunyuanVideo%252C%2520and%2520FLUX%252C%2520and%2520show%2520that%2520it%2520can%2520deliver%252028%2525%2520to%252046%2525%2520end-to-end%250Aspeedup%2520on%2520B200%2520without%2520any%2520fine-tuning.%2520We%2520will%2520open%2520source%2520our%2520simulator%2520and%250ABlackwell%2520kernels%2520directly%2520through%2520the%2520NATTEN%2520project.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Neighborhood%20Attention%3A%20Multi-dimensional%20Sparse%20Attention%0A%20%20at%20the%20Speed%20of%20Light&entry.906535625=Ali%20Hassani%20and%20Fengzhe%20Zhou%20and%20Aditya%20Kane%20and%20Jiannan%20Huang%20and%20Chieh-Yun%20Chen%20and%20Min%20Shi%20and%20Steven%20Walton%20and%20Markus%20Hoehnerbach%20and%20Vijay%20Thakkar%20and%20Michael%20Isaev%20and%20Qinsheng%20Zhang%20and%20Bing%20Xu%20and%20Haicheng%20Wu%20and%20Wen-mei%20Hwu%20and%20Ming-Yu%20Liu%20and%20Humphrey%20Shi&entry.1292438233=%20%20Many%20sparse%20attention%20mechanisms%20such%20as%20Neighborhood%20Attention%20have%0Atypically%20failed%20to%20consistently%20deliver%20speedup%20over%20the%20self%20attention%0Abaseline.%20This%20is%20largely%20due%20to%20the%20level%20of%20complexity%20in%20attention%0Ainfrastructure%2C%20and%20the%20rapid%20evolution%20of%20AI%20hardware%20architecture.%20At%20the%0Asame%20time%2C%20many%20state-of-the-art%20foundational%20models%2C%20particularly%20in%20computer%0Avision%2C%20are%20heavily%20bound%20by%20attention%2C%20and%20need%20reliable%20sparsity%20to%20escape%0Athe%20O%28n%5E2%29%20complexity.%20In%20this%20paper%2C%20we%20study%20a%20class%20of%20promising%20sparse%0Aattention%20mechanisms%20that%20focus%20on%20locality%2C%20and%20aim%20to%20develop%20a%20better%0Aanalytical%20model%20of%20their%20performance%20improvements.%20We%20first%20introduce%0AGeneralized%20Neighborhood%20Attention%20%28GNA%29%2C%20which%20can%20describe%20sliding%20window%2C%0Astrided%20sliding%20window%2C%20and%20blocked%20attention.%20We%20then%20consider%20possible%20design%0Achoices%20in%20implementing%20these%20approaches%2C%20and%20create%20a%20simulator%20that%20can%0Aprovide%20much%20more%20realistic%20speedup%20upper%20bounds%20for%20any%20given%20setting.%0AFinally%2C%20we%20implement%20GNA%20on%20top%20of%20a%20state-of-the-art%20fused%20multi-headed%0Aattention%20%28FMHA%29%20kernel%20designed%20for%20the%20NVIDIA%20Blackwell%20architecture%20in%0ACUTLASS.%20Our%20implementation%20can%20fully%20realize%20the%20maximum%20speedup%20theoretically%0Apossible%20in%20many%20perfectly%20block-sparse%20cases%2C%20and%20achieves%20an%20effective%0Autilization%20of%201.3%20petaFLOPs/second%20in%20FP16.%20In%20addition%2C%20we%20plug%20various%20GNA%0Aconfigurations%20into%20off-the-shelf%20generative%20models%2C%20such%20as%20Cosmos-7B%2C%0AHunyuanVideo%2C%20and%20FLUX%2C%20and%20show%20that%20it%20can%20deliver%2028%25%20to%2046%25%20end-to-end%0Aspeedup%20on%20B200%20without%20any%20fine-tuning.%20We%20will%20open%20source%20our%20simulator%20and%0ABlackwell%20kernels%20directly%20through%20the%20NATTEN%20project.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16922v1&entry.124074799=Read"},
{"title": "Sharp Bounds for Sequential Federated Learning on Heterogeneous Data", "author": "Yipeng Li and Xinchen Lyu", "abstract": "  There are two paradigms in Federated Learning (FL): parallel FL (PFL), where\nmodels are trained in a parallel manner across clients, and sequential FL\n(SFL), where models are trained in a sequential manner across clients.\nSpecifically, in PFL, clients perform local updates independently and send the\nupdated model parameters to a global server for aggregation; in SFL, one client\nstarts its local updates only after receiving the model parameters from the\nprevious client in the sequence. In contrast to that of PFL, the convergence\ntheory of SFL on heterogeneous data is still lacking. To resolve the\ntheoretical dilemma of SFL, we establish sharp convergence guarantees for SFL\non heterogeneous data with both upper and lower bounds. Specifically, we derive\nthe upper bounds for the strongly convex, general convex and non-convex\nobjective functions, and construct the matching lower bounds for the strongly\nconvex and general convex objective functions. Then, we compare the upper\nbounds of SFL with those of PFL, showing that SFL outperforms PFL on\nheterogeneous data (at least, when the level of heterogeneity is relatively\nhigh). Experimental results validate the counterintuitive theoretical finding.\n", "link": "http://arxiv.org/abs/2405.01142v2", "date": "2025-04-23", "relevancy": 2.2362, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4544}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.444}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharp%20Bounds%20for%20Sequential%20Federated%20Learning%20on%20Heterogeneous%20Data&body=Title%3A%20Sharp%20Bounds%20for%20Sequential%20Federated%20Learning%20on%20Heterogeneous%20Data%0AAuthor%3A%20Yipeng%20Li%20and%20Xinchen%20Lyu%0AAbstract%3A%20%20%20There%20are%20two%20paradigms%20in%20Federated%20Learning%20%28FL%29%3A%20parallel%20FL%20%28PFL%29%2C%20where%0Amodels%20are%20trained%20in%20a%20parallel%20manner%20across%20clients%2C%20and%20sequential%20FL%0A%28SFL%29%2C%20where%20models%20are%20trained%20in%20a%20sequential%20manner%20across%20clients.%0ASpecifically%2C%20in%20PFL%2C%20clients%20perform%20local%20updates%20independently%20and%20send%20the%0Aupdated%20model%20parameters%20to%20a%20global%20server%20for%20aggregation%3B%20in%20SFL%2C%20one%20client%0Astarts%20its%20local%20updates%20only%20after%20receiving%20the%20model%20parameters%20from%20the%0Aprevious%20client%20in%20the%20sequence.%20In%20contrast%20to%20that%20of%20PFL%2C%20the%20convergence%0Atheory%20of%20SFL%20on%20heterogeneous%20data%20is%20still%20lacking.%20To%20resolve%20the%0Atheoretical%20dilemma%20of%20SFL%2C%20we%20establish%20sharp%20convergence%20guarantees%20for%20SFL%0Aon%20heterogeneous%20data%20with%20both%20upper%20and%20lower%20bounds.%20Specifically%2C%20we%20derive%0Athe%20upper%20bounds%20for%20the%20strongly%20convex%2C%20general%20convex%20and%20non-convex%0Aobjective%20functions%2C%20and%20construct%20the%20matching%20lower%20bounds%20for%20the%20strongly%0Aconvex%20and%20general%20convex%20objective%20functions.%20Then%2C%20we%20compare%20the%20upper%0Abounds%20of%20SFL%20with%20those%20of%20PFL%2C%20showing%20that%20SFL%20outperforms%20PFL%20on%0Aheterogeneous%20data%20%28at%20least%2C%20when%20the%20level%20of%20heterogeneity%20is%20relatively%0Ahigh%29.%20Experimental%20results%20validate%20the%20counterintuitive%20theoretical%20finding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01142v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharp%2520Bounds%2520for%2520Sequential%2520Federated%2520Learning%2520on%2520Heterogeneous%2520Data%26entry.906535625%3DYipeng%2520Li%2520and%2520Xinchen%2520Lyu%26entry.1292438233%3D%2520%2520There%2520are%2520two%2520paradigms%2520in%2520Federated%2520Learning%2520%2528FL%2529%253A%2520parallel%2520FL%2520%2528PFL%2529%252C%2520where%250Amodels%2520are%2520trained%2520in%2520a%2520parallel%2520manner%2520across%2520clients%252C%2520and%2520sequential%2520FL%250A%2528SFL%2529%252C%2520where%2520models%2520are%2520trained%2520in%2520a%2520sequential%2520manner%2520across%2520clients.%250ASpecifically%252C%2520in%2520PFL%252C%2520clients%2520perform%2520local%2520updates%2520independently%2520and%2520send%2520the%250Aupdated%2520model%2520parameters%2520to%2520a%2520global%2520server%2520for%2520aggregation%253B%2520in%2520SFL%252C%2520one%2520client%250Astarts%2520its%2520local%2520updates%2520only%2520after%2520receiving%2520the%2520model%2520parameters%2520from%2520the%250Aprevious%2520client%2520in%2520the%2520sequence.%2520In%2520contrast%2520to%2520that%2520of%2520PFL%252C%2520the%2520convergence%250Atheory%2520of%2520SFL%2520on%2520heterogeneous%2520data%2520is%2520still%2520lacking.%2520To%2520resolve%2520the%250Atheoretical%2520dilemma%2520of%2520SFL%252C%2520we%2520establish%2520sharp%2520convergence%2520guarantees%2520for%2520SFL%250Aon%2520heterogeneous%2520data%2520with%2520both%2520upper%2520and%2520lower%2520bounds.%2520Specifically%252C%2520we%2520derive%250Athe%2520upper%2520bounds%2520for%2520the%2520strongly%2520convex%252C%2520general%2520convex%2520and%2520non-convex%250Aobjective%2520functions%252C%2520and%2520construct%2520the%2520matching%2520lower%2520bounds%2520for%2520the%2520strongly%250Aconvex%2520and%2520general%2520convex%2520objective%2520functions.%2520Then%252C%2520we%2520compare%2520the%2520upper%250Abounds%2520of%2520SFL%2520with%2520those%2520of%2520PFL%252C%2520showing%2520that%2520SFL%2520outperforms%2520PFL%2520on%250Aheterogeneous%2520data%2520%2528at%2520least%252C%2520when%2520the%2520level%2520of%2520heterogeneity%2520is%2520relatively%250Ahigh%2529.%2520Experimental%2520results%2520validate%2520the%2520counterintuitive%2520theoretical%2520finding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01142v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20Bounds%20for%20Sequential%20Federated%20Learning%20on%20Heterogeneous%20Data&entry.906535625=Yipeng%20Li%20and%20Xinchen%20Lyu&entry.1292438233=%20%20There%20are%20two%20paradigms%20in%20Federated%20Learning%20%28FL%29%3A%20parallel%20FL%20%28PFL%29%2C%20where%0Amodels%20are%20trained%20in%20a%20parallel%20manner%20across%20clients%2C%20and%20sequential%20FL%0A%28SFL%29%2C%20where%20models%20are%20trained%20in%20a%20sequential%20manner%20across%20clients.%0ASpecifically%2C%20in%20PFL%2C%20clients%20perform%20local%20updates%20independently%20and%20send%20the%0Aupdated%20model%20parameters%20to%20a%20global%20server%20for%20aggregation%3B%20in%20SFL%2C%20one%20client%0Astarts%20its%20local%20updates%20only%20after%20receiving%20the%20model%20parameters%20from%20the%0Aprevious%20client%20in%20the%20sequence.%20In%20contrast%20to%20that%20of%20PFL%2C%20the%20convergence%0Atheory%20of%20SFL%20on%20heterogeneous%20data%20is%20still%20lacking.%20To%20resolve%20the%0Atheoretical%20dilemma%20of%20SFL%2C%20we%20establish%20sharp%20convergence%20guarantees%20for%20SFL%0Aon%20heterogeneous%20data%20with%20both%20upper%20and%20lower%20bounds.%20Specifically%2C%20we%20derive%0Athe%20upper%20bounds%20for%20the%20strongly%20convex%2C%20general%20convex%20and%20non-convex%0Aobjective%20functions%2C%20and%20construct%20the%20matching%20lower%20bounds%20for%20the%20strongly%0Aconvex%20and%20general%20convex%20objective%20functions.%20Then%2C%20we%20compare%20the%20upper%0Abounds%20of%20SFL%20with%20those%20of%20PFL%2C%20showing%20that%20SFL%20outperforms%20PFL%20on%0Aheterogeneous%20data%20%28at%20least%2C%20when%20the%20level%20of%20heterogeneity%20is%20relatively%0Ahigh%29.%20Experimental%20results%20validate%20the%20counterintuitive%20theoretical%20finding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01142v2&entry.124074799=Read"},
{"title": "On Benchmarking Code LLMs for Android Malware Analysis", "author": "Yiling He and Hongyu She and Xingzhi Qian and Xinran Zheng and Zhuo Chen and Zhan Qin and Lorenzo Cavallaro", "abstract": "  Large Language Models (LLMs) have demonstrated strong capabilities in various\ncode intelligence tasks. However, their effectiveness for Android malware\nanalysis remains underexplored. Decompiled Android malware code presents unique\nchallenges for analysis, due to the malicious logic being buried within a large\nnumber of functions and the frequent lack of meaningful function names. This\npaper presents CAMA, a benchmarking framework designed to systematically\nevaluate the effectiveness of Code LLMs in Android malware analysis. CAMA\nspecifies structured model outputs to support key malware analysis tasks,\nincluding malicious function identification and malware purpose summarization.\nBuilt on these, it integrates three domain-specific evaluation metrics\n(consistency, fidelity, and semantic relevance), enabling rigorous stability\nand effectiveness assessment and cross-model comparison. We construct a\nbenchmark dataset of 118 Android malware samples from 13 families collected in\nrecent years, encompassing over 7.5 million distinct functions, and use CAMA to\nevaluate four popular open-source Code LLMs. Our experiments provide insights\ninto how Code LLMs interpret decompiled code and quantify the sensitivity to\nfunction renaming, highlighting both their potential and current limitations in\nmalware analysis.\n", "link": "http://arxiv.org/abs/2504.00694v2", "date": "2025-04-23", "relevancy": 2.2184, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.44}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Benchmarking%20Code%20LLMs%20for%20Android%20Malware%20Analysis&body=Title%3A%20On%20Benchmarking%20Code%20LLMs%20for%20Android%20Malware%20Analysis%0AAuthor%3A%20Yiling%20He%20and%20Hongyu%20She%20and%20Xingzhi%20Qian%20and%20Xinran%20Zheng%20and%20Zhuo%20Chen%20and%20Zhan%20Qin%20and%20Lorenzo%20Cavallaro%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20in%20various%0Acode%20intelligence%20tasks.%20However%2C%20their%20effectiveness%20for%20Android%20malware%0Aanalysis%20remains%20underexplored.%20Decompiled%20Android%20malware%20code%20presents%20unique%0Achallenges%20for%20analysis%2C%20due%20to%20the%20malicious%20logic%20being%20buried%20within%20a%20large%0Anumber%20of%20functions%20and%20the%20frequent%20lack%20of%20meaningful%20function%20names.%20This%0Apaper%20presents%20CAMA%2C%20a%20benchmarking%20framework%20designed%20to%20systematically%0Aevaluate%20the%20effectiveness%20of%20Code%20LLMs%20in%20Android%20malware%20analysis.%20CAMA%0Aspecifies%20structured%20model%20outputs%20to%20support%20key%20malware%20analysis%20tasks%2C%0Aincluding%20malicious%20function%20identification%20and%20malware%20purpose%20summarization.%0ABuilt%20on%20these%2C%20it%20integrates%20three%20domain-specific%20evaluation%20metrics%0A%28consistency%2C%20fidelity%2C%20and%20semantic%20relevance%29%2C%20enabling%20rigorous%20stability%0Aand%20effectiveness%20assessment%20and%20cross-model%20comparison.%20We%20construct%20a%0Abenchmark%20dataset%20of%20118%20Android%20malware%20samples%20from%2013%20families%20collected%20in%0Arecent%20years%2C%20encompassing%20over%207.5%20million%20distinct%20functions%2C%20and%20use%20CAMA%20to%0Aevaluate%20four%20popular%20open-source%20Code%20LLMs.%20Our%20experiments%20provide%20insights%0Ainto%20how%20Code%20LLMs%20interpret%20decompiled%20code%20and%20quantify%20the%20sensitivity%20to%0Afunction%20renaming%2C%20highlighting%20both%20their%20potential%20and%20current%20limitations%20in%0Amalware%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00694v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Benchmarking%2520Code%2520LLMs%2520for%2520Android%2520Malware%2520Analysis%26entry.906535625%3DYiling%2520He%2520and%2520Hongyu%2520She%2520and%2520Xingzhi%2520Qian%2520and%2520Xinran%2520Zheng%2520and%2520Zhuo%2520Chen%2520and%2520Zhan%2520Qin%2520and%2520Lorenzo%2520Cavallaro%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520in%2520various%250Acode%2520intelligence%2520tasks.%2520However%252C%2520their%2520effectiveness%2520for%2520Android%2520malware%250Aanalysis%2520remains%2520underexplored.%2520Decompiled%2520Android%2520malware%2520code%2520presents%2520unique%250Achallenges%2520for%2520analysis%252C%2520due%2520to%2520the%2520malicious%2520logic%2520being%2520buried%2520within%2520a%2520large%250Anumber%2520of%2520functions%2520and%2520the%2520frequent%2520lack%2520of%2520meaningful%2520function%2520names.%2520This%250Apaper%2520presents%2520CAMA%252C%2520a%2520benchmarking%2520framework%2520designed%2520to%2520systematically%250Aevaluate%2520the%2520effectiveness%2520of%2520Code%2520LLMs%2520in%2520Android%2520malware%2520analysis.%2520CAMA%250Aspecifies%2520structured%2520model%2520outputs%2520to%2520support%2520key%2520malware%2520analysis%2520tasks%252C%250Aincluding%2520malicious%2520function%2520identification%2520and%2520malware%2520purpose%2520summarization.%250ABuilt%2520on%2520these%252C%2520it%2520integrates%2520three%2520domain-specific%2520evaluation%2520metrics%250A%2528consistency%252C%2520fidelity%252C%2520and%2520semantic%2520relevance%2529%252C%2520enabling%2520rigorous%2520stability%250Aand%2520effectiveness%2520assessment%2520and%2520cross-model%2520comparison.%2520We%2520construct%2520a%250Abenchmark%2520dataset%2520of%2520118%2520Android%2520malware%2520samples%2520from%252013%2520families%2520collected%2520in%250Arecent%2520years%252C%2520encompassing%2520over%25207.5%2520million%2520distinct%2520functions%252C%2520and%2520use%2520CAMA%2520to%250Aevaluate%2520four%2520popular%2520open-source%2520Code%2520LLMs.%2520Our%2520experiments%2520provide%2520insights%250Ainto%2520how%2520Code%2520LLMs%2520interpret%2520decompiled%2520code%2520and%2520quantify%2520the%2520sensitivity%2520to%250Afunction%2520renaming%252C%2520highlighting%2520both%2520their%2520potential%2520and%2520current%2520limitations%2520in%250Amalware%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00694v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Benchmarking%20Code%20LLMs%20for%20Android%20Malware%20Analysis&entry.906535625=Yiling%20He%20and%20Hongyu%20She%20and%20Xingzhi%20Qian%20and%20Xinran%20Zheng%20and%20Zhuo%20Chen%20and%20Zhan%20Qin%20and%20Lorenzo%20Cavallaro&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20in%20various%0Acode%20intelligence%20tasks.%20However%2C%20their%20effectiveness%20for%20Android%20malware%0Aanalysis%20remains%20underexplored.%20Decompiled%20Android%20malware%20code%20presents%20unique%0Achallenges%20for%20analysis%2C%20due%20to%20the%20malicious%20logic%20being%20buried%20within%20a%20large%0Anumber%20of%20functions%20and%20the%20frequent%20lack%20of%20meaningful%20function%20names.%20This%0Apaper%20presents%20CAMA%2C%20a%20benchmarking%20framework%20designed%20to%20systematically%0Aevaluate%20the%20effectiveness%20of%20Code%20LLMs%20in%20Android%20malware%20analysis.%20CAMA%0Aspecifies%20structured%20model%20outputs%20to%20support%20key%20malware%20analysis%20tasks%2C%0Aincluding%20malicious%20function%20identification%20and%20malware%20purpose%20summarization.%0ABuilt%20on%20these%2C%20it%20integrates%20three%20domain-specific%20evaluation%20metrics%0A%28consistency%2C%20fidelity%2C%20and%20semantic%20relevance%29%2C%20enabling%20rigorous%20stability%0Aand%20effectiveness%20assessment%20and%20cross-model%20comparison.%20We%20construct%20a%0Abenchmark%20dataset%20of%20118%20Android%20malware%20samples%20from%2013%20families%20collected%20in%0Arecent%20years%2C%20encompassing%20over%207.5%20million%20distinct%20functions%2C%20and%20use%20CAMA%20to%0Aevaluate%20four%20popular%20open-source%20Code%20LLMs.%20Our%20experiments%20provide%20insights%0Ainto%20how%20Code%20LLMs%20interpret%20decompiled%20code%20and%20quantify%20the%20sensitivity%20to%0Afunction%20renaming%2C%20highlighting%20both%20their%20potential%20and%20current%20limitations%20in%0Amalware%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00694v2&entry.124074799=Read"},
{"title": "CF-CAM: Cluster Filter Class Activation Mapping for Reliable\n  Gradient-Based Interpretability", "author": "Hongjie He and Xu Pan and Yudong Yao", "abstract": "  As deep learning continues to advance, the transparency of neural network\ndecision-making remains a critical challenge, limiting trust and applicability\nin high-stakes domains. Class Activation Mapping (CAM) techniques have emerged\nas a key approach toward visualizing model decisions, yet existing methods face\ninherent trade-offs. Gradient-based CAM variants suffer from sensitivity to\ngradient perturbations due to gradient noise, leading to unstable and\nunreliable explanations. Conversely, gradient-free approaches mitigate gradient\ninstability but incur significant computational overhead and inference latency.\nTo address these limitations, we propose a Cluster Filter Class Activation Map\n(CF-CAM) technique, a novel framework that reintroduces gradient-based\nweighting while enhancing robustness against gradient noise. CF-CAM utilizes\nhierarchical importance weighting strategy to balance discriminative feature\npreservation and noise elimination. A density-aware channel clustering method\nvia Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups\nsemantically relevant feature channels and discard noise-prone activations.\nAdditionally, cluster-conditioned gradient filtering leverages Gaussian filters\nto refine gradient signals, preserving edge-aware localization while\nsuppressing noise impact. Experiment results demonstrate that CF-CAM achieves\nsuperior interpretability performance while enhancing computational efficiency,\noutperforming state-of-the-art CAM methods in faithfulness and robustness. By\neffectively mitigating gradient instability without excessive computational\ncost, CF-CAM provides a competitive solution for enhancing the interpretability\nof deep neural networks in critical applications such as autonomous driving and\nmedical diagnosis.\n", "link": "http://arxiv.org/abs/2504.00060v2", "date": "2025-04-23", "relevancy": 2.2154, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.569}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5684}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CF-CAM%3A%20Cluster%20Filter%20Class%20Activation%20Mapping%20for%20Reliable%0A%20%20Gradient-Based%20Interpretability&body=Title%3A%20CF-CAM%3A%20Cluster%20Filter%20Class%20Activation%20Mapping%20for%20Reliable%0A%20%20Gradient-Based%20Interpretability%0AAuthor%3A%20Hongjie%20He%20and%20Xu%20Pan%20and%20Yudong%20Yao%0AAbstract%3A%20%20%20As%20deep%20learning%20continues%20to%20advance%2C%20the%20transparency%20of%20neural%20network%0Adecision-making%20remains%20a%20critical%20challenge%2C%20limiting%20trust%20and%20applicability%0Ain%20high-stakes%20domains.%20Class%20Activation%20Mapping%20%28CAM%29%20techniques%20have%20emerged%0Aas%20a%20key%20approach%20toward%20visualizing%20model%20decisions%2C%20yet%20existing%20methods%20face%0Ainherent%20trade-offs.%20Gradient-based%20CAM%20variants%20suffer%20from%20sensitivity%20to%0Agradient%20perturbations%20due%20to%20gradient%20noise%2C%20leading%20to%20unstable%20and%0Aunreliable%20explanations.%20Conversely%2C%20gradient-free%20approaches%20mitigate%20gradient%0Ainstability%20but%20incur%20significant%20computational%20overhead%20and%20inference%20latency.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20Cluster%20Filter%20Class%20Activation%20Map%0A%28CF-CAM%29%20technique%2C%20a%20novel%20framework%20that%20reintroduces%20gradient-based%0Aweighting%20while%20enhancing%20robustness%20against%20gradient%20noise.%20CF-CAM%20utilizes%0Ahierarchical%20importance%20weighting%20strategy%20to%20balance%20discriminative%20feature%0Apreservation%20and%20noise%20elimination.%20A%20density-aware%20channel%20clustering%20method%0Avia%20Density-Based%20Spatial%20Clustering%20of%20Applications%20with%20Noise%20%28DBSCAN%29%20groups%0Asemantically%20relevant%20feature%20channels%20and%20discard%20noise-prone%20activations.%0AAdditionally%2C%20cluster-conditioned%20gradient%20filtering%20leverages%20Gaussian%20filters%0Ato%20refine%20gradient%20signals%2C%20preserving%20edge-aware%20localization%20while%0Asuppressing%20noise%20impact.%20Experiment%20results%20demonstrate%20that%20CF-CAM%20achieves%0Asuperior%20interpretability%20performance%20while%20enhancing%20computational%20efficiency%2C%0Aoutperforming%20state-of-the-art%20CAM%20methods%20in%20faithfulness%20and%20robustness.%20By%0Aeffectively%20mitigating%20gradient%20instability%20without%20excessive%20computational%0Acost%2C%20CF-CAM%20provides%20a%20competitive%20solution%20for%20enhancing%20the%20interpretability%0Aof%20deep%20neural%20networks%20in%20critical%20applications%20such%20as%20autonomous%20driving%20and%0Amedical%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCF-CAM%253A%2520Cluster%2520Filter%2520Class%2520Activation%2520Mapping%2520for%2520Reliable%250A%2520%2520Gradient-Based%2520Interpretability%26entry.906535625%3DHongjie%2520He%2520and%2520Xu%2520Pan%2520and%2520Yudong%2520Yao%26entry.1292438233%3D%2520%2520As%2520deep%2520learning%2520continues%2520to%2520advance%252C%2520the%2520transparency%2520of%2520neural%2520network%250Adecision-making%2520remains%2520a%2520critical%2520challenge%252C%2520limiting%2520trust%2520and%2520applicability%250Ain%2520high-stakes%2520domains.%2520Class%2520Activation%2520Mapping%2520%2528CAM%2529%2520techniques%2520have%2520emerged%250Aas%2520a%2520key%2520approach%2520toward%2520visualizing%2520model%2520decisions%252C%2520yet%2520existing%2520methods%2520face%250Ainherent%2520trade-offs.%2520Gradient-based%2520CAM%2520variants%2520suffer%2520from%2520sensitivity%2520to%250Agradient%2520perturbations%2520due%2520to%2520gradient%2520noise%252C%2520leading%2520to%2520unstable%2520and%250Aunreliable%2520explanations.%2520Conversely%252C%2520gradient-free%2520approaches%2520mitigate%2520gradient%250Ainstability%2520but%2520incur%2520significant%2520computational%2520overhead%2520and%2520inference%2520latency.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520Cluster%2520Filter%2520Class%2520Activation%2520Map%250A%2528CF-CAM%2529%2520technique%252C%2520a%2520novel%2520framework%2520that%2520reintroduces%2520gradient-based%250Aweighting%2520while%2520enhancing%2520robustness%2520against%2520gradient%2520noise.%2520CF-CAM%2520utilizes%250Ahierarchical%2520importance%2520weighting%2520strategy%2520to%2520balance%2520discriminative%2520feature%250Apreservation%2520and%2520noise%2520elimination.%2520A%2520density-aware%2520channel%2520clustering%2520method%250Avia%2520Density-Based%2520Spatial%2520Clustering%2520of%2520Applications%2520with%2520Noise%2520%2528DBSCAN%2529%2520groups%250Asemantically%2520relevant%2520feature%2520channels%2520and%2520discard%2520noise-prone%2520activations.%250AAdditionally%252C%2520cluster-conditioned%2520gradient%2520filtering%2520leverages%2520Gaussian%2520filters%250Ato%2520refine%2520gradient%2520signals%252C%2520preserving%2520edge-aware%2520localization%2520while%250Asuppressing%2520noise%2520impact.%2520Experiment%2520results%2520demonstrate%2520that%2520CF-CAM%2520achieves%250Asuperior%2520interpretability%2520performance%2520while%2520enhancing%2520computational%2520efficiency%252C%250Aoutperforming%2520state-of-the-art%2520CAM%2520methods%2520in%2520faithfulness%2520and%2520robustness.%2520By%250Aeffectively%2520mitigating%2520gradient%2520instability%2520without%2520excessive%2520computational%250Acost%252C%2520CF-CAM%2520provides%2520a%2520competitive%2520solution%2520for%2520enhancing%2520the%2520interpretability%250Aof%2520deep%2520neural%2520networks%2520in%2520critical%2520applications%2520such%2520as%2520autonomous%2520driving%2520and%250Amedical%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CF-CAM%3A%20Cluster%20Filter%20Class%20Activation%20Mapping%20for%20Reliable%0A%20%20Gradient-Based%20Interpretability&entry.906535625=Hongjie%20He%20and%20Xu%20Pan%20and%20Yudong%20Yao&entry.1292438233=%20%20As%20deep%20learning%20continues%20to%20advance%2C%20the%20transparency%20of%20neural%20network%0Adecision-making%20remains%20a%20critical%20challenge%2C%20limiting%20trust%20and%20applicability%0Ain%20high-stakes%20domains.%20Class%20Activation%20Mapping%20%28CAM%29%20techniques%20have%20emerged%0Aas%20a%20key%20approach%20toward%20visualizing%20model%20decisions%2C%20yet%20existing%20methods%20face%0Ainherent%20trade-offs.%20Gradient-based%20CAM%20variants%20suffer%20from%20sensitivity%20to%0Agradient%20perturbations%20due%20to%20gradient%20noise%2C%20leading%20to%20unstable%20and%0Aunreliable%20explanations.%20Conversely%2C%20gradient-free%20approaches%20mitigate%20gradient%0Ainstability%20but%20incur%20significant%20computational%20overhead%20and%20inference%20latency.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20Cluster%20Filter%20Class%20Activation%20Map%0A%28CF-CAM%29%20technique%2C%20a%20novel%20framework%20that%20reintroduces%20gradient-based%0Aweighting%20while%20enhancing%20robustness%20against%20gradient%20noise.%20CF-CAM%20utilizes%0Ahierarchical%20importance%20weighting%20strategy%20to%20balance%20discriminative%20feature%0Apreservation%20and%20noise%20elimination.%20A%20density-aware%20channel%20clustering%20method%0Avia%20Density-Based%20Spatial%20Clustering%20of%20Applications%20with%20Noise%20%28DBSCAN%29%20groups%0Asemantically%20relevant%20feature%20channels%20and%20discard%20noise-prone%20activations.%0AAdditionally%2C%20cluster-conditioned%20gradient%20filtering%20leverages%20Gaussian%20filters%0Ato%20refine%20gradient%20signals%2C%20preserving%20edge-aware%20localization%20while%0Asuppressing%20noise%20impact.%20Experiment%20results%20demonstrate%20that%20CF-CAM%20achieves%0Asuperior%20interpretability%20performance%20while%20enhancing%20computational%20efficiency%2C%0Aoutperforming%20state-of-the-art%20CAM%20methods%20in%20faithfulness%20and%20robustness.%20By%0Aeffectively%20mitigating%20gradient%20instability%20without%20excessive%20computational%0Acost%2C%20CF-CAM%20provides%20a%20competitive%20solution%20for%20enhancing%20the%20interpretability%0Aof%20deep%20neural%20networks%20in%20critical%20applications%20such%20as%20autonomous%20driving%20and%0Amedical%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00060v2&entry.124074799=Read"},
{"title": "Adapter-Enhanced Semantic Prompting for Continual Learning", "author": "Baocai Yin and Ji Zhao and Huajie Jiang and Ningning Hou and Yongli Hu and Amin Beheshti and Ming-Hsuan Yang and Yuankai Qi", "abstract": "  Continual learning (CL) enables models to adapt to evolving data streams. A\nmajor challenge of CL is catastrophic forgetting, where new knowledge will\noverwrite previously acquired knowledge. Traditional methods usually retain the\npast data for replay or add additional branches in the model to learn new\nknowledge, which has high memory requirements. In this paper, we propose a\nnovel lightweight CL framework, Adapter-Enhanced Semantic Prompting (AESP),\nwhich integrates prompt tuning and adapter techniques. Specifically, we design\nsemantic-guided prompts to enhance the generalization ability of visual\nfeatures and utilize adapters to efficiently fuse the semantic information,\naiming to learn more adaptive features for the continual learning task.\nFurthermore, to choose the right task prompt for feature adaptation, we have\ndeveloped a novel matching mechanism for prompt selection. Extensive\nexperiments on three CL datasets demonstrate that our approach achieves\nfavorable performance across multiple metrics, showing its potential for\nadvancing CL.\n", "link": "http://arxiv.org/abs/2412.11074v2", "date": "2025-04-23", "relevancy": 2.1827, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5194}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapter-Enhanced%20Semantic%20Prompting%20for%20Continual%20Learning&body=Title%3A%20Adapter-Enhanced%20Semantic%20Prompting%20for%20Continual%20Learning%0AAuthor%3A%20Baocai%20Yin%20and%20Ji%20Zhao%20and%20Huajie%20Jiang%20and%20Ningning%20Hou%20and%20Yongli%20Hu%20and%20Amin%20Beheshti%20and%20Ming-Hsuan%20Yang%20and%20Yuankai%20Qi%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20enables%20models%20to%20adapt%20to%20evolving%20data%20streams.%20A%0Amajor%20challenge%20of%20CL%20is%20catastrophic%20forgetting%2C%20where%20new%20knowledge%20will%0Aoverwrite%20previously%20acquired%20knowledge.%20Traditional%20methods%20usually%20retain%20the%0Apast%20data%20for%20replay%20or%20add%20additional%20branches%20in%20the%20model%20to%20learn%20new%0Aknowledge%2C%20which%20has%20high%20memory%20requirements.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20lightweight%20CL%20framework%2C%20Adapter-Enhanced%20Semantic%20Prompting%20%28AESP%29%2C%0Awhich%20integrates%20prompt%20tuning%20and%20adapter%20techniques.%20Specifically%2C%20we%20design%0Asemantic-guided%20prompts%20to%20enhance%20the%20generalization%20ability%20of%20visual%0Afeatures%20and%20utilize%20adapters%20to%20efficiently%20fuse%20the%20semantic%20information%2C%0Aaiming%20to%20learn%20more%20adaptive%20features%20for%20the%20continual%20learning%20task.%0AFurthermore%2C%20to%20choose%20the%20right%20task%20prompt%20for%20feature%20adaptation%2C%20we%20have%0Adeveloped%20a%20novel%20matching%20mechanism%20for%20prompt%20selection.%20Extensive%0Aexperiments%20on%20three%20CL%20datasets%20demonstrate%20that%20our%20approach%20achieves%0Afavorable%20performance%20across%20multiple%20metrics%2C%20showing%20its%20potential%20for%0Aadvancing%20CL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11074v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapter-Enhanced%2520Semantic%2520Prompting%2520for%2520Continual%2520Learning%26entry.906535625%3DBaocai%2520Yin%2520and%2520Ji%2520Zhao%2520and%2520Huajie%2520Jiang%2520and%2520Ningning%2520Hou%2520and%2520Yongli%2520Hu%2520and%2520Amin%2520Beheshti%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Yuankai%2520Qi%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520enables%2520models%2520to%2520adapt%2520to%2520evolving%2520data%2520streams.%2520A%250Amajor%2520challenge%2520of%2520CL%2520is%2520catastrophic%2520forgetting%252C%2520where%2520new%2520knowledge%2520will%250Aoverwrite%2520previously%2520acquired%2520knowledge.%2520Traditional%2520methods%2520usually%2520retain%2520the%250Apast%2520data%2520for%2520replay%2520or%2520add%2520additional%2520branches%2520in%2520the%2520model%2520to%2520learn%2520new%250Aknowledge%252C%2520which%2520has%2520high%2520memory%2520requirements.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520lightweight%2520CL%2520framework%252C%2520Adapter-Enhanced%2520Semantic%2520Prompting%2520%2528AESP%2529%252C%250Awhich%2520integrates%2520prompt%2520tuning%2520and%2520adapter%2520techniques.%2520Specifically%252C%2520we%2520design%250Asemantic-guided%2520prompts%2520to%2520enhance%2520the%2520generalization%2520ability%2520of%2520visual%250Afeatures%2520and%2520utilize%2520adapters%2520to%2520efficiently%2520fuse%2520the%2520semantic%2520information%252C%250Aaiming%2520to%2520learn%2520more%2520adaptive%2520features%2520for%2520the%2520continual%2520learning%2520task.%250AFurthermore%252C%2520to%2520choose%2520the%2520right%2520task%2520prompt%2520for%2520feature%2520adaptation%252C%2520we%2520have%250Adeveloped%2520a%2520novel%2520matching%2520mechanism%2520for%2520prompt%2520selection.%2520Extensive%250Aexperiments%2520on%2520three%2520CL%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520achieves%250Afavorable%2520performance%2520across%2520multiple%2520metrics%252C%2520showing%2520its%2520potential%2520for%250Aadvancing%2520CL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11074v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapter-Enhanced%20Semantic%20Prompting%20for%20Continual%20Learning&entry.906535625=Baocai%20Yin%20and%20Ji%20Zhao%20and%20Huajie%20Jiang%20and%20Ningning%20Hou%20and%20Yongli%20Hu%20and%20Amin%20Beheshti%20and%20Ming-Hsuan%20Yang%20and%20Yuankai%20Qi&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20enables%20models%20to%20adapt%20to%20evolving%20data%20streams.%20A%0Amajor%20challenge%20of%20CL%20is%20catastrophic%20forgetting%2C%20where%20new%20knowledge%20will%0Aoverwrite%20previously%20acquired%20knowledge.%20Traditional%20methods%20usually%20retain%20the%0Apast%20data%20for%20replay%20or%20add%20additional%20branches%20in%20the%20model%20to%20learn%20new%0Aknowledge%2C%20which%20has%20high%20memory%20requirements.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20lightweight%20CL%20framework%2C%20Adapter-Enhanced%20Semantic%20Prompting%20%28AESP%29%2C%0Awhich%20integrates%20prompt%20tuning%20and%20adapter%20techniques.%20Specifically%2C%20we%20design%0Asemantic-guided%20prompts%20to%20enhance%20the%20generalization%20ability%20of%20visual%0Afeatures%20and%20utilize%20adapters%20to%20efficiently%20fuse%20the%20semantic%20information%2C%0Aaiming%20to%20learn%20more%20adaptive%20features%20for%20the%20continual%20learning%20task.%0AFurthermore%2C%20to%20choose%20the%20right%20task%20prompt%20for%20feature%20adaptation%2C%20we%20have%0Adeveloped%20a%20novel%20matching%20mechanism%20for%20prompt%20selection.%20Extensive%0Aexperiments%20on%20three%20CL%20datasets%20demonstrate%20that%20our%20approach%20achieves%0Afavorable%20performance%20across%20multiple%20metrics%2C%20showing%20its%20potential%20for%0Aadvancing%20CL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11074v2&entry.124074799=Read"},
{"title": "High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR\n  and Contaminated Optical Data", "author": "Chenxi Duan", "abstract": "  Addressing gaps caused by cloud cover and the long revisit cycle of\nsatellites is vital for providing essential data to support remote sensing\napplications. This paper tackles the challenges of missing optical data\nsynthesis, particularly in complex scenarios with cloud cover. We propose\nCRSynthNet, a novel image synthesis network that incorporates innovative\ndesigned modules such as the DownUp Block and Fusion Attention to enhance\naccuracy. Experimental results validate the effectiveness of CRSynthNet,\ndemonstrating substantial improvements in restoring structural details,\npreserving spectral consist, and achieving superior visual effects that far\nexceed those produced by comparison methods. It achieves quantitative\nimprovements across multiple metrics: a peak signal-to-noise ratio (PSNR) of\n26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean\nsquare error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12\ndataset, a valuable resource specifically designed to address cloud cover\nchallenges in missing optical data synthesis study. The dataset uniquely\nincludes cloud-covered images and leverages earlier image to predict later\nimage, offering a realistic representation of real-world scenarios. This study\noffer practical method and valuable resources for optical satellite image\nsynthesis task.\n", "link": "http://arxiv.org/abs/2504.16870v1", "date": "2025-04-23", "relevancy": 2.1733, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5482}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5423}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Quality%20Cloud-Free%20Optical%20Image%20Synthesis%20Using%20Multi-Temporal%20SAR%0A%20%20and%20Contaminated%20Optical%20Data&body=Title%3A%20High-Quality%20Cloud-Free%20Optical%20Image%20Synthesis%20Using%20Multi-Temporal%20SAR%0A%20%20and%20Contaminated%20Optical%20Data%0AAuthor%3A%20Chenxi%20Duan%0AAbstract%3A%20%20%20Addressing%20gaps%20caused%20by%20cloud%20cover%20and%20the%20long%20revisit%20cycle%20of%0Asatellites%20is%20vital%20for%20providing%20essential%20data%20to%20support%20remote%20sensing%0Aapplications.%20This%20paper%20tackles%20the%20challenges%20of%20missing%20optical%20data%0Asynthesis%2C%20particularly%20in%20complex%20scenarios%20with%20cloud%20cover.%20We%20propose%0ACRSynthNet%2C%20a%20novel%20image%20synthesis%20network%20that%20incorporates%20innovative%0Adesigned%20modules%20such%20as%20the%20DownUp%20Block%20and%20Fusion%20Attention%20to%20enhance%0Aaccuracy.%20Experimental%20results%20validate%20the%20effectiveness%20of%20CRSynthNet%2C%0Ademonstrating%20substantial%20improvements%20in%20restoring%20structural%20details%2C%0Apreserving%20spectral%20consist%2C%20and%20achieving%20superior%20visual%20effects%20that%20far%0Aexceed%20those%20produced%20by%20comparison%20methods.%20It%20achieves%20quantitative%0Aimprovements%20across%20multiple%20metrics%3A%20a%20peak%20signal-to-noise%20ratio%20%28PSNR%29%20of%0A26.978%2C%20a%20structural%20similarity%20index%20measure%20%28SSIM%29%20of%200.648%2C%20and%20a%20root%20mean%0Asquare%20error%20%28RMSE%29%20of%200.050.%20Furthermore%2C%20this%20study%20creates%20the%20TCSEN12%0Adataset%2C%20a%20valuable%20resource%20specifically%20designed%20to%20address%20cloud%20cover%0Achallenges%20in%20missing%20optical%20data%20synthesis%20study.%20The%20dataset%20uniquely%0Aincludes%20cloud-covered%20images%20and%20leverages%20earlier%20image%20to%20predict%20later%0Aimage%2C%20offering%20a%20realistic%20representation%20of%20real-world%20scenarios.%20This%20study%0Aoffer%20practical%20method%20and%20valuable%20resources%20for%20optical%20satellite%20image%0Asynthesis%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Quality%2520Cloud-Free%2520Optical%2520Image%2520Synthesis%2520Using%2520Multi-Temporal%2520SAR%250A%2520%2520and%2520Contaminated%2520Optical%2520Data%26entry.906535625%3DChenxi%2520Duan%26entry.1292438233%3D%2520%2520Addressing%2520gaps%2520caused%2520by%2520cloud%2520cover%2520and%2520the%2520long%2520revisit%2520cycle%2520of%250Asatellites%2520is%2520vital%2520for%2520providing%2520essential%2520data%2520to%2520support%2520remote%2520sensing%250Aapplications.%2520This%2520paper%2520tackles%2520the%2520challenges%2520of%2520missing%2520optical%2520data%250Asynthesis%252C%2520particularly%2520in%2520complex%2520scenarios%2520with%2520cloud%2520cover.%2520We%2520propose%250ACRSynthNet%252C%2520a%2520novel%2520image%2520synthesis%2520network%2520that%2520incorporates%2520innovative%250Adesigned%2520modules%2520such%2520as%2520the%2520DownUp%2520Block%2520and%2520Fusion%2520Attention%2520to%2520enhance%250Aaccuracy.%2520Experimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520CRSynthNet%252C%250Ademonstrating%2520substantial%2520improvements%2520in%2520restoring%2520structural%2520details%252C%250Apreserving%2520spectral%2520consist%252C%2520and%2520achieving%2520superior%2520visual%2520effects%2520that%2520far%250Aexceed%2520those%2520produced%2520by%2520comparison%2520methods.%2520It%2520achieves%2520quantitative%250Aimprovements%2520across%2520multiple%2520metrics%253A%2520a%2520peak%2520signal-to-noise%2520ratio%2520%2528PSNR%2529%2520of%250A26.978%252C%2520a%2520structural%2520similarity%2520index%2520measure%2520%2528SSIM%2529%2520of%25200.648%252C%2520and%2520a%2520root%2520mean%250Asquare%2520error%2520%2528RMSE%2529%2520of%25200.050.%2520Furthermore%252C%2520this%2520study%2520creates%2520the%2520TCSEN12%250Adataset%252C%2520a%2520valuable%2520resource%2520specifically%2520designed%2520to%2520address%2520cloud%2520cover%250Achallenges%2520in%2520missing%2520optical%2520data%2520synthesis%2520study.%2520The%2520dataset%2520uniquely%250Aincludes%2520cloud-covered%2520images%2520and%2520leverages%2520earlier%2520image%2520to%2520predict%2520later%250Aimage%252C%2520offering%2520a%2520realistic%2520representation%2520of%2520real-world%2520scenarios.%2520This%2520study%250Aoffer%2520practical%2520method%2520and%2520valuable%2520resources%2520for%2520optical%2520satellite%2520image%250Asynthesis%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Quality%20Cloud-Free%20Optical%20Image%20Synthesis%20Using%20Multi-Temporal%20SAR%0A%20%20and%20Contaminated%20Optical%20Data&entry.906535625=Chenxi%20Duan&entry.1292438233=%20%20Addressing%20gaps%20caused%20by%20cloud%20cover%20and%20the%20long%20revisit%20cycle%20of%0Asatellites%20is%20vital%20for%20providing%20essential%20data%20to%20support%20remote%20sensing%0Aapplications.%20This%20paper%20tackles%20the%20challenges%20of%20missing%20optical%20data%0Asynthesis%2C%20particularly%20in%20complex%20scenarios%20with%20cloud%20cover.%20We%20propose%0ACRSynthNet%2C%20a%20novel%20image%20synthesis%20network%20that%20incorporates%20innovative%0Adesigned%20modules%20such%20as%20the%20DownUp%20Block%20and%20Fusion%20Attention%20to%20enhance%0Aaccuracy.%20Experimental%20results%20validate%20the%20effectiveness%20of%20CRSynthNet%2C%0Ademonstrating%20substantial%20improvements%20in%20restoring%20structural%20details%2C%0Apreserving%20spectral%20consist%2C%20and%20achieving%20superior%20visual%20effects%20that%20far%0Aexceed%20those%20produced%20by%20comparison%20methods.%20It%20achieves%20quantitative%0Aimprovements%20across%20multiple%20metrics%3A%20a%20peak%20signal-to-noise%20ratio%20%28PSNR%29%20of%0A26.978%2C%20a%20structural%20similarity%20index%20measure%20%28SSIM%29%20of%200.648%2C%20and%20a%20root%20mean%0Asquare%20error%20%28RMSE%29%20of%200.050.%20Furthermore%2C%20this%20study%20creates%20the%20TCSEN12%0Adataset%2C%20a%20valuable%20resource%20specifically%20designed%20to%20address%20cloud%20cover%0Achallenges%20in%20missing%20optical%20data%20synthesis%20study.%20The%20dataset%20uniquely%0Aincludes%20cloud-covered%20images%20and%20leverages%20earlier%20image%20to%20predict%20later%0Aimage%2C%20offering%20a%20realistic%20representation%20of%20real-world%20scenarios.%20This%20study%0Aoffer%20practical%20method%20and%20valuable%20resources%20for%20optical%20satellite%20image%0Asynthesis%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16870v1&entry.124074799=Read"},
{"title": "A Novel Adaptive Hybrid Focal-Entropy Loss for Enhancing Diabetic\n  Retinopathy Detection Using Convolutional Neural Networks", "author": "Santhosh Malarvannan and Pandiyaraju V and Shravan Venkatraman and Abeshek A and Priyadarshini B and Kannan A", "abstract": "  Diabetic retinopathy is a leading cause of blindness around the world and\ndemands precise AI-based diagnostic tools. Traditional loss functions in\nmulti-class classification, such as Categorical Cross-Entropy (CCE), are very\ncommon but break down with class imbalance, especially in cases with inherently\nchallenging or overlapping classes, which leads to biased and less sensitive\nmodels. Since a heavy imbalance exists in the number of examples for higher\nseverity stage 4 diabetic retinopathy, etc., classes compared to those very\nearly stages like class 0, achieving class balance is key. For this purpose, we\npropose the Adaptive Hybrid Focal-Entropy Loss which combines the ideas of\nfocal loss and entropy loss with adaptive weighting in order to focus on\nminority classes and highlight the challenging samples. The state-of-the art\nmodels applied for diabetic retinopathy detection with AHFE revealed good\nperformance improvements, indicating the top performances of ResNet50 at\n99.79%, DenseNet121 at 98.86%, Xception at 98.92%, MobileNetV2 at 97.84%, and\nInceptionV3 at 93.62% accuracy. This sheds light into how AHFE promotes\nenhancement in AI-driven diagnostics for complex and imbalanced medical\ndatasets.\n", "link": "http://arxiv.org/abs/2411.10843v2", "date": "2025-04-23", "relevancy": 2.1575, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5829}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5137}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Adaptive%20Hybrid%20Focal-Entropy%20Loss%20for%20Enhancing%20Diabetic%0A%20%20Retinopathy%20Detection%20Using%20Convolutional%20Neural%20Networks&body=Title%3A%20A%20Novel%20Adaptive%20Hybrid%20Focal-Entropy%20Loss%20for%20Enhancing%20Diabetic%0A%20%20Retinopathy%20Detection%20Using%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Santhosh%20Malarvannan%20and%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Priyadarshini%20B%20and%20Kannan%20A%0AAbstract%3A%20%20%20Diabetic%20retinopathy%20is%20a%20leading%20cause%20of%20blindness%20around%20the%20world%20and%0Ademands%20precise%20AI-based%20diagnostic%20tools.%20Traditional%20loss%20functions%20in%0Amulti-class%20classification%2C%20such%20as%20Categorical%20Cross-Entropy%20%28CCE%29%2C%20are%20very%0Acommon%20but%20break%20down%20with%20class%20imbalance%2C%20especially%20in%20cases%20with%20inherently%0Achallenging%20or%20overlapping%20classes%2C%20which%20leads%20to%20biased%20and%20less%20sensitive%0Amodels.%20Since%20a%20heavy%20imbalance%20exists%20in%20the%20number%20of%20examples%20for%20higher%0Aseverity%20stage%204%20diabetic%20retinopathy%2C%20etc.%2C%20classes%20compared%20to%20those%20very%0Aearly%20stages%20like%20class%200%2C%20achieving%20class%20balance%20is%20key.%20For%20this%20purpose%2C%20we%0Apropose%20the%20Adaptive%20Hybrid%20Focal-Entropy%20Loss%20which%20combines%20the%20ideas%20of%0Afocal%20loss%20and%20entropy%20loss%20with%20adaptive%20weighting%20in%20order%20to%20focus%20on%0Aminority%20classes%20and%20highlight%20the%20challenging%20samples.%20The%20state-of-the%20art%0Amodels%20applied%20for%20diabetic%20retinopathy%20detection%20with%20AHFE%20revealed%20good%0Aperformance%20improvements%2C%20indicating%20the%20top%20performances%20of%20ResNet50%20at%0A99.79%25%2C%20DenseNet121%20at%2098.86%25%2C%20Xception%20at%2098.92%25%2C%20MobileNetV2%20at%2097.84%25%2C%20and%0AInceptionV3%20at%2093.62%25%20accuracy.%20This%20sheds%20light%20into%20how%20AHFE%20promotes%0Aenhancement%20in%20AI-driven%20diagnostics%20for%20complex%20and%20imbalanced%20medical%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Adaptive%2520Hybrid%2520Focal-Entropy%2520Loss%2520for%2520Enhancing%2520Diabetic%250A%2520%2520Retinopathy%2520Detection%2520Using%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DSanthosh%2520Malarvannan%2520and%2520Pandiyaraju%2520V%2520and%2520Shravan%2520Venkatraman%2520and%2520Abeshek%2520A%2520and%2520Priyadarshini%2520B%2520and%2520Kannan%2520A%26entry.1292438233%3D%2520%2520Diabetic%2520retinopathy%2520is%2520a%2520leading%2520cause%2520of%2520blindness%2520around%2520the%2520world%2520and%250Ademands%2520precise%2520AI-based%2520diagnostic%2520tools.%2520Traditional%2520loss%2520functions%2520in%250Amulti-class%2520classification%252C%2520such%2520as%2520Categorical%2520Cross-Entropy%2520%2528CCE%2529%252C%2520are%2520very%250Acommon%2520but%2520break%2520down%2520with%2520class%2520imbalance%252C%2520especially%2520in%2520cases%2520with%2520inherently%250Achallenging%2520or%2520overlapping%2520classes%252C%2520which%2520leads%2520to%2520biased%2520and%2520less%2520sensitive%250Amodels.%2520Since%2520a%2520heavy%2520imbalance%2520exists%2520in%2520the%2520number%2520of%2520examples%2520for%2520higher%250Aseverity%2520stage%25204%2520diabetic%2520retinopathy%252C%2520etc.%252C%2520classes%2520compared%2520to%2520those%2520very%250Aearly%2520stages%2520like%2520class%25200%252C%2520achieving%2520class%2520balance%2520is%2520key.%2520For%2520this%2520purpose%252C%2520we%250Apropose%2520the%2520Adaptive%2520Hybrid%2520Focal-Entropy%2520Loss%2520which%2520combines%2520the%2520ideas%2520of%250Afocal%2520loss%2520and%2520entropy%2520loss%2520with%2520adaptive%2520weighting%2520in%2520order%2520to%2520focus%2520on%250Aminority%2520classes%2520and%2520highlight%2520the%2520challenging%2520samples.%2520The%2520state-of-the%2520art%250Amodels%2520applied%2520for%2520diabetic%2520retinopathy%2520detection%2520with%2520AHFE%2520revealed%2520good%250Aperformance%2520improvements%252C%2520indicating%2520the%2520top%2520performances%2520of%2520ResNet50%2520at%250A99.79%2525%252C%2520DenseNet121%2520at%252098.86%2525%252C%2520Xception%2520at%252098.92%2525%252C%2520MobileNetV2%2520at%252097.84%2525%252C%2520and%250AInceptionV3%2520at%252093.62%2525%2520accuracy.%2520This%2520sheds%2520light%2520into%2520how%2520AHFE%2520promotes%250Aenhancement%2520in%2520AI-driven%2520diagnostics%2520for%2520complex%2520and%2520imbalanced%2520medical%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Adaptive%20Hybrid%20Focal-Entropy%20Loss%20for%20Enhancing%20Diabetic%0A%20%20Retinopathy%20Detection%20Using%20Convolutional%20Neural%20Networks&entry.906535625=Santhosh%20Malarvannan%20and%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Priyadarshini%20B%20and%20Kannan%20A&entry.1292438233=%20%20Diabetic%20retinopathy%20is%20a%20leading%20cause%20of%20blindness%20around%20the%20world%20and%0Ademands%20precise%20AI-based%20diagnostic%20tools.%20Traditional%20loss%20functions%20in%0Amulti-class%20classification%2C%20such%20as%20Categorical%20Cross-Entropy%20%28CCE%29%2C%20are%20very%0Acommon%20but%20break%20down%20with%20class%20imbalance%2C%20especially%20in%20cases%20with%20inherently%0Achallenging%20or%20overlapping%20classes%2C%20which%20leads%20to%20biased%20and%20less%20sensitive%0Amodels.%20Since%20a%20heavy%20imbalance%20exists%20in%20the%20number%20of%20examples%20for%20higher%0Aseverity%20stage%204%20diabetic%20retinopathy%2C%20etc.%2C%20classes%20compared%20to%20those%20very%0Aearly%20stages%20like%20class%200%2C%20achieving%20class%20balance%20is%20key.%20For%20this%20purpose%2C%20we%0Apropose%20the%20Adaptive%20Hybrid%20Focal-Entropy%20Loss%20which%20combines%20the%20ideas%20of%0Afocal%20loss%20and%20entropy%20loss%20with%20adaptive%20weighting%20in%20order%20to%20focus%20on%0Aminority%20classes%20and%20highlight%20the%20challenging%20samples.%20The%20state-of-the%20art%0Amodels%20applied%20for%20diabetic%20retinopathy%20detection%20with%20AHFE%20revealed%20good%0Aperformance%20improvements%2C%20indicating%20the%20top%20performances%20of%20ResNet50%20at%0A99.79%25%2C%20DenseNet121%20at%2098.86%25%2C%20Xception%20at%2098.92%25%2C%20MobileNetV2%20at%2097.84%25%2C%20and%0AInceptionV3%20at%2093.62%25%20accuracy.%20This%20sheds%20light%20into%20how%20AHFE%20promotes%0Aenhancement%20in%20AI-driven%20diagnostics%20for%20complex%20and%20imbalanced%20medical%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10843v2&entry.124074799=Read"},
{"title": "SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from\n  Unstructured Clinical Narratives in Epilepsy", "author": "Meghal Dani and Muthu Jeyanthi Prakash and Zeynep Akata and Stefanie Liebe", "abstract": "  Large Language Models (LLMs) have been shown to encode clinical knowledge.\nMany evaluations, however, rely on structured question-answer benchmarks,\noverlooking critical challenges of interpreting and reasoning about\nunstructured clinical narratives in real-world settings. Using free-text\nclinical descriptions, we present SemioLLM, an evaluation framework that\nbenchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B,\nLlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of\n1,269 seizure descriptions, we show that most LLMs are able to accurately and\nconfidently generate probabilistic predictions of seizure onset zones in the\nbrain. Most models approach clinician-level performance after prompt\nengineering, with expert-guided chain-of-thought reasoning leading to the most\nconsistent improvements. Performance was further strongly modulated by clinical\nin-context impersonation, narrative length and language context (13.7%, 32.7%\nand 14.2% performance variation, respectively). However, expert analysis of\nreasoning outputs revealed that correct prediction can be based on hallucinated\nknowledge and deficient source citation accuracy, underscoring the need to\nimprove interpretability of LLMs in clinical use. Overall, SemioLLM provides a\nscalable, domain-adaptable framework for evaluating LLMs in clinical\ndisciplines where unstructured verbal descriptions encode diagnostic\ninformation. By identifying both the strengths and limitations of\nstate-of-the-art models, our work supports the development of clinically robust\nand globally applicable AI systems for healthcare.\n", "link": "http://arxiv.org/abs/2407.03004v2", "date": "2025-04-23", "relevancy": 2.1549, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemioLLM%3A%20Evaluating%20Large%20Language%20Models%20for%20Diagnostic%20Reasoning%20from%0A%20%20Unstructured%20Clinical%20Narratives%20in%20Epilepsy&body=Title%3A%20SemioLLM%3A%20Evaluating%20Large%20Language%20Models%20for%20Diagnostic%20Reasoning%20from%0A%20%20Unstructured%20Clinical%20Narratives%20in%20Epilepsy%0AAuthor%3A%20Meghal%20Dani%20and%20Muthu%20Jeyanthi%20Prakash%20and%20Zeynep%20Akata%20and%20Stefanie%20Liebe%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20shown%20to%20encode%20clinical%20knowledge.%0AMany%20evaluations%2C%20however%2C%20rely%20on%20structured%20question-answer%20benchmarks%2C%0Aoverlooking%20critical%20challenges%20of%20interpreting%20and%20reasoning%20about%0Aunstructured%20clinical%20narratives%20in%20real-world%20settings.%20Using%20free-text%0Aclinical%20descriptions%2C%20we%20present%20SemioLLM%2C%20an%20evaluation%20framework%20that%0Abenchmarks%206%20state-of-the-art%20models%20%28GPT-3.5%2C%20GPT-4%2C%20Mixtral-8x7B%2C%20Qwen-72B%2C%0ALlaMa2%2C%20LlaMa3%29%20on%20a%20core%20diagnostic%20task%20in%20epilepsy.%20Leveraging%20a%20database%20of%0A1%2C269%20seizure%20descriptions%2C%20we%20show%20that%20most%20LLMs%20are%20able%20to%20accurately%20and%0Aconfidently%20generate%20probabilistic%20predictions%20of%20seizure%20onset%20zones%20in%20the%0Abrain.%20Most%20models%20approach%20clinician-level%20performance%20after%20prompt%0Aengineering%2C%20with%20expert-guided%20chain-of-thought%20reasoning%20leading%20to%20the%20most%0Aconsistent%20improvements.%20Performance%20was%20further%20strongly%20modulated%20by%20clinical%0Ain-context%20impersonation%2C%20narrative%20length%20and%20language%20context%20%2813.7%25%2C%2032.7%25%0Aand%2014.2%25%20performance%20variation%2C%20respectively%29.%20However%2C%20expert%20analysis%20of%0Areasoning%20outputs%20revealed%20that%20correct%20prediction%20can%20be%20based%20on%20hallucinated%0Aknowledge%20and%20deficient%20source%20citation%20accuracy%2C%20underscoring%20the%20need%20to%0Aimprove%20interpretability%20of%20LLMs%20in%20clinical%20use.%20Overall%2C%20SemioLLM%20provides%20a%0Ascalable%2C%20domain-adaptable%20framework%20for%20evaluating%20LLMs%20in%20clinical%0Adisciplines%20where%20unstructured%20verbal%20descriptions%20encode%20diagnostic%0Ainformation.%20By%20identifying%20both%20the%20strengths%20and%20limitations%20of%0Astate-of-the-art%20models%2C%20our%20work%20supports%20the%20development%20of%20clinically%20robust%0Aand%20globally%20applicable%20AI%20systems%20for%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemioLLM%253A%2520Evaluating%2520Large%2520Language%2520Models%2520for%2520Diagnostic%2520Reasoning%2520from%250A%2520%2520Unstructured%2520Clinical%2520Narratives%2520in%2520Epilepsy%26entry.906535625%3DMeghal%2520Dani%2520and%2520Muthu%2520Jeyanthi%2520Prakash%2520and%2520Zeynep%2520Akata%2520and%2520Stefanie%2520Liebe%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520shown%2520to%2520encode%2520clinical%2520knowledge.%250AMany%2520evaluations%252C%2520however%252C%2520rely%2520on%2520structured%2520question-answer%2520benchmarks%252C%250Aoverlooking%2520critical%2520challenges%2520of%2520interpreting%2520and%2520reasoning%2520about%250Aunstructured%2520clinical%2520narratives%2520in%2520real-world%2520settings.%2520Using%2520free-text%250Aclinical%2520descriptions%252C%2520we%2520present%2520SemioLLM%252C%2520an%2520evaluation%2520framework%2520that%250Abenchmarks%25206%2520state-of-the-art%2520models%2520%2528GPT-3.5%252C%2520GPT-4%252C%2520Mixtral-8x7B%252C%2520Qwen-72B%252C%250ALlaMa2%252C%2520LlaMa3%2529%2520on%2520a%2520core%2520diagnostic%2520task%2520in%2520epilepsy.%2520Leveraging%2520a%2520database%2520of%250A1%252C269%2520seizure%2520descriptions%252C%2520we%2520show%2520that%2520most%2520LLMs%2520are%2520able%2520to%2520accurately%2520and%250Aconfidently%2520generate%2520probabilistic%2520predictions%2520of%2520seizure%2520onset%2520zones%2520in%2520the%250Abrain.%2520Most%2520models%2520approach%2520clinician-level%2520performance%2520after%2520prompt%250Aengineering%252C%2520with%2520expert-guided%2520chain-of-thought%2520reasoning%2520leading%2520to%2520the%2520most%250Aconsistent%2520improvements.%2520Performance%2520was%2520further%2520strongly%2520modulated%2520by%2520clinical%250Ain-context%2520impersonation%252C%2520narrative%2520length%2520and%2520language%2520context%2520%252813.7%2525%252C%252032.7%2525%250Aand%252014.2%2525%2520performance%2520variation%252C%2520respectively%2529.%2520However%252C%2520expert%2520analysis%2520of%250Areasoning%2520outputs%2520revealed%2520that%2520correct%2520prediction%2520can%2520be%2520based%2520on%2520hallucinated%250Aknowledge%2520and%2520deficient%2520source%2520citation%2520accuracy%252C%2520underscoring%2520the%2520need%2520to%250Aimprove%2520interpretability%2520of%2520LLMs%2520in%2520clinical%2520use.%2520Overall%252C%2520SemioLLM%2520provides%2520a%250Ascalable%252C%2520domain-adaptable%2520framework%2520for%2520evaluating%2520LLMs%2520in%2520clinical%250Adisciplines%2520where%2520unstructured%2520verbal%2520descriptions%2520encode%2520diagnostic%250Ainformation.%2520By%2520identifying%2520both%2520the%2520strengths%2520and%2520limitations%2520of%250Astate-of-the-art%2520models%252C%2520our%2520work%2520supports%2520the%2520development%2520of%2520clinically%2520robust%250Aand%2520globally%2520applicable%2520AI%2520systems%2520for%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemioLLM%3A%20Evaluating%20Large%20Language%20Models%20for%20Diagnostic%20Reasoning%20from%0A%20%20Unstructured%20Clinical%20Narratives%20in%20Epilepsy&entry.906535625=Meghal%20Dani%20and%20Muthu%20Jeyanthi%20Prakash%20and%20Zeynep%20Akata%20and%20Stefanie%20Liebe&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20shown%20to%20encode%20clinical%20knowledge.%0AMany%20evaluations%2C%20however%2C%20rely%20on%20structured%20question-answer%20benchmarks%2C%0Aoverlooking%20critical%20challenges%20of%20interpreting%20and%20reasoning%20about%0Aunstructured%20clinical%20narratives%20in%20real-world%20settings.%20Using%20free-text%0Aclinical%20descriptions%2C%20we%20present%20SemioLLM%2C%20an%20evaluation%20framework%20that%0Abenchmarks%206%20state-of-the-art%20models%20%28GPT-3.5%2C%20GPT-4%2C%20Mixtral-8x7B%2C%20Qwen-72B%2C%0ALlaMa2%2C%20LlaMa3%29%20on%20a%20core%20diagnostic%20task%20in%20epilepsy.%20Leveraging%20a%20database%20of%0A1%2C269%20seizure%20descriptions%2C%20we%20show%20that%20most%20LLMs%20are%20able%20to%20accurately%20and%0Aconfidently%20generate%20probabilistic%20predictions%20of%20seizure%20onset%20zones%20in%20the%0Abrain.%20Most%20models%20approach%20clinician-level%20performance%20after%20prompt%0Aengineering%2C%20with%20expert-guided%20chain-of-thought%20reasoning%20leading%20to%20the%20most%0Aconsistent%20improvements.%20Performance%20was%20further%20strongly%20modulated%20by%20clinical%0Ain-context%20impersonation%2C%20narrative%20length%20and%20language%20context%20%2813.7%25%2C%2032.7%25%0Aand%2014.2%25%20performance%20variation%2C%20respectively%29.%20However%2C%20expert%20analysis%20of%0Areasoning%20outputs%20revealed%20that%20correct%20prediction%20can%20be%20based%20on%20hallucinated%0Aknowledge%20and%20deficient%20source%20citation%20accuracy%2C%20underscoring%20the%20need%20to%0Aimprove%20interpretability%20of%20LLMs%20in%20clinical%20use.%20Overall%2C%20SemioLLM%20provides%20a%0Ascalable%2C%20domain-adaptable%20framework%20for%20evaluating%20LLMs%20in%20clinical%0Adisciplines%20where%20unstructured%20verbal%20descriptions%20encode%20diagnostic%0Ainformation.%20By%20identifying%20both%20the%20strengths%20and%20limitations%20of%0Astate-of-the-art%20models%2C%20our%20work%20supports%20the%20development%20of%20clinically%20robust%0Aand%20globally%20applicable%20AI%20systems%20for%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03004v2&entry.124074799=Read"},
{"title": "Evaluating Autoencoders for Parametric and Invertible Multidimensional\n  Projections", "author": "Frederik L. Dennig and Nina Geyer and Daniela Blumberg and Yannick Metz and Daniel A. Keim", "abstract": "  Recently, neural networks have gained attention for creating parametric and\ninvertible multidimensional data projections. Parametric projections allow for\nembedding previously unseen data without recomputing the projection as a whole,\nwhile invertible projections enable the generation of new data points. However,\nthese properties have never been explored simultaneously for arbitrary\nprojection methods. We evaluate three autoencoder (AE) architectures for\ncreating parametric and invertible projections. Based on a given projection, we\ntrain AEs to learn a mapping into 2D space and an inverse mapping into the\noriginal space. We perform a quantitative and qualitative comparison on four\ndatasets of varying dimensionality and pattern complexity using t-SNE. Our\nresults indicate that AEs with a customized loss function can create smoother\nparametric and inverse projections than feed-forward neural networks while\ngiving users control over the strength of the smoothing effect.\n", "link": "http://arxiv.org/abs/2504.16831v1", "date": "2025-04-23", "relevancy": 2.1543, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.594}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5056}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Autoencoders%20for%20Parametric%20and%20Invertible%20Multidimensional%0A%20%20Projections&body=Title%3A%20Evaluating%20Autoencoders%20for%20Parametric%20and%20Invertible%20Multidimensional%0A%20%20Projections%0AAuthor%3A%20Frederik%20L.%20Dennig%20and%20Nina%20Geyer%20and%20Daniela%20Blumberg%20and%20Yannick%20Metz%20and%20Daniel%20A.%20Keim%0AAbstract%3A%20%20%20Recently%2C%20neural%20networks%20have%20gained%20attention%20for%20creating%20parametric%20and%0Ainvertible%20multidimensional%20data%20projections.%20Parametric%20projections%20allow%20for%0Aembedding%20previously%20unseen%20data%20without%20recomputing%20the%20projection%20as%20a%20whole%2C%0Awhile%20invertible%20projections%20enable%20the%20generation%20of%20new%20data%20points.%20However%2C%0Athese%20properties%20have%20never%20been%20explored%20simultaneously%20for%20arbitrary%0Aprojection%20methods.%20We%20evaluate%20three%20autoencoder%20%28AE%29%20architectures%20for%0Acreating%20parametric%20and%20invertible%20projections.%20Based%20on%20a%20given%20projection%2C%20we%0Atrain%20AEs%20to%20learn%20a%20mapping%20into%202D%20space%20and%20an%20inverse%20mapping%20into%20the%0Aoriginal%20space.%20We%20perform%20a%20quantitative%20and%20qualitative%20comparison%20on%20four%0Adatasets%20of%20varying%20dimensionality%20and%20pattern%20complexity%20using%20t-SNE.%20Our%0Aresults%20indicate%20that%20AEs%20with%20a%20customized%20loss%20function%20can%20create%20smoother%0Aparametric%20and%20inverse%20projections%20than%20feed-forward%20neural%20networks%20while%0Agiving%20users%20control%20over%20the%20strength%20of%20the%20smoothing%20effect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Autoencoders%2520for%2520Parametric%2520and%2520Invertible%2520Multidimensional%250A%2520%2520Projections%26entry.906535625%3DFrederik%2520L.%2520Dennig%2520and%2520Nina%2520Geyer%2520and%2520Daniela%2520Blumberg%2520and%2520Yannick%2520Metz%2520and%2520Daniel%2520A.%2520Keim%26entry.1292438233%3D%2520%2520Recently%252C%2520neural%2520networks%2520have%2520gained%2520attention%2520for%2520creating%2520parametric%2520and%250Ainvertible%2520multidimensional%2520data%2520projections.%2520Parametric%2520projections%2520allow%2520for%250Aembedding%2520previously%2520unseen%2520data%2520without%2520recomputing%2520the%2520projection%2520as%2520a%2520whole%252C%250Awhile%2520invertible%2520projections%2520enable%2520the%2520generation%2520of%2520new%2520data%2520points.%2520However%252C%250Athese%2520properties%2520have%2520never%2520been%2520explored%2520simultaneously%2520for%2520arbitrary%250Aprojection%2520methods.%2520We%2520evaluate%2520three%2520autoencoder%2520%2528AE%2529%2520architectures%2520for%250Acreating%2520parametric%2520and%2520invertible%2520projections.%2520Based%2520on%2520a%2520given%2520projection%252C%2520we%250Atrain%2520AEs%2520to%2520learn%2520a%2520mapping%2520into%25202D%2520space%2520and%2520an%2520inverse%2520mapping%2520into%2520the%250Aoriginal%2520space.%2520We%2520perform%2520a%2520quantitative%2520and%2520qualitative%2520comparison%2520on%2520four%250Adatasets%2520of%2520varying%2520dimensionality%2520and%2520pattern%2520complexity%2520using%2520t-SNE.%2520Our%250Aresults%2520indicate%2520that%2520AEs%2520with%2520a%2520customized%2520loss%2520function%2520can%2520create%2520smoother%250Aparametric%2520and%2520inverse%2520projections%2520than%2520feed-forward%2520neural%2520networks%2520while%250Agiving%2520users%2520control%2520over%2520the%2520strength%2520of%2520the%2520smoothing%2520effect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Autoencoders%20for%20Parametric%20and%20Invertible%20Multidimensional%0A%20%20Projections&entry.906535625=Frederik%20L.%20Dennig%20and%20Nina%20Geyer%20and%20Daniela%20Blumberg%20and%20Yannick%20Metz%20and%20Daniel%20A.%20Keim&entry.1292438233=%20%20Recently%2C%20neural%20networks%20have%20gained%20attention%20for%20creating%20parametric%20and%0Ainvertible%20multidimensional%20data%20projections.%20Parametric%20projections%20allow%20for%0Aembedding%20previously%20unseen%20data%20without%20recomputing%20the%20projection%20as%20a%20whole%2C%0Awhile%20invertible%20projections%20enable%20the%20generation%20of%20new%20data%20points.%20However%2C%0Athese%20properties%20have%20never%20been%20explored%20simultaneously%20for%20arbitrary%0Aprojection%20methods.%20We%20evaluate%20three%20autoencoder%20%28AE%29%20architectures%20for%0Acreating%20parametric%20and%20invertible%20projections.%20Based%20on%20a%20given%20projection%2C%20we%0Atrain%20AEs%20to%20learn%20a%20mapping%20into%202D%20space%20and%20an%20inverse%20mapping%20into%20the%0Aoriginal%20space.%20We%20perform%20a%20quantitative%20and%20qualitative%20comparison%20on%20four%0Adatasets%20of%20varying%20dimensionality%20and%20pattern%20complexity%20using%20t-SNE.%20Our%0Aresults%20indicate%20that%20AEs%20with%20a%20customized%20loss%20function%20can%20create%20smoother%0Aparametric%20and%20inverse%20projections%20than%20feed-forward%20neural%20networks%20while%0Agiving%20users%20control%20over%20the%20strength%20of%20the%20smoothing%20effect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16831v1&entry.124074799=Read"},
{"title": "Prompt-Tuning SAM: From Generalist to Specialist with only 2048\n  Parameters and 16 Training Images", "author": "Tristan Piater and Bj\u00f6rn Barz and Alexander Freytag", "abstract": "  The Segment Anything Model (SAM) is widely used for segmenting a diverse\nrange of objects in natural images from simple user prompts like points or\nbounding boxes. However, SAM's performance decreases substantially when applied\nto non-natural domains like microscopic imaging. Furthermore, due to SAM's\ninteractive design, it requires a precise prompt for each image and object,\nwhich is unfeasible in many automated biomedical applications. Previous\nsolutions adapt SAM by training millions of parameters via fine-tuning large\nparts of the model or of adapter layers. In contrast, we show that as little as\n2,048 additional parameters are sufficient for turning SAM into a use-case\nspecialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM)\nmethod uses prompt-tuning, a parameter-efficient fine-tuning technique, to\nadapt SAM for a specific task. We validate the performance of our approach on\nmultiple microscopic and one medical dataset. Our results show that\nprompt-tuning only SAM's mask decoder already leads to a performance on-par\nwith state-of-the-art techniques while requiring roughly 2,000x less trainable\nparameters. For addressing domain gaps, we find that additionally prompt-tuning\nSAM's image encoder is beneficial, further improving segmentation accuracy by\nup to 18% over state-of-the-art results. Since PTSAM can be reliably trained\nwith as little as 16 annotated images, we find it particularly helpful for\napplications with limited training data and domain shifts.\n", "link": "http://arxiv.org/abs/2504.16739v1", "date": "2025-04-23", "relevancy": 2.1506, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5448}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5385}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-Tuning%20SAM%3A%20From%20Generalist%20to%20Specialist%20with%20only%202048%0A%20%20Parameters%20and%2016%20Training%20Images&body=Title%3A%20Prompt-Tuning%20SAM%3A%20From%20Generalist%20to%20Specialist%20with%20only%202048%0A%20%20Parameters%20and%2016%20Training%20Images%0AAuthor%3A%20Tristan%20Piater%20and%20Bj%C3%B6rn%20Barz%20and%20Alexander%20Freytag%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20is%20widely%20used%20for%20segmenting%20a%20diverse%0Arange%20of%20objects%20in%20natural%20images%20from%20simple%20user%20prompts%20like%20points%20or%0Abounding%20boxes.%20However%2C%20SAM%27s%20performance%20decreases%20substantially%20when%20applied%0Ato%20non-natural%20domains%20like%20microscopic%20imaging.%20Furthermore%2C%20due%20to%20SAM%27s%0Ainteractive%20design%2C%20it%20requires%20a%20precise%20prompt%20for%20each%20image%20and%20object%2C%0Awhich%20is%20unfeasible%20in%20many%20automated%20biomedical%20applications.%20Previous%0Asolutions%20adapt%20SAM%20by%20training%20millions%20of%20parameters%20via%20fine-tuning%20large%0Aparts%20of%20the%20model%20or%20of%20adapter%20layers.%20In%20contrast%2C%20we%20show%20that%20as%20little%20as%0A2%2C048%20additional%20parameters%20are%20sufficient%20for%20turning%20SAM%20into%20a%20use-case%0Aspecialist%20for%20a%20certain%20downstream%20task.%20Our%20novel%20PTSAM%20%28prompt-tuned%20SAM%29%0Amethod%20uses%20prompt-tuning%2C%20a%20parameter-efficient%20fine-tuning%20technique%2C%20to%0Aadapt%20SAM%20for%20a%20specific%20task.%20We%20validate%20the%20performance%20of%20our%20approach%20on%0Amultiple%20microscopic%20and%20one%20medical%20dataset.%20Our%20results%20show%20that%0Aprompt-tuning%20only%20SAM%27s%20mask%20decoder%20already%20leads%20to%20a%20performance%20on-par%0Awith%20state-of-the-art%20techniques%20while%20requiring%20roughly%202%2C000x%20less%20trainable%0Aparameters.%20For%20addressing%20domain%20gaps%2C%20we%20find%20that%20additionally%20prompt-tuning%0ASAM%27s%20image%20encoder%20is%20beneficial%2C%20further%20improving%20segmentation%20accuracy%20by%0Aup%20to%2018%25%20over%20state-of-the-art%20results.%20Since%20PTSAM%20can%20be%20reliably%20trained%0Awith%20as%20little%20as%2016%20annotated%20images%2C%20we%20find%20it%20particularly%20helpful%20for%0Aapplications%20with%20limited%20training%20data%20and%20domain%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-Tuning%2520SAM%253A%2520From%2520Generalist%2520to%2520Specialist%2520with%2520only%25202048%250A%2520%2520Parameters%2520and%252016%2520Training%2520Images%26entry.906535625%3DTristan%2520Piater%2520and%2520Bj%25C3%25B6rn%2520Barz%2520and%2520Alexander%2520Freytag%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520is%2520widely%2520used%2520for%2520segmenting%2520a%2520diverse%250Arange%2520of%2520objects%2520in%2520natural%2520images%2520from%2520simple%2520user%2520prompts%2520like%2520points%2520or%250Abounding%2520boxes.%2520However%252C%2520SAM%2527s%2520performance%2520decreases%2520substantially%2520when%2520applied%250Ato%2520non-natural%2520domains%2520like%2520microscopic%2520imaging.%2520Furthermore%252C%2520due%2520to%2520SAM%2527s%250Ainteractive%2520design%252C%2520it%2520requires%2520a%2520precise%2520prompt%2520for%2520each%2520image%2520and%2520object%252C%250Awhich%2520is%2520unfeasible%2520in%2520many%2520automated%2520biomedical%2520applications.%2520Previous%250Asolutions%2520adapt%2520SAM%2520by%2520training%2520millions%2520of%2520parameters%2520via%2520fine-tuning%2520large%250Aparts%2520of%2520the%2520model%2520or%2520of%2520adapter%2520layers.%2520In%2520contrast%252C%2520we%2520show%2520that%2520as%2520little%2520as%250A2%252C048%2520additional%2520parameters%2520are%2520sufficient%2520for%2520turning%2520SAM%2520into%2520a%2520use-case%250Aspecialist%2520for%2520a%2520certain%2520downstream%2520task.%2520Our%2520novel%2520PTSAM%2520%2528prompt-tuned%2520SAM%2529%250Amethod%2520uses%2520prompt-tuning%252C%2520a%2520parameter-efficient%2520fine-tuning%2520technique%252C%2520to%250Aadapt%2520SAM%2520for%2520a%2520specific%2520task.%2520We%2520validate%2520the%2520performance%2520of%2520our%2520approach%2520on%250Amultiple%2520microscopic%2520and%2520one%2520medical%2520dataset.%2520Our%2520results%2520show%2520that%250Aprompt-tuning%2520only%2520SAM%2527s%2520mask%2520decoder%2520already%2520leads%2520to%2520a%2520performance%2520on-par%250Awith%2520state-of-the-art%2520techniques%2520while%2520requiring%2520roughly%25202%252C000x%2520less%2520trainable%250Aparameters.%2520For%2520addressing%2520domain%2520gaps%252C%2520we%2520find%2520that%2520additionally%2520prompt-tuning%250ASAM%2527s%2520image%2520encoder%2520is%2520beneficial%252C%2520further%2520improving%2520segmentation%2520accuracy%2520by%250Aup%2520to%252018%2525%2520over%2520state-of-the-art%2520results.%2520Since%2520PTSAM%2520can%2520be%2520reliably%2520trained%250Awith%2520as%2520little%2520as%252016%2520annotated%2520images%252C%2520we%2520find%2520it%2520particularly%2520helpful%2520for%250Aapplications%2520with%2520limited%2520training%2520data%2520and%2520domain%2520shifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Tuning%20SAM%3A%20From%20Generalist%20to%20Specialist%20with%20only%202048%0A%20%20Parameters%20and%2016%20Training%20Images&entry.906535625=Tristan%20Piater%20and%20Bj%C3%B6rn%20Barz%20and%20Alexander%20Freytag&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20is%20widely%20used%20for%20segmenting%20a%20diverse%0Arange%20of%20objects%20in%20natural%20images%20from%20simple%20user%20prompts%20like%20points%20or%0Abounding%20boxes.%20However%2C%20SAM%27s%20performance%20decreases%20substantially%20when%20applied%0Ato%20non-natural%20domains%20like%20microscopic%20imaging.%20Furthermore%2C%20due%20to%20SAM%27s%0Ainteractive%20design%2C%20it%20requires%20a%20precise%20prompt%20for%20each%20image%20and%20object%2C%0Awhich%20is%20unfeasible%20in%20many%20automated%20biomedical%20applications.%20Previous%0Asolutions%20adapt%20SAM%20by%20training%20millions%20of%20parameters%20via%20fine-tuning%20large%0Aparts%20of%20the%20model%20or%20of%20adapter%20layers.%20In%20contrast%2C%20we%20show%20that%20as%20little%20as%0A2%2C048%20additional%20parameters%20are%20sufficient%20for%20turning%20SAM%20into%20a%20use-case%0Aspecialist%20for%20a%20certain%20downstream%20task.%20Our%20novel%20PTSAM%20%28prompt-tuned%20SAM%29%0Amethod%20uses%20prompt-tuning%2C%20a%20parameter-efficient%20fine-tuning%20technique%2C%20to%0Aadapt%20SAM%20for%20a%20specific%20task.%20We%20validate%20the%20performance%20of%20our%20approach%20on%0Amultiple%20microscopic%20and%20one%20medical%20dataset.%20Our%20results%20show%20that%0Aprompt-tuning%20only%20SAM%27s%20mask%20decoder%20already%20leads%20to%20a%20performance%20on-par%0Awith%20state-of-the-art%20techniques%20while%20requiring%20roughly%202%2C000x%20less%20trainable%0Aparameters.%20For%20addressing%20domain%20gaps%2C%20we%20find%20that%20additionally%20prompt-tuning%0ASAM%27s%20image%20encoder%20is%20beneficial%2C%20further%20improving%20segmentation%20accuracy%20by%0Aup%20to%2018%25%20over%20state-of-the-art%20results.%20Since%20PTSAM%20can%20be%20reliably%20trained%0Awith%20as%20little%20as%2016%20annotated%20images%2C%20we%20find%20it%20particularly%20helpful%20for%0Aapplications%20with%20limited%20training%20data%20and%20domain%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16739v1&entry.124074799=Read"},
{"title": "Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors\n  and Cross-Model Attention Mechanism", "author": "Lakshita Agarwal and Bindu Verma", "abstract": "  The examination of chest X-ray images is a crucial component in detecting\nvarious thoracic illnesses. This study introduces a new image description\ngeneration model that integrates a Vision Transformer (ViT) encoder with\ncross-modal attention and a GPT-4-based transformer decoder. The ViT captures\nhigh-quality visual features from chest X-rays, which are fused with text data\nthrough cross-modal attention to improve the accuracy, context, and richness of\nimage descriptions. The GPT-4 decoder transforms these fused features into\naccurate and relevant captions. The model was tested on the National Institutes\nof Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU\ndataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and\n0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all\nmetrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726),\nand ROUGE-L (0.705). This framework has the potential to enhance chest X-ray\nevaluation, assisting radiologists in more precise and efficient diagnosis.\n", "link": "http://arxiv.org/abs/2504.16774v1", "date": "2025-04-23", "relevancy": 2.148, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20Chest%20X-Ray%20Analysis%20via%20Transformer-Based%20Image%20Descriptors%0A%20%20and%20Cross-Model%20Attention%20Mechanism&body=Title%3A%20Advanced%20Chest%20X-Ray%20Analysis%20via%20Transformer-Based%20Image%20Descriptors%0A%20%20and%20Cross-Model%20Attention%20Mechanism%0AAuthor%3A%20Lakshita%20Agarwal%20and%20Bindu%20Verma%0AAbstract%3A%20%20%20The%20examination%20of%20chest%20X-ray%20images%20is%20a%20crucial%20component%20in%20detecting%0Avarious%20thoracic%20illnesses.%20This%20study%20introduces%20a%20new%20image%20description%0Ageneration%20model%20that%20integrates%20a%20Vision%20Transformer%20%28ViT%29%20encoder%20with%0Across-modal%20attention%20and%20a%20GPT-4-based%20transformer%20decoder.%20The%20ViT%20captures%0Ahigh-quality%20visual%20features%20from%20chest%20X-rays%2C%20which%20are%20fused%20with%20text%20data%0Athrough%20cross-modal%20attention%20to%20improve%20the%20accuracy%2C%20context%2C%20and%20richness%20of%0Aimage%20descriptions.%20The%20GPT-4%20decoder%20transforms%20these%20fused%20features%20into%0Aaccurate%20and%20relevant%20captions.%20The%20model%20was%20tested%20on%20the%20National%20Institutes%0Aof%20Health%20%28NIH%29%20and%20Indiana%20University%20%28IU%29%20Chest%20X-ray%20datasets.%20On%20the%20IU%0Adataset%2C%20it%20achieved%20scores%20of%200.854%20%28B-1%29%2C%200.883%20%28CIDEr%29%2C%200.759%20%28METEOR%29%2C%20and%0A0.712%20%28ROUGE-L%29.%20On%20the%20NIH%20dataset%2C%20it%20achieved%20the%20best%20performance%20on%20all%0Ametrics%3A%20BLEU%201--4%20%280.825%2C%200.788%2C%200.765%2C%200.752%29%2C%20CIDEr%20%280.857%29%2C%20METEOR%20%280.726%29%2C%0Aand%20ROUGE-L%20%280.705%29.%20This%20framework%20has%20the%20potential%20to%20enhance%20chest%20X-ray%0Aevaluation%2C%20assisting%20radiologists%20in%20more%20precise%20and%20efficient%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520Chest%2520X-Ray%2520Analysis%2520via%2520Transformer-Based%2520Image%2520Descriptors%250A%2520%2520and%2520Cross-Model%2520Attention%2520Mechanism%26entry.906535625%3DLakshita%2520Agarwal%2520and%2520Bindu%2520Verma%26entry.1292438233%3D%2520%2520The%2520examination%2520of%2520chest%2520X-ray%2520images%2520is%2520a%2520crucial%2520component%2520in%2520detecting%250Avarious%2520thoracic%2520illnesses.%2520This%2520study%2520introduces%2520a%2520new%2520image%2520description%250Ageneration%2520model%2520that%2520integrates%2520a%2520Vision%2520Transformer%2520%2528ViT%2529%2520encoder%2520with%250Across-modal%2520attention%2520and%2520a%2520GPT-4-based%2520transformer%2520decoder.%2520The%2520ViT%2520captures%250Ahigh-quality%2520visual%2520features%2520from%2520chest%2520X-rays%252C%2520which%2520are%2520fused%2520with%2520text%2520data%250Athrough%2520cross-modal%2520attention%2520to%2520improve%2520the%2520accuracy%252C%2520context%252C%2520and%2520richness%2520of%250Aimage%2520descriptions.%2520The%2520GPT-4%2520decoder%2520transforms%2520these%2520fused%2520features%2520into%250Aaccurate%2520and%2520relevant%2520captions.%2520The%2520model%2520was%2520tested%2520on%2520the%2520National%2520Institutes%250Aof%2520Health%2520%2528NIH%2529%2520and%2520Indiana%2520University%2520%2528IU%2529%2520Chest%2520X-ray%2520datasets.%2520On%2520the%2520IU%250Adataset%252C%2520it%2520achieved%2520scores%2520of%25200.854%2520%2528B-1%2529%252C%25200.883%2520%2528CIDEr%2529%252C%25200.759%2520%2528METEOR%2529%252C%2520and%250A0.712%2520%2528ROUGE-L%2529.%2520On%2520the%2520NIH%2520dataset%252C%2520it%2520achieved%2520the%2520best%2520performance%2520on%2520all%250Ametrics%253A%2520BLEU%25201--4%2520%25280.825%252C%25200.788%252C%25200.765%252C%25200.752%2529%252C%2520CIDEr%2520%25280.857%2529%252C%2520METEOR%2520%25280.726%2529%252C%250Aand%2520ROUGE-L%2520%25280.705%2529.%2520This%2520framework%2520has%2520the%2520potential%2520to%2520enhance%2520chest%2520X-ray%250Aevaluation%252C%2520assisting%2520radiologists%2520in%2520more%2520precise%2520and%2520efficient%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20Chest%20X-Ray%20Analysis%20via%20Transformer-Based%20Image%20Descriptors%0A%20%20and%20Cross-Model%20Attention%20Mechanism&entry.906535625=Lakshita%20Agarwal%20and%20Bindu%20Verma&entry.1292438233=%20%20The%20examination%20of%20chest%20X-ray%20images%20is%20a%20crucial%20component%20in%20detecting%0Avarious%20thoracic%20illnesses.%20This%20study%20introduces%20a%20new%20image%20description%0Ageneration%20model%20that%20integrates%20a%20Vision%20Transformer%20%28ViT%29%20encoder%20with%0Across-modal%20attention%20and%20a%20GPT-4-based%20transformer%20decoder.%20The%20ViT%20captures%0Ahigh-quality%20visual%20features%20from%20chest%20X-rays%2C%20which%20are%20fused%20with%20text%20data%0Athrough%20cross-modal%20attention%20to%20improve%20the%20accuracy%2C%20context%2C%20and%20richness%20of%0Aimage%20descriptions.%20The%20GPT-4%20decoder%20transforms%20these%20fused%20features%20into%0Aaccurate%20and%20relevant%20captions.%20The%20model%20was%20tested%20on%20the%20National%20Institutes%0Aof%20Health%20%28NIH%29%20and%20Indiana%20University%20%28IU%29%20Chest%20X-ray%20datasets.%20On%20the%20IU%0Adataset%2C%20it%20achieved%20scores%20of%200.854%20%28B-1%29%2C%200.883%20%28CIDEr%29%2C%200.759%20%28METEOR%29%2C%20and%0A0.712%20%28ROUGE-L%29.%20On%20the%20NIH%20dataset%2C%20it%20achieved%20the%20best%20performance%20on%20all%0Ametrics%3A%20BLEU%201--4%20%280.825%2C%200.788%2C%200.765%2C%200.752%29%2C%20CIDEr%20%280.857%29%2C%20METEOR%20%280.726%29%2C%0Aand%20ROUGE-L%20%280.705%29.%20This%20framework%20has%20the%20potential%20to%20enhance%20chest%20X-ray%0Aevaluation%2C%20assisting%20radiologists%20in%20more%20precise%20and%20efficient%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16774v1&entry.124074799=Read"},
{"title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering\n  for Multi-document Summarization", "author": "Shiyin Tan and Jaeeon Park and Dongyuan Li and Renhe Jiang and Manabu Okumura", "abstract": "  In the field of multi-document summarization (MDS), transformer-based models\nhave demonstrated remarkable success, yet they suffer an input length\nlimitation. Current methods apply truncation after the retrieval process to fit\nthe context length; however, they heavily depend on manually well-crafted\nqueries, which are impractical to create for each document set for MDS.\nAdditionally, these methods retrieve information at a coarse granularity,\nleading to the inclusion of irrelevant content. To address these issues, we\npropose a novel retrieval-based framework that integrates query selection and\ndocument ranking and shortening into a unified process. Our approach identifies\nthe most salient elementary discourse units (EDUs) from input documents and\nutilizes them as latent queries. These queries guide the document ranking by\ncalculating relevance scores. Instead of traditional truncation, our approach\nfilters out irrelevant EDUs to fit the context length, ensuring that only\ncritical information is preserved for summarization. We evaluate our framework\non multiple MDS datasets, demonstrating consistent improvements in ROUGE\nmetrics while confirming its scalability and flexibility across diverse model\narchitectures. Additionally, we validate its effectiveness through an in-depth\nanalysis, emphasizing its ability to dynamically select appropriate queries and\naccurately rank documents based on their relevance scores. These results\ndemonstrate that our framework effectively addresses context-length\nconstraints, establishing it as a robust and reliable solution for MDS.\n", "link": "http://arxiv.org/abs/2504.16711v1", "date": "2025-04-23", "relevancy": 2.096, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Retrieval%20Framework%20with%20Document%20Ranking%20and%20EDU%20Filtering%0A%20%20for%20Multi-document%20Summarization&body=Title%3A%20A%20Unified%20Retrieval%20Framework%20with%20Document%20Ranking%20and%20EDU%20Filtering%0A%20%20for%20Multi-document%20Summarization%0AAuthor%3A%20Shiyin%20Tan%20and%20Jaeeon%20Park%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Manabu%20Okumura%0AAbstract%3A%20%20%20In%20the%20field%20of%20multi-document%20summarization%20%28MDS%29%2C%20transformer-based%20models%0Ahave%20demonstrated%20remarkable%20success%2C%20yet%20they%20suffer%20an%20input%20length%0Alimitation.%20Current%20methods%20apply%20truncation%20after%20the%20retrieval%20process%20to%20fit%0Athe%20context%20length%3B%20however%2C%20they%20heavily%20depend%20on%20manually%20well-crafted%0Aqueries%2C%20which%20are%20impractical%20to%20create%20for%20each%20document%20set%20for%20MDS.%0AAdditionally%2C%20these%20methods%20retrieve%20information%20at%20a%20coarse%20granularity%2C%0Aleading%20to%20the%20inclusion%20of%20irrelevant%20content.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20novel%20retrieval-based%20framework%20that%20integrates%20query%20selection%20and%0Adocument%20ranking%20and%20shortening%20into%20a%20unified%20process.%20Our%20approach%20identifies%0Athe%20most%20salient%20elementary%20discourse%20units%20%28EDUs%29%20from%20input%20documents%20and%0Autilizes%20them%20as%20latent%20queries.%20These%20queries%20guide%20the%20document%20ranking%20by%0Acalculating%20relevance%20scores.%20Instead%20of%20traditional%20truncation%2C%20our%20approach%0Afilters%20out%20irrelevant%20EDUs%20to%20fit%20the%20context%20length%2C%20ensuring%20that%20only%0Acritical%20information%20is%20preserved%20for%20summarization.%20We%20evaluate%20our%20framework%0Aon%20multiple%20MDS%20datasets%2C%20demonstrating%20consistent%20improvements%20in%20ROUGE%0Ametrics%20while%20confirming%20its%20scalability%20and%20flexibility%20across%20diverse%20model%0Aarchitectures.%20Additionally%2C%20we%20validate%20its%20effectiveness%20through%20an%20in-depth%0Aanalysis%2C%20emphasizing%20its%20ability%20to%20dynamically%20select%20appropriate%20queries%20and%0Aaccurately%20rank%20documents%20based%20on%20their%20relevance%20scores.%20These%20results%0Ademonstrate%20that%20our%20framework%20effectively%20addresses%20context-length%0Aconstraints%2C%20establishing%20it%20as%20a%20robust%20and%20reliable%20solution%20for%20MDS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Retrieval%2520Framework%2520with%2520Document%2520Ranking%2520and%2520EDU%2520Filtering%250A%2520%2520for%2520Multi-document%2520Summarization%26entry.906535625%3DShiyin%2520Tan%2520and%2520Jaeeon%2520Park%2520and%2520Dongyuan%2520Li%2520and%2520Renhe%2520Jiang%2520and%2520Manabu%2520Okumura%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520multi-document%2520summarization%2520%2528MDS%2529%252C%2520transformer-based%2520models%250Ahave%2520demonstrated%2520remarkable%2520success%252C%2520yet%2520they%2520suffer%2520an%2520input%2520length%250Alimitation.%2520Current%2520methods%2520apply%2520truncation%2520after%2520the%2520retrieval%2520process%2520to%2520fit%250Athe%2520context%2520length%253B%2520however%252C%2520they%2520heavily%2520depend%2520on%2520manually%2520well-crafted%250Aqueries%252C%2520which%2520are%2520impractical%2520to%2520create%2520for%2520each%2520document%2520set%2520for%2520MDS.%250AAdditionally%252C%2520these%2520methods%2520retrieve%2520information%2520at%2520a%2520coarse%2520granularity%252C%250Aleading%2520to%2520the%2520inclusion%2520of%2520irrelevant%2520content.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520novel%2520retrieval-based%2520framework%2520that%2520integrates%2520query%2520selection%2520and%250Adocument%2520ranking%2520and%2520shortening%2520into%2520a%2520unified%2520process.%2520Our%2520approach%2520identifies%250Athe%2520most%2520salient%2520elementary%2520discourse%2520units%2520%2528EDUs%2529%2520from%2520input%2520documents%2520and%250Autilizes%2520them%2520as%2520latent%2520queries.%2520These%2520queries%2520guide%2520the%2520document%2520ranking%2520by%250Acalculating%2520relevance%2520scores.%2520Instead%2520of%2520traditional%2520truncation%252C%2520our%2520approach%250Afilters%2520out%2520irrelevant%2520EDUs%2520to%2520fit%2520the%2520context%2520length%252C%2520ensuring%2520that%2520only%250Acritical%2520information%2520is%2520preserved%2520for%2520summarization.%2520We%2520evaluate%2520our%2520framework%250Aon%2520multiple%2520MDS%2520datasets%252C%2520demonstrating%2520consistent%2520improvements%2520in%2520ROUGE%250Ametrics%2520while%2520confirming%2520its%2520scalability%2520and%2520flexibility%2520across%2520diverse%2520model%250Aarchitectures.%2520Additionally%252C%2520we%2520validate%2520its%2520effectiveness%2520through%2520an%2520in-depth%250Aanalysis%252C%2520emphasizing%2520its%2520ability%2520to%2520dynamically%2520select%2520appropriate%2520queries%2520and%250Aaccurately%2520rank%2520documents%2520based%2520on%2520their%2520relevance%2520scores.%2520These%2520results%250Ademonstrate%2520that%2520our%2520framework%2520effectively%2520addresses%2520context-length%250Aconstraints%252C%2520establishing%2520it%2520as%2520a%2520robust%2520and%2520reliable%2520solution%2520for%2520MDS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Retrieval%20Framework%20with%20Document%20Ranking%20and%20EDU%20Filtering%0A%20%20for%20Multi-document%20Summarization&entry.906535625=Shiyin%20Tan%20and%20Jaeeon%20Park%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Manabu%20Okumura&entry.1292438233=%20%20In%20the%20field%20of%20multi-document%20summarization%20%28MDS%29%2C%20transformer-based%20models%0Ahave%20demonstrated%20remarkable%20success%2C%20yet%20they%20suffer%20an%20input%20length%0Alimitation.%20Current%20methods%20apply%20truncation%20after%20the%20retrieval%20process%20to%20fit%0Athe%20context%20length%3B%20however%2C%20they%20heavily%20depend%20on%20manually%20well-crafted%0Aqueries%2C%20which%20are%20impractical%20to%20create%20for%20each%20document%20set%20for%20MDS.%0AAdditionally%2C%20these%20methods%20retrieve%20information%20at%20a%20coarse%20granularity%2C%0Aleading%20to%20the%20inclusion%20of%20irrelevant%20content.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20novel%20retrieval-based%20framework%20that%20integrates%20query%20selection%20and%0Adocument%20ranking%20and%20shortening%20into%20a%20unified%20process.%20Our%20approach%20identifies%0Athe%20most%20salient%20elementary%20discourse%20units%20%28EDUs%29%20from%20input%20documents%20and%0Autilizes%20them%20as%20latent%20queries.%20These%20queries%20guide%20the%20document%20ranking%20by%0Acalculating%20relevance%20scores.%20Instead%20of%20traditional%20truncation%2C%20our%20approach%0Afilters%20out%20irrelevant%20EDUs%20to%20fit%20the%20context%20length%2C%20ensuring%20that%20only%0Acritical%20information%20is%20preserved%20for%20summarization.%20We%20evaluate%20our%20framework%0Aon%20multiple%20MDS%20datasets%2C%20demonstrating%20consistent%20improvements%20in%20ROUGE%0Ametrics%20while%20confirming%20its%20scalability%20and%20flexibility%20across%20diverse%20model%0Aarchitectures.%20Additionally%2C%20we%20validate%20its%20effectiveness%20through%20an%20in-depth%0Aanalysis%2C%20emphasizing%20its%20ability%20to%20dynamically%20select%20appropriate%20queries%20and%0Aaccurately%20rank%20documents%20based%20on%20their%20relevance%20scores.%20These%20results%0Ademonstrate%20that%20our%20framework%20effectively%20addresses%20context-length%0Aconstraints%2C%20establishing%20it%20as%20a%20robust%20and%20reliable%20solution%20for%20MDS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16711v1&entry.124074799=Read"},
{"title": "Building Real-time Awareness of Out-of-distribution in Trajectory\n  Prediction for Autonomous Vehicles", "author": "Tongfe Guo and Taposh Banerjee and Rui Liu and Lili Su", "abstract": "  Accurate trajectory prediction is essential for the safe operation of\nautonomous vehicles in real-world environments. Even well-trained machine\nlearning models may produce unreliable predictions due to discrepancies between\ntraining data and real-world conditions encountered during inference. In\nparticular, the training dataset tends to overrepresent common scenes (e.g.,\nstraight lanes) while underrepresenting less frequent ones (e.g., traffic\ncircles). In addition, it often overlooks unpredictable real-world events such\nas sudden braking or falling objects. To ensure safety, it is critical to\ndetect in real-time when a model's predictions become unreliable. Leveraging\nthe intuition that in-distribution (ID) scenes exhibit error patterns similar\nto training data, while out-of-distribution (OOD) scenes do not, we introduce a\nprincipled, real-time approach for OOD detection by framing it as a\nchange-point detection problem. We address the challenging settings where the\nOOD scenes are deceptive, meaning that they are not easily detectable by human\nintuitions. Our lightweight solutions can handle the occurrence of OOD at any\ntime during trajectory prediction inference. Experimental results on multiple\nreal-world datasets using a benchmark trajectory prediction model demonstrate\nthe effectiveness of our methods.\n", "link": "http://arxiv.org/abs/2409.17277v2", "date": "2025-04-23", "relevancy": 2.0879, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5814}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5206}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Real-time%20Awareness%20of%20Out-of-distribution%20in%20Trajectory%0A%20%20Prediction%20for%20Autonomous%20Vehicles&body=Title%3A%20Building%20Real-time%20Awareness%20of%20Out-of-distribution%20in%20Trajectory%0A%20%20Prediction%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Tongfe%20Guo%20and%20Taposh%20Banerjee%20and%20Rui%20Liu%20and%20Lili%20Su%0AAbstract%3A%20%20%20Accurate%20trajectory%20prediction%20is%20essential%20for%20the%20safe%20operation%20of%0Aautonomous%20vehicles%20in%20real-world%20environments.%20Even%20well-trained%20machine%0Alearning%20models%20may%20produce%20unreliable%20predictions%20due%20to%20discrepancies%20between%0Atraining%20data%20and%20real-world%20conditions%20encountered%20during%20inference.%20In%0Aparticular%2C%20the%20training%20dataset%20tends%20to%20overrepresent%20common%20scenes%20%28e.g.%2C%0Astraight%20lanes%29%20while%20underrepresenting%20less%20frequent%20ones%20%28e.g.%2C%20traffic%0Acircles%29.%20In%20addition%2C%20it%20often%20overlooks%20unpredictable%20real-world%20events%20such%0Aas%20sudden%20braking%20or%20falling%20objects.%20To%20ensure%20safety%2C%20it%20is%20critical%20to%0Adetect%20in%20real-time%20when%20a%20model%27s%20predictions%20become%20unreliable.%20Leveraging%0Athe%20intuition%20that%20in-distribution%20%28ID%29%20scenes%20exhibit%20error%20patterns%20similar%0Ato%20training%20data%2C%20while%20out-of-distribution%20%28OOD%29%20scenes%20do%20not%2C%20we%20introduce%20a%0Aprincipled%2C%20real-time%20approach%20for%20OOD%20detection%20by%20framing%20it%20as%20a%0Achange-point%20detection%20problem.%20We%20address%20the%20challenging%20settings%20where%20the%0AOOD%20scenes%20are%20deceptive%2C%20meaning%20that%20they%20are%20not%20easily%20detectable%20by%20human%0Aintuitions.%20Our%20lightweight%20solutions%20can%20handle%20the%20occurrence%20of%20OOD%20at%20any%0Atime%20during%20trajectory%20prediction%20inference.%20Experimental%20results%20on%20multiple%0Areal-world%20datasets%20using%20a%20benchmark%20trajectory%20prediction%20model%20demonstrate%0Athe%20effectiveness%20of%20our%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Real-time%2520Awareness%2520of%2520Out-of-distribution%2520in%2520Trajectory%250A%2520%2520Prediction%2520for%2520Autonomous%2520Vehicles%26entry.906535625%3DTongfe%2520Guo%2520and%2520Taposh%2520Banerjee%2520and%2520Rui%2520Liu%2520and%2520Lili%2520Su%26entry.1292438233%3D%2520%2520Accurate%2520trajectory%2520prediction%2520is%2520essential%2520for%2520the%2520safe%2520operation%2520of%250Aautonomous%2520vehicles%2520in%2520real-world%2520environments.%2520Even%2520well-trained%2520machine%250Alearning%2520models%2520may%2520produce%2520unreliable%2520predictions%2520due%2520to%2520discrepancies%2520between%250Atraining%2520data%2520and%2520real-world%2520conditions%2520encountered%2520during%2520inference.%2520In%250Aparticular%252C%2520the%2520training%2520dataset%2520tends%2520to%2520overrepresent%2520common%2520scenes%2520%2528e.g.%252C%250Astraight%2520lanes%2529%2520while%2520underrepresenting%2520less%2520frequent%2520ones%2520%2528e.g.%252C%2520traffic%250Acircles%2529.%2520In%2520addition%252C%2520it%2520often%2520overlooks%2520unpredictable%2520real-world%2520events%2520such%250Aas%2520sudden%2520braking%2520or%2520falling%2520objects.%2520To%2520ensure%2520safety%252C%2520it%2520is%2520critical%2520to%250Adetect%2520in%2520real-time%2520when%2520a%2520model%2527s%2520predictions%2520become%2520unreliable.%2520Leveraging%250Athe%2520intuition%2520that%2520in-distribution%2520%2528ID%2529%2520scenes%2520exhibit%2520error%2520patterns%2520similar%250Ato%2520training%2520data%252C%2520while%2520out-of-distribution%2520%2528OOD%2529%2520scenes%2520do%2520not%252C%2520we%2520introduce%2520a%250Aprincipled%252C%2520real-time%2520approach%2520for%2520OOD%2520detection%2520by%2520framing%2520it%2520as%2520a%250Achange-point%2520detection%2520problem.%2520We%2520address%2520the%2520challenging%2520settings%2520where%2520the%250AOOD%2520scenes%2520are%2520deceptive%252C%2520meaning%2520that%2520they%2520are%2520not%2520easily%2520detectable%2520by%2520human%250Aintuitions.%2520Our%2520lightweight%2520solutions%2520can%2520handle%2520the%2520occurrence%2520of%2520OOD%2520at%2520any%250Atime%2520during%2520trajectory%2520prediction%2520inference.%2520Experimental%2520results%2520on%2520multiple%250Areal-world%2520datasets%2520using%2520a%2520benchmark%2520trajectory%2520prediction%2520model%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Real-time%20Awareness%20of%20Out-of-distribution%20in%20Trajectory%0A%20%20Prediction%20for%20Autonomous%20Vehicles&entry.906535625=Tongfe%20Guo%20and%20Taposh%20Banerjee%20and%20Rui%20Liu%20and%20Lili%20Su&entry.1292438233=%20%20Accurate%20trajectory%20prediction%20is%20essential%20for%20the%20safe%20operation%20of%0Aautonomous%20vehicles%20in%20real-world%20environments.%20Even%20well-trained%20machine%0Alearning%20models%20may%20produce%20unreliable%20predictions%20due%20to%20discrepancies%20between%0Atraining%20data%20and%20real-world%20conditions%20encountered%20during%20inference.%20In%0Aparticular%2C%20the%20training%20dataset%20tends%20to%20overrepresent%20common%20scenes%20%28e.g.%2C%0Astraight%20lanes%29%20while%20underrepresenting%20less%20frequent%20ones%20%28e.g.%2C%20traffic%0Acircles%29.%20In%20addition%2C%20it%20often%20overlooks%20unpredictable%20real-world%20events%20such%0Aas%20sudden%20braking%20or%20falling%20objects.%20To%20ensure%20safety%2C%20it%20is%20critical%20to%0Adetect%20in%20real-time%20when%20a%20model%27s%20predictions%20become%20unreliable.%20Leveraging%0Athe%20intuition%20that%20in-distribution%20%28ID%29%20scenes%20exhibit%20error%20patterns%20similar%0Ato%20training%20data%2C%20while%20out-of-distribution%20%28OOD%29%20scenes%20do%20not%2C%20we%20introduce%20a%0Aprincipled%2C%20real-time%20approach%20for%20OOD%20detection%20by%20framing%20it%20as%20a%0Achange-point%20detection%20problem.%20We%20address%20the%20challenging%20settings%20where%20the%0AOOD%20scenes%20are%20deceptive%2C%20meaning%20that%20they%20are%20not%20easily%20detectable%20by%20human%0Aintuitions.%20Our%20lightweight%20solutions%20can%20handle%20the%20occurrence%20of%20OOD%20at%20any%0Atime%20during%20trajectory%20prediction%20inference.%20Experimental%20results%20on%20multiple%0Areal-world%20datasets%20using%20a%20benchmark%20trajectory%20prediction%20model%20demonstrate%0Athe%20effectiveness%20of%20our%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17277v2&entry.124074799=Read"},
{"title": "QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization\n  Algorithm via Principal Component Analysis", "author": "Owain Parry and Phil McMinn", "abstract": "  The Quantum Approximate Optimization Algorithm (QAOA) is a promising\nvariational algorithm for solving combinatorial optimization problems on\nnear-term devices. However, as the number of layers in a QAOA circuit\nincreases, which is correlated with the quality of the solution, the number of\nparameters to optimize grows linearly. This results in more iterations required\nby the classical optimizer, which results in an increasing computational burden\nas more circuit executions are needed. To mitigate this issue, we introduce\nQAOA-PCA, a novel reparameterization technique that employs Principal Component\nAnalysis (PCA) to reduce the dimensionality of the QAOA parameter space. By\nextracting principal components from optimized parameters of smaller problem\ninstances, QAOA-PCA facilitates efficient optimization with fewer parameters on\nlarger instances. Our empirical evaluation on the prominent MaxCut problem\ndemonstrates that QAOA-PCA consistently requires fewer iterations than standard\nQAOA, achieving substantial efficiency gains. While this comes at the cost of a\nslight reduction in approximation ratio compared to QAOA with the same number\nof layers, QAOA-PCA almost always outperforms standard QAOA when matched by\nparameter count. QAOA-PCA strikes a favorable balance between efficiency and\nperformance, reducing optimization overhead without significantly compromising\nsolution quality.\n", "link": "http://arxiv.org/abs/2504.16755v1", "date": "2025-04-23", "relevancy": 2.083, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4172}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4172}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QAOA-PCA%3A%20Enhancing%20Efficiency%20in%20the%20Quantum%20Approximate%20Optimization%0A%20%20Algorithm%20via%20Principal%20Component%20Analysis&body=Title%3A%20QAOA-PCA%3A%20Enhancing%20Efficiency%20in%20the%20Quantum%20Approximate%20Optimization%0A%20%20Algorithm%20via%20Principal%20Component%20Analysis%0AAuthor%3A%20Owain%20Parry%20and%20Phil%20McMinn%0AAbstract%3A%20%20%20The%20Quantum%20Approximate%20Optimization%20Algorithm%20%28QAOA%29%20is%20a%20promising%0Avariational%20algorithm%20for%20solving%20combinatorial%20optimization%20problems%20on%0Anear-term%20devices.%20However%2C%20as%20the%20number%20of%20layers%20in%20a%20QAOA%20circuit%0Aincreases%2C%20which%20is%20correlated%20with%20the%20quality%20of%20the%20solution%2C%20the%20number%20of%0Aparameters%20to%20optimize%20grows%20linearly.%20This%20results%20in%20more%20iterations%20required%0Aby%20the%20classical%20optimizer%2C%20which%20results%20in%20an%20increasing%20computational%20burden%0Aas%20more%20circuit%20executions%20are%20needed.%20To%20mitigate%20this%20issue%2C%20we%20introduce%0AQAOA-PCA%2C%20a%20novel%20reparameterization%20technique%20that%20employs%20Principal%20Component%0AAnalysis%20%28PCA%29%20to%20reduce%20the%20dimensionality%20of%20the%20QAOA%20parameter%20space.%20By%0Aextracting%20principal%20components%20from%20optimized%20parameters%20of%20smaller%20problem%0Ainstances%2C%20QAOA-PCA%20facilitates%20efficient%20optimization%20with%20fewer%20parameters%20on%0Alarger%20instances.%20Our%20empirical%20evaluation%20on%20the%20prominent%20MaxCut%20problem%0Ademonstrates%20that%20QAOA-PCA%20consistently%20requires%20fewer%20iterations%20than%20standard%0AQAOA%2C%20achieving%20substantial%20efficiency%20gains.%20While%20this%20comes%20at%20the%20cost%20of%20a%0Aslight%20reduction%20in%20approximation%20ratio%20compared%20to%20QAOA%20with%20the%20same%20number%0Aof%20layers%2C%20QAOA-PCA%20almost%20always%20outperforms%20standard%20QAOA%20when%20matched%20by%0Aparameter%20count.%20QAOA-PCA%20strikes%20a%20favorable%20balance%20between%20efficiency%20and%0Aperformance%2C%20reducing%20optimization%20overhead%20without%20significantly%20compromising%0Asolution%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQAOA-PCA%253A%2520Enhancing%2520Efficiency%2520in%2520the%2520Quantum%2520Approximate%2520Optimization%250A%2520%2520Algorithm%2520via%2520Principal%2520Component%2520Analysis%26entry.906535625%3DOwain%2520Parry%2520and%2520Phil%2520McMinn%26entry.1292438233%3D%2520%2520The%2520Quantum%2520Approximate%2520Optimization%2520Algorithm%2520%2528QAOA%2529%2520is%2520a%2520promising%250Avariational%2520algorithm%2520for%2520solving%2520combinatorial%2520optimization%2520problems%2520on%250Anear-term%2520devices.%2520However%252C%2520as%2520the%2520number%2520of%2520layers%2520in%2520a%2520QAOA%2520circuit%250Aincreases%252C%2520which%2520is%2520correlated%2520with%2520the%2520quality%2520of%2520the%2520solution%252C%2520the%2520number%2520of%250Aparameters%2520to%2520optimize%2520grows%2520linearly.%2520This%2520results%2520in%2520more%2520iterations%2520required%250Aby%2520the%2520classical%2520optimizer%252C%2520which%2520results%2520in%2520an%2520increasing%2520computational%2520burden%250Aas%2520more%2520circuit%2520executions%2520are%2520needed.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520introduce%250AQAOA-PCA%252C%2520a%2520novel%2520reparameterization%2520technique%2520that%2520employs%2520Principal%2520Component%250AAnalysis%2520%2528PCA%2529%2520to%2520reduce%2520the%2520dimensionality%2520of%2520the%2520QAOA%2520parameter%2520space.%2520By%250Aextracting%2520principal%2520components%2520from%2520optimized%2520parameters%2520of%2520smaller%2520problem%250Ainstances%252C%2520QAOA-PCA%2520facilitates%2520efficient%2520optimization%2520with%2520fewer%2520parameters%2520on%250Alarger%2520instances.%2520Our%2520empirical%2520evaluation%2520on%2520the%2520prominent%2520MaxCut%2520problem%250Ademonstrates%2520that%2520QAOA-PCA%2520consistently%2520requires%2520fewer%2520iterations%2520than%2520standard%250AQAOA%252C%2520achieving%2520substantial%2520efficiency%2520gains.%2520While%2520this%2520comes%2520at%2520the%2520cost%2520of%2520a%250Aslight%2520reduction%2520in%2520approximation%2520ratio%2520compared%2520to%2520QAOA%2520with%2520the%2520same%2520number%250Aof%2520layers%252C%2520QAOA-PCA%2520almost%2520always%2520outperforms%2520standard%2520QAOA%2520when%2520matched%2520by%250Aparameter%2520count.%2520QAOA-PCA%2520strikes%2520a%2520favorable%2520balance%2520between%2520efficiency%2520and%250Aperformance%252C%2520reducing%2520optimization%2520overhead%2520without%2520significantly%2520compromising%250Asolution%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QAOA-PCA%3A%20Enhancing%20Efficiency%20in%20the%20Quantum%20Approximate%20Optimization%0A%20%20Algorithm%20via%20Principal%20Component%20Analysis&entry.906535625=Owain%20Parry%20and%20Phil%20McMinn&entry.1292438233=%20%20The%20Quantum%20Approximate%20Optimization%20Algorithm%20%28QAOA%29%20is%20a%20promising%0Avariational%20algorithm%20for%20solving%20combinatorial%20optimization%20problems%20on%0Anear-term%20devices.%20However%2C%20as%20the%20number%20of%20layers%20in%20a%20QAOA%20circuit%0Aincreases%2C%20which%20is%20correlated%20with%20the%20quality%20of%20the%20solution%2C%20the%20number%20of%0Aparameters%20to%20optimize%20grows%20linearly.%20This%20results%20in%20more%20iterations%20required%0Aby%20the%20classical%20optimizer%2C%20which%20results%20in%20an%20increasing%20computational%20burden%0Aas%20more%20circuit%20executions%20are%20needed.%20To%20mitigate%20this%20issue%2C%20we%20introduce%0AQAOA-PCA%2C%20a%20novel%20reparameterization%20technique%20that%20employs%20Principal%20Component%0AAnalysis%20%28PCA%29%20to%20reduce%20the%20dimensionality%20of%20the%20QAOA%20parameter%20space.%20By%0Aextracting%20principal%20components%20from%20optimized%20parameters%20of%20smaller%20problem%0Ainstances%2C%20QAOA-PCA%20facilitates%20efficient%20optimization%20with%20fewer%20parameters%20on%0Alarger%20instances.%20Our%20empirical%20evaluation%20on%20the%20prominent%20MaxCut%20problem%0Ademonstrates%20that%20QAOA-PCA%20consistently%20requires%20fewer%20iterations%20than%20standard%0AQAOA%2C%20achieving%20substantial%20efficiency%20gains.%20While%20this%20comes%20at%20the%20cost%20of%20a%0Aslight%20reduction%20in%20approximation%20ratio%20compared%20to%20QAOA%20with%20the%20same%20number%0Aof%20layers%2C%20QAOA-PCA%20almost%20always%20outperforms%20standard%20QAOA%20when%20matched%20by%0Aparameter%20count.%20QAOA-PCA%20strikes%20a%20favorable%20balance%20between%20efficiency%20and%0Aperformance%2C%20reducing%20optimization%20overhead%20without%20significantly%20compromising%0Asolution%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16755v1&entry.124074799=Read"},
{"title": "Learning Verifiable Control Policies Using Relaxed Verification", "author": "Puja Chaudhury and Alexander Estornell and Michael Everett", "abstract": "  To provide safety guarantees for learning-based control systems, recent work\nhas developed formal verification methods to apply after training ends.\nHowever, if the trained policy does not meet the specifications, or there is\nconservatism in the verification algorithm, establishing these guarantees may\nnot be possible. Instead, this work proposes to perform verification throughout\ntraining to ultimately aim for policies whose properties can be evaluated\nthroughout runtime with lightweight, relaxed verification algorithms. The\napproach is to use differentiable reachability analysis and incorporate new\ncomponents into the loss function. Numerical experiments on a quadrotor model\nand unicycle model highlight the ability of this approach to lead to learned\ncontrol policies that satisfy desired reach-avoid and invariance\nspecifications.\n", "link": "http://arxiv.org/abs/2504.16879v1", "date": "2025-04-23", "relevancy": 2.0826, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5414}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5223}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Verifiable%20Control%20Policies%20Using%20Relaxed%20Verification&body=Title%3A%20Learning%20Verifiable%20Control%20Policies%20Using%20Relaxed%20Verification%0AAuthor%3A%20Puja%20Chaudhury%20and%20Alexander%20Estornell%20and%20Michael%20Everett%0AAbstract%3A%20%20%20To%20provide%20safety%20guarantees%20for%20learning-based%20control%20systems%2C%20recent%20work%0Ahas%20developed%20formal%20verification%20methods%20to%20apply%20after%20training%20ends.%0AHowever%2C%20if%20the%20trained%20policy%20does%20not%20meet%20the%20specifications%2C%20or%20there%20is%0Aconservatism%20in%20the%20verification%20algorithm%2C%20establishing%20these%20guarantees%20may%0Anot%20be%20possible.%20Instead%2C%20this%20work%20proposes%20to%20perform%20verification%20throughout%0Atraining%20to%20ultimately%20aim%20for%20policies%20whose%20properties%20can%20be%20evaluated%0Athroughout%20runtime%20with%20lightweight%2C%20relaxed%20verification%20algorithms.%20The%0Aapproach%20is%20to%20use%20differentiable%20reachability%20analysis%20and%20incorporate%20new%0Acomponents%20into%20the%20loss%20function.%20Numerical%20experiments%20on%20a%20quadrotor%20model%0Aand%20unicycle%20model%20highlight%20the%20ability%20of%20this%20approach%20to%20lead%20to%20learned%0Acontrol%20policies%20that%20satisfy%20desired%20reach-avoid%20and%20invariance%0Aspecifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Verifiable%2520Control%2520Policies%2520Using%2520Relaxed%2520Verification%26entry.906535625%3DPuja%2520Chaudhury%2520and%2520Alexander%2520Estornell%2520and%2520Michael%2520Everett%26entry.1292438233%3D%2520%2520To%2520provide%2520safety%2520guarantees%2520for%2520learning-based%2520control%2520systems%252C%2520recent%2520work%250Ahas%2520developed%2520formal%2520verification%2520methods%2520to%2520apply%2520after%2520training%2520ends.%250AHowever%252C%2520if%2520the%2520trained%2520policy%2520does%2520not%2520meet%2520the%2520specifications%252C%2520or%2520there%2520is%250Aconservatism%2520in%2520the%2520verification%2520algorithm%252C%2520establishing%2520these%2520guarantees%2520may%250Anot%2520be%2520possible.%2520Instead%252C%2520this%2520work%2520proposes%2520to%2520perform%2520verification%2520throughout%250Atraining%2520to%2520ultimately%2520aim%2520for%2520policies%2520whose%2520properties%2520can%2520be%2520evaluated%250Athroughout%2520runtime%2520with%2520lightweight%252C%2520relaxed%2520verification%2520algorithms.%2520The%250Aapproach%2520is%2520to%2520use%2520differentiable%2520reachability%2520analysis%2520and%2520incorporate%2520new%250Acomponents%2520into%2520the%2520loss%2520function.%2520Numerical%2520experiments%2520on%2520a%2520quadrotor%2520model%250Aand%2520unicycle%2520model%2520highlight%2520the%2520ability%2520of%2520this%2520approach%2520to%2520lead%2520to%2520learned%250Acontrol%2520policies%2520that%2520satisfy%2520desired%2520reach-avoid%2520and%2520invariance%250Aspecifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Verifiable%20Control%20Policies%20Using%20Relaxed%20Verification&entry.906535625=Puja%20Chaudhury%20and%20Alexander%20Estornell%20and%20Michael%20Everett&entry.1292438233=%20%20To%20provide%20safety%20guarantees%20for%20learning-based%20control%20systems%2C%20recent%20work%0Ahas%20developed%20formal%20verification%20methods%20to%20apply%20after%20training%20ends.%0AHowever%2C%20if%20the%20trained%20policy%20does%20not%20meet%20the%20specifications%2C%20or%20there%20is%0Aconservatism%20in%20the%20verification%20algorithm%2C%20establishing%20these%20guarantees%20may%0Anot%20be%20possible.%20Instead%2C%20this%20work%20proposes%20to%20perform%20verification%20throughout%0Atraining%20to%20ultimately%20aim%20for%20policies%20whose%20properties%20can%20be%20evaluated%0Athroughout%20runtime%20with%20lightweight%2C%20relaxed%20verification%20algorithms.%20The%0Aapproach%20is%20to%20use%20differentiable%20reachability%20analysis%20and%20incorporate%20new%0Acomponents%20into%20the%20loss%20function.%20Numerical%20experiments%20on%20a%20quadrotor%20model%0Aand%20unicycle%20model%20highlight%20the%20ability%20of%20this%20approach%20to%20lead%20to%20learned%0Acontrol%20policies%20that%20satisfy%20desired%20reach-avoid%20and%20invariance%0Aspecifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16879v1&entry.124074799=Read"},
{"title": "Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous\n  Driving", "author": "Jacob Levy and Jason Gibson and Bogdan Vlahov and Erica Tevere and Evangelos Theodorou and David Fridovich-Keil and Patrick Spieler", "abstract": "  High-speed off-road autonomous driving presents unique challenges due to\ncomplex, evolving terrain characteristics and the difficulty of accurately\nmodeling terrain-vehicle interactions. While dynamics models used in\nmodel-based control can be learned from real-world data, they often struggle to\ngeneralize to unseen terrain, making real-time adaptation essential. We propose\na novel framework that combines a Kalman filter-based online adaptation scheme\nwith meta-learned parameters to address these challenges. Offline meta-learning\noptimizes the basis functions along which adaptation occurs, as well as the\nadaptation parameters, while online adaptation dynamically adjusts the onboard\ndynamics model in real time for model-based control. We validate our approach\nthrough extensive experiments, including real-world testing on a full-scale\nautonomous off-road vehicle, demonstrating that our method outperforms baseline\napproaches in prediction accuracy, performance, and safety metrics,\nparticularly in safety-critical scenarios. Our results underscore the\neffectiveness of meta-learned dynamics model adaptation, advancing the\ndevelopment of reliable autonomous systems capable of navigating diverse and\nunseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA\n", "link": "http://arxiv.org/abs/2504.16923v1", "date": "2025-04-23", "relevancy": 2.0782, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5613}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5158}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning%20Online%20Dynamics%20Model%20Adaptation%20in%20Off-Road%20Autonomous%0A%20%20Driving&body=Title%3A%20Meta-Learning%20Online%20Dynamics%20Model%20Adaptation%20in%20Off-Road%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Jacob%20Levy%20and%20Jason%20Gibson%20and%20Bogdan%20Vlahov%20and%20Erica%20Tevere%20and%20Evangelos%20Theodorou%20and%20David%20Fridovich-Keil%20and%20Patrick%20Spieler%0AAbstract%3A%20%20%20High-speed%20off-road%20autonomous%20driving%20presents%20unique%20challenges%20due%20to%0Acomplex%2C%20evolving%20terrain%20characteristics%20and%20the%20difficulty%20of%20accurately%0Amodeling%20terrain-vehicle%20interactions.%20While%20dynamics%20models%20used%20in%0Amodel-based%20control%20can%20be%20learned%20from%20real-world%20data%2C%20they%20often%20struggle%20to%0Ageneralize%20to%20unseen%20terrain%2C%20making%20real-time%20adaptation%20essential.%20We%20propose%0Aa%20novel%20framework%20that%20combines%20a%20Kalman%20filter-based%20online%20adaptation%20scheme%0Awith%20meta-learned%20parameters%20to%20address%20these%20challenges.%20Offline%20meta-learning%0Aoptimizes%20the%20basis%20functions%20along%20which%20adaptation%20occurs%2C%20as%20well%20as%20the%0Aadaptation%20parameters%2C%20while%20online%20adaptation%20dynamically%20adjusts%20the%20onboard%0Adynamics%20model%20in%20real%20time%20for%20model-based%20control.%20We%20validate%20our%20approach%0Athrough%20extensive%20experiments%2C%20including%20real-world%20testing%20on%20a%20full-scale%0Aautonomous%20off-road%20vehicle%2C%20demonstrating%20that%20our%20method%20outperforms%20baseline%0Aapproaches%20in%20prediction%20accuracy%2C%20performance%2C%20and%20safety%20metrics%2C%0Aparticularly%20in%20safety-critical%20scenarios.%20Our%20results%20underscore%20the%0Aeffectiveness%20of%20meta-learned%20dynamics%20model%20adaptation%2C%20advancing%20the%0Adevelopment%20of%20reliable%20autonomous%20systems%20capable%20of%20navigating%20diverse%20and%0Aunseen%20environments.%20Video%20is%20available%20at%3A%20https%3A//youtu.be/cCKHHrDRQEA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning%2520Online%2520Dynamics%2520Model%2520Adaptation%2520in%2520Off-Road%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DJacob%2520Levy%2520and%2520Jason%2520Gibson%2520and%2520Bogdan%2520Vlahov%2520and%2520Erica%2520Tevere%2520and%2520Evangelos%2520Theodorou%2520and%2520David%2520Fridovich-Keil%2520and%2520Patrick%2520Spieler%26entry.1292438233%3D%2520%2520High-speed%2520off-road%2520autonomous%2520driving%2520presents%2520unique%2520challenges%2520due%2520to%250Acomplex%252C%2520evolving%2520terrain%2520characteristics%2520and%2520the%2520difficulty%2520of%2520accurately%250Amodeling%2520terrain-vehicle%2520interactions.%2520While%2520dynamics%2520models%2520used%2520in%250Amodel-based%2520control%2520can%2520be%2520learned%2520from%2520real-world%2520data%252C%2520they%2520often%2520struggle%2520to%250Ageneralize%2520to%2520unseen%2520terrain%252C%2520making%2520real-time%2520adaptation%2520essential.%2520We%2520propose%250Aa%2520novel%2520framework%2520that%2520combines%2520a%2520Kalman%2520filter-based%2520online%2520adaptation%2520scheme%250Awith%2520meta-learned%2520parameters%2520to%2520address%2520these%2520challenges.%2520Offline%2520meta-learning%250Aoptimizes%2520the%2520basis%2520functions%2520along%2520which%2520adaptation%2520occurs%252C%2520as%2520well%2520as%2520the%250Aadaptation%2520parameters%252C%2520while%2520online%2520adaptation%2520dynamically%2520adjusts%2520the%2520onboard%250Adynamics%2520model%2520in%2520real%2520time%2520for%2520model-based%2520control.%2520We%2520validate%2520our%2520approach%250Athrough%2520extensive%2520experiments%252C%2520including%2520real-world%2520testing%2520on%2520a%2520full-scale%250Aautonomous%2520off-road%2520vehicle%252C%2520demonstrating%2520that%2520our%2520method%2520outperforms%2520baseline%250Aapproaches%2520in%2520prediction%2520accuracy%252C%2520performance%252C%2520and%2520safety%2520metrics%252C%250Aparticularly%2520in%2520safety-critical%2520scenarios.%2520Our%2520results%2520underscore%2520the%250Aeffectiveness%2520of%2520meta-learned%2520dynamics%2520model%2520adaptation%252C%2520advancing%2520the%250Adevelopment%2520of%2520reliable%2520autonomous%2520systems%2520capable%2520of%2520navigating%2520diverse%2520and%250Aunseen%2520environments.%2520Video%2520is%2520available%2520at%253A%2520https%253A//youtu.be/cCKHHrDRQEA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning%20Online%20Dynamics%20Model%20Adaptation%20in%20Off-Road%20Autonomous%0A%20%20Driving&entry.906535625=Jacob%20Levy%20and%20Jason%20Gibson%20and%20Bogdan%20Vlahov%20and%20Erica%20Tevere%20and%20Evangelos%20Theodorou%20and%20David%20Fridovich-Keil%20and%20Patrick%20Spieler&entry.1292438233=%20%20High-speed%20off-road%20autonomous%20driving%20presents%20unique%20challenges%20due%20to%0Acomplex%2C%20evolving%20terrain%20characteristics%20and%20the%20difficulty%20of%20accurately%0Amodeling%20terrain-vehicle%20interactions.%20While%20dynamics%20models%20used%20in%0Amodel-based%20control%20can%20be%20learned%20from%20real-world%20data%2C%20they%20often%20struggle%20to%0Ageneralize%20to%20unseen%20terrain%2C%20making%20real-time%20adaptation%20essential.%20We%20propose%0Aa%20novel%20framework%20that%20combines%20a%20Kalman%20filter-based%20online%20adaptation%20scheme%0Awith%20meta-learned%20parameters%20to%20address%20these%20challenges.%20Offline%20meta-learning%0Aoptimizes%20the%20basis%20functions%20along%20which%20adaptation%20occurs%2C%20as%20well%20as%20the%0Aadaptation%20parameters%2C%20while%20online%20adaptation%20dynamically%20adjusts%20the%20onboard%0Adynamics%20model%20in%20real%20time%20for%20model-based%20control.%20We%20validate%20our%20approach%0Athrough%20extensive%20experiments%2C%20including%20real-world%20testing%20on%20a%20full-scale%0Aautonomous%20off-road%20vehicle%2C%20demonstrating%20that%20our%20method%20outperforms%20baseline%0Aapproaches%20in%20prediction%20accuracy%2C%20performance%2C%20and%20safety%20metrics%2C%0Aparticularly%20in%20safety-critical%20scenarios.%20Our%20results%20underscore%20the%0Aeffectiveness%20of%20meta-learned%20dynamics%20model%20adaptation%2C%20advancing%20the%0Adevelopment%20of%20reliable%20autonomous%20systems%20capable%20of%20navigating%20diverse%20and%0Aunseen%20environments.%20Video%20is%20available%20at%3A%20https%3A//youtu.be/cCKHHrDRQEA%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16923v1&entry.124074799=Read"},
{"title": "TALES: Text Adventure Learning Environment Suite", "author": "Christopher Zhang Cui and Xingdi Yuan and Ziang Xiao and Prithviraj Ammanabrolu and Marc-Alexandre C\u00f4t\u00e9", "abstract": "  Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tales.\n", "link": "http://arxiv.org/abs/2504.14128v3", "date": "2025-04-23", "relevancy": 2.0695, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TALES%3A%20Text%20Adventure%20Learning%20Environment%20Suite&body=Title%3A%20TALES%3A%20Text%20Adventure%20Learning%20Environment%20Suite%0AAuthor%3A%20Christopher%20Zhang%20Cui%20and%20Xingdi%20Yuan%20and%20Ziang%20Xiao%20and%20Prithviraj%20Ammanabrolu%20and%20Marc-Alexandre%20C%C3%B4t%C3%A9%0AAbstract%3A%20%20%20Reasoning%20is%20an%20essential%20skill%20to%20enable%20Large%20Language%20Models%20%28LLMs%29%20to%0Ainteract%20with%20the%20world.%20As%20tasks%20become%20more%20complex%2C%20they%20demand%20increasingly%0Asophisticated%20and%20diverse%20reasoning%20capabilities%20for%20sequential%0Adecision-making%2C%20requiring%20structured%20reasoning%20over%20the%20context%20history%20to%0Adetermine%20the%20next%20best%20action.%20We%20introduce%20TALES%2C%20a%20diverse%20collection%20of%0Asynthetic%20and%20human-written%20text-adventure%20games%20designed%20to%20challenge%20and%0Aevaluate%20diverse%20reasoning%20capabilities.%20We%20present%20results%20over%20a%20range%20of%0ALLMs%2C%20open-%20and%20closed-weights%2C%20performing%20a%20qualitative%20analysis%20on%20the%20top%0Aperforming%20models.%20Despite%20an%20impressive%20showing%20on%20synthetic%20games%2C%20even%20the%0Atop%20LLM-driven%20agents%20fail%20to%20achieve%2015%25%20on%20games%20designed%20for%20human%0Aenjoyment.%20Code%20and%20visualization%20of%20the%20experiments%20can%20be%20found%20at%0Ahttps%3A//microsoft.github.io/tales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14128v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTALES%253A%2520Text%2520Adventure%2520Learning%2520Environment%2520Suite%26entry.906535625%3DChristopher%2520Zhang%2520Cui%2520and%2520Xingdi%2520Yuan%2520and%2520Ziang%2520Xiao%2520and%2520Prithviraj%2520Ammanabrolu%2520and%2520Marc-Alexandre%2520C%25C3%25B4t%25C3%25A9%26entry.1292438233%3D%2520%2520Reasoning%2520is%2520an%2520essential%2520skill%2520to%2520enable%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Ainteract%2520with%2520the%2520world.%2520As%2520tasks%2520become%2520more%2520complex%252C%2520they%2520demand%2520increasingly%250Asophisticated%2520and%2520diverse%2520reasoning%2520capabilities%2520for%2520sequential%250Adecision-making%252C%2520requiring%2520structured%2520reasoning%2520over%2520the%2520context%2520history%2520to%250Adetermine%2520the%2520next%2520best%2520action.%2520We%2520introduce%2520TALES%252C%2520a%2520diverse%2520collection%2520of%250Asynthetic%2520and%2520human-written%2520text-adventure%2520games%2520designed%2520to%2520challenge%2520and%250Aevaluate%2520diverse%2520reasoning%2520capabilities.%2520We%2520present%2520results%2520over%2520a%2520range%2520of%250ALLMs%252C%2520open-%2520and%2520closed-weights%252C%2520performing%2520a%2520qualitative%2520analysis%2520on%2520the%2520top%250Aperforming%2520models.%2520Despite%2520an%2520impressive%2520showing%2520on%2520synthetic%2520games%252C%2520even%2520the%250Atop%2520LLM-driven%2520agents%2520fail%2520to%2520achieve%252015%2525%2520on%2520games%2520designed%2520for%2520human%250Aenjoyment.%2520Code%2520and%2520visualization%2520of%2520the%2520experiments%2520can%2520be%2520found%2520at%250Ahttps%253A//microsoft.github.io/tales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14128v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TALES%3A%20Text%20Adventure%20Learning%20Environment%20Suite&entry.906535625=Christopher%20Zhang%20Cui%20and%20Xingdi%20Yuan%20and%20Ziang%20Xiao%20and%20Prithviraj%20Ammanabrolu%20and%20Marc-Alexandre%20C%C3%B4t%C3%A9&entry.1292438233=%20%20Reasoning%20is%20an%20essential%20skill%20to%20enable%20Large%20Language%20Models%20%28LLMs%29%20to%0Ainteract%20with%20the%20world.%20As%20tasks%20become%20more%20complex%2C%20they%20demand%20increasingly%0Asophisticated%20and%20diverse%20reasoning%20capabilities%20for%20sequential%0Adecision-making%2C%20requiring%20structured%20reasoning%20over%20the%20context%20history%20to%0Adetermine%20the%20next%20best%20action.%20We%20introduce%20TALES%2C%20a%20diverse%20collection%20of%0Asynthetic%20and%20human-written%20text-adventure%20games%20designed%20to%20challenge%20and%0Aevaluate%20diverse%20reasoning%20capabilities.%20We%20present%20results%20over%20a%20range%20of%0ALLMs%2C%20open-%20and%20closed-weights%2C%20performing%20a%20qualitative%20analysis%20on%20the%20top%0Aperforming%20models.%20Despite%20an%20impressive%20showing%20on%20synthetic%20games%2C%20even%20the%0Atop%20LLM-driven%20agents%20fail%20to%20achieve%2015%25%20on%20games%20designed%20for%20human%0Aenjoyment.%20Code%20and%20visualization%20of%20the%20experiments%20can%20be%20found%20at%0Ahttps%3A//microsoft.github.io/tales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14128v3&entry.124074799=Read"},
{"title": "Comparative Performance Evaluation of Large Language Models for\n  Extracting Molecular Interactions and Pathway Knowledge", "author": "Gilchan Park and Byung-Jun Yoon and Xihaier Luo and Vanessa L\u00f3pez-Marrero and Shinjae Yoo and Shantenu Jha", "abstract": "  Background: Identification of the interactions and regulatory relations\nbetween biomolecules play pivotal roles in understanding complex biological\nsystems and the mechanisms underlying diverse biological functions. However,\nthe collection of such molecular interactions has heavily relied on expert\ncuration in the past, making it labor-intensive and time-consuming. To mitigate\nthese challenges, we propose leveraging the capabilities of large language\nmodels (LLMs) to automate genome-scale extraction of this crucial knowledge.\n  Results: In this study, we investigate the efficacy of various LLMs in\naddressing biological tasks, such as the recognition of protein interactions,\nidentification of genes linked to pathways affected by low-dose radiation, and\nthe delineation of gene regulatory relationships. Overall, the larger models\nexhibited superior performance, indicating their potential for specific tasks\nthat involve the extraction of complex interactions among genes and proteins.\nAlthough these models possessed detailed information for distinct gene and\nprotein groups, they faced challenges in identifying groups with diverse\nfunctions and in recognizing highly correlated gene regulatory relationships.\n  Conclusions: By conducting a comprehensive assessment of the state-of-the-art\nmodels using well-established molecular interaction and pathway databases, our\nstudy reveals that LLMs can identify genes/proteins associated with pathways of\ninterest and predict their interactions to a certain extent. Furthermore, these\nmodels can provide important insights, marking a noteworthy stride toward\nadvancing our understanding of biological systems through AI-assisted knowledge\ndiscovery.\n", "link": "http://arxiv.org/abs/2307.08813v4", "date": "2025-04-23", "relevancy": 2.0648, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Performance%20Evaluation%20of%20Large%20Language%20Models%20for%0A%20%20Extracting%20Molecular%20Interactions%20and%20Pathway%20Knowledge&body=Title%3A%20Comparative%20Performance%20Evaluation%20of%20Large%20Language%20Models%20for%0A%20%20Extracting%20Molecular%20Interactions%20and%20Pathway%20Knowledge%0AAuthor%3A%20Gilchan%20Park%20and%20Byung-Jun%20Yoon%20and%20Xihaier%20Luo%20and%20Vanessa%20L%C3%B3pez-Marrero%20and%20Shinjae%20Yoo%20and%20Shantenu%20Jha%0AAbstract%3A%20%20%20Background%3A%20Identification%20of%20the%20interactions%20and%20regulatory%20relations%0Abetween%20biomolecules%20play%20pivotal%20roles%20in%20understanding%20complex%20biological%0Asystems%20and%20the%20mechanisms%20underlying%20diverse%20biological%20functions.%20However%2C%0Athe%20collection%20of%20such%20molecular%20interactions%20has%20heavily%20relied%20on%20expert%0Acuration%20in%20the%20past%2C%20making%20it%20labor-intensive%20and%20time-consuming.%20To%20mitigate%0Athese%20challenges%2C%20we%20propose%20leveraging%20the%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%20to%20automate%20genome-scale%20extraction%20of%20this%20crucial%20knowledge.%0A%20%20Results%3A%20In%20this%20study%2C%20we%20investigate%20the%20efficacy%20of%20various%20LLMs%20in%0Aaddressing%20biological%20tasks%2C%20such%20as%20the%20recognition%20of%20protein%20interactions%2C%0Aidentification%20of%20genes%20linked%20to%20pathways%20affected%20by%20low-dose%20radiation%2C%20and%0Athe%20delineation%20of%20gene%20regulatory%20relationships.%20Overall%2C%20the%20larger%20models%0Aexhibited%20superior%20performance%2C%20indicating%20their%20potential%20for%20specific%20tasks%0Athat%20involve%20the%20extraction%20of%20complex%20interactions%20among%20genes%20and%20proteins.%0AAlthough%20these%20models%20possessed%20detailed%20information%20for%20distinct%20gene%20and%0Aprotein%20groups%2C%20they%20faced%20challenges%20in%20identifying%20groups%20with%20diverse%0Afunctions%20and%20in%20recognizing%20highly%20correlated%20gene%20regulatory%20relationships.%0A%20%20Conclusions%3A%20By%20conducting%20a%20comprehensive%20assessment%20of%20the%20state-of-the-art%0Amodels%20using%20well-established%20molecular%20interaction%20and%20pathway%20databases%2C%20our%0Astudy%20reveals%20that%20LLMs%20can%20identify%20genes/proteins%20associated%20with%20pathways%20of%0Ainterest%20and%20predict%20their%20interactions%20to%20a%20certain%20extent.%20Furthermore%2C%20these%0Amodels%20can%20provide%20important%20insights%2C%20marking%20a%20noteworthy%20stride%20toward%0Aadvancing%20our%20understanding%20of%20biological%20systems%20through%20AI-assisted%20knowledge%0Adiscovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08813v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Performance%2520Evaluation%2520of%2520Large%2520Language%2520Models%2520for%250A%2520%2520Extracting%2520Molecular%2520Interactions%2520and%2520Pathway%2520Knowledge%26entry.906535625%3DGilchan%2520Park%2520and%2520Byung-Jun%2520Yoon%2520and%2520Xihaier%2520Luo%2520and%2520Vanessa%2520L%25C3%25B3pez-Marrero%2520and%2520Shinjae%2520Yoo%2520and%2520Shantenu%2520Jha%26entry.1292438233%3D%2520%2520Background%253A%2520Identification%2520of%2520the%2520interactions%2520and%2520regulatory%2520relations%250Abetween%2520biomolecules%2520play%2520pivotal%2520roles%2520in%2520understanding%2520complex%2520biological%250Asystems%2520and%2520the%2520mechanisms%2520underlying%2520diverse%2520biological%2520functions.%2520However%252C%250Athe%2520collection%2520of%2520such%2520molecular%2520interactions%2520has%2520heavily%2520relied%2520on%2520expert%250Acuration%2520in%2520the%2520past%252C%2520making%2520it%2520labor-intensive%2520and%2520time-consuming.%2520To%2520mitigate%250Athese%2520challenges%252C%2520we%2520propose%2520leveraging%2520the%2520capabilities%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520automate%2520genome-scale%2520extraction%2520of%2520this%2520crucial%2520knowledge.%250A%2520%2520Results%253A%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520efficacy%2520of%2520various%2520LLMs%2520in%250Aaddressing%2520biological%2520tasks%252C%2520such%2520as%2520the%2520recognition%2520of%2520protein%2520interactions%252C%250Aidentification%2520of%2520genes%2520linked%2520to%2520pathways%2520affected%2520by%2520low-dose%2520radiation%252C%2520and%250Athe%2520delineation%2520of%2520gene%2520regulatory%2520relationships.%2520Overall%252C%2520the%2520larger%2520models%250Aexhibited%2520superior%2520performance%252C%2520indicating%2520their%2520potential%2520for%2520specific%2520tasks%250Athat%2520involve%2520the%2520extraction%2520of%2520complex%2520interactions%2520among%2520genes%2520and%2520proteins.%250AAlthough%2520these%2520models%2520possessed%2520detailed%2520information%2520for%2520distinct%2520gene%2520and%250Aprotein%2520groups%252C%2520they%2520faced%2520challenges%2520in%2520identifying%2520groups%2520with%2520diverse%250Afunctions%2520and%2520in%2520recognizing%2520highly%2520correlated%2520gene%2520regulatory%2520relationships.%250A%2520%2520Conclusions%253A%2520By%2520conducting%2520a%2520comprehensive%2520assessment%2520of%2520the%2520state-of-the-art%250Amodels%2520using%2520well-established%2520molecular%2520interaction%2520and%2520pathway%2520databases%252C%2520our%250Astudy%2520reveals%2520that%2520LLMs%2520can%2520identify%2520genes/proteins%2520associated%2520with%2520pathways%2520of%250Ainterest%2520and%2520predict%2520their%2520interactions%2520to%2520a%2520certain%2520extent.%2520Furthermore%252C%2520these%250Amodels%2520can%2520provide%2520important%2520insights%252C%2520marking%2520a%2520noteworthy%2520stride%2520toward%250Aadvancing%2520our%2520understanding%2520of%2520biological%2520systems%2520through%2520AI-assisted%2520knowledge%250Adiscovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.08813v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Performance%20Evaluation%20of%20Large%20Language%20Models%20for%0A%20%20Extracting%20Molecular%20Interactions%20and%20Pathway%20Knowledge&entry.906535625=Gilchan%20Park%20and%20Byung-Jun%20Yoon%20and%20Xihaier%20Luo%20and%20Vanessa%20L%C3%B3pez-Marrero%20and%20Shinjae%20Yoo%20and%20Shantenu%20Jha&entry.1292438233=%20%20Background%3A%20Identification%20of%20the%20interactions%20and%20regulatory%20relations%0Abetween%20biomolecules%20play%20pivotal%20roles%20in%20understanding%20complex%20biological%0Asystems%20and%20the%20mechanisms%20underlying%20diverse%20biological%20functions.%20However%2C%0Athe%20collection%20of%20such%20molecular%20interactions%20has%20heavily%20relied%20on%20expert%0Acuration%20in%20the%20past%2C%20making%20it%20labor-intensive%20and%20time-consuming.%20To%20mitigate%0Athese%20challenges%2C%20we%20propose%20leveraging%20the%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%20to%20automate%20genome-scale%20extraction%20of%20this%20crucial%20knowledge.%0A%20%20Results%3A%20In%20this%20study%2C%20we%20investigate%20the%20efficacy%20of%20various%20LLMs%20in%0Aaddressing%20biological%20tasks%2C%20such%20as%20the%20recognition%20of%20protein%20interactions%2C%0Aidentification%20of%20genes%20linked%20to%20pathways%20affected%20by%20low-dose%20radiation%2C%20and%0Athe%20delineation%20of%20gene%20regulatory%20relationships.%20Overall%2C%20the%20larger%20models%0Aexhibited%20superior%20performance%2C%20indicating%20their%20potential%20for%20specific%20tasks%0Athat%20involve%20the%20extraction%20of%20complex%20interactions%20among%20genes%20and%20proteins.%0AAlthough%20these%20models%20possessed%20detailed%20information%20for%20distinct%20gene%20and%0Aprotein%20groups%2C%20they%20faced%20challenges%20in%20identifying%20groups%20with%20diverse%0Afunctions%20and%20in%20recognizing%20highly%20correlated%20gene%20regulatory%20relationships.%0A%20%20Conclusions%3A%20By%20conducting%20a%20comprehensive%20assessment%20of%20the%20state-of-the-art%0Amodels%20using%20well-established%20molecular%20interaction%20and%20pathway%20databases%2C%20our%0Astudy%20reveals%20that%20LLMs%20can%20identify%20genes/proteins%20associated%20with%20pathways%20of%0Ainterest%20and%20predict%20their%20interactions%20to%20a%20certain%20extent.%20Furthermore%2C%20these%0Amodels%20can%20provide%20important%20insights%2C%20marking%20a%20noteworthy%20stride%20toward%0Aadvancing%20our%20understanding%20of%20biological%20systems%20through%20AI-assisted%20knowledge%0Adiscovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08813v4&entry.124074799=Read"},
{"title": "Procedural Dataset Generation for Zero-Shot Stereo Matching", "author": "David Yan and Alexander Raistrick and Jia Deng", "abstract": "  Synthetic datasets are a crucial ingredient for training stereo matching\nnetworks, but the question of what makes a stereo dataset effective remains\nlargely unexplored. We investigate the design space of synthetic datasets by\nvarying the parameters of a procedural dataset generator, and report the\neffects on zero-shot stereo matching performance using standard benchmarks. We\ncollect the best settings to produce Infinigen-Stereo, a procedural generator\nspecifically optimized for zero-shot stereo datasets. Models trained only on\ndata from our system outperform robust baselines trained on a combination of\nexisting synthetic datasets and have stronger zero-shot stereo matching\nperformance than public checkpoints from prior works. We open source our system\nat https://github.com/princeton-vl/InfinigenStereo to enable further research\non procedural stereo datasets.\n", "link": "http://arxiv.org/abs/2504.16930v1", "date": "2025-04-23", "relevancy": 2.0605, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5333}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Procedural%20Dataset%20Generation%20for%20Zero-Shot%20Stereo%20Matching&body=Title%3A%20Procedural%20Dataset%20Generation%20for%20Zero-Shot%20Stereo%20Matching%0AAuthor%3A%20David%20Yan%20and%20Alexander%20Raistrick%20and%20Jia%20Deng%0AAbstract%3A%20%20%20Synthetic%20datasets%20are%20a%20crucial%20ingredient%20for%20training%20stereo%20matching%0Anetworks%2C%20but%20the%20question%20of%20what%20makes%20a%20stereo%20dataset%20effective%20remains%0Alargely%20unexplored.%20We%20investigate%20the%20design%20space%20of%20synthetic%20datasets%20by%0Avarying%20the%20parameters%20of%20a%20procedural%20dataset%20generator%2C%20and%20report%20the%0Aeffects%20on%20zero-shot%20stereo%20matching%20performance%20using%20standard%20benchmarks.%20We%0Acollect%20the%20best%20settings%20to%20produce%20Infinigen-Stereo%2C%20a%20procedural%20generator%0Aspecifically%20optimized%20for%20zero-shot%20stereo%20datasets.%20Models%20trained%20only%20on%0Adata%20from%20our%20system%20outperform%20robust%20baselines%20trained%20on%20a%20combination%20of%0Aexisting%20synthetic%20datasets%20and%20have%20stronger%20zero-shot%20stereo%20matching%0Aperformance%20than%20public%20checkpoints%20from%20prior%20works.%20We%20open%20source%20our%20system%0Aat%20https%3A//github.com/princeton-vl/InfinigenStereo%20to%20enable%20further%20research%0Aon%20procedural%20stereo%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcedural%2520Dataset%2520Generation%2520for%2520Zero-Shot%2520Stereo%2520Matching%26entry.906535625%3DDavid%2520Yan%2520and%2520Alexander%2520Raistrick%2520and%2520Jia%2520Deng%26entry.1292438233%3D%2520%2520Synthetic%2520datasets%2520are%2520a%2520crucial%2520ingredient%2520for%2520training%2520stereo%2520matching%250Anetworks%252C%2520but%2520the%2520question%2520of%2520what%2520makes%2520a%2520stereo%2520dataset%2520effective%2520remains%250Alargely%2520unexplored.%2520We%2520investigate%2520the%2520design%2520space%2520of%2520synthetic%2520datasets%2520by%250Avarying%2520the%2520parameters%2520of%2520a%2520procedural%2520dataset%2520generator%252C%2520and%2520report%2520the%250Aeffects%2520on%2520zero-shot%2520stereo%2520matching%2520performance%2520using%2520standard%2520benchmarks.%2520We%250Acollect%2520the%2520best%2520settings%2520to%2520produce%2520Infinigen-Stereo%252C%2520a%2520procedural%2520generator%250Aspecifically%2520optimized%2520for%2520zero-shot%2520stereo%2520datasets.%2520Models%2520trained%2520only%2520on%250Adata%2520from%2520our%2520system%2520outperform%2520robust%2520baselines%2520trained%2520on%2520a%2520combination%2520of%250Aexisting%2520synthetic%2520datasets%2520and%2520have%2520stronger%2520zero-shot%2520stereo%2520matching%250Aperformance%2520than%2520public%2520checkpoints%2520from%2520prior%2520works.%2520We%2520open%2520source%2520our%2520system%250Aat%2520https%253A//github.com/princeton-vl/InfinigenStereo%2520to%2520enable%2520further%2520research%250Aon%2520procedural%2520stereo%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Procedural%20Dataset%20Generation%20for%20Zero-Shot%20Stereo%20Matching&entry.906535625=David%20Yan%20and%20Alexander%20Raistrick%20and%20Jia%20Deng&entry.1292438233=%20%20Synthetic%20datasets%20are%20a%20crucial%20ingredient%20for%20training%20stereo%20matching%0Anetworks%2C%20but%20the%20question%20of%20what%20makes%20a%20stereo%20dataset%20effective%20remains%0Alargely%20unexplored.%20We%20investigate%20the%20design%20space%20of%20synthetic%20datasets%20by%0Avarying%20the%20parameters%20of%20a%20procedural%20dataset%20generator%2C%20and%20report%20the%0Aeffects%20on%20zero-shot%20stereo%20matching%20performance%20using%20standard%20benchmarks.%20We%0Acollect%20the%20best%20settings%20to%20produce%20Infinigen-Stereo%2C%20a%20procedural%20generator%0Aspecifically%20optimized%20for%20zero-shot%20stereo%20datasets.%20Models%20trained%20only%20on%0Adata%20from%20our%20system%20outperform%20robust%20baselines%20trained%20on%20a%20combination%20of%0Aexisting%20synthetic%20datasets%20and%20have%20stronger%20zero-shot%20stereo%20matching%0Aperformance%20than%20public%20checkpoints%20from%20prior%20works.%20We%20open%20source%20our%20system%0Aat%20https%3A//github.com/princeton-vl/InfinigenStereo%20to%20enable%20further%20research%0Aon%20procedural%20stereo%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16930v1&entry.124074799=Read"},
{"title": "On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion\n  Rate Prediction", "author": "Jinfeng Zhuang and Yinrui Li and Runze Su and Ke Xu and Zhixuan Shao and Kungang Li and Ling Leng and Han Sun and Meng Qi and Yixiong Meng and Yang Tang and Zhifang Liu and Qifei Shen and Aayush Mudgal and Caleb Lu and Jie Liu and Hongda Shen", "abstract": "  The predictions of click through rate (CTR) and conversion rate (CVR) play a\ncrucial role in the success of ad-recommendation systems. A Deep Hierarchical\nEnsemble Network (DHEN) has been proposed to integrate multiple feature\ncrossing modules and has achieved great success in CTR prediction. However, its\nperformance for CVR prediction is unclear in the conversion ads setting, where\nan ad bids for the probability of a user's off-site actions on a third party\nwebsite or app, including purchase, add to cart, sign up, etc. A few challenges\nin DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a\nfew) should be included in DHEN? 2) How deep and wide should DHEN be to achieve\nthe best trade-off between efficiency and efficacy? 3) What hyper-parameters to\nchoose in each feature-crossing module? Orthogonal to the model architecture,\nthe input personalization features also significantly impact model performance\nwith a high degree of freedom. In this paper, we attack this problem and\npresent our contributions biased to the applied data science side, including:\n  First, we propose a multitask learning framework with DHEN as the single\nbackbone model architecture to predict all CVR tasks, with a detailed study on\nhow to make DHEN work effectively in practice; Second, we build both on-site\nreal-time user behavior sequences and off-site conversion event sequences for\nCVR prediction purposes, and conduct ablation study on its importance; Last but\nnot least, we propose a self-supervised auxiliary loss to predict future\nactions in the input sequence, to help resolve the label sparseness issue in\nCVR prediction.\n  Our method achieves state-of-the-art performance compared to previous single\nfeature crossing modules with pre-trained user personalization features.\n", "link": "http://arxiv.org/abs/2504.08169v3", "date": "2025-04-23", "relevancy": 2.0387, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5446}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4851}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Practice%20of%20Deep%20Hierarchical%20Ensemble%20Network%20for%20Ad%20Conversion%0A%20%20Rate%20Prediction&body=Title%3A%20On%20the%20Practice%20of%20Deep%20Hierarchical%20Ensemble%20Network%20for%20Ad%20Conversion%0A%20%20Rate%20Prediction%0AAuthor%3A%20Jinfeng%20Zhuang%20and%20Yinrui%20Li%20and%20Runze%20Su%20and%20Ke%20Xu%20and%20Zhixuan%20Shao%20and%20Kungang%20Li%20and%20Ling%20Leng%20and%20Han%20Sun%20and%20Meng%20Qi%20and%20Yixiong%20Meng%20and%20Yang%20Tang%20and%20Zhifang%20Liu%20and%20Qifei%20Shen%20and%20Aayush%20Mudgal%20and%20Caleb%20Lu%20and%20Jie%20Liu%20and%20Hongda%20Shen%0AAbstract%3A%20%20%20The%20predictions%20of%20click%20through%20rate%20%28CTR%29%20and%20conversion%20rate%20%28CVR%29%20play%20a%0Acrucial%20role%20in%20the%20success%20of%20ad-recommendation%20systems.%20A%20Deep%20Hierarchical%0AEnsemble%20Network%20%28DHEN%29%20has%20been%20proposed%20to%20integrate%20multiple%20feature%0Acrossing%20modules%20and%20has%20achieved%20great%20success%20in%20CTR%20prediction.%20However%2C%20its%0Aperformance%20for%20CVR%20prediction%20is%20unclear%20in%20the%20conversion%20ads%20setting%2C%20where%0Aan%20ad%20bids%20for%20the%20probability%20of%20a%20user%27s%20off-site%20actions%20on%20a%20third%20party%0Awebsite%20or%20app%2C%20including%20purchase%2C%20add%20to%20cart%2C%20sign%20up%2C%20etc.%20A%20few%20challenges%0Ain%20DHEN%3A%201%29%20What%20feature-crossing%20modules%20%28MLP%2C%20DCN%2C%20Transformer%2C%20to%20name%20a%0Afew%29%20should%20be%20included%20in%20DHEN%3F%202%29%20How%20deep%20and%20wide%20should%20DHEN%20be%20to%20achieve%0Athe%20best%20trade-off%20between%20efficiency%20and%20efficacy%3F%203%29%20What%20hyper-parameters%20to%0Achoose%20in%20each%20feature-crossing%20module%3F%20Orthogonal%20to%20the%20model%20architecture%2C%0Athe%20input%20personalization%20features%20also%20significantly%20impact%20model%20performance%0Awith%20a%20high%20degree%20of%20freedom.%20In%20this%20paper%2C%20we%20attack%20this%20problem%20and%0Apresent%20our%20contributions%20biased%20to%20the%20applied%20data%20science%20side%2C%20including%3A%0A%20%20First%2C%20we%20propose%20a%20multitask%20learning%20framework%20with%20DHEN%20as%20the%20single%0Abackbone%20model%20architecture%20to%20predict%20all%20CVR%20tasks%2C%20with%20a%20detailed%20study%20on%0Ahow%20to%20make%20DHEN%20work%20effectively%20in%20practice%3B%20Second%2C%20we%20build%20both%20on-site%0Areal-time%20user%20behavior%20sequences%20and%20off-site%20conversion%20event%20sequences%20for%0ACVR%20prediction%20purposes%2C%20and%20conduct%20ablation%20study%20on%20its%20importance%3B%20Last%20but%0Anot%20least%2C%20we%20propose%20a%20self-supervised%20auxiliary%20loss%20to%20predict%20future%0Aactions%20in%20the%20input%20sequence%2C%20to%20help%20resolve%20the%20label%20sparseness%20issue%20in%0ACVR%20prediction.%0A%20%20Our%20method%20achieves%20state-of-the-art%20performance%20compared%20to%20previous%20single%0Afeature%20crossing%20modules%20with%20pre-trained%20user%20personalization%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08169v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Practice%2520of%2520Deep%2520Hierarchical%2520Ensemble%2520Network%2520for%2520Ad%2520Conversion%250A%2520%2520Rate%2520Prediction%26entry.906535625%3DJinfeng%2520Zhuang%2520and%2520Yinrui%2520Li%2520and%2520Runze%2520Su%2520and%2520Ke%2520Xu%2520and%2520Zhixuan%2520Shao%2520and%2520Kungang%2520Li%2520and%2520Ling%2520Leng%2520and%2520Han%2520Sun%2520and%2520Meng%2520Qi%2520and%2520Yixiong%2520Meng%2520and%2520Yang%2520Tang%2520and%2520Zhifang%2520Liu%2520and%2520Qifei%2520Shen%2520and%2520Aayush%2520Mudgal%2520and%2520Caleb%2520Lu%2520and%2520Jie%2520Liu%2520and%2520Hongda%2520Shen%26entry.1292438233%3D%2520%2520The%2520predictions%2520of%2520click%2520through%2520rate%2520%2528CTR%2529%2520and%2520conversion%2520rate%2520%2528CVR%2529%2520play%2520a%250Acrucial%2520role%2520in%2520the%2520success%2520of%2520ad-recommendation%2520systems.%2520A%2520Deep%2520Hierarchical%250AEnsemble%2520Network%2520%2528DHEN%2529%2520has%2520been%2520proposed%2520to%2520integrate%2520multiple%2520feature%250Acrossing%2520modules%2520and%2520has%2520achieved%2520great%2520success%2520in%2520CTR%2520prediction.%2520However%252C%2520its%250Aperformance%2520for%2520CVR%2520prediction%2520is%2520unclear%2520in%2520the%2520conversion%2520ads%2520setting%252C%2520where%250Aan%2520ad%2520bids%2520for%2520the%2520probability%2520of%2520a%2520user%2527s%2520off-site%2520actions%2520on%2520a%2520third%2520party%250Awebsite%2520or%2520app%252C%2520including%2520purchase%252C%2520add%2520to%2520cart%252C%2520sign%2520up%252C%2520etc.%2520A%2520few%2520challenges%250Ain%2520DHEN%253A%25201%2529%2520What%2520feature-crossing%2520modules%2520%2528MLP%252C%2520DCN%252C%2520Transformer%252C%2520to%2520name%2520a%250Afew%2529%2520should%2520be%2520included%2520in%2520DHEN%253F%25202%2529%2520How%2520deep%2520and%2520wide%2520should%2520DHEN%2520be%2520to%2520achieve%250Athe%2520best%2520trade-off%2520between%2520efficiency%2520and%2520efficacy%253F%25203%2529%2520What%2520hyper-parameters%2520to%250Achoose%2520in%2520each%2520feature-crossing%2520module%253F%2520Orthogonal%2520to%2520the%2520model%2520architecture%252C%250Athe%2520input%2520personalization%2520features%2520also%2520significantly%2520impact%2520model%2520performance%250Awith%2520a%2520high%2520degree%2520of%2520freedom.%2520In%2520this%2520paper%252C%2520we%2520attack%2520this%2520problem%2520and%250Apresent%2520our%2520contributions%2520biased%2520to%2520the%2520applied%2520data%2520science%2520side%252C%2520including%253A%250A%2520%2520First%252C%2520we%2520propose%2520a%2520multitask%2520learning%2520framework%2520with%2520DHEN%2520as%2520the%2520single%250Abackbone%2520model%2520architecture%2520to%2520predict%2520all%2520CVR%2520tasks%252C%2520with%2520a%2520detailed%2520study%2520on%250Ahow%2520to%2520make%2520DHEN%2520work%2520effectively%2520in%2520practice%253B%2520Second%252C%2520we%2520build%2520both%2520on-site%250Areal-time%2520user%2520behavior%2520sequences%2520and%2520off-site%2520conversion%2520event%2520sequences%2520for%250ACVR%2520prediction%2520purposes%252C%2520and%2520conduct%2520ablation%2520study%2520on%2520its%2520importance%253B%2520Last%2520but%250Anot%2520least%252C%2520we%2520propose%2520a%2520self-supervised%2520auxiliary%2520loss%2520to%2520predict%2520future%250Aactions%2520in%2520the%2520input%2520sequence%252C%2520to%2520help%2520resolve%2520the%2520label%2520sparseness%2520issue%2520in%250ACVR%2520prediction.%250A%2520%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520compared%2520to%2520previous%2520single%250Afeature%2520crossing%2520modules%2520with%2520pre-trained%2520user%2520personalization%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08169v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Practice%20of%20Deep%20Hierarchical%20Ensemble%20Network%20for%20Ad%20Conversion%0A%20%20Rate%20Prediction&entry.906535625=Jinfeng%20Zhuang%20and%20Yinrui%20Li%20and%20Runze%20Su%20and%20Ke%20Xu%20and%20Zhixuan%20Shao%20and%20Kungang%20Li%20and%20Ling%20Leng%20and%20Han%20Sun%20and%20Meng%20Qi%20and%20Yixiong%20Meng%20and%20Yang%20Tang%20and%20Zhifang%20Liu%20and%20Qifei%20Shen%20and%20Aayush%20Mudgal%20and%20Caleb%20Lu%20and%20Jie%20Liu%20and%20Hongda%20Shen&entry.1292438233=%20%20The%20predictions%20of%20click%20through%20rate%20%28CTR%29%20and%20conversion%20rate%20%28CVR%29%20play%20a%0Acrucial%20role%20in%20the%20success%20of%20ad-recommendation%20systems.%20A%20Deep%20Hierarchical%0AEnsemble%20Network%20%28DHEN%29%20has%20been%20proposed%20to%20integrate%20multiple%20feature%0Acrossing%20modules%20and%20has%20achieved%20great%20success%20in%20CTR%20prediction.%20However%2C%20its%0Aperformance%20for%20CVR%20prediction%20is%20unclear%20in%20the%20conversion%20ads%20setting%2C%20where%0Aan%20ad%20bids%20for%20the%20probability%20of%20a%20user%27s%20off-site%20actions%20on%20a%20third%20party%0Awebsite%20or%20app%2C%20including%20purchase%2C%20add%20to%20cart%2C%20sign%20up%2C%20etc.%20A%20few%20challenges%0Ain%20DHEN%3A%201%29%20What%20feature-crossing%20modules%20%28MLP%2C%20DCN%2C%20Transformer%2C%20to%20name%20a%0Afew%29%20should%20be%20included%20in%20DHEN%3F%202%29%20How%20deep%20and%20wide%20should%20DHEN%20be%20to%20achieve%0Athe%20best%20trade-off%20between%20efficiency%20and%20efficacy%3F%203%29%20What%20hyper-parameters%20to%0Achoose%20in%20each%20feature-crossing%20module%3F%20Orthogonal%20to%20the%20model%20architecture%2C%0Athe%20input%20personalization%20features%20also%20significantly%20impact%20model%20performance%0Awith%20a%20high%20degree%20of%20freedom.%20In%20this%20paper%2C%20we%20attack%20this%20problem%20and%0Apresent%20our%20contributions%20biased%20to%20the%20applied%20data%20science%20side%2C%20including%3A%0A%20%20First%2C%20we%20propose%20a%20multitask%20learning%20framework%20with%20DHEN%20as%20the%20single%0Abackbone%20model%20architecture%20to%20predict%20all%20CVR%20tasks%2C%20with%20a%20detailed%20study%20on%0Ahow%20to%20make%20DHEN%20work%20effectively%20in%20practice%3B%20Second%2C%20we%20build%20both%20on-site%0Areal-time%20user%20behavior%20sequences%20and%20off-site%20conversion%20event%20sequences%20for%0ACVR%20prediction%20purposes%2C%20and%20conduct%20ablation%20study%20on%20its%20importance%3B%20Last%20but%0Anot%20least%2C%20we%20propose%20a%20self-supervised%20auxiliary%20loss%20to%20predict%20future%0Aactions%20in%20the%20input%20sequence%2C%20to%20help%20resolve%20the%20label%20sparseness%20issue%20in%0ACVR%20prediction.%0A%20%20Our%20method%20achieves%20state-of-the-art%20performance%20compared%20to%20previous%20single%0Afeature%20crossing%20modules%20with%20pre-trained%20user%20personalization%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08169v3&entry.124074799=Read"},
{"title": "aiXamine: Simplified LLM Safety and Security", "author": "Fatih Deniz and Dorde Popovic and Yazan Boshmaf and Euisuh Jeong and Minhaj Ahmad and Sanjay Chawla and Issa Khalil", "abstract": "  Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices.\n", "link": "http://arxiv.org/abs/2504.14985v2", "date": "2025-04-23", "relevancy": 2.034, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20aiXamine%3A%20Simplified%20LLM%20Safety%20and%20Security&body=Title%3A%20aiXamine%3A%20Simplified%20LLM%20Safety%20and%20Security%0AAuthor%3A%20Fatih%20Deniz%20and%20Dorde%20Popovic%20and%20Yazan%20Boshmaf%20and%20Euisuh%20Jeong%20and%20Minhaj%20Ahmad%20and%20Sanjay%20Chawla%20and%20Issa%20Khalil%0AAbstract%3A%20%20%20Evaluating%20Large%20Language%20Models%20%28LLMs%29%20for%20safety%20and%20security%20remains%20a%0Acomplex%20task%2C%20often%20requiring%20users%20to%20navigate%20a%20fragmented%20landscape%20of%20ad%0Ahoc%20benchmarks%2C%20datasets%2C%20metrics%2C%20and%20reporting%20formats.%20To%20address%20this%0Achallenge%2C%20we%20present%20aiXamine%2C%20a%20comprehensive%20black-box%20evaluation%20platform%0Afor%20LLM%20safety%20and%20security.%20aiXamine%20integrates%20over%2040%20tests%20%28i.e.%2C%0Abenchmarks%29%20organized%20into%20eight%20key%20services%20targeting%20specific%20dimensions%20of%0Asafety%20and%20security%3A%20adversarial%20robustness%2C%20code%20security%2C%20fairness%20and%20bias%2C%0Ahallucination%2C%20model%20and%20data%20privacy%2C%20out-of-distribution%20%28OOD%29%20robustness%2C%0Aover-refusal%2C%20and%20safety%20alignment.%20The%20platform%20aggregates%20the%20evaluation%0Aresults%20into%20a%20single%20detailed%20report%20per%20model%2C%20providing%20a%20detailed%20breakdown%0Aof%20model%20performance%2C%20test%20examples%2C%20and%20rich%20visualizations.%20We%20used%20aiXamine%0Ato%20assess%20over%2050%20publicly%20available%20and%20proprietary%20LLMs%2C%20conducting%20over%202K%0Aexaminations.%20Our%20findings%20reveal%20notable%20vulnerabilities%20in%20leading%20models%2C%0Aincluding%20susceptibility%20to%20adversarial%20attacks%20in%20OpenAI%27s%20GPT-4o%2C%20biased%0Aoutputs%20in%20xAI%27s%20Grok-3%2C%20and%20privacy%20weaknesses%20in%20Google%27s%20Gemini%202.0.%0AAdditionally%2C%20we%20observe%20that%20open-source%20models%20can%20match%20or%20exceed%0Aproprietary%20models%20in%20specific%20services%20such%20as%20safety%20alignment%2C%20fairness%20and%0Abias%2C%20and%20OOD%20robustness.%20Finally%2C%20we%20identify%20trade-offs%20between%20distillation%0Astrategies%2C%20model%20size%2C%20training%20methods%2C%20and%20architectural%20choices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14985v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DaiXamine%253A%2520Simplified%2520LLM%2520Safety%2520and%2520Security%26entry.906535625%3DFatih%2520Deniz%2520and%2520Dorde%2520Popovic%2520and%2520Yazan%2520Boshmaf%2520and%2520Euisuh%2520Jeong%2520and%2520Minhaj%2520Ahmad%2520and%2520Sanjay%2520Chawla%2520and%2520Issa%2520Khalil%26entry.1292438233%3D%2520%2520Evaluating%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520safety%2520and%2520security%2520remains%2520a%250Acomplex%2520task%252C%2520often%2520requiring%2520users%2520to%2520navigate%2520a%2520fragmented%2520landscape%2520of%2520ad%250Ahoc%2520benchmarks%252C%2520datasets%252C%2520metrics%252C%2520and%2520reporting%2520formats.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520present%2520aiXamine%252C%2520a%2520comprehensive%2520black-box%2520evaluation%2520platform%250Afor%2520LLM%2520safety%2520and%2520security.%2520aiXamine%2520integrates%2520over%252040%2520tests%2520%2528i.e.%252C%250Abenchmarks%2529%2520organized%2520into%2520eight%2520key%2520services%2520targeting%2520specific%2520dimensions%2520of%250Asafety%2520and%2520security%253A%2520adversarial%2520robustness%252C%2520code%2520security%252C%2520fairness%2520and%2520bias%252C%250Ahallucination%252C%2520model%2520and%2520data%2520privacy%252C%2520out-of-distribution%2520%2528OOD%2529%2520robustness%252C%250Aover-refusal%252C%2520and%2520safety%2520alignment.%2520The%2520platform%2520aggregates%2520the%2520evaluation%250Aresults%2520into%2520a%2520single%2520detailed%2520report%2520per%2520model%252C%2520providing%2520a%2520detailed%2520breakdown%250Aof%2520model%2520performance%252C%2520test%2520examples%252C%2520and%2520rich%2520visualizations.%2520We%2520used%2520aiXamine%250Ato%2520assess%2520over%252050%2520publicly%2520available%2520and%2520proprietary%2520LLMs%252C%2520conducting%2520over%25202K%250Aexaminations.%2520Our%2520findings%2520reveal%2520notable%2520vulnerabilities%2520in%2520leading%2520models%252C%250Aincluding%2520susceptibility%2520to%2520adversarial%2520attacks%2520in%2520OpenAI%2527s%2520GPT-4o%252C%2520biased%250Aoutputs%2520in%2520xAI%2527s%2520Grok-3%252C%2520and%2520privacy%2520weaknesses%2520in%2520Google%2527s%2520Gemini%25202.0.%250AAdditionally%252C%2520we%2520observe%2520that%2520open-source%2520models%2520can%2520match%2520or%2520exceed%250Aproprietary%2520models%2520in%2520specific%2520services%2520such%2520as%2520safety%2520alignment%252C%2520fairness%2520and%250Abias%252C%2520and%2520OOD%2520robustness.%2520Finally%252C%2520we%2520identify%2520trade-offs%2520between%2520distillation%250Astrategies%252C%2520model%2520size%252C%2520training%2520methods%252C%2520and%2520architectural%2520choices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14985v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=aiXamine%3A%20Simplified%20LLM%20Safety%20and%20Security&entry.906535625=Fatih%20Deniz%20and%20Dorde%20Popovic%20and%20Yazan%20Boshmaf%20and%20Euisuh%20Jeong%20and%20Minhaj%20Ahmad%20and%20Sanjay%20Chawla%20and%20Issa%20Khalil&entry.1292438233=%20%20Evaluating%20Large%20Language%20Models%20%28LLMs%29%20for%20safety%20and%20security%20remains%20a%0Acomplex%20task%2C%20often%20requiring%20users%20to%20navigate%20a%20fragmented%20landscape%20of%20ad%0Ahoc%20benchmarks%2C%20datasets%2C%20metrics%2C%20and%20reporting%20formats.%20To%20address%20this%0Achallenge%2C%20we%20present%20aiXamine%2C%20a%20comprehensive%20black-box%20evaluation%20platform%0Afor%20LLM%20safety%20and%20security.%20aiXamine%20integrates%20over%2040%20tests%20%28i.e.%2C%0Abenchmarks%29%20organized%20into%20eight%20key%20services%20targeting%20specific%20dimensions%20of%0Asafety%20and%20security%3A%20adversarial%20robustness%2C%20code%20security%2C%20fairness%20and%20bias%2C%0Ahallucination%2C%20model%20and%20data%20privacy%2C%20out-of-distribution%20%28OOD%29%20robustness%2C%0Aover-refusal%2C%20and%20safety%20alignment.%20The%20platform%20aggregates%20the%20evaluation%0Aresults%20into%20a%20single%20detailed%20report%20per%20model%2C%20providing%20a%20detailed%20breakdown%0Aof%20model%20performance%2C%20test%20examples%2C%20and%20rich%20visualizations.%20We%20used%20aiXamine%0Ato%20assess%20over%2050%20publicly%20available%20and%20proprietary%20LLMs%2C%20conducting%20over%202K%0Aexaminations.%20Our%20findings%20reveal%20notable%20vulnerabilities%20in%20leading%20models%2C%0Aincluding%20susceptibility%20to%20adversarial%20attacks%20in%20OpenAI%27s%20GPT-4o%2C%20biased%0Aoutputs%20in%20xAI%27s%20Grok-3%2C%20and%20privacy%20weaknesses%20in%20Google%27s%20Gemini%202.0.%0AAdditionally%2C%20we%20observe%20that%20open-source%20models%20can%20match%20or%20exceed%0Aproprietary%20models%20in%20specific%20services%20such%20as%20safety%20alignment%2C%20fairness%20and%0Abias%2C%20and%20OOD%20robustness.%20Finally%2C%20we%20identify%20trade-offs%20between%20distillation%0Astrategies%2C%20model%20size%2C%20training%20methods%2C%20and%20architectural%20choices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14985v2&entry.124074799=Read"},
{"title": "EvTTC: An Event Camera Dataset for Time-to-Collision Estimation", "author": "Kaizhen Sun and Jinghang Li and Kuan Dai and Bangyan Liao and Wei Xiong and Yi Zhou", "abstract": "  Time-to-Collision (TTC) estimation lies in the core of the forward collision\nwarning (FCW) functionality, which is key to all Automatic Emergency Braking\n(AEB) systems. Although the success of solutions using frame-based cameras\n(e.g., Mobileye's solutions) has been witnessed in normal situations, some\nextreme cases, such as the sudden variation in the relative speed of leading\nvehicles and the sudden appearance of pedestrians, still pose significant risks\nthat cannot be handled. This is due to the inherent imaging principles of\nframe-based cameras, where the time interval between adjacent exposures\nintroduces considerable system latency to AEB. Event cameras, as a novel\nbio-inspired sensor, offer ultra-high temporal resolution and can\nasynchronously report brightness changes at the microsecond level. To explore\nthe potential of event cameras in the above-mentioned challenging cases, we\npropose EvTTC, which is, to the best of our knowledge, the first multi-sensor\ndataset focusing on TTC tasks under high-relative-speed scenarios. EvTTC\nconsists of data collected using standard cameras and event cameras, covering\nvarious potential collision scenarios in daily driving and involving multiple\ncollision objects. Additionally, LiDAR and GNSS/INS measurements are provided\nfor the calculation of ground-truth TTC. Considering the high cost of testing\nTTC algorithms on full-scale mobile platforms, we also provide a small-scale\nTTC testbed for experimental validation and data augmentation. All the data and\nthe design of the testbed are open sourced, and they can serve as a benchmark\nthat will facilitate the development of vision-based TTC techniques.\n", "link": "http://arxiv.org/abs/2412.05053v3", "date": "2025-04-23", "relevancy": 2.0208, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5521}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4958}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvTTC%3A%20An%20Event%20Camera%20Dataset%20for%20Time-to-Collision%20Estimation&body=Title%3A%20EvTTC%3A%20An%20Event%20Camera%20Dataset%20for%20Time-to-Collision%20Estimation%0AAuthor%3A%20Kaizhen%20Sun%20and%20Jinghang%20Li%20and%20Kuan%20Dai%20and%20Bangyan%20Liao%20and%20Wei%20Xiong%20and%20Yi%20Zhou%0AAbstract%3A%20%20%20Time-to-Collision%20%28TTC%29%20estimation%20lies%20in%20the%20core%20of%20the%20forward%20collision%0Awarning%20%28FCW%29%20functionality%2C%20which%20is%20key%20to%20all%20Automatic%20Emergency%20Braking%0A%28AEB%29%20systems.%20Although%20the%20success%20of%20solutions%20using%20frame-based%20cameras%0A%28e.g.%2C%20Mobileye%27s%20solutions%29%20has%20been%20witnessed%20in%20normal%20situations%2C%20some%0Aextreme%20cases%2C%20such%20as%20the%20sudden%20variation%20in%20the%20relative%20speed%20of%20leading%0Avehicles%20and%20the%20sudden%20appearance%20of%20pedestrians%2C%20still%20pose%20significant%20risks%0Athat%20cannot%20be%20handled.%20This%20is%20due%20to%20the%20inherent%20imaging%20principles%20of%0Aframe-based%20cameras%2C%20where%20the%20time%20interval%20between%20adjacent%20exposures%0Aintroduces%20considerable%20system%20latency%20to%20AEB.%20Event%20cameras%2C%20as%20a%20novel%0Abio-inspired%20sensor%2C%20offer%20ultra-high%20temporal%20resolution%20and%20can%0Aasynchronously%20report%20brightness%20changes%20at%20the%20microsecond%20level.%20To%20explore%0Athe%20potential%20of%20event%20cameras%20in%20the%20above-mentioned%20challenging%20cases%2C%20we%0Apropose%20EvTTC%2C%20which%20is%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20multi-sensor%0Adataset%20focusing%20on%20TTC%20tasks%20under%20high-relative-speed%20scenarios.%20EvTTC%0Aconsists%20of%20data%20collected%20using%20standard%20cameras%20and%20event%20cameras%2C%20covering%0Avarious%20potential%20collision%20scenarios%20in%20daily%20driving%20and%20involving%20multiple%0Acollision%20objects.%20Additionally%2C%20LiDAR%20and%20GNSS/INS%20measurements%20are%20provided%0Afor%20the%20calculation%20of%20ground-truth%20TTC.%20Considering%20the%20high%20cost%20of%20testing%0ATTC%20algorithms%20on%20full-scale%20mobile%20platforms%2C%20we%20also%20provide%20a%20small-scale%0ATTC%20testbed%20for%20experimental%20validation%20and%20data%20augmentation.%20All%20the%20data%20and%0Athe%20design%20of%20the%20testbed%20are%20open%20sourced%2C%20and%20they%20can%20serve%20as%20a%20benchmark%0Athat%20will%20facilitate%20the%20development%20of%20vision-based%20TTC%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05053v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvTTC%253A%2520An%2520Event%2520Camera%2520Dataset%2520for%2520Time-to-Collision%2520Estimation%26entry.906535625%3DKaizhen%2520Sun%2520and%2520Jinghang%2520Li%2520and%2520Kuan%2520Dai%2520and%2520Bangyan%2520Liao%2520and%2520Wei%2520Xiong%2520and%2520Yi%2520Zhou%26entry.1292438233%3D%2520%2520Time-to-Collision%2520%2528TTC%2529%2520estimation%2520lies%2520in%2520the%2520core%2520of%2520the%2520forward%2520collision%250Awarning%2520%2528FCW%2529%2520functionality%252C%2520which%2520is%2520key%2520to%2520all%2520Automatic%2520Emergency%2520Braking%250A%2528AEB%2529%2520systems.%2520Although%2520the%2520success%2520of%2520solutions%2520using%2520frame-based%2520cameras%250A%2528e.g.%252C%2520Mobileye%2527s%2520solutions%2529%2520has%2520been%2520witnessed%2520in%2520normal%2520situations%252C%2520some%250Aextreme%2520cases%252C%2520such%2520as%2520the%2520sudden%2520variation%2520in%2520the%2520relative%2520speed%2520of%2520leading%250Avehicles%2520and%2520the%2520sudden%2520appearance%2520of%2520pedestrians%252C%2520still%2520pose%2520significant%2520risks%250Athat%2520cannot%2520be%2520handled.%2520This%2520is%2520due%2520to%2520the%2520inherent%2520imaging%2520principles%2520of%250Aframe-based%2520cameras%252C%2520where%2520the%2520time%2520interval%2520between%2520adjacent%2520exposures%250Aintroduces%2520considerable%2520system%2520latency%2520to%2520AEB.%2520Event%2520cameras%252C%2520as%2520a%2520novel%250Abio-inspired%2520sensor%252C%2520offer%2520ultra-high%2520temporal%2520resolution%2520and%2520can%250Aasynchronously%2520report%2520brightness%2520changes%2520at%2520the%2520microsecond%2520level.%2520To%2520explore%250Athe%2520potential%2520of%2520event%2520cameras%2520in%2520the%2520above-mentioned%2520challenging%2520cases%252C%2520we%250Apropose%2520EvTTC%252C%2520which%2520is%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%2520multi-sensor%250Adataset%2520focusing%2520on%2520TTC%2520tasks%2520under%2520high-relative-speed%2520scenarios.%2520EvTTC%250Aconsists%2520of%2520data%2520collected%2520using%2520standard%2520cameras%2520and%2520event%2520cameras%252C%2520covering%250Avarious%2520potential%2520collision%2520scenarios%2520in%2520daily%2520driving%2520and%2520involving%2520multiple%250Acollision%2520objects.%2520Additionally%252C%2520LiDAR%2520and%2520GNSS/INS%2520measurements%2520are%2520provided%250Afor%2520the%2520calculation%2520of%2520ground-truth%2520TTC.%2520Considering%2520the%2520high%2520cost%2520of%2520testing%250ATTC%2520algorithms%2520on%2520full-scale%2520mobile%2520platforms%252C%2520we%2520also%2520provide%2520a%2520small-scale%250ATTC%2520testbed%2520for%2520experimental%2520validation%2520and%2520data%2520augmentation.%2520All%2520the%2520data%2520and%250Athe%2520design%2520of%2520the%2520testbed%2520are%2520open%2520sourced%252C%2520and%2520they%2520can%2520serve%2520as%2520a%2520benchmark%250Athat%2520will%2520facilitate%2520the%2520development%2520of%2520vision-based%2520TTC%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05053v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvTTC%3A%20An%20Event%20Camera%20Dataset%20for%20Time-to-Collision%20Estimation&entry.906535625=Kaizhen%20Sun%20and%20Jinghang%20Li%20and%20Kuan%20Dai%20and%20Bangyan%20Liao%20and%20Wei%20Xiong%20and%20Yi%20Zhou&entry.1292438233=%20%20Time-to-Collision%20%28TTC%29%20estimation%20lies%20in%20the%20core%20of%20the%20forward%20collision%0Awarning%20%28FCW%29%20functionality%2C%20which%20is%20key%20to%20all%20Automatic%20Emergency%20Braking%0A%28AEB%29%20systems.%20Although%20the%20success%20of%20solutions%20using%20frame-based%20cameras%0A%28e.g.%2C%20Mobileye%27s%20solutions%29%20has%20been%20witnessed%20in%20normal%20situations%2C%20some%0Aextreme%20cases%2C%20such%20as%20the%20sudden%20variation%20in%20the%20relative%20speed%20of%20leading%0Avehicles%20and%20the%20sudden%20appearance%20of%20pedestrians%2C%20still%20pose%20significant%20risks%0Athat%20cannot%20be%20handled.%20This%20is%20due%20to%20the%20inherent%20imaging%20principles%20of%0Aframe-based%20cameras%2C%20where%20the%20time%20interval%20between%20adjacent%20exposures%0Aintroduces%20considerable%20system%20latency%20to%20AEB.%20Event%20cameras%2C%20as%20a%20novel%0Abio-inspired%20sensor%2C%20offer%20ultra-high%20temporal%20resolution%20and%20can%0Aasynchronously%20report%20brightness%20changes%20at%20the%20microsecond%20level.%20To%20explore%0Athe%20potential%20of%20event%20cameras%20in%20the%20above-mentioned%20challenging%20cases%2C%20we%0Apropose%20EvTTC%2C%20which%20is%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20multi-sensor%0Adataset%20focusing%20on%20TTC%20tasks%20under%20high-relative-speed%20scenarios.%20EvTTC%0Aconsists%20of%20data%20collected%20using%20standard%20cameras%20and%20event%20cameras%2C%20covering%0Avarious%20potential%20collision%20scenarios%20in%20daily%20driving%20and%20involving%20multiple%0Acollision%20objects.%20Additionally%2C%20LiDAR%20and%20GNSS/INS%20measurements%20are%20provided%0Afor%20the%20calculation%20of%20ground-truth%20TTC.%20Considering%20the%20high%20cost%20of%20testing%0ATTC%20algorithms%20on%20full-scale%20mobile%20platforms%2C%20we%20also%20provide%20a%20small-scale%0ATTC%20testbed%20for%20experimental%20validation%20and%20data%20augmentation.%20All%20the%20data%20and%0Athe%20design%20of%20the%20testbed%20are%20open%20sourced%2C%20and%20they%20can%20serve%20as%20a%20benchmark%0Athat%20will%20facilitate%20the%20development%20of%20vision-based%20TTC%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05053v3&entry.124074799=Read"},
{"title": "How Effective are Generative Large Language Models in Performing\n  Requirements Classification?", "author": "Waad Alhoshan and Alessio Ferrari and Liping Zhao", "abstract": "  In recent years, transformer-based large language models (LLMs) have\nrevolutionised natural language processing (NLP), with generative models\nopening new possibilities for tasks that require context-aware text generation.\nRequirements engineering (RE) has also seen a surge in the experimentation of\nLLMs for different tasks, including trace-link detection, regulatory\ncompliance, and others. Requirements classification is a common task in RE.\nWhile non-generative LLMs like BERT have been successfully applied to this\ntask, there has been limited exploration of generative LLMs. This gap raises an\nimportant question: how well can generative LLMs, which produce context-aware\noutputs, perform in requirements classification? In this study, we explore the\neffectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing\nboth binary and multi-class requirements classification. We design an extensive\nexperimental study involving over 400 experiments across three widely used\ndatasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes\nthat while factors like prompt design and LLM architecture are universally\nimportant, others-such as dataset variations-have a more situational impact,\ndepending on the complexity of the classification task. This insight can guide\nfuture model development and deployment strategies, focusing on optimising\nprompt structures and aligning model architectures with task-specific needs for\nimproved performance.\n", "link": "http://arxiv.org/abs/2504.16768v1", "date": "2025-04-23", "relevancy": 2.018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Effective%20are%20Generative%20Large%20Language%20Models%20in%20Performing%0A%20%20Requirements%20Classification%3F&body=Title%3A%20How%20Effective%20are%20Generative%20Large%20Language%20Models%20in%20Performing%0A%20%20Requirements%20Classification%3F%0AAuthor%3A%20Waad%20Alhoshan%20and%20Alessio%20Ferrari%20and%20Liping%20Zhao%0AAbstract%3A%20%20%20In%20recent%20years%2C%20transformer-based%20large%20language%20models%20%28LLMs%29%20have%0Arevolutionised%20natural%20language%20processing%20%28NLP%29%2C%20with%20generative%20models%0Aopening%20new%20possibilities%20for%20tasks%20that%20require%20context-aware%20text%20generation.%0ARequirements%20engineering%20%28RE%29%20has%20also%20seen%20a%20surge%20in%20the%20experimentation%20of%0ALLMs%20for%20different%20tasks%2C%20including%20trace-link%20detection%2C%20regulatory%0Acompliance%2C%20and%20others.%20Requirements%20classification%20is%20a%20common%20task%20in%20RE.%0AWhile%20non-generative%20LLMs%20like%20BERT%20have%20been%20successfully%20applied%20to%20this%0Atask%2C%20there%20has%20been%20limited%20exploration%20of%20generative%20LLMs.%20This%20gap%20raises%20an%0Aimportant%20question%3A%20how%20well%20can%20generative%20LLMs%2C%20which%20produce%20context-aware%0Aoutputs%2C%20perform%20in%20requirements%20classification%3F%20In%20this%20study%2C%20we%20explore%20the%0Aeffectiveness%20of%20three%20generative%20LLMs-Bloom%2C%20Gemma%2C%20and%20Llama-in%20performing%0Aboth%20binary%20and%20multi-class%20requirements%20classification.%20We%20design%20an%20extensive%0Aexperimental%20study%20involving%20over%20400%20experiments%20across%20three%20widely%20used%0Adatasets%20%28PROMISE%20NFR%2C%20Functional-Quality%2C%20and%20SecReq%29.%20Our%20study%20concludes%0Athat%20while%20factors%20like%20prompt%20design%20and%20LLM%20architecture%20are%20universally%0Aimportant%2C%20others-such%20as%20dataset%20variations-have%20a%20more%20situational%20impact%2C%0Adepending%20on%20the%20complexity%20of%20the%20classification%20task.%20This%20insight%20can%20guide%0Afuture%20model%20development%20and%20deployment%20strategies%2C%20focusing%20on%20optimising%0Aprompt%20structures%20and%20aligning%20model%20architectures%20with%20task-specific%20needs%20for%0Aimproved%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Effective%2520are%2520Generative%2520Large%2520Language%2520Models%2520in%2520Performing%250A%2520%2520Requirements%2520Classification%253F%26entry.906535625%3DWaad%2520Alhoshan%2520and%2520Alessio%2520Ferrari%2520and%2520Liping%2520Zhao%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Arevolutionised%2520natural%2520language%2520processing%2520%2528NLP%2529%252C%2520with%2520generative%2520models%250Aopening%2520new%2520possibilities%2520for%2520tasks%2520that%2520require%2520context-aware%2520text%2520generation.%250ARequirements%2520engineering%2520%2528RE%2529%2520has%2520also%2520seen%2520a%2520surge%2520in%2520the%2520experimentation%2520of%250ALLMs%2520for%2520different%2520tasks%252C%2520including%2520trace-link%2520detection%252C%2520regulatory%250Acompliance%252C%2520and%2520others.%2520Requirements%2520classification%2520is%2520a%2520common%2520task%2520in%2520RE.%250AWhile%2520non-generative%2520LLMs%2520like%2520BERT%2520have%2520been%2520successfully%2520applied%2520to%2520this%250Atask%252C%2520there%2520has%2520been%2520limited%2520exploration%2520of%2520generative%2520LLMs.%2520This%2520gap%2520raises%2520an%250Aimportant%2520question%253A%2520how%2520well%2520can%2520generative%2520LLMs%252C%2520which%2520produce%2520context-aware%250Aoutputs%252C%2520perform%2520in%2520requirements%2520classification%253F%2520In%2520this%2520study%252C%2520we%2520explore%2520the%250Aeffectiveness%2520of%2520three%2520generative%2520LLMs-Bloom%252C%2520Gemma%252C%2520and%2520Llama-in%2520performing%250Aboth%2520binary%2520and%2520multi-class%2520requirements%2520classification.%2520We%2520design%2520an%2520extensive%250Aexperimental%2520study%2520involving%2520over%2520400%2520experiments%2520across%2520three%2520widely%2520used%250Adatasets%2520%2528PROMISE%2520NFR%252C%2520Functional-Quality%252C%2520and%2520SecReq%2529.%2520Our%2520study%2520concludes%250Athat%2520while%2520factors%2520like%2520prompt%2520design%2520and%2520LLM%2520architecture%2520are%2520universally%250Aimportant%252C%2520others-such%2520as%2520dataset%2520variations-have%2520a%2520more%2520situational%2520impact%252C%250Adepending%2520on%2520the%2520complexity%2520of%2520the%2520classification%2520task.%2520This%2520insight%2520can%2520guide%250Afuture%2520model%2520development%2520and%2520deployment%2520strategies%252C%2520focusing%2520on%2520optimising%250Aprompt%2520structures%2520and%2520aligning%2520model%2520architectures%2520with%2520task-specific%2520needs%2520for%250Aimproved%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Effective%20are%20Generative%20Large%20Language%20Models%20in%20Performing%0A%20%20Requirements%20Classification%3F&entry.906535625=Waad%20Alhoshan%20and%20Alessio%20Ferrari%20and%20Liping%20Zhao&entry.1292438233=%20%20In%20recent%20years%2C%20transformer-based%20large%20language%20models%20%28LLMs%29%20have%0Arevolutionised%20natural%20language%20processing%20%28NLP%29%2C%20with%20generative%20models%0Aopening%20new%20possibilities%20for%20tasks%20that%20require%20context-aware%20text%20generation.%0ARequirements%20engineering%20%28RE%29%20has%20also%20seen%20a%20surge%20in%20the%20experimentation%20of%0ALLMs%20for%20different%20tasks%2C%20including%20trace-link%20detection%2C%20regulatory%0Acompliance%2C%20and%20others.%20Requirements%20classification%20is%20a%20common%20task%20in%20RE.%0AWhile%20non-generative%20LLMs%20like%20BERT%20have%20been%20successfully%20applied%20to%20this%0Atask%2C%20there%20has%20been%20limited%20exploration%20of%20generative%20LLMs.%20This%20gap%20raises%20an%0Aimportant%20question%3A%20how%20well%20can%20generative%20LLMs%2C%20which%20produce%20context-aware%0Aoutputs%2C%20perform%20in%20requirements%20classification%3F%20In%20this%20study%2C%20we%20explore%20the%0Aeffectiveness%20of%20three%20generative%20LLMs-Bloom%2C%20Gemma%2C%20and%20Llama-in%20performing%0Aboth%20binary%20and%20multi-class%20requirements%20classification.%20We%20design%20an%20extensive%0Aexperimental%20study%20involving%20over%20400%20experiments%20across%20three%20widely%20used%0Adatasets%20%28PROMISE%20NFR%2C%20Functional-Quality%2C%20and%20SecReq%29.%20Our%20study%20concludes%0Athat%20while%20factors%20like%20prompt%20design%20and%20LLM%20architecture%20are%20universally%0Aimportant%2C%20others-such%20as%20dataset%20variations-have%20a%20more%20situational%20impact%2C%0Adepending%20on%20the%20complexity%20of%20the%20classification%20task.%20This%20insight%20can%20guide%0Afuture%20model%20development%20and%20deployment%20strategies%2C%20focusing%20on%20optimising%0Aprompt%20structures%20and%20aligning%20model%20architectures%20with%20task-specific%20needs%20for%0Aimproved%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16768v1&entry.124074799=Read"},
{"title": "Right Question is Already Half the Answer: Fully Unsupervised LLM\n  Reasoning Incentivization", "author": "Qingyang Zhang and Haitao Wu and Changqing Zhang and Peilin Zhao and Yatao Bian", "abstract": "  While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervision--such as human-labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form natural reasoning tasks. Specifically,\nwithout any supervised signals, \\ours boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves the accuracy\nof Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro.\n", "link": "http://arxiv.org/abs/2504.05812v2", "date": "2025-04-23", "relevancy": 2.0173, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.511}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5068}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Right%20Question%20is%20Already%20Half%20the%20Answer%3A%20Fully%20Unsupervised%20LLM%0A%20%20Reasoning%20Incentivization&body=Title%3A%20Right%20Question%20is%20Already%20Half%20the%20Answer%3A%20Fully%20Unsupervised%20LLM%0A%20%20Reasoning%20Incentivization%0AAuthor%3A%20Qingyang%20Zhang%20and%20Haitao%20Wu%20and%20Changqing%20Zhang%20and%20Peilin%20Zhao%20and%20Yatao%20Bian%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20exceptional%20capabilities%0Ain%20challenging%20tasks%20such%20as%20mathematical%20reasoning%2C%20existing%20methods%20to%0Aenhance%20reasoning%20ability%20predominantly%20rely%20on%20supervised%20fine-tuning%20%28SFT%29%0Afollowed%20by%20reinforcement%20learning%20%28RL%29%20on%20reasoning-specific%20data%20after%0Apre-training.%20However%2C%20these%20approaches%20critically%20depend%20on%20external%0Asupervision--such%20as%20human-labelled%20reasoning%20traces%2C%20verified%20golden%20answers%2C%0Aor%20pre-trained%20reward%20models--which%20limits%20scalability%20and%20practical%0Aapplicability.%20In%20this%20work%2C%20we%20propose%20Entropy%20Minimized%20Policy%20Optimization%0A%28EMPO%29%2C%20which%20makes%20an%20early%20attempt%20at%20fully%20unsupervised%20LLM%20reasoning%0Aincentivization.%20EMPO%20does%20not%20require%20any%20supervised%20information%20for%0Aincentivizing%20reasoning%20capabilities%20%28i.e.%2C%20neither%20verifiable%20reasoning%0Atraces%2C%20problems%20with%20golden%20answers%2C%20nor%20additional%20pre-trained%20reward%0Amodels%29.%20By%20continuously%20minimizing%20the%20predictive%20entropy%20of%20LLMs%20on%20unlabeled%0Auser%20queries%20in%20a%20latent%20semantic%20space%2C%20EMPO%20enables%20purely%20self-supervised%0Aevolution%20of%20reasoning%20capabilities%20with%20strong%20flexibility%20and%20practicality.%0AOur%20experiments%20demonstrate%20competitive%20performance%20of%20EMPO%20on%20both%0Amathematical%20reasoning%20and%20free-form%20natural%20reasoning%20tasks.%20Specifically%2C%0Awithout%20any%20supervised%20signals%2C%20%5Cours%20boosts%20the%20accuracy%20of%20Qwen2.5-Math-7B%0ABase%20from%2030.7%5C%25%20to%2048.1%5C%25%20on%20mathematical%20benchmarks%20and%20improves%20the%20accuracy%0Aof%20Qwen2.5-7B%20Base%20from%2032.1%5C%25%20to%2050.1%5C%25%20on%20MMLU-Pro.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05812v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRight%2520Question%2520is%2520Already%2520Half%2520the%2520Answer%253A%2520Fully%2520Unsupervised%2520LLM%250A%2520%2520Reasoning%2520Incentivization%26entry.906535625%3DQingyang%2520Zhang%2520and%2520Haitao%2520Wu%2520and%2520Changqing%2520Zhang%2520and%2520Peilin%2520Zhao%2520and%2520Yatao%2520Bian%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520exceptional%2520capabilities%250Ain%2520challenging%2520tasks%2520such%2520as%2520mathematical%2520reasoning%252C%2520existing%2520methods%2520to%250Aenhance%2520reasoning%2520ability%2520predominantly%2520rely%2520on%2520supervised%2520fine-tuning%2520%2528SFT%2529%250Afollowed%2520by%2520reinforcement%2520learning%2520%2528RL%2529%2520on%2520reasoning-specific%2520data%2520after%250Apre-training.%2520However%252C%2520these%2520approaches%2520critically%2520depend%2520on%2520external%250Asupervision--such%2520as%2520human-labelled%2520reasoning%2520traces%252C%2520verified%2520golden%2520answers%252C%250Aor%2520pre-trained%2520reward%2520models--which%2520limits%2520scalability%2520and%2520practical%250Aapplicability.%2520In%2520this%2520work%252C%2520we%2520propose%2520Entropy%2520Minimized%2520Policy%2520Optimization%250A%2528EMPO%2529%252C%2520which%2520makes%2520an%2520early%2520attempt%2520at%2520fully%2520unsupervised%2520LLM%2520reasoning%250Aincentivization.%2520EMPO%2520does%2520not%2520require%2520any%2520supervised%2520information%2520for%250Aincentivizing%2520reasoning%2520capabilities%2520%2528i.e.%252C%2520neither%2520verifiable%2520reasoning%250Atraces%252C%2520problems%2520with%2520golden%2520answers%252C%2520nor%2520additional%2520pre-trained%2520reward%250Amodels%2529.%2520By%2520continuously%2520minimizing%2520the%2520predictive%2520entropy%2520of%2520LLMs%2520on%2520unlabeled%250Auser%2520queries%2520in%2520a%2520latent%2520semantic%2520space%252C%2520EMPO%2520enables%2520purely%2520self-supervised%250Aevolution%2520of%2520reasoning%2520capabilities%2520with%2520strong%2520flexibility%2520and%2520practicality.%250AOur%2520experiments%2520demonstrate%2520competitive%2520performance%2520of%2520EMPO%2520on%2520both%250Amathematical%2520reasoning%2520and%2520free-form%2520natural%2520reasoning%2520tasks.%2520Specifically%252C%250Awithout%2520any%2520supervised%2520signals%252C%2520%255Cours%2520boosts%2520the%2520accuracy%2520of%2520Qwen2.5-Math-7B%250ABase%2520from%252030.7%255C%2525%2520to%252048.1%255C%2525%2520on%2520mathematical%2520benchmarks%2520and%2520improves%2520the%2520accuracy%250Aof%2520Qwen2.5-7B%2520Base%2520from%252032.1%255C%2525%2520to%252050.1%255C%2525%2520on%2520MMLU-Pro.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05812v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Right%20Question%20is%20Already%20Half%20the%20Answer%3A%20Fully%20Unsupervised%20LLM%0A%20%20Reasoning%20Incentivization&entry.906535625=Qingyang%20Zhang%20and%20Haitao%20Wu%20and%20Changqing%20Zhang%20and%20Peilin%20Zhao%20and%20Yatao%20Bian&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20exceptional%20capabilities%0Ain%20challenging%20tasks%20such%20as%20mathematical%20reasoning%2C%20existing%20methods%20to%0Aenhance%20reasoning%20ability%20predominantly%20rely%20on%20supervised%20fine-tuning%20%28SFT%29%0Afollowed%20by%20reinforcement%20learning%20%28RL%29%20on%20reasoning-specific%20data%20after%0Apre-training.%20However%2C%20these%20approaches%20critically%20depend%20on%20external%0Asupervision--such%20as%20human-labelled%20reasoning%20traces%2C%20verified%20golden%20answers%2C%0Aor%20pre-trained%20reward%20models--which%20limits%20scalability%20and%20practical%0Aapplicability.%20In%20this%20work%2C%20we%20propose%20Entropy%20Minimized%20Policy%20Optimization%0A%28EMPO%29%2C%20which%20makes%20an%20early%20attempt%20at%20fully%20unsupervised%20LLM%20reasoning%0Aincentivization.%20EMPO%20does%20not%20require%20any%20supervised%20information%20for%0Aincentivizing%20reasoning%20capabilities%20%28i.e.%2C%20neither%20verifiable%20reasoning%0Atraces%2C%20problems%20with%20golden%20answers%2C%20nor%20additional%20pre-trained%20reward%0Amodels%29.%20By%20continuously%20minimizing%20the%20predictive%20entropy%20of%20LLMs%20on%20unlabeled%0Auser%20queries%20in%20a%20latent%20semantic%20space%2C%20EMPO%20enables%20purely%20self-supervised%0Aevolution%20of%20reasoning%20capabilities%20with%20strong%20flexibility%20and%20practicality.%0AOur%20experiments%20demonstrate%20competitive%20performance%20of%20EMPO%20on%20both%0Amathematical%20reasoning%20and%20free-form%20natural%20reasoning%20tasks.%20Specifically%2C%0Awithout%20any%20supervised%20signals%2C%20%5Cours%20boosts%20the%20accuracy%20of%20Qwen2.5-Math-7B%0ABase%20from%2030.7%5C%25%20to%2048.1%5C%25%20on%20mathematical%20benchmarks%20and%20improves%20the%20accuracy%0Aof%20Qwen2.5-7B%20Base%20from%2032.1%5C%25%20to%2050.1%5C%25%20on%20MMLU-Pro.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05812v2&entry.124074799=Read"},
{"title": "Evolutionary Optimization of Physics-Informed Neural Networks: Advancing\n  Generalizability by the Baldwin Effect", "author": "Jian Cheng Wong and Chin Chun Ooi and Abhishek Gupta and Pao-Hsiung Chiu and Joshua Shao Zheng Low and My Ha Dao and Yew-Soon Ong", "abstract": "  Physics-informed neural networks (PINNs) are at the forefront of scientific\nmachine learning, making possible the creation of machine intelligence that is\ncognizant of physical laws and able to accurately simulate them. However,\ntoday's PINNs are often trained for a single physics task and require\ncomputationally expensive re-training for each new task, even for tasks from\nsimilar physics domains. To address this limitation, this paper proposes a\npioneering approach to advance the generalizability of PINNs through the\nframework of Baldwinian evolution. Drawing inspiration from the\nneurodevelopment of precocial species that have evolved to learn, predict and\nreact quickly to their environment, we envision PINNs that are pre-wired with\nconnection strengths inducing strong biases towards efficient learning of\nphysics. A novel two-stage stochastic programming formulation coupling\nevolutionary selection pressure (based on proficiency over a distribution of\nphysics tasks) with lifetime learning (to specialize on a sampled subset of\nthose tasks) is proposed to instantiate the Baldwin effect. The evolved\nBaldwinian-PINNs demonstrate fast and physics-compliant prediction capabilities\nacross a range of empirically challenging problem instances with more than an\norder of magnitude improvement in prediction accuracy at a fraction of the\ncomputation cost compared to state-of-the-art gradient-based meta-learning\nmethods. For example, when solving the diffusion-reaction equation, a 70x\nimprovement in accuracy was obtained while taking 700x less computational time.\nThis paper thus marks a leap forward in the meta-learning of PINNs as\ngeneralizable physics solvers. Sample codes are available at\nhttps://github.com/chiuph/Baldwinian-PINN.\n", "link": "http://arxiv.org/abs/2312.03243v3", "date": "2025-04-23", "relevancy": 2.007, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5219}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5052}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolutionary%20Optimization%20of%20Physics-Informed%20Neural%20Networks%3A%20Advancing%0A%20%20Generalizability%20by%20the%20Baldwin%20Effect&body=Title%3A%20Evolutionary%20Optimization%20of%20Physics-Informed%20Neural%20Networks%3A%20Advancing%0A%20%20Generalizability%20by%20the%20Baldwin%20Effect%0AAuthor%3A%20Jian%20Cheng%20Wong%20and%20Chin%20Chun%20Ooi%20and%20Abhishek%20Gupta%20and%20Pao-Hsiung%20Chiu%20and%20Joshua%20Shao%20Zheng%20Low%20and%20My%20Ha%20Dao%20and%20Yew-Soon%20Ong%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20are%20at%20the%20forefront%20of%20scientific%0Amachine%20learning%2C%20making%20possible%20the%20creation%20of%20machine%20intelligence%20that%20is%0Acognizant%20of%20physical%20laws%20and%20able%20to%20accurately%20simulate%20them.%20However%2C%0Atoday%27s%20PINNs%20are%20often%20trained%20for%20a%20single%20physics%20task%20and%20require%0Acomputationally%20expensive%20re-training%20for%20each%20new%20task%2C%20even%20for%20tasks%20from%0Asimilar%20physics%20domains.%20To%20address%20this%20limitation%2C%20this%20paper%20proposes%20a%0Apioneering%20approach%20to%20advance%20the%20generalizability%20of%20PINNs%20through%20the%0Aframework%20of%20Baldwinian%20evolution.%20Drawing%20inspiration%20from%20the%0Aneurodevelopment%20of%20precocial%20species%20that%20have%20evolved%20to%20learn%2C%20predict%20and%0Areact%20quickly%20to%20their%20environment%2C%20we%20envision%20PINNs%20that%20are%20pre-wired%20with%0Aconnection%20strengths%20inducing%20strong%20biases%20towards%20efficient%20learning%20of%0Aphysics.%20A%20novel%20two-stage%20stochastic%20programming%20formulation%20coupling%0Aevolutionary%20selection%20pressure%20%28based%20on%20proficiency%20over%20a%20distribution%20of%0Aphysics%20tasks%29%20with%20lifetime%20learning%20%28to%20specialize%20on%20a%20sampled%20subset%20of%0Athose%20tasks%29%20is%20proposed%20to%20instantiate%20the%20Baldwin%20effect.%20The%20evolved%0ABaldwinian-PINNs%20demonstrate%20fast%20and%20physics-compliant%20prediction%20capabilities%0Aacross%20a%20range%20of%20empirically%20challenging%20problem%20instances%20with%20more%20than%20an%0Aorder%20of%20magnitude%20improvement%20in%20prediction%20accuracy%20at%20a%20fraction%20of%20the%0Acomputation%20cost%20compared%20to%20state-of-the-art%20gradient-based%20meta-learning%0Amethods.%20For%20example%2C%20when%20solving%20the%20diffusion-reaction%20equation%2C%20a%2070x%0Aimprovement%20in%20accuracy%20was%20obtained%20while%20taking%20700x%20less%20computational%20time.%0AThis%20paper%20thus%20marks%20a%20leap%20forward%20in%20the%20meta-learning%20of%20PINNs%20as%0Ageneralizable%20physics%20solvers.%20Sample%20codes%20are%20available%20at%0Ahttps%3A//github.com/chiuph/Baldwinian-PINN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03243v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolutionary%2520Optimization%2520of%2520Physics-Informed%2520Neural%2520Networks%253A%2520Advancing%250A%2520%2520Generalizability%2520by%2520the%2520Baldwin%2520Effect%26entry.906535625%3DJian%2520Cheng%2520Wong%2520and%2520Chin%2520Chun%2520Ooi%2520and%2520Abhishek%2520Gupta%2520and%2520Pao-Hsiung%2520Chiu%2520and%2520Joshua%2520Shao%2520Zheng%2520Low%2520and%2520My%2520Ha%2520Dao%2520and%2520Yew-Soon%2520Ong%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520are%2520at%2520the%2520forefront%2520of%2520scientific%250Amachine%2520learning%252C%2520making%2520possible%2520the%2520creation%2520of%2520machine%2520intelligence%2520that%2520is%250Acognizant%2520of%2520physical%2520laws%2520and%2520able%2520to%2520accurately%2520simulate%2520them.%2520However%252C%250Atoday%2527s%2520PINNs%2520are%2520often%2520trained%2520for%2520a%2520single%2520physics%2520task%2520and%2520require%250Acomputationally%2520expensive%2520re-training%2520for%2520each%2520new%2520task%252C%2520even%2520for%2520tasks%2520from%250Asimilar%2520physics%2520domains.%2520To%2520address%2520this%2520limitation%252C%2520this%2520paper%2520proposes%2520a%250Apioneering%2520approach%2520to%2520advance%2520the%2520generalizability%2520of%2520PINNs%2520through%2520the%250Aframework%2520of%2520Baldwinian%2520evolution.%2520Drawing%2520inspiration%2520from%2520the%250Aneurodevelopment%2520of%2520precocial%2520species%2520that%2520have%2520evolved%2520to%2520learn%252C%2520predict%2520and%250Areact%2520quickly%2520to%2520their%2520environment%252C%2520we%2520envision%2520PINNs%2520that%2520are%2520pre-wired%2520with%250Aconnection%2520strengths%2520inducing%2520strong%2520biases%2520towards%2520efficient%2520learning%2520of%250Aphysics.%2520A%2520novel%2520two-stage%2520stochastic%2520programming%2520formulation%2520coupling%250Aevolutionary%2520selection%2520pressure%2520%2528based%2520on%2520proficiency%2520over%2520a%2520distribution%2520of%250Aphysics%2520tasks%2529%2520with%2520lifetime%2520learning%2520%2528to%2520specialize%2520on%2520a%2520sampled%2520subset%2520of%250Athose%2520tasks%2529%2520is%2520proposed%2520to%2520instantiate%2520the%2520Baldwin%2520effect.%2520The%2520evolved%250ABaldwinian-PINNs%2520demonstrate%2520fast%2520and%2520physics-compliant%2520prediction%2520capabilities%250Aacross%2520a%2520range%2520of%2520empirically%2520challenging%2520problem%2520instances%2520with%2520more%2520than%2520an%250Aorder%2520of%2520magnitude%2520improvement%2520in%2520prediction%2520accuracy%2520at%2520a%2520fraction%2520of%2520the%250Acomputation%2520cost%2520compared%2520to%2520state-of-the-art%2520gradient-based%2520meta-learning%250Amethods.%2520For%2520example%252C%2520when%2520solving%2520the%2520diffusion-reaction%2520equation%252C%2520a%252070x%250Aimprovement%2520in%2520accuracy%2520was%2520obtained%2520while%2520taking%2520700x%2520less%2520computational%2520time.%250AThis%2520paper%2520thus%2520marks%2520a%2520leap%2520forward%2520in%2520the%2520meta-learning%2520of%2520PINNs%2520as%250Ageneralizable%2520physics%2520solvers.%2520Sample%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/chiuph/Baldwinian-PINN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03243v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolutionary%20Optimization%20of%20Physics-Informed%20Neural%20Networks%3A%20Advancing%0A%20%20Generalizability%20by%20the%20Baldwin%20Effect&entry.906535625=Jian%20Cheng%20Wong%20and%20Chin%20Chun%20Ooi%20and%20Abhishek%20Gupta%20and%20Pao-Hsiung%20Chiu%20and%20Joshua%20Shao%20Zheng%20Low%20and%20My%20Ha%20Dao%20and%20Yew-Soon%20Ong&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20are%20at%20the%20forefront%20of%20scientific%0Amachine%20learning%2C%20making%20possible%20the%20creation%20of%20machine%20intelligence%20that%20is%0Acognizant%20of%20physical%20laws%20and%20able%20to%20accurately%20simulate%20them.%20However%2C%0Atoday%27s%20PINNs%20are%20often%20trained%20for%20a%20single%20physics%20task%20and%20require%0Acomputationally%20expensive%20re-training%20for%20each%20new%20task%2C%20even%20for%20tasks%20from%0Asimilar%20physics%20domains.%20To%20address%20this%20limitation%2C%20this%20paper%20proposes%20a%0Apioneering%20approach%20to%20advance%20the%20generalizability%20of%20PINNs%20through%20the%0Aframework%20of%20Baldwinian%20evolution.%20Drawing%20inspiration%20from%20the%0Aneurodevelopment%20of%20precocial%20species%20that%20have%20evolved%20to%20learn%2C%20predict%20and%0Areact%20quickly%20to%20their%20environment%2C%20we%20envision%20PINNs%20that%20are%20pre-wired%20with%0Aconnection%20strengths%20inducing%20strong%20biases%20towards%20efficient%20learning%20of%0Aphysics.%20A%20novel%20two-stage%20stochastic%20programming%20formulation%20coupling%0Aevolutionary%20selection%20pressure%20%28based%20on%20proficiency%20over%20a%20distribution%20of%0Aphysics%20tasks%29%20with%20lifetime%20learning%20%28to%20specialize%20on%20a%20sampled%20subset%20of%0Athose%20tasks%29%20is%20proposed%20to%20instantiate%20the%20Baldwin%20effect.%20The%20evolved%0ABaldwinian-PINNs%20demonstrate%20fast%20and%20physics-compliant%20prediction%20capabilities%0Aacross%20a%20range%20of%20empirically%20challenging%20problem%20instances%20with%20more%20than%20an%0Aorder%20of%20magnitude%20improvement%20in%20prediction%20accuracy%20at%20a%20fraction%20of%20the%0Acomputation%20cost%20compared%20to%20state-of-the-art%20gradient-based%20meta-learning%0Amethods.%20For%20example%2C%20when%20solving%20the%20diffusion-reaction%20equation%2C%20a%2070x%0Aimprovement%20in%20accuracy%20was%20obtained%20while%20taking%20700x%20less%20computational%20time.%0AThis%20paper%20thus%20marks%20a%20leap%20forward%20in%20the%20meta-learning%20of%20PINNs%20as%0Ageneralizable%20physics%20solvers.%20Sample%20codes%20are%20available%20at%0Ahttps%3A//github.com/chiuph/Baldwinian-PINN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03243v3&entry.124074799=Read"},
{"title": "Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM\n  Behind AI-Generated Text", "author": "Shifali Agrahari and Sanasam Ranbir Singh", "abstract": "  In recent years, the detection of AI-generated text has become a critical\narea of research due to concerns about academic integrity, misinformation, and\nethical AI deployment. This paper presents COT Fine-tuned, a novel framework\nfor detecting AI-generated text and identifying the specific language model.\nresponsible for generating the text. We propose a dual-task approach, where\nTask A involves classifying text as AI-generated or human-written, and Task B\nidentifies the specific LLM behind the text. The key innovation of our method\nlies in the use of Chain-of-Thought reasoning, which enables the model to\ngenerate explanations for its predictions, enhancing transparency and\ninterpretability. Our experiments demonstrate that COT Fine-tuned achieves high\naccuracy in both tasks, with strong performance in LLM identification and\nhuman-AI classification. We also show that the CoT reasoning process\ncontributes significantly to the models effectiveness and interpretability.\n", "link": "http://arxiv.org/abs/2504.16913v1", "date": "2025-04-23", "relevancy": 1.995, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5139}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.488}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracing%20Thought%3A%20Using%20Chain-of-Thought%20Reasoning%20to%20Identify%20the%20LLM%0A%20%20Behind%20AI-Generated%20Text&body=Title%3A%20Tracing%20Thought%3A%20Using%20Chain-of-Thought%20Reasoning%20to%20Identify%20the%20LLM%0A%20%20Behind%20AI-Generated%20Text%0AAuthor%3A%20Shifali%20Agrahari%20and%20Sanasam%20Ranbir%20Singh%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20detection%20of%20AI-generated%20text%20has%20become%20a%20critical%0Aarea%20of%20research%20due%20to%20concerns%20about%20academic%20integrity%2C%20misinformation%2C%20and%0Aethical%20AI%20deployment.%20This%20paper%20presents%20COT%20Fine-tuned%2C%20a%20novel%20framework%0Afor%20detecting%20AI-generated%20text%20and%20identifying%20the%20specific%20language%20model.%0Aresponsible%20for%20generating%20the%20text.%20We%20propose%20a%20dual-task%20approach%2C%20where%0ATask%20A%20involves%20classifying%20text%20as%20AI-generated%20or%20human-written%2C%20and%20Task%20B%0Aidentifies%20the%20specific%20LLM%20behind%20the%20text.%20The%20key%20innovation%20of%20our%20method%0Alies%20in%20the%20use%20of%20Chain-of-Thought%20reasoning%2C%20which%20enables%20the%20model%20to%0Agenerate%20explanations%20for%20its%20predictions%2C%20enhancing%20transparency%20and%0Ainterpretability.%20Our%20experiments%20demonstrate%20that%20COT%20Fine-tuned%20achieves%20high%0Aaccuracy%20in%20both%20tasks%2C%20with%20strong%20performance%20in%20LLM%20identification%20and%0Ahuman-AI%20classification.%20We%20also%20show%20that%20the%20CoT%20reasoning%20process%0Acontributes%20significantly%20to%20the%20models%20effectiveness%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracing%2520Thought%253A%2520Using%2520Chain-of-Thought%2520Reasoning%2520to%2520Identify%2520the%2520LLM%250A%2520%2520Behind%2520AI-Generated%2520Text%26entry.906535625%3DShifali%2520Agrahari%2520and%2520Sanasam%2520Ranbir%2520Singh%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520detection%2520of%2520AI-generated%2520text%2520has%2520become%2520a%2520critical%250Aarea%2520of%2520research%2520due%2520to%2520concerns%2520about%2520academic%2520integrity%252C%2520misinformation%252C%2520and%250Aethical%2520AI%2520deployment.%2520This%2520paper%2520presents%2520COT%2520Fine-tuned%252C%2520a%2520novel%2520framework%250Afor%2520detecting%2520AI-generated%2520text%2520and%2520identifying%2520the%2520specific%2520language%2520model.%250Aresponsible%2520for%2520generating%2520the%2520text.%2520We%2520propose%2520a%2520dual-task%2520approach%252C%2520where%250ATask%2520A%2520involves%2520classifying%2520text%2520as%2520AI-generated%2520or%2520human-written%252C%2520and%2520Task%2520B%250Aidentifies%2520the%2520specific%2520LLM%2520behind%2520the%2520text.%2520The%2520key%2520innovation%2520of%2520our%2520method%250Alies%2520in%2520the%2520use%2520of%2520Chain-of-Thought%2520reasoning%252C%2520which%2520enables%2520the%2520model%2520to%250Agenerate%2520explanations%2520for%2520its%2520predictions%252C%2520enhancing%2520transparency%2520and%250Ainterpretability.%2520Our%2520experiments%2520demonstrate%2520that%2520COT%2520Fine-tuned%2520achieves%2520high%250Aaccuracy%2520in%2520both%2520tasks%252C%2520with%2520strong%2520performance%2520in%2520LLM%2520identification%2520and%250Ahuman-AI%2520classification.%2520We%2520also%2520show%2520that%2520the%2520CoT%2520reasoning%2520process%250Acontributes%2520significantly%2520to%2520the%2520models%2520effectiveness%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracing%20Thought%3A%20Using%20Chain-of-Thought%20Reasoning%20to%20Identify%20the%20LLM%0A%20%20Behind%20AI-Generated%20Text&entry.906535625=Shifali%20Agrahari%20and%20Sanasam%20Ranbir%20Singh&entry.1292438233=%20%20In%20recent%20years%2C%20the%20detection%20of%20AI-generated%20text%20has%20become%20a%20critical%0Aarea%20of%20research%20due%20to%20concerns%20about%20academic%20integrity%2C%20misinformation%2C%20and%0Aethical%20AI%20deployment.%20This%20paper%20presents%20COT%20Fine-tuned%2C%20a%20novel%20framework%0Afor%20detecting%20AI-generated%20text%20and%20identifying%20the%20specific%20language%20model.%0Aresponsible%20for%20generating%20the%20text.%20We%20propose%20a%20dual-task%20approach%2C%20where%0ATask%20A%20involves%20classifying%20text%20as%20AI-generated%20or%20human-written%2C%20and%20Task%20B%0Aidentifies%20the%20specific%20LLM%20behind%20the%20text.%20The%20key%20innovation%20of%20our%20method%0Alies%20in%20the%20use%20of%20Chain-of-Thought%20reasoning%2C%20which%20enables%20the%20model%20to%0Agenerate%20explanations%20for%20its%20predictions%2C%20enhancing%20transparency%20and%0Ainterpretability.%20Our%20experiments%20demonstrate%20that%20COT%20Fine-tuned%20achieves%20high%0Aaccuracy%20in%20both%20tasks%2C%20with%20strong%20performance%20in%20LLM%20identification%20and%0Ahuman-AI%20classification.%20We%20also%20show%20that%20the%20CoT%20reasoning%20process%0Acontributes%20significantly%20to%20the%20models%20effectiveness%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16913v1&entry.124074799=Read"},
{"title": "A Measure Based Generalizable Approach to Understandability", "author": "Vikas Kushwaha and Sruti Srinivasa Ragavan and Subhajit Roy", "abstract": "  Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future.\n", "link": "http://arxiv.org/abs/2503.21615v2", "date": "2025-04-23", "relevancy": 1.9664, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4924}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Measure%20Based%20Generalizable%20Approach%20to%20Understandability&body=Title%3A%20A%20Measure%20Based%20Generalizable%20Approach%20to%20Understandability%0AAuthor%3A%20Vikas%20Kushwaha%20and%20Sruti%20Srinivasa%20Ragavan%20and%20Subhajit%20Roy%0AAbstract%3A%20%20%20Successful%20agent-human%20partnerships%20require%20that%20any%20agent%20generated%0Ainformation%20is%20understandable%20to%20the%20human%2C%20and%20that%20the%20human%20can%20easily%20steer%0Athe%20agent%20towards%20a%20goal.%20Such%20effective%20communication%20requires%20the%20agent%20to%0Adevelop%20a%20finer-level%20notion%20of%20what%20is%20understandable%20to%20the%20human.%0AState-of-the-art%20agents%2C%20including%20LLMs%2C%20lack%20this%20detailed%20notion%20of%0Aunderstandability%20because%20they%20only%20capture%20average%20human%20sensibilities%20from%0Athe%20training%20data%2C%20and%20therefore%20afford%20limited%20steerability%20%28e.g.%2C%20requiring%0Anon-trivial%20prompt%20engineering%29.%0A%20%20In%20this%20paper%2C%20instead%20of%20only%20relying%20on%20data%2C%20we%20argue%20for%20developing%0Ageneralizable%2C%20domain-agnostic%20measures%20of%20understandability%20that%20can%20be%20used%0Aas%20directives%20for%20these%20agents.%20Existing%20research%20on%20understandability%20measures%0Ais%20fragmented%2C%20we%20survey%20various%20such%20efforts%20across%20domains%2C%20and%20lay%20a%0Acognitive-science-rooted%20groundwork%20for%20more%20coherent%20and%20domain-agnostic%0Aresearch%20investigations%20in%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21615v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Measure%2520Based%2520Generalizable%2520Approach%2520to%2520Understandability%26entry.906535625%3DVikas%2520Kushwaha%2520and%2520Sruti%2520Srinivasa%2520Ragavan%2520and%2520Subhajit%2520Roy%26entry.1292438233%3D%2520%2520Successful%2520agent-human%2520partnerships%2520require%2520that%2520any%2520agent%2520generated%250Ainformation%2520is%2520understandable%2520to%2520the%2520human%252C%2520and%2520that%2520the%2520human%2520can%2520easily%2520steer%250Athe%2520agent%2520towards%2520a%2520goal.%2520Such%2520effective%2520communication%2520requires%2520the%2520agent%2520to%250Adevelop%2520a%2520finer-level%2520notion%2520of%2520what%2520is%2520understandable%2520to%2520the%2520human.%250AState-of-the-art%2520agents%252C%2520including%2520LLMs%252C%2520lack%2520this%2520detailed%2520notion%2520of%250Aunderstandability%2520because%2520they%2520only%2520capture%2520average%2520human%2520sensibilities%2520from%250Athe%2520training%2520data%252C%2520and%2520therefore%2520afford%2520limited%2520steerability%2520%2528e.g.%252C%2520requiring%250Anon-trivial%2520prompt%2520engineering%2529.%250A%2520%2520In%2520this%2520paper%252C%2520instead%2520of%2520only%2520relying%2520on%2520data%252C%2520we%2520argue%2520for%2520developing%250Ageneralizable%252C%2520domain-agnostic%2520measures%2520of%2520understandability%2520that%2520can%2520be%2520used%250Aas%2520directives%2520for%2520these%2520agents.%2520Existing%2520research%2520on%2520understandability%2520measures%250Ais%2520fragmented%252C%2520we%2520survey%2520various%2520such%2520efforts%2520across%2520domains%252C%2520and%2520lay%2520a%250Acognitive-science-rooted%2520groundwork%2520for%2520more%2520coherent%2520and%2520domain-agnostic%250Aresearch%2520investigations%2520in%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21615v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Measure%20Based%20Generalizable%20Approach%20to%20Understandability&entry.906535625=Vikas%20Kushwaha%20and%20Sruti%20Srinivasa%20Ragavan%20and%20Subhajit%20Roy&entry.1292438233=%20%20Successful%20agent-human%20partnerships%20require%20that%20any%20agent%20generated%0Ainformation%20is%20understandable%20to%20the%20human%2C%20and%20that%20the%20human%20can%20easily%20steer%0Athe%20agent%20towards%20a%20goal.%20Such%20effective%20communication%20requires%20the%20agent%20to%0Adevelop%20a%20finer-level%20notion%20of%20what%20is%20understandable%20to%20the%20human.%0AState-of-the-art%20agents%2C%20including%20LLMs%2C%20lack%20this%20detailed%20notion%20of%0Aunderstandability%20because%20they%20only%20capture%20average%20human%20sensibilities%20from%0Athe%20training%20data%2C%20and%20therefore%20afford%20limited%20steerability%20%28e.g.%2C%20requiring%0Anon-trivial%20prompt%20engineering%29.%0A%20%20In%20this%20paper%2C%20instead%20of%20only%20relying%20on%20data%2C%20we%20argue%20for%20developing%0Ageneralizable%2C%20domain-agnostic%20measures%20of%20understandability%20that%20can%20be%20used%0Aas%20directives%20for%20these%20agents.%20Existing%20research%20on%20understandability%20measures%0Ais%20fragmented%2C%20we%20survey%20various%20such%20efforts%20across%20domains%2C%20and%20lay%20a%0Acognitive-science-rooted%20groundwork%20for%20more%20coherent%20and%20domain-agnostic%0Aresearch%20investigations%20in%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21615v2&entry.124074799=Read"},
{"title": "Random Long-Context Access for Mamba via Hardware-aligned Hierarchical\n  Sparse Attention", "author": "Xiang Hu and Jiaqi Leng and Jun Zhao and Kewei Tu and Wei Wu", "abstract": "  A key advantage of Recurrent Neural Networks (RNNs) over Transformers is\ntheir linear computational and space complexity enables faster training and\ninference for long sequences. However, RNNs are fundamentally unable to\nrandomly access historical context, and simply integrating attention mechanisms\nmay undermine their efficiency advantages. To overcome this limitation, we\npropose \\textbf{H}ierarchical \\textbf{S}parse \\textbf{A}ttention (HSA), a novel\nattention mechanism that enhances RNNs with long-range random access\nflexibility while preserving their merits in efficiency and length\ngeneralization. HSA divides inputs into chunks, selecting the top-$k$ chunks\nand hierarchically aggregates information. The core innovation lies in learning\ntoken-to-chunk relevance based on fine-grained token-level information inside\neach chunk. This approach enhances the precision of chunk selection across both\nin-domain and out-of-domain context lengths. To make HSA efficient, we further\nintroduce a hardware-aligned kernel design. By combining HSA with Mamba, we\nintroduce RAMba, which achieves perfect accuracy in passkey retrieval across 64\nmillion contexts despite pre-training on only 4K-length contexts, and\nsignificant improvements on various downstream tasks, with nearly constant\nmemory footprint. These results show RAMba's huge potential in long-context\nmodeling.\n", "link": "http://arxiv.org/abs/2504.16795v1", "date": "2025-04-23", "relevancy": 1.9646, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5019}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4915}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Long-Context%20Access%20for%20Mamba%20via%20Hardware-aligned%20Hierarchical%0A%20%20Sparse%20Attention&body=Title%3A%20Random%20Long-Context%20Access%20for%20Mamba%20via%20Hardware-aligned%20Hierarchical%0A%20%20Sparse%20Attention%0AAuthor%3A%20Xiang%20Hu%20and%20Jiaqi%20Leng%20and%20Jun%20Zhao%20and%20Kewei%20Tu%20and%20Wei%20Wu%0AAbstract%3A%20%20%20A%20key%20advantage%20of%20Recurrent%20Neural%20Networks%20%28RNNs%29%20over%20Transformers%20is%0Atheir%20linear%20computational%20and%20space%20complexity%20enables%20faster%20training%20and%0Ainference%20for%20long%20sequences.%20However%2C%20RNNs%20are%20fundamentally%20unable%20to%0Arandomly%20access%20historical%20context%2C%20and%20simply%20integrating%20attention%20mechanisms%0Amay%20undermine%20their%20efficiency%20advantages.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20%5Ctextbf%7BH%7Dierarchical%20%5Ctextbf%7BS%7Dparse%20%5Ctextbf%7BA%7Dttention%20%28HSA%29%2C%20a%20novel%0Aattention%20mechanism%20that%20enhances%20RNNs%20with%20long-range%20random%20access%0Aflexibility%20while%20preserving%20their%20merits%20in%20efficiency%20and%20length%0Ageneralization.%20HSA%20divides%20inputs%20into%20chunks%2C%20selecting%20the%20top-%24k%24%20chunks%0Aand%20hierarchically%20aggregates%20information.%20The%20core%20innovation%20lies%20in%20learning%0Atoken-to-chunk%20relevance%20based%20on%20fine-grained%20token-level%20information%20inside%0Aeach%20chunk.%20This%20approach%20enhances%20the%20precision%20of%20chunk%20selection%20across%20both%0Ain-domain%20and%20out-of-domain%20context%20lengths.%20To%20make%20HSA%20efficient%2C%20we%20further%0Aintroduce%20a%20hardware-aligned%20kernel%20design.%20By%20combining%20HSA%20with%20Mamba%2C%20we%0Aintroduce%20RAMba%2C%20which%20achieves%20perfect%20accuracy%20in%20passkey%20retrieval%20across%2064%0Amillion%20contexts%20despite%20pre-training%20on%20only%204K-length%20contexts%2C%20and%0Asignificant%20improvements%20on%20various%20downstream%20tasks%2C%20with%20nearly%20constant%0Amemory%20footprint.%20These%20results%20show%20RAMba%27s%20huge%20potential%20in%20long-context%0Amodeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Long-Context%2520Access%2520for%2520Mamba%2520via%2520Hardware-aligned%2520Hierarchical%250A%2520%2520Sparse%2520Attention%26entry.906535625%3DXiang%2520Hu%2520and%2520Jiaqi%2520Leng%2520and%2520Jun%2520Zhao%2520and%2520Kewei%2520Tu%2520and%2520Wei%2520Wu%26entry.1292438233%3D%2520%2520A%2520key%2520advantage%2520of%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520over%2520Transformers%2520is%250Atheir%2520linear%2520computational%2520and%2520space%2520complexity%2520enables%2520faster%2520training%2520and%250Ainference%2520for%2520long%2520sequences.%2520However%252C%2520RNNs%2520are%2520fundamentally%2520unable%2520to%250Arandomly%2520access%2520historical%2520context%252C%2520and%2520simply%2520integrating%2520attention%2520mechanisms%250Amay%2520undermine%2520their%2520efficiency%2520advantages.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520%255Ctextbf%257BH%257Dierarchical%2520%255Ctextbf%257BS%257Dparse%2520%255Ctextbf%257BA%257Dttention%2520%2528HSA%2529%252C%2520a%2520novel%250Aattention%2520mechanism%2520that%2520enhances%2520RNNs%2520with%2520long-range%2520random%2520access%250Aflexibility%2520while%2520preserving%2520their%2520merits%2520in%2520efficiency%2520and%2520length%250Ageneralization.%2520HSA%2520divides%2520inputs%2520into%2520chunks%252C%2520selecting%2520the%2520top-%2524k%2524%2520chunks%250Aand%2520hierarchically%2520aggregates%2520information.%2520The%2520core%2520innovation%2520lies%2520in%2520learning%250Atoken-to-chunk%2520relevance%2520based%2520on%2520fine-grained%2520token-level%2520information%2520inside%250Aeach%2520chunk.%2520This%2520approach%2520enhances%2520the%2520precision%2520of%2520chunk%2520selection%2520across%2520both%250Ain-domain%2520and%2520out-of-domain%2520context%2520lengths.%2520To%2520make%2520HSA%2520efficient%252C%2520we%2520further%250Aintroduce%2520a%2520hardware-aligned%2520kernel%2520design.%2520By%2520combining%2520HSA%2520with%2520Mamba%252C%2520we%250Aintroduce%2520RAMba%252C%2520which%2520achieves%2520perfect%2520accuracy%2520in%2520passkey%2520retrieval%2520across%252064%250Amillion%2520contexts%2520despite%2520pre-training%2520on%2520only%25204K-length%2520contexts%252C%2520and%250Asignificant%2520improvements%2520on%2520various%2520downstream%2520tasks%252C%2520with%2520nearly%2520constant%250Amemory%2520footprint.%2520These%2520results%2520show%2520RAMba%2527s%2520huge%2520potential%2520in%2520long-context%250Amodeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Long-Context%20Access%20for%20Mamba%20via%20Hardware-aligned%20Hierarchical%0A%20%20Sparse%20Attention&entry.906535625=Xiang%20Hu%20and%20Jiaqi%20Leng%20and%20Jun%20Zhao%20and%20Kewei%20Tu%20and%20Wei%20Wu&entry.1292438233=%20%20A%20key%20advantage%20of%20Recurrent%20Neural%20Networks%20%28RNNs%29%20over%20Transformers%20is%0Atheir%20linear%20computational%20and%20space%20complexity%20enables%20faster%20training%20and%0Ainference%20for%20long%20sequences.%20However%2C%20RNNs%20are%20fundamentally%20unable%20to%0Arandomly%20access%20historical%20context%2C%20and%20simply%20integrating%20attention%20mechanisms%0Amay%20undermine%20their%20efficiency%20advantages.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20%5Ctextbf%7BH%7Dierarchical%20%5Ctextbf%7BS%7Dparse%20%5Ctextbf%7BA%7Dttention%20%28HSA%29%2C%20a%20novel%0Aattention%20mechanism%20that%20enhances%20RNNs%20with%20long-range%20random%20access%0Aflexibility%20while%20preserving%20their%20merits%20in%20efficiency%20and%20length%0Ageneralization.%20HSA%20divides%20inputs%20into%20chunks%2C%20selecting%20the%20top-%24k%24%20chunks%0Aand%20hierarchically%20aggregates%20information.%20The%20core%20innovation%20lies%20in%20learning%0Atoken-to-chunk%20relevance%20based%20on%20fine-grained%20token-level%20information%20inside%0Aeach%20chunk.%20This%20approach%20enhances%20the%20precision%20of%20chunk%20selection%20across%20both%0Ain-domain%20and%20out-of-domain%20context%20lengths.%20To%20make%20HSA%20efficient%2C%20we%20further%0Aintroduce%20a%20hardware-aligned%20kernel%20design.%20By%20combining%20HSA%20with%20Mamba%2C%20we%0Aintroduce%20RAMba%2C%20which%20achieves%20perfect%20accuracy%20in%20passkey%20retrieval%20across%2064%0Amillion%20contexts%20despite%20pre-training%20on%20only%204K-length%20contexts%2C%20and%0Asignificant%20improvements%20on%20various%20downstream%20tasks%2C%20with%20nearly%20constant%0Amemory%20footprint.%20These%20results%20show%20RAMba%27s%20huge%20potential%20in%20long-context%0Amodeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16795v1&entry.124074799=Read"},
{"title": "Simplified Swarm Learning Framework for Robust and Scalable Diagnostic\n  Services in Cancer Histopathology", "author": "Yanjie Wu and Yuhao Ji and Saiho Lee and Juniad Akram and Ali Braytee and Ali Anaissi", "abstract": "  The complexities of healthcare data, including privacy concerns, imbalanced\ndatasets, and interoperability issues, necessitate innovative machine learning\nsolutions. Swarm Learning (SL), a decentralized alternative to Federated\nLearning, offers privacy-preserving distributed training, but its reliance on\nblockchain technology hinders accessibility and scalability. This paper\nintroduces a \\textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}\ntailored for resource-constrained environments. By eliminating blockchain\ndependencies and adopting lightweight peer-to-peer communication, the proposed\nframework ensures robust model synchronization while maintaining data privacy.\nApplied to cancer histopathology, the framework integrates optimized\npre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,\nto improve diagnostic accuracy. Extensive experiments demonstrate the\nframework's efficacy in handling imbalanced and biased datasets, achieving\ncomparable performance to centralized models while preserving privacy. This\nstudy paves the way for democratizing advanced machine learning in healthcare,\noffering a scalable, accessible, and efficient solution for privacy-sensitive\ndiagnostic applications.\n", "link": "http://arxiv.org/abs/2504.16732v1", "date": "2025-04-23", "relevancy": 1.9255, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4899}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4775}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplified%20Swarm%20Learning%20Framework%20for%20Robust%20and%20Scalable%20Diagnostic%0A%20%20Services%20in%20Cancer%20Histopathology&body=Title%3A%20Simplified%20Swarm%20Learning%20Framework%20for%20Robust%20and%20Scalable%20Diagnostic%0A%20%20Services%20in%20Cancer%20Histopathology%0AAuthor%3A%20Yanjie%20Wu%20and%20Yuhao%20Ji%20and%20Saiho%20Lee%20and%20Juniad%20Akram%20and%20Ali%20Braytee%20and%20Ali%20Anaissi%0AAbstract%3A%20%20%20The%20complexities%20of%20healthcare%20data%2C%20including%20privacy%20concerns%2C%20imbalanced%0Adatasets%2C%20and%20interoperability%20issues%2C%20necessitate%20innovative%20machine%20learning%0Asolutions.%20Swarm%20Learning%20%28SL%29%2C%20a%20decentralized%20alternative%20to%20Federated%0ALearning%2C%20offers%20privacy-preserving%20distributed%20training%2C%20but%20its%20reliance%20on%0Ablockchain%20technology%20hinders%20accessibility%20and%20scalability.%20This%20paper%0Aintroduces%20a%20%5Ctextit%7BSimplified%20Peer-to-Peer%20Swarm%20Learning%20%28P2P-SL%29%20Framework%7D%0Atailored%20for%20resource-constrained%20environments.%20By%20eliminating%20blockchain%0Adependencies%20and%20adopting%20lightweight%20peer-to-peer%20communication%2C%20the%20proposed%0Aframework%20ensures%20robust%20model%20synchronization%20while%20maintaining%20data%20privacy.%0AApplied%20to%20cancer%20histopathology%2C%20the%20framework%20integrates%20optimized%0Apre-trained%20models%2C%20such%20as%20TorchXRayVision%2C%20enhanced%20with%20DenseNet%20decoders%2C%0Ato%20improve%20diagnostic%20accuracy.%20Extensive%20experiments%20demonstrate%20the%0Aframework%27s%20efficacy%20in%20handling%20imbalanced%20and%20biased%20datasets%2C%20achieving%0Acomparable%20performance%20to%20centralized%20models%20while%20preserving%20privacy.%20This%0Astudy%20paves%20the%20way%20for%20democratizing%20advanced%20machine%20learning%20in%20healthcare%2C%0Aoffering%20a%20scalable%2C%20accessible%2C%20and%20efficient%20solution%20for%20privacy-sensitive%0Adiagnostic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplified%2520Swarm%2520Learning%2520Framework%2520for%2520Robust%2520and%2520Scalable%2520Diagnostic%250A%2520%2520Services%2520in%2520Cancer%2520Histopathology%26entry.906535625%3DYanjie%2520Wu%2520and%2520Yuhao%2520Ji%2520and%2520Saiho%2520Lee%2520and%2520Juniad%2520Akram%2520and%2520Ali%2520Braytee%2520and%2520Ali%2520Anaissi%26entry.1292438233%3D%2520%2520The%2520complexities%2520of%2520healthcare%2520data%252C%2520including%2520privacy%2520concerns%252C%2520imbalanced%250Adatasets%252C%2520and%2520interoperability%2520issues%252C%2520necessitate%2520innovative%2520machine%2520learning%250Asolutions.%2520Swarm%2520Learning%2520%2528SL%2529%252C%2520a%2520decentralized%2520alternative%2520to%2520Federated%250ALearning%252C%2520offers%2520privacy-preserving%2520distributed%2520training%252C%2520but%2520its%2520reliance%2520on%250Ablockchain%2520technology%2520hinders%2520accessibility%2520and%2520scalability.%2520This%2520paper%250Aintroduces%2520a%2520%255Ctextit%257BSimplified%2520Peer-to-Peer%2520Swarm%2520Learning%2520%2528P2P-SL%2529%2520Framework%257D%250Atailored%2520for%2520resource-constrained%2520environments.%2520By%2520eliminating%2520blockchain%250Adependencies%2520and%2520adopting%2520lightweight%2520peer-to-peer%2520communication%252C%2520the%2520proposed%250Aframework%2520ensures%2520robust%2520model%2520synchronization%2520while%2520maintaining%2520data%2520privacy.%250AApplied%2520to%2520cancer%2520histopathology%252C%2520the%2520framework%2520integrates%2520optimized%250Apre-trained%2520models%252C%2520such%2520as%2520TorchXRayVision%252C%2520enhanced%2520with%2520DenseNet%2520decoders%252C%250Ato%2520improve%2520diagnostic%2520accuracy.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aframework%2527s%2520efficacy%2520in%2520handling%2520imbalanced%2520and%2520biased%2520datasets%252C%2520achieving%250Acomparable%2520performance%2520to%2520centralized%2520models%2520while%2520preserving%2520privacy.%2520This%250Astudy%2520paves%2520the%2520way%2520for%2520democratizing%2520advanced%2520machine%2520learning%2520in%2520healthcare%252C%250Aoffering%2520a%2520scalable%252C%2520accessible%252C%2520and%2520efficient%2520solution%2520for%2520privacy-sensitive%250Adiagnostic%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplified%20Swarm%20Learning%20Framework%20for%20Robust%20and%20Scalable%20Diagnostic%0A%20%20Services%20in%20Cancer%20Histopathology&entry.906535625=Yanjie%20Wu%20and%20Yuhao%20Ji%20and%20Saiho%20Lee%20and%20Juniad%20Akram%20and%20Ali%20Braytee%20and%20Ali%20Anaissi&entry.1292438233=%20%20The%20complexities%20of%20healthcare%20data%2C%20including%20privacy%20concerns%2C%20imbalanced%0Adatasets%2C%20and%20interoperability%20issues%2C%20necessitate%20innovative%20machine%20learning%0Asolutions.%20Swarm%20Learning%20%28SL%29%2C%20a%20decentralized%20alternative%20to%20Federated%0ALearning%2C%20offers%20privacy-preserving%20distributed%20training%2C%20but%20its%20reliance%20on%0Ablockchain%20technology%20hinders%20accessibility%20and%20scalability.%20This%20paper%0Aintroduces%20a%20%5Ctextit%7BSimplified%20Peer-to-Peer%20Swarm%20Learning%20%28P2P-SL%29%20Framework%7D%0Atailored%20for%20resource-constrained%20environments.%20By%20eliminating%20blockchain%0Adependencies%20and%20adopting%20lightweight%20peer-to-peer%20communication%2C%20the%20proposed%0Aframework%20ensures%20robust%20model%20synchronization%20while%20maintaining%20data%20privacy.%0AApplied%20to%20cancer%20histopathology%2C%20the%20framework%20integrates%20optimized%0Apre-trained%20models%2C%20such%20as%20TorchXRayVision%2C%20enhanced%20with%20DenseNet%20decoders%2C%0Ato%20improve%20diagnostic%20accuracy.%20Extensive%20experiments%20demonstrate%20the%0Aframework%27s%20efficacy%20in%20handling%20imbalanced%20and%20biased%20datasets%2C%20achieving%0Acomparable%20performance%20to%20centralized%20models%20while%20preserving%20privacy.%20This%0Astudy%20paves%20the%20way%20for%20democratizing%20advanced%20machine%20learning%20in%20healthcare%2C%0Aoffering%20a%20scalable%2C%20accessible%2C%20and%20efficient%20solution%20for%20privacy-sensitive%0Adiagnostic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16732v1&entry.124074799=Read"},
{"title": "Physically Consistent Humanoid Loco-Manipulation using Latent Diffusion\n  Models", "author": "Ilyass Taouil and Haizhou Zhao and Angela Dai and Majid Khadiv", "abstract": "  This paper uses the capabilities of latent diffusion models (LDMs) to\ngenerate realistic RGB human-object interaction scenes to guide humanoid\nloco-manipulation planning. To do so, we extract from the generated images both\nthe contact locations and robot configurations that are then used inside a\nwhole-body trajectory optimization (TO) formulation to generate physically\nconsistent trajectories for humanoids. We validate our full pipeline in\nsimulation for different long-horizon loco-manipulation scenarios and perform\nan extensive analysis of the proposed contact and robot configuration\nextraction pipeline. Our results show that using the information extracted from\nLDMs, we can generate physically consistent trajectories that require\nlong-horizon reasoning.\n", "link": "http://arxiv.org/abs/2504.16843v1", "date": "2025-04-23", "relevancy": 1.9222, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6883}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5816}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20Consistent%20Humanoid%20Loco-Manipulation%20using%20Latent%20Diffusion%0A%20%20Models&body=Title%3A%20Physically%20Consistent%20Humanoid%20Loco-Manipulation%20using%20Latent%20Diffusion%0A%20%20Models%0AAuthor%3A%20Ilyass%20Taouil%20and%20Haizhou%20Zhao%20and%20Angela%20Dai%20and%20Majid%20Khadiv%0AAbstract%3A%20%20%20This%20paper%20uses%20the%20capabilities%20of%20latent%20diffusion%20models%20%28LDMs%29%20to%0Agenerate%20realistic%20RGB%20human-object%20interaction%20scenes%20to%20guide%20humanoid%0Aloco-manipulation%20planning.%20To%20do%20so%2C%20we%20extract%20from%20the%20generated%20images%20both%0Athe%20contact%20locations%20and%20robot%20configurations%20that%20are%20then%20used%20inside%20a%0Awhole-body%20trajectory%20optimization%20%28TO%29%20formulation%20to%20generate%20physically%0Aconsistent%20trajectories%20for%20humanoids.%20We%20validate%20our%20full%20pipeline%20in%0Asimulation%20for%20different%20long-horizon%20loco-manipulation%20scenarios%20and%20perform%0Aan%20extensive%20analysis%20of%20the%20proposed%20contact%20and%20robot%20configuration%0Aextraction%20pipeline.%20Our%20results%20show%20that%20using%20the%20information%20extracted%20from%0ALDMs%2C%20we%20can%20generate%20physically%20consistent%20trajectories%20that%20require%0Along-horizon%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520Consistent%2520Humanoid%2520Loco-Manipulation%2520using%2520Latent%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DIlyass%2520Taouil%2520and%2520Haizhou%2520Zhao%2520and%2520Angela%2520Dai%2520and%2520Majid%2520Khadiv%26entry.1292438233%3D%2520%2520This%2520paper%2520uses%2520the%2520capabilities%2520of%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520to%250Agenerate%2520realistic%2520RGB%2520human-object%2520interaction%2520scenes%2520to%2520guide%2520humanoid%250Aloco-manipulation%2520planning.%2520To%2520do%2520so%252C%2520we%2520extract%2520from%2520the%2520generated%2520images%2520both%250Athe%2520contact%2520locations%2520and%2520robot%2520configurations%2520that%2520are%2520then%2520used%2520inside%2520a%250Awhole-body%2520trajectory%2520optimization%2520%2528TO%2529%2520formulation%2520to%2520generate%2520physically%250Aconsistent%2520trajectories%2520for%2520humanoids.%2520We%2520validate%2520our%2520full%2520pipeline%2520in%250Asimulation%2520for%2520different%2520long-horizon%2520loco-manipulation%2520scenarios%2520and%2520perform%250Aan%2520extensive%2520analysis%2520of%2520the%2520proposed%2520contact%2520and%2520robot%2520configuration%250Aextraction%2520pipeline.%2520Our%2520results%2520show%2520that%2520using%2520the%2520information%2520extracted%2520from%250ALDMs%252C%2520we%2520can%2520generate%2520physically%2520consistent%2520trajectories%2520that%2520require%250Along-horizon%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20Consistent%20Humanoid%20Loco-Manipulation%20using%20Latent%20Diffusion%0A%20%20Models&entry.906535625=Ilyass%20Taouil%20and%20Haizhou%20Zhao%20and%20Angela%20Dai%20and%20Majid%20Khadiv&entry.1292438233=%20%20This%20paper%20uses%20the%20capabilities%20of%20latent%20diffusion%20models%20%28LDMs%29%20to%0Agenerate%20realistic%20RGB%20human-object%20interaction%20scenes%20to%20guide%20humanoid%0Aloco-manipulation%20planning.%20To%20do%20so%2C%20we%20extract%20from%20the%20generated%20images%20both%0Athe%20contact%20locations%20and%20robot%20configurations%20that%20are%20then%20used%20inside%20a%0Awhole-body%20trajectory%20optimization%20%28TO%29%20formulation%20to%20generate%20physically%0Aconsistent%20trajectories%20for%20humanoids.%20We%20validate%20our%20full%20pipeline%20in%0Asimulation%20for%20different%20long-horizon%20loco-manipulation%20scenarios%20and%20perform%0Aan%20extensive%20analysis%20of%20the%20proposed%20contact%20and%20robot%20configuration%0Aextraction%20pipeline.%20Our%20results%20show%20that%20using%20the%20information%20extracted%20from%0ALDMs%2C%20we%20can%20generate%20physically%20consistent%20trajectories%20that%20require%0Along-horizon%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16843v1&entry.124074799=Read"},
{"title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical\n  Reasoning Models with OpenMathReasoning dataset", "author": "Ivan Moshkov and Darragh Hanley and Ivan Sorokin and Shubham Toshniwal and Christof Henkel and Benedikt Schifferer and Wei Du and Igor Gitman", "abstract": "  This paper presents our winning submission to the AI Mathematical Olympiad -\nProgress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art\nmathematical reasoning models relies on three key pillars. First, we create a\nlarge-scale dataset comprising 540K unique high-quality math problems,\nincluding olympiad-level problems, and their 3.2M long-reasoning solutions.\nSecond, we develop a novel method to integrate code execution with long\nreasoning models through iterative training, generation, and quality filtering,\nresulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we\ncreate a pipeline to train models to select the most promising solution from\nmany candidates. We show that such generative solution selection (GenSelect)\ncan significantly improve upon majority voting baseline. Combining these ideas,\nwe train a series of models that achieve state-of-the-art results on\nmathematical reasoning benchmarks. To facilitate further research, we release\nour code, models, and the complete OpenMathReasoning dataset under a\ncommercially permissive license.\n", "link": "http://arxiv.org/abs/2504.16891v1", "date": "2025-04-23", "relevancy": 1.9204, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIMO-2%20Winning%20Solution%3A%20Building%20State-of-the-Art%20Mathematical%0A%20%20Reasoning%20Models%20with%20OpenMathReasoning%20dataset&body=Title%3A%20AIMO-2%20Winning%20Solution%3A%20Building%20State-of-the-Art%20Mathematical%0A%20%20Reasoning%20Models%20with%20OpenMathReasoning%20dataset%0AAuthor%3A%20Ivan%20Moshkov%20and%20Darragh%20Hanley%20and%20Ivan%20Sorokin%20and%20Shubham%20Toshniwal%20and%20Christof%20Henkel%20and%20Benedikt%20Schifferer%20and%20Wei%20Du%20and%20Igor%20Gitman%0AAbstract%3A%20%20%20This%20paper%20presents%20our%20winning%20submission%20to%20the%20AI%20Mathematical%20Olympiad%20-%0AProgress%20Prize%202%20%28AIMO-2%29%20competition.%20Our%20recipe%20for%20building%20state-of-the-art%0Amathematical%20reasoning%20models%20relies%20on%20three%20key%20pillars.%20First%2C%20we%20create%20a%0Alarge-scale%20dataset%20comprising%20540K%20unique%20high-quality%20math%20problems%2C%0Aincluding%20olympiad-level%20problems%2C%20and%20their%203.2M%20long-reasoning%20solutions.%0ASecond%2C%20we%20develop%20a%20novel%20method%20to%20integrate%20code%20execution%20with%20long%0Areasoning%20models%20through%20iterative%20training%2C%20generation%2C%20and%20quality%20filtering%2C%0Aresulting%20in%201.7M%20high-quality%20Tool-Integrated%20Reasoning%20solutions.%20Third%2C%20we%0Acreate%20a%20pipeline%20to%20train%20models%20to%20select%20the%20most%20promising%20solution%20from%0Amany%20candidates.%20We%20show%20that%20such%20generative%20solution%20selection%20%28GenSelect%29%0Acan%20significantly%20improve%20upon%20majority%20voting%20baseline.%20Combining%20these%20ideas%2C%0Awe%20train%20a%20series%20of%20models%20that%20achieve%20state-of-the-art%20results%20on%0Amathematical%20reasoning%20benchmarks.%20To%20facilitate%20further%20research%2C%20we%20release%0Aour%20code%2C%20models%2C%20and%20the%20complete%20OpenMathReasoning%20dataset%20under%20a%0Acommercially%20permissive%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIMO-2%2520Winning%2520Solution%253A%2520Building%2520State-of-the-Art%2520Mathematical%250A%2520%2520Reasoning%2520Models%2520with%2520OpenMathReasoning%2520dataset%26entry.906535625%3DIvan%2520Moshkov%2520and%2520Darragh%2520Hanley%2520and%2520Ivan%2520Sorokin%2520and%2520Shubham%2520Toshniwal%2520and%2520Christof%2520Henkel%2520and%2520Benedikt%2520Schifferer%2520and%2520Wei%2520Du%2520and%2520Igor%2520Gitman%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520our%2520winning%2520submission%2520to%2520the%2520AI%2520Mathematical%2520Olympiad%2520-%250AProgress%2520Prize%25202%2520%2528AIMO-2%2529%2520competition.%2520Our%2520recipe%2520for%2520building%2520state-of-the-art%250Amathematical%2520reasoning%2520models%2520relies%2520on%2520three%2520key%2520pillars.%2520First%252C%2520we%2520create%2520a%250Alarge-scale%2520dataset%2520comprising%2520540K%2520unique%2520high-quality%2520math%2520problems%252C%250Aincluding%2520olympiad-level%2520problems%252C%2520and%2520their%25203.2M%2520long-reasoning%2520solutions.%250ASecond%252C%2520we%2520develop%2520a%2520novel%2520method%2520to%2520integrate%2520code%2520execution%2520with%2520long%250Areasoning%2520models%2520through%2520iterative%2520training%252C%2520generation%252C%2520and%2520quality%2520filtering%252C%250Aresulting%2520in%25201.7M%2520high-quality%2520Tool-Integrated%2520Reasoning%2520solutions.%2520Third%252C%2520we%250Acreate%2520a%2520pipeline%2520to%2520train%2520models%2520to%2520select%2520the%2520most%2520promising%2520solution%2520from%250Amany%2520candidates.%2520We%2520show%2520that%2520such%2520generative%2520solution%2520selection%2520%2528GenSelect%2529%250Acan%2520significantly%2520improve%2520upon%2520majority%2520voting%2520baseline.%2520Combining%2520these%2520ideas%252C%250Awe%2520train%2520a%2520series%2520of%2520models%2520that%2520achieve%2520state-of-the-art%2520results%2520on%250Amathematical%2520reasoning%2520benchmarks.%2520To%2520facilitate%2520further%2520research%252C%2520we%2520release%250Aour%2520code%252C%2520models%252C%2520and%2520the%2520complete%2520OpenMathReasoning%2520dataset%2520under%2520a%250Acommercially%2520permissive%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIMO-2%20Winning%20Solution%3A%20Building%20State-of-the-Art%20Mathematical%0A%20%20Reasoning%20Models%20with%20OpenMathReasoning%20dataset&entry.906535625=Ivan%20Moshkov%20and%20Darragh%20Hanley%20and%20Ivan%20Sorokin%20and%20Shubham%20Toshniwal%20and%20Christof%20Henkel%20and%20Benedikt%20Schifferer%20and%20Wei%20Du%20and%20Igor%20Gitman&entry.1292438233=%20%20This%20paper%20presents%20our%20winning%20submission%20to%20the%20AI%20Mathematical%20Olympiad%20-%0AProgress%20Prize%202%20%28AIMO-2%29%20competition.%20Our%20recipe%20for%20building%20state-of-the-art%0Amathematical%20reasoning%20models%20relies%20on%20three%20key%20pillars.%20First%2C%20we%20create%20a%0Alarge-scale%20dataset%20comprising%20540K%20unique%20high-quality%20math%20problems%2C%0Aincluding%20olympiad-level%20problems%2C%20and%20their%203.2M%20long-reasoning%20solutions.%0ASecond%2C%20we%20develop%20a%20novel%20method%20to%20integrate%20code%20execution%20with%20long%0Areasoning%20models%20through%20iterative%20training%2C%20generation%2C%20and%20quality%20filtering%2C%0Aresulting%20in%201.7M%20high-quality%20Tool-Integrated%20Reasoning%20solutions.%20Third%2C%20we%0Acreate%20a%20pipeline%20to%20train%20models%20to%20select%20the%20most%20promising%20solution%20from%0Amany%20candidates.%20We%20show%20that%20such%20generative%20solution%20selection%20%28GenSelect%29%0Acan%20significantly%20improve%20upon%20majority%20voting%20baseline.%20Combining%20these%20ideas%2C%0Awe%20train%20a%20series%20of%20models%20that%20achieve%20state-of-the-art%20results%20on%0Amathematical%20reasoning%20benchmarks.%20To%20facilitate%20further%20research%2C%20we%20release%0Aour%20code%2C%20models%2C%20and%20the%20complete%20OpenMathReasoning%20dataset%20under%20a%0Acommercially%20permissive%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16891v1&entry.124074799=Read"},
{"title": "Clinical QA 2.0: Multi-Task Learning for Answer Extraction and\n  Categorization", "author": "Priyaranjan Pattnayak and Hitesh Laxmichand Patel and Amit Agarwal and Bhargava Kumar and Srikant Panda and Tejaswini Kumar", "abstract": "  Clinical Question Answering (CQA) plays a crucial role in medical\ndecision-making, enabling physicians to extract relevant information from\nElectronic Medical Records (EMRs). While transformer-based models such as BERT,\nBioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in\nCQA, existing models lack the ability to categorize extracted answers, which is\ncritical for structured retrieval, content filtering, and medical decision\nsupport.\n  To address this limitation, we introduce a Multi-Task Learning (MTL)\nframework that jointly trains CQA models for both answer extraction and medical\ncategorization. In addition to predicting answer spans, our model classifies\nresponses into five standardized medical categories: Diagnosis, Medication,\nSymptoms, Procedure, and Lab Reports. This categorization enables more\nstructured and interpretable outputs, making clinical QA models more useful in\nreal-world healthcare settings.\n  We evaluate our approach on emrQA, a large-scale dataset for medical question\nanswering. Results show that MTL improves F1-score by 2.2% compared to standard\nfine-tuning, while achieving 90.7% accuracy in answer categorization. These\nfindings suggest that MTL not only enhances CQA performance but also introduces\nan effective mechanism for categorization and structured medical information\nretrieval.\n", "link": "http://arxiv.org/abs/2502.13108v2", "date": "2025-04-23", "relevancy": 1.9025, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5094}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clinical%20QA%202.0%3A%20Multi-Task%20Learning%20for%20Answer%20Extraction%20and%0A%20%20Categorization&body=Title%3A%20Clinical%20QA%202.0%3A%20Multi-Task%20Learning%20for%20Answer%20Extraction%20and%0A%20%20Categorization%0AAuthor%3A%20Priyaranjan%20Pattnayak%20and%20Hitesh%20Laxmichand%20Patel%20and%20Amit%20Agarwal%20and%20Bhargava%20Kumar%20and%20Srikant%20Panda%20and%20Tejaswini%20Kumar%0AAbstract%3A%20%20%20Clinical%20Question%20Answering%20%28CQA%29%20plays%20a%20crucial%20role%20in%20medical%0Adecision-making%2C%20enabling%20physicians%20to%20extract%20relevant%20information%20from%0AElectronic%20Medical%20Records%20%28EMRs%29.%20While%20transformer-based%20models%20such%20as%20BERT%2C%0ABioBERT%2C%20and%20ClinicalBERT%20have%20demonstrated%20state-of-the-art%20performance%20in%0ACQA%2C%20existing%20models%20lack%20the%20ability%20to%20categorize%20extracted%20answers%2C%20which%20is%0Acritical%20for%20structured%20retrieval%2C%20content%20filtering%2C%20and%20medical%20decision%0Asupport.%0A%20%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20Multi-Task%20Learning%20%28MTL%29%0Aframework%20that%20jointly%20trains%20CQA%20models%20for%20both%20answer%20extraction%20and%20medical%0Acategorization.%20In%20addition%20to%20predicting%20answer%20spans%2C%20our%20model%20classifies%0Aresponses%20into%20five%20standardized%20medical%20categories%3A%20Diagnosis%2C%20Medication%2C%0ASymptoms%2C%20Procedure%2C%20and%20Lab%20Reports.%20This%20categorization%20enables%20more%0Astructured%20and%20interpretable%20outputs%2C%20making%20clinical%20QA%20models%20more%20useful%20in%0Areal-world%20healthcare%20settings.%0A%20%20We%20evaluate%20our%20approach%20on%20emrQA%2C%20a%20large-scale%20dataset%20for%20medical%20question%0Aanswering.%20Results%20show%20that%20MTL%20improves%20F1-score%20by%202.2%25%20compared%20to%20standard%0Afine-tuning%2C%20while%20achieving%2090.7%25%20accuracy%20in%20answer%20categorization.%20These%0Afindings%20suggest%20that%20MTL%20not%20only%20enhances%20CQA%20performance%20but%20also%20introduces%0Aan%20effective%20mechanism%20for%20categorization%20and%20structured%20medical%20information%0Aretrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClinical%2520QA%25202.0%253A%2520Multi-Task%2520Learning%2520for%2520Answer%2520Extraction%2520and%250A%2520%2520Categorization%26entry.906535625%3DPriyaranjan%2520Pattnayak%2520and%2520Hitesh%2520Laxmichand%2520Patel%2520and%2520Amit%2520Agarwal%2520and%2520Bhargava%2520Kumar%2520and%2520Srikant%2520Panda%2520and%2520Tejaswini%2520Kumar%26entry.1292438233%3D%2520%2520Clinical%2520Question%2520Answering%2520%2528CQA%2529%2520plays%2520a%2520crucial%2520role%2520in%2520medical%250Adecision-making%252C%2520enabling%2520physicians%2520to%2520extract%2520relevant%2520information%2520from%250AElectronic%2520Medical%2520Records%2520%2528EMRs%2529.%2520While%2520transformer-based%2520models%2520such%2520as%2520BERT%252C%250ABioBERT%252C%2520and%2520ClinicalBERT%2520have%2520demonstrated%2520state-of-the-art%2520performance%2520in%250ACQA%252C%2520existing%2520models%2520lack%2520the%2520ability%2520to%2520categorize%2520extracted%2520answers%252C%2520which%2520is%250Acritical%2520for%2520structured%2520retrieval%252C%2520content%2520filtering%252C%2520and%2520medical%2520decision%250Asupport.%250A%2520%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520Multi-Task%2520Learning%2520%2528MTL%2529%250Aframework%2520that%2520jointly%2520trains%2520CQA%2520models%2520for%2520both%2520answer%2520extraction%2520and%2520medical%250Acategorization.%2520In%2520addition%2520to%2520predicting%2520answer%2520spans%252C%2520our%2520model%2520classifies%250Aresponses%2520into%2520five%2520standardized%2520medical%2520categories%253A%2520Diagnosis%252C%2520Medication%252C%250ASymptoms%252C%2520Procedure%252C%2520and%2520Lab%2520Reports.%2520This%2520categorization%2520enables%2520more%250Astructured%2520and%2520interpretable%2520outputs%252C%2520making%2520clinical%2520QA%2520models%2520more%2520useful%2520in%250Areal-world%2520healthcare%2520settings.%250A%2520%2520We%2520evaluate%2520our%2520approach%2520on%2520emrQA%252C%2520a%2520large-scale%2520dataset%2520for%2520medical%2520question%250Aanswering.%2520Results%2520show%2520that%2520MTL%2520improves%2520F1-score%2520by%25202.2%2525%2520compared%2520to%2520standard%250Afine-tuning%252C%2520while%2520achieving%252090.7%2525%2520accuracy%2520in%2520answer%2520categorization.%2520These%250Afindings%2520suggest%2520that%2520MTL%2520not%2520only%2520enhances%2520CQA%2520performance%2520but%2520also%2520introduces%250Aan%2520effective%2520mechanism%2520for%2520categorization%2520and%2520structured%2520medical%2520information%250Aretrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clinical%20QA%202.0%3A%20Multi-Task%20Learning%20for%20Answer%20Extraction%20and%0A%20%20Categorization&entry.906535625=Priyaranjan%20Pattnayak%20and%20Hitesh%20Laxmichand%20Patel%20and%20Amit%20Agarwal%20and%20Bhargava%20Kumar%20and%20Srikant%20Panda%20and%20Tejaswini%20Kumar&entry.1292438233=%20%20Clinical%20Question%20Answering%20%28CQA%29%20plays%20a%20crucial%20role%20in%20medical%0Adecision-making%2C%20enabling%20physicians%20to%20extract%20relevant%20information%20from%0AElectronic%20Medical%20Records%20%28EMRs%29.%20While%20transformer-based%20models%20such%20as%20BERT%2C%0ABioBERT%2C%20and%20ClinicalBERT%20have%20demonstrated%20state-of-the-art%20performance%20in%0ACQA%2C%20existing%20models%20lack%20the%20ability%20to%20categorize%20extracted%20answers%2C%20which%20is%0Acritical%20for%20structured%20retrieval%2C%20content%20filtering%2C%20and%20medical%20decision%0Asupport.%0A%20%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20Multi-Task%20Learning%20%28MTL%29%0Aframework%20that%20jointly%20trains%20CQA%20models%20for%20both%20answer%20extraction%20and%20medical%0Acategorization.%20In%20addition%20to%20predicting%20answer%20spans%2C%20our%20model%20classifies%0Aresponses%20into%20five%20standardized%20medical%20categories%3A%20Diagnosis%2C%20Medication%2C%0ASymptoms%2C%20Procedure%2C%20and%20Lab%20Reports.%20This%20categorization%20enables%20more%0Astructured%20and%20interpretable%20outputs%2C%20making%20clinical%20QA%20models%20more%20useful%20in%0Areal-world%20healthcare%20settings.%0A%20%20We%20evaluate%20our%20approach%20on%20emrQA%2C%20a%20large-scale%20dataset%20for%20medical%20question%0Aanswering.%20Results%20show%20that%20MTL%20improves%20F1-score%20by%202.2%25%20compared%20to%20standard%0Afine-tuning%2C%20while%20achieving%2090.7%25%20accuracy%20in%20answer%20categorization.%20These%0Afindings%20suggest%20that%20MTL%20not%20only%20enhances%20CQA%20performance%20but%20also%20introduces%0Aan%20effective%20mechanism%20for%20categorization%20and%20structured%20medical%20information%0Aretrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13108v2&entry.124074799=Read"},
{"title": "Lightweight Latent Verifiers for Efficient Meta-Generation Strategies", "author": "Bartosz Piotrowski and Witold Drzewakowski and Konrad Staniszewski and Piotr Mi\u0142o\u015b", "abstract": "  Verifiers are auxiliary models that assess the correctness of outputs\ngenerated by base large language models (LLMs). They play a crucial role in\nmany strategies for solving reasoning-intensive problems with LLMs. Typically,\nverifiers are LLMs themselves, often as large (or larger) than the base model\nthey support, making them computationally expensive. In this work, we introduce\na novel lightweight verification approach, LiLaVe, which reliably extracts\ncorrectness signals from the hidden states of the base LLM. A key advantage of\nLiLaVe is its ability to operate with only a small fraction of the\ncomputational budget required by traditional LLM-based verifiers. To\ndemonstrate its practicality, we couple LiLaVe with popular meta-generation\nstrategies, like best-of-n or self-consistency. Moreover, we design novel\nLiLaVe-based approaches, like conditional self-correction or conditional\nmajority voting, that significantly improve both accuracy and efficiency in\ngeneration tasks with smaller LLMs. Our work demonstrates the fruitfulness of\nextracting latent information from the hidden states of LLMs, and opens the\ndoor to scalable and resource-efficient solutions for reasoning-intensive\napplications.\n", "link": "http://arxiv.org/abs/2504.16760v1", "date": "2025-04-23", "relevancy": 1.9021, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4851}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Latent%20Verifiers%20for%20Efficient%20Meta-Generation%20Strategies&body=Title%3A%20Lightweight%20Latent%20Verifiers%20for%20Efficient%20Meta-Generation%20Strategies%0AAuthor%3A%20Bartosz%20Piotrowski%20and%20Witold%20Drzewakowski%20and%20Konrad%20Staniszewski%20and%20Piotr%20Mi%C5%82o%C5%9B%0AAbstract%3A%20%20%20Verifiers%20are%20auxiliary%20models%20that%20assess%20the%20correctness%20of%20outputs%0Agenerated%20by%20base%20large%20language%20models%20%28LLMs%29.%20They%20play%20a%20crucial%20role%20in%0Amany%20strategies%20for%20solving%20reasoning-intensive%20problems%20with%20LLMs.%20Typically%2C%0Averifiers%20are%20LLMs%20themselves%2C%20often%20as%20large%20%28or%20larger%29%20than%20the%20base%20model%0Athey%20support%2C%20making%20them%20computationally%20expensive.%20In%20this%20work%2C%20we%20introduce%0Aa%20novel%20lightweight%20verification%20approach%2C%20LiLaVe%2C%20which%20reliably%20extracts%0Acorrectness%20signals%20from%20the%20hidden%20states%20of%20the%20base%20LLM.%20A%20key%20advantage%20of%0ALiLaVe%20is%20its%20ability%20to%20operate%20with%20only%20a%20small%20fraction%20of%20the%0Acomputational%20budget%20required%20by%20traditional%20LLM-based%20verifiers.%20To%0Ademonstrate%20its%20practicality%2C%20we%20couple%20LiLaVe%20with%20popular%20meta-generation%0Astrategies%2C%20like%20best-of-n%20or%20self-consistency.%20Moreover%2C%20we%20design%20novel%0ALiLaVe-based%20approaches%2C%20like%20conditional%20self-correction%20or%20conditional%0Amajority%20voting%2C%20that%20significantly%20improve%20both%20accuracy%20and%20efficiency%20in%0Ageneration%20tasks%20with%20smaller%20LLMs.%20Our%20work%20demonstrates%20the%20fruitfulness%20of%0Aextracting%20latent%20information%20from%20the%20hidden%20states%20of%20LLMs%2C%20and%20opens%20the%0Adoor%20to%20scalable%20and%20resource-efficient%20solutions%20for%20reasoning-intensive%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Latent%2520Verifiers%2520for%2520Efficient%2520Meta-Generation%2520Strategies%26entry.906535625%3DBartosz%2520Piotrowski%2520and%2520Witold%2520Drzewakowski%2520and%2520Konrad%2520Staniszewski%2520and%2520Piotr%2520Mi%25C5%2582o%25C5%259B%26entry.1292438233%3D%2520%2520Verifiers%2520are%2520auxiliary%2520models%2520that%2520assess%2520the%2520correctness%2520of%2520outputs%250Agenerated%2520by%2520base%2520large%2520language%2520models%2520%2528LLMs%2529.%2520They%2520play%2520a%2520crucial%2520role%2520in%250Amany%2520strategies%2520for%2520solving%2520reasoning-intensive%2520problems%2520with%2520LLMs.%2520Typically%252C%250Averifiers%2520are%2520LLMs%2520themselves%252C%2520often%2520as%2520large%2520%2528or%2520larger%2529%2520than%2520the%2520base%2520model%250Athey%2520support%252C%2520making%2520them%2520computationally%2520expensive.%2520In%2520this%2520work%252C%2520we%2520introduce%250Aa%2520novel%2520lightweight%2520verification%2520approach%252C%2520LiLaVe%252C%2520which%2520reliably%2520extracts%250Acorrectness%2520signals%2520from%2520the%2520hidden%2520states%2520of%2520the%2520base%2520LLM.%2520A%2520key%2520advantage%2520of%250ALiLaVe%2520is%2520its%2520ability%2520to%2520operate%2520with%2520only%2520a%2520small%2520fraction%2520of%2520the%250Acomputational%2520budget%2520required%2520by%2520traditional%2520LLM-based%2520verifiers.%2520To%250Ademonstrate%2520its%2520practicality%252C%2520we%2520couple%2520LiLaVe%2520with%2520popular%2520meta-generation%250Astrategies%252C%2520like%2520best-of-n%2520or%2520self-consistency.%2520Moreover%252C%2520we%2520design%2520novel%250ALiLaVe-based%2520approaches%252C%2520like%2520conditional%2520self-correction%2520or%2520conditional%250Amajority%2520voting%252C%2520that%2520significantly%2520improve%2520both%2520accuracy%2520and%2520efficiency%2520in%250Ageneration%2520tasks%2520with%2520smaller%2520LLMs.%2520Our%2520work%2520demonstrates%2520the%2520fruitfulness%2520of%250Aextracting%2520latent%2520information%2520from%2520the%2520hidden%2520states%2520of%2520LLMs%252C%2520and%2520opens%2520the%250Adoor%2520to%2520scalable%2520and%2520resource-efficient%2520solutions%2520for%2520reasoning-intensive%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Latent%20Verifiers%20for%20Efficient%20Meta-Generation%20Strategies&entry.906535625=Bartosz%20Piotrowski%20and%20Witold%20Drzewakowski%20and%20Konrad%20Staniszewski%20and%20Piotr%20Mi%C5%82o%C5%9B&entry.1292438233=%20%20Verifiers%20are%20auxiliary%20models%20that%20assess%20the%20correctness%20of%20outputs%0Agenerated%20by%20base%20large%20language%20models%20%28LLMs%29.%20They%20play%20a%20crucial%20role%20in%0Amany%20strategies%20for%20solving%20reasoning-intensive%20problems%20with%20LLMs.%20Typically%2C%0Averifiers%20are%20LLMs%20themselves%2C%20often%20as%20large%20%28or%20larger%29%20than%20the%20base%20model%0Athey%20support%2C%20making%20them%20computationally%20expensive.%20In%20this%20work%2C%20we%20introduce%0Aa%20novel%20lightweight%20verification%20approach%2C%20LiLaVe%2C%20which%20reliably%20extracts%0Acorrectness%20signals%20from%20the%20hidden%20states%20of%20the%20base%20LLM.%20A%20key%20advantage%20of%0ALiLaVe%20is%20its%20ability%20to%20operate%20with%20only%20a%20small%20fraction%20of%20the%0Acomputational%20budget%20required%20by%20traditional%20LLM-based%20verifiers.%20To%0Ademonstrate%20its%20practicality%2C%20we%20couple%20LiLaVe%20with%20popular%20meta-generation%0Astrategies%2C%20like%20best-of-n%20or%20self-consistency.%20Moreover%2C%20we%20design%20novel%0ALiLaVe-based%20approaches%2C%20like%20conditional%20self-correction%20or%20conditional%0Amajority%20voting%2C%20that%20significantly%20improve%20both%20accuracy%20and%20efficiency%20in%0Ageneration%20tasks%20with%20smaller%20LLMs.%20Our%20work%20demonstrates%20the%20fruitfulness%20of%0Aextracting%20latent%20information%20from%20the%20hidden%20states%20of%20LLMs%2C%20and%20opens%20the%0Adoor%20to%20scalable%20and%20resource-efficient%20solutions%20for%20reasoning-intensive%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16760v1&entry.124074799=Read"},
{"title": "Radiometer Calibration using Machine Learning", "author": "S. A. K. Leeney and H. T. J. Bevins and E. de Lera Acedo and W. J. Handley and C. Kirkham and R. S. Patel and J. Zhu and D. Molnar and J. Cumner and D. Anstey and K. Artuc and G. Bernardi and M. Bucher and S. Carey and J. Cavillot and R. Chiello and W. Croukamp and D. I. L. de Villiers and J. A. Ely and A. Fialkov and T. Gessey-Jones and G. Kulkarni and A. Magro and P. D. Meerburg and S. Mittal and J. H. N. Pattison and S. Pegwal and C. M. Pieterse and J. R. Pritchard and E. Puchwein and N. Razavi-Ghods and I. L. V. Roque and A. Saxena and K. H. Scheutwinkel and P. Scott and E. Shen and P. H. Sims and M. Spinelli", "abstract": "  Radiometers are crucial instruments in radio astronomy, forming the primary\ncomponent of nearly all radio telescopes. They measure the intensity of\nelectromagnetic radiation, converting this radiation into electrical signals. A\nradiometer's primary components are an antenna and a Low Noise Amplifier (LNA),\nwhich is the core of the ``receiver'' chain. Instrumental effects introduced by\nthe receiver are typically corrected or removed during calibration. However,\nimpedance mismatches between the antenna and receiver can introduce unwanted\nsignal reflections and distortions. Traditional calibration methods, such as\nDicke switching, alternate the receiver input between the antenna and a\nwell-characterised reference source to mitigate errors by comparison. Recent\nadvances in Machine Learning (ML) offer promising alternatives. Neural\nnetworks, which are trained using known signal sources, provide a powerful\nmeans to model and calibrate complex systems where traditional analytical\napproaches struggle. These methods are especially relevant for detecting the\nfaint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is\none of the main challenges in observational Cosmology today. Here, for the\nfirst time, we introduce and test a machine learning-based calibration\nframework capable of achieving the precision required for radiometric\nexperiments aiming to detect the 21-cm line.\n", "link": "http://arxiv.org/abs/2504.16791v1", "date": "2025-04-23", "relevancy": 1.8954, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4798}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4782}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radiometer%20Calibration%20using%20Machine%20Learning&body=Title%3A%20Radiometer%20Calibration%20using%20Machine%20Learning%0AAuthor%3A%20S.%20A.%20K.%20Leeney%20and%20H.%20T.%20J.%20Bevins%20and%20E.%20de%20Lera%20Acedo%20and%20W.%20J.%20Handley%20and%20C.%20Kirkham%20and%20R.%20S.%20Patel%20and%20J.%20Zhu%20and%20D.%20Molnar%20and%20J.%20Cumner%20and%20D.%20Anstey%20and%20K.%20Artuc%20and%20G.%20Bernardi%20and%20M.%20Bucher%20and%20S.%20Carey%20and%20J.%20Cavillot%20and%20R.%20Chiello%20and%20W.%20Croukamp%20and%20D.%20I.%20L.%20de%20Villiers%20and%20J.%20A.%20Ely%20and%20A.%20Fialkov%20and%20T.%20Gessey-Jones%20and%20G.%20Kulkarni%20and%20A.%20Magro%20and%20P.%20D.%20Meerburg%20and%20S.%20Mittal%20and%20J.%20H.%20N.%20Pattison%20and%20S.%20Pegwal%20and%20C.%20M.%20Pieterse%20and%20J.%20R.%20Pritchard%20and%20E.%20Puchwein%20and%20N.%20Razavi-Ghods%20and%20I.%20L.%20V.%20Roque%20and%20A.%20Saxena%20and%20K.%20H.%20Scheutwinkel%20and%20P.%20Scott%20and%20E.%20Shen%20and%20P.%20H.%20Sims%20and%20M.%20Spinelli%0AAbstract%3A%20%20%20Radiometers%20are%20crucial%20instruments%20in%20radio%20astronomy%2C%20forming%20the%20primary%0Acomponent%20of%20nearly%20all%20radio%20telescopes.%20They%20measure%20the%20intensity%20of%0Aelectromagnetic%20radiation%2C%20converting%20this%20radiation%20into%20electrical%20signals.%20A%0Aradiometer%27s%20primary%20components%20are%20an%20antenna%20and%20a%20Low%20Noise%20Amplifier%20%28LNA%29%2C%0Awhich%20is%20the%20core%20of%20the%20%60%60receiver%27%27%20chain.%20Instrumental%20effects%20introduced%20by%0Athe%20receiver%20are%20typically%20corrected%20or%20removed%20during%20calibration.%20However%2C%0Aimpedance%20mismatches%20between%20the%20antenna%20and%20receiver%20can%20introduce%20unwanted%0Asignal%20reflections%20and%20distortions.%20Traditional%20calibration%20methods%2C%20such%20as%0ADicke%20switching%2C%20alternate%20the%20receiver%20input%20between%20the%20antenna%20and%20a%0Awell-characterised%20reference%20source%20to%20mitigate%20errors%20by%20comparison.%20Recent%0Aadvances%20in%20Machine%20Learning%20%28ML%29%20offer%20promising%20alternatives.%20Neural%0Anetworks%2C%20which%20are%20trained%20using%20known%20signal%20sources%2C%20provide%20a%20powerful%0Ameans%20to%20model%20and%20calibrate%20complex%20systems%20where%20traditional%20analytical%0Aapproaches%20struggle.%20These%20methods%20are%20especially%20relevant%20for%20detecting%20the%0Afaint%20sky-averaged%2021-cm%20signal%20from%20atomic%20hydrogen%20at%20high%20redshifts.%20This%20is%0Aone%20of%20the%20main%20challenges%20in%20observational%20Cosmology%20today.%20Here%2C%20for%20the%0Afirst%20time%2C%20we%20introduce%20and%20test%20a%20machine%20learning-based%20calibration%0Aframework%20capable%20of%20achieving%20the%20precision%20required%20for%20radiometric%0Aexperiments%20aiming%20to%20detect%20the%2021-cm%20line.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadiometer%2520Calibration%2520using%2520Machine%2520Learning%26entry.906535625%3DS.%2520A.%2520K.%2520Leeney%2520and%2520H.%2520T.%2520J.%2520Bevins%2520and%2520E.%2520de%2520Lera%2520Acedo%2520and%2520W.%2520J.%2520Handley%2520and%2520C.%2520Kirkham%2520and%2520R.%2520S.%2520Patel%2520and%2520J.%2520Zhu%2520and%2520D.%2520Molnar%2520and%2520J.%2520Cumner%2520and%2520D.%2520Anstey%2520and%2520K.%2520Artuc%2520and%2520G.%2520Bernardi%2520and%2520M.%2520Bucher%2520and%2520S.%2520Carey%2520and%2520J.%2520Cavillot%2520and%2520R.%2520Chiello%2520and%2520W.%2520Croukamp%2520and%2520D.%2520I.%2520L.%2520de%2520Villiers%2520and%2520J.%2520A.%2520Ely%2520and%2520A.%2520Fialkov%2520and%2520T.%2520Gessey-Jones%2520and%2520G.%2520Kulkarni%2520and%2520A.%2520Magro%2520and%2520P.%2520D.%2520Meerburg%2520and%2520S.%2520Mittal%2520and%2520J.%2520H.%2520N.%2520Pattison%2520and%2520S.%2520Pegwal%2520and%2520C.%2520M.%2520Pieterse%2520and%2520J.%2520R.%2520Pritchard%2520and%2520E.%2520Puchwein%2520and%2520N.%2520Razavi-Ghods%2520and%2520I.%2520L.%2520V.%2520Roque%2520and%2520A.%2520Saxena%2520and%2520K.%2520H.%2520Scheutwinkel%2520and%2520P.%2520Scott%2520and%2520E.%2520Shen%2520and%2520P.%2520H.%2520Sims%2520and%2520M.%2520Spinelli%26entry.1292438233%3D%2520%2520Radiometers%2520are%2520crucial%2520instruments%2520in%2520radio%2520astronomy%252C%2520forming%2520the%2520primary%250Acomponent%2520of%2520nearly%2520all%2520radio%2520telescopes.%2520They%2520measure%2520the%2520intensity%2520of%250Aelectromagnetic%2520radiation%252C%2520converting%2520this%2520radiation%2520into%2520electrical%2520signals.%2520A%250Aradiometer%2527s%2520primary%2520components%2520are%2520an%2520antenna%2520and%2520a%2520Low%2520Noise%2520Amplifier%2520%2528LNA%2529%252C%250Awhich%2520is%2520the%2520core%2520of%2520the%2520%2560%2560receiver%2527%2527%2520chain.%2520Instrumental%2520effects%2520introduced%2520by%250Athe%2520receiver%2520are%2520typically%2520corrected%2520or%2520removed%2520during%2520calibration.%2520However%252C%250Aimpedance%2520mismatches%2520between%2520the%2520antenna%2520and%2520receiver%2520can%2520introduce%2520unwanted%250Asignal%2520reflections%2520and%2520distortions.%2520Traditional%2520calibration%2520methods%252C%2520such%2520as%250ADicke%2520switching%252C%2520alternate%2520the%2520receiver%2520input%2520between%2520the%2520antenna%2520and%2520a%250Awell-characterised%2520reference%2520source%2520to%2520mitigate%2520errors%2520by%2520comparison.%2520Recent%250Aadvances%2520in%2520Machine%2520Learning%2520%2528ML%2529%2520offer%2520promising%2520alternatives.%2520Neural%250Anetworks%252C%2520which%2520are%2520trained%2520using%2520known%2520signal%2520sources%252C%2520provide%2520a%2520powerful%250Ameans%2520to%2520model%2520and%2520calibrate%2520complex%2520systems%2520where%2520traditional%2520analytical%250Aapproaches%2520struggle.%2520These%2520methods%2520are%2520especially%2520relevant%2520for%2520detecting%2520the%250Afaint%2520sky-averaged%252021-cm%2520signal%2520from%2520atomic%2520hydrogen%2520at%2520high%2520redshifts.%2520This%2520is%250Aone%2520of%2520the%2520main%2520challenges%2520in%2520observational%2520Cosmology%2520today.%2520Here%252C%2520for%2520the%250Afirst%2520time%252C%2520we%2520introduce%2520and%2520test%2520a%2520machine%2520learning-based%2520calibration%250Aframework%2520capable%2520of%2520achieving%2520the%2520precision%2520required%2520for%2520radiometric%250Aexperiments%2520aiming%2520to%2520detect%2520the%252021-cm%2520line.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radiometer%20Calibration%20using%20Machine%20Learning&entry.906535625=S.%20A.%20K.%20Leeney%20and%20H.%20T.%20J.%20Bevins%20and%20E.%20de%20Lera%20Acedo%20and%20W.%20J.%20Handley%20and%20C.%20Kirkham%20and%20R.%20S.%20Patel%20and%20J.%20Zhu%20and%20D.%20Molnar%20and%20J.%20Cumner%20and%20D.%20Anstey%20and%20K.%20Artuc%20and%20G.%20Bernardi%20and%20M.%20Bucher%20and%20S.%20Carey%20and%20J.%20Cavillot%20and%20R.%20Chiello%20and%20W.%20Croukamp%20and%20D.%20I.%20L.%20de%20Villiers%20and%20J.%20A.%20Ely%20and%20A.%20Fialkov%20and%20T.%20Gessey-Jones%20and%20G.%20Kulkarni%20and%20A.%20Magro%20and%20P.%20D.%20Meerburg%20and%20S.%20Mittal%20and%20J.%20H.%20N.%20Pattison%20and%20S.%20Pegwal%20and%20C.%20M.%20Pieterse%20and%20J.%20R.%20Pritchard%20and%20E.%20Puchwein%20and%20N.%20Razavi-Ghods%20and%20I.%20L.%20V.%20Roque%20and%20A.%20Saxena%20and%20K.%20H.%20Scheutwinkel%20and%20P.%20Scott%20and%20E.%20Shen%20and%20P.%20H.%20Sims%20and%20M.%20Spinelli&entry.1292438233=%20%20Radiometers%20are%20crucial%20instruments%20in%20radio%20astronomy%2C%20forming%20the%20primary%0Acomponent%20of%20nearly%20all%20radio%20telescopes.%20They%20measure%20the%20intensity%20of%0Aelectromagnetic%20radiation%2C%20converting%20this%20radiation%20into%20electrical%20signals.%20A%0Aradiometer%27s%20primary%20components%20are%20an%20antenna%20and%20a%20Low%20Noise%20Amplifier%20%28LNA%29%2C%0Awhich%20is%20the%20core%20of%20the%20%60%60receiver%27%27%20chain.%20Instrumental%20effects%20introduced%20by%0Athe%20receiver%20are%20typically%20corrected%20or%20removed%20during%20calibration.%20However%2C%0Aimpedance%20mismatches%20between%20the%20antenna%20and%20receiver%20can%20introduce%20unwanted%0Asignal%20reflections%20and%20distortions.%20Traditional%20calibration%20methods%2C%20such%20as%0ADicke%20switching%2C%20alternate%20the%20receiver%20input%20between%20the%20antenna%20and%20a%0Awell-characterised%20reference%20source%20to%20mitigate%20errors%20by%20comparison.%20Recent%0Aadvances%20in%20Machine%20Learning%20%28ML%29%20offer%20promising%20alternatives.%20Neural%0Anetworks%2C%20which%20are%20trained%20using%20known%20signal%20sources%2C%20provide%20a%20powerful%0Ameans%20to%20model%20and%20calibrate%20complex%20systems%20where%20traditional%20analytical%0Aapproaches%20struggle.%20These%20methods%20are%20especially%20relevant%20for%20detecting%20the%0Afaint%20sky-averaged%2021-cm%20signal%20from%20atomic%20hydrogen%20at%20high%20redshifts.%20This%20is%0Aone%20of%20the%20main%20challenges%20in%20observational%20Cosmology%20today.%20Here%2C%20for%20the%0Afirst%20time%2C%20we%20introduce%20and%20test%20a%20machine%20learning-based%20calibration%0Aframework%20capable%20of%20achieving%20the%20precision%20required%20for%20radiometric%0Aexperiments%20aiming%20to%20detect%20the%2021-cm%20line.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16791v1&entry.124074799=Read"},
{"title": "Process Reward Models That Think", "author": "Muhammad Khalifa and Rishabh Agarwal and Lajanugen Logeswaran and Jaekyeom Kim and Hao Peng and Moontae Lee and Honglak Lee and Lu Wang", "abstract": "  Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm.\n", "link": "http://arxiv.org/abs/2504.16828v1", "date": "2025-04-23", "relevancy": 1.8941, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Process%20Reward%20Models%20That%20Think&body=Title%3A%20Process%20Reward%20Models%20That%20Think%0AAuthor%3A%20Muhammad%20Khalifa%20and%20Rishabh%20Agarwal%20and%20Lajanugen%20Logeswaran%20and%20Jaekyeom%20Kim%20and%20Hao%20Peng%20and%20Moontae%20Lee%20and%20Honglak%20Lee%20and%20Lu%20Wang%0AAbstract%3A%20%20%20Step-by-step%20verifiers%20--%20also%20known%20as%20process%20reward%20models%20%28PRMs%29%20--%20are%20a%0Akey%20ingredient%20for%20test-time%20scaling.%20PRMs%20require%20step-level%20supervision%2C%0Amaking%20them%20expensive%20to%20train.%20This%20work%20aims%20to%20build%20data-efficient%20PRMs%20as%0Averbalized%20step-wise%20reward%20models%20that%20verify%20every%20step%20in%20the%20solution%20by%0Agenerating%20a%20verification%20chain-of-thought%20%28CoT%29.%20We%20propose%20ThinkPRM%2C%20a%20long%0ACoT%20verifier%20fine-tuned%20on%20orders%20of%20magnitude%20fewer%20process%20labels%20than%20those%0Arequired%20by%20discriminative%20PRMs.%20Our%20approach%20capitalizes%20on%20the%20inherent%0Areasoning%20abilities%20of%20long%20CoT%20models%2C%20and%20outperforms%20LLM-as-a-Judge%20and%0Adiscriminative%20verifiers%20--%20using%20only%201%25%20of%20the%20process%20labels%20in%20PRM800K%20--%0Aacross%20several%20challenging%20benchmarks.%20Specifically%2C%20ThinkPRM%20beats%20the%0Abaselines%20on%20ProcessBench%2C%20MATH-500%2C%20and%20AIME%20%2724%20under%20best-of-N%20selection%20and%0Areward-guided%20search.%20In%20an%20out-of-domain%20evaluation%20on%20a%20subset%20of%0AGPQA-Diamond%20and%20LiveCodeBench%2C%20our%20PRM%20surpasses%20discriminative%20verifiers%0Atrained%20on%20the%20full%20PRM800K%20by%208%25%20and%204.5%25%2C%20respectively.%20Lastly%2C%20under%20the%0Asame%20token%20budget%2C%20ThinkPRM%20scales%20up%20verification%20compute%20more%20effectively%0Acompared%20to%20LLM-as-a-Judge%2C%20outperforming%20it%20by%207.2%25%20on%20a%20subset%20of%0AProcessBench.%20Our%20work%20highlights%20the%20value%20of%20generative%2C%20long%20CoT%20PRMs%20that%0Acan%20scale%20test-time%20compute%20for%20verification%20while%20requiring%20minimal%0Asupervision%20for%20training.%20Our%20code%2C%20data%2C%20and%20models%20will%20be%20released%20at%0Ahttps%3A//github.com/mukhal/thinkprm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcess%2520Reward%2520Models%2520That%2520Think%26entry.906535625%3DMuhammad%2520Khalifa%2520and%2520Rishabh%2520Agarwal%2520and%2520Lajanugen%2520Logeswaran%2520and%2520Jaekyeom%2520Kim%2520and%2520Hao%2520Peng%2520and%2520Moontae%2520Lee%2520and%2520Honglak%2520Lee%2520and%2520Lu%2520Wang%26entry.1292438233%3D%2520%2520Step-by-step%2520verifiers%2520--%2520also%2520known%2520as%2520process%2520reward%2520models%2520%2528PRMs%2529%2520--%2520are%2520a%250Akey%2520ingredient%2520for%2520test-time%2520scaling.%2520PRMs%2520require%2520step-level%2520supervision%252C%250Amaking%2520them%2520expensive%2520to%2520train.%2520This%2520work%2520aims%2520to%2520build%2520data-efficient%2520PRMs%2520as%250Averbalized%2520step-wise%2520reward%2520models%2520that%2520verify%2520every%2520step%2520in%2520the%2520solution%2520by%250Agenerating%2520a%2520verification%2520chain-of-thought%2520%2528CoT%2529.%2520We%2520propose%2520ThinkPRM%252C%2520a%2520long%250ACoT%2520verifier%2520fine-tuned%2520on%2520orders%2520of%2520magnitude%2520fewer%2520process%2520labels%2520than%2520those%250Arequired%2520by%2520discriminative%2520PRMs.%2520Our%2520approach%2520capitalizes%2520on%2520the%2520inherent%250Areasoning%2520abilities%2520of%2520long%2520CoT%2520models%252C%2520and%2520outperforms%2520LLM-as-a-Judge%2520and%250Adiscriminative%2520verifiers%2520--%2520using%2520only%25201%2525%2520of%2520the%2520process%2520labels%2520in%2520PRM800K%2520--%250Aacross%2520several%2520challenging%2520benchmarks.%2520Specifically%252C%2520ThinkPRM%2520beats%2520the%250Abaselines%2520on%2520ProcessBench%252C%2520MATH-500%252C%2520and%2520AIME%2520%252724%2520under%2520best-of-N%2520selection%2520and%250Areward-guided%2520search.%2520In%2520an%2520out-of-domain%2520evaluation%2520on%2520a%2520subset%2520of%250AGPQA-Diamond%2520and%2520LiveCodeBench%252C%2520our%2520PRM%2520surpasses%2520discriminative%2520verifiers%250Atrained%2520on%2520the%2520full%2520PRM800K%2520by%25208%2525%2520and%25204.5%2525%252C%2520respectively.%2520Lastly%252C%2520under%2520the%250Asame%2520token%2520budget%252C%2520ThinkPRM%2520scales%2520up%2520verification%2520compute%2520more%2520effectively%250Acompared%2520to%2520LLM-as-a-Judge%252C%2520outperforming%2520it%2520by%25207.2%2525%2520on%2520a%2520subset%2520of%250AProcessBench.%2520Our%2520work%2520highlights%2520the%2520value%2520of%2520generative%252C%2520long%2520CoT%2520PRMs%2520that%250Acan%2520scale%2520test-time%2520compute%2520for%2520verification%2520while%2520requiring%2520minimal%250Asupervision%2520for%2520training.%2520Our%2520code%252C%2520data%252C%2520and%2520models%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/mukhal/thinkprm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Process%20Reward%20Models%20That%20Think&entry.906535625=Muhammad%20Khalifa%20and%20Rishabh%20Agarwal%20and%20Lajanugen%20Logeswaran%20and%20Jaekyeom%20Kim%20and%20Hao%20Peng%20and%20Moontae%20Lee%20and%20Honglak%20Lee%20and%20Lu%20Wang&entry.1292438233=%20%20Step-by-step%20verifiers%20--%20also%20known%20as%20process%20reward%20models%20%28PRMs%29%20--%20are%20a%0Akey%20ingredient%20for%20test-time%20scaling.%20PRMs%20require%20step-level%20supervision%2C%0Amaking%20them%20expensive%20to%20train.%20This%20work%20aims%20to%20build%20data-efficient%20PRMs%20as%0Averbalized%20step-wise%20reward%20models%20that%20verify%20every%20step%20in%20the%20solution%20by%0Agenerating%20a%20verification%20chain-of-thought%20%28CoT%29.%20We%20propose%20ThinkPRM%2C%20a%20long%0ACoT%20verifier%20fine-tuned%20on%20orders%20of%20magnitude%20fewer%20process%20labels%20than%20those%0Arequired%20by%20discriminative%20PRMs.%20Our%20approach%20capitalizes%20on%20the%20inherent%0Areasoning%20abilities%20of%20long%20CoT%20models%2C%20and%20outperforms%20LLM-as-a-Judge%20and%0Adiscriminative%20verifiers%20--%20using%20only%201%25%20of%20the%20process%20labels%20in%20PRM800K%20--%0Aacross%20several%20challenging%20benchmarks.%20Specifically%2C%20ThinkPRM%20beats%20the%0Abaselines%20on%20ProcessBench%2C%20MATH-500%2C%20and%20AIME%20%2724%20under%20best-of-N%20selection%20and%0Areward-guided%20search.%20In%20an%20out-of-domain%20evaluation%20on%20a%20subset%20of%0AGPQA-Diamond%20and%20LiveCodeBench%2C%20our%20PRM%20surpasses%20discriminative%20verifiers%0Atrained%20on%20the%20full%20PRM800K%20by%208%25%20and%204.5%25%2C%20respectively.%20Lastly%2C%20under%20the%0Asame%20token%20budget%2C%20ThinkPRM%20scales%20up%20verification%20compute%20more%20effectively%0Acompared%20to%20LLM-as-a-Judge%2C%20outperforming%20it%20by%207.2%25%20on%20a%20subset%20of%0AProcessBench.%20Our%20work%20highlights%20the%20value%20of%20generative%2C%20long%20CoT%20PRMs%20that%0Acan%20scale%20test-time%20compute%20for%20verification%20while%20requiring%20minimal%0Asupervision%20for%20training.%20Our%20code%2C%20data%2C%20and%20models%20will%20be%20released%20at%0Ahttps%3A//github.com/mukhal/thinkprm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16828v1&entry.124074799=Read"},
{"title": "Hybrid Reinforcement Learning and Model Predictive Control for Adaptive\n  Control of Hydrogen-Diesel Dual-Fuel Combustion", "author": "Julian Bedei and Murray McBain and Charles Robert Koch and Jakob Andert and David Gordon", "abstract": "  Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive\nControl (ML-MPC) are promising approaches for optimizing hydrogen-diesel\ndual-fuel engine control, as they can effectively control multiple-input\nmultiple-output systems and nonlinear processes. ML-MPC is advantageous for\nproviding safe and optimal controls, ensuring the engine operates within\npredefined safety limits. In contrast, RL is distinguished by its adaptability\nto changing conditions through its learning-based approach. However, the\npractical implementation of either method alone poses challenges. RL requires\nhigh variance in control inputs during early learning phases, which can pose\nrisks to the system by potentially executing unsafe actions, leading to\nmechanical damage. Conversely, ML-MPC relies on an accurate system model to\ngenerate optimal control inputs and has limited adaptability to system drifts,\nsuch as injector aging, which naturally occur in engine applications. To\naddress these limitations, this study proposes a hybrid RL and ML-MPC approach\nthat uses an ML-MPC framework while incorporating an RL agent to dynamically\nadjust the ML-MPC load tracking reference in response to changes in the\nenvironment. At the same time, the ML-MPC ensures that actions stay safe\nthroughout the RL agent's exploration. To evaluate the effectiveness of this\napproach, fuel pressure is deliberately varied to introduce a model-plant\nmismatch between the ML-MPC and the engine test bench. The result of this\nmismatch is a root mean square error (RMSE) in indicated mean effective\npressure of 0.57 bar when running the ML-MPC. The experimental results\ndemonstrate that RL successfully adapts to changing boundary conditions by\naltering the tracking reference while ML-MPC ensures safe control inputs. The\nquantitative improvement in load tracking by implementing RL is an RSME of 0.44\nbar.\n", "link": "http://arxiv.org/abs/2504.16875v1", "date": "2025-04-23", "relevancy": 1.875, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5065}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4922}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Reinforcement%20Learning%20and%20Model%20Predictive%20Control%20for%20Adaptive%0A%20%20Control%20of%20Hydrogen-Diesel%20Dual-Fuel%20Combustion&body=Title%3A%20Hybrid%20Reinforcement%20Learning%20and%20Model%20Predictive%20Control%20for%20Adaptive%0A%20%20Control%20of%20Hydrogen-Diesel%20Dual-Fuel%20Combustion%0AAuthor%3A%20Julian%20Bedei%20and%20Murray%20McBain%20and%20Charles%20Robert%20Koch%20and%20Jakob%20Andert%20and%20David%20Gordon%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20and%20Machine%20Learning%20Integrated%20Model%20Predictive%0AControl%20%28ML-MPC%29%20are%20promising%20approaches%20for%20optimizing%20hydrogen-diesel%0Adual-fuel%20engine%20control%2C%20as%20they%20can%20effectively%20control%20multiple-input%0Amultiple-output%20systems%20and%20nonlinear%20processes.%20ML-MPC%20is%20advantageous%20for%0Aproviding%20safe%20and%20optimal%20controls%2C%20ensuring%20the%20engine%20operates%20within%0Apredefined%20safety%20limits.%20In%20contrast%2C%20RL%20is%20distinguished%20by%20its%20adaptability%0Ato%20changing%20conditions%20through%20its%20learning-based%20approach.%20However%2C%20the%0Apractical%20implementation%20of%20either%20method%20alone%20poses%20challenges.%20RL%20requires%0Ahigh%20variance%20in%20control%20inputs%20during%20early%20learning%20phases%2C%20which%20can%20pose%0Arisks%20to%20the%20system%20by%20potentially%20executing%20unsafe%20actions%2C%20leading%20to%0Amechanical%20damage.%20Conversely%2C%20ML-MPC%20relies%20on%20an%20accurate%20system%20model%20to%0Agenerate%20optimal%20control%20inputs%20and%20has%20limited%20adaptability%20to%20system%20drifts%2C%0Asuch%20as%20injector%20aging%2C%20which%20naturally%20occur%20in%20engine%20applications.%20To%0Aaddress%20these%20limitations%2C%20this%20study%20proposes%20a%20hybrid%20RL%20and%20ML-MPC%20approach%0Athat%20uses%20an%20ML-MPC%20framework%20while%20incorporating%20an%20RL%20agent%20to%20dynamically%0Aadjust%20the%20ML-MPC%20load%20tracking%20reference%20in%20response%20to%20changes%20in%20the%0Aenvironment.%20At%20the%20same%20time%2C%20the%20ML-MPC%20ensures%20that%20actions%20stay%20safe%0Athroughout%20the%20RL%20agent%27s%20exploration.%20To%20evaluate%20the%20effectiveness%20of%20this%0Aapproach%2C%20fuel%20pressure%20is%20deliberately%20varied%20to%20introduce%20a%20model-plant%0Amismatch%20between%20the%20ML-MPC%20and%20the%20engine%20test%20bench.%20The%20result%20of%20this%0Amismatch%20is%20a%20root%20mean%20square%20error%20%28RMSE%29%20in%20indicated%20mean%20effective%0Apressure%20of%200.57%20bar%20when%20running%20the%20ML-MPC.%20The%20experimental%20results%0Ademonstrate%20that%20RL%20successfully%20adapts%20to%20changing%20boundary%20conditions%20by%0Aaltering%20the%20tracking%20reference%20while%20ML-MPC%20ensures%20safe%20control%20inputs.%20The%0Aquantitative%20improvement%20in%20load%20tracking%20by%20implementing%20RL%20is%20an%20RSME%20of%200.44%0Abar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Reinforcement%2520Learning%2520and%2520Model%2520Predictive%2520Control%2520for%2520Adaptive%250A%2520%2520Control%2520of%2520Hydrogen-Diesel%2520Dual-Fuel%2520Combustion%26entry.906535625%3DJulian%2520Bedei%2520and%2520Murray%2520McBain%2520and%2520Charles%2520Robert%2520Koch%2520and%2520Jakob%2520Andert%2520and%2520David%2520Gordon%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520and%2520Machine%2520Learning%2520Integrated%2520Model%2520Predictive%250AControl%2520%2528ML-MPC%2529%2520are%2520promising%2520approaches%2520for%2520optimizing%2520hydrogen-diesel%250Adual-fuel%2520engine%2520control%252C%2520as%2520they%2520can%2520effectively%2520control%2520multiple-input%250Amultiple-output%2520systems%2520and%2520nonlinear%2520processes.%2520ML-MPC%2520is%2520advantageous%2520for%250Aproviding%2520safe%2520and%2520optimal%2520controls%252C%2520ensuring%2520the%2520engine%2520operates%2520within%250Apredefined%2520safety%2520limits.%2520In%2520contrast%252C%2520RL%2520is%2520distinguished%2520by%2520its%2520adaptability%250Ato%2520changing%2520conditions%2520through%2520its%2520learning-based%2520approach.%2520However%252C%2520the%250Apractical%2520implementation%2520of%2520either%2520method%2520alone%2520poses%2520challenges.%2520RL%2520requires%250Ahigh%2520variance%2520in%2520control%2520inputs%2520during%2520early%2520learning%2520phases%252C%2520which%2520can%2520pose%250Arisks%2520to%2520the%2520system%2520by%2520potentially%2520executing%2520unsafe%2520actions%252C%2520leading%2520to%250Amechanical%2520damage.%2520Conversely%252C%2520ML-MPC%2520relies%2520on%2520an%2520accurate%2520system%2520model%2520to%250Agenerate%2520optimal%2520control%2520inputs%2520and%2520has%2520limited%2520adaptability%2520to%2520system%2520drifts%252C%250Asuch%2520as%2520injector%2520aging%252C%2520which%2520naturally%2520occur%2520in%2520engine%2520applications.%2520To%250Aaddress%2520these%2520limitations%252C%2520this%2520study%2520proposes%2520a%2520hybrid%2520RL%2520and%2520ML-MPC%2520approach%250Athat%2520uses%2520an%2520ML-MPC%2520framework%2520while%2520incorporating%2520an%2520RL%2520agent%2520to%2520dynamically%250Aadjust%2520the%2520ML-MPC%2520load%2520tracking%2520reference%2520in%2520response%2520to%2520changes%2520in%2520the%250Aenvironment.%2520At%2520the%2520same%2520time%252C%2520the%2520ML-MPC%2520ensures%2520that%2520actions%2520stay%2520safe%250Athroughout%2520the%2520RL%2520agent%2527s%2520exploration.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520this%250Aapproach%252C%2520fuel%2520pressure%2520is%2520deliberately%2520varied%2520to%2520introduce%2520a%2520model-plant%250Amismatch%2520between%2520the%2520ML-MPC%2520and%2520the%2520engine%2520test%2520bench.%2520The%2520result%2520of%2520this%250Amismatch%2520is%2520a%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%2520in%2520indicated%2520mean%2520effective%250Apressure%2520of%25200.57%2520bar%2520when%2520running%2520the%2520ML-MPC.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520RL%2520successfully%2520adapts%2520to%2520changing%2520boundary%2520conditions%2520by%250Aaltering%2520the%2520tracking%2520reference%2520while%2520ML-MPC%2520ensures%2520safe%2520control%2520inputs.%2520The%250Aquantitative%2520improvement%2520in%2520load%2520tracking%2520by%2520implementing%2520RL%2520is%2520an%2520RSME%2520of%25200.44%250Abar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Reinforcement%20Learning%20and%20Model%20Predictive%20Control%20for%20Adaptive%0A%20%20Control%20of%20Hydrogen-Diesel%20Dual-Fuel%20Combustion&entry.906535625=Julian%20Bedei%20and%20Murray%20McBain%20and%20Charles%20Robert%20Koch%20and%20Jakob%20Andert%20and%20David%20Gordon&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20and%20Machine%20Learning%20Integrated%20Model%20Predictive%0AControl%20%28ML-MPC%29%20are%20promising%20approaches%20for%20optimizing%20hydrogen-diesel%0Adual-fuel%20engine%20control%2C%20as%20they%20can%20effectively%20control%20multiple-input%0Amultiple-output%20systems%20and%20nonlinear%20processes.%20ML-MPC%20is%20advantageous%20for%0Aproviding%20safe%20and%20optimal%20controls%2C%20ensuring%20the%20engine%20operates%20within%0Apredefined%20safety%20limits.%20In%20contrast%2C%20RL%20is%20distinguished%20by%20its%20adaptability%0Ato%20changing%20conditions%20through%20its%20learning-based%20approach.%20However%2C%20the%0Apractical%20implementation%20of%20either%20method%20alone%20poses%20challenges.%20RL%20requires%0Ahigh%20variance%20in%20control%20inputs%20during%20early%20learning%20phases%2C%20which%20can%20pose%0Arisks%20to%20the%20system%20by%20potentially%20executing%20unsafe%20actions%2C%20leading%20to%0Amechanical%20damage.%20Conversely%2C%20ML-MPC%20relies%20on%20an%20accurate%20system%20model%20to%0Agenerate%20optimal%20control%20inputs%20and%20has%20limited%20adaptability%20to%20system%20drifts%2C%0Asuch%20as%20injector%20aging%2C%20which%20naturally%20occur%20in%20engine%20applications.%20To%0Aaddress%20these%20limitations%2C%20this%20study%20proposes%20a%20hybrid%20RL%20and%20ML-MPC%20approach%0Athat%20uses%20an%20ML-MPC%20framework%20while%20incorporating%20an%20RL%20agent%20to%20dynamically%0Aadjust%20the%20ML-MPC%20load%20tracking%20reference%20in%20response%20to%20changes%20in%20the%0Aenvironment.%20At%20the%20same%20time%2C%20the%20ML-MPC%20ensures%20that%20actions%20stay%20safe%0Athroughout%20the%20RL%20agent%27s%20exploration.%20To%20evaluate%20the%20effectiveness%20of%20this%0Aapproach%2C%20fuel%20pressure%20is%20deliberately%20varied%20to%20introduce%20a%20model-plant%0Amismatch%20between%20the%20ML-MPC%20and%20the%20engine%20test%20bench.%20The%20result%20of%20this%0Amismatch%20is%20a%20root%20mean%20square%20error%20%28RMSE%29%20in%20indicated%20mean%20effective%0Apressure%20of%200.57%20bar%20when%20running%20the%20ML-MPC.%20The%20experimental%20results%0Ademonstrate%20that%20RL%20successfully%20adapts%20to%20changing%20boundary%20conditions%20by%0Aaltering%20the%20tracking%20reference%20while%20ML-MPC%20ensures%20safe%20control%20inputs.%20The%0Aquantitative%20improvement%20in%20load%20tracking%20by%20implementing%20RL%20is%20an%20RSME%20of%200.44%0Abar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16875v1&entry.124074799=Read"},
{"title": "MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating\n  Over-Smoothing and Incorporating Outlier Scores", "author": "Fengwei Zhou and Jiafei Song and Wenjin Jason Li and Gengjian Xue and Zhikang Zhao and Yichao Lu and Bailin Na", "abstract": "  Recent advances in large language models have significantly improved their\nability to process long-context input, but practical applications are\nchallenged by increased inference time and resource consumption, particularly\nin resource-constrained environments. To address these challenges, we propose\nMOOSComp, a token-classification-based long-context compression method that\nenhances the performance of a BERT-based compressor by mitigating the\nover-smoothing problem and incorporating outlier scores. In the training phase,\nwe add an inter-class cosine similarity loss term to penalize excessively\nsimilar token representations, thereby improving the token classification\naccuracy. During the compression phase, we introduce outlier scores to preserve\nrare but critical tokens that are prone to be discarded in task-agnostic\ncompression. These scores are integrated with the classifier's output, making\nthe compressor more generalizable to various tasks. Superior performance is\nachieved at various compression ratios on long-context understanding and\nreasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x\ncompression ratio on a resource-constrained mobile device.\n", "link": "http://arxiv.org/abs/2504.16786v1", "date": "2025-04-23", "relevancy": 1.8669, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4783}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.467}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOOSComp%3A%20Improving%20Lightweight%20Long-Context%20Compressor%20via%20Mitigating%0A%20%20Over-Smoothing%20and%20Incorporating%20Outlier%20Scores&body=Title%3A%20MOOSComp%3A%20Improving%20Lightweight%20Long-Context%20Compressor%20via%20Mitigating%0A%20%20Over-Smoothing%20and%20Incorporating%20Outlier%20Scores%0AAuthor%3A%20Fengwei%20Zhou%20and%20Jiafei%20Song%20and%20Wenjin%20Jason%20Li%20and%20Gengjian%20Xue%20and%20Zhikang%20Zhao%20and%20Yichao%20Lu%20and%20Bailin%20Na%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20have%20significantly%20improved%20their%0Aability%20to%20process%20long-context%20input%2C%20but%20practical%20applications%20are%0Achallenged%20by%20increased%20inference%20time%20and%20resource%20consumption%2C%20particularly%0Ain%20resource-constrained%20environments.%20To%20address%20these%20challenges%2C%20we%20propose%0AMOOSComp%2C%20a%20token-classification-based%20long-context%20compression%20method%20that%0Aenhances%20the%20performance%20of%20a%20BERT-based%20compressor%20by%20mitigating%20the%0Aover-smoothing%20problem%20and%20incorporating%20outlier%20scores.%20In%20the%20training%20phase%2C%0Awe%20add%20an%20inter-class%20cosine%20similarity%20loss%20term%20to%20penalize%20excessively%0Asimilar%20token%20representations%2C%20thereby%20improving%20the%20token%20classification%0Aaccuracy.%20During%20the%20compression%20phase%2C%20we%20introduce%20outlier%20scores%20to%20preserve%0Arare%20but%20critical%20tokens%20that%20are%20prone%20to%20be%20discarded%20in%20task-agnostic%0Acompression.%20These%20scores%20are%20integrated%20with%20the%20classifier%27s%20output%2C%20making%0Athe%20compressor%20more%20generalizable%20to%20various%20tasks.%20Superior%20performance%20is%0Aachieved%20at%20various%20compression%20ratios%20on%20long-context%20understanding%20and%0Areasoning%20benchmarks.%20Moreover%2C%20our%20method%20obtains%20a%20speedup%20of%203.3x%20at%20a%204x%0Acompression%20ratio%20on%20a%20resource-constrained%20mobile%20device.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOOSComp%253A%2520Improving%2520Lightweight%2520Long-Context%2520Compressor%2520via%2520Mitigating%250A%2520%2520Over-Smoothing%2520and%2520Incorporating%2520Outlier%2520Scores%26entry.906535625%3DFengwei%2520Zhou%2520and%2520Jiafei%2520Song%2520and%2520Wenjin%2520Jason%2520Li%2520and%2520Gengjian%2520Xue%2520and%2520Zhikang%2520Zhao%2520and%2520Yichao%2520Lu%2520and%2520Bailin%2520Na%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520have%2520significantly%2520improved%2520their%250Aability%2520to%2520process%2520long-context%2520input%252C%2520but%2520practical%2520applications%2520are%250Achallenged%2520by%2520increased%2520inference%2520time%2520and%2520resource%2520consumption%252C%2520particularly%250Ain%2520resource-constrained%2520environments.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AMOOSComp%252C%2520a%2520token-classification-based%2520long-context%2520compression%2520method%2520that%250Aenhances%2520the%2520performance%2520of%2520a%2520BERT-based%2520compressor%2520by%2520mitigating%2520the%250Aover-smoothing%2520problem%2520and%2520incorporating%2520outlier%2520scores.%2520In%2520the%2520training%2520phase%252C%250Awe%2520add%2520an%2520inter-class%2520cosine%2520similarity%2520loss%2520term%2520to%2520penalize%2520excessively%250Asimilar%2520token%2520representations%252C%2520thereby%2520improving%2520the%2520token%2520classification%250Aaccuracy.%2520During%2520the%2520compression%2520phase%252C%2520we%2520introduce%2520outlier%2520scores%2520to%2520preserve%250Arare%2520but%2520critical%2520tokens%2520that%2520are%2520prone%2520to%2520be%2520discarded%2520in%2520task-agnostic%250Acompression.%2520These%2520scores%2520are%2520integrated%2520with%2520the%2520classifier%2527s%2520output%252C%2520making%250Athe%2520compressor%2520more%2520generalizable%2520to%2520various%2520tasks.%2520Superior%2520performance%2520is%250Aachieved%2520at%2520various%2520compression%2520ratios%2520on%2520long-context%2520understanding%2520and%250Areasoning%2520benchmarks.%2520Moreover%252C%2520our%2520method%2520obtains%2520a%2520speedup%2520of%25203.3x%2520at%2520a%25204x%250Acompression%2520ratio%2520on%2520a%2520resource-constrained%2520mobile%2520device.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOOSComp%3A%20Improving%20Lightweight%20Long-Context%20Compressor%20via%20Mitigating%0A%20%20Over-Smoothing%20and%20Incorporating%20Outlier%20Scores&entry.906535625=Fengwei%20Zhou%20and%20Jiafei%20Song%20and%20Wenjin%20Jason%20Li%20and%20Gengjian%20Xue%20and%20Zhikang%20Zhao%20and%20Yichao%20Lu%20and%20Bailin%20Na&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20have%20significantly%20improved%20their%0Aability%20to%20process%20long-context%20input%2C%20but%20practical%20applications%20are%0Achallenged%20by%20increased%20inference%20time%20and%20resource%20consumption%2C%20particularly%0Ain%20resource-constrained%20environments.%20To%20address%20these%20challenges%2C%20we%20propose%0AMOOSComp%2C%20a%20token-classification-based%20long-context%20compression%20method%20that%0Aenhances%20the%20performance%20of%20a%20BERT-based%20compressor%20by%20mitigating%20the%0Aover-smoothing%20problem%20and%20incorporating%20outlier%20scores.%20In%20the%20training%20phase%2C%0Awe%20add%20an%20inter-class%20cosine%20similarity%20loss%20term%20to%20penalize%20excessively%0Asimilar%20token%20representations%2C%20thereby%20improving%20the%20token%20classification%0Aaccuracy.%20During%20the%20compression%20phase%2C%20we%20introduce%20outlier%20scores%20to%20preserve%0Arare%20but%20critical%20tokens%20that%20are%20prone%20to%20be%20discarded%20in%20task-agnostic%0Acompression.%20These%20scores%20are%20integrated%20with%20the%20classifier%27s%20output%2C%20making%0Athe%20compressor%20more%20generalizable%20to%20various%20tasks.%20Superior%20performance%20is%0Aachieved%20at%20various%20compression%20ratios%20on%20long-context%20understanding%20and%0Areasoning%20benchmarks.%20Moreover%2C%20our%20method%20obtains%20a%20speedup%20of%203.3x%20at%20a%204x%0Acompression%20ratio%20on%20a%20resource-constrained%20mobile%20device.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16786v1&entry.124074799=Read"},
{"title": "Natural Language Processing in the Patent Domain: A Survey", "author": "Lekang Jiang and Stephan Goetz", "abstract": "  Patents, which encapsulate crucial technical and legal information in text\nform and referenced drawings, present a rich domain for natural language\nprocessing (NLP) applications. As NLP technologies evolve, large language\nmodels (LLMs) have demonstrated outstanding capabilities in general text\nprocessing and generation tasks. However, the application of LLMs in the patent\ndomain remains under-explored and under-developed due to the complexity of\npatents, particularly their language and legal framework. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation. In addition, we systematically break down the structural and\nlinguistic characteristics unique to patents and map out how NLP can be\nleveraged for patent analysis and generation. Moreover, we demonstrate the\nspectrum of text-based and multimodal patent-related tasks, including nine\npatent analysis and four patent generation tasks.\n", "link": "http://arxiv.org/abs/2403.04105v3", "date": "2025-04-23", "relevancy": 1.8498, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20Processing%20in%20the%20Patent%20Domain%3A%20A%20Survey&body=Title%3A%20Natural%20Language%20Processing%20in%20the%20Patent%20Domain%3A%20A%20Survey%0AAuthor%3A%20Lekang%20Jiang%20and%20Stephan%20Goetz%0AAbstract%3A%20%20%20Patents%2C%20which%20encapsulate%20crucial%20technical%20and%20legal%20information%20in%20text%0Aform%20and%20referenced%20drawings%2C%20present%20a%20rich%20domain%20for%20natural%20language%0Aprocessing%20%28NLP%29%20applications.%20As%20NLP%20technologies%20evolve%2C%20large%20language%0Amodels%20%28LLMs%29%20have%20demonstrated%20outstanding%20capabilities%20in%20general%20text%0Aprocessing%20and%20generation%20tasks.%20However%2C%20the%20application%20of%20LLMs%20in%20the%20patent%0Adomain%20remains%20under-explored%20and%20under-developed%20due%20to%20the%20complexity%20of%0Apatents%2C%20particularly%20their%20language%20and%20legal%20framework.%20Understanding%20the%0Aunique%20characteristics%20of%20patent%20documents%20and%20related%20research%20in%20the%20patent%0Adomain%20becomes%20essential%20for%20researchers%20to%20apply%20these%20tools%20effectively.%0ATherefore%2C%20this%20paper%20aims%20to%20equip%20NLP%20researchers%20with%20the%20essential%0Aknowledge%20to%20navigate%20this%20complex%20domain%20efficiently.%20We%20introduce%20the%0Arelevant%20fundamental%20aspects%20of%20patents%20to%20provide%20solid%20background%0Ainformation.%20In%20addition%2C%20we%20systematically%20break%20down%20the%20structural%20and%0Alinguistic%20characteristics%20unique%20to%20patents%20and%20map%20out%20how%20NLP%20can%20be%0Aleveraged%20for%20patent%20analysis%20and%20generation.%20Moreover%2C%20we%20demonstrate%20the%0Aspectrum%20of%20text-based%20and%20multimodal%20patent-related%20tasks%2C%20including%20nine%0Apatent%20analysis%20and%20four%20patent%20generation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04105v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520Processing%2520in%2520the%2520Patent%2520Domain%253A%2520A%2520Survey%26entry.906535625%3DLekang%2520Jiang%2520and%2520Stephan%2520Goetz%26entry.1292438233%3D%2520%2520Patents%252C%2520which%2520encapsulate%2520crucial%2520technical%2520and%2520legal%2520information%2520in%2520text%250Aform%2520and%2520referenced%2520drawings%252C%2520present%2520a%2520rich%2520domain%2520for%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%2520applications.%2520As%2520NLP%2520technologies%2520evolve%252C%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520have%2520demonstrated%2520outstanding%2520capabilities%2520in%2520general%2520text%250Aprocessing%2520and%2520generation%2520tasks.%2520However%252C%2520the%2520application%2520of%2520LLMs%2520in%2520the%2520patent%250Adomain%2520remains%2520under-explored%2520and%2520under-developed%2520due%2520to%2520the%2520complexity%2520of%250Apatents%252C%2520particularly%2520their%2520language%2520and%2520legal%2520framework.%2520Understanding%2520the%250Aunique%2520characteristics%2520of%2520patent%2520documents%2520and%2520related%2520research%2520in%2520the%2520patent%250Adomain%2520becomes%2520essential%2520for%2520researchers%2520to%2520apply%2520these%2520tools%2520effectively.%250ATherefore%252C%2520this%2520paper%2520aims%2520to%2520equip%2520NLP%2520researchers%2520with%2520the%2520essential%250Aknowledge%2520to%2520navigate%2520this%2520complex%2520domain%2520efficiently.%2520We%2520introduce%2520the%250Arelevant%2520fundamental%2520aspects%2520of%2520patents%2520to%2520provide%2520solid%2520background%250Ainformation.%2520In%2520addition%252C%2520we%2520systematically%2520break%2520down%2520the%2520structural%2520and%250Alinguistic%2520characteristics%2520unique%2520to%2520patents%2520and%2520map%2520out%2520how%2520NLP%2520can%2520be%250Aleveraged%2520for%2520patent%2520analysis%2520and%2520generation.%2520Moreover%252C%2520we%2520demonstrate%2520the%250Aspectrum%2520of%2520text-based%2520and%2520multimodal%2520patent-related%2520tasks%252C%2520including%2520nine%250Apatent%2520analysis%2520and%2520four%2520patent%2520generation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04105v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20Processing%20in%20the%20Patent%20Domain%3A%20A%20Survey&entry.906535625=Lekang%20Jiang%20and%20Stephan%20Goetz&entry.1292438233=%20%20Patents%2C%20which%20encapsulate%20crucial%20technical%20and%20legal%20information%20in%20text%0Aform%20and%20referenced%20drawings%2C%20present%20a%20rich%20domain%20for%20natural%20language%0Aprocessing%20%28NLP%29%20applications.%20As%20NLP%20technologies%20evolve%2C%20large%20language%0Amodels%20%28LLMs%29%20have%20demonstrated%20outstanding%20capabilities%20in%20general%20text%0Aprocessing%20and%20generation%20tasks.%20However%2C%20the%20application%20of%20LLMs%20in%20the%20patent%0Adomain%20remains%20under-explored%20and%20under-developed%20due%20to%20the%20complexity%20of%0Apatents%2C%20particularly%20their%20language%20and%20legal%20framework.%20Understanding%20the%0Aunique%20characteristics%20of%20patent%20documents%20and%20related%20research%20in%20the%20patent%0Adomain%20becomes%20essential%20for%20researchers%20to%20apply%20these%20tools%20effectively.%0ATherefore%2C%20this%20paper%20aims%20to%20equip%20NLP%20researchers%20with%20the%20essential%0Aknowledge%20to%20navigate%20this%20complex%20domain%20efficiently.%20We%20introduce%20the%0Arelevant%20fundamental%20aspects%20of%20patents%20to%20provide%20solid%20background%0Ainformation.%20In%20addition%2C%20we%20systematically%20break%20down%20the%20structural%20and%0Alinguistic%20characteristics%20unique%20to%20patents%20and%20map%20out%20how%20NLP%20can%20be%0Aleveraged%20for%20patent%20analysis%20and%20generation.%20Moreover%2C%20we%20demonstrate%20the%0Aspectrum%20of%20text-based%20and%20multimodal%20patent-related%20tasks%2C%20including%20nine%0Apatent%20analysis%20and%20four%20patent%20generation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04105v3&entry.124074799=Read"},
{"title": "Enhancing Sentiment Analysis in Bengali Texts: A Hybrid Approach Using\n  Lexicon-Based Algorithm and Pretrained Language Model Bangla-BERT", "author": "Hemal Mahmud and Hasan Mahmud and Mohammad Rifat Ahmmad Rashid", "abstract": "  Sentiment analysis (SA) is a process of identifying the emotional tone or\npolarity within a given text and aims to uncover the user's complex emotions\nand inner feelings. While sentiment analysis has been extensively studied for\nlanguages like English, research in Bengali, remains limited, particularly for\nfine-grained sentiment categorization. This work aims to connect this gap by\ndeveloping a novel approach that integrates rule-based algorithms with\npre-trained language models. We developed a dataset from scratch, comprising\nover 15,000 manually labeled reviews. Next, we constructed a Lexicon Data\nDictionary, assigning polarity scores to the reviews. We developed a novel rule\nbased algorithm Bangla Sentiment Polarity Score (BSPS), an approach capable of\ngenerating sentiment scores and classifying reviews into nine distinct\nsentiment categories. To assess the performance of this method, we evaluated\nthe classified sentiments using BanglaBERT, a pre-trained transformer-based\nlanguage model. We also performed sentiment classification directly with\nBanglaBERT on the original data and evaluated this model's results. Our\nanalysis revealed that the BSPS + BanglaBERT hybrid approach outperformed the\nstandalone BanglaBERT model, achieving higher accuracy, precision, and nuanced\nclassification across the nine sentiment categories. The results of our study\nemphasize the value and effectiveness of combining rule-based and pre-trained\nlanguage model approaches for enhanced sentiment analysis in Bengali and\nsuggest pathways for future research and application in languages with similar\nlinguistic complexities.\n", "link": "http://arxiv.org/abs/2411.19584v2", "date": "2025-04-23", "relevancy": 1.8127, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4683}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Sentiment%20Analysis%20in%20Bengali%20Texts%3A%20A%20Hybrid%20Approach%20Using%0A%20%20Lexicon-Based%20Algorithm%20and%20Pretrained%20Language%20Model%20Bangla-BERT&body=Title%3A%20Enhancing%20Sentiment%20Analysis%20in%20Bengali%20Texts%3A%20A%20Hybrid%20Approach%20Using%0A%20%20Lexicon-Based%20Algorithm%20and%20Pretrained%20Language%20Model%20Bangla-BERT%0AAuthor%3A%20Hemal%20Mahmud%20and%20Hasan%20Mahmud%20and%20Mohammad%20Rifat%20Ahmmad%20Rashid%0AAbstract%3A%20%20%20Sentiment%20analysis%20%28SA%29%20is%20a%20process%20of%20identifying%20the%20emotional%20tone%20or%0Apolarity%20within%20a%20given%20text%20and%20aims%20to%20uncover%20the%20user%27s%20complex%20emotions%0Aand%20inner%20feelings.%20While%20sentiment%20analysis%20has%20been%20extensively%20studied%20for%0Alanguages%20like%20English%2C%20research%20in%20Bengali%2C%20remains%20limited%2C%20particularly%20for%0Afine-grained%20sentiment%20categorization.%20This%20work%20aims%20to%20connect%20this%20gap%20by%0Adeveloping%20a%20novel%20approach%20that%20integrates%20rule-based%20algorithms%20with%0Apre-trained%20language%20models.%20We%20developed%20a%20dataset%20from%20scratch%2C%20comprising%0Aover%2015%2C000%20manually%20labeled%20reviews.%20Next%2C%20we%20constructed%20a%20Lexicon%20Data%0ADictionary%2C%20assigning%20polarity%20scores%20to%20the%20reviews.%20We%20developed%20a%20novel%20rule%0Abased%20algorithm%20Bangla%20Sentiment%20Polarity%20Score%20%28BSPS%29%2C%20an%20approach%20capable%20of%0Agenerating%20sentiment%20scores%20and%20classifying%20reviews%20into%20nine%20distinct%0Asentiment%20categories.%20To%20assess%20the%20performance%20of%20this%20method%2C%20we%20evaluated%0Athe%20classified%20sentiments%20using%20BanglaBERT%2C%20a%20pre-trained%20transformer-based%0Alanguage%20model.%20We%20also%20performed%20sentiment%20classification%20directly%20with%0ABanglaBERT%20on%20the%20original%20data%20and%20evaluated%20this%20model%27s%20results.%20Our%0Aanalysis%20revealed%20that%20the%20BSPS%20%2B%20BanglaBERT%20hybrid%20approach%20outperformed%20the%0Astandalone%20BanglaBERT%20model%2C%20achieving%20higher%20accuracy%2C%20precision%2C%20and%20nuanced%0Aclassification%20across%20the%20nine%20sentiment%20categories.%20The%20results%20of%20our%20study%0Aemphasize%20the%20value%20and%20effectiveness%20of%20combining%20rule-based%20and%20pre-trained%0Alanguage%20model%20approaches%20for%20enhanced%20sentiment%20analysis%20in%20Bengali%20and%0Asuggest%20pathways%20for%20future%20research%20and%20application%20in%20languages%20with%20similar%0Alinguistic%20complexities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Sentiment%2520Analysis%2520in%2520Bengali%2520Texts%253A%2520A%2520Hybrid%2520Approach%2520Using%250A%2520%2520Lexicon-Based%2520Algorithm%2520and%2520Pretrained%2520Language%2520Model%2520Bangla-BERT%26entry.906535625%3DHemal%2520Mahmud%2520and%2520Hasan%2520Mahmud%2520and%2520Mohammad%2520Rifat%2520Ahmmad%2520Rashid%26entry.1292438233%3D%2520%2520Sentiment%2520analysis%2520%2528SA%2529%2520is%2520a%2520process%2520of%2520identifying%2520the%2520emotional%2520tone%2520or%250Apolarity%2520within%2520a%2520given%2520text%2520and%2520aims%2520to%2520uncover%2520the%2520user%2527s%2520complex%2520emotions%250Aand%2520inner%2520feelings.%2520While%2520sentiment%2520analysis%2520has%2520been%2520extensively%2520studied%2520for%250Alanguages%2520like%2520English%252C%2520research%2520in%2520Bengali%252C%2520remains%2520limited%252C%2520particularly%2520for%250Afine-grained%2520sentiment%2520categorization.%2520This%2520work%2520aims%2520to%2520connect%2520this%2520gap%2520by%250Adeveloping%2520a%2520novel%2520approach%2520that%2520integrates%2520rule-based%2520algorithms%2520with%250Apre-trained%2520language%2520models.%2520We%2520developed%2520a%2520dataset%2520from%2520scratch%252C%2520comprising%250Aover%252015%252C000%2520manually%2520labeled%2520reviews.%2520Next%252C%2520we%2520constructed%2520a%2520Lexicon%2520Data%250ADictionary%252C%2520assigning%2520polarity%2520scores%2520to%2520the%2520reviews.%2520We%2520developed%2520a%2520novel%2520rule%250Abased%2520algorithm%2520Bangla%2520Sentiment%2520Polarity%2520Score%2520%2528BSPS%2529%252C%2520an%2520approach%2520capable%2520of%250Agenerating%2520sentiment%2520scores%2520and%2520classifying%2520reviews%2520into%2520nine%2520distinct%250Asentiment%2520categories.%2520To%2520assess%2520the%2520performance%2520of%2520this%2520method%252C%2520we%2520evaluated%250Athe%2520classified%2520sentiments%2520using%2520BanglaBERT%252C%2520a%2520pre-trained%2520transformer-based%250Alanguage%2520model.%2520We%2520also%2520performed%2520sentiment%2520classification%2520directly%2520with%250ABanglaBERT%2520on%2520the%2520original%2520data%2520and%2520evaluated%2520this%2520model%2527s%2520results.%2520Our%250Aanalysis%2520revealed%2520that%2520the%2520BSPS%2520%252B%2520BanglaBERT%2520hybrid%2520approach%2520outperformed%2520the%250Astandalone%2520BanglaBERT%2520model%252C%2520achieving%2520higher%2520accuracy%252C%2520precision%252C%2520and%2520nuanced%250Aclassification%2520across%2520the%2520nine%2520sentiment%2520categories.%2520The%2520results%2520of%2520our%2520study%250Aemphasize%2520the%2520value%2520and%2520effectiveness%2520of%2520combining%2520rule-based%2520and%2520pre-trained%250Alanguage%2520model%2520approaches%2520for%2520enhanced%2520sentiment%2520analysis%2520in%2520Bengali%2520and%250Asuggest%2520pathways%2520for%2520future%2520research%2520and%2520application%2520in%2520languages%2520with%2520similar%250Alinguistic%2520complexities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Sentiment%20Analysis%20in%20Bengali%20Texts%3A%20A%20Hybrid%20Approach%20Using%0A%20%20Lexicon-Based%20Algorithm%20and%20Pretrained%20Language%20Model%20Bangla-BERT&entry.906535625=Hemal%20Mahmud%20and%20Hasan%20Mahmud%20and%20Mohammad%20Rifat%20Ahmmad%20Rashid&entry.1292438233=%20%20Sentiment%20analysis%20%28SA%29%20is%20a%20process%20of%20identifying%20the%20emotional%20tone%20or%0Apolarity%20within%20a%20given%20text%20and%20aims%20to%20uncover%20the%20user%27s%20complex%20emotions%0Aand%20inner%20feelings.%20While%20sentiment%20analysis%20has%20been%20extensively%20studied%20for%0Alanguages%20like%20English%2C%20research%20in%20Bengali%2C%20remains%20limited%2C%20particularly%20for%0Afine-grained%20sentiment%20categorization.%20This%20work%20aims%20to%20connect%20this%20gap%20by%0Adeveloping%20a%20novel%20approach%20that%20integrates%20rule-based%20algorithms%20with%0Apre-trained%20language%20models.%20We%20developed%20a%20dataset%20from%20scratch%2C%20comprising%0Aover%2015%2C000%20manually%20labeled%20reviews.%20Next%2C%20we%20constructed%20a%20Lexicon%20Data%0ADictionary%2C%20assigning%20polarity%20scores%20to%20the%20reviews.%20We%20developed%20a%20novel%20rule%0Abased%20algorithm%20Bangla%20Sentiment%20Polarity%20Score%20%28BSPS%29%2C%20an%20approach%20capable%20of%0Agenerating%20sentiment%20scores%20and%20classifying%20reviews%20into%20nine%20distinct%0Asentiment%20categories.%20To%20assess%20the%20performance%20of%20this%20method%2C%20we%20evaluated%0Athe%20classified%20sentiments%20using%20BanglaBERT%2C%20a%20pre-trained%20transformer-based%0Alanguage%20model.%20We%20also%20performed%20sentiment%20classification%20directly%20with%0ABanglaBERT%20on%20the%20original%20data%20and%20evaluated%20this%20model%27s%20results.%20Our%0Aanalysis%20revealed%20that%20the%20BSPS%20%2B%20BanglaBERT%20hybrid%20approach%20outperformed%20the%0Astandalone%20BanglaBERT%20model%2C%20achieving%20higher%20accuracy%2C%20precision%2C%20and%20nuanced%0Aclassification%20across%20the%20nine%20sentiment%20categories.%20The%20results%20of%20our%20study%0Aemphasize%20the%20value%20and%20effectiveness%20of%20combining%20rule-based%20and%20pre-trained%0Alanguage%20model%20approaches%20for%20enhanced%20sentiment%20analysis%20in%20Bengali%20and%0Asuggest%20pathways%20for%20future%20research%20and%20application%20in%20languages%20with%20similar%0Alinguistic%20complexities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19584v2&entry.124074799=Read"},
{"title": "Truthful mechanisms for linear bandit games with private contexts", "author": "Yiting Hu and Lingjie Duan", "abstract": "  The contextual bandit problem, where agents arrive sequentially with personal\ncontexts and the system adapts its arm allocation decisions accordingly, has\nrecently garnered increasing attention for enabling more personalized outcomes.\nHowever, in many healthcare and recommendation applications, agents have\nprivate profiles and may misreport their contexts to gain from the system. For\nexample, in adaptive clinical trials, where hospitals sequentially recruit\nvolunteers to test multiple new treatments and adjust plans based on\nvolunteers' reported profiles such as symptoms and interim data, participants\nmay misreport severe side effects like allergy and nausea to avoid perceived\nsuboptimal treatments. We are the first to study this issue of private context\nmisreporting in a stochastic contextual bandit game between the system and\nnon-repeated agents. We show that traditional low-regret algorithms, such as\nUCB family algorithms and Thompson sampling, fail to ensure truthful reporting\nand can result in linear regret in the worst case, while traditional truthful\nalgorithms like explore-then-commit (ETC) and $\\epsilon$-greedy algorithm incur\nsublinear but high regret. We propose a mechanism that uses a linear program to\nensure truthfulness while minimizing deviation from Thompson sampling, yielding\nan $O(\\ln T)$ frequentist regret. Our numerical experiments further demonstrate\nstrong performance in multiple contexts and across other distribution families.\n", "link": "http://arxiv.org/abs/2501.03865v2", "date": "2025-04-23", "relevancy": 1.7978, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4871}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4447}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Truthful%20mechanisms%20for%20linear%20bandit%20games%20with%20private%20contexts&body=Title%3A%20Truthful%20mechanisms%20for%20linear%20bandit%20games%20with%20private%20contexts%0AAuthor%3A%20Yiting%20Hu%20and%20Lingjie%20Duan%0AAbstract%3A%20%20%20The%20contextual%20bandit%20problem%2C%20where%20agents%20arrive%20sequentially%20with%20personal%0Acontexts%20and%20the%20system%20adapts%20its%20arm%20allocation%20decisions%20accordingly%2C%20has%0Arecently%20garnered%20increasing%20attention%20for%20enabling%20more%20personalized%20outcomes.%0AHowever%2C%20in%20many%20healthcare%20and%20recommendation%20applications%2C%20agents%20have%0Aprivate%20profiles%20and%20may%20misreport%20their%20contexts%20to%20gain%20from%20the%20system.%20For%0Aexample%2C%20in%20adaptive%20clinical%20trials%2C%20where%20hospitals%20sequentially%20recruit%0Avolunteers%20to%20test%20multiple%20new%20treatments%20and%20adjust%20plans%20based%20on%0Avolunteers%27%20reported%20profiles%20such%20as%20symptoms%20and%20interim%20data%2C%20participants%0Amay%20misreport%20severe%20side%20effects%20like%20allergy%20and%20nausea%20to%20avoid%20perceived%0Asuboptimal%20treatments.%20We%20are%20the%20first%20to%20study%20this%20issue%20of%20private%20context%0Amisreporting%20in%20a%20stochastic%20contextual%20bandit%20game%20between%20the%20system%20and%0Anon-repeated%20agents.%20We%20show%20that%20traditional%20low-regret%20algorithms%2C%20such%20as%0AUCB%20family%20algorithms%20and%20Thompson%20sampling%2C%20fail%20to%20ensure%20truthful%20reporting%0Aand%20can%20result%20in%20linear%20regret%20in%20the%20worst%20case%2C%20while%20traditional%20truthful%0Aalgorithms%20like%20explore-then-commit%20%28ETC%29%20and%20%24%5Cepsilon%24-greedy%20algorithm%20incur%0Asublinear%20but%20high%20regret.%20We%20propose%20a%20mechanism%20that%20uses%20a%20linear%20program%20to%0Aensure%20truthfulness%20while%20minimizing%20deviation%20from%20Thompson%20sampling%2C%20yielding%0Aan%20%24O%28%5Cln%20T%29%24%20frequentist%20regret.%20Our%20numerical%20experiments%20further%20demonstrate%0Astrong%20performance%20in%20multiple%20contexts%20and%20across%20other%20distribution%20families.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03865v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTruthful%2520mechanisms%2520for%2520linear%2520bandit%2520games%2520with%2520private%2520contexts%26entry.906535625%3DYiting%2520Hu%2520and%2520Lingjie%2520Duan%26entry.1292438233%3D%2520%2520The%2520contextual%2520bandit%2520problem%252C%2520where%2520agents%2520arrive%2520sequentially%2520with%2520personal%250Acontexts%2520and%2520the%2520system%2520adapts%2520its%2520arm%2520allocation%2520decisions%2520accordingly%252C%2520has%250Arecently%2520garnered%2520increasing%2520attention%2520for%2520enabling%2520more%2520personalized%2520outcomes.%250AHowever%252C%2520in%2520many%2520healthcare%2520and%2520recommendation%2520applications%252C%2520agents%2520have%250Aprivate%2520profiles%2520and%2520may%2520misreport%2520their%2520contexts%2520to%2520gain%2520from%2520the%2520system.%2520For%250Aexample%252C%2520in%2520adaptive%2520clinical%2520trials%252C%2520where%2520hospitals%2520sequentially%2520recruit%250Avolunteers%2520to%2520test%2520multiple%2520new%2520treatments%2520and%2520adjust%2520plans%2520based%2520on%250Avolunteers%2527%2520reported%2520profiles%2520such%2520as%2520symptoms%2520and%2520interim%2520data%252C%2520participants%250Amay%2520misreport%2520severe%2520side%2520effects%2520like%2520allergy%2520and%2520nausea%2520to%2520avoid%2520perceived%250Asuboptimal%2520treatments.%2520We%2520are%2520the%2520first%2520to%2520study%2520this%2520issue%2520of%2520private%2520context%250Amisreporting%2520in%2520a%2520stochastic%2520contextual%2520bandit%2520game%2520between%2520the%2520system%2520and%250Anon-repeated%2520agents.%2520We%2520show%2520that%2520traditional%2520low-regret%2520algorithms%252C%2520such%2520as%250AUCB%2520family%2520algorithms%2520and%2520Thompson%2520sampling%252C%2520fail%2520to%2520ensure%2520truthful%2520reporting%250Aand%2520can%2520result%2520in%2520linear%2520regret%2520in%2520the%2520worst%2520case%252C%2520while%2520traditional%2520truthful%250Aalgorithms%2520like%2520explore-then-commit%2520%2528ETC%2529%2520and%2520%2524%255Cepsilon%2524-greedy%2520algorithm%2520incur%250Asublinear%2520but%2520high%2520regret.%2520We%2520propose%2520a%2520mechanism%2520that%2520uses%2520a%2520linear%2520program%2520to%250Aensure%2520truthfulness%2520while%2520minimizing%2520deviation%2520from%2520Thompson%2520sampling%252C%2520yielding%250Aan%2520%2524O%2528%255Cln%2520T%2529%2524%2520frequentist%2520regret.%2520Our%2520numerical%2520experiments%2520further%2520demonstrate%250Astrong%2520performance%2520in%2520multiple%2520contexts%2520and%2520across%2520other%2520distribution%2520families.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03865v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Truthful%20mechanisms%20for%20linear%20bandit%20games%20with%20private%20contexts&entry.906535625=Yiting%20Hu%20and%20Lingjie%20Duan&entry.1292438233=%20%20The%20contextual%20bandit%20problem%2C%20where%20agents%20arrive%20sequentially%20with%20personal%0Acontexts%20and%20the%20system%20adapts%20its%20arm%20allocation%20decisions%20accordingly%2C%20has%0Arecently%20garnered%20increasing%20attention%20for%20enabling%20more%20personalized%20outcomes.%0AHowever%2C%20in%20many%20healthcare%20and%20recommendation%20applications%2C%20agents%20have%0Aprivate%20profiles%20and%20may%20misreport%20their%20contexts%20to%20gain%20from%20the%20system.%20For%0Aexample%2C%20in%20adaptive%20clinical%20trials%2C%20where%20hospitals%20sequentially%20recruit%0Avolunteers%20to%20test%20multiple%20new%20treatments%20and%20adjust%20plans%20based%20on%0Avolunteers%27%20reported%20profiles%20such%20as%20symptoms%20and%20interim%20data%2C%20participants%0Amay%20misreport%20severe%20side%20effects%20like%20allergy%20and%20nausea%20to%20avoid%20perceived%0Asuboptimal%20treatments.%20We%20are%20the%20first%20to%20study%20this%20issue%20of%20private%20context%0Amisreporting%20in%20a%20stochastic%20contextual%20bandit%20game%20between%20the%20system%20and%0Anon-repeated%20agents.%20We%20show%20that%20traditional%20low-regret%20algorithms%2C%20such%20as%0AUCB%20family%20algorithms%20and%20Thompson%20sampling%2C%20fail%20to%20ensure%20truthful%20reporting%0Aand%20can%20result%20in%20linear%20regret%20in%20the%20worst%20case%2C%20while%20traditional%20truthful%0Aalgorithms%20like%20explore-then-commit%20%28ETC%29%20and%20%24%5Cepsilon%24-greedy%20algorithm%20incur%0Asublinear%20but%20high%20regret.%20We%20propose%20a%20mechanism%20that%20uses%20a%20linear%20program%20to%0Aensure%20truthfulness%20while%20minimizing%20deviation%20from%20Thompson%20sampling%2C%20yielding%0Aan%20%24O%28%5Cln%20T%29%24%20frequentist%20regret.%20Our%20numerical%20experiments%20further%20demonstrate%0Astrong%20performance%20in%20multiple%20contexts%20and%20across%20other%20distribution%20families.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03865v2&entry.124074799=Read"},
{"title": "Energy-Efficient Autonomous Aerial Navigation with Dynamic Vision\n  Sensors: A Physics-Guided Neuromorphic Approach", "author": "Sourav Sanyal and Amogh Joshi and Manish Nagaraj and Rohan Kumar Manna and Kaushik Roy", "abstract": "  Vision-based object tracking is a critical component for achieving autonomous\naerial navigation, particularly for obstacle avoidance. Neuromorphic Dynamic\nVision Sensors (DVS) or event cameras, inspired by biological vision, offer a\npromising alternative to conventional frame-based cameras. These cameras can\ndetect changes in intensity asynchronously, even in challenging lighting\nconditions, with a high dynamic range and resistance to motion blur. Spiking\nneural networks (SNNs) are increasingly used to process these event-based\nsignals efficiently and asynchronously. Meanwhile, physics-based artificial\nintelligence (AI) provides a means to incorporate system-level knowledge into\nneural networks via physical modeling. This enhances robustness, energy\nefficiency, and provides symbolic explainability. In this work, we present a\nneuromorphic navigation framework for autonomous drone navigation. The focus is\non detecting and navigating through moving gates while avoiding collisions. We\nuse event cameras for detecting moving objects through a shallow SNN\narchitecture in an unsupervised manner. This is combined with a lightweight\nenergy-aware physics-guided neural network (PgNN) trained with depth inputs to\npredict optimal flight times, generating near-minimum energy paths. The system\nis implemented in the Gazebo simulator and integrates a sensor-fused\nvision-to-planning neuro-symbolic framework built with the Robot Operating\nSystem (ROS) middleware. This work highlights the future potential of\nintegrating event-based vision with physics-guided planning for\nenergy-efficient autonomous navigation, particularly for low-latency\ndecision-making.\n", "link": "http://arxiv.org/abs/2502.05938v2", "date": "2025-04-23", "relevancy": 1.782, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6035}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6005}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Efficient%20Autonomous%20Aerial%20Navigation%20with%20Dynamic%20Vision%0A%20%20Sensors%3A%20A%20Physics-Guided%20Neuromorphic%20Approach&body=Title%3A%20Energy-Efficient%20Autonomous%20Aerial%20Navigation%20with%20Dynamic%20Vision%0A%20%20Sensors%3A%20A%20Physics-Guided%20Neuromorphic%20Approach%0AAuthor%3A%20Sourav%20Sanyal%20and%20Amogh%20Joshi%20and%20Manish%20Nagaraj%20and%20Rohan%20Kumar%20Manna%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Vision-based%20object%20tracking%20is%20a%20critical%20component%20for%20achieving%20autonomous%0Aaerial%20navigation%2C%20particularly%20for%20obstacle%20avoidance.%20Neuromorphic%20Dynamic%0AVision%20Sensors%20%28DVS%29%20or%20event%20cameras%2C%20inspired%20by%20biological%20vision%2C%20offer%20a%0Apromising%20alternative%20to%20conventional%20frame-based%20cameras.%20These%20cameras%20can%0Adetect%20changes%20in%20intensity%20asynchronously%2C%20even%20in%20challenging%20lighting%0Aconditions%2C%20with%20a%20high%20dynamic%20range%20and%20resistance%20to%20motion%20blur.%20Spiking%0Aneural%20networks%20%28SNNs%29%20are%20increasingly%20used%20to%20process%20these%20event-based%0Asignals%20efficiently%20and%20asynchronously.%20Meanwhile%2C%20physics-based%20artificial%0Aintelligence%20%28AI%29%20provides%20a%20means%20to%20incorporate%20system-level%20knowledge%20into%0Aneural%20networks%20via%20physical%20modeling.%20This%20enhances%20robustness%2C%20energy%0Aefficiency%2C%20and%20provides%20symbolic%20explainability.%20In%20this%20work%2C%20we%20present%20a%0Aneuromorphic%20navigation%20framework%20for%20autonomous%20drone%20navigation.%20The%20focus%20is%0Aon%20detecting%20and%20navigating%20through%20moving%20gates%20while%20avoiding%20collisions.%20We%0Ause%20event%20cameras%20for%20detecting%20moving%20objects%20through%20a%20shallow%20SNN%0Aarchitecture%20in%20an%20unsupervised%20manner.%20This%20is%20combined%20with%20a%20lightweight%0Aenergy-aware%20physics-guided%20neural%20network%20%28PgNN%29%20trained%20with%20depth%20inputs%20to%0Apredict%20optimal%20flight%20times%2C%20generating%20near-minimum%20energy%20paths.%20The%20system%0Ais%20implemented%20in%20the%20Gazebo%20simulator%20and%20integrates%20a%20sensor-fused%0Avision-to-planning%20neuro-symbolic%20framework%20built%20with%20the%20Robot%20Operating%0ASystem%20%28ROS%29%20middleware.%20This%20work%20highlights%20the%20future%20potential%20of%0Aintegrating%20event-based%20vision%20with%20physics-guided%20planning%20for%0Aenergy-efficient%20autonomous%20navigation%2C%20particularly%20for%20low-latency%0Adecision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05938v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Efficient%2520Autonomous%2520Aerial%2520Navigation%2520with%2520Dynamic%2520Vision%250A%2520%2520Sensors%253A%2520A%2520Physics-Guided%2520Neuromorphic%2520Approach%26entry.906535625%3DSourav%2520Sanyal%2520and%2520Amogh%2520Joshi%2520and%2520Manish%2520Nagaraj%2520and%2520Rohan%2520Kumar%2520Manna%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520Vision-based%2520object%2520tracking%2520is%2520a%2520critical%2520component%2520for%2520achieving%2520autonomous%250Aaerial%2520navigation%252C%2520particularly%2520for%2520obstacle%2520avoidance.%2520Neuromorphic%2520Dynamic%250AVision%2520Sensors%2520%2528DVS%2529%2520or%2520event%2520cameras%252C%2520inspired%2520by%2520biological%2520vision%252C%2520offer%2520a%250Apromising%2520alternative%2520to%2520conventional%2520frame-based%2520cameras.%2520These%2520cameras%2520can%250Adetect%2520changes%2520in%2520intensity%2520asynchronously%252C%2520even%2520in%2520challenging%2520lighting%250Aconditions%252C%2520with%2520a%2520high%2520dynamic%2520range%2520and%2520resistance%2520to%2520motion%2520blur.%2520Spiking%250Aneural%2520networks%2520%2528SNNs%2529%2520are%2520increasingly%2520used%2520to%2520process%2520these%2520event-based%250Asignals%2520efficiently%2520and%2520asynchronously.%2520Meanwhile%252C%2520physics-based%2520artificial%250Aintelligence%2520%2528AI%2529%2520provides%2520a%2520means%2520to%2520incorporate%2520system-level%2520knowledge%2520into%250Aneural%2520networks%2520via%2520physical%2520modeling.%2520This%2520enhances%2520robustness%252C%2520energy%250Aefficiency%252C%2520and%2520provides%2520symbolic%2520explainability.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Aneuromorphic%2520navigation%2520framework%2520for%2520autonomous%2520drone%2520navigation.%2520The%2520focus%2520is%250Aon%2520detecting%2520and%2520navigating%2520through%2520moving%2520gates%2520while%2520avoiding%2520collisions.%2520We%250Ause%2520event%2520cameras%2520for%2520detecting%2520moving%2520objects%2520through%2520a%2520shallow%2520SNN%250Aarchitecture%2520in%2520an%2520unsupervised%2520manner.%2520This%2520is%2520combined%2520with%2520a%2520lightweight%250Aenergy-aware%2520physics-guided%2520neural%2520network%2520%2528PgNN%2529%2520trained%2520with%2520depth%2520inputs%2520to%250Apredict%2520optimal%2520flight%2520times%252C%2520generating%2520near-minimum%2520energy%2520paths.%2520The%2520system%250Ais%2520implemented%2520in%2520the%2520Gazebo%2520simulator%2520and%2520integrates%2520a%2520sensor-fused%250Avision-to-planning%2520neuro-symbolic%2520framework%2520built%2520with%2520the%2520Robot%2520Operating%250ASystem%2520%2528ROS%2529%2520middleware.%2520This%2520work%2520highlights%2520the%2520future%2520potential%2520of%250Aintegrating%2520event-based%2520vision%2520with%2520physics-guided%2520planning%2520for%250Aenergy-efficient%2520autonomous%2520navigation%252C%2520particularly%2520for%2520low-latency%250Adecision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05938v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Efficient%20Autonomous%20Aerial%20Navigation%20with%20Dynamic%20Vision%0A%20%20Sensors%3A%20A%20Physics-Guided%20Neuromorphic%20Approach&entry.906535625=Sourav%20Sanyal%20and%20Amogh%20Joshi%20and%20Manish%20Nagaraj%20and%20Rohan%20Kumar%20Manna%20and%20Kaushik%20Roy&entry.1292438233=%20%20Vision-based%20object%20tracking%20is%20a%20critical%20component%20for%20achieving%20autonomous%0Aaerial%20navigation%2C%20particularly%20for%20obstacle%20avoidance.%20Neuromorphic%20Dynamic%0AVision%20Sensors%20%28DVS%29%20or%20event%20cameras%2C%20inspired%20by%20biological%20vision%2C%20offer%20a%0Apromising%20alternative%20to%20conventional%20frame-based%20cameras.%20These%20cameras%20can%0Adetect%20changes%20in%20intensity%20asynchronously%2C%20even%20in%20challenging%20lighting%0Aconditions%2C%20with%20a%20high%20dynamic%20range%20and%20resistance%20to%20motion%20blur.%20Spiking%0Aneural%20networks%20%28SNNs%29%20are%20increasingly%20used%20to%20process%20these%20event-based%0Asignals%20efficiently%20and%20asynchronously.%20Meanwhile%2C%20physics-based%20artificial%0Aintelligence%20%28AI%29%20provides%20a%20means%20to%20incorporate%20system-level%20knowledge%20into%0Aneural%20networks%20via%20physical%20modeling.%20This%20enhances%20robustness%2C%20energy%0Aefficiency%2C%20and%20provides%20symbolic%20explainability.%20In%20this%20work%2C%20we%20present%20a%0Aneuromorphic%20navigation%20framework%20for%20autonomous%20drone%20navigation.%20The%20focus%20is%0Aon%20detecting%20and%20navigating%20through%20moving%20gates%20while%20avoiding%20collisions.%20We%0Ause%20event%20cameras%20for%20detecting%20moving%20objects%20through%20a%20shallow%20SNN%0Aarchitecture%20in%20an%20unsupervised%20manner.%20This%20is%20combined%20with%20a%20lightweight%0Aenergy-aware%20physics-guided%20neural%20network%20%28PgNN%29%20trained%20with%20depth%20inputs%20to%0Apredict%20optimal%20flight%20times%2C%20generating%20near-minimum%20energy%20paths.%20The%20system%0Ais%20implemented%20in%20the%20Gazebo%20simulator%20and%20integrates%20a%20sensor-fused%0Avision-to-planning%20neuro-symbolic%20framework%20built%20with%20the%20Robot%20Operating%0ASystem%20%28ROS%29%20middleware.%20This%20work%20highlights%20the%20future%20potential%20of%0Aintegrating%20event-based%20vision%20with%20physics-guided%20planning%20for%0Aenergy-efficient%20autonomous%20navigation%2C%20particularly%20for%20low-latency%0Adecision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05938v2&entry.124074799=Read"},
{"title": "AudioX: Diffusion Transformer for Anything-to-Audio Generation", "author": "Zeyue Tian and Yizhu Jin and Zhaoyang Liu and Ruibin Yuan and Xu Tan and Qifeng Chen and Wei Xue and Yike Guo", "abstract": "  Audio and music generation have emerged as crucial tasks in many\napplications, yet existing approaches face significant limitations: they\noperate in isolation without unified capabilities across modalities, suffer\nfrom scarce high-quality, multi-modal training data, and struggle to\neffectively integrate diverse inputs. In this work, we propose AudioX, a\nunified Diffusion Transformer model for Anything-to-Audio and Music Generation.\nUnlike previous domain-specific models, AudioX can generate both general audio\nand music with high quality, while offering flexible natural language control\nand seamless processing of various modalities including text, video, image,\nmusic, and audio. Its key innovation is a multi-modal masked training strategy\nthat masks inputs across modalities and forces the model to learn from masked\ninputs, yielding robust and unified cross-modal representations. To address\ndata scarcity, we curate two comprehensive datasets: vggsound-caps with 190K\naudio captions based on the VGGSound dataset, and V2M-caps with 6 million music\ncaptions derived from the V2M dataset. Extensive experiments demonstrate that\nAudioX not only matches or outperforms state-of-the-art specialized models, but\nalso offers remarkable versatility in handling diverse input modalities and\ngeneration tasks within a unified architecture. The code and datasets will be\navailable at https://zeyuet.github.io/AudioX/\n", "link": "http://arxiv.org/abs/2503.10522v2", "date": "2025-04-23", "relevancy": 1.7632, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5947}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5912}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioX%3A%20Diffusion%20Transformer%20for%20Anything-to-Audio%20Generation&body=Title%3A%20AudioX%3A%20Diffusion%20Transformer%20for%20Anything-to-Audio%20Generation%0AAuthor%3A%20Zeyue%20Tian%20and%20Yizhu%20Jin%20and%20Zhaoyang%20Liu%20and%20Ruibin%20Yuan%20and%20Xu%20Tan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Yike%20Guo%0AAbstract%3A%20%20%20Audio%20and%20music%20generation%20have%20emerged%20as%20crucial%20tasks%20in%20many%0Aapplications%2C%20yet%20existing%20approaches%20face%20significant%20limitations%3A%20they%0Aoperate%20in%20isolation%20without%20unified%20capabilities%20across%20modalities%2C%20suffer%0Afrom%20scarce%20high-quality%2C%20multi-modal%20training%20data%2C%20and%20struggle%20to%0Aeffectively%20integrate%20diverse%20inputs.%20In%20this%20work%2C%20we%20propose%20AudioX%2C%20a%0Aunified%20Diffusion%20Transformer%20model%20for%20Anything-to-Audio%20and%20Music%20Generation.%0AUnlike%20previous%20domain-specific%20models%2C%20AudioX%20can%20generate%20both%20general%20audio%0Aand%20music%20with%20high%20quality%2C%20while%20offering%20flexible%20natural%20language%20control%0Aand%20seamless%20processing%20of%20various%20modalities%20including%20text%2C%20video%2C%20image%2C%0Amusic%2C%20and%20audio.%20Its%20key%20innovation%20is%20a%20multi-modal%20masked%20training%20strategy%0Athat%20masks%20inputs%20across%20modalities%20and%20forces%20the%20model%20to%20learn%20from%20masked%0Ainputs%2C%20yielding%20robust%20and%20unified%20cross-modal%20representations.%20To%20address%0Adata%20scarcity%2C%20we%20curate%20two%20comprehensive%20datasets%3A%20vggsound-caps%20with%20190K%0Aaudio%20captions%20based%20on%20the%20VGGSound%20dataset%2C%20and%20V2M-caps%20with%206%20million%20music%0Acaptions%20derived%20from%20the%20V2M%20dataset.%20Extensive%20experiments%20demonstrate%20that%0AAudioX%20not%20only%20matches%20or%20outperforms%20state-of-the-art%20specialized%20models%2C%20but%0Aalso%20offers%20remarkable%20versatility%20in%20handling%20diverse%20input%20modalities%20and%0Ageneration%20tasks%20within%20a%20unified%20architecture.%20The%20code%20and%20datasets%20will%20be%0Aavailable%20at%20https%3A//zeyuet.github.io/AudioX/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioX%253A%2520Diffusion%2520Transformer%2520for%2520Anything-to-Audio%2520Generation%26entry.906535625%3DZeyue%2520Tian%2520and%2520Yizhu%2520Jin%2520and%2520Zhaoyang%2520Liu%2520and%2520Ruibin%2520Yuan%2520and%2520Xu%2520Tan%2520and%2520Qifeng%2520Chen%2520and%2520Wei%2520Xue%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520Audio%2520and%2520music%2520generation%2520have%2520emerged%2520as%2520crucial%2520tasks%2520in%2520many%250Aapplications%252C%2520yet%2520existing%2520approaches%2520face%2520significant%2520limitations%253A%2520they%250Aoperate%2520in%2520isolation%2520without%2520unified%2520capabilities%2520across%2520modalities%252C%2520suffer%250Afrom%2520scarce%2520high-quality%252C%2520multi-modal%2520training%2520data%252C%2520and%2520struggle%2520to%250Aeffectively%2520integrate%2520diverse%2520inputs.%2520In%2520this%2520work%252C%2520we%2520propose%2520AudioX%252C%2520a%250Aunified%2520Diffusion%2520Transformer%2520model%2520for%2520Anything-to-Audio%2520and%2520Music%2520Generation.%250AUnlike%2520previous%2520domain-specific%2520models%252C%2520AudioX%2520can%2520generate%2520both%2520general%2520audio%250Aand%2520music%2520with%2520high%2520quality%252C%2520while%2520offering%2520flexible%2520natural%2520language%2520control%250Aand%2520seamless%2520processing%2520of%2520various%2520modalities%2520including%2520text%252C%2520video%252C%2520image%252C%250Amusic%252C%2520and%2520audio.%2520Its%2520key%2520innovation%2520is%2520a%2520multi-modal%2520masked%2520training%2520strategy%250Athat%2520masks%2520inputs%2520across%2520modalities%2520and%2520forces%2520the%2520model%2520to%2520learn%2520from%2520masked%250Ainputs%252C%2520yielding%2520robust%2520and%2520unified%2520cross-modal%2520representations.%2520To%2520address%250Adata%2520scarcity%252C%2520we%2520curate%2520two%2520comprehensive%2520datasets%253A%2520vggsound-caps%2520with%2520190K%250Aaudio%2520captions%2520based%2520on%2520the%2520VGGSound%2520dataset%252C%2520and%2520V2M-caps%2520with%25206%2520million%2520music%250Acaptions%2520derived%2520from%2520the%2520V2M%2520dataset.%2520Extensive%2520experiments%2520demonstrate%2520that%250AAudioX%2520not%2520only%2520matches%2520or%2520outperforms%2520state-of-the-art%2520specialized%2520models%252C%2520but%250Aalso%2520offers%2520remarkable%2520versatility%2520in%2520handling%2520diverse%2520input%2520modalities%2520and%250Ageneration%2520tasks%2520within%2520a%2520unified%2520architecture.%2520The%2520code%2520and%2520datasets%2520will%2520be%250Aavailable%2520at%2520https%253A//zeyuet.github.io/AudioX/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioX%3A%20Diffusion%20Transformer%20for%20Anything-to-Audio%20Generation&entry.906535625=Zeyue%20Tian%20and%20Yizhu%20Jin%20and%20Zhaoyang%20Liu%20and%20Ruibin%20Yuan%20and%20Xu%20Tan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Yike%20Guo&entry.1292438233=%20%20Audio%20and%20music%20generation%20have%20emerged%20as%20crucial%20tasks%20in%20many%0Aapplications%2C%20yet%20existing%20approaches%20face%20significant%20limitations%3A%20they%0Aoperate%20in%20isolation%20without%20unified%20capabilities%20across%20modalities%2C%20suffer%0Afrom%20scarce%20high-quality%2C%20multi-modal%20training%20data%2C%20and%20struggle%20to%0Aeffectively%20integrate%20diverse%20inputs.%20In%20this%20work%2C%20we%20propose%20AudioX%2C%20a%0Aunified%20Diffusion%20Transformer%20model%20for%20Anything-to-Audio%20and%20Music%20Generation.%0AUnlike%20previous%20domain-specific%20models%2C%20AudioX%20can%20generate%20both%20general%20audio%0Aand%20music%20with%20high%20quality%2C%20while%20offering%20flexible%20natural%20language%20control%0Aand%20seamless%20processing%20of%20various%20modalities%20including%20text%2C%20video%2C%20image%2C%0Amusic%2C%20and%20audio.%20Its%20key%20innovation%20is%20a%20multi-modal%20masked%20training%20strategy%0Athat%20masks%20inputs%20across%20modalities%20and%20forces%20the%20model%20to%20learn%20from%20masked%0Ainputs%2C%20yielding%20robust%20and%20unified%20cross-modal%20representations.%20To%20address%0Adata%20scarcity%2C%20we%20curate%20two%20comprehensive%20datasets%3A%20vggsound-caps%20with%20190K%0Aaudio%20captions%20based%20on%20the%20VGGSound%20dataset%2C%20and%20V2M-caps%20with%206%20million%20music%0Acaptions%20derived%20from%20the%20V2M%20dataset.%20Extensive%20experiments%20demonstrate%20that%0AAudioX%20not%20only%20matches%20or%20outperforms%20state-of-the-art%20specialized%20models%2C%20but%0Aalso%20offers%20remarkable%20versatility%20in%20handling%20diverse%20input%20modalities%20and%0Ageneration%20tasks%20within%20a%20unified%20architecture.%20The%20code%20and%20datasets%20will%20be%0Aavailable%20at%20https%3A//zeyuet.github.io/AudioX/%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10522v2&entry.124074799=Read"},
{"title": "Robotic World Model: A Neural Network Simulator for Robust Policy\n  Optimization in Robotics", "author": "Chenhao Li and Andreas Krause and Marco Hutter", "abstract": "  Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. This work advances model-based reinforcement learning by\naddressing the challenges of long-horizon prediction, error accumulation, and\nsim-to-real transfer. By providing a scalable and robust framework, the\nintroduced methods pave the way for adaptive and efficient robotic systems in\nreal-world applications.\n", "link": "http://arxiv.org/abs/2501.10100v2", "date": "2025-04-23", "relevancy": 1.7339, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6083}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5928}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20World%20Model%3A%20A%20Neural%20Network%20Simulator%20for%20Robust%20Policy%0A%20%20Optimization%20in%20Robotics&body=Title%3A%20Robotic%20World%20Model%3A%20A%20Neural%20Network%20Simulator%20for%20Robust%20Policy%0A%20%20Optimization%20in%20Robotics%0AAuthor%3A%20Chenhao%20Li%20and%20Andreas%20Krause%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Learning%20robust%20and%20generalizable%20world%20models%20is%20crucial%20for%20enabling%0Aefficient%20and%20scalable%20robotic%20control%20in%20real-world%20environments.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20framework%20for%20learning%20world%20models%20that%20accurately%0Acapture%20complex%2C%20partially%20observable%2C%20and%20stochastic%20dynamics.%20The%20proposed%0Amethod%20employs%20a%20dual-autoregressive%20mechanism%20and%20self-supervised%20training%20to%0Aachieve%20reliable%20long-horizon%20predictions%20without%20relying%20on%20domain-specific%0Ainductive%20biases%2C%20ensuring%20adaptability%20across%20diverse%20robotic%20tasks.%20We%0Afurther%20propose%20a%20policy%20optimization%20framework%20that%20leverages%20world%20models%20for%0Aefficient%20training%20in%20imagined%20environments%20and%20seamless%20deployment%20in%0Areal-world%20systems.%20This%20work%20advances%20model-based%20reinforcement%20learning%20by%0Aaddressing%20the%20challenges%20of%20long-horizon%20prediction%2C%20error%20accumulation%2C%20and%0Asim-to-real%20transfer.%20By%20providing%20a%20scalable%20and%20robust%20framework%2C%20the%0Aintroduced%20methods%20pave%20the%20way%20for%20adaptive%20and%20efficient%20robotic%20systems%20in%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520World%2520Model%253A%2520A%2520Neural%2520Network%2520Simulator%2520for%2520Robust%2520Policy%250A%2520%2520Optimization%2520in%2520Robotics%26entry.906535625%3DChenhao%2520Li%2520and%2520Andreas%2520Krause%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Learning%2520robust%2520and%2520generalizable%2520world%2520models%2520is%2520crucial%2520for%2520enabling%250Aefficient%2520and%2520scalable%2520robotic%2520control%2520in%2520real-world%2520environments.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520for%2520learning%2520world%2520models%2520that%2520accurately%250Acapture%2520complex%252C%2520partially%2520observable%252C%2520and%2520stochastic%2520dynamics.%2520The%2520proposed%250Amethod%2520employs%2520a%2520dual-autoregressive%2520mechanism%2520and%2520self-supervised%2520training%2520to%250Aachieve%2520reliable%2520long-horizon%2520predictions%2520without%2520relying%2520on%2520domain-specific%250Ainductive%2520biases%252C%2520ensuring%2520adaptability%2520across%2520diverse%2520robotic%2520tasks.%2520We%250Afurther%2520propose%2520a%2520policy%2520optimization%2520framework%2520that%2520leverages%2520world%2520models%2520for%250Aefficient%2520training%2520in%2520imagined%2520environments%2520and%2520seamless%2520deployment%2520in%250Areal-world%2520systems.%2520This%2520work%2520advances%2520model-based%2520reinforcement%2520learning%2520by%250Aaddressing%2520the%2520challenges%2520of%2520long-horizon%2520prediction%252C%2520error%2520accumulation%252C%2520and%250Asim-to-real%2520transfer.%2520By%2520providing%2520a%2520scalable%2520and%2520robust%2520framework%252C%2520the%250Aintroduced%2520methods%2520pave%2520the%2520way%2520for%2520adaptive%2520and%2520efficient%2520robotic%2520systems%2520in%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20World%20Model%3A%20A%20Neural%20Network%20Simulator%20for%20Robust%20Policy%0A%20%20Optimization%20in%20Robotics&entry.906535625=Chenhao%20Li%20and%20Andreas%20Krause%20and%20Marco%20Hutter&entry.1292438233=%20%20Learning%20robust%20and%20generalizable%20world%20models%20is%20crucial%20for%20enabling%0Aefficient%20and%20scalable%20robotic%20control%20in%20real-world%20environments.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20framework%20for%20learning%20world%20models%20that%20accurately%0Acapture%20complex%2C%20partially%20observable%2C%20and%20stochastic%20dynamics.%20The%20proposed%0Amethod%20employs%20a%20dual-autoregressive%20mechanism%20and%20self-supervised%20training%20to%0Aachieve%20reliable%20long-horizon%20predictions%20without%20relying%20on%20domain-specific%0Ainductive%20biases%2C%20ensuring%20adaptability%20across%20diverse%20robotic%20tasks.%20We%0Afurther%20propose%20a%20policy%20optimization%20framework%20that%20leverages%20world%20models%20for%0Aefficient%20training%20in%20imagined%20environments%20and%20seamless%20deployment%20in%0Areal-world%20systems.%20This%20work%20advances%20model-based%20reinforcement%20learning%20by%0Aaddressing%20the%20challenges%20of%20long-horizon%20prediction%2C%20error%20accumulation%2C%20and%0Asim-to-real%20transfer.%20By%20providing%20a%20scalable%20and%20robust%20framework%2C%20the%0Aintroduced%20methods%20pave%20the%20way%20for%20adaptive%20and%20efficient%20robotic%20systems%20in%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10100v2&entry.124074799=Read"},
{"title": "Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation", "author": "Tixiao Shan and Abhinav Rajvanshi and Niluthpol Mithun and Han-Pang Chiu", "abstract": "  We propose Graph2Nav, a real-time 3D object-relation graph generation\nframework, for autonomous navigation in the real world. Our framework fully\ngenerates and exploits both 3D objects and a rich set of semantic relationships\namong objects in a 3D layered scene graph, which is applicable to both indoor\nand outdoor scenes. It learns to generate 3D semantic relations among objects,\nby leveraging and advancing state-of-the-art 2D panoptic scene graph works into\nthe 3D world via 3D semantic mapping techniques. This approach avoids previous\ntraining data constraints in learning 3D scene graphs directly from 3D data. We\nconduct experiments to validate the accuracy in locating 3D objects and\nlabeling object-relations in our 3D scene graphs. We also evaluate the impact\nof Graph2Nav via integration with SayNav, a state-of-the-art planner based on\nlarge language models, on an unmanned ground robot to object search tasks in\nreal environments. Our results demonstrate that modeling object relations in\nour scene graphs improves search efficiency in these navigation tasks.\n", "link": "http://arxiv.org/abs/2504.16782v1", "date": "2025-04-23", "relevancy": 1.727, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6211}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.571}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph2Nav%3A%203D%20Object-Relation%20Graph%20Generation%20to%20Robot%20Navigation&body=Title%3A%20Graph2Nav%3A%203D%20Object-Relation%20Graph%20Generation%20to%20Robot%20Navigation%0AAuthor%3A%20Tixiao%20Shan%20and%20Abhinav%20Rajvanshi%20and%20Niluthpol%20Mithun%20and%20Han-Pang%20Chiu%0AAbstract%3A%20%20%20We%20propose%20Graph2Nav%2C%20a%20real-time%203D%20object-relation%20graph%20generation%0Aframework%2C%20for%20autonomous%20navigation%20in%20the%20real%20world.%20Our%20framework%20fully%0Agenerates%20and%20exploits%20both%203D%20objects%20and%20a%20rich%20set%20of%20semantic%20relationships%0Aamong%20objects%20in%20a%203D%20layered%20scene%20graph%2C%20which%20is%20applicable%20to%20both%20indoor%0Aand%20outdoor%20scenes.%20It%20learns%20to%20generate%203D%20semantic%20relations%20among%20objects%2C%0Aby%20leveraging%20and%20advancing%20state-of-the-art%202D%20panoptic%20scene%20graph%20works%20into%0Athe%203D%20world%20via%203D%20semantic%20mapping%20techniques.%20This%20approach%20avoids%20previous%0Atraining%20data%20constraints%20in%20learning%203D%20scene%20graphs%20directly%20from%203D%20data.%20We%0Aconduct%20experiments%20to%20validate%20the%20accuracy%20in%20locating%203D%20objects%20and%0Alabeling%20object-relations%20in%20our%203D%20scene%20graphs.%20We%20also%20evaluate%20the%20impact%0Aof%20Graph2Nav%20via%20integration%20with%20SayNav%2C%20a%20state-of-the-art%20planner%20based%20on%0Alarge%20language%20models%2C%20on%20an%20unmanned%20ground%20robot%20to%20object%20search%20tasks%20in%0Areal%20environments.%20Our%20results%20demonstrate%20that%20modeling%20object%20relations%20in%0Aour%20scene%20graphs%20improves%20search%20efficiency%20in%20these%20navigation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph2Nav%253A%25203D%2520Object-Relation%2520Graph%2520Generation%2520to%2520Robot%2520Navigation%26entry.906535625%3DTixiao%2520Shan%2520and%2520Abhinav%2520Rajvanshi%2520and%2520Niluthpol%2520Mithun%2520and%2520Han-Pang%2520Chiu%26entry.1292438233%3D%2520%2520We%2520propose%2520Graph2Nav%252C%2520a%2520real-time%25203D%2520object-relation%2520graph%2520generation%250Aframework%252C%2520for%2520autonomous%2520navigation%2520in%2520the%2520real%2520world.%2520Our%2520framework%2520fully%250Agenerates%2520and%2520exploits%2520both%25203D%2520objects%2520and%2520a%2520rich%2520set%2520of%2520semantic%2520relationships%250Aamong%2520objects%2520in%2520a%25203D%2520layered%2520scene%2520graph%252C%2520which%2520is%2520applicable%2520to%2520both%2520indoor%250Aand%2520outdoor%2520scenes.%2520It%2520learns%2520to%2520generate%25203D%2520semantic%2520relations%2520among%2520objects%252C%250Aby%2520leveraging%2520and%2520advancing%2520state-of-the-art%25202D%2520panoptic%2520scene%2520graph%2520works%2520into%250Athe%25203D%2520world%2520via%25203D%2520semantic%2520mapping%2520techniques.%2520This%2520approach%2520avoids%2520previous%250Atraining%2520data%2520constraints%2520in%2520learning%25203D%2520scene%2520graphs%2520directly%2520from%25203D%2520data.%2520We%250Aconduct%2520experiments%2520to%2520validate%2520the%2520accuracy%2520in%2520locating%25203D%2520objects%2520and%250Alabeling%2520object-relations%2520in%2520our%25203D%2520scene%2520graphs.%2520We%2520also%2520evaluate%2520the%2520impact%250Aof%2520Graph2Nav%2520via%2520integration%2520with%2520SayNav%252C%2520a%2520state-of-the-art%2520planner%2520based%2520on%250Alarge%2520language%2520models%252C%2520on%2520an%2520unmanned%2520ground%2520robot%2520to%2520object%2520search%2520tasks%2520in%250Areal%2520environments.%2520Our%2520results%2520demonstrate%2520that%2520modeling%2520object%2520relations%2520in%250Aour%2520scene%2520graphs%2520improves%2520search%2520efficiency%2520in%2520these%2520navigation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph2Nav%3A%203D%20Object-Relation%20Graph%20Generation%20to%20Robot%20Navigation&entry.906535625=Tixiao%20Shan%20and%20Abhinav%20Rajvanshi%20and%20Niluthpol%20Mithun%20and%20Han-Pang%20Chiu&entry.1292438233=%20%20We%20propose%20Graph2Nav%2C%20a%20real-time%203D%20object-relation%20graph%20generation%0Aframework%2C%20for%20autonomous%20navigation%20in%20the%20real%20world.%20Our%20framework%20fully%0Agenerates%20and%20exploits%20both%203D%20objects%20and%20a%20rich%20set%20of%20semantic%20relationships%0Aamong%20objects%20in%20a%203D%20layered%20scene%20graph%2C%20which%20is%20applicable%20to%20both%20indoor%0Aand%20outdoor%20scenes.%20It%20learns%20to%20generate%203D%20semantic%20relations%20among%20objects%2C%0Aby%20leveraging%20and%20advancing%20state-of-the-art%202D%20panoptic%20scene%20graph%20works%20into%0Athe%203D%20world%20via%203D%20semantic%20mapping%20techniques.%20This%20approach%20avoids%20previous%0Atraining%20data%20constraints%20in%20learning%203D%20scene%20graphs%20directly%20from%203D%20data.%20We%0Aconduct%20experiments%20to%20validate%20the%20accuracy%20in%20locating%203D%20objects%20and%0Alabeling%20object-relations%20in%20our%203D%20scene%20graphs.%20We%20also%20evaluate%20the%20impact%0Aof%20Graph2Nav%20via%20integration%20with%20SayNav%2C%20a%20state-of-the-art%20planner%20based%20on%0Alarge%20language%20models%2C%20on%20an%20unmanned%20ground%20robot%20to%20object%20search%20tasks%20in%0Areal%20environments.%20Our%20results%20demonstrate%20that%20modeling%20object%20relations%20in%0Aour%20scene%20graphs%20improves%20search%20efficiency%20in%20these%20navigation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16782v1&entry.124074799=Read"},
{"title": "Frequency-Compensated Network for Daily Arctic Sea Ice Concentration\n  Prediction", "author": "Jialiang Zhang and Feng Gao and Yanhai Gan and Junyu Dong and Qian Du", "abstract": "  Accurately forecasting sea ice concentration (SIC) in the Arctic is critical\nto global ecosystem health and navigation safety. However, current methods\nstill is confronted with two challenges: 1) these methods rarely explore the\nlong-term feature dependencies in the frequency domain. 2) they can hardly\npreserve the high-frequency details, and the changes in the marginal area of\nthe sea ice cannot be accurately captured. To this end, we present a\nFrequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily\nbasis. In particular, we design a dual-branch network, including branches for\nfrequency feature extraction and convolutional feature extraction. For\nfrequency feature extraction, we design an adaptive frequency filter block,\nwhich integrates trainable layers with Fourier-based filters. By adding\nfrequency features, the FCNet can achieve refined prediction of edges and\ndetails. For convolutional feature extraction, we propose a high-frequency\nenhancement block to separate high and low-frequency information. Moreover,\nhigh-frequency features are enhanced via channel-wise attention, and temporal\nattention unit is employed for low-frequency feature extraction to capture\nlong-range sea ice changes. Extensive experiments are conducted on a\nsatellite-derived daily SIC dataset, and the results verify the effectiveness\nof the proposed FCNet. Our codes and data will be made public available at:\nhttps://github.com/oucailab/FCNet .\n", "link": "http://arxiv.org/abs/2504.16745v1", "date": "2025-04-23", "relevancy": 1.7211, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4311}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4308}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Compensated%20Network%20for%20Daily%20Arctic%20Sea%20Ice%20Concentration%0A%20%20Prediction&body=Title%3A%20Frequency-Compensated%20Network%20for%20Daily%20Arctic%20Sea%20Ice%20Concentration%0A%20%20Prediction%0AAuthor%3A%20Jialiang%20Zhang%20and%20Feng%20Gao%20and%20Yanhai%20Gan%20and%20Junyu%20Dong%20and%20Qian%20Du%0AAbstract%3A%20%20%20Accurately%20forecasting%20sea%20ice%20concentration%20%28SIC%29%20in%20the%20Arctic%20is%20critical%0Ato%20global%20ecosystem%20health%20and%20navigation%20safety.%20However%2C%20current%20methods%0Astill%20is%20confronted%20with%20two%20challenges%3A%201%29%20these%20methods%20rarely%20explore%20the%0Along-term%20feature%20dependencies%20in%20the%20frequency%20domain.%202%29%20they%20can%20hardly%0Apreserve%20the%20high-frequency%20details%2C%20and%20the%20changes%20in%20the%20marginal%20area%20of%0Athe%20sea%20ice%20cannot%20be%20accurately%20captured.%20To%20this%20end%2C%20we%20present%20a%0AFrequency-Compensated%20Network%20%28FCNet%29%20for%20Arctic%20SIC%20prediction%20on%20a%20daily%0Abasis.%20In%20particular%2C%20we%20design%20a%20dual-branch%20network%2C%20including%20branches%20for%0Afrequency%20feature%20extraction%20and%20convolutional%20feature%20extraction.%20For%0Afrequency%20feature%20extraction%2C%20we%20design%20an%20adaptive%20frequency%20filter%20block%2C%0Awhich%20integrates%20trainable%20layers%20with%20Fourier-based%20filters.%20By%20adding%0Afrequency%20features%2C%20the%20FCNet%20can%20achieve%20refined%20prediction%20of%20edges%20and%0Adetails.%20For%20convolutional%20feature%20extraction%2C%20we%20propose%20a%20high-frequency%0Aenhancement%20block%20to%20separate%20high%20and%20low-frequency%20information.%20Moreover%2C%0Ahigh-frequency%20features%20are%20enhanced%20via%20channel-wise%20attention%2C%20and%20temporal%0Aattention%20unit%20is%20employed%20for%20low-frequency%20feature%20extraction%20to%20capture%0Along-range%20sea%20ice%20changes.%20Extensive%20experiments%20are%20conducted%20on%20a%0Asatellite-derived%20daily%20SIC%20dataset%2C%20and%20the%20results%20verify%20the%20effectiveness%0Aof%20the%20proposed%20FCNet.%20Our%20codes%20and%20data%20will%20be%20made%20public%20available%20at%3A%0Ahttps%3A//github.com/oucailab/FCNet%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Compensated%2520Network%2520for%2520Daily%2520Arctic%2520Sea%2520Ice%2520Concentration%250A%2520%2520Prediction%26entry.906535625%3DJialiang%2520Zhang%2520and%2520Feng%2520Gao%2520and%2520Yanhai%2520Gan%2520and%2520Junyu%2520Dong%2520and%2520Qian%2520Du%26entry.1292438233%3D%2520%2520Accurately%2520forecasting%2520sea%2520ice%2520concentration%2520%2528SIC%2529%2520in%2520the%2520Arctic%2520is%2520critical%250Ato%2520global%2520ecosystem%2520health%2520and%2520navigation%2520safety.%2520However%252C%2520current%2520methods%250Astill%2520is%2520confronted%2520with%2520two%2520challenges%253A%25201%2529%2520these%2520methods%2520rarely%2520explore%2520the%250Along-term%2520feature%2520dependencies%2520in%2520the%2520frequency%2520domain.%25202%2529%2520they%2520can%2520hardly%250Apreserve%2520the%2520high-frequency%2520details%252C%2520and%2520the%2520changes%2520in%2520the%2520marginal%2520area%2520of%250Athe%2520sea%2520ice%2520cannot%2520be%2520accurately%2520captured.%2520To%2520this%2520end%252C%2520we%2520present%2520a%250AFrequency-Compensated%2520Network%2520%2528FCNet%2529%2520for%2520Arctic%2520SIC%2520prediction%2520on%2520a%2520daily%250Abasis.%2520In%2520particular%252C%2520we%2520design%2520a%2520dual-branch%2520network%252C%2520including%2520branches%2520for%250Afrequency%2520feature%2520extraction%2520and%2520convolutional%2520feature%2520extraction.%2520For%250Afrequency%2520feature%2520extraction%252C%2520we%2520design%2520an%2520adaptive%2520frequency%2520filter%2520block%252C%250Awhich%2520integrates%2520trainable%2520layers%2520with%2520Fourier-based%2520filters.%2520By%2520adding%250Afrequency%2520features%252C%2520the%2520FCNet%2520can%2520achieve%2520refined%2520prediction%2520of%2520edges%2520and%250Adetails.%2520For%2520convolutional%2520feature%2520extraction%252C%2520we%2520propose%2520a%2520high-frequency%250Aenhancement%2520block%2520to%2520separate%2520high%2520and%2520low-frequency%2520information.%2520Moreover%252C%250Ahigh-frequency%2520features%2520are%2520enhanced%2520via%2520channel-wise%2520attention%252C%2520and%2520temporal%250Aattention%2520unit%2520is%2520employed%2520for%2520low-frequency%2520feature%2520extraction%2520to%2520capture%250Along-range%2520sea%2520ice%2520changes.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520a%250Asatellite-derived%2520daily%2520SIC%2520dataset%252C%2520and%2520the%2520results%2520verify%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520FCNet.%2520Our%2520codes%2520and%2520data%2520will%2520be%2520made%2520public%2520available%2520at%253A%250Ahttps%253A//github.com/oucailab/FCNet%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Compensated%20Network%20for%20Daily%20Arctic%20Sea%20Ice%20Concentration%0A%20%20Prediction&entry.906535625=Jialiang%20Zhang%20and%20Feng%20Gao%20and%20Yanhai%20Gan%20and%20Junyu%20Dong%20and%20Qian%20Du&entry.1292438233=%20%20Accurately%20forecasting%20sea%20ice%20concentration%20%28SIC%29%20in%20the%20Arctic%20is%20critical%0Ato%20global%20ecosystem%20health%20and%20navigation%20safety.%20However%2C%20current%20methods%0Astill%20is%20confronted%20with%20two%20challenges%3A%201%29%20these%20methods%20rarely%20explore%20the%0Along-term%20feature%20dependencies%20in%20the%20frequency%20domain.%202%29%20they%20can%20hardly%0Apreserve%20the%20high-frequency%20details%2C%20and%20the%20changes%20in%20the%20marginal%20area%20of%0Athe%20sea%20ice%20cannot%20be%20accurately%20captured.%20To%20this%20end%2C%20we%20present%20a%0AFrequency-Compensated%20Network%20%28FCNet%29%20for%20Arctic%20SIC%20prediction%20on%20a%20daily%0Abasis.%20In%20particular%2C%20we%20design%20a%20dual-branch%20network%2C%20including%20branches%20for%0Afrequency%20feature%20extraction%20and%20convolutional%20feature%20extraction.%20For%0Afrequency%20feature%20extraction%2C%20we%20design%20an%20adaptive%20frequency%20filter%20block%2C%0Awhich%20integrates%20trainable%20layers%20with%20Fourier-based%20filters.%20By%20adding%0Afrequency%20features%2C%20the%20FCNet%20can%20achieve%20refined%20prediction%20of%20edges%20and%0Adetails.%20For%20convolutional%20feature%20extraction%2C%20we%20propose%20a%20high-frequency%0Aenhancement%20block%20to%20separate%20high%20and%20low-frequency%20information.%20Moreover%2C%0Ahigh-frequency%20features%20are%20enhanced%20via%20channel-wise%20attention%2C%20and%20temporal%0Aattention%20unit%20is%20employed%20for%20low-frequency%20feature%20extraction%20to%20capture%0Along-range%20sea%20ice%20changes.%20Extensive%20experiments%20are%20conducted%20on%20a%0Asatellite-derived%20daily%20SIC%20dataset%2C%20and%20the%20results%20verify%20the%20effectiveness%0Aof%20the%20proposed%20FCNet.%20Our%20codes%20and%20data%20will%20be%20made%20public%20available%20at%3A%0Ahttps%3A//github.com/oucailab/FCNet%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16745v1&entry.124074799=Read"},
{"title": "ChatDBG: Augmenting Debugging with Large Language Models", "author": "Kyla H. Levin and Nicolas van Kempen and Emery D. Berger and Stephen N. Freund", "abstract": "  Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like \"why is x null?\". To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. By leveraging the\nreal-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable\nonly through the use of domain-specific reasoning. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more\nthan 75,000 times.\n", "link": "http://arxiv.org/abs/2403.16354v4", "date": "2025-04-23", "relevancy": 1.7031, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatDBG%3A%20Augmenting%20Debugging%20with%20Large%20Language%20Models&body=Title%3A%20ChatDBG%3A%20Augmenting%20Debugging%20with%20Large%20Language%20Models%0AAuthor%3A%20Kyla%20H.%20Levin%20and%20Nicolas%20van%20Kempen%20and%20Emery%20D.%20Berger%20and%20Stephen%20N.%20Freund%0AAbstract%3A%20%20%20Debugging%20is%20a%20critical%20but%20challenging%20task%20for%20programmers.%20This%20paper%0Aproposes%20ChatDBG%2C%20an%20AI-powered%20debugging%20assistant.%20ChatDBG%20integrates%20large%0Alanguage%20models%20%28LLMs%29%20to%20significantly%20enhance%20the%20capabilities%20and%0Auser-friendliness%20of%20conventional%20debuggers.%20ChatDBG%20lets%20programmers%20engage%20in%0Aa%20collaborative%20dialogue%20with%20the%20debugger%2C%20allowing%20them%20to%20pose%20complex%0Aquestions%20about%20program%20state%2C%20perform%20root%20cause%20analysis%20for%20crashes%20or%0Aassertion%20failures%2C%20and%20explore%20open-ended%20queries%20like%20%22why%20is%20x%20null%3F%22.%20To%0Ahandle%20these%20queries%2C%20ChatDBG%20grants%20the%20LLM%20autonomy%20to%20%22take%20the%20wheel%22%3A%20it%0Acan%20act%20as%20an%20independent%20agent%20capable%20of%20querying%20and%20controlling%20the%0Adebugger%20to%20navigate%20through%20stacks%20and%20inspect%20program%20state.%20It%20then%20reports%0Aits%20findings%20and%20yields%20back%20control%20to%20the%20programmer.%20By%20leveraging%20the%0Areal-world%20knowledge%20embedded%20in%20LLMs%2C%20ChatDBG%20can%20diagnose%20issues%20identifiable%0Aonly%20through%20the%20use%20of%20domain-specific%20reasoning.%20Our%20ChatDBG%20prototype%0Aintegrates%20with%20standard%20debuggers%20including%20LLDB%20and%20GDB%20for%20native%20code%20and%0APdb%20for%20Python.%20Our%20evaluation%20across%20a%20diverse%20set%20of%20code%2C%20including%20C/C%2B%2B%0Acode%20with%20known%20bugs%20and%20a%20suite%20of%20Python%20code%20including%20standalone%20scripts%0Aand%20Jupyter%20notebooks%2C%20demonstrates%20that%20ChatDBG%20can%20successfully%20analyze%20root%0Acauses%2C%20explain%20bugs%2C%20and%20generate%20accurate%20fixes%20for%20a%20wide%20range%20of%0Areal-world%20errors.%20For%20the%20Python%20programs%2C%20a%20single%20query%20led%20to%20an%20actionable%0Abug%20fix%2067%25%20of%20the%20time%3B%20one%20additional%20follow-up%20query%20increased%20the%20success%0Arate%20to%2085%25.%20ChatDBG%20has%20seen%20rapid%20uptake%3B%20it%20has%20already%20been%20downloaded%20more%0Athan%2075%2C000%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16354v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatDBG%253A%2520Augmenting%2520Debugging%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DKyla%2520H.%2520Levin%2520and%2520Nicolas%2520van%2520Kempen%2520and%2520Emery%2520D.%2520Berger%2520and%2520Stephen%2520N.%2520Freund%26entry.1292438233%3D%2520%2520Debugging%2520is%2520a%2520critical%2520but%2520challenging%2520task%2520for%2520programmers.%2520This%2520paper%250Aproposes%2520ChatDBG%252C%2520an%2520AI-powered%2520debugging%2520assistant.%2520ChatDBG%2520integrates%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520significantly%2520enhance%2520the%2520capabilities%2520and%250Auser-friendliness%2520of%2520conventional%2520debuggers.%2520ChatDBG%2520lets%2520programmers%2520engage%2520in%250Aa%2520collaborative%2520dialogue%2520with%2520the%2520debugger%252C%2520allowing%2520them%2520to%2520pose%2520complex%250Aquestions%2520about%2520program%2520state%252C%2520perform%2520root%2520cause%2520analysis%2520for%2520crashes%2520or%250Aassertion%2520failures%252C%2520and%2520explore%2520open-ended%2520queries%2520like%2520%2522why%2520is%2520x%2520null%253F%2522.%2520To%250Ahandle%2520these%2520queries%252C%2520ChatDBG%2520grants%2520the%2520LLM%2520autonomy%2520to%2520%2522take%2520the%2520wheel%2522%253A%2520it%250Acan%2520act%2520as%2520an%2520independent%2520agent%2520capable%2520of%2520querying%2520and%2520controlling%2520the%250Adebugger%2520to%2520navigate%2520through%2520stacks%2520and%2520inspect%2520program%2520state.%2520It%2520then%2520reports%250Aits%2520findings%2520and%2520yields%2520back%2520control%2520to%2520the%2520programmer.%2520By%2520leveraging%2520the%250Areal-world%2520knowledge%2520embedded%2520in%2520LLMs%252C%2520ChatDBG%2520can%2520diagnose%2520issues%2520identifiable%250Aonly%2520through%2520the%2520use%2520of%2520domain-specific%2520reasoning.%2520Our%2520ChatDBG%2520prototype%250Aintegrates%2520with%2520standard%2520debuggers%2520including%2520LLDB%2520and%2520GDB%2520for%2520native%2520code%2520and%250APdb%2520for%2520Python.%2520Our%2520evaluation%2520across%2520a%2520diverse%2520set%2520of%2520code%252C%2520including%2520C/C%252B%252B%250Acode%2520with%2520known%2520bugs%2520and%2520a%2520suite%2520of%2520Python%2520code%2520including%2520standalone%2520scripts%250Aand%2520Jupyter%2520notebooks%252C%2520demonstrates%2520that%2520ChatDBG%2520can%2520successfully%2520analyze%2520root%250Acauses%252C%2520explain%2520bugs%252C%2520and%2520generate%2520accurate%2520fixes%2520for%2520a%2520wide%2520range%2520of%250Areal-world%2520errors.%2520For%2520the%2520Python%2520programs%252C%2520a%2520single%2520query%2520led%2520to%2520an%2520actionable%250Abug%2520fix%252067%2525%2520of%2520the%2520time%253B%2520one%2520additional%2520follow-up%2520query%2520increased%2520the%2520success%250Arate%2520to%252085%2525.%2520ChatDBG%2520has%2520seen%2520rapid%2520uptake%253B%2520it%2520has%2520already%2520been%2520downloaded%2520more%250Athan%252075%252C000%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16354v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatDBG%3A%20Augmenting%20Debugging%20with%20Large%20Language%20Models&entry.906535625=Kyla%20H.%20Levin%20and%20Nicolas%20van%20Kempen%20and%20Emery%20D.%20Berger%20and%20Stephen%20N.%20Freund&entry.1292438233=%20%20Debugging%20is%20a%20critical%20but%20challenging%20task%20for%20programmers.%20This%20paper%0Aproposes%20ChatDBG%2C%20an%20AI-powered%20debugging%20assistant.%20ChatDBG%20integrates%20large%0Alanguage%20models%20%28LLMs%29%20to%20significantly%20enhance%20the%20capabilities%20and%0Auser-friendliness%20of%20conventional%20debuggers.%20ChatDBG%20lets%20programmers%20engage%20in%0Aa%20collaborative%20dialogue%20with%20the%20debugger%2C%20allowing%20them%20to%20pose%20complex%0Aquestions%20about%20program%20state%2C%20perform%20root%20cause%20analysis%20for%20crashes%20or%0Aassertion%20failures%2C%20and%20explore%20open-ended%20queries%20like%20%22why%20is%20x%20null%3F%22.%20To%0Ahandle%20these%20queries%2C%20ChatDBG%20grants%20the%20LLM%20autonomy%20to%20%22take%20the%20wheel%22%3A%20it%0Acan%20act%20as%20an%20independent%20agent%20capable%20of%20querying%20and%20controlling%20the%0Adebugger%20to%20navigate%20through%20stacks%20and%20inspect%20program%20state.%20It%20then%20reports%0Aits%20findings%20and%20yields%20back%20control%20to%20the%20programmer.%20By%20leveraging%20the%0Areal-world%20knowledge%20embedded%20in%20LLMs%2C%20ChatDBG%20can%20diagnose%20issues%20identifiable%0Aonly%20through%20the%20use%20of%20domain-specific%20reasoning.%20Our%20ChatDBG%20prototype%0Aintegrates%20with%20standard%20debuggers%20including%20LLDB%20and%20GDB%20for%20native%20code%20and%0APdb%20for%20Python.%20Our%20evaluation%20across%20a%20diverse%20set%20of%20code%2C%20including%20C/C%2B%2B%0Acode%20with%20known%20bugs%20and%20a%20suite%20of%20Python%20code%20including%20standalone%20scripts%0Aand%20Jupyter%20notebooks%2C%20demonstrates%20that%20ChatDBG%20can%20successfully%20analyze%20root%0Acauses%2C%20explain%20bugs%2C%20and%20generate%20accurate%20fixes%20for%20a%20wide%20range%20of%0Areal-world%20errors.%20For%20the%20Python%20programs%2C%20a%20single%20query%20led%20to%20an%20actionable%0Abug%20fix%2067%25%20of%20the%20time%3B%20one%20additional%20follow-up%20query%20increased%20the%20success%0Arate%20to%2085%25.%20ChatDBG%20has%20seen%20rapid%20uptake%3B%20it%20has%20already%20been%20downloaded%20more%0Athan%2075%2C000%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16354v4&entry.124074799=Read"},
{"title": "Hyperspectral Vision Transformers for Greenhouse Gas Estimations from\n  Space", "author": "Ruben Gonzalez Avil\u00e9s and Linus Scheibenreif and Nassim Ait Ali Braham and Benedikt Blumenstiel and Thomas Brunschwiler and Ranjini Guruprasad and Damian Borth and Conrad Albrecht and Paolo Fraccaro and Devyani Lambhate and Johannes Jakubik", "abstract": "  Hyperspectral imaging provides detailed spectral information and holds\nsignificant potential for monitoring of greenhouse gases (GHGs). However, its\napplication is constrained by limited spatial coverage and infrequent revisit\ntimes. In contrast, multispectral imaging offers broader spatial and temporal\ncoverage but often lacks the spectral detail that can enhance GHG detection. To\naddress these challenges, this study proposes a spectral transformer model that\nsynthesizes hyperspectral data from multispectral inputs. The model is\npre-trained via a band-wise masked autoencoder and subsequently fine-tuned on\nspatio-temporally aligned multispectral-hyperspectral image pairs. The\nresulting synthetic hyperspectral data retain the spatial and temporal benefits\nof multispectral imagery and improve GHG prediction accuracy relative to using\nmultispectral data alone. This approach effectively bridges the trade-off\nbetween spectral resolution and coverage, highlighting its potential to advance\natmospheric monitoring by combining the strengths of hyperspectral and\nmultispectral systems with self-supervised deep learning.\n", "link": "http://arxiv.org/abs/2504.16851v1", "date": "2025-04-23", "relevancy": 1.6676, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5797}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5389}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspectral%20Vision%20Transformers%20for%20Greenhouse%20Gas%20Estimations%20from%0A%20%20Space&body=Title%3A%20Hyperspectral%20Vision%20Transformers%20for%20Greenhouse%20Gas%20Estimations%20from%0A%20%20Space%0AAuthor%3A%20Ruben%20Gonzalez%20Avil%C3%A9s%20and%20Linus%20Scheibenreif%20and%20Nassim%20Ait%20Ali%20Braham%20and%20Benedikt%20Blumenstiel%20and%20Thomas%20Brunschwiler%20and%20Ranjini%20Guruprasad%20and%20Damian%20Borth%20and%20Conrad%20Albrecht%20and%20Paolo%20Fraccaro%20and%20Devyani%20Lambhate%20and%20Johannes%20Jakubik%0AAbstract%3A%20%20%20Hyperspectral%20imaging%20provides%20detailed%20spectral%20information%20and%20holds%0Asignificant%20potential%20for%20monitoring%20of%20greenhouse%20gases%20%28GHGs%29.%20However%2C%20its%0Aapplication%20is%20constrained%20by%20limited%20spatial%20coverage%20and%20infrequent%20revisit%0Atimes.%20In%20contrast%2C%20multispectral%20imaging%20offers%20broader%20spatial%20and%20temporal%0Acoverage%20but%20often%20lacks%20the%20spectral%20detail%20that%20can%20enhance%20GHG%20detection.%20To%0Aaddress%20these%20challenges%2C%20this%20study%20proposes%20a%20spectral%20transformer%20model%20that%0Asynthesizes%20hyperspectral%20data%20from%20multispectral%20inputs.%20The%20model%20is%0Apre-trained%20via%20a%20band-wise%20masked%20autoencoder%20and%20subsequently%20fine-tuned%20on%0Aspatio-temporally%20aligned%20multispectral-hyperspectral%20image%20pairs.%20The%0Aresulting%20synthetic%20hyperspectral%20data%20retain%20the%20spatial%20and%20temporal%20benefits%0Aof%20multispectral%20imagery%20and%20improve%20GHG%20prediction%20accuracy%20relative%20to%20using%0Amultispectral%20data%20alone.%20This%20approach%20effectively%20bridges%20the%20trade-off%0Abetween%20spectral%20resolution%20and%20coverage%2C%20highlighting%20its%20potential%20to%20advance%0Aatmospheric%20monitoring%20by%20combining%20the%20strengths%20of%20hyperspectral%20and%0Amultispectral%20systems%20with%20self-supervised%20deep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspectral%2520Vision%2520Transformers%2520for%2520Greenhouse%2520Gas%2520Estimations%2520from%250A%2520%2520Space%26entry.906535625%3DRuben%2520Gonzalez%2520Avil%25C3%25A9s%2520and%2520Linus%2520Scheibenreif%2520and%2520Nassim%2520Ait%2520Ali%2520Braham%2520and%2520Benedikt%2520Blumenstiel%2520and%2520Thomas%2520Brunschwiler%2520and%2520Ranjini%2520Guruprasad%2520and%2520Damian%2520Borth%2520and%2520Conrad%2520Albrecht%2520and%2520Paolo%2520Fraccaro%2520and%2520Devyani%2520Lambhate%2520and%2520Johannes%2520Jakubik%26entry.1292438233%3D%2520%2520Hyperspectral%2520imaging%2520provides%2520detailed%2520spectral%2520information%2520and%2520holds%250Asignificant%2520potential%2520for%2520monitoring%2520of%2520greenhouse%2520gases%2520%2528GHGs%2529.%2520However%252C%2520its%250Aapplication%2520is%2520constrained%2520by%2520limited%2520spatial%2520coverage%2520and%2520infrequent%2520revisit%250Atimes.%2520In%2520contrast%252C%2520multispectral%2520imaging%2520offers%2520broader%2520spatial%2520and%2520temporal%250Acoverage%2520but%2520often%2520lacks%2520the%2520spectral%2520detail%2520that%2520can%2520enhance%2520GHG%2520detection.%2520To%250Aaddress%2520these%2520challenges%252C%2520this%2520study%2520proposes%2520a%2520spectral%2520transformer%2520model%2520that%250Asynthesizes%2520hyperspectral%2520data%2520from%2520multispectral%2520inputs.%2520The%2520model%2520is%250Apre-trained%2520via%2520a%2520band-wise%2520masked%2520autoencoder%2520and%2520subsequently%2520fine-tuned%2520on%250Aspatio-temporally%2520aligned%2520multispectral-hyperspectral%2520image%2520pairs.%2520The%250Aresulting%2520synthetic%2520hyperspectral%2520data%2520retain%2520the%2520spatial%2520and%2520temporal%2520benefits%250Aof%2520multispectral%2520imagery%2520and%2520improve%2520GHG%2520prediction%2520accuracy%2520relative%2520to%2520using%250Amultispectral%2520data%2520alone.%2520This%2520approach%2520effectively%2520bridges%2520the%2520trade-off%250Abetween%2520spectral%2520resolution%2520and%2520coverage%252C%2520highlighting%2520its%2520potential%2520to%2520advance%250Aatmospheric%2520monitoring%2520by%2520combining%2520the%2520strengths%2520of%2520hyperspectral%2520and%250Amultispectral%2520systems%2520with%2520self-supervised%2520deep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspectral%20Vision%20Transformers%20for%20Greenhouse%20Gas%20Estimations%20from%0A%20%20Space&entry.906535625=Ruben%20Gonzalez%20Avil%C3%A9s%20and%20Linus%20Scheibenreif%20and%20Nassim%20Ait%20Ali%20Braham%20and%20Benedikt%20Blumenstiel%20and%20Thomas%20Brunschwiler%20and%20Ranjini%20Guruprasad%20and%20Damian%20Borth%20and%20Conrad%20Albrecht%20and%20Paolo%20Fraccaro%20and%20Devyani%20Lambhate%20and%20Johannes%20Jakubik&entry.1292438233=%20%20Hyperspectral%20imaging%20provides%20detailed%20spectral%20information%20and%20holds%0Asignificant%20potential%20for%20monitoring%20of%20greenhouse%20gases%20%28GHGs%29.%20However%2C%20its%0Aapplication%20is%20constrained%20by%20limited%20spatial%20coverage%20and%20infrequent%20revisit%0Atimes.%20In%20contrast%2C%20multispectral%20imaging%20offers%20broader%20spatial%20and%20temporal%0Acoverage%20but%20often%20lacks%20the%20spectral%20detail%20that%20can%20enhance%20GHG%20detection.%20To%0Aaddress%20these%20challenges%2C%20this%20study%20proposes%20a%20spectral%20transformer%20model%20that%0Asynthesizes%20hyperspectral%20data%20from%20multispectral%20inputs.%20The%20model%20is%0Apre-trained%20via%20a%20band-wise%20masked%20autoencoder%20and%20subsequently%20fine-tuned%20on%0Aspatio-temporally%20aligned%20multispectral-hyperspectral%20image%20pairs.%20The%0Aresulting%20synthetic%20hyperspectral%20data%20retain%20the%20spatial%20and%20temporal%20benefits%0Aof%20multispectral%20imagery%20and%20improve%20GHG%20prediction%20accuracy%20relative%20to%20using%0Amultispectral%20data%20alone.%20This%20approach%20effectively%20bridges%20the%20trade-off%0Abetween%20spectral%20resolution%20and%20coverage%2C%20highlighting%20its%20potential%20to%20advance%0Aatmospheric%20monitoring%20by%20combining%20the%20strengths%20of%20hyperspectral%20and%0Amultispectral%20systems%20with%20self-supervised%20deep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16851v1&entry.124074799=Read"},
{"title": "Should We Learn Contact-Rich Manipulation Policies from Sampling-Based\n  Planners?", "author": "Huaijiang Zhu and Tong Zhao and Xinpei Ni and Jiuguang Wang and Kuan Fang and Ludovic Righetti and Tao Pang", "abstract": "  The tremendous success of behavior cloning (BC) in robotic manipulation has\nbeen largely confined to tasks where demonstrations can be effectively\ncollected through human teleoperation. However, demonstrations for contact-rich\nmanipulation tasks that require complex coordination of multiple contacts are\ndifficult to collect due to the limitations of current teleoperation\ninterfaces. We investigate how to leverage model-based planning and\noptimization to generate training data for contact-rich dexterous manipulation\ntasks. Our analysis reveals that popular sampling-based planners like rapidly\nexploring random tree (RRT), while efficient for motion planning, produce\ndemonstrations with unfavorably high entropy. This motivates modifications to\nour data generation pipeline that prioritizes demonstration consistency while\nmaintaining solution diversity. Combined with a diffusion-based\ngoal-conditioned BC approach, our method enables effective policy learning and\nzero-shot transfer to hardware for two challenging contact-rich manipulation\ntasks.\n", "link": "http://arxiv.org/abs/2412.09743v2", "date": "2025-04-23", "relevancy": 1.6592, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5911}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5813}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Should%20We%20Learn%20Contact-Rich%20Manipulation%20Policies%20from%20Sampling-Based%0A%20%20Planners%3F&body=Title%3A%20Should%20We%20Learn%20Contact-Rich%20Manipulation%20Policies%20from%20Sampling-Based%0A%20%20Planners%3F%0AAuthor%3A%20Huaijiang%20Zhu%20and%20Tong%20Zhao%20and%20Xinpei%20Ni%20and%20Jiuguang%20Wang%20and%20Kuan%20Fang%20and%20Ludovic%20Righetti%20and%20Tao%20Pang%0AAbstract%3A%20%20%20The%20tremendous%20success%20of%20behavior%20cloning%20%28BC%29%20in%20robotic%20manipulation%20has%0Abeen%20largely%20confined%20to%20tasks%20where%20demonstrations%20can%20be%20effectively%0Acollected%20through%20human%20teleoperation.%20However%2C%20demonstrations%20for%20contact-rich%0Amanipulation%20tasks%20that%20require%20complex%20coordination%20of%20multiple%20contacts%20are%0Adifficult%20to%20collect%20due%20to%20the%20limitations%20of%20current%20teleoperation%0Ainterfaces.%20We%20investigate%20how%20to%20leverage%20model-based%20planning%20and%0Aoptimization%20to%20generate%20training%20data%20for%20contact-rich%20dexterous%20manipulation%0Atasks.%20Our%20analysis%20reveals%20that%20popular%20sampling-based%20planners%20like%20rapidly%0Aexploring%20random%20tree%20%28RRT%29%2C%20while%20efficient%20for%20motion%20planning%2C%20produce%0Ademonstrations%20with%20unfavorably%20high%20entropy.%20This%20motivates%20modifications%20to%0Aour%20data%20generation%20pipeline%20that%20prioritizes%20demonstration%20consistency%20while%0Amaintaining%20solution%20diversity.%20Combined%20with%20a%20diffusion-based%0Agoal-conditioned%20BC%20approach%2C%20our%20method%20enables%20effective%20policy%20learning%20and%0Azero-shot%20transfer%20to%20hardware%20for%20two%20challenging%20contact-rich%20manipulation%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09743v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShould%2520We%2520Learn%2520Contact-Rich%2520Manipulation%2520Policies%2520from%2520Sampling-Based%250A%2520%2520Planners%253F%26entry.906535625%3DHuaijiang%2520Zhu%2520and%2520Tong%2520Zhao%2520and%2520Xinpei%2520Ni%2520and%2520Jiuguang%2520Wang%2520and%2520Kuan%2520Fang%2520and%2520Ludovic%2520Righetti%2520and%2520Tao%2520Pang%26entry.1292438233%3D%2520%2520The%2520tremendous%2520success%2520of%2520behavior%2520cloning%2520%2528BC%2529%2520in%2520robotic%2520manipulation%2520has%250Abeen%2520largely%2520confined%2520to%2520tasks%2520where%2520demonstrations%2520can%2520be%2520effectively%250Acollected%2520through%2520human%2520teleoperation.%2520However%252C%2520demonstrations%2520for%2520contact-rich%250Amanipulation%2520tasks%2520that%2520require%2520complex%2520coordination%2520of%2520multiple%2520contacts%2520are%250Adifficult%2520to%2520collect%2520due%2520to%2520the%2520limitations%2520of%2520current%2520teleoperation%250Ainterfaces.%2520We%2520investigate%2520how%2520to%2520leverage%2520model-based%2520planning%2520and%250Aoptimization%2520to%2520generate%2520training%2520data%2520for%2520contact-rich%2520dexterous%2520manipulation%250Atasks.%2520Our%2520analysis%2520reveals%2520that%2520popular%2520sampling-based%2520planners%2520like%2520rapidly%250Aexploring%2520random%2520tree%2520%2528RRT%2529%252C%2520while%2520efficient%2520for%2520motion%2520planning%252C%2520produce%250Ademonstrations%2520with%2520unfavorably%2520high%2520entropy.%2520This%2520motivates%2520modifications%2520to%250Aour%2520data%2520generation%2520pipeline%2520that%2520prioritizes%2520demonstration%2520consistency%2520while%250Amaintaining%2520solution%2520diversity.%2520Combined%2520with%2520a%2520diffusion-based%250Agoal-conditioned%2520BC%2520approach%252C%2520our%2520method%2520enables%2520effective%2520policy%2520learning%2520and%250Azero-shot%2520transfer%2520to%2520hardware%2520for%2520two%2520challenging%2520contact-rich%2520manipulation%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09743v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Should%20We%20Learn%20Contact-Rich%20Manipulation%20Policies%20from%20Sampling-Based%0A%20%20Planners%3F&entry.906535625=Huaijiang%20Zhu%20and%20Tong%20Zhao%20and%20Xinpei%20Ni%20and%20Jiuguang%20Wang%20and%20Kuan%20Fang%20and%20Ludovic%20Righetti%20and%20Tao%20Pang&entry.1292438233=%20%20The%20tremendous%20success%20of%20behavior%20cloning%20%28BC%29%20in%20robotic%20manipulation%20has%0Abeen%20largely%20confined%20to%20tasks%20where%20demonstrations%20can%20be%20effectively%0Acollected%20through%20human%20teleoperation.%20However%2C%20demonstrations%20for%20contact-rich%0Amanipulation%20tasks%20that%20require%20complex%20coordination%20of%20multiple%20contacts%20are%0Adifficult%20to%20collect%20due%20to%20the%20limitations%20of%20current%20teleoperation%0Ainterfaces.%20We%20investigate%20how%20to%20leverage%20model-based%20planning%20and%0Aoptimization%20to%20generate%20training%20data%20for%20contact-rich%20dexterous%20manipulation%0Atasks.%20Our%20analysis%20reveals%20that%20popular%20sampling-based%20planners%20like%20rapidly%0Aexploring%20random%20tree%20%28RRT%29%2C%20while%20efficient%20for%20motion%20planning%2C%20produce%0Ademonstrations%20with%20unfavorably%20high%20entropy.%20This%20motivates%20modifications%20to%0Aour%20data%20generation%20pipeline%20that%20prioritizes%20demonstration%20consistency%20while%0Amaintaining%20solution%20diversity.%20Combined%20with%20a%20diffusion-based%0Agoal-conditioned%20BC%20approach%2C%20our%20method%20enables%20effective%20policy%20learning%20and%0Azero-shot%20transfer%20to%20hardware%20for%20two%20challenging%20contact-rich%20manipulation%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09743v2&entry.124074799=Read"},
{"title": "DiffArtist: Towards Structure and Appearance Controllable Image\n  Stylization", "author": "Ruixiang Jiang and Changwen Chen", "abstract": "  Artistic style includes both structural and appearance elements. Existing\nneural stylization techniques primarily focus on transferring appearance\nfeatures such as color and texture, often neglecting the equally crucial aspect\nof structural stylization. In this paper, we present a comprehensive study on\nthe simultaneous stylization of structure and appearance of 2D images.\nSpecifically, we introduce DiffArtist, which, to the best of our knowledge, is\nthe first stylization method to allow for dual controllability over structure\nand appearance. Our key insight is to represent structure and appearance as\nseparate diffusion processes to achieve complete disentanglement without\nrequiring any training, thereby endowing users with unprecedented\ncontrollability for both components. The evaluation of stylization of both\nappearance and structure, however, remains challenging as it necessitates\nsemantic understanding. To this end, we further propose a Multimodal LLM-based\nstyle evaluator, which better aligns with human preferences than metrics\nlacking semantic understanding. With this powerful evaluator, we conduct\nextensive analysis, demonstrating that DiffArtist achieves superior style\nfidelity, editability, and structure-appearance disentanglement. These merits\nmake DiffArtist a highly versatile solution for creative applications. Project\nhomepage: https://github.com/songrise/Artist.\n", "link": "http://arxiv.org/abs/2407.15842v3", "date": "2025-04-23", "relevancy": 1.6535, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5925}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5477}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffArtist%3A%20Towards%20Structure%20and%20Appearance%20Controllable%20Image%0A%20%20Stylization&body=Title%3A%20DiffArtist%3A%20Towards%20Structure%20and%20Appearance%20Controllable%20Image%0A%20%20Stylization%0AAuthor%3A%20Ruixiang%20Jiang%20and%20Changwen%20Chen%0AAbstract%3A%20%20%20Artistic%20style%20includes%20both%20structural%20and%20appearance%20elements.%20Existing%0Aneural%20stylization%20techniques%20primarily%20focus%20on%20transferring%20appearance%0Afeatures%20such%20as%20color%20and%20texture%2C%20often%20neglecting%20the%20equally%20crucial%20aspect%0Aof%20structural%20stylization.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20study%20on%0Athe%20simultaneous%20stylization%20of%20structure%20and%20appearance%20of%202D%20images.%0ASpecifically%2C%20we%20introduce%20DiffArtist%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%0Athe%20first%20stylization%20method%20to%20allow%20for%20dual%20controllability%20over%20structure%0Aand%20appearance.%20Our%20key%20insight%20is%20to%20represent%20structure%20and%20appearance%20as%0Aseparate%20diffusion%20processes%20to%20achieve%20complete%20disentanglement%20without%0Arequiring%20any%20training%2C%20thereby%20endowing%20users%20with%20unprecedented%0Acontrollability%20for%20both%20components.%20The%20evaluation%20of%20stylization%20of%20both%0Aappearance%20and%20structure%2C%20however%2C%20remains%20challenging%20as%20it%20necessitates%0Asemantic%20understanding.%20To%20this%20end%2C%20we%20further%20propose%20a%20Multimodal%20LLM-based%0Astyle%20evaluator%2C%20which%20better%20aligns%20with%20human%20preferences%20than%20metrics%0Alacking%20semantic%20understanding.%20With%20this%20powerful%20evaluator%2C%20we%20conduct%0Aextensive%20analysis%2C%20demonstrating%20that%20DiffArtist%20achieves%20superior%20style%0Afidelity%2C%20editability%2C%20and%20structure-appearance%20disentanglement.%20These%20merits%0Amake%20DiffArtist%20a%20highly%20versatile%20solution%20for%20creative%20applications.%20Project%0Ahomepage%3A%20https%3A//github.com/songrise/Artist.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15842v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffArtist%253A%2520Towards%2520Structure%2520and%2520Appearance%2520Controllable%2520Image%250A%2520%2520Stylization%26entry.906535625%3DRuixiang%2520Jiang%2520and%2520Changwen%2520Chen%26entry.1292438233%3D%2520%2520Artistic%2520style%2520includes%2520both%2520structural%2520and%2520appearance%2520elements.%2520Existing%250Aneural%2520stylization%2520techniques%2520primarily%2520focus%2520on%2520transferring%2520appearance%250Afeatures%2520such%2520as%2520color%2520and%2520texture%252C%2520often%2520neglecting%2520the%2520equally%2520crucial%2520aspect%250Aof%2520structural%2520stylization.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520study%2520on%250Athe%2520simultaneous%2520stylization%2520of%2520structure%2520and%2520appearance%2520of%25202D%2520images.%250ASpecifically%252C%2520we%2520introduce%2520DiffArtist%252C%2520which%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520is%250Athe%2520first%2520stylization%2520method%2520to%2520allow%2520for%2520dual%2520controllability%2520over%2520structure%250Aand%2520appearance.%2520Our%2520key%2520insight%2520is%2520to%2520represent%2520structure%2520and%2520appearance%2520as%250Aseparate%2520diffusion%2520processes%2520to%2520achieve%2520complete%2520disentanglement%2520without%250Arequiring%2520any%2520training%252C%2520thereby%2520endowing%2520users%2520with%2520unprecedented%250Acontrollability%2520for%2520both%2520components.%2520The%2520evaluation%2520of%2520stylization%2520of%2520both%250Aappearance%2520and%2520structure%252C%2520however%252C%2520remains%2520challenging%2520as%2520it%2520necessitates%250Asemantic%2520understanding.%2520To%2520this%2520end%252C%2520we%2520further%2520propose%2520a%2520Multimodal%2520LLM-based%250Astyle%2520evaluator%252C%2520which%2520better%2520aligns%2520with%2520human%2520preferences%2520than%2520metrics%250Alacking%2520semantic%2520understanding.%2520With%2520this%2520powerful%2520evaluator%252C%2520we%2520conduct%250Aextensive%2520analysis%252C%2520demonstrating%2520that%2520DiffArtist%2520achieves%2520superior%2520style%250Afidelity%252C%2520editability%252C%2520and%2520structure-appearance%2520disentanglement.%2520These%2520merits%250Amake%2520DiffArtist%2520a%2520highly%2520versatile%2520solution%2520for%2520creative%2520applications.%2520Project%250Ahomepage%253A%2520https%253A//github.com/songrise/Artist.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15842v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffArtist%3A%20Towards%20Structure%20and%20Appearance%20Controllable%20Image%0A%20%20Stylization&entry.906535625=Ruixiang%20Jiang%20and%20Changwen%20Chen&entry.1292438233=%20%20Artistic%20style%20includes%20both%20structural%20and%20appearance%20elements.%20Existing%0Aneural%20stylization%20techniques%20primarily%20focus%20on%20transferring%20appearance%0Afeatures%20such%20as%20color%20and%20texture%2C%20often%20neglecting%20the%20equally%20crucial%20aspect%0Aof%20structural%20stylization.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20study%20on%0Athe%20simultaneous%20stylization%20of%20structure%20and%20appearance%20of%202D%20images.%0ASpecifically%2C%20we%20introduce%20DiffArtist%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%0Athe%20first%20stylization%20method%20to%20allow%20for%20dual%20controllability%20over%20structure%0Aand%20appearance.%20Our%20key%20insight%20is%20to%20represent%20structure%20and%20appearance%20as%0Aseparate%20diffusion%20processes%20to%20achieve%20complete%20disentanglement%20without%0Arequiring%20any%20training%2C%20thereby%20endowing%20users%20with%20unprecedented%0Acontrollability%20for%20both%20components.%20The%20evaluation%20of%20stylization%20of%20both%0Aappearance%20and%20structure%2C%20however%2C%20remains%20challenging%20as%20it%20necessitates%0Asemantic%20understanding.%20To%20this%20end%2C%20we%20further%20propose%20a%20Multimodal%20LLM-based%0Astyle%20evaluator%2C%20which%20better%20aligns%20with%20human%20preferences%20than%20metrics%0Alacking%20semantic%20understanding.%20With%20this%20powerful%20evaluator%2C%20we%20conduct%0Aextensive%20analysis%2C%20demonstrating%20that%20DiffArtist%20achieves%20superior%20style%0Afidelity%2C%20editability%2C%20and%20structure-appearance%20disentanglement.%20These%20merits%0Amake%20DiffArtist%20a%20highly%20versatile%20solution%20for%20creative%20applications.%20Project%0Ahomepage%3A%20https%3A//github.com/songrise/Artist.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15842v3&entry.124074799=Read"},
{"title": "Towards Physics-Guided Foundation Models", "author": "Majid Farhadloo and Arun Sharma and Mingzhou Yang and Bharat Jayaprakash and William Northrop and Shashi Shekhar", "abstract": "  Traditional foundation models are pre-trained on broad datasets to reduce the\ntraining resources (e.g., time, energy, labeled samples) needed for fine-tuning\na wide range of downstream tasks. However, traditional foundation models\nstruggle with out-of-distribution prediction and can produce outputs that are\nunrealistic and physically infeasible. We propose the notation of\nphysics-guided foundation models (PGFM), that is, foundation models integrated\nwith broad or general domain (e.g., scientific) physical knowledge applicable\nto a wide range of downstream tasks.\n", "link": "http://arxiv.org/abs/2502.15013v3", "date": "2025-04-23", "relevancy": 1.6283, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5752}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.512}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Physics-Guided%20Foundation%20Models&body=Title%3A%20Towards%20Physics-Guided%20Foundation%20Models%0AAuthor%3A%20Majid%20Farhadloo%20and%20Arun%20Sharma%20and%20Mingzhou%20Yang%20and%20Bharat%20Jayaprakash%20and%20William%20Northrop%20and%20Shashi%20Shekhar%0AAbstract%3A%20%20%20Traditional%20foundation%20models%20are%20pre-trained%20on%20broad%20datasets%20to%20reduce%20the%0Atraining%20resources%20%28e.g.%2C%20time%2C%20energy%2C%20labeled%20samples%29%20needed%20for%20fine-tuning%0Aa%20wide%20range%20of%20downstream%20tasks.%20However%2C%20traditional%20foundation%20models%0Astruggle%20with%20out-of-distribution%20prediction%20and%20can%20produce%20outputs%20that%20are%0Aunrealistic%20and%20physically%20infeasible.%20We%20propose%20the%20notation%20of%0Aphysics-guided%20foundation%20models%20%28PGFM%29%2C%20that%20is%2C%20foundation%20models%20integrated%0Awith%20broad%20or%20general%20domain%20%28e.g.%2C%20scientific%29%20physical%20knowledge%20applicable%0Ato%20a%20wide%20range%20of%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15013v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Physics-Guided%2520Foundation%2520Models%26entry.906535625%3DMajid%2520Farhadloo%2520and%2520Arun%2520Sharma%2520and%2520Mingzhou%2520Yang%2520and%2520Bharat%2520Jayaprakash%2520and%2520William%2520Northrop%2520and%2520Shashi%2520Shekhar%26entry.1292438233%3D%2520%2520Traditional%2520foundation%2520models%2520are%2520pre-trained%2520on%2520broad%2520datasets%2520to%2520reduce%2520the%250Atraining%2520resources%2520%2528e.g.%252C%2520time%252C%2520energy%252C%2520labeled%2520samples%2529%2520needed%2520for%2520fine-tuning%250Aa%2520wide%2520range%2520of%2520downstream%2520tasks.%2520However%252C%2520traditional%2520foundation%2520models%250Astruggle%2520with%2520out-of-distribution%2520prediction%2520and%2520can%2520produce%2520outputs%2520that%2520are%250Aunrealistic%2520and%2520physically%2520infeasible.%2520We%2520propose%2520the%2520notation%2520of%250Aphysics-guided%2520foundation%2520models%2520%2528PGFM%2529%252C%2520that%2520is%252C%2520foundation%2520models%2520integrated%250Awith%2520broad%2520or%2520general%2520domain%2520%2528e.g.%252C%2520scientific%2529%2520physical%2520knowledge%2520applicable%250Ato%2520a%2520wide%2520range%2520of%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15013v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Physics-Guided%20Foundation%20Models&entry.906535625=Majid%20Farhadloo%20and%20Arun%20Sharma%20and%20Mingzhou%20Yang%20and%20Bharat%20Jayaprakash%20and%20William%20Northrop%20and%20Shashi%20Shekhar&entry.1292438233=%20%20Traditional%20foundation%20models%20are%20pre-trained%20on%20broad%20datasets%20to%20reduce%20the%0Atraining%20resources%20%28e.g.%2C%20time%2C%20energy%2C%20labeled%20samples%29%20needed%20for%20fine-tuning%0Aa%20wide%20range%20of%20downstream%20tasks.%20However%2C%20traditional%20foundation%20models%0Astruggle%20with%20out-of-distribution%20prediction%20and%20can%20produce%20outputs%20that%20are%0Aunrealistic%20and%20physically%20infeasible.%20We%20propose%20the%20notation%20of%0Aphysics-guided%20foundation%20models%20%28PGFM%29%2C%20that%20is%2C%20foundation%20models%20integrated%0Awith%20broad%20or%20general%20domain%20%28e.g.%2C%20scientific%29%20physical%20knowledge%20applicable%0Ato%20a%20wide%20range%20of%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15013v3&entry.124074799=Read"},
{"title": "Advancing Embodied Intelligence in Robotic-Assisted Endovascular\n  Procedures: A Systematic Review of AI Solutions", "author": "Tianliang Yao and Bo Lu and Markus Kowarschik and Yixuan Yuan and Hubin Zhao and Sebastien Ourselin and Kaspar Althoefer and Junbo Ge and Peng Qi", "abstract": "  Endovascular procedures have revolutionized the treatment of vascular\ndiseases thanks to minimally invasive solutions that significantly reduce\npatient recovery time and enhance clinical outcomes. However, the precision and\ndexterity required during these procedures poses considerable challenges for\ninterventionists. Robotic systems have emerged offering transformative\nsolutions, addressing issues such as operator fatigue, radiation exposure, and\nthe inherent limitations of human precision. The integration of Embodied\nIntelligence (EI) into these systems signifies a paradigm shift, enabling\nrobots to navigate complex vascular networks and adapt to dynamic physiological\nconditions. Data-driven approaches, advanced computer vision, medical image\nanalysis, and machine learning techniques, are at the forefront of this\nevolution. These methods augment procedural intelligence by facilitating\nreal-time vessel segmentation, device tracking, and anatomical landmark\ndetection. Reinforcement learning and imitation learning further refine\nnavigation strategies and replicate experts' techniques. This review\nsystematically examines the integration of EI principles into robotic\ntechnologies, in relation to endovascular procedures. We discuss recent\nadvancements in intelligent perception and data-driven control, and their\npractical applications in robot-assisted endovascular procedures. By critically\nevaluating current limitations and emerging opportunities, this review\nestablishes a framework for future developments, emphasizing the potential for\ngreater autonomy and improved clinical outcomes. Emerging trends and specific\nareas of research, such as federated learning for medical data sharing,\nexplainable AI for clinical decision support, and advanced human-robot\ncollaboration paradigms, are also explored, offering insights into the future\ndirection of this rapidly evolving field.\n", "link": "http://arxiv.org/abs/2504.15327v2", "date": "2025-04-23", "relevancy": 1.6129, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5681}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5172}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Embodied%20Intelligence%20in%20Robotic-Assisted%20Endovascular%0A%20%20Procedures%3A%20A%20Systematic%20Review%20of%20AI%20Solutions&body=Title%3A%20Advancing%20Embodied%20Intelligence%20in%20Robotic-Assisted%20Endovascular%0A%20%20Procedures%3A%20A%20Systematic%20Review%20of%20AI%20Solutions%0AAuthor%3A%20Tianliang%20Yao%20and%20Bo%20Lu%20and%20Markus%20Kowarschik%20and%20Yixuan%20Yuan%20and%20Hubin%20Zhao%20and%20Sebastien%20Ourselin%20and%20Kaspar%20Althoefer%20and%20Junbo%20Ge%20and%20Peng%20Qi%0AAbstract%3A%20%20%20Endovascular%20procedures%20have%20revolutionized%20the%20treatment%20of%20vascular%0Adiseases%20thanks%20to%20minimally%20invasive%20solutions%20that%20significantly%20reduce%0Apatient%20recovery%20time%20and%20enhance%20clinical%20outcomes.%20However%2C%20the%20precision%20and%0Adexterity%20required%20during%20these%20procedures%20poses%20considerable%20challenges%20for%0Ainterventionists.%20Robotic%20systems%20have%20emerged%20offering%20transformative%0Asolutions%2C%20addressing%20issues%20such%20as%20operator%20fatigue%2C%20radiation%20exposure%2C%20and%0Athe%20inherent%20limitations%20of%20human%20precision.%20The%20integration%20of%20Embodied%0AIntelligence%20%28EI%29%20into%20these%20systems%20signifies%20a%20paradigm%20shift%2C%20enabling%0Arobots%20to%20navigate%20complex%20vascular%20networks%20and%20adapt%20to%20dynamic%20physiological%0Aconditions.%20Data-driven%20approaches%2C%20advanced%20computer%20vision%2C%20medical%20image%0Aanalysis%2C%20and%20machine%20learning%20techniques%2C%20are%20at%20the%20forefront%20of%20this%0Aevolution.%20These%20methods%20augment%20procedural%20intelligence%20by%20facilitating%0Areal-time%20vessel%20segmentation%2C%20device%20tracking%2C%20and%20anatomical%20landmark%0Adetection.%20Reinforcement%20learning%20and%20imitation%20learning%20further%20refine%0Anavigation%20strategies%20and%20replicate%20experts%27%20techniques.%20This%20review%0Asystematically%20examines%20the%20integration%20of%20EI%20principles%20into%20robotic%0Atechnologies%2C%20in%20relation%20to%20endovascular%20procedures.%20We%20discuss%20recent%0Aadvancements%20in%20intelligent%20perception%20and%20data-driven%20control%2C%20and%20their%0Apractical%20applications%20in%20robot-assisted%20endovascular%20procedures.%20By%20critically%0Aevaluating%20current%20limitations%20and%20emerging%20opportunities%2C%20this%20review%0Aestablishes%20a%20framework%20for%20future%20developments%2C%20emphasizing%20the%20potential%20for%0Agreater%20autonomy%20and%20improved%20clinical%20outcomes.%20Emerging%20trends%20and%20specific%0Aareas%20of%20research%2C%20such%20as%20federated%20learning%20for%20medical%20data%20sharing%2C%0Aexplainable%20AI%20for%20clinical%20decision%20support%2C%20and%20advanced%20human-robot%0Acollaboration%20paradigms%2C%20are%20also%20explored%2C%20offering%20insights%20into%20the%20future%0Adirection%20of%20this%20rapidly%20evolving%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Embodied%2520Intelligence%2520in%2520Robotic-Assisted%2520Endovascular%250A%2520%2520Procedures%253A%2520A%2520Systematic%2520Review%2520of%2520AI%2520Solutions%26entry.906535625%3DTianliang%2520Yao%2520and%2520Bo%2520Lu%2520and%2520Markus%2520Kowarschik%2520and%2520Yixuan%2520Yuan%2520and%2520Hubin%2520Zhao%2520and%2520Sebastien%2520Ourselin%2520and%2520Kaspar%2520Althoefer%2520and%2520Junbo%2520Ge%2520and%2520Peng%2520Qi%26entry.1292438233%3D%2520%2520Endovascular%2520procedures%2520have%2520revolutionized%2520the%2520treatment%2520of%2520vascular%250Adiseases%2520thanks%2520to%2520minimally%2520invasive%2520solutions%2520that%2520significantly%2520reduce%250Apatient%2520recovery%2520time%2520and%2520enhance%2520clinical%2520outcomes.%2520However%252C%2520the%2520precision%2520and%250Adexterity%2520required%2520during%2520these%2520procedures%2520poses%2520considerable%2520challenges%2520for%250Ainterventionists.%2520Robotic%2520systems%2520have%2520emerged%2520offering%2520transformative%250Asolutions%252C%2520addressing%2520issues%2520such%2520as%2520operator%2520fatigue%252C%2520radiation%2520exposure%252C%2520and%250Athe%2520inherent%2520limitations%2520of%2520human%2520precision.%2520The%2520integration%2520of%2520Embodied%250AIntelligence%2520%2528EI%2529%2520into%2520these%2520systems%2520signifies%2520a%2520paradigm%2520shift%252C%2520enabling%250Arobots%2520to%2520navigate%2520complex%2520vascular%2520networks%2520and%2520adapt%2520to%2520dynamic%2520physiological%250Aconditions.%2520Data-driven%2520approaches%252C%2520advanced%2520computer%2520vision%252C%2520medical%2520image%250Aanalysis%252C%2520and%2520machine%2520learning%2520techniques%252C%2520are%2520at%2520the%2520forefront%2520of%2520this%250Aevolution.%2520These%2520methods%2520augment%2520procedural%2520intelligence%2520by%2520facilitating%250Areal-time%2520vessel%2520segmentation%252C%2520device%2520tracking%252C%2520and%2520anatomical%2520landmark%250Adetection.%2520Reinforcement%2520learning%2520and%2520imitation%2520learning%2520further%2520refine%250Anavigation%2520strategies%2520and%2520replicate%2520experts%2527%2520techniques.%2520This%2520review%250Asystematically%2520examines%2520the%2520integration%2520of%2520EI%2520principles%2520into%2520robotic%250Atechnologies%252C%2520in%2520relation%2520to%2520endovascular%2520procedures.%2520We%2520discuss%2520recent%250Aadvancements%2520in%2520intelligent%2520perception%2520and%2520data-driven%2520control%252C%2520and%2520their%250Apractical%2520applications%2520in%2520robot-assisted%2520endovascular%2520procedures.%2520By%2520critically%250Aevaluating%2520current%2520limitations%2520and%2520emerging%2520opportunities%252C%2520this%2520review%250Aestablishes%2520a%2520framework%2520for%2520future%2520developments%252C%2520emphasizing%2520the%2520potential%2520for%250Agreater%2520autonomy%2520and%2520improved%2520clinical%2520outcomes.%2520Emerging%2520trends%2520and%2520specific%250Aareas%2520of%2520research%252C%2520such%2520as%2520federated%2520learning%2520for%2520medical%2520data%2520sharing%252C%250Aexplainable%2520AI%2520for%2520clinical%2520decision%2520support%252C%2520and%2520advanced%2520human-robot%250Acollaboration%2520paradigms%252C%2520are%2520also%2520explored%252C%2520offering%2520insights%2520into%2520the%2520future%250Adirection%2520of%2520this%2520rapidly%2520evolving%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Embodied%20Intelligence%20in%20Robotic-Assisted%20Endovascular%0A%20%20Procedures%3A%20A%20Systematic%20Review%20of%20AI%20Solutions&entry.906535625=Tianliang%20Yao%20and%20Bo%20Lu%20and%20Markus%20Kowarschik%20and%20Yixuan%20Yuan%20and%20Hubin%20Zhao%20and%20Sebastien%20Ourselin%20and%20Kaspar%20Althoefer%20and%20Junbo%20Ge%20and%20Peng%20Qi&entry.1292438233=%20%20Endovascular%20procedures%20have%20revolutionized%20the%20treatment%20of%20vascular%0Adiseases%20thanks%20to%20minimally%20invasive%20solutions%20that%20significantly%20reduce%0Apatient%20recovery%20time%20and%20enhance%20clinical%20outcomes.%20However%2C%20the%20precision%20and%0Adexterity%20required%20during%20these%20procedures%20poses%20considerable%20challenges%20for%0Ainterventionists.%20Robotic%20systems%20have%20emerged%20offering%20transformative%0Asolutions%2C%20addressing%20issues%20such%20as%20operator%20fatigue%2C%20radiation%20exposure%2C%20and%0Athe%20inherent%20limitations%20of%20human%20precision.%20The%20integration%20of%20Embodied%0AIntelligence%20%28EI%29%20into%20these%20systems%20signifies%20a%20paradigm%20shift%2C%20enabling%0Arobots%20to%20navigate%20complex%20vascular%20networks%20and%20adapt%20to%20dynamic%20physiological%0Aconditions.%20Data-driven%20approaches%2C%20advanced%20computer%20vision%2C%20medical%20image%0Aanalysis%2C%20and%20machine%20learning%20techniques%2C%20are%20at%20the%20forefront%20of%20this%0Aevolution.%20These%20methods%20augment%20procedural%20intelligence%20by%20facilitating%0Areal-time%20vessel%20segmentation%2C%20device%20tracking%2C%20and%20anatomical%20landmark%0Adetection.%20Reinforcement%20learning%20and%20imitation%20learning%20further%20refine%0Anavigation%20strategies%20and%20replicate%20experts%27%20techniques.%20This%20review%0Asystematically%20examines%20the%20integration%20of%20EI%20principles%20into%20robotic%0Atechnologies%2C%20in%20relation%20to%20endovascular%20procedures.%20We%20discuss%20recent%0Aadvancements%20in%20intelligent%20perception%20and%20data-driven%20control%2C%20and%20their%0Apractical%20applications%20in%20robot-assisted%20endovascular%20procedures.%20By%20critically%0Aevaluating%20current%20limitations%20and%20emerging%20opportunities%2C%20this%20review%0Aestablishes%20a%20framework%20for%20future%20developments%2C%20emphasizing%20the%20potential%20for%0Agreater%20autonomy%20and%20improved%20clinical%20outcomes.%20Emerging%20trends%20and%20specific%0Aareas%20of%20research%2C%20such%20as%20federated%20learning%20for%20medical%20data%20sharing%2C%0Aexplainable%20AI%20for%20clinical%20decision%20support%2C%20and%20advanced%20human-robot%0Acollaboration%20paradigms%2C%20are%20also%20explored%2C%20offering%20insights%20into%20the%20future%0Adirection%20of%20this%20rapidly%20evolving%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15327v2&entry.124074799=Read"},
{"title": "DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown\n  Environments", "author": "Kota Kondo and Mason Peterson and Nicholas Rober and Juan Rached Viso and Lucas Jia and Jialin Chen and Harvey Merton and Jonathan P. How", "abstract": "  This paper introduces DYNUS, an uncertainty-aware trajectory planner designed\nfor dynamic unknown environments. Operating in such settings presents many\nchallenges -- most notably, because the agent cannot predict the ground-truth\nfuture paths of obstacles, a previously planned trajectory can become unsafe at\nany moment, requiring rapid replanning to avoid collisions.\n  Recently developed planners have used soft-constraint approaches to achieve\nthe necessary fast computation times; however, these methods do not guarantee\ncollision-free paths even with static obstacles. In contrast, hard-constraint\nmethods ensure collision-free safety, but typically have longer computation\ntimes.\n  To address these issues, we propose three key contributions. First, the DYNUS\nGlobal Planner (DGP) and Temporal Safe Corridor Generation operate in\nspatio-temporal space and handle both static and dynamic obstacles in the 3D\nenvironment. Second, the Safe Planning Framework leverages a combination of\nexploratory, safe, and contingency trajectories to flexibly re-route when\npotential future collisions with dynamic obstacles are detected. Finally, the\nFast Hard-Constraint Local Trajectory Formulation uses a variable elimination\napproach to reduce the problem size and enable faster computation by\npre-computing dependencies between free and dependent variables while still\nensuring collision-free trajectories.\n  We evaluated DYNUS in a variety of simulations, including dense forests,\nconfined office spaces, cave systems, and dynamic environments. Our experiments\nshow that DYNUS achieves a success rate of 100% and travel times that are\napproximately 25.0% faster than state-of-the-art methods. We also evaluated\nDYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped --\nin both simulation and hardware experiments.\n", "link": "http://arxiv.org/abs/2504.16734v1", "date": "2025-04-23", "relevancy": 1.6068, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5245}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DYNUS%3A%20Uncertainty-aware%20Trajectory%20Planner%20in%20Dynamic%20Unknown%0A%20%20Environments&body=Title%3A%20DYNUS%3A%20Uncertainty-aware%20Trajectory%20Planner%20in%20Dynamic%20Unknown%0A%20%20Environments%0AAuthor%3A%20Kota%20Kondo%20and%20Mason%20Peterson%20and%20Nicholas%20Rober%20and%20Juan%20Rached%20Viso%20and%20Lucas%20Jia%20and%20Jialin%20Chen%20and%20Harvey%20Merton%20and%20Jonathan%20P.%20How%0AAbstract%3A%20%20%20This%20paper%20introduces%20DYNUS%2C%20an%20uncertainty-aware%20trajectory%20planner%20designed%0Afor%20dynamic%20unknown%20environments.%20Operating%20in%20such%20settings%20presents%20many%0Achallenges%20--%20most%20notably%2C%20because%20the%20agent%20cannot%20predict%20the%20ground-truth%0Afuture%20paths%20of%20obstacles%2C%20a%20previously%20planned%20trajectory%20can%20become%20unsafe%20at%0Aany%20moment%2C%20requiring%20rapid%20replanning%20to%20avoid%20collisions.%0A%20%20Recently%20developed%20planners%20have%20used%20soft-constraint%20approaches%20to%20achieve%0Athe%20necessary%20fast%20computation%20times%3B%20however%2C%20these%20methods%20do%20not%20guarantee%0Acollision-free%20paths%20even%20with%20static%20obstacles.%20In%20contrast%2C%20hard-constraint%0Amethods%20ensure%20collision-free%20safety%2C%20but%20typically%20have%20longer%20computation%0Atimes.%0A%20%20To%20address%20these%20issues%2C%20we%20propose%20three%20key%20contributions.%20First%2C%20the%20DYNUS%0AGlobal%20Planner%20%28DGP%29%20and%20Temporal%20Safe%20Corridor%20Generation%20operate%20in%0Aspatio-temporal%20space%20and%20handle%20both%20static%20and%20dynamic%20obstacles%20in%20the%203D%0Aenvironment.%20Second%2C%20the%20Safe%20Planning%20Framework%20leverages%20a%20combination%20of%0Aexploratory%2C%20safe%2C%20and%20contingency%20trajectories%20to%20flexibly%20re-route%20when%0Apotential%20future%20collisions%20with%20dynamic%20obstacles%20are%20detected.%20Finally%2C%20the%0AFast%20Hard-Constraint%20Local%20Trajectory%20Formulation%20uses%20a%20variable%20elimination%0Aapproach%20to%20reduce%20the%20problem%20size%20and%20enable%20faster%20computation%20by%0Apre-computing%20dependencies%20between%20free%20and%20dependent%20variables%20while%20still%0Aensuring%20collision-free%20trajectories.%0A%20%20We%20evaluated%20DYNUS%20in%20a%20variety%20of%20simulations%2C%20including%20dense%20forests%2C%0Aconfined%20office%20spaces%2C%20cave%20systems%2C%20and%20dynamic%20environments.%20Our%20experiments%0Ashow%20that%20DYNUS%20achieves%20a%20success%20rate%20of%20100%25%20and%20travel%20times%20that%20are%0Aapproximately%2025.0%25%20faster%20than%20state-of-the-art%20methods.%20We%20also%20evaluated%0ADYNUS%20on%20multiple%20platforms%20--%20a%20quadrotor%2C%20a%20wheeled%20robot%2C%20and%20a%20quadruped%20--%0Ain%20both%20simulation%20and%20hardware%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDYNUS%253A%2520Uncertainty-aware%2520Trajectory%2520Planner%2520in%2520Dynamic%2520Unknown%250A%2520%2520Environments%26entry.906535625%3DKota%2520Kondo%2520and%2520Mason%2520Peterson%2520and%2520Nicholas%2520Rober%2520and%2520Juan%2520Rached%2520Viso%2520and%2520Lucas%2520Jia%2520and%2520Jialin%2520Chen%2520and%2520Harvey%2520Merton%2520and%2520Jonathan%2520P.%2520How%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520DYNUS%252C%2520an%2520uncertainty-aware%2520trajectory%2520planner%2520designed%250Afor%2520dynamic%2520unknown%2520environments.%2520Operating%2520in%2520such%2520settings%2520presents%2520many%250Achallenges%2520--%2520most%2520notably%252C%2520because%2520the%2520agent%2520cannot%2520predict%2520the%2520ground-truth%250Afuture%2520paths%2520of%2520obstacles%252C%2520a%2520previously%2520planned%2520trajectory%2520can%2520become%2520unsafe%2520at%250Aany%2520moment%252C%2520requiring%2520rapid%2520replanning%2520to%2520avoid%2520collisions.%250A%2520%2520Recently%2520developed%2520planners%2520have%2520used%2520soft-constraint%2520approaches%2520to%2520achieve%250Athe%2520necessary%2520fast%2520computation%2520times%253B%2520however%252C%2520these%2520methods%2520do%2520not%2520guarantee%250Acollision-free%2520paths%2520even%2520with%2520static%2520obstacles.%2520In%2520contrast%252C%2520hard-constraint%250Amethods%2520ensure%2520collision-free%2520safety%252C%2520but%2520typically%2520have%2520longer%2520computation%250Atimes.%250A%2520%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520three%2520key%2520contributions.%2520First%252C%2520the%2520DYNUS%250AGlobal%2520Planner%2520%2528DGP%2529%2520and%2520Temporal%2520Safe%2520Corridor%2520Generation%2520operate%2520in%250Aspatio-temporal%2520space%2520and%2520handle%2520both%2520static%2520and%2520dynamic%2520obstacles%2520in%2520the%25203D%250Aenvironment.%2520Second%252C%2520the%2520Safe%2520Planning%2520Framework%2520leverages%2520a%2520combination%2520of%250Aexploratory%252C%2520safe%252C%2520and%2520contingency%2520trajectories%2520to%2520flexibly%2520re-route%2520when%250Apotential%2520future%2520collisions%2520with%2520dynamic%2520obstacles%2520are%2520detected.%2520Finally%252C%2520the%250AFast%2520Hard-Constraint%2520Local%2520Trajectory%2520Formulation%2520uses%2520a%2520variable%2520elimination%250Aapproach%2520to%2520reduce%2520the%2520problem%2520size%2520and%2520enable%2520faster%2520computation%2520by%250Apre-computing%2520dependencies%2520between%2520free%2520and%2520dependent%2520variables%2520while%2520still%250Aensuring%2520collision-free%2520trajectories.%250A%2520%2520We%2520evaluated%2520DYNUS%2520in%2520a%2520variety%2520of%2520simulations%252C%2520including%2520dense%2520forests%252C%250Aconfined%2520office%2520spaces%252C%2520cave%2520systems%252C%2520and%2520dynamic%2520environments.%2520Our%2520experiments%250Ashow%2520that%2520DYNUS%2520achieves%2520a%2520success%2520rate%2520of%2520100%2525%2520and%2520travel%2520times%2520that%2520are%250Aapproximately%252025.0%2525%2520faster%2520than%2520state-of-the-art%2520methods.%2520We%2520also%2520evaluated%250ADYNUS%2520on%2520multiple%2520platforms%2520--%2520a%2520quadrotor%252C%2520a%2520wheeled%2520robot%252C%2520and%2520a%2520quadruped%2520--%250Ain%2520both%2520simulation%2520and%2520hardware%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DYNUS%3A%20Uncertainty-aware%20Trajectory%20Planner%20in%20Dynamic%20Unknown%0A%20%20Environments&entry.906535625=Kota%20Kondo%20and%20Mason%20Peterson%20and%20Nicholas%20Rober%20and%20Juan%20Rached%20Viso%20and%20Lucas%20Jia%20and%20Jialin%20Chen%20and%20Harvey%20Merton%20and%20Jonathan%20P.%20How&entry.1292438233=%20%20This%20paper%20introduces%20DYNUS%2C%20an%20uncertainty-aware%20trajectory%20planner%20designed%0Afor%20dynamic%20unknown%20environments.%20Operating%20in%20such%20settings%20presents%20many%0Achallenges%20--%20most%20notably%2C%20because%20the%20agent%20cannot%20predict%20the%20ground-truth%0Afuture%20paths%20of%20obstacles%2C%20a%20previously%20planned%20trajectory%20can%20become%20unsafe%20at%0Aany%20moment%2C%20requiring%20rapid%20replanning%20to%20avoid%20collisions.%0A%20%20Recently%20developed%20planners%20have%20used%20soft-constraint%20approaches%20to%20achieve%0Athe%20necessary%20fast%20computation%20times%3B%20however%2C%20these%20methods%20do%20not%20guarantee%0Acollision-free%20paths%20even%20with%20static%20obstacles.%20In%20contrast%2C%20hard-constraint%0Amethods%20ensure%20collision-free%20safety%2C%20but%20typically%20have%20longer%20computation%0Atimes.%0A%20%20To%20address%20these%20issues%2C%20we%20propose%20three%20key%20contributions.%20First%2C%20the%20DYNUS%0AGlobal%20Planner%20%28DGP%29%20and%20Temporal%20Safe%20Corridor%20Generation%20operate%20in%0Aspatio-temporal%20space%20and%20handle%20both%20static%20and%20dynamic%20obstacles%20in%20the%203D%0Aenvironment.%20Second%2C%20the%20Safe%20Planning%20Framework%20leverages%20a%20combination%20of%0Aexploratory%2C%20safe%2C%20and%20contingency%20trajectories%20to%20flexibly%20re-route%20when%0Apotential%20future%20collisions%20with%20dynamic%20obstacles%20are%20detected.%20Finally%2C%20the%0AFast%20Hard-Constraint%20Local%20Trajectory%20Formulation%20uses%20a%20variable%20elimination%0Aapproach%20to%20reduce%20the%20problem%20size%20and%20enable%20faster%20computation%20by%0Apre-computing%20dependencies%20between%20free%20and%20dependent%20variables%20while%20still%0Aensuring%20collision-free%20trajectories.%0A%20%20We%20evaluated%20DYNUS%20in%20a%20variety%20of%20simulations%2C%20including%20dense%20forests%2C%0Aconfined%20office%20spaces%2C%20cave%20systems%2C%20and%20dynamic%20environments.%20Our%20experiments%0Ashow%20that%20DYNUS%20achieves%20a%20success%20rate%20of%20100%25%20and%20travel%20times%20that%20are%0Aapproximately%2025.0%25%20faster%20than%20state-of-the-art%20methods.%20We%20also%20evaluated%0ADYNUS%20on%20multiple%20platforms%20--%20a%20quadrotor%2C%20a%20wheeled%20robot%2C%20and%20a%20quadruped%20--%0Ain%20both%20simulation%20and%20hardware%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16734v1&entry.124074799=Read"},
{"title": "Application of an attention-based CNN-BiLSTM framework for in vivo\n  two-photon calcium imaging of neuronal ensembles: decoding complex bilateral\n  forelimb movements from unilateral M1", "author": "Ghazal Mirzaee and Jonathan Chang and Shahrzad Latifi", "abstract": "  Decoding behavior, such as movement, from multiscale brain networks remains a\ncentral objective in neuroscience. Over the past decades, artificial\nintelligence and machine learning have played an increasingly significant role\nin elucidating the neural mechanisms underlying motor function. The advancement\nof brain-monitoring technologies, capable of capturing complex neuronal signals\nwith high spatial and temporal resolution, necessitates the development and\napplication of more sophisticated machine learning models for behavioral\ndecoding. In this study, we employ a hybrid deep learning framework, an\nattention-based CNN-BiLSTM model, to decode skilled and complex forelimb\nmovements using signals obtained from in vivo two-photon calcium imaging. Our\nfindings demonstrate that the intricate movements of both ipsilateral and\ncontralateral forelimbs can be accurately decoded from unilateral M1 neuronal\nensembles. These results highlight the efficacy of advanced hybrid deep\nlearning models in capturing the spatiotemporal dependencies of neuronal\nnetworks activity linked to complex movement execution.\n", "link": "http://arxiv.org/abs/2504.16917v1", "date": "2025-04-23", "relevancy": 1.6054, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5385}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20an%20attention-based%20CNN-BiLSTM%20framework%20for%20in%20vivo%0A%20%20two-photon%20calcium%20imaging%20of%20neuronal%20ensembles%3A%20decoding%20complex%20bilateral%0A%20%20forelimb%20movements%20from%20unilateral%20M1&body=Title%3A%20Application%20of%20an%20attention-based%20CNN-BiLSTM%20framework%20for%20in%20vivo%0A%20%20two-photon%20calcium%20imaging%20of%20neuronal%20ensembles%3A%20decoding%20complex%20bilateral%0A%20%20forelimb%20movements%20from%20unilateral%20M1%0AAuthor%3A%20Ghazal%20Mirzaee%20and%20Jonathan%20Chang%20and%20Shahrzad%20Latifi%0AAbstract%3A%20%20%20Decoding%20behavior%2C%20such%20as%20movement%2C%20from%20multiscale%20brain%20networks%20remains%20a%0Acentral%20objective%20in%20neuroscience.%20Over%20the%20past%20decades%2C%20artificial%0Aintelligence%20and%20machine%20learning%20have%20played%20an%20increasingly%20significant%20role%0Ain%20elucidating%20the%20neural%20mechanisms%20underlying%20motor%20function.%20The%20advancement%0Aof%20brain-monitoring%20technologies%2C%20capable%20of%20capturing%20complex%20neuronal%20signals%0Awith%20high%20spatial%20and%20temporal%20resolution%2C%20necessitates%20the%20development%20and%0Aapplication%20of%20more%20sophisticated%20machine%20learning%20models%20for%20behavioral%0Adecoding.%20In%20this%20study%2C%20we%20employ%20a%20hybrid%20deep%20learning%20framework%2C%20an%0Aattention-based%20CNN-BiLSTM%20model%2C%20to%20decode%20skilled%20and%20complex%20forelimb%0Amovements%20using%20signals%20obtained%20from%20in%20vivo%20two-photon%20calcium%20imaging.%20Our%0Afindings%20demonstrate%20that%20the%20intricate%20movements%20of%20both%20ipsilateral%20and%0Acontralateral%20forelimbs%20can%20be%20accurately%20decoded%20from%20unilateral%20M1%20neuronal%0Aensembles.%20These%20results%20highlight%20the%20efficacy%20of%20advanced%20hybrid%20deep%0Alearning%20models%20in%20capturing%20the%20spatiotemporal%20dependencies%20of%20neuronal%0Anetworks%20activity%20linked%20to%20complex%20movement%20execution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520an%2520attention-based%2520CNN-BiLSTM%2520framework%2520for%2520in%2520vivo%250A%2520%2520two-photon%2520calcium%2520imaging%2520of%2520neuronal%2520ensembles%253A%2520decoding%2520complex%2520bilateral%250A%2520%2520forelimb%2520movements%2520from%2520unilateral%2520M1%26entry.906535625%3DGhazal%2520Mirzaee%2520and%2520Jonathan%2520Chang%2520and%2520Shahrzad%2520Latifi%26entry.1292438233%3D%2520%2520Decoding%2520behavior%252C%2520such%2520as%2520movement%252C%2520from%2520multiscale%2520brain%2520networks%2520remains%2520a%250Acentral%2520objective%2520in%2520neuroscience.%2520Over%2520the%2520past%2520decades%252C%2520artificial%250Aintelligence%2520and%2520machine%2520learning%2520have%2520played%2520an%2520increasingly%2520significant%2520role%250Ain%2520elucidating%2520the%2520neural%2520mechanisms%2520underlying%2520motor%2520function.%2520The%2520advancement%250Aof%2520brain-monitoring%2520technologies%252C%2520capable%2520of%2520capturing%2520complex%2520neuronal%2520signals%250Awith%2520high%2520spatial%2520and%2520temporal%2520resolution%252C%2520necessitates%2520the%2520development%2520and%250Aapplication%2520of%2520more%2520sophisticated%2520machine%2520learning%2520models%2520for%2520behavioral%250Adecoding.%2520In%2520this%2520study%252C%2520we%2520employ%2520a%2520hybrid%2520deep%2520learning%2520framework%252C%2520an%250Aattention-based%2520CNN-BiLSTM%2520model%252C%2520to%2520decode%2520skilled%2520and%2520complex%2520forelimb%250Amovements%2520using%2520signals%2520obtained%2520from%2520in%2520vivo%2520two-photon%2520calcium%2520imaging.%2520Our%250Afindings%2520demonstrate%2520that%2520the%2520intricate%2520movements%2520of%2520both%2520ipsilateral%2520and%250Acontralateral%2520forelimbs%2520can%2520be%2520accurately%2520decoded%2520from%2520unilateral%2520M1%2520neuronal%250Aensembles.%2520These%2520results%2520highlight%2520the%2520efficacy%2520of%2520advanced%2520hybrid%2520deep%250Alearning%2520models%2520in%2520capturing%2520the%2520spatiotemporal%2520dependencies%2520of%2520neuronal%250Anetworks%2520activity%2520linked%2520to%2520complex%2520movement%2520execution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20an%20attention-based%20CNN-BiLSTM%20framework%20for%20in%20vivo%0A%20%20two-photon%20calcium%20imaging%20of%20neuronal%20ensembles%3A%20decoding%20complex%20bilateral%0A%20%20forelimb%20movements%20from%20unilateral%20M1&entry.906535625=Ghazal%20Mirzaee%20and%20Jonathan%20Chang%20and%20Shahrzad%20Latifi&entry.1292438233=%20%20Decoding%20behavior%2C%20such%20as%20movement%2C%20from%20multiscale%20brain%20networks%20remains%20a%0Acentral%20objective%20in%20neuroscience.%20Over%20the%20past%20decades%2C%20artificial%0Aintelligence%20and%20machine%20learning%20have%20played%20an%20increasingly%20significant%20role%0Ain%20elucidating%20the%20neural%20mechanisms%20underlying%20motor%20function.%20The%20advancement%0Aof%20brain-monitoring%20technologies%2C%20capable%20of%20capturing%20complex%20neuronal%20signals%0Awith%20high%20spatial%20and%20temporal%20resolution%2C%20necessitates%20the%20development%20and%0Aapplication%20of%20more%20sophisticated%20machine%20learning%20models%20for%20behavioral%0Adecoding.%20In%20this%20study%2C%20we%20employ%20a%20hybrid%20deep%20learning%20framework%2C%20an%0Aattention-based%20CNN-BiLSTM%20model%2C%20to%20decode%20skilled%20and%20complex%20forelimb%0Amovements%20using%20signals%20obtained%20from%20in%20vivo%20two-photon%20calcium%20imaging.%20Our%0Afindings%20demonstrate%20that%20the%20intricate%20movements%20of%20both%20ipsilateral%20and%0Acontralateral%20forelimbs%20can%20be%20accurately%20decoded%20from%20unilateral%20M1%20neuronal%0Aensembles.%20These%20results%20highlight%20the%20efficacy%20of%20advanced%20hybrid%20deep%0Alearning%20models%20in%20capturing%20the%20spatiotemporal%20dependencies%20of%20neuronal%0Anetworks%20activity%20linked%20to%20complex%20movement%20execution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16917v1&entry.124074799=Read"},
{"title": "MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon\n  Manipulation Planning", "author": "Itamar Mishani and Yorai Shaoul and Maxim Likhachev", "abstract": "  Planning long-horizon motions using a set of predefined skills is a key\nchallenge in robotics and AI. Addressing this challenge requires methods that\nsystematically explore skill combinations to uncover task-solving sequences,\nharness generic, easy-to-learn skills (e.g., pushing, grasping) to generalize\nacross unseen tasks, and bypass reliance on symbolic world representations that\ndemand extensive domain and task-specific knowledge. Despite significant\nprogress, these elements remain largely disjoint in existing approaches,\nleaving a critical gap in achieving robust, scalable solutions for complex,\nlong-horizon problems. In this work, we present MOSAIC, a skill-centric\nframework that unifies these elements by using the skills themselves to guide\nthe planning process. MOSAIC uses two families of skills: Generators compute\nexecutable trajectories and world configurations, and Connectors link these\nindependently generated skill trajectories by solving boundary value problems,\nenabling progress toward completing the overall task. By breaking away from the\nconventional paradigm of incrementally discovering skills from predefined start\nor goal states--a limitation that significantly restricts exploration--MOSAIC\nfocuses planning efforts on regions where skills are inherently effective. We\ndemonstrate the efficacy of MOSAIC in both simulated and real-world robotic\nmanipulation tasks, showcasing its ability to solve complex long-horizon\nplanning problems using a diverse set of skills incorporating generative\ndiffusion models, motion planning algorithms, and manipulation-specific models.\nVisit https://skill-mosaic.github.io for demonstrations and examples.\n", "link": "http://arxiv.org/abs/2504.16738v1", "date": "2025-04-23", "relevancy": 1.5981, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6109}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5141}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOSAIC%3A%20A%20Skill-Centric%20Algorithmic%20Framework%20for%20Long-Horizon%0A%20%20Manipulation%20Planning&body=Title%3A%20MOSAIC%3A%20A%20Skill-Centric%20Algorithmic%20Framework%20for%20Long-Horizon%0A%20%20Manipulation%20Planning%0AAuthor%3A%20Itamar%20Mishani%20and%20Yorai%20Shaoul%20and%20Maxim%20Likhachev%0AAbstract%3A%20%20%20Planning%20long-horizon%20motions%20using%20a%20set%20of%20predefined%20skills%20is%20a%20key%0Achallenge%20in%20robotics%20and%20AI.%20Addressing%20this%20challenge%20requires%20methods%20that%0Asystematically%20explore%20skill%20combinations%20to%20uncover%20task-solving%20sequences%2C%0Aharness%20generic%2C%20easy-to-learn%20skills%20%28e.g.%2C%20pushing%2C%20grasping%29%20to%20generalize%0Aacross%20unseen%20tasks%2C%20and%20bypass%20reliance%20on%20symbolic%20world%20representations%20that%0Ademand%20extensive%20domain%20and%20task-specific%20knowledge.%20Despite%20significant%0Aprogress%2C%20these%20elements%20remain%20largely%20disjoint%20in%20existing%20approaches%2C%0Aleaving%20a%20critical%20gap%20in%20achieving%20robust%2C%20scalable%20solutions%20for%20complex%2C%0Along-horizon%20problems.%20In%20this%20work%2C%20we%20present%20MOSAIC%2C%20a%20skill-centric%0Aframework%20that%20unifies%20these%20elements%20by%20using%20the%20skills%20themselves%20to%20guide%0Athe%20planning%20process.%20MOSAIC%20uses%20two%20families%20of%20skills%3A%20Generators%20compute%0Aexecutable%20trajectories%20and%20world%20configurations%2C%20and%20Connectors%20link%20these%0Aindependently%20generated%20skill%20trajectories%20by%20solving%20boundary%20value%20problems%2C%0Aenabling%20progress%20toward%20completing%20the%20overall%20task.%20By%20breaking%20away%20from%20the%0Aconventional%20paradigm%20of%20incrementally%20discovering%20skills%20from%20predefined%20start%0Aor%20goal%20states--a%20limitation%20that%20significantly%20restricts%20exploration--MOSAIC%0Afocuses%20planning%20efforts%20on%20regions%20where%20skills%20are%20inherently%20effective.%20We%0Ademonstrate%20the%20efficacy%20of%20MOSAIC%20in%20both%20simulated%20and%20real-world%20robotic%0Amanipulation%20tasks%2C%20showcasing%20its%20ability%20to%20solve%20complex%20long-horizon%0Aplanning%20problems%20using%20a%20diverse%20set%20of%20skills%20incorporating%20generative%0Adiffusion%20models%2C%20motion%20planning%20algorithms%2C%20and%20manipulation-specific%20models.%0AVisit%20https%3A//skill-mosaic.github.io%20for%20demonstrations%20and%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOSAIC%253A%2520A%2520Skill-Centric%2520Algorithmic%2520Framework%2520for%2520Long-Horizon%250A%2520%2520Manipulation%2520Planning%26entry.906535625%3DItamar%2520Mishani%2520and%2520Yorai%2520Shaoul%2520and%2520Maxim%2520Likhachev%26entry.1292438233%3D%2520%2520Planning%2520long-horizon%2520motions%2520using%2520a%2520set%2520of%2520predefined%2520skills%2520is%2520a%2520key%250Achallenge%2520in%2520robotics%2520and%2520AI.%2520Addressing%2520this%2520challenge%2520requires%2520methods%2520that%250Asystematically%2520explore%2520skill%2520combinations%2520to%2520uncover%2520task-solving%2520sequences%252C%250Aharness%2520generic%252C%2520easy-to-learn%2520skills%2520%2528e.g.%252C%2520pushing%252C%2520grasping%2529%2520to%2520generalize%250Aacross%2520unseen%2520tasks%252C%2520and%2520bypass%2520reliance%2520on%2520symbolic%2520world%2520representations%2520that%250Ademand%2520extensive%2520domain%2520and%2520task-specific%2520knowledge.%2520Despite%2520significant%250Aprogress%252C%2520these%2520elements%2520remain%2520largely%2520disjoint%2520in%2520existing%2520approaches%252C%250Aleaving%2520a%2520critical%2520gap%2520in%2520achieving%2520robust%252C%2520scalable%2520solutions%2520for%2520complex%252C%250Along-horizon%2520problems.%2520In%2520this%2520work%252C%2520we%2520present%2520MOSAIC%252C%2520a%2520skill-centric%250Aframework%2520that%2520unifies%2520these%2520elements%2520by%2520using%2520the%2520skills%2520themselves%2520to%2520guide%250Athe%2520planning%2520process.%2520MOSAIC%2520uses%2520two%2520families%2520of%2520skills%253A%2520Generators%2520compute%250Aexecutable%2520trajectories%2520and%2520world%2520configurations%252C%2520and%2520Connectors%2520link%2520these%250Aindependently%2520generated%2520skill%2520trajectories%2520by%2520solving%2520boundary%2520value%2520problems%252C%250Aenabling%2520progress%2520toward%2520completing%2520the%2520overall%2520task.%2520By%2520breaking%2520away%2520from%2520the%250Aconventional%2520paradigm%2520of%2520incrementally%2520discovering%2520skills%2520from%2520predefined%2520start%250Aor%2520goal%2520states--a%2520limitation%2520that%2520significantly%2520restricts%2520exploration--MOSAIC%250Afocuses%2520planning%2520efforts%2520on%2520regions%2520where%2520skills%2520are%2520inherently%2520effective.%2520We%250Ademonstrate%2520the%2520efficacy%2520of%2520MOSAIC%2520in%2520both%2520simulated%2520and%2520real-world%2520robotic%250Amanipulation%2520tasks%252C%2520showcasing%2520its%2520ability%2520to%2520solve%2520complex%2520long-horizon%250Aplanning%2520problems%2520using%2520a%2520diverse%2520set%2520of%2520skills%2520incorporating%2520generative%250Adiffusion%2520models%252C%2520motion%2520planning%2520algorithms%252C%2520and%2520manipulation-specific%2520models.%250AVisit%2520https%253A//skill-mosaic.github.io%2520for%2520demonstrations%2520and%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOSAIC%3A%20A%20Skill-Centric%20Algorithmic%20Framework%20for%20Long-Horizon%0A%20%20Manipulation%20Planning&entry.906535625=Itamar%20Mishani%20and%20Yorai%20Shaoul%20and%20Maxim%20Likhachev&entry.1292438233=%20%20Planning%20long-horizon%20motions%20using%20a%20set%20of%20predefined%20skills%20is%20a%20key%0Achallenge%20in%20robotics%20and%20AI.%20Addressing%20this%20challenge%20requires%20methods%20that%0Asystematically%20explore%20skill%20combinations%20to%20uncover%20task-solving%20sequences%2C%0Aharness%20generic%2C%20easy-to-learn%20skills%20%28e.g.%2C%20pushing%2C%20grasping%29%20to%20generalize%0Aacross%20unseen%20tasks%2C%20and%20bypass%20reliance%20on%20symbolic%20world%20representations%20that%0Ademand%20extensive%20domain%20and%20task-specific%20knowledge.%20Despite%20significant%0Aprogress%2C%20these%20elements%20remain%20largely%20disjoint%20in%20existing%20approaches%2C%0Aleaving%20a%20critical%20gap%20in%20achieving%20robust%2C%20scalable%20solutions%20for%20complex%2C%0Along-horizon%20problems.%20In%20this%20work%2C%20we%20present%20MOSAIC%2C%20a%20skill-centric%0Aframework%20that%20unifies%20these%20elements%20by%20using%20the%20skills%20themselves%20to%20guide%0Athe%20planning%20process.%20MOSAIC%20uses%20two%20families%20of%20skills%3A%20Generators%20compute%0Aexecutable%20trajectories%20and%20world%20configurations%2C%20and%20Connectors%20link%20these%0Aindependently%20generated%20skill%20trajectories%20by%20solving%20boundary%20value%20problems%2C%0Aenabling%20progress%20toward%20completing%20the%20overall%20task.%20By%20breaking%20away%20from%20the%0Aconventional%20paradigm%20of%20incrementally%20discovering%20skills%20from%20predefined%20start%0Aor%20goal%20states--a%20limitation%20that%20significantly%20restricts%20exploration--MOSAIC%0Afocuses%20planning%20efforts%20on%20regions%20where%20skills%20are%20inherently%20effective.%20We%0Ademonstrate%20the%20efficacy%20of%20MOSAIC%20in%20both%20simulated%20and%20real-world%20robotic%0Amanipulation%20tasks%2C%20showcasing%20its%20ability%20to%20solve%20complex%20long-horizon%0Aplanning%20problems%20using%20a%20diverse%20set%20of%20skills%20incorporating%20generative%0Adiffusion%20models%2C%20motion%20planning%20algorithms%2C%20and%20manipulation-specific%20models.%0AVisit%20https%3A//skill-mosaic.github.io%20for%20demonstrations%20and%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16738v1&entry.124074799=Read"},
{"title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement\n  Learning for Strategic Confrontation", "author": "Qizhen Wu and Lei Chen and Kexin Liu and Jinhu L\u00fc", "abstract": "  In swarm robotics, confrontation scenarios, including strategic\nconfrontations, require efficient decision-making that integrates discrete\ncommands and continuous actions. Traditional task and motion planning methods\nseparate decision-making into two layers, but their unidirectional structure\nfails to capture the interdependence between these layers, limiting\nadaptability in dynamic environments. Here, we propose a novel bidirectional\napproach based on hierarchical reinforcement learning, enabling dynamic\ninteraction between the layers. This method effectively maps commands to task\nallocation and actions to path planning, while leveraging cross-training\ntechniques to enhance learning across the hierarchical framework. Furthermore,\nwe introduce a trajectory prediction model that bridges abstract task\nrepresentations with actionable planning goals. In our experiments, it achieves\nover 80% in confrontation win rate and under 0.01 seconds in decision time,\noutperforming existing approaches. Demonstrations through large-scale tests and\nreal-world robot experiments further emphasize the generalization capabilities\nand practical applicability of our method.\n", "link": "http://arxiv.org/abs/2504.15876v2", "date": "2025-04-23", "relevancy": 1.5861, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5538}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5441}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bidirectional%20Task-Motion%20Planning%20Based%20on%20Hierarchical%20Reinforcement%0A%20%20Learning%20for%20Strategic%20Confrontation&body=Title%3A%20Bidirectional%20Task-Motion%20Planning%20Based%20on%20Hierarchical%20Reinforcement%0A%20%20Learning%20for%20Strategic%20Confrontation%0AAuthor%3A%20Qizhen%20Wu%20and%20Lei%20Chen%20and%20Kexin%20Liu%20and%20Jinhu%20L%C3%BC%0AAbstract%3A%20%20%20In%20swarm%20robotics%2C%20confrontation%20scenarios%2C%20including%20strategic%0Aconfrontations%2C%20require%20efficient%20decision-making%20that%20integrates%20discrete%0Acommands%20and%20continuous%20actions.%20Traditional%20task%20and%20motion%20planning%20methods%0Aseparate%20decision-making%20into%20two%20layers%2C%20but%20their%20unidirectional%20structure%0Afails%20to%20capture%20the%20interdependence%20between%20these%20layers%2C%20limiting%0Aadaptability%20in%20dynamic%20environments.%20Here%2C%20we%20propose%20a%20novel%20bidirectional%0Aapproach%20based%20on%20hierarchical%20reinforcement%20learning%2C%20enabling%20dynamic%0Ainteraction%20between%20the%20layers.%20This%20method%20effectively%20maps%20commands%20to%20task%0Aallocation%20and%20actions%20to%20path%20planning%2C%20while%20leveraging%20cross-training%0Atechniques%20to%20enhance%20learning%20across%20the%20hierarchical%20framework.%20Furthermore%2C%0Awe%20introduce%20a%20trajectory%20prediction%20model%20that%20bridges%20abstract%20task%0Arepresentations%20with%20actionable%20planning%20goals.%20In%20our%20experiments%2C%20it%20achieves%0Aover%2080%25%20in%20confrontation%20win%20rate%20and%20under%200.01%20seconds%20in%20decision%20time%2C%0Aoutperforming%20existing%20approaches.%20Demonstrations%20through%20large-scale%20tests%20and%0Areal-world%20robot%20experiments%20further%20emphasize%20the%20generalization%20capabilities%0Aand%20practical%20applicability%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBidirectional%2520Task-Motion%2520Planning%2520Based%2520on%2520Hierarchical%2520Reinforcement%250A%2520%2520Learning%2520for%2520Strategic%2520Confrontation%26entry.906535625%3DQizhen%2520Wu%2520and%2520Lei%2520Chen%2520and%2520Kexin%2520Liu%2520and%2520Jinhu%2520L%25C3%25BC%26entry.1292438233%3D%2520%2520In%2520swarm%2520robotics%252C%2520confrontation%2520scenarios%252C%2520including%2520strategic%250Aconfrontations%252C%2520require%2520efficient%2520decision-making%2520that%2520integrates%2520discrete%250Acommands%2520and%2520continuous%2520actions.%2520Traditional%2520task%2520and%2520motion%2520planning%2520methods%250Aseparate%2520decision-making%2520into%2520two%2520layers%252C%2520but%2520their%2520unidirectional%2520structure%250Afails%2520to%2520capture%2520the%2520interdependence%2520between%2520these%2520layers%252C%2520limiting%250Aadaptability%2520in%2520dynamic%2520environments.%2520Here%252C%2520we%2520propose%2520a%2520novel%2520bidirectional%250Aapproach%2520based%2520on%2520hierarchical%2520reinforcement%2520learning%252C%2520enabling%2520dynamic%250Ainteraction%2520between%2520the%2520layers.%2520This%2520method%2520effectively%2520maps%2520commands%2520to%2520task%250Aallocation%2520and%2520actions%2520to%2520path%2520planning%252C%2520while%2520leveraging%2520cross-training%250Atechniques%2520to%2520enhance%2520learning%2520across%2520the%2520hierarchical%2520framework.%2520Furthermore%252C%250Awe%2520introduce%2520a%2520trajectory%2520prediction%2520model%2520that%2520bridges%2520abstract%2520task%250Arepresentations%2520with%2520actionable%2520planning%2520goals.%2520In%2520our%2520experiments%252C%2520it%2520achieves%250Aover%252080%2525%2520in%2520confrontation%2520win%2520rate%2520and%2520under%25200.01%2520seconds%2520in%2520decision%2520time%252C%250Aoutperforming%2520existing%2520approaches.%2520Demonstrations%2520through%2520large-scale%2520tests%2520and%250Areal-world%2520robot%2520experiments%2520further%2520emphasize%2520the%2520generalization%2520capabilities%250Aand%2520practical%2520applicability%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bidirectional%20Task-Motion%20Planning%20Based%20on%20Hierarchical%20Reinforcement%0A%20%20Learning%20for%20Strategic%20Confrontation&entry.906535625=Qizhen%20Wu%20and%20Lei%20Chen%20and%20Kexin%20Liu%20and%20Jinhu%20L%C3%BC&entry.1292438233=%20%20In%20swarm%20robotics%2C%20confrontation%20scenarios%2C%20including%20strategic%0Aconfrontations%2C%20require%20efficient%20decision-making%20that%20integrates%20discrete%0Acommands%20and%20continuous%20actions.%20Traditional%20task%20and%20motion%20planning%20methods%0Aseparate%20decision-making%20into%20two%20layers%2C%20but%20their%20unidirectional%20structure%0Afails%20to%20capture%20the%20interdependence%20between%20these%20layers%2C%20limiting%0Aadaptability%20in%20dynamic%20environments.%20Here%2C%20we%20propose%20a%20novel%20bidirectional%0Aapproach%20based%20on%20hierarchical%20reinforcement%20learning%2C%20enabling%20dynamic%0Ainteraction%20between%20the%20layers.%20This%20method%20effectively%20maps%20commands%20to%20task%0Aallocation%20and%20actions%20to%20path%20planning%2C%20while%20leveraging%20cross-training%0Atechniques%20to%20enhance%20learning%20across%20the%20hierarchical%20framework.%20Furthermore%2C%0Awe%20introduce%20a%20trajectory%20prediction%20model%20that%20bridges%20abstract%20task%0Arepresentations%20with%20actionable%20planning%20goals.%20In%20our%20experiments%2C%20it%20achieves%0Aover%2080%25%20in%20confrontation%20win%20rate%20and%20under%200.01%20seconds%20in%20decision%20time%2C%0Aoutperforming%20existing%20approaches.%20Demonstrations%20through%20large-scale%20tests%20and%0Areal-world%20robot%20experiments%20further%20emphasize%20the%20generalization%20capabilities%0Aand%20practical%20applicability%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15876v2&entry.124074799=Read"},
{"title": "Approximating Optimal Labelings for Temporal Connectivity", "author": "Daniele Carnevale and Gianlorenzo D'Angelo and Martin Olsen", "abstract": "  In a temporal graph the edge set dynamically changes over time according to a\nset of time-labels associated with each edge that indicates at which time-steps\nthe edge is available. Two vertices are connected if there is a path connecting\nthem in which the edges are traversed in increasing order of their labels. We\nstudy the problem of scheduling the availability time of the edges of a\ntemporal graph in such a way that all pairs of vertices are connected within a\ngiven maximum allowed time $a$ and the overall number of labels is minimized.\n  The problem, known as \\emph{Minimum Aged Labeling} (MAL), has several\napplications in logistics, distribution scheduling, and information spreading\nin social networks, where carefully choosing the time-labels can significantly\nreduce infrastructure costs, fuel consumption, or greenhouse gases.\n  The problem MAL has previously been proved to be NP-complete on undirected\ngraphs and \\APX-hard on directed graphs. In this paper, we extend our knowledge\non the complexity and approximability of MAL in several directions. We first\nshow that the problem cannot be approximated within a factor better than\n$O(\\log n)$ when $a\\geq 2$, unless $\\text{P} = \\text{NP}$, and a factor better\nthan $2^{\\log ^{1-\\epsilon} n}$ when $a\\geq 3$, unless $\\text{NP}\\subseteq\n\\text{DTIME}(2^{\\text{polylog}(n)})$, where $n$ is the number of vertices in\nthe graph. Then we give a set of approximation algorithms that, under some\nconditions, almost match these lower bounds. In particular, we show that the\napproximation depends on a relation between $a$ and the diameter of the input\ngraph.\n  We further establish a connection with a foundational optimization problem on\nstatic graphs called \\emph{Diameter Constrained Spanning Subgraph} (DCSS) and\nshow that our hardness results also apply to DCSS.\n", "link": "http://arxiv.org/abs/2504.16837v1", "date": "2025-04-23", "relevancy": 1.5525, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3941}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3847}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximating%20Optimal%20Labelings%20for%20Temporal%20Connectivity&body=Title%3A%20Approximating%20Optimal%20Labelings%20for%20Temporal%20Connectivity%0AAuthor%3A%20Daniele%20Carnevale%20and%20Gianlorenzo%20D%27Angelo%20and%20Martin%20Olsen%0AAbstract%3A%20%20%20In%20a%20temporal%20graph%20the%20edge%20set%20dynamically%20changes%20over%20time%20according%20to%20a%0Aset%20of%20time-labels%20associated%20with%20each%20edge%20that%20indicates%20at%20which%20time-steps%0Athe%20edge%20is%20available.%20Two%20vertices%20are%20connected%20if%20there%20is%20a%20path%20connecting%0Athem%20in%20which%20the%20edges%20are%20traversed%20in%20increasing%20order%20of%20their%20labels.%20We%0Astudy%20the%20problem%20of%20scheduling%20the%20availability%20time%20of%20the%20edges%20of%20a%0Atemporal%20graph%20in%20such%20a%20way%20that%20all%20pairs%20of%20vertices%20are%20connected%20within%20a%0Agiven%20maximum%20allowed%20time%20%24a%24%20and%20the%20overall%20number%20of%20labels%20is%20minimized.%0A%20%20The%20problem%2C%20known%20as%20%5Cemph%7BMinimum%20Aged%20Labeling%7D%20%28MAL%29%2C%20has%20several%0Aapplications%20in%20logistics%2C%20distribution%20scheduling%2C%20and%20information%20spreading%0Ain%20social%20networks%2C%20where%20carefully%20choosing%20the%20time-labels%20can%20significantly%0Areduce%20infrastructure%20costs%2C%20fuel%20consumption%2C%20or%20greenhouse%20gases.%0A%20%20The%20problem%20MAL%20has%20previously%20been%20proved%20to%20be%20NP-complete%20on%20undirected%0Agraphs%20and%20%5CAPX-hard%20on%20directed%20graphs.%20In%20this%20paper%2C%20we%20extend%20our%20knowledge%0Aon%20the%20complexity%20and%20approximability%20of%20MAL%20in%20several%20directions.%20We%20first%0Ashow%20that%20the%20problem%20cannot%20be%20approximated%20within%20a%20factor%20better%20than%0A%24O%28%5Clog%20n%29%24%20when%20%24a%5Cgeq%202%24%2C%20unless%20%24%5Ctext%7BP%7D%20%3D%20%5Ctext%7BNP%7D%24%2C%20and%20a%20factor%20better%0Athan%20%242%5E%7B%5Clog%20%5E%7B1-%5Cepsilon%7D%20n%7D%24%20when%20%24a%5Cgeq%203%24%2C%20unless%20%24%5Ctext%7BNP%7D%5Csubseteq%0A%5Ctext%7BDTIME%7D%282%5E%7B%5Ctext%7Bpolylog%7D%28n%29%7D%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20vertices%20in%0Athe%20graph.%20Then%20we%20give%20a%20set%20of%20approximation%20algorithms%20that%2C%20under%20some%0Aconditions%2C%20almost%20match%20these%20lower%20bounds.%20In%20particular%2C%20we%20show%20that%20the%0Aapproximation%20depends%20on%20a%20relation%20between%20%24a%24%20and%20the%20diameter%20of%20the%20input%0Agraph.%0A%20%20We%20further%20establish%20a%20connection%20with%20a%20foundational%20optimization%20problem%20on%0Astatic%20graphs%20called%20%5Cemph%7BDiameter%20Constrained%20Spanning%20Subgraph%7D%20%28DCSS%29%20and%0Ashow%20that%20our%20hardness%20results%20also%20apply%20to%20DCSS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximating%2520Optimal%2520Labelings%2520for%2520Temporal%2520Connectivity%26entry.906535625%3DDaniele%2520Carnevale%2520and%2520Gianlorenzo%2520D%2527Angelo%2520and%2520Martin%2520Olsen%26entry.1292438233%3D%2520%2520In%2520a%2520temporal%2520graph%2520the%2520edge%2520set%2520dynamically%2520changes%2520over%2520time%2520according%2520to%2520a%250Aset%2520of%2520time-labels%2520associated%2520with%2520each%2520edge%2520that%2520indicates%2520at%2520which%2520time-steps%250Athe%2520edge%2520is%2520available.%2520Two%2520vertices%2520are%2520connected%2520if%2520there%2520is%2520a%2520path%2520connecting%250Athem%2520in%2520which%2520the%2520edges%2520are%2520traversed%2520in%2520increasing%2520order%2520of%2520their%2520labels.%2520We%250Astudy%2520the%2520problem%2520of%2520scheduling%2520the%2520availability%2520time%2520of%2520the%2520edges%2520of%2520a%250Atemporal%2520graph%2520in%2520such%2520a%2520way%2520that%2520all%2520pairs%2520of%2520vertices%2520are%2520connected%2520within%2520a%250Agiven%2520maximum%2520allowed%2520time%2520%2524a%2524%2520and%2520the%2520overall%2520number%2520of%2520labels%2520is%2520minimized.%250A%2520%2520The%2520problem%252C%2520known%2520as%2520%255Cemph%257BMinimum%2520Aged%2520Labeling%257D%2520%2528MAL%2529%252C%2520has%2520several%250Aapplications%2520in%2520logistics%252C%2520distribution%2520scheduling%252C%2520and%2520information%2520spreading%250Ain%2520social%2520networks%252C%2520where%2520carefully%2520choosing%2520the%2520time-labels%2520can%2520significantly%250Areduce%2520infrastructure%2520costs%252C%2520fuel%2520consumption%252C%2520or%2520greenhouse%2520gases.%250A%2520%2520The%2520problem%2520MAL%2520has%2520previously%2520been%2520proved%2520to%2520be%2520NP-complete%2520on%2520undirected%250Agraphs%2520and%2520%255CAPX-hard%2520on%2520directed%2520graphs.%2520In%2520this%2520paper%252C%2520we%2520extend%2520our%2520knowledge%250Aon%2520the%2520complexity%2520and%2520approximability%2520of%2520MAL%2520in%2520several%2520directions.%2520We%2520first%250Ashow%2520that%2520the%2520problem%2520cannot%2520be%2520approximated%2520within%2520a%2520factor%2520better%2520than%250A%2524O%2528%255Clog%2520n%2529%2524%2520when%2520%2524a%255Cgeq%25202%2524%252C%2520unless%2520%2524%255Ctext%257BP%257D%2520%253D%2520%255Ctext%257BNP%257D%2524%252C%2520and%2520a%2520factor%2520better%250Athan%2520%25242%255E%257B%255Clog%2520%255E%257B1-%255Cepsilon%257D%2520n%257D%2524%2520when%2520%2524a%255Cgeq%25203%2524%252C%2520unless%2520%2524%255Ctext%257BNP%257D%255Csubseteq%250A%255Ctext%257BDTIME%257D%25282%255E%257B%255Ctext%257Bpolylog%257D%2528n%2529%257D%2529%2524%252C%2520where%2520%2524n%2524%2520is%2520the%2520number%2520of%2520vertices%2520in%250Athe%2520graph.%2520Then%2520we%2520give%2520a%2520set%2520of%2520approximation%2520algorithms%2520that%252C%2520under%2520some%250Aconditions%252C%2520almost%2520match%2520these%2520lower%2520bounds.%2520In%2520particular%252C%2520we%2520show%2520that%2520the%250Aapproximation%2520depends%2520on%2520a%2520relation%2520between%2520%2524a%2524%2520and%2520the%2520diameter%2520of%2520the%2520input%250Agraph.%250A%2520%2520We%2520further%2520establish%2520a%2520connection%2520with%2520a%2520foundational%2520optimization%2520problem%2520on%250Astatic%2520graphs%2520called%2520%255Cemph%257BDiameter%2520Constrained%2520Spanning%2520Subgraph%257D%2520%2528DCSS%2529%2520and%250Ashow%2520that%2520our%2520hardness%2520results%2520also%2520apply%2520to%2520DCSS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximating%20Optimal%20Labelings%20for%20Temporal%20Connectivity&entry.906535625=Daniele%20Carnevale%20and%20Gianlorenzo%20D%27Angelo%20and%20Martin%20Olsen&entry.1292438233=%20%20In%20a%20temporal%20graph%20the%20edge%20set%20dynamically%20changes%20over%20time%20according%20to%20a%0Aset%20of%20time-labels%20associated%20with%20each%20edge%20that%20indicates%20at%20which%20time-steps%0Athe%20edge%20is%20available.%20Two%20vertices%20are%20connected%20if%20there%20is%20a%20path%20connecting%0Athem%20in%20which%20the%20edges%20are%20traversed%20in%20increasing%20order%20of%20their%20labels.%20We%0Astudy%20the%20problem%20of%20scheduling%20the%20availability%20time%20of%20the%20edges%20of%20a%0Atemporal%20graph%20in%20such%20a%20way%20that%20all%20pairs%20of%20vertices%20are%20connected%20within%20a%0Agiven%20maximum%20allowed%20time%20%24a%24%20and%20the%20overall%20number%20of%20labels%20is%20minimized.%0A%20%20The%20problem%2C%20known%20as%20%5Cemph%7BMinimum%20Aged%20Labeling%7D%20%28MAL%29%2C%20has%20several%0Aapplications%20in%20logistics%2C%20distribution%20scheduling%2C%20and%20information%20spreading%0Ain%20social%20networks%2C%20where%20carefully%20choosing%20the%20time-labels%20can%20significantly%0Areduce%20infrastructure%20costs%2C%20fuel%20consumption%2C%20or%20greenhouse%20gases.%0A%20%20The%20problem%20MAL%20has%20previously%20been%20proved%20to%20be%20NP-complete%20on%20undirected%0Agraphs%20and%20%5CAPX-hard%20on%20directed%20graphs.%20In%20this%20paper%2C%20we%20extend%20our%20knowledge%0Aon%20the%20complexity%20and%20approximability%20of%20MAL%20in%20several%20directions.%20We%20first%0Ashow%20that%20the%20problem%20cannot%20be%20approximated%20within%20a%20factor%20better%20than%0A%24O%28%5Clog%20n%29%24%20when%20%24a%5Cgeq%202%24%2C%20unless%20%24%5Ctext%7BP%7D%20%3D%20%5Ctext%7BNP%7D%24%2C%20and%20a%20factor%20better%0Athan%20%242%5E%7B%5Clog%20%5E%7B1-%5Cepsilon%7D%20n%7D%24%20when%20%24a%5Cgeq%203%24%2C%20unless%20%24%5Ctext%7BNP%7D%5Csubseteq%0A%5Ctext%7BDTIME%7D%282%5E%7B%5Ctext%7Bpolylog%7D%28n%29%7D%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20vertices%20in%0Athe%20graph.%20Then%20we%20give%20a%20set%20of%20approximation%20algorithms%20that%2C%20under%20some%0Aconditions%2C%20almost%20match%20these%20lower%20bounds.%20In%20particular%2C%20we%20show%20that%20the%0Aapproximation%20depends%20on%20a%20relation%20between%20%24a%24%20and%20the%20diameter%20of%20the%20input%0Agraph.%0A%20%20We%20further%20establish%20a%20connection%20with%20a%20foundational%20optimization%20problem%20on%0Astatic%20graphs%20called%20%5Cemph%7BDiameter%20Constrained%20Spanning%20Subgraph%7D%20%28DCSS%29%20and%0Ashow%20that%20our%20hardness%20results%20also%20apply%20to%20DCSS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16837v1&entry.124074799=Read"},
{"title": "Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras", "author": "Nektarios A. Valous and Eckhard Hitzer and Drago\u015f Du\u015fe and Rodrigo Rojas Moraleda and Ferdinand Popp and Meggy Suarez-Carmona and Anna Berthel and Ismini Papageorgiou and Carlo Fremd and Alexander R\u00f6lle and Christina C. Westhoff and B\u00e9n\u00e9dicte Lenoir and Niels Halama and Inka Z\u00f6rnig and Dirk J\u00e4ger", "abstract": "  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n", "link": "http://arxiv.org/abs/2502.07758v4", "date": "2025-04-23", "relevancy": 1.5521, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.531}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5176}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20computational%20workflows%20for%20natural%20and%20biomedical%20image%0A%20%20processing%20based%20on%20hypercomplex%20algebras&body=Title%3A%20Novel%20computational%20workflows%20for%20natural%20and%20biomedical%20image%0A%20%20processing%20based%20on%20hypercomplex%20algebras%0AAuthor%3A%20Nektarios%20A.%20Valous%20and%20Eckhard%20Hitzer%20and%20Drago%C5%9F%20Du%C5%9Fe%20and%20Rodrigo%20Rojas%20Moraleda%20and%20Ferdinand%20Popp%20and%20Meggy%20Suarez-Carmona%20and%20Anna%20Berthel%20and%20Ismini%20Papageorgiou%20and%20Carlo%20Fremd%20and%20Alexander%20R%C3%B6lle%20and%20Christina%20C.%20Westhoff%20and%20B%C3%A9n%C3%A9dicte%20Lenoir%20and%20Niels%20Halama%20and%20Inka%20Z%C3%B6rnig%20and%20Dirk%20J%C3%A4ger%0AAbstract%3A%20%20%20Hypercomplex%20image%20processing%20extends%20conventional%20techniques%20in%20a%20unified%0Aparadigm%20encompassing%20algebraic%20and%20geometric%20principles.%20This%20work%20leverages%0Aquaternions%20and%20the%20two-dimensional%20orthogonal%20planes%20split%20framework%0A%28splitting%20of%20a%20quaternion%20-%20representing%20a%20pixel%20-%20into%20pairs%20of%20orthogonal%202D%0Aplanes%29%20for%20natural/biomedical%20image%20analysis%20through%20the%20following%0Acomputational%20workflows%20and%20outcomes%3A%20natural/biomedical%20image%20re-colorization%2C%0Anatural%20image%20de-colorization%2C%20natural/biomedical%20image%20contrast%20enhancement%2C%0Acomputational%20re-staining%20and%20stain%20separation%20in%20histological%20images%2C%20and%0Aperformance%20gains%20in%20machine/deep%20learning%20pipelines%20for%20histological%20images.%0AThe%20workflows%20are%20analyzed%20separately%20for%20natural%20and%20biomedical%20images%20to%0Ashowcase%20the%20effectiveness%20of%20the%20proposed%20approaches.%20The%20proposed%20workflows%0Acan%20regulate%20color%20appearance%20%28e.g.%20with%20alternative%20renditions%20and%20grayscale%0Aconversion%29%20and%20image%20contrast%2C%20be%20part%20of%20automated%20image%20processing%20pipelines%0A%28e.g.%20isolating%20stain%20components%2C%20boosting%20learning%20models%29%2C%20and%20assist%20in%0Adigital%20pathology%20applications%20%28e.g.%20enhancing%20biomarker%20visibility%2C%20enabling%0Acolorblind-friendly%20renditions%29.%20Employing%20only%20basic%20arithmetic%20and%20matrix%0Aoperations%2C%20this%20work%20offers%20a%20computationally%20accessible%20methodology%20-%20in%20the%0Ahypercomplex%20domain%20-%20that%20showcases%20versatility%20and%20consistency%20across%20image%0Aprocessing%20tasks%20and%20a%20range%20of%20computer%20vision%20and%20biomedical%20applications.%0AThe%20proposed%20non-data-driven%20methods%20achieve%20comparable%20or%20better%20results%0A%28particularly%20in%20cases%20involving%20well-known%20methods%29%20to%20those%20reported%20in%20the%0Aliterature%2C%20showcasing%20the%20potential%20of%20robust%20theoretical%20frameworks%20with%0Apractical%20effectiveness.%20Results%2C%20methods%2C%20and%20limitations%20are%20detailed%0Aalongside%20discussion%20of%20promising%20extensions%2C%20emphasizing%20the%20potential%20of%0Afeature-rich%20mathematical/computational%20frameworks%20for%20natural%20and%20biomedical%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07758v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520computational%2520workflows%2520for%2520natural%2520and%2520biomedical%2520image%250A%2520%2520processing%2520based%2520on%2520hypercomplex%2520algebras%26entry.906535625%3DNektarios%2520A.%2520Valous%2520and%2520Eckhard%2520Hitzer%2520and%2520Drago%25C5%259F%2520Du%25C5%259Fe%2520and%2520Rodrigo%2520Rojas%2520Moraleda%2520and%2520Ferdinand%2520Popp%2520and%2520Meggy%2520Suarez-Carmona%2520and%2520Anna%2520Berthel%2520and%2520Ismini%2520Papageorgiou%2520and%2520Carlo%2520Fremd%2520and%2520Alexander%2520R%25C3%25B6lle%2520and%2520Christina%2520C.%2520Westhoff%2520and%2520B%25C3%25A9n%25C3%25A9dicte%2520Lenoir%2520and%2520Niels%2520Halama%2520and%2520Inka%2520Z%25C3%25B6rnig%2520and%2520Dirk%2520J%25C3%25A4ger%26entry.1292438233%3D%2520%2520Hypercomplex%2520image%2520processing%2520extends%2520conventional%2520techniques%2520in%2520a%2520unified%250Aparadigm%2520encompassing%2520algebraic%2520and%2520geometric%2520principles.%2520This%2520work%2520leverages%250Aquaternions%2520and%2520the%2520two-dimensional%2520orthogonal%2520planes%2520split%2520framework%250A%2528splitting%2520of%2520a%2520quaternion%2520-%2520representing%2520a%2520pixel%2520-%2520into%2520pairs%2520of%2520orthogonal%25202D%250Aplanes%2529%2520for%2520natural/biomedical%2520image%2520analysis%2520through%2520the%2520following%250Acomputational%2520workflows%2520and%2520outcomes%253A%2520natural/biomedical%2520image%2520re-colorization%252C%250Anatural%2520image%2520de-colorization%252C%2520natural/biomedical%2520image%2520contrast%2520enhancement%252C%250Acomputational%2520re-staining%2520and%2520stain%2520separation%2520in%2520histological%2520images%252C%2520and%250Aperformance%2520gains%2520in%2520machine/deep%2520learning%2520pipelines%2520for%2520histological%2520images.%250AThe%2520workflows%2520are%2520analyzed%2520separately%2520for%2520natural%2520and%2520biomedical%2520images%2520to%250Ashowcase%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approaches.%2520The%2520proposed%2520workflows%250Acan%2520regulate%2520color%2520appearance%2520%2528e.g.%2520with%2520alternative%2520renditions%2520and%2520grayscale%250Aconversion%2529%2520and%2520image%2520contrast%252C%2520be%2520part%2520of%2520automated%2520image%2520processing%2520pipelines%250A%2528e.g.%2520isolating%2520stain%2520components%252C%2520boosting%2520learning%2520models%2529%252C%2520and%2520assist%2520in%250Adigital%2520pathology%2520applications%2520%2528e.g.%2520enhancing%2520biomarker%2520visibility%252C%2520enabling%250Acolorblind-friendly%2520renditions%2529.%2520Employing%2520only%2520basic%2520arithmetic%2520and%2520matrix%250Aoperations%252C%2520this%2520work%2520offers%2520a%2520computationally%2520accessible%2520methodology%2520-%2520in%2520the%250Ahypercomplex%2520domain%2520-%2520that%2520showcases%2520versatility%2520and%2520consistency%2520across%2520image%250Aprocessing%2520tasks%2520and%2520a%2520range%2520of%2520computer%2520vision%2520and%2520biomedical%2520applications.%250AThe%2520proposed%2520non-data-driven%2520methods%2520achieve%2520comparable%2520or%2520better%2520results%250A%2528particularly%2520in%2520cases%2520involving%2520well-known%2520methods%2529%2520to%2520those%2520reported%2520in%2520the%250Aliterature%252C%2520showcasing%2520the%2520potential%2520of%2520robust%2520theoretical%2520frameworks%2520with%250Apractical%2520effectiveness.%2520Results%252C%2520methods%252C%2520and%2520limitations%2520are%2520detailed%250Aalongside%2520discussion%2520of%2520promising%2520extensions%252C%2520emphasizing%2520the%2520potential%2520of%250Afeature-rich%2520mathematical/computational%2520frameworks%2520for%2520natural%2520and%2520biomedical%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07758v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20computational%20workflows%20for%20natural%20and%20biomedical%20image%0A%20%20processing%20based%20on%20hypercomplex%20algebras&entry.906535625=Nektarios%20A.%20Valous%20and%20Eckhard%20Hitzer%20and%20Drago%C5%9F%20Du%C5%9Fe%20and%20Rodrigo%20Rojas%20Moraleda%20and%20Ferdinand%20Popp%20and%20Meggy%20Suarez-Carmona%20and%20Anna%20Berthel%20and%20Ismini%20Papageorgiou%20and%20Carlo%20Fremd%20and%20Alexander%20R%C3%B6lle%20and%20Christina%20C.%20Westhoff%20and%20B%C3%A9n%C3%A9dicte%20Lenoir%20and%20Niels%20Halama%20and%20Inka%20Z%C3%B6rnig%20and%20Dirk%20J%C3%A4ger&entry.1292438233=%20%20Hypercomplex%20image%20processing%20extends%20conventional%20techniques%20in%20a%20unified%0Aparadigm%20encompassing%20algebraic%20and%20geometric%20principles.%20This%20work%20leverages%0Aquaternions%20and%20the%20two-dimensional%20orthogonal%20planes%20split%20framework%0A%28splitting%20of%20a%20quaternion%20-%20representing%20a%20pixel%20-%20into%20pairs%20of%20orthogonal%202D%0Aplanes%29%20for%20natural/biomedical%20image%20analysis%20through%20the%20following%0Acomputational%20workflows%20and%20outcomes%3A%20natural/biomedical%20image%20re-colorization%2C%0Anatural%20image%20de-colorization%2C%20natural/biomedical%20image%20contrast%20enhancement%2C%0Acomputational%20re-staining%20and%20stain%20separation%20in%20histological%20images%2C%20and%0Aperformance%20gains%20in%20machine/deep%20learning%20pipelines%20for%20histological%20images.%0AThe%20workflows%20are%20analyzed%20separately%20for%20natural%20and%20biomedical%20images%20to%0Ashowcase%20the%20effectiveness%20of%20the%20proposed%20approaches.%20The%20proposed%20workflows%0Acan%20regulate%20color%20appearance%20%28e.g.%20with%20alternative%20renditions%20and%20grayscale%0Aconversion%29%20and%20image%20contrast%2C%20be%20part%20of%20automated%20image%20processing%20pipelines%0A%28e.g.%20isolating%20stain%20components%2C%20boosting%20learning%20models%29%2C%20and%20assist%20in%0Adigital%20pathology%20applications%20%28e.g.%20enhancing%20biomarker%20visibility%2C%20enabling%0Acolorblind-friendly%20renditions%29.%20Employing%20only%20basic%20arithmetic%20and%20matrix%0Aoperations%2C%20this%20work%20offers%20a%20computationally%20accessible%20methodology%20-%20in%20the%0Ahypercomplex%20domain%20-%20that%20showcases%20versatility%20and%20consistency%20across%20image%0Aprocessing%20tasks%20and%20a%20range%20of%20computer%20vision%20and%20biomedical%20applications.%0AThe%20proposed%20non-data-driven%20methods%20achieve%20comparable%20or%20better%20results%0A%28particularly%20in%20cases%20involving%20well-known%20methods%29%20to%20those%20reported%20in%20the%0Aliterature%2C%20showcasing%20the%20potential%20of%20robust%20theoretical%20frameworks%20with%0Apractical%20effectiveness.%20Results%2C%20methods%2C%20and%20limitations%20are%20detailed%0Aalongside%20discussion%20of%20promising%20extensions%2C%20emphasizing%20the%20potential%20of%0Afeature-rich%20mathematical/computational%20frameworks%20for%20natural%20and%20biomedical%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07758v4&entry.124074799=Read"},
{"title": "Evaluation Framework for AI Systems in \"the Wild\"", "author": "Sarah Jabbour and Trenton Chang and Anindya Das Antar and Joseph Peper and Insu Jang and Jiachen Liu and Jae-Won Chung and Shiqi He and Michael Wellman and Bryan Goodman and Elizabeth Bondi-Kelly and Kevin Samy and Rada Mihalcea and Mosharaf Chowhury and David Jurgens and Lu Wang", "abstract": "  Generative AI (GenAI) models have become vital across industries, yet current\nevaluation methods have not adapted to their widespread use. Traditional\nevaluations often rely on benchmarks and fixed datasets, frequently failing to\nreflect real-world performance, which creates a gap between lab-tested outcomes\nand practical applications. This white paper proposes a comprehensive framework\nfor how we should evaluate real-world GenAI systems, emphasizing diverse,\nevolving inputs and holistic, dynamic, and ongoing assessment approaches. The\npaper offers guidance for practitioners on how to design evaluation methods\nthat accurately reflect real-time capabilities, and provides policymakers with\nrecommendations for crafting GenAI policies focused on societal impacts, rather\nthan fixed performance numbers or parameter sizes. We advocate for holistic\nframeworks that integrate performance, fairness, and ethics and the use of\ncontinuous, outcome-oriented methods that combine human and automated\nassessments while also being transparent to foster trust among stakeholders.\nImplementing these strategies ensures GenAI models are not only technically\nproficient but also ethically responsible and impactful.\n", "link": "http://arxiv.org/abs/2504.16778v1", "date": "2025-04-23", "relevancy": 1.5465, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5598}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4689}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20Framework%20for%20AI%20Systems%20in%20%22the%20Wild%22&body=Title%3A%20Evaluation%20Framework%20for%20AI%20Systems%20in%20%22the%20Wild%22%0AAuthor%3A%20Sarah%20Jabbour%20and%20Trenton%20Chang%20and%20Anindya%20Das%20Antar%20and%20Joseph%20Peper%20and%20Insu%20Jang%20and%20Jiachen%20Liu%20and%20Jae-Won%20Chung%20and%20Shiqi%20He%20and%20Michael%20Wellman%20and%20Bryan%20Goodman%20and%20Elizabeth%20Bondi-Kelly%20and%20Kevin%20Samy%20and%20Rada%20Mihalcea%20and%20Mosharaf%20Chowhury%20and%20David%20Jurgens%20and%20Lu%20Wang%0AAbstract%3A%20%20%20Generative%20AI%20%28GenAI%29%20models%20have%20become%20vital%20across%20industries%2C%20yet%20current%0Aevaluation%20methods%20have%20not%20adapted%20to%20their%20widespread%20use.%20Traditional%0Aevaluations%20often%20rely%20on%20benchmarks%20and%20fixed%20datasets%2C%20frequently%20failing%20to%0Areflect%20real-world%20performance%2C%20which%20creates%20a%20gap%20between%20lab-tested%20outcomes%0Aand%20practical%20applications.%20This%20white%20paper%20proposes%20a%20comprehensive%20framework%0Afor%20how%20we%20should%20evaluate%20real-world%20GenAI%20systems%2C%20emphasizing%20diverse%2C%0Aevolving%20inputs%20and%20holistic%2C%20dynamic%2C%20and%20ongoing%20assessment%20approaches.%20The%0Apaper%20offers%20guidance%20for%20practitioners%20on%20how%20to%20design%20evaluation%20methods%0Athat%20accurately%20reflect%20real-time%20capabilities%2C%20and%20provides%20policymakers%20with%0Arecommendations%20for%20crafting%20GenAI%20policies%20focused%20on%20societal%20impacts%2C%20rather%0Athan%20fixed%20performance%20numbers%20or%20parameter%20sizes.%20We%20advocate%20for%20holistic%0Aframeworks%20that%20integrate%20performance%2C%20fairness%2C%20and%20ethics%20and%20the%20use%20of%0Acontinuous%2C%20outcome-oriented%20methods%20that%20combine%20human%20and%20automated%0Aassessments%20while%20also%20being%20transparent%20to%20foster%20trust%20among%20stakeholders.%0AImplementing%20these%20strategies%20ensures%20GenAI%20models%20are%20not%20only%20technically%0Aproficient%20but%20also%20ethically%20responsible%20and%20impactful.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520Framework%2520for%2520AI%2520Systems%2520in%2520%2522the%2520Wild%2522%26entry.906535625%3DSarah%2520Jabbour%2520and%2520Trenton%2520Chang%2520and%2520Anindya%2520Das%2520Antar%2520and%2520Joseph%2520Peper%2520and%2520Insu%2520Jang%2520and%2520Jiachen%2520Liu%2520and%2520Jae-Won%2520Chung%2520and%2520Shiqi%2520He%2520and%2520Michael%2520Wellman%2520and%2520Bryan%2520Goodman%2520and%2520Elizabeth%2520Bondi-Kelly%2520and%2520Kevin%2520Samy%2520and%2520Rada%2520Mihalcea%2520and%2520Mosharaf%2520Chowhury%2520and%2520David%2520Jurgens%2520and%2520Lu%2520Wang%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GenAI%2529%2520models%2520have%2520become%2520vital%2520across%2520industries%252C%2520yet%2520current%250Aevaluation%2520methods%2520have%2520not%2520adapted%2520to%2520their%2520widespread%2520use.%2520Traditional%250Aevaluations%2520often%2520rely%2520on%2520benchmarks%2520and%2520fixed%2520datasets%252C%2520frequently%2520failing%2520to%250Areflect%2520real-world%2520performance%252C%2520which%2520creates%2520a%2520gap%2520between%2520lab-tested%2520outcomes%250Aand%2520practical%2520applications.%2520This%2520white%2520paper%2520proposes%2520a%2520comprehensive%2520framework%250Afor%2520how%2520we%2520should%2520evaluate%2520real-world%2520GenAI%2520systems%252C%2520emphasizing%2520diverse%252C%250Aevolving%2520inputs%2520and%2520holistic%252C%2520dynamic%252C%2520and%2520ongoing%2520assessment%2520approaches.%2520The%250Apaper%2520offers%2520guidance%2520for%2520practitioners%2520on%2520how%2520to%2520design%2520evaluation%2520methods%250Athat%2520accurately%2520reflect%2520real-time%2520capabilities%252C%2520and%2520provides%2520policymakers%2520with%250Arecommendations%2520for%2520crafting%2520GenAI%2520policies%2520focused%2520on%2520societal%2520impacts%252C%2520rather%250Athan%2520fixed%2520performance%2520numbers%2520or%2520parameter%2520sizes.%2520We%2520advocate%2520for%2520holistic%250Aframeworks%2520that%2520integrate%2520performance%252C%2520fairness%252C%2520and%2520ethics%2520and%2520the%2520use%2520of%250Acontinuous%252C%2520outcome-oriented%2520methods%2520that%2520combine%2520human%2520and%2520automated%250Aassessments%2520while%2520also%2520being%2520transparent%2520to%2520foster%2520trust%2520among%2520stakeholders.%250AImplementing%2520these%2520strategies%2520ensures%2520GenAI%2520models%2520are%2520not%2520only%2520technically%250Aproficient%2520but%2520also%2520ethically%2520responsible%2520and%2520impactful.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20Framework%20for%20AI%20Systems%20in%20%22the%20Wild%22&entry.906535625=Sarah%20Jabbour%20and%20Trenton%20Chang%20and%20Anindya%20Das%20Antar%20and%20Joseph%20Peper%20and%20Insu%20Jang%20and%20Jiachen%20Liu%20and%20Jae-Won%20Chung%20and%20Shiqi%20He%20and%20Michael%20Wellman%20and%20Bryan%20Goodman%20and%20Elizabeth%20Bondi-Kelly%20and%20Kevin%20Samy%20and%20Rada%20Mihalcea%20and%20Mosharaf%20Chowhury%20and%20David%20Jurgens%20and%20Lu%20Wang&entry.1292438233=%20%20Generative%20AI%20%28GenAI%29%20models%20have%20become%20vital%20across%20industries%2C%20yet%20current%0Aevaluation%20methods%20have%20not%20adapted%20to%20their%20widespread%20use.%20Traditional%0Aevaluations%20often%20rely%20on%20benchmarks%20and%20fixed%20datasets%2C%20frequently%20failing%20to%0Areflect%20real-world%20performance%2C%20which%20creates%20a%20gap%20between%20lab-tested%20outcomes%0Aand%20practical%20applications.%20This%20white%20paper%20proposes%20a%20comprehensive%20framework%0Afor%20how%20we%20should%20evaluate%20real-world%20GenAI%20systems%2C%20emphasizing%20diverse%2C%0Aevolving%20inputs%20and%20holistic%2C%20dynamic%2C%20and%20ongoing%20assessment%20approaches.%20The%0Apaper%20offers%20guidance%20for%20practitioners%20on%20how%20to%20design%20evaluation%20methods%0Athat%20accurately%20reflect%20real-time%20capabilities%2C%20and%20provides%20policymakers%20with%0Arecommendations%20for%20crafting%20GenAI%20policies%20focused%20on%20societal%20impacts%2C%20rather%0Athan%20fixed%20performance%20numbers%20or%20parameter%20sizes.%20We%20advocate%20for%20holistic%0Aframeworks%20that%20integrate%20performance%2C%20fairness%2C%20and%20ethics%20and%20the%20use%20of%0Acontinuous%2C%20outcome-oriented%20methods%20that%20combine%20human%20and%20automated%0Aassessments%20while%20also%20being%20transparent%20to%20foster%20trust%20among%20stakeholders.%0AImplementing%20these%20strategies%20ensures%20GenAI%20models%20are%20not%20only%20technically%0Aproficient%20but%20also%20ethically%20responsible%20and%20impactful.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16778v1&entry.124074799=Read"},
{"title": "An Adaptive ML Framework for Power Converter Monitoring via Federated\n  Transfer Learning", "author": "Panagiotis Kakosimos and Alireza Nemat Saberi and Luca Peretti", "abstract": "  This study explores alternative framework configurations for adapting thermal\nmachine learning (ML) models for power converters by combining transfer\nlearning (TL) and federated learning (FL) in a piecewise manner. This approach\ninherently addresses challenges such as varying operating conditions, data\nsharing limitations, and security implications. The framework starts with a\nbase model that is incrementally adapted by multiple clients via adapting three\nstate-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component\nAnalysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is\nemployed for FL, using Federated Averaging for aggregation. Validation with\nfield data demonstrates that fine-tuning offers a straightforward TL approach\nwith high accuracy, making it suitable for practical applications. Benchmarking\nresults reveal a comprehensive comparison of these methods, showcasing their\nrespective strengths and weaknesses when applied in different scenarios.\nLocally hosted FL enhances performance when data aggregation is not feasible,\nwhile cloud-based FL becomes more practical with a significant increase in the\nnumber of clients, addressing scalability and connectivity challenges.\n", "link": "http://arxiv.org/abs/2504.16866v1", "date": "2025-04-23", "relevancy": 1.5382, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5183}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20ML%20Framework%20for%20Power%20Converter%20Monitoring%20via%20Federated%0A%20%20Transfer%20Learning&body=Title%3A%20An%20Adaptive%20ML%20Framework%20for%20Power%20Converter%20Monitoring%20via%20Federated%0A%20%20Transfer%20Learning%0AAuthor%3A%20Panagiotis%20Kakosimos%20and%20Alireza%20Nemat%20Saberi%20and%20Luca%20Peretti%0AAbstract%3A%20%20%20This%20study%20explores%20alternative%20framework%20configurations%20for%20adapting%20thermal%0Amachine%20learning%20%28ML%29%20models%20for%20power%20converters%20by%20combining%20transfer%0Alearning%20%28TL%29%20and%20federated%20learning%20%28FL%29%20in%20a%20piecewise%20manner.%20This%20approach%0Ainherently%20addresses%20challenges%20such%20as%20varying%20operating%20conditions%2C%20data%0Asharing%20limitations%2C%20and%20security%20implications.%20The%20framework%20starts%20with%20a%0Abase%20model%20that%20is%20incrementally%20adapted%20by%20multiple%20clients%20via%20adapting%20three%0Astate-of-the-art%20domain%20adaptation%20techniques%3A%20Fine-tuning%2C%20Transfer%20Component%0AAnalysis%20%28TCA%29%2C%20and%20Deep%20Domain%20Adaptation%20%28DDA%29.%20The%20Flower%20framework%20is%0Aemployed%20for%20FL%2C%20using%20Federated%20Averaging%20for%20aggregation.%20Validation%20with%0Afield%20data%20demonstrates%20that%20fine-tuning%20offers%20a%20straightforward%20TL%20approach%0Awith%20high%20accuracy%2C%20making%20it%20suitable%20for%20practical%20applications.%20Benchmarking%0Aresults%20reveal%20a%20comprehensive%20comparison%20of%20these%20methods%2C%20showcasing%20their%0Arespective%20strengths%20and%20weaknesses%20when%20applied%20in%20different%20scenarios.%0ALocally%20hosted%20FL%20enhances%20performance%20when%20data%20aggregation%20is%20not%20feasible%2C%0Awhile%20cloud-based%20FL%20becomes%20more%20practical%20with%20a%20significant%20increase%20in%20the%0Anumber%20of%20clients%2C%20addressing%20scalability%20and%20connectivity%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520ML%2520Framework%2520for%2520Power%2520Converter%2520Monitoring%2520via%2520Federated%250A%2520%2520Transfer%2520Learning%26entry.906535625%3DPanagiotis%2520Kakosimos%2520and%2520Alireza%2520Nemat%2520Saberi%2520and%2520Luca%2520Peretti%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520alternative%2520framework%2520configurations%2520for%2520adapting%2520thermal%250Amachine%2520learning%2520%2528ML%2529%2520models%2520for%2520power%2520converters%2520by%2520combining%2520transfer%250Alearning%2520%2528TL%2529%2520and%2520federated%2520learning%2520%2528FL%2529%2520in%2520a%2520piecewise%2520manner.%2520This%2520approach%250Ainherently%2520addresses%2520challenges%2520such%2520as%2520varying%2520operating%2520conditions%252C%2520data%250Asharing%2520limitations%252C%2520and%2520security%2520implications.%2520The%2520framework%2520starts%2520with%2520a%250Abase%2520model%2520that%2520is%2520incrementally%2520adapted%2520by%2520multiple%2520clients%2520via%2520adapting%2520three%250Astate-of-the-art%2520domain%2520adaptation%2520techniques%253A%2520Fine-tuning%252C%2520Transfer%2520Component%250AAnalysis%2520%2528TCA%2529%252C%2520and%2520Deep%2520Domain%2520Adaptation%2520%2528DDA%2529.%2520The%2520Flower%2520framework%2520is%250Aemployed%2520for%2520FL%252C%2520using%2520Federated%2520Averaging%2520for%2520aggregation.%2520Validation%2520with%250Afield%2520data%2520demonstrates%2520that%2520fine-tuning%2520offers%2520a%2520straightforward%2520TL%2520approach%250Awith%2520high%2520accuracy%252C%2520making%2520it%2520suitable%2520for%2520practical%2520applications.%2520Benchmarking%250Aresults%2520reveal%2520a%2520comprehensive%2520comparison%2520of%2520these%2520methods%252C%2520showcasing%2520their%250Arespective%2520strengths%2520and%2520weaknesses%2520when%2520applied%2520in%2520different%2520scenarios.%250ALocally%2520hosted%2520FL%2520enhances%2520performance%2520when%2520data%2520aggregation%2520is%2520not%2520feasible%252C%250Awhile%2520cloud-based%2520FL%2520becomes%2520more%2520practical%2520with%2520a%2520significant%2520increase%2520in%2520the%250Anumber%2520of%2520clients%252C%2520addressing%2520scalability%2520and%2520connectivity%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20ML%20Framework%20for%20Power%20Converter%20Monitoring%20via%20Federated%0A%20%20Transfer%20Learning&entry.906535625=Panagiotis%20Kakosimos%20and%20Alireza%20Nemat%20Saberi%20and%20Luca%20Peretti&entry.1292438233=%20%20This%20study%20explores%20alternative%20framework%20configurations%20for%20adapting%20thermal%0Amachine%20learning%20%28ML%29%20models%20for%20power%20converters%20by%20combining%20transfer%0Alearning%20%28TL%29%20and%20federated%20learning%20%28FL%29%20in%20a%20piecewise%20manner.%20This%20approach%0Ainherently%20addresses%20challenges%20such%20as%20varying%20operating%20conditions%2C%20data%0Asharing%20limitations%2C%20and%20security%20implications.%20The%20framework%20starts%20with%20a%0Abase%20model%20that%20is%20incrementally%20adapted%20by%20multiple%20clients%20via%20adapting%20three%0Astate-of-the-art%20domain%20adaptation%20techniques%3A%20Fine-tuning%2C%20Transfer%20Component%0AAnalysis%20%28TCA%29%2C%20and%20Deep%20Domain%20Adaptation%20%28DDA%29.%20The%20Flower%20framework%20is%0Aemployed%20for%20FL%2C%20using%20Federated%20Averaging%20for%20aggregation.%20Validation%20with%0Afield%20data%20demonstrates%20that%20fine-tuning%20offers%20a%20straightforward%20TL%20approach%0Awith%20high%20accuracy%2C%20making%20it%20suitable%20for%20practical%20applications.%20Benchmarking%0Aresults%20reveal%20a%20comprehensive%20comparison%20of%20these%20methods%2C%20showcasing%20their%0Arespective%20strengths%20and%20weaknesses%20when%20applied%20in%20different%20scenarios.%0ALocally%20hosted%20FL%20enhances%20performance%20when%20data%20aggregation%20is%20not%20feasible%2C%0Awhile%20cloud-based%20FL%20becomes%20more%20practical%20with%20a%20significant%20increase%20in%20the%0Anumber%20of%20clients%2C%20addressing%20scalability%20and%20connectivity%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16866v1&entry.124074799=Read"},
{"title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents", "author": "Raghav Thind and Youran Sun and Ling Liang and Haizhao Yang", "abstract": "  Optimization plays a vital role in scientific research and practical\napplications, but formulating a concrete optimization problem described in\nnatural language into a mathematical form and selecting a suitable solver to\nsolve the problem requires substantial domain expertise. We introduce\n\\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems\ndescribed in natural language by leveraging LLM-powered \\underline{AI} agents,\nachieving superior performance over current state-of-the-art methods. Our\nframework is built upon four key roles: (1) a \\emph{formulator} that translates\nnatural language problem descriptions into precise mathematical formulations;\n(2) a \\emph{planner} that constructs a high-level solution strategy prior to\nexecution; and (3) a \\emph{coder} and a \\emph{code critic} capable of\ninteracting with the environment and reflecting on outcomes to refine future\nactions. Ablation studies confirm that all roles are essential; removing the\nplanner or code critic results in $5.8\\times$ and $3.1\\times$ drops in\nproductivity, respectively. Furthermore, we introduce UCB-based debug\nscheduling to dynamically switch between alternative plans, yielding an\nadditional $3.3\\times$ productivity gain. Our design emphasizes multi-agent\ncollaboration, allowing us to conveniently explore the synergistic effect of\ncombining diverse models within a unified system. Our approach attains 88.1\\%\naccuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o\ntable) subset, reducing error rates by 58\\% and 50\\% respectively over prior\nbest results.\n", "link": "http://arxiv.org/abs/2504.16918v1", "date": "2025-04-23", "relevancy": 1.5287, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5226}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5077}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OptimAI%3A%20Optimization%20from%20Natural%20Language%20Using%20LLM-Powered%20AI%20Agents&body=Title%3A%20OptimAI%3A%20Optimization%20from%20Natural%20Language%20Using%20LLM-Powered%20AI%20Agents%0AAuthor%3A%20Raghav%20Thind%20and%20Youran%20Sun%20and%20Ling%20Liang%20and%20Haizhao%20Yang%0AAbstract%3A%20%20%20Optimization%20plays%20a%20vital%20role%20in%20scientific%20research%20and%20practical%0Aapplications%2C%20but%20formulating%20a%20concrete%20optimization%20problem%20described%20in%0Anatural%20language%20into%20a%20mathematical%20form%20and%20selecting%20a%20suitable%20solver%20to%0Asolve%20the%20problem%20requires%20substantial%20domain%20expertise.%20We%20introduce%0A%5Ctextbf%7BOptimAI%7D%2C%20a%20framework%20for%20solving%20%5Cunderline%7BOptim%7Dization%20problems%0Adescribed%20in%20natural%20language%20by%20leveraging%20LLM-powered%20%5Cunderline%7BAI%7D%20agents%2C%0Aachieving%20superior%20performance%20over%20current%20state-of-the-art%20methods.%20Our%0Aframework%20is%20built%20upon%20four%20key%20roles%3A%20%281%29%20a%20%5Cemph%7Bformulator%7D%20that%20translates%0Anatural%20language%20problem%20descriptions%20into%20precise%20mathematical%20formulations%3B%0A%282%29%20a%20%5Cemph%7Bplanner%7D%20that%20constructs%20a%20high-level%20solution%20strategy%20prior%20to%0Aexecution%3B%20and%20%283%29%20a%20%5Cemph%7Bcoder%7D%20and%20a%20%5Cemph%7Bcode%20critic%7D%20capable%20of%0Ainteracting%20with%20the%20environment%20and%20reflecting%20on%20outcomes%20to%20refine%20future%0Aactions.%20Ablation%20studies%20confirm%20that%20all%20roles%20are%20essential%3B%20removing%20the%0Aplanner%20or%20code%20critic%20results%20in%20%245.8%5Ctimes%24%20and%20%243.1%5Ctimes%24%20drops%20in%0Aproductivity%2C%20respectively.%20Furthermore%2C%20we%20introduce%20UCB-based%20debug%0Ascheduling%20to%20dynamically%20switch%20between%20alternative%20plans%2C%20yielding%20an%0Aadditional%20%243.3%5Ctimes%24%20productivity%20gain.%20Our%20design%20emphasizes%20multi-agent%0Acollaboration%2C%20allowing%20us%20to%20conveniently%20explore%20the%20synergistic%20effect%20of%0Acombining%20diverse%20models%20within%20a%20unified%20system.%20Our%20approach%20attains%2088.1%5C%25%0Aaccuracy%20on%20the%20NLP4LP%20dataset%20and%2071.2%5C%25%20on%20the%20Optibench%20%28non-linear%20w/o%0Atable%29%20subset%2C%20reducing%20error%20rates%20by%2058%5C%25%20and%2050%5C%25%20respectively%20over%20prior%0Abest%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimAI%253A%2520Optimization%2520from%2520Natural%2520Language%2520Using%2520LLM-Powered%2520AI%2520Agents%26entry.906535625%3DRaghav%2520Thind%2520and%2520Youran%2520Sun%2520and%2520Ling%2520Liang%2520and%2520Haizhao%2520Yang%26entry.1292438233%3D%2520%2520Optimization%2520plays%2520a%2520vital%2520role%2520in%2520scientific%2520research%2520and%2520practical%250Aapplications%252C%2520but%2520formulating%2520a%2520concrete%2520optimization%2520problem%2520described%2520in%250Anatural%2520language%2520into%2520a%2520mathematical%2520form%2520and%2520selecting%2520a%2520suitable%2520solver%2520to%250Asolve%2520the%2520problem%2520requires%2520substantial%2520domain%2520expertise.%2520We%2520introduce%250A%255Ctextbf%257BOptimAI%257D%252C%2520a%2520framework%2520for%2520solving%2520%255Cunderline%257BOptim%257Dization%2520problems%250Adescribed%2520in%2520natural%2520language%2520by%2520leveraging%2520LLM-powered%2520%255Cunderline%257BAI%257D%2520agents%252C%250Aachieving%2520superior%2520performance%2520over%2520current%2520state-of-the-art%2520methods.%2520Our%250Aframework%2520is%2520built%2520upon%2520four%2520key%2520roles%253A%2520%25281%2529%2520a%2520%255Cemph%257Bformulator%257D%2520that%2520translates%250Anatural%2520language%2520problem%2520descriptions%2520into%2520precise%2520mathematical%2520formulations%253B%250A%25282%2529%2520a%2520%255Cemph%257Bplanner%257D%2520that%2520constructs%2520a%2520high-level%2520solution%2520strategy%2520prior%2520to%250Aexecution%253B%2520and%2520%25283%2529%2520a%2520%255Cemph%257Bcoder%257D%2520and%2520a%2520%255Cemph%257Bcode%2520critic%257D%2520capable%2520of%250Ainteracting%2520with%2520the%2520environment%2520and%2520reflecting%2520on%2520outcomes%2520to%2520refine%2520future%250Aactions.%2520Ablation%2520studies%2520confirm%2520that%2520all%2520roles%2520are%2520essential%253B%2520removing%2520the%250Aplanner%2520or%2520code%2520critic%2520results%2520in%2520%25245.8%255Ctimes%2524%2520and%2520%25243.1%255Ctimes%2524%2520drops%2520in%250Aproductivity%252C%2520respectively.%2520Furthermore%252C%2520we%2520introduce%2520UCB-based%2520debug%250Ascheduling%2520to%2520dynamically%2520switch%2520between%2520alternative%2520plans%252C%2520yielding%2520an%250Aadditional%2520%25243.3%255Ctimes%2524%2520productivity%2520gain.%2520Our%2520design%2520emphasizes%2520multi-agent%250Acollaboration%252C%2520allowing%2520us%2520to%2520conveniently%2520explore%2520the%2520synergistic%2520effect%2520of%250Acombining%2520diverse%2520models%2520within%2520a%2520unified%2520system.%2520Our%2520approach%2520attains%252088.1%255C%2525%250Aaccuracy%2520on%2520the%2520NLP4LP%2520dataset%2520and%252071.2%255C%2525%2520on%2520the%2520Optibench%2520%2528non-linear%2520w/o%250Atable%2529%2520subset%252C%2520reducing%2520error%2520rates%2520by%252058%255C%2525%2520and%252050%255C%2525%2520respectively%2520over%2520prior%250Abest%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OptimAI%3A%20Optimization%20from%20Natural%20Language%20Using%20LLM-Powered%20AI%20Agents&entry.906535625=Raghav%20Thind%20and%20Youran%20Sun%20and%20Ling%20Liang%20and%20Haizhao%20Yang&entry.1292438233=%20%20Optimization%20plays%20a%20vital%20role%20in%20scientific%20research%20and%20practical%0Aapplications%2C%20but%20formulating%20a%20concrete%20optimization%20problem%20described%20in%0Anatural%20language%20into%20a%20mathematical%20form%20and%20selecting%20a%20suitable%20solver%20to%0Asolve%20the%20problem%20requires%20substantial%20domain%20expertise.%20We%20introduce%0A%5Ctextbf%7BOptimAI%7D%2C%20a%20framework%20for%20solving%20%5Cunderline%7BOptim%7Dization%20problems%0Adescribed%20in%20natural%20language%20by%20leveraging%20LLM-powered%20%5Cunderline%7BAI%7D%20agents%2C%0Aachieving%20superior%20performance%20over%20current%20state-of-the-art%20methods.%20Our%0Aframework%20is%20built%20upon%20four%20key%20roles%3A%20%281%29%20a%20%5Cemph%7Bformulator%7D%20that%20translates%0Anatural%20language%20problem%20descriptions%20into%20precise%20mathematical%20formulations%3B%0A%282%29%20a%20%5Cemph%7Bplanner%7D%20that%20constructs%20a%20high-level%20solution%20strategy%20prior%20to%0Aexecution%3B%20and%20%283%29%20a%20%5Cemph%7Bcoder%7D%20and%20a%20%5Cemph%7Bcode%20critic%7D%20capable%20of%0Ainteracting%20with%20the%20environment%20and%20reflecting%20on%20outcomes%20to%20refine%20future%0Aactions.%20Ablation%20studies%20confirm%20that%20all%20roles%20are%20essential%3B%20removing%20the%0Aplanner%20or%20code%20critic%20results%20in%20%245.8%5Ctimes%24%20and%20%243.1%5Ctimes%24%20drops%20in%0Aproductivity%2C%20respectively.%20Furthermore%2C%20we%20introduce%20UCB-based%20debug%0Ascheduling%20to%20dynamically%20switch%20between%20alternative%20plans%2C%20yielding%20an%0Aadditional%20%243.3%5Ctimes%24%20productivity%20gain.%20Our%20design%20emphasizes%20multi-agent%0Acollaboration%2C%20allowing%20us%20to%20conveniently%20explore%20the%20synergistic%20effect%20of%0Acombining%20diverse%20models%20within%20a%20unified%20system.%20Our%20approach%20attains%2088.1%5C%25%0Aaccuracy%20on%20the%20NLP4LP%20dataset%20and%2071.2%5C%25%20on%20the%20Optibench%20%28non-linear%20w/o%0Atable%29%20subset%2C%20reducing%20error%20rates%20by%2058%5C%25%20and%2050%5C%25%20respectively%20over%20prior%0Abest%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16918v1&entry.124074799=Read"},
{"title": "A Survey on Mixup Augmentations and Beyond", "author": "Xin Jin and Hongyu Zhu and Siyuan Li and Zedong Wang and Zicheng Liu and Juanxi Tian and Chang Yu and Huafeng Qin and Stan Z. Li", "abstract": "  As Deep Neural Networks have achieved thrilling breakthroughs in the past\ndecade, data augmentations have garnered increasing attention as regularization\ntechniques when massive labeled data are unavailable. Among existing\naugmentations, Mixup and relevant data-mixing methods that convexly combine\nselected samples and the corresponding labels are widely adopted because they\nyield high performances by generating data-dependent virtual data while easily\nmigrating to various domains. This survey presents a comprehensive review of\nfoundational mixup methods and their applications. We first elaborate on the\ntraining pipeline with mixup augmentations as a unified framework containing\nmodules. A reformulated framework could contain various mixup methods and give\nintuitive operational procedures. Then, we systematically investigate the\napplications of mixup augmentations on vision downstream tasks, various data\nmodalities, and some analysis \\& theorems of mixup. Meanwhile, we conclude the\ncurrent status and limitations of mixup research and point out further work for\neffective and efficient mixup augmentations. This survey can provide\nresearchers with the current state of the art in mixup methods and provide some\ninsights and guidance roles in the mixup arena. An online project with this\nsurvey is available at https://github.com/Westlake-AI/Awesome-Mixup.\n", "link": "http://arxiv.org/abs/2409.05202v2", "date": "2025-04-23", "relevancy": 1.4734, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4969}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4914}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Mixup%20Augmentations%20and%20Beyond&body=Title%3A%20A%20Survey%20on%20Mixup%20Augmentations%20and%20Beyond%0AAuthor%3A%20Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Siyuan%20Li%20and%20Zedong%20Wang%20and%20Zicheng%20Liu%20and%20Juanxi%20Tian%20and%20Chang%20Yu%20and%20Huafeng%20Qin%20and%20Stan%20Z.%20Li%0AAbstract%3A%20%20%20As%20Deep%20Neural%20Networks%20have%20achieved%20thrilling%20breakthroughs%20in%20the%20past%0Adecade%2C%20data%20augmentations%20have%20garnered%20increasing%20attention%20as%20regularization%0Atechniques%20when%20massive%20labeled%20data%20are%20unavailable.%20Among%20existing%0Aaugmentations%2C%20Mixup%20and%20relevant%20data-mixing%20methods%20that%20convexly%20combine%0Aselected%20samples%20and%20the%20corresponding%20labels%20are%20widely%20adopted%20because%20they%0Ayield%20high%20performances%20by%20generating%20data-dependent%20virtual%20data%20while%20easily%0Amigrating%20to%20various%20domains.%20This%20survey%20presents%20a%20comprehensive%20review%20of%0Afoundational%20mixup%20methods%20and%20their%20applications.%20We%20first%20elaborate%20on%20the%0Atraining%20pipeline%20with%20mixup%20augmentations%20as%20a%20unified%20framework%20containing%0Amodules.%20A%20reformulated%20framework%20could%20contain%20various%20mixup%20methods%20and%20give%0Aintuitive%20operational%20procedures.%20Then%2C%20we%20systematically%20investigate%20the%0Aapplications%20of%20mixup%20augmentations%20on%20vision%20downstream%20tasks%2C%20various%20data%0Amodalities%2C%20and%20some%20analysis%20%5C%26%20theorems%20of%20mixup.%20Meanwhile%2C%20we%20conclude%20the%0Acurrent%20status%20and%20limitations%20of%20mixup%20research%20and%20point%20out%20further%20work%20for%0Aeffective%20and%20efficient%20mixup%20augmentations.%20This%20survey%20can%20provide%0Aresearchers%20with%20the%20current%20state%20of%20the%20art%20in%20mixup%20methods%20and%20provide%20some%0Ainsights%20and%20guidance%20roles%20in%20the%20mixup%20arena.%20An%20online%20project%20with%20this%0Asurvey%20is%20available%20at%20https%3A//github.com/Westlake-AI/Awesome-Mixup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Mixup%2520Augmentations%2520and%2520Beyond%26entry.906535625%3DXin%2520Jin%2520and%2520Hongyu%2520Zhu%2520and%2520Siyuan%2520Li%2520and%2520Zedong%2520Wang%2520and%2520Zicheng%2520Liu%2520and%2520Juanxi%2520Tian%2520and%2520Chang%2520Yu%2520and%2520Huafeng%2520Qin%2520and%2520Stan%2520Z.%2520Li%26entry.1292438233%3D%2520%2520As%2520Deep%2520Neural%2520Networks%2520have%2520achieved%2520thrilling%2520breakthroughs%2520in%2520the%2520past%250Adecade%252C%2520data%2520augmentations%2520have%2520garnered%2520increasing%2520attention%2520as%2520regularization%250Atechniques%2520when%2520massive%2520labeled%2520data%2520are%2520unavailable.%2520Among%2520existing%250Aaugmentations%252C%2520Mixup%2520and%2520relevant%2520data-mixing%2520methods%2520that%2520convexly%2520combine%250Aselected%2520samples%2520and%2520the%2520corresponding%2520labels%2520are%2520widely%2520adopted%2520because%2520they%250Ayield%2520high%2520performances%2520by%2520generating%2520data-dependent%2520virtual%2520data%2520while%2520easily%250Amigrating%2520to%2520various%2520domains.%2520This%2520survey%2520presents%2520a%2520comprehensive%2520review%2520of%250Afoundational%2520mixup%2520methods%2520and%2520their%2520applications.%2520We%2520first%2520elaborate%2520on%2520the%250Atraining%2520pipeline%2520with%2520mixup%2520augmentations%2520as%2520a%2520unified%2520framework%2520containing%250Amodules.%2520A%2520reformulated%2520framework%2520could%2520contain%2520various%2520mixup%2520methods%2520and%2520give%250Aintuitive%2520operational%2520procedures.%2520Then%252C%2520we%2520systematically%2520investigate%2520the%250Aapplications%2520of%2520mixup%2520augmentations%2520on%2520vision%2520downstream%2520tasks%252C%2520various%2520data%250Amodalities%252C%2520and%2520some%2520analysis%2520%255C%2526%2520theorems%2520of%2520mixup.%2520Meanwhile%252C%2520we%2520conclude%2520the%250Acurrent%2520status%2520and%2520limitations%2520of%2520mixup%2520research%2520and%2520point%2520out%2520further%2520work%2520for%250Aeffective%2520and%2520efficient%2520mixup%2520augmentations.%2520This%2520survey%2520can%2520provide%250Aresearchers%2520with%2520the%2520current%2520state%2520of%2520the%2520art%2520in%2520mixup%2520methods%2520and%2520provide%2520some%250Ainsights%2520and%2520guidance%2520roles%2520in%2520the%2520mixup%2520arena.%2520An%2520online%2520project%2520with%2520this%250Asurvey%2520is%2520available%2520at%2520https%253A//github.com/Westlake-AI/Awesome-Mixup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Mixup%20Augmentations%20and%20Beyond&entry.906535625=Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Siyuan%20Li%20and%20Zedong%20Wang%20and%20Zicheng%20Liu%20and%20Juanxi%20Tian%20and%20Chang%20Yu%20and%20Huafeng%20Qin%20and%20Stan%20Z.%20Li&entry.1292438233=%20%20As%20Deep%20Neural%20Networks%20have%20achieved%20thrilling%20breakthroughs%20in%20the%20past%0Adecade%2C%20data%20augmentations%20have%20garnered%20increasing%20attention%20as%20regularization%0Atechniques%20when%20massive%20labeled%20data%20are%20unavailable.%20Among%20existing%0Aaugmentations%2C%20Mixup%20and%20relevant%20data-mixing%20methods%20that%20convexly%20combine%0Aselected%20samples%20and%20the%20corresponding%20labels%20are%20widely%20adopted%20because%20they%0Ayield%20high%20performances%20by%20generating%20data-dependent%20virtual%20data%20while%20easily%0Amigrating%20to%20various%20domains.%20This%20survey%20presents%20a%20comprehensive%20review%20of%0Afoundational%20mixup%20methods%20and%20their%20applications.%20We%20first%20elaborate%20on%20the%0Atraining%20pipeline%20with%20mixup%20augmentations%20as%20a%20unified%20framework%20containing%0Amodules.%20A%20reformulated%20framework%20could%20contain%20various%20mixup%20methods%20and%20give%0Aintuitive%20operational%20procedures.%20Then%2C%20we%20systematically%20investigate%20the%0Aapplications%20of%20mixup%20augmentations%20on%20vision%20downstream%20tasks%2C%20various%20data%0Amodalities%2C%20and%20some%20analysis%20%5C%26%20theorems%20of%20mixup.%20Meanwhile%2C%20we%20conclude%20the%0Acurrent%20status%20and%20limitations%20of%20mixup%20research%20and%20point%20out%20further%20work%20for%0Aeffective%20and%20efficient%20mixup%20augmentations.%20This%20survey%20can%20provide%0Aresearchers%20with%20the%20current%20state%20of%20the%20art%20in%20mixup%20methods%20and%20provide%20some%0Ainsights%20and%20guidance%20roles%20in%20the%20mixup%20arena.%20An%20online%20project%20with%20this%0Asurvey%20is%20available%20at%20https%3A//github.com/Westlake-AI/Awesome-Mixup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05202v2&entry.124074799=Read"},
{"title": "Feature Mixing Approach for Detecting Intraoperative Adverse Events in\n  Laparoscopic Roux-en-Y Gastric Bypass Surgery", "author": "Rupak Bose and Chinedu Innocent Nwoye and Jorge Lazo and Jo\u00ebl Lukas Lavanchy and Nicolas Padoy", "abstract": "  Intraoperative adverse events (IAEs), such as bleeding or thermal injury, can\nlead to severe postoperative complications if undetected. However, their rarity\nresults in highly imbalanced datasets, posing challenges for AI-based detection\nand severity quantification. We propose BetaMixer, a novel deep learning model\nthat addresses these challenges through a Beta distribution-based mixing\napproach, converting discrete IAE severity scores into continuous values for\nprecise severity regression (0-5 scale). BetaMixer employs Beta\ndistribution-based sampling to enhance underrepresented classes and regularizes\nintermediate embeddings to maintain a structured feature space. A generative\napproach aligns the feature space with sampled IAE severity, enabling robust\nclassification and severity regression via a transformer. Evaluated on the\nMultiBypass140 dataset, which we extended with IAE labels, BetaMixer achieves a\nweighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84,\ndemonstrating strong performance on imbalanced data. By integrating Beta\ndistribution-based sampling, feature mixing, and generative modeling, BetaMixer\noffers a robust solution for IAE detection and quantification in clinical\nsettings.\n", "link": "http://arxiv.org/abs/2504.16749v1", "date": "2025-04-23", "relevancy": 1.4655, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4936}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4889}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Mixing%20Approach%20for%20Detecting%20Intraoperative%20Adverse%20Events%20in%0A%20%20Laparoscopic%20Roux-en-Y%20Gastric%20Bypass%20Surgery&body=Title%3A%20Feature%20Mixing%20Approach%20for%20Detecting%20Intraoperative%20Adverse%20Events%20in%0A%20%20Laparoscopic%20Roux-en-Y%20Gastric%20Bypass%20Surgery%0AAuthor%3A%20Rupak%20Bose%20and%20Chinedu%20Innocent%20Nwoye%20and%20Jorge%20Lazo%20and%20Jo%C3%ABl%20Lukas%20Lavanchy%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Intraoperative%20adverse%20events%20%28IAEs%29%2C%20such%20as%20bleeding%20or%20thermal%20injury%2C%20can%0Alead%20to%20severe%20postoperative%20complications%20if%20undetected.%20However%2C%20their%20rarity%0Aresults%20in%20highly%20imbalanced%20datasets%2C%20posing%20challenges%20for%20AI-based%20detection%0Aand%20severity%20quantification.%20We%20propose%20BetaMixer%2C%20a%20novel%20deep%20learning%20model%0Athat%20addresses%20these%20challenges%20through%20a%20Beta%20distribution-based%20mixing%0Aapproach%2C%20converting%20discrete%20IAE%20severity%20scores%20into%20continuous%20values%20for%0Aprecise%20severity%20regression%20%280-5%20scale%29.%20BetaMixer%20employs%20Beta%0Adistribution-based%20sampling%20to%20enhance%20underrepresented%20classes%20and%20regularizes%0Aintermediate%20embeddings%20to%20maintain%20a%20structured%20feature%20space.%20A%20generative%0Aapproach%20aligns%20the%20feature%20space%20with%20sampled%20IAE%20severity%2C%20enabling%20robust%0Aclassification%20and%20severity%20regression%20via%20a%20transformer.%20Evaluated%20on%20the%0AMultiBypass140%20dataset%2C%20which%20we%20extended%20with%20IAE%20labels%2C%20BetaMixer%20achieves%20a%0Aweighted%20F1%20score%20of%200.76%2C%20recall%20of%200.81%2C%20PPV%20of%200.73%2C%20and%20NPV%20of%200.84%2C%0Ademonstrating%20strong%20performance%20on%20imbalanced%20data.%20By%20integrating%20Beta%0Adistribution-based%20sampling%2C%20feature%20mixing%2C%20and%20generative%20modeling%2C%20BetaMixer%0Aoffers%20a%20robust%20solution%20for%20IAE%20detection%20and%20quantification%20in%20clinical%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Mixing%2520Approach%2520for%2520Detecting%2520Intraoperative%2520Adverse%2520Events%2520in%250A%2520%2520Laparoscopic%2520Roux-en-Y%2520Gastric%2520Bypass%2520Surgery%26entry.906535625%3DRupak%2520Bose%2520and%2520Chinedu%2520Innocent%2520Nwoye%2520and%2520Jorge%2520Lazo%2520and%2520Jo%25C3%25ABl%2520Lukas%2520Lavanchy%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Intraoperative%2520adverse%2520events%2520%2528IAEs%2529%252C%2520such%2520as%2520bleeding%2520or%2520thermal%2520injury%252C%2520can%250Alead%2520to%2520severe%2520postoperative%2520complications%2520if%2520undetected.%2520However%252C%2520their%2520rarity%250Aresults%2520in%2520highly%2520imbalanced%2520datasets%252C%2520posing%2520challenges%2520for%2520AI-based%2520detection%250Aand%2520severity%2520quantification.%2520We%2520propose%2520BetaMixer%252C%2520a%2520novel%2520deep%2520learning%2520model%250Athat%2520addresses%2520these%2520challenges%2520through%2520a%2520Beta%2520distribution-based%2520mixing%250Aapproach%252C%2520converting%2520discrete%2520IAE%2520severity%2520scores%2520into%2520continuous%2520values%2520for%250Aprecise%2520severity%2520regression%2520%25280-5%2520scale%2529.%2520BetaMixer%2520employs%2520Beta%250Adistribution-based%2520sampling%2520to%2520enhance%2520underrepresented%2520classes%2520and%2520regularizes%250Aintermediate%2520embeddings%2520to%2520maintain%2520a%2520structured%2520feature%2520space.%2520A%2520generative%250Aapproach%2520aligns%2520the%2520feature%2520space%2520with%2520sampled%2520IAE%2520severity%252C%2520enabling%2520robust%250Aclassification%2520and%2520severity%2520regression%2520via%2520a%2520transformer.%2520Evaluated%2520on%2520the%250AMultiBypass140%2520dataset%252C%2520which%2520we%2520extended%2520with%2520IAE%2520labels%252C%2520BetaMixer%2520achieves%2520a%250Aweighted%2520F1%2520score%2520of%25200.76%252C%2520recall%2520of%25200.81%252C%2520PPV%2520of%25200.73%252C%2520and%2520NPV%2520of%25200.84%252C%250Ademonstrating%2520strong%2520performance%2520on%2520imbalanced%2520data.%2520By%2520integrating%2520Beta%250Adistribution-based%2520sampling%252C%2520feature%2520mixing%252C%2520and%2520generative%2520modeling%252C%2520BetaMixer%250Aoffers%2520a%2520robust%2520solution%2520for%2520IAE%2520detection%2520and%2520quantification%2520in%2520clinical%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Mixing%20Approach%20for%20Detecting%20Intraoperative%20Adverse%20Events%20in%0A%20%20Laparoscopic%20Roux-en-Y%20Gastric%20Bypass%20Surgery&entry.906535625=Rupak%20Bose%20and%20Chinedu%20Innocent%20Nwoye%20and%20Jorge%20Lazo%20and%20Jo%C3%ABl%20Lukas%20Lavanchy%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Intraoperative%20adverse%20events%20%28IAEs%29%2C%20such%20as%20bleeding%20or%20thermal%20injury%2C%20can%0Alead%20to%20severe%20postoperative%20complications%20if%20undetected.%20However%2C%20their%20rarity%0Aresults%20in%20highly%20imbalanced%20datasets%2C%20posing%20challenges%20for%20AI-based%20detection%0Aand%20severity%20quantification.%20We%20propose%20BetaMixer%2C%20a%20novel%20deep%20learning%20model%0Athat%20addresses%20these%20challenges%20through%20a%20Beta%20distribution-based%20mixing%0Aapproach%2C%20converting%20discrete%20IAE%20severity%20scores%20into%20continuous%20values%20for%0Aprecise%20severity%20regression%20%280-5%20scale%29.%20BetaMixer%20employs%20Beta%0Adistribution-based%20sampling%20to%20enhance%20underrepresented%20classes%20and%20regularizes%0Aintermediate%20embeddings%20to%20maintain%20a%20structured%20feature%20space.%20A%20generative%0Aapproach%20aligns%20the%20feature%20space%20with%20sampled%20IAE%20severity%2C%20enabling%20robust%0Aclassification%20and%20severity%20regression%20via%20a%20transformer.%20Evaluated%20on%20the%0AMultiBypass140%20dataset%2C%20which%20we%20extended%20with%20IAE%20labels%2C%20BetaMixer%20achieves%20a%0Aweighted%20F1%20score%20of%200.76%2C%20recall%20of%200.81%2C%20PPV%20of%200.73%2C%20and%20NPV%20of%200.84%2C%0Ademonstrating%20strong%20performance%20on%20imbalanced%20data.%20By%20integrating%20Beta%0Adistribution-based%20sampling%2C%20feature%20mixing%2C%20and%20generative%20modeling%2C%20BetaMixer%0Aoffers%20a%20robust%20solution%20for%20IAE%20detection%20and%20quantification%20in%20clinical%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16749v1&entry.124074799=Read"},
{"title": "Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning", "author": "Arun Sharma and Majid Farhadloo and Mingzhou Yang and Ruolei Zeng and Subhankar Ghosh and Shashi Shekhar", "abstract": "  Given inputs of diverse soil characteristics and climate data gathered from\nvarious regions, we aimed to build a model to predict accurate land emissions.\nThe problem is important since accurate quantification of the carbon cycle in\nagroecosystems is crucial for mitigating climate change and ensuring\nsustainable food production. Predicting accurate land emissions is challenging\nsince calibrating the heterogeneous nature of soil properties, moisture, and\nenvironmental conditions is hard at decision-relevant scales. Traditional\napproaches do not adequately estimate land emissions due to\nlocation-independent parameters failing to leverage the spatial heterogeneity\nand also require large datasets. To overcome these limitations, we proposed\nSpatial Distribution-Shift Aware Knowledge-Guided Machine Learning (SDSA-KGML),\nwhich leverages location-dependent parameters that account for significant\nspatial heterogeneity in soil moisture from multiple sites within the same\nregion. Experimental results demonstrate that SDSA-KGML models achieve higher\nlocal accuracy for the specified states in the Midwest Region.\n", "link": "http://arxiv.org/abs/2502.14840v2", "date": "2025-04-23", "relevancy": 1.4574, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5054}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4872}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Distribution-Shift%20Aware%20Knowledge-Guided%20Machine%20Learning&body=Title%3A%20Spatial%20Distribution-Shift%20Aware%20Knowledge-Guided%20Machine%20Learning%0AAuthor%3A%20Arun%20Sharma%20and%20Majid%20Farhadloo%20and%20Mingzhou%20Yang%20and%20Ruolei%20Zeng%20and%20Subhankar%20Ghosh%20and%20Shashi%20Shekhar%0AAbstract%3A%20%20%20Given%20inputs%20of%20diverse%20soil%20characteristics%20and%20climate%20data%20gathered%20from%0Avarious%20regions%2C%20we%20aimed%20to%20build%20a%20model%20to%20predict%20accurate%20land%20emissions.%0AThe%20problem%20is%20important%20since%20accurate%20quantification%20of%20the%20carbon%20cycle%20in%0Aagroecosystems%20is%20crucial%20for%20mitigating%20climate%20change%20and%20ensuring%0Asustainable%20food%20production.%20Predicting%20accurate%20land%20emissions%20is%20challenging%0Asince%20calibrating%20the%20heterogeneous%20nature%20of%20soil%20properties%2C%20moisture%2C%20and%0Aenvironmental%20conditions%20is%20hard%20at%20decision-relevant%20scales.%20Traditional%0Aapproaches%20do%20not%20adequately%20estimate%20land%20emissions%20due%20to%0Alocation-independent%20parameters%20failing%20to%20leverage%20the%20spatial%20heterogeneity%0Aand%20also%20require%20large%20datasets.%20To%20overcome%20these%20limitations%2C%20we%20proposed%0ASpatial%20Distribution-Shift%20Aware%20Knowledge-Guided%20Machine%20Learning%20%28SDSA-KGML%29%2C%0Awhich%20leverages%20location-dependent%20parameters%20that%20account%20for%20significant%0Aspatial%20heterogeneity%20in%20soil%20moisture%20from%20multiple%20sites%20within%20the%20same%0Aregion.%20Experimental%20results%20demonstrate%20that%20SDSA-KGML%20models%20achieve%20higher%0Alocal%20accuracy%20for%20the%20specified%20states%20in%20the%20Midwest%20Region.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14840v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Distribution-Shift%2520Aware%2520Knowledge-Guided%2520Machine%2520Learning%26entry.906535625%3DArun%2520Sharma%2520and%2520Majid%2520Farhadloo%2520and%2520Mingzhou%2520Yang%2520and%2520Ruolei%2520Zeng%2520and%2520Subhankar%2520Ghosh%2520and%2520Shashi%2520Shekhar%26entry.1292438233%3D%2520%2520Given%2520inputs%2520of%2520diverse%2520soil%2520characteristics%2520and%2520climate%2520data%2520gathered%2520from%250Avarious%2520regions%252C%2520we%2520aimed%2520to%2520build%2520a%2520model%2520to%2520predict%2520accurate%2520land%2520emissions.%250AThe%2520problem%2520is%2520important%2520since%2520accurate%2520quantification%2520of%2520the%2520carbon%2520cycle%2520in%250Aagroecosystems%2520is%2520crucial%2520for%2520mitigating%2520climate%2520change%2520and%2520ensuring%250Asustainable%2520food%2520production.%2520Predicting%2520accurate%2520land%2520emissions%2520is%2520challenging%250Asince%2520calibrating%2520the%2520heterogeneous%2520nature%2520of%2520soil%2520properties%252C%2520moisture%252C%2520and%250Aenvironmental%2520conditions%2520is%2520hard%2520at%2520decision-relevant%2520scales.%2520Traditional%250Aapproaches%2520do%2520not%2520adequately%2520estimate%2520land%2520emissions%2520due%2520to%250Alocation-independent%2520parameters%2520failing%2520to%2520leverage%2520the%2520spatial%2520heterogeneity%250Aand%2520also%2520require%2520large%2520datasets.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520proposed%250ASpatial%2520Distribution-Shift%2520Aware%2520Knowledge-Guided%2520Machine%2520Learning%2520%2528SDSA-KGML%2529%252C%250Awhich%2520leverages%2520location-dependent%2520parameters%2520that%2520account%2520for%2520significant%250Aspatial%2520heterogeneity%2520in%2520soil%2520moisture%2520from%2520multiple%2520sites%2520within%2520the%2520same%250Aregion.%2520Experimental%2520results%2520demonstrate%2520that%2520SDSA-KGML%2520models%2520achieve%2520higher%250Alocal%2520accuracy%2520for%2520the%2520specified%2520states%2520in%2520the%2520Midwest%2520Region.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14840v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Distribution-Shift%20Aware%20Knowledge-Guided%20Machine%20Learning&entry.906535625=Arun%20Sharma%20and%20Majid%20Farhadloo%20and%20Mingzhou%20Yang%20and%20Ruolei%20Zeng%20and%20Subhankar%20Ghosh%20and%20Shashi%20Shekhar&entry.1292438233=%20%20Given%20inputs%20of%20diverse%20soil%20characteristics%20and%20climate%20data%20gathered%20from%0Avarious%20regions%2C%20we%20aimed%20to%20build%20a%20model%20to%20predict%20accurate%20land%20emissions.%0AThe%20problem%20is%20important%20since%20accurate%20quantification%20of%20the%20carbon%20cycle%20in%0Aagroecosystems%20is%20crucial%20for%20mitigating%20climate%20change%20and%20ensuring%0Asustainable%20food%20production.%20Predicting%20accurate%20land%20emissions%20is%20challenging%0Asince%20calibrating%20the%20heterogeneous%20nature%20of%20soil%20properties%2C%20moisture%2C%20and%0Aenvironmental%20conditions%20is%20hard%20at%20decision-relevant%20scales.%20Traditional%0Aapproaches%20do%20not%20adequately%20estimate%20land%20emissions%20due%20to%0Alocation-independent%20parameters%20failing%20to%20leverage%20the%20spatial%20heterogeneity%0Aand%20also%20require%20large%20datasets.%20To%20overcome%20these%20limitations%2C%20we%20proposed%0ASpatial%20Distribution-Shift%20Aware%20Knowledge-Guided%20Machine%20Learning%20%28SDSA-KGML%29%2C%0Awhich%20leverages%20location-dependent%20parameters%20that%20account%20for%20significant%0Aspatial%20heterogeneity%20in%20soil%20moisture%20from%20multiple%20sites%20within%20the%20same%0Aregion.%20Experimental%20results%20demonstrate%20that%20SDSA-KGML%20models%20achieve%20higher%0Alocal%20accuracy%20for%20the%20specified%20states%20in%20the%20Midwest%20Region.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14840v2&entry.124074799=Read"},
{"title": "Rethinking and Recomputing the Value of Machine Learning Models", "author": "Burcu Sayin and Jie Yang and Xinyue Chen and Andrea Passerini and Fabio Casati", "abstract": "  In this paper, we argue that the prevailing approach to training and\nevaluating machine learning models often fails to consider their real-world\napplication within organizational or societal contexts, where they are intended\nto create beneficial value for people. We propose a shift in perspective,\nredefining model assessment and selection to emphasize integration into\nworkflows that combine machine predictions with human expertise, particularly\nin scenarios requiring human intervention for low-confidence predictions.\nTraditional metrics like accuracy and f-score fail to capture the beneficial\nvalue of models in such hybrid settings. To address this, we introduce a simple\nyet theoretically sound \"value\" metric that incorporates task-specific costs\nfor correct predictions, errors, and rejections, offering a practical framework\nfor real-world evaluation. Through extensive experiments, we show that existing\nmetrics fail to capture real-world needs, often leading to suboptimal choices\nin terms of value when used to rank classifiers. Furthermore, we emphasize the\ncritical role of calibration in determining model value, showing that simple,\nwell-calibrated models can often outperform more complex models that are\nchallenging to calibrate.\n", "link": "http://arxiv.org/abs/2209.15157v2", "date": "2025-04-23", "relevancy": 1.4386, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5019}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4801}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20and%20Recomputing%20the%20Value%20of%20Machine%20Learning%20Models&body=Title%3A%20Rethinking%20and%20Recomputing%20the%20Value%20of%20Machine%20Learning%20Models%0AAuthor%3A%20Burcu%20Sayin%20and%20Jie%20Yang%20and%20Xinyue%20Chen%20and%20Andrea%20Passerini%20and%20Fabio%20Casati%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20argue%20that%20the%20prevailing%20approach%20to%20training%20and%0Aevaluating%20machine%20learning%20models%20often%20fails%20to%20consider%20their%20real-world%0Aapplication%20within%20organizational%20or%20societal%20contexts%2C%20where%20they%20are%20intended%0Ato%20create%20beneficial%20value%20for%20people.%20We%20propose%20a%20shift%20in%20perspective%2C%0Aredefining%20model%20assessment%20and%20selection%20to%20emphasize%20integration%20into%0Aworkflows%20that%20combine%20machine%20predictions%20with%20human%20expertise%2C%20particularly%0Ain%20scenarios%20requiring%20human%20intervention%20for%20low-confidence%20predictions.%0ATraditional%20metrics%20like%20accuracy%20and%20f-score%20fail%20to%20capture%20the%20beneficial%0Avalue%20of%20models%20in%20such%20hybrid%20settings.%20To%20address%20this%2C%20we%20introduce%20a%20simple%0Ayet%20theoretically%20sound%20%22value%22%20metric%20that%20incorporates%20task-specific%20costs%0Afor%20correct%20predictions%2C%20errors%2C%20and%20rejections%2C%20offering%20a%20practical%20framework%0Afor%20real-world%20evaluation.%20Through%20extensive%20experiments%2C%20we%20show%20that%20existing%0Ametrics%20fail%20to%20capture%20real-world%20needs%2C%20often%20leading%20to%20suboptimal%20choices%0Ain%20terms%20of%20value%20when%20used%20to%20rank%20classifiers.%20Furthermore%2C%20we%20emphasize%20the%0Acritical%20role%20of%20calibration%20in%20determining%20model%20value%2C%20showing%20that%20simple%2C%0Awell-calibrated%20models%20can%20often%20outperform%20more%20complex%20models%20that%20are%0Achallenging%20to%20calibrate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.15157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520and%2520Recomputing%2520the%2520Value%2520of%2520Machine%2520Learning%2520Models%26entry.906535625%3DBurcu%2520Sayin%2520and%2520Jie%2520Yang%2520and%2520Xinyue%2520Chen%2520and%2520Andrea%2520Passerini%2520and%2520Fabio%2520Casati%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520the%2520prevailing%2520approach%2520to%2520training%2520and%250Aevaluating%2520machine%2520learning%2520models%2520often%2520fails%2520to%2520consider%2520their%2520real-world%250Aapplication%2520within%2520organizational%2520or%2520societal%2520contexts%252C%2520where%2520they%2520are%2520intended%250Ato%2520create%2520beneficial%2520value%2520for%2520people.%2520We%2520propose%2520a%2520shift%2520in%2520perspective%252C%250Aredefining%2520model%2520assessment%2520and%2520selection%2520to%2520emphasize%2520integration%2520into%250Aworkflows%2520that%2520combine%2520machine%2520predictions%2520with%2520human%2520expertise%252C%2520particularly%250Ain%2520scenarios%2520requiring%2520human%2520intervention%2520for%2520low-confidence%2520predictions.%250ATraditional%2520metrics%2520like%2520accuracy%2520and%2520f-score%2520fail%2520to%2520capture%2520the%2520beneficial%250Avalue%2520of%2520models%2520in%2520such%2520hybrid%2520settings.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520simple%250Ayet%2520theoretically%2520sound%2520%2522value%2522%2520metric%2520that%2520incorporates%2520task-specific%2520costs%250Afor%2520correct%2520predictions%252C%2520errors%252C%2520and%2520rejections%252C%2520offering%2520a%2520practical%2520framework%250Afor%2520real-world%2520evaluation.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520existing%250Ametrics%2520fail%2520to%2520capture%2520real-world%2520needs%252C%2520often%2520leading%2520to%2520suboptimal%2520choices%250Ain%2520terms%2520of%2520value%2520when%2520used%2520to%2520rank%2520classifiers.%2520Furthermore%252C%2520we%2520emphasize%2520the%250Acritical%2520role%2520of%2520calibration%2520in%2520determining%2520model%2520value%252C%2520showing%2520that%2520simple%252C%250Awell-calibrated%2520models%2520can%2520often%2520outperform%2520more%2520complex%2520models%2520that%2520are%250Achallenging%2520to%2520calibrate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.15157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20and%20Recomputing%20the%20Value%20of%20Machine%20Learning%20Models&entry.906535625=Burcu%20Sayin%20and%20Jie%20Yang%20and%20Xinyue%20Chen%20and%20Andrea%20Passerini%20and%20Fabio%20Casati&entry.1292438233=%20%20In%20this%20paper%2C%20we%20argue%20that%20the%20prevailing%20approach%20to%20training%20and%0Aevaluating%20machine%20learning%20models%20often%20fails%20to%20consider%20their%20real-world%0Aapplication%20within%20organizational%20or%20societal%20contexts%2C%20where%20they%20are%20intended%0Ato%20create%20beneficial%20value%20for%20people.%20We%20propose%20a%20shift%20in%20perspective%2C%0Aredefining%20model%20assessment%20and%20selection%20to%20emphasize%20integration%20into%0Aworkflows%20that%20combine%20machine%20predictions%20with%20human%20expertise%2C%20particularly%0Ain%20scenarios%20requiring%20human%20intervention%20for%20low-confidence%20predictions.%0ATraditional%20metrics%20like%20accuracy%20and%20f-score%20fail%20to%20capture%20the%20beneficial%0Avalue%20of%20models%20in%20such%20hybrid%20settings.%20To%20address%20this%2C%20we%20introduce%20a%20simple%0Ayet%20theoretically%20sound%20%22value%22%20metric%20that%20incorporates%20task-specific%20costs%0Afor%20correct%20predictions%2C%20errors%2C%20and%20rejections%2C%20offering%20a%20practical%20framework%0Afor%20real-world%20evaluation.%20Through%20extensive%20experiments%2C%20we%20show%20that%20existing%0Ametrics%20fail%20to%20capture%20real-world%20needs%2C%20often%20leading%20to%20suboptimal%20choices%0Ain%20terms%20of%20value%20when%20used%20to%20rank%20classifiers.%20Furthermore%2C%20we%20emphasize%20the%0Acritical%20role%20of%20calibration%20in%20determining%20model%20value%2C%20showing%20that%20simple%2C%0Awell-calibrated%20models%20can%20often%20outperform%20more%20complex%20models%20that%20are%0Achallenging%20to%20calibrate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.15157v2&entry.124074799=Read"},
{"title": "IRIS: Interactive Research Ideation System for Accelerating Scientific\n  Discovery", "author": "Aniketh Garikaparthi and Manasi Patwardhan and Lovekesh Vig and Arman Cohan", "abstract": "  The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System\n", "link": "http://arxiv.org/abs/2504.16728v1", "date": "2025-04-23", "relevancy": 1.4352, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5162}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRIS%3A%20Interactive%20Research%20Ideation%20System%20for%20Accelerating%20Scientific%0A%20%20Discovery&body=Title%3A%20IRIS%3A%20Interactive%20Research%20Ideation%20System%20for%20Accelerating%20Scientific%0A%20%20Discovery%0AAuthor%3A%20Aniketh%20Garikaparthi%20and%20Manasi%20Patwardhan%20and%20Lovekesh%20Vig%20and%20Arman%20Cohan%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20raises%0Aa%20pivotal%20question%3A%20How%20can%20LLMs%20accelerate%20scientific%20discovery%3F%20This%20work%0Atackles%20the%20crucial%20first%20stage%20of%20research%2C%20generating%20novel%20hypotheses.%20While%0Arecent%20work%20on%20automated%20hypothesis%20generation%20focuses%20on%20multi-agent%0Aframeworks%20and%20extending%20test-time%20compute%2C%20none%20of%20the%20approaches%20effectively%0Aincorporate%20transparency%20and%20steerability%20through%20a%20synergistic%0AHuman-in-the-loop%20%28HITL%29%20approach.%20To%20address%20this%20gap%2C%20we%20introduce%20IRIS%3A%0AInteractive%20Research%20Ideation%20System%2C%20an%20open-source%20platform%20designed%20for%0Aresearchers%20to%20leverage%20LLM-assisted%20scientific%20ideation.%20IRIS%20incorporates%0Ainnovative%20features%20to%20enhance%20ideation%2C%20including%20adaptive%20test-time%20compute%0Aexpansion%20via%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20fine-grained%20feedback%20mechanism%2C%0Aand%20query-based%20literature%20synthesis.%20Designed%20to%20empower%20researchers%20with%0Agreater%20control%20and%20insight%20throughout%20the%20ideation%20process.%20We%20additionally%0Aconduct%20a%20user%20study%20with%20researchers%20across%20diverse%20disciplines%2C%20validating%0Athe%20effectiveness%20of%20our%20system%20in%20enhancing%20ideation.%20We%20open-source%20our%20code%0Aat%20https%3A//github.com/Anikethh/IRIS-Interactive-Research-Ideation-System%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRIS%253A%2520Interactive%2520Research%2520Ideation%2520System%2520for%2520Accelerating%2520Scientific%250A%2520%2520Discovery%26entry.906535625%3DAniketh%2520Garikaparthi%2520and%2520Manasi%2520Patwardhan%2520and%2520Lovekesh%2520Vig%2520and%2520Arman%2520Cohan%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520raises%250Aa%2520pivotal%2520question%253A%2520How%2520can%2520LLMs%2520accelerate%2520scientific%2520discovery%253F%2520This%2520work%250Atackles%2520the%2520crucial%2520first%2520stage%2520of%2520research%252C%2520generating%2520novel%2520hypotheses.%2520While%250Arecent%2520work%2520on%2520automated%2520hypothesis%2520generation%2520focuses%2520on%2520multi-agent%250Aframeworks%2520and%2520extending%2520test-time%2520compute%252C%2520none%2520of%2520the%2520approaches%2520effectively%250Aincorporate%2520transparency%2520and%2520steerability%2520through%2520a%2520synergistic%250AHuman-in-the-loop%2520%2528HITL%2529%2520approach.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520IRIS%253A%250AInteractive%2520Research%2520Ideation%2520System%252C%2520an%2520open-source%2520platform%2520designed%2520for%250Aresearchers%2520to%2520leverage%2520LLM-assisted%2520scientific%2520ideation.%2520IRIS%2520incorporates%250Ainnovative%2520features%2520to%2520enhance%2520ideation%252C%2520including%2520adaptive%2520test-time%2520compute%250Aexpansion%2520via%2520Monte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%252C%2520fine-grained%2520feedback%2520mechanism%252C%250Aand%2520query-based%2520literature%2520synthesis.%2520Designed%2520to%2520empower%2520researchers%2520with%250Agreater%2520control%2520and%2520insight%2520throughout%2520the%2520ideation%2520process.%2520We%2520additionally%250Aconduct%2520a%2520user%2520study%2520with%2520researchers%2520across%2520diverse%2520disciplines%252C%2520validating%250Athe%2520effectiveness%2520of%2520our%2520system%2520in%2520enhancing%2520ideation.%2520We%2520open-source%2520our%2520code%250Aat%2520https%253A//github.com/Anikethh/IRIS-Interactive-Research-Ideation-System%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRIS%3A%20Interactive%20Research%20Ideation%20System%20for%20Accelerating%20Scientific%0A%20%20Discovery&entry.906535625=Aniketh%20Garikaparthi%20and%20Manasi%20Patwardhan%20and%20Lovekesh%20Vig%20and%20Arman%20Cohan&entry.1292438233=%20%20The%20rapid%20advancement%20in%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20raises%0Aa%20pivotal%20question%3A%20How%20can%20LLMs%20accelerate%20scientific%20discovery%3F%20This%20work%0Atackles%20the%20crucial%20first%20stage%20of%20research%2C%20generating%20novel%20hypotheses.%20While%0Arecent%20work%20on%20automated%20hypothesis%20generation%20focuses%20on%20multi-agent%0Aframeworks%20and%20extending%20test-time%20compute%2C%20none%20of%20the%20approaches%20effectively%0Aincorporate%20transparency%20and%20steerability%20through%20a%20synergistic%0AHuman-in-the-loop%20%28HITL%29%20approach.%20To%20address%20this%20gap%2C%20we%20introduce%20IRIS%3A%0AInteractive%20Research%20Ideation%20System%2C%20an%20open-source%20platform%20designed%20for%0Aresearchers%20to%20leverage%20LLM-assisted%20scientific%20ideation.%20IRIS%20incorporates%0Ainnovative%20features%20to%20enhance%20ideation%2C%20including%20adaptive%20test-time%20compute%0Aexpansion%20via%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20fine-grained%20feedback%20mechanism%2C%0Aand%20query-based%20literature%20synthesis.%20Designed%20to%20empower%20researchers%20with%0Agreater%20control%20and%20insight%20throughout%20the%20ideation%20process.%20We%20additionally%0Aconduct%20a%20user%20study%20with%20researchers%20across%20diverse%20disciplines%2C%20validating%0Athe%20effectiveness%20of%20our%20system%20in%20enhancing%20ideation.%20We%20open-source%20our%20code%0Aat%20https%3A//github.com/Anikethh/IRIS-Interactive-Research-Ideation-System%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16728v1&entry.124074799=Read"},
{"title": "Building A Secure Agentic AI Application Leveraging A2A Protocol", "author": "Idan Habler and Ken Huang and Vineeth Sai Narajala and Prashant Kulkarni", "abstract": "  As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust protocols such as Google's Agent2Agent (A2A) become\nessential enablers. To foster secure adoption and ensure the reliability of\nthese complex interactions, understanding the secure implementation of A2A is\nessential. This paper addresses this goal by providing a comprehensive security\nanalysis centered on the A2A protocol. We examine its fundamental elements and\noperational dynamics, situating it within the framework of agent communication\ndevelopment. Utilizing the MAESTRO framework, specifically designed for AI\nrisks, we apply proactive threat modeling to assess potential security issues\nin A2A deployments, focusing on aspects such as Agent Card management, task\nexecution integrity, and authentication methodologies.\n  Based on these insights, we recommend practical secure development\nmethodologies and architectural best practices designed to build resilient and\neffective A2A systems. Our analysis also explores how the synergy between A2A\nand the Model Context Protocol (MCP) can further enhance secure\ninteroperability. This paper equips developers and architects with the\nknowledge and practical guidance needed to confidently leverage the A2A\nprotocol for building robust and secure next generation agentic applications.\n", "link": "http://arxiv.org/abs/2504.16902v1", "date": "2025-04-23", "relevancy": 1.3728, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4833}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4379}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20A%20Secure%20Agentic%20AI%20Application%20Leveraging%20A2A%20Protocol&body=Title%3A%20Building%20A%20Secure%20Agentic%20AI%20Application%20Leveraging%20A2A%20Protocol%0AAuthor%3A%20Idan%20Habler%20and%20Ken%20Huang%20and%20Vineeth%20Sai%20Narajala%20and%20Prashant%20Kulkarni%0AAbstract%3A%20%20%20As%20Agentic%20AI%20systems%20evolve%20from%20basic%20workflows%20to%20complex%20multi%20agent%0Acollaboration%2C%20robust%20protocols%20such%20as%20Google%27s%20Agent2Agent%20%28A2A%29%20become%0Aessential%20enablers.%20To%20foster%20secure%20adoption%20and%20ensure%20the%20reliability%20of%0Athese%20complex%20interactions%2C%20understanding%20the%20secure%20implementation%20of%20A2A%20is%0Aessential.%20This%20paper%20addresses%20this%20goal%20by%20providing%20a%20comprehensive%20security%0Aanalysis%20centered%20on%20the%20A2A%20protocol.%20We%20examine%20its%20fundamental%20elements%20and%0Aoperational%20dynamics%2C%20situating%20it%20within%20the%20framework%20of%20agent%20communication%0Adevelopment.%20Utilizing%20the%20MAESTRO%20framework%2C%20specifically%20designed%20for%20AI%0Arisks%2C%20we%20apply%20proactive%20threat%20modeling%20to%20assess%20potential%20security%20issues%0Ain%20A2A%20deployments%2C%20focusing%20on%20aspects%20such%20as%20Agent%20Card%20management%2C%20task%0Aexecution%20integrity%2C%20and%20authentication%20methodologies.%0A%20%20Based%20on%20these%20insights%2C%20we%20recommend%20practical%20secure%20development%0Amethodologies%20and%20architectural%20best%20practices%20designed%20to%20build%20resilient%20and%0Aeffective%20A2A%20systems.%20Our%20analysis%20also%20explores%20how%20the%20synergy%20between%20A2A%0Aand%20the%20Model%20Context%20Protocol%20%28MCP%29%20can%20further%20enhance%20secure%0Ainteroperability.%20This%20paper%20equips%20developers%20and%20architects%20with%20the%0Aknowledge%20and%20practical%20guidance%20needed%20to%20confidently%20leverage%20the%20A2A%0Aprotocol%20for%20building%20robust%20and%20secure%20next%20generation%20agentic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520A%2520Secure%2520Agentic%2520AI%2520Application%2520Leveraging%2520A2A%2520Protocol%26entry.906535625%3DIdan%2520Habler%2520and%2520Ken%2520Huang%2520and%2520Vineeth%2520Sai%2520Narajala%2520and%2520Prashant%2520Kulkarni%26entry.1292438233%3D%2520%2520As%2520Agentic%2520AI%2520systems%2520evolve%2520from%2520basic%2520workflows%2520to%2520complex%2520multi%2520agent%250Acollaboration%252C%2520robust%2520protocols%2520such%2520as%2520Google%2527s%2520Agent2Agent%2520%2528A2A%2529%2520become%250Aessential%2520enablers.%2520To%2520foster%2520secure%2520adoption%2520and%2520ensure%2520the%2520reliability%2520of%250Athese%2520complex%2520interactions%252C%2520understanding%2520the%2520secure%2520implementation%2520of%2520A2A%2520is%250Aessential.%2520This%2520paper%2520addresses%2520this%2520goal%2520by%2520providing%2520a%2520comprehensive%2520security%250Aanalysis%2520centered%2520on%2520the%2520A2A%2520protocol.%2520We%2520examine%2520its%2520fundamental%2520elements%2520and%250Aoperational%2520dynamics%252C%2520situating%2520it%2520within%2520the%2520framework%2520of%2520agent%2520communication%250Adevelopment.%2520Utilizing%2520the%2520MAESTRO%2520framework%252C%2520specifically%2520designed%2520for%2520AI%250Arisks%252C%2520we%2520apply%2520proactive%2520threat%2520modeling%2520to%2520assess%2520potential%2520security%2520issues%250Ain%2520A2A%2520deployments%252C%2520focusing%2520on%2520aspects%2520such%2520as%2520Agent%2520Card%2520management%252C%2520task%250Aexecution%2520integrity%252C%2520and%2520authentication%2520methodologies.%250A%2520%2520Based%2520on%2520these%2520insights%252C%2520we%2520recommend%2520practical%2520secure%2520development%250Amethodologies%2520and%2520architectural%2520best%2520practices%2520designed%2520to%2520build%2520resilient%2520and%250Aeffective%2520A2A%2520systems.%2520Our%2520analysis%2520also%2520explores%2520how%2520the%2520synergy%2520between%2520A2A%250Aand%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529%2520can%2520further%2520enhance%2520secure%250Ainteroperability.%2520This%2520paper%2520equips%2520developers%2520and%2520architects%2520with%2520the%250Aknowledge%2520and%2520practical%2520guidance%2520needed%2520to%2520confidently%2520leverage%2520the%2520A2A%250Aprotocol%2520for%2520building%2520robust%2520and%2520secure%2520next%2520generation%2520agentic%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20A%20Secure%20Agentic%20AI%20Application%20Leveraging%20A2A%20Protocol&entry.906535625=Idan%20Habler%20and%20Ken%20Huang%20and%20Vineeth%20Sai%20Narajala%20and%20Prashant%20Kulkarni&entry.1292438233=%20%20As%20Agentic%20AI%20systems%20evolve%20from%20basic%20workflows%20to%20complex%20multi%20agent%0Acollaboration%2C%20robust%20protocols%20such%20as%20Google%27s%20Agent2Agent%20%28A2A%29%20become%0Aessential%20enablers.%20To%20foster%20secure%20adoption%20and%20ensure%20the%20reliability%20of%0Athese%20complex%20interactions%2C%20understanding%20the%20secure%20implementation%20of%20A2A%20is%0Aessential.%20This%20paper%20addresses%20this%20goal%20by%20providing%20a%20comprehensive%20security%0Aanalysis%20centered%20on%20the%20A2A%20protocol.%20We%20examine%20its%20fundamental%20elements%20and%0Aoperational%20dynamics%2C%20situating%20it%20within%20the%20framework%20of%20agent%20communication%0Adevelopment.%20Utilizing%20the%20MAESTRO%20framework%2C%20specifically%20designed%20for%20AI%0Arisks%2C%20we%20apply%20proactive%20threat%20modeling%20to%20assess%20potential%20security%20issues%0Ain%20A2A%20deployments%2C%20focusing%20on%20aspects%20such%20as%20Agent%20Card%20management%2C%20task%0Aexecution%20integrity%2C%20and%20authentication%20methodologies.%0A%20%20Based%20on%20these%20insights%2C%20we%20recommend%20practical%20secure%20development%0Amethodologies%20and%20architectural%20best%20practices%20designed%20to%20build%20resilient%20and%0Aeffective%20A2A%20systems.%20Our%20analysis%20also%20explores%20how%20the%20synergy%20between%20A2A%0Aand%20the%20Model%20Context%20Protocol%20%28MCP%29%20can%20further%20enhance%20secure%0Ainteroperability.%20This%20paper%20equips%20developers%20and%20architects%20with%20the%0Aknowledge%20and%20practical%20guidance%20needed%20to%20confidently%20leverage%20the%20A2A%0Aprotocol%20for%20building%20robust%20and%20secure%20next%20generation%20agentic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16902v1&entry.124074799=Read"},
{"title": "DreamO: A Unified Framework for Image Customization", "author": "Chong Mou and Yanze Wu and Wenxu Wu and Zinan Guo and Pengze Zhang and Yufeng Cheng and Yiming Luo and Fei Ding and Shiwen Zhang and Xinghui Li and Mengtian Li and Songtao Zhao and Jian Zhang and Qian He and Xinglong Wu", "abstract": "  Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.\n", "link": "http://arxiv.org/abs/2504.16915v1", "date": "2025-04-23", "relevancy": 1.1963, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6138}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5916}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamO%3A%20A%20Unified%20Framework%20for%20Image%20Customization&body=Title%3A%20DreamO%3A%20A%20Unified%20Framework%20for%20Image%20Customization%0AAuthor%3A%20Chong%20Mou%20and%20Yanze%20Wu%20and%20Wenxu%20Wu%20and%20Zinan%20Guo%20and%20Pengze%20Zhang%20and%20Yufeng%20Cheng%20and%20Yiming%20Luo%20and%20Fei%20Ding%20and%20Shiwen%20Zhang%20and%20Xinghui%20Li%20and%20Mengtian%20Li%20and%20Songtao%20Zhao%20and%20Jian%20Zhang%20and%20Qian%20He%20and%20Xinglong%20Wu%0AAbstract%3A%20%20%20Recently%2C%20extensive%20research%20on%20image%20customization%20%28e.g.%2C%20identity%2C%20subject%2C%0Astyle%2C%20background%2C%20etc.%29%20demonstrates%20strong%20customization%20capabilities%20in%0Alarge-scale%20generative%20models.%20However%2C%20most%20approaches%20are%20designed%20for%0Aspecific%20tasks%2C%20restricting%20their%20generalizability%20to%20combine%20different%20types%0Aof%20condition.%20Developing%20a%20unified%20framework%20for%20image%20customization%20remains%20an%0Aopen%20challenge.%20In%20this%20paper%2C%20we%20present%20DreamO%2C%20an%20image%20customization%0Aframework%20designed%20to%20support%20a%20wide%20range%20of%20tasks%20while%20facilitating%20seamless%0Aintegration%20of%20multiple%20conditions.%20Specifically%2C%20DreamO%20utilizes%20a%20diffusion%0Atransformer%20%28DiT%29%20framework%20to%20uniformly%20process%20input%20of%20different%20types.%0ADuring%20training%2C%20we%20construct%20a%20large-scale%20training%20dataset%20that%20includes%0Avarious%20customization%20tasks%2C%20and%20we%20introduce%20a%20feature%20routing%20constraint%20to%0Afacilitate%20the%20precise%20querying%20of%20relevant%20information%20from%20reference%20images.%0AAdditionally%2C%20we%20design%20a%20placeholder%20strategy%20that%20associates%20specific%0Aplaceholders%20with%20conditions%20at%20particular%20positions%2C%20enabling%20control%20over%20the%0Aplacement%20of%20conditions%20in%20the%20generated%20results.%20Moreover%2C%20we%20employ%20a%0Aprogressive%20training%20strategy%20consisting%20of%20three%20stages%3A%20an%20initial%20stage%0Afocused%20on%20simple%20tasks%20with%20limited%20data%20to%20establish%20baseline%20consistency%2C%20a%0Afull-scale%20training%20stage%20to%20comprehensively%20enhance%20the%20customization%0Acapabilities%2C%20and%20a%20final%20quality%20alignment%20stage%20to%20correct%20quality%20biases%0Aintroduced%20by%20low-quality%20data.%20Extensive%20experiments%20demonstrate%20that%20the%0Aproposed%20DreamO%20can%20effectively%20perform%20various%20image%20customization%20tasks%20with%0Ahigh%20quality%20and%20flexibly%20integrate%20different%20types%20of%20control%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamO%253A%2520A%2520Unified%2520Framework%2520for%2520Image%2520Customization%26entry.906535625%3DChong%2520Mou%2520and%2520Yanze%2520Wu%2520and%2520Wenxu%2520Wu%2520and%2520Zinan%2520Guo%2520and%2520Pengze%2520Zhang%2520and%2520Yufeng%2520Cheng%2520and%2520Yiming%2520Luo%2520and%2520Fei%2520Ding%2520and%2520Shiwen%2520Zhang%2520and%2520Xinghui%2520Li%2520and%2520Mengtian%2520Li%2520and%2520Songtao%2520Zhao%2520and%2520Jian%2520Zhang%2520and%2520Qian%2520He%2520and%2520Xinglong%2520Wu%26entry.1292438233%3D%2520%2520Recently%252C%2520extensive%2520research%2520on%2520image%2520customization%2520%2528e.g.%252C%2520identity%252C%2520subject%252C%250Astyle%252C%2520background%252C%2520etc.%2529%2520demonstrates%2520strong%2520customization%2520capabilities%2520in%250Alarge-scale%2520generative%2520models.%2520However%252C%2520most%2520approaches%2520are%2520designed%2520for%250Aspecific%2520tasks%252C%2520restricting%2520their%2520generalizability%2520to%2520combine%2520different%2520types%250Aof%2520condition.%2520Developing%2520a%2520unified%2520framework%2520for%2520image%2520customization%2520remains%2520an%250Aopen%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520present%2520DreamO%252C%2520an%2520image%2520customization%250Aframework%2520designed%2520to%2520support%2520a%2520wide%2520range%2520of%2520tasks%2520while%2520facilitating%2520seamless%250Aintegration%2520of%2520multiple%2520conditions.%2520Specifically%252C%2520DreamO%2520utilizes%2520a%2520diffusion%250Atransformer%2520%2528DiT%2529%2520framework%2520to%2520uniformly%2520process%2520input%2520of%2520different%2520types.%250ADuring%2520training%252C%2520we%2520construct%2520a%2520large-scale%2520training%2520dataset%2520that%2520includes%250Avarious%2520customization%2520tasks%252C%2520and%2520we%2520introduce%2520a%2520feature%2520routing%2520constraint%2520to%250Afacilitate%2520the%2520precise%2520querying%2520of%2520relevant%2520information%2520from%2520reference%2520images.%250AAdditionally%252C%2520we%2520design%2520a%2520placeholder%2520strategy%2520that%2520associates%2520specific%250Aplaceholders%2520with%2520conditions%2520at%2520particular%2520positions%252C%2520enabling%2520control%2520over%2520the%250Aplacement%2520of%2520conditions%2520in%2520the%2520generated%2520results.%2520Moreover%252C%2520we%2520employ%2520a%250Aprogressive%2520training%2520strategy%2520consisting%2520of%2520three%2520stages%253A%2520an%2520initial%2520stage%250Afocused%2520on%2520simple%2520tasks%2520with%2520limited%2520data%2520to%2520establish%2520baseline%2520consistency%252C%2520a%250Afull-scale%2520training%2520stage%2520to%2520comprehensively%2520enhance%2520the%2520customization%250Acapabilities%252C%2520and%2520a%2520final%2520quality%2520alignment%2520stage%2520to%2520correct%2520quality%2520biases%250Aintroduced%2520by%2520low-quality%2520data.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%250Aproposed%2520DreamO%2520can%2520effectively%2520perform%2520various%2520image%2520customization%2520tasks%2520with%250Ahigh%2520quality%2520and%2520flexibly%2520integrate%2520different%2520types%2520of%2520control%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamO%3A%20A%20Unified%20Framework%20for%20Image%20Customization&entry.906535625=Chong%20Mou%20and%20Yanze%20Wu%20and%20Wenxu%20Wu%20and%20Zinan%20Guo%20and%20Pengze%20Zhang%20and%20Yufeng%20Cheng%20and%20Yiming%20Luo%20and%20Fei%20Ding%20and%20Shiwen%20Zhang%20and%20Xinghui%20Li%20and%20Mengtian%20Li%20and%20Songtao%20Zhao%20and%20Jian%20Zhang%20and%20Qian%20He%20and%20Xinglong%20Wu&entry.1292438233=%20%20Recently%2C%20extensive%20research%20on%20image%20customization%20%28e.g.%2C%20identity%2C%20subject%2C%0Astyle%2C%20background%2C%20etc.%29%20demonstrates%20strong%20customization%20capabilities%20in%0Alarge-scale%20generative%20models.%20However%2C%20most%20approaches%20are%20designed%20for%0Aspecific%20tasks%2C%20restricting%20their%20generalizability%20to%20combine%20different%20types%0Aof%20condition.%20Developing%20a%20unified%20framework%20for%20image%20customization%20remains%20an%0Aopen%20challenge.%20In%20this%20paper%2C%20we%20present%20DreamO%2C%20an%20image%20customization%0Aframework%20designed%20to%20support%20a%20wide%20range%20of%20tasks%20while%20facilitating%20seamless%0Aintegration%20of%20multiple%20conditions.%20Specifically%2C%20DreamO%20utilizes%20a%20diffusion%0Atransformer%20%28DiT%29%20framework%20to%20uniformly%20process%20input%20of%20different%20types.%0ADuring%20training%2C%20we%20construct%20a%20large-scale%20training%20dataset%20that%20includes%0Avarious%20customization%20tasks%2C%20and%20we%20introduce%20a%20feature%20routing%20constraint%20to%0Afacilitate%20the%20precise%20querying%20of%20relevant%20information%20from%20reference%20images.%0AAdditionally%2C%20we%20design%20a%20placeholder%20strategy%20that%20associates%20specific%0Aplaceholders%20with%20conditions%20at%20particular%20positions%2C%20enabling%20control%20over%20the%0Aplacement%20of%20conditions%20in%20the%20generated%20results.%20Moreover%2C%20we%20employ%20a%0Aprogressive%20training%20strategy%20consisting%20of%20three%20stages%3A%20an%20initial%20stage%0Afocused%20on%20simple%20tasks%20with%20limited%20data%20to%20establish%20baseline%20consistency%2C%20a%0Afull-scale%20training%20stage%20to%20comprehensively%20enhance%20the%20customization%0Acapabilities%2C%20and%20a%20final%20quality%20alignment%20stage%20to%20correct%20quality%20biases%0Aintroduced%20by%20low-quality%20data.%20Extensive%20experiments%20demonstrate%20that%20the%0Aproposed%20DreamO%20can%20effectively%20perform%20various%20image%20customization%20tasks%20with%0Ahigh%20quality%20and%20flexibly%20integrate%20different%20types%20of%20control%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16915v1&entry.124074799=Read"},
{"title": "Simple Graph Contrastive Learning via Fractional-order Neural Diffusion\n  Networks", "author": "Yanan Zhao and Feng Ji and Kai Zhao and Xuhao Li and Qiyu Kang and Wenfei Liang and Yahya Alkhatib and Xingchao Jian and Wee Peng Tay", "abstract": "  Graph Contrastive Learning (GCL) has recently made progress as an\nunsupervised graph representation learning paradigm. GCL approaches can be\ncategorized into augmentation-based and augmentation-free methods. The former\nrelies on complex data augmentations, while the latter depends on encoders that\ncan generate distinct views of the same input. Both approaches may require\nnegative samples for training. In this paper, we introduce a novel\naugmentation-free GCL framework based on graph neural diffusion models.\nSpecifically, we utilize learnable encoders governed by Fractional Differential\nEquations (FDE). Each FDE is characterized by an order parameter of the\ndifferential operator. We demonstrate that varying these parameters allows us\nto produce learnable encoders that generate diverse views, capturing either\nlocal or global information, for contrastive learning. Our model does not\nrequire negative samples for training and is applicable to both homophilic and\nheterophilic datasets. We demonstrate its effectiveness across various\ndatasets, achieving state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2504.16748v1", "date": "2025-04-23", "relevancy": 1.0507, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5365}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5213}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Graph%20Contrastive%20Learning%20via%20Fractional-order%20Neural%20Diffusion%0A%20%20Networks&body=Title%3A%20Simple%20Graph%20Contrastive%20Learning%20via%20Fractional-order%20Neural%20Diffusion%0A%20%20Networks%0AAuthor%3A%20Yanan%20Zhao%20and%20Feng%20Ji%20and%20Kai%20Zhao%20and%20Xuhao%20Li%20and%20Qiyu%20Kang%20and%20Wenfei%20Liang%20and%20Yahya%20Alkhatib%20and%20Xingchao%20Jian%20and%20Wee%20Peng%20Tay%0AAbstract%3A%20%20%20Graph%20Contrastive%20Learning%20%28GCL%29%20has%20recently%20made%20progress%20as%20an%0Aunsupervised%20graph%20representation%20learning%20paradigm.%20GCL%20approaches%20can%20be%0Acategorized%20into%20augmentation-based%20and%20augmentation-free%20methods.%20The%20former%0Arelies%20on%20complex%20data%20augmentations%2C%20while%20the%20latter%20depends%20on%20encoders%20that%0Acan%20generate%20distinct%20views%20of%20the%20same%20input.%20Both%20approaches%20may%20require%0Anegative%20samples%20for%20training.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aaugmentation-free%20GCL%20framework%20based%20on%20graph%20neural%20diffusion%20models.%0ASpecifically%2C%20we%20utilize%20learnable%20encoders%20governed%20by%20Fractional%20Differential%0AEquations%20%28FDE%29.%20Each%20FDE%20is%20characterized%20by%20an%20order%20parameter%20of%20the%0Adifferential%20operator.%20We%20demonstrate%20that%20varying%20these%20parameters%20allows%20us%0Ato%20produce%20learnable%20encoders%20that%20generate%20diverse%20views%2C%20capturing%20either%0Alocal%20or%20global%20information%2C%20for%20contrastive%20learning.%20Our%20model%20does%20not%0Arequire%20negative%20samples%20for%20training%20and%20is%20applicable%20to%20both%20homophilic%20and%0Aheterophilic%20datasets.%20We%20demonstrate%20its%20effectiveness%20across%20various%0Adatasets%2C%20achieving%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Graph%2520Contrastive%2520Learning%2520via%2520Fractional-order%2520Neural%2520Diffusion%250A%2520%2520Networks%26entry.906535625%3DYanan%2520Zhao%2520and%2520Feng%2520Ji%2520and%2520Kai%2520Zhao%2520and%2520Xuhao%2520Li%2520and%2520Qiyu%2520Kang%2520and%2520Wenfei%2520Liang%2520and%2520Yahya%2520Alkhatib%2520and%2520Xingchao%2520Jian%2520and%2520Wee%2520Peng%2520Tay%26entry.1292438233%3D%2520%2520Graph%2520Contrastive%2520Learning%2520%2528GCL%2529%2520has%2520recently%2520made%2520progress%2520as%2520an%250Aunsupervised%2520graph%2520representation%2520learning%2520paradigm.%2520GCL%2520approaches%2520can%2520be%250Acategorized%2520into%2520augmentation-based%2520and%2520augmentation-free%2520methods.%2520The%2520former%250Arelies%2520on%2520complex%2520data%2520augmentations%252C%2520while%2520the%2520latter%2520depends%2520on%2520encoders%2520that%250Acan%2520generate%2520distinct%2520views%2520of%2520the%2520same%2520input.%2520Both%2520approaches%2520may%2520require%250Anegative%2520samples%2520for%2520training.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Aaugmentation-free%2520GCL%2520framework%2520based%2520on%2520graph%2520neural%2520diffusion%2520models.%250ASpecifically%252C%2520we%2520utilize%2520learnable%2520encoders%2520governed%2520by%2520Fractional%2520Differential%250AEquations%2520%2528FDE%2529.%2520Each%2520FDE%2520is%2520characterized%2520by%2520an%2520order%2520parameter%2520of%2520the%250Adifferential%2520operator.%2520We%2520demonstrate%2520that%2520varying%2520these%2520parameters%2520allows%2520us%250Ato%2520produce%2520learnable%2520encoders%2520that%2520generate%2520diverse%2520views%252C%2520capturing%2520either%250Alocal%2520or%2520global%2520information%252C%2520for%2520contrastive%2520learning.%2520Our%2520model%2520does%2520not%250Arequire%2520negative%2520samples%2520for%2520training%2520and%2520is%2520applicable%2520to%2520both%2520homophilic%2520and%250Aheterophilic%2520datasets.%2520We%2520demonstrate%2520its%2520effectiveness%2520across%2520various%250Adatasets%252C%2520achieving%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Graph%20Contrastive%20Learning%20via%20Fractional-order%20Neural%20Diffusion%0A%20%20Networks&entry.906535625=Yanan%20Zhao%20and%20Feng%20Ji%20and%20Kai%20Zhao%20and%20Xuhao%20Li%20and%20Qiyu%20Kang%20and%20Wenfei%20Liang%20and%20Yahya%20Alkhatib%20and%20Xingchao%20Jian%20and%20Wee%20Peng%20Tay&entry.1292438233=%20%20Graph%20Contrastive%20Learning%20%28GCL%29%20has%20recently%20made%20progress%20as%20an%0Aunsupervised%20graph%20representation%20learning%20paradigm.%20GCL%20approaches%20can%20be%0Acategorized%20into%20augmentation-based%20and%20augmentation-free%20methods.%20The%20former%0Arelies%20on%20complex%20data%20augmentations%2C%20while%20the%20latter%20depends%20on%20encoders%20that%0Acan%20generate%20distinct%20views%20of%20the%20same%20input.%20Both%20approaches%20may%20require%0Anegative%20samples%20for%20training.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aaugmentation-free%20GCL%20framework%20based%20on%20graph%20neural%20diffusion%20models.%0ASpecifically%2C%20we%20utilize%20learnable%20encoders%20governed%20by%20Fractional%20Differential%0AEquations%20%28FDE%29.%20Each%20FDE%20is%20characterized%20by%20an%20order%20parameter%20of%20the%0Adifferential%20operator.%20We%20demonstrate%20that%20varying%20these%20parameters%20allows%20us%0Ato%20produce%20learnable%20encoders%20that%20generate%20diverse%20views%2C%20capturing%20either%0Alocal%20or%20global%20information%2C%20for%20contrastive%20learning.%20Our%20model%20does%20not%0Arequire%20negative%20samples%20for%20training%20and%20is%20applicable%20to%20both%20homophilic%20and%0Aheterophilic%20datasets.%20We%20demonstrate%20its%20effectiveness%20across%20various%0Adatasets%2C%20achieving%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16748v1&entry.124074799=Read"},
{"title": "Predicting sub-population specific viral evolution", "author": "Wenxian Shi and Menghua Wu and Regina Barzilay", "abstract": "  Forecasting the change in the distribution of viral variants is crucial for\ntherapeutic design and disease surveillance. This task poses significant\nmodeling challenges due to the sharp differences in virus distributions across\nsub-populations (e.g., countries) and their dynamic interactions. Existing\nmachine learning approaches that model the variant distribution as a whole are\nincapable of making location-specific predictions and ignore transmissions that\nshape the viral landscape. In this paper, we propose a sub-population specific\nprotein evolution model, which predicts the time-resolved distributions of\nviral proteins in different locations. The algorithm explicitly models the\ntransmission rates between sub-populations and learns their interdependence\nfrom data. The change in protein distributions across all sub-populations is\ndefined through a linear ordinary differential equation (ODE) parametrized by\ntransmission rates. Solving this ODE yields the likelihood of a given protein\noccurring in particular sub-populations. Multi-year evaluation on both\nSARS-CoV-2 and influenza A/H3N2 demonstrates that our model outperforms\nbaselines in accurately predicting distributions of viral proteins across\ncontinents and countries. We also find that the transmission rates learned from\ndata are consistent with the transmission pathways discovered by retrospective\nphylogenetic analysis.\n", "link": "http://arxiv.org/abs/2410.21518v2", "date": "2025-04-23", "relevancy": 1.2429, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4204}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4142}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20sub-population%20specific%20viral%20evolution&body=Title%3A%20Predicting%20sub-population%20specific%20viral%20evolution%0AAuthor%3A%20Wenxian%20Shi%20and%20Menghua%20Wu%20and%20Regina%20Barzilay%0AAbstract%3A%20%20%20Forecasting%20the%20change%20in%20the%20distribution%20of%20viral%20variants%20is%20crucial%20for%0Atherapeutic%20design%20and%20disease%20surveillance.%20This%20task%20poses%20significant%0Amodeling%20challenges%20due%20to%20the%20sharp%20differences%20in%20virus%20distributions%20across%0Asub-populations%20%28e.g.%2C%20countries%29%20and%20their%20dynamic%20interactions.%20Existing%0Amachine%20learning%20approaches%20that%20model%20the%20variant%20distribution%20as%20a%20whole%20are%0Aincapable%20of%20making%20location-specific%20predictions%20and%20ignore%20transmissions%20that%0Ashape%20the%20viral%20landscape.%20In%20this%20paper%2C%20we%20propose%20a%20sub-population%20specific%0Aprotein%20evolution%20model%2C%20which%20predicts%20the%20time-resolved%20distributions%20of%0Aviral%20proteins%20in%20different%20locations.%20The%20algorithm%20explicitly%20models%20the%0Atransmission%20rates%20between%20sub-populations%20and%20learns%20their%20interdependence%0Afrom%20data.%20The%20change%20in%20protein%20distributions%20across%20all%20sub-populations%20is%0Adefined%20through%20a%20linear%20ordinary%20differential%20equation%20%28ODE%29%20parametrized%20by%0Atransmission%20rates.%20Solving%20this%20ODE%20yields%20the%20likelihood%20of%20a%20given%20protein%0Aoccurring%20in%20particular%20sub-populations.%20Multi-year%20evaluation%20on%20both%0ASARS-CoV-2%20and%20influenza%20A/H3N2%20demonstrates%20that%20our%20model%20outperforms%0Abaselines%20in%20accurately%20predicting%20distributions%20of%20viral%20proteins%20across%0Acontinents%20and%20countries.%20We%20also%20find%20that%20the%20transmission%20rates%20learned%20from%0Adata%20are%20consistent%20with%20the%20transmission%20pathways%20discovered%20by%20retrospective%0Aphylogenetic%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21518v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520sub-population%2520specific%2520viral%2520evolution%26entry.906535625%3DWenxian%2520Shi%2520and%2520Menghua%2520Wu%2520and%2520Regina%2520Barzilay%26entry.1292438233%3D%2520%2520Forecasting%2520the%2520change%2520in%2520the%2520distribution%2520of%2520viral%2520variants%2520is%2520crucial%2520for%250Atherapeutic%2520design%2520and%2520disease%2520surveillance.%2520This%2520task%2520poses%2520significant%250Amodeling%2520challenges%2520due%2520to%2520the%2520sharp%2520differences%2520in%2520virus%2520distributions%2520across%250Asub-populations%2520%2528e.g.%252C%2520countries%2529%2520and%2520their%2520dynamic%2520interactions.%2520Existing%250Amachine%2520learning%2520approaches%2520that%2520model%2520the%2520variant%2520distribution%2520as%2520a%2520whole%2520are%250Aincapable%2520of%2520making%2520location-specific%2520predictions%2520and%2520ignore%2520transmissions%2520that%250Ashape%2520the%2520viral%2520landscape.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520sub-population%2520specific%250Aprotein%2520evolution%2520model%252C%2520which%2520predicts%2520the%2520time-resolved%2520distributions%2520of%250Aviral%2520proteins%2520in%2520different%2520locations.%2520The%2520algorithm%2520explicitly%2520models%2520the%250Atransmission%2520rates%2520between%2520sub-populations%2520and%2520learns%2520their%2520interdependence%250Afrom%2520data.%2520The%2520change%2520in%2520protein%2520distributions%2520across%2520all%2520sub-populations%2520is%250Adefined%2520through%2520a%2520linear%2520ordinary%2520differential%2520equation%2520%2528ODE%2529%2520parametrized%2520by%250Atransmission%2520rates.%2520Solving%2520this%2520ODE%2520yields%2520the%2520likelihood%2520of%2520a%2520given%2520protein%250Aoccurring%2520in%2520particular%2520sub-populations.%2520Multi-year%2520evaluation%2520on%2520both%250ASARS-CoV-2%2520and%2520influenza%2520A/H3N2%2520demonstrates%2520that%2520our%2520model%2520outperforms%250Abaselines%2520in%2520accurately%2520predicting%2520distributions%2520of%2520viral%2520proteins%2520across%250Acontinents%2520and%2520countries.%2520We%2520also%2520find%2520that%2520the%2520transmission%2520rates%2520learned%2520from%250Adata%2520are%2520consistent%2520with%2520the%2520transmission%2520pathways%2520discovered%2520by%2520retrospective%250Aphylogenetic%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21518v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20sub-population%20specific%20viral%20evolution&entry.906535625=Wenxian%20Shi%20and%20Menghua%20Wu%20and%20Regina%20Barzilay&entry.1292438233=%20%20Forecasting%20the%20change%20in%20the%20distribution%20of%20viral%20variants%20is%20crucial%20for%0Atherapeutic%20design%20and%20disease%20surveillance.%20This%20task%20poses%20significant%0Amodeling%20challenges%20due%20to%20the%20sharp%20differences%20in%20virus%20distributions%20across%0Asub-populations%20%28e.g.%2C%20countries%29%20and%20their%20dynamic%20interactions.%20Existing%0Amachine%20learning%20approaches%20that%20model%20the%20variant%20distribution%20as%20a%20whole%20are%0Aincapable%20of%20making%20location-specific%20predictions%20and%20ignore%20transmissions%20that%0Ashape%20the%20viral%20landscape.%20In%20this%20paper%2C%20we%20propose%20a%20sub-population%20specific%0Aprotein%20evolution%20model%2C%20which%20predicts%20the%20time-resolved%20distributions%20of%0Aviral%20proteins%20in%20different%20locations.%20The%20algorithm%20explicitly%20models%20the%0Atransmission%20rates%20between%20sub-populations%20and%20learns%20their%20interdependence%0Afrom%20data.%20The%20change%20in%20protein%20distributions%20across%20all%20sub-populations%20is%0Adefined%20through%20a%20linear%20ordinary%20differential%20equation%20%28ODE%29%20parametrized%20by%0Atransmission%20rates.%20Solving%20this%20ODE%20yields%20the%20likelihood%20of%20a%20given%20protein%0Aoccurring%20in%20particular%20sub-populations.%20Multi-year%20evaluation%20on%20both%0ASARS-CoV-2%20and%20influenza%20A/H3N2%20demonstrates%20that%20our%20model%20outperforms%0Abaselines%20in%20accurately%20predicting%20distributions%20of%20viral%20proteins%20across%0Acontinents%20and%20countries.%20We%20also%20find%20that%20the%20transmission%20rates%20learned%20from%0Adata%20are%20consistent%20with%20the%20transmission%20pathways%20discovered%20by%20retrospective%0Aphylogenetic%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21518v2&entry.124074799=Read"},
{"title": "Online model learning with data-assimilated reservoir computers", "author": "Andrea N\u00f3voa and Luca Magri", "abstract": "  We propose an online learning framework for forecasting nonlinear\nspatio-temporal signals (fields). The method integrates (i) dimensionality\nreduction, here, a simple proper orthogonal decomposition (POD) projection;\n(ii) a generalized autoregressive model to forecast reduced dynamics, here, a\nreservoir computer; (iii) online adaptation to update the reservoir computer\n(the model), here, ensemble sequential data assimilation.We demonstrate the\nframework on a wake past a cylinder governed by the Navier-Stokes equations,\nexploring the assimilation of full flow fields (projected onto POD modes) and\nsparse sensors. Three scenarios are examined: a na\\\"ive physical state\nestimation; a two-fold estimation of physical and reservoir states; and a\nthree-fold estimation that also adjusts the model parameters. The two-fold\nstrategy significantly improves ensemble convergence and reduces reconstruction\nerror compared to the na\\\"ive approach. The three-fold approach enables robust\nonline training of partially-trained reservoir computers, overcoming\nlimitations of a priori training. By unifying data-driven reduced order\nmodelling with Bayesian data assimilation, this work opens new opportunities\nfor scalable online model learning for nonlinear time series forecasting.\n", "link": "http://arxiv.org/abs/2504.16767v1", "date": "2025-04-23", "relevancy": 1.0269, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5188}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5174}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20model%20learning%20with%20data-assimilated%20reservoir%20computers&body=Title%3A%20Online%20model%20learning%20with%20data-assimilated%20reservoir%20computers%0AAuthor%3A%20Andrea%20N%C3%B3voa%20and%20Luca%20Magri%0AAbstract%3A%20%20%20We%20propose%20an%20online%20learning%20framework%20for%20forecasting%20nonlinear%0Aspatio-temporal%20signals%20%28fields%29.%20The%20method%20integrates%20%28i%29%20dimensionality%0Areduction%2C%20here%2C%20a%20simple%20proper%20orthogonal%20decomposition%20%28POD%29%20projection%3B%0A%28ii%29%20a%20generalized%20autoregressive%20model%20to%20forecast%20reduced%20dynamics%2C%20here%2C%20a%0Areservoir%20computer%3B%20%28iii%29%20online%20adaptation%20to%20update%20the%20reservoir%20computer%0A%28the%20model%29%2C%20here%2C%20ensemble%20sequential%20data%20assimilation.We%20demonstrate%20the%0Aframework%20on%20a%20wake%20past%20a%20cylinder%20governed%20by%20the%20Navier-Stokes%20equations%2C%0Aexploring%20the%20assimilation%20of%20full%20flow%20fields%20%28projected%20onto%20POD%20modes%29%20and%0Asparse%20sensors.%20Three%20scenarios%20are%20examined%3A%20a%20na%5C%22ive%20physical%20state%0Aestimation%3B%20a%20two-fold%20estimation%20of%20physical%20and%20reservoir%20states%3B%20and%20a%0Athree-fold%20estimation%20that%20also%20adjusts%20the%20model%20parameters.%20The%20two-fold%0Astrategy%20significantly%20improves%20ensemble%20convergence%20and%20reduces%20reconstruction%0Aerror%20compared%20to%20the%20na%5C%22ive%20approach.%20The%20three-fold%20approach%20enables%20robust%0Aonline%20training%20of%20partially-trained%20reservoir%20computers%2C%20overcoming%0Alimitations%20of%20a%20priori%20training.%20By%20unifying%20data-driven%20reduced%20order%0Amodelling%20with%20Bayesian%20data%20assimilation%2C%20this%20work%20opens%20new%20opportunities%0Afor%20scalable%20online%20model%20learning%20for%20nonlinear%20time%20series%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520model%2520learning%2520with%2520data-assimilated%2520reservoir%2520computers%26entry.906535625%3DAndrea%2520N%25C3%25B3voa%2520and%2520Luca%2520Magri%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520online%2520learning%2520framework%2520for%2520forecasting%2520nonlinear%250Aspatio-temporal%2520signals%2520%2528fields%2529.%2520The%2520method%2520integrates%2520%2528i%2529%2520dimensionality%250Areduction%252C%2520here%252C%2520a%2520simple%2520proper%2520orthogonal%2520decomposition%2520%2528POD%2529%2520projection%253B%250A%2528ii%2529%2520a%2520generalized%2520autoregressive%2520model%2520to%2520forecast%2520reduced%2520dynamics%252C%2520here%252C%2520a%250Areservoir%2520computer%253B%2520%2528iii%2529%2520online%2520adaptation%2520to%2520update%2520the%2520reservoir%2520computer%250A%2528the%2520model%2529%252C%2520here%252C%2520ensemble%2520sequential%2520data%2520assimilation.We%2520demonstrate%2520the%250Aframework%2520on%2520a%2520wake%2520past%2520a%2520cylinder%2520governed%2520by%2520the%2520Navier-Stokes%2520equations%252C%250Aexploring%2520the%2520assimilation%2520of%2520full%2520flow%2520fields%2520%2528projected%2520onto%2520POD%2520modes%2529%2520and%250Asparse%2520sensors.%2520Three%2520scenarios%2520are%2520examined%253A%2520a%2520na%255C%2522ive%2520physical%2520state%250Aestimation%253B%2520a%2520two-fold%2520estimation%2520of%2520physical%2520and%2520reservoir%2520states%253B%2520and%2520a%250Athree-fold%2520estimation%2520that%2520also%2520adjusts%2520the%2520model%2520parameters.%2520The%2520two-fold%250Astrategy%2520significantly%2520improves%2520ensemble%2520convergence%2520and%2520reduces%2520reconstruction%250Aerror%2520compared%2520to%2520the%2520na%255C%2522ive%2520approach.%2520The%2520three-fold%2520approach%2520enables%2520robust%250Aonline%2520training%2520of%2520partially-trained%2520reservoir%2520computers%252C%2520overcoming%250Alimitations%2520of%2520a%2520priori%2520training.%2520By%2520unifying%2520data-driven%2520reduced%2520order%250Amodelling%2520with%2520Bayesian%2520data%2520assimilation%252C%2520this%2520work%2520opens%2520new%2520opportunities%250Afor%2520scalable%2520online%2520model%2520learning%2520for%2520nonlinear%2520time%2520series%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20model%20learning%20with%20data-assimilated%20reservoir%20computers&entry.906535625=Andrea%20N%C3%B3voa%20and%20Luca%20Magri&entry.1292438233=%20%20We%20propose%20an%20online%20learning%20framework%20for%20forecasting%20nonlinear%0Aspatio-temporal%20signals%20%28fields%29.%20The%20method%20integrates%20%28i%29%20dimensionality%0Areduction%2C%20here%2C%20a%20simple%20proper%20orthogonal%20decomposition%20%28POD%29%20projection%3B%0A%28ii%29%20a%20generalized%20autoregressive%20model%20to%20forecast%20reduced%20dynamics%2C%20here%2C%20a%0Areservoir%20computer%3B%20%28iii%29%20online%20adaptation%20to%20update%20the%20reservoir%20computer%0A%28the%20model%29%2C%20here%2C%20ensemble%20sequential%20data%20assimilation.We%20demonstrate%20the%0Aframework%20on%20a%20wake%20past%20a%20cylinder%20governed%20by%20the%20Navier-Stokes%20equations%2C%0Aexploring%20the%20assimilation%20of%20full%20flow%20fields%20%28projected%20onto%20POD%20modes%29%20and%0Asparse%20sensors.%20Three%20scenarios%20are%20examined%3A%20a%20na%5C%22ive%20physical%20state%0Aestimation%3B%20a%20two-fold%20estimation%20of%20physical%20and%20reservoir%20states%3B%20and%20a%0Athree-fold%20estimation%20that%20also%20adjusts%20the%20model%20parameters.%20The%20two-fold%0Astrategy%20significantly%20improves%20ensemble%20convergence%20and%20reduces%20reconstruction%0Aerror%20compared%20to%20the%20na%5C%22ive%20approach.%20The%20three-fold%20approach%20enables%20robust%0Aonline%20training%20of%20partially-trained%20reservoir%20computers%2C%20overcoming%0Alimitations%20of%20a%20priori%20training.%20By%20unifying%20data-driven%20reduced%20order%0Amodelling%20with%20Bayesian%20data%20assimilation%2C%20this%20work%20opens%20new%20opportunities%0Afor%20scalable%20online%20model%20learning%20for%20nonlinear%20time%20series%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16767v1&entry.124074799=Read"},
{"title": "Pushing the Frontier on Approximate EFX Allocations", "author": "Georgios Amanatidis and Aris Filos-Ratsikas and Alkmini Sgouritsa", "abstract": "  We study the problem of allocating a set of indivisible goods to a set of\nagents with additive valuation functions, aiming to achieve approximate\nenvy-freeness up to any good ($\\alpha$-EFX). The state-of-the-art results on\nthe problem include that (exact) EFX allocations exist when (a) there are at\nmost three agents, or (b) the agents' valuation functions can take at most two\nvalues, or (c) the agents' valuation functions can be represented via a graph.\nFor $\\alpha$-EFX, it is known that a $0.618$-EFX allocation exists for any\nnumber of agents with additive valuation functions. In this paper, we show that\n$2/3$-EFX allocations exist when (a) there are at most \\emph{seven agents}, (b)\nthe agents' valuation functions can take at most \\emph{three values}, or (c)\nthe agents' valuation functions can be represented via a \\emph{multigraph}. Our\nresults can be interpreted in two ways. First, by relaxing the notion of EFX to\n$2/3$-EFX, we obtain existence results for strict generalizations of the\nsettings for which exact EFX allocations are known to exist. Secondly, by\nimposing restrictions on the setting, we manage to beat the barrier of $0.618$\nand achieve an approximation guarantee of $2/3$. Therefore, our results push\nthe \\emph{frontier} of existence and computation of approximate EFX\nallocations, and provide insights into the challenges of settling the existence\nof exact EFX allocations.\n", "link": "http://arxiv.org/abs/2406.12413v2", "date": "2025-04-23", "relevancy": 1.1542, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4043}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.384}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pushing%20the%20Frontier%20on%20Approximate%20EFX%20Allocations&body=Title%3A%20Pushing%20the%20Frontier%20on%20Approximate%20EFX%20Allocations%0AAuthor%3A%20Georgios%20Amanatidis%20and%20Aris%20Filos-Ratsikas%20and%20Alkmini%20Sgouritsa%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20allocating%20a%20set%20of%20indivisible%20goods%20to%20a%20set%20of%0Aagents%20with%20additive%20valuation%20functions%2C%20aiming%20to%20achieve%20approximate%0Aenvy-freeness%20up%20to%20any%20good%20%28%24%5Calpha%24-EFX%29.%20The%20state-of-the-art%20results%20on%0Athe%20problem%20include%20that%20%28exact%29%20EFX%20allocations%20exist%20when%20%28a%29%20there%20are%20at%0Amost%20three%20agents%2C%20or%20%28b%29%20the%20agents%27%20valuation%20functions%20can%20take%20at%20most%20two%0Avalues%2C%20or%20%28c%29%20the%20agents%27%20valuation%20functions%20can%20be%20represented%20via%20a%20graph.%0AFor%20%24%5Calpha%24-EFX%2C%20it%20is%20known%20that%20a%20%240.618%24-EFX%20allocation%20exists%20for%20any%0Anumber%20of%20agents%20with%20additive%20valuation%20functions.%20In%20this%20paper%2C%20we%20show%20that%0A%242/3%24-EFX%20allocations%20exist%20when%20%28a%29%20there%20are%20at%20most%20%5Cemph%7Bseven%20agents%7D%2C%20%28b%29%0Athe%20agents%27%20valuation%20functions%20can%20take%20at%20most%20%5Cemph%7Bthree%20values%7D%2C%20or%20%28c%29%0Athe%20agents%27%20valuation%20functions%20can%20be%20represented%20via%20a%20%5Cemph%7Bmultigraph%7D.%20Our%0Aresults%20can%20be%20interpreted%20in%20two%20ways.%20First%2C%20by%20relaxing%20the%20notion%20of%20EFX%20to%0A%242/3%24-EFX%2C%20we%20obtain%20existence%20results%20for%20strict%20generalizations%20of%20the%0Asettings%20for%20which%20exact%20EFX%20allocations%20are%20known%20to%20exist.%20Secondly%2C%20by%0Aimposing%20restrictions%20on%20the%20setting%2C%20we%20manage%20to%20beat%20the%20barrier%20of%20%240.618%24%0Aand%20achieve%20an%20approximation%20guarantee%20of%20%242/3%24.%20Therefore%2C%20our%20results%20push%0Athe%20%5Cemph%7Bfrontier%7D%20of%20existence%20and%20computation%20of%20approximate%20EFX%0Aallocations%2C%20and%20provide%20insights%20into%20the%20challenges%20of%20settling%20the%20existence%0Aof%20exact%20EFX%20allocations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12413v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPushing%2520the%2520Frontier%2520on%2520Approximate%2520EFX%2520Allocations%26entry.906535625%3DGeorgios%2520Amanatidis%2520and%2520Aris%2520Filos-Ratsikas%2520and%2520Alkmini%2520Sgouritsa%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520allocating%2520a%2520set%2520of%2520indivisible%2520goods%2520to%2520a%2520set%2520of%250Aagents%2520with%2520additive%2520valuation%2520functions%252C%2520aiming%2520to%2520achieve%2520approximate%250Aenvy-freeness%2520up%2520to%2520any%2520good%2520%2528%2524%255Calpha%2524-EFX%2529.%2520The%2520state-of-the-art%2520results%2520on%250Athe%2520problem%2520include%2520that%2520%2528exact%2529%2520EFX%2520allocations%2520exist%2520when%2520%2528a%2529%2520there%2520are%2520at%250Amost%2520three%2520agents%252C%2520or%2520%2528b%2529%2520the%2520agents%2527%2520valuation%2520functions%2520can%2520take%2520at%2520most%2520two%250Avalues%252C%2520or%2520%2528c%2529%2520the%2520agents%2527%2520valuation%2520functions%2520can%2520be%2520represented%2520via%2520a%2520graph.%250AFor%2520%2524%255Calpha%2524-EFX%252C%2520it%2520is%2520known%2520that%2520a%2520%25240.618%2524-EFX%2520allocation%2520exists%2520for%2520any%250Anumber%2520of%2520agents%2520with%2520additive%2520valuation%2520functions.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%250A%25242/3%2524-EFX%2520allocations%2520exist%2520when%2520%2528a%2529%2520there%2520are%2520at%2520most%2520%255Cemph%257Bseven%2520agents%257D%252C%2520%2528b%2529%250Athe%2520agents%2527%2520valuation%2520functions%2520can%2520take%2520at%2520most%2520%255Cemph%257Bthree%2520values%257D%252C%2520or%2520%2528c%2529%250Athe%2520agents%2527%2520valuation%2520functions%2520can%2520be%2520represented%2520via%2520a%2520%255Cemph%257Bmultigraph%257D.%2520Our%250Aresults%2520can%2520be%2520interpreted%2520in%2520two%2520ways.%2520First%252C%2520by%2520relaxing%2520the%2520notion%2520of%2520EFX%2520to%250A%25242/3%2524-EFX%252C%2520we%2520obtain%2520existence%2520results%2520for%2520strict%2520generalizations%2520of%2520the%250Asettings%2520for%2520which%2520exact%2520EFX%2520allocations%2520are%2520known%2520to%2520exist.%2520Secondly%252C%2520by%250Aimposing%2520restrictions%2520on%2520the%2520setting%252C%2520we%2520manage%2520to%2520beat%2520the%2520barrier%2520of%2520%25240.618%2524%250Aand%2520achieve%2520an%2520approximation%2520guarantee%2520of%2520%25242/3%2524.%2520Therefore%252C%2520our%2520results%2520push%250Athe%2520%255Cemph%257Bfrontier%257D%2520of%2520existence%2520and%2520computation%2520of%2520approximate%2520EFX%250Aallocations%252C%2520and%2520provide%2520insights%2520into%2520the%2520challenges%2520of%2520settling%2520the%2520existence%250Aof%2520exact%2520EFX%2520allocations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12413v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pushing%20the%20Frontier%20on%20Approximate%20EFX%20Allocations&entry.906535625=Georgios%20Amanatidis%20and%20Aris%20Filos-Ratsikas%20and%20Alkmini%20Sgouritsa&entry.1292438233=%20%20We%20study%20the%20problem%20of%20allocating%20a%20set%20of%20indivisible%20goods%20to%20a%20set%20of%0Aagents%20with%20additive%20valuation%20functions%2C%20aiming%20to%20achieve%20approximate%0Aenvy-freeness%20up%20to%20any%20good%20%28%24%5Calpha%24-EFX%29.%20The%20state-of-the-art%20results%20on%0Athe%20problem%20include%20that%20%28exact%29%20EFX%20allocations%20exist%20when%20%28a%29%20there%20are%20at%0Amost%20three%20agents%2C%20or%20%28b%29%20the%20agents%27%20valuation%20functions%20can%20take%20at%20most%20two%0Avalues%2C%20or%20%28c%29%20the%20agents%27%20valuation%20functions%20can%20be%20represented%20via%20a%20graph.%0AFor%20%24%5Calpha%24-EFX%2C%20it%20is%20known%20that%20a%20%240.618%24-EFX%20allocation%20exists%20for%20any%0Anumber%20of%20agents%20with%20additive%20valuation%20functions.%20In%20this%20paper%2C%20we%20show%20that%0A%242/3%24-EFX%20allocations%20exist%20when%20%28a%29%20there%20are%20at%20most%20%5Cemph%7Bseven%20agents%7D%2C%20%28b%29%0Athe%20agents%27%20valuation%20functions%20can%20take%20at%20most%20%5Cemph%7Bthree%20values%7D%2C%20or%20%28c%29%0Athe%20agents%27%20valuation%20functions%20can%20be%20represented%20via%20a%20%5Cemph%7Bmultigraph%7D.%20Our%0Aresults%20can%20be%20interpreted%20in%20two%20ways.%20First%2C%20by%20relaxing%20the%20notion%20of%20EFX%20to%0A%242/3%24-EFX%2C%20we%20obtain%20existence%20results%20for%20strict%20generalizations%20of%20the%0Asettings%20for%20which%20exact%20EFX%20allocations%20are%20known%20to%20exist.%20Secondly%2C%20by%0Aimposing%20restrictions%20on%20the%20setting%2C%20we%20manage%20to%20beat%20the%20barrier%20of%20%240.618%24%0Aand%20achieve%20an%20approximation%20guarantee%20of%20%242/3%24.%20Therefore%2C%20our%20results%20push%0Athe%20%5Cemph%7Bfrontier%7D%20of%20existence%20and%20computation%20of%20approximate%20EFX%0Aallocations%2C%20and%20provide%20insights%20into%20the%20challenges%20of%20settling%20the%20existence%0Aof%20exact%20EFX%20allocations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12413v2&entry.124074799=Read"},
{"title": "Common Functional Decompositions Can Mis-attribute Differences in\n  Outcomes Between Populations", "author": "Manuel Quintero and William T. Stephenson and Advik Shreekumar and Tamara Broderick", "abstract": "  In science and social science, we often wish to explain why an outcome is\ndifferent in two populations. For instance, if a jobs program benefits members\nof one city more than another, is that due to differences in program\nparticipants (particular covariates) or the local labor markets (outcomes given\ncovariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool\nin econometrics that explains the difference in the mean outcome across two\npopulations. However, the KOB decomposition assumes a linear relationship\nbetween covariates and outcomes, while the true relationship may be\nmeaningfully nonlinear. Modern machine learning boasts a variety of nonlinear\nfunctional decompositions for the relationship between outcomes and covariates\nin one population. It seems natural to extend the KOB decomposition using these\nfunctional decompositions. We observe that a successful extension should not\nattribute the differences to covariates -- or, respectively, to outcomes given\ncovariates -- if those are the same in the two populations. Unfortunately, we\ndemonstrate that, even in simple examples, two common decompositions --\nfunctional ANOVA and Accumulated Local Effects -- can attribute differences to\noutcomes given covariates, even when they are identical in two populations. We\nprovide a characterization of when functional ANOVA misattributes, as well as a\ngeneral property that any discrete decomposition must satisfy to avoid\nmisattribution. We show that if the decomposition is independent of its input\ndistribution, it does not misattribute. We further conjecture that\nmisattribution arises in any reasonable additive decomposition that depends on\nthe distribution of the covariates.\n", "link": "http://arxiv.org/abs/2504.16864v1", "date": "2025-04-23", "relevancy": 0.8285, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4303}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4239}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Common%20Functional%20Decompositions%20Can%20Mis-attribute%20Differences%20in%0A%20%20Outcomes%20Between%20Populations&body=Title%3A%20Common%20Functional%20Decompositions%20Can%20Mis-attribute%20Differences%20in%0A%20%20Outcomes%20Between%20Populations%0AAuthor%3A%20Manuel%20Quintero%20and%20William%20T.%20Stephenson%20and%20Advik%20Shreekumar%20and%20Tamara%20Broderick%0AAbstract%3A%20%20%20In%20science%20and%20social%20science%2C%20we%20often%20wish%20to%20explain%20why%20an%20outcome%20is%0Adifferent%20in%20two%20populations.%20For%20instance%2C%20if%20a%20jobs%20program%20benefits%20members%0Aof%20one%20city%20more%20than%20another%2C%20is%20that%20due%20to%20differences%20in%20program%0Aparticipants%20%28particular%20covariates%29%20or%20the%20local%20labor%20markets%20%28outcomes%20given%0Acovariates%29%3F%20The%20Kitagawa-Oaxaca-Blinder%20%28KOB%29%20decomposition%20is%20a%20standard%20tool%0Ain%20econometrics%20that%20explains%20the%20difference%20in%20the%20mean%20outcome%20across%20two%0Apopulations.%20However%2C%20the%20KOB%20decomposition%20assumes%20a%20linear%20relationship%0Abetween%20covariates%20and%20outcomes%2C%20while%20the%20true%20relationship%20may%20be%0Ameaningfully%20nonlinear.%20Modern%20machine%20learning%20boasts%20a%20variety%20of%20nonlinear%0Afunctional%20decompositions%20for%20the%20relationship%20between%20outcomes%20and%20covariates%0Ain%20one%20population.%20It%20seems%20natural%20to%20extend%20the%20KOB%20decomposition%20using%20these%0Afunctional%20decompositions.%20We%20observe%20that%20a%20successful%20extension%20should%20not%0Aattribute%20the%20differences%20to%20covariates%20--%20or%2C%20respectively%2C%20to%20outcomes%20given%0Acovariates%20--%20if%20those%20are%20the%20same%20in%20the%20two%20populations.%20Unfortunately%2C%20we%0Ademonstrate%20that%2C%20even%20in%20simple%20examples%2C%20two%20common%20decompositions%20--%0Afunctional%20ANOVA%20and%20Accumulated%20Local%20Effects%20--%20can%20attribute%20differences%20to%0Aoutcomes%20given%20covariates%2C%20even%20when%20they%20are%20identical%20in%20two%20populations.%20We%0Aprovide%20a%20characterization%20of%20when%20functional%20ANOVA%20misattributes%2C%20as%20well%20as%20a%0Ageneral%20property%20that%20any%20discrete%20decomposition%20must%20satisfy%20to%20avoid%0Amisattribution.%20We%20show%20that%20if%20the%20decomposition%20is%20independent%20of%20its%20input%0Adistribution%2C%20it%20does%20not%20misattribute.%20We%20further%20conjecture%20that%0Amisattribution%20arises%20in%20any%20reasonable%20additive%20decomposition%20that%20depends%20on%0Athe%20distribution%20of%20the%20covariates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommon%2520Functional%2520Decompositions%2520Can%2520Mis-attribute%2520Differences%2520in%250A%2520%2520Outcomes%2520Between%2520Populations%26entry.906535625%3DManuel%2520Quintero%2520and%2520William%2520T.%2520Stephenson%2520and%2520Advik%2520Shreekumar%2520and%2520Tamara%2520Broderick%26entry.1292438233%3D%2520%2520In%2520science%2520and%2520social%2520science%252C%2520we%2520often%2520wish%2520to%2520explain%2520why%2520an%2520outcome%2520is%250Adifferent%2520in%2520two%2520populations.%2520For%2520instance%252C%2520if%2520a%2520jobs%2520program%2520benefits%2520members%250Aof%2520one%2520city%2520more%2520than%2520another%252C%2520is%2520that%2520due%2520to%2520differences%2520in%2520program%250Aparticipants%2520%2528particular%2520covariates%2529%2520or%2520the%2520local%2520labor%2520markets%2520%2528outcomes%2520given%250Acovariates%2529%253F%2520The%2520Kitagawa-Oaxaca-Blinder%2520%2528KOB%2529%2520decomposition%2520is%2520a%2520standard%2520tool%250Ain%2520econometrics%2520that%2520explains%2520the%2520difference%2520in%2520the%2520mean%2520outcome%2520across%2520two%250Apopulations.%2520However%252C%2520the%2520KOB%2520decomposition%2520assumes%2520a%2520linear%2520relationship%250Abetween%2520covariates%2520and%2520outcomes%252C%2520while%2520the%2520true%2520relationship%2520may%2520be%250Ameaningfully%2520nonlinear.%2520Modern%2520machine%2520learning%2520boasts%2520a%2520variety%2520of%2520nonlinear%250Afunctional%2520decompositions%2520for%2520the%2520relationship%2520between%2520outcomes%2520and%2520covariates%250Ain%2520one%2520population.%2520It%2520seems%2520natural%2520to%2520extend%2520the%2520KOB%2520decomposition%2520using%2520these%250Afunctional%2520decompositions.%2520We%2520observe%2520that%2520a%2520successful%2520extension%2520should%2520not%250Aattribute%2520the%2520differences%2520to%2520covariates%2520--%2520or%252C%2520respectively%252C%2520to%2520outcomes%2520given%250Acovariates%2520--%2520if%2520those%2520are%2520the%2520same%2520in%2520the%2520two%2520populations.%2520Unfortunately%252C%2520we%250Ademonstrate%2520that%252C%2520even%2520in%2520simple%2520examples%252C%2520two%2520common%2520decompositions%2520--%250Afunctional%2520ANOVA%2520and%2520Accumulated%2520Local%2520Effects%2520--%2520can%2520attribute%2520differences%2520to%250Aoutcomes%2520given%2520covariates%252C%2520even%2520when%2520they%2520are%2520identical%2520in%2520two%2520populations.%2520We%250Aprovide%2520a%2520characterization%2520of%2520when%2520functional%2520ANOVA%2520misattributes%252C%2520as%2520well%2520as%2520a%250Ageneral%2520property%2520that%2520any%2520discrete%2520decomposition%2520must%2520satisfy%2520to%2520avoid%250Amisattribution.%2520We%2520show%2520that%2520if%2520the%2520decomposition%2520is%2520independent%2520of%2520its%2520input%250Adistribution%252C%2520it%2520does%2520not%2520misattribute.%2520We%2520further%2520conjecture%2520that%250Amisattribution%2520arises%2520in%2520any%2520reasonable%2520additive%2520decomposition%2520that%2520depends%2520on%250Athe%2520distribution%2520of%2520the%2520covariates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Common%20Functional%20Decompositions%20Can%20Mis-attribute%20Differences%20in%0A%20%20Outcomes%20Between%20Populations&entry.906535625=Manuel%20Quintero%20and%20William%20T.%20Stephenson%20and%20Advik%20Shreekumar%20and%20Tamara%20Broderick&entry.1292438233=%20%20In%20science%20and%20social%20science%2C%20we%20often%20wish%20to%20explain%20why%20an%20outcome%20is%0Adifferent%20in%20two%20populations.%20For%20instance%2C%20if%20a%20jobs%20program%20benefits%20members%0Aof%20one%20city%20more%20than%20another%2C%20is%20that%20due%20to%20differences%20in%20program%0Aparticipants%20%28particular%20covariates%29%20or%20the%20local%20labor%20markets%20%28outcomes%20given%0Acovariates%29%3F%20The%20Kitagawa-Oaxaca-Blinder%20%28KOB%29%20decomposition%20is%20a%20standard%20tool%0Ain%20econometrics%20that%20explains%20the%20difference%20in%20the%20mean%20outcome%20across%20two%0Apopulations.%20However%2C%20the%20KOB%20decomposition%20assumes%20a%20linear%20relationship%0Abetween%20covariates%20and%20outcomes%2C%20while%20the%20true%20relationship%20may%20be%0Ameaningfully%20nonlinear.%20Modern%20machine%20learning%20boasts%20a%20variety%20of%20nonlinear%0Afunctional%20decompositions%20for%20the%20relationship%20between%20outcomes%20and%20covariates%0Ain%20one%20population.%20It%20seems%20natural%20to%20extend%20the%20KOB%20decomposition%20using%20these%0Afunctional%20decompositions.%20We%20observe%20that%20a%20successful%20extension%20should%20not%0Aattribute%20the%20differences%20to%20covariates%20--%20or%2C%20respectively%2C%20to%20outcomes%20given%0Acovariates%20--%20if%20those%20are%20the%20same%20in%20the%20two%20populations.%20Unfortunately%2C%20we%0Ademonstrate%20that%2C%20even%20in%20simple%20examples%2C%20two%20common%20decompositions%20--%0Afunctional%20ANOVA%20and%20Accumulated%20Local%20Effects%20--%20can%20attribute%20differences%20to%0Aoutcomes%20given%20covariates%2C%20even%20when%20they%20are%20identical%20in%20two%20populations.%20We%0Aprovide%20a%20characterization%20of%20when%20functional%20ANOVA%20misattributes%2C%20as%20well%20as%20a%0Ageneral%20property%20that%20any%20discrete%20decomposition%20must%20satisfy%20to%20avoid%0Amisattribution.%20We%20show%20that%20if%20the%20decomposition%20is%20independent%20of%20its%20input%0Adistribution%2C%20it%20does%20not%20misattribute.%20We%20further%20conjecture%20that%0Amisattribution%20arises%20in%20any%20reasonable%20additive%20decomposition%20that%20depends%20on%0Athe%20distribution%20of%20the%20covariates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16864v1&entry.124074799=Read"},
{"title": "Improving Significant Wave Height Prediction Using Chronos Models", "author": "Yilin Zhai and Hongyuan Shi and Chao Zhan and Qing Wang and Zaijin You and Nan Wang", "abstract": "  Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling.\n", "link": "http://arxiv.org/abs/2504.16834v1", "date": "2025-04-23", "relevancy": 0.8835, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4486}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4479}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Significant%20Wave%20Height%20Prediction%20Using%20Chronos%20Models&body=Title%3A%20Improving%20Significant%20Wave%20Height%20Prediction%20Using%20Chronos%20Models%0AAuthor%3A%20Yilin%20Zhai%20and%20Hongyuan%20Shi%20and%20Chao%20Zhan%20and%20Qing%20Wang%20and%20Zaijin%20You%20and%20Nan%20Wang%0AAbstract%3A%20%20%20Accurate%20wave%20height%20prediction%20is%20critical%20for%20maritime%20safety%20and%20coastal%0Aresilience%2C%20yet%20conventional%20physics-based%20models%20and%20traditional%20machine%0Alearning%20methods%20face%20challenges%20in%20computational%20efficiency%20and%20nonlinear%0Adynamics%20modeling.%20This%20study%20introduces%20Chronos%2C%20the%20first%20implementation%20of%20a%0Alarge%20language%20model%20%28LLM%29-powered%20temporal%20architecture%20%28Chronos%29%20optimized%0Afor%20wave%20forecasting.%20Through%20advanced%20temporal%20pattern%20recognition%20applied%20to%0Ahistorical%20wave%20data%20from%20three%20strategically%20chosen%20marine%20zones%20in%20the%0ANorthwest%20Pacific%20basin%2C%20our%20framework%20achieves%20multimodal%20improvements%3A%20%281%29%0A14.3%25%20reduction%20in%20training%20time%20with%202.5x%20faster%20inference%20speed%20compared%20to%0APatchTST%20baselines%2C%20achieving%200.575%20mean%20absolute%20scaled%20error%20%28MASE%29%20units%3B%0A%282%29%20superior%20short-term%20forecasting%20%281-24h%29%20across%20comprehensive%20metrics%3B%20%283%29%0Asustained%20predictive%20leadership%20in%20extended-range%20forecasts%20%281-120h%29%3B%20and%20%284%29%0Ademonstrated%20zero-shot%20capability%20maintaining%20median%20performance%20%28rank%204/12%29%0Aagainst%20specialized%20operational%20models.%20This%20LLM-enhanced%20temporal%20modeling%0Aparadigm%20establishes%20a%20new%20standard%20in%20wave%20prediction%2C%20offering%20both%0Acomputationally%20efficient%20solutions%20and%20a%20transferable%20framework%20for%20complex%0Ageophysical%20systems%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Significant%2520Wave%2520Height%2520Prediction%2520Using%2520Chronos%2520Models%26entry.906535625%3DYilin%2520Zhai%2520and%2520Hongyuan%2520Shi%2520and%2520Chao%2520Zhan%2520and%2520Qing%2520Wang%2520and%2520Zaijin%2520You%2520and%2520Nan%2520Wang%26entry.1292438233%3D%2520%2520Accurate%2520wave%2520height%2520prediction%2520is%2520critical%2520for%2520maritime%2520safety%2520and%2520coastal%250Aresilience%252C%2520yet%2520conventional%2520physics-based%2520models%2520and%2520traditional%2520machine%250Alearning%2520methods%2520face%2520challenges%2520in%2520computational%2520efficiency%2520and%2520nonlinear%250Adynamics%2520modeling.%2520This%2520study%2520introduces%2520Chronos%252C%2520the%2520first%2520implementation%2520of%2520a%250Alarge%2520language%2520model%2520%2528LLM%2529-powered%2520temporal%2520architecture%2520%2528Chronos%2529%2520optimized%250Afor%2520wave%2520forecasting.%2520Through%2520advanced%2520temporal%2520pattern%2520recognition%2520applied%2520to%250Ahistorical%2520wave%2520data%2520from%2520three%2520strategically%2520chosen%2520marine%2520zones%2520in%2520the%250ANorthwest%2520Pacific%2520basin%252C%2520our%2520framework%2520achieves%2520multimodal%2520improvements%253A%2520%25281%2529%250A14.3%2525%2520reduction%2520in%2520training%2520time%2520with%25202.5x%2520faster%2520inference%2520speed%2520compared%2520to%250APatchTST%2520baselines%252C%2520achieving%25200.575%2520mean%2520absolute%2520scaled%2520error%2520%2528MASE%2529%2520units%253B%250A%25282%2529%2520superior%2520short-term%2520forecasting%2520%25281-24h%2529%2520across%2520comprehensive%2520metrics%253B%2520%25283%2529%250Asustained%2520predictive%2520leadership%2520in%2520extended-range%2520forecasts%2520%25281-120h%2529%253B%2520and%2520%25284%2529%250Ademonstrated%2520zero-shot%2520capability%2520maintaining%2520median%2520performance%2520%2528rank%25204/12%2529%250Aagainst%2520specialized%2520operational%2520models.%2520This%2520LLM-enhanced%2520temporal%2520modeling%250Aparadigm%2520establishes%2520a%2520new%2520standard%2520in%2520wave%2520prediction%252C%2520offering%2520both%250Acomputationally%2520efficient%2520solutions%2520and%2520a%2520transferable%2520framework%2520for%2520complex%250Ageophysical%2520systems%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Significant%20Wave%20Height%20Prediction%20Using%20Chronos%20Models&entry.906535625=Yilin%20Zhai%20and%20Hongyuan%20Shi%20and%20Chao%20Zhan%20and%20Qing%20Wang%20and%20Zaijin%20You%20and%20Nan%20Wang&entry.1292438233=%20%20Accurate%20wave%20height%20prediction%20is%20critical%20for%20maritime%20safety%20and%20coastal%0Aresilience%2C%20yet%20conventional%20physics-based%20models%20and%20traditional%20machine%0Alearning%20methods%20face%20challenges%20in%20computational%20efficiency%20and%20nonlinear%0Adynamics%20modeling.%20This%20study%20introduces%20Chronos%2C%20the%20first%20implementation%20of%20a%0Alarge%20language%20model%20%28LLM%29-powered%20temporal%20architecture%20%28Chronos%29%20optimized%0Afor%20wave%20forecasting.%20Through%20advanced%20temporal%20pattern%20recognition%20applied%20to%0Ahistorical%20wave%20data%20from%20three%20strategically%20chosen%20marine%20zones%20in%20the%0ANorthwest%20Pacific%20basin%2C%20our%20framework%20achieves%20multimodal%20improvements%3A%20%281%29%0A14.3%25%20reduction%20in%20training%20time%20with%202.5x%20faster%20inference%20speed%20compared%20to%0APatchTST%20baselines%2C%20achieving%200.575%20mean%20absolute%20scaled%20error%20%28MASE%29%20units%3B%0A%282%29%20superior%20short-term%20forecasting%20%281-24h%29%20across%20comprehensive%20metrics%3B%20%283%29%0Asustained%20predictive%20leadership%20in%20extended-range%20forecasts%20%281-120h%29%3B%20and%20%284%29%0Ademonstrated%20zero-shot%20capability%20maintaining%20median%20performance%20%28rank%204/12%29%0Aagainst%20specialized%20operational%20models.%20This%20LLM-enhanced%20temporal%20modeling%0Aparadigm%20establishes%20a%20new%20standard%20in%20wave%20prediction%2C%20offering%20both%0Acomputationally%20efficient%20solutions%20and%20a%20transferable%20framework%20for%20complex%0Ageophysical%20systems%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16834v1&entry.124074799=Read"},
{"title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs", "author": "Yingjian Chen and Feiyang Li and Xingyu Song and Tianxiao Li and Zixin Xu and Xiujie Chen and Issey Sukeda and Irene Li", "abstract": "  Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.\n", "link": "http://arxiv.org/abs/2504.10982v4", "date": "2025-04-23", "relevancy": 1.3329, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4814}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Role%20of%20Knowledge%20Graph-Based%20RAG%20in%20Japanese%20Medical%0A%20%20Question%20Answering%20with%20Small-Scale%20LLMs&body=Title%3A%20Exploring%20the%20Role%20of%20Knowledge%20Graph-Based%20RAG%20in%20Japanese%20Medical%0A%20%20Question%20Answering%20with%20Small-Scale%20LLMs%0AAuthor%3A%20Yingjian%20Chen%20and%20Feiyang%20Li%20and%20Xingyu%20Song%20and%20Tianxiao%20Li%20and%20Zixin%20Xu%20and%20Xiujie%20Chen%20and%20Issey%20Sukeda%20and%20Irene%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20perform%20well%20in%20medical%20QA%2C%20but%20their%0Aeffectiveness%20in%20Japanese%20contexts%20is%20limited%20due%20to%20privacy%20constraints%20that%0Aprevent%20the%20use%20of%20commercial%20models%20like%20GPT-4%20in%20clinical%20settings.%20As%20a%0Aresult%2C%20recent%20efforts%20focus%20on%20instruction-tuning%20open-source%20LLMs%2C%20though%20the%0Apotential%20of%20combining%20them%20with%20retrieval-augmented%20generation%20%28RAG%29%20remains%0Aunderexplored.%20To%20bridge%20this%20gap%2C%20we%20are%20the%20first%20to%20explore%20a%20knowledge%0Agraph-based%20%28KG%29%20RAG%20framework%20for%20Japanese%20medical%20QA%20small-scale%20open-source%0ALLMs.%20Experimental%20results%20show%20that%20KG-based%20RAG%20has%20only%20a%20limited%20impact%20on%0AJapanese%20medical%20QA%20using%20small-scale%20open-source%20LLMs.%20Further%20case%20studies%0Areveal%20that%20the%20effectiveness%20of%20the%20RAG%20is%20sensitive%20to%20the%20quality%20and%0Arelevance%20of%20the%20external%20retrieved%20content.%20These%20findings%20offer%20valuable%0Ainsights%20into%20the%20challenges%20and%20potential%20of%20applying%20RAG%20in%20Japanese%20medical%0AQA%2C%20while%20also%20serving%20as%20a%20reference%20for%20other%20low-resource%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10982v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Role%2520of%2520Knowledge%2520Graph-Based%2520RAG%2520in%2520Japanese%2520Medical%250A%2520%2520Question%2520Answering%2520with%2520Small-Scale%2520LLMs%26entry.906535625%3DYingjian%2520Chen%2520and%2520Feiyang%2520Li%2520and%2520Xingyu%2520Song%2520and%2520Tianxiao%2520Li%2520and%2520Zixin%2520Xu%2520and%2520Xiujie%2520Chen%2520and%2520Issey%2520Sukeda%2520and%2520Irene%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520perform%2520well%2520in%2520medical%2520QA%252C%2520but%2520their%250Aeffectiveness%2520in%2520Japanese%2520contexts%2520is%2520limited%2520due%2520to%2520privacy%2520constraints%2520that%250Aprevent%2520the%2520use%2520of%2520commercial%2520models%2520like%2520GPT-4%2520in%2520clinical%2520settings.%2520As%2520a%250Aresult%252C%2520recent%2520efforts%2520focus%2520on%2520instruction-tuning%2520open-source%2520LLMs%252C%2520though%2520the%250Apotential%2520of%2520combining%2520them%2520with%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520remains%250Aunderexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520are%2520the%2520first%2520to%2520explore%2520a%2520knowledge%250Agraph-based%2520%2528KG%2529%2520RAG%2520framework%2520for%2520Japanese%2520medical%2520QA%2520small-scale%2520open-source%250ALLMs.%2520Experimental%2520results%2520show%2520that%2520KG-based%2520RAG%2520has%2520only%2520a%2520limited%2520impact%2520on%250AJapanese%2520medical%2520QA%2520using%2520small-scale%2520open-source%2520LLMs.%2520Further%2520case%2520studies%250Areveal%2520that%2520the%2520effectiveness%2520of%2520the%2520RAG%2520is%2520sensitive%2520to%2520the%2520quality%2520and%250Arelevance%2520of%2520the%2520external%2520retrieved%2520content.%2520These%2520findings%2520offer%2520valuable%250Ainsights%2520into%2520the%2520challenges%2520and%2520potential%2520of%2520applying%2520RAG%2520in%2520Japanese%2520medical%250AQA%252C%2520while%2520also%2520serving%2520as%2520a%2520reference%2520for%2520other%2520low-resource%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10982v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Role%20of%20Knowledge%20Graph-Based%20RAG%20in%20Japanese%20Medical%0A%20%20Question%20Answering%20with%20Small-Scale%20LLMs&entry.906535625=Yingjian%20Chen%20and%20Feiyang%20Li%20and%20Xingyu%20Song%20and%20Tianxiao%20Li%20and%20Zixin%20Xu%20and%20Xiujie%20Chen%20and%20Issey%20Sukeda%20and%20Irene%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20perform%20well%20in%20medical%20QA%2C%20but%20their%0Aeffectiveness%20in%20Japanese%20contexts%20is%20limited%20due%20to%20privacy%20constraints%20that%0Aprevent%20the%20use%20of%20commercial%20models%20like%20GPT-4%20in%20clinical%20settings.%20As%20a%0Aresult%2C%20recent%20efforts%20focus%20on%20instruction-tuning%20open-source%20LLMs%2C%20though%20the%0Apotential%20of%20combining%20them%20with%20retrieval-augmented%20generation%20%28RAG%29%20remains%0Aunderexplored.%20To%20bridge%20this%20gap%2C%20we%20are%20the%20first%20to%20explore%20a%20knowledge%0Agraph-based%20%28KG%29%20RAG%20framework%20for%20Japanese%20medical%20QA%20small-scale%20open-source%0ALLMs.%20Experimental%20results%20show%20that%20KG-based%20RAG%20has%20only%20a%20limited%20impact%20on%0AJapanese%20medical%20QA%20using%20small-scale%20open-source%20LLMs.%20Further%20case%20studies%0Areveal%20that%20the%20effectiveness%20of%20the%20RAG%20is%20sensitive%20to%20the%20quality%20and%0Arelevance%20of%20the%20external%20retrieved%20content.%20These%20findings%20offer%20valuable%0Ainsights%20into%20the%20challenges%20and%20potential%20of%20applying%20RAG%20in%20Japanese%20medical%0AQA%2C%20while%20also%20serving%20as%20a%20reference%20for%20other%20low-resource%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10982v4&entry.124074799=Read"},
{"title": "Zero-shot Sim-to-Real Transfer for Reinforcement Learning-based Visual\n  Servoing of Soft Continuum Arms", "author": "Hsin-Jung Yang and Mahsa Khosravi and Benjamin Walt and Girish Krishnan and Soumik Sarkar", "abstract": "  Soft continuum arms (SCAs) soft and deformable nature presents challenges in\nmodeling and control due to their infinite degrees of freedom and non-linear\nbehavior. This work introduces a reinforcement learning (RL)-based framework\nfor visual servoing tasks on SCAs with zero-shot sim-to-real transfer\ncapabilities, demonstrated on a single section pneumatic manipulator capable of\nbending and twisting. The framework decouples kinematics from mechanical\nproperties using an RL kinematic controller for motion planning and a local\ncontroller for actuation refinement, leveraging minimal sensing with visual\nfeedback. Trained entirely in simulation, the RL controller achieved a 99.8%\nsuccess rate. When deployed on hardware, it achieved a 67% success rate in\nzero-shot sim-to-real transfer, demonstrating robustness and adaptability. This\napproach offers a scalable solution for SCAs in 3D visual servoing, with\npotential for further refinement and expanded applications.\n", "link": "http://arxiv.org/abs/2504.16916v1", "date": "2025-04-23", "relevancy": 1.057, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5318}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Sim-to-Real%20Transfer%20for%20Reinforcement%20Learning-based%20Visual%0A%20%20Servoing%20of%20Soft%20Continuum%20Arms&body=Title%3A%20Zero-shot%20Sim-to-Real%20Transfer%20for%20Reinforcement%20Learning-based%20Visual%0A%20%20Servoing%20of%20Soft%20Continuum%20Arms%0AAuthor%3A%20Hsin-Jung%20Yang%20and%20Mahsa%20Khosravi%20and%20Benjamin%20Walt%20and%20Girish%20Krishnan%20and%20Soumik%20Sarkar%0AAbstract%3A%20%20%20Soft%20continuum%20arms%20%28SCAs%29%20soft%20and%20deformable%20nature%20presents%20challenges%20in%0Amodeling%20and%20control%20due%20to%20their%20infinite%20degrees%20of%20freedom%20and%20non-linear%0Abehavior.%20This%20work%20introduces%20a%20reinforcement%20learning%20%28RL%29-based%20framework%0Afor%20visual%20servoing%20tasks%20on%20SCAs%20with%20zero-shot%20sim-to-real%20transfer%0Acapabilities%2C%20demonstrated%20on%20a%20single%20section%20pneumatic%20manipulator%20capable%20of%0Abending%20and%20twisting.%20The%20framework%20decouples%20kinematics%20from%20mechanical%0Aproperties%20using%20an%20RL%20kinematic%20controller%20for%20motion%20planning%20and%20a%20local%0Acontroller%20for%20actuation%20refinement%2C%20leveraging%20minimal%20sensing%20with%20visual%0Afeedback.%20Trained%20entirely%20in%20simulation%2C%20the%20RL%20controller%20achieved%20a%2099.8%25%0Asuccess%20rate.%20When%20deployed%20on%20hardware%2C%20it%20achieved%20a%2067%25%20success%20rate%20in%0Azero-shot%20sim-to-real%20transfer%2C%20demonstrating%20robustness%20and%20adaptability.%20This%0Aapproach%20offers%20a%20scalable%20solution%20for%20SCAs%20in%203D%20visual%20servoing%2C%20with%0Apotential%20for%20further%20refinement%20and%20expanded%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Sim-to-Real%2520Transfer%2520for%2520Reinforcement%2520Learning-based%2520Visual%250A%2520%2520Servoing%2520of%2520Soft%2520Continuum%2520Arms%26entry.906535625%3DHsin-Jung%2520Yang%2520and%2520Mahsa%2520Khosravi%2520and%2520Benjamin%2520Walt%2520and%2520Girish%2520Krishnan%2520and%2520Soumik%2520Sarkar%26entry.1292438233%3D%2520%2520Soft%2520continuum%2520arms%2520%2528SCAs%2529%2520soft%2520and%2520deformable%2520nature%2520presents%2520challenges%2520in%250Amodeling%2520and%2520control%2520due%2520to%2520their%2520infinite%2520degrees%2520of%2520freedom%2520and%2520non-linear%250Abehavior.%2520This%2520work%2520introduces%2520a%2520reinforcement%2520learning%2520%2528RL%2529-based%2520framework%250Afor%2520visual%2520servoing%2520tasks%2520on%2520SCAs%2520with%2520zero-shot%2520sim-to-real%2520transfer%250Acapabilities%252C%2520demonstrated%2520on%2520a%2520single%2520section%2520pneumatic%2520manipulator%2520capable%2520of%250Abending%2520and%2520twisting.%2520The%2520framework%2520decouples%2520kinematics%2520from%2520mechanical%250Aproperties%2520using%2520an%2520RL%2520kinematic%2520controller%2520for%2520motion%2520planning%2520and%2520a%2520local%250Acontroller%2520for%2520actuation%2520refinement%252C%2520leveraging%2520minimal%2520sensing%2520with%2520visual%250Afeedback.%2520Trained%2520entirely%2520in%2520simulation%252C%2520the%2520RL%2520controller%2520achieved%2520a%252099.8%2525%250Asuccess%2520rate.%2520When%2520deployed%2520on%2520hardware%252C%2520it%2520achieved%2520a%252067%2525%2520success%2520rate%2520in%250Azero-shot%2520sim-to-real%2520transfer%252C%2520demonstrating%2520robustness%2520and%2520adaptability.%2520This%250Aapproach%2520offers%2520a%2520scalable%2520solution%2520for%2520SCAs%2520in%25203D%2520visual%2520servoing%252C%2520with%250Apotential%2520for%2520further%2520refinement%2520and%2520expanded%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Sim-to-Real%20Transfer%20for%20Reinforcement%20Learning-based%20Visual%0A%20%20Servoing%20of%20Soft%20Continuum%20Arms&entry.906535625=Hsin-Jung%20Yang%20and%20Mahsa%20Khosravi%20and%20Benjamin%20Walt%20and%20Girish%20Krishnan%20and%20Soumik%20Sarkar&entry.1292438233=%20%20Soft%20continuum%20arms%20%28SCAs%29%20soft%20and%20deformable%20nature%20presents%20challenges%20in%0Amodeling%20and%20control%20due%20to%20their%20infinite%20degrees%20of%20freedom%20and%20non-linear%0Abehavior.%20This%20work%20introduces%20a%20reinforcement%20learning%20%28RL%29-based%20framework%0Afor%20visual%20servoing%20tasks%20on%20SCAs%20with%20zero-shot%20sim-to-real%20transfer%0Acapabilities%2C%20demonstrated%20on%20a%20single%20section%20pneumatic%20manipulator%20capable%20of%0Abending%20and%20twisting.%20The%20framework%20decouples%20kinematics%20from%20mechanical%0Aproperties%20using%20an%20RL%20kinematic%20controller%20for%20motion%20planning%20and%20a%20local%0Acontroller%20for%20actuation%20refinement%2C%20leveraging%20minimal%20sensing%20with%20visual%0Afeedback.%20Trained%20entirely%20in%20simulation%2C%20the%20RL%20controller%20achieved%20a%2099.8%25%0Asuccess%20rate.%20When%20deployed%20on%20hardware%2C%20it%20achieved%20a%2067%25%20success%20rate%20in%0Azero-shot%20sim-to-real%20transfer%2C%20demonstrating%20robustness%20and%20adaptability.%20This%0Aapproach%20offers%20a%20scalable%20solution%20for%20SCAs%20in%203D%20visual%20servoing%2C%20with%0Apotential%20for%20further%20refinement%20and%20expanded%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16916v1&entry.124074799=Read"},
{"title": "Credible plan-driven RAG method for Multi-hop Question Answering", "author": "Ningning Zhang and Chi Zhang and Zhizhong Tan and Xingxing Yang and Weiping Deng and Wenyong Wang", "abstract": "  Multi-hop question answering (QA) presents a considerable challenge for\nRetrieval-Augmented Generation (RAG), requiring the structured decomposition of\ncomplex queries into logical reasoning paths and the generation of dependable\nintermediate results. However, deviations in reasoning paths or errors in\nintermediate results, which are common in current RAG methods, may propagate\nand accumulate throughout the reasoning process, diminishing the accuracy of\nthe answer to complex queries. To address this challenge, we propose the\nPlan-then-Act-and-Review (PAR RAG) framework, which is organized into three key\nstages: planning, act, and review, and aims to offer an interpretable and\nincremental reasoning paradigm for accurate and reliable multi-hop question\nanswering by mitigating error propagation.PAR RAG initially applies a top-down\nproblem decomposition strategy, formulating a comprehensive plan that\nintegrates multiple executable steps from a holistic viewpoint. This approach\navoids the pitfalls of local optima common in traditional RAG methods, ensuring\nthe accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a\nplan execution mechanism based on multi-granularity verification. By utilizing\nboth coarse-grained similarity information and fine-grained relevant data, the\nframework thoroughly checks and adjusts intermediate results, ensuring process\naccuracy while effectively managing error propagation and amplification.\nExperimental results on multi-hop QA datasets demonstrate that the PAR RAG\nframework substantially outperforms existing state-of-the-art methods in key\nmetrics, including EM and F1 scores.\n", "link": "http://arxiv.org/abs/2504.16787v1", "date": "2025-04-23", "relevancy": 1.3724, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4768}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4718}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Credible%20plan-driven%20RAG%20method%20for%20Multi-hop%20Question%20Answering&body=Title%3A%20Credible%20plan-driven%20RAG%20method%20for%20Multi-hop%20Question%20Answering%0AAuthor%3A%20Ningning%20Zhang%20and%20Chi%20Zhang%20and%20Zhizhong%20Tan%20and%20Xingxing%20Yang%20and%20Weiping%20Deng%20and%20Wenyong%20Wang%0AAbstract%3A%20%20%20Multi-hop%20question%20answering%20%28QA%29%20presents%20a%20considerable%20challenge%20for%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20requiring%20the%20structured%20decomposition%20of%0Acomplex%20queries%20into%20logical%20reasoning%20paths%20and%20the%20generation%20of%20dependable%0Aintermediate%20results.%20However%2C%20deviations%20in%20reasoning%20paths%20or%20errors%20in%0Aintermediate%20results%2C%20which%20are%20common%20in%20current%20RAG%20methods%2C%20may%20propagate%0Aand%20accumulate%20throughout%20the%20reasoning%20process%2C%20diminishing%20the%20accuracy%20of%0Athe%20answer%20to%20complex%20queries.%20To%20address%20this%20challenge%2C%20we%20propose%20the%0APlan-then-Act-and-Review%20%28PAR%20RAG%29%20framework%2C%20which%20is%20organized%20into%20three%20key%0Astages%3A%20planning%2C%20act%2C%20and%20review%2C%20and%20aims%20to%20offer%20an%20interpretable%20and%0Aincremental%20reasoning%20paradigm%20for%20accurate%20and%20reliable%20multi-hop%20question%0Aanswering%20by%20mitigating%20error%20propagation.PAR%20RAG%20initially%20applies%20a%20top-down%0Aproblem%20decomposition%20strategy%2C%20formulating%20a%20comprehensive%20plan%20that%0Aintegrates%20multiple%20executable%20steps%20from%20a%20holistic%20viewpoint.%20This%20approach%0Aavoids%20the%20pitfalls%20of%20local%20optima%20common%20in%20traditional%20RAG%20methods%2C%20ensuring%0Athe%20accuracy%20of%20the%20entire%20reasoning%20path.%20Subsequently%2C%20PAR%20RAG%20incorporates%20a%0Aplan%20execution%20mechanism%20based%20on%20multi-granularity%20verification.%20By%20utilizing%0Aboth%20coarse-grained%20similarity%20information%20and%20fine-grained%20relevant%20data%2C%20the%0Aframework%20thoroughly%20checks%20and%20adjusts%20intermediate%20results%2C%20ensuring%20process%0Aaccuracy%20while%20effectively%20managing%20error%20propagation%20and%20amplification.%0AExperimental%20results%20on%20multi-hop%20QA%20datasets%20demonstrate%20that%20the%20PAR%20RAG%0Aframework%20substantially%20outperforms%20existing%20state-of-the-art%20methods%20in%20key%0Ametrics%2C%20including%20EM%20and%20F1%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCredible%2520plan-driven%2520RAG%2520method%2520for%2520Multi-hop%2520Question%2520Answering%26entry.906535625%3DNingning%2520Zhang%2520and%2520Chi%2520Zhang%2520and%2520Zhizhong%2520Tan%2520and%2520Xingxing%2520Yang%2520and%2520Weiping%2520Deng%2520and%2520Wenyong%2520Wang%26entry.1292438233%3D%2520%2520Multi-hop%2520question%2520answering%2520%2528QA%2529%2520presents%2520a%2520considerable%2520challenge%2520for%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%252C%2520requiring%2520the%2520structured%2520decomposition%2520of%250Acomplex%2520queries%2520into%2520logical%2520reasoning%2520paths%2520and%2520the%2520generation%2520of%2520dependable%250Aintermediate%2520results.%2520However%252C%2520deviations%2520in%2520reasoning%2520paths%2520or%2520errors%2520in%250Aintermediate%2520results%252C%2520which%2520are%2520common%2520in%2520current%2520RAG%2520methods%252C%2520may%2520propagate%250Aand%2520accumulate%2520throughout%2520the%2520reasoning%2520process%252C%2520diminishing%2520the%2520accuracy%2520of%250Athe%2520answer%2520to%2520complex%2520queries.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520the%250APlan-then-Act-and-Review%2520%2528PAR%2520RAG%2529%2520framework%252C%2520which%2520is%2520organized%2520into%2520three%2520key%250Astages%253A%2520planning%252C%2520act%252C%2520and%2520review%252C%2520and%2520aims%2520to%2520offer%2520an%2520interpretable%2520and%250Aincremental%2520reasoning%2520paradigm%2520for%2520accurate%2520and%2520reliable%2520multi-hop%2520question%250Aanswering%2520by%2520mitigating%2520error%2520propagation.PAR%2520RAG%2520initially%2520applies%2520a%2520top-down%250Aproblem%2520decomposition%2520strategy%252C%2520formulating%2520a%2520comprehensive%2520plan%2520that%250Aintegrates%2520multiple%2520executable%2520steps%2520from%2520a%2520holistic%2520viewpoint.%2520This%2520approach%250Aavoids%2520the%2520pitfalls%2520of%2520local%2520optima%2520common%2520in%2520traditional%2520RAG%2520methods%252C%2520ensuring%250Athe%2520accuracy%2520of%2520the%2520entire%2520reasoning%2520path.%2520Subsequently%252C%2520PAR%2520RAG%2520incorporates%2520a%250Aplan%2520execution%2520mechanism%2520based%2520on%2520multi-granularity%2520verification.%2520By%2520utilizing%250Aboth%2520coarse-grained%2520similarity%2520information%2520and%2520fine-grained%2520relevant%2520data%252C%2520the%250Aframework%2520thoroughly%2520checks%2520and%2520adjusts%2520intermediate%2520results%252C%2520ensuring%2520process%250Aaccuracy%2520while%2520effectively%2520managing%2520error%2520propagation%2520and%2520amplification.%250AExperimental%2520results%2520on%2520multi-hop%2520QA%2520datasets%2520demonstrate%2520that%2520the%2520PAR%2520RAG%250Aframework%2520substantially%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520key%250Ametrics%252C%2520including%2520EM%2520and%2520F1%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Credible%20plan-driven%20RAG%20method%20for%20Multi-hop%20Question%20Answering&entry.906535625=Ningning%20Zhang%20and%20Chi%20Zhang%20and%20Zhizhong%20Tan%20and%20Xingxing%20Yang%20and%20Weiping%20Deng%20and%20Wenyong%20Wang&entry.1292438233=%20%20Multi-hop%20question%20answering%20%28QA%29%20presents%20a%20considerable%20challenge%20for%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20requiring%20the%20structured%20decomposition%20of%0Acomplex%20queries%20into%20logical%20reasoning%20paths%20and%20the%20generation%20of%20dependable%0Aintermediate%20results.%20However%2C%20deviations%20in%20reasoning%20paths%20or%20errors%20in%0Aintermediate%20results%2C%20which%20are%20common%20in%20current%20RAG%20methods%2C%20may%20propagate%0Aand%20accumulate%20throughout%20the%20reasoning%20process%2C%20diminishing%20the%20accuracy%20of%0Athe%20answer%20to%20complex%20queries.%20To%20address%20this%20challenge%2C%20we%20propose%20the%0APlan-then-Act-and-Review%20%28PAR%20RAG%29%20framework%2C%20which%20is%20organized%20into%20three%20key%0Astages%3A%20planning%2C%20act%2C%20and%20review%2C%20and%20aims%20to%20offer%20an%20interpretable%20and%0Aincremental%20reasoning%20paradigm%20for%20accurate%20and%20reliable%20multi-hop%20question%0Aanswering%20by%20mitigating%20error%20propagation.PAR%20RAG%20initially%20applies%20a%20top-down%0Aproblem%20decomposition%20strategy%2C%20formulating%20a%20comprehensive%20plan%20that%0Aintegrates%20multiple%20executable%20steps%20from%20a%20holistic%20viewpoint.%20This%20approach%0Aavoids%20the%20pitfalls%20of%20local%20optima%20common%20in%20traditional%20RAG%20methods%2C%20ensuring%0Athe%20accuracy%20of%20the%20entire%20reasoning%20path.%20Subsequently%2C%20PAR%20RAG%20incorporates%20a%0Aplan%20execution%20mechanism%20based%20on%20multi-granularity%20verification.%20By%20utilizing%0Aboth%20coarse-grained%20similarity%20information%20and%20fine-grained%20relevant%20data%2C%20the%0Aframework%20thoroughly%20checks%20and%20adjusts%20intermediate%20results%2C%20ensuring%20process%0Aaccuracy%20while%20effectively%20managing%20error%20propagation%20and%20amplification.%0AExperimental%20results%20on%20multi-hop%20QA%20datasets%20demonstrate%20that%20the%20PAR%20RAG%0Aframework%20substantially%20outperforms%20existing%20state-of-the-art%20methods%20in%20key%0Ametrics%2C%20including%20EM%20and%20F1%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16787v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


