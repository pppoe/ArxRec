<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241217.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds", "author": "Zhiwen Fan and Kairun Wen and Wenyan Cong and Kevin Wang and Jian Zhang and Xinghao Ding and Danfei Xu and Boris Ivanovic and Marco Pavone and Georgios Pavlakos and Zhangyang Wang and Yue Wang", "abstract": "  While neural 3D reconstruction has advanced substantially, it typically\nrequires densely captured multi-view data with carefully initialized poses\n(e.g., using COLMAP). However, this requirement limits its broader\napplicability, as Structure-from-Motion (SfM) is often unreliable in\nsparse-view scenarios where feature matches are limited, resulting in\ncumulative errors. In this paper, we introduce InstantSplat, a novel and\nlightning-fast neural reconstruction system that builds accurate 3D\nrepresentations from as few as 2-3 images. InstantSplat adopts a\nself-supervised framework that bridges the gap between 2D images and 3D\nrepresentations using Gaussian Bundle Adjustment (GauBA) and can be optimized\nin an end-to-end manner. InstantSplat integrates dense stereo priors and\nco-visibility relationships between frames to initialize pixel-aligned geometry\nby progressively expanding the scene avoiding redundancy. Gaussian Bundle\nAdjustment is used to adapt both the scene representation and camera parameters\nquickly by minimizing gradient-based photometric error. Overall, InstantSplat\nachieves large-scale 3D reconstruction in mere seconds by reducing the required\nnumber of input views. It achieves an acceleration of over 20 times in\nreconstruction, improves visual quality (SSIM) from 0.3755 to 0.7624 than\nCOLMAP with 3D-GS, and is compatible with multiple 3D representations (3D-GS,\n2D-GS, and Mip-Splatting).\n", "link": "http://arxiv.org/abs/2403.20309v4", "date": "2024-12-17", "relevancy": 3.5679, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7696}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.725}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstantSplat%3A%20Sparse-view%20SfM-free%20Gaussian%20Splatting%20in%20Seconds&body=Title%3A%20InstantSplat%3A%20Sparse-view%20SfM-free%20Gaussian%20Splatting%20in%20Seconds%0AAuthor%3A%20Zhiwen%20Fan%20and%20Kairun%20Wen%20and%20Wenyan%20Cong%20and%20Kevin%20Wang%20and%20Jian%20Zhang%20and%20Xinghao%20Ding%20and%20Danfei%20Xu%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Georgios%20Pavlakos%20and%20Zhangyang%20Wang%20and%20Yue%20Wang%0AAbstract%3A%20%20%20While%20neural%203D%20reconstruction%20has%20advanced%20substantially%2C%20it%20typically%0Arequires%20densely%20captured%20multi-view%20data%20with%20carefully%20initialized%20poses%0A%28e.g.%2C%20using%20COLMAP%29.%20However%2C%20this%20requirement%20limits%20its%20broader%0Aapplicability%2C%20as%20Structure-from-Motion%20%28SfM%29%20is%20often%20unreliable%20in%0Asparse-view%20scenarios%20where%20feature%20matches%20are%20limited%2C%20resulting%20in%0Acumulative%20errors.%20In%20this%20paper%2C%20we%20introduce%20InstantSplat%2C%20a%20novel%20and%0Alightning-fast%20neural%20reconstruction%20system%20that%20builds%20accurate%203D%0Arepresentations%20from%20as%20few%20as%202-3%20images.%20InstantSplat%20adopts%20a%0Aself-supervised%20framework%20that%20bridges%20the%20gap%20between%202D%20images%20and%203D%0Arepresentations%20using%20Gaussian%20Bundle%20Adjustment%20%28GauBA%29%20and%20can%20be%20optimized%0Ain%20an%20end-to-end%20manner.%20InstantSplat%20integrates%20dense%20stereo%20priors%20and%0Aco-visibility%20relationships%20between%20frames%20to%20initialize%20pixel-aligned%20geometry%0Aby%20progressively%20expanding%20the%20scene%20avoiding%20redundancy.%20Gaussian%20Bundle%0AAdjustment%20is%20used%20to%20adapt%20both%20the%20scene%20representation%20and%20camera%20parameters%0Aquickly%20by%20minimizing%20gradient-based%20photometric%20error.%20Overall%2C%20InstantSplat%0Aachieves%20large-scale%203D%20reconstruction%20in%20mere%20seconds%20by%20reducing%20the%20required%0Anumber%20of%20input%20views.%20It%20achieves%20an%20acceleration%20of%20over%2020%20times%20in%0Areconstruction%2C%20improves%20visual%20quality%20%28SSIM%29%20from%200.3755%20to%200.7624%20than%0ACOLMAP%20with%203D-GS%2C%20and%20is%20compatible%20with%20multiple%203D%20representations%20%283D-GS%2C%0A2D-GS%2C%20and%20Mip-Splatting%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20309v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstantSplat%253A%2520Sparse-view%2520SfM-free%2520Gaussian%2520Splatting%2520in%2520Seconds%26entry.906535625%3DZhiwen%2520Fan%2520and%2520Kairun%2520Wen%2520and%2520Wenyan%2520Cong%2520and%2520Kevin%2520Wang%2520and%2520Jian%2520Zhang%2520and%2520Xinghao%2520Ding%2520and%2520Danfei%2520Xu%2520and%2520Boris%2520Ivanovic%2520and%2520Marco%2520Pavone%2520and%2520Georgios%2520Pavlakos%2520and%2520Zhangyang%2520Wang%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520While%2520neural%25203D%2520reconstruction%2520has%2520advanced%2520substantially%252C%2520it%2520typically%250Arequires%2520densely%2520captured%2520multi-view%2520data%2520with%2520carefully%2520initialized%2520poses%250A%2528e.g.%252C%2520using%2520COLMAP%2529.%2520However%252C%2520this%2520requirement%2520limits%2520its%2520broader%250Aapplicability%252C%2520as%2520Structure-from-Motion%2520%2528SfM%2529%2520is%2520often%2520unreliable%2520in%250Asparse-view%2520scenarios%2520where%2520feature%2520matches%2520are%2520limited%252C%2520resulting%2520in%250Acumulative%2520errors.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520InstantSplat%252C%2520a%2520novel%2520and%250Alightning-fast%2520neural%2520reconstruction%2520system%2520that%2520builds%2520accurate%25203D%250Arepresentations%2520from%2520as%2520few%2520as%25202-3%2520images.%2520InstantSplat%2520adopts%2520a%250Aself-supervised%2520framework%2520that%2520bridges%2520the%2520gap%2520between%25202D%2520images%2520and%25203D%250Arepresentations%2520using%2520Gaussian%2520Bundle%2520Adjustment%2520%2528GauBA%2529%2520and%2520can%2520be%2520optimized%250Ain%2520an%2520end-to-end%2520manner.%2520InstantSplat%2520integrates%2520dense%2520stereo%2520priors%2520and%250Aco-visibility%2520relationships%2520between%2520frames%2520to%2520initialize%2520pixel-aligned%2520geometry%250Aby%2520progressively%2520expanding%2520the%2520scene%2520avoiding%2520redundancy.%2520Gaussian%2520Bundle%250AAdjustment%2520is%2520used%2520to%2520adapt%2520both%2520the%2520scene%2520representation%2520and%2520camera%2520parameters%250Aquickly%2520by%2520minimizing%2520gradient-based%2520photometric%2520error.%2520Overall%252C%2520InstantSplat%250Aachieves%2520large-scale%25203D%2520reconstruction%2520in%2520mere%2520seconds%2520by%2520reducing%2520the%2520required%250Anumber%2520of%2520input%2520views.%2520It%2520achieves%2520an%2520acceleration%2520of%2520over%252020%2520times%2520in%250Areconstruction%252C%2520improves%2520visual%2520quality%2520%2528SSIM%2529%2520from%25200.3755%2520to%25200.7624%2520than%250ACOLMAP%2520with%25203D-GS%252C%2520and%2520is%2520compatible%2520with%2520multiple%25203D%2520representations%2520%25283D-GS%252C%250A2D-GS%252C%2520and%2520Mip-Splatting%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20309v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstantSplat%3A%20Sparse-view%20SfM-free%20Gaussian%20Splatting%20in%20Seconds&entry.906535625=Zhiwen%20Fan%20and%20Kairun%20Wen%20and%20Wenyan%20Cong%20and%20Kevin%20Wang%20and%20Jian%20Zhang%20and%20Xinghao%20Ding%20and%20Danfei%20Xu%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Georgios%20Pavlakos%20and%20Zhangyang%20Wang%20and%20Yue%20Wang&entry.1292438233=%20%20While%20neural%203D%20reconstruction%20has%20advanced%20substantially%2C%20it%20typically%0Arequires%20densely%20captured%20multi-view%20data%20with%20carefully%20initialized%20poses%0A%28e.g.%2C%20using%20COLMAP%29.%20However%2C%20this%20requirement%20limits%20its%20broader%0Aapplicability%2C%20as%20Structure-from-Motion%20%28SfM%29%20is%20often%20unreliable%20in%0Asparse-view%20scenarios%20where%20feature%20matches%20are%20limited%2C%20resulting%20in%0Acumulative%20errors.%20In%20this%20paper%2C%20we%20introduce%20InstantSplat%2C%20a%20novel%20and%0Alightning-fast%20neural%20reconstruction%20system%20that%20builds%20accurate%203D%0Arepresentations%20from%20as%20few%20as%202-3%20images.%20InstantSplat%20adopts%20a%0Aself-supervised%20framework%20that%20bridges%20the%20gap%20between%202D%20images%20and%203D%0Arepresentations%20using%20Gaussian%20Bundle%20Adjustment%20%28GauBA%29%20and%20can%20be%20optimized%0Ain%20an%20end-to-end%20manner.%20InstantSplat%20integrates%20dense%20stereo%20priors%20and%0Aco-visibility%20relationships%20between%20frames%20to%20initialize%20pixel-aligned%20geometry%0Aby%20progressively%20expanding%20the%20scene%20avoiding%20redundancy.%20Gaussian%20Bundle%0AAdjustment%20is%20used%20to%20adapt%20both%20the%20scene%20representation%20and%20camera%20parameters%0Aquickly%20by%20minimizing%20gradient-based%20photometric%20error.%20Overall%2C%20InstantSplat%0Aachieves%20large-scale%203D%20reconstruction%20in%20mere%20seconds%20by%20reducing%20the%20required%0Anumber%20of%20input%20views.%20It%20achieves%20an%20acceleration%20of%20over%2020%20times%20in%0Areconstruction%2C%20improves%20visual%20quality%20%28SSIM%29%20from%200.3755%20to%200.7624%20than%0ACOLMAP%20with%203D-GS%2C%20and%20is%20compatible%20with%20multiple%203D%20representations%20%283D-GS%2C%0A2D-GS%2C%20and%20Mip-Splatting%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20309v4&entry.124074799=Read"},
{"title": "EOGS: Gaussian Splatting for Earth Observation", "author": "Luca Savant Aira and Gabriele Facciolo and Thibaud Ehret", "abstract": "  Recently, Gaussian splatting has emerged as a strong alternative to NeRF,\ndemonstrating impressive 3D modeling capabilities while requiring only a\nfraction of the training and rendering time. In this paper, we show how the\nstandard Gaussian splatting framework can be adapted for remote sensing,\nretaining its high efficiency. This enables us to achieve state-of-the-art\nperformance in just a few minutes, compared to the day-long optimization\nrequired by the best-performing NeRF-based Earth observation methods. The\nproposed framework incorporates remote-sensing improvements from EO-NeRF, such\nas radiometric correction and shadow modeling, while introducing novel\ncomponents, including sparsity, view consistency, and opacity regularizations.\n", "link": "http://arxiv.org/abs/2412.13047v1", "date": "2024-12-17", "relevancy": 3.2575, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6962}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6413}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EOGS%3A%20Gaussian%20Splatting%20for%20Earth%20Observation&body=Title%3A%20EOGS%3A%20Gaussian%20Splatting%20for%20Earth%20Observation%0AAuthor%3A%20Luca%20Savant%20Aira%20and%20Gabriele%20Facciolo%20and%20Thibaud%20Ehret%0AAbstract%3A%20%20%20Recently%2C%20Gaussian%20splatting%20has%20emerged%20as%20a%20strong%20alternative%20to%20NeRF%2C%0Ademonstrating%20impressive%203D%20modeling%20capabilities%20while%20requiring%20only%20a%0Afraction%20of%20the%20training%20and%20rendering%20time.%20In%20this%20paper%2C%20we%20show%20how%20the%0Astandard%20Gaussian%20splatting%20framework%20can%20be%20adapted%20for%20remote%20sensing%2C%0Aretaining%20its%20high%20efficiency.%20This%20enables%20us%20to%20achieve%20state-of-the-art%0Aperformance%20in%20just%20a%20few%20minutes%2C%20compared%20to%20the%20day-long%20optimization%0Arequired%20by%20the%20best-performing%20NeRF-based%20Earth%20observation%20methods.%20The%0Aproposed%20framework%20incorporates%20remote-sensing%20improvements%20from%20EO-NeRF%2C%20such%0Aas%20radiometric%20correction%20and%20shadow%20modeling%2C%20while%20introducing%20novel%0Acomponents%2C%20including%20sparsity%2C%20view%20consistency%2C%20and%20opacity%20regularizations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEOGS%253A%2520Gaussian%2520Splatting%2520for%2520Earth%2520Observation%26entry.906535625%3DLuca%2520Savant%2520Aira%2520and%2520Gabriele%2520Facciolo%2520and%2520Thibaud%2520Ehret%26entry.1292438233%3D%2520%2520Recently%252C%2520Gaussian%2520splatting%2520has%2520emerged%2520as%2520a%2520strong%2520alternative%2520to%2520NeRF%252C%250Ademonstrating%2520impressive%25203D%2520modeling%2520capabilities%2520while%2520requiring%2520only%2520a%250Afraction%2520of%2520the%2520training%2520and%2520rendering%2520time.%2520In%2520this%2520paper%252C%2520we%2520show%2520how%2520the%250Astandard%2520Gaussian%2520splatting%2520framework%2520can%2520be%2520adapted%2520for%2520remote%2520sensing%252C%250Aretaining%2520its%2520high%2520efficiency.%2520This%2520enables%2520us%2520to%2520achieve%2520state-of-the-art%250Aperformance%2520in%2520just%2520a%2520few%2520minutes%252C%2520compared%2520to%2520the%2520day-long%2520optimization%250Arequired%2520by%2520the%2520best-performing%2520NeRF-based%2520Earth%2520observation%2520methods.%2520The%250Aproposed%2520framework%2520incorporates%2520remote-sensing%2520improvements%2520from%2520EO-NeRF%252C%2520such%250Aas%2520radiometric%2520correction%2520and%2520shadow%2520modeling%252C%2520while%2520introducing%2520novel%250Acomponents%252C%2520including%2520sparsity%252C%2520view%2520consistency%252C%2520and%2520opacity%2520regularizations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EOGS%3A%20Gaussian%20Splatting%20for%20Earth%20Observation&entry.906535625=Luca%20Savant%20Aira%20and%20Gabriele%20Facciolo%20and%20Thibaud%20Ehret&entry.1292438233=%20%20Recently%2C%20Gaussian%20splatting%20has%20emerged%20as%20a%20strong%20alternative%20to%20NeRF%2C%0Ademonstrating%20impressive%203D%20modeling%20capabilities%20while%20requiring%20only%20a%0Afraction%20of%20the%20training%20and%20rendering%20time.%20In%20this%20paper%2C%20we%20show%20how%20the%0Astandard%20Gaussian%20splatting%20framework%20can%20be%20adapted%20for%20remote%20sensing%2C%0Aretaining%20its%20high%20efficiency.%20This%20enables%20us%20to%20achieve%20state-of-the-art%0Aperformance%20in%20just%20a%20few%20minutes%2C%20compared%20to%20the%20day-long%20optimization%0Arequired%20by%20the%20best-performing%20NeRF-based%20Earth%20observation%20methods.%20The%0Aproposed%20framework%20incorporates%20remote-sensing%20improvements%20from%20EO-NeRF%2C%20such%0Aas%20radiometric%20correction%20and%20shadow%20modeling%2C%20while%20introducing%20novel%0Acomponents%2C%20including%20sparsity%2C%20view%20consistency%2C%20and%20opacity%20regularizations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13047v1&entry.124074799=Read"},
{"title": "Real-time Free-view Human Rendering from Sparse-view RGB Videos using\n  Double Unprojected Textures", "author": "Guoxing Sun and Rishabh Dabral and Heming Zhu and Pascal Fua and Christian Theobalt and Marc Habermann", "abstract": "  Real-time free-view human rendering from sparse-view RGB inputs is a\nchallenging task due to the sensor scarcity and the tight time budget. To\nensure efficiency, recent methods leverage 2D CNNs operating in texture space\nto learn rendering primitives. However, they either jointly learn geometry and\nappearance, or completely ignore sparse image information for geometry\nestimation, significantly harming visual quality and robustness to unseen body\nposes. To address these issues, we present Double Unprojected Textures, which\nat the core disentangles coarse geometric deformation estimation from\nappearance synthesis, enabling robust and photorealistic 4K rendering in\nreal-time. Specifically, we first introduce a novel image-conditioned template\ndeformation network, which estimates the coarse deformation of the human\ntemplate from a first unprojected texture. This updated geometry is then used\nto apply a second and more accurate texture unprojection. The resulting texture\nmap has fewer artifacts and better alignment with input views, which benefits\nour learning of finer-level geometry and appearance represented by Gaussian\nsplats. We validate the effectiveness and efficiency of the proposed method in\nquantitative and qualitative experiments, which significantly surpasses other\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2412.13183v1", "date": "2024-12-17", "relevancy": 3.2223, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6596}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6408}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Free-view%20Human%20Rendering%20from%20Sparse-view%20RGB%20Videos%20using%0A%20%20Double%20Unprojected%20Textures&body=Title%3A%20Real-time%20Free-view%20Human%20Rendering%20from%20Sparse-view%20RGB%20Videos%20using%0A%20%20Double%20Unprojected%20Textures%0AAuthor%3A%20Guoxing%20Sun%20and%20Rishabh%20Dabral%20and%20Heming%20Zhu%20and%20Pascal%20Fua%20and%20Christian%20Theobalt%20and%20Marc%20Habermann%0AAbstract%3A%20%20%20Real-time%20free-view%20human%20rendering%20from%20sparse-view%20RGB%20inputs%20is%20a%0Achallenging%20task%20due%20to%20the%20sensor%20scarcity%20and%20the%20tight%20time%20budget.%20To%0Aensure%20efficiency%2C%20recent%20methods%20leverage%202D%20CNNs%20operating%20in%20texture%20space%0Ato%20learn%20rendering%20primitives.%20However%2C%20they%20either%20jointly%20learn%20geometry%20and%0Aappearance%2C%20or%20completely%20ignore%20sparse%20image%20information%20for%20geometry%0Aestimation%2C%20significantly%20harming%20visual%20quality%20and%20robustness%20to%20unseen%20body%0Aposes.%20To%20address%20these%20issues%2C%20we%20present%20Double%20Unprojected%20Textures%2C%20which%0Aat%20the%20core%20disentangles%20coarse%20geometric%20deformation%20estimation%20from%0Aappearance%20synthesis%2C%20enabling%20robust%20and%20photorealistic%204K%20rendering%20in%0Areal-time.%20Specifically%2C%20we%20first%20introduce%20a%20novel%20image-conditioned%20template%0Adeformation%20network%2C%20which%20estimates%20the%20coarse%20deformation%20of%20the%20human%0Atemplate%20from%20a%20first%20unprojected%20texture.%20This%20updated%20geometry%20is%20then%20used%0Ato%20apply%20a%20second%20and%20more%20accurate%20texture%20unprojection.%20The%20resulting%20texture%0Amap%20has%20fewer%20artifacts%20and%20better%20alignment%20with%20input%20views%2C%20which%20benefits%0Aour%20learning%20of%20finer-level%20geometry%20and%20appearance%20represented%20by%20Gaussian%0Asplats.%20We%20validate%20the%20effectiveness%20and%20efficiency%20of%20the%20proposed%20method%20in%0Aquantitative%20and%20qualitative%20experiments%2C%20which%20significantly%20surpasses%20other%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Free-view%2520Human%2520Rendering%2520from%2520Sparse-view%2520RGB%2520Videos%2520using%250A%2520%2520Double%2520Unprojected%2520Textures%26entry.906535625%3DGuoxing%2520Sun%2520and%2520Rishabh%2520Dabral%2520and%2520Heming%2520Zhu%2520and%2520Pascal%2520Fua%2520and%2520Christian%2520Theobalt%2520and%2520Marc%2520Habermann%26entry.1292438233%3D%2520%2520Real-time%2520free-view%2520human%2520rendering%2520from%2520sparse-view%2520RGB%2520inputs%2520is%2520a%250Achallenging%2520task%2520due%2520to%2520the%2520sensor%2520scarcity%2520and%2520the%2520tight%2520time%2520budget.%2520To%250Aensure%2520efficiency%252C%2520recent%2520methods%2520leverage%25202D%2520CNNs%2520operating%2520in%2520texture%2520space%250Ato%2520learn%2520rendering%2520primitives.%2520However%252C%2520they%2520either%2520jointly%2520learn%2520geometry%2520and%250Aappearance%252C%2520or%2520completely%2520ignore%2520sparse%2520image%2520information%2520for%2520geometry%250Aestimation%252C%2520significantly%2520harming%2520visual%2520quality%2520and%2520robustness%2520to%2520unseen%2520body%250Aposes.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520Double%2520Unprojected%2520Textures%252C%2520which%250Aat%2520the%2520core%2520disentangles%2520coarse%2520geometric%2520deformation%2520estimation%2520from%250Aappearance%2520synthesis%252C%2520enabling%2520robust%2520and%2520photorealistic%25204K%2520rendering%2520in%250Areal-time.%2520Specifically%252C%2520we%2520first%2520introduce%2520a%2520novel%2520image-conditioned%2520template%250Adeformation%2520network%252C%2520which%2520estimates%2520the%2520coarse%2520deformation%2520of%2520the%2520human%250Atemplate%2520from%2520a%2520first%2520unprojected%2520texture.%2520This%2520updated%2520geometry%2520is%2520then%2520used%250Ato%2520apply%2520a%2520second%2520and%2520more%2520accurate%2520texture%2520unprojection.%2520The%2520resulting%2520texture%250Amap%2520has%2520fewer%2520artifacts%2520and%2520better%2520alignment%2520with%2520input%2520views%252C%2520which%2520benefits%250Aour%2520learning%2520of%2520finer-level%2520geometry%2520and%2520appearance%2520represented%2520by%2520Gaussian%250Asplats.%2520We%2520validate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520the%2520proposed%2520method%2520in%250Aquantitative%2520and%2520qualitative%2520experiments%252C%2520which%2520significantly%2520surpasses%2520other%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Free-view%20Human%20Rendering%20from%20Sparse-view%20RGB%20Videos%20using%0A%20%20Double%20Unprojected%20Textures&entry.906535625=Guoxing%20Sun%20and%20Rishabh%20Dabral%20and%20Heming%20Zhu%20and%20Pascal%20Fua%20and%20Christian%20Theobalt%20and%20Marc%20Habermann&entry.1292438233=%20%20Real-time%20free-view%20human%20rendering%20from%20sparse-view%20RGB%20inputs%20is%20a%0Achallenging%20task%20due%20to%20the%20sensor%20scarcity%20and%20the%20tight%20time%20budget.%20To%0Aensure%20efficiency%2C%20recent%20methods%20leverage%202D%20CNNs%20operating%20in%20texture%20space%0Ato%20learn%20rendering%20primitives.%20However%2C%20they%20either%20jointly%20learn%20geometry%20and%0Aappearance%2C%20or%20completely%20ignore%20sparse%20image%20information%20for%20geometry%0Aestimation%2C%20significantly%20harming%20visual%20quality%20and%20robustness%20to%20unseen%20body%0Aposes.%20To%20address%20these%20issues%2C%20we%20present%20Double%20Unprojected%20Textures%2C%20which%0Aat%20the%20core%20disentangles%20coarse%20geometric%20deformation%20estimation%20from%0Aappearance%20synthesis%2C%20enabling%20robust%20and%20photorealistic%204K%20rendering%20in%0Areal-time.%20Specifically%2C%20we%20first%20introduce%20a%20novel%20image-conditioned%20template%0Adeformation%20network%2C%20which%20estimates%20the%20coarse%20deformation%20of%20the%20human%0Atemplate%20from%20a%20first%20unprojected%20texture.%20This%20updated%20geometry%20is%20then%20used%0Ato%20apply%20a%20second%20and%20more%20accurate%20texture%20unprojection.%20The%20resulting%20texture%0Amap%20has%20fewer%20artifacts%20and%20better%20alignment%20with%20input%20views%2C%20which%20benefits%0Aour%20learning%20of%20finer-level%20geometry%20and%20appearance%20represented%20by%20Gaussian%0Asplats.%20We%20validate%20the%20effectiveness%20and%20efficiency%20of%20the%20proposed%20method%20in%0Aquantitative%20and%20qualitative%20experiments%2C%20which%20significantly%20surpasses%20other%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13183v1&entry.124074799=Read"},
{"title": "GaussTR: Foundation Model-Aligned Gaussian Transformer for\n  Self-Supervised 3D Spatial Understanding", "author": "Haoyi Jiang and Liu Liu and Tianheng Cheng and Xinjie Wang and Tianwei Lin and Zhizhong Su and Wenyu Liu and Xinggang Wang", "abstract": "  3D Semantic Occupancy Prediction is fundamental for spatial understanding as\nit provides a comprehensive semantic cognition of surrounding environments.\nHowever, prevalent approaches primarily rely on extensive labeled data and\ncomputationally intensive voxel-based modeling, restricting the scalability and\ngeneralizability of 3D representation learning. In this paper, we introduce\nGaussTR, a novel Gaussian Transformer that leverages alignment with foundation\nmodels to advance self-supervised 3D spatial understanding. GaussTR adopts a\nTransformer architecture to predict sparse sets of 3D Gaussians that represent\nscenes in a feed-forward manner. Through aligning rendered Gaussian features\nwith diverse knowledge from pre-trained foundation models, GaussTR facilitates\nthe learning of versatile 3D representations and enables open-vocabulary\noccupancy prediction without explicit annotations. Empirical evaluations on the\nOcc3D-nuScenes dataset showcase GaussTR's state-of-the-art zero-shot\nperformance, achieving 11.70 mIoU while reducing training duration by\napproximately 50%. These experimental results highlight the significant\npotential of GaussTR for scalable and holistic 3D spatial understanding, with\npromising implications for autonomous driving and embodied agents. Code is\navailable at https://github.com/hustvl/GaussTR.\n", "link": "http://arxiv.org/abs/2412.13193v1", "date": "2024-12-17", "relevancy": 3.1487, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6609}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6398}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussTR%3A%20Foundation%20Model-Aligned%20Gaussian%20Transformer%20for%0A%20%20Self-Supervised%203D%20Spatial%20Understanding&body=Title%3A%20GaussTR%3A%20Foundation%20Model-Aligned%20Gaussian%20Transformer%20for%0A%20%20Self-Supervised%203D%20Spatial%20Understanding%0AAuthor%3A%20Haoyi%20Jiang%20and%20Liu%20Liu%20and%20Tianheng%20Cheng%20and%20Xinjie%20Wang%20and%20Tianwei%20Lin%20and%20Zhizhong%20Su%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%203D%20Semantic%20Occupancy%20Prediction%20is%20fundamental%20for%20spatial%20understanding%20as%0Ait%20provides%20a%20comprehensive%20semantic%20cognition%20of%20surrounding%20environments.%0AHowever%2C%20prevalent%20approaches%20primarily%20rely%20on%20extensive%20labeled%20data%20and%0Acomputationally%20intensive%20voxel-based%20modeling%2C%20restricting%20the%20scalability%20and%0Ageneralizability%20of%203D%20representation%20learning.%20In%20this%20paper%2C%20we%20introduce%0AGaussTR%2C%20a%20novel%20Gaussian%20Transformer%20that%20leverages%20alignment%20with%20foundation%0Amodels%20to%20advance%20self-supervised%203D%20spatial%20understanding.%20GaussTR%20adopts%20a%0ATransformer%20architecture%20to%20predict%20sparse%20sets%20of%203D%20Gaussians%20that%20represent%0Ascenes%20in%20a%20feed-forward%20manner.%20Through%20aligning%20rendered%20Gaussian%20features%0Awith%20diverse%20knowledge%20from%20pre-trained%20foundation%20models%2C%20GaussTR%20facilitates%0Athe%20learning%20of%20versatile%203D%20representations%20and%20enables%20open-vocabulary%0Aoccupancy%20prediction%20without%20explicit%20annotations.%20Empirical%20evaluations%20on%20the%0AOcc3D-nuScenes%20dataset%20showcase%20GaussTR%27s%20state-of-the-art%20zero-shot%0Aperformance%2C%20achieving%2011.70%20mIoU%20while%20reducing%20training%20duration%20by%0Aapproximately%2050%25.%20These%20experimental%20results%20highlight%20the%20significant%0Apotential%20of%20GaussTR%20for%20scalable%20and%20holistic%203D%20spatial%20understanding%2C%20with%0Apromising%20implications%20for%20autonomous%20driving%20and%20embodied%20agents.%20Code%20is%0Aavailable%20at%20https%3A//github.com/hustvl/GaussTR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussTR%253A%2520Foundation%2520Model-Aligned%2520Gaussian%2520Transformer%2520for%250A%2520%2520Self-Supervised%25203D%2520Spatial%2520Understanding%26entry.906535625%3DHaoyi%2520Jiang%2520and%2520Liu%2520Liu%2520and%2520Tianheng%2520Cheng%2520and%2520Xinjie%2520Wang%2520and%2520Tianwei%2520Lin%2520and%2520Zhizhong%2520Su%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%25203D%2520Semantic%2520Occupancy%2520Prediction%2520is%2520fundamental%2520for%2520spatial%2520understanding%2520as%250Ait%2520provides%2520a%2520comprehensive%2520semantic%2520cognition%2520of%2520surrounding%2520environments.%250AHowever%252C%2520prevalent%2520approaches%2520primarily%2520rely%2520on%2520extensive%2520labeled%2520data%2520and%250Acomputationally%2520intensive%2520voxel-based%2520modeling%252C%2520restricting%2520the%2520scalability%2520and%250Ageneralizability%2520of%25203D%2520representation%2520learning.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AGaussTR%252C%2520a%2520novel%2520Gaussian%2520Transformer%2520that%2520leverages%2520alignment%2520with%2520foundation%250Amodels%2520to%2520advance%2520self-supervised%25203D%2520spatial%2520understanding.%2520GaussTR%2520adopts%2520a%250ATransformer%2520architecture%2520to%2520predict%2520sparse%2520sets%2520of%25203D%2520Gaussians%2520that%2520represent%250Ascenes%2520in%2520a%2520feed-forward%2520manner.%2520Through%2520aligning%2520rendered%2520Gaussian%2520features%250Awith%2520diverse%2520knowledge%2520from%2520pre-trained%2520foundation%2520models%252C%2520GaussTR%2520facilitates%250Athe%2520learning%2520of%2520versatile%25203D%2520representations%2520and%2520enables%2520open-vocabulary%250Aoccupancy%2520prediction%2520without%2520explicit%2520annotations.%2520Empirical%2520evaluations%2520on%2520the%250AOcc3D-nuScenes%2520dataset%2520showcase%2520GaussTR%2527s%2520state-of-the-art%2520zero-shot%250Aperformance%252C%2520achieving%252011.70%2520mIoU%2520while%2520reducing%2520training%2520duration%2520by%250Aapproximately%252050%2525.%2520These%2520experimental%2520results%2520highlight%2520the%2520significant%250Apotential%2520of%2520GaussTR%2520for%2520scalable%2520and%2520holistic%25203D%2520spatial%2520understanding%252C%2520with%250Apromising%2520implications%2520for%2520autonomous%2520driving%2520and%2520embodied%2520agents.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/hustvl/GaussTR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussTR%3A%20Foundation%20Model-Aligned%20Gaussian%20Transformer%20for%0A%20%20Self-Supervised%203D%20Spatial%20Understanding&entry.906535625=Haoyi%20Jiang%20and%20Liu%20Liu%20and%20Tianheng%20Cheng%20and%20Xinjie%20Wang%20and%20Tianwei%20Lin%20and%20Zhizhong%20Su%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%203D%20Semantic%20Occupancy%20Prediction%20is%20fundamental%20for%20spatial%20understanding%20as%0Ait%20provides%20a%20comprehensive%20semantic%20cognition%20of%20surrounding%20environments.%0AHowever%2C%20prevalent%20approaches%20primarily%20rely%20on%20extensive%20labeled%20data%20and%0Acomputationally%20intensive%20voxel-based%20modeling%2C%20restricting%20the%20scalability%20and%0Ageneralizability%20of%203D%20representation%20learning.%20In%20this%20paper%2C%20we%20introduce%0AGaussTR%2C%20a%20novel%20Gaussian%20Transformer%20that%20leverages%20alignment%20with%20foundation%0Amodels%20to%20advance%20self-supervised%203D%20spatial%20understanding.%20GaussTR%20adopts%20a%0ATransformer%20architecture%20to%20predict%20sparse%20sets%20of%203D%20Gaussians%20that%20represent%0Ascenes%20in%20a%20feed-forward%20manner.%20Through%20aligning%20rendered%20Gaussian%20features%0Awith%20diverse%20knowledge%20from%20pre-trained%20foundation%20models%2C%20GaussTR%20facilitates%0Athe%20learning%20of%20versatile%203D%20representations%20and%20enables%20open-vocabulary%0Aoccupancy%20prediction%20without%20explicit%20annotations.%20Empirical%20evaluations%20on%20the%0AOcc3D-nuScenes%20dataset%20showcase%20GaussTR%27s%20state-of-the-art%20zero-shot%0Aperformance%2C%20achieving%2011.70%20mIoU%20while%20reducing%20training%20duration%20by%0Aapproximately%2050%25.%20These%20experimental%20results%20highlight%20the%20significant%0Apotential%20of%20GaussTR%20for%20scalable%20and%20holistic%203D%20spatial%20understanding%2C%20with%0Apromising%20implications%20for%20autonomous%20driving%20and%20embodied%20agents.%20Code%20is%0Aavailable%20at%20https%3A//github.com/hustvl/GaussTR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13193v1&entry.124074799=Read"},
{"title": "NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle\n  Adjustment", "author": "Andrea Dunn Beltran and Daniel Rho and Marc Niethammer and Roni Sengupta", "abstract": "  Simultaneous Localization And Mapping (SLAM) from a monocular endoscopy video\ncan enable autonomous navigation, guidance to unsurveyed regions, and 3D\nvisualizations, which can significantly improve endoscopy experience for\nsurgeons and patient outcomes. Existing dense SLAM algorithms often assume\ndistant and static lighting and textured surfaces, and alternate between\noptimizing scene geometry and camera parameters by minimizing a photometric\nrendering loss, often called Photometric Bundle Adjustment. However, endoscopic\nenvironments exhibit dynamic near-field lighting due to the co-located light\nand camera moving extremely close to the surface, textureless surfaces, and\nstrong specular reflections due to mucus layers. When not considered, these\nnear-field lighting effects can cause significant performance reductions for\nexisting SLAM algorithms from indoor/outdoor scenes when applied to endoscopy\nvideos. To mitigate this problem, we introduce a new Near-Field Lighting Bundle\nAdjustment Loss $(L_{NFL-BA})$ that can also be alternatingly optimized, along\nwith the Photometric Bundle Adjustment loss, such that the captured images'\nintensity variations match the relative distance and orientation between the\nsurface and the co-located light and camera. We derive a general NFL-BA loss\nfunction for 3D Gaussian surface representations and demonstrate that adding\n$L_{NFL-BA}$ can significantly improve the tracking and mapping performance of\ntwo state-of-the-art 3DGS-SLAM systems, MonoGS (35% improvement in tracking,\n48% improvement in mapping with predicted depth maps) and EndoGSLAM (22%\nimprovement in tracking, marginal improvement in mapping with predicted\ndepths), on the C3VD endoscopy dataset for colons. The project page is\navailable at https://asdunnbe.github.io/NFL-BA/\n", "link": "http://arxiv.org/abs/2412.13176v1", "date": "2024-12-17", "relevancy": 2.9691, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6223}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5893}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NFL-BA%3A%20Improving%20Endoscopic%20SLAM%20with%20Near-Field%20Light%20Bundle%0A%20%20Adjustment&body=Title%3A%20NFL-BA%3A%20Improving%20Endoscopic%20SLAM%20with%20Near-Field%20Light%20Bundle%0A%20%20Adjustment%0AAuthor%3A%20Andrea%20Dunn%20Beltran%20and%20Daniel%20Rho%20and%20Marc%20Niethammer%20and%20Roni%20Sengupta%0AAbstract%3A%20%20%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29%20from%20a%20monocular%20endoscopy%20video%0Acan%20enable%20autonomous%20navigation%2C%20guidance%20to%20unsurveyed%20regions%2C%20and%203D%0Avisualizations%2C%20which%20can%20significantly%20improve%20endoscopy%20experience%20for%0Asurgeons%20and%20patient%20outcomes.%20Existing%20dense%20SLAM%20algorithms%20often%20assume%0Adistant%20and%20static%20lighting%20and%20textured%20surfaces%2C%20and%20alternate%20between%0Aoptimizing%20scene%20geometry%20and%20camera%20parameters%20by%20minimizing%20a%20photometric%0Arendering%20loss%2C%20often%20called%20Photometric%20Bundle%20Adjustment.%20However%2C%20endoscopic%0Aenvironments%20exhibit%20dynamic%20near-field%20lighting%20due%20to%20the%20co-located%20light%0Aand%20camera%20moving%20extremely%20close%20to%20the%20surface%2C%20textureless%20surfaces%2C%20and%0Astrong%20specular%20reflections%20due%20to%20mucus%20layers.%20When%20not%20considered%2C%20these%0Anear-field%20lighting%20effects%20can%20cause%20significant%20performance%20reductions%20for%0Aexisting%20SLAM%20algorithms%20from%20indoor/outdoor%20scenes%20when%20applied%20to%20endoscopy%0Avideos.%20To%20mitigate%20this%20problem%2C%20we%20introduce%20a%20new%20Near-Field%20Lighting%20Bundle%0AAdjustment%20Loss%20%24%28L_%7BNFL-BA%7D%29%24%20that%20can%20also%20be%20alternatingly%20optimized%2C%20along%0Awith%20the%20Photometric%20Bundle%20Adjustment%20loss%2C%20such%20that%20the%20captured%20images%27%0Aintensity%20variations%20match%20the%20relative%20distance%20and%20orientation%20between%20the%0Asurface%20and%20the%20co-located%20light%20and%20camera.%20We%20derive%20a%20general%20NFL-BA%20loss%0Afunction%20for%203D%20Gaussian%20surface%20representations%20and%20demonstrate%20that%20adding%0A%24L_%7BNFL-BA%7D%24%20can%20significantly%20improve%20the%20tracking%20and%20mapping%20performance%20of%0Atwo%20state-of-the-art%203DGS-SLAM%20systems%2C%20MonoGS%20%2835%25%20improvement%20in%20tracking%2C%0A48%25%20improvement%20in%20mapping%20with%20predicted%20depth%20maps%29%20and%20EndoGSLAM%20%2822%25%0Aimprovement%20in%20tracking%2C%20marginal%20improvement%20in%20mapping%20with%20predicted%0Adepths%29%2C%20on%20the%20C3VD%20endoscopy%20dataset%20for%20colons.%20The%20project%20page%20is%0Aavailable%20at%20https%3A//asdunnbe.github.io/NFL-BA/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNFL-BA%253A%2520Improving%2520Endoscopic%2520SLAM%2520with%2520Near-Field%2520Light%2520Bundle%250A%2520%2520Adjustment%26entry.906535625%3DAndrea%2520Dunn%2520Beltran%2520and%2520Daniel%2520Rho%2520and%2520Marc%2520Niethammer%2520and%2520Roni%2520Sengupta%26entry.1292438233%3D%2520%2520Simultaneous%2520Localization%2520And%2520Mapping%2520%2528SLAM%2529%2520from%2520a%2520monocular%2520endoscopy%2520video%250Acan%2520enable%2520autonomous%2520navigation%252C%2520guidance%2520to%2520unsurveyed%2520regions%252C%2520and%25203D%250Avisualizations%252C%2520which%2520can%2520significantly%2520improve%2520endoscopy%2520experience%2520for%250Asurgeons%2520and%2520patient%2520outcomes.%2520Existing%2520dense%2520SLAM%2520algorithms%2520often%2520assume%250Adistant%2520and%2520static%2520lighting%2520and%2520textured%2520surfaces%252C%2520and%2520alternate%2520between%250Aoptimizing%2520scene%2520geometry%2520and%2520camera%2520parameters%2520by%2520minimizing%2520a%2520photometric%250Arendering%2520loss%252C%2520often%2520called%2520Photometric%2520Bundle%2520Adjustment.%2520However%252C%2520endoscopic%250Aenvironments%2520exhibit%2520dynamic%2520near-field%2520lighting%2520due%2520to%2520the%2520co-located%2520light%250Aand%2520camera%2520moving%2520extremely%2520close%2520to%2520the%2520surface%252C%2520textureless%2520surfaces%252C%2520and%250Astrong%2520specular%2520reflections%2520due%2520to%2520mucus%2520layers.%2520When%2520not%2520considered%252C%2520these%250Anear-field%2520lighting%2520effects%2520can%2520cause%2520significant%2520performance%2520reductions%2520for%250Aexisting%2520SLAM%2520algorithms%2520from%2520indoor/outdoor%2520scenes%2520when%2520applied%2520to%2520endoscopy%250Avideos.%2520To%2520mitigate%2520this%2520problem%252C%2520we%2520introduce%2520a%2520new%2520Near-Field%2520Lighting%2520Bundle%250AAdjustment%2520Loss%2520%2524%2528L_%257BNFL-BA%257D%2529%2524%2520that%2520can%2520also%2520be%2520alternatingly%2520optimized%252C%2520along%250Awith%2520the%2520Photometric%2520Bundle%2520Adjustment%2520loss%252C%2520such%2520that%2520the%2520captured%2520images%2527%250Aintensity%2520variations%2520match%2520the%2520relative%2520distance%2520and%2520orientation%2520between%2520the%250Asurface%2520and%2520the%2520co-located%2520light%2520and%2520camera.%2520We%2520derive%2520a%2520general%2520NFL-BA%2520loss%250Afunction%2520for%25203D%2520Gaussian%2520surface%2520representations%2520and%2520demonstrate%2520that%2520adding%250A%2524L_%257BNFL-BA%257D%2524%2520can%2520significantly%2520improve%2520the%2520tracking%2520and%2520mapping%2520performance%2520of%250Atwo%2520state-of-the-art%25203DGS-SLAM%2520systems%252C%2520MonoGS%2520%252835%2525%2520improvement%2520in%2520tracking%252C%250A48%2525%2520improvement%2520in%2520mapping%2520with%2520predicted%2520depth%2520maps%2529%2520and%2520EndoGSLAM%2520%252822%2525%250Aimprovement%2520in%2520tracking%252C%2520marginal%2520improvement%2520in%2520mapping%2520with%2520predicted%250Adepths%2529%252C%2520on%2520the%2520C3VD%2520endoscopy%2520dataset%2520for%2520colons.%2520The%2520project%2520page%2520is%250Aavailable%2520at%2520https%253A//asdunnbe.github.io/NFL-BA/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NFL-BA%3A%20Improving%20Endoscopic%20SLAM%20with%20Near-Field%20Light%20Bundle%0A%20%20Adjustment&entry.906535625=Andrea%20Dunn%20Beltran%20and%20Daniel%20Rho%20and%20Marc%20Niethammer%20and%20Roni%20Sengupta&entry.1292438233=%20%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29%20from%20a%20monocular%20endoscopy%20video%0Acan%20enable%20autonomous%20navigation%2C%20guidance%20to%20unsurveyed%20regions%2C%20and%203D%0Avisualizations%2C%20which%20can%20significantly%20improve%20endoscopy%20experience%20for%0Asurgeons%20and%20patient%20outcomes.%20Existing%20dense%20SLAM%20algorithms%20often%20assume%0Adistant%20and%20static%20lighting%20and%20textured%20surfaces%2C%20and%20alternate%20between%0Aoptimizing%20scene%20geometry%20and%20camera%20parameters%20by%20minimizing%20a%20photometric%0Arendering%20loss%2C%20often%20called%20Photometric%20Bundle%20Adjustment.%20However%2C%20endoscopic%0Aenvironments%20exhibit%20dynamic%20near-field%20lighting%20due%20to%20the%20co-located%20light%0Aand%20camera%20moving%20extremely%20close%20to%20the%20surface%2C%20textureless%20surfaces%2C%20and%0Astrong%20specular%20reflections%20due%20to%20mucus%20layers.%20When%20not%20considered%2C%20these%0Anear-field%20lighting%20effects%20can%20cause%20significant%20performance%20reductions%20for%0Aexisting%20SLAM%20algorithms%20from%20indoor/outdoor%20scenes%20when%20applied%20to%20endoscopy%0Avideos.%20To%20mitigate%20this%20problem%2C%20we%20introduce%20a%20new%20Near-Field%20Lighting%20Bundle%0AAdjustment%20Loss%20%24%28L_%7BNFL-BA%7D%29%24%20that%20can%20also%20be%20alternatingly%20optimized%2C%20along%0Awith%20the%20Photometric%20Bundle%20Adjustment%20loss%2C%20such%20that%20the%20captured%20images%27%0Aintensity%20variations%20match%20the%20relative%20distance%20and%20orientation%20between%20the%0Asurface%20and%20the%20co-located%20light%20and%20camera.%20We%20derive%20a%20general%20NFL-BA%20loss%0Afunction%20for%203D%20Gaussian%20surface%20representations%20and%20demonstrate%20that%20adding%0A%24L_%7BNFL-BA%7D%24%20can%20significantly%20improve%20the%20tracking%20and%20mapping%20performance%20of%0Atwo%20state-of-the-art%203DGS-SLAM%20systems%2C%20MonoGS%20%2835%25%20improvement%20in%20tracking%2C%0A48%25%20improvement%20in%20mapping%20with%20predicted%20depth%20maps%29%20and%20EndoGSLAM%20%2822%25%0Aimprovement%20in%20tracking%2C%20marginal%20improvement%20in%20mapping%20with%20predicted%0Adepths%29%2C%20on%20the%20C3VD%20endoscopy%20dataset%20for%20colons.%20The%20project%20page%20is%0Aavailable%20at%20https%3A//asdunnbe.github.io/NFL-BA/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13176v1&entry.124074799=Read"},
{"title": "Feather the Throttle: Revisiting Visual Token Pruning for\n  Vision-Language Model Acceleration", "author": "Mark Endo and Xiaohan Wang and Serena Yeung-Levy", "abstract": "  Recent works on accelerating Vision-Language Models show that strong\nperformance can be maintained across a variety of vision-language tasks despite\nhighly compressing visual information. In this work, we examine the popular\nacceleration approach of early pruning of visual tokens inside the language\nmodel and find that its strong performance across many tasks is not due to an\nexceptional ability to compress visual information, but rather the benchmarks'\nlimited ability to assess fine-grained visual capabilities. Namely, we\ndemonstrate a core issue with the acceleration approach where most tokens\ntowards the top of the image are pruned away. Yet, this issue is only reflected\nin performance for a small subset of tasks such as localization. For the other\nevaluated tasks, strong performance is maintained with the flawed pruning\nstrategy. Noting the limited visual capabilities of the studied acceleration\ntechnique, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble\ncRiteria), a straightforward approach that (1) resolves the identified issue\nwith early-layer pruning, (2) incorporates uniform sampling to ensure coverage\nacross all image regions, and (3) applies pruning in two stages to allow the\ncriteria to become more effective at a later layer while still achieving\nsignificant speedup through early-layer pruning. With comparable computational\nsavings, we find that FEATHER has more than $5\\times$ performance improvement\non the vision-centric localization benchmarks compared to the original\nacceleration approach.\n", "link": "http://arxiv.org/abs/2412.13180v1", "date": "2024-12-17", "relevancy": 2.8388, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feather%20the%20Throttle%3A%20Revisiting%20Visual%20Token%20Pruning%20for%0A%20%20Vision-Language%20Model%20Acceleration&body=Title%3A%20Feather%20the%20Throttle%3A%20Revisiting%20Visual%20Token%20Pruning%20for%0A%20%20Vision-Language%20Model%20Acceleration%0AAuthor%3A%20Mark%20Endo%20and%20Xiaohan%20Wang%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20%20%20Recent%20works%20on%20accelerating%20Vision-Language%20Models%20show%20that%20strong%0Aperformance%20can%20be%20maintained%20across%20a%20variety%20of%20vision-language%20tasks%20despite%0Ahighly%20compressing%20visual%20information.%20In%20this%20work%2C%20we%20examine%20the%20popular%0Aacceleration%20approach%20of%20early%20pruning%20of%20visual%20tokens%20inside%20the%20language%0Amodel%20and%20find%20that%20its%20strong%20performance%20across%20many%20tasks%20is%20not%20due%20to%20an%0Aexceptional%20ability%20to%20compress%20visual%20information%2C%20but%20rather%20the%20benchmarks%27%0Alimited%20ability%20to%20assess%20fine-grained%20visual%20capabilities.%20Namely%2C%20we%0Ademonstrate%20a%20core%20issue%20with%20the%20acceleration%20approach%20where%20most%20tokens%0Atowards%20the%20top%20of%20the%20image%20are%20pruned%20away.%20Yet%2C%20this%20issue%20is%20only%20reflected%0Ain%20performance%20for%20a%20small%20subset%20of%20tasks%20such%20as%20localization.%20For%20the%20other%0Aevaluated%20tasks%2C%20strong%20performance%20is%20maintained%20with%20the%20flawed%20pruning%0Astrategy.%20Noting%20the%20limited%20visual%20capabilities%20of%20the%20studied%20acceleration%0Atechnique%2C%20we%20propose%20FEATHER%20%28Fast%20and%20Effective%20Acceleration%20wiTH%20Ensemble%0AcRiteria%29%2C%20a%20straightforward%20approach%20that%20%281%29%20resolves%20the%20identified%20issue%0Awith%20early-layer%20pruning%2C%20%282%29%20incorporates%20uniform%20sampling%20to%20ensure%20coverage%0Aacross%20all%20image%20regions%2C%20and%20%283%29%20applies%20pruning%20in%20two%20stages%20to%20allow%20the%0Acriteria%20to%20become%20more%20effective%20at%20a%20later%20layer%20while%20still%20achieving%0Asignificant%20speedup%20through%20early-layer%20pruning.%20With%20comparable%20computational%0Asavings%2C%20we%20find%20that%20FEATHER%20has%20more%20than%20%245%5Ctimes%24%20performance%20improvement%0Aon%20the%20vision-centric%20localization%20benchmarks%20compared%20to%20the%20original%0Aacceleration%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeather%2520the%2520Throttle%253A%2520Revisiting%2520Visual%2520Token%2520Pruning%2520for%250A%2520%2520Vision-Language%2520Model%2520Acceleration%26entry.906535625%3DMark%2520Endo%2520and%2520Xiaohan%2520Wang%2520and%2520Serena%2520Yeung-Levy%26entry.1292438233%3D%2520%2520Recent%2520works%2520on%2520accelerating%2520Vision-Language%2520Models%2520show%2520that%2520strong%250Aperformance%2520can%2520be%2520maintained%2520across%2520a%2520variety%2520of%2520vision-language%2520tasks%2520despite%250Ahighly%2520compressing%2520visual%2520information.%2520In%2520this%2520work%252C%2520we%2520examine%2520the%2520popular%250Aacceleration%2520approach%2520of%2520early%2520pruning%2520of%2520visual%2520tokens%2520inside%2520the%2520language%250Amodel%2520and%2520find%2520that%2520its%2520strong%2520performance%2520across%2520many%2520tasks%2520is%2520not%2520due%2520to%2520an%250Aexceptional%2520ability%2520to%2520compress%2520visual%2520information%252C%2520but%2520rather%2520the%2520benchmarks%2527%250Alimited%2520ability%2520to%2520assess%2520fine-grained%2520visual%2520capabilities.%2520Namely%252C%2520we%250Ademonstrate%2520a%2520core%2520issue%2520with%2520the%2520acceleration%2520approach%2520where%2520most%2520tokens%250Atowards%2520the%2520top%2520of%2520the%2520image%2520are%2520pruned%2520away.%2520Yet%252C%2520this%2520issue%2520is%2520only%2520reflected%250Ain%2520performance%2520for%2520a%2520small%2520subset%2520of%2520tasks%2520such%2520as%2520localization.%2520For%2520the%2520other%250Aevaluated%2520tasks%252C%2520strong%2520performance%2520is%2520maintained%2520with%2520the%2520flawed%2520pruning%250Astrategy.%2520Noting%2520the%2520limited%2520visual%2520capabilities%2520of%2520the%2520studied%2520acceleration%250Atechnique%252C%2520we%2520propose%2520FEATHER%2520%2528Fast%2520and%2520Effective%2520Acceleration%2520wiTH%2520Ensemble%250AcRiteria%2529%252C%2520a%2520straightforward%2520approach%2520that%2520%25281%2529%2520resolves%2520the%2520identified%2520issue%250Awith%2520early-layer%2520pruning%252C%2520%25282%2529%2520incorporates%2520uniform%2520sampling%2520to%2520ensure%2520coverage%250Aacross%2520all%2520image%2520regions%252C%2520and%2520%25283%2529%2520applies%2520pruning%2520in%2520two%2520stages%2520to%2520allow%2520the%250Acriteria%2520to%2520become%2520more%2520effective%2520at%2520a%2520later%2520layer%2520while%2520still%2520achieving%250Asignificant%2520speedup%2520through%2520early-layer%2520pruning.%2520With%2520comparable%2520computational%250Asavings%252C%2520we%2520find%2520that%2520FEATHER%2520has%2520more%2520than%2520%25245%255Ctimes%2524%2520performance%2520improvement%250Aon%2520the%2520vision-centric%2520localization%2520benchmarks%2520compared%2520to%2520the%2520original%250Aacceleration%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feather%20the%20Throttle%3A%20Revisiting%20Visual%20Token%20Pruning%20for%0A%20%20Vision-Language%20Model%20Acceleration&entry.906535625=Mark%20Endo%20and%20Xiaohan%20Wang%20and%20Serena%20Yeung-Levy&entry.1292438233=%20%20Recent%20works%20on%20accelerating%20Vision-Language%20Models%20show%20that%20strong%0Aperformance%20can%20be%20maintained%20across%20a%20variety%20of%20vision-language%20tasks%20despite%0Ahighly%20compressing%20visual%20information.%20In%20this%20work%2C%20we%20examine%20the%20popular%0Aacceleration%20approach%20of%20early%20pruning%20of%20visual%20tokens%20inside%20the%20language%0Amodel%20and%20find%20that%20its%20strong%20performance%20across%20many%20tasks%20is%20not%20due%20to%20an%0Aexceptional%20ability%20to%20compress%20visual%20information%2C%20but%20rather%20the%20benchmarks%27%0Alimited%20ability%20to%20assess%20fine-grained%20visual%20capabilities.%20Namely%2C%20we%0Ademonstrate%20a%20core%20issue%20with%20the%20acceleration%20approach%20where%20most%20tokens%0Atowards%20the%20top%20of%20the%20image%20are%20pruned%20away.%20Yet%2C%20this%20issue%20is%20only%20reflected%0Ain%20performance%20for%20a%20small%20subset%20of%20tasks%20such%20as%20localization.%20For%20the%20other%0Aevaluated%20tasks%2C%20strong%20performance%20is%20maintained%20with%20the%20flawed%20pruning%0Astrategy.%20Noting%20the%20limited%20visual%20capabilities%20of%20the%20studied%20acceleration%0Atechnique%2C%20we%20propose%20FEATHER%20%28Fast%20and%20Effective%20Acceleration%20wiTH%20Ensemble%0AcRiteria%29%2C%20a%20straightforward%20approach%20that%20%281%29%20resolves%20the%20identified%20issue%0Awith%20early-layer%20pruning%2C%20%282%29%20incorporates%20uniform%20sampling%20to%20ensure%20coverage%0Aacross%20all%20image%20regions%2C%20and%20%283%29%20applies%20pruning%20in%20two%20stages%20to%20allow%20the%0Acriteria%20to%20become%20more%20effective%20at%20a%20later%20layer%20while%20still%20achieving%0Asignificant%20speedup%20through%20early-layer%20pruning.%20With%20comparable%20computational%0Asavings%2C%20we%20find%20that%20FEATHER%20has%20more%20than%20%245%5Ctimes%24%20performance%20improvement%0Aon%20the%20vision-centric%20localization%20benchmarks%20compared%20to%20the%20original%0Aacceleration%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13180v1&entry.124074799=Read"},
{"title": "Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling", "author": "Zhe Chen and Weiyun Wang and Yue Cao and Yangzhou Liu and Zhangwei Gao and Erfei Cui and Jinguo Zhu and Shenglong Ye and Hao Tian and Zhaoyang Liu and Lixin Gu and Xuehui Wang and Qingyun Li and Yimin Ren and Zixuan Chen and Jiapeng Luo and Jiahao Wang and Tan Jiang and Bo Wang and Conghui He and Botian Shi and Xingcheng Zhang and Han Lv and Yi Wang and Wenqi Shao and Pei Chu and Zhongying Tu and Tong He and Zhiyong Wu and Huipeng Deng and Jiaye Ge and Kai Chen and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang", "abstract": "  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n", "link": "http://arxiv.org/abs/2412.05271v3", "date": "2024-12-17", "relevancy": 2.7766, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%0A%20%20Model%2C%20Data%2C%20and%20Test-Time%20Scaling&body=Title%3A%20Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%0A%20%20Model%2C%20Data%2C%20and%20Test-Time%20Scaling%0AAuthor%3A%20Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhangwei%20Gao%20and%20Erfei%20Cui%20and%20Jinguo%20Zhu%20and%20Shenglong%20Ye%20and%20Hao%20Tian%20and%20Zhaoyang%20Liu%20and%20Lixin%20Gu%20and%20Xuehui%20Wang%20and%20Qingyun%20Li%20and%20Yimin%20Ren%20and%20Zixuan%20Chen%20and%20Jiapeng%20Luo%20and%20Jiahao%20Wang%20and%20Tan%20Jiang%20and%20Bo%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Xingcheng%20Zhang%20and%20Han%20Lv%20and%20Yi%20Wang%20and%20Wenqi%20Shao%20and%20Pei%20Chu%20and%20Zhongying%20Tu%20and%20Tong%20He%20and%20Zhiyong%20Wu%20and%20Huipeng%20Deng%20and%20Jiaye%20Ge%20and%20Kai%20Chen%20and%20Min%20Dou%20and%20Lewei%20Lu%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%20and%20Wenhai%20Wang%0AAbstract%3A%20%20%20We%20introduce%20InternVL%202.5%2C%20an%20advanced%20multimodal%20large%20language%20model%20%28MLLM%29%0Aseries%20that%20builds%20upon%20InternVL%202.0%2C%20maintaining%20its%20core%20model%20architecture%0Awhile%20introducing%20significant%20enhancements%20in%20training%20and%20testing%20strategies%0Aas%20well%20as%20data%20quality.%20In%20this%20work%2C%20we%20delve%20into%20the%20relationship%20between%0Amodel%20scaling%20and%20performance%2C%20systematically%20exploring%20the%20performance%20trends%0Ain%20vision%20encoders%2C%20language%20models%2C%20dataset%20sizes%2C%20and%20test-time%0Aconfigurations.%20Through%20extensive%20evaluations%20on%20a%20wide%20range%20of%20benchmarks%2C%0Aincluding%20multi-discipline%20reasoning%2C%20document%20understanding%2C%20multi-image%20/%0Avideo%20understanding%2C%20real-world%20comprehension%2C%20multimodal%20hallucination%0Adetection%2C%20visual%20grounding%2C%20multilingual%20capabilities%2C%20and%20pure%20language%0Aprocessing%2C%20InternVL%202.5%20exhibits%20competitive%20performance%2C%20rivaling%20leading%0Acommercial%20models%20such%20as%20GPT-4o%20and%20Claude-3.5-Sonnet.%20Notably%2C%20our%20model%20is%0Athe%20first%20open-source%20MLLMs%20to%20surpass%2070%25%20on%20the%20MMMU%20benchmark%2C%20achieving%20a%0A3.7-point%20improvement%20through%20Chain-of-Thought%20%28CoT%29%20reasoning%20and%20showcasing%0Astrong%20potential%20for%20test-time%20scaling.%20We%20hope%20this%20model%20contributes%20to%20the%0Aopen-source%20community%20by%20setting%20new%20standards%20for%20developing%20and%20applying%0Amultimodal%20AI%20systems.%20HuggingFace%20demo%20see%0Ahttps%3A//huggingface.co/spaces/OpenGVLab/InternVL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05271v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpanding%2520Performance%2520Boundaries%2520of%2520Open-Source%2520Multimodal%2520Models%2520with%250A%2520%2520Model%252C%2520Data%252C%2520and%2520Test-Time%2520Scaling%26entry.906535625%3DZhe%2520Chen%2520and%2520Weiyun%2520Wang%2520and%2520Yue%2520Cao%2520and%2520Yangzhou%2520Liu%2520and%2520Zhangwei%2520Gao%2520and%2520Erfei%2520Cui%2520and%2520Jinguo%2520Zhu%2520and%2520Shenglong%2520Ye%2520and%2520Hao%2520Tian%2520and%2520Zhaoyang%2520Liu%2520and%2520Lixin%2520Gu%2520and%2520Xuehui%2520Wang%2520and%2520Qingyun%2520Li%2520and%2520Yimin%2520Ren%2520and%2520Zixuan%2520Chen%2520and%2520Jiapeng%2520Luo%2520and%2520Jiahao%2520Wang%2520and%2520Tan%2520Jiang%2520and%2520Bo%2520Wang%2520and%2520Conghui%2520He%2520and%2520Botian%2520Shi%2520and%2520Xingcheng%2520Zhang%2520and%2520Han%2520Lv%2520and%2520Yi%2520Wang%2520and%2520Wenqi%2520Shao%2520and%2520Pei%2520Chu%2520and%2520Zhongying%2520Tu%2520and%2520Tong%2520He%2520and%2520Zhiyong%2520Wu%2520and%2520Huipeng%2520Deng%2520and%2520Jiaye%2520Ge%2520and%2520Kai%2520Chen%2520and%2520Min%2520Dou%2520and%2520Lewei%2520Lu%2520and%2520Xizhou%2520Zhu%2520and%2520Tong%2520Lu%2520and%2520Dahua%2520Lin%2520and%2520Yu%2520Qiao%2520and%2520Jifeng%2520Dai%2520and%2520Wenhai%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520InternVL%25202.5%252C%2520an%2520advanced%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%250Aseries%2520that%2520builds%2520upon%2520InternVL%25202.0%252C%2520maintaining%2520its%2520core%2520model%2520architecture%250Awhile%2520introducing%2520significant%2520enhancements%2520in%2520training%2520and%2520testing%2520strategies%250Aas%2520well%2520as%2520data%2520quality.%2520In%2520this%2520work%252C%2520we%2520delve%2520into%2520the%2520relationship%2520between%250Amodel%2520scaling%2520and%2520performance%252C%2520systematically%2520exploring%2520the%2520performance%2520trends%250Ain%2520vision%2520encoders%252C%2520language%2520models%252C%2520dataset%2520sizes%252C%2520and%2520test-time%250Aconfigurations.%2520Through%2520extensive%2520evaluations%2520on%2520a%2520wide%2520range%2520of%2520benchmarks%252C%250Aincluding%2520multi-discipline%2520reasoning%252C%2520document%2520understanding%252C%2520multi-image%2520/%250Avideo%2520understanding%252C%2520real-world%2520comprehension%252C%2520multimodal%2520hallucination%250Adetection%252C%2520visual%2520grounding%252C%2520multilingual%2520capabilities%252C%2520and%2520pure%2520language%250Aprocessing%252C%2520InternVL%25202.5%2520exhibits%2520competitive%2520performance%252C%2520rivaling%2520leading%250Acommercial%2520models%2520such%2520as%2520GPT-4o%2520and%2520Claude-3.5-Sonnet.%2520Notably%252C%2520our%2520model%2520is%250Athe%2520first%2520open-source%2520MLLMs%2520to%2520surpass%252070%2525%2520on%2520the%2520MMMU%2520benchmark%252C%2520achieving%2520a%250A3.7-point%2520improvement%2520through%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520and%2520showcasing%250Astrong%2520potential%2520for%2520test-time%2520scaling.%2520We%2520hope%2520this%2520model%2520contributes%2520to%2520the%250Aopen-source%2520community%2520by%2520setting%2520new%2520standards%2520for%2520developing%2520and%2520applying%250Amultimodal%2520AI%2520systems.%2520HuggingFace%2520demo%2520see%250Ahttps%253A//huggingface.co/spaces/OpenGVLab/InternVL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05271v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%0A%20%20Model%2C%20Data%2C%20and%20Test-Time%20Scaling&entry.906535625=Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhangwei%20Gao%20and%20Erfei%20Cui%20and%20Jinguo%20Zhu%20and%20Shenglong%20Ye%20and%20Hao%20Tian%20and%20Zhaoyang%20Liu%20and%20Lixin%20Gu%20and%20Xuehui%20Wang%20and%20Qingyun%20Li%20and%20Yimin%20Ren%20and%20Zixuan%20Chen%20and%20Jiapeng%20Luo%20and%20Jiahao%20Wang%20and%20Tan%20Jiang%20and%20Bo%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Xingcheng%20Zhang%20and%20Han%20Lv%20and%20Yi%20Wang%20and%20Wenqi%20Shao%20and%20Pei%20Chu%20and%20Zhongying%20Tu%20and%20Tong%20He%20and%20Zhiyong%20Wu%20and%20Huipeng%20Deng%20and%20Jiaye%20Ge%20and%20Kai%20Chen%20and%20Min%20Dou%20and%20Lewei%20Lu%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%20and%20Wenhai%20Wang&entry.1292438233=%20%20We%20introduce%20InternVL%202.5%2C%20an%20advanced%20multimodal%20large%20language%20model%20%28MLLM%29%0Aseries%20that%20builds%20upon%20InternVL%202.0%2C%20maintaining%20its%20core%20model%20architecture%0Awhile%20introducing%20significant%20enhancements%20in%20training%20and%20testing%20strategies%0Aas%20well%20as%20data%20quality.%20In%20this%20work%2C%20we%20delve%20into%20the%20relationship%20between%0Amodel%20scaling%20and%20performance%2C%20systematically%20exploring%20the%20performance%20trends%0Ain%20vision%20encoders%2C%20language%20models%2C%20dataset%20sizes%2C%20and%20test-time%0Aconfigurations.%20Through%20extensive%20evaluations%20on%20a%20wide%20range%20of%20benchmarks%2C%0Aincluding%20multi-discipline%20reasoning%2C%20document%20understanding%2C%20multi-image%20/%0Avideo%20understanding%2C%20real-world%20comprehension%2C%20multimodal%20hallucination%0Adetection%2C%20visual%20grounding%2C%20multilingual%20capabilities%2C%20and%20pure%20language%0Aprocessing%2C%20InternVL%202.5%20exhibits%20competitive%20performance%2C%20rivaling%20leading%0Acommercial%20models%20such%20as%20GPT-4o%20and%20Claude-3.5-Sonnet.%20Notably%2C%20our%20model%20is%0Athe%20first%20open-source%20MLLMs%20to%20surpass%2070%25%20on%20the%20MMMU%20benchmark%2C%20achieving%20a%0A3.7-point%20improvement%20through%20Chain-of-Thought%20%28CoT%29%20reasoning%20and%20showcasing%0Astrong%20potential%20for%20test-time%20scaling.%20We%20hope%20this%20model%20contributes%20to%20the%0Aopen-source%20community%20by%20setting%20new%20standards%20for%20developing%20and%20applying%0Amultimodal%20AI%20systems.%20HuggingFace%20demo%20see%0Ahttps%3A//huggingface.co/spaces/OpenGVLab/InternVL%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05271v3&entry.124074799=Read"},
{"title": "SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a\n  Training-Free Memory Tree", "author": "Shuangrui Ding and Rui Qian and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Yuwei Guo and Dahua Lin and Jiaqi Wang", "abstract": "  The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation\nmodel for object segmentation in both images and videos, paving the way for\nvarious downstream video applications. The crucial design of SAM 2 for video\nsegmentation is its memory module, which prompts object-aware memories from\nprevious frames for current frame prediction. However, its greedy-selection\nmemory design suffers from the \"error accumulation\" problem, where an errored\nor missed mask will cascade and influence the segmentation of the subsequent\nframes, which limits the performance of SAM 2 toward complex long-term videos.\nTo this end, we introduce SAM2Long, an improved training-free video object\nsegmentation strategy, which considers the segmentation uncertainty within each\nframe and chooses the video-level optimal results from multiple segmentation\npathways in a constrained tree search manner. In practice, we maintain a fixed\nnumber of segmentation pathways throughout the video. For each frame, multiple\nmasks are proposed based on the existing pathways, creating various candidate\nbranches. We then select the same fixed number of branches with higher\ncumulative scores as the new pathways for the next frame. After processing the\nfinal frame, the pathway with the highest cumulative score is chosen as the\nfinal segmentation result. Benefiting from its heuristic search design,\nSAM2Long is robust toward occlusions and object reappearances, and can\neffectively segment and track objects for complex long-term videos. Notably,\nSAM2Long achieves an average improvement of 3.0 points across all 24\nhead-to-head comparisons, with gains of up to 5.3 points in J&F on long-term\nvideo object segmentation benchmarks such as SA-V and LVOS. The code is\nreleased at https://github.com/Mark12Ding/SAM2Long.\n", "link": "http://arxiv.org/abs/2410.16268v2", "date": "2024-12-17", "relevancy": 2.6894, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5582}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM2Long%3A%20Enhancing%20SAM%202%20for%20Long%20Video%20Segmentation%20with%20a%0A%20%20Training-Free%20Memory%20Tree&body=Title%3A%20SAM2Long%3A%20Enhancing%20SAM%202%20for%20Long%20Video%20Segmentation%20with%20a%0A%20%20Training-Free%20Memory%20Tree%0AAuthor%3A%20Shuangrui%20Ding%20and%20Rui%20Qian%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Yuwei%20Guo%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%202%20%28SAM%202%29%20has%20emerged%20as%20a%20powerful%20foundation%0Amodel%20for%20object%20segmentation%20in%20both%20images%20and%20videos%2C%20paving%20the%20way%20for%0Avarious%20downstream%20video%20applications.%20The%20crucial%20design%20of%20SAM%202%20for%20video%0Asegmentation%20is%20its%20memory%20module%2C%20which%20prompts%20object-aware%20memories%20from%0Aprevious%20frames%20for%20current%20frame%20prediction.%20However%2C%20its%20greedy-selection%0Amemory%20design%20suffers%20from%20the%20%22error%20accumulation%22%20problem%2C%20where%20an%20errored%0Aor%20missed%20mask%20will%20cascade%20and%20influence%20the%20segmentation%20of%20the%20subsequent%0Aframes%2C%20which%20limits%20the%20performance%20of%20SAM%202%20toward%20complex%20long-term%20videos.%0ATo%20this%20end%2C%20we%20introduce%20SAM2Long%2C%20an%20improved%20training-free%20video%20object%0Asegmentation%20strategy%2C%20which%20considers%20the%20segmentation%20uncertainty%20within%20each%0Aframe%20and%20chooses%20the%20video-level%20optimal%20results%20from%20multiple%20segmentation%0Apathways%20in%20a%20constrained%20tree%20search%20manner.%20In%20practice%2C%20we%20maintain%20a%20fixed%0Anumber%20of%20segmentation%20pathways%20throughout%20the%20video.%20For%20each%20frame%2C%20multiple%0Amasks%20are%20proposed%20based%20on%20the%20existing%20pathways%2C%20creating%20various%20candidate%0Abranches.%20We%20then%20select%20the%20same%20fixed%20number%20of%20branches%20with%20higher%0Acumulative%20scores%20as%20the%20new%20pathways%20for%20the%20next%20frame.%20After%20processing%20the%0Afinal%20frame%2C%20the%20pathway%20with%20the%20highest%20cumulative%20score%20is%20chosen%20as%20the%0Afinal%20segmentation%20result.%20Benefiting%20from%20its%20heuristic%20search%20design%2C%0ASAM2Long%20is%20robust%20toward%20occlusions%20and%20object%20reappearances%2C%20and%20can%0Aeffectively%20segment%20and%20track%20objects%20for%20complex%20long-term%20videos.%20Notably%2C%0ASAM2Long%20achieves%20an%20average%20improvement%20of%203.0%20points%20across%20all%2024%0Ahead-to-head%20comparisons%2C%20with%20gains%20of%20up%20to%205.3%20points%20in%20J%26F%20on%20long-term%0Avideo%20object%20segmentation%20benchmarks%20such%20as%20SA-V%20and%20LVOS.%20The%20code%20is%0Areleased%20at%20https%3A//github.com/Mark12Ding/SAM2Long.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM2Long%253A%2520Enhancing%2520SAM%25202%2520for%2520Long%2520Video%2520Segmentation%2520with%2520a%250A%2520%2520Training-Free%2520Memory%2520Tree%26entry.906535625%3DShuangrui%2520Ding%2520and%2520Rui%2520Qian%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Yuwei%2520Guo%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%25202%2520%2528SAM%25202%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520foundation%250Amodel%2520for%2520object%2520segmentation%2520in%2520both%2520images%2520and%2520videos%252C%2520paving%2520the%2520way%2520for%250Avarious%2520downstream%2520video%2520applications.%2520The%2520crucial%2520design%2520of%2520SAM%25202%2520for%2520video%250Asegmentation%2520is%2520its%2520memory%2520module%252C%2520which%2520prompts%2520object-aware%2520memories%2520from%250Aprevious%2520frames%2520for%2520current%2520frame%2520prediction.%2520However%252C%2520its%2520greedy-selection%250Amemory%2520design%2520suffers%2520from%2520the%2520%2522error%2520accumulation%2522%2520problem%252C%2520where%2520an%2520errored%250Aor%2520missed%2520mask%2520will%2520cascade%2520and%2520influence%2520the%2520segmentation%2520of%2520the%2520subsequent%250Aframes%252C%2520which%2520limits%2520the%2520performance%2520of%2520SAM%25202%2520toward%2520complex%2520long-term%2520videos.%250ATo%2520this%2520end%252C%2520we%2520introduce%2520SAM2Long%252C%2520an%2520improved%2520training-free%2520video%2520object%250Asegmentation%2520strategy%252C%2520which%2520considers%2520the%2520segmentation%2520uncertainty%2520within%2520each%250Aframe%2520and%2520chooses%2520the%2520video-level%2520optimal%2520results%2520from%2520multiple%2520segmentation%250Apathways%2520in%2520a%2520constrained%2520tree%2520search%2520manner.%2520In%2520practice%252C%2520we%2520maintain%2520a%2520fixed%250Anumber%2520of%2520segmentation%2520pathways%2520throughout%2520the%2520video.%2520For%2520each%2520frame%252C%2520multiple%250Amasks%2520are%2520proposed%2520based%2520on%2520the%2520existing%2520pathways%252C%2520creating%2520various%2520candidate%250Abranches.%2520We%2520then%2520select%2520the%2520same%2520fixed%2520number%2520of%2520branches%2520with%2520higher%250Acumulative%2520scores%2520as%2520the%2520new%2520pathways%2520for%2520the%2520next%2520frame.%2520After%2520processing%2520the%250Afinal%2520frame%252C%2520the%2520pathway%2520with%2520the%2520highest%2520cumulative%2520score%2520is%2520chosen%2520as%2520the%250Afinal%2520segmentation%2520result.%2520Benefiting%2520from%2520its%2520heuristic%2520search%2520design%252C%250ASAM2Long%2520is%2520robust%2520toward%2520occlusions%2520and%2520object%2520reappearances%252C%2520and%2520can%250Aeffectively%2520segment%2520and%2520track%2520objects%2520for%2520complex%2520long-term%2520videos.%2520Notably%252C%250ASAM2Long%2520achieves%2520an%2520average%2520improvement%2520of%25203.0%2520points%2520across%2520all%252024%250Ahead-to-head%2520comparisons%252C%2520with%2520gains%2520of%2520up%2520to%25205.3%2520points%2520in%2520J%2526F%2520on%2520long-term%250Avideo%2520object%2520segmentation%2520benchmarks%2520such%2520as%2520SA-V%2520and%2520LVOS.%2520The%2520code%2520is%250Areleased%2520at%2520https%253A//github.com/Mark12Ding/SAM2Long.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM2Long%3A%20Enhancing%20SAM%202%20for%20Long%20Video%20Segmentation%20with%20a%0A%20%20Training-Free%20Memory%20Tree&entry.906535625=Shuangrui%20Ding%20and%20Rui%20Qian%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Yuwei%20Guo%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20The%20Segment%20Anything%20Model%202%20%28SAM%202%29%20has%20emerged%20as%20a%20powerful%20foundation%0Amodel%20for%20object%20segmentation%20in%20both%20images%20and%20videos%2C%20paving%20the%20way%20for%0Avarious%20downstream%20video%20applications.%20The%20crucial%20design%20of%20SAM%202%20for%20video%0Asegmentation%20is%20its%20memory%20module%2C%20which%20prompts%20object-aware%20memories%20from%0Aprevious%20frames%20for%20current%20frame%20prediction.%20However%2C%20its%20greedy-selection%0Amemory%20design%20suffers%20from%20the%20%22error%20accumulation%22%20problem%2C%20where%20an%20errored%0Aor%20missed%20mask%20will%20cascade%20and%20influence%20the%20segmentation%20of%20the%20subsequent%0Aframes%2C%20which%20limits%20the%20performance%20of%20SAM%202%20toward%20complex%20long-term%20videos.%0ATo%20this%20end%2C%20we%20introduce%20SAM2Long%2C%20an%20improved%20training-free%20video%20object%0Asegmentation%20strategy%2C%20which%20considers%20the%20segmentation%20uncertainty%20within%20each%0Aframe%20and%20chooses%20the%20video-level%20optimal%20results%20from%20multiple%20segmentation%0Apathways%20in%20a%20constrained%20tree%20search%20manner.%20In%20practice%2C%20we%20maintain%20a%20fixed%0Anumber%20of%20segmentation%20pathways%20throughout%20the%20video.%20For%20each%20frame%2C%20multiple%0Amasks%20are%20proposed%20based%20on%20the%20existing%20pathways%2C%20creating%20various%20candidate%0Abranches.%20We%20then%20select%20the%20same%20fixed%20number%20of%20branches%20with%20higher%0Acumulative%20scores%20as%20the%20new%20pathways%20for%20the%20next%20frame.%20After%20processing%20the%0Afinal%20frame%2C%20the%20pathway%20with%20the%20highest%20cumulative%20score%20is%20chosen%20as%20the%0Afinal%20segmentation%20result.%20Benefiting%20from%20its%20heuristic%20search%20design%2C%0ASAM2Long%20is%20robust%20toward%20occlusions%20and%20object%20reappearances%2C%20and%20can%0Aeffectively%20segment%20and%20track%20objects%20for%20complex%20long-term%20videos.%20Notably%2C%0ASAM2Long%20achieves%20an%20average%20improvement%20of%203.0%20points%20across%20all%2024%0Ahead-to-head%20comparisons%2C%20with%20gains%20of%20up%20to%205.3%20points%20in%20J%26F%20on%20long-term%0Avideo%20object%20segmentation%20benchmarks%20such%20as%20SA-V%20and%20LVOS.%20The%20code%20is%0Areleased%20at%20https%3A//github.com/Mark12Ding/SAM2Long.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16268v2&entry.124074799=Read"},
{"title": "3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and\n  High-quality Medical Image Generation", "author": "Haoshen Wang and Zhentao Liu and Kaicong Sun and Xiaodong Wang and Dinggang Shen and Zhiming Cui", "abstract": "  The generation of medical images presents significant challenges due to their\nhigh-resolution and three-dimensional nature. Existing methods often yield\nsuboptimal performance in generating high-quality 3D medical images, and there\nis currently no universal generative framework for medical imaging. In this\npaper, we introduce the 3D Medical Diffusion (3D MedDiffusion) model for\ncontrollable, high-quality 3D medical image generation. 3D MedDiffusion\nincorporates a novel, highly efficient Patch-Volume Autoencoder that compresses\nmedical images into latent space through patch-wise encoding and recovers back\ninto image space through volume-wise decoding. Additionally, we design a new\nnoise estimator to capture both local details and global structure information\nduring diffusion denoising process. 3D MedDiffusion can generate fine-detailed,\nhigh-resolution images (up to 512x512x512) and effectively adapt to various\ndownstream tasks as it is trained on large-scale datasets covering CT and MRI\nmodalities and different anatomical regions (from head to leg). Experimental\nresults demonstrate that 3D MedDiffusion surpasses state-of-the-art methods in\ngenerative quality and exhibits strong generalizability across tasks such as\nsparse-view CT reconstruction, fast MRI reconstruction, and data augmentation.\n", "link": "http://arxiv.org/abs/2412.13059v1", "date": "2024-12-17", "relevancy": 2.6759, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6696}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6696}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20MedDiffusion%3A%20A%203D%20Medical%20Diffusion%20Model%20for%20Controllable%20and%0A%20%20High-quality%20Medical%20Image%20Generation&body=Title%3A%203D%20MedDiffusion%3A%20A%203D%20Medical%20Diffusion%20Model%20for%20Controllable%20and%0A%20%20High-quality%20Medical%20Image%20Generation%0AAuthor%3A%20Haoshen%20Wang%20and%20Zhentao%20Liu%20and%20Kaicong%20Sun%20and%20Xiaodong%20Wang%20and%20Dinggang%20Shen%20and%20Zhiming%20Cui%0AAbstract%3A%20%20%20The%20generation%20of%20medical%20images%20presents%20significant%20challenges%20due%20to%20their%0Ahigh-resolution%20and%20three-dimensional%20nature.%20Existing%20methods%20often%20yield%0Asuboptimal%20performance%20in%20generating%20high-quality%203D%20medical%20images%2C%20and%20there%0Ais%20currently%20no%20universal%20generative%20framework%20for%20medical%20imaging.%20In%20this%0Apaper%2C%20we%20introduce%20the%203D%20Medical%20Diffusion%20%283D%20MedDiffusion%29%20model%20for%0Acontrollable%2C%20high-quality%203D%20medical%20image%20generation.%203D%20MedDiffusion%0Aincorporates%20a%20novel%2C%20highly%20efficient%20Patch-Volume%20Autoencoder%20that%20compresses%0Amedical%20images%20into%20latent%20space%20through%20patch-wise%20encoding%20and%20recovers%20back%0Ainto%20image%20space%20through%20volume-wise%20decoding.%20Additionally%2C%20we%20design%20a%20new%0Anoise%20estimator%20to%20capture%20both%20local%20details%20and%20global%20structure%20information%0Aduring%20diffusion%20denoising%20process.%203D%20MedDiffusion%20can%20generate%20fine-detailed%2C%0Ahigh-resolution%20images%20%28up%20to%20512x512x512%29%20and%20effectively%20adapt%20to%20various%0Adownstream%20tasks%20as%20it%20is%20trained%20on%20large-scale%20datasets%20covering%20CT%20and%20MRI%0Amodalities%20and%20different%20anatomical%20regions%20%28from%20head%20to%20leg%29.%20Experimental%0Aresults%20demonstrate%20that%203D%20MedDiffusion%20surpasses%20state-of-the-art%20methods%20in%0Agenerative%20quality%20and%20exhibits%20strong%20generalizability%20across%20tasks%20such%20as%0Asparse-view%20CT%20reconstruction%2C%20fast%20MRI%20reconstruction%2C%20and%20data%20augmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520MedDiffusion%253A%2520A%25203D%2520Medical%2520Diffusion%2520Model%2520for%2520Controllable%2520and%250A%2520%2520High-quality%2520Medical%2520Image%2520Generation%26entry.906535625%3DHaoshen%2520Wang%2520and%2520Zhentao%2520Liu%2520and%2520Kaicong%2520Sun%2520and%2520Xiaodong%2520Wang%2520and%2520Dinggang%2520Shen%2520and%2520Zhiming%2520Cui%26entry.1292438233%3D%2520%2520The%2520generation%2520of%2520medical%2520images%2520presents%2520significant%2520challenges%2520due%2520to%2520their%250Ahigh-resolution%2520and%2520three-dimensional%2520nature.%2520Existing%2520methods%2520often%2520yield%250Asuboptimal%2520performance%2520in%2520generating%2520high-quality%25203D%2520medical%2520images%252C%2520and%2520there%250Ais%2520currently%2520no%2520universal%2520generative%2520framework%2520for%2520medical%2520imaging.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520the%25203D%2520Medical%2520Diffusion%2520%25283D%2520MedDiffusion%2529%2520model%2520for%250Acontrollable%252C%2520high-quality%25203D%2520medical%2520image%2520generation.%25203D%2520MedDiffusion%250Aincorporates%2520a%2520novel%252C%2520highly%2520efficient%2520Patch-Volume%2520Autoencoder%2520that%2520compresses%250Amedical%2520images%2520into%2520latent%2520space%2520through%2520patch-wise%2520encoding%2520and%2520recovers%2520back%250Ainto%2520image%2520space%2520through%2520volume-wise%2520decoding.%2520Additionally%252C%2520we%2520design%2520a%2520new%250Anoise%2520estimator%2520to%2520capture%2520both%2520local%2520details%2520and%2520global%2520structure%2520information%250Aduring%2520diffusion%2520denoising%2520process.%25203D%2520MedDiffusion%2520can%2520generate%2520fine-detailed%252C%250Ahigh-resolution%2520images%2520%2528up%2520to%2520512x512x512%2529%2520and%2520effectively%2520adapt%2520to%2520various%250Adownstream%2520tasks%2520as%2520it%2520is%2520trained%2520on%2520large-scale%2520datasets%2520covering%2520CT%2520and%2520MRI%250Amodalities%2520and%2520different%2520anatomical%2520regions%2520%2528from%2520head%2520to%2520leg%2529.%2520Experimental%250Aresults%2520demonstrate%2520that%25203D%2520MedDiffusion%2520surpasses%2520state-of-the-art%2520methods%2520in%250Agenerative%2520quality%2520and%2520exhibits%2520strong%2520generalizability%2520across%2520tasks%2520such%2520as%250Asparse-view%2520CT%2520reconstruction%252C%2520fast%2520MRI%2520reconstruction%252C%2520and%2520data%2520augmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20MedDiffusion%3A%20A%203D%20Medical%20Diffusion%20Model%20for%20Controllable%20and%0A%20%20High-quality%20Medical%20Image%20Generation&entry.906535625=Haoshen%20Wang%20and%20Zhentao%20Liu%20and%20Kaicong%20Sun%20and%20Xiaodong%20Wang%20and%20Dinggang%20Shen%20and%20Zhiming%20Cui&entry.1292438233=%20%20The%20generation%20of%20medical%20images%20presents%20significant%20challenges%20due%20to%20their%0Ahigh-resolution%20and%20three-dimensional%20nature.%20Existing%20methods%20often%20yield%0Asuboptimal%20performance%20in%20generating%20high-quality%203D%20medical%20images%2C%20and%20there%0Ais%20currently%20no%20universal%20generative%20framework%20for%20medical%20imaging.%20In%20this%0Apaper%2C%20we%20introduce%20the%203D%20Medical%20Diffusion%20%283D%20MedDiffusion%29%20model%20for%0Acontrollable%2C%20high-quality%203D%20medical%20image%20generation.%203D%20MedDiffusion%0Aincorporates%20a%20novel%2C%20highly%20efficient%20Patch-Volume%20Autoencoder%20that%20compresses%0Amedical%20images%20into%20latent%20space%20through%20patch-wise%20encoding%20and%20recovers%20back%0Ainto%20image%20space%20through%20volume-wise%20decoding.%20Additionally%2C%20we%20design%20a%20new%0Anoise%20estimator%20to%20capture%20both%20local%20details%20and%20global%20structure%20information%0Aduring%20diffusion%20denoising%20process.%203D%20MedDiffusion%20can%20generate%20fine-detailed%2C%0Ahigh-resolution%20images%20%28up%20to%20512x512x512%29%20and%20effectively%20adapt%20to%20various%0Adownstream%20tasks%20as%20it%20is%20trained%20on%20large-scale%20datasets%20covering%20CT%20and%20MRI%0Amodalities%20and%20different%20anatomical%20regions%20%28from%20head%20to%20leg%29.%20Experimental%0Aresults%20demonstrate%20that%203D%20MedDiffusion%20surpasses%20state-of-the-art%20methods%20in%0Agenerative%20quality%20and%20exhibits%20strong%20generalizability%20across%20tasks%20such%20as%0Asparse-view%20CT%20reconstruction%2C%20fast%20MRI%20reconstruction%2C%20and%20data%20augmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13059v1&entry.124074799=Read"},
{"title": "NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation", "author": "Karan Wanchoo and Xiaoye Zuo and Hannah Gonzalez and Soham Dan and Georgios Georgakis and Dan Roth and Kostas Daniilidis and Eleni Miltsakaki", "abstract": "  We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.\n", "link": "http://arxiv.org/abs/2412.13026v1", "date": "2024-12-17", "relevancy": 2.6164, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NAVCON%3A%20A%20Cognitively%20Inspired%20and%20Linguistically%20Grounded%20Corpus%20for%0A%20%20Vision%20and%20Language%20Navigation&body=Title%3A%20NAVCON%3A%20A%20Cognitively%20Inspired%20and%20Linguistically%20Grounded%20Corpus%20for%0A%20%20Vision%20and%20Language%20Navigation%0AAuthor%3A%20Karan%20Wanchoo%20and%20Xiaoye%20Zuo%20and%20Hannah%20Gonzalez%20and%20Soham%20Dan%20and%20Georgios%20Georgakis%20and%20Dan%20Roth%20and%20Kostas%20Daniilidis%20and%20Eleni%20Miltsakaki%0AAbstract%3A%20%20%20We%20present%20NAVCON%2C%20a%20large-scale%20annotated%20Vision-Language%20Navigation%20%28VLN%29%0Acorpus%20built%20on%20top%20of%20two%20popular%20datasets%20%28R2R%20and%20RxR%29.%20The%20paper%20introduces%0Afour%20core%2C%20cognitively%20motivated%20and%20linguistically%20grounded%2C%20navigation%0Aconcepts%20and%20an%20algorithm%20for%20generating%20large-scale%20silver%20annotations%20of%0Anaturally%20occurring%20linguistic%20realizations%20of%20these%20concepts%20in%20navigation%0Ainstructions.%20We%20pair%20the%20annotated%20instructions%20with%20video%20clips%20of%20an%20agent%0Aacting%20on%20these%20instructions.%20NAVCON%20contains%20236%2C%20316%20concept%20annotations%20for%0Aapproximately%2030%2C%200000%20instructions%20and%202.7%20million%20aligned%20images%20%28from%0Aapproximately%2019%2C%20000%20instructions%29%20showing%20what%20the%20agent%20sees%20when%20executing%0Aan%20instruction.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20comprehensive%20resource%20of%0Anavigation%20concepts.%20We%20evaluated%20the%20quality%20of%20the%20silver%20annotations%20by%0Aconducting%20human%20evaluation%20studies%20on%20NAVCON%20samples.%20As%20further%20validation%20of%0Athe%20quality%20and%20usefulness%20of%20the%20resource%2C%20we%20trained%20a%20model%20for%20detecting%0Anavigation%20concepts%20and%20their%20linguistic%20realizations%20in%20unseen%20instructions.%0AAdditionally%2C%20we%20show%20that%20few-shot%20learning%20with%20GPT-4o%20performs%20well%20on%20this%0Atask%20using%20large-scale%20silver%20annotations%20of%20NAVCON.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNAVCON%253A%2520A%2520Cognitively%2520Inspired%2520and%2520Linguistically%2520Grounded%2520Corpus%2520for%250A%2520%2520Vision%2520and%2520Language%2520Navigation%26entry.906535625%3DKaran%2520Wanchoo%2520and%2520Xiaoye%2520Zuo%2520and%2520Hannah%2520Gonzalez%2520and%2520Soham%2520Dan%2520and%2520Georgios%2520Georgakis%2520and%2520Dan%2520Roth%2520and%2520Kostas%2520Daniilidis%2520and%2520Eleni%2520Miltsakaki%26entry.1292438233%3D%2520%2520We%2520present%2520NAVCON%252C%2520a%2520large-scale%2520annotated%2520Vision-Language%2520Navigation%2520%2528VLN%2529%250Acorpus%2520built%2520on%2520top%2520of%2520two%2520popular%2520datasets%2520%2528R2R%2520and%2520RxR%2529.%2520The%2520paper%2520introduces%250Afour%2520core%252C%2520cognitively%2520motivated%2520and%2520linguistically%2520grounded%252C%2520navigation%250Aconcepts%2520and%2520an%2520algorithm%2520for%2520generating%2520large-scale%2520silver%2520annotations%2520of%250Anaturally%2520occurring%2520linguistic%2520realizations%2520of%2520these%2520concepts%2520in%2520navigation%250Ainstructions.%2520We%2520pair%2520the%2520annotated%2520instructions%2520with%2520video%2520clips%2520of%2520an%2520agent%250Aacting%2520on%2520these%2520instructions.%2520NAVCON%2520contains%2520236%252C%2520316%2520concept%2520annotations%2520for%250Aapproximately%252030%252C%25200000%2520instructions%2520and%25202.7%2520million%2520aligned%2520images%2520%2528from%250Aapproximately%252019%252C%2520000%2520instructions%2529%2520showing%2520what%2520the%2520agent%2520sees%2520when%2520executing%250Aan%2520instruction.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520comprehensive%2520resource%2520of%250Anavigation%2520concepts.%2520We%2520evaluated%2520the%2520quality%2520of%2520the%2520silver%2520annotations%2520by%250Aconducting%2520human%2520evaluation%2520studies%2520on%2520NAVCON%2520samples.%2520As%2520further%2520validation%2520of%250Athe%2520quality%2520and%2520usefulness%2520of%2520the%2520resource%252C%2520we%2520trained%2520a%2520model%2520for%2520detecting%250Anavigation%2520concepts%2520and%2520their%2520linguistic%2520realizations%2520in%2520unseen%2520instructions.%250AAdditionally%252C%2520we%2520show%2520that%2520few-shot%2520learning%2520with%2520GPT-4o%2520performs%2520well%2520on%2520this%250Atask%2520using%2520large-scale%2520silver%2520annotations%2520of%2520NAVCON.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NAVCON%3A%20A%20Cognitively%20Inspired%20and%20Linguistically%20Grounded%20Corpus%20for%0A%20%20Vision%20and%20Language%20Navigation&entry.906535625=Karan%20Wanchoo%20and%20Xiaoye%20Zuo%20and%20Hannah%20Gonzalez%20and%20Soham%20Dan%20and%20Georgios%20Georgakis%20and%20Dan%20Roth%20and%20Kostas%20Daniilidis%20and%20Eleni%20Miltsakaki&entry.1292438233=%20%20We%20present%20NAVCON%2C%20a%20large-scale%20annotated%20Vision-Language%20Navigation%20%28VLN%29%0Acorpus%20built%20on%20top%20of%20two%20popular%20datasets%20%28R2R%20and%20RxR%29.%20The%20paper%20introduces%0Afour%20core%2C%20cognitively%20motivated%20and%20linguistically%20grounded%2C%20navigation%0Aconcepts%20and%20an%20algorithm%20for%20generating%20large-scale%20silver%20annotations%20of%0Anaturally%20occurring%20linguistic%20realizations%20of%20these%20concepts%20in%20navigation%0Ainstructions.%20We%20pair%20the%20annotated%20instructions%20with%20video%20clips%20of%20an%20agent%0Aacting%20on%20these%20instructions.%20NAVCON%20contains%20236%2C%20316%20concept%20annotations%20for%0Aapproximately%2030%2C%200000%20instructions%20and%202.7%20million%20aligned%20images%20%28from%0Aapproximately%2019%2C%20000%20instructions%29%20showing%20what%20the%20agent%20sees%20when%20executing%0Aan%20instruction.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20comprehensive%20resource%20of%0Anavigation%20concepts.%20We%20evaluated%20the%20quality%20of%20the%20silver%20annotations%20by%0Aconducting%20human%20evaluation%20studies%20on%20NAVCON%20samples.%20As%20further%20validation%20of%0Athe%20quality%20and%20usefulness%20of%20the%20resource%2C%20we%20trained%20a%20model%20for%20detecting%0Anavigation%20concepts%20and%20their%20linguistic%20realizations%20in%20unseen%20instructions.%0AAdditionally%2C%20we%20show%20that%20few-shot%20learning%20with%20GPT-4o%20performs%20well%20on%20this%0Atask%20using%20large-scale%20silver%20annotations%20of%20NAVCON.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13026v1&entry.124074799=Read"},
{"title": "AnyAttack: Targeted Adversarial Attacks on Vision-Language Models toward\n  Any Images", "author": "Jiaming Zhang and Junhong Ye and Xingjun Ma and Yige Li and Yunfan Yang and Jitao Sang and Dit-Yan Yeung", "abstract": "  Due to their multimodal capabilities, Vision-Language Models (VLMs) have\nfound numerous impactful applications in real-world scenarios. However, recent\nstudies have revealed that VLMs are vulnerable to image-based adversarial\nattacks, particularly targeted adversarial images that manipulate the model to\ngenerate harmful content specified by the adversary. Current attack methods\nrely on predefined target labels to create targeted adversarial attacks, which\nlimits their scalability and applicability for large-scale robustness\nevaluations. In this paper, we propose AnyAttack, a self-supervised framework\nthat generates targeted adversarial images for VLMs without label supervision,\nallowing any image to serve as a target for the attack. Our framework employs\nthe pre-training and fine-tuning paradigm, with the adversarial noise generator\npre-trained on the large-scale LAION-400M dataset. This large-scale\npre-training endows our method with powerful transferability across a wide\nrange of VLMs. Extensive experiments on five mainstream open-source VLMs (CLIP,\nBLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks\n(image-text retrieval, multimodal classification, and image captioning)\ndemonstrate the effectiveness of our attack. Additionally, we successfully\ntransfer AnyAttack to multiple commercial VLMs, including Google Gemini, Claude\nSonnet, Microsoft Copilot and OpenAI GPT. These results reveal an unprecedented\nrisk to VLMs, highlighting the need for effective countermeasures.\n", "link": "http://arxiv.org/abs/2410.05346v2", "date": "2024-12-17", "relevancy": 2.5991, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5521}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5062}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyAttack%3A%20Targeted%20Adversarial%20Attacks%20on%20Vision-Language%20Models%20toward%0A%20%20Any%20Images&body=Title%3A%20AnyAttack%3A%20Targeted%20Adversarial%20Attacks%20on%20Vision-Language%20Models%20toward%0A%20%20Any%20Images%0AAuthor%3A%20Jiaming%20Zhang%20and%20Junhong%20Ye%20and%20Xingjun%20Ma%20and%20Yige%20Li%20and%20Yunfan%20Yang%20and%20Jitao%20Sang%20and%20Dit-Yan%20Yeung%0AAbstract%3A%20%20%20Due%20to%20their%20multimodal%20capabilities%2C%20Vision-Language%20Models%20%28VLMs%29%20have%0Afound%20numerous%20impactful%20applications%20in%20real-world%20scenarios.%20However%2C%20recent%0Astudies%20have%20revealed%20that%20VLMs%20are%20vulnerable%20to%20image-based%20adversarial%0Aattacks%2C%20particularly%20targeted%20adversarial%20images%20that%20manipulate%20the%20model%20to%0Agenerate%20harmful%20content%20specified%20by%20the%20adversary.%20Current%20attack%20methods%0Arely%20on%20predefined%20target%20labels%20to%20create%20targeted%20adversarial%20attacks%2C%20which%0Alimits%20their%20scalability%20and%20applicability%20for%20large-scale%20robustness%0Aevaluations.%20In%20this%20paper%2C%20we%20propose%20AnyAttack%2C%20a%20self-supervised%20framework%0Athat%20generates%20targeted%20adversarial%20images%20for%20VLMs%20without%20label%20supervision%2C%0Aallowing%20any%20image%20to%20serve%20as%20a%20target%20for%20the%20attack.%20Our%20framework%20employs%0Athe%20pre-training%20and%20fine-tuning%20paradigm%2C%20with%20the%20adversarial%20noise%20generator%0Apre-trained%20on%20the%20large-scale%20LAION-400M%20dataset.%20This%20large-scale%0Apre-training%20endows%20our%20method%20with%20powerful%20transferability%20across%20a%20wide%0Arange%20of%20VLMs.%20Extensive%20experiments%20on%20five%20mainstream%20open-source%20VLMs%20%28CLIP%2C%0ABLIP%2C%20BLIP2%2C%20InstructBLIP%2C%20and%20MiniGPT-4%29%20across%20three%20multimodal%20tasks%0A%28image-text%20retrieval%2C%20multimodal%20classification%2C%20and%20image%20captioning%29%0Ademonstrate%20the%20effectiveness%20of%20our%20attack.%20Additionally%2C%20we%20successfully%0Atransfer%20AnyAttack%20to%20multiple%20commercial%20VLMs%2C%20including%20Google%20Gemini%2C%20Claude%0ASonnet%2C%20Microsoft%20Copilot%20and%20OpenAI%20GPT.%20These%20results%20reveal%20an%20unprecedented%0Arisk%20to%20VLMs%2C%20highlighting%20the%20need%20for%20effective%20countermeasures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyAttack%253A%2520Targeted%2520Adversarial%2520Attacks%2520on%2520Vision-Language%2520Models%2520toward%250A%2520%2520Any%2520Images%26entry.906535625%3DJiaming%2520Zhang%2520and%2520Junhong%2520Ye%2520and%2520Xingjun%2520Ma%2520and%2520Yige%2520Li%2520and%2520Yunfan%2520Yang%2520and%2520Jitao%2520Sang%2520and%2520Dit-Yan%2520Yeung%26entry.1292438233%3D%2520%2520Due%2520to%2520their%2520multimodal%2520capabilities%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%250Afound%2520numerous%2520impactful%2520applications%2520in%2520real-world%2520scenarios.%2520However%252C%2520recent%250Astudies%2520have%2520revealed%2520that%2520VLMs%2520are%2520vulnerable%2520to%2520image-based%2520adversarial%250Aattacks%252C%2520particularly%2520targeted%2520adversarial%2520images%2520that%2520manipulate%2520the%2520model%2520to%250Agenerate%2520harmful%2520content%2520specified%2520by%2520the%2520adversary.%2520Current%2520attack%2520methods%250Arely%2520on%2520predefined%2520target%2520labels%2520to%2520create%2520targeted%2520adversarial%2520attacks%252C%2520which%250Alimits%2520their%2520scalability%2520and%2520applicability%2520for%2520large-scale%2520robustness%250Aevaluations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520AnyAttack%252C%2520a%2520self-supervised%2520framework%250Athat%2520generates%2520targeted%2520adversarial%2520images%2520for%2520VLMs%2520without%2520label%2520supervision%252C%250Aallowing%2520any%2520image%2520to%2520serve%2520as%2520a%2520target%2520for%2520the%2520attack.%2520Our%2520framework%2520employs%250Athe%2520pre-training%2520and%2520fine-tuning%2520paradigm%252C%2520with%2520the%2520adversarial%2520noise%2520generator%250Apre-trained%2520on%2520the%2520large-scale%2520LAION-400M%2520dataset.%2520This%2520large-scale%250Apre-training%2520endows%2520our%2520method%2520with%2520powerful%2520transferability%2520across%2520a%2520wide%250Arange%2520of%2520VLMs.%2520Extensive%2520experiments%2520on%2520five%2520mainstream%2520open-source%2520VLMs%2520%2528CLIP%252C%250ABLIP%252C%2520BLIP2%252C%2520InstructBLIP%252C%2520and%2520MiniGPT-4%2529%2520across%2520three%2520multimodal%2520tasks%250A%2528image-text%2520retrieval%252C%2520multimodal%2520classification%252C%2520and%2520image%2520captioning%2529%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520attack.%2520Additionally%252C%2520we%2520successfully%250Atransfer%2520AnyAttack%2520to%2520multiple%2520commercial%2520VLMs%252C%2520including%2520Google%2520Gemini%252C%2520Claude%250ASonnet%252C%2520Microsoft%2520Copilot%2520and%2520OpenAI%2520GPT.%2520These%2520results%2520reveal%2520an%2520unprecedented%250Arisk%2520to%2520VLMs%252C%2520highlighting%2520the%2520need%2520for%2520effective%2520countermeasures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyAttack%3A%20Targeted%20Adversarial%20Attacks%20on%20Vision-Language%20Models%20toward%0A%20%20Any%20Images&entry.906535625=Jiaming%20Zhang%20and%20Junhong%20Ye%20and%20Xingjun%20Ma%20and%20Yige%20Li%20and%20Yunfan%20Yang%20and%20Jitao%20Sang%20and%20Dit-Yan%20Yeung&entry.1292438233=%20%20Due%20to%20their%20multimodal%20capabilities%2C%20Vision-Language%20Models%20%28VLMs%29%20have%0Afound%20numerous%20impactful%20applications%20in%20real-world%20scenarios.%20However%2C%20recent%0Astudies%20have%20revealed%20that%20VLMs%20are%20vulnerable%20to%20image-based%20adversarial%0Aattacks%2C%20particularly%20targeted%20adversarial%20images%20that%20manipulate%20the%20model%20to%0Agenerate%20harmful%20content%20specified%20by%20the%20adversary.%20Current%20attack%20methods%0Arely%20on%20predefined%20target%20labels%20to%20create%20targeted%20adversarial%20attacks%2C%20which%0Alimits%20their%20scalability%20and%20applicability%20for%20large-scale%20robustness%0Aevaluations.%20In%20this%20paper%2C%20we%20propose%20AnyAttack%2C%20a%20self-supervised%20framework%0Athat%20generates%20targeted%20adversarial%20images%20for%20VLMs%20without%20label%20supervision%2C%0Aallowing%20any%20image%20to%20serve%20as%20a%20target%20for%20the%20attack.%20Our%20framework%20employs%0Athe%20pre-training%20and%20fine-tuning%20paradigm%2C%20with%20the%20adversarial%20noise%20generator%0Apre-trained%20on%20the%20large-scale%20LAION-400M%20dataset.%20This%20large-scale%0Apre-training%20endows%20our%20method%20with%20powerful%20transferability%20across%20a%20wide%0Arange%20of%20VLMs.%20Extensive%20experiments%20on%20five%20mainstream%20open-source%20VLMs%20%28CLIP%2C%0ABLIP%2C%20BLIP2%2C%20InstructBLIP%2C%20and%20MiniGPT-4%29%20across%20three%20multimodal%20tasks%0A%28image-text%20retrieval%2C%20multimodal%20classification%2C%20and%20image%20captioning%29%0Ademonstrate%20the%20effectiveness%20of%20our%20attack.%20Additionally%2C%20we%20successfully%0Atransfer%20AnyAttack%20to%20multiple%20commercial%20VLMs%2C%20including%20Google%20Gemini%2C%20Claude%0ASonnet%2C%20Microsoft%20Copilot%20and%20OpenAI%20GPT.%20These%20results%20reveal%20an%20unprecedented%0Arisk%20to%20VLMs%2C%20highlighting%20the%20need%20for%20effective%20countermeasures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05346v2&entry.124074799=Read"},
{"title": "A Knowledge-enhanced Pathology Vision-language Foundation Model for\n  Cancer Diagnosis", "author": "Xiao Zhou and Luoyi Sun and Dexuan He and Wenbin Guan and Ruifen Wang and Lifeng Wang and Xin Sun and Kun Sun and Ya Zhang and Yanfeng Wang and Weidi Xie", "abstract": "  Deep learning has enabled the development of highly robust foundation models\nfor various pathological tasks across diverse diseases and patient cohorts.\nAmong these models, vision-language pre-training, which leverages large-scale\npaired data to align pathology image and text embedding spaces, and provides a\nnovel zero-shot paradigm for downstream tasks. However, existing models have\nbeen primarily data-driven and lack the incorporation of domain-specific\nknowledge, which limits their performance in cancer diagnosis, especially for\nrare tumor subtypes. To address this limitation, we establish a\nKnowledge-enhanced Pathology (KEEP) foundation model that harnesses disease\nknowledge to facilitate vision-language pre-training. Specifically, we first\nconstruct a disease knowledge graph (KG) that covers 11,454 human diseases with\n139,143 disease attributes, including synonyms, definitions, and hypernym\nrelations. We then systematically reorganize the millions of publicly available\nnoisy pathology image-text pairs, into 143K well-structured semantic groups\nlinked through the hierarchical relations of the disease KG. To derive more\nnuanced image and text representations, we propose a novel knowledge-enhanced\nvision-language pre-training approach that integrates disease knowledge into\nthe alignment within hierarchical semantic groups instead of unstructured\nimage-text pairs. Validated on 18 diverse benchmarks with more than 14,000\nwhole slide images (WSIs), KEEP achieves state-of-the-art performance in\nzero-shot cancer diagnostic tasks. Notably, for cancer detection, KEEP\ndemonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7\ncancer types. For cancer subtyping, KEEP achieves a median balanced accuracy of\n0.456 in subtyping 30 rare brain cancers, indicating strong generalizability\nfor diagnosing rare tumors.\n", "link": "http://arxiv.org/abs/2412.13126v1", "date": "2024-12-17", "relevancy": 2.5873, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Knowledge-enhanced%20Pathology%20Vision-language%20Foundation%20Model%20for%0A%20%20Cancer%20Diagnosis&body=Title%3A%20A%20Knowledge-enhanced%20Pathology%20Vision-language%20Foundation%20Model%20for%0A%20%20Cancer%20Diagnosis%0AAuthor%3A%20Xiao%20Zhou%20and%20Luoyi%20Sun%20and%20Dexuan%20He%20and%20Wenbin%20Guan%20and%20Ruifen%20Wang%20and%20Lifeng%20Wang%20and%20Xin%20Sun%20and%20Kun%20Sun%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%0AAbstract%3A%20%20%20Deep%20learning%20has%20enabled%20the%20development%20of%20highly%20robust%20foundation%20models%0Afor%20various%20pathological%20tasks%20across%20diverse%20diseases%20and%20patient%20cohorts.%0AAmong%20these%20models%2C%20vision-language%20pre-training%2C%20which%20leverages%20large-scale%0Apaired%20data%20to%20align%20pathology%20image%20and%20text%20embedding%20spaces%2C%20and%20provides%20a%0Anovel%20zero-shot%20paradigm%20for%20downstream%20tasks.%20However%2C%20existing%20models%20have%0Abeen%20primarily%20data-driven%20and%20lack%20the%20incorporation%20of%20domain-specific%0Aknowledge%2C%20which%20limits%20their%20performance%20in%20cancer%20diagnosis%2C%20especially%20for%0Arare%20tumor%20subtypes.%20To%20address%20this%20limitation%2C%20we%20establish%20a%0AKnowledge-enhanced%20Pathology%20%28KEEP%29%20foundation%20model%20that%20harnesses%20disease%0Aknowledge%20to%20facilitate%20vision-language%20pre-training.%20Specifically%2C%20we%20first%0Aconstruct%20a%20disease%20knowledge%20graph%20%28KG%29%20that%20covers%2011%2C454%20human%20diseases%20with%0A139%2C143%20disease%20attributes%2C%20including%20synonyms%2C%20definitions%2C%20and%20hypernym%0Arelations.%20We%20then%20systematically%20reorganize%20the%20millions%20of%20publicly%20available%0Anoisy%20pathology%20image-text%20pairs%2C%20into%20143K%20well-structured%20semantic%20groups%0Alinked%20through%20the%20hierarchical%20relations%20of%20the%20disease%20KG.%20To%20derive%20more%0Anuanced%20image%20and%20text%20representations%2C%20we%20propose%20a%20novel%20knowledge-enhanced%0Avision-language%20pre-training%20approach%20that%20integrates%20disease%20knowledge%20into%0Athe%20alignment%20within%20hierarchical%20semantic%20groups%20instead%20of%20unstructured%0Aimage-text%20pairs.%20Validated%20on%2018%20diverse%20benchmarks%20with%20more%20than%2014%2C000%0Awhole%20slide%20images%20%28WSIs%29%2C%20KEEP%20achieves%20state-of-the-art%20performance%20in%0Azero-shot%20cancer%20diagnostic%20tasks.%20Notably%2C%20for%20cancer%20detection%2C%20KEEP%0Ademonstrates%20an%20average%20sensitivity%20of%2089.8%25%20at%20a%20specificity%20of%2095.0%25%20across%207%0Acancer%20types.%20For%20cancer%20subtyping%2C%20KEEP%20achieves%20a%20median%20balanced%20accuracy%20of%0A0.456%20in%20subtyping%2030%20rare%20brain%20cancers%2C%20indicating%20strong%20generalizability%0Afor%20diagnosing%20rare%20tumors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Knowledge-enhanced%2520Pathology%2520Vision-language%2520Foundation%2520Model%2520for%250A%2520%2520Cancer%2520Diagnosis%26entry.906535625%3DXiao%2520Zhou%2520and%2520Luoyi%2520Sun%2520and%2520Dexuan%2520He%2520and%2520Wenbin%2520Guan%2520and%2520Ruifen%2520Wang%2520and%2520Lifeng%2520Wang%2520and%2520Xin%2520Sun%2520and%2520Kun%2520Sun%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%2520and%2520Weidi%2520Xie%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520enabled%2520the%2520development%2520of%2520highly%2520robust%2520foundation%2520models%250Afor%2520various%2520pathological%2520tasks%2520across%2520diverse%2520diseases%2520and%2520patient%2520cohorts.%250AAmong%2520these%2520models%252C%2520vision-language%2520pre-training%252C%2520which%2520leverages%2520large-scale%250Apaired%2520data%2520to%2520align%2520pathology%2520image%2520and%2520text%2520embedding%2520spaces%252C%2520and%2520provides%2520a%250Anovel%2520zero-shot%2520paradigm%2520for%2520downstream%2520tasks.%2520However%252C%2520existing%2520models%2520have%250Abeen%2520primarily%2520data-driven%2520and%2520lack%2520the%2520incorporation%2520of%2520domain-specific%250Aknowledge%252C%2520which%2520limits%2520their%2520performance%2520in%2520cancer%2520diagnosis%252C%2520especially%2520for%250Arare%2520tumor%2520subtypes.%2520To%2520address%2520this%2520limitation%252C%2520we%2520establish%2520a%250AKnowledge-enhanced%2520Pathology%2520%2528KEEP%2529%2520foundation%2520model%2520that%2520harnesses%2520disease%250Aknowledge%2520to%2520facilitate%2520vision-language%2520pre-training.%2520Specifically%252C%2520we%2520first%250Aconstruct%2520a%2520disease%2520knowledge%2520graph%2520%2528KG%2529%2520that%2520covers%252011%252C454%2520human%2520diseases%2520with%250A139%252C143%2520disease%2520attributes%252C%2520including%2520synonyms%252C%2520definitions%252C%2520and%2520hypernym%250Arelations.%2520We%2520then%2520systematically%2520reorganize%2520the%2520millions%2520of%2520publicly%2520available%250Anoisy%2520pathology%2520image-text%2520pairs%252C%2520into%2520143K%2520well-structured%2520semantic%2520groups%250Alinked%2520through%2520the%2520hierarchical%2520relations%2520of%2520the%2520disease%2520KG.%2520To%2520derive%2520more%250Anuanced%2520image%2520and%2520text%2520representations%252C%2520we%2520propose%2520a%2520novel%2520knowledge-enhanced%250Avision-language%2520pre-training%2520approach%2520that%2520integrates%2520disease%2520knowledge%2520into%250Athe%2520alignment%2520within%2520hierarchical%2520semantic%2520groups%2520instead%2520of%2520unstructured%250Aimage-text%2520pairs.%2520Validated%2520on%252018%2520diverse%2520benchmarks%2520with%2520more%2520than%252014%252C000%250Awhole%2520slide%2520images%2520%2528WSIs%2529%252C%2520KEEP%2520achieves%2520state-of-the-art%2520performance%2520in%250Azero-shot%2520cancer%2520diagnostic%2520tasks.%2520Notably%252C%2520for%2520cancer%2520detection%252C%2520KEEP%250Ademonstrates%2520an%2520average%2520sensitivity%2520of%252089.8%2525%2520at%2520a%2520specificity%2520of%252095.0%2525%2520across%25207%250Acancer%2520types.%2520For%2520cancer%2520subtyping%252C%2520KEEP%2520achieves%2520a%2520median%2520balanced%2520accuracy%2520of%250A0.456%2520in%2520subtyping%252030%2520rare%2520brain%2520cancers%252C%2520indicating%2520strong%2520generalizability%250Afor%2520diagnosing%2520rare%2520tumors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Knowledge-enhanced%20Pathology%20Vision-language%20Foundation%20Model%20for%0A%20%20Cancer%20Diagnosis&entry.906535625=Xiao%20Zhou%20and%20Luoyi%20Sun%20and%20Dexuan%20He%20and%20Wenbin%20Guan%20and%20Ruifen%20Wang%20and%20Lifeng%20Wang%20and%20Xin%20Sun%20and%20Kun%20Sun%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie&entry.1292438233=%20%20Deep%20learning%20has%20enabled%20the%20development%20of%20highly%20robust%20foundation%20models%0Afor%20various%20pathological%20tasks%20across%20diverse%20diseases%20and%20patient%20cohorts.%0AAmong%20these%20models%2C%20vision-language%20pre-training%2C%20which%20leverages%20large-scale%0Apaired%20data%20to%20align%20pathology%20image%20and%20text%20embedding%20spaces%2C%20and%20provides%20a%0Anovel%20zero-shot%20paradigm%20for%20downstream%20tasks.%20However%2C%20existing%20models%20have%0Abeen%20primarily%20data-driven%20and%20lack%20the%20incorporation%20of%20domain-specific%0Aknowledge%2C%20which%20limits%20their%20performance%20in%20cancer%20diagnosis%2C%20especially%20for%0Arare%20tumor%20subtypes.%20To%20address%20this%20limitation%2C%20we%20establish%20a%0AKnowledge-enhanced%20Pathology%20%28KEEP%29%20foundation%20model%20that%20harnesses%20disease%0Aknowledge%20to%20facilitate%20vision-language%20pre-training.%20Specifically%2C%20we%20first%0Aconstruct%20a%20disease%20knowledge%20graph%20%28KG%29%20that%20covers%2011%2C454%20human%20diseases%20with%0A139%2C143%20disease%20attributes%2C%20including%20synonyms%2C%20definitions%2C%20and%20hypernym%0Arelations.%20We%20then%20systematically%20reorganize%20the%20millions%20of%20publicly%20available%0Anoisy%20pathology%20image-text%20pairs%2C%20into%20143K%20well-structured%20semantic%20groups%0Alinked%20through%20the%20hierarchical%20relations%20of%20the%20disease%20KG.%20To%20derive%20more%0Anuanced%20image%20and%20text%20representations%2C%20we%20propose%20a%20novel%20knowledge-enhanced%0Avision-language%20pre-training%20approach%20that%20integrates%20disease%20knowledge%20into%0Athe%20alignment%20within%20hierarchical%20semantic%20groups%20instead%20of%20unstructured%0Aimage-text%20pairs.%20Validated%20on%2018%20diverse%20benchmarks%20with%20more%20than%2014%2C000%0Awhole%20slide%20images%20%28WSIs%29%2C%20KEEP%20achieves%20state-of-the-art%20performance%20in%0Azero-shot%20cancer%20diagnostic%20tasks.%20Notably%2C%20for%20cancer%20detection%2C%20KEEP%0Ademonstrates%20an%20average%20sensitivity%20of%2089.8%25%20at%20a%20specificity%20of%2095.0%25%20across%207%0Acancer%20types.%20For%20cancer%20subtyping%2C%20KEEP%20achieves%20a%20median%20balanced%20accuracy%20of%0A0.456%20in%20subtyping%2030%20rare%20brain%20cancers%2C%20indicating%20strong%20generalizability%0Afor%20diagnosing%20rare%20tumors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13126v1&entry.124074799=Read"},
{"title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization", "author": "Tian Qin and Naomi Saphra and David Alvarez-Melis", "abstract": "  Language models (LMs), like other neural networks, often favor shortcut\nheuristics based on surface-level patterns. Although LMs behave like n-gram\nmodels early in training, they must eventually learn hierarchical syntactic\nrepresentations to correctly apply grammatical rules out-of-distribution (OOD).\nIn this work, we use case studies of English grammar to explore how complex,\ndiverse training data drives models to generalize OOD. We construct a framework\nthat unifies our understanding of random variation with training dynamics, rule\nselection with memorization, and data diversity with complexity. We show that\nthese factors are nuanced, and that intermediate levels of diversity and\ncomplexity lead to inconsistent behavior across random seeds and to unstable\ntraining dynamics. Our findings emphasize the critical role of training data in\nshaping generalization patterns and illuminate how competing model strategies\nlead to inconsistent generalization outcomes across random seeds. Code is\navailable at https://github.com/sunnytqin/concept_comp.git.\n", "link": "http://arxiv.org/abs/2412.04619v2", "date": "2024-12-17", "relevancy": 2.535, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sometimes%20I%20am%20a%20Tree%3A%20Data%20Drives%20Unstable%20Hierarchical%20Generalization&body=Title%3A%20Sometimes%20I%20am%20a%20Tree%3A%20Data%20Drives%20Unstable%20Hierarchical%20Generalization%0AAuthor%3A%20Tian%20Qin%20and%20Naomi%20Saphra%20and%20David%20Alvarez-Melis%0AAbstract%3A%20%20%20Language%20models%20%28LMs%29%2C%20like%20other%20neural%20networks%2C%20often%20favor%20shortcut%0Aheuristics%20based%20on%20surface-level%20patterns.%20Although%20LMs%20behave%20like%20n-gram%0Amodels%20early%20in%20training%2C%20they%20must%20eventually%20learn%20hierarchical%20syntactic%0Arepresentations%20to%20correctly%20apply%20grammatical%20rules%20out-of-distribution%20%28OOD%29.%0AIn%20this%20work%2C%20we%20use%20case%20studies%20of%20English%20grammar%20to%20explore%20how%20complex%2C%0Adiverse%20training%20data%20drives%20models%20to%20generalize%20OOD.%20We%20construct%20a%20framework%0Athat%20unifies%20our%20understanding%20of%20random%20variation%20with%20training%20dynamics%2C%20rule%0Aselection%20with%20memorization%2C%20and%20data%20diversity%20with%20complexity.%20We%20show%20that%0Athese%20factors%20are%20nuanced%2C%20and%20that%20intermediate%20levels%20of%20diversity%20and%0Acomplexity%20lead%20to%20inconsistent%20behavior%20across%20random%20seeds%20and%20to%20unstable%0Atraining%20dynamics.%20Our%20findings%20emphasize%20the%20critical%20role%20of%20training%20data%20in%0Ashaping%20generalization%20patterns%20and%20illuminate%20how%20competing%20model%20strategies%0Alead%20to%20inconsistent%20generalization%20outcomes%20across%20random%20seeds.%20Code%20is%0Aavailable%20at%20https%3A//github.com/sunnytqin/concept_comp.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04619v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSometimes%2520I%2520am%2520a%2520Tree%253A%2520Data%2520Drives%2520Unstable%2520Hierarchical%2520Generalization%26entry.906535625%3DTian%2520Qin%2520and%2520Naomi%2520Saphra%2520and%2520David%2520Alvarez-Melis%26entry.1292438233%3D%2520%2520Language%2520models%2520%2528LMs%2529%252C%2520like%2520other%2520neural%2520networks%252C%2520often%2520favor%2520shortcut%250Aheuristics%2520based%2520on%2520surface-level%2520patterns.%2520Although%2520LMs%2520behave%2520like%2520n-gram%250Amodels%2520early%2520in%2520training%252C%2520they%2520must%2520eventually%2520learn%2520hierarchical%2520syntactic%250Arepresentations%2520to%2520correctly%2520apply%2520grammatical%2520rules%2520out-of-distribution%2520%2528OOD%2529.%250AIn%2520this%2520work%252C%2520we%2520use%2520case%2520studies%2520of%2520English%2520grammar%2520to%2520explore%2520how%2520complex%252C%250Adiverse%2520training%2520data%2520drives%2520models%2520to%2520generalize%2520OOD.%2520We%2520construct%2520a%2520framework%250Athat%2520unifies%2520our%2520understanding%2520of%2520random%2520variation%2520with%2520training%2520dynamics%252C%2520rule%250Aselection%2520with%2520memorization%252C%2520and%2520data%2520diversity%2520with%2520complexity.%2520We%2520show%2520that%250Athese%2520factors%2520are%2520nuanced%252C%2520and%2520that%2520intermediate%2520levels%2520of%2520diversity%2520and%250Acomplexity%2520lead%2520to%2520inconsistent%2520behavior%2520across%2520random%2520seeds%2520and%2520to%2520unstable%250Atraining%2520dynamics.%2520Our%2520findings%2520emphasize%2520the%2520critical%2520role%2520of%2520training%2520data%2520in%250Ashaping%2520generalization%2520patterns%2520and%2520illuminate%2520how%2520competing%2520model%2520strategies%250Alead%2520to%2520inconsistent%2520generalization%2520outcomes%2520across%2520random%2520seeds.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/sunnytqin/concept_comp.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04619v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sometimes%20I%20am%20a%20Tree%3A%20Data%20Drives%20Unstable%20Hierarchical%20Generalization&entry.906535625=Tian%20Qin%20and%20Naomi%20Saphra%20and%20David%20Alvarez-Melis&entry.1292438233=%20%20Language%20models%20%28LMs%29%2C%20like%20other%20neural%20networks%2C%20often%20favor%20shortcut%0Aheuristics%20based%20on%20surface-level%20patterns.%20Although%20LMs%20behave%20like%20n-gram%0Amodels%20early%20in%20training%2C%20they%20must%20eventually%20learn%20hierarchical%20syntactic%0Arepresentations%20to%20correctly%20apply%20grammatical%20rules%20out-of-distribution%20%28OOD%29.%0AIn%20this%20work%2C%20we%20use%20case%20studies%20of%20English%20grammar%20to%20explore%20how%20complex%2C%0Adiverse%20training%20data%20drives%20models%20to%20generalize%20OOD.%20We%20construct%20a%20framework%0Athat%20unifies%20our%20understanding%20of%20random%20variation%20with%20training%20dynamics%2C%20rule%0Aselection%20with%20memorization%2C%20and%20data%20diversity%20with%20complexity.%20We%20show%20that%0Athese%20factors%20are%20nuanced%2C%20and%20that%20intermediate%20levels%20of%20diversity%20and%0Acomplexity%20lead%20to%20inconsistent%20behavior%20across%20random%20seeds%20and%20to%20unstable%0Atraining%20dynamics.%20Our%20findings%20emphasize%20the%20critical%20role%20of%20training%20data%20in%0Ashaping%20generalization%20patterns%20and%20illuminate%20how%20competing%20model%20strategies%0Alead%20to%20inconsistent%20generalization%20outcomes%20across%20random%20seeds.%20Code%20is%0Aavailable%20at%20https%3A//github.com/sunnytqin/concept_comp.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04619v2&entry.124074799=Read"},
{"title": "StreetCrafter: Street View Synthesis with Controllable Video Diffusion\n  Models", "author": "Yunzhi Yan and Zhen Xu and Haotong Lin and Haian Jin and Haoyu Guo and Yida Wang and Kun Zhan and Xianpeng Lang and Hujun Bao and Xiaowei Zhou and Sida Peng", "abstract": "  This paper aims to tackle the problem of photorealistic view synthesis from\nvehicle sensor data. Recent advancements in neural scene representation have\nachieved notable success in rendering high-quality autonomous driving scenes,\nbut the performance significantly degrades as the viewpoint deviates from the\ntraining trajectory. To mitigate this problem, we introduce StreetCrafter, a\nnovel controllable video diffusion model that utilizes LiDAR point cloud\nrenderings as pixel-level conditions, which fully exploits the generative prior\nfor novel view synthesis, while preserving precise camera control. Moreover,\nthe utilization of pixel-level LiDAR conditions allows us to make accurate\npixel-level edits to target scenes. In addition, the generative prior of\nStreetCrafter can be effectively incorporated into dynamic scene\nrepresentations to achieve real-time rendering. Experiments on Waymo Open\nDataset and PandaSet demonstrate that our model enables flexible control over\nviewpoint changes, enlarging the view synthesis regions for satisfying\nrendering, which outperforms existing methods.\n", "link": "http://arxiv.org/abs/2412.13188v1", "date": "2024-12-17", "relevancy": 2.515, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.638}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.638}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreetCrafter%3A%20Street%20View%20Synthesis%20with%20Controllable%20Video%20Diffusion%0A%20%20Models&body=Title%3A%20StreetCrafter%3A%20Street%20View%20Synthesis%20with%20Controllable%20Video%20Diffusion%0A%20%20Models%0AAuthor%3A%20Yunzhi%20Yan%20and%20Zhen%20Xu%20and%20Haotong%20Lin%20and%20Haian%20Jin%20and%20Haoyu%20Guo%20and%20Yida%20Wang%20and%20Kun%20Zhan%20and%20Xianpeng%20Lang%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou%20and%20Sida%20Peng%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20tackle%20the%20problem%20of%20photorealistic%20view%20synthesis%20from%0Avehicle%20sensor%20data.%20Recent%20advancements%20in%20neural%20scene%20representation%20have%0Aachieved%20notable%20success%20in%20rendering%20high-quality%20autonomous%20driving%20scenes%2C%0Abut%20the%20performance%20significantly%20degrades%20as%20the%20viewpoint%20deviates%20from%20the%0Atraining%20trajectory.%20To%20mitigate%20this%20problem%2C%20we%20introduce%20StreetCrafter%2C%20a%0Anovel%20controllable%20video%20diffusion%20model%20that%20utilizes%20LiDAR%20point%20cloud%0Arenderings%20as%20pixel-level%20conditions%2C%20which%20fully%20exploits%20the%20generative%20prior%0Afor%20novel%20view%20synthesis%2C%20while%20preserving%20precise%20camera%20control.%20Moreover%2C%0Athe%20utilization%20of%20pixel-level%20LiDAR%20conditions%20allows%20us%20to%20make%20accurate%0Apixel-level%20edits%20to%20target%20scenes.%20In%20addition%2C%20the%20generative%20prior%20of%0AStreetCrafter%20can%20be%20effectively%20incorporated%20into%20dynamic%20scene%0Arepresentations%20to%20achieve%20real-time%20rendering.%20Experiments%20on%20Waymo%20Open%0ADataset%20and%20PandaSet%20demonstrate%20that%20our%20model%20enables%20flexible%20control%20over%0Aviewpoint%20changes%2C%20enlarging%20the%20view%20synthesis%20regions%20for%20satisfying%0Arendering%2C%20which%20outperforms%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreetCrafter%253A%2520Street%2520View%2520Synthesis%2520with%2520Controllable%2520Video%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DYunzhi%2520Yan%2520and%2520Zhen%2520Xu%2520and%2520Haotong%2520Lin%2520and%2520Haian%2520Jin%2520and%2520Haoyu%2520Guo%2520and%2520Yida%2520Wang%2520and%2520Kun%2520Zhan%2520and%2520Xianpeng%2520Lang%2520and%2520Hujun%2520Bao%2520and%2520Xiaowei%2520Zhou%2520and%2520Sida%2520Peng%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520tackle%2520the%2520problem%2520of%2520photorealistic%2520view%2520synthesis%2520from%250Avehicle%2520sensor%2520data.%2520Recent%2520advancements%2520in%2520neural%2520scene%2520representation%2520have%250Aachieved%2520notable%2520success%2520in%2520rendering%2520high-quality%2520autonomous%2520driving%2520scenes%252C%250Abut%2520the%2520performance%2520significantly%2520degrades%2520as%2520the%2520viewpoint%2520deviates%2520from%2520the%250Atraining%2520trajectory.%2520To%2520mitigate%2520this%2520problem%252C%2520we%2520introduce%2520StreetCrafter%252C%2520a%250Anovel%2520controllable%2520video%2520diffusion%2520model%2520that%2520utilizes%2520LiDAR%2520point%2520cloud%250Arenderings%2520as%2520pixel-level%2520conditions%252C%2520which%2520fully%2520exploits%2520the%2520generative%2520prior%250Afor%2520novel%2520view%2520synthesis%252C%2520while%2520preserving%2520precise%2520camera%2520control.%2520Moreover%252C%250Athe%2520utilization%2520of%2520pixel-level%2520LiDAR%2520conditions%2520allows%2520us%2520to%2520make%2520accurate%250Apixel-level%2520edits%2520to%2520target%2520scenes.%2520In%2520addition%252C%2520the%2520generative%2520prior%2520of%250AStreetCrafter%2520can%2520be%2520effectively%2520incorporated%2520into%2520dynamic%2520scene%250Arepresentations%2520to%2520achieve%2520real-time%2520rendering.%2520Experiments%2520on%2520Waymo%2520Open%250ADataset%2520and%2520PandaSet%2520demonstrate%2520that%2520our%2520model%2520enables%2520flexible%2520control%2520over%250Aviewpoint%2520changes%252C%2520enlarging%2520the%2520view%2520synthesis%2520regions%2520for%2520satisfying%250Arendering%252C%2520which%2520outperforms%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreetCrafter%3A%20Street%20View%20Synthesis%20with%20Controllable%20Video%20Diffusion%0A%20%20Models&entry.906535625=Yunzhi%20Yan%20and%20Zhen%20Xu%20and%20Haotong%20Lin%20and%20Haian%20Jin%20and%20Haoyu%20Guo%20and%20Yida%20Wang%20and%20Kun%20Zhan%20and%20Xianpeng%20Lang%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou%20and%20Sida%20Peng&entry.1292438233=%20%20This%20paper%20aims%20to%20tackle%20the%20problem%20of%20photorealistic%20view%20synthesis%20from%0Avehicle%20sensor%20data.%20Recent%20advancements%20in%20neural%20scene%20representation%20have%0Aachieved%20notable%20success%20in%20rendering%20high-quality%20autonomous%20driving%20scenes%2C%0Abut%20the%20performance%20significantly%20degrades%20as%20the%20viewpoint%20deviates%20from%20the%0Atraining%20trajectory.%20To%20mitigate%20this%20problem%2C%20we%20introduce%20StreetCrafter%2C%20a%0Anovel%20controllable%20video%20diffusion%20model%20that%20utilizes%20LiDAR%20point%20cloud%0Arenderings%20as%20pixel-level%20conditions%2C%20which%20fully%20exploits%20the%20generative%20prior%0Afor%20novel%20view%20synthesis%2C%20while%20preserving%20precise%20camera%20control.%20Moreover%2C%0Athe%20utilization%20of%20pixel-level%20LiDAR%20conditions%20allows%20us%20to%20make%20accurate%0Apixel-level%20edits%20to%20target%20scenes.%20In%20addition%2C%20the%20generative%20prior%20of%0AStreetCrafter%20can%20be%20effectively%20incorporated%20into%20dynamic%20scene%0Arepresentations%20to%20achieve%20real-time%20rendering.%20Experiments%20on%20Waymo%20Open%0ADataset%20and%20PandaSet%20demonstrate%20that%20our%20model%20enables%20flexible%20control%20over%0Aviewpoint%20changes%2C%20enlarging%20the%20view%20synthesis%20regions%20for%20satisfying%0Arendering%2C%20which%20outperforms%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13188v1&entry.124074799=Read"},
{"title": "Move-in-2D: 2D-Conditioned Human Motion Generation", "author": "Hsin-Ping Huang and Yang Zhou and Jui-Hsien Wang and Difan Liu and Feng Liu and Ming-Hsuan Yang and Zhan Xu", "abstract": "  Generating realistic human videos remains a challenging task, with the most\neffective methods currently relying on a human motion sequence as a control\nsignal. Existing approaches often use existing motion extracted from other\nvideos, which restricts applications to specific motion types and global scene\nmatching. We propose Move-in-2D, a novel approach to generate human motion\nsequences conditioned on a scene image, allowing for diverse motion that adapts\nto different scenes. Our approach utilizes a diffusion model that accepts both\na scene image and text prompt as inputs, producing a motion sequence tailored\nto the scene. To train this model, we collect a large-scale video dataset\nfeaturing single-human activities, annotating each video with the corresponding\nhuman motion as the target output. Experiments demonstrate that our method\neffectively predicts human motion that aligns with the scene image after\nprojection. Furthermore, we show that the generated motion sequence improves\nhuman motion quality in video synthesis tasks.\n", "link": "http://arxiv.org/abs/2412.13185v1", "date": "2024-12-17", "relevancy": 2.5121, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6608}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6282}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Move-in-2D%3A%202D-Conditioned%20Human%20Motion%20Generation&body=Title%3A%20Move-in-2D%3A%202D-Conditioned%20Human%20Motion%20Generation%0AAuthor%3A%20Hsin-Ping%20Huang%20and%20Yang%20Zhou%20and%20Jui-Hsien%20Wang%20and%20Difan%20Liu%20and%20Feng%20Liu%20and%20Ming-Hsuan%20Yang%20and%20Zhan%20Xu%0AAbstract%3A%20%20%20Generating%20realistic%20human%20videos%20remains%20a%20challenging%20task%2C%20with%20the%20most%0Aeffective%20methods%20currently%20relying%20on%20a%20human%20motion%20sequence%20as%20a%20control%0Asignal.%20Existing%20approaches%20often%20use%20existing%20motion%20extracted%20from%20other%0Avideos%2C%20which%20restricts%20applications%20to%20specific%20motion%20types%20and%20global%20scene%0Amatching.%20We%20propose%20Move-in-2D%2C%20a%20novel%20approach%20to%20generate%20human%20motion%0Asequences%20conditioned%20on%20a%20scene%20image%2C%20allowing%20for%20diverse%20motion%20that%20adapts%0Ato%20different%20scenes.%20Our%20approach%20utilizes%20a%20diffusion%20model%20that%20accepts%20both%0Aa%20scene%20image%20and%20text%20prompt%20as%20inputs%2C%20producing%20a%20motion%20sequence%20tailored%0Ato%20the%20scene.%20To%20train%20this%20model%2C%20we%20collect%20a%20large-scale%20video%20dataset%0Afeaturing%20single-human%20activities%2C%20annotating%20each%20video%20with%20the%20corresponding%0Ahuman%20motion%20as%20the%20target%20output.%20Experiments%20demonstrate%20that%20our%20method%0Aeffectively%20predicts%20human%20motion%20that%20aligns%20with%20the%20scene%20image%20after%0Aprojection.%20Furthermore%2C%20we%20show%20that%20the%20generated%20motion%20sequence%20improves%0Ahuman%20motion%20quality%20in%20video%20synthesis%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMove-in-2D%253A%25202D-Conditioned%2520Human%2520Motion%2520Generation%26entry.906535625%3DHsin-Ping%2520Huang%2520and%2520Yang%2520Zhou%2520and%2520Jui-Hsien%2520Wang%2520and%2520Difan%2520Liu%2520and%2520Feng%2520Liu%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Zhan%2520Xu%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520human%2520videos%2520remains%2520a%2520challenging%2520task%252C%2520with%2520the%2520most%250Aeffective%2520methods%2520currently%2520relying%2520on%2520a%2520human%2520motion%2520sequence%2520as%2520a%2520control%250Asignal.%2520Existing%2520approaches%2520often%2520use%2520existing%2520motion%2520extracted%2520from%2520other%250Avideos%252C%2520which%2520restricts%2520applications%2520to%2520specific%2520motion%2520types%2520and%2520global%2520scene%250Amatching.%2520We%2520propose%2520Move-in-2D%252C%2520a%2520novel%2520approach%2520to%2520generate%2520human%2520motion%250Asequences%2520conditioned%2520on%2520a%2520scene%2520image%252C%2520allowing%2520for%2520diverse%2520motion%2520that%2520adapts%250Ato%2520different%2520scenes.%2520Our%2520approach%2520utilizes%2520a%2520diffusion%2520model%2520that%2520accepts%2520both%250Aa%2520scene%2520image%2520and%2520text%2520prompt%2520as%2520inputs%252C%2520producing%2520a%2520motion%2520sequence%2520tailored%250Ato%2520the%2520scene.%2520To%2520train%2520this%2520model%252C%2520we%2520collect%2520a%2520large-scale%2520video%2520dataset%250Afeaturing%2520single-human%2520activities%252C%2520annotating%2520each%2520video%2520with%2520the%2520corresponding%250Ahuman%2520motion%2520as%2520the%2520target%2520output.%2520Experiments%2520demonstrate%2520that%2520our%2520method%250Aeffectively%2520predicts%2520human%2520motion%2520that%2520aligns%2520with%2520the%2520scene%2520image%2520after%250Aprojection.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520generated%2520motion%2520sequence%2520improves%250Ahuman%2520motion%2520quality%2520in%2520video%2520synthesis%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Move-in-2D%3A%202D-Conditioned%20Human%20Motion%20Generation&entry.906535625=Hsin-Ping%20Huang%20and%20Yang%20Zhou%20and%20Jui-Hsien%20Wang%20and%20Difan%20Liu%20and%20Feng%20Liu%20and%20Ming-Hsuan%20Yang%20and%20Zhan%20Xu&entry.1292438233=%20%20Generating%20realistic%20human%20videos%20remains%20a%20challenging%20task%2C%20with%20the%20most%0Aeffective%20methods%20currently%20relying%20on%20a%20human%20motion%20sequence%20as%20a%20control%0Asignal.%20Existing%20approaches%20often%20use%20existing%20motion%20extracted%20from%20other%0Avideos%2C%20which%20restricts%20applications%20to%20specific%20motion%20types%20and%20global%20scene%0Amatching.%20We%20propose%20Move-in-2D%2C%20a%20novel%20approach%20to%20generate%20human%20motion%0Asequences%20conditioned%20on%20a%20scene%20image%2C%20allowing%20for%20diverse%20motion%20that%20adapts%0Ato%20different%20scenes.%20Our%20approach%20utilizes%20a%20diffusion%20model%20that%20accepts%20both%0Aa%20scene%20image%20and%20text%20prompt%20as%20inputs%2C%20producing%20a%20motion%20sequence%20tailored%0Ato%20the%20scene.%20To%20train%20this%20model%2C%20we%20collect%20a%20large-scale%20video%20dataset%0Afeaturing%20single-human%20activities%2C%20annotating%20each%20video%20with%20the%20corresponding%0Ahuman%20motion%20as%20the%20target%20output.%20Experiments%20demonstrate%20that%20our%20method%0Aeffectively%20predicts%20human%20motion%20that%20aligns%20with%20the%20scene%20image%20after%0Aprojection.%20Furthermore%2C%20we%20show%20that%20the%20generated%20motion%20sequence%20improves%0Ahuman%20motion%20quality%20in%20video%20synthesis%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13185v1&entry.124074799=Read"},
{"title": "Reinforcement Learning Enhanced LLMs: A Survey", "author": "Shuhe Wang and Shengyu Zhang and Jie Zhang and Runyi Hu and Xiaoya Li and Tianwei Zhang and Jiwei Li and Fei Wu and Guoyin Wang and Eduard Hovy", "abstract": "  This paper surveys research in the rapidly growing field of enhancing large\nlanguage models (LLMs) with reinforcement learning (RL), a technique that\nenables LLMs to improve their performance by receiving feedback in the form of\nrewards based on the quality of their outputs, allowing them to generate more\naccurate, coherent, and contextually appropriate responses. In this work, we\nmake a systematic review of the most up-to-date state of knowledge on\nRL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing\nresearch in this field, helping researchers understand the current challenges\nand advancements. Specifically, we (1) detail the basics of RL; (2) introduce\npopular RL-enhanced LLMs; (3) review researches on two widely-used reward\nmodel-based RL techniques: Reinforcement Learning from Human Feedback (RLHF)\nand Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct\nPreference Optimization (DPO), a set of methods that bypass the reward model to\ndirectly use human preference data for aligning LLM outputs with human\nexpectations. We will also point out current challenges and deficiencies of\nexisting methods and suggest some avenues for further improvements. Project\npage of this work can be found at:\n\\url{https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey}.\n", "link": "http://arxiv.org/abs/2412.10400v2", "date": "2024-12-17", "relevancy": 2.5068, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20Enhanced%20LLMs%3A%20A%20Survey&body=Title%3A%20Reinforcement%20Learning%20Enhanced%20LLMs%3A%20A%20Survey%0AAuthor%3A%20Shuhe%20Wang%20and%20Shengyu%20Zhang%20and%20Jie%20Zhang%20and%20Runyi%20Hu%20and%20Xiaoya%20Li%20and%20Tianwei%20Zhang%20and%20Jiwei%20Li%20and%20Fei%20Wu%20and%20Guoyin%20Wang%20and%20Eduard%20Hovy%0AAbstract%3A%20%20%20This%20paper%20surveys%20research%20in%20the%20rapidly%20growing%20field%20of%20enhancing%20large%0Alanguage%20models%20%28LLMs%29%20with%20reinforcement%20learning%20%28RL%29%2C%20a%20technique%20that%0Aenables%20LLMs%20to%20improve%20their%20performance%20by%20receiving%20feedback%20in%20the%20form%20of%0Arewards%20based%20on%20the%20quality%20of%20their%20outputs%2C%20allowing%20them%20to%20generate%20more%0Aaccurate%2C%20coherent%2C%20and%20contextually%20appropriate%20responses.%20In%20this%20work%2C%20we%0Amake%20a%20systematic%20review%20of%20the%20most%20up-to-date%20state%20of%20knowledge%20on%0ARL-enhanced%20LLMs%2C%20attempting%20to%20consolidate%20and%20analyze%20the%20rapidly%20growing%0Aresearch%20in%20this%20field%2C%20helping%20researchers%20understand%20the%20current%20challenges%0Aand%20advancements.%20Specifically%2C%20we%20%281%29%20detail%20the%20basics%20of%20RL%3B%20%282%29%20introduce%0Apopular%20RL-enhanced%20LLMs%3B%20%283%29%20review%20researches%20on%20two%20widely-used%20reward%0Amodel-based%20RL%20techniques%3A%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%0Aand%20Reinforcement%20Learning%20from%20AI%20Feedback%20%28RLAIF%29%3B%20and%20%284%29%20explore%20Direct%0APreference%20Optimization%20%28DPO%29%2C%20a%20set%20of%20methods%20that%20bypass%20the%20reward%20model%20to%0Adirectly%20use%20human%20preference%20data%20for%20aligning%20LLM%20outputs%20with%20human%0Aexpectations.%20We%20will%20also%20point%20out%20current%20challenges%20and%20deficiencies%20of%0Aexisting%20methods%20and%20suggest%20some%20avenues%20for%20further%20improvements.%20Project%0Apage%20of%20this%20work%20can%20be%20found%20at%3A%0A%5Curl%7Bhttps%3A//github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10400v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520Enhanced%2520LLMs%253A%2520A%2520Survey%26entry.906535625%3DShuhe%2520Wang%2520and%2520Shengyu%2520Zhang%2520and%2520Jie%2520Zhang%2520and%2520Runyi%2520Hu%2520and%2520Xiaoya%2520Li%2520and%2520Tianwei%2520Zhang%2520and%2520Jiwei%2520Li%2520and%2520Fei%2520Wu%2520and%2520Guoyin%2520Wang%2520and%2520Eduard%2520Hovy%26entry.1292438233%3D%2520%2520This%2520paper%2520surveys%2520research%2520in%2520the%2520rapidly%2520growing%2520field%2520of%2520enhancing%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520with%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520a%2520technique%2520that%250Aenables%2520LLMs%2520to%2520improve%2520their%2520performance%2520by%2520receiving%2520feedback%2520in%2520the%2520form%2520of%250Arewards%2520based%2520on%2520the%2520quality%2520of%2520their%2520outputs%252C%2520allowing%2520them%2520to%2520generate%2520more%250Aaccurate%252C%2520coherent%252C%2520and%2520contextually%2520appropriate%2520responses.%2520In%2520this%2520work%252C%2520we%250Amake%2520a%2520systematic%2520review%2520of%2520the%2520most%2520up-to-date%2520state%2520of%2520knowledge%2520on%250ARL-enhanced%2520LLMs%252C%2520attempting%2520to%2520consolidate%2520and%2520analyze%2520the%2520rapidly%2520growing%250Aresearch%2520in%2520this%2520field%252C%2520helping%2520researchers%2520understand%2520the%2520current%2520challenges%250Aand%2520advancements.%2520Specifically%252C%2520we%2520%25281%2529%2520detail%2520the%2520basics%2520of%2520RL%253B%2520%25282%2529%2520introduce%250Apopular%2520RL-enhanced%2520LLMs%253B%2520%25283%2529%2520review%2520researches%2520on%2520two%2520widely-used%2520reward%250Amodel-based%2520RL%2520techniques%253A%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%250Aand%2520Reinforcement%2520Learning%2520from%2520AI%2520Feedback%2520%2528RLAIF%2529%253B%2520and%2520%25284%2529%2520explore%2520Direct%250APreference%2520Optimization%2520%2528DPO%2529%252C%2520a%2520set%2520of%2520methods%2520that%2520bypass%2520the%2520reward%2520model%2520to%250Adirectly%2520use%2520human%2520preference%2520data%2520for%2520aligning%2520LLM%2520outputs%2520with%2520human%250Aexpectations.%2520We%2520will%2520also%2520point%2520out%2520current%2520challenges%2520and%2520deficiencies%2520of%250Aexisting%2520methods%2520and%2520suggest%2520some%2520avenues%2520for%2520further%2520improvements.%2520Project%250Apage%2520of%2520this%2520work%2520can%2520be%2520found%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10400v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20Enhanced%20LLMs%3A%20A%20Survey&entry.906535625=Shuhe%20Wang%20and%20Shengyu%20Zhang%20and%20Jie%20Zhang%20and%20Runyi%20Hu%20and%20Xiaoya%20Li%20and%20Tianwei%20Zhang%20and%20Jiwei%20Li%20and%20Fei%20Wu%20and%20Guoyin%20Wang%20and%20Eduard%20Hovy&entry.1292438233=%20%20This%20paper%20surveys%20research%20in%20the%20rapidly%20growing%20field%20of%20enhancing%20large%0Alanguage%20models%20%28LLMs%29%20with%20reinforcement%20learning%20%28RL%29%2C%20a%20technique%20that%0Aenables%20LLMs%20to%20improve%20their%20performance%20by%20receiving%20feedback%20in%20the%20form%20of%0Arewards%20based%20on%20the%20quality%20of%20their%20outputs%2C%20allowing%20them%20to%20generate%20more%0Aaccurate%2C%20coherent%2C%20and%20contextually%20appropriate%20responses.%20In%20this%20work%2C%20we%0Amake%20a%20systematic%20review%20of%20the%20most%20up-to-date%20state%20of%20knowledge%20on%0ARL-enhanced%20LLMs%2C%20attempting%20to%20consolidate%20and%20analyze%20the%20rapidly%20growing%0Aresearch%20in%20this%20field%2C%20helping%20researchers%20understand%20the%20current%20challenges%0Aand%20advancements.%20Specifically%2C%20we%20%281%29%20detail%20the%20basics%20of%20RL%3B%20%282%29%20introduce%0Apopular%20RL-enhanced%20LLMs%3B%20%283%29%20review%20researches%20on%20two%20widely-used%20reward%0Amodel-based%20RL%20techniques%3A%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%0Aand%20Reinforcement%20Learning%20from%20AI%20Feedback%20%28RLAIF%29%3B%20and%20%284%29%20explore%20Direct%0APreference%20Optimization%20%28DPO%29%2C%20a%20set%20of%20methods%20that%20bypass%20the%20reward%20model%20to%0Adirectly%20use%20human%20preference%20data%20for%20aligning%20LLM%20outputs%20with%20human%0Aexpectations.%20We%20will%20also%20point%20out%20current%20challenges%20and%20deficiencies%20of%0Aexisting%20methods%20and%20suggest%20some%20avenues%20for%20further%20improvements.%20Project%0Apage%20of%20this%20work%20can%20be%20found%20at%3A%0A%5Curl%7Bhttps%3A//github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10400v2&entry.124074799=Read"},
{"title": "MeTHanol: Modularized Thinking Language Models with Intermediate Layer\n  Thinking, Decoding and Bootstrapping Reasoning", "author": "Ningyuan Xi and Xiaoyu Wang and Yetao Wu and Teng Chen and Qingqing Gu and Yue Zhao and Jinxian Qu and Zhonglin Jiang and Yong Chen and Luo Ji", "abstract": "  Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.\n", "link": "http://arxiv.org/abs/2409.12059v3", "date": "2024-12-17", "relevancy": 2.4952, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeTHanol%3A%20Modularized%20Thinking%20Language%20Models%20with%20Intermediate%20Layer%0A%20%20Thinking%2C%20Decoding%20and%20Bootstrapping%20Reasoning&body=Title%3A%20MeTHanol%3A%20Modularized%20Thinking%20Language%20Models%20with%20Intermediate%20Layer%0A%20%20Thinking%2C%20Decoding%20and%20Bootstrapping%20Reasoning%0AAuthor%3A%20Ningyuan%20Xi%20and%20Xiaoyu%20Wang%20and%20Yetao%20Wu%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Yue%20Zhao%20and%20Jinxian%20Qu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji%0AAbstract%3A%20%20%20Large%20Language%20Model%20can%20reasonably%20understand%20and%20generate%20human%20expressions%0Abut%20may%20lack%20of%20thorough%20thinking%20and%20reasoning%20mechanisms.%20Recently%20there%20have%0Abeen%20several%20studies%20which%20enhance%20the%20thinking%20ability%20of%20language%20models%20but%0Amost%20of%20them%20are%20not%20data-driven%20or%20training-based.%20In%20this%20paper%2C%20we%20are%0Amotivated%20by%20the%20cognitive%20mechanism%20in%20the%20natural%20world%2C%20and%20design%20a%20novel%0Amodel%20architecture%20called%20TaS%20which%20allows%20it%20to%20first%20consider%20the%20thoughts%0Aand%20then%20express%20the%20response%20based%20upon%20the%20query.%20We%20design%20several%20pipelines%0Ato%20annotate%20or%20generate%20the%20thought%20contents%20from%20prompt-response%20samples%2C%20then%0Aadd%20language%20heads%20in%20a%20middle%20layer%20which%20behaves%20as%20the%20thinking%20layer.%20We%0Atrain%20the%20language%20model%20by%20the%20thoughts-augmented%20data%20and%20successfully%20let%0Athe%20thinking%20layer%20automatically%20generate%20reasonable%20thoughts%20and%20finally%0Aoutput%20more%20reasonable%20responses.%20Both%20qualitative%20examples%20and%20quantitative%0Aresults%20validate%20the%20effectiveness%20and%20performance%20of%20TaS.%20Our%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/TadE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12059v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeTHanol%253A%2520Modularized%2520Thinking%2520Language%2520Models%2520with%2520Intermediate%2520Layer%250A%2520%2520Thinking%252C%2520Decoding%2520and%2520Bootstrapping%2520Reasoning%26entry.906535625%3DNingyuan%2520Xi%2520and%2520Xiaoyu%2520Wang%2520and%2520Yetao%2520Wu%2520and%2520Teng%2520Chen%2520and%2520Qingqing%2520Gu%2520and%2520Yue%2520Zhao%2520and%2520Jinxian%2520Qu%2520and%2520Zhonglin%2520Jiang%2520and%2520Yong%2520Chen%2520and%2520Luo%2520Ji%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520can%2520reasonably%2520understand%2520and%2520generate%2520human%2520expressions%250Abut%2520may%2520lack%2520of%2520thorough%2520thinking%2520and%2520reasoning%2520mechanisms.%2520Recently%2520there%2520have%250Abeen%2520several%2520studies%2520which%2520enhance%2520the%2520thinking%2520ability%2520of%2520language%2520models%2520but%250Amost%2520of%2520them%2520are%2520not%2520data-driven%2520or%2520training-based.%2520In%2520this%2520paper%252C%2520we%2520are%250Amotivated%2520by%2520the%2520cognitive%2520mechanism%2520in%2520the%2520natural%2520world%252C%2520and%2520design%2520a%2520novel%250Amodel%2520architecture%2520called%2520TaS%2520which%2520allows%2520it%2520to%2520first%2520consider%2520the%2520thoughts%250Aand%2520then%2520express%2520the%2520response%2520based%2520upon%2520the%2520query.%2520We%2520design%2520several%2520pipelines%250Ato%2520annotate%2520or%2520generate%2520the%2520thought%2520contents%2520from%2520prompt-response%2520samples%252C%2520then%250Aadd%2520language%2520heads%2520in%2520a%2520middle%2520layer%2520which%2520behaves%2520as%2520the%2520thinking%2520layer.%2520We%250Atrain%2520the%2520language%2520model%2520by%2520the%2520thoughts-augmented%2520data%2520and%2520successfully%2520let%250Athe%2520thinking%2520layer%2520automatically%2520generate%2520reasonable%2520thoughts%2520and%2520finally%250Aoutput%2520more%2520reasonable%2520responses.%2520Both%2520qualitative%2520examples%2520and%2520quantitative%250Aresults%2520validate%2520the%2520effectiveness%2520and%2520performance%2520of%2520TaS.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//anonymous.4open.science/r/TadE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12059v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeTHanol%3A%20Modularized%20Thinking%20Language%20Models%20with%20Intermediate%20Layer%0A%20%20Thinking%2C%20Decoding%20and%20Bootstrapping%20Reasoning&entry.906535625=Ningyuan%20Xi%20and%20Xiaoyu%20Wang%20and%20Yetao%20Wu%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Yue%20Zhao%20and%20Jinxian%20Qu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji&entry.1292438233=%20%20Large%20Language%20Model%20can%20reasonably%20understand%20and%20generate%20human%20expressions%0Abut%20may%20lack%20of%20thorough%20thinking%20and%20reasoning%20mechanisms.%20Recently%20there%20have%0Abeen%20several%20studies%20which%20enhance%20the%20thinking%20ability%20of%20language%20models%20but%0Amost%20of%20them%20are%20not%20data-driven%20or%20training-based.%20In%20this%20paper%2C%20we%20are%0Amotivated%20by%20the%20cognitive%20mechanism%20in%20the%20natural%20world%2C%20and%20design%20a%20novel%0Amodel%20architecture%20called%20TaS%20which%20allows%20it%20to%20first%20consider%20the%20thoughts%0Aand%20then%20express%20the%20response%20based%20upon%20the%20query.%20We%20design%20several%20pipelines%0Ato%20annotate%20or%20generate%20the%20thought%20contents%20from%20prompt-response%20samples%2C%20then%0Aadd%20language%20heads%20in%20a%20middle%20layer%20which%20behaves%20as%20the%20thinking%20layer.%20We%0Atrain%20the%20language%20model%20by%20the%20thoughts-augmented%20data%20and%20successfully%20let%0Athe%20thinking%20layer%20automatically%20generate%20reasonable%20thoughts%20and%20finally%0Aoutput%20more%20reasonable%20responses.%20Both%20qualitative%20examples%20and%20quantitative%0Aresults%20validate%20the%20effectiveness%20and%20performance%20of%20TaS.%20Our%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/TadE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12059v3&entry.124074799=Read"},
{"title": "Incremental Online Learning of Randomized Neural Network with Forward\n  Regularization", "author": "Junda Wang and Minghui Hu and Ning Li and Abdulaziz Al-Ali and Ponnuthurai Nagaratnam Suganthan", "abstract": "  Online learning of deep neural networks suffers from challenges such as\nhysteretic non-incremental updating, increasing memory usage, past\nretrospective retraining, and catastrophic forgetting. To alleviate these\ndrawbacks and achieve progressive immediate decision-making, we propose a novel\nIncremental Online Learning (IOL) process of Randomized Neural Networks\n(Randomized NN), a framework facilitating continuous improvements to Randomized\nNN performance in restrictive online scenarios. Within the framework, we\nfurther introduce IOL with ridge regularization (-R) and IOL with forward\nregularization (-F). -R generates stepwise incremental updates without\nretrospective retraining and avoids catastrophic forgetting. Moreover, we\nsubstituted -R with -F as it enhanced precognition learning ability using\nsemi-supervision and realized better online regrets to offline global experts\ncompared to -R during IOL. The algorithms of IOL for Randomized NN with -R/-F\non non-stationary batch stream were derived respectively, featuring recursive\nweight updates and variable learning rates. Additionally, we conducted a\ndetailed analysis and theoretically derived relative cumulative regret bounds\nof the Randomized NN learners with -R/-F in IOL under adversarial assumptions\nusing a novel methodology and presented several corollaries, from which we\nobserved the superiority on online learning acceleration and regret bounds of\nemploying -F in IOL. Finally, our proposed methods were rigorously examined\nacross regression and classification tasks on diverse datasets, which\ndistinctly validated the efficacy of IOL frameworks of Randomized NN and the\nadvantages of forward regularization.\n", "link": "http://arxiv.org/abs/2412.13096v1", "date": "2024-12-17", "relevancy": 2.4887, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5421}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4848}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incremental%20Online%20Learning%20of%20Randomized%20Neural%20Network%20with%20Forward%0A%20%20Regularization&body=Title%3A%20Incremental%20Online%20Learning%20of%20Randomized%20Neural%20Network%20with%20Forward%0A%20%20Regularization%0AAuthor%3A%20Junda%20Wang%20and%20Minghui%20Hu%20and%20Ning%20Li%20and%20Abdulaziz%20Al-Ali%20and%20Ponnuthurai%20Nagaratnam%20Suganthan%0AAbstract%3A%20%20%20Online%20learning%20of%20deep%20neural%20networks%20suffers%20from%20challenges%20such%20as%0Ahysteretic%20non-incremental%20updating%2C%20increasing%20memory%20usage%2C%20past%0Aretrospective%20retraining%2C%20and%20catastrophic%20forgetting.%20To%20alleviate%20these%0Adrawbacks%20and%20achieve%20progressive%20immediate%20decision-making%2C%20we%20propose%20a%20novel%0AIncremental%20Online%20Learning%20%28IOL%29%20process%20of%20Randomized%20Neural%20Networks%0A%28Randomized%20NN%29%2C%20a%20framework%20facilitating%20continuous%20improvements%20to%20Randomized%0ANN%20performance%20in%20restrictive%20online%20scenarios.%20Within%20the%20framework%2C%20we%0Afurther%20introduce%20IOL%20with%20ridge%20regularization%20%28-R%29%20and%20IOL%20with%20forward%0Aregularization%20%28-F%29.%20-R%20generates%20stepwise%20incremental%20updates%20without%0Aretrospective%20retraining%20and%20avoids%20catastrophic%20forgetting.%20Moreover%2C%20we%0Asubstituted%20-R%20with%20-F%20as%20it%20enhanced%20precognition%20learning%20ability%20using%0Asemi-supervision%20and%20realized%20better%20online%20regrets%20to%20offline%20global%20experts%0Acompared%20to%20-R%20during%20IOL.%20The%20algorithms%20of%20IOL%20for%20Randomized%20NN%20with%20-R/-F%0Aon%20non-stationary%20batch%20stream%20were%20derived%20respectively%2C%20featuring%20recursive%0Aweight%20updates%20and%20variable%20learning%20rates.%20Additionally%2C%20we%20conducted%20a%0Adetailed%20analysis%20and%20theoretically%20derived%20relative%20cumulative%20regret%20bounds%0Aof%20the%20Randomized%20NN%20learners%20with%20-R/-F%20in%20IOL%20under%20adversarial%20assumptions%0Ausing%20a%20novel%20methodology%20and%20presented%20several%20corollaries%2C%20from%20which%20we%0Aobserved%20the%20superiority%20on%20online%20learning%20acceleration%20and%20regret%20bounds%20of%0Aemploying%20-F%20in%20IOL.%20Finally%2C%20our%20proposed%20methods%20were%20rigorously%20examined%0Aacross%20regression%20and%20classification%20tasks%20on%20diverse%20datasets%2C%20which%0Adistinctly%20validated%20the%20efficacy%20of%20IOL%20frameworks%20of%20Randomized%20NN%20and%20the%0Aadvantages%20of%20forward%20regularization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncremental%2520Online%2520Learning%2520of%2520Randomized%2520Neural%2520Network%2520with%2520Forward%250A%2520%2520Regularization%26entry.906535625%3DJunda%2520Wang%2520and%2520Minghui%2520Hu%2520and%2520Ning%2520Li%2520and%2520Abdulaziz%2520Al-Ali%2520and%2520Ponnuthurai%2520Nagaratnam%2520Suganthan%26entry.1292438233%3D%2520%2520Online%2520learning%2520of%2520deep%2520neural%2520networks%2520suffers%2520from%2520challenges%2520such%2520as%250Ahysteretic%2520non-incremental%2520updating%252C%2520increasing%2520memory%2520usage%252C%2520past%250Aretrospective%2520retraining%252C%2520and%2520catastrophic%2520forgetting.%2520To%2520alleviate%2520these%250Adrawbacks%2520and%2520achieve%2520progressive%2520immediate%2520decision-making%252C%2520we%2520propose%2520a%2520novel%250AIncremental%2520Online%2520Learning%2520%2528IOL%2529%2520process%2520of%2520Randomized%2520Neural%2520Networks%250A%2528Randomized%2520NN%2529%252C%2520a%2520framework%2520facilitating%2520continuous%2520improvements%2520to%2520Randomized%250ANN%2520performance%2520in%2520restrictive%2520online%2520scenarios.%2520Within%2520the%2520framework%252C%2520we%250Afurther%2520introduce%2520IOL%2520with%2520ridge%2520regularization%2520%2528-R%2529%2520and%2520IOL%2520with%2520forward%250Aregularization%2520%2528-F%2529.%2520-R%2520generates%2520stepwise%2520incremental%2520updates%2520without%250Aretrospective%2520retraining%2520and%2520avoids%2520catastrophic%2520forgetting.%2520Moreover%252C%2520we%250Asubstituted%2520-R%2520with%2520-F%2520as%2520it%2520enhanced%2520precognition%2520learning%2520ability%2520using%250Asemi-supervision%2520and%2520realized%2520better%2520online%2520regrets%2520to%2520offline%2520global%2520experts%250Acompared%2520to%2520-R%2520during%2520IOL.%2520The%2520algorithms%2520of%2520IOL%2520for%2520Randomized%2520NN%2520with%2520-R/-F%250Aon%2520non-stationary%2520batch%2520stream%2520were%2520derived%2520respectively%252C%2520featuring%2520recursive%250Aweight%2520updates%2520and%2520variable%2520learning%2520rates.%2520Additionally%252C%2520we%2520conducted%2520a%250Adetailed%2520analysis%2520and%2520theoretically%2520derived%2520relative%2520cumulative%2520regret%2520bounds%250Aof%2520the%2520Randomized%2520NN%2520learners%2520with%2520-R/-F%2520in%2520IOL%2520under%2520adversarial%2520assumptions%250Ausing%2520a%2520novel%2520methodology%2520and%2520presented%2520several%2520corollaries%252C%2520from%2520which%2520we%250Aobserved%2520the%2520superiority%2520on%2520online%2520learning%2520acceleration%2520and%2520regret%2520bounds%2520of%250Aemploying%2520-F%2520in%2520IOL.%2520Finally%252C%2520our%2520proposed%2520methods%2520were%2520rigorously%2520examined%250Aacross%2520regression%2520and%2520classification%2520tasks%2520on%2520diverse%2520datasets%252C%2520which%250Adistinctly%2520validated%2520the%2520efficacy%2520of%2520IOL%2520frameworks%2520of%2520Randomized%2520NN%2520and%2520the%250Aadvantages%2520of%2520forward%2520regularization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Online%20Learning%20of%20Randomized%20Neural%20Network%20with%20Forward%0A%20%20Regularization&entry.906535625=Junda%20Wang%20and%20Minghui%20Hu%20and%20Ning%20Li%20and%20Abdulaziz%20Al-Ali%20and%20Ponnuthurai%20Nagaratnam%20Suganthan&entry.1292438233=%20%20Online%20learning%20of%20deep%20neural%20networks%20suffers%20from%20challenges%20such%20as%0Ahysteretic%20non-incremental%20updating%2C%20increasing%20memory%20usage%2C%20past%0Aretrospective%20retraining%2C%20and%20catastrophic%20forgetting.%20To%20alleviate%20these%0Adrawbacks%20and%20achieve%20progressive%20immediate%20decision-making%2C%20we%20propose%20a%20novel%0AIncremental%20Online%20Learning%20%28IOL%29%20process%20of%20Randomized%20Neural%20Networks%0A%28Randomized%20NN%29%2C%20a%20framework%20facilitating%20continuous%20improvements%20to%20Randomized%0ANN%20performance%20in%20restrictive%20online%20scenarios.%20Within%20the%20framework%2C%20we%0Afurther%20introduce%20IOL%20with%20ridge%20regularization%20%28-R%29%20and%20IOL%20with%20forward%0Aregularization%20%28-F%29.%20-R%20generates%20stepwise%20incremental%20updates%20without%0Aretrospective%20retraining%20and%20avoids%20catastrophic%20forgetting.%20Moreover%2C%20we%0Asubstituted%20-R%20with%20-F%20as%20it%20enhanced%20precognition%20learning%20ability%20using%0Asemi-supervision%20and%20realized%20better%20online%20regrets%20to%20offline%20global%20experts%0Acompared%20to%20-R%20during%20IOL.%20The%20algorithms%20of%20IOL%20for%20Randomized%20NN%20with%20-R/-F%0Aon%20non-stationary%20batch%20stream%20were%20derived%20respectively%2C%20featuring%20recursive%0Aweight%20updates%20and%20variable%20learning%20rates.%20Additionally%2C%20we%20conducted%20a%0Adetailed%20analysis%20and%20theoretically%20derived%20relative%20cumulative%20regret%20bounds%0Aof%20the%20Randomized%20NN%20learners%20with%20-R/-F%20in%20IOL%20under%20adversarial%20assumptions%0Ausing%20a%20novel%20methodology%20and%20presented%20several%20corollaries%2C%20from%20which%20we%0Aobserved%20the%20superiority%20on%20online%20learning%20acceleration%20and%20regret%20bounds%20of%0Aemploying%20-F%20in%20IOL.%20Finally%2C%20our%20proposed%20methods%20were%20rigorously%20examined%0Aacross%20regression%20and%20classification%20tasks%20on%20diverse%20datasets%2C%20which%0Adistinctly%20validated%20the%20efficacy%20of%20IOL%20frameworks%20of%20Randomized%20NN%20and%20the%0Aadvantages%20of%20forward%20regularization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13096v1&entry.124074799=Read"},
{"title": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels", "author": "Lingxiao Wei and He Yan and Xiangju Lu and Junmin Zhu and Jun Wang and Wei Zhang", "abstract": "  Large Language Models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of high-quality long-context\nsummarization datasets has hindered further advancements in this area. To\naddress this, we introduce CNNSum, a multi-scale long-context summarization\nbenchmark based on Chinese novels, featuring human-driven annotations, which\ncomprises four subsets totaling 695 samples, with lengths ranging from 16k to\n128k. We evaluate numerous LLMs and conduct detailed case analyses.\nFurthermore, we conduct extensive fine-tuning experiments to explore and\nimprove long-context summarization. In our study: (1) Advanced LLMs like GPT-4o\nmay still generate subjective commentary, leading to vague summaries. (2)\nCurrently, long-context summarization mainly relies on memory ability afforded\nby longer context lengths. The advantages of Large LLMs are hard to utilize,\nthus small LLMs are the most cost-effective. (3) Different prompt templates\npaired with various version models may cause large performance gaps. In further\nfine-tuning, these can be mitigated, and the Base version models perform\nbetter. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential;\nusing short-context data can significantly improve long-context summarization\nperformance. However, further applying other interpolation methods requires\ncareful selection. (5) CNNSum provides more reliable and insightful evaluation\nresults than other benchmarks. We release CNNSum to advance future research in\nthis field. https://github.com/CxsGhost/CNNSum\n", "link": "http://arxiv.org/abs/2412.02819v4", "date": "2024-12-17", "relevancy": 2.4874, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNNSum%3A%20Exploring%20Long-Context%20Summarization%20with%20Large%20Language%20Models%0A%20%20in%20Chinese%20Novels&body=Title%3A%20CNNSum%3A%20Exploring%20Long-Context%20Summarization%20with%20Large%20Language%20Models%0A%20%20in%20Chinese%20Novels%0AAuthor%3A%20Lingxiao%20Wei%20and%20He%20Yan%20and%20Xiangju%20Lu%20and%20Junmin%20Zhu%20and%20Jun%20Wang%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20well-researched%20in%20various%0Along-context%20tasks.%20However%2C%20the%20scarcity%20of%20high-quality%20long-context%0Asummarization%20datasets%20has%20hindered%20further%20advancements%20in%20this%20area.%20To%0Aaddress%20this%2C%20we%20introduce%20CNNSum%2C%20a%20multi-scale%20long-context%20summarization%0Abenchmark%20based%20on%20Chinese%20novels%2C%20featuring%20human-driven%20annotations%2C%20which%0Acomprises%20four%20subsets%20totaling%20695%20samples%2C%20with%20lengths%20ranging%20from%2016k%20to%0A128k.%20We%20evaluate%20numerous%20LLMs%20and%20conduct%20detailed%20case%20analyses.%0AFurthermore%2C%20we%20conduct%20extensive%20fine-tuning%20experiments%20to%20explore%20and%0Aimprove%20long-context%20summarization.%20In%20our%20study%3A%20%281%29%20Advanced%20LLMs%20like%20GPT-4o%0Amay%20still%20generate%20subjective%20commentary%2C%20leading%20to%20vague%20summaries.%20%282%29%0ACurrently%2C%20long-context%20summarization%20mainly%20relies%20on%20memory%20ability%20afforded%0Aby%20longer%20context%20lengths.%20The%20advantages%20of%20Large%20LLMs%20are%20hard%20to%20utilize%2C%0Athus%20small%20LLMs%20are%20the%20most%20cost-effective.%20%283%29%20Different%20prompt%20templates%0Apaired%20with%20various%20version%20models%20may%20cause%20large%20performance%20gaps.%20In%20further%0Afine-tuning%2C%20these%20can%20be%20mitigated%2C%20and%20the%20Base%20version%20models%20perform%0Abetter.%20%284%29%20LLMs%20with%20RoPE-base%20scaled%20exhibit%20strong%20extrapolation%20potential%3B%0Ausing%20short-context%20data%20can%20significantly%20improve%20long-context%20summarization%0Aperformance.%20However%2C%20further%20applying%20other%20interpolation%20methods%20requires%0Acareful%20selection.%20%285%29%20CNNSum%20provides%20more%20reliable%20and%20insightful%20evaluation%0Aresults%20than%20other%20benchmarks.%20We%20release%20CNNSum%20to%20advance%20future%20research%20in%0Athis%20field.%20https%3A//github.com/CxsGhost/CNNSum%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02819v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNNSum%253A%2520Exploring%2520Long-Context%2520Summarization%2520with%2520Large%2520Language%2520Models%250A%2520%2520in%2520Chinese%2520Novels%26entry.906535625%3DLingxiao%2520Wei%2520and%2520He%2520Yan%2520and%2520Xiangju%2520Lu%2520and%2520Junmin%2520Zhu%2520and%2520Jun%2520Wang%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520well-researched%2520in%2520various%250Along-context%2520tasks.%2520However%252C%2520the%2520scarcity%2520of%2520high-quality%2520long-context%250Asummarization%2520datasets%2520has%2520hindered%2520further%2520advancements%2520in%2520this%2520area.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520CNNSum%252C%2520a%2520multi-scale%2520long-context%2520summarization%250Abenchmark%2520based%2520on%2520Chinese%2520novels%252C%2520featuring%2520human-driven%2520annotations%252C%2520which%250Acomprises%2520four%2520subsets%2520totaling%2520695%2520samples%252C%2520with%2520lengths%2520ranging%2520from%252016k%2520to%250A128k.%2520We%2520evaluate%2520numerous%2520LLMs%2520and%2520conduct%2520detailed%2520case%2520analyses.%250AFurthermore%252C%2520we%2520conduct%2520extensive%2520fine-tuning%2520experiments%2520to%2520explore%2520and%250Aimprove%2520long-context%2520summarization.%2520In%2520our%2520study%253A%2520%25281%2529%2520Advanced%2520LLMs%2520like%2520GPT-4o%250Amay%2520still%2520generate%2520subjective%2520commentary%252C%2520leading%2520to%2520vague%2520summaries.%2520%25282%2529%250ACurrently%252C%2520long-context%2520summarization%2520mainly%2520relies%2520on%2520memory%2520ability%2520afforded%250Aby%2520longer%2520context%2520lengths.%2520The%2520advantages%2520of%2520Large%2520LLMs%2520are%2520hard%2520to%2520utilize%252C%250Athus%2520small%2520LLMs%2520are%2520the%2520most%2520cost-effective.%2520%25283%2529%2520Different%2520prompt%2520templates%250Apaired%2520with%2520various%2520version%2520models%2520may%2520cause%2520large%2520performance%2520gaps.%2520In%2520further%250Afine-tuning%252C%2520these%2520can%2520be%2520mitigated%252C%2520and%2520the%2520Base%2520version%2520models%2520perform%250Abetter.%2520%25284%2529%2520LLMs%2520with%2520RoPE-base%2520scaled%2520exhibit%2520strong%2520extrapolation%2520potential%253B%250Ausing%2520short-context%2520data%2520can%2520significantly%2520improve%2520long-context%2520summarization%250Aperformance.%2520However%252C%2520further%2520applying%2520other%2520interpolation%2520methods%2520requires%250Acareful%2520selection.%2520%25285%2529%2520CNNSum%2520provides%2520more%2520reliable%2520and%2520insightful%2520evaluation%250Aresults%2520than%2520other%2520benchmarks.%2520We%2520release%2520CNNSum%2520to%2520advance%2520future%2520research%2520in%250Athis%2520field.%2520https%253A//github.com/CxsGhost/CNNSum%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02819v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNNSum%3A%20Exploring%20Long-Context%20Summarization%20with%20Large%20Language%20Models%0A%20%20in%20Chinese%20Novels&entry.906535625=Lingxiao%20Wei%20and%20He%20Yan%20and%20Xiangju%20Lu%20and%20Junmin%20Zhu%20and%20Jun%20Wang%20and%20Wei%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20well-researched%20in%20various%0Along-context%20tasks.%20However%2C%20the%20scarcity%20of%20high-quality%20long-context%0Asummarization%20datasets%20has%20hindered%20further%20advancements%20in%20this%20area.%20To%0Aaddress%20this%2C%20we%20introduce%20CNNSum%2C%20a%20multi-scale%20long-context%20summarization%0Abenchmark%20based%20on%20Chinese%20novels%2C%20featuring%20human-driven%20annotations%2C%20which%0Acomprises%20four%20subsets%20totaling%20695%20samples%2C%20with%20lengths%20ranging%20from%2016k%20to%0A128k.%20We%20evaluate%20numerous%20LLMs%20and%20conduct%20detailed%20case%20analyses.%0AFurthermore%2C%20we%20conduct%20extensive%20fine-tuning%20experiments%20to%20explore%20and%0Aimprove%20long-context%20summarization.%20In%20our%20study%3A%20%281%29%20Advanced%20LLMs%20like%20GPT-4o%0Amay%20still%20generate%20subjective%20commentary%2C%20leading%20to%20vague%20summaries.%20%282%29%0ACurrently%2C%20long-context%20summarization%20mainly%20relies%20on%20memory%20ability%20afforded%0Aby%20longer%20context%20lengths.%20The%20advantages%20of%20Large%20LLMs%20are%20hard%20to%20utilize%2C%0Athus%20small%20LLMs%20are%20the%20most%20cost-effective.%20%283%29%20Different%20prompt%20templates%0Apaired%20with%20various%20version%20models%20may%20cause%20large%20performance%20gaps.%20In%20further%0Afine-tuning%2C%20these%20can%20be%20mitigated%2C%20and%20the%20Base%20version%20models%20perform%0Abetter.%20%284%29%20LLMs%20with%20RoPE-base%20scaled%20exhibit%20strong%20extrapolation%20potential%3B%0Ausing%20short-context%20data%20can%20significantly%20improve%20long-context%20summarization%0Aperformance.%20However%2C%20further%20applying%20other%20interpolation%20methods%20requires%0Acareful%20selection.%20%285%29%20CNNSum%20provides%20more%20reliable%20and%20insightful%20evaluation%0Aresults%20than%20other%20benchmarks.%20We%20release%20CNNSum%20to%20advance%20future%20research%20in%0Athis%20field.%20https%3A//github.com/CxsGhost/CNNSum%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02819v4&entry.124074799=Read"},
{"title": "Benchmarking Embedding Aggregation Methods in Computational Pathology: A\n  Clinical Data Perspective", "author": "Shengjia Chen and Gabriele Campanella and Abdulkadir Elmas and Aryeh Stock and Jennifer Zeng and Alexandros D. Polydorides and Adam J. Schoenfeld and Kuan-lin Huang and Jane Houldsworth and Chad Vanderbilt and Thomas J. Fuchs", "abstract": "  Recent advances in artificial intelligence (AI), in particular\nself-supervised learning of foundation models (FMs), are revolutionizing\nmedical imaging and computational pathology (CPath). A constant challenge in\nthe analysis of digital Whole Slide Images (WSIs) is the problem of aggregating\ntens of thousands of tile-level image embeddings to a slide-level\nrepresentation. Due to the prevalent use of datasets created for genomic\nresearch, such as TCGA, for method development, the performance of these\ntechniques on diagnostic slides from clinical practice has been inadequately\nexplored. This study conducts a thorough benchmarking analysis of ten\nslide-level aggregation techniques across nine clinically relevant tasks,\nincluding diagnostic assessment, biomarker classification, and outcome\nprediction. The results yield following key insights: (1) Embeddings derived\nfrom domain-specific (histological images) FMs outperform those from generic\nImageNet-based models across aggregation methods. (2) Spatial-aware aggregators\nenhance the performance significantly when using ImageNet pre-trained models\nbut not when using FMs. (3) No single model excels in all tasks and\nspatially-aware models do not show general superiority as it would be expected.\nThese findings underscore the need for more adaptable and universally\napplicable aggregation techniques, guiding future research towards tools that\nbetter meet the evolving needs of clinical-AI in pathology. The code used in\nthis work is available at\n\\url{https://github.com/fuchs-lab-public/CPath_SABenchmark}.\n", "link": "http://arxiv.org/abs/2407.07841v2", "date": "2024-12-17", "relevancy": 2.4694, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Embedding%20Aggregation%20Methods%20in%20Computational%20Pathology%3A%20A%0A%20%20Clinical%20Data%20Perspective&body=Title%3A%20Benchmarking%20Embedding%20Aggregation%20Methods%20in%20Computational%20Pathology%3A%20A%0A%20%20Clinical%20Data%20Perspective%0AAuthor%3A%20Shengjia%20Chen%20and%20Gabriele%20Campanella%20and%20Abdulkadir%20Elmas%20and%20Aryeh%20Stock%20and%20Jennifer%20Zeng%20and%20Alexandros%20D.%20Polydorides%20and%20Adam%20J.%20Schoenfeld%20and%20Kuan-lin%20Huang%20and%20Jane%20Houldsworth%20and%20Chad%20Vanderbilt%20and%20Thomas%20J.%20Fuchs%0AAbstract%3A%20%20%20Recent%20advances%20in%20artificial%20intelligence%20%28AI%29%2C%20in%20particular%0Aself-supervised%20learning%20of%20foundation%20models%20%28FMs%29%2C%20are%20revolutionizing%0Amedical%20imaging%20and%20computational%20pathology%20%28CPath%29.%20A%20constant%20challenge%20in%0Athe%20analysis%20of%20digital%20Whole%20Slide%20Images%20%28WSIs%29%20is%20the%20problem%20of%20aggregating%0Atens%20of%20thousands%20of%20tile-level%20image%20embeddings%20to%20a%20slide-level%0Arepresentation.%20Due%20to%20the%20prevalent%20use%20of%20datasets%20created%20for%20genomic%0Aresearch%2C%20such%20as%20TCGA%2C%20for%20method%20development%2C%20the%20performance%20of%20these%0Atechniques%20on%20diagnostic%20slides%20from%20clinical%20practice%20has%20been%20inadequately%0Aexplored.%20This%20study%20conducts%20a%20thorough%20benchmarking%20analysis%20of%20ten%0Aslide-level%20aggregation%20techniques%20across%20nine%20clinically%20relevant%20tasks%2C%0Aincluding%20diagnostic%20assessment%2C%20biomarker%20classification%2C%20and%20outcome%0Aprediction.%20The%20results%20yield%20following%20key%20insights%3A%20%281%29%20Embeddings%20derived%0Afrom%20domain-specific%20%28histological%20images%29%20FMs%20outperform%20those%20from%20generic%0AImageNet-based%20models%20across%20aggregation%20methods.%20%282%29%20Spatial-aware%20aggregators%0Aenhance%20the%20performance%20significantly%20when%20using%20ImageNet%20pre-trained%20models%0Abut%20not%20when%20using%20FMs.%20%283%29%20No%20single%20model%20excels%20in%20all%20tasks%20and%0Aspatially-aware%20models%20do%20not%20show%20general%20superiority%20as%20it%20would%20be%20expected.%0AThese%20findings%20underscore%20the%20need%20for%20more%20adaptable%20and%20universally%0Aapplicable%20aggregation%20techniques%2C%20guiding%20future%20research%20towards%20tools%20that%0Abetter%20meet%20the%20evolving%20needs%20of%20clinical-AI%20in%20pathology.%20The%20code%20used%20in%0Athis%20work%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/fuchs-lab-public/CPath_SABenchmark%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07841v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Embedding%2520Aggregation%2520Methods%2520in%2520Computational%2520Pathology%253A%2520A%250A%2520%2520Clinical%2520Data%2520Perspective%26entry.906535625%3DShengjia%2520Chen%2520and%2520Gabriele%2520Campanella%2520and%2520Abdulkadir%2520Elmas%2520and%2520Aryeh%2520Stock%2520and%2520Jennifer%2520Zeng%2520and%2520Alexandros%2520D.%2520Polydorides%2520and%2520Adam%2520J.%2520Schoenfeld%2520and%2520Kuan-lin%2520Huang%2520and%2520Jane%2520Houldsworth%2520and%2520Chad%2520Vanderbilt%2520and%2520Thomas%2520J.%2520Fuchs%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520in%2520particular%250Aself-supervised%2520learning%2520of%2520foundation%2520models%2520%2528FMs%2529%252C%2520are%2520revolutionizing%250Amedical%2520imaging%2520and%2520computational%2520pathology%2520%2528CPath%2529.%2520A%2520constant%2520challenge%2520in%250Athe%2520analysis%2520of%2520digital%2520Whole%2520Slide%2520Images%2520%2528WSIs%2529%2520is%2520the%2520problem%2520of%2520aggregating%250Atens%2520of%2520thousands%2520of%2520tile-level%2520image%2520embeddings%2520to%2520a%2520slide-level%250Arepresentation.%2520Due%2520to%2520the%2520prevalent%2520use%2520of%2520datasets%2520created%2520for%2520genomic%250Aresearch%252C%2520such%2520as%2520TCGA%252C%2520for%2520method%2520development%252C%2520the%2520performance%2520of%2520these%250Atechniques%2520on%2520diagnostic%2520slides%2520from%2520clinical%2520practice%2520has%2520been%2520inadequately%250Aexplored.%2520This%2520study%2520conducts%2520a%2520thorough%2520benchmarking%2520analysis%2520of%2520ten%250Aslide-level%2520aggregation%2520techniques%2520across%2520nine%2520clinically%2520relevant%2520tasks%252C%250Aincluding%2520diagnostic%2520assessment%252C%2520biomarker%2520classification%252C%2520and%2520outcome%250Aprediction.%2520The%2520results%2520yield%2520following%2520key%2520insights%253A%2520%25281%2529%2520Embeddings%2520derived%250Afrom%2520domain-specific%2520%2528histological%2520images%2529%2520FMs%2520outperform%2520those%2520from%2520generic%250AImageNet-based%2520models%2520across%2520aggregation%2520methods.%2520%25282%2529%2520Spatial-aware%2520aggregators%250Aenhance%2520the%2520performance%2520significantly%2520when%2520using%2520ImageNet%2520pre-trained%2520models%250Abut%2520not%2520when%2520using%2520FMs.%2520%25283%2529%2520No%2520single%2520model%2520excels%2520in%2520all%2520tasks%2520and%250Aspatially-aware%2520models%2520do%2520not%2520show%2520general%2520superiority%2520as%2520it%2520would%2520be%2520expected.%250AThese%2520findings%2520underscore%2520the%2520need%2520for%2520more%2520adaptable%2520and%2520universally%250Aapplicable%2520aggregation%2520techniques%252C%2520guiding%2520future%2520research%2520towards%2520tools%2520that%250Abetter%2520meet%2520the%2520evolving%2520needs%2520of%2520clinical-AI%2520in%2520pathology.%2520The%2520code%2520used%2520in%250Athis%2520work%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/fuchs-lab-public/CPath_SABenchmark%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07841v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Embedding%20Aggregation%20Methods%20in%20Computational%20Pathology%3A%20A%0A%20%20Clinical%20Data%20Perspective&entry.906535625=Shengjia%20Chen%20and%20Gabriele%20Campanella%20and%20Abdulkadir%20Elmas%20and%20Aryeh%20Stock%20and%20Jennifer%20Zeng%20and%20Alexandros%20D.%20Polydorides%20and%20Adam%20J.%20Schoenfeld%20and%20Kuan-lin%20Huang%20and%20Jane%20Houldsworth%20and%20Chad%20Vanderbilt%20and%20Thomas%20J.%20Fuchs&entry.1292438233=%20%20Recent%20advances%20in%20artificial%20intelligence%20%28AI%29%2C%20in%20particular%0Aself-supervised%20learning%20of%20foundation%20models%20%28FMs%29%2C%20are%20revolutionizing%0Amedical%20imaging%20and%20computational%20pathology%20%28CPath%29.%20A%20constant%20challenge%20in%0Athe%20analysis%20of%20digital%20Whole%20Slide%20Images%20%28WSIs%29%20is%20the%20problem%20of%20aggregating%0Atens%20of%20thousands%20of%20tile-level%20image%20embeddings%20to%20a%20slide-level%0Arepresentation.%20Due%20to%20the%20prevalent%20use%20of%20datasets%20created%20for%20genomic%0Aresearch%2C%20such%20as%20TCGA%2C%20for%20method%20development%2C%20the%20performance%20of%20these%0Atechniques%20on%20diagnostic%20slides%20from%20clinical%20practice%20has%20been%20inadequately%0Aexplored.%20This%20study%20conducts%20a%20thorough%20benchmarking%20analysis%20of%20ten%0Aslide-level%20aggregation%20techniques%20across%20nine%20clinically%20relevant%20tasks%2C%0Aincluding%20diagnostic%20assessment%2C%20biomarker%20classification%2C%20and%20outcome%0Aprediction.%20The%20results%20yield%20following%20key%20insights%3A%20%281%29%20Embeddings%20derived%0Afrom%20domain-specific%20%28histological%20images%29%20FMs%20outperform%20those%20from%20generic%0AImageNet-based%20models%20across%20aggregation%20methods.%20%282%29%20Spatial-aware%20aggregators%0Aenhance%20the%20performance%20significantly%20when%20using%20ImageNet%20pre-trained%20models%0Abut%20not%20when%20using%20FMs.%20%283%29%20No%20single%20model%20excels%20in%20all%20tasks%20and%0Aspatially-aware%20models%20do%20not%20show%20general%20superiority%20as%20it%20would%20be%20expected.%0AThese%20findings%20underscore%20the%20need%20for%20more%20adaptable%20and%20universally%0Aapplicable%20aggregation%20techniques%2C%20guiding%20future%20research%20towards%20tools%20that%0Abetter%20meet%20the%20evolving%20needs%20of%20clinical-AI%20in%20pathology.%20The%20code%20used%20in%0Athis%20work%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/fuchs-lab-public/CPath_SABenchmark%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07841v2&entry.124074799=Read"},
{"title": "Queries, Representation & Detection: The Next 100 Model Fingerprinting\n  Schemes", "author": "Augustin Godinot and Erwan Le Merrer and Camilla Penzo and Fran\u00e7ois Ta\u00efani and Gilles Tr\u00e9dan", "abstract": "  The deployment of machine learning models in operational contexts represents\na significant investment for any organisation. Consequently, the risk of these\nmodels being misappropriated by competitors needs to be addressed. In recent\nyears, numerous proposals have been put forth to detect instances of model\nstealing. However, these proposals operate under implicit and disparate data\nand model access assumptions; as a consequence, it remains unclear how they can\nbe effectively compared to one another. Our evaluation shows that a simple\nbaseline that we introduce performs on par with existing state-of-the-art\nfingerprints, which, on the other hand, are much more complex. To uncover the\nreasons behind this intriguing result, this paper introduces a systematic\napproach to both the creation of model fingerprinting schemes and their\nevaluation benchmarks. By dividing model fingerprinting into three core\ncomponents -- Query, Representation and Detection (QuRD) -- we are able to\nidentify $\\sim100$ previously unexplored QuRD combinations and gain insights\ninto their performance. Finally, we introduce a set of metrics to compare and\nguide the creation of more representative model stealing detection benchmarks.\nOur approach reveals the need for more challenging benchmarks and a sound\ncomparison with baselines. To foster the creation of new fingerprinting schemes\nand benchmarks, we open-source our fingerprinting toolbox.\n", "link": "http://arxiv.org/abs/2412.13021v1", "date": "2024-12-17", "relevancy": 2.4688, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Queries%2C%20Representation%20%26%20Detection%3A%20The%20Next%20100%20Model%20Fingerprinting%0A%20%20Schemes&body=Title%3A%20Queries%2C%20Representation%20%26%20Detection%3A%20The%20Next%20100%20Model%20Fingerprinting%0A%20%20Schemes%0AAuthor%3A%20Augustin%20Godinot%20and%20Erwan%20Le%20Merrer%20and%20Camilla%20Penzo%20and%20Fran%C3%A7ois%20Ta%C3%AFani%20and%20Gilles%20Tr%C3%A9dan%0AAbstract%3A%20%20%20The%20deployment%20of%20machine%20learning%20models%20in%20operational%20contexts%20represents%0Aa%20significant%20investment%20for%20any%20organisation.%20Consequently%2C%20the%20risk%20of%20these%0Amodels%20being%20misappropriated%20by%20competitors%20needs%20to%20be%20addressed.%20In%20recent%0Ayears%2C%20numerous%20proposals%20have%20been%20put%20forth%20to%20detect%20instances%20of%20model%0Astealing.%20However%2C%20these%20proposals%20operate%20under%20implicit%20and%20disparate%20data%0Aand%20model%20access%20assumptions%3B%20as%20a%20consequence%2C%20it%20remains%20unclear%20how%20they%20can%0Abe%20effectively%20compared%20to%20one%20another.%20Our%20evaluation%20shows%20that%20a%20simple%0Abaseline%20that%20we%20introduce%20performs%20on%20par%20with%20existing%20state-of-the-art%0Afingerprints%2C%20which%2C%20on%20the%20other%20hand%2C%20are%20much%20more%20complex.%20To%20uncover%20the%0Areasons%20behind%20this%20intriguing%20result%2C%20this%20paper%20introduces%20a%20systematic%0Aapproach%20to%20both%20the%20creation%20of%20model%20fingerprinting%20schemes%20and%20their%0Aevaluation%20benchmarks.%20By%20dividing%20model%20fingerprinting%20into%20three%20core%0Acomponents%20--%20Query%2C%20Representation%20and%20Detection%20%28QuRD%29%20--%20we%20are%20able%20to%0Aidentify%20%24%5Csim100%24%20previously%20unexplored%20QuRD%20combinations%20and%20gain%20insights%0Ainto%20their%20performance.%20Finally%2C%20we%20introduce%20a%20set%20of%20metrics%20to%20compare%20and%0Aguide%20the%20creation%20of%20more%20representative%20model%20stealing%20detection%20benchmarks.%0AOur%20approach%20reveals%20the%20need%20for%20more%20challenging%20benchmarks%20and%20a%20sound%0Acomparison%20with%20baselines.%20To%20foster%20the%20creation%20of%20new%20fingerprinting%20schemes%0Aand%20benchmarks%2C%20we%20open-source%20our%20fingerprinting%20toolbox.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueries%252C%2520Representation%2520%2526%2520Detection%253A%2520The%2520Next%2520100%2520Model%2520Fingerprinting%250A%2520%2520Schemes%26entry.906535625%3DAugustin%2520Godinot%2520and%2520Erwan%2520Le%2520Merrer%2520and%2520Camilla%2520Penzo%2520and%2520Fran%25C3%25A7ois%2520Ta%25C3%25AFani%2520and%2520Gilles%2520Tr%25C3%25A9dan%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520machine%2520learning%2520models%2520in%2520operational%2520contexts%2520represents%250Aa%2520significant%2520investment%2520for%2520any%2520organisation.%2520Consequently%252C%2520the%2520risk%2520of%2520these%250Amodels%2520being%2520misappropriated%2520by%2520competitors%2520needs%2520to%2520be%2520addressed.%2520In%2520recent%250Ayears%252C%2520numerous%2520proposals%2520have%2520been%2520put%2520forth%2520to%2520detect%2520instances%2520of%2520model%250Astealing.%2520However%252C%2520these%2520proposals%2520operate%2520under%2520implicit%2520and%2520disparate%2520data%250Aand%2520model%2520access%2520assumptions%253B%2520as%2520a%2520consequence%252C%2520it%2520remains%2520unclear%2520how%2520they%2520can%250Abe%2520effectively%2520compared%2520to%2520one%2520another.%2520Our%2520evaluation%2520shows%2520that%2520a%2520simple%250Abaseline%2520that%2520we%2520introduce%2520performs%2520on%2520par%2520with%2520existing%2520state-of-the-art%250Afingerprints%252C%2520which%252C%2520on%2520the%2520other%2520hand%252C%2520are%2520much%2520more%2520complex.%2520To%2520uncover%2520the%250Areasons%2520behind%2520this%2520intriguing%2520result%252C%2520this%2520paper%2520introduces%2520a%2520systematic%250Aapproach%2520to%2520both%2520the%2520creation%2520of%2520model%2520fingerprinting%2520schemes%2520and%2520their%250Aevaluation%2520benchmarks.%2520By%2520dividing%2520model%2520fingerprinting%2520into%2520three%2520core%250Acomponents%2520--%2520Query%252C%2520Representation%2520and%2520Detection%2520%2528QuRD%2529%2520--%2520we%2520are%2520able%2520to%250Aidentify%2520%2524%255Csim100%2524%2520previously%2520unexplored%2520QuRD%2520combinations%2520and%2520gain%2520insights%250Ainto%2520their%2520performance.%2520Finally%252C%2520we%2520introduce%2520a%2520set%2520of%2520metrics%2520to%2520compare%2520and%250Aguide%2520the%2520creation%2520of%2520more%2520representative%2520model%2520stealing%2520detection%2520benchmarks.%250AOur%2520approach%2520reveals%2520the%2520need%2520for%2520more%2520challenging%2520benchmarks%2520and%2520a%2520sound%250Acomparison%2520with%2520baselines.%2520To%2520foster%2520the%2520creation%2520of%2520new%2520fingerprinting%2520schemes%250Aand%2520benchmarks%252C%2520we%2520open-source%2520our%2520fingerprinting%2520toolbox.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Queries%2C%20Representation%20%26%20Detection%3A%20The%20Next%20100%20Model%20Fingerprinting%0A%20%20Schemes&entry.906535625=Augustin%20Godinot%20and%20Erwan%20Le%20Merrer%20and%20Camilla%20Penzo%20and%20Fran%C3%A7ois%20Ta%C3%AFani%20and%20Gilles%20Tr%C3%A9dan&entry.1292438233=%20%20The%20deployment%20of%20machine%20learning%20models%20in%20operational%20contexts%20represents%0Aa%20significant%20investment%20for%20any%20organisation.%20Consequently%2C%20the%20risk%20of%20these%0Amodels%20being%20misappropriated%20by%20competitors%20needs%20to%20be%20addressed.%20In%20recent%0Ayears%2C%20numerous%20proposals%20have%20been%20put%20forth%20to%20detect%20instances%20of%20model%0Astealing.%20However%2C%20these%20proposals%20operate%20under%20implicit%20and%20disparate%20data%0Aand%20model%20access%20assumptions%3B%20as%20a%20consequence%2C%20it%20remains%20unclear%20how%20they%20can%0Abe%20effectively%20compared%20to%20one%20another.%20Our%20evaluation%20shows%20that%20a%20simple%0Abaseline%20that%20we%20introduce%20performs%20on%20par%20with%20existing%20state-of-the-art%0Afingerprints%2C%20which%2C%20on%20the%20other%20hand%2C%20are%20much%20more%20complex.%20To%20uncover%20the%0Areasons%20behind%20this%20intriguing%20result%2C%20this%20paper%20introduces%20a%20systematic%0Aapproach%20to%20both%20the%20creation%20of%20model%20fingerprinting%20schemes%20and%20their%0Aevaluation%20benchmarks.%20By%20dividing%20model%20fingerprinting%20into%20three%20core%0Acomponents%20--%20Query%2C%20Representation%20and%20Detection%20%28QuRD%29%20--%20we%20are%20able%20to%0Aidentify%20%24%5Csim100%24%20previously%20unexplored%20QuRD%20combinations%20and%20gain%20insights%0Ainto%20their%20performance.%20Finally%2C%20we%20introduce%20a%20set%20of%20metrics%20to%20compare%20and%0Aguide%20the%20creation%20of%20more%20representative%20model%20stealing%20detection%20benchmarks.%0AOur%20approach%20reveals%20the%20need%20for%20more%20challenging%20benchmarks%20and%20a%20sound%0Acomparison%20with%20baselines.%20To%20foster%20the%20creation%20of%20new%20fingerprinting%20schemes%0Aand%20benchmarks%2C%20we%20open-source%20our%20fingerprinting%20toolbox.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13021v1&entry.124074799=Read"},
{"title": "Unleashing the Power of Pre-trained Language Models for Offline\n  Reinforcement Learning", "author": "Ruizhe Shi and Yuyao Liu and Yanjie Ze and Simon S. Du and Huazhe Xu", "abstract": "  Offline reinforcement learning (RL) aims to find a near-optimal policy using\npre-collected datasets. In real-world scenarios, data collection could be\ncostly and risky; therefore, offline RL becomes particularly challenging when\nthe in-domain data is limited. Given recent advances in Large Language Models\n(LLMs) and their few-shot learning prowess, this paper introduces\n$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\ngeneral framework based on Decision Transformers to effectively use pre-trained\nLanguage Models (LMs) for offline RL. Our framework highlights four crucial\ncomponents: (1) Initializing Decision Transformers with sequentially\npre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\nfull-weight fine-tuning, to combine the pre-trained knowledge from LMs and\nin-domain knowledge effectively, (3) using the non-linear MLP transformation\ninstead of linear projections, to generate embeddings, and (4) integrating an\nauxiliary language prediction loss during fine-tuning to stabilize the LMs and\nretain their original abilities on languages. Empirical results indicate\n$\\textbf{LaMo}$ achieves excellent performance in sparse-reward tasks and\ncloses the gap between value-based offline RL methods and decision transformers\nin dense-reward tasks. In particular, our method demonstrates superior\nperformance in scenarios with limited data samples.\n", "link": "http://arxiv.org/abs/2310.20587v5", "date": "2024-12-17", "relevancy": 2.4197, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4932}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4842}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Power%20of%20Pre-trained%20Language%20Models%20for%20Offline%0A%20%20Reinforcement%20Learning&body=Title%3A%20Unleashing%20the%20Power%20of%20Pre-trained%20Language%20Models%20for%20Offline%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Ruizhe%20Shi%20and%20Yuyao%20Liu%20and%20Yanjie%20Ze%20and%20Simon%20S.%20Du%20and%20Huazhe%20Xu%0AAbstract%3A%20%20%20Offline%20reinforcement%20learning%20%28RL%29%20aims%20to%20find%20a%20near-optimal%20policy%20using%0Apre-collected%20datasets.%20In%20real-world%20scenarios%2C%20data%20collection%20could%20be%0Acostly%20and%20risky%3B%20therefore%2C%20offline%20RL%20becomes%20particularly%20challenging%20when%0Athe%20in-domain%20data%20is%20limited.%20Given%20recent%20advances%20in%20Large%20Language%20Models%0A%28LLMs%29%20and%20their%20few-shot%20learning%20prowess%2C%20this%20paper%20introduces%0A%24%5Ctextbf%7BLa%7D%24nguage%20Models%20for%20%24%5Ctextbf%7BMo%7D%24tion%20Control%20%28%24%5Ctextbf%7BLaMo%7D%24%29%2C%20a%0Ageneral%20framework%20based%20on%20Decision%20Transformers%20to%20effectively%20use%20pre-trained%0ALanguage%20Models%20%28LMs%29%20for%20offline%20RL.%20Our%20framework%20highlights%20four%20crucial%0Acomponents%3A%20%281%29%20Initializing%20Decision%20Transformers%20with%20sequentially%0Apre-trained%20LMs%2C%20%282%29%20employing%20the%20LoRA%20fine-tuning%20method%2C%20in%20contrast%20to%0Afull-weight%20fine-tuning%2C%20to%20combine%20the%20pre-trained%20knowledge%20from%20LMs%20and%0Ain-domain%20knowledge%20effectively%2C%20%283%29%20using%20the%20non-linear%20MLP%20transformation%0Ainstead%20of%20linear%20projections%2C%20to%20generate%20embeddings%2C%20and%20%284%29%20integrating%20an%0Aauxiliary%20language%20prediction%20loss%20during%20fine-tuning%20to%20stabilize%20the%20LMs%20and%0Aretain%20their%20original%20abilities%20on%20languages.%20Empirical%20results%20indicate%0A%24%5Ctextbf%7BLaMo%7D%24%20achieves%20excellent%20performance%20in%20sparse-reward%20tasks%20and%0Acloses%20the%20gap%20between%20value-based%20offline%20RL%20methods%20and%20decision%20transformers%0Ain%20dense-reward%20tasks.%20In%20particular%2C%20our%20method%20demonstrates%20superior%0Aperformance%20in%20scenarios%20with%20limited%20data%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20587v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Power%2520of%2520Pre-trained%2520Language%2520Models%2520for%2520Offline%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DRuizhe%2520Shi%2520and%2520Yuyao%2520Liu%2520and%2520Yanjie%2520Ze%2520and%2520Simon%2520S.%2520Du%2520and%2520Huazhe%2520Xu%26entry.1292438233%3D%2520%2520Offline%2520reinforcement%2520learning%2520%2528RL%2529%2520aims%2520to%2520find%2520a%2520near-optimal%2520policy%2520using%250Apre-collected%2520datasets.%2520In%2520real-world%2520scenarios%252C%2520data%2520collection%2520could%2520be%250Acostly%2520and%2520risky%253B%2520therefore%252C%2520offline%2520RL%2520becomes%2520particularly%2520challenging%2520when%250Athe%2520in-domain%2520data%2520is%2520limited.%2520Given%2520recent%2520advances%2520in%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520and%2520their%2520few-shot%2520learning%2520prowess%252C%2520this%2520paper%2520introduces%250A%2524%255Ctextbf%257BLa%257D%2524nguage%2520Models%2520for%2520%2524%255Ctextbf%257BMo%257D%2524tion%2520Control%2520%2528%2524%255Ctextbf%257BLaMo%257D%2524%2529%252C%2520a%250Ageneral%2520framework%2520based%2520on%2520Decision%2520Transformers%2520to%2520effectively%2520use%2520pre-trained%250ALanguage%2520Models%2520%2528LMs%2529%2520for%2520offline%2520RL.%2520Our%2520framework%2520highlights%2520four%2520crucial%250Acomponents%253A%2520%25281%2529%2520Initializing%2520Decision%2520Transformers%2520with%2520sequentially%250Apre-trained%2520LMs%252C%2520%25282%2529%2520employing%2520the%2520LoRA%2520fine-tuning%2520method%252C%2520in%2520contrast%2520to%250Afull-weight%2520fine-tuning%252C%2520to%2520combine%2520the%2520pre-trained%2520knowledge%2520from%2520LMs%2520and%250Ain-domain%2520knowledge%2520effectively%252C%2520%25283%2529%2520using%2520the%2520non-linear%2520MLP%2520transformation%250Ainstead%2520of%2520linear%2520projections%252C%2520to%2520generate%2520embeddings%252C%2520and%2520%25284%2529%2520integrating%2520an%250Aauxiliary%2520language%2520prediction%2520loss%2520during%2520fine-tuning%2520to%2520stabilize%2520the%2520LMs%2520and%250Aretain%2520their%2520original%2520abilities%2520on%2520languages.%2520Empirical%2520results%2520indicate%250A%2524%255Ctextbf%257BLaMo%257D%2524%2520achieves%2520excellent%2520performance%2520in%2520sparse-reward%2520tasks%2520and%250Acloses%2520the%2520gap%2520between%2520value-based%2520offline%2520RL%2520methods%2520and%2520decision%2520transformers%250Ain%2520dense-reward%2520tasks.%2520In%2520particular%252C%2520our%2520method%2520demonstrates%2520superior%250Aperformance%2520in%2520scenarios%2520with%2520limited%2520data%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.20587v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Power%20of%20Pre-trained%20Language%20Models%20for%20Offline%0A%20%20Reinforcement%20Learning&entry.906535625=Ruizhe%20Shi%20and%20Yuyao%20Liu%20and%20Yanjie%20Ze%20and%20Simon%20S.%20Du%20and%20Huazhe%20Xu&entry.1292438233=%20%20Offline%20reinforcement%20learning%20%28RL%29%20aims%20to%20find%20a%20near-optimal%20policy%20using%0Apre-collected%20datasets.%20In%20real-world%20scenarios%2C%20data%20collection%20could%20be%0Acostly%20and%20risky%3B%20therefore%2C%20offline%20RL%20becomes%20particularly%20challenging%20when%0Athe%20in-domain%20data%20is%20limited.%20Given%20recent%20advances%20in%20Large%20Language%20Models%0A%28LLMs%29%20and%20their%20few-shot%20learning%20prowess%2C%20this%20paper%20introduces%0A%24%5Ctextbf%7BLa%7D%24nguage%20Models%20for%20%24%5Ctextbf%7BMo%7D%24tion%20Control%20%28%24%5Ctextbf%7BLaMo%7D%24%29%2C%20a%0Ageneral%20framework%20based%20on%20Decision%20Transformers%20to%20effectively%20use%20pre-trained%0ALanguage%20Models%20%28LMs%29%20for%20offline%20RL.%20Our%20framework%20highlights%20four%20crucial%0Acomponents%3A%20%281%29%20Initializing%20Decision%20Transformers%20with%20sequentially%0Apre-trained%20LMs%2C%20%282%29%20employing%20the%20LoRA%20fine-tuning%20method%2C%20in%20contrast%20to%0Afull-weight%20fine-tuning%2C%20to%20combine%20the%20pre-trained%20knowledge%20from%20LMs%20and%0Ain-domain%20knowledge%20effectively%2C%20%283%29%20using%20the%20non-linear%20MLP%20transformation%0Ainstead%20of%20linear%20projections%2C%20to%20generate%20embeddings%2C%20and%20%284%29%20integrating%20an%0Aauxiliary%20language%20prediction%20loss%20during%20fine-tuning%20to%20stabilize%20the%20LMs%20and%0Aretain%20their%20original%20abilities%20on%20languages.%20Empirical%20results%20indicate%0A%24%5Ctextbf%7BLaMo%7D%24%20achieves%20excellent%20performance%20in%20sparse-reward%20tasks%20and%0Acloses%20the%20gap%20between%20value-based%20offline%20RL%20methods%20and%20decision%20transformers%0Ain%20dense-reward%20tasks.%20In%20particular%2C%20our%20method%20demonstrates%20superior%0Aperformance%20in%20scenarios%20with%20limited%20data%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20587v5&entry.124074799=Read"},
{"title": "MotionBridge: Dynamic Video Inbetweening with Flexible Controls", "author": "Maham Tanveer and Yang Zhou and Simon Niklaus and Ali Mahdavi Amiri and Hao Zhang and Krishna Kumar Singh and Nanxuan Zhao", "abstract": "  By generating plausible and smooth transitions between two image frames,\nvideo inbetweening is an essential tool for video editing and long video\nsynthesis. Traditional works lack the capability to generate complex large\nmotions. While recent video generation techniques are powerful in creating\nhigh-quality results, they often lack fine control over the details of\nintermediate frames, which can lead to results that do not align with the\ncreative mind. We introduce MotionBridge, a unified video inbetweening\nframework that allows flexible controls, including trajectory strokes,\nkeyframes, masks, guide pixels, and text. However, learning such multi-modal\ncontrols in a unified framework is a challenging task. We thus design two\ngenerators to extract the control signal faithfully and encode feature through\ndual-branch embedders to resolve ambiguities. We further introduce a curriculum\ntraining strategy to smoothly learn various controls. Extensive qualitative and\nquantitative experiments have demonstrated that such multi-modal controls\nenable a more dynamic, customizable, and contextually accurate visual\nnarrative.\n", "link": "http://arxiv.org/abs/2412.13190v1", "date": "2024-12-17", "relevancy": 2.3536, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6234}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.589}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionBridge%3A%20Dynamic%20Video%20Inbetweening%20with%20Flexible%20Controls&body=Title%3A%20MotionBridge%3A%20Dynamic%20Video%20Inbetweening%20with%20Flexible%20Controls%0AAuthor%3A%20Maham%20Tanveer%20and%20Yang%20Zhou%20and%20Simon%20Niklaus%20and%20Ali%20Mahdavi%20Amiri%20and%20Hao%20Zhang%20and%20Krishna%20Kumar%20Singh%20and%20Nanxuan%20Zhao%0AAbstract%3A%20%20%20By%20generating%20plausible%20and%20smooth%20transitions%20between%20two%20image%20frames%2C%0Avideo%20inbetweening%20is%20an%20essential%20tool%20for%20video%20editing%20and%20long%20video%0Asynthesis.%20Traditional%20works%20lack%20the%20capability%20to%20generate%20complex%20large%0Amotions.%20While%20recent%20video%20generation%20techniques%20are%20powerful%20in%20creating%0Ahigh-quality%20results%2C%20they%20often%20lack%20fine%20control%20over%20the%20details%20of%0Aintermediate%20frames%2C%20which%20can%20lead%20to%20results%20that%20do%20not%20align%20with%20the%0Acreative%20mind.%20We%20introduce%20MotionBridge%2C%20a%20unified%20video%20inbetweening%0Aframework%20that%20allows%20flexible%20controls%2C%20including%20trajectory%20strokes%2C%0Akeyframes%2C%20masks%2C%20guide%20pixels%2C%20and%20text.%20However%2C%20learning%20such%20multi-modal%0Acontrols%20in%20a%20unified%20framework%20is%20a%20challenging%20task.%20We%20thus%20design%20two%0Agenerators%20to%20extract%20the%20control%20signal%20faithfully%20and%20encode%20feature%20through%0Adual-branch%20embedders%20to%20resolve%20ambiguities.%20We%20further%20introduce%20a%20curriculum%0Atraining%20strategy%20to%20smoothly%20learn%20various%20controls.%20Extensive%20qualitative%20and%0Aquantitative%20experiments%20have%20demonstrated%20that%20such%20multi-modal%20controls%0Aenable%20a%20more%20dynamic%2C%20customizable%2C%20and%20contextually%20accurate%20visual%0Anarrative.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionBridge%253A%2520Dynamic%2520Video%2520Inbetweening%2520with%2520Flexible%2520Controls%26entry.906535625%3DMaham%2520Tanveer%2520and%2520Yang%2520Zhou%2520and%2520Simon%2520Niklaus%2520and%2520Ali%2520Mahdavi%2520Amiri%2520and%2520Hao%2520Zhang%2520and%2520Krishna%2520Kumar%2520Singh%2520and%2520Nanxuan%2520Zhao%26entry.1292438233%3D%2520%2520By%2520generating%2520plausible%2520and%2520smooth%2520transitions%2520between%2520two%2520image%2520frames%252C%250Avideo%2520inbetweening%2520is%2520an%2520essential%2520tool%2520for%2520video%2520editing%2520and%2520long%2520video%250Asynthesis.%2520Traditional%2520works%2520lack%2520the%2520capability%2520to%2520generate%2520complex%2520large%250Amotions.%2520While%2520recent%2520video%2520generation%2520techniques%2520are%2520powerful%2520in%2520creating%250Ahigh-quality%2520results%252C%2520they%2520often%2520lack%2520fine%2520control%2520over%2520the%2520details%2520of%250Aintermediate%2520frames%252C%2520which%2520can%2520lead%2520to%2520results%2520that%2520do%2520not%2520align%2520with%2520the%250Acreative%2520mind.%2520We%2520introduce%2520MotionBridge%252C%2520a%2520unified%2520video%2520inbetweening%250Aframework%2520that%2520allows%2520flexible%2520controls%252C%2520including%2520trajectory%2520strokes%252C%250Akeyframes%252C%2520masks%252C%2520guide%2520pixels%252C%2520and%2520text.%2520However%252C%2520learning%2520such%2520multi-modal%250Acontrols%2520in%2520a%2520unified%2520framework%2520is%2520a%2520challenging%2520task.%2520We%2520thus%2520design%2520two%250Agenerators%2520to%2520extract%2520the%2520control%2520signal%2520faithfully%2520and%2520encode%2520feature%2520through%250Adual-branch%2520embedders%2520to%2520resolve%2520ambiguities.%2520We%2520further%2520introduce%2520a%2520curriculum%250Atraining%2520strategy%2520to%2520smoothly%2520learn%2520various%2520controls.%2520Extensive%2520qualitative%2520and%250Aquantitative%2520experiments%2520have%2520demonstrated%2520that%2520such%2520multi-modal%2520controls%250Aenable%2520a%2520more%2520dynamic%252C%2520customizable%252C%2520and%2520contextually%2520accurate%2520visual%250Anarrative.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionBridge%3A%20Dynamic%20Video%20Inbetweening%20with%20Flexible%20Controls&entry.906535625=Maham%20Tanveer%20and%20Yang%20Zhou%20and%20Simon%20Niklaus%20and%20Ali%20Mahdavi%20Amiri%20and%20Hao%20Zhang%20and%20Krishna%20Kumar%20Singh%20and%20Nanxuan%20Zhao&entry.1292438233=%20%20By%20generating%20plausible%20and%20smooth%20transitions%20between%20two%20image%20frames%2C%0Avideo%20inbetweening%20is%20an%20essential%20tool%20for%20video%20editing%20and%20long%20video%0Asynthesis.%20Traditional%20works%20lack%20the%20capability%20to%20generate%20complex%20large%0Amotions.%20While%20recent%20video%20generation%20techniques%20are%20powerful%20in%20creating%0Ahigh-quality%20results%2C%20they%20often%20lack%20fine%20control%20over%20the%20details%20of%0Aintermediate%20frames%2C%20which%20can%20lead%20to%20results%20that%20do%20not%20align%20with%20the%0Acreative%20mind.%20We%20introduce%20MotionBridge%2C%20a%20unified%20video%20inbetweening%0Aframework%20that%20allows%20flexible%20controls%2C%20including%20trajectory%20strokes%2C%0Akeyframes%2C%20masks%2C%20guide%20pixels%2C%20and%20text.%20However%2C%20learning%20such%20multi-modal%0Acontrols%20in%20a%20unified%20framework%20is%20a%20challenging%20task.%20We%20thus%20design%20two%0Agenerators%20to%20extract%20the%20control%20signal%20faithfully%20and%20encode%20feature%20through%0Adual-branch%20embedders%20to%20resolve%20ambiguities.%20We%20further%20introduce%20a%20curriculum%0Atraining%20strategy%20to%20smoothly%20learn%20various%20controls.%20Extensive%20qualitative%20and%0Aquantitative%20experiments%20have%20demonstrated%20that%20such%20multi-modal%20controls%0Aenable%20a%20more%20dynamic%2C%20customizable%2C%20and%20contextually%20accurate%20visual%0Anarrative.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13190v1&entry.124074799=Read"},
{"title": "Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation", "author": "Huaijin Pi and Ruoxi Guo and Zehong Shen and Qing Shuai and Zechen Hu and Zhumei Wang and Yajiao Dong and Ruizhen Hu and Taku Komura and Sida Peng and Xiaowei Zhou", "abstract": "  Text-driven human motion synthesis is capturing significant attention for its\nability to effortlessly generate intricate movements from abstract text cues,\nshowcasing its potential for revolutionizing motion design not only in film\nnarratives but also in virtual reality experiences and computer game\ndevelopment. Existing methods often rely on 3D motion capture data, which\nrequire special setups resulting in higher costs for data acquisition,\nultimately limiting the diversity and scope of human motion. In contrast, 2D\nhuman videos offer a vast and accessible source of motion data, covering a\nwider range of styles and activities. In this paper, we explore leveraging 2D\nhuman motion extracted from videos as an alternative data source to improve\ntext-driven 3D motion generation. Our approach introduces a novel framework\nthat disentangles local joint motion from global movements, enabling efficient\nlearning of local motion priors from 2D data. We first train a single-view 2D\nlocal motion generator on a large dataset of text-motion pairs. To enhance this\nmodel to synthesize 3D motion, we fine-tune the generator with 3D data,\ntransforming it into a multi-view generator that predicts view-consistent local\njoint motion and root dynamics. Experiments on the HumanML3D dataset and novel\ntext prompts demonstrate that our method efficiently utilizes 2D data,\nsupporting realistic 3D human motion generation and broadening the range of\nmotion types it supports. Our code will be made publicly available at\nhttps://zju3dv.github.io/Motion-2-to-3/.\n", "link": "http://arxiv.org/abs/2412.13111v1", "date": "2024-12-17", "relevancy": 2.3429, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6289}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5815}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-2-to-3%3A%20Leveraging%202D%20Motion%20Data%20to%20Boost%203D%20Motion%20Generation&body=Title%3A%20Motion-2-to-3%3A%20Leveraging%202D%20Motion%20Data%20to%20Boost%203D%20Motion%20Generation%0AAuthor%3A%20Huaijin%20Pi%20and%20Ruoxi%20Guo%20and%20Zehong%20Shen%20and%20Qing%20Shuai%20and%20Zechen%20Hu%20and%20Zhumei%20Wang%20and%20Yajiao%20Dong%20and%20Ruizhen%20Hu%20and%20Taku%20Komura%20and%20Sida%20Peng%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20Text-driven%20human%20motion%20synthesis%20is%20capturing%20significant%20attention%20for%20its%0Aability%20to%20effortlessly%20generate%20intricate%20movements%20from%20abstract%20text%20cues%2C%0Ashowcasing%20its%20potential%20for%20revolutionizing%20motion%20design%20not%20only%20in%20film%0Anarratives%20but%20also%20in%20virtual%20reality%20experiences%20and%20computer%20game%0Adevelopment.%20Existing%20methods%20often%20rely%20on%203D%20motion%20capture%20data%2C%20which%0Arequire%20special%20setups%20resulting%20in%20higher%20costs%20for%20data%20acquisition%2C%0Aultimately%20limiting%20the%20diversity%20and%20scope%20of%20human%20motion.%20In%20contrast%2C%202D%0Ahuman%20videos%20offer%20a%20vast%20and%20accessible%20source%20of%20motion%20data%2C%20covering%20a%0Awider%20range%20of%20styles%20and%20activities.%20In%20this%20paper%2C%20we%20explore%20leveraging%202D%0Ahuman%20motion%20extracted%20from%20videos%20as%20an%20alternative%20data%20source%20to%20improve%0Atext-driven%203D%20motion%20generation.%20Our%20approach%20introduces%20a%20novel%20framework%0Athat%20disentangles%20local%20joint%20motion%20from%20global%20movements%2C%20enabling%20efficient%0Alearning%20of%20local%20motion%20priors%20from%202D%20data.%20We%20first%20train%20a%20single-view%202D%0Alocal%20motion%20generator%20on%20a%20large%20dataset%20of%20text-motion%20pairs.%20To%20enhance%20this%0Amodel%20to%20synthesize%203D%20motion%2C%20we%20fine-tune%20the%20generator%20with%203D%20data%2C%0Atransforming%20it%20into%20a%20multi-view%20generator%20that%20predicts%20view-consistent%20local%0Ajoint%20motion%20and%20root%20dynamics.%20Experiments%20on%20the%20HumanML3D%20dataset%20and%20novel%0Atext%20prompts%20demonstrate%20that%20our%20method%20efficiently%20utilizes%202D%20data%2C%0Asupporting%20realistic%203D%20human%20motion%20generation%20and%20broadening%20the%20range%20of%0Amotion%20types%20it%20supports.%20Our%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//zju3dv.github.io/Motion-2-to-3/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-2-to-3%253A%2520Leveraging%25202D%2520Motion%2520Data%2520to%2520Boost%25203D%2520Motion%2520Generation%26entry.906535625%3DHuaijin%2520Pi%2520and%2520Ruoxi%2520Guo%2520and%2520Zehong%2520Shen%2520and%2520Qing%2520Shuai%2520and%2520Zechen%2520Hu%2520and%2520Zhumei%2520Wang%2520and%2520Yajiao%2520Dong%2520and%2520Ruizhen%2520Hu%2520and%2520Taku%2520Komura%2520and%2520Sida%2520Peng%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520Text-driven%2520human%2520motion%2520synthesis%2520is%2520capturing%2520significant%2520attention%2520for%2520its%250Aability%2520to%2520effortlessly%2520generate%2520intricate%2520movements%2520from%2520abstract%2520text%2520cues%252C%250Ashowcasing%2520its%2520potential%2520for%2520revolutionizing%2520motion%2520design%2520not%2520only%2520in%2520film%250Anarratives%2520but%2520also%2520in%2520virtual%2520reality%2520experiences%2520and%2520computer%2520game%250Adevelopment.%2520Existing%2520methods%2520often%2520rely%2520on%25203D%2520motion%2520capture%2520data%252C%2520which%250Arequire%2520special%2520setups%2520resulting%2520in%2520higher%2520costs%2520for%2520data%2520acquisition%252C%250Aultimately%2520limiting%2520the%2520diversity%2520and%2520scope%2520of%2520human%2520motion.%2520In%2520contrast%252C%25202D%250Ahuman%2520videos%2520offer%2520a%2520vast%2520and%2520accessible%2520source%2520of%2520motion%2520data%252C%2520covering%2520a%250Awider%2520range%2520of%2520styles%2520and%2520activities.%2520In%2520this%2520paper%252C%2520we%2520explore%2520leveraging%25202D%250Ahuman%2520motion%2520extracted%2520from%2520videos%2520as%2520an%2520alternative%2520data%2520source%2520to%2520improve%250Atext-driven%25203D%2520motion%2520generation.%2520Our%2520approach%2520introduces%2520a%2520novel%2520framework%250Athat%2520disentangles%2520local%2520joint%2520motion%2520from%2520global%2520movements%252C%2520enabling%2520efficient%250Alearning%2520of%2520local%2520motion%2520priors%2520from%25202D%2520data.%2520We%2520first%2520train%2520a%2520single-view%25202D%250Alocal%2520motion%2520generator%2520on%2520a%2520large%2520dataset%2520of%2520text-motion%2520pairs.%2520To%2520enhance%2520this%250Amodel%2520to%2520synthesize%25203D%2520motion%252C%2520we%2520fine-tune%2520the%2520generator%2520with%25203D%2520data%252C%250Atransforming%2520it%2520into%2520a%2520multi-view%2520generator%2520that%2520predicts%2520view-consistent%2520local%250Ajoint%2520motion%2520and%2520root%2520dynamics.%2520Experiments%2520on%2520the%2520HumanML3D%2520dataset%2520and%2520novel%250Atext%2520prompts%2520demonstrate%2520that%2520our%2520method%2520efficiently%2520utilizes%25202D%2520data%252C%250Asupporting%2520realistic%25203D%2520human%2520motion%2520generation%2520and%2520broadening%2520the%2520range%2520of%250Amotion%2520types%2520it%2520supports.%2520Our%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//zju3dv.github.io/Motion-2-to-3/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-2-to-3%3A%20Leveraging%202D%20Motion%20Data%20to%20Boost%203D%20Motion%20Generation&entry.906535625=Huaijin%20Pi%20and%20Ruoxi%20Guo%20and%20Zehong%20Shen%20and%20Qing%20Shuai%20and%20Zechen%20Hu%20and%20Zhumei%20Wang%20and%20Yajiao%20Dong%20and%20Ruizhen%20Hu%20and%20Taku%20Komura%20and%20Sida%20Peng%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20Text-driven%20human%20motion%20synthesis%20is%20capturing%20significant%20attention%20for%20its%0Aability%20to%20effortlessly%20generate%20intricate%20movements%20from%20abstract%20text%20cues%2C%0Ashowcasing%20its%20potential%20for%20revolutionizing%20motion%20design%20not%20only%20in%20film%0Anarratives%20but%20also%20in%20virtual%20reality%20experiences%20and%20computer%20game%0Adevelopment.%20Existing%20methods%20often%20rely%20on%203D%20motion%20capture%20data%2C%20which%0Arequire%20special%20setups%20resulting%20in%20higher%20costs%20for%20data%20acquisition%2C%0Aultimately%20limiting%20the%20diversity%20and%20scope%20of%20human%20motion.%20In%20contrast%2C%202D%0Ahuman%20videos%20offer%20a%20vast%20and%20accessible%20source%20of%20motion%20data%2C%20covering%20a%0Awider%20range%20of%20styles%20and%20activities.%20In%20this%20paper%2C%20we%20explore%20leveraging%202D%0Ahuman%20motion%20extracted%20from%20videos%20as%20an%20alternative%20data%20source%20to%20improve%0Atext-driven%203D%20motion%20generation.%20Our%20approach%20introduces%20a%20novel%20framework%0Athat%20disentangles%20local%20joint%20motion%20from%20global%20movements%2C%20enabling%20efficient%0Alearning%20of%20local%20motion%20priors%20from%202D%20data.%20We%20first%20train%20a%20single-view%202D%0Alocal%20motion%20generator%20on%20a%20large%20dataset%20of%20text-motion%20pairs.%20To%20enhance%20this%0Amodel%20to%20synthesize%203D%20motion%2C%20we%20fine-tune%20the%20generator%20with%203D%20data%2C%0Atransforming%20it%20into%20a%20multi-view%20generator%20that%20predicts%20view-consistent%20local%0Ajoint%20motion%20and%20root%20dynamics.%20Experiments%20on%20the%20HumanML3D%20dataset%20and%20novel%0Atext%20prompts%20demonstrate%20that%20our%20method%20efficiently%20utilizes%202D%20data%2C%0Asupporting%20realistic%203D%20human%20motion%20generation%20and%20broadening%20the%20range%20of%0Amotion%20types%20it%20supports.%20Our%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//zju3dv.github.io/Motion-2-to-3/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13111v1&entry.124074799=Read"},
{"title": "Can Generative Models Improve Self-Supervised Representation Learning?", "author": "Sana Ayromlou and Vahid Reza Khazaie and Fereshteh Forghani and Arash Afkanpour", "abstract": "  The rapid advancement in self-supervised representation learning has\nhighlighted its potential to leverage unlabeled data for learning rich visual\nrepresentations. However, the existing techniques, particularly those employing\ndifferent augmentations of the same image, often rely on a limited set of\nsimple transformations that cannot fully capture variations in the real world.\nThis constrains the diversity and quality of samples, which leads to\nsub-optimal representations. In this paper, we introduce a framework that\nenriches the self-supervised learning (SSL) paradigm by utilizing generative\nmodels to produce semantically consistent image augmentations. By directly\nconditioning generative models on a source image, our method enables the\ngeneration of diverse augmentations while maintaining the semantics of the\nsource image, thus offering a richer set of data for SSL. Our extensive\nexperimental results on various joint-embedding SSL techniques demonstrate that\nour framework significantly enhances the quality of learned visual\nrepresentations by up to 10\\% Top-1 accuracy in downstream tasks. This research\ndemonstrates that incorporating generative models into the joint-embedding SSL\nworkflow opens new avenues for exploring the potential of synthetic data. This\ndevelopment paves the way for more robust and versatile representation learning\ntechniques.\n", "link": "http://arxiv.org/abs/2403.05966v3", "date": "2024-12-17", "relevancy": 2.3228, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5988}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5829}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Generative%20Models%20Improve%20Self-Supervised%20Representation%20Learning%3F&body=Title%3A%20Can%20Generative%20Models%20Improve%20Self-Supervised%20Representation%20Learning%3F%0AAuthor%3A%20Sana%20Ayromlou%20and%20Vahid%20Reza%20Khazaie%20and%20Fereshteh%20Forghani%20and%20Arash%20Afkanpour%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20self-supervised%20representation%20learning%20has%0Ahighlighted%20its%20potential%20to%20leverage%20unlabeled%20data%20for%20learning%20rich%20visual%0Arepresentations.%20However%2C%20the%20existing%20techniques%2C%20particularly%20those%20employing%0Adifferent%20augmentations%20of%20the%20same%20image%2C%20often%20rely%20on%20a%20limited%20set%20of%0Asimple%20transformations%20that%20cannot%20fully%20capture%20variations%20in%20the%20real%20world.%0AThis%20constrains%20the%20diversity%20and%20quality%20of%20samples%2C%20which%20leads%20to%0Asub-optimal%20representations.%20In%20this%20paper%2C%20we%20introduce%20a%20framework%20that%0Aenriches%20the%20self-supervised%20learning%20%28SSL%29%20paradigm%20by%20utilizing%20generative%0Amodels%20to%20produce%20semantically%20consistent%20image%20augmentations.%20By%20directly%0Aconditioning%20generative%20models%20on%20a%20source%20image%2C%20our%20method%20enables%20the%0Ageneration%20of%20diverse%20augmentations%20while%20maintaining%20the%20semantics%20of%20the%0Asource%20image%2C%20thus%20offering%20a%20richer%20set%20of%20data%20for%20SSL.%20Our%20extensive%0Aexperimental%20results%20on%20various%20joint-embedding%20SSL%20techniques%20demonstrate%20that%0Aour%20framework%20significantly%20enhances%20the%20quality%20of%20learned%20visual%0Arepresentations%20by%20up%20to%2010%5C%25%20Top-1%20accuracy%20in%20downstream%20tasks.%20This%20research%0Ademonstrates%20that%20incorporating%20generative%20models%20into%20the%20joint-embedding%20SSL%0Aworkflow%20opens%20new%20avenues%20for%20exploring%20the%20potential%20of%20synthetic%20data.%20This%0Adevelopment%20paves%20the%20way%20for%20more%20robust%20and%20versatile%20representation%20learning%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05966v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Generative%2520Models%2520Improve%2520Self-Supervised%2520Representation%2520Learning%253F%26entry.906535625%3DSana%2520Ayromlou%2520and%2520Vahid%2520Reza%2520Khazaie%2520and%2520Fereshteh%2520Forghani%2520and%2520Arash%2520Afkanpour%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520self-supervised%2520representation%2520learning%2520has%250Ahighlighted%2520its%2520potential%2520to%2520leverage%2520unlabeled%2520data%2520for%2520learning%2520rich%2520visual%250Arepresentations.%2520However%252C%2520the%2520existing%2520techniques%252C%2520particularly%2520those%2520employing%250Adifferent%2520augmentations%2520of%2520the%2520same%2520image%252C%2520often%2520rely%2520on%2520a%2520limited%2520set%2520of%250Asimple%2520transformations%2520that%2520cannot%2520fully%2520capture%2520variations%2520in%2520the%2520real%2520world.%250AThis%2520constrains%2520the%2520diversity%2520and%2520quality%2520of%2520samples%252C%2520which%2520leads%2520to%250Asub-optimal%2520representations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520framework%2520that%250Aenriches%2520the%2520self-supervised%2520learning%2520%2528SSL%2529%2520paradigm%2520by%2520utilizing%2520generative%250Amodels%2520to%2520produce%2520semantically%2520consistent%2520image%2520augmentations.%2520By%2520directly%250Aconditioning%2520generative%2520models%2520on%2520a%2520source%2520image%252C%2520our%2520method%2520enables%2520the%250Ageneration%2520of%2520diverse%2520augmentations%2520while%2520maintaining%2520the%2520semantics%2520of%2520the%250Asource%2520image%252C%2520thus%2520offering%2520a%2520richer%2520set%2520of%2520data%2520for%2520SSL.%2520Our%2520extensive%250Aexperimental%2520results%2520on%2520various%2520joint-embedding%2520SSL%2520techniques%2520demonstrate%2520that%250Aour%2520framework%2520significantly%2520enhances%2520the%2520quality%2520of%2520learned%2520visual%250Arepresentations%2520by%2520up%2520to%252010%255C%2525%2520Top-1%2520accuracy%2520in%2520downstream%2520tasks.%2520This%2520research%250Ademonstrates%2520that%2520incorporating%2520generative%2520models%2520into%2520the%2520joint-embedding%2520SSL%250Aworkflow%2520opens%2520new%2520avenues%2520for%2520exploring%2520the%2520potential%2520of%2520synthetic%2520data.%2520This%250Adevelopment%2520paves%2520the%2520way%2520for%2520more%2520robust%2520and%2520versatile%2520representation%2520learning%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05966v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Generative%20Models%20Improve%20Self-Supervised%20Representation%20Learning%3F&entry.906535625=Sana%20Ayromlou%20and%20Vahid%20Reza%20Khazaie%20and%20Fereshteh%20Forghani%20and%20Arash%20Afkanpour&entry.1292438233=%20%20The%20rapid%20advancement%20in%20self-supervised%20representation%20learning%20has%0Ahighlighted%20its%20potential%20to%20leverage%20unlabeled%20data%20for%20learning%20rich%20visual%0Arepresentations.%20However%2C%20the%20existing%20techniques%2C%20particularly%20those%20employing%0Adifferent%20augmentations%20of%20the%20same%20image%2C%20often%20rely%20on%20a%20limited%20set%20of%0Asimple%20transformations%20that%20cannot%20fully%20capture%20variations%20in%20the%20real%20world.%0AThis%20constrains%20the%20diversity%20and%20quality%20of%20samples%2C%20which%20leads%20to%0Asub-optimal%20representations.%20In%20this%20paper%2C%20we%20introduce%20a%20framework%20that%0Aenriches%20the%20self-supervised%20learning%20%28SSL%29%20paradigm%20by%20utilizing%20generative%0Amodels%20to%20produce%20semantically%20consistent%20image%20augmentations.%20By%20directly%0Aconditioning%20generative%20models%20on%20a%20source%20image%2C%20our%20method%20enables%20the%0Ageneration%20of%20diverse%20augmentations%20while%20maintaining%20the%20semantics%20of%20the%0Asource%20image%2C%20thus%20offering%20a%20richer%20set%20of%20data%20for%20SSL.%20Our%20extensive%0Aexperimental%20results%20on%20various%20joint-embedding%20SSL%20techniques%20demonstrate%20that%0Aour%20framework%20significantly%20enhances%20the%20quality%20of%20learned%20visual%0Arepresentations%20by%20up%20to%2010%5C%25%20Top-1%20accuracy%20in%20downstream%20tasks.%20This%20research%0Ademonstrates%20that%20incorporating%20generative%20models%20into%20the%20joint-embedding%20SSL%0Aworkflow%20opens%20new%20avenues%20for%20exploring%20the%20potential%20of%20synthetic%20data.%20This%0Adevelopment%20paves%20the%20way%20for%20more%20robust%20and%20versatile%20representation%20learning%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05966v3&entry.124074799=Read"},
{"title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback", "author": "Zaid Khan and Elias Stengel-Eskin and Jaemin Cho and Mohit Bansal", "abstract": "  The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.\n", "link": "http://arxiv.org/abs/2410.06215v2", "date": "2024-12-17", "relevancy": 2.3124, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.638}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5427}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataEnvGym%3A%20Data%20Generation%20Agents%20in%20Teacher%20Environments%20with%20Student%0A%20%20Feedback&body=Title%3A%20DataEnvGym%3A%20Data%20Generation%20Agents%20in%20Teacher%20Environments%20with%20Student%0A%20%20Feedback%0AAuthor%3A%20Zaid%20Khan%20and%20Elias%20Stengel-Eskin%20and%20Jaemin%20Cho%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20The%20process%20of%20creating%20training%20data%20to%20teach%20models%20is%20currently%20driven%20by%0Ahumans%2C%20who%20manually%20analyze%20model%20weaknesses%20and%20plan%20how%20to%20create%20data%20that%0Aimproves%20a%20student%20model.%20Approaches%20using%20LLMs%20as%20annotators%20reduce%20human%0Aeffort%2C%20but%20still%20require%20humans%20to%20interpret%20feedback%20from%20evaluations%20and%0Acontrol%20the%20LLM%20to%20produce%20data%20the%20student%20needs.%20Automating%20this%0Alabor-intensive%20process%20by%20creating%20autonomous%20data%20generation%20agents%20-%20or%0Ateachers%20-%20is%20desirable%2C%20but%20requires%20environments%20that%20can%20simulate%20the%0Afeedback-driven%2C%20iterative%2C%20closed%20loop%20of%20data%20creation.%20To%20enable%20rapid%2C%0Ascalable%20testing%20for%20such%20agents%20and%20their%20modules%2C%20we%20introduce%20DataEnvGym%2C%20a%0Atestbed%20of%20teacher%20environments%20for%20data%20generation%20agents.%20DataEnvGym%20frames%0Adata%20generation%20as%20a%20sequential%20decision-making%20task%2C%20involving%20an%20agent%0Aconsisting%20of%20a%20data%20generation%20policy%20%28which%20generates%20a%20plan%20for%20creating%0Atraining%20data%29%20and%20a%20data%20generation%20engine%20%28which%20transforms%20the%20plan%20into%0Adata%29%2C%20inside%20an%20environment%20that%20provides%20student%20feedback.%20The%20agent%27s%20goal%0Ais%20to%20improve%20student%20performance.%20Students%20are%20iteratively%20trained%20and%0Aevaluated%20on%20generated%20data%2C%20and%20their%20feedback%20%28in%20the%20form%20of%20errors%20or%20weak%0Askills%29%20is%20reported%20to%20the%20agent%20after%20each%20iteration.%20DataEnvGym%20includes%0Amultiple%20teacher%20environment%20instantiations%20across%203%20levels%20of%20structure%20in%20the%0Astate%20representation%20and%20action%20space.%20More%20structured%20environments%20are%20based%0Aon%20inferred%20skills%20and%20offer%20more%20interpretability%20and%20curriculum%20control.%20We%0Asupport%204%20domains%20%28math%2C%20code%2C%20VQA%2C%20and%20tool-use%29%20and%20test%20multiple%20students%0Aand%20teachers.%20Example%20agents%20in%20our%20teaching%20environments%20can%20iteratively%0Aimprove%20students%20across%20tasks%20and%20settings.%20Moreover%2C%20we%20show%20that%20environments%0Ateach%20different%20skill%20levels%20and%20test%20variants%20of%20key%20modules%2C%20pointing%20to%0Afuture%20work%20in%20improving%20data%20generation%20agents%2C%20engines%2C%20and%20feedback%0Amechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06215v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataEnvGym%253A%2520Data%2520Generation%2520Agents%2520in%2520Teacher%2520Environments%2520with%2520Student%250A%2520%2520Feedback%26entry.906535625%3DZaid%2520Khan%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Jaemin%2520Cho%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520The%2520process%2520of%2520creating%2520training%2520data%2520to%2520teach%2520models%2520is%2520currently%2520driven%2520by%250Ahumans%252C%2520who%2520manually%2520analyze%2520model%2520weaknesses%2520and%2520plan%2520how%2520to%2520create%2520data%2520that%250Aimproves%2520a%2520student%2520model.%2520Approaches%2520using%2520LLMs%2520as%2520annotators%2520reduce%2520human%250Aeffort%252C%2520but%2520still%2520require%2520humans%2520to%2520interpret%2520feedback%2520from%2520evaluations%2520and%250Acontrol%2520the%2520LLM%2520to%2520produce%2520data%2520the%2520student%2520needs.%2520Automating%2520this%250Alabor-intensive%2520process%2520by%2520creating%2520autonomous%2520data%2520generation%2520agents%2520-%2520or%250Ateachers%2520-%2520is%2520desirable%252C%2520but%2520requires%2520environments%2520that%2520can%2520simulate%2520the%250Afeedback-driven%252C%2520iterative%252C%2520closed%2520loop%2520of%2520data%2520creation.%2520To%2520enable%2520rapid%252C%250Ascalable%2520testing%2520for%2520such%2520agents%2520and%2520their%2520modules%252C%2520we%2520introduce%2520DataEnvGym%252C%2520a%250Atestbed%2520of%2520teacher%2520environments%2520for%2520data%2520generation%2520agents.%2520DataEnvGym%2520frames%250Adata%2520generation%2520as%2520a%2520sequential%2520decision-making%2520task%252C%2520involving%2520an%2520agent%250Aconsisting%2520of%2520a%2520data%2520generation%2520policy%2520%2528which%2520generates%2520a%2520plan%2520for%2520creating%250Atraining%2520data%2529%2520and%2520a%2520data%2520generation%2520engine%2520%2528which%2520transforms%2520the%2520plan%2520into%250Adata%2529%252C%2520inside%2520an%2520environment%2520that%2520provides%2520student%2520feedback.%2520The%2520agent%2527s%2520goal%250Ais%2520to%2520improve%2520student%2520performance.%2520Students%2520are%2520iteratively%2520trained%2520and%250Aevaluated%2520on%2520generated%2520data%252C%2520and%2520their%2520feedback%2520%2528in%2520the%2520form%2520of%2520errors%2520or%2520weak%250Askills%2529%2520is%2520reported%2520to%2520the%2520agent%2520after%2520each%2520iteration.%2520DataEnvGym%2520includes%250Amultiple%2520teacher%2520environment%2520instantiations%2520across%25203%2520levels%2520of%2520structure%2520in%2520the%250Astate%2520representation%2520and%2520action%2520space.%2520More%2520structured%2520environments%2520are%2520based%250Aon%2520inferred%2520skills%2520and%2520offer%2520more%2520interpretability%2520and%2520curriculum%2520control.%2520We%250Asupport%25204%2520domains%2520%2528math%252C%2520code%252C%2520VQA%252C%2520and%2520tool-use%2529%2520and%2520test%2520multiple%2520students%250Aand%2520teachers.%2520Example%2520agents%2520in%2520our%2520teaching%2520environments%2520can%2520iteratively%250Aimprove%2520students%2520across%2520tasks%2520and%2520settings.%2520Moreover%252C%2520we%2520show%2520that%2520environments%250Ateach%2520different%2520skill%2520levels%2520and%2520test%2520variants%2520of%2520key%2520modules%252C%2520pointing%2520to%250Afuture%2520work%2520in%2520improving%2520data%2520generation%2520agents%252C%2520engines%252C%2520and%2520feedback%250Amechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06215v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataEnvGym%3A%20Data%20Generation%20Agents%20in%20Teacher%20Environments%20with%20Student%0A%20%20Feedback&entry.906535625=Zaid%20Khan%20and%20Elias%20Stengel-Eskin%20and%20Jaemin%20Cho%20and%20Mohit%20Bansal&entry.1292438233=%20%20The%20process%20of%20creating%20training%20data%20to%20teach%20models%20is%20currently%20driven%20by%0Ahumans%2C%20who%20manually%20analyze%20model%20weaknesses%20and%20plan%20how%20to%20create%20data%20that%0Aimproves%20a%20student%20model.%20Approaches%20using%20LLMs%20as%20annotators%20reduce%20human%0Aeffort%2C%20but%20still%20require%20humans%20to%20interpret%20feedback%20from%20evaluations%20and%0Acontrol%20the%20LLM%20to%20produce%20data%20the%20student%20needs.%20Automating%20this%0Alabor-intensive%20process%20by%20creating%20autonomous%20data%20generation%20agents%20-%20or%0Ateachers%20-%20is%20desirable%2C%20but%20requires%20environments%20that%20can%20simulate%20the%0Afeedback-driven%2C%20iterative%2C%20closed%20loop%20of%20data%20creation.%20To%20enable%20rapid%2C%0Ascalable%20testing%20for%20such%20agents%20and%20their%20modules%2C%20we%20introduce%20DataEnvGym%2C%20a%0Atestbed%20of%20teacher%20environments%20for%20data%20generation%20agents.%20DataEnvGym%20frames%0Adata%20generation%20as%20a%20sequential%20decision-making%20task%2C%20involving%20an%20agent%0Aconsisting%20of%20a%20data%20generation%20policy%20%28which%20generates%20a%20plan%20for%20creating%0Atraining%20data%29%20and%20a%20data%20generation%20engine%20%28which%20transforms%20the%20plan%20into%0Adata%29%2C%20inside%20an%20environment%20that%20provides%20student%20feedback.%20The%20agent%27s%20goal%0Ais%20to%20improve%20student%20performance.%20Students%20are%20iteratively%20trained%20and%0Aevaluated%20on%20generated%20data%2C%20and%20their%20feedback%20%28in%20the%20form%20of%20errors%20or%20weak%0Askills%29%20is%20reported%20to%20the%20agent%20after%20each%20iteration.%20DataEnvGym%20includes%0Amultiple%20teacher%20environment%20instantiations%20across%203%20levels%20of%20structure%20in%20the%0Astate%20representation%20and%20action%20space.%20More%20structured%20environments%20are%20based%0Aon%20inferred%20skills%20and%20offer%20more%20interpretability%20and%20curriculum%20control.%20We%0Asupport%204%20domains%20%28math%2C%20code%2C%20VQA%2C%20and%20tool-use%29%20and%20test%20multiple%20students%0Aand%20teachers.%20Example%20agents%20in%20our%20teaching%20environments%20can%20iteratively%0Aimprove%20students%20across%20tasks%20and%20settings.%20Moreover%2C%20we%20show%20that%20environments%0Ateach%20different%20skill%20levels%20and%20test%20variants%20of%20key%20modules%2C%20pointing%20to%0Afuture%20work%20in%20improving%20data%20generation%20agents%2C%20engines%2C%20and%20feedback%0Amechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06215v2&entry.124074799=Read"},
{"title": "VidTok: A Versatile and Open-Source Video Tokenizer", "author": "Anni Tang and Tianyu He and Junliang Guo and Xinle Cheng and Li Song and Jiang Bian", "abstract": "  Encoding video content into compact latent tokens has become a fundamental\nstep in video generation and understanding, driven by the need to address the\ninherent redundancy in pixel-level representations. Consequently, there is a\ngrowing demand for high-performance, open-source video tokenizers as\nvideo-centric research gains prominence. We introduce VidTok, a versatile video\ntokenizer that delivers state-of-the-art performance in both continuous and\ndiscrete tokenizations. VidTok incorporates several key advancements over\nexisting approaches: 1) model architecture such as convolutional layers and\nup/downsampling modules; 2) to address the training instability and codebook\ncollapse commonly associated with conventional Vector Quantization (VQ), we\nintegrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)\nimproved training strategies, including a two-stage training process and the\nuse of reduced frame rates. By integrating these advancements, VidTok achieves\nsubstantial improvements over existing methods, demonstrating superior\nperformance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,\nunder standardized evaluation settings.\n", "link": "http://arxiv.org/abs/2412.13061v1", "date": "2024-12-17", "relevancy": 2.3025, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5927}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5688}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidTok%3A%20A%20Versatile%20and%20Open-Source%20Video%20Tokenizer&body=Title%3A%20VidTok%3A%20A%20Versatile%20and%20Open-Source%20Video%20Tokenizer%0AAuthor%3A%20Anni%20Tang%20and%20Tianyu%20He%20and%20Junliang%20Guo%20and%20Xinle%20Cheng%20and%20Li%20Song%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Encoding%20video%20content%20into%20compact%20latent%20tokens%20has%20become%20a%20fundamental%0Astep%20in%20video%20generation%20and%20understanding%2C%20driven%20by%20the%20need%20to%20address%20the%0Ainherent%20redundancy%20in%20pixel-level%20representations.%20Consequently%2C%20there%20is%20a%0Agrowing%20demand%20for%20high-performance%2C%20open-source%20video%20tokenizers%20as%0Avideo-centric%20research%20gains%20prominence.%20We%20introduce%20VidTok%2C%20a%20versatile%20video%0Atokenizer%20that%20delivers%20state-of-the-art%20performance%20in%20both%20continuous%20and%0Adiscrete%20tokenizations.%20VidTok%20incorporates%20several%20key%20advancements%20over%0Aexisting%20approaches%3A%201%29%20model%20architecture%20such%20as%20convolutional%20layers%20and%0Aup/downsampling%20modules%3B%202%29%20to%20address%20the%20training%20instability%20and%20codebook%0Acollapse%20commonly%20associated%20with%20conventional%20Vector%20Quantization%20%28VQ%29%2C%20we%0Aintegrate%20Finite%20Scalar%20Quantization%20%28FSQ%29%20into%20discrete%20video%20tokenization%3B%203%29%0Aimproved%20training%20strategies%2C%20including%20a%20two-stage%20training%20process%20and%20the%0Ause%20of%20reduced%20frame%20rates.%20By%20integrating%20these%20advancements%2C%20VidTok%20achieves%0Asubstantial%20improvements%20over%20existing%20methods%2C%20demonstrating%20superior%0Aperformance%20across%20multiple%20metrics%2C%20including%20PSNR%2C%20SSIM%2C%20LPIPS%2C%20and%20FVD%2C%0Aunder%20standardized%20evaluation%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidTok%253A%2520A%2520Versatile%2520and%2520Open-Source%2520Video%2520Tokenizer%26entry.906535625%3DAnni%2520Tang%2520and%2520Tianyu%2520He%2520and%2520Junliang%2520Guo%2520and%2520Xinle%2520Cheng%2520and%2520Li%2520Song%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Encoding%2520video%2520content%2520into%2520compact%2520latent%2520tokens%2520has%2520become%2520a%2520fundamental%250Astep%2520in%2520video%2520generation%2520and%2520understanding%252C%2520driven%2520by%2520the%2520need%2520to%2520address%2520the%250Ainherent%2520redundancy%2520in%2520pixel-level%2520representations.%2520Consequently%252C%2520there%2520is%2520a%250Agrowing%2520demand%2520for%2520high-performance%252C%2520open-source%2520video%2520tokenizers%2520as%250Avideo-centric%2520research%2520gains%2520prominence.%2520We%2520introduce%2520VidTok%252C%2520a%2520versatile%2520video%250Atokenizer%2520that%2520delivers%2520state-of-the-art%2520performance%2520in%2520both%2520continuous%2520and%250Adiscrete%2520tokenizations.%2520VidTok%2520incorporates%2520several%2520key%2520advancements%2520over%250Aexisting%2520approaches%253A%25201%2529%2520model%2520architecture%2520such%2520as%2520convolutional%2520layers%2520and%250Aup/downsampling%2520modules%253B%25202%2529%2520to%2520address%2520the%2520training%2520instability%2520and%2520codebook%250Acollapse%2520commonly%2520associated%2520with%2520conventional%2520Vector%2520Quantization%2520%2528VQ%2529%252C%2520we%250Aintegrate%2520Finite%2520Scalar%2520Quantization%2520%2528FSQ%2529%2520into%2520discrete%2520video%2520tokenization%253B%25203%2529%250Aimproved%2520training%2520strategies%252C%2520including%2520a%2520two-stage%2520training%2520process%2520and%2520the%250Ause%2520of%2520reduced%2520frame%2520rates.%2520By%2520integrating%2520these%2520advancements%252C%2520VidTok%2520achieves%250Asubstantial%2520improvements%2520over%2520existing%2520methods%252C%2520demonstrating%2520superior%250Aperformance%2520across%2520multiple%2520metrics%252C%2520including%2520PSNR%252C%2520SSIM%252C%2520LPIPS%252C%2520and%2520FVD%252C%250Aunder%2520standardized%2520evaluation%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidTok%3A%20A%20Versatile%20and%20Open-Source%20Video%20Tokenizer&entry.906535625=Anni%20Tang%20and%20Tianyu%20He%20and%20Junliang%20Guo%20and%20Xinle%20Cheng%20and%20Li%20Song%20and%20Jiang%20Bian&entry.1292438233=%20%20Encoding%20video%20content%20into%20compact%20latent%20tokens%20has%20become%20a%20fundamental%0Astep%20in%20video%20generation%20and%20understanding%2C%20driven%20by%20the%20need%20to%20address%20the%0Ainherent%20redundancy%20in%20pixel-level%20representations.%20Consequently%2C%20there%20is%20a%0Agrowing%20demand%20for%20high-performance%2C%20open-source%20video%20tokenizers%20as%0Avideo-centric%20research%20gains%20prominence.%20We%20introduce%20VidTok%2C%20a%20versatile%20video%0Atokenizer%20that%20delivers%20state-of-the-art%20performance%20in%20both%20continuous%20and%0Adiscrete%20tokenizations.%20VidTok%20incorporates%20several%20key%20advancements%20over%0Aexisting%20approaches%3A%201%29%20model%20architecture%20such%20as%20convolutional%20layers%20and%0Aup/downsampling%20modules%3B%202%29%20to%20address%20the%20training%20instability%20and%20codebook%0Acollapse%20commonly%20associated%20with%20conventional%20Vector%20Quantization%20%28VQ%29%2C%20we%0Aintegrate%20Finite%20Scalar%20Quantization%20%28FSQ%29%20into%20discrete%20video%20tokenization%3B%203%29%0Aimproved%20training%20strategies%2C%20including%20a%20two-stage%20training%20process%20and%20the%0Ause%20of%20reduced%20frame%20rates.%20By%20integrating%20these%20advancements%2C%20VidTok%20achieves%0Asubstantial%20improvements%20over%20existing%20methods%2C%20demonstrating%20superior%0Aperformance%20across%20multiple%20metrics%2C%20including%20PSNR%2C%20SSIM%2C%20LPIPS%2C%20and%20FVD%2C%0Aunder%20standardized%20evaluation%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13061v1&entry.124074799=Read"},
{"title": "Locate n' Rotate: Two-stage Openable Part Detection with Foundation\n  Model Priors", "author": "Siqi Li and Xiaoxue Chen and Haoyu Cheng and Guyue Zhou and Hao Zhao and Guanzhong Tian", "abstract": "  Detecting the openable parts of articulated objects is crucial for downstream\napplications in intelligent robotics, such as pulling a drawer. This task poses\na multitasking challenge due to the necessity of understanding object\ncategories and motion. Most existing methods are either category-specific or\ntrained on specific datasets, lacking generalization to unseen environments and\nobjects. In this paper, we propose a Transformer-based Openable Part Detection\n(OPD) framework named Multi-feature Openable Part Detection (MOPD) that\nincorporates perceptual grouping and geometric priors, outperforming previous\nmethods in performance. In the first stage of the framework, we introduce a\nperceptual grouping feature model that provides perceptual grouping feature\npriors for openable part detection, enhancing detection results through a\ncross-attention mechanism. In the second stage, a geometric understanding\nfeature model offers geometric feature priors for predicting motion parameters.\nCompared to existing methods, our proposed approach shows better performance in\nboth detection and motion parameter prediction. Codes and models are publicly\navailable at https://github.com/lisiqi-zju/MOPD\n", "link": "http://arxiv.org/abs/2412.13173v1", "date": "2024-12-17", "relevancy": 2.2723, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5908}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locate%20n%27%20Rotate%3A%20Two-stage%20Openable%20Part%20Detection%20with%20Foundation%0A%20%20Model%20Priors&body=Title%3A%20Locate%20n%27%20Rotate%3A%20Two-stage%20Openable%20Part%20Detection%20with%20Foundation%0A%20%20Model%20Priors%0AAuthor%3A%20Siqi%20Li%20and%20Xiaoxue%20Chen%20and%20Haoyu%20Cheng%20and%20Guyue%20Zhou%20and%20Hao%20Zhao%20and%20Guanzhong%20Tian%0AAbstract%3A%20%20%20Detecting%20the%20openable%20parts%20of%20articulated%20objects%20is%20crucial%20for%20downstream%0Aapplications%20in%20intelligent%20robotics%2C%20such%20as%20pulling%20a%20drawer.%20This%20task%20poses%0Aa%20multitasking%20challenge%20due%20to%20the%20necessity%20of%20understanding%20object%0Acategories%20and%20motion.%20Most%20existing%20methods%20are%20either%20category-specific%20or%0Atrained%20on%20specific%20datasets%2C%20lacking%20generalization%20to%20unseen%20environments%20and%0Aobjects.%20In%20this%20paper%2C%20we%20propose%20a%20Transformer-based%20Openable%20Part%20Detection%0A%28OPD%29%20framework%20named%20Multi-feature%20Openable%20Part%20Detection%20%28MOPD%29%20that%0Aincorporates%20perceptual%20grouping%20and%20geometric%20priors%2C%20outperforming%20previous%0Amethods%20in%20performance.%20In%20the%20first%20stage%20of%20the%20framework%2C%20we%20introduce%20a%0Aperceptual%20grouping%20feature%20model%20that%20provides%20perceptual%20grouping%20feature%0Apriors%20for%20openable%20part%20detection%2C%20enhancing%20detection%20results%20through%20a%0Across-attention%20mechanism.%20In%20the%20second%20stage%2C%20a%20geometric%20understanding%0Afeature%20model%20offers%20geometric%20feature%20priors%20for%20predicting%20motion%20parameters.%0ACompared%20to%20existing%20methods%2C%20our%20proposed%20approach%20shows%20better%20performance%20in%0Aboth%20detection%20and%20motion%20parameter%20prediction.%20Codes%20and%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/lisiqi-zju/MOPD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocate%2520n%2527%2520Rotate%253A%2520Two-stage%2520Openable%2520Part%2520Detection%2520with%2520Foundation%250A%2520%2520Model%2520Priors%26entry.906535625%3DSiqi%2520Li%2520and%2520Xiaoxue%2520Chen%2520and%2520Haoyu%2520Cheng%2520and%2520Guyue%2520Zhou%2520and%2520Hao%2520Zhao%2520and%2520Guanzhong%2520Tian%26entry.1292438233%3D%2520%2520Detecting%2520the%2520openable%2520parts%2520of%2520articulated%2520objects%2520is%2520crucial%2520for%2520downstream%250Aapplications%2520in%2520intelligent%2520robotics%252C%2520such%2520as%2520pulling%2520a%2520drawer.%2520This%2520task%2520poses%250Aa%2520multitasking%2520challenge%2520due%2520to%2520the%2520necessity%2520of%2520understanding%2520object%250Acategories%2520and%2520motion.%2520Most%2520existing%2520methods%2520are%2520either%2520category-specific%2520or%250Atrained%2520on%2520specific%2520datasets%252C%2520lacking%2520generalization%2520to%2520unseen%2520environments%2520and%250Aobjects.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Transformer-based%2520Openable%2520Part%2520Detection%250A%2528OPD%2529%2520framework%2520named%2520Multi-feature%2520Openable%2520Part%2520Detection%2520%2528MOPD%2529%2520that%250Aincorporates%2520perceptual%2520grouping%2520and%2520geometric%2520priors%252C%2520outperforming%2520previous%250Amethods%2520in%2520performance.%2520In%2520the%2520first%2520stage%2520of%2520the%2520framework%252C%2520we%2520introduce%2520a%250Aperceptual%2520grouping%2520feature%2520model%2520that%2520provides%2520perceptual%2520grouping%2520feature%250Apriors%2520for%2520openable%2520part%2520detection%252C%2520enhancing%2520detection%2520results%2520through%2520a%250Across-attention%2520mechanism.%2520In%2520the%2520second%2520stage%252C%2520a%2520geometric%2520understanding%250Afeature%2520model%2520offers%2520geometric%2520feature%2520priors%2520for%2520predicting%2520motion%2520parameters.%250ACompared%2520to%2520existing%2520methods%252C%2520our%2520proposed%2520approach%2520shows%2520better%2520performance%2520in%250Aboth%2520detection%2520and%2520motion%2520parameter%2520prediction.%2520Codes%2520and%2520models%2520are%2520publicly%250Aavailable%2520at%2520https%253A//github.com/lisiqi-zju/MOPD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locate%20n%27%20Rotate%3A%20Two-stage%20Openable%20Part%20Detection%20with%20Foundation%0A%20%20Model%20Priors&entry.906535625=Siqi%20Li%20and%20Xiaoxue%20Chen%20and%20Haoyu%20Cheng%20and%20Guyue%20Zhou%20and%20Hao%20Zhao%20and%20Guanzhong%20Tian&entry.1292438233=%20%20Detecting%20the%20openable%20parts%20of%20articulated%20objects%20is%20crucial%20for%20downstream%0Aapplications%20in%20intelligent%20robotics%2C%20such%20as%20pulling%20a%20drawer.%20This%20task%20poses%0Aa%20multitasking%20challenge%20due%20to%20the%20necessity%20of%20understanding%20object%0Acategories%20and%20motion.%20Most%20existing%20methods%20are%20either%20category-specific%20or%0Atrained%20on%20specific%20datasets%2C%20lacking%20generalization%20to%20unseen%20environments%20and%0Aobjects.%20In%20this%20paper%2C%20we%20propose%20a%20Transformer-based%20Openable%20Part%20Detection%0A%28OPD%29%20framework%20named%20Multi-feature%20Openable%20Part%20Detection%20%28MOPD%29%20that%0Aincorporates%20perceptual%20grouping%20and%20geometric%20priors%2C%20outperforming%20previous%0Amethods%20in%20performance.%20In%20the%20first%20stage%20of%20the%20framework%2C%20we%20introduce%20a%0Aperceptual%20grouping%20feature%20model%20that%20provides%20perceptual%20grouping%20feature%0Apriors%20for%20openable%20part%20detection%2C%20enhancing%20detection%20results%20through%20a%0Across-attention%20mechanism.%20In%20the%20second%20stage%2C%20a%20geometric%20understanding%0Afeature%20model%20offers%20geometric%20feature%20priors%20for%20predicting%20motion%20parameters.%0ACompared%20to%20existing%20methods%2C%20our%20proposed%20approach%20shows%20better%20performance%20in%0Aboth%20detection%20and%20motion%20parameter%20prediction.%20Codes%20and%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/lisiqi-zju/MOPD%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13173v1&entry.124074799=Read"},
{"title": "A New Adversarial Perspective for LiDAR-based 3D Object Detection", "author": "Shijun Zheng and Weiquan Liu and Yu Guo and Yu Zang and Siqi Shen and Cheng Wang", "abstract": "  Autonomous vehicles (AVs) rely on LiDAR sensors for environmental perception\nand decision-making in driving scenarios. However, ensuring the safety and\nreliability of AVs in complex environments remains a pressing challenge. To\naddress this issue, we introduce a real-world dataset (ROLiD) comprising\nLiDAR-scanned point clouds of two random objects: water mist and smoke. In this\npaper, we introduce a novel adversarial perspective by proposing an attack\nframework that utilizes water mist and smoke to simulate environmental\ninterference. Specifically, we propose a point cloud sequence generation method\nusing a motion and content decomposition generative adversarial network named\nPCS-GAN to simulate the distribution of random objects. Furthermore, leveraging\nthe simulated LiDAR scanning characteristics implemented with Range Image, we\nexamine the effects of introducing random object perturbations at various\npositions on the target vehicle. Extensive experiments demonstrate that\nadversarial perturbations based on random objects effectively deceive vehicle\ndetection and reduce the recognition rate of 3D object detection models.\n", "link": "http://arxiv.org/abs/2412.13017v1", "date": "2024-12-17", "relevancy": 2.248, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5803}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5741}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Adversarial%20Perspective%20for%20LiDAR-based%203D%20Object%20Detection&body=Title%3A%20A%20New%20Adversarial%20Perspective%20for%20LiDAR-based%203D%20Object%20Detection%0AAuthor%3A%20Shijun%20Zheng%20and%20Weiquan%20Liu%20and%20Yu%20Guo%20and%20Yu%20Zang%20and%20Siqi%20Shen%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20Autonomous%20vehicles%20%28AVs%29%20rely%20on%20LiDAR%20sensors%20for%20environmental%20perception%0Aand%20decision-making%20in%20driving%20scenarios.%20However%2C%20ensuring%20the%20safety%20and%0Areliability%20of%20AVs%20in%20complex%20environments%20remains%20a%20pressing%20challenge.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20a%20real-world%20dataset%20%28ROLiD%29%20comprising%0ALiDAR-scanned%20point%20clouds%20of%20two%20random%20objects%3A%20water%20mist%20and%20smoke.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20adversarial%20perspective%20by%20proposing%20an%20attack%0Aframework%20that%20utilizes%20water%20mist%20and%20smoke%20to%20simulate%20environmental%0Ainterference.%20Specifically%2C%20we%20propose%20a%20point%20cloud%20sequence%20generation%20method%0Ausing%20a%20motion%20and%20content%20decomposition%20generative%20adversarial%20network%20named%0APCS-GAN%20to%20simulate%20the%20distribution%20of%20random%20objects.%20Furthermore%2C%20leveraging%0Athe%20simulated%20LiDAR%20scanning%20characteristics%20implemented%20with%20Range%20Image%2C%20we%0Aexamine%20the%20effects%20of%20introducing%20random%20object%20perturbations%20at%20various%0Apositions%20on%20the%20target%20vehicle.%20Extensive%20experiments%20demonstrate%20that%0Aadversarial%20perturbations%20based%20on%20random%20objects%20effectively%20deceive%20vehicle%0Adetection%20and%20reduce%20the%20recognition%20rate%20of%203D%20object%20detection%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Adversarial%2520Perspective%2520for%2520LiDAR-based%25203D%2520Object%2520Detection%26entry.906535625%3DShijun%2520Zheng%2520and%2520Weiquan%2520Liu%2520and%2520Yu%2520Guo%2520and%2520Yu%2520Zang%2520and%2520Siqi%2520Shen%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520%2528AVs%2529%2520rely%2520on%2520LiDAR%2520sensors%2520for%2520environmental%2520perception%250Aand%2520decision-making%2520in%2520driving%2520scenarios.%2520However%252C%2520ensuring%2520the%2520safety%2520and%250Areliability%2520of%2520AVs%2520in%2520complex%2520environments%2520remains%2520a%2520pressing%2520challenge.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520introduce%2520a%2520real-world%2520dataset%2520%2528ROLiD%2529%2520comprising%250ALiDAR-scanned%2520point%2520clouds%2520of%2520two%2520random%2520objects%253A%2520water%2520mist%2520and%2520smoke.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520adversarial%2520perspective%2520by%2520proposing%2520an%2520attack%250Aframework%2520that%2520utilizes%2520water%2520mist%2520and%2520smoke%2520to%2520simulate%2520environmental%250Ainterference.%2520Specifically%252C%2520we%2520propose%2520a%2520point%2520cloud%2520sequence%2520generation%2520method%250Ausing%2520a%2520motion%2520and%2520content%2520decomposition%2520generative%2520adversarial%2520network%2520named%250APCS-GAN%2520to%2520simulate%2520the%2520distribution%2520of%2520random%2520objects.%2520Furthermore%252C%2520leveraging%250Athe%2520simulated%2520LiDAR%2520scanning%2520characteristics%2520implemented%2520with%2520Range%2520Image%252C%2520we%250Aexamine%2520the%2520effects%2520of%2520introducing%2520random%2520object%2520perturbations%2520at%2520various%250Apositions%2520on%2520the%2520target%2520vehicle.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aadversarial%2520perturbations%2520based%2520on%2520random%2520objects%2520effectively%2520deceive%2520vehicle%250Adetection%2520and%2520reduce%2520the%2520recognition%2520rate%2520of%25203D%2520object%2520detection%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Adversarial%20Perspective%20for%20LiDAR-based%203D%20Object%20Detection&entry.906535625=Shijun%20Zheng%20and%20Weiquan%20Liu%20and%20Yu%20Guo%20and%20Yu%20Zang%20and%20Siqi%20Shen%20and%20Cheng%20Wang&entry.1292438233=%20%20Autonomous%20vehicles%20%28AVs%29%20rely%20on%20LiDAR%20sensors%20for%20environmental%20perception%0Aand%20decision-making%20in%20driving%20scenarios.%20However%2C%20ensuring%20the%20safety%20and%0Areliability%20of%20AVs%20in%20complex%20environments%20remains%20a%20pressing%20challenge.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20a%20real-world%20dataset%20%28ROLiD%29%20comprising%0ALiDAR-scanned%20point%20clouds%20of%20two%20random%20objects%3A%20water%20mist%20and%20smoke.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20adversarial%20perspective%20by%20proposing%20an%20attack%0Aframework%20that%20utilizes%20water%20mist%20and%20smoke%20to%20simulate%20environmental%0Ainterference.%20Specifically%2C%20we%20propose%20a%20point%20cloud%20sequence%20generation%20method%0Ausing%20a%20motion%20and%20content%20decomposition%20generative%20adversarial%20network%20named%0APCS-GAN%20to%20simulate%20the%20distribution%20of%20random%20objects.%20Furthermore%2C%20leveraging%0Athe%20simulated%20LiDAR%20scanning%20characteristics%20implemented%20with%20Range%20Image%2C%20we%0Aexamine%20the%20effects%20of%20introducing%20random%20object%20perturbations%20at%20various%0Apositions%20on%20the%20target%20vehicle.%20Extensive%20experiments%20demonstrate%20that%0Aadversarial%20perturbations%20based%20on%20random%20objects%20effectively%20deceive%20vehicle%0Adetection%20and%20reduce%20the%20recognition%20rate%20of%203D%20object%20detection%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13017v1&entry.124074799=Read"},
{"title": "HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction", "author": "Chen Bao and Jiarui Xu and Xiaolong Wang and Abhinav Gupta and Homanga Bharadhwaj", "abstract": "  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results \\url{https://www.chenbao.tech/handsonvlm/}\n", "link": "http://arxiv.org/abs/2412.13187v1", "date": "2024-12-17", "relevancy": 2.2411, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5939}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HandsOnVLM%3A%20Vision-Language%20Models%20for%20Hand-Object%20Interaction%0A%20%20Prediction&body=Title%3A%20HandsOnVLM%3A%20Vision-Language%20Models%20for%20Hand-Object%20Interaction%0A%20%20Prediction%0AAuthor%3A%20Chen%20Bao%20and%20Jiarui%20Xu%20and%20Xiaolong%20Wang%20and%20Abhinav%20Gupta%20and%20Homanga%20Bharadhwaj%0AAbstract%3A%20%20%20How%20can%20we%20predict%20future%20interaction%20trajectories%20of%20human%20hands%20in%20a%20scene%0Agiven%20high-level%20colloquial%20task%20specifications%20in%20the%20form%20of%20natural%0Alanguage%3F%20In%20this%20paper%2C%20we%20extend%20the%20classic%20hand%20trajectory%20prediction%20task%0Ato%20two%20tasks%20involving%20explicit%20or%20implicit%20language%20queries.%20Our%20proposed%0Atasks%20require%20extensive%20understanding%20of%20human%20daily%20activities%20and%20reasoning%0Aabilities%20about%20what%20should%20be%20happening%20next%20given%20cues%20from%20the%20current%0Ascene.%20We%20also%20develop%20new%20benchmarks%20to%20evaluate%20the%20proposed%20two%20tasks%2C%0AVanilla%20Hand%20Prediction%20%28VHP%29%20and%20Reasoning-Based%20Hand%20Prediction%20%28RBHP%29.%20We%0Aenable%20solving%20these%20tasks%20by%20integrating%20high-level%20world%20knowledge%20and%0Areasoning%20capabilities%20of%20Vision-Language%20Models%20%28VLMs%29%20with%20the%0Aauto-regressive%20nature%20of%20low-level%20ego-centric%20hand%20trajectories.%20Our%20model%2C%0AHandsOnVLM%20is%20a%20novel%20VLM%20that%20can%20generate%20textual%20responses%20and%20produce%0Afuture%20hand%20trajectories%20through%20natural-language%20conversations.%20Our%0Aexperiments%20show%20that%20HandsOnVLM%20outperforms%20existing%20task-specific%20methods%20and%0Aother%20VLM%20baselines%20on%20proposed%20tasks%2C%20and%20demonstrates%20its%20ability%20to%0Aeffectively%20utilize%20world%20knowledge%20for%20reasoning%20about%20low-level%20human%20hand%0Atrajectories%20based%20on%20the%20provided%20context.%20Our%20website%20contains%20code%20and%0Adetailed%20video%20results%20%5Curl%7Bhttps%3A//www.chenbao.tech/handsonvlm/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandsOnVLM%253A%2520Vision-Language%2520Models%2520for%2520Hand-Object%2520Interaction%250A%2520%2520Prediction%26entry.906535625%3DChen%2520Bao%2520and%2520Jiarui%2520Xu%2520and%2520Xiaolong%2520Wang%2520and%2520Abhinav%2520Gupta%2520and%2520Homanga%2520Bharadhwaj%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520predict%2520future%2520interaction%2520trajectories%2520of%2520human%2520hands%2520in%2520a%2520scene%250Agiven%2520high-level%2520colloquial%2520task%2520specifications%2520in%2520the%2520form%2520of%2520natural%250Alanguage%253F%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520classic%2520hand%2520trajectory%2520prediction%2520task%250Ato%2520two%2520tasks%2520involving%2520explicit%2520or%2520implicit%2520language%2520queries.%2520Our%2520proposed%250Atasks%2520require%2520extensive%2520understanding%2520of%2520human%2520daily%2520activities%2520and%2520reasoning%250Aabilities%2520about%2520what%2520should%2520be%2520happening%2520next%2520given%2520cues%2520from%2520the%2520current%250Ascene.%2520We%2520also%2520develop%2520new%2520benchmarks%2520to%2520evaluate%2520the%2520proposed%2520two%2520tasks%252C%250AVanilla%2520Hand%2520Prediction%2520%2528VHP%2529%2520and%2520Reasoning-Based%2520Hand%2520Prediction%2520%2528RBHP%2529.%2520We%250Aenable%2520solving%2520these%2520tasks%2520by%2520integrating%2520high-level%2520world%2520knowledge%2520and%250Areasoning%2520capabilities%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520with%2520the%250Aauto-regressive%2520nature%2520of%2520low-level%2520ego-centric%2520hand%2520trajectories.%2520Our%2520model%252C%250AHandsOnVLM%2520is%2520a%2520novel%2520VLM%2520that%2520can%2520generate%2520textual%2520responses%2520and%2520produce%250Afuture%2520hand%2520trajectories%2520through%2520natural-language%2520conversations.%2520Our%250Aexperiments%2520show%2520that%2520HandsOnVLM%2520outperforms%2520existing%2520task-specific%2520methods%2520and%250Aother%2520VLM%2520baselines%2520on%2520proposed%2520tasks%252C%2520and%2520demonstrates%2520its%2520ability%2520to%250Aeffectively%2520utilize%2520world%2520knowledge%2520for%2520reasoning%2520about%2520low-level%2520human%2520hand%250Atrajectories%2520based%2520on%2520the%2520provided%2520context.%2520Our%2520website%2520contains%2520code%2520and%250Adetailed%2520video%2520results%2520%255Curl%257Bhttps%253A//www.chenbao.tech/handsonvlm/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HandsOnVLM%3A%20Vision-Language%20Models%20for%20Hand-Object%20Interaction%0A%20%20Prediction&entry.906535625=Chen%20Bao%20and%20Jiarui%20Xu%20and%20Xiaolong%20Wang%20and%20Abhinav%20Gupta%20and%20Homanga%20Bharadhwaj&entry.1292438233=%20%20How%20can%20we%20predict%20future%20interaction%20trajectories%20of%20human%20hands%20in%20a%20scene%0Agiven%20high-level%20colloquial%20task%20specifications%20in%20the%20form%20of%20natural%0Alanguage%3F%20In%20this%20paper%2C%20we%20extend%20the%20classic%20hand%20trajectory%20prediction%20task%0Ato%20two%20tasks%20involving%20explicit%20or%20implicit%20language%20queries.%20Our%20proposed%0Atasks%20require%20extensive%20understanding%20of%20human%20daily%20activities%20and%20reasoning%0Aabilities%20about%20what%20should%20be%20happening%20next%20given%20cues%20from%20the%20current%0Ascene.%20We%20also%20develop%20new%20benchmarks%20to%20evaluate%20the%20proposed%20two%20tasks%2C%0AVanilla%20Hand%20Prediction%20%28VHP%29%20and%20Reasoning-Based%20Hand%20Prediction%20%28RBHP%29.%20We%0Aenable%20solving%20these%20tasks%20by%20integrating%20high-level%20world%20knowledge%20and%0Areasoning%20capabilities%20of%20Vision-Language%20Models%20%28VLMs%29%20with%20the%0Aauto-regressive%20nature%20of%20low-level%20ego-centric%20hand%20trajectories.%20Our%20model%2C%0AHandsOnVLM%20is%20a%20novel%20VLM%20that%20can%20generate%20textual%20responses%20and%20produce%0Afuture%20hand%20trajectories%20through%20natural-language%20conversations.%20Our%0Aexperiments%20show%20that%20HandsOnVLM%20outperforms%20existing%20task-specific%20methods%20and%0Aother%20VLM%20baselines%20on%20proposed%20tasks%2C%20and%20demonstrates%20its%20ability%20to%0Aeffectively%20utilize%20world%20knowledge%20for%20reasoning%20about%20low-level%20human%20hand%0Atrajectories%20based%20on%20the%20provided%20context.%20Our%20website%20contains%20code%20and%0Adetailed%20video%20results%20%5Curl%7Bhttps%3A//www.chenbao.tech/handsonvlm/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13187v1&entry.124074799=Read"},
{"title": "Equity in the Use of ChatGPT for the Classroom: A Comparison of the\n  Accuracy and Precision of ChatGPT 3.5 vs. ChatGPT4 with Respect to Statistics\n  and Data Science Exams", "author": "Monnie McGee and Bivin Sadler", "abstract": "  A college education historically has been seen as method of moving upward\nwith regards to income brackets and social status. Indeed, many colleges\nrecognize this connection and seek to enroll talented low income students.\nWhile these students might have their education, books, room, and board paid;\nthere are other items that they might be expected to use that are not part of\nmost college scholarship packages. One of those items that has recently\nsurfaced is access to generative AI platforms. The most popular of these\nplatforms is ChatGPT, and it has a paid version (ChatGPT4) and a free version\n(ChatGPT3.5). We seek to explore differences in the free and paid versions in\nthe context of homework questions and data analyses as might be seen in a\ntypical introductory statistics course. We determine the extent to which\nstudents who cannot afford newer and faster versions of generative AI programs\nwould be disadvantaged in terms of writing such projects and learning these\nmethods.\n", "link": "http://arxiv.org/abs/2412.13116v1", "date": "2024-12-17", "relevancy": 2.2287, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4776}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4315}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equity%20in%20the%20Use%20of%20ChatGPT%20for%20the%20Classroom%3A%20A%20Comparison%20of%20the%0A%20%20Accuracy%20and%20Precision%20of%20ChatGPT%203.5%20vs.%20ChatGPT4%20with%20Respect%20to%20Statistics%0A%20%20and%20Data%20Science%20Exams&body=Title%3A%20Equity%20in%20the%20Use%20of%20ChatGPT%20for%20the%20Classroom%3A%20A%20Comparison%20of%20the%0A%20%20Accuracy%20and%20Precision%20of%20ChatGPT%203.5%20vs.%20ChatGPT4%20with%20Respect%20to%20Statistics%0A%20%20and%20Data%20Science%20Exams%0AAuthor%3A%20Monnie%20McGee%20and%20Bivin%20Sadler%0AAbstract%3A%20%20%20A%20college%20education%20historically%20has%20been%20seen%20as%20method%20of%20moving%20upward%0Awith%20regards%20to%20income%20brackets%20and%20social%20status.%20Indeed%2C%20many%20colleges%0Arecognize%20this%20connection%20and%20seek%20to%20enroll%20talented%20low%20income%20students.%0AWhile%20these%20students%20might%20have%20their%20education%2C%20books%2C%20room%2C%20and%20board%20paid%3B%0Athere%20are%20other%20items%20that%20they%20might%20be%20expected%20to%20use%20that%20are%20not%20part%20of%0Amost%20college%20scholarship%20packages.%20One%20of%20those%20items%20that%20has%20recently%0Asurfaced%20is%20access%20to%20generative%20AI%20platforms.%20The%20most%20popular%20of%20these%0Aplatforms%20is%20ChatGPT%2C%20and%20it%20has%20a%20paid%20version%20%28ChatGPT4%29%20and%20a%20free%20version%0A%28ChatGPT3.5%29.%20We%20seek%20to%20explore%20differences%20in%20the%20free%20and%20paid%20versions%20in%0Athe%20context%20of%20homework%20questions%20and%20data%20analyses%20as%20might%20be%20seen%20in%20a%0Atypical%20introductory%20statistics%20course.%20We%20determine%20the%20extent%20to%20which%0Astudents%20who%20cannot%20afford%20newer%20and%20faster%20versions%20of%20generative%20AI%20programs%0Awould%20be%20disadvantaged%20in%20terms%20of%20writing%20such%20projects%20and%20learning%20these%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquity%2520in%2520the%2520Use%2520of%2520ChatGPT%2520for%2520the%2520Classroom%253A%2520A%2520Comparison%2520of%2520the%250A%2520%2520Accuracy%2520and%2520Precision%2520of%2520ChatGPT%25203.5%2520vs.%2520ChatGPT4%2520with%2520Respect%2520to%2520Statistics%250A%2520%2520and%2520Data%2520Science%2520Exams%26entry.906535625%3DMonnie%2520McGee%2520and%2520Bivin%2520Sadler%26entry.1292438233%3D%2520%2520A%2520college%2520education%2520historically%2520has%2520been%2520seen%2520as%2520method%2520of%2520moving%2520upward%250Awith%2520regards%2520to%2520income%2520brackets%2520and%2520social%2520status.%2520Indeed%252C%2520many%2520colleges%250Arecognize%2520this%2520connection%2520and%2520seek%2520to%2520enroll%2520talented%2520low%2520income%2520students.%250AWhile%2520these%2520students%2520might%2520have%2520their%2520education%252C%2520books%252C%2520room%252C%2520and%2520board%2520paid%253B%250Athere%2520are%2520other%2520items%2520that%2520they%2520might%2520be%2520expected%2520to%2520use%2520that%2520are%2520not%2520part%2520of%250Amost%2520college%2520scholarship%2520packages.%2520One%2520of%2520those%2520items%2520that%2520has%2520recently%250Asurfaced%2520is%2520access%2520to%2520generative%2520AI%2520platforms.%2520The%2520most%2520popular%2520of%2520these%250Aplatforms%2520is%2520ChatGPT%252C%2520and%2520it%2520has%2520a%2520paid%2520version%2520%2528ChatGPT4%2529%2520and%2520a%2520free%2520version%250A%2528ChatGPT3.5%2529.%2520We%2520seek%2520to%2520explore%2520differences%2520in%2520the%2520free%2520and%2520paid%2520versions%2520in%250Athe%2520context%2520of%2520homework%2520questions%2520and%2520data%2520analyses%2520as%2520might%2520be%2520seen%2520in%2520a%250Atypical%2520introductory%2520statistics%2520course.%2520We%2520determine%2520the%2520extent%2520to%2520which%250Astudents%2520who%2520cannot%2520afford%2520newer%2520and%2520faster%2520versions%2520of%2520generative%2520AI%2520programs%250Awould%2520be%2520disadvantaged%2520in%2520terms%2520of%2520writing%2520such%2520projects%2520and%2520learning%2520these%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equity%20in%20the%20Use%20of%20ChatGPT%20for%20the%20Classroom%3A%20A%20Comparison%20of%20the%0A%20%20Accuracy%20and%20Precision%20of%20ChatGPT%203.5%20vs.%20ChatGPT4%20with%20Respect%20to%20Statistics%0A%20%20and%20Data%20Science%20Exams&entry.906535625=Monnie%20McGee%20and%20Bivin%20Sadler&entry.1292438233=%20%20A%20college%20education%20historically%20has%20been%20seen%20as%20method%20of%20moving%20upward%0Awith%20regards%20to%20income%20brackets%20and%20social%20status.%20Indeed%2C%20many%20colleges%0Arecognize%20this%20connection%20and%20seek%20to%20enroll%20talented%20low%20income%20students.%0AWhile%20these%20students%20might%20have%20their%20education%2C%20books%2C%20room%2C%20and%20board%20paid%3B%0Athere%20are%20other%20items%20that%20they%20might%20be%20expected%20to%20use%20that%20are%20not%20part%20of%0Amost%20college%20scholarship%20packages.%20One%20of%20those%20items%20that%20has%20recently%0Asurfaced%20is%20access%20to%20generative%20AI%20platforms.%20The%20most%20popular%20of%20these%0Aplatforms%20is%20ChatGPT%2C%20and%20it%20has%20a%20paid%20version%20%28ChatGPT4%29%20and%20a%20free%20version%0A%28ChatGPT3.5%29.%20We%20seek%20to%20explore%20differences%20in%20the%20free%20and%20paid%20versions%20in%0Athe%20context%20of%20homework%20questions%20and%20data%20analyses%20as%20might%20be%20seen%20in%20a%0Atypical%20introductory%20statistics%20course.%20We%20determine%20the%20extent%20to%20which%0Astudents%20who%20cannot%20afford%20newer%20and%20faster%20versions%20of%20generative%20AI%20programs%0Awould%20be%20disadvantaged%20in%20terms%20of%20writing%20such%20projects%20and%20learning%20these%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13116v1&entry.124074799=Read"},
{"title": "TKAN: Temporal Kolmogorov-Arnold Networks", "author": "Remi Genet and Hugo Inzirillo", "abstract": "  Recurrent Neural Networks (RNNs) have revolutionized many areas of machine\nlearning, particularly in natural language and data sequence processing. Long\nShort-Term Memory (LSTM) has demonstrated its ability to capture long-term\ndependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks\n(KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed\na new neural networks architecture inspired by KAN and the LSTM, the Temporal\nKolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both\nnetworks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers\nembedding memory management. This innovation enables us to perform multi-step\ntime series forecasting with enhanced accuracy and efficiency. By addressing\nthe limitations of traditional models in handling complex sequential patterns,\nthe TKAN architecture offers significant potential for advancements in fields\nrequiring more than one step ahead forecasting.\n", "link": "http://arxiv.org/abs/2405.07344v3", "date": "2024-12-17", "relevancy": 2.1913, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4429}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4382}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TKAN%3A%20Temporal%20Kolmogorov-Arnold%20Networks&body=Title%3A%20TKAN%3A%20Temporal%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Remi%20Genet%20and%20Hugo%20Inzirillo%0AAbstract%3A%20%20%20Recurrent%20Neural%20Networks%20%28RNNs%29%20have%20revolutionized%20many%20areas%20of%20machine%0Alearning%2C%20particularly%20in%20natural%20language%20and%20data%20sequence%20processing.%20Long%0AShort-Term%20Memory%20%28LSTM%29%20has%20demonstrated%20its%20ability%20to%20capture%20long-term%0Adependencies%20in%20sequential%20data.%20Inspired%20by%20the%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%20a%20promising%20alternatives%20to%20Multi-Layer%20Perceptrons%20%28MLPs%29%2C%20we%20proposed%0Aa%20new%20neural%20networks%20architecture%20inspired%20by%20KAN%20and%20the%20LSTM%2C%20the%20Temporal%0AKolomogorov-Arnold%20Networks%20%28TKANs%29.%20TKANs%20combined%20the%20strenght%20of%20both%0Anetworks%2C%20it%20is%20composed%20of%20Recurring%20Kolmogorov-Arnold%20Networks%20%28RKANs%29%20Layers%0Aembedding%20memory%20management.%20This%20innovation%20enables%20us%20to%20perform%20multi-step%0Atime%20series%20forecasting%20with%20enhanced%20accuracy%20and%20efficiency.%20By%20addressing%0Athe%20limitations%20of%20traditional%20models%20in%20handling%20complex%20sequential%20patterns%2C%0Athe%20TKAN%20architecture%20offers%20significant%20potential%20for%20advancements%20in%20fields%0Arequiring%20more%20than%20one%20step%20ahead%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTKAN%253A%2520Temporal%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DRemi%2520Genet%2520and%2520Hugo%2520Inzirillo%26entry.1292438233%3D%2520%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520have%2520revolutionized%2520many%2520areas%2520of%2520machine%250Alearning%252C%2520particularly%2520in%2520natural%2520language%2520and%2520data%2520sequence%2520processing.%2520Long%250AShort-Term%2520Memory%2520%2528LSTM%2529%2520has%2520demonstrated%2520its%2520ability%2520to%2520capture%2520long-term%250Adependencies%2520in%2520sequential%2520data.%2520Inspired%2520by%2520the%2520Kolmogorov-Arnold%2520Networks%250A%2528KANs%2529%2520a%2520promising%2520alternatives%2520to%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%252C%2520we%2520proposed%250Aa%2520new%2520neural%2520networks%2520architecture%2520inspired%2520by%2520KAN%2520and%2520the%2520LSTM%252C%2520the%2520Temporal%250AKolomogorov-Arnold%2520Networks%2520%2528TKANs%2529.%2520TKANs%2520combined%2520the%2520strenght%2520of%2520both%250Anetworks%252C%2520it%2520is%2520composed%2520of%2520Recurring%2520Kolmogorov-Arnold%2520Networks%2520%2528RKANs%2529%2520Layers%250Aembedding%2520memory%2520management.%2520This%2520innovation%2520enables%2520us%2520to%2520perform%2520multi-step%250Atime%2520series%2520forecasting%2520with%2520enhanced%2520accuracy%2520and%2520efficiency.%2520By%2520addressing%250Athe%2520limitations%2520of%2520traditional%2520models%2520in%2520handling%2520complex%2520sequential%2520patterns%252C%250Athe%2520TKAN%2520architecture%2520offers%2520significant%2520potential%2520for%2520advancements%2520in%2520fields%250Arequiring%2520more%2520than%2520one%2520step%2520ahead%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TKAN%3A%20Temporal%20Kolmogorov-Arnold%20Networks&entry.906535625=Remi%20Genet%20and%20Hugo%20Inzirillo&entry.1292438233=%20%20Recurrent%20Neural%20Networks%20%28RNNs%29%20have%20revolutionized%20many%20areas%20of%20machine%0Alearning%2C%20particularly%20in%20natural%20language%20and%20data%20sequence%20processing.%20Long%0AShort-Term%20Memory%20%28LSTM%29%20has%20demonstrated%20its%20ability%20to%20capture%20long-term%0Adependencies%20in%20sequential%20data.%20Inspired%20by%20the%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%20a%20promising%20alternatives%20to%20Multi-Layer%20Perceptrons%20%28MLPs%29%2C%20we%20proposed%0Aa%20new%20neural%20networks%20architecture%20inspired%20by%20KAN%20and%20the%20LSTM%2C%20the%20Temporal%0AKolomogorov-Arnold%20Networks%20%28TKANs%29.%20TKANs%20combined%20the%20strenght%20of%20both%0Anetworks%2C%20it%20is%20composed%20of%20Recurring%20Kolmogorov-Arnold%20Networks%20%28RKANs%29%20Layers%0Aembedding%20memory%20management.%20This%20innovation%20enables%20us%20to%20perform%20multi-step%0Atime%20series%20forecasting%20with%20enhanced%20accuracy%20and%20efficiency.%20By%20addressing%0Athe%20limitations%20of%20traditional%20models%20in%20handling%20complex%20sequential%20patterns%2C%0Athe%20TKAN%20architecture%20offers%20significant%20potential%20for%20advancements%20in%20fields%0Arequiring%20more%20than%20one%20step%20ahead%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07344v3&entry.124074799=Read"},
{"title": "Unlocking the Potential of Digital Pathology: Novel Baselines for\n  Compression", "author": "Maximilian Fischer and Peter Neher and Peter Sch\u00fcffler and Sebastian Ziegler and Shuhan Xiao and Robin Peretzke and David Clunie and Constantin Ulrich and Michael Baumgartner and Alexander Muckenhuber and Silvia Dias Almeida and Michael G\u00f6tz and Jens Kleesiek and Marco Nolden and Rickmer Braren and Klaus Maier-Hein", "abstract": "  Digital pathology offers a groundbreaking opportunity to transform clinical\npractice in histopathological image analysis, yet faces a significant hurdle:\nthe substantial file sizes of pathological Whole Slide Images (WSI). While\ncurrent digital pathology solutions rely on lossy JPEG compression to address\nthis issue, lossy compression can introduce color and texture disparities,\npotentially impacting clinical decision-making. While prior research addresses\nperceptual image quality and downstream performance independently of each\nother, we jointly evaluate compression schemes for perceptual and downstream\ntask quality on four different datasets. In addition, we collect an initially\nuncompressed dataset for an unbiased perceptual evaluation of compression\nschemes. Our results show that deep learning models fine-tuned for perceptual\nquality outperform conventional compression schemes like JPEG-XL or WebP for\nfurther compression of WSI. However, they exhibit a significant bias towards\nthe compression artifacts present in the training data and struggle to\ngeneralize across various compression schemes. We introduce a novel evaluation\nmetric based on feature similarity between original files and compressed files\nthat aligns very well with the actual downstream performance on the compressed\nWSI. Our metric allows for a general and standardized evaluation of lossy\ncompression schemes and mitigates the requirement to independently assess\ndifferent downstream tasks. Our study provides novel insights for the\nassessment of lossy compression schemes for WSI and encourages a unified\nevaluation of lossy compression schemes to accelerate the clinical uptake of\ndigital pathology.\n", "link": "http://arxiv.org/abs/2412.13137v1", "date": "2024-12-17", "relevancy": 2.1828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20the%20Potential%20of%20Digital%20Pathology%3A%20Novel%20Baselines%20for%0A%20%20Compression&body=Title%3A%20Unlocking%20the%20Potential%20of%20Digital%20Pathology%3A%20Novel%20Baselines%20for%0A%20%20Compression%0AAuthor%3A%20Maximilian%20Fischer%20and%20Peter%20Neher%20and%20Peter%20Sch%C3%BCffler%20and%20Sebastian%20Ziegler%20and%20Shuhan%20Xiao%20and%20Robin%20Peretzke%20and%20David%20Clunie%20and%20Constantin%20Ulrich%20and%20Michael%20Baumgartner%20and%20Alexander%20Muckenhuber%20and%20Silvia%20Dias%20Almeida%20and%20Michael%20G%C3%B6tz%20and%20Jens%20Kleesiek%20and%20Marco%20Nolden%20and%20Rickmer%20Braren%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20Digital%20pathology%20offers%20a%20groundbreaking%20opportunity%20to%20transform%20clinical%0Apractice%20in%20histopathological%20image%20analysis%2C%20yet%20faces%20a%20significant%20hurdle%3A%0Athe%20substantial%20file%20sizes%20of%20pathological%20Whole%20Slide%20Images%20%28WSI%29.%20While%0Acurrent%20digital%20pathology%20solutions%20rely%20on%20lossy%20JPEG%20compression%20to%20address%0Athis%20issue%2C%20lossy%20compression%20can%20introduce%20color%20and%20texture%20disparities%2C%0Apotentially%20impacting%20clinical%20decision-making.%20While%20prior%20research%20addresses%0Aperceptual%20image%20quality%20and%20downstream%20performance%20independently%20of%20each%0Aother%2C%20we%20jointly%20evaluate%20compression%20schemes%20for%20perceptual%20and%20downstream%0Atask%20quality%20on%20four%20different%20datasets.%20In%20addition%2C%20we%20collect%20an%20initially%0Auncompressed%20dataset%20for%20an%20unbiased%20perceptual%20evaluation%20of%20compression%0Aschemes.%20Our%20results%20show%20that%20deep%20learning%20models%20fine-tuned%20for%20perceptual%0Aquality%20outperform%20conventional%20compression%20schemes%20like%20JPEG-XL%20or%20WebP%20for%0Afurther%20compression%20of%20WSI.%20However%2C%20they%20exhibit%20a%20significant%20bias%20towards%0Athe%20compression%20artifacts%20present%20in%20the%20training%20data%20and%20struggle%20to%0Ageneralize%20across%20various%20compression%20schemes.%20We%20introduce%20a%20novel%20evaluation%0Ametric%20based%20on%20feature%20similarity%20between%20original%20files%20and%20compressed%20files%0Athat%20aligns%20very%20well%20with%20the%20actual%20downstream%20performance%20on%20the%20compressed%0AWSI.%20Our%20metric%20allows%20for%20a%20general%20and%20standardized%20evaluation%20of%20lossy%0Acompression%20schemes%20and%20mitigates%20the%20requirement%20to%20independently%20assess%0Adifferent%20downstream%20tasks.%20Our%20study%20provides%20novel%20insights%20for%20the%0Aassessment%20of%20lossy%20compression%20schemes%20for%20WSI%20and%20encourages%20a%20unified%0Aevaluation%20of%20lossy%20compression%20schemes%20to%20accelerate%20the%20clinical%20uptake%20of%0Adigital%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520the%2520Potential%2520of%2520Digital%2520Pathology%253A%2520Novel%2520Baselines%2520for%250A%2520%2520Compression%26entry.906535625%3DMaximilian%2520Fischer%2520and%2520Peter%2520Neher%2520and%2520Peter%2520Sch%25C3%25BCffler%2520and%2520Sebastian%2520Ziegler%2520and%2520Shuhan%2520Xiao%2520and%2520Robin%2520Peretzke%2520and%2520David%2520Clunie%2520and%2520Constantin%2520Ulrich%2520and%2520Michael%2520Baumgartner%2520and%2520Alexander%2520Muckenhuber%2520and%2520Silvia%2520Dias%2520Almeida%2520and%2520Michael%2520G%25C3%25B6tz%2520and%2520Jens%2520Kleesiek%2520and%2520Marco%2520Nolden%2520and%2520Rickmer%2520Braren%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3D%2520%2520Digital%2520pathology%2520offers%2520a%2520groundbreaking%2520opportunity%2520to%2520transform%2520clinical%250Apractice%2520in%2520histopathological%2520image%2520analysis%252C%2520yet%2520faces%2520a%2520significant%2520hurdle%253A%250Athe%2520substantial%2520file%2520sizes%2520of%2520pathological%2520Whole%2520Slide%2520Images%2520%2528WSI%2529.%2520While%250Acurrent%2520digital%2520pathology%2520solutions%2520rely%2520on%2520lossy%2520JPEG%2520compression%2520to%2520address%250Athis%2520issue%252C%2520lossy%2520compression%2520can%2520introduce%2520color%2520and%2520texture%2520disparities%252C%250Apotentially%2520impacting%2520clinical%2520decision-making.%2520While%2520prior%2520research%2520addresses%250Aperceptual%2520image%2520quality%2520and%2520downstream%2520performance%2520independently%2520of%2520each%250Aother%252C%2520we%2520jointly%2520evaluate%2520compression%2520schemes%2520for%2520perceptual%2520and%2520downstream%250Atask%2520quality%2520on%2520four%2520different%2520datasets.%2520In%2520addition%252C%2520we%2520collect%2520an%2520initially%250Auncompressed%2520dataset%2520for%2520an%2520unbiased%2520perceptual%2520evaluation%2520of%2520compression%250Aschemes.%2520Our%2520results%2520show%2520that%2520deep%2520learning%2520models%2520fine-tuned%2520for%2520perceptual%250Aquality%2520outperform%2520conventional%2520compression%2520schemes%2520like%2520JPEG-XL%2520or%2520WebP%2520for%250Afurther%2520compression%2520of%2520WSI.%2520However%252C%2520they%2520exhibit%2520a%2520significant%2520bias%2520towards%250Athe%2520compression%2520artifacts%2520present%2520in%2520the%2520training%2520data%2520and%2520struggle%2520to%250Ageneralize%2520across%2520various%2520compression%2520schemes.%2520We%2520introduce%2520a%2520novel%2520evaluation%250Ametric%2520based%2520on%2520feature%2520similarity%2520between%2520original%2520files%2520and%2520compressed%2520files%250Athat%2520aligns%2520very%2520well%2520with%2520the%2520actual%2520downstream%2520performance%2520on%2520the%2520compressed%250AWSI.%2520Our%2520metric%2520allows%2520for%2520a%2520general%2520and%2520standardized%2520evaluation%2520of%2520lossy%250Acompression%2520schemes%2520and%2520mitigates%2520the%2520requirement%2520to%2520independently%2520assess%250Adifferent%2520downstream%2520tasks.%2520Our%2520study%2520provides%2520novel%2520insights%2520for%2520the%250Aassessment%2520of%2520lossy%2520compression%2520schemes%2520for%2520WSI%2520and%2520encourages%2520a%2520unified%250Aevaluation%2520of%2520lossy%2520compression%2520schemes%2520to%2520accelerate%2520the%2520clinical%2520uptake%2520of%250Adigital%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20the%20Potential%20of%20Digital%20Pathology%3A%20Novel%20Baselines%20for%0A%20%20Compression&entry.906535625=Maximilian%20Fischer%20and%20Peter%20Neher%20and%20Peter%20Sch%C3%BCffler%20and%20Sebastian%20Ziegler%20and%20Shuhan%20Xiao%20and%20Robin%20Peretzke%20and%20David%20Clunie%20and%20Constantin%20Ulrich%20and%20Michael%20Baumgartner%20and%20Alexander%20Muckenhuber%20and%20Silvia%20Dias%20Almeida%20and%20Michael%20G%C3%B6tz%20and%20Jens%20Kleesiek%20and%20Marco%20Nolden%20and%20Rickmer%20Braren%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20Digital%20pathology%20offers%20a%20groundbreaking%20opportunity%20to%20transform%20clinical%0Apractice%20in%20histopathological%20image%20analysis%2C%20yet%20faces%20a%20significant%20hurdle%3A%0Athe%20substantial%20file%20sizes%20of%20pathological%20Whole%20Slide%20Images%20%28WSI%29.%20While%0Acurrent%20digital%20pathology%20solutions%20rely%20on%20lossy%20JPEG%20compression%20to%20address%0Athis%20issue%2C%20lossy%20compression%20can%20introduce%20color%20and%20texture%20disparities%2C%0Apotentially%20impacting%20clinical%20decision-making.%20While%20prior%20research%20addresses%0Aperceptual%20image%20quality%20and%20downstream%20performance%20independently%20of%20each%0Aother%2C%20we%20jointly%20evaluate%20compression%20schemes%20for%20perceptual%20and%20downstream%0Atask%20quality%20on%20four%20different%20datasets.%20In%20addition%2C%20we%20collect%20an%20initially%0Auncompressed%20dataset%20for%20an%20unbiased%20perceptual%20evaluation%20of%20compression%0Aschemes.%20Our%20results%20show%20that%20deep%20learning%20models%20fine-tuned%20for%20perceptual%0Aquality%20outperform%20conventional%20compression%20schemes%20like%20JPEG-XL%20or%20WebP%20for%0Afurther%20compression%20of%20WSI.%20However%2C%20they%20exhibit%20a%20significant%20bias%20towards%0Athe%20compression%20artifacts%20present%20in%20the%20training%20data%20and%20struggle%20to%0Ageneralize%20across%20various%20compression%20schemes.%20We%20introduce%20a%20novel%20evaluation%0Ametric%20based%20on%20feature%20similarity%20between%20original%20files%20and%20compressed%20files%0Athat%20aligns%20very%20well%20with%20the%20actual%20downstream%20performance%20on%20the%20compressed%0AWSI.%20Our%20metric%20allows%20for%20a%20general%20and%20standardized%20evaluation%20of%20lossy%0Acompression%20schemes%20and%20mitigates%20the%20requirement%20to%20independently%20assess%0Adifferent%20downstream%20tasks.%20Our%20study%20provides%20novel%20insights%20for%20the%0Aassessment%20of%20lossy%20compression%20schemes%20for%20WSI%20and%20encourages%20a%20unified%0Aevaluation%20of%20lossy%20compression%20schemes%20to%20accelerate%20the%20clinical%20uptake%20of%0Adigital%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13137v1&entry.124074799=Read"},
{"title": "Estimating Body and Hand Motion in an Ego-sensed World", "author": "Brent Yi and Vickie Ye and Maya Zheng and Yunqi Li and Lea M\u00fcller and Georgios Pavlakos and Yi Ma and Jitendra Malik and Angjoo Kanazawa", "abstract": "  We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture a device wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve hand estimation: the resulting kinematic and temporal\nconstraints can reduce world-frame errors in single-frame estimates by 40%.\nProject page: https://egoallo.github.io/\n", "link": "http://arxiv.org/abs/2410.03665v3", "date": "2024-12-17", "relevancy": 2.1822, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5685}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5309}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Body%20and%20Hand%20Motion%20in%20an%20Ego-sensed%20World&body=Title%3A%20Estimating%20Body%20and%20Hand%20Motion%20in%20an%20Ego-sensed%20World%0AAuthor%3A%20Brent%20Yi%20and%20Vickie%20Ye%20and%20Maya%20Zheng%20and%20Yunqi%20Li%20and%20Lea%20M%C3%BCller%20and%20Georgios%20Pavlakos%20and%20Yi%20Ma%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20We%20present%20EgoAllo%2C%20a%20system%20for%20human%20motion%20estimation%20from%20a%20head-mounted%0Adevice.%20Using%20only%20egocentric%20SLAM%20poses%20and%20images%2C%20EgoAllo%20guides%20sampling%0Afrom%20a%20conditional%20diffusion%20model%20to%20estimate%203D%20body%20pose%2C%20height%2C%20and%20hand%0Aparameters%20that%20capture%20a%20device%20wearer%27s%20actions%20in%20the%20allocentric%20coordinate%0Aframe%20of%20the%20scene.%20To%20achieve%20this%2C%20our%20key%20insight%20is%20in%20representation%3A%20we%0Apropose%20spatial%20and%20temporal%20invariance%20criteria%20for%20improving%20model%0Aperformance%2C%20from%20which%20we%20derive%20a%20head%20motion%20conditioning%20parameterization%0Athat%20improves%20estimation%20by%20up%20to%2018%25.%20We%20also%20show%20how%20the%20bodies%20estimated%20by%0Aour%20system%20can%20improve%20hand%20estimation%3A%20the%20resulting%20kinematic%20and%20temporal%0Aconstraints%20can%20reduce%20world-frame%20errors%20in%20single-frame%20estimates%20by%2040%25.%0AProject%20page%3A%20https%3A//egoallo.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03665v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Body%2520and%2520Hand%2520Motion%2520in%2520an%2520Ego-sensed%2520World%26entry.906535625%3DBrent%2520Yi%2520and%2520Vickie%2520Ye%2520and%2520Maya%2520Zheng%2520and%2520Yunqi%2520Li%2520and%2520Lea%2520M%25C3%25BCller%2520and%2520Georgios%2520Pavlakos%2520and%2520Yi%2520Ma%2520and%2520Jitendra%2520Malik%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520We%2520present%2520EgoAllo%252C%2520a%2520system%2520for%2520human%2520motion%2520estimation%2520from%2520a%2520head-mounted%250Adevice.%2520Using%2520only%2520egocentric%2520SLAM%2520poses%2520and%2520images%252C%2520EgoAllo%2520guides%2520sampling%250Afrom%2520a%2520conditional%2520diffusion%2520model%2520to%2520estimate%25203D%2520body%2520pose%252C%2520height%252C%2520and%2520hand%250Aparameters%2520that%2520capture%2520a%2520device%2520wearer%2527s%2520actions%2520in%2520the%2520allocentric%2520coordinate%250Aframe%2520of%2520the%2520scene.%2520To%2520achieve%2520this%252C%2520our%2520key%2520insight%2520is%2520in%2520representation%253A%2520we%250Apropose%2520spatial%2520and%2520temporal%2520invariance%2520criteria%2520for%2520improving%2520model%250Aperformance%252C%2520from%2520which%2520we%2520derive%2520a%2520head%2520motion%2520conditioning%2520parameterization%250Athat%2520improves%2520estimation%2520by%2520up%2520to%252018%2525.%2520We%2520also%2520show%2520how%2520the%2520bodies%2520estimated%2520by%250Aour%2520system%2520can%2520improve%2520hand%2520estimation%253A%2520the%2520resulting%2520kinematic%2520and%2520temporal%250Aconstraints%2520can%2520reduce%2520world-frame%2520errors%2520in%2520single-frame%2520estimates%2520by%252040%2525.%250AProject%2520page%253A%2520https%253A//egoallo.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03665v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Body%20and%20Hand%20Motion%20in%20an%20Ego-sensed%20World&entry.906535625=Brent%20Yi%20and%20Vickie%20Ye%20and%20Maya%20Zheng%20and%20Yunqi%20Li%20and%20Lea%20M%C3%BCller%20and%20Georgios%20Pavlakos%20and%20Yi%20Ma%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20We%20present%20EgoAllo%2C%20a%20system%20for%20human%20motion%20estimation%20from%20a%20head-mounted%0Adevice.%20Using%20only%20egocentric%20SLAM%20poses%20and%20images%2C%20EgoAllo%20guides%20sampling%0Afrom%20a%20conditional%20diffusion%20model%20to%20estimate%203D%20body%20pose%2C%20height%2C%20and%20hand%0Aparameters%20that%20capture%20a%20device%20wearer%27s%20actions%20in%20the%20allocentric%20coordinate%0Aframe%20of%20the%20scene.%20To%20achieve%20this%2C%20our%20key%20insight%20is%20in%20representation%3A%20we%0Apropose%20spatial%20and%20temporal%20invariance%20criteria%20for%20improving%20model%0Aperformance%2C%20from%20which%20we%20derive%20a%20head%20motion%20conditioning%20parameterization%0Athat%20improves%20estimation%20by%20up%20to%2018%25.%20We%20also%20show%20how%20the%20bodies%20estimated%20by%0Aour%20system%20can%20improve%20hand%20estimation%3A%20the%20resulting%20kinematic%20and%20temporal%0Aconstraints%20can%20reduce%20world-frame%20errors%20in%20single-frame%20estimates%20by%2040%25.%0AProject%20page%3A%20https%3A//egoallo.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03665v3&entry.124074799=Read"},
{"title": "Learning of Patch-Based Smooth-Plus-Sparse Models for Image\n  Reconstruction", "author": "Stanislas Ducotterd and Sebastian Neumayer and Michael Unser", "abstract": "  We aim at the solution of inverse problems in imaging, by combining a\npenalized sparse representation of image patches with an unconstrained smooth\none. This allows for a straightforward interpretation of the reconstruction. We\nformulate the optimization as a bilevel problem. The inner problem deploys\nclassical algorithms while the outer problem optimizes the dictionary and the\nregularizer parameters through supervised learning. The process is carried out\nvia implicit differentiation and gradient-based optimization. We evaluate our\nmethod for denoising, super-resolution, and compressed-sensing\nmagnetic-resonance imaging. We compare it to other classical models as well as\ndeep-learning-based methods and show that it always outperforms the former and\nalso the latter in some instances.\n", "link": "http://arxiv.org/abs/2412.13070v1", "date": "2024-12-17", "relevancy": 2.1526, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5655}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5414}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20of%20Patch-Based%20Smooth-Plus-Sparse%20Models%20for%20Image%0A%20%20Reconstruction&body=Title%3A%20Learning%20of%20Patch-Based%20Smooth-Plus-Sparse%20Models%20for%20Image%0A%20%20Reconstruction%0AAuthor%3A%20Stanislas%20Ducotterd%20and%20Sebastian%20Neumayer%20and%20Michael%20Unser%0AAbstract%3A%20%20%20We%20aim%20at%20the%20solution%20of%20inverse%20problems%20in%20imaging%2C%20by%20combining%20a%0Apenalized%20sparse%20representation%20of%20image%20patches%20with%20an%20unconstrained%20smooth%0Aone.%20This%20allows%20for%20a%20straightforward%20interpretation%20of%20the%20reconstruction.%20We%0Aformulate%20the%20optimization%20as%20a%20bilevel%20problem.%20The%20inner%20problem%20deploys%0Aclassical%20algorithms%20while%20the%20outer%20problem%20optimizes%20the%20dictionary%20and%20the%0Aregularizer%20parameters%20through%20supervised%20learning.%20The%20process%20is%20carried%20out%0Avia%20implicit%20differentiation%20and%20gradient-based%20optimization.%20We%20evaluate%20our%0Amethod%20for%20denoising%2C%20super-resolution%2C%20and%20compressed-sensing%0Amagnetic-resonance%20imaging.%20We%20compare%20it%20to%20other%20classical%20models%20as%20well%20as%0Adeep-learning-based%20methods%20and%20show%20that%20it%20always%20outperforms%20the%20former%20and%0Aalso%20the%20latter%20in%20some%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520of%2520Patch-Based%2520Smooth-Plus-Sparse%2520Models%2520for%2520Image%250A%2520%2520Reconstruction%26entry.906535625%3DStanislas%2520Ducotterd%2520and%2520Sebastian%2520Neumayer%2520and%2520Michael%2520Unser%26entry.1292438233%3D%2520%2520We%2520aim%2520at%2520the%2520solution%2520of%2520inverse%2520problems%2520in%2520imaging%252C%2520by%2520combining%2520a%250Apenalized%2520sparse%2520representation%2520of%2520image%2520patches%2520with%2520an%2520unconstrained%2520smooth%250Aone.%2520This%2520allows%2520for%2520a%2520straightforward%2520interpretation%2520of%2520the%2520reconstruction.%2520We%250Aformulate%2520the%2520optimization%2520as%2520a%2520bilevel%2520problem.%2520The%2520inner%2520problem%2520deploys%250Aclassical%2520algorithms%2520while%2520the%2520outer%2520problem%2520optimizes%2520the%2520dictionary%2520and%2520the%250Aregularizer%2520parameters%2520through%2520supervised%2520learning.%2520The%2520process%2520is%2520carried%2520out%250Avia%2520implicit%2520differentiation%2520and%2520gradient-based%2520optimization.%2520We%2520evaluate%2520our%250Amethod%2520for%2520denoising%252C%2520super-resolution%252C%2520and%2520compressed-sensing%250Amagnetic-resonance%2520imaging.%2520We%2520compare%2520it%2520to%2520other%2520classical%2520models%2520as%2520well%2520as%250Adeep-learning-based%2520methods%2520and%2520show%2520that%2520it%2520always%2520outperforms%2520the%2520former%2520and%250Aalso%2520the%2520latter%2520in%2520some%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20of%20Patch-Based%20Smooth-Plus-Sparse%20Models%20for%20Image%0A%20%20Reconstruction&entry.906535625=Stanislas%20Ducotterd%20and%20Sebastian%20Neumayer%20and%20Michael%20Unser&entry.1292438233=%20%20We%20aim%20at%20the%20solution%20of%20inverse%20problems%20in%20imaging%2C%20by%20combining%20a%0Apenalized%20sparse%20representation%20of%20image%20patches%20with%20an%20unconstrained%20smooth%0Aone.%20This%20allows%20for%20a%20straightforward%20interpretation%20of%20the%20reconstruction.%20We%0Aformulate%20the%20optimization%20as%20a%20bilevel%20problem.%20The%20inner%20problem%20deploys%0Aclassical%20algorithms%20while%20the%20outer%20problem%20optimizes%20the%20dictionary%20and%20the%0Aregularizer%20parameters%20through%20supervised%20learning.%20The%20process%20is%20carried%20out%0Avia%20implicit%20differentiation%20and%20gradient-based%20optimization.%20We%20evaluate%20our%0Amethod%20for%20denoising%2C%20super-resolution%2C%20and%20compressed-sensing%0Amagnetic-resonance%20imaging.%20We%20compare%20it%20to%20other%20classical%20models%20as%20well%20as%0Adeep-learning-based%20methods%20and%20show%20that%20it%20always%20outperforms%20the%20former%20and%0Aalso%20the%20latter%20in%20some%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13070v1&entry.124074799=Read"},
{"title": "F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking\n  Face Generation, Customization, and Restoration", "author": "Lu Liu and Huiyu Duan and Qiang Hu and Liu Yang and Chunlei Cai and Tianxiao Ye and Huayu Liu and Xiaoyun Zhang and Guangtao Zhai", "abstract": "  Artificial intelligence generative models exhibit remarkable capabilities in\ncontent creation, particularly in face image generation, customization, and\nrestoration. However, current AI-generated faces (AIGFs) often fall short of\nhuman preferences due to unique distortions, unrealistic details, and\nunexpected identity shifts, underscoring the need for a comprehensive quality\nevaluation framework for AIGFs. To address this need, we introduce FaceQ, a\nlarge-scale, comprehensive database of AI-generated Face images with\nfine-grained Quality annotations reflecting human preferences. The FaceQ\ndatabase comprises 12,255 images generated by 29 models across three tasks: (1)\nface generation, (2) face customization, and (3) face restoration. It includes\n32,742 mean opinion scores (MOSs) from 180 annotators, assessed across multiple\ndimensions: quality, authenticity, identity (ID) fidelity, and text-image\ncorrespondence. Using the FaceQ database, we establish F-Bench, a benchmark for\ncomparing and evaluating face generation, customization, and restoration\nmodels, highlighting strengths and weaknesses across various prompts and\nevaluation dimensions. Additionally, we assess the performance of existing\nimage quality assessment (IQA), face quality assessment (FQA), AI-generated\ncontent image quality assessment (AIGCIQA), and preference evaluation metrics,\nmanifesting that these standard metrics are relatively ineffective in\nevaluating authenticity, ID fidelity, and text-image correspondence. The FaceQ\ndatabase will be publicly available upon publication.\n", "link": "http://arxiv.org/abs/2412.13155v1", "date": "2024-12-17", "relevancy": 2.1226, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5498}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5231}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F-Bench%3A%20Rethinking%20Human%20Preference%20Evaluation%20Metrics%20for%20Benchmarking%0A%20%20Face%20Generation%2C%20Customization%2C%20and%20Restoration&body=Title%3A%20F-Bench%3A%20Rethinking%20Human%20Preference%20Evaluation%20Metrics%20for%20Benchmarking%0A%20%20Face%20Generation%2C%20Customization%2C%20and%20Restoration%0AAuthor%3A%20Lu%20Liu%20and%20Huiyu%20Duan%20and%20Qiang%20Hu%20and%20Liu%20Yang%20and%20Chunlei%20Cai%20and%20Tianxiao%20Ye%20and%20Huayu%20Liu%20and%20Xiaoyun%20Zhang%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20Artificial%20intelligence%20generative%20models%20exhibit%20remarkable%20capabilities%20in%0Acontent%20creation%2C%20particularly%20in%20face%20image%20generation%2C%20customization%2C%20and%0Arestoration.%20However%2C%20current%20AI-generated%20faces%20%28AIGFs%29%20often%20fall%20short%20of%0Ahuman%20preferences%20due%20to%20unique%20distortions%2C%20unrealistic%20details%2C%20and%0Aunexpected%20identity%20shifts%2C%20underscoring%20the%20need%20for%20a%20comprehensive%20quality%0Aevaluation%20framework%20for%20AIGFs.%20To%20address%20this%20need%2C%20we%20introduce%20FaceQ%2C%20a%0Alarge-scale%2C%20comprehensive%20database%20of%20AI-generated%20Face%20images%20with%0Afine-grained%20Quality%20annotations%20reflecting%20human%20preferences.%20The%20FaceQ%0Adatabase%20comprises%2012%2C255%20images%20generated%20by%2029%20models%20across%20three%20tasks%3A%20%281%29%0Aface%20generation%2C%20%282%29%20face%20customization%2C%20and%20%283%29%20face%20restoration.%20It%20includes%0A32%2C742%20mean%20opinion%20scores%20%28MOSs%29%20from%20180%20annotators%2C%20assessed%20across%20multiple%0Adimensions%3A%20quality%2C%20authenticity%2C%20identity%20%28ID%29%20fidelity%2C%20and%20text-image%0Acorrespondence.%20Using%20the%20FaceQ%20database%2C%20we%20establish%20F-Bench%2C%20a%20benchmark%20for%0Acomparing%20and%20evaluating%20face%20generation%2C%20customization%2C%20and%20restoration%0Amodels%2C%20highlighting%20strengths%20and%20weaknesses%20across%20various%20prompts%20and%0Aevaluation%20dimensions.%20Additionally%2C%20we%20assess%20the%20performance%20of%20existing%0Aimage%20quality%20assessment%20%28IQA%29%2C%20face%20quality%20assessment%20%28FQA%29%2C%20AI-generated%0Acontent%20image%20quality%20assessment%20%28AIGCIQA%29%2C%20and%20preference%20evaluation%20metrics%2C%0Amanifesting%20that%20these%20standard%20metrics%20are%20relatively%20ineffective%20in%0Aevaluating%20authenticity%2C%20ID%20fidelity%2C%20and%20text-image%20correspondence.%20The%20FaceQ%0Adatabase%20will%20be%20publicly%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF-Bench%253A%2520Rethinking%2520Human%2520Preference%2520Evaluation%2520Metrics%2520for%2520Benchmarking%250A%2520%2520Face%2520Generation%252C%2520Customization%252C%2520and%2520Restoration%26entry.906535625%3DLu%2520Liu%2520and%2520Huiyu%2520Duan%2520and%2520Qiang%2520Hu%2520and%2520Liu%2520Yang%2520and%2520Chunlei%2520Cai%2520and%2520Tianxiao%2520Ye%2520and%2520Huayu%2520Liu%2520and%2520Xiaoyun%2520Zhang%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520generative%2520models%2520exhibit%2520remarkable%2520capabilities%2520in%250Acontent%2520creation%252C%2520particularly%2520in%2520face%2520image%2520generation%252C%2520customization%252C%2520and%250Arestoration.%2520However%252C%2520current%2520AI-generated%2520faces%2520%2528AIGFs%2529%2520often%2520fall%2520short%2520of%250Ahuman%2520preferences%2520due%2520to%2520unique%2520distortions%252C%2520unrealistic%2520details%252C%2520and%250Aunexpected%2520identity%2520shifts%252C%2520underscoring%2520the%2520need%2520for%2520a%2520comprehensive%2520quality%250Aevaluation%2520framework%2520for%2520AIGFs.%2520To%2520address%2520this%2520need%252C%2520we%2520introduce%2520FaceQ%252C%2520a%250Alarge-scale%252C%2520comprehensive%2520database%2520of%2520AI-generated%2520Face%2520images%2520with%250Afine-grained%2520Quality%2520annotations%2520reflecting%2520human%2520preferences.%2520The%2520FaceQ%250Adatabase%2520comprises%252012%252C255%2520images%2520generated%2520by%252029%2520models%2520across%2520three%2520tasks%253A%2520%25281%2529%250Aface%2520generation%252C%2520%25282%2529%2520face%2520customization%252C%2520and%2520%25283%2529%2520face%2520restoration.%2520It%2520includes%250A32%252C742%2520mean%2520opinion%2520scores%2520%2528MOSs%2529%2520from%2520180%2520annotators%252C%2520assessed%2520across%2520multiple%250Adimensions%253A%2520quality%252C%2520authenticity%252C%2520identity%2520%2528ID%2529%2520fidelity%252C%2520and%2520text-image%250Acorrespondence.%2520Using%2520the%2520FaceQ%2520database%252C%2520we%2520establish%2520F-Bench%252C%2520a%2520benchmark%2520for%250Acomparing%2520and%2520evaluating%2520face%2520generation%252C%2520customization%252C%2520and%2520restoration%250Amodels%252C%2520highlighting%2520strengths%2520and%2520weaknesses%2520across%2520various%2520prompts%2520and%250Aevaluation%2520dimensions.%2520Additionally%252C%2520we%2520assess%2520the%2520performance%2520of%2520existing%250Aimage%2520quality%2520assessment%2520%2528IQA%2529%252C%2520face%2520quality%2520assessment%2520%2528FQA%2529%252C%2520AI-generated%250Acontent%2520image%2520quality%2520assessment%2520%2528AIGCIQA%2529%252C%2520and%2520preference%2520evaluation%2520metrics%252C%250Amanifesting%2520that%2520these%2520standard%2520metrics%2520are%2520relatively%2520ineffective%2520in%250Aevaluating%2520authenticity%252C%2520ID%2520fidelity%252C%2520and%2520text-image%2520correspondence.%2520The%2520FaceQ%250Adatabase%2520will%2520be%2520publicly%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F-Bench%3A%20Rethinking%20Human%20Preference%20Evaluation%20Metrics%20for%20Benchmarking%0A%20%20Face%20Generation%2C%20Customization%2C%20and%20Restoration&entry.906535625=Lu%20Liu%20and%20Huiyu%20Duan%20and%20Qiang%20Hu%20and%20Liu%20Yang%20and%20Chunlei%20Cai%20and%20Tianxiao%20Ye%20and%20Huayu%20Liu%20and%20Xiaoyun%20Zhang%20and%20Guangtao%20Zhai&entry.1292438233=%20%20Artificial%20intelligence%20generative%20models%20exhibit%20remarkable%20capabilities%20in%0Acontent%20creation%2C%20particularly%20in%20face%20image%20generation%2C%20customization%2C%20and%0Arestoration.%20However%2C%20current%20AI-generated%20faces%20%28AIGFs%29%20often%20fall%20short%20of%0Ahuman%20preferences%20due%20to%20unique%20distortions%2C%20unrealistic%20details%2C%20and%0Aunexpected%20identity%20shifts%2C%20underscoring%20the%20need%20for%20a%20comprehensive%20quality%0Aevaluation%20framework%20for%20AIGFs.%20To%20address%20this%20need%2C%20we%20introduce%20FaceQ%2C%20a%0Alarge-scale%2C%20comprehensive%20database%20of%20AI-generated%20Face%20images%20with%0Afine-grained%20Quality%20annotations%20reflecting%20human%20preferences.%20The%20FaceQ%0Adatabase%20comprises%2012%2C255%20images%20generated%20by%2029%20models%20across%20three%20tasks%3A%20%281%29%0Aface%20generation%2C%20%282%29%20face%20customization%2C%20and%20%283%29%20face%20restoration.%20It%20includes%0A32%2C742%20mean%20opinion%20scores%20%28MOSs%29%20from%20180%20annotators%2C%20assessed%20across%20multiple%0Adimensions%3A%20quality%2C%20authenticity%2C%20identity%20%28ID%29%20fidelity%2C%20and%20text-image%0Acorrespondence.%20Using%20the%20FaceQ%20database%2C%20we%20establish%20F-Bench%2C%20a%20benchmark%20for%0Acomparing%20and%20evaluating%20face%20generation%2C%20customization%2C%20and%20restoration%0Amodels%2C%20highlighting%20strengths%20and%20weaknesses%20across%20various%20prompts%20and%0Aevaluation%20dimensions.%20Additionally%2C%20we%20assess%20the%20performance%20of%20existing%0Aimage%20quality%20assessment%20%28IQA%29%2C%20face%20quality%20assessment%20%28FQA%29%2C%20AI-generated%0Acontent%20image%20quality%20assessment%20%28AIGCIQA%29%2C%20and%20preference%20evaluation%20metrics%2C%0Amanifesting%20that%20these%20standard%20metrics%20are%20relatively%20ineffective%20in%0Aevaluating%20authenticity%2C%20ID%20fidelity%2C%20and%20text-image%20correspondence.%20The%20FaceQ%0Adatabase%20will%20be%20publicly%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13155v1&entry.124074799=Read"},
{"title": "Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial\n  Dynamics in the Wild", "author": "Xingjian Wang and Li Chai", "abstract": "  In-the-wild Dynamic facial expression recognition (DFER) encounters a\nsignificant challenge in recognizing emotion-related expressions, which are\noften temporally and spatially diluted by emotion-irrelevant expressions and\nglobal context respectively. Most of the prior DFER methods model tightly\ncoupled spatiotemporal representations which may incorporate weakly relevant\nfeatures, leading to information redundancy and emotion-irrelevant context\nbias. Several DFER methods have highlighted the significance of dynamic\ninformation, but utilize explicit manners to extract dynamic features with\noverly strong prior knowledge. In this paper, we propose a novel Implicit\nFacial Dynamics Disentanglement framework (IFDD). Through expanding wavelet\nlifting scheme to fully learnable framework, IFDD disentangles emotion-related\ndynamic information from emotion-irrelevant global context in an implicit\nmanner, i.e., without exploit operations and external guidance. The\ndisentanglement process of IFDD contains two stages, i.e., Inter-frame\nStatic-dynamic Splitting Module (ISSM) for rough disentanglement estimation and\nLifting-based Aggregation-Disentanglement Module (LADM) for further refinement.\nSpecifically, ISSM explores inter-frame correlation to generate content-aware\nsplitting indexes on-the-fly. We preliminarily utilize these indexes to split\nframe features into two groups, one with greater global similarity, and the\nother with more unique dynamic features. Subsequently, LADM first aggregates\nthese two groups of features to obtain fine-grained global context features by\nan updater, and then disentangles emotion-related facial dynamic features from\nthe global context by a predictor. Extensive experiments on in-the-wild\ndatasets have demonstrated that IFDD outperforms prior supervised DFER methods\nwith higher recognition accuracy and comparable efficiency.\n", "link": "http://arxiv.org/abs/2412.13168v1", "date": "2024-12-17", "relevancy": 2.097, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5259}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5253}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lifting%20Scheme-Based%20Implicit%20Disentanglement%20of%20Emotion-Related%20Facial%0A%20%20Dynamics%20in%20the%20Wild&body=Title%3A%20Lifting%20Scheme-Based%20Implicit%20Disentanglement%20of%20Emotion-Related%20Facial%0A%20%20Dynamics%20in%20the%20Wild%0AAuthor%3A%20Xingjian%20Wang%20and%20Li%20Chai%0AAbstract%3A%20%20%20In-the-wild%20Dynamic%20facial%20expression%20recognition%20%28DFER%29%20encounters%20a%0Asignificant%20challenge%20in%20recognizing%20emotion-related%20expressions%2C%20which%20are%0Aoften%20temporally%20and%20spatially%20diluted%20by%20emotion-irrelevant%20expressions%20and%0Aglobal%20context%20respectively.%20Most%20of%20the%20prior%20DFER%20methods%20model%20tightly%0Acoupled%20spatiotemporal%20representations%20which%20may%20incorporate%20weakly%20relevant%0Afeatures%2C%20leading%20to%20information%20redundancy%20and%20emotion-irrelevant%20context%0Abias.%20Several%20DFER%20methods%20have%20highlighted%20the%20significance%20of%20dynamic%0Ainformation%2C%20but%20utilize%20explicit%20manners%20to%20extract%20dynamic%20features%20with%0Aoverly%20strong%20prior%20knowledge.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Implicit%0AFacial%20Dynamics%20Disentanglement%20framework%20%28IFDD%29.%20Through%20expanding%20wavelet%0Alifting%20scheme%20to%20fully%20learnable%20framework%2C%20IFDD%20disentangles%20emotion-related%0Adynamic%20information%20from%20emotion-irrelevant%20global%20context%20in%20an%20implicit%0Amanner%2C%20i.e.%2C%20without%20exploit%20operations%20and%20external%20guidance.%20The%0Adisentanglement%20process%20of%20IFDD%20contains%20two%20stages%2C%20i.e.%2C%20Inter-frame%0AStatic-dynamic%20Splitting%20Module%20%28ISSM%29%20for%20rough%20disentanglement%20estimation%20and%0ALifting-based%20Aggregation-Disentanglement%20Module%20%28LADM%29%20for%20further%20refinement.%0ASpecifically%2C%20ISSM%20explores%20inter-frame%20correlation%20to%20generate%20content-aware%0Asplitting%20indexes%20on-the-fly.%20We%20preliminarily%20utilize%20these%20indexes%20to%20split%0Aframe%20features%20into%20two%20groups%2C%20one%20with%20greater%20global%20similarity%2C%20and%20the%0Aother%20with%20more%20unique%20dynamic%20features.%20Subsequently%2C%20LADM%20first%20aggregates%0Athese%20two%20groups%20of%20features%20to%20obtain%20fine-grained%20global%20context%20features%20by%0Aan%20updater%2C%20and%20then%20disentangles%20emotion-related%20facial%20dynamic%20features%20from%0Athe%20global%20context%20by%20a%20predictor.%20Extensive%20experiments%20on%20in-the-wild%0Adatasets%20have%20demonstrated%20that%20IFDD%20outperforms%20prior%20supervised%20DFER%20methods%0Awith%20higher%20recognition%20accuracy%20and%20comparable%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLifting%2520Scheme-Based%2520Implicit%2520Disentanglement%2520of%2520Emotion-Related%2520Facial%250A%2520%2520Dynamics%2520in%2520the%2520Wild%26entry.906535625%3DXingjian%2520Wang%2520and%2520Li%2520Chai%26entry.1292438233%3D%2520%2520In-the-wild%2520Dynamic%2520facial%2520expression%2520recognition%2520%2528DFER%2529%2520encounters%2520a%250Asignificant%2520challenge%2520in%2520recognizing%2520emotion-related%2520expressions%252C%2520which%2520are%250Aoften%2520temporally%2520and%2520spatially%2520diluted%2520by%2520emotion-irrelevant%2520expressions%2520and%250Aglobal%2520context%2520respectively.%2520Most%2520of%2520the%2520prior%2520DFER%2520methods%2520model%2520tightly%250Acoupled%2520spatiotemporal%2520representations%2520which%2520may%2520incorporate%2520weakly%2520relevant%250Afeatures%252C%2520leading%2520to%2520information%2520redundancy%2520and%2520emotion-irrelevant%2520context%250Abias.%2520Several%2520DFER%2520methods%2520have%2520highlighted%2520the%2520significance%2520of%2520dynamic%250Ainformation%252C%2520but%2520utilize%2520explicit%2520manners%2520to%2520extract%2520dynamic%2520features%2520with%250Aoverly%2520strong%2520prior%2520knowledge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Implicit%250AFacial%2520Dynamics%2520Disentanglement%2520framework%2520%2528IFDD%2529.%2520Through%2520expanding%2520wavelet%250Alifting%2520scheme%2520to%2520fully%2520learnable%2520framework%252C%2520IFDD%2520disentangles%2520emotion-related%250Adynamic%2520information%2520from%2520emotion-irrelevant%2520global%2520context%2520in%2520an%2520implicit%250Amanner%252C%2520i.e.%252C%2520without%2520exploit%2520operations%2520and%2520external%2520guidance.%2520The%250Adisentanglement%2520process%2520of%2520IFDD%2520contains%2520two%2520stages%252C%2520i.e.%252C%2520Inter-frame%250AStatic-dynamic%2520Splitting%2520Module%2520%2528ISSM%2529%2520for%2520rough%2520disentanglement%2520estimation%2520and%250ALifting-based%2520Aggregation-Disentanglement%2520Module%2520%2528LADM%2529%2520for%2520further%2520refinement.%250ASpecifically%252C%2520ISSM%2520explores%2520inter-frame%2520correlation%2520to%2520generate%2520content-aware%250Asplitting%2520indexes%2520on-the-fly.%2520We%2520preliminarily%2520utilize%2520these%2520indexes%2520to%2520split%250Aframe%2520features%2520into%2520two%2520groups%252C%2520one%2520with%2520greater%2520global%2520similarity%252C%2520and%2520the%250Aother%2520with%2520more%2520unique%2520dynamic%2520features.%2520Subsequently%252C%2520LADM%2520first%2520aggregates%250Athese%2520two%2520groups%2520of%2520features%2520to%2520obtain%2520fine-grained%2520global%2520context%2520features%2520by%250Aan%2520updater%252C%2520and%2520then%2520disentangles%2520emotion-related%2520facial%2520dynamic%2520features%2520from%250Athe%2520global%2520context%2520by%2520a%2520predictor.%2520Extensive%2520experiments%2520on%2520in-the-wild%250Adatasets%2520have%2520demonstrated%2520that%2520IFDD%2520outperforms%2520prior%2520supervised%2520DFER%2520methods%250Awith%2520higher%2520recognition%2520accuracy%2520and%2520comparable%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lifting%20Scheme-Based%20Implicit%20Disentanglement%20of%20Emotion-Related%20Facial%0A%20%20Dynamics%20in%20the%20Wild&entry.906535625=Xingjian%20Wang%20and%20Li%20Chai&entry.1292438233=%20%20In-the-wild%20Dynamic%20facial%20expression%20recognition%20%28DFER%29%20encounters%20a%0Asignificant%20challenge%20in%20recognizing%20emotion-related%20expressions%2C%20which%20are%0Aoften%20temporally%20and%20spatially%20diluted%20by%20emotion-irrelevant%20expressions%20and%0Aglobal%20context%20respectively.%20Most%20of%20the%20prior%20DFER%20methods%20model%20tightly%0Acoupled%20spatiotemporal%20representations%20which%20may%20incorporate%20weakly%20relevant%0Afeatures%2C%20leading%20to%20information%20redundancy%20and%20emotion-irrelevant%20context%0Abias.%20Several%20DFER%20methods%20have%20highlighted%20the%20significance%20of%20dynamic%0Ainformation%2C%20but%20utilize%20explicit%20manners%20to%20extract%20dynamic%20features%20with%0Aoverly%20strong%20prior%20knowledge.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Implicit%0AFacial%20Dynamics%20Disentanglement%20framework%20%28IFDD%29.%20Through%20expanding%20wavelet%0Alifting%20scheme%20to%20fully%20learnable%20framework%2C%20IFDD%20disentangles%20emotion-related%0Adynamic%20information%20from%20emotion-irrelevant%20global%20context%20in%20an%20implicit%0Amanner%2C%20i.e.%2C%20without%20exploit%20operations%20and%20external%20guidance.%20The%0Adisentanglement%20process%20of%20IFDD%20contains%20two%20stages%2C%20i.e.%2C%20Inter-frame%0AStatic-dynamic%20Splitting%20Module%20%28ISSM%29%20for%20rough%20disentanglement%20estimation%20and%0ALifting-based%20Aggregation-Disentanglement%20Module%20%28LADM%29%20for%20further%20refinement.%0ASpecifically%2C%20ISSM%20explores%20inter-frame%20correlation%20to%20generate%20content-aware%0Asplitting%20indexes%20on-the-fly.%20We%20preliminarily%20utilize%20these%20indexes%20to%20split%0Aframe%20features%20into%20two%20groups%2C%20one%20with%20greater%20global%20similarity%2C%20and%20the%0Aother%20with%20more%20unique%20dynamic%20features.%20Subsequently%2C%20LADM%20first%20aggregates%0Athese%20two%20groups%20of%20features%20to%20obtain%20fine-grained%20global%20context%20features%20by%0Aan%20updater%2C%20and%20then%20disentangles%20emotion-related%20facial%20dynamic%20features%20from%0Athe%20global%20context%20by%20a%20predictor.%20Extensive%20experiments%20on%20in-the-wild%0Adatasets%20have%20demonstrated%20that%20IFDD%20outperforms%20prior%20supervised%20DFER%20methods%0Awith%20higher%20recognition%20accuracy%20and%20comparable%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13168v1&entry.124074799=Read"},
{"title": "Stably unactivated neurons in ReLU neural networks", "author": "Natalie Brownlowe and Christopher R. Cornwell and Ethan Montes and Gabriel Quijano and Grace Stulman and Na Zhang", "abstract": "  The choice of architecture of a neural network influences which functions\nwill be realizable by that neural network and, as a result, studying the\nexpressiveness of a chosen architecture has received much attention. In ReLU\nneural networks, the presence of stably unactivated neurons can reduce the\nnetwork's expressiveness. In this work, we investigate the probability of a\nneuron in the second hidden layer of such neural networks being stably\nunactivated when the weights and biases are initialized from symmetric\nprobability distributions. For networks with input dimension $n_0$, we prove\nthat if the first hidden layer has $n_0+1$ neurons then this probability is\nexactly $\\frac{2^{n_0}+1}{4^{n_0+1}}$, and if the first hidden layer has $n_1$\nneurons, $n_1 \\le n_0$, then the probability is $\\frac{1}{2^{n_1+1}}$. Finally,\nfor the case when the first hidden layer has more neurons than $n_0+1$, a\nconjecture is proposed along with the rationale. Computational evidence is\npresented to support the conjecture.\n", "link": "http://arxiv.org/abs/2412.06829v2", "date": "2024-12-17", "relevancy": 2.0558, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4377}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.398}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stably%20unactivated%20neurons%20in%20ReLU%20neural%20networks&body=Title%3A%20Stably%20unactivated%20neurons%20in%20ReLU%20neural%20networks%0AAuthor%3A%20Natalie%20Brownlowe%20and%20Christopher%20R.%20Cornwell%20and%20Ethan%20Montes%20and%20Gabriel%20Quijano%20and%20Grace%20Stulman%20and%20Na%20Zhang%0AAbstract%3A%20%20%20The%20choice%20of%20architecture%20of%20a%20neural%20network%20influences%20which%20functions%0Awill%20be%20realizable%20by%20that%20neural%20network%20and%2C%20as%20a%20result%2C%20studying%20the%0Aexpressiveness%20of%20a%20chosen%20architecture%20has%20received%20much%20attention.%20In%20ReLU%0Aneural%20networks%2C%20the%20presence%20of%20stably%20unactivated%20neurons%20can%20reduce%20the%0Anetwork%27s%20expressiveness.%20In%20this%20work%2C%20we%20investigate%20the%20probability%20of%20a%0Aneuron%20in%20the%20second%20hidden%20layer%20of%20such%20neural%20networks%20being%20stably%0Aunactivated%20when%20the%20weights%20and%20biases%20are%20initialized%20from%20symmetric%0Aprobability%20distributions.%20For%20networks%20with%20input%20dimension%20%24n_0%24%2C%20we%20prove%0Athat%20if%20the%20first%20hidden%20layer%20has%20%24n_0%2B1%24%20neurons%20then%20this%20probability%20is%0Aexactly%20%24%5Cfrac%7B2%5E%7Bn_0%7D%2B1%7D%7B4%5E%7Bn_0%2B1%7D%7D%24%2C%20and%20if%20the%20first%20hidden%20layer%20has%20%24n_1%24%0Aneurons%2C%20%24n_1%20%5Cle%20n_0%24%2C%20then%20the%20probability%20is%20%24%5Cfrac%7B1%7D%7B2%5E%7Bn_1%2B1%7D%7D%24.%20Finally%2C%0Afor%20the%20case%20when%20the%20first%20hidden%20layer%20has%20more%20neurons%20than%20%24n_0%2B1%24%2C%20a%0Aconjecture%20is%20proposed%20along%20with%20the%20rationale.%20Computational%20evidence%20is%0Apresented%20to%20support%20the%20conjecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06829v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStably%2520unactivated%2520neurons%2520in%2520ReLU%2520neural%2520networks%26entry.906535625%3DNatalie%2520Brownlowe%2520and%2520Christopher%2520R.%2520Cornwell%2520and%2520Ethan%2520Montes%2520and%2520Gabriel%2520Quijano%2520and%2520Grace%2520Stulman%2520and%2520Na%2520Zhang%26entry.1292438233%3D%2520%2520The%2520choice%2520of%2520architecture%2520of%2520a%2520neural%2520network%2520influences%2520which%2520functions%250Awill%2520be%2520realizable%2520by%2520that%2520neural%2520network%2520and%252C%2520as%2520a%2520result%252C%2520studying%2520the%250Aexpressiveness%2520of%2520a%2520chosen%2520architecture%2520has%2520received%2520much%2520attention.%2520In%2520ReLU%250Aneural%2520networks%252C%2520the%2520presence%2520of%2520stably%2520unactivated%2520neurons%2520can%2520reduce%2520the%250Anetwork%2527s%2520expressiveness.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520probability%2520of%2520a%250Aneuron%2520in%2520the%2520second%2520hidden%2520layer%2520of%2520such%2520neural%2520networks%2520being%2520stably%250Aunactivated%2520when%2520the%2520weights%2520and%2520biases%2520are%2520initialized%2520from%2520symmetric%250Aprobability%2520distributions.%2520For%2520networks%2520with%2520input%2520dimension%2520%2524n_0%2524%252C%2520we%2520prove%250Athat%2520if%2520the%2520first%2520hidden%2520layer%2520has%2520%2524n_0%252B1%2524%2520neurons%2520then%2520this%2520probability%2520is%250Aexactly%2520%2524%255Cfrac%257B2%255E%257Bn_0%257D%252B1%257D%257B4%255E%257Bn_0%252B1%257D%257D%2524%252C%2520and%2520if%2520the%2520first%2520hidden%2520layer%2520has%2520%2524n_1%2524%250Aneurons%252C%2520%2524n_1%2520%255Cle%2520n_0%2524%252C%2520then%2520the%2520probability%2520is%2520%2524%255Cfrac%257B1%257D%257B2%255E%257Bn_1%252B1%257D%257D%2524.%2520Finally%252C%250Afor%2520the%2520case%2520when%2520the%2520first%2520hidden%2520layer%2520has%2520more%2520neurons%2520than%2520%2524n_0%252B1%2524%252C%2520a%250Aconjecture%2520is%2520proposed%2520along%2520with%2520the%2520rationale.%2520Computational%2520evidence%2520is%250Apresented%2520to%2520support%2520the%2520conjecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06829v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stably%20unactivated%20neurons%20in%20ReLU%20neural%20networks&entry.906535625=Natalie%20Brownlowe%20and%20Christopher%20R.%20Cornwell%20and%20Ethan%20Montes%20and%20Gabriel%20Quijano%20and%20Grace%20Stulman%20and%20Na%20Zhang&entry.1292438233=%20%20The%20choice%20of%20architecture%20of%20a%20neural%20network%20influences%20which%20functions%0Awill%20be%20realizable%20by%20that%20neural%20network%20and%2C%20as%20a%20result%2C%20studying%20the%0Aexpressiveness%20of%20a%20chosen%20architecture%20has%20received%20much%20attention.%20In%20ReLU%0Aneural%20networks%2C%20the%20presence%20of%20stably%20unactivated%20neurons%20can%20reduce%20the%0Anetwork%27s%20expressiveness.%20In%20this%20work%2C%20we%20investigate%20the%20probability%20of%20a%0Aneuron%20in%20the%20second%20hidden%20layer%20of%20such%20neural%20networks%20being%20stably%0Aunactivated%20when%20the%20weights%20and%20biases%20are%20initialized%20from%20symmetric%0Aprobability%20distributions.%20For%20networks%20with%20input%20dimension%20%24n_0%24%2C%20we%20prove%0Athat%20if%20the%20first%20hidden%20layer%20has%20%24n_0%2B1%24%20neurons%20then%20this%20probability%20is%0Aexactly%20%24%5Cfrac%7B2%5E%7Bn_0%7D%2B1%7D%7B4%5E%7Bn_0%2B1%7D%7D%24%2C%20and%20if%20the%20first%20hidden%20layer%20has%20%24n_1%24%0Aneurons%2C%20%24n_1%20%5Cle%20n_0%24%2C%20then%20the%20probability%20is%20%24%5Cfrac%7B1%7D%7B2%5E%7Bn_1%2B1%7D%7D%24.%20Finally%2C%0Afor%20the%20case%20when%20the%20first%20hidden%20layer%20has%20more%20neurons%20than%20%24n_0%2B1%24%2C%20a%0Aconjecture%20is%20proposed%20along%20with%20the%20rationale.%20Computational%20evidence%20is%0Apresented%20to%20support%20the%20conjecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06829v2&entry.124074799=Read"},
{"title": "SVGBuilder: Component-Based Colored SVG Generation with Text-Guided\n  Autoregressive Transformers", "author": "Zehao Chen and Rong Pan", "abstract": "  Scalable Vector Graphics (SVG) are essential XML-based formats for versatile\ngraphics, offering resolution independence and scalability. Unlike raster\nimages, SVGs use geometric shapes and support interactivity, animation, and\nmanipulation via CSS and JavaScript. Current SVG generation methods face\nchallenges related to high computational costs and complexity. In contrast,\nhuman designers use component-based tools for efficient SVG creation. Inspired\nby this, SVGBuilder introduces a component-based, autoregressive model for\ngenerating high-quality colored SVGs from textual input. It significantly\nreduces computational overhead and improves efficiency compared to traditional\nmethods. Our model generates SVGs up to 604 times faster than\noptimization-based approaches. To address the limitations of existing SVG\ndatasets and support our research, we introduce ColorSVG-100K, the first\nlarge-scale dataset of colored SVGs, comprising 100,000 graphics. This dataset\nfills the gap in color information for SVG generation models and enhances\ndiversity in model training. Evaluation against state-of-the-art models\ndemonstrates SVGBuilder's superior performance in practical applications,\nhighlighting its efficiency and quality in generating complex SVG graphics.\n", "link": "http://arxiv.org/abs/2412.10488v2", "date": "2024-12-17", "relevancy": 2.0422, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5422}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5167}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVGBuilder%3A%20Component-Based%20Colored%20SVG%20Generation%20with%20Text-Guided%0A%20%20Autoregressive%20Transformers&body=Title%3A%20SVGBuilder%3A%20Component-Based%20Colored%20SVG%20Generation%20with%20Text-Guided%0A%20%20Autoregressive%20Transformers%0AAuthor%3A%20Zehao%20Chen%20and%20Rong%20Pan%0AAbstract%3A%20%20%20Scalable%20Vector%20Graphics%20%28SVG%29%20are%20essential%20XML-based%20formats%20for%20versatile%0Agraphics%2C%20offering%20resolution%20independence%20and%20scalability.%20Unlike%20raster%0Aimages%2C%20SVGs%20use%20geometric%20shapes%20and%20support%20interactivity%2C%20animation%2C%20and%0Amanipulation%20via%20CSS%20and%20JavaScript.%20Current%20SVG%20generation%20methods%20face%0Achallenges%20related%20to%20high%20computational%20costs%20and%20complexity.%20In%20contrast%2C%0Ahuman%20designers%20use%20component-based%20tools%20for%20efficient%20SVG%20creation.%20Inspired%0Aby%20this%2C%20SVGBuilder%20introduces%20a%20component-based%2C%20autoregressive%20model%20for%0Agenerating%20high-quality%20colored%20SVGs%20from%20textual%20input.%20It%20significantly%0Areduces%20computational%20overhead%20and%20improves%20efficiency%20compared%20to%20traditional%0Amethods.%20Our%20model%20generates%20SVGs%20up%20to%20604%20times%20faster%20than%0Aoptimization-based%20approaches.%20To%20address%20the%20limitations%20of%20existing%20SVG%0Adatasets%20and%20support%20our%20research%2C%20we%20introduce%20ColorSVG-100K%2C%20the%20first%0Alarge-scale%20dataset%20of%20colored%20SVGs%2C%20comprising%20100%2C000%20graphics.%20This%20dataset%0Afills%20the%20gap%20in%20color%20information%20for%20SVG%20generation%20models%20and%20enhances%0Adiversity%20in%20model%20training.%20Evaluation%20against%20state-of-the-art%20models%0Ademonstrates%20SVGBuilder%27s%20superior%20performance%20in%20practical%20applications%2C%0Ahighlighting%20its%20efficiency%20and%20quality%20in%20generating%20complex%20SVG%20graphics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVGBuilder%253A%2520Component-Based%2520Colored%2520SVG%2520Generation%2520with%2520Text-Guided%250A%2520%2520Autoregressive%2520Transformers%26entry.906535625%3DZehao%2520Chen%2520and%2520Rong%2520Pan%26entry.1292438233%3D%2520%2520Scalable%2520Vector%2520Graphics%2520%2528SVG%2529%2520are%2520essential%2520XML-based%2520formats%2520for%2520versatile%250Agraphics%252C%2520offering%2520resolution%2520independence%2520and%2520scalability.%2520Unlike%2520raster%250Aimages%252C%2520SVGs%2520use%2520geometric%2520shapes%2520and%2520support%2520interactivity%252C%2520animation%252C%2520and%250Amanipulation%2520via%2520CSS%2520and%2520JavaScript.%2520Current%2520SVG%2520generation%2520methods%2520face%250Achallenges%2520related%2520to%2520high%2520computational%2520costs%2520and%2520complexity.%2520In%2520contrast%252C%250Ahuman%2520designers%2520use%2520component-based%2520tools%2520for%2520efficient%2520SVG%2520creation.%2520Inspired%250Aby%2520this%252C%2520SVGBuilder%2520introduces%2520a%2520component-based%252C%2520autoregressive%2520model%2520for%250Agenerating%2520high-quality%2520colored%2520SVGs%2520from%2520textual%2520input.%2520It%2520significantly%250Areduces%2520computational%2520overhead%2520and%2520improves%2520efficiency%2520compared%2520to%2520traditional%250Amethods.%2520Our%2520model%2520generates%2520SVGs%2520up%2520to%2520604%2520times%2520faster%2520than%250Aoptimization-based%2520approaches.%2520To%2520address%2520the%2520limitations%2520of%2520existing%2520SVG%250Adatasets%2520and%2520support%2520our%2520research%252C%2520we%2520introduce%2520ColorSVG-100K%252C%2520the%2520first%250Alarge-scale%2520dataset%2520of%2520colored%2520SVGs%252C%2520comprising%2520100%252C000%2520graphics.%2520This%2520dataset%250Afills%2520the%2520gap%2520in%2520color%2520information%2520for%2520SVG%2520generation%2520models%2520and%2520enhances%250Adiversity%2520in%2520model%2520training.%2520Evaluation%2520against%2520state-of-the-art%2520models%250Ademonstrates%2520SVGBuilder%2527s%2520superior%2520performance%2520in%2520practical%2520applications%252C%250Ahighlighting%2520its%2520efficiency%2520and%2520quality%2520in%2520generating%2520complex%2520SVG%2520graphics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVGBuilder%3A%20Component-Based%20Colored%20SVG%20Generation%20with%20Text-Guided%0A%20%20Autoregressive%20Transformers&entry.906535625=Zehao%20Chen%20and%20Rong%20Pan&entry.1292438233=%20%20Scalable%20Vector%20Graphics%20%28SVG%29%20are%20essential%20XML-based%20formats%20for%20versatile%0Agraphics%2C%20offering%20resolution%20independence%20and%20scalability.%20Unlike%20raster%0Aimages%2C%20SVGs%20use%20geometric%20shapes%20and%20support%20interactivity%2C%20animation%2C%20and%0Amanipulation%20via%20CSS%20and%20JavaScript.%20Current%20SVG%20generation%20methods%20face%0Achallenges%20related%20to%20high%20computational%20costs%20and%20complexity.%20In%20contrast%2C%0Ahuman%20designers%20use%20component-based%20tools%20for%20efficient%20SVG%20creation.%20Inspired%0Aby%20this%2C%20SVGBuilder%20introduces%20a%20component-based%2C%20autoregressive%20model%20for%0Agenerating%20high-quality%20colored%20SVGs%20from%20textual%20input.%20It%20significantly%0Areduces%20computational%20overhead%20and%20improves%20efficiency%20compared%20to%20traditional%0Amethods.%20Our%20model%20generates%20SVGs%20up%20to%20604%20times%20faster%20than%0Aoptimization-based%20approaches.%20To%20address%20the%20limitations%20of%20existing%20SVG%0Adatasets%20and%20support%20our%20research%2C%20we%20introduce%20ColorSVG-100K%2C%20the%20first%0Alarge-scale%20dataset%20of%20colored%20SVGs%2C%20comprising%20100%2C000%20graphics.%20This%20dataset%0Afills%20the%20gap%20in%20color%20information%20for%20SVG%20generation%20models%20and%20enhances%0Adiversity%20in%20model%20training.%20Evaluation%20against%20state-of-the-art%20models%0Ademonstrates%20SVGBuilder%27s%20superior%20performance%20in%20practical%20applications%2C%0Ahighlighting%20its%20efficiency%20and%20quality%20in%20generating%20complex%20SVG%20graphics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10488v2&entry.124074799=Read"},
{"title": "ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark\n  Detection", "author": "Jui-Che Chiang and Hou-Ning Hu and Bo-Syuan Hou and Chia-Yu Tseng and Yu-Lun Liu and Min-Hung Chen and Yen-Yu Lin", "abstract": "  Although facial landmark detection (FLD) has gained significant progress,\nexisting FLD methods still suffer from performance drops on partially\nnon-visible faces, such as faces with occlusions or under extreme lighting\nconditions or poses. To address this issue, we introduce ORFormer, a novel\ntransformer-based method that can detect non-visible regions and recover their\nmissing features from visible parts. Specifically, ORFormer associates each\nimage patch token with one additional learnable token called the messenger\ntoken. The messenger token aggregates features from all but its patch. This\nway, the consensus between a patch and other patches can be assessed by\nreferring to the similarity between its regular and messenger embeddings,\nenabling non-visible region identification. Our method then recovers occluded\npatches with features aggregated by the messenger tokens. Leveraging the\nrecovered features, ORFormer compiles high-quality heatmaps for the downstream\nFLD task. Extensive experiments show that our method generates heatmaps\nresilient to partial occlusions. By integrating the resultant heatmaps into\nexisting FLD methods, our method performs favorably against the state of the\narts on challenging datasets such as WFLW and COFW.\n", "link": "http://arxiv.org/abs/2412.13174v1", "date": "2024-12-17", "relevancy": 2.0282, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5096}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5065}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ORFormer%3A%20Occlusion-Robust%20Transformer%20for%20Accurate%20Facial%20Landmark%0A%20%20Detection&body=Title%3A%20ORFormer%3A%20Occlusion-Robust%20Transformer%20for%20Accurate%20Facial%20Landmark%0A%20%20Detection%0AAuthor%3A%20Jui-Che%20Chiang%20and%20Hou-Ning%20Hu%20and%20Bo-Syuan%20Hou%20and%20Chia-Yu%20Tseng%20and%20Yu-Lun%20Liu%20and%20Min-Hung%20Chen%20and%20Yen-Yu%20Lin%0AAbstract%3A%20%20%20Although%20facial%20landmark%20detection%20%28FLD%29%20has%20gained%20significant%20progress%2C%0Aexisting%20FLD%20methods%20still%20suffer%20from%20performance%20drops%20on%20partially%0Anon-visible%20faces%2C%20such%20as%20faces%20with%20occlusions%20or%20under%20extreme%20lighting%0Aconditions%20or%20poses.%20To%20address%20this%20issue%2C%20we%20introduce%20ORFormer%2C%20a%20novel%0Atransformer-based%20method%20that%20can%20detect%20non-visible%20regions%20and%20recover%20their%0Amissing%20features%20from%20visible%20parts.%20Specifically%2C%20ORFormer%20associates%20each%0Aimage%20patch%20token%20with%20one%20additional%20learnable%20token%20called%20the%20messenger%0Atoken.%20The%20messenger%20token%20aggregates%20features%20from%20all%20but%20its%20patch.%20This%0Away%2C%20the%20consensus%20between%20a%20patch%20and%20other%20patches%20can%20be%20assessed%20by%0Areferring%20to%20the%20similarity%20between%20its%20regular%20and%20messenger%20embeddings%2C%0Aenabling%20non-visible%20region%20identification.%20Our%20method%20then%20recovers%20occluded%0Apatches%20with%20features%20aggregated%20by%20the%20messenger%20tokens.%20Leveraging%20the%0Arecovered%20features%2C%20ORFormer%20compiles%20high-quality%20heatmaps%20for%20the%20downstream%0AFLD%20task.%20Extensive%20experiments%20show%20that%20our%20method%20generates%20heatmaps%0Aresilient%20to%20partial%20occlusions.%20By%20integrating%20the%20resultant%20heatmaps%20into%0Aexisting%20FLD%20methods%2C%20our%20method%20performs%20favorably%20against%20the%20state%20of%20the%0Aarts%20on%20challenging%20datasets%20such%20as%20WFLW%20and%20COFW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DORFormer%253A%2520Occlusion-Robust%2520Transformer%2520for%2520Accurate%2520Facial%2520Landmark%250A%2520%2520Detection%26entry.906535625%3DJui-Che%2520Chiang%2520and%2520Hou-Ning%2520Hu%2520and%2520Bo-Syuan%2520Hou%2520and%2520Chia-Yu%2520Tseng%2520and%2520Yu-Lun%2520Liu%2520and%2520Min-Hung%2520Chen%2520and%2520Yen-Yu%2520Lin%26entry.1292438233%3D%2520%2520Although%2520facial%2520landmark%2520detection%2520%2528FLD%2529%2520has%2520gained%2520significant%2520progress%252C%250Aexisting%2520FLD%2520methods%2520still%2520suffer%2520from%2520performance%2520drops%2520on%2520partially%250Anon-visible%2520faces%252C%2520such%2520as%2520faces%2520with%2520occlusions%2520or%2520under%2520extreme%2520lighting%250Aconditions%2520or%2520poses.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520ORFormer%252C%2520a%2520novel%250Atransformer-based%2520method%2520that%2520can%2520detect%2520non-visible%2520regions%2520and%2520recover%2520their%250Amissing%2520features%2520from%2520visible%2520parts.%2520Specifically%252C%2520ORFormer%2520associates%2520each%250Aimage%2520patch%2520token%2520with%2520one%2520additional%2520learnable%2520token%2520called%2520the%2520messenger%250Atoken.%2520The%2520messenger%2520token%2520aggregates%2520features%2520from%2520all%2520but%2520its%2520patch.%2520This%250Away%252C%2520the%2520consensus%2520between%2520a%2520patch%2520and%2520other%2520patches%2520can%2520be%2520assessed%2520by%250Areferring%2520to%2520the%2520similarity%2520between%2520its%2520regular%2520and%2520messenger%2520embeddings%252C%250Aenabling%2520non-visible%2520region%2520identification.%2520Our%2520method%2520then%2520recovers%2520occluded%250Apatches%2520with%2520features%2520aggregated%2520by%2520the%2520messenger%2520tokens.%2520Leveraging%2520the%250Arecovered%2520features%252C%2520ORFormer%2520compiles%2520high-quality%2520heatmaps%2520for%2520the%2520downstream%250AFLD%2520task.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520generates%2520heatmaps%250Aresilient%2520to%2520partial%2520occlusions.%2520By%2520integrating%2520the%2520resultant%2520heatmaps%2520into%250Aexisting%2520FLD%2520methods%252C%2520our%2520method%2520performs%2520favorably%2520against%2520the%2520state%2520of%2520the%250Aarts%2520on%2520challenging%2520datasets%2520such%2520as%2520WFLW%2520and%2520COFW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ORFormer%3A%20Occlusion-Robust%20Transformer%20for%20Accurate%20Facial%20Landmark%0A%20%20Detection&entry.906535625=Jui-Che%20Chiang%20and%20Hou-Ning%20Hu%20and%20Bo-Syuan%20Hou%20and%20Chia-Yu%20Tseng%20and%20Yu-Lun%20Liu%20and%20Min-Hung%20Chen%20and%20Yen-Yu%20Lin&entry.1292438233=%20%20Although%20facial%20landmark%20detection%20%28FLD%29%20has%20gained%20significant%20progress%2C%0Aexisting%20FLD%20methods%20still%20suffer%20from%20performance%20drops%20on%20partially%0Anon-visible%20faces%2C%20such%20as%20faces%20with%20occlusions%20or%20under%20extreme%20lighting%0Aconditions%20or%20poses.%20To%20address%20this%20issue%2C%20we%20introduce%20ORFormer%2C%20a%20novel%0Atransformer-based%20method%20that%20can%20detect%20non-visible%20regions%20and%20recover%20their%0Amissing%20features%20from%20visible%20parts.%20Specifically%2C%20ORFormer%20associates%20each%0Aimage%20patch%20token%20with%20one%20additional%20learnable%20token%20called%20the%20messenger%0Atoken.%20The%20messenger%20token%20aggregates%20features%20from%20all%20but%20its%20patch.%20This%0Away%2C%20the%20consensus%20between%20a%20patch%20and%20other%20patches%20can%20be%20assessed%20by%0Areferring%20to%20the%20similarity%20between%20its%20regular%20and%20messenger%20embeddings%2C%0Aenabling%20non-visible%20region%20identification.%20Our%20method%20then%20recovers%20occluded%0Apatches%20with%20features%20aggregated%20by%20the%20messenger%20tokens.%20Leveraging%20the%0Arecovered%20features%2C%20ORFormer%20compiles%20high-quality%20heatmaps%20for%20the%20downstream%0AFLD%20task.%20Extensive%20experiments%20show%20that%20our%20method%20generates%20heatmaps%0Aresilient%20to%20partial%20occlusions.%20By%20integrating%20the%20resultant%20heatmaps%20into%0Aexisting%20FLD%20methods%2C%20our%20method%20performs%20favorably%20against%20the%20state%20of%20the%0Aarts%20on%20challenging%20datasets%20such%20as%20WFLW%20and%20COFW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13174v1&entry.124074799=Read"},
{"title": "Measurement of Medial Elbow Joint Space using Landmark Detection", "author": "Shizuka Akahori and Shotaro Teruya and Pragyan Shrestha and Yuichi Yoshii and Ryuhei Michinobu and Satoshi Iizuka and Itaru Kitahara", "abstract": "  Ultrasound imaging of the medial elbow is crucial for the early\nidentification of Ulnar Collateral Ligament (UCL) injuries. Specifically,\nmeasuring the elbow joint space in ultrasound images is used to assess the\nvalgus instability of elbow. To automate this measurement, a precisely\nannotated dataset is necessary; however, no publicly available dataset has been\nproposed thus far. This study introduces a novel ultrasound medial elbow\ndataset for measuring joint space to diagnose Ulnar Collateral Ligament (UCL)\ninjuries. The dataset comprises 4,201 medial elbow ultrasound images from 22\nsubjects, with landmark annotations on the humerus and ulna. The annotations\nare made precisely by the authors under the supervision of three orthopedic\nsurgeons. We evaluated joint space measurement methods using our proposed\ndataset with several landmark detection approaches, including ViTPose, HRNet,\nPCT, YOLOv8, and U-Net. In addition, we propose using Shape Subspace (SS) for\nlandmark refinement in heatmap-based landmark detection. The results show that\nthe mean Euclidean distance error of joint space is 0.116 mm when using HRNet.\nFurthermore, the SS landmark refinement improves the mean absolute error of\nlandmark positions by 0.010 mm with HRNet and by 0.103 mm with ViTPose on\naverage. These highlight the potential for high-precision, real-time diagnosis\nof UCL injuries and associated risks, which could be leveraged in large-scale\nscreening. Lastly, we demonstrate point-based segmentation of the humerus and\nulna using the detected landmarks as input. The dataset will be made publicly\navailable upon acceptance of this paper at:\nhttps://github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset.\n", "link": "http://arxiv.org/abs/2412.13010v1", "date": "2024-12-17", "relevancy": 2.0222, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5565}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5005}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measurement%20of%20Medial%20Elbow%20Joint%20Space%20using%20Landmark%20Detection&body=Title%3A%20Measurement%20of%20Medial%20Elbow%20Joint%20Space%20using%20Landmark%20Detection%0AAuthor%3A%20Shizuka%20Akahori%20and%20Shotaro%20Teruya%20and%20Pragyan%20Shrestha%20and%20Yuichi%20Yoshii%20and%20Ryuhei%20Michinobu%20and%20Satoshi%20Iizuka%20and%20Itaru%20Kitahara%0AAbstract%3A%20%20%20Ultrasound%20imaging%20of%20the%20medial%20elbow%20is%20crucial%20for%20the%20early%0Aidentification%20of%20Ulnar%20Collateral%20Ligament%20%28UCL%29%20injuries.%20Specifically%2C%0Ameasuring%20the%20elbow%20joint%20space%20in%20ultrasound%20images%20is%20used%20to%20assess%20the%0Avalgus%20instability%20of%20elbow.%20To%20automate%20this%20measurement%2C%20a%20precisely%0Aannotated%20dataset%20is%20necessary%3B%20however%2C%20no%20publicly%20available%20dataset%20has%20been%0Aproposed%20thus%20far.%20This%20study%20introduces%20a%20novel%20ultrasound%20medial%20elbow%0Adataset%20for%20measuring%20joint%20space%20to%20diagnose%20Ulnar%20Collateral%20Ligament%20%28UCL%29%0Ainjuries.%20The%20dataset%20comprises%204%2C201%20medial%20elbow%20ultrasound%20images%20from%2022%0Asubjects%2C%20with%20landmark%20annotations%20on%20the%20humerus%20and%20ulna.%20The%20annotations%0Aare%20made%20precisely%20by%20the%20authors%20under%20the%20supervision%20of%20three%20orthopedic%0Asurgeons.%20We%20evaluated%20joint%20space%20measurement%20methods%20using%20our%20proposed%0Adataset%20with%20several%20landmark%20detection%20approaches%2C%20including%20ViTPose%2C%20HRNet%2C%0APCT%2C%20YOLOv8%2C%20and%20U-Net.%20In%20addition%2C%20we%20propose%20using%20Shape%20Subspace%20%28SS%29%20for%0Alandmark%20refinement%20in%20heatmap-based%20landmark%20detection.%20The%20results%20show%20that%0Athe%20mean%20Euclidean%20distance%20error%20of%20joint%20space%20is%200.116%20mm%20when%20using%20HRNet.%0AFurthermore%2C%20the%20SS%20landmark%20refinement%20improves%20the%20mean%20absolute%20error%20of%0Alandmark%20positions%20by%200.010%20mm%20with%20HRNet%20and%20by%200.103%20mm%20with%20ViTPose%20on%0Aaverage.%20These%20highlight%20the%20potential%20for%20high-precision%2C%20real-time%20diagnosis%0Aof%20UCL%20injuries%20and%20associated%20risks%2C%20which%20could%20be%20leveraged%20in%20large-scale%0Ascreening.%20Lastly%2C%20we%20demonstrate%20point-based%20segmentation%20of%20the%20humerus%20and%0Aulna%20using%20the%20detected%20landmarks%20as%20input.%20The%20dataset%20will%20be%20made%20publicly%0Aavailable%20upon%20acceptance%20of%20this%20paper%20at%3A%0Ahttps%3A//github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasurement%2520of%2520Medial%2520Elbow%2520Joint%2520Space%2520using%2520Landmark%2520Detection%26entry.906535625%3DShizuka%2520Akahori%2520and%2520Shotaro%2520Teruya%2520and%2520Pragyan%2520Shrestha%2520and%2520Yuichi%2520Yoshii%2520and%2520Ryuhei%2520Michinobu%2520and%2520Satoshi%2520Iizuka%2520and%2520Itaru%2520Kitahara%26entry.1292438233%3D%2520%2520Ultrasound%2520imaging%2520of%2520the%2520medial%2520elbow%2520is%2520crucial%2520for%2520the%2520early%250Aidentification%2520of%2520Ulnar%2520Collateral%2520Ligament%2520%2528UCL%2529%2520injuries.%2520Specifically%252C%250Ameasuring%2520the%2520elbow%2520joint%2520space%2520in%2520ultrasound%2520images%2520is%2520used%2520to%2520assess%2520the%250Avalgus%2520instability%2520of%2520elbow.%2520To%2520automate%2520this%2520measurement%252C%2520a%2520precisely%250Aannotated%2520dataset%2520is%2520necessary%253B%2520however%252C%2520no%2520publicly%2520available%2520dataset%2520has%2520been%250Aproposed%2520thus%2520far.%2520This%2520study%2520introduces%2520a%2520novel%2520ultrasound%2520medial%2520elbow%250Adataset%2520for%2520measuring%2520joint%2520space%2520to%2520diagnose%2520Ulnar%2520Collateral%2520Ligament%2520%2528UCL%2529%250Ainjuries.%2520The%2520dataset%2520comprises%25204%252C201%2520medial%2520elbow%2520ultrasound%2520images%2520from%252022%250Asubjects%252C%2520with%2520landmark%2520annotations%2520on%2520the%2520humerus%2520and%2520ulna.%2520The%2520annotations%250Aare%2520made%2520precisely%2520by%2520the%2520authors%2520under%2520the%2520supervision%2520of%2520three%2520orthopedic%250Asurgeons.%2520We%2520evaluated%2520joint%2520space%2520measurement%2520methods%2520using%2520our%2520proposed%250Adataset%2520with%2520several%2520landmark%2520detection%2520approaches%252C%2520including%2520ViTPose%252C%2520HRNet%252C%250APCT%252C%2520YOLOv8%252C%2520and%2520U-Net.%2520In%2520addition%252C%2520we%2520propose%2520using%2520Shape%2520Subspace%2520%2528SS%2529%2520for%250Alandmark%2520refinement%2520in%2520heatmap-based%2520landmark%2520detection.%2520The%2520results%2520show%2520that%250Athe%2520mean%2520Euclidean%2520distance%2520error%2520of%2520joint%2520space%2520is%25200.116%2520mm%2520when%2520using%2520HRNet.%250AFurthermore%252C%2520the%2520SS%2520landmark%2520refinement%2520improves%2520the%2520mean%2520absolute%2520error%2520of%250Alandmark%2520positions%2520by%25200.010%2520mm%2520with%2520HRNet%2520and%2520by%25200.103%2520mm%2520with%2520ViTPose%2520on%250Aaverage.%2520These%2520highlight%2520the%2520potential%2520for%2520high-precision%252C%2520real-time%2520diagnosis%250Aof%2520UCL%2520injuries%2520and%2520associated%2520risks%252C%2520which%2520could%2520be%2520leveraged%2520in%2520large-scale%250Ascreening.%2520Lastly%252C%2520we%2520demonstrate%2520point-based%2520segmentation%2520of%2520the%2520humerus%2520and%250Aulna%2520using%2520the%2520detected%2520landmarks%2520as%2520input.%2520The%2520dataset%2520will%2520be%2520made%2520publicly%250Aavailable%2520upon%2520acceptance%2520of%2520this%2520paper%2520at%253A%250Ahttps%253A//github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measurement%20of%20Medial%20Elbow%20Joint%20Space%20using%20Landmark%20Detection&entry.906535625=Shizuka%20Akahori%20and%20Shotaro%20Teruya%20and%20Pragyan%20Shrestha%20and%20Yuichi%20Yoshii%20and%20Ryuhei%20Michinobu%20and%20Satoshi%20Iizuka%20and%20Itaru%20Kitahara&entry.1292438233=%20%20Ultrasound%20imaging%20of%20the%20medial%20elbow%20is%20crucial%20for%20the%20early%0Aidentification%20of%20Ulnar%20Collateral%20Ligament%20%28UCL%29%20injuries.%20Specifically%2C%0Ameasuring%20the%20elbow%20joint%20space%20in%20ultrasound%20images%20is%20used%20to%20assess%20the%0Avalgus%20instability%20of%20elbow.%20To%20automate%20this%20measurement%2C%20a%20precisely%0Aannotated%20dataset%20is%20necessary%3B%20however%2C%20no%20publicly%20available%20dataset%20has%20been%0Aproposed%20thus%20far.%20This%20study%20introduces%20a%20novel%20ultrasound%20medial%20elbow%0Adataset%20for%20measuring%20joint%20space%20to%20diagnose%20Ulnar%20Collateral%20Ligament%20%28UCL%29%0Ainjuries.%20The%20dataset%20comprises%204%2C201%20medial%20elbow%20ultrasound%20images%20from%2022%0Asubjects%2C%20with%20landmark%20annotations%20on%20the%20humerus%20and%20ulna.%20The%20annotations%0Aare%20made%20precisely%20by%20the%20authors%20under%20the%20supervision%20of%20three%20orthopedic%0Asurgeons.%20We%20evaluated%20joint%20space%20measurement%20methods%20using%20our%20proposed%0Adataset%20with%20several%20landmark%20detection%20approaches%2C%20including%20ViTPose%2C%20HRNet%2C%0APCT%2C%20YOLOv8%2C%20and%20U-Net.%20In%20addition%2C%20we%20propose%20using%20Shape%20Subspace%20%28SS%29%20for%0Alandmark%20refinement%20in%20heatmap-based%20landmark%20detection.%20The%20results%20show%20that%0Athe%20mean%20Euclidean%20distance%20error%20of%20joint%20space%20is%200.116%20mm%20when%20using%20HRNet.%0AFurthermore%2C%20the%20SS%20landmark%20refinement%20improves%20the%20mean%20absolute%20error%20of%0Alandmark%20positions%20by%200.010%20mm%20with%20HRNet%20and%20by%200.103%20mm%20with%20ViTPose%20on%0Aaverage.%20These%20highlight%20the%20potential%20for%20high-precision%2C%20real-time%20diagnosis%0Aof%20UCL%20injuries%20and%20associated%20risks%2C%20which%20could%20be%20leveraged%20in%20large-scale%0Ascreening.%20Lastly%2C%20we%20demonstrate%20point-based%20segmentation%20of%20the%20humerus%20and%0Aulna%20using%20the%20detected%20landmarks%20as%20input.%20The%20dataset%20will%20be%20made%20publicly%0Aavailable%20upon%20acceptance%20of%20this%20paper%20at%3A%0Ahttps%3A//github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13010v1&entry.124074799=Read"},
{"title": "Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt\n  Many-Shot Based Factual Knowledge Extraction", "author": "Qinyuan Wu and Mohammad Aflah Khan and Soumi Das and Vedant Nanda and Bishwamittra Ghosh and Camila Kolling and Till Speicher and Laurent Bindschaedler and Krishna P. Gummadi and Evimaria Terzi", "abstract": "  In this paper, we focus on the challenging task of reliably estimating\nfactual knowledge that is embedded inside large language models (LLMs). To\navoid reliability concerns with prior approaches, we propose to eliminate\nprompt engineering when probing LLMs for factual knowledge. Our approach,\ncalled Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the\nin-context learning ability of LLMs to communicate both the factual knowledge\nquestion as well as the expected answer format. Our knowledge estimator is both\nconceptually simpler (i.e., doesn't depend on meta-linguistic judgments of\nLLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that\nit can surface more of the latent knowledge embedded in LLMs. We also\ninvestigate how different design choices affect the performance of ZP-LKE.\nUsing the proposed estimator, we perform a large-scale evaluation of the\nfactual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2),\nMistral, Gemma, etc. over a large set of relations and facts from the Wikidata\nknowledge base. We observe differences in the factual knowledge between\ndifferent model families and models of different sizes, that some relations are\nconsistently better known than others but that models differ in the precise\nfacts they know, and differences in the knowledge of base models and their\nfinetuned counterparts. Code available at:\nhttps://github.com/QinyuanWu0710/ZeroPrompt_LKE\n", "link": "http://arxiv.org/abs/2404.12957v2", "date": "2024-12-17", "relevancy": 2.0193, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Reliable%20Latent%20Knowledge%20Estimation%20in%20LLMs%3A%20Zero-Prompt%0A%20%20Many-Shot%20Based%20Factual%20Knowledge%20Extraction&body=Title%3A%20Towards%20Reliable%20Latent%20Knowledge%20Estimation%20in%20LLMs%3A%20Zero-Prompt%0A%20%20Many-Shot%20Based%20Factual%20Knowledge%20Extraction%0AAuthor%3A%20Qinyuan%20Wu%20and%20Mohammad%20Aflah%20Khan%20and%20Soumi%20Das%20and%20Vedant%20Nanda%20and%20Bishwamittra%20Ghosh%20and%20Camila%20Kolling%20and%20Till%20Speicher%20and%20Laurent%20Bindschaedler%20and%20Krishna%20P.%20Gummadi%20and%20Evimaria%20Terzi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20challenging%20task%20of%20reliably%20estimating%0Afactual%20knowledge%20that%20is%20embedded%20inside%20large%20language%20models%20%28LLMs%29.%20To%0Aavoid%20reliability%20concerns%20with%20prior%20approaches%2C%20we%20propose%20to%20eliminate%0Aprompt%20engineering%20when%20probing%20LLMs%20for%20factual%20knowledge.%20Our%20approach%2C%0Acalled%20Zero-Prompt%20Latent%20Knowledge%20Estimator%20%28ZP-LKE%29%2C%20leverages%20the%0Ain-context%20learning%20ability%20of%20LLMs%20to%20communicate%20both%20the%20factual%20knowledge%0Aquestion%20as%20well%20as%20the%20expected%20answer%20format.%20Our%20knowledge%20estimator%20is%20both%0Aconceptually%20simpler%20%28i.e.%2C%20doesn%27t%20depend%20on%20meta-linguistic%20judgments%20of%0ALLMs%29%20and%20easier%20to%20apply%20%28i.e.%2C%20is%20not%20LLM-specific%29%2C%20and%20we%20demonstrate%20that%0Ait%20can%20surface%20more%20of%20the%20latent%20knowledge%20embedded%20in%20LLMs.%20We%20also%0Ainvestigate%20how%20different%20design%20choices%20affect%20the%20performance%20of%20ZP-LKE.%0AUsing%20the%20proposed%20estimator%2C%20we%20perform%20a%20large-scale%20evaluation%20of%20the%0Afactual%20knowledge%20of%20a%20variety%20of%20open-source%20LLMs%2C%20like%20OPT%2C%20Pythia%2C%20Llama%282%29%2C%0AMistral%2C%20Gemma%2C%20etc.%20over%20a%20large%20set%20of%20relations%20and%20facts%20from%20the%20Wikidata%0Aknowledge%20base.%20We%20observe%20differences%20in%20the%20factual%20knowledge%20between%0Adifferent%20model%20families%20and%20models%20of%20different%20sizes%2C%20that%20some%20relations%20are%0Aconsistently%20better%20known%20than%20others%20but%20that%20models%20differ%20in%20the%20precise%0Afacts%20they%20know%2C%20and%20differences%20in%20the%20knowledge%20of%20base%20models%20and%20their%0Afinetuned%20counterparts.%20Code%20available%20at%3A%0Ahttps%3A//github.com/QinyuanWu0710/ZeroPrompt_LKE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Reliable%2520Latent%2520Knowledge%2520Estimation%2520in%2520LLMs%253A%2520Zero-Prompt%250A%2520%2520Many-Shot%2520Based%2520Factual%2520Knowledge%2520Extraction%26entry.906535625%3DQinyuan%2520Wu%2520and%2520Mohammad%2520Aflah%2520Khan%2520and%2520Soumi%2520Das%2520and%2520Vedant%2520Nanda%2520and%2520Bishwamittra%2520Ghosh%2520and%2520Camila%2520Kolling%2520and%2520Till%2520Speicher%2520and%2520Laurent%2520Bindschaedler%2520and%2520Krishna%2520P.%2520Gummadi%2520and%2520Evimaria%2520Terzi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520challenging%2520task%2520of%2520reliably%2520estimating%250Afactual%2520knowledge%2520that%2520is%2520embedded%2520inside%2520large%2520language%2520models%2520%2528LLMs%2529.%2520To%250Aavoid%2520reliability%2520concerns%2520with%2520prior%2520approaches%252C%2520we%2520propose%2520to%2520eliminate%250Aprompt%2520engineering%2520when%2520probing%2520LLMs%2520for%2520factual%2520knowledge.%2520Our%2520approach%252C%250Acalled%2520Zero-Prompt%2520Latent%2520Knowledge%2520Estimator%2520%2528ZP-LKE%2529%252C%2520leverages%2520the%250Ain-context%2520learning%2520ability%2520of%2520LLMs%2520to%2520communicate%2520both%2520the%2520factual%2520knowledge%250Aquestion%2520as%2520well%2520as%2520the%2520expected%2520answer%2520format.%2520Our%2520knowledge%2520estimator%2520is%2520both%250Aconceptually%2520simpler%2520%2528i.e.%252C%2520doesn%2527t%2520depend%2520on%2520meta-linguistic%2520judgments%2520of%250ALLMs%2529%2520and%2520easier%2520to%2520apply%2520%2528i.e.%252C%2520is%2520not%2520LLM-specific%2529%252C%2520and%2520we%2520demonstrate%2520that%250Ait%2520can%2520surface%2520more%2520of%2520the%2520latent%2520knowledge%2520embedded%2520in%2520LLMs.%2520We%2520also%250Ainvestigate%2520how%2520different%2520design%2520choices%2520affect%2520the%2520performance%2520of%2520ZP-LKE.%250AUsing%2520the%2520proposed%2520estimator%252C%2520we%2520perform%2520a%2520large-scale%2520evaluation%2520of%2520the%250Afactual%2520knowledge%2520of%2520a%2520variety%2520of%2520open-source%2520LLMs%252C%2520like%2520OPT%252C%2520Pythia%252C%2520Llama%25282%2529%252C%250AMistral%252C%2520Gemma%252C%2520etc.%2520over%2520a%2520large%2520set%2520of%2520relations%2520and%2520facts%2520from%2520the%2520Wikidata%250Aknowledge%2520base.%2520We%2520observe%2520differences%2520in%2520the%2520factual%2520knowledge%2520between%250Adifferent%2520model%2520families%2520and%2520models%2520of%2520different%2520sizes%252C%2520that%2520some%2520relations%2520are%250Aconsistently%2520better%2520known%2520than%2520others%2520but%2520that%2520models%2520differ%2520in%2520the%2520precise%250Afacts%2520they%2520know%252C%2520and%2520differences%2520in%2520the%2520knowledge%2520of%2520base%2520models%2520and%2520their%250Afinetuned%2520counterparts.%2520Code%2520available%2520at%253A%250Ahttps%253A//github.com/QinyuanWu0710/ZeroPrompt_LKE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Reliable%20Latent%20Knowledge%20Estimation%20in%20LLMs%3A%20Zero-Prompt%0A%20%20Many-Shot%20Based%20Factual%20Knowledge%20Extraction&entry.906535625=Qinyuan%20Wu%20and%20Mohammad%20Aflah%20Khan%20and%20Soumi%20Das%20and%20Vedant%20Nanda%20and%20Bishwamittra%20Ghosh%20and%20Camila%20Kolling%20and%20Till%20Speicher%20and%20Laurent%20Bindschaedler%20and%20Krishna%20P.%20Gummadi%20and%20Evimaria%20Terzi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20challenging%20task%20of%20reliably%20estimating%0Afactual%20knowledge%20that%20is%20embedded%20inside%20large%20language%20models%20%28LLMs%29.%20To%0Aavoid%20reliability%20concerns%20with%20prior%20approaches%2C%20we%20propose%20to%20eliminate%0Aprompt%20engineering%20when%20probing%20LLMs%20for%20factual%20knowledge.%20Our%20approach%2C%0Acalled%20Zero-Prompt%20Latent%20Knowledge%20Estimator%20%28ZP-LKE%29%2C%20leverages%20the%0Ain-context%20learning%20ability%20of%20LLMs%20to%20communicate%20both%20the%20factual%20knowledge%0Aquestion%20as%20well%20as%20the%20expected%20answer%20format.%20Our%20knowledge%20estimator%20is%20both%0Aconceptually%20simpler%20%28i.e.%2C%20doesn%27t%20depend%20on%20meta-linguistic%20judgments%20of%0ALLMs%29%20and%20easier%20to%20apply%20%28i.e.%2C%20is%20not%20LLM-specific%29%2C%20and%20we%20demonstrate%20that%0Ait%20can%20surface%20more%20of%20the%20latent%20knowledge%20embedded%20in%20LLMs.%20We%20also%0Ainvestigate%20how%20different%20design%20choices%20affect%20the%20performance%20of%20ZP-LKE.%0AUsing%20the%20proposed%20estimator%2C%20we%20perform%20a%20large-scale%20evaluation%20of%20the%0Afactual%20knowledge%20of%20a%20variety%20of%20open-source%20LLMs%2C%20like%20OPT%2C%20Pythia%2C%20Llama%282%29%2C%0AMistral%2C%20Gemma%2C%20etc.%20over%20a%20large%20set%20of%20relations%20and%20facts%20from%20the%20Wikidata%0Aknowledge%20base.%20We%20observe%20differences%20in%20the%20factual%20knowledge%20between%0Adifferent%20model%20families%20and%20models%20of%20different%20sizes%2C%20that%20some%20relations%20are%0Aconsistently%20better%20known%20than%20others%20but%20that%20models%20differ%20in%20the%20precise%0Afacts%20they%20know%2C%20and%20differences%20in%20the%20knowledge%20of%20base%20models%20and%20their%0Afinetuned%20counterparts.%20Code%20available%20at%3A%0Ahttps%3A//github.com/QinyuanWu0710/ZeroPrompt_LKE%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12957v2&entry.124074799=Read"},
{"title": "Are Your LLMs Capable of Stable Reasoning?", "author": "Junnan Liu and Hongwei Liu and Linchen Xiao and Ziyi Wang and Kuikun Liu and Songyang Gao and Wenwei Zhang and Songyang Zhang and Kai Chen", "abstract": "  The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.\n", "link": "http://arxiv.org/abs/2412.13147v1", "date": "2024-12-17", "relevancy": 2.017, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Your%20LLMs%20Capable%20of%20Stable%20Reasoning%3F&body=Title%3A%20Are%20Your%20LLMs%20Capable%20of%20Stable%20Reasoning%3F%0AAuthor%3A%20Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Linchen%20Xiao%20and%20Ziyi%20Wang%20and%20Kuikun%20Liu%20and%20Songyang%20Gao%20and%20Wenwei%20Zhang%20and%20Songyang%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20demonstrated%0Aremarkable%20progress%20in%20complex%20reasoning%20tasks.%20However%2C%20a%20significant%0Adiscrepancy%20persists%20between%20benchmark%20performances%20and%20real-world%0Aapplications.%20We%20identify%20this%20gap%20as%20primarily%20stemming%20from%20current%0Aevaluation%20protocols%20and%20metrics%2C%20which%20inadequately%20capture%20the%20full%20spectrum%0Aof%20LLM%20capabilities%2C%20particularly%20in%20complex%20reasoning%20tasks%20where%20both%0Aaccuracy%20and%20consistency%20are%20crucial.%20This%20work%20makes%20two%20key%20contributions.%0AFirst%2C%20we%20introduce%20G-Pass%40k%2C%20a%20novel%20evaluation%20metric%20that%20provides%20a%0Acontinuous%20assessment%20of%20model%20performance%20across%20multiple%20sampling%20attempts%2C%0Aquantifying%20both%20the%20model%27s%20peak%20performance%20potential%20and%20its%20stability.%0ASecond%2C%20we%20present%20LiveMathBench%2C%20a%20dynamic%20benchmark%20comprising%20challenging%2C%0Acontemporary%20mathematical%20problems%20designed%20to%20minimize%20data%20leakage%20risks%0Aduring%20evaluation.%20Through%20extensive%20experiments%20using%20G-Pass%40k%20on%0Astate-of-the-art%20LLMs%20with%20LiveMathBench%2C%20we%20provide%20comprehensive%20insights%0Ainto%20both%20their%20maximum%20capabilities%20and%20operational%20consistency.%20Our%20findings%0Areveal%20substantial%20room%20for%20improvement%20in%20LLMs%27%20%22realistic%22%20reasoning%0Acapabilities%2C%20highlighting%20the%20need%20for%20more%20robust%20evaluation%20methods.%20The%0Abenchmark%20and%20detailed%20results%20are%20available%20at%3A%0Ahttps%3A//github.com/open-compass/GPassK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Your%2520LLMs%2520Capable%2520of%2520Stable%2520Reasoning%253F%26entry.906535625%3DJunnan%2520Liu%2520and%2520Hongwei%2520Liu%2520and%2520Linchen%2520Xiao%2520and%2520Ziyi%2520Wang%2520and%2520Kuikun%2520Liu%2520and%2520Songyang%2520Gao%2520and%2520Wenwei%2520Zhang%2520and%2520Songyang%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520demonstrated%250Aremarkable%2520progress%2520in%2520complex%2520reasoning%2520tasks.%2520However%252C%2520a%2520significant%250Adiscrepancy%2520persists%2520between%2520benchmark%2520performances%2520and%2520real-world%250Aapplications.%2520We%2520identify%2520this%2520gap%2520as%2520primarily%2520stemming%2520from%2520current%250Aevaluation%2520protocols%2520and%2520metrics%252C%2520which%2520inadequately%2520capture%2520the%2520full%2520spectrum%250Aof%2520LLM%2520capabilities%252C%2520particularly%2520in%2520complex%2520reasoning%2520tasks%2520where%2520both%250Aaccuracy%2520and%2520consistency%2520are%2520crucial.%2520This%2520work%2520makes%2520two%2520key%2520contributions.%250AFirst%252C%2520we%2520introduce%2520G-Pass%2540k%252C%2520a%2520novel%2520evaluation%2520metric%2520that%2520provides%2520a%250Acontinuous%2520assessment%2520of%2520model%2520performance%2520across%2520multiple%2520sampling%2520attempts%252C%250Aquantifying%2520both%2520the%2520model%2527s%2520peak%2520performance%2520potential%2520and%2520its%2520stability.%250ASecond%252C%2520we%2520present%2520LiveMathBench%252C%2520a%2520dynamic%2520benchmark%2520comprising%2520challenging%252C%250Acontemporary%2520mathematical%2520problems%2520designed%2520to%2520minimize%2520data%2520leakage%2520risks%250Aduring%2520evaluation.%2520Through%2520extensive%2520experiments%2520using%2520G-Pass%2540k%2520on%250Astate-of-the-art%2520LLMs%2520with%2520LiveMathBench%252C%2520we%2520provide%2520comprehensive%2520insights%250Ainto%2520both%2520their%2520maximum%2520capabilities%2520and%2520operational%2520consistency.%2520Our%2520findings%250Areveal%2520substantial%2520room%2520for%2520improvement%2520in%2520LLMs%2527%2520%2522realistic%2522%2520reasoning%250Acapabilities%252C%2520highlighting%2520the%2520need%2520for%2520more%2520robust%2520evaluation%2520methods.%2520The%250Abenchmark%2520and%2520detailed%2520results%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/open-compass/GPassK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Your%20LLMs%20Capable%20of%20Stable%20Reasoning%3F&entry.906535625=Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Linchen%20Xiao%20and%20Ziyi%20Wang%20and%20Kuikun%20Liu%20and%20Songyang%20Gao%20and%20Wenwei%20Zhang%20and%20Songyang%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20demonstrated%0Aremarkable%20progress%20in%20complex%20reasoning%20tasks.%20However%2C%20a%20significant%0Adiscrepancy%20persists%20between%20benchmark%20performances%20and%20real-world%0Aapplications.%20We%20identify%20this%20gap%20as%20primarily%20stemming%20from%20current%0Aevaluation%20protocols%20and%20metrics%2C%20which%20inadequately%20capture%20the%20full%20spectrum%0Aof%20LLM%20capabilities%2C%20particularly%20in%20complex%20reasoning%20tasks%20where%20both%0Aaccuracy%20and%20consistency%20are%20crucial.%20This%20work%20makes%20two%20key%20contributions.%0AFirst%2C%20we%20introduce%20G-Pass%40k%2C%20a%20novel%20evaluation%20metric%20that%20provides%20a%0Acontinuous%20assessment%20of%20model%20performance%20across%20multiple%20sampling%20attempts%2C%0Aquantifying%20both%20the%20model%27s%20peak%20performance%20potential%20and%20its%20stability.%0ASecond%2C%20we%20present%20LiveMathBench%2C%20a%20dynamic%20benchmark%20comprising%20challenging%2C%0Acontemporary%20mathematical%20problems%20designed%20to%20minimize%20data%20leakage%20risks%0Aduring%20evaluation.%20Through%20extensive%20experiments%20using%20G-Pass%40k%20on%0Astate-of-the-art%20LLMs%20with%20LiveMathBench%2C%20we%20provide%20comprehensive%20insights%0Ainto%20both%20their%20maximum%20capabilities%20and%20operational%20consistency.%20Our%20findings%0Areveal%20substantial%20room%20for%20improvement%20in%20LLMs%27%20%22realistic%22%20reasoning%0Acapabilities%2C%20highlighting%20the%20need%20for%20more%20robust%20evaluation%20methods.%20The%0Abenchmark%20and%20detailed%20results%20are%20available%20at%3A%0Ahttps%3A//github.com/open-compass/GPassK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13147v1&entry.124074799=Read"},
{"title": "LossVal: Efficient Data Valuation for Neural Networks", "author": "Tim Wibiral and Mohamed Karim Belaid and Maximilian Rabus and Ansgar Scherp", "abstract": "  Assessing the importance of individual training samples is a key challenge in\nmachine learning. Traditional approaches retrain models with and without\nspecific samples, which is computationally expensive and ignores dependencies\nbetween data points. We introduce LossVal, an efficient data valuation method\nthat computes importance scores during neural network training by embedding a\nself-weighting mechanism into loss functions like cross-entropy and mean\nsquared error. LossVal reduces computational costs, making it suitable for\nlarge datasets and practical applications. Experiments on classification and\nregression tasks across multiple datasets show that LossVal effectively\nidentifies noisy samples and is able to distinguish helpful from harmful\nsamples. We examine the gradient calculation of LossVal to highlight its\nadvantages. The source code is available at:\nhttps://github.com/twibiral/LossVal\n", "link": "http://arxiv.org/abs/2412.04158v2", "date": "2024-12-17", "relevancy": 2.0115, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5174}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4952}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LossVal%3A%20Efficient%20Data%20Valuation%20for%20Neural%20Networks&body=Title%3A%20LossVal%3A%20Efficient%20Data%20Valuation%20for%20Neural%20Networks%0AAuthor%3A%20Tim%20Wibiral%20and%20Mohamed%20Karim%20Belaid%20and%20Maximilian%20Rabus%20and%20Ansgar%20Scherp%0AAbstract%3A%20%20%20Assessing%20the%20importance%20of%20individual%20training%20samples%20is%20a%20key%20challenge%20in%0Amachine%20learning.%20Traditional%20approaches%20retrain%20models%20with%20and%20without%0Aspecific%20samples%2C%20which%20is%20computationally%20expensive%20and%20ignores%20dependencies%0Abetween%20data%20points.%20We%20introduce%20LossVal%2C%20an%20efficient%20data%20valuation%20method%0Athat%20computes%20importance%20scores%20during%20neural%20network%20training%20by%20embedding%20a%0Aself-weighting%20mechanism%20into%20loss%20functions%20like%20cross-entropy%20and%20mean%0Asquared%20error.%20LossVal%20reduces%20computational%20costs%2C%20making%20it%20suitable%20for%0Alarge%20datasets%20and%20practical%20applications.%20Experiments%20on%20classification%20and%0Aregression%20tasks%20across%20multiple%20datasets%20show%20that%20LossVal%20effectively%0Aidentifies%20noisy%20samples%20and%20is%20able%20to%20distinguish%20helpful%20from%20harmful%0Asamples.%20We%20examine%20the%20gradient%20calculation%20of%20LossVal%20to%20highlight%20its%0Aadvantages.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/twibiral/LossVal%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04158v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLossVal%253A%2520Efficient%2520Data%2520Valuation%2520for%2520Neural%2520Networks%26entry.906535625%3DTim%2520Wibiral%2520and%2520Mohamed%2520Karim%2520Belaid%2520and%2520Maximilian%2520Rabus%2520and%2520Ansgar%2520Scherp%26entry.1292438233%3D%2520%2520Assessing%2520the%2520importance%2520of%2520individual%2520training%2520samples%2520is%2520a%2520key%2520challenge%2520in%250Amachine%2520learning.%2520Traditional%2520approaches%2520retrain%2520models%2520with%2520and%2520without%250Aspecific%2520samples%252C%2520which%2520is%2520computationally%2520expensive%2520and%2520ignores%2520dependencies%250Abetween%2520data%2520points.%2520We%2520introduce%2520LossVal%252C%2520an%2520efficient%2520data%2520valuation%2520method%250Athat%2520computes%2520importance%2520scores%2520during%2520neural%2520network%2520training%2520by%2520embedding%2520a%250Aself-weighting%2520mechanism%2520into%2520loss%2520functions%2520like%2520cross-entropy%2520and%2520mean%250Asquared%2520error.%2520LossVal%2520reduces%2520computational%2520costs%252C%2520making%2520it%2520suitable%2520for%250Alarge%2520datasets%2520and%2520practical%2520applications.%2520Experiments%2520on%2520classification%2520and%250Aregression%2520tasks%2520across%2520multiple%2520datasets%2520show%2520that%2520LossVal%2520effectively%250Aidentifies%2520noisy%2520samples%2520and%2520is%2520able%2520to%2520distinguish%2520helpful%2520from%2520harmful%250Asamples.%2520We%2520examine%2520the%2520gradient%2520calculation%2520of%2520LossVal%2520to%2520highlight%2520its%250Aadvantages.%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/twibiral/LossVal%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04158v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LossVal%3A%20Efficient%20Data%20Valuation%20for%20Neural%20Networks&entry.906535625=Tim%20Wibiral%20and%20Mohamed%20Karim%20Belaid%20and%20Maximilian%20Rabus%20and%20Ansgar%20Scherp&entry.1292438233=%20%20Assessing%20the%20importance%20of%20individual%20training%20samples%20is%20a%20key%20challenge%20in%0Amachine%20learning.%20Traditional%20approaches%20retrain%20models%20with%20and%20without%0Aspecific%20samples%2C%20which%20is%20computationally%20expensive%20and%20ignores%20dependencies%0Abetween%20data%20points.%20We%20introduce%20LossVal%2C%20an%20efficient%20data%20valuation%20method%0Athat%20computes%20importance%20scores%20during%20neural%20network%20training%20by%20embedding%20a%0Aself-weighting%20mechanism%20into%20loss%20functions%20like%20cross-entropy%20and%20mean%0Asquared%20error.%20LossVal%20reduces%20computational%20costs%2C%20making%20it%20suitable%20for%0Alarge%20datasets%20and%20practical%20applications.%20Experiments%20on%20classification%20and%0Aregression%20tasks%20across%20multiple%20datasets%20show%20that%20LossVal%20effectively%0Aidentifies%20noisy%20samples%20and%20is%20able%20to%20distinguish%20helpful%20from%20harmful%0Asamples.%20We%20examine%20the%20gradient%20calculation%20of%20LossVal%20to%20highlight%20its%0Aadvantages.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/twibiral/LossVal%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04158v2&entry.124074799=Read"},
{"title": "What is YOLOv6? A Deep Insight into the Object Detection Model", "author": "Athulya Sundaresan Geetha", "abstract": "  This work explores the YOLOv6 object detection model in depth, concentrating\non its design framework, optimization techniques, and detection capabilities.\nYOLOv6's core elements consist of the EfficientRep Backbone for robust feature\nextraction and the Rep-PAN Neck for seamless feature aggregation, ensuring\nhigh-performance object detection. Evaluated on the COCO dataset, YOLOv6-N\nachieves 37.5\\% AP at 1187 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S reaches\n45.0\\% AP at 484 FPS, outperforming models like PPYOLOE-S, YOLOv5-S, YOLOX-S,\nand YOLOv8-S in the same class. Moreover, YOLOv6-M and YOLOv6-L also show\nbetter accuracy (50.0\\% and 52.8\\%) while maintaining comparable inference\nspeeds to other detectors. With an upgraded backbone and neck structure,\nYOLOv6-L6 delivers cutting-edge accuracy in real-time.\n", "link": "http://arxiv.org/abs/2412.13006v1", "date": "2024-12-17", "relevancy": 2.0046, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20is%20YOLOv6%3F%20A%20Deep%20Insight%20into%20the%20Object%20Detection%20Model&body=Title%3A%20What%20is%20YOLOv6%3F%20A%20Deep%20Insight%20into%20the%20Object%20Detection%20Model%0AAuthor%3A%20Athulya%20Sundaresan%20Geetha%0AAbstract%3A%20%20%20This%20work%20explores%20the%20YOLOv6%20object%20detection%20model%20in%20depth%2C%20concentrating%0Aon%20its%20design%20framework%2C%20optimization%20techniques%2C%20and%20detection%20capabilities.%0AYOLOv6%27s%20core%20elements%20consist%20of%20the%20EfficientRep%20Backbone%20for%20robust%20feature%0Aextraction%20and%20the%20Rep-PAN%20Neck%20for%20seamless%20feature%20aggregation%2C%20ensuring%0Ahigh-performance%20object%20detection.%20Evaluated%20on%20the%20COCO%20dataset%2C%20YOLOv6-N%0Aachieves%2037.5%5C%25%20AP%20at%201187%20FPS%20on%20an%20NVIDIA%20Tesla%20T4%20GPU.%20YOLOv6-S%20reaches%0A45.0%5C%25%20AP%20at%20484%20FPS%2C%20outperforming%20models%20like%20PPYOLOE-S%2C%20YOLOv5-S%2C%20YOLOX-S%2C%0Aand%20YOLOv8-S%20in%20the%20same%20class.%20Moreover%2C%20YOLOv6-M%20and%20YOLOv6-L%20also%20show%0Abetter%20accuracy%20%2850.0%5C%25%20and%2052.8%5C%25%29%20while%20maintaining%20comparable%20inference%0Aspeeds%20to%20other%20detectors.%20With%20an%20upgraded%20backbone%20and%20neck%20structure%2C%0AYOLOv6-L6%20delivers%20cutting-edge%20accuracy%20in%20real-time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520is%2520YOLOv6%253F%2520A%2520Deep%2520Insight%2520into%2520the%2520Object%2520Detection%2520Model%26entry.906535625%3DAthulya%2520Sundaresan%2520Geetha%26entry.1292438233%3D%2520%2520This%2520work%2520explores%2520the%2520YOLOv6%2520object%2520detection%2520model%2520in%2520depth%252C%2520concentrating%250Aon%2520its%2520design%2520framework%252C%2520optimization%2520techniques%252C%2520and%2520detection%2520capabilities.%250AYOLOv6%2527s%2520core%2520elements%2520consist%2520of%2520the%2520EfficientRep%2520Backbone%2520for%2520robust%2520feature%250Aextraction%2520and%2520the%2520Rep-PAN%2520Neck%2520for%2520seamless%2520feature%2520aggregation%252C%2520ensuring%250Ahigh-performance%2520object%2520detection.%2520Evaluated%2520on%2520the%2520COCO%2520dataset%252C%2520YOLOv6-N%250Aachieves%252037.5%255C%2525%2520AP%2520at%25201187%2520FPS%2520on%2520an%2520NVIDIA%2520Tesla%2520T4%2520GPU.%2520YOLOv6-S%2520reaches%250A45.0%255C%2525%2520AP%2520at%2520484%2520FPS%252C%2520outperforming%2520models%2520like%2520PPYOLOE-S%252C%2520YOLOv5-S%252C%2520YOLOX-S%252C%250Aand%2520YOLOv8-S%2520in%2520the%2520same%2520class.%2520Moreover%252C%2520YOLOv6-M%2520and%2520YOLOv6-L%2520also%2520show%250Abetter%2520accuracy%2520%252850.0%255C%2525%2520and%252052.8%255C%2525%2529%2520while%2520maintaining%2520comparable%2520inference%250Aspeeds%2520to%2520other%2520detectors.%2520With%2520an%2520upgraded%2520backbone%2520and%2520neck%2520structure%252C%250AYOLOv6-L6%2520delivers%2520cutting-edge%2520accuracy%2520in%2520real-time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20is%20YOLOv6%3F%20A%20Deep%20Insight%20into%20the%20Object%20Detection%20Model&entry.906535625=Athulya%20Sundaresan%20Geetha&entry.1292438233=%20%20This%20work%20explores%20the%20YOLOv6%20object%20detection%20model%20in%20depth%2C%20concentrating%0Aon%20its%20design%20framework%2C%20optimization%20techniques%2C%20and%20detection%20capabilities.%0AYOLOv6%27s%20core%20elements%20consist%20of%20the%20EfficientRep%20Backbone%20for%20robust%20feature%0Aextraction%20and%20the%20Rep-PAN%20Neck%20for%20seamless%20feature%20aggregation%2C%20ensuring%0Ahigh-performance%20object%20detection.%20Evaluated%20on%20the%20COCO%20dataset%2C%20YOLOv6-N%0Aachieves%2037.5%5C%25%20AP%20at%201187%20FPS%20on%20an%20NVIDIA%20Tesla%20T4%20GPU.%20YOLOv6-S%20reaches%0A45.0%5C%25%20AP%20at%20484%20FPS%2C%20outperforming%20models%20like%20PPYOLOE-S%2C%20YOLOv5-S%2C%20YOLOX-S%2C%0Aand%20YOLOv8-S%20in%20the%20same%20class.%20Moreover%2C%20YOLOv6-M%20and%20YOLOv6-L%20also%20show%0Abetter%20accuracy%20%2850.0%5C%25%20and%2052.8%5C%25%29%20while%20maintaining%20comparable%20inference%0Aspeeds%20to%20other%20detectors.%20With%20an%20upgraded%20backbone%20and%20neck%20structure%2C%0AYOLOv6-L6%20delivers%20cutting-edge%20accuracy%20in%20real-time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13006v1&entry.124074799=Read"},
{"title": "SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training\n  With Significant Memory Reduction", "author": "Chao Ma and Wenbo Gong and Meyer Scetbon and Edward Meeds", "abstract": "  Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they maintain additional moving\naverage states throughout training, which results in memory requirements\nseveral times greater than the model. This overhead imposes constraints on\nscalability and computational efficiency. On the other hand, while stochastic\ngradient descent (SGD) is optimal in terms of memory efficiency, their\ncapability in LLM training is limited (Zhao et al., 2024b).\n  To address this dilemma, we show that pre-processing SGD is sufficient to\nreach Adam-level performance on LLMs. Specifically, we propose to preprocess\nthe instantaneous stochastic gradients with two simple operators:\n$\\mathtt{GradNorm}$ and $\\mathtt{GradWhitening}$. $\\mathtt{GradNorm}$\nstabilizes gradient distributions, and $\\mathtt{GradWhitening}$ counteracts the\nlocal curvature of the loss landscape, respectively. This results in SWAN (SGD\nwith Whitening And Normalization), a stochastic optimizer that eliminates the\nneed to store any accumulative state variables. Empirically, SWAN has the same\nmemory footprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end\nmemory compared to Adam. In language modeling tasks, SWAN demonstrates the same\nor even a substantial improvement over Adam. Specifically, when pre-training\nthe LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by\nreaching the same evaluation perplexity in less than half tokens seen.\n", "link": "http://arxiv.org/abs/2412.13148v1", "date": "2024-12-17", "relevancy": 1.9931, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5088}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4985}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWAN%3A%20Preprocessing%20SGD%20Enables%20Adam-Level%20Performance%20On%20LLM%20Training%0A%20%20With%20Significant%20Memory%20Reduction&body=Title%3A%20SWAN%3A%20Preprocessing%20SGD%20Enables%20Adam-Level%20Performance%20On%20LLM%20Training%0A%20%20With%20Significant%20Memory%20Reduction%0AAuthor%3A%20Chao%20Ma%20and%20Wenbo%20Gong%20and%20Meyer%20Scetbon%20and%20Edward%20Meeds%0AAbstract%3A%20%20%20Adaptive%20optimizers%20such%20as%20Adam%20%28Kingma%20%26%20Ba%2C%202015%29%20have%20been%20central%20to%20the%0Asuccess%20of%20large%20language%20models.%20However%2C%20they%20maintain%20additional%20moving%0Aaverage%20states%20throughout%20training%2C%20which%20results%20in%20memory%20requirements%0Aseveral%20times%20greater%20than%20the%20model.%20This%20overhead%20imposes%20constraints%20on%0Ascalability%20and%20computational%20efficiency.%20On%20the%20other%20hand%2C%20while%20stochastic%0Agradient%20descent%20%28SGD%29%20is%20optimal%20in%20terms%20of%20memory%20efficiency%2C%20their%0Acapability%20in%20LLM%20training%20is%20limited%20%28Zhao%20et%20al.%2C%202024b%29.%0A%20%20To%20address%20this%20dilemma%2C%20we%20show%20that%20pre-processing%20SGD%20is%20sufficient%20to%0Areach%20Adam-level%20performance%20on%20LLMs.%20Specifically%2C%20we%20propose%20to%20preprocess%0Athe%20instantaneous%20stochastic%20gradients%20with%20two%20simple%20operators%3A%0A%24%5Cmathtt%7BGradNorm%7D%24%20and%20%24%5Cmathtt%7BGradWhitening%7D%24.%20%24%5Cmathtt%7BGradNorm%7D%24%0Astabilizes%20gradient%20distributions%2C%20and%20%24%5Cmathtt%7BGradWhitening%7D%24%20counteracts%20the%0Alocal%20curvature%20of%20the%20loss%20landscape%2C%20respectively.%20This%20results%20in%20SWAN%20%28SGD%0Awith%20Whitening%20And%20Normalization%29%2C%20a%20stochastic%20optimizer%20that%20eliminates%20the%0Aneed%20to%20store%20any%20accumulative%20state%20variables.%20Empirically%2C%20SWAN%20has%20the%20same%0Amemory%20footprint%20as%20SGD%2C%20achieving%20%24%5Capprox%2050%5C%25%24%20reduction%20on%20total%20end-to-end%0Amemory%20compared%20to%20Adam.%20In%20language%20modeling%20tasks%2C%20SWAN%20demonstrates%20the%20same%0Aor%20even%20a%20substantial%20improvement%20over%20Adam.%20Specifically%2C%20when%20pre-training%0Athe%20LLaMa%20model%20with%20350M%20and%201.3B%20parameters%2C%20SWAN%20achieves%20a%202x%20speedup%20by%0Areaching%20the%20same%20evaluation%20perplexity%20in%20less%20than%20half%20tokens%20seen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWAN%253A%2520Preprocessing%2520SGD%2520Enables%2520Adam-Level%2520Performance%2520On%2520LLM%2520Training%250A%2520%2520With%2520Significant%2520Memory%2520Reduction%26entry.906535625%3DChao%2520Ma%2520and%2520Wenbo%2520Gong%2520and%2520Meyer%2520Scetbon%2520and%2520Edward%2520Meeds%26entry.1292438233%3D%2520%2520Adaptive%2520optimizers%2520such%2520as%2520Adam%2520%2528Kingma%2520%2526%2520Ba%252C%25202015%2529%2520have%2520been%2520central%2520to%2520the%250Asuccess%2520of%2520large%2520language%2520models.%2520However%252C%2520they%2520maintain%2520additional%2520moving%250Aaverage%2520states%2520throughout%2520training%252C%2520which%2520results%2520in%2520memory%2520requirements%250Aseveral%2520times%2520greater%2520than%2520the%2520model.%2520This%2520overhead%2520imposes%2520constraints%2520on%250Ascalability%2520and%2520computational%2520efficiency.%2520On%2520the%2520other%2520hand%252C%2520while%2520stochastic%250Agradient%2520descent%2520%2528SGD%2529%2520is%2520optimal%2520in%2520terms%2520of%2520memory%2520efficiency%252C%2520their%250Acapability%2520in%2520LLM%2520training%2520is%2520limited%2520%2528Zhao%2520et%2520al.%252C%25202024b%2529.%250A%2520%2520To%2520address%2520this%2520dilemma%252C%2520we%2520show%2520that%2520pre-processing%2520SGD%2520is%2520sufficient%2520to%250Areach%2520Adam-level%2520performance%2520on%2520LLMs.%2520Specifically%252C%2520we%2520propose%2520to%2520preprocess%250Athe%2520instantaneous%2520stochastic%2520gradients%2520with%2520two%2520simple%2520operators%253A%250A%2524%255Cmathtt%257BGradNorm%257D%2524%2520and%2520%2524%255Cmathtt%257BGradWhitening%257D%2524.%2520%2524%255Cmathtt%257BGradNorm%257D%2524%250Astabilizes%2520gradient%2520distributions%252C%2520and%2520%2524%255Cmathtt%257BGradWhitening%257D%2524%2520counteracts%2520the%250Alocal%2520curvature%2520of%2520the%2520loss%2520landscape%252C%2520respectively.%2520This%2520results%2520in%2520SWAN%2520%2528SGD%250Awith%2520Whitening%2520And%2520Normalization%2529%252C%2520a%2520stochastic%2520optimizer%2520that%2520eliminates%2520the%250Aneed%2520to%2520store%2520any%2520accumulative%2520state%2520variables.%2520Empirically%252C%2520SWAN%2520has%2520the%2520same%250Amemory%2520footprint%2520as%2520SGD%252C%2520achieving%2520%2524%255Capprox%252050%255C%2525%2524%2520reduction%2520on%2520total%2520end-to-end%250Amemory%2520compared%2520to%2520Adam.%2520In%2520language%2520modeling%2520tasks%252C%2520SWAN%2520demonstrates%2520the%2520same%250Aor%2520even%2520a%2520substantial%2520improvement%2520over%2520Adam.%2520Specifically%252C%2520when%2520pre-training%250Athe%2520LLaMa%2520model%2520with%2520350M%2520and%25201.3B%2520parameters%252C%2520SWAN%2520achieves%2520a%25202x%2520speedup%2520by%250Areaching%2520the%2520same%2520evaluation%2520perplexity%2520in%2520less%2520than%2520half%2520tokens%2520seen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWAN%3A%20Preprocessing%20SGD%20Enables%20Adam-Level%20Performance%20On%20LLM%20Training%0A%20%20With%20Significant%20Memory%20Reduction&entry.906535625=Chao%20Ma%20and%20Wenbo%20Gong%20and%20Meyer%20Scetbon%20and%20Edward%20Meeds&entry.1292438233=%20%20Adaptive%20optimizers%20such%20as%20Adam%20%28Kingma%20%26%20Ba%2C%202015%29%20have%20been%20central%20to%20the%0Asuccess%20of%20large%20language%20models.%20However%2C%20they%20maintain%20additional%20moving%0Aaverage%20states%20throughout%20training%2C%20which%20results%20in%20memory%20requirements%0Aseveral%20times%20greater%20than%20the%20model.%20This%20overhead%20imposes%20constraints%20on%0Ascalability%20and%20computational%20efficiency.%20On%20the%20other%20hand%2C%20while%20stochastic%0Agradient%20descent%20%28SGD%29%20is%20optimal%20in%20terms%20of%20memory%20efficiency%2C%20their%0Acapability%20in%20LLM%20training%20is%20limited%20%28Zhao%20et%20al.%2C%202024b%29.%0A%20%20To%20address%20this%20dilemma%2C%20we%20show%20that%20pre-processing%20SGD%20is%20sufficient%20to%0Areach%20Adam-level%20performance%20on%20LLMs.%20Specifically%2C%20we%20propose%20to%20preprocess%0Athe%20instantaneous%20stochastic%20gradients%20with%20two%20simple%20operators%3A%0A%24%5Cmathtt%7BGradNorm%7D%24%20and%20%24%5Cmathtt%7BGradWhitening%7D%24.%20%24%5Cmathtt%7BGradNorm%7D%24%0Astabilizes%20gradient%20distributions%2C%20and%20%24%5Cmathtt%7BGradWhitening%7D%24%20counteracts%20the%0Alocal%20curvature%20of%20the%20loss%20landscape%2C%20respectively.%20This%20results%20in%20SWAN%20%28SGD%0Awith%20Whitening%20And%20Normalization%29%2C%20a%20stochastic%20optimizer%20that%20eliminates%20the%0Aneed%20to%20store%20any%20accumulative%20state%20variables.%20Empirically%2C%20SWAN%20has%20the%20same%0Amemory%20footprint%20as%20SGD%2C%20achieving%20%24%5Capprox%2050%5C%25%24%20reduction%20on%20total%20end-to-end%0Amemory%20compared%20to%20Adam.%20In%20language%20modeling%20tasks%2C%20SWAN%20demonstrates%20the%20same%0Aor%20even%20a%20substantial%20improvement%20over%20Adam.%20Specifically%2C%20when%20pre-training%0Athe%20LLaMa%20model%20with%20350M%20and%201.3B%20parameters%2C%20SWAN%20achieves%20a%202x%20speedup%20by%0Areaching%20the%20same%20evaluation%20perplexity%20in%20less%20than%20half%20tokens%20seen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13148v1&entry.124074799=Read"},
{"title": "Practicable Black-box Evasion Attacks on Link Prediction in Dynamic\n  Graphs -- A Graph Sequential Embedding Method", "author": "Jiate Li and Meng Pang and Binghui Wang", "abstract": "  Link prediction in dynamic graphs (LPDG) has been widely applied to\nreal-world applications such as website recommendation, traffic flow\nprediction, organizational studies, etc. These models are usually kept local\nand secure, with only the interactive interface restrictively available to the\npublic. Thus, the problem of the black-box evasion attack on the LPDG model,\nwhere model interactions and data perturbations are restricted, seems to be\nessential and meaningful in practice. In this paper, we propose the first\npracticable black-box evasion attack method that achieves effective attacks\nagainst the target LPDG model, within a limited amount of interactions and\nperturbations. To perform effective attacks under limited perturbations, we\ndevelop a graph sequential embedding model to find the desired state embedding\nof the dynamic graph sequences, under a deep reinforcement learning framework.\nTo overcome the scarcity of interactions, we design a multi-environment\ntraining pipeline and train our agent for multiple instances, by sharing an\naggregate interaction buffer. Finally, we evaluate our attack against three\nadvanced LPDG models on three real-world graph datasets of different scales and\ncompare its performance with related methods under the interaction and\nperturbation constraints. Experimental results show that our attack is both\neffective and practicable.\n", "link": "http://arxiv.org/abs/2412.13134v1", "date": "2024-12-17", "relevancy": 1.9867, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5075}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4912}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Practicable%20Black-box%20Evasion%20Attacks%20on%20Link%20Prediction%20in%20Dynamic%0A%20%20Graphs%20--%20A%20Graph%20Sequential%20Embedding%20Method&body=Title%3A%20Practicable%20Black-box%20Evasion%20Attacks%20on%20Link%20Prediction%20in%20Dynamic%0A%20%20Graphs%20--%20A%20Graph%20Sequential%20Embedding%20Method%0AAuthor%3A%20Jiate%20Li%20and%20Meng%20Pang%20and%20Binghui%20Wang%0AAbstract%3A%20%20%20Link%20prediction%20in%20dynamic%20graphs%20%28LPDG%29%20has%20been%20widely%20applied%20to%0Areal-world%20applications%20such%20as%20website%20recommendation%2C%20traffic%20flow%0Aprediction%2C%20organizational%20studies%2C%20etc.%20These%20models%20are%20usually%20kept%20local%0Aand%20secure%2C%20with%20only%20the%20interactive%20interface%20restrictively%20available%20to%20the%0Apublic.%20Thus%2C%20the%20problem%20of%20the%20black-box%20evasion%20attack%20on%20the%20LPDG%20model%2C%0Awhere%20model%20interactions%20and%20data%20perturbations%20are%20restricted%2C%20seems%20to%20be%0Aessential%20and%20meaningful%20in%20practice.%20In%20this%20paper%2C%20we%20propose%20the%20first%0Apracticable%20black-box%20evasion%20attack%20method%20that%20achieves%20effective%20attacks%0Aagainst%20the%20target%20LPDG%20model%2C%20within%20a%20limited%20amount%20of%20interactions%20and%0Aperturbations.%20To%20perform%20effective%20attacks%20under%20limited%20perturbations%2C%20we%0Adevelop%20a%20graph%20sequential%20embedding%20model%20to%20find%20the%20desired%20state%20embedding%0Aof%20the%20dynamic%20graph%20sequences%2C%20under%20a%20deep%20reinforcement%20learning%20framework.%0ATo%20overcome%20the%20scarcity%20of%20interactions%2C%20we%20design%20a%20multi-environment%0Atraining%20pipeline%20and%20train%20our%20agent%20for%20multiple%20instances%2C%20by%20sharing%20an%0Aaggregate%20interaction%20buffer.%20Finally%2C%20we%20evaluate%20our%20attack%20against%20three%0Aadvanced%20LPDG%20models%20on%20three%20real-world%20graph%20datasets%20of%20different%20scales%20and%0Acompare%20its%20performance%20with%20related%20methods%20under%20the%20interaction%20and%0Aperturbation%20constraints.%20Experimental%20results%20show%20that%20our%20attack%20is%20both%0Aeffective%20and%20practicable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPracticable%2520Black-box%2520Evasion%2520Attacks%2520on%2520Link%2520Prediction%2520in%2520Dynamic%250A%2520%2520Graphs%2520--%2520A%2520Graph%2520Sequential%2520Embedding%2520Method%26entry.906535625%3DJiate%2520Li%2520and%2520Meng%2520Pang%2520and%2520Binghui%2520Wang%26entry.1292438233%3D%2520%2520Link%2520prediction%2520in%2520dynamic%2520graphs%2520%2528LPDG%2529%2520has%2520been%2520widely%2520applied%2520to%250Areal-world%2520applications%2520such%2520as%2520website%2520recommendation%252C%2520traffic%2520flow%250Aprediction%252C%2520organizational%2520studies%252C%2520etc.%2520These%2520models%2520are%2520usually%2520kept%2520local%250Aand%2520secure%252C%2520with%2520only%2520the%2520interactive%2520interface%2520restrictively%2520available%2520to%2520the%250Apublic.%2520Thus%252C%2520the%2520problem%2520of%2520the%2520black-box%2520evasion%2520attack%2520on%2520the%2520LPDG%2520model%252C%250Awhere%2520model%2520interactions%2520and%2520data%2520perturbations%2520are%2520restricted%252C%2520seems%2520to%2520be%250Aessential%2520and%2520meaningful%2520in%2520practice.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%250Apracticable%2520black-box%2520evasion%2520attack%2520method%2520that%2520achieves%2520effective%2520attacks%250Aagainst%2520the%2520target%2520LPDG%2520model%252C%2520within%2520a%2520limited%2520amount%2520of%2520interactions%2520and%250Aperturbations.%2520To%2520perform%2520effective%2520attacks%2520under%2520limited%2520perturbations%252C%2520we%250Adevelop%2520a%2520graph%2520sequential%2520embedding%2520model%2520to%2520find%2520the%2520desired%2520state%2520embedding%250Aof%2520the%2520dynamic%2520graph%2520sequences%252C%2520under%2520a%2520deep%2520reinforcement%2520learning%2520framework.%250ATo%2520overcome%2520the%2520scarcity%2520of%2520interactions%252C%2520we%2520design%2520a%2520multi-environment%250Atraining%2520pipeline%2520and%2520train%2520our%2520agent%2520for%2520multiple%2520instances%252C%2520by%2520sharing%2520an%250Aaggregate%2520interaction%2520buffer.%2520Finally%252C%2520we%2520evaluate%2520our%2520attack%2520against%2520three%250Aadvanced%2520LPDG%2520models%2520on%2520three%2520real-world%2520graph%2520datasets%2520of%2520different%2520scales%2520and%250Acompare%2520its%2520performance%2520with%2520related%2520methods%2520under%2520the%2520interaction%2520and%250Aperturbation%2520constraints.%2520Experimental%2520results%2520show%2520that%2520our%2520attack%2520is%2520both%250Aeffective%2520and%2520practicable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Practicable%20Black-box%20Evasion%20Attacks%20on%20Link%20Prediction%20in%20Dynamic%0A%20%20Graphs%20--%20A%20Graph%20Sequential%20Embedding%20Method&entry.906535625=Jiate%20Li%20and%20Meng%20Pang%20and%20Binghui%20Wang&entry.1292438233=%20%20Link%20prediction%20in%20dynamic%20graphs%20%28LPDG%29%20has%20been%20widely%20applied%20to%0Areal-world%20applications%20such%20as%20website%20recommendation%2C%20traffic%20flow%0Aprediction%2C%20organizational%20studies%2C%20etc.%20These%20models%20are%20usually%20kept%20local%0Aand%20secure%2C%20with%20only%20the%20interactive%20interface%20restrictively%20available%20to%20the%0Apublic.%20Thus%2C%20the%20problem%20of%20the%20black-box%20evasion%20attack%20on%20the%20LPDG%20model%2C%0Awhere%20model%20interactions%20and%20data%20perturbations%20are%20restricted%2C%20seems%20to%20be%0Aessential%20and%20meaningful%20in%20practice.%20In%20this%20paper%2C%20we%20propose%20the%20first%0Apracticable%20black-box%20evasion%20attack%20method%20that%20achieves%20effective%20attacks%0Aagainst%20the%20target%20LPDG%20model%2C%20within%20a%20limited%20amount%20of%20interactions%20and%0Aperturbations.%20To%20perform%20effective%20attacks%20under%20limited%20perturbations%2C%20we%0Adevelop%20a%20graph%20sequential%20embedding%20model%20to%20find%20the%20desired%20state%20embedding%0Aof%20the%20dynamic%20graph%20sequences%2C%20under%20a%20deep%20reinforcement%20learning%20framework.%0ATo%20overcome%20the%20scarcity%20of%20interactions%2C%20we%20design%20a%20multi-environment%0Atraining%20pipeline%20and%20train%20our%20agent%20for%20multiple%20instances%2C%20by%20sharing%20an%0Aaggregate%20interaction%20buffer.%20Finally%2C%20we%20evaluate%20our%20attack%20against%20three%0Aadvanced%20LPDG%20models%20on%20three%20real-world%20graph%20datasets%20of%20different%20scales%20and%0Acompare%20its%20performance%20with%20related%20methods%20under%20the%20interaction%20and%0Aperturbation%20constraints.%20Experimental%20results%20show%20that%20our%20attack%20is%20both%0Aeffective%20and%20practicable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13134v1&entry.124074799=Read"},
{"title": "Continuous Patient Monitoring with AI: Real-Time Analysis of Video in\n  Hospital Care Settings", "author": "Paolo Gabriel and Peter Rehani and Tyler Troy and Tiffany Wyatt and Michael Choma and Narinder Singh", "abstract": "  This study introduces an AI-driven platform for continuous and passive\npatient monitoring in hospital settings, developed by LookDeep Health.\nLeveraging advanced computer vision, the platform provides real-time insights\ninto patient behavior and interactions through video analysis, securely storing\ninference results in the cloud for retrospective evaluation. The dataset,\ncompiled in collaboration with 11 hospital partners, encompasses over 300\nhigh-risk fall patients and over 1,000 days of inference, enabling applications\nsuch as fall detection and safety monitoring for vulnerable patient\npopulations. To foster innovation and reproducibility, an anonymized subset of\nthis dataset is publicly available. The AI system detects key components in\nhospital rooms, including individual presence and role, furniture location,\nmotion magnitude, and boundary crossings. Performance evaluation demonstrates\nstrong accuracy in object detection (macro F1-score = 0.92) and patient-role\nclassification (F1-score = 0.98), as well as reliable trend analysis for the\n\"patient alone\" metric (mean logistic regression accuracy = 0.82 \\pm 0.15).\nThese capabilities enable automated detection of patient isolation, wandering,\nor unsupervised movement-key indicators for fall risk and other adverse events.\nThis work establishes benchmarks for validating AI-driven patient monitoring\nsystems, highlighting the platform's potential to enhance patient safety and\ncare by providing continuous, data-driven insights into patient behavior and\ninteractions.\n", "link": "http://arxiv.org/abs/2412.13152v1", "date": "2024-12-17", "relevancy": 1.977, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4896}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Patient%20Monitoring%20with%20AI%3A%20Real-Time%20Analysis%20of%20Video%20in%0A%20%20Hospital%20Care%20Settings&body=Title%3A%20Continuous%20Patient%20Monitoring%20with%20AI%3A%20Real-Time%20Analysis%20of%20Video%20in%0A%20%20Hospital%20Care%20Settings%0AAuthor%3A%20Paolo%20Gabriel%20and%20Peter%20Rehani%20and%20Tyler%20Troy%20and%20Tiffany%20Wyatt%20and%20Michael%20Choma%20and%20Narinder%20Singh%0AAbstract%3A%20%20%20This%20study%20introduces%20an%20AI-driven%20platform%20for%20continuous%20and%20passive%0Apatient%20monitoring%20in%20hospital%20settings%2C%20developed%20by%20LookDeep%20Health.%0ALeveraging%20advanced%20computer%20vision%2C%20the%20platform%20provides%20real-time%20insights%0Ainto%20patient%20behavior%20and%20interactions%20through%20video%20analysis%2C%20securely%20storing%0Ainference%20results%20in%20the%20cloud%20for%20retrospective%20evaluation.%20The%20dataset%2C%0Acompiled%20in%20collaboration%20with%2011%20hospital%20partners%2C%20encompasses%20over%20300%0Ahigh-risk%20fall%20patients%20and%20over%201%2C000%20days%20of%20inference%2C%20enabling%20applications%0Asuch%20as%20fall%20detection%20and%20safety%20monitoring%20for%20vulnerable%20patient%0Apopulations.%20To%20foster%20innovation%20and%20reproducibility%2C%20an%20anonymized%20subset%20of%0Athis%20dataset%20is%20publicly%20available.%20The%20AI%20system%20detects%20key%20components%20in%0Ahospital%20rooms%2C%20including%20individual%20presence%20and%20role%2C%20furniture%20location%2C%0Amotion%20magnitude%2C%20and%20boundary%20crossings.%20Performance%20evaluation%20demonstrates%0Astrong%20accuracy%20in%20object%20detection%20%28macro%20F1-score%20%3D%200.92%29%20and%20patient-role%0Aclassification%20%28F1-score%20%3D%200.98%29%2C%20as%20well%20as%20reliable%20trend%20analysis%20for%20the%0A%22patient%20alone%22%20metric%20%28mean%20logistic%20regression%20accuracy%20%3D%200.82%20%5Cpm%200.15%29.%0AThese%20capabilities%20enable%20automated%20detection%20of%20patient%20isolation%2C%20wandering%2C%0Aor%20unsupervised%20movement-key%20indicators%20for%20fall%20risk%20and%20other%20adverse%20events.%0AThis%20work%20establishes%20benchmarks%20for%20validating%20AI-driven%20patient%20monitoring%0Asystems%2C%20highlighting%20the%20platform%27s%20potential%20to%20enhance%20patient%20safety%20and%0Acare%20by%20providing%20continuous%2C%20data-driven%20insights%20into%20patient%20behavior%20and%0Ainteractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Patient%2520Monitoring%2520with%2520AI%253A%2520Real-Time%2520Analysis%2520of%2520Video%2520in%250A%2520%2520Hospital%2520Care%2520Settings%26entry.906535625%3DPaolo%2520Gabriel%2520and%2520Peter%2520Rehani%2520and%2520Tyler%2520Troy%2520and%2520Tiffany%2520Wyatt%2520and%2520Michael%2520Choma%2520and%2520Narinder%2520Singh%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520an%2520AI-driven%2520platform%2520for%2520continuous%2520and%2520passive%250Apatient%2520monitoring%2520in%2520hospital%2520settings%252C%2520developed%2520by%2520LookDeep%2520Health.%250ALeveraging%2520advanced%2520computer%2520vision%252C%2520the%2520platform%2520provides%2520real-time%2520insights%250Ainto%2520patient%2520behavior%2520and%2520interactions%2520through%2520video%2520analysis%252C%2520securely%2520storing%250Ainference%2520results%2520in%2520the%2520cloud%2520for%2520retrospective%2520evaluation.%2520The%2520dataset%252C%250Acompiled%2520in%2520collaboration%2520with%252011%2520hospital%2520partners%252C%2520encompasses%2520over%2520300%250Ahigh-risk%2520fall%2520patients%2520and%2520over%25201%252C000%2520days%2520of%2520inference%252C%2520enabling%2520applications%250Asuch%2520as%2520fall%2520detection%2520and%2520safety%2520monitoring%2520for%2520vulnerable%2520patient%250Apopulations.%2520To%2520foster%2520innovation%2520and%2520reproducibility%252C%2520an%2520anonymized%2520subset%2520of%250Athis%2520dataset%2520is%2520publicly%2520available.%2520The%2520AI%2520system%2520detects%2520key%2520components%2520in%250Ahospital%2520rooms%252C%2520including%2520individual%2520presence%2520and%2520role%252C%2520furniture%2520location%252C%250Amotion%2520magnitude%252C%2520and%2520boundary%2520crossings.%2520Performance%2520evaluation%2520demonstrates%250Astrong%2520accuracy%2520in%2520object%2520detection%2520%2528macro%2520F1-score%2520%253D%25200.92%2529%2520and%2520patient-role%250Aclassification%2520%2528F1-score%2520%253D%25200.98%2529%252C%2520as%2520well%2520as%2520reliable%2520trend%2520analysis%2520for%2520the%250A%2522patient%2520alone%2522%2520metric%2520%2528mean%2520logistic%2520regression%2520accuracy%2520%253D%25200.82%2520%255Cpm%25200.15%2529.%250AThese%2520capabilities%2520enable%2520automated%2520detection%2520of%2520patient%2520isolation%252C%2520wandering%252C%250Aor%2520unsupervised%2520movement-key%2520indicators%2520for%2520fall%2520risk%2520and%2520other%2520adverse%2520events.%250AThis%2520work%2520establishes%2520benchmarks%2520for%2520validating%2520AI-driven%2520patient%2520monitoring%250Asystems%252C%2520highlighting%2520the%2520platform%2527s%2520potential%2520to%2520enhance%2520patient%2520safety%2520and%250Acare%2520by%2520providing%2520continuous%252C%2520data-driven%2520insights%2520into%2520patient%2520behavior%2520and%250Ainteractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Patient%20Monitoring%20with%20AI%3A%20Real-Time%20Analysis%20of%20Video%20in%0A%20%20Hospital%20Care%20Settings&entry.906535625=Paolo%20Gabriel%20and%20Peter%20Rehani%20and%20Tyler%20Troy%20and%20Tiffany%20Wyatt%20and%20Michael%20Choma%20and%20Narinder%20Singh&entry.1292438233=%20%20This%20study%20introduces%20an%20AI-driven%20platform%20for%20continuous%20and%20passive%0Apatient%20monitoring%20in%20hospital%20settings%2C%20developed%20by%20LookDeep%20Health.%0ALeveraging%20advanced%20computer%20vision%2C%20the%20platform%20provides%20real-time%20insights%0Ainto%20patient%20behavior%20and%20interactions%20through%20video%20analysis%2C%20securely%20storing%0Ainference%20results%20in%20the%20cloud%20for%20retrospective%20evaluation.%20The%20dataset%2C%0Acompiled%20in%20collaboration%20with%2011%20hospital%20partners%2C%20encompasses%20over%20300%0Ahigh-risk%20fall%20patients%20and%20over%201%2C000%20days%20of%20inference%2C%20enabling%20applications%0Asuch%20as%20fall%20detection%20and%20safety%20monitoring%20for%20vulnerable%20patient%0Apopulations.%20To%20foster%20innovation%20and%20reproducibility%2C%20an%20anonymized%20subset%20of%0Athis%20dataset%20is%20publicly%20available.%20The%20AI%20system%20detects%20key%20components%20in%0Ahospital%20rooms%2C%20including%20individual%20presence%20and%20role%2C%20furniture%20location%2C%0Amotion%20magnitude%2C%20and%20boundary%20crossings.%20Performance%20evaluation%20demonstrates%0Astrong%20accuracy%20in%20object%20detection%20%28macro%20F1-score%20%3D%200.92%29%20and%20patient-role%0Aclassification%20%28F1-score%20%3D%200.98%29%2C%20as%20well%20as%20reliable%20trend%20analysis%20for%20the%0A%22patient%20alone%22%20metric%20%28mean%20logistic%20regression%20accuracy%20%3D%200.82%20%5Cpm%200.15%29.%0AThese%20capabilities%20enable%20automated%20detection%20of%20patient%20isolation%2C%20wandering%2C%0Aor%20unsupervised%20movement-key%20indicators%20for%20fall%20risk%20and%20other%20adverse%20events.%0AThis%20work%20establishes%20benchmarks%20for%20validating%20AI-driven%20patient%20monitoring%0Asystems%2C%20highlighting%20the%20platform%27s%20potential%20to%20enhance%20patient%20safety%20and%0Acare%20by%20providing%20continuous%2C%20data-driven%20insights%20into%20patient%20behavior%20and%0Ainteractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13152v1&entry.124074799=Read"},
{"title": "Smartphone-based Iris Recognition through High-Quality Visible Spectrum\n  Iris Capture", "author": "Naveenkumar G Venkataswamy and Yu Liu and Surendra Singh and Soumyabrata Dey and Stephanie Schuckers and Masudul H Imtiaz", "abstract": "  Iris recognition is widely acknowledged for its exceptional accuracy in\nbiometric authentication, traditionally relying on near-infrared (NIR) imaging.\nRecently, visible spectrum (VIS) imaging via accessible smartphone cameras has\nbeen explored for biometric capture. However, a thorough study of iris\nrecognition using smartphone-captured 'High-Quality' VIS images and\ncross-spectral matching with previously enrolled NIR images has not been\nconducted. The primary challenge lies in capturing high-quality biometrics, a\nknown limitation of smartphone cameras. This study introduces a novel Android\napplication designed to consistently capture high-quality VIS iris images\nthrough automated focus and zoom adjustments. The application integrates a\nYOLOv3-tiny model for precise eye and iris detection and a lightweight\nGhost-Attention U-Net (G-ATTU-Net) for segmentation, while adhering to ISO/IEC\n29794-6 standards for image quality. The approach was validated using\nsmartphone-captured VIS and NIR iris images from 47 subjects, achieving a True\nAcceptance Rate (TAR) of 96.57% for VIS images and 97.95% for NIR images, with\nconsistent performance across various capture distances and iris colors. This\nrobust solution is expected to significantly advance the field of iris\nbiometrics, with important implications for enhancing smartphone security.\n", "link": "http://arxiv.org/abs/2412.13063v1", "date": "2024-12-17", "relevancy": 1.9562, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5027}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4909}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smartphone-based%20Iris%20Recognition%20through%20High-Quality%20Visible%20Spectrum%0A%20%20Iris%20Capture&body=Title%3A%20Smartphone-based%20Iris%20Recognition%20through%20High-Quality%20Visible%20Spectrum%0A%20%20Iris%20Capture%0AAuthor%3A%20Naveenkumar%20G%20Venkataswamy%20and%20Yu%20Liu%20and%20Surendra%20Singh%20and%20Soumyabrata%20Dey%20and%20Stephanie%20Schuckers%20and%20Masudul%20H%20Imtiaz%0AAbstract%3A%20%20%20Iris%20recognition%20is%20widely%20acknowledged%20for%20its%20exceptional%20accuracy%20in%0Abiometric%20authentication%2C%20traditionally%20relying%20on%20near-infrared%20%28NIR%29%20imaging.%0ARecently%2C%20visible%20spectrum%20%28VIS%29%20imaging%20via%20accessible%20smartphone%20cameras%20has%0Abeen%20explored%20for%20biometric%20capture.%20However%2C%20a%20thorough%20study%20of%20iris%0Arecognition%20using%20smartphone-captured%20%27High-Quality%27%20VIS%20images%20and%0Across-spectral%20matching%20with%20previously%20enrolled%20NIR%20images%20has%20not%20been%0Aconducted.%20The%20primary%20challenge%20lies%20in%20capturing%20high-quality%20biometrics%2C%20a%0Aknown%20limitation%20of%20smartphone%20cameras.%20This%20study%20introduces%20a%20novel%20Android%0Aapplication%20designed%20to%20consistently%20capture%20high-quality%20VIS%20iris%20images%0Athrough%20automated%20focus%20and%20zoom%20adjustments.%20The%20application%20integrates%20a%0AYOLOv3-tiny%20model%20for%20precise%20eye%20and%20iris%20detection%20and%20a%20lightweight%0AGhost-Attention%20U-Net%20%28G-ATTU-Net%29%20for%20segmentation%2C%20while%20adhering%20to%20ISO/IEC%0A29794-6%20standards%20for%20image%20quality.%20The%20approach%20was%20validated%20using%0Asmartphone-captured%20VIS%20and%20NIR%20iris%20images%20from%2047%20subjects%2C%20achieving%20a%20True%0AAcceptance%20Rate%20%28TAR%29%20of%2096.57%25%20for%20VIS%20images%20and%2097.95%25%20for%20NIR%20images%2C%20with%0Aconsistent%20performance%20across%20various%20capture%20distances%20and%20iris%20colors.%20This%0Arobust%20solution%20is%20expected%20to%20significantly%20advance%20the%20field%20of%20iris%0Abiometrics%2C%20with%20important%20implications%20for%20enhancing%20smartphone%20security.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmartphone-based%2520Iris%2520Recognition%2520through%2520High-Quality%2520Visible%2520Spectrum%250A%2520%2520Iris%2520Capture%26entry.906535625%3DNaveenkumar%2520G%2520Venkataswamy%2520and%2520Yu%2520Liu%2520and%2520Surendra%2520Singh%2520and%2520Soumyabrata%2520Dey%2520and%2520Stephanie%2520Schuckers%2520and%2520Masudul%2520H%2520Imtiaz%26entry.1292438233%3D%2520%2520Iris%2520recognition%2520is%2520widely%2520acknowledged%2520for%2520its%2520exceptional%2520accuracy%2520in%250Abiometric%2520authentication%252C%2520traditionally%2520relying%2520on%2520near-infrared%2520%2528NIR%2529%2520imaging.%250ARecently%252C%2520visible%2520spectrum%2520%2528VIS%2529%2520imaging%2520via%2520accessible%2520smartphone%2520cameras%2520has%250Abeen%2520explored%2520for%2520biometric%2520capture.%2520However%252C%2520a%2520thorough%2520study%2520of%2520iris%250Arecognition%2520using%2520smartphone-captured%2520%2527High-Quality%2527%2520VIS%2520images%2520and%250Across-spectral%2520matching%2520with%2520previously%2520enrolled%2520NIR%2520images%2520has%2520not%2520been%250Aconducted.%2520The%2520primary%2520challenge%2520lies%2520in%2520capturing%2520high-quality%2520biometrics%252C%2520a%250Aknown%2520limitation%2520of%2520smartphone%2520cameras.%2520This%2520study%2520introduces%2520a%2520novel%2520Android%250Aapplication%2520designed%2520to%2520consistently%2520capture%2520high-quality%2520VIS%2520iris%2520images%250Athrough%2520automated%2520focus%2520and%2520zoom%2520adjustments.%2520The%2520application%2520integrates%2520a%250AYOLOv3-tiny%2520model%2520for%2520precise%2520eye%2520and%2520iris%2520detection%2520and%2520a%2520lightweight%250AGhost-Attention%2520U-Net%2520%2528G-ATTU-Net%2529%2520for%2520segmentation%252C%2520while%2520adhering%2520to%2520ISO/IEC%250A29794-6%2520standards%2520for%2520image%2520quality.%2520The%2520approach%2520was%2520validated%2520using%250Asmartphone-captured%2520VIS%2520and%2520NIR%2520iris%2520images%2520from%252047%2520subjects%252C%2520achieving%2520a%2520True%250AAcceptance%2520Rate%2520%2528TAR%2529%2520of%252096.57%2525%2520for%2520VIS%2520images%2520and%252097.95%2525%2520for%2520NIR%2520images%252C%2520with%250Aconsistent%2520performance%2520across%2520various%2520capture%2520distances%2520and%2520iris%2520colors.%2520This%250Arobust%2520solution%2520is%2520expected%2520to%2520significantly%2520advance%2520the%2520field%2520of%2520iris%250Abiometrics%252C%2520with%2520important%2520implications%2520for%2520enhancing%2520smartphone%2520security.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smartphone-based%20Iris%20Recognition%20through%20High-Quality%20Visible%20Spectrum%0A%20%20Iris%20Capture&entry.906535625=Naveenkumar%20G%20Venkataswamy%20and%20Yu%20Liu%20and%20Surendra%20Singh%20and%20Soumyabrata%20Dey%20and%20Stephanie%20Schuckers%20and%20Masudul%20H%20Imtiaz&entry.1292438233=%20%20Iris%20recognition%20is%20widely%20acknowledged%20for%20its%20exceptional%20accuracy%20in%0Abiometric%20authentication%2C%20traditionally%20relying%20on%20near-infrared%20%28NIR%29%20imaging.%0ARecently%2C%20visible%20spectrum%20%28VIS%29%20imaging%20via%20accessible%20smartphone%20cameras%20has%0Abeen%20explored%20for%20biometric%20capture.%20However%2C%20a%20thorough%20study%20of%20iris%0Arecognition%20using%20smartphone-captured%20%27High-Quality%27%20VIS%20images%20and%0Across-spectral%20matching%20with%20previously%20enrolled%20NIR%20images%20has%20not%20been%0Aconducted.%20The%20primary%20challenge%20lies%20in%20capturing%20high-quality%20biometrics%2C%20a%0Aknown%20limitation%20of%20smartphone%20cameras.%20This%20study%20introduces%20a%20novel%20Android%0Aapplication%20designed%20to%20consistently%20capture%20high-quality%20VIS%20iris%20images%0Athrough%20automated%20focus%20and%20zoom%20adjustments.%20The%20application%20integrates%20a%0AYOLOv3-tiny%20model%20for%20precise%20eye%20and%20iris%20detection%20and%20a%20lightweight%0AGhost-Attention%20U-Net%20%28G-ATTU-Net%29%20for%20segmentation%2C%20while%20adhering%20to%20ISO/IEC%0A29794-6%20standards%20for%20image%20quality.%20The%20approach%20was%20validated%20using%0Asmartphone-captured%20VIS%20and%20NIR%20iris%20images%20from%2047%20subjects%2C%20achieving%20a%20True%0AAcceptance%20Rate%20%28TAR%29%20of%2096.57%25%20for%20VIS%20images%20and%2097.95%25%20for%20NIR%20images%2C%20with%0Aconsistent%20performance%20across%20various%20capture%20distances%20and%20iris%20colors.%20This%0Arobust%20solution%20is%20expected%20to%20significantly%20advance%20the%20field%20of%20iris%0Abiometrics%2C%20with%20important%20implications%20for%20enhancing%20smartphone%20security.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13063v1&entry.124074799=Read"},
{"title": "Singularity-Free Guiding Vector Field over B\u00e9zier's Curves Applied to\n  Rovers Path Planning and Path Following", "author": "Alfredo Gonz\u00e1lez-Calvin and L\u00eda Garc\u00eda-P\u00e9rez and Juan Jim\u00e9nez", "abstract": "  This paper presents a guidance algorithm for solving the problem of following\nparametric paths, as well as a curvature-varying speed setpoint for land-based\ncar-type wheeled mobile robots (WMRs). The guidance algorithm relies on\nSingularity-Free Guiding Vector Fields SF-GVF. This novel GVF approach expands\nthe desired robot path and the Guiding vector field to a higher dimensional\nspace, in which an angular control function can be found to ensure global\nasymptotic convergence to the desired parametric path while avoiding field\nsingularities. In SF-GVF, paths should follow a parametric definition. This\nfeature makes using Bezier's curves attractive to define the robot's desired\npatch. The curvature-varying speed setpoint, combined with the guidance\nalgorithm, eases the convergence to the path when physical restrictions exist,\nsuch as minimal turning radius or maximal lateral acceleration. We provide\ntheoretical results, simulations, and outdoor experiments using a WMR platform\nassembled with off-the-shelf components.\n", "link": "http://arxiv.org/abs/2412.13033v1", "date": "2024-12-17", "relevancy": 1.9539, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5377}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5096}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Singularity-Free%20Guiding%20Vector%20Field%20over%20B%C3%A9zier%27s%20Curves%20Applied%20to%0A%20%20Rovers%20Path%20Planning%20and%20Path%20Following&body=Title%3A%20Singularity-Free%20Guiding%20Vector%20Field%20over%20B%C3%A9zier%27s%20Curves%20Applied%20to%0A%20%20Rovers%20Path%20Planning%20and%20Path%20Following%0AAuthor%3A%20Alfredo%20Gonz%C3%A1lez-Calvin%20and%20L%C3%ADa%20Garc%C3%ADa-P%C3%A9rez%20and%20Juan%20Jim%C3%A9nez%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20guidance%20algorithm%20for%20solving%20the%20problem%20of%20following%0Aparametric%20paths%2C%20as%20well%20as%20a%20curvature-varying%20speed%20setpoint%20for%20land-based%0Acar-type%20wheeled%20mobile%20robots%20%28WMRs%29.%20The%20guidance%20algorithm%20relies%20on%0ASingularity-Free%20Guiding%20Vector%20Fields%20SF-GVF.%20This%20novel%20GVF%20approach%20expands%0Athe%20desired%20robot%20path%20and%20the%20Guiding%20vector%20field%20to%20a%20higher%20dimensional%0Aspace%2C%20in%20which%20an%20angular%20control%20function%20can%20be%20found%20to%20ensure%20global%0Aasymptotic%20convergence%20to%20the%20desired%20parametric%20path%20while%20avoiding%20field%0Asingularities.%20In%20SF-GVF%2C%20paths%20should%20follow%20a%20parametric%20definition.%20This%0Afeature%20makes%20using%20Bezier%27s%20curves%20attractive%20to%20define%20the%20robot%27s%20desired%0Apatch.%20The%20curvature-varying%20speed%20setpoint%2C%20combined%20with%20the%20guidance%0Aalgorithm%2C%20eases%20the%20convergence%20to%20the%20path%20when%20physical%20restrictions%20exist%2C%0Asuch%20as%20minimal%20turning%20radius%20or%20maximal%20lateral%20acceleration.%20We%20provide%0Atheoretical%20results%2C%20simulations%2C%20and%20outdoor%20experiments%20using%20a%20WMR%20platform%0Aassembled%20with%20off-the-shelf%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingularity-Free%2520Guiding%2520Vector%2520Field%2520over%2520B%25C3%25A9zier%2527s%2520Curves%2520Applied%2520to%250A%2520%2520Rovers%2520Path%2520Planning%2520and%2520Path%2520Following%26entry.906535625%3DAlfredo%2520Gonz%25C3%25A1lez-Calvin%2520and%2520L%25C3%25ADa%2520Garc%25C3%25ADa-P%25C3%25A9rez%2520and%2520Juan%2520Jim%25C3%25A9nez%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520guidance%2520algorithm%2520for%2520solving%2520the%2520problem%2520of%2520following%250Aparametric%2520paths%252C%2520as%2520well%2520as%2520a%2520curvature-varying%2520speed%2520setpoint%2520for%2520land-based%250Acar-type%2520wheeled%2520mobile%2520robots%2520%2528WMRs%2529.%2520The%2520guidance%2520algorithm%2520relies%2520on%250ASingularity-Free%2520Guiding%2520Vector%2520Fields%2520SF-GVF.%2520This%2520novel%2520GVF%2520approach%2520expands%250Athe%2520desired%2520robot%2520path%2520and%2520the%2520Guiding%2520vector%2520field%2520to%2520a%2520higher%2520dimensional%250Aspace%252C%2520in%2520which%2520an%2520angular%2520control%2520function%2520can%2520be%2520found%2520to%2520ensure%2520global%250Aasymptotic%2520convergence%2520to%2520the%2520desired%2520parametric%2520path%2520while%2520avoiding%2520field%250Asingularities.%2520In%2520SF-GVF%252C%2520paths%2520should%2520follow%2520a%2520parametric%2520definition.%2520This%250Afeature%2520makes%2520using%2520Bezier%2527s%2520curves%2520attractive%2520to%2520define%2520the%2520robot%2527s%2520desired%250Apatch.%2520The%2520curvature-varying%2520speed%2520setpoint%252C%2520combined%2520with%2520the%2520guidance%250Aalgorithm%252C%2520eases%2520the%2520convergence%2520to%2520the%2520path%2520when%2520physical%2520restrictions%2520exist%252C%250Asuch%2520as%2520minimal%2520turning%2520radius%2520or%2520maximal%2520lateral%2520acceleration.%2520We%2520provide%250Atheoretical%2520results%252C%2520simulations%252C%2520and%2520outdoor%2520experiments%2520using%2520a%2520WMR%2520platform%250Aassembled%2520with%2520off-the-shelf%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Singularity-Free%20Guiding%20Vector%20Field%20over%20B%C3%A9zier%27s%20Curves%20Applied%20to%0A%20%20Rovers%20Path%20Planning%20and%20Path%20Following&entry.906535625=Alfredo%20Gonz%C3%A1lez-Calvin%20and%20L%C3%ADa%20Garc%C3%ADa-P%C3%A9rez%20and%20Juan%20Jim%C3%A9nez&entry.1292438233=%20%20This%20paper%20presents%20a%20guidance%20algorithm%20for%20solving%20the%20problem%20of%20following%0Aparametric%20paths%2C%20as%20well%20as%20a%20curvature-varying%20speed%20setpoint%20for%20land-based%0Acar-type%20wheeled%20mobile%20robots%20%28WMRs%29.%20The%20guidance%20algorithm%20relies%20on%0ASingularity-Free%20Guiding%20Vector%20Fields%20SF-GVF.%20This%20novel%20GVF%20approach%20expands%0Athe%20desired%20robot%20path%20and%20the%20Guiding%20vector%20field%20to%20a%20higher%20dimensional%0Aspace%2C%20in%20which%20an%20angular%20control%20function%20can%20be%20found%20to%20ensure%20global%0Aasymptotic%20convergence%20to%20the%20desired%20parametric%20path%20while%20avoiding%20field%0Asingularities.%20In%20SF-GVF%2C%20paths%20should%20follow%20a%20parametric%20definition.%20This%0Afeature%20makes%20using%20Bezier%27s%20curves%20attractive%20to%20define%20the%20robot%27s%20desired%0Apatch.%20The%20curvature-varying%20speed%20setpoint%2C%20combined%20with%20the%20guidance%0Aalgorithm%2C%20eases%20the%20convergence%20to%20the%20path%20when%20physical%20restrictions%20exist%2C%0Asuch%20as%20minimal%20turning%20radius%20or%20maximal%20lateral%20acceleration.%20We%20provide%0Atheoretical%20results%2C%20simulations%2C%20and%20outdoor%20experiments%20using%20a%20WMR%20platform%0Aassembled%20with%20off-the-shelf%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13033v1&entry.124074799=Read"},
{"title": "Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models", "author": "Anmol Mekala and Vineeth Dorna and Shreya Dubey and Abhishek Lalwani and David Koleczek and Mukund Rungta and Sadid Hasan and Elita Lobo", "abstract": "  Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/Alternate-Preference-Optimization.\n", "link": "http://arxiv.org/abs/2409.13474v3", "date": "2024-12-17", "relevancy": 1.953, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5234}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alternate%20Preference%20Optimization%20for%20Unlearning%20Factual%20Knowledge%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Alternate%20Preference%20Optimization%20for%20Unlearning%20Factual%20Knowledge%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Anmol%20Mekala%20and%20Vineeth%20Dorna%20and%20Shreya%20Dubey%20and%20Abhishek%20Lalwani%20and%20David%20Koleczek%20and%20Mukund%20Rungta%20and%20Sadid%20Hasan%20and%20Elita%20Lobo%0AAbstract%3A%20%20%20Machine%20unlearning%20aims%20to%20efficiently%20eliminate%20the%20influence%20of%20specific%0Atraining%20data%2C%20known%20as%20the%20forget%20set%2C%20from%20the%20model.%20However%2C%20existing%0Aunlearning%20methods%20for%20Large%20Language%20Models%20%28LLMs%29%20face%20a%20critical%20challenge%3A%0Athey%20rely%20solely%20on%20negative%20feedback%20to%20suppress%20responses%20related%20to%20the%0Aforget%20set%2C%20which%20often%20results%20in%20nonsensical%20or%20inconsistent%20outputs%2C%0Adiminishing%20model%20utility%20and%20posing%20potential%20privacy%20risks.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20approach%20called%20Alternate%20Preference%0AOptimization%20%28AltPO%29%2C%20which%20combines%20negative%20feedback%20with%20in-domain%20positive%0Afeedback%20on%20the%20forget%20set.%20Additionally%2C%20we%20introduce%20new%20evaluation%20metrics%0Ato%20assess%20the%20quality%20of%20responses%20related%20to%20the%20forget%20set.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20not%20only%20enables%20effective%20unlearning%20but%0Aalso%20avoids%20undesirable%20model%20behaviors%20while%20maintaining%20overall%20model%0Aperformance.%20Our%20implementation%20can%20be%20found%20at%0Ahttps%3A//github.com/molereddy/Alternate-Preference-Optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13474v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlternate%2520Preference%2520Optimization%2520for%2520Unlearning%2520Factual%2520Knowledge%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DAnmol%2520Mekala%2520and%2520Vineeth%2520Dorna%2520and%2520Shreya%2520Dubey%2520and%2520Abhishek%2520Lalwani%2520and%2520David%2520Koleczek%2520and%2520Mukund%2520Rungta%2520and%2520Sadid%2520Hasan%2520and%2520Elita%2520Lobo%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520aims%2520to%2520efficiently%2520eliminate%2520the%2520influence%2520of%2520specific%250Atraining%2520data%252C%2520known%2520as%2520the%2520forget%2520set%252C%2520from%2520the%2520model.%2520However%252C%2520existing%250Aunlearning%2520methods%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520a%2520critical%2520challenge%253A%250Athey%2520rely%2520solely%2520on%2520negative%2520feedback%2520to%2520suppress%2520responses%2520related%2520to%2520the%250Aforget%2520set%252C%2520which%2520often%2520results%2520in%2520nonsensical%2520or%2520inconsistent%2520outputs%252C%250Adiminishing%2520model%2520utility%2520and%2520posing%2520potential%2520privacy%2520risks.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520novel%2520approach%2520called%2520Alternate%2520Preference%250AOptimization%2520%2528AltPO%2529%252C%2520which%2520combines%2520negative%2520feedback%2520with%2520in-domain%2520positive%250Afeedback%2520on%2520the%2520forget%2520set.%2520Additionally%252C%2520we%2520introduce%2520new%2520evaluation%2520metrics%250Ato%2520assess%2520the%2520quality%2520of%2520responses%2520related%2520to%2520the%2520forget%2520set.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520approach%2520not%2520only%2520enables%2520effective%2520unlearning%2520but%250Aalso%2520avoids%2520undesirable%2520model%2520behaviors%2520while%2520maintaining%2520overall%2520model%250Aperformance.%2520Our%2520implementation%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/molereddy/Alternate-Preference-Optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13474v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alternate%20Preference%20Optimization%20for%20Unlearning%20Factual%20Knowledge%20in%0A%20%20Large%20Language%20Models&entry.906535625=Anmol%20Mekala%20and%20Vineeth%20Dorna%20and%20Shreya%20Dubey%20and%20Abhishek%20Lalwani%20and%20David%20Koleczek%20and%20Mukund%20Rungta%20and%20Sadid%20Hasan%20and%20Elita%20Lobo&entry.1292438233=%20%20Machine%20unlearning%20aims%20to%20efficiently%20eliminate%20the%20influence%20of%20specific%0Atraining%20data%2C%20known%20as%20the%20forget%20set%2C%20from%20the%20model.%20However%2C%20existing%0Aunlearning%20methods%20for%20Large%20Language%20Models%20%28LLMs%29%20face%20a%20critical%20challenge%3A%0Athey%20rely%20solely%20on%20negative%20feedback%20to%20suppress%20responses%20related%20to%20the%0Aforget%20set%2C%20which%20often%20results%20in%20nonsensical%20or%20inconsistent%20outputs%2C%0Adiminishing%20model%20utility%20and%20posing%20potential%20privacy%20risks.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20approach%20called%20Alternate%20Preference%0AOptimization%20%28AltPO%29%2C%20which%20combines%20negative%20feedback%20with%20in-domain%20positive%0Afeedback%20on%20the%20forget%20set.%20Additionally%2C%20we%20introduce%20new%20evaluation%20metrics%0Ato%20assess%20the%20quality%20of%20responses%20related%20to%20the%20forget%20set.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20not%20only%20enables%20effective%20unlearning%20but%0Aalso%20avoids%20undesirable%20model%20behaviors%20while%20maintaining%20overall%20model%0Aperformance.%20Our%20implementation%20can%20be%20found%20at%0Ahttps%3A//github.com/molereddy/Alternate-Preference-Optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13474v3&entry.124074799=Read"},
{"title": "LMUnit: Fine-grained Evaluation with Natural Language Unit Tests", "author": "Jon Saad-Falcon and Rajan Vivek and William Berrios and Nandita Shankar Naik and Matija Franklin and Bertie Vidgen and Amanpreet Singh and Douwe Kiela and Shikib Mehri", "abstract": "  As language models become integral to critical workflows, assessing their\nbehavior remains a fundamental challenge -- human evaluation is costly and\nnoisy, while automated metrics provide only coarse, difficult-to-interpret\nsignals. We introduce natural language unit tests, a paradigm that decomposes\nresponse quality into explicit, testable criteria, along with a unified scoring\nmodel, LMUnit, which combines multi-objective training across preferences,\ndirect ratings, and natural language rationales. Through controlled human\nstudies, we show this paradigm significantly improves inter-annotator agreement\nand enables more effective LLM development workflows. LMUnit achieves\nstate-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and\ncompetitive results on RewardBench. These results validate both our proposed\nparadigm and scoring model, suggesting a promising path forward for language\nmodel evaluation and development.\n", "link": "http://arxiv.org/abs/2412.13091v1", "date": "2024-12-17", "relevancy": 1.947, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMUnit%3A%20Fine-grained%20Evaluation%20with%20Natural%20Language%20Unit%20Tests&body=Title%3A%20LMUnit%3A%20Fine-grained%20Evaluation%20with%20Natural%20Language%20Unit%20Tests%0AAuthor%3A%20Jon%20Saad-Falcon%20and%20Rajan%20Vivek%20and%20William%20Berrios%20and%20Nandita%20Shankar%20Naik%20and%20Matija%20Franklin%20and%20Bertie%20Vidgen%20and%20Amanpreet%20Singh%20and%20Douwe%20Kiela%20and%20Shikib%20Mehri%0AAbstract%3A%20%20%20As%20language%20models%20become%20integral%20to%20critical%20workflows%2C%20assessing%20their%0Abehavior%20remains%20a%20fundamental%20challenge%20--%20human%20evaluation%20is%20costly%20and%0Anoisy%2C%20while%20automated%20metrics%20provide%20only%20coarse%2C%20difficult-to-interpret%0Asignals.%20We%20introduce%20natural%20language%20unit%20tests%2C%20a%20paradigm%20that%20decomposes%0Aresponse%20quality%20into%20explicit%2C%20testable%20criteria%2C%20along%20with%20a%20unified%20scoring%0Amodel%2C%20LMUnit%2C%20which%20combines%20multi-objective%20training%20across%20preferences%2C%0Adirect%20ratings%2C%20and%20natural%20language%20rationales.%20Through%20controlled%20human%0Astudies%2C%20we%20show%20this%20paradigm%20significantly%20improves%20inter-annotator%20agreement%0Aand%20enables%20more%20effective%20LLM%20development%20workflows.%20LMUnit%20achieves%0Astate-of-the-art%20performance%20on%20evaluation%20benchmarks%20%28FLASK%2C%20BigGenBench%29%20and%0Acompetitive%20results%20on%20RewardBench.%20These%20results%20validate%20both%20our%20proposed%0Aparadigm%20and%20scoring%20model%2C%20suggesting%20a%20promising%20path%20forward%20for%20language%0Amodel%20evaluation%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMUnit%253A%2520Fine-grained%2520Evaluation%2520with%2520Natural%2520Language%2520Unit%2520Tests%26entry.906535625%3DJon%2520Saad-Falcon%2520and%2520Rajan%2520Vivek%2520and%2520William%2520Berrios%2520and%2520Nandita%2520Shankar%2520Naik%2520and%2520Matija%2520Franklin%2520and%2520Bertie%2520Vidgen%2520and%2520Amanpreet%2520Singh%2520and%2520Douwe%2520Kiela%2520and%2520Shikib%2520Mehri%26entry.1292438233%3D%2520%2520As%2520language%2520models%2520become%2520integral%2520to%2520critical%2520workflows%252C%2520assessing%2520their%250Abehavior%2520remains%2520a%2520fundamental%2520challenge%2520--%2520human%2520evaluation%2520is%2520costly%2520and%250Anoisy%252C%2520while%2520automated%2520metrics%2520provide%2520only%2520coarse%252C%2520difficult-to-interpret%250Asignals.%2520We%2520introduce%2520natural%2520language%2520unit%2520tests%252C%2520a%2520paradigm%2520that%2520decomposes%250Aresponse%2520quality%2520into%2520explicit%252C%2520testable%2520criteria%252C%2520along%2520with%2520a%2520unified%2520scoring%250Amodel%252C%2520LMUnit%252C%2520which%2520combines%2520multi-objective%2520training%2520across%2520preferences%252C%250Adirect%2520ratings%252C%2520and%2520natural%2520language%2520rationales.%2520Through%2520controlled%2520human%250Astudies%252C%2520we%2520show%2520this%2520paradigm%2520significantly%2520improves%2520inter-annotator%2520agreement%250Aand%2520enables%2520more%2520effective%2520LLM%2520development%2520workflows.%2520LMUnit%2520achieves%250Astate-of-the-art%2520performance%2520on%2520evaluation%2520benchmarks%2520%2528FLASK%252C%2520BigGenBench%2529%2520and%250Acompetitive%2520results%2520on%2520RewardBench.%2520These%2520results%2520validate%2520both%2520our%2520proposed%250Aparadigm%2520and%2520scoring%2520model%252C%2520suggesting%2520a%2520promising%2520path%2520forward%2520for%2520language%250Amodel%2520evaluation%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMUnit%3A%20Fine-grained%20Evaluation%20with%20Natural%20Language%20Unit%20Tests&entry.906535625=Jon%20Saad-Falcon%20and%20Rajan%20Vivek%20and%20William%20Berrios%20and%20Nandita%20Shankar%20Naik%20and%20Matija%20Franklin%20and%20Bertie%20Vidgen%20and%20Amanpreet%20Singh%20and%20Douwe%20Kiela%20and%20Shikib%20Mehri&entry.1292438233=%20%20As%20language%20models%20become%20integral%20to%20critical%20workflows%2C%20assessing%20their%0Abehavior%20remains%20a%20fundamental%20challenge%20--%20human%20evaluation%20is%20costly%20and%0Anoisy%2C%20while%20automated%20metrics%20provide%20only%20coarse%2C%20difficult-to-interpret%0Asignals.%20We%20introduce%20natural%20language%20unit%20tests%2C%20a%20paradigm%20that%20decomposes%0Aresponse%20quality%20into%20explicit%2C%20testable%20criteria%2C%20along%20with%20a%20unified%20scoring%0Amodel%2C%20LMUnit%2C%20which%20combines%20multi-objective%20training%20across%20preferences%2C%0Adirect%20ratings%2C%20and%20natural%20language%20rationales.%20Through%20controlled%20human%0Astudies%2C%20we%20show%20this%20paradigm%20significantly%20improves%20inter-annotator%20agreement%0Aand%20enables%20more%20effective%20LLM%20development%20workflows.%20LMUnit%20achieves%0Astate-of-the-art%20performance%20on%20evaluation%20benchmarks%20%28FLASK%2C%20BigGenBench%29%20and%0Acompetitive%20results%20on%20RewardBench.%20These%20results%20validate%20both%20our%20proposed%0Aparadigm%20and%20scoring%20model%2C%20suggesting%20a%20promising%20path%20forward%20for%20language%0Amodel%20evaluation%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13091v1&entry.124074799=Read"},
{"title": "Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic", "author": "Thomy Phan and Benran Zhang and Shao-Hung Chan and Sven Koenig", "abstract": "  Anytime multi-agent path finding (MAPF) is a promising approach to scalable\npath optimization in multi-agent systems. MAPF-LNS, based on Large Neighborhood\nSearch (LNS), is the current state-of-the-art approach where a fast initial\nsolution is iteratively optimized by destroying and repairing selected paths of\nthe solution. Current MAPF-LNS variants commonly use an adaptive selection\nmechanism to choose among multiple destroy heuristics. However, to determine\npromising destroy heuristics, MAPF-LNS requires a considerable amount of\nexploration time. As common destroy heuristics are non-adaptive, any\nperformance bottleneck caused by these heuristics cannot be overcome via\nadaptive heuristic selection alone, thus limiting the overall effectiveness of\nMAPF-LNS in terms of solution cost. In this paper, we propose Adaptive\nDelay-based Destroy-and-Repair Enhanced with Success-based Self-Learning\n(ADDRESS) as a single-destroy-heuristic variant of MAPF-LNS. ADDRESS applies\nrestricted Thompson Sampling to the top-K set of the most delayed agents to\nselect a seed agent for adaptive LNS neighborhood generation. We evaluate\nADDRESS in multiple maps from the MAPF benchmark set and demonstrate cost\nimprovements by at least 50% in large-scale scenarios with up to a thousand\nagents, compared with the original MAPF-LNS and other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2408.02960v2", "date": "2024-12-17", "relevancy": 1.9121, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4891}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4712}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anytime%20Multi-Agent%20Path%20Finding%20with%20an%20Adaptive%20Delay-Based%20Heuristic&body=Title%3A%20Anytime%20Multi-Agent%20Path%20Finding%20with%20an%20Adaptive%20Delay-Based%20Heuristic%0AAuthor%3A%20Thomy%20Phan%20and%20Benran%20Zhang%20and%20Shao-Hung%20Chan%20and%20Sven%20Koenig%0AAbstract%3A%20%20%20Anytime%20multi-agent%20path%20finding%20%28MAPF%29%20is%20a%20promising%20approach%20to%20scalable%0Apath%20optimization%20in%20multi-agent%20systems.%20MAPF-LNS%2C%20based%20on%20Large%20Neighborhood%0ASearch%20%28LNS%29%2C%20is%20the%20current%20state-of-the-art%20approach%20where%20a%20fast%20initial%0Asolution%20is%20iteratively%20optimized%20by%20destroying%20and%20repairing%20selected%20paths%20of%0Athe%20solution.%20Current%20MAPF-LNS%20variants%20commonly%20use%20an%20adaptive%20selection%0Amechanism%20to%20choose%20among%20multiple%20destroy%20heuristics.%20However%2C%20to%20determine%0Apromising%20destroy%20heuristics%2C%20MAPF-LNS%20requires%20a%20considerable%20amount%20of%0Aexploration%20time.%20As%20common%20destroy%20heuristics%20are%20non-adaptive%2C%20any%0Aperformance%20bottleneck%20caused%20by%20these%20heuristics%20cannot%20be%20overcome%20via%0Aadaptive%20heuristic%20selection%20alone%2C%20thus%20limiting%20the%20overall%20effectiveness%20of%0AMAPF-LNS%20in%20terms%20of%20solution%20cost.%20In%20this%20paper%2C%20we%20propose%20Adaptive%0ADelay-based%20Destroy-and-Repair%20Enhanced%20with%20Success-based%20Self-Learning%0A%28ADDRESS%29%20as%20a%20single-destroy-heuristic%20variant%20of%20MAPF-LNS.%20ADDRESS%20applies%0Arestricted%20Thompson%20Sampling%20to%20the%20top-K%20set%20of%20the%20most%20delayed%20agents%20to%0Aselect%20a%20seed%20agent%20for%20adaptive%20LNS%20neighborhood%20generation.%20We%20evaluate%0AADDRESS%20in%20multiple%20maps%20from%20the%20MAPF%20benchmark%20set%20and%20demonstrate%20cost%0Aimprovements%20by%20at%20least%2050%25%20in%20large-scale%20scenarios%20with%20up%20to%20a%20thousand%0Aagents%2C%20compared%20with%20the%20original%20MAPF-LNS%20and%20other%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnytime%2520Multi-Agent%2520Path%2520Finding%2520with%2520an%2520Adaptive%2520Delay-Based%2520Heuristic%26entry.906535625%3DThomy%2520Phan%2520and%2520Benran%2520Zhang%2520and%2520Shao-Hung%2520Chan%2520and%2520Sven%2520Koenig%26entry.1292438233%3D%2520%2520Anytime%2520multi-agent%2520path%2520finding%2520%2528MAPF%2529%2520is%2520a%2520promising%2520approach%2520to%2520scalable%250Apath%2520optimization%2520in%2520multi-agent%2520systems.%2520MAPF-LNS%252C%2520based%2520on%2520Large%2520Neighborhood%250ASearch%2520%2528LNS%2529%252C%2520is%2520the%2520current%2520state-of-the-art%2520approach%2520where%2520a%2520fast%2520initial%250Asolution%2520is%2520iteratively%2520optimized%2520by%2520destroying%2520and%2520repairing%2520selected%2520paths%2520of%250Athe%2520solution.%2520Current%2520MAPF-LNS%2520variants%2520commonly%2520use%2520an%2520adaptive%2520selection%250Amechanism%2520to%2520choose%2520among%2520multiple%2520destroy%2520heuristics.%2520However%252C%2520to%2520determine%250Apromising%2520destroy%2520heuristics%252C%2520MAPF-LNS%2520requires%2520a%2520considerable%2520amount%2520of%250Aexploration%2520time.%2520As%2520common%2520destroy%2520heuristics%2520are%2520non-adaptive%252C%2520any%250Aperformance%2520bottleneck%2520caused%2520by%2520these%2520heuristics%2520cannot%2520be%2520overcome%2520via%250Aadaptive%2520heuristic%2520selection%2520alone%252C%2520thus%2520limiting%2520the%2520overall%2520effectiveness%2520of%250AMAPF-LNS%2520in%2520terms%2520of%2520solution%2520cost.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Adaptive%250ADelay-based%2520Destroy-and-Repair%2520Enhanced%2520with%2520Success-based%2520Self-Learning%250A%2528ADDRESS%2529%2520as%2520a%2520single-destroy-heuristic%2520variant%2520of%2520MAPF-LNS.%2520ADDRESS%2520applies%250Arestricted%2520Thompson%2520Sampling%2520to%2520the%2520top-K%2520set%2520of%2520the%2520most%2520delayed%2520agents%2520to%250Aselect%2520a%2520seed%2520agent%2520for%2520adaptive%2520LNS%2520neighborhood%2520generation.%2520We%2520evaluate%250AADDRESS%2520in%2520multiple%2520maps%2520from%2520the%2520MAPF%2520benchmark%2520set%2520and%2520demonstrate%2520cost%250Aimprovements%2520by%2520at%2520least%252050%2525%2520in%2520large-scale%2520scenarios%2520with%2520up%2520to%2520a%2520thousand%250Aagents%252C%2520compared%2520with%2520the%2520original%2520MAPF-LNS%2520and%2520other%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anytime%20Multi-Agent%20Path%20Finding%20with%20an%20Adaptive%20Delay-Based%20Heuristic&entry.906535625=Thomy%20Phan%20and%20Benran%20Zhang%20and%20Shao-Hung%20Chan%20and%20Sven%20Koenig&entry.1292438233=%20%20Anytime%20multi-agent%20path%20finding%20%28MAPF%29%20is%20a%20promising%20approach%20to%20scalable%0Apath%20optimization%20in%20multi-agent%20systems.%20MAPF-LNS%2C%20based%20on%20Large%20Neighborhood%0ASearch%20%28LNS%29%2C%20is%20the%20current%20state-of-the-art%20approach%20where%20a%20fast%20initial%0Asolution%20is%20iteratively%20optimized%20by%20destroying%20and%20repairing%20selected%20paths%20of%0Athe%20solution.%20Current%20MAPF-LNS%20variants%20commonly%20use%20an%20adaptive%20selection%0Amechanism%20to%20choose%20among%20multiple%20destroy%20heuristics.%20However%2C%20to%20determine%0Apromising%20destroy%20heuristics%2C%20MAPF-LNS%20requires%20a%20considerable%20amount%20of%0Aexploration%20time.%20As%20common%20destroy%20heuristics%20are%20non-adaptive%2C%20any%0Aperformance%20bottleneck%20caused%20by%20these%20heuristics%20cannot%20be%20overcome%20via%0Aadaptive%20heuristic%20selection%20alone%2C%20thus%20limiting%20the%20overall%20effectiveness%20of%0AMAPF-LNS%20in%20terms%20of%20solution%20cost.%20In%20this%20paper%2C%20we%20propose%20Adaptive%0ADelay-based%20Destroy-and-Repair%20Enhanced%20with%20Success-based%20Self-Learning%0A%28ADDRESS%29%20as%20a%20single-destroy-heuristic%20variant%20of%20MAPF-LNS.%20ADDRESS%20applies%0Arestricted%20Thompson%20Sampling%20to%20the%20top-K%20set%20of%20the%20most%20delayed%20agents%20to%0Aselect%20a%20seed%20agent%20for%20adaptive%20LNS%20neighborhood%20generation.%20We%20evaluate%0AADDRESS%20in%20multiple%20maps%20from%20the%20MAPF%20benchmark%20set%20and%20demonstrate%20cost%0Aimprovements%20by%20at%20least%2050%25%20in%20large-scale%20scenarios%20with%20up%20to%20a%20thousand%0Aagents%2C%20compared%20with%20the%20original%20MAPF-LNS%20and%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02960v2&entry.124074799=Read"},
{"title": "On the Hardness of Training Deep Neural Networks Discretely", "author": "Ilan Doron-Arad", "abstract": "  We study neural network training (NNT): optimizing a neural network's\nparameters to minimize the training loss over a given dataset. NNT has been\nstudied extensively under theoretic lenses, mainly on two-layer networks with\nlinear or ReLU activation functions where the parameters can take any real\nvalue (here referred to as continuous NNT (C-NNT)). However, less is known\nabout deeper neural networks, which exhibit substantially stronger capabilities\nin practice. In addition, the complexity of the discrete variant of the problem\n(D-NNT in short), in which the parameters are taken from a given finite set of\noptions, has remained less explored despite its theoretical and practical\nsignificance.\n  In this work, we show that the hardness of NNT is dramatically affected by\nthe network depth. Specifically, we show that, under standard complexity\nassumptions, D-NNT is not in the complexity class NP even for instances with\nfixed dimensions and dataset size, having a deep architecture. This separates\nD-NNT from any NP-complete problem. Furthermore, using a polynomial reduction\nwe show that the above result also holds for C-NNT, albeit with more structured\ninstances. We complement these results with a comprehensive list of NP-hardness\nlower bounds for D-NNT on two-layer networks, showing that fixing the number of\ndimensions, the dataset size, or the number of neurons in the hidden layer\nleaves the problem challenging. Finally, we obtain a pseudo-polynomial\nalgorithm for D-NNT on a two-layer network with a fixed dataset size.\n", "link": "http://arxiv.org/abs/2412.13057v1", "date": "2024-12-17", "relevancy": 1.9064, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4937}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4872}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Hardness%20of%20Training%20Deep%20Neural%20Networks%20Discretely&body=Title%3A%20On%20the%20Hardness%20of%20Training%20Deep%20Neural%20Networks%20Discretely%0AAuthor%3A%20Ilan%20Doron-Arad%0AAbstract%3A%20%20%20We%20study%20neural%20network%20training%20%28NNT%29%3A%20optimizing%20a%20neural%20network%27s%0Aparameters%20to%20minimize%20the%20training%20loss%20over%20a%20given%20dataset.%20NNT%20has%20been%0Astudied%20extensively%20under%20theoretic%20lenses%2C%20mainly%20on%20two-layer%20networks%20with%0Alinear%20or%20ReLU%20activation%20functions%20where%20the%20parameters%20can%20take%20any%20real%0Avalue%20%28here%20referred%20to%20as%20continuous%20NNT%20%28C-NNT%29%29.%20However%2C%20less%20is%20known%0Aabout%20deeper%20neural%20networks%2C%20which%20exhibit%20substantially%20stronger%20capabilities%0Ain%20practice.%20In%20addition%2C%20the%20complexity%20of%20the%20discrete%20variant%20of%20the%20problem%0A%28D-NNT%20in%20short%29%2C%20in%20which%20the%20parameters%20are%20taken%20from%20a%20given%20finite%20set%20of%0Aoptions%2C%20has%20remained%20less%20explored%20despite%20its%20theoretical%20and%20practical%0Asignificance.%0A%20%20In%20this%20work%2C%20we%20show%20that%20the%20hardness%20of%20NNT%20is%20dramatically%20affected%20by%0Athe%20network%20depth.%20Specifically%2C%20we%20show%20that%2C%20under%20standard%20complexity%0Aassumptions%2C%20D-NNT%20is%20not%20in%20the%20complexity%20class%20NP%20even%20for%20instances%20with%0Afixed%20dimensions%20and%20dataset%20size%2C%20having%20a%20deep%20architecture.%20This%20separates%0AD-NNT%20from%20any%20NP-complete%20problem.%20Furthermore%2C%20using%20a%20polynomial%20reduction%0Awe%20show%20that%20the%20above%20result%20also%20holds%20for%20C-NNT%2C%20albeit%20with%20more%20structured%0Ainstances.%20We%20complement%20these%20results%20with%20a%20comprehensive%20list%20of%20NP-hardness%0Alower%20bounds%20for%20D-NNT%20on%20two-layer%20networks%2C%20showing%20that%20fixing%20the%20number%20of%0Adimensions%2C%20the%20dataset%20size%2C%20or%20the%20number%20of%20neurons%20in%20the%20hidden%20layer%0Aleaves%20the%20problem%20challenging.%20Finally%2C%20we%20obtain%20a%20pseudo-polynomial%0Aalgorithm%20for%20D-NNT%20on%20a%20two-layer%20network%20with%20a%20fixed%20dataset%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Hardness%2520of%2520Training%2520Deep%2520Neural%2520Networks%2520Discretely%26entry.906535625%3DIlan%2520Doron-Arad%26entry.1292438233%3D%2520%2520We%2520study%2520neural%2520network%2520training%2520%2528NNT%2529%253A%2520optimizing%2520a%2520neural%2520network%2527s%250Aparameters%2520to%2520minimize%2520the%2520training%2520loss%2520over%2520a%2520given%2520dataset.%2520NNT%2520has%2520been%250Astudied%2520extensively%2520under%2520theoretic%2520lenses%252C%2520mainly%2520on%2520two-layer%2520networks%2520with%250Alinear%2520or%2520ReLU%2520activation%2520functions%2520where%2520the%2520parameters%2520can%2520take%2520any%2520real%250Avalue%2520%2528here%2520referred%2520to%2520as%2520continuous%2520NNT%2520%2528C-NNT%2529%2529.%2520However%252C%2520less%2520is%2520known%250Aabout%2520deeper%2520neural%2520networks%252C%2520which%2520exhibit%2520substantially%2520stronger%2520capabilities%250Ain%2520practice.%2520In%2520addition%252C%2520the%2520complexity%2520of%2520the%2520discrete%2520variant%2520of%2520the%2520problem%250A%2528D-NNT%2520in%2520short%2529%252C%2520in%2520which%2520the%2520parameters%2520are%2520taken%2520from%2520a%2520given%2520finite%2520set%2520of%250Aoptions%252C%2520has%2520remained%2520less%2520explored%2520despite%2520its%2520theoretical%2520and%2520practical%250Asignificance.%250A%2520%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520the%2520hardness%2520of%2520NNT%2520is%2520dramatically%2520affected%2520by%250Athe%2520network%2520depth.%2520Specifically%252C%2520we%2520show%2520that%252C%2520under%2520standard%2520complexity%250Aassumptions%252C%2520D-NNT%2520is%2520not%2520in%2520the%2520complexity%2520class%2520NP%2520even%2520for%2520instances%2520with%250Afixed%2520dimensions%2520and%2520dataset%2520size%252C%2520having%2520a%2520deep%2520architecture.%2520This%2520separates%250AD-NNT%2520from%2520any%2520NP-complete%2520problem.%2520Furthermore%252C%2520using%2520a%2520polynomial%2520reduction%250Awe%2520show%2520that%2520the%2520above%2520result%2520also%2520holds%2520for%2520C-NNT%252C%2520albeit%2520with%2520more%2520structured%250Ainstances.%2520We%2520complement%2520these%2520results%2520with%2520a%2520comprehensive%2520list%2520of%2520NP-hardness%250Alower%2520bounds%2520for%2520D-NNT%2520on%2520two-layer%2520networks%252C%2520showing%2520that%2520fixing%2520the%2520number%2520of%250Adimensions%252C%2520the%2520dataset%2520size%252C%2520or%2520the%2520number%2520of%2520neurons%2520in%2520the%2520hidden%2520layer%250Aleaves%2520the%2520problem%2520challenging.%2520Finally%252C%2520we%2520obtain%2520a%2520pseudo-polynomial%250Aalgorithm%2520for%2520D-NNT%2520on%2520a%2520two-layer%2520network%2520with%2520a%2520fixed%2520dataset%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Hardness%20of%20Training%20Deep%20Neural%20Networks%20Discretely&entry.906535625=Ilan%20Doron-Arad&entry.1292438233=%20%20We%20study%20neural%20network%20training%20%28NNT%29%3A%20optimizing%20a%20neural%20network%27s%0Aparameters%20to%20minimize%20the%20training%20loss%20over%20a%20given%20dataset.%20NNT%20has%20been%0Astudied%20extensively%20under%20theoretic%20lenses%2C%20mainly%20on%20two-layer%20networks%20with%0Alinear%20or%20ReLU%20activation%20functions%20where%20the%20parameters%20can%20take%20any%20real%0Avalue%20%28here%20referred%20to%20as%20continuous%20NNT%20%28C-NNT%29%29.%20However%2C%20less%20is%20known%0Aabout%20deeper%20neural%20networks%2C%20which%20exhibit%20substantially%20stronger%20capabilities%0Ain%20practice.%20In%20addition%2C%20the%20complexity%20of%20the%20discrete%20variant%20of%20the%20problem%0A%28D-NNT%20in%20short%29%2C%20in%20which%20the%20parameters%20are%20taken%20from%20a%20given%20finite%20set%20of%0Aoptions%2C%20has%20remained%20less%20explored%20despite%20its%20theoretical%20and%20practical%0Asignificance.%0A%20%20In%20this%20work%2C%20we%20show%20that%20the%20hardness%20of%20NNT%20is%20dramatically%20affected%20by%0Athe%20network%20depth.%20Specifically%2C%20we%20show%20that%2C%20under%20standard%20complexity%0Aassumptions%2C%20D-NNT%20is%20not%20in%20the%20complexity%20class%20NP%20even%20for%20instances%20with%0Afixed%20dimensions%20and%20dataset%20size%2C%20having%20a%20deep%20architecture.%20This%20separates%0AD-NNT%20from%20any%20NP-complete%20problem.%20Furthermore%2C%20using%20a%20polynomial%20reduction%0Awe%20show%20that%20the%20above%20result%20also%20holds%20for%20C-NNT%2C%20albeit%20with%20more%20structured%0Ainstances.%20We%20complement%20these%20results%20with%20a%20comprehensive%20list%20of%20NP-hardness%0Alower%20bounds%20for%20D-NNT%20on%20two-layer%20networks%2C%20showing%20that%20fixing%20the%20number%20of%0Adimensions%2C%20the%20dataset%20size%2C%20or%20the%20number%20of%20neurons%20in%20the%20hidden%20layer%0Aleaves%20the%20problem%20challenging.%20Finally%2C%20we%20obtain%20a%20pseudo-polynomial%0Aalgorithm%20for%20D-NNT%20on%20a%20two-layer%20network%20with%20a%20fixed%20dataset%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13057v1&entry.124074799=Read"},
{"title": "KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning", "author": "Junnan Liu and Qianren Mao and Weifeng Jiang and Jianxin Li", "abstract": "  Knowledge graph reasoning plays a vital role in various applications and has\ngarnered considerable attention. Recently, path-based methods have achieved\nimpressive performance. However, they may face limitations stemming from\nconstraints in message-passing neural networks, such as missing paths and\ninformation over-squashing. In this paper, we revisit the application of\ntransformers for knowledge graph reasoning to address the constraints faced by\npath-based methods and propose a novel method KnowFormer. KnowFormer utilizes a\ntransformer architecture to perform reasoning on knowledge graphs from the\nmessage-passing perspective, rather than reasoning by textual information like\nprevious pretrained language model based methods. Specifically, we define the\nattention computation based on the query prototype of knowledge graph\nreasoning, facilitating convenient construction and efficient optimization. To\nincorporate structural information into the self-attention mechanism, we\nintroduce structure-aware modules to calculate query, key, and value\nrespectively. Additionally, we present an efficient attention computation\nmethod for better scalability. Experimental results demonstrate the superior\nperformance of KnowFormer compared to prominent baseline methods on both\ntransductive and inductive benchmarks.\n", "link": "http://arxiv.org/abs/2409.12865v2", "date": "2024-12-17", "relevancy": 1.9033, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KnowFormer%3A%20Revisiting%20Transformers%20for%20Knowledge%20Graph%20Reasoning&body=Title%3A%20KnowFormer%3A%20Revisiting%20Transformers%20for%20Knowledge%20Graph%20Reasoning%0AAuthor%3A%20Junnan%20Liu%20and%20Qianren%20Mao%20and%20Weifeng%20Jiang%20and%20Jianxin%20Li%0AAbstract%3A%20%20%20Knowledge%20graph%20reasoning%20plays%20a%20vital%20role%20in%20various%20applications%20and%20has%0Agarnered%20considerable%20attention.%20Recently%2C%20path-based%20methods%20have%20achieved%0Aimpressive%20performance.%20However%2C%20they%20may%20face%20limitations%20stemming%20from%0Aconstraints%20in%20message-passing%20neural%20networks%2C%20such%20as%20missing%20paths%20and%0Ainformation%20over-squashing.%20In%20this%20paper%2C%20we%20revisit%20the%20application%20of%0Atransformers%20for%20knowledge%20graph%20reasoning%20to%20address%20the%20constraints%20faced%20by%0Apath-based%20methods%20and%20propose%20a%20novel%20method%20KnowFormer.%20KnowFormer%20utilizes%20a%0Atransformer%20architecture%20to%20perform%20reasoning%20on%20knowledge%20graphs%20from%20the%0Amessage-passing%20perspective%2C%20rather%20than%20reasoning%20by%20textual%20information%20like%0Aprevious%20pretrained%20language%20model%20based%20methods.%20Specifically%2C%20we%20define%20the%0Aattention%20computation%20based%20on%20the%20query%20prototype%20of%20knowledge%20graph%0Areasoning%2C%20facilitating%20convenient%20construction%20and%20efficient%20optimization.%20To%0Aincorporate%20structural%20information%20into%20the%20self-attention%20mechanism%2C%20we%0Aintroduce%20structure-aware%20modules%20to%20calculate%20query%2C%20key%2C%20and%20value%0Arespectively.%20Additionally%2C%20we%20present%20an%20efficient%20attention%20computation%0Amethod%20for%20better%20scalability.%20Experimental%20results%20demonstrate%20the%20superior%0Aperformance%20of%20KnowFormer%20compared%20to%20prominent%20baseline%20methods%20on%20both%0Atransductive%20and%20inductive%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12865v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowFormer%253A%2520Revisiting%2520Transformers%2520for%2520Knowledge%2520Graph%2520Reasoning%26entry.906535625%3DJunnan%2520Liu%2520and%2520Qianren%2520Mao%2520and%2520Weifeng%2520Jiang%2520and%2520Jianxin%2520Li%26entry.1292438233%3D%2520%2520Knowledge%2520graph%2520reasoning%2520plays%2520a%2520vital%2520role%2520in%2520various%2520applications%2520and%2520has%250Agarnered%2520considerable%2520attention.%2520Recently%252C%2520path-based%2520methods%2520have%2520achieved%250Aimpressive%2520performance.%2520However%252C%2520they%2520may%2520face%2520limitations%2520stemming%2520from%250Aconstraints%2520in%2520message-passing%2520neural%2520networks%252C%2520such%2520as%2520missing%2520paths%2520and%250Ainformation%2520over-squashing.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520the%2520application%2520of%250Atransformers%2520for%2520knowledge%2520graph%2520reasoning%2520to%2520address%2520the%2520constraints%2520faced%2520by%250Apath-based%2520methods%2520and%2520propose%2520a%2520novel%2520method%2520KnowFormer.%2520KnowFormer%2520utilizes%2520a%250Atransformer%2520architecture%2520to%2520perform%2520reasoning%2520on%2520knowledge%2520graphs%2520from%2520the%250Amessage-passing%2520perspective%252C%2520rather%2520than%2520reasoning%2520by%2520textual%2520information%2520like%250Aprevious%2520pretrained%2520language%2520model%2520based%2520methods.%2520Specifically%252C%2520we%2520define%2520the%250Aattention%2520computation%2520based%2520on%2520the%2520query%2520prototype%2520of%2520knowledge%2520graph%250Areasoning%252C%2520facilitating%2520convenient%2520construction%2520and%2520efficient%2520optimization.%2520To%250Aincorporate%2520structural%2520information%2520into%2520the%2520self-attention%2520mechanism%252C%2520we%250Aintroduce%2520structure-aware%2520modules%2520to%2520calculate%2520query%252C%2520key%252C%2520and%2520value%250Arespectively.%2520Additionally%252C%2520we%2520present%2520an%2520efficient%2520attention%2520computation%250Amethod%2520for%2520better%2520scalability.%2520Experimental%2520results%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520KnowFormer%2520compared%2520to%2520prominent%2520baseline%2520methods%2520on%2520both%250Atransductive%2520and%2520inductive%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12865v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KnowFormer%3A%20Revisiting%20Transformers%20for%20Knowledge%20Graph%20Reasoning&entry.906535625=Junnan%20Liu%20and%20Qianren%20Mao%20and%20Weifeng%20Jiang%20and%20Jianxin%20Li&entry.1292438233=%20%20Knowledge%20graph%20reasoning%20plays%20a%20vital%20role%20in%20various%20applications%20and%20has%0Agarnered%20considerable%20attention.%20Recently%2C%20path-based%20methods%20have%20achieved%0Aimpressive%20performance.%20However%2C%20they%20may%20face%20limitations%20stemming%20from%0Aconstraints%20in%20message-passing%20neural%20networks%2C%20such%20as%20missing%20paths%20and%0Ainformation%20over-squashing.%20In%20this%20paper%2C%20we%20revisit%20the%20application%20of%0Atransformers%20for%20knowledge%20graph%20reasoning%20to%20address%20the%20constraints%20faced%20by%0Apath-based%20methods%20and%20propose%20a%20novel%20method%20KnowFormer.%20KnowFormer%20utilizes%20a%0Atransformer%20architecture%20to%20perform%20reasoning%20on%20knowledge%20graphs%20from%20the%0Amessage-passing%20perspective%2C%20rather%20than%20reasoning%20by%20textual%20information%20like%0Aprevious%20pretrained%20language%20model%20based%20methods.%20Specifically%2C%20we%20define%20the%0Aattention%20computation%20based%20on%20the%20query%20prototype%20of%20knowledge%20graph%0Areasoning%2C%20facilitating%20convenient%20construction%20and%20efficient%20optimization.%20To%0Aincorporate%20structural%20information%20into%20the%20self-attention%20mechanism%2C%20we%0Aintroduce%20structure-aware%20modules%20to%20calculate%20query%2C%20key%2C%20and%20value%0Arespectively.%20Additionally%2C%20we%20present%20an%20efficient%20attention%20computation%0Amethod%20for%20better%20scalability.%20Experimental%20results%20demonstrate%20the%20superior%0Aperformance%20of%20KnowFormer%20compared%20to%20prominent%20baseline%20methods%20on%20both%0Atransductive%20and%20inductive%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12865v2&entry.124074799=Read"},
{"title": "GPS-IDS: An Anomaly-based GPS Spoofing Attack Detection Framework for\n  Autonomous Vehicles", "author": "Murad Mehrab Abrar and Amal Youssef and Raian Islam and Shalaka Satam and Banafsheh Saber Latibari and Salim Hariri and Sicong Shao and Soheil Salehi and Pratik Satam", "abstract": "  Autonomous Vehicles (AVs) heavily rely on sensors and communication networks\nlike Global Positioning System (GPS) to navigate autonomously. Prior research\nhas indicated that networks like GPS are vulnerable to cyber-attacks such as\nspoofing and jamming, thus posing serious risks like navigation errors and\nsystem failures. These threats are expected to intensify with the widespread\ndeployment of AVs, making it crucial to detect and mitigate such attacks. This\npaper proposes GPS Intrusion Detection System, or GPS-IDS, an Anomaly-based\nintrusion detection framework to detect GPS spoofing attacks on AVs. The\nframework uses a novel physics-based vehicle behavior model where a GPS\nnavigation model is integrated into the conventional dynamic bicycle model for\naccurate AV behavior representation. Temporal features derived from this\nbehavior model are analyzed using machine learning to detect normal and\nabnormal navigation behaviors. The performance of the GPS-IDS framework is\nevaluated on the AV-GPS-Dataset -- a GPS security dataset for AVs comprising\nreal-world data collected using an AV testbed, and simulated data representing\nurban traffic environments. To the best of our knowledge, this dataset is the\nfirst of its kind and has been publicly released for the global research\ncommunity to address such security challenges.\n", "link": "http://arxiv.org/abs/2405.08359v2", "date": "2024-12-17", "relevancy": 1.8982, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.488}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.468}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPS-IDS%3A%20An%20Anomaly-based%20GPS%20Spoofing%20Attack%20Detection%20Framework%20for%0A%20%20Autonomous%20Vehicles&body=Title%3A%20GPS-IDS%3A%20An%20Anomaly-based%20GPS%20Spoofing%20Attack%20Detection%20Framework%20for%0A%20%20Autonomous%20Vehicles%0AAuthor%3A%20Murad%20Mehrab%20Abrar%20and%20Amal%20Youssef%20and%20Raian%20Islam%20and%20Shalaka%20Satam%20and%20Banafsheh%20Saber%20Latibari%20and%20Salim%20Hariri%20and%20Sicong%20Shao%20and%20Soheil%20Salehi%20and%20Pratik%20Satam%0AAbstract%3A%20%20%20Autonomous%20Vehicles%20%28AVs%29%20heavily%20rely%20on%20sensors%20and%20communication%20networks%0Alike%20Global%20Positioning%20System%20%28GPS%29%20to%20navigate%20autonomously.%20Prior%20research%0Ahas%20indicated%20that%20networks%20like%20GPS%20are%20vulnerable%20to%20cyber-attacks%20such%20as%0Aspoofing%20and%20jamming%2C%20thus%20posing%20serious%20risks%20like%20navigation%20errors%20and%0Asystem%20failures.%20These%20threats%20are%20expected%20to%20intensify%20with%20the%20widespread%0Adeployment%20of%20AVs%2C%20making%20it%20crucial%20to%20detect%20and%20mitigate%20such%20attacks.%20This%0Apaper%20proposes%20GPS%20Intrusion%20Detection%20System%2C%20or%20GPS-IDS%2C%20an%20Anomaly-based%0Aintrusion%20detection%20framework%20to%20detect%20GPS%20spoofing%20attacks%20on%20AVs.%20The%0Aframework%20uses%20a%20novel%20physics-based%20vehicle%20behavior%20model%20where%20a%20GPS%0Anavigation%20model%20is%20integrated%20into%20the%20conventional%20dynamic%20bicycle%20model%20for%0Aaccurate%20AV%20behavior%20representation.%20Temporal%20features%20derived%20from%20this%0Abehavior%20model%20are%20analyzed%20using%20machine%20learning%20to%20detect%20normal%20and%0Aabnormal%20navigation%20behaviors.%20The%20performance%20of%20the%20GPS-IDS%20framework%20is%0Aevaluated%20on%20the%20AV-GPS-Dataset%20--%20a%20GPS%20security%20dataset%20for%20AVs%20comprising%0Areal-world%20data%20collected%20using%20an%20AV%20testbed%2C%20and%20simulated%20data%20representing%0Aurban%20traffic%20environments.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20dataset%20is%20the%0Afirst%20of%20its%20kind%20and%20has%20been%20publicly%20released%20for%20the%20global%20research%0Acommunity%20to%20address%20such%20security%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08359v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPS-IDS%253A%2520An%2520Anomaly-based%2520GPS%2520Spoofing%2520Attack%2520Detection%2520Framework%2520for%250A%2520%2520Autonomous%2520Vehicles%26entry.906535625%3DMurad%2520Mehrab%2520Abrar%2520and%2520Amal%2520Youssef%2520and%2520Raian%2520Islam%2520and%2520Shalaka%2520Satam%2520and%2520Banafsheh%2520Saber%2520Latibari%2520and%2520Salim%2520Hariri%2520and%2520Sicong%2520Shao%2520and%2520Soheil%2520Salehi%2520and%2520Pratik%2520Satam%26entry.1292438233%3D%2520%2520Autonomous%2520Vehicles%2520%2528AVs%2529%2520heavily%2520rely%2520on%2520sensors%2520and%2520communication%2520networks%250Alike%2520Global%2520Positioning%2520System%2520%2528GPS%2529%2520to%2520navigate%2520autonomously.%2520Prior%2520research%250Ahas%2520indicated%2520that%2520networks%2520like%2520GPS%2520are%2520vulnerable%2520to%2520cyber-attacks%2520such%2520as%250Aspoofing%2520and%2520jamming%252C%2520thus%2520posing%2520serious%2520risks%2520like%2520navigation%2520errors%2520and%250Asystem%2520failures.%2520These%2520threats%2520are%2520expected%2520to%2520intensify%2520with%2520the%2520widespread%250Adeployment%2520of%2520AVs%252C%2520making%2520it%2520crucial%2520to%2520detect%2520and%2520mitigate%2520such%2520attacks.%2520This%250Apaper%2520proposes%2520GPS%2520Intrusion%2520Detection%2520System%252C%2520or%2520GPS-IDS%252C%2520an%2520Anomaly-based%250Aintrusion%2520detection%2520framework%2520to%2520detect%2520GPS%2520spoofing%2520attacks%2520on%2520AVs.%2520The%250Aframework%2520uses%2520a%2520novel%2520physics-based%2520vehicle%2520behavior%2520model%2520where%2520a%2520GPS%250Anavigation%2520model%2520is%2520integrated%2520into%2520the%2520conventional%2520dynamic%2520bicycle%2520model%2520for%250Aaccurate%2520AV%2520behavior%2520representation.%2520Temporal%2520features%2520derived%2520from%2520this%250Abehavior%2520model%2520are%2520analyzed%2520using%2520machine%2520learning%2520to%2520detect%2520normal%2520and%250Aabnormal%2520navigation%2520behaviors.%2520The%2520performance%2520of%2520the%2520GPS-IDS%2520framework%2520is%250Aevaluated%2520on%2520the%2520AV-GPS-Dataset%2520--%2520a%2520GPS%2520security%2520dataset%2520for%2520AVs%2520comprising%250Areal-world%2520data%2520collected%2520using%2520an%2520AV%2520testbed%252C%2520and%2520simulated%2520data%2520representing%250Aurban%2520traffic%2520environments.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520dataset%2520is%2520the%250Afirst%2520of%2520its%2520kind%2520and%2520has%2520been%2520publicly%2520released%2520for%2520the%2520global%2520research%250Acommunity%2520to%2520address%2520such%2520security%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08359v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPS-IDS%3A%20An%20Anomaly-based%20GPS%20Spoofing%20Attack%20Detection%20Framework%20for%0A%20%20Autonomous%20Vehicles&entry.906535625=Murad%20Mehrab%20Abrar%20and%20Amal%20Youssef%20and%20Raian%20Islam%20and%20Shalaka%20Satam%20and%20Banafsheh%20Saber%20Latibari%20and%20Salim%20Hariri%20and%20Sicong%20Shao%20and%20Soheil%20Salehi%20and%20Pratik%20Satam&entry.1292438233=%20%20Autonomous%20Vehicles%20%28AVs%29%20heavily%20rely%20on%20sensors%20and%20communication%20networks%0Alike%20Global%20Positioning%20System%20%28GPS%29%20to%20navigate%20autonomously.%20Prior%20research%0Ahas%20indicated%20that%20networks%20like%20GPS%20are%20vulnerable%20to%20cyber-attacks%20such%20as%0Aspoofing%20and%20jamming%2C%20thus%20posing%20serious%20risks%20like%20navigation%20errors%20and%0Asystem%20failures.%20These%20threats%20are%20expected%20to%20intensify%20with%20the%20widespread%0Adeployment%20of%20AVs%2C%20making%20it%20crucial%20to%20detect%20and%20mitigate%20such%20attacks.%20This%0Apaper%20proposes%20GPS%20Intrusion%20Detection%20System%2C%20or%20GPS-IDS%2C%20an%20Anomaly-based%0Aintrusion%20detection%20framework%20to%20detect%20GPS%20spoofing%20attacks%20on%20AVs.%20The%0Aframework%20uses%20a%20novel%20physics-based%20vehicle%20behavior%20model%20where%20a%20GPS%0Anavigation%20model%20is%20integrated%20into%20the%20conventional%20dynamic%20bicycle%20model%20for%0Aaccurate%20AV%20behavior%20representation.%20Temporal%20features%20derived%20from%20this%0Abehavior%20model%20are%20analyzed%20using%20machine%20learning%20to%20detect%20normal%20and%0Aabnormal%20navigation%20behaviors.%20The%20performance%20of%20the%20GPS-IDS%20framework%20is%0Aevaluated%20on%20the%20AV-GPS-Dataset%20--%20a%20GPS%20security%20dataset%20for%20AVs%20comprising%0Areal-world%20data%20collected%20using%20an%20AV%20testbed%2C%20and%20simulated%20data%20representing%0Aurban%20traffic%20environments.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20dataset%20is%20the%0Afirst%20of%20its%20kind%20and%20has%20been%20publicly%20released%20for%20the%20global%20research%0Acommunity%20to%20address%20such%20security%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08359v2&entry.124074799=Read"},
{"title": "Systematic Biases in LLM Simulations of Debates", "author": "Amir Taubenfeld and Yaniv Dover and Roi Reichart and Ariel Goldstein", "abstract": "  The emergence of Large Language Models (LLMs), has opened exciting\npossibilities for constructing computational simulations designed to replicate\nhuman behavior accurately. Current research suggests that LLM-based agents\nbecome increasingly human-like in their performance, sparking interest in using\nthese AI agents as substitutes for human participants in behavioral studies.\nHowever, LLMs are complex statistical learners without straightforward\ndeductive rules, making them prone to unexpected behaviors. Hence, it is\ncrucial to study and pinpoint the key behavioral distinctions between humans\nand LLM-based agents. In this study, we highlight the limitations of LLMs in\nsimulating human interactions, particularly focusing on LLMs' ability to\nsimulate political debates on topics that are important aspects of people's\nday-to-day lives and decision-making processes. Our findings indicate a\ntendency for LLM agents to conform to the model's inherent social biases\ndespite being directed to debate from certain political perspectives. This\ntendency results in behavioral patterns that seem to deviate from\nwell-established social dynamics among humans. We reinforce these observations\nusing an automatic self-fine-tuning method, which enables us to manipulate the\nbiases within the LLM and demonstrate that agents subsequently align with the\naltered biases. These results underscore the need for further research to\ndevelop methods that help agents overcome these biases, a critical step toward\ncreating more realistic simulations.\n", "link": "http://arxiv.org/abs/2402.04049v3", "date": "2024-12-17", "relevancy": 1.8483, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systematic%20Biases%20in%20LLM%20Simulations%20of%20Debates&body=Title%3A%20Systematic%20Biases%20in%20LLM%20Simulations%20of%20Debates%0AAuthor%3A%20Amir%20Taubenfeld%20and%20Yaniv%20Dover%20and%20Roi%20Reichart%20and%20Ariel%20Goldstein%0AAbstract%3A%20%20%20The%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20has%20opened%20exciting%0Apossibilities%20for%20constructing%20computational%20simulations%20designed%20to%20replicate%0Ahuman%20behavior%20accurately.%20Current%20research%20suggests%20that%20LLM-based%20agents%0Abecome%20increasingly%20human-like%20in%20their%20performance%2C%20sparking%20interest%20in%20using%0Athese%20AI%20agents%20as%20substitutes%20for%20human%20participants%20in%20behavioral%20studies.%0AHowever%2C%20LLMs%20are%20complex%20statistical%20learners%20without%20straightforward%0Adeductive%20rules%2C%20making%20them%20prone%20to%20unexpected%20behaviors.%20Hence%2C%20it%20is%0Acrucial%20to%20study%20and%20pinpoint%20the%20key%20behavioral%20distinctions%20between%20humans%0Aand%20LLM-based%20agents.%20In%20this%20study%2C%20we%20highlight%20the%20limitations%20of%20LLMs%20in%0Asimulating%20human%20interactions%2C%20particularly%20focusing%20on%20LLMs%27%20ability%20to%0Asimulate%20political%20debates%20on%20topics%20that%20are%20important%20aspects%20of%20people%27s%0Aday-to-day%20lives%20and%20decision-making%20processes.%20Our%20findings%20indicate%20a%0Atendency%20for%20LLM%20agents%20to%20conform%20to%20the%20model%27s%20inherent%20social%20biases%0Adespite%20being%20directed%20to%20debate%20from%20certain%20political%20perspectives.%20This%0Atendency%20results%20in%20behavioral%20patterns%20that%20seem%20to%20deviate%20from%0Awell-established%20social%20dynamics%20among%20humans.%20We%20reinforce%20these%20observations%0Ausing%20an%20automatic%20self-fine-tuning%20method%2C%20which%20enables%20us%20to%20manipulate%20the%0Abiases%20within%20the%20LLM%20and%20demonstrate%20that%20agents%20subsequently%20align%20with%20the%0Aaltered%20biases.%20These%20results%20underscore%20the%20need%20for%20further%20research%20to%0Adevelop%20methods%20that%20help%20agents%20overcome%20these%20biases%2C%20a%20critical%20step%20toward%0Acreating%20more%20realistic%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04049v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystematic%2520Biases%2520in%2520LLM%2520Simulations%2520of%2520Debates%26entry.906535625%3DAmir%2520Taubenfeld%2520and%2520Yaniv%2520Dover%2520and%2520Roi%2520Reichart%2520and%2520Ariel%2520Goldstein%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520has%2520opened%2520exciting%250Apossibilities%2520for%2520constructing%2520computational%2520simulations%2520designed%2520to%2520replicate%250Ahuman%2520behavior%2520accurately.%2520Current%2520research%2520suggests%2520that%2520LLM-based%2520agents%250Abecome%2520increasingly%2520human-like%2520in%2520their%2520performance%252C%2520sparking%2520interest%2520in%2520using%250Athese%2520AI%2520agents%2520as%2520substitutes%2520for%2520human%2520participants%2520in%2520behavioral%2520studies.%250AHowever%252C%2520LLMs%2520are%2520complex%2520statistical%2520learners%2520without%2520straightforward%250Adeductive%2520rules%252C%2520making%2520them%2520prone%2520to%2520unexpected%2520behaviors.%2520Hence%252C%2520it%2520is%250Acrucial%2520to%2520study%2520and%2520pinpoint%2520the%2520key%2520behavioral%2520distinctions%2520between%2520humans%250Aand%2520LLM-based%2520agents.%2520In%2520this%2520study%252C%2520we%2520highlight%2520the%2520limitations%2520of%2520LLMs%2520in%250Asimulating%2520human%2520interactions%252C%2520particularly%2520focusing%2520on%2520LLMs%2527%2520ability%2520to%250Asimulate%2520political%2520debates%2520on%2520topics%2520that%2520are%2520important%2520aspects%2520of%2520people%2527s%250Aday-to-day%2520lives%2520and%2520decision-making%2520processes.%2520Our%2520findings%2520indicate%2520a%250Atendency%2520for%2520LLM%2520agents%2520to%2520conform%2520to%2520the%2520model%2527s%2520inherent%2520social%2520biases%250Adespite%2520being%2520directed%2520to%2520debate%2520from%2520certain%2520political%2520perspectives.%2520This%250Atendency%2520results%2520in%2520behavioral%2520patterns%2520that%2520seem%2520to%2520deviate%2520from%250Awell-established%2520social%2520dynamics%2520among%2520humans.%2520We%2520reinforce%2520these%2520observations%250Ausing%2520an%2520automatic%2520self-fine-tuning%2520method%252C%2520which%2520enables%2520us%2520to%2520manipulate%2520the%250Abiases%2520within%2520the%2520LLM%2520and%2520demonstrate%2520that%2520agents%2520subsequently%2520align%2520with%2520the%250Aaltered%2520biases.%2520These%2520results%2520underscore%2520the%2520need%2520for%2520further%2520research%2520to%250Adevelop%2520methods%2520that%2520help%2520agents%2520overcome%2520these%2520biases%252C%2520a%2520critical%2520step%2520toward%250Acreating%2520more%2520realistic%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04049v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systematic%20Biases%20in%20LLM%20Simulations%20of%20Debates&entry.906535625=Amir%20Taubenfeld%20and%20Yaniv%20Dover%20and%20Roi%20Reichart%20and%20Ariel%20Goldstein&entry.1292438233=%20%20The%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20has%20opened%20exciting%0Apossibilities%20for%20constructing%20computational%20simulations%20designed%20to%20replicate%0Ahuman%20behavior%20accurately.%20Current%20research%20suggests%20that%20LLM-based%20agents%0Abecome%20increasingly%20human-like%20in%20their%20performance%2C%20sparking%20interest%20in%20using%0Athese%20AI%20agents%20as%20substitutes%20for%20human%20participants%20in%20behavioral%20studies.%0AHowever%2C%20LLMs%20are%20complex%20statistical%20learners%20without%20straightforward%0Adeductive%20rules%2C%20making%20them%20prone%20to%20unexpected%20behaviors.%20Hence%2C%20it%20is%0Acrucial%20to%20study%20and%20pinpoint%20the%20key%20behavioral%20distinctions%20between%20humans%0Aand%20LLM-based%20agents.%20In%20this%20study%2C%20we%20highlight%20the%20limitations%20of%20LLMs%20in%0Asimulating%20human%20interactions%2C%20particularly%20focusing%20on%20LLMs%27%20ability%20to%0Asimulate%20political%20debates%20on%20topics%20that%20are%20important%20aspects%20of%20people%27s%0Aday-to-day%20lives%20and%20decision-making%20processes.%20Our%20findings%20indicate%20a%0Atendency%20for%20LLM%20agents%20to%20conform%20to%20the%20model%27s%20inherent%20social%20biases%0Adespite%20being%20directed%20to%20debate%20from%20certain%20political%20perspectives.%20This%0Atendency%20results%20in%20behavioral%20patterns%20that%20seem%20to%20deviate%20from%0Awell-established%20social%20dynamics%20among%20humans.%20We%20reinforce%20these%20observations%0Ausing%20an%20automatic%20self-fine-tuning%20method%2C%20which%20enables%20us%20to%20manipulate%20the%0Abiases%20within%20the%20LLM%20and%20demonstrate%20that%20agents%20subsequently%20align%20with%20the%0Aaltered%20biases.%20These%20results%20underscore%20the%20need%20for%20further%20research%20to%0Adevelop%20methods%20that%20help%20agents%20overcome%20these%20biases%2C%20a%20critical%20step%20toward%0Acreating%20more%20realistic%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04049v3&entry.124074799=Read"},
{"title": "Learning Visuotactile Estimation and Control for Non-prehensile\n  Manipulation under Occlusions", "author": "Juan Del Aguila Ferrandis and Jo\u00e3o Moura and Sethu Vijayakumar", "abstract": "  Manipulation without grasping, known as non-prehensile manipulation, is\nessential for dexterous robots in contact-rich environments, but presents many\nchallenges relating with underactuation, hybrid-dynamics, and frictional\nuncertainty. Additionally, object occlusions in a scenario of contact\nuncertainty and where the motion of the object evolves independently from the\nrobot becomes a critical problem, which previous literature fails to address.\nWe present a method for learning visuotactile state estimators and\nuncertainty-aware control policies for non-prehensile manipulation under\nocclusions, by leveraging diverse interaction data from privileged policies\ntrained in simulation. We formulate the estimator within a Bayesian deep\nlearning framework, to model its uncertainty, and then train uncertainty-aware\ncontrol policies by incorporating the pre-learned estimator into the\nreinforcement learning (RL) loop, both of which lead to significantly improved\nestimator and policy performance. Therefore, unlike prior non-prehensile\nresearch that relies on complex external perception set-ups, our method\nsuccessfully handles occlusions after sim-to-real transfer to robotic hardware\nwith a simple onboard camera. See our video: https://youtu.be/hW-C8i_HWgs.\n", "link": "http://arxiv.org/abs/2412.13157v1", "date": "2024-12-17", "relevancy": 1.8374, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6661}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5997}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Visuotactile%20Estimation%20and%20Control%20for%20Non-prehensile%0A%20%20Manipulation%20under%20Occlusions&body=Title%3A%20Learning%20Visuotactile%20Estimation%20and%20Control%20for%20Non-prehensile%0A%20%20Manipulation%20under%20Occlusions%0AAuthor%3A%20Juan%20Del%20Aguila%20Ferrandis%20and%20Jo%C3%A3o%20Moura%20and%20Sethu%20Vijayakumar%0AAbstract%3A%20%20%20Manipulation%20without%20grasping%2C%20known%20as%20non-prehensile%20manipulation%2C%20is%0Aessential%20for%20dexterous%20robots%20in%20contact-rich%20environments%2C%20but%20presents%20many%0Achallenges%20relating%20with%20underactuation%2C%20hybrid-dynamics%2C%20and%20frictional%0Auncertainty.%20Additionally%2C%20object%20occlusions%20in%20a%20scenario%20of%20contact%0Auncertainty%20and%20where%20the%20motion%20of%20the%20object%20evolves%20independently%20from%20the%0Arobot%20becomes%20a%20critical%20problem%2C%20which%20previous%20literature%20fails%20to%20address.%0AWe%20present%20a%20method%20for%20learning%20visuotactile%20state%20estimators%20and%0Auncertainty-aware%20control%20policies%20for%20non-prehensile%20manipulation%20under%0Aocclusions%2C%20by%20leveraging%20diverse%20interaction%20data%20from%20privileged%20policies%0Atrained%20in%20simulation.%20We%20formulate%20the%20estimator%20within%20a%20Bayesian%20deep%0Alearning%20framework%2C%20to%20model%20its%20uncertainty%2C%20and%20then%20train%20uncertainty-aware%0Acontrol%20policies%20by%20incorporating%20the%20pre-learned%20estimator%20into%20the%0Areinforcement%20learning%20%28RL%29%20loop%2C%20both%20of%20which%20lead%20to%20significantly%20improved%0Aestimator%20and%20policy%20performance.%20Therefore%2C%20unlike%20prior%20non-prehensile%0Aresearch%20that%20relies%20on%20complex%20external%20perception%20set-ups%2C%20our%20method%0Asuccessfully%20handles%20occlusions%20after%20sim-to-real%20transfer%20to%20robotic%20hardware%0Awith%20a%20simple%20onboard%20camera.%20See%20our%20video%3A%20https%3A//youtu.be/hW-C8i_HWgs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Visuotactile%2520Estimation%2520and%2520Control%2520for%2520Non-prehensile%250A%2520%2520Manipulation%2520under%2520Occlusions%26entry.906535625%3DJuan%2520Del%2520Aguila%2520Ferrandis%2520and%2520Jo%25C3%25A3o%2520Moura%2520and%2520Sethu%2520Vijayakumar%26entry.1292438233%3D%2520%2520Manipulation%2520without%2520grasping%252C%2520known%2520as%2520non-prehensile%2520manipulation%252C%2520is%250Aessential%2520for%2520dexterous%2520robots%2520in%2520contact-rich%2520environments%252C%2520but%2520presents%2520many%250Achallenges%2520relating%2520with%2520underactuation%252C%2520hybrid-dynamics%252C%2520and%2520frictional%250Auncertainty.%2520Additionally%252C%2520object%2520occlusions%2520in%2520a%2520scenario%2520of%2520contact%250Auncertainty%2520and%2520where%2520the%2520motion%2520of%2520the%2520object%2520evolves%2520independently%2520from%2520the%250Arobot%2520becomes%2520a%2520critical%2520problem%252C%2520which%2520previous%2520literature%2520fails%2520to%2520address.%250AWe%2520present%2520a%2520method%2520for%2520learning%2520visuotactile%2520state%2520estimators%2520and%250Auncertainty-aware%2520control%2520policies%2520for%2520non-prehensile%2520manipulation%2520under%250Aocclusions%252C%2520by%2520leveraging%2520diverse%2520interaction%2520data%2520from%2520privileged%2520policies%250Atrained%2520in%2520simulation.%2520We%2520formulate%2520the%2520estimator%2520within%2520a%2520Bayesian%2520deep%250Alearning%2520framework%252C%2520to%2520model%2520its%2520uncertainty%252C%2520and%2520then%2520train%2520uncertainty-aware%250Acontrol%2520policies%2520by%2520incorporating%2520the%2520pre-learned%2520estimator%2520into%2520the%250Areinforcement%2520learning%2520%2528RL%2529%2520loop%252C%2520both%2520of%2520which%2520lead%2520to%2520significantly%2520improved%250Aestimator%2520and%2520policy%2520performance.%2520Therefore%252C%2520unlike%2520prior%2520non-prehensile%250Aresearch%2520that%2520relies%2520on%2520complex%2520external%2520perception%2520set-ups%252C%2520our%2520method%250Asuccessfully%2520handles%2520occlusions%2520after%2520sim-to-real%2520transfer%2520to%2520robotic%2520hardware%250Awith%2520a%2520simple%2520onboard%2520camera.%2520See%2520our%2520video%253A%2520https%253A//youtu.be/hW-C8i_HWgs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Visuotactile%20Estimation%20and%20Control%20for%20Non-prehensile%0A%20%20Manipulation%20under%20Occlusions&entry.906535625=Juan%20Del%20Aguila%20Ferrandis%20and%20Jo%C3%A3o%20Moura%20and%20Sethu%20Vijayakumar&entry.1292438233=%20%20Manipulation%20without%20grasping%2C%20known%20as%20non-prehensile%20manipulation%2C%20is%0Aessential%20for%20dexterous%20robots%20in%20contact-rich%20environments%2C%20but%20presents%20many%0Achallenges%20relating%20with%20underactuation%2C%20hybrid-dynamics%2C%20and%20frictional%0Auncertainty.%20Additionally%2C%20object%20occlusions%20in%20a%20scenario%20of%20contact%0Auncertainty%20and%20where%20the%20motion%20of%20the%20object%20evolves%20independently%20from%20the%0Arobot%20becomes%20a%20critical%20problem%2C%20which%20previous%20literature%20fails%20to%20address.%0AWe%20present%20a%20method%20for%20learning%20visuotactile%20state%20estimators%20and%0Auncertainty-aware%20control%20policies%20for%20non-prehensile%20manipulation%20under%0Aocclusions%2C%20by%20leveraging%20diverse%20interaction%20data%20from%20privileged%20policies%0Atrained%20in%20simulation.%20We%20formulate%20the%20estimator%20within%20a%20Bayesian%20deep%0Alearning%20framework%2C%20to%20model%20its%20uncertainty%2C%20and%20then%20train%20uncertainty-aware%0Acontrol%20policies%20by%20incorporating%20the%20pre-learned%20estimator%20into%20the%0Areinforcement%20learning%20%28RL%29%20loop%2C%20both%20of%20which%20lead%20to%20significantly%20improved%0Aestimator%20and%20policy%20performance.%20Therefore%2C%20unlike%20prior%20non-prehensile%0Aresearch%20that%20relies%20on%20complex%20external%20perception%20set-ups%2C%20our%20method%0Asuccessfully%20handles%20occlusions%20after%20sim-to-real%20transfer%20to%20robotic%20hardware%0Awith%20a%20simple%20onboard%20camera.%20See%20our%20video%3A%20https%3A//youtu.be/hW-C8i_HWgs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13157v1&entry.124074799=Read"},
{"title": "An Ad-hoc graph node vector embedding algorithm for general knowledge\n  graphs using Kinetica-Graph", "author": "B. Kaan Karamete and Eli Glaser", "abstract": "  This paper discusses how to generate general graph node embeddings from\nknowledge graph representations. The embedded space is composed of a number of\nsub-features to mimic both local affinity and remote structural relevance.\nThese sub-feature dimensions are defined by several indicators that we\nspeculate to catch nodal similarities, such as hop-based topological patterns,\nthe number of overlapping labels, the transitional probabilities (markov-chain\nprobabilities), and the cluster indices computed by our recursive spectral\nbisection (RSB) algorithm. These measures are flattened over the one\ndimensional vector space into their respective sub-component ranges such that\nthe entire set of vector similarity functions could be used for finding similar\nnodes. The error is defined by the sum of pairwise square differences across a\nrandomly selected sample of graph nodes between the assumed embeddings and the\nground truth estimates as our novel loss function. The ground truth is\nestimated to be a combination of pairwise Jaccard similarity and the number of\noverlapping labels. Finally, we demonstrate a multi-variate stochastic gradient\ndescent (SGD) algorithm to compute the weighing factors among sub-vector spaces\nto minimize the average error using a random sampling logic.\n", "link": "http://arxiv.org/abs/2407.15906v2", "date": "2024-12-17", "relevancy": 1.8234, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4679}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4554}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Ad-hoc%20graph%20node%20vector%20embedding%20algorithm%20for%20general%20knowledge%0A%20%20graphs%20using%20Kinetica-Graph&body=Title%3A%20An%20Ad-hoc%20graph%20node%20vector%20embedding%20algorithm%20for%20general%20knowledge%0A%20%20graphs%20using%20Kinetica-Graph%0AAuthor%3A%20B.%20Kaan%20Karamete%20and%20Eli%20Glaser%0AAbstract%3A%20%20%20This%20paper%20discusses%20how%20to%20generate%20general%20graph%20node%20embeddings%20from%0Aknowledge%20graph%20representations.%20The%20embedded%20space%20is%20composed%20of%20a%20number%20of%0Asub-features%20to%20mimic%20both%20local%20affinity%20and%20remote%20structural%20relevance.%0AThese%20sub-feature%20dimensions%20are%20defined%20by%20several%20indicators%20that%20we%0Aspeculate%20to%20catch%20nodal%20similarities%2C%20such%20as%20hop-based%20topological%20patterns%2C%0Athe%20number%20of%20overlapping%20labels%2C%20the%20transitional%20probabilities%20%28markov-chain%0Aprobabilities%29%2C%20and%20the%20cluster%20indices%20computed%20by%20our%20recursive%20spectral%0Abisection%20%28RSB%29%20algorithm.%20These%20measures%20are%20flattened%20over%20the%20one%0Adimensional%20vector%20space%20into%20their%20respective%20sub-component%20ranges%20such%20that%0Athe%20entire%20set%20of%20vector%20similarity%20functions%20could%20be%20used%20for%20finding%20similar%0Anodes.%20The%20error%20is%20defined%20by%20the%20sum%20of%20pairwise%20square%20differences%20across%20a%0Arandomly%20selected%20sample%20of%20graph%20nodes%20between%20the%20assumed%20embeddings%20and%20the%0Aground%20truth%20estimates%20as%20our%20novel%20loss%20function.%20The%20ground%20truth%20is%0Aestimated%20to%20be%20a%20combination%20of%20pairwise%20Jaccard%20similarity%20and%20the%20number%20of%0Aoverlapping%20labels.%20Finally%2C%20we%20demonstrate%20a%20multi-variate%20stochastic%20gradient%0Adescent%20%28SGD%29%20algorithm%20to%20compute%20the%20weighing%20factors%20among%20sub-vector%20spaces%0Ato%20minimize%20the%20average%20error%20using%20a%20random%20sampling%20logic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15906v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Ad-hoc%2520graph%2520node%2520vector%2520embedding%2520algorithm%2520for%2520general%2520knowledge%250A%2520%2520graphs%2520using%2520Kinetica-Graph%26entry.906535625%3DB.%2520Kaan%2520Karamete%2520and%2520Eli%2520Glaser%26entry.1292438233%3D%2520%2520This%2520paper%2520discusses%2520how%2520to%2520generate%2520general%2520graph%2520node%2520embeddings%2520from%250Aknowledge%2520graph%2520representations.%2520The%2520embedded%2520space%2520is%2520composed%2520of%2520a%2520number%2520of%250Asub-features%2520to%2520mimic%2520both%2520local%2520affinity%2520and%2520remote%2520structural%2520relevance.%250AThese%2520sub-feature%2520dimensions%2520are%2520defined%2520by%2520several%2520indicators%2520that%2520we%250Aspeculate%2520to%2520catch%2520nodal%2520similarities%252C%2520such%2520as%2520hop-based%2520topological%2520patterns%252C%250Athe%2520number%2520of%2520overlapping%2520labels%252C%2520the%2520transitional%2520probabilities%2520%2528markov-chain%250Aprobabilities%2529%252C%2520and%2520the%2520cluster%2520indices%2520computed%2520by%2520our%2520recursive%2520spectral%250Abisection%2520%2528RSB%2529%2520algorithm.%2520These%2520measures%2520are%2520flattened%2520over%2520the%2520one%250Adimensional%2520vector%2520space%2520into%2520their%2520respective%2520sub-component%2520ranges%2520such%2520that%250Athe%2520entire%2520set%2520of%2520vector%2520similarity%2520functions%2520could%2520be%2520used%2520for%2520finding%2520similar%250Anodes.%2520The%2520error%2520is%2520defined%2520by%2520the%2520sum%2520of%2520pairwise%2520square%2520differences%2520across%2520a%250Arandomly%2520selected%2520sample%2520of%2520graph%2520nodes%2520between%2520the%2520assumed%2520embeddings%2520and%2520the%250Aground%2520truth%2520estimates%2520as%2520our%2520novel%2520loss%2520function.%2520The%2520ground%2520truth%2520is%250Aestimated%2520to%2520be%2520a%2520combination%2520of%2520pairwise%2520Jaccard%2520similarity%2520and%2520the%2520number%2520of%250Aoverlapping%2520labels.%2520Finally%252C%2520we%2520demonstrate%2520a%2520multi-variate%2520stochastic%2520gradient%250Adescent%2520%2528SGD%2529%2520algorithm%2520to%2520compute%2520the%2520weighing%2520factors%2520among%2520sub-vector%2520spaces%250Ato%2520minimize%2520the%2520average%2520error%2520using%2520a%2520random%2520sampling%2520logic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15906v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Ad-hoc%20graph%20node%20vector%20embedding%20algorithm%20for%20general%20knowledge%0A%20%20graphs%20using%20Kinetica-Graph&entry.906535625=B.%20Kaan%20Karamete%20and%20Eli%20Glaser&entry.1292438233=%20%20This%20paper%20discusses%20how%20to%20generate%20general%20graph%20node%20embeddings%20from%0Aknowledge%20graph%20representations.%20The%20embedded%20space%20is%20composed%20of%20a%20number%20of%0Asub-features%20to%20mimic%20both%20local%20affinity%20and%20remote%20structural%20relevance.%0AThese%20sub-feature%20dimensions%20are%20defined%20by%20several%20indicators%20that%20we%0Aspeculate%20to%20catch%20nodal%20similarities%2C%20such%20as%20hop-based%20topological%20patterns%2C%0Athe%20number%20of%20overlapping%20labels%2C%20the%20transitional%20probabilities%20%28markov-chain%0Aprobabilities%29%2C%20and%20the%20cluster%20indices%20computed%20by%20our%20recursive%20spectral%0Abisection%20%28RSB%29%20algorithm.%20These%20measures%20are%20flattened%20over%20the%20one%0Adimensional%20vector%20space%20into%20their%20respective%20sub-component%20ranges%20such%20that%0Athe%20entire%20set%20of%20vector%20similarity%20functions%20could%20be%20used%20for%20finding%20similar%0Anodes.%20The%20error%20is%20defined%20by%20the%20sum%20of%20pairwise%20square%20differences%20across%20a%0Arandomly%20selected%20sample%20of%20graph%20nodes%20between%20the%20assumed%20embeddings%20and%20the%0Aground%20truth%20estimates%20as%20our%20novel%20loss%20function.%20The%20ground%20truth%20is%0Aestimated%20to%20be%20a%20combination%20of%20pairwise%20Jaccard%20similarity%20and%20the%20number%20of%0Aoverlapping%20labels.%20Finally%2C%20we%20demonstrate%20a%20multi-variate%20stochastic%20gradient%0Adescent%20%28SGD%29%20algorithm%20to%20compute%20the%20weighing%20factors%20among%20sub-vector%20spaces%0Ato%20minimize%20the%20average%20error%20using%20a%20random%20sampling%20logic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15906v2&entry.124074799=Read"},
{"title": "Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A\n  Novel Weighted Retrieval-Augmented Generation Paradigm", "author": "Rajat Khanda", "abstract": "  Technical troubleshooting in enterprise environments often involves\nnavigating diverse, heterogeneous data sources to resolve complex issues\neffectively. This paper presents a novel agentic AI solution built on a\nWeighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprise\ntechnical troubleshooting. By dynamically weighting retrieval sources such as\nproduct manuals, internal knowledge bases, FAQs, and troubleshooting guides\nbased on query context, the framework prioritizes the most relevant data. For\ninstance, it gives precedence to product manuals for SKU-specific queries while\nincorporating general FAQs for broader issues. The system employs FAISS for\nefficient dense vector search, coupled with a dynamic aggregation mechanism to\nseamlessly integrate results from multiple sources. A Llama-based\nself-evaluator ensures the contextual accuracy and confidence of the generated\nresponses before delivering them. This iterative cycle of retrieval and\nvalidation enhances precision, diversity, and reliability in response\ngeneration. Preliminary evaluations on large enterprise datasets demonstrate\nthe framework's efficacy in improving troubleshooting accuracy, reducing\nresolution times, and adapting to varied technical challenges. Future research\naims to enhance the framework by integrating advanced conversational AI\ncapabilities, enabling more interactive and intuitive troubleshooting\nexperiences. Efforts will also focus on refining the dynamic weighting\nmechanism through reinforcement learning to further optimize the relevance and\nprecision of retrieved information. By incorporating these advancements, the\nproposed framework is poised to evolve into a comprehensive, autonomous AI\nsolution, redefining technical service workflows across enterprise settings.\n", "link": "http://arxiv.org/abs/2412.12006v2", "date": "2024-12-17", "relevancy": 1.7676, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4717}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4362}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20AI-Driven%20Technical%20Troubleshooting%20for%20Enterprise%20Systems%3A%20A%0A%20%20Novel%20Weighted%20Retrieval-Augmented%20Generation%20Paradigm&body=Title%3A%20Agentic%20AI-Driven%20Technical%20Troubleshooting%20for%20Enterprise%20Systems%3A%20A%0A%20%20Novel%20Weighted%20Retrieval-Augmented%20Generation%20Paradigm%0AAuthor%3A%20Rajat%20Khanda%0AAbstract%3A%20%20%20Technical%20troubleshooting%20in%20enterprise%20environments%20often%20involves%0Anavigating%20diverse%2C%20heterogeneous%20data%20sources%20to%20resolve%20complex%20issues%0Aeffectively.%20This%20paper%20presents%20a%20novel%20agentic%20AI%20solution%20built%20on%20a%0AWeighted%20Retrieval-Augmented%20Generation%20%28RAG%29%20Framework%20tailored%20for%20enterprise%0Atechnical%20troubleshooting.%20By%20dynamically%20weighting%20retrieval%20sources%20such%20as%0Aproduct%20manuals%2C%20internal%20knowledge%20bases%2C%20FAQs%2C%20and%20troubleshooting%20guides%0Abased%20on%20query%20context%2C%20the%20framework%20prioritizes%20the%20most%20relevant%20data.%20For%0Ainstance%2C%20it%20gives%20precedence%20to%20product%20manuals%20for%20SKU-specific%20queries%20while%0Aincorporating%20general%20FAQs%20for%20broader%20issues.%20The%20system%20employs%20FAISS%20for%0Aefficient%20dense%20vector%20search%2C%20coupled%20with%20a%20dynamic%20aggregation%20mechanism%20to%0Aseamlessly%20integrate%20results%20from%20multiple%20sources.%20A%20Llama-based%0Aself-evaluator%20ensures%20the%20contextual%20accuracy%20and%20confidence%20of%20the%20generated%0Aresponses%20before%20delivering%20them.%20This%20iterative%20cycle%20of%20retrieval%20and%0Avalidation%20enhances%20precision%2C%20diversity%2C%20and%20reliability%20in%20response%0Ageneration.%20Preliminary%20evaluations%20on%20large%20enterprise%20datasets%20demonstrate%0Athe%20framework%27s%20efficacy%20in%20improving%20troubleshooting%20accuracy%2C%20reducing%0Aresolution%20times%2C%20and%20adapting%20to%20varied%20technical%20challenges.%20Future%20research%0Aaims%20to%20enhance%20the%20framework%20by%20integrating%20advanced%20conversational%20AI%0Acapabilities%2C%20enabling%20more%20interactive%20and%20intuitive%20troubleshooting%0Aexperiences.%20Efforts%20will%20also%20focus%20on%20refining%20the%20dynamic%20weighting%0Amechanism%20through%20reinforcement%20learning%20to%20further%20optimize%20the%20relevance%20and%0Aprecision%20of%20retrieved%20information.%20By%20incorporating%20these%20advancements%2C%20the%0Aproposed%20framework%20is%20poised%20to%20evolve%20into%20a%20comprehensive%2C%20autonomous%20AI%0Asolution%2C%20redefining%20technical%20service%20workflows%20across%20enterprise%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520AI-Driven%2520Technical%2520Troubleshooting%2520for%2520Enterprise%2520Systems%253A%2520A%250A%2520%2520Novel%2520Weighted%2520Retrieval-Augmented%2520Generation%2520Paradigm%26entry.906535625%3DRajat%2520Khanda%26entry.1292438233%3D%2520%2520Technical%2520troubleshooting%2520in%2520enterprise%2520environments%2520often%2520involves%250Anavigating%2520diverse%252C%2520heterogeneous%2520data%2520sources%2520to%2520resolve%2520complex%2520issues%250Aeffectively.%2520This%2520paper%2520presents%2520a%2520novel%2520agentic%2520AI%2520solution%2520built%2520on%2520a%250AWeighted%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520Framework%2520tailored%2520for%2520enterprise%250Atechnical%2520troubleshooting.%2520By%2520dynamically%2520weighting%2520retrieval%2520sources%2520such%2520as%250Aproduct%2520manuals%252C%2520internal%2520knowledge%2520bases%252C%2520FAQs%252C%2520and%2520troubleshooting%2520guides%250Abased%2520on%2520query%2520context%252C%2520the%2520framework%2520prioritizes%2520the%2520most%2520relevant%2520data.%2520For%250Ainstance%252C%2520it%2520gives%2520precedence%2520to%2520product%2520manuals%2520for%2520SKU-specific%2520queries%2520while%250Aincorporating%2520general%2520FAQs%2520for%2520broader%2520issues.%2520The%2520system%2520employs%2520FAISS%2520for%250Aefficient%2520dense%2520vector%2520search%252C%2520coupled%2520with%2520a%2520dynamic%2520aggregation%2520mechanism%2520to%250Aseamlessly%2520integrate%2520results%2520from%2520multiple%2520sources.%2520A%2520Llama-based%250Aself-evaluator%2520ensures%2520the%2520contextual%2520accuracy%2520and%2520confidence%2520of%2520the%2520generated%250Aresponses%2520before%2520delivering%2520them.%2520This%2520iterative%2520cycle%2520of%2520retrieval%2520and%250Avalidation%2520enhances%2520precision%252C%2520diversity%252C%2520and%2520reliability%2520in%2520response%250Ageneration.%2520Preliminary%2520evaluations%2520on%2520large%2520enterprise%2520datasets%2520demonstrate%250Athe%2520framework%2527s%2520efficacy%2520in%2520improving%2520troubleshooting%2520accuracy%252C%2520reducing%250Aresolution%2520times%252C%2520and%2520adapting%2520to%2520varied%2520technical%2520challenges.%2520Future%2520research%250Aaims%2520to%2520enhance%2520the%2520framework%2520by%2520integrating%2520advanced%2520conversational%2520AI%250Acapabilities%252C%2520enabling%2520more%2520interactive%2520and%2520intuitive%2520troubleshooting%250Aexperiences.%2520Efforts%2520will%2520also%2520focus%2520on%2520refining%2520the%2520dynamic%2520weighting%250Amechanism%2520through%2520reinforcement%2520learning%2520to%2520further%2520optimize%2520the%2520relevance%2520and%250Aprecision%2520of%2520retrieved%2520information.%2520By%2520incorporating%2520these%2520advancements%252C%2520the%250Aproposed%2520framework%2520is%2520poised%2520to%2520evolve%2520into%2520a%2520comprehensive%252C%2520autonomous%2520AI%250Asolution%252C%2520redefining%2520technical%2520service%2520workflows%2520across%2520enterprise%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20AI-Driven%20Technical%20Troubleshooting%20for%20Enterprise%20Systems%3A%20A%0A%20%20Novel%20Weighted%20Retrieval-Augmented%20Generation%20Paradigm&entry.906535625=Rajat%20Khanda&entry.1292438233=%20%20Technical%20troubleshooting%20in%20enterprise%20environments%20often%20involves%0Anavigating%20diverse%2C%20heterogeneous%20data%20sources%20to%20resolve%20complex%20issues%0Aeffectively.%20This%20paper%20presents%20a%20novel%20agentic%20AI%20solution%20built%20on%20a%0AWeighted%20Retrieval-Augmented%20Generation%20%28RAG%29%20Framework%20tailored%20for%20enterprise%0Atechnical%20troubleshooting.%20By%20dynamically%20weighting%20retrieval%20sources%20such%20as%0Aproduct%20manuals%2C%20internal%20knowledge%20bases%2C%20FAQs%2C%20and%20troubleshooting%20guides%0Abased%20on%20query%20context%2C%20the%20framework%20prioritizes%20the%20most%20relevant%20data.%20For%0Ainstance%2C%20it%20gives%20precedence%20to%20product%20manuals%20for%20SKU-specific%20queries%20while%0Aincorporating%20general%20FAQs%20for%20broader%20issues.%20The%20system%20employs%20FAISS%20for%0Aefficient%20dense%20vector%20search%2C%20coupled%20with%20a%20dynamic%20aggregation%20mechanism%20to%0Aseamlessly%20integrate%20results%20from%20multiple%20sources.%20A%20Llama-based%0Aself-evaluator%20ensures%20the%20contextual%20accuracy%20and%20confidence%20of%20the%20generated%0Aresponses%20before%20delivering%20them.%20This%20iterative%20cycle%20of%20retrieval%20and%0Avalidation%20enhances%20precision%2C%20diversity%2C%20and%20reliability%20in%20response%0Ageneration.%20Preliminary%20evaluations%20on%20large%20enterprise%20datasets%20demonstrate%0Athe%20framework%27s%20efficacy%20in%20improving%20troubleshooting%20accuracy%2C%20reducing%0Aresolution%20times%2C%20and%20adapting%20to%20varied%20technical%20challenges.%20Future%20research%0Aaims%20to%20enhance%20the%20framework%20by%20integrating%20advanced%20conversational%20AI%0Acapabilities%2C%20enabling%20more%20interactive%20and%20intuitive%20troubleshooting%0Aexperiences.%20Efforts%20will%20also%20focus%20on%20refining%20the%20dynamic%20weighting%0Amechanism%20through%20reinforcement%20learning%20to%20further%20optimize%20the%20relevance%20and%0Aprecision%20of%20retrieved%20information.%20By%20incorporating%20these%20advancements%2C%20the%0Aproposed%20framework%20is%20poised%20to%20evolve%20into%20a%20comprehensive%2C%20autonomous%20AI%0Asolution%2C%20redefining%20technical%20service%20workflows%20across%20enterprise%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12006v2&entry.124074799=Read"},
{"title": "BiGR: Harnessing Binary Latent Codes for Image Generation and Improved\n  Visual Representation Capabilities", "author": "Shaozhe Hao and Xuantong Liu and Xianbiao Qi and Shihao Zhao and Bojia Zi and Rong Xiao and Kai Han and Kwan-Yee K. Wong", "abstract": "  We introduce BiGR, a novel conditional image generation model using compact\nbinary latent codes for generative training, focusing on enhancing both\ngeneration and representation capabilities. BiGR is the first conditional\ngenerative model that unifies generation and discrimination within the same\nframework. BiGR features a binary tokenizer, a masked modeling mechanism, and a\nbinary transcoder for binary code prediction. Additionally, we introduce a\nnovel entropy-ordered sampling method to enable efficient image generation.\nExtensive experiments validate BiGR's superior performance in generation\nquality, as measured by FID-50k, and representation capabilities, as evidenced\nby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization\nacross various vision tasks, enabling applications such as image inpainting,\noutpainting, editing, interpolation, and enrichment, without the need for\nstructural modifications. Our findings suggest that BiGR unifies generative and\ndiscriminative tasks effectively, paving the way for further advancements in\nthe field. We further enable BiGR to perform text-to-image generation,\nshowcasing its potential for broader applications.\n", "link": "http://arxiv.org/abs/2410.14672v2", "date": "2024-12-17", "relevancy": 1.7515, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5987}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5827}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiGR%3A%20Harnessing%20Binary%20Latent%20Codes%20for%20Image%20Generation%20and%20Improved%0A%20%20Visual%20Representation%20Capabilities&body=Title%3A%20BiGR%3A%20Harnessing%20Binary%20Latent%20Codes%20for%20Image%20Generation%20and%20Improved%0A%20%20Visual%20Representation%20Capabilities%0AAuthor%3A%20Shaozhe%20Hao%20and%20Xuantong%20Liu%20and%20Xianbiao%20Qi%20and%20Shihao%20Zhao%20and%20Bojia%20Zi%20and%20Rong%20Xiao%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong%0AAbstract%3A%20%20%20We%20introduce%20BiGR%2C%20a%20novel%20conditional%20image%20generation%20model%20using%20compact%0Abinary%20latent%20codes%20for%20generative%20training%2C%20focusing%20on%20enhancing%20both%0Ageneration%20and%20representation%20capabilities.%20BiGR%20is%20the%20first%20conditional%0Agenerative%20model%20that%20unifies%20generation%20and%20discrimination%20within%20the%20same%0Aframework.%20BiGR%20features%20a%20binary%20tokenizer%2C%20a%20masked%20modeling%20mechanism%2C%20and%20a%0Abinary%20transcoder%20for%20binary%20code%20prediction.%20Additionally%2C%20we%20introduce%20a%0Anovel%20entropy-ordered%20sampling%20method%20to%20enable%20efficient%20image%20generation.%0AExtensive%20experiments%20validate%20BiGR%27s%20superior%20performance%20in%20generation%0Aquality%2C%20as%20measured%20by%20FID-50k%2C%20and%20representation%20capabilities%2C%20as%20evidenced%0Aby%20linear-probe%20accuracy.%20Moreover%2C%20BiGR%20showcases%20zero-shot%20generalization%0Aacross%20various%20vision%20tasks%2C%20enabling%20applications%20such%20as%20image%20inpainting%2C%0Aoutpainting%2C%20editing%2C%20interpolation%2C%20and%20enrichment%2C%20without%20the%20need%20for%0Astructural%20modifications.%20Our%20findings%20suggest%20that%20BiGR%20unifies%20generative%20and%0Adiscriminative%20tasks%20effectively%2C%20paving%20the%20way%20for%20further%20advancements%20in%0Athe%20field.%20We%20further%20enable%20BiGR%20to%20perform%20text-to-image%20generation%2C%0Ashowcasing%20its%20potential%20for%20broader%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14672v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiGR%253A%2520Harnessing%2520Binary%2520Latent%2520Codes%2520for%2520Image%2520Generation%2520and%2520Improved%250A%2520%2520Visual%2520Representation%2520Capabilities%26entry.906535625%3DShaozhe%2520Hao%2520and%2520Xuantong%2520Liu%2520and%2520Xianbiao%2520Qi%2520and%2520Shihao%2520Zhao%2520and%2520Bojia%2520Zi%2520and%2520Rong%2520Xiao%2520and%2520Kai%2520Han%2520and%2520Kwan-Yee%2520K.%2520Wong%26entry.1292438233%3D%2520%2520We%2520introduce%2520BiGR%252C%2520a%2520novel%2520conditional%2520image%2520generation%2520model%2520using%2520compact%250Abinary%2520latent%2520codes%2520for%2520generative%2520training%252C%2520focusing%2520on%2520enhancing%2520both%250Ageneration%2520and%2520representation%2520capabilities.%2520BiGR%2520is%2520the%2520first%2520conditional%250Agenerative%2520model%2520that%2520unifies%2520generation%2520and%2520discrimination%2520within%2520the%2520same%250Aframework.%2520BiGR%2520features%2520a%2520binary%2520tokenizer%252C%2520a%2520masked%2520modeling%2520mechanism%252C%2520and%2520a%250Abinary%2520transcoder%2520for%2520binary%2520code%2520prediction.%2520Additionally%252C%2520we%2520introduce%2520a%250Anovel%2520entropy-ordered%2520sampling%2520method%2520to%2520enable%2520efficient%2520image%2520generation.%250AExtensive%2520experiments%2520validate%2520BiGR%2527s%2520superior%2520performance%2520in%2520generation%250Aquality%252C%2520as%2520measured%2520by%2520FID-50k%252C%2520and%2520representation%2520capabilities%252C%2520as%2520evidenced%250Aby%2520linear-probe%2520accuracy.%2520Moreover%252C%2520BiGR%2520showcases%2520zero-shot%2520generalization%250Aacross%2520various%2520vision%2520tasks%252C%2520enabling%2520applications%2520such%2520as%2520image%2520inpainting%252C%250Aoutpainting%252C%2520editing%252C%2520interpolation%252C%2520and%2520enrichment%252C%2520without%2520the%2520need%2520for%250Astructural%2520modifications.%2520Our%2520findings%2520suggest%2520that%2520BiGR%2520unifies%2520generative%2520and%250Adiscriminative%2520tasks%2520effectively%252C%2520paving%2520the%2520way%2520for%2520further%2520advancements%2520in%250Athe%2520field.%2520We%2520further%2520enable%2520BiGR%2520to%2520perform%2520text-to-image%2520generation%252C%250Ashowcasing%2520its%2520potential%2520for%2520broader%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14672v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiGR%3A%20Harnessing%20Binary%20Latent%20Codes%20for%20Image%20Generation%20and%20Improved%0A%20%20Visual%20Representation%20Capabilities&entry.906535625=Shaozhe%20Hao%20and%20Xuantong%20Liu%20and%20Xianbiao%20Qi%20and%20Shihao%20Zhao%20and%20Bojia%20Zi%20and%20Rong%20Xiao%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong&entry.1292438233=%20%20We%20introduce%20BiGR%2C%20a%20novel%20conditional%20image%20generation%20model%20using%20compact%0Abinary%20latent%20codes%20for%20generative%20training%2C%20focusing%20on%20enhancing%20both%0Ageneration%20and%20representation%20capabilities.%20BiGR%20is%20the%20first%20conditional%0Agenerative%20model%20that%20unifies%20generation%20and%20discrimination%20within%20the%20same%0Aframework.%20BiGR%20features%20a%20binary%20tokenizer%2C%20a%20masked%20modeling%20mechanism%2C%20and%20a%0Abinary%20transcoder%20for%20binary%20code%20prediction.%20Additionally%2C%20we%20introduce%20a%0Anovel%20entropy-ordered%20sampling%20method%20to%20enable%20efficient%20image%20generation.%0AExtensive%20experiments%20validate%20BiGR%27s%20superior%20performance%20in%20generation%0Aquality%2C%20as%20measured%20by%20FID-50k%2C%20and%20representation%20capabilities%2C%20as%20evidenced%0Aby%20linear-probe%20accuracy.%20Moreover%2C%20BiGR%20showcases%20zero-shot%20generalization%0Aacross%20various%20vision%20tasks%2C%20enabling%20applications%20such%20as%20image%20inpainting%2C%0Aoutpainting%2C%20editing%2C%20interpolation%2C%20and%20enrichment%2C%20without%20the%20need%20for%0Astructural%20modifications.%20Our%20findings%20suggest%20that%20BiGR%20unifies%20generative%20and%0Adiscriminative%20tasks%20effectively%2C%20paving%20the%20way%20for%20further%20advancements%20in%0Athe%20field.%20We%20further%20enable%20BiGR%20to%20perform%20text-to-image%20generation%2C%0Ashowcasing%20its%20potential%20for%20broader%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14672v2&entry.124074799=Read"},
{"title": "CondiMen: Conditional Multi-Person Mesh Recovery", "author": "Br\u00e9gier Romain and Baradel Fabien and Lucas Thomas and Galaaoui Salma and Armando Matthieu and Weinzaepfel Philippe and Rogez Gr\u00e9gory", "abstract": "  Multi-person human mesh recovery (HMR) consists in detecting all individuals\nin a given input image, and predicting the body shape, pose, and 3D location\nfor each detected person. The dominant approaches to this task rely on neural\nnetworks trained to output a single prediction for each detected individual. In\ncontrast, we propose CondiMen, a method that outputs a joint parametric\ndistribution over likely poses, body shapes, intrinsics and distances to the\ncamera, using a Bayesian network. This approach offers several advantages.\nFirst, a probability distribution can handle some inherent ambiguities of this\ntask -- such as the uncertainty between a person's size and their distance to\nthe camera, or simply the loss of information when projecting 3D data onto the\n2D image plane. Second, the output distribution can be combined with additional\ninformation to produce better predictions, by using e.g. known camera or body\nshape parameters, or by exploiting multi-view observations. Third, one can\nefficiently extract the most likely predictions from the output distribution,\nmaking our proposed approach suitable for real-time applications. Empirically\nwe find that our model i) achieves performance on par with or better than the\nstate-of-the-art, ii) captures uncertainties and correlations inherent in pose\nestimation and iii) can exploit additional information at test time, such as\nmulti-view consistency or body shape priors. CondiMen spices up the modeling of\nambiguity, using just the right ingredients on hand.\n", "link": "http://arxiv.org/abs/2412.13058v1", "date": "2024-12-17", "relevancy": 1.7284, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5862}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5684}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CondiMen%3A%20Conditional%20Multi-Person%20Mesh%20Recovery&body=Title%3A%20CondiMen%3A%20Conditional%20Multi-Person%20Mesh%20Recovery%0AAuthor%3A%20Br%C3%A9gier%20Romain%20and%20Baradel%20Fabien%20and%20Lucas%20Thomas%20and%20Galaaoui%20Salma%20and%20Armando%20Matthieu%20and%20Weinzaepfel%20Philippe%20and%20Rogez%20Gr%C3%A9gory%0AAbstract%3A%20%20%20Multi-person%20human%20mesh%20recovery%20%28HMR%29%20consists%20in%20detecting%20all%20individuals%0Ain%20a%20given%20input%20image%2C%20and%20predicting%20the%20body%20shape%2C%20pose%2C%20and%203D%20location%0Afor%20each%20detected%20person.%20The%20dominant%20approaches%20to%20this%20task%20rely%20on%20neural%0Anetworks%20trained%20to%20output%20a%20single%20prediction%20for%20each%20detected%20individual.%20In%0Acontrast%2C%20we%20propose%20CondiMen%2C%20a%20method%20that%20outputs%20a%20joint%20parametric%0Adistribution%20over%20likely%20poses%2C%20body%20shapes%2C%20intrinsics%20and%20distances%20to%20the%0Acamera%2C%20using%20a%20Bayesian%20network.%20This%20approach%20offers%20several%20advantages.%0AFirst%2C%20a%20probability%20distribution%20can%20handle%20some%20inherent%20ambiguities%20of%20this%0Atask%20--%20such%20as%20the%20uncertainty%20between%20a%20person%27s%20size%20and%20their%20distance%20to%0Athe%20camera%2C%20or%20simply%20the%20loss%20of%20information%20when%20projecting%203D%20data%20onto%20the%0A2D%20image%20plane.%20Second%2C%20the%20output%20distribution%20can%20be%20combined%20with%20additional%0Ainformation%20to%20produce%20better%20predictions%2C%20by%20using%20e.g.%20known%20camera%20or%20body%0Ashape%20parameters%2C%20or%20by%20exploiting%20multi-view%20observations.%20Third%2C%20one%20can%0Aefficiently%20extract%20the%20most%20likely%20predictions%20from%20the%20output%20distribution%2C%0Amaking%20our%20proposed%20approach%20suitable%20for%20real-time%20applications.%20Empirically%0Awe%20find%20that%20our%20model%20i%29%20achieves%20performance%20on%20par%20with%20or%20better%20than%20the%0Astate-of-the-art%2C%20ii%29%20captures%20uncertainties%20and%20correlations%20inherent%20in%20pose%0Aestimation%20and%20iii%29%20can%20exploit%20additional%20information%20at%20test%20time%2C%20such%20as%0Amulti-view%20consistency%20or%20body%20shape%20priors.%20CondiMen%20spices%20up%20the%20modeling%20of%0Aambiguity%2C%20using%20just%20the%20right%20ingredients%20on%20hand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCondiMen%253A%2520Conditional%2520Multi-Person%2520Mesh%2520Recovery%26entry.906535625%3DBr%25C3%25A9gier%2520Romain%2520and%2520Baradel%2520Fabien%2520and%2520Lucas%2520Thomas%2520and%2520Galaaoui%2520Salma%2520and%2520Armando%2520Matthieu%2520and%2520Weinzaepfel%2520Philippe%2520and%2520Rogez%2520Gr%25C3%25A9gory%26entry.1292438233%3D%2520%2520Multi-person%2520human%2520mesh%2520recovery%2520%2528HMR%2529%2520consists%2520in%2520detecting%2520all%2520individuals%250Ain%2520a%2520given%2520input%2520image%252C%2520and%2520predicting%2520the%2520body%2520shape%252C%2520pose%252C%2520and%25203D%2520location%250Afor%2520each%2520detected%2520person.%2520The%2520dominant%2520approaches%2520to%2520this%2520task%2520rely%2520on%2520neural%250Anetworks%2520trained%2520to%2520output%2520a%2520single%2520prediction%2520for%2520each%2520detected%2520individual.%2520In%250Acontrast%252C%2520we%2520propose%2520CondiMen%252C%2520a%2520method%2520that%2520outputs%2520a%2520joint%2520parametric%250Adistribution%2520over%2520likely%2520poses%252C%2520body%2520shapes%252C%2520intrinsics%2520and%2520distances%2520to%2520the%250Acamera%252C%2520using%2520a%2520Bayesian%2520network.%2520This%2520approach%2520offers%2520several%2520advantages.%250AFirst%252C%2520a%2520probability%2520distribution%2520can%2520handle%2520some%2520inherent%2520ambiguities%2520of%2520this%250Atask%2520--%2520such%2520as%2520the%2520uncertainty%2520between%2520a%2520person%2527s%2520size%2520and%2520their%2520distance%2520to%250Athe%2520camera%252C%2520or%2520simply%2520the%2520loss%2520of%2520information%2520when%2520projecting%25203D%2520data%2520onto%2520the%250A2D%2520image%2520plane.%2520Second%252C%2520the%2520output%2520distribution%2520can%2520be%2520combined%2520with%2520additional%250Ainformation%2520to%2520produce%2520better%2520predictions%252C%2520by%2520using%2520e.g.%2520known%2520camera%2520or%2520body%250Ashape%2520parameters%252C%2520or%2520by%2520exploiting%2520multi-view%2520observations.%2520Third%252C%2520one%2520can%250Aefficiently%2520extract%2520the%2520most%2520likely%2520predictions%2520from%2520the%2520output%2520distribution%252C%250Amaking%2520our%2520proposed%2520approach%2520suitable%2520for%2520real-time%2520applications.%2520Empirically%250Awe%2520find%2520that%2520our%2520model%2520i%2529%2520achieves%2520performance%2520on%2520par%2520with%2520or%2520better%2520than%2520the%250Astate-of-the-art%252C%2520ii%2529%2520captures%2520uncertainties%2520and%2520correlations%2520inherent%2520in%2520pose%250Aestimation%2520and%2520iii%2529%2520can%2520exploit%2520additional%2520information%2520at%2520test%2520time%252C%2520such%2520as%250Amulti-view%2520consistency%2520or%2520body%2520shape%2520priors.%2520CondiMen%2520spices%2520up%2520the%2520modeling%2520of%250Aambiguity%252C%2520using%2520just%2520the%2520right%2520ingredients%2520on%2520hand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CondiMen%3A%20Conditional%20Multi-Person%20Mesh%20Recovery&entry.906535625=Br%C3%A9gier%20Romain%20and%20Baradel%20Fabien%20and%20Lucas%20Thomas%20and%20Galaaoui%20Salma%20and%20Armando%20Matthieu%20and%20Weinzaepfel%20Philippe%20and%20Rogez%20Gr%C3%A9gory&entry.1292438233=%20%20Multi-person%20human%20mesh%20recovery%20%28HMR%29%20consists%20in%20detecting%20all%20individuals%0Ain%20a%20given%20input%20image%2C%20and%20predicting%20the%20body%20shape%2C%20pose%2C%20and%203D%20location%0Afor%20each%20detected%20person.%20The%20dominant%20approaches%20to%20this%20task%20rely%20on%20neural%0Anetworks%20trained%20to%20output%20a%20single%20prediction%20for%20each%20detected%20individual.%20In%0Acontrast%2C%20we%20propose%20CondiMen%2C%20a%20method%20that%20outputs%20a%20joint%20parametric%0Adistribution%20over%20likely%20poses%2C%20body%20shapes%2C%20intrinsics%20and%20distances%20to%20the%0Acamera%2C%20using%20a%20Bayesian%20network.%20This%20approach%20offers%20several%20advantages.%0AFirst%2C%20a%20probability%20distribution%20can%20handle%20some%20inherent%20ambiguities%20of%20this%0Atask%20--%20such%20as%20the%20uncertainty%20between%20a%20person%27s%20size%20and%20their%20distance%20to%0Athe%20camera%2C%20or%20simply%20the%20loss%20of%20information%20when%20projecting%203D%20data%20onto%20the%0A2D%20image%20plane.%20Second%2C%20the%20output%20distribution%20can%20be%20combined%20with%20additional%0Ainformation%20to%20produce%20better%20predictions%2C%20by%20using%20e.g.%20known%20camera%20or%20body%0Ashape%20parameters%2C%20or%20by%20exploiting%20multi-view%20observations.%20Third%2C%20one%20can%0Aefficiently%20extract%20the%20most%20likely%20predictions%20from%20the%20output%20distribution%2C%0Amaking%20our%20proposed%20approach%20suitable%20for%20real-time%20applications.%20Empirically%0Awe%20find%20that%20our%20model%20i%29%20achieves%20performance%20on%20par%20with%20or%20better%20than%20the%0Astate-of-the-art%2C%20ii%29%20captures%20uncertainties%20and%20correlations%20inherent%20in%20pose%0Aestimation%20and%20iii%29%20can%20exploit%20additional%20information%20at%20test%20time%2C%20such%20as%0Amulti-view%20consistency%20or%20body%20shape%20priors.%20CondiMen%20spices%20up%20the%20modeling%20of%0Aambiguity%2C%20using%20just%20the%20right%20ingredients%20on%20hand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13058v1&entry.124074799=Read"},
{"title": "Modality-Inconsistent Continual Learning of Multimodal Large Language\n  Models", "author": "Weiguo Pian and Shijian Deng and Shentong Mo and Yunhui Guo and Yapeng Tian", "abstract": "  In this paper, we introduce Modality-Inconsistent Continual Learning (MICL),\na new continual learning scenario for Multimodal Large Language Models (MLLMs)\nthat involves tasks with inconsistent modalities (image, audio, or video) and\nvarying task types (captioning or question-answering). Unlike existing\nvision-only or modality-incremental settings, MICL combines modality and task\ntype shifts, both of which drive catastrophic forgetting. To address these\nchallenges, we propose MoInCL, which employs a Pseudo Targets Generation Module\nto mitigate forgetting caused by task type shifts in previously seen\nmodalities. It also incorporates Instruction-based Knowledge Distillation to\npreserve the model's ability to handle previously learned modalities when new\nones are introduced. We benchmark MICL using a total of six tasks and conduct\nexperiments to validate the effectiveness of our proposed MoInCL. The\nexperimental results highlight the superiority of MoInCL, showing significant\nimprovements over representative and state-of-the-art continual learning\nbaselines.\n", "link": "http://arxiv.org/abs/2412.13050v1", "date": "2024-12-17", "relevancy": 1.7077, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5975}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-Inconsistent%20Continual%20Learning%20of%20Multimodal%20Large%20Language%0A%20%20Models&body=Title%3A%20Modality-Inconsistent%20Continual%20Learning%20of%20Multimodal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Weiguo%20Pian%20and%20Shijian%20Deng%20and%20Shentong%20Mo%20and%20Yunhui%20Guo%20and%20Yapeng%20Tian%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Modality-Inconsistent%20Continual%20Learning%20%28MICL%29%2C%0Aa%20new%20continual%20learning%20scenario%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Athat%20involves%20tasks%20with%20inconsistent%20modalities%20%28image%2C%20audio%2C%20or%20video%29%20and%0Avarying%20task%20types%20%28captioning%20or%20question-answering%29.%20Unlike%20existing%0Avision-only%20or%20modality-incremental%20settings%2C%20MICL%20combines%20modality%20and%20task%0Atype%20shifts%2C%20both%20of%20which%20drive%20catastrophic%20forgetting.%20To%20address%20these%0Achallenges%2C%20we%20propose%20MoInCL%2C%20which%20employs%20a%20Pseudo%20Targets%20Generation%20Module%0Ato%20mitigate%20forgetting%20caused%20by%20task%20type%20shifts%20in%20previously%20seen%0Amodalities.%20It%20also%20incorporates%20Instruction-based%20Knowledge%20Distillation%20to%0Apreserve%20the%20model%27s%20ability%20to%20handle%20previously%20learned%20modalities%20when%20new%0Aones%20are%20introduced.%20We%20benchmark%20MICL%20using%20a%20total%20of%20six%20tasks%20and%20conduct%0Aexperiments%20to%20validate%20the%20effectiveness%20of%20our%20proposed%20MoInCL.%20The%0Aexperimental%20results%20highlight%20the%20superiority%20of%20MoInCL%2C%20showing%20significant%0Aimprovements%20over%20representative%20and%20state-of-the-art%20continual%20learning%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-Inconsistent%2520Continual%2520Learning%2520of%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DWeiguo%2520Pian%2520and%2520Shijian%2520Deng%2520and%2520Shentong%2520Mo%2520and%2520Yunhui%2520Guo%2520and%2520Yapeng%2520Tian%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Modality-Inconsistent%2520Continual%2520Learning%2520%2528MICL%2529%252C%250Aa%2520new%2520continual%2520learning%2520scenario%2520for%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%250Athat%2520involves%2520tasks%2520with%2520inconsistent%2520modalities%2520%2528image%252C%2520audio%252C%2520or%2520video%2529%2520and%250Avarying%2520task%2520types%2520%2528captioning%2520or%2520question-answering%2529.%2520Unlike%2520existing%250Avision-only%2520or%2520modality-incremental%2520settings%252C%2520MICL%2520combines%2520modality%2520and%2520task%250Atype%2520shifts%252C%2520both%2520of%2520which%2520drive%2520catastrophic%2520forgetting.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520MoInCL%252C%2520which%2520employs%2520a%2520Pseudo%2520Targets%2520Generation%2520Module%250Ato%2520mitigate%2520forgetting%2520caused%2520by%2520task%2520type%2520shifts%2520in%2520previously%2520seen%250Amodalities.%2520It%2520also%2520incorporates%2520Instruction-based%2520Knowledge%2520Distillation%2520to%250Apreserve%2520the%2520model%2527s%2520ability%2520to%2520handle%2520previously%2520learned%2520modalities%2520when%2520new%250Aones%2520are%2520introduced.%2520We%2520benchmark%2520MICL%2520using%2520a%2520total%2520of%2520six%2520tasks%2520and%2520conduct%250Aexperiments%2520to%2520validate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520MoInCL.%2520The%250Aexperimental%2520results%2520highlight%2520the%2520superiority%2520of%2520MoInCL%252C%2520showing%2520significant%250Aimprovements%2520over%2520representative%2520and%2520state-of-the-art%2520continual%2520learning%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-Inconsistent%20Continual%20Learning%20of%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Weiguo%20Pian%20and%20Shijian%20Deng%20and%20Shentong%20Mo%20and%20Yunhui%20Guo%20and%20Yapeng%20Tian&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Modality-Inconsistent%20Continual%20Learning%20%28MICL%29%2C%0Aa%20new%20continual%20learning%20scenario%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Athat%20involves%20tasks%20with%20inconsistent%20modalities%20%28image%2C%20audio%2C%20or%20video%29%20and%0Avarying%20task%20types%20%28captioning%20or%20question-answering%29.%20Unlike%20existing%0Avision-only%20or%20modality-incremental%20settings%2C%20MICL%20combines%20modality%20and%20task%0Atype%20shifts%2C%20both%20of%20which%20drive%20catastrophic%20forgetting.%20To%20address%20these%0Achallenges%2C%20we%20propose%20MoInCL%2C%20which%20employs%20a%20Pseudo%20Targets%20Generation%20Module%0Ato%20mitigate%20forgetting%20caused%20by%20task%20type%20shifts%20in%20previously%20seen%0Amodalities.%20It%20also%20incorporates%20Instruction-based%20Knowledge%20Distillation%20to%0Apreserve%20the%20model%27s%20ability%20to%20handle%20previously%20learned%20modalities%20when%20new%0Aones%20are%20introduced.%20We%20benchmark%20MICL%20using%20a%20total%20of%20six%20tasks%20and%20conduct%0Aexperiments%20to%20validate%20the%20effectiveness%20of%20our%20proposed%20MoInCL.%20The%0Aexperimental%20results%20highlight%20the%20superiority%20of%20MoInCL%2C%20showing%20significant%0Aimprovements%20over%20representative%20and%20state-of-the-art%20continual%20learning%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13050v1&entry.124074799=Read"},
{"title": "ExBody2: Advanced Expressive Humanoid Whole-Body Control", "author": "Mazeyu Ji and Xuanbin Peng and Fangchen Liu and Jialong Li and Ge Yang and Xuxin Cheng and Xiaolong Wang", "abstract": "  This paper enables real-world humanoid robots to maintain stability while\nperforming expressive motions like humans do. We propose ExBody2, a generalized\nwhole-body tracking framework that can take any reference motion inputs and\ncontrol the humanoid to mimic the motion. The model is trained in simulation\nwith Reinforcement Learning and then transferred to the real world. It\ndecouples keypoint tracking with velocity control, and effectively leverages a\nprivileged teacher policy to distill precise mimic skills into the target\nstudent policy, which enables high-fidelity replication of dynamic movements\nsuch as running, crouching, dancing, and other challenging motions. We present\na comprehensive qualitative and quantitative analysis of crucial design factors\nin the paper. We conduct our experiments on two humanoid platforms and\ndemonstrate the superiority of our approach against state-of-the-arts,\nproviding practical guidelines to pursue the extreme of whole-body control for\nhumanoid robots.\n", "link": "http://arxiv.org/abs/2412.13196v1", "date": "2024-12-17", "relevancy": 1.6996, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5881}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5669}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExBody2%3A%20Advanced%20Expressive%20Humanoid%20Whole-Body%20Control&body=Title%3A%20ExBody2%3A%20Advanced%20Expressive%20Humanoid%20Whole-Body%20Control%0AAuthor%3A%20Mazeyu%20Ji%20and%20Xuanbin%20Peng%20and%20Fangchen%20Liu%20and%20Jialong%20Li%20and%20Ge%20Yang%20and%20Xuxin%20Cheng%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20This%20paper%20enables%20real-world%20humanoid%20robots%20to%20maintain%20stability%20while%0Aperforming%20expressive%20motions%20like%20humans%20do.%20We%20propose%20ExBody2%2C%20a%20generalized%0Awhole-body%20tracking%20framework%20that%20can%20take%20any%20reference%20motion%20inputs%20and%0Acontrol%20the%20humanoid%20to%20mimic%20the%20motion.%20The%20model%20is%20trained%20in%20simulation%0Awith%20Reinforcement%20Learning%20and%20then%20transferred%20to%20the%20real%20world.%20It%0Adecouples%20keypoint%20tracking%20with%20velocity%20control%2C%20and%20effectively%20leverages%20a%0Aprivileged%20teacher%20policy%20to%20distill%20precise%20mimic%20skills%20into%20the%20target%0Astudent%20policy%2C%20which%20enables%20high-fidelity%20replication%20of%20dynamic%20movements%0Asuch%20as%20running%2C%20crouching%2C%20dancing%2C%20and%20other%20challenging%20motions.%20We%20present%0Aa%20comprehensive%20qualitative%20and%20quantitative%20analysis%20of%20crucial%20design%20factors%0Ain%20the%20paper.%20We%20conduct%20our%20experiments%20on%20two%20humanoid%20platforms%20and%0Ademonstrate%20the%20superiority%20of%20our%20approach%20against%20state-of-the-arts%2C%0Aproviding%20practical%20guidelines%20to%20pursue%20the%20extreme%20of%20whole-body%20control%20for%0Ahumanoid%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExBody2%253A%2520Advanced%2520Expressive%2520Humanoid%2520Whole-Body%2520Control%26entry.906535625%3DMazeyu%2520Ji%2520and%2520Xuanbin%2520Peng%2520and%2520Fangchen%2520Liu%2520and%2520Jialong%2520Li%2520and%2520Ge%2520Yang%2520and%2520Xuxin%2520Cheng%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520enables%2520real-world%2520humanoid%2520robots%2520to%2520maintain%2520stability%2520while%250Aperforming%2520expressive%2520motions%2520like%2520humans%2520do.%2520We%2520propose%2520ExBody2%252C%2520a%2520generalized%250Awhole-body%2520tracking%2520framework%2520that%2520can%2520take%2520any%2520reference%2520motion%2520inputs%2520and%250Acontrol%2520the%2520humanoid%2520to%2520mimic%2520the%2520motion.%2520The%2520model%2520is%2520trained%2520in%2520simulation%250Awith%2520Reinforcement%2520Learning%2520and%2520then%2520transferred%2520to%2520the%2520real%2520world.%2520It%250Adecouples%2520keypoint%2520tracking%2520with%2520velocity%2520control%252C%2520and%2520effectively%2520leverages%2520a%250Aprivileged%2520teacher%2520policy%2520to%2520distill%2520precise%2520mimic%2520skills%2520into%2520the%2520target%250Astudent%2520policy%252C%2520which%2520enables%2520high-fidelity%2520replication%2520of%2520dynamic%2520movements%250Asuch%2520as%2520running%252C%2520crouching%252C%2520dancing%252C%2520and%2520other%2520challenging%2520motions.%2520We%2520present%250Aa%2520comprehensive%2520qualitative%2520and%2520quantitative%2520analysis%2520of%2520crucial%2520design%2520factors%250Ain%2520the%2520paper.%2520We%2520conduct%2520our%2520experiments%2520on%2520two%2520humanoid%2520platforms%2520and%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520against%2520state-of-the-arts%252C%250Aproviding%2520practical%2520guidelines%2520to%2520pursue%2520the%2520extreme%2520of%2520whole-body%2520control%2520for%250Ahumanoid%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExBody2%3A%20Advanced%20Expressive%20Humanoid%20Whole-Body%20Control&entry.906535625=Mazeyu%20Ji%20and%20Xuanbin%20Peng%20and%20Fangchen%20Liu%20and%20Jialong%20Li%20and%20Ge%20Yang%20and%20Xuxin%20Cheng%20and%20Xiaolong%20Wang&entry.1292438233=%20%20This%20paper%20enables%20real-world%20humanoid%20robots%20to%20maintain%20stability%20while%0Aperforming%20expressive%20motions%20like%20humans%20do.%20We%20propose%20ExBody2%2C%20a%20generalized%0Awhole-body%20tracking%20framework%20that%20can%20take%20any%20reference%20motion%20inputs%20and%0Acontrol%20the%20humanoid%20to%20mimic%20the%20motion.%20The%20model%20is%20trained%20in%20simulation%0Awith%20Reinforcement%20Learning%20and%20then%20transferred%20to%20the%20real%20world.%20It%0Adecouples%20keypoint%20tracking%20with%20velocity%20control%2C%20and%20effectively%20leverages%20a%0Aprivileged%20teacher%20policy%20to%20distill%20precise%20mimic%20skills%20into%20the%20target%0Astudent%20policy%2C%20which%20enables%20high-fidelity%20replication%20of%20dynamic%20movements%0Asuch%20as%20running%2C%20crouching%2C%20dancing%2C%20and%20other%20challenging%20motions.%20We%20present%0Aa%20comprehensive%20qualitative%20and%20quantitative%20analysis%20of%20crucial%20design%20factors%0Ain%20the%20paper.%20We%20conduct%20our%20experiments%20on%20two%20humanoid%20platforms%20and%0Ademonstrate%20the%20superiority%20of%20our%20approach%20against%20state-of-the-arts%2C%0Aproviding%20practical%20guidelines%20to%20pursue%20the%20extreme%20of%20whole-body%20control%20for%0Ahumanoid%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13196v1&entry.124074799=Read"},
{"title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents", "author": "Sheng Yin and Xianghe Pang and Yuanzhuo Ding and Menglan Chen and Yutong Bi and Yichen Xiong and Wenhao Huang and Zhen Xiang and Jing Shao and Siheng Chen", "abstract": "  With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.\n", "link": "http://arxiv.org/abs/2412.13178v1", "date": "2024-12-17", "relevancy": 1.6983, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5686}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5661}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeAgentBench%3A%20A%20Benchmark%20for%20Safe%20Task%20Planning%20of%20Embodied%20LLM%0A%20%20Agents&body=Title%3A%20SafeAgentBench%3A%20A%20Benchmark%20for%20Safe%20Task%20Planning%20of%20Embodied%20LLM%0A%20%20Agents%0AAuthor%3A%20Sheng%20Yin%20and%20Xianghe%20Pang%20and%20Yuanzhuo%20Ding%20and%20Menglan%20Chen%20and%20Yutong%20Bi%20and%20Yichen%20Xiong%20and%20Wenhao%20Huang%20and%20Zhen%20Xiang%20and%20Jing%20Shao%20and%20Siheng%20Chen%0AAbstract%3A%20%20%20With%20the%20integration%20of%20large%20language%20models%20%28LLMs%29%2C%20embodied%20agents%20have%0Astrong%20capabilities%20to%20execute%20complicated%20instructions%20in%20natural%20language%2C%0Apaving%20a%20way%20for%20the%20potential%20deployment%20of%20embodied%20robots.%20However%2C%20a%0Aforeseeable%20issue%20is%20that%20those%20embodied%20agents%20can%20also%20flawlessly%20execute%0Asome%20hazardous%20tasks%2C%20potentially%20causing%20damages%20in%20real%20world.%20To%20study%20this%0Aissue%2C%20we%20present%20SafeAgentBench%20--%20a%20new%20benchmark%20for%20safety-aware%20task%0Aplanning%20of%20embodied%20LLM%20agents.%20SafeAgentBench%20includes%3A%20%281%29%20a%20new%20dataset%0Awith%20750%20tasks%2C%20covering%2010%20potential%20hazards%20and%203%20task%20types%3B%20%282%29%0ASafeAgentEnv%2C%20a%20universal%20embodied%20environment%20with%20a%20low-level%20controller%2C%0Asupporting%20multi-agent%20execution%20with%2017%20high-level%20actions%20for%208%0Astate-of-the-art%20baselines%3B%20and%20%283%29%20reliable%20evaluation%20methods%20from%20both%0Aexecution%20and%20semantic%20perspectives.%20Experimental%20results%20show%20that%20the%0Abest-performing%20baseline%20gets%2069%25%20success%20rate%20for%20safe%20tasks%2C%20but%20only%205%25%0Arejection%20rate%20for%20hazardous%20tasks%2C%20indicating%20significant%20safety%20risks.%20More%0Adetails%20and%20codes%20are%20available%20at%0Ahttps%3A//github.com/shengyin1224/SafeAgentBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeAgentBench%253A%2520A%2520Benchmark%2520for%2520Safe%2520Task%2520Planning%2520of%2520Embodied%2520LLM%250A%2520%2520Agents%26entry.906535625%3DSheng%2520Yin%2520and%2520Xianghe%2520Pang%2520and%2520Yuanzhuo%2520Ding%2520and%2520Menglan%2520Chen%2520and%2520Yutong%2520Bi%2520and%2520Yichen%2520Xiong%2520and%2520Wenhao%2520Huang%2520and%2520Zhen%2520Xiang%2520and%2520Jing%2520Shao%2520and%2520Siheng%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520integration%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520embodied%2520agents%2520have%250Astrong%2520capabilities%2520to%2520execute%2520complicated%2520instructions%2520in%2520natural%2520language%252C%250Apaving%2520a%2520way%2520for%2520the%2520potential%2520deployment%2520of%2520embodied%2520robots.%2520However%252C%2520a%250Aforeseeable%2520issue%2520is%2520that%2520those%2520embodied%2520agents%2520can%2520also%2520flawlessly%2520execute%250Asome%2520hazardous%2520tasks%252C%2520potentially%2520causing%2520damages%2520in%2520real%2520world.%2520To%2520study%2520this%250Aissue%252C%2520we%2520present%2520SafeAgentBench%2520--%2520a%2520new%2520benchmark%2520for%2520safety-aware%2520task%250Aplanning%2520of%2520embodied%2520LLM%2520agents.%2520SafeAgentBench%2520includes%253A%2520%25281%2529%2520a%2520new%2520dataset%250Awith%2520750%2520tasks%252C%2520covering%252010%2520potential%2520hazards%2520and%25203%2520task%2520types%253B%2520%25282%2529%250ASafeAgentEnv%252C%2520a%2520universal%2520embodied%2520environment%2520with%2520a%2520low-level%2520controller%252C%250Asupporting%2520multi-agent%2520execution%2520with%252017%2520high-level%2520actions%2520for%25208%250Astate-of-the-art%2520baselines%253B%2520and%2520%25283%2529%2520reliable%2520evaluation%2520methods%2520from%2520both%250Aexecution%2520and%2520semantic%2520perspectives.%2520Experimental%2520results%2520show%2520that%2520the%250Abest-performing%2520baseline%2520gets%252069%2525%2520success%2520rate%2520for%2520safe%2520tasks%252C%2520but%2520only%25205%2525%250Arejection%2520rate%2520for%2520hazardous%2520tasks%252C%2520indicating%2520significant%2520safety%2520risks.%2520More%250Adetails%2520and%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/shengyin1224/SafeAgentBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeAgentBench%3A%20A%20Benchmark%20for%20Safe%20Task%20Planning%20of%20Embodied%20LLM%0A%20%20Agents&entry.906535625=Sheng%20Yin%20and%20Xianghe%20Pang%20and%20Yuanzhuo%20Ding%20and%20Menglan%20Chen%20and%20Yutong%20Bi%20and%20Yichen%20Xiong%20and%20Wenhao%20Huang%20and%20Zhen%20Xiang%20and%20Jing%20Shao%20and%20Siheng%20Chen&entry.1292438233=%20%20With%20the%20integration%20of%20large%20language%20models%20%28LLMs%29%2C%20embodied%20agents%20have%0Astrong%20capabilities%20to%20execute%20complicated%20instructions%20in%20natural%20language%2C%0Apaving%20a%20way%20for%20the%20potential%20deployment%20of%20embodied%20robots.%20However%2C%20a%0Aforeseeable%20issue%20is%20that%20those%20embodied%20agents%20can%20also%20flawlessly%20execute%0Asome%20hazardous%20tasks%2C%20potentially%20causing%20damages%20in%20real%20world.%20To%20study%20this%0Aissue%2C%20we%20present%20SafeAgentBench%20--%20a%20new%20benchmark%20for%20safety-aware%20task%0Aplanning%20of%20embodied%20LLM%20agents.%20SafeAgentBench%20includes%3A%20%281%29%20a%20new%20dataset%0Awith%20750%20tasks%2C%20covering%2010%20potential%20hazards%20and%203%20task%20types%3B%20%282%29%0ASafeAgentEnv%2C%20a%20universal%20embodied%20environment%20with%20a%20low-level%20controller%2C%0Asupporting%20multi-agent%20execution%20with%2017%20high-level%20actions%20for%208%0Astate-of-the-art%20baselines%3B%20and%20%283%29%20reliable%20evaluation%20methods%20from%20both%0Aexecution%20and%20semantic%20perspectives.%20Experimental%20results%20show%20that%20the%0Abest-performing%20baseline%20gets%2069%25%20success%20rate%20for%20safe%20tasks%2C%20but%20only%205%25%0Arejection%20rate%20for%20hazardous%20tasks%2C%20indicating%20significant%20safety%20risks.%20More%0Adetails%20and%20codes%20are%20available%20at%0Ahttps%3A//github.com/shengyin1224/SafeAgentBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13178v1&entry.124074799=Read"},
{"title": "Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation\n  Model Internet Agents", "author": "Yifei Zhou and Qianlan Yang and Kaixiang Lin and Min Bai and Xiong Zhou and Yu-Xiong Wang and Sergey Levine and Erran Li", "abstract": "  The vision of a broadly capable and goal-directed agent, such as an\nInternet-browsing agent in the digital world and a household humanoid in the\nphysical world, has rapidly advanced, thanks to the generalization capability\nof foundation models. Such a generalist agent needs to have a large and diverse\nskill repertoire, such as finding directions between two travel locations and\nbuying specific items from the Internet. If each skill needs to be specified\nmanually through a fixed set of human-annotated instructions, the agent's skill\nrepertoire will necessarily be limited due to the quantity and diversity of\nhuman-annotated instructions. In this work, we address this challenge by\nproposing Proposer-Agent-Evaluator, an effective learning system that enables\nfoundation model agents to autonomously discover and practice skills in the\nwild. At the heart of PAE is a context-aware task proposer that autonomously\nproposes tasks for the agent to practice with context information of the\nenvironment such as user demos or even just the name of the website itself for\nInternet-browsing agents. Then, the agent policy attempts those tasks with\nthoughts and actual grounded operations in the real world with resulting\ntrajectories evaluated by an autonomous VLM-based success evaluator. The\nsuccess evaluation serves as the reward signal for the agent to refine its\npolicies through RL. We validate PAE on challenging vision-based web\nnavigation, using both real-world and self-hosted websites from WebVoyager and\nWebArena.To the best of our knowledge, this work represents the first effective\nlearning system to apply autonomous task proposal with RL for agents that\ngeneralizes real-world human-annotated benchmarks with SOTA performances. Our\nopen-source checkpoints and code can be found in https://yanqval.github.io/PAE/\n", "link": "http://arxiv.org/abs/2412.13194v1", "date": "2024-12-17", "relevancy": 1.6939, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5867}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5606}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proposer-Agent-Evaluator%28PAE%29%3A%20Autonomous%20Skill%20Discovery%20For%20Foundation%0A%20%20Model%20Internet%20Agents&body=Title%3A%20Proposer-Agent-Evaluator%28PAE%29%3A%20Autonomous%20Skill%20Discovery%20For%20Foundation%0A%20%20Model%20Internet%20Agents%0AAuthor%3A%20Yifei%20Zhou%20and%20Qianlan%20Yang%20and%20Kaixiang%20Lin%20and%20Min%20Bai%20and%20Xiong%20Zhou%20and%20Yu-Xiong%20Wang%20and%20Sergey%20Levine%20and%20Erran%20Li%0AAbstract%3A%20%20%20The%20vision%20of%20a%20broadly%20capable%20and%20goal-directed%20agent%2C%20such%20as%20an%0AInternet-browsing%20agent%20in%20the%20digital%20world%20and%20a%20household%20humanoid%20in%20the%0Aphysical%20world%2C%20has%20rapidly%20advanced%2C%20thanks%20to%20the%20generalization%20capability%0Aof%20foundation%20models.%20Such%20a%20generalist%20agent%20needs%20to%20have%20a%20large%20and%20diverse%0Askill%20repertoire%2C%20such%20as%20finding%20directions%20between%20two%20travel%20locations%20and%0Abuying%20specific%20items%20from%20the%20Internet.%20If%20each%20skill%20needs%20to%20be%20specified%0Amanually%20through%20a%20fixed%20set%20of%20human-annotated%20instructions%2C%20the%20agent%27s%20skill%0Arepertoire%20will%20necessarily%20be%20limited%20due%20to%20the%20quantity%20and%20diversity%20of%0Ahuman-annotated%20instructions.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%0Aproposing%20Proposer-Agent-Evaluator%2C%20an%20effective%20learning%20system%20that%20enables%0Afoundation%20model%20agents%20to%20autonomously%20discover%20and%20practice%20skills%20in%20the%0Awild.%20At%20the%20heart%20of%20PAE%20is%20a%20context-aware%20task%20proposer%20that%20autonomously%0Aproposes%20tasks%20for%20the%20agent%20to%20practice%20with%20context%20information%20of%20the%0Aenvironment%20such%20as%20user%20demos%20or%20even%20just%20the%20name%20of%20the%20website%20itself%20for%0AInternet-browsing%20agents.%20Then%2C%20the%20agent%20policy%20attempts%20those%20tasks%20with%0Athoughts%20and%20actual%20grounded%20operations%20in%20the%20real%20world%20with%20resulting%0Atrajectories%20evaluated%20by%20an%20autonomous%20VLM-based%20success%20evaluator.%20The%0Asuccess%20evaluation%20serves%20as%20the%20reward%20signal%20for%20the%20agent%20to%20refine%20its%0Apolicies%20through%20RL.%20We%20validate%20PAE%20on%20challenging%20vision-based%20web%0Anavigation%2C%20using%20both%20real-world%20and%20self-hosted%20websites%20from%20WebVoyager%20and%0AWebArena.To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20represents%20the%20first%20effective%0Alearning%20system%20to%20apply%20autonomous%20task%20proposal%20with%20RL%20for%20agents%20that%0Ageneralizes%20real-world%20human-annotated%20benchmarks%20with%20SOTA%20performances.%20Our%0Aopen-source%20checkpoints%20and%20code%20can%20be%20found%20in%20https%3A//yanqval.github.io/PAE/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProposer-Agent-Evaluator%2528PAE%2529%253A%2520Autonomous%2520Skill%2520Discovery%2520For%2520Foundation%250A%2520%2520Model%2520Internet%2520Agents%26entry.906535625%3DYifei%2520Zhou%2520and%2520Qianlan%2520Yang%2520and%2520Kaixiang%2520Lin%2520and%2520Min%2520Bai%2520and%2520Xiong%2520Zhou%2520and%2520Yu-Xiong%2520Wang%2520and%2520Sergey%2520Levine%2520and%2520Erran%2520Li%26entry.1292438233%3D%2520%2520The%2520vision%2520of%2520a%2520broadly%2520capable%2520and%2520goal-directed%2520agent%252C%2520such%2520as%2520an%250AInternet-browsing%2520agent%2520in%2520the%2520digital%2520world%2520and%2520a%2520household%2520humanoid%2520in%2520the%250Aphysical%2520world%252C%2520has%2520rapidly%2520advanced%252C%2520thanks%2520to%2520the%2520generalization%2520capability%250Aof%2520foundation%2520models.%2520Such%2520a%2520generalist%2520agent%2520needs%2520to%2520have%2520a%2520large%2520and%2520diverse%250Askill%2520repertoire%252C%2520such%2520as%2520finding%2520directions%2520between%2520two%2520travel%2520locations%2520and%250Abuying%2520specific%2520items%2520from%2520the%2520Internet.%2520If%2520each%2520skill%2520needs%2520to%2520be%2520specified%250Amanually%2520through%2520a%2520fixed%2520set%2520of%2520human-annotated%2520instructions%252C%2520the%2520agent%2527s%2520skill%250Arepertoire%2520will%2520necessarily%2520be%2520limited%2520due%2520to%2520the%2520quantity%2520and%2520diversity%2520of%250Ahuman-annotated%2520instructions.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520challenge%2520by%250Aproposing%2520Proposer-Agent-Evaluator%252C%2520an%2520effective%2520learning%2520system%2520that%2520enables%250Afoundation%2520model%2520agents%2520to%2520autonomously%2520discover%2520and%2520practice%2520skills%2520in%2520the%250Awild.%2520At%2520the%2520heart%2520of%2520PAE%2520is%2520a%2520context-aware%2520task%2520proposer%2520that%2520autonomously%250Aproposes%2520tasks%2520for%2520the%2520agent%2520to%2520practice%2520with%2520context%2520information%2520of%2520the%250Aenvironment%2520such%2520as%2520user%2520demos%2520or%2520even%2520just%2520the%2520name%2520of%2520the%2520website%2520itself%2520for%250AInternet-browsing%2520agents.%2520Then%252C%2520the%2520agent%2520policy%2520attempts%2520those%2520tasks%2520with%250Athoughts%2520and%2520actual%2520grounded%2520operations%2520in%2520the%2520real%2520world%2520with%2520resulting%250Atrajectories%2520evaluated%2520by%2520an%2520autonomous%2520VLM-based%2520success%2520evaluator.%2520The%250Asuccess%2520evaluation%2520serves%2520as%2520the%2520reward%2520signal%2520for%2520the%2520agent%2520to%2520refine%2520its%250Apolicies%2520through%2520RL.%2520We%2520validate%2520PAE%2520on%2520challenging%2520vision-based%2520web%250Anavigation%252C%2520using%2520both%2520real-world%2520and%2520self-hosted%2520websites%2520from%2520WebVoyager%2520and%250AWebArena.To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520represents%2520the%2520first%2520effective%250Alearning%2520system%2520to%2520apply%2520autonomous%2520task%2520proposal%2520with%2520RL%2520for%2520agents%2520that%250Ageneralizes%2520real-world%2520human-annotated%2520benchmarks%2520with%2520SOTA%2520performances.%2520Our%250Aopen-source%2520checkpoints%2520and%2520code%2520can%2520be%2520found%2520in%2520https%253A//yanqval.github.io/PAE/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proposer-Agent-Evaluator%28PAE%29%3A%20Autonomous%20Skill%20Discovery%20For%20Foundation%0A%20%20Model%20Internet%20Agents&entry.906535625=Yifei%20Zhou%20and%20Qianlan%20Yang%20and%20Kaixiang%20Lin%20and%20Min%20Bai%20and%20Xiong%20Zhou%20and%20Yu-Xiong%20Wang%20and%20Sergey%20Levine%20and%20Erran%20Li&entry.1292438233=%20%20The%20vision%20of%20a%20broadly%20capable%20and%20goal-directed%20agent%2C%20such%20as%20an%0AInternet-browsing%20agent%20in%20the%20digital%20world%20and%20a%20household%20humanoid%20in%20the%0Aphysical%20world%2C%20has%20rapidly%20advanced%2C%20thanks%20to%20the%20generalization%20capability%0Aof%20foundation%20models.%20Such%20a%20generalist%20agent%20needs%20to%20have%20a%20large%20and%20diverse%0Askill%20repertoire%2C%20such%20as%20finding%20directions%20between%20two%20travel%20locations%20and%0Abuying%20specific%20items%20from%20the%20Internet.%20If%20each%20skill%20needs%20to%20be%20specified%0Amanually%20through%20a%20fixed%20set%20of%20human-annotated%20instructions%2C%20the%20agent%27s%20skill%0Arepertoire%20will%20necessarily%20be%20limited%20due%20to%20the%20quantity%20and%20diversity%20of%0Ahuman-annotated%20instructions.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%0Aproposing%20Proposer-Agent-Evaluator%2C%20an%20effective%20learning%20system%20that%20enables%0Afoundation%20model%20agents%20to%20autonomously%20discover%20and%20practice%20skills%20in%20the%0Awild.%20At%20the%20heart%20of%20PAE%20is%20a%20context-aware%20task%20proposer%20that%20autonomously%0Aproposes%20tasks%20for%20the%20agent%20to%20practice%20with%20context%20information%20of%20the%0Aenvironment%20such%20as%20user%20demos%20or%20even%20just%20the%20name%20of%20the%20website%20itself%20for%0AInternet-browsing%20agents.%20Then%2C%20the%20agent%20policy%20attempts%20those%20tasks%20with%0Athoughts%20and%20actual%20grounded%20operations%20in%20the%20real%20world%20with%20resulting%0Atrajectories%20evaluated%20by%20an%20autonomous%20VLM-based%20success%20evaluator.%20The%0Asuccess%20evaluation%20serves%20as%20the%20reward%20signal%20for%20the%20agent%20to%20refine%20its%0Apolicies%20through%20RL.%20We%20validate%20PAE%20on%20challenging%20vision-based%20web%0Anavigation%2C%20using%20both%20real-world%20and%20self-hosted%20websites%20from%20WebVoyager%20and%0AWebArena.To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20represents%20the%20first%20effective%0Alearning%20system%20to%20apply%20autonomous%20task%20proposal%20with%20RL%20for%20agents%20that%0Ageneralizes%20real-world%20human-annotated%20benchmarks%20with%20SOTA%20performances.%20Our%0Aopen-source%20checkpoints%20and%20code%20can%20be%20found%20in%20https%3A//yanqval.github.io/PAE/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13194v1&entry.124074799=Read"},
{"title": "FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal\n  Footstep Planning and Forecasting", "author": "Cl\u00e9ment Gaspard and Gr\u00e9goire Passault and M\u00e9lodie Daniel and Olivier Ly", "abstract": "  Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition.\n", "link": "http://arxiv.org/abs/2403.12589v2", "date": "2024-12-17", "relevancy": 1.6763, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5907}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5751}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FootstepNet%3A%20an%20Efficient%20Actor-Critic%20Method%20for%20Fast%20On-line%20Bipedal%0A%20%20Footstep%20Planning%20and%20Forecasting&body=Title%3A%20FootstepNet%3A%20an%20Efficient%20Actor-Critic%20Method%20for%20Fast%20On-line%20Bipedal%0A%20%20Footstep%20Planning%20and%20Forecasting%0AAuthor%3A%20Cl%C3%A9ment%20Gaspard%20and%20Gr%C3%A9goire%20Passault%20and%20M%C3%A9lodie%20Daniel%20and%20Olivier%20Ly%0AAbstract%3A%20%20%20Designing%20a%20humanoid%20locomotion%20controller%20is%20challenging%20and%20classically%0Asplit%20up%20in%20sub-problems.%20Footstep%20planning%20is%20one%20of%20those%2C%20where%20the%20sequence%0Aof%20footsteps%20is%20defined.%20Even%20in%20simpler%20environments%2C%20finding%20a%20minimal%0Asequence%2C%20or%20even%20a%20feasible%20sequence%2C%20yields%20a%20complex%20optimization%20problem.%0AIn%20the%20literature%2C%20this%20problem%20is%20usually%20addressed%20by%20search-based%20algorithms%0A%28e.g.%20variants%20of%20A%2A%29.%20However%2C%20such%20approaches%20are%20either%20computationally%0Aexpensive%20or%20rely%20on%20hand-crafted%20tuning%20of%20several%20parameters.%20In%20this%20work%2C%0Aat%20first%2C%20we%20propose%20an%20efficient%20footstep%20planning%20method%20to%20navigate%20in%20local%0Aenvironments%20with%20obstacles%2C%20based%20on%20state-of-the%20art%20Deep%20Reinforcement%0ALearning%20%28DRL%29%20techniques%2C%20with%20very%20low%20computational%20requirements%20for%20on-line%0Ainference.%20Our%20approach%20is%20heuristic-free%20and%20relies%20on%20a%20continuous%20set%20of%0Aactions%20to%20generate%20feasible%20footsteps.%20In%20contrast%2C%20other%20methods%20necessitate%0Athe%20selection%20of%20a%20relevant%20discrete%20set%20of%20actions.%20Second%2C%20we%20propose%20a%0Aforecasting%20method%2C%20allowing%20to%20quickly%20estimate%20the%20number%20of%20footsteps%0Arequired%20to%20reach%20different%20candidates%20of%20local%20targets.%20This%20approach%20relies%0Aon%20inherent%20computations%20made%20by%20the%20actor-critic%20DRL%20architecture.%20We%0Ademonstrate%20the%20validity%20of%20our%20approach%20with%20simulation%20results%2C%20and%20by%20a%0Adeployment%20on%20a%20kid-size%20humanoid%20robot%20during%20the%20RoboCup%202023%20competition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12589v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFootstepNet%253A%2520an%2520Efficient%2520Actor-Critic%2520Method%2520for%2520Fast%2520On-line%2520Bipedal%250A%2520%2520Footstep%2520Planning%2520and%2520Forecasting%26entry.906535625%3DCl%25C3%25A9ment%2520Gaspard%2520and%2520Gr%25C3%25A9goire%2520Passault%2520and%2520M%25C3%25A9lodie%2520Daniel%2520and%2520Olivier%2520Ly%26entry.1292438233%3D%2520%2520Designing%2520a%2520humanoid%2520locomotion%2520controller%2520is%2520challenging%2520and%2520classically%250Asplit%2520up%2520in%2520sub-problems.%2520Footstep%2520planning%2520is%2520one%2520of%2520those%252C%2520where%2520the%2520sequence%250Aof%2520footsteps%2520is%2520defined.%2520Even%2520in%2520simpler%2520environments%252C%2520finding%2520a%2520minimal%250Asequence%252C%2520or%2520even%2520a%2520feasible%2520sequence%252C%2520yields%2520a%2520complex%2520optimization%2520problem.%250AIn%2520the%2520literature%252C%2520this%2520problem%2520is%2520usually%2520addressed%2520by%2520search-based%2520algorithms%250A%2528e.g.%2520variants%2520of%2520A%252A%2529.%2520However%252C%2520such%2520approaches%2520are%2520either%2520computationally%250Aexpensive%2520or%2520rely%2520on%2520hand-crafted%2520tuning%2520of%2520several%2520parameters.%2520In%2520this%2520work%252C%250Aat%2520first%252C%2520we%2520propose%2520an%2520efficient%2520footstep%2520planning%2520method%2520to%2520navigate%2520in%2520local%250Aenvironments%2520with%2520obstacles%252C%2520based%2520on%2520state-of-the%2520art%2520Deep%2520Reinforcement%250ALearning%2520%2528DRL%2529%2520techniques%252C%2520with%2520very%2520low%2520computational%2520requirements%2520for%2520on-line%250Ainference.%2520Our%2520approach%2520is%2520heuristic-free%2520and%2520relies%2520on%2520a%2520continuous%2520set%2520of%250Aactions%2520to%2520generate%2520feasible%2520footsteps.%2520In%2520contrast%252C%2520other%2520methods%2520necessitate%250Athe%2520selection%2520of%2520a%2520relevant%2520discrete%2520set%2520of%2520actions.%2520Second%252C%2520we%2520propose%2520a%250Aforecasting%2520method%252C%2520allowing%2520to%2520quickly%2520estimate%2520the%2520number%2520of%2520footsteps%250Arequired%2520to%2520reach%2520different%2520candidates%2520of%2520local%2520targets.%2520This%2520approach%2520relies%250Aon%2520inherent%2520computations%2520made%2520by%2520the%2520actor-critic%2520DRL%2520architecture.%2520We%250Ademonstrate%2520the%2520validity%2520of%2520our%2520approach%2520with%2520simulation%2520results%252C%2520and%2520by%2520a%250Adeployment%2520on%2520a%2520kid-size%2520humanoid%2520robot%2520during%2520the%2520RoboCup%25202023%2520competition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12589v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FootstepNet%3A%20an%20Efficient%20Actor-Critic%20Method%20for%20Fast%20On-line%20Bipedal%0A%20%20Footstep%20Planning%20and%20Forecasting&entry.906535625=Cl%C3%A9ment%20Gaspard%20and%20Gr%C3%A9goire%20Passault%20and%20M%C3%A9lodie%20Daniel%20and%20Olivier%20Ly&entry.1292438233=%20%20Designing%20a%20humanoid%20locomotion%20controller%20is%20challenging%20and%20classically%0Asplit%20up%20in%20sub-problems.%20Footstep%20planning%20is%20one%20of%20those%2C%20where%20the%20sequence%0Aof%20footsteps%20is%20defined.%20Even%20in%20simpler%20environments%2C%20finding%20a%20minimal%0Asequence%2C%20or%20even%20a%20feasible%20sequence%2C%20yields%20a%20complex%20optimization%20problem.%0AIn%20the%20literature%2C%20this%20problem%20is%20usually%20addressed%20by%20search-based%20algorithms%0A%28e.g.%20variants%20of%20A%2A%29.%20However%2C%20such%20approaches%20are%20either%20computationally%0Aexpensive%20or%20rely%20on%20hand-crafted%20tuning%20of%20several%20parameters.%20In%20this%20work%2C%0Aat%20first%2C%20we%20propose%20an%20efficient%20footstep%20planning%20method%20to%20navigate%20in%20local%0Aenvironments%20with%20obstacles%2C%20based%20on%20state-of-the%20art%20Deep%20Reinforcement%0ALearning%20%28DRL%29%20techniques%2C%20with%20very%20low%20computational%20requirements%20for%20on-line%0Ainference.%20Our%20approach%20is%20heuristic-free%20and%20relies%20on%20a%20continuous%20set%20of%0Aactions%20to%20generate%20feasible%20footsteps.%20In%20contrast%2C%20other%20methods%20necessitate%0Athe%20selection%20of%20a%20relevant%20discrete%20set%20of%20actions.%20Second%2C%20we%20propose%20a%0Aforecasting%20method%2C%20allowing%20to%20quickly%20estimate%20the%20number%20of%20footsteps%0Arequired%20to%20reach%20different%20candidates%20of%20local%20targets.%20This%20approach%20relies%0Aon%20inherent%20computations%20made%20by%20the%20actor-critic%20DRL%20architecture.%20We%0Ademonstrate%20the%20validity%20of%20our%20approach%20with%20simulation%20results%2C%20and%20by%20a%0Adeployment%20on%20a%20kid-size%20humanoid%20robot%20during%20the%20RoboCup%202023%20competition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12589v2&entry.124074799=Read"},
{"title": "SMOSE: Sparse Mixture of Shallow Experts for Interpretable Reinforcement\n  Learning in Continuous Control Tasks", "author": "M\u00e1ty\u00e1s Vincze and Laura Ferrarotti and Leonardo Lucio Custode and Bruno Lepri and Giovanni Iacca", "abstract": "  Continuous control tasks often involve high-dimensional, dynamic, and\nnon-linear environments. State-of-the-art performance in these tasks is\nachieved through complex closed-box policies that are effective, but suffer\nfrom an inherent opacity. Interpretable policies, while generally\nunderperforming compared to their closed-box counterparts, advantageously\nfacilitate transparent decision-making within automated systems. Hence, their\nusage is often essential for diagnosing and mitigating errors, supporting\nethical and legal accountability, and fostering trust among stakeholders. In\nthis paper, we propose SMOSE, a novel method to train sparsely activated\ninterpretable controllers, based on a top-1 Mixture-of-Experts architecture.\nSMOSE combines a set of interpretable decisionmakers, trained to be experts in\ndifferent basic skills, and an interpretable router that assigns tasks among\nthe experts. The training is carried out via state-of-the-art Reinforcement\nLearning algorithms, exploiting load-balancing techniques to ensure fair expert\nusage. We then distill decision trees from the weights of the router,\nsignificantly improving the ease of interpretation. We evaluate SMOSE on six\nbenchmark environments from MuJoCo: our method outperforms recent interpretable\nbaselines and narrows the gap with noninterpretable state-of-the-art algorithms\n", "link": "http://arxiv.org/abs/2412.13053v1", "date": "2024-12-17", "relevancy": 1.652, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMOSE%3A%20Sparse%20Mixture%20of%20Shallow%20Experts%20for%20Interpretable%20Reinforcement%0A%20%20Learning%20in%20Continuous%20Control%20Tasks&body=Title%3A%20SMOSE%3A%20Sparse%20Mixture%20of%20Shallow%20Experts%20for%20Interpretable%20Reinforcement%0A%20%20Learning%20in%20Continuous%20Control%20Tasks%0AAuthor%3A%20M%C3%A1ty%C3%A1s%20Vincze%20and%20Laura%20Ferrarotti%20and%20Leonardo%20Lucio%20Custode%20and%20Bruno%20Lepri%20and%20Giovanni%20Iacca%0AAbstract%3A%20%20%20Continuous%20control%20tasks%20often%20involve%20high-dimensional%2C%20dynamic%2C%20and%0Anon-linear%20environments.%20State-of-the-art%20performance%20in%20these%20tasks%20is%0Aachieved%20through%20complex%20closed-box%20policies%20that%20are%20effective%2C%20but%20suffer%0Afrom%20an%20inherent%20opacity.%20Interpretable%20policies%2C%20while%20generally%0Aunderperforming%20compared%20to%20their%20closed-box%20counterparts%2C%20advantageously%0Afacilitate%20transparent%20decision-making%20within%20automated%20systems.%20Hence%2C%20their%0Ausage%20is%20often%20essential%20for%20diagnosing%20and%20mitigating%20errors%2C%20supporting%0Aethical%20and%20legal%20accountability%2C%20and%20fostering%20trust%20among%20stakeholders.%20In%0Athis%20paper%2C%20we%20propose%20SMOSE%2C%20a%20novel%20method%20to%20train%20sparsely%20activated%0Ainterpretable%20controllers%2C%20based%20on%20a%20top-1%20Mixture-of-Experts%20architecture.%0ASMOSE%20combines%20a%20set%20of%20interpretable%20decisionmakers%2C%20trained%20to%20be%20experts%20in%0Adifferent%20basic%20skills%2C%20and%20an%20interpretable%20router%20that%20assigns%20tasks%20among%0Athe%20experts.%20The%20training%20is%20carried%20out%20via%20state-of-the-art%20Reinforcement%0ALearning%20algorithms%2C%20exploiting%20load-balancing%20techniques%20to%20ensure%20fair%20expert%0Ausage.%20We%20then%20distill%20decision%20trees%20from%20the%20weights%20of%20the%20router%2C%0Asignificantly%20improving%20the%20ease%20of%20interpretation.%20We%20evaluate%20SMOSE%20on%20six%0Abenchmark%20environments%20from%20MuJoCo%3A%20our%20method%20outperforms%20recent%20interpretable%0Abaselines%20and%20narrows%20the%20gap%20with%20noninterpretable%20state-of-the-art%20algorithms%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMOSE%253A%2520Sparse%2520Mixture%2520of%2520Shallow%2520Experts%2520for%2520Interpretable%2520Reinforcement%250A%2520%2520Learning%2520in%2520Continuous%2520Control%2520Tasks%26entry.906535625%3DM%25C3%25A1ty%25C3%25A1s%2520Vincze%2520and%2520Laura%2520Ferrarotti%2520and%2520Leonardo%2520Lucio%2520Custode%2520and%2520Bruno%2520Lepri%2520and%2520Giovanni%2520Iacca%26entry.1292438233%3D%2520%2520Continuous%2520control%2520tasks%2520often%2520involve%2520high-dimensional%252C%2520dynamic%252C%2520and%250Anon-linear%2520environments.%2520State-of-the-art%2520performance%2520in%2520these%2520tasks%2520is%250Aachieved%2520through%2520complex%2520closed-box%2520policies%2520that%2520are%2520effective%252C%2520but%2520suffer%250Afrom%2520an%2520inherent%2520opacity.%2520Interpretable%2520policies%252C%2520while%2520generally%250Aunderperforming%2520compared%2520to%2520their%2520closed-box%2520counterparts%252C%2520advantageously%250Afacilitate%2520transparent%2520decision-making%2520within%2520automated%2520systems.%2520Hence%252C%2520their%250Ausage%2520is%2520often%2520essential%2520for%2520diagnosing%2520and%2520mitigating%2520errors%252C%2520supporting%250Aethical%2520and%2520legal%2520accountability%252C%2520and%2520fostering%2520trust%2520among%2520stakeholders.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520SMOSE%252C%2520a%2520novel%2520method%2520to%2520train%2520sparsely%2520activated%250Ainterpretable%2520controllers%252C%2520based%2520on%2520a%2520top-1%2520Mixture-of-Experts%2520architecture.%250ASMOSE%2520combines%2520a%2520set%2520of%2520interpretable%2520decisionmakers%252C%2520trained%2520to%2520be%2520experts%2520in%250Adifferent%2520basic%2520skills%252C%2520and%2520an%2520interpretable%2520router%2520that%2520assigns%2520tasks%2520among%250Athe%2520experts.%2520The%2520training%2520is%2520carried%2520out%2520via%2520state-of-the-art%2520Reinforcement%250ALearning%2520algorithms%252C%2520exploiting%2520load-balancing%2520techniques%2520to%2520ensure%2520fair%2520expert%250Ausage.%2520We%2520then%2520distill%2520decision%2520trees%2520from%2520the%2520weights%2520of%2520the%2520router%252C%250Asignificantly%2520improving%2520the%2520ease%2520of%2520interpretation.%2520We%2520evaluate%2520SMOSE%2520on%2520six%250Abenchmark%2520environments%2520from%2520MuJoCo%253A%2520our%2520method%2520outperforms%2520recent%2520interpretable%250Abaselines%2520and%2520narrows%2520the%2520gap%2520with%2520noninterpretable%2520state-of-the-art%2520algorithms%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMOSE%3A%20Sparse%20Mixture%20of%20Shallow%20Experts%20for%20Interpretable%20Reinforcement%0A%20%20Learning%20in%20Continuous%20Control%20Tasks&entry.906535625=M%C3%A1ty%C3%A1s%20Vincze%20and%20Laura%20Ferrarotti%20and%20Leonardo%20Lucio%20Custode%20and%20Bruno%20Lepri%20and%20Giovanni%20Iacca&entry.1292438233=%20%20Continuous%20control%20tasks%20often%20involve%20high-dimensional%2C%20dynamic%2C%20and%0Anon-linear%20environments.%20State-of-the-art%20performance%20in%20these%20tasks%20is%0Aachieved%20through%20complex%20closed-box%20policies%20that%20are%20effective%2C%20but%20suffer%0Afrom%20an%20inherent%20opacity.%20Interpretable%20policies%2C%20while%20generally%0Aunderperforming%20compared%20to%20their%20closed-box%20counterparts%2C%20advantageously%0Afacilitate%20transparent%20decision-making%20within%20automated%20systems.%20Hence%2C%20their%0Ausage%20is%20often%20essential%20for%20diagnosing%20and%20mitigating%20errors%2C%20supporting%0Aethical%20and%20legal%20accountability%2C%20and%20fostering%20trust%20among%20stakeholders.%20In%0Athis%20paper%2C%20we%20propose%20SMOSE%2C%20a%20novel%20method%20to%20train%20sparsely%20activated%0Ainterpretable%20controllers%2C%20based%20on%20a%20top-1%20Mixture-of-Experts%20architecture.%0ASMOSE%20combines%20a%20set%20of%20interpretable%20decisionmakers%2C%20trained%20to%20be%20experts%20in%0Adifferent%20basic%20skills%2C%20and%20an%20interpretable%20router%20that%20assigns%20tasks%20among%0Athe%20experts.%20The%20training%20is%20carried%20out%20via%20state-of-the-art%20Reinforcement%0ALearning%20algorithms%2C%20exploiting%20load-balancing%20techniques%20to%20ensure%20fair%20expert%0Ausage.%20We%20then%20distill%20decision%20trees%20from%20the%20weights%20of%20the%20router%2C%0Asignificantly%20improving%20the%20ease%20of%20interpretation.%20We%20evaluate%20SMOSE%20on%20six%0Abenchmark%20environments%20from%20MuJoCo%3A%20our%20method%20outperforms%20recent%20interpretable%0Abaselines%20and%20narrows%20the%20gap%20with%20noninterpretable%20state-of-the-art%20algorithms%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13053v1&entry.124074799=Read"},
{"title": "Previous Knowledge Utilization In Online Anytime Belief Space Planning", "author": "Michael Novitsky and Moran Barenboim and Vadim Indelman", "abstract": "  Online planning under uncertainty remains a critical challenge in robotics\nand autonomous systems. While tree search techniques are commonly employed to\nconstruct partial future trajectories within computational constraints, most\nexisting methods discard information from previous planning sessions\nconsidering continuous spaces. This study presents a novel, computationally\nefficient approach that leverages historical planning data in current\ndecision-making processes. We provide theoretical foundations for our\ninformation reuse strategy and introduce an algorithm based on Monte Carlo Tree\nSearch (MCTS) that implements this approach. Experimental results demonstrate\nthat our method significantly reduces computation time while maintaining high\nperformance levels. Our findings suggest that integrating historical planning\ninformation can substantially improve the efficiency of online decision-making\nin uncertain environments, paving the way for more responsive and adaptive\nautonomous systems.\n", "link": "http://arxiv.org/abs/2412.13128v1", "date": "2024-12-17", "relevancy": 1.6429, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5576}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5492}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Previous%20Knowledge%20Utilization%20In%20Online%20Anytime%20Belief%20Space%20Planning&body=Title%3A%20Previous%20Knowledge%20Utilization%20In%20Online%20Anytime%20Belief%20Space%20Planning%0AAuthor%3A%20Michael%20Novitsky%20and%20Moran%20Barenboim%20and%20Vadim%20Indelman%0AAbstract%3A%20%20%20Online%20planning%20under%20uncertainty%20remains%20a%20critical%20challenge%20in%20robotics%0Aand%20autonomous%20systems.%20While%20tree%20search%20techniques%20are%20commonly%20employed%20to%0Aconstruct%20partial%20future%20trajectories%20within%20computational%20constraints%2C%20most%0Aexisting%20methods%20discard%20information%20from%20previous%20planning%20sessions%0Aconsidering%20continuous%20spaces.%20This%20study%20presents%20a%20novel%2C%20computationally%0Aefficient%20approach%20that%20leverages%20historical%20planning%20data%20in%20current%0Adecision-making%20processes.%20We%20provide%20theoretical%20foundations%20for%20our%0Ainformation%20reuse%20strategy%20and%20introduce%20an%20algorithm%20based%20on%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20that%20implements%20this%20approach.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20significantly%20reduces%20computation%20time%20while%20maintaining%20high%0Aperformance%20levels.%20Our%20findings%20suggest%20that%20integrating%20historical%20planning%0Ainformation%20can%20substantially%20improve%20the%20efficiency%20of%20online%20decision-making%0Ain%20uncertain%20environments%2C%20paving%20the%20way%20for%20more%20responsive%20and%20adaptive%0Aautonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrevious%2520Knowledge%2520Utilization%2520In%2520Online%2520Anytime%2520Belief%2520Space%2520Planning%26entry.906535625%3DMichael%2520Novitsky%2520and%2520Moran%2520Barenboim%2520and%2520Vadim%2520Indelman%26entry.1292438233%3D%2520%2520Online%2520planning%2520under%2520uncertainty%2520remains%2520a%2520critical%2520challenge%2520in%2520robotics%250Aand%2520autonomous%2520systems.%2520While%2520tree%2520search%2520techniques%2520are%2520commonly%2520employed%2520to%250Aconstruct%2520partial%2520future%2520trajectories%2520within%2520computational%2520constraints%252C%2520most%250Aexisting%2520methods%2520discard%2520information%2520from%2520previous%2520planning%2520sessions%250Aconsidering%2520continuous%2520spaces.%2520This%2520study%2520presents%2520a%2520novel%252C%2520computationally%250Aefficient%2520approach%2520that%2520leverages%2520historical%2520planning%2520data%2520in%2520current%250Adecision-making%2520processes.%2520We%2520provide%2520theoretical%2520foundations%2520for%2520our%250Ainformation%2520reuse%2520strategy%2520and%2520introduce%2520an%2520algorithm%2520based%2520on%2520Monte%2520Carlo%2520Tree%250ASearch%2520%2528MCTS%2529%2520that%2520implements%2520this%2520approach.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520method%2520significantly%2520reduces%2520computation%2520time%2520while%2520maintaining%2520high%250Aperformance%2520levels.%2520Our%2520findings%2520suggest%2520that%2520integrating%2520historical%2520planning%250Ainformation%2520can%2520substantially%2520improve%2520the%2520efficiency%2520of%2520online%2520decision-making%250Ain%2520uncertain%2520environments%252C%2520paving%2520the%2520way%2520for%2520more%2520responsive%2520and%2520adaptive%250Aautonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Previous%20Knowledge%20Utilization%20In%20Online%20Anytime%20Belief%20Space%20Planning&entry.906535625=Michael%20Novitsky%20and%20Moran%20Barenboim%20and%20Vadim%20Indelman&entry.1292438233=%20%20Online%20planning%20under%20uncertainty%20remains%20a%20critical%20challenge%20in%20robotics%0Aand%20autonomous%20systems.%20While%20tree%20search%20techniques%20are%20commonly%20employed%20to%0Aconstruct%20partial%20future%20trajectories%20within%20computational%20constraints%2C%20most%0Aexisting%20methods%20discard%20information%20from%20previous%20planning%20sessions%0Aconsidering%20continuous%20spaces.%20This%20study%20presents%20a%20novel%2C%20computationally%0Aefficient%20approach%20that%20leverages%20historical%20planning%20data%20in%20current%0Adecision-making%20processes.%20We%20provide%20theoretical%20foundations%20for%20our%0Ainformation%20reuse%20strategy%20and%20introduce%20an%20algorithm%20based%20on%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20that%20implements%20this%20approach.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20significantly%20reduces%20computation%20time%20while%20maintaining%20high%0Aperformance%20levels.%20Our%20findings%20suggest%20that%20integrating%20historical%20planning%0Ainformation%20can%20substantially%20improve%20the%20efficiency%20of%20online%20decision-making%0Ain%20uncertain%20environments%2C%20paving%20the%20way%20for%20more%20responsive%20and%20adaptive%0Aautonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13128v1&entry.124074799=Read"},
{"title": "BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce", "author": "Mohammad Nazmush Shamael and Sabila Nawshin and Swakkhar Shatabda and Salekul Islam", "abstract": "  This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.\n", "link": "http://arxiv.org/abs/2412.13161v1", "date": "2024-12-17", "relevancy": 1.6345, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4188}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4143}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BanglishRev%3A%20A%20Large-Scale%20Bangla-English%20and%20Code-mixed%20Dataset%20of%0A%20%20Product%20Reviews%20in%20E-Commerce&body=Title%3A%20BanglishRev%3A%20A%20Large-Scale%20Bangla-English%20and%20Code-mixed%20Dataset%20of%0A%20%20Product%20Reviews%20in%20E-Commerce%0AAuthor%3A%20Mohammad%20Nazmush%20Shamael%20and%20Sabila%20Nawshin%20and%20Swakkhar%20Shatabda%20and%20Salekul%20Islam%0AAbstract%3A%20%20%20This%20work%20presents%20the%20BanglishRev%20Dataset%2C%20the%20largest%20e-commerce%20product%0Areview%20dataset%20to%20date%20for%20reviews%20written%20in%20Bengali%2C%20English%2C%20a%20mixture%20of%0Aboth%20and%20Banglish%2C%20Bengali%20words%20written%20with%20English%20alphabets.%20The%20dataset%0Acomprises%20of%201.74%20million%20written%20reviews%20from%203.2%20million%20ratings%20information%0Acollected%20from%20a%20total%20of%20128k%20products%20being%20sold%20in%20online%20e-commerce%0Aplatforms%20targeting%20the%20Bengali%20population.%20It%20includes%20an%20extensive%20array%20of%0Arelated%20metadata%20for%20each%20of%20the%20reviews%20including%20the%20rating%20given%20by%20the%0Areviewer%2C%20date%20the%20review%20was%20posted%20and%20date%20of%20purchase%2C%20number%20of%20likes%2C%0Adislikes%2C%20response%20from%20the%20seller%2C%20images%20associated%20with%20the%20review%20etc.%20With%0Asentiment%20analysis%20being%20the%20most%20prominent%20usage%20of%20review%20datasets%2C%0Aexperimentation%20with%20a%20binary%20sentiment%20analysis%20model%20with%20the%20review%20rating%0Aserving%20as%20an%20indicator%20of%20positive%20or%20negative%20sentiment%20was%20conducted%20to%0Aevaluate%20the%20effectiveness%20of%20the%20large%20amount%20of%20data%20presented%20in%20BanglishRev%0Afor%20sentiment%20analysis%20tasks.%20A%20BanglishBERT%20model%20is%20trained%20on%20the%20data%20from%0ABanglishRev%20with%20reviews%20being%20considered%20labeled%20positive%20if%20the%20rating%20is%0Agreater%20than%203%20and%20negative%20if%20the%20rating%20is%20less%20than%20or%20equal%20to%203.%20The%20model%0Ais%20evaluated%20by%20being%20testing%20against%20a%20previously%20published%20manually%20annotated%0Adataset%20for%20e-commerce%20reviews%20written%20in%20a%20mixture%20of%20Bangla%2C%20English%20and%0ABanglish.%20The%20experimental%20model%20achieved%20an%20exceptional%20accuracy%20of%2094%5C%25%20and%0AF1%20score%20of%200.94%2C%20demonstrating%20the%20dataset%27s%20efficacy%20for%20sentiment%20analysis.%0ASome%20of%20the%20intriguing%20patterns%20and%20observations%20seen%20within%20the%20dataset%20and%0Afuture%20research%20directions%20where%20the%20dataset%20can%20be%20utilized%20is%20also%20discussed%0Aand%20explored.%20The%20dataset%20can%20be%20accessed%20through%0Ahttps%3A//huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBanglishRev%253A%2520A%2520Large-Scale%2520Bangla-English%2520and%2520Code-mixed%2520Dataset%2520of%250A%2520%2520Product%2520Reviews%2520in%2520E-Commerce%26entry.906535625%3DMohammad%2520Nazmush%2520Shamael%2520and%2520Sabila%2520Nawshin%2520and%2520Swakkhar%2520Shatabda%2520and%2520Salekul%2520Islam%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520the%2520BanglishRev%2520Dataset%252C%2520the%2520largest%2520e-commerce%2520product%250Areview%2520dataset%2520to%2520date%2520for%2520reviews%2520written%2520in%2520Bengali%252C%2520English%252C%2520a%2520mixture%2520of%250Aboth%2520and%2520Banglish%252C%2520Bengali%2520words%2520written%2520with%2520English%2520alphabets.%2520The%2520dataset%250Acomprises%2520of%25201.74%2520million%2520written%2520reviews%2520from%25203.2%2520million%2520ratings%2520information%250Acollected%2520from%2520a%2520total%2520of%2520128k%2520products%2520being%2520sold%2520in%2520online%2520e-commerce%250Aplatforms%2520targeting%2520the%2520Bengali%2520population.%2520It%2520includes%2520an%2520extensive%2520array%2520of%250Arelated%2520metadata%2520for%2520each%2520of%2520the%2520reviews%2520including%2520the%2520rating%2520given%2520by%2520the%250Areviewer%252C%2520date%2520the%2520review%2520was%2520posted%2520and%2520date%2520of%2520purchase%252C%2520number%2520of%2520likes%252C%250Adislikes%252C%2520response%2520from%2520the%2520seller%252C%2520images%2520associated%2520with%2520the%2520review%2520etc.%2520With%250Asentiment%2520analysis%2520being%2520the%2520most%2520prominent%2520usage%2520of%2520review%2520datasets%252C%250Aexperimentation%2520with%2520a%2520binary%2520sentiment%2520analysis%2520model%2520with%2520the%2520review%2520rating%250Aserving%2520as%2520an%2520indicator%2520of%2520positive%2520or%2520negative%2520sentiment%2520was%2520conducted%2520to%250Aevaluate%2520the%2520effectiveness%2520of%2520the%2520large%2520amount%2520of%2520data%2520presented%2520in%2520BanglishRev%250Afor%2520sentiment%2520analysis%2520tasks.%2520A%2520BanglishBERT%2520model%2520is%2520trained%2520on%2520the%2520data%2520from%250ABanglishRev%2520with%2520reviews%2520being%2520considered%2520labeled%2520positive%2520if%2520the%2520rating%2520is%250Agreater%2520than%25203%2520and%2520negative%2520if%2520the%2520rating%2520is%2520less%2520than%2520or%2520equal%2520to%25203.%2520The%2520model%250Ais%2520evaluated%2520by%2520being%2520testing%2520against%2520a%2520previously%2520published%2520manually%2520annotated%250Adataset%2520for%2520e-commerce%2520reviews%2520written%2520in%2520a%2520mixture%2520of%2520Bangla%252C%2520English%2520and%250ABanglish.%2520The%2520experimental%2520model%2520achieved%2520an%2520exceptional%2520accuracy%2520of%252094%255C%2525%2520and%250AF1%2520score%2520of%25200.94%252C%2520demonstrating%2520the%2520dataset%2527s%2520efficacy%2520for%2520sentiment%2520analysis.%250ASome%2520of%2520the%2520intriguing%2520patterns%2520and%2520observations%2520seen%2520within%2520the%2520dataset%2520and%250Afuture%2520research%2520directions%2520where%2520the%2520dataset%2520can%2520be%2520utilized%2520is%2520also%2520discussed%250Aand%2520explored.%2520The%2520dataset%2520can%2520be%2520accessed%2520through%250Ahttps%253A//huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BanglishRev%3A%20A%20Large-Scale%20Bangla-English%20and%20Code-mixed%20Dataset%20of%0A%20%20Product%20Reviews%20in%20E-Commerce&entry.906535625=Mohammad%20Nazmush%20Shamael%20and%20Sabila%20Nawshin%20and%20Swakkhar%20Shatabda%20and%20Salekul%20Islam&entry.1292438233=%20%20This%20work%20presents%20the%20BanglishRev%20Dataset%2C%20the%20largest%20e-commerce%20product%0Areview%20dataset%20to%20date%20for%20reviews%20written%20in%20Bengali%2C%20English%2C%20a%20mixture%20of%0Aboth%20and%20Banglish%2C%20Bengali%20words%20written%20with%20English%20alphabets.%20The%20dataset%0Acomprises%20of%201.74%20million%20written%20reviews%20from%203.2%20million%20ratings%20information%0Acollected%20from%20a%20total%20of%20128k%20products%20being%20sold%20in%20online%20e-commerce%0Aplatforms%20targeting%20the%20Bengali%20population.%20It%20includes%20an%20extensive%20array%20of%0Arelated%20metadata%20for%20each%20of%20the%20reviews%20including%20the%20rating%20given%20by%20the%0Areviewer%2C%20date%20the%20review%20was%20posted%20and%20date%20of%20purchase%2C%20number%20of%20likes%2C%0Adislikes%2C%20response%20from%20the%20seller%2C%20images%20associated%20with%20the%20review%20etc.%20With%0Asentiment%20analysis%20being%20the%20most%20prominent%20usage%20of%20review%20datasets%2C%0Aexperimentation%20with%20a%20binary%20sentiment%20analysis%20model%20with%20the%20review%20rating%0Aserving%20as%20an%20indicator%20of%20positive%20or%20negative%20sentiment%20was%20conducted%20to%0Aevaluate%20the%20effectiveness%20of%20the%20large%20amount%20of%20data%20presented%20in%20BanglishRev%0Afor%20sentiment%20analysis%20tasks.%20A%20BanglishBERT%20model%20is%20trained%20on%20the%20data%20from%0ABanglishRev%20with%20reviews%20being%20considered%20labeled%20positive%20if%20the%20rating%20is%0Agreater%20than%203%20and%20negative%20if%20the%20rating%20is%20less%20than%20or%20equal%20to%203.%20The%20model%0Ais%20evaluated%20by%20being%20testing%20against%20a%20previously%20published%20manually%20annotated%0Adataset%20for%20e-commerce%20reviews%20written%20in%20a%20mixture%20of%20Bangla%2C%20English%20and%0ABanglish.%20The%20experimental%20model%20achieved%20an%20exceptional%20accuracy%20of%2094%5C%25%20and%0AF1%20score%20of%200.94%2C%20demonstrating%20the%20dataset%27s%20efficacy%20for%20sentiment%20analysis.%0ASome%20of%20the%20intriguing%20patterns%20and%20observations%20seen%20within%20the%20dataset%20and%0Afuture%20research%20directions%20where%20the%20dataset%20can%20be%20utilized%20is%20also%20discussed%0Aand%20explored.%20The%20dataset%20can%20be%20accessed%20through%0Ahttps%3A//huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13161v1&entry.124074799=Read"},
{"title": "Label Errors in the Tobacco3482 Dataset", "author": "Gordon Lim and Stefan Larson and Kevin Leach", "abstract": "  Tobacco3482 is a widely used document classification benchmark dataset.\nHowever, our manual inspection of the entire dataset uncovers widespread\nontological issues, especially large amounts of annotation label problems in\nthe dataset. We establish data label guidelines and find that 11.7% of the\ndataset is improperly annotated and should either have an unknown label or a\ncorrected label, and 16.7% of samples in the dataset have multiple valid\nlabels. We then analyze the mistakes of a top-performing model and find that\n35% of the model's mistakes can be directly attributed to these label issues,\nhighlighting the inherent problems with using a noisily labeled dataset as a\nbenchmark. Supplementary material, including dataset annotations and code, is\navailable at https://github.com/gordon-lim/tobacco3482-mistakes/.\n", "link": "http://arxiv.org/abs/2412.13140v1", "date": "2024-12-17", "relevancy": 1.6312, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4541}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4057}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label%20Errors%20in%20the%20Tobacco3482%20Dataset&body=Title%3A%20Label%20Errors%20in%20the%20Tobacco3482%20Dataset%0AAuthor%3A%20Gordon%20Lim%20and%20Stefan%20Larson%20and%20Kevin%20Leach%0AAbstract%3A%20%20%20Tobacco3482%20is%20a%20widely%20used%20document%20classification%20benchmark%20dataset.%0AHowever%2C%20our%20manual%20inspection%20of%20the%20entire%20dataset%20uncovers%20widespread%0Aontological%20issues%2C%20especially%20large%20amounts%20of%20annotation%20label%20problems%20in%0Athe%20dataset.%20We%20establish%20data%20label%20guidelines%20and%20find%20that%2011.7%25%20of%20the%0Adataset%20is%20improperly%20annotated%20and%20should%20either%20have%20an%20unknown%20label%20or%20a%0Acorrected%20label%2C%20and%2016.7%25%20of%20samples%20in%20the%20dataset%20have%20multiple%20valid%0Alabels.%20We%20then%20analyze%20the%20mistakes%20of%20a%20top-performing%20model%20and%20find%20that%0A35%25%20of%20the%20model%27s%20mistakes%20can%20be%20directly%20attributed%20to%20these%20label%20issues%2C%0Ahighlighting%20the%20inherent%20problems%20with%20using%20a%20noisily%20labeled%20dataset%20as%20a%0Abenchmark.%20Supplementary%20material%2C%20including%20dataset%20annotations%20and%20code%2C%20is%0Aavailable%20at%20https%3A//github.com/gordon-lim/tobacco3482-mistakes/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel%2520Errors%2520in%2520the%2520Tobacco3482%2520Dataset%26entry.906535625%3DGordon%2520Lim%2520and%2520Stefan%2520Larson%2520and%2520Kevin%2520Leach%26entry.1292438233%3D%2520%2520Tobacco3482%2520is%2520a%2520widely%2520used%2520document%2520classification%2520benchmark%2520dataset.%250AHowever%252C%2520our%2520manual%2520inspection%2520of%2520the%2520entire%2520dataset%2520uncovers%2520widespread%250Aontological%2520issues%252C%2520especially%2520large%2520amounts%2520of%2520annotation%2520label%2520problems%2520in%250Athe%2520dataset.%2520We%2520establish%2520data%2520label%2520guidelines%2520and%2520find%2520that%252011.7%2525%2520of%2520the%250Adataset%2520is%2520improperly%2520annotated%2520and%2520should%2520either%2520have%2520an%2520unknown%2520label%2520or%2520a%250Acorrected%2520label%252C%2520and%252016.7%2525%2520of%2520samples%2520in%2520the%2520dataset%2520have%2520multiple%2520valid%250Alabels.%2520We%2520then%2520analyze%2520the%2520mistakes%2520of%2520a%2520top-performing%2520model%2520and%2520find%2520that%250A35%2525%2520of%2520the%2520model%2527s%2520mistakes%2520can%2520be%2520directly%2520attributed%2520to%2520these%2520label%2520issues%252C%250Ahighlighting%2520the%2520inherent%2520problems%2520with%2520using%2520a%2520noisily%2520labeled%2520dataset%2520as%2520a%250Abenchmark.%2520Supplementary%2520material%252C%2520including%2520dataset%2520annotations%2520and%2520code%252C%2520is%250Aavailable%2520at%2520https%253A//github.com/gordon-lim/tobacco3482-mistakes/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Errors%20in%20the%20Tobacco3482%20Dataset&entry.906535625=Gordon%20Lim%20and%20Stefan%20Larson%20and%20Kevin%20Leach&entry.1292438233=%20%20Tobacco3482%20is%20a%20widely%20used%20document%20classification%20benchmark%20dataset.%0AHowever%2C%20our%20manual%20inspection%20of%20the%20entire%20dataset%20uncovers%20widespread%0Aontological%20issues%2C%20especially%20large%20amounts%20of%20annotation%20label%20problems%20in%0Athe%20dataset.%20We%20establish%20data%20label%20guidelines%20and%20find%20that%2011.7%25%20of%20the%0Adataset%20is%20improperly%20annotated%20and%20should%20either%20have%20an%20unknown%20label%20or%20a%0Acorrected%20label%2C%20and%2016.7%25%20of%20samples%20in%20the%20dataset%20have%20multiple%20valid%0Alabels.%20We%20then%20analyze%20the%20mistakes%20of%20a%20top-performing%20model%20and%20find%20that%0A35%25%20of%20the%20model%27s%20mistakes%20can%20be%20directly%20attributed%20to%20these%20label%20issues%2C%0Ahighlighting%20the%20inherent%20problems%20with%20using%20a%20noisily%20labeled%20dataset%20as%20a%0Abenchmark.%20Supplementary%20material%2C%20including%20dataset%20annotations%20and%20code%2C%20is%0Aavailable%20at%20https%3A//github.com/gordon-lim/tobacco3482-mistakes/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13140v1&entry.124074799=Read"},
{"title": "S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical\n  Imaging", "author": "Yimu Pan and Sitao Zhang and Alison D. Gernand and Jeffery A. Goldstein and James Z. Wang", "abstract": "  Robustness and generalizability in medical image segmentation are often\nhindered by scarcity and limited diversity of training data, which stands in\ncontrast to the variability encountered during inference. While conventional\nstrategies -- such as domain-specific augmentation, specialized architectures,\nand tailored training procedures -- can alleviate these issues, they depend on\nthe availability and reliability of domain knowledge. When such knowledge is\nunavailable, misleading, or improperly applied, performance may deteriorate. In\nresponse, we introduce a novel, domain-agnostic, add-on, and data-driven\nstrategy inspired by image stacking in image denoising. Termed ``semantic\nstacking,'' our method estimates a denoised semantic representation that\ncomplements the conventional segmentation loss during training. This method\ndoes not depend on domain-specific assumptions, making it broadly applicable\nacross diverse image modalities, model architectures, and augmentation\ntechniques. Through extensive experiments, we validate the superiority of our\napproach in improving segmentation performance under diverse conditions. Code\nis available at https://github.com/ymp5078/Semantic-Stacking.\n", "link": "http://arxiv.org/abs/2412.13156v1", "date": "2024-12-17", "relevancy": 1.6023, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5287}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S2S2%3A%20Semantic%20Stacking%20for%20Robust%20Semantic%20Segmentation%20in%20Medical%0A%20%20Imaging&body=Title%3A%20S2S2%3A%20Semantic%20Stacking%20for%20Robust%20Semantic%20Segmentation%20in%20Medical%0A%20%20Imaging%0AAuthor%3A%20Yimu%20Pan%20and%20Sitao%20Zhang%20and%20Alison%20D.%20Gernand%20and%20Jeffery%20A.%20Goldstein%20and%20James%20Z.%20Wang%0AAbstract%3A%20%20%20Robustness%20and%20generalizability%20in%20medical%20image%20segmentation%20are%20often%0Ahindered%20by%20scarcity%20and%20limited%20diversity%20of%20training%20data%2C%20which%20stands%20in%0Acontrast%20to%20the%20variability%20encountered%20during%20inference.%20While%20conventional%0Astrategies%20--%20such%20as%20domain-specific%20augmentation%2C%20specialized%20architectures%2C%0Aand%20tailored%20training%20procedures%20--%20can%20alleviate%20these%20issues%2C%20they%20depend%20on%0Athe%20availability%20and%20reliability%20of%20domain%20knowledge.%20When%20such%20knowledge%20is%0Aunavailable%2C%20misleading%2C%20or%20improperly%20applied%2C%20performance%20may%20deteriorate.%20In%0Aresponse%2C%20we%20introduce%20a%20novel%2C%20domain-agnostic%2C%20add-on%2C%20and%20data-driven%0Astrategy%20inspired%20by%20image%20stacking%20in%20image%20denoising.%20Termed%20%60%60semantic%0Astacking%2C%27%27%20our%20method%20estimates%20a%20denoised%20semantic%20representation%20that%0Acomplements%20the%20conventional%20segmentation%20loss%20during%20training.%20This%20method%0Adoes%20not%20depend%20on%20domain-specific%20assumptions%2C%20making%20it%20broadly%20applicable%0Aacross%20diverse%20image%20modalities%2C%20model%20architectures%2C%20and%20augmentation%0Atechniques.%20Through%20extensive%20experiments%2C%20we%20validate%20the%20superiority%20of%20our%0Aapproach%20in%20improving%20segmentation%20performance%20under%20diverse%20conditions.%20Code%0Ais%20available%20at%20https%3A//github.com/ymp5078/Semantic-Stacking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS2S2%253A%2520Semantic%2520Stacking%2520for%2520Robust%2520Semantic%2520Segmentation%2520in%2520Medical%250A%2520%2520Imaging%26entry.906535625%3DYimu%2520Pan%2520and%2520Sitao%2520Zhang%2520and%2520Alison%2520D.%2520Gernand%2520and%2520Jeffery%2520A.%2520Goldstein%2520and%2520James%2520Z.%2520Wang%26entry.1292438233%3D%2520%2520Robustness%2520and%2520generalizability%2520in%2520medical%2520image%2520segmentation%2520are%2520often%250Ahindered%2520by%2520scarcity%2520and%2520limited%2520diversity%2520of%2520training%2520data%252C%2520which%2520stands%2520in%250Acontrast%2520to%2520the%2520variability%2520encountered%2520during%2520inference.%2520While%2520conventional%250Astrategies%2520--%2520such%2520as%2520domain-specific%2520augmentation%252C%2520specialized%2520architectures%252C%250Aand%2520tailored%2520training%2520procedures%2520--%2520can%2520alleviate%2520these%2520issues%252C%2520they%2520depend%2520on%250Athe%2520availability%2520and%2520reliability%2520of%2520domain%2520knowledge.%2520When%2520such%2520knowledge%2520is%250Aunavailable%252C%2520misleading%252C%2520or%2520improperly%2520applied%252C%2520performance%2520may%2520deteriorate.%2520In%250Aresponse%252C%2520we%2520introduce%2520a%2520novel%252C%2520domain-agnostic%252C%2520add-on%252C%2520and%2520data-driven%250Astrategy%2520inspired%2520by%2520image%2520stacking%2520in%2520image%2520denoising.%2520Termed%2520%2560%2560semantic%250Astacking%252C%2527%2527%2520our%2520method%2520estimates%2520a%2520denoised%2520semantic%2520representation%2520that%250Acomplements%2520the%2520conventional%2520segmentation%2520loss%2520during%2520training.%2520This%2520method%250Adoes%2520not%2520depend%2520on%2520domain-specific%2520assumptions%252C%2520making%2520it%2520broadly%2520applicable%250Aacross%2520diverse%2520image%2520modalities%252C%2520model%2520architectures%252C%2520and%2520augmentation%250Atechniques.%2520Through%2520extensive%2520experiments%252C%2520we%2520validate%2520the%2520superiority%2520of%2520our%250Aapproach%2520in%2520improving%2520segmentation%2520performance%2520under%2520diverse%2520conditions.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/ymp5078/Semantic-Stacking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2S2%3A%20Semantic%20Stacking%20for%20Robust%20Semantic%20Segmentation%20in%20Medical%0A%20%20Imaging&entry.906535625=Yimu%20Pan%20and%20Sitao%20Zhang%20and%20Alison%20D.%20Gernand%20and%20Jeffery%20A.%20Goldstein%20and%20James%20Z.%20Wang&entry.1292438233=%20%20Robustness%20and%20generalizability%20in%20medical%20image%20segmentation%20are%20often%0Ahindered%20by%20scarcity%20and%20limited%20diversity%20of%20training%20data%2C%20which%20stands%20in%0Acontrast%20to%20the%20variability%20encountered%20during%20inference.%20While%20conventional%0Astrategies%20--%20such%20as%20domain-specific%20augmentation%2C%20specialized%20architectures%2C%0Aand%20tailored%20training%20procedures%20--%20can%20alleviate%20these%20issues%2C%20they%20depend%20on%0Athe%20availability%20and%20reliability%20of%20domain%20knowledge.%20When%20such%20knowledge%20is%0Aunavailable%2C%20misleading%2C%20or%20improperly%20applied%2C%20performance%20may%20deteriorate.%20In%0Aresponse%2C%20we%20introduce%20a%20novel%2C%20domain-agnostic%2C%20add-on%2C%20and%20data-driven%0Astrategy%20inspired%20by%20image%20stacking%20in%20image%20denoising.%20Termed%20%60%60semantic%0Astacking%2C%27%27%20our%20method%20estimates%20a%20denoised%20semantic%20representation%20that%0Acomplements%20the%20conventional%20segmentation%20loss%20during%20training.%20This%20method%0Adoes%20not%20depend%20on%20domain-specific%20assumptions%2C%20making%20it%20broadly%20applicable%0Aacross%20diverse%20image%20modalities%2C%20model%20architectures%2C%20and%20augmentation%0Atechniques.%20Through%20extensive%20experiments%2C%20we%20validate%20the%20superiority%20of%20our%0Aapproach%20in%20improving%20segmentation%20performance%20under%20diverse%20conditions.%20Code%0Ais%20available%20at%20https%3A//github.com/ymp5078/Semantic-Stacking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13156v1&entry.124074799=Read"},
{"title": "Active Reinforcement Learning Strategies for Offline Policy Improvement", "author": "Ambedkar Dukkipati and Ranga Shaarad Ayyagari and Bodhisattwa Dasgupta and Parag Dutta and Prabhas Reddy Onteru", "abstract": "  Learning agents that excel at sequential decision-making tasks must\ncontinuously resolve the problem of exploration and exploitation for optimal\nlearning. However, such interactions with the environment online might be\nprohibitively expensive and may involve some constraints, such as a limited\nbudget for agent-environment interactions and restricted exploration in certain\nregions of the state space. Examples include selecting candidates for medical\ntrials and training agents in complex navigation environments. This problem\nnecessitates the study of active reinforcement learning strategies that collect\nminimal additional experience trajectories by reusing existing offline data\npreviously collected by some unknown behavior policy. In this work, we propose\na representation-aware uncertainty-based active trajectory collection method\nthat intelligently decides interaction strategies that consider the\ndistribution of the existing offline data. With extensive experimentation, we\ndemonstrate that our proposed method reduces additional online interaction with\nthe environment by up to 75% over competitive baselines across various\ncontinuous control environments.\n", "link": "http://arxiv.org/abs/2412.13106v1", "date": "2024-12-17", "relevancy": 1.5936, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5417}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5356}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Reinforcement%20Learning%20Strategies%20for%20Offline%20Policy%20Improvement&body=Title%3A%20Active%20Reinforcement%20Learning%20Strategies%20for%20Offline%20Policy%20Improvement%0AAuthor%3A%20Ambedkar%20Dukkipati%20and%20Ranga%20Shaarad%20Ayyagari%20and%20Bodhisattwa%20Dasgupta%20and%20Parag%20Dutta%20and%20Prabhas%20Reddy%20Onteru%0AAbstract%3A%20%20%20Learning%20agents%20that%20excel%20at%20sequential%20decision-making%20tasks%20must%0Acontinuously%20resolve%20the%20problem%20of%20exploration%20and%20exploitation%20for%20optimal%0Alearning.%20However%2C%20such%20interactions%20with%20the%20environment%20online%20might%20be%0Aprohibitively%20expensive%20and%20may%20involve%20some%20constraints%2C%20such%20as%20a%20limited%0Abudget%20for%20agent-environment%20interactions%20and%20restricted%20exploration%20in%20certain%0Aregions%20of%20the%20state%20space.%20Examples%20include%20selecting%20candidates%20for%20medical%0Atrials%20and%20training%20agents%20in%20complex%20navigation%20environments.%20This%20problem%0Anecessitates%20the%20study%20of%20active%20reinforcement%20learning%20strategies%20that%20collect%0Aminimal%20additional%20experience%20trajectories%20by%20reusing%20existing%20offline%20data%0Apreviously%20collected%20by%20some%20unknown%20behavior%20policy.%20In%20this%20work%2C%20we%20propose%0Aa%20representation-aware%20uncertainty-based%20active%20trajectory%20collection%20method%0Athat%20intelligently%20decides%20interaction%20strategies%20that%20consider%20the%0Adistribution%20of%20the%20existing%20offline%20data.%20With%20extensive%20experimentation%2C%20we%0Ademonstrate%20that%20our%20proposed%20method%20reduces%20additional%20online%20interaction%20with%0Athe%20environment%20by%20up%20to%2075%25%20over%20competitive%20baselines%20across%20various%0Acontinuous%20control%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Reinforcement%2520Learning%2520Strategies%2520for%2520Offline%2520Policy%2520Improvement%26entry.906535625%3DAmbedkar%2520Dukkipati%2520and%2520Ranga%2520Shaarad%2520Ayyagari%2520and%2520Bodhisattwa%2520Dasgupta%2520and%2520Parag%2520Dutta%2520and%2520Prabhas%2520Reddy%2520Onteru%26entry.1292438233%3D%2520%2520Learning%2520agents%2520that%2520excel%2520at%2520sequential%2520decision-making%2520tasks%2520must%250Acontinuously%2520resolve%2520the%2520problem%2520of%2520exploration%2520and%2520exploitation%2520for%2520optimal%250Alearning.%2520However%252C%2520such%2520interactions%2520with%2520the%2520environment%2520online%2520might%2520be%250Aprohibitively%2520expensive%2520and%2520may%2520involve%2520some%2520constraints%252C%2520such%2520as%2520a%2520limited%250Abudget%2520for%2520agent-environment%2520interactions%2520and%2520restricted%2520exploration%2520in%2520certain%250Aregions%2520of%2520the%2520state%2520space.%2520Examples%2520include%2520selecting%2520candidates%2520for%2520medical%250Atrials%2520and%2520training%2520agents%2520in%2520complex%2520navigation%2520environments.%2520This%2520problem%250Anecessitates%2520the%2520study%2520of%2520active%2520reinforcement%2520learning%2520strategies%2520that%2520collect%250Aminimal%2520additional%2520experience%2520trajectories%2520by%2520reusing%2520existing%2520offline%2520data%250Apreviously%2520collected%2520by%2520some%2520unknown%2520behavior%2520policy.%2520In%2520this%2520work%252C%2520we%2520propose%250Aa%2520representation-aware%2520uncertainty-based%2520active%2520trajectory%2520collection%2520method%250Athat%2520intelligently%2520decides%2520interaction%2520strategies%2520that%2520consider%2520the%250Adistribution%2520of%2520the%2520existing%2520offline%2520data.%2520With%2520extensive%2520experimentation%252C%2520we%250Ademonstrate%2520that%2520our%2520proposed%2520method%2520reduces%2520additional%2520online%2520interaction%2520with%250Athe%2520environment%2520by%2520up%2520to%252075%2525%2520over%2520competitive%2520baselines%2520across%2520various%250Acontinuous%2520control%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Reinforcement%20Learning%20Strategies%20for%20Offline%20Policy%20Improvement&entry.906535625=Ambedkar%20Dukkipati%20and%20Ranga%20Shaarad%20Ayyagari%20and%20Bodhisattwa%20Dasgupta%20and%20Parag%20Dutta%20and%20Prabhas%20Reddy%20Onteru&entry.1292438233=%20%20Learning%20agents%20that%20excel%20at%20sequential%20decision-making%20tasks%20must%0Acontinuously%20resolve%20the%20problem%20of%20exploration%20and%20exploitation%20for%20optimal%0Alearning.%20However%2C%20such%20interactions%20with%20the%20environment%20online%20might%20be%0Aprohibitively%20expensive%20and%20may%20involve%20some%20constraints%2C%20such%20as%20a%20limited%0Abudget%20for%20agent-environment%20interactions%20and%20restricted%20exploration%20in%20certain%0Aregions%20of%20the%20state%20space.%20Examples%20include%20selecting%20candidates%20for%20medical%0Atrials%20and%20training%20agents%20in%20complex%20navigation%20environments.%20This%20problem%0Anecessitates%20the%20study%20of%20active%20reinforcement%20learning%20strategies%20that%20collect%0Aminimal%20additional%20experience%20trajectories%20by%20reusing%20existing%20offline%20data%0Apreviously%20collected%20by%20some%20unknown%20behavior%20policy.%20In%20this%20work%2C%20we%20propose%0Aa%20representation-aware%20uncertainty-based%20active%20trajectory%20collection%20method%0Athat%20intelligently%20decides%20interaction%20strategies%20that%20consider%20the%0Adistribution%20of%20the%20existing%20offline%20data.%20With%20extensive%20experimentation%2C%20we%0Ademonstrate%20that%20our%20proposed%20method%20reduces%20additional%20online%20interaction%20with%0Athe%20environment%20by%20up%20to%2075%25%20over%20competitive%20baselines%20across%20various%0Acontinuous%20control%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13106v1&entry.124074799=Read"},
{"title": "Predicting Change, Not States: An Alternate Framework for Neural PDE\n  Surrogates", "author": "Anthony Zhou and Amir Barati Farimani", "abstract": "  Neural surrogates for partial differential equations (PDEs) have become\npopular due to their potential to quickly simulate physics. With a few\nexceptions, neural surrogates generally treat the forward evolution of\ntime-dependent PDEs as a black box by directly predicting the next state. While\nthis is a natural and easy framework for applying neural surrogates, it can be\nan over-simplified and rigid framework for predicting physics. In this work, we\npropose an alternative framework in which neural solvers predict the temporal\nderivative and an ODE integrator forwards the solution in time, which has\nlittle overhead and is broadly applicable across model architectures and PDEs.\nWe find that by simply changing the training target and introducing numerical\nintegration during inference, neural surrogates can gain accuracy and\nstability. Predicting temporal derivatives also allows models to not be\nconstrained to a specific temporal discretization, allowing for flexible\ntime-stepping during inference or training on higher-resolution PDE data.\nLastly, we investigate why this new framework can be beneficial and in what\nsituations does it work well.\n", "link": "http://arxiv.org/abs/2412.13074v1", "date": "2024-12-17", "relevancy": 1.543, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5073}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Change%2C%20Not%20States%3A%20An%20Alternate%20Framework%20for%20Neural%20PDE%0A%20%20Surrogates&body=Title%3A%20Predicting%20Change%2C%20Not%20States%3A%20An%20Alternate%20Framework%20for%20Neural%20PDE%0A%20%20Surrogates%0AAuthor%3A%20Anthony%20Zhou%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20%20%20Neural%20surrogates%20for%20partial%20differential%20equations%20%28PDEs%29%20have%20become%0Apopular%20due%20to%20their%20potential%20to%20quickly%20simulate%20physics.%20With%20a%20few%0Aexceptions%2C%20neural%20surrogates%20generally%20treat%20the%20forward%20evolution%20of%0Atime-dependent%20PDEs%20as%20a%20black%20box%20by%20directly%20predicting%20the%20next%20state.%20While%0Athis%20is%20a%20natural%20and%20easy%20framework%20for%20applying%20neural%20surrogates%2C%20it%20can%20be%0Aan%20over-simplified%20and%20rigid%20framework%20for%20predicting%20physics.%20In%20this%20work%2C%20we%0Apropose%20an%20alternative%20framework%20in%20which%20neural%20solvers%20predict%20the%20temporal%0Aderivative%20and%20an%20ODE%20integrator%20forwards%20the%20solution%20in%20time%2C%20which%20has%0Alittle%20overhead%20and%20is%20broadly%20applicable%20across%20model%20architectures%20and%20PDEs.%0AWe%20find%20that%20by%20simply%20changing%20the%20training%20target%20and%20introducing%20numerical%0Aintegration%20during%20inference%2C%20neural%20surrogates%20can%20gain%20accuracy%20and%0Astability.%20Predicting%20temporal%20derivatives%20also%20allows%20models%20to%20not%20be%0Aconstrained%20to%20a%20specific%20temporal%20discretization%2C%20allowing%20for%20flexible%0Atime-stepping%20during%20inference%20or%20training%20on%20higher-resolution%20PDE%20data.%0ALastly%2C%20we%20investigate%20why%20this%20new%20framework%20can%20be%20beneficial%20and%20in%20what%0Asituations%20does%20it%20work%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Change%252C%2520Not%2520States%253A%2520An%2520Alternate%2520Framework%2520for%2520Neural%2520PDE%250A%2520%2520Surrogates%26entry.906535625%3DAnthony%2520Zhou%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3D%2520%2520Neural%2520surrogates%2520for%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520have%2520become%250Apopular%2520due%2520to%2520their%2520potential%2520to%2520quickly%2520simulate%2520physics.%2520With%2520a%2520few%250Aexceptions%252C%2520neural%2520surrogates%2520generally%2520treat%2520the%2520forward%2520evolution%2520of%250Atime-dependent%2520PDEs%2520as%2520a%2520black%2520box%2520by%2520directly%2520predicting%2520the%2520next%2520state.%2520While%250Athis%2520is%2520a%2520natural%2520and%2520easy%2520framework%2520for%2520applying%2520neural%2520surrogates%252C%2520it%2520can%2520be%250Aan%2520over-simplified%2520and%2520rigid%2520framework%2520for%2520predicting%2520physics.%2520In%2520this%2520work%252C%2520we%250Apropose%2520an%2520alternative%2520framework%2520in%2520which%2520neural%2520solvers%2520predict%2520the%2520temporal%250Aderivative%2520and%2520an%2520ODE%2520integrator%2520forwards%2520the%2520solution%2520in%2520time%252C%2520which%2520has%250Alittle%2520overhead%2520and%2520is%2520broadly%2520applicable%2520across%2520model%2520architectures%2520and%2520PDEs.%250AWe%2520find%2520that%2520by%2520simply%2520changing%2520the%2520training%2520target%2520and%2520introducing%2520numerical%250Aintegration%2520during%2520inference%252C%2520neural%2520surrogates%2520can%2520gain%2520accuracy%2520and%250Astability.%2520Predicting%2520temporal%2520derivatives%2520also%2520allows%2520models%2520to%2520not%2520be%250Aconstrained%2520to%2520a%2520specific%2520temporal%2520discretization%252C%2520allowing%2520for%2520flexible%250Atime-stepping%2520during%2520inference%2520or%2520training%2520on%2520higher-resolution%2520PDE%2520data.%250ALastly%252C%2520we%2520investigate%2520why%2520this%2520new%2520framework%2520can%2520be%2520beneficial%2520and%2520in%2520what%250Asituations%2520does%2520it%2520work%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Change%2C%20Not%20States%3A%20An%20Alternate%20Framework%20for%20Neural%20PDE%0A%20%20Surrogates&entry.906535625=Anthony%20Zhou%20and%20Amir%20Barati%20Farimani&entry.1292438233=%20%20Neural%20surrogates%20for%20partial%20differential%20equations%20%28PDEs%29%20have%20become%0Apopular%20due%20to%20their%20potential%20to%20quickly%20simulate%20physics.%20With%20a%20few%0Aexceptions%2C%20neural%20surrogates%20generally%20treat%20the%20forward%20evolution%20of%0Atime-dependent%20PDEs%20as%20a%20black%20box%20by%20directly%20predicting%20the%20next%20state.%20While%0Athis%20is%20a%20natural%20and%20easy%20framework%20for%20applying%20neural%20surrogates%2C%20it%20can%20be%0Aan%20over-simplified%20and%20rigid%20framework%20for%20predicting%20physics.%20In%20this%20work%2C%20we%0Apropose%20an%20alternative%20framework%20in%20which%20neural%20solvers%20predict%20the%20temporal%0Aderivative%20and%20an%20ODE%20integrator%20forwards%20the%20solution%20in%20time%2C%20which%20has%0Alittle%20overhead%20and%20is%20broadly%20applicable%20across%20model%20architectures%20and%20PDEs.%0AWe%20find%20that%20by%20simply%20changing%20the%20training%20target%20and%20introducing%20numerical%0Aintegration%20during%20inference%2C%20neural%20surrogates%20can%20gain%20accuracy%20and%0Astability.%20Predicting%20temporal%20derivatives%20also%20allows%20models%20to%20not%20be%0Aconstrained%20to%20a%20specific%20temporal%20discretization%2C%20allowing%20for%20flexible%0Atime-stepping%20during%20inference%20or%20training%20on%20higher-resolution%20PDE%20data.%0ALastly%2C%20we%20investigate%20why%20this%20new%20framework%20can%20be%20beneficial%20and%20in%20what%0Asituations%20does%20it%20work%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13074v1&entry.124074799=Read"},
{"title": "Open-Set Heterogeneous Domain Adaptation: Theoretical Analysis and\n  Algorithm", "author": "Thai-Hoang Pham and Yuanlong Wang and Changchang Yin and Xueru Zhang and Ping Zhang", "abstract": "  Domain adaptation (DA) tackles the issue of distribution shift by learning a\nmodel from a source domain that generalizes to a target domain. However, most\nexisting DA methods are designed for scenarios where the source and target\ndomain data lie within the same feature space, which limits their applicability\nin real-world situations. Recently, heterogeneous DA (HeDA) methods have been\nintroduced to address the challenges posed by heterogeneous feature space\nbetween source and target domains. Despite their successes, current HeDA\ntechniques fall short when there is a mismatch in both feature and label\nspaces. To address this, this paper explores a new DA scenario called open-set\nHeDA (OSHeDA). In OSHeDA, the model must not only handle heterogeneity in\nfeature space but also identify samples belonging to novel classes. To tackle\nthis challenge, we first develop a novel theoretical framework that constructs\nlearning bounds for prediction error on target domain. Guided by this\nframework, we propose a new DA method called Representation Learning for OSHeDA\n(RL-OSHeDA). This method is designed to simultaneously transfer knowledge\nbetween heterogeneous data sources and identify novel classes. Experiments\nacross text, image, and clinical data demonstrate the effectiveness of our\nalgorithm. Model implementation is available at\n\\url{https://github.com/pth1993/OSHeDA}.\n", "link": "http://arxiv.org/abs/2412.13036v1", "date": "2024-12-17", "relevancy": 1.5424, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5258}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5007}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Set%20Heterogeneous%20Domain%20Adaptation%3A%20Theoretical%20Analysis%20and%0A%20%20Algorithm&body=Title%3A%20Open-Set%20Heterogeneous%20Domain%20Adaptation%3A%20Theoretical%20Analysis%20and%0A%20%20Algorithm%0AAuthor%3A%20Thai-Hoang%20Pham%20and%20Yuanlong%20Wang%20and%20Changchang%20Yin%20and%20Xueru%20Zhang%20and%20Ping%20Zhang%0AAbstract%3A%20%20%20Domain%20adaptation%20%28DA%29%20tackles%20the%20issue%20of%20distribution%20shift%20by%20learning%20a%0Amodel%20from%20a%20source%20domain%20that%20generalizes%20to%20a%20target%20domain.%20However%2C%20most%0Aexisting%20DA%20methods%20are%20designed%20for%20scenarios%20where%20the%20source%20and%20target%0Adomain%20data%20lie%20within%20the%20same%20feature%20space%2C%20which%20limits%20their%20applicability%0Ain%20real-world%20situations.%20Recently%2C%20heterogeneous%20DA%20%28HeDA%29%20methods%20have%20been%0Aintroduced%20to%20address%20the%20challenges%20posed%20by%20heterogeneous%20feature%20space%0Abetween%20source%20and%20target%20domains.%20Despite%20their%20successes%2C%20current%20HeDA%0Atechniques%20fall%20short%20when%20there%20is%20a%20mismatch%20in%20both%20feature%20and%20label%0Aspaces.%20To%20address%20this%2C%20this%20paper%20explores%20a%20new%20DA%20scenario%20called%20open-set%0AHeDA%20%28OSHeDA%29.%20In%20OSHeDA%2C%20the%20model%20must%20not%20only%20handle%20heterogeneity%20in%0Afeature%20space%20but%20also%20identify%20samples%20belonging%20to%20novel%20classes.%20To%20tackle%0Athis%20challenge%2C%20we%20first%20develop%20a%20novel%20theoretical%20framework%20that%20constructs%0Alearning%20bounds%20for%20prediction%20error%20on%20target%20domain.%20Guided%20by%20this%0Aframework%2C%20we%20propose%20a%20new%20DA%20method%20called%20Representation%20Learning%20for%20OSHeDA%0A%28RL-OSHeDA%29.%20This%20method%20is%20designed%20to%20simultaneously%20transfer%20knowledge%0Abetween%20heterogeneous%20data%20sources%20and%20identify%20novel%20classes.%20Experiments%0Aacross%20text%2C%20image%2C%20and%20clinical%20data%20demonstrate%20the%20effectiveness%20of%20our%0Aalgorithm.%20Model%20implementation%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/pth1993/OSHeDA%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Set%2520Heterogeneous%2520Domain%2520Adaptation%253A%2520Theoretical%2520Analysis%2520and%250A%2520%2520Algorithm%26entry.906535625%3DThai-Hoang%2520Pham%2520and%2520Yuanlong%2520Wang%2520and%2520Changchang%2520Yin%2520and%2520Xueru%2520Zhang%2520and%2520Ping%2520Zhang%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520%2528DA%2529%2520tackles%2520the%2520issue%2520of%2520distribution%2520shift%2520by%2520learning%2520a%250Amodel%2520from%2520a%2520source%2520domain%2520that%2520generalizes%2520to%2520a%2520target%2520domain.%2520However%252C%2520most%250Aexisting%2520DA%2520methods%2520are%2520designed%2520for%2520scenarios%2520where%2520the%2520source%2520and%2520target%250Adomain%2520data%2520lie%2520within%2520the%2520same%2520feature%2520space%252C%2520which%2520limits%2520their%2520applicability%250Ain%2520real-world%2520situations.%2520Recently%252C%2520heterogeneous%2520DA%2520%2528HeDA%2529%2520methods%2520have%2520been%250Aintroduced%2520to%2520address%2520the%2520challenges%2520posed%2520by%2520heterogeneous%2520feature%2520space%250Abetween%2520source%2520and%2520target%2520domains.%2520Despite%2520their%2520successes%252C%2520current%2520HeDA%250Atechniques%2520fall%2520short%2520when%2520there%2520is%2520a%2520mismatch%2520in%2520both%2520feature%2520and%2520label%250Aspaces.%2520To%2520address%2520this%252C%2520this%2520paper%2520explores%2520a%2520new%2520DA%2520scenario%2520called%2520open-set%250AHeDA%2520%2528OSHeDA%2529.%2520In%2520OSHeDA%252C%2520the%2520model%2520must%2520not%2520only%2520handle%2520heterogeneity%2520in%250Afeature%2520space%2520but%2520also%2520identify%2520samples%2520belonging%2520to%2520novel%2520classes.%2520To%2520tackle%250Athis%2520challenge%252C%2520we%2520first%2520develop%2520a%2520novel%2520theoretical%2520framework%2520that%2520constructs%250Alearning%2520bounds%2520for%2520prediction%2520error%2520on%2520target%2520domain.%2520Guided%2520by%2520this%250Aframework%252C%2520we%2520propose%2520a%2520new%2520DA%2520method%2520called%2520Representation%2520Learning%2520for%2520OSHeDA%250A%2528RL-OSHeDA%2529.%2520This%2520method%2520is%2520designed%2520to%2520simultaneously%2520transfer%2520knowledge%250Abetween%2520heterogeneous%2520data%2520sources%2520and%2520identify%2520novel%2520classes.%2520Experiments%250Aacross%2520text%252C%2520image%252C%2520and%2520clinical%2520data%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aalgorithm.%2520Model%2520implementation%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/pth1993/OSHeDA%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Set%20Heterogeneous%20Domain%20Adaptation%3A%20Theoretical%20Analysis%20and%0A%20%20Algorithm&entry.906535625=Thai-Hoang%20Pham%20and%20Yuanlong%20Wang%20and%20Changchang%20Yin%20and%20Xueru%20Zhang%20and%20Ping%20Zhang&entry.1292438233=%20%20Domain%20adaptation%20%28DA%29%20tackles%20the%20issue%20of%20distribution%20shift%20by%20learning%20a%0Amodel%20from%20a%20source%20domain%20that%20generalizes%20to%20a%20target%20domain.%20However%2C%20most%0Aexisting%20DA%20methods%20are%20designed%20for%20scenarios%20where%20the%20source%20and%20target%0Adomain%20data%20lie%20within%20the%20same%20feature%20space%2C%20which%20limits%20their%20applicability%0Ain%20real-world%20situations.%20Recently%2C%20heterogeneous%20DA%20%28HeDA%29%20methods%20have%20been%0Aintroduced%20to%20address%20the%20challenges%20posed%20by%20heterogeneous%20feature%20space%0Abetween%20source%20and%20target%20domains.%20Despite%20their%20successes%2C%20current%20HeDA%0Atechniques%20fall%20short%20when%20there%20is%20a%20mismatch%20in%20both%20feature%20and%20label%0Aspaces.%20To%20address%20this%2C%20this%20paper%20explores%20a%20new%20DA%20scenario%20called%20open-set%0AHeDA%20%28OSHeDA%29.%20In%20OSHeDA%2C%20the%20model%20must%20not%20only%20handle%20heterogeneity%20in%0Afeature%20space%20but%20also%20identify%20samples%20belonging%20to%20novel%20classes.%20To%20tackle%0Athis%20challenge%2C%20we%20first%20develop%20a%20novel%20theoretical%20framework%20that%20constructs%0Alearning%20bounds%20for%20prediction%20error%20on%20target%20domain.%20Guided%20by%20this%0Aframework%2C%20we%20propose%20a%20new%20DA%20method%20called%20Representation%20Learning%20for%20OSHeDA%0A%28RL-OSHeDA%29.%20This%20method%20is%20designed%20to%20simultaneously%20transfer%20knowledge%0Abetween%20heterogeneous%20data%20sources%20and%20identify%20novel%20classes.%20Experiments%0Aacross%20text%2C%20image%2C%20and%20clinical%20data%20demonstrate%20the%20effectiveness%20of%20our%0Aalgorithm.%20Model%20implementation%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/pth1993/OSHeDA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13036v1&entry.124074799=Read"},
{"title": "Identifying Bias in Deep Neural Networks Using Image Transforms", "author": "Sai Teja Erukude and Akhil Joshi and Lior Shamir", "abstract": "  CNNs have become one of the most commonly used computational tool in the past\ntwo decades. One of the primary downsides of CNNs is that they work as a\n``black box\", where the user cannot necessarily know how the image data are\nanalyzed, and therefore needs to rely on empirical evaluation to test the\nefficacy of a trained CNN. This can lead to hidden biases that affect the\nperformance evaluation of neural networks, but are difficult to identify. Here\nwe discuss examples of such hidden biases in common and widely used benchmark\ndatasets, and propose techniques for identifying dataset biases that can affect\nthe standard performance evaluation metrics. One effective approach to identify\ndataset bias is to perform image classification by using merely blank\nbackground parts of the original images. However, in some situations a blank\nbackground in the images is not available, making it more difficult to separate\nforeground or contextual information from the bias. To overcome this, we\npropose a method to identify dataset bias without the need to crop background\ninformation from the images. That method is based on applying several image\ntransforms to the original images, including Fourier transform, wavelet\ntransforms, median filter, and their combinations. These transforms were\napplied to recover background bias information that CNNs use to classify\nimages. This transformations affect the contextual visual information in a\ndifferent manner than it affects the systemic background bias. Therefore, the\nmethod can distinguish between contextual information and the bias, and alert\non the presence of background bias even without the need to separate sub-images\nparts from the blank background of the original images. Code used in the\nexperiments is publicly available.\n", "link": "http://arxiv.org/abs/2412.13079v1", "date": "2024-12-17", "relevancy": 1.5116, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5164}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5003}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Bias%20in%20Deep%20Neural%20Networks%20Using%20Image%20Transforms&body=Title%3A%20Identifying%20Bias%20in%20Deep%20Neural%20Networks%20Using%20Image%20Transforms%0AAuthor%3A%20Sai%20Teja%20Erukude%20and%20Akhil%20Joshi%20and%20Lior%20Shamir%0AAbstract%3A%20%20%20CNNs%20have%20become%20one%20of%20the%20most%20commonly%20used%20computational%20tool%20in%20the%20past%0Atwo%20decades.%20One%20of%20the%20primary%20downsides%20of%20CNNs%20is%20that%20they%20work%20as%20a%0A%60%60black%20box%22%2C%20where%20the%20user%20cannot%20necessarily%20know%20how%20the%20image%20data%20are%0Aanalyzed%2C%20and%20therefore%20needs%20to%20rely%20on%20empirical%20evaluation%20to%20test%20the%0Aefficacy%20of%20a%20trained%20CNN.%20This%20can%20lead%20to%20hidden%20biases%20that%20affect%20the%0Aperformance%20evaluation%20of%20neural%20networks%2C%20but%20are%20difficult%20to%20identify.%20Here%0Awe%20discuss%20examples%20of%20such%20hidden%20biases%20in%20common%20and%20widely%20used%20benchmark%0Adatasets%2C%20and%20propose%20techniques%20for%20identifying%20dataset%20biases%20that%20can%20affect%0Athe%20standard%20performance%20evaluation%20metrics.%20One%20effective%20approach%20to%20identify%0Adataset%20bias%20is%20to%20perform%20image%20classification%20by%20using%20merely%20blank%0Abackground%20parts%20of%20the%20original%20images.%20However%2C%20in%20some%20situations%20a%20blank%0Abackground%20in%20the%20images%20is%20not%20available%2C%20making%20it%20more%20difficult%20to%20separate%0Aforeground%20or%20contextual%20information%20from%20the%20bias.%20To%20overcome%20this%2C%20we%0Apropose%20a%20method%20to%20identify%20dataset%20bias%20without%20the%20need%20to%20crop%20background%0Ainformation%20from%20the%20images.%20That%20method%20is%20based%20on%20applying%20several%20image%0Atransforms%20to%20the%20original%20images%2C%20including%20Fourier%20transform%2C%20wavelet%0Atransforms%2C%20median%20filter%2C%20and%20their%20combinations.%20These%20transforms%20were%0Aapplied%20to%20recover%20background%20bias%20information%20that%20CNNs%20use%20to%20classify%0Aimages.%20This%20transformations%20affect%20the%20contextual%20visual%20information%20in%20a%0Adifferent%20manner%20than%20it%20affects%20the%20systemic%20background%20bias.%20Therefore%2C%20the%0Amethod%20can%20distinguish%20between%20contextual%20information%20and%20the%20bias%2C%20and%20alert%0Aon%20the%20presence%20of%20background%20bias%20even%20without%20the%20need%20to%20separate%20sub-images%0Aparts%20from%20the%20blank%20background%20of%20the%20original%20images.%20Code%20used%20in%20the%0Aexperiments%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Bias%2520in%2520Deep%2520Neural%2520Networks%2520Using%2520Image%2520Transforms%26entry.906535625%3DSai%2520Teja%2520Erukude%2520and%2520Akhil%2520Joshi%2520and%2520Lior%2520Shamir%26entry.1292438233%3D%2520%2520CNNs%2520have%2520become%2520one%2520of%2520the%2520most%2520commonly%2520used%2520computational%2520tool%2520in%2520the%2520past%250Atwo%2520decades.%2520One%2520of%2520the%2520primary%2520downsides%2520of%2520CNNs%2520is%2520that%2520they%2520work%2520as%2520a%250A%2560%2560black%2520box%2522%252C%2520where%2520the%2520user%2520cannot%2520necessarily%2520know%2520how%2520the%2520image%2520data%2520are%250Aanalyzed%252C%2520and%2520therefore%2520needs%2520to%2520rely%2520on%2520empirical%2520evaluation%2520to%2520test%2520the%250Aefficacy%2520of%2520a%2520trained%2520CNN.%2520This%2520can%2520lead%2520to%2520hidden%2520biases%2520that%2520affect%2520the%250Aperformance%2520evaluation%2520of%2520neural%2520networks%252C%2520but%2520are%2520difficult%2520to%2520identify.%2520Here%250Awe%2520discuss%2520examples%2520of%2520such%2520hidden%2520biases%2520in%2520common%2520and%2520widely%2520used%2520benchmark%250Adatasets%252C%2520and%2520propose%2520techniques%2520for%2520identifying%2520dataset%2520biases%2520that%2520can%2520affect%250Athe%2520standard%2520performance%2520evaluation%2520metrics.%2520One%2520effective%2520approach%2520to%2520identify%250Adataset%2520bias%2520is%2520to%2520perform%2520image%2520classification%2520by%2520using%2520merely%2520blank%250Abackground%2520parts%2520of%2520the%2520original%2520images.%2520However%252C%2520in%2520some%2520situations%2520a%2520blank%250Abackground%2520in%2520the%2520images%2520is%2520not%2520available%252C%2520making%2520it%2520more%2520difficult%2520to%2520separate%250Aforeground%2520or%2520contextual%2520information%2520from%2520the%2520bias.%2520To%2520overcome%2520this%252C%2520we%250Apropose%2520a%2520method%2520to%2520identify%2520dataset%2520bias%2520without%2520the%2520need%2520to%2520crop%2520background%250Ainformation%2520from%2520the%2520images.%2520That%2520method%2520is%2520based%2520on%2520applying%2520several%2520image%250Atransforms%2520to%2520the%2520original%2520images%252C%2520including%2520Fourier%2520transform%252C%2520wavelet%250Atransforms%252C%2520median%2520filter%252C%2520and%2520their%2520combinations.%2520These%2520transforms%2520were%250Aapplied%2520to%2520recover%2520background%2520bias%2520information%2520that%2520CNNs%2520use%2520to%2520classify%250Aimages.%2520This%2520transformations%2520affect%2520the%2520contextual%2520visual%2520information%2520in%2520a%250Adifferent%2520manner%2520than%2520it%2520affects%2520the%2520systemic%2520background%2520bias.%2520Therefore%252C%2520the%250Amethod%2520can%2520distinguish%2520between%2520contextual%2520information%2520and%2520the%2520bias%252C%2520and%2520alert%250Aon%2520the%2520presence%2520of%2520background%2520bias%2520even%2520without%2520the%2520need%2520to%2520separate%2520sub-images%250Aparts%2520from%2520the%2520blank%2520background%2520of%2520the%2520original%2520images.%2520Code%2520used%2520in%2520the%250Aexperiments%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Bias%20in%20Deep%20Neural%20Networks%20Using%20Image%20Transforms&entry.906535625=Sai%20Teja%20Erukude%20and%20Akhil%20Joshi%20and%20Lior%20Shamir&entry.1292438233=%20%20CNNs%20have%20become%20one%20of%20the%20most%20commonly%20used%20computational%20tool%20in%20the%20past%0Atwo%20decades.%20One%20of%20the%20primary%20downsides%20of%20CNNs%20is%20that%20they%20work%20as%20a%0A%60%60black%20box%22%2C%20where%20the%20user%20cannot%20necessarily%20know%20how%20the%20image%20data%20are%0Aanalyzed%2C%20and%20therefore%20needs%20to%20rely%20on%20empirical%20evaluation%20to%20test%20the%0Aefficacy%20of%20a%20trained%20CNN.%20This%20can%20lead%20to%20hidden%20biases%20that%20affect%20the%0Aperformance%20evaluation%20of%20neural%20networks%2C%20but%20are%20difficult%20to%20identify.%20Here%0Awe%20discuss%20examples%20of%20such%20hidden%20biases%20in%20common%20and%20widely%20used%20benchmark%0Adatasets%2C%20and%20propose%20techniques%20for%20identifying%20dataset%20biases%20that%20can%20affect%0Athe%20standard%20performance%20evaluation%20metrics.%20One%20effective%20approach%20to%20identify%0Adataset%20bias%20is%20to%20perform%20image%20classification%20by%20using%20merely%20blank%0Abackground%20parts%20of%20the%20original%20images.%20However%2C%20in%20some%20situations%20a%20blank%0Abackground%20in%20the%20images%20is%20not%20available%2C%20making%20it%20more%20difficult%20to%20separate%0Aforeground%20or%20contextual%20information%20from%20the%20bias.%20To%20overcome%20this%2C%20we%0Apropose%20a%20method%20to%20identify%20dataset%20bias%20without%20the%20need%20to%20crop%20background%0Ainformation%20from%20the%20images.%20That%20method%20is%20based%20on%20applying%20several%20image%0Atransforms%20to%20the%20original%20images%2C%20including%20Fourier%20transform%2C%20wavelet%0Atransforms%2C%20median%20filter%2C%20and%20their%20combinations.%20These%20transforms%20were%0Aapplied%20to%20recover%20background%20bias%20information%20that%20CNNs%20use%20to%20classify%0Aimages.%20This%20transformations%20affect%20the%20contextual%20visual%20information%20in%20a%0Adifferent%20manner%20than%20it%20affects%20the%20systemic%20background%20bias.%20Therefore%2C%20the%0Amethod%20can%20distinguish%20between%20contextual%20information%20and%20the%20bias%2C%20and%20alert%0Aon%20the%20presence%20of%20background%20bias%20even%20without%20the%20need%20to%20separate%20sub-images%0Aparts%20from%20the%20blank%20background%20of%20the%20original%20images.%20Code%20used%20in%20the%0Aexperiments%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13079v1&entry.124074799=Read"},
{"title": "Flight Patterns for Swarms of Drones", "author": "Shuqin Zhu and Shahram Ghandeharizadeh", "abstract": "  We present flight patterns for a collision-free passage of swarms of drones\nthrough one or more openings. The narrow openings provide drones with access to\nan infrastructure component such as charging stations to charge their depleted\nbatteries and hangars for storage. The flight patterns are a staging area\n(queues) that match the rate at which an infrastructure component and its\nopenings process drones. They prevent collisions and may implement different\npolicies that control the order in which drones pass through an opening. We\nillustrate the flight patterns with a 3D display that uses drones configured\nwith light sources to illuminate shapes.\n", "link": "http://arxiv.org/abs/2412.13119v1", "date": "2024-12-17", "relevancy": 1.5058, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3774}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3764}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flight%20Patterns%20for%20Swarms%20of%20Drones&body=Title%3A%20Flight%20Patterns%20for%20Swarms%20of%20Drones%0AAuthor%3A%20Shuqin%20Zhu%20and%20Shahram%20Ghandeharizadeh%0AAbstract%3A%20%20%20We%20present%20flight%20patterns%20for%20a%20collision-free%20passage%20of%20swarms%20of%20drones%0Athrough%20one%20or%20more%20openings.%20The%20narrow%20openings%20provide%20drones%20with%20access%20to%0Aan%20infrastructure%20component%20such%20as%20charging%20stations%20to%20charge%20their%20depleted%0Abatteries%20and%20hangars%20for%20storage.%20The%20flight%20patterns%20are%20a%20staging%20area%0A%28queues%29%20that%20match%20the%20rate%20at%20which%20an%20infrastructure%20component%20and%20its%0Aopenings%20process%20drones.%20They%20prevent%20collisions%20and%20may%20implement%20different%0Apolicies%20that%20control%20the%20order%20in%20which%20drones%20pass%20through%20an%20opening.%20We%0Aillustrate%20the%20flight%20patterns%20with%20a%203D%20display%20that%20uses%20drones%20configured%0Awith%20light%20sources%20to%20illuminate%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlight%2520Patterns%2520for%2520Swarms%2520of%2520Drones%26entry.906535625%3DShuqin%2520Zhu%2520and%2520Shahram%2520Ghandeharizadeh%26entry.1292438233%3D%2520%2520We%2520present%2520flight%2520patterns%2520for%2520a%2520collision-free%2520passage%2520of%2520swarms%2520of%2520drones%250Athrough%2520one%2520or%2520more%2520openings.%2520The%2520narrow%2520openings%2520provide%2520drones%2520with%2520access%2520to%250Aan%2520infrastructure%2520component%2520such%2520as%2520charging%2520stations%2520to%2520charge%2520their%2520depleted%250Abatteries%2520and%2520hangars%2520for%2520storage.%2520The%2520flight%2520patterns%2520are%2520a%2520staging%2520area%250A%2528queues%2529%2520that%2520match%2520the%2520rate%2520at%2520which%2520an%2520infrastructure%2520component%2520and%2520its%250Aopenings%2520process%2520drones.%2520They%2520prevent%2520collisions%2520and%2520may%2520implement%2520different%250Apolicies%2520that%2520control%2520the%2520order%2520in%2520which%2520drones%2520pass%2520through%2520an%2520opening.%2520We%250Aillustrate%2520the%2520flight%2520patterns%2520with%2520a%25203D%2520display%2520that%2520uses%2520drones%2520configured%250Awith%2520light%2520sources%2520to%2520illuminate%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flight%20Patterns%20for%20Swarms%20of%20Drones&entry.906535625=Shuqin%20Zhu%20and%20Shahram%20Ghandeharizadeh&entry.1292438233=%20%20We%20present%20flight%20patterns%20for%20a%20collision-free%20passage%20of%20swarms%20of%20drones%0Athrough%20one%20or%20more%20openings.%20The%20narrow%20openings%20provide%20drones%20with%20access%20to%0Aan%20infrastructure%20component%20such%20as%20charging%20stations%20to%20charge%20their%20depleted%0Abatteries%20and%20hangars%20for%20storage.%20The%20flight%20patterns%20are%20a%20staging%20area%0A%28queues%29%20that%20match%20the%20rate%20at%20which%20an%20infrastructure%20component%20and%20its%0Aopenings%20process%20drones.%20They%20prevent%20collisions%20and%20may%20implement%20different%0Apolicies%20that%20control%20the%20order%20in%20which%20drones%20pass%20through%20an%20opening.%20We%0Aillustrate%20the%20flight%20patterns%20with%20a%203D%20display%20that%20uses%20drones%20configured%0Awith%20light%20sources%20to%20illuminate%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13119v1&entry.124074799=Read"},
{"title": "On Distilling the Displacement Knowledge for Few-Shot Class-Incremental\n  Learning", "author": "Pengfei Fang and Yongchun Qin and Hui Xue", "abstract": "  Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of\nevolving data distributions and the difficulty of data acquisition in\nreal-world scenarios. To counteract the catastrophic forgetting typically\nencountered in FSCIL, knowledge distillation is employed as a way to maintain\nthe knowledge from learned data distribution. Recognizing the limitations of\ngenerating discriminative feature representations in a few-shot context, our\napproach incorporates structural information between samples into knowledge\ndistillation. This structural information serves as a remedy for the low\nquality of features. Diverging from traditional structured distillation methods\nthat compute sample similarity, we introduce the Displacement Knowledge\nDistillation (DKD) method. DKD utilizes displacement rather than similarity\nbetween samples, incorporating both distance and angular information to\nsignificantly enhance the information density retained through knowledge\ndistillation. Observing performance disparities in feature distribution between\nbase and novel classes, we propose the Dual Distillation Network (DDNet). This\nnetwork applies traditional knowledge distillation to base classes and DKD to\nnovel classes, challenging the conventional integration of novel classes with\nbase classes. Additionally, we implement an instance-aware sample selector\nduring inference to dynamically adjust dual branch weights, thereby leveraging\nthe complementary strengths of each approach. Extensive testing on three\nbenchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover,\nthrough rigorous experimentation and comparison, we establish the robustness\nand general applicability of our proposed DKD method.\n", "link": "http://arxiv.org/abs/2412.11017v2", "date": "2024-12-17", "relevancy": 1.4988, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5167}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4953}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Distilling%20the%20Displacement%20Knowledge%20for%20Few-Shot%20Class-Incremental%0A%20%20Learning&body=Title%3A%20On%20Distilling%20the%20Displacement%20Knowledge%20for%20Few-Shot%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Pengfei%20Fang%20and%20Yongchun%20Qin%20and%20Hui%20Xue%0AAbstract%3A%20%20%20Few-shot%20Class-Incremental%20Learning%20%28FSCIL%29%20addresses%20the%20challenges%20of%0Aevolving%20data%20distributions%20and%20the%20difficulty%20of%20data%20acquisition%20in%0Areal-world%20scenarios.%20To%20counteract%20the%20catastrophic%20forgetting%20typically%0Aencountered%20in%20FSCIL%2C%20knowledge%20distillation%20is%20employed%20as%20a%20way%20to%20maintain%0Athe%20knowledge%20from%20learned%20data%20distribution.%20Recognizing%20the%20limitations%20of%0Agenerating%20discriminative%20feature%20representations%20in%20a%20few-shot%20context%2C%20our%0Aapproach%20incorporates%20structural%20information%20between%20samples%20into%20knowledge%0Adistillation.%20This%20structural%20information%20serves%20as%20a%20remedy%20for%20the%20low%0Aquality%20of%20features.%20Diverging%20from%20traditional%20structured%20distillation%20methods%0Athat%20compute%20sample%20similarity%2C%20we%20introduce%20the%20Displacement%20Knowledge%0ADistillation%20%28DKD%29%20method.%20DKD%20utilizes%20displacement%20rather%20than%20similarity%0Abetween%20samples%2C%20incorporating%20both%20distance%20and%20angular%20information%20to%0Asignificantly%20enhance%20the%20information%20density%20retained%20through%20knowledge%0Adistillation.%20Observing%20performance%20disparities%20in%20feature%20distribution%20between%0Abase%20and%20novel%20classes%2C%20we%20propose%20the%20Dual%20Distillation%20Network%20%28DDNet%29.%20This%0Anetwork%20applies%20traditional%20knowledge%20distillation%20to%20base%20classes%20and%20DKD%20to%0Anovel%20classes%2C%20challenging%20the%20conventional%20integration%20of%20novel%20classes%20with%0Abase%20classes.%20Additionally%2C%20we%20implement%20an%20instance-aware%20sample%20selector%0Aduring%20inference%20to%20dynamically%20adjust%20dual%20branch%20weights%2C%20thereby%20leveraging%0Athe%20complementary%20strengths%20of%20each%20approach.%20Extensive%20testing%20on%20three%0Abenchmarks%20demonstrates%20that%20DDNet%20achieves%20state-of-the-art%20results.%20Moreover%2C%0Athrough%20rigorous%20experimentation%20and%20comparison%2C%20we%20establish%20the%20robustness%0Aand%20general%20applicability%20of%20our%20proposed%20DKD%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11017v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Distilling%2520the%2520Displacement%2520Knowledge%2520for%2520Few-Shot%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DPengfei%2520Fang%2520and%2520Yongchun%2520Qin%2520and%2520Hui%2520Xue%26entry.1292438233%3D%2520%2520Few-shot%2520Class-Incremental%2520Learning%2520%2528FSCIL%2529%2520addresses%2520the%2520challenges%2520of%250Aevolving%2520data%2520distributions%2520and%2520the%2520difficulty%2520of%2520data%2520acquisition%2520in%250Areal-world%2520scenarios.%2520To%2520counteract%2520the%2520catastrophic%2520forgetting%2520typically%250Aencountered%2520in%2520FSCIL%252C%2520knowledge%2520distillation%2520is%2520employed%2520as%2520a%2520way%2520to%2520maintain%250Athe%2520knowledge%2520from%2520learned%2520data%2520distribution.%2520Recognizing%2520the%2520limitations%2520of%250Agenerating%2520discriminative%2520feature%2520representations%2520in%2520a%2520few-shot%2520context%252C%2520our%250Aapproach%2520incorporates%2520structural%2520information%2520between%2520samples%2520into%2520knowledge%250Adistillation.%2520This%2520structural%2520information%2520serves%2520as%2520a%2520remedy%2520for%2520the%2520low%250Aquality%2520of%2520features.%2520Diverging%2520from%2520traditional%2520structured%2520distillation%2520methods%250Athat%2520compute%2520sample%2520similarity%252C%2520we%2520introduce%2520the%2520Displacement%2520Knowledge%250ADistillation%2520%2528DKD%2529%2520method.%2520DKD%2520utilizes%2520displacement%2520rather%2520than%2520similarity%250Abetween%2520samples%252C%2520incorporating%2520both%2520distance%2520and%2520angular%2520information%2520to%250Asignificantly%2520enhance%2520the%2520information%2520density%2520retained%2520through%2520knowledge%250Adistillation.%2520Observing%2520performance%2520disparities%2520in%2520feature%2520distribution%2520between%250Abase%2520and%2520novel%2520classes%252C%2520we%2520propose%2520the%2520Dual%2520Distillation%2520Network%2520%2528DDNet%2529.%2520This%250Anetwork%2520applies%2520traditional%2520knowledge%2520distillation%2520to%2520base%2520classes%2520and%2520DKD%2520to%250Anovel%2520classes%252C%2520challenging%2520the%2520conventional%2520integration%2520of%2520novel%2520classes%2520with%250Abase%2520classes.%2520Additionally%252C%2520we%2520implement%2520an%2520instance-aware%2520sample%2520selector%250Aduring%2520inference%2520to%2520dynamically%2520adjust%2520dual%2520branch%2520weights%252C%2520thereby%2520leveraging%250Athe%2520complementary%2520strengths%2520of%2520each%2520approach.%2520Extensive%2520testing%2520on%2520three%250Abenchmarks%2520demonstrates%2520that%2520DDNet%2520achieves%2520state-of-the-art%2520results.%2520Moreover%252C%250Athrough%2520rigorous%2520experimentation%2520and%2520comparison%252C%2520we%2520establish%2520the%2520robustness%250Aand%2520general%2520applicability%2520of%2520our%2520proposed%2520DKD%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11017v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Distilling%20the%20Displacement%20Knowledge%20for%20Few-Shot%20Class-Incremental%0A%20%20Learning&entry.906535625=Pengfei%20Fang%20and%20Yongchun%20Qin%20and%20Hui%20Xue&entry.1292438233=%20%20Few-shot%20Class-Incremental%20Learning%20%28FSCIL%29%20addresses%20the%20challenges%20of%0Aevolving%20data%20distributions%20and%20the%20difficulty%20of%20data%20acquisition%20in%0Areal-world%20scenarios.%20To%20counteract%20the%20catastrophic%20forgetting%20typically%0Aencountered%20in%20FSCIL%2C%20knowledge%20distillation%20is%20employed%20as%20a%20way%20to%20maintain%0Athe%20knowledge%20from%20learned%20data%20distribution.%20Recognizing%20the%20limitations%20of%0Agenerating%20discriminative%20feature%20representations%20in%20a%20few-shot%20context%2C%20our%0Aapproach%20incorporates%20structural%20information%20between%20samples%20into%20knowledge%0Adistillation.%20This%20structural%20information%20serves%20as%20a%20remedy%20for%20the%20low%0Aquality%20of%20features.%20Diverging%20from%20traditional%20structured%20distillation%20methods%0Athat%20compute%20sample%20similarity%2C%20we%20introduce%20the%20Displacement%20Knowledge%0ADistillation%20%28DKD%29%20method.%20DKD%20utilizes%20displacement%20rather%20than%20similarity%0Abetween%20samples%2C%20incorporating%20both%20distance%20and%20angular%20information%20to%0Asignificantly%20enhance%20the%20information%20density%20retained%20through%20knowledge%0Adistillation.%20Observing%20performance%20disparities%20in%20feature%20distribution%20between%0Abase%20and%20novel%20classes%2C%20we%20propose%20the%20Dual%20Distillation%20Network%20%28DDNet%29.%20This%0Anetwork%20applies%20traditional%20knowledge%20distillation%20to%20base%20classes%20and%20DKD%20to%0Anovel%20classes%2C%20challenging%20the%20conventional%20integration%20of%20novel%20classes%20with%0Abase%20classes.%20Additionally%2C%20we%20implement%20an%20instance-aware%20sample%20selector%0Aduring%20inference%20to%20dynamically%20adjust%20dual%20branch%20weights%2C%20thereby%20leveraging%0Athe%20complementary%20strengths%20of%20each%20approach.%20Extensive%20testing%20on%20three%0Abenchmarks%20demonstrates%20that%20DDNet%20achieves%20state-of-the-art%20results.%20Moreover%2C%0Athrough%20rigorous%20experimentation%20and%20comparison%2C%20we%20establish%20the%20robustness%0Aand%20general%20applicability%20of%20our%20proposed%20DKD%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11017v2&entry.124074799=Read"},
{"title": "AI PERSONA: Towards Life-long Personalization of LLMs", "author": "Tiannan Wang and Meiling Tao and Ruoyu Fang and Huilin Wang and Shuai Wang and Yuchen Eleanor Jiang and Wangchunshu Zhou", "abstract": "  In this work, we introduce the task of life-long personalization of large\nlanguage models. While recent mainstream efforts in the LLM community mainly\nfocus on scaling data and compute for improved capabilities of LLMs, we argue\nthat it is also very important to enable LLM systems, or language agents, to\ncontinuously adapt to the diverse and ever-changing profiles of every distinct\nuser and provide up-to-date personalized assistance. We provide a clear task\nformulation and introduce a simple, general, effective, and scalable framework\nfor life-long personalization of LLM systems and language agents. To facilitate\nfuture research on LLM personalization, we also introduce methods to synthesize\nrealistic benchmarks and robust evaluation metrics. We will release all codes\nand data for building and benchmarking life-long personalized LLM systems.\n", "link": "http://arxiv.org/abs/2412.13103v1", "date": "2024-12-17", "relevancy": 1.4919, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4867}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20PERSONA%3A%20Towards%20Life-long%20Personalization%20of%20LLMs&body=Title%3A%20AI%20PERSONA%3A%20Towards%20Life-long%20Personalization%20of%20LLMs%0AAuthor%3A%20Tiannan%20Wang%20and%20Meiling%20Tao%20and%20Ruoyu%20Fang%20and%20Huilin%20Wang%20and%20Shuai%20Wang%20and%20Yuchen%20Eleanor%20Jiang%20and%20Wangchunshu%20Zhou%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20the%20task%20of%20life-long%20personalization%20of%20large%0Alanguage%20models.%20While%20recent%20mainstream%20efforts%20in%20the%20LLM%20community%20mainly%0Afocus%20on%20scaling%20data%20and%20compute%20for%20improved%20capabilities%20of%20LLMs%2C%20we%20argue%0Athat%20it%20is%20also%20very%20important%20to%20enable%20LLM%20systems%2C%20or%20language%20agents%2C%20to%0Acontinuously%20adapt%20to%20the%20diverse%20and%20ever-changing%20profiles%20of%20every%20distinct%0Auser%20and%20provide%20up-to-date%20personalized%20assistance.%20We%20provide%20a%20clear%20task%0Aformulation%20and%20introduce%20a%20simple%2C%20general%2C%20effective%2C%20and%20scalable%20framework%0Afor%20life-long%20personalization%20of%20LLM%20systems%20and%20language%20agents.%20To%20facilitate%0Afuture%20research%20on%20LLM%20personalization%2C%20we%20also%20introduce%20methods%20to%20synthesize%0Arealistic%20benchmarks%20and%20robust%20evaluation%20metrics.%20We%20will%20release%20all%20codes%0Aand%20data%20for%20building%20and%20benchmarking%20life-long%20personalized%20LLM%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520PERSONA%253A%2520Towards%2520Life-long%2520Personalization%2520of%2520LLMs%26entry.906535625%3DTiannan%2520Wang%2520and%2520Meiling%2520Tao%2520and%2520Ruoyu%2520Fang%2520and%2520Huilin%2520Wang%2520and%2520Shuai%2520Wang%2520and%2520Yuchen%2520Eleanor%2520Jiang%2520and%2520Wangchunshu%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520task%2520of%2520life-long%2520personalization%2520of%2520large%250Alanguage%2520models.%2520While%2520recent%2520mainstream%2520efforts%2520in%2520the%2520LLM%2520community%2520mainly%250Afocus%2520on%2520scaling%2520data%2520and%2520compute%2520for%2520improved%2520capabilities%2520of%2520LLMs%252C%2520we%2520argue%250Athat%2520it%2520is%2520also%2520very%2520important%2520to%2520enable%2520LLM%2520systems%252C%2520or%2520language%2520agents%252C%2520to%250Acontinuously%2520adapt%2520to%2520the%2520diverse%2520and%2520ever-changing%2520profiles%2520of%2520every%2520distinct%250Auser%2520and%2520provide%2520up-to-date%2520personalized%2520assistance.%2520We%2520provide%2520a%2520clear%2520task%250Aformulation%2520and%2520introduce%2520a%2520simple%252C%2520general%252C%2520effective%252C%2520and%2520scalable%2520framework%250Afor%2520life-long%2520personalization%2520of%2520LLM%2520systems%2520and%2520language%2520agents.%2520To%2520facilitate%250Afuture%2520research%2520on%2520LLM%2520personalization%252C%2520we%2520also%2520introduce%2520methods%2520to%2520synthesize%250Arealistic%2520benchmarks%2520and%2520robust%2520evaluation%2520metrics.%2520We%2520will%2520release%2520all%2520codes%250Aand%2520data%2520for%2520building%2520and%2520benchmarking%2520life-long%2520personalized%2520LLM%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20PERSONA%3A%20Towards%20Life-long%20Personalization%20of%20LLMs&entry.906535625=Tiannan%20Wang%20and%20Meiling%20Tao%20and%20Ruoyu%20Fang%20and%20Huilin%20Wang%20and%20Shuai%20Wang%20and%20Yuchen%20Eleanor%20Jiang%20and%20Wangchunshu%20Zhou&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20the%20task%20of%20life-long%20personalization%20of%20large%0Alanguage%20models.%20While%20recent%20mainstream%20efforts%20in%20the%20LLM%20community%20mainly%0Afocus%20on%20scaling%20data%20and%20compute%20for%20improved%20capabilities%20of%20LLMs%2C%20we%20argue%0Athat%20it%20is%20also%20very%20important%20to%20enable%20LLM%20systems%2C%20or%20language%20agents%2C%20to%0Acontinuously%20adapt%20to%20the%20diverse%20and%20ever-changing%20profiles%20of%20every%20distinct%0Auser%20and%20provide%20up-to-date%20personalized%20assistance.%20We%20provide%20a%20clear%20task%0Aformulation%20and%20introduce%20a%20simple%2C%20general%2C%20effective%2C%20and%20scalable%20framework%0Afor%20life-long%20personalization%20of%20LLM%20systems%20and%20language%20agents.%20To%20facilitate%0Afuture%20research%20on%20LLM%20personalization%2C%20we%20also%20introduce%20methods%20to%20synthesize%0Arealistic%20benchmarks%20and%20robust%20evaluation%20metrics.%20We%20will%20release%20all%20codes%0Aand%20data%20for%20building%20and%20benchmarking%20life-long%20personalized%20LLM%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13103v1&entry.124074799=Read"},
{"title": "Accuracy Limits as a Barrier to Biometric System Security", "author": "Axel Durbet and Paul-Marie Grollemund and Pascal Lafourcade and Kevin Thiry-Atighehchi", "abstract": "  Biometric systems are widely used for identity verification and\nidentification, including authentication (i.e., one-to-one matching to verify a\nclaimed identity) and identification (i.e., one-to-many matching to find a\nsubject in a database). The matching process relies on measuring similarities\nor dissimilarities between a fresh biometric template and enrolled templates.\nThe False Match Rate FMR is a key metric for assessing the accuracy and\nreliability of such systems. This paper analyzes biometric systems based on\ntheir FMR, with two main contributions. First, we explore untargeted attacks,\nwhere an adversary aims to impersonate any user within a database. We determine\nthe number of trials required for an attacker to successfully impersonate a\nuser and derive the critical population size (i.e., the maximum number of users\nin the database) required to maintain a given level of security. Furthermore,\nwe compute the critical FMR value needed to ensure resistance against\nuntargeted attacks as the database size increases. Second, we revisit the\nbiometric birthday problem to evaluate the approximate and exact probabilities\nthat two users in a database collide (i.e., can impersonate each other). Based\non this analysis, we derive both the approximate critical population size and\nthe critical FMR value needed to bound the likelihood of such collisions\noccurring with a given probability. These thresholds offer insights for\ndesigning systems that mitigate the risk of impersonation and collisions,\nparticularly in large-scale biometric databases. Our findings indicate that\ncurrent biometric systems fail to deliver sufficient accuracy to achieve an\nadequate security level against untargeted attacks, even in small-scale\ndatabases. Moreover, state-of-the-art systems face significant challenges in\naddressing the biometric birthday problem, especially as database sizes grow.\n", "link": "http://arxiv.org/abs/2412.13099v1", "date": "2024-12-17", "relevancy": 1.4903, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3893}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.361}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.3596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accuracy%20Limits%20as%20a%20Barrier%20to%20Biometric%20System%20Security&body=Title%3A%20Accuracy%20Limits%20as%20a%20Barrier%20to%20Biometric%20System%20Security%0AAuthor%3A%20Axel%20Durbet%20and%20Paul-Marie%20Grollemund%20and%20Pascal%20Lafourcade%20and%20Kevin%20Thiry-Atighehchi%0AAbstract%3A%20%20%20Biometric%20systems%20are%20widely%20used%20for%20identity%20verification%20and%0Aidentification%2C%20including%20authentication%20%28i.e.%2C%20one-to-one%20matching%20to%20verify%20a%0Aclaimed%20identity%29%20and%20identification%20%28i.e.%2C%20one-to-many%20matching%20to%20find%20a%0Asubject%20in%20a%20database%29.%20The%20matching%20process%20relies%20on%20measuring%20similarities%0Aor%20dissimilarities%20between%20a%20fresh%20biometric%20template%20and%20enrolled%20templates.%0AThe%20False%20Match%20Rate%20FMR%20is%20a%20key%20metric%20for%20assessing%20the%20accuracy%20and%0Areliability%20of%20such%20systems.%20This%20paper%20analyzes%20biometric%20systems%20based%20on%0Atheir%20FMR%2C%20with%20two%20main%20contributions.%20First%2C%20we%20explore%20untargeted%20attacks%2C%0Awhere%20an%20adversary%20aims%20to%20impersonate%20any%20user%20within%20a%20database.%20We%20determine%0Athe%20number%20of%20trials%20required%20for%20an%20attacker%20to%20successfully%20impersonate%20a%0Auser%20and%20derive%20the%20critical%20population%20size%20%28i.e.%2C%20the%20maximum%20number%20of%20users%0Ain%20the%20database%29%20required%20to%20maintain%20a%20given%20level%20of%20security.%20Furthermore%2C%0Awe%20compute%20the%20critical%20FMR%20value%20needed%20to%20ensure%20resistance%20against%0Auntargeted%20attacks%20as%20the%20database%20size%20increases.%20Second%2C%20we%20revisit%20the%0Abiometric%20birthday%20problem%20to%20evaluate%20the%20approximate%20and%20exact%20probabilities%0Athat%20two%20users%20in%20a%20database%20collide%20%28i.e.%2C%20can%20impersonate%20each%20other%29.%20Based%0Aon%20this%20analysis%2C%20we%20derive%20both%20the%20approximate%20critical%20population%20size%20and%0Athe%20critical%20FMR%20value%20needed%20to%20bound%20the%20likelihood%20of%20such%20collisions%0Aoccurring%20with%20a%20given%20probability.%20These%20thresholds%20offer%20insights%20for%0Adesigning%20systems%20that%20mitigate%20the%20risk%20of%20impersonation%20and%20collisions%2C%0Aparticularly%20in%20large-scale%20biometric%20databases.%20Our%20findings%20indicate%20that%0Acurrent%20biometric%20systems%20fail%20to%20deliver%20sufficient%20accuracy%20to%20achieve%20an%0Aadequate%20security%20level%20against%20untargeted%20attacks%2C%20even%20in%20small-scale%0Adatabases.%20Moreover%2C%20state-of-the-art%20systems%20face%20significant%20challenges%20in%0Aaddressing%20the%20biometric%20birthday%20problem%2C%20especially%20as%20database%20sizes%20grow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccuracy%2520Limits%2520as%2520a%2520Barrier%2520to%2520Biometric%2520System%2520Security%26entry.906535625%3DAxel%2520Durbet%2520and%2520Paul-Marie%2520Grollemund%2520and%2520Pascal%2520Lafourcade%2520and%2520Kevin%2520Thiry-Atighehchi%26entry.1292438233%3D%2520%2520Biometric%2520systems%2520are%2520widely%2520used%2520for%2520identity%2520verification%2520and%250Aidentification%252C%2520including%2520authentication%2520%2528i.e.%252C%2520one-to-one%2520matching%2520to%2520verify%2520a%250Aclaimed%2520identity%2529%2520and%2520identification%2520%2528i.e.%252C%2520one-to-many%2520matching%2520to%2520find%2520a%250Asubject%2520in%2520a%2520database%2529.%2520The%2520matching%2520process%2520relies%2520on%2520measuring%2520similarities%250Aor%2520dissimilarities%2520between%2520a%2520fresh%2520biometric%2520template%2520and%2520enrolled%2520templates.%250AThe%2520False%2520Match%2520Rate%2520FMR%2520is%2520a%2520key%2520metric%2520for%2520assessing%2520the%2520accuracy%2520and%250Areliability%2520of%2520such%2520systems.%2520This%2520paper%2520analyzes%2520biometric%2520systems%2520based%2520on%250Atheir%2520FMR%252C%2520with%2520two%2520main%2520contributions.%2520First%252C%2520we%2520explore%2520untargeted%2520attacks%252C%250Awhere%2520an%2520adversary%2520aims%2520to%2520impersonate%2520any%2520user%2520within%2520a%2520database.%2520We%2520determine%250Athe%2520number%2520of%2520trials%2520required%2520for%2520an%2520attacker%2520to%2520successfully%2520impersonate%2520a%250Auser%2520and%2520derive%2520the%2520critical%2520population%2520size%2520%2528i.e.%252C%2520the%2520maximum%2520number%2520of%2520users%250Ain%2520the%2520database%2529%2520required%2520to%2520maintain%2520a%2520given%2520level%2520of%2520security.%2520Furthermore%252C%250Awe%2520compute%2520the%2520critical%2520FMR%2520value%2520needed%2520to%2520ensure%2520resistance%2520against%250Auntargeted%2520attacks%2520as%2520the%2520database%2520size%2520increases.%2520Second%252C%2520we%2520revisit%2520the%250Abiometric%2520birthday%2520problem%2520to%2520evaluate%2520the%2520approximate%2520and%2520exact%2520probabilities%250Athat%2520two%2520users%2520in%2520a%2520database%2520collide%2520%2528i.e.%252C%2520can%2520impersonate%2520each%2520other%2529.%2520Based%250Aon%2520this%2520analysis%252C%2520we%2520derive%2520both%2520the%2520approximate%2520critical%2520population%2520size%2520and%250Athe%2520critical%2520FMR%2520value%2520needed%2520to%2520bound%2520the%2520likelihood%2520of%2520such%2520collisions%250Aoccurring%2520with%2520a%2520given%2520probability.%2520These%2520thresholds%2520offer%2520insights%2520for%250Adesigning%2520systems%2520that%2520mitigate%2520the%2520risk%2520of%2520impersonation%2520and%2520collisions%252C%250Aparticularly%2520in%2520large-scale%2520biometric%2520databases.%2520Our%2520findings%2520indicate%2520that%250Acurrent%2520biometric%2520systems%2520fail%2520to%2520deliver%2520sufficient%2520accuracy%2520to%2520achieve%2520an%250Aadequate%2520security%2520level%2520against%2520untargeted%2520attacks%252C%2520even%2520in%2520small-scale%250Adatabases.%2520Moreover%252C%2520state-of-the-art%2520systems%2520face%2520significant%2520challenges%2520in%250Aaddressing%2520the%2520biometric%2520birthday%2520problem%252C%2520especially%2520as%2520database%2520sizes%2520grow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accuracy%20Limits%20as%20a%20Barrier%20to%20Biometric%20System%20Security&entry.906535625=Axel%20Durbet%20and%20Paul-Marie%20Grollemund%20and%20Pascal%20Lafourcade%20and%20Kevin%20Thiry-Atighehchi&entry.1292438233=%20%20Biometric%20systems%20are%20widely%20used%20for%20identity%20verification%20and%0Aidentification%2C%20including%20authentication%20%28i.e.%2C%20one-to-one%20matching%20to%20verify%20a%0Aclaimed%20identity%29%20and%20identification%20%28i.e.%2C%20one-to-many%20matching%20to%20find%20a%0Asubject%20in%20a%20database%29.%20The%20matching%20process%20relies%20on%20measuring%20similarities%0Aor%20dissimilarities%20between%20a%20fresh%20biometric%20template%20and%20enrolled%20templates.%0AThe%20False%20Match%20Rate%20FMR%20is%20a%20key%20metric%20for%20assessing%20the%20accuracy%20and%0Areliability%20of%20such%20systems.%20This%20paper%20analyzes%20biometric%20systems%20based%20on%0Atheir%20FMR%2C%20with%20two%20main%20contributions.%20First%2C%20we%20explore%20untargeted%20attacks%2C%0Awhere%20an%20adversary%20aims%20to%20impersonate%20any%20user%20within%20a%20database.%20We%20determine%0Athe%20number%20of%20trials%20required%20for%20an%20attacker%20to%20successfully%20impersonate%20a%0Auser%20and%20derive%20the%20critical%20population%20size%20%28i.e.%2C%20the%20maximum%20number%20of%20users%0Ain%20the%20database%29%20required%20to%20maintain%20a%20given%20level%20of%20security.%20Furthermore%2C%0Awe%20compute%20the%20critical%20FMR%20value%20needed%20to%20ensure%20resistance%20against%0Auntargeted%20attacks%20as%20the%20database%20size%20increases.%20Second%2C%20we%20revisit%20the%0Abiometric%20birthday%20problem%20to%20evaluate%20the%20approximate%20and%20exact%20probabilities%0Athat%20two%20users%20in%20a%20database%20collide%20%28i.e.%2C%20can%20impersonate%20each%20other%29.%20Based%0Aon%20this%20analysis%2C%20we%20derive%20both%20the%20approximate%20critical%20population%20size%20and%0Athe%20critical%20FMR%20value%20needed%20to%20bound%20the%20likelihood%20of%20such%20collisions%0Aoccurring%20with%20a%20given%20probability.%20These%20thresholds%20offer%20insights%20for%0Adesigning%20systems%20that%20mitigate%20the%20risk%20of%20impersonation%20and%20collisions%2C%0Aparticularly%20in%20large-scale%20biometric%20databases.%20Our%20findings%20indicate%20that%0Acurrent%20biometric%20systems%20fail%20to%20deliver%20sufficient%20accuracy%20to%20achieve%20an%0Aadequate%20security%20level%20against%20untargeted%20attacks%2C%20even%20in%20small-scale%0Adatabases.%20Moreover%2C%20state-of-the-art%20systems%20face%20significant%20challenges%20in%0Aaddressing%20the%20biometric%20birthday%20problem%2C%20especially%20as%20database%20sizes%20grow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13099v1&entry.124074799=Read"},
{"title": "Harnessing Event Sensory Data for Error Pattern Prediction in Vehicles:\n  A Language Model Approach", "author": "Hugo Math and Rainer Lienhart and Robin Sch\u00f6n", "abstract": "  In this paper, we draw an analogy between processing natural languages and\nprocessing multivariate event streams from vehicles in order to predict\n$\\textit{when}$ and $\\textit{what}$ error pattern is most likely to occur in\nthe future for a given car. Our approach leverages the temporal dynamics and\ncontextual relationships of our event data from a fleet of cars. Event data is\ncomposed of discrete values of error codes as well as continuous values such as\ntime and mileage. Modelled by two causal Transformers, we can anticipate\nvehicle failures and malfunctions before they happen. Thus, we introduce\n$\\textit{CarFormer}$, a Transformer model trained via a new self-supervised\nlearning strategy, and $\\textit{EPredictor}$, an autoregressive Transformer\ndecoder model capable of predicting $\\textit{when}$ and $\\textit{what}$ error\npattern will most likely occur after some error code apparition. Despite the\nchallenges of high cardinality of event types, their unbalanced frequency of\nappearance and limited labelled data, our experimental results demonstrate the\nexcellent predictive ability of our novel model. Specifically, with sequences\nof $160$ error codes on average, our model is able with only half of the error\ncodes to achieve $80\\%$ F1 score for predicting $\\textit{what}$ error pattern\nwill occur and achieves an average absolute error of $58.4 \\pm 13.2$h\n$\\textit{when}$ forecasting the time of occurrence, thus enabling confident\npredictive maintenance and enhancing vehicle safety.\n", "link": "http://arxiv.org/abs/2412.13041v1", "date": "2024-12-17", "relevancy": 1.4833, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5556}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4787}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Event%20Sensory%20Data%20for%20Error%20Pattern%20Prediction%20in%20Vehicles%3A%0A%20%20A%20Language%20Model%20Approach&body=Title%3A%20Harnessing%20Event%20Sensory%20Data%20for%20Error%20Pattern%20Prediction%20in%20Vehicles%3A%0A%20%20A%20Language%20Model%20Approach%0AAuthor%3A%20Hugo%20Math%20and%20Rainer%20Lienhart%20and%20Robin%20Sch%C3%B6n%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20draw%20an%20analogy%20between%20processing%20natural%20languages%20and%0Aprocessing%20multivariate%20event%20streams%20from%20vehicles%20in%20order%20to%20predict%0A%24%5Ctextit%7Bwhen%7D%24%20and%20%24%5Ctextit%7Bwhat%7D%24%20error%20pattern%20is%20most%20likely%20to%20occur%20in%0Athe%20future%20for%20a%20given%20car.%20Our%20approach%20leverages%20the%20temporal%20dynamics%20and%0Acontextual%20relationships%20of%20our%20event%20data%20from%20a%20fleet%20of%20cars.%20Event%20data%20is%0Acomposed%20of%20discrete%20values%20of%20error%20codes%20as%20well%20as%20continuous%20values%20such%20as%0Atime%20and%20mileage.%20Modelled%20by%20two%20causal%20Transformers%2C%20we%20can%20anticipate%0Avehicle%20failures%20and%20malfunctions%20before%20they%20happen.%20Thus%2C%20we%20introduce%0A%24%5Ctextit%7BCarFormer%7D%24%2C%20a%20Transformer%20model%20trained%20via%20a%20new%20self-supervised%0Alearning%20strategy%2C%20and%20%24%5Ctextit%7BEPredictor%7D%24%2C%20an%20autoregressive%20Transformer%0Adecoder%20model%20capable%20of%20predicting%20%24%5Ctextit%7Bwhen%7D%24%20and%20%24%5Ctextit%7Bwhat%7D%24%20error%0Apattern%20will%20most%20likely%20occur%20after%20some%20error%20code%20apparition.%20Despite%20the%0Achallenges%20of%20high%20cardinality%20of%20event%20types%2C%20their%20unbalanced%20frequency%20of%0Aappearance%20and%20limited%20labelled%20data%2C%20our%20experimental%20results%20demonstrate%20the%0Aexcellent%20predictive%20ability%20of%20our%20novel%20model.%20Specifically%2C%20with%20sequences%0Aof%20%24160%24%20error%20codes%20on%20average%2C%20our%20model%20is%20able%20with%20only%20half%20of%20the%20error%0Acodes%20to%20achieve%20%2480%5C%25%24%20F1%20score%20for%20predicting%20%24%5Ctextit%7Bwhat%7D%24%20error%20pattern%0Awill%20occur%20and%20achieves%20an%20average%20absolute%20error%20of%20%2458.4%20%5Cpm%2013.2%24h%0A%24%5Ctextit%7Bwhen%7D%24%20forecasting%20the%20time%20of%20occurrence%2C%20thus%20enabling%20confident%0Apredictive%20maintenance%20and%20enhancing%20vehicle%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Event%2520Sensory%2520Data%2520for%2520Error%2520Pattern%2520Prediction%2520in%2520Vehicles%253A%250A%2520%2520A%2520Language%2520Model%2520Approach%26entry.906535625%3DHugo%2520Math%2520and%2520Rainer%2520Lienhart%2520and%2520Robin%2520Sch%25C3%25B6n%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520draw%2520an%2520analogy%2520between%2520processing%2520natural%2520languages%2520and%250Aprocessing%2520multivariate%2520event%2520streams%2520from%2520vehicles%2520in%2520order%2520to%2520predict%250A%2524%255Ctextit%257Bwhen%257D%2524%2520and%2520%2524%255Ctextit%257Bwhat%257D%2524%2520error%2520pattern%2520is%2520most%2520likely%2520to%2520occur%2520in%250Athe%2520future%2520for%2520a%2520given%2520car.%2520Our%2520approach%2520leverages%2520the%2520temporal%2520dynamics%2520and%250Acontextual%2520relationships%2520of%2520our%2520event%2520data%2520from%2520a%2520fleet%2520of%2520cars.%2520Event%2520data%2520is%250Acomposed%2520of%2520discrete%2520values%2520of%2520error%2520codes%2520as%2520well%2520as%2520continuous%2520values%2520such%2520as%250Atime%2520and%2520mileage.%2520Modelled%2520by%2520two%2520causal%2520Transformers%252C%2520we%2520can%2520anticipate%250Avehicle%2520failures%2520and%2520malfunctions%2520before%2520they%2520happen.%2520Thus%252C%2520we%2520introduce%250A%2524%255Ctextit%257BCarFormer%257D%2524%252C%2520a%2520Transformer%2520model%2520trained%2520via%2520a%2520new%2520self-supervised%250Alearning%2520strategy%252C%2520and%2520%2524%255Ctextit%257BEPredictor%257D%2524%252C%2520an%2520autoregressive%2520Transformer%250Adecoder%2520model%2520capable%2520of%2520predicting%2520%2524%255Ctextit%257Bwhen%257D%2524%2520and%2520%2524%255Ctextit%257Bwhat%257D%2524%2520error%250Apattern%2520will%2520most%2520likely%2520occur%2520after%2520some%2520error%2520code%2520apparition.%2520Despite%2520the%250Achallenges%2520of%2520high%2520cardinality%2520of%2520event%2520types%252C%2520their%2520unbalanced%2520frequency%2520of%250Aappearance%2520and%2520limited%2520labelled%2520data%252C%2520our%2520experimental%2520results%2520demonstrate%2520the%250Aexcellent%2520predictive%2520ability%2520of%2520our%2520novel%2520model.%2520Specifically%252C%2520with%2520sequences%250Aof%2520%2524160%2524%2520error%2520codes%2520on%2520average%252C%2520our%2520model%2520is%2520able%2520with%2520only%2520half%2520of%2520the%2520error%250Acodes%2520to%2520achieve%2520%252480%255C%2525%2524%2520F1%2520score%2520for%2520predicting%2520%2524%255Ctextit%257Bwhat%257D%2524%2520error%2520pattern%250Awill%2520occur%2520and%2520achieves%2520an%2520average%2520absolute%2520error%2520of%2520%252458.4%2520%255Cpm%252013.2%2524h%250A%2524%255Ctextit%257Bwhen%257D%2524%2520forecasting%2520the%2520time%2520of%2520occurrence%252C%2520thus%2520enabling%2520confident%250Apredictive%2520maintenance%2520and%2520enhancing%2520vehicle%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Event%20Sensory%20Data%20for%20Error%20Pattern%20Prediction%20in%20Vehicles%3A%0A%20%20A%20Language%20Model%20Approach&entry.906535625=Hugo%20Math%20and%20Rainer%20Lienhart%20and%20Robin%20Sch%C3%B6n&entry.1292438233=%20%20In%20this%20paper%2C%20we%20draw%20an%20analogy%20between%20processing%20natural%20languages%20and%0Aprocessing%20multivariate%20event%20streams%20from%20vehicles%20in%20order%20to%20predict%0A%24%5Ctextit%7Bwhen%7D%24%20and%20%24%5Ctextit%7Bwhat%7D%24%20error%20pattern%20is%20most%20likely%20to%20occur%20in%0Athe%20future%20for%20a%20given%20car.%20Our%20approach%20leverages%20the%20temporal%20dynamics%20and%0Acontextual%20relationships%20of%20our%20event%20data%20from%20a%20fleet%20of%20cars.%20Event%20data%20is%0Acomposed%20of%20discrete%20values%20of%20error%20codes%20as%20well%20as%20continuous%20values%20such%20as%0Atime%20and%20mileage.%20Modelled%20by%20two%20causal%20Transformers%2C%20we%20can%20anticipate%0Avehicle%20failures%20and%20malfunctions%20before%20they%20happen.%20Thus%2C%20we%20introduce%0A%24%5Ctextit%7BCarFormer%7D%24%2C%20a%20Transformer%20model%20trained%20via%20a%20new%20self-supervised%0Alearning%20strategy%2C%20and%20%24%5Ctextit%7BEPredictor%7D%24%2C%20an%20autoregressive%20Transformer%0Adecoder%20model%20capable%20of%20predicting%20%24%5Ctextit%7Bwhen%7D%24%20and%20%24%5Ctextit%7Bwhat%7D%24%20error%0Apattern%20will%20most%20likely%20occur%20after%20some%20error%20code%20apparition.%20Despite%20the%0Achallenges%20of%20high%20cardinality%20of%20event%20types%2C%20their%20unbalanced%20frequency%20of%0Aappearance%20and%20limited%20labelled%20data%2C%20our%20experimental%20results%20demonstrate%20the%0Aexcellent%20predictive%20ability%20of%20our%20novel%20model.%20Specifically%2C%20with%20sequences%0Aof%20%24160%24%20error%20codes%20on%20average%2C%20our%20model%20is%20able%20with%20only%20half%20of%20the%20error%0Acodes%20to%20achieve%20%2480%5C%25%24%20F1%20score%20for%20predicting%20%24%5Ctextit%7Bwhat%7D%24%20error%20pattern%0Awill%20occur%20and%20achieves%20an%20average%20absolute%20error%20of%20%2458.4%20%5Cpm%2013.2%24h%0A%24%5Ctextit%7Bwhen%7D%24%20forecasting%20the%20time%20of%20occurrence%2C%20thus%20enabling%20confident%0Apredictive%20maintenance%20and%20enhancing%20vehicle%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13041v1&entry.124074799=Read"},
{"title": "Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via\n  Efficient Guidance-Exploration", "author": "Zijian Wang and Bin Wang and Haifeng Jing and Huayu Li and Hongbo Dou", "abstract": "  Recent years, multi-hop reasoning has been widely studied for knowledge graph\n(KG) reasoning due to its efficacy and interpretability. However, previous\nmulti-hop reasoning approaches are subject to two primary shortcomings. First,\nagents struggle to learn effective and robust policies at the early phase due\nto sparse rewards. Second, these approaches often falter on specific datasets\nlike sparse knowledge graphs, where agents are required to traverse lengthy\nreasoning paths. To address these problems, we propose a multi-hop reasoning\nmodel with dual agents based on hierarchical reinforcement learning (HRL),\nwhich is named FULORA. FULORA tackles the above reasoning challenges by\neFficient GUidance-ExpLORAtion between dual agents. The high-level agent walks\non the simplified knowledge graph to provide stage-wise hints for the low-level\nagent walking on the original knowledge graph. In this framework, the low-level\nagent optimizes a value function that balances two objectives: (1) maximizing\nreturn, and (2) integrating efficient guidance from the high-level agent.\nExperiments conducted on three real-word knowledge graph datasets demonstrate\nthat FULORA outperforms RL-based baselines, especially in the case of\nlong-distance reasoning.\n", "link": "http://arxiv.org/abs/2408.01880v3", "date": "2024-12-17", "relevancy": 1.4778, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5246}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Walk%20Wisely%20on%20Graph%3A%20Knowledge%20Graph%20Reasoning%20with%20Dual%20Agents%20via%0A%20%20Efficient%20Guidance-Exploration&body=Title%3A%20Walk%20Wisely%20on%20Graph%3A%20Knowledge%20Graph%20Reasoning%20with%20Dual%20Agents%20via%0A%20%20Efficient%20Guidance-Exploration%0AAuthor%3A%20Zijian%20Wang%20and%20Bin%20Wang%20and%20Haifeng%20Jing%20and%20Huayu%20Li%20and%20Hongbo%20Dou%0AAbstract%3A%20%20%20Recent%20years%2C%20multi-hop%20reasoning%20has%20been%20widely%20studied%20for%20knowledge%20graph%0A%28KG%29%20reasoning%20due%20to%20its%20efficacy%20and%20interpretability.%20However%2C%20previous%0Amulti-hop%20reasoning%20approaches%20are%20subject%20to%20two%20primary%20shortcomings.%20First%2C%0Aagents%20struggle%20to%20learn%20effective%20and%20robust%20policies%20at%20the%20early%20phase%20due%0Ato%20sparse%20rewards.%20Second%2C%20these%20approaches%20often%20falter%20on%20specific%20datasets%0Alike%20sparse%20knowledge%20graphs%2C%20where%20agents%20are%20required%20to%20traverse%20lengthy%0Areasoning%20paths.%20To%20address%20these%20problems%2C%20we%20propose%20a%20multi-hop%20reasoning%0Amodel%20with%20dual%20agents%20based%20on%20hierarchical%20reinforcement%20learning%20%28HRL%29%2C%0Awhich%20is%20named%20FULORA.%20FULORA%20tackles%20the%20above%20reasoning%20challenges%20by%0AeFficient%20GUidance-ExpLORAtion%20between%20dual%20agents.%20The%20high-level%20agent%20walks%0Aon%20the%20simplified%20knowledge%20graph%20to%20provide%20stage-wise%20hints%20for%20the%20low-level%0Aagent%20walking%20on%20the%20original%20knowledge%20graph.%20In%20this%20framework%2C%20the%20low-level%0Aagent%20optimizes%20a%20value%20function%20that%20balances%20two%20objectives%3A%20%281%29%20maximizing%0Areturn%2C%20and%20%282%29%20integrating%20efficient%20guidance%20from%20the%20high-level%20agent.%0AExperiments%20conducted%20on%20three%20real-word%20knowledge%20graph%20datasets%20demonstrate%0Athat%20FULORA%20outperforms%20RL-based%20baselines%2C%20especially%20in%20the%20case%20of%0Along-distance%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01880v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWalk%2520Wisely%2520on%2520Graph%253A%2520Knowledge%2520Graph%2520Reasoning%2520with%2520Dual%2520Agents%2520via%250A%2520%2520Efficient%2520Guidance-Exploration%26entry.906535625%3DZijian%2520Wang%2520and%2520Bin%2520Wang%2520and%2520Haifeng%2520Jing%2520and%2520Huayu%2520Li%2520and%2520Hongbo%2520Dou%26entry.1292438233%3D%2520%2520Recent%2520years%252C%2520multi-hop%2520reasoning%2520has%2520been%2520widely%2520studied%2520for%2520knowledge%2520graph%250A%2528KG%2529%2520reasoning%2520due%2520to%2520its%2520efficacy%2520and%2520interpretability.%2520However%252C%2520previous%250Amulti-hop%2520reasoning%2520approaches%2520are%2520subject%2520to%2520two%2520primary%2520shortcomings.%2520First%252C%250Aagents%2520struggle%2520to%2520learn%2520effective%2520and%2520robust%2520policies%2520at%2520the%2520early%2520phase%2520due%250Ato%2520sparse%2520rewards.%2520Second%252C%2520these%2520approaches%2520often%2520falter%2520on%2520specific%2520datasets%250Alike%2520sparse%2520knowledge%2520graphs%252C%2520where%2520agents%2520are%2520required%2520to%2520traverse%2520lengthy%250Areasoning%2520paths.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520multi-hop%2520reasoning%250Amodel%2520with%2520dual%2520agents%2520based%2520on%2520hierarchical%2520reinforcement%2520learning%2520%2528HRL%2529%252C%250Awhich%2520is%2520named%2520FULORA.%2520FULORA%2520tackles%2520the%2520above%2520reasoning%2520challenges%2520by%250AeFficient%2520GUidance-ExpLORAtion%2520between%2520dual%2520agents.%2520The%2520high-level%2520agent%2520walks%250Aon%2520the%2520simplified%2520knowledge%2520graph%2520to%2520provide%2520stage-wise%2520hints%2520for%2520the%2520low-level%250Aagent%2520walking%2520on%2520the%2520original%2520knowledge%2520graph.%2520In%2520this%2520framework%252C%2520the%2520low-level%250Aagent%2520optimizes%2520a%2520value%2520function%2520that%2520balances%2520two%2520objectives%253A%2520%25281%2529%2520maximizing%250Areturn%252C%2520and%2520%25282%2529%2520integrating%2520efficient%2520guidance%2520from%2520the%2520high-level%2520agent.%250AExperiments%2520conducted%2520on%2520three%2520real-word%2520knowledge%2520graph%2520datasets%2520demonstrate%250Athat%2520FULORA%2520outperforms%2520RL-based%2520baselines%252C%2520especially%2520in%2520the%2520case%2520of%250Along-distance%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01880v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Walk%20Wisely%20on%20Graph%3A%20Knowledge%20Graph%20Reasoning%20with%20Dual%20Agents%20via%0A%20%20Efficient%20Guidance-Exploration&entry.906535625=Zijian%20Wang%20and%20Bin%20Wang%20and%20Haifeng%20Jing%20and%20Huayu%20Li%20and%20Hongbo%20Dou&entry.1292438233=%20%20Recent%20years%2C%20multi-hop%20reasoning%20has%20been%20widely%20studied%20for%20knowledge%20graph%0A%28KG%29%20reasoning%20due%20to%20its%20efficacy%20and%20interpretability.%20However%2C%20previous%0Amulti-hop%20reasoning%20approaches%20are%20subject%20to%20two%20primary%20shortcomings.%20First%2C%0Aagents%20struggle%20to%20learn%20effective%20and%20robust%20policies%20at%20the%20early%20phase%20due%0Ato%20sparse%20rewards.%20Second%2C%20these%20approaches%20often%20falter%20on%20specific%20datasets%0Alike%20sparse%20knowledge%20graphs%2C%20where%20agents%20are%20required%20to%20traverse%20lengthy%0Areasoning%20paths.%20To%20address%20these%20problems%2C%20we%20propose%20a%20multi-hop%20reasoning%0Amodel%20with%20dual%20agents%20based%20on%20hierarchical%20reinforcement%20learning%20%28HRL%29%2C%0Awhich%20is%20named%20FULORA.%20FULORA%20tackles%20the%20above%20reasoning%20challenges%20by%0AeFficient%20GUidance-ExpLORAtion%20between%20dual%20agents.%20The%20high-level%20agent%20walks%0Aon%20the%20simplified%20knowledge%20graph%20to%20provide%20stage-wise%20hints%20for%20the%20low-level%0Aagent%20walking%20on%20the%20original%20knowledge%20graph.%20In%20this%20framework%2C%20the%20low-level%0Aagent%20optimizes%20a%20value%20function%20that%20balances%20two%20objectives%3A%20%281%29%20maximizing%0Areturn%2C%20and%20%282%29%20integrating%20efficient%20guidance%20from%20the%20high-level%20agent.%0AExperiments%20conducted%20on%20three%20real-word%20knowledge%20graph%20datasets%20demonstrate%0Athat%20FULORA%20outperforms%20RL-based%20baselines%2C%20especially%20in%20the%20case%20of%0Along-distance%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01880v3&entry.124074799=Read"},
{"title": "Reservoir Computing for Fast, Simplified Reinforcement Learning on\n  Memory Tasks", "author": "Kevin McKee", "abstract": "  Tasks in which rewards depend upon past information not available in the\ncurrent observation set can only be solved by agents that are equipped with\nshort-term memory. Usual choices for memory modules include trainable recurrent\nhidden layers, often with gated memory. Reservoir computing presents an\nalternative, in which a recurrent layer is not trained, but rather has a set of\nfixed, sparse recurrent weights. The weights are scaled to produce stable\ndynamical behavior such that the reservoir state contains a high-dimensional,\nnonlinear impulse response function of the inputs. An output decoder network\ncan then be used to map the compressive history represented by the reservoir's\nstate to any outputs, including agent actions or predictions. In this study, we\nfind that reservoir computing greatly simplifies and speeds up reinforcement\nlearning on memory tasks by (1) eliminating the need for backpropagation of\ngradients through time, (2) presenting all recent history simultaneously to the\ndownstream network, and (3) performing many useful and generic nonlinear\ncomputations upstream from the trained modules. In particular, these findings\noffer significant benefit to meta-learning that depends primarily on efficient\nand highly general memory systems.\n", "link": "http://arxiv.org/abs/2412.13093v1", "date": "2024-12-17", "relevancy": 1.4767, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5015}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4842}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reservoir%20Computing%20for%20Fast%2C%20Simplified%20Reinforcement%20Learning%20on%0A%20%20Memory%20Tasks&body=Title%3A%20Reservoir%20Computing%20for%20Fast%2C%20Simplified%20Reinforcement%20Learning%20on%0A%20%20Memory%20Tasks%0AAuthor%3A%20Kevin%20McKee%0AAbstract%3A%20%20%20Tasks%20in%20which%20rewards%20depend%20upon%20past%20information%20not%20available%20in%20the%0Acurrent%20observation%20set%20can%20only%20be%20solved%20by%20agents%20that%20are%20equipped%20with%0Ashort-term%20memory.%20Usual%20choices%20for%20memory%20modules%20include%20trainable%20recurrent%0Ahidden%20layers%2C%20often%20with%20gated%20memory.%20Reservoir%20computing%20presents%20an%0Aalternative%2C%20in%20which%20a%20recurrent%20layer%20is%20not%20trained%2C%20but%20rather%20has%20a%20set%20of%0Afixed%2C%20sparse%20recurrent%20weights.%20The%20weights%20are%20scaled%20to%20produce%20stable%0Adynamical%20behavior%20such%20that%20the%20reservoir%20state%20contains%20a%20high-dimensional%2C%0Anonlinear%20impulse%20response%20function%20of%20the%20inputs.%20An%20output%20decoder%20network%0Acan%20then%20be%20used%20to%20map%20the%20compressive%20history%20represented%20by%20the%20reservoir%27s%0Astate%20to%20any%20outputs%2C%20including%20agent%20actions%20or%20predictions.%20In%20this%20study%2C%20we%0Afind%20that%20reservoir%20computing%20greatly%20simplifies%20and%20speeds%20up%20reinforcement%0Alearning%20on%20memory%20tasks%20by%20%281%29%20eliminating%20the%20need%20for%20backpropagation%20of%0Agradients%20through%20time%2C%20%282%29%20presenting%20all%20recent%20history%20simultaneously%20to%20the%0Adownstream%20network%2C%20and%20%283%29%20performing%20many%20useful%20and%20generic%20nonlinear%0Acomputations%20upstream%20from%20the%20trained%20modules.%20In%20particular%2C%20these%20findings%0Aoffer%20significant%20benefit%20to%20meta-learning%20that%20depends%20primarily%20on%20efficient%0Aand%20highly%20general%20memory%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReservoir%2520Computing%2520for%2520Fast%252C%2520Simplified%2520Reinforcement%2520Learning%2520on%250A%2520%2520Memory%2520Tasks%26entry.906535625%3DKevin%2520McKee%26entry.1292438233%3D%2520%2520Tasks%2520in%2520which%2520rewards%2520depend%2520upon%2520past%2520information%2520not%2520available%2520in%2520the%250Acurrent%2520observation%2520set%2520can%2520only%2520be%2520solved%2520by%2520agents%2520that%2520are%2520equipped%2520with%250Ashort-term%2520memory.%2520Usual%2520choices%2520for%2520memory%2520modules%2520include%2520trainable%2520recurrent%250Ahidden%2520layers%252C%2520often%2520with%2520gated%2520memory.%2520Reservoir%2520computing%2520presents%2520an%250Aalternative%252C%2520in%2520which%2520a%2520recurrent%2520layer%2520is%2520not%2520trained%252C%2520but%2520rather%2520has%2520a%2520set%2520of%250Afixed%252C%2520sparse%2520recurrent%2520weights.%2520The%2520weights%2520are%2520scaled%2520to%2520produce%2520stable%250Adynamical%2520behavior%2520such%2520that%2520the%2520reservoir%2520state%2520contains%2520a%2520high-dimensional%252C%250Anonlinear%2520impulse%2520response%2520function%2520of%2520the%2520inputs.%2520An%2520output%2520decoder%2520network%250Acan%2520then%2520be%2520used%2520to%2520map%2520the%2520compressive%2520history%2520represented%2520by%2520the%2520reservoir%2527s%250Astate%2520to%2520any%2520outputs%252C%2520including%2520agent%2520actions%2520or%2520predictions.%2520In%2520this%2520study%252C%2520we%250Afind%2520that%2520reservoir%2520computing%2520greatly%2520simplifies%2520and%2520speeds%2520up%2520reinforcement%250Alearning%2520on%2520memory%2520tasks%2520by%2520%25281%2529%2520eliminating%2520the%2520need%2520for%2520backpropagation%2520of%250Agradients%2520through%2520time%252C%2520%25282%2529%2520presenting%2520all%2520recent%2520history%2520simultaneously%2520to%2520the%250Adownstream%2520network%252C%2520and%2520%25283%2529%2520performing%2520many%2520useful%2520and%2520generic%2520nonlinear%250Acomputations%2520upstream%2520from%2520the%2520trained%2520modules.%2520In%2520particular%252C%2520these%2520findings%250Aoffer%2520significant%2520benefit%2520to%2520meta-learning%2520that%2520depends%2520primarily%2520on%2520efficient%250Aand%2520highly%2520general%2520memory%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reservoir%20Computing%20for%20Fast%2C%20Simplified%20Reinforcement%20Learning%20on%0A%20%20Memory%20Tasks&entry.906535625=Kevin%20McKee&entry.1292438233=%20%20Tasks%20in%20which%20rewards%20depend%20upon%20past%20information%20not%20available%20in%20the%0Acurrent%20observation%20set%20can%20only%20be%20solved%20by%20agents%20that%20are%20equipped%20with%0Ashort-term%20memory.%20Usual%20choices%20for%20memory%20modules%20include%20trainable%20recurrent%0Ahidden%20layers%2C%20often%20with%20gated%20memory.%20Reservoir%20computing%20presents%20an%0Aalternative%2C%20in%20which%20a%20recurrent%20layer%20is%20not%20trained%2C%20but%20rather%20has%20a%20set%20of%0Afixed%2C%20sparse%20recurrent%20weights.%20The%20weights%20are%20scaled%20to%20produce%20stable%0Adynamical%20behavior%20such%20that%20the%20reservoir%20state%20contains%20a%20high-dimensional%2C%0Anonlinear%20impulse%20response%20function%20of%20the%20inputs.%20An%20output%20decoder%20network%0Acan%20then%20be%20used%20to%20map%20the%20compressive%20history%20represented%20by%20the%20reservoir%27s%0Astate%20to%20any%20outputs%2C%20including%20agent%20actions%20or%20predictions.%20In%20this%20study%2C%20we%0Afind%20that%20reservoir%20computing%20greatly%20simplifies%20and%20speeds%20up%20reinforcement%0Alearning%20on%20memory%20tasks%20by%20%281%29%20eliminating%20the%20need%20for%20backpropagation%20of%0Agradients%20through%20time%2C%20%282%29%20presenting%20all%20recent%20history%20simultaneously%20to%20the%0Adownstream%20network%2C%20and%20%283%29%20performing%20many%20useful%20and%20generic%20nonlinear%0Acomputations%20upstream%20from%20the%20trained%20modules.%20In%20particular%2C%20these%20findings%0Aoffer%20significant%20benefit%20to%20meta-learning%20that%20depends%20primarily%20on%20efficient%0Aand%20highly%20general%20memory%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13093v1&entry.124074799=Read"},
{"title": "A Conformal Approach to Feature-based Newsvendor under Model\n  Misspecification", "author": "Junyu Cao", "abstract": "  In many data-driven decision-making problems, performance guarantees often\ndepend heavily on the correctness of model assumptions, which may frequently\nfail in practice. We address this issue in the context of a feature-based\nnewsvendor problem, where demand is influenced by observed features such as\ndemographics and seasonality. To mitigate the impact of model misspecification,\nwe propose a model-free and distribution-free framework inspired by conformal\nprediction. Our approach consists of two phases: a training phase, which can\nutilize any type of prediction method, and a calibration phase that\nconformalizes the model bias. To enhance predictive performance, we explore the\nbalance between data quality and quantity, recognizing the inherent trade-off:\nmore selective training data improves quality but reduces quantity.\nImportantly, we provide statistical guarantees for the conformalized critical\nquantile, independent of the correctness of the underlying model. Moreover, we\nquantify the confidence interval of the critical quantile, with its width\ndecreasing as data quality and quantity improve. We validate our framework\nusing both simulated data and a real-world dataset from the Capital Bikeshare\nprogram in Washington, D.C. Across these experiments, our proposed method\nconsistently outperforms benchmark algorithms, reducing newsvendor loss by up\nto 40% on the simulated data and 25% on the real-world dataset.\n", "link": "http://arxiv.org/abs/2412.13159v1", "date": "2024-12-17", "relevancy": 1.4405, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5036}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4951}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Conformal%20Approach%20to%20Feature-based%20Newsvendor%20under%20Model%0A%20%20Misspecification&body=Title%3A%20A%20Conformal%20Approach%20to%20Feature-based%20Newsvendor%20under%20Model%0A%20%20Misspecification%0AAuthor%3A%20Junyu%20Cao%0AAbstract%3A%20%20%20In%20many%20data-driven%20decision-making%20problems%2C%20performance%20guarantees%20often%0Adepend%20heavily%20on%20the%20correctness%20of%20model%20assumptions%2C%20which%20may%20frequently%0Afail%20in%20practice.%20We%20address%20this%20issue%20in%20the%20context%20of%20a%20feature-based%0Anewsvendor%20problem%2C%20where%20demand%20is%20influenced%20by%20observed%20features%20such%20as%0Ademographics%20and%20seasonality.%20To%20mitigate%20the%20impact%20of%20model%20misspecification%2C%0Awe%20propose%20a%20model-free%20and%20distribution-free%20framework%20inspired%20by%20conformal%0Aprediction.%20Our%20approach%20consists%20of%20two%20phases%3A%20a%20training%20phase%2C%20which%20can%0Autilize%20any%20type%20of%20prediction%20method%2C%20and%20a%20calibration%20phase%20that%0Aconformalizes%20the%20model%20bias.%20To%20enhance%20predictive%20performance%2C%20we%20explore%20the%0Abalance%20between%20data%20quality%20and%20quantity%2C%20recognizing%20the%20inherent%20trade-off%3A%0Amore%20selective%20training%20data%20improves%20quality%20but%20reduces%20quantity.%0AImportantly%2C%20we%20provide%20statistical%20guarantees%20for%20the%20conformalized%20critical%0Aquantile%2C%20independent%20of%20the%20correctness%20of%20the%20underlying%20model.%20Moreover%2C%20we%0Aquantify%20the%20confidence%20interval%20of%20the%20critical%20quantile%2C%20with%20its%20width%0Adecreasing%20as%20data%20quality%20and%20quantity%20improve.%20We%20validate%20our%20framework%0Ausing%20both%20simulated%20data%20and%20a%20real-world%20dataset%20from%20the%20Capital%20Bikeshare%0Aprogram%20in%20Washington%2C%20D.C.%20Across%20these%20experiments%2C%20our%20proposed%20method%0Aconsistently%20outperforms%20benchmark%20algorithms%2C%20reducing%20newsvendor%20loss%20by%20up%0Ato%2040%25%20on%20the%20simulated%20data%20and%2025%25%20on%20the%20real-world%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Conformal%2520Approach%2520to%2520Feature-based%2520Newsvendor%2520under%2520Model%250A%2520%2520Misspecification%26entry.906535625%3DJunyu%2520Cao%26entry.1292438233%3D%2520%2520In%2520many%2520data-driven%2520decision-making%2520problems%252C%2520performance%2520guarantees%2520often%250Adepend%2520heavily%2520on%2520the%2520correctness%2520of%2520model%2520assumptions%252C%2520which%2520may%2520frequently%250Afail%2520in%2520practice.%2520We%2520address%2520this%2520issue%2520in%2520the%2520context%2520of%2520a%2520feature-based%250Anewsvendor%2520problem%252C%2520where%2520demand%2520is%2520influenced%2520by%2520observed%2520features%2520such%2520as%250Ademographics%2520and%2520seasonality.%2520To%2520mitigate%2520the%2520impact%2520of%2520model%2520misspecification%252C%250Awe%2520propose%2520a%2520model-free%2520and%2520distribution-free%2520framework%2520inspired%2520by%2520conformal%250Aprediction.%2520Our%2520approach%2520consists%2520of%2520two%2520phases%253A%2520a%2520training%2520phase%252C%2520which%2520can%250Autilize%2520any%2520type%2520of%2520prediction%2520method%252C%2520and%2520a%2520calibration%2520phase%2520that%250Aconformalizes%2520the%2520model%2520bias.%2520To%2520enhance%2520predictive%2520performance%252C%2520we%2520explore%2520the%250Abalance%2520between%2520data%2520quality%2520and%2520quantity%252C%2520recognizing%2520the%2520inherent%2520trade-off%253A%250Amore%2520selective%2520training%2520data%2520improves%2520quality%2520but%2520reduces%2520quantity.%250AImportantly%252C%2520we%2520provide%2520statistical%2520guarantees%2520for%2520the%2520conformalized%2520critical%250Aquantile%252C%2520independent%2520of%2520the%2520correctness%2520of%2520the%2520underlying%2520model.%2520Moreover%252C%2520we%250Aquantify%2520the%2520confidence%2520interval%2520of%2520the%2520critical%2520quantile%252C%2520with%2520its%2520width%250Adecreasing%2520as%2520data%2520quality%2520and%2520quantity%2520improve.%2520We%2520validate%2520our%2520framework%250Ausing%2520both%2520simulated%2520data%2520and%2520a%2520real-world%2520dataset%2520from%2520the%2520Capital%2520Bikeshare%250Aprogram%2520in%2520Washington%252C%2520D.C.%2520Across%2520these%2520experiments%252C%2520our%2520proposed%2520method%250Aconsistently%2520outperforms%2520benchmark%2520algorithms%252C%2520reducing%2520newsvendor%2520loss%2520by%2520up%250Ato%252040%2525%2520on%2520the%2520simulated%2520data%2520and%252025%2525%2520on%2520the%2520real-world%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Conformal%20Approach%20to%20Feature-based%20Newsvendor%20under%20Model%0A%20%20Misspecification&entry.906535625=Junyu%20Cao&entry.1292438233=%20%20In%20many%20data-driven%20decision-making%20problems%2C%20performance%20guarantees%20often%0Adepend%20heavily%20on%20the%20correctness%20of%20model%20assumptions%2C%20which%20may%20frequently%0Afail%20in%20practice.%20We%20address%20this%20issue%20in%20the%20context%20of%20a%20feature-based%0Anewsvendor%20problem%2C%20where%20demand%20is%20influenced%20by%20observed%20features%20such%20as%0Ademographics%20and%20seasonality.%20To%20mitigate%20the%20impact%20of%20model%20misspecification%2C%0Awe%20propose%20a%20model-free%20and%20distribution-free%20framework%20inspired%20by%20conformal%0Aprediction.%20Our%20approach%20consists%20of%20two%20phases%3A%20a%20training%20phase%2C%20which%20can%0Autilize%20any%20type%20of%20prediction%20method%2C%20and%20a%20calibration%20phase%20that%0Aconformalizes%20the%20model%20bias.%20To%20enhance%20predictive%20performance%2C%20we%20explore%20the%0Abalance%20between%20data%20quality%20and%20quantity%2C%20recognizing%20the%20inherent%20trade-off%3A%0Amore%20selective%20training%20data%20improves%20quality%20but%20reduces%20quantity.%0AImportantly%2C%20we%20provide%20statistical%20guarantees%20for%20the%20conformalized%20critical%0Aquantile%2C%20independent%20of%20the%20correctness%20of%20the%20underlying%20model.%20Moreover%2C%20we%0Aquantify%20the%20confidence%20interval%20of%20the%20critical%20quantile%2C%20with%20its%20width%0Adecreasing%20as%20data%20quality%20and%20quantity%20improve.%20We%20validate%20our%20framework%0Ausing%20both%20simulated%20data%20and%20a%20real-world%20dataset%20from%20the%20Capital%20Bikeshare%0Aprogram%20in%20Washington%2C%20D.C.%20Across%20these%20experiments%2C%20our%20proposed%20method%0Aconsistently%20outperforms%20benchmark%20algorithms%2C%20reducing%20newsvendor%20loss%20by%20up%0Ato%2040%25%20on%20the%20simulated%20data%20and%2025%25%20on%20the%20real-world%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13159v1&entry.124074799=Read"},
{"title": "QEDCartographer: Automating Formal Verification Using Reward-Free\n  Reinforcement Learning", "author": "Alex Sanchez-Stern and Abhishek Varghese and Zhanna Kaufman and Dylan Zhang and Talia Ringer and Yuriy Brun", "abstract": "  Formal verification is a promising method for producing reliable software,\nbut the difficulty of manually writing verification proofs severely limits its\nutility in practice. Recent methods have automated some proof synthesis by\nguiding a search through the proof space using a theorem prover. Unfortunately,\nthe theorem prover provides only the crudest estimate of progress, resulting in\neffectively undirected search. To address this problem, we create\nQEDCartographer, an automated proof-synthesis tool that combines supervised and\nreinforcement learning to more effectively explore the proof space.\nQEDCartographer incorporates the proofs' branching structure, enabling\nreward-free search and overcoming the sparse reward problem inherent to formal\nverification. We evaluate QEDCartographer using the CoqGym benchmark of 68.5K\ntheorems from 124 open-source Coq projects. QEDCartographer fully automatically\nproves 21.4% of the test-set theorems. Previous search-based proof-synthesis\ntools Tok, Tac, ASTactic, Passport, and Proverbot9001, which rely only on\nsupervised learning, prove 9.6%, 9.8%, 10.9%, 12.5%, and 19.8%, respectively.\nDiva, which combines 62 tools, proves 19.2%. Comparing to the most effective\nprior tool, Proverbot9001, QEDCartographer produces 34% shorter proofs 29%\nfaster, on average over the theorems both tools prove. Together,\nQEDCartographer and non-learning-based CoqHammer prove 30.3% of the theorems,\nwhile CoqHammer alone proves 26.6%. Our work demonstrates that reinforcement\nlearning is a fruitful research direction for improving proof-synthesis tools'\nsearch mechanisms.\n", "link": "http://arxiv.org/abs/2408.09237v7", "date": "2024-12-17", "relevancy": 1.4209, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4833}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QEDCartographer%3A%20Automating%20Formal%20Verification%20Using%20Reward-Free%0A%20%20Reinforcement%20Learning&body=Title%3A%20QEDCartographer%3A%20Automating%20Formal%20Verification%20Using%20Reward-Free%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Alex%20Sanchez-Stern%20and%20Abhishek%20Varghese%20and%20Zhanna%20Kaufman%20and%20Dylan%20Zhang%20and%20Talia%20Ringer%20and%20Yuriy%20Brun%0AAbstract%3A%20%20%20Formal%20verification%20is%20a%20promising%20method%20for%20producing%20reliable%20software%2C%0Abut%20the%20difficulty%20of%20manually%20writing%20verification%20proofs%20severely%20limits%20its%0Autility%20in%20practice.%20Recent%20methods%20have%20automated%20some%20proof%20synthesis%20by%0Aguiding%20a%20search%20through%20the%20proof%20space%20using%20a%20theorem%20prover.%20Unfortunately%2C%0Athe%20theorem%20prover%20provides%20only%20the%20crudest%20estimate%20of%20progress%2C%20resulting%20in%0Aeffectively%20undirected%20search.%20To%20address%20this%20problem%2C%20we%20create%0AQEDCartographer%2C%20an%20automated%20proof-synthesis%20tool%20that%20combines%20supervised%20and%0Areinforcement%20learning%20to%20more%20effectively%20explore%20the%20proof%20space.%0AQEDCartographer%20incorporates%20the%20proofs%27%20branching%20structure%2C%20enabling%0Areward-free%20search%20and%20overcoming%20the%20sparse%20reward%20problem%20inherent%20to%20formal%0Averification.%20We%20evaluate%20QEDCartographer%20using%20the%20CoqGym%20benchmark%20of%2068.5K%0Atheorems%20from%20124%20open-source%20Coq%20projects.%20QEDCartographer%20fully%20automatically%0Aproves%2021.4%25%20of%20the%20test-set%20theorems.%20Previous%20search-based%20proof-synthesis%0Atools%20Tok%2C%20Tac%2C%20ASTactic%2C%20Passport%2C%20and%20Proverbot9001%2C%20which%20rely%20only%20on%0Asupervised%20learning%2C%20prove%209.6%25%2C%209.8%25%2C%2010.9%25%2C%2012.5%25%2C%20and%2019.8%25%2C%20respectively.%0ADiva%2C%20which%20combines%2062%20tools%2C%20proves%2019.2%25.%20Comparing%20to%20the%20most%20effective%0Aprior%20tool%2C%20Proverbot9001%2C%20QEDCartographer%20produces%2034%25%20shorter%20proofs%2029%25%0Afaster%2C%20on%20average%20over%20the%20theorems%20both%20tools%20prove.%20Together%2C%0AQEDCartographer%20and%20non-learning-based%20CoqHammer%20prove%2030.3%25%20of%20the%20theorems%2C%0Awhile%20CoqHammer%20alone%20proves%2026.6%25.%20Our%20work%20demonstrates%20that%20reinforcement%0Alearning%20is%20a%20fruitful%20research%20direction%20for%20improving%20proof-synthesis%20tools%27%0Asearch%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09237v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQEDCartographer%253A%2520Automating%2520Formal%2520Verification%2520Using%2520Reward-Free%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DAlex%2520Sanchez-Stern%2520and%2520Abhishek%2520Varghese%2520and%2520Zhanna%2520Kaufman%2520and%2520Dylan%2520Zhang%2520and%2520Talia%2520Ringer%2520and%2520Yuriy%2520Brun%26entry.1292438233%3D%2520%2520Formal%2520verification%2520is%2520a%2520promising%2520method%2520for%2520producing%2520reliable%2520software%252C%250Abut%2520the%2520difficulty%2520of%2520manually%2520writing%2520verification%2520proofs%2520severely%2520limits%2520its%250Autility%2520in%2520practice.%2520Recent%2520methods%2520have%2520automated%2520some%2520proof%2520synthesis%2520by%250Aguiding%2520a%2520search%2520through%2520the%2520proof%2520space%2520using%2520a%2520theorem%2520prover.%2520Unfortunately%252C%250Athe%2520theorem%2520prover%2520provides%2520only%2520the%2520crudest%2520estimate%2520of%2520progress%252C%2520resulting%2520in%250Aeffectively%2520undirected%2520search.%2520To%2520address%2520this%2520problem%252C%2520we%2520create%250AQEDCartographer%252C%2520an%2520automated%2520proof-synthesis%2520tool%2520that%2520combines%2520supervised%2520and%250Areinforcement%2520learning%2520to%2520more%2520effectively%2520explore%2520the%2520proof%2520space.%250AQEDCartographer%2520incorporates%2520the%2520proofs%2527%2520branching%2520structure%252C%2520enabling%250Areward-free%2520search%2520and%2520overcoming%2520the%2520sparse%2520reward%2520problem%2520inherent%2520to%2520formal%250Averification.%2520We%2520evaluate%2520QEDCartographer%2520using%2520the%2520CoqGym%2520benchmark%2520of%252068.5K%250Atheorems%2520from%2520124%2520open-source%2520Coq%2520projects.%2520QEDCartographer%2520fully%2520automatically%250Aproves%252021.4%2525%2520of%2520the%2520test-set%2520theorems.%2520Previous%2520search-based%2520proof-synthesis%250Atools%2520Tok%252C%2520Tac%252C%2520ASTactic%252C%2520Passport%252C%2520and%2520Proverbot9001%252C%2520which%2520rely%2520only%2520on%250Asupervised%2520learning%252C%2520prove%25209.6%2525%252C%25209.8%2525%252C%252010.9%2525%252C%252012.5%2525%252C%2520and%252019.8%2525%252C%2520respectively.%250ADiva%252C%2520which%2520combines%252062%2520tools%252C%2520proves%252019.2%2525.%2520Comparing%2520to%2520the%2520most%2520effective%250Aprior%2520tool%252C%2520Proverbot9001%252C%2520QEDCartographer%2520produces%252034%2525%2520shorter%2520proofs%252029%2525%250Afaster%252C%2520on%2520average%2520over%2520the%2520theorems%2520both%2520tools%2520prove.%2520Together%252C%250AQEDCartographer%2520and%2520non-learning-based%2520CoqHammer%2520prove%252030.3%2525%2520of%2520the%2520theorems%252C%250Awhile%2520CoqHammer%2520alone%2520proves%252026.6%2525.%2520Our%2520work%2520demonstrates%2520that%2520reinforcement%250Alearning%2520is%2520a%2520fruitful%2520research%2520direction%2520for%2520improving%2520proof-synthesis%2520tools%2527%250Asearch%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09237v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QEDCartographer%3A%20Automating%20Formal%20Verification%20Using%20Reward-Free%0A%20%20Reinforcement%20Learning&entry.906535625=Alex%20Sanchez-Stern%20and%20Abhishek%20Varghese%20and%20Zhanna%20Kaufman%20and%20Dylan%20Zhang%20and%20Talia%20Ringer%20and%20Yuriy%20Brun&entry.1292438233=%20%20Formal%20verification%20is%20a%20promising%20method%20for%20producing%20reliable%20software%2C%0Abut%20the%20difficulty%20of%20manually%20writing%20verification%20proofs%20severely%20limits%20its%0Autility%20in%20practice.%20Recent%20methods%20have%20automated%20some%20proof%20synthesis%20by%0Aguiding%20a%20search%20through%20the%20proof%20space%20using%20a%20theorem%20prover.%20Unfortunately%2C%0Athe%20theorem%20prover%20provides%20only%20the%20crudest%20estimate%20of%20progress%2C%20resulting%20in%0Aeffectively%20undirected%20search.%20To%20address%20this%20problem%2C%20we%20create%0AQEDCartographer%2C%20an%20automated%20proof-synthesis%20tool%20that%20combines%20supervised%20and%0Areinforcement%20learning%20to%20more%20effectively%20explore%20the%20proof%20space.%0AQEDCartographer%20incorporates%20the%20proofs%27%20branching%20structure%2C%20enabling%0Areward-free%20search%20and%20overcoming%20the%20sparse%20reward%20problem%20inherent%20to%20formal%0Averification.%20We%20evaluate%20QEDCartographer%20using%20the%20CoqGym%20benchmark%20of%2068.5K%0Atheorems%20from%20124%20open-source%20Coq%20projects.%20QEDCartographer%20fully%20automatically%0Aproves%2021.4%25%20of%20the%20test-set%20theorems.%20Previous%20search-based%20proof-synthesis%0Atools%20Tok%2C%20Tac%2C%20ASTactic%2C%20Passport%2C%20and%20Proverbot9001%2C%20which%20rely%20only%20on%0Asupervised%20learning%2C%20prove%209.6%25%2C%209.8%25%2C%2010.9%25%2C%2012.5%25%2C%20and%2019.8%25%2C%20respectively.%0ADiva%2C%20which%20combines%2062%20tools%2C%20proves%2019.2%25.%20Comparing%20to%20the%20most%20effective%0Aprior%20tool%2C%20Proverbot9001%2C%20QEDCartographer%20produces%2034%25%20shorter%20proofs%2029%25%0Afaster%2C%20on%20average%20over%20the%20theorems%20both%20tools%20prove.%20Together%2C%0AQEDCartographer%20and%20non-learning-based%20CoqHammer%20prove%2030.3%25%20of%20the%20theorems%2C%0Awhile%20CoqHammer%20alone%20proves%2026.6%25.%20Our%20work%20demonstrates%20that%20reinforcement%0Alearning%20is%20a%20fruitful%20research%20direction%20for%20improving%20proof-synthesis%20tools%27%0Asearch%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09237v7&entry.124074799=Read"},
{"title": "TIMESAFE: Timing Interruption Monitoring and Security Assessment for\n  Fronthaul Environments", "author": "Joshua Groen and Simone Di Valerio and Imtiaz Karim and Davide Villa and Yiewi Zhang and Leonardo Bonati and Michele Polese and Salvatore D'Oro and Tommaso Melodia and Elisa Bertino and Francesca Cuomo and Kaushik Chowdhury", "abstract": "  5G and beyond cellular systems embrace the disaggregation of Radio Access\nNetwork (RAN) components, exemplified by the evolution of the fronthual (FH)\nconnection between cellular baseband and radio unit equipment. Crucially,\nsynchronization over the FH is pivotal for reliable 5G services. In recent\nyears, there has been a push to move these links to an Ethernet-based packet\nnetwork topology, leveraging existing standards and ongoing research for\nTime-Sensitive Networking (TSN). However, TSN standards, such as Precision Time\nProtocol (PTP), focus on performance with little to no concern for security.\nThis increases the exposure of the open FH to security risks. Attacks targeting\nsynchronization mechanisms pose significant threats, potentially disrupting 5G\nnetworks and impairing connectivity.\n  In this paper, we demonstrate the impact of successful spoofing and replay\nattacks against PTP synchronization. We show how a spoofing attack is able to\ncause a production-ready O-RAN and 5G-compliant private cellular base station\nto catastrophically fail within 2 seconds of the attack, necessitating manual\nintervention to restore full network operations. To counter this, we design a\nMachine Learning (ML)-based monitoring solution capable of detecting various\nmalicious attacks with over 97.5% accuracy.\n", "link": "http://arxiv.org/abs/2412.13049v1", "date": "2024-12-17", "relevancy": 1.4172, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3574}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3531}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIMESAFE%3A%20Timing%20Interruption%20Monitoring%20and%20Security%20Assessment%20for%0A%20%20Fronthaul%20Environments&body=Title%3A%20TIMESAFE%3A%20Timing%20Interruption%20Monitoring%20and%20Security%20Assessment%20for%0A%20%20Fronthaul%20Environments%0AAuthor%3A%20Joshua%20Groen%20and%20Simone%20Di%20Valerio%20and%20Imtiaz%20Karim%20and%20Davide%20Villa%20and%20Yiewi%20Zhang%20and%20Leonardo%20Bonati%20and%20Michele%20Polese%20and%20Salvatore%20D%27Oro%20and%20Tommaso%20Melodia%20and%20Elisa%20Bertino%20and%20Francesca%20Cuomo%20and%20Kaushik%20Chowdhury%0AAbstract%3A%20%20%205G%20and%20beyond%20cellular%20systems%20embrace%20the%20disaggregation%20of%20Radio%20Access%0ANetwork%20%28RAN%29%20components%2C%20exemplified%20by%20the%20evolution%20of%20the%20fronthual%20%28FH%29%0Aconnection%20between%20cellular%20baseband%20and%20radio%20unit%20equipment.%20Crucially%2C%0Asynchronization%20over%20the%20FH%20is%20pivotal%20for%20reliable%205G%20services.%20In%20recent%0Ayears%2C%20there%20has%20been%20a%20push%20to%20move%20these%20links%20to%20an%20Ethernet-based%20packet%0Anetwork%20topology%2C%20leveraging%20existing%20standards%20and%20ongoing%20research%20for%0ATime-Sensitive%20Networking%20%28TSN%29.%20However%2C%20TSN%20standards%2C%20such%20as%20Precision%20Time%0AProtocol%20%28PTP%29%2C%20focus%20on%20performance%20with%20little%20to%20no%20concern%20for%20security.%0AThis%20increases%20the%20exposure%20of%20the%20open%20FH%20to%20security%20risks.%20Attacks%20targeting%0Asynchronization%20mechanisms%20pose%20significant%20threats%2C%20potentially%20disrupting%205G%0Anetworks%20and%20impairing%20connectivity.%0A%20%20In%20this%20paper%2C%20we%20demonstrate%20the%20impact%20of%20successful%20spoofing%20and%20replay%0Aattacks%20against%20PTP%20synchronization.%20We%20show%20how%20a%20spoofing%20attack%20is%20able%20to%0Acause%20a%20production-ready%20O-RAN%20and%205G-compliant%20private%20cellular%20base%20station%0Ato%20catastrophically%20fail%20within%202%20seconds%20of%20the%20attack%2C%20necessitating%20manual%0Aintervention%20to%20restore%20full%20network%20operations.%20To%20counter%20this%2C%20we%20design%20a%0AMachine%20Learning%20%28ML%29-based%20monitoring%20solution%20capable%20of%20detecting%20various%0Amalicious%20attacks%20with%20over%2097.5%25%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIMESAFE%253A%2520Timing%2520Interruption%2520Monitoring%2520and%2520Security%2520Assessment%2520for%250A%2520%2520Fronthaul%2520Environments%26entry.906535625%3DJoshua%2520Groen%2520and%2520Simone%2520Di%2520Valerio%2520and%2520Imtiaz%2520Karim%2520and%2520Davide%2520Villa%2520and%2520Yiewi%2520Zhang%2520and%2520Leonardo%2520Bonati%2520and%2520Michele%2520Polese%2520and%2520Salvatore%2520D%2527Oro%2520and%2520Tommaso%2520Melodia%2520and%2520Elisa%2520Bertino%2520and%2520Francesca%2520Cuomo%2520and%2520Kaushik%2520Chowdhury%26entry.1292438233%3D%2520%25205G%2520and%2520beyond%2520cellular%2520systems%2520embrace%2520the%2520disaggregation%2520of%2520Radio%2520Access%250ANetwork%2520%2528RAN%2529%2520components%252C%2520exemplified%2520by%2520the%2520evolution%2520of%2520the%2520fronthual%2520%2528FH%2529%250Aconnection%2520between%2520cellular%2520baseband%2520and%2520radio%2520unit%2520equipment.%2520Crucially%252C%250Asynchronization%2520over%2520the%2520FH%2520is%2520pivotal%2520for%2520reliable%25205G%2520services.%2520In%2520recent%250Ayears%252C%2520there%2520has%2520been%2520a%2520push%2520to%2520move%2520these%2520links%2520to%2520an%2520Ethernet-based%2520packet%250Anetwork%2520topology%252C%2520leveraging%2520existing%2520standards%2520and%2520ongoing%2520research%2520for%250ATime-Sensitive%2520Networking%2520%2528TSN%2529.%2520However%252C%2520TSN%2520standards%252C%2520such%2520as%2520Precision%2520Time%250AProtocol%2520%2528PTP%2529%252C%2520focus%2520on%2520performance%2520with%2520little%2520to%2520no%2520concern%2520for%2520security.%250AThis%2520increases%2520the%2520exposure%2520of%2520the%2520open%2520FH%2520to%2520security%2520risks.%2520Attacks%2520targeting%250Asynchronization%2520mechanisms%2520pose%2520significant%2520threats%252C%2520potentially%2520disrupting%25205G%250Anetworks%2520and%2520impairing%2520connectivity.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520the%2520impact%2520of%2520successful%2520spoofing%2520and%2520replay%250Aattacks%2520against%2520PTP%2520synchronization.%2520We%2520show%2520how%2520a%2520spoofing%2520attack%2520is%2520able%2520to%250Acause%2520a%2520production-ready%2520O-RAN%2520and%25205G-compliant%2520private%2520cellular%2520base%2520station%250Ato%2520catastrophically%2520fail%2520within%25202%2520seconds%2520of%2520the%2520attack%252C%2520necessitating%2520manual%250Aintervention%2520to%2520restore%2520full%2520network%2520operations.%2520To%2520counter%2520this%252C%2520we%2520design%2520a%250AMachine%2520Learning%2520%2528ML%2529-based%2520monitoring%2520solution%2520capable%2520of%2520detecting%2520various%250Amalicious%2520attacks%2520with%2520over%252097.5%2525%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIMESAFE%3A%20Timing%20Interruption%20Monitoring%20and%20Security%20Assessment%20for%0A%20%20Fronthaul%20Environments&entry.906535625=Joshua%20Groen%20and%20Simone%20Di%20Valerio%20and%20Imtiaz%20Karim%20and%20Davide%20Villa%20and%20Yiewi%20Zhang%20and%20Leonardo%20Bonati%20and%20Michele%20Polese%20and%20Salvatore%20D%27Oro%20and%20Tommaso%20Melodia%20and%20Elisa%20Bertino%20and%20Francesca%20Cuomo%20and%20Kaushik%20Chowdhury&entry.1292438233=%20%205G%20and%20beyond%20cellular%20systems%20embrace%20the%20disaggregation%20of%20Radio%20Access%0ANetwork%20%28RAN%29%20components%2C%20exemplified%20by%20the%20evolution%20of%20the%20fronthual%20%28FH%29%0Aconnection%20between%20cellular%20baseband%20and%20radio%20unit%20equipment.%20Crucially%2C%0Asynchronization%20over%20the%20FH%20is%20pivotal%20for%20reliable%205G%20services.%20In%20recent%0Ayears%2C%20there%20has%20been%20a%20push%20to%20move%20these%20links%20to%20an%20Ethernet-based%20packet%0Anetwork%20topology%2C%20leveraging%20existing%20standards%20and%20ongoing%20research%20for%0ATime-Sensitive%20Networking%20%28TSN%29.%20However%2C%20TSN%20standards%2C%20such%20as%20Precision%20Time%0AProtocol%20%28PTP%29%2C%20focus%20on%20performance%20with%20little%20to%20no%20concern%20for%20security.%0AThis%20increases%20the%20exposure%20of%20the%20open%20FH%20to%20security%20risks.%20Attacks%20targeting%0Asynchronization%20mechanisms%20pose%20significant%20threats%2C%20potentially%20disrupting%205G%0Anetworks%20and%20impairing%20connectivity.%0A%20%20In%20this%20paper%2C%20we%20demonstrate%20the%20impact%20of%20successful%20spoofing%20and%20replay%0Aattacks%20against%20PTP%20synchronization.%20We%20show%20how%20a%20spoofing%20attack%20is%20able%20to%0Acause%20a%20production-ready%20O-RAN%20and%205G-compliant%20private%20cellular%20base%20station%0Ato%20catastrophically%20fail%20within%202%20seconds%20of%20the%20attack%2C%20necessitating%20manual%0Aintervention%20to%20restore%20full%20network%20operations.%20To%20counter%20this%2C%20we%20design%20a%0AMachine%20Learning%20%28ML%29-based%20monitoring%20solution%20capable%20of%20detecting%20various%0Amalicious%20attacks%20with%20over%2097.5%25%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13049v1&entry.124074799=Read"},
{"title": "Causal Invariance Learning via Efficient Optimization of a Nonconvex\n  Objective", "author": "Zhenyu Wang and Yifan Hu and Peter B\u00fchlmann and Zijian Guo", "abstract": "  Data from multiple environments offer valuable opportunities to uncover\ncausal relationships among variables. Leveraging the assumption that the causal\noutcome model remains invariant across heterogeneous environments,\nstate-of-the-art methods attempt to identify causal outcome models by learning\ninvariant prediction models and rely on exhaustive searches over all\n(exponentially many) covariate subsets. These approaches present two major\nchallenges: 1) determining the conditions under which the invariant prediction\nmodel aligns with the causal outcome model, and 2) devising computationally\nefficient causal discovery algorithms that scale polynomially, instead of\nexponentially, with the number of covariates. To address both challenges, we\nfocus on the additive intervention regime and propose nearly necessary and\nsufficient conditions for ensuring that the invariant prediction model matches\nthe causal outcome model. Exploiting the essentially necessary identifiability\nconditions, we introduce Negative Weight Distributionally Robust Optimization\n(NegDRO), a nonconvex continuous minimax optimization whose global optimizer\nrecovers the causal outcome model. Unlike standard group DRO problems that\nmaximize over the simplex, NegDRO allows negative weights on environment\nlosses, which break the convexity. Despite its nonconvexity, we demonstrate\nthat a standard gradient method converges to the causal outcome model, and we\nestablish the convergence rate with respect to the sample size and the number\nof iterations. Our algorithm avoids exhaustive search, making it scalable\nespecially when the number of covariates is large. The numerical results\nfurther validate the efficiency of the proposed method.\n", "link": "http://arxiv.org/abs/2412.11850v2", "date": "2024-12-17", "relevancy": 1.4158, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4763}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4731}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Invariance%20Learning%20via%20Efficient%20Optimization%20of%20a%20Nonconvex%0A%20%20Objective&body=Title%3A%20Causal%20Invariance%20Learning%20via%20Efficient%20Optimization%20of%20a%20Nonconvex%0A%20%20Objective%0AAuthor%3A%20Zhenyu%20Wang%20and%20Yifan%20Hu%20and%20Peter%20B%C3%BChlmann%20and%20Zijian%20Guo%0AAbstract%3A%20%20%20Data%20from%20multiple%20environments%20offer%20valuable%20opportunities%20to%20uncover%0Acausal%20relationships%20among%20variables.%20Leveraging%20the%20assumption%20that%20the%20causal%0Aoutcome%20model%20remains%20invariant%20across%20heterogeneous%20environments%2C%0Astate-of-the-art%20methods%20attempt%20to%20identify%20causal%20outcome%20models%20by%20learning%0Ainvariant%20prediction%20models%20and%20rely%20on%20exhaustive%20searches%20over%20all%0A%28exponentially%20many%29%20covariate%20subsets.%20These%20approaches%20present%20two%20major%0Achallenges%3A%201%29%20determining%20the%20conditions%20under%20which%20the%20invariant%20prediction%0Amodel%20aligns%20with%20the%20causal%20outcome%20model%2C%20and%202%29%20devising%20computationally%0Aefficient%20causal%20discovery%20algorithms%20that%20scale%20polynomially%2C%20instead%20of%0Aexponentially%2C%20with%20the%20number%20of%20covariates.%20To%20address%20both%20challenges%2C%20we%0Afocus%20on%20the%20additive%20intervention%20regime%20and%20propose%20nearly%20necessary%20and%0Asufficient%20conditions%20for%20ensuring%20that%20the%20invariant%20prediction%20model%20matches%0Athe%20causal%20outcome%20model.%20Exploiting%20the%20essentially%20necessary%20identifiability%0Aconditions%2C%20we%20introduce%20Negative%20Weight%20Distributionally%20Robust%20Optimization%0A%28NegDRO%29%2C%20a%20nonconvex%20continuous%20minimax%20optimization%20whose%20global%20optimizer%0Arecovers%20the%20causal%20outcome%20model.%20Unlike%20standard%20group%20DRO%20problems%20that%0Amaximize%20over%20the%20simplex%2C%20NegDRO%20allows%20negative%20weights%20on%20environment%0Alosses%2C%20which%20break%20the%20convexity.%20Despite%20its%20nonconvexity%2C%20we%20demonstrate%0Athat%20a%20standard%20gradient%20method%20converges%20to%20the%20causal%20outcome%20model%2C%20and%20we%0Aestablish%20the%20convergence%20rate%20with%20respect%20to%20the%20sample%20size%20and%20the%20number%0Aof%20iterations.%20Our%20algorithm%20avoids%20exhaustive%20search%2C%20making%20it%20scalable%0Aespecially%20when%20the%20number%20of%20covariates%20is%20large.%20The%20numerical%20results%0Afurther%20validate%20the%20efficiency%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Invariance%2520Learning%2520via%2520Efficient%2520Optimization%2520of%2520a%2520Nonconvex%250A%2520%2520Objective%26entry.906535625%3DZhenyu%2520Wang%2520and%2520Yifan%2520Hu%2520and%2520Peter%2520B%25C3%25BChlmann%2520and%2520Zijian%2520Guo%26entry.1292438233%3D%2520%2520Data%2520from%2520multiple%2520environments%2520offer%2520valuable%2520opportunities%2520to%2520uncover%250Acausal%2520relationships%2520among%2520variables.%2520Leveraging%2520the%2520assumption%2520that%2520the%2520causal%250Aoutcome%2520model%2520remains%2520invariant%2520across%2520heterogeneous%2520environments%252C%250Astate-of-the-art%2520methods%2520attempt%2520to%2520identify%2520causal%2520outcome%2520models%2520by%2520learning%250Ainvariant%2520prediction%2520models%2520and%2520rely%2520on%2520exhaustive%2520searches%2520over%2520all%250A%2528exponentially%2520many%2529%2520covariate%2520subsets.%2520These%2520approaches%2520present%2520two%2520major%250Achallenges%253A%25201%2529%2520determining%2520the%2520conditions%2520under%2520which%2520the%2520invariant%2520prediction%250Amodel%2520aligns%2520with%2520the%2520causal%2520outcome%2520model%252C%2520and%25202%2529%2520devising%2520computationally%250Aefficient%2520causal%2520discovery%2520algorithms%2520that%2520scale%2520polynomially%252C%2520instead%2520of%250Aexponentially%252C%2520with%2520the%2520number%2520of%2520covariates.%2520To%2520address%2520both%2520challenges%252C%2520we%250Afocus%2520on%2520the%2520additive%2520intervention%2520regime%2520and%2520propose%2520nearly%2520necessary%2520and%250Asufficient%2520conditions%2520for%2520ensuring%2520that%2520the%2520invariant%2520prediction%2520model%2520matches%250Athe%2520causal%2520outcome%2520model.%2520Exploiting%2520the%2520essentially%2520necessary%2520identifiability%250Aconditions%252C%2520we%2520introduce%2520Negative%2520Weight%2520Distributionally%2520Robust%2520Optimization%250A%2528NegDRO%2529%252C%2520a%2520nonconvex%2520continuous%2520minimax%2520optimization%2520whose%2520global%2520optimizer%250Arecovers%2520the%2520causal%2520outcome%2520model.%2520Unlike%2520standard%2520group%2520DRO%2520problems%2520that%250Amaximize%2520over%2520the%2520simplex%252C%2520NegDRO%2520allows%2520negative%2520weights%2520on%2520environment%250Alosses%252C%2520which%2520break%2520the%2520convexity.%2520Despite%2520its%2520nonconvexity%252C%2520we%2520demonstrate%250Athat%2520a%2520standard%2520gradient%2520method%2520converges%2520to%2520the%2520causal%2520outcome%2520model%252C%2520and%2520we%250Aestablish%2520the%2520convergence%2520rate%2520with%2520respect%2520to%2520the%2520sample%2520size%2520and%2520the%2520number%250Aof%2520iterations.%2520Our%2520algorithm%2520avoids%2520exhaustive%2520search%252C%2520making%2520it%2520scalable%250Aespecially%2520when%2520the%2520number%2520of%2520covariates%2520is%2520large.%2520The%2520numerical%2520results%250Afurther%2520validate%2520the%2520efficiency%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Invariance%20Learning%20via%20Efficient%20Optimization%20of%20a%20Nonconvex%0A%20%20Objective&entry.906535625=Zhenyu%20Wang%20and%20Yifan%20Hu%20and%20Peter%20B%C3%BChlmann%20and%20Zijian%20Guo&entry.1292438233=%20%20Data%20from%20multiple%20environments%20offer%20valuable%20opportunities%20to%20uncover%0Acausal%20relationships%20among%20variables.%20Leveraging%20the%20assumption%20that%20the%20causal%0Aoutcome%20model%20remains%20invariant%20across%20heterogeneous%20environments%2C%0Astate-of-the-art%20methods%20attempt%20to%20identify%20causal%20outcome%20models%20by%20learning%0Ainvariant%20prediction%20models%20and%20rely%20on%20exhaustive%20searches%20over%20all%0A%28exponentially%20many%29%20covariate%20subsets.%20These%20approaches%20present%20two%20major%0Achallenges%3A%201%29%20determining%20the%20conditions%20under%20which%20the%20invariant%20prediction%0Amodel%20aligns%20with%20the%20causal%20outcome%20model%2C%20and%202%29%20devising%20computationally%0Aefficient%20causal%20discovery%20algorithms%20that%20scale%20polynomially%2C%20instead%20of%0Aexponentially%2C%20with%20the%20number%20of%20covariates.%20To%20address%20both%20challenges%2C%20we%0Afocus%20on%20the%20additive%20intervention%20regime%20and%20propose%20nearly%20necessary%20and%0Asufficient%20conditions%20for%20ensuring%20that%20the%20invariant%20prediction%20model%20matches%0Athe%20causal%20outcome%20model.%20Exploiting%20the%20essentially%20necessary%20identifiability%0Aconditions%2C%20we%20introduce%20Negative%20Weight%20Distributionally%20Robust%20Optimization%0A%28NegDRO%29%2C%20a%20nonconvex%20continuous%20minimax%20optimization%20whose%20global%20optimizer%0Arecovers%20the%20causal%20outcome%20model.%20Unlike%20standard%20group%20DRO%20problems%20that%0Amaximize%20over%20the%20simplex%2C%20NegDRO%20allows%20negative%20weights%20on%20environment%0Alosses%2C%20which%20break%20the%20convexity.%20Despite%20its%20nonconvexity%2C%20we%20demonstrate%0Athat%20a%20standard%20gradient%20method%20converges%20to%20the%20causal%20outcome%20model%2C%20and%20we%0Aestablish%20the%20convergence%20rate%20with%20respect%20to%20the%20sample%20size%20and%20the%20number%0Aof%20iterations.%20Our%20algorithm%20avoids%20exhaustive%20search%2C%20making%20it%20scalable%0Aespecially%20when%20the%20number%20of%20covariates%20is%20large.%20The%20numerical%20results%0Afurther%20validate%20the%20efficiency%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11850v2&entry.124074799=Read"},
{"title": "Dual Interpretation of Machine Learning Forecasts", "author": "Philippe Goulet Coulombe and Maximilian Goebel and Karin Klieber", "abstract": "  Machine learning predictions are typically interpreted as the sum of\ncontributions of predictors. Yet, each out-of-sample prediction can also be\nexpressed as a linear combination of in-sample values of the predicted\nvariable, with weights corresponding to pairwise proximity scores between\ncurrent and past economic events. While this dual route leads nowhere in some\ncontexts (e.g., large cross-sectional datasets), it provides sparser\ninterpretations in settings with many regressors and little training data-like\nmacroeconomic forecasting. In this case, the sequence of contributions can be\nvisualized as a time series, allowing analysts to explain predictions as\nquantifiable combinations of historical analogies. Moreover, the weights can be\nviewed as those of a data portfolio, inspiring new diagnostic measures such as\nforecast concentration, short position, and turnover. We show how weights can\nbe retrieved seamlessly for (kernel) ridge regression, random forest, boosted\ntrees, and neural networks. Then, we apply these tools to analyze post-pandemic\nforecasts of inflation, GDP growth, and recession probabilities. In all cases,\nthe approach opens the black box from a new angle and demonstrates how machine\nlearning models leverage history partly repeating itself.\n", "link": "http://arxiv.org/abs/2412.13076v1", "date": "2024-12-17", "relevancy": 1.3948, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5005}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4599}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Interpretation%20of%20Machine%20Learning%20Forecasts&body=Title%3A%20Dual%20Interpretation%20of%20Machine%20Learning%20Forecasts%0AAuthor%3A%20Philippe%20Goulet%20Coulombe%20and%20Maximilian%20Goebel%20and%20Karin%20Klieber%0AAbstract%3A%20%20%20Machine%20learning%20predictions%20are%20typically%20interpreted%20as%20the%20sum%20of%0Acontributions%20of%20predictors.%20Yet%2C%20each%20out-of-sample%20prediction%20can%20also%20be%0Aexpressed%20as%20a%20linear%20combination%20of%20in-sample%20values%20of%20the%20predicted%0Avariable%2C%20with%20weights%20corresponding%20to%20pairwise%20proximity%20scores%20between%0Acurrent%20and%20past%20economic%20events.%20While%20this%20dual%20route%20leads%20nowhere%20in%20some%0Acontexts%20%28e.g.%2C%20large%20cross-sectional%20datasets%29%2C%20it%20provides%20sparser%0Ainterpretations%20in%20settings%20with%20many%20regressors%20and%20little%20training%20data-like%0Amacroeconomic%20forecasting.%20In%20this%20case%2C%20the%20sequence%20of%20contributions%20can%20be%0Avisualized%20as%20a%20time%20series%2C%20allowing%20analysts%20to%20explain%20predictions%20as%0Aquantifiable%20combinations%20of%20historical%20analogies.%20Moreover%2C%20the%20weights%20can%20be%0Aviewed%20as%20those%20of%20a%20data%20portfolio%2C%20inspiring%20new%20diagnostic%20measures%20such%20as%0Aforecast%20concentration%2C%20short%20position%2C%20and%20turnover.%20We%20show%20how%20weights%20can%0Abe%20retrieved%20seamlessly%20for%20%28kernel%29%20ridge%20regression%2C%20random%20forest%2C%20boosted%0Atrees%2C%20and%20neural%20networks.%20Then%2C%20we%20apply%20these%20tools%20to%20analyze%20post-pandemic%0Aforecasts%20of%20inflation%2C%20GDP%20growth%2C%20and%20recession%20probabilities.%20In%20all%20cases%2C%0Athe%20approach%20opens%20the%20black%20box%20from%20a%20new%20angle%20and%20demonstrates%20how%20machine%0Alearning%20models%20leverage%20history%20partly%20repeating%20itself.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Interpretation%2520of%2520Machine%2520Learning%2520Forecasts%26entry.906535625%3DPhilippe%2520Goulet%2520Coulombe%2520and%2520Maximilian%2520Goebel%2520and%2520Karin%2520Klieber%26entry.1292438233%3D%2520%2520Machine%2520learning%2520predictions%2520are%2520typically%2520interpreted%2520as%2520the%2520sum%2520of%250Acontributions%2520of%2520predictors.%2520Yet%252C%2520each%2520out-of-sample%2520prediction%2520can%2520also%2520be%250Aexpressed%2520as%2520a%2520linear%2520combination%2520of%2520in-sample%2520values%2520of%2520the%2520predicted%250Avariable%252C%2520with%2520weights%2520corresponding%2520to%2520pairwise%2520proximity%2520scores%2520between%250Acurrent%2520and%2520past%2520economic%2520events.%2520While%2520this%2520dual%2520route%2520leads%2520nowhere%2520in%2520some%250Acontexts%2520%2528e.g.%252C%2520large%2520cross-sectional%2520datasets%2529%252C%2520it%2520provides%2520sparser%250Ainterpretations%2520in%2520settings%2520with%2520many%2520regressors%2520and%2520little%2520training%2520data-like%250Amacroeconomic%2520forecasting.%2520In%2520this%2520case%252C%2520the%2520sequence%2520of%2520contributions%2520can%2520be%250Avisualized%2520as%2520a%2520time%2520series%252C%2520allowing%2520analysts%2520to%2520explain%2520predictions%2520as%250Aquantifiable%2520combinations%2520of%2520historical%2520analogies.%2520Moreover%252C%2520the%2520weights%2520can%2520be%250Aviewed%2520as%2520those%2520of%2520a%2520data%2520portfolio%252C%2520inspiring%2520new%2520diagnostic%2520measures%2520such%2520as%250Aforecast%2520concentration%252C%2520short%2520position%252C%2520and%2520turnover.%2520We%2520show%2520how%2520weights%2520can%250Abe%2520retrieved%2520seamlessly%2520for%2520%2528kernel%2529%2520ridge%2520regression%252C%2520random%2520forest%252C%2520boosted%250Atrees%252C%2520and%2520neural%2520networks.%2520Then%252C%2520we%2520apply%2520these%2520tools%2520to%2520analyze%2520post-pandemic%250Aforecasts%2520of%2520inflation%252C%2520GDP%2520growth%252C%2520and%2520recession%2520probabilities.%2520In%2520all%2520cases%252C%250Athe%2520approach%2520opens%2520the%2520black%2520box%2520from%2520a%2520new%2520angle%2520and%2520demonstrates%2520how%2520machine%250Alearning%2520models%2520leverage%2520history%2520partly%2520repeating%2520itself.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Interpretation%20of%20Machine%20Learning%20Forecasts&entry.906535625=Philippe%20Goulet%20Coulombe%20and%20Maximilian%20Goebel%20and%20Karin%20Klieber&entry.1292438233=%20%20Machine%20learning%20predictions%20are%20typically%20interpreted%20as%20the%20sum%20of%0Acontributions%20of%20predictors.%20Yet%2C%20each%20out-of-sample%20prediction%20can%20also%20be%0Aexpressed%20as%20a%20linear%20combination%20of%20in-sample%20values%20of%20the%20predicted%0Avariable%2C%20with%20weights%20corresponding%20to%20pairwise%20proximity%20scores%20between%0Acurrent%20and%20past%20economic%20events.%20While%20this%20dual%20route%20leads%20nowhere%20in%20some%0Acontexts%20%28e.g.%2C%20large%20cross-sectional%20datasets%29%2C%20it%20provides%20sparser%0Ainterpretations%20in%20settings%20with%20many%20regressors%20and%20little%20training%20data-like%0Amacroeconomic%20forecasting.%20In%20this%20case%2C%20the%20sequence%20of%20contributions%20can%20be%0Avisualized%20as%20a%20time%20series%2C%20allowing%20analysts%20to%20explain%20predictions%20as%0Aquantifiable%20combinations%20of%20historical%20analogies.%20Moreover%2C%20the%20weights%20can%20be%0Aviewed%20as%20those%20of%20a%20data%20portfolio%2C%20inspiring%20new%20diagnostic%20measures%20such%20as%0Aforecast%20concentration%2C%20short%20position%2C%20and%20turnover.%20We%20show%20how%20weights%20can%0Abe%20retrieved%20seamlessly%20for%20%28kernel%29%20ridge%20regression%2C%20random%20forest%2C%20boosted%0Atrees%2C%20and%20neural%20networks.%20Then%2C%20we%20apply%20these%20tools%20to%20analyze%20post-pandemic%0Aforecasts%20of%20inflation%2C%20GDP%20growth%2C%20and%20recession%20probabilities.%20In%20all%20cases%2C%0Athe%20approach%20opens%20the%20black%20box%20from%20a%20new%20angle%20and%20demonstrates%20how%20machine%0Alearning%20models%20leverage%20history%20partly%20repeating%20itself.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13076v1&entry.124074799=Read"},
{"title": "Tilted Quantile Gradient Updates for Quantile-Constrained Reinforcement\n  Learning", "author": "Chenglin Li and Guangchun Ruan and Hua Geng", "abstract": "  Safe reinforcement learning (RL) is a popular and versatile paradigm to learn\nreward-maximizing policies with safety guarantees. Previous works tend to\nexpress the safety constraints in an expectation form due to the ease of\nimplementation, but this turns out to be ineffective in maintaining safety\nconstraints with high probability. To this end, we move to the\nquantile-constrained RL that enables a higher level of safety without any\nexpectation-form approximations. We directly estimate the quantile gradients\nthrough sampling and provide the theoretical proofs of convergence. Then a\ntilted update strategy for quantile gradients is implemented to compensate the\nasymmetric distributional density, with a direct benefit of return performance.\nExperiments demonstrate that the proposed model fully meets safety requirements\n(quantile constraints) while outperforming the state-of-the-art benchmarks with\nhigher return.\n", "link": "http://arxiv.org/abs/2412.13184v1", "date": "2024-12-17", "relevancy": 1.3802, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4751}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4584}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tilted%20Quantile%20Gradient%20Updates%20for%20Quantile-Constrained%20Reinforcement%0A%20%20Learning&body=Title%3A%20Tilted%20Quantile%20Gradient%20Updates%20for%20Quantile-Constrained%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Chenglin%20Li%20and%20Guangchun%20Ruan%20and%20Hua%20Geng%0AAbstract%3A%20%20%20Safe%20reinforcement%20learning%20%28RL%29%20is%20a%20popular%20and%20versatile%20paradigm%20to%20learn%0Areward-maximizing%20policies%20with%20safety%20guarantees.%20Previous%20works%20tend%20to%0Aexpress%20the%20safety%20constraints%20in%20an%20expectation%20form%20due%20to%20the%20ease%20of%0Aimplementation%2C%20but%20this%20turns%20out%20to%20be%20ineffective%20in%20maintaining%20safety%0Aconstraints%20with%20high%20probability.%20To%20this%20end%2C%20we%20move%20to%20the%0Aquantile-constrained%20RL%20that%20enables%20a%20higher%20level%20of%20safety%20without%20any%0Aexpectation-form%20approximations.%20We%20directly%20estimate%20the%20quantile%20gradients%0Athrough%20sampling%20and%20provide%20the%20theoretical%20proofs%20of%20convergence.%20Then%20a%0Atilted%20update%20strategy%20for%20quantile%20gradients%20is%20implemented%20to%20compensate%20the%0Aasymmetric%20distributional%20density%2C%20with%20a%20direct%20benefit%20of%20return%20performance.%0AExperiments%20demonstrate%20that%20the%20proposed%20model%20fully%20meets%20safety%20requirements%0A%28quantile%20constraints%29%20while%20outperforming%20the%20state-of-the-art%20benchmarks%20with%0Ahigher%20return.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTilted%2520Quantile%2520Gradient%2520Updates%2520for%2520Quantile-Constrained%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DChenglin%2520Li%2520and%2520Guangchun%2520Ruan%2520and%2520Hua%2520Geng%26entry.1292438233%3D%2520%2520Safe%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520a%2520popular%2520and%2520versatile%2520paradigm%2520to%2520learn%250Areward-maximizing%2520policies%2520with%2520safety%2520guarantees.%2520Previous%2520works%2520tend%2520to%250Aexpress%2520the%2520safety%2520constraints%2520in%2520an%2520expectation%2520form%2520due%2520to%2520the%2520ease%2520of%250Aimplementation%252C%2520but%2520this%2520turns%2520out%2520to%2520be%2520ineffective%2520in%2520maintaining%2520safety%250Aconstraints%2520with%2520high%2520probability.%2520To%2520this%2520end%252C%2520we%2520move%2520to%2520the%250Aquantile-constrained%2520RL%2520that%2520enables%2520a%2520higher%2520level%2520of%2520safety%2520without%2520any%250Aexpectation-form%2520approximations.%2520We%2520directly%2520estimate%2520the%2520quantile%2520gradients%250Athrough%2520sampling%2520and%2520provide%2520the%2520theoretical%2520proofs%2520of%2520convergence.%2520Then%2520a%250Atilted%2520update%2520strategy%2520for%2520quantile%2520gradients%2520is%2520implemented%2520to%2520compensate%2520the%250Aasymmetric%2520distributional%2520density%252C%2520with%2520a%2520direct%2520benefit%2520of%2520return%2520performance.%250AExperiments%2520demonstrate%2520that%2520the%2520proposed%2520model%2520fully%2520meets%2520safety%2520requirements%250A%2528quantile%2520constraints%2529%2520while%2520outperforming%2520the%2520state-of-the-art%2520benchmarks%2520with%250Ahigher%2520return.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tilted%20Quantile%20Gradient%20Updates%20for%20Quantile-Constrained%20Reinforcement%0A%20%20Learning&entry.906535625=Chenglin%20Li%20and%20Guangchun%20Ruan%20and%20Hua%20Geng&entry.1292438233=%20%20Safe%20reinforcement%20learning%20%28RL%29%20is%20a%20popular%20and%20versatile%20paradigm%20to%20learn%0Areward-maximizing%20policies%20with%20safety%20guarantees.%20Previous%20works%20tend%20to%0Aexpress%20the%20safety%20constraints%20in%20an%20expectation%20form%20due%20to%20the%20ease%20of%0Aimplementation%2C%20but%20this%20turns%20out%20to%20be%20ineffective%20in%20maintaining%20safety%0Aconstraints%20with%20high%20probability.%20To%20this%20end%2C%20we%20move%20to%20the%0Aquantile-constrained%20RL%20that%20enables%20a%20higher%20level%20of%20safety%20without%20any%0Aexpectation-form%20approximations.%20We%20directly%20estimate%20the%20quantile%20gradients%0Athrough%20sampling%20and%20provide%20the%20theoretical%20proofs%20of%20convergence.%20Then%20a%0Atilted%20update%20strategy%20for%20quantile%20gradients%20is%20implemented%20to%20compensate%20the%0Aasymmetric%20distributional%20density%2C%20with%20a%20direct%20benefit%20of%20return%20performance.%0AExperiments%20demonstrate%20that%20the%20proposed%20model%20fully%20meets%20safety%20requirements%0A%28quantile%20constraints%29%20while%20outperforming%20the%20state-of-the-art%20benchmarks%20with%0Ahigher%20return.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13184v1&entry.124074799=Read"},
{"title": "Deep Learning Based Superconductivity: Prediction and Experimental Tests", "author": "Daniel Kaplan and Adam Zhang and Joanna Blawat and Rongying Jin and Robert J. Cava and Viktor Oudovenko and Gabriel Kotliar and Anirvan M. Sengupta and Weiwei Xie", "abstract": "  The discovery of novel superconducting materials is a longstanding challenge\nin materials science, with a wealth of potential for applications in energy,\ntransportation, and computing. Recent advances in artificial intelligence (AI)\nhave enabled expediting the search for new materials by efficiently utilizing\nvast materials databases. In this study, we developed an approach based on deep\nlearning (DL) to predict new superconducting materials. We have synthesized a\ncompound derived from our DL network and confirmed its superconducting\nproperties in agreement with our prediction. Our approach is also compared to\nprevious work based on random forests (RFs). In particular, RFs require\nknowledge of the chem-ical properties of the compound, while our neural net\ninputs depend solely on the chemical composition. With the help of hints from\nour network, we discover a new ternary compound\n$\\textrm{Mo}_{20}\\textrm{Re}_{6}\\textrm{Si}_{4}$, which becomes superconducting\nbelow 5.4 K. We further discuss the existing limitations and challenges\nassociated with using AI to predict and, along with potential future research\ndirections.\n", "link": "http://arxiv.org/abs/2412.13012v1", "date": "2024-12-17", "relevancy": 1.3139, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4585}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4385}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Based%20Superconductivity%3A%20Prediction%20and%20Experimental%20Tests&body=Title%3A%20Deep%20Learning%20Based%20Superconductivity%3A%20Prediction%20and%20Experimental%20Tests%0AAuthor%3A%20Daniel%20Kaplan%20and%20Adam%20Zhang%20and%20Joanna%20Blawat%20and%20Rongying%20Jin%20and%20Robert%20J.%20Cava%20and%20Viktor%20Oudovenko%20and%20Gabriel%20Kotliar%20and%20Anirvan%20M.%20Sengupta%20and%20Weiwei%20Xie%0AAbstract%3A%20%20%20The%20discovery%20of%20novel%20superconducting%20materials%20is%20a%20longstanding%20challenge%0Ain%20materials%20science%2C%20with%20a%20wealth%20of%20potential%20for%20applications%20in%20energy%2C%0Atransportation%2C%20and%20computing.%20Recent%20advances%20in%20artificial%20intelligence%20%28AI%29%0Ahave%20enabled%20expediting%20the%20search%20for%20new%20materials%20by%20efficiently%20utilizing%0Avast%20materials%20databases.%20In%20this%20study%2C%20we%20developed%20an%20approach%20based%20on%20deep%0Alearning%20%28DL%29%20to%20predict%20new%20superconducting%20materials.%20We%20have%20synthesized%20a%0Acompound%20derived%20from%20our%20DL%20network%20and%20confirmed%20its%20superconducting%0Aproperties%20in%20agreement%20with%20our%20prediction.%20Our%20approach%20is%20also%20compared%20to%0Aprevious%20work%20based%20on%20random%20forests%20%28RFs%29.%20In%20particular%2C%20RFs%20require%0Aknowledge%20of%20the%20chem-ical%20properties%20of%20the%20compound%2C%20while%20our%20neural%20net%0Ainputs%20depend%20solely%20on%20the%20chemical%20composition.%20With%20the%20help%20of%20hints%20from%0Aour%20network%2C%20we%20discover%20a%20new%20ternary%20compound%0A%24%5Ctextrm%7BMo%7D_%7B20%7D%5Ctextrm%7BRe%7D_%7B6%7D%5Ctextrm%7BSi%7D_%7B4%7D%24%2C%20which%20becomes%20superconducting%0Abelow%205.4%20K.%20We%20further%20discuss%20the%20existing%20limitations%20and%20challenges%0Aassociated%20with%20using%20AI%20to%20predict%20and%2C%20along%20with%20potential%20future%20research%0Adirections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Based%2520Superconductivity%253A%2520Prediction%2520and%2520Experimental%2520Tests%26entry.906535625%3DDaniel%2520Kaplan%2520and%2520Adam%2520Zhang%2520and%2520Joanna%2520Blawat%2520and%2520Rongying%2520Jin%2520and%2520Robert%2520J.%2520Cava%2520and%2520Viktor%2520Oudovenko%2520and%2520Gabriel%2520Kotliar%2520and%2520Anirvan%2520M.%2520Sengupta%2520and%2520Weiwei%2520Xie%26entry.1292438233%3D%2520%2520The%2520discovery%2520of%2520novel%2520superconducting%2520materials%2520is%2520a%2520longstanding%2520challenge%250Ain%2520materials%2520science%252C%2520with%2520a%2520wealth%2520of%2520potential%2520for%2520applications%2520in%2520energy%252C%250Atransportation%252C%2520and%2520computing.%2520Recent%2520advances%2520in%2520artificial%2520intelligence%2520%2528AI%2529%250Ahave%2520enabled%2520expediting%2520the%2520search%2520for%2520new%2520materials%2520by%2520efficiently%2520utilizing%250Avast%2520materials%2520databases.%2520In%2520this%2520study%252C%2520we%2520developed%2520an%2520approach%2520based%2520on%2520deep%250Alearning%2520%2528DL%2529%2520to%2520predict%2520new%2520superconducting%2520materials.%2520We%2520have%2520synthesized%2520a%250Acompound%2520derived%2520from%2520our%2520DL%2520network%2520and%2520confirmed%2520its%2520superconducting%250Aproperties%2520in%2520agreement%2520with%2520our%2520prediction.%2520Our%2520approach%2520is%2520also%2520compared%2520to%250Aprevious%2520work%2520based%2520on%2520random%2520forests%2520%2528RFs%2529.%2520In%2520particular%252C%2520RFs%2520require%250Aknowledge%2520of%2520the%2520chem-ical%2520properties%2520of%2520the%2520compound%252C%2520while%2520our%2520neural%2520net%250Ainputs%2520depend%2520solely%2520on%2520the%2520chemical%2520composition.%2520With%2520the%2520help%2520of%2520hints%2520from%250Aour%2520network%252C%2520we%2520discover%2520a%2520new%2520ternary%2520compound%250A%2524%255Ctextrm%257BMo%257D_%257B20%257D%255Ctextrm%257BRe%257D_%257B6%257D%255Ctextrm%257BSi%257D_%257B4%257D%2524%252C%2520which%2520becomes%2520superconducting%250Abelow%25205.4%2520K.%2520We%2520further%2520discuss%2520the%2520existing%2520limitations%2520and%2520challenges%250Aassociated%2520with%2520using%2520AI%2520to%2520predict%2520and%252C%2520along%2520with%2520potential%2520future%2520research%250Adirections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Based%20Superconductivity%3A%20Prediction%20and%20Experimental%20Tests&entry.906535625=Daniel%20Kaplan%20and%20Adam%20Zhang%20and%20Joanna%20Blawat%20and%20Rongying%20Jin%20and%20Robert%20J.%20Cava%20and%20Viktor%20Oudovenko%20and%20Gabriel%20Kotliar%20and%20Anirvan%20M.%20Sengupta%20and%20Weiwei%20Xie&entry.1292438233=%20%20The%20discovery%20of%20novel%20superconducting%20materials%20is%20a%20longstanding%20challenge%0Ain%20materials%20science%2C%20with%20a%20wealth%20of%20potential%20for%20applications%20in%20energy%2C%0Atransportation%2C%20and%20computing.%20Recent%20advances%20in%20artificial%20intelligence%20%28AI%29%0Ahave%20enabled%20expediting%20the%20search%20for%20new%20materials%20by%20efficiently%20utilizing%0Avast%20materials%20databases.%20In%20this%20study%2C%20we%20developed%20an%20approach%20based%20on%20deep%0Alearning%20%28DL%29%20to%20predict%20new%20superconducting%20materials.%20We%20have%20synthesized%20a%0Acompound%20derived%20from%20our%20DL%20network%20and%20confirmed%20its%20superconducting%0Aproperties%20in%20agreement%20with%20our%20prediction.%20Our%20approach%20is%20also%20compared%20to%0Aprevious%20work%20based%20on%20random%20forests%20%28RFs%29.%20In%20particular%2C%20RFs%20require%0Aknowledge%20of%20the%20chem-ical%20properties%20of%20the%20compound%2C%20while%20our%20neural%20net%0Ainputs%20depend%20solely%20on%20the%20chemical%20composition.%20With%20the%20help%20of%20hints%20from%0Aour%20network%2C%20we%20discover%20a%20new%20ternary%20compound%0A%24%5Ctextrm%7BMo%7D_%7B20%7D%5Ctextrm%7BRe%7D_%7B6%7D%5Ctextrm%7BSi%7D_%7B4%7D%24%2C%20which%20becomes%20superconducting%0Abelow%205.4%20K.%20We%20further%20discuss%20the%20existing%20limitations%20and%20challenges%0Aassociated%20with%20using%20AI%20to%20predict%20and%2C%20along%20with%20potential%20future%20research%0Adirections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13012v1&entry.124074799=Read"},
{"title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben", "author": "Rainer Muehlhoff and Marte Henningsen", "abstract": "  [Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts.\n", "link": "http://arxiv.org/abs/2412.06651v4", "date": "2024-12-17", "relevancy": 1.1738, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4011}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.399}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chatbots%20im%20Schulunterricht%3A%20Wir%20testen%20das%20Fobizz-Tool%20zur%0A%20%20automatischen%20Bewertung%20von%20Hausaufgaben&body=Title%3A%20Chatbots%20im%20Schulunterricht%3A%20Wir%20testen%20das%20Fobizz-Tool%20zur%0A%20%20automatischen%20Bewertung%20von%20Hausaufgaben%0AAuthor%3A%20Rainer%20Muehlhoff%20and%20Marte%20Henningsen%0AAbstract%3A%20%20%20%5BStudy%20in%20German%20language.%5D%20This%20study%20examines%20the%20AI-powered%20grading%20tool%0A%22AI%20Grading%20Assistant%22%20by%20the%20German%20company%20Fobizz%2C%20designed%20to%20support%0Ateachers%20in%20evaluating%20and%20providing%20feedback%20on%20student%20assignments.%20Against%0Athe%20societal%20backdrop%20of%20an%20overburdened%20education%20system%20and%20rising%0Aexpectations%20for%20artificial%20intelligence%20as%20a%20solution%20to%20these%20challenges%2C%20the%0Ainvestigation%20evaluates%20the%20tool%27s%20functional%20suitability%20through%20two%20test%0Aseries.%20The%20results%20reveal%20significant%20shortcomings%3A%20The%20tool%27s%20numerical%0Agrades%20and%20qualitative%20feedback%20are%20often%20random%20and%20do%20not%20improve%20even%20when%0Aits%20suggestions%20are%20incorporated.%20The%20highest%20ratings%20are%20achievable%20only%20with%0Atexts%20generated%20by%20ChatGPT.%20False%20claims%20and%20nonsensical%20submissions%20frequently%0Ago%20undetected%2C%20while%20the%20implementation%20of%20some%20grading%20criteria%20is%20unreliable%0Aand%20opaque.%20Since%20these%20deficiencies%20stem%20from%20the%20inherent%20limitations%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20fundamental%20improvements%20to%20this%20or%20similar%20tools%0Aare%20not%20immediately%20foreseeable.%20The%20study%20critiques%20the%20broader%20trend%20of%0Aadopting%20AI%20as%20a%20quick%20fix%20for%20systemic%20problems%20in%20education%2C%20concluding%20that%0AFobizz%27s%20marketing%20of%20the%20tool%20as%20an%20objective%20and%20time-saving%20solution%20is%0Amisleading%20and%20irresponsible.%20Finally%2C%20the%20study%20calls%20for%20systematic%0Aevaluation%20and%20subject-specific%20pedagogical%20scrutiny%20of%20the%20use%20of%20AI%20tools%20in%0Aeducational%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06651v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatbots%2520im%2520Schulunterricht%253A%2520Wir%2520testen%2520das%2520Fobizz-Tool%2520zur%250A%2520%2520automatischen%2520Bewertung%2520von%2520Hausaufgaben%26entry.906535625%3DRainer%2520Muehlhoff%2520and%2520Marte%2520Henningsen%26entry.1292438233%3D%2520%2520%255BStudy%2520in%2520German%2520language.%255D%2520This%2520study%2520examines%2520the%2520AI-powered%2520grading%2520tool%250A%2522AI%2520Grading%2520Assistant%2522%2520by%2520the%2520German%2520company%2520Fobizz%252C%2520designed%2520to%2520support%250Ateachers%2520in%2520evaluating%2520and%2520providing%2520feedback%2520on%2520student%2520assignments.%2520Against%250Athe%2520societal%2520backdrop%2520of%2520an%2520overburdened%2520education%2520system%2520and%2520rising%250Aexpectations%2520for%2520artificial%2520intelligence%2520as%2520a%2520solution%2520to%2520these%2520challenges%252C%2520the%250Ainvestigation%2520evaluates%2520the%2520tool%2527s%2520functional%2520suitability%2520through%2520two%2520test%250Aseries.%2520The%2520results%2520reveal%2520significant%2520shortcomings%253A%2520The%2520tool%2527s%2520numerical%250Agrades%2520and%2520qualitative%2520feedback%2520are%2520often%2520random%2520and%2520do%2520not%2520improve%2520even%2520when%250Aits%2520suggestions%2520are%2520incorporated.%2520The%2520highest%2520ratings%2520are%2520achievable%2520only%2520with%250Atexts%2520generated%2520by%2520ChatGPT.%2520False%2520claims%2520and%2520nonsensical%2520submissions%2520frequently%250Ago%2520undetected%252C%2520while%2520the%2520implementation%2520of%2520some%2520grading%2520criteria%2520is%2520unreliable%250Aand%2520opaque.%2520Since%2520these%2520deficiencies%2520stem%2520from%2520the%2520inherent%2520limitations%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520fundamental%2520improvements%2520to%2520this%2520or%2520similar%2520tools%250Aare%2520not%2520immediately%2520foreseeable.%2520The%2520study%2520critiques%2520the%2520broader%2520trend%2520of%250Aadopting%2520AI%2520as%2520a%2520quick%2520fix%2520for%2520systemic%2520problems%2520in%2520education%252C%2520concluding%2520that%250AFobizz%2527s%2520marketing%2520of%2520the%2520tool%2520as%2520an%2520objective%2520and%2520time-saving%2520solution%2520is%250Amisleading%2520and%2520irresponsible.%2520Finally%252C%2520the%2520study%2520calls%2520for%2520systematic%250Aevaluation%2520and%2520subject-specific%2520pedagogical%2520scrutiny%2520of%2520the%2520use%2520of%2520AI%2520tools%2520in%250Aeducational%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06651v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chatbots%20im%20Schulunterricht%3A%20Wir%20testen%20das%20Fobizz-Tool%20zur%0A%20%20automatischen%20Bewertung%20von%20Hausaufgaben&entry.906535625=Rainer%20Muehlhoff%20and%20Marte%20Henningsen&entry.1292438233=%20%20%5BStudy%20in%20German%20language.%5D%20This%20study%20examines%20the%20AI-powered%20grading%20tool%0A%22AI%20Grading%20Assistant%22%20by%20the%20German%20company%20Fobizz%2C%20designed%20to%20support%0Ateachers%20in%20evaluating%20and%20providing%20feedback%20on%20student%20assignments.%20Against%0Athe%20societal%20backdrop%20of%20an%20overburdened%20education%20system%20and%20rising%0Aexpectations%20for%20artificial%20intelligence%20as%20a%20solution%20to%20these%20challenges%2C%20the%0Ainvestigation%20evaluates%20the%20tool%27s%20functional%20suitability%20through%20two%20test%0Aseries.%20The%20results%20reveal%20significant%20shortcomings%3A%20The%20tool%27s%20numerical%0Agrades%20and%20qualitative%20feedback%20are%20often%20random%20and%20do%20not%20improve%20even%20when%0Aits%20suggestions%20are%20incorporated.%20The%20highest%20ratings%20are%20achievable%20only%20with%0Atexts%20generated%20by%20ChatGPT.%20False%20claims%20and%20nonsensical%20submissions%20frequently%0Ago%20undetected%2C%20while%20the%20implementation%20of%20some%20grading%20criteria%20is%20unreliable%0Aand%20opaque.%20Since%20these%20deficiencies%20stem%20from%20the%20inherent%20limitations%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20fundamental%20improvements%20to%20this%20or%20similar%20tools%0Aare%20not%20immediately%20foreseeable.%20The%20study%20critiques%20the%20broader%20trend%20of%0Aadopting%20AI%20as%20a%20quick%20fix%20for%20systemic%20problems%20in%20education%2C%20concluding%20that%0AFobizz%27s%20marketing%20of%20the%20tool%20as%20an%20objective%20and%20time-saving%20solution%20is%0Amisleading%20and%20irresponsible.%20Finally%2C%20the%20study%20calls%20for%20systematic%0Aevaluation%20and%20subject-specific%20pedagogical%20scrutiny%20of%20the%20use%20of%20AI%20tools%20in%0Aeducational%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06651v4&entry.124074799=Read"},
{"title": "On the Readiness of Scientific Data for a Fair and Transparent Use in\n  Machine Learning", "author": "Joan Giner-Miguelez and Abel G\u00f3mez and Jordi Cabot", "abstract": "  To ensure the fairness and trustworthiness of machine learning (ML) systems,\nrecent legislative initiatives and relevant research in the ML community have\npointed out the need to document the data used to train ML models. Besides,\ndata-sharing practices in many scientific domains have evolved in recent years\nfor reproducibility purposes. In this sense, academic institutions' adoption of\nthese practices has encouraged researchers to publish their data and technical\ndocumentation in peer-reviewed publications such as data papers. In this study,\nwe analyze how this broader scientific data documentation meets the needs of\nthe ML community and regulatory bodies for its use in ML technologies. We\nexamine a sample of 4041 data papers of different domains, assessing their\ncompleteness, coverage of the requested dimensions, and trends in recent years.\nWe focus on the most and least documented dimensions and compare the results\nwith those of an ML-focused venue (NeurIPS D&B track) publishing papers\ndescribing datasets. As a result, we propose a set of recommendation guidelines\nfor data creators and scientific data publishers to increase their data's\npreparedness for its transparent and fairer use in ML technologies.\n", "link": "http://arxiv.org/abs/2401.10304v2", "date": "2024-12-17", "relevancy": 1.285, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4736}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Readiness%20of%20Scientific%20Data%20for%20a%20Fair%20and%20Transparent%20Use%20in%0A%20%20Machine%20Learning&body=Title%3A%20On%20the%20Readiness%20of%20Scientific%20Data%20for%20a%20Fair%20and%20Transparent%20Use%20in%0A%20%20Machine%20Learning%0AAuthor%3A%20Joan%20Giner-Miguelez%20and%20Abel%20G%C3%B3mez%20and%20Jordi%20Cabot%0AAbstract%3A%20%20%20To%20ensure%20the%20fairness%20and%20trustworthiness%20of%20machine%20learning%20%28ML%29%20systems%2C%0Arecent%20legislative%20initiatives%20and%20relevant%20research%20in%20the%20ML%20community%20have%0Apointed%20out%20the%20need%20to%20document%20the%20data%20used%20to%20train%20ML%20models.%20Besides%2C%0Adata-sharing%20practices%20in%20many%20scientific%20domains%20have%20evolved%20in%20recent%20years%0Afor%20reproducibility%20purposes.%20In%20this%20sense%2C%20academic%20institutions%27%20adoption%20of%0Athese%20practices%20has%20encouraged%20researchers%20to%20publish%20their%20data%20and%20technical%0Adocumentation%20in%20peer-reviewed%20publications%20such%20as%20data%20papers.%20In%20this%20study%2C%0Awe%20analyze%20how%20this%20broader%20scientific%20data%20documentation%20meets%20the%20needs%20of%0Athe%20ML%20community%20and%20regulatory%20bodies%20for%20its%20use%20in%20ML%20technologies.%20We%0Aexamine%20a%20sample%20of%204041%20data%20papers%20of%20different%20domains%2C%20assessing%20their%0Acompleteness%2C%20coverage%20of%20the%20requested%20dimensions%2C%20and%20trends%20in%20recent%20years.%0AWe%20focus%20on%20the%20most%20and%20least%20documented%20dimensions%20and%20compare%20the%20results%0Awith%20those%20of%20an%20ML-focused%20venue%20%28NeurIPS%20D%26B%20track%29%20publishing%20papers%0Adescribing%20datasets.%20As%20a%20result%2C%20we%20propose%20a%20set%20of%20recommendation%20guidelines%0Afor%20data%20creators%20and%20scientific%20data%20publishers%20to%20increase%20their%20data%27s%0Apreparedness%20for%20its%20transparent%20and%20fairer%20use%20in%20ML%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Readiness%2520of%2520Scientific%2520Data%2520for%2520a%2520Fair%2520and%2520Transparent%2520Use%2520in%250A%2520%2520Machine%2520Learning%26entry.906535625%3DJoan%2520Giner-Miguelez%2520and%2520Abel%2520G%25C3%25B3mez%2520and%2520Jordi%2520Cabot%26entry.1292438233%3D%2520%2520To%2520ensure%2520the%2520fairness%2520and%2520trustworthiness%2520of%2520machine%2520learning%2520%2528ML%2529%2520systems%252C%250Arecent%2520legislative%2520initiatives%2520and%2520relevant%2520research%2520in%2520the%2520ML%2520community%2520have%250Apointed%2520out%2520the%2520need%2520to%2520document%2520the%2520data%2520used%2520to%2520train%2520ML%2520models.%2520Besides%252C%250Adata-sharing%2520practices%2520in%2520many%2520scientific%2520domains%2520have%2520evolved%2520in%2520recent%2520years%250Afor%2520reproducibility%2520purposes.%2520In%2520this%2520sense%252C%2520academic%2520institutions%2527%2520adoption%2520of%250Athese%2520practices%2520has%2520encouraged%2520researchers%2520to%2520publish%2520their%2520data%2520and%2520technical%250Adocumentation%2520in%2520peer-reviewed%2520publications%2520such%2520as%2520data%2520papers.%2520In%2520this%2520study%252C%250Awe%2520analyze%2520how%2520this%2520broader%2520scientific%2520data%2520documentation%2520meets%2520the%2520needs%2520of%250Athe%2520ML%2520community%2520and%2520regulatory%2520bodies%2520for%2520its%2520use%2520in%2520ML%2520technologies.%2520We%250Aexamine%2520a%2520sample%2520of%25204041%2520data%2520papers%2520of%2520different%2520domains%252C%2520assessing%2520their%250Acompleteness%252C%2520coverage%2520of%2520the%2520requested%2520dimensions%252C%2520and%2520trends%2520in%2520recent%2520years.%250AWe%2520focus%2520on%2520the%2520most%2520and%2520least%2520documented%2520dimensions%2520and%2520compare%2520the%2520results%250Awith%2520those%2520of%2520an%2520ML-focused%2520venue%2520%2528NeurIPS%2520D%2526B%2520track%2529%2520publishing%2520papers%250Adescribing%2520datasets.%2520As%2520a%2520result%252C%2520we%2520propose%2520a%2520set%2520of%2520recommendation%2520guidelines%250Afor%2520data%2520creators%2520and%2520scientific%2520data%2520publishers%2520to%2520increase%2520their%2520data%2527s%250Apreparedness%2520for%2520its%2520transparent%2520and%2520fairer%2520use%2520in%2520ML%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Readiness%20of%20Scientific%20Data%20for%20a%20Fair%20and%20Transparent%20Use%20in%0A%20%20Machine%20Learning&entry.906535625=Joan%20Giner-Miguelez%20and%20Abel%20G%C3%B3mez%20and%20Jordi%20Cabot&entry.1292438233=%20%20To%20ensure%20the%20fairness%20and%20trustworthiness%20of%20machine%20learning%20%28ML%29%20systems%2C%0Arecent%20legislative%20initiatives%20and%20relevant%20research%20in%20the%20ML%20community%20have%0Apointed%20out%20the%20need%20to%20document%20the%20data%20used%20to%20train%20ML%20models.%20Besides%2C%0Adata-sharing%20practices%20in%20many%20scientific%20domains%20have%20evolved%20in%20recent%20years%0Afor%20reproducibility%20purposes.%20In%20this%20sense%2C%20academic%20institutions%27%20adoption%20of%0Athese%20practices%20has%20encouraged%20researchers%20to%20publish%20their%20data%20and%20technical%0Adocumentation%20in%20peer-reviewed%20publications%20such%20as%20data%20papers.%20In%20this%20study%2C%0Awe%20analyze%20how%20this%20broader%20scientific%20data%20documentation%20meets%20the%20needs%20of%0Athe%20ML%20community%20and%20regulatory%20bodies%20for%20its%20use%20in%20ML%20technologies.%20We%0Aexamine%20a%20sample%20of%204041%20data%20papers%20of%20different%20domains%2C%20assessing%20their%0Acompleteness%2C%20coverage%20of%20the%20requested%20dimensions%2C%20and%20trends%20in%20recent%20years.%0AWe%20focus%20on%20the%20most%20and%20least%20documented%20dimensions%20and%20compare%20the%20results%0Awith%20those%20of%20an%20ML-focused%20venue%20%28NeurIPS%20D%26B%20track%29%20publishing%20papers%0Adescribing%20datasets.%20As%20a%20result%2C%20we%20propose%20a%20set%20of%20recommendation%20guidelines%0Afor%20data%20creators%20and%20scientific%20data%20publishers%20to%20increase%20their%20data%27s%0Apreparedness%20for%20its%20transparent%20and%20fairer%20use%20in%20ML%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10304v2&entry.124074799=Read"},
{"title": "Relational Neurosymbolic Markov Models", "author": "Lennert De Smet and Gabriele Venturato and Luc De Raedt and Giuseppe Marra", "abstract": "  Sequential problems are ubiquitous in AI, such as in reinforcement learning\nor natural language processing. State-of-the-art deep sequential models, like\ntransformers, excel in these settings but fail to guarantee the satisfaction of\nconstraints necessary for trustworthy deployment. In contrast, neurosymbolic AI\n(NeSy) provides a sound formalism to enforce constraints in deep probabilistic\nmodels but scales exponentially on sequential problems. To overcome these\nlimitations, we introduce relational neurosymbolic Markov models (NeSy-MMs), a\nnew class of end-to-end differentiable sequential models that integrate and\nprovably satisfy relational logical constraints. We propose a strategy for\ninference and learning that scales on sequential settings, and that combines\napproximate Bayesian inference, automated reasoning, and gradient estimation.\nOur experiments show that NeSy-MMs can solve problems beyond the current\nstate-of-the-art in neurosymbolic AI and still provide strong guarantees with\nrespect to desired properties. Moreover, we show that our models are more\ninterpretable and that constraints can be adapted at test time to\nout-of-distribution scenarios.\n", "link": "http://arxiv.org/abs/2412.13023v1", "date": "2024-12-17", "relevancy": 0.9968, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5446}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4786}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relational%20Neurosymbolic%20Markov%20Models&body=Title%3A%20Relational%20Neurosymbolic%20Markov%20Models%0AAuthor%3A%20Lennert%20De%20Smet%20and%20Gabriele%20Venturato%20and%20Luc%20De%20Raedt%20and%20Giuseppe%20Marra%0AAbstract%3A%20%20%20Sequential%20problems%20are%20ubiquitous%20in%20AI%2C%20such%20as%20in%20reinforcement%20learning%0Aor%20natural%20language%20processing.%20State-of-the-art%20deep%20sequential%20models%2C%20like%0Atransformers%2C%20excel%20in%20these%20settings%20but%20fail%20to%20guarantee%20the%20satisfaction%20of%0Aconstraints%20necessary%20for%20trustworthy%20deployment.%20In%20contrast%2C%20neurosymbolic%20AI%0A%28NeSy%29%20provides%20a%20sound%20formalism%20to%20enforce%20constraints%20in%20deep%20probabilistic%0Amodels%20but%20scales%20exponentially%20on%20sequential%20problems.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20relational%20neurosymbolic%20Markov%20models%20%28NeSy-MMs%29%2C%20a%0Anew%20class%20of%20end-to-end%20differentiable%20sequential%20models%20that%20integrate%20and%0Aprovably%20satisfy%20relational%20logical%20constraints.%20We%20propose%20a%20strategy%20for%0Ainference%20and%20learning%20that%20scales%20on%20sequential%20settings%2C%20and%20that%20combines%0Aapproximate%20Bayesian%20inference%2C%20automated%20reasoning%2C%20and%20gradient%20estimation.%0AOur%20experiments%20show%20that%20NeSy-MMs%20can%20solve%20problems%20beyond%20the%20current%0Astate-of-the-art%20in%20neurosymbolic%20AI%20and%20still%20provide%20strong%20guarantees%20with%0Arespect%20to%20desired%20properties.%20Moreover%2C%20we%20show%20that%20our%20models%20are%20more%0Ainterpretable%20and%20that%20constraints%20can%20be%20adapted%20at%20test%20time%20to%0Aout-of-distribution%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelational%2520Neurosymbolic%2520Markov%2520Models%26entry.906535625%3DLennert%2520De%2520Smet%2520and%2520Gabriele%2520Venturato%2520and%2520Luc%2520De%2520Raedt%2520and%2520Giuseppe%2520Marra%26entry.1292438233%3D%2520%2520Sequential%2520problems%2520are%2520ubiquitous%2520in%2520AI%252C%2520such%2520as%2520in%2520reinforcement%2520learning%250Aor%2520natural%2520language%2520processing.%2520State-of-the-art%2520deep%2520sequential%2520models%252C%2520like%250Atransformers%252C%2520excel%2520in%2520these%2520settings%2520but%2520fail%2520to%2520guarantee%2520the%2520satisfaction%2520of%250Aconstraints%2520necessary%2520for%2520trustworthy%2520deployment.%2520In%2520contrast%252C%2520neurosymbolic%2520AI%250A%2528NeSy%2529%2520provides%2520a%2520sound%2520formalism%2520to%2520enforce%2520constraints%2520in%2520deep%2520probabilistic%250Amodels%2520but%2520scales%2520exponentially%2520on%2520sequential%2520problems.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520introduce%2520relational%2520neurosymbolic%2520Markov%2520models%2520%2528NeSy-MMs%2529%252C%2520a%250Anew%2520class%2520of%2520end-to-end%2520differentiable%2520sequential%2520models%2520that%2520integrate%2520and%250Aprovably%2520satisfy%2520relational%2520logical%2520constraints.%2520We%2520propose%2520a%2520strategy%2520for%250Ainference%2520and%2520learning%2520that%2520scales%2520on%2520sequential%2520settings%252C%2520and%2520that%2520combines%250Aapproximate%2520Bayesian%2520inference%252C%2520automated%2520reasoning%252C%2520and%2520gradient%2520estimation.%250AOur%2520experiments%2520show%2520that%2520NeSy-MMs%2520can%2520solve%2520problems%2520beyond%2520the%2520current%250Astate-of-the-art%2520in%2520neurosymbolic%2520AI%2520and%2520still%2520provide%2520strong%2520guarantees%2520with%250Arespect%2520to%2520desired%2520properties.%2520Moreover%252C%2520we%2520show%2520that%2520our%2520models%2520are%2520more%250Ainterpretable%2520and%2520that%2520constraints%2520can%2520be%2520adapted%2520at%2520test%2520time%2520to%250Aout-of-distribution%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relational%20Neurosymbolic%20Markov%20Models&entry.906535625=Lennert%20De%20Smet%20and%20Gabriele%20Venturato%20and%20Luc%20De%20Raedt%20and%20Giuseppe%20Marra&entry.1292438233=%20%20Sequential%20problems%20are%20ubiquitous%20in%20AI%2C%20such%20as%20in%20reinforcement%20learning%0Aor%20natural%20language%20processing.%20State-of-the-art%20deep%20sequential%20models%2C%20like%0Atransformers%2C%20excel%20in%20these%20settings%20but%20fail%20to%20guarantee%20the%20satisfaction%20of%0Aconstraints%20necessary%20for%20trustworthy%20deployment.%20In%20contrast%2C%20neurosymbolic%20AI%0A%28NeSy%29%20provides%20a%20sound%20formalism%20to%20enforce%20constraints%20in%20deep%20probabilistic%0Amodels%20but%20scales%20exponentially%20on%20sequential%20problems.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20relational%20neurosymbolic%20Markov%20models%20%28NeSy-MMs%29%2C%20a%0Anew%20class%20of%20end-to-end%20differentiable%20sequential%20models%20that%20integrate%20and%0Aprovably%20satisfy%20relational%20logical%20constraints.%20We%20propose%20a%20strategy%20for%0Ainference%20and%20learning%20that%20scales%20on%20sequential%20settings%2C%20and%20that%20combines%0Aapproximate%20Bayesian%20inference%2C%20automated%20reasoning%2C%20and%20gradient%20estimation.%0AOur%20experiments%20show%20that%20NeSy-MMs%20can%20solve%20problems%20beyond%20the%20current%0Astate-of-the-art%20in%20neurosymbolic%20AI%20and%20still%20provide%20strong%20guarantees%20with%0Arespect%20to%20desired%20properties.%20Moreover%2C%20we%20show%20that%20our%20models%20are%20more%0Ainterpretable%20and%20that%20constraints%20can%20be%20adapted%20at%20test%20time%20to%0Aout-of-distribution%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13023v1&entry.124074799=Read"},
{"title": "On Model Extrapolation in Marginal Shapley Values", "author": "Ilya Rozenfeld", "abstract": "  As the use of complex machine learning models continues to grow, so does the\nneed for reliable explainability methods. One of the most popular methods for\nmodel explainability is based on Shapley values. There are two most commonly\nused approaches to calculating Shapley values which produce different results\nwhen features are correlated, conditional and marginal. In our previous work,\nit was demonstrated that the conditional approach is fundamentally flawed due\nto implicit assumptions of causality. However, it is a well-known fact that\nmarginal approach to calculating Shapley values leads to model extrapolation\nwhere it might not be well defined. In this paper we explore the impacts of\nmodel extrapolation on Shapley values in the case of a simple linear spline\nmodel. Furthermore, we propose an approach which while using marginal averaging\navoids model extrapolation and with addition of causal information replicates\ncausal Shapley values. Finally, we demonstrate our method on the real data\nexample.\n", "link": "http://arxiv.org/abs/2412.13158v1", "date": "2024-12-17", "relevancy": 0.844, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4477}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4092}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Model%20Extrapolation%20in%20Marginal%20Shapley%20Values&body=Title%3A%20On%20Model%20Extrapolation%20in%20Marginal%20Shapley%20Values%0AAuthor%3A%20Ilya%20Rozenfeld%0AAbstract%3A%20%20%20As%20the%20use%20of%20complex%20machine%20learning%20models%20continues%20to%20grow%2C%20so%20does%20the%0Aneed%20for%20reliable%20explainability%20methods.%20One%20of%20the%20most%20popular%20methods%20for%0Amodel%20explainability%20is%20based%20on%20Shapley%20values.%20There%20are%20two%20most%20commonly%0Aused%20approaches%20to%20calculating%20Shapley%20values%20which%20produce%20different%20results%0Awhen%20features%20are%20correlated%2C%20conditional%20and%20marginal.%20In%20our%20previous%20work%2C%0Ait%20was%20demonstrated%20that%20the%20conditional%20approach%20is%20fundamentally%20flawed%20due%0Ato%20implicit%20assumptions%20of%20causality.%20However%2C%20it%20is%20a%20well-known%20fact%20that%0Amarginal%20approach%20to%20calculating%20Shapley%20values%20leads%20to%20model%20extrapolation%0Awhere%20it%20might%20not%20be%20well%20defined.%20In%20this%20paper%20we%20explore%20the%20impacts%20of%0Amodel%20extrapolation%20on%20Shapley%20values%20in%20the%20case%20of%20a%20simple%20linear%20spline%0Amodel.%20Furthermore%2C%20we%20propose%20an%20approach%20which%20while%20using%20marginal%20averaging%0Aavoids%20model%20extrapolation%20and%20with%20addition%20of%20causal%20information%20replicates%0Acausal%20Shapley%20values.%20Finally%2C%20we%20demonstrate%20our%20method%20on%20the%20real%20data%0Aexample.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Model%2520Extrapolation%2520in%2520Marginal%2520Shapley%2520Values%26entry.906535625%3DIlya%2520Rozenfeld%26entry.1292438233%3D%2520%2520As%2520the%2520use%2520of%2520complex%2520machine%2520learning%2520models%2520continues%2520to%2520grow%252C%2520so%2520does%2520the%250Aneed%2520for%2520reliable%2520explainability%2520methods.%2520One%2520of%2520the%2520most%2520popular%2520methods%2520for%250Amodel%2520explainability%2520is%2520based%2520on%2520Shapley%2520values.%2520There%2520are%2520two%2520most%2520commonly%250Aused%2520approaches%2520to%2520calculating%2520Shapley%2520values%2520which%2520produce%2520different%2520results%250Awhen%2520features%2520are%2520correlated%252C%2520conditional%2520and%2520marginal.%2520In%2520our%2520previous%2520work%252C%250Ait%2520was%2520demonstrated%2520that%2520the%2520conditional%2520approach%2520is%2520fundamentally%2520flawed%2520due%250Ato%2520implicit%2520assumptions%2520of%2520causality.%2520However%252C%2520it%2520is%2520a%2520well-known%2520fact%2520that%250Amarginal%2520approach%2520to%2520calculating%2520Shapley%2520values%2520leads%2520to%2520model%2520extrapolation%250Awhere%2520it%2520might%2520not%2520be%2520well%2520defined.%2520In%2520this%2520paper%2520we%2520explore%2520the%2520impacts%2520of%250Amodel%2520extrapolation%2520on%2520Shapley%2520values%2520in%2520the%2520case%2520of%2520a%2520simple%2520linear%2520spline%250Amodel.%2520Furthermore%252C%2520we%2520propose%2520an%2520approach%2520which%2520while%2520using%2520marginal%2520averaging%250Aavoids%2520model%2520extrapolation%2520and%2520with%2520addition%2520of%2520causal%2520information%2520replicates%250Acausal%2520Shapley%2520values.%2520Finally%252C%2520we%2520demonstrate%2520our%2520method%2520on%2520the%2520real%2520data%250Aexample.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Model%20Extrapolation%20in%20Marginal%20Shapley%20Values&entry.906535625=Ilya%20Rozenfeld&entry.1292438233=%20%20As%20the%20use%20of%20complex%20machine%20learning%20models%20continues%20to%20grow%2C%20so%20does%20the%0Aneed%20for%20reliable%20explainability%20methods.%20One%20of%20the%20most%20popular%20methods%20for%0Amodel%20explainability%20is%20based%20on%20Shapley%20values.%20There%20are%20two%20most%20commonly%0Aused%20approaches%20to%20calculating%20Shapley%20values%20which%20produce%20different%20results%0Awhen%20features%20are%20correlated%2C%20conditional%20and%20marginal.%20In%20our%20previous%20work%2C%0Ait%20was%20demonstrated%20that%20the%20conditional%20approach%20is%20fundamentally%20flawed%20due%0Ato%20implicit%20assumptions%20of%20causality.%20However%2C%20it%20is%20a%20well-known%20fact%20that%0Amarginal%20approach%20to%20calculating%20Shapley%20values%20leads%20to%20model%20extrapolation%0Awhere%20it%20might%20not%20be%20well%20defined.%20In%20this%20paper%20we%20explore%20the%20impacts%20of%0Amodel%20extrapolation%20on%20Shapley%20values%20in%20the%20case%20of%20a%20simple%20linear%20spline%0Amodel.%20Furthermore%2C%20we%20propose%20an%20approach%20which%20while%20using%20marginal%20averaging%0Aavoids%20model%20extrapolation%20and%20with%20addition%20of%20causal%20information%20replicates%0Acausal%20Shapley%20values.%20Finally%2C%20we%20demonstrate%20our%20method%20on%20the%20real%20data%0Aexample.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13158v1&entry.124074799=Read"},
{"title": "Rethinking the Alignment of Psychotherapy Dialogue Generation with\n  Motivational Interviewing Strategies", "author": "Xin Sun and Xiao Tang and Abdallah El Ali and Zhuying Li and Pengjie Ren and Jan de Wit and Jiahuan Pei and Jos A. Bosch", "abstract": "  Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, particularly in the context of\nmotivational interviewing (MI). However, the inherent lack of transparency in\nLLM outputs presents significant challenges given the sensitive nature of\npsychotherapy. Applying MI strategies, a set of MI skills, to generate more\ncontrollable therapeutic-adherent conversations with explainability provides a\npossible solution. In this work, we explore the alignment of LLMs with MI\nstrategies by first prompting the LLMs to predict the appropriate strategies as\nreasoning and then utilizing these strategies to guide the subsequent dialogue\ngeneration. We seek to investigate whether such alignment leads to more\ncontrollable and explainable generations. Multiple experiments including\nautomatic and human evaluations are conducted to validate the effectiveness of\nMI strategies in aligning psychotherapy dialogue generation. Our findings\ndemonstrate the potential of LLMs in producing strategically aligned dialogues\nand suggest directions for practical applications in psychotherapeutic\nsettings.\n", "link": "http://arxiv.org/abs/2408.06527v2", "date": "2024-12-17", "relevancy": 1.3003, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4377}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4294}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20Alignment%20of%20Psychotherapy%20Dialogue%20Generation%20with%0A%20%20Motivational%20Interviewing%20Strategies&body=Title%3A%20Rethinking%20the%20Alignment%20of%20Psychotherapy%20Dialogue%20Generation%20with%0A%20%20Motivational%20Interviewing%20Strategies%0AAuthor%3A%20Xin%20Sun%20and%20Xiao%20Tang%20and%20Abdallah%20El%20Ali%20and%20Zhuying%20Li%20and%20Pengjie%20Ren%20and%20Jan%20de%20Wit%20and%20Jiahuan%20Pei%20and%20Jos%20A.%20Bosch%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20in%0Agenerating%20psychotherapeutic%20dialogues%2C%20particularly%20in%20the%20context%20of%0Amotivational%20interviewing%20%28MI%29.%20However%2C%20the%20inherent%20lack%20of%20transparency%20in%0ALLM%20outputs%20presents%20significant%20challenges%20given%20the%20sensitive%20nature%20of%0Apsychotherapy.%20Applying%20MI%20strategies%2C%20a%20set%20of%20MI%20skills%2C%20to%20generate%20more%0Acontrollable%20therapeutic-adherent%20conversations%20with%20explainability%20provides%20a%0Apossible%20solution.%20In%20this%20work%2C%20we%20explore%20the%20alignment%20of%20LLMs%20with%20MI%0Astrategies%20by%20first%20prompting%20the%20LLMs%20to%20predict%20the%20appropriate%20strategies%20as%0Areasoning%20and%20then%20utilizing%20these%20strategies%20to%20guide%20the%20subsequent%20dialogue%0Ageneration.%20We%20seek%20to%20investigate%20whether%20such%20alignment%20leads%20to%20more%0Acontrollable%20and%20explainable%20generations.%20Multiple%20experiments%20including%0Aautomatic%20and%20human%20evaluations%20are%20conducted%20to%20validate%20the%20effectiveness%20of%0AMI%20strategies%20in%20aligning%20psychotherapy%20dialogue%20generation.%20Our%20findings%0Ademonstrate%20the%20potential%20of%20LLMs%20in%20producing%20strategically%20aligned%20dialogues%0Aand%20suggest%20directions%20for%20practical%20applications%20in%20psychotherapeutic%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520Alignment%2520of%2520Psychotherapy%2520Dialogue%2520Generation%2520with%250A%2520%2520Motivational%2520Interviewing%2520Strategies%26entry.906535625%3DXin%2520Sun%2520and%2520Xiao%2520Tang%2520and%2520Abdallah%2520El%2520Ali%2520and%2520Zhuying%2520Li%2520and%2520Pengjie%2520Ren%2520and%2520Jan%2520de%2520Wit%2520and%2520Jiahuan%2520Pei%2520and%2520Jos%2520A.%2520Bosch%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520promise%2520in%250Agenerating%2520psychotherapeutic%2520dialogues%252C%2520particularly%2520in%2520the%2520context%2520of%250Amotivational%2520interviewing%2520%2528MI%2529.%2520However%252C%2520the%2520inherent%2520lack%2520of%2520transparency%2520in%250ALLM%2520outputs%2520presents%2520significant%2520challenges%2520given%2520the%2520sensitive%2520nature%2520of%250Apsychotherapy.%2520Applying%2520MI%2520strategies%252C%2520a%2520set%2520of%2520MI%2520skills%252C%2520to%2520generate%2520more%250Acontrollable%2520therapeutic-adherent%2520conversations%2520with%2520explainability%2520provides%2520a%250Apossible%2520solution.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520alignment%2520of%2520LLMs%2520with%2520MI%250Astrategies%2520by%2520first%2520prompting%2520the%2520LLMs%2520to%2520predict%2520the%2520appropriate%2520strategies%2520as%250Areasoning%2520and%2520then%2520utilizing%2520these%2520strategies%2520to%2520guide%2520the%2520subsequent%2520dialogue%250Ageneration.%2520We%2520seek%2520to%2520investigate%2520whether%2520such%2520alignment%2520leads%2520to%2520more%250Acontrollable%2520and%2520explainable%2520generations.%2520Multiple%2520experiments%2520including%250Aautomatic%2520and%2520human%2520evaluations%2520are%2520conducted%2520to%2520validate%2520the%2520effectiveness%2520of%250AMI%2520strategies%2520in%2520aligning%2520psychotherapy%2520dialogue%2520generation.%2520Our%2520findings%250Ademonstrate%2520the%2520potential%2520of%2520LLMs%2520in%2520producing%2520strategically%2520aligned%2520dialogues%250Aand%2520suggest%2520directions%2520for%2520practical%2520applications%2520in%2520psychotherapeutic%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20Alignment%20of%20Psychotherapy%20Dialogue%20Generation%20with%0A%20%20Motivational%20Interviewing%20Strategies&entry.906535625=Xin%20Sun%20and%20Xiao%20Tang%20and%20Abdallah%20El%20Ali%20and%20Zhuying%20Li%20and%20Pengjie%20Ren%20and%20Jan%20de%20Wit%20and%20Jiahuan%20Pei%20and%20Jos%20A.%20Bosch&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20in%0Agenerating%20psychotherapeutic%20dialogues%2C%20particularly%20in%20the%20context%20of%0Amotivational%20interviewing%20%28MI%29.%20However%2C%20the%20inherent%20lack%20of%20transparency%20in%0ALLM%20outputs%20presents%20significant%20challenges%20given%20the%20sensitive%20nature%20of%0Apsychotherapy.%20Applying%20MI%20strategies%2C%20a%20set%20of%20MI%20skills%2C%20to%20generate%20more%0Acontrollable%20therapeutic-adherent%20conversations%20with%20explainability%20provides%20a%0Apossible%20solution.%20In%20this%20work%2C%20we%20explore%20the%20alignment%20of%20LLMs%20with%20MI%0Astrategies%20by%20first%20prompting%20the%20LLMs%20to%20predict%20the%20appropriate%20strategies%20as%0Areasoning%20and%20then%20utilizing%20these%20strategies%20to%20guide%20the%20subsequent%20dialogue%0Ageneration.%20We%20seek%20to%20investigate%20whether%20such%20alignment%20leads%20to%20more%0Acontrollable%20and%20explainable%20generations.%20Multiple%20experiments%20including%0Aautomatic%20and%20human%20evaluations%20are%20conducted%20to%20validate%20the%20effectiveness%20of%0AMI%20strategies%20in%20aligning%20psychotherapy%20dialogue%20generation.%20Our%20findings%0Ademonstrate%20the%20potential%20of%20LLMs%20in%20producing%20strategically%20aligned%20dialogues%0Aand%20suggest%20directions%20for%20practical%20applications%20in%20psychotherapeutic%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06527v2&entry.124074799=Read"},
{"title": "Agnosticism About Artificial Consciousness", "author": "Tom McClelland", "abstract": "  Could an AI have conscious experiences? Any answer to this question should\nconform to Evidentialism - that is, it should be based not on intuition, dogma\nor speculation but on solid scientific evidence. I argue that such evidence is\nhard to come by and that the only justifiable stance on the prospects of\nartificial consciousness is agnosticism. In the current debate, the main\ndivision is between biological views that are sceptical of artificial\nconsciousness and functional views that are sympathetic to it. I argue that\nboth camps make the same mistake of over-estimating what the evidence tells us.\nScientific insights into consciousness have been achieved through the study of\nconscious organisms. Although this has enabled cautious assessments of\nconsciousness in various creatures, extending this to AI faces serious\nobstacles. AI thus presents consciousness researchers with a dilemma: either\nreach a verdict on artificial consciousness but violate Evidentialism; or\nrespect Evidentialism but offer no verdict on the prospects of artificial\nconsciousness. The dominant trend in the literature has been to take the first\noption while purporting to follow the scientific evidence. I argue that if we\ntruly follow the evidence, we must take the second option and adopt\nagnosticism.\n", "link": "http://arxiv.org/abs/2412.13145v1", "date": "2024-12-17", "relevancy": 1.245, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.432}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4007}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agnosticism%20About%20Artificial%20Consciousness&body=Title%3A%20Agnosticism%20About%20Artificial%20Consciousness%0AAuthor%3A%20Tom%20McClelland%0AAbstract%3A%20%20%20Could%20an%20AI%20have%20conscious%20experiences%3F%20Any%20answer%20to%20this%20question%20should%0Aconform%20to%20Evidentialism%20-%20that%20is%2C%20it%20should%20be%20based%20not%20on%20intuition%2C%20dogma%0Aor%20speculation%20but%20on%20solid%20scientific%20evidence.%20I%20argue%20that%20such%20evidence%20is%0Ahard%20to%20come%20by%20and%20that%20the%20only%20justifiable%20stance%20on%20the%20prospects%20of%0Aartificial%20consciousness%20is%20agnosticism.%20In%20the%20current%20debate%2C%20the%20main%0Adivision%20is%20between%20biological%20views%20that%20are%20sceptical%20of%20artificial%0Aconsciousness%20and%20functional%20views%20that%20are%20sympathetic%20to%20it.%20I%20argue%20that%0Aboth%20camps%20make%20the%20same%20mistake%20of%20over-estimating%20what%20the%20evidence%20tells%20us.%0AScientific%20insights%20into%20consciousness%20have%20been%20achieved%20through%20the%20study%20of%0Aconscious%20organisms.%20Although%20this%20has%20enabled%20cautious%20assessments%20of%0Aconsciousness%20in%20various%20creatures%2C%20extending%20this%20to%20AI%20faces%20serious%0Aobstacles.%20AI%20thus%20presents%20consciousness%20researchers%20with%20a%20dilemma%3A%20either%0Areach%20a%20verdict%20on%20artificial%20consciousness%20but%20violate%20Evidentialism%3B%20or%0Arespect%20Evidentialism%20but%20offer%20no%20verdict%20on%20the%20prospects%20of%20artificial%0Aconsciousness.%20The%20dominant%20trend%20in%20the%20literature%20has%20been%20to%20take%20the%20first%0Aoption%20while%20purporting%20to%20follow%20the%20scientific%20evidence.%20I%20argue%20that%20if%20we%0Atruly%20follow%20the%20evidence%2C%20we%20must%20take%20the%20second%20option%20and%20adopt%0Aagnosticism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgnosticism%2520About%2520Artificial%2520Consciousness%26entry.906535625%3DTom%2520McClelland%26entry.1292438233%3D%2520%2520Could%2520an%2520AI%2520have%2520conscious%2520experiences%253F%2520Any%2520answer%2520to%2520this%2520question%2520should%250Aconform%2520to%2520Evidentialism%2520-%2520that%2520is%252C%2520it%2520should%2520be%2520based%2520not%2520on%2520intuition%252C%2520dogma%250Aor%2520speculation%2520but%2520on%2520solid%2520scientific%2520evidence.%2520I%2520argue%2520that%2520such%2520evidence%2520is%250Ahard%2520to%2520come%2520by%2520and%2520that%2520the%2520only%2520justifiable%2520stance%2520on%2520the%2520prospects%2520of%250Aartificial%2520consciousness%2520is%2520agnosticism.%2520In%2520the%2520current%2520debate%252C%2520the%2520main%250Adivision%2520is%2520between%2520biological%2520views%2520that%2520are%2520sceptical%2520of%2520artificial%250Aconsciousness%2520and%2520functional%2520views%2520that%2520are%2520sympathetic%2520to%2520it.%2520I%2520argue%2520that%250Aboth%2520camps%2520make%2520the%2520same%2520mistake%2520of%2520over-estimating%2520what%2520the%2520evidence%2520tells%2520us.%250AScientific%2520insights%2520into%2520consciousness%2520have%2520been%2520achieved%2520through%2520the%2520study%2520of%250Aconscious%2520organisms.%2520Although%2520this%2520has%2520enabled%2520cautious%2520assessments%2520of%250Aconsciousness%2520in%2520various%2520creatures%252C%2520extending%2520this%2520to%2520AI%2520faces%2520serious%250Aobstacles.%2520AI%2520thus%2520presents%2520consciousness%2520researchers%2520with%2520a%2520dilemma%253A%2520either%250Areach%2520a%2520verdict%2520on%2520artificial%2520consciousness%2520but%2520violate%2520Evidentialism%253B%2520or%250Arespect%2520Evidentialism%2520but%2520offer%2520no%2520verdict%2520on%2520the%2520prospects%2520of%2520artificial%250Aconsciousness.%2520The%2520dominant%2520trend%2520in%2520the%2520literature%2520has%2520been%2520to%2520take%2520the%2520first%250Aoption%2520while%2520purporting%2520to%2520follow%2520the%2520scientific%2520evidence.%2520I%2520argue%2520that%2520if%2520we%250Atruly%2520follow%2520the%2520evidence%252C%2520we%2520must%2520take%2520the%2520second%2520option%2520and%2520adopt%250Aagnosticism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agnosticism%20About%20Artificial%20Consciousness&entry.906535625=Tom%20McClelland&entry.1292438233=%20%20Could%20an%20AI%20have%20conscious%20experiences%3F%20Any%20answer%20to%20this%20question%20should%0Aconform%20to%20Evidentialism%20-%20that%20is%2C%20it%20should%20be%20based%20not%20on%20intuition%2C%20dogma%0Aor%20speculation%20but%20on%20solid%20scientific%20evidence.%20I%20argue%20that%20such%20evidence%20is%0Ahard%20to%20come%20by%20and%20that%20the%20only%20justifiable%20stance%20on%20the%20prospects%20of%0Aartificial%20consciousness%20is%20agnosticism.%20In%20the%20current%20debate%2C%20the%20main%0Adivision%20is%20between%20biological%20views%20that%20are%20sceptical%20of%20artificial%0Aconsciousness%20and%20functional%20views%20that%20are%20sympathetic%20to%20it.%20I%20argue%20that%0Aboth%20camps%20make%20the%20same%20mistake%20of%20over-estimating%20what%20the%20evidence%20tells%20us.%0AScientific%20insights%20into%20consciousness%20have%20been%20achieved%20through%20the%20study%20of%0Aconscious%20organisms.%20Although%20this%20has%20enabled%20cautious%20assessments%20of%0Aconsciousness%20in%20various%20creatures%2C%20extending%20this%20to%20AI%20faces%20serious%0Aobstacles.%20AI%20thus%20presents%20consciousness%20researchers%20with%20a%20dilemma%3A%20either%0Areach%20a%20verdict%20on%20artificial%20consciousness%20but%20violate%20Evidentialism%3B%20or%0Arespect%20Evidentialism%20but%20offer%20no%20verdict%20on%20the%20prospects%20of%20artificial%0Aconsciousness.%20The%20dominant%20trend%20in%20the%20literature%20has%20been%20to%20take%20the%20first%0Aoption%20while%20purporting%20to%20follow%20the%20scientific%20evidence.%20I%20argue%20that%20if%20we%0Atruly%20follow%20the%20evidence%2C%20we%20must%20take%20the%20second%20option%20and%20adopt%0Aagnosticism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13145v1&entry.124074799=Read"},
{"title": "CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion\n  Models", "author": "Gaoyang Zhang and Bingtao Fu and Qingnan Fan and Qi Zhang and Runxing Liu and Hong Gu and Huaqi Zhang and Xinguo Liu", "abstract": "  Text-to-image diffusion models excel at generating photorealistic images, but\ncommonly struggle to render accurate spatial relationships described in text\nprompts. We identify two core issues underlying this common failure: 1) the\nambiguous nature of spatial-related data in existing datasets, and 2) the\ninability of current text encoders to accurately interpret the spatial\nsemantics of input descriptions. We address these issues with CoMPaSS, a\nversatile training framework that enhances spatial understanding of any T2I\ndiffusion model. CoMPaSS solves the ambiguity of spatial-related data with the\nSpatial Constraints-Oriented Pairing (SCOP) data engine, which curates\nspatially-accurate training data through a set of principled spatial\nconstraints. To better exploit the curated high-quality spatial priors, CoMPaSS\nfurther introduces a Token ENcoding ORdering (TENOR) module to allow better\nexploitation of high-quality spatial priors, effectively compensating for the\nshortcoming of text encoders. Extensive experiments on four popular open-weight\nT2I diffusion models covering both UNet- and MMDiT-based architectures\ndemonstrate the effectiveness of CoMPaSS by setting new state-of-the-arts with\nsubstantial relative gains across well-known benchmarks on spatial\nrelationships generation, including VISOR (+98%), T2I-CompBench Spatial (+67%),\nand GenEval Position (+131%). Code will be available at\nhttps://github.com/blurgyy/CoMPaSS.\n", "link": "http://arxiv.org/abs/2412.13195v1", "date": "2024-12-17", "relevancy": 1.2144, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6199}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.606}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoMPaSS%3A%20Enhancing%20Spatial%20Understanding%20in%20Text-to-Image%20Diffusion%0A%20%20Models&body=Title%3A%20CoMPaSS%3A%20Enhancing%20Spatial%20Understanding%20in%20Text-to-Image%20Diffusion%0A%20%20Models%0AAuthor%3A%20Gaoyang%20Zhang%20and%20Bingtao%20Fu%20and%20Qingnan%20Fan%20and%20Qi%20Zhang%20and%20Runxing%20Liu%20and%20Hong%20Gu%20and%20Huaqi%20Zhang%20and%20Xinguo%20Liu%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20excel%20at%20generating%20photorealistic%20images%2C%20but%0Acommonly%20struggle%20to%20render%20accurate%20spatial%20relationships%20described%20in%20text%0Aprompts.%20We%20identify%20two%20core%20issues%20underlying%20this%20common%20failure%3A%201%29%20the%0Aambiguous%20nature%20of%20spatial-related%20data%20in%20existing%20datasets%2C%20and%202%29%20the%0Ainability%20of%20current%20text%20encoders%20to%20accurately%20interpret%20the%20spatial%0Asemantics%20of%20input%20descriptions.%20We%20address%20these%20issues%20with%20CoMPaSS%2C%20a%0Aversatile%20training%20framework%20that%20enhances%20spatial%20understanding%20of%20any%20T2I%0Adiffusion%20model.%20CoMPaSS%20solves%20the%20ambiguity%20of%20spatial-related%20data%20with%20the%0ASpatial%20Constraints-Oriented%20Pairing%20%28SCOP%29%20data%20engine%2C%20which%20curates%0Aspatially-accurate%20training%20data%20through%20a%20set%20of%20principled%20spatial%0Aconstraints.%20To%20better%20exploit%20the%20curated%20high-quality%20spatial%20priors%2C%20CoMPaSS%0Afurther%20introduces%20a%20Token%20ENcoding%20ORdering%20%28TENOR%29%20module%20to%20allow%20better%0Aexploitation%20of%20high-quality%20spatial%20priors%2C%20effectively%20compensating%20for%20the%0Ashortcoming%20of%20text%20encoders.%20Extensive%20experiments%20on%20four%20popular%20open-weight%0AT2I%20diffusion%20models%20covering%20both%20UNet-%20and%20MMDiT-based%20architectures%0Ademonstrate%20the%20effectiveness%20of%20CoMPaSS%20by%20setting%20new%20state-of-the-arts%20with%0Asubstantial%20relative%20gains%20across%20well-known%20benchmarks%20on%20spatial%0Arelationships%20generation%2C%20including%20VISOR%20%28%2B98%25%29%2C%20T2I-CompBench%20Spatial%20%28%2B67%25%29%2C%0Aand%20GenEval%20Position%20%28%2B131%25%29.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/blurgyy/CoMPaSS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoMPaSS%253A%2520Enhancing%2520Spatial%2520Understanding%2520in%2520Text-to-Image%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DGaoyang%2520Zhang%2520and%2520Bingtao%2520Fu%2520and%2520Qingnan%2520Fan%2520and%2520Qi%2520Zhang%2520and%2520Runxing%2520Liu%2520and%2520Hong%2520Gu%2520and%2520Huaqi%2520Zhang%2520and%2520Xinguo%2520Liu%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520excel%2520at%2520generating%2520photorealistic%2520images%252C%2520but%250Acommonly%2520struggle%2520to%2520render%2520accurate%2520spatial%2520relationships%2520described%2520in%2520text%250Aprompts.%2520We%2520identify%2520two%2520core%2520issues%2520underlying%2520this%2520common%2520failure%253A%25201%2529%2520the%250Aambiguous%2520nature%2520of%2520spatial-related%2520data%2520in%2520existing%2520datasets%252C%2520and%25202%2529%2520the%250Ainability%2520of%2520current%2520text%2520encoders%2520to%2520accurately%2520interpret%2520the%2520spatial%250Asemantics%2520of%2520input%2520descriptions.%2520We%2520address%2520these%2520issues%2520with%2520CoMPaSS%252C%2520a%250Aversatile%2520training%2520framework%2520that%2520enhances%2520spatial%2520understanding%2520of%2520any%2520T2I%250Adiffusion%2520model.%2520CoMPaSS%2520solves%2520the%2520ambiguity%2520of%2520spatial-related%2520data%2520with%2520the%250ASpatial%2520Constraints-Oriented%2520Pairing%2520%2528SCOP%2529%2520data%2520engine%252C%2520which%2520curates%250Aspatially-accurate%2520training%2520data%2520through%2520a%2520set%2520of%2520principled%2520spatial%250Aconstraints.%2520To%2520better%2520exploit%2520the%2520curated%2520high-quality%2520spatial%2520priors%252C%2520CoMPaSS%250Afurther%2520introduces%2520a%2520Token%2520ENcoding%2520ORdering%2520%2528TENOR%2529%2520module%2520to%2520allow%2520better%250Aexploitation%2520of%2520high-quality%2520spatial%2520priors%252C%2520effectively%2520compensating%2520for%2520the%250Ashortcoming%2520of%2520text%2520encoders.%2520Extensive%2520experiments%2520on%2520four%2520popular%2520open-weight%250AT2I%2520diffusion%2520models%2520covering%2520both%2520UNet-%2520and%2520MMDiT-based%2520architectures%250Ademonstrate%2520the%2520effectiveness%2520of%2520CoMPaSS%2520by%2520setting%2520new%2520state-of-the-arts%2520with%250Asubstantial%2520relative%2520gains%2520across%2520well-known%2520benchmarks%2520on%2520spatial%250Arelationships%2520generation%252C%2520including%2520VISOR%2520%2528%252B98%2525%2529%252C%2520T2I-CompBench%2520Spatial%2520%2528%252B67%2525%2529%252C%250Aand%2520GenEval%2520Position%2520%2528%252B131%2525%2529.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/blurgyy/CoMPaSS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoMPaSS%3A%20Enhancing%20Spatial%20Understanding%20in%20Text-to-Image%20Diffusion%0A%20%20Models&entry.906535625=Gaoyang%20Zhang%20and%20Bingtao%20Fu%20and%20Qingnan%20Fan%20and%20Qi%20Zhang%20and%20Runxing%20Liu%20and%20Hong%20Gu%20and%20Huaqi%20Zhang%20and%20Xinguo%20Liu&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20excel%20at%20generating%20photorealistic%20images%2C%20but%0Acommonly%20struggle%20to%20render%20accurate%20spatial%20relationships%20described%20in%20text%0Aprompts.%20We%20identify%20two%20core%20issues%20underlying%20this%20common%20failure%3A%201%29%20the%0Aambiguous%20nature%20of%20spatial-related%20data%20in%20existing%20datasets%2C%20and%202%29%20the%0Ainability%20of%20current%20text%20encoders%20to%20accurately%20interpret%20the%20spatial%0Asemantics%20of%20input%20descriptions.%20We%20address%20these%20issues%20with%20CoMPaSS%2C%20a%0Aversatile%20training%20framework%20that%20enhances%20spatial%20understanding%20of%20any%20T2I%0Adiffusion%20model.%20CoMPaSS%20solves%20the%20ambiguity%20of%20spatial-related%20data%20with%20the%0ASpatial%20Constraints-Oriented%20Pairing%20%28SCOP%29%20data%20engine%2C%20which%20curates%0Aspatially-accurate%20training%20data%20through%20a%20set%20of%20principled%20spatial%0Aconstraints.%20To%20better%20exploit%20the%20curated%20high-quality%20spatial%20priors%2C%20CoMPaSS%0Afurther%20introduces%20a%20Token%20ENcoding%20ORdering%20%28TENOR%29%20module%20to%20allow%20better%0Aexploitation%20of%20high-quality%20spatial%20priors%2C%20effectively%20compensating%20for%20the%0Ashortcoming%20of%20text%20encoders.%20Extensive%20experiments%20on%20four%20popular%20open-weight%0AT2I%20diffusion%20models%20covering%20both%20UNet-%20and%20MMDiT-based%20architectures%0Ademonstrate%20the%20effectiveness%20of%20CoMPaSS%20by%20setting%20new%20state-of-the-arts%20with%0Asubstantial%20relative%20gains%20across%20well-known%20benchmarks%20on%20spatial%0Arelationships%20generation%2C%20including%20VISOR%20%28%2B98%25%29%2C%20T2I-CompBench%20Spatial%20%28%2B67%25%29%2C%0Aand%20GenEval%20Position%20%28%2B131%25%29.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/blurgyy/CoMPaSS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13195v1&entry.124074799=Read"},
{"title": "Prompt Augmentation for Self-supervised Text-guided Image Manipulation", "author": "Rumeysa Bodur and Binod Bhattarai and Tae-Kyun Kim", "abstract": "  Text-guided image editing finds applications in various creative and\npractical fields. While recent studies in image generation have advanced the\nfield, they often struggle with the dual challenges of coherent image\ntransformation and context preservation. In response, our work introduces\nprompt augmentation, a method amplifying a single input prompt into several\ntarget prompts, strengthening textual context and enabling localised image\nediting. Specifically, we use the augmented prompts to delineate the intended\nmanipulation area. We propose a Contrastive Loss tailored to driving effective\nimage editing by displacing edited areas and drawing preserved regions closer.\nAcknowledging the continuous nature of image manipulations, we further refine\nour approach by incorporating the similarity concept, creating a Soft\nContrastive Loss. The new losses are incorporated to the diffusion model,\ndemonstrating improved or competitive image editing results on public datasets\nand generated images over state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2412.13081v1", "date": "2024-12-17", "relevancy": 1.122, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5891}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5505}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Augmentation%20for%20Self-supervised%20Text-guided%20Image%20Manipulation&body=Title%3A%20Prompt%20Augmentation%20for%20Self-supervised%20Text-guided%20Image%20Manipulation%0AAuthor%3A%20Rumeysa%20Bodur%20and%20Binod%20Bhattarai%20and%20Tae-Kyun%20Kim%0AAbstract%3A%20%20%20Text-guided%20image%20editing%20finds%20applications%20in%20various%20creative%20and%0Apractical%20fields.%20While%20recent%20studies%20in%20image%20generation%20have%20advanced%20the%0Afield%2C%20they%20often%20struggle%20with%20the%20dual%20challenges%20of%20coherent%20image%0Atransformation%20and%20context%20preservation.%20In%20response%2C%20our%20work%20introduces%0Aprompt%20augmentation%2C%20a%20method%20amplifying%20a%20single%20input%20prompt%20into%20several%0Atarget%20prompts%2C%20strengthening%20textual%20context%20and%20enabling%20localised%20image%0Aediting.%20Specifically%2C%20we%20use%20the%20augmented%20prompts%20to%20delineate%20the%20intended%0Amanipulation%20area.%20We%20propose%20a%20Contrastive%20Loss%20tailored%20to%20driving%20effective%0Aimage%20editing%20by%20displacing%20edited%20areas%20and%20drawing%20preserved%20regions%20closer.%0AAcknowledging%20the%20continuous%20nature%20of%20image%20manipulations%2C%20we%20further%20refine%0Aour%20approach%20by%20incorporating%20the%20similarity%20concept%2C%20creating%20a%20Soft%0AContrastive%20Loss.%20The%20new%20losses%20are%20incorporated%20to%20the%20diffusion%20model%2C%0Ademonstrating%20improved%20or%20competitive%20image%20editing%20results%20on%20public%20datasets%0Aand%20generated%20images%20over%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Augmentation%2520for%2520Self-supervised%2520Text-guided%2520Image%2520Manipulation%26entry.906535625%3DRumeysa%2520Bodur%2520and%2520Binod%2520Bhattarai%2520and%2520Tae-Kyun%2520Kim%26entry.1292438233%3D%2520%2520Text-guided%2520image%2520editing%2520finds%2520applications%2520in%2520various%2520creative%2520and%250Apractical%2520fields.%2520While%2520recent%2520studies%2520in%2520image%2520generation%2520have%2520advanced%2520the%250Afield%252C%2520they%2520often%2520struggle%2520with%2520the%2520dual%2520challenges%2520of%2520coherent%2520image%250Atransformation%2520and%2520context%2520preservation.%2520In%2520response%252C%2520our%2520work%2520introduces%250Aprompt%2520augmentation%252C%2520a%2520method%2520amplifying%2520a%2520single%2520input%2520prompt%2520into%2520several%250Atarget%2520prompts%252C%2520strengthening%2520textual%2520context%2520and%2520enabling%2520localised%2520image%250Aediting.%2520Specifically%252C%2520we%2520use%2520the%2520augmented%2520prompts%2520to%2520delineate%2520the%2520intended%250Amanipulation%2520area.%2520We%2520propose%2520a%2520Contrastive%2520Loss%2520tailored%2520to%2520driving%2520effective%250Aimage%2520editing%2520by%2520displacing%2520edited%2520areas%2520and%2520drawing%2520preserved%2520regions%2520closer.%250AAcknowledging%2520the%2520continuous%2520nature%2520of%2520image%2520manipulations%252C%2520we%2520further%2520refine%250Aour%2520approach%2520by%2520incorporating%2520the%2520similarity%2520concept%252C%2520creating%2520a%2520Soft%250AContrastive%2520Loss.%2520The%2520new%2520losses%2520are%2520incorporated%2520to%2520the%2520diffusion%2520model%252C%250Ademonstrating%2520improved%2520or%2520competitive%2520image%2520editing%2520results%2520on%2520public%2520datasets%250Aand%2520generated%2520images%2520over%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Augmentation%20for%20Self-supervised%20Text-guided%20Image%20Manipulation&entry.906535625=Rumeysa%20Bodur%20and%20Binod%20Bhattarai%20and%20Tae-Kyun%20Kim&entry.1292438233=%20%20Text-guided%20image%20editing%20finds%20applications%20in%20various%20creative%20and%0Apractical%20fields.%20While%20recent%20studies%20in%20image%20generation%20have%20advanced%20the%0Afield%2C%20they%20often%20struggle%20with%20the%20dual%20challenges%20of%20coherent%20image%0Atransformation%20and%20context%20preservation.%20In%20response%2C%20our%20work%20introduces%0Aprompt%20augmentation%2C%20a%20method%20amplifying%20a%20single%20input%20prompt%20into%20several%0Atarget%20prompts%2C%20strengthening%20textual%20context%20and%20enabling%20localised%20image%0Aediting.%20Specifically%2C%20we%20use%20the%20augmented%20prompts%20to%20delineate%20the%20intended%0Amanipulation%20area.%20We%20propose%20a%20Contrastive%20Loss%20tailored%20to%20driving%20effective%0Aimage%20editing%20by%20displacing%20edited%20areas%20and%20drawing%20preserved%20regions%20closer.%0AAcknowledging%20the%20continuous%20nature%20of%20image%20manipulations%2C%20we%20further%20refine%0Aour%20approach%20by%20incorporating%20the%20similarity%20concept%2C%20creating%20a%20Soft%0AContrastive%20Loss.%20The%20new%20losses%20are%20incorporated%20to%20the%20diffusion%20model%2C%0Ademonstrating%20improved%20or%20competitive%20image%20editing%20results%20on%20public%20datasets%0Aand%20generated%20images%20over%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13081v1&entry.124074799=Read"},
{"title": "Causal Diffusion Transformers for Generative Modeling", "author": "Chaorui Deng and Deyao Zhu and Kunchang Li and Shi Guang and Haoqi Fan", "abstract": "  We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.\n", "link": "http://arxiv.org/abs/2412.12095v2", "date": "2024-12-17", "relevancy": 1.2643, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.677}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6233}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Diffusion%20Transformers%20for%20Generative%20Modeling&body=Title%3A%20Causal%20Diffusion%20Transformers%20for%20Generative%20Modeling%0AAuthor%3A%20Chaorui%20Deng%20and%20Deyao%20Zhu%20and%20Kunchang%20Li%20and%20Shi%20Guang%20and%20Haoqi%20Fan%0AAbstract%3A%20%20%20We%20introduce%20Causal%20Diffusion%20as%20the%20autoregressive%20%28AR%29%20counterpart%20of%0ADiffusion%20models.%20It%20is%20a%20next-token%28s%29%20forecasting%20framework%20that%20is%20friendly%0Ato%20both%20discrete%20and%20continuous%20modalities%20and%20compatible%20with%20existing%0Anext-token%20prediction%20models%20like%20LLaMA%20and%20GPT.%20While%20recent%20works%20attempt%20to%0Acombine%20diffusion%20with%20AR%20models%2C%20we%20show%20that%20introducing%20sequential%0Afactorization%20to%20a%20diffusion%20model%20can%20substantially%20improve%20its%20performance%0Aand%20enables%20a%20smooth%20transition%20between%20AR%20and%20diffusion%20generation%20modes.%0AHence%2C%20we%20propose%20CausalFusion%20-%20a%20decoder-only%20transformer%20that%0Adual-factorizes%20data%20across%20sequential%20tokens%20and%20diffusion%20noise%20levels%2C%0Aleading%20to%20state-of-the-art%20results%20on%20the%20ImageNet%20generation%20benchmark%20while%0Aalso%20enjoying%20the%20AR%20advantage%20of%20generating%20an%20arbitrary%20number%20of%20tokens%20for%0Ain-context%20reasoning.%20We%20further%20demonstrate%20CausalFusion%27s%20multimodal%0Acapabilities%20through%20a%20joint%20image%20generation%20and%20captioning%20model%2C%20and%0Ashowcase%20CausalFusion%27s%20ability%20for%20zero-shot%20in-context%20image%20manipulations.%0AWe%20hope%20that%20this%20work%20could%20provide%20the%20community%20with%20a%20fresh%20perspective%20on%0Atraining%20multimodal%20models%20over%20discrete%20and%20continuous%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Diffusion%2520Transformers%2520for%2520Generative%2520Modeling%26entry.906535625%3DChaorui%2520Deng%2520and%2520Deyao%2520Zhu%2520and%2520Kunchang%2520Li%2520and%2520Shi%2520Guang%2520and%2520Haoqi%2520Fan%26entry.1292438233%3D%2520%2520We%2520introduce%2520Causal%2520Diffusion%2520as%2520the%2520autoregressive%2520%2528AR%2529%2520counterpart%2520of%250ADiffusion%2520models.%2520It%2520is%2520a%2520next-token%2528s%2529%2520forecasting%2520framework%2520that%2520is%2520friendly%250Ato%2520both%2520discrete%2520and%2520continuous%2520modalities%2520and%2520compatible%2520with%2520existing%250Anext-token%2520prediction%2520models%2520like%2520LLaMA%2520and%2520GPT.%2520While%2520recent%2520works%2520attempt%2520to%250Acombine%2520diffusion%2520with%2520AR%2520models%252C%2520we%2520show%2520that%2520introducing%2520sequential%250Afactorization%2520to%2520a%2520diffusion%2520model%2520can%2520substantially%2520improve%2520its%2520performance%250Aand%2520enables%2520a%2520smooth%2520transition%2520between%2520AR%2520and%2520diffusion%2520generation%2520modes.%250AHence%252C%2520we%2520propose%2520CausalFusion%2520-%2520a%2520decoder-only%2520transformer%2520that%250Adual-factorizes%2520data%2520across%2520sequential%2520tokens%2520and%2520diffusion%2520noise%2520levels%252C%250Aleading%2520to%2520state-of-the-art%2520results%2520on%2520the%2520ImageNet%2520generation%2520benchmark%2520while%250Aalso%2520enjoying%2520the%2520AR%2520advantage%2520of%2520generating%2520an%2520arbitrary%2520number%2520of%2520tokens%2520for%250Ain-context%2520reasoning.%2520We%2520further%2520demonstrate%2520CausalFusion%2527s%2520multimodal%250Acapabilities%2520through%2520a%2520joint%2520image%2520generation%2520and%2520captioning%2520model%252C%2520and%250Ashowcase%2520CausalFusion%2527s%2520ability%2520for%2520zero-shot%2520in-context%2520image%2520manipulations.%250AWe%2520hope%2520that%2520this%2520work%2520could%2520provide%2520the%2520community%2520with%2520a%2520fresh%2520perspective%2520on%250Atraining%2520multimodal%2520models%2520over%2520discrete%2520and%2520continuous%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Diffusion%20Transformers%20for%20Generative%20Modeling&entry.906535625=Chaorui%20Deng%20and%20Deyao%20Zhu%20and%20Kunchang%20Li%20and%20Shi%20Guang%20and%20Haoqi%20Fan&entry.1292438233=%20%20We%20introduce%20Causal%20Diffusion%20as%20the%20autoregressive%20%28AR%29%20counterpart%20of%0ADiffusion%20models.%20It%20is%20a%20next-token%28s%29%20forecasting%20framework%20that%20is%20friendly%0Ato%20both%20discrete%20and%20continuous%20modalities%20and%20compatible%20with%20existing%0Anext-token%20prediction%20models%20like%20LLaMA%20and%20GPT.%20While%20recent%20works%20attempt%20to%0Acombine%20diffusion%20with%20AR%20models%2C%20we%20show%20that%20introducing%20sequential%0Afactorization%20to%20a%20diffusion%20model%20can%20substantially%20improve%20its%20performance%0Aand%20enables%20a%20smooth%20transition%20between%20AR%20and%20diffusion%20generation%20modes.%0AHence%2C%20we%20propose%20CausalFusion%20-%20a%20decoder-only%20transformer%20that%0Adual-factorizes%20data%20across%20sequential%20tokens%20and%20diffusion%20noise%20levels%2C%0Aleading%20to%20state-of-the-art%20results%20on%20the%20ImageNet%20generation%20benchmark%20while%0Aalso%20enjoying%20the%20AR%20advantage%20of%20generating%20an%20arbitrary%20number%20of%20tokens%20for%0Ain-context%20reasoning.%20We%20further%20demonstrate%20CausalFusion%27s%20multimodal%0Acapabilities%20through%20a%20joint%20image%20generation%20and%20captioning%20model%2C%20and%0Ashowcase%20CausalFusion%27s%20ability%20for%20zero-shot%20in-context%20image%20manipulations.%0AWe%20hope%20that%20this%20work%20could%20provide%20the%20community%20with%20a%20fresh%20perspective%20on%0Atraining%20multimodal%20models%20over%20discrete%20and%20continuous%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12095v2&entry.124074799=Read"},
{"title": "FunEditor: Achieving Complex Image Edits via Function Aggregation with\n  Diffusion Models", "author": "Mohammadreza Samadi and Fred X. Han and Mohammad Salameh and Hao Wu and Fengyu Sun and Chunhua Zhou and Di Niu", "abstract": "  Diffusion models have demonstrated outstanding performance in generative\ntasks, making them ideal candidates for image editing. Recent studies highlight\ntheir ability to apply desired edits effectively by following textual\ninstructions, yet with two key challenges remaining. First, these models\nstruggle to apply multiple edits simultaneously, resulting in computational\ninefficiencies due to their reliance on sequential processing. Second, relying\non textual prompts to determine the editing region can lead to unintended\nalterations to the image. We introduce FunEditor, an efficient diffusion model\ndesigned to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. Our experiments demonstrate that FunEditor\nsignificantly outperforms recent inference-time optimization methods and\nfine-tuned models, either quantitatively across various metrics or through\nvisual comparisons or both, on complex tasks like object movement and object\npasting. In the meantime, with only 4 steps of inference, FunEditor achieves\n5-24x inference speedups over existing popular methods. The code is available\nat: mhmdsmdi.github.io/funeditor/.\n", "link": "http://arxiv.org/abs/2408.08495v2", "date": "2024-12-17", "relevancy": 1.2213, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6194}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6098}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FunEditor%3A%20Achieving%20Complex%20Image%20Edits%20via%20Function%20Aggregation%20with%0A%20%20Diffusion%20Models&body=Title%3A%20FunEditor%3A%20Achieving%20Complex%20Image%20Edits%20via%20Function%20Aggregation%20with%0A%20%20Diffusion%20Models%0AAuthor%3A%20Mohammadreza%20Samadi%20and%20Fred%20X.%20Han%20and%20Mohammad%20Salameh%20and%20Hao%20Wu%20and%20Fengyu%20Sun%20and%20Chunhua%20Zhou%20and%20Di%20Niu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20outstanding%20performance%20in%20generative%0Atasks%2C%20making%20them%20ideal%20candidates%20for%20image%20editing.%20Recent%20studies%20highlight%0Atheir%20ability%20to%20apply%20desired%20edits%20effectively%20by%20following%20textual%0Ainstructions%2C%20yet%20with%20two%20key%20challenges%20remaining.%20First%2C%20these%20models%0Astruggle%20to%20apply%20multiple%20edits%20simultaneously%2C%20resulting%20in%20computational%0Ainefficiencies%20due%20to%20their%20reliance%20on%20sequential%20processing.%20Second%2C%20relying%0Aon%20textual%20prompts%20to%20determine%20the%20editing%20region%20can%20lead%20to%20unintended%0Aalterations%20to%20the%20image.%20We%20introduce%20FunEditor%2C%20an%20efficient%20diffusion%20model%0Adesigned%20to%20learn%20atomic%20editing%20functions%20and%20perform%20complex%20edits%20by%0Aaggregating%20simpler%20functions.%20This%20approach%20enables%20complex%20editing%20tasks%2C%0Asuch%20as%20object%20movement%2C%20by%20aggregating%20multiple%20functions%20and%20applying%20them%0Asimultaneously%20to%20specific%20areas.%20Our%20experiments%20demonstrate%20that%20FunEditor%0Asignificantly%20outperforms%20recent%20inference-time%20optimization%20methods%20and%0Afine-tuned%20models%2C%20either%20quantitatively%20across%20various%20metrics%20or%20through%0Avisual%20comparisons%20or%20both%2C%20on%20complex%20tasks%20like%20object%20movement%20and%20object%0Apasting.%20In%20the%20meantime%2C%20with%20only%204%20steps%20of%20inference%2C%20FunEditor%20achieves%0A5-24x%20inference%20speedups%20over%20existing%20popular%20methods.%20The%20code%20is%20available%0Aat%3A%20mhmdsmdi.github.io/funeditor/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunEditor%253A%2520Achieving%2520Complex%2520Image%2520Edits%2520via%2520Function%2520Aggregation%2520with%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DMohammadreza%2520Samadi%2520and%2520Fred%2520X.%2520Han%2520and%2520Mohammad%2520Salameh%2520and%2520Hao%2520Wu%2520and%2520Fengyu%2520Sun%2520and%2520Chunhua%2520Zhou%2520and%2520Di%2520Niu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520outstanding%2520performance%2520in%2520generative%250Atasks%252C%2520making%2520them%2520ideal%2520candidates%2520for%2520image%2520editing.%2520Recent%2520studies%2520highlight%250Atheir%2520ability%2520to%2520apply%2520desired%2520edits%2520effectively%2520by%2520following%2520textual%250Ainstructions%252C%2520yet%2520with%2520two%2520key%2520challenges%2520remaining.%2520First%252C%2520these%2520models%250Astruggle%2520to%2520apply%2520multiple%2520edits%2520simultaneously%252C%2520resulting%2520in%2520computational%250Ainefficiencies%2520due%2520to%2520their%2520reliance%2520on%2520sequential%2520processing.%2520Second%252C%2520relying%250Aon%2520textual%2520prompts%2520to%2520determine%2520the%2520editing%2520region%2520can%2520lead%2520to%2520unintended%250Aalterations%2520to%2520the%2520image.%2520We%2520introduce%2520FunEditor%252C%2520an%2520efficient%2520diffusion%2520model%250Adesigned%2520to%2520learn%2520atomic%2520editing%2520functions%2520and%2520perform%2520complex%2520edits%2520by%250Aaggregating%2520simpler%2520functions.%2520This%2520approach%2520enables%2520complex%2520editing%2520tasks%252C%250Asuch%2520as%2520object%2520movement%252C%2520by%2520aggregating%2520multiple%2520functions%2520and%2520applying%2520them%250Asimultaneously%2520to%2520specific%2520areas.%2520Our%2520experiments%2520demonstrate%2520that%2520FunEditor%250Asignificantly%2520outperforms%2520recent%2520inference-time%2520optimization%2520methods%2520and%250Afine-tuned%2520models%252C%2520either%2520quantitatively%2520across%2520various%2520metrics%2520or%2520through%250Avisual%2520comparisons%2520or%2520both%252C%2520on%2520complex%2520tasks%2520like%2520object%2520movement%2520and%2520object%250Apasting.%2520In%2520the%2520meantime%252C%2520with%2520only%25204%2520steps%2520of%2520inference%252C%2520FunEditor%2520achieves%250A5-24x%2520inference%2520speedups%2520over%2520existing%2520popular%2520methods.%2520The%2520code%2520is%2520available%250Aat%253A%2520mhmdsmdi.github.io/funeditor/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FunEditor%3A%20Achieving%20Complex%20Image%20Edits%20via%20Function%20Aggregation%20with%0A%20%20Diffusion%20Models&entry.906535625=Mohammadreza%20Samadi%20and%20Fred%20X.%20Han%20and%20Mohammad%20Salameh%20and%20Hao%20Wu%20and%20Fengyu%20Sun%20and%20Chunhua%20Zhou%20and%20Di%20Niu&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20outstanding%20performance%20in%20generative%0Atasks%2C%20making%20them%20ideal%20candidates%20for%20image%20editing.%20Recent%20studies%20highlight%0Atheir%20ability%20to%20apply%20desired%20edits%20effectively%20by%20following%20textual%0Ainstructions%2C%20yet%20with%20two%20key%20challenges%20remaining.%20First%2C%20these%20models%0Astruggle%20to%20apply%20multiple%20edits%20simultaneously%2C%20resulting%20in%20computational%0Ainefficiencies%20due%20to%20their%20reliance%20on%20sequential%20processing.%20Second%2C%20relying%0Aon%20textual%20prompts%20to%20determine%20the%20editing%20region%20can%20lead%20to%20unintended%0Aalterations%20to%20the%20image.%20We%20introduce%20FunEditor%2C%20an%20efficient%20diffusion%20model%0Adesigned%20to%20learn%20atomic%20editing%20functions%20and%20perform%20complex%20edits%20by%0Aaggregating%20simpler%20functions.%20This%20approach%20enables%20complex%20editing%20tasks%2C%0Asuch%20as%20object%20movement%2C%20by%20aggregating%20multiple%20functions%20and%20applying%20them%0Asimultaneously%20to%20specific%20areas.%20Our%20experiments%20demonstrate%20that%20FunEditor%0Asignificantly%20outperforms%20recent%20inference-time%20optimization%20methods%20and%0Afine-tuned%20models%2C%20either%20quantitatively%20across%20various%20metrics%20or%20through%0Avisual%20comparisons%20or%20both%2C%20on%20complex%20tasks%20like%20object%20movement%20and%20object%0Apasting.%20In%20the%20meantime%2C%20with%20only%204%20steps%20of%20inference%2C%20FunEditor%20achieves%0A5-24x%20inference%20speedups%20over%20existing%20popular%20methods.%20The%20code%20is%20available%0Aat%3A%20mhmdsmdi.github.io/funeditor/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08495v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


