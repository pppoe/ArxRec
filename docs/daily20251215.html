<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251214.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting", "author": "Mengjiao Ma and Qi Ma and Yue Li and Jiahuan Cheng and Runyi Yang and Bin Ren and Nikola Popovic and Mingqiang Wei and Nicu Sebe and Luc Van Gool and Theo Gevers and Martin R. Oswald and Danda Pani Paudel", "abstract": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets are released at https://scenesplatpp.gaussianworld.ai/.", "link": "http://arxiv.org/abs/2506.08710v3", "date": "2025-12-12", "relevancy": 3.235, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6794}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6685}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneSplat%2B%2B%3A%20A%20Large%20Dataset%20and%20Comprehensive%20Benchmark%20for%20Language%20Gaussian%20Splatting&body=Title%3A%20SceneSplat%2B%2B%3A%20A%20Large%20Dataset%20and%20Comprehensive%20Benchmark%20for%20Language%20Gaussian%20Splatting%0AAuthor%3A%20Mengjiao%20Ma%20and%20Qi%20Ma%20and%20Yue%20Li%20and%20Jiahuan%20Cheng%20and%20Runyi%20Yang%20and%20Bin%20Ren%20and%20Nikola%20Popovic%20and%20Mingqiang%20Wei%20and%20Nicu%20Sebe%20and%20Luc%20Van%20Gool%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald%20and%20Danda%20Pani%20Paudel%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20serves%20as%20a%20highly%20performant%20and%20efficient%20encoding%20of%20scene%20geometry%2C%20appearance%2C%20and%20semantics.%20Moreover%2C%20grounding%20language%20in%203D%20scenes%20has%20proven%20to%20be%20an%20effective%20strategy%20for%203D%20scene%20understanding.%20Current%20Language%20Gaussian%20Splatting%20line%20of%20work%20fall%20into%20three%20main%20groups%3A%20%28i%29%20per-scene%20optimization-based%2C%20%28ii%29%20per-scene%20optimization-free%2C%20and%20%28iii%29%20generalizable%20approach.%20However%2C%20most%20of%20them%20are%20evaluated%20only%20on%20rendered%202D%20views%20of%20a%20handful%20of%20scenes%20and%20viewpoints%20close%20to%20the%20training%20views%2C%20limiting%20ability%20and%20insight%20into%20holistic%203D%20understanding.%20To%20address%20this%20gap%2C%20we%20propose%20the%20first%20large-scale%20benchmark%20that%20systematically%20assesses%20these%20three%20groups%20of%20methods%20directly%20in%203D%20space%2C%20evaluating%20on%201060%20scenes%20across%20three%20indoor%20datasets%20and%20one%20outdoor%20dataset.%20Benchmark%20results%20demonstrate%20a%20clear%20advantage%20of%20the%20generalizable%20paradigm%2C%20particularly%20in%20relaxing%20the%20scene-specific%20limitation%2C%20enabling%20fast%20feed-forward%20inference%20on%20novel%20scenes%2C%20and%20achieving%20superior%20segmentation%20performance.%20We%20further%20introduce%20GaussianWorld-49K%20a%20carefully%20curated%203DGS%20dataset%20comprising%20around%2049K%20diverse%20indoor%20and%20outdoor%20scenes%20obtained%20from%20multiple%20sources%2C%20with%20which%20we%20demonstrate%20the%20generalizable%20approach%20could%20harness%20strong%20data%20priors.%20Our%20codes%2C%20benchmark%2C%20and%20datasets%20are%20released%20at%20https%3A//scenesplatpp.gaussianworld.ai/.%0ALink%3A%20http%3A//arxiv.org/abs/2506.08710v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneSplat%252B%252B%253A%2520A%2520Large%2520Dataset%2520and%2520Comprehensive%2520Benchmark%2520for%2520Language%2520Gaussian%2520Splatting%26entry.906535625%3DMengjiao%2520Ma%2520and%2520Qi%2520Ma%2520and%2520Yue%2520Li%2520and%2520Jiahuan%2520Cheng%2520and%2520Runyi%2520Yang%2520and%2520Bin%2520Ren%2520and%2520Nikola%2520Popovic%2520and%2520Mingqiang%2520Wei%2520and%2520Nicu%2520Sebe%2520and%2520Luc%2520Van%2520Gool%2520and%2520Theo%2520Gevers%2520and%2520Martin%2520R.%2520Oswald%2520and%2520Danda%2520Pani%2520Paudel%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520serves%2520as%2520a%2520highly%2520performant%2520and%2520efficient%2520encoding%2520of%2520scene%2520geometry%252C%2520appearance%252C%2520and%2520semantics.%2520Moreover%252C%2520grounding%2520language%2520in%25203D%2520scenes%2520has%2520proven%2520to%2520be%2520an%2520effective%2520strategy%2520for%25203D%2520scene%2520understanding.%2520Current%2520Language%2520Gaussian%2520Splatting%2520line%2520of%2520work%2520fall%2520into%2520three%2520main%2520groups%253A%2520%2528i%2529%2520per-scene%2520optimization-based%252C%2520%2528ii%2529%2520per-scene%2520optimization-free%252C%2520and%2520%2528iii%2529%2520generalizable%2520approach.%2520However%252C%2520most%2520of%2520them%2520are%2520evaluated%2520only%2520on%2520rendered%25202D%2520views%2520of%2520a%2520handful%2520of%2520scenes%2520and%2520viewpoints%2520close%2520to%2520the%2520training%2520views%252C%2520limiting%2520ability%2520and%2520insight%2520into%2520holistic%25203D%2520understanding.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520the%2520first%2520large-scale%2520benchmark%2520that%2520systematically%2520assesses%2520these%2520three%2520groups%2520of%2520methods%2520directly%2520in%25203D%2520space%252C%2520evaluating%2520on%25201060%2520scenes%2520across%2520three%2520indoor%2520datasets%2520and%2520one%2520outdoor%2520dataset.%2520Benchmark%2520results%2520demonstrate%2520a%2520clear%2520advantage%2520of%2520the%2520generalizable%2520paradigm%252C%2520particularly%2520in%2520relaxing%2520the%2520scene-specific%2520limitation%252C%2520enabling%2520fast%2520feed-forward%2520inference%2520on%2520novel%2520scenes%252C%2520and%2520achieving%2520superior%2520segmentation%2520performance.%2520We%2520further%2520introduce%2520GaussianWorld-49K%2520a%2520carefully%2520curated%25203DGS%2520dataset%2520comprising%2520around%252049K%2520diverse%2520indoor%2520and%2520outdoor%2520scenes%2520obtained%2520from%2520multiple%2520sources%252C%2520with%2520which%2520we%2520demonstrate%2520the%2520generalizable%2520approach%2520could%2520harness%2520strong%2520data%2520priors.%2520Our%2520codes%252C%2520benchmark%252C%2520and%2520datasets%2520are%2520released%2520at%2520https%253A//scenesplatpp.gaussianworld.ai/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08710v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneSplat%2B%2B%3A%20A%20Large%20Dataset%20and%20Comprehensive%20Benchmark%20for%20Language%20Gaussian%20Splatting&entry.906535625=Mengjiao%20Ma%20and%20Qi%20Ma%20and%20Yue%20Li%20and%20Jiahuan%20Cheng%20and%20Runyi%20Yang%20and%20Bin%20Ren%20and%20Nikola%20Popovic%20and%20Mingqiang%20Wei%20and%20Nicu%20Sebe%20and%20Luc%20Van%20Gool%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald%20and%20Danda%20Pani%20Paudel&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20serves%20as%20a%20highly%20performant%20and%20efficient%20encoding%20of%20scene%20geometry%2C%20appearance%2C%20and%20semantics.%20Moreover%2C%20grounding%20language%20in%203D%20scenes%20has%20proven%20to%20be%20an%20effective%20strategy%20for%203D%20scene%20understanding.%20Current%20Language%20Gaussian%20Splatting%20line%20of%20work%20fall%20into%20three%20main%20groups%3A%20%28i%29%20per-scene%20optimization-based%2C%20%28ii%29%20per-scene%20optimization-free%2C%20and%20%28iii%29%20generalizable%20approach.%20However%2C%20most%20of%20them%20are%20evaluated%20only%20on%20rendered%202D%20views%20of%20a%20handful%20of%20scenes%20and%20viewpoints%20close%20to%20the%20training%20views%2C%20limiting%20ability%20and%20insight%20into%20holistic%203D%20understanding.%20To%20address%20this%20gap%2C%20we%20propose%20the%20first%20large-scale%20benchmark%20that%20systematically%20assesses%20these%20three%20groups%20of%20methods%20directly%20in%203D%20space%2C%20evaluating%20on%201060%20scenes%20across%20three%20indoor%20datasets%20and%20one%20outdoor%20dataset.%20Benchmark%20results%20demonstrate%20a%20clear%20advantage%20of%20the%20generalizable%20paradigm%2C%20particularly%20in%20relaxing%20the%20scene-specific%20limitation%2C%20enabling%20fast%20feed-forward%20inference%20on%20novel%20scenes%2C%20and%20achieving%20superior%20segmentation%20performance.%20We%20further%20introduce%20GaussianWorld-49K%20a%20carefully%20curated%203DGS%20dataset%20comprising%20around%2049K%20diverse%20indoor%20and%20outdoor%20scenes%20obtained%20from%20multiple%20sources%2C%20with%20which%20we%20demonstrate%20the%20generalizable%20approach%20could%20harness%20strong%20data%20priors.%20Our%20codes%2C%20benchmark%2C%20and%20datasets%20are%20released%20at%20https%3A//scenesplatpp.gaussianworld.ai/.&entry.1838667208=http%3A//arxiv.org/abs/2506.08710v3&entry.124074799=Read"},
{"title": "Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling", "author": "Maik Dannecker and Steven Jia and Nil Stolt-Ans\u00f3 and Nadine Girard and Guillaume Auzias and Fran\u00e7ois Rousseau and Daniel Rueckert", "abstract": "Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \\textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\\mathbf\u03a3_{obs} = \\mathbf\u03a3_{HR} + \\mathbf\u03a3_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\\times$--10$\\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.", "link": "http://arxiv.org/abs/2512.11624v1", "date": "2025-12-12", "relevancy": 3.2257, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6705}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6581}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Explicit%3A%20Slice-to-Volume%20Reconstruction%20via%203D%20Gaussian%20Primitives%20with%20Analytic%20Point%20Spread%20Function%20Modeling&body=Title%3A%20Fast%20and%20Explicit%3A%20Slice-to-Volume%20Reconstruction%20via%203D%20Gaussian%20Primitives%20with%20Analytic%20Point%20Spread%20Function%20Modeling%0AAuthor%3A%20Maik%20Dannecker%20and%20Steven%20Jia%20and%20Nil%20Stolt-Ans%C3%B3%20and%20Nadine%20Girard%20and%20Guillaume%20Auzias%20and%20Fran%C3%A7ois%20Rousseau%20and%20Daniel%20Rueckert%0AAbstract%3A%20Recovering%20high-fidelity%203D%20images%20from%20sparse%20or%20degraded%202D%20images%20is%20a%20fundamental%20challenge%20in%20medical%20imaging%2C%20with%20broad%20applications%20ranging%20from%203D%20ultrasound%20reconstruction%20to%20MRI%20super-resolution.%20In%20the%20context%20of%20fetal%20MRI%2C%20high-resolution%203D%20reconstruction%20of%20the%20brain%20from%20motion-corrupted%20low-resolution%202D%20acquisitions%20is%20a%20prerequisite%20for%20accurate%20neurodevelopmental%20diagnosis.%20While%20implicit%20neural%20representations%20%28INRs%29%20have%20recently%20established%20state-of-the-art%20performance%20in%20self-supervised%20slice-to-volume%20reconstruction%20%28SVR%29%2C%20they%20suffer%20from%20a%20critical%20computational%20bottleneck%3A%20accurately%20modeling%20the%20image%20acquisition%20physics%20requires%20expensive%20stochastic%20Monte%20Carlo%20sampling%20to%20approximate%20the%20point%20spread%20function%20%28PSF%29.%20In%20this%20work%2C%20we%20propose%20a%20shift%20from%20neural%20network%20based%20implicit%20representations%20to%20Gaussian%20based%20explicit%20representations.%20By%20parameterizing%20the%20HR%203D%20image%20volume%20as%20a%20field%20of%20anisotropic%20Gaussian%20primitives%2C%20we%20leverage%20the%20property%20of%20Gaussians%20being%20closed%20under%20convolution%20and%20thus%20derive%20a%20%5Ctextit%7Bclosed-form%20analytical%20solution%7D%20for%20the%20forward%20model.%20This%20formulation%20reduces%20the%20previously%20intractable%20acquisition%20integral%20to%20an%20exact%20covariance%20addition%20%28%24%5Cmathbf%CE%A3_%7Bobs%7D%20%3D%20%5Cmathbf%CE%A3_%7BHR%7D%20%2B%20%5Cmathbf%CE%A3_%7BPSF%7D%24%29%2C%20effectively%20bypassing%20the%20need%20for%20compute-intensive%20stochastic%20sampling%20while%20ensuring%20exact%20gradient%20propagation.%20We%20demonstrate%20that%20our%20approach%20matches%20the%20reconstruction%20quality%20of%20self-supervised%20state-of-the-art%20SVR%20frameworks%20while%20delivering%20a%205%24%5Ctimes%24--10%24%5Ctimes%24%20speed-up%20on%20neonatal%20and%20fetal%20data.%20With%20convergence%20often%20reached%20in%20under%2030%20seconds%2C%20our%20framework%20paves%20the%20way%20towards%20translation%20into%20clinical%20routine%20of%20real-time%20fetal%203D%20MRI.%20Code%20will%20be%20public%20at%20%7Bhttps%3A//github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Explicit%253A%2520Slice-to-Volume%2520Reconstruction%2520via%25203D%2520Gaussian%2520Primitives%2520with%2520Analytic%2520Point%2520Spread%2520Function%2520Modeling%26entry.906535625%3DMaik%2520Dannecker%2520and%2520Steven%2520Jia%2520and%2520Nil%2520Stolt-Ans%25C3%25B3%2520and%2520Nadine%2520Girard%2520and%2520Guillaume%2520Auzias%2520and%2520Fran%25C3%25A7ois%2520Rousseau%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3DRecovering%2520high-fidelity%25203D%2520images%2520from%2520sparse%2520or%2520degraded%25202D%2520images%2520is%2520a%2520fundamental%2520challenge%2520in%2520medical%2520imaging%252C%2520with%2520broad%2520applications%2520ranging%2520from%25203D%2520ultrasound%2520reconstruction%2520to%2520MRI%2520super-resolution.%2520In%2520the%2520context%2520of%2520fetal%2520MRI%252C%2520high-resolution%25203D%2520reconstruction%2520of%2520the%2520brain%2520from%2520motion-corrupted%2520low-resolution%25202D%2520acquisitions%2520is%2520a%2520prerequisite%2520for%2520accurate%2520neurodevelopmental%2520diagnosis.%2520While%2520implicit%2520neural%2520representations%2520%2528INRs%2529%2520have%2520recently%2520established%2520state-of-the-art%2520performance%2520in%2520self-supervised%2520slice-to-volume%2520reconstruction%2520%2528SVR%2529%252C%2520they%2520suffer%2520from%2520a%2520critical%2520computational%2520bottleneck%253A%2520accurately%2520modeling%2520the%2520image%2520acquisition%2520physics%2520requires%2520expensive%2520stochastic%2520Monte%2520Carlo%2520sampling%2520to%2520approximate%2520the%2520point%2520spread%2520function%2520%2528PSF%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520shift%2520from%2520neural%2520network%2520based%2520implicit%2520representations%2520to%2520Gaussian%2520based%2520explicit%2520representations.%2520By%2520parameterizing%2520the%2520HR%25203D%2520image%2520volume%2520as%2520a%2520field%2520of%2520anisotropic%2520Gaussian%2520primitives%252C%2520we%2520leverage%2520the%2520property%2520of%2520Gaussians%2520being%2520closed%2520under%2520convolution%2520and%2520thus%2520derive%2520a%2520%255Ctextit%257Bclosed-form%2520analytical%2520solution%257D%2520for%2520the%2520forward%2520model.%2520This%2520formulation%2520reduces%2520the%2520previously%2520intractable%2520acquisition%2520integral%2520to%2520an%2520exact%2520covariance%2520addition%2520%2528%2524%255Cmathbf%25CE%25A3_%257Bobs%257D%2520%253D%2520%255Cmathbf%25CE%25A3_%257BHR%257D%2520%252B%2520%255Cmathbf%25CE%25A3_%257BPSF%257D%2524%2529%252C%2520effectively%2520bypassing%2520the%2520need%2520for%2520compute-intensive%2520stochastic%2520sampling%2520while%2520ensuring%2520exact%2520gradient%2520propagation.%2520We%2520demonstrate%2520that%2520our%2520approach%2520matches%2520the%2520reconstruction%2520quality%2520of%2520self-supervised%2520state-of-the-art%2520SVR%2520frameworks%2520while%2520delivering%2520a%25205%2524%255Ctimes%2524--10%2524%255Ctimes%2524%2520speed-up%2520on%2520neonatal%2520and%2520fetal%2520data.%2520With%2520convergence%2520often%2520reached%2520in%2520under%252030%2520seconds%252C%2520our%2520framework%2520paves%2520the%2520way%2520towards%2520translation%2520into%2520clinical%2520routine%2520of%2520real-time%2520fetal%25203D%2520MRI.%2520Code%2520will%2520be%2520public%2520at%2520%257Bhttps%253A//github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Explicit%3A%20Slice-to-Volume%20Reconstruction%20via%203D%20Gaussian%20Primitives%20with%20Analytic%20Point%20Spread%20Function%20Modeling&entry.906535625=Maik%20Dannecker%20and%20Steven%20Jia%20and%20Nil%20Stolt-Ans%C3%B3%20and%20Nadine%20Girard%20and%20Guillaume%20Auzias%20and%20Fran%C3%A7ois%20Rousseau%20and%20Daniel%20Rueckert&entry.1292438233=Recovering%20high-fidelity%203D%20images%20from%20sparse%20or%20degraded%202D%20images%20is%20a%20fundamental%20challenge%20in%20medical%20imaging%2C%20with%20broad%20applications%20ranging%20from%203D%20ultrasound%20reconstruction%20to%20MRI%20super-resolution.%20In%20the%20context%20of%20fetal%20MRI%2C%20high-resolution%203D%20reconstruction%20of%20the%20brain%20from%20motion-corrupted%20low-resolution%202D%20acquisitions%20is%20a%20prerequisite%20for%20accurate%20neurodevelopmental%20diagnosis.%20While%20implicit%20neural%20representations%20%28INRs%29%20have%20recently%20established%20state-of-the-art%20performance%20in%20self-supervised%20slice-to-volume%20reconstruction%20%28SVR%29%2C%20they%20suffer%20from%20a%20critical%20computational%20bottleneck%3A%20accurately%20modeling%20the%20image%20acquisition%20physics%20requires%20expensive%20stochastic%20Monte%20Carlo%20sampling%20to%20approximate%20the%20point%20spread%20function%20%28PSF%29.%20In%20this%20work%2C%20we%20propose%20a%20shift%20from%20neural%20network%20based%20implicit%20representations%20to%20Gaussian%20based%20explicit%20representations.%20By%20parameterizing%20the%20HR%203D%20image%20volume%20as%20a%20field%20of%20anisotropic%20Gaussian%20primitives%2C%20we%20leverage%20the%20property%20of%20Gaussians%20being%20closed%20under%20convolution%20and%20thus%20derive%20a%20%5Ctextit%7Bclosed-form%20analytical%20solution%7D%20for%20the%20forward%20model.%20This%20formulation%20reduces%20the%20previously%20intractable%20acquisition%20integral%20to%20an%20exact%20covariance%20addition%20%28%24%5Cmathbf%CE%A3_%7Bobs%7D%20%3D%20%5Cmathbf%CE%A3_%7BHR%7D%20%2B%20%5Cmathbf%CE%A3_%7BPSF%7D%24%29%2C%20effectively%20bypassing%20the%20need%20for%20compute-intensive%20stochastic%20sampling%20while%20ensuring%20exact%20gradient%20propagation.%20We%20demonstrate%20that%20our%20approach%20matches%20the%20reconstruction%20quality%20of%20self-supervised%20state-of-the-art%20SVR%20frameworks%20while%20delivering%20a%205%24%5Ctimes%24--10%24%5Ctimes%24%20speed-up%20on%20neonatal%20and%20fetal%20data.%20With%20convergence%20often%20reached%20in%20under%2030%20seconds%2C%20our%20framework%20paves%20the%20way%20towards%20translation%20into%20clinical%20routine%20of%20real-time%20fetal%203D%20MRI.%20Code%20will%20be%20public%20at%20%7Bhttps%3A//github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.11624v1&entry.124074799=Read"},
{"title": "3D-LATTE: Latent Space 3D Editing from Textual Instructions", "author": "Maria Parelli and Michael Oechsle and Michael Niemeyer and Federico Tombari and Andreas Geiger", "abstract": "Despite the recent success of multi-view diffusion models for text/image-based 3D asset generation, instruction-based editing of 3D assets lacks surprisingly far behind the quality of generation models. The main reason is that recent approaches using 2D priors suffer from view-inconsistent editing signals. Going beyond 2D prior distillation methods and multi-view editing strategies, we propose a training-free editing method that operates within the latent space of a native 3D diffusion model, allowing us to directly manipulate 3D geometry. We guide the edit synthesis by blending 3D attention maps from the generation with the source object. Coupled with geometry-aware regularization guidance, a spectral modulation strategy in the Fourier domain and a refinement step for 3D enhancement, our method outperforms previous 3D editing methods enabling high-fidelity and precise edits across a wide range of shapes and semantic manipulations. Our project webpage is https://mparelli.github.io/3d-latte", "link": "http://arxiv.org/abs/2509.00269v3", "date": "2025-12-12", "relevancy": 3.2202, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6555}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6383}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-LATTE%3A%20Latent%20Space%203D%20Editing%20from%20Textual%20Instructions&body=Title%3A%203D-LATTE%3A%20Latent%20Space%203D%20Editing%20from%20Textual%20Instructions%0AAuthor%3A%20Maria%20Parelli%20and%20Michael%20Oechsle%20and%20Michael%20Niemeyer%20and%20Federico%20Tombari%20and%20Andreas%20Geiger%0AAbstract%3A%20Despite%20the%20recent%20success%20of%20multi-view%20diffusion%20models%20for%20text/image-based%203D%20asset%20generation%2C%20instruction-based%20editing%20of%203D%20assets%20lacks%20surprisingly%20far%20behind%20the%20quality%20of%20generation%20models.%20The%20main%20reason%20is%20that%20recent%20approaches%20using%202D%20priors%20suffer%20from%20view-inconsistent%20editing%20signals.%20Going%20beyond%202D%20prior%20distillation%20methods%20and%20multi-view%20editing%20strategies%2C%20we%20propose%20a%20training-free%20editing%20method%20that%20operates%20within%20the%20latent%20space%20of%20a%20native%203D%20diffusion%20model%2C%20allowing%20us%20to%20directly%20manipulate%203D%20geometry.%20We%20guide%20the%20edit%20synthesis%20by%20blending%203D%20attention%20maps%20from%20the%20generation%20with%20the%20source%20object.%20Coupled%20with%20geometry-aware%20regularization%20guidance%2C%20a%20spectral%20modulation%20strategy%20in%20the%20Fourier%20domain%20and%20a%20refinement%20step%20for%203D%20enhancement%2C%20our%20method%20outperforms%20previous%203D%20editing%20methods%20enabling%20high-fidelity%20and%20precise%20edits%20across%20a%20wide%20range%20of%20shapes%20and%20semantic%20manipulations.%20Our%20project%20webpage%20is%20https%3A//mparelli.github.io/3d-latte%0ALink%3A%20http%3A//arxiv.org/abs/2509.00269v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-LATTE%253A%2520Latent%2520Space%25203D%2520Editing%2520from%2520Textual%2520Instructions%26entry.906535625%3DMaria%2520Parelli%2520and%2520Michael%2520Oechsle%2520and%2520Michael%2520Niemeyer%2520and%2520Federico%2520Tombari%2520and%2520Andreas%2520Geiger%26entry.1292438233%3DDespite%2520the%2520recent%2520success%2520of%2520multi-view%2520diffusion%2520models%2520for%2520text/image-based%25203D%2520asset%2520generation%252C%2520instruction-based%2520editing%2520of%25203D%2520assets%2520lacks%2520surprisingly%2520far%2520behind%2520the%2520quality%2520of%2520generation%2520models.%2520The%2520main%2520reason%2520is%2520that%2520recent%2520approaches%2520using%25202D%2520priors%2520suffer%2520from%2520view-inconsistent%2520editing%2520signals.%2520Going%2520beyond%25202D%2520prior%2520distillation%2520methods%2520and%2520multi-view%2520editing%2520strategies%252C%2520we%2520propose%2520a%2520training-free%2520editing%2520method%2520that%2520operates%2520within%2520the%2520latent%2520space%2520of%2520a%2520native%25203D%2520diffusion%2520model%252C%2520allowing%2520us%2520to%2520directly%2520manipulate%25203D%2520geometry.%2520We%2520guide%2520the%2520edit%2520synthesis%2520by%2520blending%25203D%2520attention%2520maps%2520from%2520the%2520generation%2520with%2520the%2520source%2520object.%2520Coupled%2520with%2520geometry-aware%2520regularization%2520guidance%252C%2520a%2520spectral%2520modulation%2520strategy%2520in%2520the%2520Fourier%2520domain%2520and%2520a%2520refinement%2520step%2520for%25203D%2520enhancement%252C%2520our%2520method%2520outperforms%2520previous%25203D%2520editing%2520methods%2520enabling%2520high-fidelity%2520and%2520precise%2520edits%2520across%2520a%2520wide%2520range%2520of%2520shapes%2520and%2520semantic%2520manipulations.%2520Our%2520project%2520webpage%2520is%2520https%253A//mparelli.github.io/3d-latte%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00269v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-LATTE%3A%20Latent%20Space%203D%20Editing%20from%20Textual%20Instructions&entry.906535625=Maria%20Parelli%20and%20Michael%20Oechsle%20and%20Michael%20Niemeyer%20and%20Federico%20Tombari%20and%20Andreas%20Geiger&entry.1292438233=Despite%20the%20recent%20success%20of%20multi-view%20diffusion%20models%20for%20text/image-based%203D%20asset%20generation%2C%20instruction-based%20editing%20of%203D%20assets%20lacks%20surprisingly%20far%20behind%20the%20quality%20of%20generation%20models.%20The%20main%20reason%20is%20that%20recent%20approaches%20using%202D%20priors%20suffer%20from%20view-inconsistent%20editing%20signals.%20Going%20beyond%202D%20prior%20distillation%20methods%20and%20multi-view%20editing%20strategies%2C%20we%20propose%20a%20training-free%20editing%20method%20that%20operates%20within%20the%20latent%20space%20of%20a%20native%203D%20diffusion%20model%2C%20allowing%20us%20to%20directly%20manipulate%203D%20geometry.%20We%20guide%20the%20edit%20synthesis%20by%20blending%203D%20attention%20maps%20from%20the%20generation%20with%20the%20source%20object.%20Coupled%20with%20geometry-aware%20regularization%20guidance%2C%20a%20spectral%20modulation%20strategy%20in%20the%20Fourier%20domain%20and%20a%20refinement%20step%20for%203D%20enhancement%2C%20our%20method%20outperforms%20previous%203D%20editing%20methods%20enabling%20high-fidelity%20and%20precise%20edits%20across%20a%20wide%20range%20of%20shapes%20and%20semantic%20manipulations.%20Our%20project%20webpage%20is%20https%3A//mparelli.github.io/3d-latte&entry.1838667208=http%3A//arxiv.org/abs/2509.00269v3&entry.124074799=Read"},
{"title": "Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing", "author": "Xu Zhang and Jiabin Fang and Zhuoming Ding and Jin Yuan and Xuan Liu and Qianjun Zhang and Zhiyong Li", "abstract": "Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.", "link": "http://arxiv.org/abs/2512.11680v1", "date": "2025-12-12", "relevancy": 3.1328, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6327}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6235}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-modal%20Context-aware%20Learning%20for%20Visual%20Prompt%20Guided%20Multimodal%20Image%20Understanding%20in%20Remote%20Sensing&body=Title%3A%20Cross-modal%20Context-aware%20Learning%20for%20Visual%20Prompt%20Guided%20Multimodal%20Image%20Understanding%20in%20Remote%20Sensing%0AAuthor%3A%20Xu%20Zhang%20and%20Jiabin%20Fang%20and%20Zhuoming%20Ding%20and%20Jin%20Yuan%20and%20Xuan%20Liu%20and%20Qianjun%20Zhang%20and%20Zhiyong%20Li%0AAbstract%3A%20Recent%20advances%20in%20image%20understanding%20have%20enabled%20methods%20that%20leverage%20large%20language%20models%20for%20multimodal%20reasoning%20in%20remote%20sensing.%20However%2C%20existing%20approaches%20still%20struggle%20to%20steer%20models%20to%20the%20user-relevant%20regions%20when%20only%20simple%2C%20generic%20text%20prompts%20are%20available.%20Moreover%2C%20in%20large-scale%20aerial%20imagery%20many%20objects%20exhibit%20highly%20similar%20visual%20appearances%20and%20carry%20rich%20inter-object%20relationships%2C%20which%20further%20complicates%20accurate%20recognition.%20To%20address%20these%20challenges%2C%20we%20propose%20Cross-modal%20Context-aware%20Learning%20for%20Visual%20Prompt-Guided%20Multimodal%20Image%20Understanding%20%28CLV-Net%29.%20CLV-Net%20lets%20users%20supply%20a%20simple%20visual%20cue%2C%20a%20bounding%20box%2C%20to%20indicate%20a%20region%20of%20interest%2C%20and%20uses%20that%20cue%20to%20guide%20the%20model%20to%20generate%20correlated%20segmentation%20masks%20and%20captions%20that%20faithfully%20reflect%20user%20intent.%20Central%20to%20our%20design%20is%20a%20Context-Aware%20Mask%20Decoder%20that%20models%20and%20integrates%20inter-object%20relationships%20to%20strengthen%20target%20representations%20and%20improve%20mask%20quality.%20In%20addition%2C%20we%20introduce%20a%20Semantic%20and%20Relationship%20Alignment%20module%3A%20a%20Cross-modal%20Semantic%20Consistency%20Loss%20enhances%20fine-grained%20discrimination%20among%20visually%20similar%20targets%2C%20while%20a%20Relationship%20Consistency%20Loss%20enforces%20alignment%20between%20textual%20relations%20and%20visual%20interactions.%20Comprehensive%20experiments%20on%20two%20benchmark%20datasets%20show%20that%20CLV-Net%20outperforms%20existing%20methods%20and%20establishes%20new%20state-of-the-art%20results.%20The%20model%20effectively%20captures%20user%20intent%20and%20produces%20precise%2C%20intention-aligned%20multimodal%20outputs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-modal%2520Context-aware%2520Learning%2520for%2520Visual%2520Prompt%2520Guided%2520Multimodal%2520Image%2520Understanding%2520in%2520Remote%2520Sensing%26entry.906535625%3DXu%2520Zhang%2520and%2520Jiabin%2520Fang%2520and%2520Zhuoming%2520Ding%2520and%2520Jin%2520Yuan%2520and%2520Xuan%2520Liu%2520and%2520Qianjun%2520Zhang%2520and%2520Zhiyong%2520Li%26entry.1292438233%3DRecent%2520advances%2520in%2520image%2520understanding%2520have%2520enabled%2520methods%2520that%2520leverage%2520large%2520language%2520models%2520for%2520multimodal%2520reasoning%2520in%2520remote%2520sensing.%2520However%252C%2520existing%2520approaches%2520still%2520struggle%2520to%2520steer%2520models%2520to%2520the%2520user-relevant%2520regions%2520when%2520only%2520simple%252C%2520generic%2520text%2520prompts%2520are%2520available.%2520Moreover%252C%2520in%2520large-scale%2520aerial%2520imagery%2520many%2520objects%2520exhibit%2520highly%2520similar%2520visual%2520appearances%2520and%2520carry%2520rich%2520inter-object%2520relationships%252C%2520which%2520further%2520complicates%2520accurate%2520recognition.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Cross-modal%2520Context-aware%2520Learning%2520for%2520Visual%2520Prompt-Guided%2520Multimodal%2520Image%2520Understanding%2520%2528CLV-Net%2529.%2520CLV-Net%2520lets%2520users%2520supply%2520a%2520simple%2520visual%2520cue%252C%2520a%2520bounding%2520box%252C%2520to%2520indicate%2520a%2520region%2520of%2520interest%252C%2520and%2520uses%2520that%2520cue%2520to%2520guide%2520the%2520model%2520to%2520generate%2520correlated%2520segmentation%2520masks%2520and%2520captions%2520that%2520faithfully%2520reflect%2520user%2520intent.%2520Central%2520to%2520our%2520design%2520is%2520a%2520Context-Aware%2520Mask%2520Decoder%2520that%2520models%2520and%2520integrates%2520inter-object%2520relationships%2520to%2520strengthen%2520target%2520representations%2520and%2520improve%2520mask%2520quality.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520Semantic%2520and%2520Relationship%2520Alignment%2520module%253A%2520a%2520Cross-modal%2520Semantic%2520Consistency%2520Loss%2520enhances%2520fine-grained%2520discrimination%2520among%2520visually%2520similar%2520targets%252C%2520while%2520a%2520Relationship%2520Consistency%2520Loss%2520enforces%2520alignment%2520between%2520textual%2520relations%2520and%2520visual%2520interactions.%2520Comprehensive%2520experiments%2520on%2520two%2520benchmark%2520datasets%2520show%2520that%2520CLV-Net%2520outperforms%2520existing%2520methods%2520and%2520establishes%2520new%2520state-of-the-art%2520results.%2520The%2520model%2520effectively%2520captures%2520user%2520intent%2520and%2520produces%2520precise%252C%2520intention-aligned%2520multimodal%2520outputs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-modal%20Context-aware%20Learning%20for%20Visual%20Prompt%20Guided%20Multimodal%20Image%20Understanding%20in%20Remote%20Sensing&entry.906535625=Xu%20Zhang%20and%20Jiabin%20Fang%20and%20Zhuoming%20Ding%20and%20Jin%20Yuan%20and%20Xuan%20Liu%20and%20Qianjun%20Zhang%20and%20Zhiyong%20Li&entry.1292438233=Recent%20advances%20in%20image%20understanding%20have%20enabled%20methods%20that%20leverage%20large%20language%20models%20for%20multimodal%20reasoning%20in%20remote%20sensing.%20However%2C%20existing%20approaches%20still%20struggle%20to%20steer%20models%20to%20the%20user-relevant%20regions%20when%20only%20simple%2C%20generic%20text%20prompts%20are%20available.%20Moreover%2C%20in%20large-scale%20aerial%20imagery%20many%20objects%20exhibit%20highly%20similar%20visual%20appearances%20and%20carry%20rich%20inter-object%20relationships%2C%20which%20further%20complicates%20accurate%20recognition.%20To%20address%20these%20challenges%2C%20we%20propose%20Cross-modal%20Context-aware%20Learning%20for%20Visual%20Prompt-Guided%20Multimodal%20Image%20Understanding%20%28CLV-Net%29.%20CLV-Net%20lets%20users%20supply%20a%20simple%20visual%20cue%2C%20a%20bounding%20box%2C%20to%20indicate%20a%20region%20of%20interest%2C%20and%20uses%20that%20cue%20to%20guide%20the%20model%20to%20generate%20correlated%20segmentation%20masks%20and%20captions%20that%20faithfully%20reflect%20user%20intent.%20Central%20to%20our%20design%20is%20a%20Context-Aware%20Mask%20Decoder%20that%20models%20and%20integrates%20inter-object%20relationships%20to%20strengthen%20target%20representations%20and%20improve%20mask%20quality.%20In%20addition%2C%20we%20introduce%20a%20Semantic%20and%20Relationship%20Alignment%20module%3A%20a%20Cross-modal%20Semantic%20Consistency%20Loss%20enhances%20fine-grained%20discrimination%20among%20visually%20similar%20targets%2C%20while%20a%20Relationship%20Consistency%20Loss%20enforces%20alignment%20between%20textual%20relations%20and%20visual%20interactions.%20Comprehensive%20experiments%20on%20two%20benchmark%20datasets%20show%20that%20CLV-Net%20outperforms%20existing%20methods%20and%20establishes%20new%20state-of-the-art%20results.%20The%20model%20effectively%20captures%20user%20intent%20and%20produces%20precise%2C%20intention-aligned%20multimodal%20outputs.&entry.1838667208=http%3A//arxiv.org/abs/2512.11680v1&entry.124074799=Read"},
{"title": "Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation", "author": "Yan Zhang and Han Zou and Lincong Feng and Cong Xie and Ruiqi Yu and Zhenpeng Zhan", "abstract": "Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io", "link": "http://arxiv.org/abs/2512.11720v1", "date": "2025-12-12", "relevancy": 3.1258, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6614}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.621}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reframing%20Music-Driven%202D%20Dance%20Pose%20Generation%20as%20Multi-Channel%20Image%20Generation&body=Title%3A%20Reframing%20Music-Driven%202D%20Dance%20Pose%20Generation%20as%20Multi-Channel%20Image%20Generation%0AAuthor%3A%20Yan%20Zhang%20and%20Han%20Zou%20and%20Lincong%20Feng%20and%20Cong%20Xie%20and%20Ruiqi%20Yu%20and%20Zhenpeng%20Zhan%0AAbstract%3A%20Recent%20pose-to-video%20models%20can%20translate%202D%20pose%20sequences%20into%20photorealistic%2C%20identity-preserving%20dance%20videos%2C%20so%20the%20key%20challenge%20is%20to%20generate%20temporally%20coherent%2C%20rhythm-aligned%202D%20poses%20from%20music%2C%20especially%20under%20complex%2C%20high-variance%20in-the-wild%20distributions.%20We%20address%20this%20by%20reframing%20music-to-dance%20generation%20as%20a%20music-token-conditioned%20multi-channel%20image%20synthesis%20problem%3A%202D%20pose%20sequences%20are%20encoded%20as%20one-hot%20images%2C%20compressed%20by%20a%20pretrained%20image%20VAE%2C%20and%20modeled%20with%20a%20DiT-style%20backbone%2C%20allowing%20us%20to%20inherit%20architectural%20and%20training%20advances%20from%20modern%20text-to-image%20models%20and%20better%20capture%20high-variance%202D%20pose%20distributions.%20On%20top%20of%20this%20formulation%2C%20we%20introduce%20%28i%29%20a%20time-shared%20temporal%20indexing%20scheme%20that%20explicitly%20synchronizes%20music%20tokens%20and%20pose%20latents%20over%20time%20and%20%28ii%29%20a%20reference-pose%20conditioning%20strategy%20that%20preserves%20subject-specific%20body%20proportions%20and%20on-screen%20scale%20while%20enabling%20long-horizon%20segment-and-stitch%20generation.%20Experiments%20on%20a%20large%20in-the-wild%202D%20dance%20corpus%20and%20the%20calibrated%20AIST%2B%2B2D%20benchmark%20show%20consistent%20improvements%20over%20representative%20music-to-dance%20methods%20in%20pose-%20and%20video-space%20metrics%20and%20human%20preference%2C%20and%20ablations%20validate%20the%20contributions%20of%20the%20representation%2C%20temporal%20indexing%2C%20and%20reference%20conditioning.%20See%20supplementary%20videos%20at%20https%3A//hot-dance.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2512.11720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReframing%2520Music-Driven%25202D%2520Dance%2520Pose%2520Generation%2520as%2520Multi-Channel%2520Image%2520Generation%26entry.906535625%3DYan%2520Zhang%2520and%2520Han%2520Zou%2520and%2520Lincong%2520Feng%2520and%2520Cong%2520Xie%2520and%2520Ruiqi%2520Yu%2520and%2520Zhenpeng%2520Zhan%26entry.1292438233%3DRecent%2520pose-to-video%2520models%2520can%2520translate%25202D%2520pose%2520sequences%2520into%2520photorealistic%252C%2520identity-preserving%2520dance%2520videos%252C%2520so%2520the%2520key%2520challenge%2520is%2520to%2520generate%2520temporally%2520coherent%252C%2520rhythm-aligned%25202D%2520poses%2520from%2520music%252C%2520especially%2520under%2520complex%252C%2520high-variance%2520in-the-wild%2520distributions.%2520We%2520address%2520this%2520by%2520reframing%2520music-to-dance%2520generation%2520as%2520a%2520music-token-conditioned%2520multi-channel%2520image%2520synthesis%2520problem%253A%25202D%2520pose%2520sequences%2520are%2520encoded%2520as%2520one-hot%2520images%252C%2520compressed%2520by%2520a%2520pretrained%2520image%2520VAE%252C%2520and%2520modeled%2520with%2520a%2520DiT-style%2520backbone%252C%2520allowing%2520us%2520to%2520inherit%2520architectural%2520and%2520training%2520advances%2520from%2520modern%2520text-to-image%2520models%2520and%2520better%2520capture%2520high-variance%25202D%2520pose%2520distributions.%2520On%2520top%2520of%2520this%2520formulation%252C%2520we%2520introduce%2520%2528i%2529%2520a%2520time-shared%2520temporal%2520indexing%2520scheme%2520that%2520explicitly%2520synchronizes%2520music%2520tokens%2520and%2520pose%2520latents%2520over%2520time%2520and%2520%2528ii%2529%2520a%2520reference-pose%2520conditioning%2520strategy%2520that%2520preserves%2520subject-specific%2520body%2520proportions%2520and%2520on-screen%2520scale%2520while%2520enabling%2520long-horizon%2520segment-and-stitch%2520generation.%2520Experiments%2520on%2520a%2520large%2520in-the-wild%25202D%2520dance%2520corpus%2520and%2520the%2520calibrated%2520AIST%252B%252B2D%2520benchmark%2520show%2520consistent%2520improvements%2520over%2520representative%2520music-to-dance%2520methods%2520in%2520pose-%2520and%2520video-space%2520metrics%2520and%2520human%2520preference%252C%2520and%2520ablations%2520validate%2520the%2520contributions%2520of%2520the%2520representation%252C%2520temporal%2520indexing%252C%2520and%2520reference%2520conditioning.%2520See%2520supplementary%2520videos%2520at%2520https%253A//hot-dance.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reframing%20Music-Driven%202D%20Dance%20Pose%20Generation%20as%20Multi-Channel%20Image%20Generation&entry.906535625=Yan%20Zhang%20and%20Han%20Zou%20and%20Lincong%20Feng%20and%20Cong%20Xie%20and%20Ruiqi%20Yu%20and%20Zhenpeng%20Zhan&entry.1292438233=Recent%20pose-to-video%20models%20can%20translate%202D%20pose%20sequences%20into%20photorealistic%2C%20identity-preserving%20dance%20videos%2C%20so%20the%20key%20challenge%20is%20to%20generate%20temporally%20coherent%2C%20rhythm-aligned%202D%20poses%20from%20music%2C%20especially%20under%20complex%2C%20high-variance%20in-the-wild%20distributions.%20We%20address%20this%20by%20reframing%20music-to-dance%20generation%20as%20a%20music-token-conditioned%20multi-channel%20image%20synthesis%20problem%3A%202D%20pose%20sequences%20are%20encoded%20as%20one-hot%20images%2C%20compressed%20by%20a%20pretrained%20image%20VAE%2C%20and%20modeled%20with%20a%20DiT-style%20backbone%2C%20allowing%20us%20to%20inherit%20architectural%20and%20training%20advances%20from%20modern%20text-to-image%20models%20and%20better%20capture%20high-variance%202D%20pose%20distributions.%20On%20top%20of%20this%20formulation%2C%20we%20introduce%20%28i%29%20a%20time-shared%20temporal%20indexing%20scheme%20that%20explicitly%20synchronizes%20music%20tokens%20and%20pose%20latents%20over%20time%20and%20%28ii%29%20a%20reference-pose%20conditioning%20strategy%20that%20preserves%20subject-specific%20body%20proportions%20and%20on-screen%20scale%20while%20enabling%20long-horizon%20segment-and-stitch%20generation.%20Experiments%20on%20a%20large%20in-the-wild%202D%20dance%20corpus%20and%20the%20calibrated%20AIST%2B%2B2D%20benchmark%20show%20consistent%20improvements%20over%20representative%20music-to-dance%20methods%20in%20pose-%20and%20video-space%20metrics%20and%20human%20preference%2C%20and%20ablations%20validate%20the%20contributions%20of%20the%20representation%2C%20temporal%20indexing%2C%20and%20reference%20conditioning.%20See%20supplementary%20videos%20at%20https%3A//hot-dance.github.io&entry.1838667208=http%3A//arxiv.org/abs/2512.11720v1&entry.124074799=Read"},
{"title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance", "author": "Jan U. M\u00fcller and Robin Tim Landsgesell and Leif Van Holland and Patrick Stotko and Reinhard Klein", "abstract": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.", "link": "http://arxiv.org/abs/2512.11800v1", "date": "2025-12-12", "relevancy": 3.0667, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6565}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5926}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moment-Based%203D%20Gaussian%20Splatting%3A%20Resolving%20Volumetric%20Occlusion%20with%20Order-Independent%20Transmittance&body=Title%3A%20Moment-Based%203D%20Gaussian%20Splatting%3A%20Resolving%20Volumetric%20Occlusion%20with%20Order-Independent%20Transmittance%0AAuthor%3A%20Jan%20U.%20M%C3%BCller%20and%20Robin%20Tim%20Landsgesell%20and%20Leif%20Van%20Holland%20and%20Patrick%20Stotko%20and%20Reinhard%20Klein%0AAbstract%3A%20The%20recent%20success%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20reshaped%20novel%20view%20synthesis%20by%20enabling%20fast%20optimization%20and%20real-time%20rendering%20of%20high-quality%20radiance%20fields.%20However%2C%20it%20relies%20on%20simplified%2C%20order-dependent%20alpha%20blending%20and%20coarse%20approximations%20of%20the%20density%20integral%20within%20the%20rasterizer%2C%20thereby%20limiting%20its%20ability%20to%20render%20complex%2C%20overlapping%20semi-transparent%20objects.%20In%20this%20paper%2C%20we%20extend%20rasterization-based%20rendering%20of%203D%20Gaussian%20representations%20with%20a%20novel%20method%20for%20high-fidelity%20transmittance%20computation%2C%20entirely%20avoiding%20the%20need%20for%20ray%20tracing%20or%20per-pixel%20sample%20sorting.%20Building%20on%20prior%20work%20in%20moment-based%20order-independent%20transparency%2C%20our%20key%20idea%20is%20to%20characterize%20the%20density%20distribution%20along%20each%20camera%20ray%20with%20a%20compact%20and%20continuous%20representation%20based%20on%20statistical%20moments.%20To%20this%20end%2C%20we%20analytically%20derive%20and%20compute%20a%20set%20of%20per-pixel%20moments%20from%20all%20contributing%203D%20Gaussians.%20From%20these%20moments%2C%20a%20continuous%20transmittance%20function%20is%20reconstructed%20for%20each%20ray%2C%20which%20is%20then%20independently%20sampled%20within%20each%20Gaussian.%20As%20a%20result%2C%20our%20method%20bridges%20the%20gap%20between%20rasterization%20and%20physical%20accuracy%20by%20modeling%20light%20attenuation%20in%20complex%20translucent%20media%2C%20significantly%20improving%20overall%20reconstruction%20and%20rendering%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoment-Based%25203D%2520Gaussian%2520Splatting%253A%2520Resolving%2520Volumetric%2520Occlusion%2520with%2520Order-Independent%2520Transmittance%26entry.906535625%3DJan%2520U.%2520M%25C3%25BCller%2520and%2520Robin%2520Tim%2520Landsgesell%2520and%2520Leif%2520Van%2520Holland%2520and%2520Patrick%2520Stotko%2520and%2520Reinhard%2520Klein%26entry.1292438233%3DThe%2520recent%2520success%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520reshaped%2520novel%2520view%2520synthesis%2520by%2520enabling%2520fast%2520optimization%2520and%2520real-time%2520rendering%2520of%2520high-quality%2520radiance%2520fields.%2520However%252C%2520it%2520relies%2520on%2520simplified%252C%2520order-dependent%2520alpha%2520blending%2520and%2520coarse%2520approximations%2520of%2520the%2520density%2520integral%2520within%2520the%2520rasterizer%252C%2520thereby%2520limiting%2520its%2520ability%2520to%2520render%2520complex%252C%2520overlapping%2520semi-transparent%2520objects.%2520In%2520this%2520paper%252C%2520we%2520extend%2520rasterization-based%2520rendering%2520of%25203D%2520Gaussian%2520representations%2520with%2520a%2520novel%2520method%2520for%2520high-fidelity%2520transmittance%2520computation%252C%2520entirely%2520avoiding%2520the%2520need%2520for%2520ray%2520tracing%2520or%2520per-pixel%2520sample%2520sorting.%2520Building%2520on%2520prior%2520work%2520in%2520moment-based%2520order-independent%2520transparency%252C%2520our%2520key%2520idea%2520is%2520to%2520characterize%2520the%2520density%2520distribution%2520along%2520each%2520camera%2520ray%2520with%2520a%2520compact%2520and%2520continuous%2520representation%2520based%2520on%2520statistical%2520moments.%2520To%2520this%2520end%252C%2520we%2520analytically%2520derive%2520and%2520compute%2520a%2520set%2520of%2520per-pixel%2520moments%2520from%2520all%2520contributing%25203D%2520Gaussians.%2520From%2520these%2520moments%252C%2520a%2520continuous%2520transmittance%2520function%2520is%2520reconstructed%2520for%2520each%2520ray%252C%2520which%2520is%2520then%2520independently%2520sampled%2520within%2520each%2520Gaussian.%2520As%2520a%2520result%252C%2520our%2520method%2520bridges%2520the%2520gap%2520between%2520rasterization%2520and%2520physical%2520accuracy%2520by%2520modeling%2520light%2520attenuation%2520in%2520complex%2520translucent%2520media%252C%2520significantly%2520improving%2520overall%2520reconstruction%2520and%2520rendering%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moment-Based%203D%20Gaussian%20Splatting%3A%20Resolving%20Volumetric%20Occlusion%20with%20Order-Independent%20Transmittance&entry.906535625=Jan%20U.%20M%C3%BCller%20and%20Robin%20Tim%20Landsgesell%20and%20Leif%20Van%20Holland%20and%20Patrick%20Stotko%20and%20Reinhard%20Klein&entry.1292438233=The%20recent%20success%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20reshaped%20novel%20view%20synthesis%20by%20enabling%20fast%20optimization%20and%20real-time%20rendering%20of%20high-quality%20radiance%20fields.%20However%2C%20it%20relies%20on%20simplified%2C%20order-dependent%20alpha%20blending%20and%20coarse%20approximations%20of%20the%20density%20integral%20within%20the%20rasterizer%2C%20thereby%20limiting%20its%20ability%20to%20render%20complex%2C%20overlapping%20semi-transparent%20objects.%20In%20this%20paper%2C%20we%20extend%20rasterization-based%20rendering%20of%203D%20Gaussian%20representations%20with%20a%20novel%20method%20for%20high-fidelity%20transmittance%20computation%2C%20entirely%20avoiding%20the%20need%20for%20ray%20tracing%20or%20per-pixel%20sample%20sorting.%20Building%20on%20prior%20work%20in%20moment-based%20order-independent%20transparency%2C%20our%20key%20idea%20is%20to%20characterize%20the%20density%20distribution%20along%20each%20camera%20ray%20with%20a%20compact%20and%20continuous%20representation%20based%20on%20statistical%20moments.%20To%20this%20end%2C%20we%20analytically%20derive%20and%20compute%20a%20set%20of%20per-pixel%20moments%20from%20all%20contributing%203D%20Gaussians.%20From%20these%20moments%2C%20a%20continuous%20transmittance%20function%20is%20reconstructed%20for%20each%20ray%2C%20which%20is%20then%20independently%20sampled%20within%20each%20Gaussian.%20As%20a%20result%2C%20our%20method%20bridges%20the%20gap%20between%20rasterization%20and%20physical%20accuracy%20by%20modeling%20light%20attenuation%20in%20complex%20translucent%20media%2C%20significantly%20improving%20overall%20reconstruction%20and%20rendering%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2512.11800v1&entry.124074799=Read"},
{"title": "SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction", "author": "Lukas Radl and Felix Windisch and Thomas Deixelberger and Jozef Hladky and Michael Steiner and Dieter Schmalstieg and Markus Steinberger", "abstract": "Recent advances in 3D Gaussian representations have significantly improved the quality and efficiency of image-based scene reconstruction. Their explicit nature facilitates real-time rendering and fast optimization, yet extracting accurate surfaces - particularly in large-scale, unbounded environments - remains a difficult task. Many existing methods rely on approximate depth estimates and global sorting heuristics, which can introduce artifacts and limit the fidelity of the reconstructed mesh. In this paper, we present Sorted Opacity Fields (SOF), a method designed to recover detailed surfaces from 3D Gaussians with both speed and precision. Our approach improves upon prior work by introducing hierarchical resorting and a robust formulation of Gaussian depth, which better aligns with the level-set. To enhance mesh quality, we incorporate a level-set regularizer operating on the opacity field and introduce losses that encourage geometrically-consistent primitive shapes. In addition, we develop a parallelized Marching Tetrahedra algorithm tailored to our opacity formulation, reducing meshing time by up to an order of magnitude. As demonstrated by our quantitative evaluation, SOF achieves higher reconstruction accuracy while cutting total processing time by more than a factor of three. These results mark a step forward in turning efficient Gaussian-based rendering into equally efficient geometry extraction.", "link": "http://arxiv.org/abs/2506.19139v2", "date": "2025-12-12", "relevancy": 2.9924, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6359}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6114}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOF%3A%20Sorted%20Opacity%20Fields%20for%20Fast%20Unbounded%20Surface%20Reconstruction&body=Title%3A%20SOF%3A%20Sorted%20Opacity%20Fields%20for%20Fast%20Unbounded%20Surface%20Reconstruction%0AAuthor%3A%20Lukas%20Radl%20and%20Felix%20Windisch%20and%20Thomas%20Deixelberger%20and%20Jozef%20Hladky%20and%20Michael%20Steiner%20and%20Dieter%20Schmalstieg%20and%20Markus%20Steinberger%0AAbstract%3A%20Recent%20advances%20in%203D%20Gaussian%20representations%20have%20significantly%20improved%20the%20quality%20and%20efficiency%20of%20image-based%20scene%20reconstruction.%20Their%20explicit%20nature%20facilitates%20real-time%20rendering%20and%20fast%20optimization%2C%20yet%20extracting%20accurate%20surfaces%20-%20particularly%20in%20large-scale%2C%20unbounded%20environments%20-%20remains%20a%20difficult%20task.%20Many%20existing%20methods%20rely%20on%20approximate%20depth%20estimates%20and%20global%20sorting%20heuristics%2C%20which%20can%20introduce%20artifacts%20and%20limit%20the%20fidelity%20of%20the%20reconstructed%20mesh.%20In%20this%20paper%2C%20we%20present%20Sorted%20Opacity%20Fields%20%28SOF%29%2C%20a%20method%20designed%20to%20recover%20detailed%20surfaces%20from%203D%20Gaussians%20with%20both%20speed%20and%20precision.%20Our%20approach%20improves%20upon%20prior%20work%20by%20introducing%20hierarchical%20resorting%20and%20a%20robust%20formulation%20of%20Gaussian%20depth%2C%20which%20better%20aligns%20with%20the%20level-set.%20To%20enhance%20mesh%20quality%2C%20we%20incorporate%20a%20level-set%20regularizer%20operating%20on%20the%20opacity%20field%20and%20introduce%20losses%20that%20encourage%20geometrically-consistent%20primitive%20shapes.%20In%20addition%2C%20we%20develop%20a%20parallelized%20Marching%20Tetrahedra%20algorithm%20tailored%20to%20our%20opacity%20formulation%2C%20reducing%20meshing%20time%20by%20up%20to%20an%20order%20of%20magnitude.%20As%20demonstrated%20by%20our%20quantitative%20evaluation%2C%20SOF%20achieves%20higher%20reconstruction%20accuracy%20while%20cutting%20total%20processing%20time%20by%20more%20than%20a%20factor%20of%20three.%20These%20results%20mark%20a%20step%20forward%20in%20turning%20efficient%20Gaussian-based%20rendering%20into%20equally%20efficient%20geometry%20extraction.%0ALink%3A%20http%3A//arxiv.org/abs/2506.19139v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOF%253A%2520Sorted%2520Opacity%2520Fields%2520for%2520Fast%2520Unbounded%2520Surface%2520Reconstruction%26entry.906535625%3DLukas%2520Radl%2520and%2520Felix%2520Windisch%2520and%2520Thomas%2520Deixelberger%2520and%2520Jozef%2520Hladky%2520and%2520Michael%2520Steiner%2520and%2520Dieter%2520Schmalstieg%2520and%2520Markus%2520Steinberger%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520Gaussian%2520representations%2520have%2520significantly%2520improved%2520the%2520quality%2520and%2520efficiency%2520of%2520image-based%2520scene%2520reconstruction.%2520Their%2520explicit%2520nature%2520facilitates%2520real-time%2520rendering%2520and%2520fast%2520optimization%252C%2520yet%2520extracting%2520accurate%2520surfaces%2520-%2520particularly%2520in%2520large-scale%252C%2520unbounded%2520environments%2520-%2520remains%2520a%2520difficult%2520task.%2520Many%2520existing%2520methods%2520rely%2520on%2520approximate%2520depth%2520estimates%2520and%2520global%2520sorting%2520heuristics%252C%2520which%2520can%2520introduce%2520artifacts%2520and%2520limit%2520the%2520fidelity%2520of%2520the%2520reconstructed%2520mesh.%2520In%2520this%2520paper%252C%2520we%2520present%2520Sorted%2520Opacity%2520Fields%2520%2528SOF%2529%252C%2520a%2520method%2520designed%2520to%2520recover%2520detailed%2520surfaces%2520from%25203D%2520Gaussians%2520with%2520both%2520speed%2520and%2520precision.%2520Our%2520approach%2520improves%2520upon%2520prior%2520work%2520by%2520introducing%2520hierarchical%2520resorting%2520and%2520a%2520robust%2520formulation%2520of%2520Gaussian%2520depth%252C%2520which%2520better%2520aligns%2520with%2520the%2520level-set.%2520To%2520enhance%2520mesh%2520quality%252C%2520we%2520incorporate%2520a%2520level-set%2520regularizer%2520operating%2520on%2520the%2520opacity%2520field%2520and%2520introduce%2520losses%2520that%2520encourage%2520geometrically-consistent%2520primitive%2520shapes.%2520In%2520addition%252C%2520we%2520develop%2520a%2520parallelized%2520Marching%2520Tetrahedra%2520algorithm%2520tailored%2520to%2520our%2520opacity%2520formulation%252C%2520reducing%2520meshing%2520time%2520by%2520up%2520to%2520an%2520order%2520of%2520magnitude.%2520As%2520demonstrated%2520by%2520our%2520quantitative%2520evaluation%252C%2520SOF%2520achieves%2520higher%2520reconstruction%2520accuracy%2520while%2520cutting%2520total%2520processing%2520time%2520by%2520more%2520than%2520a%2520factor%2520of%2520three.%2520These%2520results%2520mark%2520a%2520step%2520forward%2520in%2520turning%2520efficient%2520Gaussian-based%2520rendering%2520into%2520equally%2520efficient%2520geometry%2520extraction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19139v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOF%3A%20Sorted%20Opacity%20Fields%20for%20Fast%20Unbounded%20Surface%20Reconstruction&entry.906535625=Lukas%20Radl%20and%20Felix%20Windisch%20and%20Thomas%20Deixelberger%20and%20Jozef%20Hladky%20and%20Michael%20Steiner%20and%20Dieter%20Schmalstieg%20and%20Markus%20Steinberger&entry.1292438233=Recent%20advances%20in%203D%20Gaussian%20representations%20have%20significantly%20improved%20the%20quality%20and%20efficiency%20of%20image-based%20scene%20reconstruction.%20Their%20explicit%20nature%20facilitates%20real-time%20rendering%20and%20fast%20optimization%2C%20yet%20extracting%20accurate%20surfaces%20-%20particularly%20in%20large-scale%2C%20unbounded%20environments%20-%20remains%20a%20difficult%20task.%20Many%20existing%20methods%20rely%20on%20approximate%20depth%20estimates%20and%20global%20sorting%20heuristics%2C%20which%20can%20introduce%20artifacts%20and%20limit%20the%20fidelity%20of%20the%20reconstructed%20mesh.%20In%20this%20paper%2C%20we%20present%20Sorted%20Opacity%20Fields%20%28SOF%29%2C%20a%20method%20designed%20to%20recover%20detailed%20surfaces%20from%203D%20Gaussians%20with%20both%20speed%20and%20precision.%20Our%20approach%20improves%20upon%20prior%20work%20by%20introducing%20hierarchical%20resorting%20and%20a%20robust%20formulation%20of%20Gaussian%20depth%2C%20which%20better%20aligns%20with%20the%20level-set.%20To%20enhance%20mesh%20quality%2C%20we%20incorporate%20a%20level-set%20regularizer%20operating%20on%20the%20opacity%20field%20and%20introduce%20losses%20that%20encourage%20geometrically-consistent%20primitive%20shapes.%20In%20addition%2C%20we%20develop%20a%20parallelized%20Marching%20Tetrahedra%20algorithm%20tailored%20to%20our%20opacity%20formulation%2C%20reducing%20meshing%20time%20by%20up%20to%20an%20order%20of%20magnitude.%20As%20demonstrated%20by%20our%20quantitative%20evaluation%2C%20SOF%20achieves%20higher%20reconstruction%20accuracy%20while%20cutting%20total%20processing%20time%20by%20more%20than%20a%20factor%20of%20three.%20These%20results%20mark%20a%20step%20forward%20in%20turning%20efficient%20Gaussian-based%20rendering%20into%20equally%20efficient%20geometry%20extraction.&entry.1838667208=http%3A//arxiv.org/abs/2506.19139v2&entry.124074799=Read"},
{"title": "Particulate: Feed-Forward 3D Object Articulation", "author": "Ruining Li and Yuxin Yao and Chuanxia Zheng and Christian Rupprecht and Joan Lasenby and Shangzhe Wu and Andrea Vedaldi", "abstract": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.", "link": "http://arxiv.org/abs/2512.11798v1", "date": "2025-12-12", "relevancy": 2.9509, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6226}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.574}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Particulate%3A%20Feed-Forward%203D%20Object%20Articulation&body=Title%3A%20Particulate%3A%20Feed-Forward%203D%20Object%20Articulation%0AAuthor%3A%20Ruining%20Li%20and%20Yuxin%20Yao%20and%20Chuanxia%20Zheng%20and%20Christian%20Rupprecht%20and%20Joan%20Lasenby%20and%20Shangzhe%20Wu%20and%20Andrea%20Vedaldi%0AAbstract%3A%20We%20present%20Particulate%2C%20a%20feed-forward%20approach%20that%2C%20given%20a%20single%20static%203D%20mesh%20of%20an%20everyday%20object%2C%20directly%20infers%20all%20attributes%20of%20the%20underlying%20articulated%20structure%2C%20including%20its%203D%20parts%2C%20kinematic%20structure%2C%20and%20motion%20constraints.%20At%20its%20core%20is%20a%20transformer%20network%2C%20Part%20Articulation%20Transformer%2C%20which%20processes%20a%20point%20cloud%20of%20the%20input%20mesh%20using%20a%20flexible%20and%20scalable%20architecture%20to%20predict%20all%20the%20aforementioned%20attributes%20with%20native%20multi-joint%20support.%20We%20train%20the%20network%20end-to-end%20on%20a%20diverse%20collection%20of%20articulated%203D%20assets%20from%20public%20datasets.%20During%20inference%2C%20Particulate%20lifts%20the%20network%27s%20feed-forward%20prediction%20to%20the%20input%20mesh%2C%20yielding%20a%20fully%20articulated%203D%20model%20in%20seconds%2C%20much%20faster%20than%20prior%20approaches%20that%20require%20per-object%20optimization.%20Particulate%20can%20also%20accurately%20infer%20the%20articulated%20structure%20of%20AI-generated%203D%20assets%2C%20enabling%20full-fledged%20extraction%20of%20articulated%203D%20objects%20from%20a%20single%20%28real%20or%20synthetic%29%20image%20when%20combined%20with%20an%20off-the-shelf%20image-to-3D%20generator.%20We%20further%20introduce%20a%20new%20challenging%20benchmark%20for%203D%20articulation%20estimation%20curated%20from%20high-quality%20public%203D%20assets%2C%20and%20redesign%20the%20evaluation%20protocol%20to%20be%20more%20consistent%20with%20human%20preferences.%20Quantitative%20and%20qualitative%20results%20show%20that%20Particulate%20significantly%20outperforms%20state-of-the-art%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParticulate%253A%2520Feed-Forward%25203D%2520Object%2520Articulation%26entry.906535625%3DRuining%2520Li%2520and%2520Yuxin%2520Yao%2520and%2520Chuanxia%2520Zheng%2520and%2520Christian%2520Rupprecht%2520and%2520Joan%2520Lasenby%2520and%2520Shangzhe%2520Wu%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3DWe%2520present%2520Particulate%252C%2520a%2520feed-forward%2520approach%2520that%252C%2520given%2520a%2520single%2520static%25203D%2520mesh%2520of%2520an%2520everyday%2520object%252C%2520directly%2520infers%2520all%2520attributes%2520of%2520the%2520underlying%2520articulated%2520structure%252C%2520including%2520its%25203D%2520parts%252C%2520kinematic%2520structure%252C%2520and%2520motion%2520constraints.%2520At%2520its%2520core%2520is%2520a%2520transformer%2520network%252C%2520Part%2520Articulation%2520Transformer%252C%2520which%2520processes%2520a%2520point%2520cloud%2520of%2520the%2520input%2520mesh%2520using%2520a%2520flexible%2520and%2520scalable%2520architecture%2520to%2520predict%2520all%2520the%2520aforementioned%2520attributes%2520with%2520native%2520multi-joint%2520support.%2520We%2520train%2520the%2520network%2520end-to-end%2520on%2520a%2520diverse%2520collection%2520of%2520articulated%25203D%2520assets%2520from%2520public%2520datasets.%2520During%2520inference%252C%2520Particulate%2520lifts%2520the%2520network%2527s%2520feed-forward%2520prediction%2520to%2520the%2520input%2520mesh%252C%2520yielding%2520a%2520fully%2520articulated%25203D%2520model%2520in%2520seconds%252C%2520much%2520faster%2520than%2520prior%2520approaches%2520that%2520require%2520per-object%2520optimization.%2520Particulate%2520can%2520also%2520accurately%2520infer%2520the%2520articulated%2520structure%2520of%2520AI-generated%25203D%2520assets%252C%2520enabling%2520full-fledged%2520extraction%2520of%2520articulated%25203D%2520objects%2520from%2520a%2520single%2520%2528real%2520or%2520synthetic%2529%2520image%2520when%2520combined%2520with%2520an%2520off-the-shelf%2520image-to-3D%2520generator.%2520We%2520further%2520introduce%2520a%2520new%2520challenging%2520benchmark%2520for%25203D%2520articulation%2520estimation%2520curated%2520from%2520high-quality%2520public%25203D%2520assets%252C%2520and%2520redesign%2520the%2520evaluation%2520protocol%2520to%2520be%2520more%2520consistent%2520with%2520human%2520preferences.%2520Quantitative%2520and%2520qualitative%2520results%2520show%2520that%2520Particulate%2520significantly%2520outperforms%2520state-of-the-art%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Particulate%3A%20Feed-Forward%203D%20Object%20Articulation&entry.906535625=Ruining%20Li%20and%20Yuxin%20Yao%20and%20Chuanxia%20Zheng%20and%20Christian%20Rupprecht%20and%20Joan%20Lasenby%20and%20Shangzhe%20Wu%20and%20Andrea%20Vedaldi&entry.1292438233=We%20present%20Particulate%2C%20a%20feed-forward%20approach%20that%2C%20given%20a%20single%20static%203D%20mesh%20of%20an%20everyday%20object%2C%20directly%20infers%20all%20attributes%20of%20the%20underlying%20articulated%20structure%2C%20including%20its%203D%20parts%2C%20kinematic%20structure%2C%20and%20motion%20constraints.%20At%20its%20core%20is%20a%20transformer%20network%2C%20Part%20Articulation%20Transformer%2C%20which%20processes%20a%20point%20cloud%20of%20the%20input%20mesh%20using%20a%20flexible%20and%20scalable%20architecture%20to%20predict%20all%20the%20aforementioned%20attributes%20with%20native%20multi-joint%20support.%20We%20train%20the%20network%20end-to-end%20on%20a%20diverse%20collection%20of%20articulated%203D%20assets%20from%20public%20datasets.%20During%20inference%2C%20Particulate%20lifts%20the%20network%27s%20feed-forward%20prediction%20to%20the%20input%20mesh%2C%20yielding%20a%20fully%20articulated%203D%20model%20in%20seconds%2C%20much%20faster%20than%20prior%20approaches%20that%20require%20per-object%20optimization.%20Particulate%20can%20also%20accurately%20infer%20the%20articulated%20structure%20of%20AI-generated%203D%20assets%2C%20enabling%20full-fledged%20extraction%20of%20articulated%203D%20objects%20from%20a%20single%20%28real%20or%20synthetic%29%20image%20when%20combined%20with%20an%20off-the-shelf%20image-to-3D%20generator.%20We%20further%20introduce%20a%20new%20challenging%20benchmark%20for%203D%20articulation%20estimation%20curated%20from%20high-quality%20public%203D%20assets%2C%20and%20redesign%20the%20evaluation%20protocol%20to%20be%20more%20consistent%20with%20human%20preferences.%20Quantitative%20and%20qualitative%20results%20show%20that%20Particulate%20significantly%20outperforms%20state-of-the-art%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2512.11798v1&entry.124074799=Read"},
{"title": "VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing", "author": "Emanuel S\u00e1nchez Aimar and Gulnaz Zhambulova and Fahad Shahbaz Khan and Yonghao Xu and Michael Felsberg", "abstract": "Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.", "link": "http://arxiv.org/abs/2512.11490v1", "date": "2025-12-12", "relevancy": 2.9177, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM2GeoVec%3A%20Toward%20Universal%20Multimodal%20Embeddings%20for%20Remote%20Sensing&body=Title%3A%20VLM2GeoVec%3A%20Toward%20Universal%20Multimodal%20Embeddings%20for%20Remote%20Sensing%0AAuthor%3A%20Emanuel%20S%C3%A1nchez%20Aimar%20and%20Gulnaz%20Zhambulova%20and%20Fahad%20Shahbaz%20Khan%20and%20Yonghao%20Xu%20and%20Michael%20Felsberg%0AAbstract%3A%20Satellite%20imagery%20differs%20fundamentally%20from%20natural%20images%3A%20its%20aerial%20viewpoint%2C%20very%20high%20resolution%2C%20diverse%20scale%20variations%2C%20and%20abundance%20of%20small%20objects%20demand%20both%20region-level%20spatial%20reasoning%20and%20holistic%20scene%20understanding.%20Current%20remote-sensing%20approaches%20remain%20fragmented%20between%20dual-encoder%20retrieval%20models%2C%20which%20excel%20at%20large-scale%20cross-modal%20search%20but%20cannot%20interleave%20modalities%2C%20and%20generative%20assistants%2C%20which%20support%20region-level%20interpretation%20but%20lack%20scalable%20retrieval%20capabilities.%20We%20propose%20%24%5Ctextbf%7BVLM2GeoVec%7D%24%2C%20an%20instruction-following%2C%20single-encoder%20vision-language%20model%20trained%20contrastively%20to%20embed%20interleaved%20inputs%20%28images%2C%20text%2C%20bounding%20boxes%2C%20and%20geographic%20coordinates%29%20in%20a%20unified%20vector%20space.%20Our%20single%20encoder%20interleaves%20all%20inputs%20into%20one%20joint%20embedding%20trained%20with%20a%20contrastive%20loss%2C%20eliminating%20multi-stage%20pipelines%20and%20task-specific%20modules.%20To%20evaluate%20its%20versatility%2C%20we%20introduce%20%24%5Ctextbf%7BRSMEB%7D%24%2C%20a%20novel%20benchmark%20covering%20key%20remote-sensing%20embedding%20applications%3A%20scene%20classification%3B%20cross-modal%20search%3B%20compositional%20retrieval%3B%20visual-question%20answering%3B%20visual%20grounding%20and%20region-level%20reasoning%3B%20and%20semantic%20geospatial%20retrieval.%20On%20RSMEB%2C%20it%20achieves%20%24%5Ctextbf%7B26.6%25%7D%24%20P%401%20on%20region-caption%20retrieval%20%28%2B25%20pp%20vs.%20dual-encoder%20baselines%29%2C%20%24%5Ctextbf%7B32.5%25%7D%24%20P%401%20on%20referring-expression%20retrieval%20%28%2B19%20pp%29%2C%20and%20%24%5Ctextbf%7B17.8%25%7D%24%20P%401%20on%20semantic%20geo-localization%20retrieval%20%28over%20%243%5Ctimes%24%20prior%20best%29%2C%20while%20matching%20or%20exceeding%20specialized%20baselines%20on%20conventional%20tasks%20such%20as%20scene%20classification%20and%20cross-modal%20retrieval.%20VLM2GeoVec%20unifies%20scalable%20retrieval%20with%20region-level%20spatial%20reasoning%2C%20enabling%20cohesive%20multimodal%20analysis%20in%20remote%20sensing.%20We%20will%20publicly%20release%20the%20code%2C%20checkpoints%2C%20and%20data%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM2GeoVec%253A%2520Toward%2520Universal%2520Multimodal%2520Embeddings%2520for%2520Remote%2520Sensing%26entry.906535625%3DEmanuel%2520S%25C3%25A1nchez%2520Aimar%2520and%2520Gulnaz%2520Zhambulova%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Yonghao%2520Xu%2520and%2520Michael%2520Felsberg%26entry.1292438233%3DSatellite%2520imagery%2520differs%2520fundamentally%2520from%2520natural%2520images%253A%2520its%2520aerial%2520viewpoint%252C%2520very%2520high%2520resolution%252C%2520diverse%2520scale%2520variations%252C%2520and%2520abundance%2520of%2520small%2520objects%2520demand%2520both%2520region-level%2520spatial%2520reasoning%2520and%2520holistic%2520scene%2520understanding.%2520Current%2520remote-sensing%2520approaches%2520remain%2520fragmented%2520between%2520dual-encoder%2520retrieval%2520models%252C%2520which%2520excel%2520at%2520large-scale%2520cross-modal%2520search%2520but%2520cannot%2520interleave%2520modalities%252C%2520and%2520generative%2520assistants%252C%2520which%2520support%2520region-level%2520interpretation%2520but%2520lack%2520scalable%2520retrieval%2520capabilities.%2520We%2520propose%2520%2524%255Ctextbf%257BVLM2GeoVec%257D%2524%252C%2520an%2520instruction-following%252C%2520single-encoder%2520vision-language%2520model%2520trained%2520contrastively%2520to%2520embed%2520interleaved%2520inputs%2520%2528images%252C%2520text%252C%2520bounding%2520boxes%252C%2520and%2520geographic%2520coordinates%2529%2520in%2520a%2520unified%2520vector%2520space.%2520Our%2520single%2520encoder%2520interleaves%2520all%2520inputs%2520into%2520one%2520joint%2520embedding%2520trained%2520with%2520a%2520contrastive%2520loss%252C%2520eliminating%2520multi-stage%2520pipelines%2520and%2520task-specific%2520modules.%2520To%2520evaluate%2520its%2520versatility%252C%2520we%2520introduce%2520%2524%255Ctextbf%257BRSMEB%257D%2524%252C%2520a%2520novel%2520benchmark%2520covering%2520key%2520remote-sensing%2520embedding%2520applications%253A%2520scene%2520classification%253B%2520cross-modal%2520search%253B%2520compositional%2520retrieval%253B%2520visual-question%2520answering%253B%2520visual%2520grounding%2520and%2520region-level%2520reasoning%253B%2520and%2520semantic%2520geospatial%2520retrieval.%2520On%2520RSMEB%252C%2520it%2520achieves%2520%2524%255Ctextbf%257B26.6%2525%257D%2524%2520P%25401%2520on%2520region-caption%2520retrieval%2520%2528%252B25%2520pp%2520vs.%2520dual-encoder%2520baselines%2529%252C%2520%2524%255Ctextbf%257B32.5%2525%257D%2524%2520P%25401%2520on%2520referring-expression%2520retrieval%2520%2528%252B19%2520pp%2529%252C%2520and%2520%2524%255Ctextbf%257B17.8%2525%257D%2524%2520P%25401%2520on%2520semantic%2520geo-localization%2520retrieval%2520%2528over%2520%25243%255Ctimes%2524%2520prior%2520best%2529%252C%2520while%2520matching%2520or%2520exceeding%2520specialized%2520baselines%2520on%2520conventional%2520tasks%2520such%2520as%2520scene%2520classification%2520and%2520cross-modal%2520retrieval.%2520VLM2GeoVec%2520unifies%2520scalable%2520retrieval%2520with%2520region-level%2520spatial%2520reasoning%252C%2520enabling%2520cohesive%2520multimodal%2520analysis%2520in%2520remote%2520sensing.%2520We%2520will%2520publicly%2520release%2520the%2520code%252C%2520checkpoints%252C%2520and%2520data%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM2GeoVec%3A%20Toward%20Universal%20Multimodal%20Embeddings%20for%20Remote%20Sensing&entry.906535625=Emanuel%20S%C3%A1nchez%20Aimar%20and%20Gulnaz%20Zhambulova%20and%20Fahad%20Shahbaz%20Khan%20and%20Yonghao%20Xu%20and%20Michael%20Felsberg&entry.1292438233=Satellite%20imagery%20differs%20fundamentally%20from%20natural%20images%3A%20its%20aerial%20viewpoint%2C%20very%20high%20resolution%2C%20diverse%20scale%20variations%2C%20and%20abundance%20of%20small%20objects%20demand%20both%20region-level%20spatial%20reasoning%20and%20holistic%20scene%20understanding.%20Current%20remote-sensing%20approaches%20remain%20fragmented%20between%20dual-encoder%20retrieval%20models%2C%20which%20excel%20at%20large-scale%20cross-modal%20search%20but%20cannot%20interleave%20modalities%2C%20and%20generative%20assistants%2C%20which%20support%20region-level%20interpretation%20but%20lack%20scalable%20retrieval%20capabilities.%20We%20propose%20%24%5Ctextbf%7BVLM2GeoVec%7D%24%2C%20an%20instruction-following%2C%20single-encoder%20vision-language%20model%20trained%20contrastively%20to%20embed%20interleaved%20inputs%20%28images%2C%20text%2C%20bounding%20boxes%2C%20and%20geographic%20coordinates%29%20in%20a%20unified%20vector%20space.%20Our%20single%20encoder%20interleaves%20all%20inputs%20into%20one%20joint%20embedding%20trained%20with%20a%20contrastive%20loss%2C%20eliminating%20multi-stage%20pipelines%20and%20task-specific%20modules.%20To%20evaluate%20its%20versatility%2C%20we%20introduce%20%24%5Ctextbf%7BRSMEB%7D%24%2C%20a%20novel%20benchmark%20covering%20key%20remote-sensing%20embedding%20applications%3A%20scene%20classification%3B%20cross-modal%20search%3B%20compositional%20retrieval%3B%20visual-question%20answering%3B%20visual%20grounding%20and%20region-level%20reasoning%3B%20and%20semantic%20geospatial%20retrieval.%20On%20RSMEB%2C%20it%20achieves%20%24%5Ctextbf%7B26.6%25%7D%24%20P%401%20on%20region-caption%20retrieval%20%28%2B25%20pp%20vs.%20dual-encoder%20baselines%29%2C%20%24%5Ctextbf%7B32.5%25%7D%24%20P%401%20on%20referring-expression%20retrieval%20%28%2B19%20pp%29%2C%20and%20%24%5Ctextbf%7B17.8%25%7D%24%20P%401%20on%20semantic%20geo-localization%20retrieval%20%28over%20%243%5Ctimes%24%20prior%20best%29%2C%20while%20matching%20or%20exceeding%20specialized%20baselines%20on%20conventional%20tasks%20such%20as%20scene%20classification%20and%20cross-modal%20retrieval.%20VLM2GeoVec%20unifies%20scalable%20retrieval%20with%20region-level%20spatial%20reasoning%2C%20enabling%20cohesive%20multimodal%20analysis%20in%20remote%20sensing.%20We%20will%20publicly%20release%20the%20code%2C%20checkpoints%2C%20and%20data%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2512.11490v1&entry.124074799=Read"},
{"title": "3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation", "author": "Zhiguo Lu and Jianwen Lou and Mingjun Ma and Hairong Jin and Youyi Zheng and Kun Zhou", "abstract": "3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.", "link": "http://arxiv.org/abs/2512.11557v1", "date": "2025-12-12", "relevancy": 2.8718, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6185}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5523}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DTeethSAM%3A%20Taming%20SAM2%20for%203D%20Teeth%20Segmentation&body=Title%3A%203DTeethSAM%3A%20Taming%20SAM2%20for%203D%20Teeth%20Segmentation%0AAuthor%3A%20Zhiguo%20Lu%20and%20Jianwen%20Lou%20and%20Mingjun%20Ma%20and%20Hairong%20Jin%20and%20Youyi%20Zheng%20and%20Kun%20Zhou%0AAbstract%3A%203D%20teeth%20segmentation%2C%20involving%20the%20localization%20of%20tooth%20instances%20and%20their%20semantic%20categorization%20in%203D%20dental%20models%2C%20is%20a%20critical%20yet%20challenging%20task%20in%20digital%20dentistry%20due%20to%20the%20complexity%20of%20real-world%20dentition.%20In%20this%20paper%2C%20we%20propose%203DTeethSAM%2C%20an%20adaptation%20of%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20for%203D%20teeth%20segmentation.%20SAM2%20is%20a%20pretrained%20foundation%20model%20for%20image%20and%20video%20segmentation%2C%20demonstrating%20a%20strong%20backbone%20in%20various%20downstream%20scenarios.%20To%20adapt%20SAM2%20for%203D%20teeth%20data%2C%20we%20render%20images%20of%203D%20teeth%20models%20from%20predefined%20views%2C%20apply%20SAM2%20for%202D%20segmentation%2C%20and%20reconstruct%203D%20results%20using%202D-3D%20projections.%20Since%20SAM2%27s%20performance%20depends%20on%20input%20prompts%20and%20its%20initial%20outputs%20often%20have%20deficiencies%2C%20and%20given%20its%20class-agnostic%20nature%2C%20we%20introduce%20three%20light-weight%20learnable%20modules%3A%20%281%29%20a%20prompt%20embedding%20generator%20to%20derive%20prompt%20embeddings%20from%20image%20embeddings%20for%20accurate%20mask%20decoding%2C%20%282%29%20a%20mask%20refiner%20to%20enhance%20SAM2%27s%20initial%20segmentation%20results%2C%20and%20%283%29%20a%20mask%20classifier%20to%20categorize%20the%20generated%20masks.%20Additionally%2C%20we%20incorporate%20Deformable%20Global%20Attention%20Plugins%20%28DGAP%29%20into%20SAM2%27s%20image%20encoder.%20The%20DGAP%20enhances%20both%20the%20segmentation%20accuracy%20and%20the%20speed%20of%20the%20training%20process.%20Our%20method%20has%20been%20validated%20on%20the%203DTeethSeg%20benchmark%2C%20achieving%20an%20IoU%20of%2091.90%25%20on%20high-resolution%203D%20teeth%20meshes%2C%20establishing%20a%20new%20state-of-the-art%20in%20the%20field.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DTeethSAM%253A%2520Taming%2520SAM2%2520for%25203D%2520Teeth%2520Segmentation%26entry.906535625%3DZhiguo%2520Lu%2520and%2520Jianwen%2520Lou%2520and%2520Mingjun%2520Ma%2520and%2520Hairong%2520Jin%2520and%2520Youyi%2520Zheng%2520and%2520Kun%2520Zhou%26entry.1292438233%3D3D%2520teeth%2520segmentation%252C%2520involving%2520the%2520localization%2520of%2520tooth%2520instances%2520and%2520their%2520semantic%2520categorization%2520in%25203D%2520dental%2520models%252C%2520is%2520a%2520critical%2520yet%2520challenging%2520task%2520in%2520digital%2520dentistry%2520due%2520to%2520the%2520complexity%2520of%2520real-world%2520dentition.%2520In%2520this%2520paper%252C%2520we%2520propose%25203DTeethSAM%252C%2520an%2520adaptation%2520of%2520the%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520for%25203D%2520teeth%2520segmentation.%2520SAM2%2520is%2520a%2520pretrained%2520foundation%2520model%2520for%2520image%2520and%2520video%2520segmentation%252C%2520demonstrating%2520a%2520strong%2520backbone%2520in%2520various%2520downstream%2520scenarios.%2520To%2520adapt%2520SAM2%2520for%25203D%2520teeth%2520data%252C%2520we%2520render%2520images%2520of%25203D%2520teeth%2520models%2520from%2520predefined%2520views%252C%2520apply%2520SAM2%2520for%25202D%2520segmentation%252C%2520and%2520reconstruct%25203D%2520results%2520using%25202D-3D%2520projections.%2520Since%2520SAM2%2527s%2520performance%2520depends%2520on%2520input%2520prompts%2520and%2520its%2520initial%2520outputs%2520often%2520have%2520deficiencies%252C%2520and%2520given%2520its%2520class-agnostic%2520nature%252C%2520we%2520introduce%2520three%2520light-weight%2520learnable%2520modules%253A%2520%25281%2529%2520a%2520prompt%2520embedding%2520generator%2520to%2520derive%2520prompt%2520embeddings%2520from%2520image%2520embeddings%2520for%2520accurate%2520mask%2520decoding%252C%2520%25282%2529%2520a%2520mask%2520refiner%2520to%2520enhance%2520SAM2%2527s%2520initial%2520segmentation%2520results%252C%2520and%2520%25283%2529%2520a%2520mask%2520classifier%2520to%2520categorize%2520the%2520generated%2520masks.%2520Additionally%252C%2520we%2520incorporate%2520Deformable%2520Global%2520Attention%2520Plugins%2520%2528DGAP%2529%2520into%2520SAM2%2527s%2520image%2520encoder.%2520The%2520DGAP%2520enhances%2520both%2520the%2520segmentation%2520accuracy%2520and%2520the%2520speed%2520of%2520the%2520training%2520process.%2520Our%2520method%2520has%2520been%2520validated%2520on%2520the%25203DTeethSeg%2520benchmark%252C%2520achieving%2520an%2520IoU%2520of%252091.90%2525%2520on%2520high-resolution%25203D%2520teeth%2520meshes%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520in%2520the%2520field.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DTeethSAM%3A%20Taming%20SAM2%20for%203D%20Teeth%20Segmentation&entry.906535625=Zhiguo%20Lu%20and%20Jianwen%20Lou%20and%20Mingjun%20Ma%20and%20Hairong%20Jin%20and%20Youyi%20Zheng%20and%20Kun%20Zhou&entry.1292438233=3D%20teeth%20segmentation%2C%20involving%20the%20localization%20of%20tooth%20instances%20and%20their%20semantic%20categorization%20in%203D%20dental%20models%2C%20is%20a%20critical%20yet%20challenging%20task%20in%20digital%20dentistry%20due%20to%20the%20complexity%20of%20real-world%20dentition.%20In%20this%20paper%2C%20we%20propose%203DTeethSAM%2C%20an%20adaptation%20of%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20for%203D%20teeth%20segmentation.%20SAM2%20is%20a%20pretrained%20foundation%20model%20for%20image%20and%20video%20segmentation%2C%20demonstrating%20a%20strong%20backbone%20in%20various%20downstream%20scenarios.%20To%20adapt%20SAM2%20for%203D%20teeth%20data%2C%20we%20render%20images%20of%203D%20teeth%20models%20from%20predefined%20views%2C%20apply%20SAM2%20for%202D%20segmentation%2C%20and%20reconstruct%203D%20results%20using%202D-3D%20projections.%20Since%20SAM2%27s%20performance%20depends%20on%20input%20prompts%20and%20its%20initial%20outputs%20often%20have%20deficiencies%2C%20and%20given%20its%20class-agnostic%20nature%2C%20we%20introduce%20three%20light-weight%20learnable%20modules%3A%20%281%29%20a%20prompt%20embedding%20generator%20to%20derive%20prompt%20embeddings%20from%20image%20embeddings%20for%20accurate%20mask%20decoding%2C%20%282%29%20a%20mask%20refiner%20to%20enhance%20SAM2%27s%20initial%20segmentation%20results%2C%20and%20%283%29%20a%20mask%20classifier%20to%20categorize%20the%20generated%20masks.%20Additionally%2C%20we%20incorporate%20Deformable%20Global%20Attention%20Plugins%20%28DGAP%29%20into%20SAM2%27s%20image%20encoder.%20The%20DGAP%20enhances%20both%20the%20segmentation%20accuracy%20and%20the%20speed%20of%20the%20training%20process.%20Our%20method%20has%20been%20validated%20on%20the%203DTeethSeg%20benchmark%2C%20achieving%20an%20IoU%20of%2091.90%25%20on%20high-resolution%203D%20teeth%20meshes%2C%20establishing%20a%20new%20state-of-the-art%20in%20the%20field.&entry.1838667208=http%3A//arxiv.org/abs/2512.11557v1&entry.124074799=Read"},
{"title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models", "author": "Ying Cheng and Yu-Ho Lin and Min-Hung Chen and Fu-En Yang and Shang-Hong Lai", "abstract": "Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.", "link": "http://arxiv.org/abs/2511.07299v2", "date": "2025-12-12", "relevancy": 2.8679, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VADER%3A%20Towards%20Causal%20Video%20Anomaly%20Understanding%20with%20Relation-Aware%20Large%20Language%20Models&body=Title%3A%20VADER%3A%20Towards%20Causal%20Video%20Anomaly%20Understanding%20with%20Relation-Aware%20Large%20Language%20Models%0AAuthor%3A%20Ying%20Cheng%20and%20Yu-Ho%20Lin%20and%20Min-Hung%20Chen%20and%20Fu-En%20Yang%20and%20Shang-Hong%20Lai%0AAbstract%3A%20Video%20anomaly%20understanding%20%28VAU%29%20aims%20to%20provide%20detailed%20interpretation%20and%20semantic%20comprehension%20of%20anomalous%20events%20within%20videos%2C%20addressing%20limitations%20of%20traditional%20methods%20that%20focus%20solely%20on%20detecting%20and%20localizing%20anomalies.%20However%2C%20existing%20approaches%20often%20neglect%20the%20deeper%20causal%20relationships%20and%20interactions%20between%20objects%2C%20which%20are%20critical%20for%20understanding%20anomalous%20behaviors.%20In%20this%20paper%2C%20we%20propose%20VADER%2C%20an%20LLM-driven%20framework%20for%20Video%20Anomaly%20unDErstanding%2C%20which%20integrates%20keyframe%20object%20Relation%20features%20with%20visual%20cues%20to%20enhance%20anomaly%20comprehension%20from%20video.%20Specifically%2C%20VADER%20first%20applies%20an%20Anomaly%20Scorer%20to%20assign%20per-frame%20anomaly%20scores%2C%20followed%20by%20a%20Context-AwarE%20Sampling%20%28CAES%29%20strategy%20to%20capture%20the%20causal%20context%20of%20each%20anomalous%20event.%20A%20Relation%20Feature%20Extractor%20and%20a%20COntrastive%20Relation%20Encoder%20%28CORE%29%20jointly%20model%20dynamic%20object%20interactions%2C%20producing%20compact%20relational%20representations%20for%20downstream%20reasoning.%20These%20visual%20and%20relational%20cues%20are%20integrated%20with%20LLMs%20to%20generate%20detailed%2C%20causally%20grounded%20descriptions%20and%20support%20robust%20anomaly-related%20question%20answering.%20Experiments%20on%20multiple%20real-world%20VAU%20benchmarks%20demonstrate%20that%20VADER%20achieves%20strong%20results%20across%20anomaly%20description%2C%20explanation%2C%20and%20causal%20reasoning%20tasks%2C%20advancing%20the%20frontier%20of%20explainable%20video%20anomaly%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVADER%253A%2520Towards%2520Causal%2520Video%2520Anomaly%2520Understanding%2520with%2520Relation-Aware%2520Large%2520Language%2520Models%26entry.906535625%3DYing%2520Cheng%2520and%2520Yu-Ho%2520Lin%2520and%2520Min-Hung%2520Chen%2520and%2520Fu-En%2520Yang%2520and%2520Shang-Hong%2520Lai%26entry.1292438233%3DVideo%2520anomaly%2520understanding%2520%2528VAU%2529%2520aims%2520to%2520provide%2520detailed%2520interpretation%2520and%2520semantic%2520comprehension%2520of%2520anomalous%2520events%2520within%2520videos%252C%2520addressing%2520limitations%2520of%2520traditional%2520methods%2520that%2520focus%2520solely%2520on%2520detecting%2520and%2520localizing%2520anomalies.%2520However%252C%2520existing%2520approaches%2520often%2520neglect%2520the%2520deeper%2520causal%2520relationships%2520and%2520interactions%2520between%2520objects%252C%2520which%2520are%2520critical%2520for%2520understanding%2520anomalous%2520behaviors.%2520In%2520this%2520paper%252C%2520we%2520propose%2520VADER%252C%2520an%2520LLM-driven%2520framework%2520for%2520Video%2520Anomaly%2520unDErstanding%252C%2520which%2520integrates%2520keyframe%2520object%2520Relation%2520features%2520with%2520visual%2520cues%2520to%2520enhance%2520anomaly%2520comprehension%2520from%2520video.%2520Specifically%252C%2520VADER%2520first%2520applies%2520an%2520Anomaly%2520Scorer%2520to%2520assign%2520per-frame%2520anomaly%2520scores%252C%2520followed%2520by%2520a%2520Context-AwarE%2520Sampling%2520%2528CAES%2529%2520strategy%2520to%2520capture%2520the%2520causal%2520context%2520of%2520each%2520anomalous%2520event.%2520A%2520Relation%2520Feature%2520Extractor%2520and%2520a%2520COntrastive%2520Relation%2520Encoder%2520%2528CORE%2529%2520jointly%2520model%2520dynamic%2520object%2520interactions%252C%2520producing%2520compact%2520relational%2520representations%2520for%2520downstream%2520reasoning.%2520These%2520visual%2520and%2520relational%2520cues%2520are%2520integrated%2520with%2520LLMs%2520to%2520generate%2520detailed%252C%2520causally%2520grounded%2520descriptions%2520and%2520support%2520robust%2520anomaly-related%2520question%2520answering.%2520Experiments%2520on%2520multiple%2520real-world%2520VAU%2520benchmarks%2520demonstrate%2520that%2520VADER%2520achieves%2520strong%2520results%2520across%2520anomaly%2520description%252C%2520explanation%252C%2520and%2520causal%2520reasoning%2520tasks%252C%2520advancing%2520the%2520frontier%2520of%2520explainable%2520video%2520anomaly%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VADER%3A%20Towards%20Causal%20Video%20Anomaly%20Understanding%20with%20Relation-Aware%20Large%20Language%20Models&entry.906535625=Ying%20Cheng%20and%20Yu-Ho%20Lin%20and%20Min-Hung%20Chen%20and%20Fu-En%20Yang%20and%20Shang-Hong%20Lai&entry.1292438233=Video%20anomaly%20understanding%20%28VAU%29%20aims%20to%20provide%20detailed%20interpretation%20and%20semantic%20comprehension%20of%20anomalous%20events%20within%20videos%2C%20addressing%20limitations%20of%20traditional%20methods%20that%20focus%20solely%20on%20detecting%20and%20localizing%20anomalies.%20However%2C%20existing%20approaches%20often%20neglect%20the%20deeper%20causal%20relationships%20and%20interactions%20between%20objects%2C%20which%20are%20critical%20for%20understanding%20anomalous%20behaviors.%20In%20this%20paper%2C%20we%20propose%20VADER%2C%20an%20LLM-driven%20framework%20for%20Video%20Anomaly%20unDErstanding%2C%20which%20integrates%20keyframe%20object%20Relation%20features%20with%20visual%20cues%20to%20enhance%20anomaly%20comprehension%20from%20video.%20Specifically%2C%20VADER%20first%20applies%20an%20Anomaly%20Scorer%20to%20assign%20per-frame%20anomaly%20scores%2C%20followed%20by%20a%20Context-AwarE%20Sampling%20%28CAES%29%20strategy%20to%20capture%20the%20causal%20context%20of%20each%20anomalous%20event.%20A%20Relation%20Feature%20Extractor%20and%20a%20COntrastive%20Relation%20Encoder%20%28CORE%29%20jointly%20model%20dynamic%20object%20interactions%2C%20producing%20compact%20relational%20representations%20for%20downstream%20reasoning.%20These%20visual%20and%20relational%20cues%20are%20integrated%20with%20LLMs%20to%20generate%20detailed%2C%20causally%20grounded%20descriptions%20and%20support%20robust%20anomaly-related%20question%20answering.%20Experiments%20on%20multiple%20real-world%20VAU%20benchmarks%20demonstrate%20that%20VADER%20achieves%20strong%20results%20across%20anomaly%20description%2C%20explanation%2C%20and%20causal%20reasoning%20tasks%2C%20advancing%20the%20frontier%20of%20explainable%20video%20anomaly%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2511.07299v2&entry.124074799=Read"},
{"title": "TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition", "author": "Yanan Liu and Jun Liu and Hao Zhang and Dan Xu and Hossein Rahmani and Mohammed Bennamoun and Qiuhong Ke", "abstract": "Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective.", "link": "http://arxiv.org/abs/2512.11503v1", "date": "2025-12-12", "relevancy": 2.8593, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5872}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5673}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSkel-Mamba%3A%20Temporal%20Dynamic%20Modeling%20via%20State%20Space%20Model%20for%20Human%20Skeleton-based%20Action%20Recognition&body=Title%3A%20TSkel-Mamba%3A%20Temporal%20Dynamic%20Modeling%20via%20State%20Space%20Model%20for%20Human%20Skeleton-based%20Action%20Recognition%0AAuthor%3A%20Yanan%20Liu%20and%20Jun%20Liu%20and%20Hao%20Zhang%20and%20Dan%20Xu%20and%20Hossein%20Rahmani%20and%20Mohammed%20Bennamoun%20and%20Qiuhong%20Ke%0AAbstract%3A%20Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20in%20the%20computer%20vision%20community.%20Inspired%20by%20the%20recent%20success%20of%20the%20selective%20state-space%20model%20%28SSM%29%20Mamba%20in%20modeling%201D%20temporal%20sequences%2C%20we%20propose%20TSkel-Mamba%2C%20a%20hybrid%20Transformer-Mamba%20framework%20that%20effectively%20captures%20both%20spatial%20and%20temporal%20dynamics.%20In%20particular%2C%20our%20approach%20leverages%20Spatial%20Transformer%20for%20spatial%20feature%20learning%20while%20utilizing%20Mamba%20for%20temporal%20modeling.%20Mamba%2C%20however%2C%20employs%20separate%20SSM%20blocks%20for%20individual%20channels%2C%20which%20inherently%20limits%20its%20ability%20to%20model%20inter-channel%20dependencies.%20To%20better%20adapt%20Mamba%20for%20skeleton%20data%20and%20enhance%20Mamba%60s%20ability%20to%20model%20temporal%20dependencies%2C%20we%20introduce%20a%20Temporal%20Dynamic%20Modeling%20%28TDM%29%20block%2C%20which%20is%20a%20versatile%20plug-and-play%20component%20that%20integrates%20a%20novel%20Multi-scale%20Temporal%20Interaction%20%28MTI%29%20module.%20The%20MTI%20module%20employs%20multi-scale%20Cycle%20operators%20to%20capture%20cross-channel%20temporal%20interactions%2C%20a%20critical%20factor%20in%20action%20recognition.%20Extensive%20experiments%20on%20NTU-RGB%2BD%2060%2C%20NTU-RGB%2BD%20120%2C%20NW-UCLA%20and%20UAV-Human%20datasets%20demonstrate%20that%20TSkel-Mamba%20achieves%20state-of-the-art%20performance%20while%20maintaining%20low%20inference%20time%2C%20making%20it%20both%20efficient%20and%20highly%20effective.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSkel-Mamba%253A%2520Temporal%2520Dynamic%2520Modeling%2520via%2520State%2520Space%2520Model%2520for%2520Human%2520Skeleton-based%2520Action%2520Recognition%26entry.906535625%3DYanan%2520Liu%2520and%2520Jun%2520Liu%2520and%2520Hao%2520Zhang%2520and%2520Dan%2520Xu%2520and%2520Hossein%2520Rahmani%2520and%2520Mohammed%2520Bennamoun%2520and%2520Qiuhong%2520Ke%26entry.1292438233%3DSkeleton-based%2520action%2520recognition%2520has%2520garnered%2520significant%2520attention%2520in%2520the%2520computer%2520vision%2520community.%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520the%2520selective%2520state-space%2520model%2520%2528SSM%2529%2520Mamba%2520in%2520modeling%25201D%2520temporal%2520sequences%252C%2520we%2520propose%2520TSkel-Mamba%252C%2520a%2520hybrid%2520Transformer-Mamba%2520framework%2520that%2520effectively%2520captures%2520both%2520spatial%2520and%2520temporal%2520dynamics.%2520In%2520particular%252C%2520our%2520approach%2520leverages%2520Spatial%2520Transformer%2520for%2520spatial%2520feature%2520learning%2520while%2520utilizing%2520Mamba%2520for%2520temporal%2520modeling.%2520Mamba%252C%2520however%252C%2520employs%2520separate%2520SSM%2520blocks%2520for%2520individual%2520channels%252C%2520which%2520inherently%2520limits%2520its%2520ability%2520to%2520model%2520inter-channel%2520dependencies.%2520To%2520better%2520adapt%2520Mamba%2520for%2520skeleton%2520data%2520and%2520enhance%2520Mamba%2560s%2520ability%2520to%2520model%2520temporal%2520dependencies%252C%2520we%2520introduce%2520a%2520Temporal%2520Dynamic%2520Modeling%2520%2528TDM%2529%2520block%252C%2520which%2520is%2520a%2520versatile%2520plug-and-play%2520component%2520that%2520integrates%2520a%2520novel%2520Multi-scale%2520Temporal%2520Interaction%2520%2528MTI%2529%2520module.%2520The%2520MTI%2520module%2520employs%2520multi-scale%2520Cycle%2520operators%2520to%2520capture%2520cross-channel%2520temporal%2520interactions%252C%2520a%2520critical%2520factor%2520in%2520action%2520recognition.%2520Extensive%2520experiments%2520on%2520NTU-RGB%252BD%252060%252C%2520NTU-RGB%252BD%2520120%252C%2520NW-UCLA%2520and%2520UAV-Human%2520datasets%2520demonstrate%2520that%2520TSkel-Mamba%2520achieves%2520state-of-the-art%2520performance%2520while%2520maintaining%2520low%2520inference%2520time%252C%2520making%2520it%2520both%2520efficient%2520and%2520highly%2520effective.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSkel-Mamba%3A%20Temporal%20Dynamic%20Modeling%20via%20State%20Space%20Model%20for%20Human%20Skeleton-based%20Action%20Recognition&entry.906535625=Yanan%20Liu%20and%20Jun%20Liu%20and%20Hao%20Zhang%20and%20Dan%20Xu%20and%20Hossein%20Rahmani%20and%20Mohammed%20Bennamoun%20and%20Qiuhong%20Ke&entry.1292438233=Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20in%20the%20computer%20vision%20community.%20Inspired%20by%20the%20recent%20success%20of%20the%20selective%20state-space%20model%20%28SSM%29%20Mamba%20in%20modeling%201D%20temporal%20sequences%2C%20we%20propose%20TSkel-Mamba%2C%20a%20hybrid%20Transformer-Mamba%20framework%20that%20effectively%20captures%20both%20spatial%20and%20temporal%20dynamics.%20In%20particular%2C%20our%20approach%20leverages%20Spatial%20Transformer%20for%20spatial%20feature%20learning%20while%20utilizing%20Mamba%20for%20temporal%20modeling.%20Mamba%2C%20however%2C%20employs%20separate%20SSM%20blocks%20for%20individual%20channels%2C%20which%20inherently%20limits%20its%20ability%20to%20model%20inter-channel%20dependencies.%20To%20better%20adapt%20Mamba%20for%20skeleton%20data%20and%20enhance%20Mamba%60s%20ability%20to%20model%20temporal%20dependencies%2C%20we%20introduce%20a%20Temporal%20Dynamic%20Modeling%20%28TDM%29%20block%2C%20which%20is%20a%20versatile%20plug-and-play%20component%20that%20integrates%20a%20novel%20Multi-scale%20Temporal%20Interaction%20%28MTI%29%20module.%20The%20MTI%20module%20employs%20multi-scale%20Cycle%20operators%20to%20capture%20cross-channel%20temporal%20interactions%2C%20a%20critical%20factor%20in%20action%20recognition.%20Extensive%20experiments%20on%20NTU-RGB%2BD%2060%2C%20NTU-RGB%2BD%20120%2C%20NW-UCLA%20and%20UAV-Human%20datasets%20demonstrate%20that%20TSkel-Mamba%20achieves%20state-of-the-art%20performance%20while%20maintaining%20low%20inference%20time%2C%20making%20it%20both%20efficient%20and%20highly%20effective.&entry.1838667208=http%3A//arxiv.org/abs/2512.11503v1&entry.124074799=Read"},
{"title": "Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis", "author": "Valentina Lilova and Toyesh Chakravorty and Julian I. Bibo and Emma Boccaletti and Brandon Li and L\u00edvia Baxov\u00e1 and Cees G. M. Snoek and Mohammadreza Salehi", "abstract": "Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream finetuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pretrained encoders. In this work, we introduce a novel benchmark for in-context 3D scene understanding that requires no finetuning and directly probes the quality of dense visual features. Building on the Hummingbird framework, which evaluates in-context 2D scene understanding, we extend the setup to the 3D Multi-View ImageNet (MVImgNet) dataset. Given a set of images from objects in specific angles (keys), we benchmark the performance of segmenting novel views (queries) and report the scores in 4 categories of easy, medium, hard, and extreme based on the key-query view contrast. We benchmark 8 state-of-the-art foundation models and show DINO-based encoders remain competitive across large viewpoint shifts, while 3D-aware models like VGGT require dedicated multi-view adjustments. Our code is publicly available at https://github.com/ToyeshC/open-hummingbird-3d-eval .", "link": "http://arxiv.org/abs/2512.11574v1", "date": "2025-12-12", "relevancy": 2.8533, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.733}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Foundation%20Models%27%203D%20Understanding%20Through%20Multi-View%20Correspondence%20Analysis&body=Title%3A%20Evaluating%20Foundation%20Models%27%203D%20Understanding%20Through%20Multi-View%20Correspondence%20Analysis%0AAuthor%3A%20Valentina%20Lilova%20and%20Toyesh%20Chakravorty%20and%20Julian%20I.%20Bibo%20and%20Emma%20Boccaletti%20and%20Brandon%20Li%20and%20L%C3%ADvia%20Baxov%C3%A1%20and%20Cees%20G.%20M.%20Snoek%20and%20Mohammadreza%20Salehi%0AAbstract%3A%20Benchmarking%203D%20spatial%20understanding%20of%20foundation%20models%20is%20essential%20for%20real-world%20applications%20such%20as%20robotics%20and%20autonomous%20driving.%20Existing%20evaluations%20often%20rely%20on%20downstream%20finetuning%20with%20linear%20heads%20or%20task-specific%20decoders%2C%20making%20it%20difficult%20to%20isolate%20the%20intrinsic%203D%20reasoning%20ability%20of%20pretrained%20encoders.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20benchmark%20for%20in-context%203D%20scene%20understanding%20that%20requires%20no%20finetuning%20and%20directly%20probes%20the%20quality%20of%20dense%20visual%20features.%20Building%20on%20the%20Hummingbird%20framework%2C%20which%20evaluates%20in-context%202D%20scene%20understanding%2C%20we%20extend%20the%20setup%20to%20the%203D%20Multi-View%20ImageNet%20%28MVImgNet%29%20dataset.%20Given%20a%20set%20of%20images%20from%20objects%20in%20specific%20angles%20%28keys%29%2C%20we%20benchmark%20the%20performance%20of%20segmenting%20novel%20views%20%28queries%29%20and%20report%20the%20scores%20in%204%20categories%20of%20easy%2C%20medium%2C%20hard%2C%20and%20extreme%20based%20on%20the%20key-query%20view%20contrast.%20We%20benchmark%208%20state-of-the-art%20foundation%20models%20and%20show%20DINO-based%20encoders%20remain%20competitive%20across%20large%20viewpoint%20shifts%2C%20while%203D-aware%20models%20like%20VGGT%20require%20dedicated%20multi-view%20adjustments.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/ToyeshC/open-hummingbird-3d-eval%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Foundation%2520Models%2527%25203D%2520Understanding%2520Through%2520Multi-View%2520Correspondence%2520Analysis%26entry.906535625%3DValentina%2520Lilova%2520and%2520Toyesh%2520Chakravorty%2520and%2520Julian%2520I.%2520Bibo%2520and%2520Emma%2520Boccaletti%2520and%2520Brandon%2520Li%2520and%2520L%25C3%25ADvia%2520Baxov%25C3%25A1%2520and%2520Cees%2520G.%2520M.%2520Snoek%2520and%2520Mohammadreza%2520Salehi%26entry.1292438233%3DBenchmarking%25203D%2520spatial%2520understanding%2520of%2520foundation%2520models%2520is%2520essential%2520for%2520real-world%2520applications%2520such%2520as%2520robotics%2520and%2520autonomous%2520driving.%2520Existing%2520evaluations%2520often%2520rely%2520on%2520downstream%2520finetuning%2520with%2520linear%2520heads%2520or%2520task-specific%2520decoders%252C%2520making%2520it%2520difficult%2520to%2520isolate%2520the%2520intrinsic%25203D%2520reasoning%2520ability%2520of%2520pretrained%2520encoders.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%2520for%2520in-context%25203D%2520scene%2520understanding%2520that%2520requires%2520no%2520finetuning%2520and%2520directly%2520probes%2520the%2520quality%2520of%2520dense%2520visual%2520features.%2520Building%2520on%2520the%2520Hummingbird%2520framework%252C%2520which%2520evaluates%2520in-context%25202D%2520scene%2520understanding%252C%2520we%2520extend%2520the%2520setup%2520to%2520the%25203D%2520Multi-View%2520ImageNet%2520%2528MVImgNet%2529%2520dataset.%2520Given%2520a%2520set%2520of%2520images%2520from%2520objects%2520in%2520specific%2520angles%2520%2528keys%2529%252C%2520we%2520benchmark%2520the%2520performance%2520of%2520segmenting%2520novel%2520views%2520%2528queries%2529%2520and%2520report%2520the%2520scores%2520in%25204%2520categories%2520of%2520easy%252C%2520medium%252C%2520hard%252C%2520and%2520extreme%2520based%2520on%2520the%2520key-query%2520view%2520contrast.%2520We%2520benchmark%25208%2520state-of-the-art%2520foundation%2520models%2520and%2520show%2520DINO-based%2520encoders%2520remain%2520competitive%2520across%2520large%2520viewpoint%2520shifts%252C%2520while%25203D-aware%2520models%2520like%2520VGGT%2520require%2520dedicated%2520multi-view%2520adjustments.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/ToyeshC/open-hummingbird-3d-eval%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Foundation%20Models%27%203D%20Understanding%20Through%20Multi-View%20Correspondence%20Analysis&entry.906535625=Valentina%20Lilova%20and%20Toyesh%20Chakravorty%20and%20Julian%20I.%20Bibo%20and%20Emma%20Boccaletti%20and%20Brandon%20Li%20and%20L%C3%ADvia%20Baxov%C3%A1%20and%20Cees%20G.%20M.%20Snoek%20and%20Mohammadreza%20Salehi&entry.1292438233=Benchmarking%203D%20spatial%20understanding%20of%20foundation%20models%20is%20essential%20for%20real-world%20applications%20such%20as%20robotics%20and%20autonomous%20driving.%20Existing%20evaluations%20often%20rely%20on%20downstream%20finetuning%20with%20linear%20heads%20or%20task-specific%20decoders%2C%20making%20it%20difficult%20to%20isolate%20the%20intrinsic%203D%20reasoning%20ability%20of%20pretrained%20encoders.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20benchmark%20for%20in-context%203D%20scene%20understanding%20that%20requires%20no%20finetuning%20and%20directly%20probes%20the%20quality%20of%20dense%20visual%20features.%20Building%20on%20the%20Hummingbird%20framework%2C%20which%20evaluates%20in-context%202D%20scene%20understanding%2C%20we%20extend%20the%20setup%20to%20the%203D%20Multi-View%20ImageNet%20%28MVImgNet%29%20dataset.%20Given%20a%20set%20of%20images%20from%20objects%20in%20specific%20angles%20%28keys%29%2C%20we%20benchmark%20the%20performance%20of%20segmenting%20novel%20views%20%28queries%29%20and%20report%20the%20scores%20in%204%20categories%20of%20easy%2C%20medium%2C%20hard%2C%20and%20extreme%20based%20on%20the%20key-query%20view%20contrast.%20We%20benchmark%208%20state-of-the-art%20foundation%20models%20and%20show%20DINO-based%20encoders%20remain%20competitive%20across%20large%20viewpoint%20shifts%2C%20while%203D-aware%20models%20like%20VGGT%20require%20dedicated%20multi-view%20adjustments.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/ToyeshC/open-hummingbird-3d-eval%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.11574v1&entry.124074799=Read"},
{"title": "Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects", "author": "Georgios Kouros and Minye Wu and Tinne Tuytelaars", "abstract": "Accurate reconstruction and relighting of glossy objects remains a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restrict faithful material recovery and limit relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine environment map optimization accelerates convergence, and negative-only environment map clipping preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.", "link": "http://arxiv.org/abs/2510.02069v3", "date": "2025-12-12", "relevancy": 2.8502, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6076}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5571}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spec-Gloss%20Surfels%20and%20Normal-Diffuse%20Priors%20for%20Relightable%20Glossy%20Objects&body=Title%3A%20Spec-Gloss%20Surfels%20and%20Normal-Diffuse%20Priors%20for%20Relightable%20Glossy%20Objects%0AAuthor%3A%20Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20Accurate%20reconstruction%20and%20relighting%20of%20glossy%20objects%20remains%20a%20longstanding%20challenge%2C%20as%20object%20shape%2C%20material%20properties%2C%20and%20illumination%20are%20inherently%20difficult%20to%20disentangle.%20Existing%20neural%20rendering%20approaches%20often%20rely%20on%20simplified%20BRDF%20models%20or%20parameterizations%20that%20couple%20diffuse%20and%20specular%20components%2C%20which%20restrict%20faithful%20material%20recovery%20and%20limit%20relighting%20fidelity.%20We%20propose%20a%20relightable%20framework%20that%20integrates%20a%20microfacet%20BRDF%20with%20the%20specular-glossiness%20parameterization%20into%202D%20Gaussian%20Splatting%20with%20deferred%20shading.%20This%20formulation%20enables%20more%20physically%20consistent%20material%20decomposition%2C%20while%20diffusion-based%20priors%20for%20surface%20normals%20and%20diffuse%20color%20guide%20early-stage%20optimization%20and%20mitigate%20ambiguity.%20A%20coarse-to-fine%20environment%20map%20optimization%20accelerates%20convergence%2C%20and%20negative-only%20environment%20map%20clipping%20preserves%20high-dynamic-range%20specular%20reflections.%20Extensive%20experiments%20on%20complex%2C%20glossy%20scenes%20demonstrate%20that%20our%20method%20achieves%20high-quality%20geometry%20and%20material%20reconstruction%2C%20delivering%20substantially%20more%20realistic%20and%20consistent%20relighting%20under%20novel%20illumination%20compared%20to%20existing%20Gaussian%20splatting%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02069v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpec-Gloss%2520Surfels%2520and%2520Normal-Diffuse%2520Priors%2520for%2520Relightable%2520Glossy%2520Objects%26entry.906535625%3DGeorgios%2520Kouros%2520and%2520Minye%2520Wu%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3DAccurate%2520reconstruction%2520and%2520relighting%2520of%2520glossy%2520objects%2520remains%2520a%2520longstanding%2520challenge%252C%2520as%2520object%2520shape%252C%2520material%2520properties%252C%2520and%2520illumination%2520are%2520inherently%2520difficult%2520to%2520disentangle.%2520Existing%2520neural%2520rendering%2520approaches%2520often%2520rely%2520on%2520simplified%2520BRDF%2520models%2520or%2520parameterizations%2520that%2520couple%2520diffuse%2520and%2520specular%2520components%252C%2520which%2520restrict%2520faithful%2520material%2520recovery%2520and%2520limit%2520relighting%2520fidelity.%2520We%2520propose%2520a%2520relightable%2520framework%2520that%2520integrates%2520a%2520microfacet%2520BRDF%2520with%2520the%2520specular-glossiness%2520parameterization%2520into%25202D%2520Gaussian%2520Splatting%2520with%2520deferred%2520shading.%2520This%2520formulation%2520enables%2520more%2520physically%2520consistent%2520material%2520decomposition%252C%2520while%2520diffusion-based%2520priors%2520for%2520surface%2520normals%2520and%2520diffuse%2520color%2520guide%2520early-stage%2520optimization%2520and%2520mitigate%2520ambiguity.%2520A%2520coarse-to-fine%2520environment%2520map%2520optimization%2520accelerates%2520convergence%252C%2520and%2520negative-only%2520environment%2520map%2520clipping%2520preserves%2520high-dynamic-range%2520specular%2520reflections.%2520Extensive%2520experiments%2520on%2520complex%252C%2520glossy%2520scenes%2520demonstrate%2520that%2520our%2520method%2520achieves%2520high-quality%2520geometry%2520and%2520material%2520reconstruction%252C%2520delivering%2520substantially%2520more%2520realistic%2520and%2520consistent%2520relighting%2520under%2520novel%2520illumination%2520compared%2520to%2520existing%2520Gaussian%2520splatting%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02069v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spec-Gloss%20Surfels%20and%20Normal-Diffuse%20Priors%20for%20Relightable%20Glossy%20Objects&entry.906535625=Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars&entry.1292438233=Accurate%20reconstruction%20and%20relighting%20of%20glossy%20objects%20remains%20a%20longstanding%20challenge%2C%20as%20object%20shape%2C%20material%20properties%2C%20and%20illumination%20are%20inherently%20difficult%20to%20disentangle.%20Existing%20neural%20rendering%20approaches%20often%20rely%20on%20simplified%20BRDF%20models%20or%20parameterizations%20that%20couple%20diffuse%20and%20specular%20components%2C%20which%20restrict%20faithful%20material%20recovery%20and%20limit%20relighting%20fidelity.%20We%20propose%20a%20relightable%20framework%20that%20integrates%20a%20microfacet%20BRDF%20with%20the%20specular-glossiness%20parameterization%20into%202D%20Gaussian%20Splatting%20with%20deferred%20shading.%20This%20formulation%20enables%20more%20physically%20consistent%20material%20decomposition%2C%20while%20diffusion-based%20priors%20for%20surface%20normals%20and%20diffuse%20color%20guide%20early-stage%20optimization%20and%20mitigate%20ambiguity.%20A%20coarse-to-fine%20environment%20map%20optimization%20accelerates%20convergence%2C%20and%20negative-only%20environment%20map%20clipping%20preserves%20high-dynamic-range%20specular%20reflections.%20Extensive%20experiments%20on%20complex%2C%20glossy%20scenes%20demonstrate%20that%20our%20method%20achieves%20high-quality%20geometry%20and%20material%20reconstruction%2C%20delivering%20substantially%20more%20realistic%20and%20consistent%20relighting%20under%20novel%20illumination%20compared%20to%20existing%20Gaussian%20splatting%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2510.02069v3&entry.124074799=Read"},
{"title": "DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation", "author": "Mohamed Abdelsamad and Michael Ulrich and Bin Yang and Miao Zhang and Yakov Miron and Abhinav Valada", "abstract": "Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.", "link": "http://arxiv.org/abs/2512.11465v1", "date": "2025-12-12", "relevancy": 2.8316, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5891}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5595}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOS%3A%20Distilling%20Observable%20Softmaps%20of%20Zipfian%20Prototypes%20for%20Self-Supervised%20Point%20Representation&body=Title%3A%20DOS%3A%20Distilling%20Observable%20Softmaps%20of%20Zipfian%20Prototypes%20for%20Self-Supervised%20Point%20Representation%0AAuthor%3A%20Mohamed%20Abdelsamad%20and%20Michael%20Ulrich%20and%20Bin%20Yang%20and%20Miao%20Zhang%20and%20Yakov%20Miron%20and%20Abhinav%20Valada%0AAbstract%3A%20Recent%20advances%20in%20self-supervised%20learning%20%28SSL%29%20have%20shown%20tremendous%20potential%20for%20learning%203D%20point%20cloud%20representations%20without%20human%20annotations.%20However%2C%20SSL%20for%203D%20point%20clouds%20still%20faces%20critical%20challenges%20due%20to%20irregular%20geometry%2C%20shortcut-prone%20reconstruction%2C%20and%20unbalanced%20semantics%20distribution.%20In%20this%20work%2C%20we%20propose%20DOS%20%28Distilling%20Observable%20Softmaps%29%2C%20a%20novel%20SSL%20framework%20that%20self-distills%20semantic%20relevance%20softmaps%20only%20at%20observable%20%28unmasked%29%20points.%20This%20strategy%20prevents%20information%20leakage%20from%20masked%20regions%20and%20provides%20richer%20supervision%20than%20discrete%20token-to-prototype%20assignments.%20To%20address%20the%20challenge%20of%20unbalanced%20semantics%20in%20an%20unsupervised%20setting%2C%20we%20introduce%20Zipfian%20prototypes%20and%20incorporate%20them%20using%20a%20modified%20Sinkhorn-Knopp%20algorithm%2C%20Zipf-Sinkhorn%2C%20which%20enforces%20a%20power-law%20prior%20over%20prototype%20usage%20and%20modulates%20the%20sharpness%20of%20the%20target%20softmap%20during%20training.%20DOS%20outperforms%20current%20state-of-the-art%20methods%20on%20semantic%20segmentation%20and%203D%20object%20detection%20across%20multiple%20benchmarks%2C%20including%20nuScenes%2C%20Waymo%2C%20SemanticKITTI%2C%20ScanNet%2C%20and%20ScanNet200%2C%20without%20relying%20on%20extra%20data%20or%20annotations.%20Our%20results%20demonstrate%20that%20observable-point%20softmaps%20distillation%20offers%20a%20scalable%20and%20effective%20paradigm%20for%20learning%20robust%203D%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOS%253A%2520Distilling%2520Observable%2520Softmaps%2520of%2520Zipfian%2520Prototypes%2520for%2520Self-Supervised%2520Point%2520Representation%26entry.906535625%3DMohamed%2520Abdelsamad%2520and%2520Michael%2520Ulrich%2520and%2520Bin%2520Yang%2520and%2520Miao%2520Zhang%2520and%2520Yakov%2520Miron%2520and%2520Abhinav%2520Valada%26entry.1292438233%3DRecent%2520advances%2520in%2520self-supervised%2520learning%2520%2528SSL%2529%2520have%2520shown%2520tremendous%2520potential%2520for%2520learning%25203D%2520point%2520cloud%2520representations%2520without%2520human%2520annotations.%2520However%252C%2520SSL%2520for%25203D%2520point%2520clouds%2520still%2520faces%2520critical%2520challenges%2520due%2520to%2520irregular%2520geometry%252C%2520shortcut-prone%2520reconstruction%252C%2520and%2520unbalanced%2520semantics%2520distribution.%2520In%2520this%2520work%252C%2520we%2520propose%2520DOS%2520%2528Distilling%2520Observable%2520Softmaps%2529%252C%2520a%2520novel%2520SSL%2520framework%2520that%2520self-distills%2520semantic%2520relevance%2520softmaps%2520only%2520at%2520observable%2520%2528unmasked%2529%2520points.%2520This%2520strategy%2520prevents%2520information%2520leakage%2520from%2520masked%2520regions%2520and%2520provides%2520richer%2520supervision%2520than%2520discrete%2520token-to-prototype%2520assignments.%2520To%2520address%2520the%2520challenge%2520of%2520unbalanced%2520semantics%2520in%2520an%2520unsupervised%2520setting%252C%2520we%2520introduce%2520Zipfian%2520prototypes%2520and%2520incorporate%2520them%2520using%2520a%2520modified%2520Sinkhorn-Knopp%2520algorithm%252C%2520Zipf-Sinkhorn%252C%2520which%2520enforces%2520a%2520power-law%2520prior%2520over%2520prototype%2520usage%2520and%2520modulates%2520the%2520sharpness%2520of%2520the%2520target%2520softmap%2520during%2520training.%2520DOS%2520outperforms%2520current%2520state-of-the-art%2520methods%2520on%2520semantic%2520segmentation%2520and%25203D%2520object%2520detection%2520across%2520multiple%2520benchmarks%252C%2520including%2520nuScenes%252C%2520Waymo%252C%2520SemanticKITTI%252C%2520ScanNet%252C%2520and%2520ScanNet200%252C%2520without%2520relying%2520on%2520extra%2520data%2520or%2520annotations.%2520Our%2520results%2520demonstrate%2520that%2520observable-point%2520softmaps%2520distillation%2520offers%2520a%2520scalable%2520and%2520effective%2520paradigm%2520for%2520learning%2520robust%25203D%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOS%3A%20Distilling%20Observable%20Softmaps%20of%20Zipfian%20Prototypes%20for%20Self-Supervised%20Point%20Representation&entry.906535625=Mohamed%20Abdelsamad%20and%20Michael%20Ulrich%20and%20Bin%20Yang%20and%20Miao%20Zhang%20and%20Yakov%20Miron%20and%20Abhinav%20Valada&entry.1292438233=Recent%20advances%20in%20self-supervised%20learning%20%28SSL%29%20have%20shown%20tremendous%20potential%20for%20learning%203D%20point%20cloud%20representations%20without%20human%20annotations.%20However%2C%20SSL%20for%203D%20point%20clouds%20still%20faces%20critical%20challenges%20due%20to%20irregular%20geometry%2C%20shortcut-prone%20reconstruction%2C%20and%20unbalanced%20semantics%20distribution.%20In%20this%20work%2C%20we%20propose%20DOS%20%28Distilling%20Observable%20Softmaps%29%2C%20a%20novel%20SSL%20framework%20that%20self-distills%20semantic%20relevance%20softmaps%20only%20at%20observable%20%28unmasked%29%20points.%20This%20strategy%20prevents%20information%20leakage%20from%20masked%20regions%20and%20provides%20richer%20supervision%20than%20discrete%20token-to-prototype%20assignments.%20To%20address%20the%20challenge%20of%20unbalanced%20semantics%20in%20an%20unsupervised%20setting%2C%20we%20introduce%20Zipfian%20prototypes%20and%20incorporate%20them%20using%20a%20modified%20Sinkhorn-Knopp%20algorithm%2C%20Zipf-Sinkhorn%2C%20which%20enforces%20a%20power-law%20prior%20over%20prototype%20usage%20and%20modulates%20the%20sharpness%20of%20the%20target%20softmap%20during%20training.%20DOS%20outperforms%20current%20state-of-the-art%20methods%20on%20semantic%20segmentation%20and%203D%20object%20detection%20across%20multiple%20benchmarks%2C%20including%20nuScenes%2C%20Waymo%2C%20SemanticKITTI%2C%20ScanNet%2C%20and%20ScanNet200%2C%20without%20relying%20on%20extra%20data%20or%20annotations.%20Our%20results%20demonstrate%20that%20observable-point%20softmaps%20distillation%20offers%20a%20scalable%20and%20effective%20paradigm%20for%20learning%20robust%203D%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2512.11465v1&entry.124074799=Read"},
{"title": "On Geometric Understanding and Learned Data Priors in VGGT", "author": "Jelena Bratuli\u0107 and Sudhanshu Mittal and Thomas Brox and Christian Rupprecht", "abstract": "The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.", "link": "http://arxiv.org/abs/2512.11508v1", "date": "2025-12-12", "relevancy": 2.8009, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.579}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Geometric%20Understanding%20and%20Learned%20Data%20Priors%20in%20VGGT&body=Title%3A%20On%20Geometric%20Understanding%20and%20Learned%20Data%20Priors%20in%20VGGT%0AAuthor%3A%20Jelena%20Bratuli%C4%87%20and%20Sudhanshu%20Mittal%20and%20Thomas%20Brox%20and%20Christian%20Rupprecht%0AAbstract%3A%20The%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20is%20a%203D%20foundation%20model%20that%20infers%20camera%20geometry%20and%20scene%20structure%20in%20a%20single%20feed-forward%20pass.%20Trained%20in%20a%20supervised%2C%20single-step%20fashion%20on%20large%20datasets%2C%20VGGT%20raises%20a%20key%20question%3A%20does%20it%20build%20upon%20geometric%20concepts%20like%20traditional%20multi-view%20methods%2C%20or%20does%20it%20rely%20primarily%20on%20learned%20appearance-based%20data-driven%20priors%3F%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20analysis%20of%20VGGT%27s%20internal%20mechanisms%20to%20uncover%20whether%20geometric%20understanding%20emerges%20within%20its%20representations.%20By%20probing%20intermediate%20features%2C%20analyzing%20attention%20patterns%2C%20and%20performing%20interventions%2C%20we%20examine%20how%20the%20model%20implements%20its%20functionality.%20Our%20findings%20reveal%20that%20VGGT%20implicitly%20performs%20correspondence%20matching%20within%20its%20global%20attention%20layers%20and%20encodes%20epipolar%20geometry%2C%20despite%20being%20trained%20without%20explicit%20geometric%20constraints.%20We%20further%20investigate%20VGGT%27s%20dependence%20on%20its%20learned%20data%20priors.%20Using%20spatial%20input%20masking%20and%20perturbation%20experiments%2C%20we%20assess%20its%20robustness%20to%20occlusions%2C%20appearance%20variations%2C%20and%20camera%20configurations%2C%20comparing%20it%20with%20classical%20multi-stage%20pipelines.%20Together%2C%20these%20insights%20highlight%20how%20VGGT%20internalizes%20geometric%20structure%20while%20using%20learned%20data-driven%20priors.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Geometric%2520Understanding%2520and%2520Learned%2520Data%2520Priors%2520in%2520VGGT%26entry.906535625%3DJelena%2520Bratuli%25C4%2587%2520and%2520Sudhanshu%2520Mittal%2520and%2520Thomas%2520Brox%2520and%2520Christian%2520Rupprecht%26entry.1292438233%3DThe%2520Visual%2520Geometry%2520Grounded%2520Transformer%2520%2528VGGT%2529%2520is%2520a%25203D%2520foundation%2520model%2520that%2520infers%2520camera%2520geometry%2520and%2520scene%2520structure%2520in%2520a%2520single%2520feed-forward%2520pass.%2520Trained%2520in%2520a%2520supervised%252C%2520single-step%2520fashion%2520on%2520large%2520datasets%252C%2520VGGT%2520raises%2520a%2520key%2520question%253A%2520does%2520it%2520build%2520upon%2520geometric%2520concepts%2520like%2520traditional%2520multi-view%2520methods%252C%2520or%2520does%2520it%2520rely%2520primarily%2520on%2520learned%2520appearance-based%2520data-driven%2520priors%253F%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520systematic%2520analysis%2520of%2520VGGT%2527s%2520internal%2520mechanisms%2520to%2520uncover%2520whether%2520geometric%2520understanding%2520emerges%2520within%2520its%2520representations.%2520By%2520probing%2520intermediate%2520features%252C%2520analyzing%2520attention%2520patterns%252C%2520and%2520performing%2520interventions%252C%2520we%2520examine%2520how%2520the%2520model%2520implements%2520its%2520functionality.%2520Our%2520findings%2520reveal%2520that%2520VGGT%2520implicitly%2520performs%2520correspondence%2520matching%2520within%2520its%2520global%2520attention%2520layers%2520and%2520encodes%2520epipolar%2520geometry%252C%2520despite%2520being%2520trained%2520without%2520explicit%2520geometric%2520constraints.%2520We%2520further%2520investigate%2520VGGT%2527s%2520dependence%2520on%2520its%2520learned%2520data%2520priors.%2520Using%2520spatial%2520input%2520masking%2520and%2520perturbation%2520experiments%252C%2520we%2520assess%2520its%2520robustness%2520to%2520occlusions%252C%2520appearance%2520variations%252C%2520and%2520camera%2520configurations%252C%2520comparing%2520it%2520with%2520classical%2520multi-stage%2520pipelines.%2520Together%252C%2520these%2520insights%2520highlight%2520how%2520VGGT%2520internalizes%2520geometric%2520structure%2520while%2520using%2520learned%2520data-driven%2520priors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Geometric%20Understanding%20and%20Learned%20Data%20Priors%20in%20VGGT&entry.906535625=Jelena%20Bratuli%C4%87%20and%20Sudhanshu%20Mittal%20and%20Thomas%20Brox%20and%20Christian%20Rupprecht&entry.1292438233=The%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20is%20a%203D%20foundation%20model%20that%20infers%20camera%20geometry%20and%20scene%20structure%20in%20a%20single%20feed-forward%20pass.%20Trained%20in%20a%20supervised%2C%20single-step%20fashion%20on%20large%20datasets%2C%20VGGT%20raises%20a%20key%20question%3A%20does%20it%20build%20upon%20geometric%20concepts%20like%20traditional%20multi-view%20methods%2C%20or%20does%20it%20rely%20primarily%20on%20learned%20appearance-based%20data-driven%20priors%3F%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20analysis%20of%20VGGT%27s%20internal%20mechanisms%20to%20uncover%20whether%20geometric%20understanding%20emerges%20within%20its%20representations.%20By%20probing%20intermediate%20features%2C%20analyzing%20attention%20patterns%2C%20and%20performing%20interventions%2C%20we%20examine%20how%20the%20model%20implements%20its%20functionality.%20Our%20findings%20reveal%20that%20VGGT%20implicitly%20performs%20correspondence%20matching%20within%20its%20global%20attention%20layers%20and%20encodes%20epipolar%20geometry%2C%20despite%20being%20trained%20without%20explicit%20geometric%20constraints.%20We%20further%20investigate%20VGGT%27s%20dependence%20on%20its%20learned%20data%20priors.%20Using%20spatial%20input%20masking%20and%20perturbation%20experiments%2C%20we%20assess%20its%20robustness%20to%20occlusions%2C%20appearance%20variations%2C%20and%20camera%20configurations%2C%20comparing%20it%20with%20classical%20multi-stage%20pipelines.%20Together%2C%20these%20insights%20highlight%20how%20VGGT%20internalizes%20geometric%20structure%20while%20using%20learned%20data-driven%20priors.&entry.1838667208=http%3A//arxiv.org/abs/2512.11508v1&entry.124074799=Read"},
{"title": "CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop", "author": "Weijian Ma and Shizhao Sun and Ruiyu Wang and Jiang Bian", "abstract": "A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.", "link": "http://arxiv.org/abs/2512.11480v1", "date": "2025-12-12", "relevancy": 2.7625, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5619}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5558}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CADMorph%3A%20Geometry-Driven%20Parametric%20CAD%20Editing%20via%20a%20Plan-Generate-Verify%20Loop&body=Title%3A%20CADMorph%3A%20Geometry-Driven%20Parametric%20CAD%20Editing%20via%20a%20Plan-Generate-Verify%20Loop%0AAuthor%3A%20Weijian%20Ma%20and%20Shizhao%20Sun%20and%20Ruiyu%20Wang%20and%20Jiang%20Bian%0AAbstract%3A%20A%20Computer-Aided%20Design%20%28CAD%29%20model%20encodes%20an%20object%20in%20two%20coupled%20forms%3A%20a%20parametric%20construction%20sequence%20and%20its%20resulting%20visible%20geometric%20shape.%20During%20iterative%20design%2C%20adjustments%20to%20the%20geometric%20shape%20inevitably%20require%20synchronized%20edits%20to%20the%20underlying%20parametric%20sequence%2C%20called%20geometry-driven%20parametric%20CAD%20editing.%20The%20task%20calls%20for%201%29%20preserving%20the%20original%20sequence%27s%20structure%2C%202%29%20ensuring%20each%20edit%27s%20semantic%20validity%2C%20and%203%29%20maintaining%20high%20shape%20fidelity%20to%20the%20target%20shape%2C%20all%20under%20scarce%20editing%20data%20triplets.%20We%20present%20CADMorph%2C%20an%20iterative%20plan-generate-verify%20framework%20that%20orchestrates%20pretrained%20domain-specific%20foundation%20models%20during%20inference%3A%20a%20parameter-to-shape%20%28P2S%29%20latent%20diffusion%20model%20and%20a%20masked-parameter-prediction%20%28MPP%29%20model.%20In%20the%20planning%20stage%2C%20cross-attention%20maps%20from%20the%20P2S%20model%20pinpoint%20the%20segments%20that%20need%20modification%20and%20offer%20editing%20masks.%20The%20MPP%20model%20then%20infills%20these%20masks%20with%20semantically%20valid%20edits%20in%20the%20generation%20stage.%20During%20verification%2C%20the%20P2S%20model%20embeds%20each%20candidate%20sequence%20in%20shape-latent%20space%2C%20measures%20its%20distance%20to%20the%20target%20shape%2C%20and%20selects%20the%20closest%20one.%20The%20three%20stages%20leverage%20the%20inherent%20geometric%20consciousness%20and%20design%20knowledge%20in%20pretrained%20priors%2C%20and%20thus%20tackle%20structure%20preservation%2C%20semantic%20validity%2C%20and%20shape%20fidelity%20respectively.%20Besides%2C%20both%20P2S%20and%20MPP%20models%20are%20trained%20without%20triplet%20data%2C%20bypassing%20the%20data-scarcity%20bottleneck.%20CADMorph%20surpasses%20GPT-4o%20and%20specialized%20CAD%20baselines%2C%20and%20supports%20downstream%20applications%20such%20as%20iterative%20editing%20and%20reverse-engineering%20enhancement.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCADMorph%253A%2520Geometry-Driven%2520Parametric%2520CAD%2520Editing%2520via%2520a%2520Plan-Generate-Verify%2520Loop%26entry.906535625%3DWeijian%2520Ma%2520and%2520Shizhao%2520Sun%2520and%2520Ruiyu%2520Wang%2520and%2520Jiang%2520Bian%26entry.1292438233%3DA%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520model%2520encodes%2520an%2520object%2520in%2520two%2520coupled%2520forms%253A%2520a%2520parametric%2520construction%2520sequence%2520and%2520its%2520resulting%2520visible%2520geometric%2520shape.%2520During%2520iterative%2520design%252C%2520adjustments%2520to%2520the%2520geometric%2520shape%2520inevitably%2520require%2520synchronized%2520edits%2520to%2520the%2520underlying%2520parametric%2520sequence%252C%2520called%2520geometry-driven%2520parametric%2520CAD%2520editing.%2520The%2520task%2520calls%2520for%25201%2529%2520preserving%2520the%2520original%2520sequence%2527s%2520structure%252C%25202%2529%2520ensuring%2520each%2520edit%2527s%2520semantic%2520validity%252C%2520and%25203%2529%2520maintaining%2520high%2520shape%2520fidelity%2520to%2520the%2520target%2520shape%252C%2520all%2520under%2520scarce%2520editing%2520data%2520triplets.%2520We%2520present%2520CADMorph%252C%2520an%2520iterative%2520plan-generate-verify%2520framework%2520that%2520orchestrates%2520pretrained%2520domain-specific%2520foundation%2520models%2520during%2520inference%253A%2520a%2520parameter-to-shape%2520%2528P2S%2529%2520latent%2520diffusion%2520model%2520and%2520a%2520masked-parameter-prediction%2520%2528MPP%2529%2520model.%2520In%2520the%2520planning%2520stage%252C%2520cross-attention%2520maps%2520from%2520the%2520P2S%2520model%2520pinpoint%2520the%2520segments%2520that%2520need%2520modification%2520and%2520offer%2520editing%2520masks.%2520The%2520MPP%2520model%2520then%2520infills%2520these%2520masks%2520with%2520semantically%2520valid%2520edits%2520in%2520the%2520generation%2520stage.%2520During%2520verification%252C%2520the%2520P2S%2520model%2520embeds%2520each%2520candidate%2520sequence%2520in%2520shape-latent%2520space%252C%2520measures%2520its%2520distance%2520to%2520the%2520target%2520shape%252C%2520and%2520selects%2520the%2520closest%2520one.%2520The%2520three%2520stages%2520leverage%2520the%2520inherent%2520geometric%2520consciousness%2520and%2520design%2520knowledge%2520in%2520pretrained%2520priors%252C%2520and%2520thus%2520tackle%2520structure%2520preservation%252C%2520semantic%2520validity%252C%2520and%2520shape%2520fidelity%2520respectively.%2520Besides%252C%2520both%2520P2S%2520and%2520MPP%2520models%2520are%2520trained%2520without%2520triplet%2520data%252C%2520bypassing%2520the%2520data-scarcity%2520bottleneck.%2520CADMorph%2520surpasses%2520GPT-4o%2520and%2520specialized%2520CAD%2520baselines%252C%2520and%2520supports%2520downstream%2520applications%2520such%2520as%2520iterative%2520editing%2520and%2520reverse-engineering%2520enhancement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CADMorph%3A%20Geometry-Driven%20Parametric%20CAD%20Editing%20via%20a%20Plan-Generate-Verify%20Loop&entry.906535625=Weijian%20Ma%20and%20Shizhao%20Sun%20and%20Ruiyu%20Wang%20and%20Jiang%20Bian&entry.1292438233=A%20Computer-Aided%20Design%20%28CAD%29%20model%20encodes%20an%20object%20in%20two%20coupled%20forms%3A%20a%20parametric%20construction%20sequence%20and%20its%20resulting%20visible%20geometric%20shape.%20During%20iterative%20design%2C%20adjustments%20to%20the%20geometric%20shape%20inevitably%20require%20synchronized%20edits%20to%20the%20underlying%20parametric%20sequence%2C%20called%20geometry-driven%20parametric%20CAD%20editing.%20The%20task%20calls%20for%201%29%20preserving%20the%20original%20sequence%27s%20structure%2C%202%29%20ensuring%20each%20edit%27s%20semantic%20validity%2C%20and%203%29%20maintaining%20high%20shape%20fidelity%20to%20the%20target%20shape%2C%20all%20under%20scarce%20editing%20data%20triplets.%20We%20present%20CADMorph%2C%20an%20iterative%20plan-generate-verify%20framework%20that%20orchestrates%20pretrained%20domain-specific%20foundation%20models%20during%20inference%3A%20a%20parameter-to-shape%20%28P2S%29%20latent%20diffusion%20model%20and%20a%20masked-parameter-prediction%20%28MPP%29%20model.%20In%20the%20planning%20stage%2C%20cross-attention%20maps%20from%20the%20P2S%20model%20pinpoint%20the%20segments%20that%20need%20modification%20and%20offer%20editing%20masks.%20The%20MPP%20model%20then%20infills%20these%20masks%20with%20semantically%20valid%20edits%20in%20the%20generation%20stage.%20During%20verification%2C%20the%20P2S%20model%20embeds%20each%20candidate%20sequence%20in%20shape-latent%20space%2C%20measures%20its%20distance%20to%20the%20target%20shape%2C%20and%20selects%20the%20closest%20one.%20The%20three%20stages%20leverage%20the%20inherent%20geometric%20consciousness%20and%20design%20knowledge%20in%20pretrained%20priors%2C%20and%20thus%20tackle%20structure%20preservation%2C%20semantic%20validity%2C%20and%20shape%20fidelity%20respectively.%20Besides%2C%20both%20P2S%20and%20MPP%20models%20are%20trained%20without%20triplet%20data%2C%20bypassing%20the%20data-scarcity%20bottleneck.%20CADMorph%20surpasses%20GPT-4o%20and%20specialized%20CAD%20baselines%2C%20and%20supports%20downstream%20applications%20such%20as%20iterative%20editing%20and%20reverse-engineering%20enhancement.&entry.1838667208=http%3A//arxiv.org/abs/2512.11480v1&entry.124074799=Read"},
{"title": "Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided diffusion", "author": "Jiqing Wu and Ingrid Berg and Yawei Li and Ender Konukoglu and Viktor H. Koelzer", "abstract": "Holistic 3D modeling of molecularly defined brain structures is crucial for understanding complex brain functions. Using emerging tissue profiling technologies, researchers charted comprehensive atlases of mammalian brain with sub-cellular resolution and spatially resolved transcriptomic data. However, these tera-scale volumetric atlases pose computational challenges for modeling intricate brain structures within the native spatial context. We propose \\textbf{Tera-MIND}, a novel generative framework capable of simulating \\textbf{Tera}-scale \\textbf{M}ouse bra\\textbf{IN}s in 3D using a patch-based and boundary-aware \\textbf{D}iffusion model. Taking spatial gene expression as conditional input, we generate virtual mouse brains with comprehensive cellular morphological detail at teravoxel scale. Through the lens of 3D \\textit{gene}-\\textit{gene} self-attention, we identify spatial molecular interactions for key transcriptomic pathways, including glutamatergic and dopaminergic neuronal systems. Lastly, we showcase the translational applicability of Tera-MIND on previously unseen human brain samples. Tera-MIND offers an efficient generative modeling of whole virtual organisms, paving the way for integrative applications in biomedical research. Project website: https://musikisomorphie.github.io/Tera-MIND.html", "link": "http://arxiv.org/abs/2503.01220v3", "date": "2025-12-12", "relevancy": 2.7615, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5536}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5536}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tera-MIND%3A%20Tera-scale%20mouse%20brain%20simulation%20via%20spatial%20mRNA-guided%20diffusion&body=Title%3A%20Tera-MIND%3A%20Tera-scale%20mouse%20brain%20simulation%20via%20spatial%20mRNA-guided%20diffusion%0AAuthor%3A%20Jiqing%20Wu%20and%20Ingrid%20Berg%20and%20Yawei%20Li%20and%20Ender%20Konukoglu%20and%20Viktor%20H.%20Koelzer%0AAbstract%3A%20Holistic%203D%20modeling%20of%20molecularly%20defined%20brain%20structures%20is%20crucial%20for%20understanding%20complex%20brain%20functions.%20Using%20emerging%20tissue%20profiling%20technologies%2C%20researchers%20charted%20comprehensive%20atlases%20of%20mammalian%20brain%20with%20sub-cellular%20resolution%20and%20spatially%20resolved%20transcriptomic%20data.%20However%2C%20these%20tera-scale%20volumetric%20atlases%20pose%20computational%20challenges%20for%20modeling%20intricate%20brain%20structures%20within%20the%20native%20spatial%20context.%20We%20propose%20%5Ctextbf%7BTera-MIND%7D%2C%20a%20novel%20generative%20framework%20capable%20of%20simulating%20%5Ctextbf%7BTera%7D-scale%20%5Ctextbf%7BM%7Douse%20bra%5Ctextbf%7BIN%7Ds%20in%203D%20using%20a%20patch-based%20and%20boundary-aware%20%5Ctextbf%7BD%7Diffusion%20model.%20Taking%20spatial%20gene%20expression%20as%20conditional%20input%2C%20we%20generate%20virtual%20mouse%20brains%20with%20comprehensive%20cellular%20morphological%20detail%20at%20teravoxel%20scale.%20Through%20the%20lens%20of%203D%20%5Ctextit%7Bgene%7D-%5Ctextit%7Bgene%7D%20self-attention%2C%20we%20identify%20spatial%20molecular%20interactions%20for%20key%20transcriptomic%20pathways%2C%20including%20glutamatergic%20and%20dopaminergic%20neuronal%20systems.%20Lastly%2C%20we%20showcase%20the%20translational%20applicability%20of%20Tera-MIND%20on%20previously%20unseen%20human%20brain%20samples.%20Tera-MIND%20offers%20an%20efficient%20generative%20modeling%20of%20whole%20virtual%20organisms%2C%20paving%20the%20way%20for%20integrative%20applications%20in%20biomedical%20research.%20Project%20website%3A%20https%3A//musikisomorphie.github.io/Tera-MIND.html%0ALink%3A%20http%3A//arxiv.org/abs/2503.01220v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTera-MIND%253A%2520Tera-scale%2520mouse%2520brain%2520simulation%2520via%2520spatial%2520mRNA-guided%2520diffusion%26entry.906535625%3DJiqing%2520Wu%2520and%2520Ingrid%2520Berg%2520and%2520Yawei%2520Li%2520and%2520Ender%2520Konukoglu%2520and%2520Viktor%2520H.%2520Koelzer%26entry.1292438233%3DHolistic%25203D%2520modeling%2520of%2520molecularly%2520defined%2520brain%2520structures%2520is%2520crucial%2520for%2520understanding%2520complex%2520brain%2520functions.%2520Using%2520emerging%2520tissue%2520profiling%2520technologies%252C%2520researchers%2520charted%2520comprehensive%2520atlases%2520of%2520mammalian%2520brain%2520with%2520sub-cellular%2520resolution%2520and%2520spatially%2520resolved%2520transcriptomic%2520data.%2520However%252C%2520these%2520tera-scale%2520volumetric%2520atlases%2520pose%2520computational%2520challenges%2520for%2520modeling%2520intricate%2520brain%2520structures%2520within%2520the%2520native%2520spatial%2520context.%2520We%2520propose%2520%255Ctextbf%257BTera-MIND%257D%252C%2520a%2520novel%2520generative%2520framework%2520capable%2520of%2520simulating%2520%255Ctextbf%257BTera%257D-scale%2520%255Ctextbf%257BM%257Douse%2520bra%255Ctextbf%257BIN%257Ds%2520in%25203D%2520using%2520a%2520patch-based%2520and%2520boundary-aware%2520%255Ctextbf%257BD%257Diffusion%2520model.%2520Taking%2520spatial%2520gene%2520expression%2520as%2520conditional%2520input%252C%2520we%2520generate%2520virtual%2520mouse%2520brains%2520with%2520comprehensive%2520cellular%2520morphological%2520detail%2520at%2520teravoxel%2520scale.%2520Through%2520the%2520lens%2520of%25203D%2520%255Ctextit%257Bgene%257D-%255Ctextit%257Bgene%257D%2520self-attention%252C%2520we%2520identify%2520spatial%2520molecular%2520interactions%2520for%2520key%2520transcriptomic%2520pathways%252C%2520including%2520glutamatergic%2520and%2520dopaminergic%2520neuronal%2520systems.%2520Lastly%252C%2520we%2520showcase%2520the%2520translational%2520applicability%2520of%2520Tera-MIND%2520on%2520previously%2520unseen%2520human%2520brain%2520samples.%2520Tera-MIND%2520offers%2520an%2520efficient%2520generative%2520modeling%2520of%2520whole%2520virtual%2520organisms%252C%2520paving%2520the%2520way%2520for%2520integrative%2520applications%2520in%2520biomedical%2520research.%2520Project%2520website%253A%2520https%253A//musikisomorphie.github.io/Tera-MIND.html%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01220v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tera-MIND%3A%20Tera-scale%20mouse%20brain%20simulation%20via%20spatial%20mRNA-guided%20diffusion&entry.906535625=Jiqing%20Wu%20and%20Ingrid%20Berg%20and%20Yawei%20Li%20and%20Ender%20Konukoglu%20and%20Viktor%20H.%20Koelzer&entry.1292438233=Holistic%203D%20modeling%20of%20molecularly%20defined%20brain%20structures%20is%20crucial%20for%20understanding%20complex%20brain%20functions.%20Using%20emerging%20tissue%20profiling%20technologies%2C%20researchers%20charted%20comprehensive%20atlases%20of%20mammalian%20brain%20with%20sub-cellular%20resolution%20and%20spatially%20resolved%20transcriptomic%20data.%20However%2C%20these%20tera-scale%20volumetric%20atlases%20pose%20computational%20challenges%20for%20modeling%20intricate%20brain%20structures%20within%20the%20native%20spatial%20context.%20We%20propose%20%5Ctextbf%7BTera-MIND%7D%2C%20a%20novel%20generative%20framework%20capable%20of%20simulating%20%5Ctextbf%7BTera%7D-scale%20%5Ctextbf%7BM%7Douse%20bra%5Ctextbf%7BIN%7Ds%20in%203D%20using%20a%20patch-based%20and%20boundary-aware%20%5Ctextbf%7BD%7Diffusion%20model.%20Taking%20spatial%20gene%20expression%20as%20conditional%20input%2C%20we%20generate%20virtual%20mouse%20brains%20with%20comprehensive%20cellular%20morphological%20detail%20at%20teravoxel%20scale.%20Through%20the%20lens%20of%203D%20%5Ctextit%7Bgene%7D-%5Ctextit%7Bgene%7D%20self-attention%2C%20we%20identify%20spatial%20molecular%20interactions%20for%20key%20transcriptomic%20pathways%2C%20including%20glutamatergic%20and%20dopaminergic%20neuronal%20systems.%20Lastly%2C%20we%20showcase%20the%20translational%20applicability%20of%20Tera-MIND%20on%20previously%20unseen%20human%20brain%20samples.%20Tera-MIND%20offers%20an%20efficient%20generative%20modeling%20of%20whole%20virtual%20organisms%2C%20paving%20the%20way%20for%20integrative%20applications%20in%20biomedical%20research.%20Project%20website%3A%20https%3A//musikisomorphie.github.io/Tera-MIND.html&entry.1838667208=http%3A//arxiv.org/abs/2503.01220v3&entry.124074799=Read"},
{"title": "SpecDETR: A transformer-based hyperspectral point object detection network", "author": "Zhaoxu Li and Wei An and Gaowei Guo and Longguang Wang and Yingqian Wang and Zaiping Lin", "abstract": "Hyperspectral target detection (HTD) aims to identify specific materials based on spectral information in hyperspectral imagery and can detect extremely small-sized objects, some of which occupy a smaller than one-pixel area. However, existing HTD methods are developed based on per-pixel binary classification, neglecting the three-dimensional cube structure of hyperspectral images (HSIs) that integrates both spatial and spectral dimensions. The synergistic existence of spatial and spectral features in HSIs enable objects to simultaneously exhibit both, yet the per-pixel HTD framework limits the joint expression of these features. In this paper, we rethink HTD from the perspective of spatial-spectral synergistic representation and propose hyperspectral point object detection as an innovative task framework. We introduce SpecDETR, the first specialized network for hyperspectral multi-class point object detection, which eliminates dependence on pre-trained backbone networks commonly required by vision-based object detectors. SpecDETR uses a multi-layer Transformer encoder with self-excited subpixel-scale attention modules to directly extract deep spatial-spectral joint features from hyperspectral cubes. We develop a simulated hyperspectral point object detection benchmark termed SPOD, and for the first time, evaluate and compare the performance of visual object detection networks and HTD methods on hyperspectral point object detection. Extensive experiments demonstrate that our proposed SpecDETR outperforms SOTA visual object detection networks and HTD methods. Our code and dataset are available at https://github.com/ZhaoxuLi123/SpecDETR.", "link": "http://arxiv.org/abs/2405.10148v4", "date": "2025-12-12", "relevancy": 2.751, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5767}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5478}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecDETR%3A%20A%20transformer-based%20hyperspectral%20point%20object%20detection%20network&body=Title%3A%20SpecDETR%3A%20A%20transformer-based%20hyperspectral%20point%20object%20detection%20network%0AAuthor%3A%20Zhaoxu%20Li%20and%20Wei%20An%20and%20Gaowei%20Guo%20and%20Longguang%20Wang%20and%20Yingqian%20Wang%20and%20Zaiping%20Lin%0AAbstract%3A%20Hyperspectral%20target%20detection%20%28HTD%29%20aims%20to%20identify%20specific%20materials%20based%20on%20spectral%20information%20in%20hyperspectral%20imagery%20and%20can%20detect%20extremely%20small-sized%20objects%2C%20some%20of%20which%20occupy%20a%20smaller%20than%20one-pixel%20area.%20However%2C%20existing%20HTD%20methods%20are%20developed%20based%20on%20per-pixel%20binary%20classification%2C%20neglecting%20the%20three-dimensional%20cube%20structure%20of%20hyperspectral%20images%20%28HSIs%29%20that%20integrates%20both%20spatial%20and%20spectral%20dimensions.%20The%20synergistic%20existence%20of%20spatial%20and%20spectral%20features%20in%20HSIs%20enable%20objects%20to%20simultaneously%20exhibit%20both%2C%20yet%20the%20per-pixel%20HTD%20framework%20limits%20the%20joint%20expression%20of%20these%20features.%20In%20this%20paper%2C%20we%20rethink%20HTD%20from%20the%20perspective%20of%20spatial-spectral%20synergistic%20representation%20and%20propose%20hyperspectral%20point%20object%20detection%20as%20an%20innovative%20task%20framework.%20We%20introduce%20SpecDETR%2C%20the%20first%20specialized%20network%20for%20hyperspectral%20multi-class%20point%20object%20detection%2C%20which%20eliminates%20dependence%20on%20pre-trained%20backbone%20networks%20commonly%20required%20by%20vision-based%20object%20detectors.%20SpecDETR%20uses%20a%20multi-layer%20Transformer%20encoder%20with%20self-excited%20subpixel-scale%20attention%20modules%20to%20directly%20extract%20deep%20spatial-spectral%20joint%20features%20from%20hyperspectral%20cubes.%20We%20develop%20a%20simulated%20hyperspectral%20point%20object%20detection%20benchmark%20termed%20SPOD%2C%20and%20for%20the%20first%20time%2C%20evaluate%20and%20compare%20the%20performance%20of%20visual%20object%20detection%20networks%20and%20HTD%20methods%20on%20hyperspectral%20point%20object%20detection.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20SpecDETR%20outperforms%20SOTA%20visual%20object%20detection%20networks%20and%20HTD%20methods.%20Our%20code%20and%20dataset%20are%20available%20at%20https%3A//github.com/ZhaoxuLi123/SpecDETR.%0ALink%3A%20http%3A//arxiv.org/abs/2405.10148v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecDETR%253A%2520A%2520transformer-based%2520hyperspectral%2520point%2520object%2520detection%2520network%26entry.906535625%3DZhaoxu%2520Li%2520and%2520Wei%2520An%2520and%2520Gaowei%2520Guo%2520and%2520Longguang%2520Wang%2520and%2520Yingqian%2520Wang%2520and%2520Zaiping%2520Lin%26entry.1292438233%3DHyperspectral%2520target%2520detection%2520%2528HTD%2529%2520aims%2520to%2520identify%2520specific%2520materials%2520based%2520on%2520spectral%2520information%2520in%2520hyperspectral%2520imagery%2520and%2520can%2520detect%2520extremely%2520small-sized%2520objects%252C%2520some%2520of%2520which%2520occupy%2520a%2520smaller%2520than%2520one-pixel%2520area.%2520However%252C%2520existing%2520HTD%2520methods%2520are%2520developed%2520based%2520on%2520per-pixel%2520binary%2520classification%252C%2520neglecting%2520the%2520three-dimensional%2520cube%2520structure%2520of%2520hyperspectral%2520images%2520%2528HSIs%2529%2520that%2520integrates%2520both%2520spatial%2520and%2520spectral%2520dimensions.%2520The%2520synergistic%2520existence%2520of%2520spatial%2520and%2520spectral%2520features%2520in%2520HSIs%2520enable%2520objects%2520to%2520simultaneously%2520exhibit%2520both%252C%2520yet%2520the%2520per-pixel%2520HTD%2520framework%2520limits%2520the%2520joint%2520expression%2520of%2520these%2520features.%2520In%2520this%2520paper%252C%2520we%2520rethink%2520HTD%2520from%2520the%2520perspective%2520of%2520spatial-spectral%2520synergistic%2520representation%2520and%2520propose%2520hyperspectral%2520point%2520object%2520detection%2520as%2520an%2520innovative%2520task%2520framework.%2520We%2520introduce%2520SpecDETR%252C%2520the%2520first%2520specialized%2520network%2520for%2520hyperspectral%2520multi-class%2520point%2520object%2520detection%252C%2520which%2520eliminates%2520dependence%2520on%2520pre-trained%2520backbone%2520networks%2520commonly%2520required%2520by%2520vision-based%2520object%2520detectors.%2520SpecDETR%2520uses%2520a%2520multi-layer%2520Transformer%2520encoder%2520with%2520self-excited%2520subpixel-scale%2520attention%2520modules%2520to%2520directly%2520extract%2520deep%2520spatial-spectral%2520joint%2520features%2520from%2520hyperspectral%2520cubes.%2520We%2520develop%2520a%2520simulated%2520hyperspectral%2520point%2520object%2520detection%2520benchmark%2520termed%2520SPOD%252C%2520and%2520for%2520the%2520first%2520time%252C%2520evaluate%2520and%2520compare%2520the%2520performance%2520of%2520visual%2520object%2520detection%2520networks%2520and%2520HTD%2520methods%2520on%2520hyperspectral%2520point%2520object%2520detection.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520SpecDETR%2520outperforms%2520SOTA%2520visual%2520object%2520detection%2520networks%2520and%2520HTD%2520methods.%2520Our%2520code%2520and%2520dataset%2520are%2520available%2520at%2520https%253A//github.com/ZhaoxuLi123/SpecDETR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10148v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecDETR%3A%20A%20transformer-based%20hyperspectral%20point%20object%20detection%20network&entry.906535625=Zhaoxu%20Li%20and%20Wei%20An%20and%20Gaowei%20Guo%20and%20Longguang%20Wang%20and%20Yingqian%20Wang%20and%20Zaiping%20Lin&entry.1292438233=Hyperspectral%20target%20detection%20%28HTD%29%20aims%20to%20identify%20specific%20materials%20based%20on%20spectral%20information%20in%20hyperspectral%20imagery%20and%20can%20detect%20extremely%20small-sized%20objects%2C%20some%20of%20which%20occupy%20a%20smaller%20than%20one-pixel%20area.%20However%2C%20existing%20HTD%20methods%20are%20developed%20based%20on%20per-pixel%20binary%20classification%2C%20neglecting%20the%20three-dimensional%20cube%20structure%20of%20hyperspectral%20images%20%28HSIs%29%20that%20integrates%20both%20spatial%20and%20spectral%20dimensions.%20The%20synergistic%20existence%20of%20spatial%20and%20spectral%20features%20in%20HSIs%20enable%20objects%20to%20simultaneously%20exhibit%20both%2C%20yet%20the%20per-pixel%20HTD%20framework%20limits%20the%20joint%20expression%20of%20these%20features.%20In%20this%20paper%2C%20we%20rethink%20HTD%20from%20the%20perspective%20of%20spatial-spectral%20synergistic%20representation%20and%20propose%20hyperspectral%20point%20object%20detection%20as%20an%20innovative%20task%20framework.%20We%20introduce%20SpecDETR%2C%20the%20first%20specialized%20network%20for%20hyperspectral%20multi-class%20point%20object%20detection%2C%20which%20eliminates%20dependence%20on%20pre-trained%20backbone%20networks%20commonly%20required%20by%20vision-based%20object%20detectors.%20SpecDETR%20uses%20a%20multi-layer%20Transformer%20encoder%20with%20self-excited%20subpixel-scale%20attention%20modules%20to%20directly%20extract%20deep%20spatial-spectral%20joint%20features%20from%20hyperspectral%20cubes.%20We%20develop%20a%20simulated%20hyperspectral%20point%20object%20detection%20benchmark%20termed%20SPOD%2C%20and%20for%20the%20first%20time%2C%20evaluate%20and%20compare%20the%20performance%20of%20visual%20object%20detection%20networks%20and%20HTD%20methods%20on%20hyperspectral%20point%20object%20detection.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20SpecDETR%20outperforms%20SOTA%20visual%20object%20detection%20networks%20and%20HTD%20methods.%20Our%20code%20and%20dataset%20are%20available%20at%20https%3A//github.com/ZhaoxuLi123/SpecDETR.&entry.1838667208=http%3A//arxiv.org/abs/2405.10148v4&entry.124074799=Read"},
{"title": "Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model", "author": "Sam Gijsen and Marc-Andre Schulz and Kerstin Ritter", "abstract": "The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.", "link": "http://arxiv.org/abs/2512.11582v1", "date": "2025-12-12", "relevancy": 2.707, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-Semantoks%3A%20Learning%20Semantic%20Tokens%20of%20Brain%20Dynamics%20with%20a%20Self-Distilled%20Foundation%20Model&body=Title%3A%20Brain-Semantoks%3A%20Learning%20Semantic%20Tokens%20of%20Brain%20Dynamics%20with%20a%20Self-Distilled%20Foundation%20Model%0AAuthor%3A%20Sam%20Gijsen%20and%20Marc-Andre%20Schulz%20and%20Kerstin%20Ritter%0AAbstract%3A%20The%20development%20of%20foundation%20models%20for%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20time%20series%20holds%20significant%20promise%20for%20predicting%20phenotypes%20related%20to%20disease%20and%20cognition.%20Current%20models%2C%20however%2C%20are%20often%20trained%20using%20a%20mask-and-reconstruct%20objective%20on%20small%20brain%20regions.%20This%20focus%20on%20low-level%20information%20leads%20to%20representations%20that%20are%20sensitive%20to%20noise%20and%20temporal%20fluctuations%2C%20necessitating%20extensive%20fine-tuning%20for%20downstream%20tasks.%20We%20introduce%20Brain-Semantoks%2C%20a%20self-supervised%20framework%20designed%20specifically%20to%20learn%20abstract%20representations%20of%20brain%20dynamics.%20Its%20architecture%20is%20built%20on%20two%20core%20innovations%3A%20a%20semantic%20tokenizer%20that%20aggregates%20noisy%20regional%20signals%20into%20robust%20tokens%20representing%20functional%20networks%2C%20and%20a%20self-distillation%20objective%20that%20enforces%20representational%20stability%20across%20time.%20We%20show%20that%20this%20objective%20is%20stabilized%20through%20a%20novel%20training%20curriculum%2C%20ensuring%20the%20model%20robustly%20learns%20meaningful%20features%20from%20low%20signal-to-noise%20time%20series.%20We%20demonstrate%20that%20learned%20representations%20enable%20strong%20performance%20on%20a%20variety%20of%20downstream%20tasks%20even%20when%20only%20using%20a%20linear%20probe.%20Furthermore%2C%20we%20provide%20comprehensive%20scaling%20analyses%20indicating%20more%20unlabeled%20data%20reliably%20results%20in%20out-of-distribution%20performance%20gains%20without%20domain%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-Semantoks%253A%2520Learning%2520Semantic%2520Tokens%2520of%2520Brain%2520Dynamics%2520with%2520a%2520Self-Distilled%2520Foundation%2520Model%26entry.906535625%3DSam%2520Gijsen%2520and%2520Marc-Andre%2520Schulz%2520and%2520Kerstin%2520Ritter%26entry.1292438233%3DThe%2520development%2520of%2520foundation%2520models%2520for%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529%2520time%2520series%2520holds%2520significant%2520promise%2520for%2520predicting%2520phenotypes%2520related%2520to%2520disease%2520and%2520cognition.%2520Current%2520models%252C%2520however%252C%2520are%2520often%2520trained%2520using%2520a%2520mask-and-reconstruct%2520objective%2520on%2520small%2520brain%2520regions.%2520This%2520focus%2520on%2520low-level%2520information%2520leads%2520to%2520representations%2520that%2520are%2520sensitive%2520to%2520noise%2520and%2520temporal%2520fluctuations%252C%2520necessitating%2520extensive%2520fine-tuning%2520for%2520downstream%2520tasks.%2520We%2520introduce%2520Brain-Semantoks%252C%2520a%2520self-supervised%2520framework%2520designed%2520specifically%2520to%2520learn%2520abstract%2520representations%2520of%2520brain%2520dynamics.%2520Its%2520architecture%2520is%2520built%2520on%2520two%2520core%2520innovations%253A%2520a%2520semantic%2520tokenizer%2520that%2520aggregates%2520noisy%2520regional%2520signals%2520into%2520robust%2520tokens%2520representing%2520functional%2520networks%252C%2520and%2520a%2520self-distillation%2520objective%2520that%2520enforces%2520representational%2520stability%2520across%2520time.%2520We%2520show%2520that%2520this%2520objective%2520is%2520stabilized%2520through%2520a%2520novel%2520training%2520curriculum%252C%2520ensuring%2520the%2520model%2520robustly%2520learns%2520meaningful%2520features%2520from%2520low%2520signal-to-noise%2520time%2520series.%2520We%2520demonstrate%2520that%2520learned%2520representations%2520enable%2520strong%2520performance%2520on%2520a%2520variety%2520of%2520downstream%2520tasks%2520even%2520when%2520only%2520using%2520a%2520linear%2520probe.%2520Furthermore%252C%2520we%2520provide%2520comprehensive%2520scaling%2520analyses%2520indicating%2520more%2520unlabeled%2520data%2520reliably%2520results%2520in%2520out-of-distribution%2520performance%2520gains%2520without%2520domain%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-Semantoks%3A%20Learning%20Semantic%20Tokens%20of%20Brain%20Dynamics%20with%20a%20Self-Distilled%20Foundation%20Model&entry.906535625=Sam%20Gijsen%20and%20Marc-Andre%20Schulz%20and%20Kerstin%20Ritter&entry.1292438233=The%20development%20of%20foundation%20models%20for%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20time%20series%20holds%20significant%20promise%20for%20predicting%20phenotypes%20related%20to%20disease%20and%20cognition.%20Current%20models%2C%20however%2C%20are%20often%20trained%20using%20a%20mask-and-reconstruct%20objective%20on%20small%20brain%20regions.%20This%20focus%20on%20low-level%20information%20leads%20to%20representations%20that%20are%20sensitive%20to%20noise%20and%20temporal%20fluctuations%2C%20necessitating%20extensive%20fine-tuning%20for%20downstream%20tasks.%20We%20introduce%20Brain-Semantoks%2C%20a%20self-supervised%20framework%20designed%20specifically%20to%20learn%20abstract%20representations%20of%20brain%20dynamics.%20Its%20architecture%20is%20built%20on%20two%20core%20innovations%3A%20a%20semantic%20tokenizer%20that%20aggregates%20noisy%20regional%20signals%20into%20robust%20tokens%20representing%20functional%20networks%2C%20and%20a%20self-distillation%20objective%20that%20enforces%20representational%20stability%20across%20time.%20We%20show%20that%20this%20objective%20is%20stabilized%20through%20a%20novel%20training%20curriculum%2C%20ensuring%20the%20model%20robustly%20learns%20meaningful%20features%20from%20low%20signal-to-noise%20time%20series.%20We%20demonstrate%20that%20learned%20representations%20enable%20strong%20performance%20on%20a%20variety%20of%20downstream%20tasks%20even%20when%20only%20using%20a%20linear%20probe.%20Furthermore%2C%20we%20provide%20comprehensive%20scaling%20analyses%20indicating%20more%20unlabeled%20data%20reliably%20results%20in%20out-of-distribution%20performance%20gains%20without%20domain%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2512.11582v1&entry.124074799=Read"},
{"title": "Equivariant symmetry-aware head pose estimation for fetal MRI", "author": "Ramya Muthukrishnan and Borjan Gagoski and Aryn Lee and P. Ellen Grant and Elfar Adalsteinsson and Polina Golland and Benjamin Billot", "abstract": "We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.", "link": "http://arxiv.org/abs/2512.04890v3", "date": "2025-12-12", "relevancy": 2.6916, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5514}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5318}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20symmetry-aware%20head%20pose%20estimation%20for%20fetal%20MRI&body=Title%3A%20Equivariant%20symmetry-aware%20head%20pose%20estimation%20for%20fetal%20MRI%0AAuthor%3A%20Ramya%20Muthukrishnan%20and%20Borjan%20Gagoski%20and%20Aryn%20Lee%20and%20P.%20Ellen%20Grant%20and%20Elfar%20Adalsteinsson%20and%20Polina%20Golland%20and%20Benjamin%20Billot%0AAbstract%3A%20We%20present%20E%283%29-Pose%2C%20a%20novel%20fast%20pose%20estimation%20method%20that%20jointly%20and%20explicitly%20models%20rotation%20equivariance%20and%20object%20symmetry.%20Our%20work%20is%20motivated%20by%20the%20challenging%20problem%20of%20accounting%20for%20fetal%20head%20motion%20during%20a%20diagnostic%20MRI%20scan.%20We%20aim%20to%20enable%20automatic%20adaptive%20prescription%20of%202D%20diagnostic%20MRI%20slices%20with%206-DoF%20head%20pose%20estimation%2C%20supported%20by%203D%20MRI%20volumes%20rapidly%20acquired%20before%20each%202D%20slice.%20Existing%20methods%20struggle%20to%20generalize%20to%20clinical%20volumes%2C%20due%20to%20pose%20ambiguities%20induced%20by%20inherent%20anatomical%20symmetries%2C%20as%20well%20as%20low%20resolution%2C%20noise%2C%20and%20artifacts.%20In%20contrast%2C%20E%283%29-Pose%20captures%20anatomical%20symmetries%20and%20rigid%20pose%20equivariance%20by%20construction%2C%20and%20yields%20robust%20estimates%20of%20the%20fetal%20head%20pose.%20Our%20experiments%20on%20publicly%20available%20and%20representative%20clinical%20fetal%20MRI%20datasets%20demonstrate%20the%20superior%20robustness%20and%20generalization%20of%20our%20method%20across%20domains.%20Crucially%2C%20E%283%29-Pose%20achieves%20state-of-the-art%20accuracy%20on%20clinical%20MRI%20volumes%2C%20paving%20the%20way%20for%20clinical%20translation.%20Our%20implementation%20is%20available%20at%20github.com/ramyamut/E3-Pose.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04890v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520symmetry-aware%2520head%2520pose%2520estimation%2520for%2520fetal%2520MRI%26entry.906535625%3DRamya%2520Muthukrishnan%2520and%2520Borjan%2520Gagoski%2520and%2520Aryn%2520Lee%2520and%2520P.%2520Ellen%2520Grant%2520and%2520Elfar%2520Adalsteinsson%2520and%2520Polina%2520Golland%2520and%2520Benjamin%2520Billot%26entry.1292438233%3DWe%2520present%2520E%25283%2529-Pose%252C%2520a%2520novel%2520fast%2520pose%2520estimation%2520method%2520that%2520jointly%2520and%2520explicitly%2520models%2520rotation%2520equivariance%2520and%2520object%2520symmetry.%2520Our%2520work%2520is%2520motivated%2520by%2520the%2520challenging%2520problem%2520of%2520accounting%2520for%2520fetal%2520head%2520motion%2520during%2520a%2520diagnostic%2520MRI%2520scan.%2520We%2520aim%2520to%2520enable%2520automatic%2520adaptive%2520prescription%2520of%25202D%2520diagnostic%2520MRI%2520slices%2520with%25206-DoF%2520head%2520pose%2520estimation%252C%2520supported%2520by%25203D%2520MRI%2520volumes%2520rapidly%2520acquired%2520before%2520each%25202D%2520slice.%2520Existing%2520methods%2520struggle%2520to%2520generalize%2520to%2520clinical%2520volumes%252C%2520due%2520to%2520pose%2520ambiguities%2520induced%2520by%2520inherent%2520anatomical%2520symmetries%252C%2520as%2520well%2520as%2520low%2520resolution%252C%2520noise%252C%2520and%2520artifacts.%2520In%2520contrast%252C%2520E%25283%2529-Pose%2520captures%2520anatomical%2520symmetries%2520and%2520rigid%2520pose%2520equivariance%2520by%2520construction%252C%2520and%2520yields%2520robust%2520estimates%2520of%2520the%2520fetal%2520head%2520pose.%2520Our%2520experiments%2520on%2520publicly%2520available%2520and%2520representative%2520clinical%2520fetal%2520MRI%2520datasets%2520demonstrate%2520the%2520superior%2520robustness%2520and%2520generalization%2520of%2520our%2520method%2520across%2520domains.%2520Crucially%252C%2520E%25283%2529-Pose%2520achieves%2520state-of-the-art%2520accuracy%2520on%2520clinical%2520MRI%2520volumes%252C%2520paving%2520the%2520way%2520for%2520clinical%2520translation.%2520Our%2520implementation%2520is%2520available%2520at%2520github.com/ramyamut/E3-Pose.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04890v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20symmetry-aware%20head%20pose%20estimation%20for%20fetal%20MRI&entry.906535625=Ramya%20Muthukrishnan%20and%20Borjan%20Gagoski%20and%20Aryn%20Lee%20and%20P.%20Ellen%20Grant%20and%20Elfar%20Adalsteinsson%20and%20Polina%20Golland%20and%20Benjamin%20Billot&entry.1292438233=We%20present%20E%283%29-Pose%2C%20a%20novel%20fast%20pose%20estimation%20method%20that%20jointly%20and%20explicitly%20models%20rotation%20equivariance%20and%20object%20symmetry.%20Our%20work%20is%20motivated%20by%20the%20challenging%20problem%20of%20accounting%20for%20fetal%20head%20motion%20during%20a%20diagnostic%20MRI%20scan.%20We%20aim%20to%20enable%20automatic%20adaptive%20prescription%20of%202D%20diagnostic%20MRI%20slices%20with%206-DoF%20head%20pose%20estimation%2C%20supported%20by%203D%20MRI%20volumes%20rapidly%20acquired%20before%20each%202D%20slice.%20Existing%20methods%20struggle%20to%20generalize%20to%20clinical%20volumes%2C%20due%20to%20pose%20ambiguities%20induced%20by%20inherent%20anatomical%20symmetries%2C%20as%20well%20as%20low%20resolution%2C%20noise%2C%20and%20artifacts.%20In%20contrast%2C%20E%283%29-Pose%20captures%20anatomical%20symmetries%20and%20rigid%20pose%20equivariance%20by%20construction%2C%20and%20yields%20robust%20estimates%20of%20the%20fetal%20head%20pose.%20Our%20experiments%20on%20publicly%20available%20and%20representative%20clinical%20fetal%20MRI%20datasets%20demonstrate%20the%20superior%20robustness%20and%20generalization%20of%20our%20method%20across%20domains.%20Crucially%2C%20E%283%29-Pose%20achieves%20state-of-the-art%20accuracy%20on%20clinical%20MRI%20volumes%2C%20paving%20the%20way%20for%20clinical%20translation.%20Our%20implementation%20is%20available%20at%20github.com/ramyamut/E3-Pose.&entry.1838667208=http%3A//arxiv.org/abs/2512.04890v3&entry.124074799=Read"},
{"title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation", "author": "Jingmin Zhu and Anqi Zhu and Hossein Rahmani and Jun Liu and Mohammed Bennamoun and Qiuhong Ke", "abstract": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.", "link": "http://arxiv.org/abs/2512.11458v1", "date": "2025-12-12", "relevancy": 2.635, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5425}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5219}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Skeleton-based%20Zero-Shot%20Action%20Recognition%20with%20Training-Free%20Test-Time%20Adaptation&body=Title%3A%20Boosting%20Skeleton-based%20Zero-Shot%20Action%20Recognition%20with%20Training-Free%20Test-Time%20Adaptation%0AAuthor%3A%20Jingmin%20Zhu%20and%20Anqi%20Zhu%20and%20Hossein%20Rahmani%20and%20Jun%20Liu%20and%20Mohammed%20Bennamoun%20and%20Qiuhong%20Ke%0AAbstract%3A%20We%20introduce%20Skeleton-Cache%2C%20the%20first%20training-free%20test-time%20adaptation%20framework%20for%20skeleton-based%20zero-shot%20action%20recognition%20%28SZAR%29%2C%20aimed%20at%20improving%20model%20generalization%20to%20unseen%20actions%20during%20inference.%20Skeleton-Cache%20reformulates%20inference%20as%20a%20lightweight%20retrieval%20process%20over%20a%20non-parametric%20cache%20that%20stores%20structured%20skeleton%20representations%2C%20combining%20both%20global%20and%20fine-grained%20local%20descriptors.%20To%20guide%20the%20fusion%20of%20descriptor-wise%20predictions%2C%20we%20leverage%20the%20semantic%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20to%20assign%20class-specific%20importance%20weights.%20By%20integrating%20these%20structured%20descriptors%20with%20LLM-guided%20semantic%20priors%2C%20Skeleton-Cache%20dynamically%20adapts%20to%20unseen%20actions%20without%20any%20additional%20training%20or%20access%20to%20training%20data.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2060/120%20and%20PKU-MMD%20II%20demonstrate%20that%20Skeleton-Cache%20consistently%20boosts%20the%20performance%20of%20various%20SZAR%20backbones%20under%20both%20zero-shot%20and%20generalized%20zero-shot%20settings.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/Alchemist0754/Skeleton-Cache.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Skeleton-based%2520Zero-Shot%2520Action%2520Recognition%2520with%2520Training-Free%2520Test-Time%2520Adaptation%26entry.906535625%3DJingmin%2520Zhu%2520and%2520Anqi%2520Zhu%2520and%2520Hossein%2520Rahmani%2520and%2520Jun%2520Liu%2520and%2520Mohammed%2520Bennamoun%2520and%2520Qiuhong%2520Ke%26entry.1292438233%3DWe%2520introduce%2520Skeleton-Cache%252C%2520the%2520first%2520training-free%2520test-time%2520adaptation%2520framework%2520for%2520skeleton-based%2520zero-shot%2520action%2520recognition%2520%2528SZAR%2529%252C%2520aimed%2520at%2520improving%2520model%2520generalization%2520to%2520unseen%2520actions%2520during%2520inference.%2520Skeleton-Cache%2520reformulates%2520inference%2520as%2520a%2520lightweight%2520retrieval%2520process%2520over%2520a%2520non-parametric%2520cache%2520that%2520stores%2520structured%2520skeleton%2520representations%252C%2520combining%2520both%2520global%2520and%2520fine-grained%2520local%2520descriptors.%2520To%2520guide%2520the%2520fusion%2520of%2520descriptor-wise%2520predictions%252C%2520we%2520leverage%2520the%2520semantic%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520assign%2520class-specific%2520importance%2520weights.%2520By%2520integrating%2520these%2520structured%2520descriptors%2520with%2520LLM-guided%2520semantic%2520priors%252C%2520Skeleton-Cache%2520dynamically%2520adapts%2520to%2520unseen%2520actions%2520without%2520any%2520additional%2520training%2520or%2520access%2520to%2520training%2520data.%2520Extensive%2520experiments%2520on%2520NTU%2520RGB%252BD%252060/120%2520and%2520PKU-MMD%2520II%2520demonstrate%2520that%2520Skeleton-Cache%2520consistently%2520boosts%2520the%2520performance%2520of%2520various%2520SZAR%2520backbones%2520under%2520both%2520zero-shot%2520and%2520generalized%2520zero-shot%2520settings.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Alchemist0754/Skeleton-Cache.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Skeleton-based%20Zero-Shot%20Action%20Recognition%20with%20Training-Free%20Test-Time%20Adaptation&entry.906535625=Jingmin%20Zhu%20and%20Anqi%20Zhu%20and%20Hossein%20Rahmani%20and%20Jun%20Liu%20and%20Mohammed%20Bennamoun%20and%20Qiuhong%20Ke&entry.1292438233=We%20introduce%20Skeleton-Cache%2C%20the%20first%20training-free%20test-time%20adaptation%20framework%20for%20skeleton-based%20zero-shot%20action%20recognition%20%28SZAR%29%2C%20aimed%20at%20improving%20model%20generalization%20to%20unseen%20actions%20during%20inference.%20Skeleton-Cache%20reformulates%20inference%20as%20a%20lightweight%20retrieval%20process%20over%20a%20non-parametric%20cache%20that%20stores%20structured%20skeleton%20representations%2C%20combining%20both%20global%20and%20fine-grained%20local%20descriptors.%20To%20guide%20the%20fusion%20of%20descriptor-wise%20predictions%2C%20we%20leverage%20the%20semantic%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20to%20assign%20class-specific%20importance%20weights.%20By%20integrating%20these%20structured%20descriptors%20with%20LLM-guided%20semantic%20priors%2C%20Skeleton-Cache%20dynamically%20adapts%20to%20unseen%20actions%20without%20any%20additional%20training%20or%20access%20to%20training%20data.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2060/120%20and%20PKU-MMD%20II%20demonstrate%20that%20Skeleton-Cache%20consistently%20boosts%20the%20performance%20of%20various%20SZAR%20backbones%20under%20both%20zero-shot%20and%20generalized%20zero-shot%20settings.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/Alchemist0754/Skeleton-Cache.&entry.1838667208=http%3A//arxiv.org/abs/2512.11458v1&entry.124074799=Read"},
{"title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation", "author": "Mohammed El Fallaki Idrissi and Jad Mounayer and Sebastian Rodriguez and Fodil Meraghni and Francisco Chinesta", "abstract": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.", "link": "http://arxiv.org/abs/2512.11748v1", "date": "2025-12-12", "relevancy": 2.6229, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5337}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5233}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Parametric%20Design%20%28GPD%29%3A%20A%20framework%20for%20real-time%20geometry%20generation%20and%20on-the-fly%20multiparametric%20approximation&body=Title%3A%20Generative%20Parametric%20Design%20%28GPD%29%3A%20A%20framework%20for%20real-time%20geometry%20generation%20and%20on-the-fly%20multiparametric%20approximation%0AAuthor%3A%20Mohammed%20El%20Fallaki%20Idrissi%20and%20Jad%20Mounayer%20and%20Sebastian%20Rodriguez%20and%20Fodil%20Meraghni%20and%20Francisco%20Chinesta%0AAbstract%3A%20This%20paper%20presents%20a%20novel%20paradigm%20in%20simulation-based%20engineering%20sciences%20by%20introducing%20a%20new%20framework%20called%20Generative%20Parametric%20Design%20%28GPD%29.%20The%20GPD%20framework%20enables%20the%20generation%20of%20new%20designs%20along%20with%20their%20corresponding%20parametric%20solutions%20given%20as%20a%20reduced%20basis.%20To%20achieve%20this%2C%20two%20Rank%20Reduction%20Autoencoders%20%28RRAEs%29%20are%20employed%2C%20one%20for%20encoding%20and%20generating%20the%20design%20or%20geometry%2C%20and%20the%20other%20for%20encoding%20the%20sparse%20Proper%20Generalized%20Decomposition%20%28sPGD%29%20mode%20solutions.%20These%20models%20are%20linked%20in%20the%20latent%20space%20using%20regression%20techniques%2C%20allowing%20efficient%20transitions%20between%20design%20and%20their%20associated%20sPGD%20modes.%20By%20empowering%20design%20exploration%20and%20optimization%2C%20this%20framework%20also%20advances%20digital%20and%20hybrid%20twin%20development%2C%20enhancing%20predictive%20modeling%20and%20real-time%20decision-making%20in%20engineering%20applications.%20The%20developed%20framework%20is%20demonstrated%20on%20two-phase%20microstructures%2C%20in%20which%20the%20multiparametric%20solutions%20account%20for%20variations%20in%20two%20key%20material%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Parametric%2520Design%2520%2528GPD%2529%253A%2520A%2520framework%2520for%2520real-time%2520geometry%2520generation%2520and%2520on-the-fly%2520multiparametric%2520approximation%26entry.906535625%3DMohammed%2520El%2520Fallaki%2520Idrissi%2520and%2520Jad%2520Mounayer%2520and%2520Sebastian%2520Rodriguez%2520and%2520Fodil%2520Meraghni%2520and%2520Francisco%2520Chinesta%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520novel%2520paradigm%2520in%2520simulation-based%2520engineering%2520sciences%2520by%2520introducing%2520a%2520new%2520framework%2520called%2520Generative%2520Parametric%2520Design%2520%2528GPD%2529.%2520The%2520GPD%2520framework%2520enables%2520the%2520generation%2520of%2520new%2520designs%2520along%2520with%2520their%2520corresponding%2520parametric%2520solutions%2520given%2520as%2520a%2520reduced%2520basis.%2520To%2520achieve%2520this%252C%2520two%2520Rank%2520Reduction%2520Autoencoders%2520%2528RRAEs%2529%2520are%2520employed%252C%2520one%2520for%2520encoding%2520and%2520generating%2520the%2520design%2520or%2520geometry%252C%2520and%2520the%2520other%2520for%2520encoding%2520the%2520sparse%2520Proper%2520Generalized%2520Decomposition%2520%2528sPGD%2529%2520mode%2520solutions.%2520These%2520models%2520are%2520linked%2520in%2520the%2520latent%2520space%2520using%2520regression%2520techniques%252C%2520allowing%2520efficient%2520transitions%2520between%2520design%2520and%2520their%2520associated%2520sPGD%2520modes.%2520By%2520empowering%2520design%2520exploration%2520and%2520optimization%252C%2520this%2520framework%2520also%2520advances%2520digital%2520and%2520hybrid%2520twin%2520development%252C%2520enhancing%2520predictive%2520modeling%2520and%2520real-time%2520decision-making%2520in%2520engineering%2520applications.%2520The%2520developed%2520framework%2520is%2520demonstrated%2520on%2520two-phase%2520microstructures%252C%2520in%2520which%2520the%2520multiparametric%2520solutions%2520account%2520for%2520variations%2520in%2520two%2520key%2520material%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Parametric%20Design%20%28GPD%29%3A%20A%20framework%20for%20real-time%20geometry%20generation%20and%20on-the-fly%20multiparametric%20approximation&entry.906535625=Mohammed%20El%20Fallaki%20Idrissi%20and%20Jad%20Mounayer%20and%20Sebastian%20Rodriguez%20and%20Fodil%20Meraghni%20and%20Francisco%20Chinesta&entry.1292438233=This%20paper%20presents%20a%20novel%20paradigm%20in%20simulation-based%20engineering%20sciences%20by%20introducing%20a%20new%20framework%20called%20Generative%20Parametric%20Design%20%28GPD%29.%20The%20GPD%20framework%20enables%20the%20generation%20of%20new%20designs%20along%20with%20their%20corresponding%20parametric%20solutions%20given%20as%20a%20reduced%20basis.%20To%20achieve%20this%2C%20two%20Rank%20Reduction%20Autoencoders%20%28RRAEs%29%20are%20employed%2C%20one%20for%20encoding%20and%20generating%20the%20design%20or%20geometry%2C%20and%20the%20other%20for%20encoding%20the%20sparse%20Proper%20Generalized%20Decomposition%20%28sPGD%29%20mode%20solutions.%20These%20models%20are%20linked%20in%20the%20latent%20space%20using%20regression%20techniques%2C%20allowing%20efficient%20transitions%20between%20design%20and%20their%20associated%20sPGD%20modes.%20By%20empowering%20design%20exploration%20and%20optimization%2C%20this%20framework%20also%20advances%20digital%20and%20hybrid%20twin%20development%2C%20enhancing%20predictive%20modeling%20and%20real-time%20decision-making%20in%20engineering%20applications.%20The%20developed%20framework%20is%20demonstrated%20on%20two-phase%20microstructures%2C%20in%20which%20the%20multiparametric%20solutions%20account%20for%20variations%20in%20two%20key%20material%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2512.11748v1&entry.124074799=Read"},
{"title": "Embodied Image Compression", "author": "Chunyi Li and Rui Qing and Jianbo Zhang and Yuan Tian and Xiangyang Zhu and Zicheng Zhang and Xiaohong Liu and Weisi Lin and Guangtao Zhai", "abstract": "Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.", "link": "http://arxiv.org/abs/2512.11612v1", "date": "2025-12-12", "relevancy": 2.6096, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied%20Image%20Compression&body=Title%3A%20Embodied%20Image%20Compression%0AAuthor%3A%20Chunyi%20Li%20and%20Rui%20Qing%20and%20Jianbo%20Zhang%20and%20Yuan%20Tian%20and%20Xiangyang%20Zhu%20and%20Zicheng%20Zhang%20and%20Xiaohong%20Liu%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai%0AAbstract%3A%20Image%20Compression%20for%20Machines%20%28ICM%29%20has%20emerged%20as%20a%20pivotal%20research%20direction%20in%20the%20field%20of%20visual%20data%20compression.%20However%2C%20with%20the%20rapid%20evolution%20of%20machine%20intelligence%2C%20the%20target%20of%20compression%20has%20shifted%20from%20task-specific%20virtual%20models%20to%20Embodied%20agents%20operating%20in%20real-world%20environments.%20To%20address%20the%20communication%20constraints%20of%20Embodied%20AI%20in%20multi-agent%20systems%20and%20ensure%20real-time%20task%20execution%2C%20this%20paper%20introduces%2C%20for%20the%20first%20time%2C%20the%20scientific%20problem%20of%20Embodied%20Image%20Compression.%20We%20establish%20a%20standardized%20benchmark%2C%20EmbodiedComp%2C%20to%20facilitate%20systematic%20evaluation%20under%20ultra-low%20bitrate%20conditions%20in%20a%20closed-loop%20setting.%20Through%20extensive%20empirical%20studies%20in%20both%20simulated%20and%20real-world%20settings%2C%20we%20demonstrate%20that%20existing%20Vision-Language-Action%20models%20%28VLAs%29%20fail%20to%20reliably%20perform%20even%20simple%20manipulation%20tasks%20when%20compressed%20below%20the%20Embodied%20bitrate%20threshold.%20We%20anticipate%20that%20EmbodiedComp%20will%20catalyze%20the%20development%20of%20domain-specific%20compression%20tailored%20for%20Embodied%20agents%20%2C%20thereby%20accelerating%20the%20Embodied%20AI%20deployment%20in%20the%20Real-world.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied%2520Image%2520Compression%26entry.906535625%3DChunyi%2520Li%2520and%2520Rui%2520Qing%2520and%2520Jianbo%2520Zhang%2520and%2520Yuan%2520Tian%2520and%2520Xiangyang%2520Zhu%2520and%2520Zicheng%2520Zhang%2520and%2520Xiaohong%2520Liu%2520and%2520Weisi%2520Lin%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3DImage%2520Compression%2520for%2520Machines%2520%2528ICM%2529%2520has%2520emerged%2520as%2520a%2520pivotal%2520research%2520direction%2520in%2520the%2520field%2520of%2520visual%2520data%2520compression.%2520However%252C%2520with%2520the%2520rapid%2520evolution%2520of%2520machine%2520intelligence%252C%2520the%2520target%2520of%2520compression%2520has%2520shifted%2520from%2520task-specific%2520virtual%2520models%2520to%2520Embodied%2520agents%2520operating%2520in%2520real-world%2520environments.%2520To%2520address%2520the%2520communication%2520constraints%2520of%2520Embodied%2520AI%2520in%2520multi-agent%2520systems%2520and%2520ensure%2520real-time%2520task%2520execution%252C%2520this%2520paper%2520introduces%252C%2520for%2520the%2520first%2520time%252C%2520the%2520scientific%2520problem%2520of%2520Embodied%2520Image%2520Compression.%2520We%2520establish%2520a%2520standardized%2520benchmark%252C%2520EmbodiedComp%252C%2520to%2520facilitate%2520systematic%2520evaluation%2520under%2520ultra-low%2520bitrate%2520conditions%2520in%2520a%2520closed-loop%2520setting.%2520Through%2520extensive%2520empirical%2520studies%2520in%2520both%2520simulated%2520and%2520real-world%2520settings%252C%2520we%2520demonstrate%2520that%2520existing%2520Vision-Language-Action%2520models%2520%2528VLAs%2529%2520fail%2520to%2520reliably%2520perform%2520even%2520simple%2520manipulation%2520tasks%2520when%2520compressed%2520below%2520the%2520Embodied%2520bitrate%2520threshold.%2520We%2520anticipate%2520that%2520EmbodiedComp%2520will%2520catalyze%2520the%2520development%2520of%2520domain-specific%2520compression%2520tailored%2520for%2520Embodied%2520agents%2520%252C%2520thereby%2520accelerating%2520the%2520Embodied%2520AI%2520deployment%2520in%2520the%2520Real-world.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20Image%20Compression&entry.906535625=Chunyi%20Li%20and%20Rui%20Qing%20and%20Jianbo%20Zhang%20and%20Yuan%20Tian%20and%20Xiangyang%20Zhu%20and%20Zicheng%20Zhang%20and%20Xiaohong%20Liu%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai&entry.1292438233=Image%20Compression%20for%20Machines%20%28ICM%29%20has%20emerged%20as%20a%20pivotal%20research%20direction%20in%20the%20field%20of%20visual%20data%20compression.%20However%2C%20with%20the%20rapid%20evolution%20of%20machine%20intelligence%2C%20the%20target%20of%20compression%20has%20shifted%20from%20task-specific%20virtual%20models%20to%20Embodied%20agents%20operating%20in%20real-world%20environments.%20To%20address%20the%20communication%20constraints%20of%20Embodied%20AI%20in%20multi-agent%20systems%20and%20ensure%20real-time%20task%20execution%2C%20this%20paper%20introduces%2C%20for%20the%20first%20time%2C%20the%20scientific%20problem%20of%20Embodied%20Image%20Compression.%20We%20establish%20a%20standardized%20benchmark%2C%20EmbodiedComp%2C%20to%20facilitate%20systematic%20evaluation%20under%20ultra-low%20bitrate%20conditions%20in%20a%20closed-loop%20setting.%20Through%20extensive%20empirical%20studies%20in%20both%20simulated%20and%20real-world%20settings%2C%20we%20demonstrate%20that%20existing%20Vision-Language-Action%20models%20%28VLAs%29%20fail%20to%20reliably%20perform%20even%20simple%20manipulation%20tasks%20when%20compressed%20below%20the%20Embodied%20bitrate%20threshold.%20We%20anticipate%20that%20EmbodiedComp%20will%20catalyze%20the%20development%20of%20domain-specific%20compression%20tailored%20for%20Embodied%20agents%20%2C%20thereby%20accelerating%20the%20Embodied%20AI%20deployment%20in%20the%20Real-world.&entry.1838667208=http%3A//arxiv.org/abs/2512.11612v1&entry.124074799=Read"},
{"title": "Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images", "author": "Lin Bai and Xiaoyang Li and Liqiang Huang and Quynh Nguyen and Hien Van Nguyen and Saurabh Prasad and Dragan Maric and John Redell and Pramod Dash and Badrinath Roysam", "abstract": "We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.", "link": "http://arxiv.org/abs/2512.11722v1", "date": "2025-12-12", "relevancy": 2.5978, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5374}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weak-to-Strong%20Generalization%20Enables%20Fully%20Automated%20De%20Novo%20Training%20of%20Multi-head%20Mask-RCNN%20Model%20for%20Segmenting%20Densely%20Overlapping%20Cell%20Nuclei%20in%20Multiplex%20Whole-slice%20Brain%20Images&body=Title%3A%20Weak-to-Strong%20Generalization%20Enables%20Fully%20Automated%20De%20Novo%20Training%20of%20Multi-head%20Mask-RCNN%20Model%20for%20Segmenting%20Densely%20Overlapping%20Cell%20Nuclei%20in%20Multiplex%20Whole-slice%20Brain%20Images%0AAuthor%3A%20Lin%20Bai%20and%20Xiaoyang%20Li%20and%20Liqiang%20Huang%20and%20Quynh%20Nguyen%20and%20Hien%20Van%20Nguyen%20and%20Saurabh%20Prasad%20and%20Dragan%20Maric%20and%20John%20Redell%20and%20Pramod%20Dash%20and%20Badrinath%20Roysam%0AAbstract%3A%20We%20present%20a%20weak%20to%20strong%20generalization%20methodology%20for%20fully%20automated%20training%20of%20a%20multi-head%20extension%20of%20the%20Mask-RCNN%20method%20with%20efficient%20channel%20attention%20for%20reliable%20segmentation%20of%20overlapping%20cell%20nuclei%20in%20multiplex%20cyclic%20immunofluorescent%20%28IF%29%20whole-slide%20images%20%28WSI%29%2C%20and%20present%20evidence%20for%20pseudo-label%20correction%20and%20coverage%20expansion%2C%20the%20key%20phenomena%20underlying%20weak%20to%20strong%20generalization.%20This%20method%20can%20learn%20to%20segment%20de%20novo%20a%20new%20class%20of%20images%20from%20a%20new%20instrument%20and/or%20a%20new%20imaging%20protocol%20without%20the%20need%20for%20human%20annotations.%20We%20also%20present%20metrics%20for%20automated%20self-diagnosis%20of%20segmentation%20quality%20in%20production%20environments%2C%20where%20human%20visual%20proofreading%20of%20massive%20WSI%20images%20is%20unaffordable.%20Our%20method%20was%20benchmarked%20against%20five%20current%20widely%20used%20methods%20and%20showed%20a%20significant%20improvement.%20The%20code%2C%20sample%20WSI%20images%2C%20and%20high-resolution%20segmentation%20results%20are%20provided%20in%20open%20form%20for%20community%20adoption%20and%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeak-to-Strong%2520Generalization%2520Enables%2520Fully%2520Automated%2520De%2520Novo%2520Training%2520of%2520Multi-head%2520Mask-RCNN%2520Model%2520for%2520Segmenting%2520Densely%2520Overlapping%2520Cell%2520Nuclei%2520in%2520Multiplex%2520Whole-slice%2520Brain%2520Images%26entry.906535625%3DLin%2520Bai%2520and%2520Xiaoyang%2520Li%2520and%2520Liqiang%2520Huang%2520and%2520Quynh%2520Nguyen%2520and%2520Hien%2520Van%2520Nguyen%2520and%2520Saurabh%2520Prasad%2520and%2520Dragan%2520Maric%2520and%2520John%2520Redell%2520and%2520Pramod%2520Dash%2520and%2520Badrinath%2520Roysam%26entry.1292438233%3DWe%2520present%2520a%2520weak%2520to%2520strong%2520generalization%2520methodology%2520for%2520fully%2520automated%2520training%2520of%2520a%2520multi-head%2520extension%2520of%2520the%2520Mask-RCNN%2520method%2520with%2520efficient%2520channel%2520attention%2520for%2520reliable%2520segmentation%2520of%2520overlapping%2520cell%2520nuclei%2520in%2520multiplex%2520cyclic%2520immunofluorescent%2520%2528IF%2529%2520whole-slide%2520images%2520%2528WSI%2529%252C%2520and%2520present%2520evidence%2520for%2520pseudo-label%2520correction%2520and%2520coverage%2520expansion%252C%2520the%2520key%2520phenomena%2520underlying%2520weak%2520to%2520strong%2520generalization.%2520This%2520method%2520can%2520learn%2520to%2520segment%2520de%2520novo%2520a%2520new%2520class%2520of%2520images%2520from%2520a%2520new%2520instrument%2520and/or%2520a%2520new%2520imaging%2520protocol%2520without%2520the%2520need%2520for%2520human%2520annotations.%2520We%2520also%2520present%2520metrics%2520for%2520automated%2520self-diagnosis%2520of%2520segmentation%2520quality%2520in%2520production%2520environments%252C%2520where%2520human%2520visual%2520proofreading%2520of%2520massive%2520WSI%2520images%2520is%2520unaffordable.%2520Our%2520method%2520was%2520benchmarked%2520against%2520five%2520current%2520widely%2520used%2520methods%2520and%2520showed%2520a%2520significant%2520improvement.%2520The%2520code%252C%2520sample%2520WSI%2520images%252C%2520and%2520high-resolution%2520segmentation%2520results%2520are%2520provided%2520in%2520open%2520form%2520for%2520community%2520adoption%2520and%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weak-to-Strong%20Generalization%20Enables%20Fully%20Automated%20De%20Novo%20Training%20of%20Multi-head%20Mask-RCNN%20Model%20for%20Segmenting%20Densely%20Overlapping%20Cell%20Nuclei%20in%20Multiplex%20Whole-slice%20Brain%20Images&entry.906535625=Lin%20Bai%20and%20Xiaoyang%20Li%20and%20Liqiang%20Huang%20and%20Quynh%20Nguyen%20and%20Hien%20Van%20Nguyen%20and%20Saurabh%20Prasad%20and%20Dragan%20Maric%20and%20John%20Redell%20and%20Pramod%20Dash%20and%20Badrinath%20Roysam&entry.1292438233=We%20present%20a%20weak%20to%20strong%20generalization%20methodology%20for%20fully%20automated%20training%20of%20a%20multi-head%20extension%20of%20the%20Mask-RCNN%20method%20with%20efficient%20channel%20attention%20for%20reliable%20segmentation%20of%20overlapping%20cell%20nuclei%20in%20multiplex%20cyclic%20immunofluorescent%20%28IF%29%20whole-slide%20images%20%28WSI%29%2C%20and%20present%20evidence%20for%20pseudo-label%20correction%20and%20coverage%20expansion%2C%20the%20key%20phenomena%20underlying%20weak%20to%20strong%20generalization.%20This%20method%20can%20learn%20to%20segment%20de%20novo%20a%20new%20class%20of%20images%20from%20a%20new%20instrument%20and/or%20a%20new%20imaging%20protocol%20without%20the%20need%20for%20human%20annotations.%20We%20also%20present%20metrics%20for%20automated%20self-diagnosis%20of%20segmentation%20quality%20in%20production%20environments%2C%20where%20human%20visual%20proofreading%20of%20massive%20WSI%20images%20is%20unaffordable.%20Our%20method%20was%20benchmarked%20against%20five%20current%20widely%20used%20methods%20and%20showed%20a%20significant%20improvement.%20The%20code%2C%20sample%20WSI%20images%2C%20and%20high-resolution%20segmentation%20results%20are%20provided%20in%20open%20form%20for%20community%20adoption%20and%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2512.11722v1&entry.124074799=Read"},
{"title": "Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces", "author": "Arghya Pratihar and Arnab Seal and Swagatam Das and Inesh Chattopadhyay", "abstract": "Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs M\u00f6bius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.", "link": "http://arxiv.org/abs/2512.11448v1", "date": "2025-12-12", "relevancy": 2.5809, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.521}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5162}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperbolic%20Gaussian%20Blurring%20Mean%20Shift%3A%20A%20Statistical%20Mode-Seeking%20Framework%20for%20Clustering%20in%20Curved%20Spaces&body=Title%3A%20Hyperbolic%20Gaussian%20Blurring%20Mean%20Shift%3A%20A%20Statistical%20Mode-Seeking%20Framework%20for%20Clustering%20in%20Curved%20Spaces%0AAuthor%3A%20Arghya%20Pratihar%20and%20Arnab%20Seal%20and%20Swagatam%20Das%20and%20Inesh%20Chattopadhyay%0AAbstract%3A%20Clustering%20is%20a%20fundamental%20unsupervised%20learning%20task%20for%20uncovering%20patterns%20in%20data.%20While%20Gaussian%20Blurring%20Mean%20Shift%20%28GBMS%29%20has%20proven%20effective%20for%20identifying%20arbitrarily%20shaped%20clusters%20in%20Euclidean%20space%2C%20it%20struggles%20with%20datasets%20exhibiting%20hierarchical%20or%20tree-like%20structures.%20In%20this%20work%2C%20we%20introduce%20HypeGBMS%2C%20a%20novel%20extension%20of%20GBMS%20to%20hyperbolic%20space.%20Our%20method%20replaces%20Euclidean%20computations%20with%20hyperbolic%20distances%20and%20employs%20M%C3%B6bius-weighted%20means%20to%20ensure%20that%20all%20updates%20remain%20consistent%20with%20the%20geometry%20of%20the%20space.%20HypeGBMS%20effectively%20captures%20latent%20hierarchies%20while%20retaining%20the%20density-seeking%20behavior%20of%20GBMS.%20We%20provide%20theoretical%20insights%20into%20convergence%20and%20computational%20complexity%2C%20along%20with%20empirical%20results%20that%20demonstrate%20improved%20clustering%20quality%20in%20hierarchical%20datasets.%20This%20work%20bridges%20classical%20mean-shift%20clustering%20and%20hyperbolic%20representation%20learning%2C%20offering%20a%20principled%20approach%20to%20density-based%20clustering%20in%20curved%20spaces.%20Extensive%20experimental%20evaluations%20on%20%2411%24%20real-world%20datasets%20demonstrate%20that%20HypeGBMS%20significantly%20outperforms%20conventional%20mean-shift%20clustering%20methods%20in%20non-Euclidean%20settings%2C%20underscoring%20its%20robustness%20and%20effectiveness.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperbolic%2520Gaussian%2520Blurring%2520Mean%2520Shift%253A%2520A%2520Statistical%2520Mode-Seeking%2520Framework%2520for%2520Clustering%2520in%2520Curved%2520Spaces%26entry.906535625%3DArghya%2520Pratihar%2520and%2520Arnab%2520Seal%2520and%2520Swagatam%2520Das%2520and%2520Inesh%2520Chattopadhyay%26entry.1292438233%3DClustering%2520is%2520a%2520fundamental%2520unsupervised%2520learning%2520task%2520for%2520uncovering%2520patterns%2520in%2520data.%2520While%2520Gaussian%2520Blurring%2520Mean%2520Shift%2520%2528GBMS%2529%2520has%2520proven%2520effective%2520for%2520identifying%2520arbitrarily%2520shaped%2520clusters%2520in%2520Euclidean%2520space%252C%2520it%2520struggles%2520with%2520datasets%2520exhibiting%2520hierarchical%2520or%2520tree-like%2520structures.%2520In%2520this%2520work%252C%2520we%2520introduce%2520HypeGBMS%252C%2520a%2520novel%2520extension%2520of%2520GBMS%2520to%2520hyperbolic%2520space.%2520Our%2520method%2520replaces%2520Euclidean%2520computations%2520with%2520hyperbolic%2520distances%2520and%2520employs%2520M%25C3%25B6bius-weighted%2520means%2520to%2520ensure%2520that%2520all%2520updates%2520remain%2520consistent%2520with%2520the%2520geometry%2520of%2520the%2520space.%2520HypeGBMS%2520effectively%2520captures%2520latent%2520hierarchies%2520while%2520retaining%2520the%2520density-seeking%2520behavior%2520of%2520GBMS.%2520We%2520provide%2520theoretical%2520insights%2520into%2520convergence%2520and%2520computational%2520complexity%252C%2520along%2520with%2520empirical%2520results%2520that%2520demonstrate%2520improved%2520clustering%2520quality%2520in%2520hierarchical%2520datasets.%2520This%2520work%2520bridges%2520classical%2520mean-shift%2520clustering%2520and%2520hyperbolic%2520representation%2520learning%252C%2520offering%2520a%2520principled%2520approach%2520to%2520density-based%2520clustering%2520in%2520curved%2520spaces.%2520Extensive%2520experimental%2520evaluations%2520on%2520%252411%2524%2520real-world%2520datasets%2520demonstrate%2520that%2520HypeGBMS%2520significantly%2520outperforms%2520conventional%2520mean-shift%2520clustering%2520methods%2520in%2520non-Euclidean%2520settings%252C%2520underscoring%2520its%2520robustness%2520and%2520effectiveness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperbolic%20Gaussian%20Blurring%20Mean%20Shift%3A%20A%20Statistical%20Mode-Seeking%20Framework%20for%20Clustering%20in%20Curved%20Spaces&entry.906535625=Arghya%20Pratihar%20and%20Arnab%20Seal%20and%20Swagatam%20Das%20and%20Inesh%20Chattopadhyay&entry.1292438233=Clustering%20is%20a%20fundamental%20unsupervised%20learning%20task%20for%20uncovering%20patterns%20in%20data.%20While%20Gaussian%20Blurring%20Mean%20Shift%20%28GBMS%29%20has%20proven%20effective%20for%20identifying%20arbitrarily%20shaped%20clusters%20in%20Euclidean%20space%2C%20it%20struggles%20with%20datasets%20exhibiting%20hierarchical%20or%20tree-like%20structures.%20In%20this%20work%2C%20we%20introduce%20HypeGBMS%2C%20a%20novel%20extension%20of%20GBMS%20to%20hyperbolic%20space.%20Our%20method%20replaces%20Euclidean%20computations%20with%20hyperbolic%20distances%20and%20employs%20M%C3%B6bius-weighted%20means%20to%20ensure%20that%20all%20updates%20remain%20consistent%20with%20the%20geometry%20of%20the%20space.%20HypeGBMS%20effectively%20captures%20latent%20hierarchies%20while%20retaining%20the%20density-seeking%20behavior%20of%20GBMS.%20We%20provide%20theoretical%20insights%20into%20convergence%20and%20computational%20complexity%2C%20along%20with%20empirical%20results%20that%20demonstrate%20improved%20clustering%20quality%20in%20hierarchical%20datasets.%20This%20work%20bridges%20classical%20mean-shift%20clustering%20and%20hyperbolic%20representation%20learning%2C%20offering%20a%20principled%20approach%20to%20density-based%20clustering%20in%20curved%20spaces.%20Extensive%20experimental%20evaluations%20on%20%2411%24%20real-world%20datasets%20demonstrate%20that%20HypeGBMS%20significantly%20outperforms%20conventional%20mean-shift%20clustering%20methods%20in%20non-Euclidean%20settings%2C%20underscoring%20its%20robustness%20and%20effectiveness.&entry.1838667208=http%3A//arxiv.org/abs/2512.11448v1&entry.124074799=Read"},
{"title": "How Muon's Spectral Design Benefits Generalization: A Study on Imbalanced Data", "author": "Bhavya Vasudeva and Puneesh Deora and Yize Zhao and Vatsal Sharan and Christos Thrampoulidis", "abstract": "The growing adoption of spectrum-aware matrix-valued optimizers such as Muon and Shampoo in deep learning motivates a systematic study of their generalization properties and, in particular, when they might outperform competitive algorithms. We approach this question by introducing appropriate simplifying abstractions as follows: First, we use imbalanced data as a testbed. Second, we study the canonical form of such optimizers, which is Spectral Gradient Descent (SpecGD) -- each update step is $UV^T$ where $U\u03a3V^T$ is the truncated SVD of the gradient. Third, within this framework we identify a canonical setting for which we precisely quantify when SpecGD outperforms vanilla Euclidean GD. For a Gaussian mixture data model and both linear and bilinear models, we show that unlike GD, which prioritizes learning dominant principal components of the data first, SpecGD learns all principal components of the data at equal rates. We demonstrate how this translates to a growing gap in class balanced loss favoring SpecGD early in training and further show that the gap remains consistent even when the GD counterpart uses adaptive step-sizes via normalization. By extending the analysis to deep linear models, we show that depth amplifies these effects. We empirically verify our theoretical findings on a variety of imbalanced datasets. Our experiments compare practical variants of spectral methods, like Muon and Shampoo, against their Euclidean counterparts and Adam. The results validate our findings that these spectral optimizers achieve superior generalization by promoting a more balanced learning of the data's underlying components.", "link": "http://arxiv.org/abs/2510.22980v3", "date": "2025-12-12", "relevancy": 2.5608, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5226}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5074}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Muon%27s%20Spectral%20Design%20Benefits%20Generalization%3A%20A%20Study%20on%20Imbalanced%20Data&body=Title%3A%20How%20Muon%27s%20Spectral%20Design%20Benefits%20Generalization%3A%20A%20Study%20on%20Imbalanced%20Data%0AAuthor%3A%20Bhavya%20Vasudeva%20and%20Puneesh%20Deora%20and%20Yize%20Zhao%20and%20Vatsal%20Sharan%20and%20Christos%20Thrampoulidis%0AAbstract%3A%20The%20growing%20adoption%20of%20spectrum-aware%20matrix-valued%20optimizers%20such%20as%20Muon%20and%20Shampoo%20in%20deep%20learning%20motivates%20a%20systematic%20study%20of%20their%20generalization%20properties%20and%2C%20in%20particular%2C%20when%20they%20might%20outperform%20competitive%20algorithms.%20We%20approach%20this%20question%20by%20introducing%20appropriate%20simplifying%20abstractions%20as%20follows%3A%20First%2C%20we%20use%20imbalanced%20data%20as%20a%20testbed.%20Second%2C%20we%20study%20the%20canonical%20form%20of%20such%20optimizers%2C%20which%20is%20Spectral%20Gradient%20Descent%20%28SpecGD%29%20--%20each%20update%20step%20is%20%24UV%5ET%24%20where%20%24U%CE%A3V%5ET%24%20is%20the%20truncated%20SVD%20of%20the%20gradient.%20Third%2C%20within%20this%20framework%20we%20identify%20a%20canonical%20setting%20for%20which%20we%20precisely%20quantify%20when%20SpecGD%20outperforms%20vanilla%20Euclidean%20GD.%20For%20a%20Gaussian%20mixture%20data%20model%20and%20both%20linear%20and%20bilinear%20models%2C%20we%20show%20that%20unlike%20GD%2C%20which%20prioritizes%20learning%20dominant%20principal%20components%20of%20the%20data%20first%2C%20SpecGD%20learns%20all%20principal%20components%20of%20the%20data%20at%20equal%20rates.%20We%20demonstrate%20how%20this%20translates%20to%20a%20growing%20gap%20in%20class%20balanced%20loss%20favoring%20SpecGD%20early%20in%20training%20and%20further%20show%20that%20the%20gap%20remains%20consistent%20even%20when%20the%20GD%20counterpart%20uses%20adaptive%20step-sizes%20via%20normalization.%20By%20extending%20the%20analysis%20to%20deep%20linear%20models%2C%20we%20show%20that%20depth%20amplifies%20these%20effects.%20We%20empirically%20verify%20our%20theoretical%20findings%20on%20a%20variety%20of%20imbalanced%20datasets.%20Our%20experiments%20compare%20practical%20variants%20of%20spectral%20methods%2C%20like%20Muon%20and%20Shampoo%2C%20against%20their%20Euclidean%20counterparts%20and%20Adam.%20The%20results%20validate%20our%20findings%20that%20these%20spectral%20optimizers%20achieve%20superior%20generalization%20by%20promoting%20a%20more%20balanced%20learning%20of%20the%20data%27s%20underlying%20components.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22980v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Muon%2527s%2520Spectral%2520Design%2520Benefits%2520Generalization%253A%2520A%2520Study%2520on%2520Imbalanced%2520Data%26entry.906535625%3DBhavya%2520Vasudeva%2520and%2520Puneesh%2520Deora%2520and%2520Yize%2520Zhao%2520and%2520Vatsal%2520Sharan%2520and%2520Christos%2520Thrampoulidis%26entry.1292438233%3DThe%2520growing%2520adoption%2520of%2520spectrum-aware%2520matrix-valued%2520optimizers%2520such%2520as%2520Muon%2520and%2520Shampoo%2520in%2520deep%2520learning%2520motivates%2520a%2520systematic%2520study%2520of%2520their%2520generalization%2520properties%2520and%252C%2520in%2520particular%252C%2520when%2520they%2520might%2520outperform%2520competitive%2520algorithms.%2520We%2520approach%2520this%2520question%2520by%2520introducing%2520appropriate%2520simplifying%2520abstractions%2520as%2520follows%253A%2520First%252C%2520we%2520use%2520imbalanced%2520data%2520as%2520a%2520testbed.%2520Second%252C%2520we%2520study%2520the%2520canonical%2520form%2520of%2520such%2520optimizers%252C%2520which%2520is%2520Spectral%2520Gradient%2520Descent%2520%2528SpecGD%2529%2520--%2520each%2520update%2520step%2520is%2520%2524UV%255ET%2524%2520where%2520%2524U%25CE%25A3V%255ET%2524%2520is%2520the%2520truncated%2520SVD%2520of%2520the%2520gradient.%2520Third%252C%2520within%2520this%2520framework%2520we%2520identify%2520a%2520canonical%2520setting%2520for%2520which%2520we%2520precisely%2520quantify%2520when%2520SpecGD%2520outperforms%2520vanilla%2520Euclidean%2520GD.%2520For%2520a%2520Gaussian%2520mixture%2520data%2520model%2520and%2520both%2520linear%2520and%2520bilinear%2520models%252C%2520we%2520show%2520that%2520unlike%2520GD%252C%2520which%2520prioritizes%2520learning%2520dominant%2520principal%2520components%2520of%2520the%2520data%2520first%252C%2520SpecGD%2520learns%2520all%2520principal%2520components%2520of%2520the%2520data%2520at%2520equal%2520rates.%2520We%2520demonstrate%2520how%2520this%2520translates%2520to%2520a%2520growing%2520gap%2520in%2520class%2520balanced%2520loss%2520favoring%2520SpecGD%2520early%2520in%2520training%2520and%2520further%2520show%2520that%2520the%2520gap%2520remains%2520consistent%2520even%2520when%2520the%2520GD%2520counterpart%2520uses%2520adaptive%2520step-sizes%2520via%2520normalization.%2520By%2520extending%2520the%2520analysis%2520to%2520deep%2520linear%2520models%252C%2520we%2520show%2520that%2520depth%2520amplifies%2520these%2520effects.%2520We%2520empirically%2520verify%2520our%2520theoretical%2520findings%2520on%2520a%2520variety%2520of%2520imbalanced%2520datasets.%2520Our%2520experiments%2520compare%2520practical%2520variants%2520of%2520spectral%2520methods%252C%2520like%2520Muon%2520and%2520Shampoo%252C%2520against%2520their%2520Euclidean%2520counterparts%2520and%2520Adam.%2520The%2520results%2520validate%2520our%2520findings%2520that%2520these%2520spectral%2520optimizers%2520achieve%2520superior%2520generalization%2520by%2520promoting%2520a%2520more%2520balanced%2520learning%2520of%2520the%2520data%2527s%2520underlying%2520components.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22980v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Muon%27s%20Spectral%20Design%20Benefits%20Generalization%3A%20A%20Study%20on%20Imbalanced%20Data&entry.906535625=Bhavya%20Vasudeva%20and%20Puneesh%20Deora%20and%20Yize%20Zhao%20and%20Vatsal%20Sharan%20and%20Christos%20Thrampoulidis&entry.1292438233=The%20growing%20adoption%20of%20spectrum-aware%20matrix-valued%20optimizers%20such%20as%20Muon%20and%20Shampoo%20in%20deep%20learning%20motivates%20a%20systematic%20study%20of%20their%20generalization%20properties%20and%2C%20in%20particular%2C%20when%20they%20might%20outperform%20competitive%20algorithms.%20We%20approach%20this%20question%20by%20introducing%20appropriate%20simplifying%20abstractions%20as%20follows%3A%20First%2C%20we%20use%20imbalanced%20data%20as%20a%20testbed.%20Second%2C%20we%20study%20the%20canonical%20form%20of%20such%20optimizers%2C%20which%20is%20Spectral%20Gradient%20Descent%20%28SpecGD%29%20--%20each%20update%20step%20is%20%24UV%5ET%24%20where%20%24U%CE%A3V%5ET%24%20is%20the%20truncated%20SVD%20of%20the%20gradient.%20Third%2C%20within%20this%20framework%20we%20identify%20a%20canonical%20setting%20for%20which%20we%20precisely%20quantify%20when%20SpecGD%20outperforms%20vanilla%20Euclidean%20GD.%20For%20a%20Gaussian%20mixture%20data%20model%20and%20both%20linear%20and%20bilinear%20models%2C%20we%20show%20that%20unlike%20GD%2C%20which%20prioritizes%20learning%20dominant%20principal%20components%20of%20the%20data%20first%2C%20SpecGD%20learns%20all%20principal%20components%20of%20the%20data%20at%20equal%20rates.%20We%20demonstrate%20how%20this%20translates%20to%20a%20growing%20gap%20in%20class%20balanced%20loss%20favoring%20SpecGD%20early%20in%20training%20and%20further%20show%20that%20the%20gap%20remains%20consistent%20even%20when%20the%20GD%20counterpart%20uses%20adaptive%20step-sizes%20via%20normalization.%20By%20extending%20the%20analysis%20to%20deep%20linear%20models%2C%20we%20show%20that%20depth%20amplifies%20these%20effects.%20We%20empirically%20verify%20our%20theoretical%20findings%20on%20a%20variety%20of%20imbalanced%20datasets.%20Our%20experiments%20compare%20practical%20variants%20of%20spectral%20methods%2C%20like%20Muon%20and%20Shampoo%2C%20against%20their%20Euclidean%20counterparts%20and%20Adam.%20The%20results%20validate%20our%20findings%20that%20these%20spectral%20optimizers%20achieve%20superior%20generalization%20by%20promoting%20a%20more%20balanced%20learning%20of%20the%20data%27s%20underlying%20components.&entry.1838667208=http%3A//arxiv.org/abs/2510.22980v3&entry.124074799=Read"},
{"title": "Bridging Streaming Continual Learning via In-Context Large Tabular Models", "author": "Afonso Louren\u00e7o and Jo\u00e3o Gama and Eric P. Xing and Goreti Marreiros", "abstract": "In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.", "link": "http://arxiv.org/abs/2512.11668v1", "date": "2025-12-12", "relevancy": 2.5513, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Streaming%20Continual%20Learning%20via%20In-Context%20Large%20Tabular%20Models&body=Title%3A%20Bridging%20Streaming%20Continual%20Learning%20via%20In-Context%20Large%20Tabular%20Models%0AAuthor%3A%20Afonso%20Louren%C3%A7o%20and%20Jo%C3%A3o%20Gama%20and%20Eric%20P.%20Xing%20and%20Goreti%20Marreiros%0AAbstract%3A%20In%20streaming%20scenarios%2C%20models%20must%20learn%20continuously%2C%20adapting%20to%20concept%20drifts%20without%20erasing%20previously%20acquired%20knowledge.%20However%2C%20existing%20research%20communities%20address%20these%20challenges%20in%20isolation.%20Continual%20Learning%20%28CL%29%20focuses%20on%20long-term%20retention%20and%20mitigating%20catastrophic%20forgetting%2C%20often%20without%20strict%20real-time%20constraints.%20Stream%20Learning%20%28SL%29%20emphasizes%20rapid%2C%20efficient%20adaptation%20to%20high-frequency%20data%20streams%2C%20but%20typically%20neglects%20forgetting.%20Recent%20efforts%20have%20tried%20to%20combine%20these%20paradigms%2C%20yet%20no%20clear%20algorithmic%20overlap%20exists.%20We%20argue%20that%20large%20in-context%20tabular%20models%20%28LTMs%29%20provide%20a%20natural%20bridge%20for%20Streaming%20Continual%20Learning%20%28SCL%29.%20In%20our%20view%2C%20unbounded%20streams%20should%20be%20summarized%20on-the-fly%20into%20compact%20sketches%20that%20can%20be%20consumed%20by%20LTMs.%20This%20recovers%20the%20classical%20SL%20motivation%20of%20compressing%20massive%20streams%20with%20fixed-size%20guarantees%2C%20while%20simultaneously%20aligning%20with%20the%20experience-replay%20desiderata%20of%20CL.%20To%20clarify%20this%20bridge%2C%20we%20show%20how%20the%20SL%20and%20CL%20communities%20implicitly%20adopt%20a%20divide-to-conquer%20strategy%20to%20manage%20the%20tension%20between%20plasticity%20%28performing%20well%20on%20the%20current%20distribution%29%20and%20stability%20%28retaining%20past%20knowledge%29%2C%20while%20also%20imposing%20a%20minimal%20complexity%20constraint%20that%20motivates%20diversification%20%28avoiding%20redundancy%20in%20what%20is%20stored%29%20and%20retrieval%20%28re-prioritizing%20past%20information%20when%20needed%29.%20Within%20this%20perspective%2C%20we%20propose%20structuring%20SCL%20with%20LTMs%20around%20two%20core%20principles%20of%20data%20selection%20for%20in-context%20learning%3A%20%281%29%20distribution%20matching%2C%20which%20balances%20plasticity%20and%20stability%2C%20and%20%282%29%20distribution%20compression%2C%20which%20controls%20memory%20size%20through%20diversification%20and%20retrieval%20mechanisms.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Streaming%2520Continual%2520Learning%2520via%2520In-Context%2520Large%2520Tabular%2520Models%26entry.906535625%3DAfonso%2520Louren%25C3%25A7o%2520and%2520Jo%25C3%25A3o%2520Gama%2520and%2520Eric%2520P.%2520Xing%2520and%2520Goreti%2520Marreiros%26entry.1292438233%3DIn%2520streaming%2520scenarios%252C%2520models%2520must%2520learn%2520continuously%252C%2520adapting%2520to%2520concept%2520drifts%2520without%2520erasing%2520previously%2520acquired%2520knowledge.%2520However%252C%2520existing%2520research%2520communities%2520address%2520these%2520challenges%2520in%2520isolation.%2520Continual%2520Learning%2520%2528CL%2529%2520focuses%2520on%2520long-term%2520retention%2520and%2520mitigating%2520catastrophic%2520forgetting%252C%2520often%2520without%2520strict%2520real-time%2520constraints.%2520Stream%2520Learning%2520%2528SL%2529%2520emphasizes%2520rapid%252C%2520efficient%2520adaptation%2520to%2520high-frequency%2520data%2520streams%252C%2520but%2520typically%2520neglects%2520forgetting.%2520Recent%2520efforts%2520have%2520tried%2520to%2520combine%2520these%2520paradigms%252C%2520yet%2520no%2520clear%2520algorithmic%2520overlap%2520exists.%2520We%2520argue%2520that%2520large%2520in-context%2520tabular%2520models%2520%2528LTMs%2529%2520provide%2520a%2520natural%2520bridge%2520for%2520Streaming%2520Continual%2520Learning%2520%2528SCL%2529.%2520In%2520our%2520view%252C%2520unbounded%2520streams%2520should%2520be%2520summarized%2520on-the-fly%2520into%2520compact%2520sketches%2520that%2520can%2520be%2520consumed%2520by%2520LTMs.%2520This%2520recovers%2520the%2520classical%2520SL%2520motivation%2520of%2520compressing%2520massive%2520streams%2520with%2520fixed-size%2520guarantees%252C%2520while%2520simultaneously%2520aligning%2520with%2520the%2520experience-replay%2520desiderata%2520of%2520CL.%2520To%2520clarify%2520this%2520bridge%252C%2520we%2520show%2520how%2520the%2520SL%2520and%2520CL%2520communities%2520implicitly%2520adopt%2520a%2520divide-to-conquer%2520strategy%2520to%2520manage%2520the%2520tension%2520between%2520plasticity%2520%2528performing%2520well%2520on%2520the%2520current%2520distribution%2529%2520and%2520stability%2520%2528retaining%2520past%2520knowledge%2529%252C%2520while%2520also%2520imposing%2520a%2520minimal%2520complexity%2520constraint%2520that%2520motivates%2520diversification%2520%2528avoiding%2520redundancy%2520in%2520what%2520is%2520stored%2529%2520and%2520retrieval%2520%2528re-prioritizing%2520past%2520information%2520when%2520needed%2529.%2520Within%2520this%2520perspective%252C%2520we%2520propose%2520structuring%2520SCL%2520with%2520LTMs%2520around%2520two%2520core%2520principles%2520of%2520data%2520selection%2520for%2520in-context%2520learning%253A%2520%25281%2529%2520distribution%2520matching%252C%2520which%2520balances%2520plasticity%2520and%2520stability%252C%2520and%2520%25282%2529%2520distribution%2520compression%252C%2520which%2520controls%2520memory%2520size%2520through%2520diversification%2520and%2520retrieval%2520mechanisms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Streaming%20Continual%20Learning%20via%20In-Context%20Large%20Tabular%20Models&entry.906535625=Afonso%20Louren%C3%A7o%20and%20Jo%C3%A3o%20Gama%20and%20Eric%20P.%20Xing%20and%20Goreti%20Marreiros&entry.1292438233=In%20streaming%20scenarios%2C%20models%20must%20learn%20continuously%2C%20adapting%20to%20concept%20drifts%20without%20erasing%20previously%20acquired%20knowledge.%20However%2C%20existing%20research%20communities%20address%20these%20challenges%20in%20isolation.%20Continual%20Learning%20%28CL%29%20focuses%20on%20long-term%20retention%20and%20mitigating%20catastrophic%20forgetting%2C%20often%20without%20strict%20real-time%20constraints.%20Stream%20Learning%20%28SL%29%20emphasizes%20rapid%2C%20efficient%20adaptation%20to%20high-frequency%20data%20streams%2C%20but%20typically%20neglects%20forgetting.%20Recent%20efforts%20have%20tried%20to%20combine%20these%20paradigms%2C%20yet%20no%20clear%20algorithmic%20overlap%20exists.%20We%20argue%20that%20large%20in-context%20tabular%20models%20%28LTMs%29%20provide%20a%20natural%20bridge%20for%20Streaming%20Continual%20Learning%20%28SCL%29.%20In%20our%20view%2C%20unbounded%20streams%20should%20be%20summarized%20on-the-fly%20into%20compact%20sketches%20that%20can%20be%20consumed%20by%20LTMs.%20This%20recovers%20the%20classical%20SL%20motivation%20of%20compressing%20massive%20streams%20with%20fixed-size%20guarantees%2C%20while%20simultaneously%20aligning%20with%20the%20experience-replay%20desiderata%20of%20CL.%20To%20clarify%20this%20bridge%2C%20we%20show%20how%20the%20SL%20and%20CL%20communities%20implicitly%20adopt%20a%20divide-to-conquer%20strategy%20to%20manage%20the%20tension%20between%20plasticity%20%28performing%20well%20on%20the%20current%20distribution%29%20and%20stability%20%28retaining%20past%20knowledge%29%2C%20while%20also%20imposing%20a%20minimal%20complexity%20constraint%20that%20motivates%20diversification%20%28avoiding%20redundancy%20in%20what%20is%20stored%29%20and%20retrieval%20%28re-prioritizing%20past%20information%20when%20needed%29.%20Within%20this%20perspective%2C%20we%20propose%20structuring%20SCL%20with%20LTMs%20around%20two%20core%20principles%20of%20data%20selection%20for%20in-context%20learning%3A%20%281%29%20distribution%20matching%2C%20which%20balances%20plasticity%20and%20stability%2C%20and%20%282%29%20distribution%20compression%2C%20which%20controls%20memory%20size%20through%20diversification%20and%20retrieval%20mechanisms.&entry.1838667208=http%3A//arxiv.org/abs/2512.11668v1&entry.124074799=Read"},
{"title": "mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images", "author": "Liqiang Huang and Rachel W. Mills and Saikiran Mandula and Lin Bai and Mahtab Jeyhani and John Redell and Hien Van Nguyen and Saurabh Prasad and Dragan Maric and Badrinath Roysam", "abstract": "Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a multiplex encoder for each panel with explicit visual confirmation of successful learning. Multiple panels can be combined to process visual queries for retrieving similar communities of individual cells or multicellular niches using information-theoretic methods. The retrievals can be used for diverse purposes including tissue exploration, delineating brain regions and cortical cell layers, profiling and comparing brain regions without computer programming. We validated mViSE's ability to retrieve single cells, proximal cell pairs, tissue patches, delineate cortical layers, brain regions and sub-regions. mViSE is provided as an open-source QuPath plug-in.", "link": "http://arxiv.org/abs/2512.11745v1", "date": "2025-12-12", "relevancy": 2.5183, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mViSE%3A%20A%20Visual%20Search%20Engine%20for%20Analyzing%20Multiplex%20IHC%20Brain%20Tissue%20Images&body=Title%3A%20mViSE%3A%20A%20Visual%20Search%20Engine%20for%20Analyzing%20Multiplex%20IHC%20Brain%20Tissue%20Images%0AAuthor%3A%20Liqiang%20Huang%20and%20Rachel%20W.%20Mills%20and%20Saikiran%20Mandula%20and%20Lin%20Bai%20and%20Mahtab%20Jeyhani%20and%20John%20Redell%20and%20Hien%20Van%20Nguyen%20and%20Saurabh%20Prasad%20and%20Dragan%20Maric%20and%20Badrinath%20Roysam%0AAbstract%3A%20Whole-slide%20multiplex%20imaging%20of%20brain%20tissue%20generates%20massive%20information-dense%20images%20that%20are%20challenging%20to%20analyze%20and%20require%20custom%20software.%20We%20present%20an%20alternative%20query-driven%20programming-free%20strategy%20using%20a%20multiplex%20visual%20search%20engine%20%28mViSE%29%20that%20learns%20the%20multifaceted%20brain%20tissue%20chemoarchitecture%2C%20cytoarchitecture%2C%20and%20myeloarchitecture.%20Our%20divide-and-conquer%20strategy%20organizes%20the%20data%20into%20panels%20of%20related%20molecular%20markers%20and%20uses%20self-supervised%20learning%20to%20train%20a%20multiplex%20encoder%20for%20each%20panel%20with%20explicit%20visual%20confirmation%20of%20successful%20learning.%20Multiple%20panels%20can%20be%20combined%20to%20process%20visual%20queries%20for%20retrieving%20similar%20communities%20of%20individual%20cells%20or%20multicellular%20niches%20using%20information-theoretic%20methods.%20The%20retrievals%20can%20be%20used%20for%20diverse%20purposes%20including%20tissue%20exploration%2C%20delineating%20brain%20regions%20and%20cortical%20cell%20layers%2C%20profiling%20and%20comparing%20brain%20regions%20without%20computer%20programming.%20We%20validated%20mViSE%27s%20ability%20to%20retrieve%20single%20cells%2C%20proximal%20cell%20pairs%2C%20tissue%20patches%2C%20delineate%20cortical%20layers%2C%20brain%20regions%20and%20sub-regions.%20mViSE%20is%20provided%20as%20an%20open-source%20QuPath%20plug-in.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmViSE%253A%2520A%2520Visual%2520Search%2520Engine%2520for%2520Analyzing%2520Multiplex%2520IHC%2520Brain%2520Tissue%2520Images%26entry.906535625%3DLiqiang%2520Huang%2520and%2520Rachel%2520W.%2520Mills%2520and%2520Saikiran%2520Mandula%2520and%2520Lin%2520Bai%2520and%2520Mahtab%2520Jeyhani%2520and%2520John%2520Redell%2520and%2520Hien%2520Van%2520Nguyen%2520and%2520Saurabh%2520Prasad%2520and%2520Dragan%2520Maric%2520and%2520Badrinath%2520Roysam%26entry.1292438233%3DWhole-slide%2520multiplex%2520imaging%2520of%2520brain%2520tissue%2520generates%2520massive%2520information-dense%2520images%2520that%2520are%2520challenging%2520to%2520analyze%2520and%2520require%2520custom%2520software.%2520We%2520present%2520an%2520alternative%2520query-driven%2520programming-free%2520strategy%2520using%2520a%2520multiplex%2520visual%2520search%2520engine%2520%2528mViSE%2529%2520that%2520learns%2520the%2520multifaceted%2520brain%2520tissue%2520chemoarchitecture%252C%2520cytoarchitecture%252C%2520and%2520myeloarchitecture.%2520Our%2520divide-and-conquer%2520strategy%2520organizes%2520the%2520data%2520into%2520panels%2520of%2520related%2520molecular%2520markers%2520and%2520uses%2520self-supervised%2520learning%2520to%2520train%2520a%2520multiplex%2520encoder%2520for%2520each%2520panel%2520with%2520explicit%2520visual%2520confirmation%2520of%2520successful%2520learning.%2520Multiple%2520panels%2520can%2520be%2520combined%2520to%2520process%2520visual%2520queries%2520for%2520retrieving%2520similar%2520communities%2520of%2520individual%2520cells%2520or%2520multicellular%2520niches%2520using%2520information-theoretic%2520methods.%2520The%2520retrievals%2520can%2520be%2520used%2520for%2520diverse%2520purposes%2520including%2520tissue%2520exploration%252C%2520delineating%2520brain%2520regions%2520and%2520cortical%2520cell%2520layers%252C%2520profiling%2520and%2520comparing%2520brain%2520regions%2520without%2520computer%2520programming.%2520We%2520validated%2520mViSE%2527s%2520ability%2520to%2520retrieve%2520single%2520cells%252C%2520proximal%2520cell%2520pairs%252C%2520tissue%2520patches%252C%2520delineate%2520cortical%2520layers%252C%2520brain%2520regions%2520and%2520sub-regions.%2520mViSE%2520is%2520provided%2520as%2520an%2520open-source%2520QuPath%2520plug-in.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mViSE%3A%20A%20Visual%20Search%20Engine%20for%20Analyzing%20Multiplex%20IHC%20Brain%20Tissue%20Images&entry.906535625=Liqiang%20Huang%20and%20Rachel%20W.%20Mills%20and%20Saikiran%20Mandula%20and%20Lin%20Bai%20and%20Mahtab%20Jeyhani%20and%20John%20Redell%20and%20Hien%20Van%20Nguyen%20and%20Saurabh%20Prasad%20and%20Dragan%20Maric%20and%20Badrinath%20Roysam&entry.1292438233=Whole-slide%20multiplex%20imaging%20of%20brain%20tissue%20generates%20massive%20information-dense%20images%20that%20are%20challenging%20to%20analyze%20and%20require%20custom%20software.%20We%20present%20an%20alternative%20query-driven%20programming-free%20strategy%20using%20a%20multiplex%20visual%20search%20engine%20%28mViSE%29%20that%20learns%20the%20multifaceted%20brain%20tissue%20chemoarchitecture%2C%20cytoarchitecture%2C%20and%20myeloarchitecture.%20Our%20divide-and-conquer%20strategy%20organizes%20the%20data%20into%20panels%20of%20related%20molecular%20markers%20and%20uses%20self-supervised%20learning%20to%20train%20a%20multiplex%20encoder%20for%20each%20panel%20with%20explicit%20visual%20confirmation%20of%20successful%20learning.%20Multiple%20panels%20can%20be%20combined%20to%20process%20visual%20queries%20for%20retrieving%20similar%20communities%20of%20individual%20cells%20or%20multicellular%20niches%20using%20information-theoretic%20methods.%20The%20retrievals%20can%20be%20used%20for%20diverse%20purposes%20including%20tissue%20exploration%2C%20delineating%20brain%20regions%20and%20cortical%20cell%20layers%2C%20profiling%20and%20comparing%20brain%20regions%20without%20computer%20programming.%20We%20validated%20mViSE%27s%20ability%20to%20retrieve%20single%20cells%2C%20proximal%20cell%20pairs%2C%20tissue%20patches%2C%20delineate%20cortical%20layers%2C%20brain%20regions%20and%20sub-regions.%20mViSE%20is%20provided%20as%20an%20open-source%20QuPath%20plug-in.&entry.1838667208=http%3A//arxiv.org/abs/2512.11745v1&entry.124074799=Read"},
{"title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation", "author": "Yang Fei and George Stoica and Jingyuan Liu and Qifeng Chen and Ranjay Krishna and Xiaojuan Wang and Benlin Liu", "abstract": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .", "link": "http://arxiv.org/abs/2512.11792v1", "date": "2025-12-12", "relevancy": 2.4985, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6465}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6417}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20From%20Tracking%3A%20Distilling%20Structure-Preserving%20Motion%20for%20Video%20Generation&body=Title%3A%20Structure%20From%20Tracking%3A%20Distilling%20Structure-Preserving%20Motion%20for%20Video%20Generation%0AAuthor%3A%20Yang%20Fei%20and%20George%20Stoica%20and%20Jingyuan%20Liu%20and%20Qifeng%20Chen%20and%20Ranjay%20Krishna%20and%20Xiaojuan%20Wang%20and%20Benlin%20Liu%0AAbstract%3A%20Reality%20is%20a%20dance%20between%20rigid%20constraints%20and%20deformable%20structures.%20For%20video%20models%2C%20that%20means%20generating%20motion%20that%20preserves%20fidelity%20as%20well%20as%20structure.%20Despite%20progress%20in%20diffusion%20models%2C%20producing%20realistic%20structure-preserving%20motion%20remains%20challenging%2C%20especially%20for%20articulated%20and%20deformable%20objects%20such%20as%20humans%20and%20animals.%20Scaling%20training%20data%20alone%2C%20so%20far%2C%20has%20failed%20to%20resolve%20physically%20implausible%20transitions.%20Existing%20approaches%20rely%20on%20conditioning%20with%20noisy%20motion%20representations%2C%20such%20as%20optical%20flow%20or%20skeletons%20extracted%20using%20an%20external%20imperfect%20model.%20To%20address%20these%20challenges%2C%20we%20introduce%20an%20algorithm%20to%20distill%20structure-preserving%20motion%20priors%20from%20an%20autoregressive%20video%20tracking%20model%20%28SAM2%29%20into%20a%20bidirectional%20video%20diffusion%20model%20%28CogVideoX%29.%20With%20our%20method%2C%20we%20train%20SAM2VideoX%2C%20which%20contains%20two%20innovations%3A%20%281%29%20a%20bidirectional%20feature%20fusion%20module%20that%20extracts%20global%20structure-preserving%20motion%20priors%20from%20a%20recurrent%20model%20like%20SAM2%3B%20%282%29%20a%20Local%20Gram%20Flow%20loss%20that%20aligns%20how%20local%20features%20move%20together.%20Experiments%20on%20VBench%20and%20in%20human%20studies%20show%20that%20SAM2VideoX%20delivers%20consistent%20gains%20%28%2B2.60%5C%25%20on%20VBench%2C%2021-22%5C%25%20lower%20FVD%2C%20and%2071.4%5C%25%20human%20preference%29%20over%20prior%20baselines.%20Specifically%2C%20on%20VBench%2C%20we%20achieve%2095.51%5C%25%2C%20surpassing%20REPA%20%2892.91%5C%25%29%20by%202.60%5C%25%2C%20and%20reduce%20FVD%20to%20360.57%2C%20a%2021.20%5C%25%20and%2022.46%5C%25%20improvement%20over%20REPA-%20and%20LoRA-finetuning%2C%20respectively.%20The%20project%20website%20can%20be%20found%20at%20https%3A//sam2videox.github.io/%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520From%2520Tracking%253A%2520Distilling%2520Structure-Preserving%2520Motion%2520for%2520Video%2520Generation%26entry.906535625%3DYang%2520Fei%2520and%2520George%2520Stoica%2520and%2520Jingyuan%2520Liu%2520and%2520Qifeng%2520Chen%2520and%2520Ranjay%2520Krishna%2520and%2520Xiaojuan%2520Wang%2520and%2520Benlin%2520Liu%26entry.1292438233%3DReality%2520is%2520a%2520dance%2520between%2520rigid%2520constraints%2520and%2520deformable%2520structures.%2520For%2520video%2520models%252C%2520that%2520means%2520generating%2520motion%2520that%2520preserves%2520fidelity%2520as%2520well%2520as%2520structure.%2520Despite%2520progress%2520in%2520diffusion%2520models%252C%2520producing%2520realistic%2520structure-preserving%2520motion%2520remains%2520challenging%252C%2520especially%2520for%2520articulated%2520and%2520deformable%2520objects%2520such%2520as%2520humans%2520and%2520animals.%2520Scaling%2520training%2520data%2520alone%252C%2520so%2520far%252C%2520has%2520failed%2520to%2520resolve%2520physically%2520implausible%2520transitions.%2520Existing%2520approaches%2520rely%2520on%2520conditioning%2520with%2520noisy%2520motion%2520representations%252C%2520such%2520as%2520optical%2520flow%2520or%2520skeletons%2520extracted%2520using%2520an%2520external%2520imperfect%2520model.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520an%2520algorithm%2520to%2520distill%2520structure-preserving%2520motion%2520priors%2520from%2520an%2520autoregressive%2520video%2520tracking%2520model%2520%2528SAM2%2529%2520into%2520a%2520bidirectional%2520video%2520diffusion%2520model%2520%2528CogVideoX%2529.%2520With%2520our%2520method%252C%2520we%2520train%2520SAM2VideoX%252C%2520which%2520contains%2520two%2520innovations%253A%2520%25281%2529%2520a%2520bidirectional%2520feature%2520fusion%2520module%2520that%2520extracts%2520global%2520structure-preserving%2520motion%2520priors%2520from%2520a%2520recurrent%2520model%2520like%2520SAM2%253B%2520%25282%2529%2520a%2520Local%2520Gram%2520Flow%2520loss%2520that%2520aligns%2520how%2520local%2520features%2520move%2520together.%2520Experiments%2520on%2520VBench%2520and%2520in%2520human%2520studies%2520show%2520that%2520SAM2VideoX%2520delivers%2520consistent%2520gains%2520%2528%252B2.60%255C%2525%2520on%2520VBench%252C%252021-22%255C%2525%2520lower%2520FVD%252C%2520and%252071.4%255C%2525%2520human%2520preference%2529%2520over%2520prior%2520baselines.%2520Specifically%252C%2520on%2520VBench%252C%2520we%2520achieve%252095.51%255C%2525%252C%2520surpassing%2520REPA%2520%252892.91%255C%2525%2529%2520by%25202.60%255C%2525%252C%2520and%2520reduce%2520FVD%2520to%2520360.57%252C%2520a%252021.20%255C%2525%2520and%252022.46%255C%2525%2520improvement%2520over%2520REPA-%2520and%2520LoRA-finetuning%252C%2520respectively.%2520The%2520project%2520website%2520can%2520be%2520found%2520at%2520https%253A//sam2videox.github.io/%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20From%20Tracking%3A%20Distilling%20Structure-Preserving%20Motion%20for%20Video%20Generation&entry.906535625=Yang%20Fei%20and%20George%20Stoica%20and%20Jingyuan%20Liu%20and%20Qifeng%20Chen%20and%20Ranjay%20Krishna%20and%20Xiaojuan%20Wang%20and%20Benlin%20Liu&entry.1292438233=Reality%20is%20a%20dance%20between%20rigid%20constraints%20and%20deformable%20structures.%20For%20video%20models%2C%20that%20means%20generating%20motion%20that%20preserves%20fidelity%20as%20well%20as%20structure.%20Despite%20progress%20in%20diffusion%20models%2C%20producing%20realistic%20structure-preserving%20motion%20remains%20challenging%2C%20especially%20for%20articulated%20and%20deformable%20objects%20such%20as%20humans%20and%20animals.%20Scaling%20training%20data%20alone%2C%20so%20far%2C%20has%20failed%20to%20resolve%20physically%20implausible%20transitions.%20Existing%20approaches%20rely%20on%20conditioning%20with%20noisy%20motion%20representations%2C%20such%20as%20optical%20flow%20or%20skeletons%20extracted%20using%20an%20external%20imperfect%20model.%20To%20address%20these%20challenges%2C%20we%20introduce%20an%20algorithm%20to%20distill%20structure-preserving%20motion%20priors%20from%20an%20autoregressive%20video%20tracking%20model%20%28SAM2%29%20into%20a%20bidirectional%20video%20diffusion%20model%20%28CogVideoX%29.%20With%20our%20method%2C%20we%20train%20SAM2VideoX%2C%20which%20contains%20two%20innovations%3A%20%281%29%20a%20bidirectional%20feature%20fusion%20module%20that%20extracts%20global%20structure-preserving%20motion%20priors%20from%20a%20recurrent%20model%20like%20SAM2%3B%20%282%29%20a%20Local%20Gram%20Flow%20loss%20that%20aligns%20how%20local%20features%20move%20together.%20Experiments%20on%20VBench%20and%20in%20human%20studies%20show%20that%20SAM2VideoX%20delivers%20consistent%20gains%20%28%2B2.60%5C%25%20on%20VBench%2C%2021-22%5C%25%20lower%20FVD%2C%20and%2071.4%5C%25%20human%20preference%29%20over%20prior%20baselines.%20Specifically%2C%20on%20VBench%2C%20we%20achieve%2095.51%5C%25%2C%20surpassing%20REPA%20%2892.91%5C%25%29%20by%202.60%5C%25%2C%20and%20reduce%20FVD%20to%20360.57%2C%20a%2021.20%5C%25%20and%2022.46%5C%25%20improvement%20over%20REPA-%20and%20LoRA-finetuning%2C%20respectively.%20The%20project%20website%20can%20be%20found%20at%20https%3A//sam2videox.github.io/%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.11792v1&entry.124074799=Read"},
{"title": "FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint", "author": "Jiapeng Tang and Kai Li and Chengxiang Yin and Liuhao Ge and Fei Jiang and Jiu Xu and Matthias Nie\u00dfner and Christian H\u00e4ne and Timur Bagautdinov and Egor Zakharov and Peihong Guo", "abstract": "We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Pl\u00fccker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.", "link": "http://arxiv.org/abs/2512.11645v1", "date": "2025-12-12", "relevancy": 2.4964, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6414}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6121}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FactorPortrait%3A%20Controllable%20Portrait%20Animation%20via%20Disentangled%20Expression%2C%20Pose%2C%20and%20Viewpoint&body=Title%3A%20FactorPortrait%3A%20Controllable%20Portrait%20Animation%20via%20Disentangled%20Expression%2C%20Pose%2C%20and%20Viewpoint%0AAuthor%3A%20Jiapeng%20Tang%20and%20Kai%20Li%20and%20Chengxiang%20Yin%20and%20Liuhao%20Ge%20and%20Fei%20Jiang%20and%20Jiu%20Xu%20and%20Matthias%20Nie%C3%9Fner%20and%20Christian%20H%C3%A4ne%20and%20Timur%20Bagautdinov%20and%20Egor%20Zakharov%20and%20Peihong%20Guo%0AAbstract%3A%20We%20introduce%20FactorPortrait%2C%20a%20video%20diffusion%20method%20for%20controllable%20portrait%20animation%20that%20enables%20lifelike%20synthesis%20from%20disentangled%20control%20signals%20of%20facial%20expressions%2C%20head%20movement%2C%20and%20camera%20viewpoints.%20Given%20a%20single%20portrait%20image%2C%20a%20driving%20video%2C%20and%20camera%20trajectories%2C%20our%20method%20animates%20the%20portrait%20by%20transferring%20facial%20expressions%20and%20head%20movements%20from%20the%20driving%20video%20while%20simultaneously%20enabling%20novel%20view%20synthesis%20from%20arbitrary%20viewpoints.%20We%20utilize%20a%20pre-trained%20image%20encoder%20to%20extract%20facial%20expression%20latents%20from%20the%20driving%20video%20as%20control%20signals%20for%20animation%20generation.%20Such%20latents%20implicitly%20capture%20nuanced%20facial%20expression%20dynamics%20with%20identity%20and%20pose%20information%20disentangled%2C%20and%20they%20are%20efficiently%20injected%20into%20the%20video%20diffusion%20transformer%20through%20our%20proposed%20expression%20controller.%20For%20camera%20and%20head%20pose%20control%2C%20we%20employ%20Pl%C3%BCcker%20ray%20maps%20and%20normal%20maps%20rendered%20from%203D%20body%20mesh%20tracking.%20To%20train%20our%20model%2C%20we%20curate%20a%20large-scale%20synthetic%20dataset%20containing%20diverse%20combinations%20of%20camera%20viewpoints%2C%20head%20poses%2C%20and%20facial%20expression%20dynamics.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20in%20realism%2C%20expressiveness%2C%20control%20accuracy%2C%20and%20view%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactorPortrait%253A%2520Controllable%2520Portrait%2520Animation%2520via%2520Disentangled%2520Expression%252C%2520Pose%252C%2520and%2520Viewpoint%26entry.906535625%3DJiapeng%2520Tang%2520and%2520Kai%2520Li%2520and%2520Chengxiang%2520Yin%2520and%2520Liuhao%2520Ge%2520and%2520Fei%2520Jiang%2520and%2520Jiu%2520Xu%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Christian%2520H%25C3%25A4ne%2520and%2520Timur%2520Bagautdinov%2520and%2520Egor%2520Zakharov%2520and%2520Peihong%2520Guo%26entry.1292438233%3DWe%2520introduce%2520FactorPortrait%252C%2520a%2520video%2520diffusion%2520method%2520for%2520controllable%2520portrait%2520animation%2520that%2520enables%2520lifelike%2520synthesis%2520from%2520disentangled%2520control%2520signals%2520of%2520facial%2520expressions%252C%2520head%2520movement%252C%2520and%2520camera%2520viewpoints.%2520Given%2520a%2520single%2520portrait%2520image%252C%2520a%2520driving%2520video%252C%2520and%2520camera%2520trajectories%252C%2520our%2520method%2520animates%2520the%2520portrait%2520by%2520transferring%2520facial%2520expressions%2520and%2520head%2520movements%2520from%2520the%2520driving%2520video%2520while%2520simultaneously%2520enabling%2520novel%2520view%2520synthesis%2520from%2520arbitrary%2520viewpoints.%2520We%2520utilize%2520a%2520pre-trained%2520image%2520encoder%2520to%2520extract%2520facial%2520expression%2520latents%2520from%2520the%2520driving%2520video%2520as%2520control%2520signals%2520for%2520animation%2520generation.%2520Such%2520latents%2520implicitly%2520capture%2520nuanced%2520facial%2520expression%2520dynamics%2520with%2520identity%2520and%2520pose%2520information%2520disentangled%252C%2520and%2520they%2520are%2520efficiently%2520injected%2520into%2520the%2520video%2520diffusion%2520transformer%2520through%2520our%2520proposed%2520expression%2520controller.%2520For%2520camera%2520and%2520head%2520pose%2520control%252C%2520we%2520employ%2520Pl%25C3%25BCcker%2520ray%2520maps%2520and%2520normal%2520maps%2520rendered%2520from%25203D%2520body%2520mesh%2520tracking.%2520To%2520train%2520our%2520model%252C%2520we%2520curate%2520a%2520large-scale%2520synthetic%2520dataset%2520containing%2520diverse%2520combinations%2520of%2520camera%2520viewpoints%252C%2520head%2520poses%252C%2520and%2520facial%2520expression%2520dynamics.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520approaches%2520in%2520realism%252C%2520expressiveness%252C%2520control%2520accuracy%252C%2520and%2520view%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FactorPortrait%3A%20Controllable%20Portrait%20Animation%20via%20Disentangled%20Expression%2C%20Pose%2C%20and%20Viewpoint&entry.906535625=Jiapeng%20Tang%20and%20Kai%20Li%20and%20Chengxiang%20Yin%20and%20Liuhao%20Ge%20and%20Fei%20Jiang%20and%20Jiu%20Xu%20and%20Matthias%20Nie%C3%9Fner%20and%20Christian%20H%C3%A4ne%20and%20Timur%20Bagautdinov%20and%20Egor%20Zakharov%20and%20Peihong%20Guo&entry.1292438233=We%20introduce%20FactorPortrait%2C%20a%20video%20diffusion%20method%20for%20controllable%20portrait%20animation%20that%20enables%20lifelike%20synthesis%20from%20disentangled%20control%20signals%20of%20facial%20expressions%2C%20head%20movement%2C%20and%20camera%20viewpoints.%20Given%20a%20single%20portrait%20image%2C%20a%20driving%20video%2C%20and%20camera%20trajectories%2C%20our%20method%20animates%20the%20portrait%20by%20transferring%20facial%20expressions%20and%20head%20movements%20from%20the%20driving%20video%20while%20simultaneously%20enabling%20novel%20view%20synthesis%20from%20arbitrary%20viewpoints.%20We%20utilize%20a%20pre-trained%20image%20encoder%20to%20extract%20facial%20expression%20latents%20from%20the%20driving%20video%20as%20control%20signals%20for%20animation%20generation.%20Such%20latents%20implicitly%20capture%20nuanced%20facial%20expression%20dynamics%20with%20identity%20and%20pose%20information%20disentangled%2C%20and%20they%20are%20efficiently%20injected%20into%20the%20video%20diffusion%20transformer%20through%20our%20proposed%20expression%20controller.%20For%20camera%20and%20head%20pose%20control%2C%20we%20employ%20Pl%C3%BCcker%20ray%20maps%20and%20normal%20maps%20rendered%20from%203D%20body%20mesh%20tracking.%20To%20train%20our%20model%2C%20we%20curate%20a%20large-scale%20synthetic%20dataset%20containing%20diverse%20combinations%20of%20camera%20viewpoints%2C%20head%20poses%2C%20and%20facial%20expression%20dynamics.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20in%20realism%2C%20expressiveness%2C%20control%20accuracy%2C%20and%20view%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2512.11645v1&entry.124074799=Read"},
{"title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis", "author": "Junjie Ye and Rong Xue and Basile Van Hoorick and Pavel Tokmakov and Muhammad Zubair Irshad and Yue Wang and Vitor Guizilini", "abstract": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.", "link": "http://arxiv.org/abs/2512.11797v1", "date": "2025-12-12", "relevancy": 2.4729, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6319}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6142}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnchorDream%3A%20Repurposing%20Video%20Diffusion%20for%20Embodiment-Aware%20Robot%20Data%20Synthesis&body=Title%3A%20AnchorDream%3A%20Repurposing%20Video%20Diffusion%20for%20Embodiment-Aware%20Robot%20Data%20Synthesis%0AAuthor%3A%20Junjie%20Ye%20and%20Rong%20Xue%20and%20Basile%20Van%20Hoorick%20and%20Pavel%20Tokmakov%20and%20Muhammad%20Zubair%20Irshad%20and%20Yue%20Wang%20and%20Vitor%20Guizilini%0AAbstract%3A%20The%20collection%20of%20large-scale%20and%20diverse%20robot%20demonstrations%20remains%20a%20major%20bottleneck%20for%20imitation%20learning%2C%20as%20real-world%20data%20acquisition%20is%20costly%20and%20simulators%20offer%20limited%20diversity%20and%20fidelity%20with%20pronounced%20sim-to-real%20gaps.%20While%20generative%20models%20present%20an%20attractive%20solution%2C%20existing%20methods%20often%20alter%20only%20visual%20appearances%20without%20creating%20new%20behaviors%2C%20or%20suffer%20from%20embodiment%20inconsistencies%20that%20yield%20implausible%20motions.%20To%20address%20these%20limitations%2C%20we%20introduce%20AnchorDream%2C%20an%20embodiment-aware%20world%20model%20that%20repurposes%20pretrained%20video%20diffusion%20models%20for%20robot%20data%20synthesis.%20AnchorDream%20conditions%20the%20diffusion%20process%20on%20robot%20motion%20renderings%2C%20anchoring%20the%20embodiment%20to%20prevent%20hallucination%20while%20synthesizing%20objects%20and%20environments%20consistent%20with%20the%20robot%27s%20kinematics.%20Starting%20from%20only%20a%20handful%20of%20human%20teleoperation%20demonstrations%2C%20our%20method%20scales%20them%20into%20large%2C%20diverse%2C%20high-quality%20datasets%20without%20requiring%20explicit%20environment%20modeling.%20Experiments%20show%20that%20the%20generated%20data%20leads%20to%20consistent%20improvements%20in%20downstream%20policy%20learning%2C%20with%20relative%20gains%20of%2036.4%25%20in%20simulator%20benchmarks%20and%20nearly%20double%20performance%20in%20real-world%20studies.%20These%20results%20suggest%20that%20grounding%20generative%20world%20models%20in%20robot%20motion%20provides%20a%20practical%20path%20toward%20scaling%20imitation%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnchorDream%253A%2520Repurposing%2520Video%2520Diffusion%2520for%2520Embodiment-Aware%2520Robot%2520Data%2520Synthesis%26entry.906535625%3DJunjie%2520Ye%2520and%2520Rong%2520Xue%2520and%2520Basile%2520Van%2520Hoorick%2520and%2520Pavel%2520Tokmakov%2520and%2520Muhammad%2520Zubair%2520Irshad%2520and%2520Yue%2520Wang%2520and%2520Vitor%2520Guizilini%26entry.1292438233%3DThe%2520collection%2520of%2520large-scale%2520and%2520diverse%2520robot%2520demonstrations%2520remains%2520a%2520major%2520bottleneck%2520for%2520imitation%2520learning%252C%2520as%2520real-world%2520data%2520acquisition%2520is%2520costly%2520and%2520simulators%2520offer%2520limited%2520diversity%2520and%2520fidelity%2520with%2520pronounced%2520sim-to-real%2520gaps.%2520While%2520generative%2520models%2520present%2520an%2520attractive%2520solution%252C%2520existing%2520methods%2520often%2520alter%2520only%2520visual%2520appearances%2520without%2520creating%2520new%2520behaviors%252C%2520or%2520suffer%2520from%2520embodiment%2520inconsistencies%2520that%2520yield%2520implausible%2520motions.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520AnchorDream%252C%2520an%2520embodiment-aware%2520world%2520model%2520that%2520repurposes%2520pretrained%2520video%2520diffusion%2520models%2520for%2520robot%2520data%2520synthesis.%2520AnchorDream%2520conditions%2520the%2520diffusion%2520process%2520on%2520robot%2520motion%2520renderings%252C%2520anchoring%2520the%2520embodiment%2520to%2520prevent%2520hallucination%2520while%2520synthesizing%2520objects%2520and%2520environments%2520consistent%2520with%2520the%2520robot%2527s%2520kinematics.%2520Starting%2520from%2520only%2520a%2520handful%2520of%2520human%2520teleoperation%2520demonstrations%252C%2520our%2520method%2520scales%2520them%2520into%2520large%252C%2520diverse%252C%2520high-quality%2520datasets%2520without%2520requiring%2520explicit%2520environment%2520modeling.%2520Experiments%2520show%2520that%2520the%2520generated%2520data%2520leads%2520to%2520consistent%2520improvements%2520in%2520downstream%2520policy%2520learning%252C%2520with%2520relative%2520gains%2520of%252036.4%2525%2520in%2520simulator%2520benchmarks%2520and%2520nearly%2520double%2520performance%2520in%2520real-world%2520studies.%2520These%2520results%2520suggest%2520that%2520grounding%2520generative%2520world%2520models%2520in%2520robot%2520motion%2520provides%2520a%2520practical%2520path%2520toward%2520scaling%2520imitation%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnchorDream%3A%20Repurposing%20Video%20Diffusion%20for%20Embodiment-Aware%20Robot%20Data%20Synthesis&entry.906535625=Junjie%20Ye%20and%20Rong%20Xue%20and%20Basile%20Van%20Hoorick%20and%20Pavel%20Tokmakov%20and%20Muhammad%20Zubair%20Irshad%20and%20Yue%20Wang%20and%20Vitor%20Guizilini&entry.1292438233=The%20collection%20of%20large-scale%20and%20diverse%20robot%20demonstrations%20remains%20a%20major%20bottleneck%20for%20imitation%20learning%2C%20as%20real-world%20data%20acquisition%20is%20costly%20and%20simulators%20offer%20limited%20diversity%20and%20fidelity%20with%20pronounced%20sim-to-real%20gaps.%20While%20generative%20models%20present%20an%20attractive%20solution%2C%20existing%20methods%20often%20alter%20only%20visual%20appearances%20without%20creating%20new%20behaviors%2C%20or%20suffer%20from%20embodiment%20inconsistencies%20that%20yield%20implausible%20motions.%20To%20address%20these%20limitations%2C%20we%20introduce%20AnchorDream%2C%20an%20embodiment-aware%20world%20model%20that%20repurposes%20pretrained%20video%20diffusion%20models%20for%20robot%20data%20synthesis.%20AnchorDream%20conditions%20the%20diffusion%20process%20on%20robot%20motion%20renderings%2C%20anchoring%20the%20embodiment%20to%20prevent%20hallucination%20while%20synthesizing%20objects%20and%20environments%20consistent%20with%20the%20robot%27s%20kinematics.%20Starting%20from%20only%20a%20handful%20of%20human%20teleoperation%20demonstrations%2C%20our%20method%20scales%20them%20into%20large%2C%20diverse%2C%20high-quality%20datasets%20without%20requiring%20explicit%20environment%20modeling.%20Experiments%20show%20that%20the%20generated%20data%20leads%20to%20consistent%20improvements%20in%20downstream%20policy%20learning%2C%20with%20relative%20gains%20of%2036.4%25%20in%20simulator%20benchmarks%20and%20nearly%20double%20performance%20in%20real-world%20studies.%20These%20results%20suggest%20that%20grounding%20generative%20world%20models%20in%20robot%20motion%20provides%20a%20practical%20path%20toward%20scaling%20imitation%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.11797v1&entry.124074799=Read"},
{"title": "REDELEX: A Framework for Relational Deep Learning Exploration", "author": "Jakub Pele\u0161ka and Gustav \u0160\u00edr", "abstract": "Relational databases (RDBs) are widely regarded as the gold standard for storing structured information. Consequently, predictive tasks leveraging this data format hold significant application promise. Recently, Relational Deep Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized as graph structures, enabling the application of various graph neural architectures to effectively address these tasks. However, given its novelty, there is a lack of analysis into the relationships between the performance of various RDL models and the characteristics of the underlying RDBs.\n  In this study, we present REDELEX$-$a comprehensive exploration framework for evaluating RDL models of varying complexity on the most diverse collection of over 70 RDBs, which we make available to the community. Benchmarked alongside key representatives of classic methods, we confirm the generally superior performance of RDL while providing insights into the main factors shaping performance, including model complexity, database sizes and their structural properties.", "link": "http://arxiv.org/abs/2506.22199v2", "date": "2025-12-12", "relevancy": 2.4639, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REDELEX%3A%20A%20Framework%20for%20Relational%20Deep%20Learning%20Exploration&body=Title%3A%20REDELEX%3A%20A%20Framework%20for%20Relational%20Deep%20Learning%20Exploration%0AAuthor%3A%20Jakub%20Pele%C5%A1ka%20and%20Gustav%20%C5%A0%C3%ADr%0AAbstract%3A%20Relational%20databases%20%28RDBs%29%20are%20widely%20regarded%20as%20the%20gold%20standard%20for%20storing%20structured%20information.%20Consequently%2C%20predictive%20tasks%20leveraging%20this%20data%20format%20hold%20significant%20application%20promise.%20Recently%2C%20Relational%20Deep%20Learning%20%28RDL%29%20has%20emerged%20as%20a%20novel%20paradigm%20wherein%20RDBs%20are%20conceptualized%20as%20graph%20structures%2C%20enabling%20the%20application%20of%20various%20graph%20neural%20architectures%20to%20effectively%20address%20these%20tasks.%20However%2C%20given%20its%20novelty%2C%20there%20is%20a%20lack%20of%20analysis%20into%20the%20relationships%20between%20the%20performance%20of%20various%20RDL%20models%20and%20the%20characteristics%20of%20the%20underlying%20RDBs.%0A%20%20In%20this%20study%2C%20we%20present%20REDELEX%24-%24a%20comprehensive%20exploration%20framework%20for%20evaluating%20RDL%20models%20of%20varying%20complexity%20on%20the%20most%20diverse%20collection%20of%20over%2070%20RDBs%2C%20which%20we%20make%20available%20to%20the%20community.%20Benchmarked%20alongside%20key%20representatives%20of%20classic%20methods%2C%20we%20confirm%20the%20generally%20superior%20performance%20of%20RDL%20while%20providing%20insights%20into%20the%20main%20factors%20shaping%20performance%2C%20including%20model%20complexity%2C%20database%20sizes%20and%20their%20structural%20properties.%0ALink%3A%20http%3A//arxiv.org/abs/2506.22199v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREDELEX%253A%2520A%2520Framework%2520for%2520Relational%2520Deep%2520Learning%2520Exploration%26entry.906535625%3DJakub%2520Pele%25C5%25A1ka%2520and%2520Gustav%2520%25C5%25A0%25C3%25ADr%26entry.1292438233%3DRelational%2520databases%2520%2528RDBs%2529%2520are%2520widely%2520regarded%2520as%2520the%2520gold%2520standard%2520for%2520storing%2520structured%2520information.%2520Consequently%252C%2520predictive%2520tasks%2520leveraging%2520this%2520data%2520format%2520hold%2520significant%2520application%2520promise.%2520Recently%252C%2520Relational%2520Deep%2520Learning%2520%2528RDL%2529%2520has%2520emerged%2520as%2520a%2520novel%2520paradigm%2520wherein%2520RDBs%2520are%2520conceptualized%2520as%2520graph%2520structures%252C%2520enabling%2520the%2520application%2520of%2520various%2520graph%2520neural%2520architectures%2520to%2520effectively%2520address%2520these%2520tasks.%2520However%252C%2520given%2520its%2520novelty%252C%2520there%2520is%2520a%2520lack%2520of%2520analysis%2520into%2520the%2520relationships%2520between%2520the%2520performance%2520of%2520various%2520RDL%2520models%2520and%2520the%2520characteristics%2520of%2520the%2520underlying%2520RDBs.%250A%2520%2520In%2520this%2520study%252C%2520we%2520present%2520REDELEX%2524-%2524a%2520comprehensive%2520exploration%2520framework%2520for%2520evaluating%2520RDL%2520models%2520of%2520varying%2520complexity%2520on%2520the%2520most%2520diverse%2520collection%2520of%2520over%252070%2520RDBs%252C%2520which%2520we%2520make%2520available%2520to%2520the%2520community.%2520Benchmarked%2520alongside%2520key%2520representatives%2520of%2520classic%2520methods%252C%2520we%2520confirm%2520the%2520generally%2520superior%2520performance%2520of%2520RDL%2520while%2520providing%2520insights%2520into%2520the%2520main%2520factors%2520shaping%2520performance%252C%2520including%2520model%2520complexity%252C%2520database%2520sizes%2520and%2520their%2520structural%2520properties.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22199v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REDELEX%3A%20A%20Framework%20for%20Relational%20Deep%20Learning%20Exploration&entry.906535625=Jakub%20Pele%C5%A1ka%20and%20Gustav%20%C5%A0%C3%ADr&entry.1292438233=Relational%20databases%20%28RDBs%29%20are%20widely%20regarded%20as%20the%20gold%20standard%20for%20storing%20structured%20information.%20Consequently%2C%20predictive%20tasks%20leveraging%20this%20data%20format%20hold%20significant%20application%20promise.%20Recently%2C%20Relational%20Deep%20Learning%20%28RDL%29%20has%20emerged%20as%20a%20novel%20paradigm%20wherein%20RDBs%20are%20conceptualized%20as%20graph%20structures%2C%20enabling%20the%20application%20of%20various%20graph%20neural%20architectures%20to%20effectively%20address%20these%20tasks.%20However%2C%20given%20its%20novelty%2C%20there%20is%20a%20lack%20of%20analysis%20into%20the%20relationships%20between%20the%20performance%20of%20various%20RDL%20models%20and%20the%20characteristics%20of%20the%20underlying%20RDBs.%0A%20%20In%20this%20study%2C%20we%20present%20REDELEX%24-%24a%20comprehensive%20exploration%20framework%20for%20evaluating%20RDL%20models%20of%20varying%20complexity%20on%20the%20most%20diverse%20collection%20of%20over%2070%20RDBs%2C%20which%20we%20make%20available%20to%20the%20community.%20Benchmarked%20alongside%20key%20representatives%20of%20classic%20methods%2C%20we%20confirm%20the%20generally%20superior%20performance%20of%20RDL%20while%20providing%20insights%20into%20the%20main%20factors%20shaping%20performance%2C%20including%20model%20complexity%2C%20database%20sizes%20and%20their%20structural%20properties.&entry.1838667208=http%3A//arxiv.org/abs/2506.22199v2&entry.124074799=Read"},
{"title": "CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks", "author": "Yongsheng Huang and Peibo Duan and Yujie Wu and Kai Sun and Zhipeng Liu and Changsheng Zhang and Bin Zhang and Mingkun Xu", "abstract": "Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.", "link": "http://arxiv.org/abs/2512.11743v1", "date": "2025-12-12", "relevancy": 2.4591, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5328}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4788}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogniSNN%3A%20Enabling%20Neuron-Expandability%2C%20Pathway-Reusability%2C%20and%20Dynamic-Configurability%20with%20Random%20Graph%20Architectures%20in%20Spiking%20Neural%20Networks&body=Title%3A%20CogniSNN%3A%20Enabling%20Neuron-Expandability%2C%20Pathway-Reusability%2C%20and%20Dynamic-Configurability%20with%20Random%20Graph%20Architectures%20in%20Spiking%20Neural%20Networks%0AAuthor%3A%20Yongsheng%20Huang%20and%20Peibo%20Duan%20and%20Yujie%20Wu%20and%20Kai%20Sun%20and%20Zhipeng%20Liu%20and%20Changsheng%20Zhang%20and%20Bin%20Zhang%20and%20Mingkun%20Xu%0AAbstract%3A%20Spiking%20neural%20networks%20%28SNNs%29%2C%20regarded%20as%20the%20third%20generation%20of%20artificial%20neural%20networks%2C%20are%20expected%20to%20bridge%20the%20gap%20between%20artificial%20intelligence%20and%20computational%20neuroscience.%20However%2C%20most%20mainstream%20SNN%20research%20directly%20adopts%20the%20rigid%2C%20chain-like%20hierarchical%20architecture%20of%20traditional%20artificial%20neural%20networks%20%28ANNs%29%2C%20ignoring%20key%20structural%20characteristics%20of%20the%20brain.%20Biological%20neurons%20are%20stochastically%20interconnected%2C%20forming%20complex%20neural%20pathways%20that%20exhibit%20Neuron-Expandability%2C%20Pathway-Reusability%2C%20and%20Dynamic-Configurability.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20SNN%20paradigm%2C%20named%20Cognition-aware%20SNN%20%28CogniSNN%29%2C%20by%20incorporating%20Random%20Graph%20Architecture%20%28RGA%29.%20Furthermore%2C%20we%20address%20the%20issues%20of%20network%20degradation%20and%20dimensional%20mismatch%20in%20deep%20pathways%20by%20introducing%20an%20improved%20pure%20spiking%20residual%20mechanism%20alongside%20an%20adaptive%20pooling%20strategy.%20Then%2C%20we%20design%20a%20Key%20Pathway-based%20Learning%20without%20Forgetting%20%28KP-LwF%29%20approach%2C%20which%20selectively%20reuses%20critical%20neural%20pathways%20while%20retaining%20historical%20knowledge%2C%20enabling%20efficient%20multi-task%20transfer.%20Finally%2C%20we%20propose%20a%20Dynamic%20Growth%20Learning%20%28DGL%29%20algorithm%20that%20allows%20neurons%20and%20synapses%20to%20grow%20dynamically%20along%20the%20internal%20temporal%20dimension.%20Extensive%20experiments%20demonstrate%20that%20CogniSNN%20achieves%20performance%20comparable%20to%2C%20or%20even%20surpassing%2C%20current%20state-of-the-art%20SNNs%20on%20neuromorphic%20datasets%20and%20Tiny-ImageNet.%20The%20Pathway-Reusability%20enhances%20the%20network%27s%20continuous%20learning%20capability%20across%20different%20scenarios%2C%20while%20the%20dynamic%20growth%20algorithm%20improves%20robustness%20against%20interference%20and%20mitigates%20the%20fixed-timestep%20constraints%20during%20neuromorphic%20chip%20deployment.%20This%20work%20demonstrates%20the%20potential%20of%20SNNs%20with%20random%20graph%20structures%20in%20advancing%20brain-inspired%20intelligence%20and%20lays%20the%20foundation%20for%20their%20practical%20application%20on%20neuromorphic%20hardware.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogniSNN%253A%2520Enabling%2520Neuron-Expandability%252C%2520Pathway-Reusability%252C%2520and%2520Dynamic-Configurability%2520with%2520Random%2520Graph%2520Architectures%2520in%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DYongsheng%2520Huang%2520and%2520Peibo%2520Duan%2520and%2520Yujie%2520Wu%2520and%2520Kai%2520Sun%2520and%2520Zhipeng%2520Liu%2520and%2520Changsheng%2520Zhang%2520and%2520Bin%2520Zhang%2520and%2520Mingkun%2520Xu%26entry.1292438233%3DSpiking%2520neural%2520networks%2520%2528SNNs%2529%252C%2520regarded%2520as%2520the%2520third%2520generation%2520of%2520artificial%2520neural%2520networks%252C%2520are%2520expected%2520to%2520bridge%2520the%2520gap%2520between%2520artificial%2520intelligence%2520and%2520computational%2520neuroscience.%2520However%252C%2520most%2520mainstream%2520SNN%2520research%2520directly%2520adopts%2520the%2520rigid%252C%2520chain-like%2520hierarchical%2520architecture%2520of%2520traditional%2520artificial%2520neural%2520networks%2520%2528ANNs%2529%252C%2520ignoring%2520key%2520structural%2520characteristics%2520of%2520the%2520brain.%2520Biological%2520neurons%2520are%2520stochastically%2520interconnected%252C%2520forming%2520complex%2520neural%2520pathways%2520that%2520exhibit%2520Neuron-Expandability%252C%2520Pathway-Reusability%252C%2520and%2520Dynamic-Configurability.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520SNN%2520paradigm%252C%2520named%2520Cognition-aware%2520SNN%2520%2528CogniSNN%2529%252C%2520by%2520incorporating%2520Random%2520Graph%2520Architecture%2520%2528RGA%2529.%2520Furthermore%252C%2520we%2520address%2520the%2520issues%2520of%2520network%2520degradation%2520and%2520dimensional%2520mismatch%2520in%2520deep%2520pathways%2520by%2520introducing%2520an%2520improved%2520pure%2520spiking%2520residual%2520mechanism%2520alongside%2520an%2520adaptive%2520pooling%2520strategy.%2520Then%252C%2520we%2520design%2520a%2520Key%2520Pathway-based%2520Learning%2520without%2520Forgetting%2520%2528KP-LwF%2529%2520approach%252C%2520which%2520selectively%2520reuses%2520critical%2520neural%2520pathways%2520while%2520retaining%2520historical%2520knowledge%252C%2520enabling%2520efficient%2520multi-task%2520transfer.%2520Finally%252C%2520we%2520propose%2520a%2520Dynamic%2520Growth%2520Learning%2520%2528DGL%2529%2520algorithm%2520that%2520allows%2520neurons%2520and%2520synapses%2520to%2520grow%2520dynamically%2520along%2520the%2520internal%2520temporal%2520dimension.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CogniSNN%2520achieves%2520performance%2520comparable%2520to%252C%2520or%2520even%2520surpassing%252C%2520current%2520state-of-the-art%2520SNNs%2520on%2520neuromorphic%2520datasets%2520and%2520Tiny-ImageNet.%2520The%2520Pathway-Reusability%2520enhances%2520the%2520network%2527s%2520continuous%2520learning%2520capability%2520across%2520different%2520scenarios%252C%2520while%2520the%2520dynamic%2520growth%2520algorithm%2520improves%2520robustness%2520against%2520interference%2520and%2520mitigates%2520the%2520fixed-timestep%2520constraints%2520during%2520neuromorphic%2520chip%2520deployment.%2520This%2520work%2520demonstrates%2520the%2520potential%2520of%2520SNNs%2520with%2520random%2520graph%2520structures%2520in%2520advancing%2520brain-inspired%2520intelligence%2520and%2520lays%2520the%2520foundation%2520for%2520their%2520practical%2520application%2520on%2520neuromorphic%2520hardware.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogniSNN%3A%20Enabling%20Neuron-Expandability%2C%20Pathway-Reusability%2C%20and%20Dynamic-Configurability%20with%20Random%20Graph%20Architectures%20in%20Spiking%20Neural%20Networks&entry.906535625=Yongsheng%20Huang%20and%20Peibo%20Duan%20and%20Yujie%20Wu%20and%20Kai%20Sun%20and%20Zhipeng%20Liu%20and%20Changsheng%20Zhang%20and%20Bin%20Zhang%20and%20Mingkun%20Xu&entry.1292438233=Spiking%20neural%20networks%20%28SNNs%29%2C%20regarded%20as%20the%20third%20generation%20of%20artificial%20neural%20networks%2C%20are%20expected%20to%20bridge%20the%20gap%20between%20artificial%20intelligence%20and%20computational%20neuroscience.%20However%2C%20most%20mainstream%20SNN%20research%20directly%20adopts%20the%20rigid%2C%20chain-like%20hierarchical%20architecture%20of%20traditional%20artificial%20neural%20networks%20%28ANNs%29%2C%20ignoring%20key%20structural%20characteristics%20of%20the%20brain.%20Biological%20neurons%20are%20stochastically%20interconnected%2C%20forming%20complex%20neural%20pathways%20that%20exhibit%20Neuron-Expandability%2C%20Pathway-Reusability%2C%20and%20Dynamic-Configurability.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20SNN%20paradigm%2C%20named%20Cognition-aware%20SNN%20%28CogniSNN%29%2C%20by%20incorporating%20Random%20Graph%20Architecture%20%28RGA%29.%20Furthermore%2C%20we%20address%20the%20issues%20of%20network%20degradation%20and%20dimensional%20mismatch%20in%20deep%20pathways%20by%20introducing%20an%20improved%20pure%20spiking%20residual%20mechanism%20alongside%20an%20adaptive%20pooling%20strategy.%20Then%2C%20we%20design%20a%20Key%20Pathway-based%20Learning%20without%20Forgetting%20%28KP-LwF%29%20approach%2C%20which%20selectively%20reuses%20critical%20neural%20pathways%20while%20retaining%20historical%20knowledge%2C%20enabling%20efficient%20multi-task%20transfer.%20Finally%2C%20we%20propose%20a%20Dynamic%20Growth%20Learning%20%28DGL%29%20algorithm%20that%20allows%20neurons%20and%20synapses%20to%20grow%20dynamically%20along%20the%20internal%20temporal%20dimension.%20Extensive%20experiments%20demonstrate%20that%20CogniSNN%20achieves%20performance%20comparable%20to%2C%20or%20even%20surpassing%2C%20current%20state-of-the-art%20SNNs%20on%20neuromorphic%20datasets%20and%20Tiny-ImageNet.%20The%20Pathway-Reusability%20enhances%20the%20network%27s%20continuous%20learning%20capability%20across%20different%20scenarios%2C%20while%20the%20dynamic%20growth%20algorithm%20improves%20robustness%20against%20interference%20and%20mitigates%20the%20fixed-timestep%20constraints%20during%20neuromorphic%20chip%20deployment.%20This%20work%20demonstrates%20the%20potential%20of%20SNNs%20with%20random%20graph%20structures%20in%20advancing%20brain-inspired%20intelligence%20and%20lays%20the%20foundation%20for%20their%20practical%20application%20on%20neuromorphic%20hardware.&entry.1838667208=http%3A//arxiv.org/abs/2512.11743v1&entry.124074799=Read"},
{"title": "Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation", "author": "Luca Cazzola and Ahed Alboody", "abstract": "The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).", "link": "http://arxiv.org/abs/2512.11654v1", "date": "2025-12-12", "relevancy": 2.4485, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6463}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6238}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinetic%20Mining%20in%20Context%3A%20Few-Shot%20Action%20Synthesis%20via%20Text-to-Motion%20Distillation&body=Title%3A%20Kinetic%20Mining%20in%20Context%3A%20Few-Shot%20Action%20Synthesis%20via%20Text-to-Motion%20Distillation%0AAuthor%3A%20Luca%20Cazzola%20and%20Ahed%20Alboody%0AAbstract%3A%20The%20acquisition%20cost%20for%20large%2C%20annotated%20motion%20datasets%20remains%20a%20critical%20bottleneck%20for%20skeletal-based%20Human%20Activity%20Recognition%20%28HAR%29.%20Although%20Text-to-Motion%20%28T2M%29%20generative%20models%20offer%20a%20compelling%2C%20scalable%20source%20of%20synthetic%20data%2C%20their%20training%20objectives%2C%20which%20emphasize%20general%20artistic%20motion%2C%20and%20dataset%20structures%20fundamentally%20differ%20from%20HAR%27s%20requirements%20for%20kinematically%20precise%2C%20class-discriminative%20actions.%20This%20disparity%20creates%20a%20significant%20domain%20gap%2C%20making%20generalist%20T2M%20models%20ill-equipped%20for%20generating%20motions%20suitable%20for%20HAR%20classifiers.%20To%20address%20this%20challenge%2C%20we%20propose%20KineMIC%20%28Kinetic%20Mining%20In%20Context%29%2C%20a%20transfer%20learning%20framework%20for%20few-shot%20action%20synthesis.%20KineMIC%20adapts%20a%20T2M%20diffusion%20model%20to%20an%20HAR%20domain%20by%20hypothesizing%20that%20semantic%20correspondences%20in%20the%20text%20encoding%20space%20can%20provide%20soft%20supervision%20for%20kinematic%20distillation.%20We%20operationalize%20this%20via%20a%20kinetic%20mining%20strategy%20that%20leverages%20CLIP%20text%20embeddings%20to%20establish%20correspondences%20between%20sparse%20HAR%20labels%20and%20T2M%20source%20data.%20This%20process%20guides%20fine-tuning%2C%20transforming%20the%20generalist%20T2M%20backbone%20into%20a%20specialized%20few-shot%20Action-to-Motion%20generator.%20We%20validate%20KineMIC%20using%20HumanML3D%20as%20the%20source%20T2M%20dataset%20and%20a%20subset%20of%20NTU%20RGB%2BD%20120%20as%20the%20target%20HAR%20domain%2C%20randomly%20selecting%20just%2010%20samples%20per%20action%20class.%20Our%20approach%20generates%20significantly%20more%20coherent%20motions%2C%20providing%20a%20robust%20data%20augmentation%20source%20that%20delivers%20a%20%2B23.1%25%20accuracy%20points%20improvement.%20Animated%20illustrations%20and%20supplementary%20materials%20are%20available%20at%20%28https%3A//lucazzola.github.io/publications/kinemic%29.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinetic%2520Mining%2520in%2520Context%253A%2520Few-Shot%2520Action%2520Synthesis%2520via%2520Text-to-Motion%2520Distillation%26entry.906535625%3DLuca%2520Cazzola%2520and%2520Ahed%2520Alboody%26entry.1292438233%3DThe%2520acquisition%2520cost%2520for%2520large%252C%2520annotated%2520motion%2520datasets%2520remains%2520a%2520critical%2520bottleneck%2520for%2520skeletal-based%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529.%2520Although%2520Text-to-Motion%2520%2528T2M%2529%2520generative%2520models%2520offer%2520a%2520compelling%252C%2520scalable%2520source%2520of%2520synthetic%2520data%252C%2520their%2520training%2520objectives%252C%2520which%2520emphasize%2520general%2520artistic%2520motion%252C%2520and%2520dataset%2520structures%2520fundamentally%2520differ%2520from%2520HAR%2527s%2520requirements%2520for%2520kinematically%2520precise%252C%2520class-discriminative%2520actions.%2520This%2520disparity%2520creates%2520a%2520significant%2520domain%2520gap%252C%2520making%2520generalist%2520T2M%2520models%2520ill-equipped%2520for%2520generating%2520motions%2520suitable%2520for%2520HAR%2520classifiers.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520KineMIC%2520%2528Kinetic%2520Mining%2520In%2520Context%2529%252C%2520a%2520transfer%2520learning%2520framework%2520for%2520few-shot%2520action%2520synthesis.%2520KineMIC%2520adapts%2520a%2520T2M%2520diffusion%2520model%2520to%2520an%2520HAR%2520domain%2520by%2520hypothesizing%2520that%2520semantic%2520correspondences%2520in%2520the%2520text%2520encoding%2520space%2520can%2520provide%2520soft%2520supervision%2520for%2520kinematic%2520distillation.%2520We%2520operationalize%2520this%2520via%2520a%2520kinetic%2520mining%2520strategy%2520that%2520leverages%2520CLIP%2520text%2520embeddings%2520to%2520establish%2520correspondences%2520between%2520sparse%2520HAR%2520labels%2520and%2520T2M%2520source%2520data.%2520This%2520process%2520guides%2520fine-tuning%252C%2520transforming%2520the%2520generalist%2520T2M%2520backbone%2520into%2520a%2520specialized%2520few-shot%2520Action-to-Motion%2520generator.%2520We%2520validate%2520KineMIC%2520using%2520HumanML3D%2520as%2520the%2520source%2520T2M%2520dataset%2520and%2520a%2520subset%2520of%2520NTU%2520RGB%252BD%2520120%2520as%2520the%2520target%2520HAR%2520domain%252C%2520randomly%2520selecting%2520just%252010%2520samples%2520per%2520action%2520class.%2520Our%2520approach%2520generates%2520significantly%2520more%2520coherent%2520motions%252C%2520providing%2520a%2520robust%2520data%2520augmentation%2520source%2520that%2520delivers%2520a%2520%252B23.1%2525%2520accuracy%2520points%2520improvement.%2520Animated%2520illustrations%2520and%2520supplementary%2520materials%2520are%2520available%2520at%2520%2528https%253A//lucazzola.github.io/publications/kinemic%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinetic%20Mining%20in%20Context%3A%20Few-Shot%20Action%20Synthesis%20via%20Text-to-Motion%20Distillation&entry.906535625=Luca%20Cazzola%20and%20Ahed%20Alboody&entry.1292438233=The%20acquisition%20cost%20for%20large%2C%20annotated%20motion%20datasets%20remains%20a%20critical%20bottleneck%20for%20skeletal-based%20Human%20Activity%20Recognition%20%28HAR%29.%20Although%20Text-to-Motion%20%28T2M%29%20generative%20models%20offer%20a%20compelling%2C%20scalable%20source%20of%20synthetic%20data%2C%20their%20training%20objectives%2C%20which%20emphasize%20general%20artistic%20motion%2C%20and%20dataset%20structures%20fundamentally%20differ%20from%20HAR%27s%20requirements%20for%20kinematically%20precise%2C%20class-discriminative%20actions.%20This%20disparity%20creates%20a%20significant%20domain%20gap%2C%20making%20generalist%20T2M%20models%20ill-equipped%20for%20generating%20motions%20suitable%20for%20HAR%20classifiers.%20To%20address%20this%20challenge%2C%20we%20propose%20KineMIC%20%28Kinetic%20Mining%20In%20Context%29%2C%20a%20transfer%20learning%20framework%20for%20few-shot%20action%20synthesis.%20KineMIC%20adapts%20a%20T2M%20diffusion%20model%20to%20an%20HAR%20domain%20by%20hypothesizing%20that%20semantic%20correspondences%20in%20the%20text%20encoding%20space%20can%20provide%20soft%20supervision%20for%20kinematic%20distillation.%20We%20operationalize%20this%20via%20a%20kinetic%20mining%20strategy%20that%20leverages%20CLIP%20text%20embeddings%20to%20establish%20correspondences%20between%20sparse%20HAR%20labels%20and%20T2M%20source%20data.%20This%20process%20guides%20fine-tuning%2C%20transforming%20the%20generalist%20T2M%20backbone%20into%20a%20specialized%20few-shot%20Action-to-Motion%20generator.%20We%20validate%20KineMIC%20using%20HumanML3D%20as%20the%20source%20T2M%20dataset%20and%20a%20subset%20of%20NTU%20RGB%2BD%20120%20as%20the%20target%20HAR%20domain%2C%20randomly%20selecting%20just%2010%20samples%20per%20action%20class.%20Our%20approach%20generates%20significantly%20more%20coherent%20motions%2C%20providing%20a%20robust%20data%20augmentation%20source%20that%20delivers%20a%20%2B23.1%25%20accuracy%20points%20improvement.%20Animated%20illustrations%20and%20supplementary%20materials%20are%20available%20at%20%28https%3A//lucazzola.github.io/publications/kinemic%29.&entry.1838667208=http%3A//arxiv.org/abs/2512.11654v1&entry.124074799=Read"},
{"title": "General-purpose AI models can generate actionable knowledge on agroecological crop protection", "author": "Kris A. G. Wyckhuys", "abstract": "Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.", "link": "http://arxiv.org/abs/2512.11474v1", "date": "2025-12-12", "relevancy": 2.437, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5248}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4694}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General-purpose%20AI%20models%20can%20generate%20actionable%20knowledge%20on%20agroecological%20crop%20protection&body=Title%3A%20General-purpose%20AI%20models%20can%20generate%20actionable%20knowledge%20on%20agroecological%20crop%20protection%0AAuthor%3A%20Kris%20A.%20G.%20Wyckhuys%0AAbstract%3A%20Generative%20artificial%20intelligence%20%28AI%29%20offers%20potential%20for%20democratizing%20scientific%20knowledge%20and%20converting%20this%20to%20clear%2C%20actionable%20information%2C%20yet%20its%20application%20in%20agri-food%20science%20remains%20unexplored.%20Here%2C%20we%20verify%20the%20scientific%20knowledge%20on%20agroecological%20crop%20protection%20that%20is%20generated%20by%20either%20web-grounded%20or%20non-grounded%20large%20language%20models%20%28LLMs%29%2C%20i.e.%2C%20DeepSeek%20versus%20the%20free-tier%20version%20of%20ChatGPT.%20For%20nine%20globally%20limiting%20pests%2C%20weeds%2C%20and%20plant%20diseases%2C%20we%20assessed%20the%20factual%20accuracy%2C%20data%20consistency%2C%20and%20breadth%20of%20knowledge%20or%20data%20completeness%20of%20each%20LLM.%20Overall%2C%20DeepSeek%20consistently%20screened%20a%204.8-49.7-fold%20larger%20literature%20corpus%20and%20reported%201.6-2.4-fold%20more%20biological%20control%20agents%20or%20management%20solutions%20than%20ChatGPT.%20As%20a%20result%2C%20DeepSeek%20reported%2021.6%25%20higher%20efficacy%20estimates%2C%20exhibited%20greater%20laboratory-to-field%20data%20consistency%2C%20and%20showed%20more%20realistic%20effects%20of%20pest%20identity%20and%20management%20tactics.%20However%2C%20both%20models%20hallucinated%2C%20i.e.%2C%20fabricated%20fictitious%20agents%20or%20references%2C%20reported%20on%20implausible%20ecological%20interactions%20or%20outcomes%2C%20confused%20old%20and%20new%20scientific%20nomenclatures%2C%20and%20omitted%20data%20on%20key%20agents%20or%20solutions.%20Despite%20these%20shortcomings%2C%20both%20LLMs%20correctly%20reported%20low-resolution%20efficacy%20trends.%20Overall%2C%20when%20paired%20with%20rigorous%20human%20oversight%2C%20LLMs%20may%20pose%20a%20powerful%20tool%20to%20support%20farm-level%20decision-making%20and%20unleash%20scientific%20creativity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral-purpose%2520AI%2520models%2520can%2520generate%2520actionable%2520knowledge%2520on%2520agroecological%2520crop%2520protection%26entry.906535625%3DKris%2520A.%2520G.%2520Wyckhuys%26entry.1292438233%3DGenerative%2520artificial%2520intelligence%2520%2528AI%2529%2520offers%2520potential%2520for%2520democratizing%2520scientific%2520knowledge%2520and%2520converting%2520this%2520to%2520clear%252C%2520actionable%2520information%252C%2520yet%2520its%2520application%2520in%2520agri-food%2520science%2520remains%2520unexplored.%2520Here%252C%2520we%2520verify%2520the%2520scientific%2520knowledge%2520on%2520agroecological%2520crop%2520protection%2520that%2520is%2520generated%2520by%2520either%2520web-grounded%2520or%2520non-grounded%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520i.e.%252C%2520DeepSeek%2520versus%2520the%2520free-tier%2520version%2520of%2520ChatGPT.%2520For%2520nine%2520globally%2520limiting%2520pests%252C%2520weeds%252C%2520and%2520plant%2520diseases%252C%2520we%2520assessed%2520the%2520factual%2520accuracy%252C%2520data%2520consistency%252C%2520and%2520breadth%2520of%2520knowledge%2520or%2520data%2520completeness%2520of%2520each%2520LLM.%2520Overall%252C%2520DeepSeek%2520consistently%2520screened%2520a%25204.8-49.7-fold%2520larger%2520literature%2520corpus%2520and%2520reported%25201.6-2.4-fold%2520more%2520biological%2520control%2520agents%2520or%2520management%2520solutions%2520than%2520ChatGPT.%2520As%2520a%2520result%252C%2520DeepSeek%2520reported%252021.6%2525%2520higher%2520efficacy%2520estimates%252C%2520exhibited%2520greater%2520laboratory-to-field%2520data%2520consistency%252C%2520and%2520showed%2520more%2520realistic%2520effects%2520of%2520pest%2520identity%2520and%2520management%2520tactics.%2520However%252C%2520both%2520models%2520hallucinated%252C%2520i.e.%252C%2520fabricated%2520fictitious%2520agents%2520or%2520references%252C%2520reported%2520on%2520implausible%2520ecological%2520interactions%2520or%2520outcomes%252C%2520confused%2520old%2520and%2520new%2520scientific%2520nomenclatures%252C%2520and%2520omitted%2520data%2520on%2520key%2520agents%2520or%2520solutions.%2520Despite%2520these%2520shortcomings%252C%2520both%2520LLMs%2520correctly%2520reported%2520low-resolution%2520efficacy%2520trends.%2520Overall%252C%2520when%2520paired%2520with%2520rigorous%2520human%2520oversight%252C%2520LLMs%2520may%2520pose%2520a%2520powerful%2520tool%2520to%2520support%2520farm-level%2520decision-making%2520and%2520unleash%2520scientific%2520creativity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General-purpose%20AI%20models%20can%20generate%20actionable%20knowledge%20on%20agroecological%20crop%20protection&entry.906535625=Kris%20A.%20G.%20Wyckhuys&entry.1292438233=Generative%20artificial%20intelligence%20%28AI%29%20offers%20potential%20for%20democratizing%20scientific%20knowledge%20and%20converting%20this%20to%20clear%2C%20actionable%20information%2C%20yet%20its%20application%20in%20agri-food%20science%20remains%20unexplored.%20Here%2C%20we%20verify%20the%20scientific%20knowledge%20on%20agroecological%20crop%20protection%20that%20is%20generated%20by%20either%20web-grounded%20or%20non-grounded%20large%20language%20models%20%28LLMs%29%2C%20i.e.%2C%20DeepSeek%20versus%20the%20free-tier%20version%20of%20ChatGPT.%20For%20nine%20globally%20limiting%20pests%2C%20weeds%2C%20and%20plant%20diseases%2C%20we%20assessed%20the%20factual%20accuracy%2C%20data%20consistency%2C%20and%20breadth%20of%20knowledge%20or%20data%20completeness%20of%20each%20LLM.%20Overall%2C%20DeepSeek%20consistently%20screened%20a%204.8-49.7-fold%20larger%20literature%20corpus%20and%20reported%201.6-2.4-fold%20more%20biological%20control%20agents%20or%20management%20solutions%20than%20ChatGPT.%20As%20a%20result%2C%20DeepSeek%20reported%2021.6%25%20higher%20efficacy%20estimates%2C%20exhibited%20greater%20laboratory-to-field%20data%20consistency%2C%20and%20showed%20more%20realistic%20effects%20of%20pest%20identity%20and%20management%20tactics.%20However%2C%20both%20models%20hallucinated%2C%20i.e.%2C%20fabricated%20fictitious%20agents%20or%20references%2C%20reported%20on%20implausible%20ecological%20interactions%20or%20outcomes%2C%20confused%20old%20and%20new%20scientific%20nomenclatures%2C%20and%20omitted%20data%20on%20key%20agents%20or%20solutions.%20Despite%20these%20shortcomings%2C%20both%20LLMs%20correctly%20reported%20low-resolution%20efficacy%20trends.%20Overall%2C%20when%20paired%20with%20rigorous%20human%20oversight%2C%20LLMs%20may%20pose%20a%20powerful%20tool%20to%20support%20farm-level%20decision-making%20and%20unleash%20scientific%20creativity.&entry.1838667208=http%3A//arxiv.org/abs/2512.11474v1&entry.124074799=Read"},
{"title": "Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing", "author": "Rodrigo Tertulino and Ricardo Almeida", "abstract": "This study proposes and validates a Federated Learning (FL) framework to proactively identify at-risk students while preserving data privacy. Persistently high dropout rates in distance education remain a pressing institutional challenge. Using the large-scale OULAD dataset, we simulate a privacy-centric scenario where models are trained on early academic performance and digital engagement patterns. Our work investigates the practical trade-offs between model complexity (Logistic Regression vs. a Deep Neural Network) and the impact of local data balancing. The resulting federated model achieves strong predictive power (ROC AUC approximately 85%), demonstrating that FL is a practical and scalable solution for early-warning systems that inherently respects student data sovereignty.", "link": "http://arxiv.org/abs/2508.18316v3", "date": "2025-12-12", "relevancy": 2.4161, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4887}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4864}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Federated%20Learning%20for%20At-Risk%20Student%20Prediction%3A%20A%20Comparative%20Analysis%20of%20Model%20Complexity%20and%20Data%20Balancing&body=Title%3A%20Evaluating%20Federated%20Learning%20for%20At-Risk%20Student%20Prediction%3A%20A%20Comparative%20Analysis%20of%20Model%20Complexity%20and%20Data%20Balancing%0AAuthor%3A%20Rodrigo%20Tertulino%20and%20Ricardo%20Almeida%0AAbstract%3A%20This%20study%20proposes%20and%20validates%20a%20Federated%20Learning%20%28FL%29%20framework%20to%20proactively%20identify%20at-risk%20students%20while%20preserving%20data%20privacy.%20Persistently%20high%20dropout%20rates%20in%20distance%20education%20remain%20a%20pressing%20institutional%20challenge.%20Using%20the%20large-scale%20OULAD%20dataset%2C%20we%20simulate%20a%20privacy-centric%20scenario%20where%20models%20are%20trained%20on%20early%20academic%20performance%20and%20digital%20engagement%20patterns.%20Our%20work%20investigates%20the%20practical%20trade-offs%20between%20model%20complexity%20%28Logistic%20Regression%20vs.%20a%20Deep%20Neural%20Network%29%20and%20the%20impact%20of%20local%20data%20balancing.%20The%20resulting%20federated%20model%20achieves%20strong%20predictive%20power%20%28ROC%20AUC%20approximately%2085%25%29%2C%20demonstrating%20that%20FL%20is%20a%20practical%20and%20scalable%20solution%20for%20early-warning%20systems%20that%20inherently%20respects%20student%20data%20sovereignty.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18316v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Federated%2520Learning%2520for%2520At-Risk%2520Student%2520Prediction%253A%2520A%2520Comparative%2520Analysis%2520of%2520Model%2520Complexity%2520and%2520Data%2520Balancing%26entry.906535625%3DRodrigo%2520Tertulino%2520and%2520Ricardo%2520Almeida%26entry.1292438233%3DThis%2520study%2520proposes%2520and%2520validates%2520a%2520Federated%2520Learning%2520%2528FL%2529%2520framework%2520to%2520proactively%2520identify%2520at-risk%2520students%2520while%2520preserving%2520data%2520privacy.%2520Persistently%2520high%2520dropout%2520rates%2520in%2520distance%2520education%2520remain%2520a%2520pressing%2520institutional%2520challenge.%2520Using%2520the%2520large-scale%2520OULAD%2520dataset%252C%2520we%2520simulate%2520a%2520privacy-centric%2520scenario%2520where%2520models%2520are%2520trained%2520on%2520early%2520academic%2520performance%2520and%2520digital%2520engagement%2520patterns.%2520Our%2520work%2520investigates%2520the%2520practical%2520trade-offs%2520between%2520model%2520complexity%2520%2528Logistic%2520Regression%2520vs.%2520a%2520Deep%2520Neural%2520Network%2529%2520and%2520the%2520impact%2520of%2520local%2520data%2520balancing.%2520The%2520resulting%2520federated%2520model%2520achieves%2520strong%2520predictive%2520power%2520%2528ROC%2520AUC%2520approximately%252085%2525%2529%252C%2520demonstrating%2520that%2520FL%2520is%2520a%2520practical%2520and%2520scalable%2520solution%2520for%2520early-warning%2520systems%2520that%2520inherently%2520respects%2520student%2520data%2520sovereignty.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18316v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Federated%20Learning%20for%20At-Risk%20Student%20Prediction%3A%20A%20Comparative%20Analysis%20of%20Model%20Complexity%20and%20Data%20Balancing&entry.906535625=Rodrigo%20Tertulino%20and%20Ricardo%20Almeida&entry.1292438233=This%20study%20proposes%20and%20validates%20a%20Federated%20Learning%20%28FL%29%20framework%20to%20proactively%20identify%20at-risk%20students%20while%20preserving%20data%20privacy.%20Persistently%20high%20dropout%20rates%20in%20distance%20education%20remain%20a%20pressing%20institutional%20challenge.%20Using%20the%20large-scale%20OULAD%20dataset%2C%20we%20simulate%20a%20privacy-centric%20scenario%20where%20models%20are%20trained%20on%20early%20academic%20performance%20and%20digital%20engagement%20patterns.%20Our%20work%20investigates%20the%20practical%20trade-offs%20between%20model%20complexity%20%28Logistic%20Regression%20vs.%20a%20Deep%20Neural%20Network%29%20and%20the%20impact%20of%20local%20data%20balancing.%20The%20resulting%20federated%20model%20achieves%20strong%20predictive%20power%20%28ROC%20AUC%20approximately%2085%25%29%2C%20demonstrating%20that%20FL%20is%20a%20practical%20and%20scalable%20solution%20for%20early-warning%20systems%20that%20inherently%20respects%20student%20data%20sovereignty.&entry.1838667208=http%3A//arxiv.org/abs/2508.18316v3&entry.124074799=Read"},
{"title": "Visualizing token importance for black-box language models", "author": "Paulius Rauba and Qiyao Wei and Mihaela van der Schaar", "abstract": "We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.", "link": "http://arxiv.org/abs/2512.11573v1", "date": "2025-12-12", "relevancy": 2.4043, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visualizing%20token%20importance%20for%20black-box%20language%20models&body=Title%3A%20Visualizing%20token%20importance%20for%20black-box%20language%20models%0AAuthor%3A%20Paulius%20Rauba%20and%20Qiyao%20Wei%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20We%20consider%20the%20problem%20of%20auditing%20black-box%20large%20language%20models%20%28LLMs%29%20to%20ensure%20they%20behave%20reliably%20when%20deployed%20in%20production%20settings%2C%20particularly%20in%20high-stakes%20domains%20such%20as%20legal%2C%20medical%2C%20and%20regulatory%20compliance.%20Existing%20approaches%20for%20LLM%20auditing%20often%20focus%20on%20isolated%20aspects%20of%20model%20behavior%2C%20such%20as%20detecting%20specific%20biases%20or%20evaluating%20fairness.%20We%20are%20interested%20in%20a%20more%20general%20question%20--%20can%20we%20understand%20how%20the%20outputs%20of%20black-box%20LLMs%20depend%20on%20each%20input%20token%3F%20There%20is%20a%20critical%20need%20to%20have%20such%20tools%20in%20real-world%20applications%20that%20rely%20on%20inaccessible%20API%20endpoints%20to%20language%20models.%20However%2C%20this%20is%20a%20highly%20non-trivial%20problem%2C%20as%20LLMs%20are%20stochastic%20functions%20%28i.e.%20two%20outputs%20will%20be%20different%20by%20chance%29%2C%20while%20computing%20prompt-level%20gradients%20to%20approximate%20input%20sensitivity%20is%20infeasible.%20To%20address%20this%2C%20we%20propose%20Distribution-Based%20Sensitivity%20Analysis%20%28DBSA%29%2C%20a%20lightweight%20model-agnostic%20procedure%20to%20evaluate%20the%20sensitivity%20of%20the%20output%20of%20a%20language%20model%20for%20each%20input%20token%2C%20without%20making%20any%20distributional%20assumptions%20about%20the%20LLM.%20DBSA%20is%20developed%20as%20a%20practical%20tool%20for%20practitioners%2C%20enabling%20quick%2C%20plug-and-play%20visual%20exploration%20of%20LLMs%20reliance%20on%20specific%20input%20tokens.%20Through%20illustrative%20examples%2C%20we%20demonstrate%20how%20DBSA%20can%20enable%20users%20to%20inspect%20LLM%20inputs%20and%20find%20sensitivities%20that%20may%20be%20overlooked%20by%20existing%20LLM%20interpretability%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualizing%2520token%2520importance%2520for%2520black-box%2520language%2520models%26entry.906535625%3DPaulius%2520Rauba%2520and%2520Qiyao%2520Wei%2520and%2520Mihaela%2520van%2520der%2520Schaar%26entry.1292438233%3DWe%2520consider%2520the%2520problem%2520of%2520auditing%2520black-box%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520ensure%2520they%2520behave%2520reliably%2520when%2520deployed%2520in%2520production%2520settings%252C%2520particularly%2520in%2520high-stakes%2520domains%2520such%2520as%2520legal%252C%2520medical%252C%2520and%2520regulatory%2520compliance.%2520Existing%2520approaches%2520for%2520LLM%2520auditing%2520often%2520focus%2520on%2520isolated%2520aspects%2520of%2520model%2520behavior%252C%2520such%2520as%2520detecting%2520specific%2520biases%2520or%2520evaluating%2520fairness.%2520We%2520are%2520interested%2520in%2520a%2520more%2520general%2520question%2520--%2520can%2520we%2520understand%2520how%2520the%2520outputs%2520of%2520black-box%2520LLMs%2520depend%2520on%2520each%2520input%2520token%253F%2520There%2520is%2520a%2520critical%2520need%2520to%2520have%2520such%2520tools%2520in%2520real-world%2520applications%2520that%2520rely%2520on%2520inaccessible%2520API%2520endpoints%2520to%2520language%2520models.%2520However%252C%2520this%2520is%2520a%2520highly%2520non-trivial%2520problem%252C%2520as%2520LLMs%2520are%2520stochastic%2520functions%2520%2528i.e.%2520two%2520outputs%2520will%2520be%2520different%2520by%2520chance%2529%252C%2520while%2520computing%2520prompt-level%2520gradients%2520to%2520approximate%2520input%2520sensitivity%2520is%2520infeasible.%2520To%2520address%2520this%252C%2520we%2520propose%2520Distribution-Based%2520Sensitivity%2520Analysis%2520%2528DBSA%2529%252C%2520a%2520lightweight%2520model-agnostic%2520procedure%2520to%2520evaluate%2520the%2520sensitivity%2520of%2520the%2520output%2520of%2520a%2520language%2520model%2520for%2520each%2520input%2520token%252C%2520without%2520making%2520any%2520distributional%2520assumptions%2520about%2520the%2520LLM.%2520DBSA%2520is%2520developed%2520as%2520a%2520practical%2520tool%2520for%2520practitioners%252C%2520enabling%2520quick%252C%2520plug-and-play%2520visual%2520exploration%2520of%2520LLMs%2520reliance%2520on%2520specific%2520input%2520tokens.%2520Through%2520illustrative%2520examples%252C%2520we%2520demonstrate%2520how%2520DBSA%2520can%2520enable%2520users%2520to%2520inspect%2520LLM%2520inputs%2520and%2520find%2520sensitivities%2520that%2520may%2520be%2520overlooked%2520by%2520existing%2520LLM%2520interpretability%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualizing%20token%20importance%20for%20black-box%20language%20models&entry.906535625=Paulius%20Rauba%20and%20Qiyao%20Wei%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=We%20consider%20the%20problem%20of%20auditing%20black-box%20large%20language%20models%20%28LLMs%29%20to%20ensure%20they%20behave%20reliably%20when%20deployed%20in%20production%20settings%2C%20particularly%20in%20high-stakes%20domains%20such%20as%20legal%2C%20medical%2C%20and%20regulatory%20compliance.%20Existing%20approaches%20for%20LLM%20auditing%20often%20focus%20on%20isolated%20aspects%20of%20model%20behavior%2C%20such%20as%20detecting%20specific%20biases%20or%20evaluating%20fairness.%20We%20are%20interested%20in%20a%20more%20general%20question%20--%20can%20we%20understand%20how%20the%20outputs%20of%20black-box%20LLMs%20depend%20on%20each%20input%20token%3F%20There%20is%20a%20critical%20need%20to%20have%20such%20tools%20in%20real-world%20applications%20that%20rely%20on%20inaccessible%20API%20endpoints%20to%20language%20models.%20However%2C%20this%20is%20a%20highly%20non-trivial%20problem%2C%20as%20LLMs%20are%20stochastic%20functions%20%28i.e.%20two%20outputs%20will%20be%20different%20by%20chance%29%2C%20while%20computing%20prompt-level%20gradients%20to%20approximate%20input%20sensitivity%20is%20infeasible.%20To%20address%20this%2C%20we%20propose%20Distribution-Based%20Sensitivity%20Analysis%20%28DBSA%29%2C%20a%20lightweight%20model-agnostic%20procedure%20to%20evaluate%20the%20sensitivity%20of%20the%20output%20of%20a%20language%20model%20for%20each%20input%20token%2C%20without%20making%20any%20distributional%20assumptions%20about%20the%20LLM.%20DBSA%20is%20developed%20as%20a%20practical%20tool%20for%20practitioners%2C%20enabling%20quick%2C%20plug-and-play%20visual%20exploration%20of%20LLMs%20reliance%20on%20specific%20input%20tokens.%20Through%20illustrative%20examples%2C%20we%20demonstrate%20how%20DBSA%20can%20enable%20users%20to%20inspect%20LLM%20inputs%20and%20find%20sensitivities%20that%20may%20be%20overlooked%20by%20existing%20LLM%20interpretability%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.11573v1&entry.124074799=Read"},
{"title": "Defense That Attacks: How Robust Models Become Better Attackers", "author": "Mohamed Awad and Mahmoud Akrm and Walid Gomaa", "abstract": "Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.", "link": "http://arxiv.org/abs/2512.02830v3", "date": "2025-12-12", "relevancy": 2.3996, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defense%20That%20Attacks%3A%20How%20Robust%20Models%20Become%20Better%20Attackers&body=Title%3A%20Defense%20That%20Attacks%3A%20How%20Robust%20Models%20Become%20Better%20Attackers%0AAuthor%3A%20Mohamed%20Awad%20and%20Mahmoud%20Akrm%20and%20Walid%20Gomaa%0AAbstract%3A%20Deep%20learning%20has%20achieved%20great%20success%20in%20computer%20vision%2C%20but%20remains%20vulnerable%20to%20adversarial%20attacks.%20Adversarial%20training%20is%20the%20leading%20defense%20designed%20to%20improve%20model%20robustness.%20However%2C%20its%20effect%20on%20the%20transferability%20of%20attacks%20is%20underexplored.%20In%20this%20work%2C%20we%20ask%20whether%20adversarial%20training%20unintentionally%20increases%20the%20transferability%20of%20adversarial%20examples.%20To%20answer%20this%2C%20we%20trained%20a%20diverse%20zoo%20of%2036%20models%2C%20including%20CNNs%20and%20ViTs%2C%20and%20conducted%20comprehensive%20transferability%20experiments.%20Our%20results%20reveal%20a%20clear%20paradox%3A%20adversarially%20trained%20%28AT%29%20models%20produce%20perturbations%20that%20transfer%20more%20effectively%20than%20those%20from%20standard%20models%2C%20which%20introduce%20a%20new%20ecosystem%20risk.%20To%20enable%20reproducibility%20and%20further%20study%2C%20we%20release%20all%20models%2C%20code%2C%20and%20experimental%20scripts.%20Furthermore%2C%20we%20argue%20that%20robustness%20evaluations%20should%20assess%20not%20only%20the%20resistance%20of%20a%20model%20to%20transferred%20attacks%20but%20also%20its%20propensity%20to%20produce%20transferable%20adversarial%20examples.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02830v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefense%2520That%2520Attacks%253A%2520How%2520Robust%2520Models%2520Become%2520Better%2520Attackers%26entry.906535625%3DMohamed%2520Awad%2520and%2520Mahmoud%2520Akrm%2520and%2520Walid%2520Gomaa%26entry.1292438233%3DDeep%2520learning%2520has%2520achieved%2520great%2520success%2520in%2520computer%2520vision%252C%2520but%2520remains%2520vulnerable%2520to%2520adversarial%2520attacks.%2520Adversarial%2520training%2520is%2520the%2520leading%2520defense%2520designed%2520to%2520improve%2520model%2520robustness.%2520However%252C%2520its%2520effect%2520on%2520the%2520transferability%2520of%2520attacks%2520is%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520ask%2520whether%2520adversarial%2520training%2520unintentionally%2520increases%2520the%2520transferability%2520of%2520adversarial%2520examples.%2520To%2520answer%2520this%252C%2520we%2520trained%2520a%2520diverse%2520zoo%2520of%252036%2520models%252C%2520including%2520CNNs%2520and%2520ViTs%252C%2520and%2520conducted%2520comprehensive%2520transferability%2520experiments.%2520Our%2520results%2520reveal%2520a%2520clear%2520paradox%253A%2520adversarially%2520trained%2520%2528AT%2529%2520models%2520produce%2520perturbations%2520that%2520transfer%2520more%2520effectively%2520than%2520those%2520from%2520standard%2520models%252C%2520which%2520introduce%2520a%2520new%2520ecosystem%2520risk.%2520To%2520enable%2520reproducibility%2520and%2520further%2520study%252C%2520we%2520release%2520all%2520models%252C%2520code%252C%2520and%2520experimental%2520scripts.%2520Furthermore%252C%2520we%2520argue%2520that%2520robustness%2520evaluations%2520should%2520assess%2520not%2520only%2520the%2520resistance%2520of%2520a%2520model%2520to%2520transferred%2520attacks%2520but%2520also%2520its%2520propensity%2520to%2520produce%2520transferable%2520adversarial%2520examples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02830v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defense%20That%20Attacks%3A%20How%20Robust%20Models%20Become%20Better%20Attackers&entry.906535625=Mohamed%20Awad%20and%20Mahmoud%20Akrm%20and%20Walid%20Gomaa&entry.1292438233=Deep%20learning%20has%20achieved%20great%20success%20in%20computer%20vision%2C%20but%20remains%20vulnerable%20to%20adversarial%20attacks.%20Adversarial%20training%20is%20the%20leading%20defense%20designed%20to%20improve%20model%20robustness.%20However%2C%20its%20effect%20on%20the%20transferability%20of%20attacks%20is%20underexplored.%20In%20this%20work%2C%20we%20ask%20whether%20adversarial%20training%20unintentionally%20increases%20the%20transferability%20of%20adversarial%20examples.%20To%20answer%20this%2C%20we%20trained%20a%20diverse%20zoo%20of%2036%20models%2C%20including%20CNNs%20and%20ViTs%2C%20and%20conducted%20comprehensive%20transferability%20experiments.%20Our%20results%20reveal%20a%20clear%20paradox%3A%20adversarially%20trained%20%28AT%29%20models%20produce%20perturbations%20that%20transfer%20more%20effectively%20than%20those%20from%20standard%20models%2C%20which%20introduce%20a%20new%20ecosystem%20risk.%20To%20enable%20reproducibility%20and%20further%20study%2C%20we%20release%20all%20models%2C%20code%2C%20and%20experimental%20scripts.%20Furthermore%2C%20we%20argue%20that%20robustness%20evaluations%20should%20assess%20not%20only%20the%20resistance%20of%20a%20model%20to%20transferred%20attacks%20but%20also%20its%20propensity%20to%20produce%20transferable%20adversarial%20examples.&entry.1838667208=http%3A//arxiv.org/abs/2512.02830v3&entry.124074799=Read"},
{"title": "SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning", "author": "Aditya Tripathi and Karan Sharma and Rahul Mishra and Tapas Kumar Maiti", "abstract": "Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.\n  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.\n  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.", "link": "http://arxiv.org/abs/2512.11760v1", "date": "2025-12-12", "relevancy": 2.3966, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5146}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.472}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectralKrum%3A%20A%20Spectral-Geometric%20Defense%20Against%20Byzantine%20Attacks%20in%20Federated%20Learning&body=Title%3A%20SpectralKrum%3A%20A%20Spectral-Geometric%20Defense%20Against%20Byzantine%20Attacks%20in%20Federated%20Learning%0AAuthor%3A%20Aditya%20Tripathi%20and%20Karan%20Sharma%20and%20Rahul%20Mishra%20and%20Tapas%20Kumar%20Maiti%0AAbstract%3A%20Federated%20Learning%20%28FL%29%20distributes%20model%20training%20across%20clients%20who%20retain%20their%20data%20locally%2C%20but%20this%20architecture%20exposes%20a%20fundamental%20vulnerability%3A%20Byzantine%20clients%20can%20inject%20arbitrarily%20corrupted%20updates%20that%20degrade%20or%20subvert%20the%20global%20model.%20While%20robust%20aggregation%20methods%20%28including%20Krum%2C%20Bulyan%2C%20and%20coordinate-wise%20defenses%29%20offer%20theoretical%20guarantees%20under%20idealized%20assumptions%2C%20their%20effectiveness%20erodes%20substantially%20when%20client%20data%20distributions%20are%20heterogeneous%20%28non-IID%29%20and%20adversaries%20can%20observe%20or%20approximate%20the%20defense%20mechanism.%0A%20%20This%20paper%20introduces%20SpectralKrum%2C%20a%20defense%20that%20fuses%20spectral%20subspace%20estimation%20with%20geometric%20neighbor-based%20selection.%20The%20core%20insight%20is%20that%20benign%20optimization%20trajectories%2C%20despite%20per-client%20heterogeneity%2C%20concentrate%20near%20a%20low-dimensional%20manifold%20that%20can%20be%20estimated%20from%20historical%20aggregates.%20SpectralKrum%20projects%20incoming%20updates%20into%20this%20learned%20subspace%2C%20applies%20Krum%20selection%20in%20compressed%20coordinates%2C%20and%20filters%20candidates%20whose%20orthogonal%20residual%20energy%20exceeds%20a%20data-driven%20threshold.%20The%20method%20requires%20no%20auxiliary%20data%2C%20operates%20entirely%20on%20model%20updates%2C%20and%20preserves%20FL%20privacy%20properties.%0A%20%20We%20evaluate%20SpectralKrum%20against%20eight%20robust%20baselines%20across%20seven%20attack%20scenarios%20on%20CIFAR-10%20with%20Dirichlet-distributed%20non-IID%20partitions%20%28alpha%20%3D%200.1%29.%20Experiments%20spanning%20over%2056%2C000%20training%20rounds%20show%20that%20SpectralKrum%20is%20competitive%20against%20directional%20and%20subspace-aware%20attacks%20%28adaptive-steer%2C%20buffer-drift%29%2C%20but%20offers%20limited%20advantage%20under%20label-flip%20and%20min-max%20attacks%20where%20malicious%20updates%20remain%20spectrally%20indistinguishable%20from%20benign%20ones.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectralKrum%253A%2520A%2520Spectral-Geometric%2520Defense%2520Against%2520Byzantine%2520Attacks%2520in%2520Federated%2520Learning%26entry.906535625%3DAditya%2520Tripathi%2520and%2520Karan%2520Sharma%2520and%2520Rahul%2520Mishra%2520and%2520Tapas%2520Kumar%2520Maiti%26entry.1292438233%3DFederated%2520Learning%2520%2528FL%2529%2520distributes%2520model%2520training%2520across%2520clients%2520who%2520retain%2520their%2520data%2520locally%252C%2520but%2520this%2520architecture%2520exposes%2520a%2520fundamental%2520vulnerability%253A%2520Byzantine%2520clients%2520can%2520inject%2520arbitrarily%2520corrupted%2520updates%2520that%2520degrade%2520or%2520subvert%2520the%2520global%2520model.%2520While%2520robust%2520aggregation%2520methods%2520%2528including%2520Krum%252C%2520Bulyan%252C%2520and%2520coordinate-wise%2520defenses%2529%2520offer%2520theoretical%2520guarantees%2520under%2520idealized%2520assumptions%252C%2520their%2520effectiveness%2520erodes%2520substantially%2520when%2520client%2520data%2520distributions%2520are%2520heterogeneous%2520%2528non-IID%2529%2520and%2520adversaries%2520can%2520observe%2520or%2520approximate%2520the%2520defense%2520mechanism.%250A%2520%2520This%2520paper%2520introduces%2520SpectralKrum%252C%2520a%2520defense%2520that%2520fuses%2520spectral%2520subspace%2520estimation%2520with%2520geometric%2520neighbor-based%2520selection.%2520The%2520core%2520insight%2520is%2520that%2520benign%2520optimization%2520trajectories%252C%2520despite%2520per-client%2520heterogeneity%252C%2520concentrate%2520near%2520a%2520low-dimensional%2520manifold%2520that%2520can%2520be%2520estimated%2520from%2520historical%2520aggregates.%2520SpectralKrum%2520projects%2520incoming%2520updates%2520into%2520this%2520learned%2520subspace%252C%2520applies%2520Krum%2520selection%2520in%2520compressed%2520coordinates%252C%2520and%2520filters%2520candidates%2520whose%2520orthogonal%2520residual%2520energy%2520exceeds%2520a%2520data-driven%2520threshold.%2520The%2520method%2520requires%2520no%2520auxiliary%2520data%252C%2520operates%2520entirely%2520on%2520model%2520updates%252C%2520and%2520preserves%2520FL%2520privacy%2520properties.%250A%2520%2520We%2520evaluate%2520SpectralKrum%2520against%2520eight%2520robust%2520baselines%2520across%2520seven%2520attack%2520scenarios%2520on%2520CIFAR-10%2520with%2520Dirichlet-distributed%2520non-IID%2520partitions%2520%2528alpha%2520%253D%25200.1%2529.%2520Experiments%2520spanning%2520over%252056%252C000%2520training%2520rounds%2520show%2520that%2520SpectralKrum%2520is%2520competitive%2520against%2520directional%2520and%2520subspace-aware%2520attacks%2520%2528adaptive-steer%252C%2520buffer-drift%2529%252C%2520but%2520offers%2520limited%2520advantage%2520under%2520label-flip%2520and%2520min-max%2520attacks%2520where%2520malicious%2520updates%2520remain%2520spectrally%2520indistinguishable%2520from%2520benign%2520ones.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectralKrum%3A%20A%20Spectral-Geometric%20Defense%20Against%20Byzantine%20Attacks%20in%20Federated%20Learning&entry.906535625=Aditya%20Tripathi%20and%20Karan%20Sharma%20and%20Rahul%20Mishra%20and%20Tapas%20Kumar%20Maiti&entry.1292438233=Federated%20Learning%20%28FL%29%20distributes%20model%20training%20across%20clients%20who%20retain%20their%20data%20locally%2C%20but%20this%20architecture%20exposes%20a%20fundamental%20vulnerability%3A%20Byzantine%20clients%20can%20inject%20arbitrarily%20corrupted%20updates%20that%20degrade%20or%20subvert%20the%20global%20model.%20While%20robust%20aggregation%20methods%20%28including%20Krum%2C%20Bulyan%2C%20and%20coordinate-wise%20defenses%29%20offer%20theoretical%20guarantees%20under%20idealized%20assumptions%2C%20their%20effectiveness%20erodes%20substantially%20when%20client%20data%20distributions%20are%20heterogeneous%20%28non-IID%29%20and%20adversaries%20can%20observe%20or%20approximate%20the%20defense%20mechanism.%0A%20%20This%20paper%20introduces%20SpectralKrum%2C%20a%20defense%20that%20fuses%20spectral%20subspace%20estimation%20with%20geometric%20neighbor-based%20selection.%20The%20core%20insight%20is%20that%20benign%20optimization%20trajectories%2C%20despite%20per-client%20heterogeneity%2C%20concentrate%20near%20a%20low-dimensional%20manifold%20that%20can%20be%20estimated%20from%20historical%20aggregates.%20SpectralKrum%20projects%20incoming%20updates%20into%20this%20learned%20subspace%2C%20applies%20Krum%20selection%20in%20compressed%20coordinates%2C%20and%20filters%20candidates%20whose%20orthogonal%20residual%20energy%20exceeds%20a%20data-driven%20threshold.%20The%20method%20requires%20no%20auxiliary%20data%2C%20operates%20entirely%20on%20model%20updates%2C%20and%20preserves%20FL%20privacy%20properties.%0A%20%20We%20evaluate%20SpectralKrum%20against%20eight%20robust%20baselines%20across%20seven%20attack%20scenarios%20on%20CIFAR-10%20with%20Dirichlet-distributed%20non-IID%20partitions%20%28alpha%20%3D%200.1%29.%20Experiments%20spanning%20over%2056%2C000%20training%20rounds%20show%20that%20SpectralKrum%20is%20competitive%20against%20directional%20and%20subspace-aware%20attacks%20%28adaptive-steer%2C%20buffer-drift%29%2C%20but%20offers%20limited%20advantage%20under%20label-flip%20and%20min-max%20attacks%20where%20malicious%20updates%20remain%20spectrally%20indistinguishable%20from%20benign%20ones.&entry.1838667208=http%3A//arxiv.org/abs/2512.11760v1&entry.124074799=Read"},
{"title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas", "author": "Han Lin and Xichen Pan and Ziqi Huang and Ji Hou and Jialiang Wang and Weifeng Chen and Zecheng He and Felix Juefei-Xu and Junzhe Sun and Zhipeng Fan and Ali Thabet and Mohit Bansal and Chu Wang", "abstract": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.", "link": "http://arxiv.org/abs/2512.11464v1", "date": "2025-12-12", "relevancy": 2.385, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6007}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5981}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20MLLM-Diffusion%20Information%20Transfer%20with%20MetaCanvas&body=Title%3A%20Exploring%20MLLM-Diffusion%20Information%20Transfer%20with%20MetaCanvas%0AAuthor%3A%20Han%20Lin%20and%20Xichen%20Pan%20and%20Ziqi%20Huang%20and%20Ji%20Hou%20and%20Jialiang%20Wang%20and%20Weifeng%20Chen%20and%20Zecheng%20He%20and%20Felix%20Juefei-Xu%20and%20Junzhe%20Sun%20and%20Zhipeng%20Fan%20and%20Ali%20Thabet%20and%20Mohit%20Bansal%20and%20Chu%20Wang%0AAbstract%3A%20Multimodal%20learning%20has%20rapidly%20advanced%20visual%20understanding%2C%20largely%20via%20multimodal%20large%20language%20models%20%28MLLMs%29%20that%20use%20powerful%20LLMs%20as%20cognitive%20cores.%20In%20visual%20generation%2C%20however%2C%20these%20powerful%20core%20models%20are%20typically%20reduced%20to%20global%20text%20encoders%20for%20diffusion%20models%2C%20leaving%20most%20of%20their%20reasoning%20and%20planning%20ability%20unused.%20This%20creates%20a%20gap%3A%20current%20multimodal%20LLMs%20can%20parse%20complex%20layouts%2C%20attributes%2C%20and%20knowledge-intensive%20scenes%2C%20yet%20struggle%20to%20generate%20images%20or%20videos%20with%20equally%20precise%20and%20structured%20control.%20We%20propose%20MetaCanvas%2C%20a%20lightweight%20framework%20that%20lets%20MLLMs%20reason%20and%20plan%20directly%20in%20spatial%20and%20spatiotemporal%20latent%20spaces%20and%20interface%20tightly%20with%20diffusion%20generators.%20We%20empirically%20implement%20MetaCanvas%20on%20three%20different%20diffusion%20backbones%20and%20evaluate%20it%20across%20six%20tasks%2C%20including%20text-to-image%20generation%2C%20text/image-to-video%20generation%2C%20image/video%20editing%2C%20and%20in-context%20video%20generation%2C%20each%20requiring%20precise%20layouts%2C%20robust%20attribute%20binding%2C%20and%20reasoning-intensive%20control.%20MetaCanvas%20consistently%20outperforms%20global-conditioning%20baselines%2C%20suggesting%20that%20treating%20MLLMs%20as%20latent-space%20planners%20is%20a%20promising%20direction%20for%20narrowing%20the%20gap%20between%20multimodal%20understanding%20and%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520MLLM-Diffusion%2520Information%2520Transfer%2520with%2520MetaCanvas%26entry.906535625%3DHan%2520Lin%2520and%2520Xichen%2520Pan%2520and%2520Ziqi%2520Huang%2520and%2520Ji%2520Hou%2520and%2520Jialiang%2520Wang%2520and%2520Weifeng%2520Chen%2520and%2520Zecheng%2520He%2520and%2520Felix%2520Juefei-Xu%2520and%2520Junzhe%2520Sun%2520and%2520Zhipeng%2520Fan%2520and%2520Ali%2520Thabet%2520and%2520Mohit%2520Bansal%2520and%2520Chu%2520Wang%26entry.1292438233%3DMultimodal%2520learning%2520has%2520rapidly%2520advanced%2520visual%2520understanding%252C%2520largely%2520via%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520that%2520use%2520powerful%2520LLMs%2520as%2520cognitive%2520cores.%2520In%2520visual%2520generation%252C%2520however%252C%2520these%2520powerful%2520core%2520models%2520are%2520typically%2520reduced%2520to%2520global%2520text%2520encoders%2520for%2520diffusion%2520models%252C%2520leaving%2520most%2520of%2520their%2520reasoning%2520and%2520planning%2520ability%2520unused.%2520This%2520creates%2520a%2520gap%253A%2520current%2520multimodal%2520LLMs%2520can%2520parse%2520complex%2520layouts%252C%2520attributes%252C%2520and%2520knowledge-intensive%2520scenes%252C%2520yet%2520struggle%2520to%2520generate%2520images%2520or%2520videos%2520with%2520equally%2520precise%2520and%2520structured%2520control.%2520We%2520propose%2520MetaCanvas%252C%2520a%2520lightweight%2520framework%2520that%2520lets%2520MLLMs%2520reason%2520and%2520plan%2520directly%2520in%2520spatial%2520and%2520spatiotemporal%2520latent%2520spaces%2520and%2520interface%2520tightly%2520with%2520diffusion%2520generators.%2520We%2520empirically%2520implement%2520MetaCanvas%2520on%2520three%2520different%2520diffusion%2520backbones%2520and%2520evaluate%2520it%2520across%2520six%2520tasks%252C%2520including%2520text-to-image%2520generation%252C%2520text/image-to-video%2520generation%252C%2520image/video%2520editing%252C%2520and%2520in-context%2520video%2520generation%252C%2520each%2520requiring%2520precise%2520layouts%252C%2520robust%2520attribute%2520binding%252C%2520and%2520reasoning-intensive%2520control.%2520MetaCanvas%2520consistently%2520outperforms%2520global-conditioning%2520baselines%252C%2520suggesting%2520that%2520treating%2520MLLMs%2520as%2520latent-space%2520planners%2520is%2520a%2520promising%2520direction%2520for%2520narrowing%2520the%2520gap%2520between%2520multimodal%2520understanding%2520and%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20MLLM-Diffusion%20Information%20Transfer%20with%20MetaCanvas&entry.906535625=Han%20Lin%20and%20Xichen%20Pan%20and%20Ziqi%20Huang%20and%20Ji%20Hou%20and%20Jialiang%20Wang%20and%20Weifeng%20Chen%20and%20Zecheng%20He%20and%20Felix%20Juefei-Xu%20and%20Junzhe%20Sun%20and%20Zhipeng%20Fan%20and%20Ali%20Thabet%20and%20Mohit%20Bansal%20and%20Chu%20Wang&entry.1292438233=Multimodal%20learning%20has%20rapidly%20advanced%20visual%20understanding%2C%20largely%20via%20multimodal%20large%20language%20models%20%28MLLMs%29%20that%20use%20powerful%20LLMs%20as%20cognitive%20cores.%20In%20visual%20generation%2C%20however%2C%20these%20powerful%20core%20models%20are%20typically%20reduced%20to%20global%20text%20encoders%20for%20diffusion%20models%2C%20leaving%20most%20of%20their%20reasoning%20and%20planning%20ability%20unused.%20This%20creates%20a%20gap%3A%20current%20multimodal%20LLMs%20can%20parse%20complex%20layouts%2C%20attributes%2C%20and%20knowledge-intensive%20scenes%2C%20yet%20struggle%20to%20generate%20images%20or%20videos%20with%20equally%20precise%20and%20structured%20control.%20We%20propose%20MetaCanvas%2C%20a%20lightweight%20framework%20that%20lets%20MLLMs%20reason%20and%20plan%20directly%20in%20spatial%20and%20spatiotemporal%20latent%20spaces%20and%20interface%20tightly%20with%20diffusion%20generators.%20We%20empirically%20implement%20MetaCanvas%20on%20three%20different%20diffusion%20backbones%20and%20evaluate%20it%20across%20six%20tasks%2C%20including%20text-to-image%20generation%2C%20text/image-to-video%20generation%2C%20image/video%20editing%2C%20and%20in-context%20video%20generation%2C%20each%20requiring%20precise%20layouts%2C%20robust%20attribute%20binding%2C%20and%20reasoning-intensive%20control.%20MetaCanvas%20consistently%20outperforms%20global-conditioning%20baselines%2C%20suggesting%20that%20treating%20MLLMs%20as%20latent-space%20planners%20is%20a%20promising%20direction%20for%20narrowing%20the%20gap%20between%20multimodal%20understanding%20and%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.11464v1&entry.124074799=Read"},
{"title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games", "author": "Vedansh Sharma", "abstract": "Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified \"timescale band\", a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.", "link": "http://arxiv.org/abs/2512.06791v2", "date": "2025-12-12", "relevancy": 2.3795, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5114}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4715}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Small-Gain%20Nash%3A%20Certified%20Contraction%20to%20Nash%20Equilibria%20in%20Differentiable%20Games&body=Title%3A%20Small-Gain%20Nash%3A%20Certified%20Contraction%20to%20Nash%20Equilibria%20in%20Differentiable%20Games%0AAuthor%3A%20Vedansh%20Sharma%0AAbstract%3A%20Classical%20convergence%20guarantees%20for%20gradient-based%20learning%20in%20games%20require%20the%20pseudo-gradient%20to%20be%20%28strongly%29%20monotone%20in%20Euclidean%20geometry%20as%20shown%20by%20rosen%281965%29%2C%20a%20condition%20that%20often%20fails%20even%20in%20simple%20games%20with%20strong%20cross-player%20couplings.%20We%20introduce%20Small-Gain%20Nash%20%28SGN%29%2C%20a%20block%20small-gain%20condition%20in%20a%20custom%20block-weighted%20geometry.%20SGN%20converts%20local%20curvature%20and%20cross-player%20Lipschitz%20coupling%20bounds%20into%20a%20tractable%20certificate%20of%20contraction.%20It%20constructs%20a%20weighted%20block%20metric%20in%20which%20the%20pseudo-gradient%20becomes%20strongly%20monotone%20on%20any%20region%20where%20these%20bounds%20hold%2C%20even%20when%20it%20is%20non-monotone%20in%20the%20Euclidean%20sense.%20The%20continuous%20flow%20is%20exponentially%20contracting%20in%20this%20designed%20geometry%2C%20and%20projected%20Euler%20and%20RK4%20discretizations%20converge%20under%20explicit%20step-size%20bounds%20derived%20from%20the%20SGN%20margin%20and%20a%20local%20Lipschitz%20constant.%20Our%20analysis%20reveals%20a%20certified%20%22timescale%20band%22%2C%20a%20non-asymptotic%2C%20metric-based%20certificate%20that%20plays%20a%20TTUR-like%20role%3A%20rather%20than%20forcing%20asymptotic%20timescale%20separation%20via%20vanishing%2C%20unequal%20step%20sizes%2C%20SGN%20identifies%20a%20finite%20band%20of%20relative%20metric%20weights%20for%20which%20a%20single-step-size%20dynamics%20is%20provably%20contractive.%20We%20validate%20the%20framework%20on%20quadratic%20games%20where%20Euclidean%20monotonicity%20analysis%20fails%20to%20predict%20convergence%2C%20but%20SGN%20successfully%20certifies%20it%2C%20and%20extend%20the%20construction%20to%20mirror/Fisher%20geometries%20for%20entropy-regularized%20policy%20gradient%20in%20Markov%20games.%20The%20result%20is%20an%20offline%20certification%20pipeline%20that%20estimates%20curvature%2C%20coupling%2C%20and%20Lipschitz%20parameters%20on%20compact%20regions%2C%20optimizes%20block%20weights%20to%20enlarge%20the%20SGN%20margin%2C%20and%20returns%20a%20structural%2C%20computable%20convergence%20certificate%20consisting%20of%20a%20metric%2C%20contraction%20rate%2C%20and%20safe%20step-sizes%20for%20non-monotone%20games.%0ALink%3A%20http%3A//arxiv.org/abs/2512.06791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmall-Gain%2520Nash%253A%2520Certified%2520Contraction%2520to%2520Nash%2520Equilibria%2520in%2520Differentiable%2520Games%26entry.906535625%3DVedansh%2520Sharma%26entry.1292438233%3DClassical%2520convergence%2520guarantees%2520for%2520gradient-based%2520learning%2520in%2520games%2520require%2520the%2520pseudo-gradient%2520to%2520be%2520%2528strongly%2529%2520monotone%2520in%2520Euclidean%2520geometry%2520as%2520shown%2520by%2520rosen%25281965%2529%252C%2520a%2520condition%2520that%2520often%2520fails%2520even%2520in%2520simple%2520games%2520with%2520strong%2520cross-player%2520couplings.%2520We%2520introduce%2520Small-Gain%2520Nash%2520%2528SGN%2529%252C%2520a%2520block%2520small-gain%2520condition%2520in%2520a%2520custom%2520block-weighted%2520geometry.%2520SGN%2520converts%2520local%2520curvature%2520and%2520cross-player%2520Lipschitz%2520coupling%2520bounds%2520into%2520a%2520tractable%2520certificate%2520of%2520contraction.%2520It%2520constructs%2520a%2520weighted%2520block%2520metric%2520in%2520which%2520the%2520pseudo-gradient%2520becomes%2520strongly%2520monotone%2520on%2520any%2520region%2520where%2520these%2520bounds%2520hold%252C%2520even%2520when%2520it%2520is%2520non-monotone%2520in%2520the%2520Euclidean%2520sense.%2520The%2520continuous%2520flow%2520is%2520exponentially%2520contracting%2520in%2520this%2520designed%2520geometry%252C%2520and%2520projected%2520Euler%2520and%2520RK4%2520discretizations%2520converge%2520under%2520explicit%2520step-size%2520bounds%2520derived%2520from%2520the%2520SGN%2520margin%2520and%2520a%2520local%2520Lipschitz%2520constant.%2520Our%2520analysis%2520reveals%2520a%2520certified%2520%2522timescale%2520band%2522%252C%2520a%2520non-asymptotic%252C%2520metric-based%2520certificate%2520that%2520plays%2520a%2520TTUR-like%2520role%253A%2520rather%2520than%2520forcing%2520asymptotic%2520timescale%2520separation%2520via%2520vanishing%252C%2520unequal%2520step%2520sizes%252C%2520SGN%2520identifies%2520a%2520finite%2520band%2520of%2520relative%2520metric%2520weights%2520for%2520which%2520a%2520single-step-size%2520dynamics%2520is%2520provably%2520contractive.%2520We%2520validate%2520the%2520framework%2520on%2520quadratic%2520games%2520where%2520Euclidean%2520monotonicity%2520analysis%2520fails%2520to%2520predict%2520convergence%252C%2520but%2520SGN%2520successfully%2520certifies%2520it%252C%2520and%2520extend%2520the%2520construction%2520to%2520mirror/Fisher%2520geometries%2520for%2520entropy-regularized%2520policy%2520gradient%2520in%2520Markov%2520games.%2520The%2520result%2520is%2520an%2520offline%2520certification%2520pipeline%2520that%2520estimates%2520curvature%252C%2520coupling%252C%2520and%2520Lipschitz%2520parameters%2520on%2520compact%2520regions%252C%2520optimizes%2520block%2520weights%2520to%2520enlarge%2520the%2520SGN%2520margin%252C%2520and%2520returns%2520a%2520structural%252C%2520computable%2520convergence%2520certificate%2520consisting%2520of%2520a%2520metric%252C%2520contraction%2520rate%252C%2520and%2520safe%2520step-sizes%2520for%2520non-monotone%2520games.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.06791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small-Gain%20Nash%3A%20Certified%20Contraction%20to%20Nash%20Equilibria%20in%20Differentiable%20Games&entry.906535625=Vedansh%20Sharma&entry.1292438233=Classical%20convergence%20guarantees%20for%20gradient-based%20learning%20in%20games%20require%20the%20pseudo-gradient%20to%20be%20%28strongly%29%20monotone%20in%20Euclidean%20geometry%20as%20shown%20by%20rosen%281965%29%2C%20a%20condition%20that%20often%20fails%20even%20in%20simple%20games%20with%20strong%20cross-player%20couplings.%20We%20introduce%20Small-Gain%20Nash%20%28SGN%29%2C%20a%20block%20small-gain%20condition%20in%20a%20custom%20block-weighted%20geometry.%20SGN%20converts%20local%20curvature%20and%20cross-player%20Lipschitz%20coupling%20bounds%20into%20a%20tractable%20certificate%20of%20contraction.%20It%20constructs%20a%20weighted%20block%20metric%20in%20which%20the%20pseudo-gradient%20becomes%20strongly%20monotone%20on%20any%20region%20where%20these%20bounds%20hold%2C%20even%20when%20it%20is%20non-monotone%20in%20the%20Euclidean%20sense.%20The%20continuous%20flow%20is%20exponentially%20contracting%20in%20this%20designed%20geometry%2C%20and%20projected%20Euler%20and%20RK4%20discretizations%20converge%20under%20explicit%20step-size%20bounds%20derived%20from%20the%20SGN%20margin%20and%20a%20local%20Lipschitz%20constant.%20Our%20analysis%20reveals%20a%20certified%20%22timescale%20band%22%2C%20a%20non-asymptotic%2C%20metric-based%20certificate%20that%20plays%20a%20TTUR-like%20role%3A%20rather%20than%20forcing%20asymptotic%20timescale%20separation%20via%20vanishing%2C%20unequal%20step%20sizes%2C%20SGN%20identifies%20a%20finite%20band%20of%20relative%20metric%20weights%20for%20which%20a%20single-step-size%20dynamics%20is%20provably%20contractive.%20We%20validate%20the%20framework%20on%20quadratic%20games%20where%20Euclidean%20monotonicity%20analysis%20fails%20to%20predict%20convergence%2C%20but%20SGN%20successfully%20certifies%20it%2C%20and%20extend%20the%20construction%20to%20mirror/Fisher%20geometries%20for%20entropy-regularized%20policy%20gradient%20in%20Markov%20games.%20The%20result%20is%20an%20offline%20certification%20pipeline%20that%20estimates%20curvature%2C%20coupling%2C%20and%20Lipschitz%20parameters%20on%20compact%20regions%2C%20optimizes%20block%20weights%20to%20enlarge%20the%20SGN%20margin%2C%20and%20returns%20a%20structural%2C%20computable%20convergence%20certificate%20consisting%20of%20a%20metric%2C%20contraction%20rate%2C%20and%20safe%20step-sizes%20for%20non-monotone%20games.&entry.1838667208=http%3A//arxiv.org/abs/2512.06791v2&entry.124074799=Read"},
{"title": "Free-Lunch Color-Texture Disentanglement for Stylized Image Generation", "author": "Jiang Qin and Senmao Li and Alexandra Gomez-Villa and Shiqi Yang and Yaxing Wang and Kai Wang and Joost van de Weijer", "abstract": "Recent advances in Text-to-Image (T2I) diffusion models have transformed image generation, enabling significant progress in stylized generation using only a few style reference images. However, current diffusion-based methods struggle with fine-grained style customization due to challenges in controlling multiple style attributes, such as color and texture. This paper introduces the first tuning-free approach to achieve free-lunch color-texture disentanglement in stylized T2I generation, addressing the need for independently controlled style elements for the Disentangled Stylized Image Generation (DisIG) problem. Our approach leverages the Image-Prompt Additivity property in the CLIP image embedding space to develop techniques for separating and extracting Color-Texture Embeddings (CTE) from individual color and texture reference images. To ensure that the color palette of the generated image aligns closely with the color reference, we apply a whitening and coloring transformation to enhance color consistency. Additionally, to prevent texture loss due to the signal-leak bias inherent in diffusion training, we introduce a noise term that preserves textural fidelity during the Regularized Whitening and Coloring Transformation (RegWCT). Through these methods, our Style Attributes Disentanglement approach (SADis) delivers a more precise and customizable solution for stylized image generation. Experiments on images from the WikiArt and StyleDrop datasets demonstrate that, both qualitatively and quantitatively, SADis surpasses state-of-the-art stylization methods in the DisIG task.Code is released at https://deepffff.github.io/sadis.github.io/.", "link": "http://arxiv.org/abs/2503.14275v4", "date": "2025-12-12", "relevancy": 2.3771, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6041}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5881}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-Lunch%20Color-Texture%20Disentanglement%20for%20Stylized%20Image%20Generation&body=Title%3A%20Free-Lunch%20Color-Texture%20Disentanglement%20for%20Stylized%20Image%20Generation%0AAuthor%3A%20Jiang%20Qin%20and%20Senmao%20Li%20and%20Alexandra%20Gomez-Villa%20and%20Shiqi%20Yang%20and%20Yaxing%20Wang%20and%20Kai%20Wang%20and%20Joost%20van%20de%20Weijer%0AAbstract%3A%20Recent%20advances%20in%20Text-to-Image%20%28T2I%29%20diffusion%20models%20have%20transformed%20image%20generation%2C%20enabling%20significant%20progress%20in%20stylized%20generation%20using%20only%20a%20few%20style%20reference%20images.%20However%2C%20current%20diffusion-based%20methods%20struggle%20with%20fine-grained%20style%20customization%20due%20to%20challenges%20in%20controlling%20multiple%20style%20attributes%2C%20such%20as%20color%20and%20texture.%20This%20paper%20introduces%20the%20first%20tuning-free%20approach%20to%20achieve%20free-lunch%20color-texture%20disentanglement%20in%20stylized%20T2I%20generation%2C%20addressing%20the%20need%20for%20independently%20controlled%20style%20elements%20for%20the%20Disentangled%20Stylized%20Image%20Generation%20%28DisIG%29%20problem.%20Our%20approach%20leverages%20the%20Image-Prompt%20Additivity%20property%20in%20the%20CLIP%20image%20embedding%20space%20to%20develop%20techniques%20for%20separating%20and%20extracting%20Color-Texture%20Embeddings%20%28CTE%29%20from%20individual%20color%20and%20texture%20reference%20images.%20To%20ensure%20that%20the%20color%20palette%20of%20the%20generated%20image%20aligns%20closely%20with%20the%20color%20reference%2C%20we%20apply%20a%20whitening%20and%20coloring%20transformation%20to%20enhance%20color%20consistency.%20Additionally%2C%20to%20prevent%20texture%20loss%20due%20to%20the%20signal-leak%20bias%20inherent%20in%20diffusion%20training%2C%20we%20introduce%20a%20noise%20term%20that%20preserves%20textural%20fidelity%20during%20the%20Regularized%20Whitening%20and%20Coloring%20Transformation%20%28RegWCT%29.%20Through%20these%20methods%2C%20our%20Style%20Attributes%20Disentanglement%20approach%20%28SADis%29%20delivers%20a%20more%20precise%20and%20customizable%20solution%20for%20stylized%20image%20generation.%20Experiments%20on%20images%20from%20the%20WikiArt%20and%20StyleDrop%20datasets%20demonstrate%20that%2C%20both%20qualitatively%20and%20quantitatively%2C%20SADis%20surpasses%20state-of-the-art%20stylization%20methods%20in%20the%20DisIG%20task.Code%20is%20released%20at%20https%3A//deepffff.github.io/sadis.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2503.14275v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-Lunch%2520Color-Texture%2520Disentanglement%2520for%2520Stylized%2520Image%2520Generation%26entry.906535625%3DJiang%2520Qin%2520and%2520Senmao%2520Li%2520and%2520Alexandra%2520Gomez-Villa%2520and%2520Shiqi%2520Yang%2520and%2520Yaxing%2520Wang%2520and%2520Kai%2520Wang%2520and%2520Joost%2520van%2520de%2520Weijer%26entry.1292438233%3DRecent%2520advances%2520in%2520Text-to-Image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520transformed%2520image%2520generation%252C%2520enabling%2520significant%2520progress%2520in%2520stylized%2520generation%2520using%2520only%2520a%2520few%2520style%2520reference%2520images.%2520However%252C%2520current%2520diffusion-based%2520methods%2520struggle%2520with%2520fine-grained%2520style%2520customization%2520due%2520to%2520challenges%2520in%2520controlling%2520multiple%2520style%2520attributes%252C%2520such%2520as%2520color%2520and%2520texture.%2520This%2520paper%2520introduces%2520the%2520first%2520tuning-free%2520approach%2520to%2520achieve%2520free-lunch%2520color-texture%2520disentanglement%2520in%2520stylized%2520T2I%2520generation%252C%2520addressing%2520the%2520need%2520for%2520independently%2520controlled%2520style%2520elements%2520for%2520the%2520Disentangled%2520Stylized%2520Image%2520Generation%2520%2528DisIG%2529%2520problem.%2520Our%2520approach%2520leverages%2520the%2520Image-Prompt%2520Additivity%2520property%2520in%2520the%2520CLIP%2520image%2520embedding%2520space%2520to%2520develop%2520techniques%2520for%2520separating%2520and%2520extracting%2520Color-Texture%2520Embeddings%2520%2528CTE%2529%2520from%2520individual%2520color%2520and%2520texture%2520reference%2520images.%2520To%2520ensure%2520that%2520the%2520color%2520palette%2520of%2520the%2520generated%2520image%2520aligns%2520closely%2520with%2520the%2520color%2520reference%252C%2520we%2520apply%2520a%2520whitening%2520and%2520coloring%2520transformation%2520to%2520enhance%2520color%2520consistency.%2520Additionally%252C%2520to%2520prevent%2520texture%2520loss%2520due%2520to%2520the%2520signal-leak%2520bias%2520inherent%2520in%2520diffusion%2520training%252C%2520we%2520introduce%2520a%2520noise%2520term%2520that%2520preserves%2520textural%2520fidelity%2520during%2520the%2520Regularized%2520Whitening%2520and%2520Coloring%2520Transformation%2520%2528RegWCT%2529.%2520Through%2520these%2520methods%252C%2520our%2520Style%2520Attributes%2520Disentanglement%2520approach%2520%2528SADis%2529%2520delivers%2520a%2520more%2520precise%2520and%2520customizable%2520solution%2520for%2520stylized%2520image%2520generation.%2520Experiments%2520on%2520images%2520from%2520the%2520WikiArt%2520and%2520StyleDrop%2520datasets%2520demonstrate%2520that%252C%2520both%2520qualitatively%2520and%2520quantitatively%252C%2520SADis%2520surpasses%2520state-of-the-art%2520stylization%2520methods%2520in%2520the%2520DisIG%2520task.Code%2520is%2520released%2520at%2520https%253A//deepffff.github.io/sadis.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14275v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-Lunch%20Color-Texture%20Disentanglement%20for%20Stylized%20Image%20Generation&entry.906535625=Jiang%20Qin%20and%20Senmao%20Li%20and%20Alexandra%20Gomez-Villa%20and%20Shiqi%20Yang%20and%20Yaxing%20Wang%20and%20Kai%20Wang%20and%20Joost%20van%20de%20Weijer&entry.1292438233=Recent%20advances%20in%20Text-to-Image%20%28T2I%29%20diffusion%20models%20have%20transformed%20image%20generation%2C%20enabling%20significant%20progress%20in%20stylized%20generation%20using%20only%20a%20few%20style%20reference%20images.%20However%2C%20current%20diffusion-based%20methods%20struggle%20with%20fine-grained%20style%20customization%20due%20to%20challenges%20in%20controlling%20multiple%20style%20attributes%2C%20such%20as%20color%20and%20texture.%20This%20paper%20introduces%20the%20first%20tuning-free%20approach%20to%20achieve%20free-lunch%20color-texture%20disentanglement%20in%20stylized%20T2I%20generation%2C%20addressing%20the%20need%20for%20independently%20controlled%20style%20elements%20for%20the%20Disentangled%20Stylized%20Image%20Generation%20%28DisIG%29%20problem.%20Our%20approach%20leverages%20the%20Image-Prompt%20Additivity%20property%20in%20the%20CLIP%20image%20embedding%20space%20to%20develop%20techniques%20for%20separating%20and%20extracting%20Color-Texture%20Embeddings%20%28CTE%29%20from%20individual%20color%20and%20texture%20reference%20images.%20To%20ensure%20that%20the%20color%20palette%20of%20the%20generated%20image%20aligns%20closely%20with%20the%20color%20reference%2C%20we%20apply%20a%20whitening%20and%20coloring%20transformation%20to%20enhance%20color%20consistency.%20Additionally%2C%20to%20prevent%20texture%20loss%20due%20to%20the%20signal-leak%20bias%20inherent%20in%20diffusion%20training%2C%20we%20introduce%20a%20noise%20term%20that%20preserves%20textural%20fidelity%20during%20the%20Regularized%20Whitening%20and%20Coloring%20Transformation%20%28RegWCT%29.%20Through%20these%20methods%2C%20our%20Style%20Attributes%20Disentanglement%20approach%20%28SADis%29%20delivers%20a%20more%20precise%20and%20customizable%20solution%20for%20stylized%20image%20generation.%20Experiments%20on%20images%20from%20the%20WikiArt%20and%20StyleDrop%20datasets%20demonstrate%20that%2C%20both%20qualitatively%20and%20quantitatively%2C%20SADis%20surpasses%20state-of-the-art%20stylization%20methods%20in%20the%20DisIG%20task.Code%20is%20released%20at%20https%3A//deepffff.github.io/sadis.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2503.14275v4&entry.124074799=Read"},
{"title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties", "author": "Ye Fang and Tong Wu and Valentin Deschaintre and Duygu Ceylan and Iliyan Georgiev and Chun-Hao Paul Huang and Yiwei Hu and Xuelin Chen and Tuanfeng Yang Wang", "abstract": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.", "link": "http://arxiv.org/abs/2512.11799v1", "date": "2025-12-12", "relevancy": 2.3722, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6112}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5831}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-RGBX%3A%20Video%20Editing%20with%20Accurate%20Controls%20over%20Intrinsic%20Properties&body=Title%3A%20V-RGBX%3A%20Video%20Editing%20with%20Accurate%20Controls%20over%20Intrinsic%20Properties%0AAuthor%3A%20Ye%20Fang%20and%20Tong%20Wu%20and%20Valentin%20Deschaintre%20and%20Duygu%20Ceylan%20and%20Iliyan%20Georgiev%20and%20Chun-Hao%20Paul%20Huang%20and%20Yiwei%20Hu%20and%20Xuelin%20Chen%20and%20Tuanfeng%20Yang%20Wang%0AAbstract%3A%20Large-scale%20video%20generation%20models%20have%20shown%20remarkable%20potential%20in%20modeling%20photorealistic%20appearance%20and%20lighting%20interactions%20in%20real-world%20scenes.%20However%2C%20a%20closed-loop%20framework%20that%20jointly%20understands%20intrinsic%20scene%20properties%20%28e.g.%2C%20albedo%2C%20normal%2C%20material%2C%20and%20irradiance%29%2C%20leverages%20them%20for%20video%20synthesis%2C%20and%20supports%20editable%20intrinsic%20representations%20remains%20unexplored.%20We%20present%20V-RGBX%2C%20the%20first%20end-to-end%20framework%20for%20intrinsic-aware%20video%20editing.%20V-RGBX%20unifies%20three%20key%20capabilities%3A%20%281%29%20video%20inverse%20rendering%20into%20intrinsic%20channels%2C%20%282%29%20photorealistic%20video%20synthesis%20from%20these%20intrinsic%20representations%2C%20and%20%283%29%20keyframe-based%20video%20editing%20conditioned%20on%20intrinsic%20channels.%20At%20the%20core%20of%20V-RGBX%20is%20an%20interleaved%20conditioning%20mechanism%20that%20enables%20intuitive%2C%20physically%20grounded%20video%20editing%20through%20user-selected%20keyframes%2C%20supporting%20flexible%20manipulation%20of%20any%20intrinsic%20modality.%20Extensive%20qualitative%20and%20quantitative%20results%20show%20that%20V-RGBX%20produces%20temporally%20consistent%2C%20photorealistic%20videos%20while%20propagating%20keyframe%20edits%20across%20sequences%20in%20a%20physically%20plausible%20manner.%20We%20demonstrate%20its%20effectiveness%20in%20diverse%20applications%2C%20including%20object%20appearance%20editing%20and%20scene-level%20relighting%2C%20surpassing%20the%20performance%20of%20prior%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-RGBX%253A%2520Video%2520Editing%2520with%2520Accurate%2520Controls%2520over%2520Intrinsic%2520Properties%26entry.906535625%3DYe%2520Fang%2520and%2520Tong%2520Wu%2520and%2520Valentin%2520Deschaintre%2520and%2520Duygu%2520Ceylan%2520and%2520Iliyan%2520Georgiev%2520and%2520Chun-Hao%2520Paul%2520Huang%2520and%2520Yiwei%2520Hu%2520and%2520Xuelin%2520Chen%2520and%2520Tuanfeng%2520Yang%2520Wang%26entry.1292438233%3DLarge-scale%2520video%2520generation%2520models%2520have%2520shown%2520remarkable%2520potential%2520in%2520modeling%2520photorealistic%2520appearance%2520and%2520lighting%2520interactions%2520in%2520real-world%2520scenes.%2520However%252C%2520a%2520closed-loop%2520framework%2520that%2520jointly%2520understands%2520intrinsic%2520scene%2520properties%2520%2528e.g.%252C%2520albedo%252C%2520normal%252C%2520material%252C%2520and%2520irradiance%2529%252C%2520leverages%2520them%2520for%2520video%2520synthesis%252C%2520and%2520supports%2520editable%2520intrinsic%2520representations%2520remains%2520unexplored.%2520We%2520present%2520V-RGBX%252C%2520the%2520first%2520end-to-end%2520framework%2520for%2520intrinsic-aware%2520video%2520editing.%2520V-RGBX%2520unifies%2520three%2520key%2520capabilities%253A%2520%25281%2529%2520video%2520inverse%2520rendering%2520into%2520intrinsic%2520channels%252C%2520%25282%2529%2520photorealistic%2520video%2520synthesis%2520from%2520these%2520intrinsic%2520representations%252C%2520and%2520%25283%2529%2520keyframe-based%2520video%2520editing%2520conditioned%2520on%2520intrinsic%2520channels.%2520At%2520the%2520core%2520of%2520V-RGBX%2520is%2520an%2520interleaved%2520conditioning%2520mechanism%2520that%2520enables%2520intuitive%252C%2520physically%2520grounded%2520video%2520editing%2520through%2520user-selected%2520keyframes%252C%2520supporting%2520flexible%2520manipulation%2520of%2520any%2520intrinsic%2520modality.%2520Extensive%2520qualitative%2520and%2520quantitative%2520results%2520show%2520that%2520V-RGBX%2520produces%2520temporally%2520consistent%252C%2520photorealistic%2520videos%2520while%2520propagating%2520keyframe%2520edits%2520across%2520sequences%2520in%2520a%2520physically%2520plausible%2520manner.%2520We%2520demonstrate%2520its%2520effectiveness%2520in%2520diverse%2520applications%252C%2520including%2520object%2520appearance%2520editing%2520and%2520scene-level%2520relighting%252C%2520surpassing%2520the%2520performance%2520of%2520prior%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-RGBX%3A%20Video%20Editing%20with%20Accurate%20Controls%20over%20Intrinsic%20Properties&entry.906535625=Ye%20Fang%20and%20Tong%20Wu%20and%20Valentin%20Deschaintre%20and%20Duygu%20Ceylan%20and%20Iliyan%20Georgiev%20and%20Chun-Hao%20Paul%20Huang%20and%20Yiwei%20Hu%20and%20Xuelin%20Chen%20and%20Tuanfeng%20Yang%20Wang&entry.1292438233=Large-scale%20video%20generation%20models%20have%20shown%20remarkable%20potential%20in%20modeling%20photorealistic%20appearance%20and%20lighting%20interactions%20in%20real-world%20scenes.%20However%2C%20a%20closed-loop%20framework%20that%20jointly%20understands%20intrinsic%20scene%20properties%20%28e.g.%2C%20albedo%2C%20normal%2C%20material%2C%20and%20irradiance%29%2C%20leverages%20them%20for%20video%20synthesis%2C%20and%20supports%20editable%20intrinsic%20representations%20remains%20unexplored.%20We%20present%20V-RGBX%2C%20the%20first%20end-to-end%20framework%20for%20intrinsic-aware%20video%20editing.%20V-RGBX%20unifies%20three%20key%20capabilities%3A%20%281%29%20video%20inverse%20rendering%20into%20intrinsic%20channels%2C%20%282%29%20photorealistic%20video%20synthesis%20from%20these%20intrinsic%20representations%2C%20and%20%283%29%20keyframe-based%20video%20editing%20conditioned%20on%20intrinsic%20channels.%20At%20the%20core%20of%20V-RGBX%20is%20an%20interleaved%20conditioning%20mechanism%20that%20enables%20intuitive%2C%20physically%20grounded%20video%20editing%20through%20user-selected%20keyframes%2C%20supporting%20flexible%20manipulation%20of%20any%20intrinsic%20modality.%20Extensive%20qualitative%20and%20quantitative%20results%20show%20that%20V-RGBX%20produces%20temporally%20consistent%2C%20photorealistic%20videos%20while%20propagating%20keyframe%20edits%20across%20sequences%20in%20a%20physically%20plausible%20manner.%20We%20demonstrate%20its%20effectiveness%20in%20diverse%20applications%2C%20including%20object%20appearance%20editing%20and%20scene-level%20relighting%2C%20surpassing%20the%20performance%20of%20prior%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.11799v1&entry.124074799=Read"},
{"title": "JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion", "author": "Chaochao Li and Ruikui Wang and Liangbo Zhou and Jinheng Feng and Huaishao Luo and Huan Zhang and Youzheng Wu and Xiaodong He", "abstract": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.", "link": "http://arxiv.org/abs/2512.11423v1", "date": "2025-12-12", "relevancy": 2.3716, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6005}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5899}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JoyAvatar%3A%20Real-time%20and%20Infinite%20Audio-Driven%20Avatar%20Generation%20with%20Autoregressive%20Diffusion&body=Title%3A%20JoyAvatar%3A%20Real-time%20and%20Infinite%20Audio-Driven%20Avatar%20Generation%20with%20Autoregressive%20Diffusion%0AAuthor%3A%20Chaochao%20Li%20and%20Ruikui%20Wang%20and%20Liangbo%20Zhou%20and%20Jinheng%20Feng%20and%20Huaishao%20Luo%20and%20Huan%20Zhang%20and%20Youzheng%20Wu%20and%20Xiaodong%20He%0AAbstract%3A%20Existing%20DiT-based%20audio-driven%20avatar%20generation%20methods%20have%20achieved%20considerable%20progress%2C%20yet%20their%20broader%20application%20is%20constrained%20by%20limitations%20such%20as%20high%20computational%20overhead%20and%20the%20inability%20to%20synthesize%20long-duration%20videos.%20Autoregressive%20methods%20address%20this%20problem%20by%20applying%20block-wise%20autoregressive%20diffusion%20methods.%20However%2C%20these%20methods%20suffer%20from%20the%20problem%20of%20error%20accumulation%20and%20quality%20degradation.%20To%20address%20this%2C%20we%20propose%20JoyAvatar%2C%20an%20audio-driven%20autoregressive%20model%20capable%20of%20real-time%20inference%20and%20infinite-length%20video%20generation%20with%20the%20following%20contributions%3A%20%281%29%20Progressive%20Step%20Bootstrapping%20%28PSB%29%2C%20which%20allocates%20more%20denoising%20steps%20to%20initial%20frames%20to%20stabilize%20generation%20and%20reduce%20error%20accumulation%3B%20%282%29%20Motion%20Condition%20Injection%20%28MCI%29%2C%20enhancing%20temporal%20coherence%20by%20injecting%20noise-corrupted%20previous%20frames%20as%20motion%20condition%3B%20and%20%283%29%20Unbounded%20RoPE%20via%20Cache-Resetting%20%28URCR%29%2C%20enabling%20infinite-length%20generation%20through%20dynamic%20positional%20encoding.%20Our%201.3B-parameter%20causal%20model%20achieves%2016%20FPS%20on%20a%20single%20GPU%20and%20achieves%20competitive%20results%20in%20visual%20quality%2C%20temporal%20consistency%2C%20and%20lip%20synchronization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoyAvatar%253A%2520Real-time%2520and%2520Infinite%2520Audio-Driven%2520Avatar%2520Generation%2520with%2520Autoregressive%2520Diffusion%26entry.906535625%3DChaochao%2520Li%2520and%2520Ruikui%2520Wang%2520and%2520Liangbo%2520Zhou%2520and%2520Jinheng%2520Feng%2520and%2520Huaishao%2520Luo%2520and%2520Huan%2520Zhang%2520and%2520Youzheng%2520Wu%2520and%2520Xiaodong%2520He%26entry.1292438233%3DExisting%2520DiT-based%2520audio-driven%2520avatar%2520generation%2520methods%2520have%2520achieved%2520considerable%2520progress%252C%2520yet%2520their%2520broader%2520application%2520is%2520constrained%2520by%2520limitations%2520such%2520as%2520high%2520computational%2520overhead%2520and%2520the%2520inability%2520to%2520synthesize%2520long-duration%2520videos.%2520Autoregressive%2520methods%2520address%2520this%2520problem%2520by%2520applying%2520block-wise%2520autoregressive%2520diffusion%2520methods.%2520However%252C%2520these%2520methods%2520suffer%2520from%2520the%2520problem%2520of%2520error%2520accumulation%2520and%2520quality%2520degradation.%2520To%2520address%2520this%252C%2520we%2520propose%2520JoyAvatar%252C%2520an%2520audio-driven%2520autoregressive%2520model%2520capable%2520of%2520real-time%2520inference%2520and%2520infinite-length%2520video%2520generation%2520with%2520the%2520following%2520contributions%253A%2520%25281%2529%2520Progressive%2520Step%2520Bootstrapping%2520%2528PSB%2529%252C%2520which%2520allocates%2520more%2520denoising%2520steps%2520to%2520initial%2520frames%2520to%2520stabilize%2520generation%2520and%2520reduce%2520error%2520accumulation%253B%2520%25282%2529%2520Motion%2520Condition%2520Injection%2520%2528MCI%2529%252C%2520enhancing%2520temporal%2520coherence%2520by%2520injecting%2520noise-corrupted%2520previous%2520frames%2520as%2520motion%2520condition%253B%2520and%2520%25283%2529%2520Unbounded%2520RoPE%2520via%2520Cache-Resetting%2520%2528URCR%2529%252C%2520enabling%2520infinite-length%2520generation%2520through%2520dynamic%2520positional%2520encoding.%2520Our%25201.3B-parameter%2520causal%2520model%2520achieves%252016%2520FPS%2520on%2520a%2520single%2520GPU%2520and%2520achieves%2520competitive%2520results%2520in%2520visual%2520quality%252C%2520temporal%2520consistency%252C%2520and%2520lip%2520synchronization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JoyAvatar%3A%20Real-time%20and%20Infinite%20Audio-Driven%20Avatar%20Generation%20with%20Autoregressive%20Diffusion&entry.906535625=Chaochao%20Li%20and%20Ruikui%20Wang%20and%20Liangbo%20Zhou%20and%20Jinheng%20Feng%20and%20Huaishao%20Luo%20and%20Huan%20Zhang%20and%20Youzheng%20Wu%20and%20Xiaodong%20He&entry.1292438233=Existing%20DiT-based%20audio-driven%20avatar%20generation%20methods%20have%20achieved%20considerable%20progress%2C%20yet%20their%20broader%20application%20is%20constrained%20by%20limitations%20such%20as%20high%20computational%20overhead%20and%20the%20inability%20to%20synthesize%20long-duration%20videos.%20Autoregressive%20methods%20address%20this%20problem%20by%20applying%20block-wise%20autoregressive%20diffusion%20methods.%20However%2C%20these%20methods%20suffer%20from%20the%20problem%20of%20error%20accumulation%20and%20quality%20degradation.%20To%20address%20this%2C%20we%20propose%20JoyAvatar%2C%20an%20audio-driven%20autoregressive%20model%20capable%20of%20real-time%20inference%20and%20infinite-length%20video%20generation%20with%20the%20following%20contributions%3A%20%281%29%20Progressive%20Step%20Bootstrapping%20%28PSB%29%2C%20which%20allocates%20more%20denoising%20steps%20to%20initial%20frames%20to%20stabilize%20generation%20and%20reduce%20error%20accumulation%3B%20%282%29%20Motion%20Condition%20Injection%20%28MCI%29%2C%20enhancing%20temporal%20coherence%20by%20injecting%20noise-corrupted%20previous%20frames%20as%20motion%20condition%3B%20and%20%283%29%20Unbounded%20RoPE%20via%20Cache-Resetting%20%28URCR%29%2C%20enabling%20infinite-length%20generation%20through%20dynamic%20positional%20encoding.%20Our%201.3B-parameter%20causal%20model%20achieves%2016%20FPS%20on%20a%20single%20GPU%20and%20achieves%20competitive%20results%20in%20visual%20quality%2C%20temporal%20consistency%2C%20and%20lip%20synchronization.&entry.1838667208=http%3A//arxiv.org/abs/2512.11423v1&entry.124074799=Read"},
{"title": "Text2Graph: Combining Lightweight LLMs and GNNs for Efficient Text Classification in Label-Scarce Scenarios", "author": "Jo\u00e3o Lucas Luz Lima Sarcinelli and Ricardo Marcondes Marcacini", "abstract": "Large Language Models (LLMs) have become effective zero-shot classifiers, but their high computational requirements and environmental costs limit their practicality for large-scale annotation in high-performance computing (HPC) environments. To support more sustainable workflows, we present Text2Graph, an open-source Python package that provides a modular implementation of existing text-to-graph classification approaches. The framework enables users to combine LLM-based partial annotation with Graph Neural Network (GNN) label propagation in a flexible manner, making it straightforward to swap components such as feature extractors, edge construction methods, and sampling strategies. We benchmark Text2Graph on a zero-shot setting using five datasets spanning topic classification and sentiment analysis tasks, comparing multiple variants against other zero-shot approaches for text classification. In addition to reporting performance, we provide detailed estimates of energy consumption and carbon emissions, showing that graph-based propagation achieves competitive results at a fraction of the energy and environmental cost.", "link": "http://arxiv.org/abs/2512.10061v2", "date": "2025-12-12", "relevancy": 2.3618, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4853}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4671}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Graph%3A%20Combining%20Lightweight%20LLMs%20and%20GNNs%20for%20Efficient%20Text%20Classification%20in%20Label-Scarce%20Scenarios&body=Title%3A%20Text2Graph%3A%20Combining%20Lightweight%20LLMs%20and%20GNNs%20for%20Efficient%20Text%20Classification%20in%20Label-Scarce%20Scenarios%0AAuthor%3A%20Jo%C3%A3o%20Lucas%20Luz%20Lima%20Sarcinelli%20and%20Ricardo%20Marcondes%20Marcacini%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20effective%20zero-shot%20classifiers%2C%20but%20their%20high%20computational%20requirements%20and%20environmental%20costs%20limit%20their%20practicality%20for%20large-scale%20annotation%20in%20high-performance%20computing%20%28HPC%29%20environments.%20To%20support%20more%20sustainable%20workflows%2C%20we%20present%20Text2Graph%2C%20an%20open-source%20Python%20package%20that%20provides%20a%20modular%20implementation%20of%20existing%20text-to-graph%20classification%20approaches.%20The%20framework%20enables%20users%20to%20combine%20LLM-based%20partial%20annotation%20with%20Graph%20Neural%20Network%20%28GNN%29%20label%20propagation%20in%20a%20flexible%20manner%2C%20making%20it%20straightforward%20to%20swap%20components%20such%20as%20feature%20extractors%2C%20edge%20construction%20methods%2C%20and%20sampling%20strategies.%20We%20benchmark%20Text2Graph%20on%20a%20zero-shot%20setting%20using%20five%20datasets%20spanning%20topic%20classification%20and%20sentiment%20analysis%20tasks%2C%20comparing%20multiple%20variants%20against%20other%20zero-shot%20approaches%20for%20text%20classification.%20In%20addition%20to%20reporting%20performance%2C%20we%20provide%20detailed%20estimates%20of%20energy%20consumption%20and%20carbon%20emissions%2C%20showing%20that%20graph-based%20propagation%20achieves%20competitive%20results%20at%20a%20fraction%20of%20the%20energy%20and%20environmental%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Graph%253A%2520Combining%2520Lightweight%2520LLMs%2520and%2520GNNs%2520for%2520Efficient%2520Text%2520Classification%2520in%2520Label-Scarce%2520Scenarios%26entry.906535625%3DJo%25C3%25A3o%2520Lucas%2520Luz%2520Lima%2520Sarcinelli%2520and%2520Ricardo%2520Marcondes%2520Marcacini%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520effective%2520zero-shot%2520classifiers%252C%2520but%2520their%2520high%2520computational%2520requirements%2520and%2520environmental%2520costs%2520limit%2520their%2520practicality%2520for%2520large-scale%2520annotation%2520in%2520high-performance%2520computing%2520%2528HPC%2529%2520environments.%2520To%2520support%2520more%2520sustainable%2520workflows%252C%2520we%2520present%2520Text2Graph%252C%2520an%2520open-source%2520Python%2520package%2520that%2520provides%2520a%2520modular%2520implementation%2520of%2520existing%2520text-to-graph%2520classification%2520approaches.%2520The%2520framework%2520enables%2520users%2520to%2520combine%2520LLM-based%2520partial%2520annotation%2520with%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520label%2520propagation%2520in%2520a%2520flexible%2520manner%252C%2520making%2520it%2520straightforward%2520to%2520swap%2520components%2520such%2520as%2520feature%2520extractors%252C%2520edge%2520construction%2520methods%252C%2520and%2520sampling%2520strategies.%2520We%2520benchmark%2520Text2Graph%2520on%2520a%2520zero-shot%2520setting%2520using%2520five%2520datasets%2520spanning%2520topic%2520classification%2520and%2520sentiment%2520analysis%2520tasks%252C%2520comparing%2520multiple%2520variants%2520against%2520other%2520zero-shot%2520approaches%2520for%2520text%2520classification.%2520In%2520addition%2520to%2520reporting%2520performance%252C%2520we%2520provide%2520detailed%2520estimates%2520of%2520energy%2520consumption%2520and%2520carbon%2520emissions%252C%2520showing%2520that%2520graph-based%2520propagation%2520achieves%2520competitive%2520results%2520at%2520a%2520fraction%2520of%2520the%2520energy%2520and%2520environmental%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Graph%3A%20Combining%20Lightweight%20LLMs%20and%20GNNs%20for%20Efficient%20Text%20Classification%20in%20Label-Scarce%20Scenarios&entry.906535625=Jo%C3%A3o%20Lucas%20Luz%20Lima%20Sarcinelli%20and%20Ricardo%20Marcondes%20Marcacini&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20become%20effective%20zero-shot%20classifiers%2C%20but%20their%20high%20computational%20requirements%20and%20environmental%20costs%20limit%20their%20practicality%20for%20large-scale%20annotation%20in%20high-performance%20computing%20%28HPC%29%20environments.%20To%20support%20more%20sustainable%20workflows%2C%20we%20present%20Text2Graph%2C%20an%20open-source%20Python%20package%20that%20provides%20a%20modular%20implementation%20of%20existing%20text-to-graph%20classification%20approaches.%20The%20framework%20enables%20users%20to%20combine%20LLM-based%20partial%20annotation%20with%20Graph%20Neural%20Network%20%28GNN%29%20label%20propagation%20in%20a%20flexible%20manner%2C%20making%20it%20straightforward%20to%20swap%20components%20such%20as%20feature%20extractors%2C%20edge%20construction%20methods%2C%20and%20sampling%20strategies.%20We%20benchmark%20Text2Graph%20on%20a%20zero-shot%20setting%20using%20five%20datasets%20spanning%20topic%20classification%20and%20sentiment%20analysis%20tasks%2C%20comparing%20multiple%20variants%20against%20other%20zero-shot%20approaches%20for%20text%20classification.%20In%20addition%20to%20reporting%20performance%2C%20we%20provide%20detailed%20estimates%20of%20energy%20consumption%20and%20carbon%20emissions%2C%20showing%20that%20graph-based%20propagation%20achieves%20competitive%20results%20at%20a%20fraction%20of%20the%20energy%20and%20environmental%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2512.10061v2&entry.124074799=Read"},
{"title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models", "author": "Zongzhao Li and Xiangzhe Kong and Jiahui Su and Zongyang Ma and Mingze Li and Songyou Li and Yuelin Zhang and Yu Rong and Tingyang Xu and Deli Zhao and Wenbing Huang", "abstract": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.", "link": "http://arxiv.org/abs/2512.10867v2", "date": "2025-12-12", "relevancy": 2.2523, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5746}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5746}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Macro%20to%20Micro%3A%20Benchmarking%20Microscopic%20Spatial%20Intelligence%20on%20Molecules%20via%20Vision-Language%20Models&body=Title%3A%20From%20Macro%20to%20Micro%3A%20Benchmarking%20Microscopic%20Spatial%20Intelligence%20on%20Molecules%20via%20Vision-Language%20Models%0AAuthor%3A%20Zongzhao%20Li%20and%20Xiangzhe%20Kong%20and%20Jiahui%20Su%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yuelin%20Zhang%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Deli%20Zhao%20and%20Wenbing%20Huang%0AAbstract%3A%20This%20paper%20introduces%20the%20concept%20of%20Microscopic%20Spatial%20Intelligence%20%28MiSI%29%2C%20the%20capability%20to%20perceive%20and%20reason%20about%20the%20spatial%20relationships%20of%20invisible%20microscopic%20entities%2C%20which%20is%20fundamental%20to%20scientific%20discovery.%20To%20assess%20the%20potential%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20this%20domain%2C%20we%20propose%20a%20systematic%20benchmark%20framework%20MiSI-Bench.%20This%20framework%20features%20over%20163%2C000%20question-answer%20pairs%20and%20587%2C000%20images%20derived%20from%20approximately%204%2C000%20molecular%20structures%2C%20covering%20nine%20complementary%20tasks%20that%20evaluate%20abilities%20ranging%20from%20elementary%20spatial%20transformations%20to%20complex%20relational%20identifications.%20Experimental%20results%20reveal%20that%20current%20state-of-the-art%20VLMs%20perform%20significantly%20below%20human%20level%20on%20this%20benchmark.%20However%2C%20a%20fine-tuned%207B%20model%20demonstrates%20substantial%20potential%2C%20even%20surpassing%20humans%20in%20spatial%20transformation%20tasks%2C%20while%20its%20poor%20performance%20in%20scientifically-grounded%20tasks%20like%20hydrogen%20bond%20recognition%20underscores%20the%20necessity%20of%20integrating%20explicit%20domain%20knowledge%20for%20progress%20toward%20scientific%20AGI.%20The%20datasets%20are%20available%20at%20https%3A//huggingface.co/datasets/zongzhao/MiSI-bench.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10867v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Macro%2520to%2520Micro%253A%2520Benchmarking%2520Microscopic%2520Spatial%2520Intelligence%2520on%2520Molecules%2520via%2520Vision-Language%2520Models%26entry.906535625%3DZongzhao%2520Li%2520and%2520Xiangzhe%2520Kong%2520and%2520Jiahui%2520Su%2520and%2520Zongyang%2520Ma%2520and%2520Mingze%2520Li%2520and%2520Songyou%2520Li%2520and%2520Yuelin%2520Zhang%2520and%2520Yu%2520Rong%2520and%2520Tingyang%2520Xu%2520and%2520Deli%2520Zhao%2520and%2520Wenbing%2520Huang%26entry.1292438233%3DThis%2520paper%2520introduces%2520the%2520concept%2520of%2520Microscopic%2520Spatial%2520Intelligence%2520%2528MiSI%2529%252C%2520the%2520capability%2520to%2520perceive%2520and%2520reason%2520about%2520the%2520spatial%2520relationships%2520of%2520invisible%2520microscopic%2520entities%252C%2520which%2520is%2520fundamental%2520to%2520scientific%2520discovery.%2520To%2520assess%2520the%2520potential%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520in%2520this%2520domain%252C%2520we%2520propose%2520a%2520systematic%2520benchmark%2520framework%2520MiSI-Bench.%2520This%2520framework%2520features%2520over%2520163%252C000%2520question-answer%2520pairs%2520and%2520587%252C000%2520images%2520derived%2520from%2520approximately%25204%252C000%2520molecular%2520structures%252C%2520covering%2520nine%2520complementary%2520tasks%2520that%2520evaluate%2520abilities%2520ranging%2520from%2520elementary%2520spatial%2520transformations%2520to%2520complex%2520relational%2520identifications.%2520Experimental%2520results%2520reveal%2520that%2520current%2520state-of-the-art%2520VLMs%2520perform%2520significantly%2520below%2520human%2520level%2520on%2520this%2520benchmark.%2520However%252C%2520a%2520fine-tuned%25207B%2520model%2520demonstrates%2520substantial%2520potential%252C%2520even%2520surpassing%2520humans%2520in%2520spatial%2520transformation%2520tasks%252C%2520while%2520its%2520poor%2520performance%2520in%2520scientifically-grounded%2520tasks%2520like%2520hydrogen%2520bond%2520recognition%2520underscores%2520the%2520necessity%2520of%2520integrating%2520explicit%2520domain%2520knowledge%2520for%2520progress%2520toward%2520scientific%2520AGI.%2520The%2520datasets%2520are%2520available%2520at%2520https%253A//huggingface.co/datasets/zongzhao/MiSI-bench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10867v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Macro%20to%20Micro%3A%20Benchmarking%20Microscopic%20Spatial%20Intelligence%20on%20Molecules%20via%20Vision-Language%20Models&entry.906535625=Zongzhao%20Li%20and%20Xiangzhe%20Kong%20and%20Jiahui%20Su%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yuelin%20Zhang%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Deli%20Zhao%20and%20Wenbing%20Huang&entry.1292438233=This%20paper%20introduces%20the%20concept%20of%20Microscopic%20Spatial%20Intelligence%20%28MiSI%29%2C%20the%20capability%20to%20perceive%20and%20reason%20about%20the%20spatial%20relationships%20of%20invisible%20microscopic%20entities%2C%20which%20is%20fundamental%20to%20scientific%20discovery.%20To%20assess%20the%20potential%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20this%20domain%2C%20we%20propose%20a%20systematic%20benchmark%20framework%20MiSI-Bench.%20This%20framework%20features%20over%20163%2C000%20question-answer%20pairs%20and%20587%2C000%20images%20derived%20from%20approximately%204%2C000%20molecular%20structures%2C%20covering%20nine%20complementary%20tasks%20that%20evaluate%20abilities%20ranging%20from%20elementary%20spatial%20transformations%20to%20complex%20relational%20identifications.%20Experimental%20results%20reveal%20that%20current%20state-of-the-art%20VLMs%20perform%20significantly%20below%20human%20level%20on%20this%20benchmark.%20However%2C%20a%20fine-tuned%207B%20model%20demonstrates%20substantial%20potential%2C%20even%20surpassing%20humans%20in%20spatial%20transformation%20tasks%2C%20while%20its%20poor%20performance%20in%20scientifically-grounded%20tasks%20like%20hydrogen%20bond%20recognition%20underscores%20the%20necessity%20of%20integrating%20explicit%20domain%20knowledge%20for%20progress%20toward%20scientific%20AGI.%20The%20datasets%20are%20available%20at%20https%3A//huggingface.co/datasets/zongzhao/MiSI-bench.&entry.1838667208=http%3A//arxiv.org/abs/2512.10867v2&entry.124074799=Read"},
{"title": "Data as Voters: Core Set Selection Using Approval-Based Multi-Winner Voting", "author": "Luis S\u00e1nchez-Fern\u00e1ndez and Jes\u00fas A. Fisteus and Rafael L\u00f3pez-Zaragoza", "abstract": "We present a novel approach to the core set/instance selection problem in machine learning. Our approach is based on recent results on (proportional) representation in approval-based multi-winner elections. In our model, instances play a double role as voters and candidates. The approval set of each instance in the training set (acting as a voter) is defined from the concept of local set, which already exists in the literature. We then select the election winners by using a representative voting rule, and such winners are the data instances kept in the reduced training set. We evaluate our approach in two experiments involving neural network classifiers and classic machine learning classifiers (KNN and SVM). Our experiments show that, in several cases, our approach improves the performance of state-of-the-art methods, and the differences are statistically significant.", "link": "http://arxiv.org/abs/2304.09995v3", "date": "2025-12-12", "relevancy": 2.2395, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4515}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20as%20Voters%3A%20Core%20Set%20Selection%20Using%20Approval-Based%20Multi-Winner%20Voting&body=Title%3A%20Data%20as%20Voters%3A%20Core%20Set%20Selection%20Using%20Approval-Based%20Multi-Winner%20Voting%0AAuthor%3A%20Luis%20S%C3%A1nchez-Fern%C3%A1ndez%20and%20Jes%C3%BAs%20A.%20Fisteus%20and%20Rafael%20L%C3%B3pez-Zaragoza%0AAbstract%3A%20We%20present%20a%20novel%20approach%20to%20the%20core%20set/instance%20selection%20problem%20in%20machine%20learning.%20Our%20approach%20is%20based%20on%20recent%20results%20on%20%28proportional%29%20representation%20in%20approval-based%20multi-winner%20elections.%20In%20our%20model%2C%20instances%20play%20a%20double%20role%20as%20voters%20and%20candidates.%20The%20approval%20set%20of%20each%20instance%20in%20the%20training%20set%20%28acting%20as%20a%20voter%29%20is%20defined%20from%20the%20concept%20of%20local%20set%2C%20which%20already%20exists%20in%20the%20literature.%20We%20then%20select%20the%20election%20winners%20by%20using%20a%20representative%20voting%20rule%2C%20and%20such%20winners%20are%20the%20data%20instances%20kept%20in%20the%20reduced%20training%20set.%20We%20evaluate%20our%20approach%20in%20two%20experiments%20involving%20neural%20network%20classifiers%20and%20classic%20machine%20learning%20classifiers%20%28KNN%20and%20SVM%29.%20Our%20experiments%20show%20that%2C%20in%20several%20cases%2C%20our%20approach%20improves%20the%20performance%20of%20state-of-the-art%20methods%2C%20and%20the%20differences%20are%20statistically%20significant.%0ALink%3A%20http%3A//arxiv.org/abs/2304.09995v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520as%2520Voters%253A%2520Core%2520Set%2520Selection%2520Using%2520Approval-Based%2520Multi-Winner%2520Voting%26entry.906535625%3DLuis%2520S%25C3%25A1nchez-Fern%25C3%25A1ndez%2520and%2520Jes%25C3%25BAs%2520A.%2520Fisteus%2520and%2520Rafael%2520L%25C3%25B3pez-Zaragoza%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520approach%2520to%2520the%2520core%2520set/instance%2520selection%2520problem%2520in%2520machine%2520learning.%2520Our%2520approach%2520is%2520based%2520on%2520recent%2520results%2520on%2520%2528proportional%2529%2520representation%2520in%2520approval-based%2520multi-winner%2520elections.%2520In%2520our%2520model%252C%2520instances%2520play%2520a%2520double%2520role%2520as%2520voters%2520and%2520candidates.%2520The%2520approval%2520set%2520of%2520each%2520instance%2520in%2520the%2520training%2520set%2520%2528acting%2520as%2520a%2520voter%2529%2520is%2520defined%2520from%2520the%2520concept%2520of%2520local%2520set%252C%2520which%2520already%2520exists%2520in%2520the%2520literature.%2520We%2520then%2520select%2520the%2520election%2520winners%2520by%2520using%2520a%2520representative%2520voting%2520rule%252C%2520and%2520such%2520winners%2520are%2520the%2520data%2520instances%2520kept%2520in%2520the%2520reduced%2520training%2520set.%2520We%2520evaluate%2520our%2520approach%2520in%2520two%2520experiments%2520involving%2520neural%2520network%2520classifiers%2520and%2520classic%2520machine%2520learning%2520classifiers%2520%2528KNN%2520and%2520SVM%2529.%2520Our%2520experiments%2520show%2520that%252C%2520in%2520several%2520cases%252C%2520our%2520approach%2520improves%2520the%2520performance%2520of%2520state-of-the-art%2520methods%252C%2520and%2520the%2520differences%2520are%2520statistically%2520significant.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.09995v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20as%20Voters%3A%20Core%20Set%20Selection%20Using%20Approval-Based%20Multi-Winner%20Voting&entry.906535625=Luis%20S%C3%A1nchez-Fern%C3%A1ndez%20and%20Jes%C3%BAs%20A.%20Fisteus%20and%20Rafael%20L%C3%B3pez-Zaragoza&entry.1292438233=We%20present%20a%20novel%20approach%20to%20the%20core%20set/instance%20selection%20problem%20in%20machine%20learning.%20Our%20approach%20is%20based%20on%20recent%20results%20on%20%28proportional%29%20representation%20in%20approval-based%20multi-winner%20elections.%20In%20our%20model%2C%20instances%20play%20a%20double%20role%20as%20voters%20and%20candidates.%20The%20approval%20set%20of%20each%20instance%20in%20the%20training%20set%20%28acting%20as%20a%20voter%29%20is%20defined%20from%20the%20concept%20of%20local%20set%2C%20which%20already%20exists%20in%20the%20literature.%20We%20then%20select%20the%20election%20winners%20by%20using%20a%20representative%20voting%20rule%2C%20and%20such%20winners%20are%20the%20data%20instances%20kept%20in%20the%20reduced%20training%20set.%20We%20evaluate%20our%20approach%20in%20two%20experiments%20involving%20neural%20network%20classifiers%20and%20classic%20machine%20learning%20classifiers%20%28KNN%20and%20SVM%29.%20Our%20experiments%20show%20that%2C%20in%20several%20cases%2C%20our%20approach%20improves%20the%20performance%20of%20state-of-the-art%20methods%2C%20and%20the%20differences%20are%20statistically%20significant.&entry.1838667208=http%3A//arxiv.org/abs/2304.09995v3&entry.124074799=Read"},
{"title": "MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator", "author": "Peiqing Yang and Shangchen Zhou and Kai Hao and Qingyi Tao", "abstract": "Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.", "link": "http://arxiv.org/abs/2512.11782v1", "date": "2025-12-12", "relevancy": 2.2345, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5624}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5596}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatAnyone%202%3A%20Scaling%20Video%20Matting%20via%20a%20Learned%20Quality%20Evaluator&body=Title%3A%20MatAnyone%202%3A%20Scaling%20Video%20Matting%20via%20a%20Learned%20Quality%20Evaluator%0AAuthor%3A%20Peiqing%20Yang%20and%20Shangchen%20Zhou%20and%20Kai%20Hao%20and%20Qingyi%20Tao%0AAbstract%3A%20Video%20matting%20remains%20limited%20by%20the%20scale%20and%20realism%20of%20existing%20datasets.%20While%20leveraging%20segmentation%20data%20can%20enhance%20semantic%20stability%2C%20the%20lack%20of%20effective%20boundary%20supervision%20often%20leads%20to%20segmentation-like%20mattes%20lacking%20fine%20details.%20To%20this%20end%2C%20we%20introduce%20a%20learned%20Matting%20Quality%20Evaluator%20%28MQE%29%20that%20assesses%20semantic%20and%20boundary%20quality%20of%20alpha%20mattes%20without%20ground%20truth.%20It%20produces%20a%20pixel-wise%20evaluation%20map%20that%20identifies%20reliable%20and%20erroneous%20regions%2C%20enabling%20fine-grained%20quality%20assessment.%20The%20MQE%20scales%20up%20video%20matting%20in%20two%20ways%3A%20%281%29%20as%20an%20online%20matting-quality%20feedback%20during%20training%20to%20suppress%20erroneous%20regions%2C%20providing%20comprehensive%20supervision%2C%20and%20%282%29%20as%20an%20offline%20selection%20module%20for%20data%20curation%2C%20improving%20annotation%20quality%20by%20combining%20the%20strengths%20of%20leading%20video%20and%20image%20matting%20models.%20This%20process%20allows%20us%20to%20build%20a%20large-scale%20real-world%20video%20matting%20dataset%2C%20VMReal%2C%20containing%2028K%20clips%20and%202.4M%20frames.%20To%20handle%20large%20appearance%20variations%20in%20long%20videos%2C%20we%20introduce%20a%20reference-frame%20training%20strategy%20that%20incorporates%20long-range%20frames%20beyond%20the%20local%20window%20for%20effective%20training.%20Our%20MatAnyone%202%20achieves%20state-of-the-art%20performance%20on%20both%20synthetic%20and%20real-world%20benchmarks%2C%20surpassing%20prior%20methods%20across%20all%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatAnyone%25202%253A%2520Scaling%2520Video%2520Matting%2520via%2520a%2520Learned%2520Quality%2520Evaluator%26entry.906535625%3DPeiqing%2520Yang%2520and%2520Shangchen%2520Zhou%2520and%2520Kai%2520Hao%2520and%2520Qingyi%2520Tao%26entry.1292438233%3DVideo%2520matting%2520remains%2520limited%2520by%2520the%2520scale%2520and%2520realism%2520of%2520existing%2520datasets.%2520While%2520leveraging%2520segmentation%2520data%2520can%2520enhance%2520semantic%2520stability%252C%2520the%2520lack%2520of%2520effective%2520boundary%2520supervision%2520often%2520leads%2520to%2520segmentation-like%2520mattes%2520lacking%2520fine%2520details.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520learned%2520Matting%2520Quality%2520Evaluator%2520%2528MQE%2529%2520that%2520assesses%2520semantic%2520and%2520boundary%2520quality%2520of%2520alpha%2520mattes%2520without%2520ground%2520truth.%2520It%2520produces%2520a%2520pixel-wise%2520evaluation%2520map%2520that%2520identifies%2520reliable%2520and%2520erroneous%2520regions%252C%2520enabling%2520fine-grained%2520quality%2520assessment.%2520The%2520MQE%2520scales%2520up%2520video%2520matting%2520in%2520two%2520ways%253A%2520%25281%2529%2520as%2520an%2520online%2520matting-quality%2520feedback%2520during%2520training%2520to%2520suppress%2520erroneous%2520regions%252C%2520providing%2520comprehensive%2520supervision%252C%2520and%2520%25282%2529%2520as%2520an%2520offline%2520selection%2520module%2520for%2520data%2520curation%252C%2520improving%2520annotation%2520quality%2520by%2520combining%2520the%2520strengths%2520of%2520leading%2520video%2520and%2520image%2520matting%2520models.%2520This%2520process%2520allows%2520us%2520to%2520build%2520a%2520large-scale%2520real-world%2520video%2520matting%2520dataset%252C%2520VMReal%252C%2520containing%252028K%2520clips%2520and%25202.4M%2520frames.%2520To%2520handle%2520large%2520appearance%2520variations%2520in%2520long%2520videos%252C%2520we%2520introduce%2520a%2520reference-frame%2520training%2520strategy%2520that%2520incorporates%2520long-range%2520frames%2520beyond%2520the%2520local%2520window%2520for%2520effective%2520training.%2520Our%2520MatAnyone%25202%2520achieves%2520state-of-the-art%2520performance%2520on%2520both%2520synthetic%2520and%2520real-world%2520benchmarks%252C%2520surpassing%2520prior%2520methods%2520across%2520all%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatAnyone%202%3A%20Scaling%20Video%20Matting%20via%20a%20Learned%20Quality%20Evaluator&entry.906535625=Peiqing%20Yang%20and%20Shangchen%20Zhou%20and%20Kai%20Hao%20and%20Qingyi%20Tao&entry.1292438233=Video%20matting%20remains%20limited%20by%20the%20scale%20and%20realism%20of%20existing%20datasets.%20While%20leveraging%20segmentation%20data%20can%20enhance%20semantic%20stability%2C%20the%20lack%20of%20effective%20boundary%20supervision%20often%20leads%20to%20segmentation-like%20mattes%20lacking%20fine%20details.%20To%20this%20end%2C%20we%20introduce%20a%20learned%20Matting%20Quality%20Evaluator%20%28MQE%29%20that%20assesses%20semantic%20and%20boundary%20quality%20of%20alpha%20mattes%20without%20ground%20truth.%20It%20produces%20a%20pixel-wise%20evaluation%20map%20that%20identifies%20reliable%20and%20erroneous%20regions%2C%20enabling%20fine-grained%20quality%20assessment.%20The%20MQE%20scales%20up%20video%20matting%20in%20two%20ways%3A%20%281%29%20as%20an%20online%20matting-quality%20feedback%20during%20training%20to%20suppress%20erroneous%20regions%2C%20providing%20comprehensive%20supervision%2C%20and%20%282%29%20as%20an%20offline%20selection%20module%20for%20data%20curation%2C%20improving%20annotation%20quality%20by%20combining%20the%20strengths%20of%20leading%20video%20and%20image%20matting%20models.%20This%20process%20allows%20us%20to%20build%20a%20large-scale%20real-world%20video%20matting%20dataset%2C%20VMReal%2C%20containing%2028K%20clips%20and%202.4M%20frames.%20To%20handle%20large%20appearance%20variations%20in%20long%20videos%2C%20we%20introduce%20a%20reference-frame%20training%20strategy%20that%20incorporates%20long-range%20frames%20beyond%20the%20local%20window%20for%20effective%20training.%20Our%20MatAnyone%202%20achieves%20state-of-the-art%20performance%20on%20both%20synthetic%20and%20real-world%20benchmarks%2C%20surpassing%20prior%20methods%20across%20all%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2512.11782v1&entry.124074799=Read"},
{"title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence", "author": "Sophia Hager and David Mueller and Kevin Duh and Nicholas Andrews", "abstract": "As large language models (LLMs) are increasingly used for factual question-answering, it becomes more important for LLMs to have the capability to communicate the likelihood that their answer is correct. For these verbalized expressions of uncertainty to be meaningful, they should reflect the error rates at the expressed level of confidence. However, when prompted to express confidence, the error rates of current LLMs are inconsistent with their communicated confidences, highlighting the need for uncertainty quantification methods. Many prior methods calculate lexical uncertainty, estimating a model's confidence in the specific string it generated. In some cases, however, it may be more useful to estimate semantic uncertainty, or the model's confidence in the answer regardless of how it is verbalized. We propose a simple procedure, uncertainty distillation, to teach an LLM to verbalize calibrated semantic confidences. Using held-out data to map initial uncertainty estimates to meaningful probabilities, we create examples annotated with verbalized probabilities for supervised fine-tuning. We find that our method yields verbalized confidences that correlate well with observed error rates, even when compared to strong baselines, some of which are more than twenty times slower at inference time. Additionally, we demonstrate that our method can be applied to black-box models that allow API-based fine-tuning, resulting in estimates of uncertainty that are both more effective and more efficient than any of our baselines.", "link": "http://arxiv.org/abs/2503.14749v3", "date": "2025-12-12", "relevancy": 2.2202, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6197}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5725}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Distillation%3A%20Teaching%20Language%20Models%20to%20Express%20Semantic%20Confidence&body=Title%3A%20Uncertainty%20Distillation%3A%20Teaching%20Language%20Models%20to%20Express%20Semantic%20Confidence%0AAuthor%3A%20Sophia%20Hager%20and%20David%20Mueller%20and%20Kevin%20Duh%20and%20Nicholas%20Andrews%0AAbstract%3A%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20for%20factual%20question-answering%2C%20it%20becomes%20more%20important%20for%20LLMs%20to%20have%20the%20capability%20to%20communicate%20the%20likelihood%20that%20their%20answer%20is%20correct.%20For%20these%20verbalized%20expressions%20of%20uncertainty%20to%20be%20meaningful%2C%20they%20should%20reflect%20the%20error%20rates%20at%20the%20expressed%20level%20of%20confidence.%20However%2C%20when%20prompted%20to%20express%20confidence%2C%20the%20error%20rates%20of%20current%20LLMs%20are%20inconsistent%20with%20their%20communicated%20confidences%2C%20highlighting%20the%20need%20for%20uncertainty%20quantification%20methods.%20Many%20prior%20methods%20calculate%20lexical%20uncertainty%2C%20estimating%20a%20model%27s%20confidence%20in%20the%20specific%20string%20it%20generated.%20In%20some%20cases%2C%20however%2C%20it%20may%20be%20more%20useful%20to%20estimate%20semantic%20uncertainty%2C%20or%20the%20model%27s%20confidence%20in%20the%20answer%20regardless%20of%20how%20it%20is%20verbalized.%20We%20propose%20a%20simple%20procedure%2C%20uncertainty%20distillation%2C%20to%20teach%20an%20LLM%20to%20verbalize%20calibrated%20semantic%20confidences.%20Using%20held-out%20data%20to%20map%20initial%20uncertainty%20estimates%20to%20meaningful%20probabilities%2C%20we%20create%20examples%20annotated%20with%20verbalized%20probabilities%20for%20supervised%20fine-tuning.%20We%20find%20that%20our%20method%20yields%20verbalized%20confidences%20that%20correlate%20well%20with%20observed%20error%20rates%2C%20even%20when%20compared%20to%20strong%20baselines%2C%20some%20of%20which%20are%20more%20than%20twenty%20times%20slower%20at%20inference%20time.%20Additionally%2C%20we%20demonstrate%20that%20our%20method%20can%20be%20applied%20to%20black-box%20models%20that%20allow%20API-based%20fine-tuning%2C%20resulting%20in%20estimates%20of%20uncertainty%20that%20are%20both%20more%20effective%20and%20more%20efficient%20than%20any%20of%20our%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2503.14749v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Distillation%253A%2520Teaching%2520Language%2520Models%2520to%2520Express%2520Semantic%2520Confidence%26entry.906535625%3DSophia%2520Hager%2520and%2520David%2520Mueller%2520and%2520Kevin%2520Duh%2520and%2520Nicholas%2520Andrews%26entry.1292438233%3DAs%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520for%2520factual%2520question-answering%252C%2520it%2520becomes%2520more%2520important%2520for%2520LLMs%2520to%2520have%2520the%2520capability%2520to%2520communicate%2520the%2520likelihood%2520that%2520their%2520answer%2520is%2520correct.%2520For%2520these%2520verbalized%2520expressions%2520of%2520uncertainty%2520to%2520be%2520meaningful%252C%2520they%2520should%2520reflect%2520the%2520error%2520rates%2520at%2520the%2520expressed%2520level%2520of%2520confidence.%2520However%252C%2520when%2520prompted%2520to%2520express%2520confidence%252C%2520the%2520error%2520rates%2520of%2520current%2520LLMs%2520are%2520inconsistent%2520with%2520their%2520communicated%2520confidences%252C%2520highlighting%2520the%2520need%2520for%2520uncertainty%2520quantification%2520methods.%2520Many%2520prior%2520methods%2520calculate%2520lexical%2520uncertainty%252C%2520estimating%2520a%2520model%2527s%2520confidence%2520in%2520the%2520specific%2520string%2520it%2520generated.%2520In%2520some%2520cases%252C%2520however%252C%2520it%2520may%2520be%2520more%2520useful%2520to%2520estimate%2520semantic%2520uncertainty%252C%2520or%2520the%2520model%2527s%2520confidence%2520in%2520the%2520answer%2520regardless%2520of%2520how%2520it%2520is%2520verbalized.%2520We%2520propose%2520a%2520simple%2520procedure%252C%2520uncertainty%2520distillation%252C%2520to%2520teach%2520an%2520LLM%2520to%2520verbalize%2520calibrated%2520semantic%2520confidences.%2520Using%2520held-out%2520data%2520to%2520map%2520initial%2520uncertainty%2520estimates%2520to%2520meaningful%2520probabilities%252C%2520we%2520create%2520examples%2520annotated%2520with%2520verbalized%2520probabilities%2520for%2520supervised%2520fine-tuning.%2520We%2520find%2520that%2520our%2520method%2520yields%2520verbalized%2520confidences%2520that%2520correlate%2520well%2520with%2520observed%2520error%2520rates%252C%2520even%2520when%2520compared%2520to%2520strong%2520baselines%252C%2520some%2520of%2520which%2520are%2520more%2520than%2520twenty%2520times%2520slower%2520at%2520inference%2520time.%2520Additionally%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520can%2520be%2520applied%2520to%2520black-box%2520models%2520that%2520allow%2520API-based%2520fine-tuning%252C%2520resulting%2520in%2520estimates%2520of%2520uncertainty%2520that%2520are%2520both%2520more%2520effective%2520and%2520more%2520efficient%2520than%2520any%2520of%2520our%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14749v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Distillation%3A%20Teaching%20Language%20Models%20to%20Express%20Semantic%20Confidence&entry.906535625=Sophia%20Hager%20and%20David%20Mueller%20and%20Kevin%20Duh%20and%20Nicholas%20Andrews&entry.1292438233=As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20for%20factual%20question-answering%2C%20it%20becomes%20more%20important%20for%20LLMs%20to%20have%20the%20capability%20to%20communicate%20the%20likelihood%20that%20their%20answer%20is%20correct.%20For%20these%20verbalized%20expressions%20of%20uncertainty%20to%20be%20meaningful%2C%20they%20should%20reflect%20the%20error%20rates%20at%20the%20expressed%20level%20of%20confidence.%20However%2C%20when%20prompted%20to%20express%20confidence%2C%20the%20error%20rates%20of%20current%20LLMs%20are%20inconsistent%20with%20their%20communicated%20confidences%2C%20highlighting%20the%20need%20for%20uncertainty%20quantification%20methods.%20Many%20prior%20methods%20calculate%20lexical%20uncertainty%2C%20estimating%20a%20model%27s%20confidence%20in%20the%20specific%20string%20it%20generated.%20In%20some%20cases%2C%20however%2C%20it%20may%20be%20more%20useful%20to%20estimate%20semantic%20uncertainty%2C%20or%20the%20model%27s%20confidence%20in%20the%20answer%20regardless%20of%20how%20it%20is%20verbalized.%20We%20propose%20a%20simple%20procedure%2C%20uncertainty%20distillation%2C%20to%20teach%20an%20LLM%20to%20verbalize%20calibrated%20semantic%20confidences.%20Using%20held-out%20data%20to%20map%20initial%20uncertainty%20estimates%20to%20meaningful%20probabilities%2C%20we%20create%20examples%20annotated%20with%20verbalized%20probabilities%20for%20supervised%20fine-tuning.%20We%20find%20that%20our%20method%20yields%20verbalized%20confidences%20that%20correlate%20well%20with%20observed%20error%20rates%2C%20even%20when%20compared%20to%20strong%20baselines%2C%20some%20of%20which%20are%20more%20than%20twenty%20times%20slower%20at%20inference%20time.%20Additionally%2C%20we%20demonstrate%20that%20our%20method%20can%20be%20applied%20to%20black-box%20models%20that%20allow%20API-based%20fine-tuning%2C%20resulting%20in%20estimates%20of%20uncertainty%20that%20are%20both%20more%20effective%20and%20more%20efficient%20than%20any%20of%20our%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2503.14749v3&entry.124074799=Read"},
{"title": "Reconstruction as a Bridge for Event-Based Visual Question Answering", "author": "Hanyue Lou and Jiayi Zhou and Yang Zhang and Boyu Li and Yi Wang and Guangnan Ye and Boxin Shi", "abstract": "Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.", "link": "http://arxiv.org/abs/2512.11510v1", "date": "2025-12-12", "relevancy": 2.2181, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5645}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstruction%20as%20a%20Bridge%20for%20Event-Based%20Visual%20Question%20Answering&body=Title%3A%20Reconstruction%20as%20a%20Bridge%20for%20Event-Based%20Visual%20Question%20Answering%0AAuthor%3A%20Hanyue%20Lou%20and%20Jiayi%20Zhou%20and%20Yang%20Zhang%20and%20Boyu%20Li%20and%20Yi%20Wang%20and%20Guangnan%20Ye%20and%20Boxin%20Shi%0AAbstract%3A%20Integrating%20event%20cameras%20with%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20promises%20general%20scene%20understanding%20in%20challenging%20visual%20conditions%2C%20yet%20requires%20navigating%20a%20trade-off%20between%20preserving%20the%20unique%20advantages%20of%20event%20data%20and%20ensuring%20compatibility%20with%20frame-based%20models.%20We%20address%20this%20challenge%20by%20using%20reconstruction%20as%20a%20bridge%2C%20proposing%20a%20straightforward%20Frame-based%20Reconstruction%20and%20Tokenization%20%28FRT%29%20method%20and%20designing%20an%20efficient%20Adaptive%20Reconstruction%20and%20Tokenization%20%28ART%29%20method%20that%20leverages%20event%20sparsity.%20For%20robust%20evaluation%2C%20we%20introduce%20EvQA%2C%20the%20first%20objective%2C%20real-world%20benchmark%20for%20event-based%20MLLMs%2C%20comprising%201%2C000%20event-Q%26A%20pairs%20from%2022%20public%20datasets.%20Our%20experiments%20demonstrate%20that%20our%20methods%20achieve%20state-of-the-art%20performance%20on%20EvQA%2C%20highlighting%20the%20significant%20potential%20of%20MLLMs%20in%20event-based%20vision.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstruction%2520as%2520a%2520Bridge%2520for%2520Event-Based%2520Visual%2520Question%2520Answering%26entry.906535625%3DHanyue%2520Lou%2520and%2520Jiayi%2520Zhou%2520and%2520Yang%2520Zhang%2520and%2520Boyu%2520Li%2520and%2520Yi%2520Wang%2520and%2520Guangnan%2520Ye%2520and%2520Boxin%2520Shi%26entry.1292438233%3DIntegrating%2520event%2520cameras%2520with%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520promises%2520general%2520scene%2520understanding%2520in%2520challenging%2520visual%2520conditions%252C%2520yet%2520requires%2520navigating%2520a%2520trade-off%2520between%2520preserving%2520the%2520unique%2520advantages%2520of%2520event%2520data%2520and%2520ensuring%2520compatibility%2520with%2520frame-based%2520models.%2520We%2520address%2520this%2520challenge%2520by%2520using%2520reconstruction%2520as%2520a%2520bridge%252C%2520proposing%2520a%2520straightforward%2520Frame-based%2520Reconstruction%2520and%2520Tokenization%2520%2528FRT%2529%2520method%2520and%2520designing%2520an%2520efficient%2520Adaptive%2520Reconstruction%2520and%2520Tokenization%2520%2528ART%2529%2520method%2520that%2520leverages%2520event%2520sparsity.%2520For%2520robust%2520evaluation%252C%2520we%2520introduce%2520EvQA%252C%2520the%2520first%2520objective%252C%2520real-world%2520benchmark%2520for%2520event-based%2520MLLMs%252C%2520comprising%25201%252C000%2520event-Q%2526A%2520pairs%2520from%252022%2520public%2520datasets.%2520Our%2520experiments%2520demonstrate%2520that%2520our%2520methods%2520achieve%2520state-of-the-art%2520performance%2520on%2520EvQA%252C%2520highlighting%2520the%2520significant%2520potential%2520of%2520MLLMs%2520in%2520event-based%2520vision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstruction%20as%20a%20Bridge%20for%20Event-Based%20Visual%20Question%20Answering&entry.906535625=Hanyue%20Lou%20and%20Jiayi%20Zhou%20and%20Yang%20Zhang%20and%20Boyu%20Li%20and%20Yi%20Wang%20and%20Guangnan%20Ye%20and%20Boxin%20Shi&entry.1292438233=Integrating%20event%20cameras%20with%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20promises%20general%20scene%20understanding%20in%20challenging%20visual%20conditions%2C%20yet%20requires%20navigating%20a%20trade-off%20between%20preserving%20the%20unique%20advantages%20of%20event%20data%20and%20ensuring%20compatibility%20with%20frame-based%20models.%20We%20address%20this%20challenge%20by%20using%20reconstruction%20as%20a%20bridge%2C%20proposing%20a%20straightforward%20Frame-based%20Reconstruction%20and%20Tokenization%20%28FRT%29%20method%20and%20designing%20an%20efficient%20Adaptive%20Reconstruction%20and%20Tokenization%20%28ART%29%20method%20that%20leverages%20event%20sparsity.%20For%20robust%20evaluation%2C%20we%20introduce%20EvQA%2C%20the%20first%20objective%2C%20real-world%20benchmark%20for%20event-based%20MLLMs%2C%20comprising%201%2C000%20event-Q%26A%20pairs%20from%2022%20public%20datasets.%20Our%20experiments%20demonstrate%20that%20our%20methods%20achieve%20state-of-the-art%20performance%20on%20EvQA%2C%20highlighting%20the%20significant%20potential%20of%20MLLMs%20in%20event-based%20vision.&entry.1838667208=http%3A//arxiv.org/abs/2512.11510v1&entry.124074799=Read"},
{"title": "Geometry-Informed Neural Operator Transformer", "author": "Qibang Liu and Weiheng Zhong and Hadi Meidani and Diab Abueidda and Seid Koric and Philippe Geubelle", "abstract": "Machine-learning-based surrogate models offer significant computational efficiency and faster simulations compared to traditional numerical methods, especially for problems requiring repeated evaluations of partial differential equations. This work introduces the Geometry-Informed Neural Operator Transformer (GINOT), which integrates the transformer architecture with the neural operator framework to enable forward predictions on arbitrary geometries. GINOT employs a sampling and grouping strategy together with an attention mechanism to encode surface point clouds that are unordered, exhibit non-uniform point densities, and contain varying numbers of points for different geometries. The geometry information is seamlessly integrated with query points in the solution decoder through the attention mechanism. The performance of GINOT is validated on multiple challenging datasets, showcasing its high accuracy and strong generalization capabilities for complex and arbitrary 2D and 3D geometries.", "link": "http://arxiv.org/abs/2504.19452v5", "date": "2025-12-12", "relevancy": 2.2164, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5625}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5592}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Informed%20Neural%20Operator%20Transformer&body=Title%3A%20Geometry-Informed%20Neural%20Operator%20Transformer%0AAuthor%3A%20Qibang%20Liu%20and%20Weiheng%20Zhong%20and%20Hadi%20Meidani%20and%20Diab%20Abueidda%20and%20Seid%20Koric%20and%20Philippe%20Geubelle%0AAbstract%3A%20Machine-learning-based%20surrogate%20models%20offer%20significant%20computational%20efficiency%20and%20faster%20simulations%20compared%20to%20traditional%20numerical%20methods%2C%20especially%20for%20problems%20requiring%20repeated%20evaluations%20of%20partial%20differential%20equations.%20This%20work%20introduces%20the%20Geometry-Informed%20Neural%20Operator%20Transformer%20%28GINOT%29%2C%20which%20integrates%20the%20transformer%20architecture%20with%20the%20neural%20operator%20framework%20to%20enable%20forward%20predictions%20on%20arbitrary%20geometries.%20GINOT%20employs%20a%20sampling%20and%20grouping%20strategy%20together%20with%20an%20attention%20mechanism%20to%20encode%20surface%20point%20clouds%20that%20are%20unordered%2C%20exhibit%20non-uniform%20point%20densities%2C%20and%20contain%20varying%20numbers%20of%20points%20for%20different%20geometries.%20The%20geometry%20information%20is%20seamlessly%20integrated%20with%20query%20points%20in%20the%20solution%20decoder%20through%20the%20attention%20mechanism.%20The%20performance%20of%20GINOT%20is%20validated%20on%20multiple%20challenging%20datasets%2C%20showcasing%20its%20high%20accuracy%20and%20strong%20generalization%20capabilities%20for%20complex%20and%20arbitrary%202D%20and%203D%20geometries.%0ALink%3A%20http%3A//arxiv.org/abs/2504.19452v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Informed%2520Neural%2520Operator%2520Transformer%26entry.906535625%3DQibang%2520Liu%2520and%2520Weiheng%2520Zhong%2520and%2520Hadi%2520Meidani%2520and%2520Diab%2520Abueidda%2520and%2520Seid%2520Koric%2520and%2520Philippe%2520Geubelle%26entry.1292438233%3DMachine-learning-based%2520surrogate%2520models%2520offer%2520significant%2520computational%2520efficiency%2520and%2520faster%2520simulations%2520compared%2520to%2520traditional%2520numerical%2520methods%252C%2520especially%2520for%2520problems%2520requiring%2520repeated%2520evaluations%2520of%2520partial%2520differential%2520equations.%2520This%2520work%2520introduces%2520the%2520Geometry-Informed%2520Neural%2520Operator%2520Transformer%2520%2528GINOT%2529%252C%2520which%2520integrates%2520the%2520transformer%2520architecture%2520with%2520the%2520neural%2520operator%2520framework%2520to%2520enable%2520forward%2520predictions%2520on%2520arbitrary%2520geometries.%2520GINOT%2520employs%2520a%2520sampling%2520and%2520grouping%2520strategy%2520together%2520with%2520an%2520attention%2520mechanism%2520to%2520encode%2520surface%2520point%2520clouds%2520that%2520are%2520unordered%252C%2520exhibit%2520non-uniform%2520point%2520densities%252C%2520and%2520contain%2520varying%2520numbers%2520of%2520points%2520for%2520different%2520geometries.%2520The%2520geometry%2520information%2520is%2520seamlessly%2520integrated%2520with%2520query%2520points%2520in%2520the%2520solution%2520decoder%2520through%2520the%2520attention%2520mechanism.%2520The%2520performance%2520of%2520GINOT%2520is%2520validated%2520on%2520multiple%2520challenging%2520datasets%252C%2520showcasing%2520its%2520high%2520accuracy%2520and%2520strong%2520generalization%2520capabilities%2520for%2520complex%2520and%2520arbitrary%25202D%2520and%25203D%2520geometries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19452v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Informed%20Neural%20Operator%20Transformer&entry.906535625=Qibang%20Liu%20and%20Weiheng%20Zhong%20and%20Hadi%20Meidani%20and%20Diab%20Abueidda%20and%20Seid%20Koric%20and%20Philippe%20Geubelle&entry.1292438233=Machine-learning-based%20surrogate%20models%20offer%20significant%20computational%20efficiency%20and%20faster%20simulations%20compared%20to%20traditional%20numerical%20methods%2C%20especially%20for%20problems%20requiring%20repeated%20evaluations%20of%20partial%20differential%20equations.%20This%20work%20introduces%20the%20Geometry-Informed%20Neural%20Operator%20Transformer%20%28GINOT%29%2C%20which%20integrates%20the%20transformer%20architecture%20with%20the%20neural%20operator%20framework%20to%20enable%20forward%20predictions%20on%20arbitrary%20geometries.%20GINOT%20employs%20a%20sampling%20and%20grouping%20strategy%20together%20with%20an%20attention%20mechanism%20to%20encode%20surface%20point%20clouds%20that%20are%20unordered%2C%20exhibit%20non-uniform%20point%20densities%2C%20and%20contain%20varying%20numbers%20of%20points%20for%20different%20geometries.%20The%20geometry%20information%20is%20seamlessly%20integrated%20with%20query%20points%20in%20the%20solution%20decoder%20through%20the%20attention%20mechanism.%20The%20performance%20of%20GINOT%20is%20validated%20on%20multiple%20challenging%20datasets%2C%20showcasing%20its%20high%20accuracy%20and%20strong%20generalization%20capabilities%20for%20complex%20and%20arbitrary%202D%20and%203D%20geometries.&entry.1838667208=http%3A//arxiv.org/abs/2504.19452v5&entry.124074799=Read"},
{"title": "Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection", "author": "Qiushi Guo", "abstract": "Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.", "link": "http://arxiv.org/abs/2512.11683v1", "date": "2025-12-12", "relevancy": 2.211, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.586}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Copy-Paste%3A%20Multimodal%20and%20Depth-Aware%20Compositing%20for%20Robust%20Face%20Detection&body=Title%3A%20Depth-Copy-Paste%3A%20Multimodal%20and%20Depth-Aware%20Compositing%20for%20Robust%20Face%20Detection%0AAuthor%3A%20Qiushi%20Guo%0AAbstract%3A%20Data%20augmentation%20is%20crucial%20for%20improving%20the%20robustness%20of%20face%20detection%20systems%2C%20especially%20under%20challenging%20conditions%20such%20as%20occlusion%2C%20illumination%20variation%2C%20and%20complex%20environments.%20Traditional%20copy%20paste%20augmentation%20often%20produces%20unrealistic%20composites%20due%20to%20inaccurate%20foreground%20extraction%2C%20inconsistent%20scene%20geometry%2C%20and%20mismatched%20background%20semantics.%20To%20address%20these%20limitations%2C%20we%20propose%20Depth%20Copy%20Paste%2C%20a%20multimodal%20and%20depth%20aware%20augmentation%20framework%20that%20generates%20diverse%20and%20physically%20consistent%20face%20detection%20training%20samples%20by%20copying%20full%20body%20person%20instances%20and%20pasting%20them%20into%20semantically%20compatible%20scenes.%20Our%20approach%20first%20employs%20BLIP%20and%20CLIP%20to%20jointly%20assess%20semantic%20and%20visual%20coherence%2C%20enabling%20automatic%20retrieval%20of%20the%20most%20suitable%20background%20images%20for%20the%20given%20foreground%20person.%20To%20ensure%20high%20quality%20foreground%20masks%20that%20preserve%20facial%20details%2C%20we%20integrate%20SAM3%20for%20precise%20segmentation%20and%20Depth-Anything%20to%20extract%20only%20the%20non%20occluded%20visible%20person%20regions%2C%20preventing%20corrupted%20facial%20textures%20from%20being%20used%20in%20augmentation.%20For%20geometric%20realism%2C%20we%20introduce%20a%20depth%20guided%20sliding%20window%20placement%20mechanism%20that%20searches%20over%20the%20background%20depth%20map%20to%20identify%20paste%20locations%20with%20optimal%20depth%20continuity%20and%20scale%20alignment.%20The%20resulting%20composites%20exhibit%20natural%20depth%20relationships%20and%20improved%20visual%20plausibility.%20Extensive%20experiments%20show%20that%20Depth%20Copy%20Paste%20provides%20more%20diverse%20and%20realistic%20training%20data%2C%20leading%20to%20significant%20performance%20improvements%20in%20downstream%20face%20detection%20tasks%20compared%20with%20traditional%20copy%20paste%20and%20depth%20free%20augmentation%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Copy-Paste%253A%2520Multimodal%2520and%2520Depth-Aware%2520Compositing%2520for%2520Robust%2520Face%2520Detection%26entry.906535625%3DQiushi%2520Guo%26entry.1292438233%3DData%2520augmentation%2520is%2520crucial%2520for%2520improving%2520the%2520robustness%2520of%2520face%2520detection%2520systems%252C%2520especially%2520under%2520challenging%2520conditions%2520such%2520as%2520occlusion%252C%2520illumination%2520variation%252C%2520and%2520complex%2520environments.%2520Traditional%2520copy%2520paste%2520augmentation%2520often%2520produces%2520unrealistic%2520composites%2520due%2520to%2520inaccurate%2520foreground%2520extraction%252C%2520inconsistent%2520scene%2520geometry%252C%2520and%2520mismatched%2520background%2520semantics.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Depth%2520Copy%2520Paste%252C%2520a%2520multimodal%2520and%2520depth%2520aware%2520augmentation%2520framework%2520that%2520generates%2520diverse%2520and%2520physically%2520consistent%2520face%2520detection%2520training%2520samples%2520by%2520copying%2520full%2520body%2520person%2520instances%2520and%2520pasting%2520them%2520into%2520semantically%2520compatible%2520scenes.%2520Our%2520approach%2520first%2520employs%2520BLIP%2520and%2520CLIP%2520to%2520jointly%2520assess%2520semantic%2520and%2520visual%2520coherence%252C%2520enabling%2520automatic%2520retrieval%2520of%2520the%2520most%2520suitable%2520background%2520images%2520for%2520the%2520given%2520foreground%2520person.%2520To%2520ensure%2520high%2520quality%2520foreground%2520masks%2520that%2520preserve%2520facial%2520details%252C%2520we%2520integrate%2520SAM3%2520for%2520precise%2520segmentation%2520and%2520Depth-Anything%2520to%2520extract%2520only%2520the%2520non%2520occluded%2520visible%2520person%2520regions%252C%2520preventing%2520corrupted%2520facial%2520textures%2520from%2520being%2520used%2520in%2520augmentation.%2520For%2520geometric%2520realism%252C%2520we%2520introduce%2520a%2520depth%2520guided%2520sliding%2520window%2520placement%2520mechanism%2520that%2520searches%2520over%2520the%2520background%2520depth%2520map%2520to%2520identify%2520paste%2520locations%2520with%2520optimal%2520depth%2520continuity%2520and%2520scale%2520alignment.%2520The%2520resulting%2520composites%2520exhibit%2520natural%2520depth%2520relationships%2520and%2520improved%2520visual%2520plausibility.%2520Extensive%2520experiments%2520show%2520that%2520Depth%2520Copy%2520Paste%2520provides%2520more%2520diverse%2520and%2520realistic%2520training%2520data%252C%2520leading%2520to%2520significant%2520performance%2520improvements%2520in%2520downstream%2520face%2520detection%2520tasks%2520compared%2520with%2520traditional%2520copy%2520paste%2520and%2520depth%2520free%2520augmentation%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Copy-Paste%3A%20Multimodal%20and%20Depth-Aware%20Compositing%20for%20Robust%20Face%20Detection&entry.906535625=Qiushi%20Guo&entry.1292438233=Data%20augmentation%20is%20crucial%20for%20improving%20the%20robustness%20of%20face%20detection%20systems%2C%20especially%20under%20challenging%20conditions%20such%20as%20occlusion%2C%20illumination%20variation%2C%20and%20complex%20environments.%20Traditional%20copy%20paste%20augmentation%20often%20produces%20unrealistic%20composites%20due%20to%20inaccurate%20foreground%20extraction%2C%20inconsistent%20scene%20geometry%2C%20and%20mismatched%20background%20semantics.%20To%20address%20these%20limitations%2C%20we%20propose%20Depth%20Copy%20Paste%2C%20a%20multimodal%20and%20depth%20aware%20augmentation%20framework%20that%20generates%20diverse%20and%20physically%20consistent%20face%20detection%20training%20samples%20by%20copying%20full%20body%20person%20instances%20and%20pasting%20them%20into%20semantically%20compatible%20scenes.%20Our%20approach%20first%20employs%20BLIP%20and%20CLIP%20to%20jointly%20assess%20semantic%20and%20visual%20coherence%2C%20enabling%20automatic%20retrieval%20of%20the%20most%20suitable%20background%20images%20for%20the%20given%20foreground%20person.%20To%20ensure%20high%20quality%20foreground%20masks%20that%20preserve%20facial%20details%2C%20we%20integrate%20SAM3%20for%20precise%20segmentation%20and%20Depth-Anything%20to%20extract%20only%20the%20non%20occluded%20visible%20person%20regions%2C%20preventing%20corrupted%20facial%20textures%20from%20being%20used%20in%20augmentation.%20For%20geometric%20realism%2C%20we%20introduce%20a%20depth%20guided%20sliding%20window%20placement%20mechanism%20that%20searches%20over%20the%20background%20depth%20map%20to%20identify%20paste%20locations%20with%20optimal%20depth%20continuity%20and%20scale%20alignment.%20The%20resulting%20composites%20exhibit%20natural%20depth%20relationships%20and%20improved%20visual%20plausibility.%20Extensive%20experiments%20show%20that%20Depth%20Copy%20Paste%20provides%20more%20diverse%20and%20realistic%20training%20data%2C%20leading%20to%20significant%20performance%20improvements%20in%20downstream%20face%20detection%20tasks%20compared%20with%20traditional%20copy%20paste%20and%20depth%20free%20augmentation%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.11683v1&entry.124074799=Read"},
{"title": "A Fast Interpretable Fuzzy Tree Learner", "author": "Javier Fumanal-Idocin and Raquel Fernandez-Peralta and Javier Andreu-Perez", "abstract": "Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public", "link": "http://arxiv.org/abs/2512.11616v1", "date": "2025-12-12", "relevancy": 2.2011, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4618}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4307}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Fast%20Interpretable%20Fuzzy%20Tree%20Learner&body=Title%3A%20A%20Fast%20Interpretable%20Fuzzy%20Tree%20Learner%0AAuthor%3A%20Javier%20Fumanal-Idocin%20and%20Raquel%20Fernandez-Peralta%20and%20Javier%20Andreu-Perez%0AAbstract%3A%20Fuzzy%20rule-based%20systems%20have%20been%20mostly%20used%20in%20interpretable%20decision-making%20because%20of%20their%20interpretable%20linguistic%20rules.%20However%2C%20interpretability%20requires%20both%20sensible%20linguistic%20partitions%20and%20small%20rule-base%20sizes%2C%20which%20are%20not%20guaranteed%20by%20many%20existing%20fuzzy%20rule-mining%20algorithms.%20Evolutionary%20approaches%20can%20produce%20high-quality%20models%20but%20suffer%20from%20prohibitive%20computational%20costs%2C%20while%20neural-based%20methods%20like%20ANFIS%20have%20problems%20retaining%20linguistic%20interpretations.%20In%20this%20work%2C%20we%20propose%20an%20adaptation%20of%20classical%20tree-based%20splitting%20algorithms%20from%20crisp%20rules%20to%20fuzzy%20trees%2C%20combining%20the%20computational%20efficiency%20of%20greedy%20algoritms%20with%20the%20interpretability%20advantages%20of%20fuzzy%20logic.%20This%20approach%20achieves%20interpretable%20linguistic%20partitions%20and%20substantially%20improves%20running%20time%20compared%20to%20evolutionary-based%20approaches%20while%20maintaining%20competitive%20predictive%20performance.%20Our%20experiments%20on%20tabular%20classification%20benchmarks%20proof%20that%20our%20method%20achieves%20comparable%20accuracy%20to%20state-of-the-art%20fuzzy%20classifiers%20with%20significantly%20lower%20computational%20cost%20and%20produces%20more%20interpretable%20rule%20bases%20with%20constrained%20complexity.%20Code%20is%20available%20in%3A%20https%3A//github.com/Fuminides/fuzzy_greedy_tree_public%0ALink%3A%20http%3A//arxiv.org/abs/2512.11616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Fast%2520Interpretable%2520Fuzzy%2520Tree%2520Learner%26entry.906535625%3DJavier%2520Fumanal-Idocin%2520and%2520Raquel%2520Fernandez-Peralta%2520and%2520Javier%2520Andreu-Perez%26entry.1292438233%3DFuzzy%2520rule-based%2520systems%2520have%2520been%2520mostly%2520used%2520in%2520interpretable%2520decision-making%2520because%2520of%2520their%2520interpretable%2520linguistic%2520rules.%2520However%252C%2520interpretability%2520requires%2520both%2520sensible%2520linguistic%2520partitions%2520and%2520small%2520rule-base%2520sizes%252C%2520which%2520are%2520not%2520guaranteed%2520by%2520many%2520existing%2520fuzzy%2520rule-mining%2520algorithms.%2520Evolutionary%2520approaches%2520can%2520produce%2520high-quality%2520models%2520but%2520suffer%2520from%2520prohibitive%2520computational%2520costs%252C%2520while%2520neural-based%2520methods%2520like%2520ANFIS%2520have%2520problems%2520retaining%2520linguistic%2520interpretations.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520adaptation%2520of%2520classical%2520tree-based%2520splitting%2520algorithms%2520from%2520crisp%2520rules%2520to%2520fuzzy%2520trees%252C%2520combining%2520the%2520computational%2520efficiency%2520of%2520greedy%2520algoritms%2520with%2520the%2520interpretability%2520advantages%2520of%2520fuzzy%2520logic.%2520This%2520approach%2520achieves%2520interpretable%2520linguistic%2520partitions%2520and%2520substantially%2520improves%2520running%2520time%2520compared%2520to%2520evolutionary-based%2520approaches%2520while%2520maintaining%2520competitive%2520predictive%2520performance.%2520Our%2520experiments%2520on%2520tabular%2520classification%2520benchmarks%2520proof%2520that%2520our%2520method%2520achieves%2520comparable%2520accuracy%2520to%2520state-of-the-art%2520fuzzy%2520classifiers%2520with%2520significantly%2520lower%2520computational%2520cost%2520and%2520produces%2520more%2520interpretable%2520rule%2520bases%2520with%2520constrained%2520complexity.%2520Code%2520is%2520available%2520in%253A%2520https%253A//github.com/Fuminides/fuzzy_greedy_tree_public%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fast%20Interpretable%20Fuzzy%20Tree%20Learner&entry.906535625=Javier%20Fumanal-Idocin%20and%20Raquel%20Fernandez-Peralta%20and%20Javier%20Andreu-Perez&entry.1292438233=Fuzzy%20rule-based%20systems%20have%20been%20mostly%20used%20in%20interpretable%20decision-making%20because%20of%20their%20interpretable%20linguistic%20rules.%20However%2C%20interpretability%20requires%20both%20sensible%20linguistic%20partitions%20and%20small%20rule-base%20sizes%2C%20which%20are%20not%20guaranteed%20by%20many%20existing%20fuzzy%20rule-mining%20algorithms.%20Evolutionary%20approaches%20can%20produce%20high-quality%20models%20but%20suffer%20from%20prohibitive%20computational%20costs%2C%20while%20neural-based%20methods%20like%20ANFIS%20have%20problems%20retaining%20linguistic%20interpretations.%20In%20this%20work%2C%20we%20propose%20an%20adaptation%20of%20classical%20tree-based%20splitting%20algorithms%20from%20crisp%20rules%20to%20fuzzy%20trees%2C%20combining%20the%20computational%20efficiency%20of%20greedy%20algoritms%20with%20the%20interpretability%20advantages%20of%20fuzzy%20logic.%20This%20approach%20achieves%20interpretable%20linguistic%20partitions%20and%20substantially%20improves%20running%20time%20compared%20to%20evolutionary-based%20approaches%20while%20maintaining%20competitive%20predictive%20performance.%20Our%20experiments%20on%20tabular%20classification%20benchmarks%20proof%20that%20our%20method%20achieves%20comparable%20accuracy%20to%20state-of-the-art%20fuzzy%20classifiers%20with%20significantly%20lower%20computational%20cost%20and%20produces%20more%20interpretable%20rule%20bases%20with%20constrained%20complexity.%20Code%20is%20available%20in%3A%20https%3A//github.com/Fuminides/fuzzy_greedy_tree_public&entry.1838667208=http%3A//arxiv.org/abs/2512.11616v1&entry.124074799=Read"},
{"title": "HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning", "author": "Yiqing Yang and Kin-Man Lam", "abstract": "Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.", "link": "http://arxiv.org/abs/2512.11534v1", "date": "2025-12-12", "relevancy": 2.1826, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HFS%3A%20Holistic%20Query-Aware%20Frame%20Selection%20for%20Efficient%20Video%20Reasoning&body=Title%3A%20HFS%3A%20Holistic%20Query-Aware%20Frame%20Selection%20for%20Efficient%20Video%20Reasoning%0AAuthor%3A%20Yiqing%20Yang%20and%20Kin-Man%20Lam%0AAbstract%3A%20Key%20frame%20selection%20in%20video%20understanding%20presents%20significant%20challenges.%20Traditional%20top-K%20selection%20methods%2C%20which%20score%20frames%20independently%2C%20often%20fail%20to%20optimize%20the%20selection%20as%20a%20whole.%20This%20independent%20scoring%20frequently%20results%20in%20selecting%20frames%20that%20are%20temporally%20clustered%20and%20visually%20redundant.%20Additionally%2C%20training%20lightweight%20selectors%20using%20pseudo%20labels%20generated%20offline%20by%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20prevents%20the%20supervisory%20signal%20from%20dynamically%20adapting%20to%20task%20objectives.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20end-to-end%20trainable%2C%20task-adaptive%20framework%20for%20frame%20selection.%20A%20Chain-of-Thought%20approach%20guides%20a%20Small%20Language%20Model%20%28SLM%29%20to%20generate%20task-specific%20implicit%20query%20vectors%2C%20which%20are%20combined%20with%20multimodal%20features%20to%20enable%20dynamic%20frame%20scoring.%20We%20further%20define%20a%20continuous%20set-level%20objective%20function%20that%20incorporates%20relevance%2C%20coverage%2C%20and%20redundancy%2C%20enabling%20differentiable%20optimization%20via%20Gumbel-Softmax%20to%20select%20optimal%20frame%20combinations%20at%20the%20set%20level.%20Finally%2C%20student-teacher%20mutual%20learning%20is%20employed%2C%20where%20the%20student%20selector%20%28SLM%29%20and%20teacher%20reasoner%20%28MLLM%29%20are%20trained%20to%20align%20their%20frame%20importance%20distributions%20via%20KL%20divergence.%20Combined%20with%20cross-entropy%20loss%2C%20this%20enables%20end-to-end%20optimization%2C%20eliminating%20reliance%20on%20static%20pseudo%20labels.%20Experiments%20across%20various%20benchmarks%2C%20including%20Video-MME%2C%20LongVideoBench%2C%20MLVU%2C%20and%20NExT-QA%2C%20demonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHFS%253A%2520Holistic%2520Query-Aware%2520Frame%2520Selection%2520for%2520Efficient%2520Video%2520Reasoning%26entry.906535625%3DYiqing%2520Yang%2520and%2520Kin-Man%2520Lam%26entry.1292438233%3DKey%2520frame%2520selection%2520in%2520video%2520understanding%2520presents%2520significant%2520challenges.%2520Traditional%2520top-K%2520selection%2520methods%252C%2520which%2520score%2520frames%2520independently%252C%2520often%2520fail%2520to%2520optimize%2520the%2520selection%2520as%2520a%2520whole.%2520This%2520independent%2520scoring%2520frequently%2520results%2520in%2520selecting%2520frames%2520that%2520are%2520temporally%2520clustered%2520and%2520visually%2520redundant.%2520Additionally%252C%2520training%2520lightweight%2520selectors%2520using%2520pseudo%2520labels%2520generated%2520offline%2520by%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520prevents%2520the%2520supervisory%2520signal%2520from%2520dynamically%2520adapting%2520to%2520task%2520objectives.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520an%2520end-to-end%2520trainable%252C%2520task-adaptive%2520framework%2520for%2520frame%2520selection.%2520A%2520Chain-of-Thought%2520approach%2520guides%2520a%2520Small%2520Language%2520Model%2520%2528SLM%2529%2520to%2520generate%2520task-specific%2520implicit%2520query%2520vectors%252C%2520which%2520are%2520combined%2520with%2520multimodal%2520features%2520to%2520enable%2520dynamic%2520frame%2520scoring.%2520We%2520further%2520define%2520a%2520continuous%2520set-level%2520objective%2520function%2520that%2520incorporates%2520relevance%252C%2520coverage%252C%2520and%2520redundancy%252C%2520enabling%2520differentiable%2520optimization%2520via%2520Gumbel-Softmax%2520to%2520select%2520optimal%2520frame%2520combinations%2520at%2520the%2520set%2520level.%2520Finally%252C%2520student-teacher%2520mutual%2520learning%2520is%2520employed%252C%2520where%2520the%2520student%2520selector%2520%2528SLM%2529%2520and%2520teacher%2520reasoner%2520%2528MLLM%2529%2520are%2520trained%2520to%2520align%2520their%2520frame%2520importance%2520distributions%2520via%2520KL%2520divergence.%2520Combined%2520with%2520cross-entropy%2520loss%252C%2520this%2520enables%2520end-to-end%2520optimization%252C%2520eliminating%2520reliance%2520on%2520static%2520pseudo%2520labels.%2520Experiments%2520across%2520various%2520benchmarks%252C%2520including%2520Video-MME%252C%2520LongVideoBench%252C%2520MLVU%252C%2520and%2520NExT-QA%252C%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520existing%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HFS%3A%20Holistic%20Query-Aware%20Frame%20Selection%20for%20Efficient%20Video%20Reasoning&entry.906535625=Yiqing%20Yang%20and%20Kin-Man%20Lam&entry.1292438233=Key%20frame%20selection%20in%20video%20understanding%20presents%20significant%20challenges.%20Traditional%20top-K%20selection%20methods%2C%20which%20score%20frames%20independently%2C%20often%20fail%20to%20optimize%20the%20selection%20as%20a%20whole.%20This%20independent%20scoring%20frequently%20results%20in%20selecting%20frames%20that%20are%20temporally%20clustered%20and%20visually%20redundant.%20Additionally%2C%20training%20lightweight%20selectors%20using%20pseudo%20labels%20generated%20offline%20by%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20prevents%20the%20supervisory%20signal%20from%20dynamically%20adapting%20to%20task%20objectives.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20end-to-end%20trainable%2C%20task-adaptive%20framework%20for%20frame%20selection.%20A%20Chain-of-Thought%20approach%20guides%20a%20Small%20Language%20Model%20%28SLM%29%20to%20generate%20task-specific%20implicit%20query%20vectors%2C%20which%20are%20combined%20with%20multimodal%20features%20to%20enable%20dynamic%20frame%20scoring.%20We%20further%20define%20a%20continuous%20set-level%20objective%20function%20that%20incorporates%20relevance%2C%20coverage%2C%20and%20redundancy%2C%20enabling%20differentiable%20optimization%20via%20Gumbel-Softmax%20to%20select%20optimal%20frame%20combinations%20at%20the%20set%20level.%20Finally%2C%20student-teacher%20mutual%20learning%20is%20employed%2C%20where%20the%20student%20selector%20%28SLM%29%20and%20teacher%20reasoner%20%28MLLM%29%20are%20trained%20to%20align%20their%20frame%20importance%20distributions%20via%20KL%20divergence.%20Combined%20with%20cross-entropy%20loss%2C%20this%20enables%20end-to-end%20optimization%2C%20eliminating%20reliance%20on%20static%20pseudo%20labels.%20Experiments%20across%20various%20benchmarks%2C%20including%20Video-MME%2C%20LongVideoBench%2C%20MLVU%2C%20and%20NExT-QA%2C%20demonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2512.11534v1&entry.124074799=Read"},
{"title": "MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings", "author": "Victor Rambaud and Salvador Mascarenhas and Yair Lakretz", "abstract": "A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.", "link": "http://arxiv.org/abs/2511.19279v2", "date": "2025-12-12", "relevancy": 2.1796, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5412}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapFormer%3A%20Self-Supervised%20Learning%20of%20Cognitive%20Maps%20with%20Input-Dependent%20Positional%20Embeddings&body=Title%3A%20MapFormer%3A%20Self-Supervised%20Learning%20of%20Cognitive%20Maps%20with%20Input-Dependent%20Positional%20Embeddings%0AAuthor%3A%20Victor%20Rambaud%20and%20Salvador%20Mascarenhas%20and%20Yair%20Lakretz%0AAbstract%3A%20A%20cognitive%20map%20is%20an%20internal%20model%20which%20encodes%20the%20abstract%20relationships%20among%20entities%20in%20the%20world%2C%20giving%20humans%20and%20animals%20the%20flexibility%20to%20adapt%20to%20new%20situations%2C%20with%20a%20strong%20out-of-distribution%20%28OOD%29%20generalization%20that%20current%20AI%20systems%20still%20do%20not%20possess.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MapFormers%2C%20new%20architectures%20based%20on%20Transformer%20models%2C%20which%20can%20learn%20cognitive%20maps%20from%20observational%20data%20and%20perform%20path%20integration%20in%20parallel%2C%20in%20a%20self-supervised%20manner.%20Cognitive%20maps%20are%20learned%20in%20the%20model%20by%20disentangling%20structural%20relationships%20in%20the%20inputs%20from%20their%20specific%20content%2C%20a%20property%20that%20can%20be%20achieved%20naturally%20by%20updating%20the%20positional%20encoding%20in%20Transformers%20with%20input-dependent%20matrices.%20We%20developed%20two%20variants%20of%20MapFormers%20that%20unify%20absolute%20and%20relative%20positional%20encoding%20to%20model%20episodic%20%28EM%29%20and%20working%20memory%20%28WM%29%2C%20respectively.%20We%20tested%20MapFormers%20on%20several%20tasks%2C%20including%20a%20classic%202D%20navigation%20task%2C%20showing%20that%20our%20models%20can%20learn%20a%20cognitive%20map%20of%20the%20underlying%20space%20and%20generalize%20OOD%20%28e.g.%2C%20to%20longer%20sequences%29%20with%20near-perfect%20performance%2C%20unlike%20current%20architectures.%20Together%2C%20these%20results%20demonstrate%20the%20superiority%20of%20models%20designed%20to%20learn%20a%20cognitive%20map%2C%20and%20the%20importance%20of%20introducing%20a%20structural%20bias%20for%20structure-content%20disentanglement%2C%20which%20can%20be%20achieved%20in%20Transformers%20with%20input-dependent%20positional%20encoding.%20MapFormers%20have%20broad%20applications%20in%20both%20neuroscience%20and%20AI%2C%20by%20explaining%20the%20neural%20mechanisms%20giving%20rise%20to%20cognitive%20maps%2C%20while%20allowing%20these%20relation%20models%20to%20be%20learned%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapFormer%253A%2520Self-Supervised%2520Learning%2520of%2520Cognitive%2520Maps%2520with%2520Input-Dependent%2520Positional%2520Embeddings%26entry.906535625%3DVictor%2520Rambaud%2520and%2520Salvador%2520Mascarenhas%2520and%2520Yair%2520Lakretz%26entry.1292438233%3DA%2520cognitive%2520map%2520is%2520an%2520internal%2520model%2520which%2520encodes%2520the%2520abstract%2520relationships%2520among%2520entities%2520in%2520the%2520world%252C%2520giving%2520humans%2520and%2520animals%2520the%2520flexibility%2520to%2520adapt%2520to%2520new%2520situations%252C%2520with%2520a%2520strong%2520out-of-distribution%2520%2528OOD%2529%2520generalization%2520that%2520current%2520AI%2520systems%2520still%2520do%2520not%2520possess.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520MapFormers%252C%2520new%2520architectures%2520based%2520on%2520Transformer%2520models%252C%2520which%2520can%2520learn%2520cognitive%2520maps%2520from%2520observational%2520data%2520and%2520perform%2520path%2520integration%2520in%2520parallel%252C%2520in%2520a%2520self-supervised%2520manner.%2520Cognitive%2520maps%2520are%2520learned%2520in%2520the%2520model%2520by%2520disentangling%2520structural%2520relationships%2520in%2520the%2520inputs%2520from%2520their%2520specific%2520content%252C%2520a%2520property%2520that%2520can%2520be%2520achieved%2520naturally%2520by%2520updating%2520the%2520positional%2520encoding%2520in%2520Transformers%2520with%2520input-dependent%2520matrices.%2520We%2520developed%2520two%2520variants%2520of%2520MapFormers%2520that%2520unify%2520absolute%2520and%2520relative%2520positional%2520encoding%2520to%2520model%2520episodic%2520%2528EM%2529%2520and%2520working%2520memory%2520%2528WM%2529%252C%2520respectively.%2520We%2520tested%2520MapFormers%2520on%2520several%2520tasks%252C%2520including%2520a%2520classic%25202D%2520navigation%2520task%252C%2520showing%2520that%2520our%2520models%2520can%2520learn%2520a%2520cognitive%2520map%2520of%2520the%2520underlying%2520space%2520and%2520generalize%2520OOD%2520%2528e.g.%252C%2520to%2520longer%2520sequences%2529%2520with%2520near-perfect%2520performance%252C%2520unlike%2520current%2520architectures.%2520Together%252C%2520these%2520results%2520demonstrate%2520the%2520superiority%2520of%2520models%2520designed%2520to%2520learn%2520a%2520cognitive%2520map%252C%2520and%2520the%2520importance%2520of%2520introducing%2520a%2520structural%2520bias%2520for%2520structure-content%2520disentanglement%252C%2520which%2520can%2520be%2520achieved%2520in%2520Transformers%2520with%2520input-dependent%2520positional%2520encoding.%2520MapFormers%2520have%2520broad%2520applications%2520in%2520both%2520neuroscience%2520and%2520AI%252C%2520by%2520explaining%2520the%2520neural%2520mechanisms%2520giving%2520rise%2520to%2520cognitive%2520maps%252C%2520while%2520allowing%2520these%2520relation%2520models%2520to%2520be%2520learned%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapFormer%3A%20Self-Supervised%20Learning%20of%20Cognitive%20Maps%20with%20Input-Dependent%20Positional%20Embeddings&entry.906535625=Victor%20Rambaud%20and%20Salvador%20Mascarenhas%20and%20Yair%20Lakretz&entry.1292438233=A%20cognitive%20map%20is%20an%20internal%20model%20which%20encodes%20the%20abstract%20relationships%20among%20entities%20in%20the%20world%2C%20giving%20humans%20and%20animals%20the%20flexibility%20to%20adapt%20to%20new%20situations%2C%20with%20a%20strong%20out-of-distribution%20%28OOD%29%20generalization%20that%20current%20AI%20systems%20still%20do%20not%20possess.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MapFormers%2C%20new%20architectures%20based%20on%20Transformer%20models%2C%20which%20can%20learn%20cognitive%20maps%20from%20observational%20data%20and%20perform%20path%20integration%20in%20parallel%2C%20in%20a%20self-supervised%20manner.%20Cognitive%20maps%20are%20learned%20in%20the%20model%20by%20disentangling%20structural%20relationships%20in%20the%20inputs%20from%20their%20specific%20content%2C%20a%20property%20that%20can%20be%20achieved%20naturally%20by%20updating%20the%20positional%20encoding%20in%20Transformers%20with%20input-dependent%20matrices.%20We%20developed%20two%20variants%20of%20MapFormers%20that%20unify%20absolute%20and%20relative%20positional%20encoding%20to%20model%20episodic%20%28EM%29%20and%20working%20memory%20%28WM%29%2C%20respectively.%20We%20tested%20MapFormers%20on%20several%20tasks%2C%20including%20a%20classic%202D%20navigation%20task%2C%20showing%20that%20our%20models%20can%20learn%20a%20cognitive%20map%20of%20the%20underlying%20space%20and%20generalize%20OOD%20%28e.g.%2C%20to%20longer%20sequences%29%20with%20near-perfect%20performance%2C%20unlike%20current%20architectures.%20Together%2C%20these%20results%20demonstrate%20the%20superiority%20of%20models%20designed%20to%20learn%20a%20cognitive%20map%2C%20and%20the%20importance%20of%20introducing%20a%20structural%20bias%20for%20structure-content%20disentanglement%2C%20which%20can%20be%20achieved%20in%20Transformers%20with%20input-dependent%20positional%20encoding.%20MapFormers%20have%20broad%20applications%20in%20both%20neuroscience%20and%20AI%2C%20by%20explaining%20the%20neural%20mechanisms%20giving%20rise%20to%20cognitive%20maps%2C%20while%20allowing%20these%20relation%20models%20to%20be%20learned%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2511.19279v2&entry.124074799=Read"},
{"title": "Introducing physics-informed generative models for targeting structural novelty in the exploration of chemical space", "author": "Andrij Vasylenko and Federico Ottomano and Christopher M. Collins and Rahul Savani and Matthew S. Dyer and Matthew J. Rosseinsky", "abstract": "Discovering materials with new structural chemistry is key to achieving transformative functionality. Generative artificial intelligence offers a scalable route to propose candidate crystal structures. We introduce a reliable low-cost proxy for structural novelty as a conditioning property to steer generation towards novel yet physically plausible structures. We then develop a physics-informed diffusion model that embeds this descriptor of local environment diversity together with compactness as a stability metric to balance physical plausibility with structural novelty. Conditioning on these metrics improves generative performance across diffusion models, shifting generation away from structural motifs that dominate the training data. A chemically grounded validation protocol isolates those candidates that combine plausibility with structural novelty for physics-based calculation of energetic stability. Both the stability and the novelty of candidates emerging from this workflow can however change when the full potential energy surface at a candidate composition is evaluated with crystal structure prediction (CSP). This suggests a practical generative-CSP synergy for discovery-oriented exploration, where AI targets physically viable yet structurally distinct regions of chemical space for detailed physics-based assessment of novelty and stability.", "link": "http://arxiv.org/abs/2510.23181v2", "date": "2025-12-12", "relevancy": 2.1687, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.555}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5536}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20physics-informed%20generative%20models%20for%20targeting%20structural%20novelty%20in%20the%20exploration%20of%20chemical%20space&body=Title%3A%20Introducing%20physics-informed%20generative%20models%20for%20targeting%20structural%20novelty%20in%20the%20exploration%20of%20chemical%20space%0AAuthor%3A%20Andrij%20Vasylenko%20and%20Federico%20Ottomano%20and%20Christopher%20M.%20Collins%20and%20Rahul%20Savani%20and%20Matthew%20S.%20Dyer%20and%20Matthew%20J.%20Rosseinsky%0AAbstract%3A%20Discovering%20materials%20with%20new%20structural%20chemistry%20is%20key%20to%20achieving%20transformative%20functionality.%20Generative%20artificial%20intelligence%20offers%20a%20scalable%20route%20to%20propose%20candidate%20crystal%20structures.%20We%20introduce%20a%20reliable%20low-cost%20proxy%20for%20structural%20novelty%20as%20a%20conditioning%20property%20to%20steer%20generation%20towards%20novel%20yet%20physically%20plausible%20structures.%20We%20then%20develop%20a%20physics-informed%20diffusion%20model%20that%20embeds%20this%20descriptor%20of%20local%20environment%20diversity%20together%20with%20compactness%20as%20a%20stability%20metric%20to%20balance%20physical%20plausibility%20with%20structural%20novelty.%20Conditioning%20on%20these%20metrics%20improves%20generative%20performance%20across%20diffusion%20models%2C%20shifting%20generation%20away%20from%20structural%20motifs%20that%20dominate%20the%20training%20data.%20A%20chemically%20grounded%20validation%20protocol%20isolates%20those%20candidates%20that%20combine%20plausibility%20with%20structural%20novelty%20for%20physics-based%20calculation%20of%20energetic%20stability.%20Both%20the%20stability%20and%20the%20novelty%20of%20candidates%20emerging%20from%20this%20workflow%20can%20however%20change%20when%20the%20full%20potential%20energy%20surface%20at%20a%20candidate%20composition%20is%20evaluated%20with%20crystal%20structure%20prediction%20%28CSP%29.%20This%20suggests%20a%20practical%20generative-CSP%20synergy%20for%20discovery-oriented%20exploration%2C%20where%20AI%20targets%20physically%20viable%20yet%20structurally%20distinct%20regions%20of%20chemical%20space%20for%20detailed%20physics-based%20assessment%20of%20novelty%20and%20stability.%0ALink%3A%20http%3A//arxiv.org/abs/2510.23181v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520physics-informed%2520generative%2520models%2520for%2520targeting%2520structural%2520novelty%2520in%2520the%2520exploration%2520of%2520chemical%2520space%26entry.906535625%3DAndrij%2520Vasylenko%2520and%2520Federico%2520Ottomano%2520and%2520Christopher%2520M.%2520Collins%2520and%2520Rahul%2520Savani%2520and%2520Matthew%2520S.%2520Dyer%2520and%2520Matthew%2520J.%2520Rosseinsky%26entry.1292438233%3DDiscovering%2520materials%2520with%2520new%2520structural%2520chemistry%2520is%2520key%2520to%2520achieving%2520transformative%2520functionality.%2520Generative%2520artificial%2520intelligence%2520offers%2520a%2520scalable%2520route%2520to%2520propose%2520candidate%2520crystal%2520structures.%2520We%2520introduce%2520a%2520reliable%2520low-cost%2520proxy%2520for%2520structural%2520novelty%2520as%2520a%2520conditioning%2520property%2520to%2520steer%2520generation%2520towards%2520novel%2520yet%2520physically%2520plausible%2520structures.%2520We%2520then%2520develop%2520a%2520physics-informed%2520diffusion%2520model%2520that%2520embeds%2520this%2520descriptor%2520of%2520local%2520environment%2520diversity%2520together%2520with%2520compactness%2520as%2520a%2520stability%2520metric%2520to%2520balance%2520physical%2520plausibility%2520with%2520structural%2520novelty.%2520Conditioning%2520on%2520these%2520metrics%2520improves%2520generative%2520performance%2520across%2520diffusion%2520models%252C%2520shifting%2520generation%2520away%2520from%2520structural%2520motifs%2520that%2520dominate%2520the%2520training%2520data.%2520A%2520chemically%2520grounded%2520validation%2520protocol%2520isolates%2520those%2520candidates%2520that%2520combine%2520plausibility%2520with%2520structural%2520novelty%2520for%2520physics-based%2520calculation%2520of%2520energetic%2520stability.%2520Both%2520the%2520stability%2520and%2520the%2520novelty%2520of%2520candidates%2520emerging%2520from%2520this%2520workflow%2520can%2520however%2520change%2520when%2520the%2520full%2520potential%2520energy%2520surface%2520at%2520a%2520candidate%2520composition%2520is%2520evaluated%2520with%2520crystal%2520structure%2520prediction%2520%2528CSP%2529.%2520This%2520suggests%2520a%2520practical%2520generative-CSP%2520synergy%2520for%2520discovery-oriented%2520exploration%252C%2520where%2520AI%2520targets%2520physically%2520viable%2520yet%2520structurally%2520distinct%2520regions%2520of%2520chemical%2520space%2520for%2520detailed%2520physics-based%2520assessment%2520of%2520novelty%2520and%2520stability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23181v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20physics-informed%20generative%20models%20for%20targeting%20structural%20novelty%20in%20the%20exploration%20of%20chemical%20space&entry.906535625=Andrij%20Vasylenko%20and%20Federico%20Ottomano%20and%20Christopher%20M.%20Collins%20and%20Rahul%20Savani%20and%20Matthew%20S.%20Dyer%20and%20Matthew%20J.%20Rosseinsky&entry.1292438233=Discovering%20materials%20with%20new%20structural%20chemistry%20is%20key%20to%20achieving%20transformative%20functionality.%20Generative%20artificial%20intelligence%20offers%20a%20scalable%20route%20to%20propose%20candidate%20crystal%20structures.%20We%20introduce%20a%20reliable%20low-cost%20proxy%20for%20structural%20novelty%20as%20a%20conditioning%20property%20to%20steer%20generation%20towards%20novel%20yet%20physically%20plausible%20structures.%20We%20then%20develop%20a%20physics-informed%20diffusion%20model%20that%20embeds%20this%20descriptor%20of%20local%20environment%20diversity%20together%20with%20compactness%20as%20a%20stability%20metric%20to%20balance%20physical%20plausibility%20with%20structural%20novelty.%20Conditioning%20on%20these%20metrics%20improves%20generative%20performance%20across%20diffusion%20models%2C%20shifting%20generation%20away%20from%20structural%20motifs%20that%20dominate%20the%20training%20data.%20A%20chemically%20grounded%20validation%20protocol%20isolates%20those%20candidates%20that%20combine%20plausibility%20with%20structural%20novelty%20for%20physics-based%20calculation%20of%20energetic%20stability.%20Both%20the%20stability%20and%20the%20novelty%20of%20candidates%20emerging%20from%20this%20workflow%20can%20however%20change%20when%20the%20full%20potential%20energy%20surface%20at%20a%20candidate%20composition%20is%20evaluated%20with%20crystal%20structure%20prediction%20%28CSP%29.%20This%20suggests%20a%20practical%20generative-CSP%20synergy%20for%20discovery-oriented%20exploration%2C%20where%20AI%20targets%20physically%20viable%20yet%20structurally%20distinct%20regions%20of%20chemical%20space%20for%20detailed%20physics-based%20assessment%20of%20novelty%20and%20stability.&entry.1838667208=http%3A//arxiv.org/abs/2510.23181v2&entry.124074799=Read"},
{"title": "MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression", "author": "Hai Dang Nguyen and Nguyen Dang Huy Pham and The Minh Duc Nguyen and Dac Thai Nguyen and Hang Thi Nguyen and Duong M. Nguyen", "abstract": "Spatial Transcriptomics (ST) enables the measurement of gene expression while preserving spatial information, offering critical insights into tissue architecture and disease pathology. Recent developments have explored the use of hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict transcriptome-wide gene expression profiles through deep neural networks. This task is commonly framed as a regression problem, where each input corresponds to a localized image patch extracted from the WSI. However, predicting spatial gene expression from histological images remains a challenging problem due to the significant modality gap between visual features and molecular signals. Recent studies have attempted to incorporate both local and global information into predictive models. Nevertheless, existing methods still suffer from two key limitations: (1) insufficient granularity in local feature extraction, and (2) inadequate coverage of global spatial context. In this work, we propose a novel framework, MMAP (Multi-MAgnification and Prototype-enhanced architecture), that addresses both challenges simultaneously. To enhance local feature granularity, MMAP leverages multi-magnification patch representations that capture fine-grained histological details. To improve global contextual understanding, it learns a set of latent prototype embeddings that serve as compact representations of slide-level information. Extensive experimental results demonstrate that MMAP consistently outperforms all existing state-of-the-art methods across multiple evaluation metrics, including Mean Absolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation Coefficient (PCC).", "link": "http://arxiv.org/abs/2510.11344v2", "date": "2025-12-12", "relevancy": 2.1626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5567}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5307}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMAP%3A%20A%20Multi-Magnification%20and%20Prototype-Aware%20Architecture%20for%20Predicting%20Spatial%20Gene%20Expression&body=Title%3A%20MMAP%3A%20A%20Multi-Magnification%20and%20Prototype-Aware%20Architecture%20for%20Predicting%20Spatial%20Gene%20Expression%0AAuthor%3A%20Hai%20Dang%20Nguyen%20and%20Nguyen%20Dang%20Huy%20Pham%20and%20The%20Minh%20Duc%20Nguyen%20and%20Dac%20Thai%20Nguyen%20and%20Hang%20Thi%20Nguyen%20and%20Duong%20M.%20Nguyen%0AAbstract%3A%20Spatial%20Transcriptomics%20%28ST%29%20enables%20the%20measurement%20of%20gene%20expression%20while%20preserving%20spatial%20information%2C%20offering%20critical%20insights%20into%20tissue%20architecture%20and%20disease%20pathology.%20Recent%20developments%20have%20explored%20the%20use%20of%20hematoxylin%20and%20eosin%20%28H%26E%29-stained%20whole-slide%20images%20%28WSIs%29%20to%20predict%20transcriptome-wide%20gene%20expression%20profiles%20through%20deep%20neural%20networks.%20This%20task%20is%20commonly%20framed%20as%20a%20regression%20problem%2C%20where%20each%20input%20corresponds%20to%20a%20localized%20image%20patch%20extracted%20from%20the%20WSI.%20However%2C%20predicting%20spatial%20gene%20expression%20from%20histological%20images%20remains%20a%20challenging%20problem%20due%20to%20the%20significant%20modality%20gap%20between%20visual%20features%20and%20molecular%20signals.%20Recent%20studies%20have%20attempted%20to%20incorporate%20both%20local%20and%20global%20information%20into%20predictive%20models.%20Nevertheless%2C%20existing%20methods%20still%20suffer%20from%20two%20key%20limitations%3A%20%281%29%20insufficient%20granularity%20in%20local%20feature%20extraction%2C%20and%20%282%29%20inadequate%20coverage%20of%20global%20spatial%20context.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%2C%20MMAP%20%28Multi-MAgnification%20and%20Prototype-enhanced%20architecture%29%2C%20that%20addresses%20both%20challenges%20simultaneously.%20To%20enhance%20local%20feature%20granularity%2C%20MMAP%20leverages%20multi-magnification%20patch%20representations%20that%20capture%20fine-grained%20histological%20details.%20To%20improve%20global%20contextual%20understanding%2C%20it%20learns%20a%20set%20of%20latent%20prototype%20embeddings%20that%20serve%20as%20compact%20representations%20of%20slide-level%20information.%20Extensive%20experimental%20results%20demonstrate%20that%20MMAP%20consistently%20outperforms%20all%20existing%20state-of-the-art%20methods%20across%20multiple%20evaluation%20metrics%2C%20including%20Mean%20Absolute%20Error%20%28MAE%29%2C%20Mean%20Squared%20Error%20%28MSE%29%2C%20and%20Pearson%20Correlation%20Coefficient%20%28PCC%29.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMAP%253A%2520A%2520Multi-Magnification%2520and%2520Prototype-Aware%2520Architecture%2520for%2520Predicting%2520Spatial%2520Gene%2520Expression%26entry.906535625%3DHai%2520Dang%2520Nguyen%2520and%2520Nguyen%2520Dang%2520Huy%2520Pham%2520and%2520The%2520Minh%2520Duc%2520Nguyen%2520and%2520Dac%2520Thai%2520Nguyen%2520and%2520Hang%2520Thi%2520Nguyen%2520and%2520Duong%2520M.%2520Nguyen%26entry.1292438233%3DSpatial%2520Transcriptomics%2520%2528ST%2529%2520enables%2520the%2520measurement%2520of%2520gene%2520expression%2520while%2520preserving%2520spatial%2520information%252C%2520offering%2520critical%2520insights%2520into%2520tissue%2520architecture%2520and%2520disease%2520pathology.%2520Recent%2520developments%2520have%2520explored%2520the%2520use%2520of%2520hematoxylin%2520and%2520eosin%2520%2528H%2526E%2529-stained%2520whole-slide%2520images%2520%2528WSIs%2529%2520to%2520predict%2520transcriptome-wide%2520gene%2520expression%2520profiles%2520through%2520deep%2520neural%2520networks.%2520This%2520task%2520is%2520commonly%2520framed%2520as%2520a%2520regression%2520problem%252C%2520where%2520each%2520input%2520corresponds%2520to%2520a%2520localized%2520image%2520patch%2520extracted%2520from%2520the%2520WSI.%2520However%252C%2520predicting%2520spatial%2520gene%2520expression%2520from%2520histological%2520images%2520remains%2520a%2520challenging%2520problem%2520due%2520to%2520the%2520significant%2520modality%2520gap%2520between%2520visual%2520features%2520and%2520molecular%2520signals.%2520Recent%2520studies%2520have%2520attempted%2520to%2520incorporate%2520both%2520local%2520and%2520global%2520information%2520into%2520predictive%2520models.%2520Nevertheless%252C%2520existing%2520methods%2520still%2520suffer%2520from%2520two%2520key%2520limitations%253A%2520%25281%2529%2520insufficient%2520granularity%2520in%2520local%2520feature%2520extraction%252C%2520and%2520%25282%2529%2520inadequate%2520coverage%2520of%2520global%2520spatial%2520context.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520MMAP%2520%2528Multi-MAgnification%2520and%2520Prototype-enhanced%2520architecture%2529%252C%2520that%2520addresses%2520both%2520challenges%2520simultaneously.%2520To%2520enhance%2520local%2520feature%2520granularity%252C%2520MMAP%2520leverages%2520multi-magnification%2520patch%2520representations%2520that%2520capture%2520fine-grained%2520histological%2520details.%2520To%2520improve%2520global%2520contextual%2520understanding%252C%2520it%2520learns%2520a%2520set%2520of%2520latent%2520prototype%2520embeddings%2520that%2520serve%2520as%2520compact%2520representations%2520of%2520slide-level%2520information.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520MMAP%2520consistently%2520outperforms%2520all%2520existing%2520state-of-the-art%2520methods%2520across%2520multiple%2520evaluation%2520metrics%252C%2520including%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%252C%2520Mean%2520Squared%2520Error%2520%2528MSE%2529%252C%2520and%2520Pearson%2520Correlation%2520Coefficient%2520%2528PCC%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMAP%3A%20A%20Multi-Magnification%20and%20Prototype-Aware%20Architecture%20for%20Predicting%20Spatial%20Gene%20Expression&entry.906535625=Hai%20Dang%20Nguyen%20and%20Nguyen%20Dang%20Huy%20Pham%20and%20The%20Minh%20Duc%20Nguyen%20and%20Dac%20Thai%20Nguyen%20and%20Hang%20Thi%20Nguyen%20and%20Duong%20M.%20Nguyen&entry.1292438233=Spatial%20Transcriptomics%20%28ST%29%20enables%20the%20measurement%20of%20gene%20expression%20while%20preserving%20spatial%20information%2C%20offering%20critical%20insights%20into%20tissue%20architecture%20and%20disease%20pathology.%20Recent%20developments%20have%20explored%20the%20use%20of%20hematoxylin%20and%20eosin%20%28H%26E%29-stained%20whole-slide%20images%20%28WSIs%29%20to%20predict%20transcriptome-wide%20gene%20expression%20profiles%20through%20deep%20neural%20networks.%20This%20task%20is%20commonly%20framed%20as%20a%20regression%20problem%2C%20where%20each%20input%20corresponds%20to%20a%20localized%20image%20patch%20extracted%20from%20the%20WSI.%20However%2C%20predicting%20spatial%20gene%20expression%20from%20histological%20images%20remains%20a%20challenging%20problem%20due%20to%20the%20significant%20modality%20gap%20between%20visual%20features%20and%20molecular%20signals.%20Recent%20studies%20have%20attempted%20to%20incorporate%20both%20local%20and%20global%20information%20into%20predictive%20models.%20Nevertheless%2C%20existing%20methods%20still%20suffer%20from%20two%20key%20limitations%3A%20%281%29%20insufficient%20granularity%20in%20local%20feature%20extraction%2C%20and%20%282%29%20inadequate%20coverage%20of%20global%20spatial%20context.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%2C%20MMAP%20%28Multi-MAgnification%20and%20Prototype-enhanced%20architecture%29%2C%20that%20addresses%20both%20challenges%20simultaneously.%20To%20enhance%20local%20feature%20granularity%2C%20MMAP%20leverages%20multi-magnification%20patch%20representations%20that%20capture%20fine-grained%20histological%20details.%20To%20improve%20global%20contextual%20understanding%2C%20it%20learns%20a%20set%20of%20latent%20prototype%20embeddings%20that%20serve%20as%20compact%20representations%20of%20slide-level%20information.%20Extensive%20experimental%20results%20demonstrate%20that%20MMAP%20consistently%20outperforms%20all%20existing%20state-of-the-art%20methods%20across%20multiple%20evaluation%20metrics%2C%20including%20Mean%20Absolute%20Error%20%28MAE%29%2C%20Mean%20Squared%20Error%20%28MSE%29%2C%20and%20Pearson%20Correlation%20Coefficient%20%28PCC%29.&entry.1838667208=http%3A//arxiv.org/abs/2510.11344v2&entry.124074799=Read"},
{"title": "CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios", "author": "J\u00f6rg Gamerdinger and Sven Teufel and Simon Roller and Oliver Bringmann", "abstract": "The growing number of road users has significantly increased the risk of accidents in recent years. Vulnerable Road Users (VRUs) are particularly at risk, especially in urban environments where they are often occluded by parked vehicles or buildings. Autonomous Driving (AD) and Collective Perception (CP) are promising solutions to mitigate these risks. In particular, infrastructure-assisted CP, where sensor units are mounted on infrastructure elements such as traffic lights or lamp posts, can help overcome perceptual limitations by providing enhanced points of view, which significantly reduces occlusions. To encourage decision makers to adopt this technology, comprehensive studies and datasets demonstrating safety improvements for VRUs are essential. In this paper, we propose a framework for evaluating the safety improvement by infrastructure-based CP specifically targeted at VRUs including a dataset with safety-critical EuroNCAP scenarios (CarlaNCAP) with 11k frames. Using this dataset, we conduct an in-depth simulation study and demonstrate that infrastructure-assisted CP can significantly reduce accident rates in safety-critical scenarios, achieving up to 100% accident avoidance compared to a vehicle equipped with sensors with only 33%. Code is available at https://github.com/ekut-es/carla_ncap", "link": "http://arxiv.org/abs/2512.11551v1", "date": "2025-12-12", "relevancy": 2.16, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5473}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5472}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CarlaNCAP%3A%20A%20Framework%20for%20Quantifying%20the%20Safety%20of%20Vulnerable%20Road%20Users%20in%20Infrastructure-Assisted%20Collective%20Perception%20Using%20EuroNCAP%20Scenarios&body=Title%3A%20CarlaNCAP%3A%20A%20Framework%20for%20Quantifying%20the%20Safety%20of%20Vulnerable%20Road%20Users%20in%20Infrastructure-Assisted%20Collective%20Perception%20Using%20EuroNCAP%20Scenarios%0AAuthor%3A%20J%C3%B6rg%20Gamerdinger%20and%20Sven%20Teufel%20and%20Simon%20Roller%20and%20Oliver%20Bringmann%0AAbstract%3A%20The%20growing%20number%20of%20road%20users%20has%20significantly%20increased%20the%20risk%20of%20accidents%20in%20recent%20years.%20Vulnerable%20Road%20Users%20%28VRUs%29%20are%20particularly%20at%20risk%2C%20especially%20in%20urban%20environments%20where%20they%20are%20often%20occluded%20by%20parked%20vehicles%20or%20buildings.%20Autonomous%20Driving%20%28AD%29%20and%20Collective%20Perception%20%28CP%29%20are%20promising%20solutions%20to%20mitigate%20these%20risks.%20In%20particular%2C%20infrastructure-assisted%20CP%2C%20where%20sensor%20units%20are%20mounted%20on%20infrastructure%20elements%20such%20as%20traffic%20lights%20or%20lamp%20posts%2C%20can%20help%20overcome%20perceptual%20limitations%20by%20providing%20enhanced%20points%20of%20view%2C%20which%20significantly%20reduces%20occlusions.%20To%20encourage%20decision%20makers%20to%20adopt%20this%20technology%2C%20comprehensive%20studies%20and%20datasets%20demonstrating%20safety%20improvements%20for%20VRUs%20are%20essential.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20for%20evaluating%20the%20safety%20improvement%20by%20infrastructure-based%20CP%20specifically%20targeted%20at%20VRUs%20including%20a%20dataset%20with%20safety-critical%20EuroNCAP%20scenarios%20%28CarlaNCAP%29%20with%2011k%20frames.%20Using%20this%20dataset%2C%20we%20conduct%20an%20in-depth%20simulation%20study%20and%20demonstrate%20that%20infrastructure-assisted%20CP%20can%20significantly%20reduce%20accident%20rates%20in%20safety-critical%20scenarios%2C%20achieving%20up%20to%20100%25%20accident%20avoidance%20compared%20to%20a%20vehicle%20equipped%20with%20sensors%20with%20only%2033%25.%20Code%20is%20available%20at%20https%3A//github.com/ekut-es/carla_ncap%0ALink%3A%20http%3A//arxiv.org/abs/2512.11551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCarlaNCAP%253A%2520A%2520Framework%2520for%2520Quantifying%2520the%2520Safety%2520of%2520Vulnerable%2520Road%2520Users%2520in%2520Infrastructure-Assisted%2520Collective%2520Perception%2520Using%2520EuroNCAP%2520Scenarios%26entry.906535625%3DJ%25C3%25B6rg%2520Gamerdinger%2520and%2520Sven%2520Teufel%2520and%2520Simon%2520Roller%2520and%2520Oliver%2520Bringmann%26entry.1292438233%3DThe%2520growing%2520number%2520of%2520road%2520users%2520has%2520significantly%2520increased%2520the%2520risk%2520of%2520accidents%2520in%2520recent%2520years.%2520Vulnerable%2520Road%2520Users%2520%2528VRUs%2529%2520are%2520particularly%2520at%2520risk%252C%2520especially%2520in%2520urban%2520environments%2520where%2520they%2520are%2520often%2520occluded%2520by%2520parked%2520vehicles%2520or%2520buildings.%2520Autonomous%2520Driving%2520%2528AD%2529%2520and%2520Collective%2520Perception%2520%2528CP%2529%2520are%2520promising%2520solutions%2520to%2520mitigate%2520these%2520risks.%2520In%2520particular%252C%2520infrastructure-assisted%2520CP%252C%2520where%2520sensor%2520units%2520are%2520mounted%2520on%2520infrastructure%2520elements%2520such%2520as%2520traffic%2520lights%2520or%2520lamp%2520posts%252C%2520can%2520help%2520overcome%2520perceptual%2520limitations%2520by%2520providing%2520enhanced%2520points%2520of%2520view%252C%2520which%2520significantly%2520reduces%2520occlusions.%2520To%2520encourage%2520decision%2520makers%2520to%2520adopt%2520this%2520technology%252C%2520comprehensive%2520studies%2520and%2520datasets%2520demonstrating%2520safety%2520improvements%2520for%2520VRUs%2520are%2520essential.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520framework%2520for%2520evaluating%2520the%2520safety%2520improvement%2520by%2520infrastructure-based%2520CP%2520specifically%2520targeted%2520at%2520VRUs%2520including%2520a%2520dataset%2520with%2520safety-critical%2520EuroNCAP%2520scenarios%2520%2528CarlaNCAP%2529%2520with%252011k%2520frames.%2520Using%2520this%2520dataset%252C%2520we%2520conduct%2520an%2520in-depth%2520simulation%2520study%2520and%2520demonstrate%2520that%2520infrastructure-assisted%2520CP%2520can%2520significantly%2520reduce%2520accident%2520rates%2520in%2520safety-critical%2520scenarios%252C%2520achieving%2520up%2520to%2520100%2525%2520accident%2520avoidance%2520compared%2520to%2520a%2520vehicle%2520equipped%2520with%2520sensors%2520with%2520only%252033%2525.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/ekut-es/carla_ncap%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CarlaNCAP%3A%20A%20Framework%20for%20Quantifying%20the%20Safety%20of%20Vulnerable%20Road%20Users%20in%20Infrastructure-Assisted%20Collective%20Perception%20Using%20EuroNCAP%20Scenarios&entry.906535625=J%C3%B6rg%20Gamerdinger%20and%20Sven%20Teufel%20and%20Simon%20Roller%20and%20Oliver%20Bringmann&entry.1292438233=The%20growing%20number%20of%20road%20users%20has%20significantly%20increased%20the%20risk%20of%20accidents%20in%20recent%20years.%20Vulnerable%20Road%20Users%20%28VRUs%29%20are%20particularly%20at%20risk%2C%20especially%20in%20urban%20environments%20where%20they%20are%20often%20occluded%20by%20parked%20vehicles%20or%20buildings.%20Autonomous%20Driving%20%28AD%29%20and%20Collective%20Perception%20%28CP%29%20are%20promising%20solutions%20to%20mitigate%20these%20risks.%20In%20particular%2C%20infrastructure-assisted%20CP%2C%20where%20sensor%20units%20are%20mounted%20on%20infrastructure%20elements%20such%20as%20traffic%20lights%20or%20lamp%20posts%2C%20can%20help%20overcome%20perceptual%20limitations%20by%20providing%20enhanced%20points%20of%20view%2C%20which%20significantly%20reduces%20occlusions.%20To%20encourage%20decision%20makers%20to%20adopt%20this%20technology%2C%20comprehensive%20studies%20and%20datasets%20demonstrating%20safety%20improvements%20for%20VRUs%20are%20essential.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20for%20evaluating%20the%20safety%20improvement%20by%20infrastructure-based%20CP%20specifically%20targeted%20at%20VRUs%20including%20a%20dataset%20with%20safety-critical%20EuroNCAP%20scenarios%20%28CarlaNCAP%29%20with%2011k%20frames.%20Using%20this%20dataset%2C%20we%20conduct%20an%20in-depth%20simulation%20study%20and%20demonstrate%20that%20infrastructure-assisted%20CP%20can%20significantly%20reduce%20accident%20rates%20in%20safety-critical%20scenarios%2C%20achieving%20up%20to%20100%25%20accident%20avoidance%20compared%20to%20a%20vehicle%20equipped%20with%20sensors%20with%20only%2033%25.%20Code%20is%20available%20at%20https%3A//github.com/ekut-es/carla_ncap&entry.1838667208=http%3A//arxiv.org/abs/2512.11551v1&entry.124074799=Read"},
{"title": "LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems", "author": "Ernesto Casablanca and Oliver Sch\u00f6n and Paolo Zuliani and Sadegh Soudjani", "abstract": "Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks.", "link": "http://arxiv.org/abs/2512.11750v1", "date": "2025-12-12", "relevancy": 2.1451, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5581}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5321}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUCID%3A%20Learning-Enabled%20Uncertainty-Aware%20Certification%20of%20Stochastic%20Dynamical%20Systems&body=Title%3A%20LUCID%3A%20Learning-Enabled%20Uncertainty-Aware%20Certification%20of%20Stochastic%20Dynamical%20Systems%0AAuthor%3A%20Ernesto%20Casablanca%20and%20Oliver%20Sch%C3%B6n%20and%20Paolo%20Zuliani%20and%20Sadegh%20Soudjani%0AAbstract%3A%20Ensuring%20the%20safety%20of%20AI-enabled%20systems%2C%20particularly%20in%20high-stakes%20domains%20such%20as%20autonomous%20driving%20and%20healthcare%2C%20has%20become%20increasingly%20critical.%20Traditional%20formal%20verification%20tools%20fall%20short%20when%20faced%20with%20systems%20that%20embed%20both%20opaque%2C%20black-box%20AI%20components%20and%20complex%20stochastic%20dynamics.%20To%20address%20these%20challenges%2C%20we%20introduce%20LUCID%20%28Learning-enabled%20Uncertainty-aware%20Certification%20of%20stochastIc%20Dynamical%20systems%29%2C%20a%20verification%20engine%20for%20certifying%20safety%20of%20black-box%20stochastic%20dynamical%20systems%20from%20a%20finite%20dataset%20of%20random%20state%20transitions.%20As%20such%2C%20LUCID%20is%20the%20first%20known%20tool%20capable%20of%20establishing%20quantified%20safety%20guarantees%20for%20such%20systems.%20Thanks%20to%20its%20modular%20architecture%20and%20extensive%20documentation%2C%20LUCID%20is%20designed%20for%20easy%20extensibility.%20LUCID%20employs%20a%20data-driven%20methodology%20rooted%20in%20control%20barrier%20certificates%2C%20which%20are%20learned%20directly%20from%20system%20transition%20data%2C%20to%20ensure%20formal%20safety%20guarantees.%20We%20use%20conditional%20mean%20embeddings%20to%20embed%20data%20into%20a%20reproducing%20kernel%20Hilbert%20space%20%28RKHS%29%2C%20where%20an%20RKHS%20ambiguity%20set%20is%20constructed%20that%20can%20be%20inflated%20to%20robustify%20the%20result%20to%20out-of-distribution%20behavior.%20A%20key%20innovation%20within%20LUCID%20is%20its%20use%20of%20a%20finite%20Fourier%20kernel%20expansion%20to%20reformulate%20a%20semi-infinite%20non-convex%20optimization%20problem%20into%20a%20tractable%20linear%20program.%20The%20resulting%20spectral%20barrier%20allows%20us%20to%20leverage%20the%20fast%20Fourier%20transform%20to%20generate%20the%20relaxed%20problem%20efficiently%2C%20offering%20a%20scalable%20yet%20distributionally%20robust%20framework%20for%20verifying%20safety.%20LUCID%20thus%20offers%20a%20robust%20and%20efficient%20verification%20framework%2C%20able%20to%20handle%20the%20complexities%20of%20modern%20black-box%20systems%20while%20providing%20formal%20guarantees%20of%20safety.%20These%20unique%20capabilities%20are%20demonstrated%20on%20challenging%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUCID%253A%2520Learning-Enabled%2520Uncertainty-Aware%2520Certification%2520of%2520Stochastic%2520Dynamical%2520Systems%26entry.906535625%3DErnesto%2520Casablanca%2520and%2520Oliver%2520Sch%25C3%25B6n%2520and%2520Paolo%2520Zuliani%2520and%2520Sadegh%2520Soudjani%26entry.1292438233%3DEnsuring%2520the%2520safety%2520of%2520AI-enabled%2520systems%252C%2520particularly%2520in%2520high-stakes%2520domains%2520such%2520as%2520autonomous%2520driving%2520and%2520healthcare%252C%2520has%2520become%2520increasingly%2520critical.%2520Traditional%2520formal%2520verification%2520tools%2520fall%2520short%2520when%2520faced%2520with%2520systems%2520that%2520embed%2520both%2520opaque%252C%2520black-box%2520AI%2520components%2520and%2520complex%2520stochastic%2520dynamics.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520LUCID%2520%2528Learning-enabled%2520Uncertainty-aware%2520Certification%2520of%2520stochastIc%2520Dynamical%2520systems%2529%252C%2520a%2520verification%2520engine%2520for%2520certifying%2520safety%2520of%2520black-box%2520stochastic%2520dynamical%2520systems%2520from%2520a%2520finite%2520dataset%2520of%2520random%2520state%2520transitions.%2520As%2520such%252C%2520LUCID%2520is%2520the%2520first%2520known%2520tool%2520capable%2520of%2520establishing%2520quantified%2520safety%2520guarantees%2520for%2520such%2520systems.%2520Thanks%2520to%2520its%2520modular%2520architecture%2520and%2520extensive%2520documentation%252C%2520LUCID%2520is%2520designed%2520for%2520easy%2520extensibility.%2520LUCID%2520employs%2520a%2520data-driven%2520methodology%2520rooted%2520in%2520control%2520barrier%2520certificates%252C%2520which%2520are%2520learned%2520directly%2520from%2520system%2520transition%2520data%252C%2520to%2520ensure%2520formal%2520safety%2520guarantees.%2520We%2520use%2520conditional%2520mean%2520embeddings%2520to%2520embed%2520data%2520into%2520a%2520reproducing%2520kernel%2520Hilbert%2520space%2520%2528RKHS%2529%252C%2520where%2520an%2520RKHS%2520ambiguity%2520set%2520is%2520constructed%2520that%2520can%2520be%2520inflated%2520to%2520robustify%2520the%2520result%2520to%2520out-of-distribution%2520behavior.%2520A%2520key%2520innovation%2520within%2520LUCID%2520is%2520its%2520use%2520of%2520a%2520finite%2520Fourier%2520kernel%2520expansion%2520to%2520reformulate%2520a%2520semi-infinite%2520non-convex%2520optimization%2520problem%2520into%2520a%2520tractable%2520linear%2520program.%2520The%2520resulting%2520spectral%2520barrier%2520allows%2520us%2520to%2520leverage%2520the%2520fast%2520Fourier%2520transform%2520to%2520generate%2520the%2520relaxed%2520problem%2520efficiently%252C%2520offering%2520a%2520scalable%2520yet%2520distributionally%2520robust%2520framework%2520for%2520verifying%2520safety.%2520LUCID%2520thus%2520offers%2520a%2520robust%2520and%2520efficient%2520verification%2520framework%252C%2520able%2520to%2520handle%2520the%2520complexities%2520of%2520modern%2520black-box%2520systems%2520while%2520providing%2520formal%2520guarantees%2520of%2520safety.%2520These%2520unique%2520capabilities%2520are%2520demonstrated%2520on%2520challenging%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUCID%3A%20Learning-Enabled%20Uncertainty-Aware%20Certification%20of%20Stochastic%20Dynamical%20Systems&entry.906535625=Ernesto%20Casablanca%20and%20Oliver%20Sch%C3%B6n%20and%20Paolo%20Zuliani%20and%20Sadegh%20Soudjani&entry.1292438233=Ensuring%20the%20safety%20of%20AI-enabled%20systems%2C%20particularly%20in%20high-stakes%20domains%20such%20as%20autonomous%20driving%20and%20healthcare%2C%20has%20become%20increasingly%20critical.%20Traditional%20formal%20verification%20tools%20fall%20short%20when%20faced%20with%20systems%20that%20embed%20both%20opaque%2C%20black-box%20AI%20components%20and%20complex%20stochastic%20dynamics.%20To%20address%20these%20challenges%2C%20we%20introduce%20LUCID%20%28Learning-enabled%20Uncertainty-aware%20Certification%20of%20stochastIc%20Dynamical%20systems%29%2C%20a%20verification%20engine%20for%20certifying%20safety%20of%20black-box%20stochastic%20dynamical%20systems%20from%20a%20finite%20dataset%20of%20random%20state%20transitions.%20As%20such%2C%20LUCID%20is%20the%20first%20known%20tool%20capable%20of%20establishing%20quantified%20safety%20guarantees%20for%20such%20systems.%20Thanks%20to%20its%20modular%20architecture%20and%20extensive%20documentation%2C%20LUCID%20is%20designed%20for%20easy%20extensibility.%20LUCID%20employs%20a%20data-driven%20methodology%20rooted%20in%20control%20barrier%20certificates%2C%20which%20are%20learned%20directly%20from%20system%20transition%20data%2C%20to%20ensure%20formal%20safety%20guarantees.%20We%20use%20conditional%20mean%20embeddings%20to%20embed%20data%20into%20a%20reproducing%20kernel%20Hilbert%20space%20%28RKHS%29%2C%20where%20an%20RKHS%20ambiguity%20set%20is%20constructed%20that%20can%20be%20inflated%20to%20robustify%20the%20result%20to%20out-of-distribution%20behavior.%20A%20key%20innovation%20within%20LUCID%20is%20its%20use%20of%20a%20finite%20Fourier%20kernel%20expansion%20to%20reformulate%20a%20semi-infinite%20non-convex%20optimization%20problem%20into%20a%20tractable%20linear%20program.%20The%20resulting%20spectral%20barrier%20allows%20us%20to%20leverage%20the%20fast%20Fourier%20transform%20to%20generate%20the%20relaxed%20problem%20efficiently%2C%20offering%20a%20scalable%20yet%20distributionally%20robust%20framework%20for%20verifying%20safety.%20LUCID%20thus%20offers%20a%20robust%20and%20efficient%20verification%20framework%2C%20able%20to%20handle%20the%20complexities%20of%20modern%20black-box%20systems%20while%20providing%20formal%20guarantees%20of%20safety.%20These%20unique%20capabilities%20are%20demonstrated%20on%20challenging%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.11750v1&entry.124074799=Read"},
{"title": "ReCode: Unify Plan and Action for Universal Granularity Control", "author": "Zhaoyang Yu and Jiayi Zhang and Huixue Su and Yufan Zhao and Yifan Wu and Mingyi Deng and Jinyu Xiang and Yizhang Lin and Lingxiao Tang and Yuyu Luo and Bang Liu and Chenglin Wu", "abstract": "Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.", "link": "http://arxiv.org/abs/2510.23564v3", "date": "2025-12-12", "relevancy": 2.1309, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5614}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReCode%3A%20Unify%20Plan%20and%20Action%20for%20Universal%20Granularity%20Control&body=Title%3A%20ReCode%3A%20Unify%20Plan%20and%20Action%20for%20Universal%20Granularity%20Control%0AAuthor%3A%20Zhaoyang%20Yu%20and%20Jiayi%20Zhang%20and%20Huixue%20Su%20and%20Yufan%20Zhao%20and%20Yifan%20Wu%20and%20Mingyi%20Deng%20and%20Jinyu%20Xiang%20and%20Yizhang%20Lin%20and%20Lingxiao%20Tang%20and%20Yuyu%20Luo%20and%20Bang%20Liu%20and%20Chenglin%20Wu%0AAbstract%3A%20Real-world%20tasks%20require%20decisions%20at%20varying%20granularities%2C%20and%20humans%20excel%20at%20this%20by%20leveraging%20a%20unified%20cognitive%20representation%20where%20planning%20is%20fundamentally%20understood%20as%20a%20high-level%20form%20of%20action.%20However%2C%20current%20Large%20Language%20Model%20%28LLM%29-based%20agents%20lack%20this%20crucial%20capability%20to%20operate%20fluidly%20across%20decision%20granularities.%20This%20limitation%20stems%20from%20existing%20paradigms%20that%20enforce%20a%20rigid%20separation%20between%20high-level%20planning%20and%20low-level%20action%2C%20which%20impairs%20dynamic%20adaptability%20and%20limits%20generalization.%20We%20propose%20ReCode%20%28Recursive%20Code%20Generation%29%2C%20a%20novel%20paradigm%20that%20addresses%20this%20limitation%20by%20unifying%20planning%20and%20action%20within%20a%20single%20code%20representation.%20In%20this%20representation%2C%20ReCode%20treats%20high-level%20plans%20as%20abstract%20placeholder%20functions%2C%20which%20the%20agent%20then%20recursively%20decomposes%20into%20finer-grained%20sub-functions%20until%20reaching%20primitive%20actions.%20This%20recursive%20approach%20dissolves%20the%20rigid%20boundary%20between%20plan%20and%20action%2C%20enabling%20the%20agent%20to%20dynamically%20control%20its%20decision%20granularity.%20Furthermore%2C%20the%20recursive%20structure%20inherently%20generates%20rich%2C%20multi-granularity%20training%20data%2C%20enabling%20models%20to%20learn%20hierarchical%20decision-making%20processes.%20Extensive%20experiments%20show%20ReCode%20significantly%20surpasses%20advanced%20baselines%20in%20inference%20performance%20and%20demonstrates%20exceptional%20data%20efficiency%20in%20training%2C%20validating%20our%20core%20insight%20that%20unifying%20planning%20and%20action%20through%20recursive%20code%20generation%20is%20a%20powerful%20and%20effective%20approach%20to%20achieving%20universal%20granularity%20control.%20The%20code%20is%20available%20at%20https%3A//github.com/FoundationAgents/ReCode.%0ALink%3A%20http%3A//arxiv.org/abs/2510.23564v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReCode%253A%2520Unify%2520Plan%2520and%2520Action%2520for%2520Universal%2520Granularity%2520Control%26entry.906535625%3DZhaoyang%2520Yu%2520and%2520Jiayi%2520Zhang%2520and%2520Huixue%2520Su%2520and%2520Yufan%2520Zhao%2520and%2520Yifan%2520Wu%2520and%2520Mingyi%2520Deng%2520and%2520Jinyu%2520Xiang%2520and%2520Yizhang%2520Lin%2520and%2520Lingxiao%2520Tang%2520and%2520Yuyu%2520Luo%2520and%2520Bang%2520Liu%2520and%2520Chenglin%2520Wu%26entry.1292438233%3DReal-world%2520tasks%2520require%2520decisions%2520at%2520varying%2520granularities%252C%2520and%2520humans%2520excel%2520at%2520this%2520by%2520leveraging%2520a%2520unified%2520cognitive%2520representation%2520where%2520planning%2520is%2520fundamentally%2520understood%2520as%2520a%2520high-level%2520form%2520of%2520action.%2520However%252C%2520current%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520agents%2520lack%2520this%2520crucial%2520capability%2520to%2520operate%2520fluidly%2520across%2520decision%2520granularities.%2520This%2520limitation%2520stems%2520from%2520existing%2520paradigms%2520that%2520enforce%2520a%2520rigid%2520separation%2520between%2520high-level%2520planning%2520and%2520low-level%2520action%252C%2520which%2520impairs%2520dynamic%2520adaptability%2520and%2520limits%2520generalization.%2520We%2520propose%2520ReCode%2520%2528Recursive%2520Code%2520Generation%2529%252C%2520a%2520novel%2520paradigm%2520that%2520addresses%2520this%2520limitation%2520by%2520unifying%2520planning%2520and%2520action%2520within%2520a%2520single%2520code%2520representation.%2520In%2520this%2520representation%252C%2520ReCode%2520treats%2520high-level%2520plans%2520as%2520abstract%2520placeholder%2520functions%252C%2520which%2520the%2520agent%2520then%2520recursively%2520decomposes%2520into%2520finer-grained%2520sub-functions%2520until%2520reaching%2520primitive%2520actions.%2520This%2520recursive%2520approach%2520dissolves%2520the%2520rigid%2520boundary%2520between%2520plan%2520and%2520action%252C%2520enabling%2520the%2520agent%2520to%2520dynamically%2520control%2520its%2520decision%2520granularity.%2520Furthermore%252C%2520the%2520recursive%2520structure%2520inherently%2520generates%2520rich%252C%2520multi-granularity%2520training%2520data%252C%2520enabling%2520models%2520to%2520learn%2520hierarchical%2520decision-making%2520processes.%2520Extensive%2520experiments%2520show%2520ReCode%2520significantly%2520surpasses%2520advanced%2520baselines%2520in%2520inference%2520performance%2520and%2520demonstrates%2520exceptional%2520data%2520efficiency%2520in%2520training%252C%2520validating%2520our%2520core%2520insight%2520that%2520unifying%2520planning%2520and%2520action%2520through%2520recursive%2520code%2520generation%2520is%2520a%2520powerful%2520and%2520effective%2520approach%2520to%2520achieving%2520universal%2520granularity%2520control.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/FoundationAgents/ReCode.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23564v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReCode%3A%20Unify%20Plan%20and%20Action%20for%20Universal%20Granularity%20Control&entry.906535625=Zhaoyang%20Yu%20and%20Jiayi%20Zhang%20and%20Huixue%20Su%20and%20Yufan%20Zhao%20and%20Yifan%20Wu%20and%20Mingyi%20Deng%20and%20Jinyu%20Xiang%20and%20Yizhang%20Lin%20and%20Lingxiao%20Tang%20and%20Yuyu%20Luo%20and%20Bang%20Liu%20and%20Chenglin%20Wu&entry.1292438233=Real-world%20tasks%20require%20decisions%20at%20varying%20granularities%2C%20and%20humans%20excel%20at%20this%20by%20leveraging%20a%20unified%20cognitive%20representation%20where%20planning%20is%20fundamentally%20understood%20as%20a%20high-level%20form%20of%20action.%20However%2C%20current%20Large%20Language%20Model%20%28LLM%29-based%20agents%20lack%20this%20crucial%20capability%20to%20operate%20fluidly%20across%20decision%20granularities.%20This%20limitation%20stems%20from%20existing%20paradigms%20that%20enforce%20a%20rigid%20separation%20between%20high-level%20planning%20and%20low-level%20action%2C%20which%20impairs%20dynamic%20adaptability%20and%20limits%20generalization.%20We%20propose%20ReCode%20%28Recursive%20Code%20Generation%29%2C%20a%20novel%20paradigm%20that%20addresses%20this%20limitation%20by%20unifying%20planning%20and%20action%20within%20a%20single%20code%20representation.%20In%20this%20representation%2C%20ReCode%20treats%20high-level%20plans%20as%20abstract%20placeholder%20functions%2C%20which%20the%20agent%20then%20recursively%20decomposes%20into%20finer-grained%20sub-functions%20until%20reaching%20primitive%20actions.%20This%20recursive%20approach%20dissolves%20the%20rigid%20boundary%20between%20plan%20and%20action%2C%20enabling%20the%20agent%20to%20dynamically%20control%20its%20decision%20granularity.%20Furthermore%2C%20the%20recursive%20structure%20inherently%20generates%20rich%2C%20multi-granularity%20training%20data%2C%20enabling%20models%20to%20learn%20hierarchical%20decision-making%20processes.%20Extensive%20experiments%20show%20ReCode%20significantly%20surpasses%20advanced%20baselines%20in%20inference%20performance%20and%20demonstrates%20exceptional%20data%20efficiency%20in%20training%2C%20validating%20our%20core%20insight%20that%20unifying%20planning%20and%20action%20through%20recursive%20code%20generation%20is%20a%20powerful%20and%20effective%20approach%20to%20achieving%20universal%20granularity%20control.%20The%20code%20is%20available%20at%20https%3A//github.com/FoundationAgents/ReCode.&entry.1838667208=http%3A//arxiv.org/abs/2510.23564v3&entry.124074799=Read"},
{"title": "Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting", "author": "Federico Pennino and Maurizio Gabbrielli", "abstract": "The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, \"less is more\" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal \"training diet\" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.", "link": "http://arxiv.org/abs/2512.11546v1", "date": "2025-12-12", "relevancy": 2.1287, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5392}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.534}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20the%20Training%20Diet%3A%20Data%20Mixture%20Search%20for%20Robust%20Time%20Series%20Forecasting&body=Title%3A%20Optimizing%20the%20Training%20Diet%3A%20Data%20Mixture%20Search%20for%20Robust%20Time%20Series%20Forecasting%0AAuthor%3A%20Federico%20Pennino%20and%20Maurizio%20Gabbrielli%0AAbstract%3A%20The%20standard%20paradigm%20for%20training%20deep%20learning%20models%20on%20sensor%20data%20assumes%20that%20more%20data%20is%20always%20better.%20However%2C%20raw%20sensor%20streams%20are%20often%20imbalanced%20and%20contain%20significant%20redundancy%2C%20meaning%20that%20not%20all%20data%20points%20contribute%20equally%20to%20model%20generalization.%20In%20this%20paper%2C%20we%20show%20that%2C%20in%20some%20cases%2C%20%22less%20is%20more%22%20when%20considering%20datasets.%20We%20do%20this%20by%20reframing%20the%20data%20selection%20problem%3A%20rather%20than%20tuning%20model%20hyperparameters%2C%20we%20fix%20the%20model%20and%20optimize%20the%20composition%20of%20the%20training%20data%20itself.%20We%20introduce%20a%20framework%20for%20discovering%20the%20optimal%20%22training%20diet%22%20from%20a%20large%2C%20unlabeled%20time%20series%20corpus.%20Our%20framework%20first%20uses%20a%20large-scale%20encoder%20and%20k-means%20clustering%20to%20partition%20the%20dataset%20into%20distinct%2C%20behaviorally%20consistent%20clusters.%20These%20clusters%20represent%20the%20fundamental%20%27ingredients%27%20available%20for%20training.%20We%20then%20employ%20the%20Optuna%20optimization%20framework%20to%20search%20the%20high-dimensional%20space%20of%20possible%20data%20mixtures.%20For%20each%20trial%2C%20Optuna%20proposes%20a%20specific%20sampling%20ratio%20for%20each%20cluster%2C%20and%20a%20new%20training%20set%20is%20constructed%20based%20on%20this%20recipe.%20A%20smaller%20target%20model%20is%20then%20trained%20and%20evaluated.%20Our%20experiments%20reveal%20that%20this%20data-centric%20search%20consistently%20discovers%20data%20mixtures%20that%20yield%20models%20with%20significantly%20higher%20performance%20compared%20to%20baselines%20trained%20on%20the%20entire%20dataset.%20Specifically%20-%20evaluated%20on%20PMSM%20dataset%20-%20our%20method%20improved%20performance%20from%20a%20baseline%20MSE%20of%201.70%20to%201.37%2C%20a%2019.41%25%20improvement.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520the%2520Training%2520Diet%253A%2520Data%2520Mixture%2520Search%2520for%2520Robust%2520Time%2520Series%2520Forecasting%26entry.906535625%3DFederico%2520Pennino%2520and%2520Maurizio%2520Gabbrielli%26entry.1292438233%3DThe%2520standard%2520paradigm%2520for%2520training%2520deep%2520learning%2520models%2520on%2520sensor%2520data%2520assumes%2520that%2520more%2520data%2520is%2520always%2520better.%2520However%252C%2520raw%2520sensor%2520streams%2520are%2520often%2520imbalanced%2520and%2520contain%2520significant%2520redundancy%252C%2520meaning%2520that%2520not%2520all%2520data%2520points%2520contribute%2520equally%2520to%2520model%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%252C%2520in%2520some%2520cases%252C%2520%2522less%2520is%2520more%2522%2520when%2520considering%2520datasets.%2520We%2520do%2520this%2520by%2520reframing%2520the%2520data%2520selection%2520problem%253A%2520rather%2520than%2520tuning%2520model%2520hyperparameters%252C%2520we%2520fix%2520the%2520model%2520and%2520optimize%2520the%2520composition%2520of%2520the%2520training%2520data%2520itself.%2520We%2520introduce%2520a%2520framework%2520for%2520discovering%2520the%2520optimal%2520%2522training%2520diet%2522%2520from%2520a%2520large%252C%2520unlabeled%2520time%2520series%2520corpus.%2520Our%2520framework%2520first%2520uses%2520a%2520large-scale%2520encoder%2520and%2520k-means%2520clustering%2520to%2520partition%2520the%2520dataset%2520into%2520distinct%252C%2520behaviorally%2520consistent%2520clusters.%2520These%2520clusters%2520represent%2520the%2520fundamental%2520%2527ingredients%2527%2520available%2520for%2520training.%2520We%2520then%2520employ%2520the%2520Optuna%2520optimization%2520framework%2520to%2520search%2520the%2520high-dimensional%2520space%2520of%2520possible%2520data%2520mixtures.%2520For%2520each%2520trial%252C%2520Optuna%2520proposes%2520a%2520specific%2520sampling%2520ratio%2520for%2520each%2520cluster%252C%2520and%2520a%2520new%2520training%2520set%2520is%2520constructed%2520based%2520on%2520this%2520recipe.%2520A%2520smaller%2520target%2520model%2520is%2520then%2520trained%2520and%2520evaluated.%2520Our%2520experiments%2520reveal%2520that%2520this%2520data-centric%2520search%2520consistently%2520discovers%2520data%2520mixtures%2520that%2520yield%2520models%2520with%2520significantly%2520higher%2520performance%2520compared%2520to%2520baselines%2520trained%2520on%2520the%2520entire%2520dataset.%2520Specifically%2520-%2520evaluated%2520on%2520PMSM%2520dataset%2520-%2520our%2520method%2520improved%2520performance%2520from%2520a%2520baseline%2520MSE%2520of%25201.70%2520to%25201.37%252C%2520a%252019.41%2525%2520improvement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20the%20Training%20Diet%3A%20Data%20Mixture%20Search%20for%20Robust%20Time%20Series%20Forecasting&entry.906535625=Federico%20Pennino%20and%20Maurizio%20Gabbrielli&entry.1292438233=The%20standard%20paradigm%20for%20training%20deep%20learning%20models%20on%20sensor%20data%20assumes%20that%20more%20data%20is%20always%20better.%20However%2C%20raw%20sensor%20streams%20are%20often%20imbalanced%20and%20contain%20significant%20redundancy%2C%20meaning%20that%20not%20all%20data%20points%20contribute%20equally%20to%20model%20generalization.%20In%20this%20paper%2C%20we%20show%20that%2C%20in%20some%20cases%2C%20%22less%20is%20more%22%20when%20considering%20datasets.%20We%20do%20this%20by%20reframing%20the%20data%20selection%20problem%3A%20rather%20than%20tuning%20model%20hyperparameters%2C%20we%20fix%20the%20model%20and%20optimize%20the%20composition%20of%20the%20training%20data%20itself.%20We%20introduce%20a%20framework%20for%20discovering%20the%20optimal%20%22training%20diet%22%20from%20a%20large%2C%20unlabeled%20time%20series%20corpus.%20Our%20framework%20first%20uses%20a%20large-scale%20encoder%20and%20k-means%20clustering%20to%20partition%20the%20dataset%20into%20distinct%2C%20behaviorally%20consistent%20clusters.%20These%20clusters%20represent%20the%20fundamental%20%27ingredients%27%20available%20for%20training.%20We%20then%20employ%20the%20Optuna%20optimization%20framework%20to%20search%20the%20high-dimensional%20space%20of%20possible%20data%20mixtures.%20For%20each%20trial%2C%20Optuna%20proposes%20a%20specific%20sampling%20ratio%20for%20each%20cluster%2C%20and%20a%20new%20training%20set%20is%20constructed%20based%20on%20this%20recipe.%20A%20smaller%20target%20model%20is%20then%20trained%20and%20evaluated.%20Our%20experiments%20reveal%20that%20this%20data-centric%20search%20consistently%20discovers%20data%20mixtures%20that%20yield%20models%20with%20significantly%20higher%20performance%20compared%20to%20baselines%20trained%20on%20the%20entire%20dataset.%20Specifically%20-%20evaluated%20on%20PMSM%20dataset%20-%20our%20method%20improved%20performance%20from%20a%20baseline%20MSE%20of%201.70%20to%201.37%2C%20a%2019.41%25%20improvement.&entry.1838667208=http%3A//arxiv.org/abs/2512.11546v1&entry.124074799=Read"},
{"title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "author": "Zhenyang Cai and Jiaming Zhang and Junjie Zhao and Ziyi Zeng and Yanchao Li and Jingyi Liang and Junying Chen and Yunjin Yang and Jiajun You and Shuzhi Deng and Tongfei Wang and Wanting Chen and Chunxiu Hao and Ruiqi Xie and Zhenwei Wen and Xiangyi Feng and Zou Ting and Jin Zou Lin and Jianquan Li and Guangjun Yu and Liangyi Chen and Junwen Wang and Shan Jiang and Benyou Wang", "abstract": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "link": "http://arxiv.org/abs/2512.11558v1", "date": "2025-12-12", "relevancy": 2.1222, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5389}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DentalGPT%3A%20Incentivizing%20Multimodal%20Complex%20Reasoning%20in%20Dentistry&body=Title%3A%20DentalGPT%3A%20Incentivizing%20Multimodal%20Complex%20Reasoning%20in%20Dentistry%0AAuthor%3A%20Zhenyang%20Cai%20and%20Jiaming%20Zhang%20and%20Junjie%20Zhao%20and%20Ziyi%20Zeng%20and%20Yanchao%20Li%20and%20Jingyi%20Liang%20and%20Junying%20Chen%20and%20Yunjin%20Yang%20and%20Jiajun%20You%20and%20Shuzhi%20Deng%20and%20Tongfei%20Wang%20and%20Wanting%20Chen%20and%20Chunxiu%20Hao%20and%20Ruiqi%20Xie%20and%20Zhenwei%20Wen%20and%20Xiangyi%20Feng%20and%20Zou%20Ting%20and%20Jin%20Zou%20Lin%20and%20Jianquan%20Li%20and%20Guangjun%20Yu%20and%20Liangyi%20Chen%20and%20Junwen%20Wang%20and%20Shan%20Jiang%20and%20Benyou%20Wang%0AAbstract%3A%20Reliable%20interpretation%20of%20multimodal%20data%20in%20dentistry%20is%20essential%20for%20automated%20oral%20healthcare%2C%20yet%20current%20multimodal%20large%20language%20models%20%28MLLMs%29%20struggle%20to%20capture%20fine-grained%20dental%20visual%20details%20and%20lack%20sufficient%20reasoning%20ability%20for%20precise%20diagnosis.%20To%20address%20these%20limitations%2C%20we%20present%20DentalGPT%2C%20a%20specialized%20dental%20MLLM%20developed%20through%20high-quality%20domain%20knowledge%20injection%20and%20reinforcement%20learning.%20Specifically%2C%20the%20largest%20annotated%20multimodal%20dataset%20for%20dentistry%20to%20date%20was%20constructed%20by%20aggregating%20over%20120k%20dental%20images%20paired%20with%20detailed%20descriptions%20that%20highlight%20diagnostically%20relevant%20visual%20features%2C%20making%20it%20the%20multimodal%20dataset%20with%20the%20most%20extensive%20collection%20of%20dental%20images%20to%20date.%20Training%20on%20this%20dataset%20significantly%20enhances%20the%20MLLM%27s%20visual%20understanding%20of%20dental%20conditions%2C%20while%20the%20subsequent%20reinforcement%20learning%20stage%20further%20strengthens%20its%20capability%20for%20multimodal%20complex%20reasoning.%20Comprehensive%20evaluations%20on%20intraoral%20and%20panoramic%20benchmarks%2C%20along%20with%20dental%20subsets%20of%20medical%20VQA%20benchmarks%2C%20show%20that%20DentalGPT%20achieves%20superior%20performance%20in%20disease%20classification%20and%20dental%20VQA%20tasks%2C%20outperforming%20many%20state-of-the-art%20MLLMs%20despite%20having%20only%207B%20parameters.%20These%20results%20demonstrate%20that%20high-quality%20dental%20data%20combined%20with%20staged%20adaptation%20provides%20an%20effective%20pathway%20for%20building%20capable%20and%20domain-specialized%20dental%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDentalGPT%253A%2520Incentivizing%2520Multimodal%2520Complex%2520Reasoning%2520in%2520Dentistry%26entry.906535625%3DZhenyang%2520Cai%2520and%2520Jiaming%2520Zhang%2520and%2520Junjie%2520Zhao%2520and%2520Ziyi%2520Zeng%2520and%2520Yanchao%2520Li%2520and%2520Jingyi%2520Liang%2520and%2520Junying%2520Chen%2520and%2520Yunjin%2520Yang%2520and%2520Jiajun%2520You%2520and%2520Shuzhi%2520Deng%2520and%2520Tongfei%2520Wang%2520and%2520Wanting%2520Chen%2520and%2520Chunxiu%2520Hao%2520and%2520Ruiqi%2520Xie%2520and%2520Zhenwei%2520Wen%2520and%2520Xiangyi%2520Feng%2520and%2520Zou%2520Ting%2520and%2520Jin%2520Zou%2520Lin%2520and%2520Jianquan%2520Li%2520and%2520Guangjun%2520Yu%2520and%2520Liangyi%2520Chen%2520and%2520Junwen%2520Wang%2520and%2520Shan%2520Jiang%2520and%2520Benyou%2520Wang%26entry.1292438233%3DReliable%2520interpretation%2520of%2520multimodal%2520data%2520in%2520dentistry%2520is%2520essential%2520for%2520automated%2520oral%2520healthcare%252C%2520yet%2520current%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520struggle%2520to%2520capture%2520fine-grained%2520dental%2520visual%2520details%2520and%2520lack%2520sufficient%2520reasoning%2520ability%2520for%2520precise%2520diagnosis.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520DentalGPT%252C%2520a%2520specialized%2520dental%2520MLLM%2520developed%2520through%2520high-quality%2520domain%2520knowledge%2520injection%2520and%2520reinforcement%2520learning.%2520Specifically%252C%2520the%2520largest%2520annotated%2520multimodal%2520dataset%2520for%2520dentistry%2520to%2520date%2520was%2520constructed%2520by%2520aggregating%2520over%2520120k%2520dental%2520images%2520paired%2520with%2520detailed%2520descriptions%2520that%2520highlight%2520diagnostically%2520relevant%2520visual%2520features%252C%2520making%2520it%2520the%2520multimodal%2520dataset%2520with%2520the%2520most%2520extensive%2520collection%2520of%2520dental%2520images%2520to%2520date.%2520Training%2520on%2520this%2520dataset%2520significantly%2520enhances%2520the%2520MLLM%2527s%2520visual%2520understanding%2520of%2520dental%2520conditions%252C%2520while%2520the%2520subsequent%2520reinforcement%2520learning%2520stage%2520further%2520strengthens%2520its%2520capability%2520for%2520multimodal%2520complex%2520reasoning.%2520Comprehensive%2520evaluations%2520on%2520intraoral%2520and%2520panoramic%2520benchmarks%252C%2520along%2520with%2520dental%2520subsets%2520of%2520medical%2520VQA%2520benchmarks%252C%2520show%2520that%2520DentalGPT%2520achieves%2520superior%2520performance%2520in%2520disease%2520classification%2520and%2520dental%2520VQA%2520tasks%252C%2520outperforming%2520many%2520state-of-the-art%2520MLLMs%2520despite%2520having%2520only%25207B%2520parameters.%2520These%2520results%2520demonstrate%2520that%2520high-quality%2520dental%2520data%2520combined%2520with%2520staged%2520adaptation%2520provides%2520an%2520effective%2520pathway%2520for%2520building%2520capable%2520and%2520domain-specialized%2520dental%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DentalGPT%3A%20Incentivizing%20Multimodal%20Complex%20Reasoning%20in%20Dentistry&entry.906535625=Zhenyang%20Cai%20and%20Jiaming%20Zhang%20and%20Junjie%20Zhao%20and%20Ziyi%20Zeng%20and%20Yanchao%20Li%20and%20Jingyi%20Liang%20and%20Junying%20Chen%20and%20Yunjin%20Yang%20and%20Jiajun%20You%20and%20Shuzhi%20Deng%20and%20Tongfei%20Wang%20and%20Wanting%20Chen%20and%20Chunxiu%20Hao%20and%20Ruiqi%20Xie%20and%20Zhenwei%20Wen%20and%20Xiangyi%20Feng%20and%20Zou%20Ting%20and%20Jin%20Zou%20Lin%20and%20Jianquan%20Li%20and%20Guangjun%20Yu%20and%20Liangyi%20Chen%20and%20Junwen%20Wang%20and%20Shan%20Jiang%20and%20Benyou%20Wang&entry.1292438233=Reliable%20interpretation%20of%20multimodal%20data%20in%20dentistry%20is%20essential%20for%20automated%20oral%20healthcare%2C%20yet%20current%20multimodal%20large%20language%20models%20%28MLLMs%29%20struggle%20to%20capture%20fine-grained%20dental%20visual%20details%20and%20lack%20sufficient%20reasoning%20ability%20for%20precise%20diagnosis.%20To%20address%20these%20limitations%2C%20we%20present%20DentalGPT%2C%20a%20specialized%20dental%20MLLM%20developed%20through%20high-quality%20domain%20knowledge%20injection%20and%20reinforcement%20learning.%20Specifically%2C%20the%20largest%20annotated%20multimodal%20dataset%20for%20dentistry%20to%20date%20was%20constructed%20by%20aggregating%20over%20120k%20dental%20images%20paired%20with%20detailed%20descriptions%20that%20highlight%20diagnostically%20relevant%20visual%20features%2C%20making%20it%20the%20multimodal%20dataset%20with%20the%20most%20extensive%20collection%20of%20dental%20images%20to%20date.%20Training%20on%20this%20dataset%20significantly%20enhances%20the%20MLLM%27s%20visual%20understanding%20of%20dental%20conditions%2C%20while%20the%20subsequent%20reinforcement%20learning%20stage%20further%20strengthens%20its%20capability%20for%20multimodal%20complex%20reasoning.%20Comprehensive%20evaluations%20on%20intraoral%20and%20panoramic%20benchmarks%2C%20along%20with%20dental%20subsets%20of%20medical%20VQA%20benchmarks%2C%20show%20that%20DentalGPT%20achieves%20superior%20performance%20in%20disease%20classification%20and%20dental%20VQA%20tasks%2C%20outperforming%20many%20state-of-the-art%20MLLMs%20despite%20having%20only%207B%20parameters.%20These%20results%20demonstrate%20that%20high-quality%20dental%20data%20combined%20with%20staged%20adaptation%20provides%20an%20effective%20pathway%20for%20building%20capable%20and%20domain-specialized%20dental%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.11558v1&entry.124074799=Read"},
{"title": "Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized", "author": "Er Jin and Yang Zhang and Yongli Mou and Yanfei Dong and Stefan Decker and Kenji Kawaguchi and Johannes Stegmaier", "abstract": "Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.", "link": "http://arxiv.org/abs/2512.09687v2", "date": "2025-12-12", "relevancy": 2.1194, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.581}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5336}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unconsciously%20Forget%3A%20Mitigating%20Memorization%3B%20Without%20Knowing%20What%20is%20being%20Memorized&body=Title%3A%20Unconsciously%20Forget%3A%20Mitigating%20Memorization%3B%20Without%20Knowing%20What%20is%20being%20Memorized%0AAuthor%3A%20Er%20Jin%20and%20Yang%20Zhang%20and%20Yongli%20Mou%20and%20Yanfei%20Dong%20and%20Stefan%20Decker%20and%20Kenji%20Kawaguchi%20and%20Johannes%20Stegmaier%0AAbstract%3A%20Recent%20advances%20in%20generative%20models%20have%20demonstrated%20an%20exceptional%20ability%20to%20produce%20highly%20realistic%20images.%20However%2C%20previous%20studies%20show%20that%20generated%20images%20often%20resemble%20the%20training%20data%2C%20and%20this%20problem%20becomes%20more%20severe%20as%20the%20model%20size%20increases.%20Memorizing%20training%20data%20can%20lead%20to%20legal%20challenges%2C%20including%20copyright%20infringement%2C%20violations%20of%20portrait%20rights%2C%20and%20trademark%20violations.%20Existing%20approaches%20to%20mitigating%20memorization%20mainly%20focus%20on%20manipulating%20the%20denoising%20sampling%20process%20to%20steer%20image%20embeddings%20away%20from%20the%20memorized%20embedding%20space%20or%20employ%20unlearning%20methods%20that%20require%20training%20on%20datasets%20containing%20specific%20sets%20of%20memorized%20concepts.%20However%2C%20existing%20methods%20often%20incur%20substantial%20computational%20overhead%20during%20sampling%2C%20or%20focus%20narrowly%20on%20removing%20one%20or%20more%20groups%20of%20target%20concepts%2C%20imposing%20a%20significant%20limitation%20on%20their%20scalability.%20To%20understand%20and%20mitigate%20these%20problems%2C%20our%20work%2C%20UniForget%2C%20offers%20a%20new%20perspective%20on%20understanding%20the%20root%20cause%20of%20memorization.%20Our%20work%20demonstrates%20that%20specific%20parts%20of%20the%20model%20are%20responsible%20for%20copyrighted%20content%20generation.%20By%20applying%20model%20pruning%2C%20we%20can%20effectively%20suppress%20the%20probability%20of%20generating%20copyrighted%20content%20without%20targeting%20specific%20concepts%20while%20preserving%20the%20general%20generative%20capabilities%20of%20the%20model.%20Additionally%2C%20we%20show%20that%20our%20approach%20is%20both%20orthogonal%20and%20complementary%20to%20existing%20unlearning%20methods%2C%20thereby%20highlighting%20its%20potential%20to%20improve%20current%20unlearning%20and%20de-memorization%20techniques.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnconsciously%2520Forget%253A%2520Mitigating%2520Memorization%253B%2520Without%2520Knowing%2520What%2520is%2520being%2520Memorized%26entry.906535625%3DEr%2520Jin%2520and%2520Yang%2520Zhang%2520and%2520Yongli%2520Mou%2520and%2520Yanfei%2520Dong%2520and%2520Stefan%2520Decker%2520and%2520Kenji%2520Kawaguchi%2520and%2520Johannes%2520Stegmaier%26entry.1292438233%3DRecent%2520advances%2520in%2520generative%2520models%2520have%2520demonstrated%2520an%2520exceptional%2520ability%2520to%2520produce%2520highly%2520realistic%2520images.%2520However%252C%2520previous%2520studies%2520show%2520that%2520generated%2520images%2520often%2520resemble%2520the%2520training%2520data%252C%2520and%2520this%2520problem%2520becomes%2520more%2520severe%2520as%2520the%2520model%2520size%2520increases.%2520Memorizing%2520training%2520data%2520can%2520lead%2520to%2520legal%2520challenges%252C%2520including%2520copyright%2520infringement%252C%2520violations%2520of%2520portrait%2520rights%252C%2520and%2520trademark%2520violations.%2520Existing%2520approaches%2520to%2520mitigating%2520memorization%2520mainly%2520focus%2520on%2520manipulating%2520the%2520denoising%2520sampling%2520process%2520to%2520steer%2520image%2520embeddings%2520away%2520from%2520the%2520memorized%2520embedding%2520space%2520or%2520employ%2520unlearning%2520methods%2520that%2520require%2520training%2520on%2520datasets%2520containing%2520specific%2520sets%2520of%2520memorized%2520concepts.%2520However%252C%2520existing%2520methods%2520often%2520incur%2520substantial%2520computational%2520overhead%2520during%2520sampling%252C%2520or%2520focus%2520narrowly%2520on%2520removing%2520one%2520or%2520more%2520groups%2520of%2520target%2520concepts%252C%2520imposing%2520a%2520significant%2520limitation%2520on%2520their%2520scalability.%2520To%2520understand%2520and%2520mitigate%2520these%2520problems%252C%2520our%2520work%252C%2520UniForget%252C%2520offers%2520a%2520new%2520perspective%2520on%2520understanding%2520the%2520root%2520cause%2520of%2520memorization.%2520Our%2520work%2520demonstrates%2520that%2520specific%2520parts%2520of%2520the%2520model%2520are%2520responsible%2520for%2520copyrighted%2520content%2520generation.%2520By%2520applying%2520model%2520pruning%252C%2520we%2520can%2520effectively%2520suppress%2520the%2520probability%2520of%2520generating%2520copyrighted%2520content%2520without%2520targeting%2520specific%2520concepts%2520while%2520preserving%2520the%2520general%2520generative%2520capabilities%2520of%2520the%2520model.%2520Additionally%252C%2520we%2520show%2520that%2520our%2520approach%2520is%2520both%2520orthogonal%2520and%2520complementary%2520to%2520existing%2520unlearning%2520methods%252C%2520thereby%2520highlighting%2520its%2520potential%2520to%2520improve%2520current%2520unlearning%2520and%2520de-memorization%2520techniques.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unconsciously%20Forget%3A%20Mitigating%20Memorization%3B%20Without%20Knowing%20What%20is%20being%20Memorized&entry.906535625=Er%20Jin%20and%20Yang%20Zhang%20and%20Yongli%20Mou%20and%20Yanfei%20Dong%20and%20Stefan%20Decker%20and%20Kenji%20Kawaguchi%20and%20Johannes%20Stegmaier&entry.1292438233=Recent%20advances%20in%20generative%20models%20have%20demonstrated%20an%20exceptional%20ability%20to%20produce%20highly%20realistic%20images.%20However%2C%20previous%20studies%20show%20that%20generated%20images%20often%20resemble%20the%20training%20data%2C%20and%20this%20problem%20becomes%20more%20severe%20as%20the%20model%20size%20increases.%20Memorizing%20training%20data%20can%20lead%20to%20legal%20challenges%2C%20including%20copyright%20infringement%2C%20violations%20of%20portrait%20rights%2C%20and%20trademark%20violations.%20Existing%20approaches%20to%20mitigating%20memorization%20mainly%20focus%20on%20manipulating%20the%20denoising%20sampling%20process%20to%20steer%20image%20embeddings%20away%20from%20the%20memorized%20embedding%20space%20or%20employ%20unlearning%20methods%20that%20require%20training%20on%20datasets%20containing%20specific%20sets%20of%20memorized%20concepts.%20However%2C%20existing%20methods%20often%20incur%20substantial%20computational%20overhead%20during%20sampling%2C%20or%20focus%20narrowly%20on%20removing%20one%20or%20more%20groups%20of%20target%20concepts%2C%20imposing%20a%20significant%20limitation%20on%20their%20scalability.%20To%20understand%20and%20mitigate%20these%20problems%2C%20our%20work%2C%20UniForget%2C%20offers%20a%20new%20perspective%20on%20understanding%20the%20root%20cause%20of%20memorization.%20Our%20work%20demonstrates%20that%20specific%20parts%20of%20the%20model%20are%20responsible%20for%20copyrighted%20content%20generation.%20By%20applying%20model%20pruning%2C%20we%20can%20effectively%20suppress%20the%20probability%20of%20generating%20copyrighted%20content%20without%20targeting%20specific%20concepts%20while%20preserving%20the%20general%20generative%20capabilities%20of%20the%20model.%20Additionally%2C%20we%20show%20that%20our%20approach%20is%20both%20orthogonal%20and%20complementary%20to%20existing%20unlearning%20methods%2C%20thereby%20highlighting%20its%20potential%20to%20improve%20current%20unlearning%20and%20de-memorization%20techniques.&entry.1838667208=http%3A//arxiv.org/abs/2512.09687v2&entry.124074799=Read"},
{"title": "Multi-temporal Calving Front Segmentation", "author": "Marcel Dreier and Nora Gourmelon and Dakota Pyles and Fei Wu and Matthias Braun and Thorsten Seehaus and Andreas Maier and Vincent Christlein", "abstract": "The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.", "link": "http://arxiv.org/abs/2512.11560v1", "date": "2025-12-12", "relevancy": 2.1169, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5321}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-temporal%20Calving%20Front%20Segmentation&body=Title%3A%20Multi-temporal%20Calving%20Front%20Segmentation%0AAuthor%3A%20Marcel%20Dreier%20and%20Nora%20Gourmelon%20and%20Dakota%20Pyles%20and%20Fei%20Wu%20and%20Matthias%20Braun%20and%20Thorsten%20Seehaus%20and%20Andreas%20Maier%20and%20Vincent%20Christlein%0AAbstract%3A%20The%20calving%20fronts%20of%20marine-terminating%20glaciers%20undergo%20constant%20changes.%20These%20changes%20significantly%20affect%20the%20glacier%27s%20mass%20and%20dynamics%2C%20demanding%20continuous%20monitoring.%20To%20address%20this%20need%2C%20deep%20learning%20models%20were%20developed%20that%20can%20automatically%20delineate%20the%20calving%20front%20in%20Synthetic%20Aperture%20Radar%20imagery.%20However%2C%20these%20models%20often%20struggle%20to%20correctly%20classify%20areas%20affected%20by%20seasonal%20conditions%20such%20as%20ice%20melange%20or%20snow-covered%20surfaces.%20To%20address%20this%20issue%2C%20we%20propose%20to%20process%20multiple%20frames%20from%20a%20satellite%20image%20time%20series%20of%20the%20same%20glacier%20in%20parallel%20and%20exchange%20temporal%20information%20between%20the%20corresponding%20feature%20maps%20to%20stabilize%20each%20prediction.%20We%20integrate%20our%20approach%20into%20the%20current%20state-of-the-art%20architecture%20Tyrion%20and%20accomplish%20a%20new%20state-of-the-art%20performance%20on%20the%20CaFFe%20benchmark%20dataset.%20In%20particular%2C%20we%20achieve%20a%20Mean%20Distance%20Error%20of%20184.4%20m%20and%20a%20mean%20Intersection%20over%20Union%20of%2083.6.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-temporal%2520Calving%2520Front%2520Segmentation%26entry.906535625%3DMarcel%2520Dreier%2520and%2520Nora%2520Gourmelon%2520and%2520Dakota%2520Pyles%2520and%2520Fei%2520Wu%2520and%2520Matthias%2520Braun%2520and%2520Thorsten%2520Seehaus%2520and%2520Andreas%2520Maier%2520and%2520Vincent%2520Christlein%26entry.1292438233%3DThe%2520calving%2520fronts%2520of%2520marine-terminating%2520glaciers%2520undergo%2520constant%2520changes.%2520These%2520changes%2520significantly%2520affect%2520the%2520glacier%2527s%2520mass%2520and%2520dynamics%252C%2520demanding%2520continuous%2520monitoring.%2520To%2520address%2520this%2520need%252C%2520deep%2520learning%2520models%2520were%2520developed%2520that%2520can%2520automatically%2520delineate%2520the%2520calving%2520front%2520in%2520Synthetic%2520Aperture%2520Radar%2520imagery.%2520However%252C%2520these%2520models%2520often%2520struggle%2520to%2520correctly%2520classify%2520areas%2520affected%2520by%2520seasonal%2520conditions%2520such%2520as%2520ice%2520melange%2520or%2520snow-covered%2520surfaces.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520to%2520process%2520multiple%2520frames%2520from%2520a%2520satellite%2520image%2520time%2520series%2520of%2520the%2520same%2520glacier%2520in%2520parallel%2520and%2520exchange%2520temporal%2520information%2520between%2520the%2520corresponding%2520feature%2520maps%2520to%2520stabilize%2520each%2520prediction.%2520We%2520integrate%2520our%2520approach%2520into%2520the%2520current%2520state-of-the-art%2520architecture%2520Tyrion%2520and%2520accomplish%2520a%2520new%2520state-of-the-art%2520performance%2520on%2520the%2520CaFFe%2520benchmark%2520dataset.%2520In%2520particular%252C%2520we%2520achieve%2520a%2520Mean%2520Distance%2520Error%2520of%2520184.4%2520m%2520and%2520a%2520mean%2520Intersection%2520over%2520Union%2520of%252083.6.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-temporal%20Calving%20Front%20Segmentation&entry.906535625=Marcel%20Dreier%20and%20Nora%20Gourmelon%20and%20Dakota%20Pyles%20and%20Fei%20Wu%20and%20Matthias%20Braun%20and%20Thorsten%20Seehaus%20and%20Andreas%20Maier%20and%20Vincent%20Christlein&entry.1292438233=The%20calving%20fronts%20of%20marine-terminating%20glaciers%20undergo%20constant%20changes.%20These%20changes%20significantly%20affect%20the%20glacier%27s%20mass%20and%20dynamics%2C%20demanding%20continuous%20monitoring.%20To%20address%20this%20need%2C%20deep%20learning%20models%20were%20developed%20that%20can%20automatically%20delineate%20the%20calving%20front%20in%20Synthetic%20Aperture%20Radar%20imagery.%20However%2C%20these%20models%20often%20struggle%20to%20correctly%20classify%20areas%20affected%20by%20seasonal%20conditions%20such%20as%20ice%20melange%20or%20snow-covered%20surfaces.%20To%20address%20this%20issue%2C%20we%20propose%20to%20process%20multiple%20frames%20from%20a%20satellite%20image%20time%20series%20of%20the%20same%20glacier%20in%20parallel%20and%20exchange%20temporal%20information%20between%20the%20corresponding%20feature%20maps%20to%20stabilize%20each%20prediction.%20We%20integrate%20our%20approach%20into%20the%20current%20state-of-the-art%20architecture%20Tyrion%20and%20accomplish%20a%20new%20state-of-the-art%20performance%20on%20the%20CaFFe%20benchmark%20dataset.%20In%20particular%2C%20we%20achieve%20a%20Mean%20Distance%20Error%20of%20184.4%20m%20and%20a%20mean%20Intersection%20over%20Union%20of%2083.6.&entry.1838667208=http%3A//arxiv.org/abs/2512.11560v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs", "author": "Wentao Jiang and Vamsi Varra and Caitlin Perez-Stable and Harrison Zhu and Meredith Apicella and Nicole Nyamongo", "abstract": "Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment.", "link": "http://arxiv.org/abs/2512.11791v1", "date": "2025-12-12", "relevancy": 2.1057, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5459}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5242}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Domain%20Adaptation%20for%20Vitiligo%20Segmentation%20in%20Clinical%20Photographs&body=Title%3A%20Uncertainty-Aware%20Domain%20Adaptation%20for%20Vitiligo%20Segmentation%20in%20Clinical%20Photographs%0AAuthor%3A%20Wentao%20Jiang%20and%20Vamsi%20Varra%20and%20Caitlin%20Perez-Stable%20and%20Harrison%20Zhu%20and%20Meredith%20Apicella%20and%20Nicole%20Nyamongo%0AAbstract%3A%20Accurately%20quantifying%20vitiligo%20extent%20in%20routine%20clinical%20photographs%20is%20crucial%20for%20longitudinal%20monitoring%20of%20treatment%20response.%20We%20propose%20a%20trustworthy%2C%20frequency-aware%20segmentation%20framework%20built%20on%20three%20synergistic%20pillars%3A%20%281%29%20a%20data-efficient%20training%20strategy%20combining%20domain-adaptive%20pre-training%20on%20the%20ISIC%202019%20dataset%20with%20an%20ROI-constrained%20dual-task%20loss%20to%20suppress%20background%20noise%3B%20%282%29%20an%20architectural%20refinement%20via%20a%20ConvNeXt%20V2-based%20encoder%20enhanced%20with%20a%20novel%20High-Frequency%20Spectral%20Gating%20%28HFSG%29%20module%20and%20stem-skip%20connections%20to%20capture%20subtle%20textures%3B%20and%20%283%29%20a%20clinical%20trust%20mechanism%20employing%20K-fold%20ensemble%20and%20Test-Time%20Augmentation%20%28TTA%29%20to%20generate%20pixel-wise%20uncertainty%20maps.%20Extensive%20validation%20on%20an%20expert-annotated%20clinical%20cohort%20demonstrates%20superior%20performance%2C%20achieving%20a%20Dice%20score%20of%2085.05%25%20and%20significantly%20reducing%20boundary%20error%20%2895%25%20Hausdorff%20Distance%20improved%20from%2044.79%20px%20to%2029.95%20px%29%2C%20consistently%20outperforming%20strong%20CNN%20%28ResNet-50%20and%20UNet%2B%2B%29%20and%20Transformer%20%28MiT-B5%29%20baselines.%20Notably%2C%20our%20framework%20demonstrates%20high%20reliability%20with%20zero%20catastrophic%20failures%20and%20provides%20interpretable%20entropy%20maps%20to%20identify%20ambiguous%20regions%20for%20clinician%20review.%20Our%20approach%20suggests%20that%20the%20proposed%20framework%20establishes%20a%20robust%20and%20reliable%20standard%20for%20automated%20vitiligo%20assessment.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Domain%2520Adaptation%2520for%2520Vitiligo%2520Segmentation%2520in%2520Clinical%2520Photographs%26entry.906535625%3DWentao%2520Jiang%2520and%2520Vamsi%2520Varra%2520and%2520Caitlin%2520Perez-Stable%2520and%2520Harrison%2520Zhu%2520and%2520Meredith%2520Apicella%2520and%2520Nicole%2520Nyamongo%26entry.1292438233%3DAccurately%2520quantifying%2520vitiligo%2520extent%2520in%2520routine%2520clinical%2520photographs%2520is%2520crucial%2520for%2520longitudinal%2520monitoring%2520of%2520treatment%2520response.%2520We%2520propose%2520a%2520trustworthy%252C%2520frequency-aware%2520segmentation%2520framework%2520built%2520on%2520three%2520synergistic%2520pillars%253A%2520%25281%2529%2520a%2520data-efficient%2520training%2520strategy%2520combining%2520domain-adaptive%2520pre-training%2520on%2520the%2520ISIC%25202019%2520dataset%2520with%2520an%2520ROI-constrained%2520dual-task%2520loss%2520to%2520suppress%2520background%2520noise%253B%2520%25282%2529%2520an%2520architectural%2520refinement%2520via%2520a%2520ConvNeXt%2520V2-based%2520encoder%2520enhanced%2520with%2520a%2520novel%2520High-Frequency%2520Spectral%2520Gating%2520%2528HFSG%2529%2520module%2520and%2520stem-skip%2520connections%2520to%2520capture%2520subtle%2520textures%253B%2520and%2520%25283%2529%2520a%2520clinical%2520trust%2520mechanism%2520employing%2520K-fold%2520ensemble%2520and%2520Test-Time%2520Augmentation%2520%2528TTA%2529%2520to%2520generate%2520pixel-wise%2520uncertainty%2520maps.%2520Extensive%2520validation%2520on%2520an%2520expert-annotated%2520clinical%2520cohort%2520demonstrates%2520superior%2520performance%252C%2520achieving%2520a%2520Dice%2520score%2520of%252085.05%2525%2520and%2520significantly%2520reducing%2520boundary%2520error%2520%252895%2525%2520Hausdorff%2520Distance%2520improved%2520from%252044.79%2520px%2520to%252029.95%2520px%2529%252C%2520consistently%2520outperforming%2520strong%2520CNN%2520%2528ResNet-50%2520and%2520UNet%252B%252B%2529%2520and%2520Transformer%2520%2528MiT-B5%2529%2520baselines.%2520Notably%252C%2520our%2520framework%2520demonstrates%2520high%2520reliability%2520with%2520zero%2520catastrophic%2520failures%2520and%2520provides%2520interpretable%2520entropy%2520maps%2520to%2520identify%2520ambiguous%2520regions%2520for%2520clinician%2520review.%2520Our%2520approach%2520suggests%2520that%2520the%2520proposed%2520framework%2520establishes%2520a%2520robust%2520and%2520reliable%2520standard%2520for%2520automated%2520vitiligo%2520assessment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Domain%20Adaptation%20for%20Vitiligo%20Segmentation%20in%20Clinical%20Photographs&entry.906535625=Wentao%20Jiang%20and%20Vamsi%20Varra%20and%20Caitlin%20Perez-Stable%20and%20Harrison%20Zhu%20and%20Meredith%20Apicella%20and%20Nicole%20Nyamongo&entry.1292438233=Accurately%20quantifying%20vitiligo%20extent%20in%20routine%20clinical%20photographs%20is%20crucial%20for%20longitudinal%20monitoring%20of%20treatment%20response.%20We%20propose%20a%20trustworthy%2C%20frequency-aware%20segmentation%20framework%20built%20on%20three%20synergistic%20pillars%3A%20%281%29%20a%20data-efficient%20training%20strategy%20combining%20domain-adaptive%20pre-training%20on%20the%20ISIC%202019%20dataset%20with%20an%20ROI-constrained%20dual-task%20loss%20to%20suppress%20background%20noise%3B%20%282%29%20an%20architectural%20refinement%20via%20a%20ConvNeXt%20V2-based%20encoder%20enhanced%20with%20a%20novel%20High-Frequency%20Spectral%20Gating%20%28HFSG%29%20module%20and%20stem-skip%20connections%20to%20capture%20subtle%20textures%3B%20and%20%283%29%20a%20clinical%20trust%20mechanism%20employing%20K-fold%20ensemble%20and%20Test-Time%20Augmentation%20%28TTA%29%20to%20generate%20pixel-wise%20uncertainty%20maps.%20Extensive%20validation%20on%20an%20expert-annotated%20clinical%20cohort%20demonstrates%20superior%20performance%2C%20achieving%20a%20Dice%20score%20of%2085.05%25%20and%20significantly%20reducing%20boundary%20error%20%2895%25%20Hausdorff%20Distance%20improved%20from%2044.79%20px%20to%2029.95%20px%29%2C%20consistently%20outperforming%20strong%20CNN%20%28ResNet-50%20and%20UNet%2B%2B%29%20and%20Transformer%20%28MiT-B5%29%20baselines.%20Notably%2C%20our%20framework%20demonstrates%20high%20reliability%20with%20zero%20catastrophic%20failures%20and%20provides%20interpretable%20entropy%20maps%20to%20identify%20ambiguous%20regions%20for%20clinician%20review.%20Our%20approach%20suggests%20that%20the%20proposed%20framework%20establishes%20a%20robust%20and%20reliable%20standard%20for%20automated%20vitiligo%20assessment.&entry.1838667208=http%3A//arxiv.org/abs/2512.11791v1&entry.124074799=Read"},
{"title": "Visual-Friendly Concept Protection via Selective Adversarial Perturbations", "author": "Xiaoyue Mi and Fan Tang and You Wu and Juan Cao and Peng Li and Yang Liu", "abstract": "Personalized concept generation by tuning diffusion models with a few images raises potential legal and ethical concerns regarding privacy and intellectual property rights. Researchers attempt to prevent malicious personalization using adversarial perturbations. However, previous efforts have mainly focused on the effectiveness of protection while neglecting the visibility of perturbations. They utilize global adversarial perturbations, which introduce noticeable alterations to original images and significantly degrade visual quality. In this work, we propose the Visual-Friendly Concept Protection (VCPro) framework, which prioritizes the protection of key concepts chosen by the image owner through adversarial perturbations with lower perceptibility. To ensure these perturbations are as inconspicuous as possible, we introduce a relaxed optimization objective to identify the least perceptible yet effective adversarial perturbations, solved using the Lagrangian multiplier method. Qualitative and quantitative experiments validate that VCPro achieves a better trade-off between the visibility of perturbations and protection effectiveness, effectively prioritizing the protection of target concepts in images with less perceptible perturbations.", "link": "http://arxiv.org/abs/2408.08518v3", "date": "2025-12-12", "relevancy": 2.0988, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5272}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5252}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-Friendly%20Concept%20Protection%20via%20Selective%20Adversarial%20Perturbations&body=Title%3A%20Visual-Friendly%20Concept%20Protection%20via%20Selective%20Adversarial%20Perturbations%0AAuthor%3A%20Xiaoyue%20Mi%20and%20Fan%20Tang%20and%20You%20Wu%20and%20Juan%20Cao%20and%20Peng%20Li%20and%20Yang%20Liu%0AAbstract%3A%20Personalized%20concept%20generation%20by%20tuning%20diffusion%20models%20with%20a%20few%20images%20raises%20potential%20legal%20and%20ethical%20concerns%20regarding%20privacy%20and%20intellectual%20property%20rights.%20Researchers%20attempt%20to%20prevent%20malicious%20personalization%20using%20adversarial%20perturbations.%20However%2C%20previous%20efforts%20have%20mainly%20focused%20on%20the%20effectiveness%20of%20protection%20while%20neglecting%20the%20visibility%20of%20perturbations.%20They%20utilize%20global%20adversarial%20perturbations%2C%20which%20introduce%20noticeable%20alterations%20to%20original%20images%20and%20significantly%20degrade%20visual%20quality.%20In%20this%20work%2C%20we%20propose%20the%20Visual-Friendly%20Concept%20Protection%20%28VCPro%29%20framework%2C%20which%20prioritizes%20the%20protection%20of%20key%20concepts%20chosen%20by%20the%20image%20owner%20through%20adversarial%20perturbations%20with%20lower%20perceptibility.%20To%20ensure%20these%20perturbations%20are%20as%20inconspicuous%20as%20possible%2C%20we%20introduce%20a%20relaxed%20optimization%20objective%20to%20identify%20the%20least%20perceptible%20yet%20effective%20adversarial%20perturbations%2C%20solved%20using%20the%20Lagrangian%20multiplier%20method.%20Qualitative%20and%20quantitative%20experiments%20validate%20that%20VCPro%20achieves%20a%20better%20trade-off%20between%20the%20visibility%20of%20perturbations%20and%20protection%20effectiveness%2C%20effectively%20prioritizing%20the%20protection%20of%20target%20concepts%20in%20images%20with%20less%20perceptible%20perturbations.%0ALink%3A%20http%3A//arxiv.org/abs/2408.08518v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-Friendly%2520Concept%2520Protection%2520via%2520Selective%2520Adversarial%2520Perturbations%26entry.906535625%3DXiaoyue%2520Mi%2520and%2520Fan%2520Tang%2520and%2520You%2520Wu%2520and%2520Juan%2520Cao%2520and%2520Peng%2520Li%2520and%2520Yang%2520Liu%26entry.1292438233%3DPersonalized%2520concept%2520generation%2520by%2520tuning%2520diffusion%2520models%2520with%2520a%2520few%2520images%2520raises%2520potential%2520legal%2520and%2520ethical%2520concerns%2520regarding%2520privacy%2520and%2520intellectual%2520property%2520rights.%2520Researchers%2520attempt%2520to%2520prevent%2520malicious%2520personalization%2520using%2520adversarial%2520perturbations.%2520However%252C%2520previous%2520efforts%2520have%2520mainly%2520focused%2520on%2520the%2520effectiveness%2520of%2520protection%2520while%2520neglecting%2520the%2520visibility%2520of%2520perturbations.%2520They%2520utilize%2520global%2520adversarial%2520perturbations%252C%2520which%2520introduce%2520noticeable%2520alterations%2520to%2520original%2520images%2520and%2520significantly%2520degrade%2520visual%2520quality.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Visual-Friendly%2520Concept%2520Protection%2520%2528VCPro%2529%2520framework%252C%2520which%2520prioritizes%2520the%2520protection%2520of%2520key%2520concepts%2520chosen%2520by%2520the%2520image%2520owner%2520through%2520adversarial%2520perturbations%2520with%2520lower%2520perceptibility.%2520To%2520ensure%2520these%2520perturbations%2520are%2520as%2520inconspicuous%2520as%2520possible%252C%2520we%2520introduce%2520a%2520relaxed%2520optimization%2520objective%2520to%2520identify%2520the%2520least%2520perceptible%2520yet%2520effective%2520adversarial%2520perturbations%252C%2520solved%2520using%2520the%2520Lagrangian%2520multiplier%2520method.%2520Qualitative%2520and%2520quantitative%2520experiments%2520validate%2520that%2520VCPro%2520achieves%2520a%2520better%2520trade-off%2520between%2520the%2520visibility%2520of%2520perturbations%2520and%2520protection%2520effectiveness%252C%2520effectively%2520prioritizing%2520the%2520protection%2520of%2520target%2520concepts%2520in%2520images%2520with%2520less%2520perceptible%2520perturbations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08518v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-Friendly%20Concept%20Protection%20via%20Selective%20Adversarial%20Perturbations&entry.906535625=Xiaoyue%20Mi%20and%20Fan%20Tang%20and%20You%20Wu%20and%20Juan%20Cao%20and%20Peng%20Li%20and%20Yang%20Liu&entry.1292438233=Personalized%20concept%20generation%20by%20tuning%20diffusion%20models%20with%20a%20few%20images%20raises%20potential%20legal%20and%20ethical%20concerns%20regarding%20privacy%20and%20intellectual%20property%20rights.%20Researchers%20attempt%20to%20prevent%20malicious%20personalization%20using%20adversarial%20perturbations.%20However%2C%20previous%20efforts%20have%20mainly%20focused%20on%20the%20effectiveness%20of%20protection%20while%20neglecting%20the%20visibility%20of%20perturbations.%20They%20utilize%20global%20adversarial%20perturbations%2C%20which%20introduce%20noticeable%20alterations%20to%20original%20images%20and%20significantly%20degrade%20visual%20quality.%20In%20this%20work%2C%20we%20propose%20the%20Visual-Friendly%20Concept%20Protection%20%28VCPro%29%20framework%2C%20which%20prioritizes%20the%20protection%20of%20key%20concepts%20chosen%20by%20the%20image%20owner%20through%20adversarial%20perturbations%20with%20lower%20perceptibility.%20To%20ensure%20these%20perturbations%20are%20as%20inconspicuous%20as%20possible%2C%20we%20introduce%20a%20relaxed%20optimization%20objective%20to%20identify%20the%20least%20perceptible%20yet%20effective%20adversarial%20perturbations%2C%20solved%20using%20the%20Lagrangian%20multiplier%20method.%20Qualitative%20and%20quantitative%20experiments%20validate%20that%20VCPro%20achieves%20a%20better%20trade-off%20between%20the%20visibility%20of%20perturbations%20and%20protection%20effectiveness%2C%20effectively%20prioritizing%20the%20protection%20of%20target%20concepts%20in%20images%20with%20less%20perceptible%20perturbations.&entry.1838667208=http%3A//arxiv.org/abs/2408.08518v3&entry.124074799=Read"},
{"title": "xGR: Efficient Generative Recommendation Serving at Scale", "author": "Qingxiao Sun and Tongxuan Liu and Shen Zhang and Siyu Wu and Peijun Yang and Haotian Liang and Menxin Li and Xiaolong Ma and Zhiwei Liang and Ziyi Ren and Minchao Zhang and Xinyu Liu and Ke Zhang and Depei Qian and Hailong Yang", "abstract": "Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.", "link": "http://arxiv.org/abs/2512.11529v1", "date": "2025-12-12", "relevancy": 2.0977, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5341}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5319}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xGR%3A%20Efficient%20Generative%20Recommendation%20Serving%20at%20Scale&body=Title%3A%20xGR%3A%20Efficient%20Generative%20Recommendation%20Serving%20at%20Scale%0AAuthor%3A%20Qingxiao%20Sun%20and%20Tongxuan%20Liu%20and%20Shen%20Zhang%20and%20Siyu%20Wu%20and%20Peijun%20Yang%20and%20Haotian%20Liang%20and%20Menxin%20Li%20and%20Xiaolong%20Ma%20and%20Zhiwei%20Liang%20and%20Ziyi%20Ren%20and%20Minchao%20Zhang%20and%20Xinyu%20Liu%20and%20Ke%20Zhang%20and%20Depei%20Qian%20and%20Hailong%20Yang%0AAbstract%3A%20Recommendation%20system%20delivers%20substantial%20economic%20benefits%20by%20providing%20personalized%20predictions.%20Generative%20recommendation%20%28GR%29%20integrates%20LLMs%20to%20enhance%20the%20understanding%20of%20long%20user-item%20sequences.%20Despite%20employing%20attention-based%20architectures%2C%20GR%27s%20workload%20differs%20markedly%20from%20that%20of%20LLM%20serving.%20GR%20typically%20processes%20long%20prompt%20while%20producing%20short%2C%20fixed-length%20outputs%2C%20yet%20the%20computational%20cost%20of%20each%20decode%20phase%20is%20especially%20high%20due%20to%20the%20large%20beam%20width.%20In%20addition%2C%20since%20the%20beam%20search%20involves%20a%20vast%20item%20space%2C%20the%20sorting%20overhead%20becomes%20particularly%20time-consuming.%20We%20propose%20xGR%2C%20a%20GR-oriented%20serving%20system%20that%20meets%20strict%20low-latency%20requirements%20under%20highconcurrency%20scenarios.%20First%2C%20xGR%20unifies%20the%20processing%20of%20prefill%20and%20decode%20phases%20through%20staged%20computation%20and%20separated%20KV%20cache.%20Second%2C%20xGR%20enables%20early%20sorting%20termination%20and%20mask-based%20item%20filtering%20with%20data%20structure%20reuse.%20Third%2C%20xGR%20reconstructs%20the%20overall%20pipeline%20to%20exploit%20multilevel%20overlap%20and%20multi-stream%20parallelism.%20Our%20experiments%20with%20real-world%20recommendation%20service%20datasets%20demonstrate%20that%20xGR%20achieves%20at%20least%203.49x%20throughput%20compared%20to%20the%20state-of-the-art%20baseline%20under%20strict%20latency%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxGR%253A%2520Efficient%2520Generative%2520Recommendation%2520Serving%2520at%2520Scale%26entry.906535625%3DQingxiao%2520Sun%2520and%2520Tongxuan%2520Liu%2520and%2520Shen%2520Zhang%2520and%2520Siyu%2520Wu%2520and%2520Peijun%2520Yang%2520and%2520Haotian%2520Liang%2520and%2520Menxin%2520Li%2520and%2520Xiaolong%2520Ma%2520and%2520Zhiwei%2520Liang%2520and%2520Ziyi%2520Ren%2520and%2520Minchao%2520Zhang%2520and%2520Xinyu%2520Liu%2520and%2520Ke%2520Zhang%2520and%2520Depei%2520Qian%2520and%2520Hailong%2520Yang%26entry.1292438233%3DRecommendation%2520system%2520delivers%2520substantial%2520economic%2520benefits%2520by%2520providing%2520personalized%2520predictions.%2520Generative%2520recommendation%2520%2528GR%2529%2520integrates%2520LLMs%2520to%2520enhance%2520the%2520understanding%2520of%2520long%2520user-item%2520sequences.%2520Despite%2520employing%2520attention-based%2520architectures%252C%2520GR%2527s%2520workload%2520differs%2520markedly%2520from%2520that%2520of%2520LLM%2520serving.%2520GR%2520typically%2520processes%2520long%2520prompt%2520while%2520producing%2520short%252C%2520fixed-length%2520outputs%252C%2520yet%2520the%2520computational%2520cost%2520of%2520each%2520decode%2520phase%2520is%2520especially%2520high%2520due%2520to%2520the%2520large%2520beam%2520width.%2520In%2520addition%252C%2520since%2520the%2520beam%2520search%2520involves%2520a%2520vast%2520item%2520space%252C%2520the%2520sorting%2520overhead%2520becomes%2520particularly%2520time-consuming.%2520We%2520propose%2520xGR%252C%2520a%2520GR-oriented%2520serving%2520system%2520that%2520meets%2520strict%2520low-latency%2520requirements%2520under%2520highconcurrency%2520scenarios.%2520First%252C%2520xGR%2520unifies%2520the%2520processing%2520of%2520prefill%2520and%2520decode%2520phases%2520through%2520staged%2520computation%2520and%2520separated%2520KV%2520cache.%2520Second%252C%2520xGR%2520enables%2520early%2520sorting%2520termination%2520and%2520mask-based%2520item%2520filtering%2520with%2520data%2520structure%2520reuse.%2520Third%252C%2520xGR%2520reconstructs%2520the%2520overall%2520pipeline%2520to%2520exploit%2520multilevel%2520overlap%2520and%2520multi-stream%2520parallelism.%2520Our%2520experiments%2520with%2520real-world%2520recommendation%2520service%2520datasets%2520demonstrate%2520that%2520xGR%2520achieves%2520at%2520least%25203.49x%2520throughput%2520compared%2520to%2520the%2520state-of-the-art%2520baseline%2520under%2520strict%2520latency%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xGR%3A%20Efficient%20Generative%20Recommendation%20Serving%20at%20Scale&entry.906535625=Qingxiao%20Sun%20and%20Tongxuan%20Liu%20and%20Shen%20Zhang%20and%20Siyu%20Wu%20and%20Peijun%20Yang%20and%20Haotian%20Liang%20and%20Menxin%20Li%20and%20Xiaolong%20Ma%20and%20Zhiwei%20Liang%20and%20Ziyi%20Ren%20and%20Minchao%20Zhang%20and%20Xinyu%20Liu%20and%20Ke%20Zhang%20and%20Depei%20Qian%20and%20Hailong%20Yang&entry.1292438233=Recommendation%20system%20delivers%20substantial%20economic%20benefits%20by%20providing%20personalized%20predictions.%20Generative%20recommendation%20%28GR%29%20integrates%20LLMs%20to%20enhance%20the%20understanding%20of%20long%20user-item%20sequences.%20Despite%20employing%20attention-based%20architectures%2C%20GR%27s%20workload%20differs%20markedly%20from%20that%20of%20LLM%20serving.%20GR%20typically%20processes%20long%20prompt%20while%20producing%20short%2C%20fixed-length%20outputs%2C%20yet%20the%20computational%20cost%20of%20each%20decode%20phase%20is%20especially%20high%20due%20to%20the%20large%20beam%20width.%20In%20addition%2C%20since%20the%20beam%20search%20involves%20a%20vast%20item%20space%2C%20the%20sorting%20overhead%20becomes%20particularly%20time-consuming.%20We%20propose%20xGR%2C%20a%20GR-oriented%20serving%20system%20that%20meets%20strict%20low-latency%20requirements%20under%20highconcurrency%20scenarios.%20First%2C%20xGR%20unifies%20the%20processing%20of%20prefill%20and%20decode%20phases%20through%20staged%20computation%20and%20separated%20KV%20cache.%20Second%2C%20xGR%20enables%20early%20sorting%20termination%20and%20mask-based%20item%20filtering%20with%20data%20structure%20reuse.%20Third%2C%20xGR%20reconstructs%20the%20overall%20pipeline%20to%20exploit%20multilevel%20overlap%20and%20multi-stream%20parallelism.%20Our%20experiments%20with%20real-world%20recommendation%20service%20datasets%20demonstrate%20that%20xGR%20achieves%20at%20least%203.49x%20throughput%20compared%20to%20the%20state-of-the-art%20baseline%20under%20strict%20latency%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2512.11529v1&entry.124074799=Read"},
{"title": "Open-World Object Counting in Videos", "author": "Niki Amini-Naieni and Andrew Zisserman", "abstract": "We introduce a new task of open-world object counting in videos: given a text description, or an image example, that specifies the target object, the objective is to enumerate all the unique instances of the target objects in the video. This task is especially challenging in crowded scenes with occlusions and objects of similar appearance, where avoiding double counting and identifying reappearances is crucial. To this end, we make the following contributions: we introduce a model, CountVid, for this task. It leverages an image-based counting model, and a promptable video segmentation and tracking model, to enable automated open-world object counting across video frames. To evaluate its performance, we introduce VideoCount, a new dataset for this novel task built from the TAO and MOT20 tracking datasets, as well as from videos of penguins and metal alloy crystallization captured by x-rays. Using this dataset, we demonstrate that CountVid provides accurate object counts, and significantly outperforms strong baselines. The VideoCount dataset, the CountVid model, and all the code are available at https://www.robots.ox.ac.uk/~vgg/research/countvid/.", "link": "http://arxiv.org/abs/2506.15368v2", "date": "2025-12-12", "relevancy": 2.0827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-World%20Object%20Counting%20in%20Videos&body=Title%3A%20Open-World%20Object%20Counting%20in%20Videos%0AAuthor%3A%20Niki%20Amini-Naieni%20and%20Andrew%20Zisserman%0AAbstract%3A%20We%20introduce%20a%20new%20task%20of%20open-world%20object%20counting%20in%20videos%3A%20given%20a%20text%20description%2C%20or%20an%20image%20example%2C%20that%20specifies%20the%20target%20object%2C%20the%20objective%20is%20to%20enumerate%20all%20the%20unique%20instances%20of%20the%20target%20objects%20in%20the%20video.%20This%20task%20is%20especially%20challenging%20in%20crowded%20scenes%20with%20occlusions%20and%20objects%20of%20similar%20appearance%2C%20where%20avoiding%20double%20counting%20and%20identifying%20reappearances%20is%20crucial.%20To%20this%20end%2C%20we%20make%20the%20following%20contributions%3A%20we%20introduce%20a%20model%2C%20CountVid%2C%20for%20this%20task.%20It%20leverages%20an%20image-based%20counting%20model%2C%20and%20a%20promptable%20video%20segmentation%20and%20tracking%20model%2C%20to%20enable%20automated%20open-world%20object%20counting%20across%20video%20frames.%20To%20evaluate%20its%20performance%2C%20we%20introduce%20VideoCount%2C%20a%20new%20dataset%20for%20this%20novel%20task%20built%20from%20the%20TAO%20and%20MOT20%20tracking%20datasets%2C%20as%20well%20as%20from%20videos%20of%20penguins%20and%20metal%20alloy%20crystallization%20captured%20by%20x-rays.%20Using%20this%20dataset%2C%20we%20demonstrate%20that%20CountVid%20provides%20accurate%20object%20counts%2C%20and%20significantly%20outperforms%20strong%20baselines.%20The%20VideoCount%20dataset%2C%20the%20CountVid%20model%2C%20and%20all%20the%20code%20are%20available%20at%20https%3A//www.robots.ox.ac.uk/~vgg/research/countvid/.%0ALink%3A%20http%3A//arxiv.org/abs/2506.15368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-World%2520Object%2520Counting%2520in%2520Videos%26entry.906535625%3DNiki%2520Amini-Naieni%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3DWe%2520introduce%2520a%2520new%2520task%2520of%2520open-world%2520object%2520counting%2520in%2520videos%253A%2520given%2520a%2520text%2520description%252C%2520or%2520an%2520image%2520example%252C%2520that%2520specifies%2520the%2520target%2520object%252C%2520the%2520objective%2520is%2520to%2520enumerate%2520all%2520the%2520unique%2520instances%2520of%2520the%2520target%2520objects%2520in%2520the%2520video.%2520This%2520task%2520is%2520especially%2520challenging%2520in%2520crowded%2520scenes%2520with%2520occlusions%2520and%2520objects%2520of%2520similar%2520appearance%252C%2520where%2520avoiding%2520double%2520counting%2520and%2520identifying%2520reappearances%2520is%2520crucial.%2520To%2520this%2520end%252C%2520we%2520make%2520the%2520following%2520contributions%253A%2520we%2520introduce%2520a%2520model%252C%2520CountVid%252C%2520for%2520this%2520task.%2520It%2520leverages%2520an%2520image-based%2520counting%2520model%252C%2520and%2520a%2520promptable%2520video%2520segmentation%2520and%2520tracking%2520model%252C%2520to%2520enable%2520automated%2520open-world%2520object%2520counting%2520across%2520video%2520frames.%2520To%2520evaluate%2520its%2520performance%252C%2520we%2520introduce%2520VideoCount%252C%2520a%2520new%2520dataset%2520for%2520this%2520novel%2520task%2520built%2520from%2520the%2520TAO%2520and%2520MOT20%2520tracking%2520datasets%252C%2520as%2520well%2520as%2520from%2520videos%2520of%2520penguins%2520and%2520metal%2520alloy%2520crystallization%2520captured%2520by%2520x-rays.%2520Using%2520this%2520dataset%252C%2520we%2520demonstrate%2520that%2520CountVid%2520provides%2520accurate%2520object%2520counts%252C%2520and%2520significantly%2520outperforms%2520strong%2520baselines.%2520The%2520VideoCount%2520dataset%252C%2520the%2520CountVid%2520model%252C%2520and%2520all%2520the%2520code%2520are%2520available%2520at%2520https%253A//www.robots.ox.ac.uk/~vgg/research/countvid/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-World%20Object%20Counting%20in%20Videos&entry.906535625=Niki%20Amini-Naieni%20and%20Andrew%20Zisserman&entry.1292438233=We%20introduce%20a%20new%20task%20of%20open-world%20object%20counting%20in%20videos%3A%20given%20a%20text%20description%2C%20or%20an%20image%20example%2C%20that%20specifies%20the%20target%20object%2C%20the%20objective%20is%20to%20enumerate%20all%20the%20unique%20instances%20of%20the%20target%20objects%20in%20the%20video.%20This%20task%20is%20especially%20challenging%20in%20crowded%20scenes%20with%20occlusions%20and%20objects%20of%20similar%20appearance%2C%20where%20avoiding%20double%20counting%20and%20identifying%20reappearances%20is%20crucial.%20To%20this%20end%2C%20we%20make%20the%20following%20contributions%3A%20we%20introduce%20a%20model%2C%20CountVid%2C%20for%20this%20task.%20It%20leverages%20an%20image-based%20counting%20model%2C%20and%20a%20promptable%20video%20segmentation%20and%20tracking%20model%2C%20to%20enable%20automated%20open-world%20object%20counting%20across%20video%20frames.%20To%20evaluate%20its%20performance%2C%20we%20introduce%20VideoCount%2C%20a%20new%20dataset%20for%20this%20novel%20task%20built%20from%20the%20TAO%20and%20MOT20%20tracking%20datasets%2C%20as%20well%20as%20from%20videos%20of%20penguins%20and%20metal%20alloy%20crystallization%20captured%20by%20x-rays.%20Using%20this%20dataset%2C%20we%20demonstrate%20that%20CountVid%20provides%20accurate%20object%20counts%2C%20and%20significantly%20outperforms%20strong%20baselines.%20The%20VideoCount%20dataset%2C%20the%20CountVid%20model%2C%20and%20all%20the%20code%20are%20available%20at%20https%3A//www.robots.ox.ac.uk/~vgg/research/countvid/.&entry.1838667208=http%3A//arxiv.org/abs/2506.15368v2&entry.124074799=Read"},
{"title": "Particle Image Velocimetry Refinement via Consensus ADMM", "author": "Alan Bonomi and Francesco Banelli and Antonio Terpin", "abstract": "Particle Image Velocimetry (PIV) is an imaging technique in experimental fluid dynamics that quantifies flow fields around bluff bodies by analyzing the displacement of neutrally buoyant tracer particles immersed in the fluid. Traditional PIV approaches typically depend on tuning parameters specific to the imaging setup, making the performance sensitive to variations in illumination, flow conditions, and seeding density. On the other hand, even state-of-the-art machine learning methods for flow quantification are fragile outside their training set. In our experiments, we observed that flow quantification would improve if different tunings (or algorithms) were applied to different regions of the same image pair. In this work, we parallelize the instantaneous flow quantification with multiple algorithms and adopt a consensus framework based on the alternating direction method of multipliers, seamlessly incorporating priors such as smoothness and incompressibility. We perform several numerical experiments to demonstrate the benefits of this approach. For instance, we achieve a decrease in end-point-error of up to 20% of a dense-inverse-search estimator at an inference rate of 60Hz, and we show how this performance boost can be increased further with outlier rejection. Our method is implemented in JAX, effectively exploiting hardware acceleration, and integrated in Flow Gym, enabling (i) reproducible comparisons with the state-of-the-art, (ii) testing different base algorithms, (iii) straightforward deployment for active fluids control applications.", "link": "http://arxiv.org/abs/2512.11695v1", "date": "2025-12-12", "relevancy": 2.082, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5224}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5216}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Particle%20Image%20Velocimetry%20Refinement%20via%20Consensus%20ADMM&body=Title%3A%20Particle%20Image%20Velocimetry%20Refinement%20via%20Consensus%20ADMM%0AAuthor%3A%20Alan%20Bonomi%20and%20Francesco%20Banelli%20and%20Antonio%20Terpin%0AAbstract%3A%20Particle%20Image%20Velocimetry%20%28PIV%29%20is%20an%20imaging%20technique%20in%20experimental%20fluid%20dynamics%20that%20quantifies%20flow%20fields%20around%20bluff%20bodies%20by%20analyzing%20the%20displacement%20of%20neutrally%20buoyant%20tracer%20particles%20immersed%20in%20the%20fluid.%20Traditional%20PIV%20approaches%20typically%20depend%20on%20tuning%20parameters%20specific%20to%20the%20imaging%20setup%2C%20making%20the%20performance%20sensitive%20to%20variations%20in%20illumination%2C%20flow%20conditions%2C%20and%20seeding%20density.%20On%20the%20other%20hand%2C%20even%20state-of-the-art%20machine%20learning%20methods%20for%20flow%20quantification%20are%20fragile%20outside%20their%20training%20set.%20In%20our%20experiments%2C%20we%20observed%20that%20flow%20quantification%20would%20improve%20if%20different%20tunings%20%28or%20algorithms%29%20were%20applied%20to%20different%20regions%20of%20the%20same%20image%20pair.%20In%20this%20work%2C%20we%20parallelize%20the%20instantaneous%20flow%20quantification%20with%20multiple%20algorithms%20and%20adopt%20a%20consensus%20framework%20based%20on%20the%20alternating%20direction%20method%20of%20multipliers%2C%20seamlessly%20incorporating%20priors%20such%20as%20smoothness%20and%20incompressibility.%20We%20perform%20several%20numerical%20experiments%20to%20demonstrate%20the%20benefits%20of%20this%20approach.%20For%20instance%2C%20we%20achieve%20a%20decrease%20in%20end-point-error%20of%20up%20to%2020%25%20of%20a%20dense-inverse-search%20estimator%20at%20an%20inference%20rate%20of%2060Hz%2C%20and%20we%20show%20how%20this%20performance%20boost%20can%20be%20increased%20further%20with%20outlier%20rejection.%20Our%20method%20is%20implemented%20in%20JAX%2C%20effectively%20exploiting%20hardware%20acceleration%2C%20and%20integrated%20in%20Flow%20Gym%2C%20enabling%20%28i%29%20reproducible%20comparisons%20with%20the%20state-of-the-art%2C%20%28ii%29%20testing%20different%20base%20algorithms%2C%20%28iii%29%20straightforward%20deployment%20for%20active%20fluids%20control%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParticle%2520Image%2520Velocimetry%2520Refinement%2520via%2520Consensus%2520ADMM%26entry.906535625%3DAlan%2520Bonomi%2520and%2520Francesco%2520Banelli%2520and%2520Antonio%2520Terpin%26entry.1292438233%3DParticle%2520Image%2520Velocimetry%2520%2528PIV%2529%2520is%2520an%2520imaging%2520technique%2520in%2520experimental%2520fluid%2520dynamics%2520that%2520quantifies%2520flow%2520fields%2520around%2520bluff%2520bodies%2520by%2520analyzing%2520the%2520displacement%2520of%2520neutrally%2520buoyant%2520tracer%2520particles%2520immersed%2520in%2520the%2520fluid.%2520Traditional%2520PIV%2520approaches%2520typically%2520depend%2520on%2520tuning%2520parameters%2520specific%2520to%2520the%2520imaging%2520setup%252C%2520making%2520the%2520performance%2520sensitive%2520to%2520variations%2520in%2520illumination%252C%2520flow%2520conditions%252C%2520and%2520seeding%2520density.%2520On%2520the%2520other%2520hand%252C%2520even%2520state-of-the-art%2520machine%2520learning%2520methods%2520for%2520flow%2520quantification%2520are%2520fragile%2520outside%2520their%2520training%2520set.%2520In%2520our%2520experiments%252C%2520we%2520observed%2520that%2520flow%2520quantification%2520would%2520improve%2520if%2520different%2520tunings%2520%2528or%2520algorithms%2529%2520were%2520applied%2520to%2520different%2520regions%2520of%2520the%2520same%2520image%2520pair.%2520In%2520this%2520work%252C%2520we%2520parallelize%2520the%2520instantaneous%2520flow%2520quantification%2520with%2520multiple%2520algorithms%2520and%2520adopt%2520a%2520consensus%2520framework%2520based%2520on%2520the%2520alternating%2520direction%2520method%2520of%2520multipliers%252C%2520seamlessly%2520incorporating%2520priors%2520such%2520as%2520smoothness%2520and%2520incompressibility.%2520We%2520perform%2520several%2520numerical%2520experiments%2520to%2520demonstrate%2520the%2520benefits%2520of%2520this%2520approach.%2520For%2520instance%252C%2520we%2520achieve%2520a%2520decrease%2520in%2520end-point-error%2520of%2520up%2520to%252020%2525%2520of%2520a%2520dense-inverse-search%2520estimator%2520at%2520an%2520inference%2520rate%2520of%252060Hz%252C%2520and%2520we%2520show%2520how%2520this%2520performance%2520boost%2520can%2520be%2520increased%2520further%2520with%2520outlier%2520rejection.%2520Our%2520method%2520is%2520implemented%2520in%2520JAX%252C%2520effectively%2520exploiting%2520hardware%2520acceleration%252C%2520and%2520integrated%2520in%2520Flow%2520Gym%252C%2520enabling%2520%2528i%2529%2520reproducible%2520comparisons%2520with%2520the%2520state-of-the-art%252C%2520%2528ii%2529%2520testing%2520different%2520base%2520algorithms%252C%2520%2528iii%2529%2520straightforward%2520deployment%2520for%2520active%2520fluids%2520control%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Particle%20Image%20Velocimetry%20Refinement%20via%20Consensus%20ADMM&entry.906535625=Alan%20Bonomi%20and%20Francesco%20Banelli%20and%20Antonio%20Terpin&entry.1292438233=Particle%20Image%20Velocimetry%20%28PIV%29%20is%20an%20imaging%20technique%20in%20experimental%20fluid%20dynamics%20that%20quantifies%20flow%20fields%20around%20bluff%20bodies%20by%20analyzing%20the%20displacement%20of%20neutrally%20buoyant%20tracer%20particles%20immersed%20in%20the%20fluid.%20Traditional%20PIV%20approaches%20typically%20depend%20on%20tuning%20parameters%20specific%20to%20the%20imaging%20setup%2C%20making%20the%20performance%20sensitive%20to%20variations%20in%20illumination%2C%20flow%20conditions%2C%20and%20seeding%20density.%20On%20the%20other%20hand%2C%20even%20state-of-the-art%20machine%20learning%20methods%20for%20flow%20quantification%20are%20fragile%20outside%20their%20training%20set.%20In%20our%20experiments%2C%20we%20observed%20that%20flow%20quantification%20would%20improve%20if%20different%20tunings%20%28or%20algorithms%29%20were%20applied%20to%20different%20regions%20of%20the%20same%20image%20pair.%20In%20this%20work%2C%20we%20parallelize%20the%20instantaneous%20flow%20quantification%20with%20multiple%20algorithms%20and%20adopt%20a%20consensus%20framework%20based%20on%20the%20alternating%20direction%20method%20of%20multipliers%2C%20seamlessly%20incorporating%20priors%20such%20as%20smoothness%20and%20incompressibility.%20We%20perform%20several%20numerical%20experiments%20to%20demonstrate%20the%20benefits%20of%20this%20approach.%20For%20instance%2C%20we%20achieve%20a%20decrease%20in%20end-point-error%20of%20up%20to%2020%25%20of%20a%20dense-inverse-search%20estimator%20at%20an%20inference%20rate%20of%2060Hz%2C%20and%20we%20show%20how%20this%20performance%20boost%20can%20be%20increased%20further%20with%20outlier%20rejection.%20Our%20method%20is%20implemented%20in%20JAX%2C%20effectively%20exploiting%20hardware%20acceleration%2C%20and%20integrated%20in%20Flow%20Gym%2C%20enabling%20%28i%29%20reproducible%20comparisons%20with%20the%20state-of-the-art%2C%20%28ii%29%20testing%20different%20base%20algorithms%2C%20%28iii%29%20straightforward%20deployment%20for%20active%20fluids%20control%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.11695v1&entry.124074799=Read"},
{"title": "SSL-MedSAM2: A Semi-supervised Medical Image Segmentation Framework Powered by Few-shot Learning of SAM2", "author": "Zhendi Gong and Xin Chen", "abstract": "Despite the success of deep learning based models in medical image segmentation, most state-of-the-art (SOTA) methods perform fully-supervised learning, which commonly rely on large scale annotated training datasets. However, medical image annotation is highly time-consuming, hindering its clinical applications. Semi-supervised learning (SSL) has been emerged as an appealing strategy in training with limited annotations, largely reducing the labelling cost. We propose a novel SSL framework SSL-MedSAM2, which contains a training-free few-shot learning branch TFFS-MedSAM2 based on the pretrained large foundation model Segment Anything Model 2 (SAM2) for pseudo label generation, and an iterative fully-supervised learning branch FSL-nnUNet based on nnUNet for pseudo label refinement. The results on MICCAI2025 challenge CARE-LiSeg (Liver Segmentation) demonstrate an outstanding performance of SSL-MedSAM2 among other methods. The average dice scores on the test set in GED4 and T1 MRI are 0.9710 and 0.9648 respectively, and the Hausdorff distances are 20.07 and 21.97 respectively. The code is available via https://github.com/naisops/SSL-MedSAM2/tree/main.", "link": "http://arxiv.org/abs/2512.11548v1", "date": "2025-12-12", "relevancy": 2.0819, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5423}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5415}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSL-MedSAM2%3A%20A%20Semi-supervised%20Medical%20Image%20Segmentation%20Framework%20Powered%20by%20Few-shot%20Learning%20of%20SAM2&body=Title%3A%20SSL-MedSAM2%3A%20A%20Semi-supervised%20Medical%20Image%20Segmentation%20Framework%20Powered%20by%20Few-shot%20Learning%20of%20SAM2%0AAuthor%3A%20Zhendi%20Gong%20and%20Xin%20Chen%0AAbstract%3A%20Despite%20the%20success%20of%20deep%20learning%20based%20models%20in%20medical%20image%20segmentation%2C%20most%20state-of-the-art%20%28SOTA%29%20methods%20perform%20fully-supervised%20learning%2C%20which%20commonly%20rely%20on%20large%20scale%20annotated%20training%20datasets.%20However%2C%20medical%20image%20annotation%20is%20highly%20time-consuming%2C%20hindering%20its%20clinical%20applications.%20Semi-supervised%20learning%20%28SSL%29%20has%20been%20emerged%20as%20an%20appealing%20strategy%20in%20training%20with%20limited%20annotations%2C%20largely%20reducing%20the%20labelling%20cost.%20We%20propose%20a%20novel%20SSL%20framework%20SSL-MedSAM2%2C%20which%20contains%20a%20training-free%20few-shot%20learning%20branch%20TFFS-MedSAM2%20based%20on%20the%20pretrained%20large%20foundation%20model%20Segment%20Anything%20Model%202%20%28SAM2%29%20for%20pseudo%20label%20generation%2C%20and%20an%20iterative%20fully-supervised%20learning%20branch%20FSL-nnUNet%20based%20on%20nnUNet%20for%20pseudo%20label%20refinement.%20The%20results%20on%20MICCAI2025%20challenge%20CARE-LiSeg%20%28Liver%20Segmentation%29%20demonstrate%20an%20outstanding%20performance%20of%20SSL-MedSAM2%20among%20other%20methods.%20The%20average%20dice%20scores%20on%20the%20test%20set%20in%20GED4%20and%20T1%20MRI%20are%200.9710%20and%200.9648%20respectively%2C%20and%20the%20Hausdorff%20distances%20are%2020.07%20and%2021.97%20respectively.%20The%20code%20is%20available%20via%20https%3A//github.com/naisops/SSL-MedSAM2/tree/main.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSL-MedSAM2%253A%2520A%2520Semi-supervised%2520Medical%2520Image%2520Segmentation%2520Framework%2520Powered%2520by%2520Few-shot%2520Learning%2520of%2520SAM2%26entry.906535625%3DZhendi%2520Gong%2520and%2520Xin%2520Chen%26entry.1292438233%3DDespite%2520the%2520success%2520of%2520deep%2520learning%2520based%2520models%2520in%2520medical%2520image%2520segmentation%252C%2520most%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520perform%2520fully-supervised%2520learning%252C%2520which%2520commonly%2520rely%2520on%2520large%2520scale%2520annotated%2520training%2520datasets.%2520However%252C%2520medical%2520image%2520annotation%2520is%2520highly%2520time-consuming%252C%2520hindering%2520its%2520clinical%2520applications.%2520Semi-supervised%2520learning%2520%2528SSL%2529%2520has%2520been%2520emerged%2520as%2520an%2520appealing%2520strategy%2520in%2520training%2520with%2520limited%2520annotations%252C%2520largely%2520reducing%2520the%2520labelling%2520cost.%2520We%2520propose%2520a%2520novel%2520SSL%2520framework%2520SSL-MedSAM2%252C%2520which%2520contains%2520a%2520training-free%2520few-shot%2520learning%2520branch%2520TFFS-MedSAM2%2520based%2520on%2520the%2520pretrained%2520large%2520foundation%2520model%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520for%2520pseudo%2520label%2520generation%252C%2520and%2520an%2520iterative%2520fully-supervised%2520learning%2520branch%2520FSL-nnUNet%2520based%2520on%2520nnUNet%2520for%2520pseudo%2520label%2520refinement.%2520The%2520results%2520on%2520MICCAI2025%2520challenge%2520CARE-LiSeg%2520%2528Liver%2520Segmentation%2529%2520demonstrate%2520an%2520outstanding%2520performance%2520of%2520SSL-MedSAM2%2520among%2520other%2520methods.%2520The%2520average%2520dice%2520scores%2520on%2520the%2520test%2520set%2520in%2520GED4%2520and%2520T1%2520MRI%2520are%25200.9710%2520and%25200.9648%2520respectively%252C%2520and%2520the%2520Hausdorff%2520distances%2520are%252020.07%2520and%252021.97%2520respectively.%2520The%2520code%2520is%2520available%2520via%2520https%253A//github.com/naisops/SSL-MedSAM2/tree/main.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSL-MedSAM2%3A%20A%20Semi-supervised%20Medical%20Image%20Segmentation%20Framework%20Powered%20by%20Few-shot%20Learning%20of%20SAM2&entry.906535625=Zhendi%20Gong%20and%20Xin%20Chen&entry.1292438233=Despite%20the%20success%20of%20deep%20learning%20based%20models%20in%20medical%20image%20segmentation%2C%20most%20state-of-the-art%20%28SOTA%29%20methods%20perform%20fully-supervised%20learning%2C%20which%20commonly%20rely%20on%20large%20scale%20annotated%20training%20datasets.%20However%2C%20medical%20image%20annotation%20is%20highly%20time-consuming%2C%20hindering%20its%20clinical%20applications.%20Semi-supervised%20learning%20%28SSL%29%20has%20been%20emerged%20as%20an%20appealing%20strategy%20in%20training%20with%20limited%20annotations%2C%20largely%20reducing%20the%20labelling%20cost.%20We%20propose%20a%20novel%20SSL%20framework%20SSL-MedSAM2%2C%20which%20contains%20a%20training-free%20few-shot%20learning%20branch%20TFFS-MedSAM2%20based%20on%20the%20pretrained%20large%20foundation%20model%20Segment%20Anything%20Model%202%20%28SAM2%29%20for%20pseudo%20label%20generation%2C%20and%20an%20iterative%20fully-supervised%20learning%20branch%20FSL-nnUNet%20based%20on%20nnUNet%20for%20pseudo%20label%20refinement.%20The%20results%20on%20MICCAI2025%20challenge%20CARE-LiSeg%20%28Liver%20Segmentation%29%20demonstrate%20an%20outstanding%20performance%20of%20SSL-MedSAM2%20among%20other%20methods.%20The%20average%20dice%20scores%20on%20the%20test%20set%20in%20GED4%20and%20T1%20MRI%20are%200.9710%20and%200.9648%20respectively%2C%20and%20the%20Hausdorff%20distances%20are%2020.07%20and%2021.97%20respectively.%20The%20code%20is%20available%20via%20https%3A//github.com/naisops/SSL-MedSAM2/tree/main.&entry.1838667208=http%3A//arxiv.org/abs/2512.11548v1&entry.124074799=Read"},
{"title": "YawDD+: Frame-level Annotations for Accurate Yawn Prediction", "author": "Ahmed Mujtaba and Gleb Radchenko and Marc Masana and Radu Prodan", "abstract": "Driver fatigue remains a leading cause of road accidents, with 24\\% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6\\% and mAP by 5\\% over video-level supervision, achieving 99.34\\% classification accuracy and 95.69\\% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.", "link": "http://arxiv.org/abs/2512.11446v1", "date": "2025-12-12", "relevancy": 2.0737, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5515}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5116}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YawDD%2B%3A%20Frame-level%20Annotations%20for%20Accurate%20Yawn%20Prediction&body=Title%3A%20YawDD%2B%3A%20Frame-level%20Annotations%20for%20Accurate%20Yawn%20Prediction%0AAuthor%3A%20Ahmed%20Mujtaba%20and%20Gleb%20Radchenko%20and%20Marc%20Masana%20and%20Radu%20Prodan%0AAbstract%3A%20Driver%20fatigue%20remains%20a%20leading%20cause%20of%20road%20accidents%2C%20with%2024%5C%25%20of%20crashes%20involving%20drowsy%20drivers.%20While%20yawning%20serves%20as%20an%20early%20behavioral%20indicator%20of%20fatigue%2C%20existing%20machine%20learning%20approaches%20face%20significant%20challenges%20due%20to%20video-annotated%20datasets%20that%20introduce%20systematic%20noise%20from%20coarse%20temporal%20annotations.%20We%20develop%20a%20semi-automated%20labeling%20pipeline%20with%20human-in-the-loop%20verification%2C%20which%20we%20apply%20to%20YawDD%2C%20enabling%20more%20accurate%20model%20training.%20Training%20the%20established%20MNasNet%20classifier%20and%20YOLOv11%20detector%20architectures%20on%20YawDD%2B%20improves%20frame%20accuracy%20by%20up%20to%206%5C%25%20and%20mAP%20by%205%5C%25%20over%20video-level%20supervision%2C%20achieving%2099.34%5C%25%20classification%20accuracy%20and%2095.69%5C%25%20detection%20mAP.%20The%20resulting%20approach%20deliver%20up%20to%2059.8%20FPS%20on%20edge%20AI%20hardware%20%28NVIDIA%20Jetson%20Nano%29%2C%20confirming%20that%20enhanced%20data%20quality%20alone%20supports%20on-device%20yawning%20monitoring%20without%20server-side%20computation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYawDD%252B%253A%2520Frame-level%2520Annotations%2520for%2520Accurate%2520Yawn%2520Prediction%26entry.906535625%3DAhmed%2520Mujtaba%2520and%2520Gleb%2520Radchenko%2520and%2520Marc%2520Masana%2520and%2520Radu%2520Prodan%26entry.1292438233%3DDriver%2520fatigue%2520remains%2520a%2520leading%2520cause%2520of%2520road%2520accidents%252C%2520with%252024%255C%2525%2520of%2520crashes%2520involving%2520drowsy%2520drivers.%2520While%2520yawning%2520serves%2520as%2520an%2520early%2520behavioral%2520indicator%2520of%2520fatigue%252C%2520existing%2520machine%2520learning%2520approaches%2520face%2520significant%2520challenges%2520due%2520to%2520video-annotated%2520datasets%2520that%2520introduce%2520systematic%2520noise%2520from%2520coarse%2520temporal%2520annotations.%2520We%2520develop%2520a%2520semi-automated%2520labeling%2520pipeline%2520with%2520human-in-the-loop%2520verification%252C%2520which%2520we%2520apply%2520to%2520YawDD%252C%2520enabling%2520more%2520accurate%2520model%2520training.%2520Training%2520the%2520established%2520MNasNet%2520classifier%2520and%2520YOLOv11%2520detector%2520architectures%2520on%2520YawDD%252B%2520improves%2520frame%2520accuracy%2520by%2520up%2520to%25206%255C%2525%2520and%2520mAP%2520by%25205%255C%2525%2520over%2520video-level%2520supervision%252C%2520achieving%252099.34%255C%2525%2520classification%2520accuracy%2520and%252095.69%255C%2525%2520detection%2520mAP.%2520The%2520resulting%2520approach%2520deliver%2520up%2520to%252059.8%2520FPS%2520on%2520edge%2520AI%2520hardware%2520%2528NVIDIA%2520Jetson%2520Nano%2529%252C%2520confirming%2520that%2520enhanced%2520data%2520quality%2520alone%2520supports%2520on-device%2520yawning%2520monitoring%2520without%2520server-side%2520computation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YawDD%2B%3A%20Frame-level%20Annotations%20for%20Accurate%20Yawn%20Prediction&entry.906535625=Ahmed%20Mujtaba%20and%20Gleb%20Radchenko%20and%20Marc%20Masana%20and%20Radu%20Prodan&entry.1292438233=Driver%20fatigue%20remains%20a%20leading%20cause%20of%20road%20accidents%2C%20with%2024%5C%25%20of%20crashes%20involving%20drowsy%20drivers.%20While%20yawning%20serves%20as%20an%20early%20behavioral%20indicator%20of%20fatigue%2C%20existing%20machine%20learning%20approaches%20face%20significant%20challenges%20due%20to%20video-annotated%20datasets%20that%20introduce%20systematic%20noise%20from%20coarse%20temporal%20annotations.%20We%20develop%20a%20semi-automated%20labeling%20pipeline%20with%20human-in-the-loop%20verification%2C%20which%20we%20apply%20to%20YawDD%2C%20enabling%20more%20accurate%20model%20training.%20Training%20the%20established%20MNasNet%20classifier%20and%20YOLOv11%20detector%20architectures%20on%20YawDD%2B%20improves%20frame%20accuracy%20by%20up%20to%206%5C%25%20and%20mAP%20by%205%5C%25%20over%20video-level%20supervision%2C%20achieving%2099.34%5C%25%20classification%20accuracy%20and%2095.69%5C%25%20detection%20mAP.%20The%20resulting%20approach%20deliver%20up%20to%2059.8%20FPS%20on%20edge%20AI%20hardware%20%28NVIDIA%20Jetson%20Nano%29%2C%20confirming%20that%20enhanced%20data%20quality%20alone%20supports%20on-device%20yawning%20monitoring%20without%20server-side%20computation.&entry.1838667208=http%3A//arxiv.org/abs/2512.11446v1&entry.124074799=Read"},
{"title": "Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers", "author": "Tommaso Belvedere and Michael Ziegltrum and Giulio Turrisi and Valerio Modugno", "abstract": "Model Predictive Path Integral control is a powerful sampling-based approach suitable for complex robotic tasks due to its flexibility in handling nonlinear dynamics and non-convex costs. However, its applicability in real-time, highfrequency robotic control scenarios is limited by computational demands. This paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments standard MPPI by computing local linear feedback gains derived from sensitivity analysis inspired by Riccati-based feedback used in gradient-based MPC. These gains allow for rapid closed-loop corrections around the current state without requiring full re-optimization at each timestep. We demonstrate the effectiveness of F-MPPI through simulations and real-world experiments on two robotic platforms: a quadrupedal robot performing dynamic locomotion on uneven terrain and a quadrotor executing aggressive maneuvers with onboard computation. Results illustrate that incorporating local feedback significantly improves control performance and stability, enabling robust, high-frequency operation suitable for complex robotic systems.", "link": "http://arxiv.org/abs/2506.14855v3", "date": "2025-12-12", "relevancy": 2.0726, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5746}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5106}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feedback-MPPI%3A%20Fast%20Sampling-Based%20MPC%20via%20Rollout%20Differentiation%20--%20Adios%20low-level%20controllers&body=Title%3A%20Feedback-MPPI%3A%20Fast%20Sampling-Based%20MPC%20via%20Rollout%20Differentiation%20--%20Adios%20low-level%20controllers%0AAuthor%3A%20Tommaso%20Belvedere%20and%20Michael%20Ziegltrum%20and%20Giulio%20Turrisi%20and%20Valerio%20Modugno%0AAbstract%3A%20Model%20Predictive%20Path%20Integral%20control%20is%20a%20powerful%20sampling-based%20approach%20suitable%20for%20complex%20robotic%20tasks%20due%20to%20its%20flexibility%20in%20handling%20nonlinear%20dynamics%20and%20non-convex%20costs.%20However%2C%20its%20applicability%20in%20real-time%2C%20highfrequency%20robotic%20control%20scenarios%20is%20limited%20by%20computational%20demands.%20This%20paper%20introduces%20Feedback-MPPI%20%28F-MPPI%29%2C%20a%20novel%20framework%20that%20augments%20standard%20MPPI%20by%20computing%20local%20linear%20feedback%20gains%20derived%20from%20sensitivity%20analysis%20inspired%20by%20Riccati-based%20feedback%20used%20in%20gradient-based%20MPC.%20These%20gains%20allow%20for%20rapid%20closed-loop%20corrections%20around%20the%20current%20state%20without%20requiring%20full%20re-optimization%20at%20each%20timestep.%20We%20demonstrate%20the%20effectiveness%20of%20F-MPPI%20through%20simulations%20and%20real-world%20experiments%20on%20two%20robotic%20platforms%3A%20a%20quadrupedal%20robot%20performing%20dynamic%20locomotion%20on%20uneven%20terrain%20and%20a%20quadrotor%20executing%20aggressive%20maneuvers%20with%20onboard%20computation.%20Results%20illustrate%20that%20incorporating%20local%20feedback%20significantly%20improves%20control%20performance%20and%20stability%2C%20enabling%20robust%2C%20high-frequency%20operation%20suitable%20for%20complex%20robotic%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2506.14855v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeedback-MPPI%253A%2520Fast%2520Sampling-Based%2520MPC%2520via%2520Rollout%2520Differentiation%2520--%2520Adios%2520low-level%2520controllers%26entry.906535625%3DTommaso%2520Belvedere%2520and%2520Michael%2520Ziegltrum%2520and%2520Giulio%2520Turrisi%2520and%2520Valerio%2520Modugno%26entry.1292438233%3DModel%2520Predictive%2520Path%2520Integral%2520control%2520is%2520a%2520powerful%2520sampling-based%2520approach%2520suitable%2520for%2520complex%2520robotic%2520tasks%2520due%2520to%2520its%2520flexibility%2520in%2520handling%2520nonlinear%2520dynamics%2520and%2520non-convex%2520costs.%2520However%252C%2520its%2520applicability%2520in%2520real-time%252C%2520highfrequency%2520robotic%2520control%2520scenarios%2520is%2520limited%2520by%2520computational%2520demands.%2520This%2520paper%2520introduces%2520Feedback-MPPI%2520%2528F-MPPI%2529%252C%2520a%2520novel%2520framework%2520that%2520augments%2520standard%2520MPPI%2520by%2520computing%2520local%2520linear%2520feedback%2520gains%2520derived%2520from%2520sensitivity%2520analysis%2520inspired%2520by%2520Riccati-based%2520feedback%2520used%2520in%2520gradient-based%2520MPC.%2520These%2520gains%2520allow%2520for%2520rapid%2520closed-loop%2520corrections%2520around%2520the%2520current%2520state%2520without%2520requiring%2520full%2520re-optimization%2520at%2520each%2520timestep.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520F-MPPI%2520through%2520simulations%2520and%2520real-world%2520experiments%2520on%2520two%2520robotic%2520platforms%253A%2520a%2520quadrupedal%2520robot%2520performing%2520dynamic%2520locomotion%2520on%2520uneven%2520terrain%2520and%2520a%2520quadrotor%2520executing%2520aggressive%2520maneuvers%2520with%2520onboard%2520computation.%2520Results%2520illustrate%2520that%2520incorporating%2520local%2520feedback%2520significantly%2520improves%2520control%2520performance%2520and%2520stability%252C%2520enabling%2520robust%252C%2520high-frequency%2520operation%2520suitable%2520for%2520complex%2520robotic%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14855v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feedback-MPPI%3A%20Fast%20Sampling-Based%20MPC%20via%20Rollout%20Differentiation%20--%20Adios%20low-level%20controllers&entry.906535625=Tommaso%20Belvedere%20and%20Michael%20Ziegltrum%20and%20Giulio%20Turrisi%20and%20Valerio%20Modugno&entry.1292438233=Model%20Predictive%20Path%20Integral%20control%20is%20a%20powerful%20sampling-based%20approach%20suitable%20for%20complex%20robotic%20tasks%20due%20to%20its%20flexibility%20in%20handling%20nonlinear%20dynamics%20and%20non-convex%20costs.%20However%2C%20its%20applicability%20in%20real-time%2C%20highfrequency%20robotic%20control%20scenarios%20is%20limited%20by%20computational%20demands.%20This%20paper%20introduces%20Feedback-MPPI%20%28F-MPPI%29%2C%20a%20novel%20framework%20that%20augments%20standard%20MPPI%20by%20computing%20local%20linear%20feedback%20gains%20derived%20from%20sensitivity%20analysis%20inspired%20by%20Riccati-based%20feedback%20used%20in%20gradient-based%20MPC.%20These%20gains%20allow%20for%20rapid%20closed-loop%20corrections%20around%20the%20current%20state%20without%20requiring%20full%20re-optimization%20at%20each%20timestep.%20We%20demonstrate%20the%20effectiveness%20of%20F-MPPI%20through%20simulations%20and%20real-world%20experiments%20on%20two%20robotic%20platforms%3A%20a%20quadrupedal%20robot%20performing%20dynamic%20locomotion%20on%20uneven%20terrain%20and%20a%20quadrotor%20executing%20aggressive%20maneuvers%20with%20onboard%20computation.%20Results%20illustrate%20that%20incorporating%20local%20feedback%20significantly%20improves%20control%20performance%20and%20stability%2C%20enabling%20robust%2C%20high-frequency%20operation%20suitable%20for%20complex%20robotic%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2506.14855v3&entry.124074799=Read"},
{"title": "A Variance-Based Analysis of Sample Complexity for Grid Coverage", "author": "Lyu Yuhuan", "abstract": "Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($\u03b4$), i.e., $M =O( \\tilde{C}\\ln(\\frac{2\\tilde{C}}\u03b4))$, which contrasts sharply with the classical linear $1/\u03b4$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $\u03b4\\to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes.", "link": "http://arxiv.org/abs/2511.17784v2", "date": "2025-12-12", "relevancy": 2.0712, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4182}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4158}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Variance-Based%20Analysis%20of%20Sample%20Complexity%20for%20Grid%20Coverage&body=Title%3A%20A%20Variance-Based%20Analysis%20of%20Sample%20Complexity%20for%20Grid%20Coverage%0AAuthor%3A%20Lyu%20Yuhuan%0AAbstract%3A%20Verifying%20uniform%20conditions%20over%20continuous%20spaces%20through%20random%20sampling%20is%20fundamental%20in%20machine%20learning%20and%20control%20theory%2C%20yet%20classical%20coverage%20analyses%20often%20yield%20conservative%20bounds%2C%20particularly%20at%20small%20failure%20probabilities.%20We%20study%20uniform%20random%20sampling%20on%20the%20%24d%24-dimensional%20unit%20hypercube%20and%20analyze%20the%20number%20of%20uncovered%20subcubes%20after%20discretization.%20By%20applying%20a%20concentration%20inequality%20to%20the%20uncovered-count%20statistic%2C%20we%20derive%20a%20sample%20complexity%20bound%20with%20a%20logarithmic%20dependence%20on%20the%20failure%20probability%20%28%24%CE%B4%24%29%2C%20i.e.%2C%20%24M%20%3DO%28%20%5Ctilde%7BC%7D%5Cln%28%5Cfrac%7B2%5Ctilde%7BC%7D%7D%CE%B4%29%29%24%2C%20which%20contrasts%20sharply%20with%20the%20classical%20linear%20%241/%CE%B4%24%20dependence.%20Under%20standard%20Lipschitz%20and%20uniformity%20assumptions%2C%20we%20present%20a%20self-contained%20derivation%20and%20compare%20our%20result%20with%20classical%20coupon-collector%20rates.%20Numerical%20studies%20across%20dimensions%2C%20precision%20levels%2C%20and%20confidence%20targets%20indicate%20that%20our%20bound%20tracks%20practical%20coverage%20requirements%20more%20tightly%20and%20scales%20favorably%20as%20%24%CE%B4%5Cto%200%24.%20Our%20findings%20offer%20a%20sharper%20theoretical%20tool%20for%20algorithms%20that%20rely%20on%20grid-based%20coverage%20guarantees%2C%20enabling%20more%20efficient%20sampling%2C%20especially%20in%20high-confidence%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Variance-Based%2520Analysis%2520of%2520Sample%2520Complexity%2520for%2520Grid%2520Coverage%26entry.906535625%3DLyu%2520Yuhuan%26entry.1292438233%3DVerifying%2520uniform%2520conditions%2520over%2520continuous%2520spaces%2520through%2520random%2520sampling%2520is%2520fundamental%2520in%2520machine%2520learning%2520and%2520control%2520theory%252C%2520yet%2520classical%2520coverage%2520analyses%2520often%2520yield%2520conservative%2520bounds%252C%2520particularly%2520at%2520small%2520failure%2520probabilities.%2520We%2520study%2520uniform%2520random%2520sampling%2520on%2520the%2520%2524d%2524-dimensional%2520unit%2520hypercube%2520and%2520analyze%2520the%2520number%2520of%2520uncovered%2520subcubes%2520after%2520discretization.%2520By%2520applying%2520a%2520concentration%2520inequality%2520to%2520the%2520uncovered-count%2520statistic%252C%2520we%2520derive%2520a%2520sample%2520complexity%2520bound%2520with%2520a%2520logarithmic%2520dependence%2520on%2520the%2520failure%2520probability%2520%2528%2524%25CE%25B4%2524%2529%252C%2520i.e.%252C%2520%2524M%2520%253DO%2528%2520%255Ctilde%257BC%257D%255Cln%2528%255Cfrac%257B2%255Ctilde%257BC%257D%257D%25CE%25B4%2529%2529%2524%252C%2520which%2520contrasts%2520sharply%2520with%2520the%2520classical%2520linear%2520%25241/%25CE%25B4%2524%2520dependence.%2520Under%2520standard%2520Lipschitz%2520and%2520uniformity%2520assumptions%252C%2520we%2520present%2520a%2520self-contained%2520derivation%2520and%2520compare%2520our%2520result%2520with%2520classical%2520coupon-collector%2520rates.%2520Numerical%2520studies%2520across%2520dimensions%252C%2520precision%2520levels%252C%2520and%2520confidence%2520targets%2520indicate%2520that%2520our%2520bound%2520tracks%2520practical%2520coverage%2520requirements%2520more%2520tightly%2520and%2520scales%2520favorably%2520as%2520%2524%25CE%25B4%255Cto%25200%2524.%2520Our%2520findings%2520offer%2520a%2520sharper%2520theoretical%2520tool%2520for%2520algorithms%2520that%2520rely%2520on%2520grid-based%2520coverage%2520guarantees%252C%2520enabling%2520more%2520efficient%2520sampling%252C%2520especially%2520in%2520high-confidence%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Variance-Based%20Analysis%20of%20Sample%20Complexity%20for%20Grid%20Coverage&entry.906535625=Lyu%20Yuhuan&entry.1292438233=Verifying%20uniform%20conditions%20over%20continuous%20spaces%20through%20random%20sampling%20is%20fundamental%20in%20machine%20learning%20and%20control%20theory%2C%20yet%20classical%20coverage%20analyses%20often%20yield%20conservative%20bounds%2C%20particularly%20at%20small%20failure%20probabilities.%20We%20study%20uniform%20random%20sampling%20on%20the%20%24d%24-dimensional%20unit%20hypercube%20and%20analyze%20the%20number%20of%20uncovered%20subcubes%20after%20discretization.%20By%20applying%20a%20concentration%20inequality%20to%20the%20uncovered-count%20statistic%2C%20we%20derive%20a%20sample%20complexity%20bound%20with%20a%20logarithmic%20dependence%20on%20the%20failure%20probability%20%28%24%CE%B4%24%29%2C%20i.e.%2C%20%24M%20%3DO%28%20%5Ctilde%7BC%7D%5Cln%28%5Cfrac%7B2%5Ctilde%7BC%7D%7D%CE%B4%29%29%24%2C%20which%20contrasts%20sharply%20with%20the%20classical%20linear%20%241/%CE%B4%24%20dependence.%20Under%20standard%20Lipschitz%20and%20uniformity%20assumptions%2C%20we%20present%20a%20self-contained%20derivation%20and%20compare%20our%20result%20with%20classical%20coupon-collector%20rates.%20Numerical%20studies%20across%20dimensions%2C%20precision%20levels%2C%20and%20confidence%20targets%20indicate%20that%20our%20bound%20tracks%20practical%20coverage%20requirements%20more%20tightly%20and%20scales%20favorably%20as%20%24%CE%B4%5Cto%200%24.%20Our%20findings%20offer%20a%20sharper%20theoretical%20tool%20for%20algorithms%20that%20rely%20on%20grid-based%20coverage%20guarantees%2C%20enabling%20more%20efficient%20sampling%2C%20especially%20in%20high-confidence%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2511.17784v2&entry.124074799=Read"},
{"title": "Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance", "author": "Gonca G\u00fcrsun", "abstract": "Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.\n  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.", "link": "http://arxiv.org/abs/2512.11421v1", "date": "2025-12-12", "relevancy": 2.07, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5355}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.519}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Trustworthy%20Multi-Turn%20LLM%20Agents%20via%20Behavioral%20Guidance&body=Title%3A%20Towards%20Trustworthy%20Multi-Turn%20LLM%20Agents%20via%20Behavioral%20Guidance%0AAuthor%3A%20Gonca%20G%C3%BCrsun%0AAbstract%3A%20Large%20Language%20Models%20demonstrate%20strong%20reasoning%20and%20generation%20abilities%2C%20yet%20their%20behavior%20in%20multi-turn%20tasks%20often%20lacks%20reliability%20and%20verifiability.%20We%20present%20a%20task%20completion%20framework%20that%20enables%20LLM-based%20agents%20to%20act%20under%20explicit%20behavioral%20guidance%20in%20environments%20described%20by%20reinforcement%20learning%20formalisms%20with%20defined%20observation%2C%20action%2C%20and%20reward%20signals.%0A%20%20The%20framework%20integrates%20three%20components%3A%20a%20lightweight%20task%20profiler%20that%20selects%20reasoning%20and%20generation%20strategies%2C%20a%20reasoning%20module%20that%20learns%20verifiable%20observation%20-%20action%20mappings%2C%20and%20a%20generation%20module%20that%20enforces%20constraint-compliant%20outputs%20through%20validation%20or%20deterministic%20synthesis.%20We%20show%20that%20as%20the%20agent%20interacts%20with%20the%20environment%2C%20these%20components%20co-evolve%2C%20yielding%20trustworthy%20behavior.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Trustworthy%2520Multi-Turn%2520LLM%2520Agents%2520via%2520Behavioral%2520Guidance%26entry.906535625%3DGonca%2520G%25C3%25BCrsun%26entry.1292438233%3DLarge%2520Language%2520Models%2520demonstrate%2520strong%2520reasoning%2520and%2520generation%2520abilities%252C%2520yet%2520their%2520behavior%2520in%2520multi-turn%2520tasks%2520often%2520lacks%2520reliability%2520and%2520verifiability.%2520We%2520present%2520a%2520task%2520completion%2520framework%2520that%2520enables%2520LLM-based%2520agents%2520to%2520act%2520under%2520explicit%2520behavioral%2520guidance%2520in%2520environments%2520described%2520by%2520reinforcement%2520learning%2520formalisms%2520with%2520defined%2520observation%252C%2520action%252C%2520and%2520reward%2520signals.%250A%2520%2520The%2520framework%2520integrates%2520three%2520components%253A%2520a%2520lightweight%2520task%2520profiler%2520that%2520selects%2520reasoning%2520and%2520generation%2520strategies%252C%2520a%2520reasoning%2520module%2520that%2520learns%2520verifiable%2520observation%2520-%2520action%2520mappings%252C%2520and%2520a%2520generation%2520module%2520that%2520enforces%2520constraint-compliant%2520outputs%2520through%2520validation%2520or%2520deterministic%2520synthesis.%2520We%2520show%2520that%2520as%2520the%2520agent%2520interacts%2520with%2520the%2520environment%252C%2520these%2520components%2520co-evolve%252C%2520yielding%2520trustworthy%2520behavior.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Trustworthy%20Multi-Turn%20LLM%20Agents%20via%20Behavioral%20Guidance&entry.906535625=Gonca%20G%C3%BCrsun&entry.1292438233=Large%20Language%20Models%20demonstrate%20strong%20reasoning%20and%20generation%20abilities%2C%20yet%20their%20behavior%20in%20multi-turn%20tasks%20often%20lacks%20reliability%20and%20verifiability.%20We%20present%20a%20task%20completion%20framework%20that%20enables%20LLM-based%20agents%20to%20act%20under%20explicit%20behavioral%20guidance%20in%20environments%20described%20by%20reinforcement%20learning%20formalisms%20with%20defined%20observation%2C%20action%2C%20and%20reward%20signals.%0A%20%20The%20framework%20integrates%20three%20components%3A%20a%20lightweight%20task%20profiler%20that%20selects%20reasoning%20and%20generation%20strategies%2C%20a%20reasoning%20module%20that%20learns%20verifiable%20observation%20-%20action%20mappings%2C%20and%20a%20generation%20module%20that%20enforces%20constraint-compliant%20outputs%20through%20validation%20or%20deterministic%20synthesis.%20We%20show%20that%20as%20the%20agent%20interacts%20with%20the%20environment%2C%20these%20components%20co-evolve%2C%20yielding%20trustworthy%20behavior.&entry.1838667208=http%3A//arxiv.org/abs/2512.11421v1&entry.124074799=Read"},
{"title": "M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation", "author": "Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu", "abstract": "In this paper, we introduce a new embedding model called M3-Embedding, which is distinguished for its versatility in \\textit{Multi-Linguality}, \\textit{Multi-Functionality}, and \\textit{Multi-Granularity}. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3-Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross-lingual, and long-document retrieval benchmarks.", "link": "http://arxiv.org/abs/2402.03216v5", "date": "2025-12-12", "relevancy": 2.0659, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5239}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5235}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3-Embedding%3A%20Multi-Linguality%2C%20Multi-Functionality%2C%20Multi-Granularity%20Text%20Embeddings%20Through%20Self-Knowledge%20Distillation&body=Title%3A%20M3-Embedding%3A%20Multi-Linguality%2C%20Multi-Functionality%2C%20Multi-Granularity%20Text%20Embeddings%20Through%20Self-Knowledge%20Distillation%0AAuthor%3A%20Jianlv%20Chen%20and%20Shitao%20Xiao%20and%20Peitian%20Zhang%20and%20Kun%20Luo%20and%20Defu%20Lian%20and%20Zheng%20Liu%0AAbstract%3A%20In%20this%20paper%2C%20we%20introduce%20a%20new%20embedding%20model%20called%20M3-Embedding%2C%20which%20is%20distinguished%20for%20its%20versatility%20in%20%5Ctextit%7BMulti-Linguality%7D%2C%20%5Ctextit%7BMulti-Functionality%7D%2C%20and%20%5Ctextit%7BMulti-Granularity%7D.%20It%20provides%20a%20uniform%20support%20for%20the%20semantic%20retrieval%20of%20more%20than%20100%20working%20languages.%20It%20can%20simultaneously%20accomplish%20the%20three%20common%20retrieval%20functionalities%3A%20dense%20retrieval%2C%20multi-vector%20retrieval%2C%20and%20sparse%20retrieval.%20Besides%2C%20it%20is%20also%20capable%20of%20processing%20inputs%20of%20different%20granularities%2C%20spanning%20from%20short%20sentences%20to%20long%20documents%20of%20up%20to%208%2C192%20tokens.%20The%20effective%20training%20of%20M3-Embedding%20presents%20a%20series%20of%20technical%20contributions.%20Notably%2C%20we%20propose%20a%20novel%20self-knowledge%20distillation%20approach%2C%20where%20the%20relevance%20scores%20from%20different%20retrieval%20functionalities%20can%20be%20integrated%20as%20the%20teacher%20signal%20to%20enhance%20the%20training%20quality.%20We%20also%20optimize%20the%20batching%20strategy%2C%20which%20enables%20a%20large%20batch%20size%20and%20high%20training%20throughput%20to%20improve%20the%20discriminativeness%20of%20embeddings.%20M3-Embedding%20exhibits%20a%20superior%20performance%20in%20our%20experiment%2C%20leading%20to%20new%20state-of-the-art%20results%20on%20multilingual%2C%20cross-lingual%2C%20and%20long-document%20retrieval%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2402.03216v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3-Embedding%253A%2520Multi-Linguality%252C%2520Multi-Functionality%252C%2520Multi-Granularity%2520Text%2520Embeddings%2520Through%2520Self-Knowledge%2520Distillation%26entry.906535625%3DJianlv%2520Chen%2520and%2520Shitao%2520Xiao%2520and%2520Peitian%2520Zhang%2520and%2520Kun%2520Luo%2520and%2520Defu%2520Lian%2520and%2520Zheng%2520Liu%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520embedding%2520model%2520called%2520M3-Embedding%252C%2520which%2520is%2520distinguished%2520for%2520its%2520versatility%2520in%2520%255Ctextit%257BMulti-Linguality%257D%252C%2520%255Ctextit%257BMulti-Functionality%257D%252C%2520and%2520%255Ctextit%257BMulti-Granularity%257D.%2520It%2520provides%2520a%2520uniform%2520support%2520for%2520the%2520semantic%2520retrieval%2520of%2520more%2520than%2520100%2520working%2520languages.%2520It%2520can%2520simultaneously%2520accomplish%2520the%2520three%2520common%2520retrieval%2520functionalities%253A%2520dense%2520retrieval%252C%2520multi-vector%2520retrieval%252C%2520and%2520sparse%2520retrieval.%2520Besides%252C%2520it%2520is%2520also%2520capable%2520of%2520processing%2520inputs%2520of%2520different%2520granularities%252C%2520spanning%2520from%2520short%2520sentences%2520to%2520long%2520documents%2520of%2520up%2520to%25208%252C192%2520tokens.%2520The%2520effective%2520training%2520of%2520M3-Embedding%2520presents%2520a%2520series%2520of%2520technical%2520contributions.%2520Notably%252C%2520we%2520propose%2520a%2520novel%2520self-knowledge%2520distillation%2520approach%252C%2520where%2520the%2520relevance%2520scores%2520from%2520different%2520retrieval%2520functionalities%2520can%2520be%2520integrated%2520as%2520the%2520teacher%2520signal%2520to%2520enhance%2520the%2520training%2520quality.%2520We%2520also%2520optimize%2520the%2520batching%2520strategy%252C%2520which%2520enables%2520a%2520large%2520batch%2520size%2520and%2520high%2520training%2520throughput%2520to%2520improve%2520the%2520discriminativeness%2520of%2520embeddings.%2520M3-Embedding%2520exhibits%2520a%2520superior%2520performance%2520in%2520our%2520experiment%252C%2520leading%2520to%2520new%2520state-of-the-art%2520results%2520on%2520multilingual%252C%2520cross-lingual%252C%2520and%2520long-document%2520retrieval%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03216v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3-Embedding%3A%20Multi-Linguality%2C%20Multi-Functionality%2C%20Multi-Granularity%20Text%20Embeddings%20Through%20Self-Knowledge%20Distillation&entry.906535625=Jianlv%20Chen%20and%20Shitao%20Xiao%20and%20Peitian%20Zhang%20and%20Kun%20Luo%20and%20Defu%20Lian%20and%20Zheng%20Liu&entry.1292438233=In%20this%20paper%2C%20we%20introduce%20a%20new%20embedding%20model%20called%20M3-Embedding%2C%20which%20is%20distinguished%20for%20its%20versatility%20in%20%5Ctextit%7BMulti-Linguality%7D%2C%20%5Ctextit%7BMulti-Functionality%7D%2C%20and%20%5Ctextit%7BMulti-Granularity%7D.%20It%20provides%20a%20uniform%20support%20for%20the%20semantic%20retrieval%20of%20more%20than%20100%20working%20languages.%20It%20can%20simultaneously%20accomplish%20the%20three%20common%20retrieval%20functionalities%3A%20dense%20retrieval%2C%20multi-vector%20retrieval%2C%20and%20sparse%20retrieval.%20Besides%2C%20it%20is%20also%20capable%20of%20processing%20inputs%20of%20different%20granularities%2C%20spanning%20from%20short%20sentences%20to%20long%20documents%20of%20up%20to%208%2C192%20tokens.%20The%20effective%20training%20of%20M3-Embedding%20presents%20a%20series%20of%20technical%20contributions.%20Notably%2C%20we%20propose%20a%20novel%20self-knowledge%20distillation%20approach%2C%20where%20the%20relevance%20scores%20from%20different%20retrieval%20functionalities%20can%20be%20integrated%20as%20the%20teacher%20signal%20to%20enhance%20the%20training%20quality.%20We%20also%20optimize%20the%20batching%20strategy%2C%20which%20enables%20a%20large%20batch%20size%20and%20high%20training%20throughput%20to%20improve%20the%20discriminativeness%20of%20embeddings.%20M3-Embedding%20exhibits%20a%20superior%20performance%20in%20our%20experiment%2C%20leading%20to%20new%20state-of-the-art%20results%20on%20multilingual%2C%20cross-lingual%2C%20and%20long-document%20retrieval%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2402.03216v5&entry.124074799=Read"},
{"title": "The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation", "author": "Vladimer Khasia", "abstract": "Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua.", "link": "http://arxiv.org/abs/2512.11776v1", "date": "2025-12-12", "relevancy": 2.0553, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5254}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Adaptive%20Vekua%20Cascade%3A%20A%20Differentiable%20Spectral-Analytic%20Solver%20for%20Physics-Informed%20Representation&body=Title%3A%20The%20Adaptive%20Vekua%20Cascade%3A%20A%20Differentiable%20Spectral-Analytic%20Solver%20for%20Physics-Informed%20Representation%0AAuthor%3A%20Vladimer%20Khasia%0AAbstract%3A%20Coordinate-based%20neural%20networks%20have%20emerged%20as%20a%20powerful%20tool%20for%20representing%20continuous%20physical%20fields%2C%20yet%20they%20face%20two%20fundamental%20pathologies%3A%20spectral%20bias%2C%20which%20hinders%20the%20learning%20of%20high-frequency%20dynamics%2C%20and%20the%20curse%20of%20dimensionality%2C%20which%20causes%20parameter%20explosion%20in%20discrete%20feature%20grids.%20We%20propose%20the%20Adaptive%20Vekua%20Cascade%20%28AVC%29%2C%20a%20hybrid%20architecture%20that%20bridges%20deep%20learning%20and%20classical%20approximation%20theory.%20AVC%20decouples%20manifold%20learning%20from%20function%20approximation%20by%20using%20a%20deep%20network%20to%20learn%20a%20diffeomorphic%20warping%20of%20the%20physical%20domain%2C%20projecting%20complex%20spatiotemporal%20dynamics%20onto%20a%20latent%20manifold%20where%20the%20solution%20is%20represented%20by%20a%20basis%20of%20generalized%20analytic%20functions.%20Crucially%2C%20we%20replace%20the%20standard%20gradient-descent%20output%20layer%20with%20a%20differentiable%20linear%20solver%2C%20allowing%20the%20network%20to%20optimally%20resolve%20spectral%20coefficients%20in%20a%20closed%20form%20during%20the%20forward%20pass.%20We%20evaluate%20AVC%20on%20a%20suite%20of%20five%20rigorous%20physics%20benchmarks%2C%20including%20high-frequency%20Helmholtz%20wave%20propagation%2C%20sparse%20medical%20reconstruction%2C%20and%20unsteady%203D%20Navier-Stokes%20turbulence.%20Our%20results%20demonstrate%20that%20AVC%20achieves%20state-of-the-art%20accuracy%20while%20reducing%20parameter%20counts%20by%20orders%20of%20magnitude%20%28e.g.%2C%20840%20parameters%20vs.%204.2%20million%20for%203D%20grids%29%20and%20converging%202-3x%20faster%20than%20implicit%20neural%20representations.%20This%20work%20establishes%20a%20new%20paradigm%20for%20memory-efficient%2C%20spectrally%20accurate%20scientific%20machine%20learning.%20The%20code%20is%20available%20at%20https%3A//github.com/VladimerKhasia/vecua.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Adaptive%2520Vekua%2520Cascade%253A%2520A%2520Differentiable%2520Spectral-Analytic%2520Solver%2520for%2520Physics-Informed%2520Representation%26entry.906535625%3DVladimer%2520Khasia%26entry.1292438233%3DCoordinate-based%2520neural%2520networks%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520representing%2520continuous%2520physical%2520fields%252C%2520yet%2520they%2520face%2520two%2520fundamental%2520pathologies%253A%2520spectral%2520bias%252C%2520which%2520hinders%2520the%2520learning%2520of%2520high-frequency%2520dynamics%252C%2520and%2520the%2520curse%2520of%2520dimensionality%252C%2520which%2520causes%2520parameter%2520explosion%2520in%2520discrete%2520feature%2520grids.%2520We%2520propose%2520the%2520Adaptive%2520Vekua%2520Cascade%2520%2528AVC%2529%252C%2520a%2520hybrid%2520architecture%2520that%2520bridges%2520deep%2520learning%2520and%2520classical%2520approximation%2520theory.%2520AVC%2520decouples%2520manifold%2520learning%2520from%2520function%2520approximation%2520by%2520using%2520a%2520deep%2520network%2520to%2520learn%2520a%2520diffeomorphic%2520warping%2520of%2520the%2520physical%2520domain%252C%2520projecting%2520complex%2520spatiotemporal%2520dynamics%2520onto%2520a%2520latent%2520manifold%2520where%2520the%2520solution%2520is%2520represented%2520by%2520a%2520basis%2520of%2520generalized%2520analytic%2520functions.%2520Crucially%252C%2520we%2520replace%2520the%2520standard%2520gradient-descent%2520output%2520layer%2520with%2520a%2520differentiable%2520linear%2520solver%252C%2520allowing%2520the%2520network%2520to%2520optimally%2520resolve%2520spectral%2520coefficients%2520in%2520a%2520closed%2520form%2520during%2520the%2520forward%2520pass.%2520We%2520evaluate%2520AVC%2520on%2520a%2520suite%2520of%2520five%2520rigorous%2520physics%2520benchmarks%252C%2520including%2520high-frequency%2520Helmholtz%2520wave%2520propagation%252C%2520sparse%2520medical%2520reconstruction%252C%2520and%2520unsteady%25203D%2520Navier-Stokes%2520turbulence.%2520Our%2520results%2520demonstrate%2520that%2520AVC%2520achieves%2520state-of-the-art%2520accuracy%2520while%2520reducing%2520parameter%2520counts%2520by%2520orders%2520of%2520magnitude%2520%2528e.g.%252C%2520840%2520parameters%2520vs.%25204.2%2520million%2520for%25203D%2520grids%2529%2520and%2520converging%25202-3x%2520faster%2520than%2520implicit%2520neural%2520representations.%2520This%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%2520memory-efficient%252C%2520spectrally%2520accurate%2520scientific%2520machine%2520learning.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/VladimerKhasia/vecua.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Adaptive%20Vekua%20Cascade%3A%20A%20Differentiable%20Spectral-Analytic%20Solver%20for%20Physics-Informed%20Representation&entry.906535625=Vladimer%20Khasia&entry.1292438233=Coordinate-based%20neural%20networks%20have%20emerged%20as%20a%20powerful%20tool%20for%20representing%20continuous%20physical%20fields%2C%20yet%20they%20face%20two%20fundamental%20pathologies%3A%20spectral%20bias%2C%20which%20hinders%20the%20learning%20of%20high-frequency%20dynamics%2C%20and%20the%20curse%20of%20dimensionality%2C%20which%20causes%20parameter%20explosion%20in%20discrete%20feature%20grids.%20We%20propose%20the%20Adaptive%20Vekua%20Cascade%20%28AVC%29%2C%20a%20hybrid%20architecture%20that%20bridges%20deep%20learning%20and%20classical%20approximation%20theory.%20AVC%20decouples%20manifold%20learning%20from%20function%20approximation%20by%20using%20a%20deep%20network%20to%20learn%20a%20diffeomorphic%20warping%20of%20the%20physical%20domain%2C%20projecting%20complex%20spatiotemporal%20dynamics%20onto%20a%20latent%20manifold%20where%20the%20solution%20is%20represented%20by%20a%20basis%20of%20generalized%20analytic%20functions.%20Crucially%2C%20we%20replace%20the%20standard%20gradient-descent%20output%20layer%20with%20a%20differentiable%20linear%20solver%2C%20allowing%20the%20network%20to%20optimally%20resolve%20spectral%20coefficients%20in%20a%20closed%20form%20during%20the%20forward%20pass.%20We%20evaluate%20AVC%20on%20a%20suite%20of%20five%20rigorous%20physics%20benchmarks%2C%20including%20high-frequency%20Helmholtz%20wave%20propagation%2C%20sparse%20medical%20reconstruction%2C%20and%20unsteady%203D%20Navier-Stokes%20turbulence.%20Our%20results%20demonstrate%20that%20AVC%20achieves%20state-of-the-art%20accuracy%20while%20reducing%20parameter%20counts%20by%20orders%20of%20magnitude%20%28e.g.%2C%20840%20parameters%20vs.%204.2%20million%20for%203D%20grids%29%20and%20converging%202-3x%20faster%20than%20implicit%20neural%20representations.%20This%20work%20establishes%20a%20new%20paradigm%20for%20memory-efficient%2C%20spectrally%20accurate%20scientific%20machine%20learning.%20The%20code%20is%20available%20at%20https%3A//github.com/VladimerKhasia/vecua.&entry.1838667208=http%3A//arxiv.org/abs/2512.11776v1&entry.124074799=Read"},
{"title": "Fully Inductive Node Representation Learning via Graph View Transformation", "author": "Dooho Lee and Myeong Kong and Minho Jeong and Jaemin Yoo", "abstract": "Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.", "link": "http://arxiv.org/abs/2512.11561v1", "date": "2025-12-12", "relevancy": 2.0401, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5213}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.512}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20Inductive%20Node%20Representation%20Learning%20via%20Graph%20View%20Transformation&body=Title%3A%20Fully%20Inductive%20Node%20Representation%20Learning%20via%20Graph%20View%20Transformation%0AAuthor%3A%20Dooho%20Lee%20and%20Myeong%20Kong%20and%20Minho%20Jeong%20and%20Jaemin%20Yoo%0AAbstract%3A%20Generalizing%20a%20pretrained%20model%20to%20unseen%20datasets%20without%20retraining%20is%20an%20essential%20step%20toward%20a%20foundation%20model.%20However%2C%20achieving%20such%20cross-dataset%2C%20fully%20inductive%20inference%20is%20difficult%20in%20graph-structured%20data%20where%20feature%20spaces%20vary%20widely%20in%20both%20dimensionality%20and%20semantics.%20Any%20transformation%20in%20the%20feature%20space%20can%20easily%20violate%20the%20inductive%20applicability%20to%20unseen%20datasets%2C%20strictly%20limiting%20the%20design%20space%20of%20a%20graph%20model.%20In%20this%20work%2C%20we%20introduce%20the%20view%20space%2C%20a%20novel%20representational%20axis%20in%20which%20arbitrary%20graphs%20can%20be%20naturally%20encoded%20in%20a%20unified%20manner.%20We%20then%20propose%20Graph%20View%20Transformation%20%28GVT%29%2C%20a%20node-%20and%20feature-permutation-equivariant%20mapping%20in%20the%20view%20space.%20GVT%20serves%20as%20the%20building%20block%20for%20Recurrent%20GVT%2C%20a%20fully%20inductive%20model%20for%20node%20representation%20learning.%20Pretrained%20on%20OGBN-Arxiv%20and%20evaluated%20on%2027%20node-classification%20benchmarks%2C%20Recurrent%20GVT%20outperforms%20GraphAny%2C%20the%20prior%20fully%20inductive%20graph%20model%2C%20by%20%2B8.93%25%20and%20surpasses%2012%20individually%20tuned%20GNNs%20by%20at%20least%20%2B3.30%25.%20These%20results%20establish%20the%20view%20space%20as%20a%20principled%20and%20effective%20ground%20for%20fully%20inductive%20node%20representation%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520Inductive%2520Node%2520Representation%2520Learning%2520via%2520Graph%2520View%2520Transformation%26entry.906535625%3DDooho%2520Lee%2520and%2520Myeong%2520Kong%2520and%2520Minho%2520Jeong%2520and%2520Jaemin%2520Yoo%26entry.1292438233%3DGeneralizing%2520a%2520pretrained%2520model%2520to%2520unseen%2520datasets%2520without%2520retraining%2520is%2520an%2520essential%2520step%2520toward%2520a%2520foundation%2520model.%2520However%252C%2520achieving%2520such%2520cross-dataset%252C%2520fully%2520inductive%2520inference%2520is%2520difficult%2520in%2520graph-structured%2520data%2520where%2520feature%2520spaces%2520vary%2520widely%2520in%2520both%2520dimensionality%2520and%2520semantics.%2520Any%2520transformation%2520in%2520the%2520feature%2520space%2520can%2520easily%2520violate%2520the%2520inductive%2520applicability%2520to%2520unseen%2520datasets%252C%2520strictly%2520limiting%2520the%2520design%2520space%2520of%2520a%2520graph%2520model.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520view%2520space%252C%2520a%2520novel%2520representational%2520axis%2520in%2520which%2520arbitrary%2520graphs%2520can%2520be%2520naturally%2520encoded%2520in%2520a%2520unified%2520manner.%2520We%2520then%2520propose%2520Graph%2520View%2520Transformation%2520%2528GVT%2529%252C%2520a%2520node-%2520and%2520feature-permutation-equivariant%2520mapping%2520in%2520the%2520view%2520space.%2520GVT%2520serves%2520as%2520the%2520building%2520block%2520for%2520Recurrent%2520GVT%252C%2520a%2520fully%2520inductive%2520model%2520for%2520node%2520representation%2520learning.%2520Pretrained%2520on%2520OGBN-Arxiv%2520and%2520evaluated%2520on%252027%2520node-classification%2520benchmarks%252C%2520Recurrent%2520GVT%2520outperforms%2520GraphAny%252C%2520the%2520prior%2520fully%2520inductive%2520graph%2520model%252C%2520by%2520%252B8.93%2525%2520and%2520surpasses%252012%2520individually%2520tuned%2520GNNs%2520by%2520at%2520least%2520%252B3.30%2525.%2520These%2520results%2520establish%2520the%2520view%2520space%2520as%2520a%2520principled%2520and%2520effective%2520ground%2520for%2520fully%2520inductive%2520node%2520representation%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Inductive%20Node%20Representation%20Learning%20via%20Graph%20View%20Transformation&entry.906535625=Dooho%20Lee%20and%20Myeong%20Kong%20and%20Minho%20Jeong%20and%20Jaemin%20Yoo&entry.1292438233=Generalizing%20a%20pretrained%20model%20to%20unseen%20datasets%20without%20retraining%20is%20an%20essential%20step%20toward%20a%20foundation%20model.%20However%2C%20achieving%20such%20cross-dataset%2C%20fully%20inductive%20inference%20is%20difficult%20in%20graph-structured%20data%20where%20feature%20spaces%20vary%20widely%20in%20both%20dimensionality%20and%20semantics.%20Any%20transformation%20in%20the%20feature%20space%20can%20easily%20violate%20the%20inductive%20applicability%20to%20unseen%20datasets%2C%20strictly%20limiting%20the%20design%20space%20of%20a%20graph%20model.%20In%20this%20work%2C%20we%20introduce%20the%20view%20space%2C%20a%20novel%20representational%20axis%20in%20which%20arbitrary%20graphs%20can%20be%20naturally%20encoded%20in%20a%20unified%20manner.%20We%20then%20propose%20Graph%20View%20Transformation%20%28GVT%29%2C%20a%20node-%20and%20feature-permutation-equivariant%20mapping%20in%20the%20view%20space.%20GVT%20serves%20as%20the%20building%20block%20for%20Recurrent%20GVT%2C%20a%20fully%20inductive%20model%20for%20node%20representation%20learning.%20Pretrained%20on%20OGBN-Arxiv%20and%20evaluated%20on%2027%20node-classification%20benchmarks%2C%20Recurrent%20GVT%20outperforms%20GraphAny%2C%20the%20prior%20fully%20inductive%20graph%20model%2C%20by%20%2B8.93%25%20and%20surpasses%2012%20individually%20tuned%20GNNs%20by%20at%20least%20%2B3.30%25.%20These%20results%20establish%20the%20view%20space%20as%20a%20principled%20and%20effective%20ground%20for%20fully%20inductive%20node%20representation%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.11561v1&entry.124074799=Read"},
{"title": "Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs", "author": "Mohor Banerjee and Nadya Yuki Wangsajaya and Syed Ali Redha Alsagoff and Min Sen Tan and Zachary Choy Kit Chun and Alvin Chan Guo Wei", "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.", "link": "http://arxiv.org/abs/2512.11509v1", "date": "2025-12-12", "relevancy": 2.0369, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Less%20Hallucination%20Mean%20Less%20Creativity%3F%20An%20Empirical%20Investigation%20in%20LLMs&body=Title%3A%20Does%20Less%20Hallucination%20Mean%20Less%20Creativity%3F%20An%20Empirical%20Investigation%20in%20LLMs%0AAuthor%3A%20Mohor%20Banerjee%20and%20Nadya%20Yuki%20Wangsajaya%20and%20Syed%20Ali%20Redha%20Alsagoff%20and%20Min%20Sen%20Tan%20and%20Zachary%20Choy%20Kit%20Chun%20and%20Alvin%20Chan%20Guo%20Wei%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%20in%20natural%20language%20understanding%20and%20reasoning%2C%20but%20suffer%20from%20hallucination%3A%20the%20generation%20of%20factually%20incorrect%20content.%20While%20numerous%20methods%20have%20been%20developed%20to%20reduce%20hallucinations%2C%20their%20impact%20on%20creative%20generations%20remains%20unexplored.%20This%20gap%20is%20particularly%20critical%20for%20AI-assisted%20scientific%20discovery%2C%20which%20requires%20both%20factual%20accuracy%20and%20creative%20hypothesis%20generation.%20We%20investigate%20how%20three%20hallucination-reduction%20techniques%3A%20Chain%20of%20Verification%20%28CoVe%29%2C%20Decoding%20by%20Contrasting%20Layers%20%28DoLa%29%2C%20and%20Retrieval-Augmented%20Generation%20%28RAG%29%2C%20affect%20creativity%20in%20LLMs.%20Evaluating%20multiple%20model%20families%20%28LLaMA%2C%20Qwen%2C%20Mistral%29%20at%20varying%20scales%20%281B%20-%2070B%20parameters%29%20on%20two%20creativity%20benchmarks%20%28NeoCoder%20and%20CS4%29%2C%20we%20find%20that%20these%20methods%20have%20opposing%20effects%20on%20divergent%20creativity.%20CoVe%20enhances%20divergent%20thinking%2C%20DoLa%20suppresses%20it%2C%20and%20RAG%20shows%20minimal%20impact.%20Our%20findings%20provide%20guidance%20for%20selecting%20appropriate%20hallucination-reduction%20methods%20in%20scientific%20applications%2C%20where%20the%20balance%20between%20factual%20accuracy%20and%20creative%20exploration%20is%20crucial.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Less%2520Hallucination%2520Mean%2520Less%2520Creativity%253F%2520An%2520Empirical%2520Investigation%2520in%2520LLMs%26entry.906535625%3DMohor%2520Banerjee%2520and%2520Nadya%2520Yuki%2520Wangsajaya%2520and%2520Syed%2520Ali%2520Redha%2520Alsagoff%2520and%2520Min%2520Sen%2520Tan%2520and%2520Zachary%2520Choy%2520Kit%2520Chun%2520and%2520Alvin%2520Chan%2520Guo%2520Wei%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520capabilities%2520in%2520natural%2520language%2520understanding%2520and%2520reasoning%252C%2520but%2520suffer%2520from%2520hallucination%253A%2520the%2520generation%2520of%2520factually%2520incorrect%2520content.%2520While%2520numerous%2520methods%2520have%2520been%2520developed%2520to%2520reduce%2520hallucinations%252C%2520their%2520impact%2520on%2520creative%2520generations%2520remains%2520unexplored.%2520This%2520gap%2520is%2520particularly%2520critical%2520for%2520AI-assisted%2520scientific%2520discovery%252C%2520which%2520requires%2520both%2520factual%2520accuracy%2520and%2520creative%2520hypothesis%2520generation.%2520We%2520investigate%2520how%2520three%2520hallucination-reduction%2520techniques%253A%2520Chain%2520of%2520Verification%2520%2528CoVe%2529%252C%2520Decoding%2520by%2520Contrasting%2520Layers%2520%2528DoLa%2529%252C%2520and%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%252C%2520affect%2520creativity%2520in%2520LLMs.%2520Evaluating%2520multiple%2520model%2520families%2520%2528LLaMA%252C%2520Qwen%252C%2520Mistral%2529%2520at%2520varying%2520scales%2520%25281B%2520-%252070B%2520parameters%2529%2520on%2520two%2520creativity%2520benchmarks%2520%2528NeoCoder%2520and%2520CS4%2529%252C%2520we%2520find%2520that%2520these%2520methods%2520have%2520opposing%2520effects%2520on%2520divergent%2520creativity.%2520CoVe%2520enhances%2520divergent%2520thinking%252C%2520DoLa%2520suppresses%2520it%252C%2520and%2520RAG%2520shows%2520minimal%2520impact.%2520Our%2520findings%2520provide%2520guidance%2520for%2520selecting%2520appropriate%2520hallucination-reduction%2520methods%2520in%2520scientific%2520applications%252C%2520where%2520the%2520balance%2520between%2520factual%2520accuracy%2520and%2520creative%2520exploration%2520is%2520crucial.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Less%20Hallucination%20Mean%20Less%20Creativity%3F%20An%20Empirical%20Investigation%20in%20LLMs&entry.906535625=Mohor%20Banerjee%20and%20Nadya%20Yuki%20Wangsajaya%20and%20Syed%20Ali%20Redha%20Alsagoff%20and%20Min%20Sen%20Tan%20and%20Zachary%20Choy%20Kit%20Chun%20and%20Alvin%20Chan%20Guo%20Wei&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20exhibit%20remarkable%20capabilities%20in%20natural%20language%20understanding%20and%20reasoning%2C%20but%20suffer%20from%20hallucination%3A%20the%20generation%20of%20factually%20incorrect%20content.%20While%20numerous%20methods%20have%20been%20developed%20to%20reduce%20hallucinations%2C%20their%20impact%20on%20creative%20generations%20remains%20unexplored.%20This%20gap%20is%20particularly%20critical%20for%20AI-assisted%20scientific%20discovery%2C%20which%20requires%20both%20factual%20accuracy%20and%20creative%20hypothesis%20generation.%20We%20investigate%20how%20three%20hallucination-reduction%20techniques%3A%20Chain%20of%20Verification%20%28CoVe%29%2C%20Decoding%20by%20Contrasting%20Layers%20%28DoLa%29%2C%20and%20Retrieval-Augmented%20Generation%20%28RAG%29%2C%20affect%20creativity%20in%20LLMs.%20Evaluating%20multiple%20model%20families%20%28LLaMA%2C%20Qwen%2C%20Mistral%29%20at%20varying%20scales%20%281B%20-%2070B%20parameters%29%20on%20two%20creativity%20benchmarks%20%28NeoCoder%20and%20CS4%29%2C%20we%20find%20that%20these%20methods%20have%20opposing%20effects%20on%20divergent%20creativity.%20CoVe%20enhances%20divergent%20thinking%2C%20DoLa%20suppresses%20it%2C%20and%20RAG%20shows%20minimal%20impact.%20Our%20findings%20provide%20guidance%20for%20selecting%20appropriate%20hallucination-reduction%20methods%20in%20scientific%20applications%2C%20where%20the%20balance%20between%20factual%20accuracy%20and%20creative%20exploration%20is%20crucial.&entry.1838667208=http%3A//arxiv.org/abs/2512.11509v1&entry.124074799=Read"},
{"title": "NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics", "author": "Hao Wu and Yuan Gao and Fan Xu and Fan Zhang and Guangliang Liu and Yuxuan Liang and Xiaomeng Huang", "abstract": "High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.", "link": "http://arxiv.org/abs/2512.11525v1", "date": "2025-12-12", "relevancy": 2.0318, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5132}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5043}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuralOGCM%3A%20Differentiable%20Ocean%20Modeling%20with%20Learnable%20Physics&body=Title%3A%20NeuralOGCM%3A%20Differentiable%20Ocean%20Modeling%20with%20Learnable%20Physics%0AAuthor%3A%20Hao%20Wu%20and%20Yuan%20Gao%20and%20Fan%20Xu%20and%20Fan%20Zhang%20and%20Guangliang%20Liu%20and%20Yuxuan%20Liang%20and%20Xiaomeng%20Huang%0AAbstract%3A%20High-precision%20scientific%20simulation%20faces%20a%20long-standing%20trade-off%20between%20computational%20efficiency%20and%20physical%20fidelity.%20To%20address%20this%20challenge%2C%20we%20propose%20NeuralOGCM%2C%20an%20ocean%20modeling%20framework%20that%20fuses%20differentiable%20programming%20with%20deep%20learning.%20At%20the%20core%20of%20NeuralOGCM%20is%20a%20fully%20differentiable%20dynamical%20solver%2C%20which%20leverages%20physics%20knowledge%20as%20its%20core%20inductive%20bias.%20The%20learnable%20physics%20integration%20captures%20large-scale%2C%20deterministic%20physical%20evolution%2C%20and%20transforms%20key%20physical%20parameters%20%28e.g.%2C%20diffusion%20coefficients%29%20into%20learnable%20parameters%2C%20enabling%20the%20model%20to%20autonomously%20optimize%20its%20physical%20core%20via%20end-to-end%20training.%20Concurrently%2C%20a%20deep%20neural%20network%20learns%20to%20correct%20for%20subgrid-scale%20processes%20and%20discretization%20errors%20not%20captured%20by%20the%20physics%20model.%20Both%20components%20work%20in%20synergy%2C%20with%20their%20outputs%20integrated%20by%20a%20unified%20ODE%20solver.%20Experiments%20demonstrate%20that%20NeuralOGCM%20maintains%20long-term%20stability%20and%20physical%20consistency%2C%20significantly%20outperforming%20traditional%20numerical%20models%20in%20speed%20and%20pure%20AI%20baselines%20in%20accuracy.%20Our%20work%20paves%20a%20new%20path%20for%20building%20fast%2C%20stable%2C%20and%20physically-plausible%20models%20for%20scientific%20computing.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralOGCM%253A%2520Differentiable%2520Ocean%2520Modeling%2520with%2520Learnable%2520Physics%26entry.906535625%3DHao%2520Wu%2520and%2520Yuan%2520Gao%2520and%2520Fan%2520Xu%2520and%2520Fan%2520Zhang%2520and%2520Guangliang%2520Liu%2520and%2520Yuxuan%2520Liang%2520and%2520Xiaomeng%2520Huang%26entry.1292438233%3DHigh-precision%2520scientific%2520simulation%2520faces%2520a%2520long-standing%2520trade-off%2520between%2520computational%2520efficiency%2520and%2520physical%2520fidelity.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520NeuralOGCM%252C%2520an%2520ocean%2520modeling%2520framework%2520that%2520fuses%2520differentiable%2520programming%2520with%2520deep%2520learning.%2520At%2520the%2520core%2520of%2520NeuralOGCM%2520is%2520a%2520fully%2520differentiable%2520dynamical%2520solver%252C%2520which%2520leverages%2520physics%2520knowledge%2520as%2520its%2520core%2520inductive%2520bias.%2520The%2520learnable%2520physics%2520integration%2520captures%2520large-scale%252C%2520deterministic%2520physical%2520evolution%252C%2520and%2520transforms%2520key%2520physical%2520parameters%2520%2528e.g.%252C%2520diffusion%2520coefficients%2529%2520into%2520learnable%2520parameters%252C%2520enabling%2520the%2520model%2520to%2520autonomously%2520optimize%2520its%2520physical%2520core%2520via%2520end-to-end%2520training.%2520Concurrently%252C%2520a%2520deep%2520neural%2520network%2520learns%2520to%2520correct%2520for%2520subgrid-scale%2520processes%2520and%2520discretization%2520errors%2520not%2520captured%2520by%2520the%2520physics%2520model.%2520Both%2520components%2520work%2520in%2520synergy%252C%2520with%2520their%2520outputs%2520integrated%2520by%2520a%2520unified%2520ODE%2520solver.%2520Experiments%2520demonstrate%2520that%2520NeuralOGCM%2520maintains%2520long-term%2520stability%2520and%2520physical%2520consistency%252C%2520significantly%2520outperforming%2520traditional%2520numerical%2520models%2520in%2520speed%2520and%2520pure%2520AI%2520baselines%2520in%2520accuracy.%2520Our%2520work%2520paves%2520a%2520new%2520path%2520for%2520building%2520fast%252C%2520stable%252C%2520and%2520physically-plausible%2520models%2520for%2520scientific%2520computing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralOGCM%3A%20Differentiable%20Ocean%20Modeling%20with%20Learnable%20Physics&entry.906535625=Hao%20Wu%20and%20Yuan%20Gao%20and%20Fan%20Xu%20and%20Fan%20Zhang%20and%20Guangliang%20Liu%20and%20Yuxuan%20Liang%20and%20Xiaomeng%20Huang&entry.1292438233=High-precision%20scientific%20simulation%20faces%20a%20long-standing%20trade-off%20between%20computational%20efficiency%20and%20physical%20fidelity.%20To%20address%20this%20challenge%2C%20we%20propose%20NeuralOGCM%2C%20an%20ocean%20modeling%20framework%20that%20fuses%20differentiable%20programming%20with%20deep%20learning.%20At%20the%20core%20of%20NeuralOGCM%20is%20a%20fully%20differentiable%20dynamical%20solver%2C%20which%20leverages%20physics%20knowledge%20as%20its%20core%20inductive%20bias.%20The%20learnable%20physics%20integration%20captures%20large-scale%2C%20deterministic%20physical%20evolution%2C%20and%20transforms%20key%20physical%20parameters%20%28e.g.%2C%20diffusion%20coefficients%29%20into%20learnable%20parameters%2C%20enabling%20the%20model%20to%20autonomously%20optimize%20its%20physical%20core%20via%20end-to-end%20training.%20Concurrently%2C%20a%20deep%20neural%20network%20learns%20to%20correct%20for%20subgrid-scale%20processes%20and%20discretization%20errors%20not%20captured%20by%20the%20physics%20model.%20Both%20components%20work%20in%20synergy%2C%20with%20their%20outputs%20integrated%20by%20a%20unified%20ODE%20solver.%20Experiments%20demonstrate%20that%20NeuralOGCM%20maintains%20long-term%20stability%20and%20physical%20consistency%2C%20significantly%20outperforming%20traditional%20numerical%20models%20in%20speed%20and%20pure%20AI%20baselines%20in%20accuracy.%20Our%20work%20paves%20a%20new%20path%20for%20building%20fast%2C%20stable%2C%20and%20physically-plausible%20models%20for%20scientific%20computing.&entry.1838667208=http%3A//arxiv.org/abs/2512.11525v1&entry.124074799=Read"},
{"title": "SSA3D: Text-Conditioned Assisted Self-Supervised Framework for Automatic Dental Abutment Design", "author": "Mianjie Zheng and Xinquan Yang and Along He and Xuguang Li and Feilie Zhong and Xuefen Liu and Kun Tang and Zhicheng Zhang and Linlin Shen", "abstract": "Abutment design is a critical step in dental implant restoration. However, manual design involves tedious measurement and fitting, and research on automating this process with AI is limited, due to the unavailability of large annotated datasets. Although self-supervised learning (SSL) can alleviate data scarcity, its need for pre-training and fine-tuning results in high computational costs and long training times. In this paper, we propose a Self-supervised assisted automatic abutment design framework (SS$A^3$D), which employs a dual-branch architecture with a reconstruction branch and a regression branch. The reconstruction branch learns to restore masked intraoral scan data and transfers the learned structural information to the regression branch. The regression branch then predicts the abutment parameters under supervised learning, which eliminates the separate pre-training and fine-tuning process. We also design a Text-Conditioned Prompt (TCP) module to incorporate clinical information (such as implant location, system, and series) into SS$A^3$D. This guides the network to focus on relevant regions and constrains the parameter predictions. Extensive experiments on a collected dataset show that SS$A^3$D saves half of the training time and achieves higher accuracy than traditional SSL methods. It also achieves state-of-the-art performance compared to other methods, significantly improving the accuracy and efficiency of automated abutment design.", "link": "http://arxiv.org/abs/2512.11507v1", "date": "2025-12-12", "relevancy": 2.0249, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5309}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4905}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSA3D%3A%20Text-Conditioned%20Assisted%20Self-Supervised%20Framework%20for%20Automatic%20Dental%20Abutment%20Design&body=Title%3A%20SSA3D%3A%20Text-Conditioned%20Assisted%20Self-Supervised%20Framework%20for%20Automatic%20Dental%20Abutment%20Design%0AAuthor%3A%20Mianjie%20Zheng%20and%20Xinquan%20Yang%20and%20Along%20He%20and%20Xuguang%20Li%20and%20Feilie%20Zhong%20and%20Xuefen%20Liu%20and%20Kun%20Tang%20and%20Zhicheng%20Zhang%20and%20Linlin%20Shen%0AAbstract%3A%20Abutment%20design%20is%20a%20critical%20step%20in%20dental%20implant%20restoration.%20However%2C%20manual%20design%20involves%20tedious%20measurement%20and%20fitting%2C%20and%20research%20on%20automating%20this%20process%20with%20AI%20is%20limited%2C%20due%20to%20the%20unavailability%20of%20large%20annotated%20datasets.%20Although%20self-supervised%20learning%20%28SSL%29%20can%20alleviate%20data%20scarcity%2C%20its%20need%20for%20pre-training%20and%20fine-tuning%20results%20in%20high%20computational%20costs%20and%20long%20training%20times.%20In%20this%20paper%2C%20we%20propose%20a%20Self-supervised%20assisted%20automatic%20abutment%20design%20framework%20%28SS%24A%5E3%24D%29%2C%20which%20employs%20a%20dual-branch%20architecture%20with%20a%20reconstruction%20branch%20and%20a%20regression%20branch.%20The%20reconstruction%20branch%20learns%20to%20restore%20masked%20intraoral%20scan%20data%20and%20transfers%20the%20learned%20structural%20information%20to%20the%20regression%20branch.%20The%20regression%20branch%20then%20predicts%20the%20abutment%20parameters%20under%20supervised%20learning%2C%20which%20eliminates%20the%20separate%20pre-training%20and%20fine-tuning%20process.%20We%20also%20design%20a%20Text-Conditioned%20Prompt%20%28TCP%29%20module%20to%20incorporate%20clinical%20information%20%28such%20as%20implant%20location%2C%20system%2C%20and%20series%29%20into%20SS%24A%5E3%24D.%20This%20guides%20the%20network%20to%20focus%20on%20relevant%20regions%20and%20constrains%20the%20parameter%20predictions.%20Extensive%20experiments%20on%20a%20collected%20dataset%20show%20that%20SS%24A%5E3%24D%20saves%20half%20of%20the%20training%20time%20and%20achieves%20higher%20accuracy%20than%20traditional%20SSL%20methods.%20It%20also%20achieves%20state-of-the-art%20performance%20compared%20to%20other%20methods%2C%20significantly%20improving%20the%20accuracy%20and%20efficiency%20of%20automated%20abutment%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSA3D%253A%2520Text-Conditioned%2520Assisted%2520Self-Supervised%2520Framework%2520for%2520Automatic%2520Dental%2520Abutment%2520Design%26entry.906535625%3DMianjie%2520Zheng%2520and%2520Xinquan%2520Yang%2520and%2520Along%2520He%2520and%2520Xuguang%2520Li%2520and%2520Feilie%2520Zhong%2520and%2520Xuefen%2520Liu%2520and%2520Kun%2520Tang%2520and%2520Zhicheng%2520Zhang%2520and%2520Linlin%2520Shen%26entry.1292438233%3DAbutment%2520design%2520is%2520a%2520critical%2520step%2520in%2520dental%2520implant%2520restoration.%2520However%252C%2520manual%2520design%2520involves%2520tedious%2520measurement%2520and%2520fitting%252C%2520and%2520research%2520on%2520automating%2520this%2520process%2520with%2520AI%2520is%2520limited%252C%2520due%2520to%2520the%2520unavailability%2520of%2520large%2520annotated%2520datasets.%2520Although%2520self-supervised%2520learning%2520%2528SSL%2529%2520can%2520alleviate%2520data%2520scarcity%252C%2520its%2520need%2520for%2520pre-training%2520and%2520fine-tuning%2520results%2520in%2520high%2520computational%2520costs%2520and%2520long%2520training%2520times.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Self-supervised%2520assisted%2520automatic%2520abutment%2520design%2520framework%2520%2528SS%2524A%255E3%2524D%2529%252C%2520which%2520employs%2520a%2520dual-branch%2520architecture%2520with%2520a%2520reconstruction%2520branch%2520and%2520a%2520regression%2520branch.%2520The%2520reconstruction%2520branch%2520learns%2520to%2520restore%2520masked%2520intraoral%2520scan%2520data%2520and%2520transfers%2520the%2520learned%2520structural%2520information%2520to%2520the%2520regression%2520branch.%2520The%2520regression%2520branch%2520then%2520predicts%2520the%2520abutment%2520parameters%2520under%2520supervised%2520learning%252C%2520which%2520eliminates%2520the%2520separate%2520pre-training%2520and%2520fine-tuning%2520process.%2520We%2520also%2520design%2520a%2520Text-Conditioned%2520Prompt%2520%2528TCP%2529%2520module%2520to%2520incorporate%2520clinical%2520information%2520%2528such%2520as%2520implant%2520location%252C%2520system%252C%2520and%2520series%2529%2520into%2520SS%2524A%255E3%2524D.%2520This%2520guides%2520the%2520network%2520to%2520focus%2520on%2520relevant%2520regions%2520and%2520constrains%2520the%2520parameter%2520predictions.%2520Extensive%2520experiments%2520on%2520a%2520collected%2520dataset%2520show%2520that%2520SS%2524A%255E3%2524D%2520saves%2520half%2520of%2520the%2520training%2520time%2520and%2520achieves%2520higher%2520accuracy%2520than%2520traditional%2520SSL%2520methods.%2520It%2520also%2520achieves%2520state-of-the-art%2520performance%2520compared%2520to%2520other%2520methods%252C%2520significantly%2520improving%2520the%2520accuracy%2520and%2520efficiency%2520of%2520automated%2520abutment%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSA3D%3A%20Text-Conditioned%20Assisted%20Self-Supervised%20Framework%20for%20Automatic%20Dental%20Abutment%20Design&entry.906535625=Mianjie%20Zheng%20and%20Xinquan%20Yang%20and%20Along%20He%20and%20Xuguang%20Li%20and%20Feilie%20Zhong%20and%20Xuefen%20Liu%20and%20Kun%20Tang%20and%20Zhicheng%20Zhang%20and%20Linlin%20Shen&entry.1292438233=Abutment%20design%20is%20a%20critical%20step%20in%20dental%20implant%20restoration.%20However%2C%20manual%20design%20involves%20tedious%20measurement%20and%20fitting%2C%20and%20research%20on%20automating%20this%20process%20with%20AI%20is%20limited%2C%20due%20to%20the%20unavailability%20of%20large%20annotated%20datasets.%20Although%20self-supervised%20learning%20%28SSL%29%20can%20alleviate%20data%20scarcity%2C%20its%20need%20for%20pre-training%20and%20fine-tuning%20results%20in%20high%20computational%20costs%20and%20long%20training%20times.%20In%20this%20paper%2C%20we%20propose%20a%20Self-supervised%20assisted%20automatic%20abutment%20design%20framework%20%28SS%24A%5E3%24D%29%2C%20which%20employs%20a%20dual-branch%20architecture%20with%20a%20reconstruction%20branch%20and%20a%20regression%20branch.%20The%20reconstruction%20branch%20learns%20to%20restore%20masked%20intraoral%20scan%20data%20and%20transfers%20the%20learned%20structural%20information%20to%20the%20regression%20branch.%20The%20regression%20branch%20then%20predicts%20the%20abutment%20parameters%20under%20supervised%20learning%2C%20which%20eliminates%20the%20separate%20pre-training%20and%20fine-tuning%20process.%20We%20also%20design%20a%20Text-Conditioned%20Prompt%20%28TCP%29%20module%20to%20incorporate%20clinical%20information%20%28such%20as%20implant%20location%2C%20system%2C%20and%20series%29%20into%20SS%24A%5E3%24D.%20This%20guides%20the%20network%20to%20focus%20on%20relevant%20regions%20and%20constrains%20the%20parameter%20predictions.%20Extensive%20experiments%20on%20a%20collected%20dataset%20show%20that%20SS%24A%5E3%24D%20saves%20half%20of%20the%20training%20time%20and%20achieves%20higher%20accuracy%20than%20traditional%20SSL%20methods.%20It%20also%20achieves%20state-of-the-art%20performance%20compared%20to%20other%20methods%2C%20significantly%20improving%20the%20accuracy%20and%20efficiency%20of%20automated%20abutment%20design.&entry.1838667208=http%3A//arxiv.org/abs/2512.11507v1&entry.124074799=Read"},
{"title": "Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention", "author": "Shibing Mo and Haoyang Ruan and Kai Wu and Jing Liu", "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.", "link": "http://arxiv.org/abs/2511.06682v2", "date": "2025-12-12", "relevancy": 2.0204, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5075}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Textual%20Self-attention%20Network%3A%20Test-Time%20Preference%20Optimization%20through%20Textual%20Gradient-based%20Attention&body=Title%3A%20Textual%20Self-attention%20Network%3A%20Test-Time%20Preference%20Optimization%20through%20Textual%20Gradient-based%20Attention%0AAuthor%3A%20Shibing%20Mo%20and%20Haoyang%20Ruan%20and%20Kai%20Wu%20and%20Jing%20Liu%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20generalization%20capabilities%2C%20but%20aligning%20their%20outputs%20with%20human%20preferences%20typically%20requires%20expensive%20supervised%20fine-tuning.%20Recent%20test-time%20methods%20leverage%20textual%20feedback%20to%20overcome%20this%2C%20but%20they%20often%20critique%20and%20revise%20a%20single%20candidate%20response%2C%20lacking%20a%20principled%20mechanism%20to%20systematically%20analyze%2C%20weigh%2C%20and%20synthesize%20the%20strengths%20of%20multiple%20promising%20candidates.%20Such%20a%20mechanism%20is%20crucial%20because%20different%20responses%20may%20excel%20in%20distinct%20aspects%20%28e.g.%2C%20clarity%2C%20factual%20accuracy%2C%20or%20tone%29%2C%20and%20combining%20their%20best%20elements%20may%20produce%20a%20far%20superior%20outcome.%20This%20paper%20proposes%20the%20Textual%20Self-Attention%20Network%20%28TSAN%29%2C%20a%20new%20paradigm%20for%20test-time%20preference%20optimization%20that%20requires%20no%20parameter%20updates.%20TSAN%20emulates%20self-attention%20entirely%20in%20natural%20language%20to%20overcome%20this%20gap%3A%20it%20analyzes%20multiple%20candidates%20by%20formatting%20them%20into%20textual%20keys%20and%20values%2C%20weighs%20their%20relevance%20using%20an%20LLM-based%20attention%20module%2C%20and%20synthesizes%20their%20strengths%20into%20a%20new%2C%20preference-aligned%20response%20under%20the%20guidance%20of%20the%20learned%20textual%20attention.%20This%20entire%20process%20operates%20in%20a%20textual%20gradient%20space%2C%20enabling%20iterative%20and%20interpretable%20optimization.%20Empirical%20evaluations%20demonstrate%20that%20with%20just%20three%20test-time%20iterations%20on%20a%20base%20SFT%20model%2C%20TSAN%20outperforms%20supervised%20models%20like%20Llama-3.1-70B-Instruct%20and%20surpasses%20the%20current%20state-of-the-art%20test-time%20alignment%20method%20by%20effectively%20leveraging%20multiple%20candidate%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.06682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextual%2520Self-attention%2520Network%253A%2520Test-Time%2520Preference%2520Optimization%2520through%2520Textual%2520Gradient-based%2520Attention%26entry.906535625%3DShibing%2520Mo%2520and%2520Haoyang%2520Ruan%2520and%2520Kai%2520Wu%2520and%2520Jing%2520Liu%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520generalization%2520capabilities%252C%2520but%2520aligning%2520their%2520outputs%2520with%2520human%2520preferences%2520typically%2520requires%2520expensive%2520supervised%2520fine-tuning.%2520Recent%2520test-time%2520methods%2520leverage%2520textual%2520feedback%2520to%2520overcome%2520this%252C%2520but%2520they%2520often%2520critique%2520and%2520revise%2520a%2520single%2520candidate%2520response%252C%2520lacking%2520a%2520principled%2520mechanism%2520to%2520systematically%2520analyze%252C%2520weigh%252C%2520and%2520synthesize%2520the%2520strengths%2520of%2520multiple%2520promising%2520candidates.%2520Such%2520a%2520mechanism%2520is%2520crucial%2520because%2520different%2520responses%2520may%2520excel%2520in%2520distinct%2520aspects%2520%2528e.g.%252C%2520clarity%252C%2520factual%2520accuracy%252C%2520or%2520tone%2529%252C%2520and%2520combining%2520their%2520best%2520elements%2520may%2520produce%2520a%2520far%2520superior%2520outcome.%2520This%2520paper%2520proposes%2520the%2520Textual%2520Self-Attention%2520Network%2520%2528TSAN%2529%252C%2520a%2520new%2520paradigm%2520for%2520test-time%2520preference%2520optimization%2520that%2520requires%2520no%2520parameter%2520updates.%2520TSAN%2520emulates%2520self-attention%2520entirely%2520in%2520natural%2520language%2520to%2520overcome%2520this%2520gap%253A%2520it%2520analyzes%2520multiple%2520candidates%2520by%2520formatting%2520them%2520into%2520textual%2520keys%2520and%2520values%252C%2520weighs%2520their%2520relevance%2520using%2520an%2520LLM-based%2520attention%2520module%252C%2520and%2520synthesizes%2520their%2520strengths%2520into%2520a%2520new%252C%2520preference-aligned%2520response%2520under%2520the%2520guidance%2520of%2520the%2520learned%2520textual%2520attention.%2520This%2520entire%2520process%2520operates%2520in%2520a%2520textual%2520gradient%2520space%252C%2520enabling%2520iterative%2520and%2520interpretable%2520optimization.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520with%2520just%2520three%2520test-time%2520iterations%2520on%2520a%2520base%2520SFT%2520model%252C%2520TSAN%2520outperforms%2520supervised%2520models%2520like%2520Llama-3.1-70B-Instruct%2520and%2520surpasses%2520the%2520current%2520state-of-the-art%2520test-time%2520alignment%2520method%2520by%2520effectively%2520leveraging%2520multiple%2520candidate%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.06682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Textual%20Self-attention%20Network%3A%20Test-Time%20Preference%20Optimization%20through%20Textual%20Gradient-based%20Attention&entry.906535625=Shibing%20Mo%20and%20Haoyang%20Ruan%20and%20Kai%20Wu%20and%20Jing%20Liu&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20generalization%20capabilities%2C%20but%20aligning%20their%20outputs%20with%20human%20preferences%20typically%20requires%20expensive%20supervised%20fine-tuning.%20Recent%20test-time%20methods%20leverage%20textual%20feedback%20to%20overcome%20this%2C%20but%20they%20often%20critique%20and%20revise%20a%20single%20candidate%20response%2C%20lacking%20a%20principled%20mechanism%20to%20systematically%20analyze%2C%20weigh%2C%20and%20synthesize%20the%20strengths%20of%20multiple%20promising%20candidates.%20Such%20a%20mechanism%20is%20crucial%20because%20different%20responses%20may%20excel%20in%20distinct%20aspects%20%28e.g.%2C%20clarity%2C%20factual%20accuracy%2C%20or%20tone%29%2C%20and%20combining%20their%20best%20elements%20may%20produce%20a%20far%20superior%20outcome.%20This%20paper%20proposes%20the%20Textual%20Self-Attention%20Network%20%28TSAN%29%2C%20a%20new%20paradigm%20for%20test-time%20preference%20optimization%20that%20requires%20no%20parameter%20updates.%20TSAN%20emulates%20self-attention%20entirely%20in%20natural%20language%20to%20overcome%20this%20gap%3A%20it%20analyzes%20multiple%20candidates%20by%20formatting%20them%20into%20textual%20keys%20and%20values%2C%20weighs%20their%20relevance%20using%20an%20LLM-based%20attention%20module%2C%20and%20synthesizes%20their%20strengths%20into%20a%20new%2C%20preference-aligned%20response%20under%20the%20guidance%20of%20the%20learned%20textual%20attention.%20This%20entire%20process%20operates%20in%20a%20textual%20gradient%20space%2C%20enabling%20iterative%20and%20interpretable%20optimization.%20Empirical%20evaluations%20demonstrate%20that%20with%20just%20three%20test-time%20iterations%20on%20a%20base%20SFT%20model%2C%20TSAN%20outperforms%20supervised%20models%20like%20Llama-3.1-70B-Instruct%20and%20surpasses%20the%20current%20state-of-the-art%20test-time%20alignment%20method%20by%20effectively%20leveraging%20multiple%20candidate%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2511.06682v2&entry.124074799=Read"},
{"title": "Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration", "author": "Alexander Tyurin", "abstract": "Even for the gradient descent (GD) method applied to neural network training, understanding its optimization dynamics, including convergence rate, iterate trajectories, function value oscillations, and especially its implicit acceleration, remains a challenging problem. We analyze nonlinear models with the logistic loss and show that the steps of GD reduce to those of generalized perceptron algorithms (Rosenblatt, 1958), providing a new perspective on the dynamics. This reduction yields significantly simpler algorithmic steps, which we analyze using classical linear algebra tools. Using these tools, we demonstrate on a minimalistic example that the nonlinearity in a two-layer model can provably yield a faster iteration complexity $\\tilde{O}(\\sqrt{d})$ compared to $\u03a9(d)$ achieved by linear models, where $d$ is the number of features. This helps explain the optimization dynamics and the implicit acceleration phenomenon observed in neural networks. The theoretical results are supported by extensive numerical experiments. We believe that this alternative view will further advance research on the optimization of neural networks.", "link": "http://arxiv.org/abs/2512.11587v1", "date": "2025-12-12", "relevancy": 2.0196, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5439}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4782}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Descent%20as%20a%20Perceptron%20Algorithm%3A%20Understanding%20Dynamics%20and%20Implicit%20Acceleration&body=Title%3A%20Gradient%20Descent%20as%20a%20Perceptron%20Algorithm%3A%20Understanding%20Dynamics%20and%20Implicit%20Acceleration%0AAuthor%3A%20Alexander%20Tyurin%0AAbstract%3A%20Even%20for%20the%20gradient%20descent%20%28GD%29%20method%20applied%20to%20neural%20network%20training%2C%20understanding%20its%20optimization%20dynamics%2C%20including%20convergence%20rate%2C%20iterate%20trajectories%2C%20function%20value%20oscillations%2C%20and%20especially%20its%20implicit%20acceleration%2C%20remains%20a%20challenging%20problem.%20We%20analyze%20nonlinear%20models%20with%20the%20logistic%20loss%20and%20show%20that%20the%20steps%20of%20GD%20reduce%20to%20those%20of%20generalized%20perceptron%20algorithms%20%28Rosenblatt%2C%201958%29%2C%20providing%20a%20new%20perspective%20on%20the%20dynamics.%20This%20reduction%20yields%20significantly%20simpler%20algorithmic%20steps%2C%20which%20we%20analyze%20using%20classical%20linear%20algebra%20tools.%20Using%20these%20tools%2C%20we%20demonstrate%20on%20a%20minimalistic%20example%20that%20the%20nonlinearity%20in%20a%20two-layer%20model%20can%20provably%20yield%20a%20faster%20iteration%20complexity%20%24%5Ctilde%7BO%7D%28%5Csqrt%7Bd%7D%29%24%20compared%20to%20%24%CE%A9%28d%29%24%20achieved%20by%20linear%20models%2C%20where%20%24d%24%20is%20the%20number%20of%20features.%20This%20helps%20explain%20the%20optimization%20dynamics%20and%20the%20implicit%20acceleration%20phenomenon%20observed%20in%20neural%20networks.%20The%20theoretical%20results%20are%20supported%20by%20extensive%20numerical%20experiments.%20We%20believe%20that%20this%20alternative%20view%20will%20further%20advance%20research%20on%20the%20optimization%20of%20neural%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Descent%2520as%2520a%2520Perceptron%2520Algorithm%253A%2520Understanding%2520Dynamics%2520and%2520Implicit%2520Acceleration%26entry.906535625%3DAlexander%2520Tyurin%26entry.1292438233%3DEven%2520for%2520the%2520gradient%2520descent%2520%2528GD%2529%2520method%2520applied%2520to%2520neural%2520network%2520training%252C%2520understanding%2520its%2520optimization%2520dynamics%252C%2520including%2520convergence%2520rate%252C%2520iterate%2520trajectories%252C%2520function%2520value%2520oscillations%252C%2520and%2520especially%2520its%2520implicit%2520acceleration%252C%2520remains%2520a%2520challenging%2520problem.%2520We%2520analyze%2520nonlinear%2520models%2520with%2520the%2520logistic%2520loss%2520and%2520show%2520that%2520the%2520steps%2520of%2520GD%2520reduce%2520to%2520those%2520of%2520generalized%2520perceptron%2520algorithms%2520%2528Rosenblatt%252C%25201958%2529%252C%2520providing%2520a%2520new%2520perspective%2520on%2520the%2520dynamics.%2520This%2520reduction%2520yields%2520significantly%2520simpler%2520algorithmic%2520steps%252C%2520which%2520we%2520analyze%2520using%2520classical%2520linear%2520algebra%2520tools.%2520Using%2520these%2520tools%252C%2520we%2520demonstrate%2520on%2520a%2520minimalistic%2520example%2520that%2520the%2520nonlinearity%2520in%2520a%2520two-layer%2520model%2520can%2520provably%2520yield%2520a%2520faster%2520iteration%2520complexity%2520%2524%255Ctilde%257BO%257D%2528%255Csqrt%257Bd%257D%2529%2524%2520compared%2520to%2520%2524%25CE%25A9%2528d%2529%2524%2520achieved%2520by%2520linear%2520models%252C%2520where%2520%2524d%2524%2520is%2520the%2520number%2520of%2520features.%2520This%2520helps%2520explain%2520the%2520optimization%2520dynamics%2520and%2520the%2520implicit%2520acceleration%2520phenomenon%2520observed%2520in%2520neural%2520networks.%2520The%2520theoretical%2520results%2520are%2520supported%2520by%2520extensive%2520numerical%2520experiments.%2520We%2520believe%2520that%2520this%2520alternative%2520view%2520will%2520further%2520advance%2520research%2520on%2520the%2520optimization%2520of%2520neural%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Descent%20as%20a%20Perceptron%20Algorithm%3A%20Understanding%20Dynamics%20and%20Implicit%20Acceleration&entry.906535625=Alexander%20Tyurin&entry.1292438233=Even%20for%20the%20gradient%20descent%20%28GD%29%20method%20applied%20to%20neural%20network%20training%2C%20understanding%20its%20optimization%20dynamics%2C%20including%20convergence%20rate%2C%20iterate%20trajectories%2C%20function%20value%20oscillations%2C%20and%20especially%20its%20implicit%20acceleration%2C%20remains%20a%20challenging%20problem.%20We%20analyze%20nonlinear%20models%20with%20the%20logistic%20loss%20and%20show%20that%20the%20steps%20of%20GD%20reduce%20to%20those%20of%20generalized%20perceptron%20algorithms%20%28Rosenblatt%2C%201958%29%2C%20providing%20a%20new%20perspective%20on%20the%20dynamics.%20This%20reduction%20yields%20significantly%20simpler%20algorithmic%20steps%2C%20which%20we%20analyze%20using%20classical%20linear%20algebra%20tools.%20Using%20these%20tools%2C%20we%20demonstrate%20on%20a%20minimalistic%20example%20that%20the%20nonlinearity%20in%20a%20two-layer%20model%20can%20provably%20yield%20a%20faster%20iteration%20complexity%20%24%5Ctilde%7BO%7D%28%5Csqrt%7Bd%7D%29%24%20compared%20to%20%24%CE%A9%28d%29%24%20achieved%20by%20linear%20models%2C%20where%20%24d%24%20is%20the%20number%20of%20features.%20This%20helps%20explain%20the%20optimization%20dynamics%20and%20the%20implicit%20acceleration%20phenomenon%20observed%20in%20neural%20networks.%20The%20theoretical%20results%20are%20supported%20by%20extensive%20numerical%20experiments.%20We%20believe%20that%20this%20alternative%20view%20will%20further%20advance%20research%20on%20the%20optimization%20of%20neural%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2512.11587v1&entry.124074799=Read"},
{"title": "Back to the Baseline: Examining Baseline Effects on Explainability Metrics", "author": "Agustin Martin Picard and Thibaut Boissin and Varshini Subhash and R\u00e9mi Cad\u00e8ne and Thomas Fel", "abstract": "Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline", "link": "http://arxiv.org/abs/2512.11433v1", "date": "2025-12-12", "relevancy": 2.0131, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Back%20to%20the%20Baseline%3A%20Examining%20Baseline%20Effects%20on%20Explainability%20Metrics&body=Title%3A%20Back%20to%20the%20Baseline%3A%20Examining%20Baseline%20Effects%20on%20Explainability%20Metrics%0AAuthor%3A%20Agustin%20Martin%20Picard%20and%20Thibaut%20Boissin%20and%20Varshini%20Subhash%20and%20R%C3%A9mi%20Cad%C3%A8ne%20and%20Thomas%20Fel%0AAbstract%3A%20Attribution%20methods%20are%20among%20the%20most%20prevalent%20techniques%20in%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20and%20are%20usually%20evaluated%20and%20compared%20using%20Fidelity%20metrics%2C%20with%20Insertion%20and%20Deletion%20being%20the%20most%20popular.%20These%20metrics%20rely%20on%20a%20baseline%20function%20to%20alter%20the%20pixels%20of%20the%20input%20image%20that%20the%20attribution%20map%20deems%20most%20important.%20In%20this%20work%2C%20we%20highlight%20a%20critical%20problem%20with%20these%20metrics%3A%20the%20choice%20of%20a%20given%20baseline%20will%20inevitably%20favour%20certain%20attribution%20methods%20over%20others.%20More%20concerningly%2C%20even%20a%20simple%20linear%20model%20with%20commonly%20used%20baselines%20contradicts%20itself%20by%20designating%20different%20optimal%20methods.%20A%20question%20then%20arises%3A%20which%20baseline%20should%20we%20use%3F%20We%20propose%20to%20study%20this%20problem%20through%20two%20desirable%20properties%20of%20a%20baseline%3A%20%28i%29%20that%20it%20removes%20information%20and%20%28ii%29%20that%20it%20does%20not%20produce%20overly%20out-of-distribution%20%28OOD%29%20images.%20We%20first%20show%20that%20none%20of%20the%20tested%20baselines%20satisfy%20both%20criteria%2C%20and%20there%20appears%20to%20be%20a%20trade-off%20among%20current%20baselines%3A%20either%20they%20remove%20information%20or%20they%20produce%20a%20sequence%20of%20OOD%20images.%20Finally%2C%20we%20introduce%20a%20novel%20baseline%20by%20leveraging%20recent%20work%20in%20feature%20visualisation%20to%20artificially%20produce%20a%20model-dependent%20baseline%20that%20removes%20information%20without%20being%20overly%20OOD%2C%20thus%20improving%20on%20the%20trade-off%20when%20compared%20to%20other%20existing%20baselines.%20Our%20code%20is%20available%20at%20https%3A//github.com/deel-ai-papers/Back-to-the-Baseline%0ALink%3A%20http%3A//arxiv.org/abs/2512.11433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBack%2520to%2520the%2520Baseline%253A%2520Examining%2520Baseline%2520Effects%2520on%2520Explainability%2520Metrics%26entry.906535625%3DAgustin%2520Martin%2520Picard%2520and%2520Thibaut%2520Boissin%2520and%2520Varshini%2520Subhash%2520and%2520R%25C3%25A9mi%2520Cad%25C3%25A8ne%2520and%2520Thomas%2520Fel%26entry.1292438233%3DAttribution%2520methods%2520are%2520among%2520the%2520most%2520prevalent%2520techniques%2520in%2520Explainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%2520and%2520are%2520usually%2520evaluated%2520and%2520compared%2520using%2520Fidelity%2520metrics%252C%2520with%2520Insertion%2520and%2520Deletion%2520being%2520the%2520most%2520popular.%2520These%2520metrics%2520rely%2520on%2520a%2520baseline%2520function%2520to%2520alter%2520the%2520pixels%2520of%2520the%2520input%2520image%2520that%2520the%2520attribution%2520map%2520deems%2520most%2520important.%2520In%2520this%2520work%252C%2520we%2520highlight%2520a%2520critical%2520problem%2520with%2520these%2520metrics%253A%2520the%2520choice%2520of%2520a%2520given%2520baseline%2520will%2520inevitably%2520favour%2520certain%2520attribution%2520methods%2520over%2520others.%2520More%2520concerningly%252C%2520even%2520a%2520simple%2520linear%2520model%2520with%2520commonly%2520used%2520baselines%2520contradicts%2520itself%2520by%2520designating%2520different%2520optimal%2520methods.%2520A%2520question%2520then%2520arises%253A%2520which%2520baseline%2520should%2520we%2520use%253F%2520We%2520propose%2520to%2520study%2520this%2520problem%2520through%2520two%2520desirable%2520properties%2520of%2520a%2520baseline%253A%2520%2528i%2529%2520that%2520it%2520removes%2520information%2520and%2520%2528ii%2529%2520that%2520it%2520does%2520not%2520produce%2520overly%2520out-of-distribution%2520%2528OOD%2529%2520images.%2520We%2520first%2520show%2520that%2520none%2520of%2520the%2520tested%2520baselines%2520satisfy%2520both%2520criteria%252C%2520and%2520there%2520appears%2520to%2520be%2520a%2520trade-off%2520among%2520current%2520baselines%253A%2520either%2520they%2520remove%2520information%2520or%2520they%2520produce%2520a%2520sequence%2520of%2520OOD%2520images.%2520Finally%252C%2520we%2520introduce%2520a%2520novel%2520baseline%2520by%2520leveraging%2520recent%2520work%2520in%2520feature%2520visualisation%2520to%2520artificially%2520produce%2520a%2520model-dependent%2520baseline%2520that%2520removes%2520information%2520without%2520being%2520overly%2520OOD%252C%2520thus%2520improving%2520on%2520the%2520trade-off%2520when%2520compared%2520to%2520other%2520existing%2520baselines.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/deel-ai-papers/Back-to-the-Baseline%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Back%20to%20the%20Baseline%3A%20Examining%20Baseline%20Effects%20on%20Explainability%20Metrics&entry.906535625=Agustin%20Martin%20Picard%20and%20Thibaut%20Boissin%20and%20Varshini%20Subhash%20and%20R%C3%A9mi%20Cad%C3%A8ne%20and%20Thomas%20Fel&entry.1292438233=Attribution%20methods%20are%20among%20the%20most%20prevalent%20techniques%20in%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20and%20are%20usually%20evaluated%20and%20compared%20using%20Fidelity%20metrics%2C%20with%20Insertion%20and%20Deletion%20being%20the%20most%20popular.%20These%20metrics%20rely%20on%20a%20baseline%20function%20to%20alter%20the%20pixels%20of%20the%20input%20image%20that%20the%20attribution%20map%20deems%20most%20important.%20In%20this%20work%2C%20we%20highlight%20a%20critical%20problem%20with%20these%20metrics%3A%20the%20choice%20of%20a%20given%20baseline%20will%20inevitably%20favour%20certain%20attribution%20methods%20over%20others.%20More%20concerningly%2C%20even%20a%20simple%20linear%20model%20with%20commonly%20used%20baselines%20contradicts%20itself%20by%20designating%20different%20optimal%20methods.%20A%20question%20then%20arises%3A%20which%20baseline%20should%20we%20use%3F%20We%20propose%20to%20study%20this%20problem%20through%20two%20desirable%20properties%20of%20a%20baseline%3A%20%28i%29%20that%20it%20removes%20information%20and%20%28ii%29%20that%20it%20does%20not%20produce%20overly%20out-of-distribution%20%28OOD%29%20images.%20We%20first%20show%20that%20none%20of%20the%20tested%20baselines%20satisfy%20both%20criteria%2C%20and%20there%20appears%20to%20be%20a%20trade-off%20among%20current%20baselines%3A%20either%20they%20remove%20information%20or%20they%20produce%20a%20sequence%20of%20OOD%20images.%20Finally%2C%20we%20introduce%20a%20novel%20baseline%20by%20leveraging%20recent%20work%20in%20feature%20visualisation%20to%20artificially%20produce%20a%20model-dependent%20baseline%20that%20removes%20information%20without%20being%20overly%20OOD%2C%20thus%20improving%20on%20the%20trade-off%20when%20compared%20to%20other%20existing%20baselines.%20Our%20code%20is%20available%20at%20https%3A//github.com/deel-ai-papers/Back-to-the-Baseline&entry.1838667208=http%3A//arxiv.org/abs/2512.11433v1&entry.124074799=Read"},
{"title": "High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control", "author": "Sebastian Hirt and Valentinus Suwanto and Hendrik Alsmeier and Maik Pfefferkorn and Rolf Findeisen", "abstract": "Learning controller parameters from closed-loop data has been shown to improve closed-loop performance. Bayesian optimization, a widely used black-box and sample-efficient learning method, constructs a probabilistic surrogate of the closed-loop performance from few experiments and uses it to select informative controller parameters. However, it typically struggles with dense high-dimensional controller parameterizations, as they may appear, for example, in tuning model predictive controllers, because standard surrogate models fail to capture the structure of such spaces. This work suggests that the use of Bayesian neural networks as surrogate models may help to mitigate this limitation. Through a comparison between Gaussian processes with Matern kernels, finite-width Bayesian neural networks, and infinite-width Bayesian neural networks on a cart-pole task, we find that Bayesian neural network surrogate models achieve faster and more reliable convergence of the closed-loop cost and enable successful optimization of parameterizations with hundreds of dimensions. Infinite-width Bayesian neural networks also maintain performance in settings with more than one thousand parameters, whereas Matern-kernel Gaussian processes rapidly lose effectiveness. These results indicate that Bayesian neural network surrogate models may be suitable for learning dense high-dimensional controller parameterizations and offer practical guidance for selecting surrogate models in learning-based controller design.", "link": "http://arxiv.org/abs/2512.11705v1", "date": "2025-12-12", "relevancy": 2.0001, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5564}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4934}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Dimensional%20Surrogate%20Modeling%20for%20Closed-Loop%20Learning%20of%20Neural-Network-Parameterized%20Model%20Predictive%20Control&body=Title%3A%20High-Dimensional%20Surrogate%20Modeling%20for%20Closed-Loop%20Learning%20of%20Neural-Network-Parameterized%20Model%20Predictive%20Control%0AAuthor%3A%20Sebastian%20Hirt%20and%20Valentinus%20Suwanto%20and%20Hendrik%20Alsmeier%20and%20Maik%20Pfefferkorn%20and%20Rolf%20Findeisen%0AAbstract%3A%20Learning%20controller%20parameters%20from%20closed-loop%20data%20has%20been%20shown%20to%20improve%20closed-loop%20performance.%20Bayesian%20optimization%2C%20a%20widely%20used%20black-box%20and%20sample-efficient%20learning%20method%2C%20constructs%20a%20probabilistic%20surrogate%20of%20the%20closed-loop%20performance%20from%20few%20experiments%20and%20uses%20it%20to%20select%20informative%20controller%20parameters.%20However%2C%20it%20typically%20struggles%20with%20dense%20high-dimensional%20controller%20parameterizations%2C%20as%20they%20may%20appear%2C%20for%20example%2C%20in%20tuning%20model%20predictive%20controllers%2C%20because%20standard%20surrogate%20models%20fail%20to%20capture%20the%20structure%20of%20such%20spaces.%20This%20work%20suggests%20that%20the%20use%20of%20Bayesian%20neural%20networks%20as%20surrogate%20models%20may%20help%20to%20mitigate%20this%20limitation.%20Through%20a%20comparison%20between%20Gaussian%20processes%20with%20Matern%20kernels%2C%20finite-width%20Bayesian%20neural%20networks%2C%20and%20infinite-width%20Bayesian%20neural%20networks%20on%20a%20cart-pole%20task%2C%20we%20find%20that%20Bayesian%20neural%20network%20surrogate%20models%20achieve%20faster%20and%20more%20reliable%20convergence%20of%20the%20closed-loop%20cost%20and%20enable%20successful%20optimization%20of%20parameterizations%20with%20hundreds%20of%20dimensions.%20Infinite-width%20Bayesian%20neural%20networks%20also%20maintain%20performance%20in%20settings%20with%20more%20than%20one%20thousand%20parameters%2C%20whereas%20Matern-kernel%20Gaussian%20processes%20rapidly%20lose%20effectiveness.%20These%20results%20indicate%20that%20Bayesian%20neural%20network%20surrogate%20models%20may%20be%20suitable%20for%20learning%20dense%20high-dimensional%20controller%20parameterizations%20and%20offer%20practical%20guidance%20for%20selecting%20surrogate%20models%20in%20learning-based%20controller%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Dimensional%2520Surrogate%2520Modeling%2520for%2520Closed-Loop%2520Learning%2520of%2520Neural-Network-Parameterized%2520Model%2520Predictive%2520Control%26entry.906535625%3DSebastian%2520Hirt%2520and%2520Valentinus%2520Suwanto%2520and%2520Hendrik%2520Alsmeier%2520and%2520Maik%2520Pfefferkorn%2520and%2520Rolf%2520Findeisen%26entry.1292438233%3DLearning%2520controller%2520parameters%2520from%2520closed-loop%2520data%2520has%2520been%2520shown%2520to%2520improve%2520closed-loop%2520performance.%2520Bayesian%2520optimization%252C%2520a%2520widely%2520used%2520black-box%2520and%2520sample-efficient%2520learning%2520method%252C%2520constructs%2520a%2520probabilistic%2520surrogate%2520of%2520the%2520closed-loop%2520performance%2520from%2520few%2520experiments%2520and%2520uses%2520it%2520to%2520select%2520informative%2520controller%2520parameters.%2520However%252C%2520it%2520typically%2520struggles%2520with%2520dense%2520high-dimensional%2520controller%2520parameterizations%252C%2520as%2520they%2520may%2520appear%252C%2520for%2520example%252C%2520in%2520tuning%2520model%2520predictive%2520controllers%252C%2520because%2520standard%2520surrogate%2520models%2520fail%2520to%2520capture%2520the%2520structure%2520of%2520such%2520spaces.%2520This%2520work%2520suggests%2520that%2520the%2520use%2520of%2520Bayesian%2520neural%2520networks%2520as%2520surrogate%2520models%2520may%2520help%2520to%2520mitigate%2520this%2520limitation.%2520Through%2520a%2520comparison%2520between%2520Gaussian%2520processes%2520with%2520Matern%2520kernels%252C%2520finite-width%2520Bayesian%2520neural%2520networks%252C%2520and%2520infinite-width%2520Bayesian%2520neural%2520networks%2520on%2520a%2520cart-pole%2520task%252C%2520we%2520find%2520that%2520Bayesian%2520neural%2520network%2520surrogate%2520models%2520achieve%2520faster%2520and%2520more%2520reliable%2520convergence%2520of%2520the%2520closed-loop%2520cost%2520and%2520enable%2520successful%2520optimization%2520of%2520parameterizations%2520with%2520hundreds%2520of%2520dimensions.%2520Infinite-width%2520Bayesian%2520neural%2520networks%2520also%2520maintain%2520performance%2520in%2520settings%2520with%2520more%2520than%2520one%2520thousand%2520parameters%252C%2520whereas%2520Matern-kernel%2520Gaussian%2520processes%2520rapidly%2520lose%2520effectiveness.%2520These%2520results%2520indicate%2520that%2520Bayesian%2520neural%2520network%2520surrogate%2520models%2520may%2520be%2520suitable%2520for%2520learning%2520dense%2520high-dimensional%2520controller%2520parameterizations%2520and%2520offer%2520practical%2520guidance%2520for%2520selecting%2520surrogate%2520models%2520in%2520learning-based%2520controller%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Dimensional%20Surrogate%20Modeling%20for%20Closed-Loop%20Learning%20of%20Neural-Network-Parameterized%20Model%20Predictive%20Control&entry.906535625=Sebastian%20Hirt%20and%20Valentinus%20Suwanto%20and%20Hendrik%20Alsmeier%20and%20Maik%20Pfefferkorn%20and%20Rolf%20Findeisen&entry.1292438233=Learning%20controller%20parameters%20from%20closed-loop%20data%20has%20been%20shown%20to%20improve%20closed-loop%20performance.%20Bayesian%20optimization%2C%20a%20widely%20used%20black-box%20and%20sample-efficient%20learning%20method%2C%20constructs%20a%20probabilistic%20surrogate%20of%20the%20closed-loop%20performance%20from%20few%20experiments%20and%20uses%20it%20to%20select%20informative%20controller%20parameters.%20However%2C%20it%20typically%20struggles%20with%20dense%20high-dimensional%20controller%20parameterizations%2C%20as%20they%20may%20appear%2C%20for%20example%2C%20in%20tuning%20model%20predictive%20controllers%2C%20because%20standard%20surrogate%20models%20fail%20to%20capture%20the%20structure%20of%20such%20spaces.%20This%20work%20suggests%20that%20the%20use%20of%20Bayesian%20neural%20networks%20as%20surrogate%20models%20may%20help%20to%20mitigate%20this%20limitation.%20Through%20a%20comparison%20between%20Gaussian%20processes%20with%20Matern%20kernels%2C%20finite-width%20Bayesian%20neural%20networks%2C%20and%20infinite-width%20Bayesian%20neural%20networks%20on%20a%20cart-pole%20task%2C%20we%20find%20that%20Bayesian%20neural%20network%20surrogate%20models%20achieve%20faster%20and%20more%20reliable%20convergence%20of%20the%20closed-loop%20cost%20and%20enable%20successful%20optimization%20of%20parameterizations%20with%20hundreds%20of%20dimensions.%20Infinite-width%20Bayesian%20neural%20networks%20also%20maintain%20performance%20in%20settings%20with%20more%20than%20one%20thousand%20parameters%2C%20whereas%20Matern-kernel%20Gaussian%20processes%20rapidly%20lose%20effectiveness.%20These%20results%20indicate%20that%20Bayesian%20neural%20network%20surrogate%20models%20may%20be%20suitable%20for%20learning%20dense%20high-dimensional%20controller%20parameterizations%20and%20offer%20practical%20guidance%20for%20selecting%20surrogate%20models%20in%20learning-based%20controller%20design.&entry.1838667208=http%3A//arxiv.org/abs/2512.11705v1&entry.124074799=Read"},
{"title": "iPINNER: An Iterative Physics-Informed Neural Network with Ensemble Kalman Filter", "author": "Binghang Lu and Changhong Mou and Guang Lin", "abstract": "Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems involving partial differential equations (PDEs) by incorporating physical laws into the training process. However, the performance of PINNs is often hindered in real-world scenarios involving noisy observational data and missing physics, particularly in inverse problems. In this work, we propose an iterative multi-objective PINN ensemble Kalman filter (iPINNER) framework that improves the robustness and accuracy of PINNs in both forward and inverse problems by using the \\textit{ensemble Kalman filter} and the \\textit{non-dominated sorting genetic algorithm} III (NSGA-III). Specifically, NSGA-III is used as a multi-objective optimizer that can generate various ensemble members of PINNs along the optimal Pareto front, while accounting the model uncertainty in the solution space. These ensemble members are then utilized within the EnKF to assimilate noisy observational data. The EnKF's analysis is subsequently used to refine the data loss component for retraining the PINNs, thereby iteratively updating their parameters. The iterative procedure generates improved solutions to the PDEs. The proposed method is tested on two benchmark problems: the one-dimensional viscous Burgers equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The numerical results show it outperforms standard PINNs in handling noisy data and missing physics.", "link": "http://arxiv.org/abs/2506.00731v2", "date": "2025-12-12", "relevancy": 1.988, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5045}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4921}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iPINNER%3A%20An%20Iterative%20Physics-Informed%20Neural%20Network%20with%20Ensemble%20Kalman%20Filter&body=Title%3A%20iPINNER%3A%20An%20Iterative%20Physics-Informed%20Neural%20Network%20with%20Ensemble%20Kalman%20Filter%0AAuthor%3A%20Binghang%20Lu%20and%20Changhong%20Mou%20and%20Guang%20Lin%0AAbstract%3A%20Physics-informed%20neural%20networks%20%28PINNs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20solving%20forward%20and%20inverse%20problems%20involving%20partial%20differential%20equations%20%28PDEs%29%20by%20incorporating%20physical%20laws%20into%20the%20training%20process.%20However%2C%20the%20performance%20of%20PINNs%20is%20often%20hindered%20in%20real-world%20scenarios%20involving%20noisy%20observational%20data%20and%20missing%20physics%2C%20particularly%20in%20inverse%20problems.%20In%20this%20work%2C%20we%20propose%20an%20iterative%20multi-objective%20PINN%20ensemble%20Kalman%20filter%20%28iPINNER%29%20framework%20that%20improves%20the%20robustness%20and%20accuracy%20of%20PINNs%20in%20both%20forward%20and%20inverse%20problems%20by%20using%20the%20%5Ctextit%7Bensemble%20Kalman%20filter%7D%20and%20the%20%5Ctextit%7Bnon-dominated%20sorting%20genetic%20algorithm%7D%20III%20%28NSGA-III%29.%20Specifically%2C%20NSGA-III%20is%20used%20as%20a%20multi-objective%20optimizer%20that%20can%20generate%20various%20ensemble%20members%20of%20PINNs%20along%20the%20optimal%20Pareto%20front%2C%20while%20accounting%20the%20model%20uncertainty%20in%20the%20solution%20space.%20These%20ensemble%20members%20are%20then%20utilized%20within%20the%20EnKF%20to%20assimilate%20noisy%20observational%20data.%20The%20EnKF%27s%20analysis%20is%20subsequently%20used%20to%20refine%20the%20data%20loss%20component%20for%20retraining%20the%20PINNs%2C%20thereby%20iteratively%20updating%20their%20parameters.%20The%20iterative%20procedure%20generates%20improved%20solutions%20to%20the%20PDEs.%20The%20proposed%20method%20is%20tested%20on%20two%20benchmark%20problems%3A%20the%20one-dimensional%20viscous%20Burgers%20equation%20and%20the%20time-fractional%20mixed%20diffusion-wave%20equation%20%28TFMDWE%29.%20The%20numerical%20results%20show%20it%20outperforms%20standard%20PINNs%20in%20handling%20noisy%20data%20and%20missing%20physics.%0ALink%3A%20http%3A//arxiv.org/abs/2506.00731v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiPINNER%253A%2520An%2520Iterative%2520Physics-Informed%2520Neural%2520Network%2520with%2520Ensemble%2520Kalman%2520Filter%26entry.906535625%3DBinghang%2520Lu%2520and%2520Changhong%2520Mou%2520and%2520Guang%2520Lin%26entry.1292438233%3DPhysics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520solving%2520forward%2520and%2520inverse%2520problems%2520involving%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520by%2520incorporating%2520physical%2520laws%2520into%2520the%2520training%2520process.%2520However%252C%2520the%2520performance%2520of%2520PINNs%2520is%2520often%2520hindered%2520in%2520real-world%2520scenarios%2520involving%2520noisy%2520observational%2520data%2520and%2520missing%2520physics%252C%2520particularly%2520in%2520inverse%2520problems.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520iterative%2520multi-objective%2520PINN%2520ensemble%2520Kalman%2520filter%2520%2528iPINNER%2529%2520framework%2520that%2520improves%2520the%2520robustness%2520and%2520accuracy%2520of%2520PINNs%2520in%2520both%2520forward%2520and%2520inverse%2520problems%2520by%2520using%2520the%2520%255Ctextit%257Bensemble%2520Kalman%2520filter%257D%2520and%2520the%2520%255Ctextit%257Bnon-dominated%2520sorting%2520genetic%2520algorithm%257D%2520III%2520%2528NSGA-III%2529.%2520Specifically%252C%2520NSGA-III%2520is%2520used%2520as%2520a%2520multi-objective%2520optimizer%2520that%2520can%2520generate%2520various%2520ensemble%2520members%2520of%2520PINNs%2520along%2520the%2520optimal%2520Pareto%2520front%252C%2520while%2520accounting%2520the%2520model%2520uncertainty%2520in%2520the%2520solution%2520space.%2520These%2520ensemble%2520members%2520are%2520then%2520utilized%2520within%2520the%2520EnKF%2520to%2520assimilate%2520noisy%2520observational%2520data.%2520The%2520EnKF%2527s%2520analysis%2520is%2520subsequently%2520used%2520to%2520refine%2520the%2520data%2520loss%2520component%2520for%2520retraining%2520the%2520PINNs%252C%2520thereby%2520iteratively%2520updating%2520their%2520parameters.%2520The%2520iterative%2520procedure%2520generates%2520improved%2520solutions%2520to%2520the%2520PDEs.%2520The%2520proposed%2520method%2520is%2520tested%2520on%2520two%2520benchmark%2520problems%253A%2520the%2520one-dimensional%2520viscous%2520Burgers%2520equation%2520and%2520the%2520time-fractional%2520mixed%2520diffusion-wave%2520equation%2520%2528TFMDWE%2529.%2520The%2520numerical%2520results%2520show%2520it%2520outperforms%2520standard%2520PINNs%2520in%2520handling%2520noisy%2520data%2520and%2520missing%2520physics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00731v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iPINNER%3A%20An%20Iterative%20Physics-Informed%20Neural%20Network%20with%20Ensemble%20Kalman%20Filter&entry.906535625=Binghang%20Lu%20and%20Changhong%20Mou%20and%20Guang%20Lin&entry.1292438233=Physics-informed%20neural%20networks%20%28PINNs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20solving%20forward%20and%20inverse%20problems%20involving%20partial%20differential%20equations%20%28PDEs%29%20by%20incorporating%20physical%20laws%20into%20the%20training%20process.%20However%2C%20the%20performance%20of%20PINNs%20is%20often%20hindered%20in%20real-world%20scenarios%20involving%20noisy%20observational%20data%20and%20missing%20physics%2C%20particularly%20in%20inverse%20problems.%20In%20this%20work%2C%20we%20propose%20an%20iterative%20multi-objective%20PINN%20ensemble%20Kalman%20filter%20%28iPINNER%29%20framework%20that%20improves%20the%20robustness%20and%20accuracy%20of%20PINNs%20in%20both%20forward%20and%20inverse%20problems%20by%20using%20the%20%5Ctextit%7Bensemble%20Kalman%20filter%7D%20and%20the%20%5Ctextit%7Bnon-dominated%20sorting%20genetic%20algorithm%7D%20III%20%28NSGA-III%29.%20Specifically%2C%20NSGA-III%20is%20used%20as%20a%20multi-objective%20optimizer%20that%20can%20generate%20various%20ensemble%20members%20of%20PINNs%20along%20the%20optimal%20Pareto%20front%2C%20while%20accounting%20the%20model%20uncertainty%20in%20the%20solution%20space.%20These%20ensemble%20members%20are%20then%20utilized%20within%20the%20EnKF%20to%20assimilate%20noisy%20observational%20data.%20The%20EnKF%27s%20analysis%20is%20subsequently%20used%20to%20refine%20the%20data%20loss%20component%20for%20retraining%20the%20PINNs%2C%20thereby%20iteratively%20updating%20their%20parameters.%20The%20iterative%20procedure%20generates%20improved%20solutions%20to%20the%20PDEs.%20The%20proposed%20method%20is%20tested%20on%20two%20benchmark%20problems%3A%20the%20one-dimensional%20viscous%20Burgers%20equation%20and%20the%20time-fractional%20mixed%20diffusion-wave%20equation%20%28TFMDWE%29.%20The%20numerical%20results%20show%20it%20outperforms%20standard%20PINNs%20in%20handling%20noisy%20data%20and%20missing%20physics.&entry.1838667208=http%3A//arxiv.org/abs/2506.00731v2&entry.124074799=Read"},
{"title": "Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents", "author": "Stefan Tabakov and Asen Popov and Dimitar Dimitrov and S. Ensiye Kiyamousavi and Vladimir Hristov and Boris Kraychev", "abstract": "Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)", "link": "http://arxiv.org/abs/2512.11584v1", "date": "2025-12-12", "relevancy": 1.9864, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Atomic%20Action%20Slicing%3A%20Planner-Aligned%20Options%20for%20Generalist%20VLA%20Agents&body=Title%3A%20Atomic%20Action%20Slicing%3A%20Planner-Aligned%20Options%20for%20Generalist%20VLA%20Agents%0AAuthor%3A%20Stefan%20Tabakov%20and%20Asen%20Popov%20and%20Dimitar%20Dimitrov%20and%20S.%20Ensiye%20Kiyamousavi%20and%20Vladimir%20Hristov%20and%20Boris%20Kraychev%0AAbstract%3A%20Current%20vision-language-action%20%28VLA%29%20models%20generalize%20poorly%2C%20particularly%20when%20tasks%20require%20new%20compositions%20of%20skills%20or%20objects.%20We%20introduce%20Atomic%20Action%20Slicing%20%28AAS%29%2C%20a%20planner-aligned%20approach%20that%20decomposes%20long-horizon%20demonstrations%20into%20short%2C%20typed%20atomic%20actions%20that%20are%20easier%20for%20planners%20to%20use%20and%20policies%20to%20learn.%20Using%20LIBERO%20demonstrations%2C%20AAS%20produces%20a%20validated%20dataset%20of%202%2C124%20atomic%20segments%20labeled%20with%20action%20type%2C%20temporal%20span%2C%20and%20confidence.%20A%20stronger%20segmenter%20%28Gemini%202.5%20Pro%29%20closely%20matches%20planner-defined%20plans%20and%20remains%20robust%20under%20keyframe%20jitter%2C%20while%20smaller%20models%20perform%20worse%20on%20multi-object%20tasks.%20Fine-tuning%20CLIP-RT%2B%20on%20our%20atomic%20dataset%20improves%20task%20success%20from%2094.2%25%20to%2095.3%25%20on%20LIBERO-Goal%20and%2083.8%25%20to%2088.8%25%20on%20LIBERO-Long.%20We%20publicly%20release%20the%20GATE-VLAP%20dataset%20on%20HuggingFace%28https%3A//huggingface.co/datasets/gate-institute/GATE-VLAP-datasets%29%0ALink%3A%20http%3A//arxiv.org/abs/2512.11584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtomic%2520Action%2520Slicing%253A%2520Planner-Aligned%2520Options%2520for%2520Generalist%2520VLA%2520Agents%26entry.906535625%3DStefan%2520Tabakov%2520and%2520Asen%2520Popov%2520and%2520Dimitar%2520Dimitrov%2520and%2520S.%2520Ensiye%2520Kiyamousavi%2520and%2520Vladimir%2520Hristov%2520and%2520Boris%2520Kraychev%26entry.1292438233%3DCurrent%2520vision-language-action%2520%2528VLA%2529%2520models%2520generalize%2520poorly%252C%2520particularly%2520when%2520tasks%2520require%2520new%2520compositions%2520of%2520skills%2520or%2520objects.%2520We%2520introduce%2520Atomic%2520Action%2520Slicing%2520%2528AAS%2529%252C%2520a%2520planner-aligned%2520approach%2520that%2520decomposes%2520long-horizon%2520demonstrations%2520into%2520short%252C%2520typed%2520atomic%2520actions%2520that%2520are%2520easier%2520for%2520planners%2520to%2520use%2520and%2520policies%2520to%2520learn.%2520Using%2520LIBERO%2520demonstrations%252C%2520AAS%2520produces%2520a%2520validated%2520dataset%2520of%25202%252C124%2520atomic%2520segments%2520labeled%2520with%2520action%2520type%252C%2520temporal%2520span%252C%2520and%2520confidence.%2520A%2520stronger%2520segmenter%2520%2528Gemini%25202.5%2520Pro%2529%2520closely%2520matches%2520planner-defined%2520plans%2520and%2520remains%2520robust%2520under%2520keyframe%2520jitter%252C%2520while%2520smaller%2520models%2520perform%2520worse%2520on%2520multi-object%2520tasks.%2520Fine-tuning%2520CLIP-RT%252B%2520on%2520our%2520atomic%2520dataset%2520improves%2520task%2520success%2520from%252094.2%2525%2520to%252095.3%2525%2520on%2520LIBERO-Goal%2520and%252083.8%2525%2520to%252088.8%2525%2520on%2520LIBERO-Long.%2520We%2520publicly%2520release%2520the%2520GATE-VLAP%2520dataset%2520on%2520HuggingFace%2528https%253A//huggingface.co/datasets/gate-institute/GATE-VLAP-datasets%2529%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Atomic%20Action%20Slicing%3A%20Planner-Aligned%20Options%20for%20Generalist%20VLA%20Agents&entry.906535625=Stefan%20Tabakov%20and%20Asen%20Popov%20and%20Dimitar%20Dimitrov%20and%20S.%20Ensiye%20Kiyamousavi%20and%20Vladimir%20Hristov%20and%20Boris%20Kraychev&entry.1292438233=Current%20vision-language-action%20%28VLA%29%20models%20generalize%20poorly%2C%20particularly%20when%20tasks%20require%20new%20compositions%20of%20skills%20or%20objects.%20We%20introduce%20Atomic%20Action%20Slicing%20%28AAS%29%2C%20a%20planner-aligned%20approach%20that%20decomposes%20long-horizon%20demonstrations%20into%20short%2C%20typed%20atomic%20actions%20that%20are%20easier%20for%20planners%20to%20use%20and%20policies%20to%20learn.%20Using%20LIBERO%20demonstrations%2C%20AAS%20produces%20a%20validated%20dataset%20of%202%2C124%20atomic%20segments%20labeled%20with%20action%20type%2C%20temporal%20span%2C%20and%20confidence.%20A%20stronger%20segmenter%20%28Gemini%202.5%20Pro%29%20closely%20matches%20planner-defined%20plans%20and%20remains%20robust%20under%20keyframe%20jitter%2C%20while%20smaller%20models%20perform%20worse%20on%20multi-object%20tasks.%20Fine-tuning%20CLIP-RT%2B%20on%20our%20atomic%20dataset%20improves%20task%20success%20from%2094.2%25%20to%2095.3%25%20on%20LIBERO-Goal%20and%2083.8%25%20to%2088.8%25%20on%20LIBERO-Long.%20We%20publicly%20release%20the%20GATE-VLAP%20dataset%20on%20HuggingFace%28https%3A//huggingface.co/datasets/gate-institute/GATE-VLAP-datasets%29&entry.1838667208=http%3A//arxiv.org/abs/2512.11584v1&entry.124074799=Read"},
{"title": "Using GUI Agent for Electronic Design Automation", "author": "Chunyi Li and Longfei Li and Zicheng Zhang and Xiaohong Liu and Min Tang and Weisi Lin and Guangtao Zhai", "abstract": "Graphical User Interface (GUI) agents adopt an end-to-end paradigm that maps a screenshot to an action sequence, thereby automating repetitive tasks in virtual environments. However, existing GUI agents are evaluated almost exclusively on commodity software such as Microsoft Word and Excel. Professional Computer-Aided Design (CAD) suites promise an order-of-magnitude higher economic return, yet remain the weakest performance domain for existing agents and are still far from replacing expert Electronic-Design-Automation (EDA) engineers. We therefore present the first systematic study that deploys GUI agents for EDA workflows. Our contributions are: (1) a large-scale dataset named GUI-EDA, including 5 CAD tools and 5 physical domains, comprising 2,000+ high-quality screenshot-answer-action pairs recorded by EDA scientists and engineers during real-world component design; (2) a comprehensive benchmark that evaluates 30+ mainstream GUI agents, demonstrating that EDA tasks constitute a major, unsolved challenge; and (3) an EDA-specialized metric named EDAgent, equipped with a reflection mechanism that achieves reliable performance on industrial CAD software and, for the first time, outperforms Ph.D. students majored in Electrical Engineering. This work extends GUI agents from generic office automation to specialized, high-value engineering domains and offers a new avenue for advancing EDA productivity. The dataset will be released at: https://github.com/aiben-ch/GUI-EDA.", "link": "http://arxiv.org/abs/2512.11611v1", "date": "2025-12-12", "relevancy": 1.9852, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5308}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.496}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20GUI%20Agent%20for%20Electronic%20Design%20Automation&body=Title%3A%20Using%20GUI%20Agent%20for%20Electronic%20Design%20Automation%0AAuthor%3A%20Chunyi%20Li%20and%20Longfei%20Li%20and%20Zicheng%20Zhang%20and%20Xiaohong%20Liu%20and%20Min%20Tang%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai%0AAbstract%3A%20Graphical%20User%20Interface%20%28GUI%29%20agents%20adopt%20an%20end-to-end%20paradigm%20that%20maps%20a%20screenshot%20to%20an%20action%20sequence%2C%20thereby%20automating%20repetitive%20tasks%20in%20virtual%20environments.%20However%2C%20existing%20GUI%20agents%20are%20evaluated%20almost%20exclusively%20on%20commodity%20software%20such%20as%20Microsoft%20Word%20and%20Excel.%20Professional%20Computer-Aided%20Design%20%28CAD%29%20suites%20promise%20an%20order-of-magnitude%20higher%20economic%20return%2C%20yet%20remain%20the%20weakest%20performance%20domain%20for%20existing%20agents%20and%20are%20still%20far%20from%20replacing%20expert%20Electronic-Design-Automation%20%28EDA%29%20engineers.%20We%20therefore%20present%20the%20first%20systematic%20study%20that%20deploys%20GUI%20agents%20for%20EDA%20workflows.%20Our%20contributions%20are%3A%20%281%29%20a%20large-scale%20dataset%20named%20GUI-EDA%2C%20including%205%20CAD%20tools%20and%205%20physical%20domains%2C%20comprising%202%2C000%2B%20high-quality%20screenshot-answer-action%20pairs%20recorded%20by%20EDA%20scientists%20and%20engineers%20during%20real-world%20component%20design%3B%20%282%29%20a%20comprehensive%20benchmark%20that%20evaluates%2030%2B%20mainstream%20GUI%20agents%2C%20demonstrating%20that%20EDA%20tasks%20constitute%20a%20major%2C%20unsolved%20challenge%3B%20and%20%283%29%20an%20EDA-specialized%20metric%20named%20EDAgent%2C%20equipped%20with%20a%20reflection%20mechanism%20that%20achieves%20reliable%20performance%20on%20industrial%20CAD%20software%20and%2C%20for%20the%20first%20time%2C%20outperforms%20Ph.D.%20students%20majored%20in%20Electrical%20Engineering.%20This%20work%20extends%20GUI%20agents%20from%20generic%20office%20automation%20to%20specialized%2C%20high-value%20engineering%20domains%20and%20offers%20a%20new%20avenue%20for%20advancing%20EDA%20productivity.%20The%20dataset%20will%20be%20released%20at%3A%20https%3A//github.com/aiben-ch/GUI-EDA.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520GUI%2520Agent%2520for%2520Electronic%2520Design%2520Automation%26entry.906535625%3DChunyi%2520Li%2520and%2520Longfei%2520Li%2520and%2520Zicheng%2520Zhang%2520and%2520Xiaohong%2520Liu%2520and%2520Min%2520Tang%2520and%2520Weisi%2520Lin%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3DGraphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520adopt%2520an%2520end-to-end%2520paradigm%2520that%2520maps%2520a%2520screenshot%2520to%2520an%2520action%2520sequence%252C%2520thereby%2520automating%2520repetitive%2520tasks%2520in%2520virtual%2520environments.%2520However%252C%2520existing%2520GUI%2520agents%2520are%2520evaluated%2520almost%2520exclusively%2520on%2520commodity%2520software%2520such%2520as%2520Microsoft%2520Word%2520and%2520Excel.%2520Professional%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520suites%2520promise%2520an%2520order-of-magnitude%2520higher%2520economic%2520return%252C%2520yet%2520remain%2520the%2520weakest%2520performance%2520domain%2520for%2520existing%2520agents%2520and%2520are%2520still%2520far%2520from%2520replacing%2520expert%2520Electronic-Design-Automation%2520%2528EDA%2529%2520engineers.%2520We%2520therefore%2520present%2520the%2520first%2520systematic%2520study%2520that%2520deploys%2520GUI%2520agents%2520for%2520EDA%2520workflows.%2520Our%2520contributions%2520are%253A%2520%25281%2529%2520a%2520large-scale%2520dataset%2520named%2520GUI-EDA%252C%2520including%25205%2520CAD%2520tools%2520and%25205%2520physical%2520domains%252C%2520comprising%25202%252C000%252B%2520high-quality%2520screenshot-answer-action%2520pairs%2520recorded%2520by%2520EDA%2520scientists%2520and%2520engineers%2520during%2520real-world%2520component%2520design%253B%2520%25282%2529%2520a%2520comprehensive%2520benchmark%2520that%2520evaluates%252030%252B%2520mainstream%2520GUI%2520agents%252C%2520demonstrating%2520that%2520EDA%2520tasks%2520constitute%2520a%2520major%252C%2520unsolved%2520challenge%253B%2520and%2520%25283%2529%2520an%2520EDA-specialized%2520metric%2520named%2520EDAgent%252C%2520equipped%2520with%2520a%2520reflection%2520mechanism%2520that%2520achieves%2520reliable%2520performance%2520on%2520industrial%2520CAD%2520software%2520and%252C%2520for%2520the%2520first%2520time%252C%2520outperforms%2520Ph.D.%2520students%2520majored%2520in%2520Electrical%2520Engineering.%2520This%2520work%2520extends%2520GUI%2520agents%2520from%2520generic%2520office%2520automation%2520to%2520specialized%252C%2520high-value%2520engineering%2520domains%2520and%2520offers%2520a%2520new%2520avenue%2520for%2520advancing%2520EDA%2520productivity.%2520The%2520dataset%2520will%2520be%2520released%2520at%253A%2520https%253A//github.com/aiben-ch/GUI-EDA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20GUI%20Agent%20for%20Electronic%20Design%20Automation&entry.906535625=Chunyi%20Li%20and%20Longfei%20Li%20and%20Zicheng%20Zhang%20and%20Xiaohong%20Liu%20and%20Min%20Tang%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai&entry.1292438233=Graphical%20User%20Interface%20%28GUI%29%20agents%20adopt%20an%20end-to-end%20paradigm%20that%20maps%20a%20screenshot%20to%20an%20action%20sequence%2C%20thereby%20automating%20repetitive%20tasks%20in%20virtual%20environments.%20However%2C%20existing%20GUI%20agents%20are%20evaluated%20almost%20exclusively%20on%20commodity%20software%20such%20as%20Microsoft%20Word%20and%20Excel.%20Professional%20Computer-Aided%20Design%20%28CAD%29%20suites%20promise%20an%20order-of-magnitude%20higher%20economic%20return%2C%20yet%20remain%20the%20weakest%20performance%20domain%20for%20existing%20agents%20and%20are%20still%20far%20from%20replacing%20expert%20Electronic-Design-Automation%20%28EDA%29%20engineers.%20We%20therefore%20present%20the%20first%20systematic%20study%20that%20deploys%20GUI%20agents%20for%20EDA%20workflows.%20Our%20contributions%20are%3A%20%281%29%20a%20large-scale%20dataset%20named%20GUI-EDA%2C%20including%205%20CAD%20tools%20and%205%20physical%20domains%2C%20comprising%202%2C000%2B%20high-quality%20screenshot-answer-action%20pairs%20recorded%20by%20EDA%20scientists%20and%20engineers%20during%20real-world%20component%20design%3B%20%282%29%20a%20comprehensive%20benchmark%20that%20evaluates%2030%2B%20mainstream%20GUI%20agents%2C%20demonstrating%20that%20EDA%20tasks%20constitute%20a%20major%2C%20unsolved%20challenge%3B%20and%20%283%29%20an%20EDA-specialized%20metric%20named%20EDAgent%2C%20equipped%20with%20a%20reflection%20mechanism%20that%20achieves%20reliable%20performance%20on%20industrial%20CAD%20software%20and%2C%20for%20the%20first%20time%2C%20outperforms%20Ph.D.%20students%20majored%20in%20Electrical%20Engineering.%20This%20work%20extends%20GUI%20agents%20from%20generic%20office%20automation%20to%20specialized%2C%20high-value%20engineering%20domains%20and%20offers%20a%20new%20avenue%20for%20advancing%20EDA%20productivity.%20The%20dataset%20will%20be%20released%20at%3A%20https%3A//github.com/aiben-ch/GUI-EDA.&entry.1838667208=http%3A//arxiv.org/abs/2512.11611v1&entry.124074799=Read"},
{"title": "Two-dimensional Decompositions of High-dimensional Configurations for Efficient Multi-vehicle Coordination at Intelligent Intersections", "author": "Amirreza Akbari and Johan Thunberg", "abstract": "For multi-vehicle complex traffic scenarios in shared spaces such as intelligent intersections, safe coordination and trajectory planning is challenging due to computational complexity. To meet this challenge, we introduce a computationally efficient method for generating collision-free trajectories along predefined vehicle paths. We reformulate a constrained minimum-time trajectory planning problem as a problem in a high-dimensional configuration space, where conflict zones are modeled by high-dimensional polyhedra constructed from two-dimensional rectangles. Still, in such a formulation, as the number of vehicles involved increases, the computational complexity increases significantly. To address this, we propose two algorithms for near-optimal local optimization that significantly reduce the computational complexity by decomposing the high-dimensional problem into a sequence of 2D graph search problems. The resulting trajectories are then incorporated into a Nonlinear Model Predictive Control (NMPC) framework to ensure safe and smooth vehicle motion. We furthermore show in numerical evaluation that this approach significantly outperforms existing MILP-based time-scheduling; both in terms of objective-value and computational time.", "link": "http://arxiv.org/abs/2512.11713v1", "date": "2025-12-12", "relevancy": 1.9844, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5024}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4977}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-dimensional%20Decompositions%20of%20High-dimensional%20Configurations%20for%20Efficient%20Multi-vehicle%20Coordination%20at%20Intelligent%20Intersections&body=Title%3A%20Two-dimensional%20Decompositions%20of%20High-dimensional%20Configurations%20for%20Efficient%20Multi-vehicle%20Coordination%20at%20Intelligent%20Intersections%0AAuthor%3A%20Amirreza%20Akbari%20and%20Johan%20Thunberg%0AAbstract%3A%20For%20multi-vehicle%20complex%20traffic%20scenarios%20in%20shared%20spaces%20such%20as%20intelligent%20intersections%2C%20safe%20coordination%20and%20trajectory%20planning%20is%20challenging%20due%20to%20computational%20complexity.%20To%20meet%20this%20challenge%2C%20we%20introduce%20a%20computationally%20efficient%20method%20for%20generating%20collision-free%20trajectories%20along%20predefined%20vehicle%20paths.%20We%20reformulate%20a%20constrained%20minimum-time%20trajectory%20planning%20problem%20as%20a%20problem%20in%20a%20high-dimensional%20configuration%20space%2C%20where%20conflict%20zones%20are%20modeled%20by%20high-dimensional%20polyhedra%20constructed%20from%20two-dimensional%20rectangles.%20Still%2C%20in%20such%20a%20formulation%2C%20as%20the%20number%20of%20vehicles%20involved%20increases%2C%20the%20computational%20complexity%20increases%20significantly.%20To%20address%20this%2C%20we%20propose%20two%20algorithms%20for%20near-optimal%20local%20optimization%20that%20significantly%20reduce%20the%20computational%20complexity%20by%20decomposing%20the%20high-dimensional%20problem%20into%20a%20sequence%20of%202D%20graph%20search%20problems.%20The%20resulting%20trajectories%20are%20then%20incorporated%20into%20a%20Nonlinear%20Model%20Predictive%20Control%20%28NMPC%29%20framework%20to%20ensure%20safe%20and%20smooth%20vehicle%20motion.%20We%20furthermore%20show%20in%20numerical%20evaluation%20that%20this%20approach%20significantly%20outperforms%20existing%20MILP-based%20time-scheduling%3B%20both%20in%20terms%20of%20objective-value%20and%20computational%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-dimensional%2520Decompositions%2520of%2520High-dimensional%2520Configurations%2520for%2520Efficient%2520Multi-vehicle%2520Coordination%2520at%2520Intelligent%2520Intersections%26entry.906535625%3DAmirreza%2520Akbari%2520and%2520Johan%2520Thunberg%26entry.1292438233%3DFor%2520multi-vehicle%2520complex%2520traffic%2520scenarios%2520in%2520shared%2520spaces%2520such%2520as%2520intelligent%2520intersections%252C%2520safe%2520coordination%2520and%2520trajectory%2520planning%2520is%2520challenging%2520due%2520to%2520computational%2520complexity.%2520To%2520meet%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520computationally%2520efficient%2520method%2520for%2520generating%2520collision-free%2520trajectories%2520along%2520predefined%2520vehicle%2520paths.%2520We%2520reformulate%2520a%2520constrained%2520minimum-time%2520trajectory%2520planning%2520problem%2520as%2520a%2520problem%2520in%2520a%2520high-dimensional%2520configuration%2520space%252C%2520where%2520conflict%2520zones%2520are%2520modeled%2520by%2520high-dimensional%2520polyhedra%2520constructed%2520from%2520two-dimensional%2520rectangles.%2520Still%252C%2520in%2520such%2520a%2520formulation%252C%2520as%2520the%2520number%2520of%2520vehicles%2520involved%2520increases%252C%2520the%2520computational%2520complexity%2520increases%2520significantly.%2520To%2520address%2520this%252C%2520we%2520propose%2520two%2520algorithms%2520for%2520near-optimal%2520local%2520optimization%2520that%2520significantly%2520reduce%2520the%2520computational%2520complexity%2520by%2520decomposing%2520the%2520high-dimensional%2520problem%2520into%2520a%2520sequence%2520of%25202D%2520graph%2520search%2520problems.%2520The%2520resulting%2520trajectories%2520are%2520then%2520incorporated%2520into%2520a%2520Nonlinear%2520Model%2520Predictive%2520Control%2520%2528NMPC%2529%2520framework%2520to%2520ensure%2520safe%2520and%2520smooth%2520vehicle%2520motion.%2520We%2520furthermore%2520show%2520in%2520numerical%2520evaluation%2520that%2520this%2520approach%2520significantly%2520outperforms%2520existing%2520MILP-based%2520time-scheduling%253B%2520both%2520in%2520terms%2520of%2520objective-value%2520and%2520computational%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-dimensional%20Decompositions%20of%20High-dimensional%20Configurations%20for%20Efficient%20Multi-vehicle%20Coordination%20at%20Intelligent%20Intersections&entry.906535625=Amirreza%20Akbari%20and%20Johan%20Thunberg&entry.1292438233=For%20multi-vehicle%20complex%20traffic%20scenarios%20in%20shared%20spaces%20such%20as%20intelligent%20intersections%2C%20safe%20coordination%20and%20trajectory%20planning%20is%20challenging%20due%20to%20computational%20complexity.%20To%20meet%20this%20challenge%2C%20we%20introduce%20a%20computationally%20efficient%20method%20for%20generating%20collision-free%20trajectories%20along%20predefined%20vehicle%20paths.%20We%20reformulate%20a%20constrained%20minimum-time%20trajectory%20planning%20problem%20as%20a%20problem%20in%20a%20high-dimensional%20configuration%20space%2C%20where%20conflict%20zones%20are%20modeled%20by%20high-dimensional%20polyhedra%20constructed%20from%20two-dimensional%20rectangles.%20Still%2C%20in%20such%20a%20formulation%2C%20as%20the%20number%20of%20vehicles%20involved%20increases%2C%20the%20computational%20complexity%20increases%20significantly.%20To%20address%20this%2C%20we%20propose%20two%20algorithms%20for%20near-optimal%20local%20optimization%20that%20significantly%20reduce%20the%20computational%20complexity%20by%20decomposing%20the%20high-dimensional%20problem%20into%20a%20sequence%20of%202D%20graph%20search%20problems.%20The%20resulting%20trajectories%20are%20then%20incorporated%20into%20a%20Nonlinear%20Model%20Predictive%20Control%20%28NMPC%29%20framework%20to%20ensure%20safe%20and%20smooth%20vehicle%20motion.%20We%20furthermore%20show%20in%20numerical%20evaluation%20that%20this%20approach%20significantly%20outperforms%20existing%20MILP-based%20time-scheduling%3B%20both%20in%20terms%20of%20objective-value%20and%20computational%20time.&entry.1838667208=http%3A//arxiv.org/abs/2512.11713v1&entry.124074799=Read"},
{"title": "Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems", "author": "Chong Tang and Hao Dai and Jagmohan Chauhan", "abstract": "The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.", "link": "http://arxiv.org/abs/2512.11532v1", "date": "2025-12-12", "relevancy": 1.983, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5461}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4882}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallax%3A%20Runtime%20Parallelization%20for%20Operator%20Fallbacks%20in%20Heterogeneous%20Edge%20Systems&body=Title%3A%20Parallax%3A%20Runtime%20Parallelization%20for%20Operator%20Fallbacks%20in%20Heterogeneous%20Edge%20Systems%0AAuthor%3A%20Chong%20Tang%20and%20Hao%20Dai%20and%20Jagmohan%20Chauhan%0AAbstract%3A%20The%20growing%20demand%20for%20real-time%20DNN%20applications%20on%20edge%20devices%20necessitates%20faster%20inference%20of%20increasingly%20complex%20models.%20Although%20many%20devices%20include%20specialized%20accelerators%20%28e.g.%2C%20mobile%20GPUs%29%2C%20dynamic%20control-flow%20operators%20and%20unsupported%20kernels%20often%20fall%20back%20to%20CPU%20execution.%20Existing%20frameworks%20handle%20these%20fallbacks%20poorly%2C%20leaving%20CPU%20cores%20idle%20and%20causing%20high%20latency%20and%20memory%20spikes.%20We%20introduce%20Parallax%2C%20a%20framework%20that%20accelerates%20mobile%20DNN%20inference%20without%20model%20refactoring%20or%20custom%20operator%20implementations.%20Parallax%20first%20partitions%20the%20computation%20DAG%20to%20expose%20parallelism%2C%20then%20employs%20branch-aware%20memory%20management%20with%20dedicated%20arenas%20and%20buffer%20reuse%20to%20reduce%20runtime%20footprint.%20An%20adaptive%20scheduler%20executes%20branches%20according%20to%20device%20memory%20constraints%2C%20meanwhile%2C%20fine-grained%20subgraph%20control%20enables%20heterogeneous%20inference%20of%20dynamic%20models.%20By%20evaluating%20on%20five%20representative%20DNNs%20across%20three%20different%20mobile%20devices%2C%20Parallax%20achieves%20up%20to%2046%25%20latency%20reduction%2C%20maintains%20controlled%20memory%20overhead%20%2826.5%25%20on%20average%29%2C%20and%20delivers%20up%20to%2030%25%20energy%20savings%20compared%20with%20state-of-the-art%20frameworks%2C%20offering%20improvements%20aligned%20with%20the%20responsiveness%20demands%20of%20real-time%20mobile%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallax%253A%2520Runtime%2520Parallelization%2520for%2520Operator%2520Fallbacks%2520in%2520Heterogeneous%2520Edge%2520Systems%26entry.906535625%3DChong%2520Tang%2520and%2520Hao%2520Dai%2520and%2520Jagmohan%2520Chauhan%26entry.1292438233%3DThe%2520growing%2520demand%2520for%2520real-time%2520DNN%2520applications%2520on%2520edge%2520devices%2520necessitates%2520faster%2520inference%2520of%2520increasingly%2520complex%2520models.%2520Although%2520many%2520devices%2520include%2520specialized%2520accelerators%2520%2528e.g.%252C%2520mobile%2520GPUs%2529%252C%2520dynamic%2520control-flow%2520operators%2520and%2520unsupported%2520kernels%2520often%2520fall%2520back%2520to%2520CPU%2520execution.%2520Existing%2520frameworks%2520handle%2520these%2520fallbacks%2520poorly%252C%2520leaving%2520CPU%2520cores%2520idle%2520and%2520causing%2520high%2520latency%2520and%2520memory%2520spikes.%2520We%2520introduce%2520Parallax%252C%2520a%2520framework%2520that%2520accelerates%2520mobile%2520DNN%2520inference%2520without%2520model%2520refactoring%2520or%2520custom%2520operator%2520implementations.%2520Parallax%2520first%2520partitions%2520the%2520computation%2520DAG%2520to%2520expose%2520parallelism%252C%2520then%2520employs%2520branch-aware%2520memory%2520management%2520with%2520dedicated%2520arenas%2520and%2520buffer%2520reuse%2520to%2520reduce%2520runtime%2520footprint.%2520An%2520adaptive%2520scheduler%2520executes%2520branches%2520according%2520to%2520device%2520memory%2520constraints%252C%2520meanwhile%252C%2520fine-grained%2520subgraph%2520control%2520enables%2520heterogeneous%2520inference%2520of%2520dynamic%2520models.%2520By%2520evaluating%2520on%2520five%2520representative%2520DNNs%2520across%2520three%2520different%2520mobile%2520devices%252C%2520Parallax%2520achieves%2520up%2520to%252046%2525%2520latency%2520reduction%252C%2520maintains%2520controlled%2520memory%2520overhead%2520%252826.5%2525%2520on%2520average%2529%252C%2520and%2520delivers%2520up%2520to%252030%2525%2520energy%2520savings%2520compared%2520with%2520state-of-the-art%2520frameworks%252C%2520offering%2520improvements%2520aligned%2520with%2520the%2520responsiveness%2520demands%2520of%2520real-time%2520mobile%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallax%3A%20Runtime%20Parallelization%20for%20Operator%20Fallbacks%20in%20Heterogeneous%20Edge%20Systems&entry.906535625=Chong%20Tang%20and%20Hao%20Dai%20and%20Jagmohan%20Chauhan&entry.1292438233=The%20growing%20demand%20for%20real-time%20DNN%20applications%20on%20edge%20devices%20necessitates%20faster%20inference%20of%20increasingly%20complex%20models.%20Although%20many%20devices%20include%20specialized%20accelerators%20%28e.g.%2C%20mobile%20GPUs%29%2C%20dynamic%20control-flow%20operators%20and%20unsupported%20kernels%20often%20fall%20back%20to%20CPU%20execution.%20Existing%20frameworks%20handle%20these%20fallbacks%20poorly%2C%20leaving%20CPU%20cores%20idle%20and%20causing%20high%20latency%20and%20memory%20spikes.%20We%20introduce%20Parallax%2C%20a%20framework%20that%20accelerates%20mobile%20DNN%20inference%20without%20model%20refactoring%20or%20custom%20operator%20implementations.%20Parallax%20first%20partitions%20the%20computation%20DAG%20to%20expose%20parallelism%2C%20then%20employs%20branch-aware%20memory%20management%20with%20dedicated%20arenas%20and%20buffer%20reuse%20to%20reduce%20runtime%20footprint.%20An%20adaptive%20scheduler%20executes%20branches%20according%20to%20device%20memory%20constraints%2C%20meanwhile%2C%20fine-grained%20subgraph%20control%20enables%20heterogeneous%20inference%20of%20dynamic%20models.%20By%20evaluating%20on%20five%20representative%20DNNs%20across%20three%20different%20mobile%20devices%2C%20Parallax%20achieves%20up%20to%2046%25%20latency%20reduction%2C%20maintains%20controlled%20memory%20overhead%20%2826.5%25%20on%20average%29%2C%20and%20delivers%20up%20to%2030%25%20energy%20savings%20compared%20with%20state-of-the-art%20frameworks%2C%20offering%20improvements%20aligned%20with%20the%20responsiveness%20demands%20of%20real-time%20mobile%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2512.11532v1&entry.124074799=Read"},
{"title": "UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations", "author": "Tingyu Yuan and Biaoliang Guan and Wen Ye and Ziyan Tian and Yi Yang and Weijie Zhou and Yan Huang and Peng Wang and Chaoyang Zhao and Jinqiao Wang", "abstract": "In embodied intelligence, the embodiment gap between robotic and human hands brings significant challenges for learning from human demonstrations. Although some studies have attempted to bridge this gap using reinforcement learning, they remain confined to merely reproducing human manipulation, resulting in limited task performance. In this paper, we propose UniBYD, a unified framework that uses a dynamic reinforcement learning algorithm to discover manipulation policies aligned with the robot's physical characteristics. To enable consistent modeling across diverse robotic hand morphologies, UniBYD incorporates a unified morphological representation (UMR). Building on UMR, we design a dynamic PPO with an annealed reward schedule, enabling reinforcement learning to transition from imitation of human demonstrations to explore policies adapted to diverse robotic morphologies better, thereby going beyond mere imitation of human hands. To address the frequent failures of learning human priors in the early training stage, we design a hybrid Markov-based shadow engine that enables reinforcement learning to imitate human manipulations in a fine-grained manner. To evaluate UniBYD comprehensively, we propose UniManip, the first benchmark encompassing robotic manipulation tasks spanning multiple hand morphologies. Experiments demonstrate a 67.90% improvement in success rate over the current state-of-the-art. Upon acceptance of the paper, we will release our code and benchmark at https://github.com/zhanheng-creator/UniBYD.", "link": "http://arxiv.org/abs/2512.11609v1", "date": "2025-12-12", "relevancy": 1.8852, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6141}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniBYD%3A%20A%20Unified%20Framework%20for%20Learning%20Robotic%20Manipulation%20Across%20Embodiments%20Beyond%20Imitation%20of%20Human%20Demonstrations&body=Title%3A%20UniBYD%3A%20A%20Unified%20Framework%20for%20Learning%20Robotic%20Manipulation%20Across%20Embodiments%20Beyond%20Imitation%20of%20Human%20Demonstrations%0AAuthor%3A%20Tingyu%20Yuan%20and%20Biaoliang%20Guan%20and%20Wen%20Ye%20and%20Ziyan%20Tian%20and%20Yi%20Yang%20and%20Weijie%20Zhou%20and%20Yan%20Huang%20and%20Peng%20Wang%20and%20Chaoyang%20Zhao%20and%20Jinqiao%20Wang%0AAbstract%3A%20In%20embodied%20intelligence%2C%20the%20embodiment%20gap%20between%20robotic%20and%20human%20hands%20brings%20significant%20challenges%20for%20learning%20from%20human%20demonstrations.%20Although%20some%20studies%20have%20attempted%20to%20bridge%20this%20gap%20using%20reinforcement%20learning%2C%20they%20remain%20confined%20to%20merely%20reproducing%20human%20manipulation%2C%20resulting%20in%20limited%20task%20performance.%20In%20this%20paper%2C%20we%20propose%20UniBYD%2C%20a%20unified%20framework%20that%20uses%20a%20dynamic%20reinforcement%20learning%20algorithm%20to%20discover%20manipulation%20policies%20aligned%20with%20the%20robot%27s%20physical%20characteristics.%20To%20enable%20consistent%20modeling%20across%20diverse%20robotic%20hand%20morphologies%2C%20UniBYD%20incorporates%20a%20unified%20morphological%20representation%20%28UMR%29.%20Building%20on%20UMR%2C%20we%20design%20a%20dynamic%20PPO%20with%20an%20annealed%20reward%20schedule%2C%20enabling%20reinforcement%20learning%20to%20transition%20from%20imitation%20of%20human%20demonstrations%20to%20explore%20policies%20adapted%20to%20diverse%20robotic%20morphologies%20better%2C%20thereby%20going%20beyond%20mere%20imitation%20of%20human%20hands.%20To%20address%20the%20frequent%20failures%20of%20learning%20human%20priors%20in%20the%20early%20training%20stage%2C%20we%20design%20a%20hybrid%20Markov-based%20shadow%20engine%20that%20enables%20reinforcement%20learning%20to%20imitate%20human%20manipulations%20in%20a%20fine-grained%20manner.%20To%20evaluate%20UniBYD%20comprehensively%2C%20we%20propose%20UniManip%2C%20the%20first%20benchmark%20encompassing%20robotic%20manipulation%20tasks%20spanning%20multiple%20hand%20morphologies.%20Experiments%20demonstrate%20a%2067.90%25%20improvement%20in%20success%20rate%20over%20the%20current%20state-of-the-art.%20Upon%20acceptance%20of%20the%20paper%2C%20we%20will%20release%20our%20code%20and%20benchmark%20at%20https%3A//github.com/zhanheng-creator/UniBYD.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniBYD%253A%2520A%2520Unified%2520Framework%2520for%2520Learning%2520Robotic%2520Manipulation%2520Across%2520Embodiments%2520Beyond%2520Imitation%2520of%2520Human%2520Demonstrations%26entry.906535625%3DTingyu%2520Yuan%2520and%2520Biaoliang%2520Guan%2520and%2520Wen%2520Ye%2520and%2520Ziyan%2520Tian%2520and%2520Yi%2520Yang%2520and%2520Weijie%2520Zhou%2520and%2520Yan%2520Huang%2520and%2520Peng%2520Wang%2520and%2520Chaoyang%2520Zhao%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3DIn%2520embodied%2520intelligence%252C%2520the%2520embodiment%2520gap%2520between%2520robotic%2520and%2520human%2520hands%2520brings%2520significant%2520challenges%2520for%2520learning%2520from%2520human%2520demonstrations.%2520Although%2520some%2520studies%2520have%2520attempted%2520to%2520bridge%2520this%2520gap%2520using%2520reinforcement%2520learning%252C%2520they%2520remain%2520confined%2520to%2520merely%2520reproducing%2520human%2520manipulation%252C%2520resulting%2520in%2520limited%2520task%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520UniBYD%252C%2520a%2520unified%2520framework%2520that%2520uses%2520a%2520dynamic%2520reinforcement%2520learning%2520algorithm%2520to%2520discover%2520manipulation%2520policies%2520aligned%2520with%2520the%2520robot%2527s%2520physical%2520characteristics.%2520To%2520enable%2520consistent%2520modeling%2520across%2520diverse%2520robotic%2520hand%2520morphologies%252C%2520UniBYD%2520incorporates%2520a%2520unified%2520morphological%2520representation%2520%2528UMR%2529.%2520Building%2520on%2520UMR%252C%2520we%2520design%2520a%2520dynamic%2520PPO%2520with%2520an%2520annealed%2520reward%2520schedule%252C%2520enabling%2520reinforcement%2520learning%2520to%2520transition%2520from%2520imitation%2520of%2520human%2520demonstrations%2520to%2520explore%2520policies%2520adapted%2520to%2520diverse%2520robotic%2520morphologies%2520better%252C%2520thereby%2520going%2520beyond%2520mere%2520imitation%2520of%2520human%2520hands.%2520To%2520address%2520the%2520frequent%2520failures%2520of%2520learning%2520human%2520priors%2520in%2520the%2520early%2520training%2520stage%252C%2520we%2520design%2520a%2520hybrid%2520Markov-based%2520shadow%2520engine%2520that%2520enables%2520reinforcement%2520learning%2520to%2520imitate%2520human%2520manipulations%2520in%2520a%2520fine-grained%2520manner.%2520To%2520evaluate%2520UniBYD%2520comprehensively%252C%2520we%2520propose%2520UniManip%252C%2520the%2520first%2520benchmark%2520encompassing%2520robotic%2520manipulation%2520tasks%2520spanning%2520multiple%2520hand%2520morphologies.%2520Experiments%2520demonstrate%2520a%252067.90%2525%2520improvement%2520in%2520success%2520rate%2520over%2520the%2520current%2520state-of-the-art.%2520Upon%2520acceptance%2520of%2520the%2520paper%252C%2520we%2520will%2520release%2520our%2520code%2520and%2520benchmark%2520at%2520https%253A//github.com/zhanheng-creator/UniBYD.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniBYD%3A%20A%20Unified%20Framework%20for%20Learning%20Robotic%20Manipulation%20Across%20Embodiments%20Beyond%20Imitation%20of%20Human%20Demonstrations&entry.906535625=Tingyu%20Yuan%20and%20Biaoliang%20Guan%20and%20Wen%20Ye%20and%20Ziyan%20Tian%20and%20Yi%20Yang%20and%20Weijie%20Zhou%20and%20Yan%20Huang%20and%20Peng%20Wang%20and%20Chaoyang%20Zhao%20and%20Jinqiao%20Wang&entry.1292438233=In%20embodied%20intelligence%2C%20the%20embodiment%20gap%20between%20robotic%20and%20human%20hands%20brings%20significant%20challenges%20for%20learning%20from%20human%20demonstrations.%20Although%20some%20studies%20have%20attempted%20to%20bridge%20this%20gap%20using%20reinforcement%20learning%2C%20they%20remain%20confined%20to%20merely%20reproducing%20human%20manipulation%2C%20resulting%20in%20limited%20task%20performance.%20In%20this%20paper%2C%20we%20propose%20UniBYD%2C%20a%20unified%20framework%20that%20uses%20a%20dynamic%20reinforcement%20learning%20algorithm%20to%20discover%20manipulation%20policies%20aligned%20with%20the%20robot%27s%20physical%20characteristics.%20To%20enable%20consistent%20modeling%20across%20diverse%20robotic%20hand%20morphologies%2C%20UniBYD%20incorporates%20a%20unified%20morphological%20representation%20%28UMR%29.%20Building%20on%20UMR%2C%20we%20design%20a%20dynamic%20PPO%20with%20an%20annealed%20reward%20schedule%2C%20enabling%20reinforcement%20learning%20to%20transition%20from%20imitation%20of%20human%20demonstrations%20to%20explore%20policies%20adapted%20to%20diverse%20robotic%20morphologies%20better%2C%20thereby%20going%20beyond%20mere%20imitation%20of%20human%20hands.%20To%20address%20the%20frequent%20failures%20of%20learning%20human%20priors%20in%20the%20early%20training%20stage%2C%20we%20design%20a%20hybrid%20Markov-based%20shadow%20engine%20that%20enables%20reinforcement%20learning%20to%20imitate%20human%20manipulations%20in%20a%20fine-grained%20manner.%20To%20evaluate%20UniBYD%20comprehensively%2C%20we%20propose%20UniManip%2C%20the%20first%20benchmark%20encompassing%20robotic%20manipulation%20tasks%20spanning%20multiple%20hand%20morphologies.%20Experiments%20demonstrate%20a%2067.90%25%20improvement%20in%20success%20rate%20over%20the%20current%20state-of-the-art.%20Upon%20acceptance%20of%20the%20paper%2C%20we%20will%20release%20our%20code%20and%20benchmark%20at%20https%3A//github.com/zhanheng-creator/UniBYD.&entry.1838667208=http%3A//arxiv.org/abs/2512.11609v1&entry.124074799=Read"},
{"title": "Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols", "author": "Bj\u00f6rn Deiseroth and Max Henning H\u00f6th and Kristian Kersting and Letitia Parcalabescu", "abstract": "Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.", "link": "http://arxiv.org/abs/2512.11614v1", "date": "2025-12-12", "relevancy": 1.4838, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5042}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4959}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bounding%20Hallucinations%3A%20Information-Theoretic%20Guarantees%20for%20RAG%20Systems%20via%20Merlin-Arthur%20Protocols&body=Title%3A%20Bounding%20Hallucinations%3A%20Information-Theoretic%20Guarantees%20for%20RAG%20Systems%20via%20Merlin-Arthur%20Protocols%0AAuthor%3A%20Bj%C3%B6rn%20Deiseroth%20and%20Max%20Henning%20H%C3%B6th%20and%20Kristian%20Kersting%20and%20Letitia%20Parcalabescu%0AAbstract%3A%20Retrieval-augmented%20generation%20%28RAG%29%20models%20rely%20on%20retrieved%20evidence%20to%20guide%20large%20language%20model%20%28LLM%29%20generators%2C%20yet%20current%20systems%20treat%20retrieval%20as%20a%20weak%20heuristic%20rather%20than%20verifiable%20evidence.%20As%20a%20result%2C%20LLMs%20answer%20without%20support%2C%20hallucinate%20under%20incomplete%20or%20misleading%20context%2C%20and%20rely%20on%20spurious%20evidence.%20We%20introduce%20a%20training%20framework%20that%20treats%20the%20entire%20RAG%20pipeline%20--%20both%20the%20retriever%20and%20the%20generator%20--%20as%20an%20interactive%20proof%20system%20via%20an%20adaptation%20of%20the%20Merlin-Arthur%20%28M/A%29%20protocol.%20Arthur%20%28the%20generator%20LLM%29%20trains%20on%20questions%20of%20unkown%20provenance%3A%20Merlin%20provides%20helpful%20evidence%2C%20while%20Morgana%20injects%20adversarial%2C%20misleading%20context.%20Both%20use%20a%20linear-time%20XAI%20method%20to%20identify%20and%20modify%20the%20evidence%20most%20influential%20to%20Arthur.%20Consequently%2C%20Arthur%20learns%20to%20%28i%29%20answer%20when%20the%20context%20support%20the%20answer%2C%20%28ii%29%20reject%20when%20evidence%20is%20insufficient%2C%20and%20%28iii%29%20rely%20on%20the%20specific%20context%20spans%20that%20truly%20ground%20the%20answer.%20We%20further%20introduce%20a%20rigorous%20evaluation%20framework%20to%20disentangle%20explanation%20fidelity%20from%20baseline%20predictive%20errors.%20This%20allows%20us%20to%20introduce%20and%20measure%20the%20Explained%20Information%20Fraction%20%28EIF%29%2C%20which%20normalizes%20M/A%20certified%20mutual-information%20guarantees%20relative%20to%20model%20capacity%20and%20imperfect%20benchmarks.%20Across%20three%20RAG%20datasets%20and%20two%20model%20families%20of%20varying%20sizes%2C%20M/A-trained%20LLMs%20show%20improved%20groundedness%2C%20completeness%2C%20soundness%2C%20and%20reject%20behavior%2C%20as%20well%20as%20reduced%20hallucinations%20--%20without%20needing%20manually%20annotated%20unanswerable%20questions.%20The%20retriever%20likewise%20improves%20recall%20and%20MRR%20through%20automatically%20generated%20M/A%20hard%20positives%20and%20negatives.%20Our%20results%20demonstrate%20that%20autonomous%20interactive-proof-style%20supervision%20provides%20a%20principled%20and%20practical%20path%20toward%20reliable%20RAG%20systems%20that%20treat%20retrieved%20documents%20not%20as%20suggestions%2C%20but%20as%20verifiable%20evidence.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBounding%2520Hallucinations%253A%2520Information-Theoretic%2520Guarantees%2520for%2520RAG%2520Systems%2520via%2520Merlin-Arthur%2520Protocols%26entry.906535625%3DBj%25C3%25B6rn%2520Deiseroth%2520and%2520Max%2520Henning%2520H%25C3%25B6th%2520and%2520Kristian%2520Kersting%2520and%2520Letitia%2520Parcalabescu%26entry.1292438233%3DRetrieval-augmented%2520generation%2520%2528RAG%2529%2520models%2520rely%2520on%2520retrieved%2520evidence%2520to%2520guide%2520large%2520language%2520model%2520%2528LLM%2529%2520generators%252C%2520yet%2520current%2520systems%2520treat%2520retrieval%2520as%2520a%2520weak%2520heuristic%2520rather%2520than%2520verifiable%2520evidence.%2520As%2520a%2520result%252C%2520LLMs%2520answer%2520without%2520support%252C%2520hallucinate%2520under%2520incomplete%2520or%2520misleading%2520context%252C%2520and%2520rely%2520on%2520spurious%2520evidence.%2520We%2520introduce%2520a%2520training%2520framework%2520that%2520treats%2520the%2520entire%2520RAG%2520pipeline%2520--%2520both%2520the%2520retriever%2520and%2520the%2520generator%2520--%2520as%2520an%2520interactive%2520proof%2520system%2520via%2520an%2520adaptation%2520of%2520the%2520Merlin-Arthur%2520%2528M/A%2529%2520protocol.%2520Arthur%2520%2528the%2520generator%2520LLM%2529%2520trains%2520on%2520questions%2520of%2520unkown%2520provenance%253A%2520Merlin%2520provides%2520helpful%2520evidence%252C%2520while%2520Morgana%2520injects%2520adversarial%252C%2520misleading%2520context.%2520Both%2520use%2520a%2520linear-time%2520XAI%2520method%2520to%2520identify%2520and%2520modify%2520the%2520evidence%2520most%2520influential%2520to%2520Arthur.%2520Consequently%252C%2520Arthur%2520learns%2520to%2520%2528i%2529%2520answer%2520when%2520the%2520context%2520support%2520the%2520answer%252C%2520%2528ii%2529%2520reject%2520when%2520evidence%2520is%2520insufficient%252C%2520and%2520%2528iii%2529%2520rely%2520on%2520the%2520specific%2520context%2520spans%2520that%2520truly%2520ground%2520the%2520answer.%2520We%2520further%2520introduce%2520a%2520rigorous%2520evaluation%2520framework%2520to%2520disentangle%2520explanation%2520fidelity%2520from%2520baseline%2520predictive%2520errors.%2520This%2520allows%2520us%2520to%2520introduce%2520and%2520measure%2520the%2520Explained%2520Information%2520Fraction%2520%2528EIF%2529%252C%2520which%2520normalizes%2520M/A%2520certified%2520mutual-information%2520guarantees%2520relative%2520to%2520model%2520capacity%2520and%2520imperfect%2520benchmarks.%2520Across%2520three%2520RAG%2520datasets%2520and%2520two%2520model%2520families%2520of%2520varying%2520sizes%252C%2520M/A-trained%2520LLMs%2520show%2520improved%2520groundedness%252C%2520completeness%252C%2520soundness%252C%2520and%2520reject%2520behavior%252C%2520as%2520well%2520as%2520reduced%2520hallucinations%2520--%2520without%2520needing%2520manually%2520annotated%2520unanswerable%2520questions.%2520The%2520retriever%2520likewise%2520improves%2520recall%2520and%2520MRR%2520through%2520automatically%2520generated%2520M/A%2520hard%2520positives%2520and%2520negatives.%2520Our%2520results%2520demonstrate%2520that%2520autonomous%2520interactive-proof-style%2520supervision%2520provides%2520a%2520principled%2520and%2520practical%2520path%2520toward%2520reliable%2520RAG%2520systems%2520that%2520treat%2520retrieved%2520documents%2520not%2520as%2520suggestions%252C%2520but%2520as%2520verifiable%2520evidence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bounding%20Hallucinations%3A%20Information-Theoretic%20Guarantees%20for%20RAG%20Systems%20via%20Merlin-Arthur%20Protocols&entry.906535625=Bj%C3%B6rn%20Deiseroth%20and%20Max%20Henning%20H%C3%B6th%20and%20Kristian%20Kersting%20and%20Letitia%20Parcalabescu&entry.1292438233=Retrieval-augmented%20generation%20%28RAG%29%20models%20rely%20on%20retrieved%20evidence%20to%20guide%20large%20language%20model%20%28LLM%29%20generators%2C%20yet%20current%20systems%20treat%20retrieval%20as%20a%20weak%20heuristic%20rather%20than%20verifiable%20evidence.%20As%20a%20result%2C%20LLMs%20answer%20without%20support%2C%20hallucinate%20under%20incomplete%20or%20misleading%20context%2C%20and%20rely%20on%20spurious%20evidence.%20We%20introduce%20a%20training%20framework%20that%20treats%20the%20entire%20RAG%20pipeline%20--%20both%20the%20retriever%20and%20the%20generator%20--%20as%20an%20interactive%20proof%20system%20via%20an%20adaptation%20of%20the%20Merlin-Arthur%20%28M/A%29%20protocol.%20Arthur%20%28the%20generator%20LLM%29%20trains%20on%20questions%20of%20unkown%20provenance%3A%20Merlin%20provides%20helpful%20evidence%2C%20while%20Morgana%20injects%20adversarial%2C%20misleading%20context.%20Both%20use%20a%20linear-time%20XAI%20method%20to%20identify%20and%20modify%20the%20evidence%20most%20influential%20to%20Arthur.%20Consequently%2C%20Arthur%20learns%20to%20%28i%29%20answer%20when%20the%20context%20support%20the%20answer%2C%20%28ii%29%20reject%20when%20evidence%20is%20insufficient%2C%20and%20%28iii%29%20rely%20on%20the%20specific%20context%20spans%20that%20truly%20ground%20the%20answer.%20We%20further%20introduce%20a%20rigorous%20evaluation%20framework%20to%20disentangle%20explanation%20fidelity%20from%20baseline%20predictive%20errors.%20This%20allows%20us%20to%20introduce%20and%20measure%20the%20Explained%20Information%20Fraction%20%28EIF%29%2C%20which%20normalizes%20M/A%20certified%20mutual-information%20guarantees%20relative%20to%20model%20capacity%20and%20imperfect%20benchmarks.%20Across%20three%20RAG%20datasets%20and%20two%20model%20families%20of%20varying%20sizes%2C%20M/A-trained%20LLMs%20show%20improved%20groundedness%2C%20completeness%2C%20soundness%2C%20and%20reject%20behavior%2C%20as%20well%20as%20reduced%20hallucinations%20--%20without%20needing%20manually%20annotated%20unanswerable%20questions.%20The%20retriever%20likewise%20improves%20recall%20and%20MRR%20through%20automatically%20generated%20M/A%20hard%20positives%20and%20negatives.%20Our%20results%20demonstrate%20that%20autonomous%20interactive-proof-style%20supervision%20provides%20a%20principled%20and%20practical%20path%20toward%20reliable%20RAG%20systems%20that%20treat%20retrieved%20documents%20not%20as%20suggestions%2C%20but%20as%20verifiable%20evidence.&entry.1838667208=http%3A//arxiv.org/abs/2512.11614v1&entry.124074799=Read"},
{"title": "Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis", "author": "Hyungrok Do and Yuyan Wang and Mengling Liu and Myeonggyun Lee", "abstract": "Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\\texttt{https://github.com/hyungrok-do/NeuralPLSI}).", "link": "http://arxiv.org/abs/2512.11593v1", "date": "2025-12-12", "relevancy": 1.4315, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4986}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4713}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Network-based%20Partial-Linear%20Single-Index%20Models%20for%20Environmental%20Mixtures%20Analysis&body=Title%3A%20Neural%20Network-based%20Partial-Linear%20Single-Index%20Models%20for%20Environmental%20Mixtures%20Analysis%0AAuthor%3A%20Hyungrok%20Do%20and%20Yuyan%20Wang%20and%20Mengling%20Liu%20and%20Myeonggyun%20Lee%0AAbstract%3A%20Evaluating%20the%20health%20effects%20of%20complex%20environmental%20mixtures%20remains%20a%20central%20challenge%20in%20environmental%20health%20research.%20Existing%20approaches%20vary%20in%20their%20flexibility%2C%20interpretability%2C%20scalability%2C%20and%20support%20for%20diverse%20outcome%20types%2C%20often%20limiting%20their%20utility%20in%20real-world%20applications.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20neural%20network-based%20partial-linear%20single-index%20%28NeuralPLSI%29%20modeling%20framework%20that%20bridges%20semiparametric%20regression%20modeling%20interpretability%20with%20the%20expressive%20power%20of%20deep%20learning.%20The%20NeuralPLSI%20model%20constructs%20an%20interpretable%20exposure%20index%20via%20a%20learnable%20projection%20and%20models%20its%20relationship%20with%20the%20outcome%20through%20a%20flexible%20neural%20network.%20The%20framework%20accommodates%20continuous%2C%20binary%2C%20and%20time-to-event%20outcomes%2C%20and%20supports%20inference%20through%20a%20bootstrap-based%20procedure%20that%20yields%20confidence%20intervals%20for%20key%20model%20parameters.%20We%20evaluated%20NeuralPLSI%20through%20simulation%20studies%20under%20a%20range%20of%20scenarios%20and%20applied%20it%20to%20data%20from%20the%20National%20Health%20and%20Nutrition%20Examination%20Survey%20%28NHANES%29%20to%20demonstrate%20its%20practical%20utility.%20Together%2C%20our%20contributions%20establish%20NeuralPLSI%20as%20a%20scalable%2C%20interpretable%2C%20and%20versatile%20modeling%20tool%20for%20mixture%20analysis.%20To%20promote%20adoption%20and%20reproducibility%2C%20we%20release%20a%20user-friendly%20open-source%20software%20package%20that%20implements%20the%20proposed%20methodology%20and%20supports%20downstream%20visualization%20and%20inference%20%28%5Ctexttt%7Bhttps%3A//github.com/hyungrok-do/NeuralPLSI%7D%29.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Network-based%2520Partial-Linear%2520Single-Index%2520Models%2520for%2520Environmental%2520Mixtures%2520Analysis%26entry.906535625%3DHyungrok%2520Do%2520and%2520Yuyan%2520Wang%2520and%2520Mengling%2520Liu%2520and%2520Myeonggyun%2520Lee%26entry.1292438233%3DEvaluating%2520the%2520health%2520effects%2520of%2520complex%2520environmental%2520mixtures%2520remains%2520a%2520central%2520challenge%2520in%2520environmental%2520health%2520research.%2520Existing%2520approaches%2520vary%2520in%2520their%2520flexibility%252C%2520interpretability%252C%2520scalability%252C%2520and%2520support%2520for%2520diverse%2520outcome%2520types%252C%2520often%2520limiting%2520their%2520utility%2520in%2520real-world%2520applications.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520neural%2520network-based%2520partial-linear%2520single-index%2520%2528NeuralPLSI%2529%2520modeling%2520framework%2520that%2520bridges%2520semiparametric%2520regression%2520modeling%2520interpretability%2520with%2520the%2520expressive%2520power%2520of%2520deep%2520learning.%2520The%2520NeuralPLSI%2520model%2520constructs%2520an%2520interpretable%2520exposure%2520index%2520via%2520a%2520learnable%2520projection%2520and%2520models%2520its%2520relationship%2520with%2520the%2520outcome%2520through%2520a%2520flexible%2520neural%2520network.%2520The%2520framework%2520accommodates%2520continuous%252C%2520binary%252C%2520and%2520time-to-event%2520outcomes%252C%2520and%2520supports%2520inference%2520through%2520a%2520bootstrap-based%2520procedure%2520that%2520yields%2520confidence%2520intervals%2520for%2520key%2520model%2520parameters.%2520We%2520evaluated%2520NeuralPLSI%2520through%2520simulation%2520studies%2520under%2520a%2520range%2520of%2520scenarios%2520and%2520applied%2520it%2520to%2520data%2520from%2520the%2520National%2520Health%2520and%2520Nutrition%2520Examination%2520Survey%2520%2528NHANES%2529%2520to%2520demonstrate%2520its%2520practical%2520utility.%2520Together%252C%2520our%2520contributions%2520establish%2520NeuralPLSI%2520as%2520a%2520scalable%252C%2520interpretable%252C%2520and%2520versatile%2520modeling%2520tool%2520for%2520mixture%2520analysis.%2520To%2520promote%2520adoption%2520and%2520reproducibility%252C%2520we%2520release%2520a%2520user-friendly%2520open-source%2520software%2520package%2520that%2520implements%2520the%2520proposed%2520methodology%2520and%2520supports%2520downstream%2520visualization%2520and%2520inference%2520%2528%255Ctexttt%257Bhttps%253A//github.com/hyungrok-do/NeuralPLSI%257D%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network-based%20Partial-Linear%20Single-Index%20Models%20for%20Environmental%20Mixtures%20Analysis&entry.906535625=Hyungrok%20Do%20and%20Yuyan%20Wang%20and%20Mengling%20Liu%20and%20Myeonggyun%20Lee&entry.1292438233=Evaluating%20the%20health%20effects%20of%20complex%20environmental%20mixtures%20remains%20a%20central%20challenge%20in%20environmental%20health%20research.%20Existing%20approaches%20vary%20in%20their%20flexibility%2C%20interpretability%2C%20scalability%2C%20and%20support%20for%20diverse%20outcome%20types%2C%20often%20limiting%20their%20utility%20in%20real-world%20applications.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20neural%20network-based%20partial-linear%20single-index%20%28NeuralPLSI%29%20modeling%20framework%20that%20bridges%20semiparametric%20regression%20modeling%20interpretability%20with%20the%20expressive%20power%20of%20deep%20learning.%20The%20NeuralPLSI%20model%20constructs%20an%20interpretable%20exposure%20index%20via%20a%20learnable%20projection%20and%20models%20its%20relationship%20with%20the%20outcome%20through%20a%20flexible%20neural%20network.%20The%20framework%20accommodates%20continuous%2C%20binary%2C%20and%20time-to-event%20outcomes%2C%20and%20supports%20inference%20through%20a%20bootstrap-based%20procedure%20that%20yields%20confidence%20intervals%20for%20key%20model%20parameters.%20We%20evaluated%20NeuralPLSI%20through%20simulation%20studies%20under%20a%20range%20of%20scenarios%20and%20applied%20it%20to%20data%20from%20the%20National%20Health%20and%20Nutrition%20Examination%20Survey%20%28NHANES%29%20to%20demonstrate%20its%20practical%20utility.%20Together%2C%20our%20contributions%20establish%20NeuralPLSI%20as%20a%20scalable%2C%20interpretable%2C%20and%20versatile%20modeling%20tool%20for%20mixture%20analysis.%20To%20promote%20adoption%20and%20reproducibility%2C%20we%20release%20a%20user-friendly%20open-source%20software%20package%20that%20implements%20the%20proposed%20methodology%20and%20supports%20downstream%20visualization%20and%20inference%20%28%5Ctexttt%7Bhttps%3A//github.com/hyungrok-do/NeuralPLSI%7D%29.&entry.1838667208=http%3A//arxiv.org/abs/2512.11593v1&entry.124074799=Read"},
{"title": "BAID: A Benchmark for Bias Assessment of AI Detectors", "author": "Priyam Basu and Yunfeng Zhang and Vipul Raheja", "abstract": "AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.", "link": "http://arxiv.org/abs/2512.11505v1", "date": "2025-12-12", "relevancy": 1.8473, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4913}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4584}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BAID%3A%20A%20Benchmark%20for%20Bias%20Assessment%20of%20AI%20Detectors&body=Title%3A%20BAID%3A%20A%20Benchmark%20for%20Bias%20Assessment%20of%20AI%20Detectors%0AAuthor%3A%20Priyam%20Basu%20and%20Yunfeng%20Zhang%20and%20Vipul%20Raheja%0AAbstract%3A%20AI-generated%20text%20detectors%20have%20recently%20gained%20adoption%20in%20educational%20and%20professional%20contexts.%20Prior%20research%20has%20uncovered%20isolated%20cases%20of%20bias%2C%20particularly%20against%20English%20Language%20Learners%20%28ELLs%29%20however%2C%20there%20is%20a%20lack%20of%20systematic%20evaluation%20of%20such%20systems%20across%20broader%20sociolinguistic%20factors.%20In%20this%20work%2C%20we%20propose%20BAID%2C%20a%20comprehensive%20evaluation%20framework%20for%20AI%20detectors%20across%20various%20types%20of%20biases.%20As%20a%20part%20of%20the%20framework%2C%20we%20introduce%20over%20200k%20samples%20spanning%207%20major%20categories%3A%20demographics%2C%20age%2C%20educational%20grade%20level%2C%20dialect%2C%20formality%2C%20political%20leaning%2C%20and%20topic.%20We%20also%20generated%20synthetic%20versions%20of%20each%20sample%20with%20carefully%20crafted%20prompts%20to%20preserve%20the%20original%20content%20while%20reflecting%20subgroup-specific%20writing%20styles.%20Using%20this%2C%20we%20evaluate%20four%20open-source%20state-of-the-art%20AI%20text%20detectors%20and%20find%20consistent%20disparities%20in%20detection%20performance%2C%20particularly%20low%20recall%20rates%20for%20texts%20from%20underrepresented%20groups.%20Our%20contributions%20provide%20a%20scalable%2C%20transparent%20approach%20for%20auditing%20AI%20detectors%20and%20emphasize%20the%20need%20for%20bias-aware%20evaluation%20before%20these%20tools%20are%20deployed%20for%20public%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBAID%253A%2520A%2520Benchmark%2520for%2520Bias%2520Assessment%2520of%2520AI%2520Detectors%26entry.906535625%3DPriyam%2520Basu%2520and%2520Yunfeng%2520Zhang%2520and%2520Vipul%2520Raheja%26entry.1292438233%3DAI-generated%2520text%2520detectors%2520have%2520recently%2520gained%2520adoption%2520in%2520educational%2520and%2520professional%2520contexts.%2520Prior%2520research%2520has%2520uncovered%2520isolated%2520cases%2520of%2520bias%252C%2520particularly%2520against%2520English%2520Language%2520Learners%2520%2528ELLs%2529%2520however%252C%2520there%2520is%2520a%2520lack%2520of%2520systematic%2520evaluation%2520of%2520such%2520systems%2520across%2520broader%2520sociolinguistic%2520factors.%2520In%2520this%2520work%252C%2520we%2520propose%2520BAID%252C%2520a%2520comprehensive%2520evaluation%2520framework%2520for%2520AI%2520detectors%2520across%2520various%2520types%2520of%2520biases.%2520As%2520a%2520part%2520of%2520the%2520framework%252C%2520we%2520introduce%2520over%2520200k%2520samples%2520spanning%25207%2520major%2520categories%253A%2520demographics%252C%2520age%252C%2520educational%2520grade%2520level%252C%2520dialect%252C%2520formality%252C%2520political%2520leaning%252C%2520and%2520topic.%2520We%2520also%2520generated%2520synthetic%2520versions%2520of%2520each%2520sample%2520with%2520carefully%2520crafted%2520prompts%2520to%2520preserve%2520the%2520original%2520content%2520while%2520reflecting%2520subgroup-specific%2520writing%2520styles.%2520Using%2520this%252C%2520we%2520evaluate%2520four%2520open-source%2520state-of-the-art%2520AI%2520text%2520detectors%2520and%2520find%2520consistent%2520disparities%2520in%2520detection%2520performance%252C%2520particularly%2520low%2520recall%2520rates%2520for%2520texts%2520from%2520underrepresented%2520groups.%2520Our%2520contributions%2520provide%2520a%2520scalable%252C%2520transparent%2520approach%2520for%2520auditing%2520AI%2520detectors%2520and%2520emphasize%2520the%2520need%2520for%2520bias-aware%2520evaluation%2520before%2520these%2520tools%2520are%2520deployed%2520for%2520public%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAID%3A%20A%20Benchmark%20for%20Bias%20Assessment%20of%20AI%20Detectors&entry.906535625=Priyam%20Basu%20and%20Yunfeng%20Zhang%20and%20Vipul%20Raheja&entry.1292438233=AI-generated%20text%20detectors%20have%20recently%20gained%20adoption%20in%20educational%20and%20professional%20contexts.%20Prior%20research%20has%20uncovered%20isolated%20cases%20of%20bias%2C%20particularly%20against%20English%20Language%20Learners%20%28ELLs%29%20however%2C%20there%20is%20a%20lack%20of%20systematic%20evaluation%20of%20such%20systems%20across%20broader%20sociolinguistic%20factors.%20In%20this%20work%2C%20we%20propose%20BAID%2C%20a%20comprehensive%20evaluation%20framework%20for%20AI%20detectors%20across%20various%20types%20of%20biases.%20As%20a%20part%20of%20the%20framework%2C%20we%20introduce%20over%20200k%20samples%20spanning%207%20major%20categories%3A%20demographics%2C%20age%2C%20educational%20grade%20level%2C%20dialect%2C%20formality%2C%20political%20leaning%2C%20and%20topic.%20We%20also%20generated%20synthetic%20versions%20of%20each%20sample%20with%20carefully%20crafted%20prompts%20to%20preserve%20the%20original%20content%20while%20reflecting%20subgroup-specific%20writing%20styles.%20Using%20this%2C%20we%20evaluate%20four%20open-source%20state-of-the-art%20AI%20text%20detectors%20and%20find%20consistent%20disparities%20in%20detection%20performance%2C%20particularly%20low%20recall%20rates%20for%20texts%20from%20underrepresented%20groups.%20Our%20contributions%20provide%20a%20scalable%2C%20transparent%20approach%20for%20auditing%20AI%20detectors%20and%20emphasize%20the%20need%20for%20bias-aware%20evaluation%20before%20these%20tools%20are%20deployed%20for%20public%20use.&entry.1838667208=http%3A//arxiv.org/abs/2512.11505v1&entry.124074799=Read"},
{"title": "Flowception: Temporally Expansive Flow Matching for Video Generation", "author": "Tariq Berrada Ifriqi and John Nguyen and Karteek Alahari and Jakob Verbeek and Ricky T. Q. Chen", "abstract": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.", "link": "http://arxiv.org/abs/2512.11438v1", "date": "2025-12-12", "relevancy": 1.8069, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6111}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.606}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flowception%3A%20Temporally%20Expansive%20Flow%20Matching%20for%20Video%20Generation&body=Title%3A%20Flowception%3A%20Temporally%20Expansive%20Flow%20Matching%20for%20Video%20Generation%0AAuthor%3A%20Tariq%20Berrada%20Ifriqi%20and%20John%20Nguyen%20and%20Karteek%20Alahari%20and%20Jakob%20Verbeek%20and%20Ricky%20T.%20Q.%20Chen%0AAbstract%3A%20We%20present%20Flowception%2C%20a%20novel%20non-autoregressive%20and%20variable-length%20video%20generation%20framework.%20Flowception%20learns%20a%20probability%20path%20that%20interleaves%20discrete%20frame%20insertions%20with%20continuous%20frame%20denoising.%20Compared%20to%20autoregressive%20methods%2C%20Flowception%20alleviates%20error%20accumulation/drift%20as%20the%20frame%20insertion%20mechanism%20during%20sampling%20serves%20as%20an%20efficient%20compression%20mechanism%20to%20handle%20long-term%20context.%20Compared%20to%20full-sequence%20flows%2C%20our%20method%20reduces%20FLOPs%20for%20training%20three-fold%2C%20while%20also%20being%20more%20amenable%20to%20local%20attention%20variants%2C%20and%20allowing%20to%20learn%20the%20length%20of%20videos%20jointly%20with%20their%20content.%20Quantitative%20experimental%20results%20show%20improved%20FVD%20and%20VBench%20metrics%20over%20autoregressive%20and%20full-sequence%20baselines%2C%20which%20is%20further%20validated%20with%20qualitative%20results.%20Finally%2C%20by%20learning%20to%20insert%20and%20denoise%20frames%20in%20a%20sequence%2C%20Flowception%20seamlessly%20integrates%20different%20tasks%20such%20as%20image-to-video%20generation%20and%20video%20interpolation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowception%253A%2520Temporally%2520Expansive%2520Flow%2520Matching%2520for%2520Video%2520Generation%26entry.906535625%3DTariq%2520Berrada%2520Ifriqi%2520and%2520John%2520Nguyen%2520and%2520Karteek%2520Alahari%2520and%2520Jakob%2520Verbeek%2520and%2520Ricky%2520T.%2520Q.%2520Chen%26entry.1292438233%3DWe%2520present%2520Flowception%252C%2520a%2520novel%2520non-autoregressive%2520and%2520variable-length%2520video%2520generation%2520framework.%2520Flowception%2520learns%2520a%2520probability%2520path%2520that%2520interleaves%2520discrete%2520frame%2520insertions%2520with%2520continuous%2520frame%2520denoising.%2520Compared%2520to%2520autoregressive%2520methods%252C%2520Flowception%2520alleviates%2520error%2520accumulation/drift%2520as%2520the%2520frame%2520insertion%2520mechanism%2520during%2520sampling%2520serves%2520as%2520an%2520efficient%2520compression%2520mechanism%2520to%2520handle%2520long-term%2520context.%2520Compared%2520to%2520full-sequence%2520flows%252C%2520our%2520method%2520reduces%2520FLOPs%2520for%2520training%2520three-fold%252C%2520while%2520also%2520being%2520more%2520amenable%2520to%2520local%2520attention%2520variants%252C%2520and%2520allowing%2520to%2520learn%2520the%2520length%2520of%2520videos%2520jointly%2520with%2520their%2520content.%2520Quantitative%2520experimental%2520results%2520show%2520improved%2520FVD%2520and%2520VBench%2520metrics%2520over%2520autoregressive%2520and%2520full-sequence%2520baselines%252C%2520which%2520is%2520further%2520validated%2520with%2520qualitative%2520results.%2520Finally%252C%2520by%2520learning%2520to%2520insert%2520and%2520denoise%2520frames%2520in%2520a%2520sequence%252C%2520Flowception%2520seamlessly%2520integrates%2520different%2520tasks%2520such%2520as%2520image-to-video%2520generation%2520and%2520video%2520interpolation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flowception%3A%20Temporally%20Expansive%20Flow%20Matching%20for%20Video%20Generation&entry.906535625=Tariq%20Berrada%20Ifriqi%20and%20John%20Nguyen%20and%20Karteek%20Alahari%20and%20Jakob%20Verbeek%20and%20Ricky%20T.%20Q.%20Chen&entry.1292438233=We%20present%20Flowception%2C%20a%20novel%20non-autoregressive%20and%20variable-length%20video%20generation%20framework.%20Flowception%20learns%20a%20probability%20path%20that%20interleaves%20discrete%20frame%20insertions%20with%20continuous%20frame%20denoising.%20Compared%20to%20autoregressive%20methods%2C%20Flowception%20alleviates%20error%20accumulation/drift%20as%20the%20frame%20insertion%20mechanism%20during%20sampling%20serves%20as%20an%20efficient%20compression%20mechanism%20to%20handle%20long-term%20context.%20Compared%20to%20full-sequence%20flows%2C%20our%20method%20reduces%20FLOPs%20for%20training%20three-fold%2C%20while%20also%20being%20more%20amenable%20to%20local%20attention%20variants%2C%20and%20allowing%20to%20learn%20the%20length%20of%20videos%20jointly%20with%20their%20content.%20Quantitative%20experimental%20results%20show%20improved%20FVD%20and%20VBench%20metrics%20over%20autoregressive%20and%20full-sequence%20baselines%2C%20which%20is%20further%20validated%20with%20qualitative%20results.%20Finally%2C%20by%20learning%20to%20insert%20and%20denoise%20frames%20in%20a%20sequence%2C%20Flowception%20seamlessly%20integrates%20different%20tasks%20such%20as%20image-to-video%20generation%20and%20video%20interpolation.&entry.1838667208=http%3A//arxiv.org/abs/2512.11438v1&entry.124074799=Read"},
{"title": "Diffusion-based Surrogate Model for Time-varying Underwater Acoustic Channels", "author": "Kexin Li and Mandar Chitre", "abstract": "Accurate modeling of time-varying underwater acoustic channels is essential for the design, evaluation, and deployment of reliable underwater communication systems. Conventional physics models require detailed environmental knowledge, while stochastic replay methods are constrained by the limited diversity of measured channels and often fail to generalize to unseen scenarios, reducing their practical applicability. To address these challenges, we propose StableUASim, a pre-trained conditional latent diffusion surrogate model that captures the stochastic dynamics of underwater acoustic communication channels. Leveraging generative modeling, StableUASim produces diverse and statistically realistic channel realizations, while supporting conditional generation from specific measurement samples. Pre-training enables rapid adaptation to new environments using minimal additional data, and the autoencoder latent representation facilitates efficient channel analysis and compression. Experimental results demonstrate that StableUASim accurately reproduces key channel characteristics and communication performance, providing a scalable, data-efficient, and physically consistent surrogate model for both system design and machine learning-driven underwater applications.", "link": "http://arxiv.org/abs/2511.18078v2", "date": "2025-12-12", "relevancy": 1.019, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5504}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5002}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Surrogate%20Model%20for%20Time-varying%20Underwater%20Acoustic%20Channels&body=Title%3A%20Diffusion-based%20Surrogate%20Model%20for%20Time-varying%20Underwater%20Acoustic%20Channels%0AAuthor%3A%20Kexin%20Li%20and%20Mandar%20Chitre%0AAbstract%3A%20Accurate%20modeling%20of%20time-varying%20underwater%20acoustic%20channels%20is%20essential%20for%20the%20design%2C%20evaluation%2C%20and%20deployment%20of%20reliable%20underwater%20communication%20systems.%20Conventional%20physics%20models%20require%20detailed%20environmental%20knowledge%2C%20while%20stochastic%20replay%20methods%20are%20constrained%20by%20the%20limited%20diversity%20of%20measured%20channels%20and%20often%20fail%20to%20generalize%20to%20unseen%20scenarios%2C%20reducing%20their%20practical%20applicability.%20To%20address%20these%20challenges%2C%20we%20propose%20StableUASim%2C%20a%20pre-trained%20conditional%20latent%20diffusion%20surrogate%20model%20that%20captures%20the%20stochastic%20dynamics%20of%20underwater%20acoustic%20communication%20channels.%20Leveraging%20generative%20modeling%2C%20StableUASim%20produces%20diverse%20and%20statistically%20realistic%20channel%20realizations%2C%20while%20supporting%20conditional%20generation%20from%20specific%20measurement%20samples.%20Pre-training%20enables%20rapid%20adaptation%20to%20new%20environments%20using%20minimal%20additional%20data%2C%20and%20the%20autoencoder%20latent%20representation%20facilitates%20efficient%20channel%20analysis%20and%20compression.%20Experimental%20results%20demonstrate%20that%20StableUASim%20accurately%20reproduces%20key%20channel%20characteristics%20and%20communication%20performance%2C%20providing%20a%20scalable%2C%20data-efficient%2C%20and%20physically%20consistent%20surrogate%20model%20for%20both%20system%20design%20and%20machine%20learning-driven%20underwater%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-based%2520Surrogate%2520Model%2520for%2520Time-varying%2520Underwater%2520Acoustic%2520Channels%26entry.906535625%3DKexin%2520Li%2520and%2520Mandar%2520Chitre%26entry.1292438233%3DAccurate%2520modeling%2520of%2520time-varying%2520underwater%2520acoustic%2520channels%2520is%2520essential%2520for%2520the%2520design%252C%2520evaluation%252C%2520and%2520deployment%2520of%2520reliable%2520underwater%2520communication%2520systems.%2520Conventional%2520physics%2520models%2520require%2520detailed%2520environmental%2520knowledge%252C%2520while%2520stochastic%2520replay%2520methods%2520are%2520constrained%2520by%2520the%2520limited%2520diversity%2520of%2520measured%2520channels%2520and%2520often%2520fail%2520to%2520generalize%2520to%2520unseen%2520scenarios%252C%2520reducing%2520their%2520practical%2520applicability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520StableUASim%252C%2520a%2520pre-trained%2520conditional%2520latent%2520diffusion%2520surrogate%2520model%2520that%2520captures%2520the%2520stochastic%2520dynamics%2520of%2520underwater%2520acoustic%2520communication%2520channels.%2520Leveraging%2520generative%2520modeling%252C%2520StableUASim%2520produces%2520diverse%2520and%2520statistically%2520realistic%2520channel%2520realizations%252C%2520while%2520supporting%2520conditional%2520generation%2520from%2520specific%2520measurement%2520samples.%2520Pre-training%2520enables%2520rapid%2520adaptation%2520to%2520new%2520environments%2520using%2520minimal%2520additional%2520data%252C%2520and%2520the%2520autoencoder%2520latent%2520representation%2520facilitates%2520efficient%2520channel%2520analysis%2520and%2520compression.%2520Experimental%2520results%2520demonstrate%2520that%2520StableUASim%2520accurately%2520reproduces%2520key%2520channel%2520characteristics%2520and%2520communication%2520performance%252C%2520providing%2520a%2520scalable%252C%2520data-efficient%252C%2520and%2520physically%2520consistent%2520surrogate%2520model%2520for%2520both%2520system%2520design%2520and%2520machine%2520learning-driven%2520underwater%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Surrogate%20Model%20for%20Time-varying%20Underwater%20Acoustic%20Channels&entry.906535625=Kexin%20Li%20and%20Mandar%20Chitre&entry.1292438233=Accurate%20modeling%20of%20time-varying%20underwater%20acoustic%20channels%20is%20essential%20for%20the%20design%2C%20evaluation%2C%20and%20deployment%20of%20reliable%20underwater%20communication%20systems.%20Conventional%20physics%20models%20require%20detailed%20environmental%20knowledge%2C%20while%20stochastic%20replay%20methods%20are%20constrained%20by%20the%20limited%20diversity%20of%20measured%20channels%20and%20often%20fail%20to%20generalize%20to%20unseen%20scenarios%2C%20reducing%20their%20practical%20applicability.%20To%20address%20these%20challenges%2C%20we%20propose%20StableUASim%2C%20a%20pre-trained%20conditional%20latent%20diffusion%20surrogate%20model%20that%20captures%20the%20stochastic%20dynamics%20of%20underwater%20acoustic%20communication%20channels.%20Leveraging%20generative%20modeling%2C%20StableUASim%20produces%20diverse%20and%20statistically%20realistic%20channel%20realizations%2C%20while%20supporting%20conditional%20generation%20from%20specific%20measurement%20samples.%20Pre-training%20enables%20rapid%20adaptation%20to%20new%20environments%20using%20minimal%20additional%20data%2C%20and%20the%20autoencoder%20latent%20representation%20facilitates%20efficient%20channel%20analysis%20and%20compression.%20Experimental%20results%20demonstrate%20that%20StableUASim%20accurately%20reproduces%20key%20channel%20characteristics%20and%20communication%20performance%2C%20providing%20a%20scalable%2C%20data-efficient%2C%20and%20physically%20consistent%20surrogate%20model%20for%20both%20system%20design%20and%20machine%20learning-driven%20underwater%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.18078v2&entry.124074799=Read"},
{"title": "Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling", "author": "Keerthana Murugaraj and Salima Lamsiyah and Marten During and Martin Theobald", "abstract": "Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.", "link": "http://arxiv.org/abs/2512.11635v1", "date": "2025-12-12", "relevancy": 1.8858, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4873}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20Historical%20Insight%20Extraction%20from%20Large-Scale%20Newspaper%20Archives%20via%20Neural%20Topic%20Modeling&body=Title%3A%20Automating%20Historical%20Insight%20Extraction%20from%20Large-Scale%20Newspaper%20Archives%20via%20Neural%20Topic%20Modeling%0AAuthor%3A%20Keerthana%20Murugaraj%20and%20Salima%20Lamsiyah%20and%20Marten%20During%20and%20Martin%20Theobald%0AAbstract%3A%20Extracting%20coherent%20and%20human-understandable%20themes%20from%20large%20collections%20of%20unstructured%20historical%20newspaper%20archives%20presents%20significant%20challenges%20due%20to%20topic%20evolution%2C%20Optical%20Character%20Recognition%20%28OCR%29%20noise%2C%20and%20the%20sheer%20volume%20of%20text.%20Traditional%20topic-modeling%20methods%2C%20such%20as%20Latent%20Dirichlet%20Allocation%20%28LDA%29%2C%20often%20fall%20short%20in%20capturing%20the%20complexity%20and%20dynamic%20nature%20of%20discourse%20in%20historical%20texts.%20To%20address%20these%20limitations%2C%20we%20employ%20BERTopic.%20This%20neural%20topic-modeling%20approach%20leverages%20transformerbased%20embeddings%20to%20extract%20and%20classify%20topics%2C%20which%2C%20despite%20its%20growing%20popularity%2C%20still%20remains%20underused%20in%20historical%20research.%20Our%20study%20focuses%20on%20articles%20published%20between%201955%20and%202018%2C%20specifically%20examining%20discourse%20on%20nuclear%20power%20and%20nuclear%20safety.%20We%20analyze%20various%20topic%20distributions%20across%20the%20corpus%20and%20trace%20their%20temporal%20evolution%20to%20uncover%20long-term%20trends%20and%20shifts%20in%20public%20discourse.%20This%20enables%20us%20to%20more%20accurately%20explore%20patterns%20in%20public%20discourse%2C%20including%20the%20co-occurrence%20of%20themes%20related%20to%20nuclear%20power%20and%20nuclear%20weapons%20and%20their%20shifts%20in%20topic%20importance%20over%20time.%20Our%20study%20demonstrates%20the%20scalability%20and%20contextual%20sensitivity%20of%20BERTopic%20as%20an%20alternative%20to%20traditional%20approaches%2C%20offering%20richer%20insights%20into%20historical%20discourses%20extracted%20from%20newspaper%20archives.%20These%20findings%20contribute%20to%20historical%2C%20nuclear%2C%20and%20social-science%20research%20while%20reflecting%20on%20current%20limitations%20and%20proposing%20potential%20directions%20for%20future%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520Historical%2520Insight%2520Extraction%2520from%2520Large-Scale%2520Newspaper%2520Archives%2520via%2520Neural%2520Topic%2520Modeling%26entry.906535625%3DKeerthana%2520Murugaraj%2520and%2520Salima%2520Lamsiyah%2520and%2520Marten%2520During%2520and%2520Martin%2520Theobald%26entry.1292438233%3DExtracting%2520coherent%2520and%2520human-understandable%2520themes%2520from%2520large%2520collections%2520of%2520unstructured%2520historical%2520newspaper%2520archives%2520presents%2520significant%2520challenges%2520due%2520to%2520topic%2520evolution%252C%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520noise%252C%2520and%2520the%2520sheer%2520volume%2520of%2520text.%2520Traditional%2520topic-modeling%2520methods%252C%2520such%2520as%2520Latent%2520Dirichlet%2520Allocation%2520%2528LDA%2529%252C%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520complexity%2520and%2520dynamic%2520nature%2520of%2520discourse%2520in%2520historical%2520texts.%2520To%2520address%2520these%2520limitations%252C%2520we%2520employ%2520BERTopic.%2520This%2520neural%2520topic-modeling%2520approach%2520leverages%2520transformerbased%2520embeddings%2520to%2520extract%2520and%2520classify%2520topics%252C%2520which%252C%2520despite%2520its%2520growing%2520popularity%252C%2520still%2520remains%2520underused%2520in%2520historical%2520research.%2520Our%2520study%2520focuses%2520on%2520articles%2520published%2520between%25201955%2520and%25202018%252C%2520specifically%2520examining%2520discourse%2520on%2520nuclear%2520power%2520and%2520nuclear%2520safety.%2520We%2520analyze%2520various%2520topic%2520distributions%2520across%2520the%2520corpus%2520and%2520trace%2520their%2520temporal%2520evolution%2520to%2520uncover%2520long-term%2520trends%2520and%2520shifts%2520in%2520public%2520discourse.%2520This%2520enables%2520us%2520to%2520more%2520accurately%2520explore%2520patterns%2520in%2520public%2520discourse%252C%2520including%2520the%2520co-occurrence%2520of%2520themes%2520related%2520to%2520nuclear%2520power%2520and%2520nuclear%2520weapons%2520and%2520their%2520shifts%2520in%2520topic%2520importance%2520over%2520time.%2520Our%2520study%2520demonstrates%2520the%2520scalability%2520and%2520contextual%2520sensitivity%2520of%2520BERTopic%2520as%2520an%2520alternative%2520to%2520traditional%2520approaches%252C%2520offering%2520richer%2520insights%2520into%2520historical%2520discourses%2520extracted%2520from%2520newspaper%2520archives.%2520These%2520findings%2520contribute%2520to%2520historical%252C%2520nuclear%252C%2520and%2520social-science%2520research%2520while%2520reflecting%2520on%2520current%2520limitations%2520and%2520proposing%2520potential%2520directions%2520for%2520future%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20Historical%20Insight%20Extraction%20from%20Large-Scale%20Newspaper%20Archives%20via%20Neural%20Topic%20Modeling&entry.906535625=Keerthana%20Murugaraj%20and%20Salima%20Lamsiyah%20and%20Marten%20During%20and%20Martin%20Theobald&entry.1292438233=Extracting%20coherent%20and%20human-understandable%20themes%20from%20large%20collections%20of%20unstructured%20historical%20newspaper%20archives%20presents%20significant%20challenges%20due%20to%20topic%20evolution%2C%20Optical%20Character%20Recognition%20%28OCR%29%20noise%2C%20and%20the%20sheer%20volume%20of%20text.%20Traditional%20topic-modeling%20methods%2C%20such%20as%20Latent%20Dirichlet%20Allocation%20%28LDA%29%2C%20often%20fall%20short%20in%20capturing%20the%20complexity%20and%20dynamic%20nature%20of%20discourse%20in%20historical%20texts.%20To%20address%20these%20limitations%2C%20we%20employ%20BERTopic.%20This%20neural%20topic-modeling%20approach%20leverages%20transformerbased%20embeddings%20to%20extract%20and%20classify%20topics%2C%20which%2C%20despite%20its%20growing%20popularity%2C%20still%20remains%20underused%20in%20historical%20research.%20Our%20study%20focuses%20on%20articles%20published%20between%201955%20and%202018%2C%20specifically%20examining%20discourse%20on%20nuclear%20power%20and%20nuclear%20safety.%20We%20analyze%20various%20topic%20distributions%20across%20the%20corpus%20and%20trace%20their%20temporal%20evolution%20to%20uncover%20long-term%20trends%20and%20shifts%20in%20public%20discourse.%20This%20enables%20us%20to%20more%20accurately%20explore%20patterns%20in%20public%20discourse%2C%20including%20the%20co-occurrence%20of%20themes%20related%20to%20nuclear%20power%20and%20nuclear%20weapons%20and%20their%20shifts%20in%20topic%20importance%20over%20time.%20Our%20study%20demonstrates%20the%20scalability%20and%20contextual%20sensitivity%20of%20BERTopic%20as%20an%20alternative%20to%20traditional%20approaches%2C%20offering%20richer%20insights%20into%20historical%20discourses%20extracted%20from%20newspaper%20archives.%20These%20findings%20contribute%20to%20historical%2C%20nuclear%2C%20and%20social-science%20research%20while%20reflecting%20on%20current%20limitations%20and%20proposing%20potential%20directions%20for%20future%20work.&entry.1838667208=http%3A//arxiv.org/abs/2512.11635v1&entry.124074799=Read"},
{"title": "Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models", "author": "Fabrizio Falasca", "abstract": "A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green's function experiments. To explore the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and to probe causal mechanisms. We argue that the ability of emulators of multiscale systems to reproduce perturbed statistics depends critically on (i) the choice of an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. These insights highlight reduced-order models, tailored to specific goals, processes, and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application by developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system's probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. Overall, these results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory providing a principled framework to guide model design and enhance causal understanding.", "link": "http://arxiv.org/abs/2506.22552v6", "date": "2025-12-12", "relevancy": 0.9238, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.476}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4695}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20forced%20responses%20and%20causality%20in%20data-driven%20climate%20emulators%3A%20conceptual%20limitations%20and%20the%20role%20of%20reduced-order%20models&body=Title%3A%20Probing%20forced%20responses%20and%20causality%20in%20data-driven%20climate%20emulators%3A%20conceptual%20limitations%20and%20the%20role%20of%20reduced-order%20models%0AAuthor%3A%20Fabrizio%20Falasca%0AAbstract%3A%20A%20central%20challenge%20in%20climate%20science%20and%20applied%20mathematics%20is%20developing%20data-driven%20models%20of%20multiscale%20systems%20that%20capture%20both%20stationary%20statistics%20and%20responses%20to%20external%20perturbations.%20Current%20neural%20climate%20emulators%20aim%20to%20resolve%20the%20atmosphere-ocean%20system%20in%20all%20its%20complexity%20but%20often%20struggle%20to%20reproduce%20forced%20responses%2C%20limiting%20their%20use%20in%20causal%20studies%20such%20as%20Green%27s%20function%20experiments.%20To%20explore%20the%20origin%20of%20these%20limitations%2C%20we%20first%20examine%20a%20simplified%20dynamical%20system%20that%20retains%20key%20features%20of%20climate%20variability.%20We%20interpret%20the%20results%20through%20linear%20response%20theory%2C%20providing%20a%20rigorous%20framework%20to%20evaluate%20neural%20models%20beyond%20stationary%20statistics%20and%20to%20probe%20causal%20mechanisms.%20We%20argue%20that%20the%20ability%20of%20emulators%20of%20multiscale%20systems%20to%20reproduce%20perturbed%20statistics%20depends%20critically%20on%20%28i%29%20the%20choice%20of%20an%20appropriate%20coarse-grained%20representation%20and%20%28ii%29%20careful%20parameterizations%20of%20unresolved%20processes.%20These%20insights%20highlight%20reduced-order%20models%2C%20tailored%20to%20specific%20goals%2C%20processes%2C%20and%20scales%2C%20as%20valuable%20alternatives%20to%20general-purpose%20emulators.%20We%20next%20consider%20a%20real-world%20application%20by%20developing%20a%20neural%20model%20to%20investigate%20the%20joint%20variability%20of%20the%20surface%20temperature%20field%20and%20radiative%20fluxes.%20The%20model%20infers%20a%20multiplicative%20noise%20process%20directly%20from%20data%2C%20largely%20reproduces%20the%20system%27s%20probability%20distribution%2C%20and%20enables%20causal%20studies%20through%20forced%20responses.%20We%20discuss%20its%20limitations%20and%20outline%20directions%20for%20future%20work.%20Overall%2C%20these%20results%20expose%20key%20challenges%20in%20data-driven%20modeling%20of%20multiscale%20physical%20systems%20and%20underscore%20the%20value%20of%20coarse-grained%2C%20stochastic%20approaches%2C%20with%20response%20theory%20providing%20a%20principled%20framework%20to%20guide%20model%20design%20and%20enhance%20causal%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2506.22552v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520forced%2520responses%2520and%2520causality%2520in%2520data-driven%2520climate%2520emulators%253A%2520conceptual%2520limitations%2520and%2520the%2520role%2520of%2520reduced-order%2520models%26entry.906535625%3DFabrizio%2520Falasca%26entry.1292438233%3DA%2520central%2520challenge%2520in%2520climate%2520science%2520and%2520applied%2520mathematics%2520is%2520developing%2520data-driven%2520models%2520of%2520multiscale%2520systems%2520that%2520capture%2520both%2520stationary%2520statistics%2520and%2520responses%2520to%2520external%2520perturbations.%2520Current%2520neural%2520climate%2520emulators%2520aim%2520to%2520resolve%2520the%2520atmosphere-ocean%2520system%2520in%2520all%2520its%2520complexity%2520but%2520often%2520struggle%2520to%2520reproduce%2520forced%2520responses%252C%2520limiting%2520their%2520use%2520in%2520causal%2520studies%2520such%2520as%2520Green%2527s%2520function%2520experiments.%2520To%2520explore%2520the%2520origin%2520of%2520these%2520limitations%252C%2520we%2520first%2520examine%2520a%2520simplified%2520dynamical%2520system%2520that%2520retains%2520key%2520features%2520of%2520climate%2520variability.%2520We%2520interpret%2520the%2520results%2520through%2520linear%2520response%2520theory%252C%2520providing%2520a%2520rigorous%2520framework%2520to%2520evaluate%2520neural%2520models%2520beyond%2520stationary%2520statistics%2520and%2520to%2520probe%2520causal%2520mechanisms.%2520We%2520argue%2520that%2520the%2520ability%2520of%2520emulators%2520of%2520multiscale%2520systems%2520to%2520reproduce%2520perturbed%2520statistics%2520depends%2520critically%2520on%2520%2528i%2529%2520the%2520choice%2520of%2520an%2520appropriate%2520coarse-grained%2520representation%2520and%2520%2528ii%2529%2520careful%2520parameterizations%2520of%2520unresolved%2520processes.%2520These%2520insights%2520highlight%2520reduced-order%2520models%252C%2520tailored%2520to%2520specific%2520goals%252C%2520processes%252C%2520and%2520scales%252C%2520as%2520valuable%2520alternatives%2520to%2520general-purpose%2520emulators.%2520We%2520next%2520consider%2520a%2520real-world%2520application%2520by%2520developing%2520a%2520neural%2520model%2520to%2520investigate%2520the%2520joint%2520variability%2520of%2520the%2520surface%2520temperature%2520field%2520and%2520radiative%2520fluxes.%2520The%2520model%2520infers%2520a%2520multiplicative%2520noise%2520process%2520directly%2520from%2520data%252C%2520largely%2520reproduces%2520the%2520system%2527s%2520probability%2520distribution%252C%2520and%2520enables%2520causal%2520studies%2520through%2520forced%2520responses.%2520We%2520discuss%2520its%2520limitations%2520and%2520outline%2520directions%2520for%2520future%2520work.%2520Overall%252C%2520these%2520results%2520expose%2520key%2520challenges%2520in%2520data-driven%2520modeling%2520of%2520multiscale%2520physical%2520systems%2520and%2520underscore%2520the%2520value%2520of%2520coarse-grained%252C%2520stochastic%2520approaches%252C%2520with%2520response%2520theory%2520providing%2520a%2520principled%2520framework%2520to%2520guide%2520model%2520design%2520and%2520enhance%2520causal%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22552v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20forced%20responses%20and%20causality%20in%20data-driven%20climate%20emulators%3A%20conceptual%20limitations%20and%20the%20role%20of%20reduced-order%20models&entry.906535625=Fabrizio%20Falasca&entry.1292438233=A%20central%20challenge%20in%20climate%20science%20and%20applied%20mathematics%20is%20developing%20data-driven%20models%20of%20multiscale%20systems%20that%20capture%20both%20stationary%20statistics%20and%20responses%20to%20external%20perturbations.%20Current%20neural%20climate%20emulators%20aim%20to%20resolve%20the%20atmosphere-ocean%20system%20in%20all%20its%20complexity%20but%20often%20struggle%20to%20reproduce%20forced%20responses%2C%20limiting%20their%20use%20in%20causal%20studies%20such%20as%20Green%27s%20function%20experiments.%20To%20explore%20the%20origin%20of%20these%20limitations%2C%20we%20first%20examine%20a%20simplified%20dynamical%20system%20that%20retains%20key%20features%20of%20climate%20variability.%20We%20interpret%20the%20results%20through%20linear%20response%20theory%2C%20providing%20a%20rigorous%20framework%20to%20evaluate%20neural%20models%20beyond%20stationary%20statistics%20and%20to%20probe%20causal%20mechanisms.%20We%20argue%20that%20the%20ability%20of%20emulators%20of%20multiscale%20systems%20to%20reproduce%20perturbed%20statistics%20depends%20critically%20on%20%28i%29%20the%20choice%20of%20an%20appropriate%20coarse-grained%20representation%20and%20%28ii%29%20careful%20parameterizations%20of%20unresolved%20processes.%20These%20insights%20highlight%20reduced-order%20models%2C%20tailored%20to%20specific%20goals%2C%20processes%2C%20and%20scales%2C%20as%20valuable%20alternatives%20to%20general-purpose%20emulators.%20We%20next%20consider%20a%20real-world%20application%20by%20developing%20a%20neural%20model%20to%20investigate%20the%20joint%20variability%20of%20the%20surface%20temperature%20field%20and%20radiative%20fluxes.%20The%20model%20infers%20a%20multiplicative%20noise%20process%20directly%20from%20data%2C%20largely%20reproduces%20the%20system%27s%20probability%20distribution%2C%20and%20enables%20causal%20studies%20through%20forced%20responses.%20We%20discuss%20its%20limitations%20and%20outline%20directions%20for%20future%20work.%20Overall%2C%20these%20results%20expose%20key%20challenges%20in%20data-driven%20modeling%20of%20multiscale%20physical%20systems%20and%20underscore%20the%20value%20of%20coarse-grained%2C%20stochastic%20approaches%2C%20with%20response%20theory%20providing%20a%20principled%20framework%20to%20guide%20model%20design%20and%20enhance%20causal%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2506.22552v6&entry.124074799=Read"},
{"title": "Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection", "author": "Xingyu Feng and Hebei Gao and Hong Li", "abstract": "We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised instance segmentation and object detection. COLER first uses our developed CutOnce to generate coarse pseudo labels, then enables the detector to learn from these masks. CutOnce applies Normalized Cut (NCut) only once and does not rely on any clustering methods (e.g., K-Means), but it can generate multiple object masks in an image. Our work opens a new direction for NCut algorithm in multi-object segmentation. We have designed several novel yet simple modules that not only allow CutOnce to fully leverage the object discovery capabilities of self-supervised model, but also free it from reliance on mask post-processing. During training, COLER achieves strong performance without requiring specially designed loss functions for pseudo labels, and its performance is further improved through self-training. COLER is a zero-shot unsupervised model that outperforms previous state-of-the-art methods on multiple benchmarks. We believe our method can help advance the field of unsupervised object localization. Code is available at: https://github.com/Quantumcraft616/COLER.", "link": "http://arxiv.org/abs/2508.02386v2", "date": "2025-12-12", "relevancy": 1.5797, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5277}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5266}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Object%20Discovery%20for%20Unsupervised%20Instance%20Segmentation%20and%20Object%20Detection&body=Title%3A%20Enhancing%20Object%20Discovery%20for%20Unsupervised%20Instance%20Segmentation%20and%20Object%20Detection%0AAuthor%3A%20Xingyu%20Feng%20and%20Hebei%20Gao%20and%20Hong%20Li%0AAbstract%3A%20We%20propose%20Cut-Once-and-LEaRn%20%28COLER%29%2C%20a%20simple%20approach%20for%20unsupervised%20instance%20segmentation%20and%20object%20detection.%20COLER%20first%20uses%20our%20developed%20CutOnce%20to%20generate%20coarse%20pseudo%20labels%2C%20then%20enables%20the%20detector%20to%20learn%20from%20these%20masks.%20CutOnce%20applies%20Normalized%20Cut%20%28NCut%29%20only%20once%20and%20does%20not%20rely%20on%20any%20clustering%20methods%20%28e.g.%2C%20K-Means%29%2C%20but%20it%20can%20generate%20multiple%20object%20masks%20in%20an%20image.%20Our%20work%20opens%20a%20new%20direction%20for%20NCut%20algorithm%20in%20multi-object%20segmentation.%20We%20have%20designed%20several%20novel%20yet%20simple%20modules%20that%20not%20only%20allow%20CutOnce%20to%20fully%20leverage%20the%20object%20discovery%20capabilities%20of%20self-supervised%20model%2C%20but%20also%20free%20it%20from%20reliance%20on%20mask%20post-processing.%20During%20training%2C%20COLER%20achieves%20strong%20performance%20without%20requiring%20specially%20designed%20loss%20functions%20for%20pseudo%20labels%2C%20and%20its%20performance%20is%20further%20improved%20through%20self-training.%20COLER%20is%20a%20zero-shot%20unsupervised%20model%20that%20outperforms%20previous%20state-of-the-art%20methods%20on%20multiple%20benchmarks.%20We%20believe%20our%20method%20can%20help%20advance%20the%20field%20of%20unsupervised%20object%20localization.%20Code%20is%20available%20at%3A%20https%3A//github.com/Quantumcraft616/COLER.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Object%2520Discovery%2520for%2520Unsupervised%2520Instance%2520Segmentation%2520and%2520Object%2520Detection%26entry.906535625%3DXingyu%2520Feng%2520and%2520Hebei%2520Gao%2520and%2520Hong%2520Li%26entry.1292438233%3DWe%2520propose%2520Cut-Once-and-LEaRn%2520%2528COLER%2529%252C%2520a%2520simple%2520approach%2520for%2520unsupervised%2520instance%2520segmentation%2520and%2520object%2520detection.%2520COLER%2520first%2520uses%2520our%2520developed%2520CutOnce%2520to%2520generate%2520coarse%2520pseudo%2520labels%252C%2520then%2520enables%2520the%2520detector%2520to%2520learn%2520from%2520these%2520masks.%2520CutOnce%2520applies%2520Normalized%2520Cut%2520%2528NCut%2529%2520only%2520once%2520and%2520does%2520not%2520rely%2520on%2520any%2520clustering%2520methods%2520%2528e.g.%252C%2520K-Means%2529%252C%2520but%2520it%2520can%2520generate%2520multiple%2520object%2520masks%2520in%2520an%2520image.%2520Our%2520work%2520opens%2520a%2520new%2520direction%2520for%2520NCut%2520algorithm%2520in%2520multi-object%2520segmentation.%2520We%2520have%2520designed%2520several%2520novel%2520yet%2520simple%2520modules%2520that%2520not%2520only%2520allow%2520CutOnce%2520to%2520fully%2520leverage%2520the%2520object%2520discovery%2520capabilities%2520of%2520self-supervised%2520model%252C%2520but%2520also%2520free%2520it%2520from%2520reliance%2520on%2520mask%2520post-processing.%2520During%2520training%252C%2520COLER%2520achieves%2520strong%2520performance%2520without%2520requiring%2520specially%2520designed%2520loss%2520functions%2520for%2520pseudo%2520labels%252C%2520and%2520its%2520performance%2520is%2520further%2520improved%2520through%2520self-training.%2520COLER%2520is%2520a%2520zero-shot%2520unsupervised%2520model%2520that%2520outperforms%2520previous%2520state-of-the-art%2520methods%2520on%2520multiple%2520benchmarks.%2520We%2520believe%2520our%2520method%2520can%2520help%2520advance%2520the%2520field%2520of%2520unsupervised%2520object%2520localization.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/Quantumcraft616/COLER.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Object%20Discovery%20for%20Unsupervised%20Instance%20Segmentation%20and%20Object%20Detection&entry.906535625=Xingyu%20Feng%20and%20Hebei%20Gao%20and%20Hong%20Li&entry.1292438233=We%20propose%20Cut-Once-and-LEaRn%20%28COLER%29%2C%20a%20simple%20approach%20for%20unsupervised%20instance%20segmentation%20and%20object%20detection.%20COLER%20first%20uses%20our%20developed%20CutOnce%20to%20generate%20coarse%20pseudo%20labels%2C%20then%20enables%20the%20detector%20to%20learn%20from%20these%20masks.%20CutOnce%20applies%20Normalized%20Cut%20%28NCut%29%20only%20once%20and%20does%20not%20rely%20on%20any%20clustering%20methods%20%28e.g.%2C%20K-Means%29%2C%20but%20it%20can%20generate%20multiple%20object%20masks%20in%20an%20image.%20Our%20work%20opens%20a%20new%20direction%20for%20NCut%20algorithm%20in%20multi-object%20segmentation.%20We%20have%20designed%20several%20novel%20yet%20simple%20modules%20that%20not%20only%20allow%20CutOnce%20to%20fully%20leverage%20the%20object%20discovery%20capabilities%20of%20self-supervised%20model%2C%20but%20also%20free%20it%20from%20reliance%20on%20mask%20post-processing.%20During%20training%2C%20COLER%20achieves%20strong%20performance%20without%20requiring%20specially%20designed%20loss%20functions%20for%20pseudo%20labels%2C%20and%20its%20performance%20is%20further%20improved%20through%20self-training.%20COLER%20is%20a%20zero-shot%20unsupervised%20model%20that%20outperforms%20previous%20state-of-the-art%20methods%20on%20multiple%20benchmarks.%20We%20believe%20our%20method%20can%20help%20advance%20the%20field%20of%20unsupervised%20object%20localization.%20Code%20is%20available%20at%3A%20https%3A//github.com/Quantumcraft616/COLER.&entry.1838667208=http%3A//arxiv.org/abs/2508.02386v2&entry.124074799=Read"},
{"title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation", "author": "Hugo Math and Rainer Lienhart", "abstract": "Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning.", "link": "http://arxiv.org/abs/2509.19112v3", "date": "2025-12-12", "relevancy": 0.915, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4827}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4598}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Practical%20Multi-label%20Causal%20Discovery%20in%20High-Dimensional%20Event%20Sequences%20via%20One-Shot%20Graph%20Aggregation&body=Title%3A%20Towards%20Practical%20Multi-label%20Causal%20Discovery%20in%20High-Dimensional%20Event%20Sequences%20via%20One-Shot%20Graph%20Aggregation%0AAuthor%3A%20Hugo%20Math%20and%20Rainer%20Lienhart%0AAbstract%3A%20Understanding%20causality%20in%20event%20sequences%20where%20outcome%20labels%20such%20as%20diseases%20or%20system%20failures%20arise%20from%20preceding%20events%20like%20symptoms%20or%20error%20codes%20is%20critical.%20Yet%20remains%20an%20unsolved%20challenge%20across%20domains%20like%20healthcare%20or%20vehicle%20diagnostics.%20We%20introduce%20CARGO%2C%20a%20scalable%20multi-label%20causal%20discovery%20method%20for%20sparse%2C%20high-dimensional%20event%20sequences%20comprising%20of%20thousands%20of%20unique%20event%20types.%20Using%20two%20pretrained%20causal%20Transformers%20as%20domain-specific%20foundation%20models%20for%20event%20sequences.%20CARGO%20infers%20in%20parallel%2C%20per%20sequence%20one-shot%20causal%20graphs%20and%20aggregates%20them%20using%20an%20adaptive%20frequency%20fusion%20to%20reconstruct%20the%20global%20Markov%20boundaries%20of%20labels.%20This%20two-stage%20approach%20enables%20efficient%20probabilistic%20reasoning%20at%20scale%20while%20bypassing%20the%20intractable%20cost%20of%20full-dataset%20conditional%20independence%20testing.%20Our%20results%20on%20a%20challenging%20real-world%20automotive%20fault%20prediction%20dataset%20with%20over%2029%2C100%20unique%20event%20types%20and%20474%20imbalanced%20labels%20demonstrate%20CARGO%27s%20ability%20to%20perform%20structured%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2509.19112v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Practical%2520Multi-label%2520Causal%2520Discovery%2520in%2520High-Dimensional%2520Event%2520Sequences%2520via%2520One-Shot%2520Graph%2520Aggregation%26entry.906535625%3DHugo%2520Math%2520and%2520Rainer%2520Lienhart%26entry.1292438233%3DUnderstanding%2520causality%2520in%2520event%2520sequences%2520where%2520outcome%2520labels%2520such%2520as%2520diseases%2520or%2520system%2520failures%2520arise%2520from%2520preceding%2520events%2520like%2520symptoms%2520or%2520error%2520codes%2520is%2520critical.%2520Yet%2520remains%2520an%2520unsolved%2520challenge%2520across%2520domains%2520like%2520healthcare%2520or%2520vehicle%2520diagnostics.%2520We%2520introduce%2520CARGO%252C%2520a%2520scalable%2520multi-label%2520causal%2520discovery%2520method%2520for%2520sparse%252C%2520high-dimensional%2520event%2520sequences%2520comprising%2520of%2520thousands%2520of%2520unique%2520event%2520types.%2520Using%2520two%2520pretrained%2520causal%2520Transformers%2520as%2520domain-specific%2520foundation%2520models%2520for%2520event%2520sequences.%2520CARGO%2520infers%2520in%2520parallel%252C%2520per%2520sequence%2520one-shot%2520causal%2520graphs%2520and%2520aggregates%2520them%2520using%2520an%2520adaptive%2520frequency%2520fusion%2520to%2520reconstruct%2520the%2520global%2520Markov%2520boundaries%2520of%2520labels.%2520This%2520two-stage%2520approach%2520enables%2520efficient%2520probabilistic%2520reasoning%2520at%2520scale%2520while%2520bypassing%2520the%2520intractable%2520cost%2520of%2520full-dataset%2520conditional%2520independence%2520testing.%2520Our%2520results%2520on%2520a%2520challenging%2520real-world%2520automotive%2520fault%2520prediction%2520dataset%2520with%2520over%252029%252C100%2520unique%2520event%2520types%2520and%2520474%2520imbalanced%2520labels%2520demonstrate%2520CARGO%2527s%2520ability%2520to%2520perform%2520structured%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19112v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Practical%20Multi-label%20Causal%20Discovery%20in%20High-Dimensional%20Event%20Sequences%20via%20One-Shot%20Graph%20Aggregation&entry.906535625=Hugo%20Math%20and%20Rainer%20Lienhart&entry.1292438233=Understanding%20causality%20in%20event%20sequences%20where%20outcome%20labels%20such%20as%20diseases%20or%20system%20failures%20arise%20from%20preceding%20events%20like%20symptoms%20or%20error%20codes%20is%20critical.%20Yet%20remains%20an%20unsolved%20challenge%20across%20domains%20like%20healthcare%20or%20vehicle%20diagnostics.%20We%20introduce%20CARGO%2C%20a%20scalable%20multi-label%20causal%20discovery%20method%20for%20sparse%2C%20high-dimensional%20event%20sequences%20comprising%20of%20thousands%20of%20unique%20event%20types.%20Using%20two%20pretrained%20causal%20Transformers%20as%20domain-specific%20foundation%20models%20for%20event%20sequences.%20CARGO%20infers%20in%20parallel%2C%20per%20sequence%20one-shot%20causal%20graphs%20and%20aggregates%20them%20using%20an%20adaptive%20frequency%20fusion%20to%20reconstruct%20the%20global%20Markov%20boundaries%20of%20labels.%20This%20two-stage%20approach%20enables%20efficient%20probabilistic%20reasoning%20at%20scale%20while%20bypassing%20the%20intractable%20cost%20of%20full-dataset%20conditional%20independence%20testing.%20Our%20results%20on%20a%20challenging%20real-world%20automotive%20fault%20prediction%20dataset%20with%20over%2029%2C100%20unique%20event%20types%20and%20474%20imbalanced%20labels%20demonstrate%20CARGO%27s%20ability%20to%20perform%20structured%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2509.19112v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


