<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240526.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation\n  for Natural Camera Motion", "author": "Otto Seiskari and Jerry Ylilammi and Valtteri Kaatrasalo and Pekka Rantalankila and Matias Turkulainen and Juho Kannala and Arno Solin", "abstract": "  High-quality scene reconstruction and novel view synthesis based on Gaussian\nSplatting (3DGS) typically require steady, high-quality photographs, often\nimpractical to capture with handheld cameras. We present a method that adapts\nto camera motion and allows high-quality scene reconstruction with handheld\nvideo data suffering from motion blur and rolling shutter distortion. Our\napproach is based on detailed modelling of the physical image formation process\nand utilizes velocities estimated using visual-inertial odometry (VIO). Camera\nposes are considered non-static during the exposure time of a single image\nframe and camera poses are further optimized in the reconstruction process. We\nformulate a differentiable rendering pipeline that leverages screen space\napproximation to efficiently incorporate rolling-shutter and motion blur\neffects into the 3DGS framework. Our results with both synthetic and real data\ndemonstrate superior performance in mitigating camera motion over existing\nmethods, thereby advancing 3DGS in naturalistic settings.\n", "link": "http://arxiv.org/abs/2403.13327v2", "date": "2024-05-24", "relevancy": 3.3466, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7363}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6707}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20on%20the%20Move%3A%20Blur%20and%20Rolling%20Shutter%20Compensation%0A%20%20for%20Natural%20Camera%20Motion&body=Title%3A%20Gaussian%20Splatting%20on%20the%20Move%3A%20Blur%20and%20Rolling%20Shutter%20Compensation%0A%20%20for%20Natural%20Camera%20Motion%0AAuthor%3A%20Otto%20Seiskari%20and%20Jerry%20Ylilammi%20and%20Valtteri%20Kaatrasalo%20and%20Pekka%20Rantalankila%20and%20Matias%20Turkulainen%20and%20Juho%20Kannala%20and%20Arno%20Solin%0AAbstract%3A%20%20%20High-quality%20scene%20reconstruction%20and%20novel%20view%20synthesis%20based%20on%20Gaussian%0ASplatting%20%283DGS%29%20typically%20require%20steady%2C%20high-quality%20photographs%2C%20often%0Aimpractical%20to%20capture%20with%20handheld%20cameras.%20We%20present%20a%20method%20that%20adapts%0Ato%20camera%20motion%20and%20allows%20high-quality%20scene%20reconstruction%20with%20handheld%0Avideo%20data%20suffering%20from%20motion%20blur%20and%20rolling%20shutter%20distortion.%20Our%0Aapproach%20is%20based%20on%20detailed%20modelling%20of%20the%20physical%20image%20formation%20process%0Aand%20utilizes%20velocities%20estimated%20using%20visual-inertial%20odometry%20%28VIO%29.%20Camera%0Aposes%20are%20considered%20non-static%20during%20the%20exposure%20time%20of%20a%20single%20image%0Aframe%20and%20camera%20poses%20are%20further%20optimized%20in%20the%20reconstruction%20process.%20We%0Aformulate%20a%20differentiable%20rendering%20pipeline%20that%20leverages%20screen%20space%0Aapproximation%20to%20efficiently%20incorporate%20rolling-shutter%20and%20motion%20blur%0Aeffects%20into%20the%203DGS%20framework.%20Our%20results%20with%20both%20synthetic%20and%20real%20data%0Ademonstrate%20superior%20performance%20in%20mitigating%20camera%20motion%20over%20existing%0Amethods%2C%20thereby%20advancing%203DGS%20in%20naturalistic%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520on%2520the%2520Move%253A%2520Blur%2520and%2520Rolling%2520Shutter%2520Compensation%250A%2520%2520for%2520Natural%2520Camera%2520Motion%26entry.906535625%3DOtto%2520Seiskari%2520and%2520Jerry%2520Ylilammi%2520and%2520Valtteri%2520Kaatrasalo%2520and%2520Pekka%2520Rantalankila%2520and%2520Matias%2520Turkulainen%2520and%2520Juho%2520Kannala%2520and%2520Arno%2520Solin%26entry.1292438233%3D%2520%2520High-quality%2520scene%2520reconstruction%2520and%2520novel%2520view%2520synthesis%2520based%2520on%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520typically%2520require%2520steady%252C%2520high-quality%2520photographs%252C%2520often%250Aimpractical%2520to%2520capture%2520with%2520handheld%2520cameras.%2520We%2520present%2520a%2520method%2520that%2520adapts%250Ato%2520camera%2520motion%2520and%2520allows%2520high-quality%2520scene%2520reconstruction%2520with%2520handheld%250Avideo%2520data%2520suffering%2520from%2520motion%2520blur%2520and%2520rolling%2520shutter%2520distortion.%2520Our%250Aapproach%2520is%2520based%2520on%2520detailed%2520modelling%2520of%2520the%2520physical%2520image%2520formation%2520process%250Aand%2520utilizes%2520velocities%2520estimated%2520using%2520visual-inertial%2520odometry%2520%2528VIO%2529.%2520Camera%250Aposes%2520are%2520considered%2520non-static%2520during%2520the%2520exposure%2520time%2520of%2520a%2520single%2520image%250Aframe%2520and%2520camera%2520poses%2520are%2520further%2520optimized%2520in%2520the%2520reconstruction%2520process.%2520We%250Aformulate%2520a%2520differentiable%2520rendering%2520pipeline%2520that%2520leverages%2520screen%2520space%250Aapproximation%2520to%2520efficiently%2520incorporate%2520rolling-shutter%2520and%2520motion%2520blur%250Aeffects%2520into%2520the%25203DGS%2520framework.%2520Our%2520results%2520with%2520both%2520synthetic%2520and%2520real%2520data%250Ademonstrate%2520superior%2520performance%2520in%2520mitigating%2520camera%2520motion%2520over%2520existing%250Amethods%252C%2520thereby%2520advancing%25203DGS%2520in%2520naturalistic%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20on%20the%20Move%3A%20Blur%20and%20Rolling%20Shutter%20Compensation%0A%20%20for%20Natural%20Camera%20Motion&entry.906535625=Otto%20Seiskari%20and%20Jerry%20Ylilammi%20and%20Valtteri%20Kaatrasalo%20and%20Pekka%20Rantalankila%20and%20Matias%20Turkulainen%20and%20Juho%20Kannala%20and%20Arno%20Solin&entry.1292438233=%20%20High-quality%20scene%20reconstruction%20and%20novel%20view%20synthesis%20based%20on%20Gaussian%0ASplatting%20%283DGS%29%20typically%20require%20steady%2C%20high-quality%20photographs%2C%20often%0Aimpractical%20to%20capture%20with%20handheld%20cameras.%20We%20present%20a%20method%20that%20adapts%0Ato%20camera%20motion%20and%20allows%20high-quality%20scene%20reconstruction%20with%20handheld%0Avideo%20data%20suffering%20from%20motion%20blur%20and%20rolling%20shutter%20distortion.%20Our%0Aapproach%20is%20based%20on%20detailed%20modelling%20of%20the%20physical%20image%20formation%20process%0Aand%20utilizes%20velocities%20estimated%20using%20visual-inertial%20odometry%20%28VIO%29.%20Camera%0Aposes%20are%20considered%20non-static%20during%20the%20exposure%20time%20of%20a%20single%20image%0Aframe%20and%20camera%20poses%20are%20further%20optimized%20in%20the%20reconstruction%20process.%20We%0Aformulate%20a%20differentiable%20rendering%20pipeline%20that%20leverages%20screen%20space%0Aapproximation%20to%20efficiently%20incorporate%20rolling-shutter%20and%20motion%20blur%0Aeffects%20into%20the%203DGS%20framework.%20Our%20results%20with%20both%20synthetic%20and%20real%20data%0Ademonstrate%20superior%20performance%20in%20mitigating%20camera%20motion%20over%20existing%0Amethods%2C%20thereby%20advancing%203DGS%20in%20naturalistic%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13327v2&entry.124074799=Read"},
{"title": "LAM3D: Large Image-Point-Cloud Alignment Model for 3D Reconstruction\n  from Single Image", "author": "Ruikai Cui and Xibin Song and Weixuan Sun and Senbo Wang and Weizhe Liu and Shenzhou Chen and Taizhang Shang and Yang Li and Nick Barnes and Hongdong Li and Pan Ji", "abstract": "  Large Reconstruction Models have made significant strides in the realm of\nautomated 3D content generation from single or multiple input images. Despite\ntheir success, these models often produce 3D meshes with geometric\ninaccuracies, stemming from the inherent challenges of deducing 3D shapes\nsolely from image data. In this work, we introduce a novel framework, the Large\nImage and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud\ndata to enhance the fidelity of generated 3D meshes. Our methodology begins\nwith the development of a point-cloud-based network that effectively generates\nprecise and meaningful latent tri-planes, laying the groundwork for accurate 3D\nmesh reconstruction. Building upon this, our Image-Point-Cloud Feature\nAlignment technique processes a single input image, aligning to the latent\ntri-planes to imbue image features with robust 3D information. This process not\nonly enriches the image features but also facilitates the production of\nhigh-fidelity 3D meshes without the need for multi-view input, significantly\nreducing geometric distortions. Our approach achieves state-of-the-art\nhigh-fidelity 3D mesh reconstruction from a single image in just 6 seconds, and\nexperiments on various datasets demonstrate its effectiveness.\n", "link": "http://arxiv.org/abs/2405.15622v1", "date": "2024-05-24", "relevancy": 3.1891, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6501}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6501}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAM3D%3A%20Large%20Image-Point-Cloud%20Alignment%20Model%20for%203D%20Reconstruction%0A%20%20from%20Single%20Image&body=Title%3A%20LAM3D%3A%20Large%20Image-Point-Cloud%20Alignment%20Model%20for%203D%20Reconstruction%0A%20%20from%20Single%20Image%0AAuthor%3A%20Ruikai%20Cui%20and%20Xibin%20Song%20and%20Weixuan%20Sun%20and%20Senbo%20Wang%20and%20Weizhe%20Liu%20and%20Shenzhou%20Chen%20and%20Taizhang%20Shang%20and%20Yang%20Li%20and%20Nick%20Barnes%20and%20Hongdong%20Li%20and%20Pan%20Ji%0AAbstract%3A%20%20%20Large%20Reconstruction%20Models%20have%20made%20significant%20strides%20in%20the%20realm%20of%0Aautomated%203D%20content%20generation%20from%20single%20or%20multiple%20input%20images.%20Despite%0Atheir%20success%2C%20these%20models%20often%20produce%203D%20meshes%20with%20geometric%0Ainaccuracies%2C%20stemming%20from%20the%20inherent%20challenges%20of%20deducing%203D%20shapes%0Asolely%20from%20image%20data.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%2C%20the%20Large%0AImage%20and%20Point%20Cloud%20Alignment%20Model%20%28LAM3D%29%2C%20which%20utilizes%203D%20point%20cloud%0Adata%20to%20enhance%20the%20fidelity%20of%20generated%203D%20meshes.%20Our%20methodology%20begins%0Awith%20the%20development%20of%20a%20point-cloud-based%20network%20that%20effectively%20generates%0Aprecise%20and%20meaningful%20latent%20tri-planes%2C%20laying%20the%20groundwork%20for%20accurate%203D%0Amesh%20reconstruction.%20Building%20upon%20this%2C%20our%20Image-Point-Cloud%20Feature%0AAlignment%20technique%20processes%20a%20single%20input%20image%2C%20aligning%20to%20the%20latent%0Atri-planes%20to%20imbue%20image%20features%20with%20robust%203D%20information.%20This%20process%20not%0Aonly%20enriches%20the%20image%20features%20but%20also%20facilitates%20the%20production%20of%0Ahigh-fidelity%203D%20meshes%20without%20the%20need%20for%20multi-view%20input%2C%20significantly%0Areducing%20geometric%20distortions.%20Our%20approach%20achieves%20state-of-the-art%0Ahigh-fidelity%203D%20mesh%20reconstruction%20from%20a%20single%20image%20in%20just%206%20seconds%2C%20and%0Aexperiments%20on%20various%20datasets%20demonstrate%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAM3D%253A%2520Large%2520Image-Point-Cloud%2520Alignment%2520Model%2520for%25203D%2520Reconstruction%250A%2520%2520from%2520Single%2520Image%26entry.906535625%3DRuikai%2520Cui%2520and%2520Xibin%2520Song%2520and%2520Weixuan%2520Sun%2520and%2520Senbo%2520Wang%2520and%2520Weizhe%2520Liu%2520and%2520Shenzhou%2520Chen%2520and%2520Taizhang%2520Shang%2520and%2520Yang%2520Li%2520and%2520Nick%2520Barnes%2520and%2520Hongdong%2520Li%2520and%2520Pan%2520Ji%26entry.1292438233%3D%2520%2520Large%2520Reconstruction%2520Models%2520have%2520made%2520significant%2520strides%2520in%2520the%2520realm%2520of%250Aautomated%25203D%2520content%2520generation%2520from%2520single%2520or%2520multiple%2520input%2520images.%2520Despite%250Atheir%2520success%252C%2520these%2520models%2520often%2520produce%25203D%2520meshes%2520with%2520geometric%250Ainaccuracies%252C%2520stemming%2520from%2520the%2520inherent%2520challenges%2520of%2520deducing%25203D%2520shapes%250Asolely%2520from%2520image%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520framework%252C%2520the%2520Large%250AImage%2520and%2520Point%2520Cloud%2520Alignment%2520Model%2520%2528LAM3D%2529%252C%2520which%2520utilizes%25203D%2520point%2520cloud%250Adata%2520to%2520enhance%2520the%2520fidelity%2520of%2520generated%25203D%2520meshes.%2520Our%2520methodology%2520begins%250Awith%2520the%2520development%2520of%2520a%2520point-cloud-based%2520network%2520that%2520effectively%2520generates%250Aprecise%2520and%2520meaningful%2520latent%2520tri-planes%252C%2520laying%2520the%2520groundwork%2520for%2520accurate%25203D%250Amesh%2520reconstruction.%2520Building%2520upon%2520this%252C%2520our%2520Image-Point-Cloud%2520Feature%250AAlignment%2520technique%2520processes%2520a%2520single%2520input%2520image%252C%2520aligning%2520to%2520the%2520latent%250Atri-planes%2520to%2520imbue%2520image%2520features%2520with%2520robust%25203D%2520information.%2520This%2520process%2520not%250Aonly%2520enriches%2520the%2520image%2520features%2520but%2520also%2520facilitates%2520the%2520production%2520of%250Ahigh-fidelity%25203D%2520meshes%2520without%2520the%2520need%2520for%2520multi-view%2520input%252C%2520significantly%250Areducing%2520geometric%2520distortions.%2520Our%2520approach%2520achieves%2520state-of-the-art%250Ahigh-fidelity%25203D%2520mesh%2520reconstruction%2520from%2520a%2520single%2520image%2520in%2520just%25206%2520seconds%252C%2520and%250Aexperiments%2520on%2520various%2520datasets%2520demonstrate%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAM3D%3A%20Large%20Image-Point-Cloud%20Alignment%20Model%20for%203D%20Reconstruction%0A%20%20from%20Single%20Image&entry.906535625=Ruikai%20Cui%20and%20Xibin%20Song%20and%20Weixuan%20Sun%20and%20Senbo%20Wang%20and%20Weizhe%20Liu%20and%20Shenzhou%20Chen%20and%20Taizhang%20Shang%20and%20Yang%20Li%20and%20Nick%20Barnes%20and%20Hongdong%20Li%20and%20Pan%20Ji&entry.1292438233=%20%20Large%20Reconstruction%20Models%20have%20made%20significant%20strides%20in%20the%20realm%20of%0Aautomated%203D%20content%20generation%20from%20single%20or%20multiple%20input%20images.%20Despite%0Atheir%20success%2C%20these%20models%20often%20produce%203D%20meshes%20with%20geometric%0Ainaccuracies%2C%20stemming%20from%20the%20inherent%20challenges%20of%20deducing%203D%20shapes%0Asolely%20from%20image%20data.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%2C%20the%20Large%0AImage%20and%20Point%20Cloud%20Alignment%20Model%20%28LAM3D%29%2C%20which%20utilizes%203D%20point%20cloud%0Adata%20to%20enhance%20the%20fidelity%20of%20generated%203D%20meshes.%20Our%20methodology%20begins%0Awith%20the%20development%20of%20a%20point-cloud-based%20network%20that%20effectively%20generates%0Aprecise%20and%20meaningful%20latent%20tri-planes%2C%20laying%20the%20groundwork%20for%20accurate%203D%0Amesh%20reconstruction.%20Building%20upon%20this%2C%20our%20Image-Point-Cloud%20Feature%0AAlignment%20technique%20processes%20a%20single%20input%20image%2C%20aligning%20to%20the%20latent%0Atri-planes%20to%20imbue%20image%20features%20with%20robust%203D%20information.%20This%20process%20not%0Aonly%20enriches%20the%20image%20features%20but%20also%20facilitates%20the%20production%20of%0Ahigh-fidelity%203D%20meshes%20without%20the%20need%20for%20multi-view%20input%2C%20significantly%0Areducing%20geometric%20distortions.%20Our%20approach%20achieves%20state-of-the-art%0Ahigh-fidelity%203D%20mesh%20reconstruction%20from%20a%20single%20image%20in%20just%206%20seconds%2C%20and%0Aexperiments%20on%20various%20datasets%20demonstrate%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15622v1&entry.124074799=Read"},
{"title": "D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup", "author": "Joanna Waczy\u0144ska and Piotr Borycki and Joanna Kaleta and S\u0142awomir Tadeja and Przemys\u0142aw Spurek", "abstract": "  Over the past years, we have observed an abundance of approaches for modeling\ndynamic 3D scenes using Gaussian Splatting (GS). Such solutions use GS to\nrepresent the scene's structure and the neural network to model dynamics. Such\napproaches allow fast rendering and extracting each element of such a dynamic\nscene. However, modifying such objects over time is challenging. SC-GS (Sparse\nControlled Gaussian Splatting) enhanced with Deformed Control Points partially\nsolves this issue. However, this approach necessitates selecting elements that\nneed to be kept fixed, as well as centroids that should be adjusted throughout\nediting. Moreover, this task poses additional difficulties regarding the\nre-productivity of such editing. To address this, we propose Dynamic\nMulti-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspired\nrepresentation of dynamic GS. Additionally, we propose a strategy of linking\nparameterized Gaussian splats, forming a Triangle Soup with the estimated mesh.\nConsequently, we can separately construct new trajectories for the 3D objects\ncomposing the scene. Thus, we can make the scene's dynamic editable over time\nor while maintaining partial dynamics.\n", "link": "http://arxiv.org/abs/2405.14276v2", "date": "2024-05-24", "relevancy": 3.183, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7163}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6237}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-MiSo%3A%20Editing%20Dynamic%203D%20Scenes%20using%20Multi-Gaussians%20Soup&body=Title%3A%20D-MiSo%3A%20Editing%20Dynamic%203D%20Scenes%20using%20Multi-Gaussians%20Soup%0AAuthor%3A%20Joanna%20Waczy%C5%84ska%20and%20Piotr%20Borycki%20and%20Joanna%20Kaleta%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Over%20the%20past%20years%2C%20we%20have%20observed%20an%20abundance%20of%20approaches%20for%20modeling%0Adynamic%203D%20scenes%20using%20Gaussian%20Splatting%20%28GS%29.%20Such%20solutions%20use%20GS%20to%0Arepresent%20the%20scene%27s%20structure%20and%20the%20neural%20network%20to%20model%20dynamics.%20Such%0Aapproaches%20allow%20fast%20rendering%20and%20extracting%20each%20element%20of%20such%20a%20dynamic%0Ascene.%20However%2C%20modifying%20such%20objects%20over%20time%20is%20challenging.%20SC-GS%20%28Sparse%0AControlled%20Gaussian%20Splatting%29%20enhanced%20with%20Deformed%20Control%20Points%20partially%0Asolves%20this%20issue.%20However%2C%20this%20approach%20necessitates%20selecting%20elements%20that%0Aneed%20to%20be%20kept%20fixed%2C%20as%20well%20as%20centroids%20that%20should%20be%20adjusted%20throughout%0Aediting.%20Moreover%2C%20this%20task%20poses%20additional%20difficulties%20regarding%20the%0Are-productivity%20of%20such%20editing.%20To%20address%20this%2C%20we%20propose%20Dynamic%0AMulti-Gaussian%20Soup%20%28D-MiSo%29%2C%20which%20allows%20us%20to%20model%20the%20mesh-inspired%0Arepresentation%20of%20dynamic%20GS.%20Additionally%2C%20we%20propose%20a%20strategy%20of%20linking%0Aparameterized%20Gaussian%20splats%2C%20forming%20a%20Triangle%20Soup%20with%20the%20estimated%20mesh.%0AConsequently%2C%20we%20can%20separately%20construct%20new%20trajectories%20for%20the%203D%20objects%0Acomposing%20the%20scene.%20Thus%2C%20we%20can%20make%20the%20scene%27s%20dynamic%20editable%20over%20time%0Aor%20while%20maintaining%20partial%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14276v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-MiSo%253A%2520Editing%2520Dynamic%25203D%2520Scenes%2520using%2520Multi-Gaussians%2520Soup%26entry.906535625%3DJoanna%2520Waczy%25C5%2584ska%2520and%2520Piotr%2520Borycki%2520and%2520Joanna%2520Kaleta%2520and%2520S%25C5%2582awomir%2520Tadeja%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520years%252C%2520we%2520have%2520observed%2520an%2520abundance%2520of%2520approaches%2520for%2520modeling%250Adynamic%25203D%2520scenes%2520using%2520Gaussian%2520Splatting%2520%2528GS%2529.%2520Such%2520solutions%2520use%2520GS%2520to%250Arepresent%2520the%2520scene%2527s%2520structure%2520and%2520the%2520neural%2520network%2520to%2520model%2520dynamics.%2520Such%250Aapproaches%2520allow%2520fast%2520rendering%2520and%2520extracting%2520each%2520element%2520of%2520such%2520a%2520dynamic%250Ascene.%2520However%252C%2520modifying%2520such%2520objects%2520over%2520time%2520is%2520challenging.%2520SC-GS%2520%2528Sparse%250AControlled%2520Gaussian%2520Splatting%2529%2520enhanced%2520with%2520Deformed%2520Control%2520Points%2520partially%250Asolves%2520this%2520issue.%2520However%252C%2520this%2520approach%2520necessitates%2520selecting%2520elements%2520that%250Aneed%2520to%2520be%2520kept%2520fixed%252C%2520as%2520well%2520as%2520centroids%2520that%2520should%2520be%2520adjusted%2520throughout%250Aediting.%2520Moreover%252C%2520this%2520task%2520poses%2520additional%2520difficulties%2520regarding%2520the%250Are-productivity%2520of%2520such%2520editing.%2520To%2520address%2520this%252C%2520we%2520propose%2520Dynamic%250AMulti-Gaussian%2520Soup%2520%2528D-MiSo%2529%252C%2520which%2520allows%2520us%2520to%2520model%2520the%2520mesh-inspired%250Arepresentation%2520of%2520dynamic%2520GS.%2520Additionally%252C%2520we%2520propose%2520a%2520strategy%2520of%2520linking%250Aparameterized%2520Gaussian%2520splats%252C%2520forming%2520a%2520Triangle%2520Soup%2520with%2520the%2520estimated%2520mesh.%250AConsequently%252C%2520we%2520can%2520separately%2520construct%2520new%2520trajectories%2520for%2520the%25203D%2520objects%250Acomposing%2520the%2520scene.%2520Thus%252C%2520we%2520can%2520make%2520the%2520scene%2527s%2520dynamic%2520editable%2520over%2520time%250Aor%2520while%2520maintaining%2520partial%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14276v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-MiSo%3A%20Editing%20Dynamic%203D%20Scenes%20using%20Multi-Gaussians%20Soup&entry.906535625=Joanna%20Waczy%C5%84ska%20and%20Piotr%20Borycki%20and%20Joanna%20Kaleta%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Over%20the%20past%20years%2C%20we%20have%20observed%20an%20abundance%20of%20approaches%20for%20modeling%0Adynamic%203D%20scenes%20using%20Gaussian%20Splatting%20%28GS%29.%20Such%20solutions%20use%20GS%20to%0Arepresent%20the%20scene%27s%20structure%20and%20the%20neural%20network%20to%20model%20dynamics.%20Such%0Aapproaches%20allow%20fast%20rendering%20and%20extracting%20each%20element%20of%20such%20a%20dynamic%0Ascene.%20However%2C%20modifying%20such%20objects%20over%20time%20is%20challenging.%20SC-GS%20%28Sparse%0AControlled%20Gaussian%20Splatting%29%20enhanced%20with%20Deformed%20Control%20Points%20partially%0Asolves%20this%20issue.%20However%2C%20this%20approach%20necessitates%20selecting%20elements%20that%0Aneed%20to%20be%20kept%20fixed%2C%20as%20well%20as%20centroids%20that%20should%20be%20adjusted%20throughout%0Aediting.%20Moreover%2C%20this%20task%20poses%20additional%20difficulties%20regarding%20the%0Are-productivity%20of%20such%20editing.%20To%20address%20this%2C%20we%20propose%20Dynamic%0AMulti-Gaussian%20Soup%20%28D-MiSo%29%2C%20which%20allows%20us%20to%20model%20the%20mesh-inspired%0Arepresentation%20of%20dynamic%20GS.%20Additionally%2C%20we%20propose%20a%20strategy%20of%20linking%0Aparameterized%20Gaussian%20splats%2C%20forming%20a%20Triangle%20Soup%20with%20the%20estimated%20mesh.%0AConsequently%2C%20we%20can%20separately%20construct%20new%20trajectories%20for%20the%203D%20objects%0Acomposing%20the%20scene.%20Thus%2C%20we%20can%20make%20the%20scene%27s%20dynamic%20editable%20over%20time%0Aor%20while%20maintaining%20partial%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14276v2&entry.124074799=Read"},
{"title": "Feature Splatting for Better Novel View Synthesis with Low Overlap", "author": "T. Berriel Martins and Javier Civera", "abstract": "  3D Gaussian Splatting has emerged as a very promising scene representation,\nachieving state-of-the-art quality in novel view synthesis significantly faster\nthan competing alternatives. However, its use of spherical harmonics to\nrepresent scene colors limits the expressivity of 3D Gaussians and, as a\nconsequence, the capability of the representation to generalize as we move away\nfrom the training views. In this paper, we propose to encode the color\ninformation of 3D Gaussians into per-Gaussian feature vectors, which we denote\nas Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are\nfirst \"splatted\" into the image plane, then the corresponding feature vectors\nare alpha-blended, and finally the blended vector is decoded by a small MLP to\nrender the RGB pixel values. To further inform the model, we concatenate a\ncamera embedding to the blended feature vector, to condition the decoding also\non the viewpoint information. Our experiments show that these novel model for\nencoding the radiance considerably improves novel view synthesis for low\noverlap views that are distant from the training views. Finally, we also show\nthe capacity and convenience of our feature vector representation,\ndemonstrating its capability not only to generate RGB values for novel views,\nbut also their per-pixel semantic labels. We will release the code upon\nacceptance.\n  Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting\n", "link": "http://arxiv.org/abs/2405.15518v1", "date": "2024-05-24", "relevancy": 3.0938, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6966}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5798}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Splatting%20for%20Better%20Novel%20View%20Synthesis%20with%20Low%20Overlap&body=Title%3A%20Feature%20Splatting%20for%20Better%20Novel%20View%20Synthesis%20with%20Low%20Overlap%0AAuthor%3A%20T.%20Berriel%20Martins%20and%20Javier%20Civera%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20very%20promising%20scene%20representation%2C%0Aachieving%20state-of-the-art%20quality%20in%20novel%20view%20synthesis%20significantly%20faster%0Athan%20competing%20alternatives.%20However%2C%20its%20use%20of%20spherical%20harmonics%20to%0Arepresent%20scene%20colors%20limits%20the%20expressivity%20of%203D%20Gaussians%20and%2C%20as%20a%0Aconsequence%2C%20the%20capability%20of%20the%20representation%20to%20generalize%20as%20we%20move%20away%0Afrom%20the%20training%20views.%20In%20this%20paper%2C%20we%20propose%20to%20encode%20the%20color%0Ainformation%20of%203D%20Gaussians%20into%20per-Gaussian%20feature%20vectors%2C%20which%20we%20denote%0Aas%20Feature%20Splatting%20%28FeatSplat%29.%20To%20synthesize%20a%20novel%20view%2C%20Gaussians%20are%0Afirst%20%22splatted%22%20into%20the%20image%20plane%2C%20then%20the%20corresponding%20feature%20vectors%0Aare%20alpha-blended%2C%20and%20finally%20the%20blended%20vector%20is%20decoded%20by%20a%20small%20MLP%20to%0Arender%20the%20RGB%20pixel%20values.%20To%20further%20inform%20the%20model%2C%20we%20concatenate%20a%0Acamera%20embedding%20to%20the%20blended%20feature%20vector%2C%20to%20condition%20the%20decoding%20also%0Aon%20the%20viewpoint%20information.%20Our%20experiments%20show%20that%20these%20novel%20model%20for%0Aencoding%20the%20radiance%20considerably%20improves%20novel%20view%20synthesis%20for%20low%0Aoverlap%20views%20that%20are%20distant%20from%20the%20training%20views.%20Finally%2C%20we%20also%20show%0Athe%20capacity%20and%20convenience%20of%20our%20feature%20vector%20representation%2C%0Ademonstrating%20its%20capability%20not%20only%20to%20generate%20RGB%20values%20for%20novel%20views%2C%0Abut%20also%20their%20per-pixel%20semantic%20labels.%20We%20will%20release%20the%20code%20upon%0Aacceptance.%0A%20%20Keywords%3A%20Gaussian%20Splatting%2C%20Novel%20View%20Synthesis%2C%20Feature%20Splatting%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Splatting%2520for%2520Better%2520Novel%2520View%2520Synthesis%2520with%2520Low%2520Overlap%26entry.906535625%3DT.%2520Berriel%2520Martins%2520and%2520Javier%2520Civera%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520a%2520very%2520promising%2520scene%2520representation%252C%250Aachieving%2520state-of-the-art%2520quality%2520in%2520novel%2520view%2520synthesis%2520significantly%2520faster%250Athan%2520competing%2520alternatives.%2520However%252C%2520its%2520use%2520of%2520spherical%2520harmonics%2520to%250Arepresent%2520scene%2520colors%2520limits%2520the%2520expressivity%2520of%25203D%2520Gaussians%2520and%252C%2520as%2520a%250Aconsequence%252C%2520the%2520capability%2520of%2520the%2520representation%2520to%2520generalize%2520as%2520we%2520move%2520away%250Afrom%2520the%2520training%2520views.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520encode%2520the%2520color%250Ainformation%2520of%25203D%2520Gaussians%2520into%2520per-Gaussian%2520feature%2520vectors%252C%2520which%2520we%2520denote%250Aas%2520Feature%2520Splatting%2520%2528FeatSplat%2529.%2520To%2520synthesize%2520a%2520novel%2520view%252C%2520Gaussians%2520are%250Afirst%2520%2522splatted%2522%2520into%2520the%2520image%2520plane%252C%2520then%2520the%2520corresponding%2520feature%2520vectors%250Aare%2520alpha-blended%252C%2520and%2520finally%2520the%2520blended%2520vector%2520is%2520decoded%2520by%2520a%2520small%2520MLP%2520to%250Arender%2520the%2520RGB%2520pixel%2520values.%2520To%2520further%2520inform%2520the%2520model%252C%2520we%2520concatenate%2520a%250Acamera%2520embedding%2520to%2520the%2520blended%2520feature%2520vector%252C%2520to%2520condition%2520the%2520decoding%2520also%250Aon%2520the%2520viewpoint%2520information.%2520Our%2520experiments%2520show%2520that%2520these%2520novel%2520model%2520for%250Aencoding%2520the%2520radiance%2520considerably%2520improves%2520novel%2520view%2520synthesis%2520for%2520low%250Aoverlap%2520views%2520that%2520are%2520distant%2520from%2520the%2520training%2520views.%2520Finally%252C%2520we%2520also%2520show%250Athe%2520capacity%2520and%2520convenience%2520of%2520our%2520feature%2520vector%2520representation%252C%250Ademonstrating%2520its%2520capability%2520not%2520only%2520to%2520generate%2520RGB%2520values%2520for%2520novel%2520views%252C%250Abut%2520also%2520their%2520per-pixel%2520semantic%2520labels.%2520We%2520will%2520release%2520the%2520code%2520upon%250Aacceptance.%250A%2520%2520Keywords%253A%2520Gaussian%2520Splatting%252C%2520Novel%2520View%2520Synthesis%252C%2520Feature%2520Splatting%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Splatting%20for%20Better%20Novel%20View%20Synthesis%20with%20Low%20Overlap&entry.906535625=T.%20Berriel%20Martins%20and%20Javier%20Civera&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20very%20promising%20scene%20representation%2C%0Aachieving%20state-of-the-art%20quality%20in%20novel%20view%20synthesis%20significantly%20faster%0Athan%20competing%20alternatives.%20However%2C%20its%20use%20of%20spherical%20harmonics%20to%0Arepresent%20scene%20colors%20limits%20the%20expressivity%20of%203D%20Gaussians%20and%2C%20as%20a%0Aconsequence%2C%20the%20capability%20of%20the%20representation%20to%20generalize%20as%20we%20move%20away%0Afrom%20the%20training%20views.%20In%20this%20paper%2C%20we%20propose%20to%20encode%20the%20color%0Ainformation%20of%203D%20Gaussians%20into%20per-Gaussian%20feature%20vectors%2C%20which%20we%20denote%0Aas%20Feature%20Splatting%20%28FeatSplat%29.%20To%20synthesize%20a%20novel%20view%2C%20Gaussians%20are%0Afirst%20%22splatted%22%20into%20the%20image%20plane%2C%20then%20the%20corresponding%20feature%20vectors%0Aare%20alpha-blended%2C%20and%20finally%20the%20blended%20vector%20is%20decoded%20by%20a%20small%20MLP%20to%0Arender%20the%20RGB%20pixel%20values.%20To%20further%20inform%20the%20model%2C%20we%20concatenate%20a%0Acamera%20embedding%20to%20the%20blended%20feature%20vector%2C%20to%20condition%20the%20decoding%20also%0Aon%20the%20viewpoint%20information.%20Our%20experiments%20show%20that%20these%20novel%20model%20for%0Aencoding%20the%20radiance%20considerably%20improves%20novel%20view%20synthesis%20for%20low%0Aoverlap%20views%20that%20are%20distant%20from%20the%20training%20views.%20Finally%2C%20we%20also%20show%0Athe%20capacity%20and%20convenience%20of%20our%20feature%20vector%20representation%2C%0Ademonstrating%20its%20capability%20not%20only%20to%20generate%20RGB%20values%20for%20novel%20views%2C%0Abut%20also%20their%20per-pixel%20semantic%20labels.%20We%20will%20release%20the%20code%20upon%0Aacceptance.%0A%20%20Keywords%3A%20Gaussian%20Splatting%2C%20Novel%20View%20Synthesis%2C%20Feature%20Splatting%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15518v1&entry.124074799=Read"},
{"title": "StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time\n  Rendering", "author": "Lukas Radl and Michael Steiner and Mathias Parger and Alexander Weinrauch and Bernhard Kerbl and Markus Steinberger", "abstract": "  Gaussian Splatting has emerged as a prominent model for constructing 3D\nrepresentations from images across diverse domains. However, the efficiency of\nthe 3D Gaussian Splatting rendering pipeline relies on several simplifications.\nNotably, reducing Gaussian to 2D splats with a single view-space depth\nintroduces popping and blending artifacts during view rotation. Addressing this\nissue requires accurate per-pixel depth computation, yet a full per-pixel sort\nproves excessively costly compared to a global sort operation. In this paper,\nwe present a novel hierarchical rasterization approach that systematically\nresorts and culls splats with minimal processing overhead. Our software\nrasterizer effectively eliminates popping artifacts and view inconsistencies,\nas demonstrated through both quantitative and qualitative measurements.\nSimultaneously, our method mitigates the potential for cheating view-dependent\neffects with popping, ensuring a more authentic representation. Despite the\nelimination of cheating, our approach achieves comparable quantitative results\nfor test images, while increasing the consistency for novel view synthesis in\nmotion. Due to its design, our hierarchical approach is only 4% slower on\naverage than the original Gaussian Splatting. Notably, enforcing consistency\nenables a reduction in the number of Gaussians by approximately half with\nnearly identical quality and view-consistency. Consequently, rendering\nperformance is nearly doubled, making our approach 1.6x faster than the\noriginal Gaussian Splatting, with a 50% reduction in memory requirements.\n", "link": "http://arxiv.org/abs/2402.00525v2", "date": "2024-05-24", "relevancy": 3.0763, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.717}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6053}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StopThePop%3A%20Sorted%20Gaussian%20Splatting%20for%20View-Consistent%20Real-time%0A%20%20Rendering&body=Title%3A%20StopThePop%3A%20Sorted%20Gaussian%20Splatting%20for%20View-Consistent%20Real-time%0A%20%20Rendering%0AAuthor%3A%20Lukas%20Radl%20and%20Michael%20Steiner%20and%20Mathias%20Parger%20and%20Alexander%20Weinrauch%20and%20Bernhard%20Kerbl%20and%20Markus%20Steinberger%0AAbstract%3A%20%20%20Gaussian%20Splatting%20has%20emerged%20as%20a%20prominent%20model%20for%20constructing%203D%0Arepresentations%20from%20images%20across%20diverse%20domains.%20However%2C%20the%20efficiency%20of%0Athe%203D%20Gaussian%20Splatting%20rendering%20pipeline%20relies%20on%20several%20simplifications.%0ANotably%2C%20reducing%20Gaussian%20to%202D%20splats%20with%20a%20single%20view-space%20depth%0Aintroduces%20popping%20and%20blending%20artifacts%20during%20view%20rotation.%20Addressing%20this%0Aissue%20requires%20accurate%20per-pixel%20depth%20computation%2C%20yet%20a%20full%20per-pixel%20sort%0Aproves%20excessively%20costly%20compared%20to%20a%20global%20sort%20operation.%20In%20this%20paper%2C%0Awe%20present%20a%20novel%20hierarchical%20rasterization%20approach%20that%20systematically%0Aresorts%20and%20culls%20splats%20with%20minimal%20processing%20overhead.%20Our%20software%0Arasterizer%20effectively%20eliminates%20popping%20artifacts%20and%20view%20inconsistencies%2C%0Aas%20demonstrated%20through%20both%20quantitative%20and%20qualitative%20measurements.%0ASimultaneously%2C%20our%20method%20mitigates%20the%20potential%20for%20cheating%20view-dependent%0Aeffects%20with%20popping%2C%20ensuring%20a%20more%20authentic%20representation.%20Despite%20the%0Aelimination%20of%20cheating%2C%20our%20approach%20achieves%20comparable%20quantitative%20results%0Afor%20test%20images%2C%20while%20increasing%20the%20consistency%20for%20novel%20view%20synthesis%20in%0Amotion.%20Due%20to%20its%20design%2C%20our%20hierarchical%20approach%20is%20only%204%25%20slower%20on%0Aaverage%20than%20the%20original%20Gaussian%20Splatting.%20Notably%2C%20enforcing%20consistency%0Aenables%20a%20reduction%20in%20the%20number%20of%20Gaussians%20by%20approximately%20half%20with%0Anearly%20identical%20quality%20and%20view-consistency.%20Consequently%2C%20rendering%0Aperformance%20is%20nearly%20doubled%2C%20making%20our%20approach%201.6x%20faster%20than%20the%0Aoriginal%20Gaussian%20Splatting%2C%20with%20a%2050%25%20reduction%20in%20memory%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00525v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStopThePop%253A%2520Sorted%2520Gaussian%2520Splatting%2520for%2520View-Consistent%2520Real-time%250A%2520%2520Rendering%26entry.906535625%3DLukas%2520Radl%2520and%2520Michael%2520Steiner%2520and%2520Mathias%2520Parger%2520and%2520Alexander%2520Weinrauch%2520and%2520Bernhard%2520Kerbl%2520and%2520Markus%2520Steinberger%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520a%2520prominent%2520model%2520for%2520constructing%25203D%250Arepresentations%2520from%2520images%2520across%2520diverse%2520domains.%2520However%252C%2520the%2520efficiency%2520of%250Athe%25203D%2520Gaussian%2520Splatting%2520rendering%2520pipeline%2520relies%2520on%2520several%2520simplifications.%250ANotably%252C%2520reducing%2520Gaussian%2520to%25202D%2520splats%2520with%2520a%2520single%2520view-space%2520depth%250Aintroduces%2520popping%2520and%2520blending%2520artifacts%2520during%2520view%2520rotation.%2520Addressing%2520this%250Aissue%2520requires%2520accurate%2520per-pixel%2520depth%2520computation%252C%2520yet%2520a%2520full%2520per-pixel%2520sort%250Aproves%2520excessively%2520costly%2520compared%2520to%2520a%2520global%2520sort%2520operation.%2520In%2520this%2520paper%252C%250Awe%2520present%2520a%2520novel%2520hierarchical%2520rasterization%2520approach%2520that%2520systematically%250Aresorts%2520and%2520culls%2520splats%2520with%2520minimal%2520processing%2520overhead.%2520Our%2520software%250Arasterizer%2520effectively%2520eliminates%2520popping%2520artifacts%2520and%2520view%2520inconsistencies%252C%250Aas%2520demonstrated%2520through%2520both%2520quantitative%2520and%2520qualitative%2520measurements.%250ASimultaneously%252C%2520our%2520method%2520mitigates%2520the%2520potential%2520for%2520cheating%2520view-dependent%250Aeffects%2520with%2520popping%252C%2520ensuring%2520a%2520more%2520authentic%2520representation.%2520Despite%2520the%250Aelimination%2520of%2520cheating%252C%2520our%2520approach%2520achieves%2520comparable%2520quantitative%2520results%250Afor%2520test%2520images%252C%2520while%2520increasing%2520the%2520consistency%2520for%2520novel%2520view%2520synthesis%2520in%250Amotion.%2520Due%2520to%2520its%2520design%252C%2520our%2520hierarchical%2520approach%2520is%2520only%25204%2525%2520slower%2520on%250Aaverage%2520than%2520the%2520original%2520Gaussian%2520Splatting.%2520Notably%252C%2520enforcing%2520consistency%250Aenables%2520a%2520reduction%2520in%2520the%2520number%2520of%2520Gaussians%2520by%2520approximately%2520half%2520with%250Anearly%2520identical%2520quality%2520and%2520view-consistency.%2520Consequently%252C%2520rendering%250Aperformance%2520is%2520nearly%2520doubled%252C%2520making%2520our%2520approach%25201.6x%2520faster%2520than%2520the%250Aoriginal%2520Gaussian%2520Splatting%252C%2520with%2520a%252050%2525%2520reduction%2520in%2520memory%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00525v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StopThePop%3A%20Sorted%20Gaussian%20Splatting%20for%20View-Consistent%20Real-time%0A%20%20Rendering&entry.906535625=Lukas%20Radl%20and%20Michael%20Steiner%20and%20Mathias%20Parger%20and%20Alexander%20Weinrauch%20and%20Bernhard%20Kerbl%20and%20Markus%20Steinberger&entry.1292438233=%20%20Gaussian%20Splatting%20has%20emerged%20as%20a%20prominent%20model%20for%20constructing%203D%0Arepresentations%20from%20images%20across%20diverse%20domains.%20However%2C%20the%20efficiency%20of%0Athe%203D%20Gaussian%20Splatting%20rendering%20pipeline%20relies%20on%20several%20simplifications.%0ANotably%2C%20reducing%20Gaussian%20to%202D%20splats%20with%20a%20single%20view-space%20depth%0Aintroduces%20popping%20and%20blending%20artifacts%20during%20view%20rotation.%20Addressing%20this%0Aissue%20requires%20accurate%20per-pixel%20depth%20computation%2C%20yet%20a%20full%20per-pixel%20sort%0Aproves%20excessively%20costly%20compared%20to%20a%20global%20sort%20operation.%20In%20this%20paper%2C%0Awe%20present%20a%20novel%20hierarchical%20rasterization%20approach%20that%20systematically%0Aresorts%20and%20culls%20splats%20with%20minimal%20processing%20overhead.%20Our%20software%0Arasterizer%20effectively%20eliminates%20popping%20artifacts%20and%20view%20inconsistencies%2C%0Aas%20demonstrated%20through%20both%20quantitative%20and%20qualitative%20measurements.%0ASimultaneously%2C%20our%20method%20mitigates%20the%20potential%20for%20cheating%20view-dependent%0Aeffects%20with%20popping%2C%20ensuring%20a%20more%20authentic%20representation.%20Despite%20the%0Aelimination%20of%20cheating%2C%20our%20approach%20achieves%20comparable%20quantitative%20results%0Afor%20test%20images%2C%20while%20increasing%20the%20consistency%20for%20novel%20view%20synthesis%20in%0Amotion.%20Due%20to%20its%20design%2C%20our%20hierarchical%20approach%20is%20only%204%25%20slower%20on%0Aaverage%20than%20the%20original%20Gaussian%20Splatting.%20Notably%2C%20enforcing%20consistency%0Aenables%20a%20reduction%20in%20the%20number%20of%20Gaussians%20by%20approximately%20half%20with%0Anearly%20identical%20quality%20and%20view-consistency.%20Consequently%2C%20rendering%0Aperformance%20is%20nearly%20doubled%2C%20making%20our%20approach%201.6x%20faster%20than%20the%0Aoriginal%20Gaussian%20Splatting%2C%20with%20a%2050%25%20reduction%20in%20memory%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00525v2&entry.124074799=Read"},
{"title": "Open-Vocabulary SAM3D: Understand Any 3D Scene", "author": "Hanchen Tai and Qingdong He and Jiangning Zhang and Yijie Qian and Zhenyu Zhang and Xiaobin Hu and Yabiao Wang and Yong Liu", "abstract": "  Open-vocabulary 3D scene understanding presents a significant challenge in\nthe field. Recent advancements have sought to transfer knowledge embedded in\nvision language models from the 2D domain to 3D domain. However, these\napproaches often require learning prior knowledge from specific 3D scene\ndatasets, which limits their applicability in open-world scenarios. The Segment\nAnything Model (SAM) has demonstrated remarkable zero-shot segmentation\ncapabilities, prompting us to investigate its potential for comprehending 3D\nscenes without the need for training. In this paper, we introduce OV-SAM3D, a\nuniversal framework for open-vocabulary 3D scene understanding. This framework\nis designed to perform understanding tasks for any 3D scene without requiring\nprior knowledge of the scene. Specifically, our method is composed of two key\nsub-modules: First, we initiate the process by generating superpoints as the\ninitial 3D prompts and refine these prompts using segment masks derived from\nSAM. Moreover, we then integrate a specially designed overlapping score table\nwith open tags from the Recognize Anything Model (RAM) to produce final 3D\ninstances with open-world label. Empirical evaluations conducted on the\nScanNet200 and nuScenes datasets demonstrate that our approach surpasses\nexisting open-vocabulary methods in unknown open-world environments.\n", "link": "http://arxiv.org/abs/2405.15580v1", "date": "2024-05-24", "relevancy": 3.0416, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6152}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6049}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%20SAM3D%3A%20Understand%20Any%203D%20Scene&body=Title%3A%20Open-Vocabulary%20SAM3D%3A%20Understand%20Any%203D%20Scene%0AAuthor%3A%20Hanchen%20Tai%20and%20Qingdong%20He%20and%20Jiangning%20Zhang%20and%20Yijie%20Qian%20and%20Zhenyu%20Zhang%20and%20Xiaobin%20Hu%20and%20Yabiao%20Wang%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Open-vocabulary%203D%20scene%20understanding%20presents%20a%20significant%20challenge%20in%0Athe%20field.%20Recent%20advancements%20have%20sought%20to%20transfer%20knowledge%20embedded%20in%0Avision%20language%20models%20from%20the%202D%20domain%20to%203D%20domain.%20However%2C%20these%0Aapproaches%20often%20require%20learning%20prior%20knowledge%20from%20specific%203D%20scene%0Adatasets%2C%20which%20limits%20their%20applicability%20in%20open-world%20scenarios.%20The%20Segment%0AAnything%20Model%20%28SAM%29%20has%20demonstrated%20remarkable%20zero-shot%20segmentation%0Acapabilities%2C%20prompting%20us%20to%20investigate%20its%20potential%20for%20comprehending%203D%0Ascenes%20without%20the%20need%20for%20training.%20In%20this%20paper%2C%20we%20introduce%20OV-SAM3D%2C%20a%0Auniversal%20framework%20for%20open-vocabulary%203D%20scene%20understanding.%20This%20framework%0Ais%20designed%20to%20perform%20understanding%20tasks%20for%20any%203D%20scene%20without%20requiring%0Aprior%20knowledge%20of%20the%20scene.%20Specifically%2C%20our%20method%20is%20composed%20of%20two%20key%0Asub-modules%3A%20First%2C%20we%20initiate%20the%20process%20by%20generating%20superpoints%20as%20the%0Ainitial%203D%20prompts%20and%20refine%20these%20prompts%20using%20segment%20masks%20derived%20from%0ASAM.%20Moreover%2C%20we%20then%20integrate%20a%20specially%20designed%20overlapping%20score%20table%0Awith%20open%20tags%20from%20the%20Recognize%20Anything%20Model%20%28RAM%29%20to%20produce%20final%203D%0Ainstances%20with%20open-world%20label.%20Empirical%20evaluations%20conducted%20on%20the%0AScanNet200%20and%20nuScenes%20datasets%20demonstrate%20that%20our%20approach%20surpasses%0Aexisting%20open-vocabulary%20methods%20in%20unknown%20open-world%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Vocabulary%2520SAM3D%253A%2520Understand%2520Any%25203D%2520Scene%26entry.906535625%3DHanchen%2520Tai%2520and%2520Qingdong%2520He%2520and%2520Jiangning%2520Zhang%2520and%2520Yijie%2520Qian%2520and%2520Zhenyu%2520Zhang%2520and%2520Xiaobin%2520Hu%2520and%2520Yabiao%2520Wang%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520scene%2520understanding%2520presents%2520a%2520significant%2520challenge%2520in%250Athe%2520field.%2520Recent%2520advancements%2520have%2520sought%2520to%2520transfer%2520knowledge%2520embedded%2520in%250Avision%2520language%2520models%2520from%2520the%25202D%2520domain%2520to%25203D%2520domain.%2520However%252C%2520these%250Aapproaches%2520often%2520require%2520learning%2520prior%2520knowledge%2520from%2520specific%25203D%2520scene%250Adatasets%252C%2520which%2520limits%2520their%2520applicability%2520in%2520open-world%2520scenarios.%2520The%2520Segment%250AAnything%2520Model%2520%2528SAM%2529%2520has%2520demonstrated%2520remarkable%2520zero-shot%2520segmentation%250Acapabilities%252C%2520prompting%2520us%2520to%2520investigate%2520its%2520potential%2520for%2520comprehending%25203D%250Ascenes%2520without%2520the%2520need%2520for%2520training.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520OV-SAM3D%252C%2520a%250Auniversal%2520framework%2520for%2520open-vocabulary%25203D%2520scene%2520understanding.%2520This%2520framework%250Ais%2520designed%2520to%2520perform%2520understanding%2520tasks%2520for%2520any%25203D%2520scene%2520without%2520requiring%250Aprior%2520knowledge%2520of%2520the%2520scene.%2520Specifically%252C%2520our%2520method%2520is%2520composed%2520of%2520two%2520key%250Asub-modules%253A%2520First%252C%2520we%2520initiate%2520the%2520process%2520by%2520generating%2520superpoints%2520as%2520the%250Ainitial%25203D%2520prompts%2520and%2520refine%2520these%2520prompts%2520using%2520segment%2520masks%2520derived%2520from%250ASAM.%2520Moreover%252C%2520we%2520then%2520integrate%2520a%2520specially%2520designed%2520overlapping%2520score%2520table%250Awith%2520open%2520tags%2520from%2520the%2520Recognize%2520Anything%2520Model%2520%2528RAM%2529%2520to%2520produce%2520final%25203D%250Ainstances%2520with%2520open-world%2520label.%2520Empirical%2520evaluations%2520conducted%2520on%2520the%250AScanNet200%2520and%2520nuScenes%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520surpasses%250Aexisting%2520open-vocabulary%2520methods%2520in%2520unknown%2520open-world%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%20SAM3D%3A%20Understand%20Any%203D%20Scene&entry.906535625=Hanchen%20Tai%20and%20Qingdong%20He%20and%20Jiangning%20Zhang%20and%20Yijie%20Qian%20and%20Zhenyu%20Zhang%20and%20Xiaobin%20Hu%20and%20Yabiao%20Wang%20and%20Yong%20Liu&entry.1292438233=%20%20Open-vocabulary%203D%20scene%20understanding%20presents%20a%20significant%20challenge%20in%0Athe%20field.%20Recent%20advancements%20have%20sought%20to%20transfer%20knowledge%20embedded%20in%0Avision%20language%20models%20from%20the%202D%20domain%20to%203D%20domain.%20However%2C%20these%0Aapproaches%20often%20require%20learning%20prior%20knowledge%20from%20specific%203D%20scene%0Adatasets%2C%20which%20limits%20their%20applicability%20in%20open-world%20scenarios.%20The%20Segment%0AAnything%20Model%20%28SAM%29%20has%20demonstrated%20remarkable%20zero-shot%20segmentation%0Acapabilities%2C%20prompting%20us%20to%20investigate%20its%20potential%20for%20comprehending%203D%0Ascenes%20without%20the%20need%20for%20training.%20In%20this%20paper%2C%20we%20introduce%20OV-SAM3D%2C%20a%0Auniversal%20framework%20for%20open-vocabulary%203D%20scene%20understanding.%20This%20framework%0Ais%20designed%20to%20perform%20understanding%20tasks%20for%20any%203D%20scene%20without%20requiring%0Aprior%20knowledge%20of%20the%20scene.%20Specifically%2C%20our%20method%20is%20composed%20of%20two%20key%0Asub-modules%3A%20First%2C%20we%20initiate%20the%20process%20by%20generating%20superpoints%20as%20the%0Ainitial%203D%20prompts%20and%20refine%20these%20prompts%20using%20segment%20masks%20derived%20from%0ASAM.%20Moreover%2C%20we%20then%20integrate%20a%20specially%20designed%20overlapping%20score%20table%0Awith%20open%20tags%20from%20the%20Recognize%20Anything%20Model%20%28RAM%29%20to%20produce%20final%203D%0Ainstances%20with%20open-world%20label.%20Empirical%20evaluations%20conducted%20on%20the%0AScanNet200%20and%20nuScenes%20datasets%20demonstrate%20that%20our%20approach%20surpasses%0Aexisting%20open-vocabulary%20methods%20in%20unknown%20open-world%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15580v1&entry.124074799=Read"},
{"title": "GES: Generalized Exponential Splatting for Efficient Radiance Field\n  Rendering", "author": "Abdullah Hamdi and Luke Melas-Kyriazi and Jinjie Mai and Guocheng Qian and Ruoshi Liu and Carl Vondrick and Bernard Ghanem and Andrea Vedaldi", "abstract": "  Advancements in 3D Gaussian Splatting have significantly accelerated 3D\nreconstruction and generation. However, it may require a large number of\nGaussians, which creates a substantial memory footprint. This paper introduces\nGES (Generalized Exponential Splatting), a novel representation that employs\nGeneralized Exponential Function (GEF) to model 3D scenes, requiring far fewer\nparticles to represent a scene and thus significantly outperforming Gaussian\nSplatting methods in efficiency with a plug-and-play replacement ability for\nGaussian-based utilities. GES is validated theoretically and empirically in\nboth principled 1D setup and realistic 3D scenes.\n  It is shown to represent signals with sharp edges more accurately, which are\ntypically challenging for Gaussians due to their inherent low-pass\ncharacteristics. Our empirical analysis demonstrates that GEF outperforms\nGaussians in fitting natural-occurring signals (e.g. squares, triangles, and\nparabolic signals), thereby reducing the need for extensive splitting\noperations that increase the memory footprint of Gaussian Splatting. With the\naid of a frequency-modulated loss, GES achieves competitive performance in\nnovel-view synthesis benchmarks while requiring less than half the memory\nstorage of Gaussian Splatting and increasing the rendering speed by up to 39%.\nThe code is available on the project website https://abdullahamdi.com/ges .\n", "link": "http://arxiv.org/abs/2402.10128v2", "date": "2024-05-24", "relevancy": 2.9962, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6791}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.608}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GES%3A%20Generalized%20Exponential%20Splatting%20for%20Efficient%20Radiance%20Field%0A%20%20Rendering&body=Title%3A%20GES%3A%20Generalized%20Exponential%20Splatting%20for%20Efficient%20Radiance%20Field%0A%20%20Rendering%0AAuthor%3A%20Abdullah%20Hamdi%20and%20Luke%20Melas-Kyriazi%20and%20Jinjie%20Mai%20and%20Guocheng%20Qian%20and%20Ruoshi%20Liu%20and%20Carl%20Vondrick%20and%20Bernard%20Ghanem%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20Advancements%20in%203D%20Gaussian%20Splatting%20have%20significantly%20accelerated%203D%0Areconstruction%20and%20generation.%20However%2C%20it%20may%20require%20a%20large%20number%20of%0AGaussians%2C%20which%20creates%20a%20substantial%20memory%20footprint.%20This%20paper%20introduces%0AGES%20%28Generalized%20Exponential%20Splatting%29%2C%20a%20novel%20representation%20that%20employs%0AGeneralized%20Exponential%20Function%20%28GEF%29%20to%20model%203D%20scenes%2C%20requiring%20far%20fewer%0Aparticles%20to%20represent%20a%20scene%20and%20thus%20significantly%20outperforming%20Gaussian%0ASplatting%20methods%20in%20efficiency%20with%20a%20plug-and-play%20replacement%20ability%20for%0AGaussian-based%20utilities.%20GES%20is%20validated%20theoretically%20and%20empirically%20in%0Aboth%20principled%201D%20setup%20and%20realistic%203D%20scenes.%0A%20%20It%20is%20shown%20to%20represent%20signals%20with%20sharp%20edges%20more%20accurately%2C%20which%20are%0Atypically%20challenging%20for%20Gaussians%20due%20to%20their%20inherent%20low-pass%0Acharacteristics.%20Our%20empirical%20analysis%20demonstrates%20that%20GEF%20outperforms%0AGaussians%20in%20fitting%20natural-occurring%20signals%20%28e.g.%20squares%2C%20triangles%2C%20and%0Aparabolic%20signals%29%2C%20thereby%20reducing%20the%20need%20for%20extensive%20splitting%0Aoperations%20that%20increase%20the%20memory%20footprint%20of%20Gaussian%20Splatting.%20With%20the%0Aaid%20of%20a%20frequency-modulated%20loss%2C%20GES%20achieves%20competitive%20performance%20in%0Anovel-view%20synthesis%20benchmarks%20while%20requiring%20less%20than%20half%20the%20memory%0Astorage%20of%20Gaussian%20Splatting%20and%20increasing%20the%20rendering%20speed%20by%20up%20to%2039%25.%0AThe%20code%20is%20available%20on%20the%20project%20website%20https%3A//abdullahamdi.com/ges%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10128v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGES%253A%2520Generalized%2520Exponential%2520Splatting%2520for%2520Efficient%2520Radiance%2520Field%250A%2520%2520Rendering%26entry.906535625%3DAbdullah%2520Hamdi%2520and%2520Luke%2520Melas-Kyriazi%2520and%2520Jinjie%2520Mai%2520and%2520Guocheng%2520Qian%2520and%2520Ruoshi%2520Liu%2520and%2520Carl%2520Vondrick%2520and%2520Bernard%2520Ghanem%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520Advancements%2520in%25203D%2520Gaussian%2520Splatting%2520have%2520significantly%2520accelerated%25203D%250Areconstruction%2520and%2520generation.%2520However%252C%2520it%2520may%2520require%2520a%2520large%2520number%2520of%250AGaussians%252C%2520which%2520creates%2520a%2520substantial%2520memory%2520footprint.%2520This%2520paper%2520introduces%250AGES%2520%2528Generalized%2520Exponential%2520Splatting%2529%252C%2520a%2520novel%2520representation%2520that%2520employs%250AGeneralized%2520Exponential%2520Function%2520%2528GEF%2529%2520to%2520model%25203D%2520scenes%252C%2520requiring%2520far%2520fewer%250Aparticles%2520to%2520represent%2520a%2520scene%2520and%2520thus%2520significantly%2520outperforming%2520Gaussian%250ASplatting%2520methods%2520in%2520efficiency%2520with%2520a%2520plug-and-play%2520replacement%2520ability%2520for%250AGaussian-based%2520utilities.%2520GES%2520is%2520validated%2520theoretically%2520and%2520empirically%2520in%250Aboth%2520principled%25201D%2520setup%2520and%2520realistic%25203D%2520scenes.%250A%2520%2520It%2520is%2520shown%2520to%2520represent%2520signals%2520with%2520sharp%2520edges%2520more%2520accurately%252C%2520which%2520are%250Atypically%2520challenging%2520for%2520Gaussians%2520due%2520to%2520their%2520inherent%2520low-pass%250Acharacteristics.%2520Our%2520empirical%2520analysis%2520demonstrates%2520that%2520GEF%2520outperforms%250AGaussians%2520in%2520fitting%2520natural-occurring%2520signals%2520%2528e.g.%2520squares%252C%2520triangles%252C%2520and%250Aparabolic%2520signals%2529%252C%2520thereby%2520reducing%2520the%2520need%2520for%2520extensive%2520splitting%250Aoperations%2520that%2520increase%2520the%2520memory%2520footprint%2520of%2520Gaussian%2520Splatting.%2520With%2520the%250Aaid%2520of%2520a%2520frequency-modulated%2520loss%252C%2520GES%2520achieves%2520competitive%2520performance%2520in%250Anovel-view%2520synthesis%2520benchmarks%2520while%2520requiring%2520less%2520than%2520half%2520the%2520memory%250Astorage%2520of%2520Gaussian%2520Splatting%2520and%2520increasing%2520the%2520rendering%2520speed%2520by%2520up%2520to%252039%2525.%250AThe%2520code%2520is%2520available%2520on%2520the%2520project%2520website%2520https%253A//abdullahamdi.com/ges%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10128v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GES%3A%20Generalized%20Exponential%20Splatting%20for%20Efficient%20Radiance%20Field%0A%20%20Rendering&entry.906535625=Abdullah%20Hamdi%20and%20Luke%20Melas-Kyriazi%20and%20Jinjie%20Mai%20and%20Guocheng%20Qian%20and%20Ruoshi%20Liu%20and%20Carl%20Vondrick%20and%20Bernard%20Ghanem%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20Advancements%20in%203D%20Gaussian%20Splatting%20have%20significantly%20accelerated%203D%0Areconstruction%20and%20generation.%20However%2C%20it%20may%20require%20a%20large%20number%20of%0AGaussians%2C%20which%20creates%20a%20substantial%20memory%20footprint.%20This%20paper%20introduces%0AGES%20%28Generalized%20Exponential%20Splatting%29%2C%20a%20novel%20representation%20that%20employs%0AGeneralized%20Exponential%20Function%20%28GEF%29%20to%20model%203D%20scenes%2C%20requiring%20far%20fewer%0Aparticles%20to%20represent%20a%20scene%20and%20thus%20significantly%20outperforming%20Gaussian%0ASplatting%20methods%20in%20efficiency%20with%20a%20plug-and-play%20replacement%20ability%20for%0AGaussian-based%20utilities.%20GES%20is%20validated%20theoretically%20and%20empirically%20in%0Aboth%20principled%201D%20setup%20and%20realistic%203D%20scenes.%0A%20%20It%20is%20shown%20to%20represent%20signals%20with%20sharp%20edges%20more%20accurately%2C%20which%20are%0Atypically%20challenging%20for%20Gaussians%20due%20to%20their%20inherent%20low-pass%0Acharacteristics.%20Our%20empirical%20analysis%20demonstrates%20that%20GEF%20outperforms%0AGaussians%20in%20fitting%20natural-occurring%20signals%20%28e.g.%20squares%2C%20triangles%2C%20and%0Aparabolic%20signals%29%2C%20thereby%20reducing%20the%20need%20for%20extensive%20splitting%0Aoperations%20that%20increase%20the%20memory%20footprint%20of%20Gaussian%20Splatting.%20With%20the%0Aaid%20of%20a%20frequency-modulated%20loss%2C%20GES%20achieves%20competitive%20performance%20in%0Anovel-view%20synthesis%20benchmarks%20while%20requiring%20less%20than%20half%20the%20memory%0Astorage%20of%20Gaussian%20Splatting%20and%20increasing%20the%20rendering%20speed%20by%20up%20to%2039%25.%0AThe%20code%20is%20available%20on%20the%20project%20website%20https%3A//abdullahamdi.com/ges%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10128v2&entry.124074799=Read"},
{"title": "GSDeformer: Direct Cage-based Deformation for 3D Gaussian Splatting", "author": "Jiajun Huang and Hongchuan Yu", "abstract": "  We present GSDeformer, a method that achieves free-form deformation on 3D\nGaussian Splatting(3DGS) without requiring any architectural changes. Our\nmethod extends cage-based deformation, a traditional mesh deformation method,\nto 3DGS. This is done by converting 3DGS into a novel proxy point cloud\nrepresentation, where its deformation can be used to infer the transformations\nto apply on the 3D gaussians making up 3DGS. We also propose an automatic cage\nconstruction algorithm for 3DGS to minimize manual work. Our method does not\nmodify the underlying architecture of 3DGS. Therefore, any existing trained\nvanilla 3DGS can be easily edited by our method. We compare the deformation\ncapability of our method against other existing methods, demonstrating the ease\nof use and comparable quality of our method, despite being more direct and thus\neasier to integrate with other concurrent developments on 3DGS.\n", "link": "http://arxiv.org/abs/2405.15491v1", "date": "2024-05-24", "relevancy": 2.9815, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6317}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6188}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSDeformer%3A%20Direct%20Cage-based%20Deformation%20for%203D%20Gaussian%20Splatting&body=Title%3A%20GSDeformer%3A%20Direct%20Cage-based%20Deformation%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Jiajun%20Huang%20and%20Hongchuan%20Yu%0AAbstract%3A%20%20%20We%20present%20GSDeformer%2C%20a%20method%20that%20achieves%20free-form%20deformation%20on%203D%0AGaussian%20Splatting%283DGS%29%20without%20requiring%20any%20architectural%20changes.%20Our%0Amethod%20extends%20cage-based%20deformation%2C%20a%20traditional%20mesh%20deformation%20method%2C%0Ato%203DGS.%20This%20is%20done%20by%20converting%203DGS%20into%20a%20novel%20proxy%20point%20cloud%0Arepresentation%2C%20where%20its%20deformation%20can%20be%20used%20to%20infer%20the%20transformations%0Ato%20apply%20on%20the%203D%20gaussians%20making%20up%203DGS.%20We%20also%20propose%20an%20automatic%20cage%0Aconstruction%20algorithm%20for%203DGS%20to%20minimize%20manual%20work.%20Our%20method%20does%20not%0Amodify%20the%20underlying%20architecture%20of%203DGS.%20Therefore%2C%20any%20existing%20trained%0Avanilla%203DGS%20can%20be%20easily%20edited%20by%20our%20method.%20We%20compare%20the%20deformation%0Acapability%20of%20our%20method%20against%20other%20existing%20methods%2C%20demonstrating%20the%20ease%0Aof%20use%20and%20comparable%20quality%20of%20our%20method%2C%20despite%20being%20more%20direct%20and%20thus%0Aeasier%20to%20integrate%20with%20other%20concurrent%20developments%20on%203DGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSDeformer%253A%2520Direct%2520Cage-based%2520Deformation%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DJiajun%2520Huang%2520and%2520Hongchuan%2520Yu%26entry.1292438233%3D%2520%2520We%2520present%2520GSDeformer%252C%2520a%2520method%2520that%2520achieves%2520free-form%2520deformation%2520on%25203D%250AGaussian%2520Splatting%25283DGS%2529%2520without%2520requiring%2520any%2520architectural%2520changes.%2520Our%250Amethod%2520extends%2520cage-based%2520deformation%252C%2520a%2520traditional%2520mesh%2520deformation%2520method%252C%250Ato%25203DGS.%2520This%2520is%2520done%2520by%2520converting%25203DGS%2520into%2520a%2520novel%2520proxy%2520point%2520cloud%250Arepresentation%252C%2520where%2520its%2520deformation%2520can%2520be%2520used%2520to%2520infer%2520the%2520transformations%250Ato%2520apply%2520on%2520the%25203D%2520gaussians%2520making%2520up%25203DGS.%2520We%2520also%2520propose%2520an%2520automatic%2520cage%250Aconstruction%2520algorithm%2520for%25203DGS%2520to%2520minimize%2520manual%2520work.%2520Our%2520method%2520does%2520not%250Amodify%2520the%2520underlying%2520architecture%2520of%25203DGS.%2520Therefore%252C%2520any%2520existing%2520trained%250Avanilla%25203DGS%2520can%2520be%2520easily%2520edited%2520by%2520our%2520method.%2520We%2520compare%2520the%2520deformation%250Acapability%2520of%2520our%2520method%2520against%2520other%2520existing%2520methods%252C%2520demonstrating%2520the%2520ease%250Aof%2520use%2520and%2520comparable%2520quality%2520of%2520our%2520method%252C%2520despite%2520being%2520more%2520direct%2520and%2520thus%250Aeasier%2520to%2520integrate%2520with%2520other%2520concurrent%2520developments%2520on%25203DGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSDeformer%3A%20Direct%20Cage-based%20Deformation%20for%203D%20Gaussian%20Splatting&entry.906535625=Jiajun%20Huang%20and%20Hongchuan%20Yu&entry.1292438233=%20%20We%20present%20GSDeformer%2C%20a%20method%20that%20achieves%20free-form%20deformation%20on%203D%0AGaussian%20Splatting%283DGS%29%20without%20requiring%20any%20architectural%20changes.%20Our%0Amethod%20extends%20cage-based%20deformation%2C%20a%20traditional%20mesh%20deformation%20method%2C%0Ato%203DGS.%20This%20is%20done%20by%20converting%203DGS%20into%20a%20novel%20proxy%20point%20cloud%0Arepresentation%2C%20where%20its%20deformation%20can%20be%20used%20to%20infer%20the%20transformations%0Ato%20apply%20on%20the%203D%20gaussians%20making%20up%203DGS.%20We%20also%20propose%20an%20automatic%20cage%0Aconstruction%20algorithm%20for%203DGS%20to%20minimize%20manual%20work.%20Our%20method%20does%20not%0Amodify%20the%20underlying%20architecture%20of%203DGS.%20Therefore%2C%20any%20existing%20trained%0Avanilla%203DGS%20can%20be%20easily%20edited%20by%20our%20method.%20We%20compare%20the%20deformation%0Acapability%20of%20our%20method%20against%20other%20existing%20methods%2C%20demonstrating%20the%20ease%0Aof%20use%20and%20comparable%20quality%20of%20our%20method%2C%20despite%20being%20more%20direct%20and%20thus%0Aeasier%20to%20integrate%20with%20other%20concurrent%20developments%20on%203DGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15491v1&entry.124074799=Read"},
{"title": "GLiDR: Topologically Regularized Graph Generative Network for Sparse\n  LiDAR Point Clouds", "author": "Prashant Kumar and Kshitij Madhav Bhat and Vedang Bhupesh Shenvi Nadkarni and Prem Kalra", "abstract": "  Sparse LiDAR point clouds cause severe loss of detail of static structures\nand reduce the density of static points available for navigation. Reduced\ndensity can be detrimental to navigation under several scenarios. We observe\nthat despite high sparsity, in most cases, the global topology of LiDAR\noutlining the static structures can be inferred. We utilize this property to\nobtain a backbone skeleton of a LiDAR scan in the form of a single connected\ncomponent that is a proxy to its global topology. We utilize the backbone to\naugment new points along static structures to overcome sparsity. Newly\nintroduced points could correspond to existing static structures or to static\npoints that were earlier obstructed by dynamic objects. To the best of our\nknowledge, we are the first to use such a strategy for sparse LiDAR point\nclouds. Existing solutions close to our approach fail to identify and preserve\nthe global static LiDAR topology and generate sub-optimal points. We propose\nGLiDR, a Graph Generative network that is topologically regularized using\n0-dimensional Persistent Homology ($\\mathcal{PH}$) constraints. This enables\nGLiDR to introduce newer static points along a topologically consistent global\nstatic LiDAR backbone. GLiDR generates precise static points using $32\\times$\nsparser dynamic scans and performs better than the baselines across three\ndatasets. GLiDR generates a valuable byproduct - an accurate binary\nsegmentation mask of static and dynamic objects that are helpful for navigation\nplanning and safety in constrained environments. The newly introduced static\npoints allow GLiDR to outperform LiDAR-based navigation using SLAM in several\nsettings. Source code is available at https://kshitijbhat.github.io/glidr\n", "link": "http://arxiv.org/abs/2312.00068v3", "date": "2024-05-24", "relevancy": 2.9154, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.604}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5848}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLiDR%3A%20Topologically%20Regularized%20Graph%20Generative%20Network%20for%20Sparse%0A%20%20LiDAR%20Point%20Clouds&body=Title%3A%20GLiDR%3A%20Topologically%20Regularized%20Graph%20Generative%20Network%20for%20Sparse%0A%20%20LiDAR%20Point%20Clouds%0AAuthor%3A%20Prashant%20Kumar%20and%20Kshitij%20Madhav%20Bhat%20and%20Vedang%20Bhupesh%20Shenvi%20Nadkarni%20and%20Prem%20Kalra%0AAbstract%3A%20%20%20Sparse%20LiDAR%20point%20clouds%20cause%20severe%20loss%20of%20detail%20of%20static%20structures%0Aand%20reduce%20the%20density%20of%20static%20points%20available%20for%20navigation.%20Reduced%0Adensity%20can%20be%20detrimental%20to%20navigation%20under%20several%20scenarios.%20We%20observe%0Athat%20despite%20high%20sparsity%2C%20in%20most%20cases%2C%20the%20global%20topology%20of%20LiDAR%0Aoutlining%20the%20static%20structures%20can%20be%20inferred.%20We%20utilize%20this%20property%20to%0Aobtain%20a%20backbone%20skeleton%20of%20a%20LiDAR%20scan%20in%20the%20form%20of%20a%20single%20connected%0Acomponent%20that%20is%20a%20proxy%20to%20its%20global%20topology.%20We%20utilize%20the%20backbone%20to%0Aaugment%20new%20points%20along%20static%20structures%20to%20overcome%20sparsity.%20Newly%0Aintroduced%20points%20could%20correspond%20to%20existing%20static%20structures%20or%20to%20static%0Apoints%20that%20were%20earlier%20obstructed%20by%20dynamic%20objects.%20To%20the%20best%20of%20our%0Aknowledge%2C%20we%20are%20the%20first%20to%20use%20such%20a%20strategy%20for%20sparse%20LiDAR%20point%0Aclouds.%20Existing%20solutions%20close%20to%20our%20approach%20fail%20to%20identify%20and%20preserve%0Athe%20global%20static%20LiDAR%20topology%20and%20generate%20sub-optimal%20points.%20We%20propose%0AGLiDR%2C%20a%20Graph%20Generative%20network%20that%20is%20topologically%20regularized%20using%0A0-dimensional%20Persistent%20Homology%20%28%24%5Cmathcal%7BPH%7D%24%29%20constraints.%20This%20enables%0AGLiDR%20to%20introduce%20newer%20static%20points%20along%20a%20topologically%20consistent%20global%0Astatic%20LiDAR%20backbone.%20GLiDR%20generates%20precise%20static%20points%20using%20%2432%5Ctimes%24%0Asparser%20dynamic%20scans%20and%20performs%20better%20than%20the%20baselines%20across%20three%0Adatasets.%20GLiDR%20generates%20a%20valuable%20byproduct%20-%20an%20accurate%20binary%0Asegmentation%20mask%20of%20static%20and%20dynamic%20objects%20that%20are%20helpful%20for%20navigation%0Aplanning%20and%20safety%20in%20constrained%20environments.%20The%20newly%20introduced%20static%0Apoints%20allow%20GLiDR%20to%20outperform%20LiDAR-based%20navigation%20using%20SLAM%20in%20several%0Asettings.%20Source%20code%20is%20available%20at%20https%3A//kshitijbhat.github.io/glidr%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00068v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLiDR%253A%2520Topologically%2520Regularized%2520Graph%2520Generative%2520Network%2520for%2520Sparse%250A%2520%2520LiDAR%2520Point%2520Clouds%26entry.906535625%3DPrashant%2520Kumar%2520and%2520Kshitij%2520Madhav%2520Bhat%2520and%2520Vedang%2520Bhupesh%2520Shenvi%2520Nadkarni%2520and%2520Prem%2520Kalra%26entry.1292438233%3D%2520%2520Sparse%2520LiDAR%2520point%2520clouds%2520cause%2520severe%2520loss%2520of%2520detail%2520of%2520static%2520structures%250Aand%2520reduce%2520the%2520density%2520of%2520static%2520points%2520available%2520for%2520navigation.%2520Reduced%250Adensity%2520can%2520be%2520detrimental%2520to%2520navigation%2520under%2520several%2520scenarios.%2520We%2520observe%250Athat%2520despite%2520high%2520sparsity%252C%2520in%2520most%2520cases%252C%2520the%2520global%2520topology%2520of%2520LiDAR%250Aoutlining%2520the%2520static%2520structures%2520can%2520be%2520inferred.%2520We%2520utilize%2520this%2520property%2520to%250Aobtain%2520a%2520backbone%2520skeleton%2520of%2520a%2520LiDAR%2520scan%2520in%2520the%2520form%2520of%2520a%2520single%2520connected%250Acomponent%2520that%2520is%2520a%2520proxy%2520to%2520its%2520global%2520topology.%2520We%2520utilize%2520the%2520backbone%2520to%250Aaugment%2520new%2520points%2520along%2520static%2520structures%2520to%2520overcome%2520sparsity.%2520Newly%250Aintroduced%2520points%2520could%2520correspond%2520to%2520existing%2520static%2520structures%2520or%2520to%2520static%250Apoints%2520that%2520were%2520earlier%2520obstructed%2520by%2520dynamic%2520objects.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520we%2520are%2520the%2520first%2520to%2520use%2520such%2520a%2520strategy%2520for%2520sparse%2520LiDAR%2520point%250Aclouds.%2520Existing%2520solutions%2520close%2520to%2520our%2520approach%2520fail%2520to%2520identify%2520and%2520preserve%250Athe%2520global%2520static%2520LiDAR%2520topology%2520and%2520generate%2520sub-optimal%2520points.%2520We%2520propose%250AGLiDR%252C%2520a%2520Graph%2520Generative%2520network%2520that%2520is%2520topologically%2520regularized%2520using%250A0-dimensional%2520Persistent%2520Homology%2520%2528%2524%255Cmathcal%257BPH%257D%2524%2529%2520constraints.%2520This%2520enables%250AGLiDR%2520to%2520introduce%2520newer%2520static%2520points%2520along%2520a%2520topologically%2520consistent%2520global%250Astatic%2520LiDAR%2520backbone.%2520GLiDR%2520generates%2520precise%2520static%2520points%2520using%2520%252432%255Ctimes%2524%250Asparser%2520dynamic%2520scans%2520and%2520performs%2520better%2520than%2520the%2520baselines%2520across%2520three%250Adatasets.%2520GLiDR%2520generates%2520a%2520valuable%2520byproduct%2520-%2520an%2520accurate%2520binary%250Asegmentation%2520mask%2520of%2520static%2520and%2520dynamic%2520objects%2520that%2520are%2520helpful%2520for%2520navigation%250Aplanning%2520and%2520safety%2520in%2520constrained%2520environments.%2520The%2520newly%2520introduced%2520static%250Apoints%2520allow%2520GLiDR%2520to%2520outperform%2520LiDAR-based%2520navigation%2520using%2520SLAM%2520in%2520several%250Asettings.%2520Source%2520code%2520is%2520available%2520at%2520https%253A//kshitijbhat.github.io/glidr%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00068v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLiDR%3A%20Topologically%20Regularized%20Graph%20Generative%20Network%20for%20Sparse%0A%20%20LiDAR%20Point%20Clouds&entry.906535625=Prashant%20Kumar%20and%20Kshitij%20Madhav%20Bhat%20and%20Vedang%20Bhupesh%20Shenvi%20Nadkarni%20and%20Prem%20Kalra&entry.1292438233=%20%20Sparse%20LiDAR%20point%20clouds%20cause%20severe%20loss%20of%20detail%20of%20static%20structures%0Aand%20reduce%20the%20density%20of%20static%20points%20available%20for%20navigation.%20Reduced%0Adensity%20can%20be%20detrimental%20to%20navigation%20under%20several%20scenarios.%20We%20observe%0Athat%20despite%20high%20sparsity%2C%20in%20most%20cases%2C%20the%20global%20topology%20of%20LiDAR%0Aoutlining%20the%20static%20structures%20can%20be%20inferred.%20We%20utilize%20this%20property%20to%0Aobtain%20a%20backbone%20skeleton%20of%20a%20LiDAR%20scan%20in%20the%20form%20of%20a%20single%20connected%0Acomponent%20that%20is%20a%20proxy%20to%20its%20global%20topology.%20We%20utilize%20the%20backbone%20to%0Aaugment%20new%20points%20along%20static%20structures%20to%20overcome%20sparsity.%20Newly%0Aintroduced%20points%20could%20correspond%20to%20existing%20static%20structures%20or%20to%20static%0Apoints%20that%20were%20earlier%20obstructed%20by%20dynamic%20objects.%20To%20the%20best%20of%20our%0Aknowledge%2C%20we%20are%20the%20first%20to%20use%20such%20a%20strategy%20for%20sparse%20LiDAR%20point%0Aclouds.%20Existing%20solutions%20close%20to%20our%20approach%20fail%20to%20identify%20and%20preserve%0Athe%20global%20static%20LiDAR%20topology%20and%20generate%20sub-optimal%20points.%20We%20propose%0AGLiDR%2C%20a%20Graph%20Generative%20network%20that%20is%20topologically%20regularized%20using%0A0-dimensional%20Persistent%20Homology%20%28%24%5Cmathcal%7BPH%7D%24%29%20constraints.%20This%20enables%0AGLiDR%20to%20introduce%20newer%20static%20points%20along%20a%20topologically%20consistent%20global%0Astatic%20LiDAR%20backbone.%20GLiDR%20generates%20precise%20static%20points%20using%20%2432%5Ctimes%24%0Asparser%20dynamic%20scans%20and%20performs%20better%20than%20the%20baselines%20across%20three%0Adatasets.%20GLiDR%20generates%20a%20valuable%20byproduct%20-%20an%20accurate%20binary%0Asegmentation%20mask%20of%20static%20and%20dynamic%20objects%20that%20are%20helpful%20for%20navigation%0Aplanning%20and%20safety%20in%20constrained%20environments.%20The%20newly%20introduced%20static%0Apoints%20allow%20GLiDR%20to%20outperform%20LiDAR-based%20navigation%20using%20SLAM%20in%20several%0Asettings.%20Source%20code%20is%20available%20at%20https%3A//kshitijbhat.github.io/glidr%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00068v3&entry.124074799=Read"},
{"title": "ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal\n  Models", "author": "Chunjiang Ge and Sijie Cheng and Ziming Wang and Jiale Yuan and Yuan Gao and Jun Song and Shiji Song and Gao Huang and Bo Zheng", "abstract": "  High-resolution Large Multimodal Models (LMMs) encounter the challenges of\nexcessive visual tokens and quadratic visual complexity. Current\nhigh-resolution LMMs address the quadratic complexity while still generating\nexcessive visual tokens. However, the redundancy in visual tokens is the key\nproblem as it leads to more substantial compute. To mitigate this issue, we\npropose ConvLLaVA, which employs ConvNeXt, a hierarchical backbone, as the\nvisual encoder of LMM to replace Vision Transformer (ViT). ConvLLaVA compresses\nhigh-resolution images into information-rich visual features, effectively\npreventing the generation of excessive visual tokens. To enhance the\ncapabilities of ConvLLaVA, we propose two critical optimizations. Since the\nlow-resolution pretrained ConvNeXt underperforms when directly applied on high\nresolution, we update it to bridge the gap. Moreover, since ConvNeXt's original\ncompression ratio is inadequate for much higher resolution inputs, we train a\nsuccessive stage to further compress the visual tokens, thereby reducing\nredundancy. These optimizations enable ConvLLaVA to support inputs of 1536x1536\nresolution generating only 576 visual tokens, capable of handling images of\narbitrary aspect ratios. Experimental results demonstrate that our method\nachieves competitive performance with state-of-the-art models on mainstream\nbenchmarks. The ConvLLaVA model series are publicly available at\nhttps://github.com/alibaba/conv-llava.\n", "link": "http://arxiv.org/abs/2405.15738v1", "date": "2024-05-24", "relevancy": 2.8573, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5754}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConvLLaVA%3A%20Hierarchical%20Backbones%20as%20Visual%20Encoder%20for%20Large%20Multimodal%0A%20%20Models&body=Title%3A%20ConvLLaVA%3A%20Hierarchical%20Backbones%20as%20Visual%20Encoder%20for%20Large%20Multimodal%0A%20%20Models%0AAuthor%3A%20Chunjiang%20Ge%20and%20Sijie%20Cheng%20and%20Ziming%20Wang%20and%20Jiale%20Yuan%20and%20Yuan%20Gao%20and%20Jun%20Song%20and%20Shiji%20Song%20and%20Gao%20Huang%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20High-resolution%20Large%20Multimodal%20Models%20%28LMMs%29%20encounter%20the%20challenges%20of%0Aexcessive%20visual%20tokens%20and%20quadratic%20visual%20complexity.%20Current%0Ahigh-resolution%20LMMs%20address%20the%20quadratic%20complexity%20while%20still%20generating%0Aexcessive%20visual%20tokens.%20However%2C%20the%20redundancy%20in%20visual%20tokens%20is%20the%20key%0Aproblem%20as%20it%20leads%20to%20more%20substantial%20compute.%20To%20mitigate%20this%20issue%2C%20we%0Apropose%20ConvLLaVA%2C%20which%20employs%20ConvNeXt%2C%20a%20hierarchical%20backbone%2C%20as%20the%0Avisual%20encoder%20of%20LMM%20to%20replace%20Vision%20Transformer%20%28ViT%29.%20ConvLLaVA%20compresses%0Ahigh-resolution%20images%20into%20information-rich%20visual%20features%2C%20effectively%0Apreventing%20the%20generation%20of%20excessive%20visual%20tokens.%20To%20enhance%20the%0Acapabilities%20of%20ConvLLaVA%2C%20we%20propose%20two%20critical%20optimizations.%20Since%20the%0Alow-resolution%20pretrained%20ConvNeXt%20underperforms%20when%20directly%20applied%20on%20high%0Aresolution%2C%20we%20update%20it%20to%20bridge%20the%20gap.%20Moreover%2C%20since%20ConvNeXt%27s%20original%0Acompression%20ratio%20is%20inadequate%20for%20much%20higher%20resolution%20inputs%2C%20we%20train%20a%0Asuccessive%20stage%20to%20further%20compress%20the%20visual%20tokens%2C%20thereby%20reducing%0Aredundancy.%20These%20optimizations%20enable%20ConvLLaVA%20to%20support%20inputs%20of%201536x1536%0Aresolution%20generating%20only%20576%20visual%20tokens%2C%20capable%20of%20handling%20images%20of%0Aarbitrary%20aspect%20ratios.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aachieves%20competitive%20performance%20with%20state-of-the-art%20models%20on%20mainstream%0Abenchmarks.%20The%20ConvLLaVA%20model%20series%20are%20publicly%20available%20at%0Ahttps%3A//github.com/alibaba/conv-llava.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvLLaVA%253A%2520Hierarchical%2520Backbones%2520as%2520Visual%2520Encoder%2520for%2520Large%2520Multimodal%250A%2520%2520Models%26entry.906535625%3DChunjiang%2520Ge%2520and%2520Sijie%2520Cheng%2520and%2520Ziming%2520Wang%2520and%2520Jiale%2520Yuan%2520and%2520Yuan%2520Gao%2520and%2520Jun%2520Song%2520and%2520Shiji%2520Song%2520and%2520Gao%2520Huang%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520High-resolution%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520encounter%2520the%2520challenges%2520of%250Aexcessive%2520visual%2520tokens%2520and%2520quadratic%2520visual%2520complexity.%2520Current%250Ahigh-resolution%2520LMMs%2520address%2520the%2520quadratic%2520complexity%2520while%2520still%2520generating%250Aexcessive%2520visual%2520tokens.%2520However%252C%2520the%2520redundancy%2520in%2520visual%2520tokens%2520is%2520the%2520key%250Aproblem%2520as%2520it%2520leads%2520to%2520more%2520substantial%2520compute.%2520To%2520mitigate%2520this%2520issue%252C%2520we%250Apropose%2520ConvLLaVA%252C%2520which%2520employs%2520ConvNeXt%252C%2520a%2520hierarchical%2520backbone%252C%2520as%2520the%250Avisual%2520encoder%2520of%2520LMM%2520to%2520replace%2520Vision%2520Transformer%2520%2528ViT%2529.%2520ConvLLaVA%2520compresses%250Ahigh-resolution%2520images%2520into%2520information-rich%2520visual%2520features%252C%2520effectively%250Apreventing%2520the%2520generation%2520of%2520excessive%2520visual%2520tokens.%2520To%2520enhance%2520the%250Acapabilities%2520of%2520ConvLLaVA%252C%2520we%2520propose%2520two%2520critical%2520optimizations.%2520Since%2520the%250Alow-resolution%2520pretrained%2520ConvNeXt%2520underperforms%2520when%2520directly%2520applied%2520on%2520high%250Aresolution%252C%2520we%2520update%2520it%2520to%2520bridge%2520the%2520gap.%2520Moreover%252C%2520since%2520ConvNeXt%2527s%2520original%250Acompression%2520ratio%2520is%2520inadequate%2520for%2520much%2520higher%2520resolution%2520inputs%252C%2520we%2520train%2520a%250Asuccessive%2520stage%2520to%2520further%2520compress%2520the%2520visual%2520tokens%252C%2520thereby%2520reducing%250Aredundancy.%2520These%2520optimizations%2520enable%2520ConvLLaVA%2520to%2520support%2520inputs%2520of%25201536x1536%250Aresolution%2520generating%2520only%2520576%2520visual%2520tokens%252C%2520capable%2520of%2520handling%2520images%2520of%250Aarbitrary%2520aspect%2520ratios.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520competitive%2520performance%2520with%2520state-of-the-art%2520models%2520on%2520mainstream%250Abenchmarks.%2520The%2520ConvLLaVA%2520model%2520series%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/alibaba/conv-llava.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConvLLaVA%3A%20Hierarchical%20Backbones%20as%20Visual%20Encoder%20for%20Large%20Multimodal%0A%20%20Models&entry.906535625=Chunjiang%20Ge%20and%20Sijie%20Cheng%20and%20Ziming%20Wang%20and%20Jiale%20Yuan%20and%20Yuan%20Gao%20and%20Jun%20Song%20and%20Shiji%20Song%20and%20Gao%20Huang%20and%20Bo%20Zheng&entry.1292438233=%20%20High-resolution%20Large%20Multimodal%20Models%20%28LMMs%29%20encounter%20the%20challenges%20of%0Aexcessive%20visual%20tokens%20and%20quadratic%20visual%20complexity.%20Current%0Ahigh-resolution%20LMMs%20address%20the%20quadratic%20complexity%20while%20still%20generating%0Aexcessive%20visual%20tokens.%20However%2C%20the%20redundancy%20in%20visual%20tokens%20is%20the%20key%0Aproblem%20as%20it%20leads%20to%20more%20substantial%20compute.%20To%20mitigate%20this%20issue%2C%20we%0Apropose%20ConvLLaVA%2C%20which%20employs%20ConvNeXt%2C%20a%20hierarchical%20backbone%2C%20as%20the%0Avisual%20encoder%20of%20LMM%20to%20replace%20Vision%20Transformer%20%28ViT%29.%20ConvLLaVA%20compresses%0Ahigh-resolution%20images%20into%20information-rich%20visual%20features%2C%20effectively%0Apreventing%20the%20generation%20of%20excessive%20visual%20tokens.%20To%20enhance%20the%0Acapabilities%20of%20ConvLLaVA%2C%20we%20propose%20two%20critical%20optimizations.%20Since%20the%0Alow-resolution%20pretrained%20ConvNeXt%20underperforms%20when%20directly%20applied%20on%20high%0Aresolution%2C%20we%20update%20it%20to%20bridge%20the%20gap.%20Moreover%2C%20since%20ConvNeXt%27s%20original%0Acompression%20ratio%20is%20inadequate%20for%20much%20higher%20resolution%20inputs%2C%20we%20train%20a%0Asuccessive%20stage%20to%20further%20compress%20the%20visual%20tokens%2C%20thereby%20reducing%0Aredundancy.%20These%20optimizations%20enable%20ConvLLaVA%20to%20support%20inputs%20of%201536x1536%0Aresolution%20generating%20only%20576%20visual%20tokens%2C%20capable%20of%20handling%20images%20of%0Aarbitrary%20aspect%20ratios.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aachieves%20competitive%20performance%20with%20state-of-the-art%20models%20on%20mainstream%0Abenchmarks.%20The%20ConvLLaVA%20model%20series%20are%20publicly%20available%20at%0Ahttps%3A//github.com/alibaba/conv-llava.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15738v1&entry.124074799=Read"},
{"title": "Less is more: Summarizing Patch Tokens for efficient Multi-Label\n  Class-Incremental Learning", "author": "Thomas De Min and Massimiliano Mancini and St\u00e9phane Lathuili\u00e8re and Subhankar Roy and Elisa Ricci", "abstract": "  Prompt tuning has emerged as an effective rehearsal-free technique for\nclass-incremental learning (CIL) that learns a tiny set of task-specific\nparameters (or prompts) to instruct a pre-trained transformer to learn on a\nsequence of tasks. Albeit effective, prompt tuning methods do not lend well in\nthe multi-label class incremental learning (MLCIL) scenario (where an image\ncontains multiple foreground classes) due to the ambiguity in selecting the\ncorrect prompt(s) corresponding to different foreground objects belonging to\nmultiple tasks. To circumvent this issue we propose to eliminate the prompt\nselection mechanism by maintaining task-specific pathways, which allow us to\nlearn representations that do not interact with the ones from the other tasks.\nSince independent pathways in truly incremental scenarios will result in an\nexplosion of computation due to the quadratically complex multi-head\nself-attention (MSA) operation in prompt tuning, we propose to reduce the\noriginal patch token embeddings into summarized tokens. Prompt tuning is then\napplied to these fewer summarized tokens to compute the final representation.\nOur proposed method Multi-Label class incremental learning via summarising\npAtch tokeN Embeddings (MULTI-LANE) enables learning disentangled task-specific\nrepresentations in MLCIL while ensuring fast inference. We conduct experiments\nin common benchmarks and demonstrate that our MULTI-LANE achieves a new\nstate-of-the-art in MLCIL. Additionally, we show that MULTI-LANE is also\ncompetitive in the CIL setting. Source code available at\nhttps://github.com/tdemin16/multi-lane\n", "link": "http://arxiv.org/abs/2405.15633v1", "date": "2024-05-24", "relevancy": 2.7311, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5711}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5444}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20more%3A%20Summarizing%20Patch%20Tokens%20for%20efficient%20Multi-Label%0A%20%20Class-Incremental%20Learning&body=Title%3A%20Less%20is%20more%3A%20Summarizing%20Patch%20Tokens%20for%20efficient%20Multi-Label%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Thomas%20De%20Min%20and%20Massimiliano%20Mancini%20and%20St%C3%A9phane%20Lathuili%C3%A8re%20and%20Subhankar%20Roy%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20Prompt%20tuning%20has%20emerged%20as%20an%20effective%20rehearsal-free%20technique%20for%0Aclass-incremental%20learning%20%28CIL%29%20that%20learns%20a%20tiny%20set%20of%20task-specific%0Aparameters%20%28or%20prompts%29%20to%20instruct%20a%20pre-trained%20transformer%20to%20learn%20on%20a%0Asequence%20of%20tasks.%20Albeit%20effective%2C%20prompt%20tuning%20methods%20do%20not%20lend%20well%20in%0Athe%20multi-label%20class%20incremental%20learning%20%28MLCIL%29%20scenario%20%28where%20an%20image%0Acontains%20multiple%20foreground%20classes%29%20due%20to%20the%20ambiguity%20in%20selecting%20the%0Acorrect%20prompt%28s%29%20corresponding%20to%20different%20foreground%20objects%20belonging%20to%0Amultiple%20tasks.%20To%20circumvent%20this%20issue%20we%20propose%20to%20eliminate%20the%20prompt%0Aselection%20mechanism%20by%20maintaining%20task-specific%20pathways%2C%20which%20allow%20us%20to%0Alearn%20representations%20that%20do%20not%20interact%20with%20the%20ones%20from%20the%20other%20tasks.%0ASince%20independent%20pathways%20in%20truly%20incremental%20scenarios%20will%20result%20in%20an%0Aexplosion%20of%20computation%20due%20to%20the%20quadratically%20complex%20multi-head%0Aself-attention%20%28MSA%29%20operation%20in%20prompt%20tuning%2C%20we%20propose%20to%20reduce%20the%0Aoriginal%20patch%20token%20embeddings%20into%20summarized%20tokens.%20Prompt%20tuning%20is%20then%0Aapplied%20to%20these%20fewer%20summarized%20tokens%20to%20compute%20the%20final%20representation.%0AOur%20proposed%20method%20Multi-Label%20class%20incremental%20learning%20via%20summarising%0ApAtch%20tokeN%20Embeddings%20%28MULTI-LANE%29%20enables%20learning%20disentangled%20task-specific%0Arepresentations%20in%20MLCIL%20while%20ensuring%20fast%20inference.%20We%20conduct%20experiments%0Ain%20common%20benchmarks%20and%20demonstrate%20that%20our%20MULTI-LANE%20achieves%20a%20new%0Astate-of-the-art%20in%20MLCIL.%20Additionally%2C%20we%20show%20that%20MULTI-LANE%20is%20also%0Acompetitive%20in%20the%20CIL%20setting.%20Source%20code%20available%20at%0Ahttps%3A//github.com/tdemin16/multi-lane%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520more%253A%2520Summarizing%2520Patch%2520Tokens%2520for%2520efficient%2520Multi-Label%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DThomas%2520De%2520Min%2520and%2520Massimiliano%2520Mancini%2520and%2520St%25C3%25A9phane%2520Lathuili%25C3%25A8re%2520and%2520Subhankar%2520Roy%2520and%2520Elisa%2520Ricci%26entry.1292438233%3D%2520%2520Prompt%2520tuning%2520has%2520emerged%2520as%2520an%2520effective%2520rehearsal-free%2520technique%2520for%250Aclass-incremental%2520learning%2520%2528CIL%2529%2520that%2520learns%2520a%2520tiny%2520set%2520of%2520task-specific%250Aparameters%2520%2528or%2520prompts%2529%2520to%2520instruct%2520a%2520pre-trained%2520transformer%2520to%2520learn%2520on%2520a%250Asequence%2520of%2520tasks.%2520Albeit%2520effective%252C%2520prompt%2520tuning%2520methods%2520do%2520not%2520lend%2520well%2520in%250Athe%2520multi-label%2520class%2520incremental%2520learning%2520%2528MLCIL%2529%2520scenario%2520%2528where%2520an%2520image%250Acontains%2520multiple%2520foreground%2520classes%2529%2520due%2520to%2520the%2520ambiguity%2520in%2520selecting%2520the%250Acorrect%2520prompt%2528s%2529%2520corresponding%2520to%2520different%2520foreground%2520objects%2520belonging%2520to%250Amultiple%2520tasks.%2520To%2520circumvent%2520this%2520issue%2520we%2520propose%2520to%2520eliminate%2520the%2520prompt%250Aselection%2520mechanism%2520by%2520maintaining%2520task-specific%2520pathways%252C%2520which%2520allow%2520us%2520to%250Alearn%2520representations%2520that%2520do%2520not%2520interact%2520with%2520the%2520ones%2520from%2520the%2520other%2520tasks.%250ASince%2520independent%2520pathways%2520in%2520truly%2520incremental%2520scenarios%2520will%2520result%2520in%2520an%250Aexplosion%2520of%2520computation%2520due%2520to%2520the%2520quadratically%2520complex%2520multi-head%250Aself-attention%2520%2528MSA%2529%2520operation%2520in%2520prompt%2520tuning%252C%2520we%2520propose%2520to%2520reduce%2520the%250Aoriginal%2520patch%2520token%2520embeddings%2520into%2520summarized%2520tokens.%2520Prompt%2520tuning%2520is%2520then%250Aapplied%2520to%2520these%2520fewer%2520summarized%2520tokens%2520to%2520compute%2520the%2520final%2520representation.%250AOur%2520proposed%2520method%2520Multi-Label%2520class%2520incremental%2520learning%2520via%2520summarising%250ApAtch%2520tokeN%2520Embeddings%2520%2528MULTI-LANE%2529%2520enables%2520learning%2520disentangled%2520task-specific%250Arepresentations%2520in%2520MLCIL%2520while%2520ensuring%2520fast%2520inference.%2520We%2520conduct%2520experiments%250Ain%2520common%2520benchmarks%2520and%2520demonstrate%2520that%2520our%2520MULTI-LANE%2520achieves%2520a%2520new%250Astate-of-the-art%2520in%2520MLCIL.%2520Additionally%252C%2520we%2520show%2520that%2520MULTI-LANE%2520is%2520also%250Acompetitive%2520in%2520the%2520CIL%2520setting.%2520Source%2520code%2520available%2520at%250Ahttps%253A//github.com/tdemin16/multi-lane%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20more%3A%20Summarizing%20Patch%20Tokens%20for%20efficient%20Multi-Label%0A%20%20Class-Incremental%20Learning&entry.906535625=Thomas%20De%20Min%20and%20Massimiliano%20Mancini%20and%20St%C3%A9phane%20Lathuili%C3%A8re%20and%20Subhankar%20Roy%20and%20Elisa%20Ricci&entry.1292438233=%20%20Prompt%20tuning%20has%20emerged%20as%20an%20effective%20rehearsal-free%20technique%20for%0Aclass-incremental%20learning%20%28CIL%29%20that%20learns%20a%20tiny%20set%20of%20task-specific%0Aparameters%20%28or%20prompts%29%20to%20instruct%20a%20pre-trained%20transformer%20to%20learn%20on%20a%0Asequence%20of%20tasks.%20Albeit%20effective%2C%20prompt%20tuning%20methods%20do%20not%20lend%20well%20in%0Athe%20multi-label%20class%20incremental%20learning%20%28MLCIL%29%20scenario%20%28where%20an%20image%0Acontains%20multiple%20foreground%20classes%29%20due%20to%20the%20ambiguity%20in%20selecting%20the%0Acorrect%20prompt%28s%29%20corresponding%20to%20different%20foreground%20objects%20belonging%20to%0Amultiple%20tasks.%20To%20circumvent%20this%20issue%20we%20propose%20to%20eliminate%20the%20prompt%0Aselection%20mechanism%20by%20maintaining%20task-specific%20pathways%2C%20which%20allow%20us%20to%0Alearn%20representations%20that%20do%20not%20interact%20with%20the%20ones%20from%20the%20other%20tasks.%0ASince%20independent%20pathways%20in%20truly%20incremental%20scenarios%20will%20result%20in%20an%0Aexplosion%20of%20computation%20due%20to%20the%20quadratically%20complex%20multi-head%0Aself-attention%20%28MSA%29%20operation%20in%20prompt%20tuning%2C%20we%20propose%20to%20reduce%20the%0Aoriginal%20patch%20token%20embeddings%20into%20summarized%20tokens.%20Prompt%20tuning%20is%20then%0Aapplied%20to%20these%20fewer%20summarized%20tokens%20to%20compute%20the%20final%20representation.%0AOur%20proposed%20method%20Multi-Label%20class%20incremental%20learning%20via%20summarising%0ApAtch%20tokeN%20Embeddings%20%28MULTI-LANE%29%20enables%20learning%20disentangled%20task-specific%0Arepresentations%20in%20MLCIL%20while%20ensuring%20fast%20inference.%20We%20conduct%20experiments%0Ain%20common%20benchmarks%20and%20demonstrate%20that%20our%20MULTI-LANE%20achieves%20a%20new%0Astate-of-the-art%20in%20MLCIL.%20Additionally%2C%20we%20show%20that%20MULTI-LANE%20is%20also%0Acompetitive%20in%20the%20CIL%20setting.%20Source%20code%20available%20at%0Ahttps%3A//github.com/tdemin16/multi-lane%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15633v1&entry.124074799=Read"},
{"title": "When Generative AI Meets Workplace Learning: Creating A Realistic &\n  Motivating Learning Experience With A Generative PCA", "author": "Andreas Bucher and Birgit Schenk and Mateusz Dolata and Gerhard Schwabe", "abstract": "  Workplace learning is used to train employees systematically, e.g., via\ne-learning or in 1:1 training. However, this is often deemed ineffective and\ncostly. Whereas pure e-learning lacks the possibility of conversational\nexercise and personal contact, 1:1 training with human instructors involves a\nhigh level of personnel and organizational costs. Hence, pedagogical\nconversational agents (PCAs), based on generative AI, seem to compensate for\nthe disadvantages of both forms. Following Action Design Research, this paper\ndescribes an organizational communication training with a Generative PCA\n(GenPCA). The evaluation shows promising results: the agent was perceived\npositively among employees and contributed to an improvement in self-determined\nlearning. However, the integration of such agent comes not without limitations.\nWe conclude with suggestions concerning the didactical methods, which are\nsupported by a GenPCA, and possible improvements of such an agent for workplace\nlearning.\n", "link": "http://arxiv.org/abs/2405.15561v1", "date": "2024-05-24", "relevancy": 2.6956, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5595}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5448}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Generative%20AI%20Meets%20Workplace%20Learning%3A%20Creating%20A%20Realistic%20%26%0A%20%20Motivating%20Learning%20Experience%20With%20A%20Generative%20PCA&body=Title%3A%20When%20Generative%20AI%20Meets%20Workplace%20Learning%3A%20Creating%20A%20Realistic%20%26%0A%20%20Motivating%20Learning%20Experience%20With%20A%20Generative%20PCA%0AAuthor%3A%20Andreas%20Bucher%20and%20Birgit%20Schenk%20and%20Mateusz%20Dolata%20and%20Gerhard%20Schwabe%0AAbstract%3A%20%20%20Workplace%20learning%20is%20used%20to%20train%20employees%20systematically%2C%20e.g.%2C%20via%0Ae-learning%20or%20in%201%3A1%20training.%20However%2C%20this%20is%20often%20deemed%20ineffective%20and%0Acostly.%20Whereas%20pure%20e-learning%20lacks%20the%20possibility%20of%20conversational%0Aexercise%20and%20personal%20contact%2C%201%3A1%20training%20with%20human%20instructors%20involves%20a%0Ahigh%20level%20of%20personnel%20and%20organizational%20costs.%20Hence%2C%20pedagogical%0Aconversational%20agents%20%28PCAs%29%2C%20based%20on%20generative%20AI%2C%20seem%20to%20compensate%20for%0Athe%20disadvantages%20of%20both%20forms.%20Following%20Action%20Design%20Research%2C%20this%20paper%0Adescribes%20an%20organizational%20communication%20training%20with%20a%20Generative%20PCA%0A%28GenPCA%29.%20The%20evaluation%20shows%20promising%20results%3A%20the%20agent%20was%20perceived%0Apositively%20among%20employees%20and%20contributed%20to%20an%20improvement%20in%20self-determined%0Alearning.%20However%2C%20the%20integration%20of%20such%20agent%20comes%20not%20without%20limitations.%0AWe%20conclude%20with%20suggestions%20concerning%20the%20didactical%20methods%2C%20which%20are%0Asupported%20by%20a%20GenPCA%2C%20and%20possible%20improvements%20of%20such%20an%20agent%20for%20workplace%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Generative%2520AI%2520Meets%2520Workplace%2520Learning%253A%2520Creating%2520A%2520Realistic%2520%2526%250A%2520%2520Motivating%2520Learning%2520Experience%2520With%2520A%2520Generative%2520PCA%26entry.906535625%3DAndreas%2520Bucher%2520and%2520Birgit%2520Schenk%2520and%2520Mateusz%2520Dolata%2520and%2520Gerhard%2520Schwabe%26entry.1292438233%3D%2520%2520Workplace%2520learning%2520is%2520used%2520to%2520train%2520employees%2520systematically%252C%2520e.g.%252C%2520via%250Ae-learning%2520or%2520in%25201%253A1%2520training.%2520However%252C%2520this%2520is%2520often%2520deemed%2520ineffective%2520and%250Acostly.%2520Whereas%2520pure%2520e-learning%2520lacks%2520the%2520possibility%2520of%2520conversational%250Aexercise%2520and%2520personal%2520contact%252C%25201%253A1%2520training%2520with%2520human%2520instructors%2520involves%2520a%250Ahigh%2520level%2520of%2520personnel%2520and%2520organizational%2520costs.%2520Hence%252C%2520pedagogical%250Aconversational%2520agents%2520%2528PCAs%2529%252C%2520based%2520on%2520generative%2520AI%252C%2520seem%2520to%2520compensate%2520for%250Athe%2520disadvantages%2520of%2520both%2520forms.%2520Following%2520Action%2520Design%2520Research%252C%2520this%2520paper%250Adescribes%2520an%2520organizational%2520communication%2520training%2520with%2520a%2520Generative%2520PCA%250A%2528GenPCA%2529.%2520The%2520evaluation%2520shows%2520promising%2520results%253A%2520the%2520agent%2520was%2520perceived%250Apositively%2520among%2520employees%2520and%2520contributed%2520to%2520an%2520improvement%2520in%2520self-determined%250Alearning.%2520However%252C%2520the%2520integration%2520of%2520such%2520agent%2520comes%2520not%2520without%2520limitations.%250AWe%2520conclude%2520with%2520suggestions%2520concerning%2520the%2520didactical%2520methods%252C%2520which%2520are%250Asupported%2520by%2520a%2520GenPCA%252C%2520and%2520possible%2520improvements%2520of%2520such%2520an%2520agent%2520for%2520workplace%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Generative%20AI%20Meets%20Workplace%20Learning%3A%20Creating%20A%20Realistic%20%26%0A%20%20Motivating%20Learning%20Experience%20With%20A%20Generative%20PCA&entry.906535625=Andreas%20Bucher%20and%20Birgit%20Schenk%20and%20Mateusz%20Dolata%20and%20Gerhard%20Schwabe&entry.1292438233=%20%20Workplace%20learning%20is%20used%20to%20train%20employees%20systematically%2C%20e.g.%2C%20via%0Ae-learning%20or%20in%201%3A1%20training.%20However%2C%20this%20is%20often%20deemed%20ineffective%20and%0Acostly.%20Whereas%20pure%20e-learning%20lacks%20the%20possibility%20of%20conversational%0Aexercise%20and%20personal%20contact%2C%201%3A1%20training%20with%20human%20instructors%20involves%20a%0Ahigh%20level%20of%20personnel%20and%20organizational%20costs.%20Hence%2C%20pedagogical%0Aconversational%20agents%20%28PCAs%29%2C%20based%20on%20generative%20AI%2C%20seem%20to%20compensate%20for%0Athe%20disadvantages%20of%20both%20forms.%20Following%20Action%20Design%20Research%2C%20this%20paper%0Adescribes%20an%20organizational%20communication%20training%20with%20a%20Generative%20PCA%0A%28GenPCA%29.%20The%20evaluation%20shows%20promising%20results%3A%20the%20agent%20was%20perceived%0Apositively%20among%20employees%20and%20contributed%20to%20an%20improvement%20in%20self-determined%0Alearning.%20However%2C%20the%20integration%20of%20such%20agent%20comes%20not%20without%20limitations.%0AWe%20conclude%20with%20suggestions%20concerning%20the%20didactical%20methods%2C%20which%20are%0Asupported%20by%20a%20GenPCA%2C%20and%20possible%20improvements%20of%20such%20an%20agent%20for%20workplace%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15561v1&entry.124074799=Read"},
{"title": "Neural Bounding", "author": "Stephanie Wenxin Liu and Michael Fischer and Paul D. Yoo and Tobias Ritschel", "abstract": "  Bounding volumes are an established concept in computer graphics and vision\ntasks but have seen little change since their early inception. In this work, we\nstudy the use of neural networks as bounding volumes. Our key observation is\nthat bounding, which so far has primarily been considered a problem of\ncomputational geometry, can be redefined as a problem of learning to classify\nspace into free or occupied. This learning-based approach is particularly\nadvantageous in high-dimensional spaces, such as animated scenes with complex\nqueries, where neural networks are known to excel. However, unlocking neural\nbounding requires a twist: allowing -- but also limiting -- false positives,\nwhile ensuring that the number of false negatives is strictly zero. We enable\nsuch tight and conservative results using a dynamically-weighted asymmetric\nloss function. Our results show that our neural bounding produces up to an\norder of magnitude fewer false positives than traditional methods. In addition,\nwe propose an extension of our bounding method using early exits that\naccelerates query speeds by 25%. We also demonstrate that our approach is\napplicable to non-deep learning models that train within seconds. Our project\npage is at: https://wenxin-liu.github.io/neural_bounding/.\n", "link": "http://arxiv.org/abs/2310.06822v5", "date": "2024-05-24", "relevancy": 2.6956, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5447}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5442}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Bounding&body=Title%3A%20Neural%20Bounding%0AAuthor%3A%20Stephanie%20Wenxin%20Liu%20and%20Michael%20Fischer%20and%20Paul%20D.%20Yoo%20and%20Tobias%20Ritschel%0AAbstract%3A%20%20%20Bounding%20volumes%20are%20an%20established%20concept%20in%20computer%20graphics%20and%20vision%0Atasks%20but%20have%20seen%20little%20change%20since%20their%20early%20inception.%20In%20this%20work%2C%20we%0Astudy%20the%20use%20of%20neural%20networks%20as%20bounding%20volumes.%20Our%20key%20observation%20is%0Athat%20bounding%2C%20which%20so%20far%20has%20primarily%20been%20considered%20a%20problem%20of%0Acomputational%20geometry%2C%20can%20be%20redefined%20as%20a%20problem%20of%20learning%20to%20classify%0Aspace%20into%20free%20or%20occupied.%20This%20learning-based%20approach%20is%20particularly%0Aadvantageous%20in%20high-dimensional%20spaces%2C%20such%20as%20animated%20scenes%20with%20complex%0Aqueries%2C%20where%20neural%20networks%20are%20known%20to%20excel.%20However%2C%20unlocking%20neural%0Abounding%20requires%20a%20twist%3A%20allowing%20--%20but%20also%20limiting%20--%20false%20positives%2C%0Awhile%20ensuring%20that%20the%20number%20of%20false%20negatives%20is%20strictly%20zero.%20We%20enable%0Asuch%20tight%20and%20conservative%20results%20using%20a%20dynamically-weighted%20asymmetric%0Aloss%20function.%20Our%20results%20show%20that%20our%20neural%20bounding%20produces%20up%20to%20an%0Aorder%20of%20magnitude%20fewer%20false%20positives%20than%20traditional%20methods.%20In%20addition%2C%0Awe%20propose%20an%20extension%20of%20our%20bounding%20method%20using%20early%20exits%20that%0Aaccelerates%20query%20speeds%20by%2025%25.%20We%20also%20demonstrate%20that%20our%20approach%20is%0Aapplicable%20to%20non-deep%20learning%20models%20that%20train%20within%20seconds.%20Our%20project%0Apage%20is%20at%3A%20https%3A//wenxin-liu.github.io/neural_bounding/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06822v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Bounding%26entry.906535625%3DStephanie%2520Wenxin%2520Liu%2520and%2520Michael%2520Fischer%2520and%2520Paul%2520D.%2520Yoo%2520and%2520Tobias%2520Ritschel%26entry.1292438233%3D%2520%2520Bounding%2520volumes%2520are%2520an%2520established%2520concept%2520in%2520computer%2520graphics%2520and%2520vision%250Atasks%2520but%2520have%2520seen%2520little%2520change%2520since%2520their%2520early%2520inception.%2520In%2520this%2520work%252C%2520we%250Astudy%2520the%2520use%2520of%2520neural%2520networks%2520as%2520bounding%2520volumes.%2520Our%2520key%2520observation%2520is%250Athat%2520bounding%252C%2520which%2520so%2520far%2520has%2520primarily%2520been%2520considered%2520a%2520problem%2520of%250Acomputational%2520geometry%252C%2520can%2520be%2520redefined%2520as%2520a%2520problem%2520of%2520learning%2520to%2520classify%250Aspace%2520into%2520free%2520or%2520occupied.%2520This%2520learning-based%2520approach%2520is%2520particularly%250Aadvantageous%2520in%2520high-dimensional%2520spaces%252C%2520such%2520as%2520animated%2520scenes%2520with%2520complex%250Aqueries%252C%2520where%2520neural%2520networks%2520are%2520known%2520to%2520excel.%2520However%252C%2520unlocking%2520neural%250Abounding%2520requires%2520a%2520twist%253A%2520allowing%2520--%2520but%2520also%2520limiting%2520--%2520false%2520positives%252C%250Awhile%2520ensuring%2520that%2520the%2520number%2520of%2520false%2520negatives%2520is%2520strictly%2520zero.%2520We%2520enable%250Asuch%2520tight%2520and%2520conservative%2520results%2520using%2520a%2520dynamically-weighted%2520asymmetric%250Aloss%2520function.%2520Our%2520results%2520show%2520that%2520our%2520neural%2520bounding%2520produces%2520up%2520to%2520an%250Aorder%2520of%2520magnitude%2520fewer%2520false%2520positives%2520than%2520traditional%2520methods.%2520In%2520addition%252C%250Awe%2520propose%2520an%2520extension%2520of%2520our%2520bounding%2520method%2520using%2520early%2520exits%2520that%250Aaccelerates%2520query%2520speeds%2520by%252025%2525.%2520We%2520also%2520demonstrate%2520that%2520our%2520approach%2520is%250Aapplicable%2520to%2520non-deep%2520learning%2520models%2520that%2520train%2520within%2520seconds.%2520Our%2520project%250Apage%2520is%2520at%253A%2520https%253A//wenxin-liu.github.io/neural_bounding/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06822v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Bounding&entry.906535625=Stephanie%20Wenxin%20Liu%20and%20Michael%20Fischer%20and%20Paul%20D.%20Yoo%20and%20Tobias%20Ritschel&entry.1292438233=%20%20Bounding%20volumes%20are%20an%20established%20concept%20in%20computer%20graphics%20and%20vision%0Atasks%20but%20have%20seen%20little%20change%20since%20their%20early%20inception.%20In%20this%20work%2C%20we%0Astudy%20the%20use%20of%20neural%20networks%20as%20bounding%20volumes.%20Our%20key%20observation%20is%0Athat%20bounding%2C%20which%20so%20far%20has%20primarily%20been%20considered%20a%20problem%20of%0Acomputational%20geometry%2C%20can%20be%20redefined%20as%20a%20problem%20of%20learning%20to%20classify%0Aspace%20into%20free%20or%20occupied.%20This%20learning-based%20approach%20is%20particularly%0Aadvantageous%20in%20high-dimensional%20spaces%2C%20such%20as%20animated%20scenes%20with%20complex%0Aqueries%2C%20where%20neural%20networks%20are%20known%20to%20excel.%20However%2C%20unlocking%20neural%0Abounding%20requires%20a%20twist%3A%20allowing%20--%20but%20also%20limiting%20--%20false%20positives%2C%0Awhile%20ensuring%20that%20the%20number%20of%20false%20negatives%20is%20strictly%20zero.%20We%20enable%0Asuch%20tight%20and%20conservative%20results%20using%20a%20dynamically-weighted%20asymmetric%0Aloss%20function.%20Our%20results%20show%20that%20our%20neural%20bounding%20produces%20up%20to%20an%0Aorder%20of%20magnitude%20fewer%20false%20positives%20than%20traditional%20methods.%20In%20addition%2C%0Awe%20propose%20an%20extension%20of%20our%20bounding%20method%20using%20early%20exits%20that%0Aaccelerates%20query%20speeds%20by%2025%25.%20We%20also%20demonstrate%20that%20our%20approach%20is%0Aapplicable%20to%20non-deep%20learning%20models%20that%20train%20within%20seconds.%20Our%20project%0Apage%20is%20at%3A%20https%3A//wenxin-liu.github.io/neural_bounding/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06822v5&entry.124074799=Read"},
{"title": "Out of Many, One: Designing and Scaffolding Proteins at the Scale of the\n  Structural Universe with Genie 2", "author": "Yeqing Lin and Minji Lee and Zhao Zhang and Mohammed AlQuraishi", "abstract": "  Protein diffusion models have emerged as a promising approach for protein\ndesign. One such pioneering model is Genie, a method that asymmetrically\nrepresents protein structures during the forward and backward processes, using\nsimple Gaussian noising for the former and expressive SE(3)-equivariant\nattention for the latter. In this work we introduce Genie 2, extending Genie to\ncapture a larger and more diverse protein structure space through architectural\ninnovations and massive data augmentation. Genie 2 adds motif scaffolding\ncapabilities via a novel multi-motif framework that designs co-occurring motifs\nwith unspecified inter-motif positions and orientations. This makes possible\ncomplex protein designs that engage multiple interaction partners and perform\nmultiple functions. On both unconditional and conditional generation, Genie 2\nachieves state-of-the-art performance, outperforming all known methods on key\ndesign metrics including designability, diversity, and novelty. Genie 2 also\nsolves more motif scaffolding problems than other methods and does so with more\nunique and varied solutions. Taken together, these advances set a new standard\nfor structure-based protein design. Genie 2 inference and training code, as\nwell as model weights, are freely available at:\nhttps://github.com/aqlaboratory/genie2.\n", "link": "http://arxiv.org/abs/2405.15489v1", "date": "2024-05-24", "relevancy": 2.6835, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5851}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5187}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out%20of%20Many%2C%20One%3A%20Designing%20and%20Scaffolding%20Proteins%20at%20the%20Scale%20of%20the%0A%20%20Structural%20Universe%20with%20Genie%202&body=Title%3A%20Out%20of%20Many%2C%20One%3A%20Designing%20and%20Scaffolding%20Proteins%20at%20the%20Scale%20of%20the%0A%20%20Structural%20Universe%20with%20Genie%202%0AAuthor%3A%20Yeqing%20Lin%20and%20Minji%20Lee%20and%20Zhao%20Zhang%20and%20Mohammed%20AlQuraishi%0AAbstract%3A%20%20%20Protein%20diffusion%20models%20have%20emerged%20as%20a%20promising%20approach%20for%20protein%0Adesign.%20One%20such%20pioneering%20model%20is%20Genie%2C%20a%20method%20that%20asymmetrically%0Arepresents%20protein%20structures%20during%20the%20forward%20and%20backward%20processes%2C%20using%0Asimple%20Gaussian%20noising%20for%20the%20former%20and%20expressive%20SE%283%29-equivariant%0Aattention%20for%20the%20latter.%20In%20this%20work%20we%20introduce%20Genie%202%2C%20extending%20Genie%20to%0Acapture%20a%20larger%20and%20more%20diverse%20protein%20structure%20space%20through%20architectural%0Ainnovations%20and%20massive%20data%20augmentation.%20Genie%202%20adds%20motif%20scaffolding%0Acapabilities%20via%20a%20novel%20multi-motif%20framework%20that%20designs%20co-occurring%20motifs%0Awith%20unspecified%20inter-motif%20positions%20and%20orientations.%20This%20makes%20possible%0Acomplex%20protein%20designs%20that%20engage%20multiple%20interaction%20partners%20and%20perform%0Amultiple%20functions.%20On%20both%20unconditional%20and%20conditional%20generation%2C%20Genie%202%0Aachieves%20state-of-the-art%20performance%2C%20outperforming%20all%20known%20methods%20on%20key%0Adesign%20metrics%20including%20designability%2C%20diversity%2C%20and%20novelty.%20Genie%202%20also%0Asolves%20more%20motif%20scaffolding%20problems%20than%20other%20methods%20and%20does%20so%20with%20more%0Aunique%20and%20varied%20solutions.%20Taken%20together%2C%20these%20advances%20set%20a%20new%20standard%0Afor%20structure-based%20protein%20design.%20Genie%202%20inference%20and%20training%20code%2C%20as%0Awell%20as%20model%20weights%2C%20are%20freely%20available%20at%3A%0Ahttps%3A//github.com/aqlaboratory/genie2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut%2520of%2520Many%252C%2520One%253A%2520Designing%2520and%2520Scaffolding%2520Proteins%2520at%2520the%2520Scale%2520of%2520the%250A%2520%2520Structural%2520Universe%2520with%2520Genie%25202%26entry.906535625%3DYeqing%2520Lin%2520and%2520Minji%2520Lee%2520and%2520Zhao%2520Zhang%2520and%2520Mohammed%2520AlQuraishi%26entry.1292438233%3D%2520%2520Protein%2520diffusion%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520protein%250Adesign.%2520One%2520such%2520pioneering%2520model%2520is%2520Genie%252C%2520a%2520method%2520that%2520asymmetrically%250Arepresents%2520protein%2520structures%2520during%2520the%2520forward%2520and%2520backward%2520processes%252C%2520using%250Asimple%2520Gaussian%2520noising%2520for%2520the%2520former%2520and%2520expressive%2520SE%25283%2529-equivariant%250Aattention%2520for%2520the%2520latter.%2520In%2520this%2520work%2520we%2520introduce%2520Genie%25202%252C%2520extending%2520Genie%2520to%250Acapture%2520a%2520larger%2520and%2520more%2520diverse%2520protein%2520structure%2520space%2520through%2520architectural%250Ainnovations%2520and%2520massive%2520data%2520augmentation.%2520Genie%25202%2520adds%2520motif%2520scaffolding%250Acapabilities%2520via%2520a%2520novel%2520multi-motif%2520framework%2520that%2520designs%2520co-occurring%2520motifs%250Awith%2520unspecified%2520inter-motif%2520positions%2520and%2520orientations.%2520This%2520makes%2520possible%250Acomplex%2520protein%2520designs%2520that%2520engage%2520multiple%2520interaction%2520partners%2520and%2520perform%250Amultiple%2520functions.%2520On%2520both%2520unconditional%2520and%2520conditional%2520generation%252C%2520Genie%25202%250Aachieves%2520state-of-the-art%2520performance%252C%2520outperforming%2520all%2520known%2520methods%2520on%2520key%250Adesign%2520metrics%2520including%2520designability%252C%2520diversity%252C%2520and%2520novelty.%2520Genie%25202%2520also%250Asolves%2520more%2520motif%2520scaffolding%2520problems%2520than%2520other%2520methods%2520and%2520does%2520so%2520with%2520more%250Aunique%2520and%2520varied%2520solutions.%2520Taken%2520together%252C%2520these%2520advances%2520set%2520a%2520new%2520standard%250Afor%2520structure-based%2520protein%2520design.%2520Genie%25202%2520inference%2520and%2520training%2520code%252C%2520as%250Awell%2520as%2520model%2520weights%252C%2520are%2520freely%2520available%2520at%253A%250Ahttps%253A//github.com/aqlaboratory/genie2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out%20of%20Many%2C%20One%3A%20Designing%20and%20Scaffolding%20Proteins%20at%20the%20Scale%20of%20the%0A%20%20Structural%20Universe%20with%20Genie%202&entry.906535625=Yeqing%20Lin%20and%20Minji%20Lee%20and%20Zhao%20Zhang%20and%20Mohammed%20AlQuraishi&entry.1292438233=%20%20Protein%20diffusion%20models%20have%20emerged%20as%20a%20promising%20approach%20for%20protein%0Adesign.%20One%20such%20pioneering%20model%20is%20Genie%2C%20a%20method%20that%20asymmetrically%0Arepresents%20protein%20structures%20during%20the%20forward%20and%20backward%20processes%2C%20using%0Asimple%20Gaussian%20noising%20for%20the%20former%20and%20expressive%20SE%283%29-equivariant%0Aattention%20for%20the%20latter.%20In%20this%20work%20we%20introduce%20Genie%202%2C%20extending%20Genie%20to%0Acapture%20a%20larger%20and%20more%20diverse%20protein%20structure%20space%20through%20architectural%0Ainnovations%20and%20massive%20data%20augmentation.%20Genie%202%20adds%20motif%20scaffolding%0Acapabilities%20via%20a%20novel%20multi-motif%20framework%20that%20designs%20co-occurring%20motifs%0Awith%20unspecified%20inter-motif%20positions%20and%20orientations.%20This%20makes%20possible%0Acomplex%20protein%20designs%20that%20engage%20multiple%20interaction%20partners%20and%20perform%0Amultiple%20functions.%20On%20both%20unconditional%20and%20conditional%20generation%2C%20Genie%202%0Aachieves%20state-of-the-art%20performance%2C%20outperforming%20all%20known%20methods%20on%20key%0Adesign%20metrics%20including%20designability%2C%20diversity%2C%20and%20novelty.%20Genie%202%20also%0Asolves%20more%20motif%20scaffolding%20problems%20than%20other%20methods%20and%20does%20so%20with%20more%0Aunique%20and%20varied%20solutions.%20Taken%20together%2C%20these%20advances%20set%20a%20new%20standard%0Afor%20structure-based%20protein%20design.%20Genie%202%20inference%20and%20training%20code%2C%20as%0Awell%20as%20model%20weights%2C%20are%20freely%20available%20at%3A%0Ahttps%3A//github.com/aqlaboratory/genie2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15489v1&entry.124074799=Read"},
{"title": "HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning", "author": "Kamil Ksi\u0105\u017cek and Przemys\u0142aw Spurek", "abstract": "  Artificial neural networks suffer from catastrophic forgetting when they are\nsequentially trained on multiple tasks. Many continual learning (CL) strategies\nare trying to overcome this problem. One of the most effective is the\nhypernetwork-based approach. The hypernetwork generates the weights of a target\nmodel based on the task's identity. The model's main limitation is that, in\npractice, the hypernetwork can produce completely different architectures for\nsubsequent tasks. To solve such a problem, we use the lottery ticket\nhypothesis, which postulates the existence of sparse subnetworks, named winning\ntickets, that preserve the performance of a whole network. In the paper, we\npropose a method called HyperMask, which dynamically filters a target network\ndepending on the CL task. The hypernetwork produces semi-binary masks to obtain\ndedicated target subnetworks. Moreover, due to the lottery ticket hypothesis,\nwe can use a single network with weighted subnets. Depending on the task, the\nimportance of some weights may be dynamically enhanced while others may be\nweakened. HyperMask achieves competitive results in several CL datasets and, in\nsome scenarios, goes beyond the state-of-the-art scores, both with derived and\nunknown task identities.\n", "link": "http://arxiv.org/abs/2310.00113v4", "date": "2024-05-24", "relevancy": 2.6714, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5534}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperMask%3A%20Adaptive%20Hypernetwork-based%20Masks%20for%20Continual%20Learning&body=Title%3A%20HyperMask%3A%20Adaptive%20Hypernetwork-based%20Masks%20for%20Continual%20Learning%0AAuthor%3A%20Kamil%20Ksi%C4%85%C5%BCek%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Artificial%20neural%20networks%20suffer%20from%20catastrophic%20forgetting%20when%20they%20are%0Asequentially%20trained%20on%20multiple%20tasks.%20Many%20continual%20learning%20%28CL%29%20strategies%0Aare%20trying%20to%20overcome%20this%20problem.%20One%20of%20the%20most%20effective%20is%20the%0Ahypernetwork-based%20approach.%20The%20hypernetwork%20generates%20the%20weights%20of%20a%20target%0Amodel%20based%20on%20the%20task%27s%20identity.%20The%20model%27s%20main%20limitation%20is%20that%2C%20in%0Apractice%2C%20the%20hypernetwork%20can%20produce%20completely%20different%20architectures%20for%0Asubsequent%20tasks.%20To%20solve%20such%20a%20problem%2C%20we%20use%20the%20lottery%20ticket%0Ahypothesis%2C%20which%20postulates%20the%20existence%20of%20sparse%20subnetworks%2C%20named%20winning%0Atickets%2C%20that%20preserve%20the%20performance%20of%20a%20whole%20network.%20In%20the%20paper%2C%20we%0Apropose%20a%20method%20called%20HyperMask%2C%20which%20dynamically%20filters%20a%20target%20network%0Adepending%20on%20the%20CL%20task.%20The%20hypernetwork%20produces%20semi-binary%20masks%20to%20obtain%0Adedicated%20target%20subnetworks.%20Moreover%2C%20due%20to%20the%20lottery%20ticket%20hypothesis%2C%0Awe%20can%20use%20a%20single%20network%20with%20weighted%20subnets.%20Depending%20on%20the%20task%2C%20the%0Aimportance%20of%20some%20weights%20may%20be%20dynamically%20enhanced%20while%20others%20may%20be%0Aweakened.%20HyperMask%20achieves%20competitive%20results%20in%20several%20CL%20datasets%20and%2C%20in%0Asome%20scenarios%2C%20goes%20beyond%20the%20state-of-the-art%20scores%2C%20both%20with%20derived%20and%0Aunknown%20task%20identities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00113v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperMask%253A%2520Adaptive%2520Hypernetwork-based%2520Masks%2520for%2520Continual%2520Learning%26entry.906535625%3DKamil%2520Ksi%25C4%2585%25C5%25BCek%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Artificial%2520neural%2520networks%2520suffer%2520from%2520catastrophic%2520forgetting%2520when%2520they%2520are%250Asequentially%2520trained%2520on%2520multiple%2520tasks.%2520Many%2520continual%2520learning%2520%2528CL%2529%2520strategies%250Aare%2520trying%2520to%2520overcome%2520this%2520problem.%2520One%2520of%2520the%2520most%2520effective%2520is%2520the%250Ahypernetwork-based%2520approach.%2520The%2520hypernetwork%2520generates%2520the%2520weights%2520of%2520a%2520target%250Amodel%2520based%2520on%2520the%2520task%2527s%2520identity.%2520The%2520model%2527s%2520main%2520limitation%2520is%2520that%252C%2520in%250Apractice%252C%2520the%2520hypernetwork%2520can%2520produce%2520completely%2520different%2520architectures%2520for%250Asubsequent%2520tasks.%2520To%2520solve%2520such%2520a%2520problem%252C%2520we%2520use%2520the%2520lottery%2520ticket%250Ahypothesis%252C%2520which%2520postulates%2520the%2520existence%2520of%2520sparse%2520subnetworks%252C%2520named%2520winning%250Atickets%252C%2520that%2520preserve%2520the%2520performance%2520of%2520a%2520whole%2520network.%2520In%2520the%2520paper%252C%2520we%250Apropose%2520a%2520method%2520called%2520HyperMask%252C%2520which%2520dynamically%2520filters%2520a%2520target%2520network%250Adepending%2520on%2520the%2520CL%2520task.%2520The%2520hypernetwork%2520produces%2520semi-binary%2520masks%2520to%2520obtain%250Adedicated%2520target%2520subnetworks.%2520Moreover%252C%2520due%2520to%2520the%2520lottery%2520ticket%2520hypothesis%252C%250Awe%2520can%2520use%2520a%2520single%2520network%2520with%2520weighted%2520subnets.%2520Depending%2520on%2520the%2520task%252C%2520the%250Aimportance%2520of%2520some%2520weights%2520may%2520be%2520dynamically%2520enhanced%2520while%2520others%2520may%2520be%250Aweakened.%2520HyperMask%2520achieves%2520competitive%2520results%2520in%2520several%2520CL%2520datasets%2520and%252C%2520in%250Asome%2520scenarios%252C%2520goes%2520beyond%2520the%2520state-of-the-art%2520scores%252C%2520both%2520with%2520derived%2520and%250Aunknown%2520task%2520identities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00113v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperMask%3A%20Adaptive%20Hypernetwork-based%20Masks%20for%20Continual%20Learning&entry.906535625=Kamil%20Ksi%C4%85%C5%BCek%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Artificial%20neural%20networks%20suffer%20from%20catastrophic%20forgetting%20when%20they%20are%0Asequentially%20trained%20on%20multiple%20tasks.%20Many%20continual%20learning%20%28CL%29%20strategies%0Aare%20trying%20to%20overcome%20this%20problem.%20One%20of%20the%20most%20effective%20is%20the%0Ahypernetwork-based%20approach.%20The%20hypernetwork%20generates%20the%20weights%20of%20a%20target%0Amodel%20based%20on%20the%20task%27s%20identity.%20The%20model%27s%20main%20limitation%20is%20that%2C%20in%0Apractice%2C%20the%20hypernetwork%20can%20produce%20completely%20different%20architectures%20for%0Asubsequent%20tasks.%20To%20solve%20such%20a%20problem%2C%20we%20use%20the%20lottery%20ticket%0Ahypothesis%2C%20which%20postulates%20the%20existence%20of%20sparse%20subnetworks%2C%20named%20winning%0Atickets%2C%20that%20preserve%20the%20performance%20of%20a%20whole%20network.%20In%20the%20paper%2C%20we%0Apropose%20a%20method%20called%20HyperMask%2C%20which%20dynamically%20filters%20a%20target%20network%0Adepending%20on%20the%20CL%20task.%20The%20hypernetwork%20produces%20semi-binary%20masks%20to%20obtain%0Adedicated%20target%20subnetworks.%20Moreover%2C%20due%20to%20the%20lottery%20ticket%20hypothesis%2C%0Awe%20can%20use%20a%20single%20network%20with%20weighted%20subnets.%20Depending%20on%20the%20task%2C%20the%0Aimportance%20of%20some%20weights%20may%20be%20dynamically%20enhanced%20while%20others%20may%20be%0Aweakened.%20HyperMask%20achieves%20competitive%20results%20in%20several%20CL%20datasets%20and%2C%20in%0Asome%20scenarios%2C%20goes%20beyond%20the%20state-of-the-art%20scores%2C%20both%20with%20derived%20and%0Aunknown%20task%20identities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00113v4&entry.124074799=Read"},
{"title": "PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible\n  Pose Control", "author": "Yong Zhong and Min Zhao and Zebin You and Xiaofeng Yu and Changwang Zhang and Chongxuan Li", "abstract": "  In this paper, we introduce PoseCrafter, a one-shot method for personalized\nvideo generation following the control of flexible poses. Built upon Stable\nDiffusion and ControlNet, we carefully design an inference process to produce\nhigh-quality videos without the corresponding ground-truth frames. First, we\nselect an appropriate reference frame from the training video and invert it to\ninitialize all latent variables for generation. Then, we insert the\ncorresponding training pose into the target pose sequences to enhance\nfaithfulness through a trained temporal attention module. Furthermore, to\nalleviate the face and hand degradation resulting from discrepancies between\nposes of training videos and inference poses, we implement simple latent\nediting through an affine transformation matrix involving facial and hand\nlandmarks. Extensive experiments on several datasets demonstrate that\nPoseCrafter achieves superior results to baselines pre-trained on a vast\ncollection of videos under 8 commonly used metrics. Besides, PoseCrafter can\nfollow poses from different individuals or artificial edits and simultaneously\nretain the human identity in an open-domain training video. Our project page is\navailable at https://ml-gsai.github.io/PoseCrafter-demo/.\n", "link": "http://arxiv.org/abs/2405.14582v2", "date": "2024-05-24", "relevancy": 2.6661, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6877}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.674}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseCrafter%3A%20One-Shot%20Personalized%20Video%20Synthesis%20Following%20Flexible%0A%20%20Pose%20Control&body=Title%3A%20PoseCrafter%3A%20One-Shot%20Personalized%20Video%20Synthesis%20Following%20Flexible%0A%20%20Pose%20Control%0AAuthor%3A%20Yong%20Zhong%20and%20Min%20Zhao%20and%20Zebin%20You%20and%20Xiaofeng%20Yu%20and%20Changwang%20Zhang%20and%20Chongxuan%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20PoseCrafter%2C%20a%20one-shot%20method%20for%20personalized%0Avideo%20generation%20following%20the%20control%20of%20flexible%20poses.%20Built%20upon%20Stable%0ADiffusion%20and%20ControlNet%2C%20we%20carefully%20design%20an%20inference%20process%20to%20produce%0Ahigh-quality%20videos%20without%20the%20corresponding%20ground-truth%20frames.%20First%2C%20we%0Aselect%20an%20appropriate%20reference%20frame%20from%20the%20training%20video%20and%20invert%20it%20to%0Ainitialize%20all%20latent%20variables%20for%20generation.%20Then%2C%20we%20insert%20the%0Acorresponding%20training%20pose%20into%20the%20target%20pose%20sequences%20to%20enhance%0Afaithfulness%20through%20a%20trained%20temporal%20attention%20module.%20Furthermore%2C%20to%0Aalleviate%20the%20face%20and%20hand%20degradation%20resulting%20from%20discrepancies%20between%0Aposes%20of%20training%20videos%20and%20inference%20poses%2C%20we%20implement%20simple%20latent%0Aediting%20through%20an%20affine%20transformation%20matrix%20involving%20facial%20and%20hand%0Alandmarks.%20Extensive%20experiments%20on%20several%20datasets%20demonstrate%20that%0APoseCrafter%20achieves%20superior%20results%20to%20baselines%20pre-trained%20on%20a%20vast%0Acollection%20of%20videos%20under%208%20commonly%20used%20metrics.%20Besides%2C%20PoseCrafter%20can%0Afollow%20poses%20from%20different%20individuals%20or%20artificial%20edits%20and%20simultaneously%0Aretain%20the%20human%20identity%20in%20an%20open-domain%20training%20video.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//ml-gsai.github.io/PoseCrafter-demo/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseCrafter%253A%2520One-Shot%2520Personalized%2520Video%2520Synthesis%2520Following%2520Flexible%250A%2520%2520Pose%2520Control%26entry.906535625%3DYong%2520Zhong%2520and%2520Min%2520Zhao%2520and%2520Zebin%2520You%2520and%2520Xiaofeng%2520Yu%2520and%2520Changwang%2520Zhang%2520and%2520Chongxuan%2520Li%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PoseCrafter%252C%2520a%2520one-shot%2520method%2520for%2520personalized%250Avideo%2520generation%2520following%2520the%2520control%2520of%2520flexible%2520poses.%2520Built%2520upon%2520Stable%250ADiffusion%2520and%2520ControlNet%252C%2520we%2520carefully%2520design%2520an%2520inference%2520process%2520to%2520produce%250Ahigh-quality%2520videos%2520without%2520the%2520corresponding%2520ground-truth%2520frames.%2520First%252C%2520we%250Aselect%2520an%2520appropriate%2520reference%2520frame%2520from%2520the%2520training%2520video%2520and%2520invert%2520it%2520to%250Ainitialize%2520all%2520latent%2520variables%2520for%2520generation.%2520Then%252C%2520we%2520insert%2520the%250Acorresponding%2520training%2520pose%2520into%2520the%2520target%2520pose%2520sequences%2520to%2520enhance%250Afaithfulness%2520through%2520a%2520trained%2520temporal%2520attention%2520module.%2520Furthermore%252C%2520to%250Aalleviate%2520the%2520face%2520and%2520hand%2520degradation%2520resulting%2520from%2520discrepancies%2520between%250Aposes%2520of%2520training%2520videos%2520and%2520inference%2520poses%252C%2520we%2520implement%2520simple%2520latent%250Aediting%2520through%2520an%2520affine%2520transformation%2520matrix%2520involving%2520facial%2520and%2520hand%250Alandmarks.%2520Extensive%2520experiments%2520on%2520several%2520datasets%2520demonstrate%2520that%250APoseCrafter%2520achieves%2520superior%2520results%2520to%2520baselines%2520pre-trained%2520on%2520a%2520vast%250Acollection%2520of%2520videos%2520under%25208%2520commonly%2520used%2520metrics.%2520Besides%252C%2520PoseCrafter%2520can%250Afollow%2520poses%2520from%2520different%2520individuals%2520or%2520artificial%2520edits%2520and%2520simultaneously%250Aretain%2520the%2520human%2520identity%2520in%2520an%2520open-domain%2520training%2520video.%2520Our%2520project%2520page%2520is%250Aavailable%2520at%2520https%253A//ml-gsai.github.io/PoseCrafter-demo/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseCrafter%3A%20One-Shot%20Personalized%20Video%20Synthesis%20Following%20Flexible%0A%20%20Pose%20Control&entry.906535625=Yong%20Zhong%20and%20Min%20Zhao%20and%20Zebin%20You%20and%20Xiaofeng%20Yu%20and%20Changwang%20Zhang%20and%20Chongxuan%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20PoseCrafter%2C%20a%20one-shot%20method%20for%20personalized%0Avideo%20generation%20following%20the%20control%20of%20flexible%20poses.%20Built%20upon%20Stable%0ADiffusion%20and%20ControlNet%2C%20we%20carefully%20design%20an%20inference%20process%20to%20produce%0Ahigh-quality%20videos%20without%20the%20corresponding%20ground-truth%20frames.%20First%2C%20we%0Aselect%20an%20appropriate%20reference%20frame%20from%20the%20training%20video%20and%20invert%20it%20to%0Ainitialize%20all%20latent%20variables%20for%20generation.%20Then%2C%20we%20insert%20the%0Acorresponding%20training%20pose%20into%20the%20target%20pose%20sequences%20to%20enhance%0Afaithfulness%20through%20a%20trained%20temporal%20attention%20module.%20Furthermore%2C%20to%0Aalleviate%20the%20face%20and%20hand%20degradation%20resulting%20from%20discrepancies%20between%0Aposes%20of%20training%20videos%20and%20inference%20poses%2C%20we%20implement%20simple%20latent%0Aediting%20through%20an%20affine%20transformation%20matrix%20involving%20facial%20and%20hand%0Alandmarks.%20Extensive%20experiments%20on%20several%20datasets%20demonstrate%20that%0APoseCrafter%20achieves%20superior%20results%20to%20baselines%20pre-trained%20on%20a%20vast%0Acollection%20of%20videos%20under%208%20commonly%20used%20metrics.%20Besides%2C%20PoseCrafter%20can%0Afollow%20poses%20from%20different%20individuals%20or%20artificial%20edits%20and%20simultaneously%0Aretain%20the%20human%20identity%20in%20an%20open-domain%20training%20video.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//ml-gsai.github.io/PoseCrafter-demo/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14582v2&entry.124074799=Read"},
{"title": "GroundGrid:LiDAR Point Cloud Ground Segmentation and Terrain Estimation", "author": "Nicolai Steinke and Daniel G\u00f6hring and Ra\u00f9l Rojas", "abstract": "  The precise point cloud ground segmentation is a crucial prerequisite of\nvirtually all perception tasks for LiDAR sensors in autonomous vehicles.\nEspecially the clustering and extraction of objects from a point cloud usually\nrelies on an accurate removal of ground points. The correct estimation of the\nsurrounding terrain is important for aspects of the drivability of a surface,\npath planning, and obstacle prediction. In this article, we propose our system\nGroundGrid which relies on 2D elevation maps to solve the terrain estimation\nand point cloud ground segmentation problems. We evaluate the ground\nsegmentation and terrain estimation performance of GroundGrid and compare it to\nother state-of-the-art methods using the SemanticKITTI dataset and a novel\nevaluation method relying on airborne LiDAR scanning. The results show that\nGroundGrid is capable of outperforming other state-of-the-art systems with an\naverage IoU of 94.78% while maintaining a high run-time performance of 171Hz.\nThe source code is available at https://github.com/dcmlr/groundgrid\n", "link": "http://arxiv.org/abs/2405.15664v1", "date": "2024-05-24", "relevancy": 2.6617, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5363}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5363}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GroundGrid%3ALiDAR%20Point%20Cloud%20Ground%20Segmentation%20and%20Terrain%20Estimation&body=Title%3A%20GroundGrid%3ALiDAR%20Point%20Cloud%20Ground%20Segmentation%20and%20Terrain%20Estimation%0AAuthor%3A%20Nicolai%20Steinke%20and%20Daniel%20G%C3%B6hring%20and%20Ra%C3%B9l%20Rojas%0AAbstract%3A%20%20%20The%20precise%20point%20cloud%20ground%20segmentation%20is%20a%20crucial%20prerequisite%20of%0Avirtually%20all%20perception%20tasks%20for%20LiDAR%20sensors%20in%20autonomous%20vehicles.%0AEspecially%20the%20clustering%20and%20extraction%20of%20objects%20from%20a%20point%20cloud%20usually%0Arelies%20on%20an%20accurate%20removal%20of%20ground%20points.%20The%20correct%20estimation%20of%20the%0Asurrounding%20terrain%20is%20important%20for%20aspects%20of%20the%20drivability%20of%20a%20surface%2C%0Apath%20planning%2C%20and%20obstacle%20prediction.%20In%20this%20article%2C%20we%20propose%20our%20system%0AGroundGrid%20which%20relies%20on%202D%20elevation%20maps%20to%20solve%20the%20terrain%20estimation%0Aand%20point%20cloud%20ground%20segmentation%20problems.%20We%20evaluate%20the%20ground%0Asegmentation%20and%20terrain%20estimation%20performance%20of%20GroundGrid%20and%20compare%20it%20to%0Aother%20state-of-the-art%20methods%20using%20the%20SemanticKITTI%20dataset%20and%20a%20novel%0Aevaluation%20method%20relying%20on%20airborne%20LiDAR%20scanning.%20The%20results%20show%20that%0AGroundGrid%20is%20capable%20of%20outperforming%20other%20state-of-the-art%20systems%20with%20an%0Aaverage%20IoU%20of%2094.78%25%20while%20maintaining%20a%20high%20run-time%20performance%20of%20171Hz.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/dcmlr/groundgrid%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroundGrid%253ALiDAR%2520Point%2520Cloud%2520Ground%2520Segmentation%2520and%2520Terrain%2520Estimation%26entry.906535625%3DNicolai%2520Steinke%2520and%2520Daniel%2520G%25C3%25B6hring%2520and%2520Ra%25C3%25B9l%2520Rojas%26entry.1292438233%3D%2520%2520The%2520precise%2520point%2520cloud%2520ground%2520segmentation%2520is%2520a%2520crucial%2520prerequisite%2520of%250Avirtually%2520all%2520perception%2520tasks%2520for%2520LiDAR%2520sensors%2520in%2520autonomous%2520vehicles.%250AEspecially%2520the%2520clustering%2520and%2520extraction%2520of%2520objects%2520from%2520a%2520point%2520cloud%2520usually%250Arelies%2520on%2520an%2520accurate%2520removal%2520of%2520ground%2520points.%2520The%2520correct%2520estimation%2520of%2520the%250Asurrounding%2520terrain%2520is%2520important%2520for%2520aspects%2520of%2520the%2520drivability%2520of%2520a%2520surface%252C%250Apath%2520planning%252C%2520and%2520obstacle%2520prediction.%2520In%2520this%2520article%252C%2520we%2520propose%2520our%2520system%250AGroundGrid%2520which%2520relies%2520on%25202D%2520elevation%2520maps%2520to%2520solve%2520the%2520terrain%2520estimation%250Aand%2520point%2520cloud%2520ground%2520segmentation%2520problems.%2520We%2520evaluate%2520the%2520ground%250Asegmentation%2520and%2520terrain%2520estimation%2520performance%2520of%2520GroundGrid%2520and%2520compare%2520it%2520to%250Aother%2520state-of-the-art%2520methods%2520using%2520the%2520SemanticKITTI%2520dataset%2520and%2520a%2520novel%250Aevaluation%2520method%2520relying%2520on%2520airborne%2520LiDAR%2520scanning.%2520The%2520results%2520show%2520that%250AGroundGrid%2520is%2520capable%2520of%2520outperforming%2520other%2520state-of-the-art%2520systems%2520with%2520an%250Aaverage%2520IoU%2520of%252094.78%2525%2520while%2520maintaining%2520a%2520high%2520run-time%2520performance%2520of%2520171Hz.%250AThe%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/dcmlr/groundgrid%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroundGrid%3ALiDAR%20Point%20Cloud%20Ground%20Segmentation%20and%20Terrain%20Estimation&entry.906535625=Nicolai%20Steinke%20and%20Daniel%20G%C3%B6hring%20and%20Ra%C3%B9l%20Rojas&entry.1292438233=%20%20The%20precise%20point%20cloud%20ground%20segmentation%20is%20a%20crucial%20prerequisite%20of%0Avirtually%20all%20perception%20tasks%20for%20LiDAR%20sensors%20in%20autonomous%20vehicles.%0AEspecially%20the%20clustering%20and%20extraction%20of%20objects%20from%20a%20point%20cloud%20usually%0Arelies%20on%20an%20accurate%20removal%20of%20ground%20points.%20The%20correct%20estimation%20of%20the%0Asurrounding%20terrain%20is%20important%20for%20aspects%20of%20the%20drivability%20of%20a%20surface%2C%0Apath%20planning%2C%20and%20obstacle%20prediction.%20In%20this%20article%2C%20we%20propose%20our%20system%0AGroundGrid%20which%20relies%20on%202D%20elevation%20maps%20to%20solve%20the%20terrain%20estimation%0Aand%20point%20cloud%20ground%20segmentation%20problems.%20We%20evaluate%20the%20ground%0Asegmentation%20and%20terrain%20estimation%20performance%20of%20GroundGrid%20and%20compare%20it%20to%0Aother%20state-of-the-art%20methods%20using%20the%20SemanticKITTI%20dataset%20and%20a%20novel%0Aevaluation%20method%20relying%20on%20airborne%20LiDAR%20scanning.%20The%20results%20show%20that%0AGroundGrid%20is%20capable%20of%20outperforming%20other%20state-of-the-art%20systems%20with%20an%0Aaverage%20IoU%20of%2094.78%25%20while%20maintaining%20a%20high%20run-time%20performance%20of%20171Hz.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/dcmlr/groundgrid%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15664v1&entry.124074799=Read"},
{"title": "Volumetric Primitives for Modeling and Rendering Scattering and Emissive\n  Media", "author": "Jorge Condor and Sebastien Speierer and Lukas Bode and Aljaz Bozic and Simon Green and Piotr Didyk and Adrian Jarabo", "abstract": "  We propose a volumetric representation based on primitives to model\nscattering and emissive media. Accurate scene representations enabling\nefficient rendering are essential for many computer graphics applications.\nGeneral and unified representations that can handle surface and volume-based\nrepresentations simultaneously, allowing for physically accurate modeling,\nremain a research challenge. Inspired by recent methods for scene\nreconstruction that leverage mixtures of 3D Gaussians to model radiance fields,\nwe formalize and generalize the modeling of scattering and emissive media using\nmixtures of simple kernel-based volumetric primitives. We introduce closed-form\nsolutions for transmittance and free-flight distance sampling for 3D Gaussian\nkernels, and propose several optimizations to use our method efficiently within\nany off-the-shelf volumetric path tracer by leveraging ray tracing for\nefficiently querying the medium. We demonstrate our method as an alternative to\nother forms of volume modeling (e.g. voxel grid-based representations) for\nforward and inverse rendering of scattering media. Furthermore, we adapt our\nmethod to the problem of radiance field optimization and rendering, and\ndemonstrate comparable performance to the state of the art, while providing\nadditional flexibility in terms of performance and usability.\n", "link": "http://arxiv.org/abs/2405.15425v1", "date": "2024-05-24", "relevancy": 2.6483, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5352}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5352}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Volumetric%20Primitives%20for%20Modeling%20and%20Rendering%20Scattering%20and%20Emissive%0A%20%20Media&body=Title%3A%20Volumetric%20Primitives%20for%20Modeling%20and%20Rendering%20Scattering%20and%20Emissive%0A%20%20Media%0AAuthor%3A%20Jorge%20Condor%20and%20Sebastien%20Speierer%20and%20Lukas%20Bode%20and%20Aljaz%20Bozic%20and%20Simon%20Green%20and%20Piotr%20Didyk%20and%20Adrian%20Jarabo%0AAbstract%3A%20%20%20We%20propose%20a%20volumetric%20representation%20based%20on%20primitives%20to%20model%0Ascattering%20and%20emissive%20media.%20Accurate%20scene%20representations%20enabling%0Aefficient%20rendering%20are%20essential%20for%20many%20computer%20graphics%20applications.%0AGeneral%20and%20unified%20representations%20that%20can%20handle%20surface%20and%20volume-based%0Arepresentations%20simultaneously%2C%20allowing%20for%20physically%20accurate%20modeling%2C%0Aremain%20a%20research%20challenge.%20Inspired%20by%20recent%20methods%20for%20scene%0Areconstruction%20that%20leverage%20mixtures%20of%203D%20Gaussians%20to%20model%20radiance%20fields%2C%0Awe%20formalize%20and%20generalize%20the%20modeling%20of%20scattering%20and%20emissive%20media%20using%0Amixtures%20of%20simple%20kernel-based%20volumetric%20primitives.%20We%20introduce%20closed-form%0Asolutions%20for%20transmittance%20and%20free-flight%20distance%20sampling%20for%203D%20Gaussian%0Akernels%2C%20and%20propose%20several%20optimizations%20to%20use%20our%20method%20efficiently%20within%0Aany%20off-the-shelf%20volumetric%20path%20tracer%20by%20leveraging%20ray%20tracing%20for%0Aefficiently%20querying%20the%20medium.%20We%20demonstrate%20our%20method%20as%20an%20alternative%20to%0Aother%20forms%20of%20volume%20modeling%20%28e.g.%20voxel%20grid-based%20representations%29%20for%0Aforward%20and%20inverse%20rendering%20of%20scattering%20media.%20Furthermore%2C%20we%20adapt%20our%0Amethod%20to%20the%20problem%20of%20radiance%20field%20optimization%20and%20rendering%2C%20and%0Ademonstrate%20comparable%20performance%20to%20the%20state%20of%20the%20art%2C%20while%20providing%0Aadditional%20flexibility%20in%20terms%20of%20performance%20and%20usability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolumetric%2520Primitives%2520for%2520Modeling%2520and%2520Rendering%2520Scattering%2520and%2520Emissive%250A%2520%2520Media%26entry.906535625%3DJorge%2520Condor%2520and%2520Sebastien%2520Speierer%2520and%2520Lukas%2520Bode%2520and%2520Aljaz%2520Bozic%2520and%2520Simon%2520Green%2520and%2520Piotr%2520Didyk%2520and%2520Adrian%2520Jarabo%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520volumetric%2520representation%2520based%2520on%2520primitives%2520to%2520model%250Ascattering%2520and%2520emissive%2520media.%2520Accurate%2520scene%2520representations%2520enabling%250Aefficient%2520rendering%2520are%2520essential%2520for%2520many%2520computer%2520graphics%2520applications.%250AGeneral%2520and%2520unified%2520representations%2520that%2520can%2520handle%2520surface%2520and%2520volume-based%250Arepresentations%2520simultaneously%252C%2520allowing%2520for%2520physically%2520accurate%2520modeling%252C%250Aremain%2520a%2520research%2520challenge.%2520Inspired%2520by%2520recent%2520methods%2520for%2520scene%250Areconstruction%2520that%2520leverage%2520mixtures%2520of%25203D%2520Gaussians%2520to%2520model%2520radiance%2520fields%252C%250Awe%2520formalize%2520and%2520generalize%2520the%2520modeling%2520of%2520scattering%2520and%2520emissive%2520media%2520using%250Amixtures%2520of%2520simple%2520kernel-based%2520volumetric%2520primitives.%2520We%2520introduce%2520closed-form%250Asolutions%2520for%2520transmittance%2520and%2520free-flight%2520distance%2520sampling%2520for%25203D%2520Gaussian%250Akernels%252C%2520and%2520propose%2520several%2520optimizations%2520to%2520use%2520our%2520method%2520efficiently%2520within%250Aany%2520off-the-shelf%2520volumetric%2520path%2520tracer%2520by%2520leveraging%2520ray%2520tracing%2520for%250Aefficiently%2520querying%2520the%2520medium.%2520We%2520demonstrate%2520our%2520method%2520as%2520an%2520alternative%2520to%250Aother%2520forms%2520of%2520volume%2520modeling%2520%2528e.g.%2520voxel%2520grid-based%2520representations%2529%2520for%250Aforward%2520and%2520inverse%2520rendering%2520of%2520scattering%2520media.%2520Furthermore%252C%2520we%2520adapt%2520our%250Amethod%2520to%2520the%2520problem%2520of%2520radiance%2520field%2520optimization%2520and%2520rendering%252C%2520and%250Ademonstrate%2520comparable%2520performance%2520to%2520the%2520state%2520of%2520the%2520art%252C%2520while%2520providing%250Aadditional%2520flexibility%2520in%2520terms%2520of%2520performance%2520and%2520usability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Volumetric%20Primitives%20for%20Modeling%20and%20Rendering%20Scattering%20and%20Emissive%0A%20%20Media&entry.906535625=Jorge%20Condor%20and%20Sebastien%20Speierer%20and%20Lukas%20Bode%20and%20Aljaz%20Bozic%20and%20Simon%20Green%20and%20Piotr%20Didyk%20and%20Adrian%20Jarabo&entry.1292438233=%20%20We%20propose%20a%20volumetric%20representation%20based%20on%20primitives%20to%20model%0Ascattering%20and%20emissive%20media.%20Accurate%20scene%20representations%20enabling%0Aefficient%20rendering%20are%20essential%20for%20many%20computer%20graphics%20applications.%0AGeneral%20and%20unified%20representations%20that%20can%20handle%20surface%20and%20volume-based%0Arepresentations%20simultaneously%2C%20allowing%20for%20physically%20accurate%20modeling%2C%0Aremain%20a%20research%20challenge.%20Inspired%20by%20recent%20methods%20for%20scene%0Areconstruction%20that%20leverage%20mixtures%20of%203D%20Gaussians%20to%20model%20radiance%20fields%2C%0Awe%20formalize%20and%20generalize%20the%20modeling%20of%20scattering%20and%20emissive%20media%20using%0Amixtures%20of%20simple%20kernel-based%20volumetric%20primitives.%20We%20introduce%20closed-form%0Asolutions%20for%20transmittance%20and%20free-flight%20distance%20sampling%20for%203D%20Gaussian%0Akernels%2C%20and%20propose%20several%20optimizations%20to%20use%20our%20method%20efficiently%20within%0Aany%20off-the-shelf%20volumetric%20path%20tracer%20by%20leveraging%20ray%20tracing%20for%0Aefficiently%20querying%20the%20medium.%20We%20demonstrate%20our%20method%20as%20an%20alternative%20to%0Aother%20forms%20of%20volume%20modeling%20%28e.g.%20voxel%20grid-based%20representations%29%20for%0Aforward%20and%20inverse%20rendering%20of%20scattering%20media.%20Furthermore%2C%20we%20adapt%20our%0Amethod%20to%20the%20problem%20of%20radiance%20field%20optimization%20and%20rendering%2C%20and%0Ademonstrate%20comparable%20performance%20to%20the%20state%20of%20the%20art%2C%20while%20providing%0Aadditional%20flexibility%20in%20terms%20of%20performance%20and%20usability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15425v1&entry.124074799=Read"},
{"title": "PoinTramba: A Hybrid Transformer-Mamba Framework for Point Cloud\n  Analysis", "author": "Zicheng Wang and Zhenghao Chen and Yiming Wu and Zhen Zhao and Luping Zhou and Dong Xu", "abstract": "  Point cloud analysis has seen substantial advancements due to deep learning,\nalthough previous Transformer-based methods excel at modeling long-range\ndependencies on this task, their computational demands are substantial.\nConversely, the Mamba offers greater efficiency but shows limited potential\ncompared with Transformer-based methods. In this study, we introduce\nPoinTramba, a pioneering hybrid framework that synergies the analytical power\nof Transformer with the remarkable computational efficiency of Mamba for\nenhanced point cloud analysis. Specifically, our approach first segments point\nclouds into groups, where the Transformer meticulously captures intricate\nintra-group dependencies and produces group embeddings, whose inter-group\nrelationships will be simultaneously and adeptly captured by efficient Mamba\narchitecture, ensuring comprehensive analysis. Unlike previous Mamba\napproaches, we introduce a bi-directional importance-aware ordering (BIO)\nstrategy to tackle the challenges of random ordering effects. This innovative\nstrategy intelligently reorders group embeddings based on their calculated\nimportance scores, significantly enhancing Mamba's performance and optimizing\nthe overall analytical process. Our framework achieves a superior balance\nbetween computational efficiency and analytical performance by seamlessly\nintegrating these advanced techniques, marking a substantial leap forward in\npoint cloud analysis. Extensive experiments on datasets such as ScanObjectNN,\nModelNet40, and ShapeNetPart demonstrate the effectiveness of our approach,\nestablishing a new state-of-the-art analysis benchmark on point cloud\nrecognition. For the first time, this paradigm leverages the combined strengths\nof both Transformer and Mamba architectures, facilitating a new standard in the\nfield. The code is available at https://github.com/xiaoyao3302/PoinTramba.\n", "link": "http://arxiv.org/abs/2405.15463v1", "date": "2024-05-24", "relevancy": 2.6324, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5354}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.534}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoinTramba%3A%20A%20Hybrid%20Transformer-Mamba%20Framework%20for%20Point%20Cloud%0A%20%20Analysis&body=Title%3A%20PoinTramba%3A%20A%20Hybrid%20Transformer-Mamba%20Framework%20for%20Point%20Cloud%0A%20%20Analysis%0AAuthor%3A%20Zicheng%20Wang%20and%20Zhenghao%20Chen%20and%20Yiming%20Wu%20and%20Zhen%20Zhao%20and%20Luping%20Zhou%20and%20Dong%20Xu%0AAbstract%3A%20%20%20Point%20cloud%20analysis%20has%20seen%20substantial%20advancements%20due%20to%20deep%20learning%2C%0Aalthough%20previous%20Transformer-based%20methods%20excel%20at%20modeling%20long-range%0Adependencies%20on%20this%20task%2C%20their%20computational%20demands%20are%20substantial.%0AConversely%2C%20the%20Mamba%20offers%20greater%20efficiency%20but%20shows%20limited%20potential%0Acompared%20with%20Transformer-based%20methods.%20In%20this%20study%2C%20we%20introduce%0APoinTramba%2C%20a%20pioneering%20hybrid%20framework%20that%20synergies%20the%20analytical%20power%0Aof%20Transformer%20with%20the%20remarkable%20computational%20efficiency%20of%20Mamba%20for%0Aenhanced%20point%20cloud%20analysis.%20Specifically%2C%20our%20approach%20first%20segments%20point%0Aclouds%20into%20groups%2C%20where%20the%20Transformer%20meticulously%20captures%20intricate%0Aintra-group%20dependencies%20and%20produces%20group%20embeddings%2C%20whose%20inter-group%0Arelationships%20will%20be%20simultaneously%20and%20adeptly%20captured%20by%20efficient%20Mamba%0Aarchitecture%2C%20ensuring%20comprehensive%20analysis.%20Unlike%20previous%20Mamba%0Aapproaches%2C%20we%20introduce%20a%20bi-directional%20importance-aware%20ordering%20%28BIO%29%0Astrategy%20to%20tackle%20the%20challenges%20of%20random%20ordering%20effects.%20This%20innovative%0Astrategy%20intelligently%20reorders%20group%20embeddings%20based%20on%20their%20calculated%0Aimportance%20scores%2C%20significantly%20enhancing%20Mamba%27s%20performance%20and%20optimizing%0Athe%20overall%20analytical%20process.%20Our%20framework%20achieves%20a%20superior%20balance%0Abetween%20computational%20efficiency%20and%20analytical%20performance%20by%20seamlessly%0Aintegrating%20these%20advanced%20techniques%2C%20marking%20a%20substantial%20leap%20forward%20in%0Apoint%20cloud%20analysis.%20Extensive%20experiments%20on%20datasets%20such%20as%20ScanObjectNN%2C%0AModelNet40%2C%20and%20ShapeNetPart%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Aestablishing%20a%20new%20state-of-the-art%20analysis%20benchmark%20on%20point%20cloud%0Arecognition.%20For%20the%20first%20time%2C%20this%20paradigm%20leverages%20the%20combined%20strengths%0Aof%20both%20Transformer%20and%20Mamba%20architectures%2C%20facilitating%20a%20new%20standard%20in%20the%0Afield.%20The%20code%20is%20available%20at%20https%3A//github.com/xiaoyao3302/PoinTramba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoinTramba%253A%2520A%2520Hybrid%2520Transformer-Mamba%2520Framework%2520for%2520Point%2520Cloud%250A%2520%2520Analysis%26entry.906535625%3DZicheng%2520Wang%2520and%2520Zhenghao%2520Chen%2520and%2520Yiming%2520Wu%2520and%2520Zhen%2520Zhao%2520and%2520Luping%2520Zhou%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520Point%2520cloud%2520analysis%2520has%2520seen%2520substantial%2520advancements%2520due%2520to%2520deep%2520learning%252C%250Aalthough%2520previous%2520Transformer-based%2520methods%2520excel%2520at%2520modeling%2520long-range%250Adependencies%2520on%2520this%2520task%252C%2520their%2520computational%2520demands%2520are%2520substantial.%250AConversely%252C%2520the%2520Mamba%2520offers%2520greater%2520efficiency%2520but%2520shows%2520limited%2520potential%250Acompared%2520with%2520Transformer-based%2520methods.%2520In%2520this%2520study%252C%2520we%2520introduce%250APoinTramba%252C%2520a%2520pioneering%2520hybrid%2520framework%2520that%2520synergies%2520the%2520analytical%2520power%250Aof%2520Transformer%2520with%2520the%2520remarkable%2520computational%2520efficiency%2520of%2520Mamba%2520for%250Aenhanced%2520point%2520cloud%2520analysis.%2520Specifically%252C%2520our%2520approach%2520first%2520segments%2520point%250Aclouds%2520into%2520groups%252C%2520where%2520the%2520Transformer%2520meticulously%2520captures%2520intricate%250Aintra-group%2520dependencies%2520and%2520produces%2520group%2520embeddings%252C%2520whose%2520inter-group%250Arelationships%2520will%2520be%2520simultaneously%2520and%2520adeptly%2520captured%2520by%2520efficient%2520Mamba%250Aarchitecture%252C%2520ensuring%2520comprehensive%2520analysis.%2520Unlike%2520previous%2520Mamba%250Aapproaches%252C%2520we%2520introduce%2520a%2520bi-directional%2520importance-aware%2520ordering%2520%2528BIO%2529%250Astrategy%2520to%2520tackle%2520the%2520challenges%2520of%2520random%2520ordering%2520effects.%2520This%2520innovative%250Astrategy%2520intelligently%2520reorders%2520group%2520embeddings%2520based%2520on%2520their%2520calculated%250Aimportance%2520scores%252C%2520significantly%2520enhancing%2520Mamba%2527s%2520performance%2520and%2520optimizing%250Athe%2520overall%2520analytical%2520process.%2520Our%2520framework%2520achieves%2520a%2520superior%2520balance%250Abetween%2520computational%2520efficiency%2520and%2520analytical%2520performance%2520by%2520seamlessly%250Aintegrating%2520these%2520advanced%2520techniques%252C%2520marking%2520a%2520substantial%2520leap%2520forward%2520in%250Apoint%2520cloud%2520analysis.%2520Extensive%2520experiments%2520on%2520datasets%2520such%2520as%2520ScanObjectNN%252C%250AModelNet40%252C%2520and%2520ShapeNetPart%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%250Aestablishing%2520a%2520new%2520state-of-the-art%2520analysis%2520benchmark%2520on%2520point%2520cloud%250Arecognition.%2520For%2520the%2520first%2520time%252C%2520this%2520paradigm%2520leverages%2520the%2520combined%2520strengths%250Aof%2520both%2520Transformer%2520and%2520Mamba%2520architectures%252C%2520facilitating%2520a%2520new%2520standard%2520in%2520the%250Afield.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/xiaoyao3302/PoinTramba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoinTramba%3A%20A%20Hybrid%20Transformer-Mamba%20Framework%20for%20Point%20Cloud%0A%20%20Analysis&entry.906535625=Zicheng%20Wang%20and%20Zhenghao%20Chen%20and%20Yiming%20Wu%20and%20Zhen%20Zhao%20and%20Luping%20Zhou%20and%20Dong%20Xu&entry.1292438233=%20%20Point%20cloud%20analysis%20has%20seen%20substantial%20advancements%20due%20to%20deep%20learning%2C%0Aalthough%20previous%20Transformer-based%20methods%20excel%20at%20modeling%20long-range%0Adependencies%20on%20this%20task%2C%20their%20computational%20demands%20are%20substantial.%0AConversely%2C%20the%20Mamba%20offers%20greater%20efficiency%20but%20shows%20limited%20potential%0Acompared%20with%20Transformer-based%20methods.%20In%20this%20study%2C%20we%20introduce%0APoinTramba%2C%20a%20pioneering%20hybrid%20framework%20that%20synergies%20the%20analytical%20power%0Aof%20Transformer%20with%20the%20remarkable%20computational%20efficiency%20of%20Mamba%20for%0Aenhanced%20point%20cloud%20analysis.%20Specifically%2C%20our%20approach%20first%20segments%20point%0Aclouds%20into%20groups%2C%20where%20the%20Transformer%20meticulously%20captures%20intricate%0Aintra-group%20dependencies%20and%20produces%20group%20embeddings%2C%20whose%20inter-group%0Arelationships%20will%20be%20simultaneously%20and%20adeptly%20captured%20by%20efficient%20Mamba%0Aarchitecture%2C%20ensuring%20comprehensive%20analysis.%20Unlike%20previous%20Mamba%0Aapproaches%2C%20we%20introduce%20a%20bi-directional%20importance-aware%20ordering%20%28BIO%29%0Astrategy%20to%20tackle%20the%20challenges%20of%20random%20ordering%20effects.%20This%20innovative%0Astrategy%20intelligently%20reorders%20group%20embeddings%20based%20on%20their%20calculated%0Aimportance%20scores%2C%20significantly%20enhancing%20Mamba%27s%20performance%20and%20optimizing%0Athe%20overall%20analytical%20process.%20Our%20framework%20achieves%20a%20superior%20balance%0Abetween%20computational%20efficiency%20and%20analytical%20performance%20by%20seamlessly%0Aintegrating%20these%20advanced%20techniques%2C%20marking%20a%20substantial%20leap%20forward%20in%0Apoint%20cloud%20analysis.%20Extensive%20experiments%20on%20datasets%20such%20as%20ScanObjectNN%2C%0AModelNet40%2C%20and%20ShapeNetPart%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Aestablishing%20a%20new%20state-of-the-art%20analysis%20benchmark%20on%20point%20cloud%0Arecognition.%20For%20the%20first%20time%2C%20this%20paradigm%20leverages%20the%20combined%20strengths%0Aof%20both%20Transformer%20and%20Mamba%20architectures%2C%20facilitating%20a%20new%20standard%20in%20the%0Afield.%20The%20code%20is%20available%20at%20https%3A//github.com/xiaoyao3302/PoinTramba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15463v1&entry.124074799=Read"},
{"title": "Automatic Data Curation for Self-Supervised Learning: A Clustering-Based\n  Approach", "author": "Huy V. Vo and Vasil Khalidov and Timoth\u00e9e Darcet and Th\u00e9o Moutakanni and Nikita Smetanin and Marc Szafraniec and Hugo Touvron and Camille Couprie and Maxime Oquab and Armand Joulin and Herv\u00e9 J\u00e9gou and Patrick Labatut and Piotr Bojanowski", "abstract": "  Self-supervised features are the cornerstone of modern machine learning\nsystems. They are typically pre-trained on data collections whose construction\nand curation typically require extensive human effort. This manual process has\nsome limitations similar to those encountered in supervised learning, e.g., the\ncrowd-sourced selection of data is costly and time-consuming, preventing\nscaling the dataset size. In this work, we consider the problem of automatic\ncuration of high-quality datasets for self-supervised pre-training. We posit\nthat such datasets should be large, diverse and balanced, and propose a\nclustering-based approach for building ones satisfying all these criteria. Our\nmethod involves successive and hierarchical applications of $k$-means on a\nlarge and diverse data repository to obtain clusters that distribute uniformly\namong data concepts, followed by a hierarchical, balanced sampling step from\nthese clusters. Extensive experiments on three different data domains including\nweb-based images, satellite images and text show that features trained on our\nautomatically curated datasets outperform those trained on uncurated data while\nbeing on par or better than ones trained on manually curated data.\n", "link": "http://arxiv.org/abs/2405.15613v1", "date": "2024-05-24", "relevancy": 2.6135, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5662}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5041}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Data%20Curation%20for%20Self-Supervised%20Learning%3A%20A%20Clustering-Based%0A%20%20Approach&body=Title%3A%20Automatic%20Data%20Curation%20for%20Self-Supervised%20Learning%3A%20A%20Clustering-Based%0A%20%20Approach%0AAuthor%3A%20Huy%20V.%20Vo%20and%20Vasil%20Khalidov%20and%20Timoth%C3%A9e%20Darcet%20and%20Th%C3%A9o%20Moutakanni%20and%20Nikita%20Smetanin%20and%20Marc%20Szafraniec%20and%20Hugo%20Touvron%20and%20Camille%20Couprie%20and%20Maxime%20Oquab%20and%20Armand%20Joulin%20and%20Herv%C3%A9%20J%C3%A9gou%20and%20Patrick%20Labatut%20and%20Piotr%20Bojanowski%0AAbstract%3A%20%20%20Self-supervised%20features%20are%20the%20cornerstone%20of%20modern%20machine%20learning%0Asystems.%20They%20are%20typically%20pre-trained%20on%20data%20collections%20whose%20construction%0Aand%20curation%20typically%20require%20extensive%20human%20effort.%20This%20manual%20process%20has%0Asome%20limitations%20similar%20to%20those%20encountered%20in%20supervised%20learning%2C%20e.g.%2C%20the%0Acrowd-sourced%20selection%20of%20data%20is%20costly%20and%20time-consuming%2C%20preventing%0Ascaling%20the%20dataset%20size.%20In%20this%20work%2C%20we%20consider%20the%20problem%20of%20automatic%0Acuration%20of%20high-quality%20datasets%20for%20self-supervised%20pre-training.%20We%20posit%0Athat%20such%20datasets%20should%20be%20large%2C%20diverse%20and%20balanced%2C%20and%20propose%20a%0Aclustering-based%20approach%20for%20building%20ones%20satisfying%20all%20these%20criteria.%20Our%0Amethod%20involves%20successive%20and%20hierarchical%20applications%20of%20%24k%24-means%20on%20a%0Alarge%20and%20diverse%20data%20repository%20to%20obtain%20clusters%20that%20distribute%20uniformly%0Aamong%20data%20concepts%2C%20followed%20by%20a%20hierarchical%2C%20balanced%20sampling%20step%20from%0Athese%20clusters.%20Extensive%20experiments%20on%20three%20different%20data%20domains%20including%0Aweb-based%20images%2C%20satellite%20images%20and%20text%20show%20that%20features%20trained%20on%20our%0Aautomatically%20curated%20datasets%20outperform%20those%20trained%20on%20uncurated%20data%20while%0Abeing%20on%20par%20or%20better%20than%20ones%20trained%20on%20manually%20curated%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Data%2520Curation%2520for%2520Self-Supervised%2520Learning%253A%2520A%2520Clustering-Based%250A%2520%2520Approach%26entry.906535625%3DHuy%2520V.%2520Vo%2520and%2520Vasil%2520Khalidov%2520and%2520Timoth%25C3%25A9e%2520Darcet%2520and%2520Th%25C3%25A9o%2520Moutakanni%2520and%2520Nikita%2520Smetanin%2520and%2520Marc%2520Szafraniec%2520and%2520Hugo%2520Touvron%2520and%2520Camille%2520Couprie%2520and%2520Maxime%2520Oquab%2520and%2520Armand%2520Joulin%2520and%2520Herv%25C3%25A9%2520J%25C3%25A9gou%2520and%2520Patrick%2520Labatut%2520and%2520Piotr%2520Bojanowski%26entry.1292438233%3D%2520%2520Self-supervised%2520features%2520are%2520the%2520cornerstone%2520of%2520modern%2520machine%2520learning%250Asystems.%2520They%2520are%2520typically%2520pre-trained%2520on%2520data%2520collections%2520whose%2520construction%250Aand%2520curation%2520typically%2520require%2520extensive%2520human%2520effort.%2520This%2520manual%2520process%2520has%250Asome%2520limitations%2520similar%2520to%2520those%2520encountered%2520in%2520supervised%2520learning%252C%2520e.g.%252C%2520the%250Acrowd-sourced%2520selection%2520of%2520data%2520is%2520costly%2520and%2520time-consuming%252C%2520preventing%250Ascaling%2520the%2520dataset%2520size.%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520problem%2520of%2520automatic%250Acuration%2520of%2520high-quality%2520datasets%2520for%2520self-supervised%2520pre-training.%2520We%2520posit%250Athat%2520such%2520datasets%2520should%2520be%2520large%252C%2520diverse%2520and%2520balanced%252C%2520and%2520propose%2520a%250Aclustering-based%2520approach%2520for%2520building%2520ones%2520satisfying%2520all%2520these%2520criteria.%2520Our%250Amethod%2520involves%2520successive%2520and%2520hierarchical%2520applications%2520of%2520%2524k%2524-means%2520on%2520a%250Alarge%2520and%2520diverse%2520data%2520repository%2520to%2520obtain%2520clusters%2520that%2520distribute%2520uniformly%250Aamong%2520data%2520concepts%252C%2520followed%2520by%2520a%2520hierarchical%252C%2520balanced%2520sampling%2520step%2520from%250Athese%2520clusters.%2520Extensive%2520experiments%2520on%2520three%2520different%2520data%2520domains%2520including%250Aweb-based%2520images%252C%2520satellite%2520images%2520and%2520text%2520show%2520that%2520features%2520trained%2520on%2520our%250Aautomatically%2520curated%2520datasets%2520outperform%2520those%2520trained%2520on%2520uncurated%2520data%2520while%250Abeing%2520on%2520par%2520or%2520better%2520than%2520ones%2520trained%2520on%2520manually%2520curated%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Data%20Curation%20for%20Self-Supervised%20Learning%3A%20A%20Clustering-Based%0A%20%20Approach&entry.906535625=Huy%20V.%20Vo%20and%20Vasil%20Khalidov%20and%20Timoth%C3%A9e%20Darcet%20and%20Th%C3%A9o%20Moutakanni%20and%20Nikita%20Smetanin%20and%20Marc%20Szafraniec%20and%20Hugo%20Touvron%20and%20Camille%20Couprie%20and%20Maxime%20Oquab%20and%20Armand%20Joulin%20and%20Herv%C3%A9%20J%C3%A9gou%20and%20Patrick%20Labatut%20and%20Piotr%20Bojanowski&entry.1292438233=%20%20Self-supervised%20features%20are%20the%20cornerstone%20of%20modern%20machine%20learning%0Asystems.%20They%20are%20typically%20pre-trained%20on%20data%20collections%20whose%20construction%0Aand%20curation%20typically%20require%20extensive%20human%20effort.%20This%20manual%20process%20has%0Asome%20limitations%20similar%20to%20those%20encountered%20in%20supervised%20learning%2C%20e.g.%2C%20the%0Acrowd-sourced%20selection%20of%20data%20is%20costly%20and%20time-consuming%2C%20preventing%0Ascaling%20the%20dataset%20size.%20In%20this%20work%2C%20we%20consider%20the%20problem%20of%20automatic%0Acuration%20of%20high-quality%20datasets%20for%20self-supervised%20pre-training.%20We%20posit%0Athat%20such%20datasets%20should%20be%20large%2C%20diverse%20and%20balanced%2C%20and%20propose%20a%0Aclustering-based%20approach%20for%20building%20ones%20satisfying%20all%20these%20criteria.%20Our%0Amethod%20involves%20successive%20and%20hierarchical%20applications%20of%20%24k%24-means%20on%20a%0Alarge%20and%20diverse%20data%20repository%20to%20obtain%20clusters%20that%20distribute%20uniformly%0Aamong%20data%20concepts%2C%20followed%20by%20a%20hierarchical%2C%20balanced%20sampling%20step%20from%0Athese%20clusters.%20Extensive%20experiments%20on%20three%20different%20data%20domains%20including%0Aweb-based%20images%2C%20satellite%20images%20and%20text%20show%20that%20features%20trained%20on%20our%0Aautomatically%20curated%20datasets%20outperform%20those%20trained%20on%20uncurated%20data%20while%0Abeing%20on%20par%20or%20better%20than%20ones%20trained%20on%20manually%20curated%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15613v1&entry.124074799=Read"},
{"title": "Out-of-Distribution Detection using Neural Activation Prior", "author": "Weilin Wan and Weizhong Zhang and Quan Zhou and Fan Yi and Cheng Jin", "abstract": "  Out-of-distribution detection (OOD) is a crucial technique for deploying\nmachine learning models in the real world to handle the unseen scenarios. In\nthis paper, we first propose a simple yet effective Neural Activation Prior\n(NAP) for OOD detection. Our neural activation prior is based on a key\nobservation that, for a channel before the global pooling layer of a fully\ntrained neural network, the probability of a few neurons being activated with a\nlarge response by an in-distribution (ID) sample is significantly higher than\nthat by an OOD sample. An intuitive explanation is that for a model fully\ntrained on ID dataset, each channel would play a role in detecting a certain\npattern in the ID dataset, and a few neurons can be activated with a large\nresponse when the pattern is detected in an input sample. Then, a new scoring\nfunction based on this prior is proposed to highlight the role of these\nstrongly activated neurons in OOD detection. Our approach is plug-and-play and\ndoes not lead to any performance degradation on ID data classification and\nrequires no extra training or statistics from training or external datasets.\nNotice that previous methods primarily rely on post-global-pooling features of\nthe neural networks, while the within-channel distribution information we\nleverage would be discarded by the global pooling operator. Consequently, our\nmethod is orthogonal to existing approaches and can be effectively combined\nwith them in various applications. Experimental results show that our method\nachieves the state-of-the-art performance on CIFAR benchmark and ImageNet\ndataset, which demonstrates the power of the proposed prior. Finally, we extend\nour method to Transformers and the experimental findings indicate that NAP can\nalso significantly enhance the performance of OOD detection on Transformers,\nthereby demonstrating the broad applicability of this prior knowledge.\n", "link": "http://arxiv.org/abs/2402.18162v4", "date": "2024-05-24", "relevancy": 2.5666, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5396}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5154}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out-of-Distribution%20Detection%20using%20Neural%20Activation%20Prior&body=Title%3A%20Out-of-Distribution%20Detection%20using%20Neural%20Activation%20Prior%0AAuthor%3A%20Weilin%20Wan%20and%20Weizhong%20Zhang%20and%20Quan%20Zhou%20and%20Fan%20Yi%20and%20Cheng%20Jin%0AAbstract%3A%20%20%20Out-of-distribution%20detection%20%28OOD%29%20is%20a%20crucial%20technique%20for%20deploying%0Amachine%20learning%20models%20in%20the%20real%20world%20to%20handle%20the%20unseen%20scenarios.%20In%0Athis%20paper%2C%20we%20first%20propose%20a%20simple%20yet%20effective%20Neural%20Activation%20Prior%0A%28NAP%29%20for%20OOD%20detection.%20Our%20neural%20activation%20prior%20is%20based%20on%20a%20key%0Aobservation%20that%2C%20for%20a%20channel%20before%20the%20global%20pooling%20layer%20of%20a%20fully%0Atrained%20neural%20network%2C%20the%20probability%20of%20a%20few%20neurons%20being%20activated%20with%20a%0Alarge%20response%20by%20an%20in-distribution%20%28ID%29%20sample%20is%20significantly%20higher%20than%0Athat%20by%20an%20OOD%20sample.%20An%20intuitive%20explanation%20is%20that%20for%20a%20model%20fully%0Atrained%20on%20ID%20dataset%2C%20each%20channel%20would%20play%20a%20role%20in%20detecting%20a%20certain%0Apattern%20in%20the%20ID%20dataset%2C%20and%20a%20few%20neurons%20can%20be%20activated%20with%20a%20large%0Aresponse%20when%20the%20pattern%20is%20detected%20in%20an%20input%20sample.%20Then%2C%20a%20new%20scoring%0Afunction%20based%20on%20this%20prior%20is%20proposed%20to%20highlight%20the%20role%20of%20these%0Astrongly%20activated%20neurons%20in%20OOD%20detection.%20Our%20approach%20is%20plug-and-play%20and%0Adoes%20not%20lead%20to%20any%20performance%20degradation%20on%20ID%20data%20classification%20and%0Arequires%20no%20extra%20training%20or%20statistics%20from%20training%20or%20external%20datasets.%0ANotice%20that%20previous%20methods%20primarily%20rely%20on%20post-global-pooling%20features%20of%0Athe%20neural%20networks%2C%20while%20the%20within-channel%20distribution%20information%20we%0Aleverage%20would%20be%20discarded%20by%20the%20global%20pooling%20operator.%20Consequently%2C%20our%0Amethod%20is%20orthogonal%20to%20existing%20approaches%20and%20can%20be%20effectively%20combined%0Awith%20them%20in%20various%20applications.%20Experimental%20results%20show%20that%20our%20method%0Aachieves%20the%20state-of-the-art%20performance%20on%20CIFAR%20benchmark%20and%20ImageNet%0Adataset%2C%20which%20demonstrates%20the%20power%20of%20the%20proposed%20prior.%20Finally%2C%20we%20extend%0Aour%20method%20to%20Transformers%20and%20the%20experimental%20findings%20indicate%20that%20NAP%20can%0Aalso%20significantly%20enhance%20the%20performance%20of%20OOD%20detection%20on%20Transformers%2C%0Athereby%20demonstrating%20the%20broad%20applicability%20of%20this%20prior%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18162v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut-of-Distribution%2520Detection%2520using%2520Neural%2520Activation%2520Prior%26entry.906535625%3DWeilin%2520Wan%2520and%2520Weizhong%2520Zhang%2520and%2520Quan%2520Zhou%2520and%2520Fan%2520Yi%2520and%2520Cheng%2520Jin%26entry.1292438233%3D%2520%2520Out-of-distribution%2520detection%2520%2528OOD%2529%2520is%2520a%2520crucial%2520technique%2520for%2520deploying%250Amachine%2520learning%2520models%2520in%2520the%2520real%2520world%2520to%2520handle%2520the%2520unseen%2520scenarios.%2520In%250Athis%2520paper%252C%2520we%2520first%2520propose%2520a%2520simple%2520yet%2520effective%2520Neural%2520Activation%2520Prior%250A%2528NAP%2529%2520for%2520OOD%2520detection.%2520Our%2520neural%2520activation%2520prior%2520is%2520based%2520on%2520a%2520key%250Aobservation%2520that%252C%2520for%2520a%2520channel%2520before%2520the%2520global%2520pooling%2520layer%2520of%2520a%2520fully%250Atrained%2520neural%2520network%252C%2520the%2520probability%2520of%2520a%2520few%2520neurons%2520being%2520activated%2520with%2520a%250Alarge%2520response%2520by%2520an%2520in-distribution%2520%2528ID%2529%2520sample%2520is%2520significantly%2520higher%2520than%250Athat%2520by%2520an%2520OOD%2520sample.%2520An%2520intuitive%2520explanation%2520is%2520that%2520for%2520a%2520model%2520fully%250Atrained%2520on%2520ID%2520dataset%252C%2520each%2520channel%2520would%2520play%2520a%2520role%2520in%2520detecting%2520a%2520certain%250Apattern%2520in%2520the%2520ID%2520dataset%252C%2520and%2520a%2520few%2520neurons%2520can%2520be%2520activated%2520with%2520a%2520large%250Aresponse%2520when%2520the%2520pattern%2520is%2520detected%2520in%2520an%2520input%2520sample.%2520Then%252C%2520a%2520new%2520scoring%250Afunction%2520based%2520on%2520this%2520prior%2520is%2520proposed%2520to%2520highlight%2520the%2520role%2520of%2520these%250Astrongly%2520activated%2520neurons%2520in%2520OOD%2520detection.%2520Our%2520approach%2520is%2520plug-and-play%2520and%250Adoes%2520not%2520lead%2520to%2520any%2520performance%2520degradation%2520on%2520ID%2520data%2520classification%2520and%250Arequires%2520no%2520extra%2520training%2520or%2520statistics%2520from%2520training%2520or%2520external%2520datasets.%250ANotice%2520that%2520previous%2520methods%2520primarily%2520rely%2520on%2520post-global-pooling%2520features%2520of%250Athe%2520neural%2520networks%252C%2520while%2520the%2520within-channel%2520distribution%2520information%2520we%250Aleverage%2520would%2520be%2520discarded%2520by%2520the%2520global%2520pooling%2520operator.%2520Consequently%252C%2520our%250Amethod%2520is%2520orthogonal%2520to%2520existing%2520approaches%2520and%2520can%2520be%2520effectively%2520combined%250Awith%2520them%2520in%2520various%2520applications.%2520Experimental%2520results%2520show%2520that%2520our%2520method%250Aachieves%2520the%2520state-of-the-art%2520performance%2520on%2520CIFAR%2520benchmark%2520and%2520ImageNet%250Adataset%252C%2520which%2520demonstrates%2520the%2520power%2520of%2520the%2520proposed%2520prior.%2520Finally%252C%2520we%2520extend%250Aour%2520method%2520to%2520Transformers%2520and%2520the%2520experimental%2520findings%2520indicate%2520that%2520NAP%2520can%250Aalso%2520significantly%2520enhance%2520the%2520performance%2520of%2520OOD%2520detection%2520on%2520Transformers%252C%250Athereby%2520demonstrating%2520the%2520broad%2520applicability%2520of%2520this%2520prior%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18162v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-Distribution%20Detection%20using%20Neural%20Activation%20Prior&entry.906535625=Weilin%20Wan%20and%20Weizhong%20Zhang%20and%20Quan%20Zhou%20and%20Fan%20Yi%20and%20Cheng%20Jin&entry.1292438233=%20%20Out-of-distribution%20detection%20%28OOD%29%20is%20a%20crucial%20technique%20for%20deploying%0Amachine%20learning%20models%20in%20the%20real%20world%20to%20handle%20the%20unseen%20scenarios.%20In%0Athis%20paper%2C%20we%20first%20propose%20a%20simple%20yet%20effective%20Neural%20Activation%20Prior%0A%28NAP%29%20for%20OOD%20detection.%20Our%20neural%20activation%20prior%20is%20based%20on%20a%20key%0Aobservation%20that%2C%20for%20a%20channel%20before%20the%20global%20pooling%20layer%20of%20a%20fully%0Atrained%20neural%20network%2C%20the%20probability%20of%20a%20few%20neurons%20being%20activated%20with%20a%0Alarge%20response%20by%20an%20in-distribution%20%28ID%29%20sample%20is%20significantly%20higher%20than%0Athat%20by%20an%20OOD%20sample.%20An%20intuitive%20explanation%20is%20that%20for%20a%20model%20fully%0Atrained%20on%20ID%20dataset%2C%20each%20channel%20would%20play%20a%20role%20in%20detecting%20a%20certain%0Apattern%20in%20the%20ID%20dataset%2C%20and%20a%20few%20neurons%20can%20be%20activated%20with%20a%20large%0Aresponse%20when%20the%20pattern%20is%20detected%20in%20an%20input%20sample.%20Then%2C%20a%20new%20scoring%0Afunction%20based%20on%20this%20prior%20is%20proposed%20to%20highlight%20the%20role%20of%20these%0Astrongly%20activated%20neurons%20in%20OOD%20detection.%20Our%20approach%20is%20plug-and-play%20and%0Adoes%20not%20lead%20to%20any%20performance%20degradation%20on%20ID%20data%20classification%20and%0Arequires%20no%20extra%20training%20or%20statistics%20from%20training%20or%20external%20datasets.%0ANotice%20that%20previous%20methods%20primarily%20rely%20on%20post-global-pooling%20features%20of%0Athe%20neural%20networks%2C%20while%20the%20within-channel%20distribution%20information%20we%0Aleverage%20would%20be%20discarded%20by%20the%20global%20pooling%20operator.%20Consequently%2C%20our%0Amethod%20is%20orthogonal%20to%20existing%20approaches%20and%20can%20be%20effectively%20combined%0Awith%20them%20in%20various%20applications.%20Experimental%20results%20show%20that%20our%20method%0Aachieves%20the%20state-of-the-art%20performance%20on%20CIFAR%20benchmark%20and%20ImageNet%0Adataset%2C%20which%20demonstrates%20the%20power%20of%20the%20proposed%20prior.%20Finally%2C%20we%20extend%0Aour%20method%20to%20Transformers%20and%20the%20experimental%20findings%20indicate%20that%20NAP%20can%0Aalso%20significantly%20enhance%20the%20performance%20of%20OOD%20detection%20on%20Transformers%2C%0Athereby%20demonstrating%20the%20broad%20applicability%20of%20this%20prior%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18162v4&entry.124074799=Read"},
{"title": "Trackastra: Transformer-based cell tracking for live-cell microscopy", "author": "Benjamin Gallusser and Martin Weigert", "abstract": "  Cell tracking is an omnipresent image analysis task in live-cell microscopy.\nIt is similar to multiple object tracking (MOT), however, each frame contains\nhundreds of similar-looking objects that can divide, making it a challenging\nproblem. Current state-of-the-art approaches follow the tracking-by-detection\nparadigm, i.e. first all cells are detected per frame and successively linked\nin a second step to form biologically consistent cell tracks. Linking is\ncommonly solved via discrete optimization methods, which require manual tuning\nof hyperparameters for each dataset and are therefore cumbersome to use in\npractice. Here we propose Trackastra, a general purpose cell tracking approach\nthat uses a simple transformer architecture to directly learn pairwise\nassociations of cells within a temporal window from annotated data.\nImportantly, unlike existing transformer-based MOT pipelines, our learning\narchitecture also accounts for dividing objects such as cells and allows for\naccurate tracking even with simple greedy linking, thus making strides towards\nremoving the requirement for a complex linking step. The proposed architecture\noperates on the full spatio-temporal context of detections within a time window\nby avoiding the computational burden of processing dense images. We show that\nour tracking approach performs on par with or better than highly tuned\nstate-of-the-art cell tracking algorithms for various biological datasets, such\nas bacteria, cell cultures and fluorescent particles. We provide code at\nhttps://github.com/weigertlab/trackastra.\n", "link": "http://arxiv.org/abs/2405.15700v1", "date": "2024-05-24", "relevancy": 2.56, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5129}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trackastra%3A%20Transformer-based%20cell%20tracking%20for%20live-cell%20microscopy&body=Title%3A%20Trackastra%3A%20Transformer-based%20cell%20tracking%20for%20live-cell%20microscopy%0AAuthor%3A%20Benjamin%20Gallusser%20and%20Martin%20Weigert%0AAbstract%3A%20%20%20Cell%20tracking%20is%20an%20omnipresent%20image%20analysis%20task%20in%20live-cell%20microscopy.%0AIt%20is%20similar%20to%20multiple%20object%20tracking%20%28MOT%29%2C%20however%2C%20each%20frame%20contains%0Ahundreds%20of%20similar-looking%20objects%20that%20can%20divide%2C%20making%20it%20a%20challenging%0Aproblem.%20Current%20state-of-the-art%20approaches%20follow%20the%20tracking-by-detection%0Aparadigm%2C%20i.e.%20first%20all%20cells%20are%20detected%20per%20frame%20and%20successively%20linked%0Ain%20a%20second%20step%20to%20form%20biologically%20consistent%20cell%20tracks.%20Linking%20is%0Acommonly%20solved%20via%20discrete%20optimization%20methods%2C%20which%20require%20manual%20tuning%0Aof%20hyperparameters%20for%20each%20dataset%20and%20are%20therefore%20cumbersome%20to%20use%20in%0Apractice.%20Here%20we%20propose%20Trackastra%2C%20a%20general%20purpose%20cell%20tracking%20approach%0Athat%20uses%20a%20simple%20transformer%20architecture%20to%20directly%20learn%20pairwise%0Aassociations%20of%20cells%20within%20a%20temporal%20window%20from%20annotated%20data.%0AImportantly%2C%20unlike%20existing%20transformer-based%20MOT%20pipelines%2C%20our%20learning%0Aarchitecture%20also%20accounts%20for%20dividing%20objects%20such%20as%20cells%20and%20allows%20for%0Aaccurate%20tracking%20even%20with%20simple%20greedy%20linking%2C%20thus%20making%20strides%20towards%0Aremoving%20the%20requirement%20for%20a%20complex%20linking%20step.%20The%20proposed%20architecture%0Aoperates%20on%20the%20full%20spatio-temporal%20context%20of%20detections%20within%20a%20time%20window%0Aby%20avoiding%20the%20computational%20burden%20of%20processing%20dense%20images.%20We%20show%20that%0Aour%20tracking%20approach%20performs%20on%20par%20with%20or%20better%20than%20highly%20tuned%0Astate-of-the-art%20cell%20tracking%20algorithms%20for%20various%20biological%20datasets%2C%20such%0Aas%20bacteria%2C%20cell%20cultures%20and%20fluorescent%20particles.%20We%20provide%20code%20at%0Ahttps%3A//github.com/weigertlab/trackastra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackastra%253A%2520Transformer-based%2520cell%2520tracking%2520for%2520live-cell%2520microscopy%26entry.906535625%3DBenjamin%2520Gallusser%2520and%2520Martin%2520Weigert%26entry.1292438233%3D%2520%2520Cell%2520tracking%2520is%2520an%2520omnipresent%2520image%2520analysis%2520task%2520in%2520live-cell%2520microscopy.%250AIt%2520is%2520similar%2520to%2520multiple%2520object%2520tracking%2520%2528MOT%2529%252C%2520however%252C%2520each%2520frame%2520contains%250Ahundreds%2520of%2520similar-looking%2520objects%2520that%2520can%2520divide%252C%2520making%2520it%2520a%2520challenging%250Aproblem.%2520Current%2520state-of-the-art%2520approaches%2520follow%2520the%2520tracking-by-detection%250Aparadigm%252C%2520i.e.%2520first%2520all%2520cells%2520are%2520detected%2520per%2520frame%2520and%2520successively%2520linked%250Ain%2520a%2520second%2520step%2520to%2520form%2520biologically%2520consistent%2520cell%2520tracks.%2520Linking%2520is%250Acommonly%2520solved%2520via%2520discrete%2520optimization%2520methods%252C%2520which%2520require%2520manual%2520tuning%250Aof%2520hyperparameters%2520for%2520each%2520dataset%2520and%2520are%2520therefore%2520cumbersome%2520to%2520use%2520in%250Apractice.%2520Here%2520we%2520propose%2520Trackastra%252C%2520a%2520general%2520purpose%2520cell%2520tracking%2520approach%250Athat%2520uses%2520a%2520simple%2520transformer%2520architecture%2520to%2520directly%2520learn%2520pairwise%250Aassociations%2520of%2520cells%2520within%2520a%2520temporal%2520window%2520from%2520annotated%2520data.%250AImportantly%252C%2520unlike%2520existing%2520transformer-based%2520MOT%2520pipelines%252C%2520our%2520learning%250Aarchitecture%2520also%2520accounts%2520for%2520dividing%2520objects%2520such%2520as%2520cells%2520and%2520allows%2520for%250Aaccurate%2520tracking%2520even%2520with%2520simple%2520greedy%2520linking%252C%2520thus%2520making%2520strides%2520towards%250Aremoving%2520the%2520requirement%2520for%2520a%2520complex%2520linking%2520step.%2520The%2520proposed%2520architecture%250Aoperates%2520on%2520the%2520full%2520spatio-temporal%2520context%2520of%2520detections%2520within%2520a%2520time%2520window%250Aby%2520avoiding%2520the%2520computational%2520burden%2520of%2520processing%2520dense%2520images.%2520We%2520show%2520that%250Aour%2520tracking%2520approach%2520performs%2520on%2520par%2520with%2520or%2520better%2520than%2520highly%2520tuned%250Astate-of-the-art%2520cell%2520tracking%2520algorithms%2520for%2520various%2520biological%2520datasets%252C%2520such%250Aas%2520bacteria%252C%2520cell%2520cultures%2520and%2520fluorescent%2520particles.%2520We%2520provide%2520code%2520at%250Ahttps%253A//github.com/weigertlab/trackastra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trackastra%3A%20Transformer-based%20cell%20tracking%20for%20live-cell%20microscopy&entry.906535625=Benjamin%20Gallusser%20and%20Martin%20Weigert&entry.1292438233=%20%20Cell%20tracking%20is%20an%20omnipresent%20image%20analysis%20task%20in%20live-cell%20microscopy.%0AIt%20is%20similar%20to%20multiple%20object%20tracking%20%28MOT%29%2C%20however%2C%20each%20frame%20contains%0Ahundreds%20of%20similar-looking%20objects%20that%20can%20divide%2C%20making%20it%20a%20challenging%0Aproblem.%20Current%20state-of-the-art%20approaches%20follow%20the%20tracking-by-detection%0Aparadigm%2C%20i.e.%20first%20all%20cells%20are%20detected%20per%20frame%20and%20successively%20linked%0Ain%20a%20second%20step%20to%20form%20biologically%20consistent%20cell%20tracks.%20Linking%20is%0Acommonly%20solved%20via%20discrete%20optimization%20methods%2C%20which%20require%20manual%20tuning%0Aof%20hyperparameters%20for%20each%20dataset%20and%20are%20therefore%20cumbersome%20to%20use%20in%0Apractice.%20Here%20we%20propose%20Trackastra%2C%20a%20general%20purpose%20cell%20tracking%20approach%0Athat%20uses%20a%20simple%20transformer%20architecture%20to%20directly%20learn%20pairwise%0Aassociations%20of%20cells%20within%20a%20temporal%20window%20from%20annotated%20data.%0AImportantly%2C%20unlike%20existing%20transformer-based%20MOT%20pipelines%2C%20our%20learning%0Aarchitecture%20also%20accounts%20for%20dividing%20objects%20such%20as%20cells%20and%20allows%20for%0Aaccurate%20tracking%20even%20with%20simple%20greedy%20linking%2C%20thus%20making%20strides%20towards%0Aremoving%20the%20requirement%20for%20a%20complex%20linking%20step.%20The%20proposed%20architecture%0Aoperates%20on%20the%20full%20spatio-temporal%20context%20of%20detections%20within%20a%20time%20window%0Aby%20avoiding%20the%20computational%20burden%20of%20processing%20dense%20images.%20We%20show%20that%0Aour%20tracking%20approach%20performs%20on%20par%20with%20or%20better%20than%20highly%20tuned%0Astate-of-the-art%20cell%20tracking%20algorithms%20for%20various%20biological%20datasets%2C%20such%0Aas%20bacteria%2C%20cell%20cultures%20and%20fluorescent%20particles.%20We%20provide%20code%20at%0Ahttps%3A//github.com/weigertlab/trackastra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15700v1&entry.124074799=Read"},
{"title": "Composed Image Retrieval for Remote Sensing", "author": "Bill Psomas and Ioannis Kakogeorgiou and Nikos Efthymiadis and Giorgos Tolias and Ondrej Chum and Yannis Avrithis and Konstantinos Karantzalos", "abstract": "  This work introduces composed image retrieval to remote sensing. It allows to\nquery a large image archive by image examples alternated by a textual\ndescription, enriching the descriptive power over unimodal queries, either\nvisual or textual. Various attributes can be modified by the textual part, such\nas shape, color, or context. A novel method fusing image-to-image and\ntext-to-image similarity is introduced. We demonstrate that a vision-language\nmodel possesses sufficient descriptive power and no further learning step or\ntraining data are necessary. We present a new evaluation benchmark focused on\ncolor, context, density, existence, quantity, and shape modifications. Our work\nnot only sets the state-of-the-art for this task, but also serves as a\nfoundational step in addressing a gap in the field of remote sensing image\nretrieval. Code at: https://github.com/billpsomas/rscir\n", "link": "http://arxiv.org/abs/2405.15587v1", "date": "2024-05-24", "relevancy": 2.5303, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5317}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5035}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Composed%20Image%20Retrieval%20for%20Remote%20Sensing&body=Title%3A%20Composed%20Image%20Retrieval%20for%20Remote%20Sensing%0AAuthor%3A%20Bill%20Psomas%20and%20Ioannis%20Kakogeorgiou%20and%20Nikos%20Efthymiadis%20and%20Giorgos%20Tolias%20and%20Ondrej%20Chum%20and%20Yannis%20Avrithis%20and%20Konstantinos%20Karantzalos%0AAbstract%3A%20%20%20This%20work%20introduces%20composed%20image%20retrieval%20to%20remote%20sensing.%20It%20allows%20to%0Aquery%20a%20large%20image%20archive%20by%20image%20examples%20alternated%20by%20a%20textual%0Adescription%2C%20enriching%20the%20descriptive%20power%20over%20unimodal%20queries%2C%20either%0Avisual%20or%20textual.%20Various%20attributes%20can%20be%20modified%20by%20the%20textual%20part%2C%20such%0Aas%20shape%2C%20color%2C%20or%20context.%20A%20novel%20method%20fusing%20image-to-image%20and%0Atext-to-image%20similarity%20is%20introduced.%20We%20demonstrate%20that%20a%20vision-language%0Amodel%20possesses%20sufficient%20descriptive%20power%20and%20no%20further%20learning%20step%20or%0Atraining%20data%20are%20necessary.%20We%20present%20a%20new%20evaluation%20benchmark%20focused%20on%0Acolor%2C%20context%2C%20density%2C%20existence%2C%20quantity%2C%20and%20shape%20modifications.%20Our%20work%0Anot%20only%20sets%20the%20state-of-the-art%20for%20this%20task%2C%20but%20also%20serves%20as%20a%0Afoundational%20step%20in%20addressing%20a%20gap%20in%20the%20field%20of%20remote%20sensing%20image%0Aretrieval.%20Code%20at%3A%20https%3A//github.com/billpsomas/rscir%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComposed%2520Image%2520Retrieval%2520for%2520Remote%2520Sensing%26entry.906535625%3DBill%2520Psomas%2520and%2520Ioannis%2520Kakogeorgiou%2520and%2520Nikos%2520Efthymiadis%2520and%2520Giorgos%2520Tolias%2520and%2520Ondrej%2520Chum%2520and%2520Yannis%2520Avrithis%2520and%2520Konstantinos%2520Karantzalos%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520composed%2520image%2520retrieval%2520to%2520remote%2520sensing.%2520It%2520allows%2520to%250Aquery%2520a%2520large%2520image%2520archive%2520by%2520image%2520examples%2520alternated%2520by%2520a%2520textual%250Adescription%252C%2520enriching%2520the%2520descriptive%2520power%2520over%2520unimodal%2520queries%252C%2520either%250Avisual%2520or%2520textual.%2520Various%2520attributes%2520can%2520be%2520modified%2520by%2520the%2520textual%2520part%252C%2520such%250Aas%2520shape%252C%2520color%252C%2520or%2520context.%2520A%2520novel%2520method%2520fusing%2520image-to-image%2520and%250Atext-to-image%2520similarity%2520is%2520introduced.%2520We%2520demonstrate%2520that%2520a%2520vision-language%250Amodel%2520possesses%2520sufficient%2520descriptive%2520power%2520and%2520no%2520further%2520learning%2520step%2520or%250Atraining%2520data%2520are%2520necessary.%2520We%2520present%2520a%2520new%2520evaluation%2520benchmark%2520focused%2520on%250Acolor%252C%2520context%252C%2520density%252C%2520existence%252C%2520quantity%252C%2520and%2520shape%2520modifications.%2520Our%2520work%250Anot%2520only%2520sets%2520the%2520state-of-the-art%2520for%2520this%2520task%252C%2520but%2520also%2520serves%2520as%2520a%250Afoundational%2520step%2520in%2520addressing%2520a%2520gap%2520in%2520the%2520field%2520of%2520remote%2520sensing%2520image%250Aretrieval.%2520Code%2520at%253A%2520https%253A//github.com/billpsomas/rscir%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Composed%20Image%20Retrieval%20for%20Remote%20Sensing&entry.906535625=Bill%20Psomas%20and%20Ioannis%20Kakogeorgiou%20and%20Nikos%20Efthymiadis%20and%20Giorgos%20Tolias%20and%20Ondrej%20Chum%20and%20Yannis%20Avrithis%20and%20Konstantinos%20Karantzalos&entry.1292438233=%20%20This%20work%20introduces%20composed%20image%20retrieval%20to%20remote%20sensing.%20It%20allows%20to%0Aquery%20a%20large%20image%20archive%20by%20image%20examples%20alternated%20by%20a%20textual%0Adescription%2C%20enriching%20the%20descriptive%20power%20over%20unimodal%20queries%2C%20either%0Avisual%20or%20textual.%20Various%20attributes%20can%20be%20modified%20by%20the%20textual%20part%2C%20such%0Aas%20shape%2C%20color%2C%20or%20context.%20A%20novel%20method%20fusing%20image-to-image%20and%0Atext-to-image%20similarity%20is%20introduced.%20We%20demonstrate%20that%20a%20vision-language%0Amodel%20possesses%20sufficient%20descriptive%20power%20and%20no%20further%20learning%20step%20or%0Atraining%20data%20are%20necessary.%20We%20present%20a%20new%20evaluation%20benchmark%20focused%20on%0Acolor%2C%20context%2C%20density%2C%20existence%2C%20quantity%2C%20and%20shape%20modifications.%20Our%20work%0Anot%20only%20sets%20the%20state-of-the-art%20for%20this%20task%2C%20but%20also%20serves%20as%20a%0Afoundational%20step%20in%20addressing%20a%20gap%20in%20the%20field%20of%20remote%20sensing%20image%0Aretrieval.%20Code%20at%3A%20https%3A//github.com/billpsomas/rscir%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15587v1&entry.124074799=Read"},
{"title": "Rethinking Independent Cross-Entropy Loss For Graph-Structured Data", "author": "Rui Miao and Kaixiong Zhou and Yili Wang and Ninghao Liu and Ying Wang and Xin Wang", "abstract": "  Graph neural networks (GNNs) have exhibited prominent performance in learning\ngraph-structured data. Considering node classification task, based on the i.i.d\nassumption among node labels, the traditional supervised learning simply sums\nup cross-entropy losses of the independent training nodes and applies the\naverage loss to optimize GNNs' weights. But different from other data formats,\nthe nodes are naturally connected. It is found that the independent\ndistribution modeling of node labels restricts GNNs' capability to generalize\nover the entire graph and defend adversarial attacks. In this work, we propose\na new framework, termed joint-cluster supervised learning, to model the joint\ndistribution of each node with its corresponding cluster. We learn the joint\ndistribution of node and cluster labels conditioned on their representations,\nand train GNNs with the obtained joint loss. In this way, the data-label\nreference signals extracted from the local cluster explicitly strengthen the\ndiscrimination ability on the target node. The extensive experiments\ndemonstrate that our joint-cluster supervised learning can effectively bolster\nGNNs' node classification accuracy. Furthermore, being benefited from the\nreference signals which may be free from spiteful interference, our learning\nparadigm significantly protects the node classification from being affected by\nthe adversarial attack.\n", "link": "http://arxiv.org/abs/2405.15564v1", "date": "2024-05-24", "relevancy": 2.5243, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5268}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5134}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Independent%20Cross-Entropy%20Loss%20For%20Graph-Structured%20Data&body=Title%3A%20Rethinking%20Independent%20Cross-Entropy%20Loss%20For%20Graph-Structured%20Data%0AAuthor%3A%20Rui%20Miao%20and%20Kaixiong%20Zhou%20and%20Yili%20Wang%20and%20Ninghao%20Liu%20and%20Ying%20Wang%20and%20Xin%20Wang%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20exhibited%20prominent%20performance%20in%20learning%0Agraph-structured%20data.%20Considering%20node%20classification%20task%2C%20based%20on%20the%20i.i.d%0Aassumption%20among%20node%20labels%2C%20the%20traditional%20supervised%20learning%20simply%20sums%0Aup%20cross-entropy%20losses%20of%20the%20independent%20training%20nodes%20and%20applies%20the%0Aaverage%20loss%20to%20optimize%20GNNs%27%20weights.%20But%20different%20from%20other%20data%20formats%2C%0Athe%20nodes%20are%20naturally%20connected.%20It%20is%20found%20that%20the%20independent%0Adistribution%20modeling%20of%20node%20labels%20restricts%20GNNs%27%20capability%20to%20generalize%0Aover%20the%20entire%20graph%20and%20defend%20adversarial%20attacks.%20In%20this%20work%2C%20we%20propose%0Aa%20new%20framework%2C%20termed%20joint-cluster%20supervised%20learning%2C%20to%20model%20the%20joint%0Adistribution%20of%20each%20node%20with%20its%20corresponding%20cluster.%20We%20learn%20the%20joint%0Adistribution%20of%20node%20and%20cluster%20labels%20conditioned%20on%20their%20representations%2C%0Aand%20train%20GNNs%20with%20the%20obtained%20joint%20loss.%20In%20this%20way%2C%20the%20data-label%0Areference%20signals%20extracted%20from%20the%20local%20cluster%20explicitly%20strengthen%20the%0Adiscrimination%20ability%20on%20the%20target%20node.%20The%20extensive%20experiments%0Ademonstrate%20that%20our%20joint-cluster%20supervised%20learning%20can%20effectively%20bolster%0AGNNs%27%20node%20classification%20accuracy.%20Furthermore%2C%20being%20benefited%20from%20the%0Areference%20signals%20which%20may%20be%20free%20from%20spiteful%20interference%2C%20our%20learning%0Aparadigm%20significantly%20protects%20the%20node%20classification%20from%20being%20affected%20by%0Athe%20adversarial%20attack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Independent%2520Cross-Entropy%2520Loss%2520For%2520Graph-Structured%2520Data%26entry.906535625%3DRui%2520Miao%2520and%2520Kaixiong%2520Zhou%2520and%2520Yili%2520Wang%2520and%2520Ninghao%2520Liu%2520and%2520Ying%2520Wang%2520and%2520Xin%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520exhibited%2520prominent%2520performance%2520in%2520learning%250Agraph-structured%2520data.%2520Considering%2520node%2520classification%2520task%252C%2520based%2520on%2520the%2520i.i.d%250Aassumption%2520among%2520node%2520labels%252C%2520the%2520traditional%2520supervised%2520learning%2520simply%2520sums%250Aup%2520cross-entropy%2520losses%2520of%2520the%2520independent%2520training%2520nodes%2520and%2520applies%2520the%250Aaverage%2520loss%2520to%2520optimize%2520GNNs%2527%2520weights.%2520But%2520different%2520from%2520other%2520data%2520formats%252C%250Athe%2520nodes%2520are%2520naturally%2520connected.%2520It%2520is%2520found%2520that%2520the%2520independent%250Adistribution%2520modeling%2520of%2520node%2520labels%2520restricts%2520GNNs%2527%2520capability%2520to%2520generalize%250Aover%2520the%2520entire%2520graph%2520and%2520defend%2520adversarial%2520attacks.%2520In%2520this%2520work%252C%2520we%2520propose%250Aa%2520new%2520framework%252C%2520termed%2520joint-cluster%2520supervised%2520learning%252C%2520to%2520model%2520the%2520joint%250Adistribution%2520of%2520each%2520node%2520with%2520its%2520corresponding%2520cluster.%2520We%2520learn%2520the%2520joint%250Adistribution%2520of%2520node%2520and%2520cluster%2520labels%2520conditioned%2520on%2520their%2520representations%252C%250Aand%2520train%2520GNNs%2520with%2520the%2520obtained%2520joint%2520loss.%2520In%2520this%2520way%252C%2520the%2520data-label%250Areference%2520signals%2520extracted%2520from%2520the%2520local%2520cluster%2520explicitly%2520strengthen%2520the%250Adiscrimination%2520ability%2520on%2520the%2520target%2520node.%2520The%2520extensive%2520experiments%250Ademonstrate%2520that%2520our%2520joint-cluster%2520supervised%2520learning%2520can%2520effectively%2520bolster%250AGNNs%2527%2520node%2520classification%2520accuracy.%2520Furthermore%252C%2520being%2520benefited%2520from%2520the%250Areference%2520signals%2520which%2520may%2520be%2520free%2520from%2520spiteful%2520interference%252C%2520our%2520learning%250Aparadigm%2520significantly%2520protects%2520the%2520node%2520classification%2520from%2520being%2520affected%2520by%250Athe%2520adversarial%2520attack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Independent%20Cross-Entropy%20Loss%20For%20Graph-Structured%20Data&entry.906535625=Rui%20Miao%20and%20Kaixiong%20Zhou%20and%20Yili%20Wang%20and%20Ninghao%20Liu%20and%20Ying%20Wang%20and%20Xin%20Wang&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20exhibited%20prominent%20performance%20in%20learning%0Agraph-structured%20data.%20Considering%20node%20classification%20task%2C%20based%20on%20the%20i.i.d%0Aassumption%20among%20node%20labels%2C%20the%20traditional%20supervised%20learning%20simply%20sums%0Aup%20cross-entropy%20losses%20of%20the%20independent%20training%20nodes%20and%20applies%20the%0Aaverage%20loss%20to%20optimize%20GNNs%27%20weights.%20But%20different%20from%20other%20data%20formats%2C%0Athe%20nodes%20are%20naturally%20connected.%20It%20is%20found%20that%20the%20independent%0Adistribution%20modeling%20of%20node%20labels%20restricts%20GNNs%27%20capability%20to%20generalize%0Aover%20the%20entire%20graph%20and%20defend%20adversarial%20attacks.%20In%20this%20work%2C%20we%20propose%0Aa%20new%20framework%2C%20termed%20joint-cluster%20supervised%20learning%2C%20to%20model%20the%20joint%0Adistribution%20of%20each%20node%20with%20its%20corresponding%20cluster.%20We%20learn%20the%20joint%0Adistribution%20of%20node%20and%20cluster%20labels%20conditioned%20on%20their%20representations%2C%0Aand%20train%20GNNs%20with%20the%20obtained%20joint%20loss.%20In%20this%20way%2C%20the%20data-label%0Areference%20signals%20extracted%20from%20the%20local%20cluster%20explicitly%20strengthen%20the%0Adiscrimination%20ability%20on%20the%20target%20node.%20The%20extensive%20experiments%0Ademonstrate%20that%20our%20joint-cluster%20supervised%20learning%20can%20effectively%20bolster%0AGNNs%27%20node%20classification%20accuracy.%20Furthermore%2C%20being%20benefited%20from%20the%0Areference%20signals%20which%20may%20be%20free%20from%20spiteful%20interference%2C%20our%20learning%0Aparadigm%20significantly%20protects%20the%20node%20classification%20from%20being%20affected%20by%0Athe%20adversarial%20attack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15564v1&entry.124074799=Read"},
{"title": "How to Fix a Broken Confidence Estimator: Evaluating Post-hoc Methods\n  for Selective Classification with Deep Neural Networks", "author": "Lu\u00eds Felipe P. Cattelan and Danilo Silva", "abstract": "  This paper addresses the problem of selective classification for deep neural\nnetworks, where a model is allowed to abstain from low-confidence predictions\nto avoid potential errors. We focus on so-called post-hoc methods, which\nreplace the confidence estimator of a given classifier without modifying or\nretraining it, thus being practically appealing. Considering neural networks\nwith softmax outputs, our goal is to identify the best confidence estimator\nthat can be computed directly from the unnormalized logits. This problem is\nmotivated by the intriguing observation in recent work that many classifiers\nappear to have a \"broken\" confidence estimator, in the sense that their\nselective classification performance is much worse than what could be expected\nby their corresponding accuracies. We perform an extensive experimental study\nof many existing and proposed confidence estimators applied to 84 pretrained\nImageNet classifiers available from popular repositories. Our results show that\na simple $p$-norm normalization of the logits, followed by taking the maximum\nlogit as the confidence estimator, can lead to considerable gains in selective\nclassification performance, completely fixing the pathological behavior\nobserved in many classifiers. As a consequence, the selective classification\nperformance of any classifier becomes almost entirely determined by its\ncorresponding accuracy. Moreover, these results are shown to be consistent\nunder distribution shift. Our code is available at\nhttps://github.com/lfpc/FixSelectiveClassification.\n", "link": "http://arxiv.org/abs/2305.15508v4", "date": "2024-05-24", "relevancy": 2.5007, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5418}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4806}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Fix%20a%20Broken%20Confidence%20Estimator%3A%20Evaluating%20Post-hoc%20Methods%0A%20%20for%20Selective%20Classification%20with%20Deep%20Neural%20Networks&body=Title%3A%20How%20to%20Fix%20a%20Broken%20Confidence%20Estimator%3A%20Evaluating%20Post-hoc%20Methods%0A%20%20for%20Selective%20Classification%20with%20Deep%20Neural%20Networks%0AAuthor%3A%20Lu%C3%ADs%20Felipe%20P.%20Cattelan%20and%20Danilo%20Silva%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20selective%20classification%20for%20deep%20neural%0Anetworks%2C%20where%20a%20model%20is%20allowed%20to%20abstain%20from%20low-confidence%20predictions%0Ato%20avoid%20potential%20errors.%20We%20focus%20on%20so-called%20post-hoc%20methods%2C%20which%0Areplace%20the%20confidence%20estimator%20of%20a%20given%20classifier%20without%20modifying%20or%0Aretraining%20it%2C%20thus%20being%20practically%20appealing.%20Considering%20neural%20networks%0Awith%20softmax%20outputs%2C%20our%20goal%20is%20to%20identify%20the%20best%20confidence%20estimator%0Athat%20can%20be%20computed%20directly%20from%20the%20unnormalized%20logits.%20This%20problem%20is%0Amotivated%20by%20the%20intriguing%20observation%20in%20recent%20work%20that%20many%20classifiers%0Aappear%20to%20have%20a%20%22broken%22%20confidence%20estimator%2C%20in%20the%20sense%20that%20their%0Aselective%20classification%20performance%20is%20much%20worse%20than%20what%20could%20be%20expected%0Aby%20their%20corresponding%20accuracies.%20We%20perform%20an%20extensive%20experimental%20study%0Aof%20many%20existing%20and%20proposed%20confidence%20estimators%20applied%20to%2084%20pretrained%0AImageNet%20classifiers%20available%20from%20popular%20repositories.%20Our%20results%20show%20that%0Aa%20simple%20%24p%24-norm%20normalization%20of%20the%20logits%2C%20followed%20by%20taking%20the%20maximum%0Alogit%20as%20the%20confidence%20estimator%2C%20can%20lead%20to%20considerable%20gains%20in%20selective%0Aclassification%20performance%2C%20completely%20fixing%20the%20pathological%20behavior%0Aobserved%20in%20many%20classifiers.%20As%20a%20consequence%2C%20the%20selective%20classification%0Aperformance%20of%20any%20classifier%20becomes%20almost%20entirely%20determined%20by%20its%0Acorresponding%20accuracy.%20Moreover%2C%20these%20results%20are%20shown%20to%20be%20consistent%0Aunder%20distribution%20shift.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lfpc/FixSelectiveClassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15508v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Fix%2520a%2520Broken%2520Confidence%2520Estimator%253A%2520Evaluating%2520Post-hoc%2520Methods%250A%2520%2520for%2520Selective%2520Classification%2520with%2520Deep%2520Neural%2520Networks%26entry.906535625%3DLu%25C3%25ADs%2520Felipe%2520P.%2520Cattelan%2520and%2520Danilo%2520Silva%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520selective%2520classification%2520for%2520deep%2520neural%250Anetworks%252C%2520where%2520a%2520model%2520is%2520allowed%2520to%2520abstain%2520from%2520low-confidence%2520predictions%250Ato%2520avoid%2520potential%2520errors.%2520We%2520focus%2520on%2520so-called%2520post-hoc%2520methods%252C%2520which%250Areplace%2520the%2520confidence%2520estimator%2520of%2520a%2520given%2520classifier%2520without%2520modifying%2520or%250Aretraining%2520it%252C%2520thus%2520being%2520practically%2520appealing.%2520Considering%2520neural%2520networks%250Awith%2520softmax%2520outputs%252C%2520our%2520goal%2520is%2520to%2520identify%2520the%2520best%2520confidence%2520estimator%250Athat%2520can%2520be%2520computed%2520directly%2520from%2520the%2520unnormalized%2520logits.%2520This%2520problem%2520is%250Amotivated%2520by%2520the%2520intriguing%2520observation%2520in%2520recent%2520work%2520that%2520many%2520classifiers%250Aappear%2520to%2520have%2520a%2520%2522broken%2522%2520confidence%2520estimator%252C%2520in%2520the%2520sense%2520that%2520their%250Aselective%2520classification%2520performance%2520is%2520much%2520worse%2520than%2520what%2520could%2520be%2520expected%250Aby%2520their%2520corresponding%2520accuracies.%2520We%2520perform%2520an%2520extensive%2520experimental%2520study%250Aof%2520many%2520existing%2520and%2520proposed%2520confidence%2520estimators%2520applied%2520to%252084%2520pretrained%250AImageNet%2520classifiers%2520available%2520from%2520popular%2520repositories.%2520Our%2520results%2520show%2520that%250Aa%2520simple%2520%2524p%2524-norm%2520normalization%2520of%2520the%2520logits%252C%2520followed%2520by%2520taking%2520the%2520maximum%250Alogit%2520as%2520the%2520confidence%2520estimator%252C%2520can%2520lead%2520to%2520considerable%2520gains%2520in%2520selective%250Aclassification%2520performance%252C%2520completely%2520fixing%2520the%2520pathological%2520behavior%250Aobserved%2520in%2520many%2520classifiers.%2520As%2520a%2520consequence%252C%2520the%2520selective%2520classification%250Aperformance%2520of%2520any%2520classifier%2520becomes%2520almost%2520entirely%2520determined%2520by%2520its%250Acorresponding%2520accuracy.%2520Moreover%252C%2520these%2520results%2520are%2520shown%2520to%2520be%2520consistent%250Aunder%2520distribution%2520shift.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lfpc/FixSelectiveClassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15508v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Fix%20a%20Broken%20Confidence%20Estimator%3A%20Evaluating%20Post-hoc%20Methods%0A%20%20for%20Selective%20Classification%20with%20Deep%20Neural%20Networks&entry.906535625=Lu%C3%ADs%20Felipe%20P.%20Cattelan%20and%20Danilo%20Silva&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20selective%20classification%20for%20deep%20neural%0Anetworks%2C%20where%20a%20model%20is%20allowed%20to%20abstain%20from%20low-confidence%20predictions%0Ato%20avoid%20potential%20errors.%20We%20focus%20on%20so-called%20post-hoc%20methods%2C%20which%0Areplace%20the%20confidence%20estimator%20of%20a%20given%20classifier%20without%20modifying%20or%0Aretraining%20it%2C%20thus%20being%20practically%20appealing.%20Considering%20neural%20networks%0Awith%20softmax%20outputs%2C%20our%20goal%20is%20to%20identify%20the%20best%20confidence%20estimator%0Athat%20can%20be%20computed%20directly%20from%20the%20unnormalized%20logits.%20This%20problem%20is%0Amotivated%20by%20the%20intriguing%20observation%20in%20recent%20work%20that%20many%20classifiers%0Aappear%20to%20have%20a%20%22broken%22%20confidence%20estimator%2C%20in%20the%20sense%20that%20their%0Aselective%20classification%20performance%20is%20much%20worse%20than%20what%20could%20be%20expected%0Aby%20their%20corresponding%20accuracies.%20We%20perform%20an%20extensive%20experimental%20study%0Aof%20many%20existing%20and%20proposed%20confidence%20estimators%20applied%20to%2084%20pretrained%0AImageNet%20classifiers%20available%20from%20popular%20repositories.%20Our%20results%20show%20that%0Aa%20simple%20%24p%24-norm%20normalization%20of%20the%20logits%2C%20followed%20by%20taking%20the%20maximum%0Alogit%20as%20the%20confidence%20estimator%2C%20can%20lead%20to%20considerable%20gains%20in%20selective%0Aclassification%20performance%2C%20completely%20fixing%20the%20pathological%20behavior%0Aobserved%20in%20many%20classifiers.%20As%20a%20consequence%2C%20the%20selective%20classification%0Aperformance%20of%20any%20classifier%20becomes%20almost%20entirely%20determined%20by%20its%0Acorresponding%20accuracy.%20Moreover%2C%20these%20results%20are%20shown%20to%20be%20consistent%0Aunder%20distribution%20shift.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lfpc/FixSelectiveClassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15508v4&entry.124074799=Read"},
{"title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code", "author": "Marc Oedingen and Raphael C. Engelhardt and Robin Denz and Maximilian Hammer and Wolfgang Konen", "abstract": "  In recent times, large language models (LLMs) have made significant strides\nin generating computer code, blurring the lines between code created by humans\nand code produced by artificial intelligence (AI). As these technologies evolve\nrapidly, it is crucial to explore how they influence code generation,\nespecially given the risk of misuse in areas like higher education. This paper\nexplores this issue by using advanced classification techniques to\ndifferentiate between code written by humans and that generated by ChatGPT, a\ntype of LLM. We employ a new approach that combines powerful embedding features\n(black-box) with supervised learning algorithms - including Deep Neural\nNetworks, Random Forests, and Extreme Gradient Boosting - to achieve this\ndifferentiation with an impressive accuracy of 98%. For the successful\ncombinations, we also examine their model calibration, showing that some of the\nmodels are extremely well calibrated. Additionally, we present white-box\nfeatures and an interpretable Bayes classifier to elucidate critical\ndifferences between the code sources, enhancing the explainability and\ntransparency of our approach. Both approaches work well but provide at most\n85-88% accuracy. We also show that untrained humans solve the same task not\nbetter than random guessing. This study is crucial in understanding and\nmitigating the potential risks associated with using AI in code generation,\nparticularly in the context of higher education, software development, and\ncompetitive programming.\n", "link": "http://arxiv.org/abs/2405.15512v1", "date": "2024-05-24", "relevancy": 2.49, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5451}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4773}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatGPT%20Code%20Detection%3A%20Techniques%20for%20Uncovering%20the%20Source%20of%20Code&body=Title%3A%20ChatGPT%20Code%20Detection%3A%20Techniques%20for%20Uncovering%20the%20Source%20of%20Code%0AAuthor%3A%20Marc%20Oedingen%20and%20Raphael%20C.%20Engelhardt%20and%20Robin%20Denz%20and%20Maximilian%20Hammer%20and%20Wolfgang%20Konen%0AAbstract%3A%20%20%20In%20recent%20times%2C%20large%20language%20models%20%28LLMs%29%20have%20made%20significant%20strides%0Ain%20generating%20computer%20code%2C%20blurring%20the%20lines%20between%20code%20created%20by%20humans%0Aand%20code%20produced%20by%20artificial%20intelligence%20%28AI%29.%20As%20these%20technologies%20evolve%0Arapidly%2C%20it%20is%20crucial%20to%20explore%20how%20they%20influence%20code%20generation%2C%0Aespecially%20given%20the%20risk%20of%20misuse%20in%20areas%20like%20higher%20education.%20This%20paper%0Aexplores%20this%20issue%20by%20using%20advanced%20classification%20techniques%20to%0Adifferentiate%20between%20code%20written%20by%20humans%20and%20that%20generated%20by%20ChatGPT%2C%20a%0Atype%20of%20LLM.%20We%20employ%20a%20new%20approach%20that%20combines%20powerful%20embedding%20features%0A%28black-box%29%20with%20supervised%20learning%20algorithms%20-%20including%20Deep%20Neural%0ANetworks%2C%20Random%20Forests%2C%20and%20Extreme%20Gradient%20Boosting%20-%20to%20achieve%20this%0Adifferentiation%20with%20an%20impressive%20accuracy%20of%2098%25.%20For%20the%20successful%0Acombinations%2C%20we%20also%20examine%20their%20model%20calibration%2C%20showing%20that%20some%20of%20the%0Amodels%20are%20extremely%20well%20calibrated.%20Additionally%2C%20we%20present%20white-box%0Afeatures%20and%20an%20interpretable%20Bayes%20classifier%20to%20elucidate%20critical%0Adifferences%20between%20the%20code%20sources%2C%20enhancing%20the%20explainability%20and%0Atransparency%20of%20our%20approach.%20Both%20approaches%20work%20well%20but%20provide%20at%20most%0A85-88%25%20accuracy.%20We%20also%20show%20that%20untrained%20humans%20solve%20the%20same%20task%20not%0Abetter%20than%20random%20guessing.%20This%20study%20is%20crucial%20in%20understanding%20and%0Amitigating%20the%20potential%20risks%20associated%20with%20using%20AI%20in%20code%20generation%2C%0Aparticularly%20in%20the%20context%20of%20higher%20education%2C%20software%20development%2C%20and%0Acompetitive%20programming.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatGPT%2520Code%2520Detection%253A%2520Techniques%2520for%2520Uncovering%2520the%2520Source%2520of%2520Code%26entry.906535625%3DMarc%2520Oedingen%2520and%2520Raphael%2520C.%2520Engelhardt%2520and%2520Robin%2520Denz%2520and%2520Maximilian%2520Hammer%2520and%2520Wolfgang%2520Konen%26entry.1292438233%3D%2520%2520In%2520recent%2520times%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520strides%250Ain%2520generating%2520computer%2520code%252C%2520blurring%2520the%2520lines%2520between%2520code%2520created%2520by%2520humans%250Aand%2520code%2520produced%2520by%2520artificial%2520intelligence%2520%2528AI%2529.%2520As%2520these%2520technologies%2520evolve%250Arapidly%252C%2520it%2520is%2520crucial%2520to%2520explore%2520how%2520they%2520influence%2520code%2520generation%252C%250Aespecially%2520given%2520the%2520risk%2520of%2520misuse%2520in%2520areas%2520like%2520higher%2520education.%2520This%2520paper%250Aexplores%2520this%2520issue%2520by%2520using%2520advanced%2520classification%2520techniques%2520to%250Adifferentiate%2520between%2520code%2520written%2520by%2520humans%2520and%2520that%2520generated%2520by%2520ChatGPT%252C%2520a%250Atype%2520of%2520LLM.%2520We%2520employ%2520a%2520new%2520approach%2520that%2520combines%2520powerful%2520embedding%2520features%250A%2528black-box%2529%2520with%2520supervised%2520learning%2520algorithms%2520-%2520including%2520Deep%2520Neural%250ANetworks%252C%2520Random%2520Forests%252C%2520and%2520Extreme%2520Gradient%2520Boosting%2520-%2520to%2520achieve%2520this%250Adifferentiation%2520with%2520an%2520impressive%2520accuracy%2520of%252098%2525.%2520For%2520the%2520successful%250Acombinations%252C%2520we%2520also%2520examine%2520their%2520model%2520calibration%252C%2520showing%2520that%2520some%2520of%2520the%250Amodels%2520are%2520extremely%2520well%2520calibrated.%2520Additionally%252C%2520we%2520present%2520white-box%250Afeatures%2520and%2520an%2520interpretable%2520Bayes%2520classifier%2520to%2520elucidate%2520critical%250Adifferences%2520between%2520the%2520code%2520sources%252C%2520enhancing%2520the%2520explainability%2520and%250Atransparency%2520of%2520our%2520approach.%2520Both%2520approaches%2520work%2520well%2520but%2520provide%2520at%2520most%250A85-88%2525%2520accuracy.%2520We%2520also%2520show%2520that%2520untrained%2520humans%2520solve%2520the%2520same%2520task%2520not%250Abetter%2520than%2520random%2520guessing.%2520This%2520study%2520is%2520crucial%2520in%2520understanding%2520and%250Amitigating%2520the%2520potential%2520risks%2520associated%2520with%2520using%2520AI%2520in%2520code%2520generation%252C%250Aparticularly%2520in%2520the%2520context%2520of%2520higher%2520education%252C%2520software%2520development%252C%2520and%250Acompetitive%2520programming.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatGPT%20Code%20Detection%3A%20Techniques%20for%20Uncovering%20the%20Source%20of%20Code&entry.906535625=Marc%20Oedingen%20and%20Raphael%20C.%20Engelhardt%20and%20Robin%20Denz%20and%20Maximilian%20Hammer%20and%20Wolfgang%20Konen&entry.1292438233=%20%20In%20recent%20times%2C%20large%20language%20models%20%28LLMs%29%20have%20made%20significant%20strides%0Ain%20generating%20computer%20code%2C%20blurring%20the%20lines%20between%20code%20created%20by%20humans%0Aand%20code%20produced%20by%20artificial%20intelligence%20%28AI%29.%20As%20these%20technologies%20evolve%0Arapidly%2C%20it%20is%20crucial%20to%20explore%20how%20they%20influence%20code%20generation%2C%0Aespecially%20given%20the%20risk%20of%20misuse%20in%20areas%20like%20higher%20education.%20This%20paper%0Aexplores%20this%20issue%20by%20using%20advanced%20classification%20techniques%20to%0Adifferentiate%20between%20code%20written%20by%20humans%20and%20that%20generated%20by%20ChatGPT%2C%20a%0Atype%20of%20LLM.%20We%20employ%20a%20new%20approach%20that%20combines%20powerful%20embedding%20features%0A%28black-box%29%20with%20supervised%20learning%20algorithms%20-%20including%20Deep%20Neural%0ANetworks%2C%20Random%20Forests%2C%20and%20Extreme%20Gradient%20Boosting%20-%20to%20achieve%20this%0Adifferentiation%20with%20an%20impressive%20accuracy%20of%2098%25.%20For%20the%20successful%0Acombinations%2C%20we%20also%20examine%20their%20model%20calibration%2C%20showing%20that%20some%20of%20the%0Amodels%20are%20extremely%20well%20calibrated.%20Additionally%2C%20we%20present%20white-box%0Afeatures%20and%20an%20interpretable%20Bayes%20classifier%20to%20elucidate%20critical%0Adifferences%20between%20the%20code%20sources%2C%20enhancing%20the%20explainability%20and%0Atransparency%20of%20our%20approach.%20Both%20approaches%20work%20well%20but%20provide%20at%20most%0A85-88%25%20accuracy.%20We%20also%20show%20that%20untrained%20humans%20solve%20the%20same%20task%20not%0Abetter%20than%20random%20guessing.%20This%20study%20is%20crucial%20in%20understanding%20and%0Amitigating%20the%20potential%20risks%20associated%20with%20using%20AI%20in%20code%20generation%2C%0Aparticularly%20in%20the%20context%20of%20higher%20education%2C%20software%20development%2C%20and%0Acompetitive%20programming.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15512v1&entry.124074799=Read"},
{"title": "Identifying Functionally Important Features with End-to-End Sparse\n  Dictionary Learning", "author": "Dan Braun and Jordan Taylor and Nicholas Goldowsky-Dill and Lee Sharkey", "abstract": "  Identifying the features learned by neural networks is a core challenge in\nmechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse,\novercomplete dictionary that reconstructs a network's internal activations,\nhave been used to identify these features. However, SAEs may learn more about\nthe structure of the datatset than the computational structure of the network.\nThere is therefore only indirect reason to believe that the directions found in\nthese dictionaries are functionally important to the network. We propose\nend-to-end (e2e) sparse dictionary learning, a method for training SAEs that\nensures the features learned are functionally important by minimizing the KL\ndivergence between the output distributions of the original model and the model\nwith SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a\nPareto improvement: They explain more network performance, require fewer total\nfeatures, and require fewer simultaneously active features per datapoint, all\nwith no cost to interpretability. We explore geometric and qualitative\ndifferences between e2e SAE features and standard SAE features. E2e dictionary\nlearning brings us closer to methods that can explain network behavior\nconcisely and accurately. We release our library for training e2e SAEs and\nreproducing our analysis at https://github.com/ApolloResearch/e2e_sae\n", "link": "http://arxiv.org/abs/2405.12241v2", "date": "2024-05-24", "relevancy": 2.4764, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5023}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4938}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Functionally%20Important%20Features%20with%20End-to-End%20Sparse%0A%20%20Dictionary%20Learning&body=Title%3A%20Identifying%20Functionally%20Important%20Features%20with%20End-to-End%20Sparse%0A%20%20Dictionary%20Learning%0AAuthor%3A%20Dan%20Braun%20and%20Jordan%20Taylor%20and%20Nicholas%20Goldowsky-Dill%20and%20Lee%20Sharkey%0AAbstract%3A%20%20%20Identifying%20the%20features%20learned%20by%20neural%20networks%20is%20a%20core%20challenge%20in%0Amechanistic%20interpretability.%20Sparse%20autoencoders%20%28SAEs%29%2C%20which%20learn%20a%20sparse%2C%0Aovercomplete%20dictionary%20that%20reconstructs%20a%20network%27s%20internal%20activations%2C%0Ahave%20been%20used%20to%20identify%20these%20features.%20However%2C%20SAEs%20may%20learn%20more%20about%0Athe%20structure%20of%20the%20datatset%20than%20the%20computational%20structure%20of%20the%20network.%0AThere%20is%20therefore%20only%20indirect%20reason%20to%20believe%20that%20the%20directions%20found%20in%0Athese%20dictionaries%20are%20functionally%20important%20to%20the%20network.%20We%20propose%0Aend-to-end%20%28e2e%29%20sparse%20dictionary%20learning%2C%20a%20method%20for%20training%20SAEs%20that%0Aensures%20the%20features%20learned%20are%20functionally%20important%20by%20minimizing%20the%20KL%0Adivergence%20between%20the%20output%20distributions%20of%20the%20original%20model%20and%20the%20model%0Awith%20SAE%20activations%20inserted.%20Compared%20to%20standard%20SAEs%2C%20e2e%20SAEs%20offer%20a%0APareto%20improvement%3A%20They%20explain%20more%20network%20performance%2C%20require%20fewer%20total%0Afeatures%2C%20and%20require%20fewer%20simultaneously%20active%20features%20per%20datapoint%2C%20all%0Awith%20no%20cost%20to%20interpretability.%20We%20explore%20geometric%20and%20qualitative%0Adifferences%20between%20e2e%20SAE%20features%20and%20standard%20SAE%20features.%20E2e%20dictionary%0Alearning%20brings%20us%20closer%20to%20methods%20that%20can%20explain%20network%20behavior%0Aconcisely%20and%20accurately.%20We%20release%20our%20library%20for%20training%20e2e%20SAEs%20and%0Areproducing%20our%20analysis%20at%20https%3A//github.com/ApolloResearch/e2e_sae%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Functionally%2520Important%2520Features%2520with%2520End-to-End%2520Sparse%250A%2520%2520Dictionary%2520Learning%26entry.906535625%3DDan%2520Braun%2520and%2520Jordan%2520Taylor%2520and%2520Nicholas%2520Goldowsky-Dill%2520and%2520Lee%2520Sharkey%26entry.1292438233%3D%2520%2520Identifying%2520the%2520features%2520learned%2520by%2520neural%2520networks%2520is%2520a%2520core%2520challenge%2520in%250Amechanistic%2520interpretability.%2520Sparse%2520autoencoders%2520%2528SAEs%2529%252C%2520which%2520learn%2520a%2520sparse%252C%250Aovercomplete%2520dictionary%2520that%2520reconstructs%2520a%2520network%2527s%2520internal%2520activations%252C%250Ahave%2520been%2520used%2520to%2520identify%2520these%2520features.%2520However%252C%2520SAEs%2520may%2520learn%2520more%2520about%250Athe%2520structure%2520of%2520the%2520datatset%2520than%2520the%2520computational%2520structure%2520of%2520the%2520network.%250AThere%2520is%2520therefore%2520only%2520indirect%2520reason%2520to%2520believe%2520that%2520the%2520directions%2520found%2520in%250Athese%2520dictionaries%2520are%2520functionally%2520important%2520to%2520the%2520network.%2520We%2520propose%250Aend-to-end%2520%2528e2e%2529%2520sparse%2520dictionary%2520learning%252C%2520a%2520method%2520for%2520training%2520SAEs%2520that%250Aensures%2520the%2520features%2520learned%2520are%2520functionally%2520important%2520by%2520minimizing%2520the%2520KL%250Adivergence%2520between%2520the%2520output%2520distributions%2520of%2520the%2520original%2520model%2520and%2520the%2520model%250Awith%2520SAE%2520activations%2520inserted.%2520Compared%2520to%2520standard%2520SAEs%252C%2520e2e%2520SAEs%2520offer%2520a%250APareto%2520improvement%253A%2520They%2520explain%2520more%2520network%2520performance%252C%2520require%2520fewer%2520total%250Afeatures%252C%2520and%2520require%2520fewer%2520simultaneously%2520active%2520features%2520per%2520datapoint%252C%2520all%250Awith%2520no%2520cost%2520to%2520interpretability.%2520We%2520explore%2520geometric%2520and%2520qualitative%250Adifferences%2520between%2520e2e%2520SAE%2520features%2520and%2520standard%2520SAE%2520features.%2520E2e%2520dictionary%250Alearning%2520brings%2520us%2520closer%2520to%2520methods%2520that%2520can%2520explain%2520network%2520behavior%250Aconcisely%2520and%2520accurately.%2520We%2520release%2520our%2520library%2520for%2520training%2520e2e%2520SAEs%2520and%250Areproducing%2520our%2520analysis%2520at%2520https%253A//github.com/ApolloResearch/e2e_sae%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Functionally%20Important%20Features%20with%20End-to-End%20Sparse%0A%20%20Dictionary%20Learning&entry.906535625=Dan%20Braun%20and%20Jordan%20Taylor%20and%20Nicholas%20Goldowsky-Dill%20and%20Lee%20Sharkey&entry.1292438233=%20%20Identifying%20the%20features%20learned%20by%20neural%20networks%20is%20a%20core%20challenge%20in%0Amechanistic%20interpretability.%20Sparse%20autoencoders%20%28SAEs%29%2C%20which%20learn%20a%20sparse%2C%0Aovercomplete%20dictionary%20that%20reconstructs%20a%20network%27s%20internal%20activations%2C%0Ahave%20been%20used%20to%20identify%20these%20features.%20However%2C%20SAEs%20may%20learn%20more%20about%0Athe%20structure%20of%20the%20datatset%20than%20the%20computational%20structure%20of%20the%20network.%0AThere%20is%20therefore%20only%20indirect%20reason%20to%20believe%20that%20the%20directions%20found%20in%0Athese%20dictionaries%20are%20functionally%20important%20to%20the%20network.%20We%20propose%0Aend-to-end%20%28e2e%29%20sparse%20dictionary%20learning%2C%20a%20method%20for%20training%20SAEs%20that%0Aensures%20the%20features%20learned%20are%20functionally%20important%20by%20minimizing%20the%20KL%0Adivergence%20between%20the%20output%20distributions%20of%20the%20original%20model%20and%20the%20model%0Awith%20SAE%20activations%20inserted.%20Compared%20to%20standard%20SAEs%2C%20e2e%20SAEs%20offer%20a%0APareto%20improvement%3A%20They%20explain%20more%20network%20performance%2C%20require%20fewer%20total%0Afeatures%2C%20and%20require%20fewer%20simultaneously%20active%20features%20per%20datapoint%2C%20all%0Awith%20no%20cost%20to%20interpretability.%20We%20explore%20geometric%20and%20qualitative%0Adifferences%20between%20e2e%20SAE%20features%20and%20standard%20SAE%20features.%20E2e%20dictionary%0Alearning%20brings%20us%20closer%20to%20methods%20that%20can%20explain%20network%20behavior%0Aconcisely%20and%20accurately.%20We%20release%20our%20library%20for%20training%20e2e%20SAEs%20and%0Areproducing%20our%20analysis%20at%20https%3A//github.com/ApolloResearch/e2e_sae%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12241v2&entry.124074799=Read"},
{"title": "Learning from Linear Algebra: A Graph Neural Network Approach to\n  Preconditioner Design for Conjugate Gradient Solvers", "author": "Vladislav Trifonov and Alexander Rudikov and Oleg Iliev and Ivan Oseledets and Ekaterina Muravleva", "abstract": "  Large linear systems are ubiquitous in modern computational science. The main\nrecipe for solving them is iterative solvers with well-designed\npreconditioners. Deep learning models may be used to precondition residuals\nduring iteration of such linear solvers as the conjugate gradient (CG) method.\nNeural network models require an enormous number of parameters to approximate\nwell in this setup. Another approach is to take advantage of small graph neural\nnetworks (GNNs) to construct preconditioners of the predefined sparsity\npattern. In our work, we recall well-established preconditioners from linear\nalgebra and use them as a starting point for training the GNN. Numerical\nexperiments demonstrate that our approach outperforms both classical methods\nand neural network-based preconditioning. We also provide a heuristic\njustification for the loss function used and validate our approach on complex\ndatasets.\n", "link": "http://arxiv.org/abs/2405.15557v1", "date": "2024-05-24", "relevancy": 2.4712, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.517}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.497}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Linear%20Algebra%3A%20A%20Graph%20Neural%20Network%20Approach%20to%0A%20%20Preconditioner%20Design%20for%20Conjugate%20Gradient%20Solvers&body=Title%3A%20Learning%20from%20Linear%20Algebra%3A%20A%20Graph%20Neural%20Network%20Approach%20to%0A%20%20Preconditioner%20Design%20for%20Conjugate%20Gradient%20Solvers%0AAuthor%3A%20Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Oleg%20Iliev%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva%0AAbstract%3A%20%20%20Large%20linear%20systems%20are%20ubiquitous%20in%20modern%20computational%20science.%20The%20main%0Arecipe%20for%20solving%20them%20is%20iterative%20solvers%20with%20well-designed%0Apreconditioners.%20Deep%20learning%20models%20may%20be%20used%20to%20precondition%20residuals%0Aduring%20iteration%20of%20such%20linear%20solvers%20as%20the%20conjugate%20gradient%20%28CG%29%20method.%0ANeural%20network%20models%20require%20an%20enormous%20number%20of%20parameters%20to%20approximate%0Awell%20in%20this%20setup.%20Another%20approach%20is%20to%20take%20advantage%20of%20small%20graph%20neural%0Anetworks%20%28GNNs%29%20to%20construct%20preconditioners%20of%20the%20predefined%20sparsity%0Apattern.%20In%20our%20work%2C%20we%20recall%20well-established%20preconditioners%20from%20linear%0Aalgebra%20and%20use%20them%20as%20a%20starting%20point%20for%20training%20the%20GNN.%20Numerical%0Aexperiments%20demonstrate%20that%20our%20approach%20outperforms%20both%20classical%20methods%0Aand%20neural%20network-based%20preconditioning.%20We%20also%20provide%20a%20heuristic%0Ajustification%20for%20the%20loss%20function%20used%20and%20validate%20our%20approach%20on%20complex%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Linear%2520Algebra%253A%2520A%2520Graph%2520Neural%2520Network%2520Approach%2520to%250A%2520%2520Preconditioner%2520Design%2520for%2520Conjugate%2520Gradient%2520Solvers%26entry.906535625%3DVladislav%2520Trifonov%2520and%2520Alexander%2520Rudikov%2520and%2520Oleg%2520Iliev%2520and%2520Ivan%2520Oseledets%2520and%2520Ekaterina%2520Muravleva%26entry.1292438233%3D%2520%2520Large%2520linear%2520systems%2520are%2520ubiquitous%2520in%2520modern%2520computational%2520science.%2520The%2520main%250Arecipe%2520for%2520solving%2520them%2520is%2520iterative%2520solvers%2520with%2520well-designed%250Apreconditioners.%2520Deep%2520learning%2520models%2520may%2520be%2520used%2520to%2520precondition%2520residuals%250Aduring%2520iteration%2520of%2520such%2520linear%2520solvers%2520as%2520the%2520conjugate%2520gradient%2520%2528CG%2529%2520method.%250ANeural%2520network%2520models%2520require%2520an%2520enormous%2520number%2520of%2520parameters%2520to%2520approximate%250Awell%2520in%2520this%2520setup.%2520Another%2520approach%2520is%2520to%2520take%2520advantage%2520of%2520small%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%2520to%2520construct%2520preconditioners%2520of%2520the%2520predefined%2520sparsity%250Apattern.%2520In%2520our%2520work%252C%2520we%2520recall%2520well-established%2520preconditioners%2520from%2520linear%250Aalgebra%2520and%2520use%2520them%2520as%2520a%2520starting%2520point%2520for%2520training%2520the%2520GNN.%2520Numerical%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520both%2520classical%2520methods%250Aand%2520neural%2520network-based%2520preconditioning.%2520We%2520also%2520provide%2520a%2520heuristic%250Ajustification%2520for%2520the%2520loss%2520function%2520used%2520and%2520validate%2520our%2520approach%2520on%2520complex%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Linear%20Algebra%3A%20A%20Graph%20Neural%20Network%20Approach%20to%0A%20%20Preconditioner%20Design%20for%20Conjugate%20Gradient%20Solvers&entry.906535625=Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Oleg%20Iliev%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva&entry.1292438233=%20%20Large%20linear%20systems%20are%20ubiquitous%20in%20modern%20computational%20science.%20The%20main%0Arecipe%20for%20solving%20them%20is%20iterative%20solvers%20with%20well-designed%0Apreconditioners.%20Deep%20learning%20models%20may%20be%20used%20to%20precondition%20residuals%0Aduring%20iteration%20of%20such%20linear%20solvers%20as%20the%20conjugate%20gradient%20%28CG%29%20method.%0ANeural%20network%20models%20require%20an%20enormous%20number%20of%20parameters%20to%20approximate%0Awell%20in%20this%20setup.%20Another%20approach%20is%20to%20take%20advantage%20of%20small%20graph%20neural%0Anetworks%20%28GNNs%29%20to%20construct%20preconditioners%20of%20the%20predefined%20sparsity%0Apattern.%20In%20our%20work%2C%20we%20recall%20well-established%20preconditioners%20from%20linear%0Aalgebra%20and%20use%20them%20as%20a%20starting%20point%20for%20training%20the%20GNN.%20Numerical%0Aexperiments%20demonstrate%20that%20our%20approach%20outperforms%20both%20classical%20methods%0Aand%20neural%20network-based%20preconditioning.%20We%20also%20provide%20a%20heuristic%0Ajustification%20for%20the%20loss%20function%20used%20and%20validate%20our%20approach%20on%20complex%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15557v1&entry.124074799=Read"},
{"title": "Visual Echoes: A Simple Unified Transformer for Audio-Visual Generation", "author": "Shiqi Yang and Zhi Zhong and Mengjie Zhao and Shusuke Takahashi and Masato Ishii and Takashi Shibuya and Yuki Mitsufuji", "abstract": "  In recent years, with the realistic generation results and a wide range of\npersonalized applications, diffusion-based generative models gain huge\nattention in both visual and audio generation areas. Compared to the\nconsiderable advancements of text2image or text2audio generation, research in\naudio2visual or visual2audio generation has been relatively slow. The recent\naudio-visual generation methods usually resort to huge large language model or\ncomposable diffusion models. Instead of designing another giant model for\naudio-visual generation, in this paper we take a step back showing a simple and\nlightweight generative transformer, which is not fully investigated in\nmulti-modal generation, can achieve excellent results on image2audio\ngeneration. The transformer operates in the discrete audio and visual\nVector-Quantized GAN space, and is trained in the mask denoising manner. After\ntraining, the classifier-free guidance could be deployed off-the-shelf\nachieving better performance, without any extra training or modification. Since\nthe transformer model is modality symmetrical, it could also be directly\ndeployed for audio2image generation and co-generation. In the experiments, we\nshow that our simple method surpasses recent image2audio generation methods.\nGenerated audio samples can be found at\nhttps://docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ/\n", "link": "http://arxiv.org/abs/2405.14598v2", "date": "2024-05-24", "relevancy": 2.4672, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6591}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6187}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Echoes%3A%20A%20Simple%20Unified%20Transformer%20for%20Audio-Visual%20Generation&body=Title%3A%20Visual%20Echoes%3A%20A%20Simple%20Unified%20Transformer%20for%20Audio-Visual%20Generation%0AAuthor%3A%20Shiqi%20Yang%20and%20Zhi%20Zhong%20and%20Mengjie%20Zhao%20and%20Shusuke%20Takahashi%20and%20Masato%20Ishii%20and%20Takashi%20Shibuya%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20In%20recent%20years%2C%20with%20the%20realistic%20generation%20results%20and%20a%20wide%20range%20of%0Apersonalized%20applications%2C%20diffusion-based%20generative%20models%20gain%20huge%0Aattention%20in%20both%20visual%20and%20audio%20generation%20areas.%20Compared%20to%20the%0Aconsiderable%20advancements%20of%20text2image%20or%20text2audio%20generation%2C%20research%20in%0Aaudio2visual%20or%20visual2audio%20generation%20has%20been%20relatively%20slow.%20The%20recent%0Aaudio-visual%20generation%20methods%20usually%20resort%20to%20huge%20large%20language%20model%20or%0Acomposable%20diffusion%20models.%20Instead%20of%20designing%20another%20giant%20model%20for%0Aaudio-visual%20generation%2C%20in%20this%20paper%20we%20take%20a%20step%20back%20showing%20a%20simple%20and%0Alightweight%20generative%20transformer%2C%20which%20is%20not%20fully%20investigated%20in%0Amulti-modal%20generation%2C%20can%20achieve%20excellent%20results%20on%20image2audio%0Ageneration.%20The%20transformer%20operates%20in%20the%20discrete%20audio%20and%20visual%0AVector-Quantized%20GAN%20space%2C%20and%20is%20trained%20in%20the%20mask%20denoising%20manner.%20After%0Atraining%2C%20the%20classifier-free%20guidance%20could%20be%20deployed%20off-the-shelf%0Aachieving%20better%20performance%2C%20without%20any%20extra%20training%20or%20modification.%20Since%0Athe%20transformer%20model%20is%20modality%20symmetrical%2C%20it%20could%20also%20be%20directly%0Adeployed%20for%20audio2image%20generation%20and%20co-generation.%20In%20the%20experiments%2C%20we%0Ashow%20that%20our%20simple%20method%20surpasses%20recent%20image2audio%20generation%20methods.%0AGenerated%20audio%20samples%20can%20be%20found%20at%0Ahttps%3A//docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14598v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Echoes%253A%2520A%2520Simple%2520Unified%2520Transformer%2520for%2520Audio-Visual%2520Generation%26entry.906535625%3DShiqi%2520Yang%2520and%2520Zhi%2520Zhong%2520and%2520Mengjie%2520Zhao%2520and%2520Shusuke%2520Takahashi%2520and%2520Masato%2520Ishii%2520and%2520Takashi%2520Shibuya%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520with%2520the%2520realistic%2520generation%2520results%2520and%2520a%2520wide%2520range%2520of%250Apersonalized%2520applications%252C%2520diffusion-based%2520generative%2520models%2520gain%2520huge%250Aattention%2520in%2520both%2520visual%2520and%2520audio%2520generation%2520areas.%2520Compared%2520to%2520the%250Aconsiderable%2520advancements%2520of%2520text2image%2520or%2520text2audio%2520generation%252C%2520research%2520in%250Aaudio2visual%2520or%2520visual2audio%2520generation%2520has%2520been%2520relatively%2520slow.%2520The%2520recent%250Aaudio-visual%2520generation%2520methods%2520usually%2520resort%2520to%2520huge%2520large%2520language%2520model%2520or%250Acomposable%2520diffusion%2520models.%2520Instead%2520of%2520designing%2520another%2520giant%2520model%2520for%250Aaudio-visual%2520generation%252C%2520in%2520this%2520paper%2520we%2520take%2520a%2520step%2520back%2520showing%2520a%2520simple%2520and%250Alightweight%2520generative%2520transformer%252C%2520which%2520is%2520not%2520fully%2520investigated%2520in%250Amulti-modal%2520generation%252C%2520can%2520achieve%2520excellent%2520results%2520on%2520image2audio%250Ageneration.%2520The%2520transformer%2520operates%2520in%2520the%2520discrete%2520audio%2520and%2520visual%250AVector-Quantized%2520GAN%2520space%252C%2520and%2520is%2520trained%2520in%2520the%2520mask%2520denoising%2520manner.%2520After%250Atraining%252C%2520the%2520classifier-free%2520guidance%2520could%2520be%2520deployed%2520off-the-shelf%250Aachieving%2520better%2520performance%252C%2520without%2520any%2520extra%2520training%2520or%2520modification.%2520Since%250Athe%2520transformer%2520model%2520is%2520modality%2520symmetrical%252C%2520it%2520could%2520also%2520be%2520directly%250Adeployed%2520for%2520audio2image%2520generation%2520and%2520co-generation.%2520In%2520the%2520experiments%252C%2520we%250Ashow%2520that%2520our%2520simple%2520method%2520surpasses%2520recent%2520image2audio%2520generation%2520methods.%250AGenerated%2520audio%2520samples%2520can%2520be%2520found%2520at%250Ahttps%253A//docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14598v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Echoes%3A%20A%20Simple%20Unified%20Transformer%20for%20Audio-Visual%20Generation&entry.906535625=Shiqi%20Yang%20and%20Zhi%20Zhong%20and%20Mengjie%20Zhao%20and%20Shusuke%20Takahashi%20and%20Masato%20Ishii%20and%20Takashi%20Shibuya%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20In%20recent%20years%2C%20with%20the%20realistic%20generation%20results%20and%20a%20wide%20range%20of%0Apersonalized%20applications%2C%20diffusion-based%20generative%20models%20gain%20huge%0Aattention%20in%20both%20visual%20and%20audio%20generation%20areas.%20Compared%20to%20the%0Aconsiderable%20advancements%20of%20text2image%20or%20text2audio%20generation%2C%20research%20in%0Aaudio2visual%20or%20visual2audio%20generation%20has%20been%20relatively%20slow.%20The%20recent%0Aaudio-visual%20generation%20methods%20usually%20resort%20to%20huge%20large%20language%20model%20or%0Acomposable%20diffusion%20models.%20Instead%20of%20designing%20another%20giant%20model%20for%0Aaudio-visual%20generation%2C%20in%20this%20paper%20we%20take%20a%20step%20back%20showing%20a%20simple%20and%0Alightweight%20generative%20transformer%2C%20which%20is%20not%20fully%20investigated%20in%0Amulti-modal%20generation%2C%20can%20achieve%20excellent%20results%20on%20image2audio%0Ageneration.%20The%20transformer%20operates%20in%20the%20discrete%20audio%20and%20visual%0AVector-Quantized%20GAN%20space%2C%20and%20is%20trained%20in%20the%20mask%20denoising%20manner.%20After%0Atraining%2C%20the%20classifier-free%20guidance%20could%20be%20deployed%20off-the-shelf%0Aachieving%20better%20performance%2C%20without%20any%20extra%20training%20or%20modification.%20Since%0Athe%20transformer%20model%20is%20modality%20symmetrical%2C%20it%20could%20also%20be%20directly%0Adeployed%20for%20audio2image%20generation%20and%20co-generation.%20In%20the%20experiments%2C%20we%0Ashow%20that%20our%20simple%20method%20surpasses%20recent%20image2audio%20generation%20methods.%0AGenerated%20audio%20samples%20can%20be%20found%20at%0Ahttps%3A//docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14598v2&entry.124074799=Read"},
{"title": "DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based\n  Dense Incident Map Generation", "author": "Xiankang He and Guangkai Xu and Bo Zhang and Hao Chen and Ying Cui and Dongyan Guo", "abstract": "  Monocular camera calibration is a key precondition for numerous 3D vision\napplications. Despite considerable advancements, existing methods often hinge\non specific assumptions and struggle to generalize across varied real-world\nscenarios, and the performance is limited by insufficient training data.\nRecently, diffusion models trained on expansive datasets have been confirmed to\nmaintain the capability to generate diverse, high-quality images. This success\nsuggests a strong potential of the models to effectively understand varied\nvisual information. In this work, we leverage the comprehensive visual\nknowledge embedded in pre-trained diffusion models to enable more robust and\naccurate monocular camera intrinsic estimation. Specifically, we reformulate\nthe problem of estimating the four degrees of freedom (4-DoF) of camera\nintrinsic parameters as a dense incident map generation task. The map details\nthe angle of incidence for each pixel in the RGB image, and its format aligns\nwell with the paradigm of diffusion models. The camera intrinsic then can be\nderived from the incident map with a simple non-learning RANSAC algorithm\nduring inference. Moreover, to further enhance the performance, we jointly\nestimate a depth map to provide extra geometric information for the incident\nmap estimation. Extensive experiments on multiple testing datasets demonstrate\nthat our model achieves state-of-the-art performance, gaining up to a 40%\nreduction in prediction errors. Besides, the experiments also show that the\nprecise camera intrinsic and depth maps estimated by our pipeline can greatly\nbenefit practical applications such as 3D reconstruction from a single\nin-the-wild image.\n", "link": "http://arxiv.org/abs/2405.15619v1", "date": "2024-05-24", "relevancy": 2.4591, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6134}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffCalib%3A%20Reformulating%20Monocular%20Camera%20Calibration%20as%20Diffusion-Based%0A%20%20Dense%20Incident%20Map%20Generation&body=Title%3A%20DiffCalib%3A%20Reformulating%20Monocular%20Camera%20Calibration%20as%20Diffusion-Based%0A%20%20Dense%20Incident%20Map%20Generation%0AAuthor%3A%20Xiankang%20He%20and%20Guangkai%20Xu%20and%20Bo%20Zhang%20and%20Hao%20Chen%20and%20Ying%20Cui%20and%20Dongyan%20Guo%0AAbstract%3A%20%20%20Monocular%20camera%20calibration%20is%20a%20key%20precondition%20for%20numerous%203D%20vision%0Aapplications.%20Despite%20considerable%20advancements%2C%20existing%20methods%20often%20hinge%0Aon%20specific%20assumptions%20and%20struggle%20to%20generalize%20across%20varied%20real-world%0Ascenarios%2C%20and%20the%20performance%20is%20limited%20by%20insufficient%20training%20data.%0ARecently%2C%20diffusion%20models%20trained%20on%20expansive%20datasets%20have%20been%20confirmed%20to%0Amaintain%20the%20capability%20to%20generate%20diverse%2C%20high-quality%20images.%20This%20success%0Asuggests%20a%20strong%20potential%20of%20the%20models%20to%20effectively%20understand%20varied%0Avisual%20information.%20In%20this%20work%2C%20we%20leverage%20the%20comprehensive%20visual%0Aknowledge%20embedded%20in%20pre-trained%20diffusion%20models%20to%20enable%20more%20robust%20and%0Aaccurate%20monocular%20camera%20intrinsic%20estimation.%20Specifically%2C%20we%20reformulate%0Athe%20problem%20of%20estimating%20the%20four%20degrees%20of%20freedom%20%284-DoF%29%20of%20camera%0Aintrinsic%20parameters%20as%20a%20dense%20incident%20map%20generation%20task.%20The%20map%20details%0Athe%20angle%20of%20incidence%20for%20each%20pixel%20in%20the%20RGB%20image%2C%20and%20its%20format%20aligns%0Awell%20with%20the%20paradigm%20of%20diffusion%20models.%20The%20camera%20intrinsic%20then%20can%20be%0Aderived%20from%20the%20incident%20map%20with%20a%20simple%20non-learning%20RANSAC%20algorithm%0Aduring%20inference.%20Moreover%2C%20to%20further%20enhance%20the%20performance%2C%20we%20jointly%0Aestimate%20a%20depth%20map%20to%20provide%20extra%20geometric%20information%20for%20the%20incident%0Amap%20estimation.%20Extensive%20experiments%20on%20multiple%20testing%20datasets%20demonstrate%0Athat%20our%20model%20achieves%20state-of-the-art%20performance%2C%20gaining%20up%20to%20a%2040%25%0Areduction%20in%20prediction%20errors.%20Besides%2C%20the%20experiments%20also%20show%20that%20the%0Aprecise%20camera%20intrinsic%20and%20depth%20maps%20estimated%20by%20our%20pipeline%20can%20greatly%0Abenefit%20practical%20applications%20such%20as%203D%20reconstruction%20from%20a%20single%0Ain-the-wild%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffCalib%253A%2520Reformulating%2520Monocular%2520Camera%2520Calibration%2520as%2520Diffusion-Based%250A%2520%2520Dense%2520Incident%2520Map%2520Generation%26entry.906535625%3DXiankang%2520He%2520and%2520Guangkai%2520Xu%2520and%2520Bo%2520Zhang%2520and%2520Hao%2520Chen%2520and%2520Ying%2520Cui%2520and%2520Dongyan%2520Guo%26entry.1292438233%3D%2520%2520Monocular%2520camera%2520calibration%2520is%2520a%2520key%2520precondition%2520for%2520numerous%25203D%2520vision%250Aapplications.%2520Despite%2520considerable%2520advancements%252C%2520existing%2520methods%2520often%2520hinge%250Aon%2520specific%2520assumptions%2520and%2520struggle%2520to%2520generalize%2520across%2520varied%2520real-world%250Ascenarios%252C%2520and%2520the%2520performance%2520is%2520limited%2520by%2520insufficient%2520training%2520data.%250ARecently%252C%2520diffusion%2520models%2520trained%2520on%2520expansive%2520datasets%2520have%2520been%2520confirmed%2520to%250Amaintain%2520the%2520capability%2520to%2520generate%2520diverse%252C%2520high-quality%2520images.%2520This%2520success%250Asuggests%2520a%2520strong%2520potential%2520of%2520the%2520models%2520to%2520effectively%2520understand%2520varied%250Avisual%2520information.%2520In%2520this%2520work%252C%2520we%2520leverage%2520the%2520comprehensive%2520visual%250Aknowledge%2520embedded%2520in%2520pre-trained%2520diffusion%2520models%2520to%2520enable%2520more%2520robust%2520and%250Aaccurate%2520monocular%2520camera%2520intrinsic%2520estimation.%2520Specifically%252C%2520we%2520reformulate%250Athe%2520problem%2520of%2520estimating%2520the%2520four%2520degrees%2520of%2520freedom%2520%25284-DoF%2529%2520of%2520camera%250Aintrinsic%2520parameters%2520as%2520a%2520dense%2520incident%2520map%2520generation%2520task.%2520The%2520map%2520details%250Athe%2520angle%2520of%2520incidence%2520for%2520each%2520pixel%2520in%2520the%2520RGB%2520image%252C%2520and%2520its%2520format%2520aligns%250Awell%2520with%2520the%2520paradigm%2520of%2520diffusion%2520models.%2520The%2520camera%2520intrinsic%2520then%2520can%2520be%250Aderived%2520from%2520the%2520incident%2520map%2520with%2520a%2520simple%2520non-learning%2520RANSAC%2520algorithm%250Aduring%2520inference.%2520Moreover%252C%2520to%2520further%2520enhance%2520the%2520performance%252C%2520we%2520jointly%250Aestimate%2520a%2520depth%2520map%2520to%2520provide%2520extra%2520geometric%2520information%2520for%2520the%2520incident%250Amap%2520estimation.%2520Extensive%2520experiments%2520on%2520multiple%2520testing%2520datasets%2520demonstrate%250Athat%2520our%2520model%2520achieves%2520state-of-the-art%2520performance%252C%2520gaining%2520up%2520to%2520a%252040%2525%250Areduction%2520in%2520prediction%2520errors.%2520Besides%252C%2520the%2520experiments%2520also%2520show%2520that%2520the%250Aprecise%2520camera%2520intrinsic%2520and%2520depth%2520maps%2520estimated%2520by%2520our%2520pipeline%2520can%2520greatly%250Abenefit%2520practical%2520applications%2520such%2520as%25203D%2520reconstruction%2520from%2520a%2520single%250Ain-the-wild%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffCalib%3A%20Reformulating%20Monocular%20Camera%20Calibration%20as%20Diffusion-Based%0A%20%20Dense%20Incident%20Map%20Generation&entry.906535625=Xiankang%20He%20and%20Guangkai%20Xu%20and%20Bo%20Zhang%20and%20Hao%20Chen%20and%20Ying%20Cui%20and%20Dongyan%20Guo&entry.1292438233=%20%20Monocular%20camera%20calibration%20is%20a%20key%20precondition%20for%20numerous%203D%20vision%0Aapplications.%20Despite%20considerable%20advancements%2C%20existing%20methods%20often%20hinge%0Aon%20specific%20assumptions%20and%20struggle%20to%20generalize%20across%20varied%20real-world%0Ascenarios%2C%20and%20the%20performance%20is%20limited%20by%20insufficient%20training%20data.%0ARecently%2C%20diffusion%20models%20trained%20on%20expansive%20datasets%20have%20been%20confirmed%20to%0Amaintain%20the%20capability%20to%20generate%20diverse%2C%20high-quality%20images.%20This%20success%0Asuggests%20a%20strong%20potential%20of%20the%20models%20to%20effectively%20understand%20varied%0Avisual%20information.%20In%20this%20work%2C%20we%20leverage%20the%20comprehensive%20visual%0Aknowledge%20embedded%20in%20pre-trained%20diffusion%20models%20to%20enable%20more%20robust%20and%0Aaccurate%20monocular%20camera%20intrinsic%20estimation.%20Specifically%2C%20we%20reformulate%0Athe%20problem%20of%20estimating%20the%20four%20degrees%20of%20freedom%20%284-DoF%29%20of%20camera%0Aintrinsic%20parameters%20as%20a%20dense%20incident%20map%20generation%20task.%20The%20map%20details%0Athe%20angle%20of%20incidence%20for%20each%20pixel%20in%20the%20RGB%20image%2C%20and%20its%20format%20aligns%0Awell%20with%20the%20paradigm%20of%20diffusion%20models.%20The%20camera%20intrinsic%20then%20can%20be%0Aderived%20from%20the%20incident%20map%20with%20a%20simple%20non-learning%20RANSAC%20algorithm%0Aduring%20inference.%20Moreover%2C%20to%20further%20enhance%20the%20performance%2C%20we%20jointly%0Aestimate%20a%20depth%20map%20to%20provide%20extra%20geometric%20information%20for%20the%20incident%0Amap%20estimation.%20Extensive%20experiments%20on%20multiple%20testing%20datasets%20demonstrate%0Athat%20our%20model%20achieves%20state-of-the-art%20performance%2C%20gaining%20up%20to%20a%2040%25%0Areduction%20in%20prediction%20errors.%20Besides%2C%20the%20experiments%20also%20show%20that%20the%0Aprecise%20camera%20intrinsic%20and%20depth%20maps%20estimated%20by%20our%20pipeline%20can%20greatly%0Abenefit%20practical%20applications%20such%20as%203D%20reconstruction%20from%20a%20single%0Ain-the-wild%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15619v1&entry.124074799=Read"},
{"title": "FedAWARE: Maximizing Gradient Diversity for Heterogeneous Federated\n  Server-side Optimization", "author": "Dun Zeng and Zenglin Xu and Yu Pan and Qifan Wang and Xiaoying Tang", "abstract": "  Federated learning (FL) is a distributed learning framework where numerous\nclients collaborate with a central server to train a model without sharing\nlocal data. However, the standard federated optimization in real-world\napplications faces both statistical and system heterogeneity challenges, which\nresult in unfavorable convergence behavior. The previous works attempted to\nmodify the local training process (client-side) to tackle heterogeneity\nchallenges. However, they ignored that the updates on the server side can\ncoordinate the diverse local updates efficiently. This work explores the effect\nof server-side updates against heterogeneity issues. We first introduce the\ngradient diversity maximization direction findings, suggesting the global model\nmoves continuously in this direction for fast and stable convergence. Then, we\nderive a novel server-side optimizer \\textsc{FedAWARE} with rigorous\nconvergence analysis for general non-convex settings. Our extensive experiments\nacross multiple heterogeneous federated settings using four datasets showcase\nthat \\textsc{FedAWARE} achieves competitive convergence performance in\ncomparison to state-of-the-art adaptive federated optimizers. Furthermore, our\nresults show that \\textsc{FedAWARE} can enhance the performance of FL\nalgorithms as a plug-in module. Our source code is available at\n\\url{https://github.com/dunzeng/FedAWARE}.\n", "link": "http://arxiv.org/abs/2310.02702v3", "date": "2024-05-24", "relevancy": 2.4344, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5107}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4835}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedAWARE%3A%20Maximizing%20Gradient%20Diversity%20for%20Heterogeneous%20Federated%0A%20%20Server-side%20Optimization&body=Title%3A%20FedAWARE%3A%20Maximizing%20Gradient%20Diversity%20for%20Heterogeneous%20Federated%0A%20%20Server-side%20Optimization%0AAuthor%3A%20Dun%20Zeng%20and%20Zenglin%20Xu%20and%20Yu%20Pan%20and%20Qifan%20Wang%20and%20Xiaoying%20Tang%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20distributed%20learning%20framework%20where%20numerous%0Aclients%20collaborate%20with%20a%20central%20server%20to%20train%20a%20model%20without%20sharing%0Alocal%20data.%20However%2C%20the%20standard%20federated%20optimization%20in%20real-world%0Aapplications%20faces%20both%20statistical%20and%20system%20heterogeneity%20challenges%2C%20which%0Aresult%20in%20unfavorable%20convergence%20behavior.%20The%20previous%20works%20attempted%20to%0Amodify%20the%20local%20training%20process%20%28client-side%29%20to%20tackle%20heterogeneity%0Achallenges.%20However%2C%20they%20ignored%20that%20the%20updates%20on%20the%20server%20side%20can%0Acoordinate%20the%20diverse%20local%20updates%20efficiently.%20This%20work%20explores%20the%20effect%0Aof%20server-side%20updates%20against%20heterogeneity%20issues.%20We%20first%20introduce%20the%0Agradient%20diversity%20maximization%20direction%20findings%2C%20suggesting%20the%20global%20model%0Amoves%20continuously%20in%20this%20direction%20for%20fast%20and%20stable%20convergence.%20Then%2C%20we%0Aderive%20a%20novel%20server-side%20optimizer%20%5Ctextsc%7BFedAWARE%7D%20with%20rigorous%0Aconvergence%20analysis%20for%20general%20non-convex%20settings.%20Our%20extensive%20experiments%0Aacross%20multiple%20heterogeneous%20federated%20settings%20using%20four%20datasets%20showcase%0Athat%20%5Ctextsc%7BFedAWARE%7D%20achieves%20competitive%20convergence%20performance%20in%0Acomparison%20to%20state-of-the-art%20adaptive%20federated%20optimizers.%20Furthermore%2C%20our%0Aresults%20show%20that%20%5Ctextsc%7BFedAWARE%7D%20can%20enhance%20the%20performance%20of%20FL%0Aalgorithms%20as%20a%20plug-in%20module.%20Our%20source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/dunzeng/FedAWARE%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02702v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedAWARE%253A%2520Maximizing%2520Gradient%2520Diversity%2520for%2520Heterogeneous%2520Federated%250A%2520%2520Server-side%2520Optimization%26entry.906535625%3DDun%2520Zeng%2520and%2520Zenglin%2520Xu%2520and%2520Yu%2520Pan%2520and%2520Qifan%2520Wang%2520and%2520Xiaoying%2520Tang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520distributed%2520learning%2520framework%2520where%2520numerous%250Aclients%2520collaborate%2520with%2520a%2520central%2520server%2520to%2520train%2520a%2520model%2520without%2520sharing%250Alocal%2520data.%2520However%252C%2520the%2520standard%2520federated%2520optimization%2520in%2520real-world%250Aapplications%2520faces%2520both%2520statistical%2520and%2520system%2520heterogeneity%2520challenges%252C%2520which%250Aresult%2520in%2520unfavorable%2520convergence%2520behavior.%2520The%2520previous%2520works%2520attempted%2520to%250Amodify%2520the%2520local%2520training%2520process%2520%2528client-side%2529%2520to%2520tackle%2520heterogeneity%250Achallenges.%2520However%252C%2520they%2520ignored%2520that%2520the%2520updates%2520on%2520the%2520server%2520side%2520can%250Acoordinate%2520the%2520diverse%2520local%2520updates%2520efficiently.%2520This%2520work%2520explores%2520the%2520effect%250Aof%2520server-side%2520updates%2520against%2520heterogeneity%2520issues.%2520We%2520first%2520introduce%2520the%250Agradient%2520diversity%2520maximization%2520direction%2520findings%252C%2520suggesting%2520the%2520global%2520model%250Amoves%2520continuously%2520in%2520this%2520direction%2520for%2520fast%2520and%2520stable%2520convergence.%2520Then%252C%2520we%250Aderive%2520a%2520novel%2520server-side%2520optimizer%2520%255Ctextsc%257BFedAWARE%257D%2520with%2520rigorous%250Aconvergence%2520analysis%2520for%2520general%2520non-convex%2520settings.%2520Our%2520extensive%2520experiments%250Aacross%2520multiple%2520heterogeneous%2520federated%2520settings%2520using%2520four%2520datasets%2520showcase%250Athat%2520%255Ctextsc%257BFedAWARE%257D%2520achieves%2520competitive%2520convergence%2520performance%2520in%250Acomparison%2520to%2520state-of-the-art%2520adaptive%2520federated%2520optimizers.%2520Furthermore%252C%2520our%250Aresults%2520show%2520that%2520%255Ctextsc%257BFedAWARE%257D%2520can%2520enhance%2520the%2520performance%2520of%2520FL%250Aalgorithms%2520as%2520a%2520plug-in%2520module.%2520Our%2520source%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/dunzeng/FedAWARE%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02702v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedAWARE%3A%20Maximizing%20Gradient%20Diversity%20for%20Heterogeneous%20Federated%0A%20%20Server-side%20Optimization&entry.906535625=Dun%20Zeng%20and%20Zenglin%20Xu%20and%20Yu%20Pan%20and%20Qifan%20Wang%20and%20Xiaoying%20Tang&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20distributed%20learning%20framework%20where%20numerous%0Aclients%20collaborate%20with%20a%20central%20server%20to%20train%20a%20model%20without%20sharing%0Alocal%20data.%20However%2C%20the%20standard%20federated%20optimization%20in%20real-world%0Aapplications%20faces%20both%20statistical%20and%20system%20heterogeneity%20challenges%2C%20which%0Aresult%20in%20unfavorable%20convergence%20behavior.%20The%20previous%20works%20attempted%20to%0Amodify%20the%20local%20training%20process%20%28client-side%29%20to%20tackle%20heterogeneity%0Achallenges.%20However%2C%20they%20ignored%20that%20the%20updates%20on%20the%20server%20side%20can%0Acoordinate%20the%20diverse%20local%20updates%20efficiently.%20This%20work%20explores%20the%20effect%0Aof%20server-side%20updates%20against%20heterogeneity%20issues.%20We%20first%20introduce%20the%0Agradient%20diversity%20maximization%20direction%20findings%2C%20suggesting%20the%20global%20model%0Amoves%20continuously%20in%20this%20direction%20for%20fast%20and%20stable%20convergence.%20Then%2C%20we%0Aderive%20a%20novel%20server-side%20optimizer%20%5Ctextsc%7BFedAWARE%7D%20with%20rigorous%0Aconvergence%20analysis%20for%20general%20non-convex%20settings.%20Our%20extensive%20experiments%0Aacross%20multiple%20heterogeneous%20federated%20settings%20using%20four%20datasets%20showcase%0Athat%20%5Ctextsc%7BFedAWARE%7D%20achieves%20competitive%20convergence%20performance%20in%0Acomparison%20to%20state-of-the-art%20adaptive%20federated%20optimizers.%20Furthermore%2C%20our%0Aresults%20show%20that%20%5Ctextsc%7BFedAWARE%7D%20can%20enhance%20the%20performance%20of%20FL%0Aalgorithms%20as%20a%20plug-in%20module.%20Our%20source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/dunzeng/FedAWARE%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02702v3&entry.124074799=Read"},
{"title": "InstructAvatar: Text-Guided Emotion and Motion Control for Avatar\n  Generation", "author": "Yuchi Wang and Junliang Guo and Jianhong Bai and Runyi Yu and Tianyu He and Xu Tan and Xu Sun and Jiang Bian", "abstract": "  Recent talking avatar generation models have made strides in achieving\nrealistic and accurate lip synchronization with the audio, but often fall short\nin controlling and conveying detailed expressions and emotions of the avatar,\nmaking the generated video less vivid and controllable. In this paper, we\npropose a novel text-guided approach for generating emotionally expressive 2D\navatars, offering fine-grained control, improved interactivity, and\ngeneralizability to the resulting video. Our framework, named InstructAvatar,\nleverages a natural language interface to control the emotion as well as the\nfacial motion of avatars. Technically, we design an automatic annotation\npipeline to construct an instruction-video paired training dataset, equipped\nwith a novel two-branch diffusion-based generator to predict avatars with audio\nand text instructions at the same time. Experimental results demonstrate that\nInstructAvatar produces results that align well with both conditions, and\noutperforms existing methods in fine-grained emotion control, lip-sync quality,\nand naturalness. Our project page is\nhttps://wangyuchi369.github.io/InstructAvatar/.\n", "link": "http://arxiv.org/abs/2405.15758v1", "date": "2024-05-24", "relevancy": 2.4334, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6161}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6068}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructAvatar%3A%20Text-Guided%20Emotion%20and%20Motion%20Control%20for%20Avatar%0A%20%20Generation&body=Title%3A%20InstructAvatar%3A%20Text-Guided%20Emotion%20and%20Motion%20Control%20for%20Avatar%0A%20%20Generation%0AAuthor%3A%20Yuchi%20Wang%20and%20Junliang%20Guo%20and%20Jianhong%20Bai%20and%20Runyi%20Yu%20and%20Tianyu%20He%20and%20Xu%20Tan%20and%20Xu%20Sun%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Recent%20talking%20avatar%20generation%20models%20have%20made%20strides%20in%20achieving%0Arealistic%20and%20accurate%20lip%20synchronization%20with%20the%20audio%2C%20but%20often%20fall%20short%0Ain%20controlling%20and%20conveying%20detailed%20expressions%20and%20emotions%20of%20the%20avatar%2C%0Amaking%20the%20generated%20video%20less%20vivid%20and%20controllable.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20text-guided%20approach%20for%20generating%20emotionally%20expressive%202D%0Aavatars%2C%20offering%20fine-grained%20control%2C%20improved%20interactivity%2C%20and%0Ageneralizability%20to%20the%20resulting%20video.%20Our%20framework%2C%20named%20InstructAvatar%2C%0Aleverages%20a%20natural%20language%20interface%20to%20control%20the%20emotion%20as%20well%20as%20the%0Afacial%20motion%20of%20avatars.%20Technically%2C%20we%20design%20an%20automatic%20annotation%0Apipeline%20to%20construct%20an%20instruction-video%20paired%20training%20dataset%2C%20equipped%0Awith%20a%20novel%20two-branch%20diffusion-based%20generator%20to%20predict%20avatars%20with%20audio%0Aand%20text%20instructions%20at%20the%20same%20time.%20Experimental%20results%20demonstrate%20that%0AInstructAvatar%20produces%20results%20that%20align%20well%20with%20both%20conditions%2C%20and%0Aoutperforms%20existing%20methods%20in%20fine-grained%20emotion%20control%2C%20lip-sync%20quality%2C%0Aand%20naturalness.%20Our%20project%20page%20is%0Ahttps%3A//wangyuchi369.github.io/InstructAvatar/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructAvatar%253A%2520Text-Guided%2520Emotion%2520and%2520Motion%2520Control%2520for%2520Avatar%250A%2520%2520Generation%26entry.906535625%3DYuchi%2520Wang%2520and%2520Junliang%2520Guo%2520and%2520Jianhong%2520Bai%2520and%2520Runyi%2520Yu%2520and%2520Tianyu%2520He%2520and%2520Xu%2520Tan%2520and%2520Xu%2520Sun%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Recent%2520talking%2520avatar%2520generation%2520models%2520have%2520made%2520strides%2520in%2520achieving%250Arealistic%2520and%2520accurate%2520lip%2520synchronization%2520with%2520the%2520audio%252C%2520but%2520often%2520fall%2520short%250Ain%2520controlling%2520and%2520conveying%2520detailed%2520expressions%2520and%2520emotions%2520of%2520the%2520avatar%252C%250Amaking%2520the%2520generated%2520video%2520less%2520vivid%2520and%2520controllable.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520text-guided%2520approach%2520for%2520generating%2520emotionally%2520expressive%25202D%250Aavatars%252C%2520offering%2520fine-grained%2520control%252C%2520improved%2520interactivity%252C%2520and%250Ageneralizability%2520to%2520the%2520resulting%2520video.%2520Our%2520framework%252C%2520named%2520InstructAvatar%252C%250Aleverages%2520a%2520natural%2520language%2520interface%2520to%2520control%2520the%2520emotion%2520as%2520well%2520as%2520the%250Afacial%2520motion%2520of%2520avatars.%2520Technically%252C%2520we%2520design%2520an%2520automatic%2520annotation%250Apipeline%2520to%2520construct%2520an%2520instruction-video%2520paired%2520training%2520dataset%252C%2520equipped%250Awith%2520a%2520novel%2520two-branch%2520diffusion-based%2520generator%2520to%2520predict%2520avatars%2520with%2520audio%250Aand%2520text%2520instructions%2520at%2520the%2520same%2520time.%2520Experimental%2520results%2520demonstrate%2520that%250AInstructAvatar%2520produces%2520results%2520that%2520align%2520well%2520with%2520both%2520conditions%252C%2520and%250Aoutperforms%2520existing%2520methods%2520in%2520fine-grained%2520emotion%2520control%252C%2520lip-sync%2520quality%252C%250Aand%2520naturalness.%2520Our%2520project%2520page%2520is%250Ahttps%253A//wangyuchi369.github.io/InstructAvatar/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructAvatar%3A%20Text-Guided%20Emotion%20and%20Motion%20Control%20for%20Avatar%0A%20%20Generation&entry.906535625=Yuchi%20Wang%20and%20Junliang%20Guo%20and%20Jianhong%20Bai%20and%20Runyi%20Yu%20and%20Tianyu%20He%20and%20Xu%20Tan%20and%20Xu%20Sun%20and%20Jiang%20Bian&entry.1292438233=%20%20Recent%20talking%20avatar%20generation%20models%20have%20made%20strides%20in%20achieving%0Arealistic%20and%20accurate%20lip%20synchronization%20with%20the%20audio%2C%20but%20often%20fall%20short%0Ain%20controlling%20and%20conveying%20detailed%20expressions%20and%20emotions%20of%20the%20avatar%2C%0Amaking%20the%20generated%20video%20less%20vivid%20and%20controllable.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20text-guided%20approach%20for%20generating%20emotionally%20expressive%202D%0Aavatars%2C%20offering%20fine-grained%20control%2C%20improved%20interactivity%2C%20and%0Ageneralizability%20to%20the%20resulting%20video.%20Our%20framework%2C%20named%20InstructAvatar%2C%0Aleverages%20a%20natural%20language%20interface%20to%20control%20the%20emotion%20as%20well%20as%20the%0Afacial%20motion%20of%20avatars.%20Technically%2C%20we%20design%20an%20automatic%20annotation%0Apipeline%20to%20construct%20an%20instruction-video%20paired%20training%20dataset%2C%20equipped%0Awith%20a%20novel%20two-branch%20diffusion-based%20generator%20to%20predict%20avatars%20with%20audio%0Aand%20text%20instructions%20at%20the%20same%20time.%20Experimental%20results%20demonstrate%20that%0AInstructAvatar%20produces%20results%20that%20align%20well%20with%20both%20conditions%2C%20and%0Aoutperforms%20existing%20methods%20in%20fine-grained%20emotion%20control%2C%20lip-sync%20quality%2C%0Aand%20naturalness.%20Our%20project%20page%20is%0Ahttps%3A//wangyuchi369.github.io/InstructAvatar/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15758v1&entry.124074799=Read"},
{"title": "Detecting Out-of-Distribution Through the Lens of Neural Collapse", "author": "Litian Liu and Yao Qin", "abstract": "  Efficient and versatile Out-of-Distribution (OOD) detection is essential for\nthe safe deployment of AI yet remains challenging for existing algorithms.\nInspired by Neural Collapse, we discover that features of in-distribution (ID)\nsamples cluster closer to the weight vectors compared to features of OOD\nsamples. In addition, we reveal that ID features tend to expand in space to\nstructure a simplex Equiangular Tight Framework, which nicely explains the\nprevalent observation that ID features reside further from the origin than OOD\nfeatures. Taking both insights from Neural Collapse into consideration, we\npropose to leverage feature proximity to weight vectors for OOD detection and\nfurther complement this perspective by using feature norms to filter OOD\nsamples. Extensive experiments on off-the-shelf models demonstrate the\nefficiency and effectiveness of our method across diverse classification tasks\nand model architectures, enhancing the generalization capability of OOD\ndetection.\n", "link": "http://arxiv.org/abs/2311.01479v4", "date": "2024-05-24", "relevancy": 2.4238, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5008}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4834}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Out-of-Distribution%20Through%20the%20Lens%20of%20Neural%20Collapse&body=Title%3A%20Detecting%20Out-of-Distribution%20Through%20the%20Lens%20of%20Neural%20Collapse%0AAuthor%3A%20Litian%20Liu%20and%20Yao%20Qin%0AAbstract%3A%20%20%20Efficient%20and%20versatile%20Out-of-Distribution%20%28OOD%29%20detection%20is%20essential%20for%0Athe%20safe%20deployment%20of%20AI%20yet%20remains%20challenging%20for%20existing%20algorithms.%0AInspired%20by%20Neural%20Collapse%2C%20we%20discover%20that%20features%20of%20in-distribution%20%28ID%29%0Asamples%20cluster%20closer%20to%20the%20weight%20vectors%20compared%20to%20features%20of%20OOD%0Asamples.%20In%20addition%2C%20we%20reveal%20that%20ID%20features%20tend%20to%20expand%20in%20space%20to%0Astructure%20a%20simplex%20Equiangular%20Tight%20Framework%2C%20which%20nicely%20explains%20the%0Aprevalent%20observation%20that%20ID%20features%20reside%20further%20from%20the%20origin%20than%20OOD%0Afeatures.%20Taking%20both%20insights%20from%20Neural%20Collapse%20into%20consideration%2C%20we%0Apropose%20to%20leverage%20feature%20proximity%20to%20weight%20vectors%20for%20OOD%20detection%20and%0Afurther%20complement%20this%20perspective%20by%20using%20feature%20norms%20to%20filter%20OOD%0Asamples.%20Extensive%20experiments%20on%20off-the-shelf%20models%20demonstrate%20the%0Aefficiency%20and%20effectiveness%20of%20our%20method%20across%20diverse%20classification%20tasks%0Aand%20model%20architectures%2C%20enhancing%20the%20generalization%20capability%20of%20OOD%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01479v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Out-of-Distribution%2520Through%2520the%2520Lens%2520of%2520Neural%2520Collapse%26entry.906535625%3DLitian%2520Liu%2520and%2520Yao%2520Qin%26entry.1292438233%3D%2520%2520Efficient%2520and%2520versatile%2520Out-of-Distribution%2520%2528OOD%2529%2520detection%2520is%2520essential%2520for%250Athe%2520safe%2520deployment%2520of%2520AI%2520yet%2520remains%2520challenging%2520for%2520existing%2520algorithms.%250AInspired%2520by%2520Neural%2520Collapse%252C%2520we%2520discover%2520that%2520features%2520of%2520in-distribution%2520%2528ID%2529%250Asamples%2520cluster%2520closer%2520to%2520the%2520weight%2520vectors%2520compared%2520to%2520features%2520of%2520OOD%250Asamples.%2520In%2520addition%252C%2520we%2520reveal%2520that%2520ID%2520features%2520tend%2520to%2520expand%2520in%2520space%2520to%250Astructure%2520a%2520simplex%2520Equiangular%2520Tight%2520Framework%252C%2520which%2520nicely%2520explains%2520the%250Aprevalent%2520observation%2520that%2520ID%2520features%2520reside%2520further%2520from%2520the%2520origin%2520than%2520OOD%250Afeatures.%2520Taking%2520both%2520insights%2520from%2520Neural%2520Collapse%2520into%2520consideration%252C%2520we%250Apropose%2520to%2520leverage%2520feature%2520proximity%2520to%2520weight%2520vectors%2520for%2520OOD%2520detection%2520and%250Afurther%2520complement%2520this%2520perspective%2520by%2520using%2520feature%2520norms%2520to%2520filter%2520OOD%250Asamples.%2520Extensive%2520experiments%2520on%2520off-the-shelf%2520models%2520demonstrate%2520the%250Aefficiency%2520and%2520effectiveness%2520of%2520our%2520method%2520across%2520diverse%2520classification%2520tasks%250Aand%2520model%2520architectures%252C%2520enhancing%2520the%2520generalization%2520capability%2520of%2520OOD%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01479v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Out-of-Distribution%20Through%20the%20Lens%20of%20Neural%20Collapse&entry.906535625=Litian%20Liu%20and%20Yao%20Qin&entry.1292438233=%20%20Efficient%20and%20versatile%20Out-of-Distribution%20%28OOD%29%20detection%20is%20essential%20for%0Athe%20safe%20deployment%20of%20AI%20yet%20remains%20challenging%20for%20existing%20algorithms.%0AInspired%20by%20Neural%20Collapse%2C%20we%20discover%20that%20features%20of%20in-distribution%20%28ID%29%0Asamples%20cluster%20closer%20to%20the%20weight%20vectors%20compared%20to%20features%20of%20OOD%0Asamples.%20In%20addition%2C%20we%20reveal%20that%20ID%20features%20tend%20to%20expand%20in%20space%20to%0Astructure%20a%20simplex%20Equiangular%20Tight%20Framework%2C%20which%20nicely%20explains%20the%0Aprevalent%20observation%20that%20ID%20features%20reside%20further%20from%20the%20origin%20than%20OOD%0Afeatures.%20Taking%20both%20insights%20from%20Neural%20Collapse%20into%20consideration%2C%20we%0Apropose%20to%20leverage%20feature%20proximity%20to%20weight%20vectors%20for%20OOD%20detection%20and%0Afurther%20complement%20this%20perspective%20by%20using%20feature%20norms%20to%20filter%20OOD%0Asamples.%20Extensive%20experiments%20on%20off-the-shelf%20models%20demonstrate%20the%0Aefficiency%20and%20effectiveness%20of%20our%20method%20across%20diverse%20classification%20tasks%0Aand%20model%20architectures%2C%20enhancing%20the%20generalization%20capability%20of%20OOD%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01479v4&entry.124074799=Read"},
{"title": "Harnessing Increased Client Participation with Cohort-Parallel Federated\n  Learning", "author": "Akash Dhasade and Anne-Marie Kermarrec and Tuan-Anh Nguyen and Rafael Pires and Martijn de Vos", "abstract": "  Federated Learning (FL) is a machine learning approach where nodes\ncollaboratively train a global model. As more nodes participate in a round of\nFL, the effectiveness of individual model updates by nodes also diminishes. In\nthis study, we increase the effectiveness of client updates by dividing the\nnetwork into smaller partitions, or cohorts. We introduce Cohort-Parallel\nFederated Learning (CPFL): a novel learning approach where each cohort\nindependently trains a global model using FL, until convergence, and the\nproduced models by each cohort are then unified using one-shot Knowledge\nDistillation (KD) and a cross-domain, unlabeled dataset. The insight behind\nCPFL is that smaller, isolated networks converge quicker than in a one-network\nsetting where all nodes participate. Through exhaustive experiments involving\nrealistic traces and non-IID data distributions on the CIFAR-10 and FEMNIST\nimage classification tasks, we investigate the balance between the number of\ncohorts, model accuracy, training time, and compute and communication\nresources. Compared to traditional FL, CPFL with four cohorts, non-IID data\ndistribution, and CIFAR-10 yields a 1.9$\\times$ reduction in train time and a\n1.3$\\times$ reduction in resource usage, with a minimal drop in test accuracy.\n", "link": "http://arxiv.org/abs/2405.15644v1", "date": "2024-05-24", "relevancy": 2.4195, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4917}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4835}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Increased%20Client%20Participation%20with%20Cohort-Parallel%20Federated%0A%20%20Learning&body=Title%3A%20Harnessing%20Increased%20Client%20Participation%20with%20Cohort-Parallel%20Federated%0A%20%20Learning%0AAuthor%3A%20Akash%20Dhasade%20and%20Anne-Marie%20Kermarrec%20and%20Tuan-Anh%20Nguyen%20and%20Rafael%20Pires%20and%20Martijn%20de%20Vos%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20machine%20learning%20approach%20where%20nodes%0Acollaboratively%20train%20a%20global%20model.%20As%20more%20nodes%20participate%20in%20a%20round%20of%0AFL%2C%20the%20effectiveness%20of%20individual%20model%20updates%20by%20nodes%20also%20diminishes.%20In%0Athis%20study%2C%20we%20increase%20the%20effectiveness%20of%20client%20updates%20by%20dividing%20the%0Anetwork%20into%20smaller%20partitions%2C%20or%20cohorts.%20We%20introduce%20Cohort-Parallel%0AFederated%20Learning%20%28CPFL%29%3A%20a%20novel%20learning%20approach%20where%20each%20cohort%0Aindependently%20trains%20a%20global%20model%20using%20FL%2C%20until%20convergence%2C%20and%20the%0Aproduced%20models%20by%20each%20cohort%20are%20then%20unified%20using%20one-shot%20Knowledge%0ADistillation%20%28KD%29%20and%20a%20cross-domain%2C%20unlabeled%20dataset.%20The%20insight%20behind%0ACPFL%20is%20that%20smaller%2C%20isolated%20networks%20converge%20quicker%20than%20in%20a%20one-network%0Asetting%20where%20all%20nodes%20participate.%20Through%20exhaustive%20experiments%20involving%0Arealistic%20traces%20and%20non-IID%20data%20distributions%20on%20the%20CIFAR-10%20and%20FEMNIST%0Aimage%20classification%20tasks%2C%20we%20investigate%20the%20balance%20between%20the%20number%20of%0Acohorts%2C%20model%20accuracy%2C%20training%20time%2C%20and%20compute%20and%20communication%0Aresources.%20Compared%20to%20traditional%20FL%2C%20CPFL%20with%20four%20cohorts%2C%20non-IID%20data%0Adistribution%2C%20and%20CIFAR-10%20yields%20a%201.9%24%5Ctimes%24%20reduction%20in%20train%20time%20and%20a%0A1.3%24%5Ctimes%24%20reduction%20in%20resource%20usage%2C%20with%20a%20minimal%20drop%20in%20test%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Increased%2520Client%2520Participation%2520with%2520Cohort-Parallel%2520Federated%250A%2520%2520Learning%26entry.906535625%3DAkash%2520Dhasade%2520and%2520Anne-Marie%2520Kermarrec%2520and%2520Tuan-Anh%2520Nguyen%2520and%2520Rafael%2520Pires%2520and%2520Martijn%2520de%2520Vos%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520machine%2520learning%2520approach%2520where%2520nodes%250Acollaboratively%2520train%2520a%2520global%2520model.%2520As%2520more%2520nodes%2520participate%2520in%2520a%2520round%2520of%250AFL%252C%2520the%2520effectiveness%2520of%2520individual%2520model%2520updates%2520by%2520nodes%2520also%2520diminishes.%2520In%250Athis%2520study%252C%2520we%2520increase%2520the%2520effectiveness%2520of%2520client%2520updates%2520by%2520dividing%2520the%250Anetwork%2520into%2520smaller%2520partitions%252C%2520or%2520cohorts.%2520We%2520introduce%2520Cohort-Parallel%250AFederated%2520Learning%2520%2528CPFL%2529%253A%2520a%2520novel%2520learning%2520approach%2520where%2520each%2520cohort%250Aindependently%2520trains%2520a%2520global%2520model%2520using%2520FL%252C%2520until%2520convergence%252C%2520and%2520the%250Aproduced%2520models%2520by%2520each%2520cohort%2520are%2520then%2520unified%2520using%2520one-shot%2520Knowledge%250ADistillation%2520%2528KD%2529%2520and%2520a%2520cross-domain%252C%2520unlabeled%2520dataset.%2520The%2520insight%2520behind%250ACPFL%2520is%2520that%2520smaller%252C%2520isolated%2520networks%2520converge%2520quicker%2520than%2520in%2520a%2520one-network%250Asetting%2520where%2520all%2520nodes%2520participate.%2520Through%2520exhaustive%2520experiments%2520involving%250Arealistic%2520traces%2520and%2520non-IID%2520data%2520distributions%2520on%2520the%2520CIFAR-10%2520and%2520FEMNIST%250Aimage%2520classification%2520tasks%252C%2520we%2520investigate%2520the%2520balance%2520between%2520the%2520number%2520of%250Acohorts%252C%2520model%2520accuracy%252C%2520training%2520time%252C%2520and%2520compute%2520and%2520communication%250Aresources.%2520Compared%2520to%2520traditional%2520FL%252C%2520CPFL%2520with%2520four%2520cohorts%252C%2520non-IID%2520data%250Adistribution%252C%2520and%2520CIFAR-10%2520yields%2520a%25201.9%2524%255Ctimes%2524%2520reduction%2520in%2520train%2520time%2520and%2520a%250A1.3%2524%255Ctimes%2524%2520reduction%2520in%2520resource%2520usage%252C%2520with%2520a%2520minimal%2520drop%2520in%2520test%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Increased%20Client%20Participation%20with%20Cohort-Parallel%20Federated%0A%20%20Learning&entry.906535625=Akash%20Dhasade%20and%20Anne-Marie%20Kermarrec%20and%20Tuan-Anh%20Nguyen%20and%20Rafael%20Pires%20and%20Martijn%20de%20Vos&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20machine%20learning%20approach%20where%20nodes%0Acollaboratively%20train%20a%20global%20model.%20As%20more%20nodes%20participate%20in%20a%20round%20of%0AFL%2C%20the%20effectiveness%20of%20individual%20model%20updates%20by%20nodes%20also%20diminishes.%20In%0Athis%20study%2C%20we%20increase%20the%20effectiveness%20of%20client%20updates%20by%20dividing%20the%0Anetwork%20into%20smaller%20partitions%2C%20or%20cohorts.%20We%20introduce%20Cohort-Parallel%0AFederated%20Learning%20%28CPFL%29%3A%20a%20novel%20learning%20approach%20where%20each%20cohort%0Aindependently%20trains%20a%20global%20model%20using%20FL%2C%20until%20convergence%2C%20and%20the%0Aproduced%20models%20by%20each%20cohort%20are%20then%20unified%20using%20one-shot%20Knowledge%0ADistillation%20%28KD%29%20and%20a%20cross-domain%2C%20unlabeled%20dataset.%20The%20insight%20behind%0ACPFL%20is%20that%20smaller%2C%20isolated%20networks%20converge%20quicker%20than%20in%20a%20one-network%0Asetting%20where%20all%20nodes%20participate.%20Through%20exhaustive%20experiments%20involving%0Arealistic%20traces%20and%20non-IID%20data%20distributions%20on%20the%20CIFAR-10%20and%20FEMNIST%0Aimage%20classification%20tasks%2C%20we%20investigate%20the%20balance%20between%20the%20number%20of%0Acohorts%2C%20model%20accuracy%2C%20training%20time%2C%20and%20compute%20and%20communication%0Aresources.%20Compared%20to%20traditional%20FL%2C%20CPFL%20with%20four%20cohorts%2C%20non-IID%20data%0Adistribution%2C%20and%20CIFAR-10%20yields%20a%201.9%24%5Ctimes%24%20reduction%20in%20train%20time%20and%20a%0A1.3%24%5Ctimes%24%20reduction%20in%20resource%20usage%2C%20with%20a%20minimal%20drop%20in%20test%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15644v1&entry.124074799=Read"},
{"title": "Exposing Image Classifier Shortcuts with Counterfactual Frequency (CoF)\n  Tables", "author": "James Hinns and David Martens", "abstract": "  The rise of deep learning in image classification has brought unprecedented\naccuracy but also highlighted a key issue: the use of 'shortcuts' by models.\nSuch shortcuts are easy-to-learn patterns from the training data that fail to\ngeneralise to new data. Examples include the use of a copyright watermark to\nrecognise horses, snowy background to recognise huskies, or ink markings to\ndetect malignant skin lesions. The explainable AI (XAI) community has suggested\nusing instance-level explanations to detect shortcuts without external data,\nbut this requires the examination of many explanations to confirm the presence\nof such shortcuts, making it a labour-intensive process. To address these\nchallenges, we introduce Counterfactual Frequency (CoF) tables, a novel\napproach that aggregates instance-based explanations into global insights, and\nexposes shortcuts. The aggregation implies the need for some semantic concepts\nto be used in the explanations, which we solve by labelling the segments of an\nimage. We demonstrate the utility of CoF tables across several datasets,\nrevealing the shortcuts learned from them.\n", "link": "http://arxiv.org/abs/2405.15661v1", "date": "2024-05-24", "relevancy": 2.4001, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4894}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4765}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exposing%20Image%20Classifier%20Shortcuts%20with%20Counterfactual%20Frequency%20%28CoF%29%0A%20%20Tables&body=Title%3A%20Exposing%20Image%20Classifier%20Shortcuts%20with%20Counterfactual%20Frequency%20%28CoF%29%0A%20%20Tables%0AAuthor%3A%20James%20Hinns%20and%20David%20Martens%0AAbstract%3A%20%20%20The%20rise%20of%20deep%20learning%20in%20image%20classification%20has%20brought%20unprecedented%0Aaccuracy%20but%20also%20highlighted%20a%20key%20issue%3A%20the%20use%20of%20%27shortcuts%27%20by%20models.%0ASuch%20shortcuts%20are%20easy-to-learn%20patterns%20from%20the%20training%20data%20that%20fail%20to%0Ageneralise%20to%20new%20data.%20Examples%20include%20the%20use%20of%20a%20copyright%20watermark%20to%0Arecognise%20horses%2C%20snowy%20background%20to%20recognise%20huskies%2C%20or%20ink%20markings%20to%0Adetect%20malignant%20skin%20lesions.%20The%20explainable%20AI%20%28XAI%29%20community%20has%20suggested%0Ausing%20instance-level%20explanations%20to%20detect%20shortcuts%20without%20external%20data%2C%0Abut%20this%20requires%20the%20examination%20of%20many%20explanations%20to%20confirm%20the%20presence%0Aof%20such%20shortcuts%2C%20making%20it%20a%20labour-intensive%20process.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Counterfactual%20Frequency%20%28CoF%29%20tables%2C%20a%20novel%0Aapproach%20that%20aggregates%20instance-based%20explanations%20into%20global%20insights%2C%20and%0Aexposes%20shortcuts.%20The%20aggregation%20implies%20the%20need%20for%20some%20semantic%20concepts%0Ato%20be%20used%20in%20the%20explanations%2C%20which%20we%20solve%20by%20labelling%20the%20segments%20of%20an%0Aimage.%20We%20demonstrate%20the%20utility%20of%20CoF%20tables%20across%20several%20datasets%2C%0Arevealing%20the%20shortcuts%20learned%20from%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExposing%2520Image%2520Classifier%2520Shortcuts%2520with%2520Counterfactual%2520Frequency%2520%2528CoF%2529%250A%2520%2520Tables%26entry.906535625%3DJames%2520Hinns%2520and%2520David%2520Martens%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520deep%2520learning%2520in%2520image%2520classification%2520has%2520brought%2520unprecedented%250Aaccuracy%2520but%2520also%2520highlighted%2520a%2520key%2520issue%253A%2520the%2520use%2520of%2520%2527shortcuts%2527%2520by%2520models.%250ASuch%2520shortcuts%2520are%2520easy-to-learn%2520patterns%2520from%2520the%2520training%2520data%2520that%2520fail%2520to%250Ageneralise%2520to%2520new%2520data.%2520Examples%2520include%2520the%2520use%2520of%2520a%2520copyright%2520watermark%2520to%250Arecognise%2520horses%252C%2520snowy%2520background%2520to%2520recognise%2520huskies%252C%2520or%2520ink%2520markings%2520to%250Adetect%2520malignant%2520skin%2520lesions.%2520The%2520explainable%2520AI%2520%2528XAI%2529%2520community%2520has%2520suggested%250Ausing%2520instance-level%2520explanations%2520to%2520detect%2520shortcuts%2520without%2520external%2520data%252C%250Abut%2520this%2520requires%2520the%2520examination%2520of%2520many%2520explanations%2520to%2520confirm%2520the%2520presence%250Aof%2520such%2520shortcuts%252C%2520making%2520it%2520a%2520labour-intensive%2520process.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520Counterfactual%2520Frequency%2520%2528CoF%2529%2520tables%252C%2520a%2520novel%250Aapproach%2520that%2520aggregates%2520instance-based%2520explanations%2520into%2520global%2520insights%252C%2520and%250Aexposes%2520shortcuts.%2520The%2520aggregation%2520implies%2520the%2520need%2520for%2520some%2520semantic%2520concepts%250Ato%2520be%2520used%2520in%2520the%2520explanations%252C%2520which%2520we%2520solve%2520by%2520labelling%2520the%2520segments%2520of%2520an%250Aimage.%2520We%2520demonstrate%2520the%2520utility%2520of%2520CoF%2520tables%2520across%2520several%2520datasets%252C%250Arevealing%2520the%2520shortcuts%2520learned%2520from%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exposing%20Image%20Classifier%20Shortcuts%20with%20Counterfactual%20Frequency%20%28CoF%29%0A%20%20Tables&entry.906535625=James%20Hinns%20and%20David%20Martens&entry.1292438233=%20%20The%20rise%20of%20deep%20learning%20in%20image%20classification%20has%20brought%20unprecedented%0Aaccuracy%20but%20also%20highlighted%20a%20key%20issue%3A%20the%20use%20of%20%27shortcuts%27%20by%20models.%0ASuch%20shortcuts%20are%20easy-to-learn%20patterns%20from%20the%20training%20data%20that%20fail%20to%0Ageneralise%20to%20new%20data.%20Examples%20include%20the%20use%20of%20a%20copyright%20watermark%20to%0Arecognise%20horses%2C%20snowy%20background%20to%20recognise%20huskies%2C%20or%20ink%20markings%20to%0Adetect%20malignant%20skin%20lesions.%20The%20explainable%20AI%20%28XAI%29%20community%20has%20suggested%0Ausing%20instance-level%20explanations%20to%20detect%20shortcuts%20without%20external%20data%2C%0Abut%20this%20requires%20the%20examination%20of%20many%20explanations%20to%20confirm%20the%20presence%0Aof%20such%20shortcuts%2C%20making%20it%20a%20labour-intensive%20process.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Counterfactual%20Frequency%20%28CoF%29%20tables%2C%20a%20novel%0Aapproach%20that%20aggregates%20instance-based%20explanations%20into%20global%20insights%2C%20and%0Aexposes%20shortcuts.%20The%20aggregation%20implies%20the%20need%20for%20some%20semantic%20concepts%0Ato%20be%20used%20in%20the%20explanations%2C%20which%20we%20solve%20by%20labelling%20the%20segments%20of%20an%0Aimage.%20We%20demonstrate%20the%20utility%20of%20CoF%20tables%20across%20several%20datasets%2C%0Arevealing%20the%20shortcuts%20learned%20from%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15661v1&entry.124074799=Read"},
{"title": "Towards Weakly Supervised End-to-end Learning for Long-video Action\n  Recognition", "author": "Jiaming Zhou and Hanjun Li and Kun-Yu Lin and Junwei Liang", "abstract": "  Developing end-to-end action recognition models on long videos is fundamental\nand crucial for long-video action understanding. Due to the unaffordable cost\nof end-to-end training on the whole long videos, existing works generally train\nmodels on short clips trimmed from long videos. However, this\n``trimming-then-training'' practice requires action interval annotations for\nclip-level supervision, i.e., knowing which actions are trimmed into the clips.\nUnfortunately, collecting such annotations is very expensive and prevents model\ntraining at scale. To this end, this work aims to build a weakly supervised\nend-to-end framework for training recognition models on long videos, with only\nvideo-level action category labels. Without knowing the precise temporal\nlocations of actions in long videos, our proposed weakly supervised framework,\nnamely AdaptFocus, estimates where and how likely the actions will occur to\nadaptively focus on informative action clips for end-to-end training. The\neffectiveness of the proposed AdaptFocus framework is demonstrated on three\nlong-video datasets. Furthermore, for downstream long-video tasks, our\nAdaptFocus framework provides a weakly supervised feature extraction pipeline\nfor extracting more robust long-video features, such that the state-of-the-art\nmethods on downstream tasks are significantly advanced. We will release the\ncode and models.\n", "link": "http://arxiv.org/abs/2311.17118v2", "date": "2024-05-24", "relevancy": 2.3646, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6463}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5707}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Weakly%20Supervised%20End-to-end%20Learning%20for%20Long-video%20Action%0A%20%20Recognition&body=Title%3A%20Towards%20Weakly%20Supervised%20End-to-end%20Learning%20for%20Long-video%20Action%0A%20%20Recognition%0AAuthor%3A%20Jiaming%20Zhou%20and%20Hanjun%20Li%20and%20Kun-Yu%20Lin%20and%20Junwei%20Liang%0AAbstract%3A%20%20%20Developing%20end-to-end%20action%20recognition%20models%20on%20long%20videos%20is%20fundamental%0Aand%20crucial%20for%20long-video%20action%20understanding.%20Due%20to%20the%20unaffordable%20cost%0Aof%20end-to-end%20training%20on%20the%20whole%20long%20videos%2C%20existing%20works%20generally%20train%0Amodels%20on%20short%20clips%20trimmed%20from%20long%20videos.%20However%2C%20this%0A%60%60trimming-then-training%27%27%20practice%20requires%20action%20interval%20annotations%20for%0Aclip-level%20supervision%2C%20i.e.%2C%20knowing%20which%20actions%20are%20trimmed%20into%20the%20clips.%0AUnfortunately%2C%20collecting%20such%20annotations%20is%20very%20expensive%20and%20prevents%20model%0Atraining%20at%20scale.%20To%20this%20end%2C%20this%20work%20aims%20to%20build%20a%20weakly%20supervised%0Aend-to-end%20framework%20for%20training%20recognition%20models%20on%20long%20videos%2C%20with%20only%0Avideo-level%20action%20category%20labels.%20Without%20knowing%20the%20precise%20temporal%0Alocations%20of%20actions%20in%20long%20videos%2C%20our%20proposed%20weakly%20supervised%20framework%2C%0Anamely%20AdaptFocus%2C%20estimates%20where%20and%20how%20likely%20the%20actions%20will%20occur%20to%0Aadaptively%20focus%20on%20informative%20action%20clips%20for%20end-to-end%20training.%20The%0Aeffectiveness%20of%20the%20proposed%20AdaptFocus%20framework%20is%20demonstrated%20on%20three%0Along-video%20datasets.%20Furthermore%2C%20for%20downstream%20long-video%20tasks%2C%20our%0AAdaptFocus%20framework%20provides%20a%20weakly%20supervised%20feature%20extraction%20pipeline%0Afor%20extracting%20more%20robust%20long-video%20features%2C%20such%20that%20the%20state-of-the-art%0Amethods%20on%20downstream%20tasks%20are%20significantly%20advanced.%20We%20will%20release%20the%0Acode%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Weakly%2520Supervised%2520End-to-end%2520Learning%2520for%2520Long-video%2520Action%250A%2520%2520Recognition%26entry.906535625%3DJiaming%2520Zhou%2520and%2520Hanjun%2520Li%2520and%2520Kun-Yu%2520Lin%2520and%2520Junwei%2520Liang%26entry.1292438233%3D%2520%2520Developing%2520end-to-end%2520action%2520recognition%2520models%2520on%2520long%2520videos%2520is%2520fundamental%250Aand%2520crucial%2520for%2520long-video%2520action%2520understanding.%2520Due%2520to%2520the%2520unaffordable%2520cost%250Aof%2520end-to-end%2520training%2520on%2520the%2520whole%2520long%2520videos%252C%2520existing%2520works%2520generally%2520train%250Amodels%2520on%2520short%2520clips%2520trimmed%2520from%2520long%2520videos.%2520However%252C%2520this%250A%2560%2560trimming-then-training%2527%2527%2520practice%2520requires%2520action%2520interval%2520annotations%2520for%250Aclip-level%2520supervision%252C%2520i.e.%252C%2520knowing%2520which%2520actions%2520are%2520trimmed%2520into%2520the%2520clips.%250AUnfortunately%252C%2520collecting%2520such%2520annotations%2520is%2520very%2520expensive%2520and%2520prevents%2520model%250Atraining%2520at%2520scale.%2520To%2520this%2520end%252C%2520this%2520work%2520aims%2520to%2520build%2520a%2520weakly%2520supervised%250Aend-to-end%2520framework%2520for%2520training%2520recognition%2520models%2520on%2520long%2520videos%252C%2520with%2520only%250Avideo-level%2520action%2520category%2520labels.%2520Without%2520knowing%2520the%2520precise%2520temporal%250Alocations%2520of%2520actions%2520in%2520long%2520videos%252C%2520our%2520proposed%2520weakly%2520supervised%2520framework%252C%250Anamely%2520AdaptFocus%252C%2520estimates%2520where%2520and%2520how%2520likely%2520the%2520actions%2520will%2520occur%2520to%250Aadaptively%2520focus%2520on%2520informative%2520action%2520clips%2520for%2520end-to-end%2520training.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520AdaptFocus%2520framework%2520is%2520demonstrated%2520on%2520three%250Along-video%2520datasets.%2520Furthermore%252C%2520for%2520downstream%2520long-video%2520tasks%252C%2520our%250AAdaptFocus%2520framework%2520provides%2520a%2520weakly%2520supervised%2520feature%2520extraction%2520pipeline%250Afor%2520extracting%2520more%2520robust%2520long-video%2520features%252C%2520such%2520that%2520the%2520state-of-the-art%250Amethods%2520on%2520downstream%2520tasks%2520are%2520significantly%2520advanced.%2520We%2520will%2520release%2520the%250Acode%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Weakly%20Supervised%20End-to-end%20Learning%20for%20Long-video%20Action%0A%20%20Recognition&entry.906535625=Jiaming%20Zhou%20and%20Hanjun%20Li%20and%20Kun-Yu%20Lin%20and%20Junwei%20Liang&entry.1292438233=%20%20Developing%20end-to-end%20action%20recognition%20models%20on%20long%20videos%20is%20fundamental%0Aand%20crucial%20for%20long-video%20action%20understanding.%20Due%20to%20the%20unaffordable%20cost%0Aof%20end-to-end%20training%20on%20the%20whole%20long%20videos%2C%20existing%20works%20generally%20train%0Amodels%20on%20short%20clips%20trimmed%20from%20long%20videos.%20However%2C%20this%0A%60%60trimming-then-training%27%27%20practice%20requires%20action%20interval%20annotations%20for%0Aclip-level%20supervision%2C%20i.e.%2C%20knowing%20which%20actions%20are%20trimmed%20into%20the%20clips.%0AUnfortunately%2C%20collecting%20such%20annotations%20is%20very%20expensive%20and%20prevents%20model%0Atraining%20at%20scale.%20To%20this%20end%2C%20this%20work%20aims%20to%20build%20a%20weakly%20supervised%0Aend-to-end%20framework%20for%20training%20recognition%20models%20on%20long%20videos%2C%20with%20only%0Avideo-level%20action%20category%20labels.%20Without%20knowing%20the%20precise%20temporal%0Alocations%20of%20actions%20in%20long%20videos%2C%20our%20proposed%20weakly%20supervised%20framework%2C%0Anamely%20AdaptFocus%2C%20estimates%20where%20and%20how%20likely%20the%20actions%20will%20occur%20to%0Aadaptively%20focus%20on%20informative%20action%20clips%20for%20end-to-end%20training.%20The%0Aeffectiveness%20of%20the%20proposed%20AdaptFocus%20framework%20is%20demonstrated%20on%20three%0Along-video%20datasets.%20Furthermore%2C%20for%20downstream%20long-video%20tasks%2C%20our%0AAdaptFocus%20framework%20provides%20a%20weakly%20supervised%20feature%20extraction%20pipeline%0Afor%20extracting%20more%20robust%20long-video%20features%2C%20such%20that%20the%20state-of-the-art%0Amethods%20on%20downstream%20tasks%20are%20significantly%20advanced.%20We%20will%20release%20the%0Acode%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17118v2&entry.124074799=Read"},
{"title": "HyperInterval: Hypernetwork approach to training weight interval regions\n  in continual learning", "author": "Patryk Krukowski and Anna Bielawska and Kamil Ksi\u0105\u017cek and Pawe\u0142 Wawrzy\u0144ski and Pawe\u0142 Batorski and Przemys\u0142aw Spurek", "abstract": "  Recently, a new Continual Learning (CL) paradigm was presented to control\ncatastrophic forgetting, called Interval Continual Learning (InterContiNet),\nwhich relies on enforcing interval constraints on the neural network parameter\nspace. Unfortunately, InterContiNet training is challenging due to the high\ndimensionality of the weight space, making intervals difficult to manage. To\naddress this issue, we introduce HyperInterval, a technique that employs\ninterval arithmetic within the embedding space and utilizes a hypernetwork to\nmap these intervals to the target network parameter space. We train interval\nembeddings for consecutive tasks and train a hypernetwork to transform these\nembeddings into weights of the target network. An embedding for a given task is\ntrained along with the hypernetwork, preserving the response of the target\nnetwork for the previous task embeddings. Interval arithmetic works with a more\nmanageable, lower-dimensional embedding space rather than directly preparing\nintervals in a high-dimensional weight space. Our model allows faster and more\nefficient training. Furthermore, HyperInterval maintains the guarantee of not\nforgetting. At the end of training, we can choose one universal embedding to\nproduce a single network dedicated to all tasks. In such a framework,\nhypernetwork is used only for training and can be seen as a meta-trainer.\nHyperInterval obtains significantly better results than InterContiNet and gives\nSOTA results on several benchmarks.\n", "link": "http://arxiv.org/abs/2405.15444v1", "date": "2024-05-24", "relevancy": 2.3528, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4839}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4735}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperInterval%3A%20Hypernetwork%20approach%20to%20training%20weight%20interval%20regions%0A%20%20in%20continual%20learning&body=Title%3A%20HyperInterval%3A%20Hypernetwork%20approach%20to%20training%20weight%20interval%20regions%0A%20%20in%20continual%20learning%0AAuthor%3A%20Patryk%20Krukowski%20and%20Anna%20Bielawska%20and%20Kamil%20Ksi%C4%85%C5%BCek%20and%20Pawe%C5%82%20Wawrzy%C5%84ski%20and%20Pawe%C5%82%20Batorski%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Recently%2C%20a%20new%20Continual%20Learning%20%28CL%29%20paradigm%20was%20presented%20to%20control%0Acatastrophic%20forgetting%2C%20called%20Interval%20Continual%20Learning%20%28InterContiNet%29%2C%0Awhich%20relies%20on%20enforcing%20interval%20constraints%20on%20the%20neural%20network%20parameter%0Aspace.%20Unfortunately%2C%20InterContiNet%20training%20is%20challenging%20due%20to%20the%20high%0Adimensionality%20of%20the%20weight%20space%2C%20making%20intervals%20difficult%20to%20manage.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20HyperInterval%2C%20a%20technique%20that%20employs%0Ainterval%20arithmetic%20within%20the%20embedding%20space%20and%20utilizes%20a%20hypernetwork%20to%0Amap%20these%20intervals%20to%20the%20target%20network%20parameter%20space.%20We%20train%20interval%0Aembeddings%20for%20consecutive%20tasks%20and%20train%20a%20hypernetwork%20to%20transform%20these%0Aembeddings%20into%20weights%20of%20the%20target%20network.%20An%20embedding%20for%20a%20given%20task%20is%0Atrained%20along%20with%20the%20hypernetwork%2C%20preserving%20the%20response%20of%20the%20target%0Anetwork%20for%20the%20previous%20task%20embeddings.%20Interval%20arithmetic%20works%20with%20a%20more%0Amanageable%2C%20lower-dimensional%20embedding%20space%20rather%20than%20directly%20preparing%0Aintervals%20in%20a%20high-dimensional%20weight%20space.%20Our%20model%20allows%20faster%20and%20more%0Aefficient%20training.%20Furthermore%2C%20HyperInterval%20maintains%20the%20guarantee%20of%20not%0Aforgetting.%20At%20the%20end%20of%20training%2C%20we%20can%20choose%20one%20universal%20embedding%20to%0Aproduce%20a%20single%20network%20dedicated%20to%20all%20tasks.%20In%20such%20a%20framework%2C%0Ahypernetwork%20is%20used%20only%20for%20training%20and%20can%20be%20seen%20as%20a%20meta-trainer.%0AHyperInterval%20obtains%20significantly%20better%20results%20than%20InterContiNet%20and%20gives%0ASOTA%20results%20on%20several%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperInterval%253A%2520Hypernetwork%2520approach%2520to%2520training%2520weight%2520interval%2520regions%250A%2520%2520in%2520continual%2520learning%26entry.906535625%3DPatryk%2520Krukowski%2520and%2520Anna%2520Bielawska%2520and%2520Kamil%2520Ksi%25C4%2585%25C5%25BCek%2520and%2520Pawe%25C5%2582%2520Wawrzy%25C5%2584ski%2520and%2520Pawe%25C5%2582%2520Batorski%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Recently%252C%2520a%2520new%2520Continual%2520Learning%2520%2528CL%2529%2520paradigm%2520was%2520presented%2520to%2520control%250Acatastrophic%2520forgetting%252C%2520called%2520Interval%2520Continual%2520Learning%2520%2528InterContiNet%2529%252C%250Awhich%2520relies%2520on%2520enforcing%2520interval%2520constraints%2520on%2520the%2520neural%2520network%2520parameter%250Aspace.%2520Unfortunately%252C%2520InterContiNet%2520training%2520is%2520challenging%2520due%2520to%2520the%2520high%250Adimensionality%2520of%2520the%2520weight%2520space%252C%2520making%2520intervals%2520difficult%2520to%2520manage.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520introduce%2520HyperInterval%252C%2520a%2520technique%2520that%2520employs%250Ainterval%2520arithmetic%2520within%2520the%2520embedding%2520space%2520and%2520utilizes%2520a%2520hypernetwork%2520to%250Amap%2520these%2520intervals%2520to%2520the%2520target%2520network%2520parameter%2520space.%2520We%2520train%2520interval%250Aembeddings%2520for%2520consecutive%2520tasks%2520and%2520train%2520a%2520hypernetwork%2520to%2520transform%2520these%250Aembeddings%2520into%2520weights%2520of%2520the%2520target%2520network.%2520An%2520embedding%2520for%2520a%2520given%2520task%2520is%250Atrained%2520along%2520with%2520the%2520hypernetwork%252C%2520preserving%2520the%2520response%2520of%2520the%2520target%250Anetwork%2520for%2520the%2520previous%2520task%2520embeddings.%2520Interval%2520arithmetic%2520works%2520with%2520a%2520more%250Amanageable%252C%2520lower-dimensional%2520embedding%2520space%2520rather%2520than%2520directly%2520preparing%250Aintervals%2520in%2520a%2520high-dimensional%2520weight%2520space.%2520Our%2520model%2520allows%2520faster%2520and%2520more%250Aefficient%2520training.%2520Furthermore%252C%2520HyperInterval%2520maintains%2520the%2520guarantee%2520of%2520not%250Aforgetting.%2520At%2520the%2520end%2520of%2520training%252C%2520we%2520can%2520choose%2520one%2520universal%2520embedding%2520to%250Aproduce%2520a%2520single%2520network%2520dedicated%2520to%2520all%2520tasks.%2520In%2520such%2520a%2520framework%252C%250Ahypernetwork%2520is%2520used%2520only%2520for%2520training%2520and%2520can%2520be%2520seen%2520as%2520a%2520meta-trainer.%250AHyperInterval%2520obtains%2520significantly%2520better%2520results%2520than%2520InterContiNet%2520and%2520gives%250ASOTA%2520results%2520on%2520several%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperInterval%3A%20Hypernetwork%20approach%20to%20training%20weight%20interval%20regions%0A%20%20in%20continual%20learning&entry.906535625=Patryk%20Krukowski%20and%20Anna%20Bielawska%20and%20Kamil%20Ksi%C4%85%C5%BCek%20and%20Pawe%C5%82%20Wawrzy%C5%84ski%20and%20Pawe%C5%82%20Batorski%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Recently%2C%20a%20new%20Continual%20Learning%20%28CL%29%20paradigm%20was%20presented%20to%20control%0Acatastrophic%20forgetting%2C%20called%20Interval%20Continual%20Learning%20%28InterContiNet%29%2C%0Awhich%20relies%20on%20enforcing%20interval%20constraints%20on%20the%20neural%20network%20parameter%0Aspace.%20Unfortunately%2C%20InterContiNet%20training%20is%20challenging%20due%20to%20the%20high%0Adimensionality%20of%20the%20weight%20space%2C%20making%20intervals%20difficult%20to%20manage.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20HyperInterval%2C%20a%20technique%20that%20employs%0Ainterval%20arithmetic%20within%20the%20embedding%20space%20and%20utilizes%20a%20hypernetwork%20to%0Amap%20these%20intervals%20to%20the%20target%20network%20parameter%20space.%20We%20train%20interval%0Aembeddings%20for%20consecutive%20tasks%20and%20train%20a%20hypernetwork%20to%20transform%20these%0Aembeddings%20into%20weights%20of%20the%20target%20network.%20An%20embedding%20for%20a%20given%20task%20is%0Atrained%20along%20with%20the%20hypernetwork%2C%20preserving%20the%20response%20of%20the%20target%0Anetwork%20for%20the%20previous%20task%20embeddings.%20Interval%20arithmetic%20works%20with%20a%20more%0Amanageable%2C%20lower-dimensional%20embedding%20space%20rather%20than%20directly%20preparing%0Aintervals%20in%20a%20high-dimensional%20weight%20space.%20Our%20model%20allows%20faster%20and%20more%0Aefficient%20training.%20Furthermore%2C%20HyperInterval%20maintains%20the%20guarantee%20of%20not%0Aforgetting.%20At%20the%20end%20of%20training%2C%20we%20can%20choose%20one%20universal%20embedding%20to%0Aproduce%20a%20single%20network%20dedicated%20to%20all%20tasks.%20In%20such%20a%20framework%2C%0Ahypernetwork%20is%20used%20only%20for%20training%20and%20can%20be%20seen%20as%20a%20meta-trainer.%0AHyperInterval%20obtains%20significantly%20better%20results%20than%20InterContiNet%20and%20gives%0ASOTA%20results%20on%20several%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15444v1&entry.124074799=Read"},
{"title": "Low-Light Video Enhancement via Spatial-Temporal Consistent Illumination\n  and Reflection Decomposition", "author": "Xiaogang Xu and Kun Zhou and Tao Hu and Ruixing Wang and Hujun Bao", "abstract": "  Low-Light Video Enhancement (LLVE) seeks to restore dynamic and static scenes\nplagued by severe invisibility and noise. One critical aspect is formulating a\nconsistency constraint specifically for temporal-spatial illumination and\nappearance enhanced versions, a dimension overlooked in existing methods. In\nthis paper, we present an innovative video Retinex-based decomposition strategy\nthat operates without the need for explicit supervision to delineate\nillumination and reflectance components. We leverage dynamic cross-frame\ncorrespondences for intrinsic appearance and enforce a scene-level continuity\nconstraint on the illumination field to yield satisfactory consistent\ndecomposition results. To further ensure consistent decomposition, we introduce\na dual-structure enhancement network featuring a novel cross-frame interaction\nmechanism. This mechanism can seamlessly integrate with encoder-decoder\nsingle-frame networks, incurring minimal additional parameter costs. By\nsupervising different frames simultaneously, this network encourages them to\nexhibit matching decomposition features, thus achieving the desired temporal\npropagation. Extensive experiments are conducted on widely recognized LLVE\nbenchmarks, covering diverse scenarios. Our framework consistently outperforms\nexisting methods, establishing a new state-of-the-art (SOTA) performance.\n", "link": "http://arxiv.org/abs/2405.15660v1", "date": "2024-05-24", "relevancy": 2.3419, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6072}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5731}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Light%20Video%20Enhancement%20via%20Spatial-Temporal%20Consistent%20Illumination%0A%20%20and%20Reflection%20Decomposition&body=Title%3A%20Low-Light%20Video%20Enhancement%20via%20Spatial-Temporal%20Consistent%20Illumination%0A%20%20and%20Reflection%20Decomposition%0AAuthor%3A%20Xiaogang%20Xu%20and%20Kun%20Zhou%20and%20Tao%20Hu%20and%20Ruixing%20Wang%20and%20Hujun%20Bao%0AAbstract%3A%20%20%20Low-Light%20Video%20Enhancement%20%28LLVE%29%20seeks%20to%20restore%20dynamic%20and%20static%20scenes%0Aplagued%20by%20severe%20invisibility%20and%20noise.%20One%20critical%20aspect%20is%20formulating%20a%0Aconsistency%20constraint%20specifically%20for%20temporal-spatial%20illumination%20and%0Aappearance%20enhanced%20versions%2C%20a%20dimension%20overlooked%20in%20existing%20methods.%20In%0Athis%20paper%2C%20we%20present%20an%20innovative%20video%20Retinex-based%20decomposition%20strategy%0Athat%20operates%20without%20the%20need%20for%20explicit%20supervision%20to%20delineate%0Aillumination%20and%20reflectance%20components.%20We%20leverage%20dynamic%20cross-frame%0Acorrespondences%20for%20intrinsic%20appearance%20and%20enforce%20a%20scene-level%20continuity%0Aconstraint%20on%20the%20illumination%20field%20to%20yield%20satisfactory%20consistent%0Adecomposition%20results.%20To%20further%20ensure%20consistent%20decomposition%2C%20we%20introduce%0Aa%20dual-structure%20enhancement%20network%20featuring%20a%20novel%20cross-frame%20interaction%0Amechanism.%20This%20mechanism%20can%20seamlessly%20integrate%20with%20encoder-decoder%0Asingle-frame%20networks%2C%20incurring%20minimal%20additional%20parameter%20costs.%20By%0Asupervising%20different%20frames%20simultaneously%2C%20this%20network%20encourages%20them%20to%0Aexhibit%20matching%20decomposition%20features%2C%20thus%20achieving%20the%20desired%20temporal%0Apropagation.%20Extensive%20experiments%20are%20conducted%20on%20widely%20recognized%20LLVE%0Abenchmarks%2C%20covering%20diverse%20scenarios.%20Our%20framework%20consistently%20outperforms%0Aexisting%20methods%2C%20establishing%20a%20new%20state-of-the-art%20%28SOTA%29%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Light%2520Video%2520Enhancement%2520via%2520Spatial-Temporal%2520Consistent%2520Illumination%250A%2520%2520and%2520Reflection%2520Decomposition%26entry.906535625%3DXiaogang%2520Xu%2520and%2520Kun%2520Zhou%2520and%2520Tao%2520Hu%2520and%2520Ruixing%2520Wang%2520and%2520Hujun%2520Bao%26entry.1292438233%3D%2520%2520Low-Light%2520Video%2520Enhancement%2520%2528LLVE%2529%2520seeks%2520to%2520restore%2520dynamic%2520and%2520static%2520scenes%250Aplagued%2520by%2520severe%2520invisibility%2520and%2520noise.%2520One%2520critical%2520aspect%2520is%2520formulating%2520a%250Aconsistency%2520constraint%2520specifically%2520for%2520temporal-spatial%2520illumination%2520and%250Aappearance%2520enhanced%2520versions%252C%2520a%2520dimension%2520overlooked%2520in%2520existing%2520methods.%2520In%250Athis%2520paper%252C%2520we%2520present%2520an%2520innovative%2520video%2520Retinex-based%2520decomposition%2520strategy%250Athat%2520operates%2520without%2520the%2520need%2520for%2520explicit%2520supervision%2520to%2520delineate%250Aillumination%2520and%2520reflectance%2520components.%2520We%2520leverage%2520dynamic%2520cross-frame%250Acorrespondences%2520for%2520intrinsic%2520appearance%2520and%2520enforce%2520a%2520scene-level%2520continuity%250Aconstraint%2520on%2520the%2520illumination%2520field%2520to%2520yield%2520satisfactory%2520consistent%250Adecomposition%2520results.%2520To%2520further%2520ensure%2520consistent%2520decomposition%252C%2520we%2520introduce%250Aa%2520dual-structure%2520enhancement%2520network%2520featuring%2520a%2520novel%2520cross-frame%2520interaction%250Amechanism.%2520This%2520mechanism%2520can%2520seamlessly%2520integrate%2520with%2520encoder-decoder%250Asingle-frame%2520networks%252C%2520incurring%2520minimal%2520additional%2520parameter%2520costs.%2520By%250Asupervising%2520different%2520frames%2520simultaneously%252C%2520this%2520network%2520encourages%2520them%2520to%250Aexhibit%2520matching%2520decomposition%2520features%252C%2520thus%2520achieving%2520the%2520desired%2520temporal%250Apropagation.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520widely%2520recognized%2520LLVE%250Abenchmarks%252C%2520covering%2520diverse%2520scenarios.%2520Our%2520framework%2520consistently%2520outperforms%250Aexisting%2520methods%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520%2528SOTA%2529%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Light%20Video%20Enhancement%20via%20Spatial-Temporal%20Consistent%20Illumination%0A%20%20and%20Reflection%20Decomposition&entry.906535625=Xiaogang%20Xu%20and%20Kun%20Zhou%20and%20Tao%20Hu%20and%20Ruixing%20Wang%20and%20Hujun%20Bao&entry.1292438233=%20%20Low-Light%20Video%20Enhancement%20%28LLVE%29%20seeks%20to%20restore%20dynamic%20and%20static%20scenes%0Aplagued%20by%20severe%20invisibility%20and%20noise.%20One%20critical%20aspect%20is%20formulating%20a%0Aconsistency%20constraint%20specifically%20for%20temporal-spatial%20illumination%20and%0Aappearance%20enhanced%20versions%2C%20a%20dimension%20overlooked%20in%20existing%20methods.%20In%0Athis%20paper%2C%20we%20present%20an%20innovative%20video%20Retinex-based%20decomposition%20strategy%0Athat%20operates%20without%20the%20need%20for%20explicit%20supervision%20to%20delineate%0Aillumination%20and%20reflectance%20components.%20We%20leverage%20dynamic%20cross-frame%0Acorrespondences%20for%20intrinsic%20appearance%20and%20enforce%20a%20scene-level%20continuity%0Aconstraint%20on%20the%20illumination%20field%20to%20yield%20satisfactory%20consistent%0Adecomposition%20results.%20To%20further%20ensure%20consistent%20decomposition%2C%20we%20introduce%0Aa%20dual-structure%20enhancement%20network%20featuring%20a%20novel%20cross-frame%20interaction%0Amechanism.%20This%20mechanism%20can%20seamlessly%20integrate%20with%20encoder-decoder%0Asingle-frame%20networks%2C%20incurring%20minimal%20additional%20parameter%20costs.%20By%0Asupervising%20different%20frames%20simultaneously%2C%20this%20network%20encourages%20them%20to%0Aexhibit%20matching%20decomposition%20features%2C%20thus%20achieving%20the%20desired%20temporal%0Apropagation.%20Extensive%20experiments%20are%20conducted%20on%20widely%20recognized%20LLVE%0Abenchmarks%2C%20covering%20diverse%20scenarios.%20Our%20framework%20consistently%20outperforms%0Aexisting%20methods%2C%20establishing%20a%20new%20state-of-the-art%20%28SOTA%29%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15660v1&entry.124074799=Read"},
{"title": "Dual Lagrangian Learning for Conic Optimization", "author": "Mathieu Tanneau and Pascal Van Hentenryck", "abstract": "  This paper presents Dual Lagrangian Learning (DLL), a principled learning\nmethodology for dual conic optimization proxies. DLL leverages conic duality\nand the representation power of ML models to provide high-duality,\ndual-feasible solutions, and therefore valid Lagrangian dual bounds, for linear\nand nonlinear conic optimization problems. The paper introduces a systematic\ndual completion procedure, differentiable conic projection layers, and a\nself-supervised learning framework based on Lagrangian duality. It also\nprovides closed-form dual completion formulae for broad classes of conic\nproblems, which eliminate the need for costly implicit layers. The\neffectiveness of DLL is demonstrated on linear and nonlinear conic optimization\nproblems. The proposed methodology significantly outperforms a state-of-the-art\nlearning-based method, and achieves 1000x speedups over commercial\ninterior-point solvers with optimality gaps under 0.5\\% on average.\n", "link": "http://arxiv.org/abs/2402.03086v2", "date": "2024-05-24", "relevancy": 2.3377, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4719}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4701}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Lagrangian%20Learning%20for%20Conic%20Optimization&body=Title%3A%20Dual%20Lagrangian%20Learning%20for%20Conic%20Optimization%0AAuthor%3A%20Mathieu%20Tanneau%20and%20Pascal%20Van%20Hentenryck%0AAbstract%3A%20%20%20This%20paper%20presents%20Dual%20Lagrangian%20Learning%20%28DLL%29%2C%20a%20principled%20learning%0Amethodology%20for%20dual%20conic%20optimization%20proxies.%20DLL%20leverages%20conic%20duality%0Aand%20the%20representation%20power%20of%20ML%20models%20to%20provide%20high-duality%2C%0Adual-feasible%20solutions%2C%20and%20therefore%20valid%20Lagrangian%20dual%20bounds%2C%20for%20linear%0Aand%20nonlinear%20conic%20optimization%20problems.%20The%20paper%20introduces%20a%20systematic%0Adual%20completion%20procedure%2C%20differentiable%20conic%20projection%20layers%2C%20and%20a%0Aself-supervised%20learning%20framework%20based%20on%20Lagrangian%20duality.%20It%20also%0Aprovides%20closed-form%20dual%20completion%20formulae%20for%20broad%20classes%20of%20conic%0Aproblems%2C%20which%20eliminate%20the%20need%20for%20costly%20implicit%20layers.%20The%0Aeffectiveness%20of%20DLL%20is%20demonstrated%20on%20linear%20and%20nonlinear%20conic%20optimization%0Aproblems.%20The%20proposed%20methodology%20significantly%20outperforms%20a%20state-of-the-art%0Alearning-based%20method%2C%20and%20achieves%201000x%20speedups%20over%20commercial%0Ainterior-point%20solvers%20with%20optimality%20gaps%20under%200.5%5C%25%20on%20average.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Lagrangian%2520Learning%2520for%2520Conic%2520Optimization%26entry.906535625%3DMathieu%2520Tanneau%2520and%2520Pascal%2520Van%2520Hentenryck%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Dual%2520Lagrangian%2520Learning%2520%2528DLL%2529%252C%2520a%2520principled%2520learning%250Amethodology%2520for%2520dual%2520conic%2520optimization%2520proxies.%2520DLL%2520leverages%2520conic%2520duality%250Aand%2520the%2520representation%2520power%2520of%2520ML%2520models%2520to%2520provide%2520high-duality%252C%250Adual-feasible%2520solutions%252C%2520and%2520therefore%2520valid%2520Lagrangian%2520dual%2520bounds%252C%2520for%2520linear%250Aand%2520nonlinear%2520conic%2520optimization%2520problems.%2520The%2520paper%2520introduces%2520a%2520systematic%250Adual%2520completion%2520procedure%252C%2520differentiable%2520conic%2520projection%2520layers%252C%2520and%2520a%250Aself-supervised%2520learning%2520framework%2520based%2520on%2520Lagrangian%2520duality.%2520It%2520also%250Aprovides%2520closed-form%2520dual%2520completion%2520formulae%2520for%2520broad%2520classes%2520of%2520conic%250Aproblems%252C%2520which%2520eliminate%2520the%2520need%2520for%2520costly%2520implicit%2520layers.%2520The%250Aeffectiveness%2520of%2520DLL%2520is%2520demonstrated%2520on%2520linear%2520and%2520nonlinear%2520conic%2520optimization%250Aproblems.%2520The%2520proposed%2520methodology%2520significantly%2520outperforms%2520a%2520state-of-the-art%250Alearning-based%2520method%252C%2520and%2520achieves%25201000x%2520speedups%2520over%2520commercial%250Ainterior-point%2520solvers%2520with%2520optimality%2520gaps%2520under%25200.5%255C%2525%2520on%2520average.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Lagrangian%20Learning%20for%20Conic%20Optimization&entry.906535625=Mathieu%20Tanneau%20and%20Pascal%20Van%20Hentenryck&entry.1292438233=%20%20This%20paper%20presents%20Dual%20Lagrangian%20Learning%20%28DLL%29%2C%20a%20principled%20learning%0Amethodology%20for%20dual%20conic%20optimization%20proxies.%20DLL%20leverages%20conic%20duality%0Aand%20the%20representation%20power%20of%20ML%20models%20to%20provide%20high-duality%2C%0Adual-feasible%20solutions%2C%20and%20therefore%20valid%20Lagrangian%20dual%20bounds%2C%20for%20linear%0Aand%20nonlinear%20conic%20optimization%20problems.%20The%20paper%20introduces%20a%20systematic%0Adual%20completion%20procedure%2C%20differentiable%20conic%20projection%20layers%2C%20and%20a%0Aself-supervised%20learning%20framework%20based%20on%20Lagrangian%20duality.%20It%20also%0Aprovides%20closed-form%20dual%20completion%20formulae%20for%20broad%20classes%20of%20conic%0Aproblems%2C%20which%20eliminate%20the%20need%20for%20costly%20implicit%20layers.%20The%0Aeffectiveness%20of%20DLL%20is%20demonstrated%20on%20linear%20and%20nonlinear%20conic%20optimization%0Aproblems.%20The%20proposed%20methodology%20significantly%20outperforms%20a%20state-of-the-art%0Alearning-based%20method%2C%20and%20achieves%201000x%20speedups%20over%20commercial%0Ainterior-point%20solvers%20with%20optimality%20gaps%20under%200.5%5C%25%20on%20average.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03086v2&entry.124074799=Read"},
{"title": "A Systematic Review on Custom Data Gloves", "author": "Valerio Belcamino and Alessandro Carf\u00ec and Fulvio Mastrogiovanni", "abstract": "  Hands are a fundamental tool humans use to interact with the environment and\nobjects. Through hand motions, we can obtain information about the shape and\nmaterials of the surfaces we touch, modify our surroundings by interacting with\nobjects, manipulate objects and tools, or communicate with other people by\nleveraging the power of gestures. For these reasons, sensorized gloves, which\ncan collect information about hand motions and interactions, have been of\ninterest since the 1980s in various fields, such as Human-Machine Interaction\n(HMI) and the analysis and control of human motions.\n  Over the last 40 years, research in this field explored different\ntechnological approaches and contributed to the popularity of wearable custom\nand commercial products targeting hand sensorization. Despite a positive\nresearch trend, these instruments are not widespread yet outside research\nenvironments and devices aimed at research are often ad hoc solutions with a\nlow chance of being reused. This paper aims to provide a systematic literature\nreview for custom gloves to analyze their main characteristics and critical\nissues, from the type and number of sensors to the limitations due to device\nencumbrance. The collection of this information lays the foundation for a\nstandardization process necessary for future breakthroughs in this research\nfield.\n", "link": "http://arxiv.org/abs/2405.15417v1", "date": "2024-05-24", "relevancy": 2.3366, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4761}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4667}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Review%20on%20Custom%20Data%20Gloves&body=Title%3A%20A%20Systematic%20Review%20on%20Custom%20Data%20Gloves%0AAuthor%3A%20Valerio%20Belcamino%20and%20Alessandro%20Carf%C3%AC%20and%20Fulvio%20Mastrogiovanni%0AAbstract%3A%20%20%20Hands%20are%20a%20fundamental%20tool%20humans%20use%20to%20interact%20with%20the%20environment%20and%0Aobjects.%20Through%20hand%20motions%2C%20we%20can%20obtain%20information%20about%20the%20shape%20and%0Amaterials%20of%20the%20surfaces%20we%20touch%2C%20modify%20our%20surroundings%20by%20interacting%20with%0Aobjects%2C%20manipulate%20objects%20and%20tools%2C%20or%20communicate%20with%20other%20people%20by%0Aleveraging%20the%20power%20of%20gestures.%20For%20these%20reasons%2C%20sensorized%20gloves%2C%20which%0Acan%20collect%20information%20about%20hand%20motions%20and%20interactions%2C%20have%20been%20of%0Ainterest%20since%20the%201980s%20in%20various%20fields%2C%20such%20as%20Human-Machine%20Interaction%0A%28HMI%29%20and%20the%20analysis%20and%20control%20of%20human%20motions.%0A%20%20Over%20the%20last%2040%20years%2C%20research%20in%20this%20field%20explored%20different%0Atechnological%20approaches%20and%20contributed%20to%20the%20popularity%20of%20wearable%20custom%0Aand%20commercial%20products%20targeting%20hand%20sensorization.%20Despite%20a%20positive%0Aresearch%20trend%2C%20these%20instruments%20are%20not%20widespread%20yet%20outside%20research%0Aenvironments%20and%20devices%20aimed%20at%20research%20are%20often%20ad%20hoc%20solutions%20with%20a%0Alow%20chance%20of%20being%20reused.%20This%20paper%20aims%20to%20provide%20a%20systematic%20literature%0Areview%20for%20custom%20gloves%20to%20analyze%20their%20main%20characteristics%20and%20critical%0Aissues%2C%20from%20the%20type%20and%20number%20of%20sensors%20to%20the%20limitations%20due%20to%20device%0Aencumbrance.%20The%20collection%20of%20this%20information%20lays%20the%20foundation%20for%20a%0Astandardization%20process%20necessary%20for%20future%20breakthroughs%20in%20this%20research%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Review%2520on%2520Custom%2520Data%2520Gloves%26entry.906535625%3DValerio%2520Belcamino%2520and%2520Alessandro%2520Carf%25C3%25AC%2520and%2520Fulvio%2520Mastrogiovanni%26entry.1292438233%3D%2520%2520Hands%2520are%2520a%2520fundamental%2520tool%2520humans%2520use%2520to%2520interact%2520with%2520the%2520environment%2520and%250Aobjects.%2520Through%2520hand%2520motions%252C%2520we%2520can%2520obtain%2520information%2520about%2520the%2520shape%2520and%250Amaterials%2520of%2520the%2520surfaces%2520we%2520touch%252C%2520modify%2520our%2520surroundings%2520by%2520interacting%2520with%250Aobjects%252C%2520manipulate%2520objects%2520and%2520tools%252C%2520or%2520communicate%2520with%2520other%2520people%2520by%250Aleveraging%2520the%2520power%2520of%2520gestures.%2520For%2520these%2520reasons%252C%2520sensorized%2520gloves%252C%2520which%250Acan%2520collect%2520information%2520about%2520hand%2520motions%2520and%2520interactions%252C%2520have%2520been%2520of%250Ainterest%2520since%2520the%25201980s%2520in%2520various%2520fields%252C%2520such%2520as%2520Human-Machine%2520Interaction%250A%2528HMI%2529%2520and%2520the%2520analysis%2520and%2520control%2520of%2520human%2520motions.%250A%2520%2520Over%2520the%2520last%252040%2520years%252C%2520research%2520in%2520this%2520field%2520explored%2520different%250Atechnological%2520approaches%2520and%2520contributed%2520to%2520the%2520popularity%2520of%2520wearable%2520custom%250Aand%2520commercial%2520products%2520targeting%2520hand%2520sensorization.%2520Despite%2520a%2520positive%250Aresearch%2520trend%252C%2520these%2520instruments%2520are%2520not%2520widespread%2520yet%2520outside%2520research%250Aenvironments%2520and%2520devices%2520aimed%2520at%2520research%2520are%2520often%2520ad%2520hoc%2520solutions%2520with%2520a%250Alow%2520chance%2520of%2520being%2520reused.%2520This%2520paper%2520aims%2520to%2520provide%2520a%2520systematic%2520literature%250Areview%2520for%2520custom%2520gloves%2520to%2520analyze%2520their%2520main%2520characteristics%2520and%2520critical%250Aissues%252C%2520from%2520the%2520type%2520and%2520number%2520of%2520sensors%2520to%2520the%2520limitations%2520due%2520to%2520device%250Aencumbrance.%2520The%2520collection%2520of%2520this%2520information%2520lays%2520the%2520foundation%2520for%2520a%250Astandardization%2520process%2520necessary%2520for%2520future%2520breakthroughs%2520in%2520this%2520research%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Review%20on%20Custom%20Data%20Gloves&entry.906535625=Valerio%20Belcamino%20and%20Alessandro%20Carf%C3%AC%20and%20Fulvio%20Mastrogiovanni&entry.1292438233=%20%20Hands%20are%20a%20fundamental%20tool%20humans%20use%20to%20interact%20with%20the%20environment%20and%0Aobjects.%20Through%20hand%20motions%2C%20we%20can%20obtain%20information%20about%20the%20shape%20and%0Amaterials%20of%20the%20surfaces%20we%20touch%2C%20modify%20our%20surroundings%20by%20interacting%20with%0Aobjects%2C%20manipulate%20objects%20and%20tools%2C%20or%20communicate%20with%20other%20people%20by%0Aleveraging%20the%20power%20of%20gestures.%20For%20these%20reasons%2C%20sensorized%20gloves%2C%20which%0Acan%20collect%20information%20about%20hand%20motions%20and%20interactions%2C%20have%20been%20of%0Ainterest%20since%20the%201980s%20in%20various%20fields%2C%20such%20as%20Human-Machine%20Interaction%0A%28HMI%29%20and%20the%20analysis%20and%20control%20of%20human%20motions.%0A%20%20Over%20the%20last%2040%20years%2C%20research%20in%20this%20field%20explored%20different%0Atechnological%20approaches%20and%20contributed%20to%20the%20popularity%20of%20wearable%20custom%0Aand%20commercial%20products%20targeting%20hand%20sensorization.%20Despite%20a%20positive%0Aresearch%20trend%2C%20these%20instruments%20are%20not%20widespread%20yet%20outside%20research%0Aenvironments%20and%20devices%20aimed%20at%20research%20are%20often%20ad%20hoc%20solutions%20with%20a%0Alow%20chance%20of%20being%20reused.%20This%20paper%20aims%20to%20provide%20a%20systematic%20literature%0Areview%20for%20custom%20gloves%20to%20analyze%20their%20main%20characteristics%20and%20critical%0Aissues%2C%20from%20the%20type%20and%20number%20of%20sensors%20to%20the%20limitations%20due%20to%20device%0Aencumbrance.%20The%20collection%20of%20this%20information%20lays%20the%20foundation%20for%20a%0Astandardization%20process%20necessary%20for%20future%20breakthroughs%20in%20this%20research%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15417v1&entry.124074799=Read"},
{"title": "SynGhost: Imperceptible and Universal Task-agnostic Backdoor Attack in\n  Pre-trained Language Models", "author": "Pengzhou Cheng and Wei Du and Zongru Wu and Fengwei Zhang and Libo Chen and Gongshen Liu", "abstract": "  Pre-training has been a necessary phase for deploying pre-trained language\nmodels (PLMs) to achieve remarkable performance in downstream tasks. However,\nwe empirically show that backdoor attacks exploit such a phase as a vulnerable\nentry point for task-agnostic. In this paper, we first propose\n$\\mathtt{maxEntropy}$, an entropy-based poisoning filtering defense, to prove\nthat existing task-agnostic backdoors are easily exposed, due to explicit\ntriggers used. Then, we present $\\mathtt{SynGhost}$, an imperceptible and\nuniversal task-agnostic backdoor attack in PLMs. Specifically,\n$\\mathtt{SynGhost}$ hostilely manipulates clean samples through different\nsyntactic and then maps the backdoor to representation space without disturbing\nthe primitive representation. $\\mathtt{SynGhost}$ further leverages contrastive\nlearning to achieve universal, which performs a uniform distribution of\nbackdoors in the representation space. In light of the syntactic properties, we\nalso introduce an awareness module to alleviate the interference between\ndifferent syntactic. Experiments show that $\\mathtt{SynGhost}$ holds more\nserious threats. Not only do severe harmfulness to various downstream tasks on\ntwo tuning paradigms but also to any PLMs. Meanwhile, $\\mathtt{SynGhost}$ is\nimperceptible against three countermeasures based on perplexity, fine-pruning,\nand the proposed $\\mathtt{maxEntropy}$.\n", "link": "http://arxiv.org/abs/2402.18945v2", "date": "2024-05-24", "relevancy": 2.3347, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4771}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4718}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynGhost%3A%20Imperceptible%20and%20Universal%20Task-agnostic%20Backdoor%20Attack%20in%0A%20%20Pre-trained%20Language%20Models&body=Title%3A%20SynGhost%3A%20Imperceptible%20and%20Universal%20Task-agnostic%20Backdoor%20Attack%20in%0A%20%20Pre-trained%20Language%20Models%0AAuthor%3A%20Pengzhou%20Cheng%20and%20Wei%20Du%20and%20Zongru%20Wu%20and%20Fengwei%20Zhang%20and%20Libo%20Chen%20and%20Gongshen%20Liu%0AAbstract%3A%20%20%20Pre-training%20has%20been%20a%20necessary%20phase%20for%20deploying%20pre-trained%20language%0Amodels%20%28PLMs%29%20to%20achieve%20remarkable%20performance%20in%20downstream%20tasks.%20However%2C%0Awe%20empirically%20show%20that%20backdoor%20attacks%20exploit%20such%20a%20phase%20as%20a%20vulnerable%0Aentry%20point%20for%20task-agnostic.%20In%20this%20paper%2C%20we%20first%20propose%0A%24%5Cmathtt%7BmaxEntropy%7D%24%2C%20an%20entropy-based%20poisoning%20filtering%20defense%2C%20to%20prove%0Athat%20existing%20task-agnostic%20backdoors%20are%20easily%20exposed%2C%20due%20to%20explicit%0Atriggers%20used.%20Then%2C%20we%20present%20%24%5Cmathtt%7BSynGhost%7D%24%2C%20an%20imperceptible%20and%0Auniversal%20task-agnostic%20backdoor%20attack%20in%20PLMs.%20Specifically%2C%0A%24%5Cmathtt%7BSynGhost%7D%24%20hostilely%20manipulates%20clean%20samples%20through%20different%0Asyntactic%20and%20then%20maps%20the%20backdoor%20to%20representation%20space%20without%20disturbing%0Athe%20primitive%20representation.%20%24%5Cmathtt%7BSynGhost%7D%24%20further%20leverages%20contrastive%0Alearning%20to%20achieve%20universal%2C%20which%20performs%20a%20uniform%20distribution%20of%0Abackdoors%20in%20the%20representation%20space.%20In%20light%20of%20the%20syntactic%20properties%2C%20we%0Aalso%20introduce%20an%20awareness%20module%20to%20alleviate%20the%20interference%20between%0Adifferent%20syntactic.%20Experiments%20show%20that%20%24%5Cmathtt%7BSynGhost%7D%24%20holds%20more%0Aserious%20threats.%20Not%20only%20do%20severe%20harmfulness%20to%20various%20downstream%20tasks%20on%0Atwo%20tuning%20paradigms%20but%20also%20to%20any%20PLMs.%20Meanwhile%2C%20%24%5Cmathtt%7BSynGhost%7D%24%20is%0Aimperceptible%20against%20three%20countermeasures%20based%20on%20perplexity%2C%20fine-pruning%2C%0Aand%20the%20proposed%20%24%5Cmathtt%7BmaxEntropy%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynGhost%253A%2520Imperceptible%2520and%2520Universal%2520Task-agnostic%2520Backdoor%2520Attack%2520in%250A%2520%2520Pre-trained%2520Language%2520Models%26entry.906535625%3DPengzhou%2520Cheng%2520and%2520Wei%2520Du%2520and%2520Zongru%2520Wu%2520and%2520Fengwei%2520Zhang%2520and%2520Libo%2520Chen%2520and%2520Gongshen%2520Liu%26entry.1292438233%3D%2520%2520Pre-training%2520has%2520been%2520a%2520necessary%2520phase%2520for%2520deploying%2520pre-trained%2520language%250Amodels%2520%2528PLMs%2529%2520to%2520achieve%2520remarkable%2520performance%2520in%2520downstream%2520tasks.%2520However%252C%250Awe%2520empirically%2520show%2520that%2520backdoor%2520attacks%2520exploit%2520such%2520a%2520phase%2520as%2520a%2520vulnerable%250Aentry%2520point%2520for%2520task-agnostic.%2520In%2520this%2520paper%252C%2520we%2520first%2520propose%250A%2524%255Cmathtt%257BmaxEntropy%257D%2524%252C%2520an%2520entropy-based%2520poisoning%2520filtering%2520defense%252C%2520to%2520prove%250Athat%2520existing%2520task-agnostic%2520backdoors%2520are%2520easily%2520exposed%252C%2520due%2520to%2520explicit%250Atriggers%2520used.%2520Then%252C%2520we%2520present%2520%2524%255Cmathtt%257BSynGhost%257D%2524%252C%2520an%2520imperceptible%2520and%250Auniversal%2520task-agnostic%2520backdoor%2520attack%2520in%2520PLMs.%2520Specifically%252C%250A%2524%255Cmathtt%257BSynGhost%257D%2524%2520hostilely%2520manipulates%2520clean%2520samples%2520through%2520different%250Asyntactic%2520and%2520then%2520maps%2520the%2520backdoor%2520to%2520representation%2520space%2520without%2520disturbing%250Athe%2520primitive%2520representation.%2520%2524%255Cmathtt%257BSynGhost%257D%2524%2520further%2520leverages%2520contrastive%250Alearning%2520to%2520achieve%2520universal%252C%2520which%2520performs%2520a%2520uniform%2520distribution%2520of%250Abackdoors%2520in%2520the%2520representation%2520space.%2520In%2520light%2520of%2520the%2520syntactic%2520properties%252C%2520we%250Aalso%2520introduce%2520an%2520awareness%2520module%2520to%2520alleviate%2520the%2520interference%2520between%250Adifferent%2520syntactic.%2520Experiments%2520show%2520that%2520%2524%255Cmathtt%257BSynGhost%257D%2524%2520holds%2520more%250Aserious%2520threats.%2520Not%2520only%2520do%2520severe%2520harmfulness%2520to%2520various%2520downstream%2520tasks%2520on%250Atwo%2520tuning%2520paradigms%2520but%2520also%2520to%2520any%2520PLMs.%2520Meanwhile%252C%2520%2524%255Cmathtt%257BSynGhost%257D%2524%2520is%250Aimperceptible%2520against%2520three%2520countermeasures%2520based%2520on%2520perplexity%252C%2520fine-pruning%252C%250Aand%2520the%2520proposed%2520%2524%255Cmathtt%257BmaxEntropy%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynGhost%3A%20Imperceptible%20and%20Universal%20Task-agnostic%20Backdoor%20Attack%20in%0A%20%20Pre-trained%20Language%20Models&entry.906535625=Pengzhou%20Cheng%20and%20Wei%20Du%20and%20Zongru%20Wu%20and%20Fengwei%20Zhang%20and%20Libo%20Chen%20and%20Gongshen%20Liu&entry.1292438233=%20%20Pre-training%20has%20been%20a%20necessary%20phase%20for%20deploying%20pre-trained%20language%0Amodels%20%28PLMs%29%20to%20achieve%20remarkable%20performance%20in%20downstream%20tasks.%20However%2C%0Awe%20empirically%20show%20that%20backdoor%20attacks%20exploit%20such%20a%20phase%20as%20a%20vulnerable%0Aentry%20point%20for%20task-agnostic.%20In%20this%20paper%2C%20we%20first%20propose%0A%24%5Cmathtt%7BmaxEntropy%7D%24%2C%20an%20entropy-based%20poisoning%20filtering%20defense%2C%20to%20prove%0Athat%20existing%20task-agnostic%20backdoors%20are%20easily%20exposed%2C%20due%20to%20explicit%0Atriggers%20used.%20Then%2C%20we%20present%20%24%5Cmathtt%7BSynGhost%7D%24%2C%20an%20imperceptible%20and%0Auniversal%20task-agnostic%20backdoor%20attack%20in%20PLMs.%20Specifically%2C%0A%24%5Cmathtt%7BSynGhost%7D%24%20hostilely%20manipulates%20clean%20samples%20through%20different%0Asyntactic%20and%20then%20maps%20the%20backdoor%20to%20representation%20space%20without%20disturbing%0Athe%20primitive%20representation.%20%24%5Cmathtt%7BSynGhost%7D%24%20further%20leverages%20contrastive%0Alearning%20to%20achieve%20universal%2C%20which%20performs%20a%20uniform%20distribution%20of%0Abackdoors%20in%20the%20representation%20space.%20In%20light%20of%20the%20syntactic%20properties%2C%20we%0Aalso%20introduce%20an%20awareness%20module%20to%20alleviate%20the%20interference%20between%0Adifferent%20syntactic.%20Experiments%20show%20that%20%24%5Cmathtt%7BSynGhost%7D%24%20holds%20more%0Aserious%20threats.%20Not%20only%20do%20severe%20harmfulness%20to%20various%20downstream%20tasks%20on%0Atwo%20tuning%20paradigms%20but%20also%20to%20any%20PLMs.%20Meanwhile%2C%20%24%5Cmathtt%7BSynGhost%7D%24%20is%0Aimperceptible%20against%20three%20countermeasures%20based%20on%20perplexity%2C%20fine-pruning%2C%0Aand%20the%20proposed%20%24%5Cmathtt%7BmaxEntropy%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18945v2&entry.124074799=Read"},
{"title": "Autoregressive Image Diffusion: Generation of Image Sequence and\n  Application in MRI", "author": "Guanxiong Luo and Shoujin Huang and Martin Uecker", "abstract": "  Magnetic resonance imaging (MRI) is a widely used non-invasive imaging\nmodality. However, a persistent challenge lies in balancing image quality with\nimaging speed. This trade-off is primarily constrained by k-space measurements,\nwhich traverse specific trajectories in the spatial Fourier domain (k-space).\nThese measurements are often undersampled to shorten acquisition times,\nresulting in image artifacts and compromised quality. Generative models learn\nimage distributions and can be used to reconstruct high-quality images from\nundersampled k-space data. In this work, we present the autoregressive image\ndiffusion (AID) model for image sequences and use it to sample the posterior\nfor accelerated MRI reconstruction. The algorithm incorporates both\nundersampled k-space and pre-existing information. Models trained with fastMRI\ndataset are evaluated comprehensively. The results show that the AID model can\nrobustly generate sequentially coherent image sequences. In 3D and dynamic MRI,\nthe AID can outperform the standard diffusion model and reduce hallucinations,\ndue to the learned inter-image dependencies.\n", "link": "http://arxiv.org/abs/2405.14327v2", "date": "2024-05-24", "relevancy": 2.3276, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6021}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5779}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI&body=Title%3A%20Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI%0AAuthor%3A%20Guanxiong%20Luo%20and%20Shoujin%20Huang%20and%20Martin%20Uecker%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20widely%20used%20non-invasive%20imaging%0Amodality.%20However%2C%20a%20persistent%20challenge%20lies%20in%20balancing%20image%20quality%20with%0Aimaging%20speed.%20This%20trade-off%20is%20primarily%20constrained%20by%20k-space%20measurements%2C%0Awhich%20traverse%20specific%20trajectories%20in%20the%20spatial%20Fourier%20domain%20%28k-space%29.%0AThese%20measurements%20are%20often%20undersampled%20to%20shorten%20acquisition%20times%2C%0Aresulting%20in%20image%20artifacts%20and%20compromised%20quality.%20Generative%20models%20learn%0Aimage%20distributions%20and%20can%20be%20used%20to%20reconstruct%20high-quality%20images%20from%0Aundersampled%20k-space%20data.%20In%20this%20work%2C%20we%20present%20the%20autoregressive%20image%0Adiffusion%20%28AID%29%20model%20for%20image%20sequences%20and%20use%20it%20to%20sample%20the%20posterior%0Afor%20accelerated%20MRI%20reconstruction.%20The%20algorithm%20incorporates%20both%0Aundersampled%20k-space%20and%20pre-existing%20information.%20Models%20trained%20with%20fastMRI%0Adataset%20are%20evaluated%20comprehensively.%20The%20results%20show%20that%20the%20AID%20model%20can%0Arobustly%20generate%20sequentially%20coherent%20image%20sequences.%20In%203D%20and%20dynamic%20MRI%2C%0Athe%20AID%20can%20outperform%20the%20standard%20diffusion%20model%20and%20reduce%20hallucinations%2C%0Adue%20to%20the%20learned%20inter-image%20dependencies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Image%2520Diffusion%253A%2520Generation%2520of%2520Image%2520Sequence%2520and%250A%2520%2520Application%2520in%2520MRI%26entry.906535625%3DGuanxiong%2520Luo%2520and%2520Shoujin%2520Huang%2520and%2520Martin%2520Uecker%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520a%2520widely%2520used%2520non-invasive%2520imaging%250Amodality.%2520However%252C%2520a%2520persistent%2520challenge%2520lies%2520in%2520balancing%2520image%2520quality%2520with%250Aimaging%2520speed.%2520This%2520trade-off%2520is%2520primarily%2520constrained%2520by%2520k-space%2520measurements%252C%250Awhich%2520traverse%2520specific%2520trajectories%2520in%2520the%2520spatial%2520Fourier%2520domain%2520%2528k-space%2529.%250AThese%2520measurements%2520are%2520often%2520undersampled%2520to%2520shorten%2520acquisition%2520times%252C%250Aresulting%2520in%2520image%2520artifacts%2520and%2520compromised%2520quality.%2520Generative%2520models%2520learn%250Aimage%2520distributions%2520and%2520can%2520be%2520used%2520to%2520reconstruct%2520high-quality%2520images%2520from%250Aundersampled%2520k-space%2520data.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520autoregressive%2520image%250Adiffusion%2520%2528AID%2529%2520model%2520for%2520image%2520sequences%2520and%2520use%2520it%2520to%2520sample%2520the%2520posterior%250Afor%2520accelerated%2520MRI%2520reconstruction.%2520The%2520algorithm%2520incorporates%2520both%250Aundersampled%2520k-space%2520and%2520pre-existing%2520information.%2520Models%2520trained%2520with%2520fastMRI%250Adataset%2520are%2520evaluated%2520comprehensively.%2520The%2520results%2520show%2520that%2520the%2520AID%2520model%2520can%250Arobustly%2520generate%2520sequentially%2520coherent%2520image%2520sequences.%2520In%25203D%2520and%2520dynamic%2520MRI%252C%250Athe%2520AID%2520can%2520outperform%2520the%2520standard%2520diffusion%2520model%2520and%2520reduce%2520hallucinations%252C%250Adue%2520to%2520the%2520learned%2520inter-image%2520dependencies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI&entry.906535625=Guanxiong%20Luo%20and%20Shoujin%20Huang%20and%20Martin%20Uecker&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20widely%20used%20non-invasive%20imaging%0Amodality.%20However%2C%20a%20persistent%20challenge%20lies%20in%20balancing%20image%20quality%20with%0Aimaging%20speed.%20This%20trade-off%20is%20primarily%20constrained%20by%20k-space%20measurements%2C%0Awhich%20traverse%20specific%20trajectories%20in%20the%20spatial%20Fourier%20domain%20%28k-space%29.%0AThese%20measurements%20are%20often%20undersampled%20to%20shorten%20acquisition%20times%2C%0Aresulting%20in%20image%20artifacts%20and%20compromised%20quality.%20Generative%20models%20learn%0Aimage%20distributions%20and%20can%20be%20used%20to%20reconstruct%20high-quality%20images%20from%0Aundersampled%20k-space%20data.%20In%20this%20work%2C%20we%20present%20the%20autoregressive%20image%0Adiffusion%20%28AID%29%20model%20for%20image%20sequences%20and%20use%20it%20to%20sample%20the%20posterior%0Afor%20accelerated%20MRI%20reconstruction.%20The%20algorithm%20incorporates%20both%0Aundersampled%20k-space%20and%20pre-existing%20information.%20Models%20trained%20with%20fastMRI%0Adataset%20are%20evaluated%20comprehensively.%20The%20results%20show%20that%20the%20AID%20model%20can%0Arobustly%20generate%20sequentially%20coherent%20image%20sequences.%20In%203D%20and%20dynamic%20MRI%2C%0Athe%20AID%20can%20outperform%20the%20standard%20diffusion%20model%20and%20reduce%20hallucinations%2C%0Adue%20to%20the%20learned%20inter-image%20dependencies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14327v2&entry.124074799=Read"},
{"title": "Text-guided 3D Human Motion Generation with Keyframe-based Parallel Skip\n  Transformer", "author": "Zichen Geng and Caren Han and Zeeshan Hayder and Jian Liu and Mubarak Shah and Ajmal Mian", "abstract": "  Text-driven human motion generation is an emerging task in animation and\nhumanoid robot design. Existing algorithms directly generate the full sequence\nwhich is computationally expensive and prone to errors as it does not pay\nspecial attention to key poses, a process that has been the cornerstone of\nanimation for decades. We propose KeyMotion, that generates plausible human\nmotion sequences corresponding to input text by first generating keyframes\nfollowed by in-filling. We use a Variational Autoencoder (VAE) with\nKullback-Leibler regularization to project the keyframes into a latent space to\nreduce dimensionality and further accelerate the subsequent diffusion process.\nFor the reverse diffusion, we propose a novel Parallel Skip Transformer that\nperforms cross-modal attention between the keyframe latents and text condition.\nTo complete the motion sequence, we propose a text-guided Transformer designed\nto perform motion-in-filling, ensuring the preservation of both fidelity and\nadherence to the physical constraints of human motion. Experiments show that\nour method achieves state-of-theart results on the HumanML3D dataset\noutperforming others on all R-precision metrics and MultiModal Distance.\nKeyMotion also achieves competitive performance on the KIT dataset, achieving\nthe best results on Top3 R-precision, FID, and Diversity metrics.\n", "link": "http://arxiv.org/abs/2405.15439v1", "date": "2024-05-24", "relevancy": 2.3255, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6173}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5745}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-guided%203D%20Human%20Motion%20Generation%20with%20Keyframe-based%20Parallel%20Skip%0A%20%20Transformer&body=Title%3A%20Text-guided%203D%20Human%20Motion%20Generation%20with%20Keyframe-based%20Parallel%20Skip%0A%20%20Transformer%0AAuthor%3A%20Zichen%20Geng%20and%20Caren%20Han%20and%20Zeeshan%20Hayder%20and%20Jian%20Liu%20and%20Mubarak%20Shah%20and%20Ajmal%20Mian%0AAbstract%3A%20%20%20Text-driven%20human%20motion%20generation%20is%20an%20emerging%20task%20in%20animation%20and%0Ahumanoid%20robot%20design.%20Existing%20algorithms%20directly%20generate%20the%20full%20sequence%0Awhich%20is%20computationally%20expensive%20and%20prone%20to%20errors%20as%20it%20does%20not%20pay%0Aspecial%20attention%20to%20key%20poses%2C%20a%20process%20that%20has%20been%20the%20cornerstone%20of%0Aanimation%20for%20decades.%20We%20propose%20KeyMotion%2C%20that%20generates%20plausible%20human%0Amotion%20sequences%20corresponding%20to%20input%20text%20by%20first%20generating%20keyframes%0Afollowed%20by%20in-filling.%20We%20use%20a%20Variational%20Autoencoder%20%28VAE%29%20with%0AKullback-Leibler%20regularization%20to%20project%20the%20keyframes%20into%20a%20latent%20space%20to%0Areduce%20dimensionality%20and%20further%20accelerate%20the%20subsequent%20diffusion%20process.%0AFor%20the%20reverse%20diffusion%2C%20we%20propose%20a%20novel%20Parallel%20Skip%20Transformer%20that%0Aperforms%20cross-modal%20attention%20between%20the%20keyframe%20latents%20and%20text%20condition.%0ATo%20complete%20the%20motion%20sequence%2C%20we%20propose%20a%20text-guided%20Transformer%20designed%0Ato%20perform%20motion-in-filling%2C%20ensuring%20the%20preservation%20of%20both%20fidelity%20and%0Aadherence%20to%20the%20physical%20constraints%20of%20human%20motion.%20Experiments%20show%20that%0Aour%20method%20achieves%20state-of-theart%20results%20on%20the%20HumanML3D%20dataset%0Aoutperforming%20others%20on%20all%20R-precision%20metrics%20and%20MultiModal%20Distance.%0AKeyMotion%20also%20achieves%20competitive%20performance%20on%20the%20KIT%20dataset%2C%20achieving%0Athe%20best%20results%20on%20Top3%20R-precision%2C%20FID%2C%20and%20Diversity%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-guided%25203D%2520Human%2520Motion%2520Generation%2520with%2520Keyframe-based%2520Parallel%2520Skip%250A%2520%2520Transformer%26entry.906535625%3DZichen%2520Geng%2520and%2520Caren%2520Han%2520and%2520Zeeshan%2520Hayder%2520and%2520Jian%2520Liu%2520and%2520Mubarak%2520Shah%2520and%2520Ajmal%2520Mian%26entry.1292438233%3D%2520%2520Text-driven%2520human%2520motion%2520generation%2520is%2520an%2520emerging%2520task%2520in%2520animation%2520and%250Ahumanoid%2520robot%2520design.%2520Existing%2520algorithms%2520directly%2520generate%2520the%2520full%2520sequence%250Awhich%2520is%2520computationally%2520expensive%2520and%2520prone%2520to%2520errors%2520as%2520it%2520does%2520not%2520pay%250Aspecial%2520attention%2520to%2520key%2520poses%252C%2520a%2520process%2520that%2520has%2520been%2520the%2520cornerstone%2520of%250Aanimation%2520for%2520decades.%2520We%2520propose%2520KeyMotion%252C%2520that%2520generates%2520plausible%2520human%250Amotion%2520sequences%2520corresponding%2520to%2520input%2520text%2520by%2520first%2520generating%2520keyframes%250Afollowed%2520by%2520in-filling.%2520We%2520use%2520a%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520with%250AKullback-Leibler%2520regularization%2520to%2520project%2520the%2520keyframes%2520into%2520a%2520latent%2520space%2520to%250Areduce%2520dimensionality%2520and%2520further%2520accelerate%2520the%2520subsequent%2520diffusion%2520process.%250AFor%2520the%2520reverse%2520diffusion%252C%2520we%2520propose%2520a%2520novel%2520Parallel%2520Skip%2520Transformer%2520that%250Aperforms%2520cross-modal%2520attention%2520between%2520the%2520keyframe%2520latents%2520and%2520text%2520condition.%250ATo%2520complete%2520the%2520motion%2520sequence%252C%2520we%2520propose%2520a%2520text-guided%2520Transformer%2520designed%250Ato%2520perform%2520motion-in-filling%252C%2520ensuring%2520the%2520preservation%2520of%2520both%2520fidelity%2520and%250Aadherence%2520to%2520the%2520physical%2520constraints%2520of%2520human%2520motion.%2520Experiments%2520show%2520that%250Aour%2520method%2520achieves%2520state-of-theart%2520results%2520on%2520the%2520HumanML3D%2520dataset%250Aoutperforming%2520others%2520on%2520all%2520R-precision%2520metrics%2520and%2520MultiModal%2520Distance.%250AKeyMotion%2520also%2520achieves%2520competitive%2520performance%2520on%2520the%2520KIT%2520dataset%252C%2520achieving%250Athe%2520best%2520results%2520on%2520Top3%2520R-precision%252C%2520FID%252C%2520and%2520Diversity%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-guided%203D%20Human%20Motion%20Generation%20with%20Keyframe-based%20Parallel%20Skip%0A%20%20Transformer&entry.906535625=Zichen%20Geng%20and%20Caren%20Han%20and%20Zeeshan%20Hayder%20and%20Jian%20Liu%20and%20Mubarak%20Shah%20and%20Ajmal%20Mian&entry.1292438233=%20%20Text-driven%20human%20motion%20generation%20is%20an%20emerging%20task%20in%20animation%20and%0Ahumanoid%20robot%20design.%20Existing%20algorithms%20directly%20generate%20the%20full%20sequence%0Awhich%20is%20computationally%20expensive%20and%20prone%20to%20errors%20as%20it%20does%20not%20pay%0Aspecial%20attention%20to%20key%20poses%2C%20a%20process%20that%20has%20been%20the%20cornerstone%20of%0Aanimation%20for%20decades.%20We%20propose%20KeyMotion%2C%20that%20generates%20plausible%20human%0Amotion%20sequences%20corresponding%20to%20input%20text%20by%20first%20generating%20keyframes%0Afollowed%20by%20in-filling.%20We%20use%20a%20Variational%20Autoencoder%20%28VAE%29%20with%0AKullback-Leibler%20regularization%20to%20project%20the%20keyframes%20into%20a%20latent%20space%20to%0Areduce%20dimensionality%20and%20further%20accelerate%20the%20subsequent%20diffusion%20process.%0AFor%20the%20reverse%20diffusion%2C%20we%20propose%20a%20novel%20Parallel%20Skip%20Transformer%20that%0Aperforms%20cross-modal%20attention%20between%20the%20keyframe%20latents%20and%20text%20condition.%0ATo%20complete%20the%20motion%20sequence%2C%20we%20propose%20a%20text-guided%20Transformer%20designed%0Ato%20perform%20motion-in-filling%2C%20ensuring%20the%20preservation%20of%20both%20fidelity%20and%0Aadherence%20to%20the%20physical%20constraints%20of%20human%20motion.%20Experiments%20show%20that%0Aour%20method%20achieves%20state-of-theart%20results%20on%20the%20HumanML3D%20dataset%0Aoutperforming%20others%20on%20all%20R-precision%20metrics%20and%20MultiModal%20Distance.%0AKeyMotion%20also%20achieves%20competitive%20performance%20on%20the%20KIT%20dataset%2C%20achieving%0Athe%20best%20results%20on%20Top3%20R-precision%2C%20FID%2C%20and%20Diversity%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15439v1&entry.124074799=Read"},
{"title": "Learning Generalizable Human Motion Generator with Reinforcement\n  Learning", "author": "Yunyao Mao and Xiaoyang Liu and Wengang Zhou and Zhenbo Lu and Houqiang Li", "abstract": "  Text-driven human motion generation, as one of the vital tasks in\ncomputer-aided content creation, has recently attracted increasing attention.\nWhile pioneering research has largely focused on improving numerical\nperformance metrics on given datasets, practical applications reveal a common\nchallenge: existing methods often overfit specific motion expressions in the\ntraining data, hindering their ability to generalize to novel descriptions like\nunseen combinations of motions. This limitation restricts their broader\napplicability. We argue that the aforementioned problem primarily arises from\nthe scarcity of available motion-text pairs, given the many-to-many nature of\ntext-driven motion generation. To tackle this problem, we formulate\ntext-to-motion generation as a Markov decision process and present\n\\textbf{InstructMotion}, which incorporate the trail and error paradigm in\nreinforcement learning for generalizable human motion generation. Leveraging\ncontrastive pre-trained text and motion encoders, we delve into optimizing\nreward design to enable InstructMotion to operate effectively on both paired\ndata, enhancing global semantic level text-motion alignment, and synthetic\ntext-only data, facilitating better generalization to novel prompts without the\nneed for ground-truth motion supervision. Extensive experiments on prevalent\nbenchmarks and also our synthesized unpaired dataset demonstrate that the\nproposed InstructMotion achieves outstanding performance both quantitatively\nand qualitatively.\n", "link": "http://arxiv.org/abs/2405.15541v1", "date": "2024-05-24", "relevancy": 2.3082, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.608}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Generalizable%20Human%20Motion%20Generator%20with%20Reinforcement%0A%20%20Learning&body=Title%3A%20Learning%20Generalizable%20Human%20Motion%20Generator%20with%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Yunyao%20Mao%20and%20Xiaoyang%20Liu%20and%20Wengang%20Zhou%20and%20Zhenbo%20Lu%20and%20Houqiang%20Li%0AAbstract%3A%20%20%20Text-driven%20human%20motion%20generation%2C%20as%20one%20of%20the%20vital%20tasks%20in%0Acomputer-aided%20content%20creation%2C%20has%20recently%20attracted%20increasing%20attention.%0AWhile%20pioneering%20research%20has%20largely%20focused%20on%20improving%20numerical%0Aperformance%20metrics%20on%20given%20datasets%2C%20practical%20applications%20reveal%20a%20common%0Achallenge%3A%20existing%20methods%20often%20overfit%20specific%20motion%20expressions%20in%20the%0Atraining%20data%2C%20hindering%20their%20ability%20to%20generalize%20to%20novel%20descriptions%20like%0Aunseen%20combinations%20of%20motions.%20This%20limitation%20restricts%20their%20broader%0Aapplicability.%20We%20argue%20that%20the%20aforementioned%20problem%20primarily%20arises%20from%0Athe%20scarcity%20of%20available%20motion-text%20pairs%2C%20given%20the%20many-to-many%20nature%20of%0Atext-driven%20motion%20generation.%20To%20tackle%20this%20problem%2C%20we%20formulate%0Atext-to-motion%20generation%20as%20a%20Markov%20decision%20process%20and%20present%0A%5Ctextbf%7BInstructMotion%7D%2C%20which%20incorporate%20the%20trail%20and%20error%20paradigm%20in%0Areinforcement%20learning%20for%20generalizable%20human%20motion%20generation.%20Leveraging%0Acontrastive%20pre-trained%20text%20and%20motion%20encoders%2C%20we%20delve%20into%20optimizing%0Areward%20design%20to%20enable%20InstructMotion%20to%20operate%20effectively%20on%20both%20paired%0Adata%2C%20enhancing%20global%20semantic%20level%20text-motion%20alignment%2C%20and%20synthetic%0Atext-only%20data%2C%20facilitating%20better%20generalization%20to%20novel%20prompts%20without%20the%0Aneed%20for%20ground-truth%20motion%20supervision.%20Extensive%20experiments%20on%20prevalent%0Abenchmarks%20and%20also%20our%20synthesized%20unpaired%20dataset%20demonstrate%20that%20the%0Aproposed%20InstructMotion%20achieves%20outstanding%20performance%20both%20quantitatively%0Aand%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Generalizable%2520Human%2520Motion%2520Generator%2520with%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DYunyao%2520Mao%2520and%2520Xiaoyang%2520Liu%2520and%2520Wengang%2520Zhou%2520and%2520Zhenbo%2520Lu%2520and%2520Houqiang%2520Li%26entry.1292438233%3D%2520%2520Text-driven%2520human%2520motion%2520generation%252C%2520as%2520one%2520of%2520the%2520vital%2520tasks%2520in%250Acomputer-aided%2520content%2520creation%252C%2520has%2520recently%2520attracted%2520increasing%2520attention.%250AWhile%2520pioneering%2520research%2520has%2520largely%2520focused%2520on%2520improving%2520numerical%250Aperformance%2520metrics%2520on%2520given%2520datasets%252C%2520practical%2520applications%2520reveal%2520a%2520common%250Achallenge%253A%2520existing%2520methods%2520often%2520overfit%2520specific%2520motion%2520expressions%2520in%2520the%250Atraining%2520data%252C%2520hindering%2520their%2520ability%2520to%2520generalize%2520to%2520novel%2520descriptions%2520like%250Aunseen%2520combinations%2520of%2520motions.%2520This%2520limitation%2520restricts%2520their%2520broader%250Aapplicability.%2520We%2520argue%2520that%2520the%2520aforementioned%2520problem%2520primarily%2520arises%2520from%250Athe%2520scarcity%2520of%2520available%2520motion-text%2520pairs%252C%2520given%2520the%2520many-to-many%2520nature%2520of%250Atext-driven%2520motion%2520generation.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520formulate%250Atext-to-motion%2520generation%2520as%2520a%2520Markov%2520decision%2520process%2520and%2520present%250A%255Ctextbf%257BInstructMotion%257D%252C%2520which%2520incorporate%2520the%2520trail%2520and%2520error%2520paradigm%2520in%250Areinforcement%2520learning%2520for%2520generalizable%2520human%2520motion%2520generation.%2520Leveraging%250Acontrastive%2520pre-trained%2520text%2520and%2520motion%2520encoders%252C%2520we%2520delve%2520into%2520optimizing%250Areward%2520design%2520to%2520enable%2520InstructMotion%2520to%2520operate%2520effectively%2520on%2520both%2520paired%250Adata%252C%2520enhancing%2520global%2520semantic%2520level%2520text-motion%2520alignment%252C%2520and%2520synthetic%250Atext-only%2520data%252C%2520facilitating%2520better%2520generalization%2520to%2520novel%2520prompts%2520without%2520the%250Aneed%2520for%2520ground-truth%2520motion%2520supervision.%2520Extensive%2520experiments%2520on%2520prevalent%250Abenchmarks%2520and%2520also%2520our%2520synthesized%2520unpaired%2520dataset%2520demonstrate%2520that%2520the%250Aproposed%2520InstructMotion%2520achieves%2520outstanding%2520performance%2520both%2520quantitatively%250Aand%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Generalizable%20Human%20Motion%20Generator%20with%20Reinforcement%0A%20%20Learning&entry.906535625=Yunyao%20Mao%20and%20Xiaoyang%20Liu%20and%20Wengang%20Zhou%20and%20Zhenbo%20Lu%20and%20Houqiang%20Li&entry.1292438233=%20%20Text-driven%20human%20motion%20generation%2C%20as%20one%20of%20the%20vital%20tasks%20in%0Acomputer-aided%20content%20creation%2C%20has%20recently%20attracted%20increasing%20attention.%0AWhile%20pioneering%20research%20has%20largely%20focused%20on%20improving%20numerical%0Aperformance%20metrics%20on%20given%20datasets%2C%20practical%20applications%20reveal%20a%20common%0Achallenge%3A%20existing%20methods%20often%20overfit%20specific%20motion%20expressions%20in%20the%0Atraining%20data%2C%20hindering%20their%20ability%20to%20generalize%20to%20novel%20descriptions%20like%0Aunseen%20combinations%20of%20motions.%20This%20limitation%20restricts%20their%20broader%0Aapplicability.%20We%20argue%20that%20the%20aforementioned%20problem%20primarily%20arises%20from%0Athe%20scarcity%20of%20available%20motion-text%20pairs%2C%20given%20the%20many-to-many%20nature%20of%0Atext-driven%20motion%20generation.%20To%20tackle%20this%20problem%2C%20we%20formulate%0Atext-to-motion%20generation%20as%20a%20Markov%20decision%20process%20and%20present%0A%5Ctextbf%7BInstructMotion%7D%2C%20which%20incorporate%20the%20trail%20and%20error%20paradigm%20in%0Areinforcement%20learning%20for%20generalizable%20human%20motion%20generation.%20Leveraging%0Acontrastive%20pre-trained%20text%20and%20motion%20encoders%2C%20we%20delve%20into%20optimizing%0Areward%20design%20to%20enable%20InstructMotion%20to%20operate%20effectively%20on%20both%20paired%0Adata%2C%20enhancing%20global%20semantic%20level%20text-motion%20alignment%2C%20and%20synthetic%0Atext-only%20data%2C%20facilitating%20better%20generalization%20to%20novel%20prompts%20without%20the%0Aneed%20for%20ground-truth%20motion%20supervision.%20Extensive%20experiments%20on%20prevalent%0Abenchmarks%20and%20also%20our%20synthesized%20unpaired%20dataset%20demonstrate%20that%20the%0Aproposed%20InstructMotion%20achieves%20outstanding%20performance%20both%20quantitatively%0Aand%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15541v1&entry.124074799=Read"},
{"title": "MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object\n  Diffusion", "author": "Sen Li and Ruochen Wang and Cho-Jui Hsieh and Minhao Cheng and Tianyi Zhou", "abstract": "  Existing text-to-image models still struggle to generate images of multiple\nobjects, especially in handling their spatial positions, relative sizes,\noverlapping, and attribute bindings. To efficiently address these challenges,\nwe develop a training-free Multimodal-LLM agent (MuLan), as a human painter,\nthat can progressively generate multi-object with intricate planning and\nfeedback control. MuLan harnesses a large language model (LLM) to decompose a\nprompt to a sequence of sub-tasks, each generating only one object by stable\ndiffusion, conditioned on previously generated objects. Unlike existing\nLLM-grounded methods, MuLan only produces a high-level plan at the beginning\nwhile the exact size and location of each object are determined upon each\nsub-task by an LLM and attention guidance. Moreover, MuLan adopts a\nvision-language model (VLM) to provide feedback to the image generated in each\nsub-task and control the diffusion model to re-generate the image if it\nviolates the original prompt. Hence, each model in every step of MuLan only\nneeds to address an easy sub-task it is specialized for. The multi-step process\nalso allows human users to monitor the generation process and make preferred\nchanges at any intermediate step via text prompts, thereby improving the\nhuman-AI collaboration experience. We collect 200 prompts containing\nmulti-objects with spatial relationships and attribute bindings from different\nbenchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan\nin generating multiple objects over baselines and its creativity when\ncollaborating with human users. The code is available at\nhttps://github.com/measure-infinity/mulan-code.\n", "link": "http://arxiv.org/abs/2402.12741v2", "date": "2024-05-24", "relevancy": 2.3067, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5858}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5792}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MuLan%3A%20Multimodal-LLM%20Agent%20for%20Progressive%20and%20Interactive%20Multi-Object%0A%20%20Diffusion&body=Title%3A%20MuLan%3A%20Multimodal-LLM%20Agent%20for%20Progressive%20and%20Interactive%20Multi-Object%0A%20%20Diffusion%0AAuthor%3A%20Sen%20Li%20and%20Ruochen%20Wang%20and%20Cho-Jui%20Hsieh%20and%20Minhao%20Cheng%20and%20Tianyi%20Zhou%0AAbstract%3A%20%20%20Existing%20text-to-image%20models%20still%20struggle%20to%20generate%20images%20of%20multiple%0Aobjects%2C%20especially%20in%20handling%20their%20spatial%20positions%2C%20relative%20sizes%2C%0Aoverlapping%2C%20and%20attribute%20bindings.%20To%20efficiently%20address%20these%20challenges%2C%0Awe%20develop%20a%20training-free%20Multimodal-LLM%20agent%20%28MuLan%29%2C%20as%20a%20human%20painter%2C%0Athat%20can%20progressively%20generate%20multi-object%20with%20intricate%20planning%20and%0Afeedback%20control.%20MuLan%20harnesses%20a%20large%20language%20model%20%28LLM%29%20to%20decompose%20a%0Aprompt%20to%20a%20sequence%20of%20sub-tasks%2C%20each%20generating%20only%20one%20object%20by%20stable%0Adiffusion%2C%20conditioned%20on%20previously%20generated%20objects.%20Unlike%20existing%0ALLM-grounded%20methods%2C%20MuLan%20only%20produces%20a%20high-level%20plan%20at%20the%20beginning%0Awhile%20the%20exact%20size%20and%20location%20of%20each%20object%20are%20determined%20upon%20each%0Asub-task%20by%20an%20LLM%20and%20attention%20guidance.%20Moreover%2C%20MuLan%20adopts%20a%0Avision-language%20model%20%28VLM%29%20to%20provide%20feedback%20to%20the%20image%20generated%20in%20each%0Asub-task%20and%20control%20the%20diffusion%20model%20to%20re-generate%20the%20image%20if%20it%0Aviolates%20the%20original%20prompt.%20Hence%2C%20each%20model%20in%20every%20step%20of%20MuLan%20only%0Aneeds%20to%20address%20an%20easy%20sub-task%20it%20is%20specialized%20for.%20The%20multi-step%20process%0Aalso%20allows%20human%20users%20to%20monitor%20the%20generation%20process%20and%20make%20preferred%0Achanges%20at%20any%20intermediate%20step%20via%20text%20prompts%2C%20thereby%20improving%20the%0Ahuman-AI%20collaboration%20experience.%20We%20collect%20200%20prompts%20containing%0Amulti-objects%20with%20spatial%20relationships%20and%20attribute%20bindings%20from%20different%0Abenchmarks%20to%20evaluate%20MuLan.%20The%20results%20demonstrate%20the%20superiority%20of%20MuLan%0Ain%20generating%20multiple%20objects%20over%20baselines%20and%20its%20creativity%20when%0Acollaborating%20with%20human%20users.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/measure-infinity/mulan-code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuLan%253A%2520Multimodal-LLM%2520Agent%2520for%2520Progressive%2520and%2520Interactive%2520Multi-Object%250A%2520%2520Diffusion%26entry.906535625%3DSen%2520Li%2520and%2520Ruochen%2520Wang%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Minhao%2520Cheng%2520and%2520Tianyi%2520Zhou%26entry.1292438233%3D%2520%2520Existing%2520text-to-image%2520models%2520still%2520struggle%2520to%2520generate%2520images%2520of%2520multiple%250Aobjects%252C%2520especially%2520in%2520handling%2520their%2520spatial%2520positions%252C%2520relative%2520sizes%252C%250Aoverlapping%252C%2520and%2520attribute%2520bindings.%2520To%2520efficiently%2520address%2520these%2520challenges%252C%250Awe%2520develop%2520a%2520training-free%2520Multimodal-LLM%2520agent%2520%2528MuLan%2529%252C%2520as%2520a%2520human%2520painter%252C%250Athat%2520can%2520progressively%2520generate%2520multi-object%2520with%2520intricate%2520planning%2520and%250Afeedback%2520control.%2520MuLan%2520harnesses%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520decompose%2520a%250Aprompt%2520to%2520a%2520sequence%2520of%2520sub-tasks%252C%2520each%2520generating%2520only%2520one%2520object%2520by%2520stable%250Adiffusion%252C%2520conditioned%2520on%2520previously%2520generated%2520objects.%2520Unlike%2520existing%250ALLM-grounded%2520methods%252C%2520MuLan%2520only%2520produces%2520a%2520high-level%2520plan%2520at%2520the%2520beginning%250Awhile%2520the%2520exact%2520size%2520and%2520location%2520of%2520each%2520object%2520are%2520determined%2520upon%2520each%250Asub-task%2520by%2520an%2520LLM%2520and%2520attention%2520guidance.%2520Moreover%252C%2520MuLan%2520adopts%2520a%250Avision-language%2520model%2520%2528VLM%2529%2520to%2520provide%2520feedback%2520to%2520the%2520image%2520generated%2520in%2520each%250Asub-task%2520and%2520control%2520the%2520diffusion%2520model%2520to%2520re-generate%2520the%2520image%2520if%2520it%250Aviolates%2520the%2520original%2520prompt.%2520Hence%252C%2520each%2520model%2520in%2520every%2520step%2520of%2520MuLan%2520only%250Aneeds%2520to%2520address%2520an%2520easy%2520sub-task%2520it%2520is%2520specialized%2520for.%2520The%2520multi-step%2520process%250Aalso%2520allows%2520human%2520users%2520to%2520monitor%2520the%2520generation%2520process%2520and%2520make%2520preferred%250Achanges%2520at%2520any%2520intermediate%2520step%2520via%2520text%2520prompts%252C%2520thereby%2520improving%2520the%250Ahuman-AI%2520collaboration%2520experience.%2520We%2520collect%2520200%2520prompts%2520containing%250Amulti-objects%2520with%2520spatial%2520relationships%2520and%2520attribute%2520bindings%2520from%2520different%250Abenchmarks%2520to%2520evaluate%2520MuLan.%2520The%2520results%2520demonstrate%2520the%2520superiority%2520of%2520MuLan%250Ain%2520generating%2520multiple%2520objects%2520over%2520baselines%2520and%2520its%2520creativity%2520when%250Acollaborating%2520with%2520human%2520users.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/measure-infinity/mulan-code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuLan%3A%20Multimodal-LLM%20Agent%20for%20Progressive%20and%20Interactive%20Multi-Object%0A%20%20Diffusion&entry.906535625=Sen%20Li%20and%20Ruochen%20Wang%20and%20Cho-Jui%20Hsieh%20and%20Minhao%20Cheng%20and%20Tianyi%20Zhou&entry.1292438233=%20%20Existing%20text-to-image%20models%20still%20struggle%20to%20generate%20images%20of%20multiple%0Aobjects%2C%20especially%20in%20handling%20their%20spatial%20positions%2C%20relative%20sizes%2C%0Aoverlapping%2C%20and%20attribute%20bindings.%20To%20efficiently%20address%20these%20challenges%2C%0Awe%20develop%20a%20training-free%20Multimodal-LLM%20agent%20%28MuLan%29%2C%20as%20a%20human%20painter%2C%0Athat%20can%20progressively%20generate%20multi-object%20with%20intricate%20planning%20and%0Afeedback%20control.%20MuLan%20harnesses%20a%20large%20language%20model%20%28LLM%29%20to%20decompose%20a%0Aprompt%20to%20a%20sequence%20of%20sub-tasks%2C%20each%20generating%20only%20one%20object%20by%20stable%0Adiffusion%2C%20conditioned%20on%20previously%20generated%20objects.%20Unlike%20existing%0ALLM-grounded%20methods%2C%20MuLan%20only%20produces%20a%20high-level%20plan%20at%20the%20beginning%0Awhile%20the%20exact%20size%20and%20location%20of%20each%20object%20are%20determined%20upon%20each%0Asub-task%20by%20an%20LLM%20and%20attention%20guidance.%20Moreover%2C%20MuLan%20adopts%20a%0Avision-language%20model%20%28VLM%29%20to%20provide%20feedback%20to%20the%20image%20generated%20in%20each%0Asub-task%20and%20control%20the%20diffusion%20model%20to%20re-generate%20the%20image%20if%20it%0Aviolates%20the%20original%20prompt.%20Hence%2C%20each%20model%20in%20every%20step%20of%20MuLan%20only%0Aneeds%20to%20address%20an%20easy%20sub-task%20it%20is%20specialized%20for.%20The%20multi-step%20process%0Aalso%20allows%20human%20users%20to%20monitor%20the%20generation%20process%20and%20make%20preferred%0Achanges%20at%20any%20intermediate%20step%20via%20text%20prompts%2C%20thereby%20improving%20the%0Ahuman-AI%20collaboration%20experience.%20We%20collect%20200%20prompts%20containing%0Amulti-objects%20with%20spatial%20relationships%20and%20attribute%20bindings%20from%20different%0Abenchmarks%20to%20evaluate%20MuLan.%20The%20results%20demonstrate%20the%20superiority%20of%20MuLan%0Ain%20generating%20multiple%20objects%20over%20baselines%20and%20its%20creativity%20when%0Acollaborating%20with%20human%20users.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/measure-infinity/mulan-code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12741v2&entry.124074799=Read"},
{"title": "Enhancing Pollinator Conservation towards Agriculture 4.0: Monitoring of\n  Bees through Object Recognition", "author": "Ajay John Alex and Chloe M. Barnes and Pedro Machado and Isibor Ihianle and G\u00e1bor Mark\u00f3 and Martin Bencsik and Jordan J. Bird", "abstract": "  In an era of rapid climate change and its adverse effects on food production,\ntechnological intervention to monitor pollinator conservation is of paramount\nimportance for environmental monitoring and conservation for global food\nsecurity. The survival of the human species depends on the conservation of\npollinators. This article explores the use of Computer Vision and Object\nRecognition to autonomously track and report bee behaviour from images. A novel\ndataset of 9664 images containing bees is extracted from video streams and\nannotated with bounding boxes. With training, validation and testing sets\n(6722, 1915, and 997 images, respectively), the results of the COCO-based YOLO\nmodel fine-tuning approaches show that YOLOv5m is the most effective approach\nin terms of recognition accuracy. However, YOLOv5s was shown to be the most\noptimal for real-time bee detection with an average processing and inference\ntime of 5.1ms per video frame at the cost of slightly lower ability. The\ntrained model is then packaged within an explainable AI interface, which\nconverts detection events into timestamped reports and charts, with the aim of\nfacilitating use by non-technical users such as expert stakeholders from the\napiculture industry towards informing responsible consumption and production.\n", "link": "http://arxiv.org/abs/2405.15428v1", "date": "2024-05-24", "relevancy": 2.2998, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4652}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4592}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Pollinator%20Conservation%20towards%20Agriculture%204.0%3A%20Monitoring%20of%0A%20%20Bees%20through%20Object%20Recognition&body=Title%3A%20Enhancing%20Pollinator%20Conservation%20towards%20Agriculture%204.0%3A%20Monitoring%20of%0A%20%20Bees%20through%20Object%20Recognition%0AAuthor%3A%20Ajay%20John%20Alex%20and%20Chloe%20M.%20Barnes%20and%20Pedro%20Machado%20and%20Isibor%20Ihianle%20and%20G%C3%A1bor%20Mark%C3%B3%20and%20Martin%20Bencsik%20and%20Jordan%20J.%20Bird%0AAbstract%3A%20%20%20In%20an%20era%20of%20rapid%20climate%20change%20and%20its%20adverse%20effects%20on%20food%20production%2C%0Atechnological%20intervention%20to%20monitor%20pollinator%20conservation%20is%20of%20paramount%0Aimportance%20for%20environmental%20monitoring%20and%20conservation%20for%20global%20food%0Asecurity.%20The%20survival%20of%20the%20human%20species%20depends%20on%20the%20conservation%20of%0Apollinators.%20This%20article%20explores%20the%20use%20of%20Computer%20Vision%20and%20Object%0ARecognition%20to%20autonomously%20track%20and%20report%20bee%20behaviour%20from%20images.%20A%20novel%0Adataset%20of%209664%20images%20containing%20bees%20is%20extracted%20from%20video%20streams%20and%0Aannotated%20with%20bounding%20boxes.%20With%20training%2C%20validation%20and%20testing%20sets%0A%286722%2C%201915%2C%20and%20997%20images%2C%20respectively%29%2C%20the%20results%20of%20the%20COCO-based%20YOLO%0Amodel%20fine-tuning%20approaches%20show%20that%20YOLOv5m%20is%20the%20most%20effective%20approach%0Ain%20terms%20of%20recognition%20accuracy.%20However%2C%20YOLOv5s%20was%20shown%20to%20be%20the%20most%0Aoptimal%20for%20real-time%20bee%20detection%20with%20an%20average%20processing%20and%20inference%0Atime%20of%205.1ms%20per%20video%20frame%20at%20the%20cost%20of%20slightly%20lower%20ability.%20The%0Atrained%20model%20is%20then%20packaged%20within%20an%20explainable%20AI%20interface%2C%20which%0Aconverts%20detection%20events%20into%20timestamped%20reports%20and%20charts%2C%20with%20the%20aim%20of%0Afacilitating%20use%20by%20non-technical%20users%20such%20as%20expert%20stakeholders%20from%20the%0Aapiculture%20industry%20towards%20informing%20responsible%20consumption%20and%20production.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Pollinator%2520Conservation%2520towards%2520Agriculture%25204.0%253A%2520Monitoring%2520of%250A%2520%2520Bees%2520through%2520Object%2520Recognition%26entry.906535625%3DAjay%2520John%2520Alex%2520and%2520Chloe%2520M.%2520Barnes%2520and%2520Pedro%2520Machado%2520and%2520Isibor%2520Ihianle%2520and%2520G%25C3%25A1bor%2520Mark%25C3%25B3%2520and%2520Martin%2520Bencsik%2520and%2520Jordan%2520J.%2520Bird%26entry.1292438233%3D%2520%2520In%2520an%2520era%2520of%2520rapid%2520climate%2520change%2520and%2520its%2520adverse%2520effects%2520on%2520food%2520production%252C%250Atechnological%2520intervention%2520to%2520monitor%2520pollinator%2520conservation%2520is%2520of%2520paramount%250Aimportance%2520for%2520environmental%2520monitoring%2520and%2520conservation%2520for%2520global%2520food%250Asecurity.%2520The%2520survival%2520of%2520the%2520human%2520species%2520depends%2520on%2520the%2520conservation%2520of%250Apollinators.%2520This%2520article%2520explores%2520the%2520use%2520of%2520Computer%2520Vision%2520and%2520Object%250ARecognition%2520to%2520autonomously%2520track%2520and%2520report%2520bee%2520behaviour%2520from%2520images.%2520A%2520novel%250Adataset%2520of%25209664%2520images%2520containing%2520bees%2520is%2520extracted%2520from%2520video%2520streams%2520and%250Aannotated%2520with%2520bounding%2520boxes.%2520With%2520training%252C%2520validation%2520and%2520testing%2520sets%250A%25286722%252C%25201915%252C%2520and%2520997%2520images%252C%2520respectively%2529%252C%2520the%2520results%2520of%2520the%2520COCO-based%2520YOLO%250Amodel%2520fine-tuning%2520approaches%2520show%2520that%2520YOLOv5m%2520is%2520the%2520most%2520effective%2520approach%250Ain%2520terms%2520of%2520recognition%2520accuracy.%2520However%252C%2520YOLOv5s%2520was%2520shown%2520to%2520be%2520the%2520most%250Aoptimal%2520for%2520real-time%2520bee%2520detection%2520with%2520an%2520average%2520processing%2520and%2520inference%250Atime%2520of%25205.1ms%2520per%2520video%2520frame%2520at%2520the%2520cost%2520of%2520slightly%2520lower%2520ability.%2520The%250Atrained%2520model%2520is%2520then%2520packaged%2520within%2520an%2520explainable%2520AI%2520interface%252C%2520which%250Aconverts%2520detection%2520events%2520into%2520timestamped%2520reports%2520and%2520charts%252C%2520with%2520the%2520aim%2520of%250Afacilitating%2520use%2520by%2520non-technical%2520users%2520such%2520as%2520expert%2520stakeholders%2520from%2520the%250Aapiculture%2520industry%2520towards%2520informing%2520responsible%2520consumption%2520and%2520production.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Pollinator%20Conservation%20towards%20Agriculture%204.0%3A%20Monitoring%20of%0A%20%20Bees%20through%20Object%20Recognition&entry.906535625=Ajay%20John%20Alex%20and%20Chloe%20M.%20Barnes%20and%20Pedro%20Machado%20and%20Isibor%20Ihianle%20and%20G%C3%A1bor%20Mark%C3%B3%20and%20Martin%20Bencsik%20and%20Jordan%20J.%20Bird&entry.1292438233=%20%20In%20an%20era%20of%20rapid%20climate%20change%20and%20its%20adverse%20effects%20on%20food%20production%2C%0Atechnological%20intervention%20to%20monitor%20pollinator%20conservation%20is%20of%20paramount%0Aimportance%20for%20environmental%20monitoring%20and%20conservation%20for%20global%20food%0Asecurity.%20The%20survival%20of%20the%20human%20species%20depends%20on%20the%20conservation%20of%0Apollinators.%20This%20article%20explores%20the%20use%20of%20Computer%20Vision%20and%20Object%0ARecognition%20to%20autonomously%20track%20and%20report%20bee%20behaviour%20from%20images.%20A%20novel%0Adataset%20of%209664%20images%20containing%20bees%20is%20extracted%20from%20video%20streams%20and%0Aannotated%20with%20bounding%20boxes.%20With%20training%2C%20validation%20and%20testing%20sets%0A%286722%2C%201915%2C%20and%20997%20images%2C%20respectively%29%2C%20the%20results%20of%20the%20COCO-based%20YOLO%0Amodel%20fine-tuning%20approaches%20show%20that%20YOLOv5m%20is%20the%20most%20effective%20approach%0Ain%20terms%20of%20recognition%20accuracy.%20However%2C%20YOLOv5s%20was%20shown%20to%20be%20the%20most%0Aoptimal%20for%20real-time%20bee%20detection%20with%20an%20average%20processing%20and%20inference%0Atime%20of%205.1ms%20per%20video%20frame%20at%20the%20cost%20of%20slightly%20lower%20ability.%20The%0Atrained%20model%20is%20then%20packaged%20within%20an%20explainable%20AI%20interface%2C%20which%0Aconverts%20detection%20events%20into%20timestamped%20reports%20and%20charts%2C%20with%20the%20aim%20of%0Afacilitating%20use%20by%20non-technical%20users%20such%20as%20expert%20stakeholders%20from%20the%0Aapiculture%20industry%20towards%20informing%20responsible%20consumption%20and%20production.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15428v1&entry.124074799=Read"},
{"title": "Hierarchical Uncertainty Exploration via Feedforward Posterior Trees", "author": "Elias Nehme and Rotem Mulayoff and Tomer Michaeli", "abstract": "  When solving ill-posed inverse problems, one often desires to explore the\nspace of potential solutions rather than be presented with a single plausible\nreconstruction. Valuable insights into these feasible solutions and their\nassociated probabilities are embedded in the posterior distribution. However,\nwhen confronted with data of high dimensionality (such as images), visualizing\nthis distribution becomes a formidable challenge, necessitating the application\nof effective summarization techniques before user examination. In this work, we\nintroduce a new approach for visualizing posteriors across multiple levels of\ngranularity using tree-valued predictions. Our method predicts a tree-valued\nhierarchical summarization of the posterior distribution for any input\nmeasurement, in a single forward pass of a neural network. We showcase the\nefficacy of our approach across diverse datasets and image restoration\nchallenges, highlighting its prowess in uncertainty quantification and\nvisualization. Our findings reveal that our method performs comparably to a\nbaseline that hierarchically clusters samples from a diffusion-based posterior\nsampler, yet achieves this with orders of magnitude greater speed.\n", "link": "http://arxiv.org/abs/2405.15719v1", "date": "2024-05-24", "relevancy": 2.2814, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5859}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Uncertainty%20Exploration%20via%20Feedforward%20Posterior%20Trees&body=Title%3A%20Hierarchical%20Uncertainty%20Exploration%20via%20Feedforward%20Posterior%20Trees%0AAuthor%3A%20Elias%20Nehme%20and%20Rotem%20Mulayoff%20and%20Tomer%20Michaeli%0AAbstract%3A%20%20%20When%20solving%20ill-posed%20inverse%20problems%2C%20one%20often%20desires%20to%20explore%20the%0Aspace%20of%20potential%20solutions%20rather%20than%20be%20presented%20with%20a%20single%20plausible%0Areconstruction.%20Valuable%20insights%20into%20these%20feasible%20solutions%20and%20their%0Aassociated%20probabilities%20are%20embedded%20in%20the%20posterior%20distribution.%20However%2C%0Awhen%20confronted%20with%20data%20of%20high%20dimensionality%20%28such%20as%20images%29%2C%20visualizing%0Athis%20distribution%20becomes%20a%20formidable%20challenge%2C%20necessitating%20the%20application%0Aof%20effective%20summarization%20techniques%20before%20user%20examination.%20In%20this%20work%2C%20we%0Aintroduce%20a%20new%20approach%20for%20visualizing%20posteriors%20across%20multiple%20levels%20of%0Agranularity%20using%20tree-valued%20predictions.%20Our%20method%20predicts%20a%20tree-valued%0Ahierarchical%20summarization%20of%20the%20posterior%20distribution%20for%20any%20input%0Ameasurement%2C%20in%20a%20single%20forward%20pass%20of%20a%20neural%20network.%20We%20showcase%20the%0Aefficacy%20of%20our%20approach%20across%20diverse%20datasets%20and%20image%20restoration%0Achallenges%2C%20highlighting%20its%20prowess%20in%20uncertainty%20quantification%20and%0Avisualization.%20Our%20findings%20reveal%20that%20our%20method%20performs%20comparably%20to%20a%0Abaseline%20that%20hierarchically%20clusters%20samples%20from%20a%20diffusion-based%20posterior%0Asampler%2C%20yet%20achieves%20this%20with%20orders%20of%20magnitude%20greater%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Uncertainty%2520Exploration%2520via%2520Feedforward%2520Posterior%2520Trees%26entry.906535625%3DElias%2520Nehme%2520and%2520Rotem%2520Mulayoff%2520and%2520Tomer%2520Michaeli%26entry.1292438233%3D%2520%2520When%2520solving%2520ill-posed%2520inverse%2520problems%252C%2520one%2520often%2520desires%2520to%2520explore%2520the%250Aspace%2520of%2520potential%2520solutions%2520rather%2520than%2520be%2520presented%2520with%2520a%2520single%2520plausible%250Areconstruction.%2520Valuable%2520insights%2520into%2520these%2520feasible%2520solutions%2520and%2520their%250Aassociated%2520probabilities%2520are%2520embedded%2520in%2520the%2520posterior%2520distribution.%2520However%252C%250Awhen%2520confronted%2520with%2520data%2520of%2520high%2520dimensionality%2520%2528such%2520as%2520images%2529%252C%2520visualizing%250Athis%2520distribution%2520becomes%2520a%2520formidable%2520challenge%252C%2520necessitating%2520the%2520application%250Aof%2520effective%2520summarization%2520techniques%2520before%2520user%2520examination.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520new%2520approach%2520for%2520visualizing%2520posteriors%2520across%2520multiple%2520levels%2520of%250Agranularity%2520using%2520tree-valued%2520predictions.%2520Our%2520method%2520predicts%2520a%2520tree-valued%250Ahierarchical%2520summarization%2520of%2520the%2520posterior%2520distribution%2520for%2520any%2520input%250Ameasurement%252C%2520in%2520a%2520single%2520forward%2520pass%2520of%2520a%2520neural%2520network.%2520We%2520showcase%2520the%250Aefficacy%2520of%2520our%2520approach%2520across%2520diverse%2520datasets%2520and%2520image%2520restoration%250Achallenges%252C%2520highlighting%2520its%2520prowess%2520in%2520uncertainty%2520quantification%2520and%250Avisualization.%2520Our%2520findings%2520reveal%2520that%2520our%2520method%2520performs%2520comparably%2520to%2520a%250Abaseline%2520that%2520hierarchically%2520clusters%2520samples%2520from%2520a%2520diffusion-based%2520posterior%250Asampler%252C%2520yet%2520achieves%2520this%2520with%2520orders%2520of%2520magnitude%2520greater%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Uncertainty%20Exploration%20via%20Feedforward%20Posterior%20Trees&entry.906535625=Elias%20Nehme%20and%20Rotem%20Mulayoff%20and%20Tomer%20Michaeli&entry.1292438233=%20%20When%20solving%20ill-posed%20inverse%20problems%2C%20one%20often%20desires%20to%20explore%20the%0Aspace%20of%20potential%20solutions%20rather%20than%20be%20presented%20with%20a%20single%20plausible%0Areconstruction.%20Valuable%20insights%20into%20these%20feasible%20solutions%20and%20their%0Aassociated%20probabilities%20are%20embedded%20in%20the%20posterior%20distribution.%20However%2C%0Awhen%20confronted%20with%20data%20of%20high%20dimensionality%20%28such%20as%20images%29%2C%20visualizing%0Athis%20distribution%20becomes%20a%20formidable%20challenge%2C%20necessitating%20the%20application%0Aof%20effective%20summarization%20techniques%20before%20user%20examination.%20In%20this%20work%2C%20we%0Aintroduce%20a%20new%20approach%20for%20visualizing%20posteriors%20across%20multiple%20levels%20of%0Agranularity%20using%20tree-valued%20predictions.%20Our%20method%20predicts%20a%20tree-valued%0Ahierarchical%20summarization%20of%20the%20posterior%20distribution%20for%20any%20input%0Ameasurement%2C%20in%20a%20single%20forward%20pass%20of%20a%20neural%20network.%20We%20showcase%20the%0Aefficacy%20of%20our%20approach%20across%20diverse%20datasets%20and%20image%20restoration%0Achallenges%2C%20highlighting%20its%20prowess%20in%20uncertainty%20quantification%20and%0Avisualization.%20Our%20findings%20reveal%20that%20our%20method%20performs%20comparably%20to%20a%0Abaseline%20that%20hierarchically%20clusters%20samples%20from%20a%20diffusion-based%20posterior%0Asampler%2C%20yet%20achieves%20this%20with%20orders%20of%20magnitude%20greater%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15719v1&entry.124074799=Read"},
{"title": "Heart Murmur and Abnormal PCG Detection via Wavelet Scattering Transform\n  & a 1D-CNN", "author": "Ahmed Patwa and Muhammad Mahboob Ur Rahman and Tareq Y. Al-Naffouri", "abstract": "  Heart murmurs provide valuable information about mechanical activity of the\nheart, which aids in diagnosis of various heart valve diseases. This work does\nautomatic and accurate heart murmur detection from phonocardiogram (PCG)\nrecordings. Two public PCG datasets (CirCor Digiscope 2022 dataset and PCG 2016\ndataset) from Physionet online database are utilized to train and test three\ncustom neural networks (NN): a 1D convolutional neural network (CNN), a long\nshort-term memory (LSTM) recurrent neural network (RNN), and a convolutional\nRNN (C-RNN). We first do pre-processing which includes the following key steps:\ndenoising, segmentation, re-labeling of noise-only segments, data\nnormalization, and time-frequency analysis of the PCG segments using wavelet\nscattering transform. We then conduct four experiments, first three (E1-E3)\nusing PCG 2022 dataset, and fourth (E4) using PCG 2016 dataset. It turns out\nthat our custom 1D-CNN outperforms other two NNs (LSTM-RNN and C-RNN). Further,\nour 1D-CNN model outperforms the related work in terms of accuracy, weighted\naccuracy, F1-score and AUROC, for experiment E3 (that utilizes the cleaned and\nre-labeled PCG 2022 dataset). As for experiment E1 (that utilizes the original\nPCG 2022 dataset), our model performs quite close to the related work in terms\nof weighted accuracy and F1-score.\n", "link": "http://arxiv.org/abs/2303.11423v2", "date": "2024-05-24", "relevancy": 2.2778, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4647}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4519}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heart%20Murmur%20and%20Abnormal%20PCG%20Detection%20via%20Wavelet%20Scattering%20Transform%0A%20%20%26%20a%201D-CNN&body=Title%3A%20Heart%20Murmur%20and%20Abnormal%20PCG%20Detection%20via%20Wavelet%20Scattering%20Transform%0A%20%20%26%20a%201D-CNN%0AAuthor%3A%20Ahmed%20Patwa%20and%20Muhammad%20Mahboob%20Ur%20Rahman%20and%20Tareq%20Y.%20Al-Naffouri%0AAbstract%3A%20%20%20Heart%20murmurs%20provide%20valuable%20information%20about%20mechanical%20activity%20of%20the%0Aheart%2C%20which%20aids%20in%20diagnosis%20of%20various%20heart%20valve%20diseases.%20This%20work%20does%0Aautomatic%20and%20accurate%20heart%20murmur%20detection%20from%20phonocardiogram%20%28PCG%29%0Arecordings.%20Two%20public%20PCG%20datasets%20%28CirCor%20Digiscope%202022%20dataset%20and%20PCG%202016%0Adataset%29%20from%20Physionet%20online%20database%20are%20utilized%20to%20train%20and%20test%20three%0Acustom%20neural%20networks%20%28NN%29%3A%20a%201D%20convolutional%20neural%20network%20%28CNN%29%2C%20a%20long%0Ashort-term%20memory%20%28LSTM%29%20recurrent%20neural%20network%20%28RNN%29%2C%20and%20a%20convolutional%0ARNN%20%28C-RNN%29.%20We%20first%20do%20pre-processing%20which%20includes%20the%20following%20key%20steps%3A%0Adenoising%2C%20segmentation%2C%20re-labeling%20of%20noise-only%20segments%2C%20data%0Anormalization%2C%20and%20time-frequency%20analysis%20of%20the%20PCG%20segments%20using%20wavelet%0Ascattering%20transform.%20We%20then%20conduct%20four%20experiments%2C%20first%20three%20%28E1-E3%29%0Ausing%20PCG%202022%20dataset%2C%20and%20fourth%20%28E4%29%20using%20PCG%202016%20dataset.%20It%20turns%20out%0Athat%20our%20custom%201D-CNN%20outperforms%20other%20two%20NNs%20%28LSTM-RNN%20and%20C-RNN%29.%20Further%2C%0Aour%201D-CNN%20model%20outperforms%20the%20related%20work%20in%20terms%20of%20accuracy%2C%20weighted%0Aaccuracy%2C%20F1-score%20and%20AUROC%2C%20for%20experiment%20E3%20%28that%20utilizes%20the%20cleaned%20and%0Are-labeled%20PCG%202022%20dataset%29.%20As%20for%20experiment%20E1%20%28that%20utilizes%20the%20original%0APCG%202022%20dataset%29%2C%20our%20model%20performs%20quite%20close%20to%20the%20related%20work%20in%20terms%0Aof%20weighted%20accuracy%20and%20F1-score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.11423v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeart%2520Murmur%2520and%2520Abnormal%2520PCG%2520Detection%2520via%2520Wavelet%2520Scattering%2520Transform%250A%2520%2520%2526%2520a%25201D-CNN%26entry.906535625%3DAhmed%2520Patwa%2520and%2520Muhammad%2520Mahboob%2520Ur%2520Rahman%2520and%2520Tareq%2520Y.%2520Al-Naffouri%26entry.1292438233%3D%2520%2520Heart%2520murmurs%2520provide%2520valuable%2520information%2520about%2520mechanical%2520activity%2520of%2520the%250Aheart%252C%2520which%2520aids%2520in%2520diagnosis%2520of%2520various%2520heart%2520valve%2520diseases.%2520This%2520work%2520does%250Aautomatic%2520and%2520accurate%2520heart%2520murmur%2520detection%2520from%2520phonocardiogram%2520%2528PCG%2529%250Arecordings.%2520Two%2520public%2520PCG%2520datasets%2520%2528CirCor%2520Digiscope%25202022%2520dataset%2520and%2520PCG%25202016%250Adataset%2529%2520from%2520Physionet%2520online%2520database%2520are%2520utilized%2520to%2520train%2520and%2520test%2520three%250Acustom%2520neural%2520networks%2520%2528NN%2529%253A%2520a%25201D%2520convolutional%2520neural%2520network%2520%2528CNN%2529%252C%2520a%2520long%250Ashort-term%2520memory%2520%2528LSTM%2529%2520recurrent%2520neural%2520network%2520%2528RNN%2529%252C%2520and%2520a%2520convolutional%250ARNN%2520%2528C-RNN%2529.%2520We%2520first%2520do%2520pre-processing%2520which%2520includes%2520the%2520following%2520key%2520steps%253A%250Adenoising%252C%2520segmentation%252C%2520re-labeling%2520of%2520noise-only%2520segments%252C%2520data%250Anormalization%252C%2520and%2520time-frequency%2520analysis%2520of%2520the%2520PCG%2520segments%2520using%2520wavelet%250Ascattering%2520transform.%2520We%2520then%2520conduct%2520four%2520experiments%252C%2520first%2520three%2520%2528E1-E3%2529%250Ausing%2520PCG%25202022%2520dataset%252C%2520and%2520fourth%2520%2528E4%2529%2520using%2520PCG%25202016%2520dataset.%2520It%2520turns%2520out%250Athat%2520our%2520custom%25201D-CNN%2520outperforms%2520other%2520two%2520NNs%2520%2528LSTM-RNN%2520and%2520C-RNN%2529.%2520Further%252C%250Aour%25201D-CNN%2520model%2520outperforms%2520the%2520related%2520work%2520in%2520terms%2520of%2520accuracy%252C%2520weighted%250Aaccuracy%252C%2520F1-score%2520and%2520AUROC%252C%2520for%2520experiment%2520E3%2520%2528that%2520utilizes%2520the%2520cleaned%2520and%250Are-labeled%2520PCG%25202022%2520dataset%2529.%2520As%2520for%2520experiment%2520E1%2520%2528that%2520utilizes%2520the%2520original%250APCG%25202022%2520dataset%2529%252C%2520our%2520model%2520performs%2520quite%2520close%2520to%2520the%2520related%2520work%2520in%2520terms%250Aof%2520weighted%2520accuracy%2520and%2520F1-score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.11423v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heart%20Murmur%20and%20Abnormal%20PCG%20Detection%20via%20Wavelet%20Scattering%20Transform%0A%20%20%26%20a%201D-CNN&entry.906535625=Ahmed%20Patwa%20and%20Muhammad%20Mahboob%20Ur%20Rahman%20and%20Tareq%20Y.%20Al-Naffouri&entry.1292438233=%20%20Heart%20murmurs%20provide%20valuable%20information%20about%20mechanical%20activity%20of%20the%0Aheart%2C%20which%20aids%20in%20diagnosis%20of%20various%20heart%20valve%20diseases.%20This%20work%20does%0Aautomatic%20and%20accurate%20heart%20murmur%20detection%20from%20phonocardiogram%20%28PCG%29%0Arecordings.%20Two%20public%20PCG%20datasets%20%28CirCor%20Digiscope%202022%20dataset%20and%20PCG%202016%0Adataset%29%20from%20Physionet%20online%20database%20are%20utilized%20to%20train%20and%20test%20three%0Acustom%20neural%20networks%20%28NN%29%3A%20a%201D%20convolutional%20neural%20network%20%28CNN%29%2C%20a%20long%0Ashort-term%20memory%20%28LSTM%29%20recurrent%20neural%20network%20%28RNN%29%2C%20and%20a%20convolutional%0ARNN%20%28C-RNN%29.%20We%20first%20do%20pre-processing%20which%20includes%20the%20following%20key%20steps%3A%0Adenoising%2C%20segmentation%2C%20re-labeling%20of%20noise-only%20segments%2C%20data%0Anormalization%2C%20and%20time-frequency%20analysis%20of%20the%20PCG%20segments%20using%20wavelet%0Ascattering%20transform.%20We%20then%20conduct%20four%20experiments%2C%20first%20three%20%28E1-E3%29%0Ausing%20PCG%202022%20dataset%2C%20and%20fourth%20%28E4%29%20using%20PCG%202016%20dataset.%20It%20turns%20out%0Athat%20our%20custom%201D-CNN%20outperforms%20other%20two%20NNs%20%28LSTM-RNN%20and%20C-RNN%29.%20Further%2C%0Aour%201D-CNN%20model%20outperforms%20the%20related%20work%20in%20terms%20of%20accuracy%2C%20weighted%0Aaccuracy%2C%20F1-score%20and%20AUROC%2C%20for%20experiment%20E3%20%28that%20utilizes%20the%20cleaned%20and%0Are-labeled%20PCG%202022%20dataset%29.%20As%20for%20experiment%20E1%20%28that%20utilizes%20the%20original%0APCG%202022%20dataset%29%2C%20our%20model%20performs%20quite%20close%20to%20the%20related%20work%20in%20terms%0Aof%20weighted%20accuracy%20and%20F1-score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.11423v2&entry.124074799=Read"},
{"title": "Delving into the Trajectory Long-tail Distribution for Muti-object\n  Tracking", "author": "Sijia Chen and En Yu and Jinyang Li and Wenbing Tao", "abstract": "  Multiple Object Tracking (MOT) is a critical area within computer vision,\nwith a broad spectrum of practical implementations. Current research has\nprimarily focused on the development of tracking algorithms and enhancement of\npost-processing techniques. Yet, there has been a lack of thorough examination\nconcerning the nature of tracking data it self. In this study, we pioneer an\nexploration into the distribution patterns of tracking data and identify a\npronounced long-tail distribution issue within existing MOT datasets. We note a\nsignificant imbalance in the distribution of trajectory lengths across\ndifferent pedestrians, a phenomenon we refer to as ``pedestrians trajectory\nlong-tail distribution''. Addressing this challenge, we introduce a bespoke\nstrategy designed to mitigate the effects of this skewed distribution.\nSpecifically, we propose two data augmentation strategies, including Stationary\nCamera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation\n(DVA) , designed for viewpoint states and the Group Softmax (GS) module for\nRe-ID. SVA is to backtrack and predict the pedestrian trajectory of tail\nclasses, and DVA is to use diffusion model to change the background of the\nscene. GS divides the pedestrians into unrelated groups and performs softmax\noperation on each group individually. Our proposed strategies can be integrated\ninto numerous existing tracking systems, and extensive experimentation\nvalidates the efficacy of our method in reducing the influence of long-tail\ndistribution on multi-object tracking performance. The code is available at\nhttps://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.\n", "link": "http://arxiv.org/abs/2403.04700v2", "date": "2024-05-24", "relevancy": 2.2727, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5759}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5668}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Delving%20into%20the%20Trajectory%20Long-tail%20Distribution%20for%20Muti-object%0A%20%20Tracking&body=Title%3A%20Delving%20into%20the%20Trajectory%20Long-tail%20Distribution%20for%20Muti-object%0A%20%20Tracking%0AAuthor%3A%20Sijia%20Chen%20and%20En%20Yu%20and%20Jinyang%20Li%20and%20Wenbing%20Tao%0AAbstract%3A%20%20%20Multiple%20Object%20Tracking%20%28MOT%29%20is%20a%20critical%20area%20within%20computer%20vision%2C%0Awith%20a%20broad%20spectrum%20of%20practical%20implementations.%20Current%20research%20has%0Aprimarily%20focused%20on%20the%20development%20of%20tracking%20algorithms%20and%20enhancement%20of%0Apost-processing%20techniques.%20Yet%2C%20there%20has%20been%20a%20lack%20of%20thorough%20examination%0Aconcerning%20the%20nature%20of%20tracking%20data%20it%20self.%20In%20this%20study%2C%20we%20pioneer%20an%0Aexploration%20into%20the%20distribution%20patterns%20of%20tracking%20data%20and%20identify%20a%0Apronounced%20long-tail%20distribution%20issue%20within%20existing%20MOT%20datasets.%20We%20note%20a%0Asignificant%20imbalance%20in%20the%20distribution%20of%20trajectory%20lengths%20across%0Adifferent%20pedestrians%2C%20a%20phenomenon%20we%20refer%20to%20as%20%60%60pedestrians%20trajectory%0Along-tail%20distribution%27%27.%20Addressing%20this%20challenge%2C%20we%20introduce%20a%20bespoke%0Astrategy%20designed%20to%20mitigate%20the%20effects%20of%20this%20skewed%20distribution.%0ASpecifically%2C%20we%20propose%20two%20data%20augmentation%20strategies%2C%20including%20Stationary%0ACamera%20View%20Data%20Augmentation%20%28SVA%29%20and%20Dynamic%20Camera%20View%20Data%20Augmentation%0A%28DVA%29%20%2C%20designed%20for%20viewpoint%20states%20and%20the%20Group%20Softmax%20%28GS%29%20module%20for%0ARe-ID.%20SVA%20is%20to%20backtrack%20and%20predict%20the%20pedestrian%20trajectory%20of%20tail%0Aclasses%2C%20and%20DVA%20is%20to%20use%20diffusion%20model%20to%20change%20the%20background%20of%20the%0Ascene.%20GS%20divides%20the%20pedestrians%20into%20unrelated%20groups%20and%20performs%20softmax%0Aoperation%20on%20each%20group%20individually.%20Our%20proposed%20strategies%20can%20be%20integrated%0Ainto%20numerous%20existing%20tracking%20systems%2C%20and%20extensive%20experimentation%0Avalidates%20the%20efficacy%20of%20our%20method%20in%20reducing%20the%20influence%20of%20long-tail%0Adistribution%20on%20multi-object%20tracking%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelving%2520into%2520the%2520Trajectory%2520Long-tail%2520Distribution%2520for%2520Muti-object%250A%2520%2520Tracking%26entry.906535625%3DSijia%2520Chen%2520and%2520En%2520Yu%2520and%2520Jinyang%2520Li%2520and%2520Wenbing%2520Tao%26entry.1292438233%3D%2520%2520Multiple%2520Object%2520Tracking%2520%2528MOT%2529%2520is%2520a%2520critical%2520area%2520within%2520computer%2520vision%252C%250Awith%2520a%2520broad%2520spectrum%2520of%2520practical%2520implementations.%2520Current%2520research%2520has%250Aprimarily%2520focused%2520on%2520the%2520development%2520of%2520tracking%2520algorithms%2520and%2520enhancement%2520of%250Apost-processing%2520techniques.%2520Yet%252C%2520there%2520has%2520been%2520a%2520lack%2520of%2520thorough%2520examination%250Aconcerning%2520the%2520nature%2520of%2520tracking%2520data%2520it%2520self.%2520In%2520this%2520study%252C%2520we%2520pioneer%2520an%250Aexploration%2520into%2520the%2520distribution%2520patterns%2520of%2520tracking%2520data%2520and%2520identify%2520a%250Apronounced%2520long-tail%2520distribution%2520issue%2520within%2520existing%2520MOT%2520datasets.%2520We%2520note%2520a%250Asignificant%2520imbalance%2520in%2520the%2520distribution%2520of%2520trajectory%2520lengths%2520across%250Adifferent%2520pedestrians%252C%2520a%2520phenomenon%2520we%2520refer%2520to%2520as%2520%2560%2560pedestrians%2520trajectory%250Along-tail%2520distribution%2527%2527.%2520Addressing%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520bespoke%250Astrategy%2520designed%2520to%2520mitigate%2520the%2520effects%2520of%2520this%2520skewed%2520distribution.%250ASpecifically%252C%2520we%2520propose%2520two%2520data%2520augmentation%2520strategies%252C%2520including%2520Stationary%250ACamera%2520View%2520Data%2520Augmentation%2520%2528SVA%2529%2520and%2520Dynamic%2520Camera%2520View%2520Data%2520Augmentation%250A%2528DVA%2529%2520%252C%2520designed%2520for%2520viewpoint%2520states%2520and%2520the%2520Group%2520Softmax%2520%2528GS%2529%2520module%2520for%250ARe-ID.%2520SVA%2520is%2520to%2520backtrack%2520and%2520predict%2520the%2520pedestrian%2520trajectory%2520of%2520tail%250Aclasses%252C%2520and%2520DVA%2520is%2520to%2520use%2520diffusion%2520model%2520to%2520change%2520the%2520background%2520of%2520the%250Ascene.%2520GS%2520divides%2520the%2520pedestrians%2520into%2520unrelated%2520groups%2520and%2520performs%2520softmax%250Aoperation%2520on%2520each%2520group%2520individually.%2520Our%2520proposed%2520strategies%2520can%2520be%2520integrated%250Ainto%2520numerous%2520existing%2520tracking%2520systems%252C%2520and%2520extensive%2520experimentation%250Avalidates%2520the%2520efficacy%2520of%2520our%2520method%2520in%2520reducing%2520the%2520influence%2520of%2520long-tail%250Adistribution%2520on%2520multi-object%2520tracking%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delving%20into%20the%20Trajectory%20Long-tail%20Distribution%20for%20Muti-object%0A%20%20Tracking&entry.906535625=Sijia%20Chen%20and%20En%20Yu%20and%20Jinyang%20Li%20and%20Wenbing%20Tao&entry.1292438233=%20%20Multiple%20Object%20Tracking%20%28MOT%29%20is%20a%20critical%20area%20within%20computer%20vision%2C%0Awith%20a%20broad%20spectrum%20of%20practical%20implementations.%20Current%20research%20has%0Aprimarily%20focused%20on%20the%20development%20of%20tracking%20algorithms%20and%20enhancement%20of%0Apost-processing%20techniques.%20Yet%2C%20there%20has%20been%20a%20lack%20of%20thorough%20examination%0Aconcerning%20the%20nature%20of%20tracking%20data%20it%20self.%20In%20this%20study%2C%20we%20pioneer%20an%0Aexploration%20into%20the%20distribution%20patterns%20of%20tracking%20data%20and%20identify%20a%0Apronounced%20long-tail%20distribution%20issue%20within%20existing%20MOT%20datasets.%20We%20note%20a%0Asignificant%20imbalance%20in%20the%20distribution%20of%20trajectory%20lengths%20across%0Adifferent%20pedestrians%2C%20a%20phenomenon%20we%20refer%20to%20as%20%60%60pedestrians%20trajectory%0Along-tail%20distribution%27%27.%20Addressing%20this%20challenge%2C%20we%20introduce%20a%20bespoke%0Astrategy%20designed%20to%20mitigate%20the%20effects%20of%20this%20skewed%20distribution.%0ASpecifically%2C%20we%20propose%20two%20data%20augmentation%20strategies%2C%20including%20Stationary%0ACamera%20View%20Data%20Augmentation%20%28SVA%29%20and%20Dynamic%20Camera%20View%20Data%20Augmentation%0A%28DVA%29%20%2C%20designed%20for%20viewpoint%20states%20and%20the%20Group%20Softmax%20%28GS%29%20module%20for%0ARe-ID.%20SVA%20is%20to%20backtrack%20and%20predict%20the%20pedestrian%20trajectory%20of%20tail%0Aclasses%2C%20and%20DVA%20is%20to%20use%20diffusion%20model%20to%20change%20the%20background%20of%20the%0Ascene.%20GS%20divides%20the%20pedestrians%20into%20unrelated%20groups%20and%20performs%20softmax%0Aoperation%20on%20each%20group%20individually.%20Our%20proposed%20strategies%20can%20be%20integrated%0Ainto%20numerous%20existing%20tracking%20systems%2C%20and%20extensive%20experimentation%0Avalidates%20the%20efficacy%20of%20our%20method%20in%20reducing%20the%20influence%20of%20long-tail%0Adistribution%20on%20multi-object%20tracking%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04700v2&entry.124074799=Read"},
{"title": "Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary\n  Action Recognition", "author": "Kun-Yu Lin and Henghui Ding and Jiaming Zhou and Yu-Ming Tang and Yi-Xing Peng and Zhilin Zhao and Chen Change Loy and Wei-Shi Zheng", "abstract": "  Building upon the impressive success of CLIP (Contrastive Language-Image\nPretraining), recent pioneer works have proposed to adapt the powerful CLIP to\nvideo data, leading to efficient and effective video learners for\nopen-vocabulary action recognition. Inspired by that humans perform actions in\ndiverse environments, our work delves into an intriguing question: Can\nCLIP-based video learners effectively generalize to video domains they have not\nencountered during training? To answer this, we establish a CROSS-domain\nOpen-Vocabulary Action recognition benchmark named XOV-Action, and conduct a\ncomprehensive evaluation of five state-of-the-art CLIP-based video learners\nunder various types of domain gaps. The evaluation demonstrates that previous\nmethods exhibit limited action recognition performance in unseen video domains,\nrevealing potential challenges of the cross-domain open-vocabulary action\nrecognition task. In this paper, we focus on one critical challenge of the\ntask, namely scene bias, and accordingly contribute a novel scene-aware\nvideo-text alignment method. Our key idea is to distinguish video\nrepresentations apart from scene-encoded text representations, aiming to learn\nscene-agnostic video representations for recognizing actions across domains.\nExtensive experiments demonstrate the effectiveness of our method. The\nbenchmark and code will be available at\nhttps://github.com/KunyuLin/XOV-Action/.\n", "link": "http://arxiv.org/abs/2403.01560v2", "date": "2024-05-24", "relevancy": 2.2699, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6002}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5698}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20CLIP-based%20Video%20Learners%20in%20Cross-Domain%20Open-Vocabulary%0A%20%20Action%20Recognition&body=Title%3A%20Rethinking%20CLIP-based%20Video%20Learners%20in%20Cross-Domain%20Open-Vocabulary%0A%20%20Action%20Recognition%0AAuthor%3A%20Kun-Yu%20Lin%20and%20Henghui%20Ding%20and%20Jiaming%20Zhou%20and%20Yu-Ming%20Tang%20and%20Yi-Xing%20Peng%20and%20Zhilin%20Zhao%20and%20Chen%20Change%20Loy%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20%20%20Building%20upon%20the%20impressive%20success%20of%20CLIP%20%28Contrastive%20Language-Image%0APretraining%29%2C%20recent%20pioneer%20works%20have%20proposed%20to%20adapt%20the%20powerful%20CLIP%20to%0Avideo%20data%2C%20leading%20to%20efficient%20and%20effective%20video%20learners%20for%0Aopen-vocabulary%20action%20recognition.%20Inspired%20by%20that%20humans%20perform%20actions%20in%0Adiverse%20environments%2C%20our%20work%20delves%20into%20an%20intriguing%20question%3A%20Can%0ACLIP-based%20video%20learners%20effectively%20generalize%20to%20video%20domains%20they%20have%20not%0Aencountered%20during%20training%3F%20To%20answer%20this%2C%20we%20establish%20a%20CROSS-domain%0AOpen-Vocabulary%20Action%20recognition%20benchmark%20named%20XOV-Action%2C%20and%20conduct%20a%0Acomprehensive%20evaluation%20of%20five%20state-of-the-art%20CLIP-based%20video%20learners%0Aunder%20various%20types%20of%20domain%20gaps.%20The%20evaluation%20demonstrates%20that%20previous%0Amethods%20exhibit%20limited%20action%20recognition%20performance%20in%20unseen%20video%20domains%2C%0Arevealing%20potential%20challenges%20of%20the%20cross-domain%20open-vocabulary%20action%0Arecognition%20task.%20In%20this%20paper%2C%20we%20focus%20on%20one%20critical%20challenge%20of%20the%0Atask%2C%20namely%20scene%20bias%2C%20and%20accordingly%20contribute%20a%20novel%20scene-aware%0Avideo-text%20alignment%20method.%20Our%20key%20idea%20is%20to%20distinguish%20video%0Arepresentations%20apart%20from%20scene-encoded%20text%20representations%2C%20aiming%20to%20learn%0Ascene-agnostic%20video%20representations%20for%20recognizing%20actions%20across%20domains.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method.%20The%0Abenchmark%20and%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/KunyuLin/XOV-Action/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01560v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520CLIP-based%2520Video%2520Learners%2520in%2520Cross-Domain%2520Open-Vocabulary%250A%2520%2520Action%2520Recognition%26entry.906535625%3DKun-Yu%2520Lin%2520and%2520Henghui%2520Ding%2520and%2520Jiaming%2520Zhou%2520and%2520Yu-Ming%2520Tang%2520and%2520Yi-Xing%2520Peng%2520and%2520Zhilin%2520Zhao%2520and%2520Chen%2520Change%2520Loy%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3D%2520%2520Building%2520upon%2520the%2520impressive%2520success%2520of%2520CLIP%2520%2528Contrastive%2520Language-Image%250APretraining%2529%252C%2520recent%2520pioneer%2520works%2520have%2520proposed%2520to%2520adapt%2520the%2520powerful%2520CLIP%2520to%250Avideo%2520data%252C%2520leading%2520to%2520efficient%2520and%2520effective%2520video%2520learners%2520for%250Aopen-vocabulary%2520action%2520recognition.%2520Inspired%2520by%2520that%2520humans%2520perform%2520actions%2520in%250Adiverse%2520environments%252C%2520our%2520work%2520delves%2520into%2520an%2520intriguing%2520question%253A%2520Can%250ACLIP-based%2520video%2520learners%2520effectively%2520generalize%2520to%2520video%2520domains%2520they%2520have%2520not%250Aencountered%2520during%2520training%253F%2520To%2520answer%2520this%252C%2520we%2520establish%2520a%2520CROSS-domain%250AOpen-Vocabulary%2520Action%2520recognition%2520benchmark%2520named%2520XOV-Action%252C%2520and%2520conduct%2520a%250Acomprehensive%2520evaluation%2520of%2520five%2520state-of-the-art%2520CLIP-based%2520video%2520learners%250Aunder%2520various%2520types%2520of%2520domain%2520gaps.%2520The%2520evaluation%2520demonstrates%2520that%2520previous%250Amethods%2520exhibit%2520limited%2520action%2520recognition%2520performance%2520in%2520unseen%2520video%2520domains%252C%250Arevealing%2520potential%2520challenges%2520of%2520the%2520cross-domain%2520open-vocabulary%2520action%250Arecognition%2520task.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520one%2520critical%2520challenge%2520of%2520the%250Atask%252C%2520namely%2520scene%2520bias%252C%2520and%2520accordingly%2520contribute%2520a%2520novel%2520scene-aware%250Avideo-text%2520alignment%2520method.%2520Our%2520key%2520idea%2520is%2520to%2520distinguish%2520video%250Arepresentations%2520apart%2520from%2520scene-encoded%2520text%2520representations%252C%2520aiming%2520to%2520learn%250Ascene-agnostic%2520video%2520representations%2520for%2520recognizing%2520actions%2520across%2520domains.%250AExtensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%2520The%250Abenchmark%2520and%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/KunyuLin/XOV-Action/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01560v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20CLIP-based%20Video%20Learners%20in%20Cross-Domain%20Open-Vocabulary%0A%20%20Action%20Recognition&entry.906535625=Kun-Yu%20Lin%20and%20Henghui%20Ding%20and%20Jiaming%20Zhou%20and%20Yu-Ming%20Tang%20and%20Yi-Xing%20Peng%20and%20Zhilin%20Zhao%20and%20Chen%20Change%20Loy%20and%20Wei-Shi%20Zheng&entry.1292438233=%20%20Building%20upon%20the%20impressive%20success%20of%20CLIP%20%28Contrastive%20Language-Image%0APretraining%29%2C%20recent%20pioneer%20works%20have%20proposed%20to%20adapt%20the%20powerful%20CLIP%20to%0Avideo%20data%2C%20leading%20to%20efficient%20and%20effective%20video%20learners%20for%0Aopen-vocabulary%20action%20recognition.%20Inspired%20by%20that%20humans%20perform%20actions%20in%0Adiverse%20environments%2C%20our%20work%20delves%20into%20an%20intriguing%20question%3A%20Can%0ACLIP-based%20video%20learners%20effectively%20generalize%20to%20video%20domains%20they%20have%20not%0Aencountered%20during%20training%3F%20To%20answer%20this%2C%20we%20establish%20a%20CROSS-domain%0AOpen-Vocabulary%20Action%20recognition%20benchmark%20named%20XOV-Action%2C%20and%20conduct%20a%0Acomprehensive%20evaluation%20of%20five%20state-of-the-art%20CLIP-based%20video%20learners%0Aunder%20various%20types%20of%20domain%20gaps.%20The%20evaluation%20demonstrates%20that%20previous%0Amethods%20exhibit%20limited%20action%20recognition%20performance%20in%20unseen%20video%20domains%2C%0Arevealing%20potential%20challenges%20of%20the%20cross-domain%20open-vocabulary%20action%0Arecognition%20task.%20In%20this%20paper%2C%20we%20focus%20on%20one%20critical%20challenge%20of%20the%0Atask%2C%20namely%20scene%20bias%2C%20and%20accordingly%20contribute%20a%20novel%20scene-aware%0Avideo-text%20alignment%20method.%20Our%20key%20idea%20is%20to%20distinguish%20video%0Arepresentations%20apart%20from%20scene-encoded%20text%20representations%2C%20aiming%20to%20learn%0Ascene-agnostic%20video%20representations%20for%20recognizing%20actions%20across%20domains.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method.%20The%0Abenchmark%20and%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/KunyuLin/XOV-Action/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01560v2&entry.124074799=Read"},
{"title": "Fundamental limits of weak learnability in high-dimensional multi-index\n  models", "author": "Emanuele Troiani and Yatin Dandi and Leonardo Defilippis and Lenka Zdeborov\u00e1 and Bruno Loureiro and Florent Krzakala", "abstract": "  Multi-index models -- functions which only depend on the covariates through a\nnon-linear transformation of their projection on a subspace -- are a useful\nbenchmark for investigating feature learning with neural networks. This paper\nexamines the theoretical boundaries of learnability in this hypothesis class,\nfocusing particularly on the minimum sample complexity required for weakly\nrecovering their low-dimensional structure with first-order iterative\nalgorithms, in the high-dimensional regime where the number of samples is\n$n=\\alpha d$ is proportional to the covariate dimension $d$. Our findings\nunfold in three parts: (i) first, we identify under which conditions a\n\\textit{trivial subspace} can be learned with a single step of a first-order\nalgorithm for any $\\alpha\\!>\\!0$; (ii) second, in the case where the trivial\nsubspace is empty, we provide necessary and sufficient conditions for the\nexistence of an {\\it easy subspace} consisting of directions that can be\nlearned only above a certain sample complexity $\\alpha\\!>\\!\\alpha_c$. The\ncritical threshold $\\alpha_{c}$ marks the presence of a computational phase\ntransition, in the sense that no efficient iterative algorithm can succeed for\n$\\alpha\\!<\\!\\alpha_c$. In a limited but interesting set of really hard\ndirections -- akin to the parity problem -- $\\alpha_c$ is found to diverge.\nFinally, (iii) we demonstrate that interactions between different directions\ncan result in an intricate hierarchical learning phenomenon, where some\ndirections can be learned sequentially when coupled to easier ones. Our\nanalytical approach is built on the optimality of approximate message-passing\nalgorithms among first-order iterative methods, delineating the fundamental\nlearnability limit across a broad spectrum of algorithms, including neural\nnetworks trained with gradient descent.\n", "link": "http://arxiv.org/abs/2405.15480v1", "date": "2024-05-24", "relevancy": 2.2647, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.466}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4474}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fundamental%20limits%20of%20weak%20learnability%20in%20high-dimensional%20multi-index%0A%20%20models&body=Title%3A%20Fundamental%20limits%20of%20weak%20learnability%20in%20high-dimensional%20multi-index%0A%20%20models%0AAuthor%3A%20Emanuele%20Troiani%20and%20Yatin%20Dandi%20and%20Leonardo%20Defilippis%20and%20Lenka%20Zdeborov%C3%A1%20and%20Bruno%20Loureiro%20and%20Florent%20Krzakala%0AAbstract%3A%20%20%20Multi-index%20models%20--%20functions%20which%20only%20depend%20on%20the%20covariates%20through%20a%0Anon-linear%20transformation%20of%20their%20projection%20on%20a%20subspace%20--%20are%20a%20useful%0Abenchmark%20for%20investigating%20feature%20learning%20with%20neural%20networks.%20This%20paper%0Aexamines%20the%20theoretical%20boundaries%20of%20learnability%20in%20this%20hypothesis%20class%2C%0Afocusing%20particularly%20on%20the%20minimum%20sample%20complexity%20required%20for%20weakly%0Arecovering%20their%20low-dimensional%20structure%20with%20first-order%20iterative%0Aalgorithms%2C%20in%20the%20high-dimensional%20regime%20where%20the%20number%20of%20samples%20is%0A%24n%3D%5Calpha%20d%24%20is%20proportional%20to%20the%20covariate%20dimension%20%24d%24.%20Our%20findings%0Aunfold%20in%20three%20parts%3A%20%28i%29%20first%2C%20we%20identify%20under%20which%20conditions%20a%0A%5Ctextit%7Btrivial%20subspace%7D%20can%20be%20learned%20with%20a%20single%20step%20of%20a%20first-order%0Aalgorithm%20for%20any%20%24%5Calpha%5C%21%3E%5C%210%24%3B%20%28ii%29%20second%2C%20in%20the%20case%20where%20the%20trivial%0Asubspace%20is%20empty%2C%20we%20provide%20necessary%20and%20sufficient%20conditions%20for%20the%0Aexistence%20of%20an%20%7B%5Cit%20easy%20subspace%7D%20consisting%20of%20directions%20that%20can%20be%0Alearned%20only%20above%20a%20certain%20sample%20complexity%20%24%5Calpha%5C%21%3E%5C%21%5Calpha_c%24.%20The%0Acritical%20threshold%20%24%5Calpha_%7Bc%7D%24%20marks%20the%20presence%20of%20a%20computational%20phase%0Atransition%2C%20in%20the%20sense%20that%20no%20efficient%20iterative%20algorithm%20can%20succeed%20for%0A%24%5Calpha%5C%21%3C%5C%21%5Calpha_c%24.%20In%20a%20limited%20but%20interesting%20set%20of%20really%20hard%0Adirections%20--%20akin%20to%20the%20parity%20problem%20--%20%24%5Calpha_c%24%20is%20found%20to%20diverge.%0AFinally%2C%20%28iii%29%20we%20demonstrate%20that%20interactions%20between%20different%20directions%0Acan%20result%20in%20an%20intricate%20hierarchical%20learning%20phenomenon%2C%20where%20some%0Adirections%20can%20be%20learned%20sequentially%20when%20coupled%20to%20easier%20ones.%20Our%0Aanalytical%20approach%20is%20built%20on%20the%20optimality%20of%20approximate%20message-passing%0Aalgorithms%20among%20first-order%20iterative%20methods%2C%20delineating%20the%20fundamental%0Alearnability%20limit%20across%20a%20broad%20spectrum%20of%20algorithms%2C%20including%20neural%0Anetworks%20trained%20with%20gradient%20descent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFundamental%2520limits%2520of%2520weak%2520learnability%2520in%2520high-dimensional%2520multi-index%250A%2520%2520models%26entry.906535625%3DEmanuele%2520Troiani%2520and%2520Yatin%2520Dandi%2520and%2520Leonardo%2520Defilippis%2520and%2520Lenka%2520Zdeborov%25C3%25A1%2520and%2520Bruno%2520Loureiro%2520and%2520Florent%2520Krzakala%26entry.1292438233%3D%2520%2520Multi-index%2520models%2520--%2520functions%2520which%2520only%2520depend%2520on%2520the%2520covariates%2520through%2520a%250Anon-linear%2520transformation%2520of%2520their%2520projection%2520on%2520a%2520subspace%2520--%2520are%2520a%2520useful%250Abenchmark%2520for%2520investigating%2520feature%2520learning%2520with%2520neural%2520networks.%2520This%2520paper%250Aexamines%2520the%2520theoretical%2520boundaries%2520of%2520learnability%2520in%2520this%2520hypothesis%2520class%252C%250Afocusing%2520particularly%2520on%2520the%2520minimum%2520sample%2520complexity%2520required%2520for%2520weakly%250Arecovering%2520their%2520low-dimensional%2520structure%2520with%2520first-order%2520iterative%250Aalgorithms%252C%2520in%2520the%2520high-dimensional%2520regime%2520where%2520the%2520number%2520of%2520samples%2520is%250A%2524n%253D%255Calpha%2520d%2524%2520is%2520proportional%2520to%2520the%2520covariate%2520dimension%2520%2524d%2524.%2520Our%2520findings%250Aunfold%2520in%2520three%2520parts%253A%2520%2528i%2529%2520first%252C%2520we%2520identify%2520under%2520which%2520conditions%2520a%250A%255Ctextit%257Btrivial%2520subspace%257D%2520can%2520be%2520learned%2520with%2520a%2520single%2520step%2520of%2520a%2520first-order%250Aalgorithm%2520for%2520any%2520%2524%255Calpha%255C%2521%253E%255C%25210%2524%253B%2520%2528ii%2529%2520second%252C%2520in%2520the%2520case%2520where%2520the%2520trivial%250Asubspace%2520is%2520empty%252C%2520we%2520provide%2520necessary%2520and%2520sufficient%2520conditions%2520for%2520the%250Aexistence%2520of%2520an%2520%257B%255Cit%2520easy%2520subspace%257D%2520consisting%2520of%2520directions%2520that%2520can%2520be%250Alearned%2520only%2520above%2520a%2520certain%2520sample%2520complexity%2520%2524%255Calpha%255C%2521%253E%255C%2521%255Calpha_c%2524.%2520The%250Acritical%2520threshold%2520%2524%255Calpha_%257Bc%257D%2524%2520marks%2520the%2520presence%2520of%2520a%2520computational%2520phase%250Atransition%252C%2520in%2520the%2520sense%2520that%2520no%2520efficient%2520iterative%2520algorithm%2520can%2520succeed%2520for%250A%2524%255Calpha%255C%2521%253C%255C%2521%255Calpha_c%2524.%2520In%2520a%2520limited%2520but%2520interesting%2520set%2520of%2520really%2520hard%250Adirections%2520--%2520akin%2520to%2520the%2520parity%2520problem%2520--%2520%2524%255Calpha_c%2524%2520is%2520found%2520to%2520diverge.%250AFinally%252C%2520%2528iii%2529%2520we%2520demonstrate%2520that%2520interactions%2520between%2520different%2520directions%250Acan%2520result%2520in%2520an%2520intricate%2520hierarchical%2520learning%2520phenomenon%252C%2520where%2520some%250Adirections%2520can%2520be%2520learned%2520sequentially%2520when%2520coupled%2520to%2520easier%2520ones.%2520Our%250Aanalytical%2520approach%2520is%2520built%2520on%2520the%2520optimality%2520of%2520approximate%2520message-passing%250Aalgorithms%2520among%2520first-order%2520iterative%2520methods%252C%2520delineating%2520the%2520fundamental%250Alearnability%2520limit%2520across%2520a%2520broad%2520spectrum%2520of%2520algorithms%252C%2520including%2520neural%250Anetworks%2520trained%2520with%2520gradient%2520descent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fundamental%20limits%20of%20weak%20learnability%20in%20high-dimensional%20multi-index%0A%20%20models&entry.906535625=Emanuele%20Troiani%20and%20Yatin%20Dandi%20and%20Leonardo%20Defilippis%20and%20Lenka%20Zdeborov%C3%A1%20and%20Bruno%20Loureiro%20and%20Florent%20Krzakala&entry.1292438233=%20%20Multi-index%20models%20--%20functions%20which%20only%20depend%20on%20the%20covariates%20through%20a%0Anon-linear%20transformation%20of%20their%20projection%20on%20a%20subspace%20--%20are%20a%20useful%0Abenchmark%20for%20investigating%20feature%20learning%20with%20neural%20networks.%20This%20paper%0Aexamines%20the%20theoretical%20boundaries%20of%20learnability%20in%20this%20hypothesis%20class%2C%0Afocusing%20particularly%20on%20the%20minimum%20sample%20complexity%20required%20for%20weakly%0Arecovering%20their%20low-dimensional%20structure%20with%20first-order%20iterative%0Aalgorithms%2C%20in%20the%20high-dimensional%20regime%20where%20the%20number%20of%20samples%20is%0A%24n%3D%5Calpha%20d%24%20is%20proportional%20to%20the%20covariate%20dimension%20%24d%24.%20Our%20findings%0Aunfold%20in%20three%20parts%3A%20%28i%29%20first%2C%20we%20identify%20under%20which%20conditions%20a%0A%5Ctextit%7Btrivial%20subspace%7D%20can%20be%20learned%20with%20a%20single%20step%20of%20a%20first-order%0Aalgorithm%20for%20any%20%24%5Calpha%5C%21%3E%5C%210%24%3B%20%28ii%29%20second%2C%20in%20the%20case%20where%20the%20trivial%0Asubspace%20is%20empty%2C%20we%20provide%20necessary%20and%20sufficient%20conditions%20for%20the%0Aexistence%20of%20an%20%7B%5Cit%20easy%20subspace%7D%20consisting%20of%20directions%20that%20can%20be%0Alearned%20only%20above%20a%20certain%20sample%20complexity%20%24%5Calpha%5C%21%3E%5C%21%5Calpha_c%24.%20The%0Acritical%20threshold%20%24%5Calpha_%7Bc%7D%24%20marks%20the%20presence%20of%20a%20computational%20phase%0Atransition%2C%20in%20the%20sense%20that%20no%20efficient%20iterative%20algorithm%20can%20succeed%20for%0A%24%5Calpha%5C%21%3C%5C%21%5Calpha_c%24.%20In%20a%20limited%20but%20interesting%20set%20of%20really%20hard%0Adirections%20--%20akin%20to%20the%20parity%20problem%20--%20%24%5Calpha_c%24%20is%20found%20to%20diverge.%0AFinally%2C%20%28iii%29%20we%20demonstrate%20that%20interactions%20between%20different%20directions%0Acan%20result%20in%20an%20intricate%20hierarchical%20learning%20phenomenon%2C%20where%20some%0Adirections%20can%20be%20learned%20sequentially%20when%20coupled%20to%20easier%20ones.%20Our%0Aanalytical%20approach%20is%20built%20on%20the%20optimality%20of%20approximate%20message-passing%0Aalgorithms%20among%20first-order%20iterative%20methods%2C%20delineating%20the%20fundamental%0Alearnability%20limit%20across%20a%20broad%20spectrum%20of%20algorithms%2C%20including%20neural%0Anetworks%20trained%20with%20gradient%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15480v1&entry.124074799=Read"},
{"title": "Semantic Aware Diffusion Inverse Tone Mapping", "author": "Abhishek Goswami and Aru Ranjan Singh and Francesco Banterle and Kurt Debattista and Thomas Bashford-Rogers", "abstract": "  The range of real-world scene luminance is larger than the capture capability\nof many digital camera sensors which leads to details being lost in captured\nimages, most typically in bright regions. Inverse tone mapping attempts to\nboost these captured Standard Dynamic Range (SDR) images back to High Dynamic\nRange (HDR) by creating a mapping that linearizes the well exposed values from\nthe SDR image, and provides a luminance boost to the clipped content. However,\nin most cases, the details in the clipped regions cannot be recovered or\nestimated. In this paper, we present a novel inverse tone mapping approach for\nmapping SDR images to HDR that generates lost details in clipped regions\nthrough a semantic-aware diffusion based inpainting approach. Our method\nproposes two major contributions - first, we propose to use a semantic graph to\nguide SDR diffusion based inpainting in masked regions in a saturated image.\nSecond, drawing inspiration from traditional HDR imaging and bracketing\nmethods, we propose a principled formulation to lift the SDR inpainted regions\nto HDR that is compatible with generative inpainting methods. Results show that\nour method demonstrates superior performance across different datasets on\nobjective metrics, and subjective experiments show that the proposed method\nmatches (and in most cases outperforms) state-of-art inverse tone mapping\noperators in terms of objective metrics and outperforms them for visual\nfidelity.\n", "link": "http://arxiv.org/abs/2405.15468v1", "date": "2024-05-24", "relevancy": 2.2639, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.603}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5667}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Aware%20Diffusion%20Inverse%20Tone%20Mapping&body=Title%3A%20Semantic%20Aware%20Diffusion%20Inverse%20Tone%20Mapping%0AAuthor%3A%20Abhishek%20Goswami%20and%20Aru%20Ranjan%20Singh%20and%20Francesco%20Banterle%20and%20Kurt%20Debattista%20and%20Thomas%20Bashford-Rogers%0AAbstract%3A%20%20%20The%20range%20of%20real-world%20scene%20luminance%20is%20larger%20than%20the%20capture%20capability%0Aof%20many%20digital%20camera%20sensors%20which%20leads%20to%20details%20being%20lost%20in%20captured%0Aimages%2C%20most%20typically%20in%20bright%20regions.%20Inverse%20tone%20mapping%20attempts%20to%0Aboost%20these%20captured%20Standard%20Dynamic%20Range%20%28SDR%29%20images%20back%20to%20High%20Dynamic%0ARange%20%28HDR%29%20by%20creating%20a%20mapping%20that%20linearizes%20the%20well%20exposed%20values%20from%0Athe%20SDR%20image%2C%20and%20provides%20a%20luminance%20boost%20to%20the%20clipped%20content.%20However%2C%0Ain%20most%20cases%2C%20the%20details%20in%20the%20clipped%20regions%20cannot%20be%20recovered%20or%0Aestimated.%20In%20this%20paper%2C%20we%20present%20a%20novel%20inverse%20tone%20mapping%20approach%20for%0Amapping%20SDR%20images%20to%20HDR%20that%20generates%20lost%20details%20in%20clipped%20regions%0Athrough%20a%20semantic-aware%20diffusion%20based%20inpainting%20approach.%20Our%20method%0Aproposes%20two%20major%20contributions%20-%20first%2C%20we%20propose%20to%20use%20a%20semantic%20graph%20to%0Aguide%20SDR%20diffusion%20based%20inpainting%20in%20masked%20regions%20in%20a%20saturated%20image.%0ASecond%2C%20drawing%20inspiration%20from%20traditional%20HDR%20imaging%20and%20bracketing%0Amethods%2C%20we%20propose%20a%20principled%20formulation%20to%20lift%20the%20SDR%20inpainted%20regions%0Ato%20HDR%20that%20is%20compatible%20with%20generative%20inpainting%20methods.%20Results%20show%20that%0Aour%20method%20demonstrates%20superior%20performance%20across%20different%20datasets%20on%0Aobjective%20metrics%2C%20and%20subjective%20experiments%20show%20that%20the%20proposed%20method%0Amatches%20%28and%20in%20most%20cases%20outperforms%29%20state-of-art%20inverse%20tone%20mapping%0Aoperators%20in%20terms%20of%20objective%20metrics%20and%20outperforms%20them%20for%20visual%0Afidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Aware%2520Diffusion%2520Inverse%2520Tone%2520Mapping%26entry.906535625%3DAbhishek%2520Goswami%2520and%2520Aru%2520Ranjan%2520Singh%2520and%2520Francesco%2520Banterle%2520and%2520Kurt%2520Debattista%2520and%2520Thomas%2520Bashford-Rogers%26entry.1292438233%3D%2520%2520The%2520range%2520of%2520real-world%2520scene%2520luminance%2520is%2520larger%2520than%2520the%2520capture%2520capability%250Aof%2520many%2520digital%2520camera%2520sensors%2520which%2520leads%2520to%2520details%2520being%2520lost%2520in%2520captured%250Aimages%252C%2520most%2520typically%2520in%2520bright%2520regions.%2520Inverse%2520tone%2520mapping%2520attempts%2520to%250Aboost%2520these%2520captured%2520Standard%2520Dynamic%2520Range%2520%2528SDR%2529%2520images%2520back%2520to%2520High%2520Dynamic%250ARange%2520%2528HDR%2529%2520by%2520creating%2520a%2520mapping%2520that%2520linearizes%2520the%2520well%2520exposed%2520values%2520from%250Athe%2520SDR%2520image%252C%2520and%2520provides%2520a%2520luminance%2520boost%2520to%2520the%2520clipped%2520content.%2520However%252C%250Ain%2520most%2520cases%252C%2520the%2520details%2520in%2520the%2520clipped%2520regions%2520cannot%2520be%2520recovered%2520or%250Aestimated.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520inverse%2520tone%2520mapping%2520approach%2520for%250Amapping%2520SDR%2520images%2520to%2520HDR%2520that%2520generates%2520lost%2520details%2520in%2520clipped%2520regions%250Athrough%2520a%2520semantic-aware%2520diffusion%2520based%2520inpainting%2520approach.%2520Our%2520method%250Aproposes%2520two%2520major%2520contributions%2520-%2520first%252C%2520we%2520propose%2520to%2520use%2520a%2520semantic%2520graph%2520to%250Aguide%2520SDR%2520diffusion%2520based%2520inpainting%2520in%2520masked%2520regions%2520in%2520a%2520saturated%2520image.%250ASecond%252C%2520drawing%2520inspiration%2520from%2520traditional%2520HDR%2520imaging%2520and%2520bracketing%250Amethods%252C%2520we%2520propose%2520a%2520principled%2520formulation%2520to%2520lift%2520the%2520SDR%2520inpainted%2520regions%250Ato%2520HDR%2520that%2520is%2520compatible%2520with%2520generative%2520inpainting%2520methods.%2520Results%2520show%2520that%250Aour%2520method%2520demonstrates%2520superior%2520performance%2520across%2520different%2520datasets%2520on%250Aobjective%2520metrics%252C%2520and%2520subjective%2520experiments%2520show%2520that%2520the%2520proposed%2520method%250Amatches%2520%2528and%2520in%2520most%2520cases%2520outperforms%2529%2520state-of-art%2520inverse%2520tone%2520mapping%250Aoperators%2520in%2520terms%2520of%2520objective%2520metrics%2520and%2520outperforms%2520them%2520for%2520visual%250Afidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Aware%20Diffusion%20Inverse%20Tone%20Mapping&entry.906535625=Abhishek%20Goswami%20and%20Aru%20Ranjan%20Singh%20and%20Francesco%20Banterle%20and%20Kurt%20Debattista%20and%20Thomas%20Bashford-Rogers&entry.1292438233=%20%20The%20range%20of%20real-world%20scene%20luminance%20is%20larger%20than%20the%20capture%20capability%0Aof%20many%20digital%20camera%20sensors%20which%20leads%20to%20details%20being%20lost%20in%20captured%0Aimages%2C%20most%20typically%20in%20bright%20regions.%20Inverse%20tone%20mapping%20attempts%20to%0Aboost%20these%20captured%20Standard%20Dynamic%20Range%20%28SDR%29%20images%20back%20to%20High%20Dynamic%0ARange%20%28HDR%29%20by%20creating%20a%20mapping%20that%20linearizes%20the%20well%20exposed%20values%20from%0Athe%20SDR%20image%2C%20and%20provides%20a%20luminance%20boost%20to%20the%20clipped%20content.%20However%2C%0Ain%20most%20cases%2C%20the%20details%20in%20the%20clipped%20regions%20cannot%20be%20recovered%20or%0Aestimated.%20In%20this%20paper%2C%20we%20present%20a%20novel%20inverse%20tone%20mapping%20approach%20for%0Amapping%20SDR%20images%20to%20HDR%20that%20generates%20lost%20details%20in%20clipped%20regions%0Athrough%20a%20semantic-aware%20diffusion%20based%20inpainting%20approach.%20Our%20method%0Aproposes%20two%20major%20contributions%20-%20first%2C%20we%20propose%20to%20use%20a%20semantic%20graph%20to%0Aguide%20SDR%20diffusion%20based%20inpainting%20in%20masked%20regions%20in%20a%20saturated%20image.%0ASecond%2C%20drawing%20inspiration%20from%20traditional%20HDR%20imaging%20and%20bracketing%0Amethods%2C%20we%20propose%20a%20principled%20formulation%20to%20lift%20the%20SDR%20inpainted%20regions%0Ato%20HDR%20that%20is%20compatible%20with%20generative%20inpainting%20methods.%20Results%20show%20that%0Aour%20method%20demonstrates%20superior%20performance%20across%20different%20datasets%20on%0Aobjective%20metrics%2C%20and%20subjective%20experiments%20show%20that%20the%20proposed%20method%0Amatches%20%28and%20in%20most%20cases%20outperforms%29%20state-of-art%20inverse%20tone%20mapping%0Aoperators%20in%20terms%20of%20objective%20metrics%20and%20outperforms%20them%20for%20visual%0Afidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15468v1&entry.124074799=Read"},
{"title": "UNION: Unsupervised 3D Object Detection using Object Appearance-based\n  Pseudo-Classes", "author": "Ted Lentsch and Holger Caesar and Dariu M. Gavrila", "abstract": "  Unsupervised 3D object detection methods have emerged to leverage vast\namounts of data efficiently without requiring manual labels for training.\nRecent approaches rely on dynamic objects for learning to detect objects but\npenalize the detections of static instances during training. Multiple rounds of\n(self) training are used in which detected static instances are added to the\nset of training targets; this procedure to improve performance is\ncomputationally expensive. To address this, we propose the method UNION. We use\nspatial clustering and self-supervised scene flow to obtain a set of static and\ndynamic object proposals from LiDAR. Subsequently, object proposals' visual\nappearances are encoded to distinguish static objects in the foreground and\nbackground by selecting static instances that are visually similar to dynamic\nobjects. As a result, static and dynamic foreground objects are obtained\ntogether, and existing detectors can be trained with a single training. In\naddition, we extend 3D object discovery to detection by using object\nappearance-based cluster labels as pseudo-class labels for training object\nclassification. We conduct extensive experiments on the nuScenes dataset and\nincrease the state-of-the-art performance for unsupervised object discovery,\ni.e. UNION more than doubles the average precision to 33.9. The code will be\nmade publicly available.\n", "link": "http://arxiv.org/abs/2405.15688v1", "date": "2024-05-24", "relevancy": 2.2614, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5888}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5679}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNION%3A%20Unsupervised%203D%20Object%20Detection%20using%20Object%20Appearance-based%0A%20%20Pseudo-Classes&body=Title%3A%20UNION%3A%20Unsupervised%203D%20Object%20Detection%20using%20Object%20Appearance-based%0A%20%20Pseudo-Classes%0AAuthor%3A%20Ted%20Lentsch%20and%20Holger%20Caesar%20and%20Dariu%20M.%20Gavrila%0AAbstract%3A%20%20%20Unsupervised%203D%20object%20detection%20methods%20have%20emerged%20to%20leverage%20vast%0Aamounts%20of%20data%20efficiently%20without%20requiring%20manual%20labels%20for%20training.%0ARecent%20approaches%20rely%20on%20dynamic%20objects%20for%20learning%20to%20detect%20objects%20but%0Apenalize%20the%20detections%20of%20static%20instances%20during%20training.%20Multiple%20rounds%20of%0A%28self%29%20training%20are%20used%20in%20which%20detected%20static%20instances%20are%20added%20to%20the%0Aset%20of%20training%20targets%3B%20this%20procedure%20to%20improve%20performance%20is%0Acomputationally%20expensive.%20To%20address%20this%2C%20we%20propose%20the%20method%20UNION.%20We%20use%0Aspatial%20clustering%20and%20self-supervised%20scene%20flow%20to%20obtain%20a%20set%20of%20static%20and%0Adynamic%20object%20proposals%20from%20LiDAR.%20Subsequently%2C%20object%20proposals%27%20visual%0Aappearances%20are%20encoded%20to%20distinguish%20static%20objects%20in%20the%20foreground%20and%0Abackground%20by%20selecting%20static%20instances%20that%20are%20visually%20similar%20to%20dynamic%0Aobjects.%20As%20a%20result%2C%20static%20and%20dynamic%20foreground%20objects%20are%20obtained%0Atogether%2C%20and%20existing%20detectors%20can%20be%20trained%20with%20a%20single%20training.%20In%0Aaddition%2C%20we%20extend%203D%20object%20discovery%20to%20detection%20by%20using%20object%0Aappearance-based%20cluster%20labels%20as%20pseudo-class%20labels%20for%20training%20object%0Aclassification.%20We%20conduct%20extensive%20experiments%20on%20the%20nuScenes%20dataset%20and%0Aincrease%20the%20state-of-the-art%20performance%20for%20unsupervised%20object%20discovery%2C%0Ai.e.%20UNION%20more%20than%20doubles%20the%20average%20precision%20to%2033.9.%20The%20code%20will%20be%0Amade%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNION%253A%2520Unsupervised%25203D%2520Object%2520Detection%2520using%2520Object%2520Appearance-based%250A%2520%2520Pseudo-Classes%26entry.906535625%3DTed%2520Lentsch%2520and%2520Holger%2520Caesar%2520and%2520Dariu%2520M.%2520Gavrila%26entry.1292438233%3D%2520%2520Unsupervised%25203D%2520object%2520detection%2520methods%2520have%2520emerged%2520to%2520leverage%2520vast%250Aamounts%2520of%2520data%2520efficiently%2520without%2520requiring%2520manual%2520labels%2520for%2520training.%250ARecent%2520approaches%2520rely%2520on%2520dynamic%2520objects%2520for%2520learning%2520to%2520detect%2520objects%2520but%250Apenalize%2520the%2520detections%2520of%2520static%2520instances%2520during%2520training.%2520Multiple%2520rounds%2520of%250A%2528self%2529%2520training%2520are%2520used%2520in%2520which%2520detected%2520static%2520instances%2520are%2520added%2520to%2520the%250Aset%2520of%2520training%2520targets%253B%2520this%2520procedure%2520to%2520improve%2520performance%2520is%250Acomputationally%2520expensive.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520method%2520UNION.%2520We%2520use%250Aspatial%2520clustering%2520and%2520self-supervised%2520scene%2520flow%2520to%2520obtain%2520a%2520set%2520of%2520static%2520and%250Adynamic%2520object%2520proposals%2520from%2520LiDAR.%2520Subsequently%252C%2520object%2520proposals%2527%2520visual%250Aappearances%2520are%2520encoded%2520to%2520distinguish%2520static%2520objects%2520in%2520the%2520foreground%2520and%250Abackground%2520by%2520selecting%2520static%2520instances%2520that%2520are%2520visually%2520similar%2520to%2520dynamic%250Aobjects.%2520As%2520a%2520result%252C%2520static%2520and%2520dynamic%2520foreground%2520objects%2520are%2520obtained%250Atogether%252C%2520and%2520existing%2520detectors%2520can%2520be%2520trained%2520with%2520a%2520single%2520training.%2520In%250Aaddition%252C%2520we%2520extend%25203D%2520object%2520discovery%2520to%2520detection%2520by%2520using%2520object%250Aappearance-based%2520cluster%2520labels%2520as%2520pseudo-class%2520labels%2520for%2520training%2520object%250Aclassification.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520the%2520nuScenes%2520dataset%2520and%250Aincrease%2520the%2520state-of-the-art%2520performance%2520for%2520unsupervised%2520object%2520discovery%252C%250Ai.e.%2520UNION%2520more%2520than%2520doubles%2520the%2520average%2520precision%2520to%252033.9.%2520The%2520code%2520will%2520be%250Amade%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNION%3A%20Unsupervised%203D%20Object%20Detection%20using%20Object%20Appearance-based%0A%20%20Pseudo-Classes&entry.906535625=Ted%20Lentsch%20and%20Holger%20Caesar%20and%20Dariu%20M.%20Gavrila&entry.1292438233=%20%20Unsupervised%203D%20object%20detection%20methods%20have%20emerged%20to%20leverage%20vast%0Aamounts%20of%20data%20efficiently%20without%20requiring%20manual%20labels%20for%20training.%0ARecent%20approaches%20rely%20on%20dynamic%20objects%20for%20learning%20to%20detect%20objects%20but%0Apenalize%20the%20detections%20of%20static%20instances%20during%20training.%20Multiple%20rounds%20of%0A%28self%29%20training%20are%20used%20in%20which%20detected%20static%20instances%20are%20added%20to%20the%0Aset%20of%20training%20targets%3B%20this%20procedure%20to%20improve%20performance%20is%0Acomputationally%20expensive.%20To%20address%20this%2C%20we%20propose%20the%20method%20UNION.%20We%20use%0Aspatial%20clustering%20and%20self-supervised%20scene%20flow%20to%20obtain%20a%20set%20of%20static%20and%0Adynamic%20object%20proposals%20from%20LiDAR.%20Subsequently%2C%20object%20proposals%27%20visual%0Aappearances%20are%20encoded%20to%20distinguish%20static%20objects%20in%20the%20foreground%20and%0Abackground%20by%20selecting%20static%20instances%20that%20are%20visually%20similar%20to%20dynamic%0Aobjects.%20As%20a%20result%2C%20static%20and%20dynamic%20foreground%20objects%20are%20obtained%0Atogether%2C%20and%20existing%20detectors%20can%20be%20trained%20with%20a%20single%20training.%20In%0Aaddition%2C%20we%20extend%203D%20object%20discovery%20to%20detection%20by%20using%20object%0Aappearance-based%20cluster%20labels%20as%20pseudo-class%20labels%20for%20training%20object%0Aclassification.%20We%20conduct%20extensive%20experiments%20on%20the%20nuScenes%20dataset%20and%0Aincrease%20the%20state-of-the-art%20performance%20for%20unsupervised%20object%20discovery%2C%0Ai.e.%20UNION%20more%20than%20doubles%20the%20average%20precision%20to%2033.9.%20The%20code%20will%20be%0Amade%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15688v1&entry.124074799=Read"},
{"title": "Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation", "author": "Mathis Petrovich and Or Litany and Umar Iqbal and Michael J. Black and G\u00fcl Varol and Xue Bin Peng and Davis Rempe", "abstract": "  Recent advances in generative modeling have led to promising progress on\nsynthesizing 3D human motion from text, with methods that can generate\ncharacter animations from short prompts and specified durations. However, using\na single text prompt as input lacks the fine-grained control needed by\nanimators, such as composing multiple actions and defining precise durations\nfor parts of the motion. To address this, we introduce the new problem of\ntimeline control for text-driven motion synthesis, which provides an intuitive,\nyet fine-grained, input interface for users. Instead of a single prompt, users\ncan specify a multi-track timeline of multiple prompts organized in temporal\nintervals that may overlap. This enables specifying the exact timings of each\naction and composing multiple actions in sequence or at overlapping intervals.\nTo generate composite animations from a multi-track timeline, we propose a new\ntest-time denoising method. This method can be integrated with any pre-trained\nmotion diffusion model to synthesize realistic motions that accurately reflect\nthe timeline. At every step of denoising, our method processes each timeline\ninterval (text prompt) individually, subsequently aggregating the predictions\nwith consideration for the specific body parts engaged in each action.\nExperimental comparisons and ablations validate that our method produces\nrealistic motions that respect the semantics and timing of given text prompts.\nOur code and models are publicly available at https://mathis.petrovich.fr/stmc.\n", "link": "http://arxiv.org/abs/2401.08559v2", "date": "2024-05-24", "relevancy": 2.2552, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6117}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Track%20Timeline%20Control%20for%20Text-Driven%203D%20Human%20Motion%20Generation&body=Title%3A%20Multi-Track%20Timeline%20Control%20for%20Text-Driven%203D%20Human%20Motion%20Generation%0AAuthor%3A%20Mathis%20Petrovich%20and%20Or%20Litany%20and%20Umar%20Iqbal%20and%20Michael%20J.%20Black%20and%20G%C3%BCl%20Varol%20and%20Xue%20Bin%20Peng%20and%20Davis%20Rempe%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20modeling%20have%20led%20to%20promising%20progress%20on%0Asynthesizing%203D%20human%20motion%20from%20text%2C%20with%20methods%20that%20can%20generate%0Acharacter%20animations%20from%20short%20prompts%20and%20specified%20durations.%20However%2C%20using%0Aa%20single%20text%20prompt%20as%20input%20lacks%20the%20fine-grained%20control%20needed%20by%0Aanimators%2C%20such%20as%20composing%20multiple%20actions%20and%20defining%20precise%20durations%0Afor%20parts%20of%20the%20motion.%20To%20address%20this%2C%20we%20introduce%20the%20new%20problem%20of%0Atimeline%20control%20for%20text-driven%20motion%20synthesis%2C%20which%20provides%20an%20intuitive%2C%0Ayet%20fine-grained%2C%20input%20interface%20for%20users.%20Instead%20of%20a%20single%20prompt%2C%20users%0Acan%20specify%20a%20multi-track%20timeline%20of%20multiple%20prompts%20organized%20in%20temporal%0Aintervals%20that%20may%20overlap.%20This%20enables%20specifying%20the%20exact%20timings%20of%20each%0Aaction%20and%20composing%20multiple%20actions%20in%20sequence%20or%20at%20overlapping%20intervals.%0ATo%20generate%20composite%20animations%20from%20a%20multi-track%20timeline%2C%20we%20propose%20a%20new%0Atest-time%20denoising%20method.%20This%20method%20can%20be%20integrated%20with%20any%20pre-trained%0Amotion%20diffusion%20model%20to%20synthesize%20realistic%20motions%20that%20accurately%20reflect%0Athe%20timeline.%20At%20every%20step%20of%20denoising%2C%20our%20method%20processes%20each%20timeline%0Ainterval%20%28text%20prompt%29%20individually%2C%20subsequently%20aggregating%20the%20predictions%0Awith%20consideration%20for%20the%20specific%20body%20parts%20engaged%20in%20each%20action.%0AExperimental%20comparisons%20and%20ablations%20validate%20that%20our%20method%20produces%0Arealistic%20motions%20that%20respect%20the%20semantics%20and%20timing%20of%20given%20text%20prompts.%0AOur%20code%20and%20models%20are%20publicly%20available%20at%20https%3A//mathis.petrovich.fr/stmc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Track%2520Timeline%2520Control%2520for%2520Text-Driven%25203D%2520Human%2520Motion%2520Generation%26entry.906535625%3DMathis%2520Petrovich%2520and%2520Or%2520Litany%2520and%2520Umar%2520Iqbal%2520and%2520Michael%2520J.%2520Black%2520and%2520G%25C3%25BCl%2520Varol%2520and%2520Xue%2520Bin%2520Peng%2520and%2520Davis%2520Rempe%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520modeling%2520have%2520led%2520to%2520promising%2520progress%2520on%250Asynthesizing%25203D%2520human%2520motion%2520from%2520text%252C%2520with%2520methods%2520that%2520can%2520generate%250Acharacter%2520animations%2520from%2520short%2520prompts%2520and%2520specified%2520durations.%2520However%252C%2520using%250Aa%2520single%2520text%2520prompt%2520as%2520input%2520lacks%2520the%2520fine-grained%2520control%2520needed%2520by%250Aanimators%252C%2520such%2520as%2520composing%2520multiple%2520actions%2520and%2520defining%2520precise%2520durations%250Afor%2520parts%2520of%2520the%2520motion.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520new%2520problem%2520of%250Atimeline%2520control%2520for%2520text-driven%2520motion%2520synthesis%252C%2520which%2520provides%2520an%2520intuitive%252C%250Ayet%2520fine-grained%252C%2520input%2520interface%2520for%2520users.%2520Instead%2520of%2520a%2520single%2520prompt%252C%2520users%250Acan%2520specify%2520a%2520multi-track%2520timeline%2520of%2520multiple%2520prompts%2520organized%2520in%2520temporal%250Aintervals%2520that%2520may%2520overlap.%2520This%2520enables%2520specifying%2520the%2520exact%2520timings%2520of%2520each%250Aaction%2520and%2520composing%2520multiple%2520actions%2520in%2520sequence%2520or%2520at%2520overlapping%2520intervals.%250ATo%2520generate%2520composite%2520animations%2520from%2520a%2520multi-track%2520timeline%252C%2520we%2520propose%2520a%2520new%250Atest-time%2520denoising%2520method.%2520This%2520method%2520can%2520be%2520integrated%2520with%2520any%2520pre-trained%250Amotion%2520diffusion%2520model%2520to%2520synthesize%2520realistic%2520motions%2520that%2520accurately%2520reflect%250Athe%2520timeline.%2520At%2520every%2520step%2520of%2520denoising%252C%2520our%2520method%2520processes%2520each%2520timeline%250Ainterval%2520%2528text%2520prompt%2529%2520individually%252C%2520subsequently%2520aggregating%2520the%2520predictions%250Awith%2520consideration%2520for%2520the%2520specific%2520body%2520parts%2520engaged%2520in%2520each%2520action.%250AExperimental%2520comparisons%2520and%2520ablations%2520validate%2520that%2520our%2520method%2520produces%250Arealistic%2520motions%2520that%2520respect%2520the%2520semantics%2520and%2520timing%2520of%2520given%2520text%2520prompts.%250AOur%2520code%2520and%2520models%2520are%2520publicly%2520available%2520at%2520https%253A//mathis.petrovich.fr/stmc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Track%20Timeline%20Control%20for%20Text-Driven%203D%20Human%20Motion%20Generation&entry.906535625=Mathis%20Petrovich%20and%20Or%20Litany%20and%20Umar%20Iqbal%20and%20Michael%20J.%20Black%20and%20G%C3%BCl%20Varol%20and%20Xue%20Bin%20Peng%20and%20Davis%20Rempe&entry.1292438233=%20%20Recent%20advances%20in%20generative%20modeling%20have%20led%20to%20promising%20progress%20on%0Asynthesizing%203D%20human%20motion%20from%20text%2C%20with%20methods%20that%20can%20generate%0Acharacter%20animations%20from%20short%20prompts%20and%20specified%20durations.%20However%2C%20using%0Aa%20single%20text%20prompt%20as%20input%20lacks%20the%20fine-grained%20control%20needed%20by%0Aanimators%2C%20such%20as%20composing%20multiple%20actions%20and%20defining%20precise%20durations%0Afor%20parts%20of%20the%20motion.%20To%20address%20this%2C%20we%20introduce%20the%20new%20problem%20of%0Atimeline%20control%20for%20text-driven%20motion%20synthesis%2C%20which%20provides%20an%20intuitive%2C%0Ayet%20fine-grained%2C%20input%20interface%20for%20users.%20Instead%20of%20a%20single%20prompt%2C%20users%0Acan%20specify%20a%20multi-track%20timeline%20of%20multiple%20prompts%20organized%20in%20temporal%0Aintervals%20that%20may%20overlap.%20This%20enables%20specifying%20the%20exact%20timings%20of%20each%0Aaction%20and%20composing%20multiple%20actions%20in%20sequence%20or%20at%20overlapping%20intervals.%0ATo%20generate%20composite%20animations%20from%20a%20multi-track%20timeline%2C%20we%20propose%20a%20new%0Atest-time%20denoising%20method.%20This%20method%20can%20be%20integrated%20with%20any%20pre-trained%0Amotion%20diffusion%20model%20to%20synthesize%20realistic%20motions%20that%20accurately%20reflect%0Athe%20timeline.%20At%20every%20step%20of%20denoising%2C%20our%20method%20processes%20each%20timeline%0Ainterval%20%28text%20prompt%29%20individually%2C%20subsequently%20aggregating%20the%20predictions%0Awith%20consideration%20for%20the%20specific%20body%20parts%20engaged%20in%20each%20action.%0AExperimental%20comparisons%20and%20ablations%20validate%20that%20our%20method%20produces%0Arealistic%20motions%20that%20respect%20the%20semantics%20and%20timing%20of%20given%20text%20prompts.%0AOur%20code%20and%20models%20are%20publicly%20available%20at%20https%3A//mathis.petrovich.fr/stmc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08559v2&entry.124074799=Read"},
{"title": "Nonlinear denoising score matching for enhanced learning of structured\n  distributions", "author": "Jeremiah Birrell and Markos A. Katsoulakis and Luc Rey-Bellet and Benjamin Zhang and Wei Zhu", "abstract": "  We present a novel method for training score-based generative models which\nuses nonlinear noising dynamics to improve learning of structured\ndistributions. Generalizing to a nonlinear drift allows for additional\nstructure to be incorporated into the dynamics, thus making the training better\nadapted to the data, e.g., in the case of multimodality or (approximate)\nsymmetries. Such structure can be obtained from the data by an inexpensive\npreprocessing step. The nonlinear dynamics introduces new challenges into\ntraining which we address in two ways: 1) we develop a new nonlinear denoising\nscore matching (NDSM) method, 2) we introduce neural control variates in order\nto reduce the variance of the NDSM training objective. We demonstrate the\neffectiveness of this method on several examples: a) a collection of\nlow-dimensional examples, motivated by clustering in latent space, b)\nhigh-dimensional images, addressing issues with mode collapse, small training\nsets, and approximate symmetries, the latter being a challenge for methods\nbased on equivariant neural networks, which require exact symmetries.\n", "link": "http://arxiv.org/abs/2405.15625v1", "date": "2024-05-24", "relevancy": 2.2414, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5757}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5525}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20denoising%20score%20matching%20for%20enhanced%20learning%20of%20structured%0A%20%20distributions&body=Title%3A%20Nonlinear%20denoising%20score%20matching%20for%20enhanced%20learning%20of%20structured%0A%20%20distributions%0AAuthor%3A%20Jeremiah%20Birrell%20and%20Markos%20A.%20Katsoulakis%20and%20Luc%20Rey-Bellet%20and%20Benjamin%20Zhang%20and%20Wei%20Zhu%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20for%20training%20score-based%20generative%20models%20which%0Auses%20nonlinear%20noising%20dynamics%20to%20improve%20learning%20of%20structured%0Adistributions.%20Generalizing%20to%20a%20nonlinear%20drift%20allows%20for%20additional%0Astructure%20to%20be%20incorporated%20into%20the%20dynamics%2C%20thus%20making%20the%20training%20better%0Aadapted%20to%20the%20data%2C%20e.g.%2C%20in%20the%20case%20of%20multimodality%20or%20%28approximate%29%0Asymmetries.%20Such%20structure%20can%20be%20obtained%20from%20the%20data%20by%20an%20inexpensive%0Apreprocessing%20step.%20The%20nonlinear%20dynamics%20introduces%20new%20challenges%20into%0Atraining%20which%20we%20address%20in%20two%20ways%3A%201%29%20we%20develop%20a%20new%20nonlinear%20denoising%0Ascore%20matching%20%28NDSM%29%20method%2C%202%29%20we%20introduce%20neural%20control%20variates%20in%20order%0Ato%20reduce%20the%20variance%20of%20the%20NDSM%20training%20objective.%20We%20demonstrate%20the%0Aeffectiveness%20of%20this%20method%20on%20several%20examples%3A%20a%29%20a%20collection%20of%0Alow-dimensional%20examples%2C%20motivated%20by%20clustering%20in%20latent%20space%2C%20b%29%0Ahigh-dimensional%20images%2C%20addressing%20issues%20with%20mode%20collapse%2C%20small%20training%0Asets%2C%20and%20approximate%20symmetries%2C%20the%20latter%20being%20a%20challenge%20for%20methods%0Abased%20on%20equivariant%20neural%20networks%2C%20which%20require%20exact%20symmetries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonlinear%2520denoising%2520score%2520matching%2520for%2520enhanced%2520learning%2520of%2520structured%250A%2520%2520distributions%26entry.906535625%3DJeremiah%2520Birrell%2520and%2520Markos%2520A.%2520Katsoulakis%2520and%2520Luc%2520Rey-Bellet%2520and%2520Benjamin%2520Zhang%2520and%2520Wei%2520Zhu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520method%2520for%2520training%2520score-based%2520generative%2520models%2520which%250Auses%2520nonlinear%2520noising%2520dynamics%2520to%2520improve%2520learning%2520of%2520structured%250Adistributions.%2520Generalizing%2520to%2520a%2520nonlinear%2520drift%2520allows%2520for%2520additional%250Astructure%2520to%2520be%2520incorporated%2520into%2520the%2520dynamics%252C%2520thus%2520making%2520the%2520training%2520better%250Aadapted%2520to%2520the%2520data%252C%2520e.g.%252C%2520in%2520the%2520case%2520of%2520multimodality%2520or%2520%2528approximate%2529%250Asymmetries.%2520Such%2520structure%2520can%2520be%2520obtained%2520from%2520the%2520data%2520by%2520an%2520inexpensive%250Apreprocessing%2520step.%2520The%2520nonlinear%2520dynamics%2520introduces%2520new%2520challenges%2520into%250Atraining%2520which%2520we%2520address%2520in%2520two%2520ways%253A%25201%2529%2520we%2520develop%2520a%2520new%2520nonlinear%2520denoising%250Ascore%2520matching%2520%2528NDSM%2529%2520method%252C%25202%2529%2520we%2520introduce%2520neural%2520control%2520variates%2520in%2520order%250Ato%2520reduce%2520the%2520variance%2520of%2520the%2520NDSM%2520training%2520objective.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520this%2520method%2520on%2520several%2520examples%253A%2520a%2529%2520a%2520collection%2520of%250Alow-dimensional%2520examples%252C%2520motivated%2520by%2520clustering%2520in%2520latent%2520space%252C%2520b%2529%250Ahigh-dimensional%2520images%252C%2520addressing%2520issues%2520with%2520mode%2520collapse%252C%2520small%2520training%250Asets%252C%2520and%2520approximate%2520symmetries%252C%2520the%2520latter%2520being%2520a%2520challenge%2520for%2520methods%250Abased%2520on%2520equivariant%2520neural%2520networks%252C%2520which%2520require%2520exact%2520symmetries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20denoising%20score%20matching%20for%20enhanced%20learning%20of%20structured%0A%20%20distributions&entry.906535625=Jeremiah%20Birrell%20and%20Markos%20A.%20Katsoulakis%20and%20Luc%20Rey-Bellet%20and%20Benjamin%20Zhang%20and%20Wei%20Zhu&entry.1292438233=%20%20We%20present%20a%20novel%20method%20for%20training%20score-based%20generative%20models%20which%0Auses%20nonlinear%20noising%20dynamics%20to%20improve%20learning%20of%20structured%0Adistributions.%20Generalizing%20to%20a%20nonlinear%20drift%20allows%20for%20additional%0Astructure%20to%20be%20incorporated%20into%20the%20dynamics%2C%20thus%20making%20the%20training%20better%0Aadapted%20to%20the%20data%2C%20e.g.%2C%20in%20the%20case%20of%20multimodality%20or%20%28approximate%29%0Asymmetries.%20Such%20structure%20can%20be%20obtained%20from%20the%20data%20by%20an%20inexpensive%0Apreprocessing%20step.%20The%20nonlinear%20dynamics%20introduces%20new%20challenges%20into%0Atraining%20which%20we%20address%20in%20two%20ways%3A%201%29%20we%20develop%20a%20new%20nonlinear%20denoising%0Ascore%20matching%20%28NDSM%29%20method%2C%202%29%20we%20introduce%20neural%20control%20variates%20in%20order%0Ato%20reduce%20the%20variance%20of%20the%20NDSM%20training%20objective.%20We%20demonstrate%20the%0Aeffectiveness%20of%20this%20method%20on%20several%20examples%3A%20a%29%20a%20collection%20of%0Alow-dimensional%20examples%2C%20motivated%20by%20clustering%20in%20latent%20space%2C%20b%29%0Ahigh-dimensional%20images%2C%20addressing%20issues%20with%20mode%20collapse%2C%20small%20training%0Asets%2C%20and%20approximate%20symmetries%2C%20the%20latter%20being%20a%20challenge%20for%20methods%0Abased%20on%20equivariant%20neural%20networks%2C%20which%20require%20exact%20symmetries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15625v1&entry.124074799=Read"},
{"title": "Airship Formations for Animal Motion Capture and Behavior Analysis", "author": "Eric Price and Aamir Ahmad", "abstract": "  Using UAVs for wildlife observation and motion capture offers manifold\nadvantages for studying animals in the wild, especially grazing herds in open\nterrain. The aerial perspective allows observation at a scale and depth that is\nnot possible on the ground, offering new insights into group behavior. However,\nthe very nature of wildlife field-studies puts traditional fixed wing and\nmulti-copter systems to their limits: limited flight time, noise and safety\naspects affect their efficacy, where lighter than air systems can remain on\nstation for many hours. Nevertheless, airships are challenging from a ground\nhandling perspective as well as from a control point of view, being voluminous\nand highly affected by wind. In this work, we showcase a system designed to use\nairship formations to track, follow, and visually record wild horses from\nmultiple angles, including airship design, simulation, control, on board\ncomputer vision, autonomous operation and practical aspects of field\nexperiments.\n", "link": "http://arxiv.org/abs/2404.08986v2", "date": "2024-05-24", "relevancy": 2.2245, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4619}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4516}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Airship%20Formations%20for%20Animal%20Motion%20Capture%20and%20Behavior%20Analysis&body=Title%3A%20Airship%20Formations%20for%20Animal%20Motion%20Capture%20and%20Behavior%20Analysis%0AAuthor%3A%20Eric%20Price%20and%20Aamir%20Ahmad%0AAbstract%3A%20%20%20Using%20UAVs%20for%20wildlife%20observation%20and%20motion%20capture%20offers%20manifold%0Aadvantages%20for%20studying%20animals%20in%20the%20wild%2C%20especially%20grazing%20herds%20in%20open%0Aterrain.%20The%20aerial%20perspective%20allows%20observation%20at%20a%20scale%20and%20depth%20that%20is%0Anot%20possible%20on%20the%20ground%2C%20offering%20new%20insights%20into%20group%20behavior.%20However%2C%0Athe%20very%20nature%20of%20wildlife%20field-studies%20puts%20traditional%20fixed%20wing%20and%0Amulti-copter%20systems%20to%20their%20limits%3A%20limited%20flight%20time%2C%20noise%20and%20safety%0Aaspects%20affect%20their%20efficacy%2C%20where%20lighter%20than%20air%20systems%20can%20remain%20on%0Astation%20for%20many%20hours.%20Nevertheless%2C%20airships%20are%20challenging%20from%20a%20ground%0Ahandling%20perspective%20as%20well%20as%20from%20a%20control%20point%20of%20view%2C%20being%20voluminous%0Aand%20highly%20affected%20by%20wind.%20In%20this%20work%2C%20we%20showcase%20a%20system%20designed%20to%20use%0Aairship%20formations%20to%20track%2C%20follow%2C%20and%20visually%20record%20wild%20horses%20from%0Amultiple%20angles%2C%20including%20airship%20design%2C%20simulation%2C%20control%2C%20on%20board%0Acomputer%20vision%2C%20autonomous%20operation%20and%20practical%20aspects%20of%20field%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08986v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirship%2520Formations%2520for%2520Animal%2520Motion%2520Capture%2520and%2520Behavior%2520Analysis%26entry.906535625%3DEric%2520Price%2520and%2520Aamir%2520Ahmad%26entry.1292438233%3D%2520%2520Using%2520UAVs%2520for%2520wildlife%2520observation%2520and%2520motion%2520capture%2520offers%2520manifold%250Aadvantages%2520for%2520studying%2520animals%2520in%2520the%2520wild%252C%2520especially%2520grazing%2520herds%2520in%2520open%250Aterrain.%2520The%2520aerial%2520perspective%2520allows%2520observation%2520at%2520a%2520scale%2520and%2520depth%2520that%2520is%250Anot%2520possible%2520on%2520the%2520ground%252C%2520offering%2520new%2520insights%2520into%2520group%2520behavior.%2520However%252C%250Athe%2520very%2520nature%2520of%2520wildlife%2520field-studies%2520puts%2520traditional%2520fixed%2520wing%2520and%250Amulti-copter%2520systems%2520to%2520their%2520limits%253A%2520limited%2520flight%2520time%252C%2520noise%2520and%2520safety%250Aaspects%2520affect%2520their%2520efficacy%252C%2520where%2520lighter%2520than%2520air%2520systems%2520can%2520remain%2520on%250Astation%2520for%2520many%2520hours.%2520Nevertheless%252C%2520airships%2520are%2520challenging%2520from%2520a%2520ground%250Ahandling%2520perspective%2520as%2520well%2520as%2520from%2520a%2520control%2520point%2520of%2520view%252C%2520being%2520voluminous%250Aand%2520highly%2520affected%2520by%2520wind.%2520In%2520this%2520work%252C%2520we%2520showcase%2520a%2520system%2520designed%2520to%2520use%250Aairship%2520formations%2520to%2520track%252C%2520follow%252C%2520and%2520visually%2520record%2520wild%2520horses%2520from%250Amultiple%2520angles%252C%2520including%2520airship%2520design%252C%2520simulation%252C%2520control%252C%2520on%2520board%250Acomputer%2520vision%252C%2520autonomous%2520operation%2520and%2520practical%2520aspects%2520of%2520field%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08986v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Airship%20Formations%20for%20Animal%20Motion%20Capture%20and%20Behavior%20Analysis&entry.906535625=Eric%20Price%20and%20Aamir%20Ahmad&entry.1292438233=%20%20Using%20UAVs%20for%20wildlife%20observation%20and%20motion%20capture%20offers%20manifold%0Aadvantages%20for%20studying%20animals%20in%20the%20wild%2C%20especially%20grazing%20herds%20in%20open%0Aterrain.%20The%20aerial%20perspective%20allows%20observation%20at%20a%20scale%20and%20depth%20that%20is%0Anot%20possible%20on%20the%20ground%2C%20offering%20new%20insights%20into%20group%20behavior.%20However%2C%0Athe%20very%20nature%20of%20wildlife%20field-studies%20puts%20traditional%20fixed%20wing%20and%0Amulti-copter%20systems%20to%20their%20limits%3A%20limited%20flight%20time%2C%20noise%20and%20safety%0Aaspects%20affect%20their%20efficacy%2C%20where%20lighter%20than%20air%20systems%20can%20remain%20on%0Astation%20for%20many%20hours.%20Nevertheless%2C%20airships%20are%20challenging%20from%20a%20ground%0Ahandling%20perspective%20as%20well%20as%20from%20a%20control%20point%20of%20view%2C%20being%20voluminous%0Aand%20highly%20affected%20by%20wind.%20In%20this%20work%2C%20we%20showcase%20a%20system%20designed%20to%20use%0Aairship%20formations%20to%20track%2C%20follow%2C%20and%20visually%20record%20wild%20horses%20from%0Amultiple%20angles%2C%20including%20airship%20design%2C%20simulation%2C%20control%2C%20on%20board%0Acomputer%20vision%2C%20autonomous%20operation%20and%20practical%20aspects%20of%20field%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08986v2&entry.124074799=Read"},
{"title": "SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet\n  Variational Autoencoder for Hyperspectral Pixel Unmixing", "author": "Soham Chitnis and Kiran Mantripragada and Faisal Z. Qureshi", "abstract": "  The hyperspectral pixel unmixing aims to find the underlying materials\n(endmembers) and their proportions (abundances) in pixels of a hyperspectral\nimage. This work extends the Latent Dirichlet Variational Autoencoder (LDVAE)\npixel unmixing scheme by taking into account local spatial context while\nperforming pixel unmixing. The proposed method uses an isotropic convolutional\nneural network with spatial attention to encode pixels as a dirichlet\ndistribution over endmembers. We have evaluated our model on Samson, Hydice\nUrban, Cuprite, and OnTech-HSI-Syn-21 datasets. Our model also leverages the\ntransfer learning paradigm for Cuprite Dataset, where we train the model on\nsynthetic data and evaluate it on the real-world data. The results suggest that\nincorporating spatial context improves both endmember extraction and abundance\nestimation.\n", "link": "http://arxiv.org/abs/2311.10701v2", "date": "2024-05-24", "relevancy": 2.2053, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.554}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5503}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpACNN-LDVAE%3A%20Spatial%20Attention%20Convolutional%20Latent%20Dirichlet%0A%20%20Variational%20Autoencoder%20for%20Hyperspectral%20Pixel%20Unmixing&body=Title%3A%20SpACNN-LDVAE%3A%20Spatial%20Attention%20Convolutional%20Latent%20Dirichlet%0A%20%20Variational%20Autoencoder%20for%20Hyperspectral%20Pixel%20Unmixing%0AAuthor%3A%20Soham%20Chitnis%20and%20Kiran%20Mantripragada%20and%20Faisal%20Z.%20Qureshi%0AAbstract%3A%20%20%20The%20hyperspectral%20pixel%20unmixing%20aims%20to%20find%20the%20underlying%20materials%0A%28endmembers%29%20and%20their%20proportions%20%28abundances%29%20in%20pixels%20of%20a%20hyperspectral%0Aimage.%20This%20work%20extends%20the%20Latent%20Dirichlet%20Variational%20Autoencoder%20%28LDVAE%29%0Apixel%20unmixing%20scheme%20by%20taking%20into%20account%20local%20spatial%20context%20while%0Aperforming%20pixel%20unmixing.%20The%20proposed%20method%20uses%20an%20isotropic%20convolutional%0Aneural%20network%20with%20spatial%20attention%20to%20encode%20pixels%20as%20a%20dirichlet%0Adistribution%20over%20endmembers.%20We%20have%20evaluated%20our%20model%20on%20Samson%2C%20Hydice%0AUrban%2C%20Cuprite%2C%20and%20OnTech-HSI-Syn-21%20datasets.%20Our%20model%20also%20leverages%20the%0Atransfer%20learning%20paradigm%20for%20Cuprite%20Dataset%2C%20where%20we%20train%20the%20model%20on%0Asynthetic%20data%20and%20evaluate%20it%20on%20the%20real-world%20data.%20The%20results%20suggest%20that%0Aincorporating%20spatial%20context%20improves%20both%20endmember%20extraction%20and%20abundance%0Aestimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpACNN-LDVAE%253A%2520Spatial%2520Attention%2520Convolutional%2520Latent%2520Dirichlet%250A%2520%2520Variational%2520Autoencoder%2520for%2520Hyperspectral%2520Pixel%2520Unmixing%26entry.906535625%3DSoham%2520Chitnis%2520and%2520Kiran%2520Mantripragada%2520and%2520Faisal%2520Z.%2520Qureshi%26entry.1292438233%3D%2520%2520The%2520hyperspectral%2520pixel%2520unmixing%2520aims%2520to%2520find%2520the%2520underlying%2520materials%250A%2528endmembers%2529%2520and%2520their%2520proportions%2520%2528abundances%2529%2520in%2520pixels%2520of%2520a%2520hyperspectral%250Aimage.%2520This%2520work%2520extends%2520the%2520Latent%2520Dirichlet%2520Variational%2520Autoencoder%2520%2528LDVAE%2529%250Apixel%2520unmixing%2520scheme%2520by%2520taking%2520into%2520account%2520local%2520spatial%2520context%2520while%250Aperforming%2520pixel%2520unmixing.%2520The%2520proposed%2520method%2520uses%2520an%2520isotropic%2520convolutional%250Aneural%2520network%2520with%2520spatial%2520attention%2520to%2520encode%2520pixels%2520as%2520a%2520dirichlet%250Adistribution%2520over%2520endmembers.%2520We%2520have%2520evaluated%2520our%2520model%2520on%2520Samson%252C%2520Hydice%250AUrban%252C%2520Cuprite%252C%2520and%2520OnTech-HSI-Syn-21%2520datasets.%2520Our%2520model%2520also%2520leverages%2520the%250Atransfer%2520learning%2520paradigm%2520for%2520Cuprite%2520Dataset%252C%2520where%2520we%2520train%2520the%2520model%2520on%250Asynthetic%2520data%2520and%2520evaluate%2520it%2520on%2520the%2520real-world%2520data.%2520The%2520results%2520suggest%2520that%250Aincorporating%2520spatial%2520context%2520improves%2520both%2520endmember%2520extraction%2520and%2520abundance%250Aestimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpACNN-LDVAE%3A%20Spatial%20Attention%20Convolutional%20Latent%20Dirichlet%0A%20%20Variational%20Autoencoder%20for%20Hyperspectral%20Pixel%20Unmixing&entry.906535625=Soham%20Chitnis%20and%20Kiran%20Mantripragada%20and%20Faisal%20Z.%20Qureshi&entry.1292438233=%20%20The%20hyperspectral%20pixel%20unmixing%20aims%20to%20find%20the%20underlying%20materials%0A%28endmembers%29%20and%20their%20proportions%20%28abundances%29%20in%20pixels%20of%20a%20hyperspectral%0Aimage.%20This%20work%20extends%20the%20Latent%20Dirichlet%20Variational%20Autoencoder%20%28LDVAE%29%0Apixel%20unmixing%20scheme%20by%20taking%20into%20account%20local%20spatial%20context%20while%0Aperforming%20pixel%20unmixing.%20The%20proposed%20method%20uses%20an%20isotropic%20convolutional%0Aneural%20network%20with%20spatial%20attention%20to%20encode%20pixels%20as%20a%20dirichlet%0Adistribution%20over%20endmembers.%20We%20have%20evaluated%20our%20model%20on%20Samson%2C%20Hydice%0AUrban%2C%20Cuprite%2C%20and%20OnTech-HSI-Syn-21%20datasets.%20Our%20model%20also%20leverages%20the%0Atransfer%20learning%20paradigm%20for%20Cuprite%20Dataset%2C%20where%20we%20train%20the%20model%20on%0Asynthetic%20data%20and%20evaluate%20it%20on%20the%20real-world%20data.%20The%20results%20suggest%20that%0Aincorporating%20spatial%20context%20improves%20both%20endmember%20extraction%20and%20abundance%0Aestimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10701v2&entry.124074799=Read"},
{"title": "SMART: Scalable Multi-agent Real-time Simulation via Next-token\n  Prediction", "author": "Wei Wu and Xiaoxin Feng and Ziyan Gao and Yuheng Kan", "abstract": "  Data-driven autonomous driving motion generation tasks are frequently\nimpacted by the limitations of dataset size and the domain gap between\ndatasets, which precludes their extensive application in real-world scenarios.\nTo address this issue, we introduce SMART, a novel autonomous driving motion\ngeneration paradigm that models vectorized map and agent trajectory data into\ndiscrete sequence tokens. These tokens are then processed through a\ndecoder-only transformer architecture to train for the next token prediction\ntask across spatial-temporal series. This GPT-style method allows the model to\nlearn the motion distribution in real driving scenarios. SMART achieves\nstate-of-the-art performance across most of the metrics on the generative Sim\nAgents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset\n(WOMD), demonstrating remarkable inference speed. Moreover, SMART represents\nthe generative model in the autonomous driving motion domain, exhibiting\nzero-shot generalization capabilities: Using only the NuPlan dataset for\ntraining and WOMD for validation, SMART achieved a competitive score of 0.71 on\nthe Sim Agents challenge. Lastly, we have collected over 1 billion motion\ntokens from multiple datasets, validating the model's scalability. These\nresults suggest that SMART has initially emulated two important properties:\nscalability and zero-shot generalization, and preliminarily meets the needs of\nlarge-scale real-time simulation applications. We have released all the code to\npromote the exploration of models for motion generation in the autonomous\ndriving field.\n", "link": "http://arxiv.org/abs/2405.15677v1", "date": "2024-05-24", "relevancy": 2.2047, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5662}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5521}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMART%3A%20Scalable%20Multi-agent%20Real-time%20Simulation%20via%20Next-token%0A%20%20Prediction&body=Title%3A%20SMART%3A%20Scalable%20Multi-agent%20Real-time%20Simulation%20via%20Next-token%0A%20%20Prediction%0AAuthor%3A%20Wei%20Wu%20and%20Xiaoxin%20Feng%20and%20Ziyan%20Gao%20and%20Yuheng%20Kan%0AAbstract%3A%20%20%20Data-driven%20autonomous%20driving%20motion%20generation%20tasks%20are%20frequently%0Aimpacted%20by%20the%20limitations%20of%20dataset%20size%20and%20the%20domain%20gap%20between%0Adatasets%2C%20which%20precludes%20their%20extensive%20application%20in%20real-world%20scenarios.%0ATo%20address%20this%20issue%2C%20we%20introduce%20SMART%2C%20a%20novel%20autonomous%20driving%20motion%0Ageneration%20paradigm%20that%20models%20vectorized%20map%20and%20agent%20trajectory%20data%20into%0Adiscrete%20sequence%20tokens.%20These%20tokens%20are%20then%20processed%20through%20a%0Adecoder-only%20transformer%20architecture%20to%20train%20for%20the%20next%20token%20prediction%0Atask%20across%20spatial-temporal%20series.%20This%20GPT-style%20method%20allows%20the%20model%20to%0Alearn%20the%20motion%20distribution%20in%20real%20driving%20scenarios.%20SMART%20achieves%0Astate-of-the-art%20performance%20across%20most%20of%20the%20metrics%20on%20the%20generative%20Sim%0AAgents%20challenge%2C%20ranking%201st%20on%20the%20leaderboards%20of%20Waymo%20Open%20Motion%20Dataset%0A%28WOMD%29%2C%20demonstrating%20remarkable%20inference%20speed.%20Moreover%2C%20SMART%20represents%0Athe%20generative%20model%20in%20the%20autonomous%20driving%20motion%20domain%2C%20exhibiting%0Azero-shot%20generalization%20capabilities%3A%20Using%20only%20the%20NuPlan%20dataset%20for%0Atraining%20and%20WOMD%20for%20validation%2C%20SMART%20achieved%20a%20competitive%20score%20of%200.71%20on%0Athe%20Sim%20Agents%20challenge.%20Lastly%2C%20we%20have%20collected%20over%201%20billion%20motion%0Atokens%20from%20multiple%20datasets%2C%20validating%20the%20model%27s%20scalability.%20These%0Aresults%20suggest%20that%20SMART%20has%20initially%20emulated%20two%20important%20properties%3A%0Ascalability%20and%20zero-shot%20generalization%2C%20and%20preliminarily%20meets%20the%20needs%20of%0Alarge-scale%20real-time%20simulation%20applications.%20We%20have%20released%20all%20the%20code%20to%0Apromote%20the%20exploration%20of%20models%20for%20motion%20generation%20in%20the%20autonomous%0Adriving%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMART%253A%2520Scalable%2520Multi-agent%2520Real-time%2520Simulation%2520via%2520Next-token%250A%2520%2520Prediction%26entry.906535625%3DWei%2520Wu%2520and%2520Xiaoxin%2520Feng%2520and%2520Ziyan%2520Gao%2520and%2520Yuheng%2520Kan%26entry.1292438233%3D%2520%2520Data-driven%2520autonomous%2520driving%2520motion%2520generation%2520tasks%2520are%2520frequently%250Aimpacted%2520by%2520the%2520limitations%2520of%2520dataset%2520size%2520and%2520the%2520domain%2520gap%2520between%250Adatasets%252C%2520which%2520precludes%2520their%2520extensive%2520application%2520in%2520real-world%2520scenarios.%250ATo%2520address%2520this%2520issue%252C%2520we%2520introduce%2520SMART%252C%2520a%2520novel%2520autonomous%2520driving%2520motion%250Ageneration%2520paradigm%2520that%2520models%2520vectorized%2520map%2520and%2520agent%2520trajectory%2520data%2520into%250Adiscrete%2520sequence%2520tokens.%2520These%2520tokens%2520are%2520then%2520processed%2520through%2520a%250Adecoder-only%2520transformer%2520architecture%2520to%2520train%2520for%2520the%2520next%2520token%2520prediction%250Atask%2520across%2520spatial-temporal%2520series.%2520This%2520GPT-style%2520method%2520allows%2520the%2520model%2520to%250Alearn%2520the%2520motion%2520distribution%2520in%2520real%2520driving%2520scenarios.%2520SMART%2520achieves%250Astate-of-the-art%2520performance%2520across%2520most%2520of%2520the%2520metrics%2520on%2520the%2520generative%2520Sim%250AAgents%2520challenge%252C%2520ranking%25201st%2520on%2520the%2520leaderboards%2520of%2520Waymo%2520Open%2520Motion%2520Dataset%250A%2528WOMD%2529%252C%2520demonstrating%2520remarkable%2520inference%2520speed.%2520Moreover%252C%2520SMART%2520represents%250Athe%2520generative%2520model%2520in%2520the%2520autonomous%2520driving%2520motion%2520domain%252C%2520exhibiting%250Azero-shot%2520generalization%2520capabilities%253A%2520Using%2520only%2520the%2520NuPlan%2520dataset%2520for%250Atraining%2520and%2520WOMD%2520for%2520validation%252C%2520SMART%2520achieved%2520a%2520competitive%2520score%2520of%25200.71%2520on%250Athe%2520Sim%2520Agents%2520challenge.%2520Lastly%252C%2520we%2520have%2520collected%2520over%25201%2520billion%2520motion%250Atokens%2520from%2520multiple%2520datasets%252C%2520validating%2520the%2520model%2527s%2520scalability.%2520These%250Aresults%2520suggest%2520that%2520SMART%2520has%2520initially%2520emulated%2520two%2520important%2520properties%253A%250Ascalability%2520and%2520zero-shot%2520generalization%252C%2520and%2520preliminarily%2520meets%2520the%2520needs%2520of%250Alarge-scale%2520real-time%2520simulation%2520applications.%2520We%2520have%2520released%2520all%2520the%2520code%2520to%250Apromote%2520the%2520exploration%2520of%2520models%2520for%2520motion%2520generation%2520in%2520the%2520autonomous%250Adriving%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMART%3A%20Scalable%20Multi-agent%20Real-time%20Simulation%20via%20Next-token%0A%20%20Prediction&entry.906535625=Wei%20Wu%20and%20Xiaoxin%20Feng%20and%20Ziyan%20Gao%20and%20Yuheng%20Kan&entry.1292438233=%20%20Data-driven%20autonomous%20driving%20motion%20generation%20tasks%20are%20frequently%0Aimpacted%20by%20the%20limitations%20of%20dataset%20size%20and%20the%20domain%20gap%20between%0Adatasets%2C%20which%20precludes%20their%20extensive%20application%20in%20real-world%20scenarios.%0ATo%20address%20this%20issue%2C%20we%20introduce%20SMART%2C%20a%20novel%20autonomous%20driving%20motion%0Ageneration%20paradigm%20that%20models%20vectorized%20map%20and%20agent%20trajectory%20data%20into%0Adiscrete%20sequence%20tokens.%20These%20tokens%20are%20then%20processed%20through%20a%0Adecoder-only%20transformer%20architecture%20to%20train%20for%20the%20next%20token%20prediction%0Atask%20across%20spatial-temporal%20series.%20This%20GPT-style%20method%20allows%20the%20model%20to%0Alearn%20the%20motion%20distribution%20in%20real%20driving%20scenarios.%20SMART%20achieves%0Astate-of-the-art%20performance%20across%20most%20of%20the%20metrics%20on%20the%20generative%20Sim%0AAgents%20challenge%2C%20ranking%201st%20on%20the%20leaderboards%20of%20Waymo%20Open%20Motion%20Dataset%0A%28WOMD%29%2C%20demonstrating%20remarkable%20inference%20speed.%20Moreover%2C%20SMART%20represents%0Athe%20generative%20model%20in%20the%20autonomous%20driving%20motion%20domain%2C%20exhibiting%0Azero-shot%20generalization%20capabilities%3A%20Using%20only%20the%20NuPlan%20dataset%20for%0Atraining%20and%20WOMD%20for%20validation%2C%20SMART%20achieved%20a%20competitive%20score%20of%200.71%20on%0Athe%20Sim%20Agents%20challenge.%20Lastly%2C%20we%20have%20collected%20over%201%20billion%20motion%0Atokens%20from%20multiple%20datasets%2C%20validating%20the%20model%27s%20scalability.%20These%0Aresults%20suggest%20that%20SMART%20has%20initially%20emulated%20two%20important%20properties%3A%0Ascalability%20and%20zero-shot%20generalization%2C%20and%20preliminarily%20meets%20the%20needs%20of%0Alarge-scale%20real-time%20simulation%20applications.%20We%20have%20released%20all%20the%20code%20to%0Apromote%20the%20exploration%20of%20models%20for%20motion%20generation%20in%20the%20autonomous%0Adriving%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15677v1&entry.124074799=Read"},
{"title": "ContourCraft: Learning to Resolve Intersections in Neural Multi-Garment\n  Simulations", "author": "Artur Grigorev and Giorgio Becherini and Michael J. Black and Otmar Hilliges and Bernhard Thomaszewski", "abstract": "  Learning-based approaches to cloth simulation have started to show their\npotential in recent years. However, handling collisions and intersections in\nneural simulations remains a largely unsolved problem. In this work, we present\n\\moniker{}, a learning-based solution for handling intersections in neural\ncloth simulations. Unlike conventional approaches that critically rely on\nintersection-free inputs, \\moniker{} robustly recovers from intersections\nintroduced through missed collisions, self-penetrating bodies, or errors in\nmanually designed multi-layer outfits. The technical core of \\moniker{} is a\nnovel intersection contour loss that penalizes interpenetrations and encourages\nrapid resolution thereof. We integrate our intersection loss with a\ncollision-avoiding repulsion objective into a neural cloth simulation method\nbased on graph neural networks (GNNs). We demonstrate our method's ability\nacross a challenging set of diverse multi-layer outfits under dynamic human\nmotions. Our extensive analysis indicates that \\moniker{} significantly\nimproves collision handling for learned simulation and produces visually\ncompelling results.\n", "link": "http://arxiv.org/abs/2405.09522v2", "date": "2024-05-24", "relevancy": 2.2035, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5812}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5468}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContourCraft%3A%20Learning%20to%20Resolve%20Intersections%20in%20Neural%20Multi-Garment%0A%20%20Simulations&body=Title%3A%20ContourCraft%3A%20Learning%20to%20Resolve%20Intersections%20in%20Neural%20Multi-Garment%0A%20%20Simulations%0AAuthor%3A%20Artur%20Grigorev%20and%20Giorgio%20Becherini%20and%20Michael%20J.%20Black%20and%20Otmar%20Hilliges%20and%20Bernhard%20Thomaszewski%0AAbstract%3A%20%20%20Learning-based%20approaches%20to%20cloth%20simulation%20have%20started%20to%20show%20their%0Apotential%20in%20recent%20years.%20However%2C%20handling%20collisions%20and%20intersections%20in%0Aneural%20simulations%20remains%20a%20largely%20unsolved%20problem.%20In%20this%20work%2C%20we%20present%0A%5Cmoniker%7B%7D%2C%20a%20learning-based%20solution%20for%20handling%20intersections%20in%20neural%0Acloth%20simulations.%20Unlike%20conventional%20approaches%20that%20critically%20rely%20on%0Aintersection-free%20inputs%2C%20%5Cmoniker%7B%7D%20robustly%20recovers%20from%20intersections%0Aintroduced%20through%20missed%20collisions%2C%20self-penetrating%20bodies%2C%20or%20errors%20in%0Amanually%20designed%20multi-layer%20outfits.%20The%20technical%20core%20of%20%5Cmoniker%7B%7D%20is%20a%0Anovel%20intersection%20contour%20loss%20that%20penalizes%20interpenetrations%20and%20encourages%0Arapid%20resolution%20thereof.%20We%20integrate%20our%20intersection%20loss%20with%20a%0Acollision-avoiding%20repulsion%20objective%20into%20a%20neural%20cloth%20simulation%20method%0Abased%20on%20graph%20neural%20networks%20%28GNNs%29.%20We%20demonstrate%20our%20method%27s%20ability%0Aacross%20a%20challenging%20set%20of%20diverse%20multi-layer%20outfits%20under%20dynamic%20human%0Amotions.%20Our%20extensive%20analysis%20indicates%20that%20%5Cmoniker%7B%7D%20significantly%0Aimproves%20collision%20handling%20for%20learned%20simulation%20and%20produces%20visually%0Acompelling%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContourCraft%253A%2520Learning%2520to%2520Resolve%2520Intersections%2520in%2520Neural%2520Multi-Garment%250A%2520%2520Simulations%26entry.906535625%3DArtur%2520Grigorev%2520and%2520Giorgio%2520Becherini%2520and%2520Michael%2520J.%2520Black%2520and%2520Otmar%2520Hilliges%2520and%2520Bernhard%2520Thomaszewski%26entry.1292438233%3D%2520%2520Learning-based%2520approaches%2520to%2520cloth%2520simulation%2520have%2520started%2520to%2520show%2520their%250Apotential%2520in%2520recent%2520years.%2520However%252C%2520handling%2520collisions%2520and%2520intersections%2520in%250Aneural%2520simulations%2520remains%2520a%2520largely%2520unsolved%2520problem.%2520In%2520this%2520work%252C%2520we%2520present%250A%255Cmoniker%257B%257D%252C%2520a%2520learning-based%2520solution%2520for%2520handling%2520intersections%2520in%2520neural%250Acloth%2520simulations.%2520Unlike%2520conventional%2520approaches%2520that%2520critically%2520rely%2520on%250Aintersection-free%2520inputs%252C%2520%255Cmoniker%257B%257D%2520robustly%2520recovers%2520from%2520intersections%250Aintroduced%2520through%2520missed%2520collisions%252C%2520self-penetrating%2520bodies%252C%2520or%2520errors%2520in%250Amanually%2520designed%2520multi-layer%2520outfits.%2520The%2520technical%2520core%2520of%2520%255Cmoniker%257B%257D%2520is%2520a%250Anovel%2520intersection%2520contour%2520loss%2520that%2520penalizes%2520interpenetrations%2520and%2520encourages%250Arapid%2520resolution%2520thereof.%2520We%2520integrate%2520our%2520intersection%2520loss%2520with%2520a%250Acollision-avoiding%2520repulsion%2520objective%2520into%2520a%2520neural%2520cloth%2520simulation%2520method%250Abased%2520on%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520We%2520demonstrate%2520our%2520method%2527s%2520ability%250Aacross%2520a%2520challenging%2520set%2520of%2520diverse%2520multi-layer%2520outfits%2520under%2520dynamic%2520human%250Amotions.%2520Our%2520extensive%2520analysis%2520indicates%2520that%2520%255Cmoniker%257B%257D%2520significantly%250Aimproves%2520collision%2520handling%2520for%2520learned%2520simulation%2520and%2520produces%2520visually%250Acompelling%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContourCraft%3A%20Learning%20to%20Resolve%20Intersections%20in%20Neural%20Multi-Garment%0A%20%20Simulations&entry.906535625=Artur%20Grigorev%20and%20Giorgio%20Becherini%20and%20Michael%20J.%20Black%20and%20Otmar%20Hilliges%20and%20Bernhard%20Thomaszewski&entry.1292438233=%20%20Learning-based%20approaches%20to%20cloth%20simulation%20have%20started%20to%20show%20their%0Apotential%20in%20recent%20years.%20However%2C%20handling%20collisions%20and%20intersections%20in%0Aneural%20simulations%20remains%20a%20largely%20unsolved%20problem.%20In%20this%20work%2C%20we%20present%0A%5Cmoniker%7B%7D%2C%20a%20learning-based%20solution%20for%20handling%20intersections%20in%20neural%0Acloth%20simulations.%20Unlike%20conventional%20approaches%20that%20critically%20rely%20on%0Aintersection-free%20inputs%2C%20%5Cmoniker%7B%7D%20robustly%20recovers%20from%20intersections%0Aintroduced%20through%20missed%20collisions%2C%20self-penetrating%20bodies%2C%20or%20errors%20in%0Amanually%20designed%20multi-layer%20outfits.%20The%20technical%20core%20of%20%5Cmoniker%7B%7D%20is%20a%0Anovel%20intersection%20contour%20loss%20that%20penalizes%20interpenetrations%20and%20encourages%0Arapid%20resolution%20thereof.%20We%20integrate%20our%20intersection%20loss%20with%20a%0Acollision-avoiding%20repulsion%20objective%20into%20a%20neural%20cloth%20simulation%20method%0Abased%20on%20graph%20neural%20networks%20%28GNNs%29.%20We%20demonstrate%20our%20method%27s%20ability%0Aacross%20a%20challenging%20set%20of%20diverse%20multi-layer%20outfits%20under%20dynamic%20human%0Amotions.%20Our%20extensive%20analysis%20indicates%20that%20%5Cmoniker%7B%7D%20significantly%0Aimproves%20collision%20handling%20for%20learned%20simulation%20and%20produces%20visually%0Acompelling%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09522v2&entry.124074799=Read"},
{"title": "The Road Less Scheduled", "author": "Aaron Defazio and  Xingyu and  Yang and Harsh Mehta and Konstantin Mishchenko and Ahmed Khaled and Ashok Cutkosky", "abstract": "  Existing learning rate schedules that do not require specification of the\noptimization stopping step T are greatly out-performed by learning rate\nschedules that depend on T. We propose an approach that avoids the need for\nthis stopping time by eschewing the use of schedules entirely, while exhibiting\nstate-of-the-art performance compared to schedules across a wide family of\nproblems ranging from convex problems to large-scale deep learning problems.\nOur Schedule-Free approach introduces no additional hyper-parameters over\nstandard optimizers with momentum. Our method is a direct consequence of a new\ntheory we develop that unifies scheduling and iterate averaging. An open source\nimplementation of our method is available\n(https://github.com/facebookresearch/schedule_free).\n", "link": "http://arxiv.org/abs/2405.15682v1", "date": "2024-05-24", "relevancy": 2.1981, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4487}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4406}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Road%20Less%20Scheduled&body=Title%3A%20The%20Road%20Less%20Scheduled%0AAuthor%3A%20Aaron%20Defazio%20and%20%20Xingyu%20and%20%20Yang%20and%20Harsh%20Mehta%20and%20Konstantin%20Mishchenko%20and%20Ahmed%20Khaled%20and%20Ashok%20Cutkosky%0AAbstract%3A%20%20%20Existing%20learning%20rate%20schedules%20that%20do%20not%20require%20specification%20of%20the%0Aoptimization%20stopping%20step%20T%20are%20greatly%20out-performed%20by%20learning%20rate%0Aschedules%20that%20depend%20on%20T.%20We%20propose%20an%20approach%20that%20avoids%20the%20need%20for%0Athis%20stopping%20time%20by%20eschewing%20the%20use%20of%20schedules%20entirely%2C%20while%20exhibiting%0Astate-of-the-art%20performance%20compared%20to%20schedules%20across%20a%20wide%20family%20of%0Aproblems%20ranging%20from%20convex%20problems%20to%20large-scale%20deep%20learning%20problems.%0AOur%20Schedule-Free%20approach%20introduces%20no%20additional%20hyper-parameters%20over%0Astandard%20optimizers%20with%20momentum.%20Our%20method%20is%20a%20direct%20consequence%20of%20a%20new%0Atheory%20we%20develop%20that%20unifies%20scheduling%20and%20iterate%20averaging.%20An%20open%20source%0Aimplementation%20of%20our%20method%20is%20available%0A%28https%3A//github.com/facebookresearch/schedule_free%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Road%2520Less%2520Scheduled%26entry.906535625%3DAaron%2520Defazio%2520and%2520%2520Xingyu%2520and%2520%2520Yang%2520and%2520Harsh%2520Mehta%2520and%2520Konstantin%2520Mishchenko%2520and%2520Ahmed%2520Khaled%2520and%2520Ashok%2520Cutkosky%26entry.1292438233%3D%2520%2520Existing%2520learning%2520rate%2520schedules%2520that%2520do%2520not%2520require%2520specification%2520of%2520the%250Aoptimization%2520stopping%2520step%2520T%2520are%2520greatly%2520out-performed%2520by%2520learning%2520rate%250Aschedules%2520that%2520depend%2520on%2520T.%2520We%2520propose%2520an%2520approach%2520that%2520avoids%2520the%2520need%2520for%250Athis%2520stopping%2520time%2520by%2520eschewing%2520the%2520use%2520of%2520schedules%2520entirely%252C%2520while%2520exhibiting%250Astate-of-the-art%2520performance%2520compared%2520to%2520schedules%2520across%2520a%2520wide%2520family%2520of%250Aproblems%2520ranging%2520from%2520convex%2520problems%2520to%2520large-scale%2520deep%2520learning%2520problems.%250AOur%2520Schedule-Free%2520approach%2520introduces%2520no%2520additional%2520hyper-parameters%2520over%250Astandard%2520optimizers%2520with%2520momentum.%2520Our%2520method%2520is%2520a%2520direct%2520consequence%2520of%2520a%2520new%250Atheory%2520we%2520develop%2520that%2520unifies%2520scheduling%2520and%2520iterate%2520averaging.%2520An%2520open%2520source%250Aimplementation%2520of%2520our%2520method%2520is%2520available%250A%2528https%253A//github.com/facebookresearch/schedule_free%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Road%20Less%20Scheduled&entry.906535625=Aaron%20Defazio%20and%20%20Xingyu%20and%20%20Yang%20and%20Harsh%20Mehta%20and%20Konstantin%20Mishchenko%20and%20Ahmed%20Khaled%20and%20Ashok%20Cutkosky&entry.1292438233=%20%20Existing%20learning%20rate%20schedules%20that%20do%20not%20require%20specification%20of%20the%0Aoptimization%20stopping%20step%20T%20are%20greatly%20out-performed%20by%20learning%20rate%0Aschedules%20that%20depend%20on%20T.%20We%20propose%20an%20approach%20that%20avoids%20the%20need%20for%0Athis%20stopping%20time%20by%20eschewing%20the%20use%20of%20schedules%20entirely%2C%20while%20exhibiting%0Astate-of-the-art%20performance%20compared%20to%20schedules%20across%20a%20wide%20family%20of%0Aproblems%20ranging%20from%20convex%20problems%20to%20large-scale%20deep%20learning%20problems.%0AOur%20Schedule-Free%20approach%20introduces%20no%20additional%20hyper-parameters%20over%0Astandard%20optimizers%20with%20momentum.%20Our%20method%20is%20a%20direct%20consequence%20of%20a%20new%0Atheory%20we%20develop%20that%20unifies%20scheduling%20and%20iterate%20averaging.%20An%20open%20source%0Aimplementation%20of%20our%20method%20is%20available%0A%28https%3A//github.com/facebookresearch/schedule_free%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15682v1&entry.124074799=Read"},
{"title": "FreeMotion: A Unified Framework for Number-free Text-to-Motion Synthesis", "author": "Ke Fan and Junshu Tang and Weijian Cao and Ran Yi and Moran Li and Jingyu Gong and Jiangning Zhang and Yabiao Wang and Chengjie Wang and Lizhuang Ma", "abstract": "  Text-to-motion synthesis is a crucial task in computer vision. Existing\nmethods are limited in their universality, as they are tailored for\nsingle-person or two-person scenarios and can not be applied to generate\nmotions for more individuals. To achieve the number-free motion synthesis, this\npaper reconsiders motion generation and proposes to unify the single and\nmulti-person motion by the conditional motion distribution. Furthermore, a\ngeneration module and an interaction module are designed for our FreeMotion\nframework to decouple the process of conditional motion generation and finally\nsupport the number-free motion synthesis. Besides, based on our framework, the\ncurrent single-person motion spatial control method could be seamlessly\nintegrated, achieving precise control of multi-person motion. Extensive\nexperiments demonstrate the superior performance of our method and our\ncapability to infer single and multi-human motions simultaneously.\n", "link": "http://arxiv.org/abs/2405.15763v1", "date": "2024-05-24", "relevancy": 2.1972, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5802}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5529}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeMotion%3A%20A%20Unified%20Framework%20for%20Number-free%20Text-to-Motion%20Synthesis&body=Title%3A%20FreeMotion%3A%20A%20Unified%20Framework%20for%20Number-free%20Text-to-Motion%20Synthesis%0AAuthor%3A%20Ke%20Fan%20and%20Junshu%20Tang%20and%20Weijian%20Cao%20and%20Ran%20Yi%20and%20Moran%20Li%20and%20Jingyu%20Gong%20and%20Jiangning%20Zhang%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Text-to-motion%20synthesis%20is%20a%20crucial%20task%20in%20computer%20vision.%20Existing%0Amethods%20are%20limited%20in%20their%20universality%2C%20as%20they%20are%20tailored%20for%0Asingle-person%20or%20two-person%20scenarios%20and%20can%20not%20be%20applied%20to%20generate%0Amotions%20for%20more%20individuals.%20To%20achieve%20the%20number-free%20motion%20synthesis%2C%20this%0Apaper%20reconsiders%20motion%20generation%20and%20proposes%20to%20unify%20the%20single%20and%0Amulti-person%20motion%20by%20the%20conditional%20motion%20distribution.%20Furthermore%2C%20a%0Ageneration%20module%20and%20an%20interaction%20module%20are%20designed%20for%20our%20FreeMotion%0Aframework%20to%20decouple%20the%20process%20of%20conditional%20motion%20generation%20and%20finally%0Asupport%20the%20number-free%20motion%20synthesis.%20Besides%2C%20based%20on%20our%20framework%2C%20the%0Acurrent%20single-person%20motion%20spatial%20control%20method%20could%20be%20seamlessly%0Aintegrated%2C%20achieving%20precise%20control%20of%20multi-person%20motion.%20Extensive%0Aexperiments%20demonstrate%20the%20superior%20performance%20of%20our%20method%20and%20our%0Acapability%20to%20infer%20single%20and%20multi-human%20motions%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeMotion%253A%2520A%2520Unified%2520Framework%2520for%2520Number-free%2520Text-to-Motion%2520Synthesis%26entry.906535625%3DKe%2520Fan%2520and%2520Junshu%2520Tang%2520and%2520Weijian%2520Cao%2520and%2520Ran%2520Yi%2520and%2520Moran%2520Li%2520and%2520Jingyu%2520Gong%2520and%2520Jiangning%2520Zhang%2520and%2520Yabiao%2520Wang%2520and%2520Chengjie%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520Text-to-motion%2520synthesis%2520is%2520a%2520crucial%2520task%2520in%2520computer%2520vision.%2520Existing%250Amethods%2520are%2520limited%2520in%2520their%2520universality%252C%2520as%2520they%2520are%2520tailored%2520for%250Asingle-person%2520or%2520two-person%2520scenarios%2520and%2520can%2520not%2520be%2520applied%2520to%2520generate%250Amotions%2520for%2520more%2520individuals.%2520To%2520achieve%2520the%2520number-free%2520motion%2520synthesis%252C%2520this%250Apaper%2520reconsiders%2520motion%2520generation%2520and%2520proposes%2520to%2520unify%2520the%2520single%2520and%250Amulti-person%2520motion%2520by%2520the%2520conditional%2520motion%2520distribution.%2520Furthermore%252C%2520a%250Ageneration%2520module%2520and%2520an%2520interaction%2520module%2520are%2520designed%2520for%2520our%2520FreeMotion%250Aframework%2520to%2520decouple%2520the%2520process%2520of%2520conditional%2520motion%2520generation%2520and%2520finally%250Asupport%2520the%2520number-free%2520motion%2520synthesis.%2520Besides%252C%2520based%2520on%2520our%2520framework%252C%2520the%250Acurrent%2520single-person%2520motion%2520spatial%2520control%2520method%2520could%2520be%2520seamlessly%250Aintegrated%252C%2520achieving%2520precise%2520control%2520of%2520multi-person%2520motion.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%2520method%2520and%2520our%250Acapability%2520to%2520infer%2520single%2520and%2520multi-human%2520motions%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeMotion%3A%20A%20Unified%20Framework%20for%20Number-free%20Text-to-Motion%20Synthesis&entry.906535625=Ke%20Fan%20and%20Junshu%20Tang%20and%20Weijian%20Cao%20and%20Ran%20Yi%20and%20Moran%20Li%20and%20Jingyu%20Gong%20and%20Jiangning%20Zhang%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Text-to-motion%20synthesis%20is%20a%20crucial%20task%20in%20computer%20vision.%20Existing%0Amethods%20are%20limited%20in%20their%20universality%2C%20as%20they%20are%20tailored%20for%0Asingle-person%20or%20two-person%20scenarios%20and%20can%20not%20be%20applied%20to%20generate%0Amotions%20for%20more%20individuals.%20To%20achieve%20the%20number-free%20motion%20synthesis%2C%20this%0Apaper%20reconsiders%20motion%20generation%20and%20proposes%20to%20unify%20the%20single%20and%0Amulti-person%20motion%20by%20the%20conditional%20motion%20distribution.%20Furthermore%2C%20a%0Ageneration%20module%20and%20an%20interaction%20module%20are%20designed%20for%20our%20FreeMotion%0Aframework%20to%20decouple%20the%20process%20of%20conditional%20motion%20generation%20and%20finally%0Asupport%20the%20number-free%20motion%20synthesis.%20Besides%2C%20based%20on%20our%20framework%2C%20the%0Acurrent%20single-person%20motion%20spatial%20control%20method%20could%20be%20seamlessly%0Aintegrated%2C%20achieving%20precise%20control%20of%20multi-person%20motion.%20Extensive%0Aexperiments%20demonstrate%20the%20superior%20performance%20of%20our%20method%20and%20our%0Acapability%20to%20infer%20single%20and%20multi-human%20motions%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15763v1&entry.124074799=Read"},
{"title": "A Misleading Gallery of Fluid Motion by Generative Artificial\n  Intelligence", "author": "Ali Kashefi", "abstract": "  In this technical report, we extensively investigate the accuracy of outputs\nfrom well-known generative artificial intelligence (AI) applications in\nresponse to prompts describing common fluid motion phenomena familiar to the\nfluid mechanics community. We examine a range of applications, including\nMidjourney, Dall-E, Runway ML, Microsoft Designer, Gemini, Meta AI, and\nLeonardo AI, introduced by prominent companies such as Google, OpenAI, Meta,\nand Microsoft. Our text prompts for generating images or videos include\nexamples such as \"Von Karman vortex street\", \"flow past an airfoil\",\n\"Kelvin-Helmholtz instability\", \"shock waves on a sharp-nosed supersonic body\",\netc. We compare the images generated by these applications with real images\nfrom laboratory experiments and numerical software. Our findings indicate that\nthese generative AI models are not adequately trained in fluid dynamics\nimagery, leading to potentially misleading outputs. Beyond text-to-image/video\ngeneration, we further explore the transition from image/video to text\ngeneration using these AI tools, aiming to investigate the accuracy of their\ndescriptions of fluid motion phenomena. This report serves as a cautionary note\nfor educators in academic institutions, highlighting the potential for these\ntools to mislead students. It also aims to inform researchers at these renowned\ncompanies, encouraging them to address this issue. We conjecture that a primary\nreason for this shortcoming is the limited access to copyright-protected fluid\nmotion images from scientific journals.\n", "link": "http://arxiv.org/abs/2405.15406v1", "date": "2024-05-24", "relevancy": 2.1899, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5791}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5559}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Misleading%20Gallery%20of%20Fluid%20Motion%20by%20Generative%20Artificial%0A%20%20Intelligence&body=Title%3A%20A%20Misleading%20Gallery%20of%20Fluid%20Motion%20by%20Generative%20Artificial%0A%20%20Intelligence%0AAuthor%3A%20Ali%20Kashefi%0AAbstract%3A%20%20%20In%20this%20technical%20report%2C%20we%20extensively%20investigate%20the%20accuracy%20of%20outputs%0Afrom%20well-known%20generative%20artificial%20intelligence%20%28AI%29%20applications%20in%0Aresponse%20to%20prompts%20describing%20common%20fluid%20motion%20phenomena%20familiar%20to%20the%0Afluid%20mechanics%20community.%20We%20examine%20a%20range%20of%20applications%2C%20including%0AMidjourney%2C%20Dall-E%2C%20Runway%20ML%2C%20Microsoft%20Designer%2C%20Gemini%2C%20Meta%20AI%2C%20and%0ALeonardo%20AI%2C%20introduced%20by%20prominent%20companies%20such%20as%20Google%2C%20OpenAI%2C%20Meta%2C%0Aand%20Microsoft.%20Our%20text%20prompts%20for%20generating%20images%20or%20videos%20include%0Aexamples%20such%20as%20%22Von%20Karman%20vortex%20street%22%2C%20%22flow%20past%20an%20airfoil%22%2C%0A%22Kelvin-Helmholtz%20instability%22%2C%20%22shock%20waves%20on%20a%20sharp-nosed%20supersonic%20body%22%2C%0Aetc.%20We%20compare%20the%20images%20generated%20by%20these%20applications%20with%20real%20images%0Afrom%20laboratory%20experiments%20and%20numerical%20software.%20Our%20findings%20indicate%20that%0Athese%20generative%20AI%20models%20are%20not%20adequately%20trained%20in%20fluid%20dynamics%0Aimagery%2C%20leading%20to%20potentially%20misleading%20outputs.%20Beyond%20text-to-image/video%0Ageneration%2C%20we%20further%20explore%20the%20transition%20from%20image/video%20to%20text%0Ageneration%20using%20these%20AI%20tools%2C%20aiming%20to%20investigate%20the%20accuracy%20of%20their%0Adescriptions%20of%20fluid%20motion%20phenomena.%20This%20report%20serves%20as%20a%20cautionary%20note%0Afor%20educators%20in%20academic%20institutions%2C%20highlighting%20the%20potential%20for%20these%0Atools%20to%20mislead%20students.%20It%20also%20aims%20to%20inform%20researchers%20at%20these%20renowned%0Acompanies%2C%20encouraging%20them%20to%20address%20this%20issue.%20We%20conjecture%20that%20a%20primary%0Areason%20for%20this%20shortcoming%20is%20the%20limited%20access%20to%20copyright-protected%20fluid%0Amotion%20images%20from%20scientific%20journals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Misleading%2520Gallery%2520of%2520Fluid%2520Motion%2520by%2520Generative%2520Artificial%250A%2520%2520Intelligence%26entry.906535625%3DAli%2520Kashefi%26entry.1292438233%3D%2520%2520In%2520this%2520technical%2520report%252C%2520we%2520extensively%2520investigate%2520the%2520accuracy%2520of%2520outputs%250Afrom%2520well-known%2520generative%2520artificial%2520intelligence%2520%2528AI%2529%2520applications%2520in%250Aresponse%2520to%2520prompts%2520describing%2520common%2520fluid%2520motion%2520phenomena%2520familiar%2520to%2520the%250Afluid%2520mechanics%2520community.%2520We%2520examine%2520a%2520range%2520of%2520applications%252C%2520including%250AMidjourney%252C%2520Dall-E%252C%2520Runway%2520ML%252C%2520Microsoft%2520Designer%252C%2520Gemini%252C%2520Meta%2520AI%252C%2520and%250ALeonardo%2520AI%252C%2520introduced%2520by%2520prominent%2520companies%2520such%2520as%2520Google%252C%2520OpenAI%252C%2520Meta%252C%250Aand%2520Microsoft.%2520Our%2520text%2520prompts%2520for%2520generating%2520images%2520or%2520videos%2520include%250Aexamples%2520such%2520as%2520%2522Von%2520Karman%2520vortex%2520street%2522%252C%2520%2522flow%2520past%2520an%2520airfoil%2522%252C%250A%2522Kelvin-Helmholtz%2520instability%2522%252C%2520%2522shock%2520waves%2520on%2520a%2520sharp-nosed%2520supersonic%2520body%2522%252C%250Aetc.%2520We%2520compare%2520the%2520images%2520generated%2520by%2520these%2520applications%2520with%2520real%2520images%250Afrom%2520laboratory%2520experiments%2520and%2520numerical%2520software.%2520Our%2520findings%2520indicate%2520that%250Athese%2520generative%2520AI%2520models%2520are%2520not%2520adequately%2520trained%2520in%2520fluid%2520dynamics%250Aimagery%252C%2520leading%2520to%2520potentially%2520misleading%2520outputs.%2520Beyond%2520text-to-image/video%250Ageneration%252C%2520we%2520further%2520explore%2520the%2520transition%2520from%2520image/video%2520to%2520text%250Ageneration%2520using%2520these%2520AI%2520tools%252C%2520aiming%2520to%2520investigate%2520the%2520accuracy%2520of%2520their%250Adescriptions%2520of%2520fluid%2520motion%2520phenomena.%2520This%2520report%2520serves%2520as%2520a%2520cautionary%2520note%250Afor%2520educators%2520in%2520academic%2520institutions%252C%2520highlighting%2520the%2520potential%2520for%2520these%250Atools%2520to%2520mislead%2520students.%2520It%2520also%2520aims%2520to%2520inform%2520researchers%2520at%2520these%2520renowned%250Acompanies%252C%2520encouraging%2520them%2520to%2520address%2520this%2520issue.%2520We%2520conjecture%2520that%2520a%2520primary%250Areason%2520for%2520this%2520shortcoming%2520is%2520the%2520limited%2520access%2520to%2520copyright-protected%2520fluid%250Amotion%2520images%2520from%2520scientific%2520journals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Misleading%20Gallery%20of%20Fluid%20Motion%20by%20Generative%20Artificial%0A%20%20Intelligence&entry.906535625=Ali%20Kashefi&entry.1292438233=%20%20In%20this%20technical%20report%2C%20we%20extensively%20investigate%20the%20accuracy%20of%20outputs%0Afrom%20well-known%20generative%20artificial%20intelligence%20%28AI%29%20applications%20in%0Aresponse%20to%20prompts%20describing%20common%20fluid%20motion%20phenomena%20familiar%20to%20the%0Afluid%20mechanics%20community.%20We%20examine%20a%20range%20of%20applications%2C%20including%0AMidjourney%2C%20Dall-E%2C%20Runway%20ML%2C%20Microsoft%20Designer%2C%20Gemini%2C%20Meta%20AI%2C%20and%0ALeonardo%20AI%2C%20introduced%20by%20prominent%20companies%20such%20as%20Google%2C%20OpenAI%2C%20Meta%2C%0Aand%20Microsoft.%20Our%20text%20prompts%20for%20generating%20images%20or%20videos%20include%0Aexamples%20such%20as%20%22Von%20Karman%20vortex%20street%22%2C%20%22flow%20past%20an%20airfoil%22%2C%0A%22Kelvin-Helmholtz%20instability%22%2C%20%22shock%20waves%20on%20a%20sharp-nosed%20supersonic%20body%22%2C%0Aetc.%20We%20compare%20the%20images%20generated%20by%20these%20applications%20with%20real%20images%0Afrom%20laboratory%20experiments%20and%20numerical%20software.%20Our%20findings%20indicate%20that%0Athese%20generative%20AI%20models%20are%20not%20adequately%20trained%20in%20fluid%20dynamics%0Aimagery%2C%20leading%20to%20potentially%20misleading%20outputs.%20Beyond%20text-to-image/video%0Ageneration%2C%20we%20further%20explore%20the%20transition%20from%20image/video%20to%20text%0Ageneration%20using%20these%20AI%20tools%2C%20aiming%20to%20investigate%20the%20accuracy%20of%20their%0Adescriptions%20of%20fluid%20motion%20phenomena.%20This%20report%20serves%20as%20a%20cautionary%20note%0Afor%20educators%20in%20academic%20institutions%2C%20highlighting%20the%20potential%20for%20these%0Atools%20to%20mislead%20students.%20It%20also%20aims%20to%20inform%20researchers%20at%20these%20renowned%0Acompanies%2C%20encouraging%20them%20to%20address%20this%20issue.%20We%20conjecture%20that%20a%20primary%0Areason%20for%20this%20shortcoming%20is%20the%20limited%20access%20to%20copyright-protected%20fluid%0Amotion%20images%20from%20scientific%20journals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15406v1&entry.124074799=Read"},
{"title": "TD3 Based Collision Free Motion Planning for Robot Navigation", "author": "Hao Liu and Yi Shen and Chang Zhou and Yuelin Zou and Zijun Gao and Qi Wang", "abstract": "  This paper addresses the challenge of collision-free motion planning in\nautomated navigation within complex environments. Utilizing advancements in\nDeep Reinforcement Learning (DRL) and sensor technologies like LiDAR, we\npropose the TD3-DWA algorithm, an innovative fusion of the traditional Dynamic\nWindow Approach (DWA) with the Twin Delayed Deep Deterministic Policy Gradient\n(TD3). This hybrid algorithm enhances the efficiency of robotic path planning\nby optimizing the sampling interval parameters of DWA to effectively navigate\naround both static and dynamic obstacles. The performance of the TD3-DWA\nalgorithm is validated through various simulation experiments, demonstrating\nits potential to significantly improve the reliability and safety of autonomous\nnavigation systems.\n", "link": "http://arxiv.org/abs/2405.15460v1", "date": "2024-05-24", "relevancy": 2.1687, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5595}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5307}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TD3%20Based%20Collision%20Free%20Motion%20Planning%20for%20Robot%20Navigation&body=Title%3A%20TD3%20Based%20Collision%20Free%20Motion%20Planning%20for%20Robot%20Navigation%0AAuthor%3A%20Hao%20Liu%20and%20Yi%20Shen%20and%20Chang%20Zhou%20and%20Yuelin%20Zou%20and%20Zijun%20Gao%20and%20Qi%20Wang%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20collision-free%20motion%20planning%20in%0Aautomated%20navigation%20within%20complex%20environments.%20Utilizing%20advancements%20in%0ADeep%20Reinforcement%20Learning%20%28DRL%29%20and%20sensor%20technologies%20like%20LiDAR%2C%20we%0Apropose%20the%20TD3-DWA%20algorithm%2C%20an%20innovative%20fusion%20of%20the%20traditional%20Dynamic%0AWindow%20Approach%20%28DWA%29%20with%20the%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%0A%28TD3%29.%20This%20hybrid%20algorithm%20enhances%20the%20efficiency%20of%20robotic%20path%20planning%0Aby%20optimizing%20the%20sampling%20interval%20parameters%20of%20DWA%20to%20effectively%20navigate%0Aaround%20both%20static%20and%20dynamic%20obstacles.%20The%20performance%20of%20the%20TD3-DWA%0Aalgorithm%20is%20validated%20through%20various%20simulation%20experiments%2C%20demonstrating%0Aits%20potential%20to%20significantly%20improve%20the%20reliability%20and%20safety%20of%20autonomous%0Anavigation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTD3%2520Based%2520Collision%2520Free%2520Motion%2520Planning%2520for%2520Robot%2520Navigation%26entry.906535625%3DHao%2520Liu%2520and%2520Yi%2520Shen%2520and%2520Chang%2520Zhou%2520and%2520Yuelin%2520Zou%2520and%2520Zijun%2520Gao%2520and%2520Qi%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520collision-free%2520motion%2520planning%2520in%250Aautomated%2520navigation%2520within%2520complex%2520environments.%2520Utilizing%2520advancements%2520in%250ADeep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520and%2520sensor%2520technologies%2520like%2520LiDAR%252C%2520we%250Apropose%2520the%2520TD3-DWA%2520algorithm%252C%2520an%2520innovative%2520fusion%2520of%2520the%2520traditional%2520Dynamic%250AWindow%2520Approach%2520%2528DWA%2529%2520with%2520the%2520Twin%2520Delayed%2520Deep%2520Deterministic%2520Policy%2520Gradient%250A%2528TD3%2529.%2520This%2520hybrid%2520algorithm%2520enhances%2520the%2520efficiency%2520of%2520robotic%2520path%2520planning%250Aby%2520optimizing%2520the%2520sampling%2520interval%2520parameters%2520of%2520DWA%2520to%2520effectively%2520navigate%250Aaround%2520both%2520static%2520and%2520dynamic%2520obstacles.%2520The%2520performance%2520of%2520the%2520TD3-DWA%250Aalgorithm%2520is%2520validated%2520through%2520various%2520simulation%2520experiments%252C%2520demonstrating%250Aits%2520potential%2520to%2520significantly%2520improve%2520the%2520reliability%2520and%2520safety%2520of%2520autonomous%250Anavigation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TD3%20Based%20Collision%20Free%20Motion%20Planning%20for%20Robot%20Navigation&entry.906535625=Hao%20Liu%20and%20Yi%20Shen%20and%20Chang%20Zhou%20and%20Yuelin%20Zou%20and%20Zijun%20Gao%20and%20Qi%20Wang&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20collision-free%20motion%20planning%20in%0Aautomated%20navigation%20within%20complex%20environments.%20Utilizing%20advancements%20in%0ADeep%20Reinforcement%20Learning%20%28DRL%29%20and%20sensor%20technologies%20like%20LiDAR%2C%20we%0Apropose%20the%20TD3-DWA%20algorithm%2C%20an%20innovative%20fusion%20of%20the%20traditional%20Dynamic%0AWindow%20Approach%20%28DWA%29%20with%20the%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%0A%28TD3%29.%20This%20hybrid%20algorithm%20enhances%20the%20efficiency%20of%20robotic%20path%20planning%0Aby%20optimizing%20the%20sampling%20interval%20parameters%20of%20DWA%20to%20effectively%20navigate%0Aaround%20both%20static%20and%20dynamic%20obstacles.%20The%20performance%20of%20the%20TD3-DWA%0Aalgorithm%20is%20validated%20through%20various%20simulation%20experiments%2C%20demonstrating%0Aits%20potential%20to%20significantly%20improve%20the%20reliability%20and%20safety%20of%20autonomous%0Anavigation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15460v1&entry.124074799=Read"},
{"title": "Efficient Degradation-aware Any Image Restoration", "author": "Eduard Zamfir and Zongwei Wu and Nancy Mehta and Danda Dani Paudel and Yulun Zhang and Radu Timofte", "abstract": "  Reconstructing missing details from degraded low-quality inputs poses a\nsignificant challenge. Recent progress in image restoration has demonstrated\nthe efficacy of learning large models capable of addressing various\ndegradations simultaneously. Nonetheless, these approaches introduce\nconsiderable computational overhead and complex learning paradigms, limiting\ntheir practical utility. In response, we propose \\textit{DaAIR}, an efficient\nAll-in-One image restorer employing a Degradation-aware Learner (DaLe) in the\nlow-rank regime to collaboratively mine shared aspects and subtle nuances\nacross diverse degradations, generating a degradation-aware embedding. By\ndynamically allocating model capacity to input degradations, we realize an\nefficient restorer integrating holistic and specific learning within a unified\nmodel. Furthermore, DaAIR introduces a cost-efficient parameter update\nmechanism that enhances degradation awareness while maintaining computational\nefficiency. Extensive comparisons across five image degradations demonstrate\nthat our DaAIR outperforms both state-of-the-art All-in-One models and\ndegradation-specific counterparts, affirming our efficacy and practicality. The\nsource will be publicly made available at\n\\url{https://eduardzamfir.github.io/daair/}\n", "link": "http://arxiv.org/abs/2405.15475v1", "date": "2024-05-24", "relevancy": 2.1629, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5532}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5452}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Degradation-aware%20Any%20Image%20Restoration&body=Title%3A%20Efficient%20Degradation-aware%20Any%20Image%20Restoration%0AAuthor%3A%20Eduard%20Zamfir%20and%20Zongwei%20Wu%20and%20Nancy%20Mehta%20and%20Danda%20Dani%20Paudel%20and%20Yulun%20Zhang%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20Reconstructing%20missing%20details%20from%20degraded%20low-quality%20inputs%20poses%20a%0Asignificant%20challenge.%20Recent%20progress%20in%20image%20restoration%20has%20demonstrated%0Athe%20efficacy%20of%20learning%20large%20models%20capable%20of%20addressing%20various%0Adegradations%20simultaneously.%20Nonetheless%2C%20these%20approaches%20introduce%0Aconsiderable%20computational%20overhead%20and%20complex%20learning%20paradigms%2C%20limiting%0Atheir%20practical%20utility.%20In%20response%2C%20we%20propose%20%5Ctextit%7BDaAIR%7D%2C%20an%20efficient%0AAll-in-One%20image%20restorer%20employing%20a%20Degradation-aware%20Learner%20%28DaLe%29%20in%20the%0Alow-rank%20regime%20to%20collaboratively%20mine%20shared%20aspects%20and%20subtle%20nuances%0Aacross%20diverse%20degradations%2C%20generating%20a%20degradation-aware%20embedding.%20By%0Adynamically%20allocating%20model%20capacity%20to%20input%20degradations%2C%20we%20realize%20an%0Aefficient%20restorer%20integrating%20holistic%20and%20specific%20learning%20within%20a%20unified%0Amodel.%20Furthermore%2C%20DaAIR%20introduces%20a%20cost-efficient%20parameter%20update%0Amechanism%20that%20enhances%20degradation%20awareness%20while%20maintaining%20computational%0Aefficiency.%20Extensive%20comparisons%20across%20five%20image%20degradations%20demonstrate%0Athat%20our%20DaAIR%20outperforms%20both%20state-of-the-art%20All-in-One%20models%20and%0Adegradation-specific%20counterparts%2C%20affirming%20our%20efficacy%20and%20practicality.%20The%0Asource%20will%20be%20publicly%20made%20available%20at%0A%5Curl%7Bhttps%3A//eduardzamfir.github.io/daair/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Degradation-aware%2520Any%2520Image%2520Restoration%26entry.906535625%3DEduard%2520Zamfir%2520and%2520Zongwei%2520Wu%2520and%2520Nancy%2520Mehta%2520and%2520Danda%2520Dani%2520Paudel%2520and%2520Yulun%2520Zhang%2520and%2520Radu%2520Timofte%26entry.1292438233%3D%2520%2520Reconstructing%2520missing%2520details%2520from%2520degraded%2520low-quality%2520inputs%2520poses%2520a%250Asignificant%2520challenge.%2520Recent%2520progress%2520in%2520image%2520restoration%2520has%2520demonstrated%250Athe%2520efficacy%2520of%2520learning%2520large%2520models%2520capable%2520of%2520addressing%2520various%250Adegradations%2520simultaneously.%2520Nonetheless%252C%2520these%2520approaches%2520introduce%250Aconsiderable%2520computational%2520overhead%2520and%2520complex%2520learning%2520paradigms%252C%2520limiting%250Atheir%2520practical%2520utility.%2520In%2520response%252C%2520we%2520propose%2520%255Ctextit%257BDaAIR%257D%252C%2520an%2520efficient%250AAll-in-One%2520image%2520restorer%2520employing%2520a%2520Degradation-aware%2520Learner%2520%2528DaLe%2529%2520in%2520the%250Alow-rank%2520regime%2520to%2520collaboratively%2520mine%2520shared%2520aspects%2520and%2520subtle%2520nuances%250Aacross%2520diverse%2520degradations%252C%2520generating%2520a%2520degradation-aware%2520embedding.%2520By%250Adynamically%2520allocating%2520model%2520capacity%2520to%2520input%2520degradations%252C%2520we%2520realize%2520an%250Aefficient%2520restorer%2520integrating%2520holistic%2520and%2520specific%2520learning%2520within%2520a%2520unified%250Amodel.%2520Furthermore%252C%2520DaAIR%2520introduces%2520a%2520cost-efficient%2520parameter%2520update%250Amechanism%2520that%2520enhances%2520degradation%2520awareness%2520while%2520maintaining%2520computational%250Aefficiency.%2520Extensive%2520comparisons%2520across%2520five%2520image%2520degradations%2520demonstrate%250Athat%2520our%2520DaAIR%2520outperforms%2520both%2520state-of-the-art%2520All-in-One%2520models%2520and%250Adegradation-specific%2520counterparts%252C%2520affirming%2520our%2520efficacy%2520and%2520practicality.%2520The%250Asource%2520will%2520be%2520publicly%2520made%2520available%2520at%250A%255Curl%257Bhttps%253A//eduardzamfir.github.io/daair/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Degradation-aware%20Any%20Image%20Restoration&entry.906535625=Eduard%20Zamfir%20and%20Zongwei%20Wu%20and%20Nancy%20Mehta%20and%20Danda%20Dani%20Paudel%20and%20Yulun%20Zhang%20and%20Radu%20Timofte&entry.1292438233=%20%20Reconstructing%20missing%20details%20from%20degraded%20low-quality%20inputs%20poses%20a%0Asignificant%20challenge.%20Recent%20progress%20in%20image%20restoration%20has%20demonstrated%0Athe%20efficacy%20of%20learning%20large%20models%20capable%20of%20addressing%20various%0Adegradations%20simultaneously.%20Nonetheless%2C%20these%20approaches%20introduce%0Aconsiderable%20computational%20overhead%20and%20complex%20learning%20paradigms%2C%20limiting%0Atheir%20practical%20utility.%20In%20response%2C%20we%20propose%20%5Ctextit%7BDaAIR%7D%2C%20an%20efficient%0AAll-in-One%20image%20restorer%20employing%20a%20Degradation-aware%20Learner%20%28DaLe%29%20in%20the%0Alow-rank%20regime%20to%20collaboratively%20mine%20shared%20aspects%20and%20subtle%20nuances%0Aacross%20diverse%20degradations%2C%20generating%20a%20degradation-aware%20embedding.%20By%0Adynamically%20allocating%20model%20capacity%20to%20input%20degradations%2C%20we%20realize%20an%0Aefficient%20restorer%20integrating%20holistic%20and%20specific%20learning%20within%20a%20unified%0Amodel.%20Furthermore%2C%20DaAIR%20introduces%20a%20cost-efficient%20parameter%20update%0Amechanism%20that%20enhances%20degradation%20awareness%20while%20maintaining%20computational%0Aefficiency.%20Extensive%20comparisons%20across%20five%20image%20degradations%20demonstrate%0Athat%20our%20DaAIR%20outperforms%20both%20state-of-the-art%20All-in-One%20models%20and%0Adegradation-specific%20counterparts%2C%20affirming%20our%20efficacy%20and%20practicality.%20The%0Asource%20will%20be%20publicly%20made%20available%20at%0A%5Curl%7Bhttps%3A//eduardzamfir.github.io/daair/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15475v1&entry.124074799=Read"},
{"title": "E(n) Equivariant Topological Neural Networks", "author": "Claudio Battiloro and Ege Karaismailo\u011flu and Mauricio Tec and George Dasoulas and Michelle Audirac and Francesca Dominici", "abstract": "  Graph neural networks excel at modeling pairwise interactions, but they\ncannot flexibly accommodate higher-order interactions and features. Topological\ndeep learning (TDL) has emerged recently as a promising tool for addressing\nthis issue. TDL enables the principled modeling of arbitrary multi-way,\nhierarchical higher-order interactions by operating on combinatorial\ntopological spaces, such as simplicial or cell complexes, instead of graphs.\nHowever, little is known about how to leverage geometric features such as\npositions and velocities for TDL. This paper introduces E(n)-Equivariant\nTopological Neural Networks (ETNNs), which are E(n)-equivariant message-passing\nnetworks operating on combinatorial complexes, formal objects unifying graphs,\nhypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometric\nnode features while respecting rotation and translation equivariance. Moreover,\nETNNs are natively ready for settings with heterogeneous interactions. We\nprovide a theoretical analysis to show the improved expressiveness of ETNNs\nover architectures for geometric graphs. We also show how several E(n)\nequivariant variants of TDL models can be directly derived from our framework.\nThe broad applicability of ETNNs is demonstrated through two tasks of vastly\ndifferent nature: i) molecular property prediction on the QM9 benchmark and ii)\nland-use regression for hyper-local estimation of air pollution with\nmulti-resolution irregular geospatial data. The experiment results indicate\nthat ETNNs are an effective tool for learning from diverse types of richly\nstructured data, highlighting the benefits of principled geometric inductive\nbias.\n", "link": "http://arxiv.org/abs/2405.15429v1", "date": "2024-05-24", "relevancy": 2.1537, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5797}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5161}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E%28n%29%20Equivariant%20Topological%20Neural%20Networks&body=Title%3A%20E%28n%29%20Equivariant%20Topological%20Neural%20Networks%0AAuthor%3A%20Claudio%20Battiloro%20and%20Ege%20Karaismailo%C4%9Flu%20and%20Mauricio%20Tec%20and%20George%20Dasoulas%20and%20Michelle%20Audirac%20and%20Francesca%20Dominici%0AAbstract%3A%20%20%20Graph%20neural%20networks%20excel%20at%20modeling%20pairwise%20interactions%2C%20but%20they%0Acannot%20flexibly%20accommodate%20higher-order%20interactions%20and%20features.%20Topological%0Adeep%20learning%20%28TDL%29%20has%20emerged%20recently%20as%20a%20promising%20tool%20for%20addressing%0Athis%20issue.%20TDL%20enables%20the%20principled%20modeling%20of%20arbitrary%20multi-way%2C%0Ahierarchical%20higher-order%20interactions%20by%20operating%20on%20combinatorial%0Atopological%20spaces%2C%20such%20as%20simplicial%20or%20cell%20complexes%2C%20instead%20of%20graphs.%0AHowever%2C%20little%20is%20known%20about%20how%20to%20leverage%20geometric%20features%20such%20as%0Apositions%20and%20velocities%20for%20TDL.%20This%20paper%20introduces%20E%28n%29-Equivariant%0ATopological%20Neural%20Networks%20%28ETNNs%29%2C%20which%20are%20E%28n%29-equivariant%20message-passing%0Anetworks%20operating%20on%20combinatorial%20complexes%2C%20formal%20objects%20unifying%20graphs%2C%0Ahypergraphs%2C%20simplicial%2C%20path%2C%20and%20cell%20complexes.%20ETNNs%20incorporate%20geometric%0Anode%20features%20while%20respecting%20rotation%20and%20translation%20equivariance.%20Moreover%2C%0AETNNs%20are%20natively%20ready%20for%20settings%20with%20heterogeneous%20interactions.%20We%0Aprovide%20a%20theoretical%20analysis%20to%20show%20the%20improved%20expressiveness%20of%20ETNNs%0Aover%20architectures%20for%20geometric%20graphs.%20We%20also%20show%20how%20several%20E%28n%29%0Aequivariant%20variants%20of%20TDL%20models%20can%20be%20directly%20derived%20from%20our%20framework.%0AThe%20broad%20applicability%20of%20ETNNs%20is%20demonstrated%20through%20two%20tasks%20of%20vastly%0Adifferent%20nature%3A%20i%29%20molecular%20property%20prediction%20on%20the%20QM9%20benchmark%20and%20ii%29%0Aland-use%20regression%20for%20hyper-local%20estimation%20of%20air%20pollution%20with%0Amulti-resolution%20irregular%20geospatial%20data.%20The%20experiment%20results%20indicate%0Athat%20ETNNs%20are%20an%20effective%20tool%20for%20learning%20from%20diverse%20types%20of%20richly%0Astructured%20data%2C%20highlighting%20the%20benefits%20of%20principled%20geometric%20inductive%0Abias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE%2528n%2529%2520Equivariant%2520Topological%2520Neural%2520Networks%26entry.906535625%3DClaudio%2520Battiloro%2520and%2520Ege%2520Karaismailo%25C4%259Flu%2520and%2520Mauricio%2520Tec%2520and%2520George%2520Dasoulas%2520and%2520Michelle%2520Audirac%2520and%2520Francesca%2520Dominici%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520excel%2520at%2520modeling%2520pairwise%2520interactions%252C%2520but%2520they%250Acannot%2520flexibly%2520accommodate%2520higher-order%2520interactions%2520and%2520features.%2520Topological%250Adeep%2520learning%2520%2528TDL%2529%2520has%2520emerged%2520recently%2520as%2520a%2520promising%2520tool%2520for%2520addressing%250Athis%2520issue.%2520TDL%2520enables%2520the%2520principled%2520modeling%2520of%2520arbitrary%2520multi-way%252C%250Ahierarchical%2520higher-order%2520interactions%2520by%2520operating%2520on%2520combinatorial%250Atopological%2520spaces%252C%2520such%2520as%2520simplicial%2520or%2520cell%2520complexes%252C%2520instead%2520of%2520graphs.%250AHowever%252C%2520little%2520is%2520known%2520about%2520how%2520to%2520leverage%2520geometric%2520features%2520such%2520as%250Apositions%2520and%2520velocities%2520for%2520TDL.%2520This%2520paper%2520introduces%2520E%2528n%2529-Equivariant%250ATopological%2520Neural%2520Networks%2520%2528ETNNs%2529%252C%2520which%2520are%2520E%2528n%2529-equivariant%2520message-passing%250Anetworks%2520operating%2520on%2520combinatorial%2520complexes%252C%2520formal%2520objects%2520unifying%2520graphs%252C%250Ahypergraphs%252C%2520simplicial%252C%2520path%252C%2520and%2520cell%2520complexes.%2520ETNNs%2520incorporate%2520geometric%250Anode%2520features%2520while%2520respecting%2520rotation%2520and%2520translation%2520equivariance.%2520Moreover%252C%250AETNNs%2520are%2520natively%2520ready%2520for%2520settings%2520with%2520heterogeneous%2520interactions.%2520We%250Aprovide%2520a%2520theoretical%2520analysis%2520to%2520show%2520the%2520improved%2520expressiveness%2520of%2520ETNNs%250Aover%2520architectures%2520for%2520geometric%2520graphs.%2520We%2520also%2520show%2520how%2520several%2520E%2528n%2529%250Aequivariant%2520variants%2520of%2520TDL%2520models%2520can%2520be%2520directly%2520derived%2520from%2520our%2520framework.%250AThe%2520broad%2520applicability%2520of%2520ETNNs%2520is%2520demonstrated%2520through%2520two%2520tasks%2520of%2520vastly%250Adifferent%2520nature%253A%2520i%2529%2520molecular%2520property%2520prediction%2520on%2520the%2520QM9%2520benchmark%2520and%2520ii%2529%250Aland-use%2520regression%2520for%2520hyper-local%2520estimation%2520of%2520air%2520pollution%2520with%250Amulti-resolution%2520irregular%2520geospatial%2520data.%2520The%2520experiment%2520results%2520indicate%250Athat%2520ETNNs%2520are%2520an%2520effective%2520tool%2520for%2520learning%2520from%2520diverse%2520types%2520of%2520richly%250Astructured%2520data%252C%2520highlighting%2520the%2520benefits%2520of%2520principled%2520geometric%2520inductive%250Abias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E%28n%29%20Equivariant%20Topological%20Neural%20Networks&entry.906535625=Claudio%20Battiloro%20and%20Ege%20Karaismailo%C4%9Flu%20and%20Mauricio%20Tec%20and%20George%20Dasoulas%20and%20Michelle%20Audirac%20and%20Francesca%20Dominici&entry.1292438233=%20%20Graph%20neural%20networks%20excel%20at%20modeling%20pairwise%20interactions%2C%20but%20they%0Acannot%20flexibly%20accommodate%20higher-order%20interactions%20and%20features.%20Topological%0Adeep%20learning%20%28TDL%29%20has%20emerged%20recently%20as%20a%20promising%20tool%20for%20addressing%0Athis%20issue.%20TDL%20enables%20the%20principled%20modeling%20of%20arbitrary%20multi-way%2C%0Ahierarchical%20higher-order%20interactions%20by%20operating%20on%20combinatorial%0Atopological%20spaces%2C%20such%20as%20simplicial%20or%20cell%20complexes%2C%20instead%20of%20graphs.%0AHowever%2C%20little%20is%20known%20about%20how%20to%20leverage%20geometric%20features%20such%20as%0Apositions%20and%20velocities%20for%20TDL.%20This%20paper%20introduces%20E%28n%29-Equivariant%0ATopological%20Neural%20Networks%20%28ETNNs%29%2C%20which%20are%20E%28n%29-equivariant%20message-passing%0Anetworks%20operating%20on%20combinatorial%20complexes%2C%20formal%20objects%20unifying%20graphs%2C%0Ahypergraphs%2C%20simplicial%2C%20path%2C%20and%20cell%20complexes.%20ETNNs%20incorporate%20geometric%0Anode%20features%20while%20respecting%20rotation%20and%20translation%20equivariance.%20Moreover%2C%0AETNNs%20are%20natively%20ready%20for%20settings%20with%20heterogeneous%20interactions.%20We%0Aprovide%20a%20theoretical%20analysis%20to%20show%20the%20improved%20expressiveness%20of%20ETNNs%0Aover%20architectures%20for%20geometric%20graphs.%20We%20also%20show%20how%20several%20E%28n%29%0Aequivariant%20variants%20of%20TDL%20models%20can%20be%20directly%20derived%20from%20our%20framework.%0AThe%20broad%20applicability%20of%20ETNNs%20is%20demonstrated%20through%20two%20tasks%20of%20vastly%0Adifferent%20nature%3A%20i%29%20molecular%20property%20prediction%20on%20the%20QM9%20benchmark%20and%20ii%29%0Aland-use%20regression%20for%20hyper-local%20estimation%20of%20air%20pollution%20with%0Amulti-resolution%20irregular%20geospatial%20data.%20The%20experiment%20results%20indicate%0Athat%20ETNNs%20are%20an%20effective%20tool%20for%20learning%20from%20diverse%20types%20of%20richly%0Astructured%20data%2C%20highlighting%20the%20benefits%20of%20principled%20geometric%20inductive%0Abias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15429v1&entry.124074799=Read"},
{"title": "Scale-Invariant Feature Disentanglement via Adversarial Learning for\n  UAV-based Object Detection", "author": "Fan Liu and Liang Yao and Chuanyi Zhang and Ting Wu and Xinlei Zhang and Jun Zhou and Xiruo Jiang", "abstract": "  Detecting objects from Unmanned Aerial Vehicles (UAV) is often hindered by a\nlarge number of small objects, resulting in low detection accuracy. To address\nthis issue, mainstream approaches typically utilize multi-stage inferences.\nDespite their remarkable detecting accuracies, real-time efficiency is\nsacrificed, making them less practical to handle real applications. To this\nend, we propose to improve the single-stage inference accuracy through learning\nscale-invariant features. Specifically, a Scale-Invariant Feature Disentangling\nmodule is designed to disentangle scale-related and scale-invariant features.\nThen an Adversarial Feature Learning scheme is employed to enhance\ndisentanglement. Finally, scale-invariant features are leveraged for robust\nUAV-based object detection. Furthermore, we construct a multi-modal UAV object\ndetection dataset, State-Air, which incorporates annotated UAV state\nparameters. We apply our approach to three state-of-the-art lightweight\ndetection frameworks on three benchmark datasets, including State-Air.\nExtensive experiments demonstrate that our approach can effectively improve\nmodel accuracy. Our code and dataset are provided in Supplementary Materials\nand will be publicly available once the paper is accepted.\n", "link": "http://arxiv.org/abs/2405.15465v1", "date": "2024-05-24", "relevancy": 2.1527, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5647}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5376}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scale-Invariant%20Feature%20Disentanglement%20via%20Adversarial%20Learning%20for%0A%20%20UAV-based%20Object%20Detection&body=Title%3A%20Scale-Invariant%20Feature%20Disentanglement%20via%20Adversarial%20Learning%20for%0A%20%20UAV-based%20Object%20Detection%0AAuthor%3A%20Fan%20Liu%20and%20Liang%20Yao%20and%20Chuanyi%20Zhang%20and%20Ting%20Wu%20and%20Xinlei%20Zhang%20and%20Jun%20Zhou%20and%20Xiruo%20Jiang%0AAbstract%3A%20%20%20Detecting%20objects%20from%20Unmanned%20Aerial%20Vehicles%20%28UAV%29%20is%20often%20hindered%20by%20a%0Alarge%20number%20of%20small%20objects%2C%20resulting%20in%20low%20detection%20accuracy.%20To%20address%0Athis%20issue%2C%20mainstream%20approaches%20typically%20utilize%20multi-stage%20inferences.%0ADespite%20their%20remarkable%20detecting%20accuracies%2C%20real-time%20efficiency%20is%0Asacrificed%2C%20making%20them%20less%20practical%20to%20handle%20real%20applications.%20To%20this%0Aend%2C%20we%20propose%20to%20improve%20the%20single-stage%20inference%20accuracy%20through%20learning%0Ascale-invariant%20features.%20Specifically%2C%20a%20Scale-Invariant%20Feature%20Disentangling%0Amodule%20is%20designed%20to%20disentangle%20scale-related%20and%20scale-invariant%20features.%0AThen%20an%20Adversarial%20Feature%20Learning%20scheme%20is%20employed%20to%20enhance%0Adisentanglement.%20Finally%2C%20scale-invariant%20features%20are%20leveraged%20for%20robust%0AUAV-based%20object%20detection.%20Furthermore%2C%20we%20construct%20a%20multi-modal%20UAV%20object%0Adetection%20dataset%2C%20State-Air%2C%20which%20incorporates%20annotated%20UAV%20state%0Aparameters.%20We%20apply%20our%20approach%20to%20three%20state-of-the-art%20lightweight%0Adetection%20frameworks%20on%20three%20benchmark%20datasets%2C%20including%20State-Air.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20can%20effectively%20improve%0Amodel%20accuracy.%20Our%20code%20and%20dataset%20are%20provided%20in%20Supplementary%20Materials%0Aand%20will%20be%20publicly%20available%20once%20the%20paper%20is%20accepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScale-Invariant%2520Feature%2520Disentanglement%2520via%2520Adversarial%2520Learning%2520for%250A%2520%2520UAV-based%2520Object%2520Detection%26entry.906535625%3DFan%2520Liu%2520and%2520Liang%2520Yao%2520and%2520Chuanyi%2520Zhang%2520and%2520Ting%2520Wu%2520and%2520Xinlei%2520Zhang%2520and%2520Jun%2520Zhou%2520and%2520Xiruo%2520Jiang%26entry.1292438233%3D%2520%2520Detecting%2520objects%2520from%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAV%2529%2520is%2520often%2520hindered%2520by%2520a%250Alarge%2520number%2520of%2520small%2520objects%252C%2520resulting%2520in%2520low%2520detection%2520accuracy.%2520To%2520address%250Athis%2520issue%252C%2520mainstream%2520approaches%2520typically%2520utilize%2520multi-stage%2520inferences.%250ADespite%2520their%2520remarkable%2520detecting%2520accuracies%252C%2520real-time%2520efficiency%2520is%250Asacrificed%252C%2520making%2520them%2520less%2520practical%2520to%2520handle%2520real%2520applications.%2520To%2520this%250Aend%252C%2520we%2520propose%2520to%2520improve%2520the%2520single-stage%2520inference%2520accuracy%2520through%2520learning%250Ascale-invariant%2520features.%2520Specifically%252C%2520a%2520Scale-Invariant%2520Feature%2520Disentangling%250Amodule%2520is%2520designed%2520to%2520disentangle%2520scale-related%2520and%2520scale-invariant%2520features.%250AThen%2520an%2520Adversarial%2520Feature%2520Learning%2520scheme%2520is%2520employed%2520to%2520enhance%250Adisentanglement.%2520Finally%252C%2520scale-invariant%2520features%2520are%2520leveraged%2520for%2520robust%250AUAV-based%2520object%2520detection.%2520Furthermore%252C%2520we%2520construct%2520a%2520multi-modal%2520UAV%2520object%250Adetection%2520dataset%252C%2520State-Air%252C%2520which%2520incorporates%2520annotated%2520UAV%2520state%250Aparameters.%2520We%2520apply%2520our%2520approach%2520to%2520three%2520state-of-the-art%2520lightweight%250Adetection%2520frameworks%2520on%2520three%2520benchmark%2520datasets%252C%2520including%2520State-Air.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520can%2520effectively%2520improve%250Amodel%2520accuracy.%2520Our%2520code%2520and%2520dataset%2520are%2520provided%2520in%2520Supplementary%2520Materials%250Aand%2520will%2520be%2520publicly%2520available%2520once%2520the%2520paper%2520is%2520accepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scale-Invariant%20Feature%20Disentanglement%20via%20Adversarial%20Learning%20for%0A%20%20UAV-based%20Object%20Detection&entry.906535625=Fan%20Liu%20and%20Liang%20Yao%20and%20Chuanyi%20Zhang%20and%20Ting%20Wu%20and%20Xinlei%20Zhang%20and%20Jun%20Zhou%20and%20Xiruo%20Jiang&entry.1292438233=%20%20Detecting%20objects%20from%20Unmanned%20Aerial%20Vehicles%20%28UAV%29%20is%20often%20hindered%20by%20a%0Alarge%20number%20of%20small%20objects%2C%20resulting%20in%20low%20detection%20accuracy.%20To%20address%0Athis%20issue%2C%20mainstream%20approaches%20typically%20utilize%20multi-stage%20inferences.%0ADespite%20their%20remarkable%20detecting%20accuracies%2C%20real-time%20efficiency%20is%0Asacrificed%2C%20making%20them%20less%20practical%20to%20handle%20real%20applications.%20To%20this%0Aend%2C%20we%20propose%20to%20improve%20the%20single-stage%20inference%20accuracy%20through%20learning%0Ascale-invariant%20features.%20Specifically%2C%20a%20Scale-Invariant%20Feature%20Disentangling%0Amodule%20is%20designed%20to%20disentangle%20scale-related%20and%20scale-invariant%20features.%0AThen%20an%20Adversarial%20Feature%20Learning%20scheme%20is%20employed%20to%20enhance%0Adisentanglement.%20Finally%2C%20scale-invariant%20features%20are%20leveraged%20for%20robust%0AUAV-based%20object%20detection.%20Furthermore%2C%20we%20construct%20a%20multi-modal%20UAV%20object%0Adetection%20dataset%2C%20State-Air%2C%20which%20incorporates%20annotated%20UAV%20state%0Aparameters.%20We%20apply%20our%20approach%20to%20three%20state-of-the-art%20lightweight%0Adetection%20frameworks%20on%20three%20benchmark%20datasets%2C%20including%20State-Air.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20can%20effectively%20improve%0Amodel%20accuracy.%20Our%20code%20and%20dataset%20are%20provided%20in%20Supplementary%20Materials%0Aand%20will%20be%20publicly%20available%20once%20the%20paper%20is%20accepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15465v1&entry.124074799=Read"},
{"title": "Exploring Interactive Semantic Alignment for Efficient HOI Detection\n  with Vision-language Model", "author": "Jihao Dong and Renjie Pan and Hua Yang", "abstract": "  Human-Object Interaction (HOI) detection aims to localize human-object pairs\nand comprehend their interactions. Recently, two-stage transformer-based\nmethods have demonstrated competitive performance. However, these methods\nfrequently focus on object appearance features and ignore global contextual\ninformation. Besides, vision-language model CLIP which effectively aligns\nvisual and text embeddings has shown great potential in zero-shot HOI\ndetection. Based on the former facts, We introduce a novel HOI detector named\nISA-HOI, which extensively leverages knowledge from CLIP, aligning interactive\nsemantics between visual and textual features. We first extract global context\nof image and local features of object to Improve interaction Features in images\n(IF). On the other hand, we propose a Verb Semantic Improvement (VSI) module to\nenhance textual features of verb labels via cross-modal fusion. Ultimately, our\nmethod achieves competitive results on the HICO-DET and V-COCO benchmarks with\nmuch fewer training epochs, and outperforms the state-of-the-art under\nzero-shot settings.\n", "link": "http://arxiv.org/abs/2404.12678v3", "date": "2024-05-24", "relevancy": 2.1442, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5351}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Interactive%20Semantic%20Alignment%20for%20Efficient%20HOI%20Detection%0A%20%20with%20Vision-language%20Model&body=Title%3A%20Exploring%20Interactive%20Semantic%20Alignment%20for%20Efficient%20HOI%20Detection%0A%20%20with%20Vision-language%20Model%0AAuthor%3A%20Jihao%20Dong%20and%20Renjie%20Pan%20and%20Hua%20Yang%0AAbstract%3A%20%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20aims%20to%20localize%20human-object%20pairs%0Aand%20comprehend%20their%20interactions.%20Recently%2C%20two-stage%20transformer-based%0Amethods%20have%20demonstrated%20competitive%20performance.%20However%2C%20these%20methods%0Afrequently%20focus%20on%20object%20appearance%20features%20and%20ignore%20global%20contextual%0Ainformation.%20Besides%2C%20vision-language%20model%20CLIP%20which%20effectively%20aligns%0Avisual%20and%20text%20embeddings%20has%20shown%20great%20potential%20in%20zero-shot%20HOI%0Adetection.%20Based%20on%20the%20former%20facts%2C%20We%20introduce%20a%20novel%20HOI%20detector%20named%0AISA-HOI%2C%20which%20extensively%20leverages%20knowledge%20from%20CLIP%2C%20aligning%20interactive%0Asemantics%20between%20visual%20and%20textual%20features.%20We%20first%20extract%20global%20context%0Aof%20image%20and%20local%20features%20of%20object%20to%20Improve%20interaction%20Features%20in%20images%0A%28IF%29.%20On%20the%20other%20hand%2C%20we%20propose%20a%20Verb%20Semantic%20Improvement%20%28VSI%29%20module%20to%0Aenhance%20textual%20features%20of%20verb%20labels%20via%20cross-modal%20fusion.%20Ultimately%2C%20our%0Amethod%20achieves%20competitive%20results%20on%20the%20HICO-DET%20and%20V-COCO%20benchmarks%20with%0Amuch%20fewer%20training%20epochs%2C%20and%20outperforms%20the%20state-of-the-art%20under%0Azero-shot%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12678v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Interactive%2520Semantic%2520Alignment%2520for%2520Efficient%2520HOI%2520Detection%250A%2520%2520with%2520Vision-language%2520Model%26entry.906535625%3DJihao%2520Dong%2520and%2520Renjie%2520Pan%2520and%2520Hua%2520Yang%26entry.1292438233%3D%2520%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520detection%2520aims%2520to%2520localize%2520human-object%2520pairs%250Aand%2520comprehend%2520their%2520interactions.%2520Recently%252C%2520two-stage%2520transformer-based%250Amethods%2520have%2520demonstrated%2520competitive%2520performance.%2520However%252C%2520these%2520methods%250Afrequently%2520focus%2520on%2520object%2520appearance%2520features%2520and%2520ignore%2520global%2520contextual%250Ainformation.%2520Besides%252C%2520vision-language%2520model%2520CLIP%2520which%2520effectively%2520aligns%250Avisual%2520and%2520text%2520embeddings%2520has%2520shown%2520great%2520potential%2520in%2520zero-shot%2520HOI%250Adetection.%2520Based%2520on%2520the%2520former%2520facts%252C%2520We%2520introduce%2520a%2520novel%2520HOI%2520detector%2520named%250AISA-HOI%252C%2520which%2520extensively%2520leverages%2520knowledge%2520from%2520CLIP%252C%2520aligning%2520interactive%250Asemantics%2520between%2520visual%2520and%2520textual%2520features.%2520We%2520first%2520extract%2520global%2520context%250Aof%2520image%2520and%2520local%2520features%2520of%2520object%2520to%2520Improve%2520interaction%2520Features%2520in%2520images%250A%2528IF%2529.%2520On%2520the%2520other%2520hand%252C%2520we%2520propose%2520a%2520Verb%2520Semantic%2520Improvement%2520%2528VSI%2529%2520module%2520to%250Aenhance%2520textual%2520features%2520of%2520verb%2520labels%2520via%2520cross-modal%2520fusion.%2520Ultimately%252C%2520our%250Amethod%2520achieves%2520competitive%2520results%2520on%2520the%2520HICO-DET%2520and%2520V-COCO%2520benchmarks%2520with%250Amuch%2520fewer%2520training%2520epochs%252C%2520and%2520outperforms%2520the%2520state-of-the-art%2520under%250Azero-shot%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12678v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Interactive%20Semantic%20Alignment%20for%20Efficient%20HOI%20Detection%0A%20%20with%20Vision-language%20Model&entry.906535625=Jihao%20Dong%20and%20Renjie%20Pan%20and%20Hua%20Yang&entry.1292438233=%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20aims%20to%20localize%20human-object%20pairs%0Aand%20comprehend%20their%20interactions.%20Recently%2C%20two-stage%20transformer-based%0Amethods%20have%20demonstrated%20competitive%20performance.%20However%2C%20these%20methods%0Afrequently%20focus%20on%20object%20appearance%20features%20and%20ignore%20global%20contextual%0Ainformation.%20Besides%2C%20vision-language%20model%20CLIP%20which%20effectively%20aligns%0Avisual%20and%20text%20embeddings%20has%20shown%20great%20potential%20in%20zero-shot%20HOI%0Adetection.%20Based%20on%20the%20former%20facts%2C%20We%20introduce%20a%20novel%20HOI%20detector%20named%0AISA-HOI%2C%20which%20extensively%20leverages%20knowledge%20from%20CLIP%2C%20aligning%20interactive%0Asemantics%20between%20visual%20and%20textual%20features.%20We%20first%20extract%20global%20context%0Aof%20image%20and%20local%20features%20of%20object%20to%20Improve%20interaction%20Features%20in%20images%0A%28IF%29.%20On%20the%20other%20hand%2C%20we%20propose%20a%20Verb%20Semantic%20Improvement%20%28VSI%29%20module%20to%0Aenhance%20textual%20features%20of%20verb%20labels%20via%20cross-modal%20fusion.%20Ultimately%2C%20our%0Amethod%20achieves%20competitive%20results%20on%20the%20HICO-DET%20and%20V-COCO%20benchmarks%20with%0Amuch%20fewer%20training%20epochs%2C%20and%20outperforms%20the%20state-of-the-art%20under%0Azero-shot%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12678v3&entry.124074799=Read"},
{"title": "Beyond Literal Descriptions: Understanding and Locating Open-World\n  Objects Aligned with Human Intentions", "author": "Wenxuan Wang and Yisi Zhang and Xingjian He and Yichen Yan and Zijia Zhao and Xinlong Wang and Jing Liu", "abstract": "  Visual grounding (VG) aims at locating the foreground entities that match the\ngiven natural language expressions. Previous datasets and methods for classic\nVG task mainly rely on the prior assumption that the given expression must\nliterally refer to the target object, which greatly impedes the practical\ndeployment of agents in real-world scenarios. Since users usually prefer to\nprovide intention-based expression for the desired object instead of covering\nall the details, it is necessary for the agents to interpret the\nintention-driven instructions. Thus, in this work, we take a step further to\nthe intention-driven visual-language (V-L) understanding. To promote classic VG\ntowards human intention interpretation, we propose a new intention-driven\nvisual grounding (IVG) task and build a large-scale IVG dataset termed\nIntentionVG with free-form intention expressions. Considering that practical\nagents need to move and find specific targets among various scenarios to\nrealize the grounding task, our IVG task and IntentionVG dataset have taken the\ncrucial properties of both multi-scenario perception and egocentric view into\nconsideration. Besides, various types of models are set up as the baselines to\nrealize our IVG task. Extensive experiments on our IntentionVG dataset and\nbaselines demonstrate the necessity and efficacy of our method for the V-L\nfield. To foster future research in this direction, our newly built dataset and\nbaselines will be publicly available at https://github.com/Rubics-Xuan/IVG.\n", "link": "http://arxiv.org/abs/2402.11265v2", "date": "2024-05-24", "relevancy": 2.1426, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5503}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5436}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Literal%20Descriptions%3A%20Understanding%20and%20Locating%20Open-World%0A%20%20Objects%20Aligned%20with%20Human%20Intentions&body=Title%3A%20Beyond%20Literal%20Descriptions%3A%20Understanding%20and%20Locating%20Open-World%0A%20%20Objects%20Aligned%20with%20Human%20Intentions%0AAuthor%3A%20Wenxuan%20Wang%20and%20Yisi%20Zhang%20and%20Xingjian%20He%20and%20Yichen%20Yan%20and%20Zijia%20Zhao%20and%20Xinlong%20Wang%20and%20Jing%20Liu%0AAbstract%3A%20%20%20Visual%20grounding%20%28VG%29%20aims%20at%20locating%20the%20foreground%20entities%20that%20match%20the%0Agiven%20natural%20language%20expressions.%20Previous%20datasets%20and%20methods%20for%20classic%0AVG%20task%20mainly%20rely%20on%20the%20prior%20assumption%20that%20the%20given%20expression%20must%0Aliterally%20refer%20to%20the%20target%20object%2C%20which%20greatly%20impedes%20the%20practical%0Adeployment%20of%20agents%20in%20real-world%20scenarios.%20Since%20users%20usually%20prefer%20to%0Aprovide%20intention-based%20expression%20for%20the%20desired%20object%20instead%20of%20covering%0Aall%20the%20details%2C%20it%20is%20necessary%20for%20the%20agents%20to%20interpret%20the%0Aintention-driven%20instructions.%20Thus%2C%20in%20this%20work%2C%20we%20take%20a%20step%20further%20to%0Athe%20intention-driven%20visual-language%20%28V-L%29%20understanding.%20To%20promote%20classic%20VG%0Atowards%20human%20intention%20interpretation%2C%20we%20propose%20a%20new%20intention-driven%0Avisual%20grounding%20%28IVG%29%20task%20and%20build%20a%20large-scale%20IVG%20dataset%20termed%0AIntentionVG%20with%20free-form%20intention%20expressions.%20Considering%20that%20practical%0Aagents%20need%20to%20move%20and%20find%20specific%20targets%20among%20various%20scenarios%20to%0Arealize%20the%20grounding%20task%2C%20our%20IVG%20task%20and%20IntentionVG%20dataset%20have%20taken%20the%0Acrucial%20properties%20of%20both%20multi-scenario%20perception%20and%20egocentric%20view%20into%0Aconsideration.%20Besides%2C%20various%20types%20of%20models%20are%20set%20up%20as%20the%20baselines%20to%0Arealize%20our%20IVG%20task.%20Extensive%20experiments%20on%20our%20IntentionVG%20dataset%20and%0Abaselines%20demonstrate%20the%20necessity%20and%20efficacy%20of%20our%20method%20for%20the%20V-L%0Afield.%20To%20foster%20future%20research%20in%20this%20direction%2C%20our%20newly%20built%20dataset%20and%0Abaselines%20will%20be%20publicly%20available%20at%20https%3A//github.com/Rubics-Xuan/IVG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Literal%2520Descriptions%253A%2520Understanding%2520and%2520Locating%2520Open-World%250A%2520%2520Objects%2520Aligned%2520with%2520Human%2520Intentions%26entry.906535625%3DWenxuan%2520Wang%2520and%2520Yisi%2520Zhang%2520and%2520Xingjian%2520He%2520and%2520Yichen%2520Yan%2520and%2520Zijia%2520Zhao%2520and%2520Xinlong%2520Wang%2520and%2520Jing%2520Liu%26entry.1292438233%3D%2520%2520Visual%2520grounding%2520%2528VG%2529%2520aims%2520at%2520locating%2520the%2520foreground%2520entities%2520that%2520match%2520the%250Agiven%2520natural%2520language%2520expressions.%2520Previous%2520datasets%2520and%2520methods%2520for%2520classic%250AVG%2520task%2520mainly%2520rely%2520on%2520the%2520prior%2520assumption%2520that%2520the%2520given%2520expression%2520must%250Aliterally%2520refer%2520to%2520the%2520target%2520object%252C%2520which%2520greatly%2520impedes%2520the%2520practical%250Adeployment%2520of%2520agents%2520in%2520real-world%2520scenarios.%2520Since%2520users%2520usually%2520prefer%2520to%250Aprovide%2520intention-based%2520expression%2520for%2520the%2520desired%2520object%2520instead%2520of%2520covering%250Aall%2520the%2520details%252C%2520it%2520is%2520necessary%2520for%2520the%2520agents%2520to%2520interpret%2520the%250Aintention-driven%2520instructions.%2520Thus%252C%2520in%2520this%2520work%252C%2520we%2520take%2520a%2520step%2520further%2520to%250Athe%2520intention-driven%2520visual-language%2520%2528V-L%2529%2520understanding.%2520To%2520promote%2520classic%2520VG%250Atowards%2520human%2520intention%2520interpretation%252C%2520we%2520propose%2520a%2520new%2520intention-driven%250Avisual%2520grounding%2520%2528IVG%2529%2520task%2520and%2520build%2520a%2520large-scale%2520IVG%2520dataset%2520termed%250AIntentionVG%2520with%2520free-form%2520intention%2520expressions.%2520Considering%2520that%2520practical%250Aagents%2520need%2520to%2520move%2520and%2520find%2520specific%2520targets%2520among%2520various%2520scenarios%2520to%250Arealize%2520the%2520grounding%2520task%252C%2520our%2520IVG%2520task%2520and%2520IntentionVG%2520dataset%2520have%2520taken%2520the%250Acrucial%2520properties%2520of%2520both%2520multi-scenario%2520perception%2520and%2520egocentric%2520view%2520into%250Aconsideration.%2520Besides%252C%2520various%2520types%2520of%2520models%2520are%2520set%2520up%2520as%2520the%2520baselines%2520to%250Arealize%2520our%2520IVG%2520task.%2520Extensive%2520experiments%2520on%2520our%2520IntentionVG%2520dataset%2520and%250Abaselines%2520demonstrate%2520the%2520necessity%2520and%2520efficacy%2520of%2520our%2520method%2520for%2520the%2520V-L%250Afield.%2520To%2520foster%2520future%2520research%2520in%2520this%2520direction%252C%2520our%2520newly%2520built%2520dataset%2520and%250Abaselines%2520will%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/Rubics-Xuan/IVG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Literal%20Descriptions%3A%20Understanding%20and%20Locating%20Open-World%0A%20%20Objects%20Aligned%20with%20Human%20Intentions&entry.906535625=Wenxuan%20Wang%20and%20Yisi%20Zhang%20and%20Xingjian%20He%20and%20Yichen%20Yan%20and%20Zijia%20Zhao%20and%20Xinlong%20Wang%20and%20Jing%20Liu&entry.1292438233=%20%20Visual%20grounding%20%28VG%29%20aims%20at%20locating%20the%20foreground%20entities%20that%20match%20the%0Agiven%20natural%20language%20expressions.%20Previous%20datasets%20and%20methods%20for%20classic%0AVG%20task%20mainly%20rely%20on%20the%20prior%20assumption%20that%20the%20given%20expression%20must%0Aliterally%20refer%20to%20the%20target%20object%2C%20which%20greatly%20impedes%20the%20practical%0Adeployment%20of%20agents%20in%20real-world%20scenarios.%20Since%20users%20usually%20prefer%20to%0Aprovide%20intention-based%20expression%20for%20the%20desired%20object%20instead%20of%20covering%0Aall%20the%20details%2C%20it%20is%20necessary%20for%20the%20agents%20to%20interpret%20the%0Aintention-driven%20instructions.%20Thus%2C%20in%20this%20work%2C%20we%20take%20a%20step%20further%20to%0Athe%20intention-driven%20visual-language%20%28V-L%29%20understanding.%20To%20promote%20classic%20VG%0Atowards%20human%20intention%20interpretation%2C%20we%20propose%20a%20new%20intention-driven%0Avisual%20grounding%20%28IVG%29%20task%20and%20build%20a%20large-scale%20IVG%20dataset%20termed%0AIntentionVG%20with%20free-form%20intention%20expressions.%20Considering%20that%20practical%0Aagents%20need%20to%20move%20and%20find%20specific%20targets%20among%20various%20scenarios%20to%0Arealize%20the%20grounding%20task%2C%20our%20IVG%20task%20and%20IntentionVG%20dataset%20have%20taken%20the%0Acrucial%20properties%20of%20both%20multi-scenario%20perception%20and%20egocentric%20view%20into%0Aconsideration.%20Besides%2C%20various%20types%20of%20models%20are%20set%20up%20as%20the%20baselines%20to%0Arealize%20our%20IVG%20task.%20Extensive%20experiments%20on%20our%20IntentionVG%20dataset%20and%0Abaselines%20demonstrate%20the%20necessity%20and%20efficacy%20of%20our%20method%20for%20the%20V-L%0Afield.%20To%20foster%20future%20research%20in%20this%20direction%2C%20our%20newly%20built%20dataset%20and%0Abaselines%20will%20be%20publicly%20available%20at%20https%3A//github.com/Rubics-Xuan/IVG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11265v2&entry.124074799=Read"},
{"title": "LM4LV: A Frozen Large Language Model for Low-level Vision Tasks", "author": "Boyang Zheng and Jinjin Gu and Shijun Li and Chao Dong", "abstract": "  The success of large language models (LLMs) has fostered a new research trend\nof multi-modality large language models (MLLMs), which changes the paradigm of\nvarious fields in computer vision. Though MLLMs have shown promising results in\nnumerous high-level vision and vision-language tasks such as VQA and\ntext-to-image, no works have demonstrated how low-level vision tasks can\nbenefit from MLLMs. We find that most current MLLMs are blind to low-level\nfeatures due to their design of vision modules, thus are inherently incapable\nfor solving low-level vision tasks. In this work, we purpose $\\textbf{LM4LV}$,\na framework that enables a FROZEN LLM to solve a range of low-level vision\ntasks without any multi-modal data or prior. This showcases the LLM's strong\npotential in low-level vision and bridges the gap between MLLMs and low-level\nvision tasks. We hope this work can inspire new perspectives on LLMs and deeper\nunderstanding of their mechanisms.\n", "link": "http://arxiv.org/abs/2405.15734v1", "date": "2024-05-24", "relevancy": 2.1344, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5301}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LM4LV%3A%20A%20Frozen%20Large%20Language%20Model%20for%20Low-level%20Vision%20Tasks&body=Title%3A%20LM4LV%3A%20A%20Frozen%20Large%20Language%20Model%20for%20Low-level%20Vision%20Tasks%0AAuthor%3A%20Boyang%20Zheng%20and%20Jinjin%20Gu%20and%20Shijun%20Li%20and%20Chao%20Dong%0AAbstract%3A%20%20%20The%20success%20of%20large%20language%20models%20%28LLMs%29%20has%20fostered%20a%20new%20research%20trend%0Aof%20multi-modality%20large%20language%20models%20%28MLLMs%29%2C%20which%20changes%20the%20paradigm%20of%0Avarious%20fields%20in%20computer%20vision.%20Though%20MLLMs%20have%20shown%20promising%20results%20in%0Anumerous%20high-level%20vision%20and%20vision-language%20tasks%20such%20as%20VQA%20and%0Atext-to-image%2C%20no%20works%20have%20demonstrated%20how%20low-level%20vision%20tasks%20can%0Abenefit%20from%20MLLMs.%20We%20find%20that%20most%20current%20MLLMs%20are%20blind%20to%20low-level%0Afeatures%20due%20to%20their%20design%20of%20vision%20modules%2C%20thus%20are%20inherently%20incapable%0Afor%20solving%20low-level%20vision%20tasks.%20In%20this%20work%2C%20we%20purpose%20%24%5Ctextbf%7BLM4LV%7D%24%2C%0Aa%20framework%20that%20enables%20a%20FROZEN%20LLM%20to%20solve%20a%20range%20of%20low-level%20vision%0Atasks%20without%20any%20multi-modal%20data%20or%20prior.%20This%20showcases%20the%20LLM%27s%20strong%0Apotential%20in%20low-level%20vision%20and%20bridges%20the%20gap%20between%20MLLMs%20and%20low-level%0Avision%20tasks.%20We%20hope%20this%20work%20can%20inspire%20new%20perspectives%20on%20LLMs%20and%20deeper%0Aunderstanding%20of%20their%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLM4LV%253A%2520A%2520Frozen%2520Large%2520Language%2520Model%2520for%2520Low-level%2520Vision%2520Tasks%26entry.906535625%3DBoyang%2520Zheng%2520and%2520Jinjin%2520Gu%2520and%2520Shijun%2520Li%2520and%2520Chao%2520Dong%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520fostered%2520a%2520new%2520research%2520trend%250Aof%2520multi-modality%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520which%2520changes%2520the%2520paradigm%2520of%250Avarious%2520fields%2520in%2520computer%2520vision.%2520Though%2520MLLMs%2520have%2520shown%2520promising%2520results%2520in%250Anumerous%2520high-level%2520vision%2520and%2520vision-language%2520tasks%2520such%2520as%2520VQA%2520and%250Atext-to-image%252C%2520no%2520works%2520have%2520demonstrated%2520how%2520low-level%2520vision%2520tasks%2520can%250Abenefit%2520from%2520MLLMs.%2520We%2520find%2520that%2520most%2520current%2520MLLMs%2520are%2520blind%2520to%2520low-level%250Afeatures%2520due%2520to%2520their%2520design%2520of%2520vision%2520modules%252C%2520thus%2520are%2520inherently%2520incapable%250Afor%2520solving%2520low-level%2520vision%2520tasks.%2520In%2520this%2520work%252C%2520we%2520purpose%2520%2524%255Ctextbf%257BLM4LV%257D%2524%252C%250Aa%2520framework%2520that%2520enables%2520a%2520FROZEN%2520LLM%2520to%2520solve%2520a%2520range%2520of%2520low-level%2520vision%250Atasks%2520without%2520any%2520multi-modal%2520data%2520or%2520prior.%2520This%2520showcases%2520the%2520LLM%2527s%2520strong%250Apotential%2520in%2520low-level%2520vision%2520and%2520bridges%2520the%2520gap%2520between%2520MLLMs%2520and%2520low-level%250Avision%2520tasks.%2520We%2520hope%2520this%2520work%2520can%2520inspire%2520new%2520perspectives%2520on%2520LLMs%2520and%2520deeper%250Aunderstanding%2520of%2520their%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LM4LV%3A%20A%20Frozen%20Large%20Language%20Model%20for%20Low-level%20Vision%20Tasks&entry.906535625=Boyang%20Zheng%20and%20Jinjin%20Gu%20and%20Shijun%20Li%20and%20Chao%20Dong&entry.1292438233=%20%20The%20success%20of%20large%20language%20models%20%28LLMs%29%20has%20fostered%20a%20new%20research%20trend%0Aof%20multi-modality%20large%20language%20models%20%28MLLMs%29%2C%20which%20changes%20the%20paradigm%20of%0Avarious%20fields%20in%20computer%20vision.%20Though%20MLLMs%20have%20shown%20promising%20results%20in%0Anumerous%20high-level%20vision%20and%20vision-language%20tasks%20such%20as%20VQA%20and%0Atext-to-image%2C%20no%20works%20have%20demonstrated%20how%20low-level%20vision%20tasks%20can%0Abenefit%20from%20MLLMs.%20We%20find%20that%20most%20current%20MLLMs%20are%20blind%20to%20low-level%0Afeatures%20due%20to%20their%20design%20of%20vision%20modules%2C%20thus%20are%20inherently%20incapable%0Afor%20solving%20low-level%20vision%20tasks.%20In%20this%20work%2C%20we%20purpose%20%24%5Ctextbf%7BLM4LV%7D%24%2C%0Aa%20framework%20that%20enables%20a%20FROZEN%20LLM%20to%20solve%20a%20range%20of%20low-level%20vision%0Atasks%20without%20any%20multi-modal%20data%20or%20prior.%20This%20showcases%20the%20LLM%27s%20strong%0Apotential%20in%20low-level%20vision%20and%20bridges%20the%20gap%20between%20MLLMs%20and%20low-level%0Avision%20tasks.%20We%20hope%20this%20work%20can%20inspire%20new%20perspectives%20on%20LLMs%20and%20deeper%0Aunderstanding%20of%20their%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15734v1&entry.124074799=Read"},
{"title": "Freya PAGE: First Optimal Time Complexity for Large-Scale Nonconvex\n  Finite-Sum Optimization with Heterogeneous Asynchronous Computations", "author": "Alexander Tyurin and Kaja Gruntkowska and Peter Richt\u00e1rik", "abstract": "  In practical distributed systems, workers are typically not homogeneous, and\ndue to differences in hardware configurations and network conditions, can have\nhighly varying processing times. We consider smooth nonconvex finite-sum\n(empirical risk minimization) problems in this setup and introduce a new\nparallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and\nasynchronous computations. By being robust to \"stragglers\" and adaptively\nignoring slow computations, Freya PAGE offers significantly improved time\ncomplexity guarantees compared to all previous methods, including Asynchronous\nSGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions. The\nalgorithm relies on novel generic stochastic gradient collection strategies\nwith theoretical guarantees that can be of interest on their own, and may be\nused in the design of future optimization methods. Furthermore, we establish a\nlower bound for smooth nonconvex finite-sum problems in the asynchronous setup,\nproviding a fundamental time complexity limit. This lower bound is tight and\ndemonstrates the optimality of Freya PAGE in the large-scale regime, i.e., when\n$\\sqrt{m} \\geq n$, where $n$ is # of workers, and $m$ is # of data samples.\n", "link": "http://arxiv.org/abs/2405.15545v1", "date": "2024-05-24", "relevancy": 2.1259, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4435}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4203}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Freya%20PAGE%3A%20First%20Optimal%20Time%20Complexity%20for%20Large-Scale%20Nonconvex%0A%20%20Finite-Sum%20Optimization%20with%20Heterogeneous%20Asynchronous%20Computations&body=Title%3A%20Freya%20PAGE%3A%20First%20Optimal%20Time%20Complexity%20for%20Large-Scale%20Nonconvex%0A%20%20Finite-Sum%20Optimization%20with%20Heterogeneous%20Asynchronous%20Computations%0AAuthor%3A%20Alexander%20Tyurin%20and%20Kaja%20Gruntkowska%20and%20Peter%20Richt%C3%A1rik%0AAbstract%3A%20%20%20In%20practical%20distributed%20systems%2C%20workers%20are%20typically%20not%20homogeneous%2C%20and%0Adue%20to%20differences%20in%20hardware%20configurations%20and%20network%20conditions%2C%20can%20have%0Ahighly%20varying%20processing%20times.%20We%20consider%20smooth%20nonconvex%20finite-sum%0A%28empirical%20risk%20minimization%29%20problems%20in%20this%20setup%20and%20introduce%20a%20new%0Aparallel%20method%2C%20Freya%20PAGE%2C%20designed%20to%20handle%20arbitrarily%20heterogeneous%20and%0Aasynchronous%20computations.%20By%20being%20robust%20to%20%22stragglers%22%20and%20adaptively%0Aignoring%20slow%20computations%2C%20Freya%20PAGE%20offers%20significantly%20improved%20time%0Acomplexity%20guarantees%20compared%20to%20all%20previous%20methods%2C%20including%20Asynchronous%0ASGD%2C%20Rennala%20SGD%2C%20SPIDER%2C%20and%20PAGE%2C%20while%20requiring%20weaker%20assumptions.%20The%0Aalgorithm%20relies%20on%20novel%20generic%20stochastic%20gradient%20collection%20strategies%0Awith%20theoretical%20guarantees%20that%20can%20be%20of%20interest%20on%20their%20own%2C%20and%20may%20be%0Aused%20in%20the%20design%20of%20future%20optimization%20methods.%20Furthermore%2C%20we%20establish%20a%0Alower%20bound%20for%20smooth%20nonconvex%20finite-sum%20problems%20in%20the%20asynchronous%20setup%2C%0Aproviding%20a%20fundamental%20time%20complexity%20limit.%20This%20lower%20bound%20is%20tight%20and%0Ademonstrates%20the%20optimality%20of%20Freya%20PAGE%20in%20the%20large-scale%20regime%2C%20i.e.%2C%20when%0A%24%5Csqrt%7Bm%7D%20%5Cgeq%20n%24%2C%20where%20%24n%24%20is%20%23%20of%20workers%2C%20and%20%24m%24%20is%20%23%20of%20data%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreya%2520PAGE%253A%2520First%2520Optimal%2520Time%2520Complexity%2520for%2520Large-Scale%2520Nonconvex%250A%2520%2520Finite-Sum%2520Optimization%2520with%2520Heterogeneous%2520Asynchronous%2520Computations%26entry.906535625%3DAlexander%2520Tyurin%2520and%2520Kaja%2520Gruntkowska%2520and%2520Peter%2520Richt%25C3%25A1rik%26entry.1292438233%3D%2520%2520In%2520practical%2520distributed%2520systems%252C%2520workers%2520are%2520typically%2520not%2520homogeneous%252C%2520and%250Adue%2520to%2520differences%2520in%2520hardware%2520configurations%2520and%2520network%2520conditions%252C%2520can%2520have%250Ahighly%2520varying%2520processing%2520times.%2520We%2520consider%2520smooth%2520nonconvex%2520finite-sum%250A%2528empirical%2520risk%2520minimization%2529%2520problems%2520in%2520this%2520setup%2520and%2520introduce%2520a%2520new%250Aparallel%2520method%252C%2520Freya%2520PAGE%252C%2520designed%2520to%2520handle%2520arbitrarily%2520heterogeneous%2520and%250Aasynchronous%2520computations.%2520By%2520being%2520robust%2520to%2520%2522stragglers%2522%2520and%2520adaptively%250Aignoring%2520slow%2520computations%252C%2520Freya%2520PAGE%2520offers%2520significantly%2520improved%2520time%250Acomplexity%2520guarantees%2520compared%2520to%2520all%2520previous%2520methods%252C%2520including%2520Asynchronous%250ASGD%252C%2520Rennala%2520SGD%252C%2520SPIDER%252C%2520and%2520PAGE%252C%2520while%2520requiring%2520weaker%2520assumptions.%2520The%250Aalgorithm%2520relies%2520on%2520novel%2520generic%2520stochastic%2520gradient%2520collection%2520strategies%250Awith%2520theoretical%2520guarantees%2520that%2520can%2520be%2520of%2520interest%2520on%2520their%2520own%252C%2520and%2520may%2520be%250Aused%2520in%2520the%2520design%2520of%2520future%2520optimization%2520methods.%2520Furthermore%252C%2520we%2520establish%2520a%250Alower%2520bound%2520for%2520smooth%2520nonconvex%2520finite-sum%2520problems%2520in%2520the%2520asynchronous%2520setup%252C%250Aproviding%2520a%2520fundamental%2520time%2520complexity%2520limit.%2520This%2520lower%2520bound%2520is%2520tight%2520and%250Ademonstrates%2520the%2520optimality%2520of%2520Freya%2520PAGE%2520in%2520the%2520large-scale%2520regime%252C%2520i.e.%252C%2520when%250A%2524%255Csqrt%257Bm%257D%2520%255Cgeq%2520n%2524%252C%2520where%2520%2524n%2524%2520is%2520%2523%2520of%2520workers%252C%2520and%2520%2524m%2524%2520is%2520%2523%2520of%2520data%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Freya%20PAGE%3A%20First%20Optimal%20Time%20Complexity%20for%20Large-Scale%20Nonconvex%0A%20%20Finite-Sum%20Optimization%20with%20Heterogeneous%20Asynchronous%20Computations&entry.906535625=Alexander%20Tyurin%20and%20Kaja%20Gruntkowska%20and%20Peter%20Richt%C3%A1rik&entry.1292438233=%20%20In%20practical%20distributed%20systems%2C%20workers%20are%20typically%20not%20homogeneous%2C%20and%0Adue%20to%20differences%20in%20hardware%20configurations%20and%20network%20conditions%2C%20can%20have%0Ahighly%20varying%20processing%20times.%20We%20consider%20smooth%20nonconvex%20finite-sum%0A%28empirical%20risk%20minimization%29%20problems%20in%20this%20setup%20and%20introduce%20a%20new%0Aparallel%20method%2C%20Freya%20PAGE%2C%20designed%20to%20handle%20arbitrarily%20heterogeneous%20and%0Aasynchronous%20computations.%20By%20being%20robust%20to%20%22stragglers%22%20and%20adaptively%0Aignoring%20slow%20computations%2C%20Freya%20PAGE%20offers%20significantly%20improved%20time%0Acomplexity%20guarantees%20compared%20to%20all%20previous%20methods%2C%20including%20Asynchronous%0ASGD%2C%20Rennala%20SGD%2C%20SPIDER%2C%20and%20PAGE%2C%20while%20requiring%20weaker%20assumptions.%20The%0Aalgorithm%20relies%20on%20novel%20generic%20stochastic%20gradient%20collection%20strategies%0Awith%20theoretical%20guarantees%20that%20can%20be%20of%20interest%20on%20their%20own%2C%20and%20may%20be%0Aused%20in%20the%20design%20of%20future%20optimization%20methods.%20Furthermore%2C%20we%20establish%20a%0Alower%20bound%20for%20smooth%20nonconvex%20finite-sum%20problems%20in%20the%20asynchronous%20setup%2C%0Aproviding%20a%20fundamental%20time%20complexity%20limit.%20This%20lower%20bound%20is%20tight%20and%0Ademonstrates%20the%20optimality%20of%20Freya%20PAGE%20in%20the%20large-scale%20regime%2C%20i.e.%2C%20when%0A%24%5Csqrt%7Bm%7D%20%5Cgeq%20n%24%2C%20where%20%24n%24%20is%20%23%20of%20workers%2C%20and%20%24m%24%20is%20%23%20of%20data%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15545v1&entry.124074799=Read"},
{"title": "DAGER: Exact Gradient Inversion for Large Language Models", "author": "Ivo Petrov and Dimitar I. Dimitrov and Maximilian Baader and Mark Niklas M\u00fcller and Martin Vechev", "abstract": "  Federated learning works by aggregating locally computed gradients from\nmultiple clients, thus enabling collaborative training without sharing private\nclient data. However, prior work has shown that the data can actually be\nrecovered by the server using so-called gradient inversion attacks. While these\nattacks perform well when applied on images, they are limited in the text\ndomain and only permit approximate reconstruction of small batches and short\ninput sequences. In this work, we propose DAGER, the first algorithm to recover\nwhole batches of input text exactly. DAGER leverages the low-rank structure of\nself-attention layer gradients and the discrete nature of token embeddings to\nefficiently check if a given token sequence is part of the client data. We use\nthis check to exactly recover full batches in the honest-but-curious setting\nwithout any prior on the data for both encoder- and decoder-based architectures\nusing exhaustive heuristic search and a greedy approach, respectively. We\nprovide an efficient GPU implementation of DAGER and show experimentally that\nit recovers full batches of size up to 128 on large language models (LLMs),\nbeating prior attacks in speed (20x at same batch size), scalability (10x\nlarger batches), and reconstruction quality (ROUGE-1/2 > 0.99).\n", "link": "http://arxiv.org/abs/2405.15586v1", "date": "2024-05-24", "relevancy": 2.1214, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5513}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5393}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAGER%3A%20Exact%20Gradient%20Inversion%20for%20Large%20Language%20Models&body=Title%3A%20DAGER%3A%20Exact%20Gradient%20Inversion%20for%20Large%20Language%20Models%0AAuthor%3A%20Ivo%20Petrov%20and%20Dimitar%20I.%20Dimitrov%20and%20Maximilian%20Baader%20and%20Mark%20Niklas%20M%C3%BCller%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20Federated%20learning%20works%20by%20aggregating%20locally%20computed%20gradients%20from%0Amultiple%20clients%2C%20thus%20enabling%20collaborative%20training%20without%20sharing%20private%0Aclient%20data.%20However%2C%20prior%20work%20has%20shown%20that%20the%20data%20can%20actually%20be%0Arecovered%20by%20the%20server%20using%20so-called%20gradient%20inversion%20attacks.%20While%20these%0Aattacks%20perform%20well%20when%20applied%20on%20images%2C%20they%20are%20limited%20in%20the%20text%0Adomain%20and%20only%20permit%20approximate%20reconstruction%20of%20small%20batches%20and%20short%0Ainput%20sequences.%20In%20this%20work%2C%20we%20propose%20DAGER%2C%20the%20first%20algorithm%20to%20recover%0Awhole%20batches%20of%20input%20text%20exactly.%20DAGER%20leverages%20the%20low-rank%20structure%20of%0Aself-attention%20layer%20gradients%20and%20the%20discrete%20nature%20of%20token%20embeddings%20to%0Aefficiently%20check%20if%20a%20given%20token%20sequence%20is%20part%20of%20the%20client%20data.%20We%20use%0Athis%20check%20to%20exactly%20recover%20full%20batches%20in%20the%20honest-but-curious%20setting%0Awithout%20any%20prior%20on%20the%20data%20for%20both%20encoder-%20and%20decoder-based%20architectures%0Ausing%20exhaustive%20heuristic%20search%20and%20a%20greedy%20approach%2C%20respectively.%20We%0Aprovide%20an%20efficient%20GPU%20implementation%20of%20DAGER%20and%20show%20experimentally%20that%0Ait%20recovers%20full%20batches%20of%20size%20up%20to%20128%20on%20large%20language%20models%20%28LLMs%29%2C%0Abeating%20prior%20attacks%20in%20speed%20%2820x%20at%20same%20batch%20size%29%2C%20scalability%20%2810x%0Alarger%20batches%29%2C%20and%20reconstruction%20quality%20%28ROUGE-1/2%20%3E%200.99%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAGER%253A%2520Exact%2520Gradient%2520Inversion%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DIvo%2520Petrov%2520and%2520Dimitar%2520I.%2520Dimitrov%2520and%2520Maximilian%2520Baader%2520and%2520Mark%2520Niklas%2520M%25C3%25BCller%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520Federated%2520learning%2520works%2520by%2520aggregating%2520locally%2520computed%2520gradients%2520from%250Amultiple%2520clients%252C%2520thus%2520enabling%2520collaborative%2520training%2520without%2520sharing%2520private%250Aclient%2520data.%2520However%252C%2520prior%2520work%2520has%2520shown%2520that%2520the%2520data%2520can%2520actually%2520be%250Arecovered%2520by%2520the%2520server%2520using%2520so-called%2520gradient%2520inversion%2520attacks.%2520While%2520these%250Aattacks%2520perform%2520well%2520when%2520applied%2520on%2520images%252C%2520they%2520are%2520limited%2520in%2520the%2520text%250Adomain%2520and%2520only%2520permit%2520approximate%2520reconstruction%2520of%2520small%2520batches%2520and%2520short%250Ainput%2520sequences.%2520In%2520this%2520work%252C%2520we%2520propose%2520DAGER%252C%2520the%2520first%2520algorithm%2520to%2520recover%250Awhole%2520batches%2520of%2520input%2520text%2520exactly.%2520DAGER%2520leverages%2520the%2520low-rank%2520structure%2520of%250Aself-attention%2520layer%2520gradients%2520and%2520the%2520discrete%2520nature%2520of%2520token%2520embeddings%2520to%250Aefficiently%2520check%2520if%2520a%2520given%2520token%2520sequence%2520is%2520part%2520of%2520the%2520client%2520data.%2520We%2520use%250Athis%2520check%2520to%2520exactly%2520recover%2520full%2520batches%2520in%2520the%2520honest-but-curious%2520setting%250Awithout%2520any%2520prior%2520on%2520the%2520data%2520for%2520both%2520encoder-%2520and%2520decoder-based%2520architectures%250Ausing%2520exhaustive%2520heuristic%2520search%2520and%2520a%2520greedy%2520approach%252C%2520respectively.%2520We%250Aprovide%2520an%2520efficient%2520GPU%2520implementation%2520of%2520DAGER%2520and%2520show%2520experimentally%2520that%250Ait%2520recovers%2520full%2520batches%2520of%2520size%2520up%2520to%2520128%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Abeating%2520prior%2520attacks%2520in%2520speed%2520%252820x%2520at%2520same%2520batch%2520size%2529%252C%2520scalability%2520%252810x%250Alarger%2520batches%2529%252C%2520and%2520reconstruction%2520quality%2520%2528ROUGE-1/2%2520%253E%25200.99%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAGER%3A%20Exact%20Gradient%20Inversion%20for%20Large%20Language%20Models&entry.906535625=Ivo%20Petrov%20and%20Dimitar%20I.%20Dimitrov%20and%20Maximilian%20Baader%20and%20Mark%20Niklas%20M%C3%BCller%20and%20Martin%20Vechev&entry.1292438233=%20%20Federated%20learning%20works%20by%20aggregating%20locally%20computed%20gradients%20from%0Amultiple%20clients%2C%20thus%20enabling%20collaborative%20training%20without%20sharing%20private%0Aclient%20data.%20However%2C%20prior%20work%20has%20shown%20that%20the%20data%20can%20actually%20be%0Arecovered%20by%20the%20server%20using%20so-called%20gradient%20inversion%20attacks.%20While%20these%0Aattacks%20perform%20well%20when%20applied%20on%20images%2C%20they%20are%20limited%20in%20the%20text%0Adomain%20and%20only%20permit%20approximate%20reconstruction%20of%20small%20batches%20and%20short%0Ainput%20sequences.%20In%20this%20work%2C%20we%20propose%20DAGER%2C%20the%20first%20algorithm%20to%20recover%0Awhole%20batches%20of%20input%20text%20exactly.%20DAGER%20leverages%20the%20low-rank%20structure%20of%0Aself-attention%20layer%20gradients%20and%20the%20discrete%20nature%20of%20token%20embeddings%20to%0Aefficiently%20check%20if%20a%20given%20token%20sequence%20is%20part%20of%20the%20client%20data.%20We%20use%0Athis%20check%20to%20exactly%20recover%20full%20batches%20in%20the%20honest-but-curious%20setting%0Awithout%20any%20prior%20on%20the%20data%20for%20both%20encoder-%20and%20decoder-based%20architectures%0Ausing%20exhaustive%20heuristic%20search%20and%20a%20greedy%20approach%2C%20respectively.%20We%0Aprovide%20an%20efficient%20GPU%20implementation%20of%20DAGER%20and%20show%20experimentally%20that%0Ait%20recovers%20full%20batches%20of%20size%20up%20to%20128%20on%20large%20language%20models%20%28LLMs%29%2C%0Abeating%20prior%20attacks%20in%20speed%20%2820x%20at%20same%20batch%20size%29%2C%20scalability%20%2810x%0Alarger%20batches%29%2C%20and%20reconstruction%20quality%20%28ROUGE-1/2%20%3E%200.99%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15586v1&entry.124074799=Read"},
{"title": "Align as Ideal: Cross-Modal Alignment Binding for Federated Medical\n  Vision-Language Pre-training", "author": "Zitao Shuai and Liyue Shen", "abstract": "  Vision-language pre-training (VLP) has arised as an efficient scheme for\nmultimodal representation learning, but it requires large-scale multimodal data\nfor pre-training, making it an obstacle especially for medical applications. To\novercome the data limitation, federated learning (FL) can be a promising\nstrategy to scale up the dataset for medical VLP while protecting data privacy.\nHowever, client data are often heterogeneous in real-world scenarios, and we\nobserve that local training on heterogeneous client data would distort the\nmultimodal representation learning and lead to biased cross-modal alignment. To\naddress this challenge, we propose a Federated Align as IDeal (FedAID)\nframework for federated VLP with robustness to data heterogeneity, to bind\nlocal clients with an ideal crossmodal alignment. Specifically, to reduce\ndistortions on global-aggregated features while learning diverse semantics from\nclient datasets during local training, we propose to bind the cross-model\naligned representation space learned by local models with an unbiased one via\nguidance-based regularization. Moreover, we employ a distribution-based min-max\noptimization to learn the unbiased cross-modal alignment at each communication\nturn of federated pre-training. The experiments on real-world datasets\ndemonstrate our method successfully promotes efficient federated multimodal\nlearning for medical VLP with data heterogeneity.\n", "link": "http://arxiv.org/abs/2404.03854v2", "date": "2024-05-24", "relevancy": 2.116, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5189}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Align%20as%20Ideal%3A%20Cross-Modal%20Alignment%20Binding%20for%20Federated%20Medical%0A%20%20Vision-Language%20Pre-training&body=Title%3A%20Align%20as%20Ideal%3A%20Cross-Modal%20Alignment%20Binding%20for%20Federated%20Medical%0A%20%20Vision-Language%20Pre-training%0AAuthor%3A%20Zitao%20Shuai%20and%20Liyue%20Shen%0AAbstract%3A%20%20%20Vision-language%20pre-training%20%28VLP%29%20has%20arised%20as%20an%20efficient%20scheme%20for%0Amultimodal%20representation%20learning%2C%20but%20it%20requires%20large-scale%20multimodal%20data%0Afor%20pre-training%2C%20making%20it%20an%20obstacle%20especially%20for%20medical%20applications.%20To%0Aovercome%20the%20data%20limitation%2C%20federated%20learning%20%28FL%29%20can%20be%20a%20promising%0Astrategy%20to%20scale%20up%20the%20dataset%20for%20medical%20VLP%20while%20protecting%20data%20privacy.%0AHowever%2C%20client%20data%20are%20often%20heterogeneous%20in%20real-world%20scenarios%2C%20and%20we%0Aobserve%20that%20local%20training%20on%20heterogeneous%20client%20data%20would%20distort%20the%0Amultimodal%20representation%20learning%20and%20lead%20to%20biased%20cross-modal%20alignment.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20Federated%20Align%20as%20IDeal%20%28FedAID%29%0Aframework%20for%20federated%20VLP%20with%20robustness%20to%20data%20heterogeneity%2C%20to%20bind%0Alocal%20clients%20with%20an%20ideal%20crossmodal%20alignment.%20Specifically%2C%20to%20reduce%0Adistortions%20on%20global-aggregated%20features%20while%20learning%20diverse%20semantics%20from%0Aclient%20datasets%20during%20local%20training%2C%20we%20propose%20to%20bind%20the%20cross-model%0Aaligned%20representation%20space%20learned%20by%20local%20models%20with%20an%20unbiased%20one%20via%0Aguidance-based%20regularization.%20Moreover%2C%20we%20employ%20a%20distribution-based%20min-max%0Aoptimization%20to%20learn%20the%20unbiased%20cross-modal%20alignment%20at%20each%20communication%0Aturn%20of%20federated%20pre-training.%20The%20experiments%20on%20real-world%20datasets%0Ademonstrate%20our%20method%20successfully%20promotes%20efficient%20federated%20multimodal%0Alearning%20for%20medical%20VLP%20with%20data%20heterogeneity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03854v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlign%2520as%2520Ideal%253A%2520Cross-Modal%2520Alignment%2520Binding%2520for%2520Federated%2520Medical%250A%2520%2520Vision-Language%2520Pre-training%26entry.906535625%3DZitao%2520Shuai%2520and%2520Liyue%2520Shen%26entry.1292438233%3D%2520%2520Vision-language%2520pre-training%2520%2528VLP%2529%2520has%2520arised%2520as%2520an%2520efficient%2520scheme%2520for%250Amultimodal%2520representation%2520learning%252C%2520but%2520it%2520requires%2520large-scale%2520multimodal%2520data%250Afor%2520pre-training%252C%2520making%2520it%2520an%2520obstacle%2520especially%2520for%2520medical%2520applications.%2520To%250Aovercome%2520the%2520data%2520limitation%252C%2520federated%2520learning%2520%2528FL%2529%2520can%2520be%2520a%2520promising%250Astrategy%2520to%2520scale%2520up%2520the%2520dataset%2520for%2520medical%2520VLP%2520while%2520protecting%2520data%2520privacy.%250AHowever%252C%2520client%2520data%2520are%2520often%2520heterogeneous%2520in%2520real-world%2520scenarios%252C%2520and%2520we%250Aobserve%2520that%2520local%2520training%2520on%2520heterogeneous%2520client%2520data%2520would%2520distort%2520the%250Amultimodal%2520representation%2520learning%2520and%2520lead%2520to%2520biased%2520cross-modal%2520alignment.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520a%2520Federated%2520Align%2520as%2520IDeal%2520%2528FedAID%2529%250Aframework%2520for%2520federated%2520VLP%2520with%2520robustness%2520to%2520data%2520heterogeneity%252C%2520to%2520bind%250Alocal%2520clients%2520with%2520an%2520ideal%2520crossmodal%2520alignment.%2520Specifically%252C%2520to%2520reduce%250Adistortions%2520on%2520global-aggregated%2520features%2520while%2520learning%2520diverse%2520semantics%2520from%250Aclient%2520datasets%2520during%2520local%2520training%252C%2520we%2520propose%2520to%2520bind%2520the%2520cross-model%250Aaligned%2520representation%2520space%2520learned%2520by%2520local%2520models%2520with%2520an%2520unbiased%2520one%2520via%250Aguidance-based%2520regularization.%2520Moreover%252C%2520we%2520employ%2520a%2520distribution-based%2520min-max%250Aoptimization%2520to%2520learn%2520the%2520unbiased%2520cross-modal%2520alignment%2520at%2520each%2520communication%250Aturn%2520of%2520federated%2520pre-training.%2520The%2520experiments%2520on%2520real-world%2520datasets%250Ademonstrate%2520our%2520method%2520successfully%2520promotes%2520efficient%2520federated%2520multimodal%250Alearning%2520for%2520medical%2520VLP%2520with%2520data%2520heterogeneity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03854v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Align%20as%20Ideal%3A%20Cross-Modal%20Alignment%20Binding%20for%20Federated%20Medical%0A%20%20Vision-Language%20Pre-training&entry.906535625=Zitao%20Shuai%20and%20Liyue%20Shen&entry.1292438233=%20%20Vision-language%20pre-training%20%28VLP%29%20has%20arised%20as%20an%20efficient%20scheme%20for%0Amultimodal%20representation%20learning%2C%20but%20it%20requires%20large-scale%20multimodal%20data%0Afor%20pre-training%2C%20making%20it%20an%20obstacle%20especially%20for%20medical%20applications.%20To%0Aovercome%20the%20data%20limitation%2C%20federated%20learning%20%28FL%29%20can%20be%20a%20promising%0Astrategy%20to%20scale%20up%20the%20dataset%20for%20medical%20VLP%20while%20protecting%20data%20privacy.%0AHowever%2C%20client%20data%20are%20often%20heterogeneous%20in%20real-world%20scenarios%2C%20and%20we%0Aobserve%20that%20local%20training%20on%20heterogeneous%20client%20data%20would%20distort%20the%0Amultimodal%20representation%20learning%20and%20lead%20to%20biased%20cross-modal%20alignment.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20Federated%20Align%20as%20IDeal%20%28FedAID%29%0Aframework%20for%20federated%20VLP%20with%20robustness%20to%20data%20heterogeneity%2C%20to%20bind%0Alocal%20clients%20with%20an%20ideal%20crossmodal%20alignment.%20Specifically%2C%20to%20reduce%0Adistortions%20on%20global-aggregated%20features%20while%20learning%20diverse%20semantics%20from%0Aclient%20datasets%20during%20local%20training%2C%20we%20propose%20to%20bind%20the%20cross-model%0Aaligned%20representation%20space%20learned%20by%20local%20models%20with%20an%20unbiased%20one%20via%0Aguidance-based%20regularization.%20Moreover%2C%20we%20employ%20a%20distribution-based%20min-max%0Aoptimization%20to%20learn%20the%20unbiased%20cross-modal%20alignment%20at%20each%20communication%0Aturn%20of%20federated%20pre-training.%20The%20experiments%20on%20real-world%20datasets%0Ademonstrate%20our%20method%20successfully%20promotes%20efficient%20federated%20multimodal%0Alearning%20for%20medical%20VLP%20with%20data%20heterogeneity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03854v2&entry.124074799=Read"},
{"title": "ETTrack: Enhanced Temporal Motion Predictor for Multi-Object Tracking", "author": "Xudong Han and Nobuyuki Oishi and Yueying Tian and Elif Ucurum and Rupert Young and Chris Chatwin and Philip Birch", "abstract": "  Many Multi-Object Tracking (MOT) approaches exploit motion information to\nassociate all the detected objects across frames. However, many methods that\nrely on filtering-based algorithms, such as the Kalman Filter, often work well\nin linear motion scenarios but struggle to accurately predict the locations of\nobjects undergoing complex and non-linear movements. To tackle these scenarios,\nwe propose a motion-based MOT approach with an enhanced temporal motion\npredictor, ETTrack. Specifically, the motion predictor integrates a transformer\nmodel and a Temporal Convolutional Network (TCN) to capture short-term and\nlong-term motion patterns, and it predicts the future motion of individual\nobjects based on the historical motion information. Additionally, we propose a\nnovel Momentum Correction Loss function that provides additional information\nregarding the motion direction of objects during training. This allows the\nmotion predictor rapidly adapt to motion variations and more accurately predict\nfuture motion. Our experimental results demonstrate that ETTrack achieves a\ncompetitive performance compared with state-of-the-art trackers on DanceTrack\nand SportsMOT, scoring 56.4% and 74.4% in HOTA metrics, respectively.\n", "link": "http://arxiv.org/abs/2405.15755v1", "date": "2024-05-24", "relevancy": 2.1105, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5345}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5341}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ETTrack%3A%20Enhanced%20Temporal%20Motion%20Predictor%20for%20Multi-Object%20Tracking&body=Title%3A%20ETTrack%3A%20Enhanced%20Temporal%20Motion%20Predictor%20for%20Multi-Object%20Tracking%0AAuthor%3A%20Xudong%20Han%20and%20Nobuyuki%20Oishi%20and%20Yueying%20Tian%20and%20Elif%20Ucurum%20and%20Rupert%20Young%20and%20Chris%20Chatwin%20and%20Philip%20Birch%0AAbstract%3A%20%20%20Many%20Multi-Object%20Tracking%20%28MOT%29%20approaches%20exploit%20motion%20information%20to%0Aassociate%20all%20the%20detected%20objects%20across%20frames.%20However%2C%20many%20methods%20that%0Arely%20on%20filtering-based%20algorithms%2C%20such%20as%20the%20Kalman%20Filter%2C%20often%20work%20well%0Ain%20linear%20motion%20scenarios%20but%20struggle%20to%20accurately%20predict%20the%20locations%20of%0Aobjects%20undergoing%20complex%20and%20non-linear%20movements.%20To%20tackle%20these%20scenarios%2C%0Awe%20propose%20a%20motion-based%20MOT%20approach%20with%20an%20enhanced%20temporal%20motion%0Apredictor%2C%20ETTrack.%20Specifically%2C%20the%20motion%20predictor%20integrates%20a%20transformer%0Amodel%20and%20a%20Temporal%20Convolutional%20Network%20%28TCN%29%20to%20capture%20short-term%20and%0Along-term%20motion%20patterns%2C%20and%20it%20predicts%20the%20future%20motion%20of%20individual%0Aobjects%20based%20on%20the%20historical%20motion%20information.%20Additionally%2C%20we%20propose%20a%0Anovel%20Momentum%20Correction%20Loss%20function%20that%20provides%20additional%20information%0Aregarding%20the%20motion%20direction%20of%20objects%20during%20training.%20This%20allows%20the%0Amotion%20predictor%20rapidly%20adapt%20to%20motion%20variations%20and%20more%20accurately%20predict%0Afuture%20motion.%20Our%20experimental%20results%20demonstrate%20that%20ETTrack%20achieves%20a%0Acompetitive%20performance%20compared%20with%20state-of-the-art%20trackers%20on%20DanceTrack%0Aand%20SportsMOT%2C%20scoring%2056.4%25%20and%2074.4%25%20in%20HOTA%20metrics%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DETTrack%253A%2520Enhanced%2520Temporal%2520Motion%2520Predictor%2520for%2520Multi-Object%2520Tracking%26entry.906535625%3DXudong%2520Han%2520and%2520Nobuyuki%2520Oishi%2520and%2520Yueying%2520Tian%2520and%2520Elif%2520Ucurum%2520and%2520Rupert%2520Young%2520and%2520Chris%2520Chatwin%2520and%2520Philip%2520Birch%26entry.1292438233%3D%2520%2520Many%2520Multi-Object%2520Tracking%2520%2528MOT%2529%2520approaches%2520exploit%2520motion%2520information%2520to%250Aassociate%2520all%2520the%2520detected%2520objects%2520across%2520frames.%2520However%252C%2520many%2520methods%2520that%250Arely%2520on%2520filtering-based%2520algorithms%252C%2520such%2520as%2520the%2520Kalman%2520Filter%252C%2520often%2520work%2520well%250Ain%2520linear%2520motion%2520scenarios%2520but%2520struggle%2520to%2520accurately%2520predict%2520the%2520locations%2520of%250Aobjects%2520undergoing%2520complex%2520and%2520non-linear%2520movements.%2520To%2520tackle%2520these%2520scenarios%252C%250Awe%2520propose%2520a%2520motion-based%2520MOT%2520approach%2520with%2520an%2520enhanced%2520temporal%2520motion%250Apredictor%252C%2520ETTrack.%2520Specifically%252C%2520the%2520motion%2520predictor%2520integrates%2520a%2520transformer%250Amodel%2520and%2520a%2520Temporal%2520Convolutional%2520Network%2520%2528TCN%2529%2520to%2520capture%2520short-term%2520and%250Along-term%2520motion%2520patterns%252C%2520and%2520it%2520predicts%2520the%2520future%2520motion%2520of%2520individual%250Aobjects%2520based%2520on%2520the%2520historical%2520motion%2520information.%2520Additionally%252C%2520we%2520propose%2520a%250Anovel%2520Momentum%2520Correction%2520Loss%2520function%2520that%2520provides%2520additional%2520information%250Aregarding%2520the%2520motion%2520direction%2520of%2520objects%2520during%2520training.%2520This%2520allows%2520the%250Amotion%2520predictor%2520rapidly%2520adapt%2520to%2520motion%2520variations%2520and%2520more%2520accurately%2520predict%250Afuture%2520motion.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520ETTrack%2520achieves%2520a%250Acompetitive%2520performance%2520compared%2520with%2520state-of-the-art%2520trackers%2520on%2520DanceTrack%250Aand%2520SportsMOT%252C%2520scoring%252056.4%2525%2520and%252074.4%2525%2520in%2520HOTA%2520metrics%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ETTrack%3A%20Enhanced%20Temporal%20Motion%20Predictor%20for%20Multi-Object%20Tracking&entry.906535625=Xudong%20Han%20and%20Nobuyuki%20Oishi%20and%20Yueying%20Tian%20and%20Elif%20Ucurum%20and%20Rupert%20Young%20and%20Chris%20Chatwin%20and%20Philip%20Birch&entry.1292438233=%20%20Many%20Multi-Object%20Tracking%20%28MOT%29%20approaches%20exploit%20motion%20information%20to%0Aassociate%20all%20the%20detected%20objects%20across%20frames.%20However%2C%20many%20methods%20that%0Arely%20on%20filtering-based%20algorithms%2C%20such%20as%20the%20Kalman%20Filter%2C%20often%20work%20well%0Ain%20linear%20motion%20scenarios%20but%20struggle%20to%20accurately%20predict%20the%20locations%20of%0Aobjects%20undergoing%20complex%20and%20non-linear%20movements.%20To%20tackle%20these%20scenarios%2C%0Awe%20propose%20a%20motion-based%20MOT%20approach%20with%20an%20enhanced%20temporal%20motion%0Apredictor%2C%20ETTrack.%20Specifically%2C%20the%20motion%20predictor%20integrates%20a%20transformer%0Amodel%20and%20a%20Temporal%20Convolutional%20Network%20%28TCN%29%20to%20capture%20short-term%20and%0Along-term%20motion%20patterns%2C%20and%20it%20predicts%20the%20future%20motion%20of%20individual%0Aobjects%20based%20on%20the%20historical%20motion%20information.%20Additionally%2C%20we%20propose%20a%0Anovel%20Momentum%20Correction%20Loss%20function%20that%20provides%20additional%20information%0Aregarding%20the%20motion%20direction%20of%20objects%20during%20training.%20This%20allows%20the%0Amotion%20predictor%20rapidly%20adapt%20to%20motion%20variations%20and%20more%20accurately%20predict%0Afuture%20motion.%20Our%20experimental%20results%20demonstrate%20that%20ETTrack%20achieves%20a%0Acompetitive%20performance%20compared%20with%20state-of-the-art%20trackers%20on%20DanceTrack%0Aand%20SportsMOT%2C%20scoring%2056.4%25%20and%2074.4%25%20in%20HOTA%20metrics%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15755v1&entry.124074799=Read"},
{"title": "No Filter: Cultural and Socioeconomic Diversity in Contrastive\n  Vision-Language Models", "author": "Ang\u00e9line Pouget and Lucas Beyer and Emanuele Bugliarello and Xiao Wang and Andreas Peter Steiner and Xiaohua Zhai and Ibrahim Alabdulmohsin", "abstract": "  We study cultural and socioeconomic diversity in contrastive vision-language\nmodels (VLMs). Using a broad range of benchmark datasets and evaluation\nmetrics, we bring to attention several important findings. First, the common\nfiltering of training data to English image-text pairs disadvantages\ncommunities of lower socioeconomic status and negatively impacts cultural\nunderstanding. Notably, this performance gap is not captured by - and even at\nodds with - the currently popular evaluation metrics derived from the\nWestern-centric ImageNet and COCO datasets. Second, pretraining with global,\nunfiltered data before fine-tuning on English content can improve cultural\nunderstanding without sacrificing performance on said popular benchmarks.\nThird, we introduce the task of geo-localization as a novel evaluation metric\nto assess cultural diversity in VLMs. Our work underscores the value of using\ndiverse data to create more inclusive multimodal systems and lays the\ngroundwork for developing VLMs that better represent global perspectives.\n", "link": "http://arxiv.org/abs/2405.13777v2", "date": "2024-05-24", "relevancy": 2.108, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5712}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5252}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Filter%3A%20Cultural%20and%20Socioeconomic%20Diversity%20in%20Contrastive%0A%20%20Vision-Language%20Models&body=Title%3A%20No%20Filter%3A%20Cultural%20and%20Socioeconomic%20Diversity%20in%20Contrastive%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Ang%C3%A9line%20Pouget%20and%20Lucas%20Beyer%20and%20Emanuele%20Bugliarello%20and%20Xiao%20Wang%20and%20Andreas%20Peter%20Steiner%20and%20Xiaohua%20Zhai%20and%20Ibrahim%20Alabdulmohsin%0AAbstract%3A%20%20%20We%20study%20cultural%20and%20socioeconomic%20diversity%20in%20contrastive%20vision-language%0Amodels%20%28VLMs%29.%20Using%20a%20broad%20range%20of%20benchmark%20datasets%20and%20evaluation%0Ametrics%2C%20we%20bring%20to%20attention%20several%20important%20findings.%20First%2C%20the%20common%0Afiltering%20of%20training%20data%20to%20English%20image-text%20pairs%20disadvantages%0Acommunities%20of%20lower%20socioeconomic%20status%20and%20negatively%20impacts%20cultural%0Aunderstanding.%20Notably%2C%20this%20performance%20gap%20is%20not%20captured%20by%20-%20and%20even%20at%0Aodds%20with%20-%20the%20currently%20popular%20evaluation%20metrics%20derived%20from%20the%0AWestern-centric%20ImageNet%20and%20COCO%20datasets.%20Second%2C%20pretraining%20with%20global%2C%0Aunfiltered%20data%20before%20fine-tuning%20on%20English%20content%20can%20improve%20cultural%0Aunderstanding%20without%20sacrificing%20performance%20on%20said%20popular%20benchmarks.%0AThird%2C%20we%20introduce%20the%20task%20of%20geo-localization%20as%20a%20novel%20evaluation%20metric%0Ato%20assess%20cultural%20diversity%20in%20VLMs.%20Our%20work%20underscores%20the%20value%20of%20using%0Adiverse%20data%20to%20create%20more%20inclusive%20multimodal%20systems%20and%20lays%20the%0Agroundwork%20for%20developing%20VLMs%20that%20better%20represent%20global%20perspectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Filter%253A%2520Cultural%2520and%2520Socioeconomic%2520Diversity%2520in%2520Contrastive%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DAng%25C3%25A9line%2520Pouget%2520and%2520Lucas%2520Beyer%2520and%2520Emanuele%2520Bugliarello%2520and%2520Xiao%2520Wang%2520and%2520Andreas%2520Peter%2520Steiner%2520and%2520Xiaohua%2520Zhai%2520and%2520Ibrahim%2520Alabdulmohsin%26entry.1292438233%3D%2520%2520We%2520study%2520cultural%2520and%2520socioeconomic%2520diversity%2520in%2520contrastive%2520vision-language%250Amodels%2520%2528VLMs%2529.%2520Using%2520a%2520broad%2520range%2520of%2520benchmark%2520datasets%2520and%2520evaluation%250Ametrics%252C%2520we%2520bring%2520to%2520attention%2520several%2520important%2520findings.%2520First%252C%2520the%2520common%250Afiltering%2520of%2520training%2520data%2520to%2520English%2520image-text%2520pairs%2520disadvantages%250Acommunities%2520of%2520lower%2520socioeconomic%2520status%2520and%2520negatively%2520impacts%2520cultural%250Aunderstanding.%2520Notably%252C%2520this%2520performance%2520gap%2520is%2520not%2520captured%2520by%2520-%2520and%2520even%2520at%250Aodds%2520with%2520-%2520the%2520currently%2520popular%2520evaluation%2520metrics%2520derived%2520from%2520the%250AWestern-centric%2520ImageNet%2520and%2520COCO%2520datasets.%2520Second%252C%2520pretraining%2520with%2520global%252C%250Aunfiltered%2520data%2520before%2520fine-tuning%2520on%2520English%2520content%2520can%2520improve%2520cultural%250Aunderstanding%2520without%2520sacrificing%2520performance%2520on%2520said%2520popular%2520benchmarks.%250AThird%252C%2520we%2520introduce%2520the%2520task%2520of%2520geo-localization%2520as%2520a%2520novel%2520evaluation%2520metric%250Ato%2520assess%2520cultural%2520diversity%2520in%2520VLMs.%2520Our%2520work%2520underscores%2520the%2520value%2520of%2520using%250Adiverse%2520data%2520to%2520create%2520more%2520inclusive%2520multimodal%2520systems%2520and%2520lays%2520the%250Agroundwork%2520for%2520developing%2520VLMs%2520that%2520better%2520represent%2520global%2520perspectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Filter%3A%20Cultural%20and%20Socioeconomic%20Diversity%20in%20Contrastive%0A%20%20Vision-Language%20Models&entry.906535625=Ang%C3%A9line%20Pouget%20and%20Lucas%20Beyer%20and%20Emanuele%20Bugliarello%20and%20Xiao%20Wang%20and%20Andreas%20Peter%20Steiner%20and%20Xiaohua%20Zhai%20and%20Ibrahim%20Alabdulmohsin&entry.1292438233=%20%20We%20study%20cultural%20and%20socioeconomic%20diversity%20in%20contrastive%20vision-language%0Amodels%20%28VLMs%29.%20Using%20a%20broad%20range%20of%20benchmark%20datasets%20and%20evaluation%0Ametrics%2C%20we%20bring%20to%20attention%20several%20important%20findings.%20First%2C%20the%20common%0Afiltering%20of%20training%20data%20to%20English%20image-text%20pairs%20disadvantages%0Acommunities%20of%20lower%20socioeconomic%20status%20and%20negatively%20impacts%20cultural%0Aunderstanding.%20Notably%2C%20this%20performance%20gap%20is%20not%20captured%20by%20-%20and%20even%20at%0Aodds%20with%20-%20the%20currently%20popular%20evaluation%20metrics%20derived%20from%20the%0AWestern-centric%20ImageNet%20and%20COCO%20datasets.%20Second%2C%20pretraining%20with%20global%2C%0Aunfiltered%20data%20before%20fine-tuning%20on%20English%20content%20can%20improve%20cultural%0Aunderstanding%20without%20sacrificing%20performance%20on%20said%20popular%20benchmarks.%0AThird%2C%20we%20introduce%20the%20task%20of%20geo-localization%20as%20a%20novel%20evaluation%20metric%0Ato%20assess%20cultural%20diversity%20in%20VLMs.%20Our%20work%20underscores%20the%20value%20of%20using%0Adiverse%20data%20to%20create%20more%20inclusive%20multimodal%20systems%20and%20lays%20the%0Agroundwork%20for%20developing%20VLMs%20that%20better%20represent%20global%20perspectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13777v2&entry.124074799=Read"},
{"title": "Repetita Iuvant: Data Repetition Allows SGD to Learn High-Dimensional\n  Multi-Index Functions", "author": "Luca Arnaboldi and Yatin Dandi and Florent Krzakala and Luca Pesce and Ludovic Stephan", "abstract": "  Neural networks can identify low-dimensional relevant structures within\nhigh-dimensional noisy data, yet our mathematical understanding of how they do\nso remains scarce. Here, we investigate the training dynamics of two-layer\nshallow neural networks trained with gradient-based algorithms, and discuss how\nthey learn pertinent features in multi-index models, that is target functions\nwith low-dimensional relevant directions. In the high-dimensional regime, where\nthe input dimension $d$ diverges, we show that a simple modification of the\nidealized single-pass gradient descent training scenario, where data can now be\nrepeated or iterated upon twice, drastically improves its computational\nefficiency. In particular, it surpasses the limitations previously believed to\nbe dictated by the Information and Leap exponents associated with the target\nfunction to be learned. Our results highlight the ability of networks to learn\nrelevant structures from data alone without any pre-processing. More precisely,\nwe show that (almost) all directions are learned with at most $O(d \\log d)$\nsteps. Among the exceptions is a set of hard functions that includes sparse\nparities. In the presence of coupling between directions, however, these can be\nlearned sequentially through a hierarchical mechanism that generalizes the\nnotion of staircase functions. Our results are proven by a rigorous study of\nthe evolution of the relevant statistics for high-dimensional dynamics.\n", "link": "http://arxiv.org/abs/2405.15459v1", "date": "2024-05-24", "relevancy": 2.1073, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5426}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5196}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Repetita%20Iuvant%3A%20Data%20Repetition%20Allows%20SGD%20to%20Learn%20High-Dimensional%0A%20%20Multi-Index%20Functions&body=Title%3A%20Repetita%20Iuvant%3A%20Data%20Repetition%20Allows%20SGD%20to%20Learn%20High-Dimensional%0A%20%20Multi-Index%20Functions%0AAuthor%3A%20Luca%20Arnaboldi%20and%20Yatin%20Dandi%20and%20Florent%20Krzakala%20and%20Luca%20Pesce%20and%20Ludovic%20Stephan%0AAbstract%3A%20%20%20Neural%20networks%20can%20identify%20low-dimensional%20relevant%20structures%20within%0Ahigh-dimensional%20noisy%20data%2C%20yet%20our%20mathematical%20understanding%20of%20how%20they%20do%0Aso%20remains%20scarce.%20Here%2C%20we%20investigate%20the%20training%20dynamics%20of%20two-layer%0Ashallow%20neural%20networks%20trained%20with%20gradient-based%20algorithms%2C%20and%20discuss%20how%0Athey%20learn%20pertinent%20features%20in%20multi-index%20models%2C%20that%20is%20target%20functions%0Awith%20low-dimensional%20relevant%20directions.%20In%20the%20high-dimensional%20regime%2C%20where%0Athe%20input%20dimension%20%24d%24%20diverges%2C%20we%20show%20that%20a%20simple%20modification%20of%20the%0Aidealized%20single-pass%20gradient%20descent%20training%20scenario%2C%20where%20data%20can%20now%20be%0Arepeated%20or%20iterated%20upon%20twice%2C%20drastically%20improves%20its%20computational%0Aefficiency.%20In%20particular%2C%20it%20surpasses%20the%20limitations%20previously%20believed%20to%0Abe%20dictated%20by%20the%20Information%20and%20Leap%20exponents%20associated%20with%20the%20target%0Afunction%20to%20be%20learned.%20Our%20results%20highlight%20the%20ability%20of%20networks%20to%20learn%0Arelevant%20structures%20from%20data%20alone%20without%20any%20pre-processing.%20More%20precisely%2C%0Awe%20show%20that%20%28almost%29%20all%20directions%20are%20learned%20with%20at%20most%20%24O%28d%20%5Clog%20d%29%24%0Asteps.%20Among%20the%20exceptions%20is%20a%20set%20of%20hard%20functions%20that%20includes%20sparse%0Aparities.%20In%20the%20presence%20of%20coupling%20between%20directions%2C%20however%2C%20these%20can%20be%0Alearned%20sequentially%20through%20a%20hierarchical%20mechanism%20that%20generalizes%20the%0Anotion%20of%20staircase%20functions.%20Our%20results%20are%20proven%20by%20a%20rigorous%20study%20of%0Athe%20evolution%20of%20the%20relevant%20statistics%20for%20high-dimensional%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepetita%2520Iuvant%253A%2520Data%2520Repetition%2520Allows%2520SGD%2520to%2520Learn%2520High-Dimensional%250A%2520%2520Multi-Index%2520Functions%26entry.906535625%3DLuca%2520Arnaboldi%2520and%2520Yatin%2520Dandi%2520and%2520Florent%2520Krzakala%2520and%2520Luca%2520Pesce%2520and%2520Ludovic%2520Stephan%26entry.1292438233%3D%2520%2520Neural%2520networks%2520can%2520identify%2520low-dimensional%2520relevant%2520structures%2520within%250Ahigh-dimensional%2520noisy%2520data%252C%2520yet%2520our%2520mathematical%2520understanding%2520of%2520how%2520they%2520do%250Aso%2520remains%2520scarce.%2520Here%252C%2520we%2520investigate%2520the%2520training%2520dynamics%2520of%2520two-layer%250Ashallow%2520neural%2520networks%2520trained%2520with%2520gradient-based%2520algorithms%252C%2520and%2520discuss%2520how%250Athey%2520learn%2520pertinent%2520features%2520in%2520multi-index%2520models%252C%2520that%2520is%2520target%2520functions%250Awith%2520low-dimensional%2520relevant%2520directions.%2520In%2520the%2520high-dimensional%2520regime%252C%2520where%250Athe%2520input%2520dimension%2520%2524d%2524%2520diverges%252C%2520we%2520show%2520that%2520a%2520simple%2520modification%2520of%2520the%250Aidealized%2520single-pass%2520gradient%2520descent%2520training%2520scenario%252C%2520where%2520data%2520can%2520now%2520be%250Arepeated%2520or%2520iterated%2520upon%2520twice%252C%2520drastically%2520improves%2520its%2520computational%250Aefficiency.%2520In%2520particular%252C%2520it%2520surpasses%2520the%2520limitations%2520previously%2520believed%2520to%250Abe%2520dictated%2520by%2520the%2520Information%2520and%2520Leap%2520exponents%2520associated%2520with%2520the%2520target%250Afunction%2520to%2520be%2520learned.%2520Our%2520results%2520highlight%2520the%2520ability%2520of%2520networks%2520to%2520learn%250Arelevant%2520structures%2520from%2520data%2520alone%2520without%2520any%2520pre-processing.%2520More%2520precisely%252C%250Awe%2520show%2520that%2520%2528almost%2529%2520all%2520directions%2520are%2520learned%2520with%2520at%2520most%2520%2524O%2528d%2520%255Clog%2520d%2529%2524%250Asteps.%2520Among%2520the%2520exceptions%2520is%2520a%2520set%2520of%2520hard%2520functions%2520that%2520includes%2520sparse%250Aparities.%2520In%2520the%2520presence%2520of%2520coupling%2520between%2520directions%252C%2520however%252C%2520these%2520can%2520be%250Alearned%2520sequentially%2520through%2520a%2520hierarchical%2520mechanism%2520that%2520generalizes%2520the%250Anotion%2520of%2520staircase%2520functions.%2520Our%2520results%2520are%2520proven%2520by%2520a%2520rigorous%2520study%2520of%250Athe%2520evolution%2520of%2520the%2520relevant%2520statistics%2520for%2520high-dimensional%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Repetita%20Iuvant%3A%20Data%20Repetition%20Allows%20SGD%20to%20Learn%20High-Dimensional%0A%20%20Multi-Index%20Functions&entry.906535625=Luca%20Arnaboldi%20and%20Yatin%20Dandi%20and%20Florent%20Krzakala%20and%20Luca%20Pesce%20and%20Ludovic%20Stephan&entry.1292438233=%20%20Neural%20networks%20can%20identify%20low-dimensional%20relevant%20structures%20within%0Ahigh-dimensional%20noisy%20data%2C%20yet%20our%20mathematical%20understanding%20of%20how%20they%20do%0Aso%20remains%20scarce.%20Here%2C%20we%20investigate%20the%20training%20dynamics%20of%20two-layer%0Ashallow%20neural%20networks%20trained%20with%20gradient-based%20algorithms%2C%20and%20discuss%20how%0Athey%20learn%20pertinent%20features%20in%20multi-index%20models%2C%20that%20is%20target%20functions%0Awith%20low-dimensional%20relevant%20directions.%20In%20the%20high-dimensional%20regime%2C%20where%0Athe%20input%20dimension%20%24d%24%20diverges%2C%20we%20show%20that%20a%20simple%20modification%20of%20the%0Aidealized%20single-pass%20gradient%20descent%20training%20scenario%2C%20where%20data%20can%20now%20be%0Arepeated%20or%20iterated%20upon%20twice%2C%20drastically%20improves%20its%20computational%0Aefficiency.%20In%20particular%2C%20it%20surpasses%20the%20limitations%20previously%20believed%20to%0Abe%20dictated%20by%20the%20Information%20and%20Leap%20exponents%20associated%20with%20the%20target%0Afunction%20to%20be%20learned.%20Our%20results%20highlight%20the%20ability%20of%20networks%20to%20learn%0Arelevant%20structures%20from%20data%20alone%20without%20any%20pre-processing.%20More%20precisely%2C%0Awe%20show%20that%20%28almost%29%20all%20directions%20are%20learned%20with%20at%20most%20%24O%28d%20%5Clog%20d%29%24%0Asteps.%20Among%20the%20exceptions%20is%20a%20set%20of%20hard%20functions%20that%20includes%20sparse%0Aparities.%20In%20the%20presence%20of%20coupling%20between%20directions%2C%20however%2C%20these%20can%20be%0Alearned%20sequentially%20through%20a%20hierarchical%20mechanism%20that%20generalizes%20the%0Anotion%20of%20staircase%20functions.%20Our%20results%20are%20proven%20by%20a%20rigorous%20study%20of%0Athe%20evolution%20of%20the%20relevant%20statistics%20for%20high-dimensional%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15459v1&entry.124074799=Read"},
{"title": "GRIL-Calib: Targetless Ground Robot IMU-LiDAR Extrinsic Calibration\n  Method using Ground Plane Motion Constraints", "author": "TaeYoung Kim and Gyuhyeon Pak and Euntai Kim", "abstract": "  Targetless IMU-LiDAR extrinsic calibration methods are gaining significant\nattention as the importance of the IMU-LiDAR fusion system increases. Notably,\nexisting calibration methods derive calibration parameters under the assumption\nthat the methods require full motion in all axes. When IMU and LiDAR are\nmounted on a ground robot the motion of which is restricted to planar motion,\nexisting calibration methods are likely to exhibit degraded performance. To\naddress this issue, we present GRIL-Calib: a novel targetless Ground Robot\nIMU-LiDAR Calibration method. Our proposed method leverages ground information\nto compensate for the lack of unrestricted full motion. First, we propose LiDAR\nOdometry (LO) using ground plane residuals to enhance calibration accuracy.\nSecond, we propose the Ground Plane Motion (GPM) constraint and incorporate it\ninto the optimization for calibration, enabling the determination of full 6-DoF\nextrinsic parameters, including theoretically unobservable direction. Finally,\nunlike baseline methods, we formulate the calibration not as sequential two\noptimizations but as a single optimization (SO) problem, solving all\ncalibration parameters simultaneously and improving accuracy. We validate our\nGRIL-Calib by applying it to various real-world datasets and comparing its\nperformance with that of existing state-of-the-art methods in terms of accuracy\nand robustness. Our code is available at\nhttps://github.com/Taeyoung96/GRIL-Calib.\n", "link": "http://arxiv.org/abs/2312.14035v2", "date": "2024-05-24", "relevancy": 2.1069, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5458}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5139}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRIL-Calib%3A%20Targetless%20Ground%20Robot%20IMU-LiDAR%20Extrinsic%20Calibration%0A%20%20Method%20using%20Ground%20Plane%20Motion%20Constraints&body=Title%3A%20GRIL-Calib%3A%20Targetless%20Ground%20Robot%20IMU-LiDAR%20Extrinsic%20Calibration%0A%20%20Method%20using%20Ground%20Plane%20Motion%20Constraints%0AAuthor%3A%20TaeYoung%20Kim%20and%20Gyuhyeon%20Pak%20and%20Euntai%20Kim%0AAbstract%3A%20%20%20Targetless%20IMU-LiDAR%20extrinsic%20calibration%20methods%20are%20gaining%20significant%0Aattention%20as%20the%20importance%20of%20the%20IMU-LiDAR%20fusion%20system%20increases.%20Notably%2C%0Aexisting%20calibration%20methods%20derive%20calibration%20parameters%20under%20the%20assumption%0Athat%20the%20methods%20require%20full%20motion%20in%20all%20axes.%20When%20IMU%20and%20LiDAR%20are%0Amounted%20on%20a%20ground%20robot%20the%20motion%20of%20which%20is%20restricted%20to%20planar%20motion%2C%0Aexisting%20calibration%20methods%20are%20likely%20to%20exhibit%20degraded%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20present%20GRIL-Calib%3A%20a%20novel%20targetless%20Ground%20Robot%0AIMU-LiDAR%20Calibration%20method.%20Our%20proposed%20method%20leverages%20ground%20information%0Ato%20compensate%20for%20the%20lack%20of%20unrestricted%20full%20motion.%20First%2C%20we%20propose%20LiDAR%0AOdometry%20%28LO%29%20using%20ground%20plane%20residuals%20to%20enhance%20calibration%20accuracy.%0ASecond%2C%20we%20propose%20the%20Ground%20Plane%20Motion%20%28GPM%29%20constraint%20and%20incorporate%20it%0Ainto%20the%20optimization%20for%20calibration%2C%20enabling%20the%20determination%20of%20full%206-DoF%0Aextrinsic%20parameters%2C%20including%20theoretically%20unobservable%20direction.%20Finally%2C%0Aunlike%20baseline%20methods%2C%20we%20formulate%20the%20calibration%20not%20as%20sequential%20two%0Aoptimizations%20but%20as%20a%20single%20optimization%20%28SO%29%20problem%2C%20solving%20all%0Acalibration%20parameters%20simultaneously%20and%20improving%20accuracy.%20We%20validate%20our%0AGRIL-Calib%20by%20applying%20it%20to%20various%20real-world%20datasets%20and%20comparing%20its%0Aperformance%20with%20that%20of%20existing%20state-of-the-art%20methods%20in%20terms%20of%20accuracy%0Aand%20robustness.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Taeyoung96/GRIL-Calib.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRIL-Calib%253A%2520Targetless%2520Ground%2520Robot%2520IMU-LiDAR%2520Extrinsic%2520Calibration%250A%2520%2520Method%2520using%2520Ground%2520Plane%2520Motion%2520Constraints%26entry.906535625%3DTaeYoung%2520Kim%2520and%2520Gyuhyeon%2520Pak%2520and%2520Euntai%2520Kim%26entry.1292438233%3D%2520%2520Targetless%2520IMU-LiDAR%2520extrinsic%2520calibration%2520methods%2520are%2520gaining%2520significant%250Aattention%2520as%2520the%2520importance%2520of%2520the%2520IMU-LiDAR%2520fusion%2520system%2520increases.%2520Notably%252C%250Aexisting%2520calibration%2520methods%2520derive%2520calibration%2520parameters%2520under%2520the%2520assumption%250Athat%2520the%2520methods%2520require%2520full%2520motion%2520in%2520all%2520axes.%2520When%2520IMU%2520and%2520LiDAR%2520are%250Amounted%2520on%2520a%2520ground%2520robot%2520the%2520motion%2520of%2520which%2520is%2520restricted%2520to%2520planar%2520motion%252C%250Aexisting%2520calibration%2520methods%2520are%2520likely%2520to%2520exhibit%2520degraded%2520performance.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520present%2520GRIL-Calib%253A%2520a%2520novel%2520targetless%2520Ground%2520Robot%250AIMU-LiDAR%2520Calibration%2520method.%2520Our%2520proposed%2520method%2520leverages%2520ground%2520information%250Ato%2520compensate%2520for%2520the%2520lack%2520of%2520unrestricted%2520full%2520motion.%2520First%252C%2520we%2520propose%2520LiDAR%250AOdometry%2520%2528LO%2529%2520using%2520ground%2520plane%2520residuals%2520to%2520enhance%2520calibration%2520accuracy.%250ASecond%252C%2520we%2520propose%2520the%2520Ground%2520Plane%2520Motion%2520%2528GPM%2529%2520constraint%2520and%2520incorporate%2520it%250Ainto%2520the%2520optimization%2520for%2520calibration%252C%2520enabling%2520the%2520determination%2520of%2520full%25206-DoF%250Aextrinsic%2520parameters%252C%2520including%2520theoretically%2520unobservable%2520direction.%2520Finally%252C%250Aunlike%2520baseline%2520methods%252C%2520we%2520formulate%2520the%2520calibration%2520not%2520as%2520sequential%2520two%250Aoptimizations%2520but%2520as%2520a%2520single%2520optimization%2520%2528SO%2529%2520problem%252C%2520solving%2520all%250Acalibration%2520parameters%2520simultaneously%2520and%2520improving%2520accuracy.%2520We%2520validate%2520our%250AGRIL-Calib%2520by%2520applying%2520it%2520to%2520various%2520real-world%2520datasets%2520and%2520comparing%2520its%250Aperformance%2520with%2520that%2520of%2520existing%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520accuracy%250Aand%2520robustness.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Taeyoung96/GRIL-Calib.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRIL-Calib%3A%20Targetless%20Ground%20Robot%20IMU-LiDAR%20Extrinsic%20Calibration%0A%20%20Method%20using%20Ground%20Plane%20Motion%20Constraints&entry.906535625=TaeYoung%20Kim%20and%20Gyuhyeon%20Pak%20and%20Euntai%20Kim&entry.1292438233=%20%20Targetless%20IMU-LiDAR%20extrinsic%20calibration%20methods%20are%20gaining%20significant%0Aattention%20as%20the%20importance%20of%20the%20IMU-LiDAR%20fusion%20system%20increases.%20Notably%2C%0Aexisting%20calibration%20methods%20derive%20calibration%20parameters%20under%20the%20assumption%0Athat%20the%20methods%20require%20full%20motion%20in%20all%20axes.%20When%20IMU%20and%20LiDAR%20are%0Amounted%20on%20a%20ground%20robot%20the%20motion%20of%20which%20is%20restricted%20to%20planar%20motion%2C%0Aexisting%20calibration%20methods%20are%20likely%20to%20exhibit%20degraded%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20present%20GRIL-Calib%3A%20a%20novel%20targetless%20Ground%20Robot%0AIMU-LiDAR%20Calibration%20method.%20Our%20proposed%20method%20leverages%20ground%20information%0Ato%20compensate%20for%20the%20lack%20of%20unrestricted%20full%20motion.%20First%2C%20we%20propose%20LiDAR%0AOdometry%20%28LO%29%20using%20ground%20plane%20residuals%20to%20enhance%20calibration%20accuracy.%0ASecond%2C%20we%20propose%20the%20Ground%20Plane%20Motion%20%28GPM%29%20constraint%20and%20incorporate%20it%0Ainto%20the%20optimization%20for%20calibration%2C%20enabling%20the%20determination%20of%20full%206-DoF%0Aextrinsic%20parameters%2C%20including%20theoretically%20unobservable%20direction.%20Finally%2C%0Aunlike%20baseline%20methods%2C%20we%20formulate%20the%20calibration%20not%20as%20sequential%20two%0Aoptimizations%20but%20as%20a%20single%20optimization%20%28SO%29%20problem%2C%20solving%20all%0Acalibration%20parameters%20simultaneously%20and%20improving%20accuracy.%20We%20validate%20our%0AGRIL-Calib%20by%20applying%20it%20to%20various%20real-world%20datasets%20and%20comparing%20its%0Aperformance%20with%20that%20of%20existing%20state-of-the-art%20methods%20in%20terms%20of%20accuracy%0Aand%20robustness.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Taeyoung96/GRIL-Calib.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14035v2&entry.124074799=Read"},
{"title": "Efficient Adversarial Training in LLMs with Continuous Attacks", "author": "Sophie Xhonneux and Alessandro Sordoni and Stephan G\u00fcnnemann and Gauthier Gidel and Leo Schwinn", "abstract": "  Large language models (LLMs) are vulnerable to adversarial attacks that can\nbypass their safety guardrails. In many domains, adversarial training has\nproven to be one of the most promising methods to reliably improve robustness\nagainst such attacks. Yet, in the context of LLMs, current methods for\nadversarial training are hindered by the high computational costs required to\nperform discrete adversarial attacks at each training iteration. We address\nthis problem by instead calculating adversarial attacks in the continuous\nembedding space of the LLM, which is orders of magnitudes more efficient. We\npropose a fast adversarial training algorithm (C-AdvUL) composed of two losses:\nthe first makes the model robust on continuous embedding attacks computed on an\nadversarial behaviour dataset; the second ensures the usefulness of the final\nmodel by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an\nadversarial variant of IPO that does not require utility data for adversarially\nrobust alignment. Our empirical evaluation on four models from different\nfamilies (Gemma, Phi3, Mistral, Zephyr) and at different scales (2B, 3.8B, 7B)\nshows that both algorithms substantially enhance LLM robustness against\ndiscrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results\ndemonstrate that robustness to continuous perturbations can extrapolate to\ndiscrete threat models. Thereby, we present a path toward scalable adversarial\ntraining algorithms for robustly aligning LLMs.\n", "link": "http://arxiv.org/abs/2405.15589v1", "date": "2024-05-24", "relevancy": 2.1046, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5398}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Adversarial%20Training%20in%20LLMs%20with%20Continuous%20Attacks&body=Title%3A%20Efficient%20Adversarial%20Training%20in%20LLMs%20with%20Continuous%20Attacks%0AAuthor%3A%20Sophie%20Xhonneux%20and%20Alessandro%20Sordoni%20and%20Stephan%20G%C3%BCnnemann%20and%20Gauthier%20Gidel%20and%20Leo%20Schwinn%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20vulnerable%20to%20adversarial%20attacks%20that%20can%0Abypass%20their%20safety%20guardrails.%20In%20many%20domains%2C%20adversarial%20training%20has%0Aproven%20to%20be%20one%20of%20the%20most%20promising%20methods%20to%20reliably%20improve%20robustness%0Aagainst%20such%20attacks.%20Yet%2C%20in%20the%20context%20of%20LLMs%2C%20current%20methods%20for%0Aadversarial%20training%20are%20hindered%20by%20the%20high%20computational%20costs%20required%20to%0Aperform%20discrete%20adversarial%20attacks%20at%20each%20training%20iteration.%20We%20address%0Athis%20problem%20by%20instead%20calculating%20adversarial%20attacks%20in%20the%20continuous%0Aembedding%20space%20of%20the%20LLM%2C%20which%20is%20orders%20of%20magnitudes%20more%20efficient.%20We%0Apropose%20a%20fast%20adversarial%20training%20algorithm%20%28C-AdvUL%29%20composed%20of%20two%20losses%3A%0Athe%20first%20makes%20the%20model%20robust%20on%20continuous%20embedding%20attacks%20computed%20on%20an%0Aadversarial%20behaviour%20dataset%3B%20the%20second%20ensures%20the%20usefulness%20of%20the%20final%0Amodel%20by%20fine-tuning%20on%20utility%20data.%20Moreover%2C%20we%20introduce%20C-AdvIPO%2C%20an%0Aadversarial%20variant%20of%20IPO%20that%20does%20not%20require%20utility%20data%20for%20adversarially%0Arobust%20alignment.%20Our%20empirical%20evaluation%20on%20four%20models%20from%20different%0Afamilies%20%28Gemma%2C%20Phi3%2C%20Mistral%2C%20Zephyr%29%20and%20at%20different%20scales%20%282B%2C%203.8B%2C%207B%29%0Ashows%20that%20both%20algorithms%20substantially%20enhance%20LLM%20robustness%20against%0Adiscrete%20attacks%20%28GCG%2C%20AutoDAN%2C%20PAIR%29%2C%20while%20maintaining%20utility.%20Our%20results%0Ademonstrate%20that%20robustness%20to%20continuous%20perturbations%20can%20extrapolate%20to%0Adiscrete%20threat%20models.%20Thereby%2C%20we%20present%20a%20path%20toward%20scalable%20adversarial%0Atraining%20algorithms%20for%20robustly%20aligning%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Adversarial%2520Training%2520in%2520LLMs%2520with%2520Continuous%2520Attacks%26entry.906535625%3DSophie%2520Xhonneux%2520and%2520Alessandro%2520Sordoni%2520and%2520Stephan%2520G%25C3%25BCnnemann%2520and%2520Gauthier%2520Gidel%2520and%2520Leo%2520Schwinn%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520vulnerable%2520to%2520adversarial%2520attacks%2520that%2520can%250Abypass%2520their%2520safety%2520guardrails.%2520In%2520many%2520domains%252C%2520adversarial%2520training%2520has%250Aproven%2520to%2520be%2520one%2520of%2520the%2520most%2520promising%2520methods%2520to%2520reliably%2520improve%2520robustness%250Aagainst%2520such%2520attacks.%2520Yet%252C%2520in%2520the%2520context%2520of%2520LLMs%252C%2520current%2520methods%2520for%250Aadversarial%2520training%2520are%2520hindered%2520by%2520the%2520high%2520computational%2520costs%2520required%2520to%250Aperform%2520discrete%2520adversarial%2520attacks%2520at%2520each%2520training%2520iteration.%2520We%2520address%250Athis%2520problem%2520by%2520instead%2520calculating%2520adversarial%2520attacks%2520in%2520the%2520continuous%250Aembedding%2520space%2520of%2520the%2520LLM%252C%2520which%2520is%2520orders%2520of%2520magnitudes%2520more%2520efficient.%2520We%250Apropose%2520a%2520fast%2520adversarial%2520training%2520algorithm%2520%2528C-AdvUL%2529%2520composed%2520of%2520two%2520losses%253A%250Athe%2520first%2520makes%2520the%2520model%2520robust%2520on%2520continuous%2520embedding%2520attacks%2520computed%2520on%2520an%250Aadversarial%2520behaviour%2520dataset%253B%2520the%2520second%2520ensures%2520the%2520usefulness%2520of%2520the%2520final%250Amodel%2520by%2520fine-tuning%2520on%2520utility%2520data.%2520Moreover%252C%2520we%2520introduce%2520C-AdvIPO%252C%2520an%250Aadversarial%2520variant%2520of%2520IPO%2520that%2520does%2520not%2520require%2520utility%2520data%2520for%2520adversarially%250Arobust%2520alignment.%2520Our%2520empirical%2520evaluation%2520on%2520four%2520models%2520from%2520different%250Afamilies%2520%2528Gemma%252C%2520Phi3%252C%2520Mistral%252C%2520Zephyr%2529%2520and%2520at%2520different%2520scales%2520%25282B%252C%25203.8B%252C%25207B%2529%250Ashows%2520that%2520both%2520algorithms%2520substantially%2520enhance%2520LLM%2520robustness%2520against%250Adiscrete%2520attacks%2520%2528GCG%252C%2520AutoDAN%252C%2520PAIR%2529%252C%2520while%2520maintaining%2520utility.%2520Our%2520results%250Ademonstrate%2520that%2520robustness%2520to%2520continuous%2520perturbations%2520can%2520extrapolate%2520to%250Adiscrete%2520threat%2520models.%2520Thereby%252C%2520we%2520present%2520a%2520path%2520toward%2520scalable%2520adversarial%250Atraining%2520algorithms%2520for%2520robustly%2520aligning%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Adversarial%20Training%20in%20LLMs%20with%20Continuous%20Attacks&entry.906535625=Sophie%20Xhonneux%20and%20Alessandro%20Sordoni%20and%20Stephan%20G%C3%BCnnemann%20and%20Gauthier%20Gidel%20and%20Leo%20Schwinn&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20vulnerable%20to%20adversarial%20attacks%20that%20can%0Abypass%20their%20safety%20guardrails.%20In%20many%20domains%2C%20adversarial%20training%20has%0Aproven%20to%20be%20one%20of%20the%20most%20promising%20methods%20to%20reliably%20improve%20robustness%0Aagainst%20such%20attacks.%20Yet%2C%20in%20the%20context%20of%20LLMs%2C%20current%20methods%20for%0Aadversarial%20training%20are%20hindered%20by%20the%20high%20computational%20costs%20required%20to%0Aperform%20discrete%20adversarial%20attacks%20at%20each%20training%20iteration.%20We%20address%0Athis%20problem%20by%20instead%20calculating%20adversarial%20attacks%20in%20the%20continuous%0Aembedding%20space%20of%20the%20LLM%2C%20which%20is%20orders%20of%20magnitudes%20more%20efficient.%20We%0Apropose%20a%20fast%20adversarial%20training%20algorithm%20%28C-AdvUL%29%20composed%20of%20two%20losses%3A%0Athe%20first%20makes%20the%20model%20robust%20on%20continuous%20embedding%20attacks%20computed%20on%20an%0Aadversarial%20behaviour%20dataset%3B%20the%20second%20ensures%20the%20usefulness%20of%20the%20final%0Amodel%20by%20fine-tuning%20on%20utility%20data.%20Moreover%2C%20we%20introduce%20C-AdvIPO%2C%20an%0Aadversarial%20variant%20of%20IPO%20that%20does%20not%20require%20utility%20data%20for%20adversarially%0Arobust%20alignment.%20Our%20empirical%20evaluation%20on%20four%20models%20from%20different%0Afamilies%20%28Gemma%2C%20Phi3%2C%20Mistral%2C%20Zephyr%29%20and%20at%20different%20scales%20%282B%2C%203.8B%2C%207B%29%0Ashows%20that%20both%20algorithms%20substantially%20enhance%20LLM%20robustness%20against%0Adiscrete%20attacks%20%28GCG%2C%20AutoDAN%2C%20PAIR%29%2C%20while%20maintaining%20utility.%20Our%20results%0Ademonstrate%20that%20robustness%20to%20continuous%20perturbations%20can%20extrapolate%20to%0Adiscrete%20threat%20models.%20Thereby%2C%20we%20present%20a%20path%20toward%20scalable%20adversarial%0Atraining%20algorithms%20for%20robustly%20aligning%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15589v1&entry.124074799=Read"},
{"title": "Fast Sampling Through The Reuse Of Attention Maps In Diffusion Models", "author": "Rosco Hunter and \u0141ukasz Dudziak and Mohamed S. Abdelfattah and Abhinav Mehrotra and Sourav Bhattacharya and Hongkai Wen", "abstract": "  Text-to-image diffusion models have demonstrated unprecedented capabilities\nfor flexible and realistic image synthesis. Nevertheless, these models rely on\na time-consuming sampling procedure, which has motivated attempts to reduce\ntheir latency. When improving efficiency, researchers often use the original\ndiffusion model to train an additional network designed specifically for fast\nimage generation. In contrast, our approach seeks to reduce latency directly,\nwithout any retraining, fine-tuning, or knowledge distillation. In particular,\nwe find the repeated calculation of attention maps to be costly yet redundant,\nand instead suggest reusing them during sampling. Our specific reuse strategies\nare based on ODE theory, which implies that the later a map is reused, the\nsmaller the distortion in the final image. We empirically compare these reuse\nstrategies with few-step sampling procedures of comparable latency, finding\nthat reuse generates images that are closer to those produced by the original\nhigh-latency diffusion model.\n", "link": "http://arxiv.org/abs/2401.01008v2", "date": "2024-05-24", "relevancy": 2.0903, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.7114}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7043}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Sampling%20Through%20The%20Reuse%20Of%20Attention%20Maps%20In%20Diffusion%20Models&body=Title%3A%20Fast%20Sampling%20Through%20The%20Reuse%20Of%20Attention%20Maps%20In%20Diffusion%20Models%0AAuthor%3A%20Rosco%20Hunter%20and%20%C5%81ukasz%20Dudziak%20and%20Mohamed%20S.%20Abdelfattah%20and%20Abhinav%20Mehrotra%20and%20Sourav%20Bhattacharya%20and%20Hongkai%20Wen%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20demonstrated%20unprecedented%20capabilities%0Afor%20flexible%20and%20realistic%20image%20synthesis.%20Nevertheless%2C%20these%20models%20rely%20on%0Aa%20time-consuming%20sampling%20procedure%2C%20which%20has%20motivated%20attempts%20to%20reduce%0Atheir%20latency.%20When%20improving%20efficiency%2C%20researchers%20often%20use%20the%20original%0Adiffusion%20model%20to%20train%20an%20additional%20network%20designed%20specifically%20for%20fast%0Aimage%20generation.%20In%20contrast%2C%20our%20approach%20seeks%20to%20reduce%20latency%20directly%2C%0Awithout%20any%20retraining%2C%20fine-tuning%2C%20or%20knowledge%20distillation.%20In%20particular%2C%0Awe%20find%20the%20repeated%20calculation%20of%20attention%20maps%20to%20be%20costly%20yet%20redundant%2C%0Aand%20instead%20suggest%20reusing%20them%20during%20sampling.%20Our%20specific%20reuse%20strategies%0Aare%20based%20on%20ODE%20theory%2C%20which%20implies%20that%20the%20later%20a%20map%20is%20reused%2C%20the%0Asmaller%20the%20distortion%20in%20the%20final%20image.%20We%20empirically%20compare%20these%20reuse%0Astrategies%20with%20few-step%20sampling%20procedures%20of%20comparable%20latency%2C%20finding%0Athat%20reuse%20generates%20images%20that%20are%20closer%20to%20those%20produced%20by%20the%20original%0Ahigh-latency%20diffusion%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01008v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Sampling%2520Through%2520The%2520Reuse%2520Of%2520Attention%2520Maps%2520In%2520Diffusion%2520Models%26entry.906535625%3DRosco%2520Hunter%2520and%2520%25C5%2581ukasz%2520Dudziak%2520and%2520Mohamed%2520S.%2520Abdelfattah%2520and%2520Abhinav%2520Mehrotra%2520and%2520Sourav%2520Bhattacharya%2520and%2520Hongkai%2520Wen%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520have%2520demonstrated%2520unprecedented%2520capabilities%250Afor%2520flexible%2520and%2520realistic%2520image%2520synthesis.%2520Nevertheless%252C%2520these%2520models%2520rely%2520on%250Aa%2520time-consuming%2520sampling%2520procedure%252C%2520which%2520has%2520motivated%2520attempts%2520to%2520reduce%250Atheir%2520latency.%2520When%2520improving%2520efficiency%252C%2520researchers%2520often%2520use%2520the%2520original%250Adiffusion%2520model%2520to%2520train%2520an%2520additional%2520network%2520designed%2520specifically%2520for%2520fast%250Aimage%2520generation.%2520In%2520contrast%252C%2520our%2520approach%2520seeks%2520to%2520reduce%2520latency%2520directly%252C%250Awithout%2520any%2520retraining%252C%2520fine-tuning%252C%2520or%2520knowledge%2520distillation.%2520In%2520particular%252C%250Awe%2520find%2520the%2520repeated%2520calculation%2520of%2520attention%2520maps%2520to%2520be%2520costly%2520yet%2520redundant%252C%250Aand%2520instead%2520suggest%2520reusing%2520them%2520during%2520sampling.%2520Our%2520specific%2520reuse%2520strategies%250Aare%2520based%2520on%2520ODE%2520theory%252C%2520which%2520implies%2520that%2520the%2520later%2520a%2520map%2520is%2520reused%252C%2520the%250Asmaller%2520the%2520distortion%2520in%2520the%2520final%2520image.%2520We%2520empirically%2520compare%2520these%2520reuse%250Astrategies%2520with%2520few-step%2520sampling%2520procedures%2520of%2520comparable%2520latency%252C%2520finding%250Athat%2520reuse%2520generates%2520images%2520that%2520are%2520closer%2520to%2520those%2520produced%2520by%2520the%2520original%250Ahigh-latency%2520diffusion%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01008v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Sampling%20Through%20The%20Reuse%20Of%20Attention%20Maps%20In%20Diffusion%20Models&entry.906535625=Rosco%20Hunter%20and%20%C5%81ukasz%20Dudziak%20and%20Mohamed%20S.%20Abdelfattah%20and%20Abhinav%20Mehrotra%20and%20Sourav%20Bhattacharya%20and%20Hongkai%20Wen&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20demonstrated%20unprecedented%20capabilities%0Afor%20flexible%20and%20realistic%20image%20synthesis.%20Nevertheless%2C%20these%20models%20rely%20on%0Aa%20time-consuming%20sampling%20procedure%2C%20which%20has%20motivated%20attempts%20to%20reduce%0Atheir%20latency.%20When%20improving%20efficiency%2C%20researchers%20often%20use%20the%20original%0Adiffusion%20model%20to%20train%20an%20additional%20network%20designed%20specifically%20for%20fast%0Aimage%20generation.%20In%20contrast%2C%20our%20approach%20seeks%20to%20reduce%20latency%20directly%2C%0Awithout%20any%20retraining%2C%20fine-tuning%2C%20or%20knowledge%20distillation.%20In%20particular%2C%0Awe%20find%20the%20repeated%20calculation%20of%20attention%20maps%20to%20be%20costly%20yet%20redundant%2C%0Aand%20instead%20suggest%20reusing%20them%20during%20sampling.%20Our%20specific%20reuse%20strategies%0Aare%20based%20on%20ODE%20theory%2C%20which%20implies%20that%20the%20later%20a%20map%20is%20reused%2C%20the%0Asmaller%20the%20distortion%20in%20the%20final%20image.%20We%20empirically%20compare%20these%20reuse%0Astrategies%20with%20few-step%20sampling%20procedures%20of%20comparable%20latency%2C%20finding%0Athat%20reuse%20generates%20images%20that%20are%20closer%20to%20those%20produced%20by%20the%20original%0Ahigh-latency%20diffusion%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01008v2&entry.124074799=Read"},
{"title": "Sparse Spectral Training and Inference on Euclidean and Hyperbolic\n  Neural Networks", "author": "Jialin Zhao and Yingtao Zhang and Xinghang Li and Huaping Liu and Carlo Vittorio Cannistraci", "abstract": "  The growing computational demands posed by increasingly number of neural\nnetwork's parameters necessitate low-memory-consumption training approaches.\nPrevious memory reduction techniques, such as Low-Rank Adaptation (LoRA) and\nReLoRA, suffer from the limitation of low rank and saddle point issues,\nparticularly during intensive tasks like pre-training. In this paper, we\npropose Sparse Spectral Training (SST), an advanced training methodology that\nupdates all singular values and selectively updates singular vectors of network\nweights, thereby optimizing resource usage while closely approximating\nfull-rank training. SST refines the training process by employing a targeted\nupdating strategy for singular vectors, which is determined by a multinomial\nsampling method weighted by the significance of the singular values, ensuring\nboth high performance and memory reduction. Through comprehensive testing on\nboth Euclidean and hyperbolic neural networks across various tasks, including\nnatural language generation, machine translation, node classification and link\nprediction, SST demonstrates its capability to outperform existing memory\nreduction training methods and is comparable with full-rank training in some\ncases. On OPT-125M, with rank equating to 8.3% of embedding dimension, SST\nreduces the perplexity gap to full-rank training by 67.6%, demonstrating a\nsignificant reduction of the performance loss with prevalent low-rank methods.\nThis approach offers a strong alternative to traditional training techniques,\npaving the way for more efficient and scalable neural network training\nsolutions.\n", "link": "http://arxiv.org/abs/2405.15481v1", "date": "2024-05-24", "relevancy": 2.089, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5294}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5212}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Spectral%20Training%20and%20Inference%20on%20Euclidean%20and%20Hyperbolic%0A%20%20Neural%20Networks&body=Title%3A%20Sparse%20Spectral%20Training%20and%20Inference%20on%20Euclidean%20and%20Hyperbolic%0A%20%20Neural%20Networks%0AAuthor%3A%20Jialin%20Zhao%20and%20Yingtao%20Zhang%20and%20Xinghang%20Li%20and%20Huaping%20Liu%20and%20Carlo%20Vittorio%20Cannistraci%0AAbstract%3A%20%20%20The%20growing%20computational%20demands%20posed%20by%20increasingly%20number%20of%20neural%0Anetwork%27s%20parameters%20necessitate%20low-memory-consumption%20training%20approaches.%0APrevious%20memory%20reduction%20techniques%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20and%0AReLoRA%2C%20suffer%20from%20the%20limitation%20of%20low%20rank%20and%20saddle%20point%20issues%2C%0Aparticularly%20during%20intensive%20tasks%20like%20pre-training.%20In%20this%20paper%2C%20we%0Apropose%20Sparse%20Spectral%20Training%20%28SST%29%2C%20an%20advanced%20training%20methodology%20that%0Aupdates%20all%20singular%20values%20and%20selectively%20updates%20singular%20vectors%20of%20network%0Aweights%2C%20thereby%20optimizing%20resource%20usage%20while%20closely%20approximating%0Afull-rank%20training.%20SST%20refines%20the%20training%20process%20by%20employing%20a%20targeted%0Aupdating%20strategy%20for%20singular%20vectors%2C%20which%20is%20determined%20by%20a%20multinomial%0Asampling%20method%20weighted%20by%20the%20significance%20of%20the%20singular%20values%2C%20ensuring%0Aboth%20high%20performance%20and%20memory%20reduction.%20Through%20comprehensive%20testing%20on%0Aboth%20Euclidean%20and%20hyperbolic%20neural%20networks%20across%20various%20tasks%2C%20including%0Anatural%20language%20generation%2C%20machine%20translation%2C%20node%20classification%20and%20link%0Aprediction%2C%20SST%20demonstrates%20its%20capability%20to%20outperform%20existing%20memory%0Areduction%20training%20methods%20and%20is%20comparable%20with%20full-rank%20training%20in%20some%0Acases.%20On%20OPT-125M%2C%20with%20rank%20equating%20to%208.3%25%20of%20embedding%20dimension%2C%20SST%0Areduces%20the%20perplexity%20gap%20to%20full-rank%20training%20by%2067.6%25%2C%20demonstrating%20a%0Asignificant%20reduction%20of%20the%20performance%20loss%20with%20prevalent%20low-rank%20methods.%0AThis%20approach%20offers%20a%20strong%20alternative%20to%20traditional%20training%20techniques%2C%0Apaving%20the%20way%20for%20more%20efficient%20and%20scalable%20neural%20network%20training%0Asolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Spectral%2520Training%2520and%2520Inference%2520on%2520Euclidean%2520and%2520Hyperbolic%250A%2520%2520Neural%2520Networks%26entry.906535625%3DJialin%2520Zhao%2520and%2520Yingtao%2520Zhang%2520and%2520Xinghang%2520Li%2520and%2520Huaping%2520Liu%2520and%2520Carlo%2520Vittorio%2520Cannistraci%26entry.1292438233%3D%2520%2520The%2520growing%2520computational%2520demands%2520posed%2520by%2520increasingly%2520number%2520of%2520neural%250Anetwork%2527s%2520parameters%2520necessitate%2520low-memory-consumption%2520training%2520approaches.%250APrevious%2520memory%2520reduction%2520techniques%252C%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520and%250AReLoRA%252C%2520suffer%2520from%2520the%2520limitation%2520of%2520low%2520rank%2520and%2520saddle%2520point%2520issues%252C%250Aparticularly%2520during%2520intensive%2520tasks%2520like%2520pre-training.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Sparse%2520Spectral%2520Training%2520%2528SST%2529%252C%2520an%2520advanced%2520training%2520methodology%2520that%250Aupdates%2520all%2520singular%2520values%2520and%2520selectively%2520updates%2520singular%2520vectors%2520of%2520network%250Aweights%252C%2520thereby%2520optimizing%2520resource%2520usage%2520while%2520closely%2520approximating%250Afull-rank%2520training.%2520SST%2520refines%2520the%2520training%2520process%2520by%2520employing%2520a%2520targeted%250Aupdating%2520strategy%2520for%2520singular%2520vectors%252C%2520which%2520is%2520determined%2520by%2520a%2520multinomial%250Asampling%2520method%2520weighted%2520by%2520the%2520significance%2520of%2520the%2520singular%2520values%252C%2520ensuring%250Aboth%2520high%2520performance%2520and%2520memory%2520reduction.%2520Through%2520comprehensive%2520testing%2520on%250Aboth%2520Euclidean%2520and%2520hyperbolic%2520neural%2520networks%2520across%2520various%2520tasks%252C%2520including%250Anatural%2520language%2520generation%252C%2520machine%2520translation%252C%2520node%2520classification%2520and%2520link%250Aprediction%252C%2520SST%2520demonstrates%2520its%2520capability%2520to%2520outperform%2520existing%2520memory%250Areduction%2520training%2520methods%2520and%2520is%2520comparable%2520with%2520full-rank%2520training%2520in%2520some%250Acases.%2520On%2520OPT-125M%252C%2520with%2520rank%2520equating%2520to%25208.3%2525%2520of%2520embedding%2520dimension%252C%2520SST%250Areduces%2520the%2520perplexity%2520gap%2520to%2520full-rank%2520training%2520by%252067.6%2525%252C%2520demonstrating%2520a%250Asignificant%2520reduction%2520of%2520the%2520performance%2520loss%2520with%2520prevalent%2520low-rank%2520methods.%250AThis%2520approach%2520offers%2520a%2520strong%2520alternative%2520to%2520traditional%2520training%2520techniques%252C%250Apaving%2520the%2520way%2520for%2520more%2520efficient%2520and%2520scalable%2520neural%2520network%2520training%250Asolutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Spectral%20Training%20and%20Inference%20on%20Euclidean%20and%20Hyperbolic%0A%20%20Neural%20Networks&entry.906535625=Jialin%20Zhao%20and%20Yingtao%20Zhang%20and%20Xinghang%20Li%20and%20Huaping%20Liu%20and%20Carlo%20Vittorio%20Cannistraci&entry.1292438233=%20%20The%20growing%20computational%20demands%20posed%20by%20increasingly%20number%20of%20neural%0Anetwork%27s%20parameters%20necessitate%20low-memory-consumption%20training%20approaches.%0APrevious%20memory%20reduction%20techniques%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20and%0AReLoRA%2C%20suffer%20from%20the%20limitation%20of%20low%20rank%20and%20saddle%20point%20issues%2C%0Aparticularly%20during%20intensive%20tasks%20like%20pre-training.%20In%20this%20paper%2C%20we%0Apropose%20Sparse%20Spectral%20Training%20%28SST%29%2C%20an%20advanced%20training%20methodology%20that%0Aupdates%20all%20singular%20values%20and%20selectively%20updates%20singular%20vectors%20of%20network%0Aweights%2C%20thereby%20optimizing%20resource%20usage%20while%20closely%20approximating%0Afull-rank%20training.%20SST%20refines%20the%20training%20process%20by%20employing%20a%20targeted%0Aupdating%20strategy%20for%20singular%20vectors%2C%20which%20is%20determined%20by%20a%20multinomial%0Asampling%20method%20weighted%20by%20the%20significance%20of%20the%20singular%20values%2C%20ensuring%0Aboth%20high%20performance%20and%20memory%20reduction.%20Through%20comprehensive%20testing%20on%0Aboth%20Euclidean%20and%20hyperbolic%20neural%20networks%20across%20various%20tasks%2C%20including%0Anatural%20language%20generation%2C%20machine%20translation%2C%20node%20classification%20and%20link%0Aprediction%2C%20SST%20demonstrates%20its%20capability%20to%20outperform%20existing%20memory%0Areduction%20training%20methods%20and%20is%20comparable%20with%20full-rank%20training%20in%20some%0Acases.%20On%20OPT-125M%2C%20with%20rank%20equating%20to%208.3%25%20of%20embedding%20dimension%2C%20SST%0Areduces%20the%20perplexity%20gap%20to%20full-rank%20training%20by%2067.6%25%2C%20demonstrating%20a%0Asignificant%20reduction%20of%20the%20performance%20loss%20with%20prevalent%20low-rank%20methods.%0AThis%20approach%20offers%20a%20strong%20alternative%20to%20traditional%20training%20techniques%2C%0Apaving%20the%20way%20for%20more%20efficient%20and%20scalable%20neural%20network%20training%0Asolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15481v1&entry.124074799=Read"},
{"title": "MambaVC: Learned Visual Compression with Selective State Spaces", "author": "Shiyu Qin and Jinpeng Wang and Yimin Zhou and Bin Chen and Tianci Luo and Baoyi An and Tao Dai and Shutao Xia and Yaowei Wang", "abstract": "  Learned visual compression is an important and active task in multimedia.\nExisting approaches have explored various CNN- and Transformer-based designs to\nmodel content distribution and eliminate redundancy, where balancing efficacy\n(i.e., rate-distortion trade-off) and efficiency remains a challenge. Recently,\nstate-space models (SSMs) have shown promise due to their long-range modeling\ncapacity and efficiency. Inspired by this, we take the first step to explore\nSSMs for visual compression. We introduce MambaVC, a simple, strong and\nefficient compression network based on SSM. MambaVC develops a visual state\nspace (VSS) block with a 2D selective scanning (2DSS) module as the nonlinear\nactivation function after each downsampling, which helps to capture informative\nglobal contexts and enhances compression. On compression benchmark datasets,\nMambaVC achieves superior rate-distortion performance with lower computational\nand memory overheads. Specifically, it outperforms CNN and Transformer variants\nby 9.3% and 15.6% on Kodak, respectively, while reducing computation by 42% and\n24%, and saving 12% and 71% of memory. MambaVC shows even greater improvements\nwith high-resolution images, highlighting its potential and scalability in\nreal-world applications. We also provide a comprehensive comparison of\ndifferent network designs, underscoring MambaVC's advantages.\n", "link": "http://arxiv.org/abs/2405.15413v1", "date": "2024-05-24", "relevancy": 2.0785, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5486}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.522}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaVC%3A%20Learned%20Visual%20Compression%20with%20Selective%20State%20Spaces&body=Title%3A%20MambaVC%3A%20Learned%20Visual%20Compression%20with%20Selective%20State%20Spaces%0AAuthor%3A%20Shiyu%20Qin%20and%20Jinpeng%20Wang%20and%20Yimin%20Zhou%20and%20Bin%20Chen%20and%20Tianci%20Luo%20and%20Baoyi%20An%20and%20Tao%20Dai%20and%20Shutao%20Xia%20and%20Yaowei%20Wang%0AAbstract%3A%20%20%20Learned%20visual%20compression%20is%20an%20important%20and%20active%20task%20in%20multimedia.%0AExisting%20approaches%20have%20explored%20various%20CNN-%20and%20Transformer-based%20designs%20to%0Amodel%20content%20distribution%20and%20eliminate%20redundancy%2C%20where%20balancing%20efficacy%0A%28i.e.%2C%20rate-distortion%20trade-off%29%20and%20efficiency%20remains%20a%20challenge.%20Recently%2C%0Astate-space%20models%20%28SSMs%29%20have%20shown%20promise%20due%20to%20their%20long-range%20modeling%0Acapacity%20and%20efficiency.%20Inspired%20by%20this%2C%20we%20take%20the%20first%20step%20to%20explore%0ASSMs%20for%20visual%20compression.%20We%20introduce%20MambaVC%2C%20a%20simple%2C%20strong%20and%0Aefficient%20compression%20network%20based%20on%20SSM.%20MambaVC%20develops%20a%20visual%20state%0Aspace%20%28VSS%29%20block%20with%20a%202D%20selective%20scanning%20%282DSS%29%20module%20as%20the%20nonlinear%0Aactivation%20function%20after%20each%20downsampling%2C%20which%20helps%20to%20capture%20informative%0Aglobal%20contexts%20and%20enhances%20compression.%20On%20compression%20benchmark%20datasets%2C%0AMambaVC%20achieves%20superior%20rate-distortion%20performance%20with%20lower%20computational%0Aand%20memory%20overheads.%20Specifically%2C%20it%20outperforms%20CNN%20and%20Transformer%20variants%0Aby%209.3%25%20and%2015.6%25%20on%20Kodak%2C%20respectively%2C%20while%20reducing%20computation%20by%2042%25%20and%0A24%25%2C%20and%20saving%2012%25%20and%2071%25%20of%20memory.%20MambaVC%20shows%20even%20greater%20improvements%0Awith%20high-resolution%20images%2C%20highlighting%20its%20potential%20and%20scalability%20in%0Areal-world%20applications.%20We%20also%20provide%20a%20comprehensive%20comparison%20of%0Adifferent%20network%20designs%2C%20underscoring%20MambaVC%27s%20advantages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaVC%253A%2520Learned%2520Visual%2520Compression%2520with%2520Selective%2520State%2520Spaces%26entry.906535625%3DShiyu%2520Qin%2520and%2520Jinpeng%2520Wang%2520and%2520Yimin%2520Zhou%2520and%2520Bin%2520Chen%2520and%2520Tianci%2520Luo%2520and%2520Baoyi%2520An%2520and%2520Tao%2520Dai%2520and%2520Shutao%2520Xia%2520and%2520Yaowei%2520Wang%26entry.1292438233%3D%2520%2520Learned%2520visual%2520compression%2520is%2520an%2520important%2520and%2520active%2520task%2520in%2520multimedia.%250AExisting%2520approaches%2520have%2520explored%2520various%2520CNN-%2520and%2520Transformer-based%2520designs%2520to%250Amodel%2520content%2520distribution%2520and%2520eliminate%2520redundancy%252C%2520where%2520balancing%2520efficacy%250A%2528i.e.%252C%2520rate-distortion%2520trade-off%2529%2520and%2520efficiency%2520remains%2520a%2520challenge.%2520Recently%252C%250Astate-space%2520models%2520%2528SSMs%2529%2520have%2520shown%2520promise%2520due%2520to%2520their%2520long-range%2520modeling%250Acapacity%2520and%2520efficiency.%2520Inspired%2520by%2520this%252C%2520we%2520take%2520the%2520first%2520step%2520to%2520explore%250ASSMs%2520for%2520visual%2520compression.%2520We%2520introduce%2520MambaVC%252C%2520a%2520simple%252C%2520strong%2520and%250Aefficient%2520compression%2520network%2520based%2520on%2520SSM.%2520MambaVC%2520develops%2520a%2520visual%2520state%250Aspace%2520%2528VSS%2529%2520block%2520with%2520a%25202D%2520selective%2520scanning%2520%25282DSS%2529%2520module%2520as%2520the%2520nonlinear%250Aactivation%2520function%2520after%2520each%2520downsampling%252C%2520which%2520helps%2520to%2520capture%2520informative%250Aglobal%2520contexts%2520and%2520enhances%2520compression.%2520On%2520compression%2520benchmark%2520datasets%252C%250AMambaVC%2520achieves%2520superior%2520rate-distortion%2520performance%2520with%2520lower%2520computational%250Aand%2520memory%2520overheads.%2520Specifically%252C%2520it%2520outperforms%2520CNN%2520and%2520Transformer%2520variants%250Aby%25209.3%2525%2520and%252015.6%2525%2520on%2520Kodak%252C%2520respectively%252C%2520while%2520reducing%2520computation%2520by%252042%2525%2520and%250A24%2525%252C%2520and%2520saving%252012%2525%2520and%252071%2525%2520of%2520memory.%2520MambaVC%2520shows%2520even%2520greater%2520improvements%250Awith%2520high-resolution%2520images%252C%2520highlighting%2520its%2520potential%2520and%2520scalability%2520in%250Areal-world%2520applications.%2520We%2520also%2520provide%2520a%2520comprehensive%2520comparison%2520of%250Adifferent%2520network%2520designs%252C%2520underscoring%2520MambaVC%2527s%2520advantages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaVC%3A%20Learned%20Visual%20Compression%20with%20Selective%20State%20Spaces&entry.906535625=Shiyu%20Qin%20and%20Jinpeng%20Wang%20and%20Yimin%20Zhou%20and%20Bin%20Chen%20and%20Tianci%20Luo%20and%20Baoyi%20An%20and%20Tao%20Dai%20and%20Shutao%20Xia%20and%20Yaowei%20Wang&entry.1292438233=%20%20Learned%20visual%20compression%20is%20an%20important%20and%20active%20task%20in%20multimedia.%0AExisting%20approaches%20have%20explored%20various%20CNN-%20and%20Transformer-based%20designs%20to%0Amodel%20content%20distribution%20and%20eliminate%20redundancy%2C%20where%20balancing%20efficacy%0A%28i.e.%2C%20rate-distortion%20trade-off%29%20and%20efficiency%20remains%20a%20challenge.%20Recently%2C%0Astate-space%20models%20%28SSMs%29%20have%20shown%20promise%20due%20to%20their%20long-range%20modeling%0Acapacity%20and%20efficiency.%20Inspired%20by%20this%2C%20we%20take%20the%20first%20step%20to%20explore%0ASSMs%20for%20visual%20compression.%20We%20introduce%20MambaVC%2C%20a%20simple%2C%20strong%20and%0Aefficient%20compression%20network%20based%20on%20SSM.%20MambaVC%20develops%20a%20visual%20state%0Aspace%20%28VSS%29%20block%20with%20a%202D%20selective%20scanning%20%282DSS%29%20module%20as%20the%20nonlinear%0Aactivation%20function%20after%20each%20downsampling%2C%20which%20helps%20to%20capture%20informative%0Aglobal%20contexts%20and%20enhances%20compression.%20On%20compression%20benchmark%20datasets%2C%0AMambaVC%20achieves%20superior%20rate-distortion%20performance%20with%20lower%20computational%0Aand%20memory%20overheads.%20Specifically%2C%20it%20outperforms%20CNN%20and%20Transformer%20variants%0Aby%209.3%25%20and%2015.6%25%20on%20Kodak%2C%20respectively%2C%20while%20reducing%20computation%20by%2042%25%20and%0A24%25%2C%20and%20saving%2012%25%20and%2071%25%20of%20memory.%20MambaVC%20shows%20even%20greater%20improvements%0Awith%20high-resolution%20images%2C%20highlighting%20its%20potential%20and%20scalability%20in%0Areal-world%20applications.%20We%20also%20provide%20a%20comprehensive%20comparison%20of%0Adifferent%20network%20designs%2C%20underscoring%20MambaVC%27s%20advantages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15413v1&entry.124074799=Read"},
{"title": "Domain Generalisation for Object Detection under Covariate and Concept\n  Shift", "author": "Karthik Seemakurthy and Erchan Aptoula and Charles Fox and Petra Bosilj", "abstract": "  Domain generalisation aims to promote the learning of domain-invariant\nfeatures while suppressing domain-specific features, so that a model can\ngeneralise better to previously unseen target domains. An approach to domain\ngeneralisation for object detection is proposed, the first such approach\napplicable to any object detection architecture. Based on a rigorous\nmathematical analysis, we extend approaches based on feature alignment with a\nnovel component for performing class conditional alignment at the instance\nlevel, in addition to aligning the marginal feature distributions across\ndomains at the image level. This allows us to fully address both components of\ndomain shift, i.e. covariate and concept shift, and learn a domain agnostic\nfeature representation. We perform extensive evaluation with both one-stage\n(FCOS, YOLO) and two-stage (FRCNN) detectors, on a newly proposed benchmark\ncomprising several different datasets for autonomous driving applications\n(Cityscapes, BDD10K, ACDC, IDD) as well as the GWHD dataset for precision\nagriculture, and show consistent improvements to the generalisation and\nlocalisation performance over baselines and state-of-the-art.\n", "link": "http://arxiv.org/abs/2203.05294v4", "date": "2024-05-24", "relevancy": 2.0686, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.533}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5091}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalisation%20for%20Object%20Detection%20under%20Covariate%20and%20Concept%0A%20%20Shift&body=Title%3A%20Domain%20Generalisation%20for%20Object%20Detection%20under%20Covariate%20and%20Concept%0A%20%20Shift%0AAuthor%3A%20Karthik%20Seemakurthy%20and%20Erchan%20Aptoula%20and%20Charles%20Fox%20and%20Petra%20Bosilj%0AAbstract%3A%20%20%20Domain%20generalisation%20aims%20to%20promote%20the%20learning%20of%20domain-invariant%0Afeatures%20while%20suppressing%20domain-specific%20features%2C%20so%20that%20a%20model%20can%0Ageneralise%20better%20to%20previously%20unseen%20target%20domains.%20An%20approach%20to%20domain%0Ageneralisation%20for%20object%20detection%20is%20proposed%2C%20the%20first%20such%20approach%0Aapplicable%20to%20any%20object%20detection%20architecture.%20Based%20on%20a%20rigorous%0Amathematical%20analysis%2C%20we%20extend%20approaches%20based%20on%20feature%20alignment%20with%20a%0Anovel%20component%20for%20performing%20class%20conditional%20alignment%20at%20the%20instance%0Alevel%2C%20in%20addition%20to%20aligning%20the%20marginal%20feature%20distributions%20across%0Adomains%20at%20the%20image%20level.%20This%20allows%20us%20to%20fully%20address%20both%20components%20of%0Adomain%20shift%2C%20i.e.%20covariate%20and%20concept%20shift%2C%20and%20learn%20a%20domain%20agnostic%0Afeature%20representation.%20We%20perform%20extensive%20evaluation%20with%20both%20one-stage%0A%28FCOS%2C%20YOLO%29%20and%20two-stage%20%28FRCNN%29%20detectors%2C%20on%20a%20newly%20proposed%20benchmark%0Acomprising%20several%20different%20datasets%20for%20autonomous%20driving%20applications%0A%28Cityscapes%2C%20BDD10K%2C%20ACDC%2C%20IDD%29%20as%20well%20as%20the%20GWHD%20dataset%20for%20precision%0Aagriculture%2C%20and%20show%20consistent%20improvements%20to%20the%20generalisation%20and%0Alocalisation%20performance%20over%20baselines%20and%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.05294v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Generalisation%2520for%2520Object%2520Detection%2520under%2520Covariate%2520and%2520Concept%250A%2520%2520Shift%26entry.906535625%3DKarthik%2520Seemakurthy%2520and%2520Erchan%2520Aptoula%2520and%2520Charles%2520Fox%2520and%2520Petra%2520Bosilj%26entry.1292438233%3D%2520%2520Domain%2520generalisation%2520aims%2520to%2520promote%2520the%2520learning%2520of%2520domain-invariant%250Afeatures%2520while%2520suppressing%2520domain-specific%2520features%252C%2520so%2520that%2520a%2520model%2520can%250Ageneralise%2520better%2520to%2520previously%2520unseen%2520target%2520domains.%2520An%2520approach%2520to%2520domain%250Ageneralisation%2520for%2520object%2520detection%2520is%2520proposed%252C%2520the%2520first%2520such%2520approach%250Aapplicable%2520to%2520any%2520object%2520detection%2520architecture.%2520Based%2520on%2520a%2520rigorous%250Amathematical%2520analysis%252C%2520we%2520extend%2520approaches%2520based%2520on%2520feature%2520alignment%2520with%2520a%250Anovel%2520component%2520for%2520performing%2520class%2520conditional%2520alignment%2520at%2520the%2520instance%250Alevel%252C%2520in%2520addition%2520to%2520aligning%2520the%2520marginal%2520feature%2520distributions%2520across%250Adomains%2520at%2520the%2520image%2520level.%2520This%2520allows%2520us%2520to%2520fully%2520address%2520both%2520components%2520of%250Adomain%2520shift%252C%2520i.e.%2520covariate%2520and%2520concept%2520shift%252C%2520and%2520learn%2520a%2520domain%2520agnostic%250Afeature%2520representation.%2520We%2520perform%2520extensive%2520evaluation%2520with%2520both%2520one-stage%250A%2528FCOS%252C%2520YOLO%2529%2520and%2520two-stage%2520%2528FRCNN%2529%2520detectors%252C%2520on%2520a%2520newly%2520proposed%2520benchmark%250Acomprising%2520several%2520different%2520datasets%2520for%2520autonomous%2520driving%2520applications%250A%2528Cityscapes%252C%2520BDD10K%252C%2520ACDC%252C%2520IDD%2529%2520as%2520well%2520as%2520the%2520GWHD%2520dataset%2520for%2520precision%250Aagriculture%252C%2520and%2520show%2520consistent%2520improvements%2520to%2520the%2520generalisation%2520and%250Alocalisation%2520performance%2520over%2520baselines%2520and%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.05294v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalisation%20for%20Object%20Detection%20under%20Covariate%20and%20Concept%0A%20%20Shift&entry.906535625=Karthik%20Seemakurthy%20and%20Erchan%20Aptoula%20and%20Charles%20Fox%20and%20Petra%20Bosilj&entry.1292438233=%20%20Domain%20generalisation%20aims%20to%20promote%20the%20learning%20of%20domain-invariant%0Afeatures%20while%20suppressing%20domain-specific%20features%2C%20so%20that%20a%20model%20can%0Ageneralise%20better%20to%20previously%20unseen%20target%20domains.%20An%20approach%20to%20domain%0Ageneralisation%20for%20object%20detection%20is%20proposed%2C%20the%20first%20such%20approach%0Aapplicable%20to%20any%20object%20detection%20architecture.%20Based%20on%20a%20rigorous%0Amathematical%20analysis%2C%20we%20extend%20approaches%20based%20on%20feature%20alignment%20with%20a%0Anovel%20component%20for%20performing%20class%20conditional%20alignment%20at%20the%20instance%0Alevel%2C%20in%20addition%20to%20aligning%20the%20marginal%20feature%20distributions%20across%0Adomains%20at%20the%20image%20level.%20This%20allows%20us%20to%20fully%20address%20both%20components%20of%0Adomain%20shift%2C%20i.e.%20covariate%20and%20concept%20shift%2C%20and%20learn%20a%20domain%20agnostic%0Afeature%20representation.%20We%20perform%20extensive%20evaluation%20with%20both%20one-stage%0A%28FCOS%2C%20YOLO%29%20and%20two-stage%20%28FRCNN%29%20detectors%2C%20on%20a%20newly%20proposed%20benchmark%0Acomprising%20several%20different%20datasets%20for%20autonomous%20driving%20applications%0A%28Cityscapes%2C%20BDD10K%2C%20ACDC%2C%20IDD%29%20as%20well%20as%20the%20GWHD%20dataset%20for%20precision%0Aagriculture%2C%20and%20show%20consistent%20improvements%20to%20the%20generalisation%20and%0Alocalisation%20performance%20over%20baselines%20and%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.05294v4&entry.124074799=Read"},
{"title": "Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on\n  Data Missing Not at Random", "author": "Mingming Ha and Xuewen Tao and Wenfang Lin and Qionxu Ma and Wujiang Xu and Linxun Chen", "abstract": "  In most practical applications such as recommendation systems, display\nadvertising, and so forth, the collected data often contains missing values and\nthose missing values are generally missing-not-at-random, which deteriorates\nthe prediction performance of models. Some existing estimators and regularizers\nattempt to achieve unbiased estimation to improve the predictive performance.\nHowever, variances and generalization bound of these methods are generally\nunbounded when the propensity scores tend to zero, compromising their stability\nand robustness. In this paper, we first theoretically reveal that limitations\nof regularization techniques. Besides, we further illustrate that, for more\ngeneral estimators, unbiasedness will inevitably lead to unbounded variance.\nThese general laws inspire us that the estimator designs is not merely about\neliminating bias, reducing variance, or simply achieve a bias-variance\ntrade-off. Instead, it involves a quantitative joint optimization of bias and\nvariance. Then, we develop a systematic fine-grained dynamic learning framework\nto jointly optimize bias and variance, which adaptively selects an appropriate\nestimator for each user-item pair according to the predefined objective\nfunction. With this operation, the generalization bounds and variances of\nmodels are reduced and bounded with theoretical guarantees. Extensive\nexperiments are conducted to verify the theoretical results and the\neffectiveness of the proposed dynamic learning framework.\n", "link": "http://arxiv.org/abs/2405.15403v1", "date": "2024-05-24", "relevancy": 2.0663, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5327}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5211}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Dynamic%20Framework%20for%20Bias-Variance%20Joint%20Optimization%20on%0A%20%20Data%20Missing%20Not%20at%20Random&body=Title%3A%20Fine-Grained%20Dynamic%20Framework%20for%20Bias-Variance%20Joint%20Optimization%20on%0A%20%20Data%20Missing%20Not%20at%20Random%0AAuthor%3A%20Mingming%20Ha%20and%20Xuewen%20Tao%20and%20Wenfang%20Lin%20and%20Qionxu%20Ma%20and%20Wujiang%20Xu%20and%20Linxun%20Chen%0AAbstract%3A%20%20%20In%20most%20practical%20applications%20such%20as%20recommendation%20systems%2C%20display%0Aadvertising%2C%20and%20so%20forth%2C%20the%20collected%20data%20often%20contains%20missing%20values%20and%0Athose%20missing%20values%20are%20generally%20missing-not-at-random%2C%20which%20deteriorates%0Athe%20prediction%20performance%20of%20models.%20Some%20existing%20estimators%20and%20regularizers%0Aattempt%20to%20achieve%20unbiased%20estimation%20to%20improve%20the%20predictive%20performance.%0AHowever%2C%20variances%20and%20generalization%20bound%20of%20these%20methods%20are%20generally%0Aunbounded%20when%20the%20propensity%20scores%20tend%20to%20zero%2C%20compromising%20their%20stability%0Aand%20robustness.%20In%20this%20paper%2C%20we%20first%20theoretically%20reveal%20that%20limitations%0Aof%20regularization%20techniques.%20Besides%2C%20we%20further%20illustrate%20that%2C%20for%20more%0Ageneral%20estimators%2C%20unbiasedness%20will%20inevitably%20lead%20to%20unbounded%20variance.%0AThese%20general%20laws%20inspire%20us%20that%20the%20estimator%20designs%20is%20not%20merely%20about%0Aeliminating%20bias%2C%20reducing%20variance%2C%20or%20simply%20achieve%20a%20bias-variance%0Atrade-off.%20Instead%2C%20it%20involves%20a%20quantitative%20joint%20optimization%20of%20bias%20and%0Avariance.%20Then%2C%20we%20develop%20a%20systematic%20fine-grained%20dynamic%20learning%20framework%0Ato%20jointly%20optimize%20bias%20and%20variance%2C%20which%20adaptively%20selects%20an%20appropriate%0Aestimator%20for%20each%20user-item%20pair%20according%20to%20the%20predefined%20objective%0Afunction.%20With%20this%20operation%2C%20the%20generalization%20bounds%20and%20variances%20of%0Amodels%20are%20reduced%20and%20bounded%20with%20theoretical%20guarantees.%20Extensive%0Aexperiments%20are%20conducted%20to%20verify%20the%20theoretical%20results%20and%20the%0Aeffectiveness%20of%20the%20proposed%20dynamic%20learning%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Dynamic%2520Framework%2520for%2520Bias-Variance%2520Joint%2520Optimization%2520on%250A%2520%2520Data%2520Missing%2520Not%2520at%2520Random%26entry.906535625%3DMingming%2520Ha%2520and%2520Xuewen%2520Tao%2520and%2520Wenfang%2520Lin%2520and%2520Qionxu%2520Ma%2520and%2520Wujiang%2520Xu%2520and%2520Linxun%2520Chen%26entry.1292438233%3D%2520%2520In%2520most%2520practical%2520applications%2520such%2520as%2520recommendation%2520systems%252C%2520display%250Aadvertising%252C%2520and%2520so%2520forth%252C%2520the%2520collected%2520data%2520often%2520contains%2520missing%2520values%2520and%250Athose%2520missing%2520values%2520are%2520generally%2520missing-not-at-random%252C%2520which%2520deteriorates%250Athe%2520prediction%2520performance%2520of%2520models.%2520Some%2520existing%2520estimators%2520and%2520regularizers%250Aattempt%2520to%2520achieve%2520unbiased%2520estimation%2520to%2520improve%2520the%2520predictive%2520performance.%250AHowever%252C%2520variances%2520and%2520generalization%2520bound%2520of%2520these%2520methods%2520are%2520generally%250Aunbounded%2520when%2520the%2520propensity%2520scores%2520tend%2520to%2520zero%252C%2520compromising%2520their%2520stability%250Aand%2520robustness.%2520In%2520this%2520paper%252C%2520we%2520first%2520theoretically%2520reveal%2520that%2520limitations%250Aof%2520regularization%2520techniques.%2520Besides%252C%2520we%2520further%2520illustrate%2520that%252C%2520for%2520more%250Ageneral%2520estimators%252C%2520unbiasedness%2520will%2520inevitably%2520lead%2520to%2520unbounded%2520variance.%250AThese%2520general%2520laws%2520inspire%2520us%2520that%2520the%2520estimator%2520designs%2520is%2520not%2520merely%2520about%250Aeliminating%2520bias%252C%2520reducing%2520variance%252C%2520or%2520simply%2520achieve%2520a%2520bias-variance%250Atrade-off.%2520Instead%252C%2520it%2520involves%2520a%2520quantitative%2520joint%2520optimization%2520of%2520bias%2520and%250Avariance.%2520Then%252C%2520we%2520develop%2520a%2520systematic%2520fine-grained%2520dynamic%2520learning%2520framework%250Ato%2520jointly%2520optimize%2520bias%2520and%2520variance%252C%2520which%2520adaptively%2520selects%2520an%2520appropriate%250Aestimator%2520for%2520each%2520user-item%2520pair%2520according%2520to%2520the%2520predefined%2520objective%250Afunction.%2520With%2520this%2520operation%252C%2520the%2520generalization%2520bounds%2520and%2520variances%2520of%250Amodels%2520are%2520reduced%2520and%2520bounded%2520with%2520theoretical%2520guarantees.%2520Extensive%250Aexperiments%2520are%2520conducted%2520to%2520verify%2520the%2520theoretical%2520results%2520and%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520dynamic%2520learning%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Dynamic%20Framework%20for%20Bias-Variance%20Joint%20Optimization%20on%0A%20%20Data%20Missing%20Not%20at%20Random&entry.906535625=Mingming%20Ha%20and%20Xuewen%20Tao%20and%20Wenfang%20Lin%20and%20Qionxu%20Ma%20and%20Wujiang%20Xu%20and%20Linxun%20Chen&entry.1292438233=%20%20In%20most%20practical%20applications%20such%20as%20recommendation%20systems%2C%20display%0Aadvertising%2C%20and%20so%20forth%2C%20the%20collected%20data%20often%20contains%20missing%20values%20and%0Athose%20missing%20values%20are%20generally%20missing-not-at-random%2C%20which%20deteriorates%0Athe%20prediction%20performance%20of%20models.%20Some%20existing%20estimators%20and%20regularizers%0Aattempt%20to%20achieve%20unbiased%20estimation%20to%20improve%20the%20predictive%20performance.%0AHowever%2C%20variances%20and%20generalization%20bound%20of%20these%20methods%20are%20generally%0Aunbounded%20when%20the%20propensity%20scores%20tend%20to%20zero%2C%20compromising%20their%20stability%0Aand%20robustness.%20In%20this%20paper%2C%20we%20first%20theoretically%20reveal%20that%20limitations%0Aof%20regularization%20techniques.%20Besides%2C%20we%20further%20illustrate%20that%2C%20for%20more%0Ageneral%20estimators%2C%20unbiasedness%20will%20inevitably%20lead%20to%20unbounded%20variance.%0AThese%20general%20laws%20inspire%20us%20that%20the%20estimator%20designs%20is%20not%20merely%20about%0Aeliminating%20bias%2C%20reducing%20variance%2C%20or%20simply%20achieve%20a%20bias-variance%0Atrade-off.%20Instead%2C%20it%20involves%20a%20quantitative%20joint%20optimization%20of%20bias%20and%0Avariance.%20Then%2C%20we%20develop%20a%20systematic%20fine-grained%20dynamic%20learning%20framework%0Ato%20jointly%20optimize%20bias%20and%20variance%2C%20which%20adaptively%20selects%20an%20appropriate%0Aestimator%20for%20each%20user-item%20pair%20according%20to%20the%20predefined%20objective%0Afunction.%20With%20this%20operation%2C%20the%20generalization%20bounds%20and%20variances%20of%0Amodels%20are%20reduced%20and%20bounded%20with%20theoretical%20guarantees.%20Extensive%0Aexperiments%20are%20conducted%20to%20verify%20the%20theoretical%20results%20and%20the%0Aeffectiveness%20of%20the%20proposed%20dynamic%20learning%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15403v1&entry.124074799=Read"},
{"title": "The Impact of Geometric Complexity on Neural Collapse in Transfer\n  Learning", "author": "Michael Munn and Benoit Dherin and Javier Gonzalvo", "abstract": "  Many of the recent remarkable advances in computer vision and language models\ncan be attributed to the success of transfer learning via the pre-training of\nlarge foundation models. However, a theoretical framework which explains this\nempirical success is incomplete and remains an active area of research.\nFlatness of the loss surface and neural collapse have recently emerged as\nuseful pre-training metrics which shed light on the implicit biases underlying\npre-training. In this paper, we explore the geometric complexity of a model's\nlearned representations as a fundamental mechanism that relates these two\nconcepts. We show through experiments and theory that mechanisms which affect\nthe geometric complexity of the pre-trained network also influence the neural\ncollapse. Furthermore, we show how this effect of the geometric complexity\ngeneralizes to the neural collapse of new classes as well, thus encouraging\nbetter performance on downstream tasks, particularly in the few-shot setting.\n", "link": "http://arxiv.org/abs/2405.15706v1", "date": "2024-05-24", "relevancy": 2.0553, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5242}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5188}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Geometric%20Complexity%20on%20Neural%20Collapse%20in%20Transfer%0A%20%20Learning&body=Title%3A%20The%20Impact%20of%20Geometric%20Complexity%20on%20Neural%20Collapse%20in%20Transfer%0A%20%20Learning%0AAuthor%3A%20Michael%20Munn%20and%20Benoit%20Dherin%20and%20Javier%20Gonzalvo%0AAbstract%3A%20%20%20Many%20of%20the%20recent%20remarkable%20advances%20in%20computer%20vision%20and%20language%20models%0Acan%20be%20attributed%20to%20the%20success%20of%20transfer%20learning%20via%20the%20pre-training%20of%0Alarge%20foundation%20models.%20However%2C%20a%20theoretical%20framework%20which%20explains%20this%0Aempirical%20success%20is%20incomplete%20and%20remains%20an%20active%20area%20of%20research.%0AFlatness%20of%20the%20loss%20surface%20and%20neural%20collapse%20have%20recently%20emerged%20as%0Auseful%20pre-training%20metrics%20which%20shed%20light%20on%20the%20implicit%20biases%20underlying%0Apre-training.%20In%20this%20paper%2C%20we%20explore%20the%20geometric%20complexity%20of%20a%20model%27s%0Alearned%20representations%20as%20a%20fundamental%20mechanism%20that%20relates%20these%20two%0Aconcepts.%20We%20show%20through%20experiments%20and%20theory%20that%20mechanisms%20which%20affect%0Athe%20geometric%20complexity%20of%20the%20pre-trained%20network%20also%20influence%20the%20neural%0Acollapse.%20Furthermore%2C%20we%20show%20how%20this%20effect%20of%20the%20geometric%20complexity%0Ageneralizes%20to%20the%20neural%20collapse%20of%20new%20classes%20as%20well%2C%20thus%20encouraging%0Abetter%20performance%20on%20downstream%20tasks%2C%20particularly%20in%20the%20few-shot%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Geometric%2520Complexity%2520on%2520Neural%2520Collapse%2520in%2520Transfer%250A%2520%2520Learning%26entry.906535625%3DMichael%2520Munn%2520and%2520Benoit%2520Dherin%2520and%2520Javier%2520Gonzalvo%26entry.1292438233%3D%2520%2520Many%2520of%2520the%2520recent%2520remarkable%2520advances%2520in%2520computer%2520vision%2520and%2520language%2520models%250Acan%2520be%2520attributed%2520to%2520the%2520success%2520of%2520transfer%2520learning%2520via%2520the%2520pre-training%2520of%250Alarge%2520foundation%2520models.%2520However%252C%2520a%2520theoretical%2520framework%2520which%2520explains%2520this%250Aempirical%2520success%2520is%2520incomplete%2520and%2520remains%2520an%2520active%2520area%2520of%2520research.%250AFlatness%2520of%2520the%2520loss%2520surface%2520and%2520neural%2520collapse%2520have%2520recently%2520emerged%2520as%250Auseful%2520pre-training%2520metrics%2520which%2520shed%2520light%2520on%2520the%2520implicit%2520biases%2520underlying%250Apre-training.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520geometric%2520complexity%2520of%2520a%2520model%2527s%250Alearned%2520representations%2520as%2520a%2520fundamental%2520mechanism%2520that%2520relates%2520these%2520two%250Aconcepts.%2520We%2520show%2520through%2520experiments%2520and%2520theory%2520that%2520mechanisms%2520which%2520affect%250Athe%2520geometric%2520complexity%2520of%2520the%2520pre-trained%2520network%2520also%2520influence%2520the%2520neural%250Acollapse.%2520Furthermore%252C%2520we%2520show%2520how%2520this%2520effect%2520of%2520the%2520geometric%2520complexity%250Ageneralizes%2520to%2520the%2520neural%2520collapse%2520of%2520new%2520classes%2520as%2520well%252C%2520thus%2520encouraging%250Abetter%2520performance%2520on%2520downstream%2520tasks%252C%2520particularly%2520in%2520the%2520few-shot%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Geometric%20Complexity%20on%20Neural%20Collapse%20in%20Transfer%0A%20%20Learning&entry.906535625=Michael%20Munn%20and%20Benoit%20Dherin%20and%20Javier%20Gonzalvo&entry.1292438233=%20%20Many%20of%20the%20recent%20remarkable%20advances%20in%20computer%20vision%20and%20language%20models%0Acan%20be%20attributed%20to%20the%20success%20of%20transfer%20learning%20via%20the%20pre-training%20of%0Alarge%20foundation%20models.%20However%2C%20a%20theoretical%20framework%20which%20explains%20this%0Aempirical%20success%20is%20incomplete%20and%20remains%20an%20active%20area%20of%20research.%0AFlatness%20of%20the%20loss%20surface%20and%20neural%20collapse%20have%20recently%20emerged%20as%0Auseful%20pre-training%20metrics%20which%20shed%20light%20on%20the%20implicit%20biases%20underlying%0Apre-training.%20In%20this%20paper%2C%20we%20explore%20the%20geometric%20complexity%20of%20a%20model%27s%0Alearned%20representations%20as%20a%20fundamental%20mechanism%20that%20relates%20these%20two%0Aconcepts.%20We%20show%20through%20experiments%20and%20theory%20that%20mechanisms%20which%20affect%0Athe%20geometric%20complexity%20of%20the%20pre-trained%20network%20also%20influence%20the%20neural%0Acollapse.%20Furthermore%2C%20we%20show%20how%20this%20effect%20of%20the%20geometric%20complexity%0Ageneralizes%20to%20the%20neural%20collapse%20of%20new%20classes%20as%20well%2C%20thus%20encouraging%0Abetter%20performance%20on%20downstream%20tasks%2C%20particularly%20in%20the%20few-shot%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15706v1&entry.124074799=Read"},
{"title": "LAA-Net: Localized Artifact Attention Network for Quality-Agnostic and\n  Generalizable Deepfake Detection", "author": "Dat Nguyen and Nesryne Mejri and Inder Pal Singh and Polina Kuleshova and Marcella Astrid and Anis Kacem and Enjie Ghorbel and Djamila Aouada", "abstract": "  This paper introduces a novel approach for high-quality deepfake detection\ncalled Localized Artifact Attention Network (LAA-Net). Existing methods for\nhigh-quality deepfake detection are mainly based on a supervised binary\nclassifier coupled with an implicit attention mechanism. As a result, they do\nnot generalize well to unseen manipulations. To handle this issue, two main\ncontributions are made. First, an explicit attention mechanism within a\nmulti-task learning framework is proposed. By combining heatmap-based and\nself-consistency attention strategies, LAA-Net is forced to focus on a few\nsmall artifact-prone vulnerable regions. Second, an Enhanced Feature Pyramid\nNetwork (E-FPN) is proposed as a simple and effective mechanism for spreading\ndiscriminative low-level features into the final feature output, with the\nadvantage of limiting redundancy. Experiments performed on several benchmarks\nshow the superiority of our approach in terms of Area Under the Curve (AUC) and\nAverage Precision (AP). The code is available at\nhttps://github.com/10Ring/LAA-Net.\n", "link": "http://arxiv.org/abs/2401.13856v2", "date": "2024-05-24", "relevancy": 2.0506, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5221}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5163}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAA-Net%3A%20Localized%20Artifact%20Attention%20Network%20for%20Quality-Agnostic%20and%0A%20%20Generalizable%20Deepfake%20Detection&body=Title%3A%20LAA-Net%3A%20Localized%20Artifact%20Attention%20Network%20for%20Quality-Agnostic%20and%0A%20%20Generalizable%20Deepfake%20Detection%0AAuthor%3A%20Dat%20Nguyen%20and%20Nesryne%20Mejri%20and%20Inder%20Pal%20Singh%20and%20Polina%20Kuleshova%20and%20Marcella%20Astrid%20and%20Anis%20Kacem%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20for%20high-quality%20deepfake%20detection%0Acalled%20Localized%20Artifact%20Attention%20Network%20%28LAA-Net%29.%20Existing%20methods%20for%0Ahigh-quality%20deepfake%20detection%20are%20mainly%20based%20on%20a%20supervised%20binary%0Aclassifier%20coupled%20with%20an%20implicit%20attention%20mechanism.%20As%20a%20result%2C%20they%20do%0Anot%20generalize%20well%20to%20unseen%20manipulations.%20To%20handle%20this%20issue%2C%20two%20main%0Acontributions%20are%20made.%20First%2C%20an%20explicit%20attention%20mechanism%20within%20a%0Amulti-task%20learning%20framework%20is%20proposed.%20By%20combining%20heatmap-based%20and%0Aself-consistency%20attention%20strategies%2C%20LAA-Net%20is%20forced%20to%20focus%20on%20a%20few%0Asmall%20artifact-prone%20vulnerable%20regions.%20Second%2C%20an%20Enhanced%20Feature%20Pyramid%0ANetwork%20%28E-FPN%29%20is%20proposed%20as%20a%20simple%20and%20effective%20mechanism%20for%20spreading%0Adiscriminative%20low-level%20features%20into%20the%20final%20feature%20output%2C%20with%20the%0Aadvantage%20of%20limiting%20redundancy.%20Experiments%20performed%20on%20several%20benchmarks%0Ashow%20the%20superiority%20of%20our%20approach%20in%20terms%20of%20Area%20Under%20the%20Curve%20%28AUC%29%20and%0AAverage%20Precision%20%28AP%29.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/10Ring/LAA-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAA-Net%253A%2520Localized%2520Artifact%2520Attention%2520Network%2520for%2520Quality-Agnostic%2520and%250A%2520%2520Generalizable%2520Deepfake%2520Detection%26entry.906535625%3DDat%2520Nguyen%2520and%2520Nesryne%2520Mejri%2520and%2520Inder%2520Pal%2520Singh%2520and%2520Polina%2520Kuleshova%2520and%2520Marcella%2520Astrid%2520and%2520Anis%2520Kacem%2520and%2520Enjie%2520Ghorbel%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520for%2520high-quality%2520deepfake%2520detection%250Acalled%2520Localized%2520Artifact%2520Attention%2520Network%2520%2528LAA-Net%2529.%2520Existing%2520methods%2520for%250Ahigh-quality%2520deepfake%2520detection%2520are%2520mainly%2520based%2520on%2520a%2520supervised%2520binary%250Aclassifier%2520coupled%2520with%2520an%2520implicit%2520attention%2520mechanism.%2520As%2520a%2520result%252C%2520they%2520do%250Anot%2520generalize%2520well%2520to%2520unseen%2520manipulations.%2520To%2520handle%2520this%2520issue%252C%2520two%2520main%250Acontributions%2520are%2520made.%2520First%252C%2520an%2520explicit%2520attention%2520mechanism%2520within%2520a%250Amulti-task%2520learning%2520framework%2520is%2520proposed.%2520By%2520combining%2520heatmap-based%2520and%250Aself-consistency%2520attention%2520strategies%252C%2520LAA-Net%2520is%2520forced%2520to%2520focus%2520on%2520a%2520few%250Asmall%2520artifact-prone%2520vulnerable%2520regions.%2520Second%252C%2520an%2520Enhanced%2520Feature%2520Pyramid%250ANetwork%2520%2528E-FPN%2529%2520is%2520proposed%2520as%2520a%2520simple%2520and%2520effective%2520mechanism%2520for%2520spreading%250Adiscriminative%2520low-level%2520features%2520into%2520the%2520final%2520feature%2520output%252C%2520with%2520the%250Aadvantage%2520of%2520limiting%2520redundancy.%2520Experiments%2520performed%2520on%2520several%2520benchmarks%250Ashow%2520the%2520superiority%2520of%2520our%2520approach%2520in%2520terms%2520of%2520Area%2520Under%2520the%2520Curve%2520%2528AUC%2529%2520and%250AAverage%2520Precision%2520%2528AP%2529.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/10Ring/LAA-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAA-Net%3A%20Localized%20Artifact%20Attention%20Network%20for%20Quality-Agnostic%20and%0A%20%20Generalizable%20Deepfake%20Detection&entry.906535625=Dat%20Nguyen%20and%20Nesryne%20Mejri%20and%20Inder%20Pal%20Singh%20and%20Polina%20Kuleshova%20and%20Marcella%20Astrid%20and%20Anis%20Kacem%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20for%20high-quality%20deepfake%20detection%0Acalled%20Localized%20Artifact%20Attention%20Network%20%28LAA-Net%29.%20Existing%20methods%20for%0Ahigh-quality%20deepfake%20detection%20are%20mainly%20based%20on%20a%20supervised%20binary%0Aclassifier%20coupled%20with%20an%20implicit%20attention%20mechanism.%20As%20a%20result%2C%20they%20do%0Anot%20generalize%20well%20to%20unseen%20manipulations.%20To%20handle%20this%20issue%2C%20two%20main%0Acontributions%20are%20made.%20First%2C%20an%20explicit%20attention%20mechanism%20within%20a%0Amulti-task%20learning%20framework%20is%20proposed.%20By%20combining%20heatmap-based%20and%0Aself-consistency%20attention%20strategies%2C%20LAA-Net%20is%20forced%20to%20focus%20on%20a%20few%0Asmall%20artifact-prone%20vulnerable%20regions.%20Second%2C%20an%20Enhanced%20Feature%20Pyramid%0ANetwork%20%28E-FPN%29%20is%20proposed%20as%20a%20simple%20and%20effective%20mechanism%20for%20spreading%0Adiscriminative%20low-level%20features%20into%20the%20final%20feature%20output%2C%20with%20the%0Aadvantage%20of%20limiting%20redundancy.%20Experiments%20performed%20on%20several%20benchmarks%0Ashow%20the%20superiority%20of%20our%20approach%20in%20terms%20of%20Area%20Under%20the%20Curve%20%28AUC%29%20and%0AAverage%20Precision%20%28AP%29.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/10Ring/LAA-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13856v2&entry.124074799=Read"},
{"title": "Neural Persistence Dynamics", "author": "Sebastian Zeng and Florian Graf and Martin Uray and Stefan Huber and Roland Kwitt", "abstract": "  We consider the problem of learning the dynamics in the topology of\ntime-evolving point clouds, the prevalent spatiotemporal model for systems\nexhibiting collective behavior, such as swarms of insects and birds or\nparticles in physics. In such systems, patterns emerge from (local)\ninteractions among self-propelled entities. While several well-understood\ngoverning equations for motion and interaction exist, they are difficult to fit\nto data due to the often large number of entities and missing correspondences\nbetween the observation times, which may also not be equidistant. To evade such\nconfounding factors, we investigate collective behavior from a\n\\textit{topological perspective}, but instead of summarizing entire observation\nsequences (as in prior work), we propose learning a latent dynamical model from\ntopological features \\textit{per time point}. The latter is then used to\nformulate a downstream regression task to predict the parametrization of some a\npriori specified governing equation. We implement this idea based on a latent\nODE learned from vectorized (static) persistence diagrams and show that this\nmodeling choice is justified by a combination of recent stability results for\npersistent homology. Various (ablation) experiments not only demonstrate the\nrelevance of each individual model component, but provide compelling empirical\nevidence that our proposed model -- \\textit{neural persistence dynamics} --\nsubstantially outperforms the state-of-the-art across a diverse set of\nparameter regression tasks.\n", "link": "http://arxiv.org/abs/2405.15732v1", "date": "2024-05-24", "relevancy": 2.0459, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5289}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5044}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Persistence%20Dynamics&body=Title%3A%20Neural%20Persistence%20Dynamics%0AAuthor%3A%20Sebastian%20Zeng%20and%20Florian%20Graf%20and%20Martin%20Uray%20and%20Stefan%20Huber%20and%20Roland%20Kwitt%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20learning%20the%20dynamics%20in%20the%20topology%20of%0Atime-evolving%20point%20clouds%2C%20the%20prevalent%20spatiotemporal%20model%20for%20systems%0Aexhibiting%20collective%20behavior%2C%20such%20as%20swarms%20of%20insects%20and%20birds%20or%0Aparticles%20in%20physics.%20In%20such%20systems%2C%20patterns%20emerge%20from%20%28local%29%0Ainteractions%20among%20self-propelled%20entities.%20While%20several%20well-understood%0Agoverning%20equations%20for%20motion%20and%20interaction%20exist%2C%20they%20are%20difficult%20to%20fit%0Ato%20data%20due%20to%20the%20often%20large%20number%20of%20entities%20and%20missing%20correspondences%0Abetween%20the%20observation%20times%2C%20which%20may%20also%20not%20be%20equidistant.%20To%20evade%20such%0Aconfounding%20factors%2C%20we%20investigate%20collective%20behavior%20from%20a%0A%5Ctextit%7Btopological%20perspective%7D%2C%20but%20instead%20of%20summarizing%20entire%20observation%0Asequences%20%28as%20in%20prior%20work%29%2C%20we%20propose%20learning%20a%20latent%20dynamical%20model%20from%0Atopological%20features%20%5Ctextit%7Bper%20time%20point%7D.%20The%20latter%20is%20then%20used%20to%0Aformulate%20a%20downstream%20regression%20task%20to%20predict%20the%20parametrization%20of%20some%20a%0Apriori%20specified%20governing%20equation.%20We%20implement%20this%20idea%20based%20on%20a%20latent%0AODE%20learned%20from%20vectorized%20%28static%29%20persistence%20diagrams%20and%20show%20that%20this%0Amodeling%20choice%20is%20justified%20by%20a%20combination%20of%20recent%20stability%20results%20for%0Apersistent%20homology.%20Various%20%28ablation%29%20experiments%20not%20only%20demonstrate%20the%0Arelevance%20of%20each%20individual%20model%20component%2C%20but%20provide%20compelling%20empirical%0Aevidence%20that%20our%20proposed%20model%20--%20%5Ctextit%7Bneural%20persistence%20dynamics%7D%20--%0Asubstantially%20outperforms%20the%20state-of-the-art%20across%20a%20diverse%20set%20of%0Aparameter%20regression%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Persistence%2520Dynamics%26entry.906535625%3DSebastian%2520Zeng%2520and%2520Florian%2520Graf%2520and%2520Martin%2520Uray%2520and%2520Stefan%2520Huber%2520and%2520Roland%2520Kwitt%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520learning%2520the%2520dynamics%2520in%2520the%2520topology%2520of%250Atime-evolving%2520point%2520clouds%252C%2520the%2520prevalent%2520spatiotemporal%2520model%2520for%2520systems%250Aexhibiting%2520collective%2520behavior%252C%2520such%2520as%2520swarms%2520of%2520insects%2520and%2520birds%2520or%250Aparticles%2520in%2520physics.%2520In%2520such%2520systems%252C%2520patterns%2520emerge%2520from%2520%2528local%2529%250Ainteractions%2520among%2520self-propelled%2520entities.%2520While%2520several%2520well-understood%250Agoverning%2520equations%2520for%2520motion%2520and%2520interaction%2520exist%252C%2520they%2520are%2520difficult%2520to%2520fit%250Ato%2520data%2520due%2520to%2520the%2520often%2520large%2520number%2520of%2520entities%2520and%2520missing%2520correspondences%250Abetween%2520the%2520observation%2520times%252C%2520which%2520may%2520also%2520not%2520be%2520equidistant.%2520To%2520evade%2520such%250Aconfounding%2520factors%252C%2520we%2520investigate%2520collective%2520behavior%2520from%2520a%250A%255Ctextit%257Btopological%2520perspective%257D%252C%2520but%2520instead%2520of%2520summarizing%2520entire%2520observation%250Asequences%2520%2528as%2520in%2520prior%2520work%2529%252C%2520we%2520propose%2520learning%2520a%2520latent%2520dynamical%2520model%2520from%250Atopological%2520features%2520%255Ctextit%257Bper%2520time%2520point%257D.%2520The%2520latter%2520is%2520then%2520used%2520to%250Aformulate%2520a%2520downstream%2520regression%2520task%2520to%2520predict%2520the%2520parametrization%2520of%2520some%2520a%250Apriori%2520specified%2520governing%2520equation.%2520We%2520implement%2520this%2520idea%2520based%2520on%2520a%2520latent%250AODE%2520learned%2520from%2520vectorized%2520%2528static%2529%2520persistence%2520diagrams%2520and%2520show%2520that%2520this%250Amodeling%2520choice%2520is%2520justified%2520by%2520a%2520combination%2520of%2520recent%2520stability%2520results%2520for%250Apersistent%2520homology.%2520Various%2520%2528ablation%2529%2520experiments%2520not%2520only%2520demonstrate%2520the%250Arelevance%2520of%2520each%2520individual%2520model%2520component%252C%2520but%2520provide%2520compelling%2520empirical%250Aevidence%2520that%2520our%2520proposed%2520model%2520--%2520%255Ctextit%257Bneural%2520persistence%2520dynamics%257D%2520--%250Asubstantially%2520outperforms%2520the%2520state-of-the-art%2520across%2520a%2520diverse%2520set%2520of%250Aparameter%2520regression%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Persistence%20Dynamics&entry.906535625=Sebastian%20Zeng%20and%20Florian%20Graf%20and%20Martin%20Uray%20and%20Stefan%20Huber%20and%20Roland%20Kwitt&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20learning%20the%20dynamics%20in%20the%20topology%20of%0Atime-evolving%20point%20clouds%2C%20the%20prevalent%20spatiotemporal%20model%20for%20systems%0Aexhibiting%20collective%20behavior%2C%20such%20as%20swarms%20of%20insects%20and%20birds%20or%0Aparticles%20in%20physics.%20In%20such%20systems%2C%20patterns%20emerge%20from%20%28local%29%0Ainteractions%20among%20self-propelled%20entities.%20While%20several%20well-understood%0Agoverning%20equations%20for%20motion%20and%20interaction%20exist%2C%20they%20are%20difficult%20to%20fit%0Ato%20data%20due%20to%20the%20often%20large%20number%20of%20entities%20and%20missing%20correspondences%0Abetween%20the%20observation%20times%2C%20which%20may%20also%20not%20be%20equidistant.%20To%20evade%20such%0Aconfounding%20factors%2C%20we%20investigate%20collective%20behavior%20from%20a%0A%5Ctextit%7Btopological%20perspective%7D%2C%20but%20instead%20of%20summarizing%20entire%20observation%0Asequences%20%28as%20in%20prior%20work%29%2C%20we%20propose%20learning%20a%20latent%20dynamical%20model%20from%0Atopological%20features%20%5Ctextit%7Bper%20time%20point%7D.%20The%20latter%20is%20then%20used%20to%0Aformulate%20a%20downstream%20regression%20task%20to%20predict%20the%20parametrization%20of%20some%20a%0Apriori%20specified%20governing%20equation.%20We%20implement%20this%20idea%20based%20on%20a%20latent%0AODE%20learned%20from%20vectorized%20%28static%29%20persistence%20diagrams%20and%20show%20that%20this%0Amodeling%20choice%20is%20justified%20by%20a%20combination%20of%20recent%20stability%20results%20for%0Apersistent%20homology.%20Various%20%28ablation%29%20experiments%20not%20only%20demonstrate%20the%0Arelevance%20of%20each%20individual%20model%20component%2C%20but%20provide%20compelling%20empirical%0Aevidence%20that%20our%20proposed%20model%20--%20%5Ctextit%7Bneural%20persistence%20dynamics%7D%20--%0Asubstantially%20outperforms%20the%20state-of-the-art%20across%20a%20diverse%20set%20of%0Aparameter%20regression%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15732v1&entry.124074799=Read"},
{"title": "Generating density nowcasts for U.S. GDP growth with deep learning:\n  Bayes by Backprop and Monte Carlo dropout", "author": "Krist\u00f3f N\u00e9meth and D\u00e1niel Hadh\u00e1zi", "abstract": "  Recent results in the literature indicate that artificial neural networks\n(ANNs) can outperform the dynamic factor model (DFM) in terms of the accuracy\nof GDP nowcasts. Compared to the DFM, the performance advantage of these highly\nflexible, nonlinear estimators is particularly evident in periods of recessions\nand structural breaks. From the perspective of policy-makers, however, nowcasts\nare the most useful when they are conveyed with uncertainty attached to them.\nWhile the DFM and other classical time series approaches analytically derive\nthe predictive (conditional) distribution for GDP growth, ANNs can only produce\npoint nowcasts based on their default training procedure (backpropagation). To\nfill this gap, first in the literature, we adapt two different deep learning\nalgorithms that enable ANNs to generate density nowcasts for U.S. GDP growth:\nBayes by Backprop and Monte Carlo dropout. The accuracy of point nowcasts,\ndefined as the mean of the empirical predictive distribution, is evaluated\nrelative to a naive constant growth model for GDP and a benchmark DFM\nspecification. Using a 1D CNN as the underlying ANN architecture, both\nalgorithms outperform those benchmarks during the evaluation period (2012:Q1 --\n2022:Q4). Furthermore, both algorithms are able to dynamically adjust the\nlocation (mean), scale (variance), and shape (skew) of the empirical predictive\ndistribution. The results indicate that both Bayes by Backprop and Monte Carlo\ndropout can effectively augment the scope and functionality of ANNs, rendering\nthem a fully compatible and competitive alternative for classical time series\napproaches.\n", "link": "http://arxiv.org/abs/2405.15579v1", "date": "2024-05-24", "relevancy": 2.0454, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4865}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20density%20nowcasts%20for%20U.S.%20GDP%20growth%20with%20deep%20learning%3A%0A%20%20Bayes%20by%20Backprop%20and%20Monte%20Carlo%20dropout&body=Title%3A%20Generating%20density%20nowcasts%20for%20U.S.%20GDP%20growth%20with%20deep%20learning%3A%0A%20%20Bayes%20by%20Backprop%20and%20Monte%20Carlo%20dropout%0AAuthor%3A%20Krist%C3%B3f%20N%C3%A9meth%20and%20D%C3%A1niel%20Hadh%C3%A1zi%0AAbstract%3A%20%20%20Recent%20results%20in%20the%20literature%20indicate%20that%20artificial%20neural%20networks%0A%28ANNs%29%20can%20outperform%20the%20dynamic%20factor%20model%20%28DFM%29%20in%20terms%20of%20the%20accuracy%0Aof%20GDP%20nowcasts.%20Compared%20to%20the%20DFM%2C%20the%20performance%20advantage%20of%20these%20highly%0Aflexible%2C%20nonlinear%20estimators%20is%20particularly%20evident%20in%20periods%20of%20recessions%0Aand%20structural%20breaks.%20From%20the%20perspective%20of%20policy-makers%2C%20however%2C%20nowcasts%0Aare%20the%20most%20useful%20when%20they%20are%20conveyed%20with%20uncertainty%20attached%20to%20them.%0AWhile%20the%20DFM%20and%20other%20classical%20time%20series%20approaches%20analytically%20derive%0Athe%20predictive%20%28conditional%29%20distribution%20for%20GDP%20growth%2C%20ANNs%20can%20only%20produce%0Apoint%20nowcasts%20based%20on%20their%20default%20training%20procedure%20%28backpropagation%29.%20To%0Afill%20this%20gap%2C%20first%20in%20the%20literature%2C%20we%20adapt%20two%20different%20deep%20learning%0Aalgorithms%20that%20enable%20ANNs%20to%20generate%20density%20nowcasts%20for%20U.S.%20GDP%20growth%3A%0ABayes%20by%20Backprop%20and%20Monte%20Carlo%20dropout.%20The%20accuracy%20of%20point%20nowcasts%2C%0Adefined%20as%20the%20mean%20of%20the%20empirical%20predictive%20distribution%2C%20is%20evaluated%0Arelative%20to%20a%20naive%20constant%20growth%20model%20for%20GDP%20and%20a%20benchmark%20DFM%0Aspecification.%20Using%20a%201D%20CNN%20as%20the%20underlying%20ANN%20architecture%2C%20both%0Aalgorithms%20outperform%20those%20benchmarks%20during%20the%20evaluation%20period%20%282012%3AQ1%20--%0A2022%3AQ4%29.%20Furthermore%2C%20both%20algorithms%20are%20able%20to%20dynamically%20adjust%20the%0Alocation%20%28mean%29%2C%20scale%20%28variance%29%2C%20and%20shape%20%28skew%29%20of%20the%20empirical%20predictive%0Adistribution.%20The%20results%20indicate%20that%20both%20Bayes%20by%20Backprop%20and%20Monte%20Carlo%0Adropout%20can%20effectively%20augment%20the%20scope%20and%20functionality%20of%20ANNs%2C%20rendering%0Athem%20a%20fully%20compatible%20and%20competitive%20alternative%20for%20classical%20time%20series%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520density%2520nowcasts%2520for%2520U.S.%2520GDP%2520growth%2520with%2520deep%2520learning%253A%250A%2520%2520Bayes%2520by%2520Backprop%2520and%2520Monte%2520Carlo%2520dropout%26entry.906535625%3DKrist%25C3%25B3f%2520N%25C3%25A9meth%2520and%2520D%25C3%25A1niel%2520Hadh%25C3%25A1zi%26entry.1292438233%3D%2520%2520Recent%2520results%2520in%2520the%2520literature%2520indicate%2520that%2520artificial%2520neural%2520networks%250A%2528ANNs%2529%2520can%2520outperform%2520the%2520dynamic%2520factor%2520model%2520%2528DFM%2529%2520in%2520terms%2520of%2520the%2520accuracy%250Aof%2520GDP%2520nowcasts.%2520Compared%2520to%2520the%2520DFM%252C%2520the%2520performance%2520advantage%2520of%2520these%2520highly%250Aflexible%252C%2520nonlinear%2520estimators%2520is%2520particularly%2520evident%2520in%2520periods%2520of%2520recessions%250Aand%2520structural%2520breaks.%2520From%2520the%2520perspective%2520of%2520policy-makers%252C%2520however%252C%2520nowcasts%250Aare%2520the%2520most%2520useful%2520when%2520they%2520are%2520conveyed%2520with%2520uncertainty%2520attached%2520to%2520them.%250AWhile%2520the%2520DFM%2520and%2520other%2520classical%2520time%2520series%2520approaches%2520analytically%2520derive%250Athe%2520predictive%2520%2528conditional%2529%2520distribution%2520for%2520GDP%2520growth%252C%2520ANNs%2520can%2520only%2520produce%250Apoint%2520nowcasts%2520based%2520on%2520their%2520default%2520training%2520procedure%2520%2528backpropagation%2529.%2520To%250Afill%2520this%2520gap%252C%2520first%2520in%2520the%2520literature%252C%2520we%2520adapt%2520two%2520different%2520deep%2520learning%250Aalgorithms%2520that%2520enable%2520ANNs%2520to%2520generate%2520density%2520nowcasts%2520for%2520U.S.%2520GDP%2520growth%253A%250ABayes%2520by%2520Backprop%2520and%2520Monte%2520Carlo%2520dropout.%2520The%2520accuracy%2520of%2520point%2520nowcasts%252C%250Adefined%2520as%2520the%2520mean%2520of%2520the%2520empirical%2520predictive%2520distribution%252C%2520is%2520evaluated%250Arelative%2520to%2520a%2520naive%2520constant%2520growth%2520model%2520for%2520GDP%2520and%2520a%2520benchmark%2520DFM%250Aspecification.%2520Using%2520a%25201D%2520CNN%2520as%2520the%2520underlying%2520ANN%2520architecture%252C%2520both%250Aalgorithms%2520outperform%2520those%2520benchmarks%2520during%2520the%2520evaluation%2520period%2520%25282012%253AQ1%2520--%250A2022%253AQ4%2529.%2520Furthermore%252C%2520both%2520algorithms%2520are%2520able%2520to%2520dynamically%2520adjust%2520the%250Alocation%2520%2528mean%2529%252C%2520scale%2520%2528variance%2529%252C%2520and%2520shape%2520%2528skew%2529%2520of%2520the%2520empirical%2520predictive%250Adistribution.%2520The%2520results%2520indicate%2520that%2520both%2520Bayes%2520by%2520Backprop%2520and%2520Monte%2520Carlo%250Adropout%2520can%2520effectively%2520augment%2520the%2520scope%2520and%2520functionality%2520of%2520ANNs%252C%2520rendering%250Athem%2520a%2520fully%2520compatible%2520and%2520competitive%2520alternative%2520for%2520classical%2520time%2520series%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20density%20nowcasts%20for%20U.S.%20GDP%20growth%20with%20deep%20learning%3A%0A%20%20Bayes%20by%20Backprop%20and%20Monte%20Carlo%20dropout&entry.906535625=Krist%C3%B3f%20N%C3%A9meth%20and%20D%C3%A1niel%20Hadh%C3%A1zi&entry.1292438233=%20%20Recent%20results%20in%20the%20literature%20indicate%20that%20artificial%20neural%20networks%0A%28ANNs%29%20can%20outperform%20the%20dynamic%20factor%20model%20%28DFM%29%20in%20terms%20of%20the%20accuracy%0Aof%20GDP%20nowcasts.%20Compared%20to%20the%20DFM%2C%20the%20performance%20advantage%20of%20these%20highly%0Aflexible%2C%20nonlinear%20estimators%20is%20particularly%20evident%20in%20periods%20of%20recessions%0Aand%20structural%20breaks.%20From%20the%20perspective%20of%20policy-makers%2C%20however%2C%20nowcasts%0Aare%20the%20most%20useful%20when%20they%20are%20conveyed%20with%20uncertainty%20attached%20to%20them.%0AWhile%20the%20DFM%20and%20other%20classical%20time%20series%20approaches%20analytically%20derive%0Athe%20predictive%20%28conditional%29%20distribution%20for%20GDP%20growth%2C%20ANNs%20can%20only%20produce%0Apoint%20nowcasts%20based%20on%20their%20default%20training%20procedure%20%28backpropagation%29.%20To%0Afill%20this%20gap%2C%20first%20in%20the%20literature%2C%20we%20adapt%20two%20different%20deep%20learning%0Aalgorithms%20that%20enable%20ANNs%20to%20generate%20density%20nowcasts%20for%20U.S.%20GDP%20growth%3A%0ABayes%20by%20Backprop%20and%20Monte%20Carlo%20dropout.%20The%20accuracy%20of%20point%20nowcasts%2C%0Adefined%20as%20the%20mean%20of%20the%20empirical%20predictive%20distribution%2C%20is%20evaluated%0Arelative%20to%20a%20naive%20constant%20growth%20model%20for%20GDP%20and%20a%20benchmark%20DFM%0Aspecification.%20Using%20a%201D%20CNN%20as%20the%20underlying%20ANN%20architecture%2C%20both%0Aalgorithms%20outperform%20those%20benchmarks%20during%20the%20evaluation%20period%20%282012%3AQ1%20--%0A2022%3AQ4%29.%20Furthermore%2C%20both%20algorithms%20are%20able%20to%20dynamically%20adjust%20the%0Alocation%20%28mean%29%2C%20scale%20%28variance%29%2C%20and%20shape%20%28skew%29%20of%20the%20empirical%20predictive%0Adistribution.%20The%20results%20indicate%20that%20both%20Bayes%20by%20Backprop%20and%20Monte%20Carlo%0Adropout%20can%20effectively%20augment%20the%20scope%20and%20functionality%20of%20ANNs%2C%20rendering%0Athem%20a%20fully%20compatible%20and%20competitive%20alternative%20for%20classical%20time%20series%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15579v1&entry.124074799=Read"},
{"title": "Towards Principled Graph Transformers", "author": "Luis M\u00fcller and Daniel Kusuma and Blai Bonet and Christopher Morris", "abstract": "  Graph learning architectures based on the k-dimensional Weisfeiler-Leman\n(k-WL) hierarchy offer a theoretically well-understood expressive power.\nHowever, such architectures often fail to deliver solid predictive performance\non real-world tasks, limiting their practical impact. In contrast, global\nattention-based models such as graph transformers demonstrate strong\nperformance in practice, but comparing their expressive power with the k-WL\nhierarchy remains challenging, particularly since these architectures rely on\npositional or structural encodings for their expressivity and predictive\nperformance. To address this, we show that the recently proposed Edge\nTransformer, a global attention model operating on node pairs instead of nodes,\nhas at least 3-WL expressive power. Empirically, we demonstrate that the Edge\nTransformer surpasses other theoretically aligned architectures regarding\npredictive performance while not relying on positional or structural encodings.\nOur code is available at https://github.com/luis-mueller/towards-principled-gts\n", "link": "http://arxiv.org/abs/2401.10119v3", "date": "2024-05-24", "relevancy": 2.0443, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5449}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5048}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Principled%20Graph%20Transformers&body=Title%3A%20Towards%20Principled%20Graph%20Transformers%0AAuthor%3A%20Luis%20M%C3%BCller%20and%20Daniel%20Kusuma%20and%20Blai%20Bonet%20and%20Christopher%20Morris%0AAbstract%3A%20%20%20Graph%20learning%20architectures%20based%20on%20the%20k-dimensional%20Weisfeiler-Leman%0A%28k-WL%29%20hierarchy%20offer%20a%20theoretically%20well-understood%20expressive%20power.%0AHowever%2C%20such%20architectures%20often%20fail%20to%20deliver%20solid%20predictive%20performance%0Aon%20real-world%20tasks%2C%20limiting%20their%20practical%20impact.%20In%20contrast%2C%20global%0Aattention-based%20models%20such%20as%20graph%20transformers%20demonstrate%20strong%0Aperformance%20in%20practice%2C%20but%20comparing%20their%20expressive%20power%20with%20the%20k-WL%0Ahierarchy%20remains%20challenging%2C%20particularly%20since%20these%20architectures%20rely%20on%0Apositional%20or%20structural%20encodings%20for%20their%20expressivity%20and%20predictive%0Aperformance.%20To%20address%20this%2C%20we%20show%20that%20the%20recently%20proposed%20Edge%0ATransformer%2C%20a%20global%20attention%20model%20operating%20on%20node%20pairs%20instead%20of%20nodes%2C%0Ahas%20at%20least%203-WL%20expressive%20power.%20Empirically%2C%20we%20demonstrate%20that%20the%20Edge%0ATransformer%20surpasses%20other%20theoretically%20aligned%20architectures%20regarding%0Apredictive%20performance%20while%20not%20relying%20on%20positional%20or%20structural%20encodings.%0AOur%20code%20is%20available%20at%20https%3A//github.com/luis-mueller/towards-principled-gts%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10119v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Principled%2520Graph%2520Transformers%26entry.906535625%3DLuis%2520M%25C3%25BCller%2520and%2520Daniel%2520Kusuma%2520and%2520Blai%2520Bonet%2520and%2520Christopher%2520Morris%26entry.1292438233%3D%2520%2520Graph%2520learning%2520architectures%2520based%2520on%2520the%2520k-dimensional%2520Weisfeiler-Leman%250A%2528k-WL%2529%2520hierarchy%2520offer%2520a%2520theoretically%2520well-understood%2520expressive%2520power.%250AHowever%252C%2520such%2520architectures%2520often%2520fail%2520to%2520deliver%2520solid%2520predictive%2520performance%250Aon%2520real-world%2520tasks%252C%2520limiting%2520their%2520practical%2520impact.%2520In%2520contrast%252C%2520global%250Aattention-based%2520models%2520such%2520as%2520graph%2520transformers%2520demonstrate%2520strong%250Aperformance%2520in%2520practice%252C%2520but%2520comparing%2520their%2520expressive%2520power%2520with%2520the%2520k-WL%250Ahierarchy%2520remains%2520challenging%252C%2520particularly%2520since%2520these%2520architectures%2520rely%2520on%250Apositional%2520or%2520structural%2520encodings%2520for%2520their%2520expressivity%2520and%2520predictive%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520show%2520that%2520the%2520recently%2520proposed%2520Edge%250ATransformer%252C%2520a%2520global%2520attention%2520model%2520operating%2520on%2520node%2520pairs%2520instead%2520of%2520nodes%252C%250Ahas%2520at%2520least%25203-WL%2520expressive%2520power.%2520Empirically%252C%2520we%2520demonstrate%2520that%2520the%2520Edge%250ATransformer%2520surpasses%2520other%2520theoretically%2520aligned%2520architectures%2520regarding%250Apredictive%2520performance%2520while%2520not%2520relying%2520on%2520positional%2520or%2520structural%2520encodings.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/luis-mueller/towards-principled-gts%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10119v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Principled%20Graph%20Transformers&entry.906535625=Luis%20M%C3%BCller%20and%20Daniel%20Kusuma%20and%20Blai%20Bonet%20and%20Christopher%20Morris&entry.1292438233=%20%20Graph%20learning%20architectures%20based%20on%20the%20k-dimensional%20Weisfeiler-Leman%0A%28k-WL%29%20hierarchy%20offer%20a%20theoretically%20well-understood%20expressive%20power.%0AHowever%2C%20such%20architectures%20often%20fail%20to%20deliver%20solid%20predictive%20performance%0Aon%20real-world%20tasks%2C%20limiting%20their%20practical%20impact.%20In%20contrast%2C%20global%0Aattention-based%20models%20such%20as%20graph%20transformers%20demonstrate%20strong%0Aperformance%20in%20practice%2C%20but%20comparing%20their%20expressive%20power%20with%20the%20k-WL%0Ahierarchy%20remains%20challenging%2C%20particularly%20since%20these%20architectures%20rely%20on%0Apositional%20or%20structural%20encodings%20for%20their%20expressivity%20and%20predictive%0Aperformance.%20To%20address%20this%2C%20we%20show%20that%20the%20recently%20proposed%20Edge%0ATransformer%2C%20a%20global%20attention%20model%20operating%20on%20node%20pairs%20instead%20of%20nodes%2C%0Ahas%20at%20least%203-WL%20expressive%20power.%20Empirically%2C%20we%20demonstrate%20that%20the%20Edge%0ATransformer%20surpasses%20other%20theoretically%20aligned%20architectures%20regarding%0Apredictive%20performance%20while%20not%20relying%20on%20positional%20or%20structural%20encodings.%0AOur%20code%20is%20available%20at%20https%3A//github.com/luis-mueller/towards-principled-gts%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10119v3&entry.124074799=Read"},
{"title": "Understanding the Expressive Power and Mechanisms of Transformer for\n  Sequence Modeling", "author": "Mingze Wang and Weinan E", "abstract": "  We conduct a systematic study of the approximation properties of Transformer\nfor sequence modeling with long, sparse and complicated memory. We investigate\nthe mechanisms through which different components of Transformer, such as the\ndot-product self-attention, positional encoding and feed-forward layer, affect\nits expressive power, and we study their combined effects through establishing\nexplicit approximation rates. Our study reveals the roles of critical\nparameters in the Transformer, such as the number of layers and the number of\nattention heads. These theoretical insights are validated experimentally and\noffer natural suggestions for alternative architectures.\n", "link": "http://arxiv.org/abs/2402.00522v4", "date": "2024-05-24", "relevancy": 2.0393, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.598}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4964}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Expressive%20Power%20and%20Mechanisms%20of%20Transformer%20for%0A%20%20Sequence%20Modeling&body=Title%3A%20Understanding%20the%20Expressive%20Power%20and%20Mechanisms%20of%20Transformer%20for%0A%20%20Sequence%20Modeling%0AAuthor%3A%20Mingze%20Wang%20and%20Weinan%20E%0AAbstract%3A%20%20%20We%20conduct%20a%20systematic%20study%20of%20the%20approximation%20properties%20of%20Transformer%0Afor%20sequence%20modeling%20with%20long%2C%20sparse%20and%20complicated%20memory.%20We%20investigate%0Athe%20mechanisms%20through%20which%20different%20components%20of%20Transformer%2C%20such%20as%20the%0Adot-product%20self-attention%2C%20positional%20encoding%20and%20feed-forward%20layer%2C%20affect%0Aits%20expressive%20power%2C%20and%20we%20study%20their%20combined%20effects%20through%20establishing%0Aexplicit%20approximation%20rates.%20Our%20study%20reveals%20the%20roles%20of%20critical%0Aparameters%20in%20the%20Transformer%2C%20such%20as%20the%20number%20of%20layers%20and%20the%20number%20of%0Aattention%20heads.%20These%20theoretical%20insights%20are%20validated%20experimentally%20and%0Aoffer%20natural%20suggestions%20for%20alternative%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00522v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520the%2520Expressive%2520Power%2520and%2520Mechanisms%2520of%2520Transformer%2520for%250A%2520%2520Sequence%2520Modeling%26entry.906535625%3DMingze%2520Wang%2520and%2520Weinan%2520E%26entry.1292438233%3D%2520%2520We%2520conduct%2520a%2520systematic%2520study%2520of%2520the%2520approximation%2520properties%2520of%2520Transformer%250Afor%2520sequence%2520modeling%2520with%2520long%252C%2520sparse%2520and%2520complicated%2520memory.%2520We%2520investigate%250Athe%2520mechanisms%2520through%2520which%2520different%2520components%2520of%2520Transformer%252C%2520such%2520as%2520the%250Adot-product%2520self-attention%252C%2520positional%2520encoding%2520and%2520feed-forward%2520layer%252C%2520affect%250Aits%2520expressive%2520power%252C%2520and%2520we%2520study%2520their%2520combined%2520effects%2520through%2520establishing%250Aexplicit%2520approximation%2520rates.%2520Our%2520study%2520reveals%2520the%2520roles%2520of%2520critical%250Aparameters%2520in%2520the%2520Transformer%252C%2520such%2520as%2520the%2520number%2520of%2520layers%2520and%2520the%2520number%2520of%250Aattention%2520heads.%2520These%2520theoretical%2520insights%2520are%2520validated%2520experimentally%2520and%250Aoffer%2520natural%2520suggestions%2520for%2520alternative%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00522v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Expressive%20Power%20and%20Mechanisms%20of%20Transformer%20for%0A%20%20Sequence%20Modeling&entry.906535625=Mingze%20Wang%20and%20Weinan%20E&entry.1292438233=%20%20We%20conduct%20a%20systematic%20study%20of%20the%20approximation%20properties%20of%20Transformer%0Afor%20sequence%20modeling%20with%20long%2C%20sparse%20and%20complicated%20memory.%20We%20investigate%0Athe%20mechanisms%20through%20which%20different%20components%20of%20Transformer%2C%20such%20as%20the%0Adot-product%20self-attention%2C%20positional%20encoding%20and%20feed-forward%20layer%2C%20affect%0Aits%20expressive%20power%2C%20and%20we%20study%20their%20combined%20effects%20through%20establishing%0Aexplicit%20approximation%20rates.%20Our%20study%20reveals%20the%20roles%20of%20critical%0Aparameters%20in%20the%20Transformer%2C%20such%20as%20the%20number%20of%20layers%20and%20the%20number%20of%0Aattention%20heads.%20These%20theoretical%20insights%20are%20validated%20experimentally%20and%0Aoffer%20natural%20suggestions%20for%20alternative%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00522v4&entry.124074799=Read"},
{"title": "MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and\n  Provable Convergence", "author": "Ionut-Vlad Modoranu and Mher Safaryan and Grigory Malinovsky and Eldar Kurtic and Thomas Robert and Peter Richtarik and Dan Alistarh", "abstract": "  We propose a new variant of the Adam optimizer [Kingma and Ba, 2014] called\nMICROADAM that specifically minimizes memory overheads, while maintaining\ntheoretical convergence guarantees. We achieve this by compressing the gradient\ninformation before it is fed into the optimizer state, thereby reducing its\nmemory footprint significantly. We control the resulting compression error via\na novel instance of the classical error feedback mechanism from distributed\noptimization [Seide et al., 2014, Alistarh et al., 2018, Karimireddy et al.,\n2019] in which the error correction information is itself compressed to allow\nfor practical memory gains. We prove that the resulting approach maintains\ntheoretical convergence guarantees competitive to those of AMSGrad, while\nproviding good practical performance. Specifically, we show that MICROADAM can\nbe implemented efficiently on GPUs: on both million-scale (BERT) and\nbillion-scale (LLaMA) models, MicroAdam provides practical convergence\ncompetitive to that of the uncompressed Adam baseline, with lower memory usage\nand similar running time. Our code is available at\nhttps://github.com/IST-DASLab/MicroAdam.\n", "link": "http://arxiv.org/abs/2405.15593v1", "date": "2024-05-24", "relevancy": 2.0188, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5099}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5014}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MicroAdam%3A%20Accurate%20Adaptive%20Optimization%20with%20Low%20Space%20Overhead%20and%0A%20%20Provable%20Convergence&body=Title%3A%20MicroAdam%3A%20Accurate%20Adaptive%20Optimization%20with%20Low%20Space%20Overhead%20and%0A%20%20Provable%20Convergence%0AAuthor%3A%20Ionut-Vlad%20Modoranu%20and%20Mher%20Safaryan%20and%20Grigory%20Malinovsky%20and%20Eldar%20Kurtic%20and%20Thomas%20Robert%20and%20Peter%20Richtarik%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20We%20propose%20a%20new%20variant%20of%20the%20Adam%20optimizer%20%5BKingma%20and%20Ba%2C%202014%5D%20called%0AMICROADAM%20that%20specifically%20minimizes%20memory%20overheads%2C%20while%20maintaining%0Atheoretical%20convergence%20guarantees.%20We%20achieve%20this%20by%20compressing%20the%20gradient%0Ainformation%20before%20it%20is%20fed%20into%20the%20optimizer%20state%2C%20thereby%20reducing%20its%0Amemory%20footprint%20significantly.%20We%20control%20the%20resulting%20compression%20error%20via%0Aa%20novel%20instance%20of%20the%20classical%20error%20feedback%20mechanism%20from%20distributed%0Aoptimization%20%5BSeide%20et%20al.%2C%202014%2C%20Alistarh%20et%20al.%2C%202018%2C%20Karimireddy%20et%20al.%2C%0A2019%5D%20in%20which%20the%20error%20correction%20information%20is%20itself%20compressed%20to%20allow%0Afor%20practical%20memory%20gains.%20We%20prove%20that%20the%20resulting%20approach%20maintains%0Atheoretical%20convergence%20guarantees%20competitive%20to%20those%20of%20AMSGrad%2C%20while%0Aproviding%20good%20practical%20performance.%20Specifically%2C%20we%20show%20that%20MICROADAM%20can%0Abe%20implemented%20efficiently%20on%20GPUs%3A%20on%20both%20million-scale%20%28BERT%29%20and%0Abillion-scale%20%28LLaMA%29%20models%2C%20MicroAdam%20provides%20practical%20convergence%0Acompetitive%20to%20that%20of%20the%20uncompressed%20Adam%20baseline%2C%20with%20lower%20memory%20usage%0Aand%20similar%20running%20time.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IST-DASLab/MicroAdam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicroAdam%253A%2520Accurate%2520Adaptive%2520Optimization%2520with%2520Low%2520Space%2520Overhead%2520and%250A%2520%2520Provable%2520Convergence%26entry.906535625%3DIonut-Vlad%2520Modoranu%2520and%2520Mher%2520Safaryan%2520and%2520Grigory%2520Malinovsky%2520and%2520Eldar%2520Kurtic%2520and%2520Thomas%2520Robert%2520and%2520Peter%2520Richtarik%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520variant%2520of%2520the%2520Adam%2520optimizer%2520%255BKingma%2520and%2520Ba%252C%25202014%255D%2520called%250AMICROADAM%2520that%2520specifically%2520minimizes%2520memory%2520overheads%252C%2520while%2520maintaining%250Atheoretical%2520convergence%2520guarantees.%2520We%2520achieve%2520this%2520by%2520compressing%2520the%2520gradient%250Ainformation%2520before%2520it%2520is%2520fed%2520into%2520the%2520optimizer%2520state%252C%2520thereby%2520reducing%2520its%250Amemory%2520footprint%2520significantly.%2520We%2520control%2520the%2520resulting%2520compression%2520error%2520via%250Aa%2520novel%2520instance%2520of%2520the%2520classical%2520error%2520feedback%2520mechanism%2520from%2520distributed%250Aoptimization%2520%255BSeide%2520et%2520al.%252C%25202014%252C%2520Alistarh%2520et%2520al.%252C%25202018%252C%2520Karimireddy%2520et%2520al.%252C%250A2019%255D%2520in%2520which%2520the%2520error%2520correction%2520information%2520is%2520itself%2520compressed%2520to%2520allow%250Afor%2520practical%2520memory%2520gains.%2520We%2520prove%2520that%2520the%2520resulting%2520approach%2520maintains%250Atheoretical%2520convergence%2520guarantees%2520competitive%2520to%2520those%2520of%2520AMSGrad%252C%2520while%250Aproviding%2520good%2520practical%2520performance.%2520Specifically%252C%2520we%2520show%2520that%2520MICROADAM%2520can%250Abe%2520implemented%2520efficiently%2520on%2520GPUs%253A%2520on%2520both%2520million-scale%2520%2528BERT%2529%2520and%250Abillion-scale%2520%2528LLaMA%2529%2520models%252C%2520MicroAdam%2520provides%2520practical%2520convergence%250Acompetitive%2520to%2520that%2520of%2520the%2520uncompressed%2520Adam%2520baseline%252C%2520with%2520lower%2520memory%2520usage%250Aand%2520similar%2520running%2520time.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/IST-DASLab/MicroAdam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MicroAdam%3A%20Accurate%20Adaptive%20Optimization%20with%20Low%20Space%20Overhead%20and%0A%20%20Provable%20Convergence&entry.906535625=Ionut-Vlad%20Modoranu%20and%20Mher%20Safaryan%20and%20Grigory%20Malinovsky%20and%20Eldar%20Kurtic%20and%20Thomas%20Robert%20and%20Peter%20Richtarik%20and%20Dan%20Alistarh&entry.1292438233=%20%20We%20propose%20a%20new%20variant%20of%20the%20Adam%20optimizer%20%5BKingma%20and%20Ba%2C%202014%5D%20called%0AMICROADAM%20that%20specifically%20minimizes%20memory%20overheads%2C%20while%20maintaining%0Atheoretical%20convergence%20guarantees.%20We%20achieve%20this%20by%20compressing%20the%20gradient%0Ainformation%20before%20it%20is%20fed%20into%20the%20optimizer%20state%2C%20thereby%20reducing%20its%0Amemory%20footprint%20significantly.%20We%20control%20the%20resulting%20compression%20error%20via%0Aa%20novel%20instance%20of%20the%20classical%20error%20feedback%20mechanism%20from%20distributed%0Aoptimization%20%5BSeide%20et%20al.%2C%202014%2C%20Alistarh%20et%20al.%2C%202018%2C%20Karimireddy%20et%20al.%2C%0A2019%5D%20in%20which%20the%20error%20correction%20information%20is%20itself%20compressed%20to%20allow%0Afor%20practical%20memory%20gains.%20We%20prove%20that%20the%20resulting%20approach%20maintains%0Atheoretical%20convergence%20guarantees%20competitive%20to%20those%20of%20AMSGrad%2C%20while%0Aproviding%20good%20practical%20performance.%20Specifically%2C%20we%20show%20that%20MICROADAM%20can%0Abe%20implemented%20efficiently%20on%20GPUs%3A%20on%20both%20million-scale%20%28BERT%29%20and%0Abillion-scale%20%28LLaMA%29%20models%2C%20MicroAdam%20provides%20practical%20convergence%0Acompetitive%20to%20that%20of%20the%20uncompressed%20Adam%20baseline%2C%20with%20lower%20memory%20usage%0Aand%20similar%20running%20time.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/IST-DASLab/MicroAdam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15593v1&entry.124074799=Read"},
{"title": "Neuromorphic dreaming: A pathway to efficient learning in artificial\n  agents", "author": "Ingo Blakowski and Dmitrii Zendrikov and Cristiano Capone and Giacomo Indiveri", "abstract": "  Achieving energy efficiency in learning is a key challenge for artificial\nintelligence (AI) computing platforms. Biological systems demonstrate\nremarkable abilities to learn complex skills quickly and efficiently. Inspired\nby this, we present a hardware implementation of model-based reinforcement\nlearning (MBRL) using spiking neural networks (SNNs) on mixed-signal\nanalog/digital neuromorphic hardware. This approach leverages the energy\nefficiency of mixed-signal neuromorphic chips while achieving high sample\nefficiency through an alternation of online learning, referred to as the\n\"awake\" phase, and offline learning, known as the \"dreaming\" phase. The model\nproposed includes two symbiotic networks: an agent network that learns by\ncombining real and simulated experiences, and a learned world model network\nthat generates the simulated experiences. We validate the model by training the\nhardware implementation to play the Atari game Pong. We start from a baseline\nconsisting of an agent network learning without a world model and dreaming,\nwhich successfully learns to play the game. By incorporating dreaming, the\nnumber of required real game experiences are reduced significantly compared to\nthe baseline. The networks are implemented using a mixed-signal neuromorphic\nprocessor, with the readout layers trained using a computer in-the-loop, while\nthe other layers remain fixed. These results pave the way toward\nenergy-efficient neuromorphic learning systems capable of rapid learning in\nreal world applications and use-cases.\n", "link": "http://arxiv.org/abs/2405.15616v1", "date": "2024-05-24", "relevancy": 2.0171, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5179}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5022}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuromorphic%20dreaming%3A%20A%20pathway%20to%20efficient%20learning%20in%20artificial%0A%20%20agents&body=Title%3A%20Neuromorphic%20dreaming%3A%20A%20pathway%20to%20efficient%20learning%20in%20artificial%0A%20%20agents%0AAuthor%3A%20Ingo%20Blakowski%20and%20Dmitrii%20Zendrikov%20and%20Cristiano%20Capone%20and%20Giacomo%20Indiveri%0AAbstract%3A%20%20%20Achieving%20energy%20efficiency%20in%20learning%20is%20a%20key%20challenge%20for%20artificial%0Aintelligence%20%28AI%29%20computing%20platforms.%20Biological%20systems%20demonstrate%0Aremarkable%20abilities%20to%20learn%20complex%20skills%20quickly%20and%20efficiently.%20Inspired%0Aby%20this%2C%20we%20present%20a%20hardware%20implementation%20of%20model-based%20reinforcement%0Alearning%20%28MBRL%29%20using%20spiking%20neural%20networks%20%28SNNs%29%20on%20mixed-signal%0Aanalog/digital%20neuromorphic%20hardware.%20This%20approach%20leverages%20the%20energy%0Aefficiency%20of%20mixed-signal%20neuromorphic%20chips%20while%20achieving%20high%20sample%0Aefficiency%20through%20an%20alternation%20of%20online%20learning%2C%20referred%20to%20as%20the%0A%22awake%22%20phase%2C%20and%20offline%20learning%2C%20known%20as%20the%20%22dreaming%22%20phase.%20The%20model%0Aproposed%20includes%20two%20symbiotic%20networks%3A%20an%20agent%20network%20that%20learns%20by%0Acombining%20real%20and%20simulated%20experiences%2C%20and%20a%20learned%20world%20model%20network%0Athat%20generates%20the%20simulated%20experiences.%20We%20validate%20the%20model%20by%20training%20the%0Ahardware%20implementation%20to%20play%20the%20Atari%20game%20Pong.%20We%20start%20from%20a%20baseline%0Aconsisting%20of%20an%20agent%20network%20learning%20without%20a%20world%20model%20and%20dreaming%2C%0Awhich%20successfully%20learns%20to%20play%20the%20game.%20By%20incorporating%20dreaming%2C%20the%0Anumber%20of%20required%20real%20game%20experiences%20are%20reduced%20significantly%20compared%20to%0Athe%20baseline.%20The%20networks%20are%20implemented%20using%20a%20mixed-signal%20neuromorphic%0Aprocessor%2C%20with%20the%20readout%20layers%20trained%20using%20a%20computer%20in-the-loop%2C%20while%0Athe%20other%20layers%20remain%20fixed.%20These%20results%20pave%20the%20way%20toward%0Aenergy-efficient%20neuromorphic%20learning%20systems%20capable%20of%20rapid%20learning%20in%0Areal%20world%20applications%20and%20use-cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuromorphic%2520dreaming%253A%2520A%2520pathway%2520to%2520efficient%2520learning%2520in%2520artificial%250A%2520%2520agents%26entry.906535625%3DIngo%2520Blakowski%2520and%2520Dmitrii%2520Zendrikov%2520and%2520Cristiano%2520Capone%2520and%2520Giacomo%2520Indiveri%26entry.1292438233%3D%2520%2520Achieving%2520energy%2520efficiency%2520in%2520learning%2520is%2520a%2520key%2520challenge%2520for%2520artificial%250Aintelligence%2520%2528AI%2529%2520computing%2520platforms.%2520Biological%2520systems%2520demonstrate%250Aremarkable%2520abilities%2520to%2520learn%2520complex%2520skills%2520quickly%2520and%2520efficiently.%2520Inspired%250Aby%2520this%252C%2520we%2520present%2520a%2520hardware%2520implementation%2520of%2520model-based%2520reinforcement%250Alearning%2520%2528MBRL%2529%2520using%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%2520on%2520mixed-signal%250Aanalog/digital%2520neuromorphic%2520hardware.%2520This%2520approach%2520leverages%2520the%2520energy%250Aefficiency%2520of%2520mixed-signal%2520neuromorphic%2520chips%2520while%2520achieving%2520high%2520sample%250Aefficiency%2520through%2520an%2520alternation%2520of%2520online%2520learning%252C%2520referred%2520to%2520as%2520the%250A%2522awake%2522%2520phase%252C%2520and%2520offline%2520learning%252C%2520known%2520as%2520the%2520%2522dreaming%2522%2520phase.%2520The%2520model%250Aproposed%2520includes%2520two%2520symbiotic%2520networks%253A%2520an%2520agent%2520network%2520that%2520learns%2520by%250Acombining%2520real%2520and%2520simulated%2520experiences%252C%2520and%2520a%2520learned%2520world%2520model%2520network%250Athat%2520generates%2520the%2520simulated%2520experiences.%2520We%2520validate%2520the%2520model%2520by%2520training%2520the%250Ahardware%2520implementation%2520to%2520play%2520the%2520Atari%2520game%2520Pong.%2520We%2520start%2520from%2520a%2520baseline%250Aconsisting%2520of%2520an%2520agent%2520network%2520learning%2520without%2520a%2520world%2520model%2520and%2520dreaming%252C%250Awhich%2520successfully%2520learns%2520to%2520play%2520the%2520game.%2520By%2520incorporating%2520dreaming%252C%2520the%250Anumber%2520of%2520required%2520real%2520game%2520experiences%2520are%2520reduced%2520significantly%2520compared%2520to%250Athe%2520baseline.%2520The%2520networks%2520are%2520implemented%2520using%2520a%2520mixed-signal%2520neuromorphic%250Aprocessor%252C%2520with%2520the%2520readout%2520layers%2520trained%2520using%2520a%2520computer%2520in-the-loop%252C%2520while%250Athe%2520other%2520layers%2520remain%2520fixed.%2520These%2520results%2520pave%2520the%2520way%2520toward%250Aenergy-efficient%2520neuromorphic%2520learning%2520systems%2520capable%2520of%2520rapid%2520learning%2520in%250Areal%2520world%2520applications%2520and%2520use-cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuromorphic%20dreaming%3A%20A%20pathway%20to%20efficient%20learning%20in%20artificial%0A%20%20agents&entry.906535625=Ingo%20Blakowski%20and%20Dmitrii%20Zendrikov%20and%20Cristiano%20Capone%20and%20Giacomo%20Indiveri&entry.1292438233=%20%20Achieving%20energy%20efficiency%20in%20learning%20is%20a%20key%20challenge%20for%20artificial%0Aintelligence%20%28AI%29%20computing%20platforms.%20Biological%20systems%20demonstrate%0Aremarkable%20abilities%20to%20learn%20complex%20skills%20quickly%20and%20efficiently.%20Inspired%0Aby%20this%2C%20we%20present%20a%20hardware%20implementation%20of%20model-based%20reinforcement%0Alearning%20%28MBRL%29%20using%20spiking%20neural%20networks%20%28SNNs%29%20on%20mixed-signal%0Aanalog/digital%20neuromorphic%20hardware.%20This%20approach%20leverages%20the%20energy%0Aefficiency%20of%20mixed-signal%20neuromorphic%20chips%20while%20achieving%20high%20sample%0Aefficiency%20through%20an%20alternation%20of%20online%20learning%2C%20referred%20to%20as%20the%0A%22awake%22%20phase%2C%20and%20offline%20learning%2C%20known%20as%20the%20%22dreaming%22%20phase.%20The%20model%0Aproposed%20includes%20two%20symbiotic%20networks%3A%20an%20agent%20network%20that%20learns%20by%0Acombining%20real%20and%20simulated%20experiences%2C%20and%20a%20learned%20world%20model%20network%0Athat%20generates%20the%20simulated%20experiences.%20We%20validate%20the%20model%20by%20training%20the%0Ahardware%20implementation%20to%20play%20the%20Atari%20game%20Pong.%20We%20start%20from%20a%20baseline%0Aconsisting%20of%20an%20agent%20network%20learning%20without%20a%20world%20model%20and%20dreaming%2C%0Awhich%20successfully%20learns%20to%20play%20the%20game.%20By%20incorporating%20dreaming%2C%20the%0Anumber%20of%20required%20real%20game%20experiences%20are%20reduced%20significantly%20compared%20to%0Athe%20baseline.%20The%20networks%20are%20implemented%20using%20a%20mixed-signal%20neuromorphic%0Aprocessor%2C%20with%20the%20readout%20layers%20trained%20using%20a%20computer%20in-the-loop%2C%20while%0Athe%20other%20layers%20remain%20fixed.%20These%20results%20pave%20the%20way%20toward%0Aenergy-efficient%20neuromorphic%20learning%20systems%20capable%20of%20rapid%20learning%20in%0Areal%20world%20applications%20and%20use-cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15616v1&entry.124074799=Read"},
{"title": "On the Computational Landscape of Replicable Learning", "author": "Alkis Kalavasis and Amin Karbasi and Grigoris Velegkas and Felix Zhou", "abstract": "  We study computational aspects of algorithmic replicability, a notion of\nstability introduced by Impagliazzo, Lei, Pitassi, and Sorrell [2022].\nMotivated by a recent line of work that established strong statistical\nconnections between replicability and other notions of learnability such as\nonline learning, private learning, and SQ learning, we aim to understand better\nthe computational connections between replicability and these learning\nparadigms. Our first result shows that there is a concept class that is\nefficiently replicably PAC learnable, but, under standard cryptographic\nassumptions, no efficient online learner exists for this class. Subsequently,\nwe design an efficient replicable learner for PAC learning parities when the\nmarginal distribution is far from uniform, making progress on a question posed\nby Impagliazzo et al. [2022]. To obtain this result, we design a replicable\nlifting framework inspired by Blanc, Lange, Malik, and Tan [2023] that\ntransforms in a black-box manner efficient replicable PAC learners under the\nuniform marginal distribution over the Boolean hypercube to replicable PAC\nlearners under any marginal distribution, with sample and time complexity that\ndepends on a certain measure of the complexity of the distribution. Finally, we\nshow that any pure DP learner can be transformed to a replicable one in time\npolynomial in the accuracy, confidence parameters and exponential in the\nrepresentation dimension of the underlying hypothesis class.\n", "link": "http://arxiv.org/abs/2405.15599v1", "date": "2024-05-24", "relevancy": 1.759, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4479}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Computational%20Landscape%20of%20Replicable%20Learning&body=Title%3A%20On%20the%20Computational%20Landscape%20of%20Replicable%20Learning%0AAuthor%3A%20Alkis%20Kalavasis%20and%20Amin%20Karbasi%20and%20Grigoris%20Velegkas%20and%20Felix%20Zhou%0AAbstract%3A%20%20%20We%20study%20computational%20aspects%20of%20algorithmic%20replicability%2C%20a%20notion%20of%0Astability%20introduced%20by%20Impagliazzo%2C%20Lei%2C%20Pitassi%2C%20and%20Sorrell%20%5B2022%5D.%0AMotivated%20by%20a%20recent%20line%20of%20work%20that%20established%20strong%20statistical%0Aconnections%20between%20replicability%20and%20other%20notions%20of%20learnability%20such%20as%0Aonline%20learning%2C%20private%20learning%2C%20and%20SQ%20learning%2C%20we%20aim%20to%20understand%20better%0Athe%20computational%20connections%20between%20replicability%20and%20these%20learning%0Aparadigms.%20Our%20first%20result%20shows%20that%20there%20is%20a%20concept%20class%20that%20is%0Aefficiently%20replicably%20PAC%20learnable%2C%20but%2C%20under%20standard%20cryptographic%0Aassumptions%2C%20no%20efficient%20online%20learner%20exists%20for%20this%20class.%20Subsequently%2C%0Awe%20design%20an%20efficient%20replicable%20learner%20for%20PAC%20learning%20parities%20when%20the%0Amarginal%20distribution%20is%20far%20from%20uniform%2C%20making%20progress%20on%20a%20question%20posed%0Aby%20Impagliazzo%20et%20al.%20%5B2022%5D.%20To%20obtain%20this%20result%2C%20we%20design%20a%20replicable%0Alifting%20framework%20inspired%20by%20Blanc%2C%20Lange%2C%20Malik%2C%20and%20Tan%20%5B2023%5D%20that%0Atransforms%20in%20a%20black-box%20manner%20efficient%20replicable%20PAC%20learners%20under%20the%0Auniform%20marginal%20distribution%20over%20the%20Boolean%20hypercube%20to%20replicable%20PAC%0Alearners%20under%20any%20marginal%20distribution%2C%20with%20sample%20and%20time%20complexity%20that%0Adepends%20on%20a%20certain%20measure%20of%20the%20complexity%20of%20the%20distribution.%20Finally%2C%20we%0Ashow%20that%20any%20pure%20DP%20learner%20can%20be%20transformed%20to%20a%20replicable%20one%20in%20time%0Apolynomial%20in%20the%20accuracy%2C%20confidence%20parameters%20and%20exponential%20in%20the%0Arepresentation%20dimension%20of%20the%20underlying%20hypothesis%20class.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Computational%2520Landscape%2520of%2520Replicable%2520Learning%26entry.906535625%3DAlkis%2520Kalavasis%2520and%2520Amin%2520Karbasi%2520and%2520Grigoris%2520Velegkas%2520and%2520Felix%2520Zhou%26entry.1292438233%3D%2520%2520We%2520study%2520computational%2520aspects%2520of%2520algorithmic%2520replicability%252C%2520a%2520notion%2520of%250Astability%2520introduced%2520by%2520Impagliazzo%252C%2520Lei%252C%2520Pitassi%252C%2520and%2520Sorrell%2520%255B2022%255D.%250AMotivated%2520by%2520a%2520recent%2520line%2520of%2520work%2520that%2520established%2520strong%2520statistical%250Aconnections%2520between%2520replicability%2520and%2520other%2520notions%2520of%2520learnability%2520such%2520as%250Aonline%2520learning%252C%2520private%2520learning%252C%2520and%2520SQ%2520learning%252C%2520we%2520aim%2520to%2520understand%2520better%250Athe%2520computational%2520connections%2520between%2520replicability%2520and%2520these%2520learning%250Aparadigms.%2520Our%2520first%2520result%2520shows%2520that%2520there%2520is%2520a%2520concept%2520class%2520that%2520is%250Aefficiently%2520replicably%2520PAC%2520learnable%252C%2520but%252C%2520under%2520standard%2520cryptographic%250Aassumptions%252C%2520no%2520efficient%2520online%2520learner%2520exists%2520for%2520this%2520class.%2520Subsequently%252C%250Awe%2520design%2520an%2520efficient%2520replicable%2520learner%2520for%2520PAC%2520learning%2520parities%2520when%2520the%250Amarginal%2520distribution%2520is%2520far%2520from%2520uniform%252C%2520making%2520progress%2520on%2520a%2520question%2520posed%250Aby%2520Impagliazzo%2520et%2520al.%2520%255B2022%255D.%2520To%2520obtain%2520this%2520result%252C%2520we%2520design%2520a%2520replicable%250Alifting%2520framework%2520inspired%2520by%2520Blanc%252C%2520Lange%252C%2520Malik%252C%2520and%2520Tan%2520%255B2023%255D%2520that%250Atransforms%2520in%2520a%2520black-box%2520manner%2520efficient%2520replicable%2520PAC%2520learners%2520under%2520the%250Auniform%2520marginal%2520distribution%2520over%2520the%2520Boolean%2520hypercube%2520to%2520replicable%2520PAC%250Alearners%2520under%2520any%2520marginal%2520distribution%252C%2520with%2520sample%2520and%2520time%2520complexity%2520that%250Adepends%2520on%2520a%2520certain%2520measure%2520of%2520the%2520complexity%2520of%2520the%2520distribution.%2520Finally%252C%2520we%250Ashow%2520that%2520any%2520pure%2520DP%2520learner%2520can%2520be%2520transformed%2520to%2520a%2520replicable%2520one%2520in%2520time%250Apolynomial%2520in%2520the%2520accuracy%252C%2520confidence%2520parameters%2520and%2520exponential%2520in%2520the%250Arepresentation%2520dimension%2520of%2520the%2520underlying%2520hypothesis%2520class.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Computational%20Landscape%20of%20Replicable%20Learning&entry.906535625=Alkis%20Kalavasis%20and%20Amin%20Karbasi%20and%20Grigoris%20Velegkas%20and%20Felix%20Zhou&entry.1292438233=%20%20We%20study%20computational%20aspects%20of%20algorithmic%20replicability%2C%20a%20notion%20of%0Astability%20introduced%20by%20Impagliazzo%2C%20Lei%2C%20Pitassi%2C%20and%20Sorrell%20%5B2022%5D.%0AMotivated%20by%20a%20recent%20line%20of%20work%20that%20established%20strong%20statistical%0Aconnections%20between%20replicability%20and%20other%20notions%20of%20learnability%20such%20as%0Aonline%20learning%2C%20private%20learning%2C%20and%20SQ%20learning%2C%20we%20aim%20to%20understand%20better%0Athe%20computational%20connections%20between%20replicability%20and%20these%20learning%0Aparadigms.%20Our%20first%20result%20shows%20that%20there%20is%20a%20concept%20class%20that%20is%0Aefficiently%20replicably%20PAC%20learnable%2C%20but%2C%20under%20standard%20cryptographic%0Aassumptions%2C%20no%20efficient%20online%20learner%20exists%20for%20this%20class.%20Subsequently%2C%0Awe%20design%20an%20efficient%20replicable%20learner%20for%20PAC%20learning%20parities%20when%20the%0Amarginal%20distribution%20is%20far%20from%20uniform%2C%20making%20progress%20on%20a%20question%20posed%0Aby%20Impagliazzo%20et%20al.%20%5B2022%5D.%20To%20obtain%20this%20result%2C%20we%20design%20a%20replicable%0Alifting%20framework%20inspired%20by%20Blanc%2C%20Lange%2C%20Malik%2C%20and%20Tan%20%5B2023%5D%20that%0Atransforms%20in%20a%20black-box%20manner%20efficient%20replicable%20PAC%20learners%20under%20the%0Auniform%20marginal%20distribution%20over%20the%20Boolean%20hypercube%20to%20replicable%20PAC%0Alearners%20under%20any%20marginal%20distribution%2C%20with%20sample%20and%20time%20complexity%20that%0Adepends%20on%20a%20certain%20measure%20of%20the%20complexity%20of%20the%20distribution.%20Finally%2C%20we%0Ashow%20that%20any%20pure%20DP%20learner%20can%20be%20transformed%20to%20a%20replicable%20one%20in%20time%0Apolynomial%20in%20the%20accuracy%2C%20confidence%20parameters%20and%20exponential%20in%20the%0Arepresentation%20dimension%20of%20the%20underlying%20hypothesis%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15599v1&entry.124074799=Read"},
{"title": "Towards Precision Healthcare: Robust Fusion of Time Series and Image\n  Data", "author": "Ali Rasekh and Reza Heidari and Amir Hosein Haji Mohammad Rezaie and Parsa Sharifi Sedeh and Zahra Ahmadi and Prasenjit Mitra and Wolfgang Nejdl", "abstract": "  With the increasing availability of diverse data types, particularly images\nand time series data from medical experiments, there is a growing demand for\ntechniques designed to combine various modalities of data effectively. Our\nmotivation comes from the important areas of predicting mortality and\nphenotyping where using different modalities of data could significantly\nimprove our ability to predict. To tackle this challenge, we introduce a new\nmethod that uses two separate encoders, one for each type of data, allowing the\nmodel to understand complex patterns in both visual and time-based information.\nApart from the technical challenges, our goal is to make the predictive model\nmore robust in noisy conditions and perform better than current methods. We\nalso deal with imbalanced datasets and use an uncertainty loss function,\nyielding improved results while simultaneously providing a principled means of\nmodeling uncertainty. Additionally, we include attention mechanisms to fuse\ndifferent modalities, allowing the model to focus on what's important for each\ntask. We tested our approach using the comprehensive multimodal MIMIC dataset,\ncombining MIMIC-IV and MIMIC-CXR datasets. Our experiments show that our method\nis effective in improving multimodal deep learning for clinical applications.\nThe code will be made available online.\n", "link": "http://arxiv.org/abs/2405.15442v1", "date": "2024-05-24", "relevancy": 1.6355, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5534}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5498}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Precision%20Healthcare%3A%20Robust%20Fusion%20of%20Time%20Series%20and%20Image%0A%20%20Data&body=Title%3A%20Towards%20Precision%20Healthcare%3A%20Robust%20Fusion%20of%20Time%20Series%20and%20Image%0A%20%20Data%0AAuthor%3A%20Ali%20Rasekh%20and%20Reza%20Heidari%20and%20Amir%20Hosein%20Haji%20Mohammad%20Rezaie%20and%20Parsa%20Sharifi%20Sedeh%20and%20Zahra%20Ahmadi%20and%20Prasenjit%20Mitra%20and%20Wolfgang%20Nejdl%0AAbstract%3A%20%20%20With%20the%20increasing%20availability%20of%20diverse%20data%20types%2C%20particularly%20images%0Aand%20time%20series%20data%20from%20medical%20experiments%2C%20there%20is%20a%20growing%20demand%20for%0Atechniques%20designed%20to%20combine%20various%20modalities%20of%20data%20effectively.%20Our%0Amotivation%20comes%20from%20the%20important%20areas%20of%20predicting%20mortality%20and%0Aphenotyping%20where%20using%20different%20modalities%20of%20data%20could%20significantly%0Aimprove%20our%20ability%20to%20predict.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%20new%0Amethod%20that%20uses%20two%20separate%20encoders%2C%20one%20for%20each%20type%20of%20data%2C%20allowing%20the%0Amodel%20to%20understand%20complex%20patterns%20in%20both%20visual%20and%20time-based%20information.%0AApart%20from%20the%20technical%20challenges%2C%20our%20goal%20is%20to%20make%20the%20predictive%20model%0Amore%20robust%20in%20noisy%20conditions%20and%20perform%20better%20than%20current%20methods.%20We%0Aalso%20deal%20with%20imbalanced%20datasets%20and%20use%20an%20uncertainty%20loss%20function%2C%0Ayielding%20improved%20results%20while%20simultaneously%20providing%20a%20principled%20means%20of%0Amodeling%20uncertainty.%20Additionally%2C%20we%20include%20attention%20mechanisms%20to%20fuse%0Adifferent%20modalities%2C%20allowing%20the%20model%20to%20focus%20on%20what%27s%20important%20for%20each%0Atask.%20We%20tested%20our%20approach%20using%20the%20comprehensive%20multimodal%20MIMIC%20dataset%2C%0Acombining%20MIMIC-IV%20and%20MIMIC-CXR%20datasets.%20Our%20experiments%20show%20that%20our%20method%0Ais%20effective%20in%20improving%20multimodal%20deep%20learning%20for%20clinical%20applications.%0AThe%20code%20will%20be%20made%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Precision%2520Healthcare%253A%2520Robust%2520Fusion%2520of%2520Time%2520Series%2520and%2520Image%250A%2520%2520Data%26entry.906535625%3DAli%2520Rasekh%2520and%2520Reza%2520Heidari%2520and%2520Amir%2520Hosein%2520Haji%2520Mohammad%2520Rezaie%2520and%2520Parsa%2520Sharifi%2520Sedeh%2520and%2520Zahra%2520Ahmadi%2520and%2520Prasenjit%2520Mitra%2520and%2520Wolfgang%2520Nejdl%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520availability%2520of%2520diverse%2520data%2520types%252C%2520particularly%2520images%250Aand%2520time%2520series%2520data%2520from%2520medical%2520experiments%252C%2520there%2520is%2520a%2520growing%2520demand%2520for%250Atechniques%2520designed%2520to%2520combine%2520various%2520modalities%2520of%2520data%2520effectively.%2520Our%250Amotivation%2520comes%2520from%2520the%2520important%2520areas%2520of%2520predicting%2520mortality%2520and%250Aphenotyping%2520where%2520using%2520different%2520modalities%2520of%2520data%2520could%2520significantly%250Aimprove%2520our%2520ability%2520to%2520predict.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520new%250Amethod%2520that%2520uses%2520two%2520separate%2520encoders%252C%2520one%2520for%2520each%2520type%2520of%2520data%252C%2520allowing%2520the%250Amodel%2520to%2520understand%2520complex%2520patterns%2520in%2520both%2520visual%2520and%2520time-based%2520information.%250AApart%2520from%2520the%2520technical%2520challenges%252C%2520our%2520goal%2520is%2520to%2520make%2520the%2520predictive%2520model%250Amore%2520robust%2520in%2520noisy%2520conditions%2520and%2520perform%2520better%2520than%2520current%2520methods.%2520We%250Aalso%2520deal%2520with%2520imbalanced%2520datasets%2520and%2520use%2520an%2520uncertainty%2520loss%2520function%252C%250Ayielding%2520improved%2520results%2520while%2520simultaneously%2520providing%2520a%2520principled%2520means%2520of%250Amodeling%2520uncertainty.%2520Additionally%252C%2520we%2520include%2520attention%2520mechanisms%2520to%2520fuse%250Adifferent%2520modalities%252C%2520allowing%2520the%2520model%2520to%2520focus%2520on%2520what%2527s%2520important%2520for%2520each%250Atask.%2520We%2520tested%2520our%2520approach%2520using%2520the%2520comprehensive%2520multimodal%2520MIMIC%2520dataset%252C%250Acombining%2520MIMIC-IV%2520and%2520MIMIC-CXR%2520datasets.%2520Our%2520experiments%2520show%2520that%2520our%2520method%250Ais%2520effective%2520in%2520improving%2520multimodal%2520deep%2520learning%2520for%2520clinical%2520applications.%250AThe%2520code%2520will%2520be%2520made%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Precision%20Healthcare%3A%20Robust%20Fusion%20of%20Time%20Series%20and%20Image%0A%20%20Data&entry.906535625=Ali%20Rasekh%20and%20Reza%20Heidari%20and%20Amir%20Hosein%20Haji%20Mohammad%20Rezaie%20and%20Parsa%20Sharifi%20Sedeh%20and%20Zahra%20Ahmadi%20and%20Prasenjit%20Mitra%20and%20Wolfgang%20Nejdl&entry.1292438233=%20%20With%20the%20increasing%20availability%20of%20diverse%20data%20types%2C%20particularly%20images%0Aand%20time%20series%20data%20from%20medical%20experiments%2C%20there%20is%20a%20growing%20demand%20for%0Atechniques%20designed%20to%20combine%20various%20modalities%20of%20data%20effectively.%20Our%0Amotivation%20comes%20from%20the%20important%20areas%20of%20predicting%20mortality%20and%0Aphenotyping%20where%20using%20different%20modalities%20of%20data%20could%20significantly%0Aimprove%20our%20ability%20to%20predict.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%20new%0Amethod%20that%20uses%20two%20separate%20encoders%2C%20one%20for%20each%20type%20of%20data%2C%20allowing%20the%0Amodel%20to%20understand%20complex%20patterns%20in%20both%20visual%20and%20time-based%20information.%0AApart%20from%20the%20technical%20challenges%2C%20our%20goal%20is%20to%20make%20the%20predictive%20model%0Amore%20robust%20in%20noisy%20conditions%20and%20perform%20better%20than%20current%20methods.%20We%0Aalso%20deal%20with%20imbalanced%20datasets%20and%20use%20an%20uncertainty%20loss%20function%2C%0Ayielding%20improved%20results%20while%20simultaneously%20providing%20a%20principled%20means%20of%0Amodeling%20uncertainty.%20Additionally%2C%20we%20include%20attention%20mechanisms%20to%20fuse%0Adifferent%20modalities%2C%20allowing%20the%20model%20to%20focus%20on%20what%27s%20important%20for%20each%0Atask.%20We%20tested%20our%20approach%20using%20the%20comprehensive%20multimodal%20MIMIC%20dataset%2C%0Acombining%20MIMIC-IV%20and%20MIMIC-CXR%20datasets.%20Our%20experiments%20show%20that%20our%20method%0Ais%20effective%20in%20improving%20multimodal%20deep%20learning%20for%20clinical%20applications.%0AThe%20code%20will%20be%20made%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15442v1&entry.124074799=Read"},
{"title": "Infinite Limits of Multi-head Transformer Dynamics", "author": "Blake Bordelon and Hamza Tahir Chaudhry and Cengiz Pehlevan", "abstract": "  In this work, we analyze various scaling limits of the training dynamics of\ntransformer models in the feature learning regime. We identify the set of\nparameterizations that admit well-defined infinite width and depth limits,\nallowing the attention layers to update throughout training--a relevant notion\nof feature learning in these models. We then use tools from dynamical mean\nfield theory (DMFT) to analyze various infinite limits (infinite key/query\ndimension, infinite heads, and infinite depth) which have different statistical\ndescriptions depending on which infinite limit is taken and how attention\nlayers are scaled. We provide numerical evidence of convergence to the limits\nand discuss how the parameterization qualitatively influences learned features.\n", "link": "http://arxiv.org/abs/2405.15712v1", "date": "2024-05-24", "relevancy": 1.9944, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5724}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4867}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinite%20Limits%20of%20Multi-head%20Transformer%20Dynamics&body=Title%3A%20Infinite%20Limits%20of%20Multi-head%20Transformer%20Dynamics%0AAuthor%3A%20Blake%20Bordelon%20and%20Hamza%20Tahir%20Chaudhry%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20analyze%20various%20scaling%20limits%20of%20the%20training%20dynamics%20of%0Atransformer%20models%20in%20the%20feature%20learning%20regime.%20We%20identify%20the%20set%20of%0Aparameterizations%20that%20admit%20well-defined%20infinite%20width%20and%20depth%20limits%2C%0Aallowing%20the%20attention%20layers%20to%20update%20throughout%20training--a%20relevant%20notion%0Aof%20feature%20learning%20in%20these%20models.%20We%20then%20use%20tools%20from%20dynamical%20mean%0Afield%20theory%20%28DMFT%29%20to%20analyze%20various%20infinite%20limits%20%28infinite%20key/query%0Adimension%2C%20infinite%20heads%2C%20and%20infinite%20depth%29%20which%20have%20different%20statistical%0Adescriptions%20depending%20on%20which%20infinite%20limit%20is%20taken%20and%20how%20attention%0Alayers%20are%20scaled.%20We%20provide%20numerical%20evidence%20of%20convergence%20to%20the%20limits%0Aand%20discuss%20how%20the%20parameterization%20qualitatively%20influences%20learned%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinite%2520Limits%2520of%2520Multi-head%2520Transformer%2520Dynamics%26entry.906535625%3DBlake%2520Bordelon%2520and%2520Hamza%2520Tahir%2520Chaudhry%2520and%2520Cengiz%2520Pehlevan%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520analyze%2520various%2520scaling%2520limits%2520of%2520the%2520training%2520dynamics%2520of%250Atransformer%2520models%2520in%2520the%2520feature%2520learning%2520regime.%2520We%2520identify%2520the%2520set%2520of%250Aparameterizations%2520that%2520admit%2520well-defined%2520infinite%2520width%2520and%2520depth%2520limits%252C%250Aallowing%2520the%2520attention%2520layers%2520to%2520update%2520throughout%2520training--a%2520relevant%2520notion%250Aof%2520feature%2520learning%2520in%2520these%2520models.%2520We%2520then%2520use%2520tools%2520from%2520dynamical%2520mean%250Afield%2520theory%2520%2528DMFT%2529%2520to%2520analyze%2520various%2520infinite%2520limits%2520%2528infinite%2520key/query%250Adimension%252C%2520infinite%2520heads%252C%2520and%2520infinite%2520depth%2529%2520which%2520have%2520different%2520statistical%250Adescriptions%2520depending%2520on%2520which%2520infinite%2520limit%2520is%2520taken%2520and%2520how%2520attention%250Alayers%2520are%2520scaled.%2520We%2520provide%2520numerical%2520evidence%2520of%2520convergence%2520to%2520the%2520limits%250Aand%2520discuss%2520how%2520the%2520parameterization%2520qualitatively%2520influences%2520learned%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinite%20Limits%20of%20Multi-head%20Transformer%20Dynamics&entry.906535625=Blake%20Bordelon%20and%20Hamza%20Tahir%20Chaudhry%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20In%20this%20work%2C%20we%20analyze%20various%20scaling%20limits%20of%20the%20training%20dynamics%20of%0Atransformer%20models%20in%20the%20feature%20learning%20regime.%20We%20identify%20the%20set%20of%0Aparameterizations%20that%20admit%20well-defined%20infinite%20width%20and%20depth%20limits%2C%0Aallowing%20the%20attention%20layers%20to%20update%20throughout%20training--a%20relevant%20notion%0Aof%20feature%20learning%20in%20these%20models.%20We%20then%20use%20tools%20from%20dynamical%20mean%0Afield%20theory%20%28DMFT%29%20to%20analyze%20various%20infinite%20limits%20%28infinite%20key/query%0Adimension%2C%20infinite%20heads%2C%20and%20infinite%20depth%29%20which%20have%20different%20statistical%0Adescriptions%20depending%20on%20which%20infinite%20limit%20is%20taken%20and%20how%20attention%0Alayers%20are%20scaled.%20We%20provide%20numerical%20evidence%20of%20convergence%20to%20the%20limits%0Aand%20discuss%20how%20the%20parameterization%20qualitatively%20influences%20learned%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15712v1&entry.124074799=Read"},
{"title": "Randomized heuristic repair for large-scale multidimensional knapsack\n  problem", "author": "Jean P. Martins", "abstract": "  The multidimensional knapsack problem (MKP) is an NP-hard combinatorial\noptimization problem whose solution is determining a subset of maximum total\nprofit items that do not violate capacity constraints. Due to its hardness,\nlarge-scale MKP instances are usually a target for metaheuristics, a context in\nwhich effective feasibility maintenance strategies are crucial. In 1998, Chu\nand Beasley proposed an effective heuristic repair that is still relevant for\nrecent metaheuristics. However, due to its deterministic nature, the diversity\nof solutions such heuristic provides is insufficient for long runs. As a\nresult, the search for new solutions ceases after a while. This paper proposes\nan efficiency-based randomization strategy for the heuristic repair that\nincreases the variability of the repaired solutions without deteriorating\nquality and improves the overall results.\n", "link": "http://arxiv.org/abs/2405.15569v1", "date": "2024-05-24", "relevancy": 1.1668, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4142}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3864}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Randomized%20heuristic%20repair%20for%20large-scale%20multidimensional%20knapsack%0A%20%20problem&body=Title%3A%20Randomized%20heuristic%20repair%20for%20large-scale%20multidimensional%20knapsack%0A%20%20problem%0AAuthor%3A%20Jean%20P.%20Martins%0AAbstract%3A%20%20%20The%20multidimensional%20knapsack%20problem%20%28MKP%29%20is%20an%20NP-hard%20combinatorial%0Aoptimization%20problem%20whose%20solution%20is%20determining%20a%20subset%20of%20maximum%20total%0Aprofit%20items%20that%20do%20not%20violate%20capacity%20constraints.%20Due%20to%20its%20hardness%2C%0Alarge-scale%20MKP%20instances%20are%20usually%20a%20target%20for%20metaheuristics%2C%20a%20context%20in%0Awhich%20effective%20feasibility%20maintenance%20strategies%20are%20crucial.%20In%201998%2C%20Chu%0Aand%20Beasley%20proposed%20an%20effective%20heuristic%20repair%20that%20is%20still%20relevant%20for%0Arecent%20metaheuristics.%20However%2C%20due%20to%20its%20deterministic%20nature%2C%20the%20diversity%0Aof%20solutions%20such%20heuristic%20provides%20is%20insufficient%20for%20long%20runs.%20As%20a%0Aresult%2C%20the%20search%20for%20new%20solutions%20ceases%20after%20a%20while.%20This%20paper%20proposes%0Aan%20efficiency-based%20randomization%20strategy%20for%20the%20heuristic%20repair%20that%0Aincreases%20the%20variability%20of%20the%20repaired%20solutions%20without%20deteriorating%0Aquality%20and%20improves%20the%20overall%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandomized%2520heuristic%2520repair%2520for%2520large-scale%2520multidimensional%2520knapsack%250A%2520%2520problem%26entry.906535625%3DJean%2520P.%2520Martins%26entry.1292438233%3D%2520%2520The%2520multidimensional%2520knapsack%2520problem%2520%2528MKP%2529%2520is%2520an%2520NP-hard%2520combinatorial%250Aoptimization%2520problem%2520whose%2520solution%2520is%2520determining%2520a%2520subset%2520of%2520maximum%2520total%250Aprofit%2520items%2520that%2520do%2520not%2520violate%2520capacity%2520constraints.%2520Due%2520to%2520its%2520hardness%252C%250Alarge-scale%2520MKP%2520instances%2520are%2520usually%2520a%2520target%2520for%2520metaheuristics%252C%2520a%2520context%2520in%250Awhich%2520effective%2520feasibility%2520maintenance%2520strategies%2520are%2520crucial.%2520In%25201998%252C%2520Chu%250Aand%2520Beasley%2520proposed%2520an%2520effective%2520heuristic%2520repair%2520that%2520is%2520still%2520relevant%2520for%250Arecent%2520metaheuristics.%2520However%252C%2520due%2520to%2520its%2520deterministic%2520nature%252C%2520the%2520diversity%250Aof%2520solutions%2520such%2520heuristic%2520provides%2520is%2520insufficient%2520for%2520long%2520runs.%2520As%2520a%250Aresult%252C%2520the%2520search%2520for%2520new%2520solutions%2520ceases%2520after%2520a%2520while.%2520This%2520paper%2520proposes%250Aan%2520efficiency-based%2520randomization%2520strategy%2520for%2520the%2520heuristic%2520repair%2520that%250Aincreases%2520the%2520variability%2520of%2520the%2520repaired%2520solutions%2520without%2520deteriorating%250Aquality%2520and%2520improves%2520the%2520overall%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Randomized%20heuristic%20repair%20for%20large-scale%20multidimensional%20knapsack%0A%20%20problem&entry.906535625=Jean%20P.%20Martins&entry.1292438233=%20%20The%20multidimensional%20knapsack%20problem%20%28MKP%29%20is%20an%20NP-hard%20combinatorial%0Aoptimization%20problem%20whose%20solution%20is%20determining%20a%20subset%20of%20maximum%20total%0Aprofit%20items%20that%20do%20not%20violate%20capacity%20constraints.%20Due%20to%20its%20hardness%2C%0Alarge-scale%20MKP%20instances%20are%20usually%20a%20target%20for%20metaheuristics%2C%20a%20context%20in%0Awhich%20effective%20feasibility%20maintenance%20strategies%20are%20crucial.%20In%201998%2C%20Chu%0Aand%20Beasley%20proposed%20an%20effective%20heuristic%20repair%20that%20is%20still%20relevant%20for%0Arecent%20metaheuristics.%20However%2C%20due%20to%20its%20deterministic%20nature%2C%20the%20diversity%0Aof%20solutions%20such%20heuristic%20provides%20is%20insufficient%20for%20long%20runs.%20As%20a%0Aresult%2C%20the%20search%20for%20new%20solutions%20ceases%20after%20a%20while.%20This%20paper%20proposes%0Aan%20efficiency-based%20randomization%20strategy%20for%20the%20heuristic%20repair%20that%0Aincreases%20the%20variability%20of%20the%20repaired%20solutions%20without%20deteriorating%0Aquality%20and%20improves%20the%20overall%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15569v1&entry.124074799=Read"},
{"title": "Coordinated Disclosure for AI: Beyond Security Vulnerabilities", "author": "Sven Cattell and Avijit Ghosh and Lucie-Aim\u00e9e Kaffee", "abstract": "  Harm reporting in the field of Artificial Intelligence (AI) currently\noperates on an ad hoc basis, lacking a structured process for disclosing or\naddressing algorithmic flaws. In contrast, the Coordinated Vulnerability\nDisclosure (CVD) ethos and ecosystem play a pivotal role in software security\nand transparency. Globally, there are ongoing efforts to establish frameworks\nthat promote transparency and collaboration in addressing AI-related issues,\nthough challenges persist. Algorithmic flaws in machine learning (ML) models\npresent distinct challenges compared to traditional software vulnerabilities,\nwarranting a specialized approach. To address this gap, we propose the\nimplementation of a dedicated Coordinated Flaw Disclosure (CFD) framework\ntailored to the intricacies of machine learning and artificial intelligence\nissues. This paper delves into the historical landscape of disclosures in ML,\nencompassing the ad hoc reporting of harms and the emergence of participatory\nauditing. By juxtaposing these practices with the well-established disclosure\nnorms in cybersecurity, we argue that the broader adoption of CFD has the\npotential to enhance public trust through transparent processes that carefully\nbalance the interests of both organizations and the community.\n", "link": "http://arxiv.org/abs/2402.07039v2", "date": "2024-05-24", "relevancy": 1.7447, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4729}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.441}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coordinated%20Disclosure%20for%20AI%3A%20Beyond%20Security%20Vulnerabilities&body=Title%3A%20Coordinated%20Disclosure%20for%20AI%3A%20Beyond%20Security%20Vulnerabilities%0AAuthor%3A%20Sven%20Cattell%20and%20Avijit%20Ghosh%20and%20Lucie-Aim%C3%A9e%20Kaffee%0AAbstract%3A%20%20%20Harm%20reporting%20in%20the%20field%20of%20Artificial%20Intelligence%20%28AI%29%20currently%0Aoperates%20on%20an%20ad%20hoc%20basis%2C%20lacking%20a%20structured%20process%20for%20disclosing%20or%0Aaddressing%20algorithmic%20flaws.%20In%20contrast%2C%20the%20Coordinated%20Vulnerability%0ADisclosure%20%28CVD%29%20ethos%20and%20ecosystem%20play%20a%20pivotal%20role%20in%20software%20security%0Aand%20transparency.%20Globally%2C%20there%20are%20ongoing%20efforts%20to%20establish%20frameworks%0Athat%20promote%20transparency%20and%20collaboration%20in%20addressing%20AI-related%20issues%2C%0Athough%20challenges%20persist.%20Algorithmic%20flaws%20in%20machine%20learning%20%28ML%29%20models%0Apresent%20distinct%20challenges%20compared%20to%20traditional%20software%20vulnerabilities%2C%0Awarranting%20a%20specialized%20approach.%20To%20address%20this%20gap%2C%20we%20propose%20the%0Aimplementation%20of%20a%20dedicated%20Coordinated%20Flaw%20Disclosure%20%28CFD%29%20framework%0Atailored%20to%20the%20intricacies%20of%20machine%20learning%20and%20artificial%20intelligence%0Aissues.%20This%20paper%20delves%20into%20the%20historical%20landscape%20of%20disclosures%20in%20ML%2C%0Aencompassing%20the%20ad%20hoc%20reporting%20of%20harms%20and%20the%20emergence%20of%20participatory%0Aauditing.%20By%20juxtaposing%20these%20practices%20with%20the%20well-established%20disclosure%0Anorms%20in%20cybersecurity%2C%20we%20argue%20that%20the%20broader%20adoption%20of%20CFD%20has%20the%0Apotential%20to%20enhance%20public%20trust%20through%20transparent%20processes%20that%20carefully%0Abalance%20the%20interests%20of%20both%20organizations%20and%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoordinated%2520Disclosure%2520for%2520AI%253A%2520Beyond%2520Security%2520Vulnerabilities%26entry.906535625%3DSven%2520Cattell%2520and%2520Avijit%2520Ghosh%2520and%2520Lucie-Aim%25C3%25A9e%2520Kaffee%26entry.1292438233%3D%2520%2520Harm%2520reporting%2520in%2520the%2520field%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520currently%250Aoperates%2520on%2520an%2520ad%2520hoc%2520basis%252C%2520lacking%2520a%2520structured%2520process%2520for%2520disclosing%2520or%250Aaddressing%2520algorithmic%2520flaws.%2520In%2520contrast%252C%2520the%2520Coordinated%2520Vulnerability%250ADisclosure%2520%2528CVD%2529%2520ethos%2520and%2520ecosystem%2520play%2520a%2520pivotal%2520role%2520in%2520software%2520security%250Aand%2520transparency.%2520Globally%252C%2520there%2520are%2520ongoing%2520efforts%2520to%2520establish%2520frameworks%250Athat%2520promote%2520transparency%2520and%2520collaboration%2520in%2520addressing%2520AI-related%2520issues%252C%250Athough%2520challenges%2520persist.%2520Algorithmic%2520flaws%2520in%2520machine%2520learning%2520%2528ML%2529%2520models%250Apresent%2520distinct%2520challenges%2520compared%2520to%2520traditional%2520software%2520vulnerabilities%252C%250Awarranting%2520a%2520specialized%2520approach.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520the%250Aimplementation%2520of%2520a%2520dedicated%2520Coordinated%2520Flaw%2520Disclosure%2520%2528CFD%2529%2520framework%250Atailored%2520to%2520the%2520intricacies%2520of%2520machine%2520learning%2520and%2520artificial%2520intelligence%250Aissues.%2520This%2520paper%2520delves%2520into%2520the%2520historical%2520landscape%2520of%2520disclosures%2520in%2520ML%252C%250Aencompassing%2520the%2520ad%2520hoc%2520reporting%2520of%2520harms%2520and%2520the%2520emergence%2520of%2520participatory%250Aauditing.%2520By%2520juxtaposing%2520these%2520practices%2520with%2520the%2520well-established%2520disclosure%250Anorms%2520in%2520cybersecurity%252C%2520we%2520argue%2520that%2520the%2520broader%2520adoption%2520of%2520CFD%2520has%2520the%250Apotential%2520to%2520enhance%2520public%2520trust%2520through%2520transparent%2520processes%2520that%2520carefully%250Abalance%2520the%2520interests%2520of%2520both%2520organizations%2520and%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coordinated%20Disclosure%20for%20AI%3A%20Beyond%20Security%20Vulnerabilities&entry.906535625=Sven%20Cattell%20and%20Avijit%20Ghosh%20and%20Lucie-Aim%C3%A9e%20Kaffee&entry.1292438233=%20%20Harm%20reporting%20in%20the%20field%20of%20Artificial%20Intelligence%20%28AI%29%20currently%0Aoperates%20on%20an%20ad%20hoc%20basis%2C%20lacking%20a%20structured%20process%20for%20disclosing%20or%0Aaddressing%20algorithmic%20flaws.%20In%20contrast%2C%20the%20Coordinated%20Vulnerability%0ADisclosure%20%28CVD%29%20ethos%20and%20ecosystem%20play%20a%20pivotal%20role%20in%20software%20security%0Aand%20transparency.%20Globally%2C%20there%20are%20ongoing%20efforts%20to%20establish%20frameworks%0Athat%20promote%20transparency%20and%20collaboration%20in%20addressing%20AI-related%20issues%2C%0Athough%20challenges%20persist.%20Algorithmic%20flaws%20in%20machine%20learning%20%28ML%29%20models%0Apresent%20distinct%20challenges%20compared%20to%20traditional%20software%20vulnerabilities%2C%0Awarranting%20a%20specialized%20approach.%20To%20address%20this%20gap%2C%20we%20propose%20the%0Aimplementation%20of%20a%20dedicated%20Coordinated%20Flaw%20Disclosure%20%28CFD%29%20framework%0Atailored%20to%20the%20intricacies%20of%20machine%20learning%20and%20artificial%20intelligence%0Aissues.%20This%20paper%20delves%20into%20the%20historical%20landscape%20of%20disclosures%20in%20ML%2C%0Aencompassing%20the%20ad%20hoc%20reporting%20of%20harms%20and%20the%20emergence%20of%20participatory%0Aauditing.%20By%20juxtaposing%20these%20practices%20with%20the%20well-established%20disclosure%0Anorms%20in%20cybersecurity%2C%20we%20argue%20that%20the%20broader%20adoption%20of%20CFD%20has%20the%0Apotential%20to%20enhance%20public%20trust%20through%20transparent%20processes%20that%20carefully%0Abalance%20the%20interests%20of%20both%20organizations%20and%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07039v2&entry.124074799=Read"},
{"title": "Beyond Trend and Periodicity: Guiding Time Series Forecasting with\n  Textual Cues", "author": "Zhijian Xu and Yuxuan Bian and Jianyuan Zhong and Xiangyu Wen and Qiang Xu", "abstract": "  This work introduces a novel Text-Guided Time Series Forecasting (TGTSF)\ntask. By integrating textual cues, such as channel descriptions and dynamic\nnews, TGTSF addresses the critical limitations of traditional methods that rely\npurely on historical data. To support this task, we propose TGForecaster, a\nrobust baseline model that fuses textual cues and time series data using\ncross-attention mechanisms. We then present four meticulously curated benchmark\ndatasets to validate the proposed framework, ranging from simple periodic data\nto complex, event-driven fluctuations. Our comprehensive evaluations\ndemonstrate that TGForecaster consistently achieves state-of-the-art\nperformance, highlighting the transformative potential of incorporating textual\ninformation into time series forecasting. This work not only pioneers a novel\nforecasting task but also establishes a new benchmark for future research,\ndriving advancements in multimodal data integration for time series models.\n", "link": "http://arxiv.org/abs/2405.13522v2", "date": "2024-05-24", "relevancy": 1.9531, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4983}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4897}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Trend%20and%20Periodicity%3A%20Guiding%20Time%20Series%20Forecasting%20with%0A%20%20Textual%20Cues&body=Title%3A%20Beyond%20Trend%20and%20Periodicity%3A%20Guiding%20Time%20Series%20Forecasting%20with%0A%20%20Textual%20Cues%0AAuthor%3A%20Zhijian%20Xu%20and%20Yuxuan%20Bian%20and%20Jianyuan%20Zhong%20and%20Xiangyu%20Wen%20and%20Qiang%20Xu%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20novel%20Text-Guided%20Time%20Series%20Forecasting%20%28TGTSF%29%0Atask.%20By%20integrating%20textual%20cues%2C%20such%20as%20channel%20descriptions%20and%20dynamic%0Anews%2C%20TGTSF%20addresses%20the%20critical%20limitations%20of%20traditional%20methods%20that%20rely%0Apurely%20on%20historical%20data.%20To%20support%20this%20task%2C%20we%20propose%20TGForecaster%2C%20a%0Arobust%20baseline%20model%20that%20fuses%20textual%20cues%20and%20time%20series%20data%20using%0Across-attention%20mechanisms.%20We%20then%20present%20four%20meticulously%20curated%20benchmark%0Adatasets%20to%20validate%20the%20proposed%20framework%2C%20ranging%20from%20simple%20periodic%20data%0Ato%20complex%2C%20event-driven%20fluctuations.%20Our%20comprehensive%20evaluations%0Ademonstrate%20that%20TGForecaster%20consistently%20achieves%20state-of-the-art%0Aperformance%2C%20highlighting%20the%20transformative%20potential%20of%20incorporating%20textual%0Ainformation%20into%20time%20series%20forecasting.%20This%20work%20not%20only%20pioneers%20a%20novel%0Aforecasting%20task%20but%20also%20establishes%20a%20new%20benchmark%20for%20future%20research%2C%0Adriving%20advancements%20in%20multimodal%20data%20integration%20for%20time%20series%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Trend%2520and%2520Periodicity%253A%2520Guiding%2520Time%2520Series%2520Forecasting%2520with%250A%2520%2520Textual%2520Cues%26entry.906535625%3DZhijian%2520Xu%2520and%2520Yuxuan%2520Bian%2520and%2520Jianyuan%2520Zhong%2520and%2520Xiangyu%2520Wen%2520and%2520Qiang%2520Xu%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520novel%2520Text-Guided%2520Time%2520Series%2520Forecasting%2520%2528TGTSF%2529%250Atask.%2520By%2520integrating%2520textual%2520cues%252C%2520such%2520as%2520channel%2520descriptions%2520and%2520dynamic%250Anews%252C%2520TGTSF%2520addresses%2520the%2520critical%2520limitations%2520of%2520traditional%2520methods%2520that%2520rely%250Apurely%2520on%2520historical%2520data.%2520To%2520support%2520this%2520task%252C%2520we%2520propose%2520TGForecaster%252C%2520a%250Arobust%2520baseline%2520model%2520that%2520fuses%2520textual%2520cues%2520and%2520time%2520series%2520data%2520using%250Across-attention%2520mechanisms.%2520We%2520then%2520present%2520four%2520meticulously%2520curated%2520benchmark%250Adatasets%2520to%2520validate%2520the%2520proposed%2520framework%252C%2520ranging%2520from%2520simple%2520periodic%2520data%250Ato%2520complex%252C%2520event-driven%2520fluctuations.%2520Our%2520comprehensive%2520evaluations%250Ademonstrate%2520that%2520TGForecaster%2520consistently%2520achieves%2520state-of-the-art%250Aperformance%252C%2520highlighting%2520the%2520transformative%2520potential%2520of%2520incorporating%2520textual%250Ainformation%2520into%2520time%2520series%2520forecasting.%2520This%2520work%2520not%2520only%2520pioneers%2520a%2520novel%250Aforecasting%2520task%2520but%2520also%2520establishes%2520a%2520new%2520benchmark%2520for%2520future%2520research%252C%250Adriving%2520advancements%2520in%2520multimodal%2520data%2520integration%2520for%2520time%2520series%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Trend%20and%20Periodicity%3A%20Guiding%20Time%20Series%20Forecasting%20with%0A%20%20Textual%20Cues&entry.906535625=Zhijian%20Xu%20and%20Yuxuan%20Bian%20and%20Jianyuan%20Zhong%20and%20Xiangyu%20Wen%20and%20Qiang%20Xu&entry.1292438233=%20%20This%20work%20introduces%20a%20novel%20Text-Guided%20Time%20Series%20Forecasting%20%28TGTSF%29%0Atask.%20By%20integrating%20textual%20cues%2C%20such%20as%20channel%20descriptions%20and%20dynamic%0Anews%2C%20TGTSF%20addresses%20the%20critical%20limitations%20of%20traditional%20methods%20that%20rely%0Apurely%20on%20historical%20data.%20To%20support%20this%20task%2C%20we%20propose%20TGForecaster%2C%20a%0Arobust%20baseline%20model%20that%20fuses%20textual%20cues%20and%20time%20series%20data%20using%0Across-attention%20mechanisms.%20We%20then%20present%20four%20meticulously%20curated%20benchmark%0Adatasets%20to%20validate%20the%20proposed%20framework%2C%20ranging%20from%20simple%20periodic%20data%0Ato%20complex%2C%20event-driven%20fluctuations.%20Our%20comprehensive%20evaluations%0Ademonstrate%20that%20TGForecaster%20consistently%20achieves%20state-of-the-art%0Aperformance%2C%20highlighting%20the%20transformative%20potential%20of%20incorporating%20textual%0Ainformation%20into%20time%20series%20forecasting.%20This%20work%20not%20only%20pioneers%20a%20novel%0Aforecasting%20task%20but%20also%20establishes%20a%20new%20benchmark%20for%20future%20research%2C%0Adriving%20advancements%20in%20multimodal%20data%20integration%20for%20time%20series%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13522v2&entry.124074799=Read"},
{"title": "Federated Behavioural Planes: Explaining the Evolution of Client\n  Behaviour in Federated Learning", "author": "Dario Fenoglio and Gabriele Dominici and Pietro Barbiero and Alberto Tonda and Martin Gjoreski and Marc Langheinrich", "abstract": "  Federated Learning (FL), a privacy-aware approach in distributed deep\nlearning environments, enables many clients to collaboratively train a model\nwithout sharing sensitive data, thereby reducing privacy risks. However,\nenabling human trust and control over FL systems requires understanding the\nevolving behaviour of clients, whether beneficial or detrimental for the\ntraining, which still represents a key challenge in the current literature. To\naddress this challenge, we introduce Federated Behavioural Planes (FBPs), a\nnovel method to analyse, visualise, and explain the dynamics of FL systems,\nshowing how clients behave under two different lenses: predictive performance\n(error behavioural space) and decision-making processes (counterfactual\nbehavioural space). Our experiments demonstrate that FBPs provide informative\ntrajectories describing the evolving states of clients and their contributions\nto the global model, thereby enabling the identification of clusters of clients\nwith similar behaviours. Leveraging the patterns identified by FBPs, we propose\na robust aggregation technique named Federated Behavioural Shields to detect\nmalicious or noisy client models, thereby enhancing security and surpassing the\nefficacy of existing state-of-the-art FL defense mechanisms.\n", "link": "http://arxiv.org/abs/2405.15632v1", "date": "2024-05-24", "relevancy": 1.9188, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4972}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4881}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Behavioural%20Planes%3A%20Explaining%20the%20Evolution%20of%20Client%0A%20%20Behaviour%20in%20Federated%20Learning&body=Title%3A%20Federated%20Behavioural%20Planes%3A%20Explaining%20the%20Evolution%20of%20Client%0A%20%20Behaviour%20in%20Federated%20Learning%0AAuthor%3A%20Dario%20Fenoglio%20and%20Gabriele%20Dominici%20and%20Pietro%20Barbiero%20and%20Alberto%20Tonda%20and%20Martin%20Gjoreski%20and%20Marc%20Langheinrich%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%2C%20a%20privacy-aware%20approach%20in%20distributed%20deep%0Alearning%20environments%2C%20enables%20many%20clients%20to%20collaboratively%20train%20a%20model%0Awithout%20sharing%20sensitive%20data%2C%20thereby%20reducing%20privacy%20risks.%20However%2C%0Aenabling%20human%20trust%20and%20control%20over%20FL%20systems%20requires%20understanding%20the%0Aevolving%20behaviour%20of%20clients%2C%20whether%20beneficial%20or%20detrimental%20for%20the%0Atraining%2C%20which%20still%20represents%20a%20key%20challenge%20in%20the%20current%20literature.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20Federated%20Behavioural%20Planes%20%28FBPs%29%2C%20a%0Anovel%20method%20to%20analyse%2C%20visualise%2C%20and%20explain%20the%20dynamics%20of%20FL%20systems%2C%0Ashowing%20how%20clients%20behave%20under%20two%20different%20lenses%3A%20predictive%20performance%0A%28error%20behavioural%20space%29%20and%20decision-making%20processes%20%28counterfactual%0Abehavioural%20space%29.%20Our%20experiments%20demonstrate%20that%20FBPs%20provide%20informative%0Atrajectories%20describing%20the%20evolving%20states%20of%20clients%20and%20their%20contributions%0Ato%20the%20global%20model%2C%20thereby%20enabling%20the%20identification%20of%20clusters%20of%20clients%0Awith%20similar%20behaviours.%20Leveraging%20the%20patterns%20identified%20by%20FBPs%2C%20we%20propose%0Aa%20robust%20aggregation%20technique%20named%20Federated%20Behavioural%20Shields%20to%20detect%0Amalicious%20or%20noisy%20client%20models%2C%20thereby%20enhancing%20security%20and%20surpassing%20the%0Aefficacy%20of%20existing%20state-of-the-art%20FL%20defense%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Behavioural%2520Planes%253A%2520Explaining%2520the%2520Evolution%2520of%2520Client%250A%2520%2520Behaviour%2520in%2520Federated%2520Learning%26entry.906535625%3DDario%2520Fenoglio%2520and%2520Gabriele%2520Dominici%2520and%2520Pietro%2520Barbiero%2520and%2520Alberto%2520Tonda%2520and%2520Martin%2520Gjoreski%2520and%2520Marc%2520Langheinrich%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%252C%2520a%2520privacy-aware%2520approach%2520in%2520distributed%2520deep%250Alearning%2520environments%252C%2520enables%2520many%2520clients%2520to%2520collaboratively%2520train%2520a%2520model%250Awithout%2520sharing%2520sensitive%2520data%252C%2520thereby%2520reducing%2520privacy%2520risks.%2520However%252C%250Aenabling%2520human%2520trust%2520and%2520control%2520over%2520FL%2520systems%2520requires%2520understanding%2520the%250Aevolving%2520behaviour%2520of%2520clients%252C%2520whether%2520beneficial%2520or%2520detrimental%2520for%2520the%250Atraining%252C%2520which%2520still%2520represents%2520a%2520key%2520challenge%2520in%2520the%2520current%2520literature.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520Federated%2520Behavioural%2520Planes%2520%2528FBPs%2529%252C%2520a%250Anovel%2520method%2520to%2520analyse%252C%2520visualise%252C%2520and%2520explain%2520the%2520dynamics%2520of%2520FL%2520systems%252C%250Ashowing%2520how%2520clients%2520behave%2520under%2520two%2520different%2520lenses%253A%2520predictive%2520performance%250A%2528error%2520behavioural%2520space%2529%2520and%2520decision-making%2520processes%2520%2528counterfactual%250Abehavioural%2520space%2529.%2520Our%2520experiments%2520demonstrate%2520that%2520FBPs%2520provide%2520informative%250Atrajectories%2520describing%2520the%2520evolving%2520states%2520of%2520clients%2520and%2520their%2520contributions%250Ato%2520the%2520global%2520model%252C%2520thereby%2520enabling%2520the%2520identification%2520of%2520clusters%2520of%2520clients%250Awith%2520similar%2520behaviours.%2520Leveraging%2520the%2520patterns%2520identified%2520by%2520FBPs%252C%2520we%2520propose%250Aa%2520robust%2520aggregation%2520technique%2520named%2520Federated%2520Behavioural%2520Shields%2520to%2520detect%250Amalicious%2520or%2520noisy%2520client%2520models%252C%2520thereby%2520enhancing%2520security%2520and%2520surpassing%2520the%250Aefficacy%2520of%2520existing%2520state-of-the-art%2520FL%2520defense%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Behavioural%20Planes%3A%20Explaining%20the%20Evolution%20of%20Client%0A%20%20Behaviour%20in%20Federated%20Learning&entry.906535625=Dario%20Fenoglio%20and%20Gabriele%20Dominici%20and%20Pietro%20Barbiero%20and%20Alberto%20Tonda%20and%20Martin%20Gjoreski%20and%20Marc%20Langheinrich&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%2C%20a%20privacy-aware%20approach%20in%20distributed%20deep%0Alearning%20environments%2C%20enables%20many%20clients%20to%20collaboratively%20train%20a%20model%0Awithout%20sharing%20sensitive%20data%2C%20thereby%20reducing%20privacy%20risks.%20However%2C%0Aenabling%20human%20trust%20and%20control%20over%20FL%20systems%20requires%20understanding%20the%0Aevolving%20behaviour%20of%20clients%2C%20whether%20beneficial%20or%20detrimental%20for%20the%0Atraining%2C%20which%20still%20represents%20a%20key%20challenge%20in%20the%20current%20literature.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20Federated%20Behavioural%20Planes%20%28FBPs%29%2C%20a%0Anovel%20method%20to%20analyse%2C%20visualise%2C%20and%20explain%20the%20dynamics%20of%20FL%20systems%2C%0Ashowing%20how%20clients%20behave%20under%20two%20different%20lenses%3A%20predictive%20performance%0A%28error%20behavioural%20space%29%20and%20decision-making%20processes%20%28counterfactual%0Abehavioural%20space%29.%20Our%20experiments%20demonstrate%20that%20FBPs%20provide%20informative%0Atrajectories%20describing%20the%20evolving%20states%20of%20clients%20and%20their%20contributions%0Ato%20the%20global%20model%2C%20thereby%20enabling%20the%20identification%20of%20clusters%20of%20clients%0Awith%20similar%20behaviours.%20Leveraging%20the%20patterns%20identified%20by%20FBPs%2C%20we%20propose%0Aa%20robust%20aggregation%20technique%20named%20Federated%20Behavioural%20Shields%20to%20detect%0Amalicious%20or%20noisy%20client%20models%2C%20thereby%20enhancing%20security%20and%20surpassing%20the%0Aefficacy%20of%20existing%20state-of-the-art%20FL%20defense%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15632v1&entry.124074799=Read"},
{"title": "Class Machine Unlearning for Complex Data via Concepts Inference and\n  Data Poisoning", "author": "Wenhan Chang and Tianqing Zhu and Heng Xu and Wenjian Liu and Wanlei Zhou", "abstract": "  In current AI era, users may request AI companies to delete their data from\nthe training dataset due to the privacy concerns. As a model owner, retraining\na model will consume significant computational resources. Therefore, machine\nunlearning is a new emerged technology to allow model owner to delete requested\ntraining data or a class with little affecting on the model performance.\nHowever, for large-scaling complex data, such as image or text data, unlearning\na class from a model leads to a inferior performance due to the difficulty to\nidentify the link between classes and model. An inaccurate class deleting may\nlead to over or under unlearning. In this paper, to accurately defining the\nunlearning class of complex data, we apply the definition of Concept, rather\nthan an image feature or a token of text data, to represent the semantic\ninformation of unlearning class. This new representation can cut the link\nbetween the model and the class, leading to a complete erasing of the impact of\na class. To analyze the impact of the concept of complex data, we adopt a\nPost-hoc Concept Bottleneck Model, and Integrated Gradients to precisely\nidentify concepts across different classes. Next, we take advantage of data\npoisoning with random and targeted labels to propose unlearning methods. We\ntest our methods on both image classification models and large language models\n(LLMs). The results consistently show that the proposed methods can accurately\nerase targeted information from models and can largely maintain the performance\nof the models.\n", "link": "http://arxiv.org/abs/2405.15662v1", "date": "2024-05-24", "relevancy": 1.8326, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4775}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4594}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class%20Machine%20Unlearning%20for%20Complex%20Data%20via%20Concepts%20Inference%20and%0A%20%20Data%20Poisoning&body=Title%3A%20Class%20Machine%20Unlearning%20for%20Complex%20Data%20via%20Concepts%20Inference%20and%0A%20%20Data%20Poisoning%0AAuthor%3A%20Wenhan%20Chang%20and%20Tianqing%20Zhu%20and%20Heng%20Xu%20and%20Wenjian%20Liu%20and%20Wanlei%20Zhou%0AAbstract%3A%20%20%20In%20current%20AI%20era%2C%20users%20may%20request%20AI%20companies%20to%20delete%20their%20data%20from%0Athe%20training%20dataset%20due%20to%20the%20privacy%20concerns.%20As%20a%20model%20owner%2C%20retraining%0Aa%20model%20will%20consume%20significant%20computational%20resources.%20Therefore%2C%20machine%0Aunlearning%20is%20a%20new%20emerged%20technology%20to%20allow%20model%20owner%20to%20delete%20requested%0Atraining%20data%20or%20a%20class%20with%20little%20affecting%20on%20the%20model%20performance.%0AHowever%2C%20for%20large-scaling%20complex%20data%2C%20such%20as%20image%20or%20text%20data%2C%20unlearning%0Aa%20class%20from%20a%20model%20leads%20to%20a%20inferior%20performance%20due%20to%20the%20difficulty%20to%0Aidentify%20the%20link%20between%20classes%20and%20model.%20An%20inaccurate%20class%20deleting%20may%0Alead%20to%20over%20or%20under%20unlearning.%20In%20this%20paper%2C%20to%20accurately%20defining%20the%0Aunlearning%20class%20of%20complex%20data%2C%20we%20apply%20the%20definition%20of%20Concept%2C%20rather%0Athan%20an%20image%20feature%20or%20a%20token%20of%20text%20data%2C%20to%20represent%20the%20semantic%0Ainformation%20of%20unlearning%20class.%20This%20new%20representation%20can%20cut%20the%20link%0Abetween%20the%20model%20and%20the%20class%2C%20leading%20to%20a%20complete%20erasing%20of%20the%20impact%20of%0Aa%20class.%20To%20analyze%20the%20impact%20of%20the%20concept%20of%20complex%20data%2C%20we%20adopt%20a%0APost-hoc%20Concept%20Bottleneck%20Model%2C%20and%20Integrated%20Gradients%20to%20precisely%0Aidentify%20concepts%20across%20different%20classes.%20Next%2C%20we%20take%20advantage%20of%20data%0Apoisoning%20with%20random%20and%20targeted%20labels%20to%20propose%20unlearning%20methods.%20We%0Atest%20our%20methods%20on%20both%20image%20classification%20models%20and%20large%20language%20models%0A%28LLMs%29.%20The%20results%20consistently%20show%20that%20the%20proposed%20methods%20can%20accurately%0Aerase%20targeted%20information%20from%20models%20and%20can%20largely%20maintain%20the%20performance%0Aof%20the%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass%2520Machine%2520Unlearning%2520for%2520Complex%2520Data%2520via%2520Concepts%2520Inference%2520and%250A%2520%2520Data%2520Poisoning%26entry.906535625%3DWenhan%2520Chang%2520and%2520Tianqing%2520Zhu%2520and%2520Heng%2520Xu%2520and%2520Wenjian%2520Liu%2520and%2520Wanlei%2520Zhou%26entry.1292438233%3D%2520%2520In%2520current%2520AI%2520era%252C%2520users%2520may%2520request%2520AI%2520companies%2520to%2520delete%2520their%2520data%2520from%250Athe%2520training%2520dataset%2520due%2520to%2520the%2520privacy%2520concerns.%2520As%2520a%2520model%2520owner%252C%2520retraining%250Aa%2520model%2520will%2520consume%2520significant%2520computational%2520resources.%2520Therefore%252C%2520machine%250Aunlearning%2520is%2520a%2520new%2520emerged%2520technology%2520to%2520allow%2520model%2520owner%2520to%2520delete%2520requested%250Atraining%2520data%2520or%2520a%2520class%2520with%2520little%2520affecting%2520on%2520the%2520model%2520performance.%250AHowever%252C%2520for%2520large-scaling%2520complex%2520data%252C%2520such%2520as%2520image%2520or%2520text%2520data%252C%2520unlearning%250Aa%2520class%2520from%2520a%2520model%2520leads%2520to%2520a%2520inferior%2520performance%2520due%2520to%2520the%2520difficulty%2520to%250Aidentify%2520the%2520link%2520between%2520classes%2520and%2520model.%2520An%2520inaccurate%2520class%2520deleting%2520may%250Alead%2520to%2520over%2520or%2520under%2520unlearning.%2520In%2520this%2520paper%252C%2520to%2520accurately%2520defining%2520the%250Aunlearning%2520class%2520of%2520complex%2520data%252C%2520we%2520apply%2520the%2520definition%2520of%2520Concept%252C%2520rather%250Athan%2520an%2520image%2520feature%2520or%2520a%2520token%2520of%2520text%2520data%252C%2520to%2520represent%2520the%2520semantic%250Ainformation%2520of%2520unlearning%2520class.%2520This%2520new%2520representation%2520can%2520cut%2520the%2520link%250Abetween%2520the%2520model%2520and%2520the%2520class%252C%2520leading%2520to%2520a%2520complete%2520erasing%2520of%2520the%2520impact%2520of%250Aa%2520class.%2520To%2520analyze%2520the%2520impact%2520of%2520the%2520concept%2520of%2520complex%2520data%252C%2520we%2520adopt%2520a%250APost-hoc%2520Concept%2520Bottleneck%2520Model%252C%2520and%2520Integrated%2520Gradients%2520to%2520precisely%250Aidentify%2520concepts%2520across%2520different%2520classes.%2520Next%252C%2520we%2520take%2520advantage%2520of%2520data%250Apoisoning%2520with%2520random%2520and%2520targeted%2520labels%2520to%2520propose%2520unlearning%2520methods.%2520We%250Atest%2520our%2520methods%2520on%2520both%2520image%2520classification%2520models%2520and%2520large%2520language%2520models%250A%2528LLMs%2529.%2520The%2520results%2520consistently%2520show%2520that%2520the%2520proposed%2520methods%2520can%2520accurately%250Aerase%2520targeted%2520information%2520from%2520models%2520and%2520can%2520largely%2520maintain%2520the%2520performance%250Aof%2520the%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class%20Machine%20Unlearning%20for%20Complex%20Data%20via%20Concepts%20Inference%20and%0A%20%20Data%20Poisoning&entry.906535625=Wenhan%20Chang%20and%20Tianqing%20Zhu%20and%20Heng%20Xu%20and%20Wenjian%20Liu%20and%20Wanlei%20Zhou&entry.1292438233=%20%20In%20current%20AI%20era%2C%20users%20may%20request%20AI%20companies%20to%20delete%20their%20data%20from%0Athe%20training%20dataset%20due%20to%20the%20privacy%20concerns.%20As%20a%20model%20owner%2C%20retraining%0Aa%20model%20will%20consume%20significant%20computational%20resources.%20Therefore%2C%20machine%0Aunlearning%20is%20a%20new%20emerged%20technology%20to%20allow%20model%20owner%20to%20delete%20requested%0Atraining%20data%20or%20a%20class%20with%20little%20affecting%20on%20the%20model%20performance.%0AHowever%2C%20for%20large-scaling%20complex%20data%2C%20such%20as%20image%20or%20text%20data%2C%20unlearning%0Aa%20class%20from%20a%20model%20leads%20to%20a%20inferior%20performance%20due%20to%20the%20difficulty%20to%0Aidentify%20the%20link%20between%20classes%20and%20model.%20An%20inaccurate%20class%20deleting%20may%0Alead%20to%20over%20or%20under%20unlearning.%20In%20this%20paper%2C%20to%20accurately%20defining%20the%0Aunlearning%20class%20of%20complex%20data%2C%20we%20apply%20the%20definition%20of%20Concept%2C%20rather%0Athan%20an%20image%20feature%20or%20a%20token%20of%20text%20data%2C%20to%20represent%20the%20semantic%0Ainformation%20of%20unlearning%20class.%20This%20new%20representation%20can%20cut%20the%20link%0Abetween%20the%20model%20and%20the%20class%2C%20leading%20to%20a%20complete%20erasing%20of%20the%20impact%20of%0Aa%20class.%20To%20analyze%20the%20impact%20of%20the%20concept%20of%20complex%20data%2C%20we%20adopt%20a%0APost-hoc%20Concept%20Bottleneck%20Model%2C%20and%20Integrated%20Gradients%20to%20precisely%0Aidentify%20concepts%20across%20different%20classes.%20Next%2C%20we%20take%20advantage%20of%20data%0Apoisoning%20with%20random%20and%20targeted%20labels%20to%20propose%20unlearning%20methods.%20We%0Atest%20our%20methods%20on%20both%20image%20classification%20models%20and%20large%20language%20models%0A%28LLMs%29.%20The%20results%20consistently%20show%20that%20the%20proposed%20methods%20can%20accurately%0Aerase%20targeted%20information%20from%20models%20and%20can%20largely%20maintain%20the%20performance%0Aof%20the%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15662v1&entry.124074799=Read"},
{"title": "Gradient-Free Training of Recurrent Neural Networks using Random\n  Perturbations", "author": "Jesus Garcia Fernandez and Sander Keemink and Marcel van Gerven", "abstract": "  Recurrent neural networks (RNNs) hold immense potential for computations due\nto their Turing completeness and sequential processing capabilities, yet\nexisting methods for their training encounter efficiency challenges.\nBackpropagation through time (BPTT), the prevailing method, extends the\nbackpropagation (BP) algorithm by unrolling the RNN over time. However, this\napproach suffers from significant drawbacks, including the need to interleave\nforward and backward phases and store exact gradient information. Furthermore,\nBPTT has been shown to struggle with propagating gradient information for long\nsequences, leading to vanishing gradients. An alternative strategy to using\ngradient-based methods like BPTT involves stochastically approximating\ngradients through perturbation-based methods. This learning approach is\nexceptionally simple, necessitating only forward passes in the network and a\nglobal reinforcement signal as feedback. Despite its simplicity, the random\nnature of its updates typically leads to inefficient optimization, limiting its\neffectiveness in training neural networks. In this study, we present a new\napproach to perturbation-based learning in RNNs whose performance is\ncompetitive with BPTT, while maintaining the inherent advantages over\ngradient-based learning. To this end, we extend the recently introduced\nactivity-based node perturbation (ANP) method to operate in the time domain,\nleading to more efficient learning and generalization. Subsequently, we conduct\na range of experiments to validate our approach. Our results show similar\nperformance, convergence time and scalability when compared to BPTT, strongly\noutperforming standard node perturbation and weight perturbation methods. These\nfindings suggest that perturbation-based learning methods offer a versatile\nalternative to gradient-based methods for training RNNs which can be ideally\nsuited for neuromorphic applications\n", "link": "http://arxiv.org/abs/2405.08967v2", "date": "2024-05-24", "relevancy": 2.0078, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5171}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4952}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Free%20Training%20of%20Recurrent%20Neural%20Networks%20using%20Random%0A%20%20Perturbations&body=Title%3A%20Gradient-Free%20Training%20of%20Recurrent%20Neural%20Networks%20using%20Random%0A%20%20Perturbations%0AAuthor%3A%20Jesus%20Garcia%20Fernandez%20and%20Sander%20Keemink%20and%20Marcel%20van%20Gerven%0AAbstract%3A%20%20%20Recurrent%20neural%20networks%20%28RNNs%29%20hold%20immense%20potential%20for%20computations%20due%0Ato%20their%20Turing%20completeness%20and%20sequential%20processing%20capabilities%2C%20yet%0Aexisting%20methods%20for%20their%20training%20encounter%20efficiency%20challenges.%0ABackpropagation%20through%20time%20%28BPTT%29%2C%20the%20prevailing%20method%2C%20extends%20the%0Abackpropagation%20%28BP%29%20algorithm%20by%20unrolling%20the%20RNN%20over%20time.%20However%2C%20this%0Aapproach%20suffers%20from%20significant%20drawbacks%2C%20including%20the%20need%20to%20interleave%0Aforward%20and%20backward%20phases%20and%20store%20exact%20gradient%20information.%20Furthermore%2C%0ABPTT%20has%20been%20shown%20to%20struggle%20with%20propagating%20gradient%20information%20for%20long%0Asequences%2C%20leading%20to%20vanishing%20gradients.%20An%20alternative%20strategy%20to%20using%0Agradient-based%20methods%20like%20BPTT%20involves%20stochastically%20approximating%0Agradients%20through%20perturbation-based%20methods.%20This%20learning%20approach%20is%0Aexceptionally%20simple%2C%20necessitating%20only%20forward%20passes%20in%20the%20network%20and%20a%0Aglobal%20reinforcement%20signal%20as%20feedback.%20Despite%20its%20simplicity%2C%20the%20random%0Anature%20of%20its%20updates%20typically%20leads%20to%20inefficient%20optimization%2C%20limiting%20its%0Aeffectiveness%20in%20training%20neural%20networks.%20In%20this%20study%2C%20we%20present%20a%20new%0Aapproach%20to%20perturbation-based%20learning%20in%20RNNs%20whose%20performance%20is%0Acompetitive%20with%20BPTT%2C%20while%20maintaining%20the%20inherent%20advantages%20over%0Agradient-based%20learning.%20To%20this%20end%2C%20we%20extend%20the%20recently%20introduced%0Aactivity-based%20node%20perturbation%20%28ANP%29%20method%20to%20operate%20in%20the%20time%20domain%2C%0Aleading%20to%20more%20efficient%20learning%20and%20generalization.%20Subsequently%2C%20we%20conduct%0Aa%20range%20of%20experiments%20to%20validate%20our%20approach.%20Our%20results%20show%20similar%0Aperformance%2C%20convergence%20time%20and%20scalability%20when%20compared%20to%20BPTT%2C%20strongly%0Aoutperforming%20standard%20node%20perturbation%20and%20weight%20perturbation%20methods.%20These%0Afindings%20suggest%20that%20perturbation-based%20learning%20methods%20offer%20a%20versatile%0Aalternative%20to%20gradient-based%20methods%20for%20training%20RNNs%20which%20can%20be%20ideally%0Asuited%20for%20neuromorphic%20applications%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08967v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Free%2520Training%2520of%2520Recurrent%2520Neural%2520Networks%2520using%2520Random%250A%2520%2520Perturbations%26entry.906535625%3DJesus%2520Garcia%2520Fernandez%2520and%2520Sander%2520Keemink%2520and%2520Marcel%2520van%2520Gerven%26entry.1292438233%3D%2520%2520Recurrent%2520neural%2520networks%2520%2528RNNs%2529%2520hold%2520immense%2520potential%2520for%2520computations%2520due%250Ato%2520their%2520Turing%2520completeness%2520and%2520sequential%2520processing%2520capabilities%252C%2520yet%250Aexisting%2520methods%2520for%2520their%2520training%2520encounter%2520efficiency%2520challenges.%250ABackpropagation%2520through%2520time%2520%2528BPTT%2529%252C%2520the%2520prevailing%2520method%252C%2520extends%2520the%250Abackpropagation%2520%2528BP%2529%2520algorithm%2520by%2520unrolling%2520the%2520RNN%2520over%2520time.%2520However%252C%2520this%250Aapproach%2520suffers%2520from%2520significant%2520drawbacks%252C%2520including%2520the%2520need%2520to%2520interleave%250Aforward%2520and%2520backward%2520phases%2520and%2520store%2520exact%2520gradient%2520information.%2520Furthermore%252C%250ABPTT%2520has%2520been%2520shown%2520to%2520struggle%2520with%2520propagating%2520gradient%2520information%2520for%2520long%250Asequences%252C%2520leading%2520to%2520vanishing%2520gradients.%2520An%2520alternative%2520strategy%2520to%2520using%250Agradient-based%2520methods%2520like%2520BPTT%2520involves%2520stochastically%2520approximating%250Agradients%2520through%2520perturbation-based%2520methods.%2520This%2520learning%2520approach%2520is%250Aexceptionally%2520simple%252C%2520necessitating%2520only%2520forward%2520passes%2520in%2520the%2520network%2520and%2520a%250Aglobal%2520reinforcement%2520signal%2520as%2520feedback.%2520Despite%2520its%2520simplicity%252C%2520the%2520random%250Anature%2520of%2520its%2520updates%2520typically%2520leads%2520to%2520inefficient%2520optimization%252C%2520limiting%2520its%250Aeffectiveness%2520in%2520training%2520neural%2520networks.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520new%250Aapproach%2520to%2520perturbation-based%2520learning%2520in%2520RNNs%2520whose%2520performance%2520is%250Acompetitive%2520with%2520BPTT%252C%2520while%2520maintaining%2520the%2520inherent%2520advantages%2520over%250Agradient-based%2520learning.%2520To%2520this%2520end%252C%2520we%2520extend%2520the%2520recently%2520introduced%250Aactivity-based%2520node%2520perturbation%2520%2528ANP%2529%2520method%2520to%2520operate%2520in%2520the%2520time%2520domain%252C%250Aleading%2520to%2520more%2520efficient%2520learning%2520and%2520generalization.%2520Subsequently%252C%2520we%2520conduct%250Aa%2520range%2520of%2520experiments%2520to%2520validate%2520our%2520approach.%2520Our%2520results%2520show%2520similar%250Aperformance%252C%2520convergence%2520time%2520and%2520scalability%2520when%2520compared%2520to%2520BPTT%252C%2520strongly%250Aoutperforming%2520standard%2520node%2520perturbation%2520and%2520weight%2520perturbation%2520methods.%2520These%250Afindings%2520suggest%2520that%2520perturbation-based%2520learning%2520methods%2520offer%2520a%2520versatile%250Aalternative%2520to%2520gradient-based%2520methods%2520for%2520training%2520RNNs%2520which%2520can%2520be%2520ideally%250Asuited%2520for%2520neuromorphic%2520applications%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08967v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Free%20Training%20of%20Recurrent%20Neural%20Networks%20using%20Random%0A%20%20Perturbations&entry.906535625=Jesus%20Garcia%20Fernandez%20and%20Sander%20Keemink%20and%20Marcel%20van%20Gerven&entry.1292438233=%20%20Recurrent%20neural%20networks%20%28RNNs%29%20hold%20immense%20potential%20for%20computations%20due%0Ato%20their%20Turing%20completeness%20and%20sequential%20processing%20capabilities%2C%20yet%0Aexisting%20methods%20for%20their%20training%20encounter%20efficiency%20challenges.%0ABackpropagation%20through%20time%20%28BPTT%29%2C%20the%20prevailing%20method%2C%20extends%20the%0Abackpropagation%20%28BP%29%20algorithm%20by%20unrolling%20the%20RNN%20over%20time.%20However%2C%20this%0Aapproach%20suffers%20from%20significant%20drawbacks%2C%20including%20the%20need%20to%20interleave%0Aforward%20and%20backward%20phases%20and%20store%20exact%20gradient%20information.%20Furthermore%2C%0ABPTT%20has%20been%20shown%20to%20struggle%20with%20propagating%20gradient%20information%20for%20long%0Asequences%2C%20leading%20to%20vanishing%20gradients.%20An%20alternative%20strategy%20to%20using%0Agradient-based%20methods%20like%20BPTT%20involves%20stochastically%20approximating%0Agradients%20through%20perturbation-based%20methods.%20This%20learning%20approach%20is%0Aexceptionally%20simple%2C%20necessitating%20only%20forward%20passes%20in%20the%20network%20and%20a%0Aglobal%20reinforcement%20signal%20as%20feedback.%20Despite%20its%20simplicity%2C%20the%20random%0Anature%20of%20its%20updates%20typically%20leads%20to%20inefficient%20optimization%2C%20limiting%20its%0Aeffectiveness%20in%20training%20neural%20networks.%20In%20this%20study%2C%20we%20present%20a%20new%0Aapproach%20to%20perturbation-based%20learning%20in%20RNNs%20whose%20performance%20is%0Acompetitive%20with%20BPTT%2C%20while%20maintaining%20the%20inherent%20advantages%20over%0Agradient-based%20learning.%20To%20this%20end%2C%20we%20extend%20the%20recently%20introduced%0Aactivity-based%20node%20perturbation%20%28ANP%29%20method%20to%20operate%20in%20the%20time%20domain%2C%0Aleading%20to%20more%20efficient%20learning%20and%20generalization.%20Subsequently%2C%20we%20conduct%0Aa%20range%20of%20experiments%20to%20validate%20our%20approach.%20Our%20results%20show%20similar%0Aperformance%2C%20convergence%20time%20and%20scalability%20when%20compared%20to%20BPTT%2C%20strongly%0Aoutperforming%20standard%20node%20perturbation%20and%20weight%20perturbation%20methods.%20These%0Afindings%20suggest%20that%20perturbation-based%20learning%20methods%20offer%20a%20versatile%0Aalternative%20to%20gradient-based%20methods%20for%20training%20RNNs%20which%20can%20be%20ideally%0Asuited%20for%20neuromorphic%20applications%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08967v2&entry.124074799=Read"},
{"title": "What Do You See? Enhancing Zero-Shot Image Classification with\n  Multimodal Large Language Models", "author": "Abdelrahman Abdelhamed and Mahmoud Afifi and Alec Go", "abstract": "  Large language models (LLMs) has been effectively used for many computer\nvision tasks, including image classification. In this paper, we present a\nsimple yet effective approach for zero-shot image classification using\nmultimodal LLMs. By employing multimodal LLMs, we generate comprehensive\ntextual representations from input images. These textual representations are\nthen utilized to generate fixed-dimensional features in a cross-modal embedding\nspace. Subsequently, these features are fused together to perform zero-shot\nclassification using a linear classifier. Our method does not require prompt\nengineering for each dataset; instead, we use a single, straightforward, set of\nprompts across all datasets. We evaluated our method on several datasets, and\nour results demonstrate its remarkable effectiveness, surpassing benchmark\naccuracy on multiple datasets. On average over ten benchmarks, our method\nachieved an accuracy gain of 4.1 percentage points, with an increase of 6.8\npercentage points on the ImageNet dataset, compared to prior methods. Our\nfindings highlight the potential of multimodal LLMs to enhance computer vision\ntasks such as zero-shot image classification, offering a significant\nimprovement over traditional methods.\n", "link": "http://arxiv.org/abs/2405.15668v1", "date": "2024-05-24", "relevancy": 1.7077, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5991}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5447}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Do%20You%20See%3F%20Enhancing%20Zero-Shot%20Image%20Classification%20with%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20What%20Do%20You%20See%3F%20Enhancing%20Zero-Shot%20Image%20Classification%20with%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Abdelrahman%20Abdelhamed%20and%20Mahmoud%20Afifi%20and%20Alec%20Go%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20has%20been%20effectively%20used%20for%20many%20computer%0Avision%20tasks%2C%20including%20image%20classification.%20In%20this%20paper%2C%20we%20present%20a%0Asimple%20yet%20effective%20approach%20for%20zero-shot%20image%20classification%20using%0Amultimodal%20LLMs.%20By%20employing%20multimodal%20LLMs%2C%20we%20generate%20comprehensive%0Atextual%20representations%20from%20input%20images.%20These%20textual%20representations%20are%0Athen%20utilized%20to%20generate%20fixed-dimensional%20features%20in%20a%20cross-modal%20embedding%0Aspace.%20Subsequently%2C%20these%20features%20are%20fused%20together%20to%20perform%20zero-shot%0Aclassification%20using%20a%20linear%20classifier.%20Our%20method%20does%20not%20require%20prompt%0Aengineering%20for%20each%20dataset%3B%20instead%2C%20we%20use%20a%20single%2C%20straightforward%2C%20set%20of%0Aprompts%20across%20all%20datasets.%20We%20evaluated%20our%20method%20on%20several%20datasets%2C%20and%0Aour%20results%20demonstrate%20its%20remarkable%20effectiveness%2C%20surpassing%20benchmark%0Aaccuracy%20on%20multiple%20datasets.%20On%20average%20over%20ten%20benchmarks%2C%20our%20method%0Aachieved%20an%20accuracy%20gain%20of%204.1%20percentage%20points%2C%20with%20an%20increase%20of%206.8%0Apercentage%20points%20on%20the%20ImageNet%20dataset%2C%20compared%20to%20prior%20methods.%20Our%0Afindings%20highlight%20the%20potential%20of%20multimodal%20LLMs%20to%20enhance%20computer%20vision%0Atasks%20such%20as%20zero-shot%20image%20classification%2C%20offering%20a%20significant%0Aimprovement%20over%20traditional%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Do%2520You%2520See%253F%2520Enhancing%2520Zero-Shot%2520Image%2520Classification%2520with%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DAbdelrahman%2520Abdelhamed%2520and%2520Mahmoud%2520Afifi%2520and%2520Alec%2520Go%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520has%2520been%2520effectively%2520used%2520for%2520many%2520computer%250Avision%2520tasks%252C%2520including%2520image%2520classification.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Asimple%2520yet%2520effective%2520approach%2520for%2520zero-shot%2520image%2520classification%2520using%250Amultimodal%2520LLMs.%2520By%2520employing%2520multimodal%2520LLMs%252C%2520we%2520generate%2520comprehensive%250Atextual%2520representations%2520from%2520input%2520images.%2520These%2520textual%2520representations%2520are%250Athen%2520utilized%2520to%2520generate%2520fixed-dimensional%2520features%2520in%2520a%2520cross-modal%2520embedding%250Aspace.%2520Subsequently%252C%2520these%2520features%2520are%2520fused%2520together%2520to%2520perform%2520zero-shot%250Aclassification%2520using%2520a%2520linear%2520classifier.%2520Our%2520method%2520does%2520not%2520require%2520prompt%250Aengineering%2520for%2520each%2520dataset%253B%2520instead%252C%2520we%2520use%2520a%2520single%252C%2520straightforward%252C%2520set%2520of%250Aprompts%2520across%2520all%2520datasets.%2520We%2520evaluated%2520our%2520method%2520on%2520several%2520datasets%252C%2520and%250Aour%2520results%2520demonstrate%2520its%2520remarkable%2520effectiveness%252C%2520surpassing%2520benchmark%250Aaccuracy%2520on%2520multiple%2520datasets.%2520On%2520average%2520over%2520ten%2520benchmarks%252C%2520our%2520method%250Aachieved%2520an%2520accuracy%2520gain%2520of%25204.1%2520percentage%2520points%252C%2520with%2520an%2520increase%2520of%25206.8%250Apercentage%2520points%2520on%2520the%2520ImageNet%2520dataset%252C%2520compared%2520to%2520prior%2520methods.%2520Our%250Afindings%2520highlight%2520the%2520potential%2520of%2520multimodal%2520LLMs%2520to%2520enhance%2520computer%2520vision%250Atasks%2520such%2520as%2520zero-shot%2520image%2520classification%252C%2520offering%2520a%2520significant%250Aimprovement%2520over%2520traditional%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Do%20You%20See%3F%20Enhancing%20Zero-Shot%20Image%20Classification%20with%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Abdelrahman%20Abdelhamed%20and%20Mahmoud%20Afifi%20and%20Alec%20Go&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20has%20been%20effectively%20used%20for%20many%20computer%0Avision%20tasks%2C%20including%20image%20classification.%20In%20this%20paper%2C%20we%20present%20a%0Asimple%20yet%20effective%20approach%20for%20zero-shot%20image%20classification%20using%0Amultimodal%20LLMs.%20By%20employing%20multimodal%20LLMs%2C%20we%20generate%20comprehensive%0Atextual%20representations%20from%20input%20images.%20These%20textual%20representations%20are%0Athen%20utilized%20to%20generate%20fixed-dimensional%20features%20in%20a%20cross-modal%20embedding%0Aspace.%20Subsequently%2C%20these%20features%20are%20fused%20together%20to%20perform%20zero-shot%0Aclassification%20using%20a%20linear%20classifier.%20Our%20method%20does%20not%20require%20prompt%0Aengineering%20for%20each%20dataset%3B%20instead%2C%20we%20use%20a%20single%2C%20straightforward%2C%20set%20of%0Aprompts%20across%20all%20datasets.%20We%20evaluated%20our%20method%20on%20several%20datasets%2C%20and%0Aour%20results%20demonstrate%20its%20remarkable%20effectiveness%2C%20surpassing%20benchmark%0Aaccuracy%20on%20multiple%20datasets.%20On%20average%20over%20ten%20benchmarks%2C%20our%20method%0Aachieved%20an%20accuracy%20gain%20of%204.1%20percentage%20points%2C%20with%20an%20increase%20of%206.8%0Apercentage%20points%20on%20the%20ImageNet%20dataset%2C%20compared%20to%20prior%20methods.%20Our%0Afindings%20highlight%20the%20potential%20of%20multimodal%20LLMs%20to%20enhance%20computer%20vision%0Atasks%20such%20as%20zero-shot%20image%20classification%2C%20offering%20a%20significant%0Aimprovement%20over%20traditional%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15668v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


