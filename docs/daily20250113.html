<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250112.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity\n  Surface Reconstruction", "author": "Danpeng Chen and Hai Li and Weicai Ye and Yifan Wang and Weijian Xie and Shangjin Zhai and Nan Wang and Haomin Liu and Hujun Bao and Guofeng Zhang", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due\nto its high-quality rendering, and ultra-fast training and rendering speed.\nHowever, due to the unstructured and irregular nature of Gaussian point clouds,\nit is difficult to guarantee geometric reconstruction accuracy and multi-view\nconsistency simply by relying on image reconstruction loss. Although many\nstudies on surface reconstruction based on 3DGS have emerged recently, the\nquality of their meshes is generally unsatisfactory. To address this problem,\nwe propose a fast planar-based Gaussian splatting reconstruction representation\n(PGSR) to achieve high-fidelity surface reconstruction while ensuring\nhigh-quality rendering. Specifically, we first introduce an unbiased depth\nrendering method, which directly renders the distance from the camera origin to\nthe Gaussian plane and the corresponding normal map based on the Gaussian\ndistribution of the point cloud, and divides the two to obtain the unbiased\ndepth. We then introduce single-view geometric, multi-view photometric, and\ngeometric regularization to preserve global geometric accuracy. We also propose\na camera exposure compensation model to cope with scenes with large\nillumination variations. Experiments on indoor and outdoor scenes show that our\nmethod achieves fast training and rendering while maintaining high-fidelity\nrendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based\nmethods.\n", "link": "http://arxiv.org/abs/2406.06521v2", "date": "2025-01-10", "relevancy": 3.5472, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7747}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6881}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PGSR%3A%20Planar-based%20Gaussian%20Splatting%20for%20Efficient%20and%20High-Fidelity%0A%20%20Surface%20Reconstruction&body=Title%3A%20PGSR%3A%20Planar-based%20Gaussian%20Splatting%20for%20Efficient%20and%20High-Fidelity%0A%20%20Surface%20Reconstruction%0AAuthor%3A%20Danpeng%20Chen%20and%20Hai%20Li%20and%20Weicai%20Ye%20and%20Yifan%20Wang%20and%20Weijian%20Xie%20and%20Shangjin%20Zhai%20and%20Nan%20Wang%20and%20Haomin%20Liu%20and%20Hujun%20Bao%20and%20Guofeng%20Zhang%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20widespread%20attention%20due%0Ato%20its%20high-quality%20rendering%2C%20and%20ultra-fast%20training%20and%20rendering%20speed.%0AHowever%2C%20due%20to%20the%20unstructured%20and%20irregular%20nature%20of%20Gaussian%20point%20clouds%2C%0Ait%20is%20difficult%20to%20guarantee%20geometric%20reconstruction%20accuracy%20and%20multi-view%0Aconsistency%20simply%20by%20relying%20on%20image%20reconstruction%20loss.%20Although%20many%0Astudies%20on%20surface%20reconstruction%20based%20on%203DGS%20have%20emerged%20recently%2C%20the%0Aquality%20of%20their%20meshes%20is%20generally%20unsatisfactory.%20To%20address%20this%20problem%2C%0Awe%20propose%20a%20fast%20planar-based%20Gaussian%20splatting%20reconstruction%20representation%0A%28PGSR%29%20to%20achieve%20high-fidelity%20surface%20reconstruction%20while%20ensuring%0Ahigh-quality%20rendering.%20Specifically%2C%20we%20first%20introduce%20an%20unbiased%20depth%0Arendering%20method%2C%20which%20directly%20renders%20the%20distance%20from%20the%20camera%20origin%20to%0Athe%20Gaussian%20plane%20and%20the%20corresponding%20normal%20map%20based%20on%20the%20Gaussian%0Adistribution%20of%20the%20point%20cloud%2C%20and%20divides%20the%20two%20to%20obtain%20the%20unbiased%0Adepth.%20We%20then%20introduce%20single-view%20geometric%2C%20multi-view%20photometric%2C%20and%0Ageometric%20regularization%20to%20preserve%20global%20geometric%20accuracy.%20We%20also%20propose%0Aa%20camera%20exposure%20compensation%20model%20to%20cope%20with%20scenes%20with%20large%0Aillumination%20variations.%20Experiments%20on%20indoor%20and%20outdoor%20scenes%20show%20that%20our%0Amethod%20achieves%20fast%20training%20and%20rendering%20while%20maintaining%20high-fidelity%0Arendering%20and%20geometric%20reconstruction%2C%20outperforming%203DGS-based%20and%20NeRF-based%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPGSR%253A%2520Planar-based%2520Gaussian%2520Splatting%2520for%2520Efficient%2520and%2520High-Fidelity%250A%2520%2520Surface%2520Reconstruction%26entry.906535625%3DDanpeng%2520Chen%2520and%2520Hai%2520Li%2520and%2520Weicai%2520Ye%2520and%2520Yifan%2520Wang%2520and%2520Weijian%2520Xie%2520and%2520Shangjin%2520Zhai%2520and%2520Nan%2520Wang%2520and%2520Haomin%2520Liu%2520and%2520Hujun%2520Bao%2520and%2520Guofeng%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520attracted%2520widespread%2520attention%2520due%250Ato%2520its%2520high-quality%2520rendering%252C%2520and%2520ultra-fast%2520training%2520and%2520rendering%2520speed.%250AHowever%252C%2520due%2520to%2520the%2520unstructured%2520and%2520irregular%2520nature%2520of%2520Gaussian%2520point%2520clouds%252C%250Ait%2520is%2520difficult%2520to%2520guarantee%2520geometric%2520reconstruction%2520accuracy%2520and%2520multi-view%250Aconsistency%2520simply%2520by%2520relying%2520on%2520image%2520reconstruction%2520loss.%2520Although%2520many%250Astudies%2520on%2520surface%2520reconstruction%2520based%2520on%25203DGS%2520have%2520emerged%2520recently%252C%2520the%250Aquality%2520of%2520their%2520meshes%2520is%2520generally%2520unsatisfactory.%2520To%2520address%2520this%2520problem%252C%250Awe%2520propose%2520a%2520fast%2520planar-based%2520Gaussian%2520splatting%2520reconstruction%2520representation%250A%2528PGSR%2529%2520to%2520achieve%2520high-fidelity%2520surface%2520reconstruction%2520while%2520ensuring%250Ahigh-quality%2520rendering.%2520Specifically%252C%2520we%2520first%2520introduce%2520an%2520unbiased%2520depth%250Arendering%2520method%252C%2520which%2520directly%2520renders%2520the%2520distance%2520from%2520the%2520camera%2520origin%2520to%250Athe%2520Gaussian%2520plane%2520and%2520the%2520corresponding%2520normal%2520map%2520based%2520on%2520the%2520Gaussian%250Adistribution%2520of%2520the%2520point%2520cloud%252C%2520and%2520divides%2520the%2520two%2520to%2520obtain%2520the%2520unbiased%250Adepth.%2520We%2520then%2520introduce%2520single-view%2520geometric%252C%2520multi-view%2520photometric%252C%2520and%250Ageometric%2520regularization%2520to%2520preserve%2520global%2520geometric%2520accuracy.%2520We%2520also%2520propose%250Aa%2520camera%2520exposure%2520compensation%2520model%2520to%2520cope%2520with%2520scenes%2520with%2520large%250Aillumination%2520variations.%2520Experiments%2520on%2520indoor%2520and%2520outdoor%2520scenes%2520show%2520that%2520our%250Amethod%2520achieves%2520fast%2520training%2520and%2520rendering%2520while%2520maintaining%2520high-fidelity%250Arendering%2520and%2520geometric%2520reconstruction%252C%2520outperforming%25203DGS-based%2520and%2520NeRF-based%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PGSR%3A%20Planar-based%20Gaussian%20Splatting%20for%20Efficient%20and%20High-Fidelity%0A%20%20Surface%20Reconstruction&entry.906535625=Danpeng%20Chen%20and%20Hai%20Li%20and%20Weicai%20Ye%20and%20Yifan%20Wang%20and%20Weijian%20Xie%20and%20Shangjin%20Zhai%20and%20Nan%20Wang%20and%20Haomin%20Liu%20and%20Hujun%20Bao%20and%20Guofeng%20Zhang&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20widespread%20attention%20due%0Ato%20its%20high-quality%20rendering%2C%20and%20ultra-fast%20training%20and%20rendering%20speed.%0AHowever%2C%20due%20to%20the%20unstructured%20and%20irregular%20nature%20of%20Gaussian%20point%20clouds%2C%0Ait%20is%20difficult%20to%20guarantee%20geometric%20reconstruction%20accuracy%20and%20multi-view%0Aconsistency%20simply%20by%20relying%20on%20image%20reconstruction%20loss.%20Although%20many%0Astudies%20on%20surface%20reconstruction%20based%20on%203DGS%20have%20emerged%20recently%2C%20the%0Aquality%20of%20their%20meshes%20is%20generally%20unsatisfactory.%20To%20address%20this%20problem%2C%0Awe%20propose%20a%20fast%20planar-based%20Gaussian%20splatting%20reconstruction%20representation%0A%28PGSR%29%20to%20achieve%20high-fidelity%20surface%20reconstruction%20while%20ensuring%0Ahigh-quality%20rendering.%20Specifically%2C%20we%20first%20introduce%20an%20unbiased%20depth%0Arendering%20method%2C%20which%20directly%20renders%20the%20distance%20from%20the%20camera%20origin%20to%0Athe%20Gaussian%20plane%20and%20the%20corresponding%20normal%20map%20based%20on%20the%20Gaussian%0Adistribution%20of%20the%20point%20cloud%2C%20and%20divides%20the%20two%20to%20obtain%20the%20unbiased%0Adepth.%20We%20then%20introduce%20single-view%20geometric%2C%20multi-view%20photometric%2C%20and%0Ageometric%20regularization%20to%20preserve%20global%20geometric%20accuracy.%20We%20also%20propose%0Aa%20camera%20exposure%20compensation%20model%20to%20cope%20with%20scenes%20with%20large%0Aillumination%20variations.%20Experiments%20on%20indoor%20and%20outdoor%20scenes%20show%20that%20our%0Amethod%20achieves%20fast%20training%20and%20rendering%20while%20maintaining%20high-fidelity%0Arendering%20and%20geometric%20reconstruction%2C%20outperforming%203DGS-based%20and%20NeRF-based%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06521v2&entry.124074799=Read"},
{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "author": "Stefan Popov and Amit Raj and Michael Krainin and Yuanzhen Li and William T. Freeman and Michael Rubinstein", "abstract": "  We propose a method for generating fly-through videos of a scene, from a\nsingle image and a given camera trajectory. We build upon an image-to-video\nlatent diffusion model. We condition its UNet denoiser on the camera\ntrajectory, using four techniques. (1) We condition the UNet's temporal blocks\non raw camera extrinsics, similar to MotionCtrl. (2) We use images containing\ncamera rays and directions, similar to CameraCtrl. (3) We reproject the initial\nimage to subsequent frames and use the resulting video as a condition. (4) We\nuse 2D<=>3D transformers to introduce a global 3D representation, which\nimplicitly conditions on the camera poses. We combine all conditions in a\nContolNet-style architecture. We then propose a metric that evaluates overall\nvideo quality and the ability to preserve details with view changes, which we\nuse to analyze the trade-offs of individual and combined conditions. Finally,\nwe identify an optimal combination of conditions. We calibrate camera positions\nin our datasets for scale consistency across scenes, and we train our scene\nexploration model, CamCtrl3D, demonstrating state-of-theart results.\n", "link": "http://arxiv.org/abs/2501.06006v1", "date": "2025-01-10", "relevancy": 3.3277, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6833}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6833}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.63}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CamCtrl3D%3A%20Single-Image%20Scene%20Exploration%20with%20Precise%203D%20Camera%20Control&body=Title%3A%20CamCtrl3D%3A%20Single-Image%20Scene%20Exploration%20with%20Precise%203D%20Camera%20Control%0AAuthor%3A%20Stefan%20Popov%20and%20Amit%20Raj%20and%20Michael%20Krainin%20and%20Yuanzhen%20Li%20and%20William%20T.%20Freeman%20and%20Michael%20Rubinstein%0AAbstract%3A%20%20%20We%20propose%20a%20method%20for%20generating%20fly-through%20videos%20of%20a%20scene%2C%20from%20a%0Asingle%20image%20and%20a%20given%20camera%20trajectory.%20We%20build%20upon%20an%20image-to-video%0Alatent%20diffusion%20model.%20We%20condition%20its%20UNet%20denoiser%20on%20the%20camera%0Atrajectory%2C%20using%20four%20techniques.%20%281%29%20We%20condition%20the%20UNet%27s%20temporal%20blocks%0Aon%20raw%20camera%20extrinsics%2C%20similar%20to%20MotionCtrl.%20%282%29%20We%20use%20images%20containing%0Acamera%20rays%20and%20directions%2C%20similar%20to%20CameraCtrl.%20%283%29%20We%20reproject%20the%20initial%0Aimage%20to%20subsequent%20frames%20and%20use%20the%20resulting%20video%20as%20a%20condition.%20%284%29%20We%0Ause%202D%3C%3D%3E3D%20transformers%20to%20introduce%20a%20global%203D%20representation%2C%20which%0Aimplicitly%20conditions%20on%20the%20camera%20poses.%20We%20combine%20all%20conditions%20in%20a%0AContolNet-style%20architecture.%20We%20then%20propose%20a%20metric%20that%20evaluates%20overall%0Avideo%20quality%20and%20the%20ability%20to%20preserve%20details%20with%20view%20changes%2C%20which%20we%0Ause%20to%20analyze%20the%20trade-offs%20of%20individual%20and%20combined%20conditions.%20Finally%2C%0Awe%20identify%20an%20optimal%20combination%20of%20conditions.%20We%20calibrate%20camera%20positions%0Ain%20our%20datasets%20for%20scale%20consistency%20across%20scenes%2C%20and%20we%20train%20our%20scene%0Aexploration%20model%2C%20CamCtrl3D%2C%20demonstrating%20state-of-theart%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamCtrl3D%253A%2520Single-Image%2520Scene%2520Exploration%2520with%2520Precise%25203D%2520Camera%2520Control%26entry.906535625%3DStefan%2520Popov%2520and%2520Amit%2520Raj%2520and%2520Michael%2520Krainin%2520and%2520Yuanzhen%2520Li%2520and%2520William%2520T.%2520Freeman%2520and%2520Michael%2520Rubinstein%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520method%2520for%2520generating%2520fly-through%2520videos%2520of%2520a%2520scene%252C%2520from%2520a%250Asingle%2520image%2520and%2520a%2520given%2520camera%2520trajectory.%2520We%2520build%2520upon%2520an%2520image-to-video%250Alatent%2520diffusion%2520model.%2520We%2520condition%2520its%2520UNet%2520denoiser%2520on%2520the%2520camera%250Atrajectory%252C%2520using%2520four%2520techniques.%2520%25281%2529%2520We%2520condition%2520the%2520UNet%2527s%2520temporal%2520blocks%250Aon%2520raw%2520camera%2520extrinsics%252C%2520similar%2520to%2520MotionCtrl.%2520%25282%2529%2520We%2520use%2520images%2520containing%250Acamera%2520rays%2520and%2520directions%252C%2520similar%2520to%2520CameraCtrl.%2520%25283%2529%2520We%2520reproject%2520the%2520initial%250Aimage%2520to%2520subsequent%2520frames%2520and%2520use%2520the%2520resulting%2520video%2520as%2520a%2520condition.%2520%25284%2529%2520We%250Ause%25202D%253C%253D%253E3D%2520transformers%2520to%2520introduce%2520a%2520global%25203D%2520representation%252C%2520which%250Aimplicitly%2520conditions%2520on%2520the%2520camera%2520poses.%2520We%2520combine%2520all%2520conditions%2520in%2520a%250AContolNet-style%2520architecture.%2520We%2520then%2520propose%2520a%2520metric%2520that%2520evaluates%2520overall%250Avideo%2520quality%2520and%2520the%2520ability%2520to%2520preserve%2520details%2520with%2520view%2520changes%252C%2520which%2520we%250Ause%2520to%2520analyze%2520the%2520trade-offs%2520of%2520individual%2520and%2520combined%2520conditions.%2520Finally%252C%250Awe%2520identify%2520an%2520optimal%2520combination%2520of%2520conditions.%2520We%2520calibrate%2520camera%2520positions%250Ain%2520our%2520datasets%2520for%2520scale%2520consistency%2520across%2520scenes%252C%2520and%2520we%2520train%2520our%2520scene%250Aexploration%2520model%252C%2520CamCtrl3D%252C%2520demonstrating%2520state-of-theart%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CamCtrl3D%3A%20Single-Image%20Scene%20Exploration%20with%20Precise%203D%20Camera%20Control&entry.906535625=Stefan%20Popov%20and%20Amit%20Raj%20and%20Michael%20Krainin%20and%20Yuanzhen%20Li%20and%20William%20T.%20Freeman%20and%20Michael%20Rubinstein&entry.1292438233=%20%20We%20propose%20a%20method%20for%20generating%20fly-through%20videos%20of%20a%20scene%2C%20from%20a%0Asingle%20image%20and%20a%20given%20camera%20trajectory.%20We%20build%20upon%20an%20image-to-video%0Alatent%20diffusion%20model.%20We%20condition%20its%20UNet%20denoiser%20on%20the%20camera%0Atrajectory%2C%20using%20four%20techniques.%20%281%29%20We%20condition%20the%20UNet%27s%20temporal%20blocks%0Aon%20raw%20camera%20extrinsics%2C%20similar%20to%20MotionCtrl.%20%282%29%20We%20use%20images%20containing%0Acamera%20rays%20and%20directions%2C%20similar%20to%20CameraCtrl.%20%283%29%20We%20reproject%20the%20initial%0Aimage%20to%20subsequent%20frames%20and%20use%20the%20resulting%20video%20as%20a%20condition.%20%284%29%20We%0Ause%202D%3C%3D%3E3D%20transformers%20to%20introduce%20a%20global%203D%20representation%2C%20which%0Aimplicitly%20conditions%20on%20the%20camera%20poses.%20We%20combine%20all%20conditions%20in%20a%0AContolNet-style%20architecture.%20We%20then%20propose%20a%20metric%20that%20evaluates%20overall%0Avideo%20quality%20and%20the%20ability%20to%20preserve%20details%20with%20view%20changes%2C%20which%20we%0Ause%20to%20analyze%20the%20trade-offs%20of%20individual%20and%20combined%20conditions.%20Finally%2C%0Awe%20identify%20an%20optimal%20combination%20of%20conditions.%20We%20calibrate%20camera%20positions%0Ain%20our%20datasets%20for%20scale%20consistency%20across%20scenes%2C%20and%20we%20train%20our%20scene%0Aexploration%20model%2C%20CamCtrl3D%2C%20demonstrating%20state-of-theart%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06006v1&entry.124074799=Read"},
{"title": "Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction", "author": "Cecilia Curreli and Dominik Muhle and Abhishek Saroha and Zhenzhang Ye and Riccardo Marin and Daniel Cremers", "abstract": "  Probabilistic human motion prediction aims to forecast multiple possible\nfuture movements from past observations. While current approaches report high\ndiversity and realism, they often generate motions with undetected limb\nstretching and jitter. To address this, we introduce SkeletonDiffusion, a\nlatent diffusion model that embeds an explicit inductive bias on the human body\nwithin its architecture and training. Our model is trained with a novel\nnonisotropic Gaussian diffusion formulation that aligns with the natural\nkinematic structure of the human skeleton. Results show that our approach\noutperforms conventional isotropic alternatives, consistently generating\nrealistic predictions while avoiding artifacts such as limb distortion.\nAdditionally, we identify a limitation in commonly used diversity metrics,\nwhich may inadvertently favor models that produce inconsistent limb lengths\nwithin the same sequence. SkeletonDiffusion sets a new benchmark on three\nreal-world datasets, outperforming various baselines across multiple evaluation\nmetrics. Visit our project page:\nhttps://ceveloper.github.io/publications/skeletondiffusion/\n", "link": "http://arxiv.org/abs/2501.06035v1", "date": "2025-01-10", "relevancy": 3.1826, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7342}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5994}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonisotropic%20Gaussian%20Diffusion%20for%20Realistic%203D%20Human%20Motion%20Prediction&body=Title%3A%20Nonisotropic%20Gaussian%20Diffusion%20for%20Realistic%203D%20Human%20Motion%20Prediction%0AAuthor%3A%20Cecilia%20Curreli%20and%20Dominik%20Muhle%20and%20Abhishek%20Saroha%20and%20Zhenzhang%20Ye%20and%20Riccardo%20Marin%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Probabilistic%20human%20motion%20prediction%20aims%20to%20forecast%20multiple%20possible%0Afuture%20movements%20from%20past%20observations.%20While%20current%20approaches%20report%20high%0Adiversity%20and%20realism%2C%20they%20often%20generate%20motions%20with%20undetected%20limb%0Astretching%20and%20jitter.%20To%20address%20this%2C%20we%20introduce%20SkeletonDiffusion%2C%20a%0Alatent%20diffusion%20model%20that%20embeds%20an%20explicit%20inductive%20bias%20on%20the%20human%20body%0Awithin%20its%20architecture%20and%20training.%20Our%20model%20is%20trained%20with%20a%20novel%0Anonisotropic%20Gaussian%20diffusion%20formulation%20that%20aligns%20with%20the%20natural%0Akinematic%20structure%20of%20the%20human%20skeleton.%20Results%20show%20that%20our%20approach%0Aoutperforms%20conventional%20isotropic%20alternatives%2C%20consistently%20generating%0Arealistic%20predictions%20while%20avoiding%20artifacts%20such%20as%20limb%20distortion.%0AAdditionally%2C%20we%20identify%20a%20limitation%20in%20commonly%20used%20diversity%20metrics%2C%0Awhich%20may%20inadvertently%20favor%20models%20that%20produce%20inconsistent%20limb%20lengths%0Awithin%20the%20same%20sequence.%20SkeletonDiffusion%20sets%20a%20new%20benchmark%20on%20three%0Areal-world%20datasets%2C%20outperforming%20various%20baselines%20across%20multiple%20evaluation%0Ametrics.%20Visit%20our%20project%20page%3A%0Ahttps%3A//ceveloper.github.io/publications/skeletondiffusion/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonisotropic%2520Gaussian%2520Diffusion%2520for%2520Realistic%25203D%2520Human%2520Motion%2520Prediction%26entry.906535625%3DCecilia%2520Curreli%2520and%2520Dominik%2520Muhle%2520and%2520Abhishek%2520Saroha%2520and%2520Zhenzhang%2520Ye%2520and%2520Riccardo%2520Marin%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Probabilistic%2520human%2520motion%2520prediction%2520aims%2520to%2520forecast%2520multiple%2520possible%250Afuture%2520movements%2520from%2520past%2520observations.%2520While%2520current%2520approaches%2520report%2520high%250Adiversity%2520and%2520realism%252C%2520they%2520often%2520generate%2520motions%2520with%2520undetected%2520limb%250Astretching%2520and%2520jitter.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SkeletonDiffusion%252C%2520a%250Alatent%2520diffusion%2520model%2520that%2520embeds%2520an%2520explicit%2520inductive%2520bias%2520on%2520the%2520human%2520body%250Awithin%2520its%2520architecture%2520and%2520training.%2520Our%2520model%2520is%2520trained%2520with%2520a%2520novel%250Anonisotropic%2520Gaussian%2520diffusion%2520formulation%2520that%2520aligns%2520with%2520the%2520natural%250Akinematic%2520structure%2520of%2520the%2520human%2520skeleton.%2520Results%2520show%2520that%2520our%2520approach%250Aoutperforms%2520conventional%2520isotropic%2520alternatives%252C%2520consistently%2520generating%250Arealistic%2520predictions%2520while%2520avoiding%2520artifacts%2520such%2520as%2520limb%2520distortion.%250AAdditionally%252C%2520we%2520identify%2520a%2520limitation%2520in%2520commonly%2520used%2520diversity%2520metrics%252C%250Awhich%2520may%2520inadvertently%2520favor%2520models%2520that%2520produce%2520inconsistent%2520limb%2520lengths%250Awithin%2520the%2520same%2520sequence.%2520SkeletonDiffusion%2520sets%2520a%2520new%2520benchmark%2520on%2520three%250Areal-world%2520datasets%252C%2520outperforming%2520various%2520baselines%2520across%2520multiple%2520evaluation%250Ametrics.%2520Visit%2520our%2520project%2520page%253A%250Ahttps%253A//ceveloper.github.io/publications/skeletondiffusion/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonisotropic%20Gaussian%20Diffusion%20for%20Realistic%203D%20Human%20Motion%20Prediction&entry.906535625=Cecilia%20Curreli%20and%20Dominik%20Muhle%20and%20Abhishek%20Saroha%20and%20Zhenzhang%20Ye%20and%20Riccardo%20Marin%20and%20Daniel%20Cremers&entry.1292438233=%20%20Probabilistic%20human%20motion%20prediction%20aims%20to%20forecast%20multiple%20possible%0Afuture%20movements%20from%20past%20observations.%20While%20current%20approaches%20report%20high%0Adiversity%20and%20realism%2C%20they%20often%20generate%20motions%20with%20undetected%20limb%0Astretching%20and%20jitter.%20To%20address%20this%2C%20we%20introduce%20SkeletonDiffusion%2C%20a%0Alatent%20diffusion%20model%20that%20embeds%20an%20explicit%20inductive%20bias%20on%20the%20human%20body%0Awithin%20its%20architecture%20and%20training.%20Our%20model%20is%20trained%20with%20a%20novel%0Anonisotropic%20Gaussian%20diffusion%20formulation%20that%20aligns%20with%20the%20natural%0Akinematic%20structure%20of%20the%20human%20skeleton.%20Results%20show%20that%20our%20approach%0Aoutperforms%20conventional%20isotropic%20alternatives%2C%20consistently%20generating%0Arealistic%20predictions%20while%20avoiding%20artifacts%20such%20as%20limb%20distortion.%0AAdditionally%2C%20we%20identify%20a%20limitation%20in%20commonly%20used%20diversity%20metrics%2C%0Awhich%20may%20inadvertently%20favor%20models%20that%20produce%20inconsistent%20limb%20lengths%0Awithin%20the%20same%20sequence.%20SkeletonDiffusion%20sets%20a%20new%20benchmark%20on%20three%0Areal-world%20datasets%2C%20outperforming%20various%20baselines%20across%20multiple%20evaluation%0Ametrics.%20Visit%20our%20project%20page%3A%0Ahttps%3A//ceveloper.github.io/publications/skeletondiffusion/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06035v1&entry.124074799=Read"},
{"title": "Benchmark Evaluations, Applications, and Challenges of Large Vision\n  Language Models: A Survey", "author": "Zongxia Li and Xiyang Wu and Hongyang Du and Huy Nghiem and Guangyao Shi", "abstract": "  Multimodal Vision Language Models (VLMs) have emerged as a transformative\ntechnology at the intersection of computer vision and natural language\nprocessing, enabling machines to perceive and reason about the world through\nboth visual and textual modalities. For example, models such as CLIP, Claude,\nand GPT-4V demonstrate strong reasoning and understanding abilities on visual\nand textual data and beat classical single modality vision models on zero-shot\nclassification. Despite their rapid advancements in research and growing\npopularity in applications, a comprehensive survey of existing studies on VLMs\nis notably lacking, particularly for researchers aiming to leverage VLMs in\ntheir specific domains. To this end, we provide a systematic overview of VLMs\nin the following aspects: model information of the major VLMs developed over\nthe past five years (2019-2024); the main architectures and training methods of\nthese VLMs; summary and categorization of the popular benchmarks and evaluation\nmetrics of VLMs; the applications of VLMs including embodied agents, robotics,\nand video generation; the challenges and issues faced by current VLMs such as\nhallucination, fairness, and safety. Detailed collections including papers and\nmodel repository links are listed in\nhttps://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.\n", "link": "http://arxiv.org/abs/2501.02189v2", "date": "2025-01-10", "relevancy": 3.0092, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6214}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmark%20Evaluations%2C%20Applications%2C%20and%20Challenges%20of%20Large%20Vision%0A%20%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Benchmark%20Evaluations%2C%20Applications%2C%20and%20Challenges%20of%20Large%20Vision%0A%20%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Zongxia%20Li%20and%20Xiyang%20Wu%20and%20Hongyang%20Du%20and%20Huy%20Nghiem%20and%20Guangyao%20Shi%0AAbstract%3A%20%20%20Multimodal%20Vision%20Language%20Models%20%28VLMs%29%20have%20emerged%20as%20a%20transformative%0Atechnology%20at%20the%20intersection%20of%20computer%20vision%20and%20natural%20language%0Aprocessing%2C%20enabling%20machines%20to%20perceive%20and%20reason%20about%20the%20world%20through%0Aboth%20visual%20and%20textual%20modalities.%20For%20example%2C%20models%20such%20as%20CLIP%2C%20Claude%2C%0Aand%20GPT-4V%20demonstrate%20strong%20reasoning%20and%20understanding%20abilities%20on%20visual%0Aand%20textual%20data%20and%20beat%20classical%20single%20modality%20vision%20models%20on%20zero-shot%0Aclassification.%20Despite%20their%20rapid%20advancements%20in%20research%20and%20growing%0Apopularity%20in%20applications%2C%20a%20comprehensive%20survey%20of%20existing%20studies%20on%20VLMs%0Ais%20notably%20lacking%2C%20particularly%20for%20researchers%20aiming%20to%20leverage%20VLMs%20in%0Atheir%20specific%20domains.%20To%20this%20end%2C%20we%20provide%20a%20systematic%20overview%20of%20VLMs%0Ain%20the%20following%20aspects%3A%20model%20information%20of%20the%20major%20VLMs%20developed%20over%0Athe%20past%20five%20years%20%282019-2024%29%3B%20the%20main%20architectures%20and%20training%20methods%20of%0Athese%20VLMs%3B%20summary%20and%20categorization%20of%20the%20popular%20benchmarks%20and%20evaluation%0Ametrics%20of%20VLMs%3B%20the%20applications%20of%20VLMs%20including%20embodied%20agents%2C%20robotics%2C%0Aand%20video%20generation%3B%20the%20challenges%20and%20issues%20faced%20by%20current%20VLMs%20such%20as%0Ahallucination%2C%20fairness%2C%20and%20safety.%20Detailed%20collections%20including%20papers%20and%0Amodel%20repository%20links%20are%20listed%20in%0Ahttps%3A//github.com/zli12321/Awesome-VLM-Papers-And-Models.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmark%2520Evaluations%252C%2520Applications%252C%2520and%2520Challenges%2520of%2520Large%2520Vision%250A%2520%2520Language%2520Models%253A%2520A%2520Survey%26entry.906535625%3DZongxia%2520Li%2520and%2520Xiyang%2520Wu%2520and%2520Hongyang%2520Du%2520and%2520Huy%2520Nghiem%2520and%2520Guangyao%2520Shi%26entry.1292438233%3D%2520%2520Multimodal%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520emerged%2520as%2520a%2520transformative%250Atechnology%2520at%2520the%2520intersection%2520of%2520computer%2520vision%2520and%2520natural%2520language%250Aprocessing%252C%2520enabling%2520machines%2520to%2520perceive%2520and%2520reason%2520about%2520the%2520world%2520through%250Aboth%2520visual%2520and%2520textual%2520modalities.%2520For%2520example%252C%2520models%2520such%2520as%2520CLIP%252C%2520Claude%252C%250Aand%2520GPT-4V%2520demonstrate%2520strong%2520reasoning%2520and%2520understanding%2520abilities%2520on%2520visual%250Aand%2520textual%2520data%2520and%2520beat%2520classical%2520single%2520modality%2520vision%2520models%2520on%2520zero-shot%250Aclassification.%2520Despite%2520their%2520rapid%2520advancements%2520in%2520research%2520and%2520growing%250Apopularity%2520in%2520applications%252C%2520a%2520comprehensive%2520survey%2520of%2520existing%2520studies%2520on%2520VLMs%250Ais%2520notably%2520lacking%252C%2520particularly%2520for%2520researchers%2520aiming%2520to%2520leverage%2520VLMs%2520in%250Atheir%2520specific%2520domains.%2520To%2520this%2520end%252C%2520we%2520provide%2520a%2520systematic%2520overview%2520of%2520VLMs%250Ain%2520the%2520following%2520aspects%253A%2520model%2520information%2520of%2520the%2520major%2520VLMs%2520developed%2520over%250Athe%2520past%2520five%2520years%2520%25282019-2024%2529%253B%2520the%2520main%2520architectures%2520and%2520training%2520methods%2520of%250Athese%2520VLMs%253B%2520summary%2520and%2520categorization%2520of%2520the%2520popular%2520benchmarks%2520and%2520evaluation%250Ametrics%2520of%2520VLMs%253B%2520the%2520applications%2520of%2520VLMs%2520including%2520embodied%2520agents%252C%2520robotics%252C%250Aand%2520video%2520generation%253B%2520the%2520challenges%2520and%2520issues%2520faced%2520by%2520current%2520VLMs%2520such%2520as%250Ahallucination%252C%2520fairness%252C%2520and%2520safety.%2520Detailed%2520collections%2520including%2520papers%2520and%250Amodel%2520repository%2520links%2520are%2520listed%2520in%250Ahttps%253A//github.com/zli12321/Awesome-VLM-Papers-And-Models.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmark%20Evaluations%2C%20Applications%2C%20and%20Challenges%20of%20Large%20Vision%0A%20%20Language%20Models%3A%20A%20Survey&entry.906535625=Zongxia%20Li%20and%20Xiyang%20Wu%20and%20Hongyang%20Du%20and%20Huy%20Nghiem%20and%20Guangyao%20Shi&entry.1292438233=%20%20Multimodal%20Vision%20Language%20Models%20%28VLMs%29%20have%20emerged%20as%20a%20transformative%0Atechnology%20at%20the%20intersection%20of%20computer%20vision%20and%20natural%20language%0Aprocessing%2C%20enabling%20machines%20to%20perceive%20and%20reason%20about%20the%20world%20through%0Aboth%20visual%20and%20textual%20modalities.%20For%20example%2C%20models%20such%20as%20CLIP%2C%20Claude%2C%0Aand%20GPT-4V%20demonstrate%20strong%20reasoning%20and%20understanding%20abilities%20on%20visual%0Aand%20textual%20data%20and%20beat%20classical%20single%20modality%20vision%20models%20on%20zero-shot%0Aclassification.%20Despite%20their%20rapid%20advancements%20in%20research%20and%20growing%0Apopularity%20in%20applications%2C%20a%20comprehensive%20survey%20of%20existing%20studies%20on%20VLMs%0Ais%20notably%20lacking%2C%20particularly%20for%20researchers%20aiming%20to%20leverage%20VLMs%20in%0Atheir%20specific%20domains.%20To%20this%20end%2C%20we%20provide%20a%20systematic%20overview%20of%20VLMs%0Ain%20the%20following%20aspects%3A%20model%20information%20of%20the%20major%20VLMs%20developed%20over%0Athe%20past%20five%20years%20%282019-2024%29%3B%20the%20main%20architectures%20and%20training%20methods%20of%0Athese%20VLMs%3B%20summary%20and%20categorization%20of%20the%20popular%20benchmarks%20and%20evaluation%0Ametrics%20of%20VLMs%3B%20the%20applications%20of%20VLMs%20including%20embodied%20agents%2C%20robotics%2C%0Aand%20video%20generation%3B%20the%20challenges%20and%20issues%20faced%20by%20current%20VLMs%20such%20as%0Ahallucination%2C%20fairness%2C%20and%20safety.%20Detailed%20collections%20including%20papers%20and%0Amodel%20repository%20links%20are%20listed%20in%0Ahttps%3A//github.com/zli12321/Awesome-VLM-Papers-And-Models.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02189v2&entry.124074799=Read"},
{"title": "GeoX: Geometric Problem Solving Through Unified Formalized\n  Vision-Language Pre-training", "author": "Renqiu Xia and Mingsheng Li and Hancheng Ye and Wenjie Wu and Hongbin Zhou and Jiakang Yuan and Tianshuo Peng and Xinyu Cai and Xiangchao Yan and Bin Wang and Conghui He and Botian Shi and Tao Chen and Junchi Yan and Bo Zhang", "abstract": "  Despite their proficiency in general tasks, Multi-modal Large Language Models\n(MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands\nunderstanding diagrams, interpreting symbols, and performing complex reasoning.\nThis limitation arises from their pre-training on natural images and texts,\nalong with the lack of automated verification in the problem-solving process.\nBesides, current geometric specialists are limited by their task-specific\ndesigns, making them less effective for broader geometric problems. To this\nend, we present GeoX, a multi-modal large model focusing on geometric\nunderstanding and reasoning tasks. Given the significant differences between\ngeometric diagram-symbol and natural image-text, we introduce unimodal\npre-training to develop a diagram encoder and symbol decoder, enhancing the\nunderstanding of geometric images and corpora. Furthermore, we introduce\ngeometry-language alignment, an effective pre-training paradigm that bridges\nthe modality gap between unimodal geometric experts. We propose a\nGenerator-And-Sampler Transformer (GS-Former) to generate discriminative\nqueries and eliminate uninformative representations from unevenly distributed\ngeometric signals. Finally, GeoX benefits from visual instruction tuning,\nempowering it to take geometric images and questions as input and generate\nverifiable solutions. Experiments show that GeoX outperforms both generalists\nand geometric specialists on publicly recognized benchmarks, such as GeoQA,\nUniGeo, Geometry3K, and PGPS9k.\n", "link": "http://arxiv.org/abs/2412.11863v2", "date": "2025-01-10", "relevancy": 2.9628, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoX%3A%20Geometric%20Problem%20Solving%20Through%20Unified%20Formalized%0A%20%20Vision-Language%20Pre-training&body=Title%3A%20GeoX%3A%20Geometric%20Problem%20Solving%20Through%20Unified%20Formalized%0A%20%20Vision-Language%20Pre-training%0AAuthor%3A%20Renqiu%20Xia%20and%20Mingsheng%20Li%20and%20Hancheng%20Ye%20and%20Wenjie%20Wu%20and%20Hongbin%20Zhou%20and%20Jiakang%20Yuan%20and%20Tianshuo%20Peng%20and%20Xinyu%20Cai%20and%20Xiangchao%20Yan%20and%20Bin%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Tao%20Chen%20and%20Junchi%20Yan%20and%20Bo%20Zhang%0AAbstract%3A%20%20%20Despite%20their%20proficiency%20in%20general%20tasks%2C%20Multi-modal%20Large%20Language%20Models%0A%28MLLMs%29%20struggle%20with%20automatic%20Geometry%20Problem%20Solving%20%28GPS%29%2C%20which%20demands%0Aunderstanding%20diagrams%2C%20interpreting%20symbols%2C%20and%20performing%20complex%20reasoning.%0AThis%20limitation%20arises%20from%20their%20pre-training%20on%20natural%20images%20and%20texts%2C%0Aalong%20with%20the%20lack%20of%20automated%20verification%20in%20the%20problem-solving%20process.%0ABesides%2C%20current%20geometric%20specialists%20are%20limited%20by%20their%20task-specific%0Adesigns%2C%20making%20them%20less%20effective%20for%20broader%20geometric%20problems.%20To%20this%0Aend%2C%20we%20present%20GeoX%2C%20a%20multi-modal%20large%20model%20focusing%20on%20geometric%0Aunderstanding%20and%20reasoning%20tasks.%20Given%20the%20significant%20differences%20between%0Ageometric%20diagram-symbol%20and%20natural%20image-text%2C%20we%20introduce%20unimodal%0Apre-training%20to%20develop%20a%20diagram%20encoder%20and%20symbol%20decoder%2C%20enhancing%20the%0Aunderstanding%20of%20geometric%20images%20and%20corpora.%20Furthermore%2C%20we%20introduce%0Ageometry-language%20alignment%2C%20an%20effective%20pre-training%20paradigm%20that%20bridges%0Athe%20modality%20gap%20between%20unimodal%20geometric%20experts.%20We%20propose%20a%0AGenerator-And-Sampler%20Transformer%20%28GS-Former%29%20to%20generate%20discriminative%0Aqueries%20and%20eliminate%20uninformative%20representations%20from%20unevenly%20distributed%0Ageometric%20signals.%20Finally%2C%20GeoX%20benefits%20from%20visual%20instruction%20tuning%2C%0Aempowering%20it%20to%20take%20geometric%20images%20and%20questions%20as%20input%20and%20generate%0Averifiable%20solutions.%20Experiments%20show%20that%20GeoX%20outperforms%20both%20generalists%0Aand%20geometric%20specialists%20on%20publicly%20recognized%20benchmarks%2C%20such%20as%20GeoQA%2C%0AUniGeo%2C%20Geometry3K%2C%20and%20PGPS9k.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoX%253A%2520Geometric%2520Problem%2520Solving%2520Through%2520Unified%2520Formalized%250A%2520%2520Vision-Language%2520Pre-training%26entry.906535625%3DRenqiu%2520Xia%2520and%2520Mingsheng%2520Li%2520and%2520Hancheng%2520Ye%2520and%2520Wenjie%2520Wu%2520and%2520Hongbin%2520Zhou%2520and%2520Jiakang%2520Yuan%2520and%2520Tianshuo%2520Peng%2520and%2520Xinyu%2520Cai%2520and%2520Xiangchao%2520Yan%2520and%2520Bin%2520Wang%2520and%2520Conghui%2520He%2520and%2520Botian%2520Shi%2520and%2520Tao%2520Chen%2520and%2520Junchi%2520Yan%2520and%2520Bo%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520their%2520proficiency%2520in%2520general%2520tasks%252C%2520Multi-modal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520struggle%2520with%2520automatic%2520Geometry%2520Problem%2520Solving%2520%2528GPS%2529%252C%2520which%2520demands%250Aunderstanding%2520diagrams%252C%2520interpreting%2520symbols%252C%2520and%2520performing%2520complex%2520reasoning.%250AThis%2520limitation%2520arises%2520from%2520their%2520pre-training%2520on%2520natural%2520images%2520and%2520texts%252C%250Aalong%2520with%2520the%2520lack%2520of%2520automated%2520verification%2520in%2520the%2520problem-solving%2520process.%250ABesides%252C%2520current%2520geometric%2520specialists%2520are%2520limited%2520by%2520their%2520task-specific%250Adesigns%252C%2520making%2520them%2520less%2520effective%2520for%2520broader%2520geometric%2520problems.%2520To%2520this%250Aend%252C%2520we%2520present%2520GeoX%252C%2520a%2520multi-modal%2520large%2520model%2520focusing%2520on%2520geometric%250Aunderstanding%2520and%2520reasoning%2520tasks.%2520Given%2520the%2520significant%2520differences%2520between%250Ageometric%2520diagram-symbol%2520and%2520natural%2520image-text%252C%2520we%2520introduce%2520unimodal%250Apre-training%2520to%2520develop%2520a%2520diagram%2520encoder%2520and%2520symbol%2520decoder%252C%2520enhancing%2520the%250Aunderstanding%2520of%2520geometric%2520images%2520and%2520corpora.%2520Furthermore%252C%2520we%2520introduce%250Ageometry-language%2520alignment%252C%2520an%2520effective%2520pre-training%2520paradigm%2520that%2520bridges%250Athe%2520modality%2520gap%2520between%2520unimodal%2520geometric%2520experts.%2520We%2520propose%2520a%250AGenerator-And-Sampler%2520Transformer%2520%2528GS-Former%2529%2520to%2520generate%2520discriminative%250Aqueries%2520and%2520eliminate%2520uninformative%2520representations%2520from%2520unevenly%2520distributed%250Ageometric%2520signals.%2520Finally%252C%2520GeoX%2520benefits%2520from%2520visual%2520instruction%2520tuning%252C%250Aempowering%2520it%2520to%2520take%2520geometric%2520images%2520and%2520questions%2520as%2520input%2520and%2520generate%250Averifiable%2520solutions.%2520Experiments%2520show%2520that%2520GeoX%2520outperforms%2520both%2520generalists%250Aand%2520geometric%2520specialists%2520on%2520publicly%2520recognized%2520benchmarks%252C%2520such%2520as%2520GeoQA%252C%250AUniGeo%252C%2520Geometry3K%252C%2520and%2520PGPS9k.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoX%3A%20Geometric%20Problem%20Solving%20Through%20Unified%20Formalized%0A%20%20Vision-Language%20Pre-training&entry.906535625=Renqiu%20Xia%20and%20Mingsheng%20Li%20and%20Hancheng%20Ye%20and%20Wenjie%20Wu%20and%20Hongbin%20Zhou%20and%20Jiakang%20Yuan%20and%20Tianshuo%20Peng%20and%20Xinyu%20Cai%20and%20Xiangchao%20Yan%20and%20Bin%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Tao%20Chen%20and%20Junchi%20Yan%20and%20Bo%20Zhang&entry.1292438233=%20%20Despite%20their%20proficiency%20in%20general%20tasks%2C%20Multi-modal%20Large%20Language%20Models%0A%28MLLMs%29%20struggle%20with%20automatic%20Geometry%20Problem%20Solving%20%28GPS%29%2C%20which%20demands%0Aunderstanding%20diagrams%2C%20interpreting%20symbols%2C%20and%20performing%20complex%20reasoning.%0AThis%20limitation%20arises%20from%20their%20pre-training%20on%20natural%20images%20and%20texts%2C%0Aalong%20with%20the%20lack%20of%20automated%20verification%20in%20the%20problem-solving%20process.%0ABesides%2C%20current%20geometric%20specialists%20are%20limited%20by%20their%20task-specific%0Adesigns%2C%20making%20them%20less%20effective%20for%20broader%20geometric%20problems.%20To%20this%0Aend%2C%20we%20present%20GeoX%2C%20a%20multi-modal%20large%20model%20focusing%20on%20geometric%0Aunderstanding%20and%20reasoning%20tasks.%20Given%20the%20significant%20differences%20between%0Ageometric%20diagram-symbol%20and%20natural%20image-text%2C%20we%20introduce%20unimodal%0Apre-training%20to%20develop%20a%20diagram%20encoder%20and%20symbol%20decoder%2C%20enhancing%20the%0Aunderstanding%20of%20geometric%20images%20and%20corpora.%20Furthermore%2C%20we%20introduce%0Ageometry-language%20alignment%2C%20an%20effective%20pre-training%20paradigm%20that%20bridges%0Athe%20modality%20gap%20between%20unimodal%20geometric%20experts.%20We%20propose%20a%0AGenerator-And-Sampler%20Transformer%20%28GS-Former%29%20to%20generate%20discriminative%0Aqueries%20and%20eliminate%20uninformative%20representations%20from%20unevenly%20distributed%0Ageometric%20signals.%20Finally%2C%20GeoX%20benefits%20from%20visual%20instruction%20tuning%2C%0Aempowering%20it%20to%20take%20geometric%20images%20and%20questions%20as%20input%20and%20generate%0Averifiable%20solutions.%20Experiments%20show%20that%20GeoX%20outperforms%20both%20generalists%0Aand%20geometric%20specialists%20on%20publicly%20recognized%20benchmarks%2C%20such%20as%20GeoQA%2C%0AUniGeo%2C%20Geometry3K%2C%20and%20PGPS9k.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11863v2&entry.124074799=Read"},
{"title": "Generate, Transduct, Adapt: Iterative Transduction with VLMs", "author": "Oindrila Saha and Logan Lawrence and Grant Van Horn and Subhransu Maji", "abstract": "  Transductive zero-shot learning with vision-language models leverages\nimage-image similarities within the dataset to achieve better classification\naccuracy compared to the inductive setting. However, there is little work that\nexplores the structure of the language space in this context. We propose\nGTA-CLIP, a novel technique that incorporates supervision from language models\nfor joint transduction in language and vision spaces. Our approach is iterative\nand consists of three steps: (i) incrementally exploring the attribute space by\nquerying language models, (ii) an attribute-augmented transductive inference\nprocedure, and (iii) fine-tuning the language and vision encoders based on\ninferred labels within the dataset. Through experiments with CLIP encoders, we\ndemonstrate that GTA-CLIP, yields an average performance improvement of 8.6%\nand 3.7% across 12 datasets and 3 encoders, over CLIP and transductive CLIP\nrespectively in the zero-shot setting. We also observe similar improvements in\na few-shot setting. We present ablation studies that demonstrate the value of\neach step and visualize how the vision and language spaces evolve over\niterations driven by the transductive learning.\n", "link": "http://arxiv.org/abs/2501.06031v1", "date": "2025-01-10", "relevancy": 2.9502, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generate%2C%20Transduct%2C%20Adapt%3A%20Iterative%20Transduction%20with%20VLMs&body=Title%3A%20Generate%2C%20Transduct%2C%20Adapt%3A%20Iterative%20Transduction%20with%20VLMs%0AAuthor%3A%20Oindrila%20Saha%20and%20Logan%20Lawrence%20and%20Grant%20Van%20Horn%20and%20Subhransu%20Maji%0AAbstract%3A%20%20%20Transductive%20zero-shot%20learning%20with%20vision-language%20models%20leverages%0Aimage-image%20similarities%20within%20the%20dataset%20to%20achieve%20better%20classification%0Aaccuracy%20compared%20to%20the%20inductive%20setting.%20However%2C%20there%20is%20little%20work%20that%0Aexplores%20the%20structure%20of%20the%20language%20space%20in%20this%20context.%20We%20propose%0AGTA-CLIP%2C%20a%20novel%20technique%20that%20incorporates%20supervision%20from%20language%20models%0Afor%20joint%20transduction%20in%20language%20and%20vision%20spaces.%20Our%20approach%20is%20iterative%0Aand%20consists%20of%20three%20steps%3A%20%28i%29%20incrementally%20exploring%20the%20attribute%20space%20by%0Aquerying%20language%20models%2C%20%28ii%29%20an%20attribute-augmented%20transductive%20inference%0Aprocedure%2C%20and%20%28iii%29%20fine-tuning%20the%20language%20and%20vision%20encoders%20based%20on%0Ainferred%20labels%20within%20the%20dataset.%20Through%20experiments%20with%20CLIP%20encoders%2C%20we%0Ademonstrate%20that%20GTA-CLIP%2C%20yields%20an%20average%20performance%20improvement%20of%208.6%25%0Aand%203.7%25%20across%2012%20datasets%20and%203%20encoders%2C%20over%20CLIP%20and%20transductive%20CLIP%0Arespectively%20in%20the%20zero-shot%20setting.%20We%20also%20observe%20similar%20improvements%20in%0Aa%20few-shot%20setting.%20We%20present%20ablation%20studies%20that%20demonstrate%20the%20value%20of%0Aeach%20step%20and%20visualize%20how%20the%20vision%20and%20language%20spaces%20evolve%20over%0Aiterations%20driven%20by%20the%20transductive%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerate%252C%2520Transduct%252C%2520Adapt%253A%2520Iterative%2520Transduction%2520with%2520VLMs%26entry.906535625%3DOindrila%2520Saha%2520and%2520Logan%2520Lawrence%2520and%2520Grant%2520Van%2520Horn%2520and%2520Subhransu%2520Maji%26entry.1292438233%3D%2520%2520Transductive%2520zero-shot%2520learning%2520with%2520vision-language%2520models%2520leverages%250Aimage-image%2520similarities%2520within%2520the%2520dataset%2520to%2520achieve%2520better%2520classification%250Aaccuracy%2520compared%2520to%2520the%2520inductive%2520setting.%2520However%252C%2520there%2520is%2520little%2520work%2520that%250Aexplores%2520the%2520structure%2520of%2520the%2520language%2520space%2520in%2520this%2520context.%2520We%2520propose%250AGTA-CLIP%252C%2520a%2520novel%2520technique%2520that%2520incorporates%2520supervision%2520from%2520language%2520models%250Afor%2520joint%2520transduction%2520in%2520language%2520and%2520vision%2520spaces.%2520Our%2520approach%2520is%2520iterative%250Aand%2520consists%2520of%2520three%2520steps%253A%2520%2528i%2529%2520incrementally%2520exploring%2520the%2520attribute%2520space%2520by%250Aquerying%2520language%2520models%252C%2520%2528ii%2529%2520an%2520attribute-augmented%2520transductive%2520inference%250Aprocedure%252C%2520and%2520%2528iii%2529%2520fine-tuning%2520the%2520language%2520and%2520vision%2520encoders%2520based%2520on%250Ainferred%2520labels%2520within%2520the%2520dataset.%2520Through%2520experiments%2520with%2520CLIP%2520encoders%252C%2520we%250Ademonstrate%2520that%2520GTA-CLIP%252C%2520yields%2520an%2520average%2520performance%2520improvement%2520of%25208.6%2525%250Aand%25203.7%2525%2520across%252012%2520datasets%2520and%25203%2520encoders%252C%2520over%2520CLIP%2520and%2520transductive%2520CLIP%250Arespectively%2520in%2520the%2520zero-shot%2520setting.%2520We%2520also%2520observe%2520similar%2520improvements%2520in%250Aa%2520few-shot%2520setting.%2520We%2520present%2520ablation%2520studies%2520that%2520demonstrate%2520the%2520value%2520of%250Aeach%2520step%2520and%2520visualize%2520how%2520the%2520vision%2520and%2520language%2520spaces%2520evolve%2520over%250Aiterations%2520driven%2520by%2520the%2520transductive%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generate%2C%20Transduct%2C%20Adapt%3A%20Iterative%20Transduction%20with%20VLMs&entry.906535625=Oindrila%20Saha%20and%20Logan%20Lawrence%20and%20Grant%20Van%20Horn%20and%20Subhransu%20Maji&entry.1292438233=%20%20Transductive%20zero-shot%20learning%20with%20vision-language%20models%20leverages%0Aimage-image%20similarities%20within%20the%20dataset%20to%20achieve%20better%20classification%0Aaccuracy%20compared%20to%20the%20inductive%20setting.%20However%2C%20there%20is%20little%20work%20that%0Aexplores%20the%20structure%20of%20the%20language%20space%20in%20this%20context.%20We%20propose%0AGTA-CLIP%2C%20a%20novel%20technique%20that%20incorporates%20supervision%20from%20language%20models%0Afor%20joint%20transduction%20in%20language%20and%20vision%20spaces.%20Our%20approach%20is%20iterative%0Aand%20consists%20of%20three%20steps%3A%20%28i%29%20incrementally%20exploring%20the%20attribute%20space%20by%0Aquerying%20language%20models%2C%20%28ii%29%20an%20attribute-augmented%20transductive%20inference%0Aprocedure%2C%20and%20%28iii%29%20fine-tuning%20the%20language%20and%20vision%20encoders%20based%20on%0Ainferred%20labels%20within%20the%20dataset.%20Through%20experiments%20with%20CLIP%20encoders%2C%20we%0Ademonstrate%20that%20GTA-CLIP%2C%20yields%20an%20average%20performance%20improvement%20of%208.6%25%0Aand%203.7%25%20across%2012%20datasets%20and%203%20encoders%2C%20over%20CLIP%20and%20transductive%20CLIP%0Arespectively%20in%20the%20zero-shot%20setting.%20We%20also%20observe%20similar%20improvements%20in%0Aa%20few-shot%20setting.%20We%20present%20ablation%20studies%20that%20demonstrate%20the%20value%20of%0Aeach%20step%20and%20visualize%20how%20the%20vision%20and%20language%20spaces%20evolve%20over%0Aiterations%20driven%20by%20the%20transductive%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06031v1&entry.124074799=Read"},
{"title": "VideoChat-Flash: Hierarchical Compression for Long-Context Video\n  Modeling", "author": "Xinhao Li and Yi Wang and Jiashuo Yu and Xiangyu Zeng and Yuhan Zhu and Haian Huang and Jianfei Gao and Kunchang Li and Yinan He and Chenting Wang and Yu Qiao and Yali Wang and Limin Wang", "abstract": "  Long-context modeling is a critical capability for multimodal large language\nmodels (MLLMs), enabling them to process long-form contents with implicit\nmemorization. Despite its advances, handling extremely long videos remains\nchallenging due to the difficulty in maintaining crucial features over extended\nsequences. This paper introduces a Hierarchical visual token Compression (HiCo)\nmethod designed for high-fidelity representation and a practical context\nmodeling system VideoChat-Flash tailored for multimodal long-sequence\nprocessing. HiCo capitalizes on the redundancy of visual information in long\nvideos to compress long video context from the clip-level to the video-level,\nreducing the compute significantly while preserving essential details.\nVideoChat-Flash features a multi-stage short-to-long learning scheme, a rich\ndataset of real-world long videos named LongVid, and an upgraded\n\"Needle-In-A-video-Haystack\" (NIAH) for evaluating context capacities. In\nextensive experiments, VideoChat-Flash shows the leading performance on both\nmainstream long and short video benchmarks at the 2B and 7B model scale. It\nfirstly gets 99.1% accuracy over 10,000 frames in NIAH among open-source\nmodels.\n", "link": "http://arxiv.org/abs/2501.00574v2", "date": "2025-01-10", "relevancy": 2.9278, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5987}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoChat-Flash%3A%20Hierarchical%20Compression%20for%20Long-Context%20Video%0A%20%20Modeling&body=Title%3A%20VideoChat-Flash%3A%20Hierarchical%20Compression%20for%20Long-Context%20Video%0A%20%20Modeling%0AAuthor%3A%20Xinhao%20Li%20and%20Yi%20Wang%20and%20Jiashuo%20Yu%20and%20Xiangyu%20Zeng%20and%20Yuhan%20Zhu%20and%20Haian%20Huang%20and%20Jianfei%20Gao%20and%20Kunchang%20Li%20and%20Yinan%20He%20and%20Chenting%20Wang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Long-context%20modeling%20is%20a%20critical%20capability%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20enabling%20them%20to%20process%20long-form%20contents%20with%20implicit%0Amemorization.%20Despite%20its%20advances%2C%20handling%20extremely%20long%20videos%20remains%0Achallenging%20due%20to%20the%20difficulty%20in%20maintaining%20crucial%20features%20over%20extended%0Asequences.%20This%20paper%20introduces%20a%20Hierarchical%20visual%20token%20Compression%20%28HiCo%29%0Amethod%20designed%20for%20high-fidelity%20representation%20and%20a%20practical%20context%0Amodeling%20system%20VideoChat-Flash%20tailored%20for%20multimodal%20long-sequence%0Aprocessing.%20HiCo%20capitalizes%20on%20the%20redundancy%20of%20visual%20information%20in%20long%0Avideos%20to%20compress%20long%20video%20context%20from%20the%20clip-level%20to%20the%20video-level%2C%0Areducing%20the%20compute%20significantly%20while%20preserving%20essential%20details.%0AVideoChat-Flash%20features%20a%20multi-stage%20short-to-long%20learning%20scheme%2C%20a%20rich%0Adataset%20of%20real-world%20long%20videos%20named%20LongVid%2C%20and%20an%20upgraded%0A%22Needle-In-A-video-Haystack%22%20%28NIAH%29%20for%20evaluating%20context%20capacities.%20In%0Aextensive%20experiments%2C%20VideoChat-Flash%20shows%20the%20leading%20performance%20on%20both%0Amainstream%20long%20and%20short%20video%20benchmarks%20at%20the%202B%20and%207B%20model%20scale.%20It%0Afirstly%20gets%2099.1%25%20accuracy%20over%2010%2C000%20frames%20in%20NIAH%20among%20open-source%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00574v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoChat-Flash%253A%2520Hierarchical%2520Compression%2520for%2520Long-Context%2520Video%250A%2520%2520Modeling%26entry.906535625%3DXinhao%2520Li%2520and%2520Yi%2520Wang%2520and%2520Jiashuo%2520Yu%2520and%2520Xiangyu%2520Zeng%2520and%2520Yuhan%2520Zhu%2520and%2520Haian%2520Huang%2520and%2520Jianfei%2520Gao%2520and%2520Kunchang%2520Li%2520and%2520Yinan%2520He%2520and%2520Chenting%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Yali%2520Wang%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Long-context%2520modeling%2520is%2520a%2520critical%2520capability%2520for%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%252C%2520enabling%2520them%2520to%2520process%2520long-form%2520contents%2520with%2520implicit%250Amemorization.%2520Despite%2520its%2520advances%252C%2520handling%2520extremely%2520long%2520videos%2520remains%250Achallenging%2520due%2520to%2520the%2520difficulty%2520in%2520maintaining%2520crucial%2520features%2520over%2520extended%250Asequences.%2520This%2520paper%2520introduces%2520a%2520Hierarchical%2520visual%2520token%2520Compression%2520%2528HiCo%2529%250Amethod%2520designed%2520for%2520high-fidelity%2520representation%2520and%2520a%2520practical%2520context%250Amodeling%2520system%2520VideoChat-Flash%2520tailored%2520for%2520multimodal%2520long-sequence%250Aprocessing.%2520HiCo%2520capitalizes%2520on%2520the%2520redundancy%2520of%2520visual%2520information%2520in%2520long%250Avideos%2520to%2520compress%2520long%2520video%2520context%2520from%2520the%2520clip-level%2520to%2520the%2520video-level%252C%250Areducing%2520the%2520compute%2520significantly%2520while%2520preserving%2520essential%2520details.%250AVideoChat-Flash%2520features%2520a%2520multi-stage%2520short-to-long%2520learning%2520scheme%252C%2520a%2520rich%250Adataset%2520of%2520real-world%2520long%2520videos%2520named%2520LongVid%252C%2520and%2520an%2520upgraded%250A%2522Needle-In-A-video-Haystack%2522%2520%2528NIAH%2529%2520for%2520evaluating%2520context%2520capacities.%2520In%250Aextensive%2520experiments%252C%2520VideoChat-Flash%2520shows%2520the%2520leading%2520performance%2520on%2520both%250Amainstream%2520long%2520and%2520short%2520video%2520benchmarks%2520at%2520the%25202B%2520and%25207B%2520model%2520scale.%2520It%250Afirstly%2520gets%252099.1%2525%2520accuracy%2520over%252010%252C000%2520frames%2520in%2520NIAH%2520among%2520open-source%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00574v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoChat-Flash%3A%20Hierarchical%20Compression%20for%20Long-Context%20Video%0A%20%20Modeling&entry.906535625=Xinhao%20Li%20and%20Yi%20Wang%20and%20Jiashuo%20Yu%20and%20Xiangyu%20Zeng%20and%20Yuhan%20Zhu%20and%20Haian%20Huang%20and%20Jianfei%20Gao%20and%20Kunchang%20Li%20and%20Yinan%20He%20and%20Chenting%20Wang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang&entry.1292438233=%20%20Long-context%20modeling%20is%20a%20critical%20capability%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20enabling%20them%20to%20process%20long-form%20contents%20with%20implicit%0Amemorization.%20Despite%20its%20advances%2C%20handling%20extremely%20long%20videos%20remains%0Achallenging%20due%20to%20the%20difficulty%20in%20maintaining%20crucial%20features%20over%20extended%0Asequences.%20This%20paper%20introduces%20a%20Hierarchical%20visual%20token%20Compression%20%28HiCo%29%0Amethod%20designed%20for%20high-fidelity%20representation%20and%20a%20practical%20context%0Amodeling%20system%20VideoChat-Flash%20tailored%20for%20multimodal%20long-sequence%0Aprocessing.%20HiCo%20capitalizes%20on%20the%20redundancy%20of%20visual%20information%20in%20long%0Avideos%20to%20compress%20long%20video%20context%20from%20the%20clip-level%20to%20the%20video-level%2C%0Areducing%20the%20compute%20significantly%20while%20preserving%20essential%20details.%0AVideoChat-Flash%20features%20a%20multi-stage%20short-to-long%20learning%20scheme%2C%20a%20rich%0Adataset%20of%20real-world%20long%20videos%20named%20LongVid%2C%20and%20an%20upgraded%0A%22Needle-In-A-video-Haystack%22%20%28NIAH%29%20for%20evaluating%20context%20capacities.%20In%0Aextensive%20experiments%2C%20VideoChat-Flash%20shows%20the%20leading%20performance%20on%20both%0Amainstream%20long%20and%20short%20video%20benchmarks%20at%20the%202B%20and%207B%20model%20scale.%20It%0Afirstly%20gets%2099.1%25%20accuracy%20over%2010%2C000%20frames%20in%20NIAH%20among%20open-source%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00574v2&entry.124074799=Read"},
{"title": "Self-supervised video pretraining yields robust and more human-aligned\n  visual representations", "author": "Nikhil Parthasarathy and S. M. Ali Eslami and Jo\u00e3o Carreira and Olivier J. H\u00e9naff", "abstract": "  Humans learn powerful representations of objects and scenes by observing how\nthey evolve over time. Yet, outside of specific tasks that require explicit\ntemporal understanding, static image pretraining remains the dominant paradigm\nfor learning visual foundation models. We question this mismatch, and ask\nwhether video pretraining can yield visual representations that bear the\nhallmarks of human perception: generalisation across tasks, robustness to\nperturbations, and consistency with human judgements. To that end we propose a\nnovel procedure for curating videos, and develop a contrastive framework which\nlearns from the complex transformations therein. This simple paradigm for\ndistilling knowledge from videos, called VITO, yields general representations\nthat far outperform prior video pretraining methods on image understanding\ntasks, and image pretraining methods on video understanding tasks. Moreover,\nVITO representations are significantly more robust to natural and synthetic\ndeformations than image-, video-, and adversarially-trained ones. Finally,\nVITO's predictions are strongly aligned with human judgements, surpassing\nmodels that were specifically trained for that purpose. Together, these results\nsuggest that video pretraining could be a simple way of learning unified,\nrobust, and human-aligned representations of the visual world.\n", "link": "http://arxiv.org/abs/2210.06433v3", "date": "2025-01-10", "relevancy": 2.9211, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5876}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20video%20pretraining%20yields%20robust%20and%20more%20human-aligned%0A%20%20visual%20representations&body=Title%3A%20Self-supervised%20video%20pretraining%20yields%20robust%20and%20more%20human-aligned%0A%20%20visual%20representations%0AAuthor%3A%20Nikhil%20Parthasarathy%20and%20S.%20M.%20Ali%20Eslami%20and%20Jo%C3%A3o%20Carreira%20and%20Olivier%20J.%20H%C3%A9naff%0AAbstract%3A%20%20%20Humans%20learn%20powerful%20representations%20of%20objects%20and%20scenes%20by%20observing%20how%0Athey%20evolve%20over%20time.%20Yet%2C%20outside%20of%20specific%20tasks%20that%20require%20explicit%0Atemporal%20understanding%2C%20static%20image%20pretraining%20remains%20the%20dominant%20paradigm%0Afor%20learning%20visual%20foundation%20models.%20We%20question%20this%20mismatch%2C%20and%20ask%0Awhether%20video%20pretraining%20can%20yield%20visual%20representations%20that%20bear%20the%0Ahallmarks%20of%20human%20perception%3A%20generalisation%20across%20tasks%2C%20robustness%20to%0Aperturbations%2C%20and%20consistency%20with%20human%20judgements.%20To%20that%20end%20we%20propose%20a%0Anovel%20procedure%20for%20curating%20videos%2C%20and%20develop%20a%20contrastive%20framework%20which%0Alearns%20from%20the%20complex%20transformations%20therein.%20This%20simple%20paradigm%20for%0Adistilling%20knowledge%20from%20videos%2C%20called%20VITO%2C%20yields%20general%20representations%0Athat%20far%20outperform%20prior%20video%20pretraining%20methods%20on%20image%20understanding%0Atasks%2C%20and%20image%20pretraining%20methods%20on%20video%20understanding%20tasks.%20Moreover%2C%0AVITO%20representations%20are%20significantly%20more%20robust%20to%20natural%20and%20synthetic%0Adeformations%20than%20image-%2C%20video-%2C%20and%20adversarially-trained%20ones.%20Finally%2C%0AVITO%27s%20predictions%20are%20strongly%20aligned%20with%20human%20judgements%2C%20surpassing%0Amodels%20that%20were%20specifically%20trained%20for%20that%20purpose.%20Together%2C%20these%20results%0Asuggest%20that%20video%20pretraining%20could%20be%20a%20simple%20way%20of%20learning%20unified%2C%0Arobust%2C%20and%20human-aligned%20representations%20of%20the%20visual%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.06433v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520video%2520pretraining%2520yields%2520robust%2520and%2520more%2520human-aligned%250A%2520%2520visual%2520representations%26entry.906535625%3DNikhil%2520Parthasarathy%2520and%2520S.%2520M.%2520Ali%2520Eslami%2520and%2520Jo%25C3%25A3o%2520Carreira%2520and%2520Olivier%2520J.%2520H%25C3%25A9naff%26entry.1292438233%3D%2520%2520Humans%2520learn%2520powerful%2520representations%2520of%2520objects%2520and%2520scenes%2520by%2520observing%2520how%250Athey%2520evolve%2520over%2520time.%2520Yet%252C%2520outside%2520of%2520specific%2520tasks%2520that%2520require%2520explicit%250Atemporal%2520understanding%252C%2520static%2520image%2520pretraining%2520remains%2520the%2520dominant%2520paradigm%250Afor%2520learning%2520visual%2520foundation%2520models.%2520We%2520question%2520this%2520mismatch%252C%2520and%2520ask%250Awhether%2520video%2520pretraining%2520can%2520yield%2520visual%2520representations%2520that%2520bear%2520the%250Ahallmarks%2520of%2520human%2520perception%253A%2520generalisation%2520across%2520tasks%252C%2520robustness%2520to%250Aperturbations%252C%2520and%2520consistency%2520with%2520human%2520judgements.%2520To%2520that%2520end%2520we%2520propose%2520a%250Anovel%2520procedure%2520for%2520curating%2520videos%252C%2520and%2520develop%2520a%2520contrastive%2520framework%2520which%250Alearns%2520from%2520the%2520complex%2520transformations%2520therein.%2520This%2520simple%2520paradigm%2520for%250Adistilling%2520knowledge%2520from%2520videos%252C%2520called%2520VITO%252C%2520yields%2520general%2520representations%250Athat%2520far%2520outperform%2520prior%2520video%2520pretraining%2520methods%2520on%2520image%2520understanding%250Atasks%252C%2520and%2520image%2520pretraining%2520methods%2520on%2520video%2520understanding%2520tasks.%2520Moreover%252C%250AVITO%2520representations%2520are%2520significantly%2520more%2520robust%2520to%2520natural%2520and%2520synthetic%250Adeformations%2520than%2520image-%252C%2520video-%252C%2520and%2520adversarially-trained%2520ones.%2520Finally%252C%250AVITO%2527s%2520predictions%2520are%2520strongly%2520aligned%2520with%2520human%2520judgements%252C%2520surpassing%250Amodels%2520that%2520were%2520specifically%2520trained%2520for%2520that%2520purpose.%2520Together%252C%2520these%2520results%250Asuggest%2520that%2520video%2520pretraining%2520could%2520be%2520a%2520simple%2520way%2520of%2520learning%2520unified%252C%250Arobust%252C%2520and%2520human-aligned%2520representations%2520of%2520the%2520visual%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.06433v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20video%20pretraining%20yields%20robust%20and%20more%20human-aligned%0A%20%20visual%20representations&entry.906535625=Nikhil%20Parthasarathy%20and%20S.%20M.%20Ali%20Eslami%20and%20Jo%C3%A3o%20Carreira%20and%20Olivier%20J.%20H%C3%A9naff&entry.1292438233=%20%20Humans%20learn%20powerful%20representations%20of%20objects%20and%20scenes%20by%20observing%20how%0Athey%20evolve%20over%20time.%20Yet%2C%20outside%20of%20specific%20tasks%20that%20require%20explicit%0Atemporal%20understanding%2C%20static%20image%20pretraining%20remains%20the%20dominant%20paradigm%0Afor%20learning%20visual%20foundation%20models.%20We%20question%20this%20mismatch%2C%20and%20ask%0Awhether%20video%20pretraining%20can%20yield%20visual%20representations%20that%20bear%20the%0Ahallmarks%20of%20human%20perception%3A%20generalisation%20across%20tasks%2C%20robustness%20to%0Aperturbations%2C%20and%20consistency%20with%20human%20judgements.%20To%20that%20end%20we%20propose%20a%0Anovel%20procedure%20for%20curating%20videos%2C%20and%20develop%20a%20contrastive%20framework%20which%0Alearns%20from%20the%20complex%20transformations%20therein.%20This%20simple%20paradigm%20for%0Adistilling%20knowledge%20from%20videos%2C%20called%20VITO%2C%20yields%20general%20representations%0Athat%20far%20outperform%20prior%20video%20pretraining%20methods%20on%20image%20understanding%0Atasks%2C%20and%20image%20pretraining%20methods%20on%20video%20understanding%20tasks.%20Moreover%2C%0AVITO%20representations%20are%20significantly%20more%20robust%20to%20natural%20and%20synthetic%0Adeformations%20than%20image-%2C%20video-%2C%20and%20adversarially-trained%20ones.%20Finally%2C%0AVITO%27s%20predictions%20are%20strongly%20aligned%20with%20human%20judgements%2C%20surpassing%0Amodels%20that%20were%20specifically%20trained%20for%20that%20purpose.%20Together%2C%20these%20results%0Asuggest%20that%20video%20pretraining%20could%20be%20a%20simple%20way%20of%20learning%20unified%2C%0Arobust%2C%20and%20human-aligned%20representations%20of%20the%20visual%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.06433v3&entry.124074799=Read"},
{"title": "Valley2: Exploring Multimodal Models with Scalable Vision-Language\n  Design", "author": "Ziheng Wu and Zhenghao Chen and Ruipu Luo and Can Zhang and Yuan Gao and Zhentao He and Xian Wang and Haoran Lin and Minghui Qiu", "abstract": "  Recently, vision-language models have made remarkable progress, demonstrating\noutstanding capabilities in various tasks such as image captioning and video\nunderstanding. We introduce Valley2, a novel multimodal large language model\ndesigned to enhance performance across all domains and extend the boundaries of\npractical applications in e-commerce and short video scenarios. Notably,\nValley2 achieves state-of-the-art (SOTA) performance on e-commerce benchmarks,\nsurpassing open-source models of similar size by a large margin (79.66 vs.\n72.76). Additionally, Valley2 ranks second on the OpenCompass leaderboard among\nmodels with fewer than 10B parameters, with an impressive average score of\n67.4. The code and model weights are open-sourced at\nhttps://github.com/bytedance/Valley.\n", "link": "http://arxiv.org/abs/2501.05901v1", "date": "2025-01-10", "relevancy": 2.8991, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6024}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Valley2%3A%20Exploring%20Multimodal%20Models%20with%20Scalable%20Vision-Language%0A%20%20Design&body=Title%3A%20Valley2%3A%20Exploring%20Multimodal%20Models%20with%20Scalable%20Vision-Language%0A%20%20Design%0AAuthor%3A%20Ziheng%20Wu%20and%20Zhenghao%20Chen%20and%20Ruipu%20Luo%20and%20Can%20Zhang%20and%20Yuan%20Gao%20and%20Zhentao%20He%20and%20Xian%20Wang%20and%20Haoran%20Lin%20and%20Minghui%20Qiu%0AAbstract%3A%20%20%20Recently%2C%20vision-language%20models%20have%20made%20remarkable%20progress%2C%20demonstrating%0Aoutstanding%20capabilities%20in%20various%20tasks%20such%20as%20image%20captioning%20and%20video%0Aunderstanding.%20We%20introduce%20Valley2%2C%20a%20novel%20multimodal%20large%20language%20model%0Adesigned%20to%20enhance%20performance%20across%20all%20domains%20and%20extend%20the%20boundaries%20of%0Apractical%20applications%20in%20e-commerce%20and%20short%20video%20scenarios.%20Notably%2C%0AValley2%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20e-commerce%20benchmarks%2C%0Asurpassing%20open-source%20models%20of%20similar%20size%20by%20a%20large%20margin%20%2879.66%20vs.%0A72.76%29.%20Additionally%2C%20Valley2%20ranks%20second%20on%20the%20OpenCompass%20leaderboard%20among%0Amodels%20with%20fewer%20than%2010B%20parameters%2C%20with%20an%20impressive%20average%20score%20of%0A67.4.%20The%20code%20and%20model%20weights%20are%20open-sourced%20at%0Ahttps%3A//github.com/bytedance/Valley.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValley2%253A%2520Exploring%2520Multimodal%2520Models%2520with%2520Scalable%2520Vision-Language%250A%2520%2520Design%26entry.906535625%3DZiheng%2520Wu%2520and%2520Zhenghao%2520Chen%2520and%2520Ruipu%2520Luo%2520and%2520Can%2520Zhang%2520and%2520Yuan%2520Gao%2520and%2520Zhentao%2520He%2520and%2520Xian%2520Wang%2520and%2520Haoran%2520Lin%2520and%2520Minghui%2520Qiu%26entry.1292438233%3D%2520%2520Recently%252C%2520vision-language%2520models%2520have%2520made%2520remarkable%2520progress%252C%2520demonstrating%250Aoutstanding%2520capabilities%2520in%2520various%2520tasks%2520such%2520as%2520image%2520captioning%2520and%2520video%250Aunderstanding.%2520We%2520introduce%2520Valley2%252C%2520a%2520novel%2520multimodal%2520large%2520language%2520model%250Adesigned%2520to%2520enhance%2520performance%2520across%2520all%2520domains%2520and%2520extend%2520the%2520boundaries%2520of%250Apractical%2520applications%2520in%2520e-commerce%2520and%2520short%2520video%2520scenarios.%2520Notably%252C%250AValley2%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520e-commerce%2520benchmarks%252C%250Asurpassing%2520open-source%2520models%2520of%2520similar%2520size%2520by%2520a%2520large%2520margin%2520%252879.66%2520vs.%250A72.76%2529.%2520Additionally%252C%2520Valley2%2520ranks%2520second%2520on%2520the%2520OpenCompass%2520leaderboard%2520among%250Amodels%2520with%2520fewer%2520than%252010B%2520parameters%252C%2520with%2520an%2520impressive%2520average%2520score%2520of%250A67.4.%2520The%2520code%2520and%2520model%2520weights%2520are%2520open-sourced%2520at%250Ahttps%253A//github.com/bytedance/Valley.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Valley2%3A%20Exploring%20Multimodal%20Models%20with%20Scalable%20Vision-Language%0A%20%20Design&entry.906535625=Ziheng%20Wu%20and%20Zhenghao%20Chen%20and%20Ruipu%20Luo%20and%20Can%20Zhang%20and%20Yuan%20Gao%20and%20Zhentao%20He%20and%20Xian%20Wang%20and%20Haoran%20Lin%20and%20Minghui%20Qiu&entry.1292438233=%20%20Recently%2C%20vision-language%20models%20have%20made%20remarkable%20progress%2C%20demonstrating%0Aoutstanding%20capabilities%20in%20various%20tasks%20such%20as%20image%20captioning%20and%20video%0Aunderstanding.%20We%20introduce%20Valley2%2C%20a%20novel%20multimodal%20large%20language%20model%0Adesigned%20to%20enhance%20performance%20across%20all%20domains%20and%20extend%20the%20boundaries%20of%0Apractical%20applications%20in%20e-commerce%20and%20short%20video%20scenarios.%20Notably%2C%0AValley2%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20e-commerce%20benchmarks%2C%0Asurpassing%20open-source%20models%20of%20similar%20size%20by%20a%20large%20margin%20%2879.66%20vs.%0A72.76%29.%20Additionally%2C%20Valley2%20ranks%20second%20on%20the%20OpenCompass%20leaderboard%20among%0Amodels%20with%20fewer%20than%2010B%20parameters%2C%20with%20an%20impressive%20average%20score%20of%0A67.4.%20The%20code%20and%20model%20weights%20are%20open-sourced%20at%0Ahttps%3A//github.com/bytedance/Valley.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05901v1&entry.124074799=Read"},
{"title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs", "author": "Omkar Thawakar and Dinura Dissanayake and Ketan More and Ritesh Thawkar and Ahmed Heakl and Noor Ahsan and Yuhao Li and Mohammed Zumri and Jean Lahoud and Rao Muhammad Anwer and Hisham Cholakkal and Ivan Laptev and Mubarak Shah and Fahad Shahbaz Khan and Salman Khan", "abstract": "  Reasoning is a fundamental capability for solving complex multi-step\nproblems, particularly in visual contexts where sequential step-wise\nunderstanding is essential. Existing approaches lack a comprehensive framework\nfor evaluating visual reasoning and do not emphasize step-wise problem-solving.\nTo this end, we propose a comprehensive framework for advancing step-by-step\nvisual reasoning in large language models (LMMs) through three key\ncontributions. First, we introduce a visual reasoning benchmark specifically\ndesigned to evaluate multi-step reasoning tasks. The benchmark presents a\ndiverse set of challenges with eight different categories ranging from complex\nvisual perception to scientific reasoning with over 4k reasoning steps in\ntotal, enabling robust evaluation of LLMs' abilities to perform accurate and\ninterpretable visual reasoning across multiple steps. Second, we propose a\nnovel metric that assesses visual reasoning quality at the granularity of\nindividual steps, emphasizing both correctness and logical coherence. The\nproposed metric offers deeper insights into reasoning performance compared to\ntraditional end-task accuracy metrics. Third, we present a new multimodal\nvisual reasoning model, named LlamaV-o1, trained using a multi-step curriculum\nlearning approach, where tasks are progressively organized to facilitate\nincremental skill acquisition and problem-solving. The proposed LlamaV-o1 is\ndesigned for multi-step reasoning and learns step-by-step through a structured\ntraining paradigm. Extensive experiments show that our LlamaV-o1 outperforms\nexisting open-source models and performs favorably against close-source\nproprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an\naverage score of 67.3 with an absolute gain of 3.8\\% across six benchmarks\nwhile being 5 times faster during inference scaling. Our benchmark, model, and\ncode are publicly available.\n", "link": "http://arxiv.org/abs/2501.06186v1", "date": "2025-01-10", "relevancy": 2.8166, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5879}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LlamaV-o1%3A%20Rethinking%20Step-by-step%20Visual%20Reasoning%20in%20LLMs&body=Title%3A%20LlamaV-o1%3A%20Rethinking%20Step-by-step%20Visual%20Reasoning%20in%20LLMs%0AAuthor%3A%20Omkar%20Thawakar%20and%20Dinura%20Dissanayake%20and%20Ketan%20More%20and%20Ritesh%20Thawkar%20and%20Ahmed%20Heakl%20and%20Noor%20Ahsan%20and%20Yuhao%20Li%20and%20Mohammed%20Zumri%20and%20Jean%20Lahoud%20and%20Rao%20Muhammad%20Anwer%20and%20Hisham%20Cholakkal%20and%20Ivan%20Laptev%20and%20Mubarak%20Shah%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Reasoning%20is%20a%20fundamental%20capability%20for%20solving%20complex%20multi-step%0Aproblems%2C%20particularly%20in%20visual%20contexts%20where%20sequential%20step-wise%0Aunderstanding%20is%20essential.%20Existing%20approaches%20lack%20a%20comprehensive%20framework%0Afor%20evaluating%20visual%20reasoning%20and%20do%20not%20emphasize%20step-wise%20problem-solving.%0ATo%20this%20end%2C%20we%20propose%20a%20comprehensive%20framework%20for%20advancing%20step-by-step%0Avisual%20reasoning%20in%20large%20language%20models%20%28LMMs%29%20through%20three%20key%0Acontributions.%20First%2C%20we%20introduce%20a%20visual%20reasoning%20benchmark%20specifically%0Adesigned%20to%20evaluate%20multi-step%20reasoning%20tasks.%20The%20benchmark%20presents%20a%0Adiverse%20set%20of%20challenges%20with%20eight%20different%20categories%20ranging%20from%20complex%0Avisual%20perception%20to%20scientific%20reasoning%20with%20over%204k%20reasoning%20steps%20in%0Atotal%2C%20enabling%20robust%20evaluation%20of%20LLMs%27%20abilities%20to%20perform%20accurate%20and%0Ainterpretable%20visual%20reasoning%20across%20multiple%20steps.%20Second%2C%20we%20propose%20a%0Anovel%20metric%20that%20assesses%20visual%20reasoning%20quality%20at%20the%20granularity%20of%0Aindividual%20steps%2C%20emphasizing%20both%20correctness%20and%20logical%20coherence.%20The%0Aproposed%20metric%20offers%20deeper%20insights%20into%20reasoning%20performance%20compared%20to%0Atraditional%20end-task%20accuracy%20metrics.%20Third%2C%20we%20present%20a%20new%20multimodal%0Avisual%20reasoning%20model%2C%20named%20LlamaV-o1%2C%20trained%20using%20a%20multi-step%20curriculum%0Alearning%20approach%2C%20where%20tasks%20are%20progressively%20organized%20to%20facilitate%0Aincremental%20skill%20acquisition%20and%20problem-solving.%20The%20proposed%20LlamaV-o1%20is%0Adesigned%20for%20multi-step%20reasoning%20and%20learns%20step-by-step%20through%20a%20structured%0Atraining%20paradigm.%20Extensive%20experiments%20show%20that%20our%20LlamaV-o1%20outperforms%0Aexisting%20open-source%20models%20and%20performs%20favorably%20against%20close-source%0Aproprietary%20models.%20Compared%20to%20the%20recent%20Llava-CoT%2C%20our%20LlamaV-o1%20achieves%20an%0Aaverage%20score%20of%2067.3%20with%20an%20absolute%20gain%20of%203.8%5C%25%20across%20six%20benchmarks%0Awhile%20being%205%20times%20faster%20during%20inference%20scaling.%20Our%20benchmark%2C%20model%2C%20and%0Acode%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLlamaV-o1%253A%2520Rethinking%2520Step-by-step%2520Visual%2520Reasoning%2520in%2520LLMs%26entry.906535625%3DOmkar%2520Thawakar%2520and%2520Dinura%2520Dissanayake%2520and%2520Ketan%2520More%2520and%2520Ritesh%2520Thawkar%2520and%2520Ahmed%2520Heakl%2520and%2520Noor%2520Ahsan%2520and%2520Yuhao%2520Li%2520and%2520Mohammed%2520Zumri%2520and%2520Jean%2520Lahoud%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Hisham%2520Cholakkal%2520and%2520Ivan%2520Laptev%2520and%2520Mubarak%2520Shah%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Reasoning%2520is%2520a%2520fundamental%2520capability%2520for%2520solving%2520complex%2520multi-step%250Aproblems%252C%2520particularly%2520in%2520visual%2520contexts%2520where%2520sequential%2520step-wise%250Aunderstanding%2520is%2520essential.%2520Existing%2520approaches%2520lack%2520a%2520comprehensive%2520framework%250Afor%2520evaluating%2520visual%2520reasoning%2520and%2520do%2520not%2520emphasize%2520step-wise%2520problem-solving.%250ATo%2520this%2520end%252C%2520we%2520propose%2520a%2520comprehensive%2520framework%2520for%2520advancing%2520step-by-step%250Avisual%2520reasoning%2520in%2520large%2520language%2520models%2520%2528LMMs%2529%2520through%2520three%2520key%250Acontributions.%2520First%252C%2520we%2520introduce%2520a%2520visual%2520reasoning%2520benchmark%2520specifically%250Adesigned%2520to%2520evaluate%2520multi-step%2520reasoning%2520tasks.%2520The%2520benchmark%2520presents%2520a%250Adiverse%2520set%2520of%2520challenges%2520with%2520eight%2520different%2520categories%2520ranging%2520from%2520complex%250Avisual%2520perception%2520to%2520scientific%2520reasoning%2520with%2520over%25204k%2520reasoning%2520steps%2520in%250Atotal%252C%2520enabling%2520robust%2520evaluation%2520of%2520LLMs%2527%2520abilities%2520to%2520perform%2520accurate%2520and%250Ainterpretable%2520visual%2520reasoning%2520across%2520multiple%2520steps.%2520Second%252C%2520we%2520propose%2520a%250Anovel%2520metric%2520that%2520assesses%2520visual%2520reasoning%2520quality%2520at%2520the%2520granularity%2520of%250Aindividual%2520steps%252C%2520emphasizing%2520both%2520correctness%2520and%2520logical%2520coherence.%2520The%250Aproposed%2520metric%2520offers%2520deeper%2520insights%2520into%2520reasoning%2520performance%2520compared%2520to%250Atraditional%2520end-task%2520accuracy%2520metrics.%2520Third%252C%2520we%2520present%2520a%2520new%2520multimodal%250Avisual%2520reasoning%2520model%252C%2520named%2520LlamaV-o1%252C%2520trained%2520using%2520a%2520multi-step%2520curriculum%250Alearning%2520approach%252C%2520where%2520tasks%2520are%2520progressively%2520organized%2520to%2520facilitate%250Aincremental%2520skill%2520acquisition%2520and%2520problem-solving.%2520The%2520proposed%2520LlamaV-o1%2520is%250Adesigned%2520for%2520multi-step%2520reasoning%2520and%2520learns%2520step-by-step%2520through%2520a%2520structured%250Atraining%2520paradigm.%2520Extensive%2520experiments%2520show%2520that%2520our%2520LlamaV-o1%2520outperforms%250Aexisting%2520open-source%2520models%2520and%2520performs%2520favorably%2520against%2520close-source%250Aproprietary%2520models.%2520Compared%2520to%2520the%2520recent%2520Llava-CoT%252C%2520our%2520LlamaV-o1%2520achieves%2520an%250Aaverage%2520score%2520of%252067.3%2520with%2520an%2520absolute%2520gain%2520of%25203.8%255C%2525%2520across%2520six%2520benchmarks%250Awhile%2520being%25205%2520times%2520faster%2520during%2520inference%2520scaling.%2520Our%2520benchmark%252C%2520model%252C%2520and%250Acode%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LlamaV-o1%3A%20Rethinking%20Step-by-step%20Visual%20Reasoning%20in%20LLMs&entry.906535625=Omkar%20Thawakar%20and%20Dinura%20Dissanayake%20and%20Ketan%20More%20and%20Ritesh%20Thawkar%20and%20Ahmed%20Heakl%20and%20Noor%20Ahsan%20and%20Yuhao%20Li%20and%20Mohammed%20Zumri%20and%20Jean%20Lahoud%20and%20Rao%20Muhammad%20Anwer%20and%20Hisham%20Cholakkal%20and%20Ivan%20Laptev%20and%20Mubarak%20Shah%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan&entry.1292438233=%20%20Reasoning%20is%20a%20fundamental%20capability%20for%20solving%20complex%20multi-step%0Aproblems%2C%20particularly%20in%20visual%20contexts%20where%20sequential%20step-wise%0Aunderstanding%20is%20essential.%20Existing%20approaches%20lack%20a%20comprehensive%20framework%0Afor%20evaluating%20visual%20reasoning%20and%20do%20not%20emphasize%20step-wise%20problem-solving.%0ATo%20this%20end%2C%20we%20propose%20a%20comprehensive%20framework%20for%20advancing%20step-by-step%0Avisual%20reasoning%20in%20large%20language%20models%20%28LMMs%29%20through%20three%20key%0Acontributions.%20First%2C%20we%20introduce%20a%20visual%20reasoning%20benchmark%20specifically%0Adesigned%20to%20evaluate%20multi-step%20reasoning%20tasks.%20The%20benchmark%20presents%20a%0Adiverse%20set%20of%20challenges%20with%20eight%20different%20categories%20ranging%20from%20complex%0Avisual%20perception%20to%20scientific%20reasoning%20with%20over%204k%20reasoning%20steps%20in%0Atotal%2C%20enabling%20robust%20evaluation%20of%20LLMs%27%20abilities%20to%20perform%20accurate%20and%0Ainterpretable%20visual%20reasoning%20across%20multiple%20steps.%20Second%2C%20we%20propose%20a%0Anovel%20metric%20that%20assesses%20visual%20reasoning%20quality%20at%20the%20granularity%20of%0Aindividual%20steps%2C%20emphasizing%20both%20correctness%20and%20logical%20coherence.%20The%0Aproposed%20metric%20offers%20deeper%20insights%20into%20reasoning%20performance%20compared%20to%0Atraditional%20end-task%20accuracy%20metrics.%20Third%2C%20we%20present%20a%20new%20multimodal%0Avisual%20reasoning%20model%2C%20named%20LlamaV-o1%2C%20trained%20using%20a%20multi-step%20curriculum%0Alearning%20approach%2C%20where%20tasks%20are%20progressively%20organized%20to%20facilitate%0Aincremental%20skill%20acquisition%20and%20problem-solving.%20The%20proposed%20LlamaV-o1%20is%0Adesigned%20for%20multi-step%20reasoning%20and%20learns%20step-by-step%20through%20a%20structured%0Atraining%20paradigm.%20Extensive%20experiments%20show%20that%20our%20LlamaV-o1%20outperforms%0Aexisting%20open-source%20models%20and%20performs%20favorably%20against%20close-source%0Aproprietary%20models.%20Compared%20to%20the%20recent%20Llava-CoT%2C%20our%20LlamaV-o1%20achieves%20an%0Aaverage%20score%20of%2067.3%20with%20an%20absolute%20gain%20of%203.8%5C%25%20across%20six%20benchmarks%0Awhile%20being%205%20times%20faster%20during%20inference%20scaling.%20Our%20benchmark%2C%20model%2C%20and%0Acode%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06186v1&entry.124074799=Read"},
{"title": "Swin-X2S: Reconstructing 3D Shape from 2D Biplanar X-ray with Swin\n  Transformers", "author": "Kuan Liu and Zongyuan Ying and Jie Jin and Dongyan Li and Ping Huang and Wenjian Wu and Zhe Chen and Jin Qi and Yong Lu and Lianfu Deng and Bo Chen", "abstract": "  The conversion from 2D X-ray to 3D shape holds significant potential for\nimproving diagnostic efficiency and safety. However, existing reconstruction\nmethods often rely on hand-crafted features, manual intervention, and prior\nknowledge, resulting in unstable shape errors and additional processing costs.\nIn this paper, we introduce Swin-X2S, an end-to-end deep learning method for\ndirectly reconstructing 3D segmentation and labeling from 2D biplanar\northogonal X-ray images. Swin-X2S employs an encoder-decoder architecture: the\nencoder leverages 2D Swin Transformer for X-ray information extraction, while\nthe decoder employs 3D convolution with cross-attention to integrate structural\nfeatures from orthogonal views. A dimension-expanding module is introduced to\nbridge the encoder and decoder, ensuring a smooth conversion from 2D pixels to\n3D voxels. We evaluate proposed method through extensive qualitative and\nquantitative experiments across nine publicly available datasets covering four\nanatomies (femur, hip, spine, and rib), with a total of 54 categories.\nSignificant improvements over previous methods have been observed not only in\nthe segmentation and labeling metrics but also in the clinically relevant\nparameters that are of primary concern in practical applications, which\ndemonstrates the promise of Swin-X2S to provide an effective option for\nanatomical shape reconstruction in clinical scenarios. Code implementation is\navailable at: \\url{https://github.com/liukuan5625/Swin-X2S}.\n", "link": "http://arxiv.org/abs/2501.05961v1", "date": "2025-01-10", "relevancy": 2.8069, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5807}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5741}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Swin-X2S%3A%20Reconstructing%203D%20Shape%20from%202D%20Biplanar%20X-ray%20with%20Swin%0A%20%20Transformers&body=Title%3A%20Swin-X2S%3A%20Reconstructing%203D%20Shape%20from%202D%20Biplanar%20X-ray%20with%20Swin%0A%20%20Transformers%0AAuthor%3A%20Kuan%20Liu%20and%20Zongyuan%20Ying%20and%20Jie%20Jin%20and%20Dongyan%20Li%20and%20Ping%20Huang%20and%20Wenjian%20Wu%20and%20Zhe%20Chen%20and%20Jin%20Qi%20and%20Yong%20Lu%20and%20Lianfu%20Deng%20and%20Bo%20Chen%0AAbstract%3A%20%20%20The%20conversion%20from%202D%20X-ray%20to%203D%20shape%20holds%20significant%20potential%20for%0Aimproving%20diagnostic%20efficiency%20and%20safety.%20However%2C%20existing%20reconstruction%0Amethods%20often%20rely%20on%20hand-crafted%20features%2C%20manual%20intervention%2C%20and%20prior%0Aknowledge%2C%20resulting%20in%20unstable%20shape%20errors%20and%20additional%20processing%20costs.%0AIn%20this%20paper%2C%20we%20introduce%20Swin-X2S%2C%20an%20end-to-end%20deep%20learning%20method%20for%0Adirectly%20reconstructing%203D%20segmentation%20and%20labeling%20from%202D%20biplanar%0Aorthogonal%20X-ray%20images.%20Swin-X2S%20employs%20an%20encoder-decoder%20architecture%3A%20the%0Aencoder%20leverages%202D%20Swin%20Transformer%20for%20X-ray%20information%20extraction%2C%20while%0Athe%20decoder%20employs%203D%20convolution%20with%20cross-attention%20to%20integrate%20structural%0Afeatures%20from%20orthogonal%20views.%20A%20dimension-expanding%20module%20is%20introduced%20to%0Abridge%20the%20encoder%20and%20decoder%2C%20ensuring%20a%20smooth%20conversion%20from%202D%20pixels%20to%0A3D%20voxels.%20We%20evaluate%20proposed%20method%20through%20extensive%20qualitative%20and%0Aquantitative%20experiments%20across%20nine%20publicly%20available%20datasets%20covering%20four%0Aanatomies%20%28femur%2C%20hip%2C%20spine%2C%20and%20rib%29%2C%20with%20a%20total%20of%2054%20categories.%0ASignificant%20improvements%20over%20previous%20methods%20have%20been%20observed%20not%20only%20in%0Athe%20segmentation%20and%20labeling%20metrics%20but%20also%20in%20the%20clinically%20relevant%0Aparameters%20that%20are%20of%20primary%20concern%20in%20practical%20applications%2C%20which%0Ademonstrates%20the%20promise%20of%20Swin-X2S%20to%20provide%20an%20effective%20option%20for%0Aanatomical%20shape%20reconstruction%20in%20clinical%20scenarios.%20Code%20implementation%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/liukuan5625/Swin-X2S%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwin-X2S%253A%2520Reconstructing%25203D%2520Shape%2520from%25202D%2520Biplanar%2520X-ray%2520with%2520Swin%250A%2520%2520Transformers%26entry.906535625%3DKuan%2520Liu%2520and%2520Zongyuan%2520Ying%2520and%2520Jie%2520Jin%2520and%2520Dongyan%2520Li%2520and%2520Ping%2520Huang%2520and%2520Wenjian%2520Wu%2520and%2520Zhe%2520Chen%2520and%2520Jin%2520Qi%2520and%2520Yong%2520Lu%2520and%2520Lianfu%2520Deng%2520and%2520Bo%2520Chen%26entry.1292438233%3D%2520%2520The%2520conversion%2520from%25202D%2520X-ray%2520to%25203D%2520shape%2520holds%2520significant%2520potential%2520for%250Aimproving%2520diagnostic%2520efficiency%2520and%2520safety.%2520However%252C%2520existing%2520reconstruction%250Amethods%2520often%2520rely%2520on%2520hand-crafted%2520features%252C%2520manual%2520intervention%252C%2520and%2520prior%250Aknowledge%252C%2520resulting%2520in%2520unstable%2520shape%2520errors%2520and%2520additional%2520processing%2520costs.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520Swin-X2S%252C%2520an%2520end-to-end%2520deep%2520learning%2520method%2520for%250Adirectly%2520reconstructing%25203D%2520segmentation%2520and%2520labeling%2520from%25202D%2520biplanar%250Aorthogonal%2520X-ray%2520images.%2520Swin-X2S%2520employs%2520an%2520encoder-decoder%2520architecture%253A%2520the%250Aencoder%2520leverages%25202D%2520Swin%2520Transformer%2520for%2520X-ray%2520information%2520extraction%252C%2520while%250Athe%2520decoder%2520employs%25203D%2520convolution%2520with%2520cross-attention%2520to%2520integrate%2520structural%250Afeatures%2520from%2520orthogonal%2520views.%2520A%2520dimension-expanding%2520module%2520is%2520introduced%2520to%250Abridge%2520the%2520encoder%2520and%2520decoder%252C%2520ensuring%2520a%2520smooth%2520conversion%2520from%25202D%2520pixels%2520to%250A3D%2520voxels.%2520We%2520evaluate%2520proposed%2520method%2520through%2520extensive%2520qualitative%2520and%250Aquantitative%2520experiments%2520across%2520nine%2520publicly%2520available%2520datasets%2520covering%2520four%250Aanatomies%2520%2528femur%252C%2520hip%252C%2520spine%252C%2520and%2520rib%2529%252C%2520with%2520a%2520total%2520of%252054%2520categories.%250ASignificant%2520improvements%2520over%2520previous%2520methods%2520have%2520been%2520observed%2520not%2520only%2520in%250Athe%2520segmentation%2520and%2520labeling%2520metrics%2520but%2520also%2520in%2520the%2520clinically%2520relevant%250Aparameters%2520that%2520are%2520of%2520primary%2520concern%2520in%2520practical%2520applications%252C%2520which%250Ademonstrates%2520the%2520promise%2520of%2520Swin-X2S%2520to%2520provide%2520an%2520effective%2520option%2520for%250Aanatomical%2520shape%2520reconstruction%2520in%2520clinical%2520scenarios.%2520Code%2520implementation%2520is%250Aavailable%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/liukuan5625/Swin-X2S%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swin-X2S%3A%20Reconstructing%203D%20Shape%20from%202D%20Biplanar%20X-ray%20with%20Swin%0A%20%20Transformers&entry.906535625=Kuan%20Liu%20and%20Zongyuan%20Ying%20and%20Jie%20Jin%20and%20Dongyan%20Li%20and%20Ping%20Huang%20and%20Wenjian%20Wu%20and%20Zhe%20Chen%20and%20Jin%20Qi%20and%20Yong%20Lu%20and%20Lianfu%20Deng%20and%20Bo%20Chen&entry.1292438233=%20%20The%20conversion%20from%202D%20X-ray%20to%203D%20shape%20holds%20significant%20potential%20for%0Aimproving%20diagnostic%20efficiency%20and%20safety.%20However%2C%20existing%20reconstruction%0Amethods%20often%20rely%20on%20hand-crafted%20features%2C%20manual%20intervention%2C%20and%20prior%0Aknowledge%2C%20resulting%20in%20unstable%20shape%20errors%20and%20additional%20processing%20costs.%0AIn%20this%20paper%2C%20we%20introduce%20Swin-X2S%2C%20an%20end-to-end%20deep%20learning%20method%20for%0Adirectly%20reconstructing%203D%20segmentation%20and%20labeling%20from%202D%20biplanar%0Aorthogonal%20X-ray%20images.%20Swin-X2S%20employs%20an%20encoder-decoder%20architecture%3A%20the%0Aencoder%20leverages%202D%20Swin%20Transformer%20for%20X-ray%20information%20extraction%2C%20while%0Athe%20decoder%20employs%203D%20convolution%20with%20cross-attention%20to%20integrate%20structural%0Afeatures%20from%20orthogonal%20views.%20A%20dimension-expanding%20module%20is%20introduced%20to%0Abridge%20the%20encoder%20and%20decoder%2C%20ensuring%20a%20smooth%20conversion%20from%202D%20pixels%20to%0A3D%20voxels.%20We%20evaluate%20proposed%20method%20through%20extensive%20qualitative%20and%0Aquantitative%20experiments%20across%20nine%20publicly%20available%20datasets%20covering%20four%0Aanatomies%20%28femur%2C%20hip%2C%20spine%2C%20and%20rib%29%2C%20with%20a%20total%20of%2054%20categories.%0ASignificant%20improvements%20over%20previous%20methods%20have%20been%20observed%20not%20only%20in%0Athe%20segmentation%20and%20labeling%20metrics%20but%20also%20in%20the%20clinically%20relevant%0Aparameters%20that%20are%20of%20primary%20concern%20in%20practical%20applications%2C%20which%0Ademonstrates%20the%20promise%20of%20Swin-X2S%20to%20provide%20an%20effective%20option%20for%0Aanatomical%20shape%20reconstruction%20in%20clinical%20scenarios.%20Code%20implementation%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/liukuan5625/Swin-X2S%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05961v1&entry.124074799=Read"},
{"title": "Language-Inspired Relation Transfer for Few-shot Class-Incremental\n  Learning", "author": "Yifan Zhao and Jia Li and Zeyin Song and Yonghong Tian", "abstract": "  Depicting novel classes with language descriptions by observing few-shot\nsamples is inherent in human-learning systems. This lifelong learning\ncapability helps to distinguish new knowledge from old ones through the\nincrease of open-world learning, namely Few-Shot Class-Incremental Learning\n(FSCIL). Existing works to solve this problem mainly rely on the careful tuning\nof visual encoders, which shows an evident trade-off between the base knowledge\nand incremental ones. Motivated by human learning systems, we propose a new\nLanguage-inspired Relation Transfer (LRT) paradigm to understand objects by\njoint visual clues and text depictions, composed of two major steps. We first\ntransfer the pretrained text knowledge to the visual domains by proposing a\ngraph relation transformation module and then fuse the visual and language\nembedding by a text-vision prototypical fusion module. Second, to mitigate the\ndomain gap caused by visual finetuning, we propose context prompt learning for\nfast domain alignment and imagined contrastive learning to alleviate the\ninsufficient text data during alignment. With collaborative learning of domain\nalignments and text-image transfer, our proposed LRT outperforms the\nstate-of-the-art models by over $13\\%$ and $7\\%$ on the final session of\nmini-ImageNet and CIFAR-100 FSCIL benchmarks.\n", "link": "http://arxiv.org/abs/2501.05862v1", "date": "2025-01-10", "relevancy": 2.7726, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5644}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Inspired%20Relation%20Transfer%20for%20Few-shot%20Class-Incremental%0A%20%20Learning&body=Title%3A%20Language-Inspired%20Relation%20Transfer%20for%20Few-shot%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Yifan%20Zhao%20and%20Jia%20Li%20and%20Zeyin%20Song%20and%20Yonghong%20Tian%0AAbstract%3A%20%20%20Depicting%20novel%20classes%20with%20language%20descriptions%20by%20observing%20few-shot%0Asamples%20is%20inherent%20in%20human-learning%20systems.%20This%20lifelong%20learning%0Acapability%20helps%20to%20distinguish%20new%20knowledge%20from%20old%20ones%20through%20the%0Aincrease%20of%20open-world%20learning%2C%20namely%20Few-Shot%20Class-Incremental%20Learning%0A%28FSCIL%29.%20Existing%20works%20to%20solve%20this%20problem%20mainly%20rely%20on%20the%20careful%20tuning%0Aof%20visual%20encoders%2C%20which%20shows%20an%20evident%20trade-off%20between%20the%20base%20knowledge%0Aand%20incremental%20ones.%20Motivated%20by%20human%20learning%20systems%2C%20we%20propose%20a%20new%0ALanguage-inspired%20Relation%20Transfer%20%28LRT%29%20paradigm%20to%20understand%20objects%20by%0Ajoint%20visual%20clues%20and%20text%20depictions%2C%20composed%20of%20two%20major%20steps.%20We%20first%0Atransfer%20the%20pretrained%20text%20knowledge%20to%20the%20visual%20domains%20by%20proposing%20a%0Agraph%20relation%20transformation%20module%20and%20then%20fuse%20the%20visual%20and%20language%0Aembedding%20by%20a%20text-vision%20prototypical%20fusion%20module.%20Second%2C%20to%20mitigate%20the%0Adomain%20gap%20caused%20by%20visual%20finetuning%2C%20we%20propose%20context%20prompt%20learning%20for%0Afast%20domain%20alignment%20and%20imagined%20contrastive%20learning%20to%20alleviate%20the%0Ainsufficient%20text%20data%20during%20alignment.%20With%20collaborative%20learning%20of%20domain%0Aalignments%20and%20text-image%20transfer%2C%20our%20proposed%20LRT%20outperforms%20the%0Astate-of-the-art%20models%20by%20over%20%2413%5C%25%24%20and%20%247%5C%25%24%20on%20the%20final%20session%20of%0Amini-ImageNet%20and%20CIFAR-100%20FSCIL%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Inspired%2520Relation%2520Transfer%2520for%2520Few-shot%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DYifan%2520Zhao%2520and%2520Jia%2520Li%2520and%2520Zeyin%2520Song%2520and%2520Yonghong%2520Tian%26entry.1292438233%3D%2520%2520Depicting%2520novel%2520classes%2520with%2520language%2520descriptions%2520by%2520observing%2520few-shot%250Asamples%2520is%2520inherent%2520in%2520human-learning%2520systems.%2520This%2520lifelong%2520learning%250Acapability%2520helps%2520to%2520distinguish%2520new%2520knowledge%2520from%2520old%2520ones%2520through%2520the%250Aincrease%2520of%2520open-world%2520learning%252C%2520namely%2520Few-Shot%2520Class-Incremental%2520Learning%250A%2528FSCIL%2529.%2520Existing%2520works%2520to%2520solve%2520this%2520problem%2520mainly%2520rely%2520on%2520the%2520careful%2520tuning%250Aof%2520visual%2520encoders%252C%2520which%2520shows%2520an%2520evident%2520trade-off%2520between%2520the%2520base%2520knowledge%250Aand%2520incremental%2520ones.%2520Motivated%2520by%2520human%2520learning%2520systems%252C%2520we%2520propose%2520a%2520new%250ALanguage-inspired%2520Relation%2520Transfer%2520%2528LRT%2529%2520paradigm%2520to%2520understand%2520objects%2520by%250Ajoint%2520visual%2520clues%2520and%2520text%2520depictions%252C%2520composed%2520of%2520two%2520major%2520steps.%2520We%2520first%250Atransfer%2520the%2520pretrained%2520text%2520knowledge%2520to%2520the%2520visual%2520domains%2520by%2520proposing%2520a%250Agraph%2520relation%2520transformation%2520module%2520and%2520then%2520fuse%2520the%2520visual%2520and%2520language%250Aembedding%2520by%2520a%2520text-vision%2520prototypical%2520fusion%2520module.%2520Second%252C%2520to%2520mitigate%2520the%250Adomain%2520gap%2520caused%2520by%2520visual%2520finetuning%252C%2520we%2520propose%2520context%2520prompt%2520learning%2520for%250Afast%2520domain%2520alignment%2520and%2520imagined%2520contrastive%2520learning%2520to%2520alleviate%2520the%250Ainsufficient%2520text%2520data%2520during%2520alignment.%2520With%2520collaborative%2520learning%2520of%2520domain%250Aalignments%2520and%2520text-image%2520transfer%252C%2520our%2520proposed%2520LRT%2520outperforms%2520the%250Astate-of-the-art%2520models%2520by%2520over%2520%252413%255C%2525%2524%2520and%2520%25247%255C%2525%2524%2520on%2520the%2520final%2520session%2520of%250Amini-ImageNet%2520and%2520CIFAR-100%2520FSCIL%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Inspired%20Relation%20Transfer%20for%20Few-shot%20Class-Incremental%0A%20%20Learning&entry.906535625=Yifan%20Zhao%20and%20Jia%20Li%20and%20Zeyin%20Song%20and%20Yonghong%20Tian&entry.1292438233=%20%20Depicting%20novel%20classes%20with%20language%20descriptions%20by%20observing%20few-shot%0Asamples%20is%20inherent%20in%20human-learning%20systems.%20This%20lifelong%20learning%0Acapability%20helps%20to%20distinguish%20new%20knowledge%20from%20old%20ones%20through%20the%0Aincrease%20of%20open-world%20learning%2C%20namely%20Few-Shot%20Class-Incremental%20Learning%0A%28FSCIL%29.%20Existing%20works%20to%20solve%20this%20problem%20mainly%20rely%20on%20the%20careful%20tuning%0Aof%20visual%20encoders%2C%20which%20shows%20an%20evident%20trade-off%20between%20the%20base%20knowledge%0Aand%20incremental%20ones.%20Motivated%20by%20human%20learning%20systems%2C%20we%20propose%20a%20new%0ALanguage-inspired%20Relation%20Transfer%20%28LRT%29%20paradigm%20to%20understand%20objects%20by%0Ajoint%20visual%20clues%20and%20text%20depictions%2C%20composed%20of%20two%20major%20steps.%20We%20first%0Atransfer%20the%20pretrained%20text%20knowledge%20to%20the%20visual%20domains%20by%20proposing%20a%0Agraph%20relation%20transformation%20module%20and%20then%20fuse%20the%20visual%20and%20language%0Aembedding%20by%20a%20text-vision%20prototypical%20fusion%20module.%20Second%2C%20to%20mitigate%20the%0Adomain%20gap%20caused%20by%20visual%20finetuning%2C%20we%20propose%20context%20prompt%20learning%20for%0Afast%20domain%20alignment%20and%20imagined%20contrastive%20learning%20to%20alleviate%20the%0Ainsufficient%20text%20data%20during%20alignment.%20With%20collaborative%20learning%20of%20domain%0Aalignments%20and%20text-image%20transfer%2C%20our%20proposed%20LRT%20outperforms%20the%0Astate-of-the-art%20models%20by%20over%20%2413%5C%25%24%20and%20%247%5C%25%24%20on%20the%20final%20session%20of%0Amini-ImageNet%20and%20CIFAR-100%20FSCIL%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05862v1&entry.124074799=Read"},
{"title": "Scalable Vision Language Model Training via High Quality Data Curation", "author": "Hongyuan Dong and Zijian Kang and Weijie Yin and Xiao Liang and Chao Feng and Jiao Ran", "abstract": "  In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning\nvia High QuaLity Data Curation), an open-source vision language model (VLM) of\nstate-of-the-art (SOTA) performance with 2B parameters. We introduce three key\nimprovements that contribute to SAIL-VL's leading performance: (1) Scalable\nhigh-quality visual understanding data construction: We implement a visual\nunderstanding data construction pipeline, which enables hundred-million-scale\nhigh-quality recaption data annotation. Equipped with this pipeline, we curate\nSAIL-Caption, a large-scale caption dataset with large quantity and the highest\ndata quality compared with opensource caption datasets. (2) Scalable\nPretraining with High-Quality Visual Understanding Data: We scale SAIL-VL's\npretraining budget up to 131B tokens and show that even a 2B VLM benefits from\nscaled up training data sizes, exhibiting expected data size scaling laws in\nvisual understanding and instruction following performance. (3) Scalable SFT\nvia quantity and quality scaling: We introduce general guidance for instruction\ndata curation to scale up instruction data continuously, allowing us to\nconstruct a large SFT dataset with the highest quality. To further improve\nSAIL-VL's performance, we propose quality scaling, a multi-stage training\nrecipe with curriculum learning, to improve model performance scaling curves\nw.r.t. data sizes from logarithmic to be near-linear. SAIL-VL obtains the\nhighest average score in 19 commonly used benchmarks in our evaluation and\nachieves top1 performance among VLMs of comparable sizes on OpenCompass\n(https://rank.opencompass.org.cn/leaderboard-multimodal). We release our\nSAIL-VL-2B model at HuggingFace\n(https://huggingface.co/BytedanceDouyinContent/SAIL-VL-2B).\n", "link": "http://arxiv.org/abs/2501.05952v1", "date": "2025-01-10", "relevancy": 2.7679, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Vision%20Language%20Model%20Training%20via%20High%20Quality%20Data%20Curation&body=Title%3A%20Scalable%20Vision%20Language%20Model%20Training%20via%20High%20Quality%20Data%20Curation%0AAuthor%3A%20Hongyuan%20Dong%20and%20Zijian%20Kang%20and%20Weijie%20Yin%20and%20Xiao%20Liang%20and%20Chao%20Feng%20and%20Jiao%20Ran%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20SAIL-VL%20%28ScAlable%20Vision%20Language%20Model%20TraIning%0Avia%20High%20QuaLity%20Data%20Curation%29%2C%20an%20open-source%20vision%20language%20model%20%28VLM%29%20of%0Astate-of-the-art%20%28SOTA%29%20performance%20with%202B%20parameters.%20We%20introduce%20three%20key%0Aimprovements%20that%20contribute%20to%20SAIL-VL%27s%20leading%20performance%3A%20%281%29%20Scalable%0Ahigh-quality%20visual%20understanding%20data%20construction%3A%20We%20implement%20a%20visual%0Aunderstanding%20data%20construction%20pipeline%2C%20which%20enables%20hundred-million-scale%0Ahigh-quality%20recaption%20data%20annotation.%20Equipped%20with%20this%20pipeline%2C%20we%20curate%0ASAIL-Caption%2C%20a%20large-scale%20caption%20dataset%20with%20large%20quantity%20and%20the%20highest%0Adata%20quality%20compared%20with%20opensource%20caption%20datasets.%20%282%29%20Scalable%0APretraining%20with%20High-Quality%20Visual%20Understanding%20Data%3A%20We%20scale%20SAIL-VL%27s%0Apretraining%20budget%20up%20to%20131B%20tokens%20and%20show%20that%20even%20a%202B%20VLM%20benefits%20from%0Ascaled%20up%20training%20data%20sizes%2C%20exhibiting%20expected%20data%20size%20scaling%20laws%20in%0Avisual%20understanding%20and%20instruction%20following%20performance.%20%283%29%20Scalable%20SFT%0Avia%20quantity%20and%20quality%20scaling%3A%20We%20introduce%20general%20guidance%20for%20instruction%0Adata%20curation%20to%20scale%20up%20instruction%20data%20continuously%2C%20allowing%20us%20to%0Aconstruct%20a%20large%20SFT%20dataset%20with%20the%20highest%20quality.%20To%20further%20improve%0ASAIL-VL%27s%20performance%2C%20we%20propose%20quality%20scaling%2C%20a%20multi-stage%20training%0Arecipe%20with%20curriculum%20learning%2C%20to%20improve%20model%20performance%20scaling%20curves%0Aw.r.t.%20data%20sizes%20from%20logarithmic%20to%20be%20near-linear.%20SAIL-VL%20obtains%20the%0Ahighest%20average%20score%20in%2019%20commonly%20used%20benchmarks%20in%20our%20evaluation%20and%0Aachieves%20top1%20performance%20among%20VLMs%20of%20comparable%20sizes%20on%20OpenCompass%0A%28https%3A//rank.opencompass.org.cn/leaderboard-multimodal%29.%20We%20release%20our%0ASAIL-VL-2B%20model%20at%20HuggingFace%0A%28https%3A//huggingface.co/BytedanceDouyinContent/SAIL-VL-2B%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Vision%2520Language%2520Model%2520Training%2520via%2520High%2520Quality%2520Data%2520Curation%26entry.906535625%3DHongyuan%2520Dong%2520and%2520Zijian%2520Kang%2520and%2520Weijie%2520Yin%2520and%2520Xiao%2520Liang%2520and%2520Chao%2520Feng%2520and%2520Jiao%2520Ran%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SAIL-VL%2520%2528ScAlable%2520Vision%2520Language%2520Model%2520TraIning%250Avia%2520High%2520QuaLity%2520Data%2520Curation%2529%252C%2520an%2520open-source%2520vision%2520language%2520model%2520%2528VLM%2529%2520of%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520with%25202B%2520parameters.%2520We%2520introduce%2520three%2520key%250Aimprovements%2520that%2520contribute%2520to%2520SAIL-VL%2527s%2520leading%2520performance%253A%2520%25281%2529%2520Scalable%250Ahigh-quality%2520visual%2520understanding%2520data%2520construction%253A%2520We%2520implement%2520a%2520visual%250Aunderstanding%2520data%2520construction%2520pipeline%252C%2520which%2520enables%2520hundred-million-scale%250Ahigh-quality%2520recaption%2520data%2520annotation.%2520Equipped%2520with%2520this%2520pipeline%252C%2520we%2520curate%250ASAIL-Caption%252C%2520a%2520large-scale%2520caption%2520dataset%2520with%2520large%2520quantity%2520and%2520the%2520highest%250Adata%2520quality%2520compared%2520with%2520opensource%2520caption%2520datasets.%2520%25282%2529%2520Scalable%250APretraining%2520with%2520High-Quality%2520Visual%2520Understanding%2520Data%253A%2520We%2520scale%2520SAIL-VL%2527s%250Apretraining%2520budget%2520up%2520to%2520131B%2520tokens%2520and%2520show%2520that%2520even%2520a%25202B%2520VLM%2520benefits%2520from%250Ascaled%2520up%2520training%2520data%2520sizes%252C%2520exhibiting%2520expected%2520data%2520size%2520scaling%2520laws%2520in%250Avisual%2520understanding%2520and%2520instruction%2520following%2520performance.%2520%25283%2529%2520Scalable%2520SFT%250Avia%2520quantity%2520and%2520quality%2520scaling%253A%2520We%2520introduce%2520general%2520guidance%2520for%2520instruction%250Adata%2520curation%2520to%2520scale%2520up%2520instruction%2520data%2520continuously%252C%2520allowing%2520us%2520to%250Aconstruct%2520a%2520large%2520SFT%2520dataset%2520with%2520the%2520highest%2520quality.%2520To%2520further%2520improve%250ASAIL-VL%2527s%2520performance%252C%2520we%2520propose%2520quality%2520scaling%252C%2520a%2520multi-stage%2520training%250Arecipe%2520with%2520curriculum%2520learning%252C%2520to%2520improve%2520model%2520performance%2520scaling%2520curves%250Aw.r.t.%2520data%2520sizes%2520from%2520logarithmic%2520to%2520be%2520near-linear.%2520SAIL-VL%2520obtains%2520the%250Ahighest%2520average%2520score%2520in%252019%2520commonly%2520used%2520benchmarks%2520in%2520our%2520evaluation%2520and%250Aachieves%2520top1%2520performance%2520among%2520VLMs%2520of%2520comparable%2520sizes%2520on%2520OpenCompass%250A%2528https%253A//rank.opencompass.org.cn/leaderboard-multimodal%2529.%2520We%2520release%2520our%250ASAIL-VL-2B%2520model%2520at%2520HuggingFace%250A%2528https%253A//huggingface.co/BytedanceDouyinContent/SAIL-VL-2B%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Vision%20Language%20Model%20Training%20via%20High%20Quality%20Data%20Curation&entry.906535625=Hongyuan%20Dong%20and%20Zijian%20Kang%20and%20Weijie%20Yin%20and%20Xiao%20Liang%20and%20Chao%20Feng%20and%20Jiao%20Ran&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20SAIL-VL%20%28ScAlable%20Vision%20Language%20Model%20TraIning%0Avia%20High%20QuaLity%20Data%20Curation%29%2C%20an%20open-source%20vision%20language%20model%20%28VLM%29%20of%0Astate-of-the-art%20%28SOTA%29%20performance%20with%202B%20parameters.%20We%20introduce%20three%20key%0Aimprovements%20that%20contribute%20to%20SAIL-VL%27s%20leading%20performance%3A%20%281%29%20Scalable%0Ahigh-quality%20visual%20understanding%20data%20construction%3A%20We%20implement%20a%20visual%0Aunderstanding%20data%20construction%20pipeline%2C%20which%20enables%20hundred-million-scale%0Ahigh-quality%20recaption%20data%20annotation.%20Equipped%20with%20this%20pipeline%2C%20we%20curate%0ASAIL-Caption%2C%20a%20large-scale%20caption%20dataset%20with%20large%20quantity%20and%20the%20highest%0Adata%20quality%20compared%20with%20opensource%20caption%20datasets.%20%282%29%20Scalable%0APretraining%20with%20High-Quality%20Visual%20Understanding%20Data%3A%20We%20scale%20SAIL-VL%27s%0Apretraining%20budget%20up%20to%20131B%20tokens%20and%20show%20that%20even%20a%202B%20VLM%20benefits%20from%0Ascaled%20up%20training%20data%20sizes%2C%20exhibiting%20expected%20data%20size%20scaling%20laws%20in%0Avisual%20understanding%20and%20instruction%20following%20performance.%20%283%29%20Scalable%20SFT%0Avia%20quantity%20and%20quality%20scaling%3A%20We%20introduce%20general%20guidance%20for%20instruction%0Adata%20curation%20to%20scale%20up%20instruction%20data%20continuously%2C%20allowing%20us%20to%0Aconstruct%20a%20large%20SFT%20dataset%20with%20the%20highest%20quality.%20To%20further%20improve%0ASAIL-VL%27s%20performance%2C%20we%20propose%20quality%20scaling%2C%20a%20multi-stage%20training%0Arecipe%20with%20curriculum%20learning%2C%20to%20improve%20model%20performance%20scaling%20curves%0Aw.r.t.%20data%20sizes%20from%20logarithmic%20to%20be%20near-linear.%20SAIL-VL%20obtains%20the%0Ahighest%20average%20score%20in%2019%20commonly%20used%20benchmarks%20in%20our%20evaluation%20and%0Aachieves%20top1%20performance%20among%20VLMs%20of%20comparable%20sizes%20on%20OpenCompass%0A%28https%3A//rank.opencompass.org.cn/leaderboard-multimodal%29.%20We%20release%20our%0ASAIL-VL-2B%20model%20at%20HuggingFace%0A%28https%3A//huggingface.co/BytedanceDouyinContent/SAIL-VL-2B%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05952v1&entry.124074799=Read"},
{"title": "Self-Supervised Masked Mesh Learning for Unsupervised Anomaly Detection\n  on 3D Cortical Surfaces", "author": "Hao-Chun Yang and Sicheng Dai and Saige Rutherford and Christian Gaser and Andre F Marquand and Christian F Beckmann and Thomas Wolfers", "abstract": "  Unsupervised anomaly detection in brain imaging is challenging. In this\npaper, we propose a self-supervised masked mesh learning for unsupervised\nanomaly detection in 3D cortical surfaces. Our framework leverages the\nintrinsic geometry of the cortical surface to learn a self-supervised\nrepresentation that captures the underlying structure of the brain. We\nintroduce a masked mesh convolutional neural network (MMN) that learns to\npredict masked regions of the cortical surface. By training the MMN on a large\ndataset of healthy subjects, we learn a representation that captures the normal\nvariation in the cortical surface. We then use this representation to detect\nanomalies in unseen individuals by calculating anomaly scores based on the\nreconstruction error of the MMN. We evaluate our framework by training on\npopulation-scale dataset UKB and HCP-Aging and testing on two datasets of\nAlzheimer's disease patients ADNI and OASIS3. Our results show that our\nframework can detect anomalies in cortical thickness, cortical volume, and\ncortical sulcus features, which are known to be sensitive biomarkers for\nAlzheimer's disease. Our proposed framework provides a promising approach for\nunsupervised anomaly detection based on normative variation of cortical\nfeatures.\n", "link": "http://arxiv.org/abs/2412.05580v2", "date": "2025-01-10", "relevancy": 2.7641, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5758}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Masked%20Mesh%20Learning%20for%20Unsupervised%20Anomaly%20Detection%0A%20%20on%203D%20Cortical%20Surfaces&body=Title%3A%20Self-Supervised%20Masked%20Mesh%20Learning%20for%20Unsupervised%20Anomaly%20Detection%0A%20%20on%203D%20Cortical%20Surfaces%0AAuthor%3A%20Hao-Chun%20Yang%20and%20Sicheng%20Dai%20and%20Saige%20Rutherford%20and%20Christian%20Gaser%20and%20Andre%20F%20Marquand%20and%20Christian%20F%20Beckmann%20and%20Thomas%20Wolfers%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20detection%20in%20brain%20imaging%20is%20challenging.%20In%20this%0Apaper%2C%20we%20propose%20a%20self-supervised%20masked%20mesh%20learning%20for%20unsupervised%0Aanomaly%20detection%20in%203D%20cortical%20surfaces.%20Our%20framework%20leverages%20the%0Aintrinsic%20geometry%20of%20the%20cortical%20surface%20to%20learn%20a%20self-supervised%0Arepresentation%20that%20captures%20the%20underlying%20structure%20of%20the%20brain.%20We%0Aintroduce%20a%20masked%20mesh%20convolutional%20neural%20network%20%28MMN%29%20that%20learns%20to%0Apredict%20masked%20regions%20of%20the%20cortical%20surface.%20By%20training%20the%20MMN%20on%20a%20large%0Adataset%20of%20healthy%20subjects%2C%20we%20learn%20a%20representation%20that%20captures%20the%20normal%0Avariation%20in%20the%20cortical%20surface.%20We%20then%20use%20this%20representation%20to%20detect%0Aanomalies%20in%20unseen%20individuals%20by%20calculating%20anomaly%20scores%20based%20on%20the%0Areconstruction%20error%20of%20the%20MMN.%20We%20evaluate%20our%20framework%20by%20training%20on%0Apopulation-scale%20dataset%20UKB%20and%20HCP-Aging%20and%20testing%20on%20two%20datasets%20of%0AAlzheimer%27s%20disease%20patients%20ADNI%20and%20OASIS3.%20Our%20results%20show%20that%20our%0Aframework%20can%20detect%20anomalies%20in%20cortical%20thickness%2C%20cortical%20volume%2C%20and%0Acortical%20sulcus%20features%2C%20which%20are%20known%20to%20be%20sensitive%20biomarkers%20for%0AAlzheimer%27s%20disease.%20Our%20proposed%20framework%20provides%20a%20promising%20approach%20for%0Aunsupervised%20anomaly%20detection%20based%20on%20normative%20variation%20of%20cortical%0Afeatures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05580v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Masked%2520Mesh%2520Learning%2520for%2520Unsupervised%2520Anomaly%2520Detection%250A%2520%2520on%25203D%2520Cortical%2520Surfaces%26entry.906535625%3DHao-Chun%2520Yang%2520and%2520Sicheng%2520Dai%2520and%2520Saige%2520Rutherford%2520and%2520Christian%2520Gaser%2520and%2520Andre%2520F%2520Marquand%2520and%2520Christian%2520F%2520Beckmann%2520and%2520Thomas%2520Wolfers%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520detection%2520in%2520brain%2520imaging%2520is%2520challenging.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520self-supervised%2520masked%2520mesh%2520learning%2520for%2520unsupervised%250Aanomaly%2520detection%2520in%25203D%2520cortical%2520surfaces.%2520Our%2520framework%2520leverages%2520the%250Aintrinsic%2520geometry%2520of%2520the%2520cortical%2520surface%2520to%2520learn%2520a%2520self-supervised%250Arepresentation%2520that%2520captures%2520the%2520underlying%2520structure%2520of%2520the%2520brain.%2520We%250Aintroduce%2520a%2520masked%2520mesh%2520convolutional%2520neural%2520network%2520%2528MMN%2529%2520that%2520learns%2520to%250Apredict%2520masked%2520regions%2520of%2520the%2520cortical%2520surface.%2520By%2520training%2520the%2520MMN%2520on%2520a%2520large%250Adataset%2520of%2520healthy%2520subjects%252C%2520we%2520learn%2520a%2520representation%2520that%2520captures%2520the%2520normal%250Avariation%2520in%2520the%2520cortical%2520surface.%2520We%2520then%2520use%2520this%2520representation%2520to%2520detect%250Aanomalies%2520in%2520unseen%2520individuals%2520by%2520calculating%2520anomaly%2520scores%2520based%2520on%2520the%250Areconstruction%2520error%2520of%2520the%2520MMN.%2520We%2520evaluate%2520our%2520framework%2520by%2520training%2520on%250Apopulation-scale%2520dataset%2520UKB%2520and%2520HCP-Aging%2520and%2520testing%2520on%2520two%2520datasets%2520of%250AAlzheimer%2527s%2520disease%2520patients%2520ADNI%2520and%2520OASIS3.%2520Our%2520results%2520show%2520that%2520our%250Aframework%2520can%2520detect%2520anomalies%2520in%2520cortical%2520thickness%252C%2520cortical%2520volume%252C%2520and%250Acortical%2520sulcus%2520features%252C%2520which%2520are%2520known%2520to%2520be%2520sensitive%2520biomarkers%2520for%250AAlzheimer%2527s%2520disease.%2520Our%2520proposed%2520framework%2520provides%2520a%2520promising%2520approach%2520for%250Aunsupervised%2520anomaly%2520detection%2520based%2520on%2520normative%2520variation%2520of%2520cortical%250Afeatures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05580v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Masked%20Mesh%20Learning%20for%20Unsupervised%20Anomaly%20Detection%0A%20%20on%203D%20Cortical%20Surfaces&entry.906535625=Hao-Chun%20Yang%20and%20Sicheng%20Dai%20and%20Saige%20Rutherford%20and%20Christian%20Gaser%20and%20Andre%20F%20Marquand%20and%20Christian%20F%20Beckmann%20and%20Thomas%20Wolfers&entry.1292438233=%20%20Unsupervised%20anomaly%20detection%20in%20brain%20imaging%20is%20challenging.%20In%20this%0Apaper%2C%20we%20propose%20a%20self-supervised%20masked%20mesh%20learning%20for%20unsupervised%0Aanomaly%20detection%20in%203D%20cortical%20surfaces.%20Our%20framework%20leverages%20the%0Aintrinsic%20geometry%20of%20the%20cortical%20surface%20to%20learn%20a%20self-supervised%0Arepresentation%20that%20captures%20the%20underlying%20structure%20of%20the%20brain.%20We%0Aintroduce%20a%20masked%20mesh%20convolutional%20neural%20network%20%28MMN%29%20that%20learns%20to%0Apredict%20masked%20regions%20of%20the%20cortical%20surface.%20By%20training%20the%20MMN%20on%20a%20large%0Adataset%20of%20healthy%20subjects%2C%20we%20learn%20a%20representation%20that%20captures%20the%20normal%0Avariation%20in%20the%20cortical%20surface.%20We%20then%20use%20this%20representation%20to%20detect%0Aanomalies%20in%20unseen%20individuals%20by%20calculating%20anomaly%20scores%20based%20on%20the%0Areconstruction%20error%20of%20the%20MMN.%20We%20evaluate%20our%20framework%20by%20training%20on%0Apopulation-scale%20dataset%20UKB%20and%20HCP-Aging%20and%20testing%20on%20two%20datasets%20of%0AAlzheimer%27s%20disease%20patients%20ADNI%20and%20OASIS3.%20Our%20results%20show%20that%20our%0Aframework%20can%20detect%20anomalies%20in%20cortical%20thickness%2C%20cortical%20volume%2C%20and%0Acortical%20sulcus%20features%2C%20which%20are%20known%20to%20be%20sensitive%20biomarkers%20for%0AAlzheimer%27s%20disease.%20Our%20proposed%20framework%20provides%20a%20promising%20approach%20for%0Aunsupervised%20anomaly%20detection%20based%20on%20normative%20variation%20of%20cortical%0Afeatures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05580v2&entry.124074799=Read"},
{"title": "MS-Temba : Multi-Scale Temporal Mamba for Efficient Temporal Action\n  Detection", "author": "Arkaprava Sinha and Monish Soundar Raj and Pu Wang and Ahmed Helmy and Srijan Das", "abstract": "  Action detection in real-world scenarios is particularly challenging due to\ndensely distributed actions in hour-long untrimmed videos. It requires modeling\nboth short- and long-term temporal relationships while handling significant\nintra-class temporal variations. Previous state-of-the-art (SOTA)\nTransformer-based architectures, though effective, are impractical for\nreal-world deployment due to their high parameter count, GPU memory usage, and\nlimited throughput, making them unsuitable for very long videos. In this work,\nwe innovatively adapt the Mamba architecture for action detection and propose\nMulti-scale Temporal Mamba (MS-Temba), comprising two key components: Temporal\nMamba (Temba) Blocks and the Temporal Mamba Fuser. Temba Blocks include the\nTemporal Local Module (TLM) for short-range temporal modeling and the Dilated\nTemporal SSM (DTS) for long-range dependencies. By introducing dilations, a\nnovel concept for Mamba, TLM and DTS capture local and global features at\nmultiple scales. The Temba Fuser aggregates these scale-specific features using\nMamba to learn comprehensive multi-scale representations of untrimmed videos.\nMS-Temba is validated on three public datasets, outperforming SOTA methods on\nlong videos and matching prior methods on short videos while using only\none-eighth of the parameters.\n", "link": "http://arxiv.org/abs/2501.06138v1", "date": "2025-01-10", "relevancy": 2.7579, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5971}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.559}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-Temba%20%3A%20Multi-Scale%20Temporal%20Mamba%20for%20Efficient%20Temporal%20Action%0A%20%20Detection&body=Title%3A%20MS-Temba%20%3A%20Multi-Scale%20Temporal%20Mamba%20for%20Efficient%20Temporal%20Action%0A%20%20Detection%0AAuthor%3A%20Arkaprava%20Sinha%20and%20Monish%20Soundar%20Raj%20and%20Pu%20Wang%20and%20Ahmed%20Helmy%20and%20Srijan%20Das%0AAbstract%3A%20%20%20Action%20detection%20in%20real-world%20scenarios%20is%20particularly%20challenging%20due%20to%0Adensely%20distributed%20actions%20in%20hour-long%20untrimmed%20videos.%20It%20requires%20modeling%0Aboth%20short-%20and%20long-term%20temporal%20relationships%20while%20handling%20significant%0Aintra-class%20temporal%20variations.%20Previous%20state-of-the-art%20%28SOTA%29%0ATransformer-based%20architectures%2C%20though%20effective%2C%20are%20impractical%20for%0Areal-world%20deployment%20due%20to%20their%20high%20parameter%20count%2C%20GPU%20memory%20usage%2C%20and%0Alimited%20throughput%2C%20making%20them%20unsuitable%20for%20very%20long%20videos.%20In%20this%20work%2C%0Awe%20innovatively%20adapt%20the%20Mamba%20architecture%20for%20action%20detection%20and%20propose%0AMulti-scale%20Temporal%20Mamba%20%28MS-Temba%29%2C%20comprising%20two%20key%20components%3A%20Temporal%0AMamba%20%28Temba%29%20Blocks%20and%20the%20Temporal%20Mamba%20Fuser.%20Temba%20Blocks%20include%20the%0ATemporal%20Local%20Module%20%28TLM%29%20for%20short-range%20temporal%20modeling%20and%20the%20Dilated%0ATemporal%20SSM%20%28DTS%29%20for%20long-range%20dependencies.%20By%20introducing%20dilations%2C%20a%0Anovel%20concept%20for%20Mamba%2C%20TLM%20and%20DTS%20capture%20local%20and%20global%20features%20at%0Amultiple%20scales.%20The%20Temba%20Fuser%20aggregates%20these%20scale-specific%20features%20using%0AMamba%20to%20learn%20comprehensive%20multi-scale%20representations%20of%20untrimmed%20videos.%0AMS-Temba%20is%20validated%20on%20three%20public%20datasets%2C%20outperforming%20SOTA%20methods%20on%0Along%20videos%20and%20matching%20prior%20methods%20on%20short%20videos%20while%20using%20only%0Aone-eighth%20of%20the%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-Temba%2520%253A%2520Multi-Scale%2520Temporal%2520Mamba%2520for%2520Efficient%2520Temporal%2520Action%250A%2520%2520Detection%26entry.906535625%3DArkaprava%2520Sinha%2520and%2520Monish%2520Soundar%2520Raj%2520and%2520Pu%2520Wang%2520and%2520Ahmed%2520Helmy%2520and%2520Srijan%2520Das%26entry.1292438233%3D%2520%2520Action%2520detection%2520in%2520real-world%2520scenarios%2520is%2520particularly%2520challenging%2520due%2520to%250Adensely%2520distributed%2520actions%2520in%2520hour-long%2520untrimmed%2520videos.%2520It%2520requires%2520modeling%250Aboth%2520short-%2520and%2520long-term%2520temporal%2520relationships%2520while%2520handling%2520significant%250Aintra-class%2520temporal%2520variations.%2520Previous%2520state-of-the-art%2520%2528SOTA%2529%250ATransformer-based%2520architectures%252C%2520though%2520effective%252C%2520are%2520impractical%2520for%250Areal-world%2520deployment%2520due%2520to%2520their%2520high%2520parameter%2520count%252C%2520GPU%2520memory%2520usage%252C%2520and%250Alimited%2520throughput%252C%2520making%2520them%2520unsuitable%2520for%2520very%2520long%2520videos.%2520In%2520this%2520work%252C%250Awe%2520innovatively%2520adapt%2520the%2520Mamba%2520architecture%2520for%2520action%2520detection%2520and%2520propose%250AMulti-scale%2520Temporal%2520Mamba%2520%2528MS-Temba%2529%252C%2520comprising%2520two%2520key%2520components%253A%2520Temporal%250AMamba%2520%2528Temba%2529%2520Blocks%2520and%2520the%2520Temporal%2520Mamba%2520Fuser.%2520Temba%2520Blocks%2520include%2520the%250ATemporal%2520Local%2520Module%2520%2528TLM%2529%2520for%2520short-range%2520temporal%2520modeling%2520and%2520the%2520Dilated%250ATemporal%2520SSM%2520%2528DTS%2529%2520for%2520long-range%2520dependencies.%2520By%2520introducing%2520dilations%252C%2520a%250Anovel%2520concept%2520for%2520Mamba%252C%2520TLM%2520and%2520DTS%2520capture%2520local%2520and%2520global%2520features%2520at%250Amultiple%2520scales.%2520The%2520Temba%2520Fuser%2520aggregates%2520these%2520scale-specific%2520features%2520using%250AMamba%2520to%2520learn%2520comprehensive%2520multi-scale%2520representations%2520of%2520untrimmed%2520videos.%250AMS-Temba%2520is%2520validated%2520on%2520three%2520public%2520datasets%252C%2520outperforming%2520SOTA%2520methods%2520on%250Along%2520videos%2520and%2520matching%2520prior%2520methods%2520on%2520short%2520videos%2520while%2520using%2520only%250Aone-eighth%2520of%2520the%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-Temba%20%3A%20Multi-Scale%20Temporal%20Mamba%20for%20Efficient%20Temporal%20Action%0A%20%20Detection&entry.906535625=Arkaprava%20Sinha%20and%20Monish%20Soundar%20Raj%20and%20Pu%20Wang%20and%20Ahmed%20Helmy%20and%20Srijan%20Das&entry.1292438233=%20%20Action%20detection%20in%20real-world%20scenarios%20is%20particularly%20challenging%20due%20to%0Adensely%20distributed%20actions%20in%20hour-long%20untrimmed%20videos.%20It%20requires%20modeling%0Aboth%20short-%20and%20long-term%20temporal%20relationships%20while%20handling%20significant%0Aintra-class%20temporal%20variations.%20Previous%20state-of-the-art%20%28SOTA%29%0ATransformer-based%20architectures%2C%20though%20effective%2C%20are%20impractical%20for%0Areal-world%20deployment%20due%20to%20their%20high%20parameter%20count%2C%20GPU%20memory%20usage%2C%20and%0Alimited%20throughput%2C%20making%20them%20unsuitable%20for%20very%20long%20videos.%20In%20this%20work%2C%0Awe%20innovatively%20adapt%20the%20Mamba%20architecture%20for%20action%20detection%20and%20propose%0AMulti-scale%20Temporal%20Mamba%20%28MS-Temba%29%2C%20comprising%20two%20key%20components%3A%20Temporal%0AMamba%20%28Temba%29%20Blocks%20and%20the%20Temporal%20Mamba%20Fuser.%20Temba%20Blocks%20include%20the%0ATemporal%20Local%20Module%20%28TLM%29%20for%20short-range%20temporal%20modeling%20and%20the%20Dilated%0ATemporal%20SSM%20%28DTS%29%20for%20long-range%20dependencies.%20By%20introducing%20dilations%2C%20a%0Anovel%20concept%20for%20Mamba%2C%20TLM%20and%20DTS%20capture%20local%20and%20global%20features%20at%0Amultiple%20scales.%20The%20Temba%20Fuser%20aggregates%20these%20scale-specific%20features%20using%0AMamba%20to%20learn%20comprehensive%20multi-scale%20representations%20of%20untrimmed%20videos.%0AMS-Temba%20is%20validated%20on%20three%20public%20datasets%2C%20outperforming%20SOTA%20methods%20on%0Along%20videos%20and%20matching%20prior%20methods%20on%20short%20videos%20while%20using%20only%0Aone-eighth%20of%20the%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06138v1&entry.124074799=Read"},
{"title": "Self-Supervised Partial Cycle-Consistency for Multi-View Matching", "author": "Fedor Taggenbrock and Gertjan Burghouts and Ronald Poppe", "abstract": "  Matching objects across partially overlapping camera views is crucial in\nmulti-camera systems and requires a view-invariant feature extraction network.\nTraining such a network with cycle-consistency circumvents the need for\nlabor-intensive labeling. In this paper, we extend the mathematical formulation\nof cycle-consistency to handle partial overlap. We then introduce a pseudo-mask\nwhich directs the training loss to take partial overlap into account. We\nadditionally present several new cycle variants that complement each other and\npresent a time-divergent scene sampling scheme that improves the data input for\nthis self-supervised setting. Cross-camera matching experiments on the\nchallenging DIVOTrack dataset show the merits of our approach. Compared to the\nself-supervised state-of-the-art, we achieve a 4.3 percentage point higher F1\nscore with our combined contributions. Our improvements are robust to reduced\noverlap in the training data, with substantial improvements in challenging\nscenes that need to make few matches between many people. Self-supervised\nfeature networks trained with our method are effective at matching objects in a\nrange of multi-camera settings, providing opportunities for complex tasks like\nlarge-scale multi-camera scene understanding.\n", "link": "http://arxiv.org/abs/2501.06000v1", "date": "2025-01-10", "relevancy": 2.7414, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5757}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Partial%20Cycle-Consistency%20for%20Multi-View%20Matching&body=Title%3A%20Self-Supervised%20Partial%20Cycle-Consistency%20for%20Multi-View%20Matching%0AAuthor%3A%20Fedor%20Taggenbrock%20and%20Gertjan%20Burghouts%20and%20Ronald%20Poppe%0AAbstract%3A%20%20%20Matching%20objects%20across%20partially%20overlapping%20camera%20views%20is%20crucial%20in%0Amulti-camera%20systems%20and%20requires%20a%20view-invariant%20feature%20extraction%20network.%0ATraining%20such%20a%20network%20with%20cycle-consistency%20circumvents%20the%20need%20for%0Alabor-intensive%20labeling.%20In%20this%20paper%2C%20we%20extend%20the%20mathematical%20formulation%0Aof%20cycle-consistency%20to%20handle%20partial%20overlap.%20We%20then%20introduce%20a%20pseudo-mask%0Awhich%20directs%20the%20training%20loss%20to%20take%20partial%20overlap%20into%20account.%20We%0Aadditionally%20present%20several%20new%20cycle%20variants%20that%20complement%20each%20other%20and%0Apresent%20a%20time-divergent%20scene%20sampling%20scheme%20that%20improves%20the%20data%20input%20for%0Athis%20self-supervised%20setting.%20Cross-camera%20matching%20experiments%20on%20the%0Achallenging%20DIVOTrack%20dataset%20show%20the%20merits%20of%20our%20approach.%20Compared%20to%20the%0Aself-supervised%20state-of-the-art%2C%20we%20achieve%20a%204.3%20percentage%20point%20higher%20F1%0Ascore%20with%20our%20combined%20contributions.%20Our%20improvements%20are%20robust%20to%20reduced%0Aoverlap%20in%20the%20training%20data%2C%20with%20substantial%20improvements%20in%20challenging%0Ascenes%20that%20need%20to%20make%20few%20matches%20between%20many%20people.%20Self-supervised%0Afeature%20networks%20trained%20with%20our%20method%20are%20effective%20at%20matching%20objects%20in%20a%0Arange%20of%20multi-camera%20settings%2C%20providing%20opportunities%20for%20complex%20tasks%20like%0Alarge-scale%20multi-camera%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Partial%2520Cycle-Consistency%2520for%2520Multi-View%2520Matching%26entry.906535625%3DFedor%2520Taggenbrock%2520and%2520Gertjan%2520Burghouts%2520and%2520Ronald%2520Poppe%26entry.1292438233%3D%2520%2520Matching%2520objects%2520across%2520partially%2520overlapping%2520camera%2520views%2520is%2520crucial%2520in%250Amulti-camera%2520systems%2520and%2520requires%2520a%2520view-invariant%2520feature%2520extraction%2520network.%250ATraining%2520such%2520a%2520network%2520with%2520cycle-consistency%2520circumvents%2520the%2520need%2520for%250Alabor-intensive%2520labeling.%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520mathematical%2520formulation%250Aof%2520cycle-consistency%2520to%2520handle%2520partial%2520overlap.%2520We%2520then%2520introduce%2520a%2520pseudo-mask%250Awhich%2520directs%2520the%2520training%2520loss%2520to%2520take%2520partial%2520overlap%2520into%2520account.%2520We%250Aadditionally%2520present%2520several%2520new%2520cycle%2520variants%2520that%2520complement%2520each%2520other%2520and%250Apresent%2520a%2520time-divergent%2520scene%2520sampling%2520scheme%2520that%2520improves%2520the%2520data%2520input%2520for%250Athis%2520self-supervised%2520setting.%2520Cross-camera%2520matching%2520experiments%2520on%2520the%250Achallenging%2520DIVOTrack%2520dataset%2520show%2520the%2520merits%2520of%2520our%2520approach.%2520Compared%2520to%2520the%250Aself-supervised%2520state-of-the-art%252C%2520we%2520achieve%2520a%25204.3%2520percentage%2520point%2520higher%2520F1%250Ascore%2520with%2520our%2520combined%2520contributions.%2520Our%2520improvements%2520are%2520robust%2520to%2520reduced%250Aoverlap%2520in%2520the%2520training%2520data%252C%2520with%2520substantial%2520improvements%2520in%2520challenging%250Ascenes%2520that%2520need%2520to%2520make%2520few%2520matches%2520between%2520many%2520people.%2520Self-supervised%250Afeature%2520networks%2520trained%2520with%2520our%2520method%2520are%2520effective%2520at%2520matching%2520objects%2520in%2520a%250Arange%2520of%2520multi-camera%2520settings%252C%2520providing%2520opportunities%2520for%2520complex%2520tasks%2520like%250Alarge-scale%2520multi-camera%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Partial%20Cycle-Consistency%20for%20Multi-View%20Matching&entry.906535625=Fedor%20Taggenbrock%20and%20Gertjan%20Burghouts%20and%20Ronald%20Poppe&entry.1292438233=%20%20Matching%20objects%20across%20partially%20overlapping%20camera%20views%20is%20crucial%20in%0Amulti-camera%20systems%20and%20requires%20a%20view-invariant%20feature%20extraction%20network.%0ATraining%20such%20a%20network%20with%20cycle-consistency%20circumvents%20the%20need%20for%0Alabor-intensive%20labeling.%20In%20this%20paper%2C%20we%20extend%20the%20mathematical%20formulation%0Aof%20cycle-consistency%20to%20handle%20partial%20overlap.%20We%20then%20introduce%20a%20pseudo-mask%0Awhich%20directs%20the%20training%20loss%20to%20take%20partial%20overlap%20into%20account.%20We%0Aadditionally%20present%20several%20new%20cycle%20variants%20that%20complement%20each%20other%20and%0Apresent%20a%20time-divergent%20scene%20sampling%20scheme%20that%20improves%20the%20data%20input%20for%0Athis%20self-supervised%20setting.%20Cross-camera%20matching%20experiments%20on%20the%0Achallenging%20DIVOTrack%20dataset%20show%20the%20merits%20of%20our%20approach.%20Compared%20to%20the%0Aself-supervised%20state-of-the-art%2C%20we%20achieve%20a%204.3%20percentage%20point%20higher%20F1%0Ascore%20with%20our%20combined%20contributions.%20Our%20improvements%20are%20robust%20to%20reduced%0Aoverlap%20in%20the%20training%20data%2C%20with%20substantial%20improvements%20in%20challenging%0Ascenes%20that%20need%20to%20make%20few%20matches%20between%20many%20people.%20Self-supervised%0Afeature%20networks%20trained%20with%20our%20method%20are%20effective%20at%20matching%20objects%20in%20a%0Arange%20of%20multi-camera%20settings%2C%20providing%20opportunities%20for%20complex%20tasks%20like%0Alarge-scale%20multi-camera%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06000v1&entry.124074799=Read"},
{"title": "Improving Medical Visual Representations via Radiology Report Generation", "author": "Keegan Quigley and Miriam Cha and Josh Barua and Geeticka Chauhan and Seth Berkowitz and Steven Horng and Polina Golland", "abstract": "  Vision-language pretraining has been shown to produce high-quality visual\nencoders which transfer efficiently to downstream computer vision tasks.\nContrastive learning approaches have increasingly been adopted for medical\nvision language pretraining (MVLP), yet recent developments in generative AI\noffer new modeling alternatives. This paper introduces RadTex, a CNN-encoder\ntransformer-decoder architecture optimized for radiology. We explore\nbidirectional captioning as an alternative MVLP strategy and demonstrate that\nRadTex's captioning pretraining is competitive with established contrastive\nmethods, achieving a CheXpert macro-AUC of 89.4%. Additionally, RadTex's\nlightweight text decoder not only generates clinically relevant radiology\nreports (macro-F1 score of 0.349), but also provides targeted, interactive\nresponses, highlighting the utility of bidirectional captioning in advancing\nmedical image analysis.\n", "link": "http://arxiv.org/abs/2310.19635v2", "date": "2025-01-10", "relevancy": 2.6998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Medical%20Visual%20Representations%20via%20Radiology%20Report%20Generation&body=Title%3A%20Improving%20Medical%20Visual%20Representations%20via%20Radiology%20Report%20Generation%0AAuthor%3A%20Keegan%20Quigley%20and%20Miriam%20Cha%20and%20Josh%20Barua%20and%20Geeticka%20Chauhan%20and%20Seth%20Berkowitz%20and%20Steven%20Horng%20and%20Polina%20Golland%0AAbstract%3A%20%20%20Vision-language%20pretraining%20has%20been%20shown%20to%20produce%20high-quality%20visual%0Aencoders%20which%20transfer%20efficiently%20to%20downstream%20computer%20vision%20tasks.%0AContrastive%20learning%20approaches%20have%20increasingly%20been%20adopted%20for%20medical%0Avision%20language%20pretraining%20%28MVLP%29%2C%20yet%20recent%20developments%20in%20generative%20AI%0Aoffer%20new%20modeling%20alternatives.%20This%20paper%20introduces%20RadTex%2C%20a%20CNN-encoder%0Atransformer-decoder%20architecture%20optimized%20for%20radiology.%20We%20explore%0Abidirectional%20captioning%20as%20an%20alternative%20MVLP%20strategy%20and%20demonstrate%20that%0ARadTex%27s%20captioning%20pretraining%20is%20competitive%20with%20established%20contrastive%0Amethods%2C%20achieving%20a%20CheXpert%20macro-AUC%20of%2089.4%25.%20Additionally%2C%20RadTex%27s%0Alightweight%20text%20decoder%20not%20only%20generates%20clinically%20relevant%20radiology%0Areports%20%28macro-F1%20score%20of%200.349%29%2C%20but%20also%20provides%20targeted%2C%20interactive%0Aresponses%2C%20highlighting%20the%20utility%20of%20bidirectional%20captioning%20in%20advancing%0Amedical%20image%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Medical%2520Visual%2520Representations%2520via%2520Radiology%2520Report%2520Generation%26entry.906535625%3DKeegan%2520Quigley%2520and%2520Miriam%2520Cha%2520and%2520Josh%2520Barua%2520and%2520Geeticka%2520Chauhan%2520and%2520Seth%2520Berkowitz%2520and%2520Steven%2520Horng%2520and%2520Polina%2520Golland%26entry.1292438233%3D%2520%2520Vision-language%2520pretraining%2520has%2520been%2520shown%2520to%2520produce%2520high-quality%2520visual%250Aencoders%2520which%2520transfer%2520efficiently%2520to%2520downstream%2520computer%2520vision%2520tasks.%250AContrastive%2520learning%2520approaches%2520have%2520increasingly%2520been%2520adopted%2520for%2520medical%250Avision%2520language%2520pretraining%2520%2528MVLP%2529%252C%2520yet%2520recent%2520developments%2520in%2520generative%2520AI%250Aoffer%2520new%2520modeling%2520alternatives.%2520This%2520paper%2520introduces%2520RadTex%252C%2520a%2520CNN-encoder%250Atransformer-decoder%2520architecture%2520optimized%2520for%2520radiology.%2520We%2520explore%250Abidirectional%2520captioning%2520as%2520an%2520alternative%2520MVLP%2520strategy%2520and%2520demonstrate%2520that%250ARadTex%2527s%2520captioning%2520pretraining%2520is%2520competitive%2520with%2520established%2520contrastive%250Amethods%252C%2520achieving%2520a%2520CheXpert%2520macro-AUC%2520of%252089.4%2525.%2520Additionally%252C%2520RadTex%2527s%250Alightweight%2520text%2520decoder%2520not%2520only%2520generates%2520clinically%2520relevant%2520radiology%250Areports%2520%2528macro-F1%2520score%2520of%25200.349%2529%252C%2520but%2520also%2520provides%2520targeted%252C%2520interactive%250Aresponses%252C%2520highlighting%2520the%2520utility%2520of%2520bidirectional%2520captioning%2520in%2520advancing%250Amedical%2520image%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Medical%20Visual%20Representations%20via%20Radiology%20Report%20Generation&entry.906535625=Keegan%20Quigley%20and%20Miriam%20Cha%20and%20Josh%20Barua%20and%20Geeticka%20Chauhan%20and%20Seth%20Berkowitz%20and%20Steven%20Horng%20and%20Polina%20Golland&entry.1292438233=%20%20Vision-language%20pretraining%20has%20been%20shown%20to%20produce%20high-quality%20visual%0Aencoders%20which%20transfer%20efficiently%20to%20downstream%20computer%20vision%20tasks.%0AContrastive%20learning%20approaches%20have%20increasingly%20been%20adopted%20for%20medical%0Avision%20language%20pretraining%20%28MVLP%29%2C%20yet%20recent%20developments%20in%20generative%20AI%0Aoffer%20new%20modeling%20alternatives.%20This%20paper%20introduces%20RadTex%2C%20a%20CNN-encoder%0Atransformer-decoder%20architecture%20optimized%20for%20radiology.%20We%20explore%0Abidirectional%20captioning%20as%20an%20alternative%20MVLP%20strategy%20and%20demonstrate%20that%0ARadTex%27s%20captioning%20pretraining%20is%20competitive%20with%20established%20contrastive%0Amethods%2C%20achieving%20a%20CheXpert%20macro-AUC%20of%2089.4%25.%20Additionally%2C%20RadTex%27s%0Alightweight%20text%20decoder%20not%20only%20generates%20clinically%20relevant%20radiology%0Areports%20%28macro-F1%20score%20of%200.349%29%2C%20but%20also%20provides%20targeted%2C%20interactive%0Aresponses%2C%20highlighting%20the%20utility%20of%20bidirectional%20captioning%20in%20advancing%0Amedical%20image%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19635v2&entry.124074799=Read"},
{"title": "Strip R-CNN: Large Strip Convolution for Remote Sensing Object Detection", "author": "Xinbin Yuan and Zhaohui Zheng and Yuxuan Li and Xialei Liu and Li Liu and Xiang Li and Qibin Hou and Ming-Ming Cheng", "abstract": "  While witnessed with rapid development, remote sensing object detection\nremains challenging for detecting high aspect ratio objects. This paper shows\nthat large strip convolutions are good feature representation learners for\nremote sensing object detection and can detect objects of various aspect ratios\nwell. Based on large strip convolutions, we build a new network architecture\ncalled Strip R-CNN, which is simple, efficient, and powerful. Unlike recent\nremote sensing object detectors that leverage large-kernel convolutions with\nsquare shapes, our Strip R-CNN takes advantage of sequential orthogonal large\nstrip convolutions to capture spatial information. In addition, we enhance the\nlocalization capability of remote-sensing object detectors by decoupling the\ndetection heads and equipping the localization head with strip convolutions to\nbetter localize the target objects. Extensive experiments on several\nbenchmarks, e.g., DOTA, FAIR1M, HRSC2016, and DIOR, show that our Strip R-CNN\ncan largely improve previous works. Notably, our 30M model achieves 82.75% mAP\non DOTA-v1.0, setting a new state-of-the-art record.Code is available at\nhttps://github.com/YXB-NKU/Strip-R-CNN.\n", "link": "http://arxiv.org/abs/2501.03775v3", "date": "2025-01-10", "relevancy": 2.6832, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5566}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5303}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strip%20R-CNN%3A%20Large%20Strip%20Convolution%20for%20Remote%20Sensing%20Object%20Detection&body=Title%3A%20Strip%20R-CNN%3A%20Large%20Strip%20Convolution%20for%20Remote%20Sensing%20Object%20Detection%0AAuthor%3A%20Xinbin%20Yuan%20and%20Zhaohui%20Zheng%20and%20Yuxuan%20Li%20and%20Xialei%20Liu%20and%20Li%20Liu%20and%20Xiang%20Li%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20%20%20While%20witnessed%20with%20rapid%20development%2C%20remote%20sensing%20object%20detection%0Aremains%20challenging%20for%20detecting%20high%20aspect%20ratio%20objects.%20This%20paper%20shows%0Athat%20large%20strip%20convolutions%20are%20good%20feature%20representation%20learners%20for%0Aremote%20sensing%20object%20detection%20and%20can%20detect%20objects%20of%20various%20aspect%20ratios%0Awell.%20Based%20on%20large%20strip%20convolutions%2C%20we%20build%20a%20new%20network%20architecture%0Acalled%20Strip%20R-CNN%2C%20which%20is%20simple%2C%20efficient%2C%20and%20powerful.%20Unlike%20recent%0Aremote%20sensing%20object%20detectors%20that%20leverage%20large-kernel%20convolutions%20with%0Asquare%20shapes%2C%20our%20Strip%20R-CNN%20takes%20advantage%20of%20sequential%20orthogonal%20large%0Astrip%20convolutions%20to%20capture%20spatial%20information.%20In%20addition%2C%20we%20enhance%20the%0Alocalization%20capability%20of%20remote-sensing%20object%20detectors%20by%20decoupling%20the%0Adetection%20heads%20and%20equipping%20the%20localization%20head%20with%20strip%20convolutions%20to%0Abetter%20localize%20the%20target%20objects.%20Extensive%20experiments%20on%20several%0Abenchmarks%2C%20e.g.%2C%20DOTA%2C%20FAIR1M%2C%20HRSC2016%2C%20and%20DIOR%2C%20show%20that%20our%20Strip%20R-CNN%0Acan%20largely%20improve%20previous%20works.%20Notably%2C%20our%2030M%20model%20achieves%2082.75%25%20mAP%0Aon%20DOTA-v1.0%2C%20setting%20a%20new%20state-of-the-art%20record.Code%20is%20available%20at%0Ahttps%3A//github.com/YXB-NKU/Strip-R-CNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03775v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrip%2520R-CNN%253A%2520Large%2520Strip%2520Convolution%2520for%2520Remote%2520Sensing%2520Object%2520Detection%26entry.906535625%3DXinbin%2520Yuan%2520and%2520Zhaohui%2520Zheng%2520and%2520Yuxuan%2520Li%2520and%2520Xialei%2520Liu%2520and%2520Li%2520Liu%2520and%2520Xiang%2520Li%2520and%2520Qibin%2520Hou%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3D%2520%2520While%2520witnessed%2520with%2520rapid%2520development%252C%2520remote%2520sensing%2520object%2520detection%250Aremains%2520challenging%2520for%2520detecting%2520high%2520aspect%2520ratio%2520objects.%2520This%2520paper%2520shows%250Athat%2520large%2520strip%2520convolutions%2520are%2520good%2520feature%2520representation%2520learners%2520for%250Aremote%2520sensing%2520object%2520detection%2520and%2520can%2520detect%2520objects%2520of%2520various%2520aspect%2520ratios%250Awell.%2520Based%2520on%2520large%2520strip%2520convolutions%252C%2520we%2520build%2520a%2520new%2520network%2520architecture%250Acalled%2520Strip%2520R-CNN%252C%2520which%2520is%2520simple%252C%2520efficient%252C%2520and%2520powerful.%2520Unlike%2520recent%250Aremote%2520sensing%2520object%2520detectors%2520that%2520leverage%2520large-kernel%2520convolutions%2520with%250Asquare%2520shapes%252C%2520our%2520Strip%2520R-CNN%2520takes%2520advantage%2520of%2520sequential%2520orthogonal%2520large%250Astrip%2520convolutions%2520to%2520capture%2520spatial%2520information.%2520In%2520addition%252C%2520we%2520enhance%2520the%250Alocalization%2520capability%2520of%2520remote-sensing%2520object%2520detectors%2520by%2520decoupling%2520the%250Adetection%2520heads%2520and%2520equipping%2520the%2520localization%2520head%2520with%2520strip%2520convolutions%2520to%250Abetter%2520localize%2520the%2520target%2520objects.%2520Extensive%2520experiments%2520on%2520several%250Abenchmarks%252C%2520e.g.%252C%2520DOTA%252C%2520FAIR1M%252C%2520HRSC2016%252C%2520and%2520DIOR%252C%2520show%2520that%2520our%2520Strip%2520R-CNN%250Acan%2520largely%2520improve%2520previous%2520works.%2520Notably%252C%2520our%252030M%2520model%2520achieves%252082.75%2525%2520mAP%250Aon%2520DOTA-v1.0%252C%2520setting%2520a%2520new%2520state-of-the-art%2520record.Code%2520is%2520available%2520at%250Ahttps%253A//github.com/YXB-NKU/Strip-R-CNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03775v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strip%20R-CNN%3A%20Large%20Strip%20Convolution%20for%20Remote%20Sensing%20Object%20Detection&entry.906535625=Xinbin%20Yuan%20and%20Zhaohui%20Zheng%20and%20Yuxuan%20Li%20and%20Xialei%20Liu%20and%20Li%20Liu%20and%20Xiang%20Li%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%20While%20witnessed%20with%20rapid%20development%2C%20remote%20sensing%20object%20detection%0Aremains%20challenging%20for%20detecting%20high%20aspect%20ratio%20objects.%20This%20paper%20shows%0Athat%20large%20strip%20convolutions%20are%20good%20feature%20representation%20learners%20for%0Aremote%20sensing%20object%20detection%20and%20can%20detect%20objects%20of%20various%20aspect%20ratios%0Awell.%20Based%20on%20large%20strip%20convolutions%2C%20we%20build%20a%20new%20network%20architecture%0Acalled%20Strip%20R-CNN%2C%20which%20is%20simple%2C%20efficient%2C%20and%20powerful.%20Unlike%20recent%0Aremote%20sensing%20object%20detectors%20that%20leverage%20large-kernel%20convolutions%20with%0Asquare%20shapes%2C%20our%20Strip%20R-CNN%20takes%20advantage%20of%20sequential%20orthogonal%20large%0Astrip%20convolutions%20to%20capture%20spatial%20information.%20In%20addition%2C%20we%20enhance%20the%0Alocalization%20capability%20of%20remote-sensing%20object%20detectors%20by%20decoupling%20the%0Adetection%20heads%20and%20equipping%20the%20localization%20head%20with%20strip%20convolutions%20to%0Abetter%20localize%20the%20target%20objects.%20Extensive%20experiments%20on%20several%0Abenchmarks%2C%20e.g.%2C%20DOTA%2C%20FAIR1M%2C%20HRSC2016%2C%20and%20DIOR%2C%20show%20that%20our%20Strip%20R-CNN%0Acan%20largely%20improve%20previous%20works.%20Notably%2C%20our%2030M%20model%20achieves%2082.75%25%20mAP%0Aon%20DOTA-v1.0%2C%20setting%20a%20new%20state-of-the-art%20record.Code%20is%20available%20at%0Ahttps%3A//github.com/YXB-NKU/Strip-R-CNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03775v3&entry.124074799=Read"},
{"title": "AI-powered virtual tissues from spatial proteomics for clinical\n  diagnostics and biomedical discovery", "author": "Johann Wenckstern and Eeshaan Jain and Kiril Vasilev and Matteo Pariset and Andreas Wicki and Gabriele Gut and Charlotte Bunne", "abstract": "  Spatial proteomics technologies have transformed our understanding of complex\ntissue architectures by enabling simultaneous analysis of multiple molecular\nmarkers and their spatial organization. The high dimensionality of these data,\nvarying marker combinations across experiments and heterogeneous study designs\npose unique challenges for computational analysis. Here, we present Virtual\nTissues (VirTues), a foundation model framework for biological tissues that\noperates across the molecular, cellular and tissue scale. VirTues introduces\ninnovations in transformer architecture design, including a novel tokenization\nscheme that captures both spatial and marker dimensions, and attention\nmechanisms that scale to high-dimensional multiplex data while maintaining\ninterpretability. Trained on diverse cancer and non-cancer tissue datasets,\nVirTues demonstrates strong generalization capabilities without task-specific\nfine-tuning, enabling cross-study analysis and novel marker integration. As a\ngeneralist model, VirTues outperforms existing approaches across clinical\ndiagnostics, biological discovery and patient case retrieval tasks, while\nproviding insights into tissue function and disease mechanisms.\n", "link": "http://arxiv.org/abs/2501.06039v1", "date": "2025-01-10", "relevancy": 2.6768, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5391}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5391}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-powered%20virtual%20tissues%20from%20spatial%20proteomics%20for%20clinical%0A%20%20diagnostics%20and%20biomedical%20discovery&body=Title%3A%20AI-powered%20virtual%20tissues%20from%20spatial%20proteomics%20for%20clinical%0A%20%20diagnostics%20and%20biomedical%20discovery%0AAuthor%3A%20Johann%20Wenckstern%20and%20Eeshaan%20Jain%20and%20Kiril%20Vasilev%20and%20Matteo%20Pariset%20and%20Andreas%20Wicki%20and%20Gabriele%20Gut%20and%20Charlotte%20Bunne%0AAbstract%3A%20%20%20Spatial%20proteomics%20technologies%20have%20transformed%20our%20understanding%20of%20complex%0Atissue%20architectures%20by%20enabling%20simultaneous%20analysis%20of%20multiple%20molecular%0Amarkers%20and%20their%20spatial%20organization.%20The%20high%20dimensionality%20of%20these%20data%2C%0Avarying%20marker%20combinations%20across%20experiments%20and%20heterogeneous%20study%20designs%0Apose%20unique%20challenges%20for%20computational%20analysis.%20Here%2C%20we%20present%20Virtual%0ATissues%20%28VirTues%29%2C%20a%20foundation%20model%20framework%20for%20biological%20tissues%20that%0Aoperates%20across%20the%20molecular%2C%20cellular%20and%20tissue%20scale.%20VirTues%20introduces%0Ainnovations%20in%20transformer%20architecture%20design%2C%20including%20a%20novel%20tokenization%0Ascheme%20that%20captures%20both%20spatial%20and%20marker%20dimensions%2C%20and%20attention%0Amechanisms%20that%20scale%20to%20high-dimensional%20multiplex%20data%20while%20maintaining%0Ainterpretability.%20Trained%20on%20diverse%20cancer%20and%20non-cancer%20tissue%20datasets%2C%0AVirTues%20demonstrates%20strong%20generalization%20capabilities%20without%20task-specific%0Afine-tuning%2C%20enabling%20cross-study%20analysis%20and%20novel%20marker%20integration.%20As%20a%0Ageneralist%20model%2C%20VirTues%20outperforms%20existing%20approaches%20across%20clinical%0Adiagnostics%2C%20biological%20discovery%20and%20patient%20case%20retrieval%20tasks%2C%20while%0Aproviding%20insights%20into%20tissue%20function%20and%20disease%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-powered%2520virtual%2520tissues%2520from%2520spatial%2520proteomics%2520for%2520clinical%250A%2520%2520diagnostics%2520and%2520biomedical%2520discovery%26entry.906535625%3DJohann%2520Wenckstern%2520and%2520Eeshaan%2520Jain%2520and%2520Kiril%2520Vasilev%2520and%2520Matteo%2520Pariset%2520and%2520Andreas%2520Wicki%2520and%2520Gabriele%2520Gut%2520and%2520Charlotte%2520Bunne%26entry.1292438233%3D%2520%2520Spatial%2520proteomics%2520technologies%2520have%2520transformed%2520our%2520understanding%2520of%2520complex%250Atissue%2520architectures%2520by%2520enabling%2520simultaneous%2520analysis%2520of%2520multiple%2520molecular%250Amarkers%2520and%2520their%2520spatial%2520organization.%2520The%2520high%2520dimensionality%2520of%2520these%2520data%252C%250Avarying%2520marker%2520combinations%2520across%2520experiments%2520and%2520heterogeneous%2520study%2520designs%250Apose%2520unique%2520challenges%2520for%2520computational%2520analysis.%2520Here%252C%2520we%2520present%2520Virtual%250ATissues%2520%2528VirTues%2529%252C%2520a%2520foundation%2520model%2520framework%2520for%2520biological%2520tissues%2520that%250Aoperates%2520across%2520the%2520molecular%252C%2520cellular%2520and%2520tissue%2520scale.%2520VirTues%2520introduces%250Ainnovations%2520in%2520transformer%2520architecture%2520design%252C%2520including%2520a%2520novel%2520tokenization%250Ascheme%2520that%2520captures%2520both%2520spatial%2520and%2520marker%2520dimensions%252C%2520and%2520attention%250Amechanisms%2520that%2520scale%2520to%2520high-dimensional%2520multiplex%2520data%2520while%2520maintaining%250Ainterpretability.%2520Trained%2520on%2520diverse%2520cancer%2520and%2520non-cancer%2520tissue%2520datasets%252C%250AVirTues%2520demonstrates%2520strong%2520generalization%2520capabilities%2520without%2520task-specific%250Afine-tuning%252C%2520enabling%2520cross-study%2520analysis%2520and%2520novel%2520marker%2520integration.%2520As%2520a%250Ageneralist%2520model%252C%2520VirTues%2520outperforms%2520existing%2520approaches%2520across%2520clinical%250Adiagnostics%252C%2520biological%2520discovery%2520and%2520patient%2520case%2520retrieval%2520tasks%252C%2520while%250Aproviding%2520insights%2520into%2520tissue%2520function%2520and%2520disease%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-powered%20virtual%20tissues%20from%20spatial%20proteomics%20for%20clinical%0A%20%20diagnostics%20and%20biomedical%20discovery&entry.906535625=Johann%20Wenckstern%20and%20Eeshaan%20Jain%20and%20Kiril%20Vasilev%20and%20Matteo%20Pariset%20and%20Andreas%20Wicki%20and%20Gabriele%20Gut%20and%20Charlotte%20Bunne&entry.1292438233=%20%20Spatial%20proteomics%20technologies%20have%20transformed%20our%20understanding%20of%20complex%0Atissue%20architectures%20by%20enabling%20simultaneous%20analysis%20of%20multiple%20molecular%0Amarkers%20and%20their%20spatial%20organization.%20The%20high%20dimensionality%20of%20these%20data%2C%0Avarying%20marker%20combinations%20across%20experiments%20and%20heterogeneous%20study%20designs%0Apose%20unique%20challenges%20for%20computational%20analysis.%20Here%2C%20we%20present%20Virtual%0ATissues%20%28VirTues%29%2C%20a%20foundation%20model%20framework%20for%20biological%20tissues%20that%0Aoperates%20across%20the%20molecular%2C%20cellular%20and%20tissue%20scale.%20VirTues%20introduces%0Ainnovations%20in%20transformer%20architecture%20design%2C%20including%20a%20novel%20tokenization%0Ascheme%20that%20captures%20both%20spatial%20and%20marker%20dimensions%2C%20and%20attention%0Amechanisms%20that%20scale%20to%20high-dimensional%20multiplex%20data%20while%20maintaining%0Ainterpretability.%20Trained%20on%20diverse%20cancer%20and%20non-cancer%20tissue%20datasets%2C%0AVirTues%20demonstrates%20strong%20generalization%20capabilities%20without%20task-specific%0Afine-tuning%2C%20enabling%20cross-study%20analysis%20and%20novel%20marker%20integration.%20As%20a%0Ageneralist%20model%2C%20VirTues%20outperforms%20existing%20approaches%20across%20clinical%0Adiagnostics%2C%20biological%20discovery%20and%20patient%20case%20retrieval%20tasks%2C%20while%0Aproviding%20insights%20into%20tissue%20function%20and%20disease%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06039v1&entry.124074799=Read"},
{"title": "Enhancing, Refining, and Fusing: Towards Robust Multi-Scale and Dense\n  Ship Detection", "author": "Congxia Zhao and Xiongjun Fu and Jian Dong and Shen Cao and Chunyan Zhang", "abstract": "  Synthetic aperture radar (SAR) imaging, celebrated for its high resolution,\nall-weather capability, and day-night operability, is indispensable for\nmaritime applications. However, ship detection in SAR imagery faces significant\nchallenges, including complex backgrounds, densely arranged targets, and large\nscale variations. To address these issues, we propose a novel framework,\nCenter-Aware SAR Ship Detector (CASS-Det), designed for robust multi-scale and\ndensely packed ship detection. CASS-Det integrates three key innovations: (1) a\ncenter enhancement module (CEM) that employs rotational convolution to\nemphasize ship centers, improving localization while suppressing background\ninterference; (2) a neighbor attention module (NAM) that leverages cross-layer\ndependencies to refine ship boundaries in densely populated scenes; and (3) a\ncross-connected feature pyramid network (CC-FPN) that enhances multi-scale\nfeature fusion by integrating shallow and deep features. Extensive experiments\non the SSDD, HRSID, and LS-SSDD-v1.0 datasets demonstrate the state-of-the-art\nperformance of CASS-Det, excelling at detecting multi-scale and densely\narranged ships.\n", "link": "http://arxiv.org/abs/2501.06053v1", "date": "2025-01-10", "relevancy": 2.5995, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%2C%20Refining%2C%20and%20Fusing%3A%20Towards%20Robust%20Multi-Scale%20and%20Dense%0A%20%20Ship%20Detection&body=Title%3A%20Enhancing%2C%20Refining%2C%20and%20Fusing%3A%20Towards%20Robust%20Multi-Scale%20and%20Dense%0A%20%20Ship%20Detection%0AAuthor%3A%20Congxia%20Zhao%20and%20Xiongjun%20Fu%20and%20Jian%20Dong%20and%20Shen%20Cao%20and%20Chunyan%20Zhang%0AAbstract%3A%20%20%20Synthetic%20aperture%20radar%20%28SAR%29%20imaging%2C%20celebrated%20for%20its%20high%20resolution%2C%0Aall-weather%20capability%2C%20and%20day-night%20operability%2C%20is%20indispensable%20for%0Amaritime%20applications.%20However%2C%20ship%20detection%20in%20SAR%20imagery%20faces%20significant%0Achallenges%2C%20including%20complex%20backgrounds%2C%20densely%20arranged%20targets%2C%20and%20large%0Ascale%20variations.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20framework%2C%0ACenter-Aware%20SAR%20Ship%20Detector%20%28CASS-Det%29%2C%20designed%20for%20robust%20multi-scale%20and%0Adensely%20packed%20ship%20detection.%20CASS-Det%20integrates%20three%20key%20innovations%3A%20%281%29%20a%0Acenter%20enhancement%20module%20%28CEM%29%20that%20employs%20rotational%20convolution%20to%0Aemphasize%20ship%20centers%2C%20improving%20localization%20while%20suppressing%20background%0Ainterference%3B%20%282%29%20a%20neighbor%20attention%20module%20%28NAM%29%20that%20leverages%20cross-layer%0Adependencies%20to%20refine%20ship%20boundaries%20in%20densely%20populated%20scenes%3B%20and%20%283%29%20a%0Across-connected%20feature%20pyramid%20network%20%28CC-FPN%29%20that%20enhances%20multi-scale%0Afeature%20fusion%20by%20integrating%20shallow%20and%20deep%20features.%20Extensive%20experiments%0Aon%20the%20SSDD%2C%20HRSID%2C%20and%20LS-SSDD-v1.0%20datasets%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20CASS-Det%2C%20excelling%20at%20detecting%20multi-scale%20and%20densely%0Aarranged%20ships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%252C%2520Refining%252C%2520and%2520Fusing%253A%2520Towards%2520Robust%2520Multi-Scale%2520and%2520Dense%250A%2520%2520Ship%2520Detection%26entry.906535625%3DCongxia%2520Zhao%2520and%2520Xiongjun%2520Fu%2520and%2520Jian%2520Dong%2520and%2520Shen%2520Cao%2520and%2520Chunyan%2520Zhang%26entry.1292438233%3D%2520%2520Synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520imaging%252C%2520celebrated%2520for%2520its%2520high%2520resolution%252C%250Aall-weather%2520capability%252C%2520and%2520day-night%2520operability%252C%2520is%2520indispensable%2520for%250Amaritime%2520applications.%2520However%252C%2520ship%2520detection%2520in%2520SAR%2520imagery%2520faces%2520significant%250Achallenges%252C%2520including%2520complex%2520backgrounds%252C%2520densely%2520arranged%2520targets%252C%2520and%2520large%250Ascale%2520variations.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%250ACenter-Aware%2520SAR%2520Ship%2520Detector%2520%2528CASS-Det%2529%252C%2520designed%2520for%2520robust%2520multi-scale%2520and%250Adensely%2520packed%2520ship%2520detection.%2520CASS-Det%2520integrates%2520three%2520key%2520innovations%253A%2520%25281%2529%2520a%250Acenter%2520enhancement%2520module%2520%2528CEM%2529%2520that%2520employs%2520rotational%2520convolution%2520to%250Aemphasize%2520ship%2520centers%252C%2520improving%2520localization%2520while%2520suppressing%2520background%250Ainterference%253B%2520%25282%2529%2520a%2520neighbor%2520attention%2520module%2520%2528NAM%2529%2520that%2520leverages%2520cross-layer%250Adependencies%2520to%2520refine%2520ship%2520boundaries%2520in%2520densely%2520populated%2520scenes%253B%2520and%2520%25283%2529%2520a%250Across-connected%2520feature%2520pyramid%2520network%2520%2528CC-FPN%2529%2520that%2520enhances%2520multi-scale%250Afeature%2520fusion%2520by%2520integrating%2520shallow%2520and%2520deep%2520features.%2520Extensive%2520experiments%250Aon%2520the%2520SSDD%252C%2520HRSID%252C%2520and%2520LS-SSDD-v1.0%2520datasets%2520demonstrate%2520the%2520state-of-the-art%250Aperformance%2520of%2520CASS-Det%252C%2520excelling%2520at%2520detecting%2520multi-scale%2520and%2520densely%250Aarranged%2520ships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%2C%20Refining%2C%20and%20Fusing%3A%20Towards%20Robust%20Multi-Scale%20and%20Dense%0A%20%20Ship%20Detection&entry.906535625=Congxia%20Zhao%20and%20Xiongjun%20Fu%20and%20Jian%20Dong%20and%20Shen%20Cao%20and%20Chunyan%20Zhang&entry.1292438233=%20%20Synthetic%20aperture%20radar%20%28SAR%29%20imaging%2C%20celebrated%20for%20its%20high%20resolution%2C%0Aall-weather%20capability%2C%20and%20day-night%20operability%2C%20is%20indispensable%20for%0Amaritime%20applications.%20However%2C%20ship%20detection%20in%20SAR%20imagery%20faces%20significant%0Achallenges%2C%20including%20complex%20backgrounds%2C%20densely%20arranged%20targets%2C%20and%20large%0Ascale%20variations.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20framework%2C%0ACenter-Aware%20SAR%20Ship%20Detector%20%28CASS-Det%29%2C%20designed%20for%20robust%20multi-scale%20and%0Adensely%20packed%20ship%20detection.%20CASS-Det%20integrates%20three%20key%20innovations%3A%20%281%29%20a%0Acenter%20enhancement%20module%20%28CEM%29%20that%20employs%20rotational%20convolution%20to%0Aemphasize%20ship%20centers%2C%20improving%20localization%20while%20suppressing%20background%0Ainterference%3B%20%282%29%20a%20neighbor%20attention%20module%20%28NAM%29%20that%20leverages%20cross-layer%0Adependencies%20to%20refine%20ship%20boundaries%20in%20densely%20populated%20scenes%3B%20and%20%283%29%20a%0Across-connected%20feature%20pyramid%20network%20%28CC-FPN%29%20that%20enhances%20multi-scale%0Afeature%20fusion%20by%20integrating%20shallow%20and%20deep%20features.%20Extensive%20experiments%0Aon%20the%20SSDD%2C%20HRSID%2C%20and%20LS-SSDD-v1.0%20datasets%20demonstrate%20the%20state-of-the-art%0Aperformance%20of%20CASS-Det%2C%20excelling%20at%20detecting%20multi-scale%20and%20densely%0Aarranged%20ships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06053v1&entry.124074799=Read"},
{"title": "Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact\n  Convolutional Transformers and SAM2", "author": "Olivier Morelle and Justus Bisten and Maximilian W. M. Wintergerst and Robert P. Finger and Thomas Schultz", "abstract": "  Weakly supervised segmentation has the potential to greatly reduce the\nannotation effort for training segmentation models for small structures such as\nhyper-reflective foci (HRF) in optical coherence tomography (OCT). However,\nmost weakly supervised methods either involve a strong downsampling of input\nimages, or only achieve localization at a coarse resolution, both of which are\nunsatisfactory for small structures. We propose a novel framework that\nincreases the spatial resolution of a traditional attention-based Multiple\nInstance Learning (MIL) approach by using Layer-wise Relevance Propagation\n(LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with\niterative inference. Moreover, we demonstrate that replacing MIL with a Compact\nConvolutional Transformer (CCT), which adds a positional encoding, and permits\nan exchange of information between different regions of the OCT image, leads to\na further and substantial increase in segmentation accuracy.\n", "link": "http://arxiv.org/abs/2501.05933v1", "date": "2025-01-10", "relevancy": 2.5983, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5228}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Segmentation%20of%20Hyper-Reflective%20Foci%20with%20Compact%0A%20%20Convolutional%20Transformers%20and%20SAM2&body=Title%3A%20Weakly%20Supervised%20Segmentation%20of%20Hyper-Reflective%20Foci%20with%20Compact%0A%20%20Convolutional%20Transformers%20and%20SAM2%0AAuthor%3A%20Olivier%20Morelle%20and%20Justus%20Bisten%20and%20Maximilian%20W.%20M.%20Wintergerst%20and%20Robert%20P.%20Finger%20and%20Thomas%20Schultz%0AAbstract%3A%20%20%20Weakly%20supervised%20segmentation%20has%20the%20potential%20to%20greatly%20reduce%20the%0Aannotation%20effort%20for%20training%20segmentation%20models%20for%20small%20structures%20such%20as%0Ahyper-reflective%20foci%20%28HRF%29%20in%20optical%20coherence%20tomography%20%28OCT%29.%20However%2C%0Amost%20weakly%20supervised%20methods%20either%20involve%20a%20strong%20downsampling%20of%20input%0Aimages%2C%20or%20only%20achieve%20localization%20at%20a%20coarse%20resolution%2C%20both%20of%20which%20are%0Aunsatisfactory%20for%20small%20structures.%20We%20propose%20a%20novel%20framework%20that%0Aincreases%20the%20spatial%20resolution%20of%20a%20traditional%20attention-based%20Multiple%0AInstance%20Learning%20%28MIL%29%20approach%20by%20using%20Layer-wise%20Relevance%20Propagation%0A%28LRP%29%20to%20prompt%20the%20Segment%20Anything%20Model%20%28SAM~2%29%2C%20and%20increases%20recall%20with%0Aiterative%20inference.%20Moreover%2C%20we%20demonstrate%20that%20replacing%20MIL%20with%20a%20Compact%0AConvolutional%20Transformer%20%28CCT%29%2C%20which%20adds%20a%20positional%20encoding%2C%20and%20permits%0Aan%20exchange%20of%20information%20between%20different%20regions%20of%20the%20OCT%20image%2C%20leads%20to%0Aa%20further%20and%20substantial%20increase%20in%20segmentation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Segmentation%2520of%2520Hyper-Reflective%2520Foci%2520with%2520Compact%250A%2520%2520Convolutional%2520Transformers%2520and%2520SAM2%26entry.906535625%3DOlivier%2520Morelle%2520and%2520Justus%2520Bisten%2520and%2520Maximilian%2520W.%2520M.%2520Wintergerst%2520and%2520Robert%2520P.%2520Finger%2520and%2520Thomas%2520Schultz%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520segmentation%2520has%2520the%2520potential%2520to%2520greatly%2520reduce%2520the%250Aannotation%2520effort%2520for%2520training%2520segmentation%2520models%2520for%2520small%2520structures%2520such%2520as%250Ahyper-reflective%2520foci%2520%2528HRF%2529%2520in%2520optical%2520coherence%2520tomography%2520%2528OCT%2529.%2520However%252C%250Amost%2520weakly%2520supervised%2520methods%2520either%2520involve%2520a%2520strong%2520downsampling%2520of%2520input%250Aimages%252C%2520or%2520only%2520achieve%2520localization%2520at%2520a%2520coarse%2520resolution%252C%2520both%2520of%2520which%2520are%250Aunsatisfactory%2520for%2520small%2520structures.%2520We%2520propose%2520a%2520novel%2520framework%2520that%250Aincreases%2520the%2520spatial%2520resolution%2520of%2520a%2520traditional%2520attention-based%2520Multiple%250AInstance%2520Learning%2520%2528MIL%2529%2520approach%2520by%2520using%2520Layer-wise%2520Relevance%2520Propagation%250A%2528LRP%2529%2520to%2520prompt%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM~2%2529%252C%2520and%2520increases%2520recall%2520with%250Aiterative%2520inference.%2520Moreover%252C%2520we%2520demonstrate%2520that%2520replacing%2520MIL%2520with%2520a%2520Compact%250AConvolutional%2520Transformer%2520%2528CCT%2529%252C%2520which%2520adds%2520a%2520positional%2520encoding%252C%2520and%2520permits%250Aan%2520exchange%2520of%2520information%2520between%2520different%2520regions%2520of%2520the%2520OCT%2520image%252C%2520leads%2520to%250Aa%2520further%2520and%2520substantial%2520increase%2520in%2520segmentation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Segmentation%20of%20Hyper-Reflective%20Foci%20with%20Compact%0A%20%20Convolutional%20Transformers%20and%20SAM2&entry.906535625=Olivier%20Morelle%20and%20Justus%20Bisten%20and%20Maximilian%20W.%20M.%20Wintergerst%20and%20Robert%20P.%20Finger%20and%20Thomas%20Schultz&entry.1292438233=%20%20Weakly%20supervised%20segmentation%20has%20the%20potential%20to%20greatly%20reduce%20the%0Aannotation%20effort%20for%20training%20segmentation%20models%20for%20small%20structures%20such%20as%0Ahyper-reflective%20foci%20%28HRF%29%20in%20optical%20coherence%20tomography%20%28OCT%29.%20However%2C%0Amost%20weakly%20supervised%20methods%20either%20involve%20a%20strong%20downsampling%20of%20input%0Aimages%2C%20or%20only%20achieve%20localization%20at%20a%20coarse%20resolution%2C%20both%20of%20which%20are%0Aunsatisfactory%20for%20small%20structures.%20We%20propose%20a%20novel%20framework%20that%0Aincreases%20the%20spatial%20resolution%20of%20a%20traditional%20attention-based%20Multiple%0AInstance%20Learning%20%28MIL%29%20approach%20by%20using%20Layer-wise%20Relevance%20Propagation%0A%28LRP%29%20to%20prompt%20the%20Segment%20Anything%20Model%20%28SAM~2%29%2C%20and%20increases%20recall%20with%0Aiterative%20inference.%20Moreover%2C%20we%20demonstrate%20that%20replacing%20MIL%20with%20a%20Compact%0AConvolutional%20Transformer%20%28CCT%29%2C%20which%20adds%20a%20positional%20encoding%2C%20and%20permits%0Aan%20exchange%20of%20information%20between%20different%20regions%20of%20the%20OCT%20image%2C%20leads%20to%0Aa%20further%20and%20substantial%20increase%20in%20segmentation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05933v1&entry.124074799=Read"},
{"title": "MSCViT: A Small-size ViT architecture with Multi-Scale Self-Attention\n  Mechanism for Tiny Datasets", "author": "Bowei Zhang and Yi Zhang", "abstract": "  Vision Transformer (ViT) has demonstrated significant potential in various\nvision tasks due to its strong ability in modelling long-range dependencies.\nHowever, such success is largely fueled by training on massive samples. In real\napplications, the large-scale datasets are not always available, and ViT\nperforms worse than Convolutional Neural Networks (CNNs) if it is only trained\non small scale dataset (called tiny dataset), since it requires large amount of\ntraining data to ensure its representational capacity. In this paper, a\nsmall-size ViT architecture with multi-scale self-attention mechanism and\nconvolution blocks is presented (dubbed MSCViT) to model different scales of\nattention at each layer. Firstly, we introduced wavelet convolution, which\nselectively combines the high-frequency components obtained by frequency\ndivision with our convolution channel to extract local features. Then, a\nlightweight multi-head attention module is developed to reduce the number of\ntokens and computational costs. Finally, the positional encoding (PE) in the\nbackbone is replaced by a local feature extraction module. Compared with the\noriginal ViT, it is parameter-efficient and is particularly suitable for tiny\ndatasets. Extensive experiments have been conducted on tiny datasets, in which\nour model achieves an accuracy of 84.68% on CIFAR-100 with 14.0M parameters and\n2.5 GFLOPs, without pre-training on large datasets.\n", "link": "http://arxiv.org/abs/2501.06040v1", "date": "2025-01-10", "relevancy": 2.581, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5279}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5216}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSCViT%3A%20A%20Small-size%20ViT%20architecture%20with%20Multi-Scale%20Self-Attention%0A%20%20Mechanism%20for%20Tiny%20Datasets&body=Title%3A%20MSCViT%3A%20A%20Small-size%20ViT%20architecture%20with%20Multi-Scale%20Self-Attention%0A%20%20Mechanism%20for%20Tiny%20Datasets%0AAuthor%3A%20Bowei%20Zhang%20and%20Yi%20Zhang%0AAbstract%3A%20%20%20Vision%20Transformer%20%28ViT%29%20has%20demonstrated%20significant%20potential%20in%20various%0Avision%20tasks%20due%20to%20its%20strong%20ability%20in%20modelling%20long-range%20dependencies.%0AHowever%2C%20such%20success%20is%20largely%20fueled%20by%20training%20on%20massive%20samples.%20In%20real%0Aapplications%2C%20the%20large-scale%20datasets%20are%20not%20always%20available%2C%20and%20ViT%0Aperforms%20worse%20than%20Convolutional%20Neural%20Networks%20%28CNNs%29%20if%20it%20is%20only%20trained%0Aon%20small%20scale%20dataset%20%28called%20tiny%20dataset%29%2C%20since%20it%20requires%20large%20amount%20of%0Atraining%20data%20to%20ensure%20its%20representational%20capacity.%20In%20this%20paper%2C%20a%0Asmall-size%20ViT%20architecture%20with%20multi-scale%20self-attention%20mechanism%20and%0Aconvolution%20blocks%20is%20presented%20%28dubbed%20MSCViT%29%20to%20model%20different%20scales%20of%0Aattention%20at%20each%20layer.%20Firstly%2C%20we%20introduced%20wavelet%20convolution%2C%20which%0Aselectively%20combines%20the%20high-frequency%20components%20obtained%20by%20frequency%0Adivision%20with%20our%20convolution%20channel%20to%20extract%20local%20features.%20Then%2C%20a%0Alightweight%20multi-head%20attention%20module%20is%20developed%20to%20reduce%20the%20number%20of%0Atokens%20and%20computational%20costs.%20Finally%2C%20the%20positional%20encoding%20%28PE%29%20in%20the%0Abackbone%20is%20replaced%20by%20a%20local%20feature%20extraction%20module.%20Compared%20with%20the%0Aoriginal%20ViT%2C%20it%20is%20parameter-efficient%20and%20is%20particularly%20suitable%20for%20tiny%0Adatasets.%20Extensive%20experiments%20have%20been%20conducted%20on%20tiny%20datasets%2C%20in%20which%0Aour%20model%20achieves%20an%20accuracy%20of%2084.68%25%20on%20CIFAR-100%20with%2014.0M%20parameters%20and%0A2.5%20GFLOPs%2C%20without%20pre-training%20on%20large%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSCViT%253A%2520A%2520Small-size%2520ViT%2520architecture%2520with%2520Multi-Scale%2520Self-Attention%250A%2520%2520Mechanism%2520for%2520Tiny%2520Datasets%26entry.906535625%3DBowei%2520Zhang%2520and%2520Yi%2520Zhang%26entry.1292438233%3D%2520%2520Vision%2520Transformer%2520%2528ViT%2529%2520has%2520demonstrated%2520significant%2520potential%2520in%2520various%250Avision%2520tasks%2520due%2520to%2520its%2520strong%2520ability%2520in%2520modelling%2520long-range%2520dependencies.%250AHowever%252C%2520such%2520success%2520is%2520largely%2520fueled%2520by%2520training%2520on%2520massive%2520samples.%2520In%2520real%250Aapplications%252C%2520the%2520large-scale%2520datasets%2520are%2520not%2520always%2520available%252C%2520and%2520ViT%250Aperforms%2520worse%2520than%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520if%2520it%2520is%2520only%2520trained%250Aon%2520small%2520scale%2520dataset%2520%2528called%2520tiny%2520dataset%2529%252C%2520since%2520it%2520requires%2520large%2520amount%2520of%250Atraining%2520data%2520to%2520ensure%2520its%2520representational%2520capacity.%2520In%2520this%2520paper%252C%2520a%250Asmall-size%2520ViT%2520architecture%2520with%2520multi-scale%2520self-attention%2520mechanism%2520and%250Aconvolution%2520blocks%2520is%2520presented%2520%2528dubbed%2520MSCViT%2529%2520to%2520model%2520different%2520scales%2520of%250Aattention%2520at%2520each%2520layer.%2520Firstly%252C%2520we%2520introduced%2520wavelet%2520convolution%252C%2520which%250Aselectively%2520combines%2520the%2520high-frequency%2520components%2520obtained%2520by%2520frequency%250Adivision%2520with%2520our%2520convolution%2520channel%2520to%2520extract%2520local%2520features.%2520Then%252C%2520a%250Alightweight%2520multi-head%2520attention%2520module%2520is%2520developed%2520to%2520reduce%2520the%2520number%2520of%250Atokens%2520and%2520computational%2520costs.%2520Finally%252C%2520the%2520positional%2520encoding%2520%2528PE%2529%2520in%2520the%250Abackbone%2520is%2520replaced%2520by%2520a%2520local%2520feature%2520extraction%2520module.%2520Compared%2520with%2520the%250Aoriginal%2520ViT%252C%2520it%2520is%2520parameter-efficient%2520and%2520is%2520particularly%2520suitable%2520for%2520tiny%250Adatasets.%2520Extensive%2520experiments%2520have%2520been%2520conducted%2520on%2520tiny%2520datasets%252C%2520in%2520which%250Aour%2520model%2520achieves%2520an%2520accuracy%2520of%252084.68%2525%2520on%2520CIFAR-100%2520with%252014.0M%2520parameters%2520and%250A2.5%2520GFLOPs%252C%2520without%2520pre-training%2520on%2520large%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSCViT%3A%20A%20Small-size%20ViT%20architecture%20with%20Multi-Scale%20Self-Attention%0A%20%20Mechanism%20for%20Tiny%20Datasets&entry.906535625=Bowei%20Zhang%20and%20Yi%20Zhang&entry.1292438233=%20%20Vision%20Transformer%20%28ViT%29%20has%20demonstrated%20significant%20potential%20in%20various%0Avision%20tasks%20due%20to%20its%20strong%20ability%20in%20modelling%20long-range%20dependencies.%0AHowever%2C%20such%20success%20is%20largely%20fueled%20by%20training%20on%20massive%20samples.%20In%20real%0Aapplications%2C%20the%20large-scale%20datasets%20are%20not%20always%20available%2C%20and%20ViT%0Aperforms%20worse%20than%20Convolutional%20Neural%20Networks%20%28CNNs%29%20if%20it%20is%20only%20trained%0Aon%20small%20scale%20dataset%20%28called%20tiny%20dataset%29%2C%20since%20it%20requires%20large%20amount%20of%0Atraining%20data%20to%20ensure%20its%20representational%20capacity.%20In%20this%20paper%2C%20a%0Asmall-size%20ViT%20architecture%20with%20multi-scale%20self-attention%20mechanism%20and%0Aconvolution%20blocks%20is%20presented%20%28dubbed%20MSCViT%29%20to%20model%20different%20scales%20of%0Aattention%20at%20each%20layer.%20Firstly%2C%20we%20introduced%20wavelet%20convolution%2C%20which%0Aselectively%20combines%20the%20high-frequency%20components%20obtained%20by%20frequency%0Adivision%20with%20our%20convolution%20channel%20to%20extract%20local%20features.%20Then%2C%20a%0Alightweight%20multi-head%20attention%20module%20is%20developed%20to%20reduce%20the%20number%20of%0Atokens%20and%20computational%20costs.%20Finally%2C%20the%20positional%20encoding%20%28PE%29%20in%20the%0Abackbone%20is%20replaced%20by%20a%20local%20feature%20extraction%20module.%20Compared%20with%20the%0Aoriginal%20ViT%2C%20it%20is%20parameter-efficient%20and%20is%20particularly%20suitable%20for%20tiny%0Adatasets.%20Extensive%20experiments%20have%20been%20conducted%20on%20tiny%20datasets%2C%20in%20which%0Aour%20model%20achieves%20an%20accuracy%20of%2084.68%25%20on%20CIFAR-100%20with%2014.0M%20parameters%20and%0A2.5%20GFLOPs%2C%20without%20pre-training%20on%20large%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06040v1&entry.124074799=Read"},
{"title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  Attacks leveraging internal LLM states", "author": "Luis Ibanez-Lissen and Lorena Gonzalez-Manzano and Jose Maria de Fuentes and Nicolas Anciaux and Joaquin Garcia-Alfaro", "abstract": "  Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.\n", "link": "http://arxiv.org/abs/2411.19876v3", "date": "2025-01-10", "relevancy": 2.5802, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5176}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUMIA%3A%20Linear%20probing%20for%20Unimodal%20and%20MultiModal%20Membership%20Inference%0A%20%20Attacks%20leveraging%20internal%20LLM%20states&body=Title%3A%20LUMIA%3A%20Linear%20probing%20for%20Unimodal%20and%20MultiModal%20Membership%20Inference%0A%20%20Attacks%20leveraging%20internal%20LLM%20states%0AAuthor%3A%20Luis%20Ibanez-Lissen%20and%20Lorena%20Gonzalez-Manzano%20and%20Jose%20Maria%20de%20Fuentes%20and%20Nicolas%20Anciaux%20and%20Joaquin%20Garcia-Alfaro%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20a%20variety%20of%0Aapplications%2C%20but%20concerns%20around%20membership%20inference%20have%20grown%20in%20parallel.%0APrevious%20efforts%20focus%20on%20black-to-grey-box%20models%2C%20thus%20neglecting%20the%0Apotential%20benefit%20from%20internal%20LLM%20information.%20To%20address%20this%2C%20we%20propose%0Athe%20use%20of%20Linear%20Probes%20%28LPs%29%20as%20a%20method%20to%20detect%20Membership%20Inference%0AAttacks%20%28MIAs%29%20by%20examining%20internal%20activations%20of%20LLMs.%20Our%20approach%2C%20dubbed%0ALUMIA%2C%20applies%20LPs%20layer-by-layer%20to%20get%20fine-grained%20data%20on%20the%20model%20inner%0Aworkings.%20We%20test%20this%20method%20across%20several%20model%20architectures%2C%20sizes%20and%0Adatasets%2C%20including%20unimodal%20and%20multimodal%20tasks.%20In%20unimodal%20MIA%2C%20LUMIA%0Aachieves%20an%20average%20gain%20of%2015.71%20%25%20in%20Area%20Under%20the%20Curve%20%28AUC%29%20over%20previous%0Atechniques.%20Remarkably%2C%20LUMIA%20reaches%20AUC%3E60%25%20in%2065.33%25%20of%20cases%20--%20an%0Aincrement%20of%2046.80%25%20against%20the%20state%20of%20the%20art.%20Furthermore%2C%20our%20approach%0Areveals%20key%20insights%2C%20such%20as%20the%20model%20layers%20where%20MIAs%20are%20most%20detectable.%0AIn%20multimodal%20models%2C%20LPs%20indicate%20that%20visual%20inputs%20can%20significantly%0Acontribute%20to%20detect%20MIAs%20--%20AUC%3E60%25%20is%20reached%20in%2085.90%25%20of%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19876v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUMIA%253A%2520Linear%2520probing%2520for%2520Unimodal%2520and%2520MultiModal%2520Membership%2520Inference%250A%2520%2520Attacks%2520leveraging%2520internal%2520LLM%2520states%26entry.906535625%3DLuis%2520Ibanez-Lissen%2520and%2520Lorena%2520Gonzalez-Manzano%2520and%2520Jose%2520Maria%2520de%2520Fuentes%2520and%2520Nicolas%2520Anciaux%2520and%2520Joaquin%2520Garcia-Alfaro%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520in%2520a%2520variety%2520of%250Aapplications%252C%2520but%2520concerns%2520around%2520membership%2520inference%2520have%2520grown%2520in%2520parallel.%250APrevious%2520efforts%2520focus%2520on%2520black-to-grey-box%2520models%252C%2520thus%2520neglecting%2520the%250Apotential%2520benefit%2520from%2520internal%2520LLM%2520information.%2520To%2520address%2520this%252C%2520we%2520propose%250Athe%2520use%2520of%2520Linear%2520Probes%2520%2528LPs%2529%2520as%2520a%2520method%2520to%2520detect%2520Membership%2520Inference%250AAttacks%2520%2528MIAs%2529%2520by%2520examining%2520internal%2520activations%2520of%2520LLMs.%2520Our%2520approach%252C%2520dubbed%250ALUMIA%252C%2520applies%2520LPs%2520layer-by-layer%2520to%2520get%2520fine-grained%2520data%2520on%2520the%2520model%2520inner%250Aworkings.%2520We%2520test%2520this%2520method%2520across%2520several%2520model%2520architectures%252C%2520sizes%2520and%250Adatasets%252C%2520including%2520unimodal%2520and%2520multimodal%2520tasks.%2520In%2520unimodal%2520MIA%252C%2520LUMIA%250Aachieves%2520an%2520average%2520gain%2520of%252015.71%2520%2525%2520in%2520Area%2520Under%2520the%2520Curve%2520%2528AUC%2529%2520over%2520previous%250Atechniques.%2520Remarkably%252C%2520LUMIA%2520reaches%2520AUC%253E60%2525%2520in%252065.33%2525%2520of%2520cases%2520--%2520an%250Aincrement%2520of%252046.80%2525%2520against%2520the%2520state%2520of%2520the%2520art.%2520Furthermore%252C%2520our%2520approach%250Areveals%2520key%2520insights%252C%2520such%2520as%2520the%2520model%2520layers%2520where%2520MIAs%2520are%2520most%2520detectable.%250AIn%2520multimodal%2520models%252C%2520LPs%2520indicate%2520that%2520visual%2520inputs%2520can%2520significantly%250Acontribute%2520to%2520detect%2520MIAs%2520--%2520AUC%253E60%2525%2520is%2520reached%2520in%252085.90%2525%2520of%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19876v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUMIA%3A%20Linear%20probing%20for%20Unimodal%20and%20MultiModal%20Membership%20Inference%0A%20%20Attacks%20leveraging%20internal%20LLM%20states&entry.906535625=Luis%20Ibanez-Lissen%20and%20Lorena%20Gonzalez-Manzano%20and%20Jose%20Maria%20de%20Fuentes%20and%20Nicolas%20Anciaux%20and%20Joaquin%20Garcia-Alfaro&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20a%20variety%20of%0Aapplications%2C%20but%20concerns%20around%20membership%20inference%20have%20grown%20in%20parallel.%0APrevious%20efforts%20focus%20on%20black-to-grey-box%20models%2C%20thus%20neglecting%20the%0Apotential%20benefit%20from%20internal%20LLM%20information.%20To%20address%20this%2C%20we%20propose%0Athe%20use%20of%20Linear%20Probes%20%28LPs%29%20as%20a%20method%20to%20detect%20Membership%20Inference%0AAttacks%20%28MIAs%29%20by%20examining%20internal%20activations%20of%20LLMs.%20Our%20approach%2C%20dubbed%0ALUMIA%2C%20applies%20LPs%20layer-by-layer%20to%20get%20fine-grained%20data%20on%20the%20model%20inner%0Aworkings.%20We%20test%20this%20method%20across%20several%20model%20architectures%2C%20sizes%20and%0Adatasets%2C%20including%20unimodal%20and%20multimodal%20tasks.%20In%20unimodal%20MIA%2C%20LUMIA%0Aachieves%20an%20average%20gain%20of%2015.71%20%25%20in%20Area%20Under%20the%20Curve%20%28AUC%29%20over%20previous%0Atechniques.%20Remarkably%2C%20LUMIA%20reaches%20AUC%3E60%25%20in%2065.33%25%20of%20cases%20--%20an%0Aincrement%20of%2046.80%25%20against%20the%20state%20of%20the%20art.%20Furthermore%2C%20our%20approach%0Areveals%20key%20insights%2C%20such%20as%20the%20model%20layers%20where%20MIAs%20are%20most%20detectable.%0AIn%20multimodal%20models%2C%20LPs%20indicate%20that%20visual%20inputs%20can%20significantly%0Acontribute%20to%20detect%20MIAs%20--%20AUC%3E60%25%20is%20reached%20in%2085.90%25%20of%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19876v3&entry.124074799=Read"},
{"title": "Encoded Spatial Attribute in Multi-Tier Federated Learning", "author": "Asfia Kawnine and Francis Palma and Seyed Alireza Rahimi Azghadi and Hung Cao", "abstract": "  This research presents an Encoded Spatial Multi-Tier Federated Learning\napproach for a comprehensive evaluation of aggregated models for geospatial\ndata. In the client tier, encoding spatial information is introduced to better\npredict the target outcome. The research aims to assess the performance of\nthese models across diverse datasets and spatial attributes, highlighting\nvariations in predictive accuracy. Using evaluation metrics such as accuracy,\nour research reveals insights into the complexities of spatial granularity and\nthe challenges of capturing underlying patterns in the data. We extended the\nscope of federated learning (FL) by having multi-tier along with the\nfunctionality of encoding spatial attributes. Our N-tier FL approach used\nencoded spatial data to aggregate in different tiers. We obtained multiple\nmodels that predicted the different granularities of spatial data. Our findings\nunderscore the need for further research to improve predictive accuracy and\nmodel generalization, with potential avenues including incorporating additional\nfeatures, refining model architectures, and exploring alternative modeling\napproaches. Our experiments have several tiers representing different levels of\nspatial aspects. We obtained accuracy of 75.62% and 89.52% for the global model\nwithout having to train the model using the data constituted with the\ndesignated tier. The research also highlights the importance of the proposed\napproach in real-time applications.\n", "link": "http://arxiv.org/abs/2501.05934v1", "date": "2025-01-10", "relevancy": 2.5727, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Encoded%20Spatial%20Attribute%20in%20Multi-Tier%20Federated%20Learning&body=Title%3A%20Encoded%20Spatial%20Attribute%20in%20Multi-Tier%20Federated%20Learning%0AAuthor%3A%20Asfia%20Kawnine%20and%20Francis%20Palma%20and%20Seyed%20Alireza%20Rahimi%20Azghadi%20and%20Hung%20Cao%0AAbstract%3A%20%20%20This%20research%20presents%20an%20Encoded%20Spatial%20Multi-Tier%20Federated%20Learning%0Aapproach%20for%20a%20comprehensive%20evaluation%20of%20aggregated%20models%20for%20geospatial%0Adata.%20In%20the%20client%20tier%2C%20encoding%20spatial%20information%20is%20introduced%20to%20better%0Apredict%20the%20target%20outcome.%20The%20research%20aims%20to%20assess%20the%20performance%20of%0Athese%20models%20across%20diverse%20datasets%20and%20spatial%20attributes%2C%20highlighting%0Avariations%20in%20predictive%20accuracy.%20Using%20evaluation%20metrics%20such%20as%20accuracy%2C%0Aour%20research%20reveals%20insights%20into%20the%20complexities%20of%20spatial%20granularity%20and%0Athe%20challenges%20of%20capturing%20underlying%20patterns%20in%20the%20data.%20We%20extended%20the%0Ascope%20of%20federated%20learning%20%28FL%29%20by%20having%20multi-tier%20along%20with%20the%0Afunctionality%20of%20encoding%20spatial%20attributes.%20Our%20N-tier%20FL%20approach%20used%0Aencoded%20spatial%20data%20to%20aggregate%20in%20different%20tiers.%20We%20obtained%20multiple%0Amodels%20that%20predicted%20the%20different%20granularities%20of%20spatial%20data.%20Our%20findings%0Aunderscore%20the%20need%20for%20further%20research%20to%20improve%20predictive%20accuracy%20and%0Amodel%20generalization%2C%20with%20potential%20avenues%20including%20incorporating%20additional%0Afeatures%2C%20refining%20model%20architectures%2C%20and%20exploring%20alternative%20modeling%0Aapproaches.%20Our%20experiments%20have%20several%20tiers%20representing%20different%20levels%20of%0Aspatial%20aspects.%20We%20obtained%20accuracy%20of%2075.62%25%20and%2089.52%25%20for%20the%20global%20model%0Awithout%20having%20to%20train%20the%20model%20using%20the%20data%20constituted%20with%20the%0Adesignated%20tier.%20The%20research%20also%20highlights%20the%20importance%20of%20the%20proposed%0Aapproach%20in%20real-time%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEncoded%2520Spatial%2520Attribute%2520in%2520Multi-Tier%2520Federated%2520Learning%26entry.906535625%3DAsfia%2520Kawnine%2520and%2520Francis%2520Palma%2520and%2520Seyed%2520Alireza%2520Rahimi%2520Azghadi%2520and%2520Hung%2520Cao%26entry.1292438233%3D%2520%2520This%2520research%2520presents%2520an%2520Encoded%2520Spatial%2520Multi-Tier%2520Federated%2520Learning%250Aapproach%2520for%2520a%2520comprehensive%2520evaluation%2520of%2520aggregated%2520models%2520for%2520geospatial%250Adata.%2520In%2520the%2520client%2520tier%252C%2520encoding%2520spatial%2520information%2520is%2520introduced%2520to%2520better%250Apredict%2520the%2520target%2520outcome.%2520The%2520research%2520aims%2520to%2520assess%2520the%2520performance%2520of%250Athese%2520models%2520across%2520diverse%2520datasets%2520and%2520spatial%2520attributes%252C%2520highlighting%250Avariations%2520in%2520predictive%2520accuracy.%2520Using%2520evaluation%2520metrics%2520such%2520as%2520accuracy%252C%250Aour%2520research%2520reveals%2520insights%2520into%2520the%2520complexities%2520of%2520spatial%2520granularity%2520and%250Athe%2520challenges%2520of%2520capturing%2520underlying%2520patterns%2520in%2520the%2520data.%2520We%2520extended%2520the%250Ascope%2520of%2520federated%2520learning%2520%2528FL%2529%2520by%2520having%2520multi-tier%2520along%2520with%2520the%250Afunctionality%2520of%2520encoding%2520spatial%2520attributes.%2520Our%2520N-tier%2520FL%2520approach%2520used%250Aencoded%2520spatial%2520data%2520to%2520aggregate%2520in%2520different%2520tiers.%2520We%2520obtained%2520multiple%250Amodels%2520that%2520predicted%2520the%2520different%2520granularities%2520of%2520spatial%2520data.%2520Our%2520findings%250Aunderscore%2520the%2520need%2520for%2520further%2520research%2520to%2520improve%2520predictive%2520accuracy%2520and%250Amodel%2520generalization%252C%2520with%2520potential%2520avenues%2520including%2520incorporating%2520additional%250Afeatures%252C%2520refining%2520model%2520architectures%252C%2520and%2520exploring%2520alternative%2520modeling%250Aapproaches.%2520Our%2520experiments%2520have%2520several%2520tiers%2520representing%2520different%2520levels%2520of%250Aspatial%2520aspects.%2520We%2520obtained%2520accuracy%2520of%252075.62%2525%2520and%252089.52%2525%2520for%2520the%2520global%2520model%250Awithout%2520having%2520to%2520train%2520the%2520model%2520using%2520the%2520data%2520constituted%2520with%2520the%250Adesignated%2520tier.%2520The%2520research%2520also%2520highlights%2520the%2520importance%2520of%2520the%2520proposed%250Aapproach%2520in%2520real-time%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Encoded%20Spatial%20Attribute%20in%20Multi-Tier%20Federated%20Learning&entry.906535625=Asfia%20Kawnine%20and%20Francis%20Palma%20and%20Seyed%20Alireza%20Rahimi%20Azghadi%20and%20Hung%20Cao&entry.1292438233=%20%20This%20research%20presents%20an%20Encoded%20Spatial%20Multi-Tier%20Federated%20Learning%0Aapproach%20for%20a%20comprehensive%20evaluation%20of%20aggregated%20models%20for%20geospatial%0Adata.%20In%20the%20client%20tier%2C%20encoding%20spatial%20information%20is%20introduced%20to%20better%0Apredict%20the%20target%20outcome.%20The%20research%20aims%20to%20assess%20the%20performance%20of%0Athese%20models%20across%20diverse%20datasets%20and%20spatial%20attributes%2C%20highlighting%0Avariations%20in%20predictive%20accuracy.%20Using%20evaluation%20metrics%20such%20as%20accuracy%2C%0Aour%20research%20reveals%20insights%20into%20the%20complexities%20of%20spatial%20granularity%20and%0Athe%20challenges%20of%20capturing%20underlying%20patterns%20in%20the%20data.%20We%20extended%20the%0Ascope%20of%20federated%20learning%20%28FL%29%20by%20having%20multi-tier%20along%20with%20the%0Afunctionality%20of%20encoding%20spatial%20attributes.%20Our%20N-tier%20FL%20approach%20used%0Aencoded%20spatial%20data%20to%20aggregate%20in%20different%20tiers.%20We%20obtained%20multiple%0Amodels%20that%20predicted%20the%20different%20granularities%20of%20spatial%20data.%20Our%20findings%0Aunderscore%20the%20need%20for%20further%20research%20to%20improve%20predictive%20accuracy%20and%0Amodel%20generalization%2C%20with%20potential%20avenues%20including%20incorporating%20additional%0Afeatures%2C%20refining%20model%20architectures%2C%20and%20exploring%20alternative%20modeling%0Aapproaches.%20Our%20experiments%20have%20several%20tiers%20representing%20different%20levels%20of%0Aspatial%20aspects.%20We%20obtained%20accuracy%20of%2075.62%25%20and%2089.52%25%20for%20the%20global%20model%0Awithout%20having%20to%20train%20the%20model%20using%20the%20data%20constituted%20with%20the%0Adesignated%20tier.%20The%20research%20also%20highlights%20the%20importance%20of%20the%20proposed%0Aapproach%20in%20real-time%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05934v1&entry.124074799=Read"},
{"title": "BIV-Priv-Seg: Locating Private Content in Images Taken by People With\n  Visual Impairments", "author": "Yu-Yun Tseng and Tanusree Sharma and Lotus Zhang and Abigale Stangl and Leah Findlater and Yang Wang and Danna Gurari", "abstract": "  Individuals who are blind or have low vision (BLV) are at a heightened risk\nof sharing private information if they share photographs they have taken. To\nfacilitate developing technologies that can help them preserve privacy, we\nintroduce BIV-Priv-Seg, the first localization dataset originating from people\nwith visual impairments that shows private content. It contains 1,028 images\nwith segmentation annotations for 16 private object categories. We first\ncharacterize BIV-Priv-Seg and then evaluate modern models' performance for\nlocating private content in the dataset. We find modern models struggle most\nwith locating private objects that are not salient, small, and lack text as\nwell as recognizing when private content is absent from an image. We facilitate\nfuture extensions by sharing our new dataset with the evaluation server at\nhttps://vizwiz.org/tasks-and-datasets/object-localization.\n", "link": "http://arxiv.org/abs/2407.18243v3", "date": "2025-01-10", "relevancy": 2.5181, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIV-Priv-Seg%3A%20Locating%20Private%20Content%20in%20Images%20Taken%20by%20People%20With%0A%20%20Visual%20Impairments&body=Title%3A%20BIV-Priv-Seg%3A%20Locating%20Private%20Content%20in%20Images%20Taken%20by%20People%20With%0A%20%20Visual%20Impairments%0AAuthor%3A%20Yu-Yun%20Tseng%20and%20Tanusree%20Sharma%20and%20Lotus%20Zhang%20and%20Abigale%20Stangl%20and%20Leah%20Findlater%20and%20Yang%20Wang%20and%20Danna%20Gurari%0AAbstract%3A%20%20%20Individuals%20who%20are%20blind%20or%20have%20low%20vision%20%28BLV%29%20are%20at%20a%20heightened%20risk%0Aof%20sharing%20private%20information%20if%20they%20share%20photographs%20they%20have%20taken.%20To%0Afacilitate%20developing%20technologies%20that%20can%20help%20them%20preserve%20privacy%2C%20we%0Aintroduce%20BIV-Priv-Seg%2C%20the%20first%20localization%20dataset%20originating%20from%20people%0Awith%20visual%20impairments%20that%20shows%20private%20content.%20It%20contains%201%2C028%20images%0Awith%20segmentation%20annotations%20for%2016%20private%20object%20categories.%20We%20first%0Acharacterize%20BIV-Priv-Seg%20and%20then%20evaluate%20modern%20models%27%20performance%20for%0Alocating%20private%20content%20in%20the%20dataset.%20We%20find%20modern%20models%20struggle%20most%0Awith%20locating%20private%20objects%20that%20are%20not%20salient%2C%20small%2C%20and%20lack%20text%20as%0Awell%20as%20recognizing%20when%20private%20content%20is%20absent%20from%20an%20image.%20We%20facilitate%0Afuture%20extensions%20by%20sharing%20our%20new%20dataset%20with%20the%20evaluation%20server%20at%0Ahttps%3A//vizwiz.org/tasks-and-datasets/object-localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18243v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIV-Priv-Seg%253A%2520Locating%2520Private%2520Content%2520in%2520Images%2520Taken%2520by%2520People%2520With%250A%2520%2520Visual%2520Impairments%26entry.906535625%3DYu-Yun%2520Tseng%2520and%2520Tanusree%2520Sharma%2520and%2520Lotus%2520Zhang%2520and%2520Abigale%2520Stangl%2520and%2520Leah%2520Findlater%2520and%2520Yang%2520Wang%2520and%2520Danna%2520Gurari%26entry.1292438233%3D%2520%2520Individuals%2520who%2520are%2520blind%2520or%2520have%2520low%2520vision%2520%2528BLV%2529%2520are%2520at%2520a%2520heightened%2520risk%250Aof%2520sharing%2520private%2520information%2520if%2520they%2520share%2520photographs%2520they%2520have%2520taken.%2520To%250Afacilitate%2520developing%2520technologies%2520that%2520can%2520help%2520them%2520preserve%2520privacy%252C%2520we%250Aintroduce%2520BIV-Priv-Seg%252C%2520the%2520first%2520localization%2520dataset%2520originating%2520from%2520people%250Awith%2520visual%2520impairments%2520that%2520shows%2520private%2520content.%2520It%2520contains%25201%252C028%2520images%250Awith%2520segmentation%2520annotations%2520for%252016%2520private%2520object%2520categories.%2520We%2520first%250Acharacterize%2520BIV-Priv-Seg%2520and%2520then%2520evaluate%2520modern%2520models%2527%2520performance%2520for%250Alocating%2520private%2520content%2520in%2520the%2520dataset.%2520We%2520find%2520modern%2520models%2520struggle%2520most%250Awith%2520locating%2520private%2520objects%2520that%2520are%2520not%2520salient%252C%2520small%252C%2520and%2520lack%2520text%2520as%250Awell%2520as%2520recognizing%2520when%2520private%2520content%2520is%2520absent%2520from%2520an%2520image.%2520We%2520facilitate%250Afuture%2520extensions%2520by%2520sharing%2520our%2520new%2520dataset%2520with%2520the%2520evaluation%2520server%2520at%250Ahttps%253A//vizwiz.org/tasks-and-datasets/object-localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18243v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIV-Priv-Seg%3A%20Locating%20Private%20Content%20in%20Images%20Taken%20by%20People%20With%0A%20%20Visual%20Impairments&entry.906535625=Yu-Yun%20Tseng%20and%20Tanusree%20Sharma%20and%20Lotus%20Zhang%20and%20Abigale%20Stangl%20and%20Leah%20Findlater%20and%20Yang%20Wang%20and%20Danna%20Gurari&entry.1292438233=%20%20Individuals%20who%20are%20blind%20or%20have%20low%20vision%20%28BLV%29%20are%20at%20a%20heightened%20risk%0Aof%20sharing%20private%20information%20if%20they%20share%20photographs%20they%20have%20taken.%20To%0Afacilitate%20developing%20technologies%20that%20can%20help%20them%20preserve%20privacy%2C%20we%0Aintroduce%20BIV-Priv-Seg%2C%20the%20first%20localization%20dataset%20originating%20from%20people%0Awith%20visual%20impairments%20that%20shows%20private%20content.%20It%20contains%201%2C028%20images%0Awith%20segmentation%20annotations%20for%2016%20private%20object%20categories.%20We%20first%0Acharacterize%20BIV-Priv-Seg%20and%20then%20evaluate%20modern%20models%27%20performance%20for%0Alocating%20private%20content%20in%20the%20dataset.%20We%20find%20modern%20models%20struggle%20most%0Awith%20locating%20private%20objects%20that%20are%20not%20salient%2C%20small%2C%20and%20lack%20text%20as%0Awell%20as%20recognizing%20when%20private%20content%20is%20absent%20from%20an%20image.%20We%20facilitate%0Afuture%20extensions%20by%20sharing%20our%20new%20dataset%20with%20the%20evaluation%20server%20at%0Ahttps%3A//vizwiz.org/tasks-and-datasets/object-localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18243v3&entry.124074799=Read"},
{"title": "How to Tune a Multilingual Encoder Model for Germanic Languages: A Study\n  of PEFT, Full Fine-Tuning, and Language Adapters", "author": "Romina Oji and Jenny Kunz", "abstract": "  This paper investigates the optimal use of the multilingual encoder model\nmDeBERTa for tasks in three Germanic languages -- German, Swedish, and\nIcelandic -- representing varying levels of presence and likely data quality in\nmDeBERTas pre-training data. We compare full fine-tuning with the\nparameter-efficient fine-tuning (PEFT) methods LoRA and Pfeiffer bottleneck\nadapters, finding that PEFT is more effective for the higher-resource language,\nGerman. However, results for Swedish and Icelandic are less consistent. We also\nobserve differences between tasks: While PEFT tends to work better for question\nanswering, full fine-tuning is preferable for named entity recognition.\nInspired by previous research on modular approaches that combine task and\nlanguage adapters, we evaluate the impact of adding PEFT modules trained on\nunstructured text, finding that this approach is not beneficial.\n", "link": "http://arxiv.org/abs/2501.06025v1", "date": "2025-01-10", "relevancy": 2.4952, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Tune%20a%20Multilingual%20Encoder%20Model%20for%20Germanic%20Languages%3A%20A%20Study%0A%20%20of%20PEFT%2C%20Full%20Fine-Tuning%2C%20and%20Language%20Adapters&body=Title%3A%20How%20to%20Tune%20a%20Multilingual%20Encoder%20Model%20for%20Germanic%20Languages%3A%20A%20Study%0A%20%20of%20PEFT%2C%20Full%20Fine-Tuning%2C%20and%20Language%20Adapters%0AAuthor%3A%20Romina%20Oji%20and%20Jenny%20Kunz%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20optimal%20use%20of%20the%20multilingual%20encoder%20model%0AmDeBERTa%20for%20tasks%20in%20three%20Germanic%20languages%20--%20German%2C%20Swedish%2C%20and%0AIcelandic%20--%20representing%20varying%20levels%20of%20presence%20and%20likely%20data%20quality%20in%0AmDeBERTas%20pre-training%20data.%20We%20compare%20full%20fine-tuning%20with%20the%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20LoRA%20and%20Pfeiffer%20bottleneck%0Aadapters%2C%20finding%20that%20PEFT%20is%20more%20effective%20for%20the%20higher-resource%20language%2C%0AGerman.%20However%2C%20results%20for%20Swedish%20and%20Icelandic%20are%20less%20consistent.%20We%20also%0Aobserve%20differences%20between%20tasks%3A%20While%20PEFT%20tends%20to%20work%20better%20for%20question%0Aanswering%2C%20full%20fine-tuning%20is%20preferable%20for%20named%20entity%20recognition.%0AInspired%20by%20previous%20research%20on%20modular%20approaches%20that%20combine%20task%20and%0Alanguage%20adapters%2C%20we%20evaluate%20the%20impact%20of%20adding%20PEFT%20modules%20trained%20on%0Aunstructured%20text%2C%20finding%20that%20this%20approach%20is%20not%20beneficial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Tune%2520a%2520Multilingual%2520Encoder%2520Model%2520for%2520Germanic%2520Languages%253A%2520A%2520Study%250A%2520%2520of%2520PEFT%252C%2520Full%2520Fine-Tuning%252C%2520and%2520Language%2520Adapters%26entry.906535625%3DRomina%2520Oji%2520and%2520Jenny%2520Kunz%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520optimal%2520use%2520of%2520the%2520multilingual%2520encoder%2520model%250AmDeBERTa%2520for%2520tasks%2520in%2520three%2520Germanic%2520languages%2520--%2520German%252C%2520Swedish%252C%2520and%250AIcelandic%2520--%2520representing%2520varying%2520levels%2520of%2520presence%2520and%2520likely%2520data%2520quality%2520in%250AmDeBERTas%2520pre-training%2520data.%2520We%2520compare%2520full%2520fine-tuning%2520with%2520the%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%2520LoRA%2520and%2520Pfeiffer%2520bottleneck%250Aadapters%252C%2520finding%2520that%2520PEFT%2520is%2520more%2520effective%2520for%2520the%2520higher-resource%2520language%252C%250AGerman.%2520However%252C%2520results%2520for%2520Swedish%2520and%2520Icelandic%2520are%2520less%2520consistent.%2520We%2520also%250Aobserve%2520differences%2520between%2520tasks%253A%2520While%2520PEFT%2520tends%2520to%2520work%2520better%2520for%2520question%250Aanswering%252C%2520full%2520fine-tuning%2520is%2520preferable%2520for%2520named%2520entity%2520recognition.%250AInspired%2520by%2520previous%2520research%2520on%2520modular%2520approaches%2520that%2520combine%2520task%2520and%250Alanguage%2520adapters%252C%2520we%2520evaluate%2520the%2520impact%2520of%2520adding%2520PEFT%2520modules%2520trained%2520on%250Aunstructured%2520text%252C%2520finding%2520that%2520this%2520approach%2520is%2520not%2520beneficial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Tune%20a%20Multilingual%20Encoder%20Model%20for%20Germanic%20Languages%3A%20A%20Study%0A%20%20of%20PEFT%2C%20Full%20Fine-Tuning%2C%20and%20Language%20Adapters&entry.906535625=Romina%20Oji%20and%20Jenny%20Kunz&entry.1292438233=%20%20This%20paper%20investigates%20the%20optimal%20use%20of%20the%20multilingual%20encoder%20model%0AmDeBERTa%20for%20tasks%20in%20three%20Germanic%20languages%20--%20German%2C%20Swedish%2C%20and%0AIcelandic%20--%20representing%20varying%20levels%20of%20presence%20and%20likely%20data%20quality%20in%0AmDeBERTas%20pre-training%20data.%20We%20compare%20full%20fine-tuning%20with%20the%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20LoRA%20and%20Pfeiffer%20bottleneck%0Aadapters%2C%20finding%20that%20PEFT%20is%20more%20effective%20for%20the%20higher-resource%20language%2C%0AGerman.%20However%2C%20results%20for%20Swedish%20and%20Icelandic%20are%20less%20consistent.%20We%20also%0Aobserve%20differences%20between%20tasks%3A%20While%20PEFT%20tends%20to%20work%20better%20for%20question%0Aanswering%2C%20full%20fine-tuning%20is%20preferable%20for%20named%20entity%20recognition.%0AInspired%20by%20previous%20research%20on%20modular%20approaches%20that%20combine%20task%20and%0Alanguage%20adapters%2C%20we%20evaluate%20the%20impact%20of%20adding%20PEFT%20modules%20trained%20on%0Aunstructured%20text%2C%20finding%20that%20this%20approach%20is%20not%20beneficial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06025v1&entry.124074799=Read"},
{"title": "A General Control Method for Human-Robot Integration", "author": "Maddalena Feder and Giorgio Grioli and Manuel G. Catalano and Antonio Bicchi", "abstract": "  This paper introduces a new generalized control method designed for\nmulti-degrees-of-freedom devices to help people with limited motion\ncapabilities in their daily activities. The challenge lies in finding the most\nadapted strategy for the control interface to effectively map user's motions in\na low-dimensional space to complex robotic assistive devices, such as\nprostheses, supernumerary limbs, up to remote robotic avatars. The goal is a\nsystem which integrates the human and the robotic parts into a unique system,\nmoving so as to reach the targets decided by the human while autonomously\nreducing the user's effort and discomfort. We present a framework to control\ngeneral multi DoFs assistive systems, which translates user-performed\ncompensatory motions into the necessary robot commands for reaching targets\nwhile canceling or reducing compensation. The framework extends to prostheses\nof any number of DoF up to full robotic avatars, regarded here as a sort of\nwhole-body prosthesis of the person who sees the robot as an artificial\nextension of their own body without a physical link but with a sensory-motor\nintegration. We have validated and applied this control strategy through tests\nencompassing simulated scenarios and real-world trials involving a virtual twin\nof the robotic parts (prosthesis and robot) and a physical humanoid avatar.\n", "link": "http://arxiv.org/abs/2412.14762v2", "date": "2025-01-10", "relevancy": 2.489, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6609}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5979}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20General%20Control%20Method%20for%20Human-Robot%20Integration&body=Title%3A%20A%20General%20Control%20Method%20for%20Human-Robot%20Integration%0AAuthor%3A%20Maddalena%20Feder%20and%20Giorgio%20Grioli%20and%20Manuel%20G.%20Catalano%20and%20Antonio%20Bicchi%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20new%20generalized%20control%20method%20designed%20for%0Amulti-degrees-of-freedom%20devices%20to%20help%20people%20with%20limited%20motion%0Acapabilities%20in%20their%20daily%20activities.%20The%20challenge%20lies%20in%20finding%20the%20most%0Aadapted%20strategy%20for%20the%20control%20interface%20to%20effectively%20map%20user%27s%20motions%20in%0Aa%20low-dimensional%20space%20to%20complex%20robotic%20assistive%20devices%2C%20such%20as%0Aprostheses%2C%20supernumerary%20limbs%2C%20up%20to%20remote%20robotic%20avatars.%20The%20goal%20is%20a%0Asystem%20which%20integrates%20the%20human%20and%20the%20robotic%20parts%20into%20a%20unique%20system%2C%0Amoving%20so%20as%20to%20reach%20the%20targets%20decided%20by%20the%20human%20while%20autonomously%0Areducing%20the%20user%27s%20effort%20and%20discomfort.%20We%20present%20a%20framework%20to%20control%0Ageneral%20multi%20DoFs%20assistive%20systems%2C%20which%20translates%20user-performed%0Acompensatory%20motions%20into%20the%20necessary%20robot%20commands%20for%20reaching%20targets%0Awhile%20canceling%20or%20reducing%20compensation.%20The%20framework%20extends%20to%20prostheses%0Aof%20any%20number%20of%20DoF%20up%20to%20full%20robotic%20avatars%2C%20regarded%20here%20as%20a%20sort%20of%0Awhole-body%20prosthesis%20of%20the%20person%20who%20sees%20the%20robot%20as%20an%20artificial%0Aextension%20of%20their%20own%20body%20without%20a%20physical%20link%20but%20with%20a%20sensory-motor%0Aintegration.%20We%20have%20validated%20and%20applied%20this%20control%20strategy%20through%20tests%0Aencompassing%20simulated%20scenarios%20and%20real-world%20trials%20involving%20a%20virtual%20twin%0Aof%20the%20robotic%20parts%20%28prosthesis%20and%20robot%29%20and%20a%20physical%20humanoid%20avatar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520General%2520Control%2520Method%2520for%2520Human-Robot%2520Integration%26entry.906535625%3DMaddalena%2520Feder%2520and%2520Giorgio%2520Grioli%2520and%2520Manuel%2520G.%2520Catalano%2520and%2520Antonio%2520Bicchi%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520new%2520generalized%2520control%2520method%2520designed%2520for%250Amulti-degrees-of-freedom%2520devices%2520to%2520help%2520people%2520with%2520limited%2520motion%250Acapabilities%2520in%2520their%2520daily%2520activities.%2520The%2520challenge%2520lies%2520in%2520finding%2520the%2520most%250Aadapted%2520strategy%2520for%2520the%2520control%2520interface%2520to%2520effectively%2520map%2520user%2527s%2520motions%2520in%250Aa%2520low-dimensional%2520space%2520to%2520complex%2520robotic%2520assistive%2520devices%252C%2520such%2520as%250Aprostheses%252C%2520supernumerary%2520limbs%252C%2520up%2520to%2520remote%2520robotic%2520avatars.%2520The%2520goal%2520is%2520a%250Asystem%2520which%2520integrates%2520the%2520human%2520and%2520the%2520robotic%2520parts%2520into%2520a%2520unique%2520system%252C%250Amoving%2520so%2520as%2520to%2520reach%2520the%2520targets%2520decided%2520by%2520the%2520human%2520while%2520autonomously%250Areducing%2520the%2520user%2527s%2520effort%2520and%2520discomfort.%2520We%2520present%2520a%2520framework%2520to%2520control%250Ageneral%2520multi%2520DoFs%2520assistive%2520systems%252C%2520which%2520translates%2520user-performed%250Acompensatory%2520motions%2520into%2520the%2520necessary%2520robot%2520commands%2520for%2520reaching%2520targets%250Awhile%2520canceling%2520or%2520reducing%2520compensation.%2520The%2520framework%2520extends%2520to%2520prostheses%250Aof%2520any%2520number%2520of%2520DoF%2520up%2520to%2520full%2520robotic%2520avatars%252C%2520regarded%2520here%2520as%2520a%2520sort%2520of%250Awhole-body%2520prosthesis%2520of%2520the%2520person%2520who%2520sees%2520the%2520robot%2520as%2520an%2520artificial%250Aextension%2520of%2520their%2520own%2520body%2520without%2520a%2520physical%2520link%2520but%2520with%2520a%2520sensory-motor%250Aintegration.%2520We%2520have%2520validated%2520and%2520applied%2520this%2520control%2520strategy%2520through%2520tests%250Aencompassing%2520simulated%2520scenarios%2520and%2520real-world%2520trials%2520involving%2520a%2520virtual%2520twin%250Aof%2520the%2520robotic%2520parts%2520%2528prosthesis%2520and%2520robot%2529%2520and%2520a%2520physical%2520humanoid%2520avatar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Control%20Method%20for%20Human-Robot%20Integration&entry.906535625=Maddalena%20Feder%20and%20Giorgio%20Grioli%20and%20Manuel%20G.%20Catalano%20and%20Antonio%20Bicchi&entry.1292438233=%20%20This%20paper%20introduces%20a%20new%20generalized%20control%20method%20designed%20for%0Amulti-degrees-of-freedom%20devices%20to%20help%20people%20with%20limited%20motion%0Acapabilities%20in%20their%20daily%20activities.%20The%20challenge%20lies%20in%20finding%20the%20most%0Aadapted%20strategy%20for%20the%20control%20interface%20to%20effectively%20map%20user%27s%20motions%20in%0Aa%20low-dimensional%20space%20to%20complex%20robotic%20assistive%20devices%2C%20such%20as%0Aprostheses%2C%20supernumerary%20limbs%2C%20up%20to%20remote%20robotic%20avatars.%20The%20goal%20is%20a%0Asystem%20which%20integrates%20the%20human%20and%20the%20robotic%20parts%20into%20a%20unique%20system%2C%0Amoving%20so%20as%20to%20reach%20the%20targets%20decided%20by%20the%20human%20while%20autonomously%0Areducing%20the%20user%27s%20effort%20and%20discomfort.%20We%20present%20a%20framework%20to%20control%0Ageneral%20multi%20DoFs%20assistive%20systems%2C%20which%20translates%20user-performed%0Acompensatory%20motions%20into%20the%20necessary%20robot%20commands%20for%20reaching%20targets%0Awhile%20canceling%20or%20reducing%20compensation.%20The%20framework%20extends%20to%20prostheses%0Aof%20any%20number%20of%20DoF%20up%20to%20full%20robotic%20avatars%2C%20regarded%20here%20as%20a%20sort%20of%0Awhole-body%20prosthesis%20of%20the%20person%20who%20sees%20the%20robot%20as%20an%20artificial%0Aextension%20of%20their%20own%20body%20without%20a%20physical%20link%20but%20with%20a%20sensory-motor%0Aintegration.%20We%20have%20validated%20and%20applied%20this%20control%20strategy%20through%20tests%0Aencompassing%20simulated%20scenarios%20and%20real-world%20trials%20involving%20a%20virtual%20twin%0Aof%20the%20robotic%20parts%20%28prosthesis%20and%20robot%29%20and%20a%20physical%20humanoid%20avatar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14762v2&entry.124074799=Read"},
{"title": "Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language\n  Understanding", "author": "Fabian David Schmidt and Ivan Vuli\u0107 and Goran Glava\u0161 and David Ifeoluwa Adelani", "abstract": "  While recent multilingual automatic speech recognition models claim to\nsupport thousands of languages, ASR for low-resource languages remains highly\nunreliable due to limited bimodal speech and text training data. Better\nmultilingual spoken language understanding (SLU) can strengthen massively the\nrobustness of multilingual ASR by levering language semantics to compensate for\nscarce training data, such as disambiguating utterances via context or\nexploiting semantic similarities across languages. Even more so, SLU is\nindispensable for inclusive speech technology in roughly half of all living\nlanguages that lack a formal writing system. However, the evaluation of\nmultilingual SLU remains limited to shallower tasks such as intent\nclassification or language identification. To address this, we present\nFleurs-SLU, a multilingual SLU benchmark that encompasses topical speech\nclassification in 102 languages and multiple-choice question answering through\nlistening comprehension in 92 languages. We extensively evaluate both\nend-to-end speech classification models and cascaded systems that combine\nspeech-to-text transcription with subsequent classification by large language\nmodels on Fleurs-SLU. Our results show that cascaded systems exhibit greater\nrobustness in multilingual SLU tasks, though speech encoders can achieve\ncompetitive performance in topical speech classification when appropriately\npre-trained. We further find a strong correlation between robust multilingual\nASR, effective speech-to-text translation, and strong multilingual SLU,\nhighlighting the mutual benefits between acoustic and semantic speech\nrepresentations.\n", "link": "http://arxiv.org/abs/2501.06117v1", "date": "2025-01-10", "relevancy": 2.4871, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fleurs-SLU%3A%20A%20Massively%20Multilingual%20Benchmark%20for%20Spoken%20Language%0A%20%20Understanding&body=Title%3A%20Fleurs-SLU%3A%20A%20Massively%20Multilingual%20Benchmark%20for%20Spoken%20Language%0A%20%20Understanding%0AAuthor%3A%20Fabian%20David%20Schmidt%20and%20Ivan%20Vuli%C4%87%20and%20Goran%20Glava%C5%A1%20and%20David%20Ifeoluwa%20Adelani%0AAbstract%3A%20%20%20While%20recent%20multilingual%20automatic%20speech%20recognition%20models%20claim%20to%0Asupport%20thousands%20of%20languages%2C%20ASR%20for%20low-resource%20languages%20remains%20highly%0Aunreliable%20due%20to%20limited%20bimodal%20speech%20and%20text%20training%20data.%20Better%0Amultilingual%20spoken%20language%20understanding%20%28SLU%29%20can%20strengthen%20massively%20the%0Arobustness%20of%20multilingual%20ASR%20by%20levering%20language%20semantics%20to%20compensate%20for%0Ascarce%20training%20data%2C%20such%20as%20disambiguating%20utterances%20via%20context%20or%0Aexploiting%20semantic%20similarities%20across%20languages.%20Even%20more%20so%2C%20SLU%20is%0Aindispensable%20for%20inclusive%20speech%20technology%20in%20roughly%20half%20of%20all%20living%0Alanguages%20that%20lack%20a%20formal%20writing%20system.%20However%2C%20the%20evaluation%20of%0Amultilingual%20SLU%20remains%20limited%20to%20shallower%20tasks%20such%20as%20intent%0Aclassification%20or%20language%20identification.%20To%20address%20this%2C%20we%20present%0AFleurs-SLU%2C%20a%20multilingual%20SLU%20benchmark%20that%20encompasses%20topical%20speech%0Aclassification%20in%20102%20languages%20and%20multiple-choice%20question%20answering%20through%0Alistening%20comprehension%20in%2092%20languages.%20We%20extensively%20evaluate%20both%0Aend-to-end%20speech%20classification%20models%20and%20cascaded%20systems%20that%20combine%0Aspeech-to-text%20transcription%20with%20subsequent%20classification%20by%20large%20language%0Amodels%20on%20Fleurs-SLU.%20Our%20results%20show%20that%20cascaded%20systems%20exhibit%20greater%0Arobustness%20in%20multilingual%20SLU%20tasks%2C%20though%20speech%20encoders%20can%20achieve%0Acompetitive%20performance%20in%20topical%20speech%20classification%20when%20appropriately%0Apre-trained.%20We%20further%20find%20a%20strong%20correlation%20between%20robust%20multilingual%0AASR%2C%20effective%20speech-to-text%20translation%2C%20and%20strong%20multilingual%20SLU%2C%0Ahighlighting%20the%20mutual%20benefits%20between%20acoustic%20and%20semantic%20speech%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFleurs-SLU%253A%2520A%2520Massively%2520Multilingual%2520Benchmark%2520for%2520Spoken%2520Language%250A%2520%2520Understanding%26entry.906535625%3DFabian%2520David%2520Schmidt%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Goran%2520Glava%25C5%25A1%2520and%2520David%2520Ifeoluwa%2520Adelani%26entry.1292438233%3D%2520%2520While%2520recent%2520multilingual%2520automatic%2520speech%2520recognition%2520models%2520claim%2520to%250Asupport%2520thousands%2520of%2520languages%252C%2520ASR%2520for%2520low-resource%2520languages%2520remains%2520highly%250Aunreliable%2520due%2520to%2520limited%2520bimodal%2520speech%2520and%2520text%2520training%2520data.%2520Better%250Amultilingual%2520spoken%2520language%2520understanding%2520%2528SLU%2529%2520can%2520strengthen%2520massively%2520the%250Arobustness%2520of%2520multilingual%2520ASR%2520by%2520levering%2520language%2520semantics%2520to%2520compensate%2520for%250Ascarce%2520training%2520data%252C%2520such%2520as%2520disambiguating%2520utterances%2520via%2520context%2520or%250Aexploiting%2520semantic%2520similarities%2520across%2520languages.%2520Even%2520more%2520so%252C%2520SLU%2520is%250Aindispensable%2520for%2520inclusive%2520speech%2520technology%2520in%2520roughly%2520half%2520of%2520all%2520living%250Alanguages%2520that%2520lack%2520a%2520formal%2520writing%2520system.%2520However%252C%2520the%2520evaluation%2520of%250Amultilingual%2520SLU%2520remains%2520limited%2520to%2520shallower%2520tasks%2520such%2520as%2520intent%250Aclassification%2520or%2520language%2520identification.%2520To%2520address%2520this%252C%2520we%2520present%250AFleurs-SLU%252C%2520a%2520multilingual%2520SLU%2520benchmark%2520that%2520encompasses%2520topical%2520speech%250Aclassification%2520in%2520102%2520languages%2520and%2520multiple-choice%2520question%2520answering%2520through%250Alistening%2520comprehension%2520in%252092%2520languages.%2520We%2520extensively%2520evaluate%2520both%250Aend-to-end%2520speech%2520classification%2520models%2520and%2520cascaded%2520systems%2520that%2520combine%250Aspeech-to-text%2520transcription%2520with%2520subsequent%2520classification%2520by%2520large%2520language%250Amodels%2520on%2520Fleurs-SLU.%2520Our%2520results%2520show%2520that%2520cascaded%2520systems%2520exhibit%2520greater%250Arobustness%2520in%2520multilingual%2520SLU%2520tasks%252C%2520though%2520speech%2520encoders%2520can%2520achieve%250Acompetitive%2520performance%2520in%2520topical%2520speech%2520classification%2520when%2520appropriately%250Apre-trained.%2520We%2520further%2520find%2520a%2520strong%2520correlation%2520between%2520robust%2520multilingual%250AASR%252C%2520effective%2520speech-to-text%2520translation%252C%2520and%2520strong%2520multilingual%2520SLU%252C%250Ahighlighting%2520the%2520mutual%2520benefits%2520between%2520acoustic%2520and%2520semantic%2520speech%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fleurs-SLU%3A%20A%20Massively%20Multilingual%20Benchmark%20for%20Spoken%20Language%0A%20%20Understanding&entry.906535625=Fabian%20David%20Schmidt%20and%20Ivan%20Vuli%C4%87%20and%20Goran%20Glava%C5%A1%20and%20David%20Ifeoluwa%20Adelani&entry.1292438233=%20%20While%20recent%20multilingual%20automatic%20speech%20recognition%20models%20claim%20to%0Asupport%20thousands%20of%20languages%2C%20ASR%20for%20low-resource%20languages%20remains%20highly%0Aunreliable%20due%20to%20limited%20bimodal%20speech%20and%20text%20training%20data.%20Better%0Amultilingual%20spoken%20language%20understanding%20%28SLU%29%20can%20strengthen%20massively%20the%0Arobustness%20of%20multilingual%20ASR%20by%20levering%20language%20semantics%20to%20compensate%20for%0Ascarce%20training%20data%2C%20such%20as%20disambiguating%20utterances%20via%20context%20or%0Aexploiting%20semantic%20similarities%20across%20languages.%20Even%20more%20so%2C%20SLU%20is%0Aindispensable%20for%20inclusive%20speech%20technology%20in%20roughly%20half%20of%20all%20living%0Alanguages%20that%20lack%20a%20formal%20writing%20system.%20However%2C%20the%20evaluation%20of%0Amultilingual%20SLU%20remains%20limited%20to%20shallower%20tasks%20such%20as%20intent%0Aclassification%20or%20language%20identification.%20To%20address%20this%2C%20we%20present%0AFleurs-SLU%2C%20a%20multilingual%20SLU%20benchmark%20that%20encompasses%20topical%20speech%0Aclassification%20in%20102%20languages%20and%20multiple-choice%20question%20answering%20through%0Alistening%20comprehension%20in%2092%20languages.%20We%20extensively%20evaluate%20both%0Aend-to-end%20speech%20classification%20models%20and%20cascaded%20systems%20that%20combine%0Aspeech-to-text%20transcription%20with%20subsequent%20classification%20by%20large%20language%0Amodels%20on%20Fleurs-SLU.%20Our%20results%20show%20that%20cascaded%20systems%20exhibit%20greater%0Arobustness%20in%20multilingual%20SLU%20tasks%2C%20though%20speech%20encoders%20can%20achieve%0Acompetitive%20performance%20in%20topical%20speech%20classification%20when%20appropriately%0Apre-trained.%20We%20further%20find%20a%20strong%20correlation%20between%20robust%20multilingual%0AASR%2C%20effective%20speech-to-text%20translation%2C%20and%20strong%20multilingual%20SLU%2C%0Ahighlighting%20the%20mutual%20benefits%20between%20acoustic%20and%20semantic%20speech%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06117v1&entry.124074799=Read"},
{"title": "Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent", "author": "Zhiyu Liu and Zhi Han and Yandong Tang and Xi-Le Zhao and Yao Wang", "abstract": "  This paper considers the problem of recovering a tensor with an underlying\nlow-tubal-rank structure from a small number of corrupted linear measurements.\nTraditional approaches tackling such a problem require the computation of\ntensor Singular Value Decomposition (t-SVD), that is a computationally\nintensive process, rendering them impractical for dealing with large-scale\ntensors. Aim to address this challenge, we propose an efficient and effective\nlow-tubal-rank tensor recovery method based on a factorization procedure akin\nto the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves\ndecomposing a large tensor into two smaller factor tensors, followed by solving\nthe problem through factorized gradient descent (FGD). This strategy eliminates\nthe need for t-SVD computation, thereby reducing computational costs and\nstorage requirements. We provide rigorous theoretical analysis to ensure the\nconvergence of FGD under both noise-free and noisy situations. Additionally, it\nis worth noting that our method does not require the precise estimation of the\ntensor tubal-rank. Even in cases where the tubal-rank is slightly\noverestimated, our approach continues to demonstrate robust performance. A\nseries of experiments have been carried out to demonstrate that, as compared to\nother popular ones, our approach exhibits superior performance in multiple\nscenarios, in terms of the faster computational speed and the smaller\nconvergence error.\n", "link": "http://arxiv.org/abs/2401.11940v3", "date": "2025-01-10", "relevancy": 2.4864, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5067}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5034}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Tubal-Rank%20Tensor%20Recovery%20via%20Factorized%20Gradient%20Descent&body=Title%3A%20Low-Tubal-Rank%20Tensor%20Recovery%20via%20Factorized%20Gradient%20Descent%0AAuthor%3A%20Zhiyu%20Liu%20and%20Zhi%20Han%20and%20Yandong%20Tang%20and%20Xi-Le%20Zhao%20and%20Yao%20Wang%0AAbstract%3A%20%20%20This%20paper%20considers%20the%20problem%20of%20recovering%20a%20tensor%20with%20an%20underlying%0Alow-tubal-rank%20structure%20from%20a%20small%20number%20of%20corrupted%20linear%20measurements.%0ATraditional%20approaches%20tackling%20such%20a%20problem%20require%20the%20computation%20of%0Atensor%20Singular%20Value%20Decomposition%20%28t-SVD%29%2C%20that%20is%20a%20computationally%0Aintensive%20process%2C%20rendering%20them%20impractical%20for%20dealing%20with%20large-scale%0Atensors.%20Aim%20to%20address%20this%20challenge%2C%20we%20propose%20an%20efficient%20and%20effective%0Alow-tubal-rank%20tensor%20recovery%20method%20based%20on%20a%20factorization%20procedure%20akin%0Ato%20the%20Burer-Monteiro%20%28BM%29%20method.%20Precisely%2C%20our%20fundamental%20approach%20involves%0Adecomposing%20a%20large%20tensor%20into%20two%20smaller%20factor%20tensors%2C%20followed%20by%20solving%0Athe%20problem%20through%20factorized%20gradient%20descent%20%28FGD%29.%20This%20strategy%20eliminates%0Athe%20need%20for%20t-SVD%20computation%2C%20thereby%20reducing%20computational%20costs%20and%0Astorage%20requirements.%20We%20provide%20rigorous%20theoretical%20analysis%20to%20ensure%20the%0Aconvergence%20of%20FGD%20under%20both%20noise-free%20and%20noisy%20situations.%20Additionally%2C%20it%0Ais%20worth%20noting%20that%20our%20method%20does%20not%20require%20the%20precise%20estimation%20of%20the%0Atensor%20tubal-rank.%20Even%20in%20cases%20where%20the%20tubal-rank%20is%20slightly%0Aoverestimated%2C%20our%20approach%20continues%20to%20demonstrate%20robust%20performance.%20A%0Aseries%20of%20experiments%20have%20been%20carried%20out%20to%20demonstrate%20that%2C%20as%20compared%20to%0Aother%20popular%20ones%2C%20our%20approach%20exhibits%20superior%20performance%20in%20multiple%0Ascenarios%2C%20in%20terms%20of%20the%20faster%20computational%20speed%20and%20the%20smaller%0Aconvergence%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Tubal-Rank%2520Tensor%2520Recovery%2520via%2520Factorized%2520Gradient%2520Descent%26entry.906535625%3DZhiyu%2520Liu%2520and%2520Zhi%2520Han%2520and%2520Yandong%2520Tang%2520and%2520Xi-Le%2520Zhao%2520and%2520Yao%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520the%2520problem%2520of%2520recovering%2520a%2520tensor%2520with%2520an%2520underlying%250Alow-tubal-rank%2520structure%2520from%2520a%2520small%2520number%2520of%2520corrupted%2520linear%2520measurements.%250ATraditional%2520approaches%2520tackling%2520such%2520a%2520problem%2520require%2520the%2520computation%2520of%250Atensor%2520Singular%2520Value%2520Decomposition%2520%2528t-SVD%2529%252C%2520that%2520is%2520a%2520computationally%250Aintensive%2520process%252C%2520rendering%2520them%2520impractical%2520for%2520dealing%2520with%2520large-scale%250Atensors.%2520Aim%2520to%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520efficient%2520and%2520effective%250Alow-tubal-rank%2520tensor%2520recovery%2520method%2520based%2520on%2520a%2520factorization%2520procedure%2520akin%250Ato%2520the%2520Burer-Monteiro%2520%2528BM%2529%2520method.%2520Precisely%252C%2520our%2520fundamental%2520approach%2520involves%250Adecomposing%2520a%2520large%2520tensor%2520into%2520two%2520smaller%2520factor%2520tensors%252C%2520followed%2520by%2520solving%250Athe%2520problem%2520through%2520factorized%2520gradient%2520descent%2520%2528FGD%2529.%2520This%2520strategy%2520eliminates%250Athe%2520need%2520for%2520t-SVD%2520computation%252C%2520thereby%2520reducing%2520computational%2520costs%2520and%250Astorage%2520requirements.%2520We%2520provide%2520rigorous%2520theoretical%2520analysis%2520to%2520ensure%2520the%250Aconvergence%2520of%2520FGD%2520under%2520both%2520noise-free%2520and%2520noisy%2520situations.%2520Additionally%252C%2520it%250Ais%2520worth%2520noting%2520that%2520our%2520method%2520does%2520not%2520require%2520the%2520precise%2520estimation%2520of%2520the%250Atensor%2520tubal-rank.%2520Even%2520in%2520cases%2520where%2520the%2520tubal-rank%2520is%2520slightly%250Aoverestimated%252C%2520our%2520approach%2520continues%2520to%2520demonstrate%2520robust%2520performance.%2520A%250Aseries%2520of%2520experiments%2520have%2520been%2520carried%2520out%2520to%2520demonstrate%2520that%252C%2520as%2520compared%2520to%250Aother%2520popular%2520ones%252C%2520our%2520approach%2520exhibits%2520superior%2520performance%2520in%2520multiple%250Ascenarios%252C%2520in%2520terms%2520of%2520the%2520faster%2520computational%2520speed%2520and%2520the%2520smaller%250Aconvergence%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Tubal-Rank%20Tensor%20Recovery%20via%20Factorized%20Gradient%20Descent&entry.906535625=Zhiyu%20Liu%20and%20Zhi%20Han%20and%20Yandong%20Tang%20and%20Xi-Le%20Zhao%20and%20Yao%20Wang&entry.1292438233=%20%20This%20paper%20considers%20the%20problem%20of%20recovering%20a%20tensor%20with%20an%20underlying%0Alow-tubal-rank%20structure%20from%20a%20small%20number%20of%20corrupted%20linear%20measurements.%0ATraditional%20approaches%20tackling%20such%20a%20problem%20require%20the%20computation%20of%0Atensor%20Singular%20Value%20Decomposition%20%28t-SVD%29%2C%20that%20is%20a%20computationally%0Aintensive%20process%2C%20rendering%20them%20impractical%20for%20dealing%20with%20large-scale%0Atensors.%20Aim%20to%20address%20this%20challenge%2C%20we%20propose%20an%20efficient%20and%20effective%0Alow-tubal-rank%20tensor%20recovery%20method%20based%20on%20a%20factorization%20procedure%20akin%0Ato%20the%20Burer-Monteiro%20%28BM%29%20method.%20Precisely%2C%20our%20fundamental%20approach%20involves%0Adecomposing%20a%20large%20tensor%20into%20two%20smaller%20factor%20tensors%2C%20followed%20by%20solving%0Athe%20problem%20through%20factorized%20gradient%20descent%20%28FGD%29.%20This%20strategy%20eliminates%0Athe%20need%20for%20t-SVD%20computation%2C%20thereby%20reducing%20computational%20costs%20and%0Astorage%20requirements.%20We%20provide%20rigorous%20theoretical%20analysis%20to%20ensure%20the%0Aconvergence%20of%20FGD%20under%20both%20noise-free%20and%20noisy%20situations.%20Additionally%2C%20it%0Ais%20worth%20noting%20that%20our%20method%20does%20not%20require%20the%20precise%20estimation%20of%20the%0Atensor%20tubal-rank.%20Even%20in%20cases%20where%20the%20tubal-rank%20is%20slightly%0Aoverestimated%2C%20our%20approach%20continues%20to%20demonstrate%20robust%20performance.%20A%0Aseries%20of%20experiments%20have%20been%20carried%20out%20to%20demonstrate%20that%2C%20as%20compared%20to%0Aother%20popular%20ones%2C%20our%20approach%20exhibits%20superior%20performance%20in%20multiple%0Ascenarios%2C%20in%20terms%20of%20the%20faster%20computational%20speed%20and%20the%20smaller%0Aconvergence%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11940v3&entry.124074799=Read"},
{"title": "Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charit\u00e9, and\n  Aignostics", "author": "Maximilian Alber and Stephan Tietz and Jonas Dippel and Timo Milbich and Timoth\u00e9e Lesort and Panos Korfiatis and Moritz Kr\u00fcgener and Beatriz Perez Cancer and Neelay Shah and Alexander M\u00f6llers and Philipp Seegerer and Alexandra Carpen-Amarie and Kai Standvoss and Gabriel Dernbach and Edwin de Jong and Simon Schallenberg and Andreas Kunft and Helmut Hoffer von Ankershoffen and Gavin Schaeferle and Patrick Duffy and Matt Redlon and Philipp Jurmeister and David Horst and Lukas Ruff and Klaus-Robert M\u00fcller and Frederick Klauschen and Andrew Norgan", "abstract": "  Recent advances in digital pathology have demonstrated the effectiveness of\nfoundation models across diverse applications. In this report, we present\nAtlas, a novel vision foundation model based on the RudolfV approach. Our model\nwas trained on a dataset comprising 1.2 million histopathology whole slide\nimages, collected from two medical institutions: Mayo Clinic and Charit\\'e -\nUniverst\\\"atsmedizin Berlin. Comprehensive evaluations show that Atlas achieves\nstate-of-the-art performance across twenty-one public benchmark datasets, even\nthough it is neither the largest model by parameter count nor by training\ndataset size.\n", "link": "http://arxiv.org/abs/2501.05409v2", "date": "2025-01-10", "relevancy": 2.4376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Atlas%3A%20A%20Novel%20Pathology%20Foundation%20Model%20by%20Mayo%20Clinic%2C%20Charit%C3%A9%2C%20and%0A%20%20Aignostics&body=Title%3A%20Atlas%3A%20A%20Novel%20Pathology%20Foundation%20Model%20by%20Mayo%20Clinic%2C%20Charit%C3%A9%2C%20and%0A%20%20Aignostics%0AAuthor%3A%20Maximilian%20Alber%20and%20Stephan%20Tietz%20and%20Jonas%20Dippel%20and%20Timo%20Milbich%20and%20Timoth%C3%A9e%20Lesort%20and%20Panos%20Korfiatis%20and%20Moritz%20Kr%C3%BCgener%20and%20Beatriz%20Perez%20Cancer%20and%20Neelay%20Shah%20and%20Alexander%20M%C3%B6llers%20and%20Philipp%20Seegerer%20and%20Alexandra%20Carpen-Amarie%20and%20Kai%20Standvoss%20and%20Gabriel%20Dernbach%20and%20Edwin%20de%20Jong%20and%20Simon%20Schallenberg%20and%20Andreas%20Kunft%20and%20Helmut%20Hoffer%20von%20Ankershoffen%20and%20Gavin%20Schaeferle%20and%20Patrick%20Duffy%20and%20Matt%20Redlon%20and%20Philipp%20Jurmeister%20and%20David%20Horst%20and%20Lukas%20Ruff%20and%20Klaus-Robert%20M%C3%BCller%20and%20Frederick%20Klauschen%20and%20Andrew%20Norgan%0AAbstract%3A%20%20%20Recent%20advances%20in%20digital%20pathology%20have%20demonstrated%20the%20effectiveness%20of%0Afoundation%20models%20across%20diverse%20applications.%20In%20this%20report%2C%20we%20present%0AAtlas%2C%20a%20novel%20vision%20foundation%20model%20based%20on%20the%20RudolfV%20approach.%20Our%20model%0Awas%20trained%20on%20a%20dataset%20comprising%201.2%20million%20histopathology%20whole%20slide%0Aimages%2C%20collected%20from%20two%20medical%20institutions%3A%20Mayo%20Clinic%20and%20Charit%5C%27e%20-%0AUniverst%5C%22atsmedizin%20Berlin.%20Comprehensive%20evaluations%20show%20that%20Atlas%20achieves%0Astate-of-the-art%20performance%20across%20twenty-one%20public%20benchmark%20datasets%2C%20even%0Athough%20it%20is%20neither%20the%20largest%20model%20by%20parameter%20count%20nor%20by%20training%0Adataset%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05409v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtlas%253A%2520A%2520Novel%2520Pathology%2520Foundation%2520Model%2520by%2520Mayo%2520Clinic%252C%2520Charit%25C3%25A9%252C%2520and%250A%2520%2520Aignostics%26entry.906535625%3DMaximilian%2520Alber%2520and%2520Stephan%2520Tietz%2520and%2520Jonas%2520Dippel%2520and%2520Timo%2520Milbich%2520and%2520Timoth%25C3%25A9e%2520Lesort%2520and%2520Panos%2520Korfiatis%2520and%2520Moritz%2520Kr%25C3%25BCgener%2520and%2520Beatriz%2520Perez%2520Cancer%2520and%2520Neelay%2520Shah%2520and%2520Alexander%2520M%25C3%25B6llers%2520and%2520Philipp%2520Seegerer%2520and%2520Alexandra%2520Carpen-Amarie%2520and%2520Kai%2520Standvoss%2520and%2520Gabriel%2520Dernbach%2520and%2520Edwin%2520de%2520Jong%2520and%2520Simon%2520Schallenberg%2520and%2520Andreas%2520Kunft%2520and%2520Helmut%2520Hoffer%2520von%2520Ankershoffen%2520and%2520Gavin%2520Schaeferle%2520and%2520Patrick%2520Duffy%2520and%2520Matt%2520Redlon%2520and%2520Philipp%2520Jurmeister%2520and%2520David%2520Horst%2520and%2520Lukas%2520Ruff%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Frederick%2520Klauschen%2520and%2520Andrew%2520Norgan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520digital%2520pathology%2520have%2520demonstrated%2520the%2520effectiveness%2520of%250Afoundation%2520models%2520across%2520diverse%2520applications.%2520In%2520this%2520report%252C%2520we%2520present%250AAtlas%252C%2520a%2520novel%2520vision%2520foundation%2520model%2520based%2520on%2520the%2520RudolfV%2520approach.%2520Our%2520model%250Awas%2520trained%2520on%2520a%2520dataset%2520comprising%25201.2%2520million%2520histopathology%2520whole%2520slide%250Aimages%252C%2520collected%2520from%2520two%2520medical%2520institutions%253A%2520Mayo%2520Clinic%2520and%2520Charit%255C%2527e%2520-%250AUniverst%255C%2522atsmedizin%2520Berlin.%2520Comprehensive%2520evaluations%2520show%2520that%2520Atlas%2520achieves%250Astate-of-the-art%2520performance%2520across%2520twenty-one%2520public%2520benchmark%2520datasets%252C%2520even%250Athough%2520it%2520is%2520neither%2520the%2520largest%2520model%2520by%2520parameter%2520count%2520nor%2520by%2520training%250Adataset%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05409v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Atlas%3A%20A%20Novel%20Pathology%20Foundation%20Model%20by%20Mayo%20Clinic%2C%20Charit%C3%A9%2C%20and%0A%20%20Aignostics&entry.906535625=Maximilian%20Alber%20and%20Stephan%20Tietz%20and%20Jonas%20Dippel%20and%20Timo%20Milbich%20and%20Timoth%C3%A9e%20Lesort%20and%20Panos%20Korfiatis%20and%20Moritz%20Kr%C3%BCgener%20and%20Beatriz%20Perez%20Cancer%20and%20Neelay%20Shah%20and%20Alexander%20M%C3%B6llers%20and%20Philipp%20Seegerer%20and%20Alexandra%20Carpen-Amarie%20and%20Kai%20Standvoss%20and%20Gabriel%20Dernbach%20and%20Edwin%20de%20Jong%20and%20Simon%20Schallenberg%20and%20Andreas%20Kunft%20and%20Helmut%20Hoffer%20von%20Ankershoffen%20and%20Gavin%20Schaeferle%20and%20Patrick%20Duffy%20and%20Matt%20Redlon%20and%20Philipp%20Jurmeister%20and%20David%20Horst%20and%20Lukas%20Ruff%20and%20Klaus-Robert%20M%C3%BCller%20and%20Frederick%20Klauschen%20and%20Andrew%20Norgan&entry.1292438233=%20%20Recent%20advances%20in%20digital%20pathology%20have%20demonstrated%20the%20effectiveness%20of%0Afoundation%20models%20across%20diverse%20applications.%20In%20this%20report%2C%20we%20present%0AAtlas%2C%20a%20novel%20vision%20foundation%20model%20based%20on%20the%20RudolfV%20approach.%20Our%20model%0Awas%20trained%20on%20a%20dataset%20comprising%201.2%20million%20histopathology%20whole%20slide%0Aimages%2C%20collected%20from%20two%20medical%20institutions%3A%20Mayo%20Clinic%20and%20Charit%5C%27e%20-%0AUniverst%5C%22atsmedizin%20Berlin.%20Comprehensive%20evaluations%20show%20that%20Atlas%20achieves%0Astate-of-the-art%20performance%20across%20twenty-one%20public%20benchmark%20datasets%2C%20even%0Athough%20it%20is%20neither%20the%20largest%20model%20by%20parameter%20count%20nor%20by%20training%0Adataset%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05409v2&entry.124074799=Read"},
{"title": "NDOB-Based Control of a UAV with Delta-Arm Considering Manipulator\n  Dynamics", "author": "Hongming Chen and Biyu Ye and Xianqi Liang and Weiliang Deng and Ximin Lyu", "abstract": "  Aerial Manipulators (AMs) provide a versatile platform for various\napplications, including 3D printing, architecture, and aerial grasping\nmissions. However, their operational speed is often sacrificed to uphold\nprecision. Existing control strategies for AMs often regard the manipulator as\na disturbance and employ robust control methods to mitigate its influence. This\nresearch focuses on elevating the precision of the end-effector and enhancing\nthe agility of aerial manipulator movements. We present a composite control\nscheme to address these challenges. Initially, a Nonlinear Disturbance Observer\n(NDOB) is utilized to compensate for internal coupling effects and external\ndisturbances. Subsequently, manipulator dynamics are processed through a high\npass filter to facilitate agile movements. By integrating the proposed control\nmethod into a fully autonomous delta-arm-based AM system, we substantiate the\ncontroller's efficacy through extensive real-world experiments. The outcomes\nillustrate that the end-effector can achieve accuracy at the millimeter level.\n", "link": "http://arxiv.org/abs/2501.06122v1", "date": "2025-01-10", "relevancy": 2.4264, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4915}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4859}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NDOB-Based%20Control%20of%20a%20UAV%20with%20Delta-Arm%20Considering%20Manipulator%0A%20%20Dynamics&body=Title%3A%20NDOB-Based%20Control%20of%20a%20UAV%20with%20Delta-Arm%20Considering%20Manipulator%0A%20%20Dynamics%0AAuthor%3A%20Hongming%20Chen%20and%20Biyu%20Ye%20and%20Xianqi%20Liang%20and%20Weiliang%20Deng%20and%20Ximin%20Lyu%0AAbstract%3A%20%20%20Aerial%20Manipulators%20%28AMs%29%20provide%20a%20versatile%20platform%20for%20various%0Aapplications%2C%20including%203D%20printing%2C%20architecture%2C%20and%20aerial%20grasping%0Amissions.%20However%2C%20their%20operational%20speed%20is%20often%20sacrificed%20to%20uphold%0Aprecision.%20Existing%20control%20strategies%20for%20AMs%20often%20regard%20the%20manipulator%20as%0Aa%20disturbance%20and%20employ%20robust%20control%20methods%20to%20mitigate%20its%20influence.%20This%0Aresearch%20focuses%20on%20elevating%20the%20precision%20of%20the%20end-effector%20and%20enhancing%0Athe%20agility%20of%20aerial%20manipulator%20movements.%20We%20present%20a%20composite%20control%0Ascheme%20to%20address%20these%20challenges.%20Initially%2C%20a%20Nonlinear%20Disturbance%20Observer%0A%28NDOB%29%20is%20utilized%20to%20compensate%20for%20internal%20coupling%20effects%20and%20external%0Adisturbances.%20Subsequently%2C%20manipulator%20dynamics%20are%20processed%20through%20a%20high%0Apass%20filter%20to%20facilitate%20agile%20movements.%20By%20integrating%20the%20proposed%20control%0Amethod%20into%20a%20fully%20autonomous%20delta-arm-based%20AM%20system%2C%20we%20substantiate%20the%0Acontroller%27s%20efficacy%20through%20extensive%20real-world%20experiments.%20The%20outcomes%0Aillustrate%20that%20the%20end-effector%20can%20achieve%20accuracy%20at%20the%20millimeter%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNDOB-Based%2520Control%2520of%2520a%2520UAV%2520with%2520Delta-Arm%2520Considering%2520Manipulator%250A%2520%2520Dynamics%26entry.906535625%3DHongming%2520Chen%2520and%2520Biyu%2520Ye%2520and%2520Xianqi%2520Liang%2520and%2520Weiliang%2520Deng%2520and%2520Ximin%2520Lyu%26entry.1292438233%3D%2520%2520Aerial%2520Manipulators%2520%2528AMs%2529%2520provide%2520a%2520versatile%2520platform%2520for%2520various%250Aapplications%252C%2520including%25203D%2520printing%252C%2520architecture%252C%2520and%2520aerial%2520grasping%250Amissions.%2520However%252C%2520their%2520operational%2520speed%2520is%2520often%2520sacrificed%2520to%2520uphold%250Aprecision.%2520Existing%2520control%2520strategies%2520for%2520AMs%2520often%2520regard%2520the%2520manipulator%2520as%250Aa%2520disturbance%2520and%2520employ%2520robust%2520control%2520methods%2520to%2520mitigate%2520its%2520influence.%2520This%250Aresearch%2520focuses%2520on%2520elevating%2520the%2520precision%2520of%2520the%2520end-effector%2520and%2520enhancing%250Athe%2520agility%2520of%2520aerial%2520manipulator%2520movements.%2520We%2520present%2520a%2520composite%2520control%250Ascheme%2520to%2520address%2520these%2520challenges.%2520Initially%252C%2520a%2520Nonlinear%2520Disturbance%2520Observer%250A%2528NDOB%2529%2520is%2520utilized%2520to%2520compensate%2520for%2520internal%2520coupling%2520effects%2520and%2520external%250Adisturbances.%2520Subsequently%252C%2520manipulator%2520dynamics%2520are%2520processed%2520through%2520a%2520high%250Apass%2520filter%2520to%2520facilitate%2520agile%2520movements.%2520By%2520integrating%2520the%2520proposed%2520control%250Amethod%2520into%2520a%2520fully%2520autonomous%2520delta-arm-based%2520AM%2520system%252C%2520we%2520substantiate%2520the%250Acontroller%2527s%2520efficacy%2520through%2520extensive%2520real-world%2520experiments.%2520The%2520outcomes%250Aillustrate%2520that%2520the%2520end-effector%2520can%2520achieve%2520accuracy%2520at%2520the%2520millimeter%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NDOB-Based%20Control%20of%20a%20UAV%20with%20Delta-Arm%20Considering%20Manipulator%0A%20%20Dynamics&entry.906535625=Hongming%20Chen%20and%20Biyu%20Ye%20and%20Xianqi%20Liang%20and%20Weiliang%20Deng%20and%20Ximin%20Lyu&entry.1292438233=%20%20Aerial%20Manipulators%20%28AMs%29%20provide%20a%20versatile%20platform%20for%20various%0Aapplications%2C%20including%203D%20printing%2C%20architecture%2C%20and%20aerial%20grasping%0Amissions.%20However%2C%20their%20operational%20speed%20is%20often%20sacrificed%20to%20uphold%0Aprecision.%20Existing%20control%20strategies%20for%20AMs%20often%20regard%20the%20manipulator%20as%0Aa%20disturbance%20and%20employ%20robust%20control%20methods%20to%20mitigate%20its%20influence.%20This%0Aresearch%20focuses%20on%20elevating%20the%20precision%20of%20the%20end-effector%20and%20enhancing%0Athe%20agility%20of%20aerial%20manipulator%20movements.%20We%20present%20a%20composite%20control%0Ascheme%20to%20address%20these%20challenges.%20Initially%2C%20a%20Nonlinear%20Disturbance%20Observer%0A%28NDOB%29%20is%20utilized%20to%20compensate%20for%20internal%20coupling%20effects%20and%20external%0Adisturbances.%20Subsequently%2C%20manipulator%20dynamics%20are%20processed%20through%20a%20high%0Apass%20filter%20to%20facilitate%20agile%20movements.%20By%20integrating%20the%20proposed%20control%0Amethod%20into%20a%20fully%20autonomous%20delta-arm-based%20AM%20system%2C%20we%20substantiate%20the%0Acontroller%27s%20efficacy%20through%20extensive%20real-world%20experiments.%20The%20outcomes%0Aillustrate%20that%20the%20end-effector%20can%20achieve%20accuracy%20at%20the%20millimeter%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06122v1&entry.124074799=Read"},
{"title": "Comparing Self-Supervised Learning Models Pre-Trained on Human Speech\n  and Animal Vocalizations for Bioacoustics Processing", "author": "Eklavya Sarkar and Mathew Magimai. -Doss", "abstract": "  Self-supervised learning (SSL) foundation models have emerged as powerful,\ndomain-agnostic, general-purpose feature extractors applicable to a wide range\nof tasks. Such models pre-trained on human speech have demonstrated high\ntransferability for bioacoustic processing. This paper investigates (i) whether\nSSL models pre-trained directly on animal vocalizations offer a significant\nadvantage over those pre-trained on speech, and (ii) whether fine-tuning\nspeech-pretrained models on automatic speech recognition (ASR) tasks can\nenhance bioacoustic classification. We conduct a comparative analysis using\nthree diverse bioacoustic datasets and two different bioacoustic tasks. Results\nindicate that pre-training on bioacoustic data provides only marginal\nimprovements over speech-pretrained models, with comparable performance in most\nscenarios. Fine-tuning on ASR tasks yields mixed outcomes, suggesting that the\ngeneral-purpose representations learned during SSL pre-training are already\nwell-suited for bioacoustic tasks. These findings highlight the robustness of\nspeech-pretrained SSL models for bioacoustics and imply that extensive\nfine-tuning may not be necessary for optimal performance.\n", "link": "http://arxiv.org/abs/2501.05987v1", "date": "2025-01-10", "relevancy": 2.4123, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Self-Supervised%20Learning%20Models%20Pre-Trained%20on%20Human%20Speech%0A%20%20and%20Animal%20Vocalizations%20for%20Bioacoustics%20Processing&body=Title%3A%20Comparing%20Self-Supervised%20Learning%20Models%20Pre-Trained%20on%20Human%20Speech%0A%20%20and%20Animal%20Vocalizations%20for%20Bioacoustics%20Processing%0AAuthor%3A%20Eklavya%20Sarkar%20and%20Mathew%20Magimai.%20-Doss%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20foundation%20models%20have%20emerged%20as%20powerful%2C%0Adomain-agnostic%2C%20general-purpose%20feature%20extractors%20applicable%20to%20a%20wide%20range%0Aof%20tasks.%20Such%20models%20pre-trained%20on%20human%20speech%20have%20demonstrated%20high%0Atransferability%20for%20bioacoustic%20processing.%20This%20paper%20investigates%20%28i%29%20whether%0ASSL%20models%20pre-trained%20directly%20on%20animal%20vocalizations%20offer%20a%20significant%0Aadvantage%20over%20those%20pre-trained%20on%20speech%2C%20and%20%28ii%29%20whether%20fine-tuning%0Aspeech-pretrained%20models%20on%20automatic%20speech%20recognition%20%28ASR%29%20tasks%20can%0Aenhance%20bioacoustic%20classification.%20We%20conduct%20a%20comparative%20analysis%20using%0Athree%20diverse%20bioacoustic%20datasets%20and%20two%20different%20bioacoustic%20tasks.%20Results%0Aindicate%20that%20pre-training%20on%20bioacoustic%20data%20provides%20only%20marginal%0Aimprovements%20over%20speech-pretrained%20models%2C%20with%20comparable%20performance%20in%20most%0Ascenarios.%20Fine-tuning%20on%20ASR%20tasks%20yields%20mixed%20outcomes%2C%20suggesting%20that%20the%0Ageneral-purpose%20representations%20learned%20during%20SSL%20pre-training%20are%20already%0Awell-suited%20for%20bioacoustic%20tasks.%20These%20findings%20highlight%20the%20robustness%20of%0Aspeech-pretrained%20SSL%20models%20for%20bioacoustics%20and%20imply%20that%20extensive%0Afine-tuning%20may%20not%20be%20necessary%20for%20optimal%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Self-Supervised%2520Learning%2520Models%2520Pre-Trained%2520on%2520Human%2520Speech%250A%2520%2520and%2520Animal%2520Vocalizations%2520for%2520Bioacoustics%2520Processing%26entry.906535625%3DEklavya%2520Sarkar%2520and%2520Mathew%2520Magimai.%2520-Doss%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520foundation%2520models%2520have%2520emerged%2520as%2520powerful%252C%250Adomain-agnostic%252C%2520general-purpose%2520feature%2520extractors%2520applicable%2520to%2520a%2520wide%2520range%250Aof%2520tasks.%2520Such%2520models%2520pre-trained%2520on%2520human%2520speech%2520have%2520demonstrated%2520high%250Atransferability%2520for%2520bioacoustic%2520processing.%2520This%2520paper%2520investigates%2520%2528i%2529%2520whether%250ASSL%2520models%2520pre-trained%2520directly%2520on%2520animal%2520vocalizations%2520offer%2520a%2520significant%250Aadvantage%2520over%2520those%2520pre-trained%2520on%2520speech%252C%2520and%2520%2528ii%2529%2520whether%2520fine-tuning%250Aspeech-pretrained%2520models%2520on%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520tasks%2520can%250Aenhance%2520bioacoustic%2520classification.%2520We%2520conduct%2520a%2520comparative%2520analysis%2520using%250Athree%2520diverse%2520bioacoustic%2520datasets%2520and%2520two%2520different%2520bioacoustic%2520tasks.%2520Results%250Aindicate%2520that%2520pre-training%2520on%2520bioacoustic%2520data%2520provides%2520only%2520marginal%250Aimprovements%2520over%2520speech-pretrained%2520models%252C%2520with%2520comparable%2520performance%2520in%2520most%250Ascenarios.%2520Fine-tuning%2520on%2520ASR%2520tasks%2520yields%2520mixed%2520outcomes%252C%2520suggesting%2520that%2520the%250Ageneral-purpose%2520representations%2520learned%2520during%2520SSL%2520pre-training%2520are%2520already%250Awell-suited%2520for%2520bioacoustic%2520tasks.%2520These%2520findings%2520highlight%2520the%2520robustness%2520of%250Aspeech-pretrained%2520SSL%2520models%2520for%2520bioacoustics%2520and%2520imply%2520that%2520extensive%250Afine-tuning%2520may%2520not%2520be%2520necessary%2520for%2520optimal%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Self-Supervised%20Learning%20Models%20Pre-Trained%20on%20Human%20Speech%0A%20%20and%20Animal%20Vocalizations%20for%20Bioacoustics%20Processing&entry.906535625=Eklavya%20Sarkar%20and%20Mathew%20Magimai.%20-Doss&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20foundation%20models%20have%20emerged%20as%20powerful%2C%0Adomain-agnostic%2C%20general-purpose%20feature%20extractors%20applicable%20to%20a%20wide%20range%0Aof%20tasks.%20Such%20models%20pre-trained%20on%20human%20speech%20have%20demonstrated%20high%0Atransferability%20for%20bioacoustic%20processing.%20This%20paper%20investigates%20%28i%29%20whether%0ASSL%20models%20pre-trained%20directly%20on%20animal%20vocalizations%20offer%20a%20significant%0Aadvantage%20over%20those%20pre-trained%20on%20speech%2C%20and%20%28ii%29%20whether%20fine-tuning%0Aspeech-pretrained%20models%20on%20automatic%20speech%20recognition%20%28ASR%29%20tasks%20can%0Aenhance%20bioacoustic%20classification.%20We%20conduct%20a%20comparative%20analysis%20using%0Athree%20diverse%20bioacoustic%20datasets%20and%20two%20different%20bioacoustic%20tasks.%20Results%0Aindicate%20that%20pre-training%20on%20bioacoustic%20data%20provides%20only%20marginal%0Aimprovements%20over%20speech-pretrained%20models%2C%20with%20comparable%20performance%20in%20most%0Ascenarios.%20Fine-tuning%20on%20ASR%20tasks%20yields%20mixed%20outcomes%2C%20suggesting%20that%20the%0Ageneral-purpose%20representations%20learned%20during%20SSL%20pre-training%20are%20already%0Awell-suited%20for%20bioacoustic%20tasks.%20These%20findings%20highlight%20the%20robustness%20of%0Aspeech-pretrained%20SSL%20models%20for%20bioacoustics%20and%20imply%20that%20extensive%0Afine-tuning%20may%20not%20be%20necessary%20for%20optimal%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05987v1&entry.124074799=Read"},
{"title": "Pose-independent 3D Anthropometry from Sparse Data", "author": "David Bojani\u0107 and Stefanie Wuhrer and Tomislav Petkovi\u0107 and Tomislav Pribani\u0107", "abstract": "  3D digital anthropometry is the study of estimating human body measurements\nfrom 3D scans. Precise body measurements are important health indicators in the\nmedical industry, and guiding factors in the fashion, ergonomic and\nentertainment industries. The measuring protocol consists of scanning the whole\nsubject in the static A-pose, which is maintained without breathing or movement\nduring the scanning process. However, the A-pose is not easy to maintain during\nthe whole scanning process, which can last even up to a couple of minutes. This\nconstraint affects the final quality of the scan, which in turn affects the\naccuracy of the estimated body measurements obtained from methods that rely on\ndense geometric data. Additionally, this constraint makes it impossible to\ndevelop a digital anthropometry method for subjects unable to assume the\nA-pose, such as those with injuries or disabilities. We propose a method that\ncan obtain body measurements from sparse landmarks acquired in any pose. We\nmake use of the sparse landmarks of the posed subject to create\npose-independent features, and train a network to predict the body measurements\nas taken from the standard A-pose. We show that our method achieves comparable\nresults to competing methods that use dense geometry in the standard A-pose,\nbut has the capability of estimating the body measurements from any pose using\nsparse landmarks only. Finally, we address the lack of open-source 3D\nanthropometry methods by making our method available to the research community\nat https://github.com/DavidBoja/pose-independent-anthropometry.\n", "link": "http://arxiv.org/abs/2501.06014v1", "date": "2025-01-10", "relevancy": 2.4103, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6172}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6046}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose-independent%203D%20Anthropometry%20from%20Sparse%20Data&body=Title%3A%20Pose-independent%203D%20Anthropometry%20from%20Sparse%20Data%0AAuthor%3A%20David%20Bojani%C4%87%20and%20Stefanie%20Wuhrer%20and%20Tomislav%20Petkovi%C4%87%20and%20Tomislav%20Pribani%C4%87%0AAbstract%3A%20%20%203D%20digital%20anthropometry%20is%20the%20study%20of%20estimating%20human%20body%20measurements%0Afrom%203D%20scans.%20Precise%20body%20measurements%20are%20important%20health%20indicators%20in%20the%0Amedical%20industry%2C%20and%20guiding%20factors%20in%20the%20fashion%2C%20ergonomic%20and%0Aentertainment%20industries.%20The%20measuring%20protocol%20consists%20of%20scanning%20the%20whole%0Asubject%20in%20the%20static%20A-pose%2C%20which%20is%20maintained%20without%20breathing%20or%20movement%0Aduring%20the%20scanning%20process.%20However%2C%20the%20A-pose%20is%20not%20easy%20to%20maintain%20during%0Athe%20whole%20scanning%20process%2C%20which%20can%20last%20even%20up%20to%20a%20couple%20of%20minutes.%20This%0Aconstraint%20affects%20the%20final%20quality%20of%20the%20scan%2C%20which%20in%20turn%20affects%20the%0Aaccuracy%20of%20the%20estimated%20body%20measurements%20obtained%20from%20methods%20that%20rely%20on%0Adense%20geometric%20data.%20Additionally%2C%20this%20constraint%20makes%20it%20impossible%20to%0Adevelop%20a%20digital%20anthropometry%20method%20for%20subjects%20unable%20to%20assume%20the%0AA-pose%2C%20such%20as%20those%20with%20injuries%20or%20disabilities.%20We%20propose%20a%20method%20that%0Acan%20obtain%20body%20measurements%20from%20sparse%20landmarks%20acquired%20in%20any%20pose.%20We%0Amake%20use%20of%20the%20sparse%20landmarks%20of%20the%20posed%20subject%20to%20create%0Apose-independent%20features%2C%20and%20train%20a%20network%20to%20predict%20the%20body%20measurements%0Aas%20taken%20from%20the%20standard%20A-pose.%20We%20show%20that%20our%20method%20achieves%20comparable%0Aresults%20to%20competing%20methods%20that%20use%20dense%20geometry%20in%20the%20standard%20A-pose%2C%0Abut%20has%20the%20capability%20of%20estimating%20the%20body%20measurements%20from%20any%20pose%20using%0Asparse%20landmarks%20only.%20Finally%2C%20we%20address%20the%20lack%20of%20open-source%203D%0Aanthropometry%20methods%20by%20making%20our%20method%20available%20to%20the%20research%20community%0Aat%20https%3A//github.com/DavidBoja/pose-independent-anthropometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose-independent%25203D%2520Anthropometry%2520from%2520Sparse%2520Data%26entry.906535625%3DDavid%2520Bojani%25C4%2587%2520and%2520Stefanie%2520Wuhrer%2520and%2520Tomislav%2520Petkovi%25C4%2587%2520and%2520Tomislav%2520Pribani%25C4%2587%26entry.1292438233%3D%2520%25203D%2520digital%2520anthropometry%2520is%2520the%2520study%2520of%2520estimating%2520human%2520body%2520measurements%250Afrom%25203D%2520scans.%2520Precise%2520body%2520measurements%2520are%2520important%2520health%2520indicators%2520in%2520the%250Amedical%2520industry%252C%2520and%2520guiding%2520factors%2520in%2520the%2520fashion%252C%2520ergonomic%2520and%250Aentertainment%2520industries.%2520The%2520measuring%2520protocol%2520consists%2520of%2520scanning%2520the%2520whole%250Asubject%2520in%2520the%2520static%2520A-pose%252C%2520which%2520is%2520maintained%2520without%2520breathing%2520or%2520movement%250Aduring%2520the%2520scanning%2520process.%2520However%252C%2520the%2520A-pose%2520is%2520not%2520easy%2520to%2520maintain%2520during%250Athe%2520whole%2520scanning%2520process%252C%2520which%2520can%2520last%2520even%2520up%2520to%2520a%2520couple%2520of%2520minutes.%2520This%250Aconstraint%2520affects%2520the%2520final%2520quality%2520of%2520the%2520scan%252C%2520which%2520in%2520turn%2520affects%2520the%250Aaccuracy%2520of%2520the%2520estimated%2520body%2520measurements%2520obtained%2520from%2520methods%2520that%2520rely%2520on%250Adense%2520geometric%2520data.%2520Additionally%252C%2520this%2520constraint%2520makes%2520it%2520impossible%2520to%250Adevelop%2520a%2520digital%2520anthropometry%2520method%2520for%2520subjects%2520unable%2520to%2520assume%2520the%250AA-pose%252C%2520such%2520as%2520those%2520with%2520injuries%2520or%2520disabilities.%2520We%2520propose%2520a%2520method%2520that%250Acan%2520obtain%2520body%2520measurements%2520from%2520sparse%2520landmarks%2520acquired%2520in%2520any%2520pose.%2520We%250Amake%2520use%2520of%2520the%2520sparse%2520landmarks%2520of%2520the%2520posed%2520subject%2520to%2520create%250Apose-independent%2520features%252C%2520and%2520train%2520a%2520network%2520to%2520predict%2520the%2520body%2520measurements%250Aas%2520taken%2520from%2520the%2520standard%2520A-pose.%2520We%2520show%2520that%2520our%2520method%2520achieves%2520comparable%250Aresults%2520to%2520competing%2520methods%2520that%2520use%2520dense%2520geometry%2520in%2520the%2520standard%2520A-pose%252C%250Abut%2520has%2520the%2520capability%2520of%2520estimating%2520the%2520body%2520measurements%2520from%2520any%2520pose%2520using%250Asparse%2520landmarks%2520only.%2520Finally%252C%2520we%2520address%2520the%2520lack%2520of%2520open-source%25203D%250Aanthropometry%2520methods%2520by%2520making%2520our%2520method%2520available%2520to%2520the%2520research%2520community%250Aat%2520https%253A//github.com/DavidBoja/pose-independent-anthropometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose-independent%203D%20Anthropometry%20from%20Sparse%20Data&entry.906535625=David%20Bojani%C4%87%20and%20Stefanie%20Wuhrer%20and%20Tomislav%20Petkovi%C4%87%20and%20Tomislav%20Pribani%C4%87&entry.1292438233=%20%203D%20digital%20anthropometry%20is%20the%20study%20of%20estimating%20human%20body%20measurements%0Afrom%203D%20scans.%20Precise%20body%20measurements%20are%20important%20health%20indicators%20in%20the%0Amedical%20industry%2C%20and%20guiding%20factors%20in%20the%20fashion%2C%20ergonomic%20and%0Aentertainment%20industries.%20The%20measuring%20protocol%20consists%20of%20scanning%20the%20whole%0Asubject%20in%20the%20static%20A-pose%2C%20which%20is%20maintained%20without%20breathing%20or%20movement%0Aduring%20the%20scanning%20process.%20However%2C%20the%20A-pose%20is%20not%20easy%20to%20maintain%20during%0Athe%20whole%20scanning%20process%2C%20which%20can%20last%20even%20up%20to%20a%20couple%20of%20minutes.%20This%0Aconstraint%20affects%20the%20final%20quality%20of%20the%20scan%2C%20which%20in%20turn%20affects%20the%0Aaccuracy%20of%20the%20estimated%20body%20measurements%20obtained%20from%20methods%20that%20rely%20on%0Adense%20geometric%20data.%20Additionally%2C%20this%20constraint%20makes%20it%20impossible%20to%0Adevelop%20a%20digital%20anthropometry%20method%20for%20subjects%20unable%20to%20assume%20the%0AA-pose%2C%20such%20as%20those%20with%20injuries%20or%20disabilities.%20We%20propose%20a%20method%20that%0Acan%20obtain%20body%20measurements%20from%20sparse%20landmarks%20acquired%20in%20any%20pose.%20We%0Amake%20use%20of%20the%20sparse%20landmarks%20of%20the%20posed%20subject%20to%20create%0Apose-independent%20features%2C%20and%20train%20a%20network%20to%20predict%20the%20body%20measurements%0Aas%20taken%20from%20the%20standard%20A-pose.%20We%20show%20that%20our%20method%20achieves%20comparable%0Aresults%20to%20competing%20methods%20that%20use%20dense%20geometry%20in%20the%20standard%20A-pose%2C%0Abut%20has%20the%20capability%20of%20estimating%20the%20body%20measurements%20from%20any%20pose%20using%0Asparse%20landmarks%20only.%20Finally%2C%20we%20address%20the%20lack%20of%20open-source%203D%0Aanthropometry%20methods%20by%20making%20our%20method%20available%20to%20the%20research%20community%0Aat%20https%3A//github.com/DavidBoja/pose-independent-anthropometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06014v1&entry.124074799=Read"},
{"title": "Non-planar 3D Printing of Double Shells", "author": "Ioanna Mitropoulou and Amir Vaxman and Olga Diamanti and Benjamin Dillenburger", "abstract": "  We present a method to fabricate double shell structures printed in\ntrans-versal directions using multi-axis fused-deposition-modeling (FDM)\nrobot-ic 3D printing. Shell structures, characterized by lightweight, thin\nwalls, fast buildup, and minimal material usage, find diverse applications in\npro-totyping and architecture for uses such as fa\\c{c}ade panels, molds for\nconcrete casting, or full-scale pavilions. We leverage an underlying\nrepresentation of transversal strip networks generated using existing methods\nand propose a methodology for converting them into printable partitions. Each\npartition is printed separately and assembled into a double-shell structure. We\nout-line the specifications and workflow that make the printing of each piece\nand the subsequent assembly process feasible. The versatility and robust-ness\nof our method are demonstrated with both digital and fabricated re-sults on\nsurfaces of different scales and geometric complexity.\n", "link": "http://arxiv.org/abs/2501.06088v1", "date": "2025-01-10", "relevancy": 2.3859, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4941}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4941}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-planar%203D%20Printing%20of%20Double%20Shells&body=Title%3A%20Non-planar%203D%20Printing%20of%20Double%20Shells%0AAuthor%3A%20Ioanna%20Mitropoulou%20and%20Amir%20Vaxman%20and%20Olga%20Diamanti%20and%20Benjamin%20Dillenburger%0AAbstract%3A%20%20%20We%20present%20a%20method%20to%20fabricate%20double%20shell%20structures%20printed%20in%0Atrans-versal%20directions%20using%20multi-axis%20fused-deposition-modeling%20%28FDM%29%0Arobot-ic%203D%20printing.%20Shell%20structures%2C%20characterized%20by%20lightweight%2C%20thin%0Awalls%2C%20fast%20buildup%2C%20and%20minimal%20material%20usage%2C%20find%20diverse%20applications%20in%0Apro-totyping%20and%20architecture%20for%20uses%20such%20as%20fa%5Cc%7Bc%7Dade%20panels%2C%20molds%20for%0Aconcrete%20casting%2C%20or%20full-scale%20pavilions.%20We%20leverage%20an%20underlying%0Arepresentation%20of%20transversal%20strip%20networks%20generated%20using%20existing%20methods%0Aand%20propose%20a%20methodology%20for%20converting%20them%20into%20printable%20partitions.%20Each%0Apartition%20is%20printed%20separately%20and%20assembled%20into%20a%20double-shell%20structure.%20We%0Aout-line%20the%20specifications%20and%20workflow%20that%20make%20the%20printing%20of%20each%20piece%0Aand%20the%20subsequent%20assembly%20process%20feasible.%20The%20versatility%20and%20robust-ness%0Aof%20our%20method%20are%20demonstrated%20with%20both%20digital%20and%20fabricated%20re-sults%20on%0Asurfaces%20of%20different%20scales%20and%20geometric%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-planar%25203D%2520Printing%2520of%2520Double%2520Shells%26entry.906535625%3DIoanna%2520Mitropoulou%2520and%2520Amir%2520Vaxman%2520and%2520Olga%2520Diamanti%2520and%2520Benjamin%2520Dillenburger%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520to%2520fabricate%2520double%2520shell%2520structures%2520printed%2520in%250Atrans-versal%2520directions%2520using%2520multi-axis%2520fused-deposition-modeling%2520%2528FDM%2529%250Arobot-ic%25203D%2520printing.%2520Shell%2520structures%252C%2520characterized%2520by%2520lightweight%252C%2520thin%250Awalls%252C%2520fast%2520buildup%252C%2520and%2520minimal%2520material%2520usage%252C%2520find%2520diverse%2520applications%2520in%250Apro-totyping%2520and%2520architecture%2520for%2520uses%2520such%2520as%2520fa%255Cc%257Bc%257Dade%2520panels%252C%2520molds%2520for%250Aconcrete%2520casting%252C%2520or%2520full-scale%2520pavilions.%2520We%2520leverage%2520an%2520underlying%250Arepresentation%2520of%2520transversal%2520strip%2520networks%2520generated%2520using%2520existing%2520methods%250Aand%2520propose%2520a%2520methodology%2520for%2520converting%2520them%2520into%2520printable%2520partitions.%2520Each%250Apartition%2520is%2520printed%2520separately%2520and%2520assembled%2520into%2520a%2520double-shell%2520structure.%2520We%250Aout-line%2520the%2520specifications%2520and%2520workflow%2520that%2520make%2520the%2520printing%2520of%2520each%2520piece%250Aand%2520the%2520subsequent%2520assembly%2520process%2520feasible.%2520The%2520versatility%2520and%2520robust-ness%250Aof%2520our%2520method%2520are%2520demonstrated%2520with%2520both%2520digital%2520and%2520fabricated%2520re-sults%2520on%250Asurfaces%2520of%2520different%2520scales%2520and%2520geometric%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-planar%203D%20Printing%20of%20Double%20Shells&entry.906535625=Ioanna%20Mitropoulou%20and%20Amir%20Vaxman%20and%20Olga%20Diamanti%20and%20Benjamin%20Dillenburger&entry.1292438233=%20%20We%20present%20a%20method%20to%20fabricate%20double%20shell%20structures%20printed%20in%0Atrans-versal%20directions%20using%20multi-axis%20fused-deposition-modeling%20%28FDM%29%0Arobot-ic%203D%20printing.%20Shell%20structures%2C%20characterized%20by%20lightweight%2C%20thin%0Awalls%2C%20fast%20buildup%2C%20and%20minimal%20material%20usage%2C%20find%20diverse%20applications%20in%0Apro-totyping%20and%20architecture%20for%20uses%20such%20as%20fa%5Cc%7Bc%7Dade%20panels%2C%20molds%20for%0Aconcrete%20casting%2C%20or%20full-scale%20pavilions.%20We%20leverage%20an%20underlying%0Arepresentation%20of%20transversal%20strip%20networks%20generated%20using%20existing%20methods%0Aand%20propose%20a%20methodology%20for%20converting%20them%20into%20printable%20partitions.%20Each%0Apartition%20is%20printed%20separately%20and%20assembled%20into%20a%20double-shell%20structure.%20We%0Aout-line%20the%20specifications%20and%20workflow%20that%20make%20the%20printing%20of%20each%20piece%0Aand%20the%20subsequent%20assembly%20process%20feasible.%20The%20versatility%20and%20robust-ness%0Aof%20our%20method%20are%20demonstrated%20with%20both%20digital%20and%20fabricated%20re-sults%20on%0Asurfaces%20of%20different%20scales%20and%20geometric%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06088v1&entry.124074799=Read"},
{"title": "Fast unsupervised ground metric learning with tree-Wasserstein distance", "author": "Kira M. D\u00fcsterwald and Samo Hromadka and Makoto Yamada", "abstract": "  The performance of unsupervised methods such as clustering depends on the\nchoice of distance metric between features, or ground metric. Commonly, ground\nmetrics are decided with heuristics or learned via supervised algorithms.\nHowever, since many interesting datasets are unlabelled, unsupervised ground\nmetric learning approaches have been introduced. One promising option employs\nWasserstein singular vectors (WSVs), which emerge when computing optimal\ntransport distances between features and samples simultaneously. WSVs are\neffective, but can be prohibitively computationally expensive in some\napplications: $\\mathcal{O}(n^2m^2(n \\log(n) + m \\log(m))$ for $n$ samples and\n$m$ features. In this work, we propose to augment the WSV method by embedding\nsamples and features on trees, on which we compute the tree-Wasserstein\ndistance (TWD). We demonstrate theoretically and empirically that the algorithm\nconverges to a better approximation of the standard WSV approach than the best\nknown alternatives, and does so with $\\mathcal{O}(n^3+m^3+mn)$ complexity. In\naddition, we prove that the initial tree structure can be chosen flexibly,\nsince tree geometry does not constrain the richness of the approximation up to\nthe number of edge weights. This proof suggests a fast and recursive algorithm\nfor computing the tree parameter basis set, which we find crucial to realising\nthe efficiency gains at scale. Finally, we employ the tree-WSV algorithm to\nseveral single-cell RNA sequencing genomics datasets, demonstrating its\nscalability and utility for unsupervised cell-type clustering problems. These\nresults poise unsupervised ground metric learning with TWD as a low-rank\napproximation of WSV with the potential for widespread application.\n", "link": "http://arxiv.org/abs/2411.07432v2", "date": "2025-01-10", "relevancy": 2.3588, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4792}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4693}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20unsupervised%20ground%20metric%20learning%20with%20tree-Wasserstein%20distance&body=Title%3A%20Fast%20unsupervised%20ground%20metric%20learning%20with%20tree-Wasserstein%20distance%0AAuthor%3A%20Kira%20M.%20D%C3%BCsterwald%20and%20Samo%20Hromadka%20and%20Makoto%20Yamada%0AAbstract%3A%20%20%20The%20performance%20of%20unsupervised%20methods%20such%20as%20clustering%20depends%20on%20the%0Achoice%20of%20distance%20metric%20between%20features%2C%20or%20ground%20metric.%20Commonly%2C%20ground%0Ametrics%20are%20decided%20with%20heuristics%20or%20learned%20via%20supervised%20algorithms.%0AHowever%2C%20since%20many%20interesting%20datasets%20are%20unlabelled%2C%20unsupervised%20ground%0Ametric%20learning%20approaches%20have%20been%20introduced.%20One%20promising%20option%20employs%0AWasserstein%20singular%20vectors%20%28WSVs%29%2C%20which%20emerge%20when%20computing%20optimal%0Atransport%20distances%20between%20features%20and%20samples%20simultaneously.%20WSVs%20are%0Aeffective%2C%20but%20can%20be%20prohibitively%20computationally%20expensive%20in%20some%0Aapplications%3A%20%24%5Cmathcal%7BO%7D%28n%5E2m%5E2%28n%20%5Clog%28n%29%20%2B%20m%20%5Clog%28m%29%29%24%20for%20%24n%24%20samples%20and%0A%24m%24%20features.%20In%20this%20work%2C%20we%20propose%20to%20augment%20the%20WSV%20method%20by%20embedding%0Asamples%20and%20features%20on%20trees%2C%20on%20which%20we%20compute%20the%20tree-Wasserstein%0Adistance%20%28TWD%29.%20We%20demonstrate%20theoretically%20and%20empirically%20that%20the%20algorithm%0Aconverges%20to%20a%20better%20approximation%20of%20the%20standard%20WSV%20approach%20than%20the%20best%0Aknown%20alternatives%2C%20and%20does%20so%20with%20%24%5Cmathcal%7BO%7D%28n%5E3%2Bm%5E3%2Bmn%29%24%20complexity.%20In%0Aaddition%2C%20we%20prove%20that%20the%20initial%20tree%20structure%20can%20be%20chosen%20flexibly%2C%0Asince%20tree%20geometry%20does%20not%20constrain%20the%20richness%20of%20the%20approximation%20up%20to%0Athe%20number%20of%20edge%20weights.%20This%20proof%20suggests%20a%20fast%20and%20recursive%20algorithm%0Afor%20computing%20the%20tree%20parameter%20basis%20set%2C%20which%20we%20find%20crucial%20to%20realising%0Athe%20efficiency%20gains%20at%20scale.%20Finally%2C%20we%20employ%20the%20tree-WSV%20algorithm%20to%0Aseveral%20single-cell%20RNA%20sequencing%20genomics%20datasets%2C%20demonstrating%20its%0Ascalability%20and%20utility%20for%20unsupervised%20cell-type%20clustering%20problems.%20These%0Aresults%20poise%20unsupervised%20ground%20metric%20learning%20with%20TWD%20as%20a%20low-rank%0Aapproximation%20of%20WSV%20with%20the%20potential%20for%20widespread%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520unsupervised%2520ground%2520metric%2520learning%2520with%2520tree-Wasserstein%2520distance%26entry.906535625%3DKira%2520M.%2520D%25C3%25BCsterwald%2520and%2520Samo%2520Hromadka%2520and%2520Makoto%2520Yamada%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520unsupervised%2520methods%2520such%2520as%2520clustering%2520depends%2520on%2520the%250Achoice%2520of%2520distance%2520metric%2520between%2520features%252C%2520or%2520ground%2520metric.%2520Commonly%252C%2520ground%250Ametrics%2520are%2520decided%2520with%2520heuristics%2520or%2520learned%2520via%2520supervised%2520algorithms.%250AHowever%252C%2520since%2520many%2520interesting%2520datasets%2520are%2520unlabelled%252C%2520unsupervised%2520ground%250Ametric%2520learning%2520approaches%2520have%2520been%2520introduced.%2520One%2520promising%2520option%2520employs%250AWasserstein%2520singular%2520vectors%2520%2528WSVs%2529%252C%2520which%2520emerge%2520when%2520computing%2520optimal%250Atransport%2520distances%2520between%2520features%2520and%2520samples%2520simultaneously.%2520WSVs%2520are%250Aeffective%252C%2520but%2520can%2520be%2520prohibitively%2520computationally%2520expensive%2520in%2520some%250Aapplications%253A%2520%2524%255Cmathcal%257BO%257D%2528n%255E2m%255E2%2528n%2520%255Clog%2528n%2529%2520%252B%2520m%2520%255Clog%2528m%2529%2529%2524%2520for%2520%2524n%2524%2520samples%2520and%250A%2524m%2524%2520features.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520augment%2520the%2520WSV%2520method%2520by%2520embedding%250Asamples%2520and%2520features%2520on%2520trees%252C%2520on%2520which%2520we%2520compute%2520the%2520tree-Wasserstein%250Adistance%2520%2528TWD%2529.%2520We%2520demonstrate%2520theoretically%2520and%2520empirically%2520that%2520the%2520algorithm%250Aconverges%2520to%2520a%2520better%2520approximation%2520of%2520the%2520standard%2520WSV%2520approach%2520than%2520the%2520best%250Aknown%2520alternatives%252C%2520and%2520does%2520so%2520with%2520%2524%255Cmathcal%257BO%257D%2528n%255E3%252Bm%255E3%252Bmn%2529%2524%2520complexity.%2520In%250Aaddition%252C%2520we%2520prove%2520that%2520the%2520initial%2520tree%2520structure%2520can%2520be%2520chosen%2520flexibly%252C%250Asince%2520tree%2520geometry%2520does%2520not%2520constrain%2520the%2520richness%2520of%2520the%2520approximation%2520up%2520to%250Athe%2520number%2520of%2520edge%2520weights.%2520This%2520proof%2520suggests%2520a%2520fast%2520and%2520recursive%2520algorithm%250Afor%2520computing%2520the%2520tree%2520parameter%2520basis%2520set%252C%2520which%2520we%2520find%2520crucial%2520to%2520realising%250Athe%2520efficiency%2520gains%2520at%2520scale.%2520Finally%252C%2520we%2520employ%2520the%2520tree-WSV%2520algorithm%2520to%250Aseveral%2520single-cell%2520RNA%2520sequencing%2520genomics%2520datasets%252C%2520demonstrating%2520its%250Ascalability%2520and%2520utility%2520for%2520unsupervised%2520cell-type%2520clustering%2520problems.%2520These%250Aresults%2520poise%2520unsupervised%2520ground%2520metric%2520learning%2520with%2520TWD%2520as%2520a%2520low-rank%250Aapproximation%2520of%2520WSV%2520with%2520the%2520potential%2520for%2520widespread%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20unsupervised%20ground%20metric%20learning%20with%20tree-Wasserstein%20distance&entry.906535625=Kira%20M.%20D%C3%BCsterwald%20and%20Samo%20Hromadka%20and%20Makoto%20Yamada&entry.1292438233=%20%20The%20performance%20of%20unsupervised%20methods%20such%20as%20clustering%20depends%20on%20the%0Achoice%20of%20distance%20metric%20between%20features%2C%20or%20ground%20metric.%20Commonly%2C%20ground%0Ametrics%20are%20decided%20with%20heuristics%20or%20learned%20via%20supervised%20algorithms.%0AHowever%2C%20since%20many%20interesting%20datasets%20are%20unlabelled%2C%20unsupervised%20ground%0Ametric%20learning%20approaches%20have%20been%20introduced.%20One%20promising%20option%20employs%0AWasserstein%20singular%20vectors%20%28WSVs%29%2C%20which%20emerge%20when%20computing%20optimal%0Atransport%20distances%20between%20features%20and%20samples%20simultaneously.%20WSVs%20are%0Aeffective%2C%20but%20can%20be%20prohibitively%20computationally%20expensive%20in%20some%0Aapplications%3A%20%24%5Cmathcal%7BO%7D%28n%5E2m%5E2%28n%20%5Clog%28n%29%20%2B%20m%20%5Clog%28m%29%29%24%20for%20%24n%24%20samples%20and%0A%24m%24%20features.%20In%20this%20work%2C%20we%20propose%20to%20augment%20the%20WSV%20method%20by%20embedding%0Asamples%20and%20features%20on%20trees%2C%20on%20which%20we%20compute%20the%20tree-Wasserstein%0Adistance%20%28TWD%29.%20We%20demonstrate%20theoretically%20and%20empirically%20that%20the%20algorithm%0Aconverges%20to%20a%20better%20approximation%20of%20the%20standard%20WSV%20approach%20than%20the%20best%0Aknown%20alternatives%2C%20and%20does%20so%20with%20%24%5Cmathcal%7BO%7D%28n%5E3%2Bm%5E3%2Bmn%29%24%20complexity.%20In%0Aaddition%2C%20we%20prove%20that%20the%20initial%20tree%20structure%20can%20be%20chosen%20flexibly%2C%0Asince%20tree%20geometry%20does%20not%20constrain%20the%20richness%20of%20the%20approximation%20up%20to%0Athe%20number%20of%20edge%20weights.%20This%20proof%20suggests%20a%20fast%20and%20recursive%20algorithm%0Afor%20computing%20the%20tree%20parameter%20basis%20set%2C%20which%20we%20find%20crucial%20to%20realising%0Athe%20efficiency%20gains%20at%20scale.%20Finally%2C%20we%20employ%20the%20tree-WSV%20algorithm%20to%0Aseveral%20single-cell%20RNA%20sequencing%20genomics%20datasets%2C%20demonstrating%20its%0Ascalability%20and%20utility%20for%20unsupervised%20cell-type%20clustering%20problems.%20These%0Aresults%20poise%20unsupervised%20ground%20metric%20learning%20with%20TWD%20as%20a%20low-rank%0Aapproximation%20of%20WSV%20with%20the%20potential%20for%20widespread%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07432v2&entry.124074799=Read"},
{"title": "FaceMe: Robust Blind Face Restoration with Personal Identification", "author": "Siyu Liu and Zheng-Peng Duan and Jia OuYang and Jiayi Fu and Hyunhee Park and Zikun Liu and Chun-Le Guo and Chongyi Li", "abstract": "  Blind face restoration is a highly ill-posed problem due to the lack of\nnecessary context. Although existing methods produce high-quality outputs, they\noften fail to faithfully preserve the individual's identity. In this paper, we\npropose a personalized face restoration method, FaceMe, based on a diffusion\nmodel. Given a single or a few reference images, we use an identity encoder to\nextract identity-related features, which serve as prompts to guide the\ndiffusion model in restoring high-quality and identity-consistent facial\nimages. By simply combining identity-related features, we effectively minimize\nthe impact of identity-irrelevant features during training and support any\nnumber of reference image inputs during inference. Additionally, thanks to the\nrobustness of the identity encoder, synthesized images can be used as reference\nimages during training, and identity changing during inference does not require\nfine-tuning the model. We also propose a pipeline for constructing a reference\nimage training pool that simulates the poses and expressions that may appear in\nreal-world scenarios. Experimental results demonstrate that our FaceMe can\nrestore high-quality facial images while maintaining identity consistency,\nachieving excellent performance and robustness.\n", "link": "http://arxiv.org/abs/2501.05177v2", "date": "2025-01-10", "relevancy": 2.3503, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6299}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5982}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceMe%3A%20Robust%20Blind%20Face%20Restoration%20with%20Personal%20Identification&body=Title%3A%20FaceMe%3A%20Robust%20Blind%20Face%20Restoration%20with%20Personal%20Identification%0AAuthor%3A%20Siyu%20Liu%20and%20Zheng-Peng%20Duan%20and%20Jia%20OuYang%20and%20Jiayi%20Fu%20and%20Hyunhee%20Park%20and%20Zikun%20Liu%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li%0AAbstract%3A%20%20%20Blind%20face%20restoration%20is%20a%20highly%20ill-posed%20problem%20due%20to%20the%20lack%20of%0Anecessary%20context.%20Although%20existing%20methods%20produce%20high-quality%20outputs%2C%20they%0Aoften%20fail%20to%20faithfully%20preserve%20the%20individual%27s%20identity.%20In%20this%20paper%2C%20we%0Apropose%20a%20personalized%20face%20restoration%20method%2C%20FaceMe%2C%20based%20on%20a%20diffusion%0Amodel.%20Given%20a%20single%20or%20a%20few%20reference%20images%2C%20we%20use%20an%20identity%20encoder%20to%0Aextract%20identity-related%20features%2C%20which%20serve%20as%20prompts%20to%20guide%20the%0Adiffusion%20model%20in%20restoring%20high-quality%20and%20identity-consistent%20facial%0Aimages.%20By%20simply%20combining%20identity-related%20features%2C%20we%20effectively%20minimize%0Athe%20impact%20of%20identity-irrelevant%20features%20during%20training%20and%20support%20any%0Anumber%20of%20reference%20image%20inputs%20during%20inference.%20Additionally%2C%20thanks%20to%20the%0Arobustness%20of%20the%20identity%20encoder%2C%20synthesized%20images%20can%20be%20used%20as%20reference%0Aimages%20during%20training%2C%20and%20identity%20changing%20during%20inference%20does%20not%20require%0Afine-tuning%20the%20model.%20We%20also%20propose%20a%20pipeline%20for%20constructing%20a%20reference%0Aimage%20training%20pool%20that%20simulates%20the%20poses%20and%20expressions%20that%20may%20appear%20in%0Areal-world%20scenarios.%20Experimental%20results%20demonstrate%20that%20our%20FaceMe%20can%0Arestore%20high-quality%20facial%20images%20while%20maintaining%20identity%20consistency%2C%0Aachieving%20excellent%20performance%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05177v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceMe%253A%2520Robust%2520Blind%2520Face%2520Restoration%2520with%2520Personal%2520Identification%26entry.906535625%3DSiyu%2520Liu%2520and%2520Zheng-Peng%2520Duan%2520and%2520Jia%2520OuYang%2520and%2520Jiayi%2520Fu%2520and%2520Hyunhee%2520Park%2520and%2520Zikun%2520Liu%2520and%2520Chun-Le%2520Guo%2520and%2520Chongyi%2520Li%26entry.1292438233%3D%2520%2520Blind%2520face%2520restoration%2520is%2520a%2520highly%2520ill-posed%2520problem%2520due%2520to%2520the%2520lack%2520of%250Anecessary%2520context.%2520Although%2520existing%2520methods%2520produce%2520high-quality%2520outputs%252C%2520they%250Aoften%2520fail%2520to%2520faithfully%2520preserve%2520the%2520individual%2527s%2520identity.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520personalized%2520face%2520restoration%2520method%252C%2520FaceMe%252C%2520based%2520on%2520a%2520diffusion%250Amodel.%2520Given%2520a%2520single%2520or%2520a%2520few%2520reference%2520images%252C%2520we%2520use%2520an%2520identity%2520encoder%2520to%250Aextract%2520identity-related%2520features%252C%2520which%2520serve%2520as%2520prompts%2520to%2520guide%2520the%250Adiffusion%2520model%2520in%2520restoring%2520high-quality%2520and%2520identity-consistent%2520facial%250Aimages.%2520By%2520simply%2520combining%2520identity-related%2520features%252C%2520we%2520effectively%2520minimize%250Athe%2520impact%2520of%2520identity-irrelevant%2520features%2520during%2520training%2520and%2520support%2520any%250Anumber%2520of%2520reference%2520image%2520inputs%2520during%2520inference.%2520Additionally%252C%2520thanks%2520to%2520the%250Arobustness%2520of%2520the%2520identity%2520encoder%252C%2520synthesized%2520images%2520can%2520be%2520used%2520as%2520reference%250Aimages%2520during%2520training%252C%2520and%2520identity%2520changing%2520during%2520inference%2520does%2520not%2520require%250Afine-tuning%2520the%2520model.%2520We%2520also%2520propose%2520a%2520pipeline%2520for%2520constructing%2520a%2520reference%250Aimage%2520training%2520pool%2520that%2520simulates%2520the%2520poses%2520and%2520expressions%2520that%2520may%2520appear%2520in%250Areal-world%2520scenarios.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520FaceMe%2520can%250Arestore%2520high-quality%2520facial%2520images%2520while%2520maintaining%2520identity%2520consistency%252C%250Aachieving%2520excellent%2520performance%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05177v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceMe%3A%20Robust%20Blind%20Face%20Restoration%20with%20Personal%20Identification&entry.906535625=Siyu%20Liu%20and%20Zheng-Peng%20Duan%20and%20Jia%20OuYang%20and%20Jiayi%20Fu%20and%20Hyunhee%20Park%20and%20Zikun%20Liu%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li&entry.1292438233=%20%20Blind%20face%20restoration%20is%20a%20highly%20ill-posed%20problem%20due%20to%20the%20lack%20of%0Anecessary%20context.%20Although%20existing%20methods%20produce%20high-quality%20outputs%2C%20they%0Aoften%20fail%20to%20faithfully%20preserve%20the%20individual%27s%20identity.%20In%20this%20paper%2C%20we%0Apropose%20a%20personalized%20face%20restoration%20method%2C%20FaceMe%2C%20based%20on%20a%20diffusion%0Amodel.%20Given%20a%20single%20or%20a%20few%20reference%20images%2C%20we%20use%20an%20identity%20encoder%20to%0Aextract%20identity-related%20features%2C%20which%20serve%20as%20prompts%20to%20guide%20the%0Adiffusion%20model%20in%20restoring%20high-quality%20and%20identity-consistent%20facial%0Aimages.%20By%20simply%20combining%20identity-related%20features%2C%20we%20effectively%20minimize%0Athe%20impact%20of%20identity-irrelevant%20features%20during%20training%20and%20support%20any%0Anumber%20of%20reference%20image%20inputs%20during%20inference.%20Additionally%2C%20thanks%20to%20the%0Arobustness%20of%20the%20identity%20encoder%2C%20synthesized%20images%20can%20be%20used%20as%20reference%0Aimages%20during%20training%2C%20and%20identity%20changing%20during%20inference%20does%20not%20require%0Afine-tuning%20the%20model.%20We%20also%20propose%20a%20pipeline%20for%20constructing%20a%20reference%0Aimage%20training%20pool%20that%20simulates%20the%20poses%20and%20expressions%20that%20may%20appear%20in%0Areal-world%20scenarios.%20Experimental%20results%20demonstrate%20that%20our%20FaceMe%20can%0Arestore%20high-quality%20facial%20images%20while%20maintaining%20identity%20consistency%2C%0Aachieving%20excellent%20performance%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05177v2&entry.124074799=Read"},
{"title": "VideoAuteur: Towards Long Narrative Video Generation", "author": "Junfei Xiao and Feng Cheng and Lu Qi and Liangke Gui and Jiepeng Cen and Zhibei Ma and Alan Yuille and Lu Jiang", "abstract": "  Recent video generation models have shown promising results in producing\nhigh-quality video clips lasting several seconds. However, these models face\nchallenges in generating long sequences that convey clear and informative\nevents, limiting their ability to support coherent narrations. In this paper,\nwe present a large-scale cooking video dataset designed to advance long-form\nnarrative generation in the cooking domain. We validate the quality of our\nproposed dataset in terms of visual fidelity and textual caption accuracy using\nstate-of-the-art Vision-Language Models (VLMs) and video generation models,\nrespectively. We further introduce a Long Narrative Video Director to enhance\nboth visual and semantic coherence in generated videos and emphasize the role\nof aligning visual embeddings to achieve improved overall video quality. Our\nmethod demonstrates substantial improvements in generating visually detailed\nand semantically aligned keyframes, supported by finetuning techniques that\nintegrate text and image embeddings within the video generation process.\nProject page: https://videoauteur.github.io/\n", "link": "http://arxiv.org/abs/2501.06173v1", "date": "2025-01-10", "relevancy": 2.3241, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5972}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5897}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoAuteur%3A%20Towards%20Long%20Narrative%20Video%20Generation&body=Title%3A%20VideoAuteur%3A%20Towards%20Long%20Narrative%20Video%20Generation%0AAuthor%3A%20Junfei%20Xiao%20and%20Feng%20Cheng%20and%20Lu%20Qi%20and%20Liangke%20Gui%20and%20Jiepeng%20Cen%20and%20Zhibei%20Ma%20and%20Alan%20Yuille%20and%20Lu%20Jiang%0AAbstract%3A%20%20%20Recent%20video%20generation%20models%20have%20shown%20promising%20results%20in%20producing%0Ahigh-quality%20video%20clips%20lasting%20several%20seconds.%20However%2C%20these%20models%20face%0Achallenges%20in%20generating%20long%20sequences%20that%20convey%20clear%20and%20informative%0Aevents%2C%20limiting%20their%20ability%20to%20support%20coherent%20narrations.%20In%20this%20paper%2C%0Awe%20present%20a%20large-scale%20cooking%20video%20dataset%20designed%20to%20advance%20long-form%0Anarrative%20generation%20in%20the%20cooking%20domain.%20We%20validate%20the%20quality%20of%20our%0Aproposed%20dataset%20in%20terms%20of%20visual%20fidelity%20and%20textual%20caption%20accuracy%20using%0Astate-of-the-art%20Vision-Language%20Models%20%28VLMs%29%20and%20video%20generation%20models%2C%0Arespectively.%20We%20further%20introduce%20a%20Long%20Narrative%20Video%20Director%20to%20enhance%0Aboth%20visual%20and%20semantic%20coherence%20in%20generated%20videos%20and%20emphasize%20the%20role%0Aof%20aligning%20visual%20embeddings%20to%20achieve%20improved%20overall%20video%20quality.%20Our%0Amethod%20demonstrates%20substantial%20improvements%20in%20generating%20visually%20detailed%0Aand%20semantically%20aligned%20keyframes%2C%20supported%20by%20finetuning%20techniques%20that%0Aintegrate%20text%20and%20image%20embeddings%20within%20the%20video%20generation%20process.%0AProject%20page%3A%20https%3A//videoauteur.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoAuteur%253A%2520Towards%2520Long%2520Narrative%2520Video%2520Generation%26entry.906535625%3DJunfei%2520Xiao%2520and%2520Feng%2520Cheng%2520and%2520Lu%2520Qi%2520and%2520Liangke%2520Gui%2520and%2520Jiepeng%2520Cen%2520and%2520Zhibei%2520Ma%2520and%2520Alan%2520Yuille%2520and%2520Lu%2520Jiang%26entry.1292438233%3D%2520%2520Recent%2520video%2520generation%2520models%2520have%2520shown%2520promising%2520results%2520in%2520producing%250Ahigh-quality%2520video%2520clips%2520lasting%2520several%2520seconds.%2520However%252C%2520these%2520models%2520face%250Achallenges%2520in%2520generating%2520long%2520sequences%2520that%2520convey%2520clear%2520and%2520informative%250Aevents%252C%2520limiting%2520their%2520ability%2520to%2520support%2520coherent%2520narrations.%2520In%2520this%2520paper%252C%250Awe%2520present%2520a%2520large-scale%2520cooking%2520video%2520dataset%2520designed%2520to%2520advance%2520long-form%250Anarrative%2520generation%2520in%2520the%2520cooking%2520domain.%2520We%2520validate%2520the%2520quality%2520of%2520our%250Aproposed%2520dataset%2520in%2520terms%2520of%2520visual%2520fidelity%2520and%2520textual%2520caption%2520accuracy%2520using%250Astate-of-the-art%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520and%2520video%2520generation%2520models%252C%250Arespectively.%2520We%2520further%2520introduce%2520a%2520Long%2520Narrative%2520Video%2520Director%2520to%2520enhance%250Aboth%2520visual%2520and%2520semantic%2520coherence%2520in%2520generated%2520videos%2520and%2520emphasize%2520the%2520role%250Aof%2520aligning%2520visual%2520embeddings%2520to%2520achieve%2520improved%2520overall%2520video%2520quality.%2520Our%250Amethod%2520demonstrates%2520substantial%2520improvements%2520in%2520generating%2520visually%2520detailed%250Aand%2520semantically%2520aligned%2520keyframes%252C%2520supported%2520by%2520finetuning%2520techniques%2520that%250Aintegrate%2520text%2520and%2520image%2520embeddings%2520within%2520the%2520video%2520generation%2520process.%250AProject%2520page%253A%2520https%253A//videoauteur.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoAuteur%3A%20Towards%20Long%20Narrative%20Video%20Generation&entry.906535625=Junfei%20Xiao%20and%20Feng%20Cheng%20and%20Lu%20Qi%20and%20Liangke%20Gui%20and%20Jiepeng%20Cen%20and%20Zhibei%20Ma%20and%20Alan%20Yuille%20and%20Lu%20Jiang&entry.1292438233=%20%20Recent%20video%20generation%20models%20have%20shown%20promising%20results%20in%20producing%0Ahigh-quality%20video%20clips%20lasting%20several%20seconds.%20However%2C%20these%20models%20face%0Achallenges%20in%20generating%20long%20sequences%20that%20convey%20clear%20and%20informative%0Aevents%2C%20limiting%20their%20ability%20to%20support%20coherent%20narrations.%20In%20this%20paper%2C%0Awe%20present%20a%20large-scale%20cooking%20video%20dataset%20designed%20to%20advance%20long-form%0Anarrative%20generation%20in%20the%20cooking%20domain.%20We%20validate%20the%20quality%20of%20our%0Aproposed%20dataset%20in%20terms%20of%20visual%20fidelity%20and%20textual%20caption%20accuracy%20using%0Astate-of-the-art%20Vision-Language%20Models%20%28VLMs%29%20and%20video%20generation%20models%2C%0Arespectively.%20We%20further%20introduce%20a%20Long%20Narrative%20Video%20Director%20to%20enhance%0Aboth%20visual%20and%20semantic%20coherence%20in%20generated%20videos%20and%20emphasize%20the%20role%0Aof%20aligning%20visual%20embeddings%20to%20achieve%20improved%20overall%20video%20quality.%20Our%0Amethod%20demonstrates%20substantial%20improvements%20in%20generating%20visually%20detailed%0Aand%20semantically%20aligned%20keyframes%2C%20supported%20by%20finetuning%20techniques%20that%0Aintegrate%20text%20and%20image%20embeddings%20within%20the%20video%20generation%20process.%0AProject%20page%3A%20https%3A//videoauteur.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06173v1&entry.124074799=Read"},
{"title": "A Steerable Deep Network for Model-Free Diffusion MRI Registration", "author": "Gianfranco Cortes and Xiaoda Qu and Baba C. Vemuri", "abstract": "  Nonrigid registration is vital to medical image analysis but remains\nchallenging for diffusion MRI (dMRI) due to its high-dimensional,\norientation-dependent nature. While classical methods are accurate, they are\ncomputationally demanding, and deep neural networks, though efficient, have\nbeen underexplored for nonrigid dMRI registration compared to structural\nimaging. We present a novel, deep learning framework for model-free, nonrigid\nregistration of raw diffusion MRI data that does not require explicit\nreorientation. Unlike previous methods relying on derived representations such\nas diffusion tensors or fiber orientation distribution functions, in our\napproach, we formulate the registration as an equivariant diffeomorphism of\nposition-and-orientation space. Central to our method is an\n$\\mathsf{SE}(3)$-equivariant UNet that generates velocity fields while\npreserving the geometric properties of a raw dMRI's domain. We introduce a new\nloss function based on the maximum mean discrepancy in Fourier space,\nimplicitly matching ensemble average propagators across images. Experimental\nresults on Human Connectome Project dMRI data demonstrate competitive\nperformance compared to state-of-the-art approaches, with the added advantage\nof bypassing the overhead for estimating derived representations. This work\nestablishes a foundation for data-driven, geometry-aware dMRI registration\ndirectly in the acquisition space.\n", "link": "http://arxiv.org/abs/2501.04794v2", "date": "2025-01-10", "relevancy": 2.2908, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5945}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5597}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Steerable%20Deep%20Network%20for%20Model-Free%20Diffusion%20MRI%20Registration&body=Title%3A%20A%20Steerable%20Deep%20Network%20for%20Model-Free%20Diffusion%20MRI%20Registration%0AAuthor%3A%20Gianfranco%20Cortes%20and%20Xiaoda%20Qu%20and%20Baba%20C.%20Vemuri%0AAbstract%3A%20%20%20Nonrigid%20registration%20is%20vital%20to%20medical%20image%20analysis%20but%20remains%0Achallenging%20for%20diffusion%20MRI%20%28dMRI%29%20due%20to%20its%20high-dimensional%2C%0Aorientation-dependent%20nature.%20While%20classical%20methods%20are%20accurate%2C%20they%20are%0Acomputationally%20demanding%2C%20and%20deep%20neural%20networks%2C%20though%20efficient%2C%20have%0Abeen%20underexplored%20for%20nonrigid%20dMRI%20registration%20compared%20to%20structural%0Aimaging.%20We%20present%20a%20novel%2C%20deep%20learning%20framework%20for%20model-free%2C%20nonrigid%0Aregistration%20of%20raw%20diffusion%20MRI%20data%20that%20does%20not%20require%20explicit%0Areorientation.%20Unlike%20previous%20methods%20relying%20on%20derived%20representations%20such%0Aas%20diffusion%20tensors%20or%20fiber%20orientation%20distribution%20functions%2C%20in%20our%0Aapproach%2C%20we%20formulate%20the%20registration%20as%20an%20equivariant%20diffeomorphism%20of%0Aposition-and-orientation%20space.%20Central%20to%20our%20method%20is%20an%0A%24%5Cmathsf%7BSE%7D%283%29%24-equivariant%20UNet%20that%20generates%20velocity%20fields%20while%0Apreserving%20the%20geometric%20properties%20of%20a%20raw%20dMRI%27s%20domain.%20We%20introduce%20a%20new%0Aloss%20function%20based%20on%20the%20maximum%20mean%20discrepancy%20in%20Fourier%20space%2C%0Aimplicitly%20matching%20ensemble%20average%20propagators%20across%20images.%20Experimental%0Aresults%20on%20Human%20Connectome%20Project%20dMRI%20data%20demonstrate%20competitive%0Aperformance%20compared%20to%20state-of-the-art%20approaches%2C%20with%20the%20added%20advantage%0Aof%20bypassing%20the%20overhead%20for%20estimating%20derived%20representations.%20This%20work%0Aestablishes%20a%20foundation%20for%20data-driven%2C%20geometry-aware%20dMRI%20registration%0Adirectly%20in%20the%20acquisition%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Steerable%2520Deep%2520Network%2520for%2520Model-Free%2520Diffusion%2520MRI%2520Registration%26entry.906535625%3DGianfranco%2520Cortes%2520and%2520Xiaoda%2520Qu%2520and%2520Baba%2520C.%2520Vemuri%26entry.1292438233%3D%2520%2520Nonrigid%2520registration%2520is%2520vital%2520to%2520medical%2520image%2520analysis%2520but%2520remains%250Achallenging%2520for%2520diffusion%2520MRI%2520%2528dMRI%2529%2520due%2520to%2520its%2520high-dimensional%252C%250Aorientation-dependent%2520nature.%2520While%2520classical%2520methods%2520are%2520accurate%252C%2520they%2520are%250Acomputationally%2520demanding%252C%2520and%2520deep%2520neural%2520networks%252C%2520though%2520efficient%252C%2520have%250Abeen%2520underexplored%2520for%2520nonrigid%2520dMRI%2520registration%2520compared%2520to%2520structural%250Aimaging.%2520We%2520present%2520a%2520novel%252C%2520deep%2520learning%2520framework%2520for%2520model-free%252C%2520nonrigid%250Aregistration%2520of%2520raw%2520diffusion%2520MRI%2520data%2520that%2520does%2520not%2520require%2520explicit%250Areorientation.%2520Unlike%2520previous%2520methods%2520relying%2520on%2520derived%2520representations%2520such%250Aas%2520diffusion%2520tensors%2520or%2520fiber%2520orientation%2520distribution%2520functions%252C%2520in%2520our%250Aapproach%252C%2520we%2520formulate%2520the%2520registration%2520as%2520an%2520equivariant%2520diffeomorphism%2520of%250Aposition-and-orientation%2520space.%2520Central%2520to%2520our%2520method%2520is%2520an%250A%2524%255Cmathsf%257BSE%257D%25283%2529%2524-equivariant%2520UNet%2520that%2520generates%2520velocity%2520fields%2520while%250Apreserving%2520the%2520geometric%2520properties%2520of%2520a%2520raw%2520dMRI%2527s%2520domain.%2520We%2520introduce%2520a%2520new%250Aloss%2520function%2520based%2520on%2520the%2520maximum%2520mean%2520discrepancy%2520in%2520Fourier%2520space%252C%250Aimplicitly%2520matching%2520ensemble%2520average%2520propagators%2520across%2520images.%2520Experimental%250Aresults%2520on%2520Human%2520Connectome%2520Project%2520dMRI%2520data%2520demonstrate%2520competitive%250Aperformance%2520compared%2520to%2520state-of-the-art%2520approaches%252C%2520with%2520the%2520added%2520advantage%250Aof%2520bypassing%2520the%2520overhead%2520for%2520estimating%2520derived%2520representations.%2520This%2520work%250Aestablishes%2520a%2520foundation%2520for%2520data-driven%252C%2520geometry-aware%2520dMRI%2520registration%250Adirectly%2520in%2520the%2520acquisition%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Steerable%20Deep%20Network%20for%20Model-Free%20Diffusion%20MRI%20Registration&entry.906535625=Gianfranco%20Cortes%20and%20Xiaoda%20Qu%20and%20Baba%20C.%20Vemuri&entry.1292438233=%20%20Nonrigid%20registration%20is%20vital%20to%20medical%20image%20analysis%20but%20remains%0Achallenging%20for%20diffusion%20MRI%20%28dMRI%29%20due%20to%20its%20high-dimensional%2C%0Aorientation-dependent%20nature.%20While%20classical%20methods%20are%20accurate%2C%20they%20are%0Acomputationally%20demanding%2C%20and%20deep%20neural%20networks%2C%20though%20efficient%2C%20have%0Abeen%20underexplored%20for%20nonrigid%20dMRI%20registration%20compared%20to%20structural%0Aimaging.%20We%20present%20a%20novel%2C%20deep%20learning%20framework%20for%20model-free%2C%20nonrigid%0Aregistration%20of%20raw%20diffusion%20MRI%20data%20that%20does%20not%20require%20explicit%0Areorientation.%20Unlike%20previous%20methods%20relying%20on%20derived%20representations%20such%0Aas%20diffusion%20tensors%20or%20fiber%20orientation%20distribution%20functions%2C%20in%20our%0Aapproach%2C%20we%20formulate%20the%20registration%20as%20an%20equivariant%20diffeomorphism%20of%0Aposition-and-orientation%20space.%20Central%20to%20our%20method%20is%20an%0A%24%5Cmathsf%7BSE%7D%283%29%24-equivariant%20UNet%20that%20generates%20velocity%20fields%20while%0Apreserving%20the%20geometric%20properties%20of%20a%20raw%20dMRI%27s%20domain.%20We%20introduce%20a%20new%0Aloss%20function%20based%20on%20the%20maximum%20mean%20discrepancy%20in%20Fourier%20space%2C%0Aimplicitly%20matching%20ensemble%20average%20propagators%20across%20images.%20Experimental%0Aresults%20on%20Human%20Connectome%20Project%20dMRI%20data%20demonstrate%20competitive%0Aperformance%20compared%20to%20state-of-the-art%20approaches%2C%20with%20the%20added%20advantage%0Aof%20bypassing%20the%20overhead%20for%20estimating%20derived%20representations.%20This%20work%0Aestablishes%20a%20foundation%20for%20data-driven%2C%20geometry-aware%20dMRI%20registration%0Adirectly%20in%20the%20acquisition%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04794v2&entry.124074799=Read"},
{"title": "Neural Differential Appearance Equations", "author": "Chen Liu and Tobias Ritschel", "abstract": "  We propose a method to reproduce dynamic appearance textures with\nspace-stationary but time-varying visual statistics. While most previous work\ndecomposes dynamic textures into static appearance and motion, we focus on\ndynamic appearance that results not from motion but variations of fundamental\nproperties, such as rusting, decaying, melting, and weathering. To this end, we\nadopt the neural ordinary differential equation (ODE) to learn the underlying\ndynamics of appearance from a target exemplar. We simulate the ODE in two\nphases. At the \"warm-up\" phase, the ODE diffuses a random noise to an initial\nstate. We then constrain the further evolution of this ODE to replicate the\nevolution of visual feature statistics in the exemplar during the generation\nphase. The particular innovation of this work is the neural ODE achieving both\ndenoising and evolution for dynamics synthesis, with a proposed temporal\ntraining scheme. We study both relightable (BRDF) and non-relightable (RGB)\nappearance models. For both we introduce new pilot datasets, allowing, for the\nfirst time, to study such phenomena: For RGB we provide 22 dynamic textures\nacquired from free online sources; For BRDFs, we further acquire a dataset of\n21 flash-lit videos of time-varying materials, enabled by a simple-to-construct\nsetup. Our experiments show that our method consistently yields realistic and\ncoherent results, whereas prior works falter under pronounced temporal\nappearance variations. A user study confirms our approach is preferred to\nprevious work for such exemplars.\n", "link": "http://arxiv.org/abs/2410.07128v2", "date": "2025-01-10", "relevancy": 2.282, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.59}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5689}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Differential%20Appearance%20Equations&body=Title%3A%20Neural%20Differential%20Appearance%20Equations%0AAuthor%3A%20Chen%20Liu%20and%20Tobias%20Ritschel%0AAbstract%3A%20%20%20We%20propose%20a%20method%20to%20reproduce%20dynamic%20appearance%20textures%20with%0Aspace-stationary%20but%20time-varying%20visual%20statistics.%20While%20most%20previous%20work%0Adecomposes%20dynamic%20textures%20into%20static%20appearance%20and%20motion%2C%20we%20focus%20on%0Adynamic%20appearance%20that%20results%20not%20from%20motion%20but%20variations%20of%20fundamental%0Aproperties%2C%20such%20as%20rusting%2C%20decaying%2C%20melting%2C%20and%20weathering.%20To%20this%20end%2C%20we%0Aadopt%20the%20neural%20ordinary%20differential%20equation%20%28ODE%29%20to%20learn%20the%20underlying%0Adynamics%20of%20appearance%20from%20a%20target%20exemplar.%20We%20simulate%20the%20ODE%20in%20two%0Aphases.%20At%20the%20%22warm-up%22%20phase%2C%20the%20ODE%20diffuses%20a%20random%20noise%20to%20an%20initial%0Astate.%20We%20then%20constrain%20the%20further%20evolution%20of%20this%20ODE%20to%20replicate%20the%0Aevolution%20of%20visual%20feature%20statistics%20in%20the%20exemplar%20during%20the%20generation%0Aphase.%20The%20particular%20innovation%20of%20this%20work%20is%20the%20neural%20ODE%20achieving%20both%0Adenoising%20and%20evolution%20for%20dynamics%20synthesis%2C%20with%20a%20proposed%20temporal%0Atraining%20scheme.%20We%20study%20both%20relightable%20%28BRDF%29%20and%20non-relightable%20%28RGB%29%0Aappearance%20models.%20For%20both%20we%20introduce%20new%20pilot%20datasets%2C%20allowing%2C%20for%20the%0Afirst%20time%2C%20to%20study%20such%20phenomena%3A%20For%20RGB%20we%20provide%2022%20dynamic%20textures%0Aacquired%20from%20free%20online%20sources%3B%20For%20BRDFs%2C%20we%20further%20acquire%20a%20dataset%20of%0A21%20flash-lit%20videos%20of%20time-varying%20materials%2C%20enabled%20by%20a%20simple-to-construct%0Asetup.%20Our%20experiments%20show%20that%20our%20method%20consistently%20yields%20realistic%20and%0Acoherent%20results%2C%20whereas%20prior%20works%20falter%20under%20pronounced%20temporal%0Aappearance%20variations.%20A%20user%20study%20confirms%20our%20approach%20is%20preferred%20to%0Aprevious%20work%20for%20such%20exemplars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07128v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Differential%2520Appearance%2520Equations%26entry.906535625%3DChen%2520Liu%2520and%2520Tobias%2520Ritschel%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520method%2520to%2520reproduce%2520dynamic%2520appearance%2520textures%2520with%250Aspace-stationary%2520but%2520time-varying%2520visual%2520statistics.%2520While%2520most%2520previous%2520work%250Adecomposes%2520dynamic%2520textures%2520into%2520static%2520appearance%2520and%2520motion%252C%2520we%2520focus%2520on%250Adynamic%2520appearance%2520that%2520results%2520not%2520from%2520motion%2520but%2520variations%2520of%2520fundamental%250Aproperties%252C%2520such%2520as%2520rusting%252C%2520decaying%252C%2520melting%252C%2520and%2520weathering.%2520To%2520this%2520end%252C%2520we%250Aadopt%2520the%2520neural%2520ordinary%2520differential%2520equation%2520%2528ODE%2529%2520to%2520learn%2520the%2520underlying%250Adynamics%2520of%2520appearance%2520from%2520a%2520target%2520exemplar.%2520We%2520simulate%2520the%2520ODE%2520in%2520two%250Aphases.%2520At%2520the%2520%2522warm-up%2522%2520phase%252C%2520the%2520ODE%2520diffuses%2520a%2520random%2520noise%2520to%2520an%2520initial%250Astate.%2520We%2520then%2520constrain%2520the%2520further%2520evolution%2520of%2520this%2520ODE%2520to%2520replicate%2520the%250Aevolution%2520of%2520visual%2520feature%2520statistics%2520in%2520the%2520exemplar%2520during%2520the%2520generation%250Aphase.%2520The%2520particular%2520innovation%2520of%2520this%2520work%2520is%2520the%2520neural%2520ODE%2520achieving%2520both%250Adenoising%2520and%2520evolution%2520for%2520dynamics%2520synthesis%252C%2520with%2520a%2520proposed%2520temporal%250Atraining%2520scheme.%2520We%2520study%2520both%2520relightable%2520%2528BRDF%2529%2520and%2520non-relightable%2520%2528RGB%2529%250Aappearance%2520models.%2520For%2520both%2520we%2520introduce%2520new%2520pilot%2520datasets%252C%2520allowing%252C%2520for%2520the%250Afirst%2520time%252C%2520to%2520study%2520such%2520phenomena%253A%2520For%2520RGB%2520we%2520provide%252022%2520dynamic%2520textures%250Aacquired%2520from%2520free%2520online%2520sources%253B%2520For%2520BRDFs%252C%2520we%2520further%2520acquire%2520a%2520dataset%2520of%250A21%2520flash-lit%2520videos%2520of%2520time-varying%2520materials%252C%2520enabled%2520by%2520a%2520simple-to-construct%250Asetup.%2520Our%2520experiments%2520show%2520that%2520our%2520method%2520consistently%2520yields%2520realistic%2520and%250Acoherent%2520results%252C%2520whereas%2520prior%2520works%2520falter%2520under%2520pronounced%2520temporal%250Aappearance%2520variations.%2520A%2520user%2520study%2520confirms%2520our%2520approach%2520is%2520preferred%2520to%250Aprevious%2520work%2520for%2520such%2520exemplars.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07128v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Differential%20Appearance%20Equations&entry.906535625=Chen%20Liu%20and%20Tobias%20Ritschel&entry.1292438233=%20%20We%20propose%20a%20method%20to%20reproduce%20dynamic%20appearance%20textures%20with%0Aspace-stationary%20but%20time-varying%20visual%20statistics.%20While%20most%20previous%20work%0Adecomposes%20dynamic%20textures%20into%20static%20appearance%20and%20motion%2C%20we%20focus%20on%0Adynamic%20appearance%20that%20results%20not%20from%20motion%20but%20variations%20of%20fundamental%0Aproperties%2C%20such%20as%20rusting%2C%20decaying%2C%20melting%2C%20and%20weathering.%20To%20this%20end%2C%20we%0Aadopt%20the%20neural%20ordinary%20differential%20equation%20%28ODE%29%20to%20learn%20the%20underlying%0Adynamics%20of%20appearance%20from%20a%20target%20exemplar.%20We%20simulate%20the%20ODE%20in%20two%0Aphases.%20At%20the%20%22warm-up%22%20phase%2C%20the%20ODE%20diffuses%20a%20random%20noise%20to%20an%20initial%0Astate.%20We%20then%20constrain%20the%20further%20evolution%20of%20this%20ODE%20to%20replicate%20the%0Aevolution%20of%20visual%20feature%20statistics%20in%20the%20exemplar%20during%20the%20generation%0Aphase.%20The%20particular%20innovation%20of%20this%20work%20is%20the%20neural%20ODE%20achieving%20both%0Adenoising%20and%20evolution%20for%20dynamics%20synthesis%2C%20with%20a%20proposed%20temporal%0Atraining%20scheme.%20We%20study%20both%20relightable%20%28BRDF%29%20and%20non-relightable%20%28RGB%29%0Aappearance%20models.%20For%20both%20we%20introduce%20new%20pilot%20datasets%2C%20allowing%2C%20for%20the%0Afirst%20time%2C%20to%20study%20such%20phenomena%3A%20For%20RGB%20we%20provide%2022%20dynamic%20textures%0Aacquired%20from%20free%20online%20sources%3B%20For%20BRDFs%2C%20we%20further%20acquire%20a%20dataset%20of%0A21%20flash-lit%20videos%20of%20time-varying%20materials%2C%20enabled%20by%20a%20simple-to-construct%0Asetup.%20Our%20experiments%20show%20that%20our%20method%20consistently%20yields%20realistic%20and%0Acoherent%20results%2C%20whereas%20prior%20works%20falter%20under%20pronounced%20temporal%0Aappearance%20variations.%20A%20user%20study%20confirms%20our%20approach%20is%20preferred%20to%0Aprevious%20work%20for%20such%20exemplars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07128v2&entry.124074799=Read"},
{"title": "Minimizing Occlusion Effect on Multi-View Camera Perception in BEV with\n  Multi-Sensor Fusion", "author": "Sanjay Kumar and Hiep Truong and Sushil Sharma and Ganesh Sistu and Tony Scanlan and Eoin Grua and Ciar\u00e1n Eising", "abstract": "  Autonomous driving technology is rapidly evolving, offering the potential for\nsafer and more efficient transportation. However, the performance of these\nsystems can be significantly compromised by the occlusion on sensors due to\nenvironmental factors like dirt, dust, rain, and fog. These occlusions severely\naffect vision-based tasks such as object detection, vehicle segmentation, and\nlane recognition. In this paper, we investigate the impact of various kinds of\nocclusions on camera sensor by projecting their effects from multi-view camera\nimages of the nuScenes dataset into the Bird's-Eye View (BEV) domain. This\napproach allows us to analyze how occlusions spatially distribute and influence\nvehicle segmentation accuracy within the BEV domain. Despite significant\nadvances in sensor technology and multi-sensor fusion, a gap remains in the\nexisting literature regarding the specific effects of camera occlusions on\nBEV-based perception systems. To address this gap, we use a multi-sensor fusion\ntechnique that integrates LiDAR and radar sensor data to mitigate the\nperformance degradation caused by occluded cameras. Our findings demonstrate\nthat this approach significantly enhances the accuracy and robustness of\nvehicle segmentation tasks, leading to more reliable autonomous driving\nsystems.\n", "link": "http://arxiv.org/abs/2501.05997v1", "date": "2025-01-10", "relevancy": 2.2607, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.59}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimizing%20Occlusion%20Effect%20on%20Multi-View%20Camera%20Perception%20in%20BEV%20with%0A%20%20Multi-Sensor%20Fusion&body=Title%3A%20Minimizing%20Occlusion%20Effect%20on%20Multi-View%20Camera%20Perception%20in%20BEV%20with%0A%20%20Multi-Sensor%20Fusion%0AAuthor%3A%20Sanjay%20Kumar%20and%20Hiep%20Truong%20and%20Sushil%20Sharma%20and%20Ganesh%20Sistu%20and%20Tony%20Scanlan%20and%20Eoin%20Grua%20and%20Ciar%C3%A1n%20Eising%0AAbstract%3A%20%20%20Autonomous%20driving%20technology%20is%20rapidly%20evolving%2C%20offering%20the%20potential%20for%0Asafer%20and%20more%20efficient%20transportation.%20However%2C%20the%20performance%20of%20these%0Asystems%20can%20be%20significantly%20compromised%20by%20the%20occlusion%20on%20sensors%20due%20to%0Aenvironmental%20factors%20like%20dirt%2C%20dust%2C%20rain%2C%20and%20fog.%20These%20occlusions%20severely%0Aaffect%20vision-based%20tasks%20such%20as%20object%20detection%2C%20vehicle%20segmentation%2C%20and%0Alane%20recognition.%20In%20this%20paper%2C%20we%20investigate%20the%20impact%20of%20various%20kinds%20of%0Aocclusions%20on%20camera%20sensor%20by%20projecting%20their%20effects%20from%20multi-view%20camera%0Aimages%20of%20the%20nuScenes%20dataset%20into%20the%20Bird%27s-Eye%20View%20%28BEV%29%20domain.%20This%0Aapproach%20allows%20us%20to%20analyze%20how%20occlusions%20spatially%20distribute%20and%20influence%0Avehicle%20segmentation%20accuracy%20within%20the%20BEV%20domain.%20Despite%20significant%0Aadvances%20in%20sensor%20technology%20and%20multi-sensor%20fusion%2C%20a%20gap%20remains%20in%20the%0Aexisting%20literature%20regarding%20the%20specific%20effects%20of%20camera%20occlusions%20on%0ABEV-based%20perception%20systems.%20To%20address%20this%20gap%2C%20we%20use%20a%20multi-sensor%20fusion%0Atechnique%20that%20integrates%20LiDAR%20and%20radar%20sensor%20data%20to%20mitigate%20the%0Aperformance%20degradation%20caused%20by%20occluded%20cameras.%20Our%20findings%20demonstrate%0Athat%20this%20approach%20significantly%20enhances%20the%20accuracy%20and%20robustness%20of%0Avehicle%20segmentation%20tasks%2C%20leading%20to%20more%20reliable%20autonomous%20driving%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimizing%2520Occlusion%2520Effect%2520on%2520Multi-View%2520Camera%2520Perception%2520in%2520BEV%2520with%250A%2520%2520Multi-Sensor%2520Fusion%26entry.906535625%3DSanjay%2520Kumar%2520and%2520Hiep%2520Truong%2520and%2520Sushil%2520Sharma%2520and%2520Ganesh%2520Sistu%2520and%2520Tony%2520Scanlan%2520and%2520Eoin%2520Grua%2520and%2520Ciar%25C3%25A1n%2520Eising%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520technology%2520is%2520rapidly%2520evolving%252C%2520offering%2520the%2520potential%2520for%250Asafer%2520and%2520more%2520efficient%2520transportation.%2520However%252C%2520the%2520performance%2520of%2520these%250Asystems%2520can%2520be%2520significantly%2520compromised%2520by%2520the%2520occlusion%2520on%2520sensors%2520due%2520to%250Aenvironmental%2520factors%2520like%2520dirt%252C%2520dust%252C%2520rain%252C%2520and%2520fog.%2520These%2520occlusions%2520severely%250Aaffect%2520vision-based%2520tasks%2520such%2520as%2520object%2520detection%252C%2520vehicle%2520segmentation%252C%2520and%250Alane%2520recognition.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520impact%2520of%2520various%2520kinds%2520of%250Aocclusions%2520on%2520camera%2520sensor%2520by%2520projecting%2520their%2520effects%2520from%2520multi-view%2520camera%250Aimages%2520of%2520the%2520nuScenes%2520dataset%2520into%2520the%2520Bird%2527s-Eye%2520View%2520%2528BEV%2529%2520domain.%2520This%250Aapproach%2520allows%2520us%2520to%2520analyze%2520how%2520occlusions%2520spatially%2520distribute%2520and%2520influence%250Avehicle%2520segmentation%2520accuracy%2520within%2520the%2520BEV%2520domain.%2520Despite%2520significant%250Aadvances%2520in%2520sensor%2520technology%2520and%2520multi-sensor%2520fusion%252C%2520a%2520gap%2520remains%2520in%2520the%250Aexisting%2520literature%2520regarding%2520the%2520specific%2520effects%2520of%2520camera%2520occlusions%2520on%250ABEV-based%2520perception%2520systems.%2520To%2520address%2520this%2520gap%252C%2520we%2520use%2520a%2520multi-sensor%2520fusion%250Atechnique%2520that%2520integrates%2520LiDAR%2520and%2520radar%2520sensor%2520data%2520to%2520mitigate%2520the%250Aperformance%2520degradation%2520caused%2520by%2520occluded%2520cameras.%2520Our%2520findings%2520demonstrate%250Athat%2520this%2520approach%2520significantly%2520enhances%2520the%2520accuracy%2520and%2520robustness%2520of%250Avehicle%2520segmentation%2520tasks%252C%2520leading%2520to%2520more%2520reliable%2520autonomous%2520driving%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimizing%20Occlusion%20Effect%20on%20Multi-View%20Camera%20Perception%20in%20BEV%20with%0A%20%20Multi-Sensor%20Fusion&entry.906535625=Sanjay%20Kumar%20and%20Hiep%20Truong%20and%20Sushil%20Sharma%20and%20Ganesh%20Sistu%20and%20Tony%20Scanlan%20and%20Eoin%20Grua%20and%20Ciar%C3%A1n%20Eising&entry.1292438233=%20%20Autonomous%20driving%20technology%20is%20rapidly%20evolving%2C%20offering%20the%20potential%20for%0Asafer%20and%20more%20efficient%20transportation.%20However%2C%20the%20performance%20of%20these%0Asystems%20can%20be%20significantly%20compromised%20by%20the%20occlusion%20on%20sensors%20due%20to%0Aenvironmental%20factors%20like%20dirt%2C%20dust%2C%20rain%2C%20and%20fog.%20These%20occlusions%20severely%0Aaffect%20vision-based%20tasks%20such%20as%20object%20detection%2C%20vehicle%20segmentation%2C%20and%0Alane%20recognition.%20In%20this%20paper%2C%20we%20investigate%20the%20impact%20of%20various%20kinds%20of%0Aocclusions%20on%20camera%20sensor%20by%20projecting%20their%20effects%20from%20multi-view%20camera%0Aimages%20of%20the%20nuScenes%20dataset%20into%20the%20Bird%27s-Eye%20View%20%28BEV%29%20domain.%20This%0Aapproach%20allows%20us%20to%20analyze%20how%20occlusions%20spatially%20distribute%20and%20influence%0Avehicle%20segmentation%20accuracy%20within%20the%20BEV%20domain.%20Despite%20significant%0Aadvances%20in%20sensor%20technology%20and%20multi-sensor%20fusion%2C%20a%20gap%20remains%20in%20the%0Aexisting%20literature%20regarding%20the%20specific%20effects%20of%20camera%20occlusions%20on%0ABEV-based%20perception%20systems.%20To%20address%20this%20gap%2C%20we%20use%20a%20multi-sensor%20fusion%0Atechnique%20that%20integrates%20LiDAR%20and%20radar%20sensor%20data%20to%20mitigate%20the%0Aperformance%20degradation%20caused%20by%20occluded%20cameras.%20Our%20findings%20demonstrate%0Athat%20this%20approach%20significantly%20enhances%20the%20accuracy%20and%20robustness%20of%0Avehicle%20segmentation%20tasks%2C%20leading%20to%20more%20reliable%20autonomous%20driving%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05997v1&entry.124074799=Read"},
{"title": "EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster\n  Context Attention, Better Feature Fusion, and Hardware Acceleration", "author": "Zhifan Song and Yuan Zhang and Abd Al Rahman M. Abu Ebayyeh", "abstract": "  Detecting small targets in drone imagery is challenging due to low\nresolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel\nedge-target detection framework built on an enhanced YOLOv10 architecture,\noptimized for real-time applications without post-processing. EDNet\nincorporates an XSmall detection head and a Cross Concat strategy to improve\nfeature fusion and multi-scale context awareness for detecting tiny targets in\ndiverse environments. Our unique C2f-FCA block employs Faster Context Attention\nto enhance feature extraction while reducing computational complexity. The WIoU\nloss function is employed for improved bounding box regression. With seven\nmodel sizes ranging from Tiny to XL, EDNet accommodates various deployment\nenvironments, enabling local real-time inference and ensuring data privacy.\nNotably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer\nparameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16\nto 55 FPS, providing a scalable and efficient solution for edge-based object\ndetection in challenging drone imagery. The source code and pre-trained models\nare available at: https://github.com/zsniko/EDNet.\n", "link": "http://arxiv.org/abs/2501.05885v1", "date": "2025-01-10", "relevancy": 2.2361, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5873}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5767}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EDNet%3A%20Edge-Optimized%20Small%20Target%20Detection%20in%20UAV%20Imagery%20--%20Faster%0A%20%20Context%20Attention%2C%20Better%20Feature%20Fusion%2C%20and%20Hardware%20Acceleration&body=Title%3A%20EDNet%3A%20Edge-Optimized%20Small%20Target%20Detection%20in%20UAV%20Imagery%20--%20Faster%0A%20%20Context%20Attention%2C%20Better%20Feature%20Fusion%2C%20and%20Hardware%20Acceleration%0AAuthor%3A%20Zhifan%20Song%20and%20Yuan%20Zhang%20and%20Abd%20Al%20Rahman%20M.%20Abu%20Ebayyeh%0AAbstract%3A%20%20%20Detecting%20small%20targets%20in%20drone%20imagery%20is%20challenging%20due%20to%20low%0Aresolution%2C%20complex%20backgrounds%2C%20and%20dynamic%20scenes.%20We%20propose%20EDNet%2C%20a%20novel%0Aedge-target%20detection%20framework%20built%20on%20an%20enhanced%20YOLOv10%20architecture%2C%0Aoptimized%20for%20real-time%20applications%20without%20post-processing.%20EDNet%0Aincorporates%20an%20XSmall%20detection%20head%20and%20a%20Cross%20Concat%20strategy%20to%20improve%0Afeature%20fusion%20and%20multi-scale%20context%20awareness%20for%20detecting%20tiny%20targets%20in%0Adiverse%20environments.%20Our%20unique%20C2f-FCA%20block%20employs%20Faster%20Context%20Attention%0Ato%20enhance%20feature%20extraction%20while%20reducing%20computational%20complexity.%20The%20WIoU%0Aloss%20function%20is%20employed%20for%20improved%20bounding%20box%20regression.%20With%20seven%0Amodel%20sizes%20ranging%20from%20Tiny%20to%20XL%2C%20EDNet%20accommodates%20various%20deployment%0Aenvironments%2C%20enabling%20local%20real-time%20inference%20and%20ensuring%20data%20privacy.%0ANotably%2C%20EDNet%20achieves%20up%20to%20a%205.6%25%20gain%20in%20mAP%4050%20with%20significantly%20fewer%0Aparameters.%20On%20an%20iPhone%2012%2C%20EDNet%20variants%20operate%20at%20speeds%20ranging%20from%2016%0Ato%2055%20FPS%2C%20providing%20a%20scalable%20and%20efficient%20solution%20for%20edge-based%20object%0Adetection%20in%20challenging%20drone%20imagery.%20The%20source%20code%20and%20pre-trained%20models%0Aare%20available%20at%3A%20https%3A//github.com/zsniko/EDNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEDNet%253A%2520Edge-Optimized%2520Small%2520Target%2520Detection%2520in%2520UAV%2520Imagery%2520--%2520Faster%250A%2520%2520Context%2520Attention%252C%2520Better%2520Feature%2520Fusion%252C%2520and%2520Hardware%2520Acceleration%26entry.906535625%3DZhifan%2520Song%2520and%2520Yuan%2520Zhang%2520and%2520Abd%2520Al%2520Rahman%2520M.%2520Abu%2520Ebayyeh%26entry.1292438233%3D%2520%2520Detecting%2520small%2520targets%2520in%2520drone%2520imagery%2520is%2520challenging%2520due%2520to%2520low%250Aresolution%252C%2520complex%2520backgrounds%252C%2520and%2520dynamic%2520scenes.%2520We%2520propose%2520EDNet%252C%2520a%2520novel%250Aedge-target%2520detection%2520framework%2520built%2520on%2520an%2520enhanced%2520YOLOv10%2520architecture%252C%250Aoptimized%2520for%2520real-time%2520applications%2520without%2520post-processing.%2520EDNet%250Aincorporates%2520an%2520XSmall%2520detection%2520head%2520and%2520a%2520Cross%2520Concat%2520strategy%2520to%2520improve%250Afeature%2520fusion%2520and%2520multi-scale%2520context%2520awareness%2520for%2520detecting%2520tiny%2520targets%2520in%250Adiverse%2520environments.%2520Our%2520unique%2520C2f-FCA%2520block%2520employs%2520Faster%2520Context%2520Attention%250Ato%2520enhance%2520feature%2520extraction%2520while%2520reducing%2520computational%2520complexity.%2520The%2520WIoU%250Aloss%2520function%2520is%2520employed%2520for%2520improved%2520bounding%2520box%2520regression.%2520With%2520seven%250Amodel%2520sizes%2520ranging%2520from%2520Tiny%2520to%2520XL%252C%2520EDNet%2520accommodates%2520various%2520deployment%250Aenvironments%252C%2520enabling%2520local%2520real-time%2520inference%2520and%2520ensuring%2520data%2520privacy.%250ANotably%252C%2520EDNet%2520achieves%2520up%2520to%2520a%25205.6%2525%2520gain%2520in%2520mAP%254050%2520with%2520significantly%2520fewer%250Aparameters.%2520On%2520an%2520iPhone%252012%252C%2520EDNet%2520variants%2520operate%2520at%2520speeds%2520ranging%2520from%252016%250Ato%252055%2520FPS%252C%2520providing%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%2520edge-based%2520object%250Adetection%2520in%2520challenging%2520drone%2520imagery.%2520The%2520source%2520code%2520and%2520pre-trained%2520models%250Aare%2520available%2520at%253A%2520https%253A//github.com/zsniko/EDNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EDNet%3A%20Edge-Optimized%20Small%20Target%20Detection%20in%20UAV%20Imagery%20--%20Faster%0A%20%20Context%20Attention%2C%20Better%20Feature%20Fusion%2C%20and%20Hardware%20Acceleration&entry.906535625=Zhifan%20Song%20and%20Yuan%20Zhang%20and%20Abd%20Al%20Rahman%20M.%20Abu%20Ebayyeh&entry.1292438233=%20%20Detecting%20small%20targets%20in%20drone%20imagery%20is%20challenging%20due%20to%20low%0Aresolution%2C%20complex%20backgrounds%2C%20and%20dynamic%20scenes.%20We%20propose%20EDNet%2C%20a%20novel%0Aedge-target%20detection%20framework%20built%20on%20an%20enhanced%20YOLOv10%20architecture%2C%0Aoptimized%20for%20real-time%20applications%20without%20post-processing.%20EDNet%0Aincorporates%20an%20XSmall%20detection%20head%20and%20a%20Cross%20Concat%20strategy%20to%20improve%0Afeature%20fusion%20and%20multi-scale%20context%20awareness%20for%20detecting%20tiny%20targets%20in%0Adiverse%20environments.%20Our%20unique%20C2f-FCA%20block%20employs%20Faster%20Context%20Attention%0Ato%20enhance%20feature%20extraction%20while%20reducing%20computational%20complexity.%20The%20WIoU%0Aloss%20function%20is%20employed%20for%20improved%20bounding%20box%20regression.%20With%20seven%0Amodel%20sizes%20ranging%20from%20Tiny%20to%20XL%2C%20EDNet%20accommodates%20various%20deployment%0Aenvironments%2C%20enabling%20local%20real-time%20inference%20and%20ensuring%20data%20privacy.%0ANotably%2C%20EDNet%20achieves%20up%20to%20a%205.6%25%20gain%20in%20mAP%4050%20with%20significantly%20fewer%0Aparameters.%20On%20an%20iPhone%2012%2C%20EDNet%20variants%20operate%20at%20speeds%20ranging%20from%2016%0Ato%2055%20FPS%2C%20providing%20a%20scalable%20and%20efficient%20solution%20for%20edge-based%20object%0Adetection%20in%20challenging%20drone%20imagery.%20The%20source%20code%20and%20pre-trained%20models%0Aare%20available%20at%3A%20https%3A//github.com/zsniko/EDNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05885v1&entry.124074799=Read"},
{"title": "A Holistically Point-guided Text Framework for Weakly-Supervised\n  Camouflaged Object Detection", "author": "Tsui Qin Mok and Shuyong Gao and Haozhe Xing and Miaoyang He and Yan Wang and Wenqiang Zhang", "abstract": "  Weakly-Supervised Camouflaged Object Detection (WSCOD) has gained popularity\nfor its promise to train models with weak labels to segment objects that\nvisually blend into their surroundings. Recently, some methods using\nsparsely-annotated supervision shown promising results through scribbling in\nWSCOD, while point-text supervision remains underexplored. Hence, this paper\nintroduces a novel holistically point-guided text framework for WSCOD by\ndecomposing into three phases: segment, choose, train. Specifically, we propose\nPoint-guided Candidate Generation (PCG), where the point's foreground serves as\na correction for the text path to explicitly correct and rejuvenate the loss\ndetection object during the mask generation process (SEGMENT). We also\nintroduce a Qualified Candidate Discriminator (QCD) to choose the optimal mask\nfrom a given text prompt using CLIP (CHOOSE), and employ the chosen pseudo mask\nfor training with a self-supervised Vision Transformer (TRAIN). Additionally,\nwe developed a new point-supervised dataset (P2C-COD) and a text-supervised\ndataset (T-COD). Comprehensive experiments on four benchmark datasets\ndemonstrate our method outperforms state-of-the-art methods by a large margin,\nand also outperforms some existing fully-supervised camouflaged object\ndetection methods.\n", "link": "http://arxiv.org/abs/2501.06038v1", "date": "2025-01-10", "relevancy": 2.2348, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5672}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5576}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Holistically%20Point-guided%20Text%20Framework%20for%20Weakly-Supervised%0A%20%20Camouflaged%20Object%20Detection&body=Title%3A%20A%20Holistically%20Point-guided%20Text%20Framework%20for%20Weakly-Supervised%0A%20%20Camouflaged%20Object%20Detection%0AAuthor%3A%20Tsui%20Qin%20Mok%20and%20Shuyong%20Gao%20and%20Haozhe%20Xing%20and%20Miaoyang%20He%20and%20Yan%20Wang%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20Weakly-Supervised%20Camouflaged%20Object%20Detection%20%28WSCOD%29%20has%20gained%20popularity%0Afor%20its%20promise%20to%20train%20models%20with%20weak%20labels%20to%20segment%20objects%20that%0Avisually%20blend%20into%20their%20surroundings.%20Recently%2C%20some%20methods%20using%0Asparsely-annotated%20supervision%20shown%20promising%20results%20through%20scribbling%20in%0AWSCOD%2C%20while%20point-text%20supervision%20remains%20underexplored.%20Hence%2C%20this%20paper%0Aintroduces%20a%20novel%20holistically%20point-guided%20text%20framework%20for%20WSCOD%20by%0Adecomposing%20into%20three%20phases%3A%20segment%2C%20choose%2C%20train.%20Specifically%2C%20we%20propose%0APoint-guided%20Candidate%20Generation%20%28PCG%29%2C%20where%20the%20point%27s%20foreground%20serves%20as%0Aa%20correction%20for%20the%20text%20path%20to%20explicitly%20correct%20and%20rejuvenate%20the%20loss%0Adetection%20object%20during%20the%20mask%20generation%20process%20%28SEGMENT%29.%20We%20also%0Aintroduce%20a%20Qualified%20Candidate%20Discriminator%20%28QCD%29%20to%20choose%20the%20optimal%20mask%0Afrom%20a%20given%20text%20prompt%20using%20CLIP%20%28CHOOSE%29%2C%20and%20employ%20the%20chosen%20pseudo%20mask%0Afor%20training%20with%20a%20self-supervised%20Vision%20Transformer%20%28TRAIN%29.%20Additionally%2C%0Awe%20developed%20a%20new%20point-supervised%20dataset%20%28P2C-COD%29%20and%20a%20text-supervised%0Adataset%20%28T-COD%29.%20Comprehensive%20experiments%20on%20four%20benchmark%20datasets%0Ademonstrate%20our%20method%20outperforms%20state-of-the-art%20methods%20by%20a%20large%20margin%2C%0Aand%20also%20outperforms%20some%20existing%20fully-supervised%20camouflaged%20object%0Adetection%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Holistically%2520Point-guided%2520Text%2520Framework%2520for%2520Weakly-Supervised%250A%2520%2520Camouflaged%2520Object%2520Detection%26entry.906535625%3DTsui%2520Qin%2520Mok%2520and%2520Shuyong%2520Gao%2520and%2520Haozhe%2520Xing%2520and%2520Miaoyang%2520He%2520and%2520Yan%2520Wang%2520and%2520Wenqiang%2520Zhang%26entry.1292438233%3D%2520%2520Weakly-Supervised%2520Camouflaged%2520Object%2520Detection%2520%2528WSCOD%2529%2520has%2520gained%2520popularity%250Afor%2520its%2520promise%2520to%2520train%2520models%2520with%2520weak%2520labels%2520to%2520segment%2520objects%2520that%250Avisually%2520blend%2520into%2520their%2520surroundings.%2520Recently%252C%2520some%2520methods%2520using%250Asparsely-annotated%2520supervision%2520shown%2520promising%2520results%2520through%2520scribbling%2520in%250AWSCOD%252C%2520while%2520point-text%2520supervision%2520remains%2520underexplored.%2520Hence%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520holistically%2520point-guided%2520text%2520framework%2520for%2520WSCOD%2520by%250Adecomposing%2520into%2520three%2520phases%253A%2520segment%252C%2520choose%252C%2520train.%2520Specifically%252C%2520we%2520propose%250APoint-guided%2520Candidate%2520Generation%2520%2528PCG%2529%252C%2520where%2520the%2520point%2527s%2520foreground%2520serves%2520as%250Aa%2520correction%2520for%2520the%2520text%2520path%2520to%2520explicitly%2520correct%2520and%2520rejuvenate%2520the%2520loss%250Adetection%2520object%2520during%2520the%2520mask%2520generation%2520process%2520%2528SEGMENT%2529.%2520We%2520also%250Aintroduce%2520a%2520Qualified%2520Candidate%2520Discriminator%2520%2528QCD%2529%2520to%2520choose%2520the%2520optimal%2520mask%250Afrom%2520a%2520given%2520text%2520prompt%2520using%2520CLIP%2520%2528CHOOSE%2529%252C%2520and%2520employ%2520the%2520chosen%2520pseudo%2520mask%250Afor%2520training%2520with%2520a%2520self-supervised%2520Vision%2520Transformer%2520%2528TRAIN%2529.%2520Additionally%252C%250Awe%2520developed%2520a%2520new%2520point-supervised%2520dataset%2520%2528P2C-COD%2529%2520and%2520a%2520text-supervised%250Adataset%2520%2528T-COD%2529.%2520Comprehensive%2520experiments%2520on%2520four%2520benchmark%2520datasets%250Ademonstrate%2520our%2520method%2520outperforms%2520state-of-the-art%2520methods%2520by%2520a%2520large%2520margin%252C%250Aand%2520also%2520outperforms%2520some%2520existing%2520fully-supervised%2520camouflaged%2520object%250Adetection%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Holistically%20Point-guided%20Text%20Framework%20for%20Weakly-Supervised%0A%20%20Camouflaged%20Object%20Detection&entry.906535625=Tsui%20Qin%20Mok%20and%20Shuyong%20Gao%20and%20Haozhe%20Xing%20and%20Miaoyang%20He%20and%20Yan%20Wang%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20Weakly-Supervised%20Camouflaged%20Object%20Detection%20%28WSCOD%29%20has%20gained%20popularity%0Afor%20its%20promise%20to%20train%20models%20with%20weak%20labels%20to%20segment%20objects%20that%0Avisually%20blend%20into%20their%20surroundings.%20Recently%2C%20some%20methods%20using%0Asparsely-annotated%20supervision%20shown%20promising%20results%20through%20scribbling%20in%0AWSCOD%2C%20while%20point-text%20supervision%20remains%20underexplored.%20Hence%2C%20this%20paper%0Aintroduces%20a%20novel%20holistically%20point-guided%20text%20framework%20for%20WSCOD%20by%0Adecomposing%20into%20three%20phases%3A%20segment%2C%20choose%2C%20train.%20Specifically%2C%20we%20propose%0APoint-guided%20Candidate%20Generation%20%28PCG%29%2C%20where%20the%20point%27s%20foreground%20serves%20as%0Aa%20correction%20for%20the%20text%20path%20to%20explicitly%20correct%20and%20rejuvenate%20the%20loss%0Adetection%20object%20during%20the%20mask%20generation%20process%20%28SEGMENT%29.%20We%20also%0Aintroduce%20a%20Qualified%20Candidate%20Discriminator%20%28QCD%29%20to%20choose%20the%20optimal%20mask%0Afrom%20a%20given%20text%20prompt%20using%20CLIP%20%28CHOOSE%29%2C%20and%20employ%20the%20chosen%20pseudo%20mask%0Afor%20training%20with%20a%20self-supervised%20Vision%20Transformer%20%28TRAIN%29.%20Additionally%2C%0Awe%20developed%20a%20new%20point-supervised%20dataset%20%28P2C-COD%29%20and%20a%20text-supervised%0Adataset%20%28T-COD%29.%20Comprehensive%20experiments%20on%20four%20benchmark%20datasets%0Ademonstrate%20our%20method%20outperforms%20state-of-the-art%20methods%20by%20a%20large%20margin%2C%0Aand%20also%20outperforms%20some%20existing%20fully-supervised%20camouflaged%20object%0Adetection%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06038v1&entry.124074799=Read"},
{"title": "Solving nonograms using Neural Networks", "author": "Jos\u00e9 Mar\u00eda Buades Rubio and Antoni Jaume-i-Cap\u00f3 and David L\u00f3pez Gonz\u00e1lez and Gabriel Moy\u00e0 Alcover", "abstract": "  Nonograms are logic puzzles in which cells in a grid must be colored or left\nblank according to the numbers that are located in its headers. In this study,\nwe analyze different techniques to solve this type of logical problem using an\nHeuristic Algorithm, Genetic Algorithm, and Heuristic Algorithm with Neural\nNetwork. Furthermore, we generate a public dataset to train the neural\nnetworks. We published this dataset and the code of the algorithms. Combination\nof the heuristic algorithm with a neural network obtained the best results.\nFrom state of the art review, no previous works used neural network to solve\nnonograms, nor combined a network with other algorithms to accelerate the\nresolution process.\n", "link": "http://arxiv.org/abs/2501.05882v1", "date": "2025-01-10", "relevancy": 2.2254, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4449}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20nonograms%20using%20Neural%20Networks&body=Title%3A%20Solving%20nonograms%20using%20Neural%20Networks%0AAuthor%3A%20Jos%C3%A9%20Mar%C3%ADa%20Buades%20Rubio%20and%20Antoni%20Jaume-i-Cap%C3%B3%20and%20David%20L%C3%B3pez%20Gonz%C3%A1lez%20and%20Gabriel%20Moy%C3%A0%20Alcover%0AAbstract%3A%20%20%20Nonograms%20are%20logic%20puzzles%20in%20which%20cells%20in%20a%20grid%20must%20be%20colored%20or%20left%0Ablank%20according%20to%20the%20numbers%20that%20are%20located%20in%20its%20headers.%20In%20this%20study%2C%0Awe%20analyze%20different%20techniques%20to%20solve%20this%20type%20of%20logical%20problem%20using%20an%0AHeuristic%20Algorithm%2C%20Genetic%20Algorithm%2C%20and%20Heuristic%20Algorithm%20with%20Neural%0ANetwork.%20Furthermore%2C%20we%20generate%20a%20public%20dataset%20to%20train%20the%20neural%0Anetworks.%20We%20published%20this%20dataset%20and%20the%20code%20of%20the%20algorithms.%20Combination%0Aof%20the%20heuristic%20algorithm%20with%20a%20neural%20network%20obtained%20the%20best%20results.%0AFrom%20state%20of%20the%20art%20review%2C%20no%20previous%20works%20used%20neural%20network%20to%20solve%0Anonograms%2C%20nor%20combined%20a%20network%20with%20other%20algorithms%20to%20accelerate%20the%0Aresolution%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520nonograms%2520using%2520Neural%2520Networks%26entry.906535625%3DJos%25C3%25A9%2520Mar%25C3%25ADa%2520Buades%2520Rubio%2520and%2520Antoni%2520Jaume-i-Cap%25C3%25B3%2520and%2520David%2520L%25C3%25B3pez%2520Gonz%25C3%25A1lez%2520and%2520Gabriel%2520Moy%25C3%25A0%2520Alcover%26entry.1292438233%3D%2520%2520Nonograms%2520are%2520logic%2520puzzles%2520in%2520which%2520cells%2520in%2520a%2520grid%2520must%2520be%2520colored%2520or%2520left%250Ablank%2520according%2520to%2520the%2520numbers%2520that%2520are%2520located%2520in%2520its%2520headers.%2520In%2520this%2520study%252C%250Awe%2520analyze%2520different%2520techniques%2520to%2520solve%2520this%2520type%2520of%2520logical%2520problem%2520using%2520an%250AHeuristic%2520Algorithm%252C%2520Genetic%2520Algorithm%252C%2520and%2520Heuristic%2520Algorithm%2520with%2520Neural%250ANetwork.%2520Furthermore%252C%2520we%2520generate%2520a%2520public%2520dataset%2520to%2520train%2520the%2520neural%250Anetworks.%2520We%2520published%2520this%2520dataset%2520and%2520the%2520code%2520of%2520the%2520algorithms.%2520Combination%250Aof%2520the%2520heuristic%2520algorithm%2520with%2520a%2520neural%2520network%2520obtained%2520the%2520best%2520results.%250AFrom%2520state%2520of%2520the%2520art%2520review%252C%2520no%2520previous%2520works%2520used%2520neural%2520network%2520to%2520solve%250Anonograms%252C%2520nor%2520combined%2520a%2520network%2520with%2520other%2520algorithms%2520to%2520accelerate%2520the%250Aresolution%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20nonograms%20using%20Neural%20Networks&entry.906535625=Jos%C3%A9%20Mar%C3%ADa%20Buades%20Rubio%20and%20Antoni%20Jaume-i-Cap%C3%B3%20and%20David%20L%C3%B3pez%20Gonz%C3%A1lez%20and%20Gabriel%20Moy%C3%A0%20Alcover&entry.1292438233=%20%20Nonograms%20are%20logic%20puzzles%20in%20which%20cells%20in%20a%20grid%20must%20be%20colored%20or%20left%0Ablank%20according%20to%20the%20numbers%20that%20are%20located%20in%20its%20headers.%20In%20this%20study%2C%0Awe%20analyze%20different%20techniques%20to%20solve%20this%20type%20of%20logical%20problem%20using%20an%0AHeuristic%20Algorithm%2C%20Genetic%20Algorithm%2C%20and%20Heuristic%20Algorithm%20with%20Neural%0ANetwork.%20Furthermore%2C%20we%20generate%20a%20public%20dataset%20to%20train%20the%20neural%0Anetworks.%20We%20published%20this%20dataset%20and%20the%20code%20of%20the%20algorithms.%20Combination%0Aof%20the%20heuristic%20algorithm%20with%20a%20neural%20network%20obtained%20the%20best%20results.%0AFrom%20state%20of%20the%20art%20review%2C%20no%20previous%20works%20used%20neural%20network%20to%20solve%0Anonograms%2C%20nor%20combined%20a%20network%20with%20other%20algorithms%20to%20accelerate%20the%0Aresolution%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05882v1&entry.124074799=Read"},
{"title": "Chimera: Improving Generalist Model with Domain-Specific Experts", "author": "Tianshuo Peng and Mingsheng Li and Hongbin Zhou and Renqiu Xia and Renrui Zhang and Lei Bai and Song Mao and Bin Wang and Conghui He and Aojun Zhou and Botian Shi and Tao Chen and Bo Zhang and Xiangyu Yue", "abstract": "  Recent advancements in Large Multi-modal Models (LMMs) underscore the\nimportance of scaling by increasing image-text paired data, achieving\nimpressive performance on general tasks. Despite their effectiveness in broad\napplications, generalist models are primarily trained on web-scale datasets\ndominated by natural images, resulting in the sacrifice of specialized\ncapabilities for domain-specific tasks that require extensive domain prior\nknowledge. Moreover, directly integrating expert models tailored for specific\ndomains is challenging due to the representational gap and imbalanced\noptimization between the generalist model and experts. To address these\nchallenges, we introduce Chimera, a scalable and low-cost multi-modal pipeline\ndesigned to boost the ability of existing LMMs with domain-specific experts.\nSpecifically, we design a progressive training strategy to integrate features\nfrom expert models into the input of a generalist LMM. To address the\nimbalanced optimization caused by the well-aligned general visual encoder, we\nintroduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism.\nThis results in a versatile model that excels across the chart, table, math,\nand document domains, achieving state-of-the-art performance on multi-modal\nreasoning and visual content extraction tasks, both of which are challenging\ntasks for assessing existing LMMs.\n", "link": "http://arxiv.org/abs/2412.05983v2", "date": "2025-01-10", "relevancy": 2.2078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chimera%3A%20Improving%20Generalist%20Model%20with%20Domain-Specific%20Experts&body=Title%3A%20Chimera%3A%20Improving%20Generalist%20Model%20with%20Domain-Specific%20Experts%0AAuthor%3A%20Tianshuo%20Peng%20and%20Mingsheng%20Li%20and%20Hongbin%20Zhou%20and%20Renqiu%20Xia%20and%20Renrui%20Zhang%20and%20Lei%20Bai%20and%20Song%20Mao%20and%20Bin%20Wang%20and%20Conghui%20He%20and%20Aojun%20Zhou%20and%20Botian%20Shi%20and%20Tao%20Chen%20and%20Bo%20Zhang%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Multi-modal%20Models%20%28LMMs%29%20underscore%20the%0Aimportance%20of%20scaling%20by%20increasing%20image-text%20paired%20data%2C%20achieving%0Aimpressive%20performance%20on%20general%20tasks.%20Despite%20their%20effectiveness%20in%20broad%0Aapplications%2C%20generalist%20models%20are%20primarily%20trained%20on%20web-scale%20datasets%0Adominated%20by%20natural%20images%2C%20resulting%20in%20the%20sacrifice%20of%20specialized%0Acapabilities%20for%20domain-specific%20tasks%20that%20require%20extensive%20domain%20prior%0Aknowledge.%20Moreover%2C%20directly%20integrating%20expert%20models%20tailored%20for%20specific%0Adomains%20is%20challenging%20due%20to%20the%20representational%20gap%20and%20imbalanced%0Aoptimization%20between%20the%20generalist%20model%20and%20experts.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Chimera%2C%20a%20scalable%20and%20low-cost%20multi-modal%20pipeline%0Adesigned%20to%20boost%20the%20ability%20of%20existing%20LMMs%20with%20domain-specific%20experts.%0ASpecifically%2C%20we%20design%20a%20progressive%20training%20strategy%20to%20integrate%20features%0Afrom%20expert%20models%20into%20the%20input%20of%20a%20generalist%20LMM.%20To%20address%20the%0Aimbalanced%20optimization%20caused%20by%20the%20well-aligned%20general%20visual%20encoder%2C%20we%0Aintroduce%20a%20novel%20Generalist-Specialist%20Collaboration%20Masking%20%28GSCM%29%20mechanism.%0AThis%20results%20in%20a%20versatile%20model%20that%20excels%20across%20the%20chart%2C%20table%2C%20math%2C%0Aand%20document%20domains%2C%20achieving%20state-of-the-art%20performance%20on%20multi-modal%0Areasoning%20and%20visual%20content%20extraction%20tasks%2C%20both%20of%20which%20are%20challenging%0Atasks%20for%20assessing%20existing%20LMMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChimera%253A%2520Improving%2520Generalist%2520Model%2520with%2520Domain-Specific%2520Experts%26entry.906535625%3DTianshuo%2520Peng%2520and%2520Mingsheng%2520Li%2520and%2520Hongbin%2520Zhou%2520and%2520Renqiu%2520Xia%2520and%2520Renrui%2520Zhang%2520and%2520Lei%2520Bai%2520and%2520Song%2520Mao%2520and%2520Bin%2520Wang%2520and%2520Conghui%2520He%2520and%2520Aojun%2520Zhou%2520and%2520Botian%2520Shi%2520and%2520Tao%2520Chen%2520and%2520Bo%2520Zhang%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Multi-modal%2520Models%2520%2528LMMs%2529%2520underscore%2520the%250Aimportance%2520of%2520scaling%2520by%2520increasing%2520image-text%2520paired%2520data%252C%2520achieving%250Aimpressive%2520performance%2520on%2520general%2520tasks.%2520Despite%2520their%2520effectiveness%2520in%2520broad%250Aapplications%252C%2520generalist%2520models%2520are%2520primarily%2520trained%2520on%2520web-scale%2520datasets%250Adominated%2520by%2520natural%2520images%252C%2520resulting%2520in%2520the%2520sacrifice%2520of%2520specialized%250Acapabilities%2520for%2520domain-specific%2520tasks%2520that%2520require%2520extensive%2520domain%2520prior%250Aknowledge.%2520Moreover%252C%2520directly%2520integrating%2520expert%2520models%2520tailored%2520for%2520specific%250Adomains%2520is%2520challenging%2520due%2520to%2520the%2520representational%2520gap%2520and%2520imbalanced%250Aoptimization%2520between%2520the%2520generalist%2520model%2520and%2520experts.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520Chimera%252C%2520a%2520scalable%2520and%2520low-cost%2520multi-modal%2520pipeline%250Adesigned%2520to%2520boost%2520the%2520ability%2520of%2520existing%2520LMMs%2520with%2520domain-specific%2520experts.%250ASpecifically%252C%2520we%2520design%2520a%2520progressive%2520training%2520strategy%2520to%2520integrate%2520features%250Afrom%2520expert%2520models%2520into%2520the%2520input%2520of%2520a%2520generalist%2520LMM.%2520To%2520address%2520the%250Aimbalanced%2520optimization%2520caused%2520by%2520the%2520well-aligned%2520general%2520visual%2520encoder%252C%2520we%250Aintroduce%2520a%2520novel%2520Generalist-Specialist%2520Collaboration%2520Masking%2520%2528GSCM%2529%2520mechanism.%250AThis%2520results%2520in%2520a%2520versatile%2520model%2520that%2520excels%2520across%2520the%2520chart%252C%2520table%252C%2520math%252C%250Aand%2520document%2520domains%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520multi-modal%250Areasoning%2520and%2520visual%2520content%2520extraction%2520tasks%252C%2520both%2520of%2520which%2520are%2520challenging%250Atasks%2520for%2520assessing%2520existing%2520LMMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chimera%3A%20Improving%20Generalist%20Model%20with%20Domain-Specific%20Experts&entry.906535625=Tianshuo%20Peng%20and%20Mingsheng%20Li%20and%20Hongbin%20Zhou%20and%20Renqiu%20Xia%20and%20Renrui%20Zhang%20and%20Lei%20Bai%20and%20Song%20Mao%20and%20Bin%20Wang%20and%20Conghui%20He%20and%20Aojun%20Zhou%20and%20Botian%20Shi%20and%20Tao%20Chen%20and%20Bo%20Zhang%20and%20Xiangyu%20Yue&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Multi-modal%20Models%20%28LMMs%29%20underscore%20the%0Aimportance%20of%20scaling%20by%20increasing%20image-text%20paired%20data%2C%20achieving%0Aimpressive%20performance%20on%20general%20tasks.%20Despite%20their%20effectiveness%20in%20broad%0Aapplications%2C%20generalist%20models%20are%20primarily%20trained%20on%20web-scale%20datasets%0Adominated%20by%20natural%20images%2C%20resulting%20in%20the%20sacrifice%20of%20specialized%0Acapabilities%20for%20domain-specific%20tasks%20that%20require%20extensive%20domain%20prior%0Aknowledge.%20Moreover%2C%20directly%20integrating%20expert%20models%20tailored%20for%20specific%0Adomains%20is%20challenging%20due%20to%20the%20representational%20gap%20and%20imbalanced%0Aoptimization%20between%20the%20generalist%20model%20and%20experts.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Chimera%2C%20a%20scalable%20and%20low-cost%20multi-modal%20pipeline%0Adesigned%20to%20boost%20the%20ability%20of%20existing%20LMMs%20with%20domain-specific%20experts.%0ASpecifically%2C%20we%20design%20a%20progressive%20training%20strategy%20to%20integrate%20features%0Afrom%20expert%20models%20into%20the%20input%20of%20a%20generalist%20LMM.%20To%20address%20the%0Aimbalanced%20optimization%20caused%20by%20the%20well-aligned%20general%20visual%20encoder%2C%20we%0Aintroduce%20a%20novel%20Generalist-Specialist%20Collaboration%20Masking%20%28GSCM%29%20mechanism.%0AThis%20results%20in%20a%20versatile%20model%20that%20excels%20across%20the%20chart%2C%20table%2C%20math%2C%0Aand%20document%20domains%2C%20achieving%20state-of-the-art%20performance%20on%20multi-modal%0Areasoning%20and%20visual%20content%20extraction%20tasks%2C%20both%20of%20which%20are%20challenging%0Atasks%20for%20assessing%20existing%20LMMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05983v2&entry.124074799=Read"},
{"title": "Two Stage Segmentation of Cervical Tumors using PocketNet", "author": "Awj Twam and Megan Jacobsen and Rachel Glenn and Peng Wei and Jia Sun and Ann Klopp and Aradhana M. Venkatesan and David Fuentes", "abstract": "  Cervical cancer remains the fourth most common malignancy amongst women\nworldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstay\ndefinitive treatment regimen for locally advanced cervical cancers and includes\nexternal beam radiation followed by brachytherapy.2 Integral to radiotherapy\ntreatment planning is the routine contouring of both the target tumor at the\nlevel of the cervix, associated gynecologic anatomy and the adjacent organs at\nrisk (OARs). However, manual contouring of these structures is both time and\nlabor intensive and associated with known interobserver variability that can\nimpact treatment outcomes. While multiple tools have been developed to\nautomatically segment OARs and the high-risk clinical tumor volume (HR-CTV)\nusing computed tomography (CT) images,3,4,5,6 the development of deep\nlearning-based tumor segmentation tools using routine T2-weighted (T2w)\nmagnetic resonance imaging (MRI) addresses an unmet clinical need to improve\nthe routine contouring of both anatomical structures and cervical cancers,\nthereby increasing quality and consistency of radiotherapy planning. This work\napplied a novel deep-learning model (PocketNet) to segment the cervix, vagina,\nuterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecture\nwas evaluated, when trained on data via 5-fold cross validation. PocketNet\nachieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% for\ntumor segmentation and 80% for organ segmentation. These results suggest that\nPocketNet is robust to variations in contrast protocols, providing reliable\nsegmentation of the regions of interest.\n", "link": "http://arxiv.org/abs/2409.11456v2", "date": "2025-01-10", "relevancy": 2.2049, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4524}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4433}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Stage%20Segmentation%20of%20Cervical%20Tumors%20using%20PocketNet&body=Title%3A%20Two%20Stage%20Segmentation%20of%20Cervical%20Tumors%20using%20PocketNet%0AAuthor%3A%20Awj%20Twam%20and%20Megan%20Jacobsen%20and%20Rachel%20Glenn%20and%20Peng%20Wei%20and%20Jia%20Sun%20and%20Ann%20Klopp%20and%20Aradhana%20M.%20Venkatesan%20and%20David%20Fuentes%0AAbstract%3A%20%20%20Cervical%20cancer%20remains%20the%20fourth%20most%20common%20malignancy%20amongst%20women%0Aworldwide.1%20Concurrent%20chemoradiotherapy%20%28CRT%29%20serves%20as%20the%20mainstay%0Adefinitive%20treatment%20regimen%20for%20locally%20advanced%20cervical%20cancers%20and%20includes%0Aexternal%20beam%20radiation%20followed%20by%20brachytherapy.2%20Integral%20to%20radiotherapy%0Atreatment%20planning%20is%20the%20routine%20contouring%20of%20both%20the%20target%20tumor%20at%20the%0Alevel%20of%20the%20cervix%2C%20associated%20gynecologic%20anatomy%20and%20the%20adjacent%20organs%20at%0Arisk%20%28OARs%29.%20However%2C%20manual%20contouring%20of%20these%20structures%20is%20both%20time%20and%0Alabor%20intensive%20and%20associated%20with%20known%20interobserver%20variability%20that%20can%0Aimpact%20treatment%20outcomes.%20While%20multiple%20tools%20have%20been%20developed%20to%0Aautomatically%20segment%20OARs%20and%20the%20high-risk%20clinical%20tumor%20volume%20%28HR-CTV%29%0Ausing%20computed%20tomography%20%28CT%29%20images%2C3%2C4%2C5%2C6%20the%20development%20of%20deep%0Alearning-based%20tumor%20segmentation%20tools%20using%20routine%20T2-weighted%20%28T2w%29%0Amagnetic%20resonance%20imaging%20%28MRI%29%20addresses%20an%20unmet%20clinical%20need%20to%20improve%0Athe%20routine%20contouring%20of%20both%20anatomical%20structures%20and%20cervical%20cancers%2C%0Athereby%20increasing%20quality%20and%20consistency%20of%20radiotherapy%20planning.%20This%20work%0Aapplied%20a%20novel%20deep-learning%20model%20%28PocketNet%29%20to%20segment%20the%20cervix%2C%20vagina%2C%0Auterus%2C%20and%20tumor%28s%29%20on%20T2w%20MRI.%20The%20performance%20of%20the%20PocketNet%20architecture%0Awas%20evaluated%2C%20when%20trained%20on%20data%20via%205-fold%20cross%20validation.%20PocketNet%0Aachieved%20a%20mean%20Dice-Sorensen%20similarity%20coefficient%20%28DSC%29%20exceeding%2070%25%20for%0Atumor%20segmentation%20and%2080%25%20for%20organ%20segmentation.%20These%20results%20suggest%20that%0APocketNet%20is%20robust%20to%20variations%20in%20contrast%20protocols%2C%20providing%20reliable%0Asegmentation%20of%20the%20regions%20of%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Stage%2520Segmentation%2520of%2520Cervical%2520Tumors%2520using%2520PocketNet%26entry.906535625%3DAwj%2520Twam%2520and%2520Megan%2520Jacobsen%2520and%2520Rachel%2520Glenn%2520and%2520Peng%2520Wei%2520and%2520Jia%2520Sun%2520and%2520Ann%2520Klopp%2520and%2520Aradhana%2520M.%2520Venkatesan%2520and%2520David%2520Fuentes%26entry.1292438233%3D%2520%2520Cervical%2520cancer%2520remains%2520the%2520fourth%2520most%2520common%2520malignancy%2520amongst%2520women%250Aworldwide.1%2520Concurrent%2520chemoradiotherapy%2520%2528CRT%2529%2520serves%2520as%2520the%2520mainstay%250Adefinitive%2520treatment%2520regimen%2520for%2520locally%2520advanced%2520cervical%2520cancers%2520and%2520includes%250Aexternal%2520beam%2520radiation%2520followed%2520by%2520brachytherapy.2%2520Integral%2520to%2520radiotherapy%250Atreatment%2520planning%2520is%2520the%2520routine%2520contouring%2520of%2520both%2520the%2520target%2520tumor%2520at%2520the%250Alevel%2520of%2520the%2520cervix%252C%2520associated%2520gynecologic%2520anatomy%2520and%2520the%2520adjacent%2520organs%2520at%250Arisk%2520%2528OARs%2529.%2520However%252C%2520manual%2520contouring%2520of%2520these%2520structures%2520is%2520both%2520time%2520and%250Alabor%2520intensive%2520and%2520associated%2520with%2520known%2520interobserver%2520variability%2520that%2520can%250Aimpact%2520treatment%2520outcomes.%2520While%2520multiple%2520tools%2520have%2520been%2520developed%2520to%250Aautomatically%2520segment%2520OARs%2520and%2520the%2520high-risk%2520clinical%2520tumor%2520volume%2520%2528HR-CTV%2529%250Ausing%2520computed%2520tomography%2520%2528CT%2529%2520images%252C3%252C4%252C5%252C6%2520the%2520development%2520of%2520deep%250Alearning-based%2520tumor%2520segmentation%2520tools%2520using%2520routine%2520T2-weighted%2520%2528T2w%2529%250Amagnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520addresses%2520an%2520unmet%2520clinical%2520need%2520to%2520improve%250Athe%2520routine%2520contouring%2520of%2520both%2520anatomical%2520structures%2520and%2520cervical%2520cancers%252C%250Athereby%2520increasing%2520quality%2520and%2520consistency%2520of%2520radiotherapy%2520planning.%2520This%2520work%250Aapplied%2520a%2520novel%2520deep-learning%2520model%2520%2528PocketNet%2529%2520to%2520segment%2520the%2520cervix%252C%2520vagina%252C%250Auterus%252C%2520and%2520tumor%2528s%2529%2520on%2520T2w%2520MRI.%2520The%2520performance%2520of%2520the%2520PocketNet%2520architecture%250Awas%2520evaluated%252C%2520when%2520trained%2520on%2520data%2520via%25205-fold%2520cross%2520validation.%2520PocketNet%250Aachieved%2520a%2520mean%2520Dice-Sorensen%2520similarity%2520coefficient%2520%2528DSC%2529%2520exceeding%252070%2525%2520for%250Atumor%2520segmentation%2520and%252080%2525%2520for%2520organ%2520segmentation.%2520These%2520results%2520suggest%2520that%250APocketNet%2520is%2520robust%2520to%2520variations%2520in%2520contrast%2520protocols%252C%2520providing%2520reliable%250Asegmentation%2520of%2520the%2520regions%2520of%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Stage%20Segmentation%20of%20Cervical%20Tumors%20using%20PocketNet&entry.906535625=Awj%20Twam%20and%20Megan%20Jacobsen%20and%20Rachel%20Glenn%20and%20Peng%20Wei%20and%20Jia%20Sun%20and%20Ann%20Klopp%20and%20Aradhana%20M.%20Venkatesan%20and%20David%20Fuentes&entry.1292438233=%20%20Cervical%20cancer%20remains%20the%20fourth%20most%20common%20malignancy%20amongst%20women%0Aworldwide.1%20Concurrent%20chemoradiotherapy%20%28CRT%29%20serves%20as%20the%20mainstay%0Adefinitive%20treatment%20regimen%20for%20locally%20advanced%20cervical%20cancers%20and%20includes%0Aexternal%20beam%20radiation%20followed%20by%20brachytherapy.2%20Integral%20to%20radiotherapy%0Atreatment%20planning%20is%20the%20routine%20contouring%20of%20both%20the%20target%20tumor%20at%20the%0Alevel%20of%20the%20cervix%2C%20associated%20gynecologic%20anatomy%20and%20the%20adjacent%20organs%20at%0Arisk%20%28OARs%29.%20However%2C%20manual%20contouring%20of%20these%20structures%20is%20both%20time%20and%0Alabor%20intensive%20and%20associated%20with%20known%20interobserver%20variability%20that%20can%0Aimpact%20treatment%20outcomes.%20While%20multiple%20tools%20have%20been%20developed%20to%0Aautomatically%20segment%20OARs%20and%20the%20high-risk%20clinical%20tumor%20volume%20%28HR-CTV%29%0Ausing%20computed%20tomography%20%28CT%29%20images%2C3%2C4%2C5%2C6%20the%20development%20of%20deep%0Alearning-based%20tumor%20segmentation%20tools%20using%20routine%20T2-weighted%20%28T2w%29%0Amagnetic%20resonance%20imaging%20%28MRI%29%20addresses%20an%20unmet%20clinical%20need%20to%20improve%0Athe%20routine%20contouring%20of%20both%20anatomical%20structures%20and%20cervical%20cancers%2C%0Athereby%20increasing%20quality%20and%20consistency%20of%20radiotherapy%20planning.%20This%20work%0Aapplied%20a%20novel%20deep-learning%20model%20%28PocketNet%29%20to%20segment%20the%20cervix%2C%20vagina%2C%0Auterus%2C%20and%20tumor%28s%29%20on%20T2w%20MRI.%20The%20performance%20of%20the%20PocketNet%20architecture%0Awas%20evaluated%2C%20when%20trained%20on%20data%20via%205-fold%20cross%20validation.%20PocketNet%0Aachieved%20a%20mean%20Dice-Sorensen%20similarity%20coefficient%20%28DSC%29%20exceeding%2070%25%20for%0Atumor%20segmentation%20and%2080%25%20for%20organ%20segmentation.%20These%20results%20suggest%20that%0APocketNet%20is%20robust%20to%20variations%20in%20contrast%20protocols%2C%20providing%20reliable%0Asegmentation%20of%20the%20regions%20of%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11456v2&entry.124074799=Read"},
{"title": "COMIX: Compositional Explanations using Prototypes", "author": "Sarath Sivaprasad and Dmitry Kangin and Plamen Angelov and Mario Fritz", "abstract": "  Aligning machine representations with human understanding is key to improving\ninterpretability of machine learning (ML) models. When classifying a new image,\nhumans often explain their decisions by decomposing the image into concepts and\npointing to corresponding regions in familiar images. Current ML explanation\ntechniques typically either trace decision-making processes to reference\nprototypes, generate attribution maps highlighting feature importance, or\nincorporate intermediate bottlenecks designed to align with human-interpretable\nconcepts. The proposed method, named COMIX, classifies an image by decomposing\nit into regions based on learned concepts and tracing each region to\ncorresponding ones in images from the training dataset, assuring that\nexplanations fully represent the actual decision-making process. We dissect the\ntest image into selected internal representations of a neural network to derive\nprototypical parts (primitives) and match them with the corresponding\nprimitives derived from the training data. In a series of qualitative and\nquantitative experiments, we theoretically prove and demonstrate that our\nmethod, in contrast to post hoc analysis, provides fidelity of explanations and\nshows that the efficiency is competitive with other inherently interpretable\narchitectures. Notably, it shows substantial improvements in fidelity and\nsparsity metrics, including 48.82% improvement in the C-insertion score on the\nImageNet dataset over the best state-of-the-art baseline.\n", "link": "http://arxiv.org/abs/2501.06059v1", "date": "2025-01-10", "relevancy": 2.1962, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMIX%3A%20Compositional%20Explanations%20using%20Prototypes&body=Title%3A%20COMIX%3A%20Compositional%20Explanations%20using%20Prototypes%0AAuthor%3A%20Sarath%20Sivaprasad%20and%20Dmitry%20Kangin%20and%20Plamen%20Angelov%20and%20Mario%20Fritz%0AAbstract%3A%20%20%20Aligning%20machine%20representations%20with%20human%20understanding%20is%20key%20to%20improving%0Ainterpretability%20of%20machine%20learning%20%28ML%29%20models.%20When%20classifying%20a%20new%20image%2C%0Ahumans%20often%20explain%20their%20decisions%20by%20decomposing%20the%20image%20into%20concepts%20and%0Apointing%20to%20corresponding%20regions%20in%20familiar%20images.%20Current%20ML%20explanation%0Atechniques%20typically%20either%20trace%20decision-making%20processes%20to%20reference%0Aprototypes%2C%20generate%20attribution%20maps%20highlighting%20feature%20importance%2C%20or%0Aincorporate%20intermediate%20bottlenecks%20designed%20to%20align%20with%20human-interpretable%0Aconcepts.%20The%20proposed%20method%2C%20named%20COMIX%2C%20classifies%20an%20image%20by%20decomposing%0Ait%20into%20regions%20based%20on%20learned%20concepts%20and%20tracing%20each%20region%20to%0Acorresponding%20ones%20in%20images%20from%20the%20training%20dataset%2C%20assuring%20that%0Aexplanations%20fully%20represent%20the%20actual%20decision-making%20process.%20We%20dissect%20the%0Atest%20image%20into%20selected%20internal%20representations%20of%20a%20neural%20network%20to%20derive%0Aprototypical%20parts%20%28primitives%29%20and%20match%20them%20with%20the%20corresponding%0Aprimitives%20derived%20from%20the%20training%20data.%20In%20a%20series%20of%20qualitative%20and%0Aquantitative%20experiments%2C%20we%20theoretically%20prove%20and%20demonstrate%20that%20our%0Amethod%2C%20in%20contrast%20to%20post%20hoc%20analysis%2C%20provides%20fidelity%20of%20explanations%20and%0Ashows%20that%20the%20efficiency%20is%20competitive%20with%20other%20inherently%20interpretable%0Aarchitectures.%20Notably%2C%20it%20shows%20substantial%20improvements%20in%20fidelity%20and%0Asparsity%20metrics%2C%20including%2048.82%25%20improvement%20in%20the%20C-insertion%20score%20on%20the%0AImageNet%20dataset%20over%20the%20best%20state-of-the-art%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMIX%253A%2520Compositional%2520Explanations%2520using%2520Prototypes%26entry.906535625%3DSarath%2520Sivaprasad%2520and%2520Dmitry%2520Kangin%2520and%2520Plamen%2520Angelov%2520and%2520Mario%2520Fritz%26entry.1292438233%3D%2520%2520Aligning%2520machine%2520representations%2520with%2520human%2520understanding%2520is%2520key%2520to%2520improving%250Ainterpretability%2520of%2520machine%2520learning%2520%2528ML%2529%2520models.%2520When%2520classifying%2520a%2520new%2520image%252C%250Ahumans%2520often%2520explain%2520their%2520decisions%2520by%2520decomposing%2520the%2520image%2520into%2520concepts%2520and%250Apointing%2520to%2520corresponding%2520regions%2520in%2520familiar%2520images.%2520Current%2520ML%2520explanation%250Atechniques%2520typically%2520either%2520trace%2520decision-making%2520processes%2520to%2520reference%250Aprototypes%252C%2520generate%2520attribution%2520maps%2520highlighting%2520feature%2520importance%252C%2520or%250Aincorporate%2520intermediate%2520bottlenecks%2520designed%2520to%2520align%2520with%2520human-interpretable%250Aconcepts.%2520The%2520proposed%2520method%252C%2520named%2520COMIX%252C%2520classifies%2520an%2520image%2520by%2520decomposing%250Ait%2520into%2520regions%2520based%2520on%2520learned%2520concepts%2520and%2520tracing%2520each%2520region%2520to%250Acorresponding%2520ones%2520in%2520images%2520from%2520the%2520training%2520dataset%252C%2520assuring%2520that%250Aexplanations%2520fully%2520represent%2520the%2520actual%2520decision-making%2520process.%2520We%2520dissect%2520the%250Atest%2520image%2520into%2520selected%2520internal%2520representations%2520of%2520a%2520neural%2520network%2520to%2520derive%250Aprototypical%2520parts%2520%2528primitives%2529%2520and%2520match%2520them%2520with%2520the%2520corresponding%250Aprimitives%2520derived%2520from%2520the%2520training%2520data.%2520In%2520a%2520series%2520of%2520qualitative%2520and%250Aquantitative%2520experiments%252C%2520we%2520theoretically%2520prove%2520and%2520demonstrate%2520that%2520our%250Amethod%252C%2520in%2520contrast%2520to%2520post%2520hoc%2520analysis%252C%2520provides%2520fidelity%2520of%2520explanations%2520and%250Ashows%2520that%2520the%2520efficiency%2520is%2520competitive%2520with%2520other%2520inherently%2520interpretable%250Aarchitectures.%2520Notably%252C%2520it%2520shows%2520substantial%2520improvements%2520in%2520fidelity%2520and%250Asparsity%2520metrics%252C%2520including%252048.82%2525%2520improvement%2520in%2520the%2520C-insertion%2520score%2520on%2520the%250AImageNet%2520dataset%2520over%2520the%2520best%2520state-of-the-art%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMIX%3A%20Compositional%20Explanations%20using%20Prototypes&entry.906535625=Sarath%20Sivaprasad%20and%20Dmitry%20Kangin%20and%20Plamen%20Angelov%20and%20Mario%20Fritz&entry.1292438233=%20%20Aligning%20machine%20representations%20with%20human%20understanding%20is%20key%20to%20improving%0Ainterpretability%20of%20machine%20learning%20%28ML%29%20models.%20When%20classifying%20a%20new%20image%2C%0Ahumans%20often%20explain%20their%20decisions%20by%20decomposing%20the%20image%20into%20concepts%20and%0Apointing%20to%20corresponding%20regions%20in%20familiar%20images.%20Current%20ML%20explanation%0Atechniques%20typically%20either%20trace%20decision-making%20processes%20to%20reference%0Aprototypes%2C%20generate%20attribution%20maps%20highlighting%20feature%20importance%2C%20or%0Aincorporate%20intermediate%20bottlenecks%20designed%20to%20align%20with%20human-interpretable%0Aconcepts.%20The%20proposed%20method%2C%20named%20COMIX%2C%20classifies%20an%20image%20by%20decomposing%0Ait%20into%20regions%20based%20on%20learned%20concepts%20and%20tracing%20each%20region%20to%0Acorresponding%20ones%20in%20images%20from%20the%20training%20dataset%2C%20assuring%20that%0Aexplanations%20fully%20represent%20the%20actual%20decision-making%20process.%20We%20dissect%20the%0Atest%20image%20into%20selected%20internal%20representations%20of%20a%20neural%20network%20to%20derive%0Aprototypical%20parts%20%28primitives%29%20and%20match%20them%20with%20the%20corresponding%0Aprimitives%20derived%20from%20the%20training%20data.%20In%20a%20series%20of%20qualitative%20and%0Aquantitative%20experiments%2C%20we%20theoretically%20prove%20and%20demonstrate%20that%20our%0Amethod%2C%20in%20contrast%20to%20post%20hoc%20analysis%2C%20provides%20fidelity%20of%20explanations%20and%0Ashows%20that%20the%20efficiency%20is%20competitive%20with%20other%20inherently%20interpretable%0Aarchitectures.%20Notably%2C%20it%20shows%20substantial%20improvements%20in%20fidelity%20and%0Asparsity%20metrics%2C%20including%2048.82%25%20improvement%20in%20the%20C-insertion%20score%20on%20the%0AImageNet%20dataset%20over%20the%20best%20state-of-the-art%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06059v1&entry.124074799=Read"},
{"title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus", "author": "Soyeong Jeong and Kangsan Kim and Jinheon Baek and Sung Ju Hwang", "abstract": "  Retrieval-Augmented Generation (RAG) is a powerful strategy to address the\nissue of generating factually incorrect outputs in foundation models by\nretrieving external knowledge relevant to queries and incorporating it into\ntheir generation process. However, existing RAG approaches have primarily\nfocused on textual information, with some recent advancements beginning to\nconsider images, and they largely overlook videos, a rich source of multimodal\nknowledge capable of representing events, processes, and contextual details\nmore effectively than any other modality. While a few recent studies explore\nthe integration of videos in the response generation process, they either\npredefine query-associated videos without retrieving them according to queries,\nor convert videos into the textual descriptions without harnessing their\nmultimodal richness. To tackle these, we introduce VideoRAG, a novel framework\nthat not only dynamically retrieves relevant videos based on their relevance\nwith queries but also utilizes both visual and textual information of videos in\nthe output generation. Further, to operationalize this, our method revolves\naround the recent advance of Large Video Language Models (LVLMs), which enable\nthe direct processing of video content to represent it for retrieval and\nseamless integration of the retrieved videos jointly with queries. We\nexperimentally validate the effectiveness of VideoRAG, showcasing that it is\nsuperior to relevant baselines.\n", "link": "http://arxiv.org/abs/2501.05874v1", "date": "2025-01-10", "relevancy": 2.188, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5618}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5397}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoRAG%3A%20Retrieval-Augmented%20Generation%20over%20Video%20Corpus&body=Title%3A%20VideoRAG%3A%20Retrieval-Augmented%20Generation%20over%20Video%20Corpus%0AAuthor%3A%20Soyeong%20Jeong%20and%20Kangsan%20Kim%20and%20Jinheon%20Baek%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20is%20a%20powerful%20strategy%20to%20address%20the%0Aissue%20of%20generating%20factually%20incorrect%20outputs%20in%20foundation%20models%20by%0Aretrieving%20external%20knowledge%20relevant%20to%20queries%20and%20incorporating%20it%20into%0Atheir%20generation%20process.%20However%2C%20existing%20RAG%20approaches%20have%20primarily%0Afocused%20on%20textual%20information%2C%20with%20some%20recent%20advancements%20beginning%20to%0Aconsider%20images%2C%20and%20they%20largely%20overlook%20videos%2C%20a%20rich%20source%20of%20multimodal%0Aknowledge%20capable%20of%20representing%20events%2C%20processes%2C%20and%20contextual%20details%0Amore%20effectively%20than%20any%20other%20modality.%20While%20a%20few%20recent%20studies%20explore%0Athe%20integration%20of%20videos%20in%20the%20response%20generation%20process%2C%20they%20either%0Apredefine%20query-associated%20videos%20without%20retrieving%20them%20according%20to%20queries%2C%0Aor%20convert%20videos%20into%20the%20textual%20descriptions%20without%20harnessing%20their%0Amultimodal%20richness.%20To%20tackle%20these%2C%20we%20introduce%20VideoRAG%2C%20a%20novel%20framework%0Athat%20not%20only%20dynamically%20retrieves%20relevant%20videos%20based%20on%20their%20relevance%0Awith%20queries%20but%20also%20utilizes%20both%20visual%20and%20textual%20information%20of%20videos%20in%0Athe%20output%20generation.%20Further%2C%20to%20operationalize%20this%2C%20our%20method%20revolves%0Aaround%20the%20recent%20advance%20of%20Large%20Video%20Language%20Models%20%28LVLMs%29%2C%20which%20enable%0Athe%20direct%20processing%20of%20video%20content%20to%20represent%20it%20for%20retrieval%20and%0Aseamless%20integration%20of%20the%20retrieved%20videos%20jointly%20with%20queries.%20We%0Aexperimentally%20validate%20the%20effectiveness%20of%20VideoRAG%2C%20showcasing%20that%20it%20is%0Asuperior%20to%20relevant%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoRAG%253A%2520Retrieval-Augmented%2520Generation%2520over%2520Video%2520Corpus%26entry.906535625%3DSoyeong%2520Jeong%2520and%2520Kangsan%2520Kim%2520and%2520Jinheon%2520Baek%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520is%2520a%2520powerful%2520strategy%2520to%2520address%2520the%250Aissue%2520of%2520generating%2520factually%2520incorrect%2520outputs%2520in%2520foundation%2520models%2520by%250Aretrieving%2520external%2520knowledge%2520relevant%2520to%2520queries%2520and%2520incorporating%2520it%2520into%250Atheir%2520generation%2520process.%2520However%252C%2520existing%2520RAG%2520approaches%2520have%2520primarily%250Afocused%2520on%2520textual%2520information%252C%2520with%2520some%2520recent%2520advancements%2520beginning%2520to%250Aconsider%2520images%252C%2520and%2520they%2520largely%2520overlook%2520videos%252C%2520a%2520rich%2520source%2520of%2520multimodal%250Aknowledge%2520capable%2520of%2520representing%2520events%252C%2520processes%252C%2520and%2520contextual%2520details%250Amore%2520effectively%2520than%2520any%2520other%2520modality.%2520While%2520a%2520few%2520recent%2520studies%2520explore%250Athe%2520integration%2520of%2520videos%2520in%2520the%2520response%2520generation%2520process%252C%2520they%2520either%250Apredefine%2520query-associated%2520videos%2520without%2520retrieving%2520them%2520according%2520to%2520queries%252C%250Aor%2520convert%2520videos%2520into%2520the%2520textual%2520descriptions%2520without%2520harnessing%2520their%250Amultimodal%2520richness.%2520To%2520tackle%2520these%252C%2520we%2520introduce%2520VideoRAG%252C%2520a%2520novel%2520framework%250Athat%2520not%2520only%2520dynamically%2520retrieves%2520relevant%2520videos%2520based%2520on%2520their%2520relevance%250Awith%2520queries%2520but%2520also%2520utilizes%2520both%2520visual%2520and%2520textual%2520information%2520of%2520videos%2520in%250Athe%2520output%2520generation.%2520Further%252C%2520to%2520operationalize%2520this%252C%2520our%2520method%2520revolves%250Aaround%2520the%2520recent%2520advance%2520of%2520Large%2520Video%2520Language%2520Models%2520%2528LVLMs%2529%252C%2520which%2520enable%250Athe%2520direct%2520processing%2520of%2520video%2520content%2520to%2520represent%2520it%2520for%2520retrieval%2520and%250Aseamless%2520integration%2520of%2520the%2520retrieved%2520videos%2520jointly%2520with%2520queries.%2520We%250Aexperimentally%2520validate%2520the%2520effectiveness%2520of%2520VideoRAG%252C%2520showcasing%2520that%2520it%2520is%250Asuperior%2520to%2520relevant%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoRAG%3A%20Retrieval-Augmented%20Generation%20over%20Video%20Corpus&entry.906535625=Soyeong%20Jeong%20and%20Kangsan%20Kim%20and%20Jinheon%20Baek%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20is%20a%20powerful%20strategy%20to%20address%20the%0Aissue%20of%20generating%20factually%20incorrect%20outputs%20in%20foundation%20models%20by%0Aretrieving%20external%20knowledge%20relevant%20to%20queries%20and%20incorporating%20it%20into%0Atheir%20generation%20process.%20However%2C%20existing%20RAG%20approaches%20have%20primarily%0Afocused%20on%20textual%20information%2C%20with%20some%20recent%20advancements%20beginning%20to%0Aconsider%20images%2C%20and%20they%20largely%20overlook%20videos%2C%20a%20rich%20source%20of%20multimodal%0Aknowledge%20capable%20of%20representing%20events%2C%20processes%2C%20and%20contextual%20details%0Amore%20effectively%20than%20any%20other%20modality.%20While%20a%20few%20recent%20studies%20explore%0Athe%20integration%20of%20videos%20in%20the%20response%20generation%20process%2C%20they%20either%0Apredefine%20query-associated%20videos%20without%20retrieving%20them%20according%20to%20queries%2C%0Aor%20convert%20videos%20into%20the%20textual%20descriptions%20without%20harnessing%20their%0Amultimodal%20richness.%20To%20tackle%20these%2C%20we%20introduce%20VideoRAG%2C%20a%20novel%20framework%0Athat%20not%20only%20dynamically%20retrieves%20relevant%20videos%20based%20on%20their%20relevance%0Awith%20queries%20but%20also%20utilizes%20both%20visual%20and%20textual%20information%20of%20videos%20in%0Athe%20output%20generation.%20Further%2C%20to%20operationalize%20this%2C%20our%20method%20revolves%0Aaround%20the%20recent%20advance%20of%20Large%20Video%20Language%20Models%20%28LVLMs%29%2C%20which%20enable%0Athe%20direct%20processing%20of%20video%20content%20to%20represent%20it%20for%20retrieval%20and%0Aseamless%20integration%20of%20the%20retrieved%20videos%20jointly%20with%20queries.%20We%0Aexperimentally%20validate%20the%20effectiveness%20of%20VideoRAG%2C%20showcasing%20that%20it%20is%0Asuperior%20to%20relevant%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05874v1&entry.124074799=Read"},
{"title": "Geometry and Optimization of Shallow Polynomial Networks", "author": "Yossi Arjevani and Joan Bruna and Joe Kileel and Elzbieta Polak and Matthew Trager", "abstract": "  We study shallow neural networks with polynomial activations. The function\nspace for these models can be identified with a set of symmetric tensors with\nbounded rank. We describe general features of these networks, focusing on the\nrelationship between width and optimization. We then consider teacher-student\nproblems, that can be viewed as a problem of low-rank tensor approximation with\nrespect to a non-standard inner product that is induced by the data\ndistribution. In this setting, we introduce a teacher-metric discriminant which\nencodes the qualitative behavior of the optimization as a function of the\ntraining data distribution. Finally, we focus on networks with quadratic\nactivations, presenting an in-depth analysis of the optimization landscape. In\nparticular, we present a variation of the Eckart-Young Theorem characterizing\nall critical points and their Hessian signatures for teacher-student problems\nwith quadratic networks and Gaussian training data.\n", "link": "http://arxiv.org/abs/2501.06074v1", "date": "2025-01-10", "relevancy": 2.174, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4414}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4322}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20and%20Optimization%20of%20Shallow%20Polynomial%20Networks&body=Title%3A%20Geometry%20and%20Optimization%20of%20Shallow%20Polynomial%20Networks%0AAuthor%3A%20Yossi%20Arjevani%20and%20Joan%20Bruna%20and%20Joe%20Kileel%20and%20Elzbieta%20Polak%20and%20Matthew%20Trager%0AAbstract%3A%20%20%20We%20study%20shallow%20neural%20networks%20with%20polynomial%20activations.%20The%20function%0Aspace%20for%20these%20models%20can%20be%20identified%20with%20a%20set%20of%20symmetric%20tensors%20with%0Abounded%20rank.%20We%20describe%20general%20features%20of%20these%20networks%2C%20focusing%20on%20the%0Arelationship%20between%20width%20and%20optimization.%20We%20then%20consider%20teacher-student%0Aproblems%2C%20that%20can%20be%20viewed%20as%20a%20problem%20of%20low-rank%20tensor%20approximation%20with%0Arespect%20to%20a%20non-standard%20inner%20product%20that%20is%20induced%20by%20the%20data%0Adistribution.%20In%20this%20setting%2C%20we%20introduce%20a%20teacher-metric%20discriminant%20which%0Aencodes%20the%20qualitative%20behavior%20of%20the%20optimization%20as%20a%20function%20of%20the%0Atraining%20data%20distribution.%20Finally%2C%20we%20focus%20on%20networks%20with%20quadratic%0Aactivations%2C%20presenting%20an%20in-depth%20analysis%20of%20the%20optimization%20landscape.%20In%0Aparticular%2C%20we%20present%20a%20variation%20of%20the%20Eckart-Young%20Theorem%20characterizing%0Aall%20critical%20points%20and%20their%20Hessian%20signatures%20for%20teacher-student%20problems%0Awith%20quadratic%20networks%20and%20Gaussian%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520and%2520Optimization%2520of%2520Shallow%2520Polynomial%2520Networks%26entry.906535625%3DYossi%2520Arjevani%2520and%2520Joan%2520Bruna%2520and%2520Joe%2520Kileel%2520and%2520Elzbieta%2520Polak%2520and%2520Matthew%2520Trager%26entry.1292438233%3D%2520%2520We%2520study%2520shallow%2520neural%2520networks%2520with%2520polynomial%2520activations.%2520The%2520function%250Aspace%2520for%2520these%2520models%2520can%2520be%2520identified%2520with%2520a%2520set%2520of%2520symmetric%2520tensors%2520with%250Abounded%2520rank.%2520We%2520describe%2520general%2520features%2520of%2520these%2520networks%252C%2520focusing%2520on%2520the%250Arelationship%2520between%2520width%2520and%2520optimization.%2520We%2520then%2520consider%2520teacher-student%250Aproblems%252C%2520that%2520can%2520be%2520viewed%2520as%2520a%2520problem%2520of%2520low-rank%2520tensor%2520approximation%2520with%250Arespect%2520to%2520a%2520non-standard%2520inner%2520product%2520that%2520is%2520induced%2520by%2520the%2520data%250Adistribution.%2520In%2520this%2520setting%252C%2520we%2520introduce%2520a%2520teacher-metric%2520discriminant%2520which%250Aencodes%2520the%2520qualitative%2520behavior%2520of%2520the%2520optimization%2520as%2520a%2520function%2520of%2520the%250Atraining%2520data%2520distribution.%2520Finally%252C%2520we%2520focus%2520on%2520networks%2520with%2520quadratic%250Aactivations%252C%2520presenting%2520an%2520in-depth%2520analysis%2520of%2520the%2520optimization%2520landscape.%2520In%250Aparticular%252C%2520we%2520present%2520a%2520variation%2520of%2520the%2520Eckart-Young%2520Theorem%2520characterizing%250Aall%2520critical%2520points%2520and%2520their%2520Hessian%2520signatures%2520for%2520teacher-student%2520problems%250Awith%2520quadratic%2520networks%2520and%2520Gaussian%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20and%20Optimization%20of%20Shallow%20Polynomial%20Networks&entry.906535625=Yossi%20Arjevani%20and%20Joan%20Bruna%20and%20Joe%20Kileel%20and%20Elzbieta%20Polak%20and%20Matthew%20Trager&entry.1292438233=%20%20We%20study%20shallow%20neural%20networks%20with%20polynomial%20activations.%20The%20function%0Aspace%20for%20these%20models%20can%20be%20identified%20with%20a%20set%20of%20symmetric%20tensors%20with%0Abounded%20rank.%20We%20describe%20general%20features%20of%20these%20networks%2C%20focusing%20on%20the%0Arelationship%20between%20width%20and%20optimization.%20We%20then%20consider%20teacher-student%0Aproblems%2C%20that%20can%20be%20viewed%20as%20a%20problem%20of%20low-rank%20tensor%20approximation%20with%0Arespect%20to%20a%20non-standard%20inner%20product%20that%20is%20induced%20by%20the%20data%0Adistribution.%20In%20this%20setting%2C%20we%20introduce%20a%20teacher-metric%20discriminant%20which%0Aencodes%20the%20qualitative%20behavior%20of%20the%20optimization%20as%20a%20function%20of%20the%0Atraining%20data%20distribution.%20Finally%2C%20we%20focus%20on%20networks%20with%20quadratic%0Aactivations%2C%20presenting%20an%20in-depth%20analysis%20of%20the%20optimization%20landscape.%20In%0Aparticular%2C%20we%20present%20a%20variation%20of%20the%20Eckart-Young%20Theorem%20characterizing%0Aall%20critical%20points%20and%20their%20Hessian%20signatures%20for%20teacher-student%20problems%0Awith%20quadratic%20networks%20and%20Gaussian%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06074v1&entry.124074799=Read"},
{"title": "TakuNet: an Energy-Efficient CNN for Real-Time Inference on Embedded UAV\n  systems in Emergency Response Scenarios", "author": "Daniel Rossi and Guido Borghi and Roberto Vezzani", "abstract": "  Designing efficient neural networks for embedded devices is a critical\nchallenge, particularly in applications requiring real-time performance, such\nas aerial imaging with drones and UAVs for emergency responses. In this work,\nwe introduce TakuNet, a novel light-weight architecture which employs\ntechniques such as depth-wise convolutions and an early downsampling stem to\nreduce computational complexity while maintaining high accuracy. It leverages\ndense connections for fast convergence during training and uses 16-bit\nfloating-point precision for optimization on embedded hardware accelerators.\nExperimental evaluation on two public datasets shows that TakuNet achieves\nnear-state-of-the-art accuracy in classifying aerial images of emergency\nsituations, despite its minimal parameter count. Real-world tests on embedded\ndevices, namely Jetson Orin Nano and Raspberry Pi, confirm TakuNet's\nefficiency, achieving more than 650 fps on the 15W Jetson board, making it\nsuitable for real-time AI processing on resource-constrained platforms and\nadvancing the applicability of drones in emergency scenarios. The code and\nimplementation details are publicly released.\n", "link": "http://arxiv.org/abs/2501.05880v1", "date": "2025-01-10", "relevancy": 2.1529, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5504}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5397}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TakuNet%3A%20an%20Energy-Efficient%20CNN%20for%20Real-Time%20Inference%20on%20Embedded%20UAV%0A%20%20systems%20in%20Emergency%20Response%20Scenarios&body=Title%3A%20TakuNet%3A%20an%20Energy-Efficient%20CNN%20for%20Real-Time%20Inference%20on%20Embedded%20UAV%0A%20%20systems%20in%20Emergency%20Response%20Scenarios%0AAuthor%3A%20Daniel%20Rossi%20and%20Guido%20Borghi%20and%20Roberto%20Vezzani%0AAbstract%3A%20%20%20Designing%20efficient%20neural%20networks%20for%20embedded%20devices%20is%20a%20critical%0Achallenge%2C%20particularly%20in%20applications%20requiring%20real-time%20performance%2C%20such%0Aas%20aerial%20imaging%20with%20drones%20and%20UAVs%20for%20emergency%20responses.%20In%20this%20work%2C%0Awe%20introduce%20TakuNet%2C%20a%20novel%20light-weight%20architecture%20which%20employs%0Atechniques%20such%20as%20depth-wise%20convolutions%20and%20an%20early%20downsampling%20stem%20to%0Areduce%20computational%20complexity%20while%20maintaining%20high%20accuracy.%20It%20leverages%0Adense%20connections%20for%20fast%20convergence%20during%20training%20and%20uses%2016-bit%0Afloating-point%20precision%20for%20optimization%20on%20embedded%20hardware%20accelerators.%0AExperimental%20evaluation%20on%20two%20public%20datasets%20shows%20that%20TakuNet%20achieves%0Anear-state-of-the-art%20accuracy%20in%20classifying%20aerial%20images%20of%20emergency%0Asituations%2C%20despite%20its%20minimal%20parameter%20count.%20Real-world%20tests%20on%20embedded%0Adevices%2C%20namely%20Jetson%20Orin%20Nano%20and%20Raspberry%20Pi%2C%20confirm%20TakuNet%27s%0Aefficiency%2C%20achieving%20more%20than%20650%20fps%20on%20the%2015W%20Jetson%20board%2C%20making%20it%0Asuitable%20for%20real-time%20AI%20processing%20on%20resource-constrained%20platforms%20and%0Aadvancing%20the%20applicability%20of%20drones%20in%20emergency%20scenarios.%20The%20code%20and%0Aimplementation%20details%20are%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTakuNet%253A%2520an%2520Energy-Efficient%2520CNN%2520for%2520Real-Time%2520Inference%2520on%2520Embedded%2520UAV%250A%2520%2520systems%2520in%2520Emergency%2520Response%2520Scenarios%26entry.906535625%3DDaniel%2520Rossi%2520and%2520Guido%2520Borghi%2520and%2520Roberto%2520Vezzani%26entry.1292438233%3D%2520%2520Designing%2520efficient%2520neural%2520networks%2520for%2520embedded%2520devices%2520is%2520a%2520critical%250Achallenge%252C%2520particularly%2520in%2520applications%2520requiring%2520real-time%2520performance%252C%2520such%250Aas%2520aerial%2520imaging%2520with%2520drones%2520and%2520UAVs%2520for%2520emergency%2520responses.%2520In%2520this%2520work%252C%250Awe%2520introduce%2520TakuNet%252C%2520a%2520novel%2520light-weight%2520architecture%2520which%2520employs%250Atechniques%2520such%2520as%2520depth-wise%2520convolutions%2520and%2520an%2520early%2520downsampling%2520stem%2520to%250Areduce%2520computational%2520complexity%2520while%2520maintaining%2520high%2520accuracy.%2520It%2520leverages%250Adense%2520connections%2520for%2520fast%2520convergence%2520during%2520training%2520and%2520uses%252016-bit%250Afloating-point%2520precision%2520for%2520optimization%2520on%2520embedded%2520hardware%2520accelerators.%250AExperimental%2520evaluation%2520on%2520two%2520public%2520datasets%2520shows%2520that%2520TakuNet%2520achieves%250Anear-state-of-the-art%2520accuracy%2520in%2520classifying%2520aerial%2520images%2520of%2520emergency%250Asituations%252C%2520despite%2520its%2520minimal%2520parameter%2520count.%2520Real-world%2520tests%2520on%2520embedded%250Adevices%252C%2520namely%2520Jetson%2520Orin%2520Nano%2520and%2520Raspberry%2520Pi%252C%2520confirm%2520TakuNet%2527s%250Aefficiency%252C%2520achieving%2520more%2520than%2520650%2520fps%2520on%2520the%252015W%2520Jetson%2520board%252C%2520making%2520it%250Asuitable%2520for%2520real-time%2520AI%2520processing%2520on%2520resource-constrained%2520platforms%2520and%250Aadvancing%2520the%2520applicability%2520of%2520drones%2520in%2520emergency%2520scenarios.%2520The%2520code%2520and%250Aimplementation%2520details%2520are%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TakuNet%3A%20an%20Energy-Efficient%20CNN%20for%20Real-Time%20Inference%20on%20Embedded%20UAV%0A%20%20systems%20in%20Emergency%20Response%20Scenarios&entry.906535625=Daniel%20Rossi%20and%20Guido%20Borghi%20and%20Roberto%20Vezzani&entry.1292438233=%20%20Designing%20efficient%20neural%20networks%20for%20embedded%20devices%20is%20a%20critical%0Achallenge%2C%20particularly%20in%20applications%20requiring%20real-time%20performance%2C%20such%0Aas%20aerial%20imaging%20with%20drones%20and%20UAVs%20for%20emergency%20responses.%20In%20this%20work%2C%0Awe%20introduce%20TakuNet%2C%20a%20novel%20light-weight%20architecture%20which%20employs%0Atechniques%20such%20as%20depth-wise%20convolutions%20and%20an%20early%20downsampling%20stem%20to%0Areduce%20computational%20complexity%20while%20maintaining%20high%20accuracy.%20It%20leverages%0Adense%20connections%20for%20fast%20convergence%20during%20training%20and%20uses%2016-bit%0Afloating-point%20precision%20for%20optimization%20on%20embedded%20hardware%20accelerators.%0AExperimental%20evaluation%20on%20two%20public%20datasets%20shows%20that%20TakuNet%20achieves%0Anear-state-of-the-art%20accuracy%20in%20classifying%20aerial%20images%20of%20emergency%0Asituations%2C%20despite%20its%20minimal%20parameter%20count.%20Real-world%20tests%20on%20embedded%0Adevices%2C%20namely%20Jetson%20Orin%20Nano%20and%20Raspberry%20Pi%2C%20confirm%20TakuNet%27s%0Aefficiency%2C%20achieving%20more%20than%20650%20fps%20on%20the%2015W%20Jetson%20board%2C%20making%20it%0Asuitable%20for%20real-time%20AI%20processing%20on%20resource-constrained%20platforms%20and%0Aadvancing%20the%20applicability%20of%20drones%20in%20emergency%20scenarios.%20The%20code%20and%0Aimplementation%20details%20are%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05880v1&entry.124074799=Read"},
{"title": "Neural Network Verification is a Programming Language Challenge", "author": "Lucas C. Cordeiro and Matthew L. Daggitt and Julien Girard-Satabin and Omri Isac and Taylor T. Johnson and Guy Katz and Ekaterina Komendantskaya and Augustin Lemesle and Edoardo Manino and Artjoms \u0160inkarovs and Haoze Wu", "abstract": "  Neural network verification is a new and rapidly developing field of\nresearch. So far, the main priority has been establishing efficient\nverification algorithms and tools, while proper support from the programming\nlanguage perspective has been considered secondary or unimportant. Yet, there\nis mounting evidence that insights from the programming language community may\nmake a difference in the future development of this domain. In this paper, we\nformulate neural network verification challenges as programming language\nchallenges and suggest possible future solutions.\n", "link": "http://arxiv.org/abs/2501.05867v1", "date": "2025-01-10", "relevancy": 2.1415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4385}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Network%20Verification%20is%20a%20Programming%20Language%20Challenge&body=Title%3A%20Neural%20Network%20Verification%20is%20a%20Programming%20Language%20Challenge%0AAuthor%3A%20Lucas%20C.%20Cordeiro%20and%20Matthew%20L.%20Daggitt%20and%20Julien%20Girard-Satabin%20and%20Omri%20Isac%20and%20Taylor%20T.%20Johnson%20and%20Guy%20Katz%20and%20Ekaterina%20Komendantskaya%20and%20Augustin%20Lemesle%20and%20Edoardo%20Manino%20and%20Artjoms%20%C5%A0inkarovs%20and%20Haoze%20Wu%0AAbstract%3A%20%20%20Neural%20network%20verification%20is%20a%20new%20and%20rapidly%20developing%20field%20of%0Aresearch.%20So%20far%2C%20the%20main%20priority%20has%20been%20establishing%20efficient%0Averification%20algorithms%20and%20tools%2C%20while%20proper%20support%20from%20the%20programming%0Alanguage%20perspective%20has%20been%20considered%20secondary%20or%20unimportant.%20Yet%2C%20there%0Ais%20mounting%20evidence%20that%20insights%20from%20the%20programming%20language%20community%20may%0Amake%20a%20difference%20in%20the%20future%20development%20of%20this%20domain.%20In%20this%20paper%2C%20we%0Aformulate%20neural%20network%20verification%20challenges%20as%20programming%20language%0Achallenges%20and%20suggest%20possible%20future%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Network%2520Verification%2520is%2520a%2520Programming%2520Language%2520Challenge%26entry.906535625%3DLucas%2520C.%2520Cordeiro%2520and%2520Matthew%2520L.%2520Daggitt%2520and%2520Julien%2520Girard-Satabin%2520and%2520Omri%2520Isac%2520and%2520Taylor%2520T.%2520Johnson%2520and%2520Guy%2520Katz%2520and%2520Ekaterina%2520Komendantskaya%2520and%2520Augustin%2520Lemesle%2520and%2520Edoardo%2520Manino%2520and%2520Artjoms%2520%25C5%25A0inkarovs%2520and%2520Haoze%2520Wu%26entry.1292438233%3D%2520%2520Neural%2520network%2520verification%2520is%2520a%2520new%2520and%2520rapidly%2520developing%2520field%2520of%250Aresearch.%2520So%2520far%252C%2520the%2520main%2520priority%2520has%2520been%2520establishing%2520efficient%250Averification%2520algorithms%2520and%2520tools%252C%2520while%2520proper%2520support%2520from%2520the%2520programming%250Alanguage%2520perspective%2520has%2520been%2520considered%2520secondary%2520or%2520unimportant.%2520Yet%252C%2520there%250Ais%2520mounting%2520evidence%2520that%2520insights%2520from%2520the%2520programming%2520language%2520community%2520may%250Amake%2520a%2520difference%2520in%2520the%2520future%2520development%2520of%2520this%2520domain.%2520In%2520this%2520paper%252C%2520we%250Aformulate%2520neural%2520network%2520verification%2520challenges%2520as%2520programming%2520language%250Achallenges%2520and%2520suggest%2520possible%2520future%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network%20Verification%20is%20a%20Programming%20Language%20Challenge&entry.906535625=Lucas%20C.%20Cordeiro%20and%20Matthew%20L.%20Daggitt%20and%20Julien%20Girard-Satabin%20and%20Omri%20Isac%20and%20Taylor%20T.%20Johnson%20and%20Guy%20Katz%20and%20Ekaterina%20Komendantskaya%20and%20Augustin%20Lemesle%20and%20Edoardo%20Manino%20and%20Artjoms%20%C5%A0inkarovs%20and%20Haoze%20Wu&entry.1292438233=%20%20Neural%20network%20verification%20is%20a%20new%20and%20rapidly%20developing%20field%20of%0Aresearch.%20So%20far%2C%20the%20main%20priority%20has%20been%20establishing%20efficient%0Averification%20algorithms%20and%20tools%2C%20while%20proper%20support%20from%20the%20programming%0Alanguage%20perspective%20has%20been%20considered%20secondary%20or%20unimportant.%20Yet%2C%20there%0Ais%20mounting%20evidence%20that%20insights%20from%20the%20programming%20language%20community%20may%0Amake%20a%20difference%20in%20the%20future%20development%20of%20this%20domain.%20In%20this%20paper%2C%20we%0Aformulate%20neural%20network%20verification%20challenges%20as%20programming%20language%0Achallenges%20and%20suggest%20possible%20future%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05867v1&entry.124074799=Read"},
{"title": "ViM-Disparity: Bridging the Gap of Speed, Accuracy and Memory for\n  Disparity Map Generation", "author": "Maheswar Bora and Tushar Anand and Saurabh Atreya and Aritra Mukherjee and Abhijit Das", "abstract": "  In this work we propose a Visual Mamba (ViM) based architecture, to dissolve\nthe existing trade-off for real-time and accurate model with low computation\noverhead for disparity map generation (DMG). Moreover, we proposed a\nperformance measure that can jointly evaluate the inference speed, computation\noverhead and the accurateness of a DMG model. The code implementation and\ncorresponding models are available at: https://github.com/MBora/ViM-Disparity.\n", "link": "http://arxiv.org/abs/2412.16745v2", "date": "2025-01-10", "relevancy": 2.1257, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5438}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViM-Disparity%3A%20Bridging%20the%20Gap%20of%20Speed%2C%20Accuracy%20and%20Memory%20for%0A%20%20Disparity%20Map%20Generation&body=Title%3A%20ViM-Disparity%3A%20Bridging%20the%20Gap%20of%20Speed%2C%20Accuracy%20and%20Memory%20for%0A%20%20Disparity%20Map%20Generation%0AAuthor%3A%20Maheswar%20Bora%20and%20Tushar%20Anand%20and%20Saurabh%20Atreya%20and%20Aritra%20Mukherjee%20and%20Abhijit%20Das%0AAbstract%3A%20%20%20In%20this%20work%20we%20propose%20a%20Visual%20Mamba%20%28ViM%29%20based%20architecture%2C%20to%20dissolve%0Athe%20existing%20trade-off%20for%20real-time%20and%20accurate%20model%20with%20low%20computation%0Aoverhead%20for%20disparity%20map%20generation%20%28DMG%29.%20Moreover%2C%20we%20proposed%20a%0Aperformance%20measure%20that%20can%20jointly%20evaluate%20the%20inference%20speed%2C%20computation%0Aoverhead%20and%20the%20accurateness%20of%20a%20DMG%20model.%20The%20code%20implementation%20and%0Acorresponding%20models%20are%20available%20at%3A%20https%3A//github.com/MBora/ViM-Disparity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViM-Disparity%253A%2520Bridging%2520the%2520Gap%2520of%2520Speed%252C%2520Accuracy%2520and%2520Memory%2520for%250A%2520%2520Disparity%2520Map%2520Generation%26entry.906535625%3DMaheswar%2520Bora%2520and%2520Tushar%2520Anand%2520and%2520Saurabh%2520Atreya%2520and%2520Aritra%2520Mukherjee%2520and%2520Abhijit%2520Das%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520propose%2520a%2520Visual%2520Mamba%2520%2528ViM%2529%2520based%2520architecture%252C%2520to%2520dissolve%250Athe%2520existing%2520trade-off%2520for%2520real-time%2520and%2520accurate%2520model%2520with%2520low%2520computation%250Aoverhead%2520for%2520disparity%2520map%2520generation%2520%2528DMG%2529.%2520Moreover%252C%2520we%2520proposed%2520a%250Aperformance%2520measure%2520that%2520can%2520jointly%2520evaluate%2520the%2520inference%2520speed%252C%2520computation%250Aoverhead%2520and%2520the%2520accurateness%2520of%2520a%2520DMG%2520model.%2520The%2520code%2520implementation%2520and%250Acorresponding%2520models%2520are%2520available%2520at%253A%2520https%253A//github.com/MBora/ViM-Disparity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViM-Disparity%3A%20Bridging%20the%20Gap%20of%20Speed%2C%20Accuracy%20and%20Memory%20for%0A%20%20Disparity%20Map%20Generation&entry.906535625=Maheswar%20Bora%20and%20Tushar%20Anand%20and%20Saurabh%20Atreya%20and%20Aritra%20Mukherjee%20and%20Abhijit%20Das&entry.1292438233=%20%20In%20this%20work%20we%20propose%20a%20Visual%20Mamba%20%28ViM%29%20based%20architecture%2C%20to%20dissolve%0Athe%20existing%20trade-off%20for%20real-time%20and%20accurate%20model%20with%20low%20computation%0Aoverhead%20for%20disparity%20map%20generation%20%28DMG%29.%20Moreover%2C%20we%20proposed%20a%0Aperformance%20measure%20that%20can%20jointly%20evaluate%20the%20inference%20speed%2C%20computation%0Aoverhead%20and%20the%20accurateness%20of%20a%20DMG%20model.%20The%20code%20implementation%20and%0Acorresponding%20models%20are%20available%20at%3A%20https%3A//github.com/MBora/ViM-Disparity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16745v2&entry.124074799=Read"},
{"title": "Theoretical Error Analysis of Entropy Approximation for Gaussian Mixture", "author": "Takashi Furuya and Hiroyuki Kusumoto and Koichi Taniguchi and Naoya Kanno and Kazuma Suetake", "abstract": "  Gaussian mixture distributions are commonly employed to represent general\nprobability distributions. Despite the importance of using Gaussian mixtures\nfor uncertainty estimation, the entropy of a Gaussian mixture cannot be\ncalculated analytically. In this paper, we study the approximate entropy\nrepresented as the sum of the entropies of unimodal Gaussian distributions with\nmixing coefficients. This approximation is easy to calculate analytically\nregardless of dimension, but there is a lack of theoretical guarantees. We\ntheoretically analyze the approximation error between the true and the\napproximate entropy to reveal when this approximation works effectively. This\nerror is essentially controlled by how far apart each Gaussian component of the\nGaussian mixture is. To measure such separation, we introduce the ratios of the\ndistances between the means to the sum of the variances of each Gaussian\ncomponent of the Gaussian mixture, and we reveal that the error converges to\nzero as the ratios tend to infinity. In addition, the probabilistic estimate\nindicates that this convergence situation is more likely to occur in\nhigher-dimensional spaces. Therefore, our results provide a guarantee that this\napproximation works well for high-dimensional problems, such as neural networks\nthat involve a large number of parameters.\n", "link": "http://arxiv.org/abs/2202.13059v5", "date": "2025-01-10", "relevancy": 2.1124, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4417}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4129}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theoretical%20Error%20Analysis%20of%20Entropy%20Approximation%20for%20Gaussian%20Mixture&body=Title%3A%20Theoretical%20Error%20Analysis%20of%20Entropy%20Approximation%20for%20Gaussian%20Mixture%0AAuthor%3A%20Takashi%20Furuya%20and%20Hiroyuki%20Kusumoto%20and%20Koichi%20Taniguchi%20and%20Naoya%20Kanno%20and%20Kazuma%20Suetake%0AAbstract%3A%20%20%20Gaussian%20mixture%20distributions%20are%20commonly%20employed%20to%20represent%20general%0Aprobability%20distributions.%20Despite%20the%20importance%20of%20using%20Gaussian%20mixtures%0Afor%20uncertainty%20estimation%2C%20the%20entropy%20of%20a%20Gaussian%20mixture%20cannot%20be%0Acalculated%20analytically.%20In%20this%20paper%2C%20we%20study%20the%20approximate%20entropy%0Arepresented%20as%20the%20sum%20of%20the%20entropies%20of%20unimodal%20Gaussian%20distributions%20with%0Amixing%20coefficients.%20This%20approximation%20is%20easy%20to%20calculate%20analytically%0Aregardless%20of%20dimension%2C%20but%20there%20is%20a%20lack%20of%20theoretical%20guarantees.%20We%0Atheoretically%20analyze%20the%20approximation%20error%20between%20the%20true%20and%20the%0Aapproximate%20entropy%20to%20reveal%20when%20this%20approximation%20works%20effectively.%20This%0Aerror%20is%20essentially%20controlled%20by%20how%20far%20apart%20each%20Gaussian%20component%20of%20the%0AGaussian%20mixture%20is.%20To%20measure%20such%20separation%2C%20we%20introduce%20the%20ratios%20of%20the%0Adistances%20between%20the%20means%20to%20the%20sum%20of%20the%20variances%20of%20each%20Gaussian%0Acomponent%20of%20the%20Gaussian%20mixture%2C%20and%20we%20reveal%20that%20the%20error%20converges%20to%0Azero%20as%20the%20ratios%20tend%20to%20infinity.%20In%20addition%2C%20the%20probabilistic%20estimate%0Aindicates%20that%20this%20convergence%20situation%20is%20more%20likely%20to%20occur%20in%0Ahigher-dimensional%20spaces.%20Therefore%2C%20our%20results%20provide%20a%20guarantee%20that%20this%0Aapproximation%20works%20well%20for%20high-dimensional%20problems%2C%20such%20as%20neural%20networks%0Athat%20involve%20a%20large%20number%20of%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.13059v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheoretical%2520Error%2520Analysis%2520of%2520Entropy%2520Approximation%2520for%2520Gaussian%2520Mixture%26entry.906535625%3DTakashi%2520Furuya%2520and%2520Hiroyuki%2520Kusumoto%2520and%2520Koichi%2520Taniguchi%2520and%2520Naoya%2520Kanno%2520and%2520Kazuma%2520Suetake%26entry.1292438233%3D%2520%2520Gaussian%2520mixture%2520distributions%2520are%2520commonly%2520employed%2520to%2520represent%2520general%250Aprobability%2520distributions.%2520Despite%2520the%2520importance%2520of%2520using%2520Gaussian%2520mixtures%250Afor%2520uncertainty%2520estimation%252C%2520the%2520entropy%2520of%2520a%2520Gaussian%2520mixture%2520cannot%2520be%250Acalculated%2520analytically.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520approximate%2520entropy%250Arepresented%2520as%2520the%2520sum%2520of%2520the%2520entropies%2520of%2520unimodal%2520Gaussian%2520distributions%2520with%250Amixing%2520coefficients.%2520This%2520approximation%2520is%2520easy%2520to%2520calculate%2520analytically%250Aregardless%2520of%2520dimension%252C%2520but%2520there%2520is%2520a%2520lack%2520of%2520theoretical%2520guarantees.%2520We%250Atheoretically%2520analyze%2520the%2520approximation%2520error%2520between%2520the%2520true%2520and%2520the%250Aapproximate%2520entropy%2520to%2520reveal%2520when%2520this%2520approximation%2520works%2520effectively.%2520This%250Aerror%2520is%2520essentially%2520controlled%2520by%2520how%2520far%2520apart%2520each%2520Gaussian%2520component%2520of%2520the%250AGaussian%2520mixture%2520is.%2520To%2520measure%2520such%2520separation%252C%2520we%2520introduce%2520the%2520ratios%2520of%2520the%250Adistances%2520between%2520the%2520means%2520to%2520the%2520sum%2520of%2520the%2520variances%2520of%2520each%2520Gaussian%250Acomponent%2520of%2520the%2520Gaussian%2520mixture%252C%2520and%2520we%2520reveal%2520that%2520the%2520error%2520converges%2520to%250Azero%2520as%2520the%2520ratios%2520tend%2520to%2520infinity.%2520In%2520addition%252C%2520the%2520probabilistic%2520estimate%250Aindicates%2520that%2520this%2520convergence%2520situation%2520is%2520more%2520likely%2520to%2520occur%2520in%250Ahigher-dimensional%2520spaces.%2520Therefore%252C%2520our%2520results%2520provide%2520a%2520guarantee%2520that%2520this%250Aapproximation%2520works%2520well%2520for%2520high-dimensional%2520problems%252C%2520such%2520as%2520neural%2520networks%250Athat%2520involve%2520a%2520large%2520number%2520of%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.13059v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theoretical%20Error%20Analysis%20of%20Entropy%20Approximation%20for%20Gaussian%20Mixture&entry.906535625=Takashi%20Furuya%20and%20Hiroyuki%20Kusumoto%20and%20Koichi%20Taniguchi%20and%20Naoya%20Kanno%20and%20Kazuma%20Suetake&entry.1292438233=%20%20Gaussian%20mixture%20distributions%20are%20commonly%20employed%20to%20represent%20general%0Aprobability%20distributions.%20Despite%20the%20importance%20of%20using%20Gaussian%20mixtures%0Afor%20uncertainty%20estimation%2C%20the%20entropy%20of%20a%20Gaussian%20mixture%20cannot%20be%0Acalculated%20analytically.%20In%20this%20paper%2C%20we%20study%20the%20approximate%20entropy%0Arepresented%20as%20the%20sum%20of%20the%20entropies%20of%20unimodal%20Gaussian%20distributions%20with%0Amixing%20coefficients.%20This%20approximation%20is%20easy%20to%20calculate%20analytically%0Aregardless%20of%20dimension%2C%20but%20there%20is%20a%20lack%20of%20theoretical%20guarantees.%20We%0Atheoretically%20analyze%20the%20approximation%20error%20between%20the%20true%20and%20the%0Aapproximate%20entropy%20to%20reveal%20when%20this%20approximation%20works%20effectively.%20This%0Aerror%20is%20essentially%20controlled%20by%20how%20far%20apart%20each%20Gaussian%20component%20of%20the%0AGaussian%20mixture%20is.%20To%20measure%20such%20separation%2C%20we%20introduce%20the%20ratios%20of%20the%0Adistances%20between%20the%20means%20to%20the%20sum%20of%20the%20variances%20of%20each%20Gaussian%0Acomponent%20of%20the%20Gaussian%20mixture%2C%20and%20we%20reveal%20that%20the%20error%20converges%20to%0Azero%20as%20the%20ratios%20tend%20to%20infinity.%20In%20addition%2C%20the%20probabilistic%20estimate%0Aindicates%20that%20this%20convergence%20situation%20is%20more%20likely%20to%20occur%20in%0Ahigher-dimensional%20spaces.%20Therefore%2C%20our%20results%20provide%20a%20guarantee%20that%20this%0Aapproximation%20works%20well%20for%20high-dimensional%20problems%2C%20such%20as%20neural%20networks%0Athat%20involve%20a%20large%20number%20of%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.13059v5&entry.124074799=Read"},
{"title": "Geometric-Based Nail Segmentation for Clinical Measurements", "author": "Bernat Galm\u00e9s and Gabriel Moy\u00e0-Alcover and Pedro Bibiloni and Javier Varona and Antoni Jaume-i-Cap\u00f3", "abstract": "  A robust segmentation method that can be used to perform measurements on\ntoenails is presented. The proposed method is used as the first step in a\nclinical trial to objectively quantify the incidence of a particular pathology.\nFor such an assessment, it is necessary to distinguish a nail, which locally\nappears to be similar to the skin. Many algorithms have been used, each of\nwhich leverages different aspects of toenail appearance. We used the Hough\ntransform to locate the tip of the toe and estimate the nail location and size.\nSubsequently, we classified the super-pixels of the image based on their\ngeometric and photometric information. Thereafter, the watershed transform\ndelineated the border of the nail. The method was validated using a 348-image\nmedical dataset, achieving an accuracy of 0.993 and an F-measure of 0.925. The\nproposed method is considerably robust across samples, with respect to factors\nsuch as nail shape, skin pigmentation, illumination conditions, and appearance\nof large regions affected by a medical condition\n", "link": "http://arxiv.org/abs/2501.06027v1", "date": "2025-01-10", "relevancy": 2.1114, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4373}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4154}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric-Based%20Nail%20Segmentation%20for%20Clinical%20Measurements&body=Title%3A%20Geometric-Based%20Nail%20Segmentation%20for%20Clinical%20Measurements%0AAuthor%3A%20Bernat%20Galm%C3%A9s%20and%20Gabriel%20Moy%C3%A0-Alcover%20and%20Pedro%20Bibiloni%20and%20Javier%20Varona%20and%20Antoni%20Jaume-i-Cap%C3%B3%0AAbstract%3A%20%20%20A%20robust%20segmentation%20method%20that%20can%20be%20used%20to%20perform%20measurements%20on%0Atoenails%20is%20presented.%20The%20proposed%20method%20is%20used%20as%20the%20first%20step%20in%20a%0Aclinical%20trial%20to%20objectively%20quantify%20the%20incidence%20of%20a%20particular%20pathology.%0AFor%20such%20an%20assessment%2C%20it%20is%20necessary%20to%20distinguish%20a%20nail%2C%20which%20locally%0Aappears%20to%20be%20similar%20to%20the%20skin.%20Many%20algorithms%20have%20been%20used%2C%20each%20of%0Awhich%20leverages%20different%20aspects%20of%20toenail%20appearance.%20We%20used%20the%20Hough%0Atransform%20to%20locate%20the%20tip%20of%20the%20toe%20and%20estimate%20the%20nail%20location%20and%20size.%0ASubsequently%2C%20we%20classified%20the%20super-pixels%20of%20the%20image%20based%20on%20their%0Ageometric%20and%20photometric%20information.%20Thereafter%2C%20the%20watershed%20transform%0Adelineated%20the%20border%20of%20the%20nail.%20The%20method%20was%20validated%20using%20a%20348-image%0Amedical%20dataset%2C%20achieving%20an%20accuracy%20of%200.993%20and%20an%20F-measure%20of%200.925.%20The%0Aproposed%20method%20is%20considerably%20robust%20across%20samples%2C%20with%20respect%20to%20factors%0Asuch%20as%20nail%20shape%2C%20skin%20pigmentation%2C%20illumination%20conditions%2C%20and%20appearance%0Aof%20large%20regions%20affected%20by%20a%20medical%20condition%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric-Based%2520Nail%2520Segmentation%2520for%2520Clinical%2520Measurements%26entry.906535625%3DBernat%2520Galm%25C3%25A9s%2520and%2520Gabriel%2520Moy%25C3%25A0-Alcover%2520and%2520Pedro%2520Bibiloni%2520and%2520Javier%2520Varona%2520and%2520Antoni%2520Jaume-i-Cap%25C3%25B3%26entry.1292438233%3D%2520%2520A%2520robust%2520segmentation%2520method%2520that%2520can%2520be%2520used%2520to%2520perform%2520measurements%2520on%250Atoenails%2520is%2520presented.%2520The%2520proposed%2520method%2520is%2520used%2520as%2520the%2520first%2520step%2520in%2520a%250Aclinical%2520trial%2520to%2520objectively%2520quantify%2520the%2520incidence%2520of%2520a%2520particular%2520pathology.%250AFor%2520such%2520an%2520assessment%252C%2520it%2520is%2520necessary%2520to%2520distinguish%2520a%2520nail%252C%2520which%2520locally%250Aappears%2520to%2520be%2520similar%2520to%2520the%2520skin.%2520Many%2520algorithms%2520have%2520been%2520used%252C%2520each%2520of%250Awhich%2520leverages%2520different%2520aspects%2520of%2520toenail%2520appearance.%2520We%2520used%2520the%2520Hough%250Atransform%2520to%2520locate%2520the%2520tip%2520of%2520the%2520toe%2520and%2520estimate%2520the%2520nail%2520location%2520and%2520size.%250ASubsequently%252C%2520we%2520classified%2520the%2520super-pixels%2520of%2520the%2520image%2520based%2520on%2520their%250Ageometric%2520and%2520photometric%2520information.%2520Thereafter%252C%2520the%2520watershed%2520transform%250Adelineated%2520the%2520border%2520of%2520the%2520nail.%2520The%2520method%2520was%2520validated%2520using%2520a%2520348-image%250Amedical%2520dataset%252C%2520achieving%2520an%2520accuracy%2520of%25200.993%2520and%2520an%2520F-measure%2520of%25200.925.%2520The%250Aproposed%2520method%2520is%2520considerably%2520robust%2520across%2520samples%252C%2520with%2520respect%2520to%2520factors%250Asuch%2520as%2520nail%2520shape%252C%2520skin%2520pigmentation%252C%2520illumination%2520conditions%252C%2520and%2520appearance%250Aof%2520large%2520regions%2520affected%2520by%2520a%2520medical%2520condition%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric-Based%20Nail%20Segmentation%20for%20Clinical%20Measurements&entry.906535625=Bernat%20Galm%C3%A9s%20and%20Gabriel%20Moy%C3%A0-Alcover%20and%20Pedro%20Bibiloni%20and%20Javier%20Varona%20and%20Antoni%20Jaume-i-Cap%C3%B3&entry.1292438233=%20%20A%20robust%20segmentation%20method%20that%20can%20be%20used%20to%20perform%20measurements%20on%0Atoenails%20is%20presented.%20The%20proposed%20method%20is%20used%20as%20the%20first%20step%20in%20a%0Aclinical%20trial%20to%20objectively%20quantify%20the%20incidence%20of%20a%20particular%20pathology.%0AFor%20such%20an%20assessment%2C%20it%20is%20necessary%20to%20distinguish%20a%20nail%2C%20which%20locally%0Aappears%20to%20be%20similar%20to%20the%20skin.%20Many%20algorithms%20have%20been%20used%2C%20each%20of%0Awhich%20leverages%20different%20aspects%20of%20toenail%20appearance.%20We%20used%20the%20Hough%0Atransform%20to%20locate%20the%20tip%20of%20the%20toe%20and%20estimate%20the%20nail%20location%20and%20size.%0ASubsequently%2C%20we%20classified%20the%20super-pixels%20of%20the%20image%20based%20on%20their%0Ageometric%20and%20photometric%20information.%20Thereafter%2C%20the%20watershed%20transform%0Adelineated%20the%20border%20of%20the%20nail.%20The%20method%20was%20validated%20using%20a%20348-image%0Amedical%20dataset%2C%20achieving%20an%20accuracy%20of%200.993%20and%20an%20F-measure%20of%200.925.%20The%0Aproposed%20method%20is%20considerably%20robust%20across%20samples%2C%20with%20respect%20to%20factors%0Asuch%20as%20nail%20shape%2C%20skin%20pigmentation%2C%20illumination%20conditions%2C%20and%20appearance%0Aof%20large%20regions%20affected%20by%20a%20medical%20condition%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06027v1&entry.124074799=Read"},
{"title": "CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion\n  Planning for Future Autonomous Mobility on Demand Systems", "author": "Haichao Liu and Ruoyu Yao and Wenru Liu and Zhenmin Huang and Shaojie Shen and Jun Ma", "abstract": "  The increasing demand for flexible and efficient urban transportation\nsolutions has spotlighted the limitations of traditional Demand Responsive\nTransport (DRT) systems, particularly in accommodating diverse passenger needs\nand dynamic urban environments. Autonomous Mobility-on-Demand (AMoD) systems\nhave emerged as a promising alternative, leveraging connected and autonomous\nvehicles (CAVs) to provide responsive and adaptable services. However, existing\nmethods primarily focus on either vehicle scheduling or path planning, which\noften simplify complex urban layouts and neglect the necessity for simultaneous\ncoordination and mutual avoidance among CAVs. This oversimplification poses\nsignificant challenges to the deployment of AMoD systems in real-world\nscenarios. To address these gaps, we propose CoDriveVLM, a novel framework that\nintegrates high-fidelity simultaneous dispatching and cooperative motion\nplanning for future AMoD systems. Our method harnesses Vision-Language Models\n(VLMs) to enhance multi-modality information processing, and this enables\ncomprehensive dispatching and collision risk evaluation. The VLM-enhanced CAV\ndispatching coordinator is introduced to effectively manage complex and\nunforeseen AMoD conditions, thus supporting efficient scheduling\ndecision-making. Furthermore, we propose a scalable decentralized cooperative\nmotion planning method via consensus alternating direction method of\nmultipliers (ADMM) focusing on collision risk evaluation and decentralized\ntrajectory optimization. Simulation results demonstrate the feasibility and\nrobustness of CoDriveVLM in various traffic conditions, showcasing its\npotential to significantly improve the fidelity and effectiveness of AMoD\nsystems in future urban transportation networks. The code is available at\nhttps://github.com/henryhcliu/CoDriveVLM.git.\n", "link": "http://arxiv.org/abs/2501.06132v1", "date": "2025-01-10", "relevancy": 2.1092, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5258}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoDriveVLM%3A%20VLM-Enhanced%20Urban%20Cooperative%20Dispatching%20and%20Motion%0A%20%20Planning%20for%20Future%20Autonomous%20Mobility%20on%20Demand%20Systems&body=Title%3A%20CoDriveVLM%3A%20VLM-Enhanced%20Urban%20Cooperative%20Dispatching%20and%20Motion%0A%20%20Planning%20for%20Future%20Autonomous%20Mobility%20on%20Demand%20Systems%0AAuthor%3A%20Haichao%20Liu%20and%20Ruoyu%20Yao%20and%20Wenru%20Liu%20and%20Zhenmin%20Huang%20and%20Shaojie%20Shen%20and%20Jun%20Ma%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20flexible%20and%20efficient%20urban%20transportation%0Asolutions%20has%20spotlighted%20the%20limitations%20of%20traditional%20Demand%20Responsive%0ATransport%20%28DRT%29%20systems%2C%20particularly%20in%20accommodating%20diverse%20passenger%20needs%0Aand%20dynamic%20urban%20environments.%20Autonomous%20Mobility-on-Demand%20%28AMoD%29%20systems%0Ahave%20emerged%20as%20a%20promising%20alternative%2C%20leveraging%20connected%20and%20autonomous%0Avehicles%20%28CAVs%29%20to%20provide%20responsive%20and%20adaptable%20services.%20However%2C%20existing%0Amethods%20primarily%20focus%20on%20either%20vehicle%20scheduling%20or%20path%20planning%2C%20which%0Aoften%20simplify%20complex%20urban%20layouts%20and%20neglect%20the%20necessity%20for%20simultaneous%0Acoordination%20and%20mutual%20avoidance%20among%20CAVs.%20This%20oversimplification%20poses%0Asignificant%20challenges%20to%20the%20deployment%20of%20AMoD%20systems%20in%20real-world%0Ascenarios.%20To%20address%20these%20gaps%2C%20we%20propose%20CoDriveVLM%2C%20a%20novel%20framework%20that%0Aintegrates%20high-fidelity%20simultaneous%20dispatching%20and%20cooperative%20motion%0Aplanning%20for%20future%20AMoD%20systems.%20Our%20method%20harnesses%20Vision-Language%20Models%0A%28VLMs%29%20to%20enhance%20multi-modality%20information%20processing%2C%20and%20this%20enables%0Acomprehensive%20dispatching%20and%20collision%20risk%20evaluation.%20The%20VLM-enhanced%20CAV%0Adispatching%20coordinator%20is%20introduced%20to%20effectively%20manage%20complex%20and%0Aunforeseen%20AMoD%20conditions%2C%20thus%20supporting%20efficient%20scheduling%0Adecision-making.%20Furthermore%2C%20we%20propose%20a%20scalable%20decentralized%20cooperative%0Amotion%20planning%20method%20via%20consensus%20alternating%20direction%20method%20of%0Amultipliers%20%28ADMM%29%20focusing%20on%20collision%20risk%20evaluation%20and%20decentralized%0Atrajectory%20optimization.%20Simulation%20results%20demonstrate%20the%20feasibility%20and%0Arobustness%20of%20CoDriveVLM%20in%20various%20traffic%20conditions%2C%20showcasing%20its%0Apotential%20to%20significantly%20improve%20the%20fidelity%20and%20effectiveness%20of%20AMoD%0Asystems%20in%20future%20urban%20transportation%20networks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/henryhcliu/CoDriveVLM.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoDriveVLM%253A%2520VLM-Enhanced%2520Urban%2520Cooperative%2520Dispatching%2520and%2520Motion%250A%2520%2520Planning%2520for%2520Future%2520Autonomous%2520Mobility%2520on%2520Demand%2520Systems%26entry.906535625%3DHaichao%2520Liu%2520and%2520Ruoyu%2520Yao%2520and%2520Wenru%2520Liu%2520and%2520Zhenmin%2520Huang%2520and%2520Shaojie%2520Shen%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520The%2520increasing%2520demand%2520for%2520flexible%2520and%2520efficient%2520urban%2520transportation%250Asolutions%2520has%2520spotlighted%2520the%2520limitations%2520of%2520traditional%2520Demand%2520Responsive%250ATransport%2520%2528DRT%2529%2520systems%252C%2520particularly%2520in%2520accommodating%2520diverse%2520passenger%2520needs%250Aand%2520dynamic%2520urban%2520environments.%2520Autonomous%2520Mobility-on-Demand%2520%2528AMoD%2529%2520systems%250Ahave%2520emerged%2520as%2520a%2520promising%2520alternative%252C%2520leveraging%2520connected%2520and%2520autonomous%250Avehicles%2520%2528CAVs%2529%2520to%2520provide%2520responsive%2520and%2520adaptable%2520services.%2520However%252C%2520existing%250Amethods%2520primarily%2520focus%2520on%2520either%2520vehicle%2520scheduling%2520or%2520path%2520planning%252C%2520which%250Aoften%2520simplify%2520complex%2520urban%2520layouts%2520and%2520neglect%2520the%2520necessity%2520for%2520simultaneous%250Acoordination%2520and%2520mutual%2520avoidance%2520among%2520CAVs.%2520This%2520oversimplification%2520poses%250Asignificant%2520challenges%2520to%2520the%2520deployment%2520of%2520AMoD%2520systems%2520in%2520real-world%250Ascenarios.%2520To%2520address%2520these%2520gaps%252C%2520we%2520propose%2520CoDriveVLM%252C%2520a%2520novel%2520framework%2520that%250Aintegrates%2520high-fidelity%2520simultaneous%2520dispatching%2520and%2520cooperative%2520motion%250Aplanning%2520for%2520future%2520AMoD%2520systems.%2520Our%2520method%2520harnesses%2520Vision-Language%2520Models%250A%2528VLMs%2529%2520to%2520enhance%2520multi-modality%2520information%2520processing%252C%2520and%2520this%2520enables%250Acomprehensive%2520dispatching%2520and%2520collision%2520risk%2520evaluation.%2520The%2520VLM-enhanced%2520CAV%250Adispatching%2520coordinator%2520is%2520introduced%2520to%2520effectively%2520manage%2520complex%2520and%250Aunforeseen%2520AMoD%2520conditions%252C%2520thus%2520supporting%2520efficient%2520scheduling%250Adecision-making.%2520Furthermore%252C%2520we%2520propose%2520a%2520scalable%2520decentralized%2520cooperative%250Amotion%2520planning%2520method%2520via%2520consensus%2520alternating%2520direction%2520method%2520of%250Amultipliers%2520%2528ADMM%2529%2520focusing%2520on%2520collision%2520risk%2520evaluation%2520and%2520decentralized%250Atrajectory%2520optimization.%2520Simulation%2520results%2520demonstrate%2520the%2520feasibility%2520and%250Arobustness%2520of%2520CoDriveVLM%2520in%2520various%2520traffic%2520conditions%252C%2520showcasing%2520its%250Apotential%2520to%2520significantly%2520improve%2520the%2520fidelity%2520and%2520effectiveness%2520of%2520AMoD%250Asystems%2520in%2520future%2520urban%2520transportation%2520networks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/henryhcliu/CoDriveVLM.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoDriveVLM%3A%20VLM-Enhanced%20Urban%20Cooperative%20Dispatching%20and%20Motion%0A%20%20Planning%20for%20Future%20Autonomous%20Mobility%20on%20Demand%20Systems&entry.906535625=Haichao%20Liu%20and%20Ruoyu%20Yao%20and%20Wenru%20Liu%20and%20Zhenmin%20Huang%20and%20Shaojie%20Shen%20and%20Jun%20Ma&entry.1292438233=%20%20The%20increasing%20demand%20for%20flexible%20and%20efficient%20urban%20transportation%0Asolutions%20has%20spotlighted%20the%20limitations%20of%20traditional%20Demand%20Responsive%0ATransport%20%28DRT%29%20systems%2C%20particularly%20in%20accommodating%20diverse%20passenger%20needs%0Aand%20dynamic%20urban%20environments.%20Autonomous%20Mobility-on-Demand%20%28AMoD%29%20systems%0Ahave%20emerged%20as%20a%20promising%20alternative%2C%20leveraging%20connected%20and%20autonomous%0Avehicles%20%28CAVs%29%20to%20provide%20responsive%20and%20adaptable%20services.%20However%2C%20existing%0Amethods%20primarily%20focus%20on%20either%20vehicle%20scheduling%20or%20path%20planning%2C%20which%0Aoften%20simplify%20complex%20urban%20layouts%20and%20neglect%20the%20necessity%20for%20simultaneous%0Acoordination%20and%20mutual%20avoidance%20among%20CAVs.%20This%20oversimplification%20poses%0Asignificant%20challenges%20to%20the%20deployment%20of%20AMoD%20systems%20in%20real-world%0Ascenarios.%20To%20address%20these%20gaps%2C%20we%20propose%20CoDriveVLM%2C%20a%20novel%20framework%20that%0Aintegrates%20high-fidelity%20simultaneous%20dispatching%20and%20cooperative%20motion%0Aplanning%20for%20future%20AMoD%20systems.%20Our%20method%20harnesses%20Vision-Language%20Models%0A%28VLMs%29%20to%20enhance%20multi-modality%20information%20processing%2C%20and%20this%20enables%0Acomprehensive%20dispatching%20and%20collision%20risk%20evaluation.%20The%20VLM-enhanced%20CAV%0Adispatching%20coordinator%20is%20introduced%20to%20effectively%20manage%20complex%20and%0Aunforeseen%20AMoD%20conditions%2C%20thus%20supporting%20efficient%20scheduling%0Adecision-making.%20Furthermore%2C%20we%20propose%20a%20scalable%20decentralized%20cooperative%0Amotion%20planning%20method%20via%20consensus%20alternating%20direction%20method%20of%0Amultipliers%20%28ADMM%29%20focusing%20on%20collision%20risk%20evaluation%20and%20decentralized%0Atrajectory%20optimization.%20Simulation%20results%20demonstrate%20the%20feasibility%20and%0Arobustness%20of%20CoDriveVLM%20in%20various%20traffic%20conditions%2C%20showcasing%20its%0Apotential%20to%20significantly%20improve%20the%20fidelity%20and%20effectiveness%20of%20AMoD%0Asystems%20in%20future%20urban%20transportation%20networks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/henryhcliu/CoDriveVLM.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06132v1&entry.124074799=Read"},
{"title": "DeltaGNN: Graph Neural Network with Information Flow Control", "author": "Kevin Mancini and Islem Rekik", "abstract": "  Graph Neural Networks (GNNs) are popular deep learning models designed to\nprocess graph-structured data through recursive neighborhood aggregations in\nthe message passing process. When applied to semi-supervised node\nclassification, the message-passing enables GNNs to understand short-range\nspatial interactions, but also causes them to suffer from over-smoothing and\nover-squashing. These challenges hinder model expressiveness and prevent the\nuse of deeper models to capture long-range node interactions (LRIs) within the\ngraph. Popular solutions for LRIs detection are either too expensive to process\nlarge graphs due to high time complexity or fail to generalize across diverse\ngraph structures. To address these limitations, we propose a mechanism called\n\\emph{information flow control}, which leverages a novel connectivity measure,\ncalled \\emph{information flow score}, to address over-smoothing and\nover-squashing with linear computational overhead, supported by theoretical\nevidence. Finally, to prove the efficacy of our methodology we design DeltaGNN,\nthe first scalable and generalizable approach for detecting long-range and\nshort-range interactions. We benchmark our model across 10 real-world datasets,\nincluding graphs with varying sizes, topologies, densities, and homophilic\nratios, showing superior performance with limited computational complexity. The\nimplementation of the proposed methods are publicly available at\nhttps://github.com/basiralab/DeltaGNN.\n", "link": "http://arxiv.org/abs/2501.06002v1", "date": "2025-01-10", "relevancy": 2.1066, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5384}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5344}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeltaGNN%3A%20Graph%20Neural%20Network%20with%20Information%20Flow%20Control&body=Title%3A%20DeltaGNN%3A%20Graph%20Neural%20Network%20with%20Information%20Flow%20Control%0AAuthor%3A%20Kevin%20Mancini%20and%20Islem%20Rekik%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20popular%20deep%20learning%20models%20designed%20to%0Aprocess%20graph-structured%20data%20through%20recursive%20neighborhood%20aggregations%20in%0Athe%20message%20passing%20process.%20When%20applied%20to%20semi-supervised%20node%0Aclassification%2C%20the%20message-passing%20enables%20GNNs%20to%20understand%20short-range%0Aspatial%20interactions%2C%20but%20also%20causes%20them%20to%20suffer%20from%20over-smoothing%20and%0Aover-squashing.%20These%20challenges%20hinder%20model%20expressiveness%20and%20prevent%20the%0Ause%20of%20deeper%20models%20to%20capture%20long-range%20node%20interactions%20%28LRIs%29%20within%20the%0Agraph.%20Popular%20solutions%20for%20LRIs%20detection%20are%20either%20too%20expensive%20to%20process%0Alarge%20graphs%20due%20to%20high%20time%20complexity%20or%20fail%20to%20generalize%20across%20diverse%0Agraph%20structures.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20mechanism%20called%0A%5Cemph%7Binformation%20flow%20control%7D%2C%20which%20leverages%20a%20novel%20connectivity%20measure%2C%0Acalled%20%5Cemph%7Binformation%20flow%20score%7D%2C%20to%20address%20over-smoothing%20and%0Aover-squashing%20with%20linear%20computational%20overhead%2C%20supported%20by%20theoretical%0Aevidence.%20Finally%2C%20to%20prove%20the%20efficacy%20of%20our%20methodology%20we%20design%20DeltaGNN%2C%0Athe%20first%20scalable%20and%20generalizable%20approach%20for%20detecting%20long-range%20and%0Ashort-range%20interactions.%20We%20benchmark%20our%20model%20across%2010%20real-world%20datasets%2C%0Aincluding%20graphs%20with%20varying%20sizes%2C%20topologies%2C%20densities%2C%20and%20homophilic%0Aratios%2C%20showing%20superior%20performance%20with%20limited%20computational%20complexity.%20The%0Aimplementation%20of%20the%20proposed%20methods%20are%20publicly%20available%20at%0Ahttps%3A//github.com/basiralab/DeltaGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeltaGNN%253A%2520Graph%2520Neural%2520Network%2520with%2520Information%2520Flow%2520Control%26entry.906535625%3DKevin%2520Mancini%2520and%2520Islem%2520Rekik%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520popular%2520deep%2520learning%2520models%2520designed%2520to%250Aprocess%2520graph-structured%2520data%2520through%2520recursive%2520neighborhood%2520aggregations%2520in%250Athe%2520message%2520passing%2520process.%2520When%2520applied%2520to%2520semi-supervised%2520node%250Aclassification%252C%2520the%2520message-passing%2520enables%2520GNNs%2520to%2520understand%2520short-range%250Aspatial%2520interactions%252C%2520but%2520also%2520causes%2520them%2520to%2520suffer%2520from%2520over-smoothing%2520and%250Aover-squashing.%2520These%2520challenges%2520hinder%2520model%2520expressiveness%2520and%2520prevent%2520the%250Ause%2520of%2520deeper%2520models%2520to%2520capture%2520long-range%2520node%2520interactions%2520%2528LRIs%2529%2520within%2520the%250Agraph.%2520Popular%2520solutions%2520for%2520LRIs%2520detection%2520are%2520either%2520too%2520expensive%2520to%2520process%250Alarge%2520graphs%2520due%2520to%2520high%2520time%2520complexity%2520or%2520fail%2520to%2520generalize%2520across%2520diverse%250Agraph%2520structures.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520mechanism%2520called%250A%255Cemph%257Binformation%2520flow%2520control%257D%252C%2520which%2520leverages%2520a%2520novel%2520connectivity%2520measure%252C%250Acalled%2520%255Cemph%257Binformation%2520flow%2520score%257D%252C%2520to%2520address%2520over-smoothing%2520and%250Aover-squashing%2520with%2520linear%2520computational%2520overhead%252C%2520supported%2520by%2520theoretical%250Aevidence.%2520Finally%252C%2520to%2520prove%2520the%2520efficacy%2520of%2520our%2520methodology%2520we%2520design%2520DeltaGNN%252C%250Athe%2520first%2520scalable%2520and%2520generalizable%2520approach%2520for%2520detecting%2520long-range%2520and%250Ashort-range%2520interactions.%2520We%2520benchmark%2520our%2520model%2520across%252010%2520real-world%2520datasets%252C%250Aincluding%2520graphs%2520with%2520varying%2520sizes%252C%2520topologies%252C%2520densities%252C%2520and%2520homophilic%250Aratios%252C%2520showing%2520superior%2520performance%2520with%2520limited%2520computational%2520complexity.%2520The%250Aimplementation%2520of%2520the%2520proposed%2520methods%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/basiralab/DeltaGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeltaGNN%3A%20Graph%20Neural%20Network%20with%20Information%20Flow%20Control&entry.906535625=Kevin%20Mancini%20and%20Islem%20Rekik&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20popular%20deep%20learning%20models%20designed%20to%0Aprocess%20graph-structured%20data%20through%20recursive%20neighborhood%20aggregations%20in%0Athe%20message%20passing%20process.%20When%20applied%20to%20semi-supervised%20node%0Aclassification%2C%20the%20message-passing%20enables%20GNNs%20to%20understand%20short-range%0Aspatial%20interactions%2C%20but%20also%20causes%20them%20to%20suffer%20from%20over-smoothing%20and%0Aover-squashing.%20These%20challenges%20hinder%20model%20expressiveness%20and%20prevent%20the%0Ause%20of%20deeper%20models%20to%20capture%20long-range%20node%20interactions%20%28LRIs%29%20within%20the%0Agraph.%20Popular%20solutions%20for%20LRIs%20detection%20are%20either%20too%20expensive%20to%20process%0Alarge%20graphs%20due%20to%20high%20time%20complexity%20or%20fail%20to%20generalize%20across%20diverse%0Agraph%20structures.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20mechanism%20called%0A%5Cemph%7Binformation%20flow%20control%7D%2C%20which%20leverages%20a%20novel%20connectivity%20measure%2C%0Acalled%20%5Cemph%7Binformation%20flow%20score%7D%2C%20to%20address%20over-smoothing%20and%0Aover-squashing%20with%20linear%20computational%20overhead%2C%20supported%20by%20theoretical%0Aevidence.%20Finally%2C%20to%20prove%20the%20efficacy%20of%20our%20methodology%20we%20design%20DeltaGNN%2C%0Athe%20first%20scalable%20and%20generalizable%20approach%20for%20detecting%20long-range%20and%0Ashort-range%20interactions.%20We%20benchmark%20our%20model%20across%2010%20real-world%20datasets%2C%0Aincluding%20graphs%20with%20varying%20sizes%2C%20topologies%2C%20densities%2C%20and%20homophilic%0Aratios%2C%20showing%20superior%20performance%20with%20limited%20computational%20complexity.%20The%0Aimplementation%20of%20the%20proposed%20methods%20are%20publicly%20available%20at%0Ahttps%3A//github.com/basiralab/DeltaGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06002v1&entry.124074799=Read"},
{"title": "GenMol: A Drug Discovery Generalist with Discrete Diffusion", "author": "Seul Lee and Karsten Kreis and Srimukh Prasad Veccham and Meng Liu and Danny Reidenbach and Yuxing Peng and Saee Paliwal and Weili Nie and Arash Vahdat", "abstract": "  Drug discovery is a complex process that involves multiple scenarios and\nstages, such as fragment-constrained molecule generation, hit generation and\nlead optimization. However, existing molecular generative models can only\ntackle one or two of these scenarios and lack the flexibility to address\nvarious aspects of the drug discovery pipeline. In this paper, we present\nGeneralist Molecular generative model (GenMol), a versatile framework that\naddresses these limitations by applying discrete diffusion to the Sequential\nAttachment-based Fragment Embedding (SAFE) molecular representation. GenMol\ngenerates SAFE sequences through non-autoregressive bidirectional parallel\ndecoding, thereby allowing utilization of a molecular context that does not\nrely on the specific token ordering and enhanced computational efficiency.\nMoreover, under the discrete diffusion framework, we introduce fragment\nremasking, a strategy that optimizes molecules by replacing fragments with\nmasked tokens and regenerating them, enabling effective exploration of chemical\nspace. GenMol significantly outperforms the previous GPT-based model trained on\nSAFE representations in de novo generation and fragment-constrained generation,\nand achieves state-of-the-art performance in goal-directed hit generation and\nlead optimization. These experimental results demonstrate that GenMol can\ntackle a wide range of drug discovery tasks, providing a unified and versatile\napproach for molecular design.\n", "link": "http://arxiv.org/abs/2501.06158v1", "date": "2025-01-10", "relevancy": 2.0867, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5323}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5239}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenMol%3A%20A%20Drug%20Discovery%20Generalist%20with%20Discrete%20Diffusion&body=Title%3A%20GenMol%3A%20A%20Drug%20Discovery%20Generalist%20with%20Discrete%20Diffusion%0AAuthor%3A%20Seul%20Lee%20and%20Karsten%20Kreis%20and%20Srimukh%20Prasad%20Veccham%20and%20Meng%20Liu%20and%20Danny%20Reidenbach%20and%20Yuxing%20Peng%20and%20Saee%20Paliwal%20and%20Weili%20Nie%20and%20Arash%20Vahdat%0AAbstract%3A%20%20%20Drug%20discovery%20is%20a%20complex%20process%20that%20involves%20multiple%20scenarios%20and%0Astages%2C%20such%20as%20fragment-constrained%20molecule%20generation%2C%20hit%20generation%20and%0Alead%20optimization.%20However%2C%20existing%20molecular%20generative%20models%20can%20only%0Atackle%20one%20or%20two%20of%20these%20scenarios%20and%20lack%20the%20flexibility%20to%20address%0Avarious%20aspects%20of%20the%20drug%20discovery%20pipeline.%20In%20this%20paper%2C%20we%20present%0AGeneralist%20Molecular%20generative%20model%20%28GenMol%29%2C%20a%20versatile%20framework%20that%0Aaddresses%20these%20limitations%20by%20applying%20discrete%20diffusion%20to%20the%20Sequential%0AAttachment-based%20Fragment%20Embedding%20%28SAFE%29%20molecular%20representation.%20GenMol%0Agenerates%20SAFE%20sequences%20through%20non-autoregressive%20bidirectional%20parallel%0Adecoding%2C%20thereby%20allowing%20utilization%20of%20a%20molecular%20context%20that%20does%20not%0Arely%20on%20the%20specific%20token%20ordering%20and%20enhanced%20computational%20efficiency.%0AMoreover%2C%20under%20the%20discrete%20diffusion%20framework%2C%20we%20introduce%20fragment%0Aremasking%2C%20a%20strategy%20that%20optimizes%20molecules%20by%20replacing%20fragments%20with%0Amasked%20tokens%20and%20regenerating%20them%2C%20enabling%20effective%20exploration%20of%20chemical%0Aspace.%20GenMol%20significantly%20outperforms%20the%20previous%20GPT-based%20model%20trained%20on%0ASAFE%20representations%20in%20de%20novo%20generation%20and%20fragment-constrained%20generation%2C%0Aand%20achieves%20state-of-the-art%20performance%20in%20goal-directed%20hit%20generation%20and%0Alead%20optimization.%20These%20experimental%20results%20demonstrate%20that%20GenMol%20can%0Atackle%20a%20wide%20range%20of%20drug%20discovery%20tasks%2C%20providing%20a%20unified%20and%20versatile%0Aapproach%20for%20molecular%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenMol%253A%2520A%2520Drug%2520Discovery%2520Generalist%2520with%2520Discrete%2520Diffusion%26entry.906535625%3DSeul%2520Lee%2520and%2520Karsten%2520Kreis%2520and%2520Srimukh%2520Prasad%2520Veccham%2520and%2520Meng%2520Liu%2520and%2520Danny%2520Reidenbach%2520and%2520Yuxing%2520Peng%2520and%2520Saee%2520Paliwal%2520and%2520Weili%2520Nie%2520and%2520Arash%2520Vahdat%26entry.1292438233%3D%2520%2520Drug%2520discovery%2520is%2520a%2520complex%2520process%2520that%2520involves%2520multiple%2520scenarios%2520and%250Astages%252C%2520such%2520as%2520fragment-constrained%2520molecule%2520generation%252C%2520hit%2520generation%2520and%250Alead%2520optimization.%2520However%252C%2520existing%2520molecular%2520generative%2520models%2520can%2520only%250Atackle%2520one%2520or%2520two%2520of%2520these%2520scenarios%2520and%2520lack%2520the%2520flexibility%2520to%2520address%250Avarious%2520aspects%2520of%2520the%2520drug%2520discovery%2520pipeline.%2520In%2520this%2520paper%252C%2520we%2520present%250AGeneralist%2520Molecular%2520generative%2520model%2520%2528GenMol%2529%252C%2520a%2520versatile%2520framework%2520that%250Aaddresses%2520these%2520limitations%2520by%2520applying%2520discrete%2520diffusion%2520to%2520the%2520Sequential%250AAttachment-based%2520Fragment%2520Embedding%2520%2528SAFE%2529%2520molecular%2520representation.%2520GenMol%250Agenerates%2520SAFE%2520sequences%2520through%2520non-autoregressive%2520bidirectional%2520parallel%250Adecoding%252C%2520thereby%2520allowing%2520utilization%2520of%2520a%2520molecular%2520context%2520that%2520does%2520not%250Arely%2520on%2520the%2520specific%2520token%2520ordering%2520and%2520enhanced%2520computational%2520efficiency.%250AMoreover%252C%2520under%2520the%2520discrete%2520diffusion%2520framework%252C%2520we%2520introduce%2520fragment%250Aremasking%252C%2520a%2520strategy%2520that%2520optimizes%2520molecules%2520by%2520replacing%2520fragments%2520with%250Amasked%2520tokens%2520and%2520regenerating%2520them%252C%2520enabling%2520effective%2520exploration%2520of%2520chemical%250Aspace.%2520GenMol%2520significantly%2520outperforms%2520the%2520previous%2520GPT-based%2520model%2520trained%2520on%250ASAFE%2520representations%2520in%2520de%2520novo%2520generation%2520and%2520fragment-constrained%2520generation%252C%250Aand%2520achieves%2520state-of-the-art%2520performance%2520in%2520goal-directed%2520hit%2520generation%2520and%250Alead%2520optimization.%2520These%2520experimental%2520results%2520demonstrate%2520that%2520GenMol%2520can%250Atackle%2520a%2520wide%2520range%2520of%2520drug%2520discovery%2520tasks%252C%2520providing%2520a%2520unified%2520and%2520versatile%250Aapproach%2520for%2520molecular%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenMol%3A%20A%20Drug%20Discovery%20Generalist%20with%20Discrete%20Diffusion&entry.906535625=Seul%20Lee%20and%20Karsten%20Kreis%20and%20Srimukh%20Prasad%20Veccham%20and%20Meng%20Liu%20and%20Danny%20Reidenbach%20and%20Yuxing%20Peng%20and%20Saee%20Paliwal%20and%20Weili%20Nie%20and%20Arash%20Vahdat&entry.1292438233=%20%20Drug%20discovery%20is%20a%20complex%20process%20that%20involves%20multiple%20scenarios%20and%0Astages%2C%20such%20as%20fragment-constrained%20molecule%20generation%2C%20hit%20generation%20and%0Alead%20optimization.%20However%2C%20existing%20molecular%20generative%20models%20can%20only%0Atackle%20one%20or%20two%20of%20these%20scenarios%20and%20lack%20the%20flexibility%20to%20address%0Avarious%20aspects%20of%20the%20drug%20discovery%20pipeline.%20In%20this%20paper%2C%20we%20present%0AGeneralist%20Molecular%20generative%20model%20%28GenMol%29%2C%20a%20versatile%20framework%20that%0Aaddresses%20these%20limitations%20by%20applying%20discrete%20diffusion%20to%20the%20Sequential%0AAttachment-based%20Fragment%20Embedding%20%28SAFE%29%20molecular%20representation.%20GenMol%0Agenerates%20SAFE%20sequences%20through%20non-autoregressive%20bidirectional%20parallel%0Adecoding%2C%20thereby%20allowing%20utilization%20of%20a%20molecular%20context%20that%20does%20not%0Arely%20on%20the%20specific%20token%20ordering%20and%20enhanced%20computational%20efficiency.%0AMoreover%2C%20under%20the%20discrete%20diffusion%20framework%2C%20we%20introduce%20fragment%0Aremasking%2C%20a%20strategy%20that%20optimizes%20molecules%20by%20replacing%20fragments%20with%0Amasked%20tokens%20and%20regenerating%20them%2C%20enabling%20effective%20exploration%20of%20chemical%0Aspace.%20GenMol%20significantly%20outperforms%20the%20previous%20GPT-based%20model%20trained%20on%0ASAFE%20representations%20in%20de%20novo%20generation%20and%20fragment-constrained%20generation%2C%0Aand%20achieves%20state-of-the-art%20performance%20in%20goal-directed%20hit%20generation%20and%0Alead%20optimization.%20These%20experimental%20results%20demonstrate%20that%20GenMol%20can%0Atackle%20a%20wide%20range%20of%20drug%20discovery%20tasks%2C%20providing%20a%20unified%20and%20versatile%0Aapproach%20for%20molecular%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06158v1&entry.124074799=Read"},
{"title": "Learning to generate feasible graphs using graph grammars", "author": "Stefan Mautner and Rolf Backofen and Fabrizio Costa", "abstract": "  Generative methods for graphs need to be sufficiently flexible to model\ncomplex dependencies between sets of nodes. At the same time, the generated\ngraphs need to satisfy domain-dependent feasibility conditions, that is, they\nshould not violate certain constraints that would make their interpretation\nimpossible within the given application domain (e.g. a molecular graph where an\natom has a very large number of chemical bounds). Crucially, constraints can\ninvolve not only local but also long-range dependencies: for example, the\nmaximal length of a cycle can be bounded.\n  Currently, a large class of generative approaches for graphs, such as methods\nbased on artificial neural networks, is based on message passing schemes. These\napproaches suffer from information 'dilution' issues that severely limit the\nmaximal range of the dependencies that can be modeled. To address this problem,\nwe propose a generative approach based on the notion of graph grammars. The key\nnovel idea is to introduce a domain-dependent coarsening procedure to provide\nshort-cuts for long-range dependencies.\n  We show the effectiveness of our proposal in two domains: 1) small drugs and\n2) RNA secondary structures. In the first case, we compare the quality of the\ngenerated molecular graphs via the Molecular Sets (MOSES) benchmark suite,\nwhich evaluates the distance between generated and real molecules, their\nlipophilicity, synthesizability, and drug-likeness. In the second case, we show\nthat the approach can generate very large graphs (with hundreds of nodes) that\nare accepted as valid examples for a desired RNA family by the \"Infernal\"\ncovariance model, a state-of-the-art RNA classifier.\n  Our implementation is available on github:\ngithub.com/fabriziocosta/GraphLearn\n", "link": "http://arxiv.org/abs/2501.06003v1", "date": "2025-01-10", "relevancy": 2.0862, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.523}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5218}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20generate%20feasible%20graphs%20using%20graph%20grammars&body=Title%3A%20Learning%20to%20generate%20feasible%20graphs%20using%20graph%20grammars%0AAuthor%3A%20Stefan%20Mautner%20and%20Rolf%20Backofen%20and%20Fabrizio%20Costa%0AAbstract%3A%20%20%20Generative%20methods%20for%20graphs%20need%20to%20be%20sufficiently%20flexible%20to%20model%0Acomplex%20dependencies%20between%20sets%20of%20nodes.%20At%20the%20same%20time%2C%20the%20generated%0Agraphs%20need%20to%20satisfy%20domain-dependent%20feasibility%20conditions%2C%20that%20is%2C%20they%0Ashould%20not%20violate%20certain%20constraints%20that%20would%20make%20their%20interpretation%0Aimpossible%20within%20the%20given%20application%20domain%20%28e.g.%20a%20molecular%20graph%20where%20an%0Aatom%20has%20a%20very%20large%20number%20of%20chemical%20bounds%29.%20Crucially%2C%20constraints%20can%0Ainvolve%20not%20only%20local%20but%20also%20long-range%20dependencies%3A%20for%20example%2C%20the%0Amaximal%20length%20of%20a%20cycle%20can%20be%20bounded.%0A%20%20Currently%2C%20a%20large%20class%20of%20generative%20approaches%20for%20graphs%2C%20such%20as%20methods%0Abased%20on%20artificial%20neural%20networks%2C%20is%20based%20on%20message%20passing%20schemes.%20These%0Aapproaches%20suffer%20from%20information%20%27dilution%27%20issues%20that%20severely%20limit%20the%0Amaximal%20range%20of%20the%20dependencies%20that%20can%20be%20modeled.%20To%20address%20this%20problem%2C%0Awe%20propose%20a%20generative%20approach%20based%20on%20the%20notion%20of%20graph%20grammars.%20The%20key%0Anovel%20idea%20is%20to%20introduce%20a%20domain-dependent%20coarsening%20procedure%20to%20provide%0Ashort-cuts%20for%20long-range%20dependencies.%0A%20%20We%20show%20the%20effectiveness%20of%20our%20proposal%20in%20two%20domains%3A%201%29%20small%20drugs%20and%0A2%29%20RNA%20secondary%20structures.%20In%20the%20first%20case%2C%20we%20compare%20the%20quality%20of%20the%0Agenerated%20molecular%20graphs%20via%20the%20Molecular%20Sets%20%28MOSES%29%20benchmark%20suite%2C%0Awhich%20evaluates%20the%20distance%20between%20generated%20and%20real%20molecules%2C%20their%0Alipophilicity%2C%20synthesizability%2C%20and%20drug-likeness.%20In%20the%20second%20case%2C%20we%20show%0Athat%20the%20approach%20can%20generate%20very%20large%20graphs%20%28with%20hundreds%20of%20nodes%29%20that%0Aare%20accepted%20as%20valid%20examples%20for%20a%20desired%20RNA%20family%20by%20the%20%22Infernal%22%0Acovariance%20model%2C%20a%20state-of-the-art%20RNA%20classifier.%0A%20%20Our%20implementation%20is%20available%20on%20github%3A%0Agithub.com/fabriziocosta/GraphLearn%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520generate%2520feasible%2520graphs%2520using%2520graph%2520grammars%26entry.906535625%3DStefan%2520Mautner%2520and%2520Rolf%2520Backofen%2520and%2520Fabrizio%2520Costa%26entry.1292438233%3D%2520%2520Generative%2520methods%2520for%2520graphs%2520need%2520to%2520be%2520sufficiently%2520flexible%2520to%2520model%250Acomplex%2520dependencies%2520between%2520sets%2520of%2520nodes.%2520At%2520the%2520same%2520time%252C%2520the%2520generated%250Agraphs%2520need%2520to%2520satisfy%2520domain-dependent%2520feasibility%2520conditions%252C%2520that%2520is%252C%2520they%250Ashould%2520not%2520violate%2520certain%2520constraints%2520that%2520would%2520make%2520their%2520interpretation%250Aimpossible%2520within%2520the%2520given%2520application%2520domain%2520%2528e.g.%2520a%2520molecular%2520graph%2520where%2520an%250Aatom%2520has%2520a%2520very%2520large%2520number%2520of%2520chemical%2520bounds%2529.%2520Crucially%252C%2520constraints%2520can%250Ainvolve%2520not%2520only%2520local%2520but%2520also%2520long-range%2520dependencies%253A%2520for%2520example%252C%2520the%250Amaximal%2520length%2520of%2520a%2520cycle%2520can%2520be%2520bounded.%250A%2520%2520Currently%252C%2520a%2520large%2520class%2520of%2520generative%2520approaches%2520for%2520graphs%252C%2520such%2520as%2520methods%250Abased%2520on%2520artificial%2520neural%2520networks%252C%2520is%2520based%2520on%2520message%2520passing%2520schemes.%2520These%250Aapproaches%2520suffer%2520from%2520information%2520%2527dilution%2527%2520issues%2520that%2520severely%2520limit%2520the%250Amaximal%2520range%2520of%2520the%2520dependencies%2520that%2520can%2520be%2520modeled.%2520To%2520address%2520this%2520problem%252C%250Awe%2520propose%2520a%2520generative%2520approach%2520based%2520on%2520the%2520notion%2520of%2520graph%2520grammars.%2520The%2520key%250Anovel%2520idea%2520is%2520to%2520introduce%2520a%2520domain-dependent%2520coarsening%2520procedure%2520to%2520provide%250Ashort-cuts%2520for%2520long-range%2520dependencies.%250A%2520%2520We%2520show%2520the%2520effectiveness%2520of%2520our%2520proposal%2520in%2520two%2520domains%253A%25201%2529%2520small%2520drugs%2520and%250A2%2529%2520RNA%2520secondary%2520structures.%2520In%2520the%2520first%2520case%252C%2520we%2520compare%2520the%2520quality%2520of%2520the%250Agenerated%2520molecular%2520graphs%2520via%2520the%2520Molecular%2520Sets%2520%2528MOSES%2529%2520benchmark%2520suite%252C%250Awhich%2520evaluates%2520the%2520distance%2520between%2520generated%2520and%2520real%2520molecules%252C%2520their%250Alipophilicity%252C%2520synthesizability%252C%2520and%2520drug-likeness.%2520In%2520the%2520second%2520case%252C%2520we%2520show%250Athat%2520the%2520approach%2520can%2520generate%2520very%2520large%2520graphs%2520%2528with%2520hundreds%2520of%2520nodes%2529%2520that%250Aare%2520accepted%2520as%2520valid%2520examples%2520for%2520a%2520desired%2520RNA%2520family%2520by%2520the%2520%2522Infernal%2522%250Acovariance%2520model%252C%2520a%2520state-of-the-art%2520RNA%2520classifier.%250A%2520%2520Our%2520implementation%2520is%2520available%2520on%2520github%253A%250Agithub.com/fabriziocosta/GraphLearn%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20generate%20feasible%20graphs%20using%20graph%20grammars&entry.906535625=Stefan%20Mautner%20and%20Rolf%20Backofen%20and%20Fabrizio%20Costa&entry.1292438233=%20%20Generative%20methods%20for%20graphs%20need%20to%20be%20sufficiently%20flexible%20to%20model%0Acomplex%20dependencies%20between%20sets%20of%20nodes.%20At%20the%20same%20time%2C%20the%20generated%0Agraphs%20need%20to%20satisfy%20domain-dependent%20feasibility%20conditions%2C%20that%20is%2C%20they%0Ashould%20not%20violate%20certain%20constraints%20that%20would%20make%20their%20interpretation%0Aimpossible%20within%20the%20given%20application%20domain%20%28e.g.%20a%20molecular%20graph%20where%20an%0Aatom%20has%20a%20very%20large%20number%20of%20chemical%20bounds%29.%20Crucially%2C%20constraints%20can%0Ainvolve%20not%20only%20local%20but%20also%20long-range%20dependencies%3A%20for%20example%2C%20the%0Amaximal%20length%20of%20a%20cycle%20can%20be%20bounded.%0A%20%20Currently%2C%20a%20large%20class%20of%20generative%20approaches%20for%20graphs%2C%20such%20as%20methods%0Abased%20on%20artificial%20neural%20networks%2C%20is%20based%20on%20message%20passing%20schemes.%20These%0Aapproaches%20suffer%20from%20information%20%27dilution%27%20issues%20that%20severely%20limit%20the%0Amaximal%20range%20of%20the%20dependencies%20that%20can%20be%20modeled.%20To%20address%20this%20problem%2C%0Awe%20propose%20a%20generative%20approach%20based%20on%20the%20notion%20of%20graph%20grammars.%20The%20key%0Anovel%20idea%20is%20to%20introduce%20a%20domain-dependent%20coarsening%20procedure%20to%20provide%0Ashort-cuts%20for%20long-range%20dependencies.%0A%20%20We%20show%20the%20effectiveness%20of%20our%20proposal%20in%20two%20domains%3A%201%29%20small%20drugs%20and%0A2%29%20RNA%20secondary%20structures.%20In%20the%20first%20case%2C%20we%20compare%20the%20quality%20of%20the%0Agenerated%20molecular%20graphs%20via%20the%20Molecular%20Sets%20%28MOSES%29%20benchmark%20suite%2C%0Awhich%20evaluates%20the%20distance%20between%20generated%20and%20real%20molecules%2C%20their%0Alipophilicity%2C%20synthesizability%2C%20and%20drug-likeness.%20In%20the%20second%20case%2C%20we%20show%0Athat%20the%20approach%20can%20generate%20very%20large%20graphs%20%28with%20hundreds%20of%20nodes%29%20that%0Aare%20accepted%20as%20valid%20examples%20for%20a%20desired%20RNA%20family%20by%20the%20%22Infernal%22%0Acovariance%20model%2C%20a%20state-of-the-art%20RNA%20classifier.%0A%20%20Our%20implementation%20is%20available%20on%20github%3A%0Agithub.com/fabriziocosta/GraphLearn%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06003v1&entry.124074799=Read"},
{"title": "Learning In-Distribution Representations for Anomaly Detection", "author": "Willian T. Lunardi and Abdulrahman Banabila and Dania Herzalla and Martin Andreoni", "abstract": "  Anomaly detection involves identifying data patterns that deviate from the\nanticipated norm. Traditional methods struggle in high-dimensional spaces due\nto the curse of dimensionality. In recent years, self-supervised learning,\nparticularly through contrastive objectives, has driven advances in anomaly\ndetection. However, vanilla contrastive learning struggles to align with the\nunique demands of anomaly detection, as it lacks a pretext task tailored to the\nhomogeneous nature of In-Distribution (ID) data and the diversity of\nOut-of-Distribution (OOD) anomalies. Methods that attempt to address these\nchallenges, such as introducing hard negatives through synthetic outliers,\nOutlier Exposure (OE), and supervised objectives, often rely on pretext tasks\nthat fail to balance compact clustering of ID samples with sufficient\nseparation from OOD data. In this work, we propose Focused In-distribution\nRepresentation Modeling (FIRM), a contrastive learning objective specifically\ndesigned for anomaly detection. Unlike existing approaches, FIRM incorporates\nsynthetic outliers into its pretext task in a way that actively shapes the\nrepresentation space, promoting compact clustering of ID samples while\nenforcing strong separation from outliers. This formulation addresses the\nchallenges of class collision, enhancing both the compactness of ID\nrepresentations and the discriminative power of the learned feature space. We\nshow that FIRM surpasses other contrastive methods in standard benchmarks,\nsignificantly enhancing anomaly detection compared to both traditional and\nsupervised contrastive learning objectives. Our ablation studies confirm that\nFIRM consistently improves the quality of representations and shows robustness\nacross a range of scoring methods. The code is available at:\nhttps://github.com/willtl/firm.\n", "link": "http://arxiv.org/abs/2501.05130v2", "date": "2025-01-10", "relevancy": 2.0836, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5327}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20In-Distribution%20Representations%20for%20Anomaly%20Detection&body=Title%3A%20Learning%20In-Distribution%20Representations%20for%20Anomaly%20Detection%0AAuthor%3A%20Willian%20T.%20Lunardi%20and%20Abdulrahman%20Banabila%20and%20Dania%20Herzalla%20and%20Martin%20Andreoni%0AAbstract%3A%20%20%20Anomaly%20detection%20involves%20identifying%20data%20patterns%20that%20deviate%20from%20the%0Aanticipated%20norm.%20Traditional%20methods%20struggle%20in%20high-dimensional%20spaces%20due%0Ato%20the%20curse%20of%20dimensionality.%20In%20recent%20years%2C%20self-supervised%20learning%2C%0Aparticularly%20through%20contrastive%20objectives%2C%20has%20driven%20advances%20in%20anomaly%0Adetection.%20However%2C%20vanilla%20contrastive%20learning%20struggles%20to%20align%20with%20the%0Aunique%20demands%20of%20anomaly%20detection%2C%20as%20it%20lacks%20a%20pretext%20task%20tailored%20to%20the%0Ahomogeneous%20nature%20of%20In-Distribution%20%28ID%29%20data%20and%20the%20diversity%20of%0AOut-of-Distribution%20%28OOD%29%20anomalies.%20Methods%20that%20attempt%20to%20address%20these%0Achallenges%2C%20such%20as%20introducing%20hard%20negatives%20through%20synthetic%20outliers%2C%0AOutlier%20Exposure%20%28OE%29%2C%20and%20supervised%20objectives%2C%20often%20rely%20on%20pretext%20tasks%0Athat%20fail%20to%20balance%20compact%20clustering%20of%20ID%20samples%20with%20sufficient%0Aseparation%20from%20OOD%20data.%20In%20this%20work%2C%20we%20propose%20Focused%20In-distribution%0ARepresentation%20Modeling%20%28FIRM%29%2C%20a%20contrastive%20learning%20objective%20specifically%0Adesigned%20for%20anomaly%20detection.%20Unlike%20existing%20approaches%2C%20FIRM%20incorporates%0Asynthetic%20outliers%20into%20its%20pretext%20task%20in%20a%20way%20that%20actively%20shapes%20the%0Arepresentation%20space%2C%20promoting%20compact%20clustering%20of%20ID%20samples%20while%0Aenforcing%20strong%20separation%20from%20outliers.%20This%20formulation%20addresses%20the%0Achallenges%20of%20class%20collision%2C%20enhancing%20both%20the%20compactness%20of%20ID%0Arepresentations%20and%20the%20discriminative%20power%20of%20the%20learned%20feature%20space.%20We%0Ashow%20that%20FIRM%20surpasses%20other%20contrastive%20methods%20in%20standard%20benchmarks%2C%0Asignificantly%20enhancing%20anomaly%20detection%20compared%20to%20both%20traditional%20and%0Asupervised%20contrastive%20learning%20objectives.%20Our%20ablation%20studies%20confirm%20that%0AFIRM%20consistently%20improves%20the%20quality%20of%20representations%20and%20shows%20robustness%0Aacross%20a%20range%20of%20scoring%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/willtl/firm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05130v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520In-Distribution%2520Representations%2520for%2520Anomaly%2520Detection%26entry.906535625%3DWillian%2520T.%2520Lunardi%2520and%2520Abdulrahman%2520Banabila%2520and%2520Dania%2520Herzalla%2520and%2520Martin%2520Andreoni%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520involves%2520identifying%2520data%2520patterns%2520that%2520deviate%2520from%2520the%250Aanticipated%2520norm.%2520Traditional%2520methods%2520struggle%2520in%2520high-dimensional%2520spaces%2520due%250Ato%2520the%2520curse%2520of%2520dimensionality.%2520In%2520recent%2520years%252C%2520self-supervised%2520learning%252C%250Aparticularly%2520through%2520contrastive%2520objectives%252C%2520has%2520driven%2520advances%2520in%2520anomaly%250Adetection.%2520However%252C%2520vanilla%2520contrastive%2520learning%2520struggles%2520to%2520align%2520with%2520the%250Aunique%2520demands%2520of%2520anomaly%2520detection%252C%2520as%2520it%2520lacks%2520a%2520pretext%2520task%2520tailored%2520to%2520the%250Ahomogeneous%2520nature%2520of%2520In-Distribution%2520%2528ID%2529%2520data%2520and%2520the%2520diversity%2520of%250AOut-of-Distribution%2520%2528OOD%2529%2520anomalies.%2520Methods%2520that%2520attempt%2520to%2520address%2520these%250Achallenges%252C%2520such%2520as%2520introducing%2520hard%2520negatives%2520through%2520synthetic%2520outliers%252C%250AOutlier%2520Exposure%2520%2528OE%2529%252C%2520and%2520supervised%2520objectives%252C%2520often%2520rely%2520on%2520pretext%2520tasks%250Athat%2520fail%2520to%2520balance%2520compact%2520clustering%2520of%2520ID%2520samples%2520with%2520sufficient%250Aseparation%2520from%2520OOD%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520Focused%2520In-distribution%250ARepresentation%2520Modeling%2520%2528FIRM%2529%252C%2520a%2520contrastive%2520learning%2520objective%2520specifically%250Adesigned%2520for%2520anomaly%2520detection.%2520Unlike%2520existing%2520approaches%252C%2520FIRM%2520incorporates%250Asynthetic%2520outliers%2520into%2520its%2520pretext%2520task%2520in%2520a%2520way%2520that%2520actively%2520shapes%2520the%250Arepresentation%2520space%252C%2520promoting%2520compact%2520clustering%2520of%2520ID%2520samples%2520while%250Aenforcing%2520strong%2520separation%2520from%2520outliers.%2520This%2520formulation%2520addresses%2520the%250Achallenges%2520of%2520class%2520collision%252C%2520enhancing%2520both%2520the%2520compactness%2520of%2520ID%250Arepresentations%2520and%2520the%2520discriminative%2520power%2520of%2520the%2520learned%2520feature%2520space.%2520We%250Ashow%2520that%2520FIRM%2520surpasses%2520other%2520contrastive%2520methods%2520in%2520standard%2520benchmarks%252C%250Asignificantly%2520enhancing%2520anomaly%2520detection%2520compared%2520to%2520both%2520traditional%2520and%250Asupervised%2520contrastive%2520learning%2520objectives.%2520Our%2520ablation%2520studies%2520confirm%2520that%250AFIRM%2520consistently%2520improves%2520the%2520quality%2520of%2520representations%2520and%2520shows%2520robustness%250Aacross%2520a%2520range%2520of%2520scoring%2520methods.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/willtl/firm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05130v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20In-Distribution%20Representations%20for%20Anomaly%20Detection&entry.906535625=Willian%20T.%20Lunardi%20and%20Abdulrahman%20Banabila%20and%20Dania%20Herzalla%20and%20Martin%20Andreoni&entry.1292438233=%20%20Anomaly%20detection%20involves%20identifying%20data%20patterns%20that%20deviate%20from%20the%0Aanticipated%20norm.%20Traditional%20methods%20struggle%20in%20high-dimensional%20spaces%20due%0Ato%20the%20curse%20of%20dimensionality.%20In%20recent%20years%2C%20self-supervised%20learning%2C%0Aparticularly%20through%20contrastive%20objectives%2C%20has%20driven%20advances%20in%20anomaly%0Adetection.%20However%2C%20vanilla%20contrastive%20learning%20struggles%20to%20align%20with%20the%0Aunique%20demands%20of%20anomaly%20detection%2C%20as%20it%20lacks%20a%20pretext%20task%20tailored%20to%20the%0Ahomogeneous%20nature%20of%20In-Distribution%20%28ID%29%20data%20and%20the%20diversity%20of%0AOut-of-Distribution%20%28OOD%29%20anomalies.%20Methods%20that%20attempt%20to%20address%20these%0Achallenges%2C%20such%20as%20introducing%20hard%20negatives%20through%20synthetic%20outliers%2C%0AOutlier%20Exposure%20%28OE%29%2C%20and%20supervised%20objectives%2C%20often%20rely%20on%20pretext%20tasks%0Athat%20fail%20to%20balance%20compact%20clustering%20of%20ID%20samples%20with%20sufficient%0Aseparation%20from%20OOD%20data.%20In%20this%20work%2C%20we%20propose%20Focused%20In-distribution%0ARepresentation%20Modeling%20%28FIRM%29%2C%20a%20contrastive%20learning%20objective%20specifically%0Adesigned%20for%20anomaly%20detection.%20Unlike%20existing%20approaches%2C%20FIRM%20incorporates%0Asynthetic%20outliers%20into%20its%20pretext%20task%20in%20a%20way%20that%20actively%20shapes%20the%0Arepresentation%20space%2C%20promoting%20compact%20clustering%20of%20ID%20samples%20while%0Aenforcing%20strong%20separation%20from%20outliers.%20This%20formulation%20addresses%20the%0Achallenges%20of%20class%20collision%2C%20enhancing%20both%20the%20compactness%20of%20ID%0Arepresentations%20and%20the%20discriminative%20power%20of%20the%20learned%20feature%20space.%20We%0Ashow%20that%20FIRM%20surpasses%20other%20contrastive%20methods%20in%20standard%20benchmarks%2C%0Asignificantly%20enhancing%20anomaly%20detection%20compared%20to%20both%20traditional%20and%0Asupervised%20contrastive%20learning%20objectives.%20Our%20ablation%20studies%20confirm%20that%0AFIRM%20consistently%20improves%20the%20quality%20of%20representations%20and%20shows%20robustness%0Aacross%20a%20range%20of%20scoring%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/willtl/firm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05130v2&entry.124074799=Read"},
{"title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs", "author": "Yangyu Huang and Tianyi Gao and Haoran Xu and Qihao Zhao and Yang Song and Zhipeng Gui and Tengchao Lv and Hao Chen and Lei Cui and Scarlett Li and Furu Wei", "abstract": "  Geologic map, as a fundamental diagram in geology science, provides critical\ninsights into the structure and composition of Earth's subsurface and surface.\nThese maps are indispensable in various fields, including disaster detection,\nresource exploration, and civil engineering. Despite their significance,\ncurrent Multimodal Large Language Models (MLLMs) often fall short in geologic\nmap understanding. This gap is primarily due to the challenging nature of\ncartographic generalization, which involves handling high-resolution map,\nmanaging multiple associated components, and requiring domain-specific\nknowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever\nbenchmark for evaluating MLLMs in geologic map understanding, which assesses\nthe full-scale abilities in extracting, referring, grounding, reasoning, and\nanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent\ndesigned for geologic map understanding, which features three modules:\nHierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),\nand Prompt-enhanced Question Answering (PEQA). Inspired by the\ninterdisciplinary collaboration among human scientists, an AI expert group acts\nas consultants, utilizing a diverse tool pool to comprehensively analyze\nquestions. Through comprehensive experiments, GeoMap-Agent achieves an overall\nscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.\nOur work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,\npaves the way for advanced AI applications in geology, enhancing the efficiency\nand accuracy of geological investigations.\n", "link": "http://arxiv.org/abs/2501.06184v1", "date": "2025-01-10", "relevancy": 2.0775, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5416}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEACE%3A%20Empowering%20Geologic%20Map%20Holistic%20Understanding%20with%20MLLMs&body=Title%3A%20PEACE%3A%20Empowering%20Geologic%20Map%20Holistic%20Understanding%20with%20MLLMs%0AAuthor%3A%20Yangyu%20Huang%20and%20Tianyi%20Gao%20and%20Haoran%20Xu%20and%20Qihao%20Zhao%20and%20Yang%20Song%20and%20Zhipeng%20Gui%20and%20Tengchao%20Lv%20and%20Hao%20Chen%20and%20Lei%20Cui%20and%20Scarlett%20Li%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Geologic%20map%2C%20as%20a%20fundamental%20diagram%20in%20geology%20science%2C%20provides%20critical%0Ainsights%20into%20the%20structure%20and%20composition%20of%20Earth%27s%20subsurface%20and%20surface.%0AThese%20maps%20are%20indispensable%20in%20various%20fields%2C%20including%20disaster%20detection%2C%0Aresource%20exploration%2C%20and%20civil%20engineering.%20Despite%20their%20significance%2C%0Acurrent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20often%20fall%20short%20in%20geologic%0Amap%20understanding.%20This%20gap%20is%20primarily%20due%20to%20the%20challenging%20nature%20of%0Acartographic%20generalization%2C%20which%20involves%20handling%20high-resolution%20map%2C%0Amanaging%20multiple%20associated%20components%2C%20and%20requiring%20domain-specific%0Aknowledge.%20To%20quantify%20this%20gap%2C%20we%20construct%20GeoMap-Bench%2C%20the%20first-ever%0Abenchmark%20for%20evaluating%20MLLMs%20in%20geologic%20map%20understanding%2C%20which%20assesses%0Athe%20full-scale%20abilities%20in%20extracting%2C%20referring%2C%20grounding%2C%20reasoning%2C%20and%0Aanalyzing.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GeoMap-Agent%2C%20the%20inaugural%20agent%0Adesigned%20for%20geologic%20map%20understanding%2C%20which%20features%20three%20modules%3A%0AHierarchical%20Information%20Extraction%20%28HIE%29%2C%20Domain%20Knowledge%20Injection%20%28DKI%29%2C%0Aand%20Prompt-enhanced%20Question%20Answering%20%28PEQA%29.%20Inspired%20by%20the%0Ainterdisciplinary%20collaboration%20among%20human%20scientists%2C%20an%20AI%20expert%20group%20acts%0Aas%20consultants%2C%20utilizing%20a%20diverse%20tool%20pool%20to%20comprehensively%20analyze%0Aquestions.%20Through%20comprehensive%20experiments%2C%20GeoMap-Agent%20achieves%20an%20overall%0Ascore%20of%200.811%20on%20GeoMap-Bench%2C%20significantly%20outperforming%200.369%20of%20GPT-4o.%0AOur%20work%2C%20emPowering%20gEologic%20mAp%20holistiC%20undErstanding%20%28PEACE%29%20with%20MLLMs%2C%0Apaves%20the%20way%20for%20advanced%20AI%20applications%20in%20geology%2C%20enhancing%20the%20efficiency%0Aand%20accuracy%20of%20geological%20investigations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEACE%253A%2520Empowering%2520Geologic%2520Map%2520Holistic%2520Understanding%2520with%2520MLLMs%26entry.906535625%3DYangyu%2520Huang%2520and%2520Tianyi%2520Gao%2520and%2520Haoran%2520Xu%2520and%2520Qihao%2520Zhao%2520and%2520Yang%2520Song%2520and%2520Zhipeng%2520Gui%2520and%2520Tengchao%2520Lv%2520and%2520Hao%2520Chen%2520and%2520Lei%2520Cui%2520and%2520Scarlett%2520Li%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Geologic%2520map%252C%2520as%2520a%2520fundamental%2520diagram%2520in%2520geology%2520science%252C%2520provides%2520critical%250Ainsights%2520into%2520the%2520structure%2520and%2520composition%2520of%2520Earth%2527s%2520subsurface%2520and%2520surface.%250AThese%2520maps%2520are%2520indispensable%2520in%2520various%2520fields%252C%2520including%2520disaster%2520detection%252C%250Aresource%2520exploration%252C%2520and%2520civil%2520engineering.%2520Despite%2520their%2520significance%252C%250Acurrent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520often%2520fall%2520short%2520in%2520geologic%250Amap%2520understanding.%2520This%2520gap%2520is%2520primarily%2520due%2520to%2520the%2520challenging%2520nature%2520of%250Acartographic%2520generalization%252C%2520which%2520involves%2520handling%2520high-resolution%2520map%252C%250Amanaging%2520multiple%2520associated%2520components%252C%2520and%2520requiring%2520domain-specific%250Aknowledge.%2520To%2520quantify%2520this%2520gap%252C%2520we%2520construct%2520GeoMap-Bench%252C%2520the%2520first-ever%250Abenchmark%2520for%2520evaluating%2520MLLMs%2520in%2520geologic%2520map%2520understanding%252C%2520which%2520assesses%250Athe%2520full-scale%2520abilities%2520in%2520extracting%252C%2520referring%252C%2520grounding%252C%2520reasoning%252C%2520and%250Aanalyzing.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520GeoMap-Agent%252C%2520the%2520inaugural%2520agent%250Adesigned%2520for%2520geologic%2520map%2520understanding%252C%2520which%2520features%2520three%2520modules%253A%250AHierarchical%2520Information%2520Extraction%2520%2528HIE%2529%252C%2520Domain%2520Knowledge%2520Injection%2520%2528DKI%2529%252C%250Aand%2520Prompt-enhanced%2520Question%2520Answering%2520%2528PEQA%2529.%2520Inspired%2520by%2520the%250Ainterdisciplinary%2520collaboration%2520among%2520human%2520scientists%252C%2520an%2520AI%2520expert%2520group%2520acts%250Aas%2520consultants%252C%2520utilizing%2520a%2520diverse%2520tool%2520pool%2520to%2520comprehensively%2520analyze%250Aquestions.%2520Through%2520comprehensive%2520experiments%252C%2520GeoMap-Agent%2520achieves%2520an%2520overall%250Ascore%2520of%25200.811%2520on%2520GeoMap-Bench%252C%2520significantly%2520outperforming%25200.369%2520of%2520GPT-4o.%250AOur%2520work%252C%2520emPowering%2520gEologic%2520mAp%2520holistiC%2520undErstanding%2520%2528PEACE%2529%2520with%2520MLLMs%252C%250Apaves%2520the%2520way%2520for%2520advanced%2520AI%2520applications%2520in%2520geology%252C%2520enhancing%2520the%2520efficiency%250Aand%2520accuracy%2520of%2520geological%2520investigations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEACE%3A%20Empowering%20Geologic%20Map%20Holistic%20Understanding%20with%20MLLMs&entry.906535625=Yangyu%20Huang%20and%20Tianyi%20Gao%20and%20Haoran%20Xu%20and%20Qihao%20Zhao%20and%20Yang%20Song%20and%20Zhipeng%20Gui%20and%20Tengchao%20Lv%20and%20Hao%20Chen%20and%20Lei%20Cui%20and%20Scarlett%20Li%20and%20Furu%20Wei&entry.1292438233=%20%20Geologic%20map%2C%20as%20a%20fundamental%20diagram%20in%20geology%20science%2C%20provides%20critical%0Ainsights%20into%20the%20structure%20and%20composition%20of%20Earth%27s%20subsurface%20and%20surface.%0AThese%20maps%20are%20indispensable%20in%20various%20fields%2C%20including%20disaster%20detection%2C%0Aresource%20exploration%2C%20and%20civil%20engineering.%20Despite%20their%20significance%2C%0Acurrent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20often%20fall%20short%20in%20geologic%0Amap%20understanding.%20This%20gap%20is%20primarily%20due%20to%20the%20challenging%20nature%20of%0Acartographic%20generalization%2C%20which%20involves%20handling%20high-resolution%20map%2C%0Amanaging%20multiple%20associated%20components%2C%20and%20requiring%20domain-specific%0Aknowledge.%20To%20quantify%20this%20gap%2C%20we%20construct%20GeoMap-Bench%2C%20the%20first-ever%0Abenchmark%20for%20evaluating%20MLLMs%20in%20geologic%20map%20understanding%2C%20which%20assesses%0Athe%20full-scale%20abilities%20in%20extracting%2C%20referring%2C%20grounding%2C%20reasoning%2C%20and%0Aanalyzing.%20To%20bridge%20this%20gap%2C%20we%20introduce%20GeoMap-Agent%2C%20the%20inaugural%20agent%0Adesigned%20for%20geologic%20map%20understanding%2C%20which%20features%20three%20modules%3A%0AHierarchical%20Information%20Extraction%20%28HIE%29%2C%20Domain%20Knowledge%20Injection%20%28DKI%29%2C%0Aand%20Prompt-enhanced%20Question%20Answering%20%28PEQA%29.%20Inspired%20by%20the%0Ainterdisciplinary%20collaboration%20among%20human%20scientists%2C%20an%20AI%20expert%20group%20acts%0Aas%20consultants%2C%20utilizing%20a%20diverse%20tool%20pool%20to%20comprehensively%20analyze%0Aquestions.%20Through%20comprehensive%20experiments%2C%20GeoMap-Agent%20achieves%20an%20overall%0Ascore%20of%200.811%20on%20GeoMap-Bench%2C%20significantly%20outperforming%200.369%20of%20GPT-4o.%0AOur%20work%2C%20emPowering%20gEologic%20mAp%20holistiC%20undErstanding%20%28PEACE%29%20with%20MLLMs%2C%0Apaves%20the%20way%20for%20advanced%20AI%20applications%20in%20geology%2C%20enhancing%20the%20efficiency%0Aand%20accuracy%20of%20geological%20investigations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06184v1&entry.124074799=Read"},
{"title": "Vehicle-in-Virtual-Environment (VVE) Based Autonomous Driving Function\n  Development and Evaluation Methodology for Vulnerable Road User Safety", "author": "Haochong Chen and Xincheng Cao and Levent Guvenc and Bilin Aksun Guvenc", "abstract": "  Traditional methods for developing and evaluating autonomous driving\nfunctions, such as model-in-the-loop (MIL) and hardware-in-the-loop (HIL)\nsimulations, heavily depend on the accuracy of simulated vehicle models and\nhuman factors, especially for vulnerable road user safety systems. Continuation\nof development during public road deployment forces other road users including\nvulnerable ones to involuntarily participate in the development process,\nleading to safety risks, inefficiencies, and a decline in public trust. To\naddress these deficiencies, the Vehicle-in-Virtual-Environment (VVE) method was\nproposed as a safer, more efficient, and cost-effective solution for developing\nand testing connected and autonomous driving technologies by operating the real\nvehicle and multiple other actors like vulnerable road users in different test\nareas while being immersed within the same highly realistic virtual\nenvironment. This VVE approach synchronizes real-world vehicle and vulnerable\nroad user motion within the same virtual scenario, enabling the safe and\nrealistic testing of various traffic situations in a safe and repeatable\nmanner. In this paper, we propose a new testing pipeline that sequentially\nintegrates MIL, HIL, and VVE methods to comprehensively develop and evaluate\nautonomous driving functions. The effectiveness of this testing pipeline will\nbe demonstrated using an autonomous driving path-tracking algorithm with local\ndeep reinforcement learning modification for vulnerable road user collision\navoidance.\n", "link": "http://arxiv.org/abs/2501.06113v1", "date": "2025-01-10", "relevancy": 2.0695, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4972}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vehicle-in-Virtual-Environment%20%28VVE%29%20Based%20Autonomous%20Driving%20Function%0A%20%20Development%20and%20Evaluation%20Methodology%20for%20Vulnerable%20Road%20User%20Safety&body=Title%3A%20Vehicle-in-Virtual-Environment%20%28VVE%29%20Based%20Autonomous%20Driving%20Function%0A%20%20Development%20and%20Evaluation%20Methodology%20for%20Vulnerable%20Road%20User%20Safety%0AAuthor%3A%20Haochong%20Chen%20and%20Xincheng%20Cao%20and%20Levent%20Guvenc%20and%20Bilin%20Aksun%20Guvenc%0AAbstract%3A%20%20%20Traditional%20methods%20for%20developing%20and%20evaluating%20autonomous%20driving%0Afunctions%2C%20such%20as%20model-in-the-loop%20%28MIL%29%20and%20hardware-in-the-loop%20%28HIL%29%0Asimulations%2C%20heavily%20depend%20on%20the%20accuracy%20of%20simulated%20vehicle%20models%20and%0Ahuman%20factors%2C%20especially%20for%20vulnerable%20road%20user%20safety%20systems.%20Continuation%0Aof%20development%20during%20public%20road%20deployment%20forces%20other%20road%20users%20including%0Avulnerable%20ones%20to%20involuntarily%20participate%20in%20the%20development%20process%2C%0Aleading%20to%20safety%20risks%2C%20inefficiencies%2C%20and%20a%20decline%20in%20public%20trust.%20To%0Aaddress%20these%20deficiencies%2C%20the%20Vehicle-in-Virtual-Environment%20%28VVE%29%20method%20was%0Aproposed%20as%20a%20safer%2C%20more%20efficient%2C%20and%20cost-effective%20solution%20for%20developing%0Aand%20testing%20connected%20and%20autonomous%20driving%20technologies%20by%20operating%20the%20real%0Avehicle%20and%20multiple%20other%20actors%20like%20vulnerable%20road%20users%20in%20different%20test%0Aareas%20while%20being%20immersed%20within%20the%20same%20highly%20realistic%20virtual%0Aenvironment.%20This%20VVE%20approach%20synchronizes%20real-world%20vehicle%20and%20vulnerable%0Aroad%20user%20motion%20within%20the%20same%20virtual%20scenario%2C%20enabling%20the%20safe%20and%0Arealistic%20testing%20of%20various%20traffic%20situations%20in%20a%20safe%20and%20repeatable%0Amanner.%20In%20this%20paper%2C%20we%20propose%20a%20new%20testing%20pipeline%20that%20sequentially%0Aintegrates%20MIL%2C%20HIL%2C%20and%20VVE%20methods%20to%20comprehensively%20develop%20and%20evaluate%0Aautonomous%20driving%20functions.%20The%20effectiveness%20of%20this%20testing%20pipeline%20will%0Abe%20demonstrated%20using%20an%20autonomous%20driving%20path-tracking%20algorithm%20with%20local%0Adeep%20reinforcement%20learning%20modification%20for%20vulnerable%20road%20user%20collision%0Aavoidance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVehicle-in-Virtual-Environment%2520%2528VVE%2529%2520Based%2520Autonomous%2520Driving%2520Function%250A%2520%2520Development%2520and%2520Evaluation%2520Methodology%2520for%2520Vulnerable%2520Road%2520User%2520Safety%26entry.906535625%3DHaochong%2520Chen%2520and%2520Xincheng%2520Cao%2520and%2520Levent%2520Guvenc%2520and%2520Bilin%2520Aksun%2520Guvenc%26entry.1292438233%3D%2520%2520Traditional%2520methods%2520for%2520developing%2520and%2520evaluating%2520autonomous%2520driving%250Afunctions%252C%2520such%2520as%2520model-in-the-loop%2520%2528MIL%2529%2520and%2520hardware-in-the-loop%2520%2528HIL%2529%250Asimulations%252C%2520heavily%2520depend%2520on%2520the%2520accuracy%2520of%2520simulated%2520vehicle%2520models%2520and%250Ahuman%2520factors%252C%2520especially%2520for%2520vulnerable%2520road%2520user%2520safety%2520systems.%2520Continuation%250Aof%2520development%2520during%2520public%2520road%2520deployment%2520forces%2520other%2520road%2520users%2520including%250Avulnerable%2520ones%2520to%2520involuntarily%2520participate%2520in%2520the%2520development%2520process%252C%250Aleading%2520to%2520safety%2520risks%252C%2520inefficiencies%252C%2520and%2520a%2520decline%2520in%2520public%2520trust.%2520To%250Aaddress%2520these%2520deficiencies%252C%2520the%2520Vehicle-in-Virtual-Environment%2520%2528VVE%2529%2520method%2520was%250Aproposed%2520as%2520a%2520safer%252C%2520more%2520efficient%252C%2520and%2520cost-effective%2520solution%2520for%2520developing%250Aand%2520testing%2520connected%2520and%2520autonomous%2520driving%2520technologies%2520by%2520operating%2520the%2520real%250Avehicle%2520and%2520multiple%2520other%2520actors%2520like%2520vulnerable%2520road%2520users%2520in%2520different%2520test%250Aareas%2520while%2520being%2520immersed%2520within%2520the%2520same%2520highly%2520realistic%2520virtual%250Aenvironment.%2520This%2520VVE%2520approach%2520synchronizes%2520real-world%2520vehicle%2520and%2520vulnerable%250Aroad%2520user%2520motion%2520within%2520the%2520same%2520virtual%2520scenario%252C%2520enabling%2520the%2520safe%2520and%250Arealistic%2520testing%2520of%2520various%2520traffic%2520situations%2520in%2520a%2520safe%2520and%2520repeatable%250Amanner.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520testing%2520pipeline%2520that%2520sequentially%250Aintegrates%2520MIL%252C%2520HIL%252C%2520and%2520VVE%2520methods%2520to%2520comprehensively%2520develop%2520and%2520evaluate%250Aautonomous%2520driving%2520functions.%2520The%2520effectiveness%2520of%2520this%2520testing%2520pipeline%2520will%250Abe%2520demonstrated%2520using%2520an%2520autonomous%2520driving%2520path-tracking%2520algorithm%2520with%2520local%250Adeep%2520reinforcement%2520learning%2520modification%2520for%2520vulnerable%2520road%2520user%2520collision%250Aavoidance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vehicle-in-Virtual-Environment%20%28VVE%29%20Based%20Autonomous%20Driving%20Function%0A%20%20Development%20and%20Evaluation%20Methodology%20for%20Vulnerable%20Road%20User%20Safety&entry.906535625=Haochong%20Chen%20and%20Xincheng%20Cao%20and%20Levent%20Guvenc%20and%20Bilin%20Aksun%20Guvenc&entry.1292438233=%20%20Traditional%20methods%20for%20developing%20and%20evaluating%20autonomous%20driving%0Afunctions%2C%20such%20as%20model-in-the-loop%20%28MIL%29%20and%20hardware-in-the-loop%20%28HIL%29%0Asimulations%2C%20heavily%20depend%20on%20the%20accuracy%20of%20simulated%20vehicle%20models%20and%0Ahuman%20factors%2C%20especially%20for%20vulnerable%20road%20user%20safety%20systems.%20Continuation%0Aof%20development%20during%20public%20road%20deployment%20forces%20other%20road%20users%20including%0Avulnerable%20ones%20to%20involuntarily%20participate%20in%20the%20development%20process%2C%0Aleading%20to%20safety%20risks%2C%20inefficiencies%2C%20and%20a%20decline%20in%20public%20trust.%20To%0Aaddress%20these%20deficiencies%2C%20the%20Vehicle-in-Virtual-Environment%20%28VVE%29%20method%20was%0Aproposed%20as%20a%20safer%2C%20more%20efficient%2C%20and%20cost-effective%20solution%20for%20developing%0Aand%20testing%20connected%20and%20autonomous%20driving%20technologies%20by%20operating%20the%20real%0Avehicle%20and%20multiple%20other%20actors%20like%20vulnerable%20road%20users%20in%20different%20test%0Aareas%20while%20being%20immersed%20within%20the%20same%20highly%20realistic%20virtual%0Aenvironment.%20This%20VVE%20approach%20synchronizes%20real-world%20vehicle%20and%20vulnerable%0Aroad%20user%20motion%20within%20the%20same%20virtual%20scenario%2C%20enabling%20the%20safe%20and%0Arealistic%20testing%20of%20various%20traffic%20situations%20in%20a%20safe%20and%20repeatable%0Amanner.%20In%20this%20paper%2C%20we%20propose%20a%20new%20testing%20pipeline%20that%20sequentially%0Aintegrates%20MIL%2C%20HIL%2C%20and%20VVE%20methods%20to%20comprehensively%20develop%20and%20evaluate%0Aautonomous%20driving%20functions.%20The%20effectiveness%20of%20this%20testing%20pipeline%20will%0Abe%20demonstrated%20using%20an%20autonomous%20driving%20path-tracking%20algorithm%20with%20local%0Adeep%20reinforcement%20learning%20modification%20for%20vulnerable%20road%20user%20collision%0Aavoidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06113v1&entry.124074799=Read"},
{"title": "Deep Variational Sequential Monte Carlo for High-Dimensional\n  Observations", "author": "Wessel L. van Nierop and Nir Shlezinger and Ruud J. G. van Sloun", "abstract": "  Sequential Monte Carlo (SMC), or particle filtering, is widely used in\nnonlinear state-space systems, but its performance often suffers from poorly\napproximated proposal and state-transition distributions. This work introduces\na differentiable particle filter that leverages the unsupervised variational\nSMC objective to parameterize the proposal and transition distributions with a\nneural network, designed to learn from high-dimensional observations.\nExperimental results demonstrate that our approach outperforms established\nbaselines in tracking the challenging Lorenz attractor from high-dimensional\nand partial observations. Furthermore, an evidence lower bound based evaluation\nindicates that our method offers a more accurate representation of the\nposterior distribution.\n", "link": "http://arxiv.org/abs/2501.05982v1", "date": "2025-01-10", "relevancy": 2.0585, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5573}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5063}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Variational%20Sequential%20Monte%20Carlo%20for%20High-Dimensional%0A%20%20Observations&body=Title%3A%20Deep%20Variational%20Sequential%20Monte%20Carlo%20for%20High-Dimensional%0A%20%20Observations%0AAuthor%3A%20Wessel%20L.%20van%20Nierop%20and%20Nir%20Shlezinger%20and%20Ruud%20J.%20G.%20van%20Sloun%0AAbstract%3A%20%20%20Sequential%20Monte%20Carlo%20%28SMC%29%2C%20or%20particle%20filtering%2C%20is%20widely%20used%20in%0Anonlinear%20state-space%20systems%2C%20but%20its%20performance%20often%20suffers%20from%20poorly%0Aapproximated%20proposal%20and%20state-transition%20distributions.%20This%20work%20introduces%0Aa%20differentiable%20particle%20filter%20that%20leverages%20the%20unsupervised%20variational%0ASMC%20objective%20to%20parameterize%20the%20proposal%20and%20transition%20distributions%20with%20a%0Aneural%20network%2C%20designed%20to%20learn%20from%20high-dimensional%20observations.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20outperforms%20established%0Abaselines%20in%20tracking%20the%20challenging%20Lorenz%20attractor%20from%20high-dimensional%0Aand%20partial%20observations.%20Furthermore%2C%20an%20evidence%20lower%20bound%20based%20evaluation%0Aindicates%20that%20our%20method%20offers%20a%20more%20accurate%20representation%20of%20the%0Aposterior%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Variational%2520Sequential%2520Monte%2520Carlo%2520for%2520High-Dimensional%250A%2520%2520Observations%26entry.906535625%3DWessel%2520L.%2520van%2520Nierop%2520and%2520Nir%2520Shlezinger%2520and%2520Ruud%2520J.%2520G.%2520van%2520Sloun%26entry.1292438233%3D%2520%2520Sequential%2520Monte%2520Carlo%2520%2528SMC%2529%252C%2520or%2520particle%2520filtering%252C%2520is%2520widely%2520used%2520in%250Anonlinear%2520state-space%2520systems%252C%2520but%2520its%2520performance%2520often%2520suffers%2520from%2520poorly%250Aapproximated%2520proposal%2520and%2520state-transition%2520distributions.%2520This%2520work%2520introduces%250Aa%2520differentiable%2520particle%2520filter%2520that%2520leverages%2520the%2520unsupervised%2520variational%250ASMC%2520objective%2520to%2520parameterize%2520the%2520proposal%2520and%2520transition%2520distributions%2520with%2520a%250Aneural%2520network%252C%2520designed%2520to%2520learn%2520from%2520high-dimensional%2520observations.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520established%250Abaselines%2520in%2520tracking%2520the%2520challenging%2520Lorenz%2520attractor%2520from%2520high-dimensional%250Aand%2520partial%2520observations.%2520Furthermore%252C%2520an%2520evidence%2520lower%2520bound%2520based%2520evaluation%250Aindicates%2520that%2520our%2520method%2520offers%2520a%2520more%2520accurate%2520representation%2520of%2520the%250Aposterior%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Variational%20Sequential%20Monte%20Carlo%20for%20High-Dimensional%0A%20%20Observations&entry.906535625=Wessel%20L.%20van%20Nierop%20and%20Nir%20Shlezinger%20and%20Ruud%20J.%20G.%20van%20Sloun&entry.1292438233=%20%20Sequential%20Monte%20Carlo%20%28SMC%29%2C%20or%20particle%20filtering%2C%20is%20widely%20used%20in%0Anonlinear%20state-space%20systems%2C%20but%20its%20performance%20often%20suffers%20from%20poorly%0Aapproximated%20proposal%20and%20state-transition%20distributions.%20This%20work%20introduces%0Aa%20differentiable%20particle%20filter%20that%20leverages%20the%20unsupervised%20variational%0ASMC%20objective%20to%20parameterize%20the%20proposal%20and%20transition%20distributions%20with%20a%0Aneural%20network%2C%20designed%20to%20learn%20from%20high-dimensional%20observations.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20outperforms%20established%0Abaselines%20in%20tracking%20the%20challenging%20Lorenz%20attractor%20from%20high-dimensional%0Aand%20partial%20observations.%20Furthermore%2C%20an%20evidence%20lower%20bound%20based%20evaluation%0Aindicates%20that%20our%20method%20offers%20a%20more%20accurate%20representation%20of%20the%0Aposterior%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05982v1&entry.124074799=Read"},
{"title": "The Harmonic Exponential Filter for Nonparametric Estimation on Motion\n  Groups", "author": "Miguel Saavedra-Ruiz and Steven A. Parkison and Ria Arora and James Richard Forbes and Liam Paull", "abstract": "  Bayesian estimation is a vital tool in robotics as it allows systems to\nupdate the robot state belief using incomplete information from noisy sensors.\nTo render the state estimation problem tractable, many systems assume that the\nmotion and measurement noise, as well as the state distribution, are unimodal\nand Gaussian. However, there are numerous scenarios and systems that do not\ncomply with these assumptions. Existing nonparametric filters that are used to\nmodel multimodal distributions have drawbacks that limit their ability to\nrepresent a diverse set of distributions. This paper introduces a novel\napproach to nonparametric Bayesian filtering on motion groups, designed to\nhandle multimodal distributions using harmonic exponential distributions. This\napproach leverages two key insights of harmonic exponential distributions: a)\nthe product of two distributions can be expressed as the element-wise addition\nof their log-likelihood Fourier coefficients, and b) the convolution of two\ndistributions can be efficiently computed as the tensor product of their\nFourier coefficients. These observations enable the development of an efficient\nand asymptotically exact solution to the Bayes filter up to the band limit of a\nFourier transform. We demonstrate our filter's performance compared with\nestablished nonparametric filtering methods across simulated and real-world\nlocalization tasks.\n", "link": "http://arxiv.org/abs/2408.00907v3", "date": "2025-01-10", "relevancy": 2.0209, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5408}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5025}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Harmonic%20Exponential%20Filter%20for%20Nonparametric%20Estimation%20on%20Motion%0A%20%20Groups&body=Title%3A%20The%20Harmonic%20Exponential%20Filter%20for%20Nonparametric%20Estimation%20on%20Motion%0A%20%20Groups%0AAuthor%3A%20Miguel%20Saavedra-Ruiz%20and%20Steven%20A.%20Parkison%20and%20Ria%20Arora%20and%20James%20Richard%20Forbes%20and%20Liam%20Paull%0AAbstract%3A%20%20%20Bayesian%20estimation%20is%20a%20vital%20tool%20in%20robotics%20as%20it%20allows%20systems%20to%0Aupdate%20the%20robot%20state%20belief%20using%20incomplete%20information%20from%20noisy%20sensors.%0ATo%20render%20the%20state%20estimation%20problem%20tractable%2C%20many%20systems%20assume%20that%20the%0Amotion%20and%20measurement%20noise%2C%20as%20well%20as%20the%20state%20distribution%2C%20are%20unimodal%0Aand%20Gaussian.%20However%2C%20there%20are%20numerous%20scenarios%20and%20systems%20that%20do%20not%0Acomply%20with%20these%20assumptions.%20Existing%20nonparametric%20filters%20that%20are%20used%20to%0Amodel%20multimodal%20distributions%20have%20drawbacks%20that%20limit%20their%20ability%20to%0Arepresent%20a%20diverse%20set%20of%20distributions.%20This%20paper%20introduces%20a%20novel%0Aapproach%20to%20nonparametric%20Bayesian%20filtering%20on%20motion%20groups%2C%20designed%20to%0Ahandle%20multimodal%20distributions%20using%20harmonic%20exponential%20distributions.%20This%0Aapproach%20leverages%20two%20key%20insights%20of%20harmonic%20exponential%20distributions%3A%20a%29%0Athe%20product%20of%20two%20distributions%20can%20be%20expressed%20as%20the%20element-wise%20addition%0Aof%20their%20log-likelihood%20Fourier%20coefficients%2C%20and%20b%29%20the%20convolution%20of%20two%0Adistributions%20can%20be%20efficiently%20computed%20as%20the%20tensor%20product%20of%20their%0AFourier%20coefficients.%20These%20observations%20enable%20the%20development%20of%20an%20efficient%0Aand%20asymptotically%20exact%20solution%20to%20the%20Bayes%20filter%20up%20to%20the%20band%20limit%20of%20a%0AFourier%20transform.%20We%20demonstrate%20our%20filter%27s%20performance%20compared%20with%0Aestablished%20nonparametric%20filtering%20methods%20across%20simulated%20and%20real-world%0Alocalization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00907v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Harmonic%2520Exponential%2520Filter%2520for%2520Nonparametric%2520Estimation%2520on%2520Motion%250A%2520%2520Groups%26entry.906535625%3DMiguel%2520Saavedra-Ruiz%2520and%2520Steven%2520A.%2520Parkison%2520and%2520Ria%2520Arora%2520and%2520James%2520Richard%2520Forbes%2520and%2520Liam%2520Paull%26entry.1292438233%3D%2520%2520Bayesian%2520estimation%2520is%2520a%2520vital%2520tool%2520in%2520robotics%2520as%2520it%2520allows%2520systems%2520to%250Aupdate%2520the%2520robot%2520state%2520belief%2520using%2520incomplete%2520information%2520from%2520noisy%2520sensors.%250ATo%2520render%2520the%2520state%2520estimation%2520problem%2520tractable%252C%2520many%2520systems%2520assume%2520that%2520the%250Amotion%2520and%2520measurement%2520noise%252C%2520as%2520well%2520as%2520the%2520state%2520distribution%252C%2520are%2520unimodal%250Aand%2520Gaussian.%2520However%252C%2520there%2520are%2520numerous%2520scenarios%2520and%2520systems%2520that%2520do%2520not%250Acomply%2520with%2520these%2520assumptions.%2520Existing%2520nonparametric%2520filters%2520that%2520are%2520used%2520to%250Amodel%2520multimodal%2520distributions%2520have%2520drawbacks%2520that%2520limit%2520their%2520ability%2520to%250Arepresent%2520a%2520diverse%2520set%2520of%2520distributions.%2520This%2520paper%2520introduces%2520a%2520novel%250Aapproach%2520to%2520nonparametric%2520Bayesian%2520filtering%2520on%2520motion%2520groups%252C%2520designed%2520to%250Ahandle%2520multimodal%2520distributions%2520using%2520harmonic%2520exponential%2520distributions.%2520This%250Aapproach%2520leverages%2520two%2520key%2520insights%2520of%2520harmonic%2520exponential%2520distributions%253A%2520a%2529%250Athe%2520product%2520of%2520two%2520distributions%2520can%2520be%2520expressed%2520as%2520the%2520element-wise%2520addition%250Aof%2520their%2520log-likelihood%2520Fourier%2520coefficients%252C%2520and%2520b%2529%2520the%2520convolution%2520of%2520two%250Adistributions%2520can%2520be%2520efficiently%2520computed%2520as%2520the%2520tensor%2520product%2520of%2520their%250AFourier%2520coefficients.%2520These%2520observations%2520enable%2520the%2520development%2520of%2520an%2520efficient%250Aand%2520asymptotically%2520exact%2520solution%2520to%2520the%2520Bayes%2520filter%2520up%2520to%2520the%2520band%2520limit%2520of%2520a%250AFourier%2520transform.%2520We%2520demonstrate%2520our%2520filter%2527s%2520performance%2520compared%2520with%250Aestablished%2520nonparametric%2520filtering%2520methods%2520across%2520simulated%2520and%2520real-world%250Alocalization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00907v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Harmonic%20Exponential%20Filter%20for%20Nonparametric%20Estimation%20on%20Motion%0A%20%20Groups&entry.906535625=Miguel%20Saavedra-Ruiz%20and%20Steven%20A.%20Parkison%20and%20Ria%20Arora%20and%20James%20Richard%20Forbes%20and%20Liam%20Paull&entry.1292438233=%20%20Bayesian%20estimation%20is%20a%20vital%20tool%20in%20robotics%20as%20it%20allows%20systems%20to%0Aupdate%20the%20robot%20state%20belief%20using%20incomplete%20information%20from%20noisy%20sensors.%0ATo%20render%20the%20state%20estimation%20problem%20tractable%2C%20many%20systems%20assume%20that%20the%0Amotion%20and%20measurement%20noise%2C%20as%20well%20as%20the%20state%20distribution%2C%20are%20unimodal%0Aand%20Gaussian.%20However%2C%20there%20are%20numerous%20scenarios%20and%20systems%20that%20do%20not%0Acomply%20with%20these%20assumptions.%20Existing%20nonparametric%20filters%20that%20are%20used%20to%0Amodel%20multimodal%20distributions%20have%20drawbacks%20that%20limit%20their%20ability%20to%0Arepresent%20a%20diverse%20set%20of%20distributions.%20This%20paper%20introduces%20a%20novel%0Aapproach%20to%20nonparametric%20Bayesian%20filtering%20on%20motion%20groups%2C%20designed%20to%0Ahandle%20multimodal%20distributions%20using%20harmonic%20exponential%20distributions.%20This%0Aapproach%20leverages%20two%20key%20insights%20of%20harmonic%20exponential%20distributions%3A%20a%29%0Athe%20product%20of%20two%20distributions%20can%20be%20expressed%20as%20the%20element-wise%20addition%0Aof%20their%20log-likelihood%20Fourier%20coefficients%2C%20and%20b%29%20the%20convolution%20of%20two%0Adistributions%20can%20be%20efficiently%20computed%20as%20the%20tensor%20product%20of%20their%0AFourier%20coefficients.%20These%20observations%20enable%20the%20development%20of%20an%20efficient%0Aand%20asymptotically%20exact%20solution%20to%20the%20Bayes%20filter%20up%20to%20the%20band%20limit%20of%20a%0AFourier%20transform.%20We%20demonstrate%20our%20filter%27s%20performance%20compared%20with%0Aestablished%20nonparametric%20filtering%20methods%20across%20simulated%20and%20real-world%0Alocalization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00907v3&entry.124074799=Read"},
{"title": "Multi-subject Open-set Personalization in Video Generation", "author": "Tsai-Shien Chen and Aliaksandr Siarohin and Willi Menapace and Yuwei Fang and Kwot Sin Lee and Ivan Skorokhodov and Kfir Aberman and Jun-Yan Zhu and Ming-Hsuan Yang and Sergey Tulyakov", "abstract": "  Video personalization methods allow us to synthesize videos with specific\nconcepts such as people, pets, and places. However, existing methods often\nfocus on limited domains, require time-consuming optimization per subject, or\nsupport only a single subject. We present Video Alchemist $-$ a video model\nwith built-in multi-subject, open-set personalization capabilities for both\nforeground objects and background, eliminating the need for time-consuming\ntest-time optimization. Our model is built on a new Diffusion Transformer\nmodule that fuses each conditional reference image and its corresponding\nsubject-level text prompt with cross-attention layers. Developing such a large\nmodel presents two main challenges: dataset and evaluation. First, as paired\ndatasets of reference images and videos are extremely hard to collect, we\nsample selected video frames as reference images and synthesize a clip of the\ntarget video. However, while models can easily denoise training videos given\nreference frames, they fail to generalize to new contexts. To mitigate this\nissue, we design a new automatic data construction pipeline with extensive\nimage augmentations. Second, evaluating open-set video personalization is a\nchallenge in itself. To address this, we introduce a personalization benchmark\nthat focuses on accurate subject fidelity and supports diverse personalization\nscenarios. Finally, our extensive experiments show that our method\nsignificantly outperforms existing personalization methods in both quantitative\nand qualitative evaluations.\n", "link": "http://arxiv.org/abs/2501.06187v1", "date": "2025-01-10", "relevancy": 2.002, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7005}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6804}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-subject%20Open-set%20Personalization%20in%20Video%20Generation&body=Title%3A%20Multi-subject%20Open-set%20Personalization%20in%20Video%20Generation%0AAuthor%3A%20Tsai-Shien%20Chen%20and%20Aliaksandr%20Siarohin%20and%20Willi%20Menapace%20and%20Yuwei%20Fang%20and%20Kwot%20Sin%20Lee%20and%20Ivan%20Skorokhodov%20and%20Kfir%20Aberman%20and%20Jun-Yan%20Zhu%20and%20Ming-Hsuan%20Yang%20and%20Sergey%20Tulyakov%0AAbstract%3A%20%20%20Video%20personalization%20methods%20allow%20us%20to%20synthesize%20videos%20with%20specific%0Aconcepts%20such%20as%20people%2C%20pets%2C%20and%20places.%20However%2C%20existing%20methods%20often%0Afocus%20on%20limited%20domains%2C%20require%20time-consuming%20optimization%20per%20subject%2C%20or%0Asupport%20only%20a%20single%20subject.%20We%20present%20Video%20Alchemist%20%24-%24%20a%20video%20model%0Awith%20built-in%20multi-subject%2C%20open-set%20personalization%20capabilities%20for%20both%0Aforeground%20objects%20and%20background%2C%20eliminating%20the%20need%20for%20time-consuming%0Atest-time%20optimization.%20Our%20model%20is%20built%20on%20a%20new%20Diffusion%20Transformer%0Amodule%20that%20fuses%20each%20conditional%20reference%20image%20and%20its%20corresponding%0Asubject-level%20text%20prompt%20with%20cross-attention%20layers.%20Developing%20such%20a%20large%0Amodel%20presents%20two%20main%20challenges%3A%20dataset%20and%20evaluation.%20First%2C%20as%20paired%0Adatasets%20of%20reference%20images%20and%20videos%20are%20extremely%20hard%20to%20collect%2C%20we%0Asample%20selected%20video%20frames%20as%20reference%20images%20and%20synthesize%20a%20clip%20of%20the%0Atarget%20video.%20However%2C%20while%20models%20can%20easily%20denoise%20training%20videos%20given%0Areference%20frames%2C%20they%20fail%20to%20generalize%20to%20new%20contexts.%20To%20mitigate%20this%0Aissue%2C%20we%20design%20a%20new%20automatic%20data%20construction%20pipeline%20with%20extensive%0Aimage%20augmentations.%20Second%2C%20evaluating%20open-set%20video%20personalization%20is%20a%0Achallenge%20in%20itself.%20To%20address%20this%2C%20we%20introduce%20a%20personalization%20benchmark%0Athat%20focuses%20on%20accurate%20subject%20fidelity%20and%20supports%20diverse%20personalization%0Ascenarios.%20Finally%2C%20our%20extensive%20experiments%20show%20that%20our%20method%0Asignificantly%20outperforms%20existing%20personalization%20methods%20in%20both%20quantitative%0Aand%20qualitative%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-subject%2520Open-set%2520Personalization%2520in%2520Video%2520Generation%26entry.906535625%3DTsai-Shien%2520Chen%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Willi%2520Menapace%2520and%2520Yuwei%2520Fang%2520and%2520Kwot%2520Sin%2520Lee%2520and%2520Ivan%2520Skorokhodov%2520and%2520Kfir%2520Aberman%2520and%2520Jun-Yan%2520Zhu%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Sergey%2520Tulyakov%26entry.1292438233%3D%2520%2520Video%2520personalization%2520methods%2520allow%2520us%2520to%2520synthesize%2520videos%2520with%2520specific%250Aconcepts%2520such%2520as%2520people%252C%2520pets%252C%2520and%2520places.%2520However%252C%2520existing%2520methods%2520often%250Afocus%2520on%2520limited%2520domains%252C%2520require%2520time-consuming%2520optimization%2520per%2520subject%252C%2520or%250Asupport%2520only%2520a%2520single%2520subject.%2520We%2520present%2520Video%2520Alchemist%2520%2524-%2524%2520a%2520video%2520model%250Awith%2520built-in%2520multi-subject%252C%2520open-set%2520personalization%2520capabilities%2520for%2520both%250Aforeground%2520objects%2520and%2520background%252C%2520eliminating%2520the%2520need%2520for%2520time-consuming%250Atest-time%2520optimization.%2520Our%2520model%2520is%2520built%2520on%2520a%2520new%2520Diffusion%2520Transformer%250Amodule%2520that%2520fuses%2520each%2520conditional%2520reference%2520image%2520and%2520its%2520corresponding%250Asubject-level%2520text%2520prompt%2520with%2520cross-attention%2520layers.%2520Developing%2520such%2520a%2520large%250Amodel%2520presents%2520two%2520main%2520challenges%253A%2520dataset%2520and%2520evaluation.%2520First%252C%2520as%2520paired%250Adatasets%2520of%2520reference%2520images%2520and%2520videos%2520are%2520extremely%2520hard%2520to%2520collect%252C%2520we%250Asample%2520selected%2520video%2520frames%2520as%2520reference%2520images%2520and%2520synthesize%2520a%2520clip%2520of%2520the%250Atarget%2520video.%2520However%252C%2520while%2520models%2520can%2520easily%2520denoise%2520training%2520videos%2520given%250Areference%2520frames%252C%2520they%2520fail%2520to%2520generalize%2520to%2520new%2520contexts.%2520To%2520mitigate%2520this%250Aissue%252C%2520we%2520design%2520a%2520new%2520automatic%2520data%2520construction%2520pipeline%2520with%2520extensive%250Aimage%2520augmentations.%2520Second%252C%2520evaluating%2520open-set%2520video%2520personalization%2520is%2520a%250Achallenge%2520in%2520itself.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520personalization%2520benchmark%250Athat%2520focuses%2520on%2520accurate%2520subject%2520fidelity%2520and%2520supports%2520diverse%2520personalization%250Ascenarios.%2520Finally%252C%2520our%2520extensive%2520experiments%2520show%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520existing%2520personalization%2520methods%2520in%2520both%2520quantitative%250Aand%2520qualitative%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-subject%20Open-set%20Personalization%20in%20Video%20Generation&entry.906535625=Tsai-Shien%20Chen%20and%20Aliaksandr%20Siarohin%20and%20Willi%20Menapace%20and%20Yuwei%20Fang%20and%20Kwot%20Sin%20Lee%20and%20Ivan%20Skorokhodov%20and%20Kfir%20Aberman%20and%20Jun-Yan%20Zhu%20and%20Ming-Hsuan%20Yang%20and%20Sergey%20Tulyakov&entry.1292438233=%20%20Video%20personalization%20methods%20allow%20us%20to%20synthesize%20videos%20with%20specific%0Aconcepts%20such%20as%20people%2C%20pets%2C%20and%20places.%20However%2C%20existing%20methods%20often%0Afocus%20on%20limited%20domains%2C%20require%20time-consuming%20optimization%20per%20subject%2C%20or%0Asupport%20only%20a%20single%20subject.%20We%20present%20Video%20Alchemist%20%24-%24%20a%20video%20model%0Awith%20built-in%20multi-subject%2C%20open-set%20personalization%20capabilities%20for%20both%0Aforeground%20objects%20and%20background%2C%20eliminating%20the%20need%20for%20time-consuming%0Atest-time%20optimization.%20Our%20model%20is%20built%20on%20a%20new%20Diffusion%20Transformer%0Amodule%20that%20fuses%20each%20conditional%20reference%20image%20and%20its%20corresponding%0Asubject-level%20text%20prompt%20with%20cross-attention%20layers.%20Developing%20such%20a%20large%0Amodel%20presents%20two%20main%20challenges%3A%20dataset%20and%20evaluation.%20First%2C%20as%20paired%0Adatasets%20of%20reference%20images%20and%20videos%20are%20extremely%20hard%20to%20collect%2C%20we%0Asample%20selected%20video%20frames%20as%20reference%20images%20and%20synthesize%20a%20clip%20of%20the%0Atarget%20video.%20However%2C%20while%20models%20can%20easily%20denoise%20training%20videos%20given%0Areference%20frames%2C%20they%20fail%20to%20generalize%20to%20new%20contexts.%20To%20mitigate%20this%0Aissue%2C%20we%20design%20a%20new%20automatic%20data%20construction%20pipeline%20with%20extensive%0Aimage%20augmentations.%20Second%2C%20evaluating%20open-set%20video%20personalization%20is%20a%0Achallenge%20in%20itself.%20To%20address%20this%2C%20we%20introduce%20a%20personalization%20benchmark%0Athat%20focuses%20on%20accurate%20subject%20fidelity%20and%20supports%20diverse%20personalization%0Ascenarios.%20Finally%2C%20our%20extensive%20experiments%20show%20that%20our%20method%0Asignificantly%20outperforms%20existing%20personalization%20methods%20in%20both%20quantitative%0Aand%20qualitative%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06187v1&entry.124074799=Read"},
{"title": "Meta-Learning for Physically-Constrained Neural System Identification", "author": "Ankush Chakrabarty and Gordon Wichern and Vedang M. Deshpande and Abraham P. Vinod and Karl Berntorp and Christopher R. Laughman", "abstract": "  We present a gradient-based meta-learning framework for rapid adaptation of\nneural state-space models (NSSMs) for black-box system identification. When\napplicable, we also incorporate domain-specific physical constraints to improve\nthe accuracy of the NSSM. The major benefit of our approach is that instead of\nrelying solely on data from a single target system, our framework utilizes data\nfrom a diverse set of source systems, enabling learning from limited target\ndata, as well as with few online training iterations. Through benchmark\nexamples, we demonstrate the potential of our approach, study the effect of\nfine-tuning subnetworks rather than full fine-tuning, and report real-world\ncase studies to illustrate the practical application and generalizability of\nthe approach to practical problems with physical-constraints. Specifically, we\nshow that the meta-learned models result in improved downstream performance in\nmodel-based state estimation in indoor localization and energy systems.\n", "link": "http://arxiv.org/abs/2501.06167v1", "date": "2025-01-10", "relevancy": 2.0019, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5635}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4901}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning%20for%20Physically-Constrained%20Neural%20System%20Identification&body=Title%3A%20Meta-Learning%20for%20Physically-Constrained%20Neural%20System%20Identification%0AAuthor%3A%20Ankush%20Chakrabarty%20and%20Gordon%20Wichern%20and%20Vedang%20M.%20Deshpande%20and%20Abraham%20P.%20Vinod%20and%20Karl%20Berntorp%20and%20Christopher%20R.%20Laughman%0AAbstract%3A%20%20%20We%20present%20a%20gradient-based%20meta-learning%20framework%20for%20rapid%20adaptation%20of%0Aneural%20state-space%20models%20%28NSSMs%29%20for%20black-box%20system%20identification.%20When%0Aapplicable%2C%20we%20also%20incorporate%20domain-specific%20physical%20constraints%20to%20improve%0Athe%20accuracy%20of%20the%20NSSM.%20The%20major%20benefit%20of%20our%20approach%20is%20that%20instead%20of%0Arelying%20solely%20on%20data%20from%20a%20single%20target%20system%2C%20our%20framework%20utilizes%20data%0Afrom%20a%20diverse%20set%20of%20source%20systems%2C%20enabling%20learning%20from%20limited%20target%0Adata%2C%20as%20well%20as%20with%20few%20online%20training%20iterations.%20Through%20benchmark%0Aexamples%2C%20we%20demonstrate%20the%20potential%20of%20our%20approach%2C%20study%20the%20effect%20of%0Afine-tuning%20subnetworks%20rather%20than%20full%20fine-tuning%2C%20and%20report%20real-world%0Acase%20studies%20to%20illustrate%20the%20practical%20application%20and%20generalizability%20of%0Athe%20approach%20to%20practical%20problems%20with%20physical-constraints.%20Specifically%2C%20we%0Ashow%20that%20the%20meta-learned%20models%20result%20in%20improved%20downstream%20performance%20in%0Amodel-based%20state%20estimation%20in%20indoor%20localization%20and%20energy%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning%2520for%2520Physically-Constrained%2520Neural%2520System%2520Identification%26entry.906535625%3DAnkush%2520Chakrabarty%2520and%2520Gordon%2520Wichern%2520and%2520Vedang%2520M.%2520Deshpande%2520and%2520Abraham%2520P.%2520Vinod%2520and%2520Karl%2520Berntorp%2520and%2520Christopher%2520R.%2520Laughman%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520gradient-based%2520meta-learning%2520framework%2520for%2520rapid%2520adaptation%2520of%250Aneural%2520state-space%2520models%2520%2528NSSMs%2529%2520for%2520black-box%2520system%2520identification.%2520When%250Aapplicable%252C%2520we%2520also%2520incorporate%2520domain-specific%2520physical%2520constraints%2520to%2520improve%250Athe%2520accuracy%2520of%2520the%2520NSSM.%2520The%2520major%2520benefit%2520of%2520our%2520approach%2520is%2520that%2520instead%2520of%250Arelying%2520solely%2520on%2520data%2520from%2520a%2520single%2520target%2520system%252C%2520our%2520framework%2520utilizes%2520data%250Afrom%2520a%2520diverse%2520set%2520of%2520source%2520systems%252C%2520enabling%2520learning%2520from%2520limited%2520target%250Adata%252C%2520as%2520well%2520as%2520with%2520few%2520online%2520training%2520iterations.%2520Through%2520benchmark%250Aexamples%252C%2520we%2520demonstrate%2520the%2520potential%2520of%2520our%2520approach%252C%2520study%2520the%2520effect%2520of%250Afine-tuning%2520subnetworks%2520rather%2520than%2520full%2520fine-tuning%252C%2520and%2520report%2520real-world%250Acase%2520studies%2520to%2520illustrate%2520the%2520practical%2520application%2520and%2520generalizability%2520of%250Athe%2520approach%2520to%2520practical%2520problems%2520with%2520physical-constraints.%2520Specifically%252C%2520we%250Ashow%2520that%2520the%2520meta-learned%2520models%2520result%2520in%2520improved%2520downstream%2520performance%2520in%250Amodel-based%2520state%2520estimation%2520in%2520indoor%2520localization%2520and%2520energy%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning%20for%20Physically-Constrained%20Neural%20System%20Identification&entry.906535625=Ankush%20Chakrabarty%20and%20Gordon%20Wichern%20and%20Vedang%20M.%20Deshpande%20and%20Abraham%20P.%20Vinod%20and%20Karl%20Berntorp%20and%20Christopher%20R.%20Laughman&entry.1292438233=%20%20We%20present%20a%20gradient-based%20meta-learning%20framework%20for%20rapid%20adaptation%20of%0Aneural%20state-space%20models%20%28NSSMs%29%20for%20black-box%20system%20identification.%20When%0Aapplicable%2C%20we%20also%20incorporate%20domain-specific%20physical%20constraints%20to%20improve%0Athe%20accuracy%20of%20the%20NSSM.%20The%20major%20benefit%20of%20our%20approach%20is%20that%20instead%20of%0Arelying%20solely%20on%20data%20from%20a%20single%20target%20system%2C%20our%20framework%20utilizes%20data%0Afrom%20a%20diverse%20set%20of%20source%20systems%2C%20enabling%20learning%20from%20limited%20target%0Adata%2C%20as%20well%20as%20with%20few%20online%20training%20iterations.%20Through%20benchmark%0Aexamples%2C%20we%20demonstrate%20the%20potential%20of%20our%20approach%2C%20study%20the%20effect%20of%0Afine-tuning%20subnetworks%20rather%20than%20full%20fine-tuning%2C%20and%20report%20real-world%0Acase%20studies%20to%20illustrate%20the%20practical%20application%20and%20generalizability%20of%0Athe%20approach%20to%20practical%20problems%20with%20physical-constraints.%20Specifically%2C%20we%0Ashow%20that%20the%20meta-learned%20models%20result%20in%20improved%20downstream%20performance%20in%0Amodel-based%20state%20estimation%20in%20indoor%20localization%20and%20energy%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06167v1&entry.124074799=Read"},
{"title": "Gender Bias in Text-to-Video Generation Models: A case study of Sora", "author": "Mohammad Nadeem and Shahab Saquib Sohail and Erik Cambria and Bj\u00f6rn W. Schuller and Amir Hussain", "abstract": "  The advent of text-to-video generation models has revolutionized content\ncreation as it produces high-quality videos from textual prompts. However,\nconcerns regarding inherent biases in such models have prompted scrutiny,\nparticularly regarding gender representation. Our study investigates the\npresence of gender bias in OpenAI's Sora, a state-of-the-art text-to-video\ngeneration model. We uncover significant evidence of bias by analyzing the\ngenerated videos from a diverse set of gender-neutral and stereotypical\nprompts. The results indicate that Sora disproportionately associates specific\ngenders with stereotypical behaviors and professions, which reflects societal\nprejudices embedded in its training data.\n", "link": "http://arxiv.org/abs/2501.01987v2", "date": "2025-01-10", "relevancy": 1.9936, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5037}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4995}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gender%20Bias%20in%20Text-to-Video%20Generation%20Models%3A%20A%20case%20study%20of%20Sora&body=Title%3A%20Gender%20Bias%20in%20Text-to-Video%20Generation%20Models%3A%20A%20case%20study%20of%20Sora%0AAuthor%3A%20Mohammad%20Nadeem%20and%20Shahab%20Saquib%20Sohail%20and%20Erik%20Cambria%20and%20Bj%C3%B6rn%20W.%20Schuller%20and%20Amir%20Hussain%0AAbstract%3A%20%20%20The%20advent%20of%20text-to-video%20generation%20models%20has%20revolutionized%20content%0Acreation%20as%20it%20produces%20high-quality%20videos%20from%20textual%20prompts.%20However%2C%0Aconcerns%20regarding%20inherent%20biases%20in%20such%20models%20have%20prompted%20scrutiny%2C%0Aparticularly%20regarding%20gender%20representation.%20Our%20study%20investigates%20the%0Apresence%20of%20gender%20bias%20in%20OpenAI%27s%20Sora%2C%20a%20state-of-the-art%20text-to-video%0Ageneration%20model.%20We%20uncover%20significant%20evidence%20of%20bias%20by%20analyzing%20the%0Agenerated%20videos%20from%20a%20diverse%20set%20of%20gender-neutral%20and%20stereotypical%0Aprompts.%20The%20results%20indicate%20that%20Sora%20disproportionately%20associates%20specific%0Agenders%20with%20stereotypical%20behaviors%20and%20professions%2C%20which%20reflects%20societal%0Aprejudices%20embedded%20in%20its%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01987v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGender%2520Bias%2520in%2520Text-to-Video%2520Generation%2520Models%253A%2520A%2520case%2520study%2520of%2520Sora%26entry.906535625%3DMohammad%2520Nadeem%2520and%2520Shahab%2520Saquib%2520Sohail%2520and%2520Erik%2520Cambria%2520and%2520Bj%25C3%25B6rn%2520W.%2520Schuller%2520and%2520Amir%2520Hussain%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520text-to-video%2520generation%2520models%2520has%2520revolutionized%2520content%250Acreation%2520as%2520it%2520produces%2520high-quality%2520videos%2520from%2520textual%2520prompts.%2520However%252C%250Aconcerns%2520regarding%2520inherent%2520biases%2520in%2520such%2520models%2520have%2520prompted%2520scrutiny%252C%250Aparticularly%2520regarding%2520gender%2520representation.%2520Our%2520study%2520investigates%2520the%250Apresence%2520of%2520gender%2520bias%2520in%2520OpenAI%2527s%2520Sora%252C%2520a%2520state-of-the-art%2520text-to-video%250Ageneration%2520model.%2520We%2520uncover%2520significant%2520evidence%2520of%2520bias%2520by%2520analyzing%2520the%250Agenerated%2520videos%2520from%2520a%2520diverse%2520set%2520of%2520gender-neutral%2520and%2520stereotypical%250Aprompts.%2520The%2520results%2520indicate%2520that%2520Sora%2520disproportionately%2520associates%2520specific%250Agenders%2520with%2520stereotypical%2520behaviors%2520and%2520professions%252C%2520which%2520reflects%2520societal%250Aprejudices%2520embedded%2520in%2520its%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01987v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gender%20Bias%20in%20Text-to-Video%20Generation%20Models%3A%20A%20case%20study%20of%20Sora&entry.906535625=Mohammad%20Nadeem%20and%20Shahab%20Saquib%20Sohail%20and%20Erik%20Cambria%20and%20Bj%C3%B6rn%20W.%20Schuller%20and%20Amir%20Hussain&entry.1292438233=%20%20The%20advent%20of%20text-to-video%20generation%20models%20has%20revolutionized%20content%0Acreation%20as%20it%20produces%20high-quality%20videos%20from%20textual%20prompts.%20However%2C%0Aconcerns%20regarding%20inherent%20biases%20in%20such%20models%20have%20prompted%20scrutiny%2C%0Aparticularly%20regarding%20gender%20representation.%20Our%20study%20investigates%20the%0Apresence%20of%20gender%20bias%20in%20OpenAI%27s%20Sora%2C%20a%20state-of-the-art%20text-to-video%0Ageneration%20model.%20We%20uncover%20significant%20evidence%20of%20bias%20by%20analyzing%20the%0Agenerated%20videos%20from%20a%20diverse%20set%20of%20gender-neutral%20and%20stereotypical%0Aprompts.%20The%20results%20indicate%20that%20Sora%20disproportionately%20associates%20specific%0Agenders%20with%20stereotypical%20behaviors%20and%20professions%2C%20which%20reflects%20societal%0Aprejudices%20embedded%20in%20its%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01987v2&entry.124074799=Read"},
{"title": "Dr. Tongue: Sign-Oriented Multi-label Detection for Remote Tongue\n  Diagnosis", "author": "Yiliang Chen and Steven SC Ho and Cheng Xu and Yao Jie Xie and Wing-Fai Yeung and Shengfeng He and Jing Qin", "abstract": "  Tongue diagnosis is a vital tool in Western and Traditional Chinese Medicine,\nproviding key insights into a patient's health by analyzing tongue attributes.\nThe COVID-19 pandemic has heightened the need for accurate remote medical\nassessments, emphasizing the importance of precise tongue attribute recognition\nvia telehealth. To address this, we propose a Sign-Oriented multi-label\nAttributes Detection framework. Our approach begins with an adaptive tongue\nfeature extraction module that standardizes tongue images and mitigates\nenvironmental factors. This is followed by a Sign-oriented Network (SignNet)\nthat identifies specific tongue attributes, emulating the diagnostic process of\nexperienced practitioners and enabling comprehensive health evaluations. To\nvalidate our methodology, we developed an extensive tongue image dataset\nspecifically designed for telemedicine. Unlike existing datasets, ours is\ntailored for remote diagnosis, with a comprehensive set of attribute labels.\nThis dataset will be openly available, providing a valuable resource for\nresearch. Initial tests have shown improved accuracy in detecting various\ntongue attributes, highlighting our framework's potential as an essential tool\nfor remote medical assessments.\n", "link": "http://arxiv.org/abs/2501.03053v2", "date": "2025-01-10", "relevancy": 1.958, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4914}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dr.%20Tongue%3A%20Sign-Oriented%20Multi-label%20Detection%20for%20Remote%20Tongue%0A%20%20Diagnosis&body=Title%3A%20Dr.%20Tongue%3A%20Sign-Oriented%20Multi-label%20Detection%20for%20Remote%20Tongue%0A%20%20Diagnosis%0AAuthor%3A%20Yiliang%20Chen%20and%20Steven%20SC%20Ho%20and%20Cheng%20Xu%20and%20Yao%20Jie%20Xie%20and%20Wing-Fai%20Yeung%20and%20Shengfeng%20He%20and%20Jing%20Qin%0AAbstract%3A%20%20%20Tongue%20diagnosis%20is%20a%20vital%20tool%20in%20Western%20and%20Traditional%20Chinese%20Medicine%2C%0Aproviding%20key%20insights%20into%20a%20patient%27s%20health%20by%20analyzing%20tongue%20attributes.%0AThe%20COVID-19%20pandemic%20has%20heightened%20the%20need%20for%20accurate%20remote%20medical%0Aassessments%2C%20emphasizing%20the%20importance%20of%20precise%20tongue%20attribute%20recognition%0Avia%20telehealth.%20To%20address%20this%2C%20we%20propose%20a%20Sign-Oriented%20multi-label%0AAttributes%20Detection%20framework.%20Our%20approach%20begins%20with%20an%20adaptive%20tongue%0Afeature%20extraction%20module%20that%20standardizes%20tongue%20images%20and%20mitigates%0Aenvironmental%20factors.%20This%20is%20followed%20by%20a%20Sign-oriented%20Network%20%28SignNet%29%0Athat%20identifies%20specific%20tongue%20attributes%2C%20emulating%20the%20diagnostic%20process%20of%0Aexperienced%20practitioners%20and%20enabling%20comprehensive%20health%20evaluations.%20To%0Avalidate%20our%20methodology%2C%20we%20developed%20an%20extensive%20tongue%20image%20dataset%0Aspecifically%20designed%20for%20telemedicine.%20Unlike%20existing%20datasets%2C%20ours%20is%0Atailored%20for%20remote%20diagnosis%2C%20with%20a%20comprehensive%20set%20of%20attribute%20labels.%0AThis%20dataset%20will%20be%20openly%20available%2C%20providing%20a%20valuable%20resource%20for%0Aresearch.%20Initial%20tests%20have%20shown%20improved%20accuracy%20in%20detecting%20various%0Atongue%20attributes%2C%20highlighting%20our%20framework%27s%20potential%20as%20an%20essential%20tool%0Afor%20remote%20medical%20assessments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03053v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDr.%2520Tongue%253A%2520Sign-Oriented%2520Multi-label%2520Detection%2520for%2520Remote%2520Tongue%250A%2520%2520Diagnosis%26entry.906535625%3DYiliang%2520Chen%2520and%2520Steven%2520SC%2520Ho%2520and%2520Cheng%2520Xu%2520and%2520Yao%2520Jie%2520Xie%2520and%2520Wing-Fai%2520Yeung%2520and%2520Shengfeng%2520He%2520and%2520Jing%2520Qin%26entry.1292438233%3D%2520%2520Tongue%2520diagnosis%2520is%2520a%2520vital%2520tool%2520in%2520Western%2520and%2520Traditional%2520Chinese%2520Medicine%252C%250Aproviding%2520key%2520insights%2520into%2520a%2520patient%2527s%2520health%2520by%2520analyzing%2520tongue%2520attributes.%250AThe%2520COVID-19%2520pandemic%2520has%2520heightened%2520the%2520need%2520for%2520accurate%2520remote%2520medical%250Aassessments%252C%2520emphasizing%2520the%2520importance%2520of%2520precise%2520tongue%2520attribute%2520recognition%250Avia%2520telehealth.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Sign-Oriented%2520multi-label%250AAttributes%2520Detection%2520framework.%2520Our%2520approach%2520begins%2520with%2520an%2520adaptive%2520tongue%250Afeature%2520extraction%2520module%2520that%2520standardizes%2520tongue%2520images%2520and%2520mitigates%250Aenvironmental%2520factors.%2520This%2520is%2520followed%2520by%2520a%2520Sign-oriented%2520Network%2520%2528SignNet%2529%250Athat%2520identifies%2520specific%2520tongue%2520attributes%252C%2520emulating%2520the%2520diagnostic%2520process%2520of%250Aexperienced%2520practitioners%2520and%2520enabling%2520comprehensive%2520health%2520evaluations.%2520To%250Avalidate%2520our%2520methodology%252C%2520we%2520developed%2520an%2520extensive%2520tongue%2520image%2520dataset%250Aspecifically%2520designed%2520for%2520telemedicine.%2520Unlike%2520existing%2520datasets%252C%2520ours%2520is%250Atailored%2520for%2520remote%2520diagnosis%252C%2520with%2520a%2520comprehensive%2520set%2520of%2520attribute%2520labels.%250AThis%2520dataset%2520will%2520be%2520openly%2520available%252C%2520providing%2520a%2520valuable%2520resource%2520for%250Aresearch.%2520Initial%2520tests%2520have%2520shown%2520improved%2520accuracy%2520in%2520detecting%2520various%250Atongue%2520attributes%252C%2520highlighting%2520our%2520framework%2527s%2520potential%2520as%2520an%2520essential%2520tool%250Afor%2520remote%2520medical%2520assessments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03053v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dr.%20Tongue%3A%20Sign-Oriented%20Multi-label%20Detection%20for%20Remote%20Tongue%0A%20%20Diagnosis&entry.906535625=Yiliang%20Chen%20and%20Steven%20SC%20Ho%20and%20Cheng%20Xu%20and%20Yao%20Jie%20Xie%20and%20Wing-Fai%20Yeung%20and%20Shengfeng%20He%20and%20Jing%20Qin&entry.1292438233=%20%20Tongue%20diagnosis%20is%20a%20vital%20tool%20in%20Western%20and%20Traditional%20Chinese%20Medicine%2C%0Aproviding%20key%20insights%20into%20a%20patient%27s%20health%20by%20analyzing%20tongue%20attributes.%0AThe%20COVID-19%20pandemic%20has%20heightened%20the%20need%20for%20accurate%20remote%20medical%0Aassessments%2C%20emphasizing%20the%20importance%20of%20precise%20tongue%20attribute%20recognition%0Avia%20telehealth.%20To%20address%20this%2C%20we%20propose%20a%20Sign-Oriented%20multi-label%0AAttributes%20Detection%20framework.%20Our%20approach%20begins%20with%20an%20adaptive%20tongue%0Afeature%20extraction%20module%20that%20standardizes%20tongue%20images%20and%20mitigates%0Aenvironmental%20factors.%20This%20is%20followed%20by%20a%20Sign-oriented%20Network%20%28SignNet%29%0Athat%20identifies%20specific%20tongue%20attributes%2C%20emulating%20the%20diagnostic%20process%20of%0Aexperienced%20practitioners%20and%20enabling%20comprehensive%20health%20evaluations.%20To%0Avalidate%20our%20methodology%2C%20we%20developed%20an%20extensive%20tongue%20image%20dataset%0Aspecifically%20designed%20for%20telemedicine.%20Unlike%20existing%20datasets%2C%20ours%20is%0Atailored%20for%20remote%20diagnosis%2C%20with%20a%20comprehensive%20set%20of%20attribute%20labels.%0AThis%20dataset%20will%20be%20openly%20available%2C%20providing%20a%20valuable%20resource%20for%0Aresearch.%20Initial%20tests%20have%20shown%20improved%20accuracy%20in%20detecting%20various%0Atongue%20attributes%2C%20highlighting%20our%20framework%27s%20potential%20as%20an%20essential%20tool%0Afor%20remote%20medical%20assessments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03053v2&entry.124074799=Read"},
{"title": "Multilingual Performance of a Multimodal Artificial Intelligence System\n  on Multisubject Physics Concept Inventories", "author": "Gerd Kortemeyer and Marina Babayeva and Giulia Polverini and Bor Gregorcic and Ralf Widenhorn", "abstract": "  We investigate the multilingual and multimodal performance of a large\nlanguage model-based artificial intelligence (AI) system, GPT-4o, on a diverse\nset of physics concept inventories spanning multiple languages and subject\nareas. The inventories taken from the PhysPort website cover the classical\nphysics topics of mechanics, electromagnetism, optics, and thermodynamics as\nwell as relativity, quantum mechanics, astronomy, mathematics, and laboratory\nskills. Unlike previous text-only studies, we uploaded the inventories as\nimages mirroring what a student would see on paper, assessing the system's\nmultimodal functionality. The AI is prompted in English and autonomously\nchooses the language of its response - either remaining in the nominal language\nof the test, switching entirely to English, or mixing languages - revealing\nadaptive behavior dependent on linguistic complexity and data availability. Our\nresults indicate some variation in performance across subject areas, with\nlaboratory skills standing out as the area of poorest performance. Furthermore,\nthe AI's performance on questions that require visual interpretation of images\nis worse than on purely text-based questions. Questions that are difficult for\nthe AI tend to be that way invariably of the inventory language. We also find\nlarge variations in performance across languages, with some appearing to\nbenefit substantially from language switching, a phenomenon similar to\ncode-switching ofhuman speakers. Overall, comparing the obtained AI results to\nthe existing literature, we find that the AI system outperforms average\nundergraduate students post-instruction in all subject areas but laboratory\nskills.\n", "link": "http://arxiv.org/abs/2501.06143v1", "date": "2025-01-10", "relevancy": 1.9535, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Performance%20of%20a%20Multimodal%20Artificial%20Intelligence%20System%0A%20%20on%20Multisubject%20Physics%20Concept%20Inventories&body=Title%3A%20Multilingual%20Performance%20of%20a%20Multimodal%20Artificial%20Intelligence%20System%0A%20%20on%20Multisubject%20Physics%20Concept%20Inventories%0AAuthor%3A%20Gerd%20Kortemeyer%20and%20Marina%20Babayeva%20and%20Giulia%20Polverini%20and%20Bor%20Gregorcic%20and%20Ralf%20Widenhorn%0AAbstract%3A%20%20%20We%20investigate%20the%20multilingual%20and%20multimodal%20performance%20of%20a%20large%0Alanguage%20model-based%20artificial%20intelligence%20%28AI%29%20system%2C%20GPT-4o%2C%20on%20a%20diverse%0Aset%20of%20physics%20concept%20inventories%20spanning%20multiple%20languages%20and%20subject%0Aareas.%20The%20inventories%20taken%20from%20the%20PhysPort%20website%20cover%20the%20classical%0Aphysics%20topics%20of%20mechanics%2C%20electromagnetism%2C%20optics%2C%20and%20thermodynamics%20as%0Awell%20as%20relativity%2C%20quantum%20mechanics%2C%20astronomy%2C%20mathematics%2C%20and%20laboratory%0Askills.%20Unlike%20previous%20text-only%20studies%2C%20we%20uploaded%20the%20inventories%20as%0Aimages%20mirroring%20what%20a%20student%20would%20see%20on%20paper%2C%20assessing%20the%20system%27s%0Amultimodal%20functionality.%20The%20AI%20is%20prompted%20in%20English%20and%20autonomously%0Achooses%20the%20language%20of%20its%20response%20-%20either%20remaining%20in%20the%20nominal%20language%0Aof%20the%20test%2C%20switching%20entirely%20to%20English%2C%20or%20mixing%20languages%20-%20revealing%0Aadaptive%20behavior%20dependent%20on%20linguistic%20complexity%20and%20data%20availability.%20Our%0Aresults%20indicate%20some%20variation%20in%20performance%20across%20subject%20areas%2C%20with%0Alaboratory%20skills%20standing%20out%20as%20the%20area%20of%20poorest%20performance.%20Furthermore%2C%0Athe%20AI%27s%20performance%20on%20questions%20that%20require%20visual%20interpretation%20of%20images%0Ais%20worse%20than%20on%20purely%20text-based%20questions.%20Questions%20that%20are%20difficult%20for%0Athe%20AI%20tend%20to%20be%20that%20way%20invariably%20of%20the%20inventory%20language.%20We%20also%20find%0Alarge%20variations%20in%20performance%20across%20languages%2C%20with%20some%20appearing%20to%0Abenefit%20substantially%20from%20language%20switching%2C%20a%20phenomenon%20similar%20to%0Acode-switching%20ofhuman%20speakers.%20Overall%2C%20comparing%20the%20obtained%20AI%20results%20to%0Athe%20existing%20literature%2C%20we%20find%20that%20the%20AI%20system%20outperforms%20average%0Aundergraduate%20students%20post-instruction%20in%20all%20subject%20areas%20but%20laboratory%0Askills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Performance%2520of%2520a%2520Multimodal%2520Artificial%2520Intelligence%2520System%250A%2520%2520on%2520Multisubject%2520Physics%2520Concept%2520Inventories%26entry.906535625%3DGerd%2520Kortemeyer%2520and%2520Marina%2520Babayeva%2520and%2520Giulia%2520Polverini%2520and%2520Bor%2520Gregorcic%2520and%2520Ralf%2520Widenhorn%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520multilingual%2520and%2520multimodal%2520performance%2520of%2520a%2520large%250Alanguage%2520model-based%2520artificial%2520intelligence%2520%2528AI%2529%2520system%252C%2520GPT-4o%252C%2520on%2520a%2520diverse%250Aset%2520of%2520physics%2520concept%2520inventories%2520spanning%2520multiple%2520languages%2520and%2520subject%250Aareas.%2520The%2520inventories%2520taken%2520from%2520the%2520PhysPort%2520website%2520cover%2520the%2520classical%250Aphysics%2520topics%2520of%2520mechanics%252C%2520electromagnetism%252C%2520optics%252C%2520and%2520thermodynamics%2520as%250Awell%2520as%2520relativity%252C%2520quantum%2520mechanics%252C%2520astronomy%252C%2520mathematics%252C%2520and%2520laboratory%250Askills.%2520Unlike%2520previous%2520text-only%2520studies%252C%2520we%2520uploaded%2520the%2520inventories%2520as%250Aimages%2520mirroring%2520what%2520a%2520student%2520would%2520see%2520on%2520paper%252C%2520assessing%2520the%2520system%2527s%250Amultimodal%2520functionality.%2520The%2520AI%2520is%2520prompted%2520in%2520English%2520and%2520autonomously%250Achooses%2520the%2520language%2520of%2520its%2520response%2520-%2520either%2520remaining%2520in%2520the%2520nominal%2520language%250Aof%2520the%2520test%252C%2520switching%2520entirely%2520to%2520English%252C%2520or%2520mixing%2520languages%2520-%2520revealing%250Aadaptive%2520behavior%2520dependent%2520on%2520linguistic%2520complexity%2520and%2520data%2520availability.%2520Our%250Aresults%2520indicate%2520some%2520variation%2520in%2520performance%2520across%2520subject%2520areas%252C%2520with%250Alaboratory%2520skills%2520standing%2520out%2520as%2520the%2520area%2520of%2520poorest%2520performance.%2520Furthermore%252C%250Athe%2520AI%2527s%2520performance%2520on%2520questions%2520that%2520require%2520visual%2520interpretation%2520of%2520images%250Ais%2520worse%2520than%2520on%2520purely%2520text-based%2520questions.%2520Questions%2520that%2520are%2520difficult%2520for%250Athe%2520AI%2520tend%2520to%2520be%2520that%2520way%2520invariably%2520of%2520the%2520inventory%2520language.%2520We%2520also%2520find%250Alarge%2520variations%2520in%2520performance%2520across%2520languages%252C%2520with%2520some%2520appearing%2520to%250Abenefit%2520substantially%2520from%2520language%2520switching%252C%2520a%2520phenomenon%2520similar%2520to%250Acode-switching%2520ofhuman%2520speakers.%2520Overall%252C%2520comparing%2520the%2520obtained%2520AI%2520results%2520to%250Athe%2520existing%2520literature%252C%2520we%2520find%2520that%2520the%2520AI%2520system%2520outperforms%2520average%250Aundergraduate%2520students%2520post-instruction%2520in%2520all%2520subject%2520areas%2520but%2520laboratory%250Askills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Performance%20of%20a%20Multimodal%20Artificial%20Intelligence%20System%0A%20%20on%20Multisubject%20Physics%20Concept%20Inventories&entry.906535625=Gerd%20Kortemeyer%20and%20Marina%20Babayeva%20and%20Giulia%20Polverini%20and%20Bor%20Gregorcic%20and%20Ralf%20Widenhorn&entry.1292438233=%20%20We%20investigate%20the%20multilingual%20and%20multimodal%20performance%20of%20a%20large%0Alanguage%20model-based%20artificial%20intelligence%20%28AI%29%20system%2C%20GPT-4o%2C%20on%20a%20diverse%0Aset%20of%20physics%20concept%20inventories%20spanning%20multiple%20languages%20and%20subject%0Aareas.%20The%20inventories%20taken%20from%20the%20PhysPort%20website%20cover%20the%20classical%0Aphysics%20topics%20of%20mechanics%2C%20electromagnetism%2C%20optics%2C%20and%20thermodynamics%20as%0Awell%20as%20relativity%2C%20quantum%20mechanics%2C%20astronomy%2C%20mathematics%2C%20and%20laboratory%0Askills.%20Unlike%20previous%20text-only%20studies%2C%20we%20uploaded%20the%20inventories%20as%0Aimages%20mirroring%20what%20a%20student%20would%20see%20on%20paper%2C%20assessing%20the%20system%27s%0Amultimodal%20functionality.%20The%20AI%20is%20prompted%20in%20English%20and%20autonomously%0Achooses%20the%20language%20of%20its%20response%20-%20either%20remaining%20in%20the%20nominal%20language%0Aof%20the%20test%2C%20switching%20entirely%20to%20English%2C%20or%20mixing%20languages%20-%20revealing%0Aadaptive%20behavior%20dependent%20on%20linguistic%20complexity%20and%20data%20availability.%20Our%0Aresults%20indicate%20some%20variation%20in%20performance%20across%20subject%20areas%2C%20with%0Alaboratory%20skills%20standing%20out%20as%20the%20area%20of%20poorest%20performance.%20Furthermore%2C%0Athe%20AI%27s%20performance%20on%20questions%20that%20require%20visual%20interpretation%20of%20images%0Ais%20worse%20than%20on%20purely%20text-based%20questions.%20Questions%20that%20are%20difficult%20for%0Athe%20AI%20tend%20to%20be%20that%20way%20invariably%20of%20the%20inventory%20language.%20We%20also%20find%0Alarge%20variations%20in%20performance%20across%20languages%2C%20with%20some%20appearing%20to%0Abenefit%20substantially%20from%20language%20switching%2C%20a%20phenomenon%20similar%20to%0Acode-switching%20ofhuman%20speakers.%20Overall%2C%20comparing%20the%20obtained%20AI%20results%20to%0Athe%20existing%20literature%2C%20we%20find%20that%20the%20AI%20system%20outperforms%20average%0Aundergraduate%20students%20post-instruction%20in%20all%20subject%20areas%20but%20laboratory%0Askills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06143v1&entry.124074799=Read"},
{"title": "Learning a Consensus Sub-Network with Polarization Regularization and\n  One Pass Training", "author": "Xiaoying Zhi and Varun Babbar and Rundong Liu and Pheobe Sun and Fran Silavong and Ruibo Shi and Sean Moran", "abstract": "  The subject of green AI has been gaining attention within the deep learning\ncommunity given the recent trend of ever larger and more complex neural network\nmodels. Existing solutions for reducing the computational load of training at\ninference time usually involve pruning the network parameters. Pruning schemes\noften create extra overhead either by iterative training and fine-tuning for\nstatic pruning or repeated computation of a dynamic pruning graph. We propose a\nnew parameter pruning strategy for learning a lighter-weight sub-network that\nminimizes the energy cost while maintaining comparable performance to the fully\nparameterised network on given downstream tasks. Our proposed pruning scheme is\ngreen-oriented, as it only requires a one-off training to discover the optimal\nstatic sub-networks by dynamic pruning methods. The pruning scheme consists of\na binary gating module and a polarizing loss function to uncover sub-networks\nwith user-defined sparsity. Our method enables pruning and training\nsimultaneously, which saves energy in both the training and inference phases\nand avoids extra computational overhead from gating modules at inference time.\nOur results on CIFAR-10, CIFAR-100, and Tiny Imagenet suggest that our scheme\ncan remove 50% of connections in deep networks with <1% reduction in\nclassification accuracy. Compared to other related pruning methods, our method\ndemonstrates a lower drop in accuracy for equivalent reductions in\ncomputational cost.\n", "link": "http://arxiv.org/abs/2302.10798v5", "date": "2025-01-10", "relevancy": 1.9442, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4943}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4931}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20a%20Consensus%20Sub-Network%20with%20Polarization%20Regularization%20and%0A%20%20One%20Pass%20Training&body=Title%3A%20Learning%20a%20Consensus%20Sub-Network%20with%20Polarization%20Regularization%20and%0A%20%20One%20Pass%20Training%0AAuthor%3A%20Xiaoying%20Zhi%20and%20Varun%20Babbar%20and%20Rundong%20Liu%20and%20Pheobe%20Sun%20and%20Fran%20Silavong%20and%20Ruibo%20Shi%20and%20Sean%20Moran%0AAbstract%3A%20%20%20The%20subject%20of%20green%20AI%20has%20been%20gaining%20attention%20within%20the%20deep%20learning%0Acommunity%20given%20the%20recent%20trend%20of%20ever%20larger%20and%20more%20complex%20neural%20network%0Amodels.%20Existing%20solutions%20for%20reducing%20the%20computational%20load%20of%20training%20at%0Ainference%20time%20usually%20involve%20pruning%20the%20network%20parameters.%20Pruning%20schemes%0Aoften%20create%20extra%20overhead%20either%20by%20iterative%20training%20and%20fine-tuning%20for%0Astatic%20pruning%20or%20repeated%20computation%20of%20a%20dynamic%20pruning%20graph.%20We%20propose%20a%0Anew%20parameter%20pruning%20strategy%20for%20learning%20a%20lighter-weight%20sub-network%20that%0Aminimizes%20the%20energy%20cost%20while%20maintaining%20comparable%20performance%20to%20the%20fully%0Aparameterised%20network%20on%20given%20downstream%20tasks.%20Our%20proposed%20pruning%20scheme%20is%0Agreen-oriented%2C%20as%20it%20only%20requires%20a%20one-off%20training%20to%20discover%20the%20optimal%0Astatic%20sub-networks%20by%20dynamic%20pruning%20methods.%20The%20pruning%20scheme%20consists%20of%0Aa%20binary%20gating%20module%20and%20a%20polarizing%20loss%20function%20to%20uncover%20sub-networks%0Awith%20user-defined%20sparsity.%20Our%20method%20enables%20pruning%20and%20training%0Asimultaneously%2C%20which%20saves%20energy%20in%20both%20the%20training%20and%20inference%20phases%0Aand%20avoids%20extra%20computational%20overhead%20from%20gating%20modules%20at%20inference%20time.%0AOur%20results%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny%20Imagenet%20suggest%20that%20our%20scheme%0Acan%20remove%2050%25%20of%20connections%20in%20deep%20networks%20with%20%3C1%25%20reduction%20in%0Aclassification%20accuracy.%20Compared%20to%20other%20related%20pruning%20methods%2C%20our%20method%0Ademonstrates%20a%20lower%20drop%20in%20accuracy%20for%20equivalent%20reductions%20in%0Acomputational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.10798v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520a%2520Consensus%2520Sub-Network%2520with%2520Polarization%2520Regularization%2520and%250A%2520%2520One%2520Pass%2520Training%26entry.906535625%3DXiaoying%2520Zhi%2520and%2520Varun%2520Babbar%2520and%2520Rundong%2520Liu%2520and%2520Pheobe%2520Sun%2520and%2520Fran%2520Silavong%2520and%2520Ruibo%2520Shi%2520and%2520Sean%2520Moran%26entry.1292438233%3D%2520%2520The%2520subject%2520of%2520green%2520AI%2520has%2520been%2520gaining%2520attention%2520within%2520the%2520deep%2520learning%250Acommunity%2520given%2520the%2520recent%2520trend%2520of%2520ever%2520larger%2520and%2520more%2520complex%2520neural%2520network%250Amodels.%2520Existing%2520solutions%2520for%2520reducing%2520the%2520computational%2520load%2520of%2520training%2520at%250Ainference%2520time%2520usually%2520involve%2520pruning%2520the%2520network%2520parameters.%2520Pruning%2520schemes%250Aoften%2520create%2520extra%2520overhead%2520either%2520by%2520iterative%2520training%2520and%2520fine-tuning%2520for%250Astatic%2520pruning%2520or%2520repeated%2520computation%2520of%2520a%2520dynamic%2520pruning%2520graph.%2520We%2520propose%2520a%250Anew%2520parameter%2520pruning%2520strategy%2520for%2520learning%2520a%2520lighter-weight%2520sub-network%2520that%250Aminimizes%2520the%2520energy%2520cost%2520while%2520maintaining%2520comparable%2520performance%2520to%2520the%2520fully%250Aparameterised%2520network%2520on%2520given%2520downstream%2520tasks.%2520Our%2520proposed%2520pruning%2520scheme%2520is%250Agreen-oriented%252C%2520as%2520it%2520only%2520requires%2520a%2520one-off%2520training%2520to%2520discover%2520the%2520optimal%250Astatic%2520sub-networks%2520by%2520dynamic%2520pruning%2520methods.%2520The%2520pruning%2520scheme%2520consists%2520of%250Aa%2520binary%2520gating%2520module%2520and%2520a%2520polarizing%2520loss%2520function%2520to%2520uncover%2520sub-networks%250Awith%2520user-defined%2520sparsity.%2520Our%2520method%2520enables%2520pruning%2520and%2520training%250Asimultaneously%252C%2520which%2520saves%2520energy%2520in%2520both%2520the%2520training%2520and%2520inference%2520phases%250Aand%2520avoids%2520extra%2520computational%2520overhead%2520from%2520gating%2520modules%2520at%2520inference%2520time.%250AOur%2520results%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520Tiny%2520Imagenet%2520suggest%2520that%2520our%2520scheme%250Acan%2520remove%252050%2525%2520of%2520connections%2520in%2520deep%2520networks%2520with%2520%253C1%2525%2520reduction%2520in%250Aclassification%2520accuracy.%2520Compared%2520to%2520other%2520related%2520pruning%2520methods%252C%2520our%2520method%250Ademonstrates%2520a%2520lower%2520drop%2520in%2520accuracy%2520for%2520equivalent%2520reductions%2520in%250Acomputational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.10798v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20a%20Consensus%20Sub-Network%20with%20Polarization%20Regularization%20and%0A%20%20One%20Pass%20Training&entry.906535625=Xiaoying%20Zhi%20and%20Varun%20Babbar%20and%20Rundong%20Liu%20and%20Pheobe%20Sun%20and%20Fran%20Silavong%20and%20Ruibo%20Shi%20and%20Sean%20Moran&entry.1292438233=%20%20The%20subject%20of%20green%20AI%20has%20been%20gaining%20attention%20within%20the%20deep%20learning%0Acommunity%20given%20the%20recent%20trend%20of%20ever%20larger%20and%20more%20complex%20neural%20network%0Amodels.%20Existing%20solutions%20for%20reducing%20the%20computational%20load%20of%20training%20at%0Ainference%20time%20usually%20involve%20pruning%20the%20network%20parameters.%20Pruning%20schemes%0Aoften%20create%20extra%20overhead%20either%20by%20iterative%20training%20and%20fine-tuning%20for%0Astatic%20pruning%20or%20repeated%20computation%20of%20a%20dynamic%20pruning%20graph.%20We%20propose%20a%0Anew%20parameter%20pruning%20strategy%20for%20learning%20a%20lighter-weight%20sub-network%20that%0Aminimizes%20the%20energy%20cost%20while%20maintaining%20comparable%20performance%20to%20the%20fully%0Aparameterised%20network%20on%20given%20downstream%20tasks.%20Our%20proposed%20pruning%20scheme%20is%0Agreen-oriented%2C%20as%20it%20only%20requires%20a%20one-off%20training%20to%20discover%20the%20optimal%0Astatic%20sub-networks%20by%20dynamic%20pruning%20methods.%20The%20pruning%20scheme%20consists%20of%0Aa%20binary%20gating%20module%20and%20a%20polarizing%20loss%20function%20to%20uncover%20sub-networks%0Awith%20user-defined%20sparsity.%20Our%20method%20enables%20pruning%20and%20training%0Asimultaneously%2C%20which%20saves%20energy%20in%20both%20the%20training%20and%20inference%20phases%0Aand%20avoids%20extra%20computational%20overhead%20from%20gating%20modules%20at%20inference%20time.%0AOur%20results%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny%20Imagenet%20suggest%20that%20our%20scheme%0Acan%20remove%2050%25%20of%20connections%20in%20deep%20networks%20with%20%3C1%25%20reduction%20in%0Aclassification%20accuracy.%20Compared%20to%20other%20related%20pruning%20methods%2C%20our%20method%0Ademonstrates%20a%20lower%20drop%20in%20accuracy%20for%20equivalent%20reductions%20in%0Acomputational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.10798v5&entry.124074799=Read"},
{"title": "Towards Early Prediction of Self-Supervised Speech Model Performance", "author": "Ryan Whetten and Lucas Maison and Titouan Parcollet and Marco Dinarelli and Yannick Est\u00e8ve", "abstract": "  In Self-Supervised Learning (SSL), pre-training and evaluation are resource\nintensive. In the speech domain, current indicators of the quality of SSL\nmodels during pre-training, such as the loss, do not correlate well with\ndownstream performance. Consequently, it is often difficult to gauge the final\ndownstream performance in a cost efficient manner during pre-training. In this\nwork, we propose unsupervised efficient methods that give insights into the\nquality of the pre-training of SSL speech models, namely, measuring the cluster\nquality and rank of the embeddings of the SSL model. Results show that measures\nof cluster quality and rank correlate better with downstream performance than\nthe pre-training loss with only one hour of unlabeled audio, reducing the need\nfor GPU hours and labeled data in SSL model evaluation.\n", "link": "http://arxiv.org/abs/2501.05966v1", "date": "2025-01-10", "relevancy": 1.9427, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5213}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4621}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Early%20Prediction%20of%20Self-Supervised%20Speech%20Model%20Performance&body=Title%3A%20Towards%20Early%20Prediction%20of%20Self-Supervised%20Speech%20Model%20Performance%0AAuthor%3A%20Ryan%20Whetten%20and%20Lucas%20Maison%20and%20Titouan%20Parcollet%20and%20Marco%20Dinarelli%20and%20Yannick%20Est%C3%A8ve%0AAbstract%3A%20%20%20In%20Self-Supervised%20Learning%20%28SSL%29%2C%20pre-training%20and%20evaluation%20are%20resource%0Aintensive.%20In%20the%20speech%20domain%2C%20current%20indicators%20of%20the%20quality%20of%20SSL%0Amodels%20during%20pre-training%2C%20such%20as%20the%20loss%2C%20do%20not%20correlate%20well%20with%0Adownstream%20performance.%20Consequently%2C%20it%20is%20often%20difficult%20to%20gauge%20the%20final%0Adownstream%20performance%20in%20a%20cost%20efficient%20manner%20during%20pre-training.%20In%20this%0Awork%2C%20we%20propose%20unsupervised%20efficient%20methods%20that%20give%20insights%20into%20the%0Aquality%20of%20the%20pre-training%20of%20SSL%20speech%20models%2C%20namely%2C%20measuring%20the%20cluster%0Aquality%20and%20rank%20of%20the%20embeddings%20of%20the%20SSL%20model.%20Results%20show%20that%20measures%0Aof%20cluster%20quality%20and%20rank%20correlate%20better%20with%20downstream%20performance%20than%0Athe%20pre-training%20loss%20with%20only%20one%20hour%20of%20unlabeled%20audio%2C%20reducing%20the%20need%0Afor%20GPU%20hours%20and%20labeled%20data%20in%20SSL%20model%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Early%2520Prediction%2520of%2520Self-Supervised%2520Speech%2520Model%2520Performance%26entry.906535625%3DRyan%2520Whetten%2520and%2520Lucas%2520Maison%2520and%2520Titouan%2520Parcollet%2520and%2520Marco%2520Dinarelli%2520and%2520Yannick%2520Est%25C3%25A8ve%26entry.1292438233%3D%2520%2520In%2520Self-Supervised%2520Learning%2520%2528SSL%2529%252C%2520pre-training%2520and%2520evaluation%2520are%2520resource%250Aintensive.%2520In%2520the%2520speech%2520domain%252C%2520current%2520indicators%2520of%2520the%2520quality%2520of%2520SSL%250Amodels%2520during%2520pre-training%252C%2520such%2520as%2520the%2520loss%252C%2520do%2520not%2520correlate%2520well%2520with%250Adownstream%2520performance.%2520Consequently%252C%2520it%2520is%2520often%2520difficult%2520to%2520gauge%2520the%2520final%250Adownstream%2520performance%2520in%2520a%2520cost%2520efficient%2520manner%2520during%2520pre-training.%2520In%2520this%250Awork%252C%2520we%2520propose%2520unsupervised%2520efficient%2520methods%2520that%2520give%2520insights%2520into%2520the%250Aquality%2520of%2520the%2520pre-training%2520of%2520SSL%2520speech%2520models%252C%2520namely%252C%2520measuring%2520the%2520cluster%250Aquality%2520and%2520rank%2520of%2520the%2520embeddings%2520of%2520the%2520SSL%2520model.%2520Results%2520show%2520that%2520measures%250Aof%2520cluster%2520quality%2520and%2520rank%2520correlate%2520better%2520with%2520downstream%2520performance%2520than%250Athe%2520pre-training%2520loss%2520with%2520only%2520one%2520hour%2520of%2520unlabeled%2520audio%252C%2520reducing%2520the%2520need%250Afor%2520GPU%2520hours%2520and%2520labeled%2520data%2520in%2520SSL%2520model%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Early%20Prediction%20of%20Self-Supervised%20Speech%20Model%20Performance&entry.906535625=Ryan%20Whetten%20and%20Lucas%20Maison%20and%20Titouan%20Parcollet%20and%20Marco%20Dinarelli%20and%20Yannick%20Est%C3%A8ve&entry.1292438233=%20%20In%20Self-Supervised%20Learning%20%28SSL%29%2C%20pre-training%20and%20evaluation%20are%20resource%0Aintensive.%20In%20the%20speech%20domain%2C%20current%20indicators%20of%20the%20quality%20of%20SSL%0Amodels%20during%20pre-training%2C%20such%20as%20the%20loss%2C%20do%20not%20correlate%20well%20with%0Adownstream%20performance.%20Consequently%2C%20it%20is%20often%20difficult%20to%20gauge%20the%20final%0Adownstream%20performance%20in%20a%20cost%20efficient%20manner%20during%20pre-training.%20In%20this%0Awork%2C%20we%20propose%20unsupervised%20efficient%20methods%20that%20give%20insights%20into%20the%0Aquality%20of%20the%20pre-training%20of%20SSL%20speech%20models%2C%20namely%2C%20measuring%20the%20cluster%0Aquality%20and%20rank%20of%20the%20embeddings%20of%20the%20SSL%20model.%20Results%20show%20that%20measures%0Aof%20cluster%20quality%20and%20rank%20correlate%20better%20with%20downstream%20performance%20than%0Athe%20pre-training%20loss%20with%20only%20one%20hour%20of%20unlabeled%20audio%2C%20reducing%20the%20need%0Afor%20GPU%20hours%20and%20labeled%20data%20in%20SSL%20model%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05966v1&entry.124074799=Read"},
{"title": "Discovery of sustainable energy materials via the machine-learned\n  material space", "author": "Malte Grunert and Max Gro\u00dfmann and Erich Runge", "abstract": "  Does a machine learning model actually gain an understanding of the material\nspace? We answer this question in the affirmative on the example of the\nOptiMate model, a graph attention network trained to predict the optical\nproperties of semiconductors and insulators. By applying the UMAP\ndimensionality reduction technique to its latent embeddings, we demonstrate\nthat the model captures a nuanced and interpretable representation of the\nmaterials space, reflecting chemical and physical principles, without any\nuser-induced bias. This enables clustering of almost 10,000 materials based on\noptical properties and chemical similarities. Beyond this understanding, we\ndemonstrate how the learned material space can be used to identify more\nsustainable alternatives to critical materials in energy-related technologies,\nsuch as photovoltaics. These findings demonstrate the dual utility of machine\nlearning models in materials science: Accurately predicting material properties\nwhile providing insights into the underlying materials space. The approach\ndemonstrates the broader potential of leveraging learned materials spaces for\nthe discovery and design of materials for diverse applications, and is easily\napplicable to any state-of-the-art machine learning model.\n", "link": "http://arxiv.org/abs/2501.05903v1", "date": "2025-01-10", "relevancy": 1.9362, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4806}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovery%20of%20sustainable%20energy%20materials%20via%20the%20machine-learned%0A%20%20material%20space&body=Title%3A%20Discovery%20of%20sustainable%20energy%20materials%20via%20the%20machine-learned%0A%20%20material%20space%0AAuthor%3A%20Malte%20Grunert%20and%20Max%20Gro%C3%9Fmann%20and%20Erich%20Runge%0AAbstract%3A%20%20%20Does%20a%20machine%20learning%20model%20actually%20gain%20an%20understanding%20of%20the%20material%0Aspace%3F%20We%20answer%20this%20question%20in%20the%20affirmative%20on%20the%20example%20of%20the%0AOptiMate%20model%2C%20a%20graph%20attention%20network%20trained%20to%20predict%20the%20optical%0Aproperties%20of%20semiconductors%20and%20insulators.%20By%20applying%20the%20UMAP%0Adimensionality%20reduction%20technique%20to%20its%20latent%20embeddings%2C%20we%20demonstrate%0Athat%20the%20model%20captures%20a%20nuanced%20and%20interpretable%20representation%20of%20the%0Amaterials%20space%2C%20reflecting%20chemical%20and%20physical%20principles%2C%20without%20any%0Auser-induced%20bias.%20This%20enables%20clustering%20of%20almost%2010%2C000%20materials%20based%20on%0Aoptical%20properties%20and%20chemical%20similarities.%20Beyond%20this%20understanding%2C%20we%0Ademonstrate%20how%20the%20learned%20material%20space%20can%20be%20used%20to%20identify%20more%0Asustainable%20alternatives%20to%20critical%20materials%20in%20energy-related%20technologies%2C%0Asuch%20as%20photovoltaics.%20These%20findings%20demonstrate%20the%20dual%20utility%20of%20machine%0Alearning%20models%20in%20materials%20science%3A%20Accurately%20predicting%20material%20properties%0Awhile%20providing%20insights%20into%20the%20underlying%20materials%20space.%20The%20approach%0Ademonstrates%20the%20broader%20potential%20of%20leveraging%20learned%20materials%20spaces%20for%0Athe%20discovery%20and%20design%20of%20materials%20for%20diverse%20applications%2C%20and%20is%20easily%0Aapplicable%20to%20any%20state-of-the-art%20machine%20learning%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovery%2520of%2520sustainable%2520energy%2520materials%2520via%2520the%2520machine-learned%250A%2520%2520material%2520space%26entry.906535625%3DMalte%2520Grunert%2520and%2520Max%2520Gro%25C3%259Fmann%2520and%2520Erich%2520Runge%26entry.1292438233%3D%2520%2520Does%2520a%2520machine%2520learning%2520model%2520actually%2520gain%2520an%2520understanding%2520of%2520the%2520material%250Aspace%253F%2520We%2520answer%2520this%2520question%2520in%2520the%2520affirmative%2520on%2520the%2520example%2520of%2520the%250AOptiMate%2520model%252C%2520a%2520graph%2520attention%2520network%2520trained%2520to%2520predict%2520the%2520optical%250Aproperties%2520of%2520semiconductors%2520and%2520insulators.%2520By%2520applying%2520the%2520UMAP%250Adimensionality%2520reduction%2520technique%2520to%2520its%2520latent%2520embeddings%252C%2520we%2520demonstrate%250Athat%2520the%2520model%2520captures%2520a%2520nuanced%2520and%2520interpretable%2520representation%2520of%2520the%250Amaterials%2520space%252C%2520reflecting%2520chemical%2520and%2520physical%2520principles%252C%2520without%2520any%250Auser-induced%2520bias.%2520This%2520enables%2520clustering%2520of%2520almost%252010%252C000%2520materials%2520based%2520on%250Aoptical%2520properties%2520and%2520chemical%2520similarities.%2520Beyond%2520this%2520understanding%252C%2520we%250Ademonstrate%2520how%2520the%2520learned%2520material%2520space%2520can%2520be%2520used%2520to%2520identify%2520more%250Asustainable%2520alternatives%2520to%2520critical%2520materials%2520in%2520energy-related%2520technologies%252C%250Asuch%2520as%2520photovoltaics.%2520These%2520findings%2520demonstrate%2520the%2520dual%2520utility%2520of%2520machine%250Alearning%2520models%2520in%2520materials%2520science%253A%2520Accurately%2520predicting%2520material%2520properties%250Awhile%2520providing%2520insights%2520into%2520the%2520underlying%2520materials%2520space.%2520The%2520approach%250Ademonstrates%2520the%2520broader%2520potential%2520of%2520leveraging%2520learned%2520materials%2520spaces%2520for%250Athe%2520discovery%2520and%2520design%2520of%2520materials%2520for%2520diverse%2520applications%252C%2520and%2520is%2520easily%250Aapplicable%2520to%2520any%2520state-of-the-art%2520machine%2520learning%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovery%20of%20sustainable%20energy%20materials%20via%20the%20machine-learned%0A%20%20material%20space&entry.906535625=Malte%20Grunert%20and%20Max%20Gro%C3%9Fmann%20and%20Erich%20Runge&entry.1292438233=%20%20Does%20a%20machine%20learning%20model%20actually%20gain%20an%20understanding%20of%20the%20material%0Aspace%3F%20We%20answer%20this%20question%20in%20the%20affirmative%20on%20the%20example%20of%20the%0AOptiMate%20model%2C%20a%20graph%20attention%20network%20trained%20to%20predict%20the%20optical%0Aproperties%20of%20semiconductors%20and%20insulators.%20By%20applying%20the%20UMAP%0Adimensionality%20reduction%20technique%20to%20its%20latent%20embeddings%2C%20we%20demonstrate%0Athat%20the%20model%20captures%20a%20nuanced%20and%20interpretable%20representation%20of%20the%0Amaterials%20space%2C%20reflecting%20chemical%20and%20physical%20principles%2C%20without%20any%0Auser-induced%20bias.%20This%20enables%20clustering%20of%20almost%2010%2C000%20materials%20based%20on%0Aoptical%20properties%20and%20chemical%20similarities.%20Beyond%20this%20understanding%2C%20we%0Ademonstrate%20how%20the%20learned%20material%20space%20can%20be%20used%20to%20identify%20more%0Asustainable%20alternatives%20to%20critical%20materials%20in%20energy-related%20technologies%2C%0Asuch%20as%20photovoltaics.%20These%20findings%20demonstrate%20the%20dual%20utility%20of%20machine%0Alearning%20models%20in%20materials%20science%3A%20Accurately%20predicting%20material%20properties%0Awhile%20providing%20insights%20into%20the%20underlying%20materials%20space.%20The%20approach%0Ademonstrates%20the%20broader%20potential%20of%20leveraging%20learned%20materials%20spaces%20for%0Athe%20discovery%20and%20design%20of%20materials%20for%20diverse%20applications%2C%20and%20is%20easily%0Aapplicable%20to%20any%20state-of-the-art%20machine%20learning%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05903v1&entry.124074799=Read"},
{"title": "BRIGHT: A globally distributed multimodal building damage assessment\n  dataset with very-high-resolution for all-weather disaster response", "author": "Hongruixuan Chen and Jian Song and Olivier Dietrich and Clifford Broni-Bediako and Weihao Xuan and Junjue Wang and Xinlei Shao and Yimin Wei and Junshi Xia and Cuiling Lan and Konrad Schindler and Naoto Yokoya", "abstract": "  Disaster events occur around the world and cause significant damage to human\nlife and property. Earth observation (EO) data enables rapid and comprehensive\nbuilding damage assessment (BDA), an essential capability in the aftermath of a\ndisaster to reduce human casualties and to inform disaster relief efforts.\nRecent research focuses on the development of AI models to achieve accurate\nmapping of unseen disaster events, mostly using optical EO data. However,\nsolutions based on optical data are limited to clear skies and daylight hours,\npreventing a prompt response to disasters. Integrating multimodal (MM) EO data,\nparticularly the combination of optical and SAR imagery, makes it possible to\nprovide all-weather, day-and-night disaster responses. Despite this potential,\nthe development of robust multimodal AI models has been constrained by the lack\nof suitable benchmark datasets. In this paper, we present a BDA dataset using\nveRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based\nall-weather disaster response. To the best of our knowledge, BRIGHT is the\nfirst open-access, globally distributed, event-diverse MM dataset specifically\ncurated to support AI-based disaster response. It covers five types of natural\ndisasters and two types of man-made disasters across 12 regions worldwide, with\na particular focus on developing countries where external assistance is most\nneeded. The optical and SAR imagery in BRIGHT, with a spatial resolution\nbetween 0.3-1 meters, provides detailed representations of individual\nbuildings, making it ideal for precise BDA. In our experiments, we have tested\nseven advanced AI models trained with our BRIGHT to validate the\ntransferability and robustness. The dataset and code are available at\nhttps://github.com/ChenHongruixuan/BRIGHT. BRIGHT also serves as the official\ndataset for the 2025 IEEE GRSS Data Fusion Contest.\n", "link": "http://arxiv.org/abs/2501.06019v1", "date": "2025-01-10", "relevancy": 1.9318, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.489}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRIGHT%3A%20A%20globally%20distributed%20multimodal%20building%20damage%20assessment%0A%20%20dataset%20with%20very-high-resolution%20for%20all-weather%20disaster%20response&body=Title%3A%20BRIGHT%3A%20A%20globally%20distributed%20multimodal%20building%20damage%20assessment%0A%20%20dataset%20with%20very-high-resolution%20for%20all-weather%20disaster%20response%0AAuthor%3A%20Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Olivier%20Dietrich%20and%20Clifford%20Broni-Bediako%20and%20Weihao%20Xuan%20and%20Junjue%20Wang%20and%20Xinlei%20Shao%20and%20Yimin%20Wei%20and%20Junshi%20Xia%20and%20Cuiling%20Lan%20and%20Konrad%20Schindler%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Disaster%20events%20occur%20around%20the%20world%20and%20cause%20significant%20damage%20to%20human%0Alife%20and%20property.%20Earth%20observation%20%28EO%29%20data%20enables%20rapid%20and%20comprehensive%0Abuilding%20damage%20assessment%20%28BDA%29%2C%20an%20essential%20capability%20in%20the%20aftermath%20of%20a%0Adisaster%20to%20reduce%20human%20casualties%20and%20to%20inform%20disaster%20relief%20efforts.%0ARecent%20research%20focuses%20on%20the%20development%20of%20AI%20models%20to%20achieve%20accurate%0Amapping%20of%20unseen%20disaster%20events%2C%20mostly%20using%20optical%20EO%20data.%20However%2C%0Asolutions%20based%20on%20optical%20data%20are%20limited%20to%20clear%20skies%20and%20daylight%20hours%2C%0Apreventing%20a%20prompt%20response%20to%20disasters.%20Integrating%20multimodal%20%28MM%29%20EO%20data%2C%0Aparticularly%20the%20combination%20of%20optical%20and%20SAR%20imagery%2C%20makes%20it%20possible%20to%0Aprovide%20all-weather%2C%20day-and-night%20disaster%20responses.%20Despite%20this%20potential%2C%0Athe%20development%20of%20robust%20multimodal%20AI%20models%20has%20been%20constrained%20by%20the%20lack%0Aof%20suitable%20benchmark%20datasets.%20In%20this%20paper%2C%20we%20present%20a%20BDA%20dataset%20using%0AveRy-hIGH-resoluTion%20optical%20and%20SAR%20imagery%20%28BRIGHT%29%20to%20support%20AI-based%0Aall-weather%20disaster%20response.%20To%20the%20best%20of%20our%20knowledge%2C%20BRIGHT%20is%20the%0Afirst%20open-access%2C%20globally%20distributed%2C%20event-diverse%20MM%20dataset%20specifically%0Acurated%20to%20support%20AI-based%20disaster%20response.%20It%20covers%20five%20types%20of%20natural%0Adisasters%20and%20two%20types%20of%20man-made%20disasters%20across%2012%20regions%20worldwide%2C%20with%0Aa%20particular%20focus%20on%20developing%20countries%20where%20external%20assistance%20is%20most%0Aneeded.%20The%20optical%20and%20SAR%20imagery%20in%20BRIGHT%2C%20with%20a%20spatial%20resolution%0Abetween%200.3-1%20meters%2C%20provides%20detailed%20representations%20of%20individual%0Abuildings%2C%20making%20it%20ideal%20for%20precise%20BDA.%20In%20our%20experiments%2C%20we%20have%20tested%0Aseven%20advanced%20AI%20models%20trained%20with%20our%20BRIGHT%20to%20validate%20the%0Atransferability%20and%20robustness.%20The%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ChenHongruixuan/BRIGHT.%20BRIGHT%20also%20serves%20as%20the%20official%0Adataset%20for%20the%202025%20IEEE%20GRSS%20Data%20Fusion%20Contest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRIGHT%253A%2520A%2520globally%2520distributed%2520multimodal%2520building%2520damage%2520assessment%250A%2520%2520dataset%2520with%2520very-high-resolution%2520for%2520all-weather%2520disaster%2520response%26entry.906535625%3DHongruixuan%2520Chen%2520and%2520Jian%2520Song%2520and%2520Olivier%2520Dietrich%2520and%2520Clifford%2520Broni-Bediako%2520and%2520Weihao%2520Xuan%2520and%2520Junjue%2520Wang%2520and%2520Xinlei%2520Shao%2520and%2520Yimin%2520Wei%2520and%2520Junshi%2520Xia%2520and%2520Cuiling%2520Lan%2520and%2520Konrad%2520Schindler%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Disaster%2520events%2520occur%2520around%2520the%2520world%2520and%2520cause%2520significant%2520damage%2520to%2520human%250Alife%2520and%2520property.%2520Earth%2520observation%2520%2528EO%2529%2520data%2520enables%2520rapid%2520and%2520comprehensive%250Abuilding%2520damage%2520assessment%2520%2528BDA%2529%252C%2520an%2520essential%2520capability%2520in%2520the%2520aftermath%2520of%2520a%250Adisaster%2520to%2520reduce%2520human%2520casualties%2520and%2520to%2520inform%2520disaster%2520relief%2520efforts.%250ARecent%2520research%2520focuses%2520on%2520the%2520development%2520of%2520AI%2520models%2520to%2520achieve%2520accurate%250Amapping%2520of%2520unseen%2520disaster%2520events%252C%2520mostly%2520using%2520optical%2520EO%2520data.%2520However%252C%250Asolutions%2520based%2520on%2520optical%2520data%2520are%2520limited%2520to%2520clear%2520skies%2520and%2520daylight%2520hours%252C%250Apreventing%2520a%2520prompt%2520response%2520to%2520disasters.%2520Integrating%2520multimodal%2520%2528MM%2529%2520EO%2520data%252C%250Aparticularly%2520the%2520combination%2520of%2520optical%2520and%2520SAR%2520imagery%252C%2520makes%2520it%2520possible%2520to%250Aprovide%2520all-weather%252C%2520day-and-night%2520disaster%2520responses.%2520Despite%2520this%2520potential%252C%250Athe%2520development%2520of%2520robust%2520multimodal%2520AI%2520models%2520has%2520been%2520constrained%2520by%2520the%2520lack%250Aof%2520suitable%2520benchmark%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520BDA%2520dataset%2520using%250AveRy-hIGH-resoluTion%2520optical%2520and%2520SAR%2520imagery%2520%2528BRIGHT%2529%2520to%2520support%2520AI-based%250Aall-weather%2520disaster%2520response.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520BRIGHT%2520is%2520the%250Afirst%2520open-access%252C%2520globally%2520distributed%252C%2520event-diverse%2520MM%2520dataset%2520specifically%250Acurated%2520to%2520support%2520AI-based%2520disaster%2520response.%2520It%2520covers%2520five%2520types%2520of%2520natural%250Adisasters%2520and%2520two%2520types%2520of%2520man-made%2520disasters%2520across%252012%2520regions%2520worldwide%252C%2520with%250Aa%2520particular%2520focus%2520on%2520developing%2520countries%2520where%2520external%2520assistance%2520is%2520most%250Aneeded.%2520The%2520optical%2520and%2520SAR%2520imagery%2520in%2520BRIGHT%252C%2520with%2520a%2520spatial%2520resolution%250Abetween%25200.3-1%2520meters%252C%2520provides%2520detailed%2520representations%2520of%2520individual%250Abuildings%252C%2520making%2520it%2520ideal%2520for%2520precise%2520BDA.%2520In%2520our%2520experiments%252C%2520we%2520have%2520tested%250Aseven%2520advanced%2520AI%2520models%2520trained%2520with%2520our%2520BRIGHT%2520to%2520validate%2520the%250Atransferability%2520and%2520robustness.%2520The%2520dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/ChenHongruixuan/BRIGHT.%2520BRIGHT%2520also%2520serves%2520as%2520the%2520official%250Adataset%2520for%2520the%25202025%2520IEEE%2520GRSS%2520Data%2520Fusion%2520Contest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRIGHT%3A%20A%20globally%20distributed%20multimodal%20building%20damage%20assessment%0A%20%20dataset%20with%20very-high-resolution%20for%20all-weather%20disaster%20response&entry.906535625=Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Olivier%20Dietrich%20and%20Clifford%20Broni-Bediako%20and%20Weihao%20Xuan%20and%20Junjue%20Wang%20and%20Xinlei%20Shao%20and%20Yimin%20Wei%20and%20Junshi%20Xia%20and%20Cuiling%20Lan%20and%20Konrad%20Schindler%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Disaster%20events%20occur%20around%20the%20world%20and%20cause%20significant%20damage%20to%20human%0Alife%20and%20property.%20Earth%20observation%20%28EO%29%20data%20enables%20rapid%20and%20comprehensive%0Abuilding%20damage%20assessment%20%28BDA%29%2C%20an%20essential%20capability%20in%20the%20aftermath%20of%20a%0Adisaster%20to%20reduce%20human%20casualties%20and%20to%20inform%20disaster%20relief%20efforts.%0ARecent%20research%20focuses%20on%20the%20development%20of%20AI%20models%20to%20achieve%20accurate%0Amapping%20of%20unseen%20disaster%20events%2C%20mostly%20using%20optical%20EO%20data.%20However%2C%0Asolutions%20based%20on%20optical%20data%20are%20limited%20to%20clear%20skies%20and%20daylight%20hours%2C%0Apreventing%20a%20prompt%20response%20to%20disasters.%20Integrating%20multimodal%20%28MM%29%20EO%20data%2C%0Aparticularly%20the%20combination%20of%20optical%20and%20SAR%20imagery%2C%20makes%20it%20possible%20to%0Aprovide%20all-weather%2C%20day-and-night%20disaster%20responses.%20Despite%20this%20potential%2C%0Athe%20development%20of%20robust%20multimodal%20AI%20models%20has%20been%20constrained%20by%20the%20lack%0Aof%20suitable%20benchmark%20datasets.%20In%20this%20paper%2C%20we%20present%20a%20BDA%20dataset%20using%0AveRy-hIGH-resoluTion%20optical%20and%20SAR%20imagery%20%28BRIGHT%29%20to%20support%20AI-based%0Aall-weather%20disaster%20response.%20To%20the%20best%20of%20our%20knowledge%2C%20BRIGHT%20is%20the%0Afirst%20open-access%2C%20globally%20distributed%2C%20event-diverse%20MM%20dataset%20specifically%0Acurated%20to%20support%20AI-based%20disaster%20response.%20It%20covers%20five%20types%20of%20natural%0Adisasters%20and%20two%20types%20of%20man-made%20disasters%20across%2012%20regions%20worldwide%2C%20with%0Aa%20particular%20focus%20on%20developing%20countries%20where%20external%20assistance%20is%20most%0Aneeded.%20The%20optical%20and%20SAR%20imagery%20in%20BRIGHT%2C%20with%20a%20spatial%20resolution%0Abetween%200.3-1%20meters%2C%20provides%20detailed%20representations%20of%20individual%0Abuildings%2C%20making%20it%20ideal%20for%20precise%20BDA.%20In%20our%20experiments%2C%20we%20have%20tested%0Aseven%20advanced%20AI%20models%20trained%20with%20our%20BRIGHT%20to%20validate%20the%0Atransferability%20and%20robustness.%20The%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ChenHongruixuan/BRIGHT.%20BRIGHT%20also%20serves%20as%20the%20official%0Adataset%20for%20the%202025%20IEEE%20GRSS%20Data%20Fusion%20Contest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06019v1&entry.124074799=Read"},
{"title": "Are We Done with MMLU?", "author": "Aryo Pradipta Gema and Joshua Ong Jun Leang and Giwon Hong and Alessio Devoto and Alberto Carlo Maria Mancino and Rohit Saxena and Xuanli He and Yu Zhao and Xiaotang Du and Mohammad Reza Ghasemi Madani and Claire Barale and Robert McHardy and Joshua Harris and Jean Kaddour and Emile van Krieken and Pasquale Minervini", "abstract": "  Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\nannotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700\nmanually re-annotated questions across all 57 MMLU subjects. We estimate that\n6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate\nsignificant discrepancies with the model performance metrics that were\noriginally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.\n", "link": "http://arxiv.org/abs/2406.04127v3", "date": "2025-01-10", "relevancy": 1.9243, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20We%20Done%20with%20MMLU%3F&body=Title%3A%20Are%20We%20Done%20with%20MMLU%3F%0AAuthor%3A%20Aryo%20Pradipta%20Gema%20and%20Joshua%20Ong%20Jun%20Leang%20and%20Giwon%20Hong%20and%20Alessio%20Devoto%20and%20Alberto%20Carlo%20Maria%20Mancino%20and%20Rohit%20Saxena%20and%20Xuanli%20He%20and%20Yu%20Zhao%20and%20Xiaotang%20Du%20and%20Mohammad%20Reza%20Ghasemi%20Madani%20and%20Claire%20Barale%20and%20Robert%20McHardy%20and%20Joshua%20Harris%20and%20Jean%20Kaddour%20and%20Emile%20van%20Krieken%20and%20Pasquale%20Minervini%0AAbstract%3A%20%20%20Maybe%20not.%20We%20identify%20and%20analyse%20errors%20in%20the%20popular%20Massive%20Multitask%0ALanguage%20Understanding%20%28MMLU%29%20benchmark.%20Even%20though%20MMLU%20is%20widely%20adopted%2C%0Aour%20analysis%20demonstrates%20numerous%20ground%20truth%20errors%20that%20obscure%20the%20true%0Acapabilities%20of%20LLMs.%20For%20example%2C%20we%20find%20that%2057%25%20of%20the%20analysed%20questions%0Ain%20the%20Virology%20subset%20contain%20errors.%20To%20address%20this%20issue%2C%20we%20introduce%20a%0Acomprehensive%20framework%20for%20identifying%20dataset%20errors%20using%20a%20novel%20error%0Aannotation%20protocol.%20Then%2C%20we%20create%20MMLU-Redux%2C%20which%20is%20a%20subset%20of%205%2C700%0Amanually%20re-annotated%20questions%20across%20all%2057%20MMLU%20subjects.%20We%20estimate%20that%0A6.49%25%20of%20MMLU%20questions%20contain%20errors.%20Using%20MMLU-Redux%2C%20we%20demonstrate%0Asignificant%20discrepancies%20with%20the%20model%20performance%20metrics%20that%20were%0Aoriginally%20reported.%20Our%20results%20strongly%20advocate%20for%20revising%20MMLU%27s%0Aerror-ridden%20questions%20to%20enhance%20its%20future%20utility%20and%20reliability%20as%20a%0Abenchmark.%20https%3A//huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04127v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520We%2520Done%2520with%2520MMLU%253F%26entry.906535625%3DAryo%2520Pradipta%2520Gema%2520and%2520Joshua%2520Ong%2520Jun%2520Leang%2520and%2520Giwon%2520Hong%2520and%2520Alessio%2520Devoto%2520and%2520Alberto%2520Carlo%2520Maria%2520Mancino%2520and%2520Rohit%2520Saxena%2520and%2520Xuanli%2520He%2520and%2520Yu%2520Zhao%2520and%2520Xiaotang%2520Du%2520and%2520Mohammad%2520Reza%2520Ghasemi%2520Madani%2520and%2520Claire%2520Barale%2520and%2520Robert%2520McHardy%2520and%2520Joshua%2520Harris%2520and%2520Jean%2520Kaddour%2520and%2520Emile%2520van%2520Krieken%2520and%2520Pasquale%2520Minervini%26entry.1292438233%3D%2520%2520Maybe%2520not.%2520We%2520identify%2520and%2520analyse%2520errors%2520in%2520the%2520popular%2520Massive%2520Multitask%250ALanguage%2520Understanding%2520%2528MMLU%2529%2520benchmark.%2520Even%2520though%2520MMLU%2520is%2520widely%2520adopted%252C%250Aour%2520analysis%2520demonstrates%2520numerous%2520ground%2520truth%2520errors%2520that%2520obscure%2520the%2520true%250Acapabilities%2520of%2520LLMs.%2520For%2520example%252C%2520we%2520find%2520that%252057%2525%2520of%2520the%2520analysed%2520questions%250Ain%2520the%2520Virology%2520subset%2520contain%2520errors.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%250Acomprehensive%2520framework%2520for%2520identifying%2520dataset%2520errors%2520using%2520a%2520novel%2520error%250Aannotation%2520protocol.%2520Then%252C%2520we%2520create%2520MMLU-Redux%252C%2520which%2520is%2520a%2520subset%2520of%25205%252C700%250Amanually%2520re-annotated%2520questions%2520across%2520all%252057%2520MMLU%2520subjects.%2520We%2520estimate%2520that%250A6.49%2525%2520of%2520MMLU%2520questions%2520contain%2520errors.%2520Using%2520MMLU-Redux%252C%2520we%2520demonstrate%250Asignificant%2520discrepancies%2520with%2520the%2520model%2520performance%2520metrics%2520that%2520were%250Aoriginally%2520reported.%2520Our%2520results%2520strongly%2520advocate%2520for%2520revising%2520MMLU%2527s%250Aerror-ridden%2520questions%2520to%2520enhance%2520its%2520future%2520utility%2520and%2520reliability%2520as%2520a%250Abenchmark.%2520https%253A//huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04127v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20We%20Done%20with%20MMLU%3F&entry.906535625=Aryo%20Pradipta%20Gema%20and%20Joshua%20Ong%20Jun%20Leang%20and%20Giwon%20Hong%20and%20Alessio%20Devoto%20and%20Alberto%20Carlo%20Maria%20Mancino%20and%20Rohit%20Saxena%20and%20Xuanli%20He%20and%20Yu%20Zhao%20and%20Xiaotang%20Du%20and%20Mohammad%20Reza%20Ghasemi%20Madani%20and%20Claire%20Barale%20and%20Robert%20McHardy%20and%20Joshua%20Harris%20and%20Jean%20Kaddour%20and%20Emile%20van%20Krieken%20and%20Pasquale%20Minervini&entry.1292438233=%20%20Maybe%20not.%20We%20identify%20and%20analyse%20errors%20in%20the%20popular%20Massive%20Multitask%0ALanguage%20Understanding%20%28MMLU%29%20benchmark.%20Even%20though%20MMLU%20is%20widely%20adopted%2C%0Aour%20analysis%20demonstrates%20numerous%20ground%20truth%20errors%20that%20obscure%20the%20true%0Acapabilities%20of%20LLMs.%20For%20example%2C%20we%20find%20that%2057%25%20of%20the%20analysed%20questions%0Ain%20the%20Virology%20subset%20contain%20errors.%20To%20address%20this%20issue%2C%20we%20introduce%20a%0Acomprehensive%20framework%20for%20identifying%20dataset%20errors%20using%20a%20novel%20error%0Aannotation%20protocol.%20Then%2C%20we%20create%20MMLU-Redux%2C%20which%20is%20a%20subset%20of%205%2C700%0Amanually%20re-annotated%20questions%20across%20all%2057%20MMLU%20subjects.%20We%20estimate%20that%0A6.49%25%20of%20MMLU%20questions%20contain%20errors.%20Using%20MMLU-Redux%2C%20we%20demonstrate%0Asignificant%20discrepancies%20with%20the%20model%20performance%20metrics%20that%20were%0Aoriginally%20reported.%20Our%20results%20strongly%20advocate%20for%20revising%20MMLU%27s%0Aerror-ridden%20questions%20to%20enhance%20its%20future%20utility%20and%20reliability%20as%20a%0Abenchmark.%20https%3A//huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04127v3&entry.124074799=Read"},
{"title": "PySpatial: A High-Speed Whole Slide Image Pathomics Toolkit", "author": "Yuechen Yang and Yu Wang and Tianyuan Yao and Ruining Deng and Mengmeng Yin and Shilin Zhao and Haichun Yang and Yuankai Huo", "abstract": "  Whole Slide Image (WSI) analysis plays a crucial role in modern digital\npathology, enabling large-scale feature extraction from tissue samples.\nHowever, traditional feature extraction pipelines based on tools like\nCellProfiler often involve lengthy workflows, requiring WSI segmentation into\npatches, feature extraction at the patch level, and subsequent mapping back to\nthe original WSI. To address these challenges, we present PySpatial, a\nhigh-speed pathomics toolkit specifically designed for WSI-level analysis.\nPySpatial streamlines the conventional pipeline by directly operating on\ncomputational regions of interest, reducing redundant processing steps.\nUtilizing rtree-based spatial indexing and matrix-based computation, PySpatial\nefficiently maps and processes computational regions, significantly\naccelerating feature extraction while maintaining high accuracy. Our\nexperiments on two datasets-Perivascular Epithelioid Cell (PEC) and data from\nthe Kidney Precision Medicine Project (KPMP)-demonstrate substantial\nperformance improvements. For smaller and sparse objects in PEC datasets,\nPySpatial achieves nearly a 10-fold speedup compared to standard CellProfiler\npipelines. For larger objects, such as glomeruli and arteries in KPMP datasets,\nPySpatial achieves a 2-fold speedup. These results highlight PySpatial's\npotential to handle large-scale WSI analysis with enhanced efficiency and\naccuracy, paving the way for broader applications in digital pathology.\n", "link": "http://arxiv.org/abs/2501.06151v1", "date": "2025-01-10", "relevancy": 1.9079, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5025}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4599}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PySpatial%3A%20A%20High-Speed%20Whole%20Slide%20Image%20Pathomics%20Toolkit&body=Title%3A%20PySpatial%3A%20A%20High-Speed%20Whole%20Slide%20Image%20Pathomics%20Toolkit%0AAuthor%3A%20Yuechen%20Yang%20and%20Yu%20Wang%20and%20Tianyuan%20Yao%20and%20Ruining%20Deng%20and%20Mengmeng%20Yin%20and%20Shilin%20Zhao%20and%20Haichun%20Yang%20and%20Yuankai%20Huo%0AAbstract%3A%20%20%20Whole%20Slide%20Image%20%28WSI%29%20analysis%20plays%20a%20crucial%20role%20in%20modern%20digital%0Apathology%2C%20enabling%20large-scale%20feature%20extraction%20from%20tissue%20samples.%0AHowever%2C%20traditional%20feature%20extraction%20pipelines%20based%20on%20tools%20like%0ACellProfiler%20often%20involve%20lengthy%20workflows%2C%20requiring%20WSI%20segmentation%20into%0Apatches%2C%20feature%20extraction%20at%20the%20patch%20level%2C%20and%20subsequent%20mapping%20back%20to%0Athe%20original%20WSI.%20To%20address%20these%20challenges%2C%20we%20present%20PySpatial%2C%20a%0Ahigh-speed%20pathomics%20toolkit%20specifically%20designed%20for%20WSI-level%20analysis.%0APySpatial%20streamlines%20the%20conventional%20pipeline%20by%20directly%20operating%20on%0Acomputational%20regions%20of%20interest%2C%20reducing%20redundant%20processing%20steps.%0AUtilizing%20rtree-based%20spatial%20indexing%20and%20matrix-based%20computation%2C%20PySpatial%0Aefficiently%20maps%20and%20processes%20computational%20regions%2C%20significantly%0Aaccelerating%20feature%20extraction%20while%20maintaining%20high%20accuracy.%20Our%0Aexperiments%20on%20two%20datasets-Perivascular%20Epithelioid%20Cell%20%28PEC%29%20and%20data%20from%0Athe%20Kidney%20Precision%20Medicine%20Project%20%28KPMP%29-demonstrate%20substantial%0Aperformance%20improvements.%20For%20smaller%20and%20sparse%20objects%20in%20PEC%20datasets%2C%0APySpatial%20achieves%20nearly%20a%2010-fold%20speedup%20compared%20to%20standard%20CellProfiler%0Apipelines.%20For%20larger%20objects%2C%20such%20as%20glomeruli%20and%20arteries%20in%20KPMP%20datasets%2C%0APySpatial%20achieves%20a%202-fold%20speedup.%20These%20results%20highlight%20PySpatial%27s%0Apotential%20to%20handle%20large-scale%20WSI%20analysis%20with%20enhanced%20efficiency%20and%0Aaccuracy%2C%20paving%20the%20way%20for%20broader%20applications%20in%20digital%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPySpatial%253A%2520A%2520High-Speed%2520Whole%2520Slide%2520Image%2520Pathomics%2520Toolkit%26entry.906535625%3DYuechen%2520Yang%2520and%2520Yu%2520Wang%2520and%2520Tianyuan%2520Yao%2520and%2520Ruining%2520Deng%2520and%2520Mengmeng%2520Yin%2520and%2520Shilin%2520Zhao%2520and%2520Haichun%2520Yang%2520and%2520Yuankai%2520Huo%26entry.1292438233%3D%2520%2520Whole%2520Slide%2520Image%2520%2528WSI%2529%2520analysis%2520plays%2520a%2520crucial%2520role%2520in%2520modern%2520digital%250Apathology%252C%2520enabling%2520large-scale%2520feature%2520extraction%2520from%2520tissue%2520samples.%250AHowever%252C%2520traditional%2520feature%2520extraction%2520pipelines%2520based%2520on%2520tools%2520like%250ACellProfiler%2520often%2520involve%2520lengthy%2520workflows%252C%2520requiring%2520WSI%2520segmentation%2520into%250Apatches%252C%2520feature%2520extraction%2520at%2520the%2520patch%2520level%252C%2520and%2520subsequent%2520mapping%2520back%2520to%250Athe%2520original%2520WSI.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520PySpatial%252C%2520a%250Ahigh-speed%2520pathomics%2520toolkit%2520specifically%2520designed%2520for%2520WSI-level%2520analysis.%250APySpatial%2520streamlines%2520the%2520conventional%2520pipeline%2520by%2520directly%2520operating%2520on%250Acomputational%2520regions%2520of%2520interest%252C%2520reducing%2520redundant%2520processing%2520steps.%250AUtilizing%2520rtree-based%2520spatial%2520indexing%2520and%2520matrix-based%2520computation%252C%2520PySpatial%250Aefficiently%2520maps%2520and%2520processes%2520computational%2520regions%252C%2520significantly%250Aaccelerating%2520feature%2520extraction%2520while%2520maintaining%2520high%2520accuracy.%2520Our%250Aexperiments%2520on%2520two%2520datasets-Perivascular%2520Epithelioid%2520Cell%2520%2528PEC%2529%2520and%2520data%2520from%250Athe%2520Kidney%2520Precision%2520Medicine%2520Project%2520%2528KPMP%2529-demonstrate%2520substantial%250Aperformance%2520improvements.%2520For%2520smaller%2520and%2520sparse%2520objects%2520in%2520PEC%2520datasets%252C%250APySpatial%2520achieves%2520nearly%2520a%252010-fold%2520speedup%2520compared%2520to%2520standard%2520CellProfiler%250Apipelines.%2520For%2520larger%2520objects%252C%2520such%2520as%2520glomeruli%2520and%2520arteries%2520in%2520KPMP%2520datasets%252C%250APySpatial%2520achieves%2520a%25202-fold%2520speedup.%2520These%2520results%2520highlight%2520PySpatial%2527s%250Apotential%2520to%2520handle%2520large-scale%2520WSI%2520analysis%2520with%2520enhanced%2520efficiency%2520and%250Aaccuracy%252C%2520paving%2520the%2520way%2520for%2520broader%2520applications%2520in%2520digital%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PySpatial%3A%20A%20High-Speed%20Whole%20Slide%20Image%20Pathomics%20Toolkit&entry.906535625=Yuechen%20Yang%20and%20Yu%20Wang%20and%20Tianyuan%20Yao%20and%20Ruining%20Deng%20and%20Mengmeng%20Yin%20and%20Shilin%20Zhao%20and%20Haichun%20Yang%20and%20Yuankai%20Huo&entry.1292438233=%20%20Whole%20Slide%20Image%20%28WSI%29%20analysis%20plays%20a%20crucial%20role%20in%20modern%20digital%0Apathology%2C%20enabling%20large-scale%20feature%20extraction%20from%20tissue%20samples.%0AHowever%2C%20traditional%20feature%20extraction%20pipelines%20based%20on%20tools%20like%0ACellProfiler%20often%20involve%20lengthy%20workflows%2C%20requiring%20WSI%20segmentation%20into%0Apatches%2C%20feature%20extraction%20at%20the%20patch%20level%2C%20and%20subsequent%20mapping%20back%20to%0Athe%20original%20WSI.%20To%20address%20these%20challenges%2C%20we%20present%20PySpatial%2C%20a%0Ahigh-speed%20pathomics%20toolkit%20specifically%20designed%20for%20WSI-level%20analysis.%0APySpatial%20streamlines%20the%20conventional%20pipeline%20by%20directly%20operating%20on%0Acomputational%20regions%20of%20interest%2C%20reducing%20redundant%20processing%20steps.%0AUtilizing%20rtree-based%20spatial%20indexing%20and%20matrix-based%20computation%2C%20PySpatial%0Aefficiently%20maps%20and%20processes%20computational%20regions%2C%20significantly%0Aaccelerating%20feature%20extraction%20while%20maintaining%20high%20accuracy.%20Our%0Aexperiments%20on%20two%20datasets-Perivascular%20Epithelioid%20Cell%20%28PEC%29%20and%20data%20from%0Athe%20Kidney%20Precision%20Medicine%20Project%20%28KPMP%29-demonstrate%20substantial%0Aperformance%20improvements.%20For%20smaller%20and%20sparse%20objects%20in%20PEC%20datasets%2C%0APySpatial%20achieves%20nearly%20a%2010-fold%20speedup%20compared%20to%20standard%20CellProfiler%0Apipelines.%20For%20larger%20objects%2C%20such%20as%20glomeruli%20and%20arteries%20in%20KPMP%20datasets%2C%0APySpatial%20achieves%20a%202-fold%20speedup.%20These%20results%20highlight%20PySpatial%27s%0Apotential%20to%20handle%20large-scale%20WSI%20analysis%20with%20enhanced%20efficiency%20and%0Aaccuracy%2C%20paving%20the%20way%20for%20broader%20applications%20in%20digital%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06151v1&entry.124074799=Read"},
{"title": "Model Alignment Search", "author": "Satchel Grant", "abstract": "  When can we say that two neural systems are the same? The answer to this\nquestion is goal-dependent, and it is often addressed through correlative\nmethods such as Representational Similarity Analysis (RSA) and Centered Kernel\nAlignment (CKA). What do we miss when we forgo causal explorations, and how can\nwe target specific types of similarity? In this work, we introduce Model\nAlignment Search (MAS), a method for causally exploring distributed\nrepresentational similarity. The method learns invertible linear\ntransformations that align a subspace between two distributed networks'\nrepresentations where causal information can be freely interchanged. We first\nshow that the method can be used to transfer specific causal variables, such as\nthe number of items in a counting task, between networks with different\ntraining seeds. We then explore open questions in number cognition by comparing\ndifferent types of numeric representations in models trained on structurally\ndifferent numeric tasks. We then explore differences between MAS vs preexisting\ncausal similarity methods, showing MAS to be more resistant to unwanted\nexchanges. Lastly, we introduce a counterfactual latent auxiliary loss function\nthat helps shape causally relevant alignments even in cases where we do not\nhave causal access to one of the two models for training.\n", "link": "http://arxiv.org/abs/2501.06164v1", "date": "2025-01-10", "relevancy": 1.9074, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4993}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4764}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Alignment%20Search&body=Title%3A%20Model%20Alignment%20Search%0AAuthor%3A%20Satchel%20Grant%0AAbstract%3A%20%20%20When%20can%20we%20say%20that%20two%20neural%20systems%20are%20the%20same%3F%20The%20answer%20to%20this%0Aquestion%20is%20goal-dependent%2C%20and%20it%20is%20often%20addressed%20through%20correlative%0Amethods%20such%20as%20Representational%20Similarity%20Analysis%20%28RSA%29%20and%20Centered%20Kernel%0AAlignment%20%28CKA%29.%20What%20do%20we%20miss%20when%20we%20forgo%20causal%20explorations%2C%20and%20how%20can%0Awe%20target%20specific%20types%20of%20similarity%3F%20In%20this%20work%2C%20we%20introduce%20Model%0AAlignment%20Search%20%28MAS%29%2C%20a%20method%20for%20causally%20exploring%20distributed%0Arepresentational%20similarity.%20The%20method%20learns%20invertible%20linear%0Atransformations%20that%20align%20a%20subspace%20between%20two%20distributed%20networks%27%0Arepresentations%20where%20causal%20information%20can%20be%20freely%20interchanged.%20We%20first%0Ashow%20that%20the%20method%20can%20be%20used%20to%20transfer%20specific%20causal%20variables%2C%20such%20as%0Athe%20number%20of%20items%20in%20a%20counting%20task%2C%20between%20networks%20with%20different%0Atraining%20seeds.%20We%20then%20explore%20open%20questions%20in%20number%20cognition%20by%20comparing%0Adifferent%20types%20of%20numeric%20representations%20in%20models%20trained%20on%20structurally%0Adifferent%20numeric%20tasks.%20We%20then%20explore%20differences%20between%20MAS%20vs%20preexisting%0Acausal%20similarity%20methods%2C%20showing%20MAS%20to%20be%20more%20resistant%20to%20unwanted%0Aexchanges.%20Lastly%2C%20we%20introduce%20a%20counterfactual%20latent%20auxiliary%20loss%20function%0Athat%20helps%20shape%20causally%20relevant%20alignments%20even%20in%20cases%20where%20we%20do%20not%0Ahave%20causal%20access%20to%20one%20of%20the%20two%20models%20for%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Alignment%2520Search%26entry.906535625%3DSatchel%2520Grant%26entry.1292438233%3D%2520%2520When%2520can%2520we%2520say%2520that%2520two%2520neural%2520systems%2520are%2520the%2520same%253F%2520The%2520answer%2520to%2520this%250Aquestion%2520is%2520goal-dependent%252C%2520and%2520it%2520is%2520often%2520addressed%2520through%2520correlative%250Amethods%2520such%2520as%2520Representational%2520Similarity%2520Analysis%2520%2528RSA%2529%2520and%2520Centered%2520Kernel%250AAlignment%2520%2528CKA%2529.%2520What%2520do%2520we%2520miss%2520when%2520we%2520forgo%2520causal%2520explorations%252C%2520and%2520how%2520can%250Awe%2520target%2520specific%2520types%2520of%2520similarity%253F%2520In%2520this%2520work%252C%2520we%2520introduce%2520Model%250AAlignment%2520Search%2520%2528MAS%2529%252C%2520a%2520method%2520for%2520causally%2520exploring%2520distributed%250Arepresentational%2520similarity.%2520The%2520method%2520learns%2520invertible%2520linear%250Atransformations%2520that%2520align%2520a%2520subspace%2520between%2520two%2520distributed%2520networks%2527%250Arepresentations%2520where%2520causal%2520information%2520can%2520be%2520freely%2520interchanged.%2520We%2520first%250Ashow%2520that%2520the%2520method%2520can%2520be%2520used%2520to%2520transfer%2520specific%2520causal%2520variables%252C%2520such%2520as%250Athe%2520number%2520of%2520items%2520in%2520a%2520counting%2520task%252C%2520between%2520networks%2520with%2520different%250Atraining%2520seeds.%2520We%2520then%2520explore%2520open%2520questions%2520in%2520number%2520cognition%2520by%2520comparing%250Adifferent%2520types%2520of%2520numeric%2520representations%2520in%2520models%2520trained%2520on%2520structurally%250Adifferent%2520numeric%2520tasks.%2520We%2520then%2520explore%2520differences%2520between%2520MAS%2520vs%2520preexisting%250Acausal%2520similarity%2520methods%252C%2520showing%2520MAS%2520to%2520be%2520more%2520resistant%2520to%2520unwanted%250Aexchanges.%2520Lastly%252C%2520we%2520introduce%2520a%2520counterfactual%2520latent%2520auxiliary%2520loss%2520function%250Athat%2520helps%2520shape%2520causally%2520relevant%2520alignments%2520even%2520in%2520cases%2520where%2520we%2520do%2520not%250Ahave%2520causal%2520access%2520to%2520one%2520of%2520the%2520two%2520models%2520for%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Alignment%20Search&entry.906535625=Satchel%20Grant&entry.1292438233=%20%20When%20can%20we%20say%20that%20two%20neural%20systems%20are%20the%20same%3F%20The%20answer%20to%20this%0Aquestion%20is%20goal-dependent%2C%20and%20it%20is%20often%20addressed%20through%20correlative%0Amethods%20such%20as%20Representational%20Similarity%20Analysis%20%28RSA%29%20and%20Centered%20Kernel%0AAlignment%20%28CKA%29.%20What%20do%20we%20miss%20when%20we%20forgo%20causal%20explorations%2C%20and%20how%20can%0Awe%20target%20specific%20types%20of%20similarity%3F%20In%20this%20work%2C%20we%20introduce%20Model%0AAlignment%20Search%20%28MAS%29%2C%20a%20method%20for%20causally%20exploring%20distributed%0Arepresentational%20similarity.%20The%20method%20learns%20invertible%20linear%0Atransformations%20that%20align%20a%20subspace%20between%20two%20distributed%20networks%27%0Arepresentations%20where%20causal%20information%20can%20be%20freely%20interchanged.%20We%20first%0Ashow%20that%20the%20method%20can%20be%20used%20to%20transfer%20specific%20causal%20variables%2C%20such%20as%0Athe%20number%20of%20items%20in%20a%20counting%20task%2C%20between%20networks%20with%20different%0Atraining%20seeds.%20We%20then%20explore%20open%20questions%20in%20number%20cognition%20by%20comparing%0Adifferent%20types%20of%20numeric%20representations%20in%20models%20trained%20on%20structurally%0Adifferent%20numeric%20tasks.%20We%20then%20explore%20differences%20between%20MAS%20vs%20preexisting%0Acausal%20similarity%20methods%2C%20showing%20MAS%20to%20be%20more%20resistant%20to%20unwanted%0Aexchanges.%20Lastly%2C%20we%20introduce%20a%20counterfactual%20latent%20auxiliary%20loss%20function%0Athat%20helps%20shape%20causally%20relevant%20alignments%20even%20in%20cases%20where%20we%20do%20not%0Ahave%20causal%20access%20to%20one%20of%20the%20two%20models%20for%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06164v1&entry.124074799=Read"},
{"title": "Supervision policies can shape long-term risk management in\n  general-purpose AI models", "author": "Manuel Cebrian and Emilia Gomez and David Fernandez Llorca", "abstract": "  The rapid proliferation and deployment of General-Purpose AI (GPAI) models,\nincluding large language models (LLMs), present unprecedented challenges for AI\nsupervisory entities. We hypothesize that these entities will need to navigate\nan emergent ecosystem of risk and incident reporting, likely to exceed their\nsupervision capacity. To investigate this, we develop a simulation framework\nparameterized by features extracted from the diverse landscape of risk,\nincident, or hazard reporting ecosystems, including community-driven platforms,\ncrowdsourcing initiatives, and expert assessments. We evaluate four supervision\npolicies: non-prioritized (first-come, first-served), random selection,\npriority-based (addressing the highest-priority risks first), and\ndiversity-prioritized (balancing high-priority risks with comprehensive\ncoverage across risk types). Our results indicate that while priority-based and\ndiversity-prioritized policies are more effective at mitigating high-impact\nrisks, particularly those identified by experts, they may inadvertently neglect\nsystemic issues reported by the broader community. This oversight can create\nfeedback loops that amplify certain types of reporting while discouraging\nothers, leading to a skewed perception of the overall risk landscape. We\nvalidate our simulation results with several real-world datasets, including one\nwith over a million ChatGPT interactions, of which more than 150,000\nconversations were identified as risky. This validation underscores the complex\ntrade-offs inherent in AI risk supervision and highlights how the choice of\nrisk management policies can shape the future landscape of AI risks across\ndiverse GPAI models used in society.\n", "link": "http://arxiv.org/abs/2501.06137v1", "date": "2025-01-10", "relevancy": 1.8878, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4801}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4795}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervision%20policies%20can%20shape%20long-term%20risk%20management%20in%0A%20%20general-purpose%20AI%20models&body=Title%3A%20Supervision%20policies%20can%20shape%20long-term%20risk%20management%20in%0A%20%20general-purpose%20AI%20models%0AAuthor%3A%20Manuel%20Cebrian%20and%20Emilia%20Gomez%20and%20David%20Fernandez%20Llorca%0AAbstract%3A%20%20%20The%20rapid%20proliferation%20and%20deployment%20of%20General-Purpose%20AI%20%28GPAI%29%20models%2C%0Aincluding%20large%20language%20models%20%28LLMs%29%2C%20present%20unprecedented%20challenges%20for%20AI%0Asupervisory%20entities.%20We%20hypothesize%20that%20these%20entities%20will%20need%20to%20navigate%0Aan%20emergent%20ecosystem%20of%20risk%20and%20incident%20reporting%2C%20likely%20to%20exceed%20their%0Asupervision%20capacity.%20To%20investigate%20this%2C%20we%20develop%20a%20simulation%20framework%0Aparameterized%20by%20features%20extracted%20from%20the%20diverse%20landscape%20of%20risk%2C%0Aincident%2C%20or%20hazard%20reporting%20ecosystems%2C%20including%20community-driven%20platforms%2C%0Acrowdsourcing%20initiatives%2C%20and%20expert%20assessments.%20We%20evaluate%20four%20supervision%0Apolicies%3A%20non-prioritized%20%28first-come%2C%20first-served%29%2C%20random%20selection%2C%0Apriority-based%20%28addressing%20the%20highest-priority%20risks%20first%29%2C%20and%0Adiversity-prioritized%20%28balancing%20high-priority%20risks%20with%20comprehensive%0Acoverage%20across%20risk%20types%29.%20Our%20results%20indicate%20that%20while%20priority-based%20and%0Adiversity-prioritized%20policies%20are%20more%20effective%20at%20mitigating%20high-impact%0Arisks%2C%20particularly%20those%20identified%20by%20experts%2C%20they%20may%20inadvertently%20neglect%0Asystemic%20issues%20reported%20by%20the%20broader%20community.%20This%20oversight%20can%20create%0Afeedback%20loops%20that%20amplify%20certain%20types%20of%20reporting%20while%20discouraging%0Aothers%2C%20leading%20to%20a%20skewed%20perception%20of%20the%20overall%20risk%20landscape.%20We%0Avalidate%20our%20simulation%20results%20with%20several%20real-world%20datasets%2C%20including%20one%0Awith%20over%20a%20million%20ChatGPT%20interactions%2C%20of%20which%20more%20than%20150%2C000%0Aconversations%20were%20identified%20as%20risky.%20This%20validation%20underscores%20the%20complex%0Atrade-offs%20inherent%20in%20AI%20risk%20supervision%20and%20highlights%20how%20the%20choice%20of%0Arisk%20management%20policies%20can%20shape%20the%20future%20landscape%20of%20AI%20risks%20across%0Adiverse%20GPAI%20models%20used%20in%20society.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervision%2520policies%2520can%2520shape%2520long-term%2520risk%2520management%2520in%250A%2520%2520general-purpose%2520AI%2520models%26entry.906535625%3DManuel%2520Cebrian%2520and%2520Emilia%2520Gomez%2520and%2520David%2520Fernandez%2520Llorca%26entry.1292438233%3D%2520%2520The%2520rapid%2520proliferation%2520and%2520deployment%2520of%2520General-Purpose%2520AI%2520%2528GPAI%2529%2520models%252C%250Aincluding%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520present%2520unprecedented%2520challenges%2520for%2520AI%250Asupervisory%2520entities.%2520We%2520hypothesize%2520that%2520these%2520entities%2520will%2520need%2520to%2520navigate%250Aan%2520emergent%2520ecosystem%2520of%2520risk%2520and%2520incident%2520reporting%252C%2520likely%2520to%2520exceed%2520their%250Asupervision%2520capacity.%2520To%2520investigate%2520this%252C%2520we%2520develop%2520a%2520simulation%2520framework%250Aparameterized%2520by%2520features%2520extracted%2520from%2520the%2520diverse%2520landscape%2520of%2520risk%252C%250Aincident%252C%2520or%2520hazard%2520reporting%2520ecosystems%252C%2520including%2520community-driven%2520platforms%252C%250Acrowdsourcing%2520initiatives%252C%2520and%2520expert%2520assessments.%2520We%2520evaluate%2520four%2520supervision%250Apolicies%253A%2520non-prioritized%2520%2528first-come%252C%2520first-served%2529%252C%2520random%2520selection%252C%250Apriority-based%2520%2528addressing%2520the%2520highest-priority%2520risks%2520first%2529%252C%2520and%250Adiversity-prioritized%2520%2528balancing%2520high-priority%2520risks%2520with%2520comprehensive%250Acoverage%2520across%2520risk%2520types%2529.%2520Our%2520results%2520indicate%2520that%2520while%2520priority-based%2520and%250Adiversity-prioritized%2520policies%2520are%2520more%2520effective%2520at%2520mitigating%2520high-impact%250Arisks%252C%2520particularly%2520those%2520identified%2520by%2520experts%252C%2520they%2520may%2520inadvertently%2520neglect%250Asystemic%2520issues%2520reported%2520by%2520the%2520broader%2520community.%2520This%2520oversight%2520can%2520create%250Afeedback%2520loops%2520that%2520amplify%2520certain%2520types%2520of%2520reporting%2520while%2520discouraging%250Aothers%252C%2520leading%2520to%2520a%2520skewed%2520perception%2520of%2520the%2520overall%2520risk%2520landscape.%2520We%250Avalidate%2520our%2520simulation%2520results%2520with%2520several%2520real-world%2520datasets%252C%2520including%2520one%250Awith%2520over%2520a%2520million%2520ChatGPT%2520interactions%252C%2520of%2520which%2520more%2520than%2520150%252C000%250Aconversations%2520were%2520identified%2520as%2520risky.%2520This%2520validation%2520underscores%2520the%2520complex%250Atrade-offs%2520inherent%2520in%2520AI%2520risk%2520supervision%2520and%2520highlights%2520how%2520the%2520choice%2520of%250Arisk%2520management%2520policies%2520can%2520shape%2520the%2520future%2520landscape%2520of%2520AI%2520risks%2520across%250Adiverse%2520GPAI%2520models%2520used%2520in%2520society.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervision%20policies%20can%20shape%20long-term%20risk%20management%20in%0A%20%20general-purpose%20AI%20models&entry.906535625=Manuel%20Cebrian%20and%20Emilia%20Gomez%20and%20David%20Fernandez%20Llorca&entry.1292438233=%20%20The%20rapid%20proliferation%20and%20deployment%20of%20General-Purpose%20AI%20%28GPAI%29%20models%2C%0Aincluding%20large%20language%20models%20%28LLMs%29%2C%20present%20unprecedented%20challenges%20for%20AI%0Asupervisory%20entities.%20We%20hypothesize%20that%20these%20entities%20will%20need%20to%20navigate%0Aan%20emergent%20ecosystem%20of%20risk%20and%20incident%20reporting%2C%20likely%20to%20exceed%20their%0Asupervision%20capacity.%20To%20investigate%20this%2C%20we%20develop%20a%20simulation%20framework%0Aparameterized%20by%20features%20extracted%20from%20the%20diverse%20landscape%20of%20risk%2C%0Aincident%2C%20or%20hazard%20reporting%20ecosystems%2C%20including%20community-driven%20platforms%2C%0Acrowdsourcing%20initiatives%2C%20and%20expert%20assessments.%20We%20evaluate%20four%20supervision%0Apolicies%3A%20non-prioritized%20%28first-come%2C%20first-served%29%2C%20random%20selection%2C%0Apriority-based%20%28addressing%20the%20highest-priority%20risks%20first%29%2C%20and%0Adiversity-prioritized%20%28balancing%20high-priority%20risks%20with%20comprehensive%0Acoverage%20across%20risk%20types%29.%20Our%20results%20indicate%20that%20while%20priority-based%20and%0Adiversity-prioritized%20policies%20are%20more%20effective%20at%20mitigating%20high-impact%0Arisks%2C%20particularly%20those%20identified%20by%20experts%2C%20they%20may%20inadvertently%20neglect%0Asystemic%20issues%20reported%20by%20the%20broader%20community.%20This%20oversight%20can%20create%0Afeedback%20loops%20that%20amplify%20certain%20types%20of%20reporting%20while%20discouraging%0Aothers%2C%20leading%20to%20a%20skewed%20perception%20of%20the%20overall%20risk%20landscape.%20We%0Avalidate%20our%20simulation%20results%20with%20several%20real-world%20datasets%2C%20including%20one%0Awith%20over%20a%20million%20ChatGPT%20interactions%2C%20of%20which%20more%20than%20150%2C000%0Aconversations%20were%20identified%20as%20risky.%20This%20validation%20underscores%20the%20complex%0Atrade-offs%20inherent%20in%20AI%20risk%20supervision%20and%20highlights%20how%20the%20choice%20of%0Arisk%20management%20policies%20can%20shape%20the%20future%20landscape%20of%20AI%20risks%20across%0Adiverse%20GPAI%20models%20used%20in%20society.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06137v1&entry.124074799=Read"},
{"title": "Model Inversion in Split Learning for Personalized LLMs: New Insights\n  from Information Bottleneck Theory", "author": "Yunmeng Shu and Shaofeng Li and Tian Dong and Yan Meng and Haojin Zhu", "abstract": "  Personalized Large Language Models (LLMs) have become increasingly prevalent,\nshowcasing the impressive capabilities of models like GPT-4. This trend has\nalso catalyzed extensive research on deploying LLMs on mobile devices. Feasible\napproaches for such edge-cloud deployment include using split learning.\nHowever, previous research has largely overlooked the privacy leakage\nassociated with intermediate representations transmitted from devices to\nservers. This work is the first to identify model inversion attacks in the\nsplit learning framework for LLMs, emphasizing the necessity of secure defense.\nFor the first time, we introduce mutual information entropy to understand the\ninformation propagation of Transformer-based LLMs and assess privacy attack\nperformance for LLM blocks. To address the issue of representations being\nsparser and containing less information than embeddings, we propose a two-stage\nattack system in which the first part projects representations into the\nembedding space, and the second part uses a generative model to recover text\nfrom these embeddings. This design breaks down the complexity and achieves\nattack scores of 38%-75% in various scenarios, with an over 60% improvement\nover the SOTA. This work comprehensively highlights the potential privacy risks\nduring the deployment of personalized LLMs on the edge side.\n", "link": "http://arxiv.org/abs/2501.05965v1", "date": "2025-01-10", "relevancy": 1.8851, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.517}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4677}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Inversion%20in%20Split%20Learning%20for%20Personalized%20LLMs%3A%20New%20Insights%0A%20%20from%20Information%20Bottleneck%20Theory&body=Title%3A%20Model%20Inversion%20in%20Split%20Learning%20for%20Personalized%20LLMs%3A%20New%20Insights%0A%20%20from%20Information%20Bottleneck%20Theory%0AAuthor%3A%20Yunmeng%20Shu%20and%20Shaofeng%20Li%20and%20Tian%20Dong%20and%20Yan%20Meng%20and%20Haojin%20Zhu%0AAbstract%3A%20%20%20Personalized%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20increasingly%20prevalent%2C%0Ashowcasing%20the%20impressive%20capabilities%20of%20models%20like%20GPT-4.%20This%20trend%20has%0Aalso%20catalyzed%20extensive%20research%20on%20deploying%20LLMs%20on%20mobile%20devices.%20Feasible%0Aapproaches%20for%20such%20edge-cloud%20deployment%20include%20using%20split%20learning.%0AHowever%2C%20previous%20research%20has%20largely%20overlooked%20the%20privacy%20leakage%0Aassociated%20with%20intermediate%20representations%20transmitted%20from%20devices%20to%0Aservers.%20This%20work%20is%20the%20first%20to%20identify%20model%20inversion%20attacks%20in%20the%0Asplit%20learning%20framework%20for%20LLMs%2C%20emphasizing%20the%20necessity%20of%20secure%20defense.%0AFor%20the%20first%20time%2C%20we%20introduce%20mutual%20information%20entropy%20to%20understand%20the%0Ainformation%20propagation%20of%20Transformer-based%20LLMs%20and%20assess%20privacy%20attack%0Aperformance%20for%20LLM%20blocks.%20To%20address%20the%20issue%20of%20representations%20being%0Asparser%20and%20containing%20less%20information%20than%20embeddings%2C%20we%20propose%20a%20two-stage%0Aattack%20system%20in%20which%20the%20first%20part%20projects%20representations%20into%20the%0Aembedding%20space%2C%20and%20the%20second%20part%20uses%20a%20generative%20model%20to%20recover%20text%0Afrom%20these%20embeddings.%20This%20design%20breaks%20down%20the%20complexity%20and%20achieves%0Aattack%20scores%20of%2038%25-75%25%20in%20various%20scenarios%2C%20with%20an%20over%2060%25%20improvement%0Aover%20the%20SOTA.%20This%20work%20comprehensively%20highlights%20the%20potential%20privacy%20risks%0Aduring%20the%20deployment%20of%20personalized%20LLMs%20on%20the%20edge%20side.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Inversion%2520in%2520Split%2520Learning%2520for%2520Personalized%2520LLMs%253A%2520New%2520Insights%250A%2520%2520from%2520Information%2520Bottleneck%2520Theory%26entry.906535625%3DYunmeng%2520Shu%2520and%2520Shaofeng%2520Li%2520and%2520Tian%2520Dong%2520and%2520Yan%2520Meng%2520and%2520Haojin%2520Zhu%26entry.1292438233%3D%2520%2520Personalized%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520increasingly%2520prevalent%252C%250Ashowcasing%2520the%2520impressive%2520capabilities%2520of%2520models%2520like%2520GPT-4.%2520This%2520trend%2520has%250Aalso%2520catalyzed%2520extensive%2520research%2520on%2520deploying%2520LLMs%2520on%2520mobile%2520devices.%2520Feasible%250Aapproaches%2520for%2520such%2520edge-cloud%2520deployment%2520include%2520using%2520split%2520learning.%250AHowever%252C%2520previous%2520research%2520has%2520largely%2520overlooked%2520the%2520privacy%2520leakage%250Aassociated%2520with%2520intermediate%2520representations%2520transmitted%2520from%2520devices%2520to%250Aservers.%2520This%2520work%2520is%2520the%2520first%2520to%2520identify%2520model%2520inversion%2520attacks%2520in%2520the%250Asplit%2520learning%2520framework%2520for%2520LLMs%252C%2520emphasizing%2520the%2520necessity%2520of%2520secure%2520defense.%250AFor%2520the%2520first%2520time%252C%2520we%2520introduce%2520mutual%2520information%2520entropy%2520to%2520understand%2520the%250Ainformation%2520propagation%2520of%2520Transformer-based%2520LLMs%2520and%2520assess%2520privacy%2520attack%250Aperformance%2520for%2520LLM%2520blocks.%2520To%2520address%2520the%2520issue%2520of%2520representations%2520being%250Asparser%2520and%2520containing%2520less%2520information%2520than%2520embeddings%252C%2520we%2520propose%2520a%2520two-stage%250Aattack%2520system%2520in%2520which%2520the%2520first%2520part%2520projects%2520representations%2520into%2520the%250Aembedding%2520space%252C%2520and%2520the%2520second%2520part%2520uses%2520a%2520generative%2520model%2520to%2520recover%2520text%250Afrom%2520these%2520embeddings.%2520This%2520design%2520breaks%2520down%2520the%2520complexity%2520and%2520achieves%250Aattack%2520scores%2520of%252038%2525-75%2525%2520in%2520various%2520scenarios%252C%2520with%2520an%2520over%252060%2525%2520improvement%250Aover%2520the%2520SOTA.%2520This%2520work%2520comprehensively%2520highlights%2520the%2520potential%2520privacy%2520risks%250Aduring%2520the%2520deployment%2520of%2520personalized%2520LLMs%2520on%2520the%2520edge%2520side.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Inversion%20in%20Split%20Learning%20for%20Personalized%20LLMs%3A%20New%20Insights%0A%20%20from%20Information%20Bottleneck%20Theory&entry.906535625=Yunmeng%20Shu%20and%20Shaofeng%20Li%20and%20Tian%20Dong%20and%20Yan%20Meng%20and%20Haojin%20Zhu&entry.1292438233=%20%20Personalized%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20increasingly%20prevalent%2C%0Ashowcasing%20the%20impressive%20capabilities%20of%20models%20like%20GPT-4.%20This%20trend%20has%0Aalso%20catalyzed%20extensive%20research%20on%20deploying%20LLMs%20on%20mobile%20devices.%20Feasible%0Aapproaches%20for%20such%20edge-cloud%20deployment%20include%20using%20split%20learning.%0AHowever%2C%20previous%20research%20has%20largely%20overlooked%20the%20privacy%20leakage%0Aassociated%20with%20intermediate%20representations%20transmitted%20from%20devices%20to%0Aservers.%20This%20work%20is%20the%20first%20to%20identify%20model%20inversion%20attacks%20in%20the%0Asplit%20learning%20framework%20for%20LLMs%2C%20emphasizing%20the%20necessity%20of%20secure%20defense.%0AFor%20the%20first%20time%2C%20we%20introduce%20mutual%20information%20entropy%20to%20understand%20the%0Ainformation%20propagation%20of%20Transformer-based%20LLMs%20and%20assess%20privacy%20attack%0Aperformance%20for%20LLM%20blocks.%20To%20address%20the%20issue%20of%20representations%20being%0Asparser%20and%20containing%20less%20information%20than%20embeddings%2C%20we%20propose%20a%20two-stage%0Aattack%20system%20in%20which%20the%20first%20part%20projects%20representations%20into%20the%0Aembedding%20space%2C%20and%20the%20second%20part%20uses%20a%20generative%20model%20to%20recover%20text%0Afrom%20these%20embeddings.%20This%20design%20breaks%20down%20the%20complexity%20and%20achieves%0Aattack%20scores%20of%2038%25-75%25%20in%20various%20scenarios%2C%20with%20an%20over%2060%25%20improvement%0Aover%20the%20SOTA.%20This%20work%20comprehensively%20highlights%20the%20potential%20privacy%20risks%0Aduring%20the%20deployment%20of%20personalized%20LLMs%20on%20the%20edge%20side.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05965v1&entry.124074799=Read"},
{"title": "Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented\n  Conversational AI", "author": "Yuya Asano and Sabit Hassan and Paras Sharma and Anthony Sicilia and Katherine Atwell and Diane Litman and Malihe Alikhani", "abstract": "  General-purpose automatic speech recognition (ASR) systems do not always\nperform well in goal-oriented dialogue. Existing ASR correction methods rely on\nprior user data or named entities. We extend correction to tasks that have no\nprior user data and exhibit linguistic flexibility such as lexical and\nsyntactic variations. We propose a novel context augmentation with a large\nlanguage model and a ranking strategy that incorporates contextual information\nfrom the dialogue states of a goal-oriented conversational AI and its tasks.\nOur method ranks (1) n-best ASR hypotheses by their lexical and semantic\nsimilarity with context and (2) context by phonetic correspondence with ASR\nhypotheses. Evaluated in home improvement and cooking domains with real-world\nusers, our method improves recall and F1 of correction by 34% and 16%,\nrespectively, while maintaining precision and false positive rate. Users rated\n.8-1 point (out of 5) higher when our correction method worked properly, with\nno decrease due to false positives.\n", "link": "http://arxiv.org/abs/2501.06129v1", "date": "2025-01-10", "relevancy": 1.8834, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20ASR%20Error%20Handling%20with%20LLMs%20Augmentation%20for%20Goal-Oriented%0A%20%20Conversational%20AI&body=Title%3A%20Contextual%20ASR%20Error%20Handling%20with%20LLMs%20Augmentation%20for%20Goal-Oriented%0A%20%20Conversational%20AI%0AAuthor%3A%20Yuya%20Asano%20and%20Sabit%20Hassan%20and%20Paras%20Sharma%20and%20Anthony%20Sicilia%20and%20Katherine%20Atwell%20and%20Diane%20Litman%20and%20Malihe%20Alikhani%0AAbstract%3A%20%20%20General-purpose%20automatic%20speech%20recognition%20%28ASR%29%20systems%20do%20not%20always%0Aperform%20well%20in%20goal-oriented%20dialogue.%20Existing%20ASR%20correction%20methods%20rely%20on%0Aprior%20user%20data%20or%20named%20entities.%20We%20extend%20correction%20to%20tasks%20that%20have%20no%0Aprior%20user%20data%20and%20exhibit%20linguistic%20flexibility%20such%20as%20lexical%20and%0Asyntactic%20variations.%20We%20propose%20a%20novel%20context%20augmentation%20with%20a%20large%0Alanguage%20model%20and%20a%20ranking%20strategy%20that%20incorporates%20contextual%20information%0Afrom%20the%20dialogue%20states%20of%20a%20goal-oriented%20conversational%20AI%20and%20its%20tasks.%0AOur%20method%20ranks%20%281%29%20n-best%20ASR%20hypotheses%20by%20their%20lexical%20and%20semantic%0Asimilarity%20with%20context%20and%20%282%29%20context%20by%20phonetic%20correspondence%20with%20ASR%0Ahypotheses.%20Evaluated%20in%20home%20improvement%20and%20cooking%20domains%20with%20real-world%0Ausers%2C%20our%20method%20improves%20recall%20and%20F1%20of%20correction%20by%2034%25%20and%2016%25%2C%0Arespectively%2C%20while%20maintaining%20precision%20and%20false%20positive%20rate.%20Users%20rated%0A.8-1%20point%20%28out%20of%205%29%20higher%20when%20our%20correction%20method%20worked%20properly%2C%20with%0Ano%20decrease%20due%20to%20false%20positives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520ASR%2520Error%2520Handling%2520with%2520LLMs%2520Augmentation%2520for%2520Goal-Oriented%250A%2520%2520Conversational%2520AI%26entry.906535625%3DYuya%2520Asano%2520and%2520Sabit%2520Hassan%2520and%2520Paras%2520Sharma%2520and%2520Anthony%2520Sicilia%2520and%2520Katherine%2520Atwell%2520and%2520Diane%2520Litman%2520and%2520Malihe%2520Alikhani%26entry.1292438233%3D%2520%2520General-purpose%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520systems%2520do%2520not%2520always%250Aperform%2520well%2520in%2520goal-oriented%2520dialogue.%2520Existing%2520ASR%2520correction%2520methods%2520rely%2520on%250Aprior%2520user%2520data%2520or%2520named%2520entities.%2520We%2520extend%2520correction%2520to%2520tasks%2520that%2520have%2520no%250Aprior%2520user%2520data%2520and%2520exhibit%2520linguistic%2520flexibility%2520such%2520as%2520lexical%2520and%250Asyntactic%2520variations.%2520We%2520propose%2520a%2520novel%2520context%2520augmentation%2520with%2520a%2520large%250Alanguage%2520model%2520and%2520a%2520ranking%2520strategy%2520that%2520incorporates%2520contextual%2520information%250Afrom%2520the%2520dialogue%2520states%2520of%2520a%2520goal-oriented%2520conversational%2520AI%2520and%2520its%2520tasks.%250AOur%2520method%2520ranks%2520%25281%2529%2520n-best%2520ASR%2520hypotheses%2520by%2520their%2520lexical%2520and%2520semantic%250Asimilarity%2520with%2520context%2520and%2520%25282%2529%2520context%2520by%2520phonetic%2520correspondence%2520with%2520ASR%250Ahypotheses.%2520Evaluated%2520in%2520home%2520improvement%2520and%2520cooking%2520domains%2520with%2520real-world%250Ausers%252C%2520our%2520method%2520improves%2520recall%2520and%2520F1%2520of%2520correction%2520by%252034%2525%2520and%252016%2525%252C%250Arespectively%252C%2520while%2520maintaining%2520precision%2520and%2520false%2520positive%2520rate.%2520Users%2520rated%250A.8-1%2520point%2520%2528out%2520of%25205%2529%2520higher%2520when%2520our%2520correction%2520method%2520worked%2520properly%252C%2520with%250Ano%2520decrease%2520due%2520to%2520false%2520positives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20ASR%20Error%20Handling%20with%20LLMs%20Augmentation%20for%20Goal-Oriented%0A%20%20Conversational%20AI&entry.906535625=Yuya%20Asano%20and%20Sabit%20Hassan%20and%20Paras%20Sharma%20and%20Anthony%20Sicilia%20and%20Katherine%20Atwell%20and%20Diane%20Litman%20and%20Malihe%20Alikhani&entry.1292438233=%20%20General-purpose%20automatic%20speech%20recognition%20%28ASR%29%20systems%20do%20not%20always%0Aperform%20well%20in%20goal-oriented%20dialogue.%20Existing%20ASR%20correction%20methods%20rely%20on%0Aprior%20user%20data%20or%20named%20entities.%20We%20extend%20correction%20to%20tasks%20that%20have%20no%0Aprior%20user%20data%20and%20exhibit%20linguistic%20flexibility%20such%20as%20lexical%20and%0Asyntactic%20variations.%20We%20propose%20a%20novel%20context%20augmentation%20with%20a%20large%0Alanguage%20model%20and%20a%20ranking%20strategy%20that%20incorporates%20contextual%20information%0Afrom%20the%20dialogue%20states%20of%20a%20goal-oriented%20conversational%20AI%20and%20its%20tasks.%0AOur%20method%20ranks%20%281%29%20n-best%20ASR%20hypotheses%20by%20their%20lexical%20and%20semantic%0Asimilarity%20with%20context%20and%20%282%29%20context%20by%20phonetic%20correspondence%20with%20ASR%0Ahypotheses.%20Evaluated%20in%20home%20improvement%20and%20cooking%20domains%20with%20real-world%0Ausers%2C%20our%20method%20improves%20recall%20and%20F1%20of%20correction%20by%2034%25%20and%2016%25%2C%0Arespectively%2C%20while%20maintaining%20precision%20and%20false%20positive%20rate.%20Users%20rated%0A.8-1%20point%20%28out%20of%205%29%20higher%20when%20our%20correction%20method%20worked%20properly%2C%20with%0Ano%20decrease%20due%20to%20false%20positives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06129v1&entry.124074799=Read"},
{"title": "Uncovering the Genetic Basis of Glioblastoma Heterogeneity through\n  Multimodal Analysis of Whole Slide Images and RNA Sequencing Data", "author": "Ahmad Berjaoui and Louis Roussel and Eduardo Hugo Sanchez and Elizabeth Cohen-Jonathan Moyal", "abstract": "  Glioblastoma is a highly aggressive form of brain cancer characterized by\nrapid progression and poor prognosis. Despite advances in treatment, the\nunderlying genetic mechanisms driving this aggressiveness remain poorly\nunderstood. In this study, we employed multimodal deep learning approaches to\ninvestigate glioblastoma heterogeneity using joint image/RNA-seq analysis. Our\nresults reveal novel genes associated with glioblastoma. By leveraging a\ncombination of whole-slide images and RNA-seq, as well as introducing novel\nmethods to encode RNA-seq data, we identified specific genetic profiles that\nmay explain different patterns of glioblastoma progression. These findings\nprovide new insights into the genetic mechanisms underlying glioblastoma\nheterogeneity and highlight potential targets for therapeutic intervention.\n", "link": "http://arxiv.org/abs/2410.18710v2", "date": "2025-01-10", "relevancy": 1.8818, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20the%20Genetic%20Basis%20of%20Glioblastoma%20Heterogeneity%20through%0A%20%20Multimodal%20Analysis%20of%20Whole%20Slide%20Images%20and%20RNA%20Sequencing%20Data&body=Title%3A%20Uncovering%20the%20Genetic%20Basis%20of%20Glioblastoma%20Heterogeneity%20through%0A%20%20Multimodal%20Analysis%20of%20Whole%20Slide%20Images%20and%20RNA%20Sequencing%20Data%0AAuthor%3A%20Ahmad%20Berjaoui%20and%20Louis%20Roussel%20and%20Eduardo%20Hugo%20Sanchez%20and%20Elizabeth%20Cohen-Jonathan%20Moyal%0AAbstract%3A%20%20%20Glioblastoma%20is%20a%20highly%20aggressive%20form%20of%20brain%20cancer%20characterized%20by%0Arapid%20progression%20and%20poor%20prognosis.%20Despite%20advances%20in%20treatment%2C%20the%0Aunderlying%20genetic%20mechanisms%20driving%20this%20aggressiveness%20remain%20poorly%0Aunderstood.%20In%20this%20study%2C%20we%20employed%20multimodal%20deep%20learning%20approaches%20to%0Ainvestigate%20glioblastoma%20heterogeneity%20using%20joint%20image/RNA-seq%20analysis.%20Our%0Aresults%20reveal%20novel%20genes%20associated%20with%20glioblastoma.%20By%20leveraging%20a%0Acombination%20of%20whole-slide%20images%20and%20RNA-seq%2C%20as%20well%20as%20introducing%20novel%0Amethods%20to%20encode%20RNA-seq%20data%2C%20we%20identified%20specific%20genetic%20profiles%20that%0Amay%20explain%20different%20patterns%20of%20glioblastoma%20progression.%20These%20findings%0Aprovide%20new%20insights%20into%20the%20genetic%20mechanisms%20underlying%20glioblastoma%0Aheterogeneity%20and%20highlight%20potential%20targets%20for%20therapeutic%20intervention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520the%2520Genetic%2520Basis%2520of%2520Glioblastoma%2520Heterogeneity%2520through%250A%2520%2520Multimodal%2520Analysis%2520of%2520Whole%2520Slide%2520Images%2520and%2520RNA%2520Sequencing%2520Data%26entry.906535625%3DAhmad%2520Berjaoui%2520and%2520Louis%2520Roussel%2520and%2520Eduardo%2520Hugo%2520Sanchez%2520and%2520Elizabeth%2520Cohen-Jonathan%2520Moyal%26entry.1292438233%3D%2520%2520Glioblastoma%2520is%2520a%2520highly%2520aggressive%2520form%2520of%2520brain%2520cancer%2520characterized%2520by%250Arapid%2520progression%2520and%2520poor%2520prognosis.%2520Despite%2520advances%2520in%2520treatment%252C%2520the%250Aunderlying%2520genetic%2520mechanisms%2520driving%2520this%2520aggressiveness%2520remain%2520poorly%250Aunderstood.%2520In%2520this%2520study%252C%2520we%2520employed%2520multimodal%2520deep%2520learning%2520approaches%2520to%250Ainvestigate%2520glioblastoma%2520heterogeneity%2520using%2520joint%2520image/RNA-seq%2520analysis.%2520Our%250Aresults%2520reveal%2520novel%2520genes%2520associated%2520with%2520glioblastoma.%2520By%2520leveraging%2520a%250Acombination%2520of%2520whole-slide%2520images%2520and%2520RNA-seq%252C%2520as%2520well%2520as%2520introducing%2520novel%250Amethods%2520to%2520encode%2520RNA-seq%2520data%252C%2520we%2520identified%2520specific%2520genetic%2520profiles%2520that%250Amay%2520explain%2520different%2520patterns%2520of%2520glioblastoma%2520progression.%2520These%2520findings%250Aprovide%2520new%2520insights%2520into%2520the%2520genetic%2520mechanisms%2520underlying%2520glioblastoma%250Aheterogeneity%2520and%2520highlight%2520potential%2520targets%2520for%2520therapeutic%2520intervention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20the%20Genetic%20Basis%20of%20Glioblastoma%20Heterogeneity%20through%0A%20%20Multimodal%20Analysis%20of%20Whole%20Slide%20Images%20and%20RNA%20Sequencing%20Data&entry.906535625=Ahmad%20Berjaoui%20and%20Louis%20Roussel%20and%20Eduardo%20Hugo%20Sanchez%20and%20Elizabeth%20Cohen-Jonathan%20Moyal&entry.1292438233=%20%20Glioblastoma%20is%20a%20highly%20aggressive%20form%20of%20brain%20cancer%20characterized%20by%0Arapid%20progression%20and%20poor%20prognosis.%20Despite%20advances%20in%20treatment%2C%20the%0Aunderlying%20genetic%20mechanisms%20driving%20this%20aggressiveness%20remain%20poorly%0Aunderstood.%20In%20this%20study%2C%20we%20employed%20multimodal%20deep%20learning%20approaches%20to%0Ainvestigate%20glioblastoma%20heterogeneity%20using%20joint%20image/RNA-seq%20analysis.%20Our%0Aresults%20reveal%20novel%20genes%20associated%20with%20glioblastoma.%20By%20leveraging%20a%0Acombination%20of%20whole-slide%20images%20and%20RNA-seq%2C%20as%20well%20as%20introducing%20novel%0Amethods%20to%20encode%20RNA-seq%20data%2C%20we%20identified%20specific%20genetic%20profiles%20that%0Amay%20explain%20different%20patterns%20of%20glioblastoma%20progression.%20These%20findings%0Aprovide%20new%20insights%20into%20the%20genetic%20mechanisms%20underlying%20glioblastoma%0Aheterogeneity%20and%20highlight%20potential%20targets%20for%20therapeutic%20intervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18710v2&entry.124074799=Read"},
{"title": "Beyond Flat Text: Dual Self-inherited Guidance for Visual Text\n  Generation", "author": "Minxing Luo and Zixun Xia and Liaojun Chen and Zhenhang Li and Weichao Zeng and Jianye Wang and Wentao Cheng and Yaxing Wang and Yu Zhou and Jian Yang", "abstract": "  In real-world images, slanted or curved texts, especially those on cans,\nbanners, or badges, appear as frequently, if not more so, than flat texts due\nto artistic design or layout constraints. While high-quality visual text\ngeneration has become available with the advanced generative capabilities of\ndiffusion models, these models often produce distorted text and inharmonious\ntext background when given slanted or curved text layouts due to training data\nlimitation. In this paper, we introduce a new training-free framework, STGen,\nwhich accurately generates visual texts in challenging scenarios (\\eg, slanted\nor curved text layouts) while harmonizing them with the text background. Our\nframework decomposes the visual text generation process into two branches: (i)\n\\textbf{Semantic Rectification Branch}, which leverages the ability in\ngenerating flat but accurate visual texts of the model to guide the generation\nof challenging scenarios. The generated latent of flat text is abundant in\naccurate semantic information related both to the text itself and its\nbackground. By incorporating this, we rectify the semantic information of the\ntexts and harmonize the integration of the text with its background in complex\nlayouts. (ii) \\textbf{Structure Injection Branch}, which reinforces the visual\ntext structure during inference. We incorporate the latent information of the\nglyph image, rich in glyph structure, as a new condition to further strengthen\nthe text structure. To enhance image harmony, we also apply an effective\ncombination method to merge the priors, providing a solid foundation for\ngeneration. Extensive experiments across a variety of visual text layouts\ndemonstrate that our framework achieves superior accuracy and outstanding\nquality.\n", "link": "http://arxiv.org/abs/2501.05892v1", "date": "2025-01-10", "relevancy": 1.8744, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6529}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6266}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Flat%20Text%3A%20Dual%20Self-inherited%20Guidance%20for%20Visual%20Text%0A%20%20Generation&body=Title%3A%20Beyond%20Flat%20Text%3A%20Dual%20Self-inherited%20Guidance%20for%20Visual%20Text%0A%20%20Generation%0AAuthor%3A%20Minxing%20Luo%20and%20Zixun%20Xia%20and%20Liaojun%20Chen%20and%20Zhenhang%20Li%20and%20Weichao%20Zeng%20and%20Jianye%20Wang%20and%20Wentao%20Cheng%20and%20Yaxing%20Wang%20and%20Yu%20Zhou%20and%20Jian%20Yang%0AAbstract%3A%20%20%20In%20real-world%20images%2C%20slanted%20or%20curved%20texts%2C%20especially%20those%20on%20cans%2C%0Abanners%2C%20or%20badges%2C%20appear%20as%20frequently%2C%20if%20not%20more%20so%2C%20than%20flat%20texts%20due%0Ato%20artistic%20design%20or%20layout%20constraints.%20While%20high-quality%20visual%20text%0Ageneration%20has%20become%20available%20with%20the%20advanced%20generative%20capabilities%20of%0Adiffusion%20models%2C%20these%20models%20often%20produce%20distorted%20text%20and%20inharmonious%0Atext%20background%20when%20given%20slanted%20or%20curved%20text%20layouts%20due%20to%20training%20data%0Alimitation.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20training-free%20framework%2C%20STGen%2C%0Awhich%20accurately%20generates%20visual%20texts%20in%20challenging%20scenarios%20%28%5Ceg%2C%20slanted%0Aor%20curved%20text%20layouts%29%20while%20harmonizing%20them%20with%20the%20text%20background.%20Our%0Aframework%20decomposes%20the%20visual%20text%20generation%20process%20into%20two%20branches%3A%20%28i%29%0A%5Ctextbf%7BSemantic%20Rectification%20Branch%7D%2C%20which%20leverages%20the%20ability%20in%0Agenerating%20flat%20but%20accurate%20visual%20texts%20of%20the%20model%20to%20guide%20the%20generation%0Aof%20challenging%20scenarios.%20The%20generated%20latent%20of%20flat%20text%20is%20abundant%20in%0Aaccurate%20semantic%20information%20related%20both%20to%20the%20text%20itself%20and%20its%0Abackground.%20By%20incorporating%20this%2C%20we%20rectify%20the%20semantic%20information%20of%20the%0Atexts%20and%20harmonize%20the%20integration%20of%20the%20text%20with%20its%20background%20in%20complex%0Alayouts.%20%28ii%29%20%5Ctextbf%7BStructure%20Injection%20Branch%7D%2C%20which%20reinforces%20the%20visual%0Atext%20structure%20during%20inference.%20We%20incorporate%20the%20latent%20information%20of%20the%0Aglyph%20image%2C%20rich%20in%20glyph%20structure%2C%20as%20a%20new%20condition%20to%20further%20strengthen%0Athe%20text%20structure.%20To%20enhance%20image%20harmony%2C%20we%20also%20apply%20an%20effective%0Acombination%20method%20to%20merge%20the%20priors%2C%20providing%20a%20solid%20foundation%20for%0Ageneration.%20Extensive%20experiments%20across%20a%20variety%20of%20visual%20text%20layouts%0Ademonstrate%20that%20our%20framework%20achieves%20superior%20accuracy%20and%20outstanding%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Flat%2520Text%253A%2520Dual%2520Self-inherited%2520Guidance%2520for%2520Visual%2520Text%250A%2520%2520Generation%26entry.906535625%3DMinxing%2520Luo%2520and%2520Zixun%2520Xia%2520and%2520Liaojun%2520Chen%2520and%2520Zhenhang%2520Li%2520and%2520Weichao%2520Zeng%2520and%2520Jianye%2520Wang%2520and%2520Wentao%2520Cheng%2520and%2520Yaxing%2520Wang%2520and%2520Yu%2520Zhou%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520In%2520real-world%2520images%252C%2520slanted%2520or%2520curved%2520texts%252C%2520especially%2520those%2520on%2520cans%252C%250Abanners%252C%2520or%2520badges%252C%2520appear%2520as%2520frequently%252C%2520if%2520not%2520more%2520so%252C%2520than%2520flat%2520texts%2520due%250Ato%2520artistic%2520design%2520or%2520layout%2520constraints.%2520While%2520high-quality%2520visual%2520text%250Ageneration%2520has%2520become%2520available%2520with%2520the%2520advanced%2520generative%2520capabilities%2520of%250Adiffusion%2520models%252C%2520these%2520models%2520often%2520produce%2520distorted%2520text%2520and%2520inharmonious%250Atext%2520background%2520when%2520given%2520slanted%2520or%2520curved%2520text%2520layouts%2520due%2520to%2520training%2520data%250Alimitation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520training-free%2520framework%252C%2520STGen%252C%250Awhich%2520accurately%2520generates%2520visual%2520texts%2520in%2520challenging%2520scenarios%2520%2528%255Ceg%252C%2520slanted%250Aor%2520curved%2520text%2520layouts%2529%2520while%2520harmonizing%2520them%2520with%2520the%2520text%2520background.%2520Our%250Aframework%2520decomposes%2520the%2520visual%2520text%2520generation%2520process%2520into%2520two%2520branches%253A%2520%2528i%2529%250A%255Ctextbf%257BSemantic%2520Rectification%2520Branch%257D%252C%2520which%2520leverages%2520the%2520ability%2520in%250Agenerating%2520flat%2520but%2520accurate%2520visual%2520texts%2520of%2520the%2520model%2520to%2520guide%2520the%2520generation%250Aof%2520challenging%2520scenarios.%2520The%2520generated%2520latent%2520of%2520flat%2520text%2520is%2520abundant%2520in%250Aaccurate%2520semantic%2520information%2520related%2520both%2520to%2520the%2520text%2520itself%2520and%2520its%250Abackground.%2520By%2520incorporating%2520this%252C%2520we%2520rectify%2520the%2520semantic%2520information%2520of%2520the%250Atexts%2520and%2520harmonize%2520the%2520integration%2520of%2520the%2520text%2520with%2520its%2520background%2520in%2520complex%250Alayouts.%2520%2528ii%2529%2520%255Ctextbf%257BStructure%2520Injection%2520Branch%257D%252C%2520which%2520reinforces%2520the%2520visual%250Atext%2520structure%2520during%2520inference.%2520We%2520incorporate%2520the%2520latent%2520information%2520of%2520the%250Aglyph%2520image%252C%2520rich%2520in%2520glyph%2520structure%252C%2520as%2520a%2520new%2520condition%2520to%2520further%2520strengthen%250Athe%2520text%2520structure.%2520To%2520enhance%2520image%2520harmony%252C%2520we%2520also%2520apply%2520an%2520effective%250Acombination%2520method%2520to%2520merge%2520the%2520priors%252C%2520providing%2520a%2520solid%2520foundation%2520for%250Ageneration.%2520Extensive%2520experiments%2520across%2520a%2520variety%2520of%2520visual%2520text%2520layouts%250Ademonstrate%2520that%2520our%2520framework%2520achieves%2520superior%2520accuracy%2520and%2520outstanding%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Flat%20Text%3A%20Dual%20Self-inherited%20Guidance%20for%20Visual%20Text%0A%20%20Generation&entry.906535625=Minxing%20Luo%20and%20Zixun%20Xia%20and%20Liaojun%20Chen%20and%20Zhenhang%20Li%20and%20Weichao%20Zeng%20and%20Jianye%20Wang%20and%20Wentao%20Cheng%20and%20Yaxing%20Wang%20and%20Yu%20Zhou%20and%20Jian%20Yang&entry.1292438233=%20%20In%20real-world%20images%2C%20slanted%20or%20curved%20texts%2C%20especially%20those%20on%20cans%2C%0Abanners%2C%20or%20badges%2C%20appear%20as%20frequently%2C%20if%20not%20more%20so%2C%20than%20flat%20texts%20due%0Ato%20artistic%20design%20or%20layout%20constraints.%20While%20high-quality%20visual%20text%0Ageneration%20has%20become%20available%20with%20the%20advanced%20generative%20capabilities%20of%0Adiffusion%20models%2C%20these%20models%20often%20produce%20distorted%20text%20and%20inharmonious%0Atext%20background%20when%20given%20slanted%20or%20curved%20text%20layouts%20due%20to%20training%20data%0Alimitation.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20training-free%20framework%2C%20STGen%2C%0Awhich%20accurately%20generates%20visual%20texts%20in%20challenging%20scenarios%20%28%5Ceg%2C%20slanted%0Aor%20curved%20text%20layouts%29%20while%20harmonizing%20them%20with%20the%20text%20background.%20Our%0Aframework%20decomposes%20the%20visual%20text%20generation%20process%20into%20two%20branches%3A%20%28i%29%0A%5Ctextbf%7BSemantic%20Rectification%20Branch%7D%2C%20which%20leverages%20the%20ability%20in%0Agenerating%20flat%20but%20accurate%20visual%20texts%20of%20the%20model%20to%20guide%20the%20generation%0Aof%20challenging%20scenarios.%20The%20generated%20latent%20of%20flat%20text%20is%20abundant%20in%0Aaccurate%20semantic%20information%20related%20both%20to%20the%20text%20itself%20and%20its%0Abackground.%20By%20incorporating%20this%2C%20we%20rectify%20the%20semantic%20information%20of%20the%0Atexts%20and%20harmonize%20the%20integration%20of%20the%20text%20with%20its%20background%20in%20complex%0Alayouts.%20%28ii%29%20%5Ctextbf%7BStructure%20Injection%20Branch%7D%2C%20which%20reinforces%20the%20visual%0Atext%20structure%20during%20inference.%20We%20incorporate%20the%20latent%20information%20of%20the%0Aglyph%20image%2C%20rich%20in%20glyph%20structure%2C%20as%20a%20new%20condition%20to%20further%20strengthen%0Athe%20text%20structure.%20To%20enhance%20image%20harmony%2C%20we%20also%20apply%20an%20effective%0Acombination%20method%20to%20merge%20the%20priors%2C%20providing%20a%20solid%20foundation%20for%0Ageneration.%20Extensive%20experiments%20across%20a%20variety%20of%20visual%20text%20layouts%0Ademonstrate%20that%20our%20framework%20achieves%20superior%20accuracy%20and%20outstanding%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05892v1&entry.124074799=Read"},
{"title": "Benchmarking Rotary Position Embeddings for Automatic Speech Recognition", "author": "Shucong Zhang and Titouan Parcollet and Rogier van Dalen and Sourav Bhattacharya", "abstract": "  Rotary Position Embedding (RoPE) encodes relative and absolute positional\ninformation in Transformer-based models through rotation matrices applied to\ninput vectors within sequences. While RoPE has demonstrated superior\nperformance compared to other positional embedding technologies in natural\nlanguage processing tasks, its effectiveness in speech processing applications\nremains understudied. In this work, we conduct a comprehensive evaluation of\nRoPE across diverse automatic speech recognition (ASR) tasks. Our experimental\nresults demonstrate that for ASR tasks, RoPE consistently achieves lower error\nrates compared to the currently widely used relative positional embedding. To\nfacilitate further research, we release the implementation and all experimental\nrecipes through the SpeechBrain toolkit.\n", "link": "http://arxiv.org/abs/2501.06051v1", "date": "2025-01-10", "relevancy": 1.8729, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Rotary%20Position%20Embeddings%20for%20Automatic%20Speech%20Recognition&body=Title%3A%20Benchmarking%20Rotary%20Position%20Embeddings%20for%20Automatic%20Speech%20Recognition%0AAuthor%3A%20Shucong%20Zhang%20and%20Titouan%20Parcollet%20and%20Rogier%20van%20Dalen%20and%20Sourav%20Bhattacharya%0AAbstract%3A%20%20%20Rotary%20Position%20Embedding%20%28RoPE%29%20encodes%20relative%20and%20absolute%20positional%0Ainformation%20in%20Transformer-based%20models%20through%20rotation%20matrices%20applied%20to%0Ainput%20vectors%20within%20sequences.%20While%20RoPE%20has%20demonstrated%20superior%0Aperformance%20compared%20to%20other%20positional%20embedding%20technologies%20in%20natural%0Alanguage%20processing%20tasks%2C%20its%20effectiveness%20in%20speech%20processing%20applications%0Aremains%20understudied.%20In%20this%20work%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%0ARoPE%20across%20diverse%20automatic%20speech%20recognition%20%28ASR%29%20tasks.%20Our%20experimental%0Aresults%20demonstrate%20that%20for%20ASR%20tasks%2C%20RoPE%20consistently%20achieves%20lower%20error%0Arates%20compared%20to%20the%20currently%20widely%20used%20relative%20positional%20embedding.%20To%0Afacilitate%20further%20research%2C%20we%20release%20the%20implementation%20and%20all%20experimental%0Arecipes%20through%20the%20SpeechBrain%20toolkit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Rotary%2520Position%2520Embeddings%2520for%2520Automatic%2520Speech%2520Recognition%26entry.906535625%3DShucong%2520Zhang%2520and%2520Titouan%2520Parcollet%2520and%2520Rogier%2520van%2520Dalen%2520and%2520Sourav%2520Bhattacharya%26entry.1292438233%3D%2520%2520Rotary%2520Position%2520Embedding%2520%2528RoPE%2529%2520encodes%2520relative%2520and%2520absolute%2520positional%250Ainformation%2520in%2520Transformer-based%2520models%2520through%2520rotation%2520matrices%2520applied%2520to%250Ainput%2520vectors%2520within%2520sequences.%2520While%2520RoPE%2520has%2520demonstrated%2520superior%250Aperformance%2520compared%2520to%2520other%2520positional%2520embedding%2520technologies%2520in%2520natural%250Alanguage%2520processing%2520tasks%252C%2520its%2520effectiveness%2520in%2520speech%2520processing%2520applications%250Aremains%2520understudied.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%250ARoPE%2520across%2520diverse%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520tasks.%2520Our%2520experimental%250Aresults%2520demonstrate%2520that%2520for%2520ASR%2520tasks%252C%2520RoPE%2520consistently%2520achieves%2520lower%2520error%250Arates%2520compared%2520to%2520the%2520currently%2520widely%2520used%2520relative%2520positional%2520embedding.%2520To%250Afacilitate%2520further%2520research%252C%2520we%2520release%2520the%2520implementation%2520and%2520all%2520experimental%250Arecipes%2520through%2520the%2520SpeechBrain%2520toolkit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Rotary%20Position%20Embeddings%20for%20Automatic%20Speech%20Recognition&entry.906535625=Shucong%20Zhang%20and%20Titouan%20Parcollet%20and%20Rogier%20van%20Dalen%20and%20Sourav%20Bhattacharya&entry.1292438233=%20%20Rotary%20Position%20Embedding%20%28RoPE%29%20encodes%20relative%20and%20absolute%20positional%0Ainformation%20in%20Transformer-based%20models%20through%20rotation%20matrices%20applied%20to%0Ainput%20vectors%20within%20sequences.%20While%20RoPE%20has%20demonstrated%20superior%0Aperformance%20compared%20to%20other%20positional%20embedding%20technologies%20in%20natural%0Alanguage%20processing%20tasks%2C%20its%20effectiveness%20in%20speech%20processing%20applications%0Aremains%20understudied.%20In%20this%20work%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%0ARoPE%20across%20diverse%20automatic%20speech%20recognition%20%28ASR%29%20tasks.%20Our%20experimental%0Aresults%20demonstrate%20that%20for%20ASR%20tasks%2C%20RoPE%20consistently%20achieves%20lower%20error%0Arates%20compared%20to%20the%20currently%20widely%20used%20relative%20positional%20embedding.%20To%0Afacilitate%20further%20research%2C%20we%20release%20the%20implementation%20and%20all%20experimental%0Arecipes%20through%20the%20SpeechBrain%20toolkit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06051v1&entry.124074799=Read"},
{"title": "xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement", "author": "Nikolai Lund K\u00fchne and Jan \u00d8stergaard and Jesper Jensen and Zheng-Hua Tan", "abstract": "  While attention-based architectures, such as Conformers, excel in speech\nenhancement, they face challenges such as scalability with respect to input\nsequence length. In contrast, the recently proposed Extended Long Short-Term\nMemory (xLSTM) architecture offers linear scalability. However, xLSTM-based\nmodels remain unexplored for speech enhancement. This paper introduces\nxLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. A\ncomparative analysis reveals that xLSTM-and notably, even LSTM-can match or\noutperform state-of-the-art Mamba- and Conformer-based systems across various\nmodel sizes in speech enhancement on the VoiceBank+Demand dataset. Through\nablation studies, we identify key architectural design choices such as\nexponential gating and bidirectionality contributing to its effectiveness. Our\nbest xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- and\nConformer-based systems on the Voicebank+DEMAND dataset.\n", "link": "http://arxiv.org/abs/2501.06146v1", "date": "2025-01-10", "relevancy": 1.8712, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4731}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xLSTM-SENet%3A%20xLSTM%20for%20Single-Channel%20Speech%20Enhancement&body=Title%3A%20xLSTM-SENet%3A%20xLSTM%20for%20Single-Channel%20Speech%20Enhancement%0AAuthor%3A%20Nikolai%20Lund%20K%C3%BChne%20and%20Jan%20%C3%98stergaard%20and%20Jesper%20Jensen%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20While%20attention-based%20architectures%2C%20such%20as%20Conformers%2C%20excel%20in%20speech%0Aenhancement%2C%20they%20face%20challenges%20such%20as%20scalability%20with%20respect%20to%20input%0Asequence%20length.%20In%20contrast%2C%20the%20recently%20proposed%20Extended%20Long%20Short-Term%0AMemory%20%28xLSTM%29%20architecture%20offers%20linear%20scalability.%20However%2C%20xLSTM-based%0Amodels%20remain%20unexplored%20for%20speech%20enhancement.%20This%20paper%20introduces%0AxLSTM-SENet%2C%20the%20first%20xLSTM-based%20single-channel%20speech%20enhancement%20system.%20A%0Acomparative%20analysis%20reveals%20that%20xLSTM-and%20notably%2C%20even%20LSTM-can%20match%20or%0Aoutperform%20state-of-the-art%20Mamba-%20and%20Conformer-based%20systems%20across%20various%0Amodel%20sizes%20in%20speech%20enhancement%20on%20the%20VoiceBank%2BDemand%20dataset.%20Through%0Aablation%20studies%2C%20we%20identify%20key%20architectural%20design%20choices%20such%20as%0Aexponential%20gating%20and%20bidirectionality%20contributing%20to%20its%20effectiveness.%20Our%0Abest%20xLSTM-based%20model%2C%20xLSTM-SENet2%2C%20outperforms%20state-of-the-art%20Mamba-%20and%0AConformer-based%20systems%20on%20the%20Voicebank%2BDEMAND%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxLSTM-SENet%253A%2520xLSTM%2520for%2520Single-Channel%2520Speech%2520Enhancement%26entry.906535625%3DNikolai%2520Lund%2520K%25C3%25BChne%2520and%2520Jan%2520%25C3%2598stergaard%2520and%2520Jesper%2520Jensen%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3D%2520%2520While%2520attention-based%2520architectures%252C%2520such%2520as%2520Conformers%252C%2520excel%2520in%2520speech%250Aenhancement%252C%2520they%2520face%2520challenges%2520such%2520as%2520scalability%2520with%2520respect%2520to%2520input%250Asequence%2520length.%2520In%2520contrast%252C%2520the%2520recently%2520proposed%2520Extended%2520Long%2520Short-Term%250AMemory%2520%2528xLSTM%2529%2520architecture%2520offers%2520linear%2520scalability.%2520However%252C%2520xLSTM-based%250Amodels%2520remain%2520unexplored%2520for%2520speech%2520enhancement.%2520This%2520paper%2520introduces%250AxLSTM-SENet%252C%2520the%2520first%2520xLSTM-based%2520single-channel%2520speech%2520enhancement%2520system.%2520A%250Acomparative%2520analysis%2520reveals%2520that%2520xLSTM-and%2520notably%252C%2520even%2520LSTM-can%2520match%2520or%250Aoutperform%2520state-of-the-art%2520Mamba-%2520and%2520Conformer-based%2520systems%2520across%2520various%250Amodel%2520sizes%2520in%2520speech%2520enhancement%2520on%2520the%2520VoiceBank%252BDemand%2520dataset.%2520Through%250Aablation%2520studies%252C%2520we%2520identify%2520key%2520architectural%2520design%2520choices%2520such%2520as%250Aexponential%2520gating%2520and%2520bidirectionality%2520contributing%2520to%2520its%2520effectiveness.%2520Our%250Abest%2520xLSTM-based%2520model%252C%2520xLSTM-SENet2%252C%2520outperforms%2520state-of-the-art%2520Mamba-%2520and%250AConformer-based%2520systems%2520on%2520the%2520Voicebank%252BDEMAND%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xLSTM-SENet%3A%20xLSTM%20for%20Single-Channel%20Speech%20Enhancement&entry.906535625=Nikolai%20Lund%20K%C3%BChne%20and%20Jan%20%C3%98stergaard%20and%20Jesper%20Jensen%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20While%20attention-based%20architectures%2C%20such%20as%20Conformers%2C%20excel%20in%20speech%0Aenhancement%2C%20they%20face%20challenges%20such%20as%20scalability%20with%20respect%20to%20input%0Asequence%20length.%20In%20contrast%2C%20the%20recently%20proposed%20Extended%20Long%20Short-Term%0AMemory%20%28xLSTM%29%20architecture%20offers%20linear%20scalability.%20However%2C%20xLSTM-based%0Amodels%20remain%20unexplored%20for%20speech%20enhancement.%20This%20paper%20introduces%0AxLSTM-SENet%2C%20the%20first%20xLSTM-based%20single-channel%20speech%20enhancement%20system.%20A%0Acomparative%20analysis%20reveals%20that%20xLSTM-and%20notably%2C%20even%20LSTM-can%20match%20or%0Aoutperform%20state-of-the-art%20Mamba-%20and%20Conformer-based%20systems%20across%20various%0Amodel%20sizes%20in%20speech%20enhancement%20on%20the%20VoiceBank%2BDemand%20dataset.%20Through%0Aablation%20studies%2C%20we%20identify%20key%20architectural%20design%20choices%20such%20as%0Aexponential%20gating%20and%20bidirectionality%20contributing%20to%20its%20effectiveness.%20Our%0Abest%20xLSTM-based%20model%2C%20xLSTM-SENet2%2C%20outperforms%20state-of-the-art%20Mamba-%20and%0AConformer-based%20systems%20on%20the%20Voicebank%2BDEMAND%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06146v1&entry.124074799=Read"},
{"title": "High-dimensional classification problems with Barron regular boundaries\n  under margin conditions", "author": "Jonathan Garc\u00eda and Philipp Petersen", "abstract": "  We prove that a classifier with a Barron-regular decision boundary can be\napproximated with a rate of high polynomial degree by ReLU neural networks with\nthree hidden layers when a margin condition is assumed. In particular, for\nstrong margin conditions, high-dimensional discontinuous classifiers can be\napproximated with a rate that is typically only achievable when approximating a\nlow-dimensional smooth function. We demonstrate how these expression rate\nbounds imply fast-rate learning bounds that are close to $n^{-1}$ where $n$ is\nthe number of samples. In addition, we carry out comprehensive numerical\nexperimentation on binary classification problems with various margins. We\nstudy three different dimensions, with the highest dimensional problem\ncorresponding to images from the MNIST data set.\n", "link": "http://arxiv.org/abs/2412.07312v2", "date": "2025-01-10", "relevancy": 1.8653, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4749}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4629}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-dimensional%20classification%20problems%20with%20Barron%20regular%20boundaries%0A%20%20under%20margin%20conditions&body=Title%3A%20High-dimensional%20classification%20problems%20with%20Barron%20regular%20boundaries%0A%20%20under%20margin%20conditions%0AAuthor%3A%20Jonathan%20Garc%C3%ADa%20and%20Philipp%20Petersen%0AAbstract%3A%20%20%20We%20prove%20that%20a%20classifier%20with%20a%20Barron-regular%20decision%20boundary%20can%20be%0Aapproximated%20with%20a%20rate%20of%20high%20polynomial%20degree%20by%20ReLU%20neural%20networks%20with%0Athree%20hidden%20layers%20when%20a%20margin%20condition%20is%20assumed.%20In%20particular%2C%20for%0Astrong%20margin%20conditions%2C%20high-dimensional%20discontinuous%20classifiers%20can%20be%0Aapproximated%20with%20a%20rate%20that%20is%20typically%20only%20achievable%20when%20approximating%20a%0Alow-dimensional%20smooth%20function.%20We%20demonstrate%20how%20these%20expression%20rate%0Abounds%20imply%20fast-rate%20learning%20bounds%20that%20are%20close%20to%20%24n%5E%7B-1%7D%24%20where%20%24n%24%20is%0Athe%20number%20of%20samples.%20In%20addition%2C%20we%20carry%20out%20comprehensive%20numerical%0Aexperimentation%20on%20binary%20classification%20problems%20with%20various%20margins.%20We%0Astudy%20three%20different%20dimensions%2C%20with%20the%20highest%20dimensional%20problem%0Acorresponding%20to%20images%20from%20the%20MNIST%20data%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-dimensional%2520classification%2520problems%2520with%2520Barron%2520regular%2520boundaries%250A%2520%2520under%2520margin%2520conditions%26entry.906535625%3DJonathan%2520Garc%25C3%25ADa%2520and%2520Philipp%2520Petersen%26entry.1292438233%3D%2520%2520We%2520prove%2520that%2520a%2520classifier%2520with%2520a%2520Barron-regular%2520decision%2520boundary%2520can%2520be%250Aapproximated%2520with%2520a%2520rate%2520of%2520high%2520polynomial%2520degree%2520by%2520ReLU%2520neural%2520networks%2520with%250Athree%2520hidden%2520layers%2520when%2520a%2520margin%2520condition%2520is%2520assumed.%2520In%2520particular%252C%2520for%250Astrong%2520margin%2520conditions%252C%2520high-dimensional%2520discontinuous%2520classifiers%2520can%2520be%250Aapproximated%2520with%2520a%2520rate%2520that%2520is%2520typically%2520only%2520achievable%2520when%2520approximating%2520a%250Alow-dimensional%2520smooth%2520function.%2520We%2520demonstrate%2520how%2520these%2520expression%2520rate%250Abounds%2520imply%2520fast-rate%2520learning%2520bounds%2520that%2520are%2520close%2520to%2520%2524n%255E%257B-1%257D%2524%2520where%2520%2524n%2524%2520is%250Athe%2520number%2520of%2520samples.%2520In%2520addition%252C%2520we%2520carry%2520out%2520comprehensive%2520numerical%250Aexperimentation%2520on%2520binary%2520classification%2520problems%2520with%2520various%2520margins.%2520We%250Astudy%2520three%2520different%2520dimensions%252C%2520with%2520the%2520highest%2520dimensional%2520problem%250Acorresponding%2520to%2520images%2520from%2520the%2520MNIST%2520data%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-dimensional%20classification%20problems%20with%20Barron%20regular%20boundaries%0A%20%20under%20margin%20conditions&entry.906535625=Jonathan%20Garc%C3%ADa%20and%20Philipp%20Petersen&entry.1292438233=%20%20We%20prove%20that%20a%20classifier%20with%20a%20Barron-regular%20decision%20boundary%20can%20be%0Aapproximated%20with%20a%20rate%20of%20high%20polynomial%20degree%20by%20ReLU%20neural%20networks%20with%0Athree%20hidden%20layers%20when%20a%20margin%20condition%20is%20assumed.%20In%20particular%2C%20for%0Astrong%20margin%20conditions%2C%20high-dimensional%20discontinuous%20classifiers%20can%20be%0Aapproximated%20with%20a%20rate%20that%20is%20typically%20only%20achievable%20when%20approximating%20a%0Alow-dimensional%20smooth%20function.%20We%20demonstrate%20how%20these%20expression%20rate%0Abounds%20imply%20fast-rate%20learning%20bounds%20that%20are%20close%20to%20%24n%5E%7B-1%7D%24%20where%20%24n%24%20is%0Athe%20number%20of%20samples.%20In%20addition%2C%20we%20carry%20out%20comprehensive%20numerical%0Aexperimentation%20on%20binary%20classification%20problems%20with%20various%20margins.%20We%0Astudy%20three%20different%20dimensions%2C%20with%20the%20highest%20dimensional%20problem%0Acorresponding%20to%20images%20from%20the%20MNIST%20data%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07312v2&entry.124074799=Read"},
{"title": "A unified cross-attention model for predicting antigen binding\n  specificity to both HLA and TCR molecules", "author": "Chenpeng Yu and Xing Fang and Hui Liu", "abstract": "  The immune checkpoint inhibitors have demonstrated promising clinical\nefficacy across various tumor types, yet the percentage of patients who benefit\nfrom them remains low. The bindings between tumor antigens and HLA-I/TCR\nmolecules determine the antigen presentation and T-cell activation, thereby\nplaying an important role in the immunotherapy response. In this paper, we\npropose UnifyImmun, a unified cross-attention transformer model designed to\nsimultaneously predict the bindings of peptides to both receptors, providing\nmore comprehensive evaluation of antigen immunogenicity. We devise a two-phase\nstrategy using virtual adversarial training that enables these two tasks to\nreinforce each other mutually, by compelling the encoders to extract more\nexpressive features. Our method demonstrates superior performance in predicting\nboth pHLA and pTCR binding on multiple independent and external test sets.\nNotably, on a large-scale COVID-19 pTCR binding test set without any seen\npeptide in training set, our method outperforms the current state-of-the-art\nmethods by more than 10\\%. The predicted binding scores significantly correlate\nwith the immunotherapy response and clinical outcomes on two clinical cohorts.\nFurthermore, the cross-attention scores and integrated gradients reveal the\namino-acid sites critical for peptide binding to receptors. In essence, our\napproach marks a significant step toward comprehensive evaluation of antigen\nimmunogenicity.\n", "link": "http://arxiv.org/abs/2405.06653v2", "date": "2025-01-10", "relevancy": 1.8487, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4759}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4656}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20unified%20cross-attention%20model%20for%20predicting%20antigen%20binding%0A%20%20specificity%20to%20both%20HLA%20and%20TCR%20molecules&body=Title%3A%20A%20unified%20cross-attention%20model%20for%20predicting%20antigen%20binding%0A%20%20specificity%20to%20both%20HLA%20and%20TCR%20molecules%0AAuthor%3A%20Chenpeng%20Yu%20and%20Xing%20Fang%20and%20Hui%20Liu%0AAbstract%3A%20%20%20The%20immune%20checkpoint%20inhibitors%20have%20demonstrated%20promising%20clinical%0Aefficacy%20across%20various%20tumor%20types%2C%20yet%20the%20percentage%20of%20patients%20who%20benefit%0Afrom%20them%20remains%20low.%20The%20bindings%20between%20tumor%20antigens%20and%20HLA-I/TCR%0Amolecules%20determine%20the%20antigen%20presentation%20and%20T-cell%20activation%2C%20thereby%0Aplaying%20an%20important%20role%20in%20the%20immunotherapy%20response.%20In%20this%20paper%2C%20we%0Apropose%20UnifyImmun%2C%20a%20unified%20cross-attention%20transformer%20model%20designed%20to%0Asimultaneously%20predict%20the%20bindings%20of%20peptides%20to%20both%20receptors%2C%20providing%0Amore%20comprehensive%20evaluation%20of%20antigen%20immunogenicity.%20We%20devise%20a%20two-phase%0Astrategy%20using%20virtual%20adversarial%20training%20that%20enables%20these%20two%20tasks%20to%0Areinforce%20each%20other%20mutually%2C%20by%20compelling%20the%20encoders%20to%20extract%20more%0Aexpressive%20features.%20Our%20method%20demonstrates%20superior%20performance%20in%20predicting%0Aboth%20pHLA%20and%20pTCR%20binding%20on%20multiple%20independent%20and%20external%20test%20sets.%0ANotably%2C%20on%20a%20large-scale%20COVID-19%20pTCR%20binding%20test%20set%20without%20any%20seen%0Apeptide%20in%20training%20set%2C%20our%20method%20outperforms%20the%20current%20state-of-the-art%0Amethods%20by%20more%20than%2010%5C%25.%20The%20predicted%20binding%20scores%20significantly%20correlate%0Awith%20the%20immunotherapy%20response%20and%20clinical%20outcomes%20on%20two%20clinical%20cohorts.%0AFurthermore%2C%20the%20cross-attention%20scores%20and%20integrated%20gradients%20reveal%20the%0Aamino-acid%20sites%20critical%20for%20peptide%20binding%20to%20receptors.%20In%20essence%2C%20our%0Aapproach%20marks%20a%20significant%20step%20toward%20comprehensive%20evaluation%20of%20antigen%0Aimmunogenicity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520unified%2520cross-attention%2520model%2520for%2520predicting%2520antigen%2520binding%250A%2520%2520specificity%2520to%2520both%2520HLA%2520and%2520TCR%2520molecules%26entry.906535625%3DChenpeng%2520Yu%2520and%2520Xing%2520Fang%2520and%2520Hui%2520Liu%26entry.1292438233%3D%2520%2520The%2520immune%2520checkpoint%2520inhibitors%2520have%2520demonstrated%2520promising%2520clinical%250Aefficacy%2520across%2520various%2520tumor%2520types%252C%2520yet%2520the%2520percentage%2520of%2520patients%2520who%2520benefit%250Afrom%2520them%2520remains%2520low.%2520The%2520bindings%2520between%2520tumor%2520antigens%2520and%2520HLA-I/TCR%250Amolecules%2520determine%2520the%2520antigen%2520presentation%2520and%2520T-cell%2520activation%252C%2520thereby%250Aplaying%2520an%2520important%2520role%2520in%2520the%2520immunotherapy%2520response.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520UnifyImmun%252C%2520a%2520unified%2520cross-attention%2520transformer%2520model%2520designed%2520to%250Asimultaneously%2520predict%2520the%2520bindings%2520of%2520peptides%2520to%2520both%2520receptors%252C%2520providing%250Amore%2520comprehensive%2520evaluation%2520of%2520antigen%2520immunogenicity.%2520We%2520devise%2520a%2520two-phase%250Astrategy%2520using%2520virtual%2520adversarial%2520training%2520that%2520enables%2520these%2520two%2520tasks%2520to%250Areinforce%2520each%2520other%2520mutually%252C%2520by%2520compelling%2520the%2520encoders%2520to%2520extract%2520more%250Aexpressive%2520features.%2520Our%2520method%2520demonstrates%2520superior%2520performance%2520in%2520predicting%250Aboth%2520pHLA%2520and%2520pTCR%2520binding%2520on%2520multiple%2520independent%2520and%2520external%2520test%2520sets.%250ANotably%252C%2520on%2520a%2520large-scale%2520COVID-19%2520pTCR%2520binding%2520test%2520set%2520without%2520any%2520seen%250Apeptide%2520in%2520training%2520set%252C%2520our%2520method%2520outperforms%2520the%2520current%2520state-of-the-art%250Amethods%2520by%2520more%2520than%252010%255C%2525.%2520The%2520predicted%2520binding%2520scores%2520significantly%2520correlate%250Awith%2520the%2520immunotherapy%2520response%2520and%2520clinical%2520outcomes%2520on%2520two%2520clinical%2520cohorts.%250AFurthermore%252C%2520the%2520cross-attention%2520scores%2520and%2520integrated%2520gradients%2520reveal%2520the%250Aamino-acid%2520sites%2520critical%2520for%2520peptide%2520binding%2520to%2520receptors.%2520In%2520essence%252C%2520our%250Aapproach%2520marks%2520a%2520significant%2520step%2520toward%2520comprehensive%2520evaluation%2520of%2520antigen%250Aimmunogenicity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20unified%20cross-attention%20model%20for%20predicting%20antigen%20binding%0A%20%20specificity%20to%20both%20HLA%20and%20TCR%20molecules&entry.906535625=Chenpeng%20Yu%20and%20Xing%20Fang%20and%20Hui%20Liu&entry.1292438233=%20%20The%20immune%20checkpoint%20inhibitors%20have%20demonstrated%20promising%20clinical%0Aefficacy%20across%20various%20tumor%20types%2C%20yet%20the%20percentage%20of%20patients%20who%20benefit%0Afrom%20them%20remains%20low.%20The%20bindings%20between%20tumor%20antigens%20and%20HLA-I/TCR%0Amolecules%20determine%20the%20antigen%20presentation%20and%20T-cell%20activation%2C%20thereby%0Aplaying%20an%20important%20role%20in%20the%20immunotherapy%20response.%20In%20this%20paper%2C%20we%0Apropose%20UnifyImmun%2C%20a%20unified%20cross-attention%20transformer%20model%20designed%20to%0Asimultaneously%20predict%20the%20bindings%20of%20peptides%20to%20both%20receptors%2C%20providing%0Amore%20comprehensive%20evaluation%20of%20antigen%20immunogenicity.%20We%20devise%20a%20two-phase%0Astrategy%20using%20virtual%20adversarial%20training%20that%20enables%20these%20two%20tasks%20to%0Areinforce%20each%20other%20mutually%2C%20by%20compelling%20the%20encoders%20to%20extract%20more%0Aexpressive%20features.%20Our%20method%20demonstrates%20superior%20performance%20in%20predicting%0Aboth%20pHLA%20and%20pTCR%20binding%20on%20multiple%20independent%20and%20external%20test%20sets.%0ANotably%2C%20on%20a%20large-scale%20COVID-19%20pTCR%20binding%20test%20set%20without%20any%20seen%0Apeptide%20in%20training%20set%2C%20our%20method%20outperforms%20the%20current%20state-of-the-art%0Amethods%20by%20more%20than%2010%5C%25.%20The%20predicted%20binding%20scores%20significantly%20correlate%0Awith%20the%20immunotherapy%20response%20and%20clinical%20outcomes%20on%20two%20clinical%20cohorts.%0AFurthermore%2C%20the%20cross-attention%20scores%20and%20integrated%20gradients%20reveal%20the%0Aamino-acid%20sites%20critical%20for%20peptide%20binding%20to%20receptors.%20In%20essence%2C%20our%0Aapproach%20marks%20a%20significant%20step%20toward%20comprehensive%20evaluation%20of%20antigen%0Aimmunogenicity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06653v2&entry.124074799=Read"},
{"title": "Inferring High-Order Couplings with Neural Networks", "author": "Aur\u00e9lien Decelle and Alfonso de Jes\u00fas Navas G\u00f3mez and Beatriz Seoane", "abstract": "  Maximum-entropy methods, rooted in the inverse Ising/Potts problem from\nstatistical mechanics, have become indispensable tools for modeling pairwise\ninteractions in disciplines such as bioinformatics, ecology, and neuroscience.\nDespite their remarkable success, these methods often overlook high-order\ninteractions that may be crucial in complex systems. Conversely, while modern\nmachine learning approaches can capture such interactions, existing\ninterpretable frameworks are computationally expensive, making it impractical\nto assess the relevance of high-order interactions in real-world scenarios.\nRestricted Boltzmann Machines (RBMs) offer a computationally efficient\nalternative by encoding statistical correlations via hidden nodes in a\nbipartite neural network. Here, we present a method that maps RBMs exactly onto\ngeneralized Potts models with interactions of arbitrary high order. This\napproach leverages large-$N$ approximations, facilitated by the simple\narchitecture of the RBM, to enable the efficient extraction of effective\nmany-body couplings with minimal computational cost. This mapping also enables\nthe development of a general formal framework for the extraction of effective\nhigher-order interactions in arbitrarily complex probabilistic models.\nAdditionally, we introduce a robust formalism for gauge fixing within the\ngeneralized Potts model. We validate our method by accurately recovering two-\nand three-body interactions from synthetic datasets. Additionally, applying our\nframework to protein sequence data demonstrates its effectiveness in\nreconstructing protein contact maps, achieving performance comparable to\nstate-of-the-art inverse Potts models. These results position RBMs as a\npowerful and efficient tool for investigating high-order interactions in\ncomplex systems.\n", "link": "http://arxiv.org/abs/2501.06108v1", "date": "2025-01-10", "relevancy": 1.8465, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5234}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4546}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inferring%20High-Order%20Couplings%20with%20Neural%20Networks&body=Title%3A%20Inferring%20High-Order%20Couplings%20with%20Neural%20Networks%0AAuthor%3A%20Aur%C3%A9lien%20Decelle%20and%20Alfonso%20de%20Jes%C3%BAs%20Navas%20G%C3%B3mez%20and%20Beatriz%20Seoane%0AAbstract%3A%20%20%20Maximum-entropy%20methods%2C%20rooted%20in%20the%20inverse%20Ising/Potts%20problem%20from%0Astatistical%20mechanics%2C%20have%20become%20indispensable%20tools%20for%20modeling%20pairwise%0Ainteractions%20in%20disciplines%20such%20as%20bioinformatics%2C%20ecology%2C%20and%20neuroscience.%0ADespite%20their%20remarkable%20success%2C%20these%20methods%20often%20overlook%20high-order%0Ainteractions%20that%20may%20be%20crucial%20in%20complex%20systems.%20Conversely%2C%20while%20modern%0Amachine%20learning%20approaches%20can%20capture%20such%20interactions%2C%20existing%0Ainterpretable%20frameworks%20are%20computationally%20expensive%2C%20making%20it%20impractical%0Ato%20assess%20the%20relevance%20of%20high-order%20interactions%20in%20real-world%20scenarios.%0ARestricted%20Boltzmann%20Machines%20%28RBMs%29%20offer%20a%20computationally%20efficient%0Aalternative%20by%20encoding%20statistical%20correlations%20via%20hidden%20nodes%20in%20a%0Abipartite%20neural%20network.%20Here%2C%20we%20present%20a%20method%20that%20maps%20RBMs%20exactly%20onto%0Ageneralized%20Potts%20models%20with%20interactions%20of%20arbitrary%20high%20order.%20This%0Aapproach%20leverages%20large-%24N%24%20approximations%2C%20facilitated%20by%20the%20simple%0Aarchitecture%20of%20the%20RBM%2C%20to%20enable%20the%20efficient%20extraction%20of%20effective%0Amany-body%20couplings%20with%20minimal%20computational%20cost.%20This%20mapping%20also%20enables%0Athe%20development%20of%20a%20general%20formal%20framework%20for%20the%20extraction%20of%20effective%0Ahigher-order%20interactions%20in%20arbitrarily%20complex%20probabilistic%20models.%0AAdditionally%2C%20we%20introduce%20a%20robust%20formalism%20for%20gauge%20fixing%20within%20the%0Ageneralized%20Potts%20model.%20We%20validate%20our%20method%20by%20accurately%20recovering%20two-%0Aand%20three-body%20interactions%20from%20synthetic%20datasets.%20Additionally%2C%20applying%20our%0Aframework%20to%20protein%20sequence%20data%20demonstrates%20its%20effectiveness%20in%0Areconstructing%20protein%20contact%20maps%2C%20achieving%20performance%20comparable%20to%0Astate-of-the-art%20inverse%20Potts%20models.%20These%20results%20position%20RBMs%20as%20a%0Apowerful%20and%20efficient%20tool%20for%20investigating%20high-order%20interactions%20in%0Acomplex%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInferring%2520High-Order%2520Couplings%2520with%2520Neural%2520Networks%26entry.906535625%3DAur%25C3%25A9lien%2520Decelle%2520and%2520Alfonso%2520de%2520Jes%25C3%25BAs%2520Navas%2520G%25C3%25B3mez%2520and%2520Beatriz%2520Seoane%26entry.1292438233%3D%2520%2520Maximum-entropy%2520methods%252C%2520rooted%2520in%2520the%2520inverse%2520Ising/Potts%2520problem%2520from%250Astatistical%2520mechanics%252C%2520have%2520become%2520indispensable%2520tools%2520for%2520modeling%2520pairwise%250Ainteractions%2520in%2520disciplines%2520such%2520as%2520bioinformatics%252C%2520ecology%252C%2520and%2520neuroscience.%250ADespite%2520their%2520remarkable%2520success%252C%2520these%2520methods%2520often%2520overlook%2520high-order%250Ainteractions%2520that%2520may%2520be%2520crucial%2520in%2520complex%2520systems.%2520Conversely%252C%2520while%2520modern%250Amachine%2520learning%2520approaches%2520can%2520capture%2520such%2520interactions%252C%2520existing%250Ainterpretable%2520frameworks%2520are%2520computationally%2520expensive%252C%2520making%2520it%2520impractical%250Ato%2520assess%2520the%2520relevance%2520of%2520high-order%2520interactions%2520in%2520real-world%2520scenarios.%250ARestricted%2520Boltzmann%2520Machines%2520%2528RBMs%2529%2520offer%2520a%2520computationally%2520efficient%250Aalternative%2520by%2520encoding%2520statistical%2520correlations%2520via%2520hidden%2520nodes%2520in%2520a%250Abipartite%2520neural%2520network.%2520Here%252C%2520we%2520present%2520a%2520method%2520that%2520maps%2520RBMs%2520exactly%2520onto%250Ageneralized%2520Potts%2520models%2520with%2520interactions%2520of%2520arbitrary%2520high%2520order.%2520This%250Aapproach%2520leverages%2520large-%2524N%2524%2520approximations%252C%2520facilitated%2520by%2520the%2520simple%250Aarchitecture%2520of%2520the%2520RBM%252C%2520to%2520enable%2520the%2520efficient%2520extraction%2520of%2520effective%250Amany-body%2520couplings%2520with%2520minimal%2520computational%2520cost.%2520This%2520mapping%2520also%2520enables%250Athe%2520development%2520of%2520a%2520general%2520formal%2520framework%2520for%2520the%2520extraction%2520of%2520effective%250Ahigher-order%2520interactions%2520in%2520arbitrarily%2520complex%2520probabilistic%2520models.%250AAdditionally%252C%2520we%2520introduce%2520a%2520robust%2520formalism%2520for%2520gauge%2520fixing%2520within%2520the%250Ageneralized%2520Potts%2520model.%2520We%2520validate%2520our%2520method%2520by%2520accurately%2520recovering%2520two-%250Aand%2520three-body%2520interactions%2520from%2520synthetic%2520datasets.%2520Additionally%252C%2520applying%2520our%250Aframework%2520to%2520protein%2520sequence%2520data%2520demonstrates%2520its%2520effectiveness%2520in%250Areconstructing%2520protein%2520contact%2520maps%252C%2520achieving%2520performance%2520comparable%2520to%250Astate-of-the-art%2520inverse%2520Potts%2520models.%2520These%2520results%2520position%2520RBMs%2520as%2520a%250Apowerful%2520and%2520efficient%2520tool%2520for%2520investigating%2520high-order%2520interactions%2520in%250Acomplex%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inferring%20High-Order%20Couplings%20with%20Neural%20Networks&entry.906535625=Aur%C3%A9lien%20Decelle%20and%20Alfonso%20de%20Jes%C3%BAs%20Navas%20G%C3%B3mez%20and%20Beatriz%20Seoane&entry.1292438233=%20%20Maximum-entropy%20methods%2C%20rooted%20in%20the%20inverse%20Ising/Potts%20problem%20from%0Astatistical%20mechanics%2C%20have%20become%20indispensable%20tools%20for%20modeling%20pairwise%0Ainteractions%20in%20disciplines%20such%20as%20bioinformatics%2C%20ecology%2C%20and%20neuroscience.%0ADespite%20their%20remarkable%20success%2C%20these%20methods%20often%20overlook%20high-order%0Ainteractions%20that%20may%20be%20crucial%20in%20complex%20systems.%20Conversely%2C%20while%20modern%0Amachine%20learning%20approaches%20can%20capture%20such%20interactions%2C%20existing%0Ainterpretable%20frameworks%20are%20computationally%20expensive%2C%20making%20it%20impractical%0Ato%20assess%20the%20relevance%20of%20high-order%20interactions%20in%20real-world%20scenarios.%0ARestricted%20Boltzmann%20Machines%20%28RBMs%29%20offer%20a%20computationally%20efficient%0Aalternative%20by%20encoding%20statistical%20correlations%20via%20hidden%20nodes%20in%20a%0Abipartite%20neural%20network.%20Here%2C%20we%20present%20a%20method%20that%20maps%20RBMs%20exactly%20onto%0Ageneralized%20Potts%20models%20with%20interactions%20of%20arbitrary%20high%20order.%20This%0Aapproach%20leverages%20large-%24N%24%20approximations%2C%20facilitated%20by%20the%20simple%0Aarchitecture%20of%20the%20RBM%2C%20to%20enable%20the%20efficient%20extraction%20of%20effective%0Amany-body%20couplings%20with%20minimal%20computational%20cost.%20This%20mapping%20also%20enables%0Athe%20development%20of%20a%20general%20formal%20framework%20for%20the%20extraction%20of%20effective%0Ahigher-order%20interactions%20in%20arbitrarily%20complex%20probabilistic%20models.%0AAdditionally%2C%20we%20introduce%20a%20robust%20formalism%20for%20gauge%20fixing%20within%20the%0Ageneralized%20Potts%20model.%20We%20validate%20our%20method%20by%20accurately%20recovering%20two-%0Aand%20three-body%20interactions%20from%20synthetic%20datasets.%20Additionally%2C%20applying%20our%0Aframework%20to%20protein%20sequence%20data%20demonstrates%20its%20effectiveness%20in%0Areconstructing%20protein%20contact%20maps%2C%20achieving%20performance%20comparable%20to%0Astate-of-the-art%20inverse%20Potts%20models.%20These%20results%20position%20RBMs%20as%20a%0Apowerful%20and%20efficient%20tool%20for%20investigating%20high-order%20interactions%20in%0Acomplex%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06108v1&entry.124074799=Read"},
{"title": "Learning Affordances from Interactive Exploration using an Object-level\n  Map", "author": "Paula Wulkop and Halil Umut \u00d6zdemir and Antonia H\u00fcfner and Jen Jen Chung and Roland Siegwart and Lionel Ott", "abstract": "  Many robotic tasks in real-world environments require physical interactions\nwith an object such as pick up or push. For successful interactions, the robot\nneeds to know the object's affordances, which are defined as the potential\nactions the robot can perform with the object. In order to learn a\nrobot-specific affordance predictor, we propose an interactive exploration\npipeline which allows the robot to collect interaction experiences while\nexploring an unknown environment. We integrate an object-level map in the\nexploration pipeline such that the robot can identify different object\ninstances and track objects across diverse viewpoints. This results in denser\nand more accurate affordance annotations compared to state-of-the-art methods,\nwhich do not incorporate a map. We show that our affordance exploration\napproach makes exploration more efficient and results in more accurate\naffordance prediction models compared to baseline methods.\n", "link": "http://arxiv.org/abs/2501.06047v1", "date": "2025-01-10", "relevancy": 1.8431, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6617}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6426}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Affordances%20from%20Interactive%20Exploration%20using%20an%20Object-level%0A%20%20Map&body=Title%3A%20Learning%20Affordances%20from%20Interactive%20Exploration%20using%20an%20Object-level%0A%20%20Map%0AAuthor%3A%20Paula%20Wulkop%20and%20Halil%20Umut%20%C3%96zdemir%20and%20Antonia%20H%C3%BCfner%20and%20Jen%20Jen%20Chung%20and%20Roland%20Siegwart%20and%20Lionel%20Ott%0AAbstract%3A%20%20%20Many%20robotic%20tasks%20in%20real-world%20environments%20require%20physical%20interactions%0Awith%20an%20object%20such%20as%20pick%20up%20or%20push.%20For%20successful%20interactions%2C%20the%20robot%0Aneeds%20to%20know%20the%20object%27s%20affordances%2C%20which%20are%20defined%20as%20the%20potential%0Aactions%20the%20robot%20can%20perform%20with%20the%20object.%20In%20order%20to%20learn%20a%0Arobot-specific%20affordance%20predictor%2C%20we%20propose%20an%20interactive%20exploration%0Apipeline%20which%20allows%20the%20robot%20to%20collect%20interaction%20experiences%20while%0Aexploring%20an%20unknown%20environment.%20We%20integrate%20an%20object-level%20map%20in%20the%0Aexploration%20pipeline%20such%20that%20the%20robot%20can%20identify%20different%20object%0Ainstances%20and%20track%20objects%20across%20diverse%20viewpoints.%20This%20results%20in%20denser%0Aand%20more%20accurate%20affordance%20annotations%20compared%20to%20state-of-the-art%20methods%2C%0Awhich%20do%20not%20incorporate%20a%20map.%20We%20show%20that%20our%20affordance%20exploration%0Aapproach%20makes%20exploration%20more%20efficient%20and%20results%20in%20more%20accurate%0Aaffordance%20prediction%20models%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Affordances%2520from%2520Interactive%2520Exploration%2520using%2520an%2520Object-level%250A%2520%2520Map%26entry.906535625%3DPaula%2520Wulkop%2520and%2520Halil%2520Umut%2520%25C3%2596zdemir%2520and%2520Antonia%2520H%25C3%25BCfner%2520and%2520Jen%2520Jen%2520Chung%2520and%2520Roland%2520Siegwart%2520and%2520Lionel%2520Ott%26entry.1292438233%3D%2520%2520Many%2520robotic%2520tasks%2520in%2520real-world%2520environments%2520require%2520physical%2520interactions%250Awith%2520an%2520object%2520such%2520as%2520pick%2520up%2520or%2520push.%2520For%2520successful%2520interactions%252C%2520the%2520robot%250Aneeds%2520to%2520know%2520the%2520object%2527s%2520affordances%252C%2520which%2520are%2520defined%2520as%2520the%2520potential%250Aactions%2520the%2520robot%2520can%2520perform%2520with%2520the%2520object.%2520In%2520order%2520to%2520learn%2520a%250Arobot-specific%2520affordance%2520predictor%252C%2520we%2520propose%2520an%2520interactive%2520exploration%250Apipeline%2520which%2520allows%2520the%2520robot%2520to%2520collect%2520interaction%2520experiences%2520while%250Aexploring%2520an%2520unknown%2520environment.%2520We%2520integrate%2520an%2520object-level%2520map%2520in%2520the%250Aexploration%2520pipeline%2520such%2520that%2520the%2520robot%2520can%2520identify%2520different%2520object%250Ainstances%2520and%2520track%2520objects%2520across%2520diverse%2520viewpoints.%2520This%2520results%2520in%2520denser%250Aand%2520more%2520accurate%2520affordance%2520annotations%2520compared%2520to%2520state-of-the-art%2520methods%252C%250Awhich%2520do%2520not%2520incorporate%2520a%2520map.%2520We%2520show%2520that%2520our%2520affordance%2520exploration%250Aapproach%2520makes%2520exploration%2520more%2520efficient%2520and%2520results%2520in%2520more%2520accurate%250Aaffordance%2520prediction%2520models%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Affordances%20from%20Interactive%20Exploration%20using%20an%20Object-level%0A%20%20Map&entry.906535625=Paula%20Wulkop%20and%20Halil%20Umut%20%C3%96zdemir%20and%20Antonia%20H%C3%BCfner%20and%20Jen%20Jen%20Chung%20and%20Roland%20Siegwart%20and%20Lionel%20Ott&entry.1292438233=%20%20Many%20robotic%20tasks%20in%20real-world%20environments%20require%20physical%20interactions%0Awith%20an%20object%20such%20as%20pick%20up%20or%20push.%20For%20successful%20interactions%2C%20the%20robot%0Aneeds%20to%20know%20the%20object%27s%20affordances%2C%20which%20are%20defined%20as%20the%20potential%0Aactions%20the%20robot%20can%20perform%20with%20the%20object.%20In%20order%20to%20learn%20a%0Arobot-specific%20affordance%20predictor%2C%20we%20propose%20an%20interactive%20exploration%0Apipeline%20which%20allows%20the%20robot%20to%20collect%20interaction%20experiences%20while%0Aexploring%20an%20unknown%20environment.%20We%20integrate%20an%20object-level%20map%20in%20the%0Aexploration%20pipeline%20such%20that%20the%20robot%20can%20identify%20different%20object%0Ainstances%20and%20track%20objects%20across%20diverse%20viewpoints.%20This%20results%20in%20denser%0Aand%20more%20accurate%20affordance%20annotations%20compared%20to%20state-of-the-art%20methods%2C%0Awhich%20do%20not%20incorporate%20a%20map.%20We%20show%20that%20our%20affordance%20exploration%0Aapproach%20makes%20exploration%20more%20efficient%20and%20results%20in%20more%20accurate%0Aaffordance%20prediction%20models%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06047v1&entry.124074799=Read"},
{"title": "Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal\n  LLMs", "author": "Dabing Cheng and Haosen Zhan and Xingchen Zhao and Guisheng Liu and Zemin Li and Jinghui Xie and Zhao Song and Weiguo Feng and Bingyue Peng", "abstract": "  The exponential growth of short-video content has ignited a surge in the\nnecessity for efficient, automated solutions to video editing, with challenges\narising from the need to understand videos and tailor the editing according to\nuser requirements. Addressing this need, we propose an innovative end-to-end\nfoundational framework, ultimately actualizing precise control over the final\nvideo content editing. Leveraging the flexibility and generalizability of\nMultimodal Large Language Models (MLLMs), we defined clear input-output\nmappings for efficient video creation. To bolster the model's capability in\nprocessing and comprehending video content, we introduce a strategic\ncombination of a denser frame rate and a slow-fast processing technique,\nsignificantly enhancing the extraction and understanding of both temporal and\nspatial video information. Furthermore, we introduce a text-to-edit mechanism\nthat allows users to achieve desired video outcomes through textual input,\nthereby enhancing the quality and controllability of the edited videos. Through\ncomprehensive experimentation, our method has not only showcased significant\neffectiveness within advertising datasets, but also yields universally\napplicable conclusions on public datasets.\n", "link": "http://arxiv.org/abs/2501.05884v1", "date": "2025-01-10", "relevancy": 1.8363, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6575}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6062}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-Edit%3A%20Controllable%20End-to-End%20Video%20Ad%20Creation%20via%20Multimodal%0A%20%20LLMs&body=Title%3A%20Text-to-Edit%3A%20Controllable%20End-to-End%20Video%20Ad%20Creation%20via%20Multimodal%0A%20%20LLMs%0AAuthor%3A%20Dabing%20Cheng%20and%20Haosen%20Zhan%20and%20Xingchen%20Zhao%20and%20Guisheng%20Liu%20and%20Zemin%20Li%20and%20Jinghui%20Xie%20and%20Zhao%20Song%20and%20Weiguo%20Feng%20and%20Bingyue%20Peng%0AAbstract%3A%20%20%20The%20exponential%20growth%20of%20short-video%20content%20has%20ignited%20a%20surge%20in%20the%0Anecessity%20for%20efficient%2C%20automated%20solutions%20to%20video%20editing%2C%20with%20challenges%0Aarising%20from%20the%20need%20to%20understand%20videos%20and%20tailor%20the%20editing%20according%20to%0Auser%20requirements.%20Addressing%20this%20need%2C%20we%20propose%20an%20innovative%20end-to-end%0Afoundational%20framework%2C%20ultimately%20actualizing%20precise%20control%20over%20the%20final%0Avideo%20content%20editing.%20Leveraging%20the%20flexibility%20and%20generalizability%20of%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20we%20defined%20clear%20input-output%0Amappings%20for%20efficient%20video%20creation.%20To%20bolster%20the%20model%27s%20capability%20in%0Aprocessing%20and%20comprehending%20video%20content%2C%20we%20introduce%20a%20strategic%0Acombination%20of%20a%20denser%20frame%20rate%20and%20a%20slow-fast%20processing%20technique%2C%0Asignificantly%20enhancing%20the%20extraction%20and%20understanding%20of%20both%20temporal%20and%0Aspatial%20video%20information.%20Furthermore%2C%20we%20introduce%20a%20text-to-edit%20mechanism%0Athat%20allows%20users%20to%20achieve%20desired%20video%20outcomes%20through%20textual%20input%2C%0Athereby%20enhancing%20the%20quality%20and%20controllability%20of%20the%20edited%20videos.%20Through%0Acomprehensive%20experimentation%2C%20our%20method%20has%20not%20only%20showcased%20significant%0Aeffectiveness%20within%20advertising%20datasets%2C%20but%20also%20yields%20universally%0Aapplicable%20conclusions%20on%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-Edit%253A%2520Controllable%2520End-to-End%2520Video%2520Ad%2520Creation%2520via%2520Multimodal%250A%2520%2520LLMs%26entry.906535625%3DDabing%2520Cheng%2520and%2520Haosen%2520Zhan%2520and%2520Xingchen%2520Zhao%2520and%2520Guisheng%2520Liu%2520and%2520Zemin%2520Li%2520and%2520Jinghui%2520Xie%2520and%2520Zhao%2520Song%2520and%2520Weiguo%2520Feng%2520and%2520Bingyue%2520Peng%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520of%2520short-video%2520content%2520has%2520ignited%2520a%2520surge%2520in%2520the%250Anecessity%2520for%2520efficient%252C%2520automated%2520solutions%2520to%2520video%2520editing%252C%2520with%2520challenges%250Aarising%2520from%2520the%2520need%2520to%2520understand%2520videos%2520and%2520tailor%2520the%2520editing%2520according%2520to%250Auser%2520requirements.%2520Addressing%2520this%2520need%252C%2520we%2520propose%2520an%2520innovative%2520end-to-end%250Afoundational%2520framework%252C%2520ultimately%2520actualizing%2520precise%2520control%2520over%2520the%2520final%250Avideo%2520content%2520editing.%2520Leveraging%2520the%2520flexibility%2520and%2520generalizability%2520of%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520we%2520defined%2520clear%2520input-output%250Amappings%2520for%2520efficient%2520video%2520creation.%2520To%2520bolster%2520the%2520model%2527s%2520capability%2520in%250Aprocessing%2520and%2520comprehending%2520video%2520content%252C%2520we%2520introduce%2520a%2520strategic%250Acombination%2520of%2520a%2520denser%2520frame%2520rate%2520and%2520a%2520slow-fast%2520processing%2520technique%252C%250Asignificantly%2520enhancing%2520the%2520extraction%2520and%2520understanding%2520of%2520both%2520temporal%2520and%250Aspatial%2520video%2520information.%2520Furthermore%252C%2520we%2520introduce%2520a%2520text-to-edit%2520mechanism%250Athat%2520allows%2520users%2520to%2520achieve%2520desired%2520video%2520outcomes%2520through%2520textual%2520input%252C%250Athereby%2520enhancing%2520the%2520quality%2520and%2520controllability%2520of%2520the%2520edited%2520videos.%2520Through%250Acomprehensive%2520experimentation%252C%2520our%2520method%2520has%2520not%2520only%2520showcased%2520significant%250Aeffectiveness%2520within%2520advertising%2520datasets%252C%2520but%2520also%2520yields%2520universally%250Aapplicable%2520conclusions%2520on%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-Edit%3A%20Controllable%20End-to-End%20Video%20Ad%20Creation%20via%20Multimodal%0A%20%20LLMs&entry.906535625=Dabing%20Cheng%20and%20Haosen%20Zhan%20and%20Xingchen%20Zhao%20and%20Guisheng%20Liu%20and%20Zemin%20Li%20and%20Jinghui%20Xie%20and%20Zhao%20Song%20and%20Weiguo%20Feng%20and%20Bingyue%20Peng&entry.1292438233=%20%20The%20exponential%20growth%20of%20short-video%20content%20has%20ignited%20a%20surge%20in%20the%0Anecessity%20for%20efficient%2C%20automated%20solutions%20to%20video%20editing%2C%20with%20challenges%0Aarising%20from%20the%20need%20to%20understand%20videos%20and%20tailor%20the%20editing%20according%20to%0Auser%20requirements.%20Addressing%20this%20need%2C%20we%20propose%20an%20innovative%20end-to-end%0Afoundational%20framework%2C%20ultimately%20actualizing%20precise%20control%20over%20the%20final%0Avideo%20content%20editing.%20Leveraging%20the%20flexibility%20and%20generalizability%20of%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20we%20defined%20clear%20input-output%0Amappings%20for%20efficient%20video%20creation.%20To%20bolster%20the%20model%27s%20capability%20in%0Aprocessing%20and%20comprehending%20video%20content%2C%20we%20introduce%20a%20strategic%0Acombination%20of%20a%20denser%20frame%20rate%20and%20a%20slow-fast%20processing%20technique%2C%0Asignificantly%20enhancing%20the%20extraction%20and%20understanding%20of%20both%20temporal%20and%0Aspatial%20video%20information.%20Furthermore%2C%20we%20introduce%20a%20text-to-edit%20mechanism%0Athat%20allows%20users%20to%20achieve%20desired%20video%20outcomes%20through%20textual%20input%2C%0Athereby%20enhancing%20the%20quality%20and%20controllability%20of%20the%20edited%20videos.%20Through%0Acomprehensive%20experimentation%2C%20our%20method%20has%20not%20only%20showcased%20significant%0Aeffectiveness%20within%20advertising%20datasets%2C%20but%20also%20yields%20universally%0Aapplicable%20conclusions%20on%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05884v1&entry.124074799=Read"},
{"title": "A Neighbor-based Approach to Pitch Ownership Models in Soccer", "author": "Tiago Mendes-Neves and Lu\u00eds Meireles and Jo\u00e3o Mendes-Moreira", "abstract": "  Pitch ownership models allow many types of analysis in soccer and provide\nvaluable assistance to tactical analysts in understanding the game's dynamics.\nThe novelty they provide over event-based analysis is that tracking data\nincorporates context that event-based data does not possess, like player\npositioning. This paper proposes a novel approach to building pitch ownership\nmodels in soccer games using the K-Nearest Neighbors (KNN) algorithm. Our\napproach provides a fast inference mechanism that can model different\napproaches to pitch control using the same algorithm. Despite its flexibility,\nit uses only three hyperparameters to tune the model, facilitating the tuning\nprocess for different player skill levels. The flexibility of the approach\nallows for the emulation of different methods available in the literature by\nadjusting a small number of parameters, including adjusting for different\nlevels of uncertainty. In summary, the proposed model provides a new and more\nflexible strategy for building pitch ownership models, extending beyond just\nreplicating existing algorithms, and can provide valuable insights for tactical\nanalysts and open up new avenues for future research. We thoroughly visualize\nseveral examples demonstrating the presented models' strengths and weaknesses.\nThe code is available at github.com/nvsclub/KNNPitchControl.\n", "link": "http://arxiv.org/abs/2501.05870v1", "date": "2025-01-10", "relevancy": 1.8356, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5283}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4466}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Neighbor-based%20Approach%20to%20Pitch%20Ownership%20Models%20in%20Soccer&body=Title%3A%20A%20Neighbor-based%20Approach%20to%20Pitch%20Ownership%20Models%20in%20Soccer%0AAuthor%3A%20Tiago%20Mendes-Neves%20and%20Lu%C3%ADs%20Meireles%20and%20Jo%C3%A3o%20Mendes-Moreira%0AAbstract%3A%20%20%20Pitch%20ownership%20models%20allow%20many%20types%20of%20analysis%20in%20soccer%20and%20provide%0Avaluable%20assistance%20to%20tactical%20analysts%20in%20understanding%20the%20game%27s%20dynamics.%0AThe%20novelty%20they%20provide%20over%20event-based%20analysis%20is%20that%20tracking%20data%0Aincorporates%20context%20that%20event-based%20data%20does%20not%20possess%2C%20like%20player%0Apositioning.%20This%20paper%20proposes%20a%20novel%20approach%20to%20building%20pitch%20ownership%0Amodels%20in%20soccer%20games%20using%20the%20K-Nearest%20Neighbors%20%28KNN%29%20algorithm.%20Our%0Aapproach%20provides%20a%20fast%20inference%20mechanism%20that%20can%20model%20different%0Aapproaches%20to%20pitch%20control%20using%20the%20same%20algorithm.%20Despite%20its%20flexibility%2C%0Ait%20uses%20only%20three%20hyperparameters%20to%20tune%20the%20model%2C%20facilitating%20the%20tuning%0Aprocess%20for%20different%20player%20skill%20levels.%20The%20flexibility%20of%20the%20approach%0Aallows%20for%20the%20emulation%20of%20different%20methods%20available%20in%20the%20literature%20by%0Aadjusting%20a%20small%20number%20of%20parameters%2C%20including%20adjusting%20for%20different%0Alevels%20of%20uncertainty.%20In%20summary%2C%20the%20proposed%20model%20provides%20a%20new%20and%20more%0Aflexible%20strategy%20for%20building%20pitch%20ownership%20models%2C%20extending%20beyond%20just%0Areplicating%20existing%20algorithms%2C%20and%20can%20provide%20valuable%20insights%20for%20tactical%0Aanalysts%20and%20open%20up%20new%20avenues%20for%20future%20research.%20We%20thoroughly%20visualize%0Aseveral%20examples%20demonstrating%20the%20presented%20models%27%20strengths%20and%20weaknesses.%0AThe%20code%20is%20available%20at%20github.com/nvsclub/KNNPitchControl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Neighbor-based%2520Approach%2520to%2520Pitch%2520Ownership%2520Models%2520in%2520Soccer%26entry.906535625%3DTiago%2520Mendes-Neves%2520and%2520Lu%25C3%25ADs%2520Meireles%2520and%2520Jo%25C3%25A3o%2520Mendes-Moreira%26entry.1292438233%3D%2520%2520Pitch%2520ownership%2520models%2520allow%2520many%2520types%2520of%2520analysis%2520in%2520soccer%2520and%2520provide%250Avaluable%2520assistance%2520to%2520tactical%2520analysts%2520in%2520understanding%2520the%2520game%2527s%2520dynamics.%250AThe%2520novelty%2520they%2520provide%2520over%2520event-based%2520analysis%2520is%2520that%2520tracking%2520data%250Aincorporates%2520context%2520that%2520event-based%2520data%2520does%2520not%2520possess%252C%2520like%2520player%250Apositioning.%2520This%2520paper%2520proposes%2520a%2520novel%2520approach%2520to%2520building%2520pitch%2520ownership%250Amodels%2520in%2520soccer%2520games%2520using%2520the%2520K-Nearest%2520Neighbors%2520%2528KNN%2529%2520algorithm.%2520Our%250Aapproach%2520provides%2520a%2520fast%2520inference%2520mechanism%2520that%2520can%2520model%2520different%250Aapproaches%2520to%2520pitch%2520control%2520using%2520the%2520same%2520algorithm.%2520Despite%2520its%2520flexibility%252C%250Ait%2520uses%2520only%2520three%2520hyperparameters%2520to%2520tune%2520the%2520model%252C%2520facilitating%2520the%2520tuning%250Aprocess%2520for%2520different%2520player%2520skill%2520levels.%2520The%2520flexibility%2520of%2520the%2520approach%250Aallows%2520for%2520the%2520emulation%2520of%2520different%2520methods%2520available%2520in%2520the%2520literature%2520by%250Aadjusting%2520a%2520small%2520number%2520of%2520parameters%252C%2520including%2520adjusting%2520for%2520different%250Alevels%2520of%2520uncertainty.%2520In%2520summary%252C%2520the%2520proposed%2520model%2520provides%2520a%2520new%2520and%2520more%250Aflexible%2520strategy%2520for%2520building%2520pitch%2520ownership%2520models%252C%2520extending%2520beyond%2520just%250Areplicating%2520existing%2520algorithms%252C%2520and%2520can%2520provide%2520valuable%2520insights%2520for%2520tactical%250Aanalysts%2520and%2520open%2520up%2520new%2520avenues%2520for%2520future%2520research.%2520We%2520thoroughly%2520visualize%250Aseveral%2520examples%2520demonstrating%2520the%2520presented%2520models%2527%2520strengths%2520and%2520weaknesses.%250AThe%2520code%2520is%2520available%2520at%2520github.com/nvsclub/KNNPitchControl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Neighbor-based%20Approach%20to%20Pitch%20Ownership%20Models%20in%20Soccer&entry.906535625=Tiago%20Mendes-Neves%20and%20Lu%C3%ADs%20Meireles%20and%20Jo%C3%A3o%20Mendes-Moreira&entry.1292438233=%20%20Pitch%20ownership%20models%20allow%20many%20types%20of%20analysis%20in%20soccer%20and%20provide%0Avaluable%20assistance%20to%20tactical%20analysts%20in%20understanding%20the%20game%27s%20dynamics.%0AThe%20novelty%20they%20provide%20over%20event-based%20analysis%20is%20that%20tracking%20data%0Aincorporates%20context%20that%20event-based%20data%20does%20not%20possess%2C%20like%20player%0Apositioning.%20This%20paper%20proposes%20a%20novel%20approach%20to%20building%20pitch%20ownership%0Amodels%20in%20soccer%20games%20using%20the%20K-Nearest%20Neighbors%20%28KNN%29%20algorithm.%20Our%0Aapproach%20provides%20a%20fast%20inference%20mechanism%20that%20can%20model%20different%0Aapproaches%20to%20pitch%20control%20using%20the%20same%20algorithm.%20Despite%20its%20flexibility%2C%0Ait%20uses%20only%20three%20hyperparameters%20to%20tune%20the%20model%2C%20facilitating%20the%20tuning%0Aprocess%20for%20different%20player%20skill%20levels.%20The%20flexibility%20of%20the%20approach%0Aallows%20for%20the%20emulation%20of%20different%20methods%20available%20in%20the%20literature%20by%0Aadjusting%20a%20small%20number%20of%20parameters%2C%20including%20adjusting%20for%20different%0Alevels%20of%20uncertainty.%20In%20summary%2C%20the%20proposed%20model%20provides%20a%20new%20and%20more%0Aflexible%20strategy%20for%20building%20pitch%20ownership%20models%2C%20extending%20beyond%20just%0Areplicating%20existing%20algorithms%2C%20and%20can%20provide%20valuable%20insights%20for%20tactical%0Aanalysts%20and%20open%20up%20new%20avenues%20for%20future%20research.%20We%20thoroughly%20visualize%0Aseveral%20examples%20demonstrating%20the%20presented%20models%27%20strengths%20and%20weaknesses.%0AThe%20code%20is%20available%20at%20github.com/nvsclub/KNNPitchControl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05870v1&entry.124074799=Read"},
{"title": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific\n  MCQs", "author": "Bianca Raimondi and Saverio Giallorenzo and Maurizio Gabbrielli", "abstract": "  In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable.\n", "link": "http://arxiv.org/abs/2501.05891v1", "date": "2025-01-10", "relevancy": 1.8333, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Affordably%20Fine-tuned%20LLMs%20Provide%20Better%20Answers%20to%20Course-specific%0A%20%20MCQs&body=Title%3A%20Affordably%20Fine-tuned%20LLMs%20Provide%20Better%20Answers%20to%20Course-specific%0A%20%20MCQs%0AAuthor%3A%20Bianca%20Raimondi%20and%20Saverio%20Giallorenzo%20and%20Maurizio%20Gabbrielli%0AAbstract%3A%20%20%20In%20education%2C%20the%20capability%20of%20generating%20human-like%20text%20of%20Large%20Language%0AModels%20%28LLMs%29%20inspired%20work%20on%20how%20they%20can%20increase%20the%20efficiency%20of%20learning%0Aand%20teaching.%20We%20study%20the%20affordability%20of%20these%20models%20for%20educators%20and%0Astudents%20by%20investigating%20how%20LLMs%20answer%20multiple-choice%20questions%20%28MCQs%29%20with%0Arespect%20to%20hardware%20constraints%20and%20refinement%20techniques.%20We%20explore%20this%0Aspace%20by%20using%20generic%20pre-trained%20LLMs%20%28the%207B%2C%2013B%2C%20and%2070B%20variants%20of%0ALLaMA-2%29%20to%20answer%20162%20undergraduate-level%20MCQs%20from%20a%20course%20on%20Programming%0ALanguages%20%28PL%29%20--%20the%20MCQ%20dataset%20is%20a%20contribution%20of%20this%20work%2C%20which%20we%20make%0Apublicly%20available.%20Specifically%2C%20we%20dissect%20how%20different%20factors%2C%20such%20as%0Ausing%20readily-available%20material%20--%20%28parts%20of%29%20the%20course%27s%20textbook%20--%20for%0Afine-tuning%20and%20quantisation%20%28to%20decrease%20resource%20usage%29%20can%20change%20the%0Aaccuracy%20of%20the%20responses.%20The%20main%20takeaway%20is%20that%20smaller%20textbook-based%0Afine-tuned%20models%20outperform%20generic%20larger%20ones%20%28whose%20pre-training%20requires%0Aconspicuous%20resources%29%2C%20making%20the%20usage%20of%20LLMs%20for%20answering%20MCQs%20resource-%0Aand%20material-wise%20affordable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAffordably%2520Fine-tuned%2520LLMs%2520Provide%2520Better%2520Answers%2520to%2520Course-specific%250A%2520%2520MCQs%26entry.906535625%3DBianca%2520Raimondi%2520and%2520Saverio%2520Giallorenzo%2520and%2520Maurizio%2520Gabbrielli%26entry.1292438233%3D%2520%2520In%2520education%252C%2520the%2520capability%2520of%2520generating%2520human-like%2520text%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520inspired%2520work%2520on%2520how%2520they%2520can%2520increase%2520the%2520efficiency%2520of%2520learning%250Aand%2520teaching.%2520We%2520study%2520the%2520affordability%2520of%2520these%2520models%2520for%2520educators%2520and%250Astudents%2520by%2520investigating%2520how%2520LLMs%2520answer%2520multiple-choice%2520questions%2520%2528MCQs%2529%2520with%250Arespect%2520to%2520hardware%2520constraints%2520and%2520refinement%2520techniques.%2520We%2520explore%2520this%250Aspace%2520by%2520using%2520generic%2520pre-trained%2520LLMs%2520%2528the%25207B%252C%252013B%252C%2520and%252070B%2520variants%2520of%250ALLaMA-2%2529%2520to%2520answer%2520162%2520undergraduate-level%2520MCQs%2520from%2520a%2520course%2520on%2520Programming%250ALanguages%2520%2528PL%2529%2520--%2520the%2520MCQ%2520dataset%2520is%2520a%2520contribution%2520of%2520this%2520work%252C%2520which%2520we%2520make%250Apublicly%2520available.%2520Specifically%252C%2520we%2520dissect%2520how%2520different%2520factors%252C%2520such%2520as%250Ausing%2520readily-available%2520material%2520--%2520%2528parts%2520of%2529%2520the%2520course%2527s%2520textbook%2520--%2520for%250Afine-tuning%2520and%2520quantisation%2520%2528to%2520decrease%2520resource%2520usage%2529%2520can%2520change%2520the%250Aaccuracy%2520of%2520the%2520responses.%2520The%2520main%2520takeaway%2520is%2520that%2520smaller%2520textbook-based%250Afine-tuned%2520models%2520outperform%2520generic%2520larger%2520ones%2520%2528whose%2520pre-training%2520requires%250Aconspicuous%2520resources%2529%252C%2520making%2520the%2520usage%2520of%2520LLMs%2520for%2520answering%2520MCQs%2520resource-%250Aand%2520material-wise%2520affordable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Affordably%20Fine-tuned%20LLMs%20Provide%20Better%20Answers%20to%20Course-specific%0A%20%20MCQs&entry.906535625=Bianca%20Raimondi%20and%20Saverio%20Giallorenzo%20and%20Maurizio%20Gabbrielli&entry.1292438233=%20%20In%20education%2C%20the%20capability%20of%20generating%20human-like%20text%20of%20Large%20Language%0AModels%20%28LLMs%29%20inspired%20work%20on%20how%20they%20can%20increase%20the%20efficiency%20of%20learning%0Aand%20teaching.%20We%20study%20the%20affordability%20of%20these%20models%20for%20educators%20and%0Astudents%20by%20investigating%20how%20LLMs%20answer%20multiple-choice%20questions%20%28MCQs%29%20with%0Arespect%20to%20hardware%20constraints%20and%20refinement%20techniques.%20We%20explore%20this%0Aspace%20by%20using%20generic%20pre-trained%20LLMs%20%28the%207B%2C%2013B%2C%20and%2070B%20variants%20of%0ALLaMA-2%29%20to%20answer%20162%20undergraduate-level%20MCQs%20from%20a%20course%20on%20Programming%0ALanguages%20%28PL%29%20--%20the%20MCQ%20dataset%20is%20a%20contribution%20of%20this%20work%2C%20which%20we%20make%0Apublicly%20available.%20Specifically%2C%20we%20dissect%20how%20different%20factors%2C%20such%20as%0Ausing%20readily-available%20material%20--%20%28parts%20of%29%20the%20course%27s%20textbook%20--%20for%0Afine-tuning%20and%20quantisation%20%28to%20decrease%20resource%20usage%29%20can%20change%20the%0Aaccuracy%20of%20the%20responses.%20The%20main%20takeaway%20is%20that%20smaller%20textbook-based%0Afine-tuned%20models%20outperform%20generic%20larger%20ones%20%28whose%20pre-training%20requires%0Aconspicuous%20resources%29%2C%20making%20the%20usage%20of%20LLMs%20for%20answering%20MCQs%20resource-%0Aand%20material-wise%20affordable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05891v1&entry.124074799=Read"},
{"title": "Q-MAML: Quantum Model-Agnostic Meta-Learning for Variational Quantum\n  Algorithms", "author": "Junyong Lee and JeiHee Cho and Shiho Kim", "abstract": "  In the Noisy Intermediate-Scale Quantum (NISQ) era, using variational quantum\nalgorithms (VQAs) to solve optimization problems has become a key application.\nHowever, these algorithms face significant challenges, such as choosing an\neffective initial set of parameters and the limited quantum processing time\nthat restricts the number of optimization iterations. In this study, we\nintroduce a new framework for optimizing parameterized quantum circuits (PQCs)\nthat employs a classical optimizer, inspired by Model-Agnostic Meta-Learning\n(MAML) technique. This approach aim to achieve better parameter initialization\nthat ensures fast convergence. Our framework features a classical neural\nnetwork, called Learner}, which interacts with a PQC using the output of\nLearner as an initial parameter. During the pre-training phase, Learner is\ntrained with a meta-objective based on the quantum circuit cost function. In\nthe adaptation phase, the framework requires only a few PQC updates to converge\nto a more accurate value, while the learner remains unchanged. This method is\nhighly adaptable and is effectively extended to various Hamiltonian\noptimization problems. We validate our approach through experiments, including\ndistribution function mapping and optimization of the Heisenberg XYZ\nHamiltonian. The result implies that the Learner successfully estimates initial\nparameters that generalize across the problem space, enabling fast adaptation.\n", "link": "http://arxiv.org/abs/2501.05906v1", "date": "2025-01-10", "relevancy": 1.8303, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4876}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4526}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-MAML%3A%20Quantum%20Model-Agnostic%20Meta-Learning%20for%20Variational%20Quantum%0A%20%20Algorithms&body=Title%3A%20Q-MAML%3A%20Quantum%20Model-Agnostic%20Meta-Learning%20for%20Variational%20Quantum%0A%20%20Algorithms%0AAuthor%3A%20Junyong%20Lee%20and%20JeiHee%20Cho%20and%20Shiho%20Kim%0AAbstract%3A%20%20%20In%20the%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%20era%2C%20using%20variational%20quantum%0Aalgorithms%20%28VQAs%29%20to%20solve%20optimization%20problems%20has%20become%20a%20key%20application.%0AHowever%2C%20these%20algorithms%20face%20significant%20challenges%2C%20such%20as%20choosing%20an%0Aeffective%20initial%20set%20of%20parameters%20and%20the%20limited%20quantum%20processing%20time%0Athat%20restricts%20the%20number%20of%20optimization%20iterations.%20In%20this%20study%2C%20we%0Aintroduce%20a%20new%20framework%20for%20optimizing%20parameterized%20quantum%20circuits%20%28PQCs%29%0Athat%20employs%20a%20classical%20optimizer%2C%20inspired%20by%20Model-Agnostic%20Meta-Learning%0A%28MAML%29%20technique.%20This%20approach%20aim%20to%20achieve%20better%20parameter%20initialization%0Athat%20ensures%20fast%20convergence.%20Our%20framework%20features%20a%20classical%20neural%0Anetwork%2C%20called%20Learner%7D%2C%20which%20interacts%20with%20a%20PQC%20using%20the%20output%20of%0ALearner%20as%20an%20initial%20parameter.%20During%20the%20pre-training%20phase%2C%20Learner%20is%0Atrained%20with%20a%20meta-objective%20based%20on%20the%20quantum%20circuit%20cost%20function.%20In%0Athe%20adaptation%20phase%2C%20the%20framework%20requires%20only%20a%20few%20PQC%20updates%20to%20converge%0Ato%20a%20more%20accurate%20value%2C%20while%20the%20learner%20remains%20unchanged.%20This%20method%20is%0Ahighly%20adaptable%20and%20is%20effectively%20extended%20to%20various%20Hamiltonian%0Aoptimization%20problems.%20We%20validate%20our%20approach%20through%20experiments%2C%20including%0Adistribution%20function%20mapping%20and%20optimization%20of%20the%20Heisenberg%20XYZ%0AHamiltonian.%20The%20result%20implies%20that%20the%20Learner%20successfully%20estimates%20initial%0Aparameters%20that%20generalize%20across%20the%20problem%20space%2C%20enabling%20fast%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-MAML%253A%2520Quantum%2520Model-Agnostic%2520Meta-Learning%2520for%2520Variational%2520Quantum%250A%2520%2520Algorithms%26entry.906535625%3DJunyong%2520Lee%2520and%2520JeiHee%2520Cho%2520and%2520Shiho%2520Kim%26entry.1292438233%3D%2520%2520In%2520the%2520Noisy%2520Intermediate-Scale%2520Quantum%2520%2528NISQ%2529%2520era%252C%2520using%2520variational%2520quantum%250Aalgorithms%2520%2528VQAs%2529%2520to%2520solve%2520optimization%2520problems%2520has%2520become%2520a%2520key%2520application.%250AHowever%252C%2520these%2520algorithms%2520face%2520significant%2520challenges%252C%2520such%2520as%2520choosing%2520an%250Aeffective%2520initial%2520set%2520of%2520parameters%2520and%2520the%2520limited%2520quantum%2520processing%2520time%250Athat%2520restricts%2520the%2520number%2520of%2520optimization%2520iterations.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520a%2520new%2520framework%2520for%2520optimizing%2520parameterized%2520quantum%2520circuits%2520%2528PQCs%2529%250Athat%2520employs%2520a%2520classical%2520optimizer%252C%2520inspired%2520by%2520Model-Agnostic%2520Meta-Learning%250A%2528MAML%2529%2520technique.%2520This%2520approach%2520aim%2520to%2520achieve%2520better%2520parameter%2520initialization%250Athat%2520ensures%2520fast%2520convergence.%2520Our%2520framework%2520features%2520a%2520classical%2520neural%250Anetwork%252C%2520called%2520Learner%257D%252C%2520which%2520interacts%2520with%2520a%2520PQC%2520using%2520the%2520output%2520of%250ALearner%2520as%2520an%2520initial%2520parameter.%2520During%2520the%2520pre-training%2520phase%252C%2520Learner%2520is%250Atrained%2520with%2520a%2520meta-objective%2520based%2520on%2520the%2520quantum%2520circuit%2520cost%2520function.%2520In%250Athe%2520adaptation%2520phase%252C%2520the%2520framework%2520requires%2520only%2520a%2520few%2520PQC%2520updates%2520to%2520converge%250Ato%2520a%2520more%2520accurate%2520value%252C%2520while%2520the%2520learner%2520remains%2520unchanged.%2520This%2520method%2520is%250Ahighly%2520adaptable%2520and%2520is%2520effectively%2520extended%2520to%2520various%2520Hamiltonian%250Aoptimization%2520problems.%2520We%2520validate%2520our%2520approach%2520through%2520experiments%252C%2520including%250Adistribution%2520function%2520mapping%2520and%2520optimization%2520of%2520the%2520Heisenberg%2520XYZ%250AHamiltonian.%2520The%2520result%2520implies%2520that%2520the%2520Learner%2520successfully%2520estimates%2520initial%250Aparameters%2520that%2520generalize%2520across%2520the%2520problem%2520space%252C%2520enabling%2520fast%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-MAML%3A%20Quantum%20Model-Agnostic%20Meta-Learning%20for%20Variational%20Quantum%0A%20%20Algorithms&entry.906535625=Junyong%20Lee%20and%20JeiHee%20Cho%20and%20Shiho%20Kim&entry.1292438233=%20%20In%20the%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%20era%2C%20using%20variational%20quantum%0Aalgorithms%20%28VQAs%29%20to%20solve%20optimization%20problems%20has%20become%20a%20key%20application.%0AHowever%2C%20these%20algorithms%20face%20significant%20challenges%2C%20such%20as%20choosing%20an%0Aeffective%20initial%20set%20of%20parameters%20and%20the%20limited%20quantum%20processing%20time%0Athat%20restricts%20the%20number%20of%20optimization%20iterations.%20In%20this%20study%2C%20we%0Aintroduce%20a%20new%20framework%20for%20optimizing%20parameterized%20quantum%20circuits%20%28PQCs%29%0Athat%20employs%20a%20classical%20optimizer%2C%20inspired%20by%20Model-Agnostic%20Meta-Learning%0A%28MAML%29%20technique.%20This%20approach%20aim%20to%20achieve%20better%20parameter%20initialization%0Athat%20ensures%20fast%20convergence.%20Our%20framework%20features%20a%20classical%20neural%0Anetwork%2C%20called%20Learner%7D%2C%20which%20interacts%20with%20a%20PQC%20using%20the%20output%20of%0ALearner%20as%20an%20initial%20parameter.%20During%20the%20pre-training%20phase%2C%20Learner%20is%0Atrained%20with%20a%20meta-objective%20based%20on%20the%20quantum%20circuit%20cost%20function.%20In%0Athe%20adaptation%20phase%2C%20the%20framework%20requires%20only%20a%20few%20PQC%20updates%20to%20converge%0Ato%20a%20more%20accurate%20value%2C%20while%20the%20learner%20remains%20unchanged.%20This%20method%20is%0Ahighly%20adaptable%20and%20is%20effectively%20extended%20to%20various%20Hamiltonian%0Aoptimization%20problems.%20We%20validate%20our%20approach%20through%20experiments%2C%20including%0Adistribution%20function%20mapping%20and%20optimization%20of%20the%20Heisenberg%20XYZ%0AHamiltonian.%20The%20result%20implies%20that%20the%20Learner%20successfully%20estimates%20initial%0Aparameters%20that%20generalize%20across%20the%20problem%20space%2C%20enabling%20fast%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05906v1&entry.124074799=Read"},
{"title": "Advances in Diffusion Models for Image Data Augmentation: A Review of\n  Methods, Models, Evaluation Metrics and Future Research Directions", "author": "Panagiotis Alimisis and Ioannis Mademlis and Panagiotis Radoglou-Grammatikis and Panagiotis Sarigiannidis and Georgios Th. Papadopoulos", "abstract": "  Image data augmentation constitutes a critical methodology in modern computer\nvision tasks, since it can facilitate towards enhancing the diversity and\nquality of training datasets; thereby, improving the performance and robustness\nof machine learning models in downstream tasks. In parallel, augmentation\napproaches can also be used for editing/modifying a given image in a context-\nand semantics-aware way. Diffusion Models (DMs), which comprise one of the most\nrecent and highly promising classes of methods in the field of generative\nArtificial Intelligence (AI), have emerged as a powerful tool for image data\naugmentation, capable of generating realistic and diverse images by learning\nthe underlying data distribution. The current study realizes a systematic,\ncomprehensive and in-depth review of DM-based approaches for image\naugmentation, covering a wide range of strategies, tasks and applications. In\nparticular, a comprehensive analysis of the fundamental principles, model\narchitectures and training strategies of DMs is initially performed.\nSubsequently, a taxonomy of the relevant image augmentation methods is\nintroduced, focusing on techniques regarding semantic manipulation,\npersonalization and adaptation, and application-specific augmentation tasks.\nThen, performance assessment methodologies and respective evaluation metrics\nare analyzed. Finally, current challenges and future research directions in the\nfield are discussed.\n", "link": "http://arxiv.org/abs/2407.04103v2", "date": "2025-01-10", "relevancy": 1.8301, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6302}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.616}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Diffusion%20Models%20for%20Image%20Data%20Augmentation%3A%20A%20Review%20of%0A%20%20Methods%2C%20Models%2C%20Evaluation%20Metrics%20and%20Future%20Research%20Directions&body=Title%3A%20Advances%20in%20Diffusion%20Models%20for%20Image%20Data%20Augmentation%3A%20A%20Review%20of%0A%20%20Methods%2C%20Models%2C%20Evaluation%20Metrics%20and%20Future%20Research%20Directions%0AAuthor%3A%20Panagiotis%20Alimisis%20and%20Ioannis%20Mademlis%20and%20Panagiotis%20Radoglou-Grammatikis%20and%20Panagiotis%20Sarigiannidis%20and%20Georgios%20Th.%20Papadopoulos%0AAbstract%3A%20%20%20Image%20data%20augmentation%20constitutes%20a%20critical%20methodology%20in%20modern%20computer%0Avision%20tasks%2C%20since%20it%20can%20facilitate%20towards%20enhancing%20the%20diversity%20and%0Aquality%20of%20training%20datasets%3B%20thereby%2C%20improving%20the%20performance%20and%20robustness%0Aof%20machine%20learning%20models%20in%20downstream%20tasks.%20In%20parallel%2C%20augmentation%0Aapproaches%20can%20also%20be%20used%20for%20editing/modifying%20a%20given%20image%20in%20a%20context-%0Aand%20semantics-aware%20way.%20Diffusion%20Models%20%28DMs%29%2C%20which%20comprise%20one%20of%20the%20most%0Arecent%20and%20highly%20promising%20classes%20of%20methods%20in%20the%20field%20of%20generative%0AArtificial%20Intelligence%20%28AI%29%2C%20have%20emerged%20as%20a%20powerful%20tool%20for%20image%20data%0Aaugmentation%2C%20capable%20of%20generating%20realistic%20and%20diverse%20images%20by%20learning%0Athe%20underlying%20data%20distribution.%20The%20current%20study%20realizes%20a%20systematic%2C%0Acomprehensive%20and%20in-depth%20review%20of%20DM-based%20approaches%20for%20image%0Aaugmentation%2C%20covering%20a%20wide%20range%20of%20strategies%2C%20tasks%20and%20applications.%20In%0Aparticular%2C%20a%20comprehensive%20analysis%20of%20the%20fundamental%20principles%2C%20model%0Aarchitectures%20and%20training%20strategies%20of%20DMs%20is%20initially%20performed.%0ASubsequently%2C%20a%20taxonomy%20of%20the%20relevant%20image%20augmentation%20methods%20is%0Aintroduced%2C%20focusing%20on%20techniques%20regarding%20semantic%20manipulation%2C%0Apersonalization%20and%20adaptation%2C%20and%20application-specific%20augmentation%20tasks.%0AThen%2C%20performance%20assessment%20methodologies%20and%20respective%20evaluation%20metrics%0Aare%20analyzed.%20Finally%2C%20current%20challenges%20and%20future%20research%20directions%20in%20the%0Afield%20are%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04103v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Diffusion%2520Models%2520for%2520Image%2520Data%2520Augmentation%253A%2520A%2520Review%2520of%250A%2520%2520Methods%252C%2520Models%252C%2520Evaluation%2520Metrics%2520and%2520Future%2520Research%2520Directions%26entry.906535625%3DPanagiotis%2520Alimisis%2520and%2520Ioannis%2520Mademlis%2520and%2520Panagiotis%2520Radoglou-Grammatikis%2520and%2520Panagiotis%2520Sarigiannidis%2520and%2520Georgios%2520Th.%2520Papadopoulos%26entry.1292438233%3D%2520%2520Image%2520data%2520augmentation%2520constitutes%2520a%2520critical%2520methodology%2520in%2520modern%2520computer%250Avision%2520tasks%252C%2520since%2520it%2520can%2520facilitate%2520towards%2520enhancing%2520the%2520diversity%2520and%250Aquality%2520of%2520training%2520datasets%253B%2520thereby%252C%2520improving%2520the%2520performance%2520and%2520robustness%250Aof%2520machine%2520learning%2520models%2520in%2520downstream%2520tasks.%2520In%2520parallel%252C%2520augmentation%250Aapproaches%2520can%2520also%2520be%2520used%2520for%2520editing/modifying%2520a%2520given%2520image%2520in%2520a%2520context-%250Aand%2520semantics-aware%2520way.%2520Diffusion%2520Models%2520%2528DMs%2529%252C%2520which%2520comprise%2520one%2520of%2520the%2520most%250Arecent%2520and%2520highly%2520promising%2520classes%2520of%2520methods%2520in%2520the%2520field%2520of%2520generative%250AArtificial%2520Intelligence%2520%2528AI%2529%252C%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520image%2520data%250Aaugmentation%252C%2520capable%2520of%2520generating%2520realistic%2520and%2520diverse%2520images%2520by%2520learning%250Athe%2520underlying%2520data%2520distribution.%2520The%2520current%2520study%2520realizes%2520a%2520systematic%252C%250Acomprehensive%2520and%2520in-depth%2520review%2520of%2520DM-based%2520approaches%2520for%2520image%250Aaugmentation%252C%2520covering%2520a%2520wide%2520range%2520of%2520strategies%252C%2520tasks%2520and%2520applications.%2520In%250Aparticular%252C%2520a%2520comprehensive%2520analysis%2520of%2520the%2520fundamental%2520principles%252C%2520model%250Aarchitectures%2520and%2520training%2520strategies%2520of%2520DMs%2520is%2520initially%2520performed.%250ASubsequently%252C%2520a%2520taxonomy%2520of%2520the%2520relevant%2520image%2520augmentation%2520methods%2520is%250Aintroduced%252C%2520focusing%2520on%2520techniques%2520regarding%2520semantic%2520manipulation%252C%250Apersonalization%2520and%2520adaptation%252C%2520and%2520application-specific%2520augmentation%2520tasks.%250AThen%252C%2520performance%2520assessment%2520methodologies%2520and%2520respective%2520evaluation%2520metrics%250Aare%2520analyzed.%2520Finally%252C%2520current%2520challenges%2520and%2520future%2520research%2520directions%2520in%2520the%250Afield%2520are%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04103v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Diffusion%20Models%20for%20Image%20Data%20Augmentation%3A%20A%20Review%20of%0A%20%20Methods%2C%20Models%2C%20Evaluation%20Metrics%20and%20Future%20Research%20Directions&entry.906535625=Panagiotis%20Alimisis%20and%20Ioannis%20Mademlis%20and%20Panagiotis%20Radoglou-Grammatikis%20and%20Panagiotis%20Sarigiannidis%20and%20Georgios%20Th.%20Papadopoulos&entry.1292438233=%20%20Image%20data%20augmentation%20constitutes%20a%20critical%20methodology%20in%20modern%20computer%0Avision%20tasks%2C%20since%20it%20can%20facilitate%20towards%20enhancing%20the%20diversity%20and%0Aquality%20of%20training%20datasets%3B%20thereby%2C%20improving%20the%20performance%20and%20robustness%0Aof%20machine%20learning%20models%20in%20downstream%20tasks.%20In%20parallel%2C%20augmentation%0Aapproaches%20can%20also%20be%20used%20for%20editing/modifying%20a%20given%20image%20in%20a%20context-%0Aand%20semantics-aware%20way.%20Diffusion%20Models%20%28DMs%29%2C%20which%20comprise%20one%20of%20the%20most%0Arecent%20and%20highly%20promising%20classes%20of%20methods%20in%20the%20field%20of%20generative%0AArtificial%20Intelligence%20%28AI%29%2C%20have%20emerged%20as%20a%20powerful%20tool%20for%20image%20data%0Aaugmentation%2C%20capable%20of%20generating%20realistic%20and%20diverse%20images%20by%20learning%0Athe%20underlying%20data%20distribution.%20The%20current%20study%20realizes%20a%20systematic%2C%0Acomprehensive%20and%20in-depth%20review%20of%20DM-based%20approaches%20for%20image%0Aaugmentation%2C%20covering%20a%20wide%20range%20of%20strategies%2C%20tasks%20and%20applications.%20In%0Aparticular%2C%20a%20comprehensive%20analysis%20of%20the%20fundamental%20principles%2C%20model%0Aarchitectures%20and%20training%20strategies%20of%20DMs%20is%20initially%20performed.%0ASubsequently%2C%20a%20taxonomy%20of%20the%20relevant%20image%20augmentation%20methods%20is%0Aintroduced%2C%20focusing%20on%20techniques%20regarding%20semantic%20manipulation%2C%0Apersonalization%20and%20adaptation%2C%20and%20application-specific%20augmentation%20tasks.%0AThen%2C%20performance%20assessment%20methodologies%20and%20respective%20evaluation%20metrics%0Aare%20analyzed.%20Finally%2C%20current%20challenges%20and%20future%20research%20directions%20in%20the%0Afield%20are%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04103v2&entry.124074799=Read"},
{"title": "CURing Large Models: Compression via CUR Decomposition", "author": "Sanghyeon Park and Soo-Mook Moon", "abstract": "  Large deep learning models have achieved remarkable success but are\nresource-intensive, posing challenges such as memory usage. We introduce\nCURing, a novel model compression method based on CUR matrix decomposition,\nwhich approximates weight matrices as the product of selected columns (C) and\nrows (R), and a small linking matrix (U). We apply this decomposition to\nweights chosen based on the combined influence of their magnitudes and\nactivations. By identifying and retaining informative rows and columns, CURing\nsignificantly reduces model size with minimal performance loss. For example, it\nreduces Llama3.1-8B's parameters to 7.32B (-9%) in just 129 seconds, over 20\ntimes faster than prior compression methods.\n", "link": "http://arxiv.org/abs/2501.04211v2", "date": "2025-01-10", "relevancy": 1.5002, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5338}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CURing%20Large%20Models%3A%20Compression%20via%20CUR%20Decomposition&body=Title%3A%20CURing%20Large%20Models%3A%20Compression%20via%20CUR%20Decomposition%0AAuthor%3A%20Sanghyeon%20Park%20and%20Soo-Mook%20Moon%0AAbstract%3A%20%20%20Large%20deep%20learning%20models%20have%20achieved%20remarkable%20success%20but%20are%0Aresource-intensive%2C%20posing%20challenges%20such%20as%20memory%20usage.%20We%20introduce%0ACURing%2C%20a%20novel%20model%20compression%20method%20based%20on%20CUR%20matrix%20decomposition%2C%0Awhich%20approximates%20weight%20matrices%20as%20the%20product%20of%20selected%20columns%20%28C%29%20and%0Arows%20%28R%29%2C%20and%20a%20small%20linking%20matrix%20%28U%29.%20We%20apply%20this%20decomposition%20to%0Aweights%20chosen%20based%20on%20the%20combined%20influence%20of%20their%20magnitudes%20and%0Aactivations.%20By%20identifying%20and%20retaining%20informative%20rows%20and%20columns%2C%20CURing%0Asignificantly%20reduces%20model%20size%20with%20minimal%20performance%20loss.%20For%20example%2C%20it%0Areduces%20Llama3.1-8B%27s%20parameters%20to%207.32B%20%28-9%25%29%20in%20just%20129%20seconds%2C%20over%2020%0Atimes%20faster%20than%20prior%20compression%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCURing%2520Large%2520Models%253A%2520Compression%2520via%2520CUR%2520Decomposition%26entry.906535625%3DSanghyeon%2520Park%2520and%2520Soo-Mook%2520Moon%26entry.1292438233%3D%2520%2520Large%2520deep%2520learning%2520models%2520have%2520achieved%2520remarkable%2520success%2520but%2520are%250Aresource-intensive%252C%2520posing%2520challenges%2520such%2520as%2520memory%2520usage.%2520We%2520introduce%250ACURing%252C%2520a%2520novel%2520model%2520compression%2520method%2520based%2520on%2520CUR%2520matrix%2520decomposition%252C%250Awhich%2520approximates%2520weight%2520matrices%2520as%2520the%2520product%2520of%2520selected%2520columns%2520%2528C%2529%2520and%250Arows%2520%2528R%2529%252C%2520and%2520a%2520small%2520linking%2520matrix%2520%2528U%2529.%2520We%2520apply%2520this%2520decomposition%2520to%250Aweights%2520chosen%2520based%2520on%2520the%2520combined%2520influence%2520of%2520their%2520magnitudes%2520and%250Aactivations.%2520By%2520identifying%2520and%2520retaining%2520informative%2520rows%2520and%2520columns%252C%2520CURing%250Asignificantly%2520reduces%2520model%2520size%2520with%2520minimal%2520performance%2520loss.%2520For%2520example%252C%2520it%250Areduces%2520Llama3.1-8B%2527s%2520parameters%2520to%25207.32B%2520%2528-9%2525%2529%2520in%2520just%2520129%2520seconds%252C%2520over%252020%250Atimes%2520faster%2520than%2520prior%2520compression%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CURing%20Large%20Models%3A%20Compression%20via%20CUR%20Decomposition&entry.906535625=Sanghyeon%20Park%20and%20Soo-Mook%20Moon&entry.1292438233=%20%20Large%20deep%20learning%20models%20have%20achieved%20remarkable%20success%20but%20are%0Aresource-intensive%2C%20posing%20challenges%20such%20as%20memory%20usage.%20We%20introduce%0ACURing%2C%20a%20novel%20model%20compression%20method%20based%20on%20CUR%20matrix%20decomposition%2C%0Awhich%20approximates%20weight%20matrices%20as%20the%20product%20of%20selected%20columns%20%28C%29%20and%0Arows%20%28R%29%2C%20and%20a%20small%20linking%20matrix%20%28U%29.%20We%20apply%20this%20decomposition%20to%0Aweights%20chosen%20based%20on%20the%20combined%20influence%20of%20their%20magnitudes%20and%0Aactivations.%20By%20identifying%20and%20retaining%20informative%20rows%20and%20columns%2C%20CURing%0Asignificantly%20reduces%20model%20size%20with%20minimal%20performance%20loss.%20For%20example%2C%20it%0Areduces%20Llama3.1-8B%27s%20parameters%20to%207.32B%20%28-9%25%29%20in%20just%20129%20seconds%2C%20over%2020%0Atimes%20faster%20than%20prior%20compression%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04211v2&entry.124074799=Read"},
{"title": "DUET: Dual Clustering Enhanced Multivariate Time Series Forecasting", "author": "Xiangfei Qiu and Xingjian Wu and Yan Lin and Chenjuan Guo and Jilin Hu and Bin Yang", "abstract": "  Multivariate time series forecasting is crucial for various applications,\nsuch as financial investment, energy management, weather forecasting, and\ntraffic optimization. However, accurate forecasting is challenging due to two\nmain factors. First, real-world time series often show heterogeneous temporal\npatterns caused by distribution shifts over time. Second, correlations among\nchannels are complex and intertwined, making it hard to model the interactions\namong channels precisely and flexibly.\n  In this study, we address these challenges by proposing a general framework\ncalled DUET, which introduces dual clustering on the temporal and channel\ndimensions to enhance multivariate time series forecasting. First, we design a\nTemporal Clustering Module (TCM) that clusters time series into fine-grained\ndistributions to handle heterogeneous temporal patterns. For different\ndistribution clusters, we design various pattern extractors to capture their\nintrinsic temporal patterns, thus modeling the heterogeneity. Second, we\nintroduce a novel Channel-Soft-Clustering strategy and design a Channel\nClustering Module (CCM), which captures the relationships among channels in the\nfrequency domain through metric learning and applies sparsification to mitigate\nthe adverse effects of noisy channels. Finally, DUET combines TCM and CCM to\nincorporate both the temporal and channel dimensions. Extensive experiments on\n25 real-world datasets from 10 application domains, demonstrate the\nstate-of-the-art performance of DUET.\n", "link": "http://arxiv.org/abs/2412.10859v3", "date": "2025-01-10", "relevancy": 1.4038, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4788}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4723}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUET%3A%20Dual%20Clustering%20Enhanced%20Multivariate%20Time%20Series%20Forecasting&body=Title%3A%20DUET%3A%20Dual%20Clustering%20Enhanced%20Multivariate%20Time%20Series%20Forecasting%0AAuthor%3A%20Xiangfei%20Qiu%20and%20Xingjian%20Wu%20and%20Yan%20Lin%20and%20Chenjuan%20Guo%20and%20Jilin%20Hu%20and%20Bin%20Yang%0AAbstract%3A%20%20%20Multivariate%20time%20series%20forecasting%20is%20crucial%20for%20various%20applications%2C%0Asuch%20as%20financial%20investment%2C%20energy%20management%2C%20weather%20forecasting%2C%20and%0Atraffic%20optimization.%20However%2C%20accurate%20forecasting%20is%20challenging%20due%20to%20two%0Amain%20factors.%20First%2C%20real-world%20time%20series%20often%20show%20heterogeneous%20temporal%0Apatterns%20caused%20by%20distribution%20shifts%20over%20time.%20Second%2C%20correlations%20among%0Achannels%20are%20complex%20and%20intertwined%2C%20making%20it%20hard%20to%20model%20the%20interactions%0Aamong%20channels%20precisely%20and%20flexibly.%0A%20%20In%20this%20study%2C%20we%20address%20these%20challenges%20by%20proposing%20a%20general%20framework%0Acalled%20DUET%2C%20which%20introduces%20dual%20clustering%20on%20the%20temporal%20and%20channel%0Adimensions%20to%20enhance%20multivariate%20time%20series%20forecasting.%20First%2C%20we%20design%20a%0ATemporal%20Clustering%20Module%20%28TCM%29%20that%20clusters%20time%20series%20into%20fine-grained%0Adistributions%20to%20handle%20heterogeneous%20temporal%20patterns.%20For%20different%0Adistribution%20clusters%2C%20we%20design%20various%20pattern%20extractors%20to%20capture%20their%0Aintrinsic%20temporal%20patterns%2C%20thus%20modeling%20the%20heterogeneity.%20Second%2C%20we%0Aintroduce%20a%20novel%20Channel-Soft-Clustering%20strategy%20and%20design%20a%20Channel%0AClustering%20Module%20%28CCM%29%2C%20which%20captures%20the%20relationships%20among%20channels%20in%20the%0Afrequency%20domain%20through%20metric%20learning%20and%20applies%20sparsification%20to%20mitigate%0Athe%20adverse%20effects%20of%20noisy%20channels.%20Finally%2C%20DUET%20combines%20TCM%20and%20CCM%20to%0Aincorporate%20both%20the%20temporal%20and%20channel%20dimensions.%20Extensive%20experiments%20on%0A25%20real-world%20datasets%20from%2010%20application%20domains%2C%20demonstrate%20the%0Astate-of-the-art%20performance%20of%20DUET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10859v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUET%253A%2520Dual%2520Clustering%2520Enhanced%2520Multivariate%2520Time%2520Series%2520Forecasting%26entry.906535625%3DXiangfei%2520Qiu%2520and%2520Xingjian%2520Wu%2520and%2520Yan%2520Lin%2520and%2520Chenjuan%2520Guo%2520and%2520Jilin%2520Hu%2520and%2520Bin%2520Yang%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520forecasting%2520is%2520crucial%2520for%2520various%2520applications%252C%250Asuch%2520as%2520financial%2520investment%252C%2520energy%2520management%252C%2520weather%2520forecasting%252C%2520and%250Atraffic%2520optimization.%2520However%252C%2520accurate%2520forecasting%2520is%2520challenging%2520due%2520to%2520two%250Amain%2520factors.%2520First%252C%2520real-world%2520time%2520series%2520often%2520show%2520heterogeneous%2520temporal%250Apatterns%2520caused%2520by%2520distribution%2520shifts%2520over%2520time.%2520Second%252C%2520correlations%2520among%250Achannels%2520are%2520complex%2520and%2520intertwined%252C%2520making%2520it%2520hard%2520to%2520model%2520the%2520interactions%250Aamong%2520channels%2520precisely%2520and%2520flexibly.%250A%2520%2520In%2520this%2520study%252C%2520we%2520address%2520these%2520challenges%2520by%2520proposing%2520a%2520general%2520framework%250Acalled%2520DUET%252C%2520which%2520introduces%2520dual%2520clustering%2520on%2520the%2520temporal%2520and%2520channel%250Adimensions%2520to%2520enhance%2520multivariate%2520time%2520series%2520forecasting.%2520First%252C%2520we%2520design%2520a%250ATemporal%2520Clustering%2520Module%2520%2528TCM%2529%2520that%2520clusters%2520time%2520series%2520into%2520fine-grained%250Adistributions%2520to%2520handle%2520heterogeneous%2520temporal%2520patterns.%2520For%2520different%250Adistribution%2520clusters%252C%2520we%2520design%2520various%2520pattern%2520extractors%2520to%2520capture%2520their%250Aintrinsic%2520temporal%2520patterns%252C%2520thus%2520modeling%2520the%2520heterogeneity.%2520Second%252C%2520we%250Aintroduce%2520a%2520novel%2520Channel-Soft-Clustering%2520strategy%2520and%2520design%2520a%2520Channel%250AClustering%2520Module%2520%2528CCM%2529%252C%2520which%2520captures%2520the%2520relationships%2520among%2520channels%2520in%2520the%250Afrequency%2520domain%2520through%2520metric%2520learning%2520and%2520applies%2520sparsification%2520to%2520mitigate%250Athe%2520adverse%2520effects%2520of%2520noisy%2520channels.%2520Finally%252C%2520DUET%2520combines%2520TCM%2520and%2520CCM%2520to%250Aincorporate%2520both%2520the%2520temporal%2520and%2520channel%2520dimensions.%2520Extensive%2520experiments%2520on%250A25%2520real-world%2520datasets%2520from%252010%2520application%2520domains%252C%2520demonstrate%2520the%250Astate-of-the-art%2520performance%2520of%2520DUET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10859v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUET%3A%20Dual%20Clustering%20Enhanced%20Multivariate%20Time%20Series%20Forecasting&entry.906535625=Xiangfei%20Qiu%20and%20Xingjian%20Wu%20and%20Yan%20Lin%20and%20Chenjuan%20Guo%20and%20Jilin%20Hu%20and%20Bin%20Yang&entry.1292438233=%20%20Multivariate%20time%20series%20forecasting%20is%20crucial%20for%20various%20applications%2C%0Asuch%20as%20financial%20investment%2C%20energy%20management%2C%20weather%20forecasting%2C%20and%0Atraffic%20optimization.%20However%2C%20accurate%20forecasting%20is%20challenging%20due%20to%20two%0Amain%20factors.%20First%2C%20real-world%20time%20series%20often%20show%20heterogeneous%20temporal%0Apatterns%20caused%20by%20distribution%20shifts%20over%20time.%20Second%2C%20correlations%20among%0Achannels%20are%20complex%20and%20intertwined%2C%20making%20it%20hard%20to%20model%20the%20interactions%0Aamong%20channels%20precisely%20and%20flexibly.%0A%20%20In%20this%20study%2C%20we%20address%20these%20challenges%20by%20proposing%20a%20general%20framework%0Acalled%20DUET%2C%20which%20introduces%20dual%20clustering%20on%20the%20temporal%20and%20channel%0Adimensions%20to%20enhance%20multivariate%20time%20series%20forecasting.%20First%2C%20we%20design%20a%0ATemporal%20Clustering%20Module%20%28TCM%29%20that%20clusters%20time%20series%20into%20fine-grained%0Adistributions%20to%20handle%20heterogeneous%20temporal%20patterns.%20For%20different%0Adistribution%20clusters%2C%20we%20design%20various%20pattern%20extractors%20to%20capture%20their%0Aintrinsic%20temporal%20patterns%2C%20thus%20modeling%20the%20heterogeneity.%20Second%2C%20we%0Aintroduce%20a%20novel%20Channel-Soft-Clustering%20strategy%20and%20design%20a%20Channel%0AClustering%20Module%20%28CCM%29%2C%20which%20captures%20the%20relationships%20among%20channels%20in%20the%0Afrequency%20domain%20through%20metric%20learning%20and%20applies%20sparsification%20to%20mitigate%0Athe%20adverse%20effects%20of%20noisy%20channels.%20Finally%2C%20DUET%20combines%20TCM%20and%20CCM%20to%0Aincorporate%20both%20the%20temporal%20and%20channel%20dimensions.%20Extensive%20experiments%20on%0A25%20real-world%20datasets%20from%2010%20application%20domains%2C%20demonstrate%20the%0Astate-of-the-art%20performance%20of%20DUET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10859v3&entry.124074799=Read"},
{"title": "A stochastic first-order method with multi-extrapolated momentum for\n  highly smooth unconstrained optimization", "author": "Chuan He", "abstract": "  In this paper, we consider an unconstrained stochastic optimization problem\nwhere the objective function exhibits high-order smoothness. Specifically, we\npropose a new stochastic first-order method (SFOM) with multi-extrapolated\nmomentum, in which multiple extrapolations are performed in each iteration,\nfollowed by a momentum update based on these extrapolations. We demonstrate\nthat the proposed SFOM can accelerate optimization by exploiting the high-order\nsmoothness of the objective function $f$. Assuming that the $p$th-order\nderivative of $f$ is Lipschitz continuous for some $p\\ge2$, and under\nadditional mild assumptions, we establish that our method achieves a sample\ncomplexity of $\\widetilde{\\mathcal{O}}(\\epsilon^{-(3p+1)/p})$ for finding a\npoint $x$ such that $\\mathbb{E}[\\|\\nabla f(x)\\|]\\le\\epsilon$. To the best of\nour knowledge, this is the first SFOM to leverage arbitrary-order smoothness of\nthe objective function for acceleration, resulting in a sample complexity that\nimproves upon the best-known results without assuming the mean-squared\nsmoothness condition. Preliminary numerical experiments validate the practical\nperformance of our method and support our theoretical findings.\n", "link": "http://arxiv.org/abs/2412.14488v2", "date": "2025-01-10", "relevancy": 1.2622, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4374}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4187}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20stochastic%20first-order%20method%20with%20multi-extrapolated%20momentum%20for%0A%20%20highly%20smooth%20unconstrained%20optimization&body=Title%3A%20A%20stochastic%20first-order%20method%20with%20multi-extrapolated%20momentum%20for%0A%20%20highly%20smooth%20unconstrained%20optimization%0AAuthor%3A%20Chuan%20He%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20an%20unconstrained%20stochastic%20optimization%20problem%0Awhere%20the%20objective%20function%20exhibits%20high-order%20smoothness.%20Specifically%2C%20we%0Apropose%20a%20new%20stochastic%20first-order%20method%20%28SFOM%29%20with%20multi-extrapolated%0Amomentum%2C%20in%20which%20multiple%20extrapolations%20are%20performed%20in%20each%20iteration%2C%0Afollowed%20by%20a%20momentum%20update%20based%20on%20these%20extrapolations.%20We%20demonstrate%0Athat%20the%20proposed%20SFOM%20can%20accelerate%20optimization%20by%20exploiting%20the%20high-order%0Asmoothness%20of%20the%20objective%20function%20%24f%24.%20Assuming%20that%20the%20%24p%24th-order%0Aderivative%20of%20%24f%24%20is%20Lipschitz%20continuous%20for%20some%20%24p%5Cge2%24%2C%20and%20under%0Aadditional%20mild%20assumptions%2C%20we%20establish%20that%20our%20method%20achieves%20a%20sample%0Acomplexity%20of%20%24%5Cwidetilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-%283p%2B1%29/p%7D%29%24%20for%20finding%20a%0Apoint%20%24x%24%20such%20that%20%24%5Cmathbb%7BE%7D%5B%5C%7C%5Cnabla%20f%28x%29%5C%7C%5D%5Cle%5Cepsilon%24.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20SFOM%20to%20leverage%20arbitrary-order%20smoothness%20of%0Athe%20objective%20function%20for%20acceleration%2C%20resulting%20in%20a%20sample%20complexity%20that%0Aimproves%20upon%20the%20best-known%20results%20without%20assuming%20the%20mean-squared%0Asmoothness%20condition.%20Preliminary%20numerical%20experiments%20validate%20the%20practical%0Aperformance%20of%20our%20method%20and%20support%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520stochastic%2520first-order%2520method%2520with%2520multi-extrapolated%2520momentum%2520for%250A%2520%2520highly%2520smooth%2520unconstrained%2520optimization%26entry.906535625%3DChuan%2520He%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520an%2520unconstrained%2520stochastic%2520optimization%2520problem%250Awhere%2520the%2520objective%2520function%2520exhibits%2520high-order%2520smoothness.%2520Specifically%252C%2520we%250Apropose%2520a%2520new%2520stochastic%2520first-order%2520method%2520%2528SFOM%2529%2520with%2520multi-extrapolated%250Amomentum%252C%2520in%2520which%2520multiple%2520extrapolations%2520are%2520performed%2520in%2520each%2520iteration%252C%250Afollowed%2520by%2520a%2520momentum%2520update%2520based%2520on%2520these%2520extrapolations.%2520We%2520demonstrate%250Athat%2520the%2520proposed%2520SFOM%2520can%2520accelerate%2520optimization%2520by%2520exploiting%2520the%2520high-order%250Asmoothness%2520of%2520the%2520objective%2520function%2520%2524f%2524.%2520Assuming%2520that%2520the%2520%2524p%2524th-order%250Aderivative%2520of%2520%2524f%2524%2520is%2520Lipschitz%2520continuous%2520for%2520some%2520%2524p%255Cge2%2524%252C%2520and%2520under%250Aadditional%2520mild%2520assumptions%252C%2520we%2520establish%2520that%2520our%2520method%2520achieves%2520a%2520sample%250Acomplexity%2520of%2520%2524%255Cwidetilde%257B%255Cmathcal%257BO%257D%257D%2528%255Cepsilon%255E%257B-%25283p%252B1%2529/p%257D%2529%2524%2520for%2520finding%2520a%250Apoint%2520%2524x%2524%2520such%2520that%2520%2524%255Cmathbb%257BE%257D%255B%255C%257C%255Cnabla%2520f%2528x%2529%255C%257C%255D%255Cle%255Cepsilon%2524.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520this%2520is%2520the%2520first%2520SFOM%2520to%2520leverage%2520arbitrary-order%2520smoothness%2520of%250Athe%2520objective%2520function%2520for%2520acceleration%252C%2520resulting%2520in%2520a%2520sample%2520complexity%2520that%250Aimproves%2520upon%2520the%2520best-known%2520results%2520without%2520assuming%2520the%2520mean-squared%250Asmoothness%2520condition.%2520Preliminary%2520numerical%2520experiments%2520validate%2520the%2520practical%250Aperformance%2520of%2520our%2520method%2520and%2520support%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20stochastic%20first-order%20method%20with%20multi-extrapolated%20momentum%20for%0A%20%20highly%20smooth%20unconstrained%20optimization&entry.906535625=Chuan%20He&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20an%20unconstrained%20stochastic%20optimization%20problem%0Awhere%20the%20objective%20function%20exhibits%20high-order%20smoothness.%20Specifically%2C%20we%0Apropose%20a%20new%20stochastic%20first-order%20method%20%28SFOM%29%20with%20multi-extrapolated%0Amomentum%2C%20in%20which%20multiple%20extrapolations%20are%20performed%20in%20each%20iteration%2C%0Afollowed%20by%20a%20momentum%20update%20based%20on%20these%20extrapolations.%20We%20demonstrate%0Athat%20the%20proposed%20SFOM%20can%20accelerate%20optimization%20by%20exploiting%20the%20high-order%0Asmoothness%20of%20the%20objective%20function%20%24f%24.%20Assuming%20that%20the%20%24p%24th-order%0Aderivative%20of%20%24f%24%20is%20Lipschitz%20continuous%20for%20some%20%24p%5Cge2%24%2C%20and%20under%0Aadditional%20mild%20assumptions%2C%20we%20establish%20that%20our%20method%20achieves%20a%20sample%0Acomplexity%20of%20%24%5Cwidetilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-%283p%2B1%29/p%7D%29%24%20for%20finding%20a%0Apoint%20%24x%24%20such%20that%20%24%5Cmathbb%7BE%7D%5B%5C%7C%5Cnabla%20f%28x%29%5C%7C%5D%5Cle%5Cepsilon%24.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20SFOM%20to%20leverage%20arbitrary-order%20smoothness%20of%0Athe%20objective%20function%20for%20acceleration%2C%20resulting%20in%20a%20sample%20complexity%20that%0Aimproves%20upon%20the%20best-known%20results%20without%20assuming%20the%20mean-squared%0Asmoothness%20condition.%20Preliminary%20numerical%20experiments%20validate%20the%20practical%0Aperformance%20of%20our%20method%20and%20support%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14488v2&entry.124074799=Read"},
{"title": "Closing the Gap: A User Study on the Real-world Usefulness of AI-powered\n  Vulnerability Detection & Repair in the IDE", "author": "Benjamin Steenhoek and Kalpathy Sivaraman and Renata Saldivar Gonzalez and Yevhen Mohylevskyy and Roshanak Zilouchian Moghaddam and Wei Le", "abstract": "  This paper presents the first empirical study of a vulnerability detection\nand fix tool with professional software developers on real projects that they\nown. We implemented DeepVulGuard, an IDE-integrated tool based on\nstate-of-the-art detection and fix models, and show that it has promising\nperformance on benchmarks of historic vulnerability data. DeepVulGuard scans\ncode for vulnerabilities (including identifying the vulnerability type and\nvulnerable region of code), suggests fixes, provides natural-language\nexplanations for alerts and fixes, leveraging chat interfaces. We recruited 17\nprofessional software developers at Microsoft, observed their usage of the tool\non their code, and conducted interviews to assess the tool's usefulness, speed,\ntrust, relevance, and workflow integration. We also gathered detailed\nqualitative feedback on users' perceptions and their desired features. Study\nparticipants scanned a total of 24 projects, 6.9k files, and over 1.7 million\nlines of source code, and generated 170 alerts and 50 fix suggestions. We find\nthat although state-of-the-art AI-powered detection and fix tools show promise,\nthey are not yet practical for real-world use due to a high rate of false\npositives and non-applicable fixes. User feedback reveals several actionable\npain points, ranging from incomplete context to lack of customization for the\nuser's codebase. Additionally, we explore how AI features, including confidence\nscores, explanations, and chat interaction, can apply to vulnerability\ndetection and fixing. Based on these insights, we offer practical\nrecommendations for evaluating and deploying AI detection and fix models. Our\ncode and data are available at https://doi.org/10.6084/m9.figshare.26367139.\n", "link": "http://arxiv.org/abs/2412.14306v2", "date": "2025-01-10", "relevancy": 1.3325, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4577}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4447}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Gap%3A%20A%20User%20Study%20on%20the%20Real-world%20Usefulness%20of%20AI-powered%0A%20%20Vulnerability%20Detection%20%26%20Repair%20in%20the%20IDE&body=Title%3A%20Closing%20the%20Gap%3A%20A%20User%20Study%20on%20the%20Real-world%20Usefulness%20of%20AI-powered%0A%20%20Vulnerability%20Detection%20%26%20Repair%20in%20the%20IDE%0AAuthor%3A%20Benjamin%20Steenhoek%20and%20Kalpathy%20Sivaraman%20and%20Renata%20Saldivar%20Gonzalez%20and%20Yevhen%20Mohylevskyy%20and%20Roshanak%20Zilouchian%20Moghaddam%20and%20Wei%20Le%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20first%20empirical%20study%20of%20a%20vulnerability%20detection%0Aand%20fix%20tool%20with%20professional%20software%20developers%20on%20real%20projects%20that%20they%0Aown.%20We%20implemented%20DeepVulGuard%2C%20an%20IDE-integrated%20tool%20based%20on%0Astate-of-the-art%20detection%20and%20fix%20models%2C%20and%20show%20that%20it%20has%20promising%0Aperformance%20on%20benchmarks%20of%20historic%20vulnerability%20data.%20DeepVulGuard%20scans%0Acode%20for%20vulnerabilities%20%28including%20identifying%20the%20vulnerability%20type%20and%0Avulnerable%20region%20of%20code%29%2C%20suggests%20fixes%2C%20provides%20natural-language%0Aexplanations%20for%20alerts%20and%20fixes%2C%20leveraging%20chat%20interfaces.%20We%20recruited%2017%0Aprofessional%20software%20developers%20at%20Microsoft%2C%20observed%20their%20usage%20of%20the%20tool%0Aon%20their%20code%2C%20and%20conducted%20interviews%20to%20assess%20the%20tool%27s%20usefulness%2C%20speed%2C%0Atrust%2C%20relevance%2C%20and%20workflow%20integration.%20We%20also%20gathered%20detailed%0Aqualitative%20feedback%20on%20users%27%20perceptions%20and%20their%20desired%20features.%20Study%0Aparticipants%20scanned%20a%20total%20of%2024%20projects%2C%206.9k%20files%2C%20and%20over%201.7%20million%0Alines%20of%20source%20code%2C%20and%20generated%20170%20alerts%20and%2050%20fix%20suggestions.%20We%20find%0Athat%20although%20state-of-the-art%20AI-powered%20detection%20and%20fix%20tools%20show%20promise%2C%0Athey%20are%20not%20yet%20practical%20for%20real-world%20use%20due%20to%20a%20high%20rate%20of%20false%0Apositives%20and%20non-applicable%20fixes.%20User%20feedback%20reveals%20several%20actionable%0Apain%20points%2C%20ranging%20from%20incomplete%20context%20to%20lack%20of%20customization%20for%20the%0Auser%27s%20codebase.%20Additionally%2C%20we%20explore%20how%20AI%20features%2C%20including%20confidence%0Ascores%2C%20explanations%2C%20and%20chat%20interaction%2C%20can%20apply%20to%20vulnerability%0Adetection%20and%20fixing.%20Based%20on%20these%20insights%2C%20we%20offer%20practical%0Arecommendations%20for%20evaluating%20and%20deploying%20AI%20detection%20and%20fix%20models.%20Our%0Acode%20and%20data%20are%20available%20at%20https%3A//doi.org/10.6084/m9.figshare.26367139.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520Gap%253A%2520A%2520User%2520Study%2520on%2520the%2520Real-world%2520Usefulness%2520of%2520AI-powered%250A%2520%2520Vulnerability%2520Detection%2520%2526%2520Repair%2520in%2520the%2520IDE%26entry.906535625%3DBenjamin%2520Steenhoek%2520and%2520Kalpathy%2520Sivaraman%2520and%2520Renata%2520Saldivar%2520Gonzalez%2520and%2520Yevhen%2520Mohylevskyy%2520and%2520Roshanak%2520Zilouchian%2520Moghaddam%2520and%2520Wei%2520Le%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520first%2520empirical%2520study%2520of%2520a%2520vulnerability%2520detection%250Aand%2520fix%2520tool%2520with%2520professional%2520software%2520developers%2520on%2520real%2520projects%2520that%2520they%250Aown.%2520We%2520implemented%2520DeepVulGuard%252C%2520an%2520IDE-integrated%2520tool%2520based%2520on%250Astate-of-the-art%2520detection%2520and%2520fix%2520models%252C%2520and%2520show%2520that%2520it%2520has%2520promising%250Aperformance%2520on%2520benchmarks%2520of%2520historic%2520vulnerability%2520data.%2520DeepVulGuard%2520scans%250Acode%2520for%2520vulnerabilities%2520%2528including%2520identifying%2520the%2520vulnerability%2520type%2520and%250Avulnerable%2520region%2520of%2520code%2529%252C%2520suggests%2520fixes%252C%2520provides%2520natural-language%250Aexplanations%2520for%2520alerts%2520and%2520fixes%252C%2520leveraging%2520chat%2520interfaces.%2520We%2520recruited%252017%250Aprofessional%2520software%2520developers%2520at%2520Microsoft%252C%2520observed%2520their%2520usage%2520of%2520the%2520tool%250Aon%2520their%2520code%252C%2520and%2520conducted%2520interviews%2520to%2520assess%2520the%2520tool%2527s%2520usefulness%252C%2520speed%252C%250Atrust%252C%2520relevance%252C%2520and%2520workflow%2520integration.%2520We%2520also%2520gathered%2520detailed%250Aqualitative%2520feedback%2520on%2520users%2527%2520perceptions%2520and%2520their%2520desired%2520features.%2520Study%250Aparticipants%2520scanned%2520a%2520total%2520of%252024%2520projects%252C%25206.9k%2520files%252C%2520and%2520over%25201.7%2520million%250Alines%2520of%2520source%2520code%252C%2520and%2520generated%2520170%2520alerts%2520and%252050%2520fix%2520suggestions.%2520We%2520find%250Athat%2520although%2520state-of-the-art%2520AI-powered%2520detection%2520and%2520fix%2520tools%2520show%2520promise%252C%250Athey%2520are%2520not%2520yet%2520practical%2520for%2520real-world%2520use%2520due%2520to%2520a%2520high%2520rate%2520of%2520false%250Apositives%2520and%2520non-applicable%2520fixes.%2520User%2520feedback%2520reveals%2520several%2520actionable%250Apain%2520points%252C%2520ranging%2520from%2520incomplete%2520context%2520to%2520lack%2520of%2520customization%2520for%2520the%250Auser%2527s%2520codebase.%2520Additionally%252C%2520we%2520explore%2520how%2520AI%2520features%252C%2520including%2520confidence%250Ascores%252C%2520explanations%252C%2520and%2520chat%2520interaction%252C%2520can%2520apply%2520to%2520vulnerability%250Adetection%2520and%2520fixing.%2520Based%2520on%2520these%2520insights%252C%2520we%2520offer%2520practical%250Arecommendations%2520for%2520evaluating%2520and%2520deploying%2520AI%2520detection%2520and%2520fix%2520models.%2520Our%250Acode%2520and%2520data%2520are%2520available%2520at%2520https%253A//doi.org/10.6084/m9.figshare.26367139.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Gap%3A%20A%20User%20Study%20on%20the%20Real-world%20Usefulness%20of%20AI-powered%0A%20%20Vulnerability%20Detection%20%26%20Repair%20in%20the%20IDE&entry.906535625=Benjamin%20Steenhoek%20and%20Kalpathy%20Sivaraman%20and%20Renata%20Saldivar%20Gonzalez%20and%20Yevhen%20Mohylevskyy%20and%20Roshanak%20Zilouchian%20Moghaddam%20and%20Wei%20Le&entry.1292438233=%20%20This%20paper%20presents%20the%20first%20empirical%20study%20of%20a%20vulnerability%20detection%0Aand%20fix%20tool%20with%20professional%20software%20developers%20on%20real%20projects%20that%20they%0Aown.%20We%20implemented%20DeepVulGuard%2C%20an%20IDE-integrated%20tool%20based%20on%0Astate-of-the-art%20detection%20and%20fix%20models%2C%20and%20show%20that%20it%20has%20promising%0Aperformance%20on%20benchmarks%20of%20historic%20vulnerability%20data.%20DeepVulGuard%20scans%0Acode%20for%20vulnerabilities%20%28including%20identifying%20the%20vulnerability%20type%20and%0Avulnerable%20region%20of%20code%29%2C%20suggests%20fixes%2C%20provides%20natural-language%0Aexplanations%20for%20alerts%20and%20fixes%2C%20leveraging%20chat%20interfaces.%20We%20recruited%2017%0Aprofessional%20software%20developers%20at%20Microsoft%2C%20observed%20their%20usage%20of%20the%20tool%0Aon%20their%20code%2C%20and%20conducted%20interviews%20to%20assess%20the%20tool%27s%20usefulness%2C%20speed%2C%0Atrust%2C%20relevance%2C%20and%20workflow%20integration.%20We%20also%20gathered%20detailed%0Aqualitative%20feedback%20on%20users%27%20perceptions%20and%20their%20desired%20features.%20Study%0Aparticipants%20scanned%20a%20total%20of%2024%20projects%2C%206.9k%20files%2C%20and%20over%201.7%20million%0Alines%20of%20source%20code%2C%20and%20generated%20170%20alerts%20and%2050%20fix%20suggestions.%20We%20find%0Athat%20although%20state-of-the-art%20AI-powered%20detection%20and%20fix%20tools%20show%20promise%2C%0Athey%20are%20not%20yet%20practical%20for%20real-world%20use%20due%20to%20a%20high%20rate%20of%20false%0Apositives%20and%20non-applicable%20fixes.%20User%20feedback%20reveals%20several%20actionable%0Apain%20points%2C%20ranging%20from%20incomplete%20context%20to%20lack%20of%20customization%20for%20the%0Auser%27s%20codebase.%20Additionally%2C%20we%20explore%20how%20AI%20features%2C%20including%20confidence%0Ascores%2C%20explanations%2C%20and%20chat%20interaction%2C%20can%20apply%20to%20vulnerability%0Adetection%20and%20fixing.%20Based%20on%20these%20insights%2C%20we%20offer%20practical%0Arecommendations%20for%20evaluating%20and%20deploying%20AI%20detection%20and%20fix%20models.%20Our%0Acode%20and%20data%20are%20available%20at%20https%3A//doi.org/10.6084/m9.figshare.26367139.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14306v2&entry.124074799=Read"},
{"title": "The Expressive Power of Graph Neural Networks: A Survey", "author": "Bingxu Zhang and Changjun Fan and Shixuan Liu and Kuihua Huang and Xiang Zhao and Jincai Huang and Zhong Liu", "abstract": "  Graph neural networks (GNNs) are effective machine learning models for many\ngraph-related applications. Despite their empirical success, many research\nefforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressive\npower. Early works in this domain mainly focus on studying the graph\nisomorphism recognition ability of GNNs, and recent works try to leverage the\nproperties such as subgraph counting and connectivity learning to characterize\nthe expressive power of GNNs, which are more practical and closer to\nreal-world. However, no survey papers and open-source repositories\ncomprehensively summarize and discuss models in this important direction. To\nfill the gap, we conduct a first survey for models for enhancing expressive\npower under different forms of definition. Concretely, the models are reviewed\nbased on three categories, i.e., Graph feature enhancement, Graph topology\nenhancement, and GNNs architecture enhancement.\n", "link": "http://arxiv.org/abs/2308.08235v2", "date": "2025-01-10", "relevancy": 1.4312, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4836}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4696}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Expressive%20Power%20of%20Graph%20Neural%20Networks%3A%20A%20Survey&body=Title%3A%20The%20Expressive%20Power%20of%20Graph%20Neural%20Networks%3A%20A%20Survey%0AAuthor%3A%20Bingxu%20Zhang%20and%20Changjun%20Fan%20and%20Shixuan%20Liu%20and%20Kuihua%20Huang%20and%20Xiang%20Zhao%20and%20Jincai%20Huang%20and%20Zhong%20Liu%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20effective%20machine%20learning%20models%20for%20many%0Agraph-related%20applications.%20Despite%20their%20empirical%20success%2C%20many%20research%0Aefforts%20focus%20on%20the%20theoretical%20limitations%20of%20GNNs%2C%20i.e.%2C%20the%20GNNs%20expressive%0Apower.%20Early%20works%20in%20this%20domain%20mainly%20focus%20on%20studying%20the%20graph%0Aisomorphism%20recognition%20ability%20of%20GNNs%2C%20and%20recent%20works%20try%20to%20leverage%20the%0Aproperties%20such%20as%20subgraph%20counting%20and%20connectivity%20learning%20to%20characterize%0Athe%20expressive%20power%20of%20GNNs%2C%20which%20are%20more%20practical%20and%20closer%20to%0Areal-world.%20However%2C%20no%20survey%20papers%20and%20open-source%20repositories%0Acomprehensively%20summarize%20and%20discuss%20models%20in%20this%20important%20direction.%20To%0Afill%20the%20gap%2C%20we%20conduct%20a%20first%20survey%20for%20models%20for%20enhancing%20expressive%0Apower%20under%20different%20forms%20of%20definition.%20Concretely%2C%20the%20models%20are%20reviewed%0Abased%20on%20three%20categories%2C%20i.e.%2C%20Graph%20feature%20enhancement%2C%20Graph%20topology%0Aenhancement%2C%20and%20GNNs%20architecture%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Expressive%2520Power%2520of%2520Graph%2520Neural%2520Networks%253A%2520A%2520Survey%26entry.906535625%3DBingxu%2520Zhang%2520and%2520Changjun%2520Fan%2520and%2520Shixuan%2520Liu%2520and%2520Kuihua%2520Huang%2520and%2520Xiang%2520Zhao%2520and%2520Jincai%2520Huang%2520and%2520Zhong%2520Liu%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520effective%2520machine%2520learning%2520models%2520for%2520many%250Agraph-related%2520applications.%2520Despite%2520their%2520empirical%2520success%252C%2520many%2520research%250Aefforts%2520focus%2520on%2520the%2520theoretical%2520limitations%2520of%2520GNNs%252C%2520i.e.%252C%2520the%2520GNNs%2520expressive%250Apower.%2520Early%2520works%2520in%2520this%2520domain%2520mainly%2520focus%2520on%2520studying%2520the%2520graph%250Aisomorphism%2520recognition%2520ability%2520of%2520GNNs%252C%2520and%2520recent%2520works%2520try%2520to%2520leverage%2520the%250Aproperties%2520such%2520as%2520subgraph%2520counting%2520and%2520connectivity%2520learning%2520to%2520characterize%250Athe%2520expressive%2520power%2520of%2520GNNs%252C%2520which%2520are%2520more%2520practical%2520and%2520closer%2520to%250Areal-world.%2520However%252C%2520no%2520survey%2520papers%2520and%2520open-source%2520repositories%250Acomprehensively%2520summarize%2520and%2520discuss%2520models%2520in%2520this%2520important%2520direction.%2520To%250Afill%2520the%2520gap%252C%2520we%2520conduct%2520a%2520first%2520survey%2520for%2520models%2520for%2520enhancing%2520expressive%250Apower%2520under%2520different%2520forms%2520of%2520definition.%2520Concretely%252C%2520the%2520models%2520are%2520reviewed%250Abased%2520on%2520three%2520categories%252C%2520i.e.%252C%2520Graph%2520feature%2520enhancement%252C%2520Graph%2520topology%250Aenhancement%252C%2520and%2520GNNs%2520architecture%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.08235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Expressive%20Power%20of%20Graph%20Neural%20Networks%3A%20A%20Survey&entry.906535625=Bingxu%20Zhang%20and%20Changjun%20Fan%20and%20Shixuan%20Liu%20and%20Kuihua%20Huang%20and%20Xiang%20Zhao%20and%20Jincai%20Huang%20and%20Zhong%20Liu&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20are%20effective%20machine%20learning%20models%20for%20many%0Agraph-related%20applications.%20Despite%20their%20empirical%20success%2C%20many%20research%0Aefforts%20focus%20on%20the%20theoretical%20limitations%20of%20GNNs%2C%20i.e.%2C%20the%20GNNs%20expressive%0Apower.%20Early%20works%20in%20this%20domain%20mainly%20focus%20on%20studying%20the%20graph%0Aisomorphism%20recognition%20ability%20of%20GNNs%2C%20and%20recent%20works%20try%20to%20leverage%20the%0Aproperties%20such%20as%20subgraph%20counting%20and%20connectivity%20learning%20to%20characterize%0Athe%20expressive%20power%20of%20GNNs%2C%20which%20are%20more%20practical%20and%20closer%20to%0Areal-world.%20However%2C%20no%20survey%20papers%20and%20open-source%20repositories%0Acomprehensively%20summarize%20and%20discuss%20models%20in%20this%20important%20direction.%20To%0Afill%20the%20gap%2C%20we%20conduct%20a%20first%20survey%20for%20models%20for%20enhancing%20expressive%0Apower%20under%20different%20forms%20of%20definition.%20Concretely%2C%20the%20models%20are%20reviewed%0Abased%20on%20three%20categories%2C%20i.e.%2C%20Graph%20feature%20enhancement%2C%20Graph%20topology%0Aenhancement%2C%20and%20GNNs%20architecture%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08235v2&entry.124074799=Read"},
{"title": "Scale-up Unlearnable Examples Learning with High-Performance Computing", "author": "Yanfan Zhu and Issac Lyngaas and Murali Gopalakrishnan Meena and Mary Ellen I. Koran and Bradley Malin and Daniel Moyer and Shunxing Bao and Anuj Kapadia and Xiao Wang and Bennett Landman and Yuankai Huo", "abstract": "  Recent advancements in AI models are structured to retain user interactions,\nwhich could inadvertently include sensitive healthcare data. In the healthcare\nfield, particularly when radiologists use AI-driven diagnostic tools hosted on\nonline platforms, there is a risk that medical imaging data may be repurposed\nfor future AI training without explicit consent, spotlighting critical privacy\nand intellectual property concerns around healthcare data usage. Addressing\nthese privacy challenges, a novel approach known as Unlearnable Examples (UEs)\nhas been introduced, aiming to make data unlearnable to deep learning models. A\nprominent method within this area, called Unlearnable Clustering (UC), has\nshown improved UE performance with larger batch sizes but was previously\nlimited by computational resources. To push the boundaries of UE performance\nwith theoretically unlimited resources, we scaled up UC learning across various\ndatasets using Distributed Data Parallel (DDP) training on the Summit\nsupercomputer. Our goal was to examine UE efficacy at high-performance\ncomputing (HPC) levels to prevent unauthorized learning and enhance data\nsecurity, particularly exploring the impact of batch size on UE's\nunlearnability. Utilizing the robust computational capabilities of the Summit,\nextensive experiments were conducted on diverse datasets such as Pets,\nMedMNist, Flowers, and Flowers102. Our findings reveal that both overly large\nand overly small batch sizes can lead to performance instability and affect\naccuracy. However, the relationship between batch size and unlearnability\nvaried across datasets, highlighting the necessity for tailored batch size\nstrategies to achieve optimal data protection. Our results underscore the\ncritical role of selecting appropriate batch sizes based on the specific\ncharacteristics of each dataset to prevent learning and ensure data security in\ndeep learning applications.\n", "link": "http://arxiv.org/abs/2501.06080v1", "date": "2025-01-10", "relevancy": 1.4766, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5351}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4867}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scale-up%20Unlearnable%20Examples%20Learning%20with%20High-Performance%20Computing&body=Title%3A%20Scale-up%20Unlearnable%20Examples%20Learning%20with%20High-Performance%20Computing%0AAuthor%3A%20Yanfan%20Zhu%20and%20Issac%20Lyngaas%20and%20Murali%20Gopalakrishnan%20Meena%20and%20Mary%20Ellen%20I.%20Koran%20and%20Bradley%20Malin%20and%20Daniel%20Moyer%20and%20Shunxing%20Bao%20and%20Anuj%20Kapadia%20and%20Xiao%20Wang%20and%20Bennett%20Landman%20and%20Yuankai%20Huo%0AAbstract%3A%20%20%20Recent%20advancements%20in%20AI%20models%20are%20structured%20to%20retain%20user%20interactions%2C%0Awhich%20could%20inadvertently%20include%20sensitive%20healthcare%20data.%20In%20the%20healthcare%0Afield%2C%20particularly%20when%20radiologists%20use%20AI-driven%20diagnostic%20tools%20hosted%20on%0Aonline%20platforms%2C%20there%20is%20a%20risk%20that%20medical%20imaging%20data%20may%20be%20repurposed%0Afor%20future%20AI%20training%20without%20explicit%20consent%2C%20spotlighting%20critical%20privacy%0Aand%20intellectual%20property%20concerns%20around%20healthcare%20data%20usage.%20Addressing%0Athese%20privacy%20challenges%2C%20a%20novel%20approach%20known%20as%20Unlearnable%20Examples%20%28UEs%29%0Ahas%20been%20introduced%2C%20aiming%20to%20make%20data%20unlearnable%20to%20deep%20learning%20models.%20A%0Aprominent%20method%20within%20this%20area%2C%20called%20Unlearnable%20Clustering%20%28UC%29%2C%20has%0Ashown%20improved%20UE%20performance%20with%20larger%20batch%20sizes%20but%20was%20previously%0Alimited%20by%20computational%20resources.%20To%20push%20the%20boundaries%20of%20UE%20performance%0Awith%20theoretically%20unlimited%20resources%2C%20we%20scaled%20up%20UC%20learning%20across%20various%0Adatasets%20using%20Distributed%20Data%20Parallel%20%28DDP%29%20training%20on%20the%20Summit%0Asupercomputer.%20Our%20goal%20was%20to%20examine%20UE%20efficacy%20at%20high-performance%0Acomputing%20%28HPC%29%20levels%20to%20prevent%20unauthorized%20learning%20and%20enhance%20data%0Asecurity%2C%20particularly%20exploring%20the%20impact%20of%20batch%20size%20on%20UE%27s%0Aunlearnability.%20Utilizing%20the%20robust%20computational%20capabilities%20of%20the%20Summit%2C%0Aextensive%20experiments%20were%20conducted%20on%20diverse%20datasets%20such%20as%20Pets%2C%0AMedMNist%2C%20Flowers%2C%20and%20Flowers102.%20Our%20findings%20reveal%20that%20both%20overly%20large%0Aand%20overly%20small%20batch%20sizes%20can%20lead%20to%20performance%20instability%20and%20affect%0Aaccuracy.%20However%2C%20the%20relationship%20between%20batch%20size%20and%20unlearnability%0Avaried%20across%20datasets%2C%20highlighting%20the%20necessity%20for%20tailored%20batch%20size%0Astrategies%20to%20achieve%20optimal%20data%20protection.%20Our%20results%20underscore%20the%0Acritical%20role%20of%20selecting%20appropriate%20batch%20sizes%20based%20on%20the%20specific%0Acharacteristics%20of%20each%20dataset%20to%20prevent%20learning%20and%20ensure%20data%20security%20in%0Adeep%20learning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScale-up%2520Unlearnable%2520Examples%2520Learning%2520with%2520High-Performance%2520Computing%26entry.906535625%3DYanfan%2520Zhu%2520and%2520Issac%2520Lyngaas%2520and%2520Murali%2520Gopalakrishnan%2520Meena%2520and%2520Mary%2520Ellen%2520I.%2520Koran%2520and%2520Bradley%2520Malin%2520and%2520Daniel%2520Moyer%2520and%2520Shunxing%2520Bao%2520and%2520Anuj%2520Kapadia%2520and%2520Xiao%2520Wang%2520and%2520Bennett%2520Landman%2520and%2520Yuankai%2520Huo%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520AI%2520models%2520are%2520structured%2520to%2520retain%2520user%2520interactions%252C%250Awhich%2520could%2520inadvertently%2520include%2520sensitive%2520healthcare%2520data.%2520In%2520the%2520healthcare%250Afield%252C%2520particularly%2520when%2520radiologists%2520use%2520AI-driven%2520diagnostic%2520tools%2520hosted%2520on%250Aonline%2520platforms%252C%2520there%2520is%2520a%2520risk%2520that%2520medical%2520imaging%2520data%2520may%2520be%2520repurposed%250Afor%2520future%2520AI%2520training%2520without%2520explicit%2520consent%252C%2520spotlighting%2520critical%2520privacy%250Aand%2520intellectual%2520property%2520concerns%2520around%2520healthcare%2520data%2520usage.%2520Addressing%250Athese%2520privacy%2520challenges%252C%2520a%2520novel%2520approach%2520known%2520as%2520Unlearnable%2520Examples%2520%2528UEs%2529%250Ahas%2520been%2520introduced%252C%2520aiming%2520to%2520make%2520data%2520unlearnable%2520to%2520deep%2520learning%2520models.%2520A%250Aprominent%2520method%2520within%2520this%2520area%252C%2520called%2520Unlearnable%2520Clustering%2520%2528UC%2529%252C%2520has%250Ashown%2520improved%2520UE%2520performance%2520with%2520larger%2520batch%2520sizes%2520but%2520was%2520previously%250Alimited%2520by%2520computational%2520resources.%2520To%2520push%2520the%2520boundaries%2520of%2520UE%2520performance%250Awith%2520theoretically%2520unlimited%2520resources%252C%2520we%2520scaled%2520up%2520UC%2520learning%2520across%2520various%250Adatasets%2520using%2520Distributed%2520Data%2520Parallel%2520%2528DDP%2529%2520training%2520on%2520the%2520Summit%250Asupercomputer.%2520Our%2520goal%2520was%2520to%2520examine%2520UE%2520efficacy%2520at%2520high-performance%250Acomputing%2520%2528HPC%2529%2520levels%2520to%2520prevent%2520unauthorized%2520learning%2520and%2520enhance%2520data%250Asecurity%252C%2520particularly%2520exploring%2520the%2520impact%2520of%2520batch%2520size%2520on%2520UE%2527s%250Aunlearnability.%2520Utilizing%2520the%2520robust%2520computational%2520capabilities%2520of%2520the%2520Summit%252C%250Aextensive%2520experiments%2520were%2520conducted%2520on%2520diverse%2520datasets%2520such%2520as%2520Pets%252C%250AMedMNist%252C%2520Flowers%252C%2520and%2520Flowers102.%2520Our%2520findings%2520reveal%2520that%2520both%2520overly%2520large%250Aand%2520overly%2520small%2520batch%2520sizes%2520can%2520lead%2520to%2520performance%2520instability%2520and%2520affect%250Aaccuracy.%2520However%252C%2520the%2520relationship%2520between%2520batch%2520size%2520and%2520unlearnability%250Avaried%2520across%2520datasets%252C%2520highlighting%2520the%2520necessity%2520for%2520tailored%2520batch%2520size%250Astrategies%2520to%2520achieve%2520optimal%2520data%2520protection.%2520Our%2520results%2520underscore%2520the%250Acritical%2520role%2520of%2520selecting%2520appropriate%2520batch%2520sizes%2520based%2520on%2520the%2520specific%250Acharacteristics%2520of%2520each%2520dataset%2520to%2520prevent%2520learning%2520and%2520ensure%2520data%2520security%2520in%250Adeep%2520learning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scale-up%20Unlearnable%20Examples%20Learning%20with%20High-Performance%20Computing&entry.906535625=Yanfan%20Zhu%20and%20Issac%20Lyngaas%20and%20Murali%20Gopalakrishnan%20Meena%20and%20Mary%20Ellen%20I.%20Koran%20and%20Bradley%20Malin%20and%20Daniel%20Moyer%20and%20Shunxing%20Bao%20and%20Anuj%20Kapadia%20and%20Xiao%20Wang%20and%20Bennett%20Landman%20and%20Yuankai%20Huo&entry.1292438233=%20%20Recent%20advancements%20in%20AI%20models%20are%20structured%20to%20retain%20user%20interactions%2C%0Awhich%20could%20inadvertently%20include%20sensitive%20healthcare%20data.%20In%20the%20healthcare%0Afield%2C%20particularly%20when%20radiologists%20use%20AI-driven%20diagnostic%20tools%20hosted%20on%0Aonline%20platforms%2C%20there%20is%20a%20risk%20that%20medical%20imaging%20data%20may%20be%20repurposed%0Afor%20future%20AI%20training%20without%20explicit%20consent%2C%20spotlighting%20critical%20privacy%0Aand%20intellectual%20property%20concerns%20around%20healthcare%20data%20usage.%20Addressing%0Athese%20privacy%20challenges%2C%20a%20novel%20approach%20known%20as%20Unlearnable%20Examples%20%28UEs%29%0Ahas%20been%20introduced%2C%20aiming%20to%20make%20data%20unlearnable%20to%20deep%20learning%20models.%20A%0Aprominent%20method%20within%20this%20area%2C%20called%20Unlearnable%20Clustering%20%28UC%29%2C%20has%0Ashown%20improved%20UE%20performance%20with%20larger%20batch%20sizes%20but%20was%20previously%0Alimited%20by%20computational%20resources.%20To%20push%20the%20boundaries%20of%20UE%20performance%0Awith%20theoretically%20unlimited%20resources%2C%20we%20scaled%20up%20UC%20learning%20across%20various%0Adatasets%20using%20Distributed%20Data%20Parallel%20%28DDP%29%20training%20on%20the%20Summit%0Asupercomputer.%20Our%20goal%20was%20to%20examine%20UE%20efficacy%20at%20high-performance%0Acomputing%20%28HPC%29%20levels%20to%20prevent%20unauthorized%20learning%20and%20enhance%20data%0Asecurity%2C%20particularly%20exploring%20the%20impact%20of%20batch%20size%20on%20UE%27s%0Aunlearnability.%20Utilizing%20the%20robust%20computational%20capabilities%20of%20the%20Summit%2C%0Aextensive%20experiments%20were%20conducted%20on%20diverse%20datasets%20such%20as%20Pets%2C%0AMedMNist%2C%20Flowers%2C%20and%20Flowers102.%20Our%20findings%20reveal%20that%20both%20overly%20large%0Aand%20overly%20small%20batch%20sizes%20can%20lead%20to%20performance%20instability%20and%20affect%0Aaccuracy.%20However%2C%20the%20relationship%20between%20batch%20size%20and%20unlearnability%0Avaried%20across%20datasets%2C%20highlighting%20the%20necessity%20for%20tailored%20batch%20size%0Astrategies%20to%20achieve%20optimal%20data%20protection.%20Our%20results%20underscore%20the%0Acritical%20role%20of%20selecting%20appropriate%20batch%20sizes%20based%20on%20the%20specific%0Acharacteristics%20of%20each%20dataset%20to%20prevent%20learning%20and%20ensure%20data%20security%20in%0Adeep%20learning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06080v1&entry.124074799=Read"},
{"title": "A monthly sub-national Harmonized Food Insecurity Dataset for\n  comprehensive analysis and predictive modeling", "author": "Machefer M\u00e9lissande and Michele Ronco and Anne-Claire Thomas and Michael Assouline and Melanie Rabier and Christina Corbane and Felix Rembold", "abstract": "  Food security is a complex, multidimensional concept challenging to measure\ncomprehensively. Effective anticipation, monitoring, and mitigation of food\ncrises require timely and comprehensive global data. This paper introduces the\nHarmonized Food Insecurity Dataset (HFID), an open-source resource\nconsolidating four key data sources: the Integrated Food Security Phase\nClassification (IPC)/Cadre Harmonis\\'e (CH) phases, the Famine Early Warning\nSystems Network (FEWS NET) IPC-compatible phases, and the World Food Program's\n(WFP) Food Consumption Score (FCS) and reduced Coping Strategy Index (rCSI).\nUpdated monthly and using a common reference system for administrative units,\nthe HFID offers extensive spatial and temporal coverage. It serves as a vital\ntool for food security experts and humanitarian agencies, providing a unified\nresource for analyzing food security conditions and highlighting global data\ndisparities. The scientific community can also leverage the HFID to develop\ndata-driven predictive models, enhancing the capacity to forecast and prevent\nfuture food crises.\n", "link": "http://arxiv.org/abs/2501.06076v1", "date": "2025-01-10", "relevancy": 1.1637, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4037}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3773}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20monthly%20sub-national%20Harmonized%20Food%20Insecurity%20Dataset%20for%0A%20%20comprehensive%20analysis%20and%20predictive%20modeling&body=Title%3A%20A%20monthly%20sub-national%20Harmonized%20Food%20Insecurity%20Dataset%20for%0A%20%20comprehensive%20analysis%20and%20predictive%20modeling%0AAuthor%3A%20Machefer%20M%C3%A9lissande%20and%20Michele%20Ronco%20and%20Anne-Claire%20Thomas%20and%20Michael%20Assouline%20and%20Melanie%20Rabier%20and%20Christina%20Corbane%20and%20Felix%20Rembold%0AAbstract%3A%20%20%20Food%20security%20is%20a%20complex%2C%20multidimensional%20concept%20challenging%20to%20measure%0Acomprehensively.%20Effective%20anticipation%2C%20monitoring%2C%20and%20mitigation%20of%20food%0Acrises%20require%20timely%20and%20comprehensive%20global%20data.%20This%20paper%20introduces%20the%0AHarmonized%20Food%20Insecurity%20Dataset%20%28HFID%29%2C%20an%20open-source%20resource%0Aconsolidating%20four%20key%20data%20sources%3A%20the%20Integrated%20Food%20Security%20Phase%0AClassification%20%28IPC%29/Cadre%20Harmonis%5C%27e%20%28CH%29%20phases%2C%20the%20Famine%20Early%20Warning%0ASystems%20Network%20%28FEWS%20NET%29%20IPC-compatible%20phases%2C%20and%20the%20World%20Food%20Program%27s%0A%28WFP%29%20Food%20Consumption%20Score%20%28FCS%29%20and%20reduced%20Coping%20Strategy%20Index%20%28rCSI%29.%0AUpdated%20monthly%20and%20using%20a%20common%20reference%20system%20for%20administrative%20units%2C%0Athe%20HFID%20offers%20extensive%20spatial%20and%20temporal%20coverage.%20It%20serves%20as%20a%20vital%0Atool%20for%20food%20security%20experts%20and%20humanitarian%20agencies%2C%20providing%20a%20unified%0Aresource%20for%20analyzing%20food%20security%20conditions%20and%20highlighting%20global%20data%0Adisparities.%20The%20scientific%20community%20can%20also%20leverage%20the%20HFID%20to%20develop%0Adata-driven%20predictive%20models%2C%20enhancing%20the%20capacity%20to%20forecast%20and%20prevent%0Afuture%20food%20crises.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520monthly%2520sub-national%2520Harmonized%2520Food%2520Insecurity%2520Dataset%2520for%250A%2520%2520comprehensive%2520analysis%2520and%2520predictive%2520modeling%26entry.906535625%3DMachefer%2520M%25C3%25A9lissande%2520and%2520Michele%2520Ronco%2520and%2520Anne-Claire%2520Thomas%2520and%2520Michael%2520Assouline%2520and%2520Melanie%2520Rabier%2520and%2520Christina%2520Corbane%2520and%2520Felix%2520Rembold%26entry.1292438233%3D%2520%2520Food%2520security%2520is%2520a%2520complex%252C%2520multidimensional%2520concept%2520challenging%2520to%2520measure%250Acomprehensively.%2520Effective%2520anticipation%252C%2520monitoring%252C%2520and%2520mitigation%2520of%2520food%250Acrises%2520require%2520timely%2520and%2520comprehensive%2520global%2520data.%2520This%2520paper%2520introduces%2520the%250AHarmonized%2520Food%2520Insecurity%2520Dataset%2520%2528HFID%2529%252C%2520an%2520open-source%2520resource%250Aconsolidating%2520four%2520key%2520data%2520sources%253A%2520the%2520Integrated%2520Food%2520Security%2520Phase%250AClassification%2520%2528IPC%2529/Cadre%2520Harmonis%255C%2527e%2520%2528CH%2529%2520phases%252C%2520the%2520Famine%2520Early%2520Warning%250ASystems%2520Network%2520%2528FEWS%2520NET%2529%2520IPC-compatible%2520phases%252C%2520and%2520the%2520World%2520Food%2520Program%2527s%250A%2528WFP%2529%2520Food%2520Consumption%2520Score%2520%2528FCS%2529%2520and%2520reduced%2520Coping%2520Strategy%2520Index%2520%2528rCSI%2529.%250AUpdated%2520monthly%2520and%2520using%2520a%2520common%2520reference%2520system%2520for%2520administrative%2520units%252C%250Athe%2520HFID%2520offers%2520extensive%2520spatial%2520and%2520temporal%2520coverage.%2520It%2520serves%2520as%2520a%2520vital%250Atool%2520for%2520food%2520security%2520experts%2520and%2520humanitarian%2520agencies%252C%2520providing%2520a%2520unified%250Aresource%2520for%2520analyzing%2520food%2520security%2520conditions%2520and%2520highlighting%2520global%2520data%250Adisparities.%2520The%2520scientific%2520community%2520can%2520also%2520leverage%2520the%2520HFID%2520to%2520develop%250Adata-driven%2520predictive%2520models%252C%2520enhancing%2520the%2520capacity%2520to%2520forecast%2520and%2520prevent%250Afuture%2520food%2520crises.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20monthly%20sub-national%20Harmonized%20Food%20Insecurity%20Dataset%20for%0A%20%20comprehensive%20analysis%20and%20predictive%20modeling&entry.906535625=Machefer%20M%C3%A9lissande%20and%20Michele%20Ronco%20and%20Anne-Claire%20Thomas%20and%20Michael%20Assouline%20and%20Melanie%20Rabier%20and%20Christina%20Corbane%20and%20Felix%20Rembold&entry.1292438233=%20%20Food%20security%20is%20a%20complex%2C%20multidimensional%20concept%20challenging%20to%20measure%0Acomprehensively.%20Effective%20anticipation%2C%20monitoring%2C%20and%20mitigation%20of%20food%0Acrises%20require%20timely%20and%20comprehensive%20global%20data.%20This%20paper%20introduces%20the%0AHarmonized%20Food%20Insecurity%20Dataset%20%28HFID%29%2C%20an%20open-source%20resource%0Aconsolidating%20four%20key%20data%20sources%3A%20the%20Integrated%20Food%20Security%20Phase%0AClassification%20%28IPC%29/Cadre%20Harmonis%5C%27e%20%28CH%29%20phases%2C%20the%20Famine%20Early%20Warning%0ASystems%20Network%20%28FEWS%20NET%29%20IPC-compatible%20phases%2C%20and%20the%20World%20Food%20Program%27s%0A%28WFP%29%20Food%20Consumption%20Score%20%28FCS%29%20and%20reduced%20Coping%20Strategy%20Index%20%28rCSI%29.%0AUpdated%20monthly%20and%20using%20a%20common%20reference%20system%20for%20administrative%20units%2C%0Athe%20HFID%20offers%20extensive%20spatial%20and%20temporal%20coverage.%20It%20serves%20as%20a%20vital%0Atool%20for%20food%20security%20experts%20and%20humanitarian%20agencies%2C%20providing%20a%20unified%0Aresource%20for%20analyzing%20food%20security%20conditions%20and%20highlighting%20global%20data%0Adisparities.%20The%20scientific%20community%20can%20also%20leverage%20the%20HFID%20to%20develop%0Adata-driven%20predictive%20models%2C%20enhancing%20the%20capacity%20to%20forecast%20and%20prevent%0Afuture%20food%20crises.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06076v1&entry.124074799=Read"},
{"title": "Environment Modeling for Service Robots From a Task Execution\n  Perspective", "author": "Ying Zhang and Guohui Tian and Cui-Hua Zhang and Changchun Hua and Weili Ding and Choon Ki Ahn", "abstract": "  Service robots are increasingly entering the home to provide domestic tasks\nfor residents. However, when working in an open, dynamic, and unstructured home\nenvironment, service robots still face challenges such as low intelligence for\ntask execution and poor long-term autonomy (LTA), which has limited their\ndeployment. As the basis of robotic task execution, environment modeling has\nattracted significant attention. This integrates core technologies such as\nenvironment perception, understanding, and representation to accurately\nrecognize environmental information. This paper presents a comprehensive survey\nof environmental modeling from a new task-executionoriented perspective. In\nparticular, guided by the requirements of robots in performing domestic service\ntasks in the home environment, we systematically review the progress that has\nbeen made in task-execution-oriented environmental modeling in four respects:\n1) localization, 2) navigation, 3) manipulation, and 4) LTA. Current challenges\nare discussed, and potential research opportunities are also highlighted.\n", "link": "http://arxiv.org/abs/2501.05931v1", "date": "2025-01-10", "relevancy": 1.7148, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6736}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5647}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Environment%20Modeling%20for%20Service%20Robots%20From%20a%20Task%20Execution%0A%20%20Perspective&body=Title%3A%20Environment%20Modeling%20for%20Service%20Robots%20From%20a%20Task%20Execution%0A%20%20Perspective%0AAuthor%3A%20Ying%20Zhang%20and%20Guohui%20Tian%20and%20Cui-Hua%20Zhang%20and%20Changchun%20Hua%20and%20Weili%20Ding%20and%20Choon%20Ki%20Ahn%0AAbstract%3A%20%20%20Service%20robots%20are%20increasingly%20entering%20the%20home%20to%20provide%20domestic%20tasks%0Afor%20residents.%20However%2C%20when%20working%20in%20an%20open%2C%20dynamic%2C%20and%20unstructured%20home%0Aenvironment%2C%20service%20robots%20still%20face%20challenges%20such%20as%20low%20intelligence%20for%0Atask%20execution%20and%20poor%20long-term%20autonomy%20%28LTA%29%2C%20which%20has%20limited%20their%0Adeployment.%20As%20the%20basis%20of%20robotic%20task%20execution%2C%20environment%20modeling%20has%0Aattracted%20significant%20attention.%20This%20integrates%20core%20technologies%20such%20as%0Aenvironment%20perception%2C%20understanding%2C%20and%20representation%20to%20accurately%0Arecognize%20environmental%20information.%20This%20paper%20presents%20a%20comprehensive%20survey%0Aof%20environmental%20modeling%20from%20a%20new%20task-executionoriented%20perspective.%20In%0Aparticular%2C%20guided%20by%20the%20requirements%20of%20robots%20in%20performing%20domestic%20service%0Atasks%20in%20the%20home%20environment%2C%20we%20systematically%20review%20the%20progress%20that%20has%0Abeen%20made%20in%20task-execution-oriented%20environmental%20modeling%20in%20four%20respects%3A%0A1%29%20localization%2C%202%29%20navigation%2C%203%29%20manipulation%2C%20and%204%29%20LTA.%20Current%20challenges%0Aare%20discussed%2C%20and%20potential%20research%20opportunities%20are%20also%20highlighted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvironment%2520Modeling%2520for%2520Service%2520Robots%2520From%2520a%2520Task%2520Execution%250A%2520%2520Perspective%26entry.906535625%3DYing%2520Zhang%2520and%2520Guohui%2520Tian%2520and%2520Cui-Hua%2520Zhang%2520and%2520Changchun%2520Hua%2520and%2520Weili%2520Ding%2520and%2520Choon%2520Ki%2520Ahn%26entry.1292438233%3D%2520%2520Service%2520robots%2520are%2520increasingly%2520entering%2520the%2520home%2520to%2520provide%2520domestic%2520tasks%250Afor%2520residents.%2520However%252C%2520when%2520working%2520in%2520an%2520open%252C%2520dynamic%252C%2520and%2520unstructured%2520home%250Aenvironment%252C%2520service%2520robots%2520still%2520face%2520challenges%2520such%2520as%2520low%2520intelligence%2520for%250Atask%2520execution%2520and%2520poor%2520long-term%2520autonomy%2520%2528LTA%2529%252C%2520which%2520has%2520limited%2520their%250Adeployment.%2520As%2520the%2520basis%2520of%2520robotic%2520task%2520execution%252C%2520environment%2520modeling%2520has%250Aattracted%2520significant%2520attention.%2520This%2520integrates%2520core%2520technologies%2520such%2520as%250Aenvironment%2520perception%252C%2520understanding%252C%2520and%2520representation%2520to%2520accurately%250Arecognize%2520environmental%2520information.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520survey%250Aof%2520environmental%2520modeling%2520from%2520a%2520new%2520task-executionoriented%2520perspective.%2520In%250Aparticular%252C%2520guided%2520by%2520the%2520requirements%2520of%2520robots%2520in%2520performing%2520domestic%2520service%250Atasks%2520in%2520the%2520home%2520environment%252C%2520we%2520systematically%2520review%2520the%2520progress%2520that%2520has%250Abeen%2520made%2520in%2520task-execution-oriented%2520environmental%2520modeling%2520in%2520four%2520respects%253A%250A1%2529%2520localization%252C%25202%2529%2520navigation%252C%25203%2529%2520manipulation%252C%2520and%25204%2529%2520LTA.%2520Current%2520challenges%250Aare%2520discussed%252C%2520and%2520potential%2520research%2520opportunities%2520are%2520also%2520highlighted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Environment%20Modeling%20for%20Service%20Robots%20From%20a%20Task%20Execution%0A%20%20Perspective&entry.906535625=Ying%20Zhang%20and%20Guohui%20Tian%20and%20Cui-Hua%20Zhang%20and%20Changchun%20Hua%20and%20Weili%20Ding%20and%20Choon%20Ki%20Ahn&entry.1292438233=%20%20Service%20robots%20are%20increasingly%20entering%20the%20home%20to%20provide%20domestic%20tasks%0Afor%20residents.%20However%2C%20when%20working%20in%20an%20open%2C%20dynamic%2C%20and%20unstructured%20home%0Aenvironment%2C%20service%20robots%20still%20face%20challenges%20such%20as%20low%20intelligence%20for%0Atask%20execution%20and%20poor%20long-term%20autonomy%20%28LTA%29%2C%20which%20has%20limited%20their%0Adeployment.%20As%20the%20basis%20of%20robotic%20task%20execution%2C%20environment%20modeling%20has%0Aattracted%20significant%20attention.%20This%20integrates%20core%20technologies%20such%20as%0Aenvironment%20perception%2C%20understanding%2C%20and%20representation%20to%20accurately%0Arecognize%20environmental%20information.%20This%20paper%20presents%20a%20comprehensive%20survey%0Aof%20environmental%20modeling%20from%20a%20new%20task-executionoriented%20perspective.%20In%0Aparticular%2C%20guided%20by%20the%20requirements%20of%20robots%20in%20performing%20domestic%20service%0Atasks%20in%20the%20home%20environment%2C%20we%20systematically%20review%20the%20progress%20that%20has%0Abeen%20made%20in%20task-execution-oriented%20environmental%20modeling%20in%20four%20respects%3A%0A1%29%20localization%2C%202%29%20navigation%2C%203%29%20manipulation%2C%20and%204%29%20LTA.%20Current%20challenges%0Aare%20discussed%2C%20and%20potential%20research%20opportunities%20are%20also%20highlighted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05931v1&entry.124074799=Read"},
{"title": "Exploring the Use of Robots for Diary Studies", "author": "Michael F. Xu and Bilge Mutlu", "abstract": "  As interest in studying in-the-wild human-robot interaction grows, there is a\nneed for methods to collect data over time and in naturalistic or potentially\nprivate environments. HRI researchers have increasingly used the diary method\nfor these studies, asking study participants to self-administer a structured\ndata collection instrument, i.e., a diary, over a period of time. Although the\ndiary method offers a unique window into settings that researchers may not have\naccess to, they also lack the interactivity and probing that interview-based\nmethods offer. In this paper, we explore a novel data collection method in\nwhich a robot plays the role of an interactive diary. We developed the Diary\nRobot system and performed in-home deployments for a week to evaluate the\nfeasibility and effectiveness of this approach. Using traditional text-based\nand audio-based diaries as benchmarks, we found that robots are able to\neffectively elicit the intended information. We reflect on our findings, and\ndescribe scenarios where the utilization of robots in diary studies as a data\ncollection instrument may be especially applicable.\n", "link": "http://arxiv.org/abs/2501.04860v2", "date": "2025-01-10", "relevancy": 1.4715, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5586}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4764}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Use%20of%20Robots%20for%20Diary%20Studies&body=Title%3A%20Exploring%20the%20Use%20of%20Robots%20for%20Diary%20Studies%0AAuthor%3A%20Michael%20F.%20Xu%20and%20Bilge%20Mutlu%0AAbstract%3A%20%20%20As%20interest%20in%20studying%20in-the-wild%20human-robot%20interaction%20grows%2C%20there%20is%20a%0Aneed%20for%20methods%20to%20collect%20data%20over%20time%20and%20in%20naturalistic%20or%20potentially%0Aprivate%20environments.%20HRI%20researchers%20have%20increasingly%20used%20the%20diary%20method%0Afor%20these%20studies%2C%20asking%20study%20participants%20to%20self-administer%20a%20structured%0Adata%20collection%20instrument%2C%20i.e.%2C%20a%20diary%2C%20over%20a%20period%20of%20time.%20Although%20the%0Adiary%20method%20offers%20a%20unique%20window%20into%20settings%20that%20researchers%20may%20not%20have%0Aaccess%20to%2C%20they%20also%20lack%20the%20interactivity%20and%20probing%20that%20interview-based%0Amethods%20offer.%20In%20this%20paper%2C%20we%20explore%20a%20novel%20data%20collection%20method%20in%0Awhich%20a%20robot%20plays%20the%20role%20of%20an%20interactive%20diary.%20We%20developed%20the%20Diary%0ARobot%20system%20and%20performed%20in-home%20deployments%20for%20a%20week%20to%20evaluate%20the%0Afeasibility%20and%20effectiveness%20of%20this%20approach.%20Using%20traditional%20text-based%0Aand%20audio-based%20diaries%20as%20benchmarks%2C%20we%20found%20that%20robots%20are%20able%20to%0Aeffectively%20elicit%20the%20intended%20information.%20We%20reflect%20on%20our%20findings%2C%20and%0Adescribe%20scenarios%20where%20the%20utilization%20of%20robots%20in%20diary%20studies%20as%20a%20data%0Acollection%20instrument%20may%20be%20especially%20applicable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04860v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Use%2520of%2520Robots%2520for%2520Diary%2520Studies%26entry.906535625%3DMichael%2520F.%2520Xu%2520and%2520Bilge%2520Mutlu%26entry.1292438233%3D%2520%2520As%2520interest%2520in%2520studying%2520in-the-wild%2520human-robot%2520interaction%2520grows%252C%2520there%2520is%2520a%250Aneed%2520for%2520methods%2520to%2520collect%2520data%2520over%2520time%2520and%2520in%2520naturalistic%2520or%2520potentially%250Aprivate%2520environments.%2520HRI%2520researchers%2520have%2520increasingly%2520used%2520the%2520diary%2520method%250Afor%2520these%2520studies%252C%2520asking%2520study%2520participants%2520to%2520self-administer%2520a%2520structured%250Adata%2520collection%2520instrument%252C%2520i.e.%252C%2520a%2520diary%252C%2520over%2520a%2520period%2520of%2520time.%2520Although%2520the%250Adiary%2520method%2520offers%2520a%2520unique%2520window%2520into%2520settings%2520that%2520researchers%2520may%2520not%2520have%250Aaccess%2520to%252C%2520they%2520also%2520lack%2520the%2520interactivity%2520and%2520probing%2520that%2520interview-based%250Amethods%2520offer.%2520In%2520this%2520paper%252C%2520we%2520explore%2520a%2520novel%2520data%2520collection%2520method%2520in%250Awhich%2520a%2520robot%2520plays%2520the%2520role%2520of%2520an%2520interactive%2520diary.%2520We%2520developed%2520the%2520Diary%250ARobot%2520system%2520and%2520performed%2520in-home%2520deployments%2520for%2520a%2520week%2520to%2520evaluate%2520the%250Afeasibility%2520and%2520effectiveness%2520of%2520this%2520approach.%2520Using%2520traditional%2520text-based%250Aand%2520audio-based%2520diaries%2520as%2520benchmarks%252C%2520we%2520found%2520that%2520robots%2520are%2520able%2520to%250Aeffectively%2520elicit%2520the%2520intended%2520information.%2520We%2520reflect%2520on%2520our%2520findings%252C%2520and%250Adescribe%2520scenarios%2520where%2520the%2520utilization%2520of%2520robots%2520in%2520diary%2520studies%2520as%2520a%2520data%250Acollection%2520instrument%2520may%2520be%2520especially%2520applicable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04860v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Use%20of%20Robots%20for%20Diary%20Studies&entry.906535625=Michael%20F.%20Xu%20and%20Bilge%20Mutlu&entry.1292438233=%20%20As%20interest%20in%20studying%20in-the-wild%20human-robot%20interaction%20grows%2C%20there%20is%20a%0Aneed%20for%20methods%20to%20collect%20data%20over%20time%20and%20in%20naturalistic%20or%20potentially%0Aprivate%20environments.%20HRI%20researchers%20have%20increasingly%20used%20the%20diary%20method%0Afor%20these%20studies%2C%20asking%20study%20participants%20to%20self-administer%20a%20structured%0Adata%20collection%20instrument%2C%20i.e.%2C%20a%20diary%2C%20over%20a%20period%20of%20time.%20Although%20the%0Adiary%20method%20offers%20a%20unique%20window%20into%20settings%20that%20researchers%20may%20not%20have%0Aaccess%20to%2C%20they%20also%20lack%20the%20interactivity%20and%20probing%20that%20interview-based%0Amethods%20offer.%20In%20this%20paper%2C%20we%20explore%20a%20novel%20data%20collection%20method%20in%0Awhich%20a%20robot%20plays%20the%20role%20of%20an%20interactive%20diary.%20We%20developed%20the%20Diary%0ARobot%20system%20and%20performed%20in-home%20deployments%20for%20a%20week%20to%20evaluate%20the%0Afeasibility%20and%20effectiveness%20of%20this%20approach.%20Using%20traditional%20text-based%0Aand%20audio-based%20diaries%20as%20benchmarks%2C%20we%20found%20that%20robots%20are%20able%20to%0Aeffectively%20elicit%20the%20intended%20information.%20We%20reflect%20on%20our%20findings%2C%20and%0Adescribe%20scenarios%20where%20the%20utilization%20of%20robots%20in%20diary%20studies%20as%20a%20data%0Acollection%20instrument%20may%20be%20especially%20applicable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04860v2&entry.124074799=Read"},
{"title": "Averaged Adam accelerates stochastic optimization in the training of\n  deep neural network approximations for partial differential equation and\n  optimal control problems", "author": "Steffen Dereich and Arnulf Jentzen and Adrian Riekert", "abstract": "  Deep learning methods - usually consisting of a class of deep neural networks\n(DNNs) trained by a stochastic gradient descent (SGD) optimization method - are\nnowadays omnipresent in data-driven learning problems as well as in scientific\ncomputing tasks such as optimal control (OC) and partial differential equation\n(PDE) problems. In practically relevant learning tasks, often not the\nplain-vanilla standard SGD optimization method is employed to train the\nconsidered class of DNNs but instead more sophisticated adaptive and\naccelerated variants of the standard SGD method such as the popular Adam\noptimizer are used. Inspired by the classical Polyak-Ruppert averaging\napproach, in this work we apply averaged variants of the Adam optimizer to\ntrain DNNs to approximately solve exemplary scientific computing problems in\nthe form of PDEs and OC problems. We test the averaged variants of Adam in a\nseries of learning problems including physics-informed neural network (PINN),\ndeep backward stochastic differential equation (deep BSDE), and deep Kolmogorov\napproximations for PDEs (such as heat, Black-Scholes, Burgers, and Allen-Cahn\nPDEs), including DNN approximations for OC problems, and including DNN\napproximations for image classification problems (ResNet for CIFAR-10). In each\nof the numerical examples the employed averaged variants of Adam outperform the\nstandard Adam and the standard SGD optimizers, particularly, in the situation\nof the scientific machine learning problems. The Python source codes for the\nnumerical experiments associated to this work can be found on GitHub at\nhttps://github.com/deeplearningmethods/averaged-adam.\n", "link": "http://arxiv.org/abs/2501.06081v1", "date": "2025-01-10", "relevancy": 1.5413, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5429}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4856}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Averaged%20Adam%20accelerates%20stochastic%20optimization%20in%20the%20training%20of%0A%20%20deep%20neural%20network%20approximations%20for%20partial%20differential%20equation%20and%0A%20%20optimal%20control%20problems&body=Title%3A%20Averaged%20Adam%20accelerates%20stochastic%20optimization%20in%20the%20training%20of%0A%20%20deep%20neural%20network%20approximations%20for%20partial%20differential%20equation%20and%0A%20%20optimal%20control%20problems%0AAuthor%3A%20Steffen%20Dereich%20and%20Arnulf%20Jentzen%20and%20Adrian%20Riekert%0AAbstract%3A%20%20%20Deep%20learning%20methods%20-%20usually%20consisting%20of%20a%20class%20of%20deep%20neural%20networks%0A%28DNNs%29%20trained%20by%20a%20stochastic%20gradient%20descent%20%28SGD%29%20optimization%20method%20-%20are%0Anowadays%20omnipresent%20in%20data-driven%20learning%20problems%20as%20well%20as%20in%20scientific%0Acomputing%20tasks%20such%20as%20optimal%20control%20%28OC%29%20and%20partial%20differential%20equation%0A%28PDE%29%20problems.%20In%20practically%20relevant%20learning%20tasks%2C%20often%20not%20the%0Aplain-vanilla%20standard%20SGD%20optimization%20method%20is%20employed%20to%20train%20the%0Aconsidered%20class%20of%20DNNs%20but%20instead%20more%20sophisticated%20adaptive%20and%0Aaccelerated%20variants%20of%20the%20standard%20SGD%20method%20such%20as%20the%20popular%20Adam%0Aoptimizer%20are%20used.%20Inspired%20by%20the%20classical%20Polyak-Ruppert%20averaging%0Aapproach%2C%20in%20this%20work%20we%20apply%20averaged%20variants%20of%20the%20Adam%20optimizer%20to%0Atrain%20DNNs%20to%20approximately%20solve%20exemplary%20scientific%20computing%20problems%20in%0Athe%20form%20of%20PDEs%20and%20OC%20problems.%20We%20test%20the%20averaged%20variants%20of%20Adam%20in%20a%0Aseries%20of%20learning%20problems%20including%20physics-informed%20neural%20network%20%28PINN%29%2C%0Adeep%20backward%20stochastic%20differential%20equation%20%28deep%20BSDE%29%2C%20and%20deep%20Kolmogorov%0Aapproximations%20for%20PDEs%20%28such%20as%20heat%2C%20Black-Scholes%2C%20Burgers%2C%20and%20Allen-Cahn%0APDEs%29%2C%20including%20DNN%20approximations%20for%20OC%20problems%2C%20and%20including%20DNN%0Aapproximations%20for%20image%20classification%20problems%20%28ResNet%20for%20CIFAR-10%29.%20In%20each%0Aof%20the%20numerical%20examples%20the%20employed%20averaged%20variants%20of%20Adam%20outperform%20the%0Astandard%20Adam%20and%20the%20standard%20SGD%20optimizers%2C%20particularly%2C%20in%20the%20situation%0Aof%20the%20scientific%20machine%20learning%20problems.%20The%20Python%20source%20codes%20for%20the%0Anumerical%20experiments%20associated%20to%20this%20work%20can%20be%20found%20on%20GitHub%20at%0Ahttps%3A//github.com/deeplearningmethods/averaged-adam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAveraged%2520Adam%2520accelerates%2520stochastic%2520optimization%2520in%2520the%2520training%2520of%250A%2520%2520deep%2520neural%2520network%2520approximations%2520for%2520partial%2520differential%2520equation%2520and%250A%2520%2520optimal%2520control%2520problems%26entry.906535625%3DSteffen%2520Dereich%2520and%2520Arnulf%2520Jentzen%2520and%2520Adrian%2520Riekert%26entry.1292438233%3D%2520%2520Deep%2520learning%2520methods%2520-%2520usually%2520consisting%2520of%2520a%2520class%2520of%2520deep%2520neural%2520networks%250A%2528DNNs%2529%2520trained%2520by%2520a%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520optimization%2520method%2520-%2520are%250Anowadays%2520omnipresent%2520in%2520data-driven%2520learning%2520problems%2520as%2520well%2520as%2520in%2520scientific%250Acomputing%2520tasks%2520such%2520as%2520optimal%2520control%2520%2528OC%2529%2520and%2520partial%2520differential%2520equation%250A%2528PDE%2529%2520problems.%2520In%2520practically%2520relevant%2520learning%2520tasks%252C%2520often%2520not%2520the%250Aplain-vanilla%2520standard%2520SGD%2520optimization%2520method%2520is%2520employed%2520to%2520train%2520the%250Aconsidered%2520class%2520of%2520DNNs%2520but%2520instead%2520more%2520sophisticated%2520adaptive%2520and%250Aaccelerated%2520variants%2520of%2520the%2520standard%2520SGD%2520method%2520such%2520as%2520the%2520popular%2520Adam%250Aoptimizer%2520are%2520used.%2520Inspired%2520by%2520the%2520classical%2520Polyak-Ruppert%2520averaging%250Aapproach%252C%2520in%2520this%2520work%2520we%2520apply%2520averaged%2520variants%2520of%2520the%2520Adam%2520optimizer%2520to%250Atrain%2520DNNs%2520to%2520approximately%2520solve%2520exemplary%2520scientific%2520computing%2520problems%2520in%250Athe%2520form%2520of%2520PDEs%2520and%2520OC%2520problems.%2520We%2520test%2520the%2520averaged%2520variants%2520of%2520Adam%2520in%2520a%250Aseries%2520of%2520learning%2520problems%2520including%2520physics-informed%2520neural%2520network%2520%2528PINN%2529%252C%250Adeep%2520backward%2520stochastic%2520differential%2520equation%2520%2528deep%2520BSDE%2529%252C%2520and%2520deep%2520Kolmogorov%250Aapproximations%2520for%2520PDEs%2520%2528such%2520as%2520heat%252C%2520Black-Scholes%252C%2520Burgers%252C%2520and%2520Allen-Cahn%250APDEs%2529%252C%2520including%2520DNN%2520approximations%2520for%2520OC%2520problems%252C%2520and%2520including%2520DNN%250Aapproximations%2520for%2520image%2520classification%2520problems%2520%2528ResNet%2520for%2520CIFAR-10%2529.%2520In%2520each%250Aof%2520the%2520numerical%2520examples%2520the%2520employed%2520averaged%2520variants%2520of%2520Adam%2520outperform%2520the%250Astandard%2520Adam%2520and%2520the%2520standard%2520SGD%2520optimizers%252C%2520particularly%252C%2520in%2520the%2520situation%250Aof%2520the%2520scientific%2520machine%2520learning%2520problems.%2520The%2520Python%2520source%2520codes%2520for%2520the%250Anumerical%2520experiments%2520associated%2520to%2520this%2520work%2520can%2520be%2520found%2520on%2520GitHub%2520at%250Ahttps%253A//github.com/deeplearningmethods/averaged-adam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Averaged%20Adam%20accelerates%20stochastic%20optimization%20in%20the%20training%20of%0A%20%20deep%20neural%20network%20approximations%20for%20partial%20differential%20equation%20and%0A%20%20optimal%20control%20problems&entry.906535625=Steffen%20Dereich%20and%20Arnulf%20Jentzen%20and%20Adrian%20Riekert&entry.1292438233=%20%20Deep%20learning%20methods%20-%20usually%20consisting%20of%20a%20class%20of%20deep%20neural%20networks%0A%28DNNs%29%20trained%20by%20a%20stochastic%20gradient%20descent%20%28SGD%29%20optimization%20method%20-%20are%0Anowadays%20omnipresent%20in%20data-driven%20learning%20problems%20as%20well%20as%20in%20scientific%0Acomputing%20tasks%20such%20as%20optimal%20control%20%28OC%29%20and%20partial%20differential%20equation%0A%28PDE%29%20problems.%20In%20practically%20relevant%20learning%20tasks%2C%20often%20not%20the%0Aplain-vanilla%20standard%20SGD%20optimization%20method%20is%20employed%20to%20train%20the%0Aconsidered%20class%20of%20DNNs%20but%20instead%20more%20sophisticated%20adaptive%20and%0Aaccelerated%20variants%20of%20the%20standard%20SGD%20method%20such%20as%20the%20popular%20Adam%0Aoptimizer%20are%20used.%20Inspired%20by%20the%20classical%20Polyak-Ruppert%20averaging%0Aapproach%2C%20in%20this%20work%20we%20apply%20averaged%20variants%20of%20the%20Adam%20optimizer%20to%0Atrain%20DNNs%20to%20approximately%20solve%20exemplary%20scientific%20computing%20problems%20in%0Athe%20form%20of%20PDEs%20and%20OC%20problems.%20We%20test%20the%20averaged%20variants%20of%20Adam%20in%20a%0Aseries%20of%20learning%20problems%20including%20physics-informed%20neural%20network%20%28PINN%29%2C%0Adeep%20backward%20stochastic%20differential%20equation%20%28deep%20BSDE%29%2C%20and%20deep%20Kolmogorov%0Aapproximations%20for%20PDEs%20%28such%20as%20heat%2C%20Black-Scholes%2C%20Burgers%2C%20and%20Allen-Cahn%0APDEs%29%2C%20including%20DNN%20approximations%20for%20OC%20problems%2C%20and%20including%20DNN%0Aapproximations%20for%20image%20classification%20problems%20%28ResNet%20for%20CIFAR-10%29.%20In%20each%0Aof%20the%20numerical%20examples%20the%20employed%20averaged%20variants%20of%20Adam%20outperform%20the%0Astandard%20Adam%20and%20the%20standard%20SGD%20optimizers%2C%20particularly%2C%20in%20the%20situation%0Aof%20the%20scientific%20machine%20learning%20problems.%20The%20Python%20source%20codes%20for%20the%0Anumerical%20experiments%20associated%20to%20this%20work%20can%20be%20found%20on%20GitHub%20at%0Ahttps%3A//github.com/deeplearningmethods/averaged-adam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06081v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


